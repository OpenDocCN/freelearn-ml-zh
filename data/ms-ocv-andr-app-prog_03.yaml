- en: Chapter 3. Detecting Objects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章. 检测对象
- en: 'One of the common applications of computer vision is to detect objects in an
    image or video. For example, we can use this method to detect a particular book
    in a heap of many books. One of the methods to detect objects is **feature matching**.
    In this chapter, we will learn the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机视觉的一个常见应用是在图像或视频中检测对象。例如，我们可以使用这种方法在许多书籍中检测特定的书籍。检测对象的一种方法是**特征匹配**。在本章中，我们将学习以下主题：
- en: What are features?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是特征？
- en: Feature detection, description, and matching in images
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像中的特征检测、描述和匹配
- en: SIFT detector and descriptor
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SIFT检测器和描述符
- en: SURF detector and descriptor
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SURF检测器和描述符
- en: ORB detector and descriptor
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ORB检测器和描述符
- en: BRISK detector and descriptor
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BRISK检测器和描述符
- en: FREAK descriptor
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FREAK描述符
- en: What are features?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是特征？
- en: 'Features are specific patterns that are unique and can be easily tracked and
    compared. Good features are those that can be distinctly localized. The following
    image shows the different kind of features:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 特征是独特且易于追踪和比较的特定模式。好的特征是可以明显定位的。以下图像显示了不同类型的特征：
- en: '![What are features?](img/B02052_03_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![什么是特征？](img/B02052_03_01.jpg)'
- en: Explains types of features
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 解释特征类型
- en: In the preceding image, patch A is a flat area and is difficult to locate precisely.
    If we move the rectangle anywhere within the box, the patch contents remain the
    same. Patch B, being along an edge, is a slightly better feature because if you
    move it perpendicular to the edge, it changes. However, if you move it parallel
    to the edge, it is identical to the initial patch. Thus, we can localize these
    kind of features in at least one dimension. Patch C, being a corner, is a good
    feature because on moving the rectangle in any direction, the contents of the
    patch change and can be easily localized. Thus, good features are those which
    can be easily localized and thus are easy to track.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图像中，补丁A是一个平坦区域，难以精确定位。如果我们把矩形移动到框内的任何地方，补丁内容保持不变。补丁B沿着边缘，是一个稍微好一些的特征，因为如果你垂直于边缘移动它，它会改变。然而，如果你平行于边缘移动它，它就与初始补丁相同。因此，我们可以在至少一个维度上定位这类特征。补丁C是一个角，是一个好的特征，因为无论你将矩形向哪个方向移动，补丁的内容都会改变，并且可以很容易地定位。因此，好的特征是那些可以很容易定位的，因此也容易追踪。
- en: In the previous chapters, we have seen some of the edge and corner detection
    algorithms. In this chapter, we will take a look at some more algorithms by which
    we can find features. This is called **feature detection**. Just detecting features
    is not enough. We need to be able to differentiate one feature from the other.
    Hence, we use **feature description** to describe the detected features. The descriptions
    enable us to find similar features in other images, thereby enabling us to identify
    objects. Features can also be used to align images and to stitch them together.
    We will take a look at these applications in the later chapters of this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到了一些边缘和角点检测算法。在本章中，我们将探讨一些更多可以通过它们找到特征的算法。这被称为**特征检测**。仅仅检测特征是不够的。我们需要能够区分不同的特征。因此，我们使用**特征描述**来描述检测到的特征。这些描述使我们能够在其他图像中找到相似的特征，从而使我们能够识别物体。特征也可以用来对齐图像并将它们拼接在一起。我们将在本书的后续章节中探讨这些应用。
- en: Now we will take a look at some common algorithms available to detect features,
    such as SIFT, SURF, BRIEF, FAST, and BRISK.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨一些常见的检测特征的算法，例如SIFT、SURF、BRIEF、FAST和BRISK。
- en: Note
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that SIFT and SURF are patented algorithms and hence, their free use is
    only limited to academic and research purposes. For any commercial use of these
    algorithms, you need to abide by the patent rules and regulations, or speak to
    the concerned personal.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，SIFT和SURF是专利算法，因此它们的免费使用仅限于学术和研究目的。对于这些算法的任何商业用途，您需要遵守专利规则和法规，或者与相关人员联系。
- en: Scale Invariant Feature Transform
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尺度不变特征变换
- en: '**Scale Invariant Feature Transform** (**SIFT**) is one of the most widely
    recognized feature detection algorithms. It was proposed by David Lowe in 2004.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '**尺度不变特征变换**（**SIFT**）是最广泛认可的特征检测算法之一。它由David Lowe于2004年提出。'
- en: Note
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Link to the paper: [http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 论文链接：[http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)
- en: 'Some of the properties of SIFT are as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT的一些特性如下：
- en: It is invariant to scaling and rotation changes in objects
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对物体缩放和旋转变化具有不变性
- en: It is also partially invariant to 3D viewpoint and illumination changes
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对3D视点和光照变化也有部分不变性
- en: A large number of keypoints (features) can be extracted from a single image
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以从一个单独的图像中提取大量关键点（特征）
- en: Understanding how SIFT works
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解SIFT的工作原理
- en: 'SIFT follows a strategy of matching robust local features. It is divided into
    four parts:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT遵循匹配鲁棒局部特征的战略。它分为四个部分：
- en: Scale-space extrema detection
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尺度空间极值检测
- en: Keypoint localization
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键点定位
- en: Orientation assignment
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方向分配
- en: Keypoint descriptor
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键点描述符
- en: Scale-space extrema detection
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尺度空间极值检测
- en: In this step, an image is progressively blurred out using Gaussian blur to get
    rid of some details in the images. It has been mathematically proven (under reasonable
    assumptions) that performing Gaussian blur is the only way to carry this out effectively.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个步骤中，通过高斯模糊逐步模糊图像以去除图像中的某些细节。在合理的假设下，已经从数学上证明（进行高斯模糊是唯一有效执行此操作的方法）。
- en: '![Scale-space extrema detection](img/B02052_03_02.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![尺度空间极值检测](img/B02052_03_02.jpg)'
- en: Images of one octave
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个八度的图像
- en: Progressively blurred images constitute an octave. A new octave is formed by
    resizing the original image of the previous octave to half and then progressively
    blurring it. Lowe recommends that you use four octaves of five images each for
    the best results.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步模糊的图像构成一个八度。通过将前一个八度的原始图像大小减半然后逐步模糊它来形成一个新的八度。Lowe建议您使用每个八度五幅图像的最佳结果。
- en: Thus, we see that the images in the first octave are formed by progressively
    blurring the original image. The first image of the second octave is obtained
    by resizing the original image in the first octave. Other images in the second
    octave are formed by the progressive blurring of the first image in the second
    octave, and so on.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们看到第一个八度的图像是通过逐步模糊原始图像形成的。第二个八度的第一幅图像是通过调整第一个八度中原始图像的大小获得的。第二个八度中的其他图像是通过逐步模糊第二个八度的第一幅图像形成的，依此类推。
- en: '![Scale-space extrema detection](img/B02052_03_03.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![尺度空间极值检测](img/B02052_03_03.jpg)'
- en: Images of all octaves
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所有八度的图像
- en: To precisely detect edges in an image, we use the Laplacian operator. In this
    method, second we blur the image a little and then calculate its second derivative.
    This locates the edges and corners that are good for finding the keypoints. This
    operation is called the Laplacian of Gaussian.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了精确检测图像中的边缘，我们使用拉普拉斯算子。在这个方法中，首先稍微模糊图像，然后计算其二阶导数。这定位了边缘和角落，这对于找到关键点是很有帮助的。这个操作被称为高斯拉普拉斯。
- en: 'The second order derivative is extremely sensitive to noise. The blur helps
    in smoothing out the noise and in stabilizing the second order derivative. The
    problem is that calculating all these second order derivatives is computationally
    expensive. So, we cheat a bit:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 二阶导数对噪声极其敏感。模糊有助于平滑噪声并稳定二阶导数。问题是计算所有这些二阶导数在计算上非常昂贵。因此，我们稍微作弊一下：
- en: '![Scale-space extrema detection](img/B02052_03_21.jpg)![Scale-space extrema
    detection](img/B02052_03_22.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![尺度空间极值检测](img/B02052_03_21.jpg)![尺度空间极值检测](img/B02052_03_22.jpg)'
- en: 'Here, *k* is a constant multiplicative factor, which represents the amount
    of blurring in each image in the scale space. A scale space represents the set
    of images that have been either scaled-up or scaled-down for the purpose of computing
    keypoints. For example, as shown in the following figure, there are two sets of
    images: one set is the original set of five images that have been blurred with
    different blurring radius and another set of scaled down images. The different
    parameter values can be seen in the following table:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*k*是一个常数乘法因子，它表示尺度空间中每个图像的模糊量。尺度空间表示为了计算关键点而放大或缩小的一组图像。例如，如图所示，有两组图像：一组是经过不同模糊半径模糊的五幅原始图像，另一组是缩小后的图像。不同的参数值可以在下表中看到：
- en: '|   | Scale |   |   |   |   |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|   | 尺度 |   |   |   |   |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **Octave** | 0.707107 | 1.000000 | 1.414214 | 2.000000 | 2.828427 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| **八度** | 0.707107 | 1.000000 | 1.414214 | 2.000000 | 2.828427 |'
- en: '|   | 1.414214 | 2.000000 | 2.828427 | 4.000000 | 5.656854 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '|   | 1.414214 | 2.000000 | 2.828427 | 4.000000 | 5.656854 |'
- en: '|   | 2.828427 | 4.000000 | 5.656854 | 8.000000 | 11.313708 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|   | 2.828427 | 4.000000 | 5.656854 | 8.000000 | 11.313708 |'
- en: '|   | 5.656854 | 8.000000 | 11.313708 | 16.000000 | 22.627417 |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|   | 5.656854 | 8.000000 | 11.313708 | 16.000000 | 22.627417 |'
- en: To generate the Laplacian of Gaussian images, we calculate the difference between
    two consecutive images in an octave. This is called the **Difference of Gaussian**
    (**DoG**). These DoG images are approximately equal to those obtained by calculating
    the Laplacian of Gaussian. Using DoG also has an added benefit. The images obtained
    are also scale invariant.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成高斯拉普拉斯图像，我们在八度音阶中计算连续两幅图像之间的差异。这被称为**高斯差分**（**DoG**）。这些DoG图像大约等于通过计算高斯拉普拉斯得到的图像。使用DoG还有额外的优点。得到的图像也是尺度不变的。
- en: '![Scale-space extrema detection](img/B02052_03_04.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![尺度空间极值检测](img/B02052_03_04.jpg)'
- en: Difference of Gaussian
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯差分
- en: Using the Laplacian of Gaussian is not only computationally expensive, but it
    also depends on the amount of blur applied. This is taken care of in the DoG images
    as a result of normalization.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用高斯拉普拉斯不仅计算量大，而且依赖于应用模糊的程度。由于归一化的结果，这在DoG图像中得到了处理。
- en: Keypoint localization
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键点定位
- en: 'Now these images have been sufficiently preprocessed to enable us to find local
    extremas. To locate keypoints, we need to iterate over each pixel and compare
    it with all its neighbors. Instead of just comparing the eight neighbors in that
    image, we compare the value with its neighbors in that image and also with the
    images above and below it in that octave, which have nine pixels each:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这些图像已经足够预处理，使我们能够找到局部极值。为了定位关键点，我们需要遍历每个像素，并将其与其所有邻居进行比较。我们不仅比较该图像中的八个邻居，还比较该像素在该图像中的值以及在该八度音阶上下方的图像中的值，这些图像各有九个像素：
- en: '![Keypoint localization](img/B02052_03_05.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![关键点定位](img/B02052_03_05.jpg)'
- en: Keypoint localization
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点定位
- en: So, we can see that we compare a pixel's value with its **26 neighbors**. A
    pixel is a keypoint if it is the minimum or the maximum among all its 26 neighbors.
    Usually, a non-maxima or a non-minima doesn't have to go through all 26 comparisons
    as we may have found its result within a few comparisons.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以看到我们比较了一个像素的值与其**26个邻居**。如果一个像素是其26个邻居中的最小值或最大值，则它是一个关键点。通常，非极大值或非极小值不需要进行所有26次比较，因为我们可能在几次比较中就找到了结果。
- en: We do not calculate the keypoints in the uppermost and lowermost images in an
    octave because we do not have enough neighbors to identify the extremas.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在八度音阶中最顶层和最底层图像中不计算关键点，因为我们没有足够的邻居来识别极值。
- en: 'Most of the time, the extremas are never located at the exact pixels. They
    may be present in between the pixels, but we have no way to access this information
    in an image. The keypoints located are just their average positions. We use the
    Taylor series expansion of the scale space function ![Keypoint localization](img/B02052_03_24.jpg)
    (up to the quadratic term) shifted till the current point as origin gives us:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数情况下，极值永远不会位于确切的像素上。它们可能存在于像素之间，但我们无法在图像中访问这些信息。定位到的关键点是它们的平均位置。我们使用尺度空间函数![关键点定位](img/B02052_03_24.jpg)的泰勒级数展开（到二次项）并平移到当前点作为原点，得到：
- en: '![Keypoint localization](img/B02052_03_25.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![关键点定位](img/B02052_03_25.jpg)'
- en: 'Here, *D* and its derivatives are calculated at the point we are currently
    testing for extrema. Using this formula, by differentiating and equating the result
    to zero, we can easily find the subpixel keypoint locations:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*D*及其导数是在我们当前测试极值的点上计算的。使用这个公式，通过微分并将结果等于零，我们可以轻松找到亚像素关键点位置：
- en: '![Keypoint localization](img/B02052_03_06.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![关键点定位](img/B02052_03_06.jpg)'
- en: Subpixel extrema localization
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 亚像素极值定位
- en: SIFT recommends that you generate two such extrema images. Thus, to generate
    two extremas, we need four DoG images. To generate these four DoG images, we need
    five Gaussian blurred images. Thus, we need five images in a single octave. It
    has also been found that the optimal results are obtained when ![Keypoint localization](img/B02052_03_26.jpg)
    and ![Keypoint localization](img/B02052_03_27.jpg).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT建议生成两个这样的极值图像。因此，为了生成两个极值，我们需要四个DoG图像。为了生成这四个DoG图像，我们需要五个高斯模糊图像。因此，我们需要一个八度音阶中的五个图像。还发现，当![关键点定位](img/B02052_03_26.jpg)和![关键点定位](img/B02052_03_27.jpg)时，可以获得最佳结果。
- en: The number of keypoints located so far is quite high. Some of these keypoints
    either lie on an edge or don't have enough contrast to be useful to us. So we
    need to get rid of these keypoints. This approach is similar to that used in the
    **Harris corner detector** to remove edges.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，定位的关键点数量相当高。其中一些关键点要么位于边缘上，要么对比度不足，对我们来说没有用处。因此，我们需要去除这些关键点。这种方法与**哈里斯角检测器**中用于去除边缘的方法类似。
- en: To remove low contrast keypoints, we simply compare the intensity value of the
    current pixel to a preselected threshold value. If it is less than the threshold
    value, it is rejected. Because we have used subpixel keypoints, we again need
    to use the Taylor series expansion to get the intensity value at subpixel locations.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 为了去除低对比度的关键点，我们只需将当前像素的强度值与预先选择的阈值值进行比较。如果它小于阈值值，则被拒绝。因为我们已经使用了亚像素关键点，所以我们再次需要使用泰勒级数展开来获取亚像素位置的强度值。
- en: For stability, it is not sufficient to reject keypoints with low contrast. The
    DoG function will have a strong response along edges, even if the location along
    the edge is poorly determined and therefore, unstable to small amounts of noise.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了稳定性，仅仅拒绝低对比度的关键点是不够的。DoG函数会在边缘处产生强烈的响应，即使边缘的位置确定得不好，因此对噪声的稳定性较差。
- en: 'To eliminate keypoints along the edges, we calculate two gradients at the keypoint,
    which are perpendicular to each other. The region around the keypoint can be one
    of the following three types:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了消除边缘上的关键点，我们在关键点处计算两个相互垂直的梯度。关键点周围区域可以是以下三种类型之一：
- en: A flat region (both gradients will be small)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个平坦区域（两个梯度都会很小）
- en: An edge (here, the gradient parallel to the edge will be small, but the one
    perpendicular to it will be large)
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个边缘（在这里，与边缘平行的梯度会很小，但垂直于它的梯度会很大）
- en: A corner (both gradients will be large)
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个角（两个梯度都会很大）
- en: As we want only corners as our keypoints, we only accept those keypoints whose
    both gradient values are high.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们只想将角作为我们的关键点，所以我们只接受那些两个梯度值都很高的关键点。
- en: 'To calculate this, we use the **Hessian matrix**. This is similar to the Harris
    corner detector. In the Harris corner detector, we calculate two different eigenvalues,
    whereas, in SIFT, we save the computation by just calculating their ratios directly.
    The Hessian matrix is shown as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算这个，我们使用**海森矩阵**。这与哈里斯角检测器类似。在哈里斯角检测器中，我们计算两个不同的特征值，而在SIFT中，我们通过直接计算它们的比率来节省计算。海森矩阵如下所示：
- en: '![Keypoint localization](img/B02052_03_28.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![关键点定位](img/B02052_03_28.jpg)'
- en: Orientation assignment
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方向分配
- en: Till now, we have stable keypoints and we know the scales at which these were
    detected. So, we have scale invariance. Now we try to assign an orientation to
    each keypoint. This orientation helps us achieve rotation invariance.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了稳定的关键点，并且我们知道这些关键点是在哪些尺度上被检测到的。因此，我们具有尺度不变性。现在我们尝试为每个关键点分配一个方向。这个方向帮助我们实现旋转不变性。
- en: 'We try to compute the magnitude and direction of the Gaussian blurred images
    for each keypoint. The magnitudes and directions are calculated using these formulae:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试计算每个关键点的高斯模糊图像的幅度和方向。这些幅度和方向是使用以下公式计算的：
- en: '![Orientation assignment](img/B02052_03_07.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![方向分配](img/B02052_03_07.jpg)'
- en: 'The magnitude and orientation are calculated for all pixels around the keypoint.
    We create a 36-bin histogram covering the 360-degree range of orientations. Each
    sample added to the histogram is weighted by its gradient magnitude and by a Gaussian-weighted
    circular window with σ, which is 1.5 times that of the scale of the keypoint.
    Suppose you get a histogram, as shown in the following figure:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算关键点周围所有像素的幅度和方向。我们创建一个覆盖360度方向范围的36个桶直方图。添加到直方图中的每个样本都由其梯度幅度和一个以σ为宽度的高斯加权圆形窗口加权，σ是关键点尺度的1.5倍。假设你得到一个如图所示的直方图：
- en: '![Orientation assignment](img/B02052_03_08.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![方向分配](img/B02052_03_08.jpg)'
- en: After this has been done for all the neighboring pixels of a particular keypoint,
    we will get a peak in the histogram. In the preceding figure, we can see that
    the histogram peaks in the region **20-29**. So, we assign this orientation to
    the keypoint. Also, any peaks above **80%** value are also converted into keypoints.
    These new keypoints have the same location and scale as the original keypoint,
    but its orientation is assigned to the value corresponding to the new peak.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在对特定关键点的所有相邻像素完成此操作后，我们将在直方图中得到一个峰值。在先前的图中，我们可以看到直方图在**20-29**的区域达到峰值。因此，我们将这个方向分配给关键点。此外，任何超过**80%**值的峰值也被转换为关键点。这些新关键点与原始关键点具有相同的位置和比例，但其方向被分配给对应于新峰值的值。
- en: Keypoint descriptor
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键点描述符
- en: Till now, we have achieved scale and rotation invariance. We now need to create
    a descriptor for various keypoints so as to be able to differentiate it from the
    other keypoints.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经实现了缩放和旋转不变性。现在我们需要为各种关键点创建一个描述符，以便能够将其与其他关键点区分开来。
- en: 'To generate a descriptor, we take a 16x16 window around the keypoint and break
    it into 16 windows of size 4x4\. This can be seen in the following image:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成一个描述符，我们在关键点周围取一个16x16的窗口，并将其分成16个4x4大小的窗口。这可以在以下图像中看到：
- en: '![Keypoint descriptor](img/B02052_03_09.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![关键点描述符](img/B02052_03_09.jpg)'
- en: We do this in order to incorporate the fact that objects in two images are rarely
    never exactly the same. Hence, we try to lose some precision in our calculations.
    Within each 4x4 window, gradient magnitudes and orientations are calculated. These
    orientations are put in an 8-bin histogram. Each bin represents an orientation
    angle of 45 degrees.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们这样做是为了考虑到两个图像中的对象很少是完全相同的。因此，我们在计算中尝试牺牲一些精度。在每个4x4窗口内，计算梯度幅度和方向。这些方向被放入一个8个分箱的直方图中。每个分箱代表45度的方向角。
- en: 'Now that we have a large area to consider, we need to take the distance of
    the vectors from the keypoint into consideration. To achieve this, we use the
    Gaussian weighting function:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一个较大的区域需要考虑，我们需要考虑从关键点到向量的距离。为了实现这一点，我们使用高斯加权函数：
- en: '![Keypoint descriptor](img/B02052_03_10.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![关键点描述符](img/B02052_03_10.jpg)'
- en: We put the 16 vectors into 8-bin histograms each, and doing this for each of
    the 4x4 windows we get 4x4x8 = 128 numbers. Once we have all these 128 numbers,
    we normalize the numbers (by dividing each by the sum of their squares). This
    set of 128 normalized numbers forms the feature vector.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将16个向量放入每个8个分箱的直方图中，并对每个4x4窗口进行此操作，我们得到4x4x8 = 128个数字。一旦我们有了所有这些128个数字，我们就对这些数字进行归一化（通过将每个数字除以它们的平方和）。这组128个归一化数字形成了特征向量。
- en: 'By the introduction of the feature vector, some unwanted dependencies arise,
    which are as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 通过引入特征向量，会产生一些不希望的依赖关系，如下所示：
- en: '**Rotation dependence**: The feature vector uses gradient orientations. So,
    if we rotate the image, our feature vector changes and the gradient orientations
    are also affected. To achieve rotation independence, we subtract the keypoint''s
    rotation from each orientation. Thus, each gradient orientation is now relative
    to the keypoint''s orientation.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**旋转依赖性**：特征向量使用梯度方向。因此，如果我们旋转图像，我们的特征向量会改变，梯度方向也会受到影响。为了实现旋转不变性，我们从每个方向中减去关键点的旋转。因此，每个梯度方向现在相对于关键点的方向。'
- en: '**Illumination dependence**: Illumination independence can be achieved by thresholding
    large values in the feature vector. So any value greater than 0.2 is changed to
    0.2 and the resultant feature vector is normalized again. We have now obtained
    an illumination independent feature vector.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**光照依赖性**：通过在特征向量中设置大值阈值，可以实现光照独立性。因此，任何大于0.2的值都被改变为0.2，并且结果特征向量再次进行归一化。我们现在已经获得了一个光照独立特征向量。'
- en: So, now that we have seen how SIFT works in theory, let's see how it works in
    OpenCV and its capability to match objects.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经从理论上了解了SIFT是如何工作的，让我们看看它在OpenCV中是如何工作的以及它匹配对象的能力。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Images and a simplified explanation of SIFT by Utkarsh Sinha can be found at
    [http://www.aishack.in/](http://www.aishack.in/).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[http://www.aishack.in/](http://www.aishack.in/)找到Utkarsh Sinha对SIFT的图像和简化解释。
- en: SIFT in OpenCV
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenCV中的SIFT
- en: We will set up a new application called `Chapter3`, which is similar to the
    one created in the earlier chapters. We will make changes to `MainActivity.java`.
    Some changes also have to be made to `HomeActivity.java`, but they will be self-explanatory.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将设置一个新的应用程序，名为`Chapter3`，它类似于前面章节中创建的应用程序。我们将对`MainActivity.java`进行修改。还需要对`HomeActivity.java`进行一些修改，但它们将是自解释的。
- en: 'First, we open `res` | `main_menu.xml`. In this file, we will create two items.
    One to select each image to be matched. As a convention, we will have the first
    image as the object to detect and the second image as the scene in which we want
    to detect it:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们打开`res` | `main_menu.xml`文件。在这个文件中，我们将创建两个条目。一个用于选择要匹配的每一张图像。按照惯例，我们将第一张图像作为要检测的对象，第二张图像作为我们想要在其中检测它的场景：
- en: '[PRE0]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now we need to program these items in to our Java code. This is similar to [Chapter
    1](ch01.html "Chapter 1. Applying Effects to Images"), *Applying Effects to Images*,
    where we opened the photo picker using intents. We will have two flag variables
    that will store each image that has been selected. If it is selected, we will
    perform our computations.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要将这些项目编程到我们的Java代码中。这类似于[第1章](ch01.html "第1章。对图像应用效果")，*对图像应用效果*，在那里我们使用意图打开了照片选择器。我们将有两个标志变量，将存储每个已选的图像。如果它被选中，我们将执行我们的计算。
- en: 'We will perform our actual computations in `AsyncTask`, as these tasks are
    computationally expensive; and to avoid blocking the UI thread for a long time,
    we offload the computation onto an asynchronous background worker—`AsyncTasks`
    that enables us to perform threading:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在`AsyncTask`中执行我们的实际计算，因为这些任务计算量很大；为了避免长时间阻塞UI线程，我们将计算卸载到异步后台工作线程——`AsyncTasks`，它使我们能够执行多线程：
- en: '[PRE1]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, the `executeTask` function has been called, which will perform all our
    computations. First, we need to detect the keypoints, and then we need to use
    descriptors to describe them.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，已经调用了`executeTask`函数，它将执行所有我们的计算。首先，我们需要检测关键点，然后我们需要使用描述符来描述它们。
- en: 'We first declare all our variables:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先声明所有变量：
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, depending on the algorithm, we initialize these variables. For SIFT,
    we use the following code snippet:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据算法，我们初始化这些变量。对于SIFT，我们使用以下代码片段：
- en: '[PRE3]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we detect the keypoints:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们检测关键点：
- en: '[PRE4]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now that we have the keypoints, we will compute their descriptors:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了关键点，我们将计算它们的描述符：
- en: '[PRE5]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Matching features and detecting objects
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 匹配特征和检测对象
- en: 'Once we have detected features in two or more objects, and have their descriptors,
    we can match the features to check whether the images have any similarities. For
    example, suppose we want to search for a particular book in a heap of many books.
    OpenCV provides us with two feature matching algorithms:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在两个或更多对象中检测到特征，并且有它们的描述符，我们就可以匹配特征以检查图像是否有任何相似性。例如，假设我们想在许多书籍堆中搜索一本书。OpenCV为我们提供了两种特征匹配算法：
- en: Brute-force matcher
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴力匹配器
- en: FLANN based matcher
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于FLANN的匹配器
- en: We will see how the two work in the following sections.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在以下章节中看到这两个是如何工作的。
- en: 'For matching, we first need to declare some variables:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于匹配，我们首先需要声明一些变量：
- en: '[PRE6]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Brute-force matcher
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 暴力匹配器
- en: It takes the descriptor of one feature in the first set and matches it with
    all other features in the second set, using distance calculations, and the closest
    one is returned.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 它将第一组中的一个特征的描述符与第二组中的所有其他特征进行匹配，使用距离计算，并返回最近的那个。
- en: The BF matcher takes two optional parameters. The first one is the distance
    measurement type, `normType`. We should use `NORM_L2` for descriptors such as
    SIFT and SURF. For descriptors that are based on a binary string, such as ORB
    and BRISK, we use `NORM_HAMMING` as the distance measurement. The second one is
    `crosscheck`. If it is set to true, the matcher only returns matches with values
    (i, j) such that the i^(th) descriptor in the first image has the j^(th) descriptor
    in the second set, as the best matches, and vice versa.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: BF匹配器有两个可选参数。第一个是距离测量类型，`normType`。对于SIFT和SURF等描述符，我们应该使用`NORM_L2`。对于基于二进制字符串的描述符，如ORB和BRISK，我们使用`NORM_HAMMING`作为距离测量。第二个是`crosscheck`。如果设置为true，匹配器只返回具有值（i，j）的匹配，其中第一张图像中的第i个描述符与第二组中的第j个描述符是最佳匹配，反之亦然。
- en: 'In our case for SIFT, we add the following code:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的SIFT案例中，我们添加以下代码：
- en: '[PRE7]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: FLANN based matcher
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于FLANN的匹配器
- en: '**FLANN** stands for **Fast Library for Approximate Nearest Neighbors**. It
    contains a collection of algorithms optimized for a fast nearest neighbor search
    in large datasets and for high-dimensional features. It works faster than the
    BF matcher for large datasets.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**FLANN**代表**快速近似最近邻库**。它包含了一组针对在大数据集和高维特征中进行快速最近邻搜索优化的算法。对于大数据集，它比BF匹配器运行得更快。'
- en: For FLANN based matcher, we need to pass two dictionaries, which specifies the
    algorithm to be used, its related parameters, and so on. The first one is `IndexParams`.
    For various algorithms, the information to be passed is explained in the FLANN
    docs.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于FLANN的匹配器，我们需要传递两个字典，这些字典指定了要使用的算法、相关参数等。第一个是`IndexParams`。对于不同的算法，要传递的信息在FLANN文档中有解释。
- en: The second dictionary is `SearchParams`. It specifies the number of times the
    trees in the index should be recursively traversed. Higher values give better
    precision, but also take more time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个字典是`SearchParams`。它指定了索引中树应该递归遍历的次数。更高的值会提供更好的精度，但也会花费更多的时间。
- en: 'To use the FLANN based matcher, we need to initialize it as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用基于FLANN的匹配器，我们需要按照以下方式初始化它：
- en: '[PRE8]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Matching the points
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 匹配点
- en: Once we have the `DescriptorMatcher` object, we use the `match()` and `knnMatch()`
    functions. The first one returns all the matches, while the second one returns
    *k* matches, where *k* is defined by the user.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了`DescriptorMatcher`对象，我们就使用`match()`和`knnMatch()`函数。第一个函数返回所有匹配项，而第二个函数返回用户定义的*k*个匹配项。
- en: 'After we have computed the descriptors, we can use the following to match the
    keypoints:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们计算了描述符之后，我们可以使用以下方法来匹配关键点：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Now we show the matches obtained using `drawMatches()`, which helps us draw
    the matches. It stacks two images horizontally and draws lines from the first
    image to the second image, showing the best matches. There is also a `drawMatchesKnn()`
    function, which draws all the *k* best matches. If *k = 2*, it will draw two match
    lines for each keypoint. So, we have to pass a mask if we want to selectively
    draw it.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们展示使用`drawMatches()`获得的匹配项，这有助于我们绘制匹配项。它将两个图像水平堆叠，并从第一幅图像到第二幅图像绘制线条，显示最佳匹配。还有一个`drawMatchesKnn()`函数，它绘制所有*k*个最佳匹配。如果*k
    = 2*，它将为每个关键点绘制两条匹配线。因此，如果我们想选择性地绘制它，我们必须传递一个掩码。
- en: 'To draw the matches, we will add a function that will merge the query and train
    image into one and also display the matches in the same image:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了绘制匹配项，我们将添加一个函数，该函数将查询图像和训练图像合并到一张图中，并在同一张图中显示匹配项：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Because SIFT and SURF are patented algorithms, they are not automatically built
    by OpenCV. We need to manually build the `nonfree` module so as to be able to
    use them in OpenCV. For this, you will need to download Android NDK, which allows
    us to use the native C++ code along with the Java code. It is available at [https://developer.android.com/tools/sdk/ndk/index.html](https://developer.android.com/tools/sdk/ndk/index.html).
    Then, extract it to a suitable location.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 由于SIFT和SURF是受专利保护的算法，它们不是由OpenCV自动构建的。我们需要手动构建`nonfree`模块，以便能够在OpenCV中使用它们。为此，您需要下载Android
    NDK，它允许我们与Java代码一起使用原生C++代码。它可在[https://developer.android.com/tools/sdk/ndk/index.html](https://developer.android.com/tools/sdk/ndk/index.html)找到。然后，将其提取到合适的位置。
- en: First, you need to download some files from OpenCV's source repository, which
    is located at [https://github.com/Itseez/opencv/tree/master/modules](https://github.com/Itseez/opencv/tree/master/modules).
    These are `nonfree_init.cpp`, `precomp.cpp`, `sift.cpp`, and `surf.cpp`. These
    will also be available with the code for this chapter, so you can download them
    directly from there as well. Now, create a folder in your `src` directory called
    `jni` and copy these files to there. We need to modify these files a bit.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要从OpenCV的源代码库下载一些文件，该库位于[https://github.com/Itseez/opencv/tree/master/modules](https://github.com/Itseez/opencv/tree/master/modules)。这些是`nonfree_init.cpp`、`precomp.cpp`、`sift.cpp`和`surf.cpp`。这些文件也将与本章的代码一起提供，因此您也可以直接从那里下载它们。现在，在您的`src`目录中创建一个名为`jni`的文件夹，并将这些文件复制到那里。我们需要对这些文件进行一些修改。
- en: Open `precomp.hpp` and remove the lines `#include "cvconfig.h"` and `#include
    "opencv2/ocl/private/util.hpp"`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`precomp.hpp`并删除`#include "cvconfig.h"`和`#include "opencv2/ocl/private/util.hpp"`这两行代码。
- en: Open `nonfree_init.cpp` and remove the lines of code starting from `#ifdef HAVE_OPENCV_OCL`
    and ending at `#endif`.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 打开`nonfree_init.cpp`并删除从`#ifdef HAVE_OPENCV_OCL`开始到`#endif`结束的代码行。
- en: 'Now we will create a file called `Android.mk` and copy the following lines
    of code to it. You need to replace `<OpenCV4Android_SDK_location>` accordingly:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将创建一个名为`Android.mk`的文件，并将以下代码行复制到其中。您需要相应地替换`<OpenCV4Android_SDK_location>`：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, create a file named `Application.mk` and copy the following lines of
    code to it. These define the architecture for which our library would be built:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个名为`Application.mk`的文件，并将以下代码行复制到其中。这些定义了我们的库将要构建的架构：
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Open the `build.gradle` file in your `app` folder. Under the `android` section,
    add the following:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的 `app` 文件夹中打开 `build.gradle` 文件。在 `android` 部分下，添加以下内容：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Open a terminal or a command window if you are on Windows. Then, change the
    directory to your project using the `cd` command. Type the following in the command
    window:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Windows上，请打开一个终端或命令窗口。然后，使用 `cd` 命令将目录更改为你的项目。在命令窗口中输入以下内容：
- en: '[PRE14]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the terminal window, type the following, replacing `<ndk_dir>` with the
    appropriate directory location:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在终端窗口中，输入以下内容，将 `<ndk_dir>` 替换为适当的目录位置：
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: After this, our library should have been successfully built and should be available
    in the `src` | `obj` folder, under the correct architecture.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们的库应该已经成功构建，并且应该可以在 `src` | `obj` 文件夹下，在正确的架构中找到。
- en: 'Now we need to load this library from our Java code. Open `MainActivity.java`,
    and in our OpenCV Manager''s callback variable (the `mOpenCVCallback` file''s
    `onManagerConnected` function) within the case for `LoaderCallbackInterface.SUCCESS`,
    add the following line of code:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要从我们的Java代码中加载这个库。打开 `MainActivity.java`，在我们的OpenCV Manager的回调变量（`mOpenCVCallback`
    文件的 `onManagerConnected` 函数）中，在 `LoaderCallbackInterface.SUCCESS` 的情况下添加以下代码行：
- en: '[PRE16]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The name of the library, `nonfree`, is the same as the module name defined in
    the `Android.mk` file.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 库的名称 `nonfree` 与 `Android.mk` 文件中定义的模块名称相同。
- en: '![Matching the points](img/B02052_03_11.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![匹配点](img/B02052_03_11.jpg)'
- en: SIFT feature matching
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT特征匹配
- en: Detecting objects
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检测对象
- en: In the previous sections, we detected features in multiple images and matched
    them to their corresponding features in the other images. The information we obtained
    is enough to locate objects in a scene.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们在多张图像中检测特征并将它们与另一张图像中相应的特征匹配。我们获得的信息足以在场景中定位对象。
- en: We use a function from OpenCV's `calib3d` module, `findHomography()`.Using this
    function, we can find a perspective transformation of the object, that is, a rotated
    and skewed result. Then we use `perspectiveTransform()` to locate the object in
    the scene. We need at least four matching points to calculate the transformation
    successfully.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用OpenCV的 `calib3d` 模块中的函数 `findHomography()`。使用此函数，我们可以找到对象的透视变换，即旋转和倾斜的结果。然后我们使用
    `perspectiveTransform()` 在场景中定位对象。我们需要至少四个匹配点才能成功计算变换。
- en: We have seen that there can be some possible errors while matching, which may
    affect the result. To solve this problem, the algorithm uses either `RANSAC` or
    `LEAST_MEDIAN` (which can be specified by the flags). Good matches that provide
    the correct estimation are called inliers and the remaining are called outliers.
    `findHomography()` returns a mask, which specifies the inlier and outlier points.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到在匹配过程中可能会出现一些可能的错误，这可能会影响结果。为了解决这个问题，算法使用 `RANSAC` 或 `LEAST_MEDIAN`（可以通过标志指定）。提供正确估计的匹配称为内点，其余的称为外点。`findHomography()`
    返回一个掩码，指定内点和外点。
- en: Now we will look at the algorithm to implement it.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将查看实现该算法的算法。
- en: First, we detect and match keypoints in both the images. This has already been
    done in the previous sections. Then we set a condition that there has to be a
    certain number of matches to detect an object.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在两幅图像中检测和匹配关键点。这已经在前面的章节中完成。然后，我们设定一个条件，必须有一定数量的匹配才能检测到对象。
- en: If enough matches are found, we extract the locations of matched keypoints in
    both the images. They are passed to find the perspective transformation. Once
    we get this 3x3 transformation matrix, we use it to transform the corners of `queryImage`
    to the corresponding points in `trainImage`. Then, we draw it.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果找到足够的匹配，我们将从两幅图像中提取匹配关键点的位置。它们被传递以找到透视变换。一旦我们得到这个3x3的变换矩阵，我们就用它将 `queryImage`
    的角点变换到 `trainImage` 中相应的点。然后，我们绘制它。
- en: Finally, we draw our inliers (if we successfully find the object) or matching
    keypoints (if it failed).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制我们的内点（如果我们成功找到对象）或匹配的关键点（如果失败）。
- en: Speeded Up Robust Features
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速鲁棒特征
- en: '**Speeded Up Robust Features** (**SURF**) was proposed by Herbert Bay, Tinne
    Tuytelaars, and Luc Van Gool in 2006\. Some of the drawbacks of SIFT are that
    it is slow and computationally expensive. To target this problem, SURF was thought
    of. Apart from the increase in speed, the other motivations behind SURF were as
    follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**加速鲁棒特征** (**SURF**) 由赫伯特·贝（Herbert Bay）、蒂内·图伊特劳斯（Tinne Tuytelaars）和卢克·范·古尔（Luc
    Van Gool）于2006年提出。SIFT的一些缺点是它速度慢且计算成本高。为了解决这个问题，人们想到了SURF。除了提高速度之外，SURF背后的其他动机如下：'
- en: Fast interest point detection
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 快速兴趣点检测
- en: Distinctive interest point description
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 独特兴趣点描述
- en: Speeded up descriptor matching
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加速描述符匹配
- en: 'Invariant to the following common image transformations:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对以下常见图像变换保持不变：
- en: Image rotation
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像旋转
- en: Scale changes
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尺度变化
- en: Illumination changes
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 照明变化
- en: Small changes in viewpoint
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 视点的小变化
- en: SURF detector
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SURF检测器
- en: Just as SIFT approximate Laplacian of Gaussian images to Difference of Gaussian,
    SURF uses integral images to approximate Laplacian of Gaussian images. An integral
    image (summed area tables) is an intermediate representation of the image and
    contains the sum of grayscale pixel values of the image. It is called the **fast
    Hessian** detector. The descriptor, on the other hand, describes a distribution
    of Haar wavelet responses within the interest point neighborhood.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 正如SIFT近似高斯图像的拉普拉斯变换为高斯差分，SURF使用积分图像来近似高斯图像的拉普拉斯变换。积分图像（求和面积表）是图像的中间表示，包含图像的灰度像素值之和。它被称为**快速Hessian**检测器。另一方面，描述符描述了兴趣点邻域内Haar小波响应的分布。
- en: Note
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: You can refer to the paper at [http://www.vision.ee.ethz.ch/~surf/eccv06.pdf](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下论文：[http://www.vision.ee.ethz.ch/~surf/eccv06.pdf](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf)。
- en: 'To select the location and scale of keypoints, SURF uses the determinant of
    the Hessian matrix. SURF proves that Gaussian is overrated as the property that
    no new structures can appear while going down to lower resolutions has only been
    proved in 1D, but does not apply to the 2D case. Given SIFT''s success with the
    LoG approximation, SURF further approximates LoG using box filters. Box filters
    approximate Gaussians and can be calculated very quickly. The following image
    shows an approximation of Gaussians as box filters:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择关键点的位置和尺度，SURF使用Hessian矩阵的行列式。SURF证明高斯被高估了，因为当降低分辨率时没有新的结构出现这一性质仅在1D中得到了证明，但不适用于2D情况。鉴于SIFT在LoG近似方面的成功，SURF进一步使用盒滤波器近似LoG。盒滤波器近似高斯，并且可以非常快速地计算。以下图像显示了高斯作为盒滤波器的近似：
- en: '![SURF detector](img/B02052_03_12.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![SURF检测器](img/B02052_03_12.jpg)'
- en: Due to the use of box filters and integral images, we no longer have to perform
    repeated Gaussian smoothing. We apply box filters of different sizes directly
    to the integral image. Instead of iteratively down-scaling images, we up-scale
    the filter size. Hence, scale analysis is done using only a single image. The
    output of the preceding 9x9 filter is considered as the initial scale layer. Other
    layers are obtained by filtering, using gradually bigger filters. Images of the
    first octave are obtained using filters of size 9x9, 15x15, 21x21, and 27x27\.
    At larger scales, the step size between the filters should also scale accordingly.
    Hence, for each new octave, the filter size step is doubled (that is, from 6 to
    12 to 24). In the next octave, the filter sizes are 39x39, 51x51, and so on.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了盒滤波器和积分图像，我们不再需要执行重复的高斯平滑。我们直接将不同大小的盒滤波器应用到积分图像上。我们不是迭代地缩小图像，而是放大滤波器的大小。因此，尺度分析仅使用单个图像完成。前一个9x9滤波器的输出被认为是初始尺度层。其他层通过使用逐渐更大的滤波器进行过滤获得。第一八度图像使用9x9、15x15、21x21和27x27大小的滤波器获得。在更大的尺度上，滤波器之间的步长也应相应地放大。因此，对于每个新的八度，滤波器大小的步长加倍（即从6到12到24）。在下八度中，滤波器的大小是39x39、51x51等等。
- en: In order to localize interest points in the image and over scales, a non-maximum
    suppression in a 3x3x3 neighborhood is applied. The maxima of the determinant
    of the Hessian matrix is then interpolated in scale and image space using the
    method proposed by Brown, and others. Scale space interpolation is especially
    important in our case, as the difference in scale between the first layers of
    every octave is relatively large.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在图像和不同尺度上定位兴趣点，对3x3x3邻域应用了非极大值抑制。然后使用布朗和其他人提出的方法，在尺度空间和图像空间中对Hessian矩阵行列式的极大值进行插值。在我们的情况下，尺度空间插值尤为重要，因为每个八度第一层的尺度差异相对较大。
- en: SURF descriptor
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SURF描述符
- en: Now that we have localized the keypoints, we need to create a descriptor for
    each, so as to uniquely identify it from other keypoints. SURF works on similar
    principles of SIFT, but with lesser complexity. Bay and others also proposed a
    variation of SURF that doesn't take rotation invariance into account, which is
    called **U-SURF** (upright SURF). In many applications, the camera orientation
    remains more or less constant. Hence, we can save a lot of computation by ignoring
    rotation invariance.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定位了关键点，我们需要为每个关键点创建一个描述符，以便能够从其他关键点中唯一地识别它。SURF在类似SIFT的原理上工作，但复杂性较低。Bay和其他人也提出了一种不考虑旋转不变性的SURF变体，称为**U-SURF**（垂直SURF）。在许多应用中，相机的方向保持相对稳定。因此，我们可以通过忽略旋转不变性来节省大量的计算。
- en: First, we need to fix a reproducible orientation based on the information obtained
    from a circular region centered about the keypoint. Then we construct a square
    region that is rotated and aligned based on the selected orientation, and then
    we can extract the SURF descriptor from it.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要基于从以关键点为中心的圆形区域获得的信息来固定一个可重复的定位。然后，我们构建一个基于所选定位旋转并对齐的正方形区域，然后我们可以从中提取SURF描述符。
- en: Orientation assignment
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定位分配
- en: 'In order to add rotation invariance, the orientation of the keypoints must
    be robust and reproducible. For this, SURF proposes calculating Haar wavelet responses
    in the *x* and *y* directions. The responses are calculated in a circular neighborhood
    of radius 6 s around the keypoint, where s is the scale of the image (that is,
    the value of σ). To calculate the Haar wavelet responses, SURF proposes using
    a wavelet size of 4 s. After obtaining the wavelet responses and weighing them
    with a Gaussian kernel ![Orientation assignment](img/B02052_03_29.jpg) centered
    about the keypoint, the responses are represented as vectors. The vectors are
    represented as the response strength in the horizontal direction along the abscissa,
    and the response strength in the vertical direction along the ordinate. All the
    responses within a sliding orientation window covering an angle of 60 degrees
    are then summed up. The longest vector calculated is set as the direction of the
    descriptor:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加旋转不变性，关键点的定位必须稳健且可重复。为此，SURF提出在 *x* 和 *y* 方向上计算Haar小波响应。响应是在关键点周围半径为6 s的圆形邻域内计算的，其中s是图像的尺度（即σ的值）。为了计算Haar小波响应，SURF提出使用4
    s大小的波let。在获得小波响应并用以关键点为中心的高斯核 ![定位分配](img/B02052_03_29.jpg) 加权后，响应表示为向量。向量表示为沿横轴的横向响应强度，以及沿纵轴的纵向响应强度。然后，将覆盖60度角度的滑动定位窗口内的所有响应相加。计算出的最长向量被设置为描述符的方向：
- en: '![Orientation assignment](img/B02052_03_13.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![定位分配](img/B02052_03_13.jpg)'
- en: Haar wavelet responses in a 60 degree angle
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 60度角度内的Haar小波响应
- en: The size of the sliding window is taken as a parameter, which has to be calculated
    experimentally. Small window sizes result in single dominating wavelet responses,
    whereas large window sizes result in maxima in vector lengths that are not descriptive
    enough. Both result in an unstable orientation of the interest region. This step
    is skipped for U-SURF, as it is doesn't require rotation invariance.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 滑动窗口的大小被作为一个参数，需要通过实验计算。小的窗口大小会导致单一的占主导地位的波let响应，而大的窗口大小会导致向量长度上的极大值不足以描述。两者都会导致兴趣区域的定位不稳定。对于U-SURF，这一步被跳过，因为它不需要旋转不变性。
- en: Descriptor based on Haar wavelet responses
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于Haar小波响应的描述符
- en: 'For the extraction of the descriptor, the first step consists of constructing
    a square region centered around the interest point and oriented along the orientation
    selected in the previous section. This is not required for U-SURF. The size of
    the window is 20 s. The steps to find the descriptor are as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于描述符的提取，第一步是构建一个以兴趣点为中心、沿前一小节中选定的方向对齐的正方形区域。对于U-SURF来说，这一步不是必需的。窗口的大小为20 s。寻找描述符的步骤如下：
- en: Split the interest region into 4x4 square subregions with 5x5 regularly spaced
    sample points inside.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将兴趣区域分割成4x4的正方形子区域，每个子区域内部有5x5均匀分布的采样点。
- en: Calculate Haar wavelet responses *d^x* and *d^y* [*d^x* = Haar wavelet response
    in *x* direction; d^y = Haar wavelet response in *y* direction. The filter size
    used is 2 s].
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算Haar小波响应 *d^x* 和 *d^y* [*d^x* = x方向上的Haar小波响应；*d^y* = y方向上的Haar小波响应。使用的滤波器大小为2
    s]。
- en: Weight the response with a Gaussian kernel centered at the interest point.
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以兴趣点为中心的高斯核对响应进行加权。
- en: Sum the response over each subregion for *d^x* and *d^y* separately, to form
    a feature vector of length 32.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分别对每个子区域的响应进行求和，形成长度为32的特征向量，用于*d^x*和*d^y*。
- en: In order to bring in information about the polarity of the intensity changes,
    extract the sum of the absolute value of the responses, which is a feature vector
    of length 64.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了引入关于强度变化极性的信息，提取响应绝对值的和，这是一个长度为64的特征向量。
- en: Normalize the vector to unit length.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将向量归一化到单位长度。
- en: The wavelet responses are invariant to a bias in illumination (offset). Invariance
    to contrast (a scale factor) is achieved by turning the descriptor into a unit
    vector (normalization).
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 小波响应对光照偏差（偏移）是不变的。通过将描述符转换为单位向量（归一化）来实现对对比度（尺度因子）的不变性。
- en: Experimentally, Bay and others tested a variation of SURF that adds some more
    features (SURF-128). The sums of *d^x* and *|d^x|* are computed separately for
    *d^y < 0* and *d^y ≥ 0*. Similarly, the sums of *d^y* and *|d^y|* are split according
    to the sign of *d^x*, thereby doubling the number of features. This version of
    SURF-128 outperforms SURF.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 实验中，Bay等人测试了一种SURF的变体，它添加了一些额外的特征（SURF-128）。对于*d^y < 0*和*d^y ≥ 0*，分别单独计算*d^x*和*|d^x|*的和。同样，根据*d^x*的符号，将*d^y*和*|d^y|*的和分开，从而将特征数量翻倍。这个版本的SURF-128优于SURF。
- en: 'The following table shows a comparison between the various algorithms in finding
    features:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了在各种算法中寻找特征时的比较：
- en: '|   | U-SURF | SURF | SIFT |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '|   | U-SURF | SURF | SIFT |'
- en: '| --- | --- | --- | --- |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Time (ms)** | 225 | 354 | 1036 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **时间 (ms)** | 225 | 354 | 1036 |'
- en: While SIFT and SURF work well in finding good features, they are **patented**
    for commercial use. So, you have to pay some money if you use them for commercial
    purposes.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 SIFT 和 SURF 在寻找良好特征方面表现良好，但它们用于商业用途是**专利的**。因此，如果您用于商业目的，您必须支付一些费用。
- en: 'Some of the results we obtain from SURF are as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从SURF获得的一些结果如下：
- en: SURF is faster than SIFT by three times and has a recall precision no worse
    than SIFT
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SURF的速度比SIFT快三倍，并且召回精度不低于SIFT
- en: SURF is good at handling images with blurring or rotation
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SURF擅长处理模糊或旋转的图像
- en: SURF is poor at handling images with viewpoint changes
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SURF在处理视角变化的图像方面表现不佳
- en: SURF in OpenCV
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenCV中的SURF
- en: 'The code for SURF needs only a little modification. We just need to add a case
    in our switch case construct:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: SURF的代码只需要稍作修改。我们只需要在我们的switch case结构中添加一个case：
- en: '[PRE17]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![SURF in OpenCV](img/B02052_03_14.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![OpenCV中的SURF](img/B02052_03_14.jpg)'
- en: SURF feature matching
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: SURF特征匹配
- en: Oriented FAST and Rotated BRIEF
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定向FAST和旋转BRIEF
- en: '**Oriented FAST and Rotated BRIEF** (**ORB**) was developed at OpenCV labs
    by Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary R. Bradski in 2011, as
    an efficient and viable alternative to SIFT and SURF. ORB was conceived mainly
    because SIFT and SURF are patented algorithms. ORB, however, is free to use.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '**定向FAST和旋转BRIEF**（**ORB**）由Ethan Rublee、Vincent Rabaud、Kurt Konolige和Gary
    R. Bradski于2011年在OpenCV实验室开发，作为一种高效且可行的SIFT和SURF的替代方案。ORB主要因为SIFT和SURF是专利算法而构思。然而，ORB是免费使用的。'
- en: 'ORB performs as well as SIFT on these tasks (and better than SURF), while being
    almost two order of magnitude faster. ORB builds on the well-known FAST keypoint
    detector and the BRIEF descriptor. Both these techniques are attractive because
    of their good performance and low cost. ORB''s main contributions are as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ORB在这些任务上的表现与SIFT相当（并且优于SURF），同时几乎快两个数量级。ORB建立在著名的FAST关键点检测器和BRIEF描述符之上。这两种技术都因其良好的性能和低成本而具有吸引力。ORB的主要贡献如下：
- en: The addition of a fast and accurate orientation component to FAST
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将快速且精确的定向组件添加到FAST中
- en: The efficient computation of oriented BRIEF features
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定向BRIEF特征的快速计算
- en: Analysis of variance and correlation of oriented BRIEF features
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对定向BRIEF特征的方差和相关性分析
- en: A learning method for decorrelating BRIEF features under rotational invariance,
    leading to better performance in nearest-neighbor applications
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种在旋转不变性下去相关BRIEF特征的学习方法，从而在最近邻应用中实现更好的性能
- en: oFAST – FAST keypoint orientation
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: oFAST – FAST关键点方向
- en: FAST is a feature detection algorithm that is widely recognized due its fast
    computation properties. It doesn't propose a descriptor to uniquely identify features.
    Moreover, it does not have any orientation component, so it performs poorly to
    in-plane rotation and scale changes. We will take a look at how ORB added an orientation
    component to FAST features.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: FAST是一种广为人知的特征检测算法，因其快速计算特性而受到认可。它不提出一个描述符来唯一标识特征。此外，它没有方向组件，因此在平面旋转和尺度变化方面表现不佳。我们将探讨ORB是如何为FAST特征添加方向组件的。
- en: FAST detector
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FAST检测器
- en: First, we detect FAST keypoints. FAST takes one parameter from the user, the
    threshold value between the center pixel, and those in a circular ring around
    it. We use a ring radius of 9 pixels as it gives good performance. FAST also produces
    keypoints that are along edges. To overcome this, we use the Harris corner measure
    to order the keypoints. If we want N keypoints, we first keep the threshold low
    enough to generate more than N keypoints, and then pick the topmost N based on
    the Harris corner measure.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检测FAST关键点。FAST从用户那里获取一个参数，即中心像素和围绕它的圆形环中的像素之间的阈值值。我们使用9像素的环半径，因为它提供了良好的性能。FAST还会产生沿边缘的关键点。为了克服这一点，我们使用Harris角测量法对关键点进行排序。如果我们想得到N个关键点，我们首先将阈值设置得足够低，以生成超过N个关键点，然后根据Harris角测量法选择最上面的N个。
- en: FAST does not produce multiscale features. ORB employs a scale pyramid of the
    image and produces FAST features (filtered by Harris) at each level in the pyramid.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: FAST不产生多尺度特征。ORB使用图像的尺度金字塔，并在金字塔的每一级产生FAST特征（通过Harris滤波器过滤）。
- en: Orientation by intensity centroid
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过强度质心进行定位
- en: To assign orientation to corners, we use the intensity centroid. We assume that
    the corner is offset from the intensity centroid and this vector is used to assign
    orientation to a keypoint.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给角赋予方向，我们使用强度质心。我们假设角偏离强度质心，并使用此向量将方向赋予一个关键点。
- en: 'To compute the coordinates of the centroid, we use moments. Moments are calculated
    as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算质心的坐标，我们使用矩。矩的计算如下：
- en: '![Orientation by intensity centroid](img/B02052_03_30.jpg)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![通过强度质心进行定位](img/B02052_03_30.jpg)'
- en: 'The coordinates of the centroid can be calculated as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 质心的坐标可以计算如下：
- en: '![Orientation by intensity centroid](img/B02052_03_31.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![通过强度质心进行定位](img/B02052_03_31.jpg)'
- en: 'We construct a vector ![Orientation by intensity centroid](img/B02052_03_32.jpg)
    from the keypoint''s center, *O*, to the centroid, *C*. The orientation of the
    patch is obtained by:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从关键点的中心*O*到质心*C*构建一个向量![通过强度质心进行定位](img/B02052_03_32.jpg)。通过以下方式获得补丁的方向：
- en: '![Orientation by intensity centroid](img/B02052_03_33.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![通过强度质心进行定位](img/B02052_03_33.jpg)'
- en: Here, *atan2* is the quadrant-aware version of *arctan*. To improve the rotation
    invariance of this measure, we make sure that the moments are computed with *x*
    and *y* remaining within a circular region of radius *r*. We empirically choose
    *r* to be the patch size so that *x* and *y* run from *[−r, r]*. As *|C|* approaches
    *0*, the measure becomes unstable; with FAST corners, we have found that this
    is rarely the case. This method can also work well in images with heavy noise.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*atan2*是具有象限意识的*arctan*版本。为了提高该测量的旋转不变性，我们确保在半径*r*的圆形区域内计算*x*和*y*的矩。我们经验性地选择*r*为补丁大小，使得*x*和*y*从*[−r,
    r]*运行。当*|C|*接近*0*时，该测量变得不稳定；通过FAST角，我们发现这种情况很少发生。这种方法也可以在噪声严重的图像中很好地工作。
- en: rBRIEF – Rotation-aware BRIEF
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: rBRIEF – 旋转感知BRIEF
- en: 'BRIEF is a feature description algorithm that is also known for its fast speed
    of computation. However, BRIEF also isn''t invariant to rotation. ORB tries to
    add this functionality, without losing out on the speed aspect of BRIEF. The feature
    vector obtained by *n* binary tests in BRIEF is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: BRIEF是一种特征描述算法，也因其计算速度快而闻名。然而，BRIEF对旋转并不具有不变性。ORB试图添加这一功能，同时不牺牲BRIEF的速度特性。BRIEF通过*n*个二进制测试获得的特征向量如下：
- en: '![rBRIEF – Rotation-aware BRIEF](img/B02052_03_34.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![rBRIEF – 旋转感知BRIEF](img/B02052_03_34.jpg)'
- en: 'Where ![rBRIEF – Rotation-aware BRIEF](img/B02052_03_35.jpg) is defined as:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![rBRIEF – 旋转感知BRIEF](img/B02052_03_35.jpg)定义为：
- en: '![rBRIEF – Rotation-aware BRIEF](img/B02052_03_36.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![rBRIEF – 旋转感知BRIEF](img/B02052_03_36.jpg)'
- en: '*p(x)* is the intensity value at pixel *x*.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x)*是像素*x*的强度值。'
- en: Steered BRIEF
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 指向BRIEF
- en: 'The matching performance of BRIEF falls off sharply for in-plane rotation of
    more than a few degrees. ORB proposes a method to steer BRIEF according to the
    orientation of the keypoints. For any feature set of n binary tests at location
    (x^i, y^i), we define the *2 x n* matrix:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: BRIEF的匹配性能在平面旋转超过几度时急剧下降。ORB提出了一种根据关键点方向调整BRIEF的方法。对于任何位于(x^i, y^i)位置的n个二进制测试的特征集，我们定义一个*2
    x n*矩阵：
- en: '![Steered BRIEF](img/B02052_03_37.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![Steered BRIEF](img/B02052_03_37.jpg)'
- en: 'We use the patch orientation *θ* and the corresponding rotation matrix *R^θ*,
    and construct a *steered* version *S^θ* of *S*:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用补丁方向*θ*和相应的旋转矩阵*R^θ*，构建*Steered*版本*S^θ*的*S*：
- en: '![Steered BRIEF](img/B02052_03_38.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![Steered BRIEF](img/B02052_03_38.jpg)'
- en: 'Now the steered BRIEF operator becomes:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Steered BRIEF算子变为：
- en: '![Steered BRIEF](img/B02052_03_39.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![Steered BRIEF](img/B02052_03_39.jpg)'
- en: We discretize the angle to increments of *2π/30* (12 degrees), and construct
    a lookup table of precomputed BRIEF patterns. As long as the keypoint orientation
    *θ* is consistent across views, the correct set of points *S^θ* will be used to
    compute its descriptor.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将角度离散化为*2π/30*（12度）的增量，并构建一个预计算的BRIEF模式的查找表。只要关键点方向*θ*在视图中保持一致，就会使用正确的点集*S^θ*来计算其描述符。
- en: Variance and correlation
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 方差和相关性
- en: One of the properties of BRIEF is that each bit feature has a large variance
    and a mean near 0.5\. A mean of 0.5 gives a maximum sample variance of 0.25 for
    a bit feature. Steered BRIEF produces a more uniform appearance to binary tests.
    High variance causes a feature to respond more differently to inputs.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: BRIEF的一个特性是每个位特征都有很大的方差和接近0.5的平均值。0.5的平均值给位特征提供了最大样本方差0.25。Steered BRIEF为二进制测试产生更均匀的外观。高方差导致特征对输入的反应差异更大。
- en: Having uncorrelated features is desirable as in that case, each test has a contribution
    to the results. We search among all the possible binary tests to find ones that
    have a high variance (and a mean close to 0.5) as well as being uncorrelated.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有不相关特征是理想的，因为在这种情况下，每个测试都对结果有贡献。我们在所有可能的二进制测试中搜索，以找到那些具有高方差（以及平均值接近0.5）且不相关的测试。
- en: 'ORB specifies the rBRIEF algorithm as follows:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ORB指定rBRIEF算法如下：
- en: 'Set up a training set of some 300 k keypoints drawn from images in the PASCAL
    2006 set. Then, enumerate all the possible binary tests drawn from a 31x31 pixel
    patch. Each test is a pair of 5x5 subwindows of the patch. If we note the width
    of our patch as *w^p = 31* and the width of the test subwindow as *w^t = 5*, then
    we have *N = (wp − w^t)²* possible subwindows. We would like to select pairs of
    two from these, so we have ![Variance and correlation](img/B02052_03_40.jpg) 2
    binary tests. We eliminate tests that overlap, so we end up with *N = 205590*
    possible tests. The algorithm is as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 设置一个由PASCAL 2006集中的图像抽取的约300 k个关键点的训练集。然后，枚举从31x31像素补丁中抽取的所有可能的二进制测试。每个测试是补丁的5x5子窗口对。如果我们记补丁的宽度为*w^p
    = 31*，测试子窗口的宽度为*w^t = 5*，那么我们有*N = (wp − w^t)²*个可能的子窗口。我们希望从中选择两个，所以我们有![方差和相关性](img/B02052_03_40.jpg)2个二进制测试。我们消除重叠的测试，最终得到*N
    = 205590*个可能的测试。算法如下：
- en: Run each test against all training patches.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个测试与所有训练补丁进行匹配。
- en: Order the tests by their distance from a mean of 0.5, forming the vector T.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按测试与0.5平均值的距离对测试进行排序，形成向量T。
- en: 'Perform a greedy search:'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行贪婪搜索：
- en: Put the first test into the result vector R and remove it from T.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将第一个测试放入结果向量R中，并从T中移除它。
- en: Take the next test from T, and compare it against all tests in R. If its absolute
    correlation is greater than a threshold, discard it; else add it to R.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从T中取出下一个测试，并将其与R中的所有测试进行比较。如果其绝对相关性大于阈值，则丢弃它；否则将其添加到R中。
- en: Repeat the previous step until there are 256 tests in R. If there are fewer
    than 256, raise the threshold and try again.
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复前面的步骤，直到R中有256个测试。如果少于256个，则提高阈值并再次尝试。
- en: rBRIEF shows significant improvement in the variance and correlation over steered
    BRIEF. ORB outperforms SIFT and SURF on the outdoor dataset. It is about the same
    on the indoor set; note that blob detection keypoints, such as SIFT, tend to be
    better on graffiti type images.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: rBRIEF在方差和相关性方面相对于Steered BRIEF有显著改进。ORB在户外数据集上优于SIFT和SURF。在室内数据集上表现大致相同；请注意，像SIFT这样的块检测关键点在涂鸦类型图像上往往表现更好。
- en: ORB in OpenCV
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenCV中的ORB
- en: The code for ORB is similar to SIFT and SURF. However, ORB being a binary string-based
    descriptor, we will use the hamming code in our BF matcher.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ORB的代码与SIFT和SURF类似。然而，ORB作为一个基于二进制字符串的描述符，我们将在我们的BF匹配器中使用汉明码。
- en: 'The code for SURF needs only a little modification. We just need to add a case
    to our switch case construct:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对SURF的代码只需稍作修改。我们只需在我们的switch case结构中添加一个情况：
- en: '[PRE18]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![ORB in OpenCV](img/B02052_03_15.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![ORB在OpenCV中的使用](img/B02052_03_15.jpg)'
- en: ORB feature matching
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ORB特征匹配
- en: Binary Robust Invariant Scalable Keypoints
  id: totrans-269
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二值鲁棒可伸缩关键点
- en: '**Binary Robust Invariant Scalable Keypoints** (**BRISK**) was conceived by
    Leutenegger, Chli, and Siegwart to be an efficient replacement to the state-of-the-art
    feature detection, description, and matching algorithms. The motivation behind
    BRISK was to develop a robust algorithm that can reproduce features in a computationally
    efficient manner. In some cases, BRISK achieves comparable quality of feature
    matching as SURF, while requiring much less computation time.'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**二值鲁棒可伸缩关键点**（**BRISK**）是由Leutenegger、Chli和Siegwart提出的，旨在成为最先进特征检测、描述和匹配算法的高效替代方案。BRISK背后的动机是开发一种鲁棒算法，能够以计算效率的方式重现特征。在某些情况下，BRISK在特征匹配的质量上与SURF相当，但所需的计算时间却少得多。'
- en: Scale-space keypoint detection
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 尺度空间关键点检测
- en: The BRISK detector is based on the AGAST detector, which is an extension of
    a faster performance version of FAST. To achieve scale invariance, BRISK searches
    for the maxima in a scale space using the FAST score(s) as the comparison parameter.
    Despite discretizing the scale axis at coarser intervals than in alternative high-performance
    detectors (for example, the fast Hessian), the BRISK detector estimates the true
    scale of each keypoint in the continuous scale space. The BRISK scale space comprises
    of n octaves, c^i and *n* intra-octaves, and *d^i [i = {0, 1, 2, …, n-1}]*. BRISK
    suggests using *n = 4*.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK检测器基于AGAST检测器，它是FAST更快性能版本的一个扩展。为了实现尺度不变性，BRISK使用FAST分数（s）作为比较参数，在尺度空间中寻找最大值。尽管在比其他高性能检测器（例如快速Hessian）更粗的间隔处对尺度轴进行离散化，但BRISK检测器估计每个关键点在连续尺度空间中的真实尺度。BRISK的尺度空间包括n个八度，c^i和n个跨八度，以及*d^i
    [i = {0, 1, 2, …, n-1}]*。BRISK建议使用*n = 4*。
- en: The original image is taken as *c⁰*, and each successive octave is half-sampled
    from the previous octave. Each intra-octave d^i is down-sampled such that it lies
    between *c^i* and *c^i+1*. The first intra-octave *d⁰* is obtained by down sampling
    *c⁰* by a factor of 1.5\. The subsequent intra-octaves are obtained by half sampling
    the previous intra-octave.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 将原始图像作为*c⁰*，每个后续八度是从前一个八度的一半采样得到的。每个跨八度d^i是下采样得到的，使其位于*c^i*和*c^i+1*之间。第一个跨八度*d⁰*是通过将*c⁰*以1.5的倍数下采样得到的。后续的跨八度是通过前一个跨八度的一半采样得到的。
- en: '![Scale-space keypoint detection](img/B02052_03_16.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![尺度空间关键点检测](img/B02052_03_16.jpg)'
- en: An image showing octaves and intra-octaves
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 展示八度和跨八度的图像
- en: The FAST 9-16 detector requires that in a 16 pixel circular radius, at least
    9 pixels must be brighter than or darker than the center pixel for the FAST criterion
    to be fulfilled. BRISK proposes the use of this FAST 9-16 detector.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: FAST 9-16检测器要求在16像素的圆形半径内，至少有9个像素比中心像素亮或暗，以满足FAST标准。BRISK提出了使用这种FAST 9-16检测器。
- en: The FAST score is computed for each octave and intra-octave separately. The
    FAST detector score, *s*, is calculated for each pixel as the maximum threshold
    for FAST detection, such that an image point is considered as a corner.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个八度和跨八度分别计算FAST分数。FAST检测器的分数s是计算每个像素的最大阈值，使得图像点被认为是角点。
- en: A non-maximum suppression in scale space is carried out on the keypoints obtained
    after applying the FAST 9-16 detector. The keypoint should be the maximum among
    its eight neighboring FAST scores in the same octave or intra-octave. This point
    must also have a higher FAST score than points in the layers above and below it.
    We then check inside the equally sized square patches having a 2 pixel side length
    in the layer, where the maximum value is suspected to be present. Interpolation
    is carried out at the boundaries of the patch, as neighboring layers are represented
    with different discretizations than that of the current later.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用FAST 9-16检测器后，对尺度空间中的关键点进行非极大值抑制。关键点应该是其八个相邻FAST分数在同一八度或跨八度中的最大值。此点还必须比其上下层的点具有更高的FAST分数。然后我们在具有2像素边长的等大小正方形块中检查该层，其中怀疑存在最大值。在块的边界处进行插值，因为相邻层使用与当前层不同的离散化表示。
- en: We try to calculate a subpixel location for each maximum detected in the earlier
    step. A 2D quadratic function is fitted to the 3x3 patch surrounding the pixel,
    and the subpixel maximum is determined. This is also done for the layers above
    and below the current layer. These maximas are then interpolated using a 1D parabola
    across the scale space, and the local maximum is chosen as the scale for the feature
    is found.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尝试计算在早期步骤中检测到的每个最大值的亚像素位置。将一个二维二次函数拟合到围绕像素的 3x3 像素块，并确定亚像素最大值。这同样适用于当前层上下方的层。然后使用一维抛物线在尺度空间中进行插值，并选择局部最大值作为特征的尺度。
- en: Keypoint description
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键点描述
- en: The BRISK descriptor is composed of a binary string by concatenating the results
    of simple brightness comparison tests. In BRISK, we need to identify the characteristic
    direction of each keypoint to achieve the rotation invariance.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK描述符通过连接简单亮度比较测试的结果来构成一个二进制字符串。在BRISK中，我们需要识别每个关键点的特征方向以实现旋转不变性。
- en: Sampling pattern and rotation estimation
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 采样模式及旋转估计
- en: 'The BRISK descriptor makes use of a pattern used for sampling the neighborhood
    of the keypoint. The pattern defines N locations equally spaced on circles concentric
    with the keypoint, as shown in the following figure:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK描述符利用一种用于采样关键点邻域的模式。该模式定义了与关键点同心圆上等间距的 N 个位置，如图所示：
- en: '![Sampling pattern and rotation estimation](img/B02052_03_17.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![采样模式及旋转估计](img/B02052_03_17.jpg)'
- en: BRISK sampling pattern
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK采样模式
- en: In order to avoid aliasing effects when sampling the image intensity of a point
    p^i in the pattern, we apply Gaussian smoothing with the standard deviation ![Sampling
    pattern and rotation estimation](img/B02052_03_41.jpg) proportional to the distance
    between the points on the respective circles. We then calculate the gradient between
    two sampling points.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免在采样模式中点 p^i 的图像强度时出现混叠效应，我们应用高斯平滑，其标准差 ![采样模式及旋转估计](img/B02052_03_41.jpg)
    与相应圆上点之间的距离成正比。然后我们计算两个采样点之间的梯度。
- en: 'The formula used is:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的公式是：
- en: '![Sampling pattern and rotation estimation](img/B02052_03_42.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![采样模式及旋转估计](img/B02052_03_42.jpg)'
- en: 'BRISK defines a subset of short distance pairings, *S*, and another subset
    of long distance pairings, *L*, as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK定义了一个短距离配对子集 *S* 和另一个长距离配对子集 *L*，如下所示：
- en: '![Sampling pattern and rotation estimation](img/B02052_03_43.jpg)![Sampling
    pattern and rotation estimation](img/B02052_03_44.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![采样模式及旋转估计](img/B02052_03_43.jpg)![采样模式及旋转估计](img/B02052_03_44.jpg)'
- en: 'Where *A* is the set of all sampling point pairs as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A* 是所有采样点对的集合，如下所示：
- en: '![Sampling pattern and rotation estimation](img/B02052_03_45.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![采样模式及旋转估计](img/B02052_03_45.jpg)'
- en: 'The threshold distances are set to ![Sampling pattern and rotation estimation](img/B02052_03_46.jpg)
    and ![Sampling pattern and rotation estimation](img/B02052_03_47.jpg) (*t* is
    the scale of the keypoint). BRISK estimates the overall characteristic pattern
    direction of the keypoint *k* to be:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值距离设置为 ![采样模式及旋转估计](img/B02052_03_46.jpg) 和 ![采样模式及旋转估计](img/B02052_03_47.jpg)
    (*t* 是关键点的尺度)。BRISK估计关键点 *k* 的整体特征模式方向为：
- en: '![Sampling pattern and rotation estimation](img/B02052_03_48.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![采样模式及旋转估计](img/B02052_03_48.jpg)'
- en: Building the descriptor
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建描述符
- en: 'In order to develop a rotation and scale invariant descriptor, BRISK applies
    the sampling pattern rotated by an angle, ![Building the descriptor](img/B02052_03_49.jpg),
    around the keypoint *k*. Short distance intensity comparisons of point pairs,
    ![Building the descriptor](img/B02052_03_50.jpg) (that is, in the rotated pattern),
    are calculated to get the bit vector descriptor *d^k*. Each bit *b* corresponds
    to:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发一个旋转和尺度不变描述符，BRISK在关键点 *k* 附近应用了一个旋转角度 ![构建描述符](img/B02052_03_49.jpg) 的采样模式。计算点对之间的短距离强度比较，![构建描述符](img/B02052_03_50.jpg)（即在旋转模式中），以获得位向量描述符
    *d^k*。每个位 *b* 对应于：
- en: '![Building the descriptor](img/B02052_03_51.jpg)![Building the descriptor](img/B02052_03_52.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![构建描述符](img/B02052_03_51.jpg)![构建描述符](img/B02052_03_52.jpg)'
- en: BRISK uses a deterministic sampling pattern, resulting in a uniform sampling
    point density at a given radius around the keypoint. Due to this, the Gaussian
    smoothing does not modify the information content of a brightness comparison by
    blurring two close sampling points while comparing them. BRISK uses a lesser number
    of sampling points than a simple pairwise comparison (because a single point participates
    in more comparisons), thereby reducing the complexity of looking up the intensity
    values. As the brightness variations only need to be locally consistent, the comparisons
    done here are restricted spatially. We obtain a bit string of length 512 using
    the sampling pattern and the distance thresholds as shown previously.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK使用确定性采样模式，导致在关键点周围给定半径处的采样点密度均匀。由于这个原因，高斯平滑在比较两个相邻采样点时不会修改亮度比较的信息内容。BRISK使用的采样点比简单的成对比较少（因为单个点参与更多的比较），从而减少了查找强度值的复杂性。由于亮度变化只需要局部一致，因此这里所做的比较在空间上受到限制。我们使用采样模式和之前显示的距离阈值获得长度为512位的字符串。
- en: BRISK In OpenCV
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenCV中的BRISK
- en: 'Again, the only change that we will make is to add another case to our switch
    case construct:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们将做出的唯一改变是向我们的switch case结构中添加另一个情况：
- en: '[PRE19]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![BRISK In OpenCV](img/B02052_03_18.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![OpenCV中的BRISK](img/B02052_03_18.jpg)'
- en: BRISK feature matching
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: BRISK特征匹配
- en: Fast Retina Keypoint
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速视网膜关键点
- en: '**Fast Retina Keypoint** (**FREAK**) proposes a robust descriptor to uniquely
    identify keypoints and in the process, require less computation time and memory.
    FREAK has been inspired by the human retina.'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '**快速视网膜关键点**（**FREAK**）提出了一种鲁棒的描述符，用于唯一标识关键点，在此过程中，需要更少的计算时间和内存。FREAK受到了人类视网膜的启发。'
- en: A retinal sampling pattern
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视网膜采样模式
- en: FREAK proposes to use the retinal sampling grid, which is also circular, with
    the difference of having higher density of points near the center. The density
    of points drops exponentially as we move away from the center point. This is similar
    to BRISK, except for the exponential decrease.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK建议使用视网膜采样网格，它也是圆形的，但与BRISK不同的是，中心附近点的密度更高。随着我们远离中心点，点的密度呈指数下降。这与BRISK类似，只是指数下降。
- en: Each keypoint needs to be smoothed to be less sensitive to noise. Unlike BRIEF
    and ORB, which use the same kernel for all points, FREAK uses a different kernel
    for each keypoint. The radius of the Gaussian kernel is proportional to the value
    of σ.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 每个关键点都需要进行平滑处理，以降低对噪声的敏感性。与BRIEF和ORB不同，它们对所有点使用相同的核，FREAK为每个关键点使用不同的核。高斯核的半径与σ的值成比例。
- en: '![A retinal sampling pattern](img/B02052_03_19.jpg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![视网膜采样模式](img/B02052_03_19.jpg)'
- en: Retinal sampling pattern
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 视网膜采样模式
- en: FREAK follows ORB's approach and tries to learn about the pairs by maximizing
    the variance of the pairs and taking pairs that are not correlated, so as to provide
    maximum information on each keypoint.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK遵循ORB的方法，通过最大化对之间的方差并选择不相关的对来尝试了解对，以便为每个关键点提供最大信息。
- en: A coarse-to-fine descriptor
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 粗到细的描述符
- en: 'We need to find pairs of sampling points in order to create a bit-vector. We
    use a method similar to ORB, that is, instead of matching each pair, we try to
    learn about which pairs would give the best results. We need to find points that
    are not correlated. The algorithm is as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要找到采样点的对，以便创建一个位向量。我们使用类似于ORB的方法，即不是匹配每一对，而是尝试了解哪些对会给出最佳结果。我们需要找到不相关的点。算法如下：
- en: We create a matrix D of nearly 50,000 extracted keypoints. Each row corresponds
    to a keypoint that is represented with its large descriptor made of all possible
    pairs in the retina sampling pattern. We use 43 receptive fields, leading to about
    1,000 pairs.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们创建了一个包含近50,000个提取的关键点的矩阵D。每一行对应一个关键点，该关键点通过其大描述符表示，描述符由视网膜采样模式中的所有可能对组成。我们使用43个感受野，导致大约1,000对。
- en: We compute the mean of each column. A mean of 0.5 produces the highest variance.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们计算每列的平均值。平均值为0.5会产生最高的方差。
- en: Order the columns according to the variance in descending order.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照列的方差降序排列。
- en: Select the best column and iteratively add the remaining columns so that they
    have low correlation with the chosen columns.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最佳列，并迭代地添加剩余的列，使它们与所选列的低相关性。
- en: In this approach, we first select pairs that compare sampling points in the
    outer regions, whereas the last pairs are comparison points in the inner rings
    of the pattern. This is similar to how our retina works in the sense that we first
    try to locate an object and then try to verify it by precisely matching points
    that are densely located near the object.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们首先选择比较外区域采样点的对，而最后几对是在图案的内环中的比较点。这在某种程度上类似于我们的视网膜工作方式，即我们首先尝试定位一个对象，然后通过精确匹配靠近对象密集分布的点来验证它。
- en: Saccadic search
  id: totrans-319
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 眨眼搜索
- en: Humans do not look at a scene in a fixed manner. Their eyes move around with
    discontinuous individual movements called saccades. The fovea captures high-resolution
    information; hence, it is critical in the recognition and matching of objects.
    The perifoveal region captures low-resolution information, and hence, it used
    to approximately locate objects.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 人类不会以固定方式观察场景。他们的眼睛以不连续的单独运动（称为眨眼）在场景中移动。黄斑区域捕捉高分辨率信息；因此，它在对象的识别和匹配中至关重要。周边区域捕捉低分辨率信息，因此，它被用来大致定位对象。
- en: FREAK tries to mimic this function of the retina by searching the first 16 bytes
    of the descriptor, representing the coarse information. If the distance is smaller
    than a threshold, we continue by searching the next bytes to obtain a more refined
    result. Due to this, a cascade of comparisons is performed, accelerating the matching
    step even further as more than 90 percent of the sampling points are discarded
    with the first 16 byte comparisons.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK 尝试通过搜索描述符的前 16 个字节来模仿视网膜的功能，这些字节代表粗略信息。如果距离小于一个阈值，我们继续搜索下一个字节以获得更精确的结果。由于这个原因，进行了一系列比较，进一步加速了匹配步骤，因为超过
    90% 的采样点在第一个 16 字节比较中被丢弃。
- en: Orientation
  id: totrans-322
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方向
- en: The method FREAK uses to assign orientation is similar to that of BRISK with
    the difference being that, instead of using long distance pairs, FREAK uses a
    predefined set of 45 symmetric sampling pairs.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK 方法用于赋值方向的方法与 BRISK 类似，不同之处在于，FREAK 不是使用长距离对，而是使用一组预定义的 45 对对称采样对。
- en: FREAK in OpenCV
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FREAK 在 OpenCV 中的应用
- en: 'The code for FREAK is similar to that used for the previous algorithms. However,
    given that FREAK just provides a descriptor, we will use the FAST detector to
    detect keypoints:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK 的代码与之前算法使用的代码类似。然而，鉴于 FREAK 只提供描述符，我们将使用 FAST 检测器来检测关键点：
- en: '[PRE20]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![FREAK in OpenCV](img/B02052_03_20.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![FREAK 在 OpenCV 中的应用](img/B02052_03_20.jpg)'
- en: FREAK feature matching
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: FREAK 特征匹配
- en: Summary
  id: totrans-329
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have seen how to detect features in an image and match them
    to features in other images. To perform this task, we looked at various algorithms,
    such as SIFT, SURF, ORB, BRISK, and FREAK, and their pros and cons. We also saw
    how we can use these to localize specific objects in a scene. There is one restriction
    to these methods in that the exact object must be present in the scene image to
    be detected correctly. In the next chapter, we will take a step forward to detect
    more general classes of objects, such as human beings, faces, hands, and so on.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何检测图像中的特征并将它们与其它图像中的特征进行匹配。为了执行这个任务，我们研究了各种算法，如 SIFT、SURF、ORB、BRISK
    和 FREAK，以及它们的优缺点。我们还看到了如何使用这些算法来定位场景中的特定物体。这些方法有一个限制，即必须场景图像中存在确切的对象才能正确检测。在下一章中，我们将进一步探索检测更一般类别的对象，例如人类、面部、手等。
