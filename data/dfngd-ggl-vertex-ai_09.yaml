- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Model Optimizations – Hyperparameter Tuning and NAS
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型优化 – 超参数调整和NAS
- en: We have now become quite familiar with some of the Vertex AI offerings related
    to managing data, training no-code and low-code models, and launching large-scale
    custom model training jobs (with metadata tracking and monitoring capabilities).
    As ML practitioners, we know that it is highly unlikely that the first model we
    train would be the best model for a given use case and dataset. Thus, in order
    to find the best model (which is the most accurate and least biased), we often
    use different model optimization techniques. **Hyperparameter Tuning** (**HPT**)
    and **Neural Architecture Search** (**NAS**) are two such model optimization techniques.
    In this chapter, we will learn how to configure and launch model optimization
    experiments using Vertex AI on Google Cloud.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经非常熟悉Vertex AI提供的一些与数据管理、无代码和低代码模型训练以及启动大规模定制模型训练作业（具有元数据跟踪和监控功能）相关的服务。作为机器学习从业者，我们知道我们训练的第一个模型很可能不是针对特定用例和数据集的最佳模型。因此，为了找到最佳模型（即最准确且偏差最小的模型），我们通常使用不同的模型优化技术。**超参数调整**（**HPT**）和**神经架构搜索**（**NAS**）是两种这样的模型优化技术。在本章中，我们将学习如何使用Vertex
    AI在Google Cloud上配置和启动模型优化实验。
- en: 'In this chapter, we will first learn about the importance of model optimization
    techniques such as HPT and then learn how to quickly set up and launch HPT jobs
    within Google Vertex AI. We will also understand how NAS works and how it is different
    from HPT. The topics covered in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先了解模型优化技术（如HPT）的重要性，然后学习如何在Google Vertex AI中快速设置和启动HPT作业。我们还将了解NAS的工作原理以及它与HPT的不同之处。本章涵盖的主题如下：
- en: What is HPT and why is it important?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是HPT以及为什么它很重要？
- en: Setting up HPT jobs on Vertex AI
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Vertex AI上设置HPT作业
- en: What is NAS and how is it different from HPT?
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAS是什么，它与HPT有何不同？
- en: NAS on Vertex AI overview
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI上的NAS概述
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The code examples shown in this chapter can be found in the following GitHub
    repo: [https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter09](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter09)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中展示的代码示例可以在以下GitHub仓库中找到：[https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter09](https://github.com/PacktPublishing/The-Definitive-Guide-to-Google-Vertex-AI/tree/main/Chapter09)
- en: What is HPT and why is it important?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是HPT以及为什么它很重要？
- en: Hyperparameter tuning, or HPT for short, is a popular model optimization technique
    that is very commonly used across ML projects. In this section, we will learn
    about hyperparameters, the importance of tuning them, and different methods of
    finding the best hyperparameters for a machine learning algorithm.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整，或简称HPT，是一种在机器学习项目中非常常用的模型优化技术。在本节中，我们将了解超参数、调整它们的重要性以及寻找机器学习算法最佳超参数的不同方法。
- en: What are hyperparameters?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是超参数？
- en: When we train an ML system, we basically have three kinds of data – input data,
    model parameters, and model hyperparameters. Input data refers to our training
    or test data that is associated with the problem we are solving. Model parameters
    are the variables that we modify during the model training process and we try
    to adjust them to fit the training data. Model hyperparameters, on the other hand,
    are variables that govern the training process itself. These hyperparameters are
    fixed before we start to train our model. For example, learning rate, optimizer,
    batch size, number of hidden layers in a neural network, and the max depth in
    a tree-based algorithm are some examples of model hyperparameters.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练一个机器学习系统时，我们基本上有三类数据 – 输入数据、模型参数和模型超参数。输入数据指的是与我们要解决的问题相关的训练或测试数据。模型参数是我们修改的变量，我们试图调整它们以适应训练数据。另一方面，模型超参数是控制训练过程本身的变量。这些超参数在我们开始训练模型之前是固定的。例如，学习率、优化器、批量大小、神经网络中的隐藏层数量以及基于树的算法中的最大深度都是模型超参数的例子。
- en: Why HPT?
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为什么选择HPT？
- en: How well your machine learning model will perform largely depends upon the hyperparameters
    you choose before training. The values of hyperparameters can make all the difference
    to performance metrics (such as accuracy), training time, bias, fairness, and
    so on for your model. Hyperparameter tuning or HPT is a model optimization technique
    that chooses a set of optimal hyperparameters for a learning algorithm. The same
    ML algorithm can require totally different values of hyperparameters to generalize
    for different data patterns. There is an objective function associated with every
    HPT job that it tries to optimize (minimize or maximize) and it returns the values
    of hyperparameters that achieve that optimal value. This objective function can
    be similar to the model training objective (e.g., loss function) or it can be
    a completely new metric.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器学习模型将表现如何很大程度上取决于你在训练之前选择的超参数。超参数的值可以对模型性能指标（如准确率）、训练时间、偏差、公平性等方面产生重大影响。超参数调整或HPT是一种模型优化技术，它为学习算法选择一组最优的超参数。相同的机器学习算法可能需要完全不同的超参数值来泛化不同的数据模式。每个HPT作业都与一个目标函数相关联，它试图优化（最小化或最大化）该目标函数，并返回实现该最优值的超参数值。这个目标函数可以类似于模型训练目标（例如，损失函数）或者可以是一个完全新的指标。
- en: We run model optimization operations such as HPT or NAS when we are at a stage
    where our final model (i.e., XGBoost) is fixed and we have a fixed test set for
    which we want to optimize the hyperparameters of our chosen model. A typical HPT
    job runs multiple trials with different sets of hyperparameters and returns the
    hyperparameters that lead to the best trial. The best trial here represents the
    trial that optimizes the objective function associated with the HPT job.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的最终模型（即XGBoost）固定，并且我们有固定的测试集，我们想要优化所选模型的超参数时，我们会运行模型优化操作，如HPT或NAS。一个典型的HPT作业运行多个试验，使用不同的超参数集，并返回导致最佳试验的超参数。这里的最佳试验代表优化与HPT作业相关的目标函数的试验。
- en: Search algorithms
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索算法
- en: 'While running HPT, we have to decide what kind of search algorithm we want
    to run over our hyperparameter space. There are multiple different kinds of search
    algorithms that we can choose from based on our needs. A few commonly used approaches
    are as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行超参数优化（HPT）时，我们必须决定在超参数空间中运行哪种搜索算法。根据我们的需求，我们可以从多种不同的搜索算法中进行选择。以下是一些常用的方法：
- en: Grid search
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索
- en: Random search
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Bayesian optimization
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Let’s discuss these approaches!
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论这些方法！
- en: Grid search
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 网格搜索
- en: The traditional way of performing HPT has been grid search, which is basically
    an exhaustive search over a manually specified search space. A grid search must
    be provided with a performance metric that it tries to calculate over all possible
    sets of hyperparameter combinations, measured over a hold-out validation set (or
    cross-validation on a training set). As it runs all possible combinations of provided
    hyperparameter ranges, it is important to set those ranges carefully and with
    discrete values. As grid search runs all the trials independently, it can be parallelized
    for faster outcomes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的HPT执行方式是网格搜索，这基本上是在手动指定的搜索空间上进行穷举搜索。网格搜索必须提供一个性能指标，它试图计算所有可能的超参数组合集，这些组合在保留的验证集（或训练集上的交叉验证）上测量。由于它运行所有提供的超参数范围的组合，因此设置这些范围时必须小心，并使用离散值。由于网格搜索独立运行所有试验，因此它可以并行化以获得更快的输出。
- en: Random search
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索
- en: Instead of trying all combinations sequentially and exhaustively like grid search,
    random search selects hyperparameters randomly from the provided search space
    during each trial. As it chooses hyperparameter values randomly, it also generalizes
    to continuous spaces along with discrete spaces, as discussed above. Random search
    again is highly parallelizable as all the trials are independent. Despite its
    simplicity, random search is one of the most important baselines to test new optimization
    or search techniques against.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索尝试所有组合的顺序和穷举搜索不同，随机搜索在每次试验中从提供的搜索空间中随机选择超参数。由于它随机选择超参数值，它也可以泛化到连续空间，如上所述。随机搜索再次高度可并行化，因为所有试验都是独立的。尽管它很简单，但随机搜索是测试新优化或搜索技术的重要基线之一。
- en: Bayesian optimization
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贝叶斯优化
- en: Unlike grid search and random search, the Bayesian optimization method builds
    a probabilistic model of the function that maps hyperparameter values to the HPT
    objective function. Thus, at each new trial, it learns better about the direction
    it should take to find the optimal hyperparameters for the given objective function
    on a fixed validation set. It tries to balance exploration and exploitation and
    has been shown to obtain better results than the preceding techniques in fewer
    trials. But as it learns from the ongoing trials, it often runs the trials iteratively
    (thus it’s less parallelizable).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索和随机搜索不同，贝叶斯优化方法构建了一个将超参数值映射到HPT目标函数的概率模型。因此，在每次新的试验中，它都会更好地了解它应该采取的方向，以找到给定目标函数在固定验证集上的最优超参数。它试图平衡探索和利用，并且已经证明在更少的试验中可以获得比先前技术更好的结果。但是，由于它从正在进行的试验中学习，它通常迭代地运行试验（因此它不太并行化）。
- en: Now that we have a good understanding of HPT, let’s understand how to set up
    and launch HPT jobs on Vertex AI.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对HPT有了很好的理解，让我们了解如何在Vertex AI上设置和启动HPT作业。
- en: Setting up HPT jobs on Vertex AI
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Vertex AI上设置HPT作业
- en: In this section, we will learn how to set up HPT jobs with Vertex AI. We will
    use the same neural network model experiment from [*Chapter 7*](B17792_07.xhtml#_idTextAnchor093),
    *Training Fully Custom ML Models with Vertex AI*, and optimize its hyperparameters
    to get the best model settings.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用Vertex AI设置HPT作业。我们将使用[*第7章*](B17792_07.xhtml#_idTextAnchor093)，*使用Vertex
    AI训练完全自定义的机器学习模型*中相同的神经网络模型实验，并优化其超参数以获得最佳模型设置。
- en: 'The first step is to create a new Jupyter Notebook in Vertex AI Workbench and
    import useful libraries:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在Vertex AI Workbench中创建一个新的Jupyter Notebook并导入有用的库：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we set up project configurations:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们设置项目配置：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, we initialize the Vertex AI SDK:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化Vertex AI SDK：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to containerize the full training application code. We will
    put our full training code into a Python file, `task.py`, here. The `task.py`
    file should have an entire flow, including the following:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将完整的训练应用程序代码容器化。我们将把完整的训练代码放入一个Python文件中，即`task.py`。`task.py`文件应该包含整个流程，包括以下内容：
- en: Loading and preparing the training data
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载和准备训练数据
- en: Defining the model architecture
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义模型架构
- en: Training the model (running a trial with given hyperparameters as args)
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型（运行具有给定超参数作为参数的试验）
- en: Saving the model (optional)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存模型（可选）
- en: Passing the training trial output to the `hypertune()` method
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练试验输出传递给`hypertune()`方法
- en: 'The training script should have a list of hyperparameters that it wants to
    tune, defined as arguments:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 训练脚本应该有一个它想要调整的超参数列表，定义为参数：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, we have other important hyperparameters, such as learning rate,
    batch size, loss function, and so on:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们还有其他重要的超参数，如学习率、批量大小、损失函数等：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The script should have a function that loads and prepares the training and
    validation datasets:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本应该有一个函数用于加载和准备训练集和验证集：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Similarly, the validation and test data parts are loaded:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，验证集和测试数据部分也被加载：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The preceding function loads the already prepared dataset from GCS. We can refer
    to [*Chapter 7*](B17792_07.xhtml#_idTextAnchor093), *Training Fully Custom ML
    Models with Vertex AI*, to fully understand the data preparation part.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个函数从GCS加载已准备好的数据集。我们可以参考[*第7章*](B17792_07.xhtml#_idTextAnchor093)，*使用Vertex
    AI训练完全自定义的机器学习模型*，以全面了解数据准备部分。
- en: 'Next, we define the **TensorFlow** (**TF**) model architecture:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义**TensorFlow**（**TF**）模型架构：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We then define the encoder part of the model:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来定义模型的编码器部分：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, we will define two more encoder layers with an increasing number
    of filters, a kernel size of 3, and a stride of 2 so that we can compress the
    image into important features:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将定义另外两个编码器层，具有递增的过滤器数量、3的内核大小和2的步长，以便我们可以将图像压缩到重要的特征：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the decoder part of the TF model within the same function:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一函数内定义TF模型的解码器部分：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see, the decoder design is almost opposite to the encoder part. Here,
    we re-create the image from compressed features by using multiple layers of transpose
    convolutions and reducing the channels gradually to 3 to generate the final color
    image output:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，解码器的设计几乎与编码器部分相反。在这里，我们通过使用多层转置卷积和逐步减少通道到3来从压缩特征中重新创建图像，以生成最终的彩色图像输出：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Add a `tanh` activation function to get the final colored output image:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 添加一个`tanh`激活函数以获得最终的彩色输出图像：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Also, add a function to build and compile the TF model:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 还添加了一个函数来构建和编译TF模型：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, add a `main` function that trains the model and provides the hyperparameter
    tuning metric value to the `hypertune()` function. In our case, we will be optimizing
    the loss over the validation dataset. See the following snippet:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加一个`main`函数来训练模型，并将超参数调整指标值提供给`hypertune()`函数。在我们的案例中，我们将优化验证数据集上的损失。请参见以下代码片段：
- en: '[PRE14]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Set up the configurations and load the data:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 设置配置并加载数据：
- en: '[PRE15]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, let’s build the TF model and fit it on the training data:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建TF模型并将其拟合到训练数据上：
- en: '[PRE16]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Use `hypertune` to define and report hyperparameter tuning metrics to the HPT
    algorithm:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`hypertune`定义并报告超参数调整指标给HPT算法：
- en: '[PRE17]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'If we put this all into a single Python file, our `task.py` file should look
    something like the following:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有这些放入一个单独的Python文件中，我们的`task.py`文件应该看起来像以下这样：
- en: '[PRE18]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Load all the dependencies for our task:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们任务的所有依赖项：
- en: '[PRE19]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Parse arguments where we define the hyperparameters used for tuning:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 解析参数，其中我们定义了用于调整的超参数：
- en: '[PRE20]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Define a few more hyperparameter-related arguments for tuning the learning
    rate, batch size, and loss functions:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一些与超参数相关的更多参数，用于调整学习率、批大小和损失函数：
- en: '[PRE21]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Set up configurations for training purposes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 设置用于训练的配置：
- en: '[PRE22]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Define configuration settings for training distribution strategies based on
    the requirements – it can be a single, mirror, or multi-worker strategy:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 根据要求定义基于训练分布策略的配置设置——可以是单一、镜像或多工作者策略：
- en: '[PRE23]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Load and prepare the training, validation, and test partitions of data from
    the GCS bucket:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从GCS存储桶加载数据并准备训练、验证和测试分区：
- en: '[PRE24]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Similarly, load the validation and test partitions:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，加载验证和测试分区：
- en: '[PRE25]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Define the TF model architecture for converting a black-and-white image to
    a color image:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 定义将黑白图像转换为彩色图像的TF模型架构：
- en: '[PRE26]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Define the encoder part of the model:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型编码器部分：
- en: '[PRE27]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Similarly, we will define two more encoder layers with an increasing number
    of filters, a kernel size of 3, and a stride of 2 so that we can compress the
    image into important features:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们将定义两个额外的编码器层，具有递增的过滤器数量、3的核大小和2的步长，以便我们可以将图像压缩到重要的特征：
- en: '[PRE28]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Define the decoder part of the model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型的解码器部分：
- en: '[PRE29]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'As we can see, the decoder design is almost opposite to the encoder part. Here,
    we re-create the image from compressed features by using multiple layers of transpose
    convolutions and reduce the channels gradually to 3 to generate the final color
    image output:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，解码器设计几乎与编码器部分相反。在这里，我们通过使用多层转置卷积重新创建图像，并逐步减少通道到3以生成最终的彩色图像输出：
- en: '[PRE30]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Finally, generate the color image output by using the `tanh` activation function:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`tanh`激活函数生成彩色图像输出：
- en: '[PRE31]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following function will build and compile the TF model for us:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数将为我们构建和编译TF模型：
- en: '[PRE32]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s define the main function to start executing our training and tuning
    task. Here, the `num_replicas_in_sync` parameter defines how many training tasks
    are running in parallel on different workers in a multi-worker training strategy:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义主函数以开始执行我们的训练和调整任务。在这里，`num_replicas_in_sync`参数定义了在多工作者训练策略中不同工作者上并行运行多少个训练任务：
- en: '[PRE33]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Load the training and validation data to start training our TF model:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 加载训练和验证数据以开始训练我们的TF模型：
- en: '[PRE34]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Finally, define the HPT metric with the help of the `hypertune` package:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用`hypertune`包定义HPT指标：
- en: '[PRE35]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we create a staging bucket in GCS that will be used for storing artifacts
    such as trial outcomes from our HPT job:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在GCS中创建一个用于存储HPT作业中试验结果等工件的中转存储桶：
- en: '[PRE36]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The next step is to containerize the entire training code defined in the `task.py`
    file. The hyperparameter tuning job will use this container to launch different
    trials with different hyperparameters as arguments:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将`task.py`文件中定义的整个训练代码容器化。超参数调整作业将使用此容器以不同的超参数作为参数启动不同的试验：
- en: '[PRE37]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Our Dockerfile is ready – let’s build and push the Docker image to **Google
    Container** **Registry** (**GCR**):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的Dockerfile已经准备好了——让我们构建并推送Docker镜像到**Google Container Registry**（**GCR**）：
- en: '[PRE38]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Now we have a container image ready with all the training code that we need.
    Let’s configure the HPT job.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了包含所有所需训练代码的容器镜像。让我们配置HPT作业。
- en: 'First, we define the type of machine we want our trials to run on. The machine
    specification will depend upon the size of the model and training dataset. As
    this is a small experiment, we will use the `n1-standard-8` machine to run it:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义我们希望试验运行的机器类型。机器规格将取决于模型的大小和训练数据集的大小。由于这是一个小实验，我们将使用 `n1-standard-8`
    机器来运行它：
- en: '[PRE39]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Note that, within the worker pool spec, we have also passed the path to the
    training image that we created.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在工作者池规格中，我们还传递了我们创建的训练镜像的路径。
- en: 'Next, we will define the parameter space that our job will use to find the
    best hyperparameters:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义我们的作业将使用以找到最佳超参数的参数空间：
- en: '[PRE40]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The parameter space should be carefully defined based on best practices and
    prior knowledge so that the HPT job doesn’t have to perform unnecessary trials
    over unimportant hyperparameter ranges.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 参数空间应根据最佳实践和先验知识仔细定义，以便 HPT 作业不必在无关紧要的超参数范围内执行不必要的试验。
- en: 'Next, we need to define the metric specifications. In our case, as we are trying
    to optimize the validation loss value, we would like to minimize it. In the case
    of accuracy, we should have maximized our metric:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要定义指标规格。在我们的案例中，因为我们正在尝试优化验证损失值，我们希望最小化它。在准确度的情况下，我们应该最大化我们的指标：
- en: '[PRE41]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Vertex AI HPT jobs use the Bayesian optimization approach by default to find
    the best hyperparameters for our settings. We also have the option to use other
    optimization approaches. As Bayesian optimization works best for most cases, we
    will be using it in our experiment.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI HPT 作业默认使用贝叶斯优化方法来找到我们设置的最好超参数。我们还有使用其他优化方法的选择。由于贝叶斯优化在大多数情况下效果最好，我们将在实验中使用它。
- en: 'Next, we define the custom job that will run our hyperparameter tuning trials:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义将运行我们的超参数调整试验的自定义作业：
- en: '[PRE42]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Finally, we define the HPT job that will launch the trials using the preceding
    custom job:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们定义将使用前面定义的自定义作业启动试验的 HPT 作业：
- en: '[PRE43]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Note that the `max_trial_count` and `parallel_trial_count` parameters are important
    here:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`max_trial_count` 和 `parallel_trial_count` 参数在这里非常重要：
- en: '`max_trial_count`: You need to put an upper bound on the number of trials the
    service will run. More trials generally lead to better results, but there will
    be a point of diminishing returns, after which additional trials have little or
    no effect on the metric you’re trying to optimize. It is best practice to start
    with a smaller number of trials and get a sense of how impactful your chosen hyperparameters
    are before scaling up.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_trial_count`：您需要为服务将运行的试验数量设置一个上限。更多的试验通常会导致更好的结果，但会有一个收益递减的点，在此之后，额外的试验对您试图优化的指标的影响很小或没有影响。最佳实践是从较小的试验数量开始，在扩展之前了解您选择的超参数的影响程度。'
- en: '`parallel_trial_count`: If you use parallel trials, the service provisions
    multiple training processing clusters. Increasing the number of parallel trials
    reduces the amount of time the hyperparameter tuning job takes to run; however,
    it can reduce the effectiveness of the job overall. This is because the default
    tuning strategy uses the results of previous trials to inform the assignment of
    values in subsequent trials. If we keep the parallel trial count equal to the
    number of maximum trials, then all trials will start running in parallel, and
    we will end up running a “random parameter search” here as there will not be any
    scope of learning from the performance of previous trials.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallel_trial_count`：如果您使用并行试验，服务将提供多个训练处理集群。增加并行试验的数量可以减少超参数调整作业运行所需的时间；然而，这可能会降低作业的整体效果。这是因为默认的调整策略使用先前试验的结果来告知后续试验值分配。如果我们保持并行试验计数等于最大试验数，那么所有试验都将并行启动，并且我们将最终运行一个“随机参数搜索”，因为没有从先前试验的性能中学习的空间。'
- en: 'Now that we are all set, we can launch the HPT job:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了，我们可以启动 HPT 任务：
- en: '[PRE44]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: As soon as we launch the job, it provides us with a link to the Cloud console
    UI, where we can monitor the progress of our HPT trials and jobs. The Cloud console
    UI looks something similar to what’s shown in *Figure 9**.1*.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启动作业，它就会提供一个链接到云控制台 UI，在那里我们可以监控我们的 HPT 试验和作业的进度。云控制台 UI 的外观类似于 *图 9**.1*
    中所示。
- en: '![Figure 9.1 – HPT job monitoring within the Cloud console UI](img/B17792_09_1.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – 在云控制台 UI 中监控 HPT 任务](img/B17792_09_1.jpg)'
- en: Figure 9.1 – HPT job monitoring within the Cloud console UI
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 在云控制台 UI 中监控 HPT 任务
- en: Now that we have successfully understood and launched an HPT job on Vertex AI,
    let’s jump to the next section and understand the NAS model optimization technique.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功理解和启动了在Vertex AI上的HPT作业，让我们跳到下一部分，了解NAS模型优化技术。
- en: What is NAS and how is it different from HPT?
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是NAS以及它与HPT有何不同？
- en: '**Artificial Neural Networks** or **ANNs** are widely used today for solving
    complex ML problems. Most of the time, these network architectures are hand-designed
    by ML experts, which may not be optimal every time. **Neural Architecture Search**
    or **NAS** is a technique that automates the process of designing neural network
    architectures that usually outperform hand-designed networks.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**或**ANNs**今天被广泛用于解决复杂的机器学习问题。大多数时候，这些网络架构是由机器学习专家手工设计的，这并不总是最优的。**神经架构搜索**或**NAS**是一种自动化设计神经网络架构的过程的技术，通常比手工设计的网络表现更好。'
- en: Although both HPT and NAS are used as model optimization techniques, there are
    certain differences in how they both work. HPT assumes a given architecture and
    focuses on optimizing the hyperparameters that lead to the best model. HPT optimizes
    hyperparameters such as learning rate, optimizer, batch size, activation function,
    and so on. NAS, on the other hand, focuses on optimizing architecture-specific
    parameters (in a way, it automates the process of designing a neural network architecture).
    NAS optimizes parameters such as the number of layers, number of units, types
    of connections between layers, and so on. Using NAS, we can search for optimal
    neural architectures in terms of accuracy, latency, memory, a combination of these,
    or a custom metric.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然HPT和NAS都被用作模型优化技术，但它们的工作方式存在某些差异。HPT假设一个给定的架构，并专注于优化导致最佳模型的超参数。HPT优化超参数，如学习率、优化器、批量大小、激活函数等。另一方面，NAS专注于优化特定于架构的参数（在某种程度上，它自动化了设计神经网络架构的过程）。NAS优化参数，如层数、单元数、层之间的连接类型等。使用NAS，我们可以根据准确性、延迟、内存、这些特性的组合或自定义指标来搜索最优的神经网络架构。
- en: 'NAS usually works with a relatively larger search space than HPT and controls
    different aspects of network architectures. However, the underlying problem addressed
    is the same as HPT optimization. There are many NAS-based optimization approaches,
    but on a high level, any NAS approach has three main components, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: NAS通常与比HPT更大的搜索空间一起工作，并控制网络架构的不同方面。然而，解决的根本问题是与HPT优化相同的。有许多基于NAS的优化方法，但在高层次上，任何NAS方法都有三个主要组件，如下所示：
- en: Search space
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索空间
- en: Optimization method
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化方法
- en: Evaluation method
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估方法
- en: Let’s learn more about each of these components.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解这些组件的每一个。
- en: Search space
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 搜索空间
- en: This component controls the set of possible neural architectures to consider.
    The search space is often problem-specific, as a vision-related problem would
    have the possibility of having **Convolutional Neural Network** (**CNN**) layers
    as well. However, the process of identifying the best architecture is automated
    by NAS. Carefully designing these search spaces still depends upon human expertise.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 此组件控制要考虑的可能神经网络架构的集合。搜索空间通常是特定于问题的，例如，与视觉相关的问题可能会有**卷积神经网络**（**CNN**）层的可能性。然而，识别最佳架构的过程是通过NAS自动化的。仔细设计这些搜索空间仍然依赖于人类的专业知识。
- en: Optimization method
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化方法
- en: This component decides how to navigate the search space to find the best possible
    architecture for a given application. Many different optimization methods have
    been applied to NAS, such as **reinforcement learning** (**RL**), Bayesian optimization,
    gradient-based optimization, evolutionary search, and so on. Each of these methods
    has its own way of evaluating the architectures, but the high-level goal is to
    focus on the area of the search space that provides better performance. This aspect
    of NAS is quite similar to HPT optimization methods.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此组件决定如何导航搜索空间以找到给定应用的最佳可能架构。许多不同的优化方法已经应用于NAS，例如**强化学习**（**RL**）、贝叶斯优化、基于梯度的优化、进化搜索等。这些方法中的每一种都有其评估架构的独特方式，但高层次的目标是专注于提供更好性能的搜索空间区域。这一方面的NAS与HPT优化方法相当相似。
- en: Evaluation method
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估方法
- en: The evaluation method is a component that is used for assessing the quality
    of architectures designed by the chosen optimization method. One simple way to
    evaluate neural architecture is to fully train it, but this method is quite computationally
    expensive. Alternatively, to make NAS more efficient, partial training and evaluation
    methods have been developed. In order to provide cheaper heuristic measures of
    neural network quality, some evaluation methods have been developed. These evaluation
    methods are quite specific to NAS and exploit the basic structure of a neural
    network to estimate the quality of a network. Some examples of these methods include
    weight-sharing, hypernetworks, network morphism, and so on. These NAS-specific
    evaluation methods are practically way cheaper than full training.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 评估方法是用于评估所选优化方法设计的架构质量的一个组件。评估神经架构的一个简单方法是对其进行完全训练，但这种方法在计算上相当昂贵。作为替代，为了使神经架构搜索（NAS）更高效，已经开发出了部分训练和评估方法。为了提供更便宜的神经网络质量启发式度量，一些评估方法已经被开发出来。这些评估方法非常特定于NAS，并利用神经网络的基本结构来估计网络的质量。这些方法的一些例子包括权重共享、超网络、网络形态学等。这些针对NAS的特定评估方法在实际上比完全训练便宜得多。
- en: We now have a good understanding of the NAS optimization method and how it works.
    Next, let’s explore the Vertex AI offering and its features for launching NAS
    on Google Cloud.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对NAS优化方法及其工作原理有了很好的理解。接下来，让我们探索Vertex AI提供的产品及其在Google Cloud上启动NAS的功能。
- en: NAS on Vertex AI overview
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Vertex AI NAS概述
- en: Vertex AI NAS is an optimization technique that can be leveraged to find the
    best neural network architecture for a given ML use case. NAS-based optimization
    searches for the best network in terms of accuracy but can also be augmented with
    other constraints such as latency, memory, or a custom metric as per the requirements.
    In general, the search space of possible neural networks can be quite large and
    NAS may support a search space as large as 10^20\. In the past few years, NAS
    has been able to successfully generate some state-of-the-art computer vision network
    architectures, including NASNet, MNasNet, EfficientNet, SpineNet, NAS-FPN, and
    so on.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI NAS是一种优化技术，可以用来为给定的机器学习用例找到最佳的神经网络架构。基于NAS的优化在准确性方面寻找最佳网络，但也可以通过其他约束来增强，例如延迟、内存或根据需求定制的指标。一般来说，可能的神经网络搜索空间可以相当大，NAS可能支持高达10^20的搜索空间。在过去的几年里，NAS已经能够成功生成一些最先进的计算机视觉网络架构，包括NASNet、MNasNet、EfficientNet、SpineNet、NAS-FPN等。
- en: It may seem complex, but NAS features are quite flexible and easy to use. A
    beginner can leverage prebuilt modules for search spaces, trainer scripts, and
    Jupyter notebooks to start exploring Vertex AI NAS on a custom dataset. If you
    are an expert, you could potentially develop custom trainer scripts, custom search
    spaces, custom evaluation methods, and even develop applications for non-vision-based
    use cases.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看起来可能很复杂，但NAS功能非常灵活且易于使用。初学者可以利用预构建的模块来探索搜索空间、训练脚本和Jupyter笔记本，以在自定义数据集上开始探索Vertex
    AI NAS。如果您是专家，您可能开发自定义训练脚本、自定义搜索空间、自定义评估方法，甚至为非视觉用例开发应用程序。
- en: 'Vertex AI can be leveraged to explore the full set of NAS features for our
    customized architectures and use cases. Here is what Vertex AI provides us with
    to help in implementing NAS more conveniently:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI可以用来探索针对我们定制架构和用例的完整NAS功能集。以下是Vertex AI为我们提供的一些帮助，以便更方便地实现NAS：
- en: Vertex AI provides a NAS-specific language that can be leveraged to define a
    custom search space to try out the desired set of possible neural network architectures
    and integrate this space with our custom trainer scripts.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI提供了一种特定的NAS语言，可以利用它来定义一个自定义搜索空间，以尝试所需的可能神经网络架构集合，并将此空间与我们的自定义训练脚本集成。
- en: Pre-built state-of-the-art search spaces and a trainer that are ready to use
    and can run on a GPU.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预构建的最先进的搜索空间和训练器，可以立即使用并在GPU上运行。
- en: A pre-defined NAS controller that samples our custom-defined search space to
    find the best neural network architecture.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个预定义的NAS控制器，它从我们自定义定义的搜索空间中采样，以找到最佳的神经网络架构。
- en: A set of prebuilt libraries and Docker images that can be leveraged to calculate
    latency, FLOPS (Floating-point operations per second), or memory usage on a custom
    hardware setting.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组预构建的库和Docker镜像，可以用来在自定义硬件设置上计算延迟、FLOPS（每秒浮点运算次数）或内存使用情况。
- en: Google Cloud provides tutorials to explain the usage of NAS. It also provides
    examples and guidance for setting up NAS for PyTorch-based applications efficiently.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud 提供教程来解释 NAS 的使用方法。它还提供了设置 NAS 以高效运行 PyTorch 应用程序的示例和指导。
- en: Pre-built tools to design proxy tasks.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预构建工具用于设计代理任务。
- en: There is library support that can be leveraged to report custom-defined metrics
    and perform analysis on them.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有库支持可以利用，以报告自定义定义的指标并对它们进行分析。
- en: The Google Cloud console is very helpful in monitoring and managing NAS jobs.
    We also get some easy-to-use example notebooks to kick-start the search.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud 控制台在监控和管理 NAS 作业方面非常有帮助。我们还获得了一些易于使用的示例笔记本，以启动搜索。
- en: Management of CPU/GPU resource usage on the basis of per project or per job,
    with the help of a prebuilt library.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于每个项目或每个作业的 CPU/GPU 资源使用管理，借助预构建库。
- en: A NAS client to build Docker images, launch NAS jobs, and resume an old NAS
    search job that is Python-based.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 NAS 客户端，用于构建 Docker 镜像、启动 NAS 作业以及恢复基于 Python 的旧 NAS 搜索作业。
- en: Customer support is Google Cloud console UI-based.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户支持是基于 Google Cloud 控制台用户界面的。
- en: These features can help us in setting up a custom NAS job without putting in
    too much effort. Now let’s discuss some of the best practices while working with
    NAS.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这些功能可以帮助我们在不花费太多努力的情况下设置定制的 NAS 作业。现在让我们讨论一些与 NAS 一起工作的最佳实践。
- en: NAS best practices
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NAS 最佳实践
- en: 'The important thing to note here is that NAS is not an optimization method
    that we should apply to all our ML problems. There are certain things to keep
    in mind before deciding to run a NAS job for our use case. Some of these best
    practices are as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的重要事项是，NAS 不是一个我们应该应用于所有机器学习问题的优化方法。在决定为我们的用例运行 NAS 作业之前，有一些事情需要牢记。以下是一些最佳实践：
- en: NAS is not meant for tuning the hyperparameters of a model. It only performs
    an architecture search and it is not advised to compare the results of these two
    methods. In some setups, HPT can be followed by NAS.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAS 不适用于调整模型的超参数。它只执行架构搜索，不建议比较这两种方法的成果。在某些配置中，HPT 可以在 NAS 之后进行。
- en: NAS is not recommended for smaller or highly imbalanced datasets.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAS 不建议用于较小的或高度不平衡的数据集。
- en: NAS is expensive, so unless we can spend a few thousand dollars without extremely
    high expectations, it’s not meant for us.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NAS 成本高昂，除非我们可以在没有极高期望的情况下花费几千美元，否则它不适合我们。
- en: You should first try other traditional and conventional machine learning methods
    and techniques such as hyperparameter tuning. You should use neural architecture
    search only if you don’t see further gains with traditional methods.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你应该首先尝试其他传统和常规的机器学习方法和技术，如超参数调整。只有当你没有看到传统方法带来进一步收益时，才应使用神经架构搜索。
- en: Setting up NAS jobs on Vertex AI is not very complex, thanks to the prebuilt
    assets and publicly released code examples. With these prebuilt features, examples,
    and best practices, we should be able to set up a custom NAS job that can help
    us find an optimal architecture to meet our project goals.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预构建资源和公开发布的代码示例，在 Vertex AI 上设置 NAS 作业并不复杂。有了这些预构建功能、示例和最佳实践，我们应该能够设置一个定制的
    NAS 作业，帮助我们找到满足项目目标的最佳架构。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the importance of applying model optimization
    techniques to get the best performance for our application. We learned about two
    model optimization methods – HPT and NAS, with their similarities and differences.
    We also learned how to set up and launch large-scale HPT jobs on Vertex AI with
    code examples. Additionally, we discussed some best practices to get the best
    out of both HPT and NAS.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了将模型优化技术应用于我们的应用程序以获得最佳性能的重要性。我们学习了两种模型优化方法——HPT 和 NAS，以及它们的相似之处和不同之处。我们还通过代码示例学习了如何在
    Vertex AI 上设置和启动大规模 HPT 作业。此外，我们还讨论了一些最佳实践，以充分利用 HPT 和 NAS。
- en: After reading this chapter, you should have a fair understanding of the term
    “model optimization” and its importance while developing ML applications. Additionally,
    you should now be confident about quickly setting up small to large-scale hyperparameter
    tuning experiments with the help of Vertex AI tooling on Google Cloud. You should
    also have a fair understanding of NAS, its differences from HPT, and the best
    practices for setting up a NAS job.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章之后，你应该对“模型优化”这个术语及其在开发机器学习应用程序中的重要性有一个公正的理解。此外，你现在应该对使用 Google Cloud 上的
    Vertex AI 工具快速设置从小型到大型规模的超参数调整实验充满信心。你还应该对 NAS、它与 HPT 的区别以及设置 NAS 作业的最佳实践有一个公正的理解。
- en: Now that we understand the importance and common methods of model optimization
    techniques, we are in good shape to develop high-quality models. Next, let’s learn
    about how to deploy these models so that they can be consumed by downstream applications.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了模型优化技术的重要性以及常见的方法，我们具备了开发高质量模型的良好基础。接下来，让我们学习如何部署这些模型，以便它们可以被下游应用所使用。
