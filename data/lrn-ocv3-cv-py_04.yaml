- en: Chapter 4. Depth Estimation and Segmentation
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章 深度估计与分割
- en: This chapter shows you how to use data from a depth camera to identify foreground
    and background regions, so that we can limit an effect to only the foreground
    or only the background. As prerequisites, we need a depth camera, such as Microsoft
    Kinect, and we need to build OpenCV with support for our depth camera. For build
    instructions, see [Chapter 1](part0014.xhtml#aid-DB7S2 "Chapter 1. Setting Up
    OpenCV"), *Setting Up OpenCV*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章向您展示如何使用深度相机的数据来识别前景和背景区域，以便我们可以将效果限制在仅前景或仅背景。作为先决条件，我们需要一个深度相机，例如微软Kinect，并且我们需要构建支持我们深度相机的OpenCV。有关构建说明，请参阅[第1章](part0014.xhtml#aid-DB7S2
    "第1章。设置OpenCV")，*设置OpenCV*。
- en: 'We''ll deal with two main topics in this chapter: depth estimation and segmentation.
    We will explore depth estimation with two distinct approaches: firstly, by using
    a depth camera (a prerequisite of the first part of the chapter), such as Microsoft
    Kinect, and then, by using stereo images, for which a normal camera will suffice.
    For instructions on how to build OpenCV with support for depth cameras, see [Chapter
    1](part0014.xhtml#aid-DB7S2 "Chapter 1. Setting Up OpenCV"), *Setting Up OpenCV*.
    The second part of the chapter is about segmentation, the technique that allows
    us to extract foreground objects from an image.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将处理两个主要主题：深度估计和分割。我们将通过两种不同的方法来探索深度估计：首先，使用深度相机（本章第一部分的先决条件），例如微软Kinect；然后，使用立体图像，对于普通相机就足够了。有关如何构建支持深度相机的OpenCV的说明，请参阅[第1章](part0014.xhtml#aid-DB7S2
    "第1章。设置OpenCV")，*设置OpenCV*。本章的第二部分是关于分割，这是一种允许我们从图像中提取前景对象的技术。
- en: Creating modules
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建模块
- en: 'The code to capture and manipulate depth-camera data will be reusable outside
    `Cameo.py`. So, we should separate it into a new module. Let''s create a file
    called `depth.py` in the same directory as `Cameo.py`. We need the following `import`
    statement in `depth.py`:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获和处理深度相机数据的代码将在`Cameo.py`外部可重用。因此，我们应该将其分离到一个新的模块中。让我们在`Cameo.py`相同的目录下创建一个名为`depth.py`的文件。在`depth.py`中，我们需要以下`import`语句：
- en: '[PRE0]'
  id: totrans-5
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will also need to modify our preexisting `rects.py` file so that our copy
    operations can be limited to a nonrectangular subregion of a rectangle. To support
    the changes we are going to make, let''s add the following `import` statements
    to `rects.py`:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要修改现有的`rects.py`文件，以便我们的复制操作可以限制在矩形的非矩形子区域内。为了支持我们将要进行的更改，让我们向`rects.py`添加以下`import`语句：
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, the new version of our application will use depth-related functionalities.
    So, let''s add the following `import` statement to `Cameo.py`:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应用的新版本将使用与深度相关的功能。因此，让我们向`Cameo.py`添加以下`import`语句：
- en: '[PRE2]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now, let's go deeper into the subject of depth.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地探讨深度主题。
- en: Capturing frames from a depth camera
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从深度相机捕获帧
- en: Back in [Chapter 2](part0019.xhtml#aid-I3QM2 "Chapter 2. Handling Files, Cameras,
    and GUIs"), *Handling Files, Cameras, and GUIs*, we discussed the concept that
    a computer can have multiple video capture devices and each device can have multiple
    channels. Suppose a given device is a stereo camera. Each channel might correspond
    to a different lens and sensor. Also, each channel might correspond to different
    kinds of data, such as a normal color image versus a depth map. The C++ version
    of OpenCV defines some constants for the identifiers of certain devices and channels.
    However, these constants are not defined in the Python version.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第2章](part0019.xhtml#aid-I3QM2 "第2章。处理文件、相机和GUI")，*处理文件、相机和GUI*中，我们讨论了计算机可以拥有多个视频捕获设备，并且每个设备可以有多个通道的概念。假设一个给定的设备是立体相机。每个通道可能对应不同的镜头和传感器。此外，每个通道可能对应不同类型的数据，例如正常彩色图像与深度图。OpenCV的C++版本定义了一些用于某些设备和通道标识符的常量。然而，这些常量在Python版本中并未定义。
- en: 'To remedy this situation, let''s add the following definitions in `depth.py`:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，让我们在`depth.py`中添加以下定义：
- en: '[PRE3]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The depth-related channels require some explanation, as given in the following
    list:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 深度相关通道需要一些解释，如下所示列表中所述：
- en: A **depth map** is a grayscale image in which each pixel value is the estimated
    distance from the camera to a surface. Specifically, an image from the `CAP_OPENNI_DEPTH_MAP`
    channel gives the distance as a floating-point number of millimeters.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度图**是一种灰度图像，其中每个像素值代表从相机到表面的估计距离。具体来说，来自`CAP_OPENNI_DEPTH_MAP`通道的图像将距离表示为毫米的浮点数。'
- en: A **point cloud** **map** is a color image in which each color corresponds to
    an (x, y, or z) spatial dimension. Specifically, the `CAP_OPENNI_POINT_CLOUD_MAP`
    channel yields a BGR image, where B is x (blue is right), G is y (green is up),
    and R is z (red is deep), from the camera's perspective. The values are in meters.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点云图**是一个彩色图像，其中每个颜色对应于一个（x，y或z）空间维度。具体来说，`CAP_OPENNI_POINT_CLOUD_MAP` 通道产生一个BGR图像，其中B是x（蓝色是右侧），G是y（绿色是上方），R是z（红色是深度），从相机的视角来看。这些值以米为单位。'
- en: A **disparity map** is a grayscale image in which each pixel value is the stereo
    disparity of a surface. To conceptualize stereo disparity, let's suppose we overlay
    two images of a scene, shot from different viewpoints. The result would be similar
    to seeing double images. For points on any pair of twin objects in the scene,
    we can measure the distance in pixels. This measurement is the stereo disparity.
    Nearby objects exhibit greater stereo disparity than far-off objects. Thus, nearby
    objects appear brighter in a disparity map.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视差图**是一个灰度图像，其中每个像素值是表面的立体视差。为了概念化立体视差，让我们假设我们叠加了从不同视角拍摄的同一场景的两个图像。结果将类似于看到双重图像。对于场景中任何一对孪生物体上的点，我们可以测量像素距离。这种测量是立体视差。靠近的物体表现出比远处的物体更大的立体视差。因此，靠近的物体在视差图中看起来更亮。'
- en: A **valid depth mask** shows whether the depth information at a given pixel
    is believed to be valid (shown by a nonzero value) or invalid (shown by a value
    of zero). For example, if the depth camera depends on an infrared illuminator
    (an infrared flash), depth information is invalid in regions that are occluded
    (shadowed) from this light.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**有效的深度掩码**显示给定像素处的深度信息是否被认为是有效的（通过非零值表示）或无效的（通过零值表示）。例如，如果深度相机依赖于红外照明器（红外闪光灯），那么从该光源被遮挡（阴影）的区域中的深度信息是无效的。
- en: 'The following screenshot shows a point cloud map of a man sitting behind a
    sculpture of a cat:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个坐在猫雕塑后面的男人的点云图：
- en: '![Capturing frames from a depth camera](img/image00200.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![从深度相机捕获帧](img/image00200.jpeg)'
- en: 'The following screenshot has a disparity map of a man sitting behind a sculpture
    of a cat:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了一个坐在猫雕塑后面的男人的视差图：
- en: '![Capturing frames from a depth camera](img/image00201.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![从深度相机捕获帧](img/image00201.jpeg)'
- en: 'A valid depth mask of a man sitting behind a sculpture of a cat is shown in
    the following screenshot:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了坐在猫雕塑后面的男人的有效深度掩码：
- en: '![Capturing frames from a depth camera](img/image00202.jpeg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![从深度相机捕获帧](img/image00202.jpeg)'
- en: Creating a mask from a disparity map
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从视差图创建掩码
- en: For the purposes of Cameo, we are interested in disparity maps and valid depth
    masks. They can help us refine our estimates of facial regions.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Cameo的目的，我们感兴趣的是视差图和有效深度掩码。它们可以帮助我们细化我们对面部区域的估计。
- en: Using the `FaceTracker` function and a normal color image, we can obtain rectangular
    estimates of facial regions. By analyzing such a rectangular region in the corresponding
    disparity map, we can tell that some pixels within the rectangle are outliers—too
    near or too far to really be a part of the face. We can refine the facial region
    to exclude these outliers. However, we should only apply this test where the data
    is valid, as indicated by the valid depth mask.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `FaceTracker` 函数和正常彩色图像，我们可以获得面部区域的矩形估计。通过分析相应的视差图中的这样一个矩形区域，我们可以知道矩形内的某些像素是异常值——太近或太远，实际上不可能是面部的一部分。我们可以细化面部区域以排除这些异常值。然而，我们只应在数据有效的地方应用此测试，正如有效深度掩码所示。
- en: 'Let''s write a function to generate a mask whose values are `0` for the rejected
    regions of the facial rectangle and `1` for the accepted regions. This function
    should take a disparity map, valid depth mask, and a rectangle as arguments. We
    can implement it in `depth.py` as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写一个函数来生成一个掩码，其值对于被拒绝的面部矩形区域为 `0`，对于接受的区域为 `1`。这个函数应该接受视差图、有效深度掩码和一个矩形作为参数。我们可以在
    `depth.py` 中实现它如下：
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To identify outliers in the disparity map, we first find the median using `numpy.median()`,
    which takes an array as an argument. If the array is of an odd length, `median()`
    returns the value that would lie in the middle of the array if the array were
    sorted. If the array is of even length, `median()` returns the average of the
    two values that would be sorted nearest to the middle of the array.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在视差图中识别异常值，我们首先使用 `numpy.median()` 函数找到中位数，该函数需要一个数组作为参数。如果数组长度为奇数，`median()`
    函数返回如果数组排序后位于中间的值。如果数组长度为偶数，`median()` 函数返回位于数组中间两个排序值之间的平均值。
- en: To generate a mask based on per-pixel Boolean operations, we use `numpy.where()`
    with three arguments. In the first argument, `where()` takes an array whose elements
    are evaluated for truth or falsity. An output array of like dimensions is returned.
    Wherever an element in the input array is `true`, the `where()` function's second
    argument is assigned to the corresponding element in the output array. Conversely,
    wherever an element in the input array is `false`, the `where()` function's third
    argument is assigned to the corresponding element in the output array.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要根据每个像素的布尔操作生成掩码，我们使用 `numpy.where()` 并提供三个参数。在第一个参数中，`where()` 接收一个数组，其元素被评估为真或假。返回一个具有相同维度的输出数组。在输入数组中的任何元素为
    `true` 时，`where()` 函数的第二参数被分配给输出数组中的相应元素。相反，在输入数组中的任何元素为 `false` 时，`where()` 函数的第三参数被分配给输出数组中的相应元素。
- en: Our implementation treats a pixel as an outlier when it has a valid disparity
    value that deviates from the median disparity value by 12 or more. I've chosen
    the value of 12 just by experimentation. Feel free to tweak this value later based
    on the results you encounter when running Cameo with your particular camera setup.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现将具有有效视差值且与中值视差值偏差12或更多的像素视为异常值。我仅通过实验选择了12这个值。请根据您使用特定相机设置运行 Cameo 时遇到的结果自由调整此值。
- en: Masking a copy operation
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对复制操作进行掩码处理
- en: As part of the previous chapter's work, we wrote `copyRect()` as a copy operation
    that limits itself to the given rectangles of a source and destination image.
    Now, we want to apply further limits to this copy operation. We want to use a
    given mask that has the same dimensions as the source rectangle.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 作为前一章工作的部分，我们将 `copyRect()` 编写为一个复制操作，该操作限制自己仅限于源和目标图像的给定矩形。现在，我们想要进一步限制这个复制操作。我们想要使用一个与源矩形具有相同尺寸的给定掩码。
- en: We shall copy only those pixels in the source rectangle where the mask's value
    is not zero. Other pixels shall retain their old values from the destination image.
    This logic, with an array of conditions and two arrays of possible output values,
    can be expressed concisely with the `numpy.where()` function that we have recently
    learned.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只复制源矩形中掩码值为非零的像素。其他像素将保留其从目标图像中的旧值。这种逻辑，使用条件数组以及两个可能的输出值数组，可以用我们最近学习的 `numpy.where()`
    函数简洁地表达。
- en: Let's open `rects.py` and edit `copyRect()` to add a new mask argument. This
    argument may be `None`, in which case, we fall back to our old implementation
    of the copy operation. Otherwise, we next ensure that mask and the images have
    the same number of channels. We assume that mask has one channel but the images
    may have three channels (BGR). We can add duplicate channels to mask using the
    `repeat()` and `reshape()` methods of `numpy.array`.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打开 `rects.py` 并编辑 `copyRect()` 以添加一个新的掩码参数。这个参数可能是 `None`，在这种情况下，我们将回退到我们旧的复制操作实现。否则，我们接下来确保掩码和图像具有相同数量的通道。我们假设掩码有一个通道，但图像可能有三个通道（BGR）。我们可以使用
    `numpy.array` 的 `repeat()` 和 `reshape()` 方法向掩码添加重复的通道。
- en: 'Finally, we perform the copy operation using `where()`. The complete implementation
    is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `where()` 执行复制操作。完整的实现如下：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We also need to modify our `swapRects()` function, which uses `copyRect()` to
    perform a circular swap of a list of rectangular regions. The modifications to
    `swapRects()` are quite simple. We just need to add a new `masks` argument, which
    is a list of masks whose elements are passed to the respective `copyRect()` calls.
    If the value of the given `masks` argument is `None`, we pass `None` to every
    `copyRect()` call.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要修改我们的 `swapRects()` 函数，该函数使用 `copyRect()` 来执行一系列矩形区域的环形交换。对 `swapRects()`
    的修改非常简单。我们只需要添加一个新的 `masks` 参数，它是一个包含掩码的列表，这些掩码的元素被传递到相应的 `copyRect()` 调用中。如果给定的
    `masks` 参数值为 `None`，我们将 `None` 传递给每个 `copyRect()` 调用。
- en: 'The following code shows you the full implementation of this:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了这一实现的完整内容：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that the `masks` argument in `copyRect()` and `swapRects()` both default
    to `None`. Thus, our new versions of these functions are backward compatible with
    our previous versions of Cameo.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`copyRect()` 和 `swapRects()` 中的 `masks` 参数默认为 `None`。因此，我们这些函数的新版本与我们的 Cameo
    旧版本向后兼容。
- en: Depth estimation with a normal camera
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用普通相机进行深度估计
- en: A depth camera is a fantastic little device to capture images and estimate the
    distance of objects from the camera itself, but, how does the depth camera retrieve
    depth information? Also, is it possible to reproduce the same kind of calculations
    with a normal camera?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 深度相机是一种捕捉图像并估计物体与相机之间距离的神奇小设备，但是，深度相机是如何检索深度信息的？此外，是否可以使用普通相机重现同样的计算？
- en: A depth camera, such as Microsoft Kinect, uses a traditional camera combined
    with an infrared sensor that helps the camera differentiate similar objects and
    calculate their distance from the camera. However, not everybody has access to
    a depth camera or a Kinect, and especially when you're just learning OpenCV, you're
    probably not going to invest in an expensive piece of equipment until you feel
    your skills are well-sharpened, and your interest in the subject is confirmed.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度相机，如Microsoft Kinect，使用一个传统的相机结合一个红外传感器，这有助于相机区分相似的对象并计算它们与相机的距离。然而，并不是每个人都能接触到深度相机或Kinect，尤其是在你刚开始学习OpenCV时，你可能不会投资昂贵的设备，直到你觉得自己技能已经磨炼得很好，你对这个主题的兴趣也得到了确认。
- en: Our setup includes a simple camera, which is most likely integrated in our machine,
    or a webcam attached to our computer. So, we need to resort to less fancy means
    of estimating the difference in distance of objects from the camera.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的设置包括一个简单的相机，这很可能是集成在我们的机器中，或者是一个连接到我们电脑的摄像头。因此，我们需要求助于不那么花哨的方法来估计物体与相机之间的距离差异。
- en: Geometry will come to the rescue in this case, and in particular, Epipolar Geometry,
    which is the geometry of stereo vision. Stereo vision is a branch of computer
    vision that extracts three-dimensional information out of two different images
    of the same subject.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，几何学将提供帮助，特别是极线几何，它是立体视觉的几何学。立体视觉是计算机视觉的一个分支，它从同一主题的两个不同图像中提取三维信息。
- en: 'How does epipolar geometry work? Conceptually, it traces imaginary lines from
    the camera to each object in the image, then does the same on the second image,
    and calculates the distance of objects based on the intersection of the lines
    corresponding to the same object. Here is a representation of this concept:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 极线几何是如何工作的呢？从概念上讲，它从相机向图像中的每个对象绘制想象中的线条，然后在第二张图像上做同样的事情，并基于对应对象的线条交点来计算物体的距离。以下是这个概念的一个表示：
- en: '![Depth estimation with a normal camera](img/image00203.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![使用普通相机进行深度估计](img/image00203.jpeg)'
- en: Let's see how OpenCV applies epipolar geometry to calculate a so-called disparity
    map, which is basically a representation of the different depths detected in the
    images. This will enable us to extract the foreground of a picture and discard
    the rest.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看OpenCV是如何应用极线几何来计算所谓的视差图，这基本上是图像中检测到的不同深度的表示。这将使我们能够提取图片的前景并丢弃其余部分。
- en: Firstly, we need two images of the same subject taken from different points
    of view, but paying attention to the fact that the pictures are taken at an equal
    distance from the object, otherwise the calculations will fail and the disparity
    map will be meaningless.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要从不同的视角拍摄同一主题的两个图像，但要注意，图片是从与物体等距离的位置拍摄的，否则计算将失败，视差图将没有意义。
- en: 'So, moving on to an example:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们继续一个例子：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this example, we take two images of the same subject and calculate a disparity
    map, showing in brighter colors the points in the map that are closer to the camera.
    The areas marked in black represent the disparities.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们取同一主题的两个图像，并计算一个视差图，用较亮的颜色显示地图中靠近相机的点。用黑色标记的区域代表视差。
- en: First of all, we import `numpy` and `cv2` as usual.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们像往常一样导入`numpy`和`cv2`。
- en: 'Let''s skip the definition of the `update` function for a second and take a
    look at the main code; the process is quite simple: load two images, create a
    `StereoSGBM` instance (`StereoSGBM` stands for **semiglobal block matching**,
    and it is an algorithm used for computing disparity maps), and also create a few
    trackbars to play around with the parameters of the algorithm and call the `update`
    function.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先暂时跳过`update`函数的定义，看看主要代码；这个过程相当简单：加载两个图像，创建一个`StereoSGBM`实例（`StereoSGBM`代表**半全局块匹配**，这是一种用于计算视差图的算法），并创建一些滑块来调整算法的参数，然后调用`update`函数。
- en: 'The `update` function applies the trackbar values to the `StereoSGBM` instance,
    and then calls the `compute` method, which produces a disparity map. All in all,
    pretty simple! Here is the first image I''ve used:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '`update`函数将滑块值应用于`StereoSGBM`实例，然后调用`compute`方法，该方法生成视差图。总的来说，相当简单！以下是第一个我使用的图像：'
- en: '![Depth estimation with a normal camera](img/image00204.jpeg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![使用普通相机进行深度估计](img/image00204.jpeg)'
- en: 'This is the second one:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是第二个：
- en: '![Depth estimation with a normal camera](img/image00205.jpeg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![使用普通相机进行深度估计](img/image00205.jpeg)'
- en: 'There you go: a nice and quite easy to interpret disparity map.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 你看：这是一个既好又容易解释的视差图。
- en: '![Depth estimation with a normal camera](img/image00206.jpeg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![使用普通相机进行深度估计](img/image00206.jpeg)'
- en: 'The parameters used by `StereoSGBM` are as follows (taken from the OpenCV documentation):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`StereoSGBM`使用的参数如下（摘自OpenCV文档）：'
- en: '| Parameter | Description |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `minDisparity` | This parameter refers to the minimum possible disparity
    value. Normally, it is zero but sometimes, rectification algorithms can shift
    images, so this parameter needs to be adjusted accordingly. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `minDisparity` | 此参数表示可能的最小视差值。通常为零，但有时校正算法可以移动图像，因此需要相应地调整此参数。 |'
- en: '| `numDisparities` | This parameter refers to the maximum disparity minus minimum
    disparity. The resultant value is always greater than zero. In the current implementation,
    this parameter must be divisible by 16. |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `numDisparities` | 此参数表示最大视差减去最小视差。结果值始终大于零。在当前实现中，此参数必须是16的倍数。 |'
- en: '| `windowSize` | This parameter refers to a matched block size. It must be
    an odd number greater than or equal to 1\. Normally, it should be somewhere in
    the 3-11 range. |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `windowSize` | 此参数表示匹配块的大小。它必须是一个大于或等于1的奇数。通常，它应该在3-11的范围内。 |'
- en: '| `P1` | This parameter refers to the first parameter controlling the disparity
    smoothness. See the next point. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `P1` | 此参数表示控制视差平滑度的第一个参数。参见下一点。 |'
- en: '| `P2` | This parameter refers to the second parameter that controls the disparity
    smoothness. The larger the values are, the smoother the disparity is. `P1` is
    the penalty on the disparity change by plus or minus 1 between neighbor pixels.
    `P2` is the penalty on the disparity change by more than 1 between neighbor pixels.
    The algorithm requires `P2 > P1`.See the `stereo_match.cpp` sample where some
    reasonably good `P1` and `P2` values are shown (such as `8*number_of_image_channels*windowSize*windowSize`
    and `32*number_of_image_channels*windowSize*windowSize`, respectively). |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `P2` | 此参数表示控制视差平滑度的第二个参数。值越大，视差越平滑。`P1`是相邻像素之间视差变化加减1的惩罚。`P2`是相邻像素之间视差变化超过1的惩罚。算法要求`P2
    > P1`。请参见`stereo_match.cpp`示例，其中显示了某些合理的`P1`和`P2`值（例如`8*number_of_image_channels*windowSize*windowSize`和`32*number_of_image_channels*windowSize*windowSize`，分别）。
    |'
- en: '| `disp12MaxDiff` | This parameter refers to the maximum allowed difference
    (in integer pixel units) in the left-right disparity check. Set it to a nonpositive
    value to disable the check. |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `disp12MaxDiff` | 此参数表示左右视差检查允许的最大差异（以整数像素为单位）。将其设置为非正值以禁用检查。 |'
- en: '| `preFilterCap` | This parameter refers to the truncation value for prefiltered
    image pixels. The algorithm first computes the x-derivative at each pixel and
    clips its value by the `[-preFilterCap, preFilterCap]` interval. The resultant
    values are passed to the Birchfield-Tomasi pixel cost function. |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '| `preFilterCap` | 此参数表示预滤波图像像素的截断值。算法首先计算每个像素的x导数，并通过`[-preFilterCap, preFilterCap]`区间剪辑其值。结果值传递给Birchfield-Tomasi像素成本函数。
    |'
- en: '| `uniquenessRatio` | This parameter refers to the margin in percentage by
    which the best (minimum) computed cost function value should "win" the second
    best value to consider the found match to be correct. Normally, a value within
    the 5-15 range is good enough. |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| `uniquenessRatio` | 此参数表示最佳（最小）计算成本函数值相对于第二最佳值应“获胜”的百分比边缘。通常，5-15范围内的值就足够好了。
    |'
- en: '| `speckleWindowSize` | This parameter refers to the maximum size of smooth
    disparity regions to consider their noise speckles and invalidate. Set it to `0`
    to disable speckle filtering. Otherwise, set it somewhere in the 50-200 range.
    |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| `speckleWindowSize` | 此参数表示考虑其噪声斑点和无效化的平滑视差区域的最大大小。将其设置为`0`以禁用斑点滤波。否则，将其设置为50-200范围内的某个值。
    |'
- en: '| `speckleRange` | This parameter refers to the maximum disparity variation
    within each connected component. If you do speckle filtering, set the parameter
    to a positive value; it will implicitly be multiplied by 16\. Normally, 1 or 2
    is good enough. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `speckleRange` | 此参数指代每个连通组件内的最大视差变化。如果你进行斑点滤波，将参数设置为正值；它将隐式地乘以16。通常，1或2就足够了。|'
- en: With the preceding script, you'll be able to load the images and play around
    with parameters until you're happy with the disparity map generated by `StereoSGBM`.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的脚本，你可以加载图像并调整参数，直到你对`StereoSGBM`生成的视差图满意为止。
- en: Object segmentation using the Watershed and GrabCut algorithms
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Watershed和GrabCut算法进行对象分割
- en: 'Calculating a disparity map can be very useful to detect the foreground of
    an image, but `StereoSGBM` is not the only algorithm available to accomplish this,
    and in fact, `StereoSGBM` is more about gathering 3D information from 2D pictures,
    than anything else. **GrabCut**, however, is a perfect tool for this purpose.
    The GrabCut algorithm follows a precise sequence of steps:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 计算视差图对于检测图像的前景非常有用，但`StereoSGBM`并非完成此任务的唯一算法，实际上，`StereoSGBM`更多的是从二维图片中收集三维信息，而不是其他。然而，**GrabCut**是完成此目的的完美工具。GrabCut算法遵循一系列精确的步骤：
- en: A rectangle including the subject(s) of the picture is defined.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个包含图片主题（s）的矩形。
- en: The area lying outside the rectangle is automatically defined as a background.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 位于矩形外部区域自动定义为背景。
- en: The data contained in the background is used as a reference to distinguish background
    areas from foreground areas within the user-defined rectangle.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 背景中的数据用作参考，以区分用户定义矩形内的背景区域和前景区域。
- en: A **Gaussians Mixture Model** (**GMM**) models the foreground and background,
    and labels undefined pixels as probable background and foregrounds.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（**GMM**）对前景和背景进行建模，并将未定义的像素标记为可能的背景和前景。'
- en: Each pixel in the image is virtually connected to the surrounding pixels through
    virtual edges, and each edge gets a probability of being foreground or background,
    based on how similar it is in color to the pixels surrounding it.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 图像中的每个像素通过虚拟边与周围的像素虚拟连接，每个边根据其与周围像素在颜色上的相似性获得成为前景或背景的概率。
- en: Each pixel (or node as it is conceptualized in the algorithm) is connected to
    either a foreground or a background node, which you can picture looking like this:![Object
    segmentation using the Watershed and GrabCut algorithms](img/image00207.jpeg)
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个像素（或算法中概念化的节点）连接到前景节点或背景节点，你可以想象成这样：![使用Watershed和GrabCut算法进行对象分割](img/image00207.jpeg)
- en: After the nodes have been connected to either terminal (background or foreground,
    also called a source and sink), the edges between nodes belonging to different
    terminals are cut (the famous cut part of the algorithm), which enables the separation
    of the parts of the image. This graph adequately represents the algorithm:![Object
    segmentation using the Watershed and GrabCut algorithms](img/image00208.jpeg)
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在节点连接到任一终端（背景或前景，也称为源和汇）之后，属于不同终端的节点之间的边被切断（算法中著名的切割部分），这使得图像部分的分离成为可能。此图充分代表了该算法：![使用Watershed和GrabCut算法进行对象分割](img/image00208.jpeg)
- en: Example of foreground detection with GrabCut
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GrabCut进行前景检测的示例
- en: Let's look at an example. We start with the picture of a beautiful statue of
    an angel.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个例子。我们从一个美丽的天使雕像的图片开始。
- en: '![Example of foreground detection with GrabCut](img/image00209.jpeg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![使用GrabCut进行前景检测的示例](img/image00209.jpeg)'
- en: 'We want to grab our angel and discard the background. To do this, we will create
    a relatively short script that will instantiate GrabCut, operate the separation,
    and then display the resulting image side by side to the original. We will do
    this using `matplotlib`, a very useful Python library, which makes displaying
    charts and images a trivial task:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要抓住我们的天使并丢弃背景。为此，我们将创建一个相对简短的脚本，该脚本将实例化GrabCut，执行分离，然后将生成的图像与原始图像并排显示。我们将使用`matplotlib`，这是一个非常有用的Python库，它使得显示图表和图像变得非常简单：
- en: '[PRE8]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This code is actually quite straightforward. Firstly, we load the image we
    want to process, and then we create a mask populated with zeros with the same
    shape as the image we''ve loaded:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码实际上非常直接。首先，我们加载我们想要处理的图像，然后我们创建一个与加载的图像形状相同的零填充掩码：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We then create zero-filled foreground and background models:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建零填充的前景和背景模型：
- en: '[PRE10]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We could have populated these models with data, but we''re going to initialize
    the GrabCut algorithm with a rectangle identifying the subject we want to isolate.
    So, background and foreground models are going to be determined based on the areas
    left out of the initial rectangle. This rectangle is defined in the next line:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用数据填充这些模型，但我们将使用一个矩形来初始化GrabCut算法，以识别我们想要隔离的主题。因此，背景和前景模型将基于初始矩形之外的区域来确定。这个矩形在下一行定义：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now to the interesting part! We run the GrabCut algorithm specifying the empty
    models and mask, and the fact that we''re going to use a rectangle to initialize
    the operation:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来到有趣的部分！我们运行GrabCut算法，指定空模型和掩码，以及我们将使用矩形来初始化操作：
- en: '[PRE12]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You'll also notice an integer after `fgdModel`, which is the number of iterations
    the algorithm is going to run on the image. You can increase these, but there
    is a point in which pixel classifications will converge, and effectively, you'll
    just be adding iterations without obtaining any more improvements.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 你也会注意到`fgdModel`后面有一个整数，这是算法将在图像上运行的迭代次数。你可以增加这些迭代次数，但像素分类会收敛到一个点，实际上，你只是在增加迭代次数而没有获得任何更多的改进。
- en: 'After this, our mask will have changed to contain values between 0 and 3\.
    The values, `0` and `2`, will be converted into zeros, and 1-3 into ones, and
    stored into `mask2`, which we can then use to filter out all zero-value pixels
    (theoretically leaving all foreground pixels intact):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们的掩码将改变，包含介于0和3之间的值。值`0`和`2`将被转换为零，1-3将被转换为1，并存储到`mask2`中，然后我们可以使用它来过滤掉所有零值像素（理论上留下所有前景像素）：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The last part of the code displays the images side by side, and here''s the
    result:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的最后部分显示了并排的图像，这是结果：
- en: '![Example of foreground detection with GrabCut](img/image00210.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![使用GrabCut进行前景检测的示例](img/image00210.jpeg)'
- en: This is quite a satisfactory result. You'll notice that an area of background
    is left under the angel's arm. It is possible to apply touch strokes to apply
    more iterations; the technique is quite well illustrated in the `grabcut.py` file
    in `samples/python2` of your OpenCV installation.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个相当令人满意的结果。你会注意到天使的胳膊下留下了一块背景区域。可以通过触摸笔触来应用更多迭代；这种技术在`samples/python2`目录下的`grabcut.py`文件中有很好的说明。
- en: Image segmentation with the Watershed algorithm
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Watershed算法进行图像分割
- en: Finally, we take a quick look at the Watershed algorithm. The algorithm is called
    Watershed, because its conceptualization involves water. Imagine areas with low
    density (little to no change) in an image as valleys, and areas with high density
    (lots of change) as peaks. Start filling the valleys with water to the point where
    water from two different valleys is about to merge. To prevent the merging of
    water from different valleys, you build a barrier to keep them separated. The
    resulting barrier is the image segmentation.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们简要了解一下Watershed算法。该算法被称为Watershed，因为其概念化涉及水。想象一下图像中低密度（几乎没有变化）的区域为山谷，高密度（变化很多）的区域为山峰。开始往山谷中注水，直到两个不同山谷的水即将汇合。为了防止不同山谷的水汇合，你建立一道屏障来保持它们分离。形成的屏障就是图像分割。
- en: 'As an Italian, I love food, and one of the things I love the most is a good
    plate of pasta with a pesto sauce. So here''s a picture of the most vital ingredient
    for a pesto, basil:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名意大利人，我喜欢食物，我最喜欢的东西之一就是一份美味的意面配以香蒜酱。所以，这是香蒜酱最重要的成分罗勒的图片：
- en: '![Image segmentation with the Watershed algorithm](img/image00211.jpeg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![使用Watershed算法进行图像分割](img/image00211.jpeg)'
- en: Now, we want to segment the image to separate the basil leaves from the white
    background.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要分割图像，将罗勒叶从白色背景中分离出来。
- en: 'Once more, we import `numpy`, `cv2`, and `matplotlib`, and then import our
    basil leaves'' image:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们导入`numpy`、`cv2`和`matplotlib`，然后导入我们的罗勒叶图像：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'After changing the color to grayscale, we run a threshold on the image. This
    operation helps dividing the image in two, blacks and whites:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在将颜色转换为灰度后，我们对图像进行阈值处理。这个操作有助于将图像分为黑白两部分：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next up, we remove noise from the image by applying the `morphologyEx` transformation,
    an operation that consists of dilating and then eroding an image to extract features:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过应用`morphologyEx`变换来从图像中去除噪声，这是一个由膨胀和侵蚀图像以提取特征的操作：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'By dilating the result of the `morphology` transformation, we can obtain areas
    of the image that are most certainly background:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通过膨胀`morphology`变换的结果，我们可以获得图像中几乎肯定是背景的区域：
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Conversely, we can obtain sure foreground areas by applying `distanceTransform`.
    In practical terms, of all the areas most likely to be foreground, the farther
    away from the "border" with the background a point is, the higher the chance it
    is foreground. Once we''ve obtained the `distanceTransform` representation of
    the image, we apply a threshold to determine with a highly mathematical probability
    whether the areas are foreground:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以通过应用`distanceTransform`来获得确切的背景区域。在实践中，在所有最可能成为前景的区域中，一个点离背景“边界”越远，它成为前景的可能性就越高。一旦我们获得了图像的`distanceTransform`表示，我们就应用一个阈值，以高度数学的概率确定这些区域是否为前景：
- en: '[PRE18]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'At this stage, we have some `sure` foregrounds and backgrounds. Now, what about
    the areas in between? First of all, we need to determine these regions, which
    can be done by subtracting the `sure` foreground from the background:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们有一些确切的背景和前景。那么，中间区域怎么办？首先，我们需要确定这些区域，这可以通过从背景中减去确切的背景来完成：
- en: '[PRE19]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now that we have these areas, we can build our famous "barriers" to stop the
    water from merging. This is done with the `connectedComponents` function. We took
    a glimpse at the graph theory when we analyzed the GrabCut algorithm, and conceptualized
    an image as a set of nodes that are connected by edges. Given the sure foreground
    areas, some of these nodes will be connected together, but some won''t. This means
    that they belong to different water valleys, and there should be a barrier between
    them:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这些区域，我们可以构建我们著名的“障碍物”来阻止水合并。这是通过`connectedComponents`函数完成的。当我们分析GrabCut算法时，我们瞥了一眼图论，并将图像视为由边连接的节点集合。给定确切的背景区域，这些节点中的一些将连接在一起，但也有一些不会。这意味着它们属于不同的水谷，它们之间应该有一个障碍物：
- en: '[PRE20]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now we add 1 to the background areas because we only want unknowns to stay
    at `0`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将背景区域的值加1，因为我们只想让未知区域保持在`0`：
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we open the gates! Let the water fall and our barriers be drawn in
    red:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打开大门！让水落下，我们的障碍物用红色绘制：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Now, let''s show the result:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们展示结果：
- en: '![Image segmentation with the Watershed algorithm](img/image00212.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![使用Watershed算法进行图像分割](img/image00212.jpeg)'
- en: Needless to say, I am now hungry!
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 不言而喻，我现在很饿！
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we learned about gathering three-dimensional information from
    bi-dimensional input (a video frame or an image). Firstly, we examined depth cameras,
    and then epipolar geometry and stereo images, so we are now able to calculate
    disparity maps. Finally, we looked at image segmentation with two of the most
    popular methods: GrabCut and Watershed.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何从二维输入（视频帧或图像）中收集三维信息。首先，我们考察了深度相机，然后是极线几何和立体图像，因此我们现在能够计算视差图。最后，我们探讨了两种最流行的图像分割方法：GrabCut和Watershed。
- en: 'This chapter has introduced us to the world of interpreting information provided
    by images and we are now ready to explore another important feature of OpenCV:
    feature descriptors and keypoint detection.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 本章带我们进入了从图像中解释信息的世界，我们现在准备好探索OpenCV的另一个重要特性：特征描述符和关键点检测。
