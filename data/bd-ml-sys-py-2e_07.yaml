- en: Chapter 7. Regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章 回归
- en: You probably learned about regression in your high school mathematics class.
    The specific method you learned was probably what is called **ordinary least squares**
    (**OLS**) regression. This 200-year-old technique is computationally fast and
    can be used for many real-world problems. This chapter will start by reviewing
    it and showing you how it is available in scikit-learn.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在高中数学课上学过回归分析。你学到的具体方法可能就是所谓的**普通最小二乘**（**OLS**）回归。这个已有 200 年历史的技术计算速度很快，并且可以应用于许多现实世界的问题。本章将首先回顾这一方法，并展示它是如何在
    scikit-learn 中实现的。
- en: For some problems, however, this method is insufficient. This is particularly
    true when we have many features, and it completely fails when we have more features
    than datapoints. For those cases, we need more advanced methods. These methods
    are very modern, with major developments happening in the last decade. They go
    by names such as Lasso, Ridge, or ElasticNets. We will go into these in detail.
    They are also available in scikit-learn.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于某些问题，这种方法是不够的。特别是当我们拥有许多特征时，这种方法就显得不够用了，尤其是在特征数比数据点数更多的情况下，这种方法完全无法工作。在这些情况下，我们需要更先进的方法。这些方法非常现代，过去十年取得了重大进展，名字如
    Lasso、Ridge 或 ElasticNets。我们将详细介绍这些方法，它们也可以在 scikit-learn 中使用。
- en: Predicting house prices with regression
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用回归预测房价
- en: Let's start with a simple problem, predicting house prices in Boston; a problem
    for which we can use a publicly available dataset. We are given several demographic
    and geographical attributes, such as the crime rate or the pupil-teacher ratio
    in the neighborhood. The goal is to predict the median value of a house in a particular
    area. As usual, we have some training data, where the answer is known to us.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的问题开始——预测波士顿的房价；这是一个可以使用公开数据集的问题。我们得到了一些人口统计学和地理信息，如犯罪率或邻里的师生比例。目标是预测某个地区房屋的中位数价值。像往常一样，我们有一些训练数据，答案是已知的。
- en: 'This is one of the built-in datasets that scikit-learn comes with, so it is
    very easy to load the data into memory:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 scikit-learn 自带的内置数据集之一，因此非常容易将数据加载到内存中：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `boston` object contains several attributes; in particular, `boston.data`
    contains the input data and `boston.target` contains the price of houses.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '`boston` 对象包含多个属性；特别地，`boston.data` 包含输入数据，`boston.target` 包含房价。'
- en: 'We will start with a simple one-dimensional regression, trying to regress the
    price on a single attribute, the average number of rooms per dwelling in the neighborhood,
    which is stored at position `5` (you can consult `boston.DESCR` and `boston.feature_names`
    for detailed information on the data):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的一维回归开始，尝试用一个特征来回归房价，这个特征是每个住宅区域的平均房间数，存储在位置 `5`（你可以查阅 `boston.DESCR`
    和 `boston.feature_names` 以获取数据的详细信息）：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The `boston.target` attribute contains the average house price (our target
    variable). We can use the standard least squares regression you probably first
    saw in high-school. Our first attempt looks like this:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`boston.target` 属性包含了平均房价（我们的目标变量）。我们可以使用你可能在高中时学到的标准最小二乘回归。我们第一次尝试的代码如下：'
- en: '[PRE2]'
  id: totrans-11
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We import `LinearRegression` from the `sklearn.linear_model` module and construct
    a `LinearRegression` object. This object will behave analogously to the classifier
    objects from scikit-learn that we used earlier.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 `sklearn.linear_model` 模块导入 `LinearRegression` 并构造一个 `LinearRegression`
    对象。这个对象的行为与我们之前使用的 scikit-learn 分类器对象类似。
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The only nonobvious line in this code block is the call to `np.atleast_2d`,
    which converts `x` from a one-dimensional to a two-dimensional array. This conversion
    is necessary as the `fit` method expects a two-dimensional array as its first
    argument. Finally, for the dimensions to work out correctly, we need to transpose
    this array.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中唯一不显而易见的行是对 `np.atleast_2d` 的调用，它将 `x` 从一维数组转换为二维数组。这个转换是必要的，因为 `fit` 方法期望其第一个参数是一个二维数组。最后，为了确保维度正确，我们需要对这个数组进行转置。
- en: Note that we are calling methods named `fit` and predict on the `LinearRegression`
    object, just as we did with classifier objects, even though we are now performing
    regression. This regularity in the API is one of the nicer features of scikit-learn.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在 `LinearRegression` 对象上调用了名为 `fit` 和 `predict` 的方法，就像之前使用分类器对象时一样，尽管现在我们执行的是回归操作。这种
    API 的一致性是 scikit-learn 的一个优点。
- en: '![Predicting house prices with regression](img/2772OS_07_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测房价](img/2772OS_07_02.jpg)'
- en: The preceding graph shows all the points (as dots) and our fit (the solid line).
    We can see that visually it looks good, except for a few outliers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了所有的点（以点表示）和我们的拟合曲线（实线）。我们可以看到，视觉效果很好，除了少数几个离群点。
- en: 'Ideally, though, we would like to measure how good of a fit this is quantitatively.
    This will be critical in order to be able to compare alternative methods. To do
    so, we can measure how close our prediction is to the true values. For this task,
    we can use the `mean_squared_error` function from the `sklearn.metrics` module:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从理想角度来看，我们希望定量衡量拟合的好坏。这对于能够比较不同方法非常关键。为此，我们可以测量预测值与真实值之间的接近程度。为此，我们可以使用`sklearn.metrics`模块中的`mean_squared_error`函数：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This function takes two arguments, the true value and the predictions, as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数有两个参数，真实值和预测值，如下所示：
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This value can sometimes be hard to interpret, and it''s better to take the
    square root, to obtain the **root mean square error** (**RMSE**):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值有时可能很难解读，最好取其平方根，得到**均方根误差**（**RMSE**）：
- en: '[PRE6]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: One advantage of using RMSE is that we can quickly obtain a very rough estimate
    of the error by multiplying it by two. In our case, we can expect the estimated
    price to be different from the real price by, at most, 13 thousand dollars.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RMSE的一个优势是，我们可以通过将其乘以二，快速获得误差的粗略估计。在我们的案例中，我们可以预计估算的价格与实际价格之间的差异最多为1.3万美元。
- en: Tip
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Root mean squared error and prediction**'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**均方根误差与预测**'
- en: Root mean squared error corresponds approximately to an estimate of the standard
    deviation. Since most data is at most two standard deviations from the mean, we
    can double our RMSE to obtain a rough confident interval. This is only completely
    valid if the errors are normally distributed, but it is often roughly correct
    even if they are not.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差大致对应于标准差的估计。由于大多数数据距离均值最多两个标准差，因此我们可以将RMSE翻倍来获得一个粗略的置信区间。如果误差符合正态分布，这种方法完全有效，但即使误差不是正态分布，它通常也大致正确。
- en: 'A number such as 6.6 is still hard to immediately intuit. Is this a good prediction?
    One possible way to answer this question is to compare it with the most simple
    baseline, the constant model. If we knew nothing of the input, the best we could
    do is predict that the output will always be the average value of `y`. We can
    then compare the mean-squared error of this model with the mean-squared error
    of the null model. This idea is formalized in the **coefficient of determination**,
    which is defined as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 像6.6这样的数字仍然很难直观理解。这是一个好的预测吗？回答这个问题的一种可能方法是将其与最简单的基准模型——常数模型进行比较。如果我们对输入一无所知，最好的方法就是预测输出始终是`y`的平均值。然后，我们可以将这个模型的均方误差与零模型的均方误差进行比较。这个思想在**决定系数**中得到了形式化定义，其计算公式如下：
- en: '![Predicting house prices with regression](img/2772OS_07_09.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![使用回归预测房价](img/2772OS_07_09.jpg)'
- en: In this formula, *y[i]* represents the value of the element with index *i*,
    while ![Predicting house prices with regression](img/2772OS_07_19.jpg) is the
    estimate for the same element obtained by the regression model. Finally, ![Predicting
    house prices with regression](img/2772OS_07_20.jpg) is the mean value of *y*,
    which represents the *null model* that always returns the same value. This is
    roughly the same as first computing the ratio of the mean squared error with the
    variance of the output and, finally, considering one minus this ratio. This way,
    a perfect model obtains a score of one, while the null model obtains a score of
    zero. Note that it is possible to obtain a negative score, which means that the
    model is so poor that one is better off using the mean as a prediction.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*y[i]*表示索引为*i*的元素的值，而![使用回归预测房价](img/2772OS_07_19.jpg)是回归模型为同一元素提供的估计值。最后，![使用回归预测房价](img/2772OS_07_20.jpg)是*y*的均值，代表着*零模型*，即始终返回相同值的模型。这个公式大致等同于首先计算均方误差与输出方差的比率，最后计算1减去这个比率。这样，完美的模型得分为1，而零模型得分为0。请注意，可能会得到负分数，这意味着模型非常糟糕，以至于使用均值作为预测值更好。
- en: 'The coefficient of determination can be obtained using `r2_score` of the `sklearn.metrics`
    module:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 决定系数可以使用`sklearn.metrics`模块中的`r2_score`来获得：
- en: '[PRE7]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This measure is also called the R² score. If you are using linear regression
    and evaluating the error on the training data, then it does correspond to the
    square of the correlation coefficient, R. However, this measure is more general,
    and as we discussed, may even return a negative value.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标也叫做R²得分。如果你使用线性回归并在训练数据上评估误差，那么它确实对应于相关系数R的平方。然而，这个指标更为通用，正如我们讨论的那样，可能会返回一个负值。
- en: 'An alternative way to compute the coefficient of determination is to use the
    `score` method of the `LinearRegression` object:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 计算决定系数的另一种方法是使用`LinearRegression`对象的`score`方法：
- en: '[PRE8]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Multidimensional regression
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多维回归
- en: So far, we have only used a single variable for prediction, the number of rooms
    per dwelling. We will now use all the data we have to fit a model, using multidimensional
    regression. We now try to predict a single output (the average house price) based
    on multiple inputs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用了一个变量进行预测，即每个住宅的房间数。接下来，我们将使用所有数据来拟合模型，使用多维回归。我们现在尝试基于多个输入预测一个单一的输出（平均房价）。
- en: 'The code looks very much like before. In fact, it''s even simpler as we can
    now pass the value of `boston.data` directly to the `fit` method:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 代码看起来和之前非常相似。实际上，它现在更简单了，因为我们可以直接将`boston.data`的值传递给`fit`方法：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using all the input variables, the root mean squared error is only 4.7, which
    corresponds to a coefficient of determination of 0.74\. This is better than what
    we had before, which indicates that the extra variables did help. We can no longer
    easily display the regression line as we did, because we have a 14-dimensional
    regression hyperplane instead of a single line.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有输入变量，均方根误差仅为4.7，对应的决定系数为0.74。这比我们之前的结果要好，说明额外的变量确实有帮助。我们不再能像之前那样轻松显示回归线，因为我们有一个14维的回归超平面，而不是一条单独的直线。
- en: 'We can, however, plot the prediction versus the actual value. The code is as
    follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以绘制预测值与实际值的对比图。代码如下：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The last line plots a diagonal line that corresponds to perfect agreement.
    This aids with visualization. The results are shown in the following plot, where
    the solid line shows the diagonal (where all the points would lie if there was
    perfect agreement between the prediction and the underlying value):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一行绘制了一条对角线，表示完美一致的情况。这有助于可视化。结果如下面的图所示，其中实线表示对角线（如果预测与真实值完全一致，所有点都会落在这条线上）：
- en: '![Multidimensional regression](img/2772OS_07_04.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![多维回归](img/2772OS_07_04.jpg)'
- en: Cross-validation for regression
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归问题的交叉验证
- en: If you remember when we first introduced classification, we stressed the importance
    of cross-validation for checking the quality of our predictions. In regression,
    this is not always done. In fact, we discussed only the training error in this
    chapter so far. This is a mistake if you want to confidently infer the generalization
    ability. Since ordinary least squares is a very simple model, this is often not
    a very serious mistake. In other words, the amount of overfitting is slight. However,
    we should still test this empirically, which we can easily do with scikit-learn.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得我们首次介绍分类时，强调了交叉验证在检验预测质量中的重要性。在回归中，这并不总是做的。事实上，到目前为止，我们在本章中只讨论了训练误差。如果你想自信地推断模型的泛化能力，这是一个错误。因为普通最小二乘法是一个非常简单的模型，这通常不是一个非常严重的错误。换句话说，过拟合的程度较轻。然而，我们仍然应该通过经验来测试这一点，这可以通过scikit-learn轻松实现。
- en: 'We will use the `Kfold` class to build a 5 fold cross-validation loop and test
    the generalization ability of linear regression:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`Kfold`类来构建一个5折交叉验证循环，测试线性回归的泛化能力：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With cross-validation, we obtain a more conservative estimate (that is, the
    error is larger): `5.6`. As in the case of classification, the cross-validation
    estimate is a better estimate of how well we could generalize to predict on unseen
    data.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用交叉验证，我们得到一个更为保守的估计（即，误差更大）：`5.6`。与分类问题一样，交叉验证的估计是我们如何将模型泛化到未见数据的更好估计。
- en: Ordinary least squares is fast at learning time and returns a simple model,
    which is fast at prediction time. For these reasons, it should often be the first
    model that you try in a regression problem. However, we are now going to see more
    advanced methods and why they are sometimes preferable.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 普通最小二乘法在学习阶段很快，并且返回一个简单的模型，在预测时也非常快。因此，它通常应该是回归问题中你首先尝试的模型。然而，我们现在将看到更先进的方法，并了解为什么有时它们更为优越。
- en: Penalized or regularized regression
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 惩罚或正则化回归
- en: This section introduces penalized regression, also called **regularized regression**,
    an important class of regression models.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了惩罚回归，也叫做**正则化回归**，它是回归模型的一个重要类别。
- en: In ordinary regression, the returned fit is the best fit on the training data.
    This can lead to over-fitting. Penalizing means that we add a penalty for over-confidence
    in the parameter values. Thus, we accept a slightly worse fit in order to have
    a simpler model.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在普通回归中，返回的拟合结果是训练数据上的最佳拟合。这可能导致过拟合。惩罚意味着我们为参数值的过度自信添加惩罚。因此，我们接受稍差的拟合，以便拥有一个更简单的模型。
- en: Another way to think about it is to consider that the default is that there
    is no relationship between the input variables and the output prediction. When
    we have data, we change this opinion, but adding a penalty means that we require
    more data to convince us that this is a strong relationship.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考方式是，将默认设定为输入变量与输出预测之间没有关系。当我们拥有数据时，我们会改变这一观点，但添加惩罚意味着我们需要更多数据来说服我们，这确实是一个强关系。
- en: Tip
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Penalized regression is about tradeoffs**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**惩罚回归是关于权衡的**'
- en: Penalized regression is another example of the bias-variance tradeoff. When
    using a penalty, we get a worse fit in the training data, as we are adding bias.
    On the other hand, we reduce the variance and tend to avoid over-fitting. Therefore,
    the overall result might generalize better to unseen (test) data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚回归是偏差-方差权衡的另一个例子。在使用惩罚时，训练数据的拟合度会变差，因为我们引入了偏差。另一方面，我们减少了方差，倾向于避免过拟合。因此，整体结果可能更好地推广到未见过的（测试）数据。
- en: L1 and L2 penalties
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: L1 和 L2 惩罚
- en: We now explore these ideas in detail. Readers who do not care about some of
    the mathematical aspects should feel free to skip directly to the next section
    on how to use regularized regression in scikit-learn.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将详细探讨这些思想。对于不关心某些数学方面的读者，可以直接跳到下一节，了解如何在 scikit-learn 中使用正则化回归。
- en: 'The problem, in general, is that we are given a matrix *X* of training data
    (rows are observations and each column is a different feature), and a vector *y*
    of output values. The goal is to obtain a vector of weights, which we will call
    *b**. The ordinary least squares regression is given by the following formula:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，问题在于我们给定了一个训练数据矩阵 *X*（行是观测值，每列是不同的特征），以及一个输出值向量 *y*。目标是得到一个权重向量，我们将其称为
    *b*。普通最小二乘回归由以下公式给出：
- en: '![L1 and L2 penalties](img/2772OS_07_13.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![L1 和 L2 惩罚](img/2772OS_07_13.jpg)'
- en: That is, we find vector *b* that minimizes the squared distance to the target
    *y*. In these equations, we ignore the issue of setting an intercept by assuming
    that the training data has been preprocessed so that the mean of *y* is zero.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，我们找到一个向量 *b*，使其最小化与目标 *y* 的平方距离。在这些方程中，我们忽略了设置截距的问题，假设训练数据已经预处理，使得 *y*
    的均值为零。
- en: 'Adding a penalty or a regularization means that we do not simply consider the
    best fit on the training data, but also how vector ![L1 and L2 penalties](img/2772OS_07_21.jpg)
    is composed. There are two types of penalties that are typically used for regression:
    L1 and L2 penalties. An L1 penalty means that we penalize the regression by the
    sum of the absolute values of the coefficients, while an L2 penalty penalizes
    by the sum of squares.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 添加惩罚项或正则化意味着我们不仅考虑训练数据的最佳拟合，还考虑向量![L1 和 L2 惩罚](img/2772OS_07_21.jpg)的组成。回归中通常使用两种类型的惩罚：L1
    惩罚和 L2 惩罚。L1 惩罚意味着我们通过系数的绝对值之和来惩罚回归，而 L2 惩罚则是通过系数平方和来惩罚。
- en: 'When we add an L1 penalty, instead of the preceding equation, we instead optimize
    the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们添加 L1 惩罚时，我们不再使用之前的方程，而是优化以下方程：
- en: '![L1 and L2 penalties](img/2772OS_07_14.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![L1 和 L2 惩罚](img/2772OS_07_14.jpg)'
- en: 'Here, we are trying to simultaneously make the error small, but also make the
    values of the coefficients small (in absolute terms). Using an L2 penalty, means
    that we use the following formula:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们试图同时使误差变小，同时也使系数的值（绝对值）变小。使用 L2 惩罚意味着我们使用以下公式：
- en: '![L1 and L2 penalties](img/2772OS_07_15.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![L1 和 L2 惩罚](img/2772OS_07_15.jpg)'
- en: 'The difference is rather subtle: we now penalize by the square of the coefficient
    rather than their absolute value. However, the difference in the results is dramatic.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 差异相当微妙：我们现在通过系数的平方来惩罚，而不是它们的绝对值。然而，结果的差异却是显著的。
- en: Tip
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Ridge, Lasso, and ElasticNets**'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**岭回归、套索回归和弹性网回归**'
- en: These penalized models often go by rather interesting names. The L1 penalized
    model is often called the **Lasso**, while an L2 penalized one is known as **Ridge
    Regression**. When using both, we call this an **ElasticNet** model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这些带惩罚的模型通常都有一些非常有趣的名字。L1 惩罚模型通常被称为**Lasso**，而 L2 惩罚模型则被称为**岭回归**。当同时使用这两者时，我们称之为**ElasticNet**模型。
- en: Both the Lasso and the Ridge result in smaller coefficients than unpenalized
    regression (smaller in absolute value, ignoring the sign). However, the Lasso
    has the additional property that it results in many coefficients being set to
    exactly zero! This means that the final model does not even use some of its input
    features, the model is **sparse**. This is often a very desirable property as
    the model performs both feature selection and **regression** in a single step.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso 和 Ridge 都会比无惩罚回归产生更小的系数（绝对值较小，忽略符号）。然而，Lasso 还有一个额外的特性，即它会将许多系数设为零！这意味着最终模型甚至不使用它的一些输入特征，模型是**稀疏的**。这一特性通常是非常受欢迎的，因为模型在单一步骤中既进行特征选择，又进行**回归**。
- en: You will notice that whenever we add a penalty, we also add a weight *α*, which
    governs how much penalization we want. When *α* is close to zero, we are very
    close to unpenalized regression (in fact, if you set *α* to zero, you will simply
    perform OLS), and when *α* is large, we have a model that is very different from
    the unpenalized one.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，每当我们添加一个惩罚时，我们也会添加一个权重 *α*，它控制着我们希望的惩罚程度。当*α*接近零时，我们非常接近于无惩罚回归（事实上，如果你将*α*设置为零，你将仅执行普通最小二乘法
    (OLS)），而当*α*较大时，我们得到的模型与无惩罚模型非常不同。
- en: 'The Ridge model is older as the Lasso is hard to compute with pen and paper.
    However, with modern computers, we can use the Lasso as easily as Ridge, or even
    combine them to form ElasticNets. An ElasticNet has two penalties, one for the
    absolute value and the other for the squares and it solves the following equation:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归模型较为传统，因为 Lasso 用纸笔计算比较困难。然而，随着现代计算机的发展，我们可以像使用 Ridge 一样轻松地使用 Lasso，甚至可以将它们结合起来形成
    ElasticNets。ElasticNet 有两个惩罚项，一个用于绝对值，另一个用于平方项，它解决以下方程：
- en: '![L1 and L2 penalties](img/2772OS_07_16.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![L1 和 L2 惩罚](img/2772OS_07_16.jpg)'
- en: This formula is a combination of the two previous ones, with two parameters,
    *α[1]* and *α[2]*. Later in this chapter, we will discuss how to choose a good
    value for parameters.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式是前两个公式的组合，包含了两个参数，*α[1]* 和 *α[2]*。在本章稍后，我们将讨论如何为这些参数选择合适的值。
- en: Using Lasso or ElasticNet in scikit-learn
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中使用 Lasso 或 ElasticNet
- en: 'Let''s adapt the preceding example to use ElasticNets. Using scikit-learn,
    it is very easy to swap in the ElasticNet regressor for the least squares one
    that we had before:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调整之前的例子，使用 ElasticNets。在 scikit-learn 中，替换成 ElasticNet 回归器非常简单，和之前使用最小二乘法回归器一样：
- en: '[PRE12]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Now, we use `en`, whereas earlier we had used `lr`. This is the only change
    that is needed. The results are exactly what we would have expected. The training
    error increases to 5.0 (it was 4.6 before), but the cross-validation error decreases
    to 5.4 (it was 5.6 before). We trade a larger error on the training data in order
    to gain better generalization. We could have tried an L1 penalty using the `Lasso`
    class or L2 using the `Ridge` class with the same code.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用的是`en`，而之前我们使用的是`lr`。这是唯一需要改变的地方。结果正是我们预期的那样。训练误差增加到了 5.0（之前是 4.6），但交叉验证误差下降到了
    5.4（之前是 5.6）。我们在训练数据上牺牲了较大的误差，以获得更好的泛化能力。我们本可以使用相同的代码，尝试通过`Lasso`类应用 L1 惩罚，或者使用`Ridge`类应用
    L2 惩罚。
- en: Visualizing the Lasso path
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化 Lasso 路径
- en: 'Using scikit-learn, we can easily visualize what happens as the value of the
    regularization parameter (alpha) changes. We will again use the Boston data, but
    now we will use the `Lasso` regression object:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn，我们可以轻松地可视化当正则化参数（alpha）变化时所发生的情况。我们将再次使用波士顿数据集，但这次我们将使用`Lasso`回归对象：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: For each value in alphas, the `path` method on the `Lasso` object returns the
    coefficients that solve the lasso problem with that parameter value. Because the
    result changes smoothly with alpha, this can be computed very efficiently.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个 alpha 值，`path` 方法在 `Lasso` 对象上返回能够解决该参数值下 Lasso 问题的系数。由于结果随着 alpha 的变化而平滑变化，因此可以非常高效地计算。
- en: 'A typical way to visualize this path is to plot the value of the coefficients
    as alpha decreases. You can do so as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化这个路径的典型方法是绘制当 alpha 减小时系数的变化值。你可以按如下方式进行：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This results in the following plot (we left out the trivial code that adds
    axis labels and the title):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生如下图所示的结果（我们省略了添加轴标签和标题的简单代码）：
- en: '![Visualizing the Lasso path](img/2772OS_07_10.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![可视化Lasso路径](img/2772OS_07_10.jpg)'
- en: In this plot, the *x* axis shows decreasing amounts of regularization from left
    to right (alpha is decreasing). Each line shows how a different coefficient varies
    as alpha changes. The plot shows that when using very strong regularization (left
    side, very high alpha), the best solution is to have all values be exactly zero.
    As the regularization becomes weaker, one by one, the values of the different
    coefficients first shoot up, then stabilize. At some point, they all plateau as
    we are probably already close to the unpenalized solution.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，*x*轴显示了从左到右逐渐减弱的正则化（alpha逐渐减小）。每条线显示了不同系数随着alpha变化的情况。图表显示，当使用非常强的正则化时（左侧，alpha非常高），最佳解决方案是所有值都为零。随着正则化的减弱，各个系数的值会一个接一个地首先急剧增加，然后稳定下来。到了某个点，它们都会趋于平稳，因为我们可能已经接近未惩罚的解。
- en: P-greater-than-N scenarios
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: P大于N的情境
- en: The title of this section is a bit of inside jargon, which you will learn now.
    Starting in the 1990s, first in the biomedical domain, and then on the Web, problems
    started to appear where P was greater than N. What this means is that the number
    of features, P, was greater than the number of examples, N (these letters were
    the conventional statistical shorthand for these concepts). These became known
    as *P greater than N* problems.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 本节标题是一些内部术语，你现在将会学习这些。自1990年代起，首先在生物医学领域，然后在互联网领域，出现了P大于N的问题。这意味着特征的数量P大于样本的数量N（这些字母是这些概念的常用统计缩写）。这些问题被称为*P大于N*问题。
- en: For example, if your input is a set of written documents, a simple way to approach
    it is to consider each possible word in the dictionary as a feature and regress
    on those (we will later work on one such problem ourselves). In the English language,
    you have over 20,000 words (this is if you perform some stemming and only consider
    common words; it is more than ten times that if you skip this preprocessing step).
    If you only have a few hundred or a few thousand examples, you will have more
    features than examples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你的输入是一组书面文档，一种简单的方法是将字典中的每个可能单词视为一个特征，并基于这些特征进行回归（稍后我们会亲自处理类似问题）。在英语中，你有超过2万个单词（如果进行词干化并只考虑常见单词的话；如果跳过这个预处理步骤，单词数是它的十倍还多）。如果你只有几百或几千个样本，你将会有更多的特征而非样本。
- en: In this case, as the number of features is greater than the number of examples,
    it is possible to have a perfect fit on the training data. This is a mathematical
    fact, which is independent of your data. You are, in effect, solving a system
    of linear equations with fewer equations than variables. You can find a set of
    regression coefficients with zero training error (in fact, you can find more than
    one perfect solution, infinitely many).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，由于特征的数量大于样本的数量，因此可能会在训练数据上实现完美拟合。这是一个数学事实，与数据本身无关。实际上，你是在解决一个线性方程组，其中方程数量少于变量数量。你可以找到一组回归系数，训练误差为零（实际上，你可以找到多个完美解，无限多个）。
- en: However, and this is a major problem, *zero training error does not mean that
    your solution will generalize well*. In fact, it may generalize very poorly. Whereas
    earlier regularization could give you a little extra boost, it is now absolutely
    required for a meaningful result.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，且这是一个重大问题，*零训练误差并不意味着你的解决方案会很好地泛化*。事实上，它可能泛化得非常差。虽然早期的正则化可能给你一些额外的提升，但现在它是得到有意义结果的绝对必要条件。
- en: An example based on text documents
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于文本文档的示例
- en: We will now turn to an example that comes from a study performed at Carnegie
    Mellon University by Prof. Noah Smith's research group. The study was based on
    mining the so-called 10-K reports that companies file with the **Securities and
    Exchange Commission** (**SEC**) in the United States. This filing is mandated
    by law for all publicly traded companies. The goal of their study was to predict,
    based on this piece of public information, what the future volatility of the company's
    stock will be. In the training data, we are actually using historical data for
    which we already know what happened.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在转向一个来自卡内基梅隆大学诺亚·史密斯教授研究小组的研究示例。这项研究基于挖掘公司向美国**证券交易委员会**（**SEC**）提交的所谓10-K报告。这项申报是法律要求所有上市公司进行的。该研究的目标是基于这份公开信息预测公司股票未来的波动性。在训练数据中，我们实际上使用的是已经知道结果的历史数据。
- en: There are 16,087 examples available. The features, which have already been preprocessed
    for us, correspond to different words, 150,360 in total. Thus, we have many more
    features than examples, almost ten times as much. In the introduction, it was
    stated that ordinary least regression fails in these cases and we will now see
    why by attempting to blindly apply it.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 有16,087个可用示例。这些特征已经为我们预处理过，表示不同的单词，总共150,360个。因此，我们有的特征比示例多得多，几乎是它们的十倍。在引言中提到，普通最小二乘法在这些情况下失败，我们现在通过盲目应用它来看到原因。
- en: 'The dataset is available in SVMLight format from multiple sources, including
    the book''s companion website. This is a format that scikit-learn can read. SVMLight
    is, as the name says, a support vector machine implementation, which is also available
    through scikit-learn; right now, we are only interested in the file format:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可以通过多个来源获得SVMLight格式，包括本书的配套网站。这是一个scikit-learn可以读取的格式。正如名字所示，SVMLight是一个支持向量机实现，也可以通过scikit-learn使用；目前，我们只关心文件格式：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, data is a sparse matrix (that is, most of its entries
    are zeros and, therefore, only the nonzero entries are saved in memory), while
    the target is a simple one-dimensional vector. We can start by looking at some
    attributes of the target:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，数据是一个稀疏矩阵（即大多数条目为零，因此只保存非零条目在内存中），而目标是一个简单的一维向量。我们可以首先查看目标的一些属性：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, we can see that the data lies between -7.9 and -0.5\. Now that we have
    a feel for the data, we can check what happens when we use OLS to predict. Note
    that we can use exactly the same classes and methods as we did earlier:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们可以看到数据位于-7.9和-0.5之间。现在我们对数据有了大致的了解，我们可以检查使用OLS进行预测时会发生什么。请注意，我们可以使用与之前在波士顿示例中完全相同的类和方法：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The root mean squared error is not exactly zero because of rounding errors,
    but it is very close. The coefficient of determination is `1.0`. That is, the
    linear model is reporting a perfect prediction on its training data.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 均方根误差由于四舍五入误差不是完全为零，但它非常接近。决定系数为`1.0`。也就是说，线性模型在其训练数据上报告了完美的预测结果。
- en: 'When we use cross-validation (the code is very similar to what we used earlier
    in the Boston example), we get something very different: RMSE of 0.75, which corresponds
    to a negative coefficient of determination of -0.42\. This means that if we always
    "predict" the mean value of -3.5, we do better than when using the regression
    model!'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用交叉验证时（代码与我们之前在波士顿示例中使用的非常相似），我们得到的结果非常不同：RMSE为0.75，决定系数为负值-0.42。这意味着如果我们总是“预测”均值-3.5，我们比使用回归模型效果更好！
- en: Tip
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Training and generalization error**'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练和泛化误差**'
- en: When the number of features is greater than the number of examples, you always
    get zero training errors with OLS, except perhaps for issues due to rounding off.
    However, this is rarely a sign that your model will do well in terms of generalization.
    In fact, you may get zero training error and have a completely useless model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 当特征的数量大于示例的数量时，使用OLS总是会得到零训练误差，除非是由于四舍五入问题。然而，这很少是模型在泛化方面表现良好的标志。事实上，你可能会得到零训练误差，却拥有一个完全无用的模型。
- en: 'The natural solution is to use regularization to counteract the overfitting.
    We can try the same cross-validation loop with an ElasticNet learner, having set
    the penalty parameter to `0.1`:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 自然的解决方案是使用正则化来抵消过拟合。我们可以尝试使用ElasticNet学习器的相同交叉验证循环，并将惩罚参数设置为`0.1`：
- en: '[PRE18]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now, we get `0.4` RMSE and an R2 of `0.61`, much better than just predicting
    the mean. There is one problem with this solution, though, which is the choice
    of alpha. When using the default value (`1.0`), the result is very different (and
    worse).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们得到`0.4`的RMSE和`0.61`的R2，远比仅预测均值要好。不过，这个解决方案有一个问题，那就是alpha的选择。当使用默认值（`1.0`）时，结果非常不同（而且更差）。
- en: In this case, we cheated as the author had previously tried a few values to
    see which ones would give a good result. This is not effective and can lead to
    over estimates of confidence (we are looking at the test data to decide which
    parameter values to use and which we should never use). The next section explains
    how to do it properly and how this is supported by scikit-learn.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们作弊了，因为作者之前尝试了几个值，看看哪些值能给出好的结果。这是无效的，并且可能导致对信心的高估（我们正在查看测试数据来决定使用哪些参数值，而哪些应该永远不使用）。下一节将解释如何正确地做这件事，以及scikit-learn如何支持这一点。
- en: Setting hyperparameters in a principled way
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 以原则化的方式设置超参数
- en: In the preceding example, we set the penalty parameter to `0.1`. We could just
    as well have set it to 0.7 or 23.9\. Naturally, the results vary each time. If
    we pick an overly large value, we get underfitting. In the extreme case, the learning
    system will just return every coefficient equal to zero. If we pick a value that
    is too small, we are very close to OLS, which overfits and generalizes poorly
    (as we saw earlier).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们将惩罚参数设置为`0.1`。我们同样也可以将其设置为0.7或23.9。显然，结果会因每次而异。如果我们选择一个过大的值，我们会出现欠拟合。在极端情况下，学习系统将返回所有系数为零的结果。如果我们选择一个过小的值，我们就非常接近普通最小二乘法（OLS），这会导致过拟合且泛化能力差（正如我们之前所看到的）。
- en: 'How do we choose a good value? This is a general problem in machine learning:
    setting parameters for our learning methods. A generic solution is to use cross-validation.
    We pick a set of possible values, and then use cross-validation to choose which
    one is best. This performs more computation (five times more if we use five folds),
    but is always applicable and unbiased.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择一个合适的值呢？这是机器学习中的一个普遍问题：为我们的学习方法设置参数。一个通用的解决方案是使用交叉验证。我们选择一组可能的值，然后使用交叉验证来选择最优值。这需要更多的计算（如果我们使用五个子集，计算量是五倍），但始终适用且没有偏差。
- en: 'We must be careful, though. In order to obtain an estimate of generalization,
    we have to use **two-levels of cross-validation**: one level is to estimate the
    generalization, while the second level is to get good parameters. That is, we
    split the data in, for example, five folds. We start by holding out the first
    fold and will learn on the other four. Now, we split these again into 5 folds
    in order to choose the parameters. Once we have set our parameters, we test on
    the first fold. Now, we repeat this four other times:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须小心。为了获得泛化的估计，我们需要使用**两级交叉验证**：一级是估计泛化能力，二级是获得好的参数。也就是说，我们将数据分成例如五个子集。我们首先保留第一个子集，并在其他四个子集上进行学习。接下来，我们将这四个子集再次分成五个子集，用以选择参数。一旦设置好参数，我们在第一个子集上进行测试。然后，我们重复这一过程另外四次：
- en: '![Setting hyperparameters in a principled way](img/2772OS_07_12.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![以规范的方式设置超参数](img/2772OS_07_12.jpg)'
- en: The preceding figure shows how you break up a single training fold into subfolds.
    We would need to repeat it for all the other folds. In this case, we are looking
    at five outer folds and five inner folds, but there is no reason to use the same
    number of outer and inner folds, you can use any number you want as long as you
    keep the folds separate.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了如何将单一的训练子集分割成子子集。我们需要对所有其他子集重复这一过程。在这个例子中，我们有五个外部子集和五个内部子集，但并不一定非要使用相同数量的外部和内部子集，只要确保每个子集之间是分开的，你可以选择任何数量。
- en: This leads to a lot of computation, but it is necessary in order to do things
    correctly. The problem is that if you use a piece of data to make any decisions
    about your model (including which parameters to set), you have contaminated it
    and you can no longer use it to test the generalization ability of your model.
    This is a subtle point and it may not be immediately obvious. In fact, it is still
    the case that many users of machine learning get this wrong and overestimate how
    well their systems are doing, because they do not perform cross-validation correctly!
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致大量的计算，但为了正确地执行，确实是必要的。问题在于，如果你使用一部分数据来对模型做出任何决策（包括选择设置哪些参数），那么你就已经污染了这部分数据，无法再用它来测试模型的泛化能力。这是一个微妙的点，可能并不容易立刻察觉。事实上，许多机器学习用户仍然犯这个错误，过高估计他们系统的表现，因为他们没有正确地执行交叉验证！
- en: 'Fortunately, scikit-learn makes it very easy to do the right thing; it provides
    classes named `LassoCV`, `RidgeCV`, and `ElasticNetCV`, all of which encapsulate
    an inner cross-validation loop to optimize for the necessary parameter. The code
    is almost exactly like the previous one, except that we do not need to specify
    any value for alpha:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn 使得做对的事情变得非常容易；它提供了名为`LassoCV`、`RidgeCV`和`ElasticNetCV`的类，这些类都封装了一个内部交叉验证循环，用于优化必要的参数。代码几乎和之前的一样，唯一不同的是我们不需要为
    alpha 指定任何值：
- en: '[PRE19]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This results in a lot of computation, so you may want to get some coffee while
    you are waiting (depending on how fast your computer is). You might get better
    performance by taking advantage of multiple processors. This is a built-in feature
    of scikit-learn, which can be accessed quite trivially by using the `n_jobs` parameter
    to the `ElasticNetCV` constructor. To use four CPUs, make use of the following
    code:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致大量计算，因此在等待时你可能想要喝杯咖啡（取决于你的计算机速度有多快）。通过利用多个处理器，你可能会获得更好的性能。这是 scikit-learn
    的内置功能，可以通过将`n_jobs`参数传递给`ElasticNetCV`构造函数来轻松访问。要使用四个 CPU，可以使用以下代码：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Set the `n_jobs` parameter to `-1` to use all the available CPUs:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 将`n_jobs`参数设置为`-1`以使用所有可用的 CPU：
- en: '[PRE21]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'You may have wondered why, if ElasticNets have two penalties, the L1 and the
    L2 penalty, we only need to set a single value for alpha. In fact, the two values
    are specified by separately specifying alpha and the `l1_ratio` variable (that
    is spelled *ell-1-underscore-ratio*). Then, α1 and α2 are set as follows (where
    *ρ* stands for `l1_ratio`):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你会想知道，如果 ElasticNets 有两个惩罚项，L1 和 L2 惩罚项，为什么我们只需要设置一个 alpha 值。实际上，通过分别指定 alpha
    和 `l1_ratio` 变量（拼写为 *ell-1-underscore-ratio*），可以指定这两个值。然后，α1 和 α2 设置如下（其中 *ρ*
    代表 `l1_ratio`）：
- en: '![Setting hyperparameters in a principled way](img/2772OS_07_18.jpg)![Setting
    hyperparameters in a principled way](img/2772OS_07_17.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![以原则方式设置超参数](img/2772OS_07_18.jpg)![以原则方式设置超参数](img/2772OS_07_17.jpg)'
- en: In an intuitive sense, alpha sets the overall amount of regularization while
    `l1_ratio` sets the tradeoff between the different types of regularization, L1
    and L2.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，alpha 设置了总体正则化的量，而`l1_ratio`设置了不同类型正则化（L1 和 L2）之间的权衡。
- en: 'We can request that the `ElasticNetCV` object tests different values of `l1_ratio`,
    as is shown in the following code:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以要求`ElasticNetCV`对象测试不同的`l1_ratio`值，如下所示的代码：
- en: '[PRE22]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This set of `l1_ratio` values is recommended in the documentation. It will test
    models that are almost like Ridge (when `l1_ratio` is 0.01 or 0.05) as well as
    models that are almost like Lasso (when `l1_ratio` is 0.95 or 0.99). Thus, we
    explore a full range of different options.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这组`l1_ratio`值在文档中被推荐使用。它将测试几乎类似于 Ridge（当`l1_ratio`为 0.01 或 0.05 时）以及几乎类似于 Lasso（当`l1_ratio`为
    0.95 或 0.99 时）的模型。因此，我们探索了各种不同选项的完整范围。
- en: Because of its flexibility and the ability to use multiple CPUs, `ElasticNetCV`
    is an excellent default solution for regression problems when you don't have any
    particular reason to prefer one type of model over the rest.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其灵活性和能够使用多个 CPU 的能力，当你没有任何特定原因偏好一种模型而不是其他模型时，`ElasticNetCV`是回归问题的一个出色的默认解决方案。
- en: 'Putting all this together, we can now visualize the prediction versus real
    fit on this large dataset:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们现在可以在这个大数据集上可视化预测与真实拟合：
- en: '[PRE23]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This results in the following plot:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致以下图表：
- en: '![Setting hyperparameters in a principled way](img/2772OS_07_11.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![以原则方式设置超参数](img/2772OS_07_11.jpg)'
- en: We can see that the predictions do not match very well on the bottom end of
    the value range. This is perhaps because there are so many fewer elements on this
    end of the target range (which also implies that this affects only a small minority
    of datapoints).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，在值范围的底端，预测结果并不很好匹配。这可能是因为在目标范围的这一端元素较少（这也意味着这只影响了少数数据点）。
- en: 'One last note: the approach of using an inner cross-validation loop to set
    a parameter is also available in scikit-learn using a grid search. In fact, we
    already used it in the previous chapter.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点：使用内部交叉验证循环来设置参数的方法在 scikit-learn 中也是可用的，可以使用网格搜索。实际上，我们在上一章中已经使用过了。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started with the oldest trick in the book, ordinary least
    squares regression. Although centuries old, it is still often the best solution
    for regression. However, we also saw more modern approaches that avoid overfitting
    and can give us better results especially when we have a large number of features.
    We used Ridge, Lasso, and ElasticNets; these are the state-of-the-art methods
    for regression.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们从最古老的技巧开始，普通最小二乘回归。尽管有几个世纪的历史，但它仍然经常是回归问题的最佳解决方案。然而，我们也看到了更现代的方法，避免过拟合，并且在具有大量特征时可以给出更好的结果。我们使用了
    Ridge、Lasso 和 ElasticNets；这些是回归问题的最先进方法。
- en: 'We saw, once again, the danger of relying on training error to estimate generalization:
    it can be an overly optimistic estimate to the point where our model has zero
    training error, but we know that it is completely useless. When thinking through
    these issues, we were led into two-level cross-validation, an important point
    that many in the field still have not completely internalized.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次看到了依赖训练误差来估计泛化能力的危险：它可能会给出过于乐观的估计，甚至使我们的模型在训练误差上为零，但我们知道它完全没有用。在思考这些问题时，我们引入了二级交叉验证，这是一个重要的概念，许多领域的从业者仍未完全理解。
- en: Throughout this chapter, we were able to rely on scikit-learn to support all
    the operations we wanted to perform, including an easy way to achieve correct
    cross-validation. ElasticNets with an inner cross-validation loop for parameter
    optimization (as implemented in scikit-learn by `ElasticNetCV`) should probably
    become your default method for regression.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们能够依赖scikit-learn来支持我们想要执行的所有操作，包括一种简便的方法来实现正确的交叉验证。带有内部交叉验证循环的ElasticNet（用于参数优化，scikit-learn中由`ElasticNetCV`实现）可能应该成为你回归分析的默认方法。
- en: One reason to use an alternative is when you are interested in a sparse solution.
    In this case, a pure Lasso solution is more appropriate as it will set many coefficients
    to zero. It will also allow you to discover from the data a small number of variables,
    which are important to the output. Knowing the identity of these may be interesting
    in and of itself, in addition to having a good regression model.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用替代方法的一个原因是当你对稀疏解感兴趣时。在这种情况下，纯Lasso解更为合适，因为它会将许多系数设为零。它还会让你从数据中发现少数几个对输出至关重要的变量。了解这些变量的身份本身可能就很有趣，除了获得一个优秀的回归模型之外。
- en: In the next chapter, we will look at recommendations, another machine learning
    problem. Our first approach will be to use regression to predict consumer product
    ratings. We will then see alternative models to generate recommendations.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论推荐系统，这是另一个机器学习问题。我们首先的方法是使用回归来预测消费者产品评分。然后，我们将看到生成推荐的替代模型。
