- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: Going Deeper – The Mechanics of TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探讨 – TensorFlow 的机制
- en: In *Chapter 13*, *Parallelizing Neural Network Training with TensorFlow*, we
    covered how to define and manipulate tensors and worked with the `tf.data` API
    to build input pipelines. We further built and trained a multilayer perceptron
    to classify the Iris dataset using the TensorFlow Keras API (`tf.keras`).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第 13 章* 中，*使用 TensorFlow 并行化神经网络训练*，我们介绍了如何定义和操作张量，并使用 `tf.data` API 构建输入流水线。我们进一步使用
    TensorFlow Keras API (`tf.keras`) 构建和训练了一个多层感知器，以对鸢尾花数据集进行分类。
- en: Now that we have some hands-on experience with TensorFlow neural network (NN)
    training and machine learning, it's time to take a deeper dive into the TensorFlow
    library and explore its rich set of features, which will allow us to implement
    more advanced deep learning models in upcoming chapters.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过 TensorFlow 神经网络（NN）训练和机器学习有了一些实际经验，是时候深入研究 TensorFlow 库并探索其丰富的功能集了，这将使我们能够在接下来的章节中实现更高级的深度学习模型。
- en: In this chapter, we will use different aspects of TensorFlow's API to implement
    NNs. In particular, we will again use the Keras API, which provides multiple layers
    of abstraction to make the implementation of standard architectures very convenient.
    TensorFlow also allows us to implement custom NN layers, which is very useful
    in research-oriented projects that require more customization. Later in this chapter,
    we will implement such a custom layer.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用 TensorFlow API 的不同方面来实现 NNs。特别是，我们将再次使用 Keras API，它提供了多层抽象，使得实现标准架构非常方便。TensorFlow
    还允许我们实现自定义 NN 层，这在需要更多定制的研究项目中非常有用。本章后面，我们将实现这样一个自定义层。
- en: To illustrate the different ways of model building using the Keras API, we will
    also consider the classic **exclusive or** (**XOR**) problem. Firstly, we will
    build multilayer perceptrons using the `Sequential` class. Then, we will consider
    other methods, such as subclassing `tf.keras.Model` for defining custom layers.
    Finally, we will cover `tf.estimator`, a high-level TensorFlow API that encapsulates
    the machine learning steps from raw input to prediction.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明使用 Keras API 构建模型的不同方式，我们还将考虑经典的**异或**（**XOR**）问题。首先，我们将使用 `Sequential`
    类构建多层感知器。然后，我们将考虑其他方法，如使用 `tf.keras.Model` 的子类化定义自定义层。最后，我们将介绍 `tf.estimator`，这是一个高级
    TensorFlow API，将从原始输入到预测的机器学习步骤封装起来。
- en: 'The topics that we will cover are as follows:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖的主题如下：
- en: Understanding and working with TensorFlow graphs and migration to TensorFlow
    v2
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解和使用 TensorFlow 图以及迁移到 TensorFlow v2
- en: Function decoration for graph compilation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图编译的函数装饰
- en: Working with TensorFlow variables
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 变量
- en: Solving the classic XOR problem and understanding model capacity
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决经典的 XOR 问题并理解模型容量
- en: Building complex NN models using Keras' `Model` class and the Keras functional
    API
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 的 `Model` 类和 Keras 函数式 API 构建复杂的 NN 模型
- en: Computing gradients using automatic differentiation and `tf.GradientTape`
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动微分和 `tf.GradientTape` 计算梯度
- en: Working with TensorFlow Estimators
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow Estimators
- en: The key features of TensorFlow
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 的关键特性
- en: TensorFlow provides us with a scalable, multiplatform programming interface
    for implementing and running machine learning algorithms. The TensorFlow API has
    been relatively stable and mature since its 1.0 release in 2017, but it just experienced
    a major redesign with its recent 2.0 release in 2019, which we are using in this
    book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 为我们提供了一个可扩展的、多平台的编程接口，用于实现和运行机器学习算法。自 2017 年的 1.0 版本发布以来，TensorFlow
    API 相对稳定和成熟，但在 2019 年的最新 2.0 版本中经历了重大重新设计，本书中我们使用的就是这个版本。
- en: Since its initial release in 2015, TensorFlow has become the most widely adopted
    deep learning library. However, one of its main friction points was that it was
    built around static computation graphs. Static computation graphs have certain
    advantages, such as better graph optimizations behind the scenes and support for
    a wider range of hardware devices; however, static computation graphs require
    separate graph declaration and graph evaluation steps, which make it cumbersome
    for users to develop and work with NNs interactively.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自 2015 年首次发布以来，TensorFlow 已成为最广泛采用的深度学习库。然而，它的一个主要痛点是它是围绕静态计算图构建的。静态计算图具有某些优势，如更好的图优化和支持更广泛的硬件设备；但是，静态计算图需要单独的图声明和图评估步骤，这使得用户难以与
    NNs 进行互动式开发和工作。
- en: Taking all the user feedback to heart, the TensorFlow team decided to make dynamic
    computation graphs the default in TensorFlow 2.0, which makes the development
    and training of NNs much more convenient. In the next section, we will cover some
    of the important changes from TensorFlow v1.x to v2\. Dynamic computation graphs
    allow for interleaving the graph declaration and graph evaluation steps such that
    TensorFlow 2.0 feels much more natural for Python and NumPy users compared to
    previous versions of TensorFlow. However, note that TensorFlow 2.0 still allows
    users to use the "old" TensorFlow v1.x API via the `tf.compat` submodule. This
    helps users to transition their code bases more smoothly to the new TensorFlow
    v2 API.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有用户的反馈，TensorFlow 团队决定在 TensorFlow 2.0 中将动态计算图作为默认选项，这使得神经网络的开发和训练更加便捷。在接下来的章节中，我们将介绍从
    TensorFlow v1.x 到 v2 的一些重要变化。动态计算图使得图的声明和评估步骤能够交替进行，这使得 TensorFlow 2.0 对于 Python
    和 NumPy 用户来说，比起以前的版本更加自然。不过，请注意，TensorFlow 2.0 仍然允许用户通过 `tf.compat` 子模块使用 "旧版"
    TensorFlow v1.x API。这有助于用户更顺利地将代码库过渡到新的 TensorFlow v2 API。
- en: A key feature of TensorFlow, which was also noted in *Chapter 13*, *Parallelizing
    Neural Network Training with TensorFlow*, is its ability to work with single or
    multiple graphical processing units (GPUs). This allows users to train deep learning
    models very efficiently on large datasets and large-scale systems.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的一个关键特性，*第13章*中也提到过，*使用 TensorFlow 并行化神经网络训练*，是它能与单个或多个图形处理单元（GPU）协同工作。这使得用户能够在大型数据集和大规模系统上高效地训练深度学习模型。
- en: While TensorFlow is an open source library and can be freely used by everyone,
    its development is funded and supported by Google. This involves a large team
    of software engineers who expand and improve the library continuously. Since TensorFlow
    is an open source library, it also has strong support from other developers outside
    of Google, who avidly contribute and provide user feedback.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 TensorFlow 是一个开源库，任何人都可以自由使用，但其开发由 Google 提供资金支持，并且有一个庞大的软件工程团队在持续扩展和改进该库。由于
    TensorFlow 是开源的，它还得到了来自 Google 以外的其他开发者的强大支持，他们热心地贡献代码并提供用户反馈。
- en: This has made the TensorFlow library more useful to both academic researchers
    and developers. A further consequence of these factors is that TensorFlow has
    extensive documentation and tutorials to help new users.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得 TensorFlow 库对学术研究人员和开发人员都更加有用。这些因素的进一步影响是，TensorFlow 拥有丰富的文档和教程，帮助新用户上手。
- en: Last, but not least, TensorFlow supports mobile deployment, which also makes
    it a very suitable tool for production.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，TensorFlow 支持移动端部署，这使得它成为一个非常适合生产环境的工具。
- en: 'TensorFlow''s computation graphs: migrating to TensorFlow v2'
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 的计算图：迁移到 TensorFlow v2
- en: TensorFlow performs its computations based on a directed acyclic graph (DAG).
    In TensorFlow v1.x, such graphs could be explicitly defined in the low-level API,
    although this was not trivial for large and complex models. In this section, we
    will see how these graphs can be defined for a simple arithmetic computation.
    Then, we will see how to migrate a graph to TensorFlow v2, the **eager execution**
    and dynamic graph paradigm, as well as the function decoration for faster computations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 基于有向无环图（DAG）进行计算。在 TensorFlow v1.x 中，这些图可以在低级 API 中显式定义，尽管对于大型和复杂的模型来说，这并非易事。在这一节中，我们将看到如何为简单的算术计算定义这些图。然后，我们将了解如何将图迁移到
    TensorFlow v2，**即时执行**和动态图范式，以及如何通过函数装饰器加速计算。
- en: Understanding computation graphs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解计算图
- en: 'TensorFlow relies on building a computation graph at its core, and it uses
    this computation graph to derive relationships between tensors from the input
    all the way to the output. Let''s say that we have rank 0 (scalar) tensors *a*,
    *b*, and *c* and we want to evaluate ![](img/B13208_14_001.png). This evaluation
    can be represented as a computation graph, as shown in the following figure:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 核心依赖于构建计算图，它利用这个计算图从输入到输出推导张量之间的关系。假设我们有 0 阶（标量）张量 *a*、*b* 和 *c*，并且我们希望计算
    ![](img/B13208_14_001.png)。这个计算可以表示为一个计算图，如下图所示：
- en: '![](img/13208_14_01.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_01.png)'
- en: As you can see, the computation graph is simply a network of nodes. Each node
    resembles an operation, which applies a function to its input tensor or tensors
    and returns zero or more tensors as the output. TensorFlow builds this computation
    graph and uses it to compute the gradients accordingly. In the next subsections,
    we will see some examples of creating a graph for this computation using TensorFlow
    v1.x and v2 styles.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，计算图实际上是一个节点网络。每个节点类似于一个操作，它将一个或多个输入张量应用一个函数，并返回一个或多个输出张量。TensorFlow 构建这个计算图并使用它来相应地计算梯度。在接下来的小节中，我们将看到使用
    TensorFlow v1.x 和 v2 风格创建计算图的一些示例。
- en: Creating a graph in TensorFlow v1.x
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow v1.x 中创建图
- en: 'In the earlier version of the TensorFlow (v1.x) low-level API, this graph had
    to be explicitly declared. The individual steps for building, compiling, and evaluating
    such a computation graph in TensorFlow v1.x are as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow 较早版本（v1.x）中的低级 API 中，这个图必须显式声明。构建、编译和评估这种计算图的各个步骤如下所示：
- en: Instantiate a new, empty computation graph
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个新的空计算图
- en: Add nodes (tensors and operations) to the computation graph
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向计算图中添加节点（张量和操作）
- en: 'Evaluate (execute) the graph:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估（执行）图：
- en: Start a new session
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个新的 session
- en: Initialize the variables in the graph
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化图中的变量
- en: Run the computation graph in this session
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在此 session 中运行计算图
- en: 'Before we take a look at the dynamic approach in TensorFlow v2, let''s look
    at a simple example that illustrates how to create a graph in TensorFlow v1.x
    for evaluating ![](img/B13208_14_002.png), as shown in the previous figure. The
    variables *a*, *b*, and *c* are scalars (single numbers), and we define these
    as TensorFlow constants. A graph can then be created by calling `tf.Graph()`.
    Variables, as well as computations, represent the nodes of the graph, which we
    will define as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们研究 TensorFlow v2 中的动态方法之前，让我们先看一个简单的例子，说明如何在 TensorFlow v1.x 中创建一个图来评估上图所示的
    ![](img/B13208_14_002.png)。变量 *a*、*b* 和 *c* 是标量（单个数字），我们将这些定义为 TensorFlow 常量。然后可以通过调用
    `tf.Graph()` 创建一个图。变量和计算代表图的节点，我们将按照以下方式定义它们：
- en: '[PRE0]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, we first defined graph `g` via `g=tf.Graph()`. Then, we added
    nodes to the graph, `g`, using `with g.as_default()`. However, note that if we
    do not explicitly create a graph, there is always a default graph to which variables
    and computations will be added automatically.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们首先通过 `g=tf.Graph()` 定义了图 `g`。然后，我们使用 `with g.as_default()` 向图 `g` 中添加了节点。然而，请注意，如果我们没有显式创建图，总是会有一个默认图，变量和计算将自动添加到这个图中。
- en: In TensorFlow v1.x, a session is an environment in which the operations and
    tensors of a graph can be executed. The `Session` class was removed from TensorFlow
    v2; However, for the time being, it is still available via the `tf.compat` submodule
    to allow compatibility with TensorFlow v1.x. A session object can be created by
    calling `tf.compat.v1.Session()`, which can receive an existing graph (here, `g`)
    as an argument, as in `Session(graph=g)`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow v1.x 中，session 是一个环境，在这个环境中可以执行图的操作和张量。`Session` 类在 TensorFlow
    v2 中被移除；然而，目前它仍然可以通过 `tf.compat` 子模块使用，以保持与 TensorFlow v1.x 的兼容性。可以通过调用 `tf.compat.v1.Session()`
    创建一个 session 对象，该对象可以接收一个现有的图（这里是 `g`）作为参数，如 `Session(graph=g)`。
- en: After launching a graph in a TensorFlow session, we can execute its nodes, that
    is, evaluate its tensors or execute its operators. Evaluating each individual
    tensor involves calling its `eval()` method inside the current session. When evaluating
    a specific tensor in the graph, TensorFlow has to execute all the preceding nodes
    in the graph until it reaches the given node of interest. In case there are one
    or more placeholder variables, we also need to provide values for those through
    the session's `run` method, as we will see later in the chapter.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow session 中启动图后，我们可以执行它的节点，也就是说，评估它的张量或执行它的操作符。评估每个独立的张量涉及在当前 session
    中调用它的 `eval()` 方法。当评估图中的特定张量时，TensorFlow 必须执行图中的所有前置节点，直到它到达给定的目标节点。如果有一个或多个占位符变量，我们还需要通过
    session 的 `run` 方法为这些变量提供值，正如我们稍后在本章中会看到的那样。
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Migrating a graph to TensorFlow v2
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将图迁移到 TensorFlow v2
- en: 'Next, let''s look at how this code can be migrated to TensorFlow v2\. TensorFlow
    v2 uses dynamic (as opposed to static) graphs by default (this is also called
    eager execution in TensorFlow), which allows us to evaluate an operation on the
    fly. Therefore, we do not have to explicitly create a graph and a session, which
    makes the development workflow much more convenient:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何将这段代码迁移到 TensorFlow v2。TensorFlow v2 默认使用动态图（与静态图相对）（这也被称为 TensorFlow
    中的急切执行），允许我们即时评估一个操作。因此，我们不必显式创建图和会话，这使得开发工作流更加方便：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Loading input data into a model: TensorFlow v1.x style'
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将输入数据加载到模型中：TensorFlow v1.x 风格
- en: 'Another important improvement from TensorFlow v1.x to v2 is regarding how data
    can be loaded into our models. In TensorFlow v2, we can directly feed data in
    the form of Python variables or NumPy arrays. However, when using the TensorFlow
    v1.x low-level API, we had to create placeholder variables for providing input
    data to a model. For the preceding simple computation graph example, ![](img/B13208_14_003.png),
    let''s assume that *a*, *b*, and *c* are the input tensors of rank 0\. We can
    then define three placeholders, which we will then use to "feed" data to the model
    via a so-called `feed_dict` dictionary, as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从 TensorFlow v1.x 到 v2 的另一个重要改进是关于如何将数据加载到模型中。在 TensorFlow v2 中，我们可以直接以 Python
    变量或 NumPy 数组的形式提供数据。然而，在使用 TensorFlow v1.x 的低级 API 时，我们必须创建占位符变量来为模型提供输入数据。对于前面的简单计算图示例，![](img/B13208_14_003.png)，假设
    *a*、*b* 和 *c* 是秩 0 的输入张量。我们可以定义三个占位符，然后通过一个名为 `feed_dict` 的字典将数据“输入”到模型中，如下所示：
- en: '[PRE4]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Loading input data into a model: TensorFlow v2 style'
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将输入数据加载到模型中：TensorFlow v2 风格
- en: 'In TensorFlow v2, all this can simply be done by *defining a regular Python
    function* with `a`, `b`, and `c` as its input arguments, for example:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TensorFlow v2 中，所有这些操作可以简单地通过*定义一个普通的 Python 函数*，将 `a`、`b` 和 `c` 作为输入参数来完成，例如：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, to carry out the computation, we can simply call this function with `Tensor`
    objects as function arguments. Note that TensorFlow functions such as `add`, `subtract`,
    and `multiply` also allow us to provide inputs of higher ranks in the form of
    a TensorFlow `Tensor` object, a NumPy array, or possibly other Python objects,
    such as lists and tuples. In the following code example, we provide scalar inputs
    (rank 0), as well as rank 1 and rank 2 inputs, as lists:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了执行计算，我们只需将`Tensor`对象作为函数参数调用此函数。请注意，TensorFlow 函数如 `add`、`subtract` 和 `multiply`
    也允许我们以 TensorFlow `Tensor` 对象、NumPy 数组或其他 Python 对象（如列表和元组）的形式提供更高秩的输入。在下面的代码示例中，我们提供了标量输入（秩
    0），以及秩 1 和秩 2 的输入，这些输入以列表的形式提供：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In this section, you saw how migrating to TensorFlow v2 makes the programming
    style simple and efficient by avoiding explicit graph and session creation steps.
    Now that we have seen how TensorFlow v1.x compares to TensorFlow v2, we will focus
    only on TensorFlow v2 for the remainder of this book. Next, we will take a deeper
    look into decorating Python functions into a graph that allows for faster computation.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您看到如何通过避免显式创建图和会话的步骤，迁移到 TensorFlow v2 使编程风格变得简单高效。现在，我们已经了解了 TensorFlow
    v1.x 与 TensorFlow v2 的比较，接下来我们将专注于 TensorFlow v2，在本书的剩余部分，我们将更深入地探讨如何将 Python
    函数装饰成一个图，以便实现更快速的计算。
- en: Improving computational performance with function decorators
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用函数装饰器提高计算性能
- en: As you saw in the previous section, we can easily write a normal Python function
    and utilize TensorFlow operations. However, computations via the eager execution
    (dynamic graph) mode are not as efficient as the static graph execution in TensorFlow
    v1.x. Thus, TensorFlow v2 provides a tool called AutoGraph that can automatically
    transform Python code into TensorFlow's graph code for faster execution. In addition,
    TensorFlow provides a simple mechanism for compiling a normal Python function
    to a static TensorFlow graph in order to make the computations more efficient.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一节所示，我们可以轻松编写一个普通的 Python 函数并利用 TensorFlow 操作。然而，通过动态执行（动态图）模式进行的计算效率不如 TensorFlow
    v1.x 中的静态图执行。因此，TensorFlow v2 提供了一种名为 AutoGraph 的工具，它可以自动将 Python 代码转换为 TensorFlow
    的图代码，从而实现更快的执行。此外，TensorFlow 还提供了一种简单的机制，将普通的 Python 函数编译为静态 TensorFlow 图，以提高计算效率。
- en: 'To see how this works in practice, let''s work with our previous `compute_z`
    function and annotate it for graph compilation using the `@tf.function` decorator:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看这如何在实际中工作，让我们用之前的 `compute_z` 函数并为其添加注解，以便使用 `@tf.function` 装饰器进行图编译：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that we can use and call this function the same way as before, but now
    TensorFlow will construct a static graph based on the input arguments. Python
    supports dynamic typing and polymorphism, so we can define a function such as
    `def f(a, b): return a+b` and then call it using integer, float, list, or string
    inputs (recall that `a+b` is a valid operation for lists and strings). While TensorFlow
    graphs require static types and shapes, `tf.function` supports such a dynamic
    typing capability. For example, let''s call this function with the following inputs:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '请注意，我们可以像以前一样使用和调用此函数，但现在 TensorFlow 将根据输入参数构建一个静态图。Python 支持动态类型和多态性，因此我们可以定义一个函数，如
    `def f(a, b): return a+b`，然后使用整数、浮动、列表或字符串作为输入来调用它（回顾一下，`a+b` 是一个有效的列表和字符串操作）。虽然
    TensorFlow 图需要静态类型和形状，`tf.function` 支持这种动态类型能力。例如，让我们使用以下输入调用此函数：'
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will produce the same outputs as before. Here, TensorFlow uses a tracing
    mechanism to construct a graph based on the input arguments. For this tracing
    mechanism, TensorFlow generates a tuple of keys based on the input signatures
    given for calling the function. The generated keys are as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生与之前相同的输出。在这里，TensorFlow 使用跟踪机制根据输入参数构建图。对于这个跟踪机制，TensorFlow 会基于调用函数时提供的输入签名生成一个键的元组。生成的键如下所示：
- en: For `tf.Tensor` arguments, the key is based on their shapes and dtypes.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `tf.Tensor` 参数，键是基于它们的形状和数据类型的。
- en: For Python types, such as lists, their `id()` is used to generate cache keys.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Python 类型，例如列表，它们的 `id()` 被用来生成缓存键。
- en: For Python primitive values, the cache keys are based on the input values.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 Python 原始值，缓存键是基于输入值的。
- en: 'Upon calling such a decorated function, TensorFlow will check whether a graph
    with the corresponding key has already been generated. If such a graph does not
    exist, TensorFlow will generate a new graph and store the new key. On the other
    hand, if we want to limit the way a function can be called, we can specify its
    input signature via a tuple of `tf.TensorSpec` objects when defining the function.
    For example, let''s redefine the previous function, `compute_z`, and specify that
    only rank 1 tensors of type `tf.int32` are allowed:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 调用这样的装饰函数时，TensorFlow 会检查是否已经生成了具有相应键的图。如果该图不存在，TensorFlow 会生成一个新图并存储新的键。另一方面，如果我们希望限制函数的调用方式，我们可以在定义函数时通过一组
    `tf.TensorSpec` 对象指定其输入签名。例如，让我们重新定义之前的函数 `compute_z`，并指定只允许秩为 1 的 `tf.int32`
    类型张量：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can call this function using rank 1 tensors (or lists that can be converted
    to rank 1 tensors):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用秩为 1 的张量（或可以转换为秩 1 张量的列表）来调用此函数：
- en: '[PRE10]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'However, calling this function using tensors with ranks other than 1 will result
    in an error since the rank will not match the specified input signature, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用秩不为 1 的张量调用此函数将导致错误，因为秩将与指定的输入签名不匹配，具体如下：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In this section, we learned how to annotate a normal Python function so that
    TensorFlow will compile it into a graph for faster execution. Next, we will look
    at TensorFlow variables: how to create them and how to use them.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何注解一个普通的 Python 函数，以便 TensorFlow 会将其编译成图以加速执行。接下来，我们将了解 TensorFlow
    变量：如何创建它们以及如何使用它们。
- en: TensorFlow Variable objects for storing and updating model parameters
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于存储和更新模型参数的 TensorFlow 变量对象
- en: 'We covered `Tensor` objects in *Chapter 13*, *Parallelizing Neural Network
    Training with TensorFlow*. In the context of TensorFlow, a `Variable` is a special
    `Tensor` object that allows us to store and update the parameters of our models
    during training. A `Variable` can be created by just calling the `tf.Variable`
    class on user-specified initial values. In the following code, we will generate
    `Variable` objects of type `float32`, `int32`, `bool`, and `string`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第 13 章*《使用 TensorFlow 并行训练神经网络》中讲解了 `Tensor` 对象。在 TensorFlow 中，`Variable`
    是一个特殊的 `Tensor` 对象，允许我们在训练过程中存储和更新模型的参数。通过调用 `tf.Variable` 类并传入用户指定的初始值，可以创建一个
    `Variable`。在下面的代码中，我们将生成 `float32`、`int32`、`bool` 和 `string` 类型的 `Variable` 对象：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Notice that we always have to provide the initial values when creating a `Variable`.
    Variables have an attribute called `trainable`, which, by default, is set to `True`.
    Higher-level APIs such as Keras will use this attribute to manage the trainable
    variables and non-trainable ones. You can define a non-trainable `Variable` as
    follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在创建 `Variable` 时总是需要提供初始值。变量有一个名为 `trainable` 的属性，默认值为 `True`。像 Keras 这样的高级
    API 将使用此属性来管理可训练变量和不可训练变量。你可以如下定义一个不可训练的 `Variable`：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The values of a `Variable` can be efficiently modified by running some operations
    such as `.assign()`, `.assign_add()` and related methods. Let''s take a look at
    some examples:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Variable`的值可以通过执行一些操作，如`.assign()`、`.assign_add()`和相关方法来高效修改。让我们来看一些例子：'
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: When the `read_value` argument is set to `True` (which is also the default),
    these operations will automatically return the new values after updating the current
    values of the `Variable`. Setting the `read_value` to `False` will suppress the
    automatic return of the updated value (but the `Variable` will still be updated
    in place). Calling `w.value()` will return the values in a tensor format. Note
    that we cannot change the shape or type of the `Variable` during assignment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当`read_value`参数设置为`True`（这也是默认值）时，这些操作会在更新`Variable`的当前值后自动返回新值。将`read_value`设置为`False`将抑制更新值的自动返回（但`Variable`仍然会就地更新）。调用`w.value()`将返回张量格式的值。请注意，在赋值期间，我们不能更改`Variable`的形状或类型。
- en: You will recall that for NN models, initializing model parameters with random
    weights is necessary to break the symmetry during backpropagation—otherwise, a
    multilayer NN would be no more useful than a single-layer NN like logistic regression.
    When creating a TensorFlow `Variable`, we can also use a random initialization
    scheme. TensorFlow can generate random numbers based on a variety of distributions
    via `tf.random` (see [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random)).
    In the following example, we will take a look at some standard initialization
    methods that are also available in Keras (see [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers)).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，对于神经网络模型，使用随机权重初始化模型参数是必要的，以便在反向传播过程中打破对称性——否则，多层神经网络将和单层神经网络（例如逻辑回归）一样没有意义。在创建TensorFlow的`Variable`时，我们也可以使用随机初始化方案。TensorFlow可以通过`tf.random`基于多种分布生成随机数（请参见[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/random)）。在以下示例中，我们将查看Keras中也可用的一些标准初始化方法（请参见[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers)）。
- en: 'So, let''s look at how we can create a `Variable` with Glorot initialization,
    which is a classic random initialization scheme that was proposed by Xavier Glorot
    and Yoshua Bengio. For this, we create an operator called `init` as an object
    of class `GlorotNormal`. Then, we call this operator and provide the desired shape
    of the output tensor:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，接下来我们来看一下如何使用Glorot初始化创建一个`Variable`，这是一种经典的随机初始化方案，由Xavier Glorot和Yoshua
    Bengio提出。为此，我们创建一个名为`init`的操作符，作为`GlorotNormal`类的对象。然后，我们调用这个操作符并提供所需输出张量的形状：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can use this operator to initialize a `Variable` of shape ![](img/B13208_14_004.png):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个操作符来初始化形状为![](img/B13208_14_004.png)的`Variable`：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '**Xavier (or Glorot) initialization**'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '**Xavier（或Glorot）初始化**'
- en: In the early development of deep learning, it was observed that random uniform
    or random normal weight initialization could often result in a poor performance
    of the model during training.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习的早期发展中，人们观察到随机均匀或随机正态权重初始化往往会导致模型在训练期间性能不佳。
- en: In 2010, Glorot and Bengio investigated the effect of initialization and proposed
    a novel, more robust initialization scheme to facilitate the training of deep
    networks. The general idea behind Xavier initialization is to roughly balance
    the variance of the gradients across different layers. Otherwise, some layers
    may get too much attention during training while the other layers lag behind.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年，Glorot和Bengio研究了初始化的效果，并提出了一种新的、更稳健的初始化方案，以促进深度网络的训练。Xavier初始化的基本思想是大致平衡不同层之间的梯度方差。否则，一些层可能在训练中获得过多关注，而其他层则滞后。
- en: 'According to the research paper by Glorot and Bengio, if we want to initialize
    the weights from uniform distribution, we should choose the interval of this uniform
    distribution as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Glorot和Bengio的研究论文，如果我们想要从均匀分布初始化权重，我们应该选择以下均匀分布的区间：
- en: '![](img/B13208_14_005.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_14_005.png)'
- en: Here, ![](img/B13208_14_006.png) is the number of input neurons that are multiplied
    by the weights, and ![](img/B13208_14_007.png) is the number of output neurons
    that feed into the next layer. For initializing the weights from Gaussian (normal)
    distribution, it is recommended that you choose the standard deviation of this
    Gaussian to be ![](img/B13208_14_008.png).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_14_006.png) 是乘以权重的输入神经元数量，![](img/B13208_14_007.png) 是传递到下一层的输出神经元数量。为了从高斯（正态）分布初始化权重，建议选择此高斯分布的标准差为
    ![](img/B13208_14_008.png)。
- en: TensorFlow supports Xavier initialization in both uniform and normal distributions
    of weights.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 支持 Xavier 初始化，既可以使用均匀分布，也可以使用正态分布的权重。
- en: For more information about Glorot and Bengio's initialization scheme, including
    the mathematical derivation and proof, read their original paper (*Understanding
    the difficulty of deep feedforward neural networks*, *Xavier Glorot* and *Yoshua
    Bengio*, 2010), which is freely available at [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Glorot 和 Bengio 的初始化方案的更多信息，包括数学推导和证明，请阅读他们的原始论文（*理解深度前馈神经网络的困难*，*Xavier
    Glorot* 和 *Yoshua Bengio*，2010），该论文可以免费在 [http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf)
    获取。
- en: 'Now, to put this into the context of a more practical use case, let''s see
    how we can define a `Variable` inside the base `tf.Module` class. We will define
    two variables: a trainable one and a non-trainable one:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了将其置于更实际的用例上下文中，让我们看看如何在基础的 `tf.Module` 类中定义 `Variable`。我们将定义两个变量：一个是可训练的，另一个是不可训练的：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see in this code example, subclassing the `tf.Module` class gives
    us direct access to all variables defined in a given object (here, an instance
    of our custom `MyModule` class) via the `.variables` attribute.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在这个代码示例中看到的，子类化 `tf.Module` 类使我们可以通过 `.variables` 属性直接访问给定对象（这里是我们自定义的 `MyModule`
    类的实例）中定义的所有变量。
- en: 'Finally, let''s look at using variables inside a function decorated with `tf.function`.
    When we define a TensorFlow `Variable` inside a normal function (not decorated),
    we might expect that a new `Variable` will be created and initialized each time
    the function is called. However, `tf.function` will try to reuse the `Variable`
    based on tracing and graph creation. Therefore, TensorFlow does not allow the
    creation of a `Variable` inside a decorated function and, as a result, the following
    code will raise an error:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看一下在装饰有 `tf.function` 的函数中使用变量。当我们在普通函数（未装饰）中定义一个 TensorFlow `Variable`
    时，我们可能会预期每次调用该函数时都会创建并初始化一个新的 `Variable`。然而，`tf.function` 会基于追踪和图的创建尝试重用 `Variable`。因此，TensorFlow
    不允许在装饰的函数内部创建 `Variable`，因此，以下代码将引发错误：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'One way to avoid this problem is to define the `Variable` outside of the decorated
    function and use it inside the function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 避免此问题的一种方法是将 `Variable` 定义在装饰的函数外部，并在函数内部使用它：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Computing gradients via automatic differentiation and GradientTape
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过自动微分和 GradientTape 计算梯度
- en: As you already know, optimizing NNs requires computing the gradients of the
    cost with respect to the NN weights. This is required for optimization algorithms
    such as stochastic gradient descent (SGD). In addition, gradients have other applications,
    such as diagnosing the network to find out why an NN model is making a particular
    prediction for a test example. Therefore, in this section, we will cover how to
    compute gradients of a computation with respect to some variables.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经知道的，优化神经网络（NN）需要计算成本函数相对于神经网络权重的梯度。这是优化算法（如随机梯度下降（SGD））所必需的。此外，梯度还有其他应用，如诊断网络，找出为什么神经网络模型会对某个测试样本做出特定的预测。因此，在本节中，我们将讨论如何计算某些变量相对于计算的梯度。
- en: Computing the gradients of the loss with respect to trainable variables
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算损失相对于可训练变量的梯度
- en: TensorFlow supports *automatic differentiation*, which can be thought of as
    an implementation of the *chain rule* for computing gradients of nested functions.
    When we define a series of operations that results in some output or even intermediate
    tensors, TensorFlow provides a context for calculating gradients of these computed
    tensors with respect to its dependent nodes in the computation graph. In order
    to compute these gradients, we have to "record" the computations via `tf.GradientTape`.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 支持 *自动微分*，它可以看作是计算嵌套函数梯度的 *链式法则* 的实现。当我们定义一系列操作，得到某个输出甚至中间张量时，TensorFlow
    提供了一个计算这些张量梯度的上下文，这些梯度是相对于计算图中的依赖节点计算的。为了计算这些梯度，我们需要通过 `tf.GradientTape` 来“记录”这些计算。
- en: 'Let''s work with a simple example where we will compute ![](img/B13208_14_009.png)
    and define the loss as the squared loss between the target and prediction, ![](img/B13208_14_010.png).
    In the more general case, where we may have multiple predictions and targets,
    we compute the loss as the sum of the squared error, ![](img/B13208_14_011.png).
    In order to implement this computation in TensorFlow, we will define the model
    parameters, *w* and *b*, as variables, and the input, *x* and *y*, as tensors.
    We will place the computation of *z* and the loss within the `tf.GradientTape`
    context:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来计算！[](img/B13208_14_009.png)，并将损失定义为目标与预测之间的平方损失，![](img/B13208_14_010.png)。在更一般的情况下，当我们可能有多个预测和目标时，我们将损失计算为平方误差的和，![](img/B13208_14_011.png)。为了在
    TensorFlow 中实现这个计算，我们将定义模型参数，*w* 和 *b*，作为变量，而输入 *x* 和 *y* 作为张量。我们将在 `tf.GradientTape`
    上下文中进行 *z* 和损失的计算：
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When computing the value *z*, we could think of the required operations, which
    we recorded to the "gradient tape," as a forward pass in an NN. We used `tape.gradient`
    to compute ![](img/B13208_14_012.png). Since this is a very simple example, we
    can obtain the derivatives, ![](img/B13208_14_013.png), symbolically to verify
    that the computed gradients match the results we obtained in the previous code
    example:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算 *z* 的值时，我们可以将需要的操作（我们已记录到“梯度带”中）视为神经网络中的前向传播。我们使用 `tape.gradient` 来计算 ![](img/B13208_14_012.png)。由于这是一个非常简单的例子，我们可以象征性地获得导数，![](img/B13208_14_013.png)，以验证计算出的梯度是否与我们在前面的代码示例中得到的结果一致：
- en: '[PRE21]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Understanding automatic differentiation**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解自动微分**'
- en: 'Automatic differentiation represents a set of computational techniques for
    computing derivatives or gradients of arbitrary arithmetic operations. During
    this process, gradients of a computation (expressed as a series of operations)
    are obtained by accumulating the gradients through repeated applications of the
    chain rule. To better understand the concept behind automatic differentiation,
    let''s consider a series of computations, ![](img/B13208_14_014.png), with input
    *x* and output *y*. This can be broken into a series of steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 自动微分代表了一组计算技术，用于计算任意算术操作的导数或梯度。在这个过程中，通过重复应用链式法则来积累梯度，从而得到计算（表示为一系列操作）的梯度。为了更好地理解自动微分的概念，让我们考虑一系列计算，![](img/B13208_14_014.png)，其中输入为
    *x*，输出为 *y*。这可以分解为以下几个步骤：
- en: '![](img/B13208_14_015.png)'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_14_015.png)'
- en: '![](img/B13208_14_016.png)'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_14_016.png)'
- en: '![](img/B13208_14_017.png)'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_14_017.png)'
- en: '![](img/B13208_14_018.png)'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_14_018.png)'
- en: 'The derivative ![](img/B13208_14_019.png) can be computed in two different
    ways: forward accumulation, which starts with ![](img/B13208_14_020.png), and
    reverse accumulation, which starts with ![](img/B13208_14_021.png) Note that TensorFlow
    uses the latter, reverse accumulation.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 导数 ![](img/B13208_14_019.png) 可以通过两种不同的方式计算：前向积累，从 ![](img/B13208_14_020.png)
    开始，和反向积累，从 ![](img/B13208_14_021.png) 开始。请注意，TensorFlow 使用的是后者，即反向积累。
- en: Computing gradients with respect to non-trainable tensors
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算相对于不可训练张量的梯度
- en: '`tf.GradientTape` automatically supports the gradients for trainable variables.
    However, for non-trainable variables and other `Tensor` objects, we need to add
    an additional modification to the `GradientTape` called `tape.watch()` to monitor
    those as well. For example, if we are interested in computing ![](img/B13208_14_022.png),
    the code will be as follows:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.GradientTape` 自动支持可训练变量的梯度。但是，对于不可训练变量和其他 `Tensor` 对象，我们需要对 `GradientTape`
    进行额外修改，称为 `tape.watch()`，以便监视它们。例如，如果我们想计算 ![](img/B13208_14_022.png)，代码如下：'
- en: '[PRE22]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '**Adversarial examples**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**对抗样本**'
- en: Computing gradients of the loss with respect to the input example is used for
    generating *adversarial examples* (or *adversarial attacks*). In computer vision,
    adversarial examples are examples that are generated by adding some small imperceptible
    noise (or perturbations) to the input example, which results in a deep NN misclassifying
    them. Covering adversarial examples is beyond the scope of this book, but if you
    are interested, you can find the original paper by Christian Szegedy et al., titled
    *Intriguing properties of neural networks,* at [https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 计算损失相对于输入示例的梯度用于生成*对抗样本*（或*对抗攻击*）。在计算机视觉中，对抗样本是通过向输入示例添加一些微小但不可察觉的噪音（或扰动）而生成的示例，这导致深度神经网络对其进行错误分类。覆盖对抗样本超出了本书的范围，但如果您感兴趣，可以在Christian
    Szegedy等人撰写的原始论文《神经网络的有趣性质》中找到，标题为*Intriguing properties of neural networks*，网址为[https://arxiv.org/pdf/1312.6199.pdf](https://arxiv.org/pdf/1312.6199.pdf)。
- en: Keeping resources for multiple gradient computations
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保持多个梯度计算的资源
- en: 'When we monitor the computations in the context of `tf.GradientTape`, by default,
    the tape will keep the resources only for a single gradient computation. For instance,
    after calling `tape.gradient()` once, the resources will be released and the tape
    will be cleared. Hence, if we want to compute more than one gradient, for example,
    both ![](img/B13208_14_023.png) and ![](img/B13208_14_024.png), we need to make
    the tape persistent:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在`tf.GradientTape`的上下文中监视计算时，默认情况下，磁带将仅保留单次梯度计算的资源。例如，调用一次`tape.gradient()`后，资源将被释放并清除磁带。因此，如果我们要计算多个梯度，例如，![](img/B13208_14_023.png)和![](img/B13208_14_024.png)，我们需要使磁带持久化：
- en: '[PRE23]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: However, keep in mind that this is only needed when we want to compute more
    than one gradient, as recording and keeping the gradient tape is less memory-efficient
    compared to releasing the memory after a single gradient computation. This is
    also why the default setting is `persistent=False`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，请记住，这仅在我们想要计算多个梯度时才需要，因为记录并保留梯度磁带比在单次梯度计算后释放内存不那么内存高效。这也是为什么默认设置为`persistent=False`的原因。
- en: 'Finally, if we are computing gradients of a loss term with respect to the parameters
    of a model, we can define an optimizer and apply the gradients to optimize the
    model parameters using the `tf.keras` API, as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们正在计算损失项相对于模型参数的梯度，我们可以定义一个优化器，并使用`tf.keras` API应用梯度来优化模型参数，如下所示：
- en: '[PRE24]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You will recall that the initial weight and bias unit were *w* = 1.0 and *b*
    = 0.5, and applying the gradients of the loss with respect to the model parameters
    changed the model parameters to *w* = 1.0056 and *b* = 0.504.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得初始权重和偏置单元分别为*w* = 1.0和*b* = 0.5，应用于损失相对于模型参数的梯度后，模型参数变为*w* = 1.0056和*b*
    = 0.504。
- en: Simplifying implementations of common architectures via the Keras API
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Keras API简化常见架构的实现
- en: 'You have already seen some examples of building a feedforward NN model (for
    instance, a multilayer perceptron) and defining a sequence of layers using Keras''
    `Sequential` class. Before we look at different approaches for configuring those
    layers, let''s briefly recap the basic steps by building a model with two densely
    (fully) connected layers:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了构建前馈神经网络模型的一些示例（例如，多层感知器），并使用Keras的`Sequential`类定义了一系列层。在我们探讨配置这些层的不同方法之前，让我们简要回顾一下通过构建一个具有两个全连接层的模型的基本步骤：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We specified the input shape with `model.build()`, instantiating the variables
    after defining the model for that particular shape. The number of parameters of
    each layer is displayed: ![](img/B13208_14_025.png) for the first layer, and ![](img/B13208_14_026.png)
    for the second layer. Once variables (or model parameters) are created, we can
    access both trainable and non-trainable variables as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`model.build()`指定了输入形状，在为该特定形状定义模型后实例化变量。每层的参数数量显示如下：![](img/B13208_14_025.png)是第一层，![](img/B13208_14_026.png)是第二层。一旦创建变量（或模型参数），我们可以访问可训练和不可训练变量如下：
- en: '[PRE26]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this case, each layer has a weight matrix called `kernel` as well as a bias
    vector.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每个层都有一个称为`kernel`的权重矩阵以及一个偏置向量。
- en: 'Next, let''s configure these layers, for example, by applying different activation
    functions, variable initializers, or regularization methods to the parameters.
    A comprehensive and complete list of available options for these categories can
    be found in the official documentation:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们配置这些层，例如，通过为参数应用不同的激活函数、变量初始化器或正则化方法。这些类别的所有可用选项的完整列表可以在官方文档中找到：
- en: 'Choosing activation functions via `tf.keras.activations`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `tf.keras.activations` 选择激活函数：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations)
- en: 'Initializing the layer parameters via `tf.keras.initializers`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers)'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `tf.keras.initializers` 初始化层参数：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/initializers)
- en: 'Applying regularization to the layer parameters (to prevent overfitting) via
    `tf.keras.regularizers`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers)'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `tf.keras.regularizers` 将正则化应用于层参数（以防止过拟合）：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/regularizers)
- en: 'In the following code example, we will configure the first layer by specifying
    initializers for the kernel and bias variables. Then, we will configure the second
    layer by specifying an L1 regularizer for the kernel (weight matrix):'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们将通过为内核和偏差变量指定初始化器来配置第一层。然后，我们将通过为内核（权重矩阵）指定 L1 正则化器来配置第二层：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Furthermore, in addition to configuring the individual layers, we can also
    configure the model when we compile it. We can specify the type of optimizer and
    the loss function for training, as well as which metrics to use for reporting
    the performance on the training, validation, and test datasets. Again, a comprehensive
    list of all available options can be found in the official documentation:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，除了配置单独的层，我们还可以在编译模型时对其进行配置。我们可以为训练指定优化器的类型和损失函数，以及为报告训练、验证和测试数据集的性能指定哪些指标。再次提醒，所有可用选项的完整列表可以在官方文档中找到：
- en: 'Optimizers via `tf.keras.optimizers`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers)'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `tf.keras.optimizers` 获取优化器：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers)
- en: 'Loss functions via `tf.keras.losses`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses)'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `tf.keras.losses` 获取损失函数：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses)
- en: 'Performance metrics via `tf.keras.metrics`: [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics)'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `tf.keras.metrics` 获取性能指标：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/metrics)
- en: '**Choosing a loss function**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**选择损失函数**'
- en: Regarding the choices for optimization algorithms, SGD and Adam are the most
    widely used methods. The choice of loss function depends on the task; for example,
    you might use mean square error loss for a regression problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关于优化算法的选择，SGD 和 Adam 是最广泛使用的方法。损失函数的选择取决于任务；例如，对于回归问题，您可能会使用均方误差损失函数。
- en: The family of cross-entropy loss functions supplies the possible choices for
    classification tasks, which are extensively discussed in *Chapter 15*, *Classifying
    Images with Deep Convolutional Neural Networks*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉熵损失函数家族为分类任务提供了可能的选择，这些内容在*第 15 章*《使用深度卷积神经网络进行图像分类》中有广泛讨论。
- en: Furthermore, you can use the techniques you have learned from previous chapters
    (for example, techniques for model evaluation from *Chapter 6*, *Learning Best
    Practices for Model Evaluation and Hyperparameter Tuning*) combined with the appropriate
    metrics for the problem. For example, precision and recall, accuracy, area under
    the curve (AUC), and false negative and false positive scores are appropriate
    metrics for evaluating classification models.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以使用在前几章学到的技巧（例如，*第 6 章：模型评估的最佳实践与超参数调优* 中的模型评估技巧），结合问题的适当评估指标。例如，精度、召回率、准确率、曲线下面积（AUC），以及假阴性和假阳性评分，都是评估分类模型的适当指标。
- en: 'In this example, we will compile the model using the SGD optimizer, cross-entropy
    loss for binary classification, and a specific list of metrics, including accuracy,
    precision, and recall:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 SGD 优化器、交叉熵损失（适用于二分类）和一系列特定的评估指标来编译模型，其中包括准确率、精度和召回率：
- en: '[PRE28]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: When we train this model by calling `model.fit(...)`, the history of the loss
    and the specified metrics for evaluating training and validation performance (if
    a validation dataset is used) will be returned, which can be used to diagnose
    the learning behavior.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过调用 `model.fit(...)` 来训练这个模型时，将返回包含损失值和指定评估指标的历史记录（如果使用了验证数据集），这些信息可以帮助诊断学习行为。
- en: 'Next, we will look at a more practical example: solving the classic XOR classification
    problem using the Keras API. First, we will use the `tf.keras.Sequential()` class
    to build the model. Along the way, you will also learn about the capacity of a
    model for handling nonlinear decision boundaries. Then, we will cover other ways
    of building a model that will give us more flexibility and control over the layers
    of the network.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一个更实际的例子：使用 Keras API 解决经典的 XOR 分类问题。首先，我们将使用 `tf.keras.Sequential()`
    类构建模型。在这个过程中，你还将了解模型处理非线性决策边界的能力。然后，我们将介绍其他构建模型的方式，这些方式将为我们提供更多的灵活性和对网络层的控制。
- en: Solving an XOR classification problem
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决 XOR 分类问题
- en: 'The XOR classification problem is a classic problem for analyzing the capacity
    of a model with regard to capturing the nonlinear decision boundary between two
    classes. We generate a toy dataset of 200 training examples with two features
    ![](img/B13208_14_027.png) drawn from a uniform distribution between ![](img/B13208_14_028.png).
    Then, we assign the ground truth label for training example *i* according to the
    following rule:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: XOR 分类问题是分析模型捕捉两个类别之间非线性决策边界能力的经典问题。我们生成了一个包含 200 个训练示例的玩具数据集，具有两个特征 ![](img/B13208_14_027.png)，这些特征是从
    ![](img/B13208_14_028.png) 之间的均匀分布中抽取的。然后，我们根据以下规则为训练示例 *i* 分配真实标签：
- en: '![](img/B13208_14_029.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_14_029.png)'
- en: 'We will use half of the data (100 training examples) for training and the remaining
    half for validation. The code for generating the data and splitting it into the
    training and validation datasets is as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一半的数据（100 个训练示例）用于训练，另一半用于验证。生成数据并将其拆分为训练集和验证集的代码如下：
- en: '[PRE29]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The code results in the following scatterplot of the training and validation
    examples, shown with different markers based on their class label:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的结果是以下散点图，展示了不同类别标签的训练和验证示例，且根据其类别标签使用不同的标记表示：
- en: '![](img/13208_14_02.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_02.png)'
- en: 'In the previous subsection, we covered the essential tools that we need to
    implement a classifier in TensorFlow. We now need to decide what architecture
    we should choose for this task and dataset. As a general rule of thumb, the more
    layers we have, and the more neurons we have in each layer, the larger the capacity
    of the model will be. Here, the model capacity can be thought of as a measure
    of how readily the model can approximate complex functions. While having more
    parameters means the network can fit more complex functions, larger models are
    usually harder to train (and prone to overfitting). In practice, it is always
    a good idea to start with a simple model as a base line, for example, a single-layer
    NN like logistic regression:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一小节中，我们介绍了在 TensorFlow 中实现分类器所需的基本工具。现在，我们需要决定应该选择什么架构来处理此任务和数据集。作为一个一般规则，层数越多，每层神经元数量越多，模型的容量就越大。在这里，模型容量可以看作是衡量模型如何轻松逼近复杂函数的一个标准。尽管更多的参数意味着网络能够拟合更复杂的函数，但较大的模型通常更难训练（并且容易过拟合）。实际上，最好从一个简单的模型开始作为基线，例如，像逻辑回归这样的单层神经网络（NN）：
- en: '[PRE30]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The total size of the parameters for this simple logistic regression model
    is 3: a weight matrix (or kernel) of size ![](img/B13208_14_030.png) and a bias
    vector of size 1\. After defining the model, we will compile the model and train
    it for 200 epochs using a batch size of 2:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的逻辑回归模型的参数总大小为3：一个大小为 ![](img/B13208_14_030.png) 的权重矩阵（或内核）和一个大小为1的偏置向量。定义好模型后，我们将编译模型，并用批量大小为2训练200个周期：
- en: '[PRE31]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Notice that `model.fit()` returns a history of training epochs, which is useful
    for visual inspection after training. In the following code, we will plot the
    learning curves, including the training and validation loss, as well as their
    accuracies.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`model.fit()`返回的是训练周期的历史记录，这对于训练后进行可视化检查非常有用。在以下代码中，我们将绘制学习曲线，包括训练损失、验证损失及其准确率。
- en: We will also use the MLxtend library to visualize the validation data and the
    decision boundary.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用MLxtend库来可视化验证数据和决策边界。
- en: 'MLxtend can be installed via `conda` or `pip` as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过`conda`或`pip`安装MLxtend，如下所示：
- en: '[PRE32]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following code will plot the training performance along with the decision
    region bias:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将绘制训练表现以及决策区域的偏差：
- en: '[PRE33]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This results in the following figure, with three separate panels for the losses,
    accuracies, and the scatterplot of the validation examples, along with the decision
    boundary:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下图形，展示了损失、准确率以及验证示例的散点图，同时包括决策边界：
- en: '![](img/13208_14_03.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_03.png)'
- en: As you can see, a simple model with no hidden layer can only derive a linear
    decision boundary, which is unable to solve the XOR problem. As a consequence,
    we can observe that the loss terms for both the training and the validation datasets
    are very high, and the classification accuracy is very low.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，一个没有隐藏层的简单模型只能得出一个线性决策边界，无法解决XOR问题。因此，我们可以观察到训练集和验证集的损失值都非常高，分类准确率也很低。
- en: In order to derive a nonlinear decision boundary, we can add one or more hidden
    layers connected via nonlinear activation functions. The universal approximation
    theorem states that a feedforward NN with a single hidden layer and a relatively
    large number of hidden units can approximate arbitrary continuous functions relatively
    well. Thus, one approach for tackling the XOR problem more satisfactorily is to
    add a hidden layer and compare different numbers of hidden units until we observe
    satisfactory results on the validation dataset. Adding more hidden units would
    correspond to increasing the width of a layer.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了得出非线性决策边界，我们可以添加一个或多个通过非线性激活函数连接的隐藏层。通用逼近定理指出，一个具有单个隐藏层和相对较多隐藏单元的前馈神经网络（NN）可以相对较好地逼近任意连续函数。因此，解决XOR问题的一种更令人满意的方法是添加一个隐藏层，并比较不同数量的隐藏单元，直到我们在验证集上观察到满意的结果。添加更多隐藏单元相当于增加一个层的宽度。
- en: Alternatively, we can also add more hidden layers, which will make the model
    deeper. The advantage of making a network deeper rather than wider is that fewer
    parameters are required to achieve a comparable model capacity. However, a downside
    of deep (versus wide) models is that deep models are prone to vanishing and exploding
    gradients, which make them harder to train.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以添加更多的隐藏层，这将使模型更深。使网络更深而不是更宽的优势在于，达到相当的模型容量所需的参数较少。然而，深度（与宽度）模型的一个缺点是，深度模型容易出现梯度消失和爆炸问题，这使得它们更难训练。
- en: 'As an exercise, try adding one, two, three, and four hidden layers, each with
    four hidden units. In the following example, we will take a look at the results
    of a feedforward NN with three hidden layers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个练习，尝试添加一、二、三和四个隐藏层，每个层包含四个隐藏单元。在以下示例中，我们将查看具有三个隐藏层的前馈神经网络（NN）结果：
- en: '[PRE34]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We can repeat the previous code for visualization, which produces the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重复前面的代码进行可视化，得到以下结果：
- en: '![](img/13208_14_04.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_04.png)'
- en: Now, we can see that the model is able to derive a nonlinear decision boundary
    for this data, and the model reaches 100 percent accuracy on the training dataset.
    The validation dataset's accuracy is 95 percent, which indicates that the model
    is slightly overfitting.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到模型能够为这些数据得出一个非线性的决策边界，并且该模型在训练集上的准确率达到了100%。验证集的准确率为95%，这表明模型有轻微的过拟合现象。
- en: Making model building more flexible with Keras' functional API
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Keras的函数式API让模型构建更具灵活性
- en: In the previous example, we used the Keras `Sequential` class to create a fully
    connected NN with multiple layers. This is a very common and convenient way of
    building models. However, it unfortunately doesn't allow us to create more complex
    models that have multiple input, output, or intermediate branches. That's where
    Keras' so-called functional API comes in handy.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的示例中，我们使用了 Keras 的 `Sequential` 类来创建一个具有多层的全连接神经网络（NN）。这是一种非常常见且便捷的构建模型的方式。然而，它不允许我们创建具有多个输入、输出或中间分支的更复杂模型。在这种情况下，Keras
    的所谓功能性 API 就显得非常有用。
- en: 'To illustrate how the functional API can be used, we will implement the same
    architecture that we built using the objected-oriented (`Sequential`) approach
    in the previous section; however, this time, we will use the functional approach.
    In this approach, we first specify the input. Then, the hidden layers are constructed,
    with their outputs named `h1`, `h2`, and `h3`. For this problem, we use the output
    of each layer as the input to the succedent layer (note that if you are building
    more complex models that have multiple branches, this may not be the case, but
    it can still be done via the functional API). Finally, we specify the output as
    the final dense layer that receives `h3` as input. The code for this is as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何使用功能性 API，我们将实现与上一节中使用面向对象（`Sequential`）方法构建的相同架构；不过这次我们将使用功能性方法。在这种方法中，我们首先指定输入。然后，构建隐藏层，并将它们的输出命名为
    `h1`、`h2` 和 `h3`。对于这个问题，我们将每一层的输出作为下一层的输入（注意，如果你正在构建更复杂的模型，有多个分支，这种方法可能不适用，但通过功能性
    API 仍然可以做到）。最后，我们指定输出为接收 `h3` 作为输入的最终全连接层。代码如下：
- en: '[PRE35]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Compiling and training this model is similar to what we did previously:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 编译和训练该模型与我们之前做的类似：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Implementing models based on Keras' Model class
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于 Keras 的 Model 类实现模型
- en: 'An alternative way to build complex models is by subclassing `tf.keras.Model`.
    In this approach, we create a new class derived from `tf.keras.Model` and define
    the function, `__init__()`, as a constructor. The `call()` method is used to specify
    the forward pass. In the constructor function, `__init__()`, we define the layers
    as attributes of the class so that they can be accessed via the `self` reference
    attribute. Then, in the `call()` method, we specify how these layers are to be
    used in the forward pass of the NN. The code for defining a new class that implements
    the previous model is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 构建复杂模型的另一种方式是通过子类化 `tf.keras.Model`。在这种方法中，我们创建一个从 `tf.keras.Model` 派生的新类，并将
    `__init__()` 函数定义为构造函数。`call()` 方法用于指定前向传播。在构造函数 `__init__()` 中，我们将层定义为类的属性，以便通过
    `self` 引用访问它们。然后，在 `call()` 方法中，我们指定这些层如何在神经网络的前向传播中使用。定义实现前面模型的新类的代码如下：
- en: '[PRE37]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Notice that we used the same output name, `h`, for all hidden layers. This makes
    the code more readable and easier to follow.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们为所有隐藏层使用了相同的输出名称 `h`。这样使得代码更加可读和易于跟随。
- en: 'A model class derived from `tf.keras.Model` through subclassing inherits general
    model attributes, such as `build()`, `compile()`, and `fit()`. Therefore, once
    we define an instance of this new class, we can compile and train it like any
    other model built by Keras:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `tf.keras.Model` 类派生的模型类通过继承获得了一般的模型属性，如 `build()`、`compile()` 和 `fit()`。因此，一旦我们定义了这个新类的实例，就可以像使用
    Keras 构建的其他模型一样编译和训练它：
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Writing custom Keras layers
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写自定义 Keras 层
- en: In cases where we want to define a new layer that is not already supported by
    Keras, we can define a new class derived from the `tf.keras.layers.Layer` class.
    This is especially useful when designing a new layer or customizing an existing
    layer.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想定义一个 Keras 不支持的新层时，我们可以定义一个从 `tf.keras.layers.Layer` 类派生的新类。这对于设计新的层或定制现有层尤其有用。
- en: To illustrate the concept of implementing custom layers, let's consider a simple
    example. Imagine we want to define a new linear layer that computes ![](img/B13208_14_031.png),
    where ![](img/B13208_14_032.png) refers to a random variable as a noise variable.
    To implement this computation, we define a new class as a subclass of `tf.keras.layers.Layer`.
    For this new class, we have to define both the constructor `__init__()` method
    and the `call()` method. In the constructor, we define the variables and other
    required tensors for our customized layer. We have the option to create variables
    and initialize them in the constructor if the `input_shape` is given to the constructor.
    Alternatively, we can delay the variable initialization (for instance, if we do
    not know the exact input shape upfront) and delegate it to the `build()` method
    for late variable creation. In addition, we can define `get_config()` for serialization,
    which means that a model using our custom layer can be efficiently saved using
    TensorFlow's model saving and loading capabilities.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明实现自定义层的概念，让我们考虑一个简单的例子。假设我们想定义一个新的线性层，它计算 ![](img/B13208_14_031.png)，其中
    ![](img/B13208_14_032.png) 代表一个作为噪声变量的随机变量。为了实现这个计算，我们定义一个新的类，作为 `tf.keras.layers.Layer`
    的子类。对于这个新类，我们必须同时定义构造函数 `__init__()` 方法和 `call()` 方法。在构造函数中，我们为我们的自定义层定义变量和其他所需的张量。如果构造函数给定了
    `input_shape`，我们可以选择在构造函数中创建变量并初始化它们。或者，如果我们事先不知道确切的输入形状，我们可以延迟变量初始化，并将其委托给 `build()`
    方法进行后期创建。此外，我们可以定义 `get_config()` 来进行序列化，这意味着使用我们自定义层的模型可以通过 TensorFlow 的模型保存和加载功能有效地保存。
- en: 'To look at a concrete example, we are going to define a new layer called `NoisyLinear`,
    which implements the computation ![](img/B13208_14_033.png), which was mentioned
    in the preceding paragraph:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看一个具体的例子，我们将定义一个新的层，名为 `NoisyLinear`，它实现前面段落中提到的计算 ![](img/B13208_14_033.png)：
- en: '[PRE39]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Before we go a step further and use our custom `NoisyLinear` layer in a model,
    let's test it in the context of a simple example.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更进一步并在模型中使用自定义的 `NoisyLinear` 层之前，让我们在一个简单的例子中测试它。
- en: 'In the following code, we will define a new instance of this layer, initialize
    it by calling `.build()`, and execute it on an input tensor. Then, we will serialize
    it via `.get_config()` and restore the serialized object via `.from_config()`:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们将定义这个层的新实例，通过调用 `.build()` 初始化它，并在输入张量上执行它。然后，我们将通过 `.get_config()`
    序列化它，并通过 `.from_config()` 恢复序列化的对象：
- en: '[PRE40]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s create a new model similar to the previous one for solving the
    XOR classification task. As before, we will use Keras'' `Sequential` class, but
    this time, we will use our `NoisyLinear` layer as the first hidden layer of the
    multilayer perceptron. The code is as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个类似于之前的模型，用于解决 XOR 分类任务。和之前一样，我们将使用 Keras 的 `Sequential` 类，但这次我们将使用
    `NoisyLinear` 层作为多层感知机的第一个隐藏层。代码如下：
- en: '[PRE41]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The resulting figure will be as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图形将如下所示：
- en: '![](img/13208_14_05.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_05.png)'
- en: Here, our goal was to learn how to define a new custom layer subclassed from
    `tf.keras.layers.Layer` and to use it as we would use any other standard Keras
    layer. Although, with this particular example, `NoisyLinear` did not help to improve
    the performance, please keep in mind that our objective was to mainly learn how
    to write a customized layer from scratch. In general, writing a new customized
    layer can be useful in other applications, for example, if you develop a new algorithm
    that depends on a new layer beyond the existing ones.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的目标是学习如何定义一个从 `tf.keras.layers.Layer` 子类化的新自定义层，并像使用其他标准 Keras 层一样使用它。虽然在这个特定的例子中，`NoisyLinear`
    并没有帮助提高性能，但请记住，我们的主要目标是学习如何从头编写一个自定义层。通常，编写新的自定义层在其他应用中是有用的，例如，如果你开发了一个依赖于新层的算法，而这个新层超出了现有的层的范畴。
- en: TensorFlow Estimators
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 估算器
- en: So far, in this chapter, we have mostly focused on the low-level TensorFlow
    API. We used decorators to modify functions to compile the computational graphs
    explicitly for computational efficiency. Then, we worked with the Keras API and
    implemented feedforward NNs, to which we added customized layers. In this section,
    we will switch gears and work with TensorFlow Estimators. The `tf.estimator` API
    encapsulates the underlying steps in machine learning tasks, such as training,
    prediction (inference), and evaluation. Estimators are more encapsulated but also
    more scalable when compared to the previous approaches that we have covered in
    this chapter. Also, the `tf.estimator` API adds support for running models on
    multiple platforms without requiring major code changes, which makes them more
    suitable for the so-called "production phase" in industry applications. In addition,
    TensorFlow comes with a selection of off-the-shelf estimators for common machine
    learning and deep learning architectures that are useful for comparison studies,
    for example, to quickly assess whether a certain approach is applicable to a particular
    dataset or problem.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中我们大多集中于低级别的TensorFlow API。我们使用装饰器修改函数，以显式地编译计算图，以提高计算效率。然后，我们使用了Keras
    API并实现了前馈神经网络，并为其添加了自定义层。在本节中，我们将转换思路，使用TensorFlow Estimators。`tf.estimator` API封装了机器学习任务中的基本步骤，如训练、预测（推理）和评估。与本章前面讲解的其他方法相比，Estimators封装性更强，同时也更具可扩展性。此外，`tf.estimator`
    API还支持在多个平台上运行模型，而无需进行重大代码更改，这使得它们更适合工业应用中的所谓“生产阶段”。另外，TensorFlow还提供了一些现成的Estimators，用于常见的机器学习和深度学习架构，适用于比较研究，例如快速评估某种方法是否适用于特定的数据集或问题。
- en: In the remaining sections of this chapter, you will learn how to use such pre-made
    Estimators and how to create an Estimator from an existing Keras model. One of
    the essential elements of Estimators is defining the feature columns as a mechanism
    for importing data into an Estimator-based model, which we will cover in the next
    section.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后续部分，你将学习如何使用这些预制的Estimators，以及如何从现有的Keras模型创建Estimator。Estimator的一个关键要素是定义特征列，这是将数据导入基于Estimator的模型的一种机制，下一节我们将详细讲解。
- en: Working with feature columns
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用特征列
- en: 'In machine learning and deep learning applications, we can encounter various
    different types of features: continuous, unordered categorical (nominal), and
    ordered categorical (ordinal). You will recall that in *Chapter 4*, *Building
    Good Training Datasets – Data Preprocessing*, we covered different types of features
    and learned how to handle each type. Note that while numeric data can be either
    continuous or discrete, in the context of the TensorFlow API, "numeric" data specifically
    refers to continuous data of the floating point type.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习和深度学习应用中，我们可能会遇到各种不同类型的特征：连续型、无序类别型（名义型）和有序类别型（顺序型）。你会记得在*第4章*，*构建良好的训练数据集
    – 数据预处理*中，我们讲解了不同类型的特征，并学习了如何处理每种类型。需要注意的是，虽然数值数据可以是连续型或离散型，但在TensorFlow API的语境下，“数值”数据特指浮动点类型的连续数据。
- en: 'Sometimes, feature sets are comprised of a mixture of different feature types.
    While TensorFlow Estimators were designed to handle all these different types
    of features, we must specify how each feature should be interpreted by the Estimator.
    For example, consider a scenario with a set of seven different features, as shown
    in the following figure:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，特征集由多种不同类型的特征组成。尽管TensorFlow Estimators被设计用来处理这些不同类型的特征，但我们必须指定每个特征应该如何被Estimator解释。例如，考虑以下图示中的七个不同特征：
- en: '![](img/13208_14_06.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_06.png)'
- en: The features shown in the figure (model year, cylinders, displacement, horsepower,
    weight, acceleration, and origin) were obtained from the Auto MPG dataset, which
    is a common machine learning benchmark dataset for predicting the fuel efficiency
    of a car in miles per gallon (MPG). The full dataset and its description are available
    from UCI's machine learning repository at [https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图中显示的特征（车型年份、气缸数、排量、马力、重量、加速度和来源）来自Auto MPG数据集，这是一个常见的机器学习基准数据集，用于预测汽车的油耗（每加仑多少英里）。完整的数据集及其描述可以通过UCI的机器学习仓库访问，网址为[https://archive.ics.uci.edu/ml/datasets/auto+mpg](https://archive.ics.uci.edu/ml/datasets/auto+mpg)。
- en: We are going to treat five features from the Auto MPG dataset (number of cylinders,
    displacement, horsepower, weight, and acceleration) as "numeric" (here, continuous)
    features. The model year can be regarded as an ordered categorical (ordinal) feature.
    Lastly, the manufacturing origin can be regarded as an unordered categorical (nominal)
    feature with three possible discrete values, 1, 2, and 3, which correspond to
    the US, Europe, and Japan, respectively.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从Auto MPG数据集中选择五个特征（气缸数、排量、马力、重量和加速度）作为“数值型”（即连续）特征。车型年份可以视为有序的类别（ordinal）特征。最后，制造原产地可以视为无序的类别（nominal）特征，具有三个可能的离散值，1、2和3，分别对应美国、欧洲和日本。
- en: 'Let''s first load the data and apply the necessary preprocessing steps, such
    as partitioning the dataset into training and test datasets, as well as standardizing
    the continuous features:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 首先加载数据并应用必要的预处理步骤，如将数据集划分为训练集和测试集，以及标准化连续特征：
- en: '[PRE42]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This results in the following:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到如下结果：
- en: '![](img/13208_14_07.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13208_14_07.png)'
- en: '[PRE43]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Next, let''s group the rather fine-grained model year information into buckets
    to simplify the learning task for the model that we are going to train later.
    Concretely, we are going to assign each car into one of four "year" buckets, as
    follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将较为精细的车型年份信息分组到桶中，以简化我们稍后将要训练的模型的学习任务。具体来说，我们将把每辆车分配到以下四个“年份”桶中的一个：
- en: '![](img/B13208_14_036.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_14_036.png)'
- en: 'Note that the chosen intervals were selected arbitrarily to illustrate the
    concepts of "bucketing." In order to group the cars into these buckets, we will
    first define a numeric feature based on each original model year. Then, these
    numeric features will be passed to the `bucketized_column` function for which
    we will specify three interval cut-off values: [73, 76, 79]. The specified values
    include the right cut-off value. These cut-off values are used to specify half-closed
    intervals, for instance, ![](img/B13208_14_037.png), ![](img/B13208_14_038.png),
    ![](img/B13208_14_039.png), and ![](img/B13208_14_040.png). The code is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，选择的区间是为了说明“分桶”（bucketing）概念而任意选择的。为了将汽车分组到这些桶中，我们将首先根据每个原始车型年份定义一个数值特征。然后，这些数值特征将传递给`bucketized_column`函数，我们将为其指定三个区间切割值：[73,
    76, 79]。指定的值包括右侧切割值。这些切割值用于指定半闭区间，例如，![](img/B13208_14_037.png)，![](img/B13208_14_038.png)，![](img/B13208_14_039.png)，和![](img/B13208_14_040.png)。代码如下：
- en: '[PRE45]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: For consistency, we added this bucketized feature column to a Python list, even
    though the list consists of only one entry. In the following steps, we will merge
    this list with the lists made from other features, which will then be provided
    as input to the TensorFlow Estimator-based model.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持一致性，我们将此分桶特征列添加到一个Python列表中，尽管该列表只有一个条目。在接下来的步骤中，我们将把这个列表与其他特征列表合并，然后将合并后的结果作为输入提供给基于TensorFlow
    Estimator的模型。
- en: Next, we will proceed with defining a list for the unordered categorical feature,
    `Origin`. In TensorFlow, there are different ways of creating a categorical feature
    column. If the data contains the category names (for example, in string format
    like "US," "Europe," and "Japan"), then we can use `tf.feature_column.categorical_column_with_vocabulary_list`
    and provide a list of unique, possible category names as input. If the list of
    possible categories is too large, for example, in a typical text analysis context,
    then we can use `tf.feature_column.categorical_column_with_vocabulary_file` instead.
    When using this function, we simply provide a file that contains all the categories/words
    so that we do not have to store a list of all possible words in memory. Moreover,
    if the features are already associated with an index of categories in the range
    [0, `num_categories`), then we can use the `tf.feature_column.categorical_column_with_identity`
    function. However, in this case, the feature `Origin` is given as integer values
    1, 2, 3 (as opposed to 0, 1, 2), which does not match the requirement for categorical
    indexing, as it expects the indices to start from 0.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续为无序的分类特征 `Origin` 定义一个列表。在 TensorFlow 中，有多种方式可以创建分类特征列。如果数据包含类别名称（例如，像“US”，“Europe”和“Japan”这样的字符串格式），则可以使用
    `tf.feature_column.categorical_column_with_vocabulary_list` 并提供一个唯一的类别名称列表作为输入。如果可能的类别列表过大，例如，在典型的文本分析上下文中，可以使用
    `tf.feature_column.categorical_column_with_vocabulary_file`。使用此函数时，我们只需提供一个包含所有类别/单词的文件，这样就无需将所有可能的单词列表存储在内存中。此外，如果特征已经与类别索引相关联，索引范围为
    [0, `num_categories`)，则可以使用 `tf.feature_column.categorical_column_with_identity`
    函数。然而，在这种情况下，特征 `Origin` 被表示为整数值 1，2，3（而不是 0，1，2），这与分类索引的要求不匹配，因为它期望索引从 0 开始。
- en: 'In the following code example, we will proceed with the vocabulary list:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们将使用词汇表列表：
- en: '[PRE46]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Certain Estimators, such as `DNNClassifier` and `DNNRegressor`, only accept
    so-called "dense columns." Therefore, the next step is to convert the existing
    categorical feature column to such a dense column. There are two ways to do this:
    using an embedding column via `embedding_column` or an indicator column via `indicator_column`.
    An indicator column converts the categorical indices to one-hot encoded vectors,
    for example, index 0 will be encoded as [1, 0, 0], index 1 will be encoded as
    [0, 1, 0], and so on. On the other hand, the embedding column maps each index
    to a vector of random number of the type `float`, which can be trained.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 某些 Estimators，如 `DNNClassifier` 和 `DNNRegressor`，仅接受所谓的“稠密列”。因此，下一步是将现有的分类特征列转换为这种稠密列。有两种方法可以做到这一点：通过
    `embedding_column` 使用嵌入列，或通过 `indicator_column` 使用指示符列。指示符列将分类索引转换为独热编码向量，例如，索引
    0 会被编码为 [1, 0, 0]，索引 1 会被编码为 [0, 1, 0]，依此类推。另一方面，嵌入列将每个索引映射到一个随机数向量，类型为 `float`，该向量可以训练。
- en: 'When the number of categories is large, using the embedding column with fewer
    dimensions than the number of categories can improve the performance. In the following
    code snippet, we will use the indicator column approach on the categorical feature
    in order to convert it into the dense format:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当类别数量较多时，使用维度少于类别数量的嵌入列可以提高性能。在以下代码片段中，我们将使用指示符列方法处理分类特征，以将其转换为稠密格式：
- en: '[PRE47]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: In this section, we have covered the most common approaches for creating feature
    columns that can be used with TensorFlow Estimators. However, there are several
    additional feature columns that we haven't discussed, including hashed columns
    and crossed columns. More information about these other feature columns can be
    found in the official TensorFlow documentation at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经介绍了几种常见的创建特征列的方法，这些特征列可以与 TensorFlow Estimators 配合使用。然而，还有一些额外的特征列我们没有讨论，包括哈希列和交叉列。有关这些其他特征列的更多信息，可以在官方
    TensorFlow 文档中找到，链接：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/feature_column)。
- en: Machine learning with pre-made Estimators
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预制估计器的机器学习
- en: 'Now, after constructing the mandatory feature columns, we can finally utilize
    TensorFlow''s Estimators. Using pre-made Estimators can be summarized in four
    steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在构建了必要的特征列后，我们终于可以使用 TensorFlow 的 Estimators 了。使用预制的 Estimators 可以概括为四个步骤：
- en: Define an input function for data loading
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据加载的输入函数
- en: Convert the dataset into feature columns
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集转换为特征列
- en: Instantiate an Estimator (use a pre-made Estimator or create a new one, for
    example, by converting a Keras model into an Estimator)
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个估算器（使用预制的估算器或创建一个新的估算器，例如通过将Keras模型转换为估算器）
- en: Use the Estimator methods `train()`, `evaluate()`, and `predict()`
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用估算器方法`train()`、`evaluate()`和`predict()`。
- en: Continuing with the Auto MPG example from the previous section, we will apply
    these four steps to illustrate how we can use Estimators in practice. For the
    first step, we need to define a function that processes the data and returns a
    TensorFlow dataset consisting of a tuple that contains the input features and
    the labels (ground truth MPG values). Note that the features must be in a dictionary
    format, and the keys of the dictionary must match the feature columns' names.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用上一节中的Auto MPG示例，我们将应用这四个步骤来说明如何在实践中使用估算器。在第一步中，我们需要定义一个处理数据并返回一个TensorFlow数据集的函数，该数据集包含一个元组，其中包含输入特征和标签（真实的MPG值）。请注意，特征必须以字典格式提供，并且字典的键必须与特征列的名称匹配。
- en: 'Starting with the first step, we will define the input function for the training
    data as follows:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 从第一步开始，我们将定义训练数据的输入函数，如下所示：
- en: '[PRE48]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Notice that we used `dict(train_x)` in this function to convert the pandas
    `DataFrame` object into a Python dictionary. Let''s load a batch from this dataset
    to see how it looks:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这个函数中使用了`dict(train_x)`来将pandas的`DataFrame`对象转换为Python字典。我们来加载一个批次的数据，看看它的样子：
- en: '[PRE49]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We also need to define an input function for the test dataset that will be
    used for evaluation after model training:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要定义一个测试数据集的输入函数，用于在模型训练后进行评估：
- en: '[PRE50]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Now, moving on to step 2, we need to define the feature columns. We have already
    defined a list containing the continuous features, a list for the bucketized feature
    column, and a list for the categorical feature column. We can now concatenate
    these individual lists to a single list containing all feature columns:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入第2步，我们需要定义特征列。我们已经定义了一个包含连续特征的列表、一个包含桶化特征列的列表以及一个包含类别特征列的列表。现在，我们可以将这些单独的列表合并成一个包含所有特征列的单一列表：
- en: '[PRE51]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'For step 3, we need to instantiate a new Estimator. Since predicting MPG values
    is a typical regression problem, we will use `tf.estimator.DNNRegressor`. When
    instantiating the regression Estimator, we will provide the list of feature columns
    and specify the number of hidden units that we want to have in each hidden layer
    using the argument `hidden_units`. Here, we will use two hidden layers, where
    the first hidden layer has 32 units and the second hidden layer has 10 units:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第3步，我们需要实例化一个新的估算器。由于预测MPG值是一个典型的回归问题，我们将使用`tf.estimator.DNNRegressor`。在实例化回归估算器时，我们将提供特征列的列表，并使用`hidden_units`参数指定每个隐藏层的单元数。这里，我们将使用两个隐藏层，第一个隐藏层有32个单元，第二个隐藏层有10个单元：
- en: '[PRE52]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The other argument, `model_dir`, that we have provided specifies the directory
    for saving model parameters. One of the advantages of Estimators is that they
    automatically checkpoint the model during training, so that in case the training
    of the model crashes for an unexpected reason (like power failure), we can easily
    load the last saved checkpoint and continue training from there. The checkpoints
    will also be saved in the directory specified by `model_dir`. If we do not specify
    the `model_dir` argument, the Estimator will create a random temporary folder
    (for example, in the Linux operating system, a random folder in the `/tmp/` directory
    will be created), which will be used for this purpose.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个参数`model_dir`指定了用于保存模型参数的目录。估算器的一个优势是它们在训练过程中会自动保存模型检查点，因此，如果模型训练因为意外原因（如停电）中断，我们可以轻松加载最后保存的检查点并从那里继续训练。检查点也将保存在`model_dir`指定的目录中。如果我们没有指定`model_dir`参数，估算器将创建一个随机的临时文件夹（例如，在Linux操作系统中，会在`/tmp/`目录中创建一个随机文件夹），用于保存检查点。
- en: 'After these three basic setup steps, we can finally use the Estimator for training,
    evaluation, and, eventually, prediction. The regressor can be trained by calling
    the `train()` method, for which we require the previously defined input function:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这三步基本设置后，我们终于可以使用估算器进行训练、评估，并最终进行预测。回归器可以通过调用`train()`方法进行训练，而我们需要之前定义的输入函数：
- en: '[PRE53]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Calling `.train()` will automatically save the checkpoints during the training
    of the model. We can then reload the last checkpoint:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 调用`.train()`将自动在模型训练过程中保存检查点。我们可以随后重新加载最后一个检查点：
- en: '[PRE54]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, in order to evaluate the predictive performance of the trained model,
    we can use the `evaluate()` method, as follows:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了评估训练模型的预测性能，我们可以使用 `evaluate()` 方法，如下所示：
- en: '[PRE55]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Finally, to predict the target values on new data points, we can use the `predict()`
    method. For the purposes of this example, suppose that the test dataset represents
    a dataset of new, unlabeled data points in a real-world application.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了对新数据点预测目标值，我们可以使用 `predict()` 方法。对于本示例，假设测试数据集表示的是现实应用中的新、未标记的数据点。
- en: 'Note that in a real-world prediction task, the input function will only need
    to return a dataset consisting of features, assuming that the labels are not available.
    Here, we will simply use the same input function that we used for evaluation to
    get the predictions for each example:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在真实的预测任务中，输入函数只需要返回一个包含特征的数据集，假设标签不可用。在这里，我们将简单地使用我们在评估时使用的相同输入函数来获取每个示例的预测值：
- en: '[PRE56]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'While the preceding code snippets conclude the illustration of the four steps
    that are required for using pre-made Estimators, for practice, let''s take a look
    at another pre-made Estimator: the boosted tree regressor, `tf.estimator.BoostedTreeRegressor`.
    Since, the input functions and the feature columns are already built, we just
    need to repeat steps 3 and 4\. For step 3, we will create an instance of `BoostedTreeRegressor`
    and configure it to have 200 trees.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的代码片段展示了使用预制 Estimators 所需的四个步骤，但为了练习，让我们看看另一个预制 Estimator：提升树回归器 `tf.estimator.BoostedTreeRegressor`。由于输入函数和特征列已经构建完成，我们只需要重复步骤
    3 和步骤 4。对于步骤 3，我们将创建一个 `BoostedTreeRegressor` 实例，并将其配置为拥有 200 棵树。
- en: '**Decision tree boosting**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**决策树提升**'
- en: We already covered the ensemble algorithms, including boosting, in *Chapter
    7,* *Combining Different Models for Ensemble Learning*. The boosted tree algorithm
    is a special family of boosting algorithms that is based on the optimization of
    an arbitrary loss function. Feel free to visit [https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
    to learn more.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在*第 7 章*《*结合不同模型进行集成学习*》中介绍了集成算法，包括提升算法。提升树算法是提升算法的一个特殊家族，它基于任意损失函数的优化。请随时访问
    [https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d](https://medium.com/mlreview/gradient-boosting-from-scratch-1e317ae4587d)
    了解更多信息。
- en: '[PRE57]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: As you can see, the boosted tree regressor achieves lower average loss than
    the `DNNRegressor`. For a small dataset like this, this is expected.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，提升树回归器的平均损失低于 `DNNRegressor`。对于这样一个小数据集，这是预期中的结果。
- en: In this section, we covered the essential steps for using TensorFlow's Estimators
    for regression. In the next subsection, we will take a look at a typical classification
    example using Estimators.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了使用 TensorFlow 的 Estimators 进行回归的基本步骤。在下一小节中，我们将通过一个典型的分类示例来展示如何使用
    Estimators。
- en: Using Estimators for MNIST handwritten digit classification
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Estimators 进行 MNIST 手写数字分类
- en: For this classification problem, we are going to use the `DNNClassifier` Estimator
    provided by TensorFlow, which lets us implement a multilayer perceptron very conveniently.
    In the previous section, we covered the four essential steps for using the pre-made
    Estimators in detail, which we will need to repeat in this section. First, we
    are going to import the `tensorflow_datasets` (`tfds`) submodule, which we can
    use to load the MNIST dataset and specify the hyperparameters of the model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个分类问题，我们将使用 TensorFlow 提供的 `DNNClassifier` Estimator，它可以让我们方便地实现多层感知机。在上一节中，我们详细介绍了使用预制
    Estimators 的四个基本步骤，在本节中我们将需要重复这些步骤。首先，我们将导入 `tensorflow_datasets` (`tfds`) 子模块，利用它加载
    MNIST 数据集，并指定模型的超参数。
- en: '**Estimator API and graph issues**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**Estimator API 和图问题**'
- en: 'Since parts of TensorFlow 2.0 are still a bit rough around the edges, you may
    encounter the following issue when executing the next code block: `RuntimeError:
    Graph is finalized and cannot be modified.` Currently, there is no good solution
    for this issue, and a suggested workaround is to restart your Python, IPython,
    or Jupyter Notebook session before executing the next code block.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '由于 TensorFlow 2.0 的某些部分仍然存在一些不完善的地方，你在执行下一个代码块时可能会遇到以下问题：`RuntimeError: Graph
    is finalized and cannot be modified.` 目前，没有很好的解决方案，建议的解决方法是在执行下一个代码块之前，重新启动 Python、IPython
    或 Jupyter Notebook 会话。'
- en: 'The setup step includes loading the dataset and specifying hyperparameters
    (`BUFFER_SIZE` for shuffling the dataset, `BATCH_SIZE` for the size of mini-batches,
    and the number of training epochs):'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 设置步骤包括加载数据集并指定超参数（`BUFFER_SIZE`用于数据集的洗牌，`BATCH_SIZE`用于小批量的大小，以及训练周期数）：
- en: '[PRE58]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note that `steps_per_epoch` determines the number of iterations in each epoch,
    which is needed for infinitely repeated datasets (as discussed in *Chapter 13*,
    *Parallelizing Neural Network Training with TensorFlow*. Next, we will define
    a helper function that will preprocess the input image and its label.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`steps_per_epoch`决定了每个周期中的迭代次数，这是处理无限重复数据集所必需的（如在*第13章*《使用TensorFlow并行化神经网络训练》中讨论的）。接下来，我们将定义一个辅助函数，用于预处理输入图像及其标签。
- en: 'Since the input image is originally of the type `''uint8''` (in the range [0,
    255]), we will use `tf.image.convert_image_dtype()` to convert its type to `tf.float32`
    (and thereby, within the range [0, 1]):'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于输入图像最初是`'uint8'`类型（在[0, 255]范围内），我们将使用`tf.image.convert_image_dtype()`将其类型转换为`tf.float32`（从而使其在[0,
    1]范围内）：
- en: '[PRE59]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '**Step 1**: Define two input functions (one for training and one for evaluation):'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤1**：定义两个输入函数（一个用于训练，另一个用于评估）：'
- en: '[PRE60]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Notice that the dictionary of features has only one key, `'image-pixels'`. We
    will use this key in the next step.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，特征字典中只有一个键，`'image-pixels'`。我们将在下一步中使用这个键。
- en: '**Step 2**: Define the feature columns:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤2**：定义特征列：'
- en: '[PRE61]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Note that here, we defined the feature columns of size 784 (that is, ![](img/B13208_14_041.png)),
    which is the size of the input MNIST images after they are flattened.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们定义的特征列的大小为784（即 ![](img/B13208_14_041.png)），这是输入MNIST图像在扁平化后得到的大小。
- en: '**Step 3**: Create a new Estimator. Here, we specify two hidden layers: 32
    units in the first hidden layer and 16 units in the second.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤3**：创建一个新的估计器。在这里，我们指定两个隐藏层：第一个隐藏层有32个单元，第二个隐藏层有16个单元。'
- en: 'We also specify the number of classes (remember that MNIST consists of 10 different
    digits, 0-9) using the argument `n_classes`:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还使用`n_classes`参数指定类别的数量（请记住，MNIST包含10个不同的数字，0-9）：
- en: '[PRE62]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '**Step 4**: Use the Estimator for training, evaluation, and prediction:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '**步骤4**：使用估计器进行训练、评估和预测：'
- en: '[PRE63]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: So far, you have learned how to use pre-made Estimators and apply them for preliminary
    assessment to see, for example, whether an existing model is suitable for a particular
    problem. Besides using pre-made Estimators, we can also create an Estimator by
    converting a Keras model to an Estimator, which we will do in the next subsection.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学会了如何使用预制的估计器（Estimators）并将其应用于初步评估，以便查看，例如，现有的模型是否适用于特定的问题。除了使用预制的估计器，我们还可以通过将一个Keras模型转换为估计器来创建一个新的估计器，我们将在下一小节中进行此操作。
- en: Creating a custom Estimator from an existing Keras model
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从现有的Keras模型创建一个自定义估计器
- en: Converting a Keras model to an Estimator is useful in academia as well as industry
    for cases where you have developed a model and want to publish it or share the
    model with other members in your organization. Such a conversion allows us to
    access the strengths of Estimators, such as distributed training and automatic
    checkpointing. In addition, it will make it easy for others to use this model,
    and particularly to avoid confusions in interpreting the input features by specifying
    the feature columns and the input function.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 将Keras模型转换为估计器在学术界和工业界都非常有用，特别是在你已经开发了一个模型并希望发布或与组织中的其他成员共享该模型的情况下。这样的转换使我们能够利用估计器的优势，比如分布式训练和自动检查点。此外，这将使其他人能够轻松使用此模型，特别是通过指定特征列和输入函数来避免在解释输入特征时的混乱。
- en: 'To learn how we can create our own Estimator from a Keras model, we will work
    with the previous XOR problem. First, we will regenerate the data and split it
    into training and validation datasets:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习如何从Keras模型创建我们自己的估计器，我们将使用之前的XOR问题。首先，我们将重新生成数据并将其拆分为训练集和验证集：
- en: '[PRE64]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Let''s also build a Keras model that we want to convert to an Estimator later.
    We will define the model using the `Sequential` class as before. This time, we
    will also add an input layer defined as `tf.keras.layers.Input` to give a name
    to the input to this model:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也构建一个我们想要稍后转换为估计器的Keras模型。我们将像之前一样使用`Sequential`类定义模型。这一次，我们还将添加一个输入层，定义为`tf.keras.layers.Input`，以给该模型的输入命名：
- en: '[PRE65]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we will go through the four steps that we described in the previous subsection.
    Steps 1, 2, and 4 will be the same as the ones we used with the pre-made estimators.
    Note that the key name for the input features that we use in steps 1 and 2 must
    match with what we defined in the input layer of our model. The code is as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将回顾我们在前一小节中描述的四个步骤。步骤 1、2 和 4 与我们使用预制 Estimator 时相同。请注意，我们在步骤 1 和 2 中使用的输入特征的关键名称必须与我们在模型的输入层中定义的名称一致。代码如下：
- en: '[PRE66]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'For step 3, we will convert the model to an Estimator using `tf.keras.estimator.model_to_estimator`
    instead of instantiating one of the pre-made Estimators. Before converting the
    model, we first need to compile it:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 中，我们将使用 `tf.keras.estimator.model_to_estimator` 将模型转换为 Estimator，而不是实例化一个预先制作好的
    Estimator。在转换模型之前，我们首先需要对其进行编译：
- en: '[PRE67]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Finally, in step 4, we can train our model using the Estimator and evaluate
    it on the validation dataset:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在步骤 4 中，我们可以使用 Estimator 训练模型并在验证数据集上评估其表现：
- en: '[PRE68]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: As you can see, converting a Keras model to an Estimator is very straightforward.
    Doing this allows us to easily benefit from the various Estimator strengths, such
    as distributed training and automatically saving the checkpoints during training.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，将一个 Keras 模型转换为 Estimator 非常直接。这样做使我们能够轻松利用 Estimator 的各种优势，例如分布式训练和在训练过程中自动保存检查点。
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we covered TensorFlow's most essential and useful features.
    We started by discussing the migration from TensorFlow v1.x to v2\. In particular,
    we used TensorFlow's dynamic computation graph approach, the so-called eager execution
    mode, which makes implementing computations more convenient compared to using
    static graphs. We also covered the semantics of defining TensorFlow `Variable`
    objects as model parameters, annotating Python functions using the `tf.function`
    decorator to improve computational efficiency via graph compilation.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了 TensorFlow 最基本和最有用的特性。我们首先讨论了从 TensorFlow v1.x 到 v2 的迁移，特别是我们使用了
    TensorFlow 的动态计算图方法——即所谓的即时执行模式（eager execution），这种方式相比使用静态图使得实现计算更加便捷。我们还介绍了将
    TensorFlow `Variable` 对象定义为模型参数的语义，并使用 `tf.function` 装饰器注解 Python 函数，以通过图编译提高计算效率。
- en: After we considered the concept of computing partial derivatives and gradients
    of arbitrary functions, we covered the Keras API in more detail. It provides us
    with a user-friendly interface for building more complex deep NN models. Finally,
    we utilized TensorFlow's `tf.estimator` API to provide a consistent interface
    that is typically preferred in production environments. We concluded this chapter
    by converting a Keras model into a custom Estimator.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们考虑了计算任意函数的偏导数和梯度的概念之后，我们更加详细地介绍了 Keras API。它为我们提供了一个用户友好的接口，用于构建更复杂的深度神经网络模型。最后，我们利用
    TensorFlow 的 `tf.estimator` API 提供了一个一致的接口，这通常在生产环境中更为优选。我们通过将 Keras 模型转换为自定义
    Estimator 来结束本章内容。
- en: Now that we have covered the core mechanics of TensorFlow, the next chapter
    will introduce the concept behind **convolutional neural network** (**CNN**) architectures
    for deep learning. CNNs are powerful models and have shown great performance in
    the field of computer vision.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 TensorFlow 的核心机制，下一章将介绍 **卷积神经网络**（**CNN**）架构在深度学习中的概念。卷积神经网络是强大的模型，在计算机视觉领域表现出色。
