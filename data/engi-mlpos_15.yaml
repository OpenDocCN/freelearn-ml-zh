- en: 'Chapter 12: Model Serving and Monitoring'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第12章：模型服务和监控
- en: In this chapter, we will reflect on the need to serve and monitor **machine
    learning** (**ML**) models in production and explore different means of serving
    ML models for users or consumers of the model. Then, we will revisit the **Explainable
    Monitoring framework** from [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206),
    *Key Principles for Monitoring Your ML System*, and implement it for the business
    use case we have been solving using MLOps to predict the weather. The implementation
    of an Explainable Monitoring framework is hands-on. We will infer the deployed
    API and monitor and analyze the inference data using **drifts** (such as data
    drift, feature drift, and model drift) to measure the performance of an ML system.
    Finally, we will look at several concepts to govern ML systems for the robust
    performance of ML systems to drive continuous learning and delivery.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将反思在生产环境中服务和监控机器学习（ML）模型的需求，并探讨为模型用户或消费者提供不同服务ML模型的方法。然后，我们将重新审视第11章中讨论的**可解释监控框架**，即*监控您的ML系统的主要原则*，并将其应用于我们使用MLOps预测天气的业务用例中。可解释监控框架的实施是实践性的。我们将推断部署的API，并使用**漂移**（如数据漂移、特征漂移和模型漂移）来监控和分析推理数据，以衡量ML系统的性能。最后，我们将探讨一些概念，以管理ML系统，确保ML系统的稳健性能，以驱动持续学习和交付。
- en: 'Let''s start by reflecting on the need to monitor ML in production. Then, we
    will move on to explore the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从反思在生产环境中监控ML的需求开始。然后，我们将继续在本章中探讨以下主题：
- en: Serving, monitoring, and maintaining models in production
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中服务、监控和维护模型
- en: Exploring different modes of serving ML models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索服务机器学习模型的不同模式
- en: Implementing the Explainable Monitoring framework
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施解释监控框架
- en: Governing your ML system
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理您的ML系统
- en: Serving, monitoring, and maintaining models in production
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中服务、监控和维护模型
- en: 'There is no point in deploying a model or an ML system and not monitoring it.
    Monitoring performance is one of the most important aspects of an ML system. Monitoring
    enables us to analyze and map out the business impact an ML system offers to stakeholders
    in a qualitative and quantitative manner. In order to achieve maximum business
    impact, users of ML systems need to be served in the most convenient manner. After
    that, they can consume the ML system and generate value. In previous chapters,
    we developed and deployed an ML model to predict the weather conditions at a port
    as part of the business use case that we had been solving for practical implementation.
    In this chapter, we will revisit the Explainable Monitoring framework that we
    discussed in [*Chapter 11*](B16572_11_Final_JM_ePub.xhtml#_idTextAnchor206), *Key
    Principles for Monitoring Your ML System*, and implement it within our business
    use case. In *Figure 12.1*, we can see the **Explainable Monitoring** framework
    and some of its components, as highlighted in green:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个模型或ML系统而不监控它是没有意义的。监控性能是ML系统最重要的方面之一。监控使我们能够以定性和定量的方式分析和绘制ML系统对利益相关者的业务影响。为了实现最大的业务影响，ML系统的用户需要以最方便的方式提供服务。之后，他们可以消费ML系统并产生价值。在前几章中，我们开发和部署了一个ML模型来预测港口的天气条件，这是我们一直在解决的实际实施业务用例的一部分。在本章中，我们将重新审视我们在第11章中讨论的**可解释监控框架**，即*监控您的ML系统的主要原则*，并将其应用于我们的业务用例中。在*图12.1*中，我们可以看到**可解释监控**框架及其一些组件，如绿色高亮所示：
- en: '![Figure 12.1 – Components of the Explainable Monitoring framework to be implemented'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.1 – 要实施的解释监控框架的组件'
- en: '](img/image0011.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0011.jpg)'
- en: Figure 12.1 – Components of the Explainable Monitoring framework to be implemented
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 – 要实施的解释监控框架的组件
- en: 'We will implement Explainable Monitoring for these areas: *Data Integrity*,
    *Model Drift*, *Application Performance*, *Bias and Threat Detection*, *Local
    and Global Explanations*, *Alerts and Actions*, *Model QA and Control*, and *Model
    Auditing and Reports*. These components are the most significant, in our use case,
    to understand the implementation of Explainable Monitoring. We will leave out
    *Data Slicing* because we do not have much variety in terms of the demographics
    or samples within the data (for example, sex, age groups, and more). By using
    information from other components, we can assess the model''s performance and
    its fairness. In this chapter, we will implement components of the **Monitor**
    and **Analyze** modules: *Data Integrity*, *Model Drift*, *Application Performance*,
    *Bias and Threat Detection*, and *Local and Global Explanations*. The remaining
    component implementations will be covered in [*Chapter 13*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234),
    *Governing the ML System for Continual Learning*. Before we move on to the implementation
    process, let''s take a look at how models can be served for users to consume.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为以下领域实现可解释监控：*数据完整性*、*模型漂移*、*应用性能*、*偏差和威胁检测*、*局部和全局解释*、*警报和操作*、*模型QA和控制*以及*模型审计和报告*。这些组件在我们使用案例中是最重要的，以理解可解释监控的实施。我们将省略*数据切片*，因为我们数据中的人口统计或样本（例如，性别、年龄组等）没有太多多样性。通过使用其他组件的信息，我们可以评估模型的表现及其公平性。在本章中，我们将实现**监控**和**分析**模块的组件：*数据完整性*、*模型漂移*、*应用性能*、*偏差和威胁检测*以及*局部和全局解释*。其余组件的实现将在[*第13章*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234)中介绍，*持续学习的ML系统治理*。在我们继续到实现过程之前，让我们看看模型如何为用户消费提供服务。
- en: Exploring different modes of serving ML models
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索ML模型的不同服务模式
- en: 'In this section, we will consider how a model can be served for users (both
    humans and machines) to consume the ML service efficiently. Model serving is a
    critical area, which an ML system needs to succeed at to fulfill its business
    impact, as any lag or bug in this area can be costly in terms of serving users.
    Robustness, availability, and convenience are key factors to keep in mind while
    serving ML models. Let''s take a look at some ways in which ML models can be served:
    this can be via batch service or on-demand mode (for instance, when a query is
    made on demand in order to get a prediction). A model can be served to either
    a machine or a human user in on-demand mode. Here is an example of serving a model
    to a user:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将考虑如何为用户（无论是人类还是机器）提供模型以高效地消费ML服务。模型服务是一个关键领域，ML系统需要成功实现以发挥其商业影响，因为在这个领域的任何延迟或错误都可能对服务用户造成高昂的成本。鲁棒性、可用性和便利性是在提供服务模型时需要考虑的关键因素。让我们看看ML模型可以以哪些方式提供服务：这可以是批量服务或按需模式（例如，当需要查询以获取预测时）。在按需模式下，模型可以服务于机器或人类用户。以下是将模型提供给用户的示例：
- en: '![Figure 12.2 – Serving a model to users'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.2 – 向用户提供服务模型'
- en: '](img/image002.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image002.jpg)'
- en: Figure 12.2 – Serving a model to users
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 向用户提供服务模型
- en: In a typical scenario (in on-demand mode), a model is served as a service for
    users to consume, as shown in *Figure 12.2*. Then, an external application on
    a machine or a human makes a query to the prediction or ML service using their
    data. The ML service, upon receiving a request, uses a load balancer to route
    the request to an available resource (such as a container or an application) within
    the ML application. The load balancer also manages resources within the ML service
    to orchestrate and generate new containers or resources on demand. The load balance
    redirects the query from the user to the model running in a container within the
    ML application to get the prediction. On getting the prediction, the load balance
    reverts back to the external application on a machine, or to a human who is making
    the request, or to the query within the model prediction. In this way, the ML
    service is able to serve its users. The ML system orchestrates with the model
    store or registry to keep itself updated with either the latest or best-performing
    models in order to serve the users in the best manner. In comparison to this typical
    scenario where users make a query, there is another use case where the model is
    served as a batch service.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型场景（按需模式）中，模型作为服务提供给用户消费，如图*图12.2*所示。然后，机器上的外部应用程序或人类使用他们的数据向预测或机器学习服务发出查询。机器学习服务在收到请求后，使用负载均衡器将请求路由到机器学习应用程序内的可用资源（如容器或应用程序）。负载均衡器还管理机器学习服务内的资源，以按需编排和生成新的容器或资源。负载均衡器将查询从用户重定向到机器学习应用程序内运行的容器中的模型以获取预测。获取预测后，负载均衡器将返回到机器上的外部应用程序，或请求查询的人类，或模型预测内的查询。这样，机器学习服务能够为其用户提供服务。机器学习系统与模型存储或注册表协同工作，以保持其与最新或性能最佳的模型同步，以便以最佳方式为用户提供服务。与用户进行查询的典型场景相比，还有另一种用例，即模型作为批量服务提供。
- en: Serving the model as a batch service
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型作为批量服务提供
- en: 'Batch processing or serving is applied to large quantities or batches of input
    data (that is, not single observations but bunches of observations together).
    In cases where there is a large bunch of data to be inferred, a model is normally
    served in batch mode. One example of this is when the model is used to process
    the data of all consumers or users of a product or service in one go. Alternatively,
    a batch of data from a factory for a fixed timeline might need to be processed
    to detect anomalies in the machines. Compared to on-demand mode, batch mode is
    more resource-efficient and is usually employed when some latency can be afforded:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理或服务应用于大量或批次的输入数据（即不是单个观察值，而是成组的观察值）。在存在大量数据需要推断的情况下，模型通常以批量模式提供服务。一个例子是当模型一次性处理产品或服务的所有消费者或用户的数据。或者，工厂在固定时间线上的数据批次可能需要被处理以检测机器中的异常。与按需模式相比，批量模式更节省资源，通常在可以承受一定延迟的情况下使用：
- en: '![Figure 12.3 – Batch Inference'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.3 – 批量推断'
- en: '](img/image0031.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image0031.jpg)'
- en: Figure 12.3 – Batch inference
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 – 批量推断
- en: One of the key advantages of batch processing is that unlike a REST API-based
    service, a batch service might require lighter or less infrastructure. Writing
    a batch job is easier for a data scientist compared to deploying an online REST
    service. This is because the data scientist just needs to train a model or deserialize
    a trained model on a machine and perform batch inference on a batch of data. The
    results of batch inference can be stored in a database as opposed to sending responses
    to users or consumers. However, one major disadvantage is the high latency and
    it not being in real time. Typically, a batch service can process hundreds or
    thousands of features at once. A series of tests can be used to determine the
    optimal batch size to arrive at an acceptable latency for the use case. Typical
    batch sizes can be 32, 64, 128, or 518 to the power of 2\. Batch inference can
    be scheduled periodically and can serve many use cases where latency is not an
    issue. One such example is discussed next.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 批量处理的一个关键优势是，与基于REST API的服务不同，批量服务可能需要更轻量或更少的基础设施。对于数据科学家来说，编写批量作业比部署在线REST服务更容易。这是因为数据科学家只需要在机器上训练一个模型或反序列化一个训练好的模型，并对一批数据进行批量推理。批量推理的结果可以存储在数据库中，而不是发送响应给用户或消费者。然而，一个主要的缺点是高延迟且不是实时。通常，批量服务可以一次处理数百或数千个特征。可以使用一系列测试来确定最佳批量大小，以达到可接受的延迟。典型的批量大小可以是32、64、128或2的518次方。批量推理可以定期安排，并可以服务于许多延迟不是问题的用例。以下将讨论一个这样的例子。
- en: A real-world example
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子
- en: One real-world example is a bank extracting information from batches of text
    documents. A bank receives thousands of documents a day from its partner institutions.
    It is not possible for a human agent to read through all of them and highlight
    any red flags in the operations listed in the documents. Batch inferencing is
    used to extract name entities and red flags from all the documents received by
    the bank in one go. The results of the batch inference or serving are then stored
    in a database.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子是银行从一批文本文档中提取信息。银行每天从其合作伙伴机构接收数千份文件。人类代理不可能阅读所有这些文件并突出显示文档中列出的操作中的任何红旗。批量推理用于一次性从银行接收的所有文档中提取命名实体和红旗。然后，批量推理或服务的成果存储在数据库中。
- en: Serving the model to a human user
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型提供给人类用户
- en: Before processing a request from human users, it is essential to check whether
    the user has adequate permissions to use the model. Additionally, in most cases,
    it is helpful to know the context in which the request was made. Gathering the
    context of the request will enable the model to produce better predictions. After
    gathering the context, we can transform it into model-readable input and infer
    the model to get a prediction.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理来自人类用户的请求之前，检查用户是否有足够的权限使用模型至关重要。此外，在大多数情况下，了解请求的上下文也很有帮助。收集请求的上下文将使模型能够产生更好的预测。收集上下文后，我们可以将其转换为模型可读的输入，并推断模型以获得预测。
- en: 'In practice, here are the key steps in serving an on-demand model to human
    users:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，以下是向人类用户提供按需模型的关键步骤：
- en: Validate or authorize the request.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证或授权请求。
- en: Analyze and gather contextual information (for example, historic data, user
    experience data, or any other personal data of the user).
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析和收集上下文信息（例如，历史数据、用户体验数据或任何其他用户个人数据）。
- en: Transform any contextual information into a model-readable input or schema.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将任何上下文信息转换为模型可读的输入或模式。
- en: Infer the model with input data (with the request and contextual information)
    to make a prediction or get an output.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用输入数据（包括请求和上下文信息）推断模型以做出预测或获取输出。
- en: Interpret the output as per the context.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据上下文解释输出结果。
- en: Relay the output to the user.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出传递给用户。
- en: A real-world example
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一个现实世界的例子
- en: Consider a chatbot serving human customers to book flight tickets. It performs
    contextual inference to serve human users.
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑一个聊天机器人服务于人类客户预订机票。它执行上下文推理以服务于人类用户。
- en: Serving the model to a machine
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将模型提供给机器
- en: 'We can serve a machine or an external application using a `REST` API or a streaming
    service based on the use case. Typically, machine inference data requirements
    are either predetermined or within a standard schema. A well-defined topology
    and data schema in the form of a REST API or streaming service will work. Serving
    on demand to a machine or human varies from case to case, as, in some scenarios,
    demand may vary (for example, at a particular time of day when the demand for
    serving the user might be high, such as in the afternoon). To handle a high demand
    from the service, autoscaling (on the cloud) can help spawn more resources on
    demand and kill any idle resources to free up more resources. However, autoscaling
    is not a one-stop solution for scaling, as it cannot handle sudden or peculiar
    spikes in demand on its own:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据用例使用`REST` API或基于流的服务的机器或外部应用程序。通常，机器推理数据需求要么是预先确定的，要么在标准模式内。一个定义良好的拓扑和数据模式，无论是REST
    API还是流式服务，都将有效。对机器或人类的需求式服务因情况而异，因为在某些场景中，需求可能会变化（例如，在一天中的特定时间，用户服务的需求可能很高，如下午）。为了处理服务的高需求，自动扩展（在云上）可以帮助按需生成更多资源，并杀死任何空闲资源以释放更多资源。然而，自动扩展不是缩放的万能解决方案，因为它不能单独处理需求上的突然或特殊峰值：
- en: '![Figure 12.4 – Message Broker for on-demand serving'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.4 – 需求式服务的消息代理'
- en: '](img/image004.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/image004.jpg)'
- en: Figure 12.4 – Message broker for on-demand serving
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – 需求式服务的消息代理
- en: 'The approach shown in *Figure 12.4* is resource-efficient to handle high volume
    demand spikes. To handle sudden spikes, message brokers such as Apache Kafka or
    Spark can be useful. A message broker runs processes to write and read to a queue:
    one process to write messages in a queue and another process to read from that
    queue. The served model is periodically connected to the message broker to process
    batches of input data from the queue to make predictions for each element in the
    batch. After processing the input data batches and generating predictions, the
    predictions are written to the output queue, which is then pushed to the users
    as per their requests.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图12.4*所示的方法在处理高量需求峰值时资源效率高。为了处理突然的峰值，可以使用如Apache Kafka或Spark这样的消息代理。消息代理运行进程以向队列写入和从队列读取：一个进程用于在队列中写入消息，另一个进程用于从该队列读取。服务模型定期连接到消息代理以处理队列中的输入数据批次，并对批次中的每个元素进行预测。在处理输入数据批次并生成预测后，预测被写入输出队列，然后根据用户请求推送给用户。
- en: A real-world example
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一个现实世界的例子
- en: Consider a social media company that has millions of users. The company uses
    a single or common ML model for the recommender system to recommend newsfeed articles
    or posts to users. As the volume of requests is high in order to serve many users,
    it cannot depend on a REST API-based ML system (as it is synchronous). A streaming
    solution is better as it provides asynchronous inference for the company to serve
    its users. When a user logs into their application or account hosted on a machine
    (such as a social media company server), the application running on their machine
    infers the ML model (that is, the recommender system) via a streaming service
    for recommendations for the user newsfeed. Likewise, thousands of other users
    log in at the same time. The streaming service can serve all of these users seamlessly.
    Note that this wouldn't have been possible with the REST API service. By using
    a streaming service for the recommender system model, the social media company
    is able to serve its high volume of users in real time, avoiding significant lags.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个拥有数百万用户的社交媒体公司。该公司使用单个或公共机器学习模型作为推荐系统，向用户推荐新闻文章或帖子。由于请求量很大，为了服务众多用户，它不能依赖于基于REST
    API的机器学习系统（因为它同步）。流式解决方案更好，因为它为公司提供异步推理，以便实时服务其用户。当用户登录到其机器上的应用程序或账户（例如社交媒体公司服务器上的应用程序）时，运行在其机器上的应用程序通过流式服务推断机器学习模型（即推荐系统），以提供用户新闻源的推荐。同样，成千上万的用户同时登录。流式服务可以无缝地服务所有这些用户。请注意，这不可能通过REST
    API服务实现。通过为推荐系统模型使用流式服务，社交媒体公司能够实时服务其大量用户，避免出现重大延迟。
- en: Implementing the Explainable Monitoring framework
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实施可解释监控框架
- en: 'To implement the Explainable Monitoring framework, it is worth doing a recap
    of what has been discussed so far, in terms of implementing hypothetical use cases.
    Here is a recap of what we did for our use case implementation, including the
    problem and solution:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现可解释监控框架，回顾一下到目前为止所讨论的内容，特别是关于实现假设用例的内容是值得的。以下是关于我们用例实现回顾，包括问题和解决方案：
- en: '**Problem context**: You work as a data scientist in a small team with three
    other data scientists for a cargo shipping company based in the port of Turku
    in Finland. 90% of the goods imported into Finland arrive via cargo shipping at
    various ports across the country. For cargo shipping, weather conditions and logistics
    can be challenging at times. Rainy conditions can distort operations and logistics
    at the ports, which can affect supply chain operations. Forecasting rainy conditions
    in advance allows us to optimize resources such as human resources, logistics,
    and transport resources for efficient supply chain operations at ports. Business-wise,
    forecasting rainy conditions in advance enables ports to reduce their operational
    costs by up to approximately 20% by enabling the efficient planning and scheduling
    of human resources, logistics, and transport resources for supply chain operations.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题背景**：您在芬兰图尔库港的一家货运公司的小团队中担任数据科学家，与另外三名数据科学家一起工作。90% 的进入芬兰的商品通过货运在国家的各个港口抵达。对于货运来说，天气条件和物流有时可能具有挑战性。雨天可能会扭曲港口的运营和物流，从而影响供应链运营。提前预测雨天条件可以使我们优化人力资源、物流和运输资源，以实现港口供应链运营的高效。从业务角度来看，提前预测雨天条件可以使港口通过高效规划和调度人力资源、物流和运输资源，将运营成本降低约
    20%。'
- en: '**Task or solution**: You, as a data scientist, are tasked to develop an ML-driven
    solution to forecast weather conditions 4 hours in advance at the port of Turku
    in Finland. This will enable the port to optimize its resources, thereby enabling
    cost savings of up to 20%. To get started, you are provided with a historic weather
    dataset with a 10-year-timeline from the port of Turku (the dataset can be accessed
    in the Git repository of this book). Your task is to build a continuous learning-driven
    ML solution to optimize operations at the port of Turku.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务或解决方案**：作为数据科学家，您被要求开发一个机器学习驱动的解决方案，以提前 4 小时预测芬兰图尔库港的天气状况。这将使港口能够优化其资源，从而实现高达
    20% 的成本节约。为了开始，您将获得一个包含 10 年时间线的图尔库港历史天气数据集（数据集可通过本书的 Git 仓库访问）。您的任务是构建一个持续学习驱动的机器学习解决方案，以优化图尔库港的运营。'
- en: So far, we have developed ML models and deployed them as REST API endpoints
    inside a Kubernetes cluster at [http://20.82.202.164:80/api/v1/service/weather-prod-service/score](http://20.82.202.164:80/api/v1/service/weather-prod-service/score)
    (the address of your endpoint will be different).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经开发了机器学习模型并将它们作为 REST API 端点部署在 Kubernetes 集群中，地址为 [http://20.82.202.164:80/api/v1/service/weather-prod-service/score](http://20.82.202.164:80/api/v1/service/weather-prod-service/score)（您的端点地址将不同）。
- en: 'Next, we will replicate a real-life inference scenario for this endpoint. To
    do this, we will use the test dataset we had split and registered in [*Chapter
    4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074), *Machine Learning Pipelines*,
    in the *Data ingestion and feature engineering* section. Go to your Azure ML workspace
    and download the `test_data.csv` dataset (which was registered as `test_dataset`)
    from the **Datasets** section or the Blob storage that is connected to your workspace,
    as shown in *Figure 12.5*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为该端点复制一个真实的推断场景。为此，我们将使用我们在 [*第 4 章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074)
    的 *机器学习管道* 部分中分割并注册的测试数据集。前往您的 Azure ML 工作区，从 **数据集** 部分或与您的工作区连接的 Blob 存储下载 `test_data.csv`
    数据集（该数据集已注册为 `test_dataset`），如图 12.5 所示：
- en: '![Figure 12.5 – Downloading the validation dataset (which was previously split
    and registered)'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.5 – 下载验证数据集（之前已分割并注册）'
- en: '](img/image0051.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image0051.jpg]'
- en: Figure 12.5 – Downloading the validation dataset (which was previously split
    and registered)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 – 下载验证数据集（之前已分割并注册）
- en: 'Get ready to infer the `test_data.csv` data with the REST API endpoint or ML
    service. Go to the `12_Model_Serving_Monitoring` folder and place the downloaded
    dataset (`test_data.csv`) inside the folder. Next, access the `inference.` `py`
    file:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 准备使用 REST API 端点或机器学习服务推断 `test_data.csv` 数据。前往 `12_Model_Serving_Monitoring`
    文件夹，并将下载的数据集 (`test_data.csv`) 放入文件夹中。接下来，访问 `inference.` `py` 文件：
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In the preceding code, we perform the following steps:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行以下步骤：
- en: In the `inference.py` file, begin by importing the necessary libraries, such
    as `json`, `requests`, and `pandas`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `inference.py` 文件中，首先导入必要的库，例如 `json`、`requests` 和 `pandas`。
- en: Next, import the dataset (`test_data.csv`) to use to infer with the endpoint.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入用于与端点推断的 `test_data.csv` 数据集。
- en: Drop the unnecessary columns for inference, such as `Timestamp`, `Location`,
    and `Future_weather_condition` (we will predict this final column by querying
    the endpoint).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除推理中不必要的列，如 `Timestamp`、`Location` 和 `Future_weather_condition`（我们将通过查询端点来预测这个最终列）。
- en: Next, point to the URL of the endpoint (you can find this by navigating to **Azure
    ML Workspace** | **Endpoints** | **Weather-prod-service** | **Consume**). For
    simplicity, since we did not have authentication or keys set up for the service,
    we have the header application/JSON with no keys or authentication.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步，指向端点的 URL（您可以通过导航到 **Azure ML 工作区** | **端点** | **Weather-prod-service**
    | **消费**）来。为了简单起见，因为我们没有为该服务设置身份验证或密钥，所以我们有 application/JSON 的标题，没有密钥或身份验证。
- en: 'Finally, we will loop through the data array by inferring each element in the
    array with the endpoint. To run the script, simply replace `''url''` with your
    endpoint and run the following command in the Terminal (from the folder location)
    to execute the script:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将通过端点推断数组中的每个元素来遍历数据数组。要运行脚本，只需将 `'url'` 替换为您的端点，然后在终端（从文件夹位置）运行以下命令以执行脚本：
- en: '[PRE1]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The running script will take around 10–15 minutes to infer all of the elements
    of the inference data. After this, we can monitor the inference and analyze the
    results of the inferring data. Let's monitor and analyze this starting with data
    integrity.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本将花费大约 10-15 分钟来推断推理数据中的所有元素。之后，我们可以监控推理并分析推断数据的结果。让我们从数据完整性开始监控和分析。
- en: Monitoring your ML system
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控您的机器学习系统
- en: 'The **Monitor** module is dedicated to monitoring the application in production
    (that is, serving the ML model). The action monitor module has the following three
    functionalities:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**监控**模块专门用于监控生产中的应用（即，提供机器学习模型）。动作监控模块具有以下三个功能：'
- en: 'Data integrity:'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据完整性：
- en: -To register the target dataset
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 注册目标数据集'
- en: -To create a data drift monitor
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 创建数据漂移监控器'
- en: -To perform data drift analysis
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 执行数据漂移分析'
- en: -To perform feature drift analysis
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '- 执行特征漂移分析'
- en: Model drift
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型漂移
- en: Application performance
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用性能
- en: Let's take a look at each of these functionalities in more detail next.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地查看这些功能。
- en: Data integrity
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据完整性
- en: 'To monitor data integrity for inference data, we need to monitor data drift
    and feature drift to see whether there are any anomalous changes in the incoming
    data or any new patterns:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控推理数据的数据完整性，我们需要监控数据漂移和特征漂移，以查看是否有任何异常变化或任何新的模式：
- en: '**Data drift**: This is when the properties of the independent variables change.
    For example, data changes can occur due to seasonality or the addition of new
    products or changes in consumer desires or habits, as it did during the COVID-19
    pandemic.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据漂移**：这是指独立变量的属性发生变化的情况。例如，由于季节性或新产品的添加、消费者需求或习惯的变化，数据可能会发生变化，就像在 COVID-19
    大流行期间所发生的那样。'
- en: '**Feature drift**: This is when properties of the feature(s) change over time.
    For example, the temperature is changing due to changing seasons or seasonality,
    that is, in summer, the temperature is warmer compared to temperatures during
    winter or autumn.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征漂移**：这是指特征（的）属性随时间变化的情况。例如，由于季节变化或季节性，温度在夏季比冬季或秋季的温度要暖和。'
- en: 'To monitor drifts, we will measure the difference for the baseline dataset
    versus the target dataset. The first step is to define the baseline dataset and
    the target dataset. This depends on use case to use case; we will use the following
    datasets as the baseline and target datasets:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要监控漂移，我们将测量基线数据集与目标数据集之间的差异。第一步是定义基线数据集和目标数据集。这取决于用例；我们将使用以下数据集作为基线数据集和目标数据集：
- en: '**Baseline dataset**: This is the training dataset.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基线数据集**：这是训练数据集。'
- en: '**Target dataset**: This is the inference dataset.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**目标数据集**：这是推理数据集。'
- en: We will use the training dataset that we previously used to train our models
    as the baseline dataset. This is because the model used in inference knows the
    patterns in the training dataset very well. The training dataset is ideal for
    comparing how inference data changes over time. We will compile all the inference
    data collected during inference into the inference dataset and compare these two
    datasets (that is, the baseline dataset and the target dataset) to gauge data
    and feature drifts for the target dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用之前用于训练模型的训练数据集作为基线数据集。这是因为用于推理的模型非常了解训练数据集中的模式。训练数据集非常适合比较推理数据随时间的变化。我们将收集推理过程中收集的所有推理数据并将其编译到推理数据集中，并将这两个数据集（即基线数据集和目标数据集）进行比较，以评估目标数据集的数据和特征漂移。
- en: Registering the target dataset
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注册目标数据集
- en: The training dataset was registered in [*Chapter 4*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074),
    *Machine Learning Pipelines*, in the *Data ingestion and feature engineering*
    section. We need to register the inference dataset within the **Datasets** section
    of the Azure ML workspace.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集已在[*第 4 章*](B16572_04_Final_JM_ePub.xhtml#_idTextAnchor074)的*机器学习管道*部分注册，在*数据摄取和特征工程*部分。我们需要在
    Azure ML 工作区的**数据集**部分内注册推理数据集。
- en: 'Inference data is collected as a result of using the `azureml.monitoring` SDK
    (the `modelDataCollector` function). By enabling monitoring functions using the
    `modelDataCollector` function in your scoring file (in `score.py`, as we did in
    [*Chapter 6*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124), *Key Principles
    of Deploying Your ML System*), we store inference data in the form of a time-series
    dataset in the Blob storage. In the Blob storage connected to your Azure ML workspace,
    inference data is stored in the `modeldata` container. In the `modeldata` container,
    the inference data (including both inputs and outputs) is stored the form of CSV
    files that are partitioned inside folders. These are structured as per year, per
    month, and per day (when the inference data was recorded in the production). Inside
    the partitioned folders, inference data is stored in CSV files that are named
    `inputs.csv` and `outputs.csv`. We need to register these `input.csv` files to
    monitor data drift and feature drift. Follow these steps to register the `input.csv`
    files:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 推理数据是使用 `azureml.monitoring` SDK（`modelDataCollector` 函数）收集的结果。通过在评分文件（在 `score.py`
    中，正如我们在[*第 6 章*](B16572_06_Final_JM_ePub.xhtml#_idTextAnchor124)的*部署您的 ML 系统的关键原则*中做的那样）中使用
    `modelDataCollector` 函数启用监控功能，我们将推理数据以时间序列数据集的形式存储在 Blob 存储中。在连接到您的 Azure ML 工作区的
    Blob 存储中，推理数据存储在 `modeldata` 容器中。在 `modeldata` 容器中，推理数据（包括输入和输出）以 CSV 文件的形式存储，这些文件被分在文件夹内。这些文件按照年份、月份和日期（在生产中记录推理数据时）进行结构化。在分区文件夹内，推理数据存储在名为
    `inputs.csv` 和 `outputs.csv` 的 CSV 文件中。我们需要注册这些 `input.csv` 文件以监控数据漂移和特征漂移。按照以下步骤注册
    `input.csv` 文件：
- en: Go to the **Datasets** section and click on **Create dataset**. Then, select
    the **From datastore** option, as shown in *Figure 12.6*:![Figure 12.6 – Registering
    the inference dataset
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往**数据集**部分，点击**创建数据集**。然后，选择**从数据存储**选项，如图 12.6 所示：![图 12.6 – 注册推理数据集
- en: '](img/image006.jpg)'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 006](img/image006.jpg)'
- en: Figure 12.6 – Registering the inference dataset
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.6 – 注册推理数据集
- en: Name the dataset (for example, `Inputs-Inference-Dataset`), select the dataset
    type as **Tabular**, and write an appropriate description in the **Description**
    field name by describing the purpose of your dataset. Click on **Next** to specify
    the datastore selection. Select the **modeldata** datastore, as shown in *Figure
    12.7*:![Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为数据集命名（例如，`Inputs-Inference-Dataset`），选择数据集类型为**表格型**，并在**描述**字段中写入适当的描述，描述数据集的目的。点击**下一步**以指定数据存储选择。选择**modeldata**数据存储，如图
    12.7 所示：![图 12.7 – 数据存储选择（输入-推理数据注册）
- en: '](img/image0071.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 0071](img/image0071.jpg)'
- en: Figure 12.7 – Datastore selection (the Inputs-Inference-data registration)
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.7 – 数据存储选择（输入-推理数据注册）
- en: After selecting the `input.csv` file. You can find this in the folder of your
    `support vectorclassifier model`, which is inside the folder with your service
    name (for example, `prod-webservice`). Then, go into the subfolders (the default,
    inputs, and folders structured with dates), and go to the folder of your current
    date to find the `input.csv` file. Select the `input.csv` file, as shown in *Figure
    12.8*:![Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data
    registration)
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择 `input.csv` 文件后。您可以在您的 `support vectorclassifier 模型` 文件夹中找到它，该文件夹位于您的服务名称（例如，`prod-webservice`）文件夹内。然后，进入子文件夹（默认、inputs
    以及按日期结构化的文件夹），并进入您当前日期的文件夹以找到 `input.csv` 文件。选择 `input.csv` 文件，如图 12.8 所示：![图
    12.8 – 选择 input.csv 文件的路径（输入-推理数据注册）
- en: '](img/image008.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 008](img/image008.jpg)'
- en: Figure 12.8 – Selecting path of the input.csv file (the Inputs-Inference-data
    registration)
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.8 – 选择 input.csv 文件的路径（输入-推理数据注册）
- en: After selecting the `input.csv` file, click on the `/**/inputs*.csv` (as shown
    in *Figure 12.9*). This is an important step that will refer to all of the `input.csv`
    files in the `inputs` folder dynamically. Without referencing all of the `input.csv`
    files, we will confine the path to only one `input.csv` file (which was selected
    previously in *Figure 12.8*). By referring to all of the `input.csv` files, we
    will compile all of the input data (the `inputs.csv` files) into the target dataset
    (for example, `Inputs-Inference-Data`):![Figure 12.9 – Referencing the path to
    dynamically access all the input.csv files
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择 `input.csv` 文件后，点击 `/**/inputs*.csv`（如图 12.9 所示）。这是一个重要的步骤，将动态引用 `inputs`
    文件夹中的所有 `input.csv` 文件。如果不引用所有 `input.csv` 文件，我们将路径限制为仅一个 `input.csv` 文件（如图 12.8
    所示之前已选择）。通过引用所有 `input.csv` 文件，我们将所有输入数据（`inputs.csv` 文件）编译到目标数据集中（例如，`Inputs-Inference-Data`）：![图
    12.9 – 引用路径以动态访问所有 input.csv 文件
- en: '](img/image0091.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 0091](img/image0091.jpg)'
- en: Figure 12.9 – Referencing the path to dynamically access all the input.csv files
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.9 – 引用路径以动态访问所有 input.csv 文件
- en: Click on the **Next** button to advance to **Settings and preview**:![Figure
    12.10 – Settings and preview (the inference dataset registration)
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**下一步**按钮进入**设置和预览**：![图 12.10 – 设置和预览（推理数据集注册）]
- en: '](img/image010.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 010](img/image010.jpg)'
- en: Figure 12.10 – Settings and preview (the inference dataset registration)
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.10 – 设置和预览（推理数据集注册）
- en: 'As shown in *Figure 12.10*, we can configure the settings and preview the dataset.
    Point to the correct column names by selecting the **Column headers** dropdown
    and then selecting **Combine headers from all files**. Check for the correct column
    names (for example, **Temperature_C** and **Humidity**). After selecting the appropriate
    column names, click on the **Next** button to advance to the next window. Select
    the right schema by selecting all the columns you would like to monitor, along
    with their data types, as shown in *Figure 12.11*:'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如图 12.10 所示，我们可以配置设置并预览数据集。通过选择 **列标题** 下拉菜单并选择 **从所有文件合并标题** 来指向正确的列名。检查正确的列名（例如，**Temperature_C**
    和 **Humidity**）。在选择了适当的列名后，点击 **下一步** 按钮进入下一个窗口。通过选择您想要监控的所有列及其数据类型，选择正确的模式，如图
    12.11 所示：
- en: '![Figure 12.11 – Schema selection (the inference dataset registration)'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 12.11 – 模式选择（推理数据集注册）]'
- en: '](img/image0111.jpg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图片 0111](img/image0111.jpg)'
- en: Figure 12.11 – Schema selection (the inference dataset registration)
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 12.11 – 模式选择（推理数据集注册）
- en: Make sure that you select the **Timestamp** and **Date** properties in the **$aml_dc_scoring_timestamp**
    column as these contain the timestamps of the inference. This step is important.
    Only a time-series format dataset can be used to compute drift (by the Azure drift
    model); otherwise, we cannot compute drift. After selecting the right schema by
    selecting all of the columns, click on **Next** to confirm all of the necessary
    details (such as the name of the dataset, the dataset version, its path, and more).
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 确保你在 **$aml_dc_scoring_timestamp** 列中选择 **时间戳** 和 **日期** 属性，因为这些属性包含推理的时间戳。这一步很重要。只有时间序列格式的数据集才能用于计算漂移（通过
    Azure 漂移模型）；否则，我们无法计算漂移。通过选择所有列来选择正确的模式后，点击 **下一步** 以确认所有必要的详细信息（例如数据集名称、数据集版本、其路径等）。
- en: 'Click on the **Create** button to create the dataset. When your dataset has
    been created successfully, you can view the dataset from the **Dataset** section
    in your Azure ML workspace. Go to the **Datasets** section to confirm your dataset
    has been created. Identify and click on your created dataset. Upon clicking, you
    will be able to view the details of your registered inference dataset, as shown
    in *Figure 12.12*:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**创建**按钮以创建数据集。当您的数据集创建成功后，您可以从Azure ML工作区的**数据集**部分查看数据集。转到**数据集**部分以确认您的数据集已创建。识别并点击您创建的数据集。点击后，您将能够查看您已注册的推理数据集的详细信息，如图*图12.12*所示：
- en: '![Figure 12.12 – Viewing the registered inference dataset'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.12 – 查看已注册的推理数据集'
- en: '](img/image012.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image012.jpg](img/image012.jpg)'
- en: Figure 12.12 – Viewing the registered inference dataset
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.12 – 查看已注册的推理数据集
- en: You can see all the essential attributes of your registered dataset in *Figure
    12.12*. It is important to note that the relative path is dynamic and it points
    to the referencing all of the `input.csv` files. The result of referencing all
    of the input files is shown in `input.csv` files will keep increasing as a new
    `input.csv` file is created in the datastore in Blob storage each day. Congratulations
    on registering the inference data. Next, we will configure the data drift monitor.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在*图12.12*中看到您已注册数据集的所有基本属性。需要注意的是，相对路径是动态的，它指向引用所有的`input.csv`文件。所有输入文件的引用结果将显示在`input.csv`文件中，随着在Blob存储的数据存储中每天创建新的`input.csv`文件，这些文件的数量将持续增加。恭喜您注册了推理数据。接下来，我们将配置数据漂移监控器。
- en: Creating the data drift monitor
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建数据漂移监控器
- en: 'To monitor data drift and feature drift, we will use built-in drift monitoring
    features from the Azure ML workspace as part of the `Data Drift Monitor` feature
    on our Azure ML workspace:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控数据漂移和特征漂移，我们将使用Azure ML工作区内置的漂移监控功能，作为我们Azure ML工作区上的`Data Drift Monitor`功能的一部分：
- en: Go to your workspace and access the **Datasets** section. Then, select **Dataset
    Monitors** (it is in preview mode at the moment, as this feature is still being
    tested). Click on **Create**, as shown in *Figure 12.13*:![Figure 12.13 – Creating
    the data drift monitor
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往您的 workspace 并访问**数据集**部分。然后，选择**数据集监控器**（目前处于预览模式，因为此功能仍在测试中）。点击**创建**，如图*图12.13*所示：![图12.13
    – 创建数据漂移监控器
- en: '](img/image0131.jpg)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image0131.jpg](img/image0131.jpg)'
- en: Figure 12.13 – Creating the data drift monitor
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.13 – 创建数据漂移监控器
- en: Upon selecting the **Create** button, you will be prompted to create a new data
    drift monitor. Select the target dataset of your choice.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择**创建**按钮后，系统将提示您创建一个新的数据漂移监控器。选择您所需的靶数据集。
- en: In the *Registering the target dataset* section, we registered the `inputs.csv`
    files as `Input-InferenceData`. Select your inference dataset as the target dataset,
    as shown in *Figure 12.14*:![Figure 12.14 – Select target dataset
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*注册靶数据集*部分，我们将`inputs.csv`文件注册为`Input-InferenceData`。选择您的推理数据集作为靶数据集，如图*图12.14*所示：![图12.14
    – 选择靶数据集
- en: '](img/image014.jpg)'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image014.jpg](img/image014.jpg)'
- en: Figure 12.14 – Select target dataset
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.14 – 选择靶数据集
- en: After selecting your target dataset, you will be prompted to point to your baseline
    dataset, which should be your training dataset (it was used to train your deployed
    ML model). Select your baseline dataset, as shown in *Figure 12.15*:![Figure 12.15
    – Select base dataset and configuring the monitor settings
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择您的靶数据集后，系统将提示您指向您的基线数据集，这应该是您的训练数据集（它被用来训练您的已部署ML模型）。选择您的基线数据集，如图*图12.15*所示：![图12.15
    – 选择基线数据集和配置监控设置
- en: '](img/image0151.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image0151.jpg](img/image0151.jpg)'
- en: Figure 12.15 – Select the baseline dataset and configure the monitor settings
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.15 – 选择基线数据集和配置监控设置
- en: After selecting the baseline dataset, you will be prompted to set up monitor
    settings, such as the name of data drift monitor (for example, `weather-Data-Drift`),
    the compute target to run data drift jobs, the frequency of data drift jobs (for
    example, once a day), and the threshold for monitoring drift (for example, 60).
    You will also be asked to give an email of your choice to receive notifications
    when the data drift surpasses a set threshold.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在选择基线数据集后，系统将提示您设置监控设置，例如数据漂移监控器的名称（例如，`weather-Data-Drift`）、运行数据漂移作业的计算目标、数据漂移作业的频率（例如，每天一次）以及监控漂移的阈值（例如，60）。您还将被要求提供一个电子邮件地址，以便在数据漂移超过设定的阈值时接收通知。
- en: After configuring the settings, create a data drift monitor. Go to your newly
    created data drift (in the **Datasets** section, click on **Dataset Monitors**
    to view your drift monitors), as shown in *Figure 12.16*:![Figure 12.16 – Data
    drift overview (it is currently empty)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在配置设置后，创建一个数据漂移监控器。转到您新创建的数据漂移（在**数据集**部分，点击**数据集监控器**以查看您的漂移监控器），如图*图12.16*所示：![Figure
    12.16 – 数据漂移概述（目前为空）
- en: '](img/image016.jpg)'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image016.jpg]'
- en: Figure 12.16 – Data drift overview (it is currently empty)
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.16 – 数据漂移概述（目前为空）
- en: When you access your data drift monitor, you will see that there is no data.
    This is because we haven't computed any drift yet. In order to compute drift,
    we need a compute resource.
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当您访问您的数据漂移监控器时，您会看到没有数据。这是因为我们还没有计算任何漂移。为了计算漂移，我们需要一个计算资源。
- en: Go to the **Compute** section, access the **Compute clusters** tab, and create
    a new compute resource (for example, **drift-compute – Standard_DS_V2 machine**),
    as shown in *Figure 12.17*:![Figure 12.17 – Creating a compute cluster to compute
    data drift
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转到**计算**部分，访问**计算集群**选项卡，创建一个新的计算资源（例如，**drift-compute – Standard_DS_V2 machine**），如图*图12.17*所示：![Figure
    12.17 – Creating a compute cluster to compute data drift
- en: '](img/image0171.jpg)'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image0171.jpg]'
- en: Figure 12.17 – Creating a compute cluster to compute data drift
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.17 – 创建计算集群以计算数据漂移
- en: After creating the compute cluster, go back to your data drift monitor (for
    example, **Weather-Data-Drift**). Next, we will compute the data drift.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在创建计算集群后，返回您的数据漂移监控器（例如，**Weather-Data-Drift**）。接下来，我们将计算数据漂移。
- en: Click on **Analyze existing data** and submit a run to analyze any existing
    inference data, as shown in *Figure 12.18*:![Figure 12.18 – Submitting run to
    analyze any data drift
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**分析现有数据**并提交一个运行以分析任何现有推断数据，如图*图12.18*所示：![Figure 12.18 – Submitting run
    to analyze any data drift
- en: '](img/image018.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/image018.jpg]'
- en: Figure 12.18 – Submitting run to analyze any data drift
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图12.18 – 提交运行以分析任何数据漂移
- en: Select the start and end dates and the compute target (which was created previously,
    that is, **drift-compute**). Then, click on **Submit** to run drift computation.
    It will usually take around 10 minutes to analyze and compute data drift. You
    can track the progress of your runs in the **Experiments** section of your Azure
    ML workspace.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择开始和结束日期以及计算目标（即之前创建的，即**drift-compute**）。然后，点击**提交**以运行漂移计算。通常需要大约10分钟来分析和计算数据漂移。您可以在Azure
    ML工作区的**实验**部分跟踪您运行的进度。
- en: '**Data drift analysis**: After successfully finishing the run, data drift has
    been computed. Using the drift overview, as shown in *Figure 12.19*, we can monitor
    and analyze your ML model performance in production. We can view the data drift
    magnitude and drift distribution by features:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**数据漂移分析**：在成功完成运行后，数据漂移已经被计算。使用如图*图12.19*所示的漂移概述，我们可以监控和分析您的ML模型在生产中的性能。我们可以通过特征查看数据漂移幅度和漂移分布：'
- en: '![Figure 12.19 – Data Drift magnitude trend'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 12.19 – Data Drift magnitude trend'
- en: '](img/image019.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image019.jpg]'
- en: Figure 12.19 – Data Drift magnitude trend
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.19 – 数据漂移幅度趋势
- en: The way that model drift is measured by the Azure ML service is that it uses
    a separate drift model (maintained by Azure), which looks at the baseline and
    compares inference data. This comparison results in a simple statistical percentage
    or degree of change in data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Azure ML服务测量模型漂移的方式是使用一个独立的漂移模型（由Azure维护），该模型查看基线并比较推断数据。这种比较结果是一个简单的百分比或数据变化的程度。
- en: In *Figure 12.19*, the **Drift magnitude trend** suggests that we have had inferences
    made to the model on 3 days (that is, **03/23/21**, **04/03/21**, and **04/04/21**).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图12.19*中，**漂移幅度趋势**表明我们对模型进行了3天的推断（即**03/23/21**，**04/03/21**，和**04/04/21**）。
- en: The analysis shows that the data drift on these three occasions is below the
    threshold of 70% (this is the red line, which indicates the threshold). The data
    drift on **03/23/21** is around 50%; on **04/03/21**, it is around 44%; and on
    **04/04/21**, it is 40%. This gives us an idea of the changing trend in the incoming
    inference data to the model. Likewise, we can monitor feature drift.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 分析显示，这三次的数据漂移低于70%的阈值（这是红线，表示阈值）。**03/23/21**的数据漂移大约为50%；**04/03/21**大约为44%；**04/04/21**为40%。这给我们提供了一个关于模型接收到的推断数据变化趋势的线索。同样，我们可以监控特征漂移。
- en: '**Feature drift analysis**: You can assess individual features and their drift
    by scrolling down to the **Feature details** section and selecting a feature of
    your choice. For example, we can see the **Temperature_C** distribution over time
    feature, as shown in *Figure 12.20*:'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征漂移分析**：你可以通过滚动到**特征详情**部分并选择一个你喜欢的特征来评估单个特征及其漂移。例如，我们可以看到**Temperature_C**随时间分布的特征，如图*图12.20*所示：'
- en: '![Figure 12.20 – Feature drift trend (Temperature_C)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.20 – 特征漂移趋势（Temperature_C）'
- en: '](img/image020.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片020](img/image020.jpg)'
- en: Figure 12.20 – Feature drift trend (Temperature_C)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.20 – 特征漂移趋势（Temperature_C）
- en: 'To monitor the feature change over time, we can select some metrics of our
    choice for the feature. Metrics such as **Mean value**, **Min value**, **Max value**,
    **Euclidean distance**, or **Wasserstein distance** are available to analyze feature
    drift. Select a metric of your choice (for example, **Mean value**). We have selected
    the **Mean value** metric to assess the temperature drift, as shown in *Figure
    12.20*. The **Mean value** metric has changed from 14 to 8 as time has progressed;
    this shows the change of drift in the **Temperature_C** feature. Such a change
    is expected as seasonal changes give rise to changes in temperature. We can also
    monitor feature distribution change, as shown in *Figure 12.21*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控特征随时间的变化，我们可以选择一些我们喜欢的特征指标。例如，**平均值**、**最小值**、**最大值**、**欧几里得距离**或**Wasserstein距离**，这些指标可用于分析特征漂移。选择一个你喜欢的指标（例如，**平均值**）。我们选择了**平均值**指标来评估温度漂移，如图*图12.20*所示。**平均值**指标从14变为8，随着时间的推移发生了变化；这显示了**Temperature_C**特征漂移的变化。这种变化是预期的，因为季节性变化会导致温度的变化。我们还可以监控特征分布的变化，如图*图12.21*所示：
- en: '![Figure 12.21 – The Feature distribution trend'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.21 – 特征分布趋势'
- en: '](img/image021.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片021](img/image021.jpg)'
- en: Figure 12.21 – The Feature distribution trend
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.21 – 特征分布趋势
- en: If the drift is drastic or anomalous, we need to check for the quality of input
    data being inferred into the system. Insights into feature drift enable us to
    understand the changing data and world around us. Likewise, we can monitor model
    drift to understand the model performance in accordance with the changing data
    and world.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果漂移剧烈或异常，我们需要检查被推断到系统中的输入数据的质量。对特征漂移的了解使我们能够理解周围不断变化的数据和世界。同样，我们可以监控模型漂移，以了解模型性能随数据和世界的变化。
- en: Model drift
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型漂移
- en: 'Monitoring model drift enables us to keep a check on our model performance
    in production. Model drift is where the properties of dependent variables change.
    For example, in our case, this is the classification results of the weather (that
    is, rain or no rain). Just as we set up data drift in the *Creating the data drift
    monitor* section, we can also set up a model drift monitor to monitor model outputs.
    Here are the high-level steps to set up model drift:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 监控模型漂移使我们能够监控我们的模型在生产中的性能。模型漂移是依赖变量属性发生变化的地方。例如，在我们的案例中，这是天气（即下雨或不下雨）的分类结果。正如我们在*创建数据漂移监控器*部分设置数据漂移一样，我们也可以设置模型漂移监控器来监控模型输出。以下是设置模型漂移的高级步骤：
- en: Register a new dataset (for example, `Outputs.csv` files. The **Outputs** dataset
    can be created from the **Datasets** section. When creating an outputs inference
    dataset, select the important columns (for example, **Future_Weather_Condition**)
    and change the dataset into tabular and time-series format (drift can only be
    computed in time-series data) by selecting a column with **Timestamp**.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注册一个新的数据集（例如，`Outputs.csv`文件。**输出**数据集可以从**数据集**部分创建。在创建输出推理数据集时，选择重要的列（例如，**未来天气条件**）并将数据集转换为表格和时间序列格式（漂移只能在时间序列数据中计算）通过选择带有**时间戳**的列。
- en: Create a new monitor (for example, the model drift monitor) from the **Dataset**
    section, and click on **Dataset Monitor**. Select the feature to monitor (for
    example, **future weather condition**) and set a threshold that you want to monitor.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从**数据集**部分创建一个新的监控器（例如，模型漂移监控器），然后点击**数据集监控器**。选择要监控的特征（例如，**未来天气条件**）并设置你想要监控的阈值。
- en: 'Analyze the model drift in the overview (as shown in *Figure 12.22*):'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在概述中分析模型漂移（如图*图12.22*所示）：
- en: '![ Figure 12.22 – Submitting run to analyze the data drift'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.22 – 提交运行以分析数据漂移'
- en: '](img/image022.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片022](img/image022.jpg)'
- en: Figure 12.22 – Submitting a run to analyze the data drift
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.22 – 提交运行以分析数据漂移
- en: If your model drift has surpassed a set threshold, then that may be an indication
    that you should retrain or train your model comparison results in a simple statistical
    percentage or a degree of change in data. When the data drift has gone past the
    threshold (for example, 70%), we can notify the administrator or product owner
    via email or take actions such as deploying another model or retraining the existing
    model. Using smart actions, we can govern ML systems to produce maximum value.
    We will explore ways to govern ML systems in the next chapter ([*Chapter 13*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234),
    *Governing the ML System for Continual Learning*). So far, we have implemented
    the setting up data drift, feature drift, and model drift. Next, let's monitor
    the ML system's application performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型漂移超过了设定的阈值，那么这可能是一个迹象，表明您应该重新训练或训练模型比较结果，这可以简单地表示为数据变化的百分比或程度。当数据漂移超过阈值（例如，70%）时，我们可以通过电子邮件通知管理员或产品所有者，或者采取部署另一个模型或重新训练现有模型等行动。使用智能操作，我们可以管理机器学习系统以产生最大价值。我们将在下一章中探讨管理机器学习系统的方法（[*第
    13 章*](B16572_13_Final_JM_ePub.xhtml#_idTextAnchor234)，*持续学习的机器学习系统管理*）。到目前为止，我们已经实现了数据漂移、特征漂移和模型漂移的设置。接下来，让我们监控机器学习系统的应用程序性能。
- en: Application performance
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用性能
- en: 'You have deployed the ML service in the form of REST API endpoints, which can
    be consumed by users. We can monitor these endpoints using Azure Application Insights
    (enabled by Azure Monitor). To monitor our application performance, access the
    Application Insights dashboard, as shown in *Figure 12.23*. Go to the **Endpoints**
    section in your Azure ML service workspace and select the REST API endpoint your
    ML model is deployed on. Click on **Application Insights url** to access the Application
    Insights endpoint connected to your REST API endpoint:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您已将机器学习服务以 REST API 端点形式部署，用户可以消费这些端点。我们可以使用 Azure Application Insights（由 Azure
    Monitor 启用）来监控这些端点。为了监控应用程序性能，访问应用洞察仪表板，如图 *图 12.23* 所示。转到您的 Azure ML 服务工作区中的
    **端点** 部分，并选择您的机器学习模型部署在其上的 REST API 端点。点击 **Application Insights url** 以访问与您的
    REST API 端点连接的应用洞察端点：
- en: '![Figure 12.23 – Application Insights Overview'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.23 – 应用洞察概览'
- en: '](img/image023.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image023.jpg)'
- en: Figure 12.23 – Application Insights Overview
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.23 – 应用洞察概览
- en: 'From the **Application Insights Overview** section, we can monitor and analyze
    critical application performance information for your ML service. Additionally,
    we can monitor information such as failed requests, server response time, server
    requests, and availability from the **Overview** section, as shown in *Figure
    12.24*:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从 **应用洞察概览** 部分中，我们可以监控和分析您的机器学习服务的关键应用性能信息。此外，我们还可以从 **概览** 部分监控失败请求、服务器响应时间、服务器请求和可用性等信息，如图
    *图 12.24* 所示：
- en: '![Figure 12.24 – Application Insights Overview'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.24 – 应用洞察概览'
- en: '](img/image024.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image024.jpg)'
- en: Figure 12.24 – Application Insights Overview
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.24 – 应用洞察概览
- en: 'Based on these metrics and this information, we can monitor the application
    performance. Ideally, we should not have any failed requests or long server response
    times. To get deeper insights into the application performance, we can access
    the application dashboard (by clicking on the button at the top of the screen),
    as shown in *Figure 12.25*:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些指标和这些信息，我们可以监控应用性能。理想情况下，我们不应该有任何失败请求或长的服务器响应时间。为了更深入地了解应用性能，我们可以访问应用仪表板（通过点击屏幕顶部的按钮），如图
    *图 12.25* 所示：
- en: '![Figure 12.25 – Application dashboard with a more detailed performance assessment'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 12.25 – 具有更详细性能评估的应用仪表板'
- en: '](img/image025.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image025.jpg)'
- en: Figure 12.25 – Application dashboard with a more detailed performance assessment
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.25 – 具有更详细性能评估的应用仪表板
- en: From the application dashboard, we can monitor the application performance in
    more detail. For instance, we can monitor application usage, reliability, and
    other information. In terms of usage, **Unique sessions and users** is critical
    information to monitor the number of unique users the application is able to serve.
    Additionally, the **Average availability** information is useful to assess the
    availability of the service for our users. With this information, we can make
    scaling decisions if more resources are needed to serve users.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从应用程序仪表板中，我们可以更详细地监控应用程序的性能。例如，我们可以监控应用程序的使用情况、可靠性和其他信息。在用途方面，**唯一会话和用户**是监控应用程序能够服务的唯一用户数量的关键信息。此外，**平均可用性**信息有助于评估服务对我们用户的可用性。有了这些信息，如果需要更多资源来服务用户，我们可以做出扩展决策。
- en: 'We can monitor application reliability by assessing information such as the
    number of failed requests, server exceptions, and dependency failures. We can
    monitor responsiveness using information such as the average server response time
    and CPU utilization. Ideally, the application should not have any failures, and
    if there are any failures, we can take a deeper look into the application logs,
    as shown in *Figure 12.26*, by accessing **Transaction search** or logs:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过评估诸如失败请求的数量、服务器异常和依赖性故障等信息来监控应用程序的可靠性。我们可以使用诸如平均服务器响应时间和CPU利用率等信息来监控响应性。理想情况下，应用程序不应有任何故障，如果有任何故障，我们可以通过访问**事务搜索**或日志，如图*12.26*所示，进行更深入的检查：
- en: '![Figure 12.26 – Accessing the logs to understand any errors or failures'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 12.26 – Accessing the logs to understand any errors or failures'
- en: '](img/image026.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/image026.jpg]'
- en: Figure 12.26 – Accessing the logs to understand any errors or failures
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.26 – 访问日志以了解任何错误或故障
- en: We can take a closer look into the logs of applications to understand any failures
    or errors in order to debug the application and maintain the healthy functioning
    of the application. A functional ML application results in satisfied users and
    maximum business impact. Therefore, monitoring applications can be rewarding to
    reveal potential failures and maintain the application in order to serve users
    in the most efficient way.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更仔细地查看应用程序的日志，以了解任何故障或错误，以便调试应用程序并维护应用程序的健康运行。一个功能性的机器学习应用程序会导致用户满意和最大的商业影响。因此，监控应用程序可以揭示潜在的故障并维护应用程序，以便以最有效的方式为用户提供服务。
- en: Analyzing your ML system
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析您的机器学习系统
- en: Monitoring and analyzing your ML system in production in real time is key to
    understanding the performance of your ML system and ensuring its robustness to
    produce maximized business value. Humans play a key role in analyzing model performance
    and detecting subtle anomalies and threats. We can analyze model performance to
    detect any biases or threats and to understand why the model makes decisions in
    a certain pattern. We can do this by applying advanced techniques, such as data
    slicing, adversarial attack prevention techniques, or by understanding local and
    global explanations.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中实时监控和分析您的机器学习系统对于理解机器学习系统的性能和确保其稳健性以产生最大化的商业价值至关重要。人类在分析模型性能和检测细微异常和威胁中扮演着关键角色。我们可以通过应用高级技术，如数据切片、对抗攻击预防技术，或通过理解局部和全局解释来分析模型性能，以检测任何偏差或威胁，并理解模型为何以某种模式做出决策。
- en: Data slicing
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据切片
- en: For our use case, we will leave out data slicing as we do not have much variety
    in terms of demographics or samples within the data (for example, sex, age groups,
    and more). To measure the fairness of the model, we will focus on bias detection.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的用例，我们将省略数据切片，因为我们没有太多关于人口统计学或数据样本（例如，性别、年龄组等）的多样性。为了衡量模型的公平性，我们将专注于偏差检测。
- en: Bias and threat detection
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 偏差和威胁检测
- en: To determine the model bias in production, we can use a bias-variance trade-off
    method. This makes it simple to monitor and analyze the model bias or any possible
    threat. It goes without saying that there might be better methods to monitor bias,
    but the idea here is to keep it simple, as, sometimes, simplicity is better and
    more efficient.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定生产中的模型偏差，我们可以使用偏差-方差权衡方法。这使得监控和分析模型偏差或任何可能的威胁变得简单。不言而喻，可能存在更好的方法来监控偏差，但这里的想法是保持简单，因为有时简单更好、更高效。
- en: The disparity between our model's average prediction and the right value we
    are attempting to predict is the bias. Variance is the variability of the estimation
    of the model for a given data point or a value that informs us of our data spread.
    Analyzing bias and variance for inference data for the deployed model reveals
    the bias to be 20.1 and variance to be 1.23 (you can read more on analyzing bias
    and variance at [https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/)).
    This means our model has high bias and low variance; therefore, it might be a
    good idea to train or retrain our model with inference data to balance the bias-variance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型平均预测值与我们试图预测的正确值之间的差异是偏差。方差是模型对给定数据点或值的估计的变异性，它告诉我们我们的数据分布。分析部署模型的推理数据的偏差和方差揭示了偏差为20.1，方差为1.23（你可以在[https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/](https://machinelearningmastery.com/calculate-the-bias-variance-trade-off/)上了解更多关于偏差和方差分析的内容）。这意味着我们的模型具有高偏差和低方差；因此，使用推理数据训练或重新训练我们的模型以平衡偏差-方差可能是一个好主意。
- en: Local and global explanations
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地解释和全局解释
- en: Local and global explanations offer different perspectives on model performance.
    Local explanation offers a justification for model prediction for a specific or
    individual input, whereas global explanation provides insights into the model's
    predictive process, independent of any particular input. We previously looked
    at global explanations while exploring monitoring drifts in *Figure 12.19*. We
    can further investigate feature distribution, as shown in *Figure 12.21*, to understand
    local explanations in detail.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本地解释和全局解释提供了对模型性能的不同视角。本地解释为特定或单个输入的模型预测提供了合理性，而全局解释则提供了对模型预测过程的洞察，不受任何特定输入的影响。我们在探索*图12.19*中的监控漂移时，之前已经研究了全局解释。我们可以进一步调查特征分布，如图*图12.21*所示，以详细了解本地解释。
- en: Analyzing your ML system for fairness, bias, and local and global explanations
    gives us key insights into model performance, and we can use this information
    to govern our ML system.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 分析你的ML系统以公平性、偏差以及本地和全局解释，为我们提供了对模型性能的关键洞察，我们可以利用这些信息来治理我们的ML系统。
- en: Governing your ML system
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理你的ML系统
- en: 'A great part of system governance involves quality assurance and control, model
    auditing, and reporting to have end-to-end trackability and compliance with regulations.
    The ML systems'' efficacy (that is, its ability to produce a desired or intended
    result) is dependent on the way it is governed to achieve maximum business value.
    So far, we have monitored and analyzed our deployed model for inference data:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 系统治理的大部分工作涉及质量保证和控制、模型审计以及报告，以确保端到端的可追踪性和符合法规。ML系统的有效性（即其产生预期或意图结果的能力）取决于其治理方式，以实现最大商业价值。到目前为止，我们已经监控并分析了我们的部署模型用于推理数据：
- en: '![Figure 12.27 – Components of governing your ML system'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '![图12.27 – 管理你的ML系统的组件'
- en: '](img/image027.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/image027.jpg)'
- en: Figure 12.27 – Components of governing your ML system
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.27 – 管理你的ML系统的组件
- en: The efficacy of an ML system can be determined by using smart actions that are
    taken based on monitoring and alerting. In the next chapter, we will explore ML
    system governance in terms of alerts and actions, model QA and control, and model
    auditing and reports.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ML系统的有效性可以通过基于监控和警报采取的智能行动来确定。在下一章中，我们将探讨ML系统治理，包括警报和行动、模型QA和控制以及模型审计和报告。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the key principles of serving ML models to
    our users and monitoring them to achieve maximized business value. We explored
    the different means of serving ML models for users or consumers of the model and
    implemented the Explainable Monitoring framework for a hypothetical business use
    case and deployed a model. We carried out this hands-on implementation of an Explainable
    Monitoring framework to measure the performance of ML systems. Finally, we discussed
    the need for governing ML systems to ensure the robust performance of ML systems.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了向用户提供服务ML模型以及监控它们以实现最大商业价值的关键原则。我们探讨了为模型用户或消费者提供ML模型的不同方式，并为一个假设的商业用例实现了可解释监控框架并部署了一个模型。我们进行了可解释监控框架的动手实现，以衡量ML系统的性能。最后，我们讨论了治理ML系统的必要性，以确保ML系统的稳健性能。
- en: We will further explore the governance of ML systems and continual learning
    concepts in the next and final chapter!
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章和最后一章进一步探讨ML系统的治理和持续学习概念！
