- en: Chapter 7
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第七章
- en: Mixture Models
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 混合模型
- en: '...the father has the form of a lion, the mother of an ant; the father eats
    flesh and the mother herbs. And these breed the ant-lion... –The Book of Imaginary
    Beings'
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: …父亲有狮子的形态，母亲有蚂蚁的形态；父亲吃肉，母亲吃草。于是他们孕育了蚂蚁狮子…——《虚构生物志》
- en: The River Plate (also known as La Plata River or Río de la Plata) is the widest
    river on Earth and a natural border between Argentina and Uruguay. During the
    late 19^(th) century, the port area along this river was a place where indigenous
    people mixed with Africans (most of them slaves) and European immigrants. One
    consequence of this encounter was the mix of European music, such as the waltz
    and mazurka, with the African candombe and Argentinian milonga (which, in turn,
    is a mix of Afro-American rhythms), giving origin to the dance and music we now
    call the tango.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉塔河（也称为拉普拉塔河或拉普拉塔江）是地球上最宽的河流，也是阿根廷和乌拉圭之间的自然边界。在19世纪末，这条河沿岸的港口区是土著人、非洲人（大多数是奴隶）和欧洲移民的混居地。这种文化交汇的一个结果是，欧洲音乐如华尔兹和马祖卡与非洲坎东贝舞曲和阿根廷米隆加（后者又是非洲裔美国节奏的混合）相融合，创造了我们现在所称的探戈舞和音乐。
- en: Mixing previously existing elements is a great way to create new things, not
    only in the context of music. In statistics, mixture models are one common approach
    to model building. These models are built by mixing simpler distributions to obtain
    more complex ones. For example, we can combine two Gaussians to describe a bimodal
    distribution or many Gaussians to describe arbitrary distributions. While using
    Gaussians is very common, in principle we can mix any family of distributions
    we want. Mixture models are used for different purposes, such as directly modeling
    sub-populations or as a useful trick for handling complicated distributions that
    cannot be described with simpler distributions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 将先前存在的元素混合在一起是创造新事物的好方法，这不仅仅是在音乐领域。在统计学中，混合模型是建模的常见方法之一。这些模型通过混合更简单的分布来获得更复杂的分布。例如，我们可以结合两个高斯分布来描述双峰分布，或者结合多个高斯分布来描述任意分布。尽管使用高斯分布非常普遍，但原则上我们可以混合任何我们想要的分布族。混合模型用于不同的目的，例如直接建模子人群，或者作为处理无法用简单分布描述的复杂分布的有用技巧。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Finite mixture models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有限混合模型
- en: Zero-inflated and hurdle models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零膨胀模型和障碍模型
- en: Infinite mixture models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无限混合模型
- en: Continuous mixture models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续混合模型
- en: 7.1 Understanding mixture models
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 理解混合模型
- en: Mixture models naturally arise when the overall population is a combination
    of distinct sub-populations. A familiar example is the distribution of heights
    in a given adult human population, which can be described as a mixture of female
    and male sub-populations. Another classical example is the clustering of handwritten
    digits. In this case, it is very reasonable to expect 10 sub-populations, at least
    in a base 10 system! If we know to which sub-population each observation belongs,
    it is generally a good idea to use that information to model each sub-population
    as a separate group. However, when we do not have direct access to this information,
    mixture models come in handy.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当总体人群是不同子人群的组合时，自然会出现混合模型。一个常见的例子是给定成人群体中的身高分布，它可以被描述为女性和男性子人群的混合。另一个经典的例子是手写数字的聚类。在这种情况下，期望有10个子人群是非常合理的，至少在十进制系统中是这样！如果我们知道每个观察值属于哪个子人群，通常来说，利用这些信息将每个子人群建模为一个独立的群体是一个好主意。然而，当我们无法直接访问这些信息时，混合模型就显得特别有用。
- en: Blends of Distributions
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的混合
- en: Many datasets cannot be properly described using a single probability distribution,
    but they can be described as a mixture of such distributions. Models that assume
    data comes from a mixture of distributions are known as mixture models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据集无法用单一的概率分布准确描述，但可以通过将其描述为多种分布的混合来进行建模。假设数据来自混合分布的模型被称为混合模型。
- en: When building a mixture model, it is not necessary to believe we are describing
    true sub-populations in the data. Mixture models can also be used as a statistical
    trick to add flexibility to our toolbox. Take, for example, the Gaussian distribution.
    We can use it as a reasonable approximation for many unimodal and approximately
    symmetrical distributions. But what about multimodal or skewed distributions?
    Can we use Gaussian distributions to model them? Yes, we can, if we use a mixture
    of Gaussians.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建混合模型时，并不一定需要相信我们正在描述数据中的真实子群体。混合模型也可以作为一种统计技巧，为我们的工具箱增添灵活性。以高斯分布为例，我们可以将其作为许多单峰且近似对称分布的合理近似。那么，对于多峰或偏斜分布呢？我们能用高斯分布来建模吗？当然可以，如果我们使用高斯混合。
- en: In a Gaussian mixture model, each component will be a Gaussian with a different
    mean and, generally (but not necessarily), a different standard deviation. By
    combining Gaussians, we can add flexibility to our models and fit complex data
    distributions. In fact, we can approximate practically any distribution we want
    by using a proper combination of Gaussians. The exact number of distributions
    will depend on the accuracy of the approximation and the details of the data.
    Actually, we have been using this idea in many of the plots throughout this book.
    The Kernel Density Estimation (KDE) technique is a non-Bayesian implementation
    of this idea. Conceptually, when we call `az.plot_kde`, the function places a
    Gaussian, with a fixed variance, on top of each data point and then sums all the
    individual Gaussians to approximate the empirical distribution of the data. *Figure
    [7.1](#x1-138002r1)* shows an example of how we can mix 8 Gaussians to represent
    a complex distribution, like a boa constrictor digesting an elephant, or a hat,
    depending on your perspective.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯混合模型中，每个成分都是一个均值不同的高斯分布，并且通常（但不一定）具有不同的标准差。通过组合高斯分布，我们可以为我们的模型增添灵活性，并拟合复杂的数据分布。事实上，我们可以通过适当组合高斯分布来逼近几乎任何我们想要的分布。分布的具体数量将取决于近似的准确度和数据的细节。实际上，我们在本书中的许多图表中已经使用了这个思想。核密度估计（KDE）技术就是这一思想的非贝叶斯实现。从概念上讲，当我们调用
    `az.plot_kde` 时，函数会在每个数据点上方放置一个固定方差的高斯分布，然后将所有单独的高斯分布求和，以逼近数据的经验分布。*图 [7.1](#x1-138002r1)*
    显示了如何将 8 个高斯分布混合以表示复杂分布的示例，就像一条蟒蛇消化一只大象，或者一个帽子，取决于你的视角。
- en: In *Figure [7.1](#x1-138002r1)*, all Gaussians have the same variance and they
    are centered at the gray dots, which represent sample points from a possible unknown
    population. If you look carefully, you may notice that two of the Gaussians are
    on top of each other.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *图 [7.1](#x1-138002r1)* 中，所有高斯分布具有相同的方差，并且它们都集中在灰色的点上，这些点代表可能的未知人群中的样本点。如果你仔细观察，可能会发现两个高斯分布重叠在一起。
- en: '![PIC](img/file196.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file196.png)'
- en: '**Figure 7.1**: Example of a KDE as a Gaussian mixture model'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.1**：作为高斯混合模型的 KDE 示例'
- en: Whether we believe in sub-populations or use them for mathematical convenience
    (or even something in the middle), mixture models are a useful way of adding flexibility
    to our models by using a mixture of distributions to describe the data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是相信子群体的存在，还是将其作为数学上的便利（甚至是介于两者之间的某种观点），混合模型通过使用分布的混合来描述数据，是为我们的模型增添灵活性的一种有效方式。
- en: 7.2 Finite mixture models
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 有限混合模型
- en: 'One way to build mixture models is to consider a finite weighted mixture of
    two or more distributions. Then the probability density of the observed data is
    a weighted sum of the probability density of *K* subgroups:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 构建混合模型的一种方法是考虑两个或更多分布的有限加权混合。然后，观察数据的概率密度是 *K* 个子群体的概率密度加权求和：
- en: '![ ∑K p(y) = wip (y | θi) i=1 ](img/file197.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![ ∑K p(y) = wip (y | θi) i=1 ](img/file197.jpg)'
- en: We can interpret *w*[*i*] as the probability of the component *i*, and thus
    its values are restricted to the interval [0, 1] and they need to sum up to 1\.
    The components *p*(*y*|*θ*[*i*]) are usually simple distributions, such as a Gaussian
    or a Poisson. If *K* is finite, we have a finite mixture model. To fit such a
    model, we need to provide a value of *K*, either because we know the correct value
    beforehand or because we can make an educated guess.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 *w*[*i*] 解释为成分 *i* 的概率，因此其值被限制在区间 [0, 1] 内，并且它们的和需要为 1。成分 *p*(*y*|*θ*[*i*])
    通常是简单的分布，比如高斯分布或泊松分布。如果 *K* 是有限的，那么我们就得到了一个有限混合模型。为了拟合这样的模型，我们需要提供一个 *K* 值，无论是因为我们事先知道正确的值，还是因为我们能做出有根据的猜测。
- en: Conceptually, to solve a mixture model, all we need to do is properly assign
    each data point to one of the components. In a probabilistic model, we can do
    this by introducing a random variable, whose function is to specify to which component
    a particular observation is assigned. This variable is generally referred to as
    a **latent variable** because we cannot directly observe it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，解决混合模型的问题，我们只需要正确地将每个数据点分配到一个成分。在一个概率模型中，我们可以通过引入一个随机变量来实现这一点，随机变量的功能是指定某个观察值被分配到哪个成分。这个变量通常被称为**潜变量**，因为我们不能直接观察到它。
- en: For a mixture of only two components (*K* = 2), we can use the coin-flipping
    problem model as a building block. For that model, we have two possible outcomes
    and we use the Bernoulli distribution to describe them. Since we do not know the
    probability of getting heads or tails, we use a Beta distribution as a prior distribution.
    If instead of head or tails, we think of any two groups (or components or classes),
    we can assign the observation to one group if we get 0 and to the other if we
    get 1\. This is all very nice, but in a mixture model, we can have two or more
    groups, so we need to make this idea more general. We can do it by noticing that
    the generalization of the Bernoulli distribution to *K* outcomes is the Categorical
    distribution and the generalization of the Beta distribution to higher dimensions
    is the Dirichlet distribution. If the components we are assigning are Gaussians,
    like in *Figure [7.1](#x1-138002r1)*, then *Figure [7.2](#x1-139003r2)* shows
    a Kruschke-style diagram of such a model. The rounded-corner box indicates that
    we have *K* components and the Categorical variables decide which of them we use
    to describe a given data point. Notice that only *μ*[*k*] depends on the different
    components, while *σ*[*μ*] and *σ*[*σ*] are shared for all of them. This is just
    a modeling choice; if necessary, we can change it and allow other parameters to
    be conditioned on each component.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于只有两个成分的混合模型（*K* = 2），我们可以使用抛硬币问题模型作为构建模块。在该模型中，我们有两种可能的结果，并且使用伯努利分布来描述它们。由于我们不知道正面或反面的概率，我们使用
    Beta 分布作为先验分布。如果我们把硬币的正面和反面换成任何两个组（或成分或类别），我们可以在得到 0 时将观察值分配到一个组，得到 1 时分配到另一个组。这一切都很好，但在混合模型中，我们可能有两个或更多组，因此我们需要使这个想法更加通用。我们可以通过注意到伯努利分布到
    *K* 个结果的推广是类别分布，而 Beta 分布到更高维度的推广是狄利克雷分布来实现这一点。如果我们分配的成分是高斯分布，就像在*图 [7.1](#x1-138002r1)*
    中一样，那么*图 [7.2](#x1-139003r2)* 显示了一个类似 Kruschke 风格的模型图。圆角框表示我们有 *K* 个成分，类别变量决定了我们用哪个成分来描述给定的数据点。请注意，只有
    *μ*[*k*] 依赖于不同的成分，而 *σ*[*μ*] 和 *σ*[*σ*] 是所有成分共享的。这只是一个建模选择；如果需要，我们可以改变它，并允许其他参数依赖于每个成分。
- en: '![PIC](img/file198.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file198.png)'
- en: '**Figure 7.2**: Kruschke-style diagram of a mixture model with Gaussian components'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.2**：带有高斯成分的混合模型的 Kruschke 风格图'
- en: We are going to implement this model in PyMC, but before doing that, let me
    introduce the Categorical and Dirichlet distributions. If you are already familiar
    with these distributions, you can skip the next two sections and jump to the example.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 PyMC 中实现这个模型，但在此之前，让我介绍一下类别分布和狄利克雷分布。如果你已经熟悉这些分布，可以跳过接下来的两节，直接跳到示例部分。
- en: 7.2.1 The Categorical distribution
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 类别分布
- en: 'The Categorical distribution is the most general discrete distribution and
    is parameterized by a vector where each element specifies the probabilities of
    each possible outcome. *Figure [7.3](#x1-140003r3)* represents two possible instances
    of the Categorical distribution. The dots represent the values of the Categorical
    distribution, while the continuous dashed lines are a visual aid to help us easily
    grasp the *shape* of the distribution:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 类别分布是最一般的离散分布，它由一个向量进行参数化，其中每个元素指定每个可能结果的概率。*图 [7.3](#x1-140003r3)* 表示了类别分布的两个可能实例。点表示类别分布的值，而连续的虚线是一个视觉辅助，帮助我们轻松理解分布的*形状*：
- en: '![PIC](img/file199.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file199.png)'
- en: '**Figure 7.3**: Two members of the Categorical distribution family. The dashed
    lines are just a visual aid.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.3**：类别分布族的两个成员。虚线只是视觉辅助。'
- en: 7.2.2 The Dirichlet distribution
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 狄利克雷分布
- en: 'The Dirichlet distribution lives in the simplex, which you can think of as
    an n-dimensional triangle; a 1-simplex is a line, a 2-simplex is a triangle, a
    3-simplex is a tetrahedron, and so on. Why a simplex? Intuitively, because the
    output of this distribution is a *K*-length vector, whose elements are restricted
    to be on the interval [0*,*1] and sum up to 1\. As we said, the Dirichlet distribution
    is the generalization of the Beta distribution. Thus, a good way to understand
    the former is to compare it to the latter. We use the Beta for problems with two
    outcomes: one with probability *p* and the other 1 − *p*. As we can see, *p* +
    (1 − *p*) = 1\. The Beta returns a two-element vector, (*p,q* = 1 − *p*), but
    in practice, we omit *q* as the outcome is entirely determined once we know the
    value of *p*. If we want to extend the Beta distribution to three outcomes, we
    need a three-element vector (*p,q,r*), where each element is in the interval [0*,*1]
    and their values sum up to 1\. Similar to the Beta distribution, we could use
    three scalars to parameterize such a distribution, and we may call them *α*, *β*,
    and *γ*; however, we could easily run out of Greek letters as there are only 24
    of them. Instead, we can just use a vector named *α* of length *K*. Note that
    we can think of the Beta and Dirichlet as distributions over proportions. *Figure
    [7.4](#x1-141003r4)* shows 4 members of the Dirichlet distribution when *k* =
    3\. On the top, we have the pdf and on the bottom, we have samples from the distribution.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Dirichlet 分布存在于单纯形中，你可以将其视为一个n维三角形；一个1-单纯形是一个线段，2-单纯形是一个三角形，3-单纯形是一个四面体，依此类推。为什么是单纯形？直观上，因为这个分布的输出是一个*K*维向量，其元素被限制在区间
    [0,1] 内，并且它们的和为1。正如我们所说，Dirichlet 分布是 Beta 分布的推广。因此，理解 Dirichlet 分布的一个好方法是将其与
    Beta 分布进行比较。Beta 分布用于处理两种结果的问题：一种的概率是*p*，另一种的概率是1 − *p*。正如我们所见，*p* + (1 − *p*)
    = 1。Beta 分布返回一个两元素向量（*p, q* = 1 − *p*），但在实际应用中，我们省略了*q*，因为一旦知道*p*的值，结果就完全确定了。如果我们要将
    Beta 分布扩展到三种结果，我们需要一个三元素向量（*p, q, r*），其中每个元素都在区间 [0,1] 内，且它们的和为1。类似于 Beta 分布，我们可以使用三个标量来对这种分布进行参数化，我们可以称它们为*α*,
    *β* 和 *γ*；然而，由于希腊字母只有24个，我们很容易就会用完它们。相反，我们可以使用一个名为*α*的*K*维向量。请注意，我们可以将 Beta 和
    Dirichlet 看作是比例分布。*图 [7.4](#x1-141003r4)*展示了当*k* = 3时，Dirichlet 分布的4个成员。在顶部是概率密度函数（pdf），在底部是来自该分布的样本。
- en: '![PIC](img/file200.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file200.png)'
- en: '**Figure 7.4**: Four members of the Dirichlet distribution family'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.4**：Dirichlet 分布族的四个成员'
- en: Okay, now we have all the *components* to implement our first mixture model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经具备了实现我们第一个混合模型的*所有组件*。
- en: 7.2.3 Chemical mixture
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 化学混合物
- en: To keep things very concrete, let’s work on an example. We are going to use
    the chemical shifts data we already saw in *Chapter [2](CH02.xhtml#x1-440002)*.
    *Figure [7.5](#x1-142002r5)* shows a histogram of this data.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让事情更具体，我们来做一个例子。我们将使用我们在*第 [2](CH02.xhtml#x1-440002)* 章中看到的化学位移数据。*图 [7.5](#x1-142002r5)*展示了这组数据的直方图。
- en: '![PIC](img/file201.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file201.png)'
- en: '**Figure 7.5**: Histogram for the chemical shifts data'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.5**：化学位移数据的直方图'
- en: We can see that this data cannot be properly described using a single distribution
    like a Gaussian, but we can do it if we use many as we already showed in *Figure
    [7.1](#x1-138002r1)*; maybe three or four would do the trick. There are good theoretical
    reasons, which we will ignore and not discuss here, indicating this chemical shifts
    data comes from a mixture of 40 sub-populations. But just by looking at the data,
    it seems impossible to recover the true groups as there is a lot of overlap between
    them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这些数据无法用像高斯这样的单一分布来准确描述，但如果我们使用多个分布，就能做到这一点，就像我们在*图 [7.1](#x1-138002r1)*中展示的那样；也许三个或四个分布就能解决问题。虽然有充分的理论依据（我们在这里不讨论），表明这些化学位移数据来自于40个子群体的混合分布，但仅通过观察数据，我们似乎无法恢复出真正的群体，因为它们之间有大量的重叠。
- en: 'The following code block shows the implementation of a Gaussian mixture model
    of two components in PyMC:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块展示了在 PyMC 中实现两个分量的高斯混合模型：
- en: '**Code 7.1**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.1**'
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you run this code, you will find that it is very slow and the trace looks
    very bad (refer to *Chapter [10](CH10.xhtml#x1-18900010)* to learn more about
    diagnostics). Can we make this model run faster? Yes, let’s see how.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行这段代码，你会发现它非常慢，且轨迹看起来非常糟糕（参考*第 [10](CH10.xhtml#x1-18900010) 章*了解更多诊断信息）。我们能让这个模型跑得更快吗？可以，让我们看看如何操作。
- en: 'In `model_kg`, we have explicitly included the latent variable *z* in the model.
    Sampling this discrete variable usually leads to poor posterior sampling. One
    way to solve this issue is to reparametrize the model so *z* is no longer explicitly
    a part of the model. This type of reparametrization is called marginalization
    . Marginalizing out discrete variables usually provides speed-up and better sampling.
    Unfortunately, it requires some mathematical skills that not everyone has. Luckily
    for us, we don’t need to do this ourselves as PyMC includes a `NormalMixture`
    distribution. So, we can write the mixture model as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在`model_kg`中，我们已经明确将潜在变量*z*包含在模型中。对这个离散变量进行采样通常会导致后验采样不良。解决这个问题的一种方法是对模型进行重新参数化，使得*z*不再是模型的显式组成部分。这种重新参数化方法叫做边缘化（marginalization）。将离散变量边缘化通常能够提高速度并改进采样。不幸的是，这需要一些数学技能，而不是每个人都具备。不过幸运的是，我们不需要自己动手，因为PyMC已经包含了一个`NormalMixture`分布。所以，我们可以将混合模型写成如下形式：
- en: '**Code 7.2**'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.2**'
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let’s check the results with a forest plot. *Figure [7.6](#x1-142023r6)* shows
    something really funny going on. Before moving on to the next section, take some
    time to think about it. Can you spot it? We will discuss it in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过森林图来检查结果。*图 [7.6](#x1-142023r6)*显示了一些有趣的现象。在进入下一个章节之前，花些时间思考一下。你能发现问题吗？我们将在下一节讨论它。
- en: '![PIC](img/file202.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file202.png)'
- en: '**Figure 7.6**: Forest plot of the means from `model_mg`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.6**：`model_mg`的均值森林图'
- en: 7.3 The non-identifiability of mixture models
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 混合模型的非可识别性
- en: The `means` parameter has shape 2, and from *Figure [7.6](#x1-142023r6)* we
    can see that one of its values is around 47 and the other is close to 57.5\. The
    funny thing is that we have one chain saying that `means[0]` is 47 and the other
    3 saying it is 57.5, and the opposite for `mmeans[1]`. Thus, if we compute the
    mean of `mmeans[0]`, we will get some value close to 55 (57*.*5 × 3 + 47 × 1),
    which is not the correct value. What we are seeing is an example of a phenomenon
    known as parameter non-identifiability. This happens because, from the perspective
    of the model, there is no difference if component 1 has a mean of 47 and component
    2 has a mean of 57.5 or vice versa; both scenarios are equivalent. In the context
    of mixture models, this is also known as the label-switching problem.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`means`参数的形状为2，从*图 [7.6](#x1-142023r6)*我们可以看到，其中一个值大约是47，另一个接近57.5。搞笑的是，我们有一条链说`means[0]`是47，其他三条链说它是57.5，`means[1]`则正好相反。因此，如果我们计算`means[0]`的均值，我们会得到一个接近55的值（57.5
    × 3 + 47 × 1），这显然不是正确的值。我们看到的正是一个名为参数非可识别性（parameter non-identifiability）现象的例子。这是因为，从模型的角度来看，如果组件1的均值为47而组件2的均值为57.5，或者反之，两种情况是等价的。在混合模型的背景下，这也被称为标签交换问题（label-switching
    problem）。'
- en: Non-Identifiability
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 非可识别性
- en: A statistical model is non-identifiable if one or more of its parameters cannot
    be uniquely determined. Parameters in a model are not identified if the same likelihood
    function is obtained for more than one choice of the model parameters. It could
    happen that the data does not contain enough information to estimate the parameters.
    In other cases, parameters may not be identifiable because the model is not structurally
    identifiable, meaning that the parameters cannot be uniquely determined even if
    all the necessary data is available.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型的一个或多个参数无法被唯一确定，则该统计模型是不可识别的。如果对于模型参数的多个选择，得到相同的似然函数，则该模型的参数没有被识别。可能出现的情况是数据中没有足够的信息来估计这些参数。在其他情况下，参数可能无法识别，因为模型在结构上不可识别，这意味着即使所有必要的数据都可用，参数仍然无法唯一确定。
- en: With mixture models, there are at least two ways of parameterizing a model to
    remove the non-identifiability issue. We can force the components to be ordered;
    for example, arrange the means of the components in strictly increasing order
    and/or use informative priors.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于混合模型，有至少两种方式可以对模型进行参数化，从而消除非可识别性问题。我们可以强制使组件按顺序排列；例如，将组件的均值按严格递增的顺序排列和/或使用信息量大的先验分布。
- en: Using PyMC, we can implement the first option with a transformation as in the
    next code block. Notice that we also provide initial values for the means; anything
    to ensure that the first mean is smaller than the second one will work.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用PyMC，我们可以通过下一个代码块中的转换实现第一个选项。请注意，我们还提供了均值的初始值；任何能确保第一个均值小于第二个均值的方式都可以。
- en: '**Code 7.3**'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.3**'
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s check the new results with a forest plot. *Figure [7.7](#x1-143014r7)*
    confirms that we have removed the non-identifiability issue:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过森林图检查新的结果。*图 [7.7](#x1-143014r7)* 确认我们已解决了不可识别性问题：
- en: '![PIC](img/file203.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file203.png)'
- en: '**Figure 7.7**: Forest plot of the means from `model_mgo`'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.7**：`model_mgo` 的均值森林图'
- en: 7.4 How to choose K
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 如何选择 K
- en: One of the main concerns with finite mixture models is how to decide on the
    number of components. A rule of thumb is to begin with a relatively small number
    of components and then increase it to improve the model-fit evaluation. As we
    already know from *Chapter [5](CH05.xhtml#x1-950005)*, model fit can be evaluated
    using posterior-predictive checks, metrics such as the ELPD, and the expertise
    of the modeler(s).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有关有限混合模型的一个主要问题是如何确定成分的数量。一个经验法则是从相对较小的成分数开始，然后增加成分数来改善模型拟合度。如我们在 *第 [5](CH05.xhtml#x1-950005)
    章* 中已经知道的，模型拟合可以通过后验预测检查、ELPD 等指标以及模型者的专业知识来评估。
- en: 'Let us compare the model for *K* = {2*,*3*,*4*,*5}. To do this, we are going
    to fit the model four times and then save the data and model objects for later
    use:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较 *K* = {2*,*3*,*4*,*5*} 的模型。为此，我们将拟合该模型四次，然后保存数据和模型对象以备后用：
- en: '**Code 7.4**'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.4**'
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '*Figure [7.8](#x1-144023r8)* shows the mixture models for *K* number of Gaussians.
    The black solid lines are the posterior means, and the gray lines are samples
    from the posterior. The mean-Gaussian components are represented using a black
    dashed line.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [7.8](#x1-144023r8)* 展示了 *K* 个高斯分布的混合模型。黑色实线表示后验均值，灰色线表示后验样本。均值高斯分量使用黑色虚线表示。'
- en: '![PIC](img/file204.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file204.png)'
- en: '**Figure 7.8**: Gaussian mixture models for different numbers of Gaussians
    (*K*)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.8**：不同数量高斯分布的高斯混合模型 (*K*)'
- en: Visually, it seems *K* = 2 is too low, but how do we choose a better value?
    As we have already discussed in *Chapter [5](CH05.xhtml#x1-950005)*, we can use
    posterior predictive checks of test quantities of interest and compute Bayesian
    p-values. *Figure [7.9](#x1-144024r9)* shows an example of such a calculation
    and visualization. *K* = 5 is the best solution and *K* = 4 comes closes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉效果来看，*K* = 2 太低了，但我们如何选择一个更好的值呢？正如我们在 *第 [5](CH05.xhtml#x1-950005) 章* 中讨论过的，我们可以使用后验预测检查来评估感兴趣的测试量，并计算贝叶斯
    p 值。*图 [7.9](#x1-144024r9)* 展示了此类计算和可视化的示例。*K* = 5 是最佳解，*K* = 4 接近最佳解。
- en: '![PIC](img/file205.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file205.png)'
- en: '**Figure 7.9**: Posterior predictive check to choose *K*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.9**：后验预测检查以选择 *K*'
- en: To complement posterior predictive checks, we can compute the ELPD as approximated
    with the LOO method. This is shown in *Figure [7.10](#x1-144025r10)*. We are comparing
    the same model but with different values of *K*. We can see that *K* = 5 is the
    best solution and *K* = 4 comes close. This is in agreement with the Bayesian
    p-values shown in *Figure [7.9](#x1-144024r9)*.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了补充后验预测检查，我们可以计算使用 LOO 方法近似的 ELPD。这在 *图 [7.10](#x1-144025r10)* 中展示。我们比较的是相同的模型，但
    *K* 值不同。我们可以看到 *K* = 5 是最佳解，而 *K* = 4 也接近最佳解。这与 *图 [7.9](#x1-144024r9)* 中显示的贝叶斯
    p 值一致。
- en: '![PIC](img/file206.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file206.png)'
- en: '**Figure 7.10**: Model selection with LOO to choose *K*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.10**：使用 LOO 进行模型选择以选择 *K*'
- en: 'The chemical shifts example, while simple, shows the main ideas about finite
    mixture models. For this example, we used Gaussians as they provide a good approximation
    to model the data. However, we are free to use non-Gaussian components if needed.
    For example, we could use a:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 化学位移的例子虽然简单，但展示了有限混合模型的主要思想。在这个例子中，我们使用了高斯分布，因为它们提供了一个良好的数据拟合近似。然而，如果需要，我们也可以使用非高斯分量。例如，我们可以使用：
- en: '**Poisson mixture model**: Suppose you are monitoring the number of customers
    entering a store every hour. A Poisson mixture model can help to identify different
    patterns of customer traffic, such as peak hours or days, by assuming that the
    data follows a mixture of Poisson distributions.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泊松混合模型**：假设你在监控每小时进入商店的顾客数量。泊松混合模型可以帮助识别顾客流量的不同模式，例如高峰时段或高峰日，通过假设数据遵循泊松分布的混合。'
- en: '**Exponential mixture model**: Imagine you are studying the lifetimes of a
    certain type of light bulb. An Exponential mixture model can assist in identifying
    different groups of light bulbs with varying lifetimes, suggesting potential differences
    in manufacturing quality or environmental factors.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指数混合模型**：假设你在研究某种类型的灯泡寿命。指数混合模型可以帮助识别不同寿命的灯泡群体，提示制造质量或环境因素的潜在差异。'
- en: 'In the next section, we will explore a very particular type of mixture model,
    one that involves two processes: one generating zeros and the other generating
    zeros or non-zeros.'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将探索一种非常特殊的混合模型，这种模型涉及两个过程：一个生成零，另一个生成零或非零。
- en: 7.5 Zero-Inflated and hurdle models
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 零膨胀和障碍模型
- en: 'When counting things, like cars on a road, stars in the sky, moles on your
    skin, or virtually anything else, one option is to not count a thing, that is,
    to get zero. The number zero can generally occur for many reasons; we get a zero
    because we were counting red cars and a red car did not go down the street or
    because we missed it. If we use a Poisson or NegativeBinomial distribution to
    model such data, we will notice that the model generates fewer zeros compared
    to the data. How do we fix that? We may try to address the exact cause of our
    model predicting fewer zeros than the observed and include that factor in the
    model. But, as is often the case, it may be enough, and simpler, to assume that
    we have a mixture of two processes:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在计数事物时，比如道路上的汽车、天空中的星星、皮肤上的痣，或几乎任何其他事物，一个选项是不计数某个事物，也就是说得到零。零这个数字通常会由于很多原因出现；比如我们在数红色的车，但是一辆红色的车没有经过街道，或者我们错过了它。如果我们使用泊松分布或负二项分布来建模这种数据，我们会注意到模型生成的零比实际数据少。我们该如何解决这个问题呢？我们可以尝试解决模型预测零比实际观察到的少的具体原因，并将这一因素纳入模型中。但通常情况下，假设我们有两个过程的混合模型可能就足够了，而且更简单：
- en: One modeled by a discrete distribution with probability ![](img/Phi_01.png)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个过程由一个离散分布建模，概率为 ![](img/Phi_01.png)
- en: One giving extra zeros with probability 1 − ![](img/Phi_01.png)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个过程以概率 1 − ![](img/Phi_01.png) 生成额外的零
- en: In some texts, you will find that ![](img/Phi_01.png) represents the extra zeros
    instead of 1 − ![](img/Phi_01.png). This is not a big deal; just pay attention
    to which is which for a concrete example.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些文本中，你会发现 ![](img/Phi_01.png) 代表额外的零，而不是 1 − ![](img/Phi_01.png)。这不是什么大问题；只需注意哪个是哪个，尤其是具体示例时。
- en: 'The family of distributions allowing for ”extra” zeros is known as a Zero-Inflated
    distribution. The most common members of that family are:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 允许生成“额外”零的分布族被称为零膨胀分布。该家族中最常见的成员包括：
- en: Zero-Inflated Poisson
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零膨胀泊松分布
- en: Zero-Inflated NegativeBinomial
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零膨胀负二项分布
- en: Zero-Inflated Binomial
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零膨胀二项分布
- en: In the next section, we are going to use Zero-Inflated Poisson to solve a regression
    problem. Once you understand how to work with this distribution, working with
    the Zero-Inflated NegativeBinomial or Zero-Inflated Binomial will become very
    easy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将使用零膨胀泊松分布来解决回归问题。一旦你了解如何使用这个分布，使用零膨胀负二项分布或零膨胀二项分布将变得非常简单。
- en: 7.5.1 Zero-Inflated Poisson regression
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 零膨胀泊松回归
- en: 'To exemplify a Zero-Inflated Poisson regression model, we are going to work
    with a dataset taken from the Institute for Digital Research and Education ( [http://www.ats.ucla.edu/stat/data](http://www.ats.ucla.edu/stat/data)).
    We have 250 groups of visitors to a park. Here are some parts of the data per
    group: the number of fish they caught (`count`), how many children were in the
    group (`child`), and whether they brought a camper to the park (`camper`). Using
    this data, we are going to build a model that predicts the number of caught fish
    as a function of the child and camper variables.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 为了举例说明零膨胀泊松回归模型，我们将使用来自数字研究与教育研究所的数据集（[http://www.ats.ucla.edu/stat/data](http://www.ats.ucla.edu/stat/data)）。我们有250组游客数据，数据包括：每组捕捉到的鱼的数量（`count`）、每组中的儿童数量（`child`）以及他们是否带了露营车到公园（`camper`）。使用这些数据，我们将建立一个模型，通过儿童和露营车变量来预测捕捉到的鱼的数量。
- en: 'Using PyMC we can write a model for this data like:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PyMC，我们可以为这个数据编写一个模型，如下所示：
- en: '**Code 7.5**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.5**'
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '`camper` is a binary variable with 0 for not-camper and 1 for camper. A variable
    indicating the absence/presence of an attribute is usually denoted as a dummy
    variable or indicator variable. Note that when `camper` takes the value of 0,
    the term involving *β*[1] will also be 0 and the model reduces to a regression
    with a single independent variable. We already discussed this in *Chapter [6](CH06.xhtml#x1-1200006)*
    when talking about Categorical predictors.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`camper` 是一个二元变量，值为 0 表示不是露营车，1 表示是露营车。表示某个属性是否存在的变量通常被称为虚拟变量或指示变量。请注意，当 `camper`
    的值为 0 时，涉及 *β*[1] 的项也会变为 0，模型就会简化为一个只有一个自变量的回归模型。我们在*第六章 [6](CH06.xhtml#x1-1200006)*
    中讨论了分类预测变量时已经提到过这一点。'
- en: 'The results are shown in *Figure [7.11](#x1-146011r11)*. We can see that the
    higher the number of children, the lower the number of fish caught. Also, people
    who travel with a camper generally catch more fish. If you check the coefficients
    for `child` and `camper`, you will see that we can say the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如*图 [7.11](#x1-146011r11)*所示。我们可以看到，孩子数量越多，捕获的鱼的数量越少。而且，带着露营车旅行的人通常会捕到更多的鱼。如果你检查
    `child` 和 `camper` 的系数，你会发现我们可以得出以下结论：
- en: For each additional child, the expected count of the fish caught decreases by
    ≈ 0*.*4
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每增加一个孩子，捕获的鱼的期望数量会减少约 ≈ 0*.*4
- en: Camping with a camper increases the expected count of the fish caught by ≈ 2
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用露营车露营会使捕获的鱼的期望数量增加约 2
- en: '![PIC](img/file207.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file207.png)'
- en: '**Figure 7.11**: Fish caught as a function of the number of children and use
    of a camper'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.11**：捕获的鱼数作为孩子数量和是否使用露营车的函数'
- en: Zero-Inflated models are closely associated with hurdle models, making it beneficial
    to learn about hurdle models while the concept of Zero-Inflated models is still
    fresh in our minds.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 零膨胀模型与障碍模型密切相关，因此在零膨胀模型的概念仍然清晰的情况下学习障碍模型是非常有益的。
- en: 7.5.2 Hurdle models
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 障碍模型
- en: In hurdle models, the Bernoulli probability determines whether a count variable
    has a value of zero or something greater. If it’s greater than zero, we consider
    the *hurdle* crossed, and the distribution of these positive values is determined
    using a distribution truncated at zero.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在障碍模型中，伯努利概率决定一个计数变量是否为零或大于零。如果大于零，我们认为已跨越 *障碍*，这些正值的分布是通过截断在零的分布来确定的。
- en: In terms of mixture models, we can think of a Zero-Inflated model as a mixture
    of zeros and something else, which can be zero or non-zero. Instead, a hurdle
    model is a mixture of zeros and non-zeros. As a consequence, a Zero-Inflated model
    can only increase the probability of *P*(*x* = 0), but for hurdle models, the
    probability can be smaller or larger.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在混合模型的框架下，我们可以将零膨胀模型视为零和其他某些值（可能是零或非零）的混合。而障碍模型则是零和非零的混合。因此，零膨胀模型只能增加 *P*(*x*
    = 0) 的概率，而对于障碍模型，概率可以变得更小或更大。
- en: 'The available distributions in PyMC and Bambi for hurdle models are:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyMC 和 Bambi 中可用于障碍模型的分布包括：
- en: Hurdle Poisson
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 障碍 Poisson 分布
- en: Hurdle NegativeBinomial
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 障碍负二项分布
- en: Hurdle Gamma
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 障碍伽玛分布
- en: Hurdle LogNormal
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 障碍对数正态分布
- en: To illustrate hurdle models, we are going to use the horseshoe crab dataset
    [[Brockmann](Bibliography.xhtml#XBrockmann_1996), [1996](Bibliography.xhtml#XBrockmann_1996)].
    Horseshoe crabs arrive at the beach in pairs for their spawning ritual. Additionally,
    solitary males also make their way to the shoreline, gathering around the nesting
    couples and vying for the opportunity to fertilize the eggs. These individuals,
    known as satellite males, often congregate in sizable groups near certain nesting
    pairs while disregarding others. We want to model the number of male `satellites`.
    We suspect this number is related to the properties of the female crabs. As predictors,
    we are going to use the carapace `width` and `color`. The carapace is the hard
    upper shell of crabs. The color is encoded using the integers 1 to 4, from lighter
    to darker tones.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明障碍模型，我们将使用螯虾数据集 [[Brockmann](Bibliography.xhtml#XBrockmann_1996), [1996](Bibliography.xhtml#XBrockmann_1996)]。螯虾成对地来到海滩进行产卵仪式。此外，孤独的雄性也会来到海岸，聚集在已成对的雌雄螯虾周围，争夺受精卵的机会。这些被称为卫星雄性个体，通常会在特定的巢对附近聚集，忽略其他巢对。我们想要建立模型来预测雄性
    `卫星` 的数量。我们怀疑这个数量与雌性螯虾的特征有关。作为预测变量，我们将使用螯甲的 `宽度` 和 `颜色`。螯甲是螯虾的坚硬上壳。颜色则使用从 1 到
    4 的整数编码，从浅色到深色。
- en: 'We are going to use Bambi to encode and fit four models. The main difference
    between the four models is that we are going to use four different likelihoods,
    or families, namely Poisson, Hurdle Poisson, NegativeBinomial, and Hurdle NegativeBinomial.
    The models are shown in the next code block:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Bambi 来编码并拟合四个模型。这四个模型的主要区别在于我们将使用四种不同的似然函数或分布族，分别是 Poisson 分布、障碍 Poisson
    分布、负二项分布和障碍负二项分布。模型展示在下一个代码块中：
- en: '**Code 7.6**'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.6**'
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Notice that we have encoded color as `C(color)` to indicate to Bambi that it
    should treat it as a Categorical variable, and not a numeric one.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们已经将颜色编码为 `C(color)`，以向 Bambi 指示应将其视为分类变量，而不是数值型变量。
- en: '*Figure [7.12](#x1-147012r12)* shows a posterior predictive check for the four
    models we fitted to the horseshoe data. The gray bars represent the frequency
    of the observed data. The dots are the expected values, according to the model.
    The dashed line is just a visual aid. We can see that the NegativeBinomial is
    an improvement on the Poisson and the hurdle model is an improvement on the non-inflated
    models.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [7.12](#x1-147012r12)* 显示了我们为马蹄铁数据拟合的四个模型的后验预测检查。灰色条形图代表观察数据的频率。点是根据模型计算的预期值。虚线只是一个视觉辅助工具。我们可以看到，NegativeBinomial
    比 Poisson 模型有了改善，而 Hurdle 模型比非膨胀模型有所提升。'
- en: '![PIC](img/file208.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file208.png)'
- en: '**Figure 7.12**: Posterior predictive check for 4 models for the horseshoe
    data'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.12**：四个模型在马蹄铁数据上的后验预测检查'
- en: '*Figure [7.13](#x1-147013r13)* shows a model comparison in terms of the ELPD
    as computed with LOO. The Hurdle NegativeBinomial is the best model and the Poisson
    one is the worst.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [7.13](#x1-147013r13)* 显示了通过 LOO 计算的 ELPD 模型比较。Hurdle NegativeBinomial 是最佳模型，而
    Poisson 模型是最差的。'
- en: '![PIC](img/file209.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file209.png)'
- en: '**Figure 7.13**: Model comparison using LOO for 4 models for the horseshoe
    data'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.13**：使用 LOO 进行的四个模型在马蹄铁数据上的模型比较'
- en: '*Figure [7.12](#x1-147012r12)* is nice, but there is an alternative representation
    called hanging rootograms [[Kleiber and Zeileis](Bibliography.xhtml#Xkleiber_2016), [2016](Bibliography.xhtml#Xkleiber_2016)],
    which is particularly useful for diagnosing and treating issues such as overdispersion
    and/or excess zeros in count data models. See *Figure [7.14](#x1-147014r14)* for
    an example of the horseshoe data and our four models.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [7.12](#x1-147012r12)* 很好，但还有一种替代的表示方式，叫做悬挂根图 [[Kleiber and Zeileis](Bibliography.xhtml#Xkleiber_2016),
    [2016](Bibliography.xhtml#Xkleiber_2016)]，它在诊断和处理计数数据模型中的问题（如过度离散和/或零值过多）时特别有用。请参见*图
    [7.14](#x1-147014r14)*，这是针对马蹄铁数据和我们的四个模型的一个示例。'
- en: '![PIC](img/file210.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file210.png)'
- en: '**Figure 7.14**: Posterior predictive check, with rootograms, for 4 models
    for the horseshoe data'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.14**：四个模型在马蹄铁数据上的后验预测检查，附带根图'
- en: In hanging rootograms, we plot the square roots of the observed and predicted
    values. This is a quick way of approximately adjusting for the scale differences
    across the different counts. In other words, it makes it easier to compare observed
    and expected frequencies even for low frequencies. Second, the bars for the observed
    data are *hanging* from the expected values, instead of *growing* from zero, as
    in *Figure [7.12](#x1-147012r12)*. Because the bars are hanging, if a bar doesn’t
    reach zero (dashed gray line), then the model is overpredicting a count, and if
    the bar goes below zero, then the model is underpredicting that count.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在悬挂的根图中，我们绘制了观察值和预测值的平方根。这是一种快速的方式，用于大致调整不同计数之间的尺度差异。换句话说，它使得即使是低频率的观察值和预期频率也能更容易进行比较。其次，观察数据的条形图是从预期值“悬挂”下来的，而不是像在*图
    [7.12](#x1-147012r12)* 中那样从零开始“生长”。由于条形图是悬挂的，如果条形图没有达到零（虚线灰线），则表示模型对该计数的预测过高；如果条形图低于零，则表示模型对该计数的预测过低。
- en: 'Let’s summarize each of the subplots from *Figure [7.12](#x1-147012r12)*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下*图 [7.12](#x1-147012r12)* 中每个子图的内容：
- en: 'Poisson: The zeros are underpredicted, and counts 1 to 4 are overpredicted.
    Most counts from 6 onward are also underpredicted. This pattern is an indication
    of overdispersion in the data, and the huge difference for 0 indicates an excess
    of zeros.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Poisson：零值被低估，1 到 4 的计数被高估。6 及以后的大部分计数也被低估。这个模式表明数据中存在过度离散，而 0 的巨大差异表明零值过多。
- en: 'NegativeBinomial: We see that overdispersion is much better handled compared
    to the Poisson model. We still see that the zeros are underpredicted and counts
    1 and 2 are overpredicted, probably indicating an excess of zeros.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NegativeBinomial：我们可以看到，与 Poisson 模型相比，过度离散得到了更好的处理。我们仍然看到零值被低估，计数 1 和 2 被高估，这可能表明零值过多。
- en: 'Hurdle Poisson: As expected for a hurdle model, we get a perfect fit for the
    zeros. For the positive values, we still get some deviations.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hurdle Poisson：正如预期的那样，对于一个 Hurdle 模型，我们对零值的拟合非常完美。对于正值，我们仍然会看到一些偏差。
- en: 'Hurdle NegativeBinomial: We see that the model can fit the data very well,
    with the deviations being very small for most of the counts.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hurdle NegativeBinomial：我们可以看到，该模型能够很好地拟合数据，对于大多数计数，偏差非常小。
- en: 7.6 Mixture models and clustering
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 混合模型和聚类
- en: Clustering or cluster analysis is the data analysis task of grouping objects
    in such a way that objects in a given group are closer to each other than to those
    in the other groups. The groups are called clusters and the degree of closeness
    can be computed in many different ways, for example, by using metrics, such as
    the Euclidean distance. If instead we take the probabilistic route, then a mixture
    model arises as a natural candidate to solve clustering tasks.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类或聚类分析是将对象分组的任务，目的是使得同一组中的对象彼此之间比与其他组的对象更接近。这些组被称为簇，接近程度可以通过多种不同的方式计算，例如使用度量，如欧几里得距离。如果我们选择概率方法，那么混合模型作为解决聚类任务的自然候选者出现。
- en: Performing clustering using probabilistic models is usually known as model-based
    clustering. Using a probabilistic model allows us to compute the probability of
    each data point belonging to each one of the clusters. This is known as soft clustering
    instead of hard clustering, where each data point belongs to a cluster with a
    probability of 0 or 1\. We can turn soft clustering into hard clustering by introducing
    some rule or boundary. In fact, you may remember that this is exactly what we
    do to turn logistic regression into a classification method, where we use as the
    default boundary the value of 0.5\. For clustering, a reasonable choice is to
    assign a data point to the cluster with the highest probability.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用概率模型进行聚类通常被称为基于模型的聚类。使用概率模型可以计算每个数据点属于每个簇的概率。这被称为软聚类，而不是硬聚类，在硬聚类中，每个数据点要么属于一个簇，概率为0或1。我们可以通过引入一些规则或边界将软聚类转化为硬聚类。事实上，你可能还记得，这正是我们将逻辑回归转化为分类方法时所做的操作，其中我们使用0.5作为默认边界值。对于聚类，合理的选择是将数据点分配给概率最高的簇。
- en: In summary, when people talk about clustering, they are generally talking about
    grouping objects, and when people talk about mixture models, they are talking
    about using a mix of simple distributions to model a more complex distribution,
    either to identify subgroups or just to have a more flexible model to describe
    the data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当人们谈论聚类时，通常是在谈论将对象分组，而当人们谈论混合模型时，他们是在谈论使用简单分布的混合来建模更复杂的分布，目的是识别子群体或仅仅为了拥有一个更灵活的模型来描述数据。
- en: 7.7 Non-finite mixture model
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 非有限混合模型
- en: For some problems, such as trying to cluster handwritten digits, it is easy
    to justify the number of groups we expect to find in the data. For other problems,
    we can have good guesses; for example, we may know that our sample of Iris flowers
    was taken from a region where only three species of Iris grow, thus using three
    components is a reasonable starting point. When we are not that sure about the
    number of components, we can use model selection to help us choose the number
    of groups. Nevertheless, for other problems, choosing the number of groups a priori
    can be a shortcoming, or we may instead be interested in estimating this number
    directly from the data. A Bayesian solution for this type of problem is related
    to the Dirichlet process.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些问题，例如尝试对手写数字进行聚类，容易确定我们期望在数据中找到的组数。对于其他问题，我们可以有很好的猜测；例如，我们可能知道我们的鸢尾花样本来自一个只有三种鸢尾花生长的区域，因此使用三个成分是一个合理的起点。当我们不确定成分的数量时，我们可以使用模型选择来帮助我们选择组数。然而，对于其他问题，事先选择组数可能是一个缺点，或者我们可能更感兴趣的是直接从数据中估计这个数量。对于这种类型的问题，贝叶斯解决方案与狄利克雷过程相关。
- en: 7.7.1 Dirichlet process
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7.1 狄利克雷过程
- en: 'All the models that we have seen so far have been parametric models, meaning
    models with a fixed number of parameters that we are interested in estimating,
    like a fixed number of clusters. We can also have non-parametric models. A better
    name for these models would probably be non-fixed-parametric models or models
    with a variable number of parameters, but someone already decided the name for
    us. We can think of non-parametric models as models with a theoretically infinite
    number of parameters. In practice, we somehow let the data reduce the theoretically
    infinite number of parameters to some finite number. As the data *decides* on
    the actual number of parameters, non-parametric models are very flexible and potentially
    robust against underfitting and overfitting. In this book, we are going to see
    three examples of such models: the Gaussian process (GP), Bayesian Additive Regression
    Trees (BART), and the Dirichlet process (DPs). While the upcoming chapters will
    focus on GPs and BART individually, our immediate attention will be directed toward
    exploring DPs.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止我们所看到的所有模型都是参数模型，意味着它们具有固定数量的参数，我们感兴趣的是估计这些参数，如固定数量的聚类。我们也可以有非参数模型。这些模型的更好名称可能是非固定参数模型或具有可变数量参数的模型，但已经有人为我们决定了名称。我们可以将非参数模型看作是具有理论上无限数量参数的模型。在实践中，我们在某种程度上让数据将理论上无限的参数数量减少到有限的数量。由于数据*决定*了实际的参数数量，非参数模型非常灵活，并且有可能对欠拟合和过拟合具有鲁棒性。在本书中，我们将看到三种此类模型的例子：高斯过程（GP）、贝叶斯加法回归树（BART）和
    Dirichlet 过程（DP）。虽然接下来的章节将分别集中讨论 GP 和 BART，但我们当前的重点将是探索 DPs。
- en: As the Dirichlet distribution is the n-dimensional generalization of the beta
    distribution, the Dirichlet process is the infinite-dimensional generalization
    of the Dirichlet distribution. I know this can be puzzling at first, so let’s
    take the time to re-read the previous sentence before continuing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Dirichlet 分布是 Beta 分布的 n 维推广，Dirichlet 过程是 Dirichlet 分布的无限维推广。我知道这一开始可能让人感到困惑，所以在继续之前，花点时间重新阅读上一句。
- en: The Dirichlet distribution is a probability distribution on the space of probabilities,
    while the DP is a probability distribution on the space of distributions. This
    means that a single draw from a DP is actually a distribution. For finite mixture
    models, we used the Dirichlet distribution to assign a prior for the fixed number
    of clusters or groups. A DP is a way to assign a prior distribution to a non-fixed
    number of clusters. We can think of a DP as a way to sample from a prior distribution
    of distributions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Dirichlet 分布是在概率空间上的一种概率分布，而 DP 是在分布空间上的概率分布。这意味着从 DP 中单次抽样实际上是一个分布。对于有限混合模型，我们使用
    Dirichlet 分布来为固定数量的聚类或组分配先验分布。DP 是一种为非固定数量的聚类分配先验分布的方法。我们可以将 DP 看作是从分布的先验分布中进行抽样的一种方法。
- en: 'Before we move on to the actual non-parametric mixture model, let us take a
    moment to discuss some of the details of the DP. The formal definition of a DP
    is somewhat obscure unless you know probability theory very well, so instead let
    me describe some of the properties of a DP that are relevant to understanding
    its role in non-finite mixture models:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进入实际的非参数混合模型之前，让我们花点时间讨论一些 DP 的细节。DP 的正式定义相对晦涩，除非你非常了解概率论，否则不容易理解，因此让我描述一些
    DP 的属性，这些属性对于理解其在非有限混合模型中的作用是非常重要的：
- en: A DP is a distribution whose realizations are probability distributions. For
    instance, from a Gaussian distribution, you sample numbers, while from a DP, you
    sample distributions.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP 是一种其实现为概率分布的分布。例如，从高斯分布中，你会抽样数值，而从 DP 中，你会抽样分布。
- en: A DP is specified by a base distribution ![](img/H.PNG) and *α*, a positive
    real number, called the concentration parameter. *α* is analog to the concentration
    parameter in the Dirichlet distribution.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DP 由基础分布 ![](img/H.PNG) 和 *α*（一个正实数，称为浓度参数）来指定。*α* 类似于 Dirichlet 分布中的浓度参数。
- en: '![](img/H.PNG) is the expected value of the DP. This means that a DP will generate
    distributions around the base distribution. This is somehow equivalent to the
    mean of a Gaussian distribution.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/H.PNG) 是 DP 的期望值。这意味着 DP 会围绕基础分布生成分布。这在某种程度上等同于高斯分布的均值。'
- en: As *α* increases, the realizations become less and less concentrated.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着 *α* 的增加，实现在分布上的集中度越来越低。
- en: In practice, a DP always generates discrete distributions.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实践中，DP 总是生成离散分布。
- en: In the limit *α* →∞, the realizations from a DP will be equal to the base distribution,
    thus if the base distribution is continuous, the DP will generate a continuous
    distribution. For this reason, mathematicians say that the distributions generated
    from a DP are almost surely discrete. In practice, *alpha* is a finite number,
    thus we always work with discrete distributions.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在极限情况下，*α* → ∞，DP 的实现将等于基础分布，因此如果基础分布是连续的，DP 将生成一个连续分布。因为这个原因，数学家们说从 DP 生成的分布几乎总是离散的。实际上，*alpha*
    是一个有限数值，因此我们总是使用离散分布。
- en: Priors Over Distributions
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 分布的先验
- en: We can think of a DP as the prior on a random distribution *f*, where the base
    distribution ![](img/H_gray.PNG) is what we expected *f* to be and the concentration
    parameter *α* represents how confident we are about our prior guess.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 DP 看作是随机分布 *f* 的先验，其中基础分布 ![](img/H_gray.PNG) 是我们预期 *f* 应该是什么，而浓度参数 *α*
    表示我们对先验猜测的信心程度。
- en: To make these properties more concrete, let us take a look again at the Categorical
    distribution in *Figure [7.3](#x1-140003r3)*. We can completely specify this distribution
    by indicating the position on the x-axis and the height on the y-axis. For the
    Categorical distribution, the positions on the x-axis are restricted to be integers
    and the sum of the heights has to be 1\. Let’s keep the last restriction but relax
    the former one. To generate the positions on the x-axis, we are going to sample
    from a base distribution ![](img/H.PNG). In principle, it can be any distribution
    we want; thus if we choose a Gaussian, the locations would be any value from the
    real line. Instead, if we choose a Beta, the locations will be restricted to the
    interval [0, 1], and if we choose a Poisson as the base distribution, the locations
    will be restricted to be non-negative integers 0, 1, 2, ....
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些特性更具体，我们再来看一下 *图 [7.3](#x1-140003r3)* 中的分类分布。我们可以通过指明 x 轴上的位置和 y 轴上的高度来完全指定这个分布。对于分类分布，x
    轴上的位置被限制为整数，并且高度之和必须等于 1。我们保持最后一个限制，但放宽前一个限制。为了生成 x 轴上的位置，我们将从基础分布 ![](img/H.PNG)
    中采样。原则上，它可以是我们想要的任何分布；因此，如果我们选择高斯分布，位置将是实数线上的任意值。相反，如果我们选择 Beta 分布，位置将限制在区间 [0,
    1] 内，如果我们选择泊松分布作为基础分布，位置将限制为非负整数 0, 1, 2, ….
- en: So far so good, but how do we choose the values on the y-axis? We follow a *Gedanken
    experiment* known as the stick-breaking process. Imagine we have a stick of length
    1, then we break it into two parts (not necessarily equal). We set one part aside
    and break the other part into two, and then we just keep doing this forever and
    ever. In practice, as we cannot really repeat the process infinitely, we truncate
    it at some predefined value *K*, but the general idea holds, at least in practice.
    To control the stick-breaking process, we use a parameter *α*. As we increase
    the value of *α*, we will break the stick into smaller and smaller portions. Thus,
    for *α* = 0, we don’t break the stick, and for *α* = ∞, we break it into infinite
    pieces. *Figure [7.15](#x1-150003r15)* shows four draws from a DP, for four different
    values of *α*.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止都很好，但我们如何选择 y 轴上的值呢？我们遵循一个被称为“想象实验”（*Gedanken experiment*）的过程，称为“断棒过程”。假设我们有一根长度为
    1 的棒子，然后我们把它分成两部分（不一定相等）。我们把一部分放一边，接着把另一部分再分成两部分，然后我们就这样不断地做下去，永远做下去。实际上，由于我们不能真正无限地重复这个过程，我们在某个预定义的值
    *K* 处截断它，但这个基本思路至少在实践中是成立的。为了控制断棒过程，我们使用一个参数 *α*。当我们增大 *α* 的值时，我们会把棒子分成越来越小的部分。因此，当
    *α* = 0 时，我们不分棒子，而当 *α* = ∞ 时，我们会把它分成无限多的部分。*图 [7.15](#x1-150003r15)* 展示了从 DP
    中抽取的四个样本，分别对应四个不同的 *α* 值。
- en: '![PIC](img/file211.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/file211.png)'
- en: '**Figure 7.15**: Stick-breaking process with a Gaussian as the base distribution'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.15**：以高斯分布为基础的断棒过程'
- en: We can see from *Figure [7.15](#x1-150003r15)* that the DP is a discrete distribution.
    When *α* increases, we obtain smaller pieces from the initial unit-length stick;
    notice the change on the scale of the y-axis. The base distribution, a Normal(0,
    1) in this figure, controls the locations. With increasing *α*, the sticks progressively
    resemble the base distribution more. In the accompanying notebook for this chapter,
    you will find the code to generate *Figure [7.15](#x1-150003r15)*. I highly recommend
    you play with this code to gain a better intuition of DPs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 从*图 [7.15](#x1-150003r15)*中我们可以看到，DP是一个离散分布。当*α*增大时，我们从初始单位长度的棒中获得较小的块；注意y轴刻度的变化。基础分布（在此图中为Normal(0,
    1)）控制位置。随着*α*的增加，棒逐渐更多地呈现出基础分布的特征。在本章的配套笔记本中，你将找到生成*图 [7.15](#x1-150003r15)*的代码。我强烈建议你玩一下这段代码，以便更好地理解DP的直觉。
- en: '*Figure [7.1](#x1-138002r1)* shows that if you place a Gaussian on top of each
    data point and then sum all the Gaussians, you can approximate the distribution
    of the data. We can use a DP to do something similar, but instead of placing a
    Gaussian on top of each data point, we can place a Gaussian at the location of
    each piece of the original unit-length stick. We then weigh each Gaussian by the
    length of each piece. This procedure provides a general recipe for a non-finite
    Gaussian-mixture model.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [7.1](#x1-138002r1)* 显示了，如果你在每个数据点上放置一个高斯分布，然后将所有高斯分布相加，你可以近似数据的分布。我们可以使用DP做类似的事情，但不是在每个数据点上放置一个高斯分布，而是在每个原始单位长度的棒的的位置上放置一个高斯分布。然后我们通过每块棒的长度来加权每个高斯分布。这个过程为非有限高斯混合模型提供了一般的方法。'
- en: 'Alternatively, we can replace the Gaussian for any other distribution and we
    will have a general recipe for a non-finite mixture model. *Figure [7.16](#x1-150004r16)*
    shows an example of such a model. I used a mixture of Laplace distributions, just
    to reinforce the idea that you are by no means restricted to just using Gaussian
    mixture models:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以用任何其他分布替代高斯分布，这样我们就得到了一个非有限混合模型的一般方法。*图 [7.16](#x1-150004r16)* 展示了这种模型的一个示例。我使用了拉普拉斯分布的混合模型，这只是为了强调你绝不局限于仅使用高斯混合模型：
- en: '![PIC](img/file212.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file212.png)'
- en: '**Figure 7.16**: Laplace mixture model using a DP'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.16**：使用DP的拉普拉斯混合模型'
- en: 'Now we are more than ready to try to implement a DP in PyMC. Let’s first define
    a `stick_breaking` function that works with PyMC:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好尝试在PyMC中实现DP。让我们首先定义一个与PyMC兼容的`stick_breaking`函数：
- en: '**Code 7.7**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.7**'
- en: '[PRE6]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Instead of fixing the value of *α*, the concentration parameter, we are going
    to define a prior for it. A common choice for this is a Gamma distribution, as
    shown in the following code block:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是固定*α*（浓度参数）的值，而是为其定义一个先验。一个常见的选择是Gamma分布，如下代码块所示：
- en: '**Code 7.8**'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**代码 7.8**'
- en: '[PRE7]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because we are approximating the infinite DP with a truncated stick-breaking
    procedure, it is important to check that the truncation value (*K* = 10 in this
    example) is not introducing any bias. A simple way to do this is to compute the
    average weight of each component, sort them, and then plot their cumulative sum.
    To be on the safe side, we should have at least a few components with negligible
    weight; otherwise, we must increase the truncation value. An example of this type
    of plot is *Figure [7.17](#x1-150027r17)*.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们是通过截断的stick-breaking过程来近似无限DP，因此重要的是检查截断值（在此例中为*K* = 10）是否引入了任何偏差。一种简单的方法是计算每个组件的平均权重，对其进行排序，然后绘制其累积和。为了保险起见，我们应该确保至少有一些组件的权重可以忽略不计；否则，我们必须增加截断值。此类图的示例见*图
    [7.17](#x1-150027r17)*。
- en: '![PIC](img/file213.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file213.png)'
- en: '**Figure 7.17**: Ordered cumulative distribution of average weights of the
    DP components'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.17**：DP组件平均权重的有序累积分布'
- en: We can see that only the first 7 components are somewhat important. The first
    7 components represent more than 99.9% of the total weight (gray dashed line in
    *Figure [7.17](#x1-150027r17)*) and thus we can be confident that the chosen upper
    value (*K* = 10) is large enough for this data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，只有前7个组件在某种程度上是重要的。前7个组件占总权重的99.9%以上（在*图 [7.17](#x1-150027r17)*中的灰色虚线），因此我们可以确信选择的上限值（*K*
    = 10）对于这组数据来说足够大。
- en: '*Figure [7.18](#x1-150028r18)* shows the mean density estimated using the DP
    model (black line) together with samples from the posterior (gray lines) to reflect
    the uncertainty in the estimation.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 [7.18](#x1-150028r18)* 显示了使用DP模型估计的平均密度（黑线），以及从后验样本中获得的样本（灰线），以反映估计的不确定性。'
- en: '![PIC](img/file214.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file214.png)'
- en: '**Figure 7.18**: DP mixture model for the chemical shifts data'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '**图 7.18**：用于化学位移数据的DP混合模型'
- en: 7.8 Continuous mixtures
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 连续混合
- en: The focus of this chapter was on discrete mixture models, but we can also have
    continuous mixture models. And indeed we already know some of them. For instance,
    hierarchical models can also be interpreted as continuous mixture models where
    the parameters in each group come from a continuous distribution in the upper
    level. To make it more concrete, think about performing linear regression for
    several groups. We can assume that each group has its own slope or that all the
    groups share the same slope. Alternatively, instead of framing our problem as
    two extreme discrete options, a hierarchical model allows us to effectively model
    a continuous mixture of these two options.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点是离散混合模型，但我们也可以有连续混合模型。事实上，我们已经知道其中一些。例如，层次模型也可以解释为连续混合模型，其中每个组的参数来自上层的连续分布。为了更具体一点，想象一下对多个组进行线性回归。我们可以假设每个组都有自己的斜率，或者所有组共享相同的斜率。或者，与其将问题框架设置为两个极端的离散选项，层次模型让我们能够有效地建模这两种选项的连续混合。
- en: 7.8.1 Some common distributions are mixtures
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.8.1 一些常见分布是混合分布
- en: 'The BetaBinomial is a discrete distribution generally used to describe the
    number of successes *y* for *n* Bernoulli trials when the probability of success
    *p* at each trial is unknown and assumed to follow a beta distribution with parameters
    *α* and *β*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: BetaBinomial是一种离散分布，通常用于描述在每次试验成功的概率*p*未知并假设服从参数*α*和*β*的Beta分布的情况下，进行*n*次伯努利试验时成功的次数*y*：
- en: '![ ∫ 1 BetaBinomial(y | n,𝛼,𝛽 ) = Bin(y | p,n ) Beta(p | 𝛼,𝛽)dp 0 ](img/file215.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![ ∫ 1 BetaBinomial(y | n,𝛼,𝛽 ) = Bin(y | p,n ) Beta(p | 𝛼,𝛽)dp 0 ](img/file215.jpg)'
- en: That is, to find the probability of observing the outcome *y*, we average over
    all the possible (and continuous) values of *p*. Thus, the BetaBinomial can be
    considered as a continuous mixture model. If the BetaBinomial model sounds familiar
    to you, it is because you were paying attention in the first two chapters of the
    book! This is the model we used for the coin-flipping problem, although we explicitly
    used a Beta and Binomial distribution, instead of using the already *mixed* Beta-Binomial
    distribution.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 也就是说，为了找到观察到结果*y*的概率，我们需要对所有可能的（且连续的）*p*值进行平均。因此，BetaBinomial可以被视为一个连续混合模型。如果BetaBinomial模型对你来说听起来很熟悉，那是因为你在书的前两章时已经注意到了！这是我们用于掷硬币问题的模型，尽管我们明确使用了Beta和Binomial分布，而不是使用已经*混合*的Beta-Binomial分布。
- en: In a similar fashion, we have the NegativeBinomial distribution, which can be
    understood as a Gamma-Poisson mixture. That is, a mixture of Poisson distributions
    where the rate parameter is Gamma distributed. The Negative-Binomial distribution
    is often used to circumvent a common problem encountered when dealing with count
    data. This problem is known as over-dispersion. Suppose you are using a Poisson
    distribution to model count data, and then you realize that the variance in your
    data exceeds that of the model; the problem with using a Poisson distribution
    is that mean and variance are described by the same parameter. One way to account
    for over-dispersion is to model the data as a (continuous) mixture of Poisson
    distributions. By considering a mixture of distributions, our model has more flexibility
    and can better accommodate the mean and variance of the data.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们有负二项分布，它可以被理解为伽马-泊松混合模型。也就是说，这是泊松分布的混合，其中速率参数服从伽马分布。负二项分布通常用于解决处理计数数据时常遇到的一个问题。这个问题被称为过度离散。假设你使用泊松分布来建模计数数据，然后你意识到数据中的方差超过了模型的方差；使用泊松分布的一个问题是，均值和方差由同一个参数描述。解决过度离散的一种方法是将数据建模为泊松分布的（连续）混合。通过考虑分布的混合，我们的模型具有更大的灵活性，能够更好地适应数据的均值和方差。
- en: Another example of a mixture of distributions is the Student’s t-distribution.
    We introduced this distribution as a robust alternative to the Gaussian distribution.
    In this case, the t-distribution results from a mixture of Gaussian distributions
    with mean *μ* and unknown variance distributed as an InverseGamma distribution.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分布混合的例子是Student's t分布。我们将该分布介绍为高斯分布的稳健替代。在这种情况下，t分布是由均值*μ*且方差未知的高斯分布的混合所产生的，该方差服从InverseGamma分布。
- en: 7.9 Summary
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.9 总结
- en: Many problems can be described as an overall population composed of distinct
    sub-populations. When we know to which sub-population each observation belongs,
    we can specifically model each sub-population as a separate group. However, many
    times we do not have direct access to this information, thus it may be appropriate
    to model that data using mixture models. We can use mixture models to try to capture
    true sub-populations in the data or as a general statistical trick to model complex
    distributions by combining simpler distributions.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 许多问题可以描述为由不同子群体组成的整体群体。当我们知道每个观察值属于哪个子群体时，就可以将每个子群体特定地建模为一个独立的群体。然而，许多时候我们无法直接获得这些信息，因此使用混合模型来建模这些数据可能是合适的。我们可以利用混合模型来尝试捕捉数据中的真实子群体，或者作为一种通用的统计技巧，通过将简单分布组合来建模复杂的分布。
- en: 'In this chapter, we divided mixture models into three classes: finite mixture
    models, non-finite mixture models, and continuous mixture models. A finite mixture
    model is a finite weighted mixture of two or more distributions, each distribution
    or component representing a subgroup of the data. In principle, the components
    can be virtually anything we may consider useful from simple distributions, such
    as a Gaussian or a Poisson, to more complex objects, such as hierarchical models
    or neural networks. Conceptually, to solve a mixture model, all we need to do
    is to properly assign each data point to one of the components. We can do this
    by introducing a latent variable *z*. We use a Categorical distribution for *z*,
    which is the most general discrete distribution, with a Dirichlet prior, which
    is the n-dimensional generalization of the Beta distribution. Sampling the discrete
    variable *z* can be problematic, thus it may be convenient to marginalize it.
    PyMC includes a normal mixture distribution and a mixture distribution that performs
    this marginalization for us, making it easier to build mixture models with PyMC.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将混合模型分为三类：有限混合模型、非有限混合模型和连续混合模型。有限混合模型是由两个或多个分布的有限加权混合组成，每个分布或组分代表数据的一个子群体。原则上，组分可以是我们认为有用的任何东西，从简单的分布（如高斯分布或泊松分布）到更复杂的对象（如层次模型或神经网络）。从概念上讲，为了解决混合模型，我们所需要做的就是将每个数据点正确地分配给一个组分。我们可以通过引入潜在变量*z*来实现这一点。我们为*z*使用类别分布，这是一种最通用的离散分布，具有狄利克雷先验，它是Beta分布的n维推广。对离散变量*z*进行采样可能会出现问题，因此将其边缘化可能更为方便。PyMC包括一个正态混合分布和一个执行此边缘化的混合分布，这使得使用PyMC构建混合模型变得更加简单。
- en: One common problem we looked at in this chapter when working with mixture models
    is that this model can lead to the label-switching problem, a form of non-identifiability.
    One way to remove non-identifiability is to force the components to be ordered.
    One challenge with finite mixture models is how to decide on the number of components.
    One solution is to perform a model comparison for a set of models around an estimated
    number of components. That estimation should be guided, when possible, by our
    knowledge of the problem at hand. Another option is to try to automatically estimate
    the number of components from the data. For this reason, we introduced the concept
    of the Dirichlet process as an infinite-dimensional version of the Dirichlet distribution
    that we can use to build a non-parametric mixture model.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究混合模型时遇到的一个常见问题是，这种模型可能会导致标签交换问题，这是一种非可识别性问题。解决非可识别性的一种方法是强制将各个组分进行排序。有限混合模型的一个挑战是如何确定组分的数量。一种解决方案是对一组模型进行模型比较，基于估计的组分数量来选择最合适的模型。这一估计应当尽可能地借助我们对当前问题的理解。另一个选择是尝试从数据中自动估计组分的数量。为此，我们引入了狄利克雷过程的概念，作为狄利克雷分布的无限维版本，借此我们可以构建一个非参数的混合模型。
- en: Finally, to close the chapter, we briefly discussed how many models, such as
    the BetaBinomial (the one used for the coin-flipping problem), the NegativeBinomial,
    the Student’s t-distribution, and even hierarchical models, can be interpreted
    as continuous mixture models.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了结束本章，我们简要讨论了许多模型（例如Beta二项分布（用于硬币抛掷问题）、负二项分布、学生t分布，甚至层次模型）如何被解释为连续混合模型。
- en: 7.10 Exercises
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.10 习题
- en: Generate synthetic data from a mixture of 3 Gaussians. Check the accompanying
    Jupyter notebook for this chapter for an example of how to do this. Fit a finite
    Gaussian mixture model with 2, 3, or 4 components.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从3个高斯分布的混合中生成合成数据。请查阅本章附带的Jupyter笔记本，了解如何执行此操作。拟合一个具有2、3或4个组分的有限高斯混合模型。
- en: Use LOO to compare the results from exercise 1.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 LOO 比较练习1的结果。
- en: 'Read and run through the following examples about mixture models from the PyMC
    documentation:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 阅读并运行以下关于混合模型的 PyMC 文档示例：
- en: 'Marginalized Gaussian mixture model: [https://www.pymc.io/projects/examples/en/latest/mixture_models/marginalized_gaussian_mixture_model.html](https://www.pymc.io/projects/examples/en/latest/mixture_models/marginalized_gaussian_mixture_model.html)'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边际高斯混合模型：[https://www.pymc.io/projects/examples/en/latest/mixture_models/marginalized_gaussian_mixture_model.html](https://www.pymc.io/projects/examples/en/latest/mixture_models/marginalized_gaussian_mixture_model.html)
- en: 'Dependent density regression: [https://www.pymc.io/projects/examples/en/latest/mixture_models/dependent_density_regression.html](https://www.pymc.io/projects/examples/en/latest/mixture_models/dependent_density_regression.html)'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关密度回归：[https://www.pymc.io/projects/examples/en/latest/mixture_models/dependent_density_regression.html](https://www.pymc.io/projects/examples/en/latest/mixture_models/dependent_density_regression.html)
- en: Refit `fish_data` using a NegativeBinomial and a Hurdle NegativeBinomial model.
    Use rootograms to compare these two models with the Zero-Inflated Poisson model
    shown in this chapter.
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用负二项分布和障碍负二项分布模型重新拟合 `fish_data`。使用根图比较这两个模型与本章展示的零膨胀泊松模型。
- en: Repeat exercise 1 using a Dirichlet process.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用狄利克雷过程重复练习1。
- en: Assuming for a moment that you do not know the correct species/labels for the
    iris dataset, use a mixture model to cluster the three Iris species, using one
    feature of your choice (like the length of the sepal).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你暂时不知道鸢尾花数据集的正确物种/标签，使用混合模型将三个鸢尾花物种进行聚类，选择一个特征（例如花萼的长度）。
- en: Repeat exercise 6 but this time use two features.
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复练习6，但这次使用两个特征。
- en: Join our community Discord space
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加入我们的社区 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at: [https://packt.link/bayesian](https://packt.link/bayesian)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，结识志同道合的人，与超过5000名成员一起学习，网址：[https://packt.link/bayesian](https://packt.link/bayesian)
- en: '![PIC](img/file1.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![PIC](img/file1.png)'
