- en: Advanced Topics in Supervised Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督式机器学习中的高级主题
- en: 'In this chapter, we''re going to focus on some advanced topics. We''ll cover
    two topics: recommender systems and neural networks. We''ll start with collaborative
    filtering, and then we''ll look at integrating content-based similarities into
    collaborative filtering systems. We''ll get into neural networks and transfer
    learning. Finally, we''ll introduce the math and concept behind each of these,
    before getting into Python code.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于一些高级主题。我们将讨论两个主题：推荐系统和神经网络。我们将从协同过滤开始，然后探讨如何将基于内容的相似性融入协同过滤系统中。接着我们将讲解神经网络和迁移学习。最后，我们将介绍每个主题背后的数学和概念，再深入到
    Python 代码。
- en: 'We will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Recommended systems and an introduction to collaborative filtering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统与协同过滤介绍
- en: Matrix factorization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: Content-based filtering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于内容的过滤
- en: Neural networks and deep learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络和深度学习
- en: Using transfer learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'For this chapter, you will need to install the following software, if you haven''t
    already done so:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章内容，如果你还没有安装以下软件，需进行安装：
- en: Jupyter Notebook
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: Anaconda
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anaconda
- en: Python
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python
- en: The code files for this chapter can be found at [https:/​/​github.​com/​PacktPublishing/
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 [https:/​/​github.​com/​PacktPublishing/](https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python)
    上找到。
- en: Supervised-Machine-Learning-with-Python](https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[Supervised-Machine-Learning-with-Python](https://github.com/PacktPublishing/Supervised-Machine-Learning-with-Python)。'
- en: Recommended systems and an introduction to collaborative filtering
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 推荐系统与协同过滤介绍
- en: In this section, we'll cover collaborative filtering and recommender systems.
    We'll start out by explaining what may constitute a recommender system, how users
    willingly share loads of data about themselves, without knowing it, and then we'll
    cover collaborative filtering.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将讨论协同过滤和推荐系统。我们将从解释什么构成推荐系统开始，探讨用户是如何在不知情的情况下自愿分享大量关于自己的数据，接着讲解协同过滤。
- en: Whether you realize it or not, you interact with numerous recommender systems
    on a daily basis. If you've ever purchased from Amazon, or browsed on Facebook,
    or watched a show on Netflix, you've been served some form of personalized content.
    This is how e-commerce platforms maximize conversion rates and keep you coming
    back for more.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你是否意识到，你每天都在与大量的推荐系统互动。如果你曾经在亚马逊上购物，或者在 Facebook 上浏览，或者在 Netflix 上观看节目，你就已经接触到某种形式的个性化内容。这就是电子商务平台如何最大化转化率并让你不断回头购买的方式。
- en: 'One of the marks of a really good recommender system is that it knows what
    you want whether you already know it or not. A good one will make you really wonder:
    how did they know that? So, it turns out that humans are extraordinarily predictable
    in their behavior, even without having to share information about themselves,
    and we call that voting with our feet, meaning that a user may profess to enjoy
    one genre of movie, say comedy, but disproportionately consume another, say romance.
    So, the goal of a recommender system is simply to get you to bite; However, the
    secondary goal generally differs based on the platform itself. It could be to
    maximize revenue for the seller, create satisfaction for the customer, or any
    number of other metrics. But what really makes these so interesting is that they''re
    directly consumable by human beings, whereas so many other **machine learning**
    (**ML**) models exist to replace an automated process.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一个真正优秀的推荐系统的标志是它知道你想要什么，不论你是否已经意识到这一点。一个好的推荐系统会让你真的想：他们是怎么知道的？事实证明，人类的行为是非常可预测的，即使不需要分享个人信息，我们也能通过“脚投票”来表达偏好，意思是用户可能声称喜欢某种电影类型，比如喜剧，但实际却偏好观看另一种类型，比如爱情片。所以，推荐系统的目标仅仅是让你点击；然而，第二个目标通常根据平台的不同而有所不同。它可能是为了最大化卖家的收入，创造客户满意度，或任何其他的度量标准。但真正让这些系统有趣的是，它们是直接供人类消费的，而许多其他**机器学习**（**ML**）模型则存在于替代自动化过程。
- en: 'Here''s an example, explaining voting with your feet:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个示例，解释“脚投票”：
- en: '![](img/c749c0af-0f08-44c1-b582-95fa2b95603b.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c749c0af-0f08-44c1-b582-95fa2b95603b.png)'
- en: This user says he likes football, hot wings, and water skiing. And yet his ratings
    history shows that he's thumbed up one wing restaurant, thumbed down another,
    and then thumbed up a movie cinema. So, what this means is that there's something
    about the second wing restaurant that he didn't like. Maybe it was the ambiance,
    or maybe it was a wing sauce. Whatever it was, his interest in hot wings—his professed
    interest in hot wings—is more nuanced than he originally led us to believe. And,
    likewise, he's expressed an interest in movies, even though he's not disclosed
    it. So, the point here is that people say more with their actions than they do
    with their words, and they're more honest with their actions than they are with
    their words. We can exploit that with recommender systems to learn these nuanced
    patterns between items and people's interests.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用户说他喜欢足球、辣翅和水上滑雪。然而，他的评分历史显示他给一家翅膀餐厅点赞，给另一家餐厅点了踩，然后又给一家电影院点赞。那这意味着，他对第二家翅膀餐厅有不满之处。也许是环境不佳，或者是翅膀酱的味道问题。不管怎样，他对辣翅的兴趣——他宣称的兴趣——比他最初告诉我们的更为复杂。同样，他对电影也表示了兴趣，尽管他没有明确说出来。因此，关键在于，人们通过行动传达的信息比通过语言更多，他们的行动比语言更真实。我们可以利用这一点，通过推荐系统来学习物品与人们兴趣之间这些细微的模式。
- en: Collaborative filtering is a common family of recommender systems. It's based
    on a concept known as **homophily**, which is basically *birds of a feather flock
    together*. So, that is, if you like something, people who also like that item
    probably share some other common interests with you; now we have a good pool of
    interest to start recommending things to one another.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤是一类常见的推荐系统。它基于一个叫做**同质性**的概念，意思是*物以类聚，人以群分*。也就是说，如果你喜欢某样东西，那么其他也喜欢它的人可能与你有一些共同的兴趣；这样，我们就有了一个很好的兴趣池，可以开始彼此推荐东西了。
- en: 'In a typical collaborative filtering system, this is the format our data is
    going to resemble:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的协同过滤系统中，我们的数据将类似于这种格式：
- en: '![](img/614624c4-1538-437a-afc7-0bb5068263f1.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/614624c4-1538-437a-afc7-0bb5068263f1.png)'
- en: In the preceding screenshot, users are shown along the *y* axis—which are rows—and
    items are shown along the *x* axis—which are columns. You might have explicit
    ratings, which are usually continuous along this continuum, or implicit, which
    are commonly binary. What we're showing here is explicit. The question we seek
    to answer is what's the predicted rating for a user? But to get there, we have
    to somehow compute the similarities between the items. This is a form of collaborative
    filtering called item-to-item collaborative filtering, and we can only compute
    similarities between the items that have been mutually rated by a user. This usually
    works best for explicitly rated systems; it's based on a paper that was published
    by Amazon several years ago.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图中，用户沿着 *y* 轴（即行）展示，而物品沿着 *x* 轴（即列）展示。你可能有明确的评分，通常是连续的，也可能是隐式评分，通常是二进制的。这里展示的是明确评分。我们要回答的问题是，预测用户的评分是多少？但要做到这一点，我们必须计算物品之间的相似度。这是一种叫做物品对物品协同过滤的协同过滤形式，我们只能计算用户共同评分的物品之间的相似度。这种方法通常对明确评分的系统效果最佳；它基于亚马逊几年前发布的一篇论文。
- en: 'Computing similarities between items is straightforward. We can compute pairwise
    similarities using one of several common metrics, including the **Pearson correlation**
    or cosine similarity. For example, we''re going to use cosine similarity as follows:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 计算物品之间的相似度非常简单。我们可以使用几种常见的度量方法来计算成对的相似度，包括**皮尔逊相关系数**或余弦相似度。例如，我们将使用余弦相似度，方法如下：
- en: '![](img/0a4dde7f-f4c5-4c07-bd5b-7ea6512187f0.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0a4dde7f-f4c5-4c07-bd5b-7ea6512187f0.png)'
- en: It's computed in a very similar fashion to what we looked at with clustering
    in [Chapter 3](028b1786-df10-4e2b-96be-541675edd2cd.xhtml),*Working with Non-Parametric
    Models*, the **Euclidean distance**. However, this is computing similarity rather
    than spatial distance. So, it's the exact inverse of the concept, but computed
    in a similar fashion.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其计算方法与我们在[第3章](028b1786-df10-4e2b-96be-541675edd2cd.xhtml)《与非参数模型的工作》中看到的聚类方法非常相似，即**欧几里得距离**。然而，这里计算的是相似度，而不是空间距离。因此，它是这一概念的完全逆向计算，但采用类似的方法。
- en: 'Since our data is so sparse, we''re going to start out by putting it into a
    sparse CSR matrix using SciPy, and rather than having to store 32 elements, now
    we only have to store 14:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据非常稀疏，我们将首先通过 SciPy 将其转换为稀疏 CSR 矩阵，而不是存储 32 个元素，我们现在只需要存储 14 个：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This is a dense matrix based on what we would actually see. So, you can imagine
    how handy this becomes when we have thousands of users and millions of items—as
    Amazon does, for instance.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基于实际情况的稠密矩阵。所以，你可以想象当我们有成千上万的用户和数百万个物品时，这会变得非常方便——比如亚马逊。
- en: We're simply going to compute the pairwise cosine similarities of the transpose
    of the matrix. We have a lot of zeros in here. It's not that a lot of these are
    orthogonal, which, mathematically, is what a cosine similarity would represent
    with a zero; it's that we're experiencing something called the item cold start,
    where there are several items that have never been mutually rated together. And,
    therefore, we cannot effectively compute the similarity on the basis of ratings
    alone.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简单地计算矩阵转置后的成对余弦相似度。这里面有很多零。这并不是说这些项目是正交的，数学上来说，余弦相似度为零意味着正交；而是因为我们遇到了一种被称为物品冷启动的问题，即有一些物品从未被共同评分。因此，我们不能仅凭评分有效地计算相似度。
- en: 'Now we will see how to generate predictions for a given user giving their history
    in the computed items similarities. In the following example, we are using the
    same user and we''re just predicting for `user_3`:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将看看如何根据给定用户的历史数据生成预测，这些预测基于计算出的物品相似度。在以下示例中，我们使用相同的用户，并且只是为`user_3`做预测：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: So, computing predictions is easy enough in this algorithm. You just compute
    the dot product of that user's ratings vector and the similarities matrix. Then, `argsort`
    it to descending order, in a very similar fashion to how we did with nearest neighbors,
    but the inverse in terms of descending versus ascending. So, there are things
    to note here. First, the predicted rating exceeds the scale of the ground truth
    rating of `6.12`. We only rated up to five, but we can't guarantee bounded ratings.
    So, we could either call those ratings or use some other strategy, but the other
    two ratings are actually the ones that the user has rated before. If you look
    back to the ratings matrix, both of these were rated as one star by the user.
    So, we can see that this is not a great recommender model with its low rank and
    low number of users.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，在这个算法中，计算预测其实很简单。你只需要计算该用户的评分向量与相似度矩阵的点积。然后，像我们对最近邻做的那样，使用`argsort`按降序排列，但与升序相反。需要注意的是，首先，预测评分超过了实际评分的范围，实际评分是`6.12`。我们最多只评分到五分，但我们无法保证评分的范围是有界的。因此，我们可以选择保留这些评分或者使用其他策略，但另外两个评分实际上是用户之前给出的评分。如果你查看评分矩阵，你会发现这两个评分都被用户打了1星。所以，我们可以看出，这个推荐模型的效果不太好，表现出低秩和较少的用户数量。
- en: Recommender systems are technically supervised learning, but they differ in
    the traditional sense of the *x*, *y* pairing since our ground truth is technically
    our data itself. So, in our example, we could look at the ratings for item four and
    one, and say how far we were off from the ground truth.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统技术上是监督学习，但它们与传统的*x*，*y*配对有所不同，因为我们的真实数据实际上就是我们的数据本身。所以，在我们的例子中，我们可以查看物品四和物品一的评分，然后看看我们与真实值的差距有多大。
- en: Item-to-item collaborative filtering
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 物品之间的协同过滤
- en: 'Let''s look at the code. This is item-to-item collaborative filtering. Let''s
    start with the `base.py` file that is present in `packtml/recommendation`:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下代码。这是物品之间的协同过滤。我们从`packtml/recommendation`中的`base.py`文件开始：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This `base` class is called `RecommenderMixin`. It''s simply an interface.
    There are two methods: one is already written for all subclasses, and that''s
    `recommend_for_all_users`; the other is `recommended_for_user`. So, we need to
    override it based on the subclass. The subclass we''re going to look at is item-to-item
    collaborative filtering.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`base`类叫做`RecommenderMixin`。它只是一个接口。里面有两个方法：一个是为所有子类写好的，就是`recommend_for_all_users`；另一个是`recommended_for_user`。所以，我们需要根据子类来重写它。我们接下来要看的子类是基于物品之间的协同过滤。
- en: 'In the following `itemitem.py` file, we see two parameters:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的`itemitem.py`文件中，我们看到了两个参数：
- en: '[PRE5]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We have `R` and `k`. `R`, which is our ratings matrix, it is different from
    other base estimators in that we don''t have the corresponding `y` value. `R`
    is our ground truth as well as the training array. `k` is a parameter that we
    can use to limit the top number of items that are similar. It helps reduce our
    space that we''re comparing within and makes computations easier. So, for the constructor,
    the fit procedure is simply computing the similarity array via the `compute_sim`
    function. We take the `R` array, transpose it so items are along the row axis,
    and then we compute the cosine similarity between the rows, which are now the
    items. We have an *n x n* matrix, the first *n* stands for the November matrix
    and the second *n* is the dimensionality of the number of items. Basically, we''re
    going to say anything that''s not in `top_k`, we''ll set to zero similarity. One
    of the strategies here is that it allows us to augment our similarity matrix in
    a way that, otherwise, we couldn''t. And that''s what we''re doing: argsorting
    into the descending order. We want the most similar first, argsorting along the
    columns. We take the similarity matrix and store that in `self.similarity`. And
    we''re going to use that when we compute predictions.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有 `R` 和 `k`。`R` 是我们的评分矩阵，它不同于其他基础估计器，因为我们没有相应的 `y` 值。`R` 是我们的真实数据以及训练数组。`k`
    是一个参数，我们可以用它来限制相似的物品的数量。它有助于减少我们比较的空间，并使得计算更容易。因此，对于构造函数，拟合过程只是通过 `compute_sim`
    函数计算相似度数组。我们获取 `R` 数组，进行转置，使得物品排列在行轴上，然后计算行之间的余弦相似度，行现在表示物品。我们得到一个 *n x n* 矩阵，第一个
    *n* 表示十一月矩阵，第二个 *n* 是物品数量的维度。基本上，我们会说任何不在 `top_k` 中的物品，将其相似度设为零。这里的一种策略是，它允许我们以一种否则无法实现的方式增强我们的相似度矩阵。这就是我们正在做的：将结果按降序进行
    argsort。我们希望最相似的排在前面，沿列进行 argsort。我们将相似度矩阵存储在 `self.similarity` 中。在计算预测时，我们将使用它。
- en: 'So, `recommend_for_user` is the function that we have to override in the super
    abstract interface. We can take several arguments. So, we have the user vector,
    which is an index, and *n*,which is the number of recommendations we want to produce.
    Now we get `user_vector` out of `R`:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，`recommend_for_user` 是我们需要在超抽象接口中重写的函数。我们可以传递几个参数。首先是用户向量，它是一个索引，和 *n*，即我们想要生成的推荐数量。现在，我们从
    `R` 中获取 `user_vector`：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The recommendations—the raw recommendations—are the inner products between the
    user vector and the similarity matrix, which produces an *nD* or *1D* array in
    NumPy.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐——原始推荐——是用户向量与相似度矩阵的内积，它在 NumPy 中生成一个 *nD* 或 *1D* 数组。
- en: 'We get `item_indices` with the help of an `arange` method in NumPy:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过 NumPy 中的 `arange` 方法获取 `item_indices`：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We're going to order this based on the descending `argsort` of the recommendations.
    Now we can limit them to the top `n` if we want to.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据推荐的降序 `argsort` 排序这个列表。现在，如果我们想，可以将它们限制为前 `n` 个。
- en: 'If you want to produce recommendations for everything, you can just pass `None`
    as `n`. We''re going to return `items`, `indices`, and `recommendations`, which
    are the predicted ratings for each of those corresponding items, as shown here:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想为所有项目生成推荐，可以将 `None` 作为 `n` 传递。我们将返回 `items`、`indices` 和 `recommendations`，即每个对应物品的预测评分，如下所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We go to the `example_item_item_recommender.py` file. We''ll load up the interestingly
    `titled` dataset called `get_completely_fabricated_ratings_data`, which is available
    in the `data.py` file. Here, we''ve several users, as shown in the following code:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进入 `example_item_item_recommender.py` 文件。我们将加载一个名为 `get_completely_fabricated_ratings_data`
    的有趣数据集，它可以在 `data.py` 文件中找到。这里，我们有多个用户，如以下代码所示：
- en: '[PRE9]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let's say that user 0 is a classic 30-year-old millennial who loves the nostalgia
    of the 90s. So, they highly rate `The Princess Bride`, `Ghost Busters`, and `Ghost
    Busters 2`. User 1 is a 40-year-old who only likes action movies. So, they rated
    `Die Hard` and `Pulp Fiction`. User 2 is a 12-year-old whose parents are fairly
    strict, so we can assume that user 2 has not watched `Pulp Fiction` or anything
    like that. But user 2 has watched `Ghost Busters`, `Ghost Busters 2`, and `The
    Goonies`. And user 2 rated them all pretty highly. User 3 has seen it all. And
    user 4 has just opened a Netflix account and hasn't had the chance to watch too
    much. So, user 4 is probably going to be the one we're interested in producing
    recommendations for.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 假设用户0是一个经典的30岁千禧一代，喜欢90年代的怀旧感。所以，他们会高度评价《公主新娘》，《幽灵busters》和《幽灵busters 2》。用户1是一个40岁的人，只喜欢动作电影。所以，他们会评分《虎胆龙威》和《低俗小说》。用户2是一个12岁的孩子，父母相对严格，因此我们可以假设用户2没有看过《低俗小说》之类的电影。但用户2看过《幽灵busters》，《幽灵busters
    2》以及《怪物史瑞克》。并且用户2对这些电影的评分都很高。用户3什么都看过。用户4刚刚开通了Netflix账户，还没有机会看太多东西。所以，用户4可能是我们最感兴趣的推荐对象。
- en: All this is a NumPy array. We're returning a dense array. You can return this
    as a sparse array.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都是一个NumPy数组。我们返回的是一个密集数组。你也可以将其返回为稀疏数组。
- en: 'In the `example_item_item_recommender.py` file that is present in `examples/recommendation`,
    we''re going to get the `R` ratings matrix and `titles` from `get_completely_fabricated_ratings_data`:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在`examples/recommendation`目录下的`example_item_item_recommender.py`文件中，我们将从`get_completely_fabricated_ratings_data`函数中获取`R`评分矩阵和`titles`：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We create a `recommender` item with `k=3`. We only retain the three most similar
    corresponding items for each of the items. And then we produce the recommendations
    for user 0.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个`recommender`对象，设置`k=3`。我们只保留每个项目的三个最相似的项目。然后，我们为用户0生成推荐。
- en: 'Let''s see what the top three rated movies are for user 0 if we run the `example_item_item_recommender.py`
    file:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行`example_item_item_recommender.py`文件，我们可以查看用户0的前三部评分电影：
- en: '![](img/4b81ee6d-5478-4e70-86f7-dde29b7d95dd.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b81ee6d-5478-4e70-86f7-dde29b7d95dd.png)'
- en: 'User 0''s top three rated movies are: `Ghost Busters`, `The Goonies`, and `Pulp
    Fiction`. This means user 0 has rated `Ghost Busters` and `The Goonies` highly
    but has not rated `Pulp Fiction`.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用户0的前三部评分电影是：《幽灵busters》，《怪物史瑞克》，和《低俗小说》。这意味着用户0高度评价了《幽灵busters》和《怪物史瑞克》，但并未评价《低俗小说》。
- en: We can also see that the mean average precision is roughly 2/3\. The mean average
    precision is a metric that we're going to use for recommender systems. It actually
    comes out of the information retrieval domain. It's not like, say, mean absolute
    error or mean squared error. What we're doing is stating what proportion of the
    ones we recommend existed in the ground truth set. In this case, it means which
    ones the user rated highly to begin with, which shows that the ones we produced
    were pretty good.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，平均精准度大约是2/3。平均精准度是我们在推荐系统中将要使用的一个指标。它实际上来自信息检索领域。它不像例如平均绝对误差或均方误差。我们所做的是表明我们推荐的项在实际结果集中所占的比例。在这个案例中，它表示用户最初评分很高的那些项目，这显示我们产生的推荐是相当不错的。
- en: Matrix factorization
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵分解
- en: In this section, we're going to look into recommender systems and introduce
    matrix factorization techniques. In typical collaborative filtering problems,
    we have users along one axis and items or offers along the other axis. We want
    to solve for the predicted rating for a user for any given item, but to get there
    we have to somehow compute the affinity between the users or the item. In the
    previous section, we looked at item-to-item collaborative filtering, where we
    explicitly computed the similarity matrix using the cosine similarity metric,
    but now we want to explore a method that's not going to explicitly compare items
    to items or users to users.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将深入探讨推荐系统，并介绍矩阵分解技术。在典型的协同过滤问题中，我们在一个轴上有用户，另一个轴上有项目或物品。我们想为每个给定的项目计算用户的预测评分，但为了达到这个目的，我们必须以某种方式计算用户或项目之间的相似度。在前一节中，我们探讨了项对项的协同过滤方法，其中我们显式地使用余弦相似度度量来计算相似度矩阵，但现在我们希望探索一种不显式地将项目与项目或用户与用户进行比较的方法。
- en: 'Matrix factorization is a form of collaborative filtering that focuses on the
    intangibles of products. At a conceptual level, every product or restaurant, for
    example, has intangibles that cause you to like, dislike, or remain indifferent
    toward them. For example, for a restaurant, maybe the atmosphere or the vibe you
    get outweighs the menu. Or, consider the following statement: the food''s terrible
    but the happy hour is great. In this case, we''re interested in learning the hidden
    or latent variables that underlie and manifest themselves throughout patterns
    in the data.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解是一种协同过滤的形式，侧重于产品的无形因素。从概念上讲，每个产品或餐厅都有无形因素，导致你喜欢、不喜欢或对它们保持中立。例如，对于一家餐厅，可能是气氛或你感受到的氛围比菜单更重要。或者，考虑以下表述：食物很糟糕，但欢乐时光很棒。在这种情况下，我们感兴趣的是学习隐藏或潜在变量，它们在数据模式中以某种方式体现出来。
- en: 'Matrix factorization is going to allow us to discover these latent variables
    by decomposing our single ratings matrix into two low-rank matrices that, 2 when
    multiplied, approximate the original ratings matrix. Intuitively, we''re learning
    about these hidden factors or latent variables and learning how our users and
    items score against them. As shown in the following diagram, one of the low-rank
    matrices maps the users'' affinities for the discovered factors and the other
    maps that item''s rankings on the factors:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解将允许我们通过将单一的评分矩阵分解成两个低秩矩阵来发现这些潜在变量，这两个矩阵相乘时能够近似原始评分矩阵。直观地说，我们在学习这些隐藏因素或潜在变量，并且学习我们的用户和物品如何在这些因素上得分。如下图所示，其中一个低秩矩阵映射用户对已发现因素的偏好，另一个映射该物品在这些因素上的排名：
- en: '![](img/4934bcc4-61ec-4baf-9f3a-3d5399ace61b.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4934bcc4-61ec-4baf-9f3a-3d5399ace61b.png)'
- en: A drawback in matrix factorization is the lack of clarity or intuition behind
    what can make up a factor. It's similar to a **principal component **analysis (**PCA**)
    type technique, where a factor can be conceptualized as a topic. A careful, insightful
    analyst who has lots of subject matter expertise could feasibly extract meaning
    from topics, but it's very difficult to do so and, as a result, it's not typically
    pursued given its difficulty. For example, maybe **Factor 1** in the preceding
    diagram is a divey atmosphere. So, the wing shop is rated in varying degrees of
    divey-ness. As you can see on the right-hand side of the preceding diagram, there's
    a strong affinity between **Wing Store A** and the first factor, which is **Dive
    bar**. You can also assume that **The Sports Bar** might rate pretty highly on
    that scale. Then, perhaps **Factor 2** is a place that has some health-conscious
    options. So, the strength of that connection is the level at which a person or
    an offering ranks against the latent factor. You can see this on both the left-and
    the right-hand sides of the preceding diagram.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解的一个缺点是缺乏清晰的直觉或对因素组成的理解。这类似于**主成分分析**（**PCA**）类型的技术，其中一个因素可以被概念化为一个主题。一个有深厚专业知识的、仔细且有洞察力的分析师可能能够从这些主题中提取出意义，但这是非常困难的，因此通常不被追求。例如，也许前面图中的**因素1**是一个低调的氛围。所以，鸡翅店在不同程度的低调程度上被评分。正如你在前面图的右侧所看到的，**鸡翅店A**和第一个因素之间有很强的亲和力，即**低调酒吧**。你也可以假设**体育酒吧**在这个尺度上可能会评分较高。那么，可能**因素2**是一个有一些健康选项的地方。所以，这个连接的强度是某个人或某个产品在潜在因素上的排名程度。你可以在前面图的左右两边看到这一点。
- en: 'Essentially, we have a ratings matrix, *Q*. In different literature, it''s
    referred to as either *Q* or *R*. We''re going to call it *Q* here. We want to
    discover two lower rank matrices, *X* and *Y*, such that the product of the two
    approximate the ratings matrix. That is, *Q* or *Q* prime is approximately equal
    to *X.Y^T*:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们有一个评分矩阵，*Q*。在不同的文献中，它被称为*Q*或*R*。我们在这里称之为*Q*。我们想要发现两个低秩矩阵，*X*和*Y*，使得它们的乘积近似评分矩阵。也就是说，*Q*或*Q*
    prime大致等于*X.Y^T*：
- en: '![](img/9d2b83e8-9d17-4d53-9871-343eaf4841a3.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9d2b83e8-9d17-4d53-9871-343eaf4841a3.png)'
- en: Our objective function is at the bottom and is basically a regularized mean
    squared error. So, we're looking at the mean squared error, or the reconstruction
    error, between *X* and *Y* and *Q* prime, and then we have the regularization
    term over on the other side, with lambda.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标函数在底部，基本上是一个正则化的均方误差。所以，我们在关注*X*和*Y*与*Q* prime之间的均方误差，或者叫重构误差，然后我们在另一边有正则化项，带有lambda。
- en: For the math folks, factorizing a matrix is nothing new. But doing so in the
    context of finding such low-rank matrices in a non-convex optimization problem
    might be a bit of a challenge. So, the approach we're going to see is called **Alternating
    Least Squares** (**ALS**).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数学领域的人来说，矩阵分解并不新鲜。但在非凸优化问题中，寻找这样的低秩矩阵可能是一个挑战。因此，我们将看到的这种方法叫做**交替最小二乘法**（**ALS**）。
- en: 'The ALS algorithm is as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ALS算法如下：
- en: Initialize two random matrices, *X* and *Y*
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化两个随机矩阵，*X*和*Y*
- en: Set empty values of *Q* and *O*
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置*Q*和*O*的空值
- en: 'Beginning with *X*, solve the following:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*X*开始，求解以下问题：
- en: '![](img/7d8bb3e8-3235-4620-928a-0563034418ed.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d8bb3e8-3235-4620-928a-0563034418ed.png)'
- en: 'Now solve for *Y* with the new *X*:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在用新的*X*来求解*Y*：
- en: '![](img/4aa21afa-3c85-45b0-81d8-c752d7bc6bdb.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4aa21afa-3c85-45b0-81d8-c752d7bc6bdb.png)'
- en: Iterate, alternating between *X* and *Y* until convergence
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 迭代，在*X*和*Y*之间交替，直到收敛
- en: Essentially, we're going to alternate between solving each respective matrix
    with respect to the other, and we'll eventually reach a point of convergence.
    So, we start out by initializing both *X* and *Y* to random values. Then, starting
    with *X*, we solve for *X* prime. Now that we have a more refined version of *X*
    prime, we can use that to solve for *Y* prime. Each matrix creates a better solution
    for the other at each iteration. And we can alternate like this for as many iterations
    as we like, or until we hit a point of diminishing returns, where we would say
    that we've converged.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们将交替求解每个矩阵，并最终达到收敛点。因此，我们从初始化*X*和*Y*为随机值开始。然后，从*X*开始，我们求解*X*的改进版本。现在，我们有了更精细的*X*，可以用它来求解*Y*。每次迭代时，每个矩阵都会为另一个矩阵提供更好的解。我们可以像这样交替进行多次迭代，直到达到收益递减点，此时我们可以认为已收敛。
- en: 'A quick note on the notation here: the *I* that you can see next to lambda
    is simply an *F x F* identity matrix, where *F* is the number of latent factors
    that we want to discover. We multiply that by the regularization parameter lambda.
    So, along the diagonal axis we have lambda, and then the rest is simply zeros.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个关于符号的简要说明：你可以看到与lambda旁边的*I*实际上是一个*F x F*的单位矩阵，其中*F*是我们希望发现的潜在因子的数量。我们将其与正则化参数lambda相乘。因此，在对角轴上我们有lambda，其他位置则是零。
- en: 'Here''s a hackneyed 30-line approximation of ALS in Python. We start out with
    defining `Q` or the ratings matrix:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个常见的30行Python代码，用于近似ALS。我们从定义`Q`或评分矩阵开始：
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is the rating that we've seen in the earlier example, and in the previous
    section. Now we're going to get a Boolean mask, `nan_mask`. First, we're going
    to set all the missing values to zero for the ensuing computations. Next, we're
    going to initialize `I` as our identity matrix and multiply it by lambda. We only
    have to do that one time, which is nice. Lambda is just 0.01 for now, but that's
    a hyperparameter that can be tuned using cross-validation. So, the higher lambda
    is, the more we'll regularize. Then, we initialize `X` and `Y` with `random_state`.
    `X` is going to be equal to *M x F*, that is, the number of users by the number
    of factors. `Y` is going to be equal to the number of factors by the number of
    items: *F x N*.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们在前面示例和章节中看到的评分。现在，我们将得到一个布尔掩码，`nan_mask`。首先，我们将所有缺失的值设置为零，便于后续计算。接下来，我们将初始化`I`为单位矩阵，并将其与lambda相乘。我们只需要做一次这一步，这很方便。现在lambda值是0.01，但它是一个可以通过交叉验证调优的超参数。因此，lambda值越高，我们的正则化越强。然后，我们使用`random_state`初始化`X`和`Y`。`X`将等于*M
    x F*，即用户数量乘以因子数量。`Y`将等于因子数量乘以项目数量：*F x N*。
- en: In iterating, we solve for `X`, and then we solve for `Y` given the new `X`.
    Then, we compute our training loss, which is again the masked version of the mean
    squared error, where we mask out the missing values from the original ground truth
    array, which is our ratings array. And then we continue to iterate until we reach
    convergence.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在迭代中，我们先求解`X`，然后在给定新的`X`的情况下求解`Y`。接着，我们计算训练误差，这也是均方误差的掩码版本，我们将原始真实值数组中的缺失值（即评分数组）去除。然后我们继续迭代，直到达到收敛。
- en: At the bottom of the preceding code, you can see the output of the approximation
    between `X` and `Y`. It is an approximation. If you look at the definition of
    `Q`, 3 and then the output at the bottom, it looks pretty similar. So, the way
    that we would create predictions at the end is that we exploit the error in the
    whole system, and return the highest predicted items for a user filtering the
    previously rated ones. So, user 4, (the very last user), would get a recommendation
    for the steakhouse that is *2.0*, and this is the highest non-previously rated
    item for that user. This is actually just a result of the multiplication error
    or the approximation error.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面代码的底部，你可以看到`X`和`Y`之间逼近的输出。这是一个逼近。如果你查看`Q`的定义，3 和底部的输出，看起来是非常相似的。因此，我们最终创建预测的方式是利用整个系统中的误差，并返回用户的最高预测项目，同时过滤掉先前已评分的项目。所以，用户
    4（最后一个用户）将得到一条推荐，推荐的牛排馆是*2.0*，这是该用户未评分的最高项目。这实际上只是乘法误差或逼近误差的结果。
- en: 'In the following graph, you can see how the training loss diminishes over each
    iteration:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，你可以看到训练损失如何随着每次迭代而减少：
- en: '![](img/7195d7ee-a2ae-4df0-a5e9-142bc00d8fb4.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7195d7ee-a2ae-4df0-a5e9-142bc00d8fb4.png)'
- en: Matrix factorization in Python
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的矩阵分解
- en: In the previous section, we wanted to decompose our ratings matrix into two
    low-rank matrices in order to discover the intangible latent factors that drive
    consumers' decisions. One matrix maps the users' affinities for the discovered
    factors and the other maps the items' rankings on those factors.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们想将评分矩阵分解为两个低秩矩阵，以发现驱动消费者决策的无形潜在因子。一个矩阵映射了用户对已发现因子的偏好，另一个矩阵映射了物品在这些因子上的排名。
- en: 'So, let''s look at how this can be implemented in Python. We''ve two files,
    `als.py` and `example_als_recommender`. Let''s see our `als.py` file. In the last
    section, we saw the item-to-item collaborative filter; ALS is very similar. It''s
    going to implement `RecommenderMixin`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来看一下如何在Python中实现这一点。我们有两个文件，`als.py` 和 `example_als_recommender`。让我们看看我们的`als.py`文件。在上一节中，我们看到了基于物品的协同过滤；ALS非常相似。它将实现`RecommenderMixin`：
- en: '[PRE12]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We have several parameters for ALS. The first one, and the only non-optional
    one, is `R`, our ratings matrix. In some of the math we've seen, we've referred
    to this interchangeably as `R` and `Q`. Again, that's kind of a quirk of the literature.
    Depending on what papers you're reading, it's one or the other. And the second
    parameter we're going to take is `factors`.The `factors` parameter is the number
    of latent variables we want to discover. I have used float, but you can use an
    integer. The floating point is just going to be bound between zero and one. `n_iter`
    is the number of iterations. ALS, in this module, does not support early convergence
    or early stopping. That's something that you could absolutely write. But if you
    have too many iterations, what happens is you're probably going to overfit your
    data. Lambda is our regularization parameter, and then you can just pass `random_state` as
    a way for reproducibility.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几个参数用于ALS。第一个，也是唯一的非可选参数，是`R`，我们的评分矩阵。在我们看到的一些数学中，我们将其交替称为`R`和`Q`。这也是文献中的一个特点。根据你阅读的论文，它可能是其中之一。第二个参数是`factors`。`factors`参数是我们想要发现的潜在变量的数量。我使用的是浮动点数，但你也可以使用整数。浮动点数的范围将限制在零到一之间。`n_iter`是迭代次数。这个模块中的ALS不支持提前收敛或提前停止。这是你完全可以编写的功能。但如果你有太多的迭代，结果可能会是过拟合数据。Lambda是我们的正则化参数，然后你可以传递`random_state`来确保可复现性。
- en: 'For the first step, as always, we''re going to check our array to make sure
    that we have only floating points:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一步，像往常一样，我们将检查我们的数组，确保只有浮动点数：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We are going to allow missing data here, because missing data is natural in
    recommender systems. And we can almost guarantee there's always going to be missing
    data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在这里允许缺失数据，因为在推荐系统中，缺失数据是很自然的。我们几乎可以保证总会有缺失数据。
- en: 'In the following code, we''re making sure that our factor is an integer. And
    if it''s `float`, we figure out the number of `factors` we''re going to discover:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们确保我们的因子是一个整数。如果它是`float`，我们会计算出我们要发现的`factors`的数量：
- en: '[PRE14]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'So, `W` here is equal to `nan_mask`, which we looked at in the previous section:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，这里的`W`等于`nan_mask`，我们在上一节中已经查看过：
- en: '[PRE15]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This is going to be, essentially, a weighting array that says whether or not
    the value was missing to begin with. And so, we use this to mask our ground truth
    out of the ratings matrix when we compute our mean squared error during our iterations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这将基本上是一个权重数组，用来表示某个值是否最初缺失。因此，我们使用它来掩盖我们在迭代中计算均方误差时的评分矩阵中的真实值。
- en: 'Here, we initialize `Y`:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们初始化`Y`：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We are not initializing `X` because we know that that's going to be the first
    one we solve for in our iterations. So, as we have seen in the previous section,
    we also initialize `I` as the identity matrix—that is, *F x F*—and multiply it
    by our regularization parameter.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有初始化`X`，因为我们知道它将在迭代中首先被求解。因此，正如我们在前面的部分所看到的，我们也将`I`初始化为单位矩阵——即*F x F*——并将其与我们的正则化参数相乘。
- en: 'Now we''re going to iterate, as shown in the following code:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们要进行迭代，如下代码所示：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Begin by solving for `X`, and then solve for `Y`. At each iteration, we're going
    to just calculate the training error, which is the mean squared error. We append
    it to the list that we store as a `self` parameter in the following code.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从解决`X`开始，然后解算`Y`。在每次迭代中，我们只计算训练误差，即均方误差。我们将其添加到在以下代码中作为`self`参数存储的列表中。
- en: 'The training phase is actually extraordinarily easy for ALS. Now, in the previous,
    section we didn''t see how to concretely generate predictions. We saw the math
    behind it, but we haven''t implemented it. If you call predict on ALS, as shown
    in the following code, it''s simply going to compute the product of the user factors
    and the item factors to return the `R` prime—basically the approximation:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ALS，训练阶段实际上是异常简单的。现在，在前面的部分中，我们没有看到如何具体生成预测。我们看到了背后的数学原理，但还没有实现它。如果你像下面的代码所示调用`predict`，它只是计算用户因子和项目因子的乘积，从而返回`R`的近似值：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: You can pass in `R`, which would ostensibly be the test data. This is the data
    to include new users who weren't included in the fit originally, or it could mean
    that the users have updated their data. But we can recompute the user factors
    if we want to. So, if the users have moved on in time and our fit is about a week
    old, then we can recompute the user factors with respect to the existing item
    factors. Then, at the end, we're just returning the product of `X` and `Y`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以传入`R`，这通常是测试数据。这些数据包括最初未参与拟合的新用户，或者可能意味着用户已经更新了他们的数据。如果需要，我们可以重新计算用户因子。所以，如果用户已经有了时间上的变化，而我们的拟合大约有一周的历史，那么我们可以根据现有的项目因子重新计算用户因子。最后，我们只是返回`X`和`Y`的乘积。
- en: 'Now we''ll call the `recommend_for_user` function. So, given your test matrix
    and the user index, we want to know what the top `n` items are to recommend for
    a user and we do largely the same thing:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将调用`recommend_for_user`函数。所以，给定你的测试矩阵和用户索引，我们想知道推荐给某个用户的前`n`个项目是什么，我们基本上做的是同样的事情：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We''re going to create this prediction, but extract out the predicted user
    vector. So, we''re using the `self.predict` method, as shown in the following
    code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建这个预测，但提取出预测的用户向量。所以，我们使用`self.predict`方法，如下代码所示：
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If we are interested in filtering out the ones we previously saw, we just mask
    those out and return the descending argsorted indices of items that we're interested
    in. This is very similar to what we've seen before when we were looking at spatial
    clustering, but here, all we're doing is computing the approximation of `X` and
    `Y` and argsorting the columns.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有兴趣过滤掉之前看到的项目，我们只需将它们掩盖掉，并返回按降序排列的我们感兴趣的项目的索引。这与我们之前在进行空间聚类时看到的非常相似，不过在这里，我们所做的只是计算`X`和`Y`的近似值，并对列进行argsort排序。
- en: 'Let''s look at an example in the `example_als_recommender.py` file:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下`example_als_recommender.py`文件中的示例：
- en: '[PRE21]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You may recall from the preceding code the recommended data. This is the completely
    fabricated data that we went on about in the previous sections. We're going to
    take this same data and we're going to fit ALS on it. We want to know user 0's
    predictions, so, before we run it, we need some information. Let's say user 0
    rated `Ghost Busters` pretty highly, and rated `The Goonies` pretty highly as
    well. This guy knows their stuff! So, this guy is a classic 90s/late 80s millennial.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得前面代码中的推荐数据。这些完全是我们在前几节中提到的虚构数据。我们将使用相同的数据，并在其上进行ALS拟合。我们想知道用户0的预测值，因此，在运行之前，我们需要一些信息。假设用户0对《Ghost
    Busters》评价很高，并且对《The Goonies》评价也很高。这个人真是行家！所以，他是典型的90年代/80年代末期的千禧一代。
- en: 'You''ll notice, in the following screenshot, that we have activated my `packt-sml` conda
    environment:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，你会注意到我们已经激活了我的`packt-sml` conda 环境：
- en: '![](img/bfc0e9e8-ca51-4a3a-be29-063f86051706.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bfc0e9e8-ca51-4a3a-be29-063f86051706.png)'
- en: 'The output of the preceding code is as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '![](img/8e71358d-9e6c-4a09-9a07-701681ebf40f.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e71358d-9e6c-4a09-9a07-701681ebf40f.png)'
- en: You need to do the same. So, when we run this, we'll get the preceding graph,
    which is showing how the training error diminishes over the iterations, as we
    expect it would. As a result, we would recommend that user 0 watch `Weekend at
    Bernie's` as the top-rated suggestion. And that seems to make sense given `The
    Goonies` and `Ghost Busters`. But then `Pulp Fiction` is a bit violent, and so
    we also recommended `Clockwork Orange`, which also seems to jive with that. So,
    the mean average precision is, essentially, looking at the recommendations and
    then comparing them to the ground truth and saying how many of those were actually
    previously rated highly.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 你也需要做同样的事情。所以，当我们运行这个时，我们会得到前面的图表，它展示了训练误差如何随着迭代而减少，正如我们预期的那样。因此，我们会推荐用户 0 观看`Weekend
    at Bernie's`，作为评分最高的推荐。这似乎很有道理，因为他已经看过`The Goonies`和`Ghost Busters`。但是，`Pulp Fiction`有点暴力，所以我们也推荐了`Clockwork
    Orange`，这也似乎很吻合。因此，平均精度，基本上是查看推荐结果，然后将其与真实情况进行比较，看看其中有多少是之前被高度评价过的。
- en: Limitations of ALS
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ALS 的局限性
- en: We've been using explicit ratings. For example, on Amazon, ratings are between
    one and five stars. The problem here is that explicit rating systems typically
    have trouble getting users to rate the items, because it's easier to consume that
    content than it is to evaluate it from the user side. So, implicit ratings are
    the inverse of explicit ratings and they can be collected by a system, usually,
    without the user's awareness. A lot of times that's more favorable, because it
    doesn't require the user to interact with the system in a secondary sense to explicitly
    rate items, and we can get more data, which means less sparse data. So, implicit
    ratings might include the number of listens to a song. There's really well-known
    ratings dataset collected by the Last FM team that uses implicit ratings, and
    it's commonly used for benchmarking recommender systems. There is an implicit
    variation of ALS, but we only covered the explicit version. But if you check on
    Google for implicit ALS, there's all sorts of literature around it. We encourage
    you to go look it up.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用显式评分。例如，在亚马逊上，评分是介于一到五星之间的问题。这里的问题是，显式评分系统通常会遇到让用户进行评分的困难，因为与其从用户角度评估内容，不如直接消费内容更为容易。所以，隐式评分是显式评分的反面，它们通常能在用户不知情的情况下被系统收集。很多时候，这更有利，因为它不需要用户在二次交互中显式评分，从而可以收集更多数据，也意味着数据更不稀疏。所以，隐式评分可能包括歌曲的收听次数。Last
    FM 团队收集的一个非常著名的评分数据集就使用了隐式评分，通常用于推荐系统的基准测试。ALS 有一个隐式版本，但我们只讲解了显式版本。不过，如果你在 Google
    上搜索隐式 ALS，会有很多相关文献。我们鼓励你去查阅。
- en: The next challenge of recommenders is sparsity versus density. As we've seen,
    ratings matrices can be pretty sparse. For some systems, such as Amazon, there
    may only be ratings for less than approximately one percent of all items per user,
    and a lot of times even less than that. So, dense matrices are not usually the
    best solution, and oftentimes they're not even feasible. So, we either have to
    use sparse matrices or get really clever with how we distribute the data, so we
    don't totally blow up our memory.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统的下一个挑战是稀疏性与密集性。正如我们所见，评分矩阵通常是非常稀疏的。对于一些系统，比如亚马逊，每个用户的所有项目评分可能不到百分之一，很多时候甚至更少。因此，密集矩阵通常不是最佳的解决方案，很多时候甚至不可行。所以，我们要么使用稀疏矩阵，要么在如何分配数据上足够聪明，这样就不会把内存消耗得太高。
- en: Recommenders typically take a very long time to train. Like many other machine
    learning models, we run into that same kind of thing, but recommenders are a bit
    different in the sense that they have to be updated in much greater frequency,
    in many cases, multiple times per day, depending on the system itself. So, new
    items arriving in a catalog or new users beginning to consume media means that
    the recommender has to be refreshed. But we can't do this online or in real time,
    or we risk taking the system down. So, generally, recommenders are retrained on
    a periodic basis in an offline fashion. And the models are scored in an online
    or more real-time fashion.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统通常需要很长时间进行训练。像许多其他机器学习模型一样，我们会遇到类似的情况，但推荐系统有些不同，因为它们需要更加频繁地更新，在许多情况下，每天多次更新，具体取决于系统本身。因此，新项目的加入或新用户开始使用媒体意味着推荐系统需要刷新。但我们不能在线或实时进行更新，否则可能会使系统崩溃。因此，一般来说，推荐系统会定期离线重新训练。而模型的评分则是在在线或更实时的方式下进行的。
- en: In this section, we looked at the Python implementation of ALS in the `packtml`
    library and an example. Finally, we discussed some of the real-world challenges
    we face in recommender systems.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们看了 `packtml` 库中 ALS 的 Python 实现以及一个示例。最后，我们讨论了推荐系统中一些现实世界的挑战。
- en: Content-based filtering
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于内容的过滤
- en: In this section, we're going to wrap up our discussion around recommender systems
    by introducing an entirely separate approach to computing similarities and look
    at how we can use it to augment our collaborative filtering systems.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将通过介绍一种完全不同的计算相似度的方法来结束我们对推荐系统的讨论，并看看如何利用这种方法来增强我们的协同过滤系统。
- en: Content-based recommenders operate similarly to the original item-to-item collaborative
    system that we saw earlier, but they don't use ratings data to compute the similarities.
    Instead, they compute the similarities directly by using provided attributes of
    the items in the catalog. Predictions can then be computed in the same fashion
    as item-to-item collaborative filtering by calculating the product of the ratings
    matrix and similarity matrix.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的推荐系统的工作方式与我们之前看到的原始的项目对项目协同系统类似，但它们并不使用评分数据来计算相似度。相反，它们通过使用目录中项目的属性直接计算相似度。然后，预测可以像项目对项目协同过滤一样通过计算评分矩阵和相似度矩阵的乘积来得出。
- en: 'Here''s an example of how we might use content vectors to directly compute
    the item similarity matrix:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何使用内容向量直接计算项目相似度矩阵的一个示例：
- en: '[PRE22]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下：
- en: '[PRE23]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We're using the same ratings matrix as we have over the last few sections, and
    we've created 11 different attributes about the various restaurants. Generally,
    the content vectors of these dummy-encoded features indicate whether an item belongs
    to a given category. So, you can see the similarity is computed in exactly the
    same fashion. So, we just compute the cosine similarity between the rows. And
    then we even generate predictions in the same way. We compute the product of the
    similarities and the ratings.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的是与之前几个部分相同的评分矩阵，并且我们为不同的餐厅创建了11个不同的属性。通常，这些虚拟编码特征的内容向量表示一个项目是否属于某个特定类别。因此，您可以看到，相似度的计算方式完全相同。所以，我们只是计算行之间的余弦相似度。然后，我们也以相同的方式生成预测。我们计算相似度和评分的乘积。
- en: Limitations of content-based systems
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于内容的系统的局限性
- en: There are several notable limitations to content-based systems that make them
    less than ideal in most scenarios. The first of these is the manual nature of
    the feature engineering, which can be extraordinarily tough given that the difficulty
    of collecting the data about the items can be really time-consuming, and many
    times, the data we're presented about an item is limited to a text description.
    So, we're not given this nice encoded matrix and that means we have to extract
    the attributes from descriptions, which can be challenging and extremely time-intensive.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基于内容的系统有几个显著的局限性，使得它们在大多数场景中并不理想。首先是特征工程的手动性质，由于收集有关项目的数据可能非常耗时，而我们关于项目的数据往往仅限于文本描述，因此这会非常困难。所以，我们并没有这个很好的编码矩阵，这意味着我们必须从描述中提取属性，而这可能是具有挑战性并且非常耗时的。
- en: Next, we end up with the largely dummy-encoded set of content vectors, meaning
    it's heavily zero inflated. So, naturally, our similarity computations are going
    to be fairly low with respect to what we might get out of a comparable collaborative
    approaches computation. And, finally, as our feature matrix grows in rank, the
    similarity between the two given items will be orthogonal or zero, so the likelihood
    of that approaches 1\. For more information, you can refer to [https://math.stackexchange.com/questions/995623/why-are-randomly-drawn-vectors-nearly-perpendicular-in-high-dimensions](https://math.stackexchange.com/questions/995623/why-are-randomly-drawn-vectors-nearly-perpendicular-in-high-dimensions).
    It's a loose proof showing that the higher the rank, the more likely it is that
    you approach that orthogonality, which we don't want. All these limitations make
    a good case for why content-based systems are less favorable than collaboratively
    based systems.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们得到了一个大多数为虚拟编码的内容向量集合，这意味着它大部分是零膨胀的。因此，自然地，我们的相似度计算会相对较低，和其他可比的协同过滤方法的计算结果相比，可能会较差。最后，随着我们的特征矩阵秩的增加，两个给定项目之间的相似度将变得正交或为零，因此其概率接近1。有关更多信息，您可以参考 [https://math.stackexchange.com/questions/995623/why-are-randomly-drawn-vectors-nearly-perpendicular-in-high-dimensions](https://math.stackexchange.com/questions/995623/why-are-randomly-drawn-vectors-nearly-perpendicular-in-high-dimensions)。这是一个松散的证明，表明秩越高，越有可能接近这种正交性，而我们并不希望这样。所有这些局限性为内容驱动的系统相较于协同驱动的系统较不有利提供了一个充分的理由。
- en: But there're also some cases where they can be really useful. One of these is
    called the **cold-start problem**, which we discussed earlier in this section,
    and we encounter in every collaborative filtering application. This is when a
    new item is added and it cannot be compared to an existing item on the basis of
    ratings due to its own lack of ratings. So, the challenge here, apart from being
    unable to compute that similarity, is that if you impute it with a 0 or some other
    random value, you may never present that to a consumer. You implicitly diminish
    the chance that you would ever recommend that item.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 但是也有一些情况，它们实际上可以非常有用。其中之一被称为**冷启动问题**，我们在本节前面讨论过，它出现在每个协同过滤应用中。这是指当一个新项目被添加时，由于缺乏评分，无法根据评分与现有项目进行比较。因此，这里的挑战除了无法计算相似度外，还是如果你用0或其他随机值填充它，你可能永远无法将其推荐给消费者。你实际上是减少了推荐该项目的机会。
- en: In item-to-item collaborative filtering, it also occurs in situations where
    there are two items that have not been mutually rated by the same user, since
    we can't compute the similarity. So, that's an additional case and, in this one,
    it's going to result in a similarity of 0 in our matrix because we impute all
    the missing values with 0, even though we, theoretically, have ratings on which
    to gauge the affinity. In these scenarios, it's useful to have a fallback plan.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于项目的协同过滤中，也会出现两项未被同一用户互相评分的情况，因为我们无法计算它们的相似度。因此，这是一个额外的情况，在这种情况下，由于我们用0填充了所有缺失的值，即使我们在理论上有评分可以用来衡量亲和度，最终我们矩阵中的相似度也将为0。在这些情况下，制定一个备用方案是非常有用的。
- en: 'Here, we''re fitting an item-to-item collaborative filtering recommender:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在拟合一个基于项目的协同过滤推荐系统：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output of the preceding code is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码的输出如下：
- en: '[PRE25]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: From preceding code, we see several sections from the `packtml` package on our
    ratings data, which we've been using for the last few sections. We're going to
    use the content similarity computations to impute the data that suffers from the
    cold-start problem. When we examine the similarity matrix, you can see that there
    are no more 0s. So, there is a corner case where you might get a 0, and that's
    if you had a missing mutual similarity or a cold-start problem, and then perfect
    orthogonality in the actual content vectors. But we don't see that. So, ostensibly,
    this gets us closer to a more robust model. But you're still restricted to the
    limitations that we have seen before, namely, collecting the content attributes
    and computing those potentially orthogonal vectors.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码中，我们可以看到`packtml`包中的几个部分，这些部分涉及我们在过去几节中使用的评分数据。我们将使用内容相似度计算来填补受冷启动问题影响的数据。当我们检查相似度矩阵时，可以看到不再有0的存在。因此，可能会出现一个极端情况，那就是如果你遇到缺失的互相评分相似度或冷启动问题，且实际内容向量之间存在完美的正交性，那么你可能会得到一个0。但我们没有看到这种情况。因此，表面上看，这让我们离更强健的模型更近了一步。但你仍然受限于我们之前看到的局限性，即收集内容属性并计算这些可能的正交向量。
- en: So, at this point, you're familiar with the concept and you realize content-based
    similarities alone are not very feasible. But they can actually augment your collaborative
    filtering method if you have the right situation and setup. There's been a lot
    of research around using neural networks to automatically hybridize content-based
    and collaborative systems. A lot of them are using neural networks to create features
    from text descriptions a touch informal in an automatic sense, and then creating
    a separate network to factorize the matrices. So, there's a lot of hope in the
    future that content and collaborative systems can exist in parity.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经熟悉了这个概念，并且意识到单纯基于内容的相似性并不是很可行。但实际上，如果你有合适的情境和设置，它们可以增强你的协同过滤方法。很多研究围绕使用神经网络来自动混合基于内容和协同的系统展开。许多方法使用神经网络从文本描述中创建一些稍微非正式的特征，然后创建一个独立的网络来对矩阵进行因式分解。所以，未来在内容和协同系统并存的希望是非常大的。
- en: 'The following are two papers that are pursuing this approach:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是追求这种方法的两篇论文：
- en: '*Hybrid Collaborative Filtering with Neural Networks*, Florian Strub, Jeremie
    Mary, and Romaric Gaudel, 2016'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用神经网络的混合协同过滤*，Florian Strub，Jeremie Mary，Romaric Gaudel，2016年'
- en: '*Hybrid Recommender System Using Semi-supervised Clustering Based on Gaussian
    Mixture Model*, Cyberworlds (CW), 2016 International Conference, pp. 155-158,
    2016'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*基于高斯混合模型的半监督聚类混合推荐系统*，《Cyberworlds（CW）》2016国际会议论文集，第155-158页，2016年'
- en: Neural networks and deep learning
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络与深度学习
- en: This is a huge topic in machine learning, so we can't cover everything in this
    chapter. If you've never seen a neural network before, they look like a giant
    spider web. The vertices of these spider webs are called neurons, or units, and
    they are based on an old-school linear classifier known as a perceptron. The idea
    is that your vector comes in, computes a dot product with a corresponding weight
    vector of parameters, and then gets a bias value added to it. Then, we transform
    it via an activation function. A perceptron, in general, can be canonically the
    same as logistic regression if you're using a sigmoid transformation.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是机器学习中的一个巨大话题，所以我们无法在这一章中涵盖所有内容。如果你之前从未见过神经网络，它们看起来像一张巨大的蜘蛛网。这些蜘蛛网的顶点被称为神经元或单元，它们基于一种旧式的线性分类器，叫做感知机。其基本思路是，输入你的向量，计算与相应权重向量的点积，然后加上一个偏置值。接着，我们通过激活函数进行转换。一般来说，如果使用sigmoid转换，感知机在本质上可以与逻辑回归是一样的。
- en: 'When you string a whole bunch of these together, what you get is the massive
    web of perceptrons feeding perceptrons: this is called a multi layer perceptron,
    but it''s also known as a neural network. As each of these perceptrons feeds the
    next layer, the neurons end up learning a series of nonlinear transformations
    in the input space, ultimately producing a prediction in the final layer.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当你将一大堆这样的感知机串在一起时，你得到的是一个巨大的感知机网络：这叫做多层感知机，但它也被称为神经网络。当这些感知机逐层传递信息时，神经元最终会在输入空间中学习一系列非线性变换，最终在最后一层产生预测结果。
- en: The history of these models is actually really fascinating. They were first
    proposed in the early 1950s, but their potential was not really unlocked for quite
    a long time, since they're so computationally intensive. Nowadays, though, we
    hear a bout deep learning everywhere, and it's really just referring to the broader
    family of neural networks, including some of their unsupervised and generative
    variants.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型的历史实际上非常有趣。它们最早是在1950年代初提出的，但由于计算开销巨大，其潜力一直没有得到真正的释放。如今，深度学习已经成为一个随处可见的话题，实际上它指的是更广泛的神经网络家族，包括一些无监督和生成的变体。
- en: So, how does a neural network actually learn? Well, we're going to iteratively
    feed the data through layers of the networks in epochs. Feeding the layer forward
    is as simple as computing a matrix product between one layer and the next, adding
    the bias vector along the column axis, and then transforming the output via the
    activation function. There are a lot of different activation functions you can
    use, but some of the most common ones are the sigmoid; the hyperbolic tangent,
    which is similar to the sigmoid but bounds between negative one and one rather
    than zero and one; and **rectified linear units** (**ReLUs**), which really are
    just flooring functions between the value and zero. It makes sure that nothing
    negative comes out of the units. So, after each epoch or iteration, outside the
    output layer we're going to compute the error of the network, and pass the message
    back up through the layers and they can adjust their weights accordingly. This
    process is called backpropagation. We usually use gradient descent for this.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，神经网络究竟是如何学习的呢？我们将通过多次循环将数据通过网络的各层，在每个周期中输入数据。前向传播的过程非常简单，就是计算当前层和下一层之间的矩阵乘积，将偏置向量加到列轴上，然后通过激活函数转换输出。你可以使用许多不同的激活函数，但一些最常见的包括sigmoid函数；双曲正切函数，它类似于sigmoid，但它的输出范围是从负一到一，而不是零到一；还有**修正线性单元**（**ReLUs**），它们实际上只是将值和零之间的最小值作为输出，确保输出值没有负数。所以，在每个周期或迭代之后，在输出层外，我们将计算网络的误差，并通过各层传递误差信息，以便它们相应地调整权重。这个过程叫做反向传播。我们通常使用梯度下降法来实现这一过程。
- en: For our two-layer example, which is really just a single layer in the middle
    with an output layer at the end, we only have to compute two matrix products for
    each epoch. It's been found that how you initialize your weights makes a huge
    difference in the capacity for the network to learn. There are several approaches
    to the strategies for this, but the easiest way is to just initialize them to
    very small values. We typically pick random values between negative and positive
    0.1\. You can go smaller; you can get more clever. We will initialize our biases
    as 1 vectors. Again, there are other clever ways to do this. We're just going
    to use 1, and the weight matrices themselves map one layer to the next. So, going
    from layer 1 to layer 2, we go from three units to four. You can see that dimensionality
    in the number of units. Our corresponding weight matrix is going to be *3 x 4*
    and, likewise, for the second one it's going to be *4 x 2*.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的两层示例，实际上只是中间有一层，最后有一个输出层，我们每个时期只需要计算两个矩阵乘积。已经发现，如何初始化权重对网络学习能力有很大影响。对于这个策略有几种方法，但最简单的方法是将它们初始化为非常小的值。我们通常选择在负0.1到正0.1之间的随机值。你可以选择更小的值，也可以做得更聪明。我们将偏置初始化为1的向量。同样，也有其他更巧妙的方法来实现这一点。我们这里只用1，而权重矩阵本身将一层映射到下一层。所以，从第一层到第二层，我们从三个单元到四个单元。你可以看到在单元数中维度的变化。我们相应的权重矩阵将是*3
    x 4*，同样，第二个权重矩阵将是*4 x 2*。
- en: 'Here, we''re just expressing our network as a system of linear equations:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是将我们的网络表示为一组线性方程：
- en: '![](img/e1a9a508-461c-4283-8b21-ad4998d3eba1.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1a9a508-461c-4283-8b21-ad4998d3eba1.png)'
- en: The first layer is passed to the second layer in that nested parentheses on
    the inside, and then to the last layer on the outer parentheses. And what we end
    up with is this real matrix in *m x 2*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层被传递到第二层，在内部的嵌套括号中，然后传递到外部括号中的最后一层。最终得到的结果是一个*m x 2*的真实矩阵。
- en: 'Here''s a forward pass in a snippet of highly oversimplified Python code:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个非常简化的Python代码片段中的前向传播：
- en: '[PRE26]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We're defining our activation function. `f` is a logistic or sigmoid transformation.
    `lam`, or `lambda`, is going to be our learning rate, which we learned about when
    we talked about gradient descent. And you'll remember this from logistic regression,
    where we can control the rate of how we descend that gradient. After initializing
    `X` and `y`, which we're just using as random values, we create hidden `H1` and
    `H2` layers, and `b1` and `b2` biases. In this example, we created the layers
    using the NumPy `rand` function. But this is where you'd want to get clever and
    bound them between negative `0.1` and `0.1` on the positive scale. Then, the result
    of our hidden layer one, `H1_res`, is computed by applying our `f` activation
    function to the `AX + b` linear equation. So, we just compute the inner product
    between `X` and `H1`, and then add the bias vector along the column vectors.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里定义了激活函数。`f`是一个逻辑或sigmoid变换。`lam`，或称`lambda`，将是我们的学习率，我们在讨论梯度下降时已经了解过它。你还记得在逻辑回归中，我们可以控制下降梯度的速率。在初始化`X`和`y`（我们这里只是用随机值）后，我们创建了隐藏层`H1`和`H2`，以及偏置`b1`和`b2`。在这个例子中，我们使用了NumPy的`rand`函数来创建这些层。但在这里，你应该聪明一点，将它们限制在正负`0.1`之间。然后，隐藏层一的结果`H1_res`通过将我们的`f`激活函数应用于`AX
    + b`线性方程来计算。所以，我们只是计算`X`和`H1`之间的内积，然后沿列向量加上偏置向量。
- en: The output is computed by applying the second hidden layer to the output of
    the first in the same fashion. So, we're chaining these linear systems into one
    another, and applying this nonlinear transformation to that output.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是通过将第二个隐藏层应用于第一个隐藏层的输出来计算的，方式与之前相同。因此，我们将这些线性系统彼此链接，并对该输出应用非线性转换。
- en: 'So, now that we have our first epoch complete, we need to adjust the weights
    of the network to get an error-minimizing state because, right now, the chances
    are our network produced a terrible error. And so, here begins the fun of backpropagation,
    and if you thought we had a lot of calculus earlier in this book, you''re in for
    a treat here. We''re going to compute four derivatives: two for each layer. We
    use them to adjust the weight in the layer immediately above, much like we did
    in logistic regression. Then, the next time we do a forward pass, the weights
    have been adjusted and we''ll, in theory, have less error in the network than
    we did previously.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，现在我们已经完成了第一次训练周期，我们需要调整网络的权重，以达到一个最小化误差的状态，因为目前网络可能产生了一个非常大的误差。因此，反向传播的乐趣就开始了，如果你认为我们之前在本书中涉及了很多微积分，那么接下来你将大快朵颐。我们将计算四个导数：每一层两个。我们用它们来调整立即上一层的权重，就像在逻辑回归中做的那样。然后，在下一次前向传播时，权重已经被调整，理论上，网络的误差会比之前小。
- en: 'Here, we''re implementing backpropagation from scratch:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们从零开始实现反向传播：
- en: '[PRE27]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We''re going to compute four derivatives: the derivative and loss function
    with respect to each of the weights layers—that''s two—and the bias layers—that''s
    another two. The first delta is really easy to compute: it''s simply the predicted
    probabilities, which is this matrix minus the truth indices of `y`. Next, we''re
    going to compute the first layer''s output with the delta we just computed, which
    is going to be a derivative with respect to the last layer, which is the output
    layer. And, after that, we can sum along the columns of our results to get the
    derivative of our second layer biases.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算四个导数：每个权重层关于损失函数的导数——两个；每个偏置层的导数——另两个。第一个delta很容易计算：它仅仅是预测的概率，即这个矩阵减去`y`的真实索引。接下来，我们将使用刚才计算的delta来计算第一层的输出，这将是关于最后一层（即输出层）的导数。之后，我们可以沿着结果的列求和，得到第二层偏置的导数。
- en: 'We can use the same process to compute our derivatives for the next `H1` and
    `b1` layer. Once we have those gradients computed, we can update the weights and
    biases in the same fashion as we did in logistic regression, which is by multiplying
    each derivative by the negative learning rate, and adding that to the weights
    matrices and `H1` and `b1`, and `H2` and `b2` bias vectors, respectively. And
    now we''ve updated our weights and biases along the axis of greatest change in
    our function: the loss function.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的过程来计算下一个`H1`和`b1`层的导数。一旦我们计算出这些梯度，就可以像在逻辑回归中那样更新权重和偏置，即将每个导数乘以负学习率，并将结果加到权重矩阵和`H1`、`b1`、`H2`、`b2`偏置向量中。现在，我们已经沿着损失函数的最大变化轴更新了权重和偏置。
- en: 'So, if you backpropagate correctly, you''re going to get error terms that converge
    similarly to the following graph:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果你正确地进行反向传播，你将得到类似于以下图表中误差项的收敛结果：
- en: '![](img/b327eaec-85b2-43ee-9655-f34f1aef814b.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b327eaec-85b2-43ee-9655-f34f1aef814b.png)'
- en: Tips and tricks for training a neural network
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络的技巧和窍门
- en: Here are some tricks that can make your life easier when you're actually training
    a neural network from scratch. You can stop your training a bit early to avoid
    overfitting. In the preceding graph, you can see there's a long tail where the
    error does not decrease anymore and we're still training. It's at a point around
    epoch 25 or 30\. We could have stopped early.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些技巧，当你从零开始训练神经网络时，这些技巧可以让你的工作更轻松。你可以提前停止训练，以避免过拟合。在前面的图表中，你可以看到在误差不再下降而我们仍在训练的地方有一条长尾。大约是在第25或30轮时。我们本可以提前停止训练。
- en: Regularization and dropout are ways that can prevent your network from overfitting.
    Now, for extremely large data, you can do partial fits per epoch, meaning that
    you can fit many batches through your network for each forward pass so that you
    don't have to hold everything in memory. It also makes backpropagation a little
    easier, and different activation functions are going to give you different results.
    So, always try them out. And, finally, always use cross-validation, as we've talked
    about before, to select your model hyperparameters, so that you don't inadvertently
    create model leakage with the validation set, or even with overfitting your training
    set.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化和Dropout是防止网络过拟合的方式。对于极大的数据集，你可以进行每轮的部分拟合，这意味着你可以在每次前向传播时通过网络拟合多个批次，以便不必将所有数据都加载到内存中。这也使得反向传播变得更容易，而且不同的激活函数会给你不同的结果。所以，始终尝试它们。最后，像之前所讲的，始终使用交叉验证来选择你的模型超参数，以避免无意间造成验证集泄漏，或甚至训练集过拟合的情况。
- en: Neural networks
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络
- en: We're going to iteratively feed the data through layers in the network in epochs.
    After each iteration, we're going to compute the error of the network and the
    output, and pass the signal back up through the layers so they can adjust their
    weights accordingly. So, that's all for the theory and recaps.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过网络中的各层，按轮次（epoch）逐步输入数据。每次迭代后，我们将计算网络的误差和输出，并将信号反向传递通过各层，以便它们可以相应地调整权重。所以，以上就是理论部分和总结。
- en: 'We have two files we''re going to look at. We have the source code and an example: `base.py`
    and `mlp.py`, which stands for multilayer perceptron. Let''s start with `base.py`:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个文件要查看。我们有源代码和一个示例：`base.py`和`mlp.py`，它代表多层感知器。我们先从`base.py`开始：
- en: '[PRE28]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We have two functions. One function, `tanh`, is a hyperbolic tangent function
    we're going to use as our activation function. And this is just a wrapper for
    `np.tanh`. Then, we have a `NeuralMixin` class, which is kind of an abstract interface
    we're going to use for exporting the weights and biases of each of our networks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个函数。一个函数，`tanh`，是我们将用作激活函数的双曲正切函数。它实际上是`np.tanh`的一个封装。然后，我们有一个`NeuralMixin`类，它是我们用来导出每个网络的权重和偏置的抽象接口。
- en: In `mlp.py`, we're going to depend on the typical `check_X_y` from scikit-learn,
    `check_classification_targets`. Because we're only performing either binary or
    multiclass classification, we're going to use softmax, and then `check_random_state`.
    So, we can use a replicable `random_state` inside of our neural network.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在`mlp.py`中，我们将依赖于scikit-learn的典型`check_X_y`、`check_classification_targets`。由于我们只进行二分类或多分类任务，我们将使用softmax，然后是`check_random_state`。这样，我们就可以在神经网络中使用可复现的`random_state`。
- en: 'There is a function outside of the class itself—`calculate_loss`:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在类本身之外，有一个函数——`calculate_loss`：
- en: '[PRE29]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Essentially, this is going to be our objective function inside of our neural
    network that we can compute, and backpropagate that loss up through the network.
    Softmax is going to be the generalization, that is, our logistic function applied
    to multiple classes. So, that's what we get out of this. From the `K` matrix,
    where `K` is the dimension of the number of classes, we have a three-class problem;
    we can compute probabilities for the membership of each of those classes. And
    that's what softmax does.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，这是我们神经网络中的目标函数，我们可以计算它，并将损失反向传播到网络中。Softmax是广义化的，也就是我们将逻辑函数应用于多个类别。所以，这就是我们从中得到的。从`K`矩阵中，`K`是类别数的维度，我们有一个三类问题；我们可以计算每个类别的归属概率。这就是softmax的作用。
- en: 'Now our neural net classifier is going to take a number of different parameters,
    as shown here:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的神经网络分类器将接受多个不同的参数，如下所示：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As usual, we have our `X` and `y`, and then we have `hidden`, which is going
    to be a tuple or some other iterable that has positional elements indicating the
    number of units in each layer. So, if we wanted to have two layers, we might have
    `X`, `25`, where each layer would have `25` units. There is no exact science to
    determining how many units you want and it kind of depends on your objective.
    If you want to compress the dimensionality, you might make the number of units
    smaller than the input dimensionality. If you want to discover all sorts of nuanced
    features, then you might expand the number of units. The number of iterations
    is actually the number of epochs we're going to perform. The learning rate is
    the lambda that we've seen in logistic regression. Regularization is our `l2`
    penalty that's going to help us prevent overfitting. And `random_state`, again,
    is the seed that we'll use to control `random_state` so this is replicable.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们有`X`和`y`，然后我们有`hidden`，它将是一个元组或其他可迭代对象，包含表示每一层单元数量的位置信息。所以，如果我们想要两个层，我们可能会有`X`、`25`，其中每一层都会有`25`个单元。确定你想要多少单元并没有确切的科学依据，这取决于你的目标。如果你想压缩维度，你可能会将单元的数量设置得比输入维度小。如果你想发现各种细致的特征，那么你可能会增加单元的数量。迭代次数实际上就是我们要执行的epoch的数量。学习率是我们在逻辑回归中看到的lambda。正则化是我们的`l2`惩罚，它将帮助我们防止过拟合。而`random_state`，再次是我们用来控制`random_state`的种子，这样就可以保证可重复性。
- en: 'In the constructor, all we''re doing is self-assigning different attributes
    to the algorithm:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们所做的只是将不同的属性自我分配给算法：
- en: '[PRE31]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Then, we initialize the weights and biases. We're tracking the last dimension
    of the last matrix, or hidden weight matrix. So, we will start the input with
    `none`. We're going to use the column dimensionality as the input dimensionality
    of the next layer. So, we mentioned in the example that we went from three to
    four. Our dimensionality of the first hidden matrix or hidden layer may be *3
    x 4*. We're tracking the last column dimensionality because that becomes the row
    dimensionality of the next layer. We return to `X`, `y`, `weights`, `biases`,
    and this will be used by subclasses later, as well, which is why it's a class
    function.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化权重和偏置。我们正在追踪最后一个矩阵的最后一个维度，或者说是隐藏权重矩阵。所以，我们将输入设置为`none`。我们将使用列的维度作为下一层的输入维度。所以，在例子中我们提到我们从三维到了四维。我们第一个隐藏矩阵或隐藏层的维度可能是*3
    x 4*。我们追踪最后一列的维度，因为它将成为下一层的行维度。我们返回到`X`、`y`、`weights`、`biases`，这些稍后会被子类使用，这也是为什么它是一个类函数的原因。
- en: 'Now we start progressing through forward passes of our network. First, we compute
    the forward step:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始推进网络的前向传播。首先，我们计算前向步骤：
- en: '[PRE32]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: A forward step is pretty easy. We have `X`, our weights, and our biases. We're
    going to ZIP our weights and biases together so we can track them together. And
    we're just going to compute that product of `X.dot(w)`, `w` being weight, and
    add biases. This is again that `AX` linear system plus `b`. Then, we apply this
    nonlinear transformation, `tanh`. But if you wanted to use sigmoid, you could
    do that. The last layer is slightly different. We're not running `tanh` on the
    last layer, we're actually running softmax. This is a classification problem,
    so we apply softmax to the output of `X` as opposed to `tanh`. And that's the
    output layer.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 前向步骤很简单。我们有`X`、我们的权重和偏置。我们将把权重和偏置一起ZIP，以便一起追踪。然后我们将计算`X.dot(w)`的乘积，`w`是权重，并加上偏置。这再次是那个`AX`线性系统加`b`。接着，我们应用这个非线性变换`tanh`。但是，如果你想使用sigmoid，你也可以这么做。最后一层略有不同。我们在最后一层并没有使用`tanh`，而是使用了softmax。因为这是一个分类问题，所以我们将softmax应用于`X`的输出，而不是`tanh`。这就是输出层。
- en: 'In the constructor, we''ve computed the first forward step and our first epoch:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在构造函数中，我们计算了第一次前向步骤和第一次迭代：
- en: '[PRE33]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now we want to calculate the loss; the loss is just that log loss that we saw
    previously. We're going to track loss per epoch here in `train_loss`. If you want
    to speed this up, you might only calculate the loss, say, every five iterations.
    In the following backpropagation example, we will get a clever idea regarding
    how we implement these gradients in a fashion that's a bit more extensible than
    the two-layer example from the last one.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想计算损失；损失就是我们之前看到的对数损失。我们将在`train_loss`中跟踪每个epoch的损失。如果你想加速这一过程，你可以选择每五次迭代计算一次损失。在接下来的反向传播示例中，我们将得到一个巧妙的想法，关于如何以比上一个例子中的两层模型更具可扩展性的方式实现这些梯度。
- en: 'Now, in the backpropagation function, we compute delta again, which is the
    probabilities of each of the classes minus the truth indices:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在反向传播函数中，我们再次计算delta，即每个类别的概率减去真实索引：
- en: '[PRE34]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: That's our first delta. And now, iteratively, what we're going to do is compute
    the derivative as that layer's result times the current delta. We start out with
    the current delta of these probabilities that we just subtracted from. So, now
    that we've got our gradient, we can compute the derivative of our biases by summing
    over the columns in the derivative. Now we have the derivative of the biases,
    and we're going to compute the next delta for the next time we iterate through
    this. The way we use regularization is by multiplying the regularization by `next_weights`.
    So, `next_weights` is the weight's matrix that we will compute the gradient against.
    We regularize it and add that to the derivative, and then we are going to adjust
    the weights. So, we can add `learning_rate` times the delta, or the gradient,
    and we do the same for our biases. We've changed `next_weights` and `next_biases` inside
    of weights and biases. This is a `void` function. It doesn't return anything because
    it all happened in place.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的第一个delta。现在，我们要做的是通过将该层的结果与当前的delta相乘来迭代计算导数。我们从刚刚减去的概率的当前delta开始。所以，现在我们得到了梯度，我们可以通过对导数中的列求和来计算偏差的导数。现在我们有了偏差的导数，并且我们将计算下一个delta，为下一次迭代做准备。我们使用正则化的方法是通过将正则化项与`next_weights`相乘。所以，`next_weights`是我们将要对其计算梯度的权重矩阵。我们对它进行正则化，并将其加到导数上，然后调整权重。因此，我们可以加上`learning_rate`乘以delta，或者梯度，对偏差做同样的操作。我们已经在权重和偏差中更改了`next_weights`和`next_biases`。这是一个`void`函数。它不返回任何东西，因为一切都在原地完成了。
- en: 'Now, weights and biases have been iteratively updated. And the next time we
    progress through the iteration – the next epoch—we should see a lower error. As
    such, we''ll continue this through the number of iterations, progress through
    all of our epochs, and save our weights and biases. Then, we will produce a prediction
    and compute those probabilities by doing a forward pass with the softmax at the
    very end. Take the `argmax` of the column: that''s the class that has the highest
    probability. And that''s what we return in a squashed vector.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，权重和偏差已经被反复更新。下次我们执行迭代时——即下一轮epoch——我们应该会看到更低的误差。因此，我们会在多个迭代中继续这一过程，完成所有的epochs，并保存我们的权重和偏差。接下来，我们将生成一个预测，并通过执行一次前向传播，最终使用softmax计算这些概率。取该列的`argmax`值：这就是具有最高概率的类别。我们将在一个压缩的向量中返回这个结果。
- en: 'In the `example_mlp_classifier` file, we use a similar dataset to what we use
    in decision tree classification, which are these `multivariate_normal` bubbles
    that are kind of clusters, in our two-dimensional space. We''ll do `train_test_split`
    as usual:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在`example_mlp_classifier`文件中，我们使用了与决策树分类中类似的数据集，这些是我们二维空间中的`multivariate_normal`簇状数据。我们将像往常一样执行`train_test_split`：
- en: '[PRE35]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: And now we're going to train two neural networks. The first one is only going
    to use four iterations and have a single hidden layer of 10 units. The second
    one is a little more complex. We're going to do 150 iterations with two hidden
    layers of `25` units each.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将训练两个神经网络。第一个神经网络只会使用四次迭代，并且有一个包含10个单元的单隐藏层。第二个神经网络稍微复杂一点，我们将进行150次迭代，包含两个各有`25`个单元的隐藏层。
- en: 'So, we run the `example_mlp_classifier.py` file:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们运行`example_mlp_classifier.py`文件：
- en: '![](img/ddf90324-3d58-4c30-8efb-ca9745dcad6e.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ddf90324-3d58-4c30-8efb-ca9745dcad6e.png)'
- en: 'We got a pretty good test accuracy with a single hidden layer of 10 units:
    94.4 percent. But you can see that we almost get 100 percent if we have two hidden
    layers at 25 each. We also have the training iterations for the first one.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 使用10个单元的单隐藏层，我们得到了一个相当不错的测试准确率：94.4%。但是可以看到，如果我们使用两个25单元的隐藏层，几乎可以达到100%的准确率。我们还得到第一个网络的训练迭代次数。
- en: 'You can see in the following graph how the loss kind of jitters around a bit:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图表中看到损失值有些波动：
- en: '![](img/64669de4-b82b-4bfd-a530-913e36491b02.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/64669de4-b82b-4bfd-a530-913e36491b02.png)'
- en: But over time, that loss decreases. It's not guaranteed to be a perfect drop
    and it might jump up or drop down a bit, but we can see that, over time, our loss
    hits a point where it's very small. This function that we've learned here in the
    more complex one is a really interesting nonlinear decision boundary. It has a
    little bit of trouble classifying these border points, but this is how we can
    use a neural network to learn a function that's much more complex than something
    that a logistic regression can learn.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着时间的推移，损失值会逐渐减小。虽然不能保证损失值会完美下降，它可能会有小幅的上升或下降，但我们可以看到，随着时间的推移，损失值最终会变得非常小。我们在更复杂的模型中学到的这个函数是一个非常有趣的非线性决策边界。它在对这些边界点进行分类时有些困难，但这就是我们如何使用神经网络来学习一个远比逻辑回归能学到的更复杂的函数。
- en: Using transfer learning
  id: totrans-213
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习
- en: In this section, we're going to take it one step further and explore the question
    of whether a neural network could learn from other neural networks and what they've
    already learned. We'll start by covering the concept of transfer learning, and
    then we'll get into some Python code.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将进一步探讨一个问题：神经网络是否能够从其他神经网络中学习，以及它们已经学到的内容。我们将首先介绍迁移学习的概念，然后进入一些 Python
    代码。
- en: 'Transfer learning is essentially the Frankenstein''s monster of machine learning.
    The idea arose from this question: how can I take what some other network has
    already learned and go from there? We''re basically going to do a brain splice
    between several different networks. This can be extremely valuable in cases where
    a network is trained on data that you don''t have access to or the training process
    is the one that would have taken hours or days, as is commonly the case in text
    or image processing domains.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习本质上是机器学习中的“弗兰肯斯坦怪物”。这个想法来源于这样一个问题：我如何能够利用其他网络已经学到的东西，从那里继续学习？我们基本上是将几个不同的网络进行“大脑拼接”。在一些情况下，这非常有价值，尤其是当一个网络是在你无法访问的数据上进行训练，或者训练过程本身需要几个小时甚至几天的时间时，比如文本或图像处理领域。
- en: We don't want to retrain our model because it would take forever, but we want
    to take what we've already learned about the other two classes and start learning
    something else about the other class. Rather than retrain the whole thing, we
    can just use transfer learning to pick up where we left off. So, now that you
    have the idea and the concept behind it, let's look at how that's going to be
    applied to the existing multilayer perceptron framework that we're now familiar
    with.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想重新训练我们的模型，因为那样会花费很长时间，但我们希望利用已经学习到的其他两个类别的知识，继续学习关于另一个类别的内容。与其重新训练整个模型，我们可以直接使用迁移学习从之前的进展继续。所以，既然你已经理解了背后的概念和原理，我们来看一下如何将它应用到我们现在熟悉的现有多层感知机框架中。
- en: 'In the `transfer.py` file, starting with `TransferLearningClassifier`, there''s
    one more argument than there was in `MLPClassifier`: and that''s the pretrained
    network. That can either be `NeuralNetClassifier` or `TransferLearningClassifier`. But
    we''re just going to take `NeuralNetClassifier` for this example. Similar to the
    MLP constructor, we''re going to spend the first few lines saving everything as
    self attributes, and then we''re going to make sure that whatever you''ve passed
    in as the pretrained network is going to be some form of `NeuralMixin`:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `transfer.py` 文件中，从 `TransferLearningClassifier` 开始，相较于 `MLPClassifier`，它多了一个参数：预训练网络。这个预训练网络可以是
    `NeuralNetClassifier` 或者 `TransferLearningClassifier`。但在这个示例中，我们只会选择 `NeuralNetClassifier`。和
    MLP 构造函数类似，我们将用几行代码将一切保存为自定义属性，然后确保你传入的预训练网络是某种形式的 `NeuralMixin`。
- en: '[PRE36]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Because we have to have access to the weights and the biases from the previous
    classes, we get the pretrained weights and the pretrained biases. We only want
    to initialize the new weights and biases that we can kind of stack on to the end.
    So, if we have a network of four layers before, those are just going to be ancillary.
    We're not going to train those—we're just going to freeze them. Then, we want
    to stack a few layers on the end that we can train and teach new features—new
    characteristics about the new classes we may want to predict. We're going to do
    the initialized weights and biases only for the new weights and biases.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们必须访问之前类的权重和偏差，所以我们获取了预训练的权重和偏差。我们只需要初始化新的权重和偏差，这些新权重和偏差可以附加到网络的末尾。因此，如果我们之前有一个四层的网络，那些层只是辅助层。我们不会训练它们——我们只会冻结它们。然后，我们希望在末尾堆叠一些可以训练的新层，用来学习新的特征——关于我们可能想要预测的新类别的特征。我们只会为新的权重和偏差初始化权重和偏差。
- en: Epochs look slightly different; they look a lot like MLPs, but there's a little
    bit of difference.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Epochs稍微有些不同；它们看起来很像MLP，但也有一点不同。
- en: So, for each epoch, we're going to perform one pretrained forward step. Basically,
    all we're going to do here is that for each of those layers in the pretrained
    weights and biases, we're going to compute *AX + b* with our `tanh` function on
    it. Notice that even on the output layer, rather than compute a softmax, we're
    going to compute `tanh`, because we're not interested in getting those class probabilities
    anymore. Now we just want to pipe it into the next layer. So, we're going to use
    whatever that activation function is. It could be `sigmoid` or `relu`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在每一个epoch中，我们将执行一次预训练的前向传播步骤。基本上，我们在这里做的就是，对于预训练的每一层权重和偏差，我们将使用`tanh`函数计算*AX
    + b*。注意，即使在输出层，我们也不会计算softmax，而是计算`tanh`，因为我们不再关心获得类别的概率了。现在，我们只希望将其传递到下一层。因此，我们将使用任何激活函数，可能是`sigmoid`或`relu`。
- en: 'Now we want to take a forward step on the existing or the new weights and bias
    layers that we do want to train:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们希望在现有的或我们希望训练的新的权重和偏差层上进行前向步骤：
- en: '[PRE37]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We're going to calculate `loss`, and then we're going to backpropagate only
    on the new layers. So, we're not training the old weights and biases at all, but
    we are doing that to the new ones.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将计算`loss`，然后只在新的层上进行反向传播。因此，我们完全不会训练旧的权重和偏差，但会对新的权重和偏差进行训练。
- en: 'The predictions are slightly different:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 预测稍有不同：
- en: '[PRE38]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Rather than just compute that single forward step, we're going to compute the
    pretrained forward step, again, because we don't want that softmax in the end
    of the other network. Then, we will compute the normal forward step with the output
    of the pretrained forward step, which will stack the softmax onto the end.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会仅仅计算单一的前向步骤，而是再次计算预训练的前向步骤，因为我们不希望其他网络的末端有softmax。然后，我们将使用预训练前向步骤的输出计算常规的前向步骤，这会将softmax添加到末尾。
- en: For the predictions, again, we're taking `argmax` of the columns. That is, getting
    the highest probability class from the predict probabilities.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测，我们再次对列使用`argmax`。也就是说，从预测的概率中获取最高的概率类别。
- en: 'Let''s look at an example file. This is going to look a lot like what we set
    up in our previous MLP example, except we have two datasets:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个示例文件。这看起来与我们在之前的MLP示例中设置的非常相似，唯一不同的是我们有两个数据集：
- en: '[PRE39]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The first one is going to have those two blobs: the `multivariate_normal` blobs
    that we''ve been using and the majority class. The third here is going to stack
    this third class in between the two. Our transfer learning task is going to be
    learning this new class based on what it''s already learned from the binary classification
    example.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个将具有两个部分：我们一直在使用的`multivariate_normal`部分和主要类别。第三个将把第三个类别堆叠在两个类别之间。我们的迁移学习任务就是基于它已经从二元分类示例中学到的知识来学习这个新类别。
- en: 'Let''s fit the first neural network that we''ll use, which is our pretrained
    network:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练第一个神经网络，即我们预训练的网络：
- en: '[PRE40]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This is going to be very similar to what we saw in the first example, where
    we have a two-layer network with `25` units in each layer. We're going to fit
    `75` epochs with a pretty low learning rate and we'll see how it does on learning
    the binary classification task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这将与我们在第一个示例中看到的非常相似，那里我们有一个两层网络，每层有`25`个单元。我们将进行`75`次训练，并使用一个相当低的学习率，我们将看看它在学习二元分类任务时的表现。
- en: 'Now, let''s say we''re predicting some type of disease, and there''s type one
    something and type two something. I''m not going to use diabetes because there''s
    only two types. But let''s say, a third type comes out. Maybe it''s a type of
    Zika virus, and we want to predict whether this new class is present in a patient
    who comes in. We don''t want to retrain everything, because it''s going to take
    forever, perhaps. So, we''re going to just stack this new layer on the end that
    says learn these new features about this third class. And then we''ll produce
    a new output layer for three classes rather than two. We''re only going to do
    `25` new epochs, just based on what we''ve already learned from the previous binary
    classification task. We want to see if we can learn this new class without retraining
    everything. And that''s all we''re going to do here:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在预测某种疾病，并且有第一型和第二型的分类。我不会用糖尿病，因为只有两种类型。但假设出现了第三种类型，可能是某种寨卡病毒，我们想预测这种新类型是否出现在前来就诊的患者身上。我们不想重新训练所有内容，因为这可能需要很长时间。因此，我们只会在末尾加上这一新层，学习关于第三类的新特征。然后我们将生成一个新的输出层，以适应三个类别而不是两个类别。我们只需要进行`25`个新的周期，基于我们已经从之前的二分类任务中学到的内容。我们想看看能否在不重新训练所有内容的情况下学习这个新类别。这就是我们在这里要做的所有事情：
- en: '[PRE41]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: And then we're going to plot both out so you can see the decision boundary from
    both the binary and this three-class classification problem.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将把两者都绘制出来，这样你可以看到来自二分类和这个三分类问题的决策边界。
- en: 'Let''s run an example of transfer learning:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行一个迁移学习的示例：
- en: '![](img/e8008426-04ac-43df-aef3-d7cc146fa30d.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8008426-04ac-43df-aef3-d7cc146fa30d.png)'
- en: Our test accuracy is down to `95.2` percent.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试准确率降至`95.2`百分比。
- en: 'You can see in the following graph that we are able to learn a complex decision
    boundary in the binary classification task:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图表中看到，我们能够在二分类任务中学习到一个复杂的决策边界：
- en: '![](img/c7aee88f-511e-437d-af4c-8807b5852299.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7aee88f-511e-437d-af4c-8807b5852299.png)'
- en: And then we took that and we said let's do transfer learning with a new class,
    and we were still able to learn it really well. So, now we've learned the second
    decision boundary that we built on top of our initial decision boundary and it
    looks really good. So, we get 95.2 percent accuracy.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们就用这个方法说，来做迁移学习，添加一个新类别，我们仍然能够很好地学习它。所以，现在我们已经学会了第二个决策边界，它是在我们最初的决策边界上构建的，看起来非常好。因此，我们获得了95.2%的准确率。
- en: Summary
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: Transfer learning is a flexible concept that'll allow you to stack networks
    together to accomplish far more complex tasks than you thought possible. We covered
    recommender systems and collaborative filtering in particular, and then we looked
    at matrix factorization techniques and how to supplement your recommenders with
    content-based similarities. Lastly, we worked with neural networks and transfer
    learning.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是一个灵活的概念，它允许你将网络堆叠在一起，以完成比你想象的更复杂的任务。我们特别讨论了推荐系统和协同过滤，然后我们研究了矩阵分解技术，以及如何通过基于内容的相似度来补充你的推荐系统。最后，我们探讨了神经网络和迁移学习。
