- en: '*Chapter 16*: Bringing Models into Production with MLOps'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第16章*：使用 MLOps 将模型投入生产'
- en: In the previous chapter, we looked into model interoperability using ONNX, hardware
    optimization using FPGAs, and the integration of trained models into other services
    and platforms. So far, you have learned how to implement each step in an end-to-end
    machine learning pipeline with data cleansing, preprocessing, labeling, experimentation,
    model training, optimization, and deployment. In this chapter, we will connect
    the bits and pieces from all the previous chapters to integrate and automate them
    in a build and release pipeline. We will reuse all these concepts to build a version-controlled,
    reproducible, automated ML training and deployment process as a **continuous integration
    and continuous deployment** (**CI/CD**) pipeline in Azure. In analogy to the **DevOps**
    methodology in software development, we will refer to this topic as **MLOps**
    in ML.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了使用 ONNX 进行模型互操作性、使用 FPGA 进行硬件优化以及将训练模型集成到其他服务和平台中的方法。到目前为止，你已经学习了如何在一个端到端机器学习管道中实现每个步骤，包括数据清洗、预处理、标记、实验、模型训练、优化和部署。在本章中，我们将把之前所有章节中的片段连接起来，在构建和发布管道中集成和自动化它们。我们将重用所有这些概念，在
    Azure 中构建一个版本控制的、可重复的、自动化的 ML 训练和部署过程，作为一个 **持续集成和持续部署**（**CI/CD**）管道。类似于软件开发中的
    **DevOps** 方法论，我们将把这个主题称为 **MLOps**。
- en: First, we will take a look at how to produce reproducible builds, environments,
    and deployments for ML projects. We will cover version control for code, as well
    as the versioning/snapshotting of data and building artifacts.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将看看如何为 ML 项目生成可重复的构建、环境和部署。我们将涵盖代码的版本控制，以及数据和构建工件版本化/快照。
- en: Next, we will learn how to automatically test our code and validate our code
    quality with a focus on ML projects. To do this, we will see how unit, integration,
    and end-to-end tests can be adapted for ensuring good quality of training data
    and ML models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何自动测试我们的代码，并重点关注 ML 项目来验证代码质量。为此，我们将了解如何将单元测试、集成测试和端到端测试适应以确保训练数据和
    ML 模型的良好质量。
- en: Finally, you will build your own MLOps pipeline. First, you will learn how to
    set up Azure DevOps as your orchestration and coordination layer for MLOps, and
    then you will implement a build (CI) and release (CD) pipeline.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将构建自己的 MLOps 管道。首先，你将学习如何设置 Azure DevOps 作为 MLOps 的编排和协调层，然后你将实现构建（CI）和发布（CD）管道。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Ensuring reproducible builds and deployments
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保可重复构建和部署
- en: Validating the code, data, and models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证代码、数据和模型
- en: Building an end-to-end MLOps pipeline
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建端到端 MLOps 管道
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'In this chapter, we will use the following Python libraries and versions to
    create MLOps pipelines in Azure DevOps:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下 Python 库和版本在 Azure DevOps 中创建 MLOps 管道：
- en: '`azureml-core 1.34.0`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-core 1.34.0`'
- en: '`azureml-sdk` `1.34.0`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`azureml-sdk` `1.34.0`'
- en: '`pandas 1.3.3`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas 1.3.3`'
- en: '`tensorflow 2.6.0`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow 2.6.0`'
- en: '`pytest 7.1.1`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytest 7.1.1`'
- en: '`pytest-cov 3.0.0`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytest-cov 3.0.0`'
- en: '`mock 4.0.3`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mock 4.0.3`'
- en: '`tox 3.24.5`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tox 3.24.5`'
- en: Most of the scripts and pipelines discussed in this chapter need to be scheduled
    to execute in Azure DevOps.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的大多数脚本和管道需要在 Azure DevOps 中进行调度执行。
- en: 'All code examples in this chapter can be found in the GitHub repository for
    this book: [https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter16](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter16).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中所有的代码示例都可以在本书的 GitHub 仓库中找到：[https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter16](https://github.com/PacktPublishing/Mastering-Azure-Machine-Learning-Second-Edition/tree/main/chapter16).
- en: Ensuring reproducible builds and deployments
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保可重复构建和部署
- en: DevOps has many different meanings but is usually about enabling rapid and high-quality
    deployments when the source code changes. One way of achieving high-quality operational
    code is by guaranteeing reproducible and predictable builds. While it seems obvious
    that the compiled binary will look and behave similarly for application development
    with only a few minor configuration changes, the same is not true for the development
    of ML pipelines.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps 有许多不同的含义，但通常是指在源代码更改时实现快速和高品质的部署。实现高质量操作代码的一种方法是通过保证可重复和可预测的构建。虽然对于只有少量配置更改的应用程序开发来说，编译的二进制文件看起来和行为相似似乎是显而易见的，但对于
    ML 管道的开发来说，情况并非如此。
- en: 'ML engineers and data scientists face many problems that make building reproducible
    deployments very difficult:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工程师和数据科学家面临着许多问题，这使得构建可重复部署变得非常困难：
- en: The development process is often performed in notebooks and so it is not always
    linear.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发过程通常在笔记本中执行，因此它不总是线性的。
- en: Refactoring notebook code often breaks older notebooks.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重构笔记本代码经常会破坏旧的笔记本。
- en: There are mismatching library versions and drivers.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在库版本和驱动程序不匹配的问题。
- en: Source data can be changed or modified.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源数据可能会被更改或修改。
- en: Non-deterministic optimization techniques can lead to completely different outputs.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非确定性优化技术可能导致完全不同的输出。
- en: We discussed interactive notebooks (such as Jupyter, Databricks, Zeppelin, and
    Azure notebooks) in the first few chapters of this book, and you have probably
    seen them in a lot of places when implementing ML models and data pipelines. While
    interactive notebooks have the great advantage of executing cells to validate
    blocks of models iteratively, they also often encourage a user to run cells in
    a non-linear order. The main benefit of using a notebook environment becomes a
    pain when trying to productionize or automate a pipeline.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的前几章我们讨论了交互式笔记本（例如 Jupyter、Databricks、Zeppelin 和 Azure 笔记本），你可能在实现机器学习模型和数据管道时已经见过它们。虽然交互式笔记本有执行单元格以迭代验证模型块块的优势，但它们也常常鼓励用户以非线性顺序运行单元格。当尝试将管道投入生产或自动化时，使用笔记本环境的主要好处变成了痛点。
- en: The second issue that is common in ML is ensuring that the correct drivers,
    libraries, and runtimes are installed. While it is easy to run a small linear
    model based on scikit-learn in Python 2, it makes a big difference for deep learning
    models if the deployed CUDA, cuDNN, libgpu, Open MPI, Horovod, TensorFlow, PyTorch,
    and similar libraries match the versions from development. Containerization via
    Docker or similar technologies helps to build reproducible environments, but it's
    not straightforward to use them throughout the experimentation, training, optimization,
    and deployment processes.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的第二个常见问题是确保安装了正确的驱动程序、库和运行时。虽然使用 Python 2 运行基于 scikit-learn 的小型线性模型很容易，但如果部署的
    CUDA、cuDNN、libgpu、Open MPI、Horovod、TensorFlow、PyTorch 和类似库与开发版本匹配，对深度学习模型来说就大不相同了。通过
    Docker 或类似技术进行容器化有助于构建可重复的环境，但在实验、训练、优化和部署过程中使用它们并不简单。
- en: Another challenge faced by data scientists is that often data changes over time.
    Either a new batch of data is added during development or data is cleaned, written
    back to the storage, and reused as input for other experiments. Data, due to its
    variability in format, scale, and quality, can be one of the biggest issues when
    producing reproducible models. Versioning data similar to version-controlling
    code is essential, not only for reproducible builds but also for auditing purposes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家面临的另一个挑战是数据通常会随时间变化。在开发过程中，可能添加了新的数据批次，或者数据被清理、写回存储，并作为其他实验的输入重新使用。由于数据在格式、规模和质量上的可变性，它可能是生产可重复模型时遇到的最大问题之一。与代码版本控制类似，对数据进行版本控制对于可重复构建和审计目的都是必不可少的。
- en: One more challenge that makes reproducible ML builds difficult is that they
    often contain an optimization step, as discussed in [*Chapter 11*](B17928_11_ePub.xhtml#_idTextAnchor178),
    *Hyperparameter Tuning and Automated Machine Learning*. While optimization is
    an essential step for ML (for example, for model selection, training, hyperparameter
    tuning, or stacking), it can add non-deterministic behavior to the training process.
    Let's find out how we can fight these problems step by step.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个使可重复的机器学习构建变得困难的挑战是，它们通常包含一个优化步骤，如第 [*第 11 章*](B17928_11_ePub.xhtml#_idTextAnchor178)
    中讨论的，*超参数调整和自动化机器学习*。虽然优化是机器学习（例如，用于模型选择、训练、超参数调整或堆叠）的必要步骤，但它可能会给训练过程添加非确定性行为。让我们一步一步地找出如何解决这些问题。
- en: Version-controlling your code
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 代码版本控制
- en: Version-controlling source code is a best practice, not only for software development
    but also for data engineering, data science, and machine learning As an organization,
    you have the option to set up your own internal source code repository or use
    an external service. **GitHub**, **GitLab**, **Bitbucket**, and **Azure DevOps**
    are popular services for managing source control repositories. The benefit of
    these services is that some of them offer additional features, such as support
    for CI workers and workflows. We will use the CI runner integration of Azure DevOps
    later in this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制源代码是一种最佳实践，不仅适用于软件开发，也适用于数据工程、数据科学和机器学习。作为一个组织，您可以选择建立自己的内部源代码仓库或使用外部服务。**GitHub**、**GitLab**、**Bitbucket**和**Azure
    DevOps**是管理源代码仓库的流行服务。这些服务的优势在于它们中的一些提供了额外的功能，例如对CI工作流的支持。在本章的后面，我们将使用Azure DevOps的CI运行器集成。
- en: Using version control for your code is more important than the version control
    system you use. Yes, **Git** works pretty well, but so does **Mercurial** and
    **Subversion** (**SVN**). For our example MLOps pipeline, we will use Git as it
    is the most widely used and supported. It's essential that you make yourself familiar
    with the basic workflows of the version control system that you choose. You should
    be able to create commits and branches, submit **pull requests** (**PRs**), comment
    on and review requests, and merge changes.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用版本控制对您的代码来说比您使用的版本控制系统更重要。是的，**Git**工作得相当好，但**Mercurial**和**Subversion**（**SVN**）也是如此。对于我们的示例MLOps管道，我们将使用Git，因为它是最广泛使用和支持的。您必须熟悉您选择的版本控制系统的基本工作流程。您应该能够创建提交和分支，提交**pull
    requests**（**PRs**），对请求进行评论和审查，以及合并更改。
- en: The power of version-controlling source code is to document changes. On each
    such change, we want to trigger an automatic pipeline that tests your changes,
    validates the code quality, and when successful and merged, trains your model
    and automatically deploys it to staging or production. Your commit and PR history
    will not only become a source of documenting changes but also triggering, running,
    and documenting whether these changes were tested and ready for production.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制源代码的力量在于记录更改。在每个这样的更改上，我们希望触发一个自动的管道来测试您的更改，验证代码质量，并在成功合并后训练模型并自动将其部署到预发布或生产环境。您的提交和PR历史记录不仅将成为记录更改的来源，还将触发、运行和记录这些更改是否经过测试并准备好投入生产。
- en: In order to work effectively with version control, it is essential that you
    try to move business logic out of your interactive notebooks as soon as possible.
    Notebooks store the code and output of each cell in custom data formats – for
    example, serialized to JSON files. This makes it very difficult to review changes
    in the serialized notebook. A good trade-off is to follow a hybrid approach, where
    you first test your code experiments in a notebook and gradually move the logic
    to a module that is imported into each file. Using auto-reload plugins, you can
    make sure that these modules get automatically reloaded whenever you change the
    logic, without needing to restart your kernel.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地使用版本控制，您必须尽快将业务逻辑从您的交互式笔记本中移除。笔记本以自定义数据格式存储每个单元格的代码和输出，例如序列化为JSON文件。这使得在序列化的笔记本中审查更改变得非常困难。一种好的折衷方法是采用混合方法，首先在笔记本中测试您的代码实验，然后逐渐将逻辑移动到导入到每个文件中的模块。使用自动重新加载插件，您可以确保在更改逻辑时，这些模块会自动重新加载，而无需重启内核。
- en: Moving code from notebooks to modules will not only make your code reusable
    for all other experiments (no need to copy utility functions from notebook to
    notebook) but it will also make your commits much more readable. When multiple
    people change a few lines of code in a massive JSON file (that's how your notebook
    environment stores the code and output of every cell), then the changes made to
    the file will be almost impossible to review and merge. However, if those changes
    are made in a module (a separate file containing only executable code), then these
    changes will be a lot easier to read, review, reason about, and merge.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码从笔记本移动到模块不仅会使您的代码对所有其他实验可重用（无需从笔记本复制实用函数），而且会使您的提交更加易于阅读。当多个人在一个庞大的JSON文件（这是您的笔记本环境存储每个单元格的代码和输出的方式）中更改几行代码时，对文件的更改将几乎无法审查和合并。然而，如果这些更改是在模块（仅包含可执行代码的单独文件）中进行的，那么这些更改将更容易阅读、审查、推理和合并。
- en: Before we continue looking into the versioning of training data, this would
    be a good opportunity to brush up on your Git skills, create a (private) repository,
    and experiment with your version control features.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续查看训练数据的版本化之前，这是一个复习你的Git技能、创建（私有）存储库并实验版本控制功能的好机会。
- en: Registering snapshots of your data
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 注册数据快照
- en: Your ML model is the output of your training code and your training data. If
    we version-control the training source code to create reproducible builds, we
    also need to version the training data. While it sounds reasonable to check small,
    text, non-binary, and non-compressed files into the version control system together
    with your source code, it doesn't sound reasonable for large binary or compressed
    data sources. In this section, we will discuss a solution on how to deal with
    the latter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你的机器学习模型是训练代码和训练数据的输出。如果我们对训练源代码进行版本控制以创建可重复构建，我们也需要对训练数据进行版本控制。虽然将小型、文本、非二进制和非压缩文件与源代码一起检查到版本控制系统中听起来是合理的，但对于大型二进制或压缩数据源来说，这听起来并不合理。在本节中，我们将讨论如何处理后者的解决方案。
- en: 'Let''s re-iterate the idea of reproducible builds: regardless of when the training
    is executed – it could run today, or a year from now – the output should be identical.
    This means that any modifications to the training data should create a new version
    of the dataset, and training should use a specific version of the dataset. We
    differentiate between operational transactional data and historical data. While
    the former is usually stateful and mutable, the latter is often immutable. Sometimes,
    we also see a mix of both, for example, mutable historical event data.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次强调可重复构建的概念：无论何时执行训练——它可能是今天，也可能是一年后——输出应该是相同的。这意味着对训练数据的任何修改都应该创建数据集的新版本，并且训练应该使用数据集的特定版本。我们区分操作事务数据和历史数据。前者通常是状态性和可变的，而后者通常是不可变的。有时，我们也会看到两者的混合，例如，可变的历史事件数据。
- en: When working with mutable data (for example, an operational database storing
    customer information), we need to create snapshots before pulling in the data
    for training. For ML, it's easier to use full snapshots than incremental snapshots,
    as each snapshot contains the complete dataset. While incremental snapshots are
    often created to save costs, full snapshots can also be stored cost-efficiently
    using column-compressed data formats and scalable blob storage systems (such as
    Azure Blob storage), even if you have multiple TBs of data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理可变数据（例如，存储客户信息的操作数据库）时，我们需要在拉取数据用于训练之前创建快照。对于机器学习，使用完整快照比增量快照更容易，因为每个快照都包含完整的数据集。虽然增量快照通常创建来节省成本，但使用列压缩数据格式和可扩展的blob存储系统（如Azure
    Blob存储）也可以高效地存储完整快照，即使你有多个TB的数据。
- en: When dealing with historical or immutable data, we don't usually need to create
    full snapshots, since the data is partitioned—that is, organized in directories
    where directories correspond to the values of the partition key. Historical data
    is often partitioned by processing date or time, such as the time when the data
    ingestion was executed. Date or time partitions make it easier to point your training
    pipelines to a specific range of partitions instead of pointing to a set of files
    directly.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理历史数据或不可变数据时，我们通常不需要创建完整快照，因为数据是分区的——也就是说，组织在目录中，目录对应于分区键的值。历史数据通常按处理日期或时间分区，例如数据摄取执行的时间。日期或时间分区使得将训练管道指向特定范围的分区而不是直接指向一组文件变得更容易。
- en: There are multiple ways to take snapshots of your training data. However, when
    working with the Azure Machine Learning workspace, it is recommended to wrap your
    data in Azure Machine Learning datasets, as discussed in [*Chapter 4*](B17928_04_ePub.xhtml#_idTextAnchor071),
    *Ingesting Data and Managing Datasets*. This makes it easy to take data snapshots
    or version your data. When processing and modifying data in Azure Machine Learning,
    you should make a habit of incrementing the dataset's version. In addition, you
    should pass a specific version of the dataset when fetching the data in the training
    script.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有多种方法可以创建训练数据的快照。然而，当使用Azure机器学习工作区时，建议将数据包装在Azure机器学习数据集中，如[第4章](B17928_04_ePub.xhtml#_idTextAnchor071)“导入数据和管理数据集”中所述。这使得创建数据快照或版本化数据变得容易。在Azure机器学习中处理和修改数据时，你应该养成增加数据集版本的惯例。此外，在训练脚本中获取数据时，你应该传递数据集的特定版本。
- en: Whenever you pass parameters to your training scripts, it is helpful to parameterize
    the pipeline using deterministic placeholders. Parameters such as dates and timestamps
    should be created in the pipeline scheduling step rather than in the code itself.
    This ensures you can always re-run failed pipelines with historical parameters,
    and it will create the same outputs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你向你的训练脚本传递参数时，使用确定性占位符参数化管道是有帮助的。如日期和时间戳之类的参数应在管道调度步骤中创建，而不是在代码本身中创建。这确保了你总是可以用历史参数重新运行失败的管道，并且会创建相同的输出。
- en: So, make sure your input data is registered and versioned and your output data
    is registered and parameterized. This takes a bit of fiddling to set up properly
    but is worth it for the whole project life cycle.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，确保你的输入数据已注册并版本控制，你的输出数据也已注册并参数化。这需要一点时间来正确设置，但整个项目生命周期都是值得的。
- en: Tracking your model metadata and artifacts
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪你的模型元数据和工件
- en: Moving your code to modules, checking it into version control, and versioning
    your data will help to create reproducible models. If you are building an ML model
    for an enterprise, or you are building a model for your start-up, knowing which
    model version is deployed and with which dataset it was trained is essential.
    This is relevant for auditing, debugging, or resolving customers' inquiries about
    the predictions of your service.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 将你的代码移到模块中，将其检查到版本控制中，并对你的数据进行版本控制将有助于创建可重复的模型。如果你正在为企业构建ML模型，或者你正在为你的初创公司构建模型，了解哪个模型版本被部署以及它使用的数据集进行训练是至关重要的。这对于审计、调试或解决客户对你服务预测的询问是相关的。
- en: We have seen in the previous chapters that a few simple steps can enable you
    to track model artifacts and model versions in a model registry. Versioning the
    model artifacts is an essential step for continuous deployments. The model consists
    of artifacts, files that are generated while training, and metadata. Model assets
    contain the definition of the model architecture, parameters, and weights, whereas
    model metadata contains the dataset, commit hash, experiment and run IDs, and
    more of the training run.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中看到，几个简单的步骤就可以让你在模型注册表中跟踪模型工件和模型版本。对模型工件进行版本化是持续部署的必要步骤。模型由工件组成，这些工件是在训练过程中生成的文件和元数据。模型资产包含模型架构、参数和权重的定义，而模型元数据包含数据集、提交哈希、实验和运行ID以及更多训练运行信息。
- en: Another important consideration is to specify and version-control the seed for
    your random number generators. During most training and optimization steps, algorithms
    will use pseudo-random numbers based on a random seed to shuffle data and parameter
    choices. So, in order to produce the same model after running your code multiple
    times, you need to ensure that you set a fixed random seed for every operation
    that uses randomized behaviors.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个重要的考虑因素是指定和版本控制你的随机数生成器的种子。在大多数训练和优化步骤中，算法将使用基于随机种子的伪随机数来洗牌数据和参数选择。因此，为了在多次运行代码后产生相同的模型，你需要确保为每个使用随机行为的操作设置一个固定的随机种子。
- en: Once you understand the benefit of source code version control for your application
    code and versioning your datasets, you will understand that it makes a lot of
    sense for your trained models as well. However, instead of readable code, you
    now store the model artifacts (binaries that contain the model weights and architecture)
    and metadata for each model.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你了解了源代码版本控制对你应用程序代码和版本化数据集的好处，你就会明白这对你的训练模型来说也是非常有意义的。然而，现在你存储的是每个模型的模型工件（包含模型权重和架构的二进制文件）和元数据，而不是可读的代码。
- en: Scripting your environments and deployments
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写你的环境和部署脚本
- en: Automating every operation that you perform during the training and deployment
    process will increase the initial time of development, testing, and deployment,
    but ultimately save you a ton of time when these steps have to be executed again.
    The benefit of cloud services, such as Azure Machine Learning and Azure DevOps,
    is that they provide you with all the necessary tools to automate every step of
    the development and deployment process.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化你在训练和部署过程中执行的每个操作将增加开发、测试和部署的初始时间，但最终在再次执行这些步骤时将节省大量时间。云服务，如Azure机器学习和Azure
    DevOps，的好处是它们为你提供了自动化开发部署过程中每一步所需的所有工具。
- en: If you haven't already done so, you should start organizing your Python in virtual
    environments. Popular options are `requirements`, `pyenv`, `Pipenv`, or `conda`
    files that help you to track development and test dependencies. This helps you
    to specify dependencies as part of the virtual environment and not rely on global
    packages or the global state of the development machine.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，你应该开始组织你的 Python 代码到虚拟环境中。流行的选项包括 `requirements`、`pyenv`、`Pipenv`
    或 `conda` 文件，这些文件可以帮助你跟踪开发和测试依赖项。这有助于你将依赖项作为虚拟环境的一部分进行指定，而不是依赖于全局包或开发机器的全局状态。
- en: Azure DevOps and other CI runners will help you define dependencies because
    running integration tests will install all the defined dependencies automatically
    during the test. This is usually one of the first steps in a CI pipeline. Then,
    whenever you check in new code or tests to your version control system, the CI
    pipeline is executed and also tests the installation of your environment automatically.
    Therefore, it is good practice to add integration tests to all of your modules,
    so that you can never miss a package definition in your environment. If you miss
    declaring a dependency, the CI build will fail.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Azure DevOps 和其他 CI 运行器将帮助你定义依赖关系，因为运行集成测试将在测试过程中自动安装所有定义的依赖项。这通常是 CI 流水线中的第一步。然后，无论何时你将新代码或测试检入到你的版本控制系统，CI
    流水线都会执行并自动测试你的环境安装。因此，将集成测试添加到所有模块中是一个好习惯，这样你就不会错过环境中任何包的定义。如果你遗漏了依赖项的声明，CI 构建将失败。
- en: Next, you also need to script, configure, and automate all your infrastructure.
    If you have followed the previous chapters in this book, you might have figured
    out by now why we did all the infrastructure automation and deployments through
    an authoring environment in Python. If you have scripted these steps previously,
    you can simply run and parameterize these scripts in your CI pipelines.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你还需要编写脚本、配置和自动化所有基础设施。如果你已经阅读了本书的前几章，你现在可能已经明白为什么我们通过 Python 编写环境自动化和部署。如果你之前已经编写了这些步骤的脚本，你可以在
    CI 流水线中简单地运行和参数化这些脚本。
- en: If you run a CI pipeline that generates a model, you most likely want to spin
    up a fresh Azure Machine Learning cluster for this job so you don't interfere
    with other releases, build pipelines, or experimentation. While this level of
    automation is very hard to achieve on on-premises infrastructures, you can do
    this easily in the cloud. Many services, such as YAML files in Azure Machine Learning,
    ARM templates in Azure, or Terraform from HashiCorp, provide full control over
    your infrastructure and configuration.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行一个生成模型的 CI 流水线，你很可能希望为这项工作启动一个新的 Azure Machine Learning 集群，这样你就不会干扰到其他发布、构建流水线或实验。虽然这种自动化程度在本地基础设施上非常难以实现，但在云中你可以轻松做到。许多服务，如
    Azure Machine Learning 中的 YAML 文件、Azure 中的 ARM 模板或 HashiCorp 的 Terraform，都提供了对你基础设施和配置的完全控制。
- en: The last part is to automate deployments within Azure Machine Learning. Performing
    deployments through code doesn't take much longer than through the UI but it gives
    you the benefit of a repeatable and reproducible deployment script. You will often
    be confronted to do the same operation in multiple ways; for example, deploying
    an ML model from Azure Machine Learning via the CLI, Python SDK, YAML, the Studio,
    or a plugin in Azure DevOps. It is recommended to pick whatever works for you,
    stick with one way of doing things, and perform all automation and deployments
    in the same way. Having said this, using Python as the scripting language for
    deployments and checking your deployment code in version control is a good and
    popular choice.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的部分是在 Azure Machine Learning 中自动化部署。通过代码执行部署并不比通过 UI 花费更多时间，但它提供了可重复和可再现的部署脚本的优点。你经常会面临以多种方式执行相同操作的情况；例如，通过
    Azure Machine Learning CLI、Python SDK、YAML、Studio 或 Azure DevOps 中的插件部署 ML 模型。建议选择对你来说最有效的方法，坚持一种做事方式，并以相同的方式进行所有自动化和部署。话虽如此，使用
    Python 作为部署的脚本语言并在版本控制中检查你的部署代码是一个好主意，也是流行的选择。
- en: The key to reproducible builds and CI pipelines is to automate the infrastructure
    and environment from the beginning. In the cloud, especially in Azure, this should
    be very easy as most tools and services can be automated through the SDK. The
    Azure Machine Learning team put a ton of work into the SDK so that you can automate
    each step –from ingestion to deployment – from within Python.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 可重复构建和CI管道的关键是从一开始就自动化基础设施和环境。在云中，特别是在Azure中，这应该非常容易，因为大多数工具和服务都可以通过SDK进行自动化。Azure机器学习团队在SDK上投入了大量工作，以便你可以在Python内部自动化从摄入到部署的每个步骤。
- en: Next, let's take a look into the validation of code and assets to ensure the
    code and trained model work as expected.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来看看代码和资产验证，以确保代码和训练好的模型按预期工作。
- en: Validating the code, data, and models
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 验证代码、数据和模型
- en: When implementing a CI/CD pipeline, you need to make sure you have all the necessary
    tests in place to deploy your newly created code with ease and confidence. Once
    you are running a CI or CI/CD pipeline, the power of automated tests will become
    immediately visible. It not only helps you to detect failures in your code, but
    it also helps to detect future issues in the whole ML process, including the environment
    setup, build dependencies, data requirements, model initialization, optimization,
    resource requirements, and deployment.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现CI/CD管道时，你需要确保你已经设置了所有必要的测试，以便轻松自信地部署你新创建的代码。一旦你运行了CI或CI/CD管道，自动化测试的力量将立即显现。它不仅可以帮助你检测代码中的故障，还可以帮助你检测整个机器学习过程中的未来问题，包括环境设置、构建依赖项、数据需求、模型初始化、优化、资源需求和部署。
- en: When implementing a validation pipeline for our ML process, we can take inspiration
    from traditional software development principles (for example, unit testing, integration
    testing, and end-to-end testing). We can translate these techniques directly to
    steps during the ML process, such as input data, models, and the application code
    of the scoring service. Let's understand how we can adapt these testing techniques
    for ML projects.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当实现我们机器学习过程的验证管道时，我们可以从传统的软件开发原则中汲取灵感（例如，单元测试、集成测试和端到端测试）。我们可以将这些技术直接转换为机器学习过程中的步骤，例如输入数据、模型和评分服务的应用程序代码。让我们了解如何将这些测试技术适应机器学习项目。
- en: Testing data quality with unit tests
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用单元测试测试数据质量
- en: Unit tests are essential to writing good-quality code. A unit test aims to test
    the smallest unit of code (a function) independently of all other code. Each test
    should only test one thing at a time and should run and finish quickly. Many application
    developers run unit tests either every time they change the code, or at least
    every time they submit a new commit to version control.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试对于编写高质量的代码至关重要。单元测试旨在独立于所有其他代码测试代码的最小单元（一个函数）。每个测试应该一次只测试一件事情，并且应该快速运行和完成。许多应用程序开发人员在他们更改代码时运行单元测试，或者至少在将新提交提交到版本控制时运行单元测试。
- en: 'Here is a simple example of a unit test written in Python using the `unittest`
    module provided by the standard library in Python 3:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个使用Python 3标准库中提供的`unittest`模块编写的单元测试的简单示例：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see in the code snippet, we run a single function and test whether
    the outcome matches a predefined variable. We can add more tests as additional
    methods to the test class.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码片段所示，我们运行一个函数并测试结果是否与预定义的变量匹配。我们可以将更多测试作为额外的方法添加到测试类中。
- en: In Python and many other languages, we differentiate between test frameworks
    and libraries that help us to author and organize tests, and libraries to execute
    tests and create reports. `pytest` and `tox` are great libraries to execute tests;
    `unittest` and `mock` help you to author and organize your tests in classes and
    mock out dependencies on other functions.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python和许多其他语言中，我们区分测试框架和库，这些框架和库帮助我们编写和组织测试，以及执行测试和创建报告的库。`pytest`和`tox`是执行测试的出色库；`unittest`和`mock`帮助你以类为单位编写和组织测试，并模拟对其他函数的依赖。
- en: 'When you write code for your ML model, you will also find units of code that
    can, and probably should, be unit tested on every commit. However, ML engineers,
    data engineers, and data scientists now deal with another source of errors in
    their development cycle: the data. Therefore, it is a good idea to rethink what
    unit tests could mean in terms of data quality.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当你为你的机器学习模型编写代码时，你也会发现一些代码单元，这些代码单元可以在每次提交时进行单元测试，并且可能应该进行单元测试。然而，机器学习工程师、数据工程师和数据科学家现在在他们的开发周期中还要处理另一个错误来源：数据。因此，重新思考单元测试在数据质量方面的意义是一个好主意。
- en: Once you get the hang of it, you will quickly understand the power of using
    unit tests to measure data quality. You can interpret feature dimensions of your
    input data as a single testable unit and write tests to ensure each unit is fulfilling
    the defined requirements. This is especially important when new training data
    is collected over time and it is planned to retrain the model in the future. In
    such a case, we always want to ensure that the data is clean and matches our assumptions
    before we start the training process.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你掌握了这个技巧，你将很快理解使用单元测试来衡量数据质量的力量。你可以将输入数据的特征维度视为一个可测试的单个单元，并编写测试来确保每个单元都满足定义的要求。这在随着时间的推移收集新的训练数据并计划未来重新训练模型时尤为重要。在这种情况下，我们总是希望在开始训练过程之前确保数据是干净的并且符合我们的假设。
- en: 'Here are some examples of what your unit tests can test in the training data:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些示例，说明你的单元测试可以在训练数据中测试的内容：
- en: Number of unique/distinct values
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 唯一/不同值的数量
- en: Correlation of feature dimensions
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征维度的相关性
- en: Skewness
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏度
- en: Minimum and maximum values
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小值和最大值
- en: Most common value
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最常见值
- en: Values containing zero or undefined values
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含零或未定义值的值
- en: 'Let''s put this into practice and write a unit test that ensures that the minimum
    value of a dataset is `0`. This simple test will ensure that your CI/CD pipeline
    will fail if your dataset contains unexpected values:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其付诸实践，编写一个单元测试来确保数据集的最小值为 `0`。这个简单的测试将确保如果数据集中包含意外的值，你的 CI/CD 管道将失败：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In the preceding code, we use `unittest` to organize the tests in multiple functions
    within the same class. Each class corresponds to a specific data source, and in
    each class, we can test all feature dimensions. Once set up, we can install `pytest`
    and simply execute it from the command line to run the test.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们使用 `unittest` 在同一个类中的多个函数内组织测试。每个类对应一个特定的数据源，在每一个类中，我们可以测试所有特征维度。一旦设置好，我们就可以安装
    `pytest` 并简单地从命令行执行它来运行测试。
- en: 'In Azure DevOps, we can set up `pytest` or `tox` as a simple step in our build
    pipeline. For a build pipeline step, we can simply add the following block to
    the `azure-pipelines.yml` file:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure DevOps 中，我们可以在构建管道中设置 `pytest` 或 `tox` 作为简单步骤。对于构建管道步骤，我们只需将以下块添加到 `azure-pipelines.yml`
    文件中：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In the preceding code, we first installed `pytest` and `pytest-cov` to create
    a `pytest` coverage report. In the next line, we executed the tests, which will
    now use the dataset and compute all the statistical requirements. If the requirements
    are not met according to the tests, the tests will fail, and we will see these
    errors in the UI for this build. This adds protection to your ML pipeline, as
    you can now make sure no unforeseen problems with the training data make it into
    the release without you noticing.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们首先安装了 `pytest` 和 `pytest-cov` 来创建 `pytest` 覆盖率报告。在下一行，我们执行了测试，现在它将使用数据集并计算所有统计要求。如果测试未满足要求，测试将失败，我们将在构建的
    UI 中看到这些错误。这为你的 ML 管道增加了保护，因为你现在可以确保没有未预见的问题的训练数据在没有你注意到的情况下进入发布。
- en: Unit testing is essential for software development, and so is unit testing for
    data. As with testing in general, it will take some initial effort to be implemented,
    which doesn't immediately turn into value. However, you will soon see that having
    these tests in place will give you good peace of mind when deploying new models
    faster, as it will catch errors with the training data at build time and not when
    the model is already deployed.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 单元测试对于软件开发至关重要，对于数据也是如此。与一般的测试一样，实施它需要一些初始的努力，这不会立即转化为价值。然而，你很快就会看到，有了这些测试，在部署新模型时你会感到更加安心，因为它会在构建时捕捉到训练数据中的错误，而不是当模型已经部署时。
- en: Integration testing for ML
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习集成测试
- en: In software development, integration testing verifies individual so-called components
    often made up of multiple smaller units. You normally use a test driver to run
    the test suite and mock or stub other components in your tests that you don't
    want to test. In graphical applications, you could test a simple visual component
    while imitating the modules the component is interacting with. In the backend
    code, you test your business logic module while mocking all dependent persistence,
    configuration, and UI components.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在软件开发中，集成测试验证所谓的单个组件，这些组件通常由多个较小的单元组成。你通常使用测试驱动程序来运行测试套件，并在测试中模拟或存根你不想测试的其他组件。在图形应用程序中，你可以测试一个简单的视觉组件，同时模仿该组件交互的模块。在后端代码中，你测试你的业务逻辑模块，同时模拟所有依赖的持久化、配置和
    UI 组件。
- en: Integration tests, therefore, help you to detect critical errors when combining
    multiple units together, without the expense of scaffolding the whole application
    infrastructure. They are placed between unit testing and end-to-end testing and
    are typically run per commit, branch, or PR on the CI runtime.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，集成测试有助于你在组合多个单元时检测关键错误，而无需构建整个应用程序基础设施。它们位于单元测试和端到端测试之间，通常在CI运行时按提交、分支或PR运行。
- en: In ML, we can use the concept of integration testing to test the training process
    of an ML pipeline. This can help your training run to find potential bugs and
    errors during the build phase. Integration testing allows you to test whether
    your model, pre-trained weights, a piece of test data, and optimizer can yield
    a successful output. However, different algorithms require different integration
    tests to test whether something is wrong in the training process.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习（ML）中，我们可以使用集成测试的概念来测试机器学习管道的训练过程。这可以帮助你的训练运行在构建阶段发现潜在的错误和错误。集成测试允许你测试你的模型、预训练权重、测试数据片段和优化器是否能够产生成功的输出。然而，不同的算法需要不同的集成测试来测试训练过程中是否存在问题。
- en: 'When training a **DNN** model, you can verify a lot of aspects of the model
    with integration tests. Here is a non-exhaustive list of steps to verify:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练一个**深度神经网络（DNN**）模型时，你可以通过集成测试验证模型的许多方面。以下是一个非详尽的步骤列表，用于验证：
- en: Weights initialization
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Default loss
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认损失函数
- en: Zero input
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零输入
- en: Single batch fitting
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单批次拟合
- en: Default activations
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认激活函数
- en: Default gradients
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认梯度
- en: Using a similar list, you can easily identify and catch cases where all activations
    are capped at the maximum value in a forward pass, or when all gradients are `0`
    during a backward pass. Theoretically, you can run any experiment, test, or check
    you would do manually before working with a fresh dataset and your model, continuously
    in your CI runtime. So, any time your model gets retrained or fine-tuned, these
    checks run automatically in the background.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用类似的列表，你可以轻松地识别和捕捉到所有激活函数在正向传播中都被限制在最大值，或者在反向传播中所有梯度都是`0`的情况。理论上，你可以在CI运行时连续运行任何实验、测试或检查，就像在处理新的数据集和模型之前手动执行的那样。因此，每次你的模型被重新训练或微调时，这些检查都会在后台自动运行。
- en: A more general assumption is that when training a regression model, the default
    mean should be close to the mean prediction value. When training a classifier,
    you could test the distribution of the output classes. In both cases, you can
    detect issues due to modeling, data, or initialization error already, before starting
    the expensive training and optimization process.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更一般的假设是，在训练回归模型时，默认的均值应该接近预测值的均值。在训练分类器时，你可以测试输出类别的分布。在这两种情况下，你可以在开始昂贵的训练和优化过程之前，检测到由于建模、数据或初始化错误引起的问题。
- en: In terms of the runner and framework, you can choose the same libraries as used
    for unit testing because, in this case, integration testing differs only in the
    components that are tested and the way they are combined. Therefore, choosing
    `unittest`, `mock`, and `pytest` is a popular choice to scaffold your integration
    testing pipeline.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行者和框架方面，你可以选择与单元测试相同的库，因为在这种情况下，集成测试仅在测试的组件及其组合方式上有所不同。因此，选择`unittest`、`mock`和`pytest`来构建你的集成测试管道是一种流行的选择。
- en: Integration testing is essential for application development and for running
    end-to-end ML pipelines. It will save you a lot of time and lowers your operational
    costs, if you can detect and avoid such problems automatically.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 集成测试对于应用程序开发和运行端到端机器学习（ML）管道至关重要。如果你能够自动检测和避免这些问题，这将节省你大量时间并降低运营成本。
- en: End-to-end testing using Azure Machine Learning
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Azure Machine Learning进行端到端测试
- en: In end-to-end testing, we want to verify all components involved in a request
    to a deployed and fully functional service. To do so, we need to deploy the complete
    service all together. End-to-end testing is critical for catching errors that
    are triggered only when combining all the components together and running the
    service in a staging or testing environment without mocking any of the other components.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在端到端测试中，我们希望验证所有参与请求已部署和完全功能服务的组件。为此，我们需要一起部署完整的服务。端到端测试对于捕获仅在所有组件组合在一起并在一个没有模拟其他组件的预发布或测试环境中运行服务时触发的错误至关重要。
- en: 'In ML deployments, there are multiple steps where a lot of things can go very
    wrong if not tested properly. Let''s discard the easy ones where we need to make
    sure that the environment is correctly installed and configured. A more critical
    piece of the deployment in Azure Machine Learning is the code for the application
    logic itself: the scoring file. There is no easy way to test the scoring file,
    the format of the request, and the output together without a proper end-to-end
    test.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习部署中，有许多步骤，如果不进行适当的测试，可能会出现很多问题。让我们先排除那些需要确保环境正确安装和配置的简单问题。在Azure机器学习中的部署中，一个更关键的部分是应用程序逻辑本身的代码：评分文件。没有简单的方法可以在没有适当端到端测试的情况下测试评分文件、请求格式和输出。
- en: As you might imagine, end-to-end tests are usually quite expensive to build
    and operate. First, you need to write code and deploy applications to only test
    the code, which requires extra work, effort, and costs. However, this is the only
    way to truly test the scoring endpoint in a production-like end-to-end environment.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，端到端测试通常构建和操作成本很高。首先，你需要编写代码并部署应用程序来测试代码，这需要额外的工作、努力和成本。然而，这是在类似生产环境的端到端环境中真正测试评分端点的唯一方法。
- en: The good thing is that by using Azure Machine Learning deployments, end-to-end
    testing becomes so easy that it should be part of everyone's pipeline. If the
    model allows it, we could even do a no-code deployment where we don't specify
    the deployment target. If this is not possible, we can specify an Azure Container
    Image as a compute target and deploy the model independently. This means taking
    the code from the previous chapter, wrapping it in a Python script, and including
    it as a step in the build process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，通过使用Azure机器学习部署，端到端测试变得如此简单，以至于它应该成为每个人的管道的一部分。如果模型允许，我们甚至可以进行无代码部署，我们不需要指定部署目标。如果这不可能，我们可以指定Azure容器镜像作为计算目标，并独立部署模型。这意味着将前一章的代码放入Python脚本中，并将其作为构建过程中的一个步骤。
- en: End-to-end testing is usually complicated and expensive. However, with Azure
    Machine Learning and automated deployments, a model deployment and sample request
    could just be part of the build pipeline.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 端到端测试通常很复杂且成本高昂。然而，使用Azure机器学习和自动化部署，模型部署和样本请求可以仅仅是构建管道的一部分。
- en: Continuous profiling of your model
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续监控你的模型
- en: Model profiling is an important step during your experimentation and training
    phase. This will give you a good understanding of the resources your model will
    require when used as a scoring service. This is critical information for designing
    and choosing a properly sized inference environment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控是实验和训练阶段的重要步骤。这将帮助你了解当模型作为评分服务使用时所需的资源。这是设计和选择适当规模的推理环境的关键信息。
- en: Whenever training and optimization processes run continuously, the model requirements
    and profile evolve over time. If you use optimization for model stacking or automated
    ML, your resulting models could grow bigger to fit the new data. So, it is good
    to keep an eye on your model requirements to account for deviations from your
    initial resource choices.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 无论训练和优化过程是否持续进行，模型需求和配置都会随着时间的推移而演变。如果你使用优化进行模型堆叠或自动化机器学习，你的模型可能会变得更大以适应新的数据。因此，密切关注你的模型需求，以考虑与初始资源选择偏差是很重要的。
- en: Luckily, Azure Machine Learning provides a model profiling interface that you
    can feed with a model, scoring function, and test data. It will instantiate an
    inferencing environment for you, start the scoring service, run the test data
    through the service, and track the resource utilization. Let's bring all the pieces
    together and set up an end-to-end MLOps pipeline.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Azure机器学习提供了一个模型监控接口，你可以用模型、评分函数和测试数据来填充它。它将为你实例化一个推理环境，启动评分服务，将测试数据通过服务运行，并跟踪资源利用率。让我们将所有这些部分组合起来，并设置一个端到端MLOps管道。
- en: Building an end-to-end MLOps pipeline
  id: totrans-112
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建端到端MLOps管道
- en: In this section, we want to set up an end-to-end MLOps pipeline. All required
    training code should be checked into version control, and the datasets and model
    will be versioned as well. We want to trigger a CI pipeline to build the code
    and retrain the model when the code or training data changes. Through unit and
    integration tests we will ensure that the training and inferencing code works
    in isolation and that the data and model fulfill all requirements and don't deviate
    from our initial assumptions. Therefore, the CI pipeline will be responsible for
    automatic continuous code builds, training, and tests.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们希望设置一个端到端的 MLOps 管道。所有必需的训练代码都应该被检查到版本控制中，数据集和模型也将进行版本控制。我们希望在代码或训练数据发生变化时触发
    CI 管道以构建代码和重新训练模型。通过单元和集成测试，我们将确保训练和推理代码在隔离状态下工作，并且数据和模型满足所有要求，不偏离我们的初始假设。因此，CI
    管道将负责自动的持续代码构建、训练和测试。
- en: Next, we will trigger the CD pipeline whenever a new model version is ready.
    This will deploy the model and inferencing configuration to a staging environment
    and run the end-to-end tests. After the tests have been completed successfully,
    we automatically want to deploy the model to production. Therefore, the CD pipeline
    will be responsible for the automatic deployment.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，每当一个新的模型版本准备好时，我们将触发 CD 管道。这将部署模型和推理配置到预发布环境，并运行端到端测试。测试成功完成后，我们希望自动将模型部署到生产环境。因此，CD
    管道将负责自动部署。
- en: The separation of the pipeline into CI and CD parts makes it easy to decouple
    the process of building assets from deploying assets. However, you can also combine
    both parts into a single CI/CD pipeline, and so build, train, optimize, and deploy
    it all with a single pipeline. It's up to you and your organization how to model
    the CI and CD components of your pipeline, and how to set up any triggers and
    (manual) approvals. You can choose between either deploying every commit to production
    or deploying a number of commits each day or week after manual approval.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将管道分为 CI 和 CD 部分使得将构建资产的过程与部署资产的过程解耦变得容易。然而，您也可以将这两部分合并为一个单一的 CI/CD 管道，从而使用一个管道进行构建、训练、优化和部署。如何对您的管道的
    CI 和 CD 组件进行建模，以及如何设置任何触发器和（手动）批准，取决于您和您的组织。您可以选择将每个提交部署到生产环境，或者在每个工作日或每周在手动批准后部署一定数量的提交。
- en: In this section, we will use Azure DevOps to author and execute the CI/CD pipelines
    and, therefore, to set up triggers, run the build, training, and testing steps,
    and handle the deployment of the trained model. Azure DevOps has built-in functionalities
    to automate the end-to-end CI/CD process. In general, it lets you run pieces of
    functionality, called tasks, grouped together in pipelines on a compute infrastructure
    that you define. You can either run pipelines that are triggered automatically
    through a new commit in your version control system or trigger them through a
    new revision of a build artifact or a button, for example, for semi-automated
    deployments. The former is called a **code pipeline** and refers to CI, while
    the latter is called a **release pipeline** and refers to CD.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用 Azure DevOps 来编写和执行 CI/CD 管道，因此设置触发器、运行构建、训练和测试步骤，并处理训练模型的部署。Azure
    DevOps 具有内置的功能来自动化端到端的 CI/CD 流程。通常，它允许您在您定义的计算基础设施上运行管道中的功能块，称为任务。您可以通过版本控制系统中新提交的提交自动触发管道，或者通过构建工件的新版本或按钮等触发它们，例如，用于半自动部署。前者称为**代码管道**，指的是
    CI，而后者称为**发布管道**，指的是 CD。
- en: Let's start setting up an Azure DevOps project.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始设置一个 Azure DevOps 项目。
- en: Setting up Azure DevOps
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置 Azure DevOps
- en: Azure DevOps will be the container for authoring, configuring, triggering, and
    executing all our CI/CD pipelines. It provides useful abstractions to work with
    version-controlled resources, such as code repositories and a connection to Azure
    and the Azure Machine Learning workspace, and lets you collaboratively access
    runners, pipelines, and build artifacts.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Azure DevOps 将作为编写、配置、触发和执行所有我们的 CI/CD 管道的容器。它提供了与版本控制资源（如代码仓库和与 Azure 及 Azure
    机器学习工作区的连接）一起工作的有用抽象，并允许您协作访问运行器、管道和构建工件。
- en: Important Note
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '**Azure DevOps** refers to the managed Azure DevOps Services accessible via
    [https://dev.azure.com/](https://dev.azure.com/). There also exists an on-premises
    offering for similar CI/CD integration capabilities called **Azure DevOps Server**,
    which was formerly known as Visual Studio **Team Foundation Server** (**TFS**).'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**Azure DevOps**指的是通过[https://dev.azure.com/](https://dev.azure.com/)可访问的托管Azure
    DevOps服务。还存在一个名为**Azure DevOps Server**（以前称为Visual Studio **Team Foundation Server**（**TFS**））的本地提供方案，它提供了类似的CI/CD集成功能。'
- en: As a first step, we are going to set up the Azure DevOps workspace, so that
    we can author and execute Azure MLOps pipelines. Let's start by setting up the
    organization and projects.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，我们将设置Azure DevOps工作区，以便我们可以编写和执行Azure MLOps管道。让我们从设置组织和项目开始。
- en: Organization and projects
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 组织和项目
- en: First, you need to set up your organization. An organization is a workspace
    to manage similar projects and collaborate with a group of people. You can create
    an organization by either using your Microsoft account, GitHub account, or even
    connecting to **Azure Active Directory** (**AAD**). To create an organization,
    you need to log into Azure DevOps ([https://dev.azure.com/](https://dev.azure.com/)),
    provide the slug name for your organization, and select a region to host your
    organization's assets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要设置您的组织。组织是一个用于管理类似项目并与一组人协作的工作区。您可以通过使用Microsoft账户、GitHub账户或连接到**Azure
    Active Directory**（**AAD**）来创建组织。要创建组织，您需要登录到Azure DevOps([https://dev.azure.com/](https://dev.azure.com/))，提供您组织的slug名称，并选择一个区域来托管您组织资产。
- en: 'The following figure shows the screen for creating a new Azure DevOps organization:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了创建新的Azure DevOps组织的屏幕：
- en: '![Figure 16.1 – Creating a new Azure DevOps organization ](img/B17928_16_001.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图16.1 – 创建新的Azure DevOps组织](img/B17928_16_001.jpg)'
- en: Figure 16.1 – Creating a new Azure DevOps organization
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 – 创建新的Azure DevOps组织
- en: Next, you can set up projects in your organization; we will start with one project
    that will contain the configuration and code to run your MLOps pipelines. A project
    is a place to keep all assets for a specific ML project logically grouped. You
    will be able to manage your code repositories, sprint boards, issues, PRs, build
    artifacts, test plans, and CI/CD pipelines within an Azure DevOps project.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您可以在组织中设置项目；我们将从一个包含运行MLOps管道的配置和代码的项目开始。项目是一个用于逻辑分组特定ML项目所有资产的地方。您将能够在Azure
    DevOps项目中管理您的代码仓库、冲刺板、问题、PR、构建工件、测试计划和CI/CD管道。
- en: 'The following figure shows the process of creating a new Azure DevOps project.
    This will be the container for our pipelines, as well as testing and deployment
    configuration:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了创建新的Azure DevOps项目的流程。这将作为我们的管道、测试和部署配置的容器：
- en: '![Figure 16.2 – Creating a new Azure DevOps project ](img/B17928_16_002.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2 – 创建新的Azure DevOps项目](img/B17928_16_002.jpg)'
- en: Figure 16.2 – Creating a new Azure DevOps project
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 – 创建新的Azure DevOps项目
- en: Once we have the organization and project set up, we need to add the Azure Machine
    Learning capabilities to Azure DevOps by installing the appropriate Azure DevOps
    extension.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们设置了组织和项目，我们需要通过安装适当的Azure DevOps扩展将Azure机器学习功能添加到Azure DevOps中。
- en: Azure Machine Learning extension
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Azure机器学习扩展
- en: 'Next, it is recommended to install the Azure Machine Learning extension for
    your Azure DevOps organization. This will tightly integrate your Azure Machine
    Learning workspace into Azure DevOps so that you can do the following things within
    Azure DevOps:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，建议为您的Azure DevOps组织安装Azure机器学习扩展。这将紧密集成您的Azure机器学习工作区到Azure DevOps中，以便您可以在Azure
    DevOps内执行以下操作：
- en: Assign automatic permissions to access your Azure Machine Learning workspace
    resources automatically through Azure Resource Manager.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过Azure资源管理器自动分配访问您的Azure机器学习工作区资源的自动权限。
- en: Trigger release pipelines for new model revisions.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 触发新模型修订的发布管道。
- en: Run Azure Machine Learning pipelines as tasks.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Azure机器学习管道作为任务运行。
- en: Set pre-configured tasks for model deployment and model profiling.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置模型部署和模型分析预配置任务。
- en: It's fair to say that all the preceding things can also be set up manually using
    custom credentials and the Azure ML Python SDK, but the tight integration makes
    it a lot easier to set up.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 公平地说，所有上述内容也可以通过使用自定义凭据和Azure ML Python SDK手动设置，但紧密集成使得设置变得更加容易。
- en: Important Note
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can install the Azure Machine Learning extension for Azure DevOps from [https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml](https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从 [https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml](https://marketplace.visualstudio.com/items?itemName=ms-air-aiagility.vss-services-azureml)
    安装 Azure Machine Learning 扩展到 Azure DevOps。
- en: Next, we will use the extension to set up the service connections and access
    permissions for your Azure and Azure Machine Learning workspace accounts.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用扩展来设置服务连接和你的 Azure 及 Azure Machine Learning 工作区账户的访问权限。
- en: Service connections
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 服务连接
- en: You might remember from previous code examples that interacting with Azure and
    Azure Machine Learning resources requires the appropriate permissions, tenants,
    and subscriptions to be configured. Permissions to access these services and resources
    are often defined through **service principals**. In Azure DevOps, we can set
    up permissions for our Azure DevOps pipelines to access Azure and Azure Machine
    Learning resources, create compute resources, and submit ML experiments through
    **service connections**.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得从之前的代码示例中，与 Azure 和 Azure Machine Learning 资源交互需要配置适当的权限、租户和订阅。访问这些服务和资源的权限通常通过
    **服务主体** 定义。在 Azure DevOps 中，我们可以设置我们的 Azure DevOps 管道访问 Azure 和 Azure Machine
    Learning 资源、创建计算资源以及通过 **服务连接** 提交 ML 实验的权限。
- en: 'In your Azure DevOps project, go to **Settings** | **Service connections**
    and configure a new Azure service connection with service principal authentication
    for your Azure Machine Learning workspace. The following figure shows how to set
    this up in Azure DevOps:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的 Azure DevOps 项目中，转到 **设置** | **服务连接** 并配置一个新的 Azure 服务连接，使用服务主体身份验证你的 Azure
    Machine Learning 工作区。以下图显示了如何在 Azure DevOps 中设置此连接：
- en: '![Figure 16.3 – Creating an Azure DevOps service connection ](img/B17928_16_003.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.3 – 创建 Azure DevOps 服务连接](img/B17928_16_003.jpg)'
- en: Figure 16.3 – Creating an Azure DevOps service connection
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.3 – 创建 Azure DevOps 服务连接
- en: Similarly, you can also permit Azure DevOps pipelines to manage resources in
    an Azure resource group programmatically. It is recommended that you create both
    permissions through service principals and note the name of both newly created
    connections.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，你也可以允许 Azure DevOps 管道以编程方式管理 Azure 资源组中的资源。建议你通过服务主体创建这两种权限，并注意新创建的连接的名称。
- en: Secrets
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 密钥
- en: In the next step, we want to store and manage all the variables and credentials
    outside of the actual CI/CD pipelines. We don't want to embed credentials or configuration
    parameters (such as subscription ID, workspace name, and tenant ID) into the pipeline,
    but pass them as parameters to the running pipeline.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们希望将所有变量和凭证存储和管理在实际的 CI/CD 管道之外。我们不希望将凭证或配置参数（如订阅 ID、工作区名称和租户 ID）嵌入到管道中，而是将它们作为参数传递给正在运行的管道。
- en: In Azure DevOps, you can achieve this by using **variable groups** and **secure
    files**. You can even connect a variable group to an Azure Key Vault instance
    to manage your secrets for you.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure DevOps 中，你可以通过使用 **变量组** 和 **安全文件** 来实现这一点。你甚至可以将变量组连接到 Azure Key Vault
    实例来为你管理密钥。
- en: 'It is recommended that you navigate to **Pipelines** | **Library** to set up
    a variable group that contains your subscription ID, tenant ID, names of your
    service connections, and so on as variables, so that they can be reused in pipelines.
    You can always come back later and add more variables if you need them. The following
    figure shows a sample variable group definition that can be included in your pipelines:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 建议你导航到 **管道** | **库** 来设置一个包含你的订阅 ID、租户 ID、服务连接名称等作为变量的变量组，以便它们可以在管道中重复使用。如果你需要，你总是可以稍后回来添加更多变量。以下图显示了可以包含在你的管道中的示例变量组定义：
- en: '![Figure 16.4 – Creating an Azure DevOps variable group ](img/B17928_16_004.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.4 – 创建 Azure DevOps 变量组](img/B17928_16_004.jpg)'
- en: Figure 16.4 – Creating an Azure DevOps variable group
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.4 – 创建 Azure DevOps 变量组
- en: Next, we will set up a repository and write a code pipeline.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将设置一个仓库并编写代码管道。
- en: Agents and agent pools
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理和代理池
- en: Your CI and CD tasks will eventually check out the project, build it, train
    the model, run the tests, and deploy it. To do all this (and more), you need a
    compute infrastructure to run the CI/CD jobs. In Azure DevOps, these compute resources
    are called **agents**.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你的 CI 和 CD 任务最终将检出项目，构建它，训练模型，运行测试，并部署它。为了完成所有这些（以及更多），你需要一个计算基础设施来运行 CI/CD
    作业。在 Azure DevOps 中，这些计算资源被称为 **代理**。
- en: Azure DevOps Services provides Microsoft-hosted agents, which will execute your
    pipeline jobs either in VMs or Docker images. Both compute resources are ephemeral
    and torn down after each pipeline job.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Azure DevOps 服务提供 Microsoft 托管代理，这些代理将在 VM 或 Docker 镜像中执行你的管道作业。这两种计算资源都是临时的，并在每个管道作业后拆除。
- en: When using Azure DevOps with public projects, Azure Pipelines is free and provides
    you with Microsoft-hosted agents for your CI/CD pipeline jobs. This allows you
    to run 10 parallel jobs for up to 6 hours each. For private projects, you are
    limited to one parallel job for up to 1 hour each with at most 30 hours per month.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Azure DevOps 与公共项目时，Azure Pipelines 是免费的，并为你的 CI/CD 管道作业提供 Microsoft 托管代理。这允许你运行最多
    10 个并行作业，每个作业最多运行 6 小时。对于私有项目，你每月最多只能运行一个并行作业，每个作业最多运行 1 小时。
- en: Important Note
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'To prevent abuse, all free pipeline resources need to be requested for an organization
    via this form: [https://aka.ms/azpipelines-parallelism-request](https://aka.ms/azpipelines-parallelism-request).'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止滥用，所有免费管道资源都需要通过此表单请求组织：[https://aka.ms/azpipelines-parallelism-request](https://aka.ms/azpipelines-parallelism-request)。
- en: If more capacity is needed, we can either run self-hosted agents via Azure DevOps
    Server and/or Azure VM scale set agents or purchase additional Microsoft-hosted
    agents through Azure DevOps Services. For the purpose of this book, you should
    be able to start experimenting comfortably with the free capacity on private repositories.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要更多容量，我们可以通过 Azure DevOps Server 和/或 Azure VM 规模集代理运行自托管代理，或者通过 Azure DevOps
    服务购买额外的 Microsoft 托管代理。为了本书的目的，你应该能够利用私有仓库上的免费容量舒适地进行实验。
- en: Continuous integration – building code with pipelines
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续集成 – 使用管道构建代码
- en: Now, we can start to set up an automatic build, test, and training pipeline
    for our ML model using Azure DevOps pipelines. Conceptually, we will create or
    import a Git repository to Azure DevOps that serves as a container for our ML
    project and will contain the CI pipeline definitions. By convention, we will store
    the pipelines in the `.pipeline/` directory.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始使用 Azure DevOps 管道设置我们的 ML 模型的自动构建、测试和训练管道。从概念上讲，我们将在 Azure DevOps
    中创建或导入一个 Git 仓库，作为我们的 ML 项目的容器，并将包含 CI 管道定义。按照惯例，我们将管道存储在 `.pipeline/` 目录中。
- en: 'The following figure shows how to set up or import a repository in Azure DevOps:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了如何在 Azure DevOps 中设置或导入一个仓库：
- en: '![Figure 16.5 – Cloning or importing a repository ](img/B17928_16_005.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图 16.5 – 克隆或导入仓库](img/B17928_16_005.jpg)'
- en: Figure 16.5 – Cloning or importing a repository
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图 16.5 – 克隆或导入仓库
- en: Next, we open Visual Studio Code and start authoring our pipeline. Instead of
    constructing the CI pipeline from widgets and plugins, we will choose YAML to
    author the pipeline code. This is very similar to how GitHub CI or Jenkins workflows
    are written.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打开 Visual Studio Code 并开始编写我们的管道。我们不会从小部件和插件中构建 CI 管道，而是选择 YAML 来编写管道代码。这与
    GitHub CI 或 Jenkins 工作流程的编写方式非常相似。
- en: 'A pipeline contains a linear series of tasks to be executed to build, test,
    and train the ML model that can be triggered by a condition in the repository.
    In the Azure DevOps pipeline, tasks are organized in the following hierarchy:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 管道包含一系列线性任务，用于构建、测试和训练 ML 模型，这些任务可以通过仓库中的条件触发。在 Azure DevOps 管道中，任务按以下层次结构组织：
- en: 'Stage A:'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '阶段 A:'
- en: 'Job 1:'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业 1：
- en: Step 1.1
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤 1.1
- en: Step 1.2
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤 1.2
- en: 'Job 2:'
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业 2：
- en: Step 2.1
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步骤 2.1
- en: 'Therefore, a pipeline is made up of stages, where each stage contains multiple
    jobs. Each job can contain multiple tasks called steps. Besides stages and jobs,
    the pipeline can contain the following sections:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个管道由阶段组成，其中每个阶段包含多个作业。每个作业可以包含多个称为步骤的任务。除了阶段和作业之外，管道还可以包含以下部分：
- en: 'Pipeline definition:'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道定义：
- en: '`name`: The name of the pipeline'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`name`：管道的名称'
- en: 'Pipeline triggers:'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道触发器：
- en: '`schedules`: Scheduling-based pipeline trigger configuration'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`schedules`：基于计划的管道触发配置'
- en: '`trigger`: Code-based pipeline trigger configuration'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trigger`：基于代码的管道触发配置'
- en: '`pr`: PR-based pipeline trigger configuration'
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pr`：基于 PR 的管道触发配置'
- en: 'Pipeline compute resources:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道计算资源：
- en: '`resources`: Containers and repository configuration'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resources`: 容器和存储库配置'
- en: '`pool`: Agent pool configuration for pipeline compute resources'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pool`: 管道计算资源的代理池配置'
- en: 'Pipeline customization:'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道定制：
- en: '`variables`: Pipeline variables'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`variables`: 管道变量'
- en: '`parameters`: Pipeline parameters'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parameters`: 管道参数'
- en: 'Pipeline job definition:'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道作业定义：
- en: '`stages`: Grouping of pipeline jobs, can be skipped if the pipeline contains
    only a single stage'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stages`: 管道作业的分组，如果管道只有一个阶段，则可以跳过'
- en: '`jobs`: Pipeline jobs to be executed'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jobs`: 要执行的管道作业'
- en: As you can see in the preceding list, the Azure DevOps pipeline YAML schema
    allows you to customize pipeline triggers, compute resources, variables, and configurations,
    and lets you define the tasks to run in the pipeline. Azure DevOps pipelines also
    understand the concept of templating. You can use the `template` directive for
    stages, pipelines, jobs, steps, parameters, and variables to reference files from
    the template.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的列表所示，Azure DevOps 管道 YAML 方案允许您自定义管道触发器、计算资源、变量和配置，并允许您定义管道中要运行的作业。Azure
    DevOps 管道还理解模板的概念。您可以使用 `template` 指令为阶段、管道、作业、步骤、参数和变量引用模板中的文件。
- en: Important Note
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find the documentation of the pipeline's YAML schema in the Microsoft
    documentation at [https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/](https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在微软文档中找到管道 YAML 方案的文档，网址为 [https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/](https://docs.microsoft.com/en-us/azure/devops/pipelines/yaml-schema/).
- en: 'Let''s use these step definitions and construct a simple pipeline to test the
    model code and start model training:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这些步骤定义来构建一个简单的管道，以测试模型代码并开始模型训练：
- en: ci-pipeline.yaml
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ci-pipeline.yaml
- en: '[PRE3]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding pipeline, we define the trigger to start the pipeline for
    new commits on the `main` branch. For execution, we run each job on the Microsoft-hosted
    free agent pool using an Ubuntu VM. Then, we group the tasks into two stages:
    `CI` and `Train`. The former will build and test the code and datasets, whereas
    the latter will train the ML model and create a new version of the model in the
    model registry.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的管道中，我们定义了触发器，当 `main` 分支上有新提交时启动管道。对于执行，我们使用 Ubuntu VM 在微软托管的免费代理池上运行每个作业。然后，我们将任务分为两个阶段：`CI`
    和 `Train`。前者将构建和测试代码及数据集，而后者将训练机器学习模型并在模型注册表中创建模型的新版本。
- en: Now, we can add a commit to the repository and merge it to the `main` branch,
    and the CI pipeline will be triggered and train a new model version. You can use
    the preceding pipeline definition as a starting point to add additional steps,
    tests, configurations, and triggers to fully customize your CI pipeline.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在存储库中添加一个提交并将其合并到 `main` 分支，CI 管道将被触发并训练一个新的模型版本。您可以使用前面的管道定义作为起点，添加额外的步骤、测试、配置和触发器，以完全定制您的
    CI 管道。
- en: Important Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find an up-to-date example of an MLOps pipeline in the Microsoft GitHub
    repository at [https://github.com/microsoft/MLOpsPython](https://github.com/microsoft/MLOpsPython).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在微软 GitHub 存储库中找到最新的 MLOps 管道示例，网址为 [https://github.com/microsoft/MLOpsPython](https://github.com/microsoft/MLOpsPython)。
- en: You can find more examples for MLOps starting points on the Azure MLOps repository
    [https://github.com/Azure/mlops-v2](https://github.com/Azure/mlops-v2)
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 Azure MLOps 存储库中找到更多 MLOps 起始点的示例 [https://github.com/Azure/mlops-v2](https://github.com/Azure/mlops-v2)
- en: Next, we will take a look at a CD pipeline to deploy the trained model to production.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一个 CD 管道，以将训练好的模型部署到生产环境中。
- en: Continuous deployment – deploying models with release pipelines
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 持续部署 - 使用发布管道部署模型
- en: An additional benefit of tracking model artifacts in a model registry (for example,
    in Azure Machine Learning) is that you can automatically trigger release pipelines
    in Azure DevOps when the artifacts change. Any artifact, such as a new ML model
    or version, can be configured to trigger a release in Azure DevOps. Therefore,
    code changes trigger CI build pipelines, and artifact changes trigger CD release
    pipelines. In this section, we will create a CD pipeline for our model and automatically
    roll the model out into staging and production.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型注册库（例如 Azure 机器学习）中跟踪模型工件（例如，新机器学习模型或版本）的另一个好处是，当工件发生变化时，您可以在 Azure DevOps
    中自动触发发布管道。任何工件，如新的机器学习模型或版本，都可以配置为在 Azure DevOps 中触发发布。因此，代码更改触发 CI 构建管道，而工件更改触发
    CD 发布管道。在本节中，我们将为我们的模型创建一个 CD 管道，并自动将模型推出到预发布和生产环境。
- en: While the triggering mechanism for release pipelines is different from build
    pipelines, most of the concepts for pipeline execution are very similar. Release
    pipelines also have pipeline stages, whereas each stage can have multiple tasks.
    One additional feature of release pipelines, since they deal with the deployment
    of artifacts, is that each stage can have additional **triggers**, as well as
    **pre-deployment** and **post-deployment conditions**, such as **manual approval**
    and **gates**.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管发布管道的触发机制与构建管道不同，但大多数关于管道执行的概念都非常相似。发布管道也有管道阶段，而每个阶段可以有多个任务。发布管道的一个额外功能是，由于它们处理工件部署，因此每个阶段可以具有额外的**触发器**，以及**部署前**和**部署后**条件，例如**手动批准**和**门控**。
- en: Triggers will allow you to continue the pipeline execution during a specified
    schedule only. Manual approvals will halt the pipeline until it is approved by
    the defined user or user group, whereas gates will halt the pipeline for a predefined
    time before executing a programmatic check. Multiple stages, triggers, and pre-
    and post-deployment conditions are often combined to safely deploy artifacts to
    different environments.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 触发器将允许您在指定的计划期间继续管道执行。手动批准将使管道暂停，直到被定义的用户或用户组批准，而门控将使管道在执行程序性检查之前暂停预定义的时间。通常将多个阶段、触发器以及部署前和部署后的条件结合起来，以安全地将工件部署到不同的环境。
- en: If you have the Azure Machine Learning plugin installed, you can select triggers
    and deployment tasks specifically for Azure Machine Learning, such as artifacts
    based on ML model versions and Azure Machine Learning model deployment and profiling
    tasks. In this section, we will choose both the ML model artifact trigger and
    the ML model deployment task.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已安装Azure Machine Learning插件，您可以选择专门针对Azure Machine Learning的触发器和部署任务，例如基于ML模型版本的工件和Azure
    Machine Learning模型部署和性能分析任务。在本节中，我们将选择ML模型工件触发器和ML模型部署任务。
- en: Important Note
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can find the available Azure DevOps tasks in the Microsoft documentation
    at [https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在Microsoft文档中找到可用的Azure DevOps任务，网址为[https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/](https://docs.microsoft.com/en-us/azure/devops/pipelines/tasks/)。
- en: 'The following figure shows you an Azure DevOps release pipeline, where we select
    an ML model as an artifact for the release pipeline trigger. We configure the
    pipeline with two stages, a deployment to staging and a deployment to production.
    In addition, we add a manual approval as a post-deployment condition of the staging
    deployment:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了Azure DevOps发布管道，其中我们选择一个机器学习模型作为发布管道触发的工件。我们配置了两个阶段，一个是部署到预发布环境，另一个是部署到生产环境。此外，我们还添加了一个手动批准作为预发布部署的部署后条件：
- en: '![Figure 16.6 – Defining an Azure DevOps Release Pipeline ](img/B17928_16_006.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图16.6 – 定义Azure DevOps发布管道](img/B17928_16_006.jpg)'
- en: Figure 16.6 – Defining an Azure DevOps Release Pipeline
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 – 定义Azure DevOps发布管道
- en: By default, the release pipeline will require a user to create a release by
    pressing the **Create release** button in the top-right corner. This mode is intended
    to create releases only when an operator decides to trigger a deployment, and
    helps us avoid any automated deployments while configuring the release pipeline.
    However, once the operator is confident that the pipeline and release process
    are working as intended, we can enable automated deployments by toggling the flash
    icon on the asset in the release pipeline. This will enable the CD trigger and,
    therefore, trigger a release and deployment whenever the asset has changed. As
    a final task in this chapter, you can go ahead and activate the CD trigger to
    fully automate your CD pipeline.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，发布管道将要求用户通过在右上角点击**创建发布**按钮来创建发布。这种模式旨在仅在操作员决定触发部署时创建发布，并有助于我们在配置发布管道时避免任何自动化部署。然而，一旦操作员确信管道和发布过程按预期工作，我们可以通过切换发布管道中资产上的闪光图标来启用自动化部署。这将启用CD触发器，因此，每当资产发生变化时，都会触发发布和部署。在本章的最后一个任务中，您可以继续激活CD触发器，以完全自动化您的CD管道。
- en: Summary
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced MLOps, a DevOps-like workflow for developing,
    deploying, and operating ML services. DevOps stands for a quick and high-quality
    way of making changes to code and deploying these changes to production.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了MLOps，这是一种类似于DevOps的工作流程，用于开发、部署和操作机器学习服务。DevOps代表了一种快速且高质量地更改代码并将这些更改部署到生产环境的方法。
- en: We first learned that Azure DevOps gives us all the features to run powerful
    CI/CD pipelines. We can run either build pipelines, where steps are coded in YAML,
    or release pipelines, which are configured in the UI. Release pipelines can have
    manual or multiple automatic triggers (for example, a commit in the version control
    repository or if the artifact of a model registry was updated) and create an output
    artifact for release or deployment.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先了解到 Azure DevOps 提供了所有功能来运行强大的 CI/CD 流水线。我们可以运行构建流水线，其中步骤用 YAML 编码，或者发布流水线，它们在
    UI 中进行配置。发布流水线可以有手动或多个自动触发器（例如，版本控制仓库中的提交或如果模型注册表的工件已更新）并为发布或部署创建输出工件。
- en: Version-controlling your code is necessary, but it's not enough to run proper
    CI/CD pipelines. In order to create reproducible builds, we need to make sure
    that the dataset is also versioned and pseudo-random generators are seeded with
    a specified parameter. Environments and infrastructure should also be automated,
    and deployments can be done from the authoring environment.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 版本控制您的代码是必要的，但仅此不足以运行适当的 CI/CD 流水线。为了创建可重复构建，我们需要确保数据集也进行了版本控制，并且伪随机生成器使用指定的参数进行初始化。环境和基础设施也应自动化，部署可以从创作环境进行。
- en: In order to keep the code quality high, you need to add tests to the ML pipeline.
    In application development, we differentiate between unit, integration, and end-to-end
    tests, where they test different parts of the code, either independently or together
    with other services. For data pipelines with changing or increasing data, unit
    tests should test the data quality as well as units of code in the application.
    Integration tests are great for loading a model or performing a forward or backward
    pass through a model independently from other components. With Azure Machine Learning,
    writing end-to-end tests becomes a real joy as they can be completely automated
    with very low effort and costs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持代码质量高，您需要将测试添加到机器学习流水线中。在应用程序开发中，我们区分单元测试、集成测试和端到端测试，它们测试代码的不同部分，要么独立进行，要么与其他服务一起进行。对于数据流水线，如果数据发生变化或增加，单元测试应测试数据质量以及应用程序中的代码单元。集成测试非常适合独立于其他组件加载模型或执行正向或反向传递。使用
    Azure Machine Learning，编写端到端测试变得非常愉快，因为它们可以非常低效和低成本地完全自动化。
- en: Now, you have learned how to set up continuous pipelines that can retrain and
    optimize your models and then automatically build and redeploy the models to production.
    In the last chapter, we will look at what's next for you, your company, and your
    ML services in Azure.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经学会了如何设置连续流水线，这些流水线可以重新训练和优化您的模型，然后自动构建和重新部署模型到生产环境中。在最后一章中，我们将探讨您、您的公司以及您在
    Azure 中的 ML 服务下一步将是什么。
