- en: Dimensionality Reduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下食谱：
- en: Reducing dimensionality with PCA
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA进行降维
- en: Using factor analysis for decomposition
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用因子分析进行分解
- en: Using kernel PCA for nonlinear dimensionality reduction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用核PCA进行非线性降维
- en: Using truncated SVD to reduce dimensionality
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用截断SVD进行降维
- en: Using decomposition to classify with DictionaryLearning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分解进行分类与DictionaryLearning
- en: Doing dimensionality reduction with manifolds – t-SNE
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流形进行降维——t-SNE
- en: Testing methods to reduce dimensionality with pipelines
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试通过管道减少维度的方法
- en: Introduction
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will reduce the number of features or inputs into the machine
    learning models. This is a very important operation because sometimes datasets
    have a lot of input columns, and reducing the number of columns creates simpler
    models that take less computing power to predict.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将减少输入到机器学习模型中的特征或输入数量。这是一个非常重要的操作，因为有时数据集有很多输入列，减少列数可以创建更简单的模型，减少计算能力的需求以进行预测。
- en: The main model used in this section is **principal component analysis** (**PCA**).
    You do not have to know how many features you can reduce the dataset to, thanks
    to PCA's explained variance. A similar model in performance is **truncated singular
    value decomposition** (**truncated SVD**). It is always best to first choose a
    linear model that allows you to know how many columns you can reduce the set to,
    such as PCA or truncated SVD.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节使用的主要模型是**主成分分析**（**PCA**）。由于PCA的解释方差，您无需知道可以将数据集减少到多少特征。一个性能相似的模型是**截断奇异值分解**（**truncated
    SVD**）。最好首先选择一个线性模型，允许您知道可以将数据集减少到多少列，例如PCA或截断SVD。
- en: Later in the chapter, check out the modern method of **t-distributed stochastic
    neighbor embedding** (**t-SNE**), which makes features easier to visualize in
    lower dimensions. In the final recipe, you can examine a complex pipeline and
    grid search that finds the best composite estimator consisting of dimensionality
    reductions joined with several support vector machines.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章后面，查看现代方法**t分布随机邻居嵌入**（**t-SNE**），它使特征在低维度中更容易可视化。在最后一个食谱中，您可以检查一个复杂的管道和网格搜索，找到由降维与多个支持向量机组成的最佳复合估计器。
- en: Reducing dimensionality with PCA
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用PCA进行降维
- en: Now it's time to take the math up a level! PCA is the first somewhat advanced
    technique discussed in this book. While everything else thus far has been simple
    statistics, PCA will combine statistics and linear algebra to produce a preprocessing
    step that can help to reduce dimensionality, which can be the enemy of a simple
    model.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候将数学提升到一个新层次了！PCA是本书中讨论的第一个相对高级的技术。到目前为止，所有的内容都只是简单的统计学，而PCA将统计学与线性代数结合起来，产生一个预处理步骤，有助于减少维度，这是简化模型的敌人。
- en: Getting ready
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'PCA is a member of the decomposition module of scikit-learn. There are several
    other decomposition methods available, which will be covered later in this recipe.
    Let''s use the iris dataset, but it''s better if you use your own data:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是scikit-learn中分解模块的一个成员。还有几种其他的分解方法，稍后将在本食谱中介绍。我们将使用鸢尾花数据集，但如果你使用自己的数据会更好：
- en: '[PRE0]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: How to do it...
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Import the `decomposition` module:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`decomposition`模块：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Instantiate a default PCA object:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个默认的PCA对象：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Compared to other objects in scikit-learn, the PCA object takes relatively
    few arguments. Now that the PCA object (an instance PCA) has been created, simply
    transform the data by calling the `fit_transform` method, with `iris_X` as the
    argument:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与scikit-learn中的其他对象相比，PCA对象所需的参数相对较少。现在PCA对象（一个PCA实例）已经创建，只需通过调用`fit_transform`方法来转换数据，`iris_X`作为参数：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now that the PCA object has been fitted, we can see how well it has done at
    explaining the variance (explained in the following *How it works...* section):'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在PCA对象已经拟合完成，我们可以看到它在解释方差方面的效果如何（将在接下来的*工作原理...*部分中进行说明）：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: How it works...
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: PCA has a general mathematical definition and a specific use case in data analysis.
    PCA finds the set of orthogonal directions that represent the original data matrix.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: PCA有一个通用的数学定义，并在数据分析中有特定的应用案例。PCA找到一组正交方向，这些方向表示原始数据矩阵。
- en: Generally, PCA works by mapping the original dataset into a new space where
    each of the new column vectors of the matrix are orthogonal. From a data analysis
    perspective, PCA transforms the covariance matrix of the data into column vectors
    that can explain certain percentages of the variance. For example, with the iris
    dataset, 92.5 percent of the variance of the overall dataset can be explained
    by the first component.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，PCA通过将原始数据集映射到一个新空间来工作，其中矩阵的每个新列向量都是正交的。从数据分析的角度来看，PCA将数据的协方差矩阵转换为可以解释方差某些百分比的列向量。例如，使用鸢尾花数据集，92.5%的整体方差可以通过第一个分量来解释。
- en: 'This is extremely useful because dimensionality is problematic in data analysis.
    Quite often, algorithms applied to high-dimensional datasets will overfit on the
    initial training, and thus lose generality to the test set. If most of the underlying
    structure of the data can be faithfully represented by fewer dimensions, then
    it''s generally considered a worthwhile trade-off:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常有用，因为维度问题在数据分析中很常见。许多应用于高维数据集的算法会在初始训练时出现过拟合，从而失去对测试集的泛化能力。如果数据的绝大部分结构可以通过更少的维度忠实地表示，那么这通常被认为是值得的权衡：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Our data matrix is now 150 x 2, instead of 150 x 4\. The separability of the
    classes remains even after reducing the dimensionality by two. We can see how
    much of the variance is represented by the two components that remain:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据矩阵是150 x 2，而不是150 x 4。即便在减少维度为二之后，类别的可分性依然保持。我们可以看到这两维所表示的方差有多少：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To visualize what PCA has done, let''s plot the first two dimensions of the
    iris dataset with before-after pictures of the PCA transformation:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化PCA的效果，让我们绘制鸢尾花数据集的前两维，并展示PCA变换前后的对比图：
- en: '[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/de1daad3-cf27-468d-95aa-895177d2e8e5.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/de1daad3-cf27-468d-95aa-895177d2e8e5.png)'
- en: 'The `PCA` object can also be created with the amount of explained variance
    in mind from the start. For example, if we want to be able to explain at least
    98 percent of the variance, the `PCA` object will be created as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`PCA`对象也可以在一开始就考虑解释方差的数量。例如，如果我们希望至少解释98%的方差，那么`PCA`对象将按如下方式创建：'
- en: '[PRE8]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Since we wanted to explain variance slightly more than the two component examples,
    a third was included.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望解释的方差稍微多于两个分量示例，因此包含了第三个分量。
- en: Even though the final dimensions of the data are two or three, these two or
    three columns contain information from all four original columns.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 即使最终数据的维度是二维或三维，这两三列也包含了所有四个原始列的信息。
- en: There's more...
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'It is recommended that PCA is scaled beforehand. Do so as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 建议在使用PCA之前进行缩放。操作步骤如下：
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This leads to the following graph:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了如下图：
- en: '[PRE10]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/f1597dbd-7243-49f4-a582-d7d3c68c63ce.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f1597dbd-7243-49f4-a582-d7d3c68c63ce.png)'
- en: 'This looks a bit worse. Regardless, you should always consider the scaled PCA
    if you consider PCA. Preferably, you can scale with a pipeline as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来有点差。无论如何，如果你考虑使用PCA，始终应该考虑使用缩放后的PCA。最好能通过管道按如下方式进行缩放：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Using pipelines prevents errors and reduces the amount of debugging of complex
    code.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道可以防止错误，并减少复杂代码的调试工作量。
- en: Using factor analysis for decomposition
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用因子分析进行分解
- en: Factor analysis is another technique that we can use to reduce dimensionality.
    However, factor analysis makes assumptions and PCA does not. The basic assumption
    is that there are implicit features responsible for the features of the dataset.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分析是我们可以用来减少维度的另一种技术。然而，因子分析有前提假设，而PCA没有。基本假设是存在一些隐含特征，它们决定了数据集的特征。
- en: This recipe will boil down to the explicit features from our samples in an attempt
    to understand the independent variables as much as the dependent variables.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法将提取样本中的显式特征，以期理解独立变量和因变量。
- en: Getting ready
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备就绪
- en: 'To compare PCA and factor analysis, let''s use the iris dataset again, but
    we''ll first need to load the `FactorAnalysis` class:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较PCA和因子分析，我们再次使用鸢尾花数据集，但我们首先需要加载`FactorAnalysis`类：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: How to do it...
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'From a programming perspective, factor analysis isn''t much different from
    PCA:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 从编程角度来看，因子分析与PCA没有太大区别：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Compare the following plot to the plot in the last section:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下图与上一节中的图进行比较：
- en: '![](img/46688b30-dcfe-4db6-8ce7-df0ecc70da1a.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46688b30-dcfe-4db6-8ce7-df0ecc70da1a.png)'
- en: Since factor analysis is a probabilistic transform, we can examine different
    aspects, such as the log likelihood of the observations under the model, and better
    still, compare the log likelihoods across models.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于因子分析是一个概率变换，我们可以检查不同的方面，例如模型下观测值的对数似然性，甚至更好的是，比较不同模型的对数似然性。
- en: Factor analysis is not without flaws. The reason is that you're not fitting
    a model to predict an outcome, you're fitting a model as a preparation step. This
    isn't a bad thing, but errors here are compounded when training the actual model.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分析并非没有缺点。原因在于你不是在拟合一个模型来预测结果，而是将模型作为准备步骤来拟合。这并不是什么坏事，但当你在训练实际模型时，错误会被累积。
- en: How it works...
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: Factor analysis is similar to PCA, which was covered previously. However, there
    is an important distinction to be made. PCA is a linear transformation of the
    data to a different space where the first component explains the variance of the
    data, and each subsequent component is orthogonal to the first component.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 因子分析与之前讲解的PCA相似，然而它们之间有一个重要的区别。PCA是数据的线性变换，转到一个不同的空间，在这个空间里，第一个主成分解释了数据的方差，而每个后续主成分与第一个主成分正交。
- en: For example, you can think of PCA as taking a dataset of *N* dimensions and
    going down to some space of *M* dimensions, where *M* < *N*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以将PCA想象成将一个*N*维的数据集降到某个*M*维的空间，其中*M* < *N*。
- en: Factor analysis, on the other hand, works under the assumption that there are
    only *M* important features and a linear combination of these features (plus noise)
    creates the dataset in *N* dimensions. To put it another way, you don't do regression
    on an outcome variable, you do regression on the features to determine the latent
    factors of the dataset.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，因子分析假设只有*M*个重要特征，这些特征的线性组合（加噪声）创建了*N*维度的数据集。换句话说，你不是在对结果变量做回归，而是在对特征做回归，以确定数据集的潜在因子。
- en: Additionally, a big drawback is that you do not know how many columns you can
    reduce the data to. PCA gives you the explained variance metric to guide you through
    the process.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一个大缺点是你不知道可以将数据降到多少列。PCA会提供解释方差的指标，以指导你完成这一过程。
- en: Using kernel PCA for nonlinear dimensionality reduction
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核PCA进行非线性降维
- en: Most of the techniques in statistics are linear by nature, so in order to capture
    nonlinearity, we might need to apply some transformation. PCA is, of course, a
    linear transformation. In this recipe, we'll look at applying nonlinear transformations,
    and then apply PCA for dimensionality reduction.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学中的大多数技术本质上是线性的，因此为了捕捉非线性，我们可能需要应用一些变换。PCA当然是线性变换。在本步骤中，我们将看一下应用非线性变换，然后应用PCA进行降维。
- en: Getting ready
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Life would be so easy if data was always linearly separable, but unfortunately,
    it's not. Kernel PCA can help to circumvent this issue. Data is first run through
    the kernel function that projects the data onto a different space; then, PCA is
    performed.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据总是线性可分，生活会变得非常简单，但不幸的是，数据并非总是如此。核主成分分析（Kernel PCA）可以帮助解决这个问题。数据首先通过核函数进行处理，将数据投影到一个不同的空间；然后，执行PCA。
- en: To familiarize yourself with the kernel functions, it will be a good exercise
    to think of how to generate data that is separable by the kernel functions available
    in the kernel PCA. Here, we'll do that with the cosine kernel. This recipe will
    have a bit more theory than the previous recipes.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了熟悉核函数，一个很好的练习是思考如何生成能够被核PCA中可用的核函数分离的数据。在这里，我们将使用余弦核来完成。这个步骤的理论内容会比之前的更多。
- en: 'Before starting, load the iris dataset:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，加载鸢尾花数据集：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: How to do it...
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'The cosine kernel works by comparing the angle between two samples represented
    in the feature space. It is useful when the magnitude of the vector perturbs the
    typical distance measure used to compare samples. As a reminder, the cosine between
    two vectors is given by the following formula:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦核通过比较在特征空间中表示的两个样本之间的角度来工作。当向量的大小扰动了用于比较样本的典型距离度量时，它就显得很有用。提醒一下，两个向量之间的余弦通过以下公式给出：
- en: '![](img/232fac96-295d-456a-b747-3919da10a26d.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/232fac96-295d-456a-b747-3919da10a26d.png)'
- en: This means that the cosine between *A* and *B* is the dot product of the two
    vectors normalized by the product of the individual norms. The magnitude of vectors
    *A* and *B* have no influence on this calculation.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着*A*和*B*之间的余弦是这两个向量的点积，通过各自的范数的乘积进行归一化。向量*A*和*B*的大小对这个计算没有影响。
- en: 'So, let''s go back to the iris dataset to use it for visual comparisons:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们回到鸢尾花数据集，使用它进行视觉对比：
- en: '[PRE15]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, visualize the result:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，展示结果：
- en: '![](img/2fbf706e-bfd2-42eb-89bb-f052348e25b8.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fbf706e-bfd2-42eb-89bb-f052348e25b8.png)'
- en: The result looks slightly better, although we would have to measure it to know
    for sure.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来稍微好一些，尽管我们需要进行测量才能确认。
- en: How it works...
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'There are several different kernels available besides the cosine kernel. You
    can even write your own kernel function. The available kernels are as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 除了余弦核，还有几种不同的核可供选择。你甚至可以编写自己的核函数。可用的核如下：
- en: Poly (polynomial)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式（Poly）
- en: RBF (radial basis function)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RBF（径向基函数）
- en: Sigmoid
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid
- en: Cosine
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦
- en: Pre-computed
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预计算
- en: There are also options that are contingent on the kernel choice. For example,
    the degree argument will specify the degree for the poly, RBF, and sigmoid kernels;
    also, gamma will affect the RBF or poly kernels.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些选项依赖于核的选择。例如，degree参数将为多项式核、RBF核和Sigmoid核指定度数；此外，gamma将影响RBF或多项式核。
- en: The recipe on SVM will cover the RBF kernel function in more detail.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: SVM中的食谱将更详细地介绍RBF核函数。
- en: Kernel methods are great to create separability, but they can also cause overfitting
    if used without care. Make sure to train-test them properly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 核方法非常适合创建可分性，但如果使用不当，也可能导致过拟合。确保适当进行训练和测试。
- en: Luckily, the available kernels are smooth, continuous, and differentiable functions.
    They do not create the jagged edges of regression trees.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，现有的核是平滑的、连续的且可微的函数。它们不会像回归树那样产生锯齿状的边缘。
- en: Using truncated SVD to reduce dimensionality
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用截断SVD来减少维度
- en: Truncated SVD is a matrix factorization technique that factors a matrix *M*
    into the three matrices *U*, Σ, and *V*. This is very similar to PCA, except that
    the factorization for SVD is done on the data matrix, whereas for PCA, the factorization
    is done on the covariance matrix. Typically, SVD is used under the hood to find
    the principle components of a matrix.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 截断SVD是一种矩阵分解技术，将矩阵*M*分解为三个矩阵*U*、Σ和*V*。这与PCA非常相似，不同之处在于，SVD的分解是在数据矩阵上进行的，而PCA的分解则是在协方差矩阵上进行的。通常，SVD在幕后用于找到矩阵的主成分。
- en: Getting ready
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Truncated SVD is different from regular SVDs in that it produces a factorization
    where the number of columns is equal to the specified truncation. For example,
    given an *n* x *n* matrix, SVD will produce matrices with *n* columns, whereas
    truncated SVD will produce matrices with the specified number of columns. This
    is how the dimensionality is reduced. Here, we''ll again use the iris dataset
    so that you can compare this outcome against the PCA outcome:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 截断SVD不同于常规SVD，它产生的分解结果列数等于指定的截断数。例如，对于一个*n* x *n*的矩阵，SVD将产生*n*列的矩阵，而截断SVD将产生指定列数的矩阵。通过这种方式，维度得以减少。这里我们将再次使用鸢尾花数据集，供你与PCA结果进行比较：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: How to do it...
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: This object follows the same form as the other objects we've used.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这个对象与我们使用的其他对象形式相同。
- en: 'First, we''ll import the required object, then we''ll fit the model and examine
    the results:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所需的对象，然后拟合模型并检查结果：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, visualize the results:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，展示结果：
- en: '![](img/72fd3041-d160-41b6-b57b-97cee44d0df9.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/72fd3041-d160-41b6-b57b-97cee44d0df9.png)'
- en: 'The results look pretty good. Like PCA, there is explained variance with `explained_variance_ratio_`:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来相当不错。与PCA一样，有`explained_variance_ratio_`的解释方差：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Now that we've walked through how, is performed in scikit-learn, let's look
    at how we can use only SciPy, and learn a bit in the process.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了在scikit-learn中如何执行，让我们看看如何只使用SciPy，并在这个过程中学到一些东西。
- en: 'First, we need to use SciPy''s `linalg` to perform SVD:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要使用SciPy的`linalg`执行SVD：
- en: '[PRE19]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can reconstruct the original matrix `D` to confirm `U`, `S`, and `V` as
    a decomposition:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重构原始矩阵`D`，以确认`U`、`S`和`V`作为分解：
- en: '[PRE20]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The matrix that is actually returned by truncated SVD is the dot product of
    the `U` and `S` matrices. If we want to simulate the truncation, we will drop
    the smallest singular values and the corresponding column vectors of `U`. So,
    if we want a single component here, we do the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，由截断SVD返回的矩阵是`U`和`S`矩阵的点积。如果我们想模拟截断，我们将丢弃最小的奇异值及其对应的`U`列向量。因此，如果我们想要一个单一的组件，我们将执行以下操作：
- en: '[PRE21]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In general, if we want to truncate to some dimensionality, for example, *t*,
    we drop *N - t* singular values.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，如果我们想要截断到某个维度，例如*t*，我们会丢弃*N - t*个奇异值。
- en: There's more...
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: Truncated SVD has a few miscellaneous things that are worth noting with respect
    to the method.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 截断SVD有一些杂项内容值得注意，特别是在方法方面。
- en: Sign flipping
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 符号翻转
- en: There's a gotcha with truncated SVDs. Depending on the state of the random number
    generator, successive fittings of truncated SVD can flip the signs of the output.
    In order to avoid this, it's advisable to fit truncated SVD once, and then use
    transforms from then on. This is another good reason for pipelines!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 截断SVD有个陷阱。根据随机数生成器的状态，连续应用截断SVD可能会翻转输出的符号。为了避免这种情况，建议只进行一次截断SVD拟合，然后从那时起使用变换。这是管道方法的另一个好理由！
- en: 'To carry this out, do the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，请执行以下操作：
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Sparse matrices
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 稀疏矩阵
- en: One advantage of truncated SVD over PCA is that truncated SVD can operate on
    sparse matrices, while PCA cannot. This is due to the fact that the covariance
    matrix must be computed for PCA, which requires operating on the entire matrix.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 截断SVD相对于PCA的一个优势是，截断SVD可以作用于稀疏矩阵，而PCA不能。这是因为PCA必须计算协方差矩阵，而这需要在整个矩阵上进行操作。
- en: Using decomposition to classify with DictionaryLearning
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分解方法通过DictionaryLearning进行分类
- en: In this recipe, we'll show how a decomposition method can actually be used for
    classification. `DictionaryLearning` attempts to take a dataset and transform
    it into a sparse representation.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将展示如何使用分解方法进行分类。`DictionaryLearning`试图将数据集转化为稀疏表示。
- en: Getting ready
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'With `DictionaryLearning`, the idea is that the features are the basis for
    the resulting datasets. Load the iris dataset:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DictionaryLearning`，其思路是特征是结果数据集的基础。加载iris数据集：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Additionally, create a training set by taking every other element of `iris_X`
    and `y`. Take the remaining elements for testing:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，通过取`iris_X`和`y`的每隔一个元素来创建训练集。剩下的元素用来进行测试：
- en: '[PRE24]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: How to do it...
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Import `DictionaryLearning`:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`DictionaryLearning`：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Use three components to represent the three species of iris:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用三个组件来表示三种鸢尾花：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Transform every other data point so that we can test the classifier on the
    resulting data points after the learner is trained:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换每隔一个数据点，这样我们就可以在学习器训练后，在结果数据点上测试分类器：
- en: '[PRE27]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now test the transform simply by typing the following:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在通过简单输入以下命令来测试变换：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can visualize the output. Notice how each value is sited on the *x*, *y*,
    or *z* axis, along with the other values and zero; this is called sparseness:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以可视化输出。注意每个值是如何在*x*、*y*或*z*轴上定位的，并且与其他值和零一起显示；这被称为稀疏性：
- en: '[PRE29]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![](img/c4c5530f-20de-4344-aa5b-67921fe012e5.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c4c5530f-20de-4344-aa5b-67921fe012e5.png)'
- en: If you look closely, you can see there was a training error. One of the classes
    was misclassified. Only being wrong once isn't a big deal, though. There was also
    an error in the classification. If you remember some of the other visualizations,
    the red and green classes were the two classes that often appeared close together.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，你会发现有一个训练错误。某个类别被误分类了。不过，错误只发生一次并不算大问题。分类中也有错误。如果你记得其他一些可视化，红色和绿色类别经常出现在彼此接近的位置。
- en: How it works...
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`DictionaryLearning` has a background in signal processing and neurology. The
    idea is that only few features can be active at any given time. Therefore, `DictionaryLearning`
    attempts to find a suitable representation of the underlying data, given the constraint
    that most of the features should be zero.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`DictionaryLearning`的背景包括信号处理和神经学。其思路是，在任何给定时刻，只有少数特征是活跃的。因此，`DictionaryLearning`试图在大多数特征应该为零的约束下，找到数据的合适表示。'
- en: Doing dimensionality reduction with manifolds – t-SNE
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用流形进行维度降维 – t-SNE
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: This is a short and practical recipe.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简短而实用的示例。
- en: 'If you read the rest of the chapter, we have been doing a lot of dimensionality
    reduction with the iris dataset. Let''s continue the pattern for additional easy
    comparisons. Load the iris dataset:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你阅读了本章的其余部分，你会发现我们已经在使用iris数据集进行很多维度降维。我们继续这种模式进行额外的简便比较。加载iris数据集：
- en: '[PRE30]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Load `PCA` and some classes from the `manifold` module:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 加载`PCA`以及`manifold`模块中的一些类：
- en: '[PRE31]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: How to do it...
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Run all the transforms on `iris_X`. One of the transforms is t-SNE:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对`iris_X`应用所有变换。其中一个变换是t-SNE：
- en: '[PRE32]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the results:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果：
- en: '[PRE33]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](img/cf664e0a-cb29-454d-88ae-c57c6156f69c.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cf664e0a-cb29-454d-88ae-c57c6156f69c.png)'
- en: The t-SNE algorithm has been popular recently, yet it takes a lot of computing
    time and power. ISO produces an interesting graphic.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE算法最近很受欢迎，但它需要大量的计算时间和算力。ISO生成了一个有趣的图形。
- en: 'Additionally, in cases where the dimensionality of the data is very high (more
    than 50 columns) the scikit-learn documentation suggests doing PCA or truncated
    SVD before t-SNE. The iris dataset is small, but we can write the syntax to perform
    t-SNE after PCA:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在数据的维度非常高（超过50列）的情况下，scikit-learn文档建议在t-SNE之前进行PCA或截断SVD。鸢尾花数据集较小，但我们可以编写语法，在PCA之后执行t-SNE：
- en: '[PRE34]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![](img/e1289eae-8719-49d6-a074-408e9ad9bc21.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1289eae-8719-49d6-a074-408e9ad9bc21.png)'
- en: How it works...
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In mathematics, a manifold is a space that is locally Euclidean at every point,
    yet is embedded in a higher-dimensional space. For example, the outer surface
    of a sphere is a two-dimensional manifold in three dimensions. When we walk around
    on the surface of the sphere of the Earth, we tend to perceive the 2D plane of
    the ground rather than all of 3D space. We navigate using 2D maps, not higher-dimensional
    ones.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，流形是一个在每个点局部欧几里得的空间，但它嵌入在更高维的空间中。例如，球体的外表面在三维空间中是一个二维流形。当我们在地球表面行走时，我们倾向于感知地面的二维平面，而不是整个三维空间。我们使用二维地图导航，而不是更高维的地图。
- en: The `manifold` module in scikit-learn is useful for understanding high-dimensional
    spaces in two or three dimensions. The algorithms in the module gather information
    about the local structure around a point and seek to preserve it. What are the
    neighbors of a point? How far away are the neighbors of a point?
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的`manifold`模块对于理解高维空间中的二维或三维空间非常有用。该模块中的算法收集关于某个点周围局部结构的信息，并试图保持这一结构。什么是一个点的邻居？一个点的邻居有多远？
- en: For example, the Isomap algorithm attempts to preserve geodesic distances between
    all of the points in an algorithm, starting with a nearest neighbor search, followed
    by a graph search, and then a partial eigenvalue decomposition. The point of the
    algorithm is to preserve distances and a manifold's local geometric structure.
    The **multi-dimensional scaling** (**MDS**) algorithm also respects distances.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Isomap算法试图在算法中保持所有点之间的测地距离，从最近邻搜索开始，接着是图搜索，再到部分特征值分解。该算法的目的是保持距离和流形的局部几何结构。**多维尺度法**（**MDS**）算法同样尊重距离。
- en: t-SNE converts Euclidean distances between pairs of points in the dataset into
    probabilities. Around each point there is a Gaussian centered at that point, and
    the probability distribution represents the chance of any other point being a
    neighbor. Points very far away from each other have a low chance of being neighbors.
    Here, we have turned point locations into distances and then probabilities. t-SNE
    maintains the local structure very well by utilizing the probabilities of two
    points being neighbors.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE将数据集中点对之间的欧几里得距离转化为概率。在每个点周围都有一个以该点为中心的高斯分布，概率分布表示任何其他点成为邻居的概率。相距很远的点，成为邻居的概率很低。在这里，我们将点的位置转化为距离，再转化为概率。t-SNE通过利用两点成为邻居的概率，能够很好地保持局部结构。
- en: In a very general sense manifold methods start by examining the neighbors of
    every point, which represent the local structure of a manifold, and attempt to
    preserve that local structure in different ways. It is similar to you walking
    around your neighborhood or block constructing a 2D map of the local structure
    around you and focusing on two dimensions rather than three.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 从非常一般的角度来看，流形方法通过检查每个点的邻居来开始，这些邻居表示流形的局部结构，并试图以不同的方式保持该局部结构。这类似于你在邻里或街区上走动，构建你周围局部结构的二维地图，并专注于二维而不是三维。
- en: Testing methods to reduce dimensionality with pipelines
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道进行降维的测试方法
- en: Here we will see how different estimators composed of dimensionality reduction
    and a support vector machine perform.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将看到由降维和支持向量机组成的不同估算器的表现。
- en: Getting ready
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Load the iris dataset and some dimensionality reduction libraries. This is
    a big step for this particular recipe:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 加载鸢尾花数据集和一些降维库。对于这个特定的步骤来说，这是一个重要的步骤：
- en: '[PRE35]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: How to do it...
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现...
- en: 'Instantiate a pipeline object with two main parts:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个包含两大部分的管道对象：
- en: An object to reduce dimensionality
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于降维的对象
- en: An estimator with a predict method
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有predict方法的估算器
- en: '[PRE36]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Note in the following code that Isomap comes from the `manifold` module and
    that the **n****on-negative matrix factorization** (**NMF**) algorithm utilizes
    SVDs to break up a matrix into non-negative factors, its main purpose in this
    section is to compare its performance with other algorithms, but it is useful
    in **natural language processing** (**NLP**) where matrix factorizations cannot
    be negative. Now type the following parameter grid:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意以下代码中，Isomap来自`manifold`模块，并且**非负矩阵分解**（**NMF**）算法利用SVD将矩阵分解为非负因子，其主要目的是与其他算法进行性能比较，但在**自然语言处理**（**NLP**）中非常有用，因为矩阵分解的结果不能为负。现在输入以下参数网格：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This parameter grid will allow scikit-learn to cycle through a few dimensionality
    reduction techniques coupled with two SVM types: linear SVC and SVC for classification.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这个参数网格将允许scikit-learn通过一些降维技术与两种SVM类型结合：线性SVC和用于分类的SVC。
- en: 'Now run a grid search:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在运行一次网格搜索：
- en: '[PRE38]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now look at the best parameters to determine the best model. A PCA with SVC
    was the best model:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在查看最佳参数，以确定最佳模型。使用PCA和SVC是最佳模型：
- en: '[PRE39]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If you would like to create a dataframe of results, use the following command:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你想创建一个结果的数据框，使用以下命令：
- en: '[PRE40]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Finally, you can predict on an unseen instance with the `grid.predict(X_test)` method for
    a testing set `X_test`. We will do several grid searches in later chapters.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以通过`grid.predict(X_test)`方法对未见实例进行预测，`X_test`是测试集。我们将在后续章节中进行几次网格搜索。
- en: How it works...
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: Grid search does cross-validation to determine the best score. In this case,
    all the data was used for three-fold cross-validation. For the rest of the book,
    we will save some data for testing to make sure the models do not run into anomalous
    behavior.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索进行交叉验证，以确定最佳得分。在这种情况下，所有数据都用于三折交叉验证。对于本书的其余部分，我们将保留一些数据用于测试，以确保模型不会出现异常行为。
- en: 'A final note on the pipeline you just saw: the `sklearn.decomposition` methods
    will work for the first step of reducing dimensionality within the pipeline, but
    not all of the manifold methods were designed for pipelines.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 关于你刚才看到的管道的最后一点：`sklearn.decomposition`方法将用于管道中的第一个降维步骤，但并非所有流形方法都设计为适用于管道。
