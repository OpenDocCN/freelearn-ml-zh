- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Examples and Case Studies
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä¾‹å­å’Œæ¡ˆä¾‹ç ”ç©¶
- en: 'This chapter embarks on a journey into the realm of machine learning, exploring
    practical applications and real-world examples that demonstrate its power and
    potential. In the previous chapters, we have learned all the essential skills
    required to build a good machine learning solution. In this chapter, we will utilize
    all the knowledge gained and build the following examples from scratch:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å¼€å§‹äº†ä¸€æ®µæ¢ç´¢æœºå™¨å­¦ä¹ é¢†åŸŸçš„æ—…ç¨‹ï¼Œæ¢è®¨äº†å…¶å®é™…åº”ç”¨å’Œç°å®ä¸–ç•Œçš„ä¾‹å­ï¼Œå±•ç¤ºäº†å…¶åŠ›é‡å’Œæ½œåŠ›ã€‚åœ¨å‰å‡ ç« ä¸­ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¹ äº†æ„å»ºè‰¯å¥½çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆæ‰€éœ€çš„æ‰€æœ‰åŸºæœ¬æŠ€èƒ½ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬å°†åˆ©ç”¨æ‰€è·å¾—çš„æ‰€æœ‰çŸ¥è¯†ï¼Œä»å¤´å¼€å§‹æ„å»ºä»¥ä¸‹ç¤ºä¾‹ï¼š
- en: Linear regression example
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’ç¤ºä¾‹
- en: Customer churn example
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å®¢æˆ·æµå¤±ç¤ºä¾‹
- en: Linear regression example
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çº¿æ€§å›å½’ç¤ºä¾‹
- en: In this example, we will create a linear regression model to predict the value
    of a house in the California area. Letâ€™s begin by getting familiar with the dataset.
    We will use a common California house values dataset. This is a collection of
    data related to residential real estate properties in various regions of California,
    USA. It is commonly used in machine learning and data analysis tasks for predicting
    house prices based on various features.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹æ¥é¢„æµ‹åŠ åˆ©ç¦å°¼äºšåœ°åŒºæˆ¿å±‹çš„ä»·å€¼ã€‚è®©æˆ‘ä»¬é¦–å…ˆç†Ÿæ‚‰æ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ä¸€ä¸ªå¸¸è§çš„åŠ åˆ©ç¦å°¼äºšæˆ¿å±‹ä»·å€¼æ•°æ®é›†ã€‚è¿™æ˜¯ä¸€ä¸ªå…³äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·å„ä¸ªåœ°åŒºä½å®…æˆ¿åœ°äº§æ•°æ®çš„é›†åˆï¼Œå¸¸ç”¨äºæœºå™¨å­¦ä¹ å’Œæ•°æ®åˆ†æä»»åŠ¡ä¸­ï¼ŒåŸºäºå„ç§ç‰¹å¾é¢„æµ‹æˆ¿ä»·ã€‚
- en: 'The dataset we will use contains the following fields:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä½¿ç”¨çš„æ•°æ®é›†åŒ…å«ä»¥ä¸‹å­—æ®µï¼š
- en: '`medianIncome`: The median income of households in a specific block.'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`medianIncome`: ç‰¹å®šåŒºåŸŸå®¶åº­çš„ä¸­ä½æ•°æ”¶å…¥ã€‚'
- en: '`housingMedianAge`: The median age of houses in a block.'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`housingMedianAge`: è¯¥åŒºåŸŸå†…æˆ¿å±‹çš„ä¸­ä½æ•°å¹´é¾„ã€‚'
- en: '`totalRooms`: The total number of rooms in the houses in a block.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`totalRooms`: è¯¥åŒºåŸŸå†…æˆ¿å±‹çš„æ€»æˆ¿é—´æ•°ã€‚'
- en: '`totalBedrooms`: The total number of bedrooms in the houses in a block.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`totalBedrooms`: è¯¥åŒºåŸŸå†…æˆ¿å±‹çš„æ€»å§å®¤æ•°ã€‚'
- en: '`population`: The total population of the block.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`population`: è¯¥åŒºåŸŸçš„æ€»äººå£ã€‚'
- en: '`households`: The total number of households (a group of people residing within
    a home unit) within a block.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`households`: è¯¥åŒºåŸŸå†…å®¶åº­çš„æ€»æ•°ï¼ˆå±…ä½åœ¨å®¶åº­å•å…ƒå†…çš„ä¸€ç¾¤äººï¼‰ã€‚'
- en: '`latitude`: The latitude of the geographical location of the house.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`latitude`: æˆ¿å±‹åœ°ç†ä½ç½®çš„çº¬åº¦ã€‚'
- en: '`longitude`: The longitude of the geographical location of the house.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`longitude`: æˆ¿å±‹åœ°ç†ä½ç½®çš„ç»åº¦ã€‚'
- en: '`medianHouseValue`: The median value of houses in the block.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`medianHouseValue`: è¯¥åŒºåŸŸå†…æˆ¿å±‹çš„ä¸­ä½æ•°ä»·å€¼ã€‚'
- en: '`oceanProximity`: Categorical description of the distance to the ocean'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oceanProximity`: æµ·æ´‹è·ç¦»çš„ç±»åˆ«æè¿°'
- en: 'Here is a sample of the data:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æ•°æ®çš„ä¸€ä¸ªæ ·æœ¬ï¼š
- en: '![Figure 10.1: Sample data from a California housing dataset](img/B19863_10_01.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.1ï¼šåŠ åˆ©ç¦å°¼äºšä½æˆ¿æ•°æ®é›†çš„æ ·æœ¬æ•°æ®](img/B19863_10_01.jpg)'
- en: 'Figure 10.1: Sample data from a California housing dataset'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.1ï¼šåŠ åˆ©ç¦å°¼äºšä½æˆ¿æ•°æ®é›†çš„æ ·æœ¬æ•°æ®
- en: Note
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: The example dataset can be found in the GitHub repository of this book. A good
    place to find other datasets is, for example, [https://www.kaggle.com/datasets](https://www.kaggle.com/datasets).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ç¤ºä¾‹æ•°æ®é›†å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚å…¶ä»–æ•°æ®é›†çš„ä¸€ä¸ªå¥½åœ°æ–¹æ˜¯ï¼Œä¾‹å¦‚ï¼Œ[https://www.kaggle.com/datasets](https://www.kaggle.com/datasets)ã€‚
- en: 'The first step in our machine learning project is to define a question we want
    to answer using our model. In this case, we are using a rather simple historical
    dataset and therefore the framework used is modified a bit. Letâ€™s determine the
    following characteristics to start:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœºå™¨å­¦ä¹ é¡¹ç›®çš„ç¬¬ä¸€æ­¥æ˜¯å®šä¹‰æˆ‘ä»¬æƒ³ç”¨æˆ‘ä»¬çš„æ¨¡å‹å›ç­”çš„é—®é¢˜ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ä¸€ä¸ªç›¸å¯¹ç®€å•çš„å†å²æ•°æ®é›†ï¼Œå› æ­¤æ‰€ä½¿ç”¨çš„æ¡†æ¶ç•¥æœ‰ä¿®æ”¹ã€‚è®©æˆ‘ä»¬ç¡®å®šä»¥ä¸‹ç‰¹å¾ä»¥å¼€å§‹ï¼š
- en: '**Trigger**: A new house data is inserted into the dataset'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è§¦å‘å™¨**: æ•°æ®é›†ä¸­æ’å…¥æ–°çš„æˆ¿å±‹æ•°æ®'
- en: '**Target**: The value of the house in US dollars'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**: ç¾å…ƒè®¡ä»·çš„æˆ¿å±‹ä»·å€¼'
- en: '**Features**: Latitude, longitude, median age, total rooms, total bedrooms,
    population, households, median income, ocean proximity'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç‰¹å¾**: çº¬åº¦ï¼Œç»åº¦ï¼Œä¸­ä½æ•°å¹´é¾„ï¼Œæ€»æˆ¿é—´æ•°ï¼Œæ€»å§å®¤æ•°ï¼Œäººå£ï¼Œå®¶åº­æ•°ï¼Œä¸­ä½æ•°æ”¶å…¥ï¼Œæµ·æ´‹æ¥è¿‘åº¦'
- en: '**Machine learning question**: Predicting what will the house value be in the
    California area?'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœºå™¨å­¦ä¹ é—®é¢˜**: é¢„æµ‹åŠ åˆ©ç¦å°¼äºšåœ°åŒºçš„æˆ¿å±‹ä»·å€¼ä¼šæ˜¯å¤šå°‘ï¼Ÿ'
- en: To begin our actual work, letâ€™s first upload `housing_test.csv` and `housing_train.csv`
    into our Qlik cloud tenant. These files can be found in the GitHub repository
    of this book. As you can see, the dataset is already split into train and test
    datasets.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†å¼€å§‹æˆ‘ä»¬çš„å®é™…å·¥ä½œï¼Œè®©æˆ‘ä»¬é¦–å…ˆå°†`housing_test.csv`å’Œ`housing_train.csv`ä¸Šä¼ åˆ°æˆ‘ä»¬çš„Qlikäº‘ç§Ÿæˆ·ã€‚è¿™äº›æ–‡ä»¶å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚å¦‚æ‚¨æ‰€è§ï¼Œæ•°æ®é›†å·²ç»è¢«åˆ†å‰²æˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ã€‚
- en: In a normal machine learning project, we would need to take care of encoding
    the categorical fields, handling null values, scaling and so on, but in our case,
    Qlik AutoML takes care of all these steps. Our next task is to create a new machine
    learning experiment (**Add New** ğŸ¡ª **New** **ML Experiment**).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸€ä¸ªæ­£å¸¸çš„æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­ï¼Œæˆ‘ä»¬éœ€è¦æ³¨æ„å¯¹åˆ†ç±»å­—æ®µè¿›è¡Œç¼–ç ã€å¤„ç†ç©ºå€¼ã€ç¼©æ”¾ç­‰ï¼Œä½†åœ¨è¿™ä¸ªæ¡ˆä¾‹ä¸­ï¼ŒQlik AutoMLè´Ÿè´£æ‰€æœ‰è¿™äº›æ­¥éª¤ã€‚æˆ‘ä»¬çš„ä¸‹ä¸€ä¸ªä»»åŠ¡æ˜¯åˆ›å»ºä¸€ä¸ªæ–°çš„æœºå™¨å­¦ä¹ å®éªŒï¼ˆ**æ·»åŠ æ–°**
    ğŸ¡ª **æ–°** **MLå®éªŒ**ï¼‰ã€‚
- en: 'Give a name to your experiment, define a space you want to use, and press `housing_train.csv`,
    which we uploaded earlier. You should see the following:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ç»™æ‚¨çš„å®éªŒèµ·ä¸€ä¸ªåå­—ï¼Œå®šä¹‰æ‚¨æƒ³è¦ä½¿ç”¨çš„ç©ºé—´ï¼Œç„¶åæŒ‰`housing_train.csv`ï¼Œè¿™æ˜¯æˆ‘ä»¬ä¹‹å‰ä¸Šä¼ çš„ã€‚ä½ åº”è¯¥ä¼šçœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: '![Figure 10.2: Housing prices experiment â€“ target selection](img/B19863_10_02.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.2ï¼šæˆ¿ä»·å®éªŒ â€“ ç›®æ ‡é€‰æ‹©](img/B19863_10_02.jpg)'
- en: 'Figure 10.2: Housing prices experiment â€“ target selection'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.2ï¼šæˆ¿ä»·å®éªŒ â€“ ç›®æ ‡é€‰æ‹©
- en: 'Next, we will select our target variable. We can also select the features to
    be used in our experiment. Select `median_house_value` as the target and all other
    fields should be automatically selected to be included in our experiment. You
    should see something like the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€‰æ‹©æˆ‘ä»¬çš„ç›®æ ‡å˜é‡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥é€‰æ‹©ç”¨äºå®éªŒçš„ç‰¹å¾ã€‚é€‰æ‹©`median_house_value`ä½œä¸ºç›®æ ‡ï¼Œæ‰€æœ‰å…¶ä»–å­—æ®µåº”è‡ªåŠ¨é€‰æ‹©ä»¥åŒ…å«åœ¨æˆ‘ä»¬çš„å®éªŒä¸­ã€‚ä½ åº”è¯¥ä¼šçœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: '![Figure 10.3: Target and features selected](img/B19863_10_03.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.3ï¼šé€‰å®šçš„ç›®æ ‡å’Œç‰¹å¾](img/B19863_10_03.jpg)'
- en: 'Figure 10.3: Target and features selected'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.3ï¼šé€‰å®šçš„ç›®æ ‡å’Œç‰¹å¾
- en: 'In the previous image, we have also marked the feature type of `total_bedrooms`
    with a red square. Qlik has recognized this field as a string and forms a categorical
    feature by default. Change that to `total_bedrooms`, we can select **Run Experiment**
    from the bottom right corner. After a while, you should see the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¹‹å‰çš„å›¾åƒä¸­ï¼Œæˆ‘ä»¬è¿˜ç”¨çº¢è‰²æ–¹å—æ ‡è®°äº†`total_bedrooms`çš„ç‰¹å¾ç±»å‹ã€‚Qlikå·²ç»è¯†åˆ«è¿™ä¸ªå­—æ®µä¸ºå­—ç¬¦ä¸²ï¼Œå¹¶é»˜è®¤å°†å…¶å½¢æˆä¸€ä¸ªåˆ†ç±»ç‰¹å¾ã€‚å°†å…¶æ›´æ”¹ä¸º`total_bedrooms`ï¼Œæˆ‘ä»¬å¯ä»¥ä»å³ä¸‹è§’é€‰æ‹©**è¿è¡Œå®éªŒ**ã€‚è¿‡äº†ä¸€ä¼šå„¿ï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: '![Figure 10.4: Housing prices experiment â€“ first results](img/B19863_10_04.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.4ï¼šæˆ¿ä»·å®éªŒ â€“ ç¬¬ä¸€æ¬¡ç»“æœ](img/B19863_10_04.jpg)'
- en: 'Figure 10.4: Housing prices experiment â€“ first results'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.4ï¼šæˆ¿ä»·å®éªŒ â€“ ç¬¬ä¸€æ¬¡ç»“æœ
- en: When looking at the SHAP diagrams for our first version of the experiment, we
    can see that the `median_income` field has a rather high correlation with the
    predicted house values. Letâ€™s try to configure a second version of our experiment
    without that field.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æŸ¥çœ‹æˆ‘ä»¬å®éªŒçš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬çš„SHAPå›¾æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°`median_income`å­—æ®µä¸é¢„æµ‹çš„æˆ¿ä»·æœ‰ç›¸å½“é«˜çš„ç›¸å…³æ€§ã€‚è®©æˆ‘ä»¬å°è¯•é…ç½®ä¸€ä¸ªæ²¡æœ‰è¯¥å­—æ®µçš„ç¬¬äºŒä¸ªç‰ˆæœ¬çš„å®éªŒã€‚
- en: 'Select `median_income` as in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä¸‹å›¾ä¸­é€‰æ‹©`median_income`ï¼š
- en: '![Figure 10.5: Features reconfiguration](img/B19863_10_05.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.5ï¼šç‰¹å¾é‡æ–°é…ç½®](img/B19863_10_05.jpg)'
- en: 'Figure 10.5: Features reconfiguration'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.5ï¼šç‰¹å¾é‡æ–°é…ç½®
- en: Select `total_rooms` is the most determining feature, but our R2 score has also
    dropped. In this case, we will go with the first version of our experiment since
    it gave us better accuracy. You can try to configure multiple versions and experiment
    with the models to get a better model.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©`total_rooms`æ˜¯æœ€å†³å®šæ€§çš„ç‰¹å¾ï¼Œä½†æˆ‘ä»¬çš„R2åˆ†æ•°ä¹Ÿæœ‰æ‰€ä¸‹é™ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†é€‰æ‹©å®éªŒçš„ç¬¬ä¸€ä¸ªç‰ˆæœ¬ï¼Œå› ä¸ºå®ƒç»™äº†æˆ‘ä»¬æ›´å¥½çš„å‡†ç¡®åº¦ã€‚æ‚¨å¯ä»¥å°è¯•é…ç½®å¤šä¸ªç‰ˆæœ¬å¹¶å®éªŒæ¨¡å‹ä»¥è·å¾—æ›´å¥½çš„æ¨¡å‹ã€‚
- en: 'In the list at the top part of the screen, scroll down until you see the top-performing
    model of the first run and select it. Your screen should look like the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å±å¹•é¡¶éƒ¨éƒ¨åˆ†çš„åˆ—è¡¨ä¸­ï¼Œå‘ä¸‹æ»šåŠ¨ç›´åˆ°æ‚¨çœ‹åˆ°ç¬¬ä¸€æ¬¡è¿è¡Œçš„æœ€ä½³æ¨¡å‹å¹¶é€‰æ‹©å®ƒã€‚æ‚¨çš„å±å¹•åº”è¯¥å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '![Figure 10.6: The model selected for deployment.](img/B19863_10_06.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.6ï¼šéƒ¨ç½²æ‰€é€‰æ¨¡å‹](img/B19863_10_06.jpg)'
- en: 'Figure 10.6: The model selected for deployment.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.6ï¼šéƒ¨ç½²æ‰€é€‰æ¨¡å‹ã€‚
- en: Select **Deploy** from the bottom right corner. Enter a name and define the
    space for your newly deployed model. Make sure that **Enable real-time API access**
    is selected and press **Deploy**. Our model is now deployed and ready for use.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä»å³ä¸‹è§’é€‰æ‹©**éƒ¨ç½²**ã€‚è¾“å…¥ä¸€ä¸ªåç§°å¹¶å®šä¹‰æ‚¨æ–°éƒ¨ç½²çš„æ¨¡å‹çš„ç©ºé—´ã€‚ç¡®ä¿**å¯ç”¨å®æ—¶APIè®¿é—®**è¢«é€‰ä¸­ï¼Œç„¶åæŒ‰**éƒ¨ç½²**ã€‚æˆ‘ä»¬çš„æ¨¡å‹ç°åœ¨å·²éƒ¨ç½²å¹¶å‡†å¤‡å¥½ä½¿ç”¨ã€‚
- en: As we have learned in previous chapters, the deployed model itself provides
    information about the required schema, algorithm deployed, and some metadata from
    the experiment. You can open the deployed model and have a closer look at this
    information at this point.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨å‰é¢çš„ç« èŠ‚ä¸­å­¦åˆ°çš„ï¼Œéƒ¨ç½²çš„æ¨¡å‹æœ¬èº«æä¾›äº†æœ‰å…³æ‰€éœ€æ¨¡å¼ã€éƒ¨ç½²çš„ç®—æ³•å’Œå®éªŒçš„ä¸€äº›å…ƒæ•°æ®çš„ä¿¡æ¯ã€‚æ‚¨ç°åœ¨å¯ä»¥æ‰“å¼€éƒ¨ç½²çš„æ¨¡å‹å¹¶æ›´è¯¦ç»†åœ°æŸ¥çœ‹è¿™äº›ä¿¡æ¯ã€‚
- en: Our next task to get the predicted results into a finalized application is to
    create a new Qlik analytics application.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸‹ä¸€ä¸ªä»»åŠ¡æ˜¯å°†é¢„æµ‹ç»“æœæ”¾å…¥æœ€ç»ˆçš„åº”ç”¨ç¨‹åºä¸­ï¼Œéœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„Qlikåˆ†æåº”ç”¨ç¨‹åºã€‚
- en: For the data in our application, we will import `housing_test.csv`. After that,
    we will create a new data connection for the Qlik AutoML model that we deployed
    in the earlier step.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬åº”ç”¨ç¨‹åºä¸­çš„æ•°æ®ï¼Œæˆ‘ä»¬å°†å¯¼å…¥`housing_test.csv`ã€‚ä¹‹åï¼Œæˆ‘ä»¬å°†ä¸ºæˆ‘ä»¬åœ¨æ—©æœŸæ­¥éª¤ä¸­éƒ¨ç½²çš„Qlik AutoMLæ¨¡å‹åˆ›å»ºä¸€ä¸ªæ–°çš„æ•°æ®è¿æ¥ã€‚
- en: 'Create a new Qlik AutoML connection under `Id`. The connector settings should
    look similar to the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨`Id`ä¸‹åˆ›å»ºä¸€ä¸ªæ–°çš„Qlik AutoMLè¿æ¥ã€‚è¿æ¥å™¨è®¾ç½®åº”ç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š
- en: '![Figure 10.7: Connector settings](img/B19863_10_07.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.7ï¼šè¿æ¥å™¨è®¾ç½®](img/B19863_10_07.jpg)'
- en: 'Figure 10.7: Connector settings'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.7ï¼šè¿æ¥å™¨è®¾ç½®
- en: 'Test the connection and save it after that. Next, we will select the data to
    load. In the `housing_test.csv` and select `housing_predictions` to be included.
    You should see the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: æµ‹è¯•è¿æ¥å¹¶åœ¨ä¹‹åä¿å­˜å®ƒã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†é€‰æ‹©è¦åŠ è½½çš„æ•°æ®ã€‚åœ¨`housing_test.csv`ä¸­ï¼Œé€‰æ‹©`housing_predictions`ä»¥åŒ…å«åœ¨å†…ã€‚ä½ åº”è¯¥çœ‹åˆ°ä»¥ä¸‹å†…å®¹ï¼š
- en: '![Figure 10.8: The Select data to load window.](img/B19863_10_08.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.8ï¼šé€‰æ‹©è¦åŠ è½½çš„æ•°æ®çª—å£ã€‚](img/B19863_10_08.jpg)'
- en: 'Figure 10.8: The Select data to load window.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.8ï¼šé€‰æ‹©è¦åŠ è½½çš„æ•°æ®çª—å£ã€‚
- en: Select **Insert script** and load the data into the application. You are now
    ready to create the actual dashboard.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: é€‰æ‹©**æ’å…¥è„šæœ¬**å¹¶å°†æ•°æ®åŠ è½½åˆ°åº”ç”¨ç¨‹åºä¸­ã€‚ä½ ç°åœ¨å¯ä»¥åˆ›å»ºå®é™…çš„ä»ªè¡¨æ¿äº†ã€‚
- en: Note
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„
- en: If you run into problems during the application creation, there is a sample
    application in the Github repository of this book for your reference.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åœ¨åˆ›å»ºåº”ç”¨ç¨‹åºæ—¶é‡åˆ°é—®é¢˜ï¼Œè¿™æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æœ‰ç¤ºä¾‹åº”ç”¨ç¨‹åºä¾›ä½ å‚è€ƒã€‚
- en: We will not cover the dashboard creation part in detail to give you a chance
    to play with the different visualization options. The following image represents
    a sample dashboard that shows predicted house values on a map using a gradient
    color scheme, a histogram to show the distribution of prices, and a waterfall
    diagram to visualize SHAP values. Use the skills acquired from previous chapters
    and create a dashboard of your own to visualize the data.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ä¸ä¼šè¯¦ç»†è®²è§£ä»ªè¡¨æ¿åˆ›å»ºéƒ¨åˆ†ï¼Œä»¥ä¾¿ç»™ä½ æœºä¼šå°è¯•ä¸åŒçš„å¯è§†åŒ–é€‰é¡¹ã€‚ä»¥ä¸‹å›¾åƒä»£è¡¨ä¸€ä¸ªç¤ºä¾‹ä»ªè¡¨æ¿ï¼Œå®ƒä½¿ç”¨æ¸å˜è‰²æ–¹æ¡ˆåœ¨åœ°å›¾ä¸Šæ˜¾ç¤ºé¢„æµ‹çš„æˆ¿å±‹ä»·å€¼ï¼Œä½¿ç”¨ç›´æ–¹å›¾æ˜¾ç¤ºä»·æ ¼åˆ†å¸ƒï¼Œä»¥åŠä½¿ç”¨ç€‘å¸ƒå›¾å¯è§†åŒ–SHAPå€¼ã€‚åˆ©ç”¨å‰å‡ ç« å­¦åˆ°çš„æŠ€èƒ½ï¼Œåˆ›å»ºä¸€ä¸ªè‡ªå·±çš„ä»ªè¡¨æ¿æ¥å¯è§†åŒ–æ•°æ®ã€‚
- en: '![Figure 10.9: Housing prices â€“ sample dashboard](img/B19863_10_09.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.9ï¼šæˆ¿ä»· â€“ ç¤ºä¾‹ä»ªè¡¨æ¿](img/B19863_10_09.jpg)'
- en: 'Figure 10.9: Housing prices â€“ sample dashboard'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.9ï¼šæˆ¿ä»· â€“ ç¤ºä¾‹ä»ªè¡¨æ¿
- en: You can create multiple dashboards and try to cross-reference the data from
    multiple models if you re-run the experiment with different parameters. Try to
    play with the different settings and graph types to find an effective visualization.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ ç”¨ä¸åŒçš„å‚æ•°é‡æ–°è¿è¡Œå®éªŒï¼Œå¯ä»¥åˆ›å»ºå¤šä¸ªä»ªè¡¨æ¿å¹¶å°è¯•äº¤å‰å¼•ç”¨å¤šä¸ªæ¨¡å‹çš„æ•°æ®ã€‚å°è¯•è°ƒæ•´ä¸åŒçš„è®¾ç½®å’Œå›¾è¡¨ç±»å‹ï¼Œä»¥æ‰¾åˆ°æœ‰æ•ˆçš„å¯è§†åŒ–æ–¹æ³•ã€‚
- en: Now that we have implemented a linear regression example, it is time to move
    on to another example with a slightly more complex scenario. We will investigate
    the customer churn example next.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®ç°äº†ä¸€ä¸ªçº¿æ€§å›å½’ç¤ºä¾‹ï¼Œæ˜¯æ—¶å€™è½¬å‘å¦ä¸€ä¸ªç¨å¾®å¤æ‚ä¸€ç‚¹çš„ç¤ºä¾‹äº†ã€‚æˆ‘ä»¬å°†ç ”ç©¶å®¢æˆ·æµå¤±ç¤ºä¾‹ã€‚
- en: Customer churn example
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å®¢æˆ·æµå¤±ç¤ºä¾‹
- en: 'In our second example, we will create a binary model to predict customer churn
    for a bank. We are going to use a dataset that contains the following fields:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¬¬äºŒä¸ªç¤ºä¾‹ä¸­ï¼Œæˆ‘ä»¬å°†åˆ›å»ºä¸€ä¸ªäºŒè¿›åˆ¶æ¨¡å‹æ¥é¢„æµ‹é“¶è¡Œå®¢æˆ·çš„å®¢æˆ·æµå¤±ã€‚æˆ‘ä»¬å°†ä½¿ç”¨åŒ…å«ä»¥ä¸‹å­—æ®µçš„æ•°æ®é›†ï¼š
- en: '`customer_id`: A unique identifier for each customer.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`customer_id`ï¼šæ¯ä¸ªå®¢æˆ·çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚'
- en: '`credit_score`: A numerical representation of a customerâ€™s creditworthiness.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`credit_score`ï¼šå®¢æˆ·ä¿¡ç”¨åº¦çš„æ•°å€¼è¡¨ç¤ºã€‚'
- en: '`country`: The country where the customer resides.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`country`ï¼šå®¢æˆ·å±…ä½çš„å›½å®¶ã€‚'
- en: '`gender`: The gender of the customer.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gender`ï¼šå®¢æˆ·çš„æ€§åˆ«ã€‚'
- en: '`age`: The age of the customer.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`ï¼šå®¢æˆ·çš„å¹´é¾„ã€‚'
- en: '`tenure`: The duration of the customerâ€™s relationship with the company.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tenure`ï¼šå®¢æˆ·ä¸å…¬å¸å…³ç³»çš„æŒç»­æ—¶é—´ã€‚'
- en: '`balance`: The current balance in the customerâ€™s account.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`balance`ï¼šå®¢æˆ·è´¦æˆ·ä¸­çš„å½“å‰ä½™é¢ã€‚'
- en: '`products_number`: The number of products the customer has brought from the
    company.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`products_number`ï¼šå®¢æˆ·ä»å…¬å¸è´­ä¹°çš„äº§å“æ•°é‡ã€‚'
- en: '`credit_card`: A binary indicator showing whether the customer holds a credit
    card with the company.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`credit_card`ï¼šä¸€ä¸ªäºŒè¿›åˆ¶æŒ‡ç¤ºå™¨ï¼Œæ˜¾ç¤ºå®¢æˆ·æ˜¯å¦æŒæœ‰å…¬å¸çš„ä¿¡ç”¨å¡ã€‚'
- en: '`active_member`: A binary indicator indicating whether the customer is currently
    an active member of the company.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`active_member`ï¼šä¸€ä¸ªäºŒè¿›åˆ¶æŒ‡ç¤ºå™¨ï¼Œè¡¨ç¤ºå®¢æˆ·æ˜¯å¦ç›®å‰æ˜¯å…¬å¸çš„æ´»è·ƒæˆå‘˜ã€‚'
- en: '`estimated_salary`: An approximate estimation of the customerâ€™s salary.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`estimated_salary`ï¼šå¯¹å®¢æˆ·å·¥èµ„çš„å¤§è‡´ä¼°è®¡ã€‚'
- en: '`churn`: A binary indicator showing whether the customer has churned (`1`)
    or not (`0`). Churning refers to customers who have ended their relationship with
    the company.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`churn`ï¼šä¸€ä¸ªäºŒå…ƒæŒ‡ç¤ºå™¨ï¼Œæ˜¾ç¤ºå®¢æˆ·æ˜¯å¦å·²ç»æµå¤±ï¼ˆ`1`ï¼‰æˆ–æœªæµå¤±ï¼ˆ`0`ï¼‰ã€‚æµå¤±æŒ‡çš„æ˜¯ä¸å…¬å¸ç»“æŸå…³ç³»çš„å®¢æˆ·ã€‚'
- en: 'Here is a sample of the dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯æ•°æ®é›†çš„ä¸€ä¸ªæ ·æœ¬ï¼š
- en: '![Figure 10.10: Customer churn â€“ sample data](img/B19863_10_10.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.10ï¼šå®¢æˆ·æµå¤± - æ ·æœ¬æ•°æ®](img/B19863_10_10.jpg)'
- en: 'Figure 10.10: Customer churn â€“ sample data'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.10ï¼šå®¢æˆ·æµå¤± - æ ·æœ¬æ•°æ®
- en: To start with our machine learning project, we will first upload `Bank Customer
    Churn Prediction.csv` into Qlik Cloud. This data file can be found in the GitHub
    repository of this book. We also must split the dataset. Qlik AutoML splits the
    dataset during the experiment phase, but we will also create separate training
    and test datasets to get a better understanding of the modelâ€™s performance. To
    split the dataset, we will use the training and test data in a ratio of 70:30.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹æˆ‘ä»¬çš„æœºå™¨å­¦ä¹ é¡¹ç›®ï¼Œæˆ‘ä»¬é¦–å…ˆå°†`Bank Customer Churn Prediction.csv`ä¸Šä¼ åˆ°Qlik Cloudã€‚è¿™ä¸ªæ•°æ®æ–‡ä»¶å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚æˆ‘ä»¬è¿˜å¿…é¡»åˆ†å‰²æ•°æ®é›†ã€‚Qlik
    AutoMLåœ¨å®éªŒé˜¶æ®µåˆ†å‰²æ•°æ®é›†ï¼Œä½†æˆ‘ä»¬å°†åˆ›å»ºå•ç‹¬çš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œä»¥æ›´å¥½åœ°ç†è§£æ¨¡å‹çš„æ€§èƒ½ã€‚ä¸ºäº†åˆ†å‰²æ•°æ®é›†ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨70:30çš„æ¯”ä¾‹æ¥åˆ†å‰²è®­ç»ƒå’Œæµ‹è¯•æ•°æ®ã€‚
- en: 'Splitting the dataset can be done in Qlik. To do that, we will first create
    a new analytics application and name it `Churn Data Prep`. Next, create a data
    connection to the location that contains the previously uploaded `Bank Customer
    Churn Prediction.csv` file. You can use the following code to do the splitting:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Qlikä¸­å¯ä»¥åˆ†å‰²æ•°æ®é›†ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†æåº”ç”¨ç¨‹åºï¼Œå¹¶å°†å…¶å‘½åä¸º`Churn Data Prep`ã€‚ç„¶åï¼Œåˆ›å»ºä¸€ä¸ªæ•°æ®è¿æ¥åˆ°åŒ…å«ä¹‹å‰ä¸Šä¼ çš„`Bank
    Customer Churn Prediction.csv`æ–‡ä»¶çš„ç›®å½•ã€‚æ‚¨å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç è¿›è¡Œåˆ†å‰²ï¼š
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding code reads `Bank Customer Churn.csv` using the file data connection
    and adds a row number to each row. Then, two subsets (training and test) are created
    using this row number and are stored in QVD files. The `Row number` field is dropped
    before storing because we donâ€™t need it in our machine learning project. As a
    result of the script, the new data files (`banking_churn_train.qvd` and `banking_churn_test.qvd`)
    are stored in the same location as the original data file.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹‹å‰çš„ä»£ç ä½¿ç”¨æ–‡ä»¶æ•°æ®è¿æ¥è¯»å–`Bank Customer Churn.csv`ï¼Œå¹¶ä¸ºæ¯ä¸€è¡Œæ·»åŠ ä¸€ä¸ªè¡Œå·ã€‚ç„¶åï¼Œä½¿ç”¨è¿™ä¸ªè¡Œå·åˆ›å»ºäº†ä¸¤ä¸ªå­é›†ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰ï¼Œå¹¶å°†å®ƒä»¬å­˜å‚¨åœ¨QVDæ–‡ä»¶ä¸­ã€‚åœ¨å­˜å‚¨ä¹‹å‰åˆ é™¤äº†`è¡Œå·`å­—æ®µï¼Œå› ä¸ºæˆ‘ä»¬ä¸éœ€è¦åœ¨æœºå™¨å­¦ä¹ é¡¹ç›®ä¸­ä½¿ç”¨å®ƒã€‚å› æ­¤ï¼Œè„šæœ¬ç”Ÿæˆçš„æ–°çš„æ•°æ®æ–‡ä»¶ï¼ˆ`banking_churn_train.qvd`å’Œ`banking_churn_test.qvd`ï¼‰å­˜å‚¨åœ¨åŸå§‹æ•°æ®æ–‡ä»¶ç›¸åŒçš„ç›®å½•ä¸‹ã€‚
- en: 'Next, we will start to investigate the data to form a machine learning question.
    Data exploration can be done in the same Qlik application where you split the
    data. Remember to drop the test and train tables before continuing with the visualizations.
    An example of an analysis view is represented in the following figure:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†å¼€å§‹è°ƒæŸ¥æ•°æ®ä»¥å½¢æˆæœºå™¨å­¦ä¹ é—®é¢˜ã€‚æ•°æ®æ¢ç´¢å¯ä»¥åœ¨æ‚¨åˆ†å‰²æ•°æ®çš„åŒä¸€Qlikåº”ç”¨ç¨‹åºä¸­è¿›è¡Œã€‚è®°ä½åœ¨ç»§ç»­è¿›è¡Œå¯è§†åŒ–ä¹‹å‰åˆ é™¤æµ‹è¯•å’Œè®­ç»ƒè¡¨ã€‚ä»¥ä¸‹å›¾è¡¨ç¤ºä¾‹äº†ä¸€ä¸ªåˆ†æè§†å›¾ï¼š
- en: '![Figure 10.11: Customer churn example - Initial analysis](img/B19863_10_11.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.11ï¼šå®¢æˆ·æµå¤±ç¤ºä¾‹ - åˆå§‹åˆ†æ](img/B19863_10_11.jpg)'
- en: 'Figure 10.11: Customer churn example - Initial analysis'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.11ï¼šå®¢æˆ·æµå¤±ç¤ºä¾‹ - åˆå§‹åˆ†æ
- en: If we plot the values using a histogram, we can see that `credit_score` and
    `age` follow a normal distribution. If we look at the balance, there are lot of
    zero values, but the rest of the data is normally distributed. If we look at the
    tenure, there are only a few customers with 1 year of tenure and 95 of those have
    been churned. This is important information when defining our prediction window.
    We can also see that there are no issues in the data. You can play with different
    visualizations when getting familiar with the data. An example can be found in
    the GitHub repository of this book.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç›´æ–¹å›¾æ¥ç»˜åˆ¶è¿™äº›å€¼ï¼Œæˆ‘ä»¬å¯ä»¥çœ‹åˆ°`credit_score`å’Œ`age`éµå¾ªæ­£æ€åˆ†å¸ƒã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä½™é¢ï¼Œæœ‰å¾ˆå¤šé›¶å€¼ï¼Œä½†å…¶ä½™æ•°æ®æ˜¯æ­£æ€åˆ†å¸ƒçš„ã€‚å¦‚æœæˆ‘ä»¬æŸ¥çœ‹ä»»æœŸï¼Œåªæœ‰å°‘æ•°å®¢æˆ·æœ‰1å¹´çš„ä»»æœŸï¼Œå…¶ä¸­95ä¸ªå·²ç»æµå¤±ã€‚è¿™åœ¨å®šä¹‰æˆ‘ä»¬çš„é¢„æµ‹çª—å£æ—¶æ˜¯é‡è¦ä¿¡æ¯ã€‚æˆ‘ä»¬è¿˜å¯ä»¥çœ‹åˆ°æ•°æ®ä¸­æ²¡æœ‰é—®é¢˜ã€‚æ‚¨å¯ä»¥åœ¨ç†Ÿæ‚‰æ•°æ®æ—¶å°è¯•ä¸åŒçš„å¯è§†åŒ–ã€‚ä¸€ä¸ªä¾‹å­å¯ä»¥åœ¨æœ¬ä¹¦çš„GitHubä»“åº“ä¸­æ‰¾åˆ°ã€‚
- en: 'After investigating the data, we can use the following framework to form our
    machine learning question:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒæŸ¥æ•°æ®åï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æ¡†æ¶æ¥å½¢æˆæˆ‘ä»¬çš„æœºå™¨å­¦ä¹ é—®é¢˜ï¼š
- en: '**Event trigger**: When a new customer subscribes'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**äº‹ä»¶è§¦å‘å™¨**ï¼šå½“æ–°å®¢æˆ·è®¢é˜…æ—¶'
- en: '**Target**: When a customer leaves the company services (churn)'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç›®æ ‡**ï¼šå½“å®¢æˆ·ç¦»å¼€å…¬å¸æœåŠ¡ï¼ˆæµå¤±ï¼‰'
- en: 'Binary outcome: Yes or No'
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: äºŒå…ƒç»“æœï¼šæ˜¯æˆ–å¦
- en: The horizon is based on the average churned customer tenure length (around five
    years)
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¢„æµ‹èŒƒå›´åŸºäºå¹³å‡æµå¤±å®¢æˆ·ä»»æœŸé•¿åº¦ï¼ˆå¤§çº¦äº”å¹´ï¼‰
- en: '`active_member`, `age`, `balance`, `country`, `credit_card`, `credit_score`,
    `estimated_salary`, `gender`, `products_number`, and `tenure`'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`æ´»è·ƒä¼šå‘˜`ã€`å¹´é¾„`ã€`ä½™é¢`ã€`å›½å®¶`ã€`ä¿¡ç”¨å¡`ã€`ä¿¡ç”¨è¯„åˆ†`ã€`ä¼°è®¡å·¥èµ„`ã€`æ€§åˆ«`ã€`äº§å“æ•°é‡`å’Œ`ä»»æœŸ`'
- en: '**Prediction point**: One year after subscription'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**é¢„æµ‹ç‚¹**ï¼šè®¢é˜…åä¸€å¹´'
- en: '**Machine learning question**: After one year of activity as a customer, will
    the customer churn during the first five years?'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœºå™¨å­¦ä¹ é—®é¢˜**ï¼šä½œä¸ºå®¢æˆ·æ´»åŠ¨ä¸€å¹´åï¼Œå®¢æˆ·åœ¨ç¬¬ä¸€äº”å¹´å†…ä¼šæµå¤±å—ï¼Ÿ'
- en: By defining our model using the framework, we have defined that after a new
    customer has signed, we will collect data during the first year and then predict
    whether the customer will churn during the first five years. We can re-calculate
    the predictions periodically after the initial results when we get new data (for
    example, every six months after the initial results). Since we had a minimal number
    of customers that churned during the first year, our data accumulation window
    (the time between the event trigger and the prediction point) is not too long.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨æ¡†æ¶å®šä¹‰æˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å®šä¹‰äº†åœ¨æ–°å®¢æˆ·ç­¾çº¦åï¼Œæˆ‘ä»¬å°†æ”¶é›†ç¬¬ä¸€å¹´çš„æ•°æ®ï¼Œç„¶åé¢„æµ‹å®¢æˆ·åœ¨ç¬¬ä¸€äº”å¹´å†…æ˜¯å¦ä¼šæµå¤±ã€‚åœ¨è·å¾—æ–°æ•°æ®ï¼ˆä¾‹å¦‚ï¼Œåœ¨åˆå§‹ç»“æœåçš„æ¯å…­ä¸ªæœˆï¼‰åï¼Œæˆ‘ä»¬å¯ä»¥åœ¨åˆå§‹ç»“æœä¹‹åå®šæœŸé‡æ–°è®¡ç®—é¢„æµ‹ã€‚ç”±äºæˆ‘ä»¬åœ¨ç¬¬ä¸€å¹´å†…æµå¤±çš„å®¢æˆ·æ•°é‡å¾ˆå°‘ï¼Œæˆ‘ä»¬çš„æ•°æ®ç§¯ç´¯çª—å£ï¼ˆäº‹ä»¶è§¦å‘å’Œé¢„æµ‹ç‚¹ä¹‹é—´çš„æ—¶é—´ï¼‰å¹¶ä¸å¤ªé•¿ã€‚
- en: 'Letâ€™s now create the actual machine learning experiment using our training
    dataset. Start by creating a new experiment and select the correct dataset (`banking_churn_train.qvd`).
    Select `churn` as the target and all the other fields except `customer_id` as
    features. The following figure represents the first experiment setup:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„è®­ç»ƒæ•°æ®é›†åˆ›å»ºå®é™…çš„æœºå™¨å­¦ä¹ å®éªŒã€‚é¦–å…ˆåˆ›å»ºä¸€ä¸ªæ–°çš„å®éªŒï¼Œå¹¶é€‰æ‹©æ­£ç¡®çš„æ•°æ®é›†ï¼ˆ`banking_churn_train.qvd`ï¼‰ã€‚é€‰æ‹©`æµå¤±`ä½œä¸ºç›®æ ‡ï¼Œå¹¶å°†é™¤`customer_id`ä¹‹å¤–çš„æ‰€æœ‰å…¶ä»–å­—æ®µä½œä¸ºç‰¹å¾ã€‚ä»¥ä¸‹å›¾è¡¨ç¤ºç¬¬ä¸€ä¸ªå®éªŒè®¾ç½®ï¼š
- en: '![Figure 10.12: Experiment setup](img/B19863_10_12.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.12ï¼šå®éªŒè®¾ç½®](img/B19863_10_12.jpg)'
- en: 'Figure 10.12: Experiment setup'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.12ï¼šå®éªŒè®¾ç½®
- en: 'We can see that Qlik AutoML recognized the categorical features and will automatically
    apply one-hot encoding to these fields. You can now run the experiment. After
    the first run, the experiment returns the following results:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼ŒQlik AutoML è¯†åˆ«äº†åˆ†ç±»ç‰¹å¾ï¼Œå¹¶å°†è‡ªåŠ¨å°†è¿™äº›å­—æ®µåº”ç”¨ä¸€çƒ­ç¼–ç ã€‚ä½ ç°åœ¨å¯ä»¥è¿è¡Œå®éªŒã€‚ç¬¬ä¸€æ¬¡è¿è¡Œåï¼Œå®éªŒè¿”å›ä»¥ä¸‹ç»“æœï¼š
- en: '![Figure 10.13: First results from the experiment](img/B19863_10_13.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.13ï¼šå®éªŒçš„ç¬¬ä¸€æ‰¹ç»“æœ](img/B19863_10_13.jpg)'
- en: 'Figure 10.13: First results from the experiment'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.13ï¼šå®éªŒçš„ç¬¬ä¸€æ‰¹ç»“æœ
- en: 'As we can see from the confusion matrix, the rate of false negatives is quite
    large and our ROC curve indicates that our model is not performing well. From
    the permutation and SHAP importance graphs we can see that `age` and `products_number`
    correlate highly with the result. Letâ€™s try to make another run without these
    variables and see if we will get more accurate results. Select `age` and `products_number`
    and press **Run v2**. You should see the following results:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä»æ··æ·†çŸ©é˜µä¸­å¯ä»¥çœ‹åˆ°ï¼Œå‡é˜´æ€§çš„æ¯”ç‡ç›¸å½“å¤§ï¼Œæˆ‘ä»¬çš„ROCæ›²çº¿è¡¨æ˜æˆ‘ä»¬çš„æ¨¡å‹è¡¨ç°ä¸ä½³ã€‚ä»æ’åˆ—å’ŒSHAPé‡è¦æ€§å›¾ä¸­æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œ`å¹´é¾„`å’Œ`äº§å“æ•°é‡`ä¸ç»“æœé«˜åº¦ç›¸å…³ã€‚è®©æˆ‘ä»¬å°è¯•å†æ¬¡è¿è¡Œï¼Œä½†ä¸åŒ…æ‹¬è¿™äº›å˜é‡ï¼Œçœ‹çœ‹æˆ‘ä»¬æ˜¯å¦ä¼šå¾—åˆ°æ›´å‡†ç¡®çš„ç»“æœã€‚é€‰æ‹©`å¹´é¾„`å’Œ`äº§å“æ•°é‡`ï¼Œç„¶åæŒ‰**è¿è¡Œv2**ã€‚ä½ åº”è¯¥çœ‹åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![Figure 10.14: Results after modification](img/B19863_10_14.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.14ï¼šä¿®æ”¹åçš„ç»“æœ](img/B19863_10_14.jpg)'
- en: 'Figure 10.14: Results after modification'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.14ï¼šä¿®æ”¹åçš„ç»“æœ
- en: 'As we can see, the accuracy of the model dropped so our changes were not beneficial.
    After a few iterations, we will find the best combination of features. After that,
    it is possible to finetune the model even more by enabling hyperparameter optimization
    from the settings and defining a time window for that. You can try different combinations
    and investigate the model performance. Once you are done, configure the new version
    and select the following fields: `age`, `products_number`, `active_member`, `gender`,
    `balance`, and `country`. Also, enable hyperparameter optimization and set the
    window to one hour. You should see the following results:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬æ‰€è§ï¼Œæ¨¡å‹çš„å‡†ç¡®æ€§ä¸‹é™ï¼Œå› æ­¤æˆ‘ä»¬çš„æ›´æ”¹æ²¡æœ‰å¸¦æ¥å¥½å¤„ã€‚ç»è¿‡å‡ æ¬¡è¿­ä»£åï¼Œæˆ‘ä»¬å°†æ‰¾åˆ°æœ€ä½³çš„ç‰¹å¾ç»„åˆã€‚ä¹‹åï¼Œé€šè¿‡ä»è®¾ç½®ä¸­å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–å¹¶å®šä¹‰ä¸€ä¸ªæ—¶é—´çª—å£ï¼Œæˆ‘ä»¬å¯ä»¥è¿›ä¸€æ­¥å¾®è°ƒæ¨¡å‹ã€‚ä½ å¯ä»¥å°è¯•ä¸åŒçš„ç»„åˆå¹¶è°ƒæŸ¥æ¨¡å‹æ€§èƒ½ã€‚å®Œæˆåï¼Œé…ç½®æ–°ç‰ˆæœ¬å¹¶é€‰æ‹©ä»¥ä¸‹å­—æ®µï¼š`å¹´é¾„`ã€`äº§å“æ•°é‡`ã€`æ´»è·ƒä¼šå‘˜`ã€`æ€§åˆ«`ã€`ä½™é¢`å’Œ`å›½å®¶`ã€‚è¿˜è¦å¯ç”¨è¶…å‚æ•°ä¼˜åŒ–å¹¶å°†çª—å£è®¾ç½®ä¸ºä¸€ä¸ªå°æ—¶ã€‚ä½ åº”è¯¥çœ‹åˆ°ä»¥ä¸‹ç»“æœï¼š
- en: '![Figure 10.15: Results from the optimized experiment](img/B19863_10_15.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.15ï¼šä¼˜åŒ–å®éªŒçš„ç»“æœ](img/B19863_10_15.jpg)'
- en: 'Figure 10.15: Results from the optimized experiment'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.15ï¼šä¼˜åŒ–å®éªŒçš„ç»“æœ
- en: 'We can see that the accuracy of our final model is 84.4% and the F1 score is
    0.623\. The model is not a top performer but will give us relatively good results.
    Select the top-performing model and press **Deploy**. This will create a machine
    learning deployment for us. You can open the deployed model. Verify that you see
    the following schema:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬æœ€ç»ˆæ¨¡å‹çš„å‡†ç¡®ç‡ä¸º84.4%ï¼ŒF1åˆ†æ•°ä¸º0.623ã€‚è¯¥æ¨¡å‹å¹¶éè¡¨ç°æœ€ä½³ï¼Œä½†ä¼šç»™æˆ‘ä»¬å¸¦æ¥ç›¸å¯¹è¾ƒå¥½çš„ç»“æœã€‚é€‰æ‹©è¡¨ç°æœ€ä½³çš„æ¨¡å‹å¹¶æŒ‰ä¸‹**éƒ¨ç½²**ã€‚è¿™å°†ä¸ºæˆ‘ä»¬åˆ›å»ºä¸€ä¸ªæœºå™¨å­¦ä¹ éƒ¨ç½²ã€‚æ‚¨å¯ä»¥æ‰“å¼€å·²éƒ¨ç½²çš„æ¨¡å‹ã€‚éªŒè¯æ‚¨æ˜¯å¦çœ‹åˆ°äº†ä»¥ä¸‹æ¶æ„ï¼š
- en: '![Figure 10.16: Banking churn schema](img/B19863_10_16.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.16ï¼šé“¶è¡Œå®¢æˆ·æµå¤±æ¶æ„](img/B19863_10_16.jpg)'
- en: 'Figure 10.16: Banking churn schema'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.16ï¼šé“¶è¡Œå®¢æˆ·æµå¤±æ¶æ„
- en: 'We can now create a new analytics application and load `banking_churn_test.qvd`
    into it as a data table. You should do this in script view. We should also create
    a data connection to our newly deployed model. To create the correct data connection,
    select Qlik AutoML from the connection list and select the model from the dropdown
    menu. Give a name to the returned table and select SHAP -values and errors to
    be included. Type `customer_id` into **Association Field**. Your settings should
    look similar to the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ†æåº”ç”¨ç¨‹åºå¹¶å°†`banking_churn_test.qvd`åŠ è½½åˆ°å…¶ä¸­ä½œä¸ºæ•°æ®è¡¨ã€‚æ‚¨åº”è¯¥åœ¨è„šæœ¬è§†å›¾ä¸­è¿™æ ·åšã€‚æˆ‘ä»¬è¿˜åº”è¯¥åˆ›å»ºåˆ°æˆ‘ä»¬æ–°éƒ¨ç½²çš„æ¨¡å‹çš„æ•°æ®è¿æ¥ã€‚è¦åˆ›å»ºæ­£ç¡®çš„æ•°æ®è¿æ¥ï¼Œä»è¿æ¥åˆ—è¡¨ä¸­é€‰æ‹©Qlik
    AutoMLå¹¶ä»ä¸‹æ‹‰èœå•ä¸­é€‰æ‹©æ¨¡å‹ã€‚ä¸ºè¿”å›çš„è¡¨å‘½åå¹¶é€‰æ‹©è¦åŒ…å«çš„SHAPå€¼å’Œé”™è¯¯ã€‚åœ¨**å…³è”å­—æ®µ**ä¸­è¾“å…¥`customer_id`ã€‚æ‚¨çš„è®¾ç½®åº”ç±»ä¼¼äºä»¥ä¸‹å†…å®¹ï¼š
- en: '![Figure 10.17: Connection settings](img/B19863_10_17.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.17ï¼šè¿æ¥è®¾ç½®](img/B19863_10_17.jpg)'
- en: 'Figure 10.17: Connection settings'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.17ï¼šè¿æ¥è®¾ç½®
- en: 'Next, add the predictions to the script. Enter the name of the `banking_churn_test`
    table into the **Resident Table** field and select the result set. Select **Insert
    script**. Your code should look as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå°†é¢„æµ‹ç»“æœæ·»åŠ åˆ°è„šæœ¬ä¸­ã€‚åœ¨**å±…æ°‘è¡¨**å­—æ®µä¸­è¾“å…¥`banking_churn_test`è¡¨çš„åç§°å¹¶é€‰æ‹©ç»“æœé›†ã€‚é€‰æ‹©**æ’å…¥è„šæœ¬**ã€‚æ‚¨çš„ä»£ç åº”å¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE1]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We will load the test dataset in the preceding code, and then call the model
    endpoint through the data connector. After you complete the script, select **Load
    data**. Open **Data model viewer** and verify that you can see two tables connected.
    Next, we will focus on creating the actual application. You should try different
    visualization types and connect to the model using the **server-side extension**
    (**SSE**) syntax. An example dashboard may look like the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†åœ¨å‰é¢çš„ä»£ç ä¸­åŠ è½½æµ‹è¯•æ•°æ®é›†ï¼Œç„¶åé€šè¿‡æ•°æ®è¿æ¥å™¨è°ƒç”¨æ¨¡å‹ç«¯ç‚¹ã€‚å®Œæˆè„šæœ¬åï¼Œé€‰æ‹©**åŠ è½½æ•°æ®**ã€‚æ‰“å¼€**æ•°æ®æ¨¡å‹æŸ¥çœ‹å™¨**å¹¶éªŒè¯æ‚¨æ˜¯å¦å¯ä»¥çœ‹åˆ°ä¸¤ä¸ªè¡¨è¿æ¥åœ¨ä¸€èµ·ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†ä¸“æ³¨äºåˆ›å»ºå®é™…çš„åº”ç”¨ç¨‹åºã€‚æ‚¨åº”è¯¥å°è¯•ä¸åŒçš„å¯è§†åŒ–ç±»å‹ï¼Œå¹¶ä½¿ç”¨**æœåŠ¡å™¨ç«¯æ‰©å±•**ï¼ˆ**SSE**ï¼‰è¯­æ³•è¿æ¥åˆ°æ¨¡å‹ã€‚ä¸€ä¸ªç¤ºä¾‹ä»ªè¡¨æ¿å¯èƒ½çœ‹èµ·æ¥å¦‚ä¸‹ï¼š
- en: '![Figure 10.18: Churn analysis example](img/B19863_10_18.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.18ï¼šå®¢æˆ·æµå¤±åˆ†æç¤ºä¾‹](img/B19863_10_18.jpg)'
- en: 'Figure 10.18: Churn analysis example'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾10.18ï¼šå®¢æˆ·æµå¤±åˆ†æç¤ºä¾‹
- en: We will not cover the creation of every visualization in this chapter since
    you should experiment with the data and visualizations. A sample application is
    provided as part of the materials in the GitHub repository. We have now successfully
    implemented two different machine learning solutions with different use cases
    and studied how to form a machine learning question to be answered. We have also
    learned how to optimize and finetune a model.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæ‚¨åº”è¯¥å¯¹æ•°æ®å’Œå¯è§†åŒ–è¿›è¡Œå®éªŒï¼Œå› æ­¤æœ¬ç« ä¸ä¼šæ¶µç›–æ¯ä¸ªå¯è§†åŒ–çš„åˆ›å»ºã€‚ä¸€ä¸ªç¤ºä¾‹åº”ç”¨ç¨‹åºä½œä¸ºGitHubä»“åº“ä¸­çš„ææ–™æä¾›ã€‚æˆ‘ä»¬ç°åœ¨å·²ç»æˆåŠŸå®ç°äº†ä¸¤ä¸ªä¸åŒç”¨ä¾‹çš„æœºå™¨å­¦ä¹ è§£å†³æ–¹æ¡ˆï¼Œå¹¶ç ”ç©¶äº†å¦‚ä½•å½¢æˆè¦å›ç­”çš„æœºå™¨å­¦ä¹ é—®é¢˜ã€‚æˆ‘ä»¬è¿˜å­¦ä¹ äº†å¦‚ä½•ä¼˜åŒ–å’Œå¾®è°ƒæ¨¡å‹ã€‚
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: In this chapter, we utilized the skills learned during the previous chapters
    by implementing two different use cases. In our first example, we studied the
    data of houses in California and created a model to predict their prices based
    on house-related variables. We created an application to utilize our model and
    learned about the iterations and how to interpret the experiment results.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡å®ç°ä¸¤ä¸ªä¸åŒçš„ç”¨ä¾‹æ¥åˆ©ç”¨äº†å‰å‡ ç« å­¦åˆ°çš„æŠ€èƒ½ã€‚åœ¨æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ç ”ç©¶äº†åŠ åˆ©ç¦å°¼äºšå·çš„æˆ¿å±‹æ•°æ®ï¼Œå¹¶åˆ›å»ºäº†ä¸€ä¸ªåŸºäºæˆ¿å±‹ç›¸å…³å˜é‡çš„æ¨¡å‹æ¥é¢„æµ‹å…¶ä»·æ ¼ã€‚æˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªåº”ç”¨ç¨‹åºæ¥åˆ©ç”¨æˆ‘ä»¬çš„æ¨¡å‹ï¼Œå¹¶å­¦ä¹ äº†è¿­ä»£ä»¥åŠå¦‚ä½•è§£é‡Šå®éªŒç»“æœã€‚
- en: In our second example, we learned how to form a customer churn model and utilize
    it in multiple ways. We also learned how to create different datasets from our
    original data file and how to form a machine learning question using a framework.
    We visualized the results using native visualizations in Qlik Sense.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„ç¬¬äºŒä¸ªä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•æ„å»ºå®¢æˆ·æµå¤±æ¨¡å‹ï¼Œå¹¶åˆ©ç”¨å®ƒåœ¨å¤šç§æ–¹å¼ä¸‹ã€‚æˆ‘ä»¬è¿˜å­¦ä¹ äº†å¦‚ä½•ä»åŸå§‹æ•°æ®æ–‡ä»¶ä¸­åˆ›å»ºä¸åŒçš„æ•°æ®é›†ï¼Œä»¥åŠå¦‚ä½•ä½¿ç”¨æ¡†æ¶æ¥æ„å»ºæœºå™¨å­¦ä¹ é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨Qlik
    Senseçš„æœ¬åœ°å¯è§†åŒ–æ¥å±•ç¤ºç»“æœã€‚
- en: In our next and last chapter, we will look into the future. We will investigate
    current trends in machine learning and artificial intelligence and try to predict
    how these might evolve in the future. We will also investigate megatrends and
    get familiar with the characteristics of a megatrend. We will also think about
    the evaluation of possible megatrends. Understanding megatrends is a crucial skill
    in being able to compete and evolve.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬æ¥ä¸‹æ¥çš„æœ€åä¸€ç« ä¸­ï¼Œæˆ‘ä»¬å°†å±•æœ›æœªæ¥ã€‚æˆ‘ä»¬å°†æ¢è®¨å½“å‰æœºå™¨å­¦ä¹ å’Œäººå·¥æ™ºèƒ½çš„è¶‹åŠ¿ï¼Œå¹¶å°è¯•é¢„æµ‹è¿™äº›è¶‹åŠ¿åœ¨æœªæ¥å¯èƒ½å¦‚ä½•æ¼”å˜ã€‚æˆ‘ä»¬è¿˜å°†ç ”ç©¶é‡å¤§è¶‹åŠ¿ï¼Œå¹¶ç†Ÿæ‚‰é‡å¤§è¶‹åŠ¿çš„ç‰¹å¾ã€‚æˆ‘ä»¬è¿˜å°†æ€è€ƒå¯¹å¯èƒ½å‡ºç°çš„é‡å¤§è¶‹åŠ¿çš„è¯„ä¼°ã€‚ç†è§£é‡å¤§è¶‹åŠ¿æ˜¯èƒ½å¤Ÿåœ¨ç«äº‰ä¸­å‘å±•å’Œè¿›åŒ–çš„å…³é”®æŠ€èƒ½ã€‚
