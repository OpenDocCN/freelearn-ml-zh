- en: '*Chapter 3*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第 3 章*'
- en: Neighborhood Approaches and DBSCAN
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 邻域方法与 DBSCAN
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将能够：
- en: Understand how neighborhood approaches to clustering work from beginning to
    end
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解邻域方法在聚类中的工作原理，从头到尾
- en: Implement the DBSCAN algorithm from scratch by using packages
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头实现 DBSCAN 算法，使用相关软件包
- en: Identify the best suited algorithm from k-means, hierarchical clustering, and
    DBSCAN to solve your problem
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 k-means、层次聚类和 DBSCAN 中选择最适合的算法来解决你的问题
- en: In this chapter, we will have a look at DBSCAN clustering approach that will
    serve us best in the highly complex data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将介绍 DBSCAN 聚类方法，这对于处理高度复杂的数据最为适用。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: 'So far, we have covered two popular ways of approaching the clustering problem:
    k-means and hierarchical clustering. Both clustering techniques have pros and
    cons associated with how they are carried out. Once again, let''s revisit where
    we have been in the first two chapters so we can gain further context to where
    we will be going in this chapter.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了两种流行的聚类方法：k-means 和层次聚类。每种聚类技术在实施时都有优缺点。让我们再次回顾前两章的内容，以便为本章的内容提供更好的背景。
- en: In the challenge space of unsupervised learning, you will be presented with
    a collection of feature data, but no complementary labels telling you what these
    feature variables necessarily mean. While you may not get a discrete view into
    what the target labels are, you can get some semblance of structure out of the
    data by clustering similar groups together and seeing what is similar within groups.
    The first approach we covered to achieve this goal of clustering similar data
    points is k-means.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习的挑战领域中，你将会面临一组特征数据，但没有补充标签告诉你这些特征变量的具体含义。尽管你无法直接得知目标标签是什么，但你可以通过将相似的群体聚集在一起，查看组内的相似性，来从数据中挖掘出一定的结构。我们之前讲解的聚类相似数据点的第一个方法是
    k-means。
- en: k-means works best for simpler data challenges where speed is paramount. By
    simply looking at the closest data points, there is not a lot of computational
    overhead, however, there is also a greater degree of challenge when it comes to
    higher-dimensional datasets. k-means is also not ideal if you are unaware of the
    potential number of clusters you would be looking for. An example we have worked
    with in *Chapter 2*, *Hierarchical Clustering*, was looking at chemical profiles
    to determine which wines belonged together in a disorganized shipment. This exercise
    only worked well because you knew that three wine types were ordered; however,
    k-means would have been less successful if you had no intuition on what the original
    order was made up of.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 最适用于简单的数据挑战，其中速度至关重要。通过简单地查看最接近的数据点，计算开销不大，但当面对高维数据集时，挑战也会增大。如果你不知道可能需要寻找多少个聚类，k-means
    也不适合使用。在*第 2 章*，*层次聚类*中，我们曾经探讨过通过化学分析数据来判断哪些葡萄酒属于同一类别。这一练习之所以有效，是因为你知道有三种类型的葡萄酒已经被订购。然而，如果你对最初的排序没有任何直觉，k-means
    的效果会大打折扣。
- en: The second clustering approach we explored was hierarchical clustering. This
    method can work in multiple ways – either agglomerative or divisive. Agglomerative
    clustering works with a bottom-up approach, treating each data point as its own
    cluster and recursively grouping them together with linkage criteria. Divisive
    clustering works in the opposite direction by treating all data points as one
    large class and recursively breaking them down into smaller clusters. This approach
    has the benefit of fully understanding the entire data distribution, as it calculates
    splitting potential, however, it is typically not done in practice due to its
    greater complexity. Hierarchical clustering is a strong contender for your clustering
    needs when it comes to not knowing anything about the data. Using a dendrogram,
    you can visualize all the splits in your data and consider what number of clusters
    makes sense after the fact. This can be really helpful in your specific use case;
    however, it also comes at a higher computational cost that is seen in k-means.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们探讨的第二种聚类方法是层次聚类。这种方法可以通过两种方式工作——合并式或分裂式。合并式聚类采用自下而上的方式，将每个数据点视为一个独立的簇，并通过连接准则递归地将它们聚集在一起。分裂式聚类则朝相反方向工作，它将所有数据点视为一个大的类别，并递归地将其拆分为更小的簇。这种方法的优势在于能够全面理解整个数据分布，因为它计算了分割的潜力；然而，由于其更高的复杂性，通常在实际操作中并不常用。在不知道数据任何信息的情况下，层次聚类是聚类需求的有力竞争者。通过使用树状图，你可以可视化数据中的所有分割，并在事后考虑哪个簇的数量更合理。在你的特定使用案例中，这非常有帮助；然而，它也带来了与k均值聚类相似的较高计算成本。
- en: 'In this chapter, we will cover a clustering approach that will serve us best
    in the highly complex data: **DBSCAN** (Density-Based Spatial Clustering of Applications
    with Noise). Canonically, this method has always been seen as a high performer
    in datasets that have a lot of densely interspersed data. Let''s walk through
    why it does so well in these use cases.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍一种在高度复杂数据中最适合的聚类方法：**DBSCAN**（基于密度的空间聚类与噪声）。经典地，这种方法一直被认为在数据集密集分布的情况下表现优秀。让我们一起探讨它为什么在这些用例中表现如此出色。
- en: Clusters as Neighborhoods
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 聚类作为邻域
- en: 'In the first two chapters, we explored the concept of likeness being described
    as a function of Euclidean distance – data points that are closer to any one point
    can be seen as similar, while those that are further away in Euclidean space can
    be seen as dissimilar. This notion is seen once again in the DBSCAN algorithm.
    As alluded to by the lengthy name, the DBSCAN approach expands upon basic distance
    metric evaluation by also incorporating the notion of density. If there are clumps
    of data points that all exist in the same area as each other, they can be seen
    as members of the same cluster:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两章中，我们探讨了相似性的概念，这一概念通过欧几里得距离来描述——与某一点较近的数据点可以视为相似，而在欧几里得空间中距离较远的数据点则可以视为不相似。这一概念在DBSCAN算法中再次出现。正如其冗长的名称所暗示的，DBSCAN方法通过引入密度的概念，扩展了基本的距离度量评估。如果一群数据点都位于彼此相同的区域中，它们可以被视为同一簇的成员：
- en: '![Figure 3.1: Neighbors have a direct connection to clusters'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.1：邻居与聚类有直接的联系'
- en: '](img/C12626_03_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_01.jpg)'
- en: 'Figure 3.1: Neighbors have a direct connection to clusters'
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.1：邻居与聚类有直接的联系
- en: In the preceding example, we can see four neighborhoods.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，我们可以看到四个邻域。
- en: The density-based approach has a number of benefits when compared to the past
    approaches we've covered that focus exclusively on distance. If you were just
    focusing on distance as a clustering threshold, then you may find your clustering
    makes little sense if faced with a sparse feature space with outliers. Both k-means
    and hierarchical clustering will automatically group together all data points
    in the space until no points are left.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们之前讨论过的仅关注距离的传统方法相比，基于密度的方法有许多优势。如果你仅将距离作为聚类的阈值，那么在面对稀疏特征空间和离群点时，可能会发现你的聚类结果毫无意义。无论是k均值聚类还是层次聚类，都会自动将空间中的所有数据点分组，直到没有剩余点为止。
- en: 'While hierarchical clustering does provide a path around this issue somewhat,
    since you can dictate where clusters are formed using a dendrogram post-clustering
    run, k-means is the most susceptible to failing, as it is the simplest approach
    to clustering. These pitfalls are less evident when we begin evaluating neighborhood
    approaches to clustering:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然层次聚类确实在某种程度上绕过了这个问题，因为你可以在聚类后使用树状图来指定聚类的形成位置，但k-means仍然是最容易失败的，因为它是最简单的聚类方法。当我们开始评估基于邻域的聚类方法时，这些问题就不那么明显了：
- en: '![Figure 3.2: Example dendrogram'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.2：示例树状图'
- en: '](img/C12626_03_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_02.jpg)'
- en: 'Figure 3.2: Example dendrogram'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.2：示例树状图
- en: By incorporating the notion of neighbor density in DBSCAN, we can leave outliers
    out of clusters if we choose to, based on the hyperparameters we choose at run
    time. Only the data points that have close neighbors will be seen as members within
    the same cluster and those that are farther away can be left as unclustered outliers.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在 DBSCAN 中引入邻居密度的概念，我们可以根据运行时选择的超参数，选择是否将异常值排除在聚类之外。只有具有密切邻居的数据点才会被视为同一聚类的成员，而那些距离较远的数据点则可以被视为未聚类的异常值。
- en: Introduction to DBSCAN
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN简介
- en: As mentioned in the previous section, the strength of DBSCAN becomes apparent
    when we analyze the benefits of taking a density-based approach to clustering.
    DBSCAN evaluates density as a combination of neighborhood radius and minimum points
    found in a neighborhood deemed a cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一节所提到的，当我们分析基于密度的聚类方法的优势时，DBSCAN 的强大之处就显现出来了。DBSCAN 将密度评估为邻域半径和在邻域中找到的最小点数的组合，这些点数被视为一个聚类。
- en: This concept can be driven home if we re-consider the scenario where you are
    tasked with organizing an unlabeled shipment of wine for your store. In the past
    example, it was made clear that we can find similar wines based off their features,
    such as scientific chemical traits. Knowing this information, we can more easily
    group together similar wines and efficiently have our products organized for sale
    in no time. Hopefully, that is clear by now – but what may not have been clear
    is the fact that products that you order to stock your store often reflect real-world
    purchase patterns. To promote variety in your inventory, but still have enough
    stock of the most popular wines, there is a highly uneven distribution of product
    types that you have available. Most people love the classic wines, such as white
    and red, however, you may still carry more exotic wines for your customers who
    love expensive varieties. This makes clustering more difficult, since there are
    uneven class distributions (you don't order 10 of every wine available, for example).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们重新考虑你被要求为商店整理一批未标记的葡萄酒货物的场景，可能更容易理解这个概念。在之前的示例中，已经明确说明，我们可以根据葡萄酒的特征（如科学化学特性）找到相似的葡萄酒。了解这些信息后，我们可以更轻松地将相似的葡萄酒分组，并且快速将产品整理好以供销售。希望到现在为止这一点已经清楚了——但可能不太清楚的是，你为商店订购的商品通常反映了现实世界的购买模式。为了在库存中促进品种多样性，但又能保证最受欢迎的葡萄酒有足够的库存，你的商品种类往往会呈现出高度不均衡的分布。大多数人喜欢经典的葡萄酒，如白葡萄酒和红葡萄酒，但你可能仍会为那些喜欢昂贵葡萄酒的顾客提供更多异国情调的葡萄酒。这使得聚类更加困难，因为存在不均衡的类别分布（例如，你不会订购每种葡萄酒各10瓶）。
- en: DBSCAN differs from k-means and hierarchical clustering because you can build
    this intuition into how we evaluate the clusters of customers we are interested
    in forming. It can cut through the noise in an easier fashion and only point out
    customers who have the highest potential for remarketing in a campaign.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 与 k-means 和层次聚类的不同之处在于，你可以将直觉融入到如何评估我们感兴趣的顾客聚类的过程中。它可以以更简单的方式去除噪声，并且仅指出那些在营销活动中具有最高潜力的顾客。
- en: By clustering through the concept of a neighborhood, we can separate out the
    one-off customers that can be seen as random noise, relative to the more valuable
    customers that come back to our store time and time again. This approach, of book,
    calls into question how we establish the best numbers when it comes to neighborhood
    radius and minimum points per neighborhood.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基于邻域的聚类方法，我们可以区分出那些可以视为随机噪声的偶尔顾客，以及那些一次次光顾我们商店的更有价值的顾客。这种方法自然会引发关于如何确定邻域半径和每个邻域最小点数的最佳数值的问题。
- en: As a high-level heuristic, we want to have our neighborhood radius small, but
    not too small. At one end of the extreme, you can have the neighborhood radius
    be quite high – this can max out at treating all points in the feature space as
    one massive cluster. On the opposite end of the extreme, you can have a very small
    neighborhood radius. Too small neighborhood radii can result in no points being
    clustered together and having a large collection of single member clusters.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种高级启发式方法，我们希望将邻域半径设置得较小，但又不能太小。在极端的一端，你可以将邻域半径设置得非常大——这可能导致将所有点视为一个庞大的聚类。而在另一端，你可以将邻域半径设置得非常小。过小的邻域半径可能导致没有任何点被聚集在一起，并且出现大量单一成员的聚类。
- en: Similar logic applies when it comes to the minimum number of points that can
    make up a cluster. Minimum points can be seen as a secondary threshold that tunes
    the neighborhood radius a bit depending on what data you have available in your
    space. If all of the data in your feature space is extremely sparse, minimum points
    become extremely valuable, in tandem with neighborhood radius, to make sure you
    don't just have a large number of uncorrelated data points. When you have very
    dense data, the minimum points threshold becomes less of a driving factor as opposed
    to neighborhood radius.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的逻辑适用于构成聚类的最小点数。最小点数可以看作是一个次要阈值，它根据你数据空间中可用的数据来调整邻域半径。如果你在特征空间中的所有数据非常稀疏，最小点数会变得尤为重要，它与邻域半径配合使用，以确保不会只是大量无关的数据点。当数据非常密集时，最小点数阈值就不像邻域半径那样成为主导因素。
- en: As you can see from these two hyperparameter rules, the best options are, as
    usual, dependent on what your dataset looks like. Oftentimes, you will want to
    find the perfect "goldilocks" zone of not being too small in your hyperparameters,
    but also not too large.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这两个超参数规则的最佳选择通常依赖于数据集的具体情况。很多时候，你需要找到一个“恰到好处”的区间，即超参数既不过小，也不过大。
- en: DBSCAN In-Depth
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN 深入解析
- en: 'To see how DBSCAN works, we can trace the path of a simple toy program as it
    merges together to form a variety of clusters and noise-labeled data points:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了观察 DBSCAN 如何工作，我们可以通过一个简单的示例程序，跟踪其如何合并形成不同的聚类和噪声标记数据点：
- en: Given *n* unvisited sample data points, move through each point in a loop and
    mark as visited.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定 *n* 个未访问的样本数据点，在循环中依次遍历每个点并标记为已访问。
- en: From each point, look at the distance to every other point in the dataset.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个点出发，查看与数据集中其他所有点的距离。
- en: For all points that fall within the neighborhood radius hyperparameter, connect
    them as neighbors.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有位于邻域半径超参数内的点，将它们连接为邻居。
- en: Check to see whether the number of neighbors is at least as many as the minimum
    points required.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查邻居的数量是否至少达到所需的最小点数。
- en: If the minimum point threshold is reached, group together as a cluster. If not,
    mark the point as noise.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果达到最小点数阈值，将点归为一个聚类。如果没有，将该点标记为噪声。
- en: Repeat until all data points are categorized in clusters or as noise.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复此过程，直到所有数据点被分类到聚类中或标记为噪声。
- en: DBSCAN is fairly straightforward in some senses – while there are the new concepts
    of density through neighborhood radius and minimum points, at its core, it is
    still just evaluating using a distance metric.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些方面，DBSCAN 算法相对简单——虽然引入了通过邻域半径和最小点数来衡量密度的新概念，但它的核心仍然是使用距离度量进行评估。
- en: Walkthrough of the DBSCAN Algorithm
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN 算法演示
- en: 'Here is a simple example walking through the preceding steps in slightly more
    detail:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单的示例，稍微详细地演示了前述步骤：
- en: 'Given four sample data points, view each point as its own cluster [ (1,7) ],
    [ (-8,6) ], [ (-9,4) ] , [ (4, -2) ]:![Figure 3.3: Plot of sample data points'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定四个样本数据点，将每个点视为一个独立的聚类 [ (1,7) ]、[ (-8,6) ]、[ (-9,4) ] 、[ (4, -2) ]:![图 3.3：样本数据点的绘制
- en: '](img/C12626_03_03.jpg)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_03_03.jpg)'
- en: 'Figure 3.3: Plot of sample data points'
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.3：样本数据点的绘制
- en: 'Calculate pairwise the Euclidean distance between each of the points:![Figure
    3.4: Point distances'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每一对点之间的欧几里得距离：![图 3.4：点之间的距离
- en: '](img/C12626_03_04.jpg)'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_03_04.jpg)'
- en: 'Figure 3.4: Point distances'
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.4：点之间的距离
- en: From each point, expand out a neighborhood size and form clusters. For the purpose
    of this example, let's imagine we passed through a neighborhood radius of three.
    This means that any two points will be neighbors if the distance between them
    is less than three. Points (-8,6) and (-9,4) are now candidates for clustering.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从每个点出发，扩展一个邻域大小并形成簇。为了这个示例，假设我们通过了半径为三的邻域。这意味着任何两个点，如果它们之间的距离小于三，就会被视为邻居。点(-8,6)和(-9,4)现在是聚类的候选点。
- en: Points that have no neighbors are marked as noise and remain unclustered. Points
    (1,7) and (4,-2) fall out of our frame of interest as being useless in terms of
    clustering.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有邻居的点被标记为噪声，并且保持未聚类状态。点(1,7)和(4,-2)由于在聚类中无用，超出了我们的兴趣范围。
- en: Points that have neighbors are then evaluated to see whether they pass the minimum
    points threshold. In this example, if we had passed through a minimum points threshold
    of two, then points (-8,6) and (-9,4) can formally be grouped together as a cluster.
    If we had a minimum points threshold of three, then all four data points in this
    set would be considered superfluous noise.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有邻居的点随后会被评估，看它们是否符合最小点数阈值。在这个示例中，如果我们设置最小点数阈值为二，那么点(-8,6)和(-9,4)就可以正式组合成一个簇。如果最小点数阈值为三，那么这个集合中的所有四个数据点将被视为多余的噪声。
- en: Repeat this process on remaining un-visited data points.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在剩余的未访问数据点上重复此过程。
- en: At the end of this process, you will have your entire dataset established as
    either within clusters or as unrelated noise. Hopefully, as you can tell by walking
    through the toy example, DBSCAN performance is highly dependent on the threshold
    hyperparameters you choose a priori. This means that you may have to run DBSCAN
    a couple of times with different hyperparameter options to get an understanding
    of how they influence overall performance.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程结束时，你将获得整个数据集，所有数据要么被归类为簇的一部分，要么被视为无关的噪声。通过走完整个玩具示例，你可以发现，DBSCAN的性能高度依赖于你事先选择的阈值超参数。这意味着你可能需要多次运行DBSCAN，并尝试不同的超参数选项，以了解它们如何影响整体性能。
- en: One great thing to notice about DBSCAN is that it does away with concepts of
    centroids that we saw in both k-means and a centroid-focused implementation of
    hierarchical clustering. This feature allows DBSCAN to work better for complex
    datasets, since most data in the wild is not shaped like clean blobs.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN的一个优点是，它摒弃了我们在k均值和以质心为中心的层次聚类实现中看到的质心概念。这个特性使得DBSCAN更适合处理复杂的数据集，因为大多数现实世界中的数据并不像干净的簇那样分布。
- en: 'Exercise 9: Evaluating the Impact of Neighborhood Radius Size'
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习9：评估邻域半径大小的影响
- en: For this exercise, we will work in reverse of what we have typically seen in
    previous examples, by first seeing the packaged implementation of DBSCAN in scikit-learn,
    and then implementing it on our own. This is done on purpose to fully explore
    how different neighborhood radius sizes drastically impact DBSCAN performance.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将采用与之前示例中常见的方式相反的顺序，先查看scikit-learn中DBSCAN的封装实现，然后再自己实现。这是故意为之，以充分探讨不同邻域半径大小如何极大地影响DBSCAN的性能。
- en: By completing this exercise, you will become familiar with how tuning neighborhood
    radius size can change how well DBSCAN performs. It is important to understand
    these facets of DBSCAN, as they can save you time in the future by troubleshooting
    your clustering algorithms efficiently.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个练习后，你将熟悉调整邻域半径大小如何影响DBSCAN的性能。理解这些DBSCAN的特点很重要，因为它们可以帮助你在未来通过高效地排查聚类算法问题来节省时间。
- en: 'Generate some dummy data:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一些虚拟数据：
- en: '[PRE0]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The output is as follows:'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 3.5: Visualized Toy Data Example'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图3.5：可视化的玩具数据示例'
- en: '](img/C12626_03_05.jpg)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_03_05.jpg)'
- en: 'Figure 3.5: Visualized Toy Data Example'
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.5：可视化的玩具数据示例
- en: 'After plotting the dummy data for this toy problem, you will see that the dataset
    has two features and approximately seven to eight clusters. To implement DBSCAN
    using scikit-learn, you will need to instantiate a new scikit-learn class:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在为这个玩具问题绘制虚拟数据后，你将看到数据集有两个特征，并大约有七到八个簇。要使用scikit-learn实现DBSCAN，你需要实例化一个新的scikit-learn类：
- en: '[PRE1]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our example DBSCAN instance is stored in the `db` variable, and our hyperparameters
    are passed through on creation. For the sake of this example, you can see that
    the neighborhood radius (`eps`) is set to 0.5, while the minimum number of points
    is set to 10\. To keep in line with our past chapters, we will once again be using
    Euclidean distance as our distance metric.
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们的DBSCAN实例存储在`db`变量中，超参数在创建时传入。为了这个例子，你可以看到邻域半径（`eps`）设置为0.5，而最小点数设置为10。为了与前几章一致，我们将继续使用欧几里得距离作为度量标准。
- en: 'Let''s set up a loop that allows us to explore potential neighborhood radius
    size options interactively:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置一个循环，允许我们交互式地探索潜在的邻域半径大小选项：
- en: '[PRE2]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code results in the following two plots:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码产生以下两张图：
- en: '![Figure: 3.6: Resulting plots'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.6: 结果图'
- en: '](img/C12626_03_06.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_06.jpg)'
- en: 'Figure: 3.6: Resulting plots'
  id: totrans-74
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图：3.6: 结果图'
- en: As you can see from the plots, setting our neighborhood size too small will
    cause everything to be seen as random noise (purple points). Bumping our neighborhood
    size up a little bit allows us to form clusters that make more sense. Try recreating
    the preceding plots and experiment with varying `eps` sizes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看到，设置邻域半径过小会导致所有数据被视为随机噪声（紫色点）。稍微增加邻域半径，可以让我们形成更有意义的簇。试着重新创建前面的图形并尝试不同的`eps`值。
- en: DBSCAN Attributes – Neighborhood Radius
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN 属性 - 邻域半径
- en: In *Exercise 9*, *Evaluating the Impact of Neighborhood Radius Size,* you saw
    how impactful setting the proper neighborhood radius is on the performance of
    your DBSCAN implementation. If your neighborhood is too small, then you will run
    into issues where all the data is left unclustered. If you set your neighborhood
    too large, then all of the data will similarly be grouped together into one cluster
    and not provide any value. If you explored the preceding exercise further with
    your own `eps` sizes, you may have noticed that it is very difficult to land on
    great clustering using only the neighborhood size. This is where a minimum points
    threshold comes in handy. We will visit that topic later.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 9*，*评估邻域半径大小的影响*中，你看到了设置合适的邻域半径对你的DBSCAN实现性能的影响。如果邻域太小，你会遇到所有数据都未被聚类的问题。如果你设置的邻域过大，那么所有数据会被聚集到一个簇中，且无法提供任何价值。如果你用自己的`eps`值进一步探索前面的练习，你可能会注意到，仅凭邻域大小很难得到理想的聚类效果。这时，最小点数阈值就显得非常重要。我们稍后会讨论这一主题。
- en: 'To go deeper into the neighborhood concept of DBSCAN, let''s take a deeper
    look at the `eps` hyperparameter you pass at instantiation time. `eps` stands
    for epsilon and is the distance that your algorithm will look within when searching
    for neighbors. This epsilon value is converted to a radius that sweeps around
    any given data point in a circular manner to serve as a neighborhood:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更深入了解DBSCAN的邻域概念，我们来看一下你在实例化时传入的`eps`超参数。`eps`代表epsilon，是算法在寻找邻居时查看的距离。这个epsilon值会被转换为一个半径，围绕任意给定数据点以圆形方式进行扫描，以形成邻域：
- en: '![Figure 3.7: Visualization of neighborhood radius where red circle is the
    neighborhood'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.7: 邻域半径可视化，红色圆圈为邻域'
- en: '](img/C12626_03_07.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_07.jpg)'
- en: 'Figure 3.7: Visualization of neighborhood radius where red circle is the neighborhood'
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.7: 邻域半径可视化，红色圆圈为邻域'
- en: In this instance, there will be four neighbors of the center point.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，中心点将有四个邻居。
- en: 'One key aspect to notice here is that the shape formed by your neighborhood
    search is a circle in two dimensions, and a sphere in three dimensions. This may
    impact the performance of your model simply based on how the data is structured.
    Once again, blobs may seem like an intuitive structure to find – this may not
    always be the case. Fortunately, DBSCAN is well equipped to handle this dilemma
    of clusters that you may be interested in, yet that do not fit the explicit blob
    structure:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里需要注意的一个关键点是，你的邻域搜索形成的形状在二维空间中是圆形，在三维空间中是球形。根据数据的结构，这可能会影响你模型的表现。再次强调，簇可能看起来像是一个直观的结构，但这并不总是如此。幸运的是，DBSCAN在处理这种你感兴趣的簇，但又不符合明确的簇状结构时非常有效。
- en: '![Figure 3.8: Impact of varying neighborhood radius size'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.8: 不同邻域半径大小的影响'
- en: '](img/C12626_03_08.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_08.jpg)'
- en: 'Figure 3.8: Impact of varying neighborhood radius size'
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 3.8: 不同邻域半径大小的影响'
- en: On the left, the data point will be classified as random noise. On the right,
    the data point has multiple neighbors and could be its own cluster.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧，数据点会被分类为随机噪声。右侧，数据点有多个邻居，可能会成为一个独立的簇。
- en: 'Activity 4: Implement DBSCAN from Scratch'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动4：从头实现DBSCAN
- en: Using a generated two-dimensional dataset during an interview, you are asked
    to create the DBSCAN algorithm from scratch. To do this, you will need to code
    the intuition behind neighborhood searches and have a recursive call that adds
    neighbors.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在面试中使用生成的二维数据集时，你被要求从头开始实现DBSCAN算法。为此，你需要编码邻域搜索的直觉，并进行递归调用以添加邻居。
- en: Given what you've learned about DBSCAN and distance metrics from prior chapters,
    build an implementation of DBSCAN from scratch in Python. You are free to use
    NumPy and SciPy to evaluate distances here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你在前几章中学到的DBSCAN和距离度量，使用Python从头实现DBSCAN。你可以自由使用NumPy和SciPy来评估距离。
- en: 'These steps will help you complete the activity:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动的步骤如下：
- en: Generate a random cluster dataset
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个随机簇数据集
- en: Visualize the data
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据
- en: Create functions from scratch that allow you to call DBSCAN on a dataset
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头创建函数，允许你在数据集上调用DBSCAN
- en: Use your created DBSCAN implementation to find clusters in the generated dataset.
    Feel free to use hyperparameters as you see fit, tuning them based on their performance
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用你创建的DBSCAN实现来寻找生成数据集中的簇。可以根据需要调整超参数，并根据其表现进行调优。
- en: Visualize the clustering performance of your DBSCAN implementation from scratch
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化从头开始实现的DBSCAN聚类效果
- en: 'The desired outcome of this exercise is for you to understand how DBSCAN works
    from the ground up before you use the fully packaged implementation in scikit-learn.
    Taking this approach to any machine learning algorithm from scratch is important,
    as it helps you "earn" the ability to use easier implementations, while still
    being able to discuss DBSCAN in depth in the future:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习的预期结果是让你理解DBSCAN的工作原理，从而在使用scikit-learn的完整实现之前，能够从头开始实现DBSCAN。采取这种方法来学习任何机器学习算法是很重要的，它有助于你“获得”使用更简单实现的能力，同时在未来仍能深入讨论DBSCAN：
- en: '![Figure 3.9: Expected outcome'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.9：预期结果'
- en: '](img/C12626_03_09.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_09.jpg)'
- en: 'Figure 3.9: Expected outcome'
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.9：预期结果
- en: Note
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Solution for this activity can be found on page 316.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第316页找到。
- en: DBSCAN Attributes – Minimum Points
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN属性 – 最小点数
- en: The other core component to a successful implementation of DBSCAN beyond the
    neighborhood radius is the minimum number of points required to justify membership
    within a cluster. As mentioned earlier, it is more obvious that this lower bound
    benefits your algorithm when it comes to sparser datasets. That's not to say it
    is a useless parameter when you have very dense data, however – while having single
    data points randomly interspersed through your feature space can be easily bucketed
    as noise, it becomes more of a grey area when we have random patches of two to
    three, for example. Should these data points be their own cluster, or should they
    also be categorized as noise? Minimum points thresholding helps solve this problem.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了邻域半径之外，DBSCAN成功实现的另一个核心组件是需要的最小点数，以证明某个数据点属于某个簇。如前所述，当数据集较为稀疏时，这个下限能明显有利于你的算法。不过，当数据非常密集时，这个参数并非无用——虽然将单个数据点随机散布在特征空间中可以很容易地被归类为噪声，但当数据随机形成两到三个点的“斑块”时，问题就变得模糊了。例如，这些数据点是应该作为一个独立的簇，还是也应该归类为噪声？最小点数阈值帮助解决了这个问题。
- en: 'In the scikit-learn implementation of DBSCAN, this hyperparameter is seen in
    the `min_samples` field passed on DBSCAN instance creation. This field is very
    valuable to play with in tandem with the neighborhood radius size hyperparameter
    to fully round out your density-based clustering approach:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn实现的DBSCAN中，此超参数在`min_samples`字段中设置，该字段会在创建DBSCAN实例时传递。此字段与邻域半径大小超参数配合使用时非常有价值，可以帮助完善基于密度的聚类方法：
- en: '![Figure 3.10: Minimum points threshold deciding whether a group of data points
    is noise or a cluster'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图3.10：最小点数阈值决定数据点是否为噪声或簇'
- en: '](img/C12626_03_10.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_10.jpg)'
- en: 'Figure 3.10: Minimum points threshold deciding whether a group of data points
    is noise or a cluster'
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3.10：最小点数阈值决定数据点是否为噪声或簇
- en: On the right, if minimum points threshold is 10 points, it will classify data
    in this neighborhood as noise.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧，如果最小点数阈值为10个点，它会将该邻域中的数据分类为噪声。
- en: In real-world scenarios, you can see minimum points being highly impactful when
    you have truly large amounts of data. Going back to the wine-clustering example,
    if your store was actually a large wine warehouse, you could have thousands of
    individual wines with only one or two bottles that can easily be viewed as their
    own cluster. This may be helpful depending on your use case; however, it is important
    to keep in mind the subjective magnitudes that come with your data. If you have
    millions of data points, then random noise can easily be seen as hundreds or even
    thousands of random one-off sales. However, if your data is on the scale of hundreds
    or thousands, single data points can be seen as random noise.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实场景中，当你拥有大量数据时，最小点数将产生显著影响。以葡萄酒聚类为例，如果你的商店实际上是一个大型酒类仓库，你可能会有成千上万种葡萄酒，每种酒只有一两瓶，这些酒可能会被轻松视为自己的独立聚类。根据你的使用场景，这可能会有帮助；然而，重要的是要记住数据的主观大小。如果你的数据有数百万个数据点，那么随机噪声可能会被视为数百甚至数千个随机的单次销售。然而，如果你的数据量在几百或几千个数据点的规模上，单个数据点可能会被视为随机噪声。
- en: 'Exercise 10: Evaluating the Impact of Minimum Points Threshold'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 10：评估最小点数阈值的影响
- en: Similar to our *Exercise 9, Evaluating the Impact of Neighborhood Radius Size*,
    where we explored the value of setting a proper neighborhood radius size, we will
    repeat the exercise, but instead will change the minimum points threshold on a
    variety of datasets.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于我们的*练习 9，评估邻域半径大小的影响*，我们探索了设置适当邻域半径大小的值，我们将重复这一练习，但这次会在多种数据集上更改最小点数阈值。
- en: Using our current implementation of DBSCAN, we can easily tune the minimum points
    threshold. Tune this hyperparameter and see how it performs on generated data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们当前实现的 DBSCAN，我们可以轻松地调整最小点数阈值。调整这个超参数，并观察它在生成的数据上的表现。
- en: By tuning the minimum points threshold for DBSCAN, you will understand how it
    can affect the quality of your clustering predictions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整 DBSCAN 的最小点数阈值，你将理解它如何影响聚类预测的质量。
- en: 'Once again, let''s start with randomly generated data:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，从随机生成的数据开始：
- en: 'Generate a random cluster dataset, as follows:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式生成随机聚类数据集：
- en: '[PRE3]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Visualize the data as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式可视化数据：
- en: '[PRE4]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![Figure 3.11: Plot of generated data'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 3.11：生成数据的绘图'
- en: '](img/C12626_03_11.jpg)'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_03_11.jpg)'
- en: 'Figure 3.11: Plot of generated data'
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.11：生成数据的绘图
- en: 'With the same plotted data as before, let''s grab one of the better performing
    neighborhood radius sizes from *Exercise 1*, *Evaluating the Impact of Neighborhood
    Radius Size* – `eps` = 0.7:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与之前相同的绘图数据，让我们从*练习 1*，*评估邻域半径大小的影响*中选择一个表现较好的邻域半径大小——`eps` = 0.7：
- en: '[PRE5]'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '`eps` is a tunable hyperparameter. However, earlier in the same line, we establish
    that 0.7 comes from previous experimentation, leading us to `eps = 0.7` as the
    optimal value.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`eps` 是一个可调的超参数。然而，正如前文所述，0.7 是来自之前实验的结果，因此我们选择 `eps = 0.7` 作为最优值。'
- en: 'After instantiating the DBSCAN clustering algorithm, let''s treat the `min_samples`
    hyperparameter as the variable we wish to tune. We can cycle through a loop to
    find which minimum number of points works best for our use case:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实例化 DBSCAN 聚类算法后，让我们将 `min_samples` 超参数视为我们希望调整的变量。我们可以通过循环来查找最适合我们使用场景的最小点数：
- en: '[PRE6]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Looking at the first plot generated, we can see where we ended if you followed
    *Exercise 1*, *Evaluating the Impact of Neighborhood Radius Size* exactly, using
    10 minimum points to mark the threshold for cluster membership:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 观察生成的第一个图，我们可以看到，如果你按照*练习 1*，*评估邻域半径大小的影响*，使用 10 个最小点作为聚类成员资格的阈值，你将达到的结果：
- en: '![Figure 3.12: Plot of Toy problem with a minimum of 10 points'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.12：具有 10 个最小点的玩具问题绘图'
- en: '](img/C12626_03_12.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_12.jpg)'
- en: 'Figure 3.12: Plot of Toy problem with a minimum of 10 points'
  id: totrans-132
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.12：具有 10 个最小点的玩具问题绘图
- en: 'The remaining two hyperparameter options can be seen to greatly impact the
    performance of your DBSCAN clustering algorithm, and show how a shift in one number
    can greatly influence performance:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的两个超参数选项对 DBSCAN 聚类算法的性能有很大影响，并展示了单一数值的变化如何显著影响性能：
- en: '![Figure 3.13: Plots of the Toy problem'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 3.13：玩具问题的绘图'
- en: '](img/C12626_03_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_03_13.jpg)'
- en: 'Figure 3.13: Plots of the Toy problem'
  id: totrans-136
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3.13：玩具问题的绘图
- en: As you can see, simply changing the number of minimum points from 19 to 20 adds
    an additional (incorrect!) cluster to our feature space. Given what you've learned
    about minimum points through this exercise, you can now tweak both epsilon and
    minimum points thresholding in your scikit-learn implementation to achieve the
    optimal number of clusters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，仅仅将最小点数从19改为20，就为我们的特征空间添加了一个额外的（错误的！）聚类。通过这次练习，你已了解最小点数的概念，现在你可以调整scikit-learn实现中的epsilon值和最小点数阈值，以获得最优的聚类数。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In our original generation of the data, we created eight clusters. This points
    out that small changes in minimum points can add entire new clusters that we know
    shouldn't be there.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们最初生成数据时，我们创建了八个聚类。这表明，最小点数的小变化可以添加整个新的聚类，而这些聚类显然不应出现在数据中。
- en: 'Activity 5: Comparing DBSCAN with k-means and Hierarchical Clustering'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动5：将DBSCAN与k-means和层次聚类进行比较
- en: You are managing store inventory and have received a large shipment of wine,
    but the brand labels fell off the bottles during transit. Fortunately, your supplier
    provided you with the chemical readings for each bottle along with their respective
    serial numbers. Unfortunately, you aren't able to open each bottle of wine and
    taste test the difference – you must find a way to group the unlabeled bottles
    back together according to their chemical readings! You know from the order list
    that you ordered three different types of wine and are given only two wine attributes
    to group the wine types back together.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在管理商店库存，收到了一大批葡萄酒货物，但在运输过程中瓶子的品牌标签掉落了。幸运的是，供应商提供了每瓶葡萄酒的化学成分数据以及各自的序列号。不幸的是，你不能打开每瓶酒并品尝其差异——你必须找到一种方法，根据化学成分将未标记的酒瓶重新分组！你从订单列表中得知，订购了三种不同类型的葡萄酒，并且只提供了两个酒的属性来将这些葡萄酒分组。
- en: In *Chapter 2*, *Hierarchical Clustering* we were able to see how k-means and
    hierarchical clustering performed on the wine dataset. In our best case scenario,
    we were able to achieve a silhouette score of 0.59\. Using scikit-learn's implementation
    of DBSCAN, let's see if we can get even better clustering.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第2章*、*层次聚类*中，我们已经看到k-means和层次聚类如何在葡萄酒数据集上表现。在我们的最佳情况下，得到了0.59的轮廓系数。现在，使用scikit-learn实现的DBSCAN，让我们看看能否获得更好的聚类结果。
- en: 'These steps will help you complete the activity:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成此活动：
- en: Import the necessary packages
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的包
- en: Load the wine dataset and check what the data looks like
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载葡萄酒数据集并检查数据的结构
- en: Visualize the data
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据
- en: Generate clusters using k-means, agglomerative clustering, and DBSCAN
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用k-means、凝聚层次聚类和DBSCAN生成聚类
- en: Evaluate a few different options for DSBSCAN hyperparameters and their effect
    on the silhouette score
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估不同的DBSCAN超参数选项及其对轮廓系数的影响
- en: Generate the final clusters based on the highest silhouette score
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于最高的轮廓系数生成最终的聚类
- en: Visualize clusters generated using each of the three methods
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化使用三种方法生成的聚类
- en: Note
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: We have downloaded this dataset from [https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine).
    You can access it at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson03/Activity05](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson03/Activity05).
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们已从[https://archive.ics.uci.edu/ml/datasets/wine](https://archive.ics.uci.edu/ml/datasets/wine)下载了此数据集。你可以通过[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson03/Activity05](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson03/Activity05)访问该数据集。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。
- en: By completing this activity, you will be recreating a full workflow of a clustering
    problem. You have already made yourself familiar with the data in *Chapter 2*,
    *Hierarchical Clustering*, and, by the end of this activity, you will have performed
    model selection to find the best model and hyperparameters for your dataset. You
    will have silhouette scores of the wine dataset for each type of clustering.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，你将重新创建一个聚类问题的完整工作流。你已经在*第2章*、*层次聚类*中熟悉了数据，并且在完成此活动后，你将执行模型选择，找到最适合你数据集的最佳模型和超参数。你将得到葡萄酒数据集在每种聚类方法下的轮廓系数。
- en: Note
  id: totrans-155
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 319.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第319页找到。
- en: DBSCAN Versus k-means and Hierarchical Clustering
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DBSCAN与k-means和层次聚类的比较
- en: Now that you've reached an understanding of how DBSCAN is implemented and how
    many different hyperparameters you can tweak to drive performance, let's survey
    how it compares to the clustering methods we covered in *Chapter 1*, *Introduction
    to Clustering* and *Chapter 2*, *Hierarchical Clustering*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了DBSCAN是如何实现的，以及可以调整哪些不同的超参数来优化性能，我们来看看它与我们在*第1章*，*聚类简介*和*第2章*，*层次聚类*中介绍的聚类方法的比较。
- en: You may have noticed in *Activity 5*, *Comparing DBSCAN with k-means and Hierarchical
    Clustering,* that DBSCAN can be a bit finnicky when it comes to finding the optimal
    clusters via silhouette score. This is a downside of the neighborhood approach
    – k-means and hierarchical clustering really excel when you have some idea regarding
    the number of clusters in your data. In most cases, this number is low enough
    that you can iteratively try a few different numbers and see how it performs.
    DBSCAN, instead, takes a more bottom-up approach by working with your hyperparameters
    and finding the clusters it views as important. In practice, it is helpful to
    consider DBSCAN when the first two options fail, simply because of the amount
    of tweaking needed to get it to work properly. That being said, when your DBSCAN
    implementation is working correctly, it will often immensely outperform k-means
    and hierarchical clustering. (In practice, this often happens with highly intertwined,
    yet still discrete data, such as a feature space containing two half-moons).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在*活动5*，*将DBSCAN与k-means和层次聚类进行比较*中注意到，DBSCAN在通过轮廓分数寻找最优簇时可能有些挑剔。这是邻域方法的一个缺点——当你对数据中的簇的数量有一些了解时，k-means和层次聚类表现得更为出色。在大多数情况下，这个数量足够小，你可以迭代地尝试几个不同的数量，看看效果如何。相反，DBSCAN采用更自下而上的方法，通过调整超参数来发现它认为重要的簇。在实践中，当前两种方法失败时，考虑使用DBSCAN是很有帮助的，因为它需要大量的调整才能正常工作。话虽如此，当DBSCAN实现正常工作时，它通常会远远超越k-means和层次聚类。（实际上，这种情况通常发生在高度交织但仍然离散的数据上，比如包含两个半月形状的特征空间）。
- en: Compared to k-means and hierarchical clustering, DBSCAN can be seen as being
    potentially more efficient, since it only has to look at each data point once.
    Instead of multiple iterations of finding new centroids and evaluating where their
    nearest neighbors are, once a point has been assigned to a cluster in DBSCAN,
    it does not change cluster membership. The other key difference that DBSCAN and
    hierarchical clustering both share, compared to k-means, is not needing to explicitly
    pass a number of clusters expected at the time of creation. This can be extremely
    helpful when you have no external guidance on how to break your dataset down.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 与k-means和层次聚类相比，DBSCAN可能更加高效，因为它只需要对每个数据点进行一次检查。与需要多次迭代寻找新质心并评估其最近邻的位置不同，一旦一个点被分配到DBSCAN中的一个簇，它的簇成员就不会再改变。DBSCAN与层次聚类相比，与k-means的另一个关键区别是，它不需要在创建时明确指定预期的簇的数量。这在没有外部指导如何将数据集拆分时非常有用。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: DBSCAN takes an interesting approach to clustering compared to k-means and hierarchical
    clustering. While hierarchical clustering can, in some aspects, be seen as an
    extension of the nearest neighbors approach seen in k-means, DBSCAN approaches
    the problem of finding neighbors by applying a notion of density. This can prove
    extremely beneficial when it comes to highly complex data that is intertwined
    in a complex fashion. While DBSCAN is very powerful, it is not infallible and
    can be seen as potentially overkill, depending on what your original data looks
    like.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN在聚类方法上采取了与k-means和层次聚类不同的有趣方式。虽然层次聚类在某些方面可以看作是k-means中最近邻方法的扩展，但DBSCAN通过应用密度的概念来处理查找邻居的问题。当数据非常复杂且交织在一起时，这种方法尤为有用。虽然DBSCAN非常强大，但并不是万无一失的，并且根据原始数据的情况，它有时可能会显得过于复杂。
- en: Combined with k-means and hierarchical clustering, however, DBSCAN completes
    a strong toolbox when it comes to the unsupervised learning task of clustering
    your data. When faced with any problem in this space, it is worthwhile to compare
    the performance of each method and see which performs best.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将DBSCAN与k-means和层次聚类结合使用时，DBSCAN为无监督学习任务中的数据聚类提供了强大的工具箱。在遇到这类问题时，比较每种方法的性能并找出最合适的方案是值得的。
- en: 'With clustering explored, we will now move onto another key piece of rounding
    out your skills in unsupervised learning: dimensionality reduction. Through smart
    reduction of dimensions, we can make clustering easier to understand and communicate
    to stakeholders. Dimensionality reduction is also key to creating all types of
    machine learning models in the most efficient manner possible.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在探索完聚类之后，我们将进入无监督学习中另一个关键的技能：降维。通过智能地减少维度，我们可以让聚类变得更易理解，并能够向利益相关者传达。降维对于以最有效的方式创建各种机器学习模型也是至关重要的。
