- en: '12'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '12'
- en: Linear Regression in R
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R中的线性回归
- en: 'In this chapter, we will introduce linear regression, a fundamental statistical
    approach that’s used to model the relationship between a target variable and multiple
    explanatory (also called independent) variables. We will cover the basics of linear
    regression, starting with simple linear regression and then extending the concepts
    to multiple linear regression. We will learn how to estimate the model coefficients,
    evaluate the goodness of fit, and test the significance of the coefficients using
    hypothesis testing. Additionally, we will discuss the assumptions underlying linear
    regression and explore techniques to address potential issues, such as nonlinearity,
    interaction effect, multicollinearity, and heteroskedasticity. We will also introduce
    two widely used regularization techniques: the ridge and **Least Absolute Shrinkage
    and Selection Operator** (**lasso**) penalties.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍线性回归，这是一种基本的统计方法，用于建模目标变量与多个解释变量（也称为独立变量）之间的关系。我们将从简单线性回归开始，然后扩展到多元线性回归的概念。我们将学习如何估计模型系数，评估拟合优度，并使用假设检验测试系数的显著性。此外，我们还将讨论线性回归背后的假设，并探讨解决潜在问题（如非线性、交互效应、多重共线性异方差性）的技术。我们还将介绍两种广泛使用的正则化技术：岭回归和**最小绝对收缩和选择算子**（**lasso**）惩罚。
- en: By the end of this chapter, you will learn the core principles of linear regression,
    its extensions to regularized linear regression, and the implementation details
    involved.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将学习线性回归的核心原则，其扩展到正则化线性回归，以及涉及的实现细节。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introducing linear regression
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍线性回归
- en: Introducing penalized linear regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍惩罚线性回归
- en: Working with ridge regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用岭回归进行工作
- en: Working with lasso regression
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用lasso回归进行工作
- en: 'To run the code in this chapter, you will need to have the latest versions
    of the following packages:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的代码，您需要拥有以下包的最新版本：
- en: '`ggplot2`, 3.4.0'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ggplot2`, 3.4.0'
- en: '`tidyr`, 1.2.1'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyr`, 1.2.1'
- en: '`dplyr`, 1.0.10'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dplyr`, 1.0.10'
- en: '`car`, 3.1.1'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`car`, 3.1.1'
- en: '`lmtest`, 0.9.40'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lmtest`, 0.9.40'
- en: '`glmnet`, 4.1.7'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`glmnet`, 4.1.7'
- en: Please note that the versions of the packages mentioned in the preceding list
    are the latest ones at the time of writing this chapter. All the code and data
    for this chapter are available at [https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前面提到的包的版本是在编写本章时的最新版本。本章的所有代码和数据均可在[https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R](https://github.com/PacktPublishing/The-Statistics-and-Machine-Learning-with-R-Workshop/blob/main/Chapter_12/working.R)找到。
- en: Introducing linear regression
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍线性回归
- en: At the core of linear regression is the concept of fitting a straight line –
    or more generally, a hyperplane – to the data points. Such fitting aims to minimize
    the deviation between the observed and predicted values. When it comes to simple
    linear regression, one target variable is regressed by one predictor, and the
    goal is to fit a straight line that best mimics the relationship between the two
    variables. For multiple linear regression, there is more than one predictor, and
    the goal is to fit a hyperplane that best describes the relationship among the
    variables. Both tasks can be achieved by minimizing a measure of deviation between
    the predictions and the corresponding targets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的核心是拟合一条直线——或者更一般地说，一个超平面——到数据点。这种拟合旨在最小化观察值和预测值之间的偏差。对于简单线性回归，一个目标变量由一个预测变量回归，目标是拟合一条最佳模拟两个变量之间关系的直线。对于多元线性回归，存在多个预测变量，目标是拟合一个最佳描述变量之间关系的超平面。这两个任务都可以通过最小化预测值和相应目标之间的偏差度量来实现。
- en: In linear regression, obtaining an optimal model means identifying the best
    coefficients that define the relationship between the target variable and the
    input predictors. These coefficients represent the change in the target associated
    with a single unit change in the associated predictor, assuming all other variables
    are constant. This allows us to quantify the magnitude (size of the coefficient)
    and direction (sign of the coefficient) of the relationship between the variables,
    which can be used for inference (highlighting explainability) and prediction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，获得最佳模型意味着识别定义目标变量和输入预测变量之间关系的最佳系数。这些系数代表与相关预测变量的单位变化相关的目标变量的变化，假设其他所有变量保持不变。这使我们能够量化变量之间关系的大小（系数的大小）和方向（系数的符号），这些可以用于推断（强调可解释性）和预测。
- en: When it comes to inference, we often look at the relative impact on the target
    variable given a unit change to the input variable. Examples of such explanatory
    modeling include how marketing spend affects quarterly sales, how smoker status
    affects insurance premiums, and how education affects income. On the other hand,
    predictive modeling focuses on predicting a target quantity. Examples include
    predicting quarterly sales given the marketing spend, predicting the insurance
    premium given a policyholder’s profile information, such as age and gender, and
    predicting income given someone’s education, age, work experience, and industry.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到推断时，我们通常会观察输入变量单位变化对目标变量的相对影响。此类解释建模的例子包括营销支出如何影响季度销售额、吸烟状态如何影响保险费率、以及教育如何影响收入。另一方面，预测建模侧重于预测目标量。例如，根据营销支出预测季度销售额、根据投保人的个人资料信息（如年龄和性别）预测保险费率，以及根据某人的教育、年龄、工作经验和行业预测收入。
- en: In linear regression, the expected outcome is modeled as a weighted sum of all
    the input variables. It also assumes that the change in the output is linearly
    proportional to the change in any input variable. This is the simplest form of
    the regression method.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，预期的结果被建模为所有输入变量的加权总和。它还假设输出的变化与任何输入变量的变化成线性比例。这是回归方法的最简单形式。
- en: Let’s start with **simple linear** **regression** (**SLR**).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从**简单线性** **回归**（**SLR**）开始。
- en: Understanding simple linear regression
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解简单线性回归
- en: SLR is a powerful and widely used statistical model that specifies the relationship
    between two continuous variables, including one input and one output. It allows
    us to understand how a response variable (also referred to as the dependent or
    target variable) changes as the explanatory variable (also called the independent
    variable or the input variable) varies. By fitting a straight line to the observed
    data, SLR quantifies the strength and direction of the linear association between
    the two variables. This straight line is called the SLR **model**. It enables
    us to make predictions and infer the impact of the predictor on the target variable.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: SLR（简单线性回归）是一种强大且广泛使用的统计模型，它规定了两个连续变量之间的关系，包括一个输入和一个输出。它使我们能够理解响应变量（也称为因变量或目标变量）如何随着解释变量（也称为自变量或输入变量）的变化而变化。通过将一条直线拟合到观察到的数据，SLR量化了两个变量之间线性关联的强度和方向。这条直线被称为SLR
    **模型**。它使我们能够进行预测并推断预测变量对目标变量的影响。
- en: 'Specifically, in an SLR model, we assume a linear relationship between the
    target variable (y) and the input variable (x). The model can be represented mathematically
    as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在SLR模型中，我们假设目标变量（y）和输入变量（x）之间存在线性关系。该模型可以用以下数学公式表示：
- en: y = β 0 + β 1 x + ϵ
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: y = β 0 + β 1 x + ϵ
- en: Here, y is called the response variable, dependent variable, explained variable,
    predicted variable, target variable, or regressand. x is called the explanatory
    variable, independent variable, control variable, predictor variable, or regressor.
    β 0 is the intercept of the linear line that represents the expected value of
    y when x is 0\. β 1 is the slope that represents the change in y for a one-unit
    increase in x. Finally, ϵ is the random error term that accounts for the variability
    in the target, y, that the predictor, x, cannot explain.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，y被称为响应变量、因变量、解释变量、预测变量、目标变量或回归量。x被称为解释变量、自变量、控制变量、预测变量或回归变量。β 0是线性线的截距，表示当x为0时y的期望值。β 1是斜率，表示x增加一个单位时y的变化。最后，ϵ是随机误差项，它解释了目标变量y中预测变量x无法解释的变异性。
- en: 'The main objective of SLR is to estimate the β 0 and β 1 parameters. An optimal
    set of β 0 and β 1 would minimize the total squared deviations between the observed
    target values, y, and the predicted values,  ˆ y , using the model. This is called
    the **least squares method**, where we seek the optimal β 0 and β 1 parameters
    that correspond to the minimum **sum of squared** **error** (**SSR**):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型（SLR）的主要目标是估计 β₀ 和 β₁ 参数。一组最优的 β₀ 和 β₁ 参数将最小化观测目标值 y 和预测值 ˆy 之间的总平方偏差。这被称为**最小二乘法**，我们寻求最优的
    β₀ 和 β₁ 参数，它们对应于最小**平方误差和**（SSR）：
- en: minSSR = min∑ i=1 n u i 2 = min(y i − ˆ y i) 2
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: minSSR = min∑i=1nui² = min(yi − ˆyi)²
- en: 'Here, each residual, u i, is the difference between the observation, y i, and
    its fitted value,  ˆ y  i. In simple terms, the objective is to locate the straight
    line that is closest to the data points given. *Figure 12**.1* illustrates a collection
    of data points (in blue) and the linear model (in red):'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个残差 ui 是观测值 yi 和其拟合值 ˆyi 之间的差异。简单来说，目标是找到最接近数据点的直线。*图 12**.1* 展示了一组数据点（蓝色）和线性模型（红色）：
- en: '![Figure 12.1 – The SLR model, where the linear model appears as a line and
    is trained by minimizing the SSR](img/B18680_12_001.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.1 – 线性回归模型，其中线性模型以线的形式出现，并通过最小化 SSR 进行训练](img/B18680_12_001.jpg)'
- en: Figure 12.1 – The SLR model, where the linear model appears as a line and is
    trained by minimizing the SSR
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.1 – 线性回归模型，其中线性模型以线的形式出现，并通过最小化 SSR 进行训练
- en: 'Once we have estimated the model coefficients, β 0 and β 1, we can use the
    model to make predictions and inferences on the intensity of the linear relationship
    between the variables. Such a linear relationship indicates the goodness of fit,
    which is often measured using the coefficient of determination, or R 2\. R 2 ranges
    from `0` to `1` and quantifies the proportion of the total variation in y that
    can be explained by x. It is defined as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们估计了模型系数 β₀ 和 β₁，我们就可以使用该模型对变量之间线性关系的强度进行预测和推断。这种线性关系表明了拟合优度，通常使用确定系数，或 R²
    来衡量。R² 的范围从 `0` 到 `1`，并量化了 y 的总变异中可以由 x 解释的部分。它定义如下：
- en: R 2 = 1 −  ∑ i (y i − ˆ y i) 2 _ ∑ i (y i −  _ y ) 2
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: R² = 1 − ∑i(yi − ˆyi)² / ∑i(yi −  _y)²
- en: Here,  _ y  denotes the average value of the observed target variable, y.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，_y 表示观测目标变量 y 的平均值。
- en: Besides this, we can also use hypothesis testing to test the significance of
    the resulting coefficients, β 0 and β 1, thus helping us determine whether the
    observed relationship between the variables is statistically significant.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以使用假设检验来检验得到的系数 β₀ 和 β₁ 的显著性，从而帮助我们确定变量之间的观测关系是否具有统计学意义。
- en: Let’s go through an example of building a simple linear model using a simulated
    dataset.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个使用模拟数据集构建简单线性模型的例子来进行分析。
- en: Exercise 12.1 – building an SLR model
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 12.1 – 构建线性回归模型
- en: 'In this exercise, we will demonstrate the implementation of an SLR model in
    R. We’ll be using a combination of built-in functions and packages to accomplish
    this task using a simulated dataset:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将演示在 R 中实现线性回归模型。我们将使用内置函数和包的组合，使用模拟数据集来完成这项任务：
- en: 'Simulate a dataset such that the response variable, `Y`, is linearly dependent
    on the explanatory variable, `X`, with some added noise:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模拟一个数据集，使得响应变量 `Y` 线性依赖于解释变量 `X`，并添加了一些噪声：
- en: '[PRE0]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here, we use the `runif()` function to generate the independent variable, `X`,
    which is a vector of random uniform numbers. Then, we add some “noise” to the
    dependent variable, `Y`, making the observed data more realistic and less perfectly
    linear. This is achieved using the `rnorm()` function, which creates a vector
    of random normal numbers. The target variable, `Y`, is then created as a function
    of `X`, plus the noise.
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `runif()` 函数生成自变量 `X`，它是一个随机均匀数的向量。然后，我们在因变量 `Y` 上添加一些“噪声”，使观测数据更真实，且不那么完美地线性。这是通过使用
    `rnorm()` 函数实现的，它创建了一个随机正态数的向量。然后，目标变量 `Y` 被创建为 `X` 的函数，加上噪声。
- en: Besides this, we use a seed (`set.seed(123)`) at the beginning to ensure reproducibility.
    This means that we will get the same set of random numbers every time we run this
    code. Each run will produce a different list of random numbers if we don’t set
    a seed.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，我们在开始时使用了一个种子（`set.seed(123)`）来确保可重复性。这意味着每次运行此代码时，我们都会得到相同的一组随机数。如果我们不设置种子，每次运行都会产生不同的随机数列表。
- en: In this simulation, the true intercept (β 0) is 5, the true slope (β 1) is 0.5,
    and the noise is normally distributed with a mean of 0 and a standard deviation
    of 10.
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个模拟中，真实的截距（β 0）是5，真实的斜率（β 1）是0.5，噪声是均值为0、标准差为10的正态分布。
- en: 'Train a linear regression model based on the simulated dataset using the `lm()`
    function:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`lm()`函数基于模拟数据集训练线性回归模型：
- en: '[PRE1]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here, we use the `lm()` function to fit the data, where `lm` stands for “linear
    model.” This function creates our SLR model. The `Y ~ X` syntax is how we specify
    our model: it tells the function that `Y` is being modeled as a function of `X`.'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`lm()`函数来拟合数据，其中`lm`代表“线性模型”。此函数创建我们的简单线性回归（SLR）模型。`Y ~ X`语法是我们指定模型的方式：它告诉函数`Y`被建模为`X`的函数。
- en: The `summary()` function provides a comprehensive overview of the model, including
    the estimated coefficients, the standard errors, the t-values, and the p-values,
    among other statistics. Since the resulting p-value is extremely low, we can conclude
    that the input variable is predictive with strong statistical significance.
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`summary()`函数提供了模型的全面概述，包括估计系数、标准误差、t值和p值等统计量。由于得到的p值极低，我们可以得出结论，输入变量具有强烈的统计显著性。'
- en: 'Use the `plot()` and `abline()` functions to visualize the data and the fitted
    regression line:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot()`和`abline()`函数可视化数据和拟合的回归线：
- en: '[PRE2]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Running this code generates *Figure 12**.2*:'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码生成*图12.2*：
- en: '![Figure 12.2 – Visualizing the data and the fitted regression line](img/B18680_12_002.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图12.2 – 可视化数据和拟合的回归线](img/B18680_12_002.jpg)'
- en: Figure 12.2 – Visualizing the data and the fitted regression line
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 – 可视化数据和拟合的回归线
- en: Here, the `plot()` function creates a scatter plot of our data, and the `abline()`
    function adds the regression line to this plot. Such a visual representation is
    very useful for understanding the quality of the fitting.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`plot()`函数创建了我们数据的散点图，而`abline()`函数将回归线添加到该图上。这种视觉表示对于理解拟合的质量非常有用。
- en: We’ll move on to the **multiple linear regression** (**MLR**) model in the next
    section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将继续介绍**多元线性回归**（**MLR**）模型。
- en: Introducing multiple linear regression
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍多元线性回归
- en: MLR expands the single predictor in SLR to predict the target outcome based
    on multiple predictor variables. Here, the term “multiple” in MLR refers to the
    multiple predictors in the model, where each feature is given a coefficient. A
    specific coefficient, β, represents the change in the outcome variable for a single
    unit change in the associated predictor variable, assuming all other predictors
    are held constant.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: MLR将SLR中的单个预测因子扩展到基于多个预测变量预测目标结果。在这里，MLR中的“多个”一词指的是模型中的多个预测因子，其中每个特征都被赋予一个系数。特定的系数β代表在相关预测变量单位变化的情况下，假设所有其他预测变量保持不变，结果变量的变化。
- en: 'One of the great advantages of MLR is its ability to include multiple predictors,
    allowing for a more complex and realistic (linear) representation of the real
    world. It can provide a holistic view of the connection between the target and
    all input variables. This is particularly useful in fields where the outcome variable
    is likely influenced by more than one predictor variable. It is modeled via the
    following formula:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MLR（多元线性回归）的一个显著优点是它能够包含多个预测因子，从而允许对现实世界进行更复杂和真实的（线性）表示。它可以提供对目标变量与所有输入变量之间关系的整体视角。这在结果变量可能受多个预测变量影响的领域中特别有用。它通过以下公式进行建模：
- en: y = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p + ϵ
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: y = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p + ϵ
- en: 'Here, we have a total of p features, and therefore, (p + 1) coefficients due
    to the intercept term. ϵ is the usual noise term that represents the unexplained
    part. In other words, our prediction using MLR is as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们总共有p个特征，因此由于截距项，有(p + 1)个系数。ϵ是表示未解释部分的常规噪声项。换句话说，我们使用MLR的预测如下：
- en: ˆ y  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ˆ y  = β 0 + β 1 x 1 + β 2 x 2 + … + β p x p
- en: We can perform ceteris paribus analysis with this formulation, which is a Latin
    way of saying all other things are equal, and we only change one input variable
    to assess its impact on the outcome variable. In other words, MLR allows us to
    explicitly control (that is, keep unchanged) many other factors that simultaneously
    affect the target variable and observe the impact of only one factor.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此公式进行ceteris paribus分析，这是一种拉丁语表达方式，意思是所有其他事物都相等，我们只改变一个输入变量以评估其对结果变量的影响。换句话说，MLR允许我们明确控制（即保持不变）许多同时影响目标变量的其他因素，并观察仅一个因素的影响。
- en: 'For example, suppose we add a small increment, Δ x j, to the feature, x j,
    and keep all other features unchanged. The new prediction,  ˆ y  new, is obtained
    by the following formula:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们向特征x_j添加一个小的增量Δx_j，并保持所有其他特征不变。新的预测ˆy_new是通过以下公式获得的：
- en: ˆ y  new = β 0 + β 1 x 1 + … + β j( x j + Δ x j) + … + β p x p
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ˆy_new = β_0 + β_1 x_1 + … + β_j( x_j + Δx_j) + … + β_p x_p
- en: 'We know that the original prediction is as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道原始预测如下：
- en: ˆ y  old = β 0 + β 1 x 1 + … + β j x j + … + β p x p
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ˆy_old = β_0 + β_1 x_1 + … + β_j x_j + … + β_p x_p
- en: 'The difference between these two gives us the change in the output variable:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个之间的差异给我们带来了输出变量的变化：
- en: Δ ˆ y  =  ˆ y  new −  ˆ y  old = β j Δ x j
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Δˆy = ˆy_new − ˆy_old = β_j Δx_j
- en: What we are doing here is essentially controlling all other input variables
    but only bumping x j to see the impact on the prediction,  ˆ y . So, the coefficient,
    β j, measures the sensitivity of the outcome to a specific feature. When we have
    a unit change, with Δ x j = 1, the change is exactly the coefficient itself, giving
    us Δ ˆ y  = β j.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的实质上是控制所有其他输入变量，但只提高x_j以观察对预测ˆy的影响。因此，系数β_j衡量结果对特定特征的敏感性。当我们有一个单位变化，Δx_j
    = 1时，变化正好是系数本身，给我们Δˆy = β_j。
- en: The next section discusses the measure of the predictiveness of the MLR model.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节讨论了MLR模型的预测性度量。
- en: Seeking a higher coefficient of determination
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 寻求更高的确定系数
- en: MLR tends to perform better than SLR due to the multiple predictors used in
    the model, such as a higher coefficient of determination (R 2). However, a regression
    model with more input variables and a higher R 2 does not necessarily mean that
    the model is a better fit and can predict better for the test set.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: MLR由于模型中使用的多个预测因子（例如更高的确定系数R²）而往往比SLR表现更好。然而，具有更多输入变量和更高R²的回归模型并不一定意味着模型拟合得更好，并且可以更好地预测测试集。
- en: A higher R 2, as a result of more input features, could likely be due to overfitting.
    Overfitting occurs when a model is excessively complex, including too many predictors
    or even interaction terms between predictors. In such cases, the model may fit
    the observed data well (thus leading to a high R 2), but it may perform poorly
    when applied to new, unseen test data. This is because the model might have learned
    not only the underlying structure of the training data but also the random noise
    specific to the dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 由于更多输入特征，更高的R²可能是因为过拟合。过拟合发生在模型过于复杂，包括过多的预测因子甚至预测因子之间的交互项。在这种情况下，模型可能很好地拟合观察到的数据（从而导致高R²），但当应用于新的、未见过的测试数据时，它可能表现不佳。这是因为模型可能不仅学会了训练数据的潜在结构，还学会了特定于数据集的随机噪声。
- en: 'Let’s look at the metric of R 2 more closely. While R 2 measures how well the
    model explains the variance in the outcome variable, it has a major limitation:
    it tends to get bigger as more predictors enter the model, even if those predictors
    are irrelevant. As a remedy, we can use the adjusted R 2\. Unlike R 2, the adjusted
    R 2 explicitly considers the number of predictors and adjusts the resulting statistic
    accordingly. If a predictor improves the model substantially, the adjusted R 2
    will increase, but if a predictor does not improve the model by a significant
    amount, the adjusted R 2 may even decrease.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看R²的度量。虽然R²衡量模型解释结果变量方差的好坏，但它有一个主要限制：随着更多预测因子的进入，它往往会变得更大，即使这些预测因子是不相关的。作为一种补救措施，我们可以使用调整后的R²。与R²不同，调整后的R²明确考虑了预测因子的数量，并相应地调整了结果统计量。如果一个预测因子显著提高了模型，调整后的R²将增加，但如果一个预测因子没有通过显著的数量提高模型，调整后的R²甚至可能减少。
- en: When building statistical models, simpler models are usually preferred when
    they perform similarly to more complex models. This principle of parsimony, also
    known as Occam’s razor, suggests that among models with similar predictive power,
    the simplest one should be chosen. In other words, adding more predictors to the
    model makes it more complex, harder to interpret, and more likely to overfit.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建统计模型时，当简单模型与更复杂的模型表现相似时，通常更倾向于选择简单模型。这个简约原则，也称为奥卡姆剃刀，表明在具有相似预测能力的模型中，应该选择最简单的一个。换句话说，向模型中添加更多的预测器会使模型更加复杂，更难以解释，并且更有可能过拟合。
- en: More on adjusted R 2
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于调整后的R²
- en: The adjusted R 2 improves upon R 2 by adjusting for the number of features in
    the selected model. Specifically, the value of the adjusted R 2 only increases
    if adding this feature is worth more than what would have been expected from adding
    a random feature. Essentially, the additional predictors that are added to the
    model must be meaningful and predictive to lead to a higher adjusted R 2\. These
    additional predictors, however, would always increase R 2 when added to the model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R²通过调整所选模型中的特征数量来改进R²。具体来说，只有当添加这个特征所带来的价值超过添加随机特征所期望的价值时，调整后的R²的值才会增加。本质上，添加到模型中的额外预测器必须是具有意义和预测性的，以导致调整后的R²更高。然而，这些额外的预测器在添加到模型中时，总是会提高R²。
- en: 'The adjusted R 2 addresses this issue by incorporating the model’s degree of
    freedom. Here, the degree of freedom refers to the number of values in a statistical
    calculation that is free to vary. In the context of regression models, this typically
    means the number of predictors. The adjusted R 2 can be expressed as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R²通过包含模型的自由度来解决这一问题。在这里，自由度指的是在统计计算中可以自由变化的值的数量。在回归模型的背景下，这通常意味着预测器的数量。调整后的R²可以表示如下：
- en: Adjusted R 2 = 1 − (1 − R 2)  (n − 1) _ n − p − 1
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的R² = 1 − (1 − R²) × (n − 1) × (n − p − 1)
- en: Here, n denotes the number of observations and p represents the number of features
    in the model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，n表示观测值的数量，p表示模型中的特征数量。
- en: The formula works by adjusting the scale of R 2 based on the count of observations
    and predictors. The term  (n − 1) _ n − p − 1 is a ratio that reflects the degrees
    of freedom in the model, where (n − 1) represents the total degrees of freedom
    in the model. We subtract by 1 because we are estimating the mean of the dependent
    variable from the data. (n − p − 1) represents the degrees of freedom for the
    error, which, in turn, represents the number of observations left over after estimating
    the model parameters. The whole term, (1 − R 2)  (n − 1) _ n − p − 1, denotes
    the error variance that’s been adjusted for the count of predictors.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 该公式通过根据观测值和预测器的数量调整R²的尺度来工作。术语(n − 1) × (n − p − 1)是一个反映模型中自由度的比率，其中(n − 1)代表模型中的总自由度。我们减去1是因为我们是从数据中估计因变量的均值。而(n
    − p − 1)代表误差的自由度，它反过来代表在估计模型参数后剩余的观测值数量。整个术语(1 − R²) × (n − 1) × (n − p − 1)表示调整了预测器数量的误差方差。
- en: Subtracting this from 1 results in the proportion of the total variance explained
    by the model, after adjusting for the number of predictors in the model. In other
    words, it’s a version of R 2 that penalizes the addition of unnecessary predictors.
    This helps to prevent overfitting and makes adjusted R 2 a more balanced measure
    of a model’s explanatory power when comparing models with different numbers of
    predictors.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从1中减去这个值，得到的是模型解释的总方差的比例，这是在调整了模型中预测器的数量之后的结果。换句话说，它是一种R²的版本，惩罚了不必要的预测器的添加。这有助于防止过拟合，使得调整后的R²在比较具有不同预测器数量的模型时，成为一个更平衡的解释力度量。
- en: Let’s look at how to develop an MLR model in R.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在R中开发一个多元线性回归（MLR）模型。
- en: Developing an MLR model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 开发一个MLR模型
- en: In this section, we will develop an MLR model using the same `lm()` function
    in R based on the `mtcars` dataset, which comes preloaded with R and was used
    in previous exercises. Again, the `mtcars` dataset contains measurements for 32
    vehicles from a 1974 Motor Trend issue. These measurements include attributes
    such as miles per gallon (`mpg`), number of cylinders (`cyl`), horsepower (`hp`),
    and weight (`wt`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用R中的相同`lm()`函数，基于预加载的`mtcars`数据集来开发一个MLR模型，该数据集在之前的练习中使用过。再次强调，`mtcars`数据集包含了1974年《汽车趋势》杂志中32辆汽车的测量数据。这些测量包括诸如每加仑英里数（`mpg`）、汽缸数（`cyl`）、马力（`hp`）和重量（`wt`）等属性。
- en: Exercise 12.2 – building an MLR model
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.2 – 构建一个MLR模型
- en: 'In this exercise, we will develop an MLR model to predict `mpg` using `cyl`,
    `hp`, and `wt`. We will then interpret the model results:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将开发一个MLR模型，使用`cyl`、`hp`和`wt`预测`mpg`。然后我们将解释模型结果：
- en: 'Load the `mtcars` dataset and build an MLR that predicts `mpg` based on `cyl`,
    `hp`, and `wt`:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`mtcars`数据集并构建一个基于`cyl`、`hp`和`wt`预测`mpg`的MLR模型：
- en: '[PRE3]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we first load the `mtcars` dataset using the `data()` function, then construct
    the MLR model using the `lm()` function. The `mpg ~ cyl + hp + wt` formula is
    used to specify the model. This formula tells R that we want to model `mpg` as
    a function of `cyl`, `hp`, and `wt`. The `data = mtcars` argument tells R to look
    for these variables in the `mtcars` dataset. The `lm()` function fits the model
    to the data and returns a model object, which we store in the variable model.
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先使用`data()`函数加载`mtcars`数据集，然后使用`lm()`函数构建MLR模型。`mpg ~ cyl + hp + wt`公式用于指定模型。这个公式告诉R，我们想要将`mpg`建模为`cyl`、`hp`和`wt`的函数。`data
    = mtcars`参数告诉R在`mtcars`数据集中寻找这些变量。`lm()`函数将模型拟合到数据，并返回一个模型对象，我们将其存储在变量model中。
- en: 'View the summary of the model:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看模型的摘要：
- en: '[PRE4]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The summary includes the model’s coefficients (the intercept and the slopes
    for each predictor), the residuals (differences between the actual observations
    and predicted values for the target), and several statistics that tell us how
    well the model fits the data, including R 2 and the adjusted R 2.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 摘要包括模型的系数（每个预测变量的截距和斜率）、残差（实际观察值与目标预测值之间的差异），以及一些统计量，告诉我们模型如何拟合数据，包括R²和调整R²。
- en: Let’s interpret the output. Each coefficient represents the expected change
    in `mpg` for a single unit increase in the associated predictor, assuming all
    other predictors are constant. The R 2 value, which is `0.8431`, denotes the proportion
    of variance (over 84%) in `mpg` that can be explained by the predictors together.
    Again, the adjusted R 2 value, which is `0.8263`, is a modified R 2 that accounts
    for the number of features in the model.
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们解释输出结果。每个系数代表在相关预测变量增加一个单位时，`mpg`的预期变化，假设其他所有预测变量保持不变。R²值，即`0.8431`，表示`mpg`中可由预测变量共同解释的方差比例（超过84%）。再次强调，调整R²值，即`0.8263`，是一个经过修改的R²，它考虑了模型中的特征数量。
- en: In addition, the p-values for each predictor test the null hypothesis that the
    true value of the coefficient is zero. If a predictor’s p-value is smaller than
    a preset significance level (such as 0.05), we would reject this null hypothesis
    and conclude that the predictor is statistically significant. In this case, `wt`
    is the only statistically significant factor compared with others using a significance
    level of 5%.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，每个预测变量的p值测试了系数真实值为零的零假设。如果一个预测变量的p值小于预设的显著性水平（如0.05），我们会拒绝这个零假设，并得出结论说该预测变量具有统计学意义。在这种情况下，与使用5%显著性水平比较的其他因素相比，`wt`是唯一具有统计学意义的因素。
- en: In the MLR model, all coefficients are negative, indicating a reverse direction
    of travel between the input variable and the target. However, we cannot conclude
    that all the predictors negatively correlate with the target variable. The correlation
    between the individual predictor and the target variable could be positive or
    negative in SLR.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLR模型中，所有系数都是负数，表明输入变量和目标变量之间存在反向的旅行方向。然而，我们不能得出所有预测变量都与目标变量负相关的结论。在SLR中，单个预测变量与目标变量的相关性可能是正的或负的。
- en: The next section provides more context on this phenomenon.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将提供更多关于这一现象的背景信息。
- en: Introducing Simpson’s Paradox
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍辛普森悖论
- en: Simpson’s Paradox says that a trend appears in different data groups but disappears
    or changes when combined. In the context of regression analysis, Simpson’s Paradox
    can appear when a variable that seems positively correlated with the outcome might
    be negatively correlated when we control other variables.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 辛普森悖论指出，趋势在不同数据组中显现，但在合并时消失或改变。在回归分析的情况下，当控制其他变量时，一个看似与结果正相关的变量可能具有负相关性，辛普森悖论就可能出现。
- en: Essentially, this paradox illustrates the importance of considering confounding
    variables and not drawing conclusions from aggregated data without understanding
    the context. The confounding variables are those not among the explanatory variables
    under consideration but are related to both the target variable and the predictors.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，这个悖论说明了考虑混杂变量的重要性，以及在未理解背景的情况下，不要从汇总数据中得出结论。混杂变量是那些不在考虑的解释变量中，但与目标变量和预测变量都相关的变量。
- en: Let’s consider a simple example through the following exercise.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过以下练习来考虑一个简单的例子。
- en: Exercise 12.3 – illustrating Simpson’s Paradox
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.3 – 说明辛普森悖论
- en: 'In this exercise, we will look at two scenarios with opposite signs of coefficient
    values for the same feature in both SLR and MLR:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将观察两个场景，在SLR和MLR中，相同特征的系数值符号相反：
- en: 'Create a dummy dataset with two predictors and one output variable:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含两个预测变量和一个输出变量的虚拟数据集：
- en: '[PRE5]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `x1` is a set of 100 numbers randomly generated from a standard normal
    distribution. `x2` is a linear function of `x1` but with a negative correlation,
    and some random noise is added (via `rnorm(100)`). `y` is then generated as a
    linear function of `x1` and `x2`, again with some random noise added. All three
    variables are compiled into a DataFrame, `df`.
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`x1`是一组从标准正态分布中随机生成的100个数字。`x2`是`x1`的线性函数，但具有负相关性，并添加了一些随机噪声（通过`rnorm(100)`）。然后，`y`作为`x1`和`x2`的线性函数生成，同样添加了一些随机噪声。所有三个变量都编译到一个DataFrame，`df`中。
- en: 'Train an SLR model with `y` as the outcome and `x1` as the input features.
    Check the summary of the model:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`y`作为结果变量和`x1`作为输入特征来训练一个简单线性回归（SLR）模型。检查模型的摘要：
- en: '[PRE6]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The result shows that `x1` is negatively correlated with `y` due to a negative
    coefficient of `-2.1869`.
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，由于系数为`-2.1869`，`x1`与`y`呈负相关。
- en: 'Train an SLR model with `y` as the target and `x1` and `x2` as the input features.
    Check the summary of the model:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`y`作为目标变量，`x1`和`x2`作为输入特征来训练一个SLR模型。检查模型的摘要：
- en: '[PRE7]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The result shows that the estimated coefficient for `x1` is now a positive quantity.
    Does this suggest that `x1` is suddenly positively correlated with `y`? No, since
    there are likely other confounding variables that lead to a positive coefficient.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，`x1`的估计系数现在是一个正数。这表明`x1`突然与`y`呈正相关吗？不，因为可能存在其他导致系数为正的混杂变量。
- en: The key takeaway is that we can only make inferences on the (positive or negative)
    correlation between a predictor and a target outcome in an SLR setting. For example,
    if we build an SLR model to regress `y` against `x`, we can conclude that `x`
    and `y` are positively correlated if the resulting coefficient is positive (β
    > 0). Similarly, if β > 0, we can conclude that `x` and `y` are positively correlated.
    The same applies to the case of negative correlation.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的启示是，我们只能在SLR设置中（正或负）对预测变量和目标结果之间的相关性进行推断。例如，如果我们构建一个SLR模型来对`y`进行回归，如果得到的系数是正的（β
    > 0），我们可以得出结论，`x`和`y`是正相关的。同样，如果β > 0，我们可以得出结论，`x`和`y`是正相关的。同样适用于负相关的情况。
- en: However, such inference breaks in an MLR setting – that is, we cannot conclude
    a positive correlation if β > 0, and vice versa.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多元线性回归（MLR）设置中，这样的推断会失效 – 也就是说，如果我们有β > 0，我们不能得出正相关的结论，反之亦然。
- en: Let’s take this opportunity to interpret the results. The `Estimate` column
    shows the estimated regression coefficients. These values indicate how much the
    `y` variable is expected to increase when the corresponding predictor variable
    increases by one unit while holding all other features constant. In this case,
    for each unit increase in `x1`, `y` is expected to increase by approximately `0.93826`
    units, and for each unit increase in `x2`, `y` is expected to increase by approximately
    `1.02381` units. The `(Intercept)` row shows the estimated value of `y` when all
    predictor variables in the model are zero. In this model, the estimated intercept
    is `2.13507`.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用这个机会来解释结果。`Estimate`列显示了估计的回归系数。这些值表示当相应的预测变量增加一个单位，同时保持所有其他特征不变时，`y`变量预期会增加多少。在这种情况下，对于`x1`每增加一个单位，`y`预计会增加大约`0.93826`个单位，对于`x2`每增加一个单位，`y`预计会增加大约`1.02381`个单位。`(Intercept)`行显示了当模型中的所有预测变量都为零时`y`的估计值。在这个模型中，估计的截距是`2.13507`。
- en: '`Std. Error` represents the standard errors for the estimates. Smaller values
    here indicate more precise estimates. The `t value` column shows the t-statistics
    for the hypothesis test that the corresponding population regression coefficient
    is zero, given that the other predictors are in the model. A larger absolute value
    of the t-statistic indicates stronger evidence against the null hypothesis. The
    `Pr(>|t|)` column gives the p-values for the hypothesis tests. In this case, both
    `x1` and `x2` have p-values below `0.05`, indicating that both are statistically
    significant predictors of `y` at the 5% significance level.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`标准误差`表示估计的标准误差。这里的值越小，表示估计越精确。`t 值`列显示了假设检验的 t 统计量，即给定其他预测变量在模型中，对应的总体回归系数为零。t
    统计量绝对值越大，表明对零假设的证据越强。`Pr(>|t|)`列给出了假设检验的 p 值。在这种情况下，`x1` 和 `x2` 的 p 值都低于 `0.05`，表明在
    5% 的显著性水平下，两者都是 y 的统计显著预测变量。'
- en: Finally, the multiple R-squared and adjusted R-squared values provide measures
    of how well the model fits the data. The multiple R-squared value is `0.8484`,
    indicating that this model explains approximately 84.84% of the variability in
    `y`. The adjusted R-squared value adjusts this measure for the number of features
    in the model. As discussed, it is a better measure when comparing models with
    different numbers of predictors. Here, the adjusted R-squared value is `0.8453`.
    The F-statistic and its associated p-value are used to test the hypothesis that
    all population regression coefficients are zero. A small p-value (less than `0.05`)
    indicates that we can reject this hypothesis, and conclude that at least one of
    the predictors is useful in predicting `y`.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，多重 R 平方和调整 R 平方值提供了模型拟合数据的度量。多重 R 平方值为 `0.8484`，表明该模型解释了 y 的变异性约 84.84%。调整
    R 平方值对此度量进行了模型中特征数量的调整。如前所述，当比较具有不同数量预测变量的模型时，这是一个更好的度量。这里的调整 R 平方值为 `0.8453`。F
    统计量及其相关的 p 值用于检验所有总体回归系数为零的假设。小的 p 值（小于 `0.05`）表明我们可以拒绝这个假设，并得出结论，至少有一个预测变量在预测
    y 时是有用的。
- en: The next section looks at the situation when we have a categorical predictor
    in the MLR model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将探讨在 MLR 模型中有一个分类预测变量的情况。
- en: Working with categorical variables
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与分类变量一起工作
- en: 'In MLR, the process of including a binary predictor is similar to including
    a numeric predictor. However, the interpretation differs. Consider a dataset where
    y is the target variable, x 1 is a numeric predictor, and x 2 is a binary predictor:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在多元线性回归（MLR）中，包含一个二元预测变量的过程与包含一个数值预测变量的过程类似。然而，解释不同。考虑一个数据集，其中 y 是目标变量，x 1 是数值预测变量，x 2
    是二元预测变量：
- en: y = β 0 + β 1 x 1 + β 2 x 2 + ϵ
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: y = β 0 + β 1 x 1 + β 2 x 2 + ϵ
- en: In this model, x 2 is coded as 0 and 1, and its corresponding coefficient, β 2,
    represents the difference in the mean values of y between the two groups identified
    by x 2.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，x 2 被编码为 0 和 1，其对应的系数 β 2 代表了由 x 2 识别的两个组之间 y 的均值差异。
- en: For example, if x 2 is a binary variable representing sex (0 for males, 1 for
    females), and y is salary, then β 2 represents the average difference in salary
    between females and males, after accounting for the value of x 1.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 x 2 是一个表示性别的二元变量（男性为 0，女性为 1），y 是工资，那么 β 2 代表在考虑 x 1 的值后，女性和男性之间的平均工资差异。
- en: Note that the interpretation of the coefficient of a binary predictor is dependent
    on the other variables in the model. So, in the preceding example, β 2 is the
    difference in salary between females and males for given values of x 1.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，二元预测变量的系数解释依赖于模型中的其他变量。因此，在前面的例子中，β 2 是在给定 x 1 的值时，女性和男性之间的工资差异。
- en: On the implementation side, R automatically creates dummy variables when a factor
    is used in a regression model. So, if x 2 were a factor with levels of “male”
    and “female,” R would handle the conversion to 0 and 1 internally when fitting
    the model.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现方面，当在回归模型中使用因子时，R 会自动创建虚拟变量。因此，如果 x 2 是一个具有“男性”和“女性”级别的因子，R 将在拟合模型时内部处理将其转换为
    0 和 1。
- en: 'Let’s look at a concrete example. In the following code, we’re building an
    MLR model to predict `mpg` using `qsec` and `am`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个具体的例子。在下面的代码中，我们正在构建一个 MLR 模型，使用 `qsec` 和 `am` 来预测 `mpg`：
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Note that the `am` variable is treated as a numeric variable. Since it represents
    the type of transmission in the car (`0` = automatic, `1` = manual), it should
    have been treated as a categorical variable. This can be achieved by converting
    it into a factor, as shown here:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`am` 变量被当作数值变量处理。因为它代表汽车中的传动类型（`0` = 自动，`1` = 手动），它本应被当作分类变量处理。这可以通过将其转换为因子来实现，如下所示：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that only one variable, `am_cat1`, is created for the categorical variable,
    `am_cat`. This is because `am_cat` is binary, thus we only need one dummy column
    (keeping only `am_cat1` and removing `am_cat0` in this case) to represent the
    original categorical variable. In general, for a categorical variable with k categorical,
    R will automatically create (k − 1) dummy variables in the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，只为分类变量 `am_cat` 创建了一个变量 `am_cat1`。这是因为 `am_cat` 是二元的，因此我们只需要一个虚拟列（在这种情况下保留
    `am_cat1` 并删除 `am_cat0`）来表示原始分类变量。一般来说，对于一个有 k 个分类的分类变量，R 将在模型中自动创建 (k − 1) 个虚拟变量。
- en: This process is called `am` was equal to the corresponding level, and 0 otherwise.
    This essentially creates a set of indicators that capture the presence or absence
    of each category. Finally, since we can infer the last dummy variable based on
    the values of the previous (`k-1`) dummy variables, we can remove the last dummy
    variable in the resulting one-hot encoded set of variables.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为 `am` 等于相应的水平，否则为 0。这实际上创建了一组指标，捕捉每个类别的存在或不存在。最后，由于我们可以根据前一个（`k-1`）虚拟变量的值推断最后一个虚拟变量，因此我们可以从结果中删除最后一个虚拟变量。
- en: 'Using a categorical variable introduces a vertical shift to the model estimate,
    as discussed in the following section. To see this, let’s look more closely at
    the impact of the categorical variable, `am_cat1`. Our MLR model now assumes the
    following form:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分类变量会给模型估计引入一个垂直位移，如以下章节所述。为了看到这一点，让我们更仔细地看看分类变量 `am_cat1` 的影响。我们的 MLR 模型现在假设以下形式：
- en: ˆ y  = β 0 + β 1 x qsec + β 2 x am_cat
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ˆ y  = β 0 + β 1 x qsec + β 2 x am_cat
- en: 'We know that x am_cat is a binary variable. When x am_cat = 0, the prediction
    becomes as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 x am_cat 是一个二元变量。当 x am_cat = 0 时，预测如下：
- en: ˆ y  = β 0 + β 1 x qsec
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ˆ y  = β 0 + β 1 x qsec
- en: 'When x am_cat = 1, the prediction is as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 当 x am_cat = 1 时，预测如下：
- en: ˆ y  = β 0 + β 1 x qsec + β 2 = (β 0 + β 2) + β 1 x qsec
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ˆ y  = β 0 + β 1 x qsec + β 2 = (β 0 + β 2) + β 1 x qsec
- en: By comparing these two quantities, we can see that they are two linear models
    parallel to each other since the slope is the same and the only difference is
    β 2 in the intercept term.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较这两个量，我们可以看出它们是两个相互平行的线性模型，因为斜率相同，唯一的区别是截距项中的 β 2。
- en: 'A visual illustration helps here. In the following code snippet, we first create
    a new DataFrame, `newdata`, that covers the range of `qsec` values in the original
    data, for each of the `am_cat` values (0 and 1). Then, we use the `predict()`
    function to get the predicted `mpg` values from the model for this new data. Next,
    we plot the original data points with `geom_point()` and add two regression lines
    with `geom_line()`, where the lines are based on the predicted values in `newdata`.
    The `color = am_cat` aesthetic setting adds different colors to the points and
    lines for the different `am_cat` values, and the labels are adjusted in `scale_color_discrete()`
    so that 0 corresponds to “Automatic” and 1 corresponds to “Manual”:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一个视觉说明在这里很有帮助。在下面的代码片段中，我们首先创建一个新的 DataFrame，`newdata`，它覆盖了原始数据中 `qsec` 值的范围，对于每个
    `am_cat` 值（0 和 1）。然后，我们使用 `predict()` 函数从模型中获取新数据的预测 `mpg` 值。接下来，我们使用 `geom_point()`
    绘制原始数据点，并使用 `geom_line()` 添加两条回归线，其中线基于 `newdata` 中的预测值。`color = am_cat` 美学设置为不同的
    `am_cat` 值添加不同的颜色，并在 `scale_color_discrete()` 中调整标签，以便 0 对应于“自动”，1 对应于“手动”：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running this code generates *Figure 12**.3*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码生成 *图 12**.3*：
- en: '![Figure 12.3 – Visualizing the two linear regression models based on different
    transmission types. These two lines are parallel to each other due to a shift
    in the intercept term](img/B18680_12_003.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 12.3 – 基于不同传动类型的两个线性回归模型的可视化。由于截距项的位移，这两条线是平行的](img/B18680_12_003.jpg)'
- en: Figure 12.3 – Visualizing the two linear regression models based on different
    transmission types. These two lines are parallel to each other due to a shift
    in the intercept term
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.3 – 基于不同传动类型的两个线性回归模型的可视化。由于截距项的位移，这两条线是平行的
- en: What this figure suggests is that manual transmission cars have the same miles
    per gallon (`mpg`) more than automatic transmission cars given the same quarter-mile
    time (`qsec`). However, this is unlikely in practice since different car types
    (manual versus automatic) will likely have different quarter-mile times. In other
    words, there is an interaction effect between these two variables.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表明，在相同的四分之一英里时间（`qsec`）下，手动变速汽车比自动变速汽车的每加仑英里数（`mpg`）相同。然而，在实际情况中，由于不同类型的汽车（手动与自动）可能具有不同的四分之一英里时间，这不太可能。换句话说，这两个变量之间存在交互作用。
- en: The following section introduces the interaction term as a remedy to this situation.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将介绍交互项作为这种情况的补救措施。
- en: Introducing the interaction term
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引入交互项
- en: In regression analysis, an interaction occurs when the effect of one predictor
    on the target variable differs depending on the level of another predictor variable.
    In our running example, we are essentially looking at whether the relationship
    between `mpg` and `qsec` is different for different values of `am`. In other words,
    we are testing whether the slope of the line relating `mpg` and `qsec` differs
    for manual (`am`=1) and automatic (`am`=0) transmissions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归分析中，当一个预测变量的影响因另一个预测变量的水平而不同时，就会发生交互作用。在我们的运行示例中，我们实际上是在查看`mpg`和`qsec`之间的关系是否因`am`的不同值而不同。换句话说，我们正在测试将`mpg`和`qsec`联系起来的线的斜率是否因手动（`am`=1）和自动（`am`=0）变速而不同。
- en: For example, if there is no interaction effect, then the impact of `qsec` on
    `mpg` is the same, regardless of whether the car has a manual or automatic transmission.
    This would mean that the lines depicting the relationship between `mpg` and `qsec`
    for manual and automatic cars would be parallel.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果没有交互作用，那么`qsec`对`mpg`的影响是相同的，无论汽车是手动还是自动。这意味着描述手动和自动汽车之间`mpg`与`qsec`关系的线条将是平行的。
- en: If there is an interaction effect, then the effect of `qsec` on `mpg` differs
    for manual and automatic cars. This would mean that the lines depicting the relationship
    between `mpg` and `qsec` for manual and automatic cars would not be parallel.
    They could either cross or, more commonly, just have different slopes.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在交互作用，那么`qsec`对`mpg`的影响在手动和自动汽车之间是不同的。这意味着描述手动和自动汽车之间`mpg`与`qsec`关系的线条不会平行。它们可能相交，或者更常见的是，具有不同的斜率。
- en: 'To depict these differences in relationships, we can add an interaction term
    to the model, which is done using the `*` operator. For example, the formula for
    a regression model with an interaction between `qsec` and `am_cat` would be `mpg
    ~ qsec * am_cat`. This is equivalent to `mpg ~ qsec + am``_cat + qsec:am_cat`,
    where `qsec:am_cat` represents the interaction term. The following code shows
    the details:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描绘这些关系中的差异，我们可以在模型中添加一个交互项，这可以通过`*`运算符来完成。例如，具有`qsec`和`am_cat`之间交互的回归模型的公式将是`mpg
    ~ qsec * am_cat`。这相当于`mpg ~ qsec + am``_cat + qsec:am_cat`，其中`qsec:am_cat`代表交互项。以下代码显示了详细信息：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s also plot the updated model, which consists of two intersecting lines
    due to the interaction effect. In the following code snippet, `geom_smooth(method
    =""l"", se = FALSE)` is used to fit different linear lines to each group of points
    (automatic and manual cars). `as.factor(am_cat)` is used to treat `am_cat` as
    a factor (categorical) variable so that a separate line is fit for each category:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再绘制一个更新后的模型，该模型由由于交互作用而产生的两条相交线组成。在以下代码片段中，使用`geom_smooth(method =""l"",
    se = FALSE)`为每组点（自动和手动汽车）拟合不同的线性线。`as.factor(am_cat)`用于将`am_cat`视为一个因子（分类）变量，以便为每个类别拟合一条单独的线：
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Running this code generates *Figure 12**.4*:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码生成*图12*.4：
- en: '![Figure 12.4 – Two intersecting lines due to the intersection term between
    quarter-mile time and transmission type](img/B18680_12_004.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图12.4 – 由于四分之一英里时间和变速类型之间的交叉项而产生的两条相交线](img/B18680_12_004.jpg)'
- en: Figure 12.4 – Two intersecting lines due to the intersection term between quarter-mile
    time and transmission type
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 – 由于四分之一英里时间和变速类型之间的交叉项而产生的两条相交线
- en: 'The next section focuses on another related topic: nonlinear terms.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将关注另一个相关主题：非线性项。
- en: Handling nonlinear terms
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理非线性项
- en: Linear regression is a widely used model for understanding the linear relationships
    between a response and explanatory variables. However, not all underlying relationships
    in the data are linear. In many situations, a feature and a response variable
    may not have a straight-line relationship, thus necessitating the handling of
    nonlinear terms in the linear regression model to increase its flexibility.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是理解响应变量和解释变量之间线性关系的一个广泛使用的模型。然而，数据中的所有潜在关系并不都是线性的。在许多情况下，特征和响应变量可能没有直线关系，因此需要在线性回归模型中处理非线性项以增加其灵活性。
- en: The simplest way to incorporate nonlinearity, and therefore build a curve instead
    of a straight line, is by including polynomial terms of predictors in the regression
    model. In polynomial regression, some or all predictors are raised to a specific
    polynomial term – for example, transforming a feature, x, into x 2 or x 3.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 将非线性引入的最简单方法，即构建曲线而不是直线，是在回归模型中包含预测变量的多项式项。在多项式回归中，一些或所有预测变量被提升到特定的多项式项 – 例如，将特征x转换为x²或x³。
- en: Let’s go through an exercise to understand the impact of adding polynomial features
    to a linear regression model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来了解向线性回归模型添加多项式特征的影响。
- en: Exercise 12.4 – adding polynomial features to a linear regression model
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.4 – 向线性回归模型添加多项式特征
- en: 'In this exercise, we will create a simple dataset where the relationship between
    the input feature, x, and the target variable, y, is not linear, but quadratic.
    First, we will fit an SLR model, then add a quadratic term and compare the results:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将创建一个简单的数据集，其中输入特征x和目标变量y之间的关系不是线性的，而是二次的。首先，我们将拟合一个简单线性回归（SLR）模型，然后添加二次项并比较结果：
- en: 'Generate a sequence of `x` values ranging from -10 to 10\. For each `x`, compute
    the corresponding `y` as the square of `x`, plus some random noise to show a (noisy)
    quadratic relationship. Put `x` and `y` in a DataFrame:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个从-10到10的`x`值序列。对于每个`x`，计算相应的`y`作为`x`的平方，并添加一些随机噪声以显示（有噪声的）二次关系。将`x`和`y`放入DataFrame中：
- en: '[PRE13]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The result suggests a not-so-good model fitting to the data, which possesses
    a nonlinear relationship.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果表明，模型对数据的拟合度不佳，数据具有非线性关系。
- en: 'Fit a quadratic model to the data by including x 2 as a predictor using the
    `I()` function. Print the summary of the model:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`I()`函数将x²作为预测变量包括在内，对数据进行二次模型拟合。打印模型的摘要：
- en: '[PRE14]'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The result shows that the polynomial model fits the data better than the simple
    linear model. Thus, adding nonlinear terms can improve model fit when the relationship
    between predictors and the response is not strictly linear.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果显示，多项式模型比简单线性模型更好地拟合数据。因此，当预测变量和响应变量之间的关系不是严格线性时，添加非线性项可以提高模型拟合度。
- en: 'Plot the linear and quadratic models together with the data:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将线性模型和二次模型与数据一起绘制：
- en: '[PRE15]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Here, we first calculate the predicted values for both models and add them to
    the DataFrame. Then, we create a scatter plot of the data and add two lines representing
    the predicted values from the linear model (in blue) and the quadratic model (in
    red).
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们首先计算两个模型的预测值并将它们添加到DataFrame中。然后，我们创建一个散点图来表示数据，并添加两条线来表示线性模型（蓝色）和二次模型（红色）的预测值。
- en: 'Running this code generates *Figure 12**.5*. The result suggests that adding
    a polynomial feature could extend the flexibility of a linear model:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码生成**图12.5**。结果表明，添加多项式特征可以扩展线性模型的灵活性：
- en: '![Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear
    data](img/B18680_12_005.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图12.5 – 非线性数据的线性与二次拟合可视化](img/B18680_12_005.jpg)'
- en: Figure 12.5 – Visualizing the linear and quadratic fits to the nonlinear data
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.5 – 非线性数据的线性与二次拟合可视化
- en: Other common ways to introduce nonlinearity include the logarithmic transformation
    (logx) or a square root transformation (√ _ x ). These transformations can also
    be applied to the target variable, y, and we can have multiple transformed features
    in the same linear model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 引入非线性的一些其他常见方法包括对数变换（logx）或平方根变换（√x）。这些变换也可以应用于目标变量y，我们可以在同一个线性模型中有多个转换特征。
- en: Note that the model with transformed features remains a linear model. If there
    is a nonlinear transformation to the coefficients, the model would be a nonlinear
    one.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，具有转换特征的模型仍然是一个线性模型。如果系数有非线性转换，则模型将是非线性模型。
- en: 'The next section sheds more light on a widely used type of transformation:
    the logarithmic transformation.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将更详细地介绍一种广泛使用的变换类型：对数变换。
- en: More on the logarithmic transformation
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多关于对数变换的内容
- en: The logarithmic transformation, or log transformation, maps an input to the
    corresponding output based on the logarithmic function, giving y = logx. A popular
    reason behind using such a transformation is to introduce nonlinearity in the
    linear regression model. When the relationship between the input features and
    the target output is nonlinear, applying a transformation can sometimes linearize
    the relationship, making it possible to model the relationship with a linear regression
    model. For the logarithmic transformation, it can help when the rate of change
    in the outcome variable increases or decreases as the value of the predictor increases.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 对数变换，或称对数转换，基于对数函数将输入映射到相应的输出，即 y = logx。使用这种变换的一个流行原因是向线性回归模型中引入非线性。当输入特征与目标输出之间的关系是非线性时，应用变换有时可以使关系线性化，从而可以使用线性回归模型来建模这种关系。对于对数变换，当预测变量的值增加时，结果变量的变化率增加或减少时，它可以帮助实现这一点。
- en: To be specific, the rate of change decreases as the input becomes more extreme.
    The natural consequence of such a transformation is that potential outliers in
    the input data are squeezed so that they appear less extreme in the transformed
    column. In other words, the resulting linear regression model will be less sensitive
    to the original outliers due to the log transformation.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，当输入变得更为极端时，变化率会降低。这种变换的自然结果是，输入数据中的潜在异常值被压缩，使得它们在变换后的列中看起来不那么极端。换句话说，由此产生的线性回归模型将由于对数变换而对原始异常值不那么敏感。
- en: Another side benefit of using log transformation is its ability to deal with
    heteroscedasticity. Heteroscedasticity is when the variability of the error term
    in a regression model is not constant across all levels of the predictors. This
    violates one of the assumptions of linear regression models and can lead to inefficient
    and biased estimates. In this case, log transformations can stabilize the variance
    of the error term by shrinking the potential big error terms, making it more constant
    across different levels of the predictors.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数变换的另一个副作用是它能够处理异方差性。异方差性是指回归模型中误差项的变异性在预测变量的所有水平上并不恒定。这违反了线性回归模型的一个假设，可能导致效率低下和有偏的估计。在这种情况下，通过对数变换可以缩小潜在的大的误差项，使得误差项的方差在不同预测变量的水平上更加恒定。
- en: Lastly, when the relationship between predictors and the outcome is multiplicative
    rather than additive, taking the log of the predictors and/or the outcome variable
    can convert the relationship into an additive one, which can be modeled using
    linear regression.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，当预测变量与结果之间的关系是乘法而非加法时，对预测变量和/或结果变量取对数可以将关系转换为加法关系，这可以使用线性回归进行建模。
- en: 'Let’s consider an example where we predict the miles per gallon (`mpg`) from
    horsepower (`hp`). We’ll compare the model where we predict `mpg` directly from
    `hp` and another model where we predict the log of `mpg` from `hp`, as shown in
    the following code snippet:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个例子，其中我们预测每加仑英里数（`mpg`）从马力（`hp`）。我们将比较直接从`hp`预测`mpg`的模型，以及从`hp`预测`mpg`的对数的模型，如下面的代码片段所示：
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Running this code generates *Figure 12**.6*, where we can see a slight curvature
    in the blue line:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码生成*图12*.6，其中我们可以看到蓝色线有轻微的弯曲：
- en: '![Figure 12.6 – Visualizing the original and log-transformed model](img/B18680_12_006.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图12.6 – 可视化原始和对数变换后的模型](img/B18680_12_006.jpg)'
- en: Figure 12.6 – Visualizing the original and log-transformed model
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.6 – 可视化原始和对数变换后的模型
- en: Note that the log transformation can only be applied to positive data. In the
    case of `mtcars$mpg`, all values are positive, so we can safely apply the log
    transformation. If the variable included zero or negative values, we would need
    to consider a different transformation or approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对数变换只能应用于正数数据。在`mtcars$mpg`的情况下，所有值都是正数，因此我们可以安全地应用对数变换。如果变量包含零或负值，我们需要考虑不同的变换或方法。
- en: The next section focuses on deriving and using the closed-form solution to the
    linear regression model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将重点介绍推导和使用线性回归模型的闭式解。
- en: Working with the closed-form solution
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用闭式解
- en: When developing a linear regression model, the available training set (X, y)
    is given, and the only unknown parameters are the coefficients, β. Here, a bold
    lowercase letter means a vector (such as β and y), and a bold uppercase letter
    denotes a matrix (such as X). It turns out that the closed-form solution to a
    linear regression model can be derived using the concept of the **ordinary least
    squares** (**OLS**) estimator, which aims to minimize the sum of the squared residuals
    in the model. Having the closed-form solution means we can simply plug in the
    required elements (in this case, X and y) and perform the calculation to obtain
    the solution, without resorting to any optimization procedure.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发线性回归模型时，已知的训练集（X, y）被给出，唯一未知的参数是系数，β。在这里，粗体小写字母表示向量（例如 β 和 y），而粗体大写字母表示矩阵（例如
    X）。结果证明，线性回归模型的闭式解可以通过使用**普通最小二乘法**（**OLS**）估计的概念来推导，该估计法旨在最小化模型中残差的平方和。拥有闭式解意味着我们可以简单地插入所需元素（在这种情况下，X
    和 y）并执行计算以获得解，而无需求助于任何优化过程。
- en: 'Specifically, given a data matrix, X (which includes a column of ones for the
    intercept term and is in bold to indicate more than one feature), of dimensions
    n × p (where n is the number of observations and p is the number of predictors)
    and a response vector, y, of length n, the OLS estimator for the coefficient vector,
    β, is given by the following formula:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，给定一个数据矩阵，X（其中包含一个用于截距项的列，并用粗体表示表示有多个特征），其维度为 n × p（其中 n 是观测数，p 是预测数）和一个长度为
    n 的响应向量，y，OLS 对系数向量 β 的估计由以下公式给出：
- en: β = (X T X) −1 X T y
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: β = (X T X) −1 X T y
- en: This solution assumes that the term (X T X) is invertible, meaning it should
    be a full-rank matrix. If this is not the case, the solution either does not exist
    or is not unique.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解假设项 (X T X) 是可逆的，这意味着它应该是一个满秩矩阵。如果不是这种情况，解可能不存在或不唯一。
- en: 'Now, let’s look at how to derive this solution. We start with the minimization
    problem for the least squares: minimizing (y − Xβ) T(y − Xβ) over β. This quadratic
    form can be expanded to y T y − β T X T y − y T Xβ + β T X T Xβ. Note that β T
    X T y = y T Xβ since both terms are scalars and therefore are equal to each other
    after the transpose operation. We can write the **residual sum of squares** (**RSS**)
    expression as y T y − 2 β T X T y + β T X T Xβ.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何推导这个解。我们从最小二乘法的最小化问题开始：最小化 (y − Xβ) T(y − Xβ) 关于 β。这个二次型可以展开为 y T
    y − β T X T y − y T Xβ + β T X T Xβ。注意 β T X T y = y T Xβ，因为这两个项都是标量，因此在转置操作后它们相等。我们可以将**残差平方和**（**RSS**）表达式写为
    y T y − 2 β T X T y + β T X T Xβ。
- en: 'Here, we apply the first-order condition to solve for the value of β that minimizes
    this expression (recall that the point that either minimizes or maximizes a graph
    has a derivative of 0). This means that we would set its first derivative to zero,
    leading to the following formula:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们应用一阶条件来求解使此表达式最小化的 β 值（回想一下，图形上最小化或最大化的点具有导数为 0）。这意味着我们会将其一阶导数设为零，从而得到以下公式：
- en: ∂ ( y T y − 2 β T X T y + β T X T Xβ)  ____________________  ∂ β  = − 2 X T
    y + 2 X T Xβ = 0
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ∂ ( y T y − 2 β T X T y + β T X T Xβ)  ____________________  ∂ β  = − 2 X T
    y + 2 X T Xβ = 0
- en: X T Xβ = X T y
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: X T Xβ = X T y
- en: β = (X T X) −1 X T y
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: β = (X T X) −1 X T y
- en: Thus, we have derived the closed-form solution of β that minimizes the sum of
    the squared residuals. Let’s go through an example to see how it can be implemented.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经推导出 β 的闭式解，该解最小化了残差平方和。让我们通过一个示例来看看它是如何实现的。
- en: Implementing the closed-form solution
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现闭式解
- en: 'Let’s look at implementing the OLS estimation in R for an SLR model. An example
    that uses synthetic data is shown in the following code snippet:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在 R 中实现线性回归模型的 OLS 估计。以下代码片段展示了使用合成数据的示例：
- en: '[PRE17]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Here, we generate 100 observations with a single input feature, where the observation
    is noise-perturbed and follows a process given by y = β 0 + β 1 x + ϵ. The error
    term assumes a normal distribution that’s parameterized by a mean of 0 and a standard
    deviation of 2.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们生成具有单个输入特征的 100 个观测值，观测值受到噪声扰动，并遵循 y = β 0 + β 1 x + ϵ 的过程。误差项假设为具有均值为
    0 和标准差为 2 的正态分布。
- en: Before proceeding to the estimation, note that we also appended a column of
    1s on the left of the input feature, `x`, to form a matrix, `X`. This column of
    1s is used to indicate the intercept term and is often referred to as the bias
    trick. That is, the coefficient, β 0, for the intercept term will be part of the
    coefficient vector, and there is no need to create a separate coefficient just
    for the intercept.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行估计之前，请注意，我们在输入特征`x`的左侧也添加了一个1的列，以形成一个矩阵`X`。这个1的列用于表示截距项，通常被称为偏差技巧。也就是说，截距项的系数β0将是系数向量的一部分，因此不需要为截距创建一个单独的系数。
- en: 'Let’s calculate the result using the closed-form solution:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用闭式解来计算结果：
- en: '[PRE18]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, `%*%` is used for matrix multiplication, `t(X)` is the transpose of `X`,
    and `solve()` is used to calculate the inverse of a matrix.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`%*%`用于矩阵乘法，`t(X)`是`X`的转置，`solve()`用于计算矩阵的逆。
- en: 'We can also run the linear regression procedure using the `lm()` function for
    comparison:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`lm()`函数运行线性回归过程进行比较：
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The results are the same as the ones that we obtained via the manual approach.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 结果与通过手动方法获得的结果相同。
- en: 'The next two sections cover two common issues in linear regression settings:
    multicollinearity and heteroskedasticity.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 下两节涵盖了线性回归设置中的两个常见问题：共线性和非同方差性。
- en: Dealing with multicollinearity
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理共线性
- en: Multicollinearity refers to the case when two (or more) predictors are highly
    correlated in a multiple regression model. This means that one independent variable
    can be linearly predicted from the others with a high degree of accuracy. This
    is a situation that we do not want to fall into. In other words, we would like
    to see a high degree of correlation between the predictors and the target variable,
    while a low degree of correlation among these predictors themselves.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 共线性指的是在多元回归模型中，两个（或更多）预测变量高度相关的情况。这意味着一个自变量可以以很高的精度从其他变量线性预测出来。这是我们不想陷入的情况。换句话说，我们希望看到预测变量与目标变量之间有高度的相关性，而预测变量之间本身的相关性较低。
- en: In the face of multicollinearity in a linear regression model, the resultant
    model tends to generate unreliable and unstable estimates of the regression coefficients.
    It can inflate the coefficients of the parameters, making them statistically insignificant,
    even though they might be substantively important. In addition, multicollinearity
    makes it difficult to assess the effect of each independent variable on the dependent
    variable as the effects are intertwined. However, it does not affect the predictive
    power or interpretability of the model; instead, it only changes the calculations
    for individual features.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 面对线性回归模型中的共线性，得到的模型往往会生成不可靠和不稳定的回归系数估计。它可能会膨胀参数的系数，使它们在统计上不显著，尽管它们可能具有实质性的重要性。此外，共线性使得难以评估每个自变量对因变量的影响，因为影响是交织在一起的。然而，它不会影响模型的预测能力或可解释性；相反，它只改变了单个特征的计算。
- en: Detecting any potential multicollinearity among the predictors can be performed
    by examining the pair-wise correlation. Alternatively, we can resort to a particular
    test statistic called the **variance inflation factor** (**VIF**), which quantifies
    how much the variance is increased due to multicollinearity. A VIF of 1 indicates
    that two variables are not correlated, while a VIF greater than 5 (in many fields)
    would suggest a problematic amount of multicollinearity.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 检测预测变量之间是否存在潜在的共线性可以通过检查成对的相关性来完成。或者，我们可以求助于一个特定的统计量，称为**方差膨胀因子**（**VIF**），它量化了由于共线性导致方差增加的程度。VIF值为1表示两个变量不相关，而VIF值大于5（在许多领域）则表明存在问题的共线性程度。
- en: When multicollinearity exists in the linear regression model, we could choose
    to keep one predictor only and remove all other highly correlated predictors.
    We can also combine these correlated variables into a few uncorrelated ones via
    **principle component analysis** (**PCA**), a widely used technique for dimension
    reduction. Besides this, we can resort to ridge regression to control the magnitude
    of the coefficients; this will be introduced later in this chapter.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当线性回归模型中存在共线性时，我们可以选择只保留一个预测变量，并移除所有其他高度相关的预测变量。我们还可以通过**主成分分析**（**PCA**），这是一种广泛用于降维的技术，将这些相关变量组合成几个不相关的变量。此外，我们可以求助于岭回归来控制系数的大小；这将在本章后面介绍。
- en: 'To check multicollinearity using VIF, we can use the `vif()` function from
    the `car` package, as shown in the following code snippet:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用VIF检查多重共线性，我们可以使用`car`包中的`vif()`函数，如下面的代码片段所示：
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Looking at the result, `disp` seems to have high multicollinearity (VIF = 7.32
    > 5), suggesting that it has a strong correlation with `hp` and `wt`. This implies
    that `disp` is not providing much information that is not already contained in
    the other two predictors.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果来看，`disp`似乎具有高度的多重共线性（VIF = 7.32 > 5），这表明它与`hp`和`wt`有很强的相关性。这意味着`disp`并没有提供其他两个预测变量中已经包含的信息。
- en: To handle the multicollinearity here, we can consider removing `disp` from the
    model since it has the highest VIF, applying PCA to combine the three predictors,
    or using ridge or lasso regression (more on this in the last two sections of this
    chapter).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理这里的多重共线性问题，我们可以考虑从模型中移除`disp`，因为它具有最高的VIF值，应用PCA来合并三个预测变量，或者使用岭回归或Lasso回归（关于这一点，请参阅本章的最后两部分）。
- en: The next section focuses on the issue of heteroskedasticity.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将重点讨论异方差性的问题。
- en: Dealing with heteroskedasticity
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理异方差性
- en: Heteroskedasticity (or heteroscedasticity) refers to the situation in which
    the variability of the error term, or residuals, is not the same across all levels
    of the independent variables. This violates one of the key assumptions of OLS
    linear regression, which assumes that the residuals have a constant variance –
    in other words, the residuals are homoskedastic. Violating this assumption could
    lead to incorrect inferences on the statistical significance of the coefficients
    since the resulting standard errors of the regression coefficients could be larger
    or smaller than they should be.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 异方差性（或异方差性）指的是误差项或残差的变异性在不同独立变量的所有水平上并不相同的情况。这违反了OLS线性回归的一个关键假设，即假设残差具有恒定的方差——换句话说，残差是同方差的。违反这个假设可能导致对系数的统计显著性推断不正确，因为回归系数的估计标准误差可能比应有的更大或更小。
- en: There are a few ways to handle heteroskedasticity. First, we can transform the
    outcome variable using the logarithmic function, as introduced earlier. Other
    functions, such as taking the square root or inverse of the original outcome variable,
    could also help reduce heteroskedasticity. Advanced regression models such as
    **weighted least squares** (**WLS**) or **generalized least squares** (**GLS**)
    may also be explored to reduce the impact of heteroskedasticity.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 处理异方差性的方法有几种。首先，我们可以使用之前介绍的对数函数对结果变量进行转换。其他函数，如取原始结果变量的平方根或倒数，也可能有助于减少异方差性。探索高级回归模型，如**加权最小二乘法**（WLS）或**广义最小二乘法**（GLS），也可能有助于减少异方差性的影响。
- en: 'To formally test for heteroskedasticity, we can conduct a Breusch-Pagan test
    using the `bptest()` function from the `lmtest` package. In the following code
    snippet, we fit an MLR model to predict `mpg` using `wt` and `hp`, followed by
    performing the Breusch-Pagan test:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正式检验异方差性，我们可以使用`lmtest`包中的`bptest()`函数进行Breusch-Pagan检验。在下面的代码片段中，我们拟合了一个MLR模型，使用`wt`和`hp`预测`mpg`，然后执行Breusch-Pagan检验：
- en: '[PRE21]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since the p-value (`0.6438`) is greater than 0.05, we do not reject the null
    hypothesis of the Breusch-Pagan test. This suggests that there is not enough evidence
    to say that heteroskedasticity is present in the regression model. So, we would
    conclude that the variances of the residuals are not significantly different from
    being constant, or homoskedastic.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 由于p值（`0.6438`）大于0.05，我们不能拒绝Breusch-Pagan检验的零假设。这表明没有足够的证据表明回归模型中存在异方差性。因此，我们可以得出结论，残差的方差与恒定的方差没有显著差异，或者说，残差是同方差的。
- en: The next section shifts to looking at regularized linear regression models and
    ridge and lasso penalties.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将转向查看正则化线性回归模型和岭回归和Lasso惩罚。
- en: Introducing penalized linear regression
  id: totrans-231
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入惩罚线性回归
- en: Penalized regression models, such as ridge and lasso, are techniques that are
    used to handle problems such as multicollinearity, reduce overfitting, and even
    perform variable selection, especially when dealing with high-dimensional data
    with multiple input features.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚回归模型，如岭回归和Lasso，是用于处理多重共线性、减少过拟合甚至进行变量选择的技术，尤其是在处理具有多个输入特征的高维数据时。
- en: Ridge regression (also called L2 regularization) is a method that adds a penalty
    equivalent to the square of the magnitude of coefficients. We would add this term
    to the loss function after weighting it by an additional hyperparameter, often
    denoted as λ, to control the strength of the penalty term.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归（也称为 L2 正则化）是一种方法，它添加了一个与系数幅度的平方相当的惩罚。我们会在加权一个额外的超参数后，通常表示为 λ，将其添加到损失函数中，以控制惩罚项的强度。
- en: Lasso regression (L1 regularization), on the other hand, is a method that, similar
    to ridge regression, adds a penalty for non-zero coefficients, but unlike ridge
    regression, it can force some coefficients to be exactly equal to zero when the
    penalty tuning parameter is large enough. The larger the value of the hyperparameter,
    λ, the greater the amount of shrinkage. The penalty on the size of coefficients
    helps reduce model complexity and multicollinearity, leading to a model that can
    generalize better on unseen data. However, ridge regression includes all the features
    in the final model, so it doesn’t induce any sparsity. Therefore, it’s not particularly
    useful for variable selection.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，Lasso 回归（L1 正则化）是一种方法，类似于岭回归，它对非零系数添加惩罚，但与岭回归不同，当惩罚调整参数足够大时，它可以强制某些系数精确等于零。超参数
    λ 的值越大，收缩量就越大。对系数大小的惩罚有助于减少模型复杂性和多重共线性，从而使得模型在未见数据上具有更好的泛化能力。然而，岭回归包括最终模型中的所有特征，因此不会产生任何稀疏性。因此，它对变量选择并不特别有用。
- en: In summary, ridge and lasso regression are both penalized linear regression
    methods that add a constraint regarding the magnitude of the estimated coefficients
    to the model optimization process, which helps prevent overfitting, manage multicollinearity,
    and reduce model complexity. However, ridge tends to include all predictors in
    the model and helps reduce their effect, while lasso can exclude predictors from
    the model altogether, leading to a simpler and more interpretable model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，岭回归和 Lasso 回归都是惩罚线性回归方法，它们在模型优化过程中添加了对估计系数幅度的约束，这有助于防止过度拟合、管理多重共线性并减少模型复杂度。然而，岭回归倾向于将所有预测变量包含在模型中并帮助减少它们的影响，而
    Lasso 可以完全排除模型中的预测变量，从而得到更简单、更可解释的模型。
- en: Let’s start with ridge regression and look at its loss function more closely.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从岭回归开始，更仔细地看看它的损失函数。
- en: Working with ridge regression
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用岭回归
- en: Ridge regression, also referred to as L2 regularization, is a commonly used
    technique to alleviate overfitting in linear regression models by penalizing the
    magnitude of the estimated coefficients in the resulting model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归，也称为 L2 正则化，是一种常用的技术，通过惩罚线性回归模型中估计系数的幅度来减轻过度拟合。
- en: 'Recall that in an SLR model, we seek to minimize the sum of the squared differences
    between our predicted and actual values, which we refer to as the least squares
    method. The loss function we wish to minimize is the RSS:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在简单线性回归（SLR）模型中，我们试图最小化预测值和实际值之间平方差的和，这被称为最小二乘法。我们希望最小化的损失函数是均方误差（RSS）：
- en: RSS = ∑ i=1 n (y i − (β 0 + ∑ j=1 p β j x ij)) 2
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: RSS = ∑_i=1^n (y_i - (β_0 + ∑_j=1^p β_j x_ij))^2
- en: Here, y i is the actual target value, β 0 is the intercept term, {β j} are the
    coefficient estimates for each predictor, x ij, and the summations are overall
    observations and predictors.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，y_i 是实际的目标值，β_0 是截距项，{β_j} 是每个预测变量 x_ij 的系数估计，求和是总的观测值和预测变量。
- en: 'Purely minimizing the RSS would give us an overfitting model, as represented
    by the high magnitude of the resulting coefficients. As a remedy, we could apply
    ridge regression by adding a penalty term to this loss function. This penalty
    term is the sum of the squares of each coefficient multiplied by a tuning parameter,
    λ. So, the ridge regression loss function (also known as the **cost function**)
    is as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 仅最小化 RSS 会导致过度拟合的模型，如结果系数的高幅度所示。作为补救措施，我们可以通过向这个损失函数添加惩罚项来应用岭回归。这个惩罚项是每个系数的平方和乘以一个调整参数
    λ。因此，岭回归损失函数（也称为**成本函数**）如下：
- en: L ridge = RSS + λ∑ j=1 p β j 2
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: L_ridge = RSS + λ∑_j=1^p β_j^2
- en: Here, the λ parameter is a user-defined tuning parameter. A larger λ means a
    higher penalty and a smaller λ means less regularization effect. λ = 0 gives the
    ordinary least squares regression result, while as λ approaches infinity, the
    impact of the penalty term dominates, and the coefficient estimates approach zero.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，λ参数是一个用户定义的调整参数。较大的λ意味着更高的惩罚，较小的λ意味着更少的正则化效果。λ = 0给出普通最小二乘回归结果，而当λ接近无穷大时，惩罚项的影响占主导地位，系数估计接近零。
- en: By adding this penalty term, ridge regression tends to decrease the size of
    the coefficients, which can help mitigate the problem of multicollinearity (where
    predictors are highly correlated). It does this by spreading the coefficient estimates
    of correlated predictors across each other, which can lead to a more stable and
    interpretable model.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加这个惩罚项，岭回归倾向于减小系数的大小，这有助于缓解多重共线性问题（预测变量高度相关）。它是通过将相关预测变量的系数估计分散到彼此之间来实现的，这可能导致一个更稳定且可解释的模型。
- en: However, it’s important to note that ridge regression does not typically produce
    sparse solutions and does not perform variable selection. In other words, it will
    not result in a model where some coefficients are exactly zero (unless λ is infinite),
    thus all predictors are included in the model. If feature selection is important,
    methods such as lasso (L1 regularization) or elastic net (a combination of L1
    and L2 regularization) might be more appropriate.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，需要注意的是，岭回归通常不会产生稀疏解，并且不执行变量选择。换句话说，它不会导致某些系数恰好为零的模型（除非λ是无穷大），因此所有预测变量都包含在模型中。如果特征选择很重要，那么lasso（L1正则化）或弹性网络（L1和L2正则化的组合）等方法可能更合适。
- en: Note that we would often penalize the intercept, β 0.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们通常会惩罚截距，β 0。
- en: Let’s go through an exercise to learn how to develop a ridge regression model.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来学习如何开发岭回归模型。
- en: Exercise 12.5 – implementing ridge regression
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.5 – 实现岭回归
- en: 'In this exercise, we will implement a ridge regression model and compare the
    estimated coefficients with the OLS model. Our implementation will be based on
    the `glmnet` package:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将实现一个岭回归模型，并将估计系数与OLS模型进行比较。我们的实现将基于`glmnet`包：
- en: 'Install and load the `glmnet` package:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安装并加载`glmnet`包：
- en: '[PRE22]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Here, we use an `if-else` statement to detect if the `glmnet` package is installed.
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用`if-else`语句来检测`glmnet`包是否已安装。
- en: 'Store all columns other than `mpg` as predictors in `X` and `mpg` as the target
    variable in `y`:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将除`mpg`之外的所有列作为预测变量存储在`X`中，将`mpg`作为目标变量存储在`y`中：
- en: '[PRE23]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit a ridge regression model using the `glmnet()` function:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`glmnet()`函数拟合岭回归模型：
- en: '[PRE24]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Here, the `alpha` parameter controls the type of model we fit. `alpha = 0` fits
    a ridge regression model, `alpha = 1` fits a lasso model, and any value in between
    fits an elastic net model.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，`alpha` 参数控制着我们拟合的模型类型。`alpha = 0` 拟合岭回归模型，`alpha = 1` 拟合lasso模型，而介于两者之间的任何值都会拟合弹性网络模型。
- en: 'Use cross-validation to choose the best `lambda` value:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证选择最佳的`lambda`值：
- en: '[PRE25]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Here, we use the cross-validation approach to identify the optimal `lambda`
    that gives the lowest error on the cross-validation set on average. All repeated
    cross-validation steps are completed via the `cv.glmnet()` function.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用交叉验证方法来识别平均误差最低的`lambda`值。所有重复的交叉验证步骤都是通过`cv.glmnet()`函数完成的。
- en: 'Fit a new ridge regression model using the optimal `lambda` and extract its
    coefficients without the intercept:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最优`lambda`拟合一个新的岭回归模型，并提取没有截距的系数：
- en: '[PRE26]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Fit a linear regression model and extract its coefficients:'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合线性回归模型并提取其系数：
- en: '[PRE27]'
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Plot the coefficients of both models on the same graph:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一张图上绘制两个模型的系数：
- en: '[PRE28]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Running this code generates *Figure 12**.7*:'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码生成*图12**.7*：
- en: '![Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS
    models](img/B18680_12_007.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![图12.7 – 可视化岭回归和OLS模型的估计系数](img/B18680_12_007.jpg)'
- en: Figure 12.7 – Visualizing the estimated coefficients from the ridge and OLS
    models
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.7 – 可视化岭回归和OLS模型的估计系数
- en: This plot shows that the estimated coefficients from the ridge regression model
    are, in general, smaller than those from the OLS model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 此图显示，岭回归模型的估计系数通常小于OLS模型的估计系数。
- en: The next section focuses on lasso regression.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将重点介绍lasso回归。
- en: Working with lasso regression
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用lasso回归进行工作
- en: Lasso regression is another type of regularized linear regression. It is similar
    to ridge regression but differs in terms of the specific process of calculating
    the magnitude of the coefficients. Specifically, it uses the L1 norm of the coefficients,
    which consists of the total sum of absolute values of the coefficients, as the
    penalty that’s added to the OLS loss function.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归是另一种正则化线性回归。它与岭回归相似，但在计算系数大小具体过程上有所不同。具体来说，它使用系数的L1范数，即系数绝对值总和的总和，作为添加到OLS损失函数中的惩罚项。
- en: 'The lasso regression cost function can be written as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归的成本函数可以写成如下形式：
- en: L lasso = RSS + λ∑ j=1 p | β j|
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: L lasso = RSS + λ∑ j=1 p | β j|
- en: The key characteristic of lasso regression is that it can reduce some coefficients
    exactly to 0, effectively performing variable selection. This is a consequence
    of the L1 penalty term and is not the case for ridge regression, which can only
    shrink coefficients close to 0\. Therefore, lasso regression is particularly useful
    when we believe that only a subset of the predictors matters when it comes to
    predicting the outcome.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归的关键特性是它可以精确地将一些系数减少到0，从而有效地执行变量选择。这是L1惩罚项的结果，而岭回归只能将系数缩小到接近0。因此，当我们认为只有预测变量的一部分对预测结果有影响时，Lasso回归特别有用。
- en: In addition, in contrast to ridge regression, which can’t perform variable selection
    and therefore may be less interpretable, lasso regression automatically selects
    the most important features and discards the rest, which can make the final model
    easier to interpret.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，与岭回归不同，岭回归无法执行变量选择，因此可能不太容易解释，而Lasso回归自动选择最重要的特征并丢弃其余特征，这使得最终模型更容易解释。
- en: Like ridge regression, the lasso regression penalty term is also subject to
    a tuning parameter, λ. The optimal λ parameter is typically chosen via cross-validation
    or a more intelligent search policy such as Bayesian optimization.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 与岭回归一样，Lasso回归的惩罚项也受一个调整参数λ的影响。最佳λ参数通常通过交叉验证或更智能的搜索策略，如贝叶斯优化来选择。
- en: Let’s go through an exercise to understand how to develop a lasso regression
    model.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个练习来了解如何开发Lasso回归模型。
- en: Exercise 12.6 – implementing lasso regression
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习12.6 – 实现Lasso回归
- en: 'To implement a lasso regression model, we can follow a similar process as we
    did for the ridge regression model:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现Lasso回归模型，我们可以遵循与岭回归模型类似的过程：
- en: 'To fit a lasso regression model, set `alpha = 1` in the `glmnet()` function:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要拟合Lasso回归模型，在`glmnet()`函数中将`alpha`设置为`1`：
- en: '[PRE29]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Use the same cross-validation procedure to identify the optimal value of `lambda`:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的交叉验证过程来识别`lambda`的最佳值：
- en: '[PRE30]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Fit a new lasso regression model using the optimal `lambda`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳`lambda`值拟合一个新的Lasso回归模型：
- en: '[PRE31]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The resulting coefficients can also be extracted using the `coef()` function,
    followed by `[-1]` to remove the intercept term:'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 也可以使用`coef()`函数提取结果系数，然后跟`[-1]`以移除截距项：
- en: '[PRE32]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Plot the estimated coefficients together with the previous two models:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将估计系数与前面两个模型一起绘制：
- en: '[PRE33]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Running this code generates *Figure 12**.8*, which suggests that only two variables
    are kept in the resultant model. So, the lasso regression model can produce a
    sparse solution by setting the coefficients of some features equal to zero:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 运行此代码生成*图12*.8，这表明结果模型中只保留了两个变量。因此，Lasso回归模型可以通过将某些特征的系数设置为0来生成稀疏解：
- en: '![Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso,
    and OLS regression models](img/B18680_12_008.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图12.8 – 可视化岭回归、Lasso回归和OLS回归模型的估计系数](img/B18680_12_008.jpg)'
- en: Figure 12.8 – Visualizing the estimated coefficients from the ridge, lasso,
    and OLS regression models
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.8 – 可视化岭回归、Lasso回归和OLS回归模型的估计系数
- en: In summary, the lasso regression model gives us a sparse model by setting the
    coefficients of non-significant variables to zero, thus achieving feature selection
    and model estimation at the same time.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 总结，Lasso回归模型通过将非显著变量的系数设置为0，从而实现特征选择和模型估计。
- en: Summary
  id: totrans-297
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we covered the nuts and bolts of the linear regression model.
    We started by introducing the SLR model, which consists of only one input variable
    and one target variable, and then extended to the MLR model with two or more predictors.
    Both models can be assessed using R 2, or more preferably, the adjusted R 2 metric.
    Next, we discussed specific scenarios, such as working with categorical variables
    and interaction terms, handling nonlinear terms via transformations, working with
    the closed-form solution, and dealing with multicollinearity and heteroskedasticity.
    Lastly, we introduced widely used regularization techniques such as ridge and
    lasso penalties, which can be incorporated into the loss function as a penalty
    term and generate a regularized model, and, additionally, a sparse solution in
    the case of lasso regression.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了线性回归模型的基本原理。我们首先介绍了单变量线性回归模型（SLR），它只包含一个输入变量和一个目标变量，然后扩展到多变量线性回归模型（MLR），它包含两个或更多预测变量。这两个模型都可以使用R²来评估，或者更偏好使用调整后的R²指标。接下来，我们讨论了特定场景，例如处理分类变量和交互项，通过变换处理非线性项，使用闭式解，以及处理多重共线性异方差性。最后，我们介绍了广泛使用的正则化技术，如岭回归和Lasso惩罚，这些技术可以作为损失函数中的惩罚项被纳入，生成正则化模型，在Lasso回归的情况下，还可以生成稀疏解。
- en: 'In the next chapter, we will cover another type of widely used linear model:
    the logistic regression model.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种广泛使用的线性模型：逻辑回归模型。
