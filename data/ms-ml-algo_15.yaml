- en: Advanced Policy Estimation Algorithms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级策略估计算法
- en: In this chapter, we will continue our exploration of the world of **Reinforcement
    Learning** (**RL**), focusing our attention on complex algorithms that can be
    employed to solve difficult problems. As this is still the introductory part of
    RL (the whole topic is extremely large), the structure of the chapter is based
    on many practical examples that can be used as a basis to work on more complex
    scenarios.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探索强化学习（**Reinforcement Learning**，**RL**）的世界，关注那些可以用于解决难题的复杂算法。由于这仍然是强化学习的入门部分（整个主题极其庞大），本章的结构基于许多实用的例子，这些例子可以作为在更复杂场景中工作的基础。
- en: 'The topics that will be discussed in this chapter are:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将讨论的主题包括：
- en: TD(λ) algorithm
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TD(λ) 算法
- en: Action-Critic TD(0)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行为-评论家 TD(0)
- en: SARSA
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSA
- en: Q-learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning with a simple visual input and a neural network
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于简单视觉输入和神经网络的 Q-learning
- en: TD(λ) algorithm
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TD(λ) 算法
- en: 'In the previous chapter, we introduced the temporal difference strategy, and
    we discussed a simple example called TD(0). In the case of TD(0), the discounted
    reward is approximated by using a one-step backup. Hence, if the agent performs
    an action *a[t]* in the state *s[t]*, and the transition to the state *s[t][+1]*
    is observed, the approximation becomes the following:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了时间差分策略，并讨论了一个简单的例子，称为 TD(0)。在 TD(0) 的情况下，折现奖励是通过使用一步回溯来近似的。因此，如果智能体在状态
    *s[t]* 执行动作 *a[t]*，并且观察到转换到状态 *s[t][+1]*，则近似变为以下：
- en: '![](img/aa44f3c3-5483-4558-a869-23c89e357ce9.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aa44f3c3-5483-4558-a869-23c89e357ce9.png)'
- en: 'If the task is episodic (as in many real-life scenarios) and has *T(e[i])*
    steps, the complete backup for the episode *e[i]* is as follows:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任务是分段的（如在许多现实场景中）并且有 *T(e[i])* 步，则分段 *e[i]* 的完整回溯如下：
- en: '![](img/99c8410c-cb8c-4dd6-8873-2a1648151699.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/99c8410c-cb8c-4dd6-8873-2a1648151699.png)'
- en: 'The previous expression ends when the MDP process reaches an absorbing state;
    therefore, *R[t]* is the actual value of the discounted reward. The difference
    between TD(0) and this choice is clear: in the first case, we can update the value
    function after each transition, whereas with a complete backup, we need to wait
    for the end of the episode. We can say that this method (which is called Monte
    Carlo, because it''s based on the idea of averaging the overall reward of an entire
    sequence) is exactly the opposite of TD(0); therefore, it''s reasonable to think
    about an intermediate solution, based on *k*-step backups. In particular, our
    goal is to find an online algorithm that can exploit the backups once they are
    available.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 上述表达式在 MDP 过程达到吸收状态时结束；因此，*R[t]* 是实际折现奖励的价值。TD(0) 和此选择之间的区别很明显：在前一种情况下，我们可以在每次转换后更新价值函数，而使用完整回溯则需要等待分段的结束。我们可以说这种方法（被称为蒙特卡洛，因为它基于对整个序列总体奖励的平均化思想）正好是
    TD(0) 的对立面；因此，考虑基于 *k*-步回溯的中间解决方案是合理的。特别是，我们的目标是找到一个在线算法，一旦回溯可用，就可以利用它们。
- en: 'Let''s imagine a sequence of four steps. The agent is in the first state and
    observes a transition; at this point, only a one-step backup is possible, and
    it''s a good idea to update the value function in order to improve the convergence
    speed. After the second transition, the agent can use a two-step backup; however,
    it can also consider the first one-step backup in addition to the newer, longer
    one. So, we have two approximations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一个由四个步骤组成的序列。智能体处于第一个状态并观察到一次转换；在这个时候，只能进行一步回溯，并且更新价值函数以提高收敛速度是个好主意。在第二次转换之后，智能体可以使用两步回溯；然而，它也可以考虑除了较新的、更长的回溯之外的第一步回溯。因此，我们有两种近似：
- en: '![](img/b661dfb8-7ffc-4f7f-a4fc-45b5503b9c5b.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b661dfb8-7ffc-4f7f-a4fc-45b5503b9c5b.png)'
- en: 'Which of the preceding is the most reliable? Obviously, the second one depends
    on the first one (in particular, when the value function is almost stabilized),
    and so on until the end of the episode. Hence, the most common strategy is to
    employ a weighted average that assigns a different level of importance to each
    backup (assuming the longest backup has *k* steps):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前面哪一个是更可靠的？显然，第二个依赖于第一个（特别是当价值函数几乎稳定时），等等，直到分段的结束。因此，最常见的策略是采用加权平均，为每个回溯分配不同的重要性水平（假设最长的回溯有
    *k* 步）：
- en: '![](img/48597f36-c27f-4ee8-919c-dd9aa6a04017.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/48597f36-c27f-4ee8-919c-dd9aa6a04017.png)'
- en: 'Watkins (in *Learning from Delayed Rewards*, *Watkins C.I.C.H.*, *Ph.D. Thesis*,
    *University of Cambridge*, *1989*) proved that this approach (with or without
    averaging) has the fundamental property of reducing the absolute error of the
    expected *R[t]^k*, with respect to the optimal value function, *V(s; π)*. In fact,
    he proved that the following inequality holds:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Watkins 在 *《延迟奖励学习》*（Watkins C.I.C.H.，博士论文，剑桥大学，1989年）中证明了这种方法（无论是否有平均）具有减少相对于最优值函数
    *V(s; π)* 的期望 *R[t]^k* 的绝对误差的基本性质。事实上，他证明了以下不等式成立：
- en: '![](img/a0fda46a-9ff2-4faf-a08b-b5303e14a459.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a0fda46a-9ff2-4faf-a08b-b5303e14a459.png)'
- en: As *γ* is bounded between 0 and 1, the right-hand side is always smaller than
    the maximum absolute error *V(t) - V(s;π)*, where *V(s)* is the value of a state
    during an episode. Therefore, the expected discounted return of a *k*-step backup
    (or of a combination of different backups) yields a more accurate estimation of
    the optimal value function if the policy is chosen to be greedy with respect to
    it. This is not surprising, as a longer backup incorporates more actual returns,
    but the importance of this theorem resides in its validity when an average of
    different *k*-step backups are employed. In other words, it provides us with the
    mathematical proof that an intuitive approach actually converges, and it can also
    effectively improve both the convergence speed and the final accuracy.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 *γ* 被限制在 0 和 1 之间，右侧始终小于最大绝对误差 *V(t) - V(s;π)*，其中 *V(s)* 是在一段时间内某个状态的价值。因此，*k*-步回溯（或不同回溯的组合）的期望折现回报提供了对最优值函数更准确的估计，如果策略被选择为相对于它贪婪的话。这并不令人惊讶，因为更长的回溯包含了更多的实际回报，但这个定理的重要性在于它在使用不同
    *k*-步回溯的平均值时的有效性。换句话说，它为我们提供了数学证明，即直观的方法实际上会收敛，并且它还可以有效地提高收敛速度和最终精度。
- en: 'However, managing *k* coefficients is generally problematic, and in many cases,
    useless. The main idea behind TD(λ) is to employ a single factor, *λ*, that can
    be tuned in order to meet specific requirements. The theoretical analysis (or
    *forward view*, as referred to by Sutton and Barto) is based, in a general case,
    on an exponentially decaying average. If we consider a geometric series with *λ*
    bounded between 0 and 1 (exclusive), we get:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，管理 *k* 系数通常是有问题的，在许多情况下是无用的。TD(λ)背后的主要思想是使用一个可以调整以满足特定需求的单一因子，*λ*。理论分析（或称为Sutton和Barto所指的“前视图”）在一般情况下基于指数衰减平均。如果我们考虑一个
    *λ* 被限制在 0 和 1（不包括）之间的几何级数，我们得到：
- en: '![](img/4e979110-e2a8-4991-8e52-26b61805f81b.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4e979110-e2a8-4991-8e52-26b61805f81b.png)'
- en: 'Hence, we can consider the averaged discounted return *R[t]^((λ))* with infinite
    backups as:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '因此，我们可以考虑无限回溯的平均折现回报 *R[t]^((λ)*):'
- en: '![](img/fae3add0-ff56-49ca-9f3c-1549039fb66f.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fae3add0-ff56-49ca-9f3c-1549039fb66f.png)'
- en: Before defining the finite case, it's helpful to understand how *R[t]^((λ))*
    was built. As *λ* is bounded between 0 and 1, the factors decay proportionally
    to *λ*, so the first backup has the highest impact, and all of the subsequent
    ones have smaller and smaller influences on the estimation. This means that, in
    general, we are assuming that the estimation of *R[t]* has more importance to
    the *immediate* backups (which become more and more precise), and we exploit the
    longer ones only to improve the estimated value. Now, it should be clear that *λ*
    = 0 is equivalent to TD(0), because only the one-step backup remains in the sum
    (remember that *0⁰ = 1*), while higher values involve all of the remaining backups. Let's
    now consider an episode *e[i]* whose length is *T(e[i])*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义有限情况之前，了解 *R[t]^((λ)*) 是如何构建的有助于理解。由于 *λ* 被限制在 0 和 1 之间，因子按比例衰减到 *λ*，因此第一次回溯影响最大，所有后续的回溯对估计的影响越来越小。这意味着，在一般情况下，我们假设对
    *R[t]* 的估计对 *立即*回溯（它们变得越来越精确）更重要，我们只利用较长的回溯来提高估计的价值。现在，应该清楚的是，*λ* = 0 等同于 TD(0)，因为只有一步回溯保留在求和中（记住
    *0⁰ = 1*），而更高的值涉及所有剩余的回溯。现在，让我们考虑一个长度为 *T(e[i]*) 的回溯 *e[i]*。
- en: 'Conventionally, if the agent reached an absorbing state at *t = T(e[i])*, all
    of the remaining *t*+*i* returns are equal to *R*[*t* ](this is straightforward,
    as all of the possible rewards have already been collected); therefore, we can
    truncate *R[t]^((λ))*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '传统上，如果智能体在 *t = T(e[i]*) 达到吸收状态，所有剩余的 *t+i* 返回都等于 *R[t*](这是直截了当的，因为所有可能奖励都已经收集完毕)；因此，我们可以截断
    *R[t]^((λ)*):'
- en: '![](img/fe22f4da-626c-4928-ab5c-6d920feedfba.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fe22f4da-626c-4928-ab5c-6d920feedfba.png)'
- en: The first term of the previous expression involves all of the non-terminal states,
    while the second is equal to *R[t]* discounted proportionally to the distance
    between the first time step and the final state. Again, if *λ* = 0, we obtain
    TD(0), but we are now also authorized to consider *λ* = *1* (because the sum is
    always extended to a finite number of elements). When λ = 1, we obtain *R[t]^((λ))*
    = *R[t]*, which means that we need to wait until the end of the episode to get
    the actual discounted reward. As explained previously, this method is normally
    not a first-choice solution, because when the episodes are very long, the agent
    selects the actions with a value function that is not up to date in the majority
    of cases. Therefore, *TD(λ)* is normally employed with *λ* values less than 1,
    in order to obtain the advantage of an online update, together with a correction
    based on the new states. To achieve this goal without looking at the future (we
    want to update *V(s)* as soon as new pieces of information are available), we
    need to introduce the concept of *eligibility trace**e(s)* (sometimes, in the
    context of computational neuroscience, *e(s)* is also called *stimulus trace*).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 之前表达式的第一项涉及所有非终止状态，而第二项等于 *R[t]*，按第一时间步和最终状态之间的距离成比例折扣。再次强调，如果 *λ* = 0，我们得到
    TD(0)，但现在我们也被授权考虑 *λ* = *1*（因为总和总是扩展到有限数量的元素）。当 λ = 1 时，我们得到 *R[t]^((λ))* = *R[t]*，这意味着我们需要等待直到剧集的结束才能获得实际的折扣奖励。如前所述，这种方法通常不是首选解决方案，因为当剧集非常长时，智能体选择的动作在大多数情况下都是基于过时的价值函数。因此，通常使用
    *λ* 值小于1的 *TD(λ)*，以获得在线更新的优势，同时基于新状态的纠正。为了在不看未来的情况下实现这一目标（我们希望在新的信息可用时立即更新 *V(s)*），我们需要引入
    *资格迹**e(s)* 的概念（有时，在计算神经科学背景下，*e(s)* 也被称为 *刺激迹*）。
- en: An eligibility trace for a state *s* is a function of time that returns the
    weight (greater than 0) of the specific state. Let's imagine a sequence, *s[1],
    s[2], ..., s[n]*, and consider a state, *s[i]*. After a backup *V(s[i])* is updated,
    the agent continues its exploration. When is a new update of *s[i]* (given longer
    backups) important? If *s[i]* is not visited anymore, the effect of longer backups
    must be smaller and smaller, and *s[i]* is said to not be eligible for changes
    in *V(s)*. This is a consequence of the previous assumption that shorter backups
    must generally have higher importance. So, if *s[i]* is an initial state (or is
    immediately after the initial state) and the agent moves to other states, the
    effect of *s[i]* must decay. Conversely, if *s[i]* is revisited, it means that
    the previous estimation of *V(s[i])* is probably wrong, and hence *s[i]* is eligible
    for a change. (To better understand this concept, imagine a sequence, *s[1], s[2],
    s[1], ...*. It's clear that when the agent is in *s[1]*, as well as in *s[2]*,
    it cannot select the right action; therefore, it's necessary to reevaluate *V(s)*
    until the agent is able to move forward.)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 状态 *s* 的资格迹是一个随时间变化的函数，它返回特定状态的权重（大于0）。让我们想象一个序列，*s[1], s[2], ..., s[n]*，并考虑一个状态，*s[i]*。在备份
    *V(s[i])* 更新后，智能体继续其探索。在什么情况下，*s[i]* 的新更新（给定更长的备份）是重要的？如果 *s[i]* 不再被访问，更长的备份的影响必须越来越小，并且
    *s[i]* 被说成是不适合在 *V(s)* 中进行更改。这是之前假设的后果，即较短的备份通常具有更高的重要性。因此，如果 *s[i]* 是初始状态（或紧接在初始状态之后）并且智能体移动到其他状态，*s[i]*
    的影响必须衰减。相反，如果 *s[i]* 被重新访问，这意味着之前对 *V(s[i])* 的估计可能是错误的，因此 *s[i]* 是适合进行更改的。（为了更好地理解这个概念，想象一个序列，*s[1],
    s[2], s[1], ...*。很明显，当智能体处于 *s[1]* 以及 *s[2]* 时，它不能选择正确的动作；因此，有必要重新评估 *V(s)*，直到智能体能够继续前进。）
- en: 'The most common strategy (which is also discussed in *Reinforcement Learning*, *Sutton
    R. S.*,*‎ Barto A. G.*, *The MIT Press*) is to define the eligibility traces in
    a recursive fashion. After each time step, *e[t](s)* decays by a factor equal
    to *γλ* (to meet the requirement imposed by the forward view); but, when the state
    *s* is revisited, *e[t](s)* is also increased by 1 (*e[t](s)* =* γλe[t-1](s)*
    + *1*). In this way, we impose a jump in the trend of *e(s)* whenever we desire
    to emphasize its impact. However, as *e(s)* decays independently of the jumps,
    the states that are visited and revisited later have a lower impact than the ones
    that are revisited very soon. The reason for this choice is very intuitive: the
    importance of a state revisited after a long sequence is clearly lower than the
    importance of a state that is revisited after a few steps. In fact, the estimation
    of *R[t]* is obviously wrong if the agent moves back and forth between two states
    at the beginning of the episode, but the error becomes less significant when the
    agent revisits a state after having explored other areas. For example, a policy
    can allow an initial phase in order to reach a partial goal, and then it can force
    the agent to move back to reach a terminal state.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的策略（在*Reinforcement Learning*，*Sutton R. S.*，*Barto A. G.*，*The MIT Press*中也有讨论）是以递归的方式定义资格迹。在每个时间步之后，*e[t](s)*会以一个等于*γλ*的因子衰减（以满足前向视角的要求）；但是，当状态*s*被重新访问时，*e[t](s)*也会增加1（*e[t](s)*
    =* γλe[t-1](s)* + *1*）。这样，我们可以在需要强调其影响时，对*e(s)*的趋势进行跳跃。然而，由于*e(s)*的衰减与跳跃独立，后来访问和重新访问的状态的影响比很快重新访问的状态要低。这种选择的原因非常直观：在长时间序列之后重新访问的状态的重要性显然低于在几步之后重新访问的状态的重要性。实际上，如果代理在场景开始时在两个状态之间来回移动，那么*R[t]*的估计显然是错误的，但当代理在探索其他区域之后重新访问状态时，错误变得不那么显著。例如，一个策略可以允许一个初始阶段以实现部分目标，然后可以强制代理返回以到达终端状态。
- en: 'Exploiting the eligibility traces, *TD(λ)* can achieve a very fast convergence
    in more complex environments, with a trade-off between a one-step TD method and
    a Monte Carlo one (which is normally avoided). At this point, the reader might
    wonder if we are sure about the convergence, and luckily, the answer is positive.
    Dayan proved (in* The convergence of TD (λ) for General λ*, *Dayan P.*, *Machine
    Learning 8*, *3–4/1992*) that *TD(λ)* converges for a generic *λ* with only a
    few specific assumptions and the fundamental condition that the policy is GLIE.
    The proof is very technical, and it''s beyond the scope of this book; however,
    the most important assumptions (which are generally met) are:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 利用资格迹，*TD(λ)*可以在更复杂的环境中实现非常快的收敛，在一步TD方法和蒙特卡洛方法之间进行权衡（通常避免使用蒙特卡洛方法）。在此阶段，读者可能会想知道我们是否确信收敛性，幸运的是，答案是肯定的。Dayan在*The
    convergence of TD (λ) for General λ*，*Dayan P.*，*Machine Learning 8*，*3–4/1992*中证明了对于通用的*λ*，只要满足一些特定的假设和基本条件，即策略是GLIE，*TD(λ)*就会收敛。证明非常技术性，超出了本书的范围；然而，最重要的假设（通常是满足的）是：
- en: The **Markov Decision Process** (**MDP**) has absorbing states (in other words,
    all of the episodes end in a finite number of steps).
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**马尔可夫决策过程**（**MDP**）有吸收状态（换句话说，所有场景都在有限步骤内结束）。'
- en: All of the transition probabilities are not-null (all states can be visited
    an infinite number of times).
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的转移概率均非空（所有状态可以无限次访问）。
- en: The first condition is obvious, the absence of absorbing states yields infinite
    explorations, which are not compatible with a TD method (sometimes it's possible
    to prematurely end an episode, but this can either be unacceptable (in some contexts)
    or a sub-optimal choice (in many others)). Moreover, Sutton and Barto (in the
    aforementioned book) proved that TD(λ) is equivalent to employing the weighted
    average of discounted return approximations, but without the constraint of looking
    ahead in the future (which is clearly impossible).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个条件很明显，没有吸收状态会导致无限探索，这与TD方法不相容（有时可以提前结束场景，但这可能是不被接受的（在某些情况下）或次优选择（在许多情况下））。此外，Sutton和Barto（在上述书中）证明了TD(λ)等价于使用加权平均的折现回报近似，但没有向前看未来（这显然是不可能的）。
- en: 'The complete TD(λ) algorithm (with an optional forced termination of the episode)
    is:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的TD(λ)算法（带有可选的强制终止场景）如下：
- en: Set an initial deterministic random policy, *π(s)*
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个初始的确定性随机策略，*π(s)*
- en: Set the initial value array, *V(s) = 0 ∀ s ∈ S*
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置初始值数组，*V(s) = 0 ∀ s ∈ S*
- en: Set the initial eligibility trace array, *e(s) = 0 **∀ s ∈ S*
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置初始资格迹数组，*e(s) = 0 **∀ s ∈ S**
- en: Set the number of episodes, *N*[*episodes*]
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置剧集数，*N*[*episodes*]
- en: Set a maximum number of steps per episode, *N*[*max*]
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置每剧集的最大步数，*N*[*max*]
- en: Set a constant, α (α = *0.1*)
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个常数，α (α = *0.1*)
- en: Set a constant, γ (γ = *0.9*)
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个常数，γ (γ = *0.9*)
- en: Set a constant, λ (λ = *0.5*)
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个常数，λ (λ = *0.5*)
- en: Set a counter, *e* = *0*
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个计数器，*e* = *0*
- en: 'For *i* = *1* to *N[episodes]*:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '对于 *i* = *1* 到 *N[episodes]*:'
- en: Create an empty state list, *L*
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空的状态列表，*L*
- en: Observe the initial state, *s[i]*, and append *s[i]* to *L*
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察初始状态，*s[i]*，并将 *s[i]* 添加到 *L*
- en: While *s[j]* is non-terminal and *e < N[max:]*
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *s[j]* 是非终结符且 *e < N[max:]*
- en: '*e += 1*'
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*e += 1*'
- en: Select the action, *a[t] = **π(s[i])*
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '选择动作，*a[t] = **π(s[i])** '
- en: Observe the transition, *(a[t], s[i]) → (s[j], r[ij])*
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察转换，*(a[t], s[i]) → (s[j], r[ij])*
- en: Compute the TD error as *TD[error] = r[ij] + γV(s[j]) - V(s[i])*
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算TD错误作为 *TD[error] = r[ij] + γV(s[j]) - V(s[i])*
- en: Increment the eligibility trace, *e(s[i]) += 1.0*
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 增加资格迹，*e(s[i]) += 1.0*
- en: 'For *s* in *L*:'
  id: totrans-54
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *s* 在 *L* 中：
- en: Update the value, *V(s) += α · TD[error] · e(s)*
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新值，*V(s) += α · TD[error] · e(s)*
- en: Update the eligibility trace, *e(s) *= γλ*
  id: totrans-56
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新资格迹，*e(s) *= γλ*
- en: Set *s[i]* = *s*[*j*]
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *s[i]* = *s*[*j*]
- en: Append *s[j]* to *L*
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 *s[j]* 添加到 *L*
- en: Update the policy to be greedy with respect to the value function, *π(s) = argmax[a] Q(s,
    a)*
  id: totrans-59
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新策略，使其对值函数贪婪，*π(s) = argmax[a] Q(s, a)*
- en: The reader can better understand the logic of this algorithm by considering
    the TD error and its back-propagation. Even if this is only a comparison, it's
    possible to imagine the behavior of TD(λ) as similar to the **Stochastic Gradient
    Descent** (**SGD**) algorithms employed to train a neural network. In fact, the
    error is propagated to the previous states (analogous to the lower layers of an
    MLP) and affects them proportionally to their importance, which is defined by
    their eligibility traces. Hence, a state with a higher eligibility trace can be
    considered more responsible for the error; therefore, the corresponding value
    must be corrected proportionally. This isn't a formal explanation, but it can
    simplify comprehension of the dynamics without an excessive loss of rigor.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可以通过考虑 TD 错误及其反向传播来更好地理解此算法的逻辑。即使这只是一个比较，也可以想象 TD(λ) 的行为类似于用于训练神经网络的 **随机梯度下降**
    (**SGD**) 算法。事实上，错误被传播到前面的状态（类似于 MLP 的底层），并按其重要性成比例地影响它们，这种重要性由它们的资格迹定义。因此，资格迹较高的状态可以被认为是错误的责任更大；因此，相应的值必须按比例进行纠正。这不是一个正式的解释，但它可以简化对动态的理解，而不会过度损失严谨性。
- en: TD(λ) in a more complex Checkerboard environment
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在更复杂的棋盘环境中的 TD(λ)
- en: 'At this point, we want to test the TD(λ) algorithm with a slightly more complex
    tunnel environment. In fact, together with the absorbing states, we will also
    consider some intermediate positive states, which can be imagined as *checkpoints*.
    An agent should learn the optimal path from any cell to the final state, trying
    to pass through the highest number of checkpoints possible. Let''s start by defining
    the new structure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们想要用稍微复杂一点的地道环境测试 TD(λ) 算法。实际上，连同吸收状态一起，我们还将考虑一些中间的正状态，这些状态可以想象为 *检查点*。智能体应该学习从任何单元格到最终状态的最佳路径，试图通过尽可能多的检查点。让我们先定义新的结构：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The reward structure is shown in the following diagram:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励结构如下所示：
- en: '![](img/45b37fed-90f7-4956-8996-7738cbbfb0a5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/45b37fed-90f7-4956-8996-7738cbbfb0a5.png)'
- en: Reward schema in the new tunnel environment
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 新地道环境中的奖励方案
- en: 'At this point, we can proceed to initialize all of the constants (in particular,
    we have chosen *λ = 0.6*, which is an intermediate solution that guarantees an
    awareness close to a Monte Carlo method, without compromising the learning speed):'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以继续初始化所有常数（特别是，我们选择了 *λ = 0.6*，这是一个中间解决方案，保证了接近蒙特卡洛方法的意识，同时不损害学习速度）：
- en: '[PRE1]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As in Python, the keyword `lambda` is reserved; we used the truncated expression
    `lambd` to declare the constant.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在 Python 中，关键字 `lambda` 是保留的；我们使用了截断的表达式 `lambd` 来声明常数。
- en: 'As we want to start from a random cell, we need to repeat the same procedure
    presented in the previous chapter; but, in this case, we are also including the
    checkpoint states:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要从一个随机单元格开始，我们需要重复上一章中介绍的相同程序；但在这个情况下，我们还包括检查点状态：
- en: '[PRE2]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now define the `episode()` function, which implements a complete TD(λ)
    cycle. As we don''t want the agent to roam around trying to pass through the checkpoints
    an infinite number of times, we have decided to reduce the reward during the exploration,
    to incentivize the agent to pass through only the necessary checkpoints—trying,
    at the same time, to reach the final state as soon as possible:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以定义`episode()`函数，它实现了完整的TD(λ)周期。由于我们不希望智能体无限次地四处游荡尝试通过检查点，我们决定在探索期间减少奖励，以激励智能体仅通过必要的检查点——同时尽可能快地达到最终状态：
- en: '[PRE3]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `is_final()` and `policy_selection()` functions are the same ones defined
    in the previous chapter, and need no explanation. Even if it''s not really necessary,
    we have decided to implement a forced termination after a number of steps, equal
    to `max_steps`. This is helpful at the beginning because as the policy is not *ε*-greedy,
    the agent can remain stuck in a looping exploration that never ends. We can now
    train the model for a fixed number of episodes (alternatively, it''s possible
    to stop the process when the value array doesn''t change anymore):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`is_final()`和`policy_selection()`函数与上一章中定义的相同，无需解释。即使实际上并不必要，我们决定在达到`max_steps`步数后强制终止。这有助于开始时，因为策略不是*ε*-贪婪的，智能体可能会陷入无限循环探索中。现在我们可以为固定数量的轮次训练模型（或者，当值数组不再变化时，可以停止过程）：'
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `episode()` function returns the total rewards; therefore, it''s useful
    to check how the agent learning process evolved:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`episode()`函数返回总奖励；因此，检查智能体学习过程是如何发展的很有用：'
- en: '![](img/08409634-f125-445e-baef-8f952ac52c77.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08409634-f125-445e-baef-8f952ac52c77.png)'
- en: Total rewards achieved by the agent
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体获得的总奖励
- en: 'At the beginning (for about 500 episodes), the agent employs an unacceptable
    policy that yields very negative total rewards. However, in about 1,000 iterations,
    the algorithm reaches an optimal policy that is only slightly improved by the
    following episodes. The oscillations are due to the different starting points;
    however, the total rewards are never negative, and as the checkpoint weights decay,
    this is a positive signal, indicating that the agent reaches the final positive
    state. To have a confirmation of this hypothesis, we can plot the learned value
    function:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时（大约500轮），智能体采用不可接受的策略，导致非常负面的总奖励。然而，在大约1,000次迭代后，算法达到了一个最优策略，后续的轮次仅略有改进。振荡是由于不同的起始点；然而，总奖励从未为负，并且随着检查点权重的衰减，这是一个积极的信号，表明智能体达到了最终的正状态。为了证实这一假设，我们可以绘制学习到的值函数：
- en: '![](img/939757a7-49a4-4c84-8b7e-1d8b10fdeaf7.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/939757a7-49a4-4c84-8b7e-1d8b10fdeaf7.png)'
- en: Final value matrix
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最终值矩阵
- en: 'The values are coherent with our initial analysis; in fact, they tend to be
    higher when the cell is close to a checkpoint, but, at the same time, the global
    configuration (considering a policy greedy with respect to *V(s)*) forces the
    agent to reach the ending state whose surrounding values are the highest. The
    last step is checking the actual policy, with a particular focus on the checkpoints:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值与我们的初始分析一致；事实上，当单元格接近检查点时，它们往往更高，但与此同时，全局配置（考虑到对*V(s)*贪婪的策略）迫使智能体达到周围值最高的结束状态。最后一步是检查实际策略，特别关注检查点：
- en: '![](img/6f326296-76bc-4afa-9773-9b73d6ac581e.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6f326296-76bc-4afa-9773-9b73d6ac581e.png)'
- en: Final policy
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 最终策略
- en: As it's possible to observe, the agent tries to pass through the checkpoints,
    but when it's close to the final state, it (correctly) prefers to end the episode
    as soon as possible. I invite the reader to repeat the experiment using different
    values for the constant *λ*, and changing the environment dynamics for the checkpoints.
    What happens if their values remain the same? Is it possible to improve the policy
    with a higher *λ*?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，智能体试图通过检查点，但当它接近最终状态时，它（正确地）更倾向于尽快结束这一轮。我邀请读者使用不同的常数*λ*值重复实验，并改变检查点的环境动态。如果它们的值保持不变会发生什么？是否可以通过更高的*λ*来改善策略？
- en: It's important to remember that, as we are extensively using random values,
    successive experiments can yield different results due to different initial conditions.
    However, the algorithm should always converge to an optimal policy when the number
    of episodes is high enough.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，由于我们广泛使用随机值，连续的实验可能会由于不同的初始条件而产生不同的结果。然而，当轮次足够多时，算法应该始终收敛到最优策略。
- en: Actor-Critic TD(0) in the checkerboard environment
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 棋盘环境中的 Actor-Critic TD(0)
- en: In this example, we want to employ an alternative algorithm called *Actor-Critic*,
    together with TD(0). In this method, the agent is split into two components, a
    Critic, which is responsible for evaluating the quality of the value estimation,
    and an actor, which selects and performs an action. As pointed out by Dayan (in
    *Theoretical Neuroscience*, *Dayan P*., *Abbott L*. *F*., *The MIT Press*), the
    dynamics of an Actor-Critic approach are similar to the interleaving policy evaluation
    and policy improvement steps. In fact, the knowledge of the Critic is obtained
    through an iterative process, and its initial evaluations are normally sub-optimal.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们想要使用一个名为 *Actor-Critic* 的替代算法，结合 TD(0)。在这个方法中，智能体被分为两个部分，一个 Critic，它负责评估价值估计的质量，以及一个
    actor，它选择并执行动作。正如 Dayan 在 *Theoretical Neuroscience* 中所指出的（*Dayan P*., *Abbott
    L*. *F*., *The MIT Press*），Actor-Critic 方法中的动态类似于策略评估和策略改进步骤的交织。实际上，Critic 的知识是通过一个迭代过程获得的，其初始评估通常是次优的。
- en: 'The structural schema is shown in the following diagram:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 结构架构如下图所示：
- en: '![](img/0ddd636c-72f0-4b0d-a647-c56bc60322dc.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0ddd636c-72f0-4b0d-a647-c56bc60322dc.png)'
- en: Actor-Critic schema
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Actor-Critic 架构
- en: 'In this particular case, it''s preferable to employ a *ε*-greedy soft policy,
    based on the softmax function. The model stores a matrix (or an approximating
    function) called *policy importance*, where each entry *p[i](s, a)* is a value
    representing the preference for a specific action in a certain state. The actual
    stochastic policy is obtained by applying the softmax with a simple trick to increase
    the numerical stability when the exponentials become very large:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定情况下，最好采用基于 softmax 函数的 *ε*-greedy 软策略。模型存储一个矩阵（或一个近似函数），称为 *策略重要性*，其中每个条目
    *p[i](s, a)* 代表在特定状态下对特定动作的偏好值。实际的随机策略是通过应用 softmax 并使用一个简单的技巧来增加当指数变得非常大时的数值稳定性来获得的：
- en: '![](img/a2711708-7293-46b6-a984-5519dc987c4e.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a2711708-7293-46b6-a984-5519dc987c4e.png)'
- en: 'After performing the action *a* in the state *s[i]* and observing the transition
    to the state *s[j]* with a reward *r[ij]*, the Critic evaluates the TD error:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态 *s[i]* 中执行动作 *a* 并观察到过渡到状态 *s[j]* 以及奖励 *r[ij]* 后，Critic 评估 TD 错误：
- en: '![](img/412ea371-91ba-4bb1-9eff-9ed4517c265b.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/412ea371-91ba-4bb1-9eff-9ed4517c265b.png)'
- en: 'If *V(s[i]) < r[ij] + γV(s[j])*, the transition is considered positive, because
    the value is increasing. Conversely, when *V(s[i]) > r[ij] + γV(s[j]**)*, the
    Critic evaluates the action as negative, because the previous value was higher
    than the new estimation. A more general approach is based on the concept of *advantage*,
    which is defined as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 *V(s[i]) < r[ij] + γV(s[j]*)，则认为转换是积极的，因为值在增加。相反，当 *V(s[i]) > r[ij] + γV(s[j]*)*
    时，Critic 将动作评估为负，因为之前的值高于新的估计。一种更通用的方法是基于 *优势* 的概念，它被定义为：
- en: '![](img/484d36a2-70f6-4ee6-931d-624bf0f71188.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/484d36a2-70f6-4ee6-931d-624bf0f71188.png)'
- en: Normally, one of the terms from the previous expression can be approximated.
    In our case, we cannot compute the *Q* function directly; hence, we approximate
    it with the term *r[ij] + γV(s[j])*. It's clear that the role of the advantage
    is analogous to the one of the TD error (which is an approximation) and must represent
    the confirmation that an action in a certain state is a good or bad choice. An
    analysis of all **advantage Actor-Critic** (**A3C**) algorithms (in other words,
    improvements of the standard *policy gradient* algorithm) is beyond the scope
    of this book. However, the reader can find some helpful pieces of information
    in *High-Dimensional Continuous Control Using Generalized Advantage Estimation*, *Schulman
    J*., *Moritz P*., *Levine S*., *Jordan M*. *I*., *Abbeel P*., *ICLR 2016.*
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，前一个表达式中的一个项可以被近似。在我们的情况下，我们无法直接计算 *Q* 函数；因此，我们用项 *r[ij] + γV(s[j]*)* 来近似它。很明显，优势的作用类似于
    TD 错误（这是一个近似）的作用，并且必须代表在某个状态下采取的动作是一个好选择还是坏选择。对所有 **优势 Actor-Critic** (*A3C*)
    算法（换句话说，标准 *策略梯度* 算法的改进）的分析超出了本书的范围。然而，读者可以在 *High-Dimensional Continuous Control
    Using Generalized Advantage Estimation* 中找到一些有用的信息，*Schulman J*., *Moritz P*.,
    *Levine S*., *Jordan M*. *I*., *Abbeel P*., *ICLR 2016*。
- en: Of course, an Actor-Critic correction is not sufficient. To improve the policy,
    it's necessary to employ a standard algorithm (such as TD(0), TD(λ), or least
    square regression, which can be implemented using a neural network) in order to
    learn the correct value function, *V(s)*. As for many other algorithms, this process
    can converge only after a sufficiently high number of iterations, which must be
    exploited to visit the states many times, experimenting with all possible actions.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Actor-Critic校正是不够的。为了改进策略，有必要使用标准算法（如TD(0)、TD(λ)或最小二乘回归，这可以通过神经网络实现）来学习正确的值函数
    *V(s)*。对于许多其他算法，这个过程只有在足够多的迭代次数之后才能收敛，这必须被利用来多次访问状态，尝试所有可能的行为。
- en: 'Hence, with a TD(0) approach, the first step after evaluating the TD error
    is updating *V(s)* using the rule defined in the previous chapter:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，使用TD(0)方法，在评估TD误差后的第一步是使用前一章中定义的规则更新 *V(s)*：
- en: '![](img/c695ed0f-e214-4e0d-8e35-f3e2b1727564.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c695ed0f-e214-4e0d-8e35-f3e2b1727564.png)'
- en: 'The second step is more pragmatic; in fact, the main role of the Critic is
    actually to criticize every action, deciding when it''s better to increase or
    decrease the probability of selecting it again in a certain state. This goal can
    be achieved by simply updating the policy importance:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步更加实用；实际上，Critic的主要作用实际上是对每个动作进行批评，决定在某种状态下是增加还是减少再次选择该动作的概率。这个目标可以通过简单地更新策略重要性来实现：
- en: '![](img/63fdf0cc-74d2-4e4e-b770-ccf7aa03529a.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/63fdf0cc-74d2-4e4e-b770-ccf7aa03529a.png)'
- en: The role of the learning rate *ρ* is extremely important; in fact, incorrect
    values (in other words, values that are too high) can yield initial wrong corrections
    that may compromise the convergence. It's essential to not forget that the value
    function is almost completely unknown at the beginning, and therefore the Critic
    has no chance to increase the right probability with awareness. For this reason,
    I always suggest to start with very small value (*ρ = 0.001*) and increase it
    only if the convergence speed of the algorithm is effectively improved.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 *ρ* 的作用极其重要；事实上，不正确的值（换句话说，过高的值）可能导致初始错误的校正，从而损害收敛。必须记住，值函数在开始时几乎完全未知，因此Critic没有机会通过意识来增加正确的概率。因此，我总是建议从非常小的值（*ρ
    = 0.001*）开始，并且只有在算法的收敛速度确实得到有效提高时才增加它。
- en: 'As the policy is based on the softmax function, after a Critic update, the
    values will always be renormalized, resulting in an actual probability distribution.
    After an adequately large number of iterations, with the right choice of both *ρ*
    and *γ*, the model is able to learn both a stochastic policy and a value function.
    Therefore, it''s possible to employ the trained agent by always selecting the
    action with the highest probability (which corresponds to an implicitly greedy
    behavior):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 由于策略基于softmax函数，在Critic更新后，值将始终被重新归一化，从而形成一个实际的概率分布。经过足够多的迭代次数，并且正确选择 *ρ* 和
    *γ*，模型能够学习到随机策略和值函数。因此，可以通过始终选择概率最高的动作（这对应于隐式贪婪行为）来使用训练好的智能体：
- en: '![](img/7046f44f-1702-4122-b3b1-3154667a5506.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7046f44f-1702-4122-b3b1-3154667a5506.png)'
- en: 'Let''s now apply this algorithm to the tunnel environment. The first step is
    defining the constants (as we are looking for a long sighted agent, we are setting
    the discount factor *γ = 0.99*):'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这个算法应用到隧道环境中。第一步是定义常数（因为我们正在寻找一个远视的智能体，所以我们设置折现因子 *γ = 0.99*）：
- en: '[PRE5]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'At this point, we need to define the policy importance array, and a function
    to generate the softmax policy:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要定义策略重要性数组和一个生成softmax策略的函数：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The functions needed to implement a single training step are very straightforward,
    and the reader should already be familiar with their structure:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 实现单个训练步骤所需的函数非常简单，读者应该已经熟悉它们的结构：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At this point, we can train the model with 50,000 iterations, and 30,000 explorative
    ones (with a linear decay of the exploration factor):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以用50,000次迭代和30,000次探索性迭代（探索因子线性衰减）来训练模型：
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The resulting greedy policy is shown in the following figure:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的贪婪策略如图所示：
- en: '![](img/7c061246-8480-4b13-ba41-f488adbcd9e2.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c061246-8480-4b13-ba41-f488adbcd9e2.png)'
- en: Final greedy policy
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最终贪婪策略
- en: The final greedy policy is consistent with the objective, and the agent always
    reaches the final positive state by avoiding the wells. This kind of algorithm
    can appear more complex than necessary; however, in complex situations, it turns
    out to be extremely effective. In fact, the learning process can be dramatically
    improved, thanks to the fast corrections performed by the Critic. Moreover, the
    author has noticed that the Actor-Critic is more robust to wrong (or noisy) evaluations.
    As the policy is learned separately, the effect of small variations in *V(s)*
    cannot easily change the probabilities *π(s, a)* (in particular, when an action
    is generally much *stronger* than the others). On the other hand, as discussed
    previously, it's necessary to avoid a premature convergence in order to let the
    algorithm modify the importance/probabilities, without an excessive number of
    iterations. The right trade-off can be found only after a complete analysis of
    each specific scenario, and unfortunately, there are no general rules that work
    in every case. My suggestion is to test various configurations, starting with
    small values (and, for example, a discount factor of *γ ∈ [0.7, 0.9]*), evaluating
    the total reward achieved after the same exploration period.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的贪婪策略与目标一致，智能体通过避免陷阱始终达到最终的正状态。这种算法可能比必要的更复杂；然而，在复杂情况下，它证明非常有效。事实上，学习过程可以通过Critic执行的快速校正而显著改进。此外，作者注意到Actor-Critic对错误的（或噪声的）评估更稳健。由于策略是分别学习的，*V(s)*的小幅变化不会轻易改变概率*π(s,
    a)*（特别是当动作通常比其他动作强得多时）。另一方面，如前所述，为了避免过早收敛，有必要避免算法修改重要性/概率，而不需要过多的迭代。只有在分析每个具体场景之后，才能找到正确的权衡，不幸的是，没有适用于所有情况的通用规则。我的建议是测试各种配置，从小的值开始（例如，折扣因子*γ
    ∈ [0.7, 0.9]*），评估在相同的探索期后获得的累计奖励。
- en: 'Complex deep learning models (such as asynchronous A3C; see *Asynchronous Methods
    for Deep Reinforcement Learning*, *Mnih V*., *Puigdomènech Badia A*., *Mirza M*., *Graves
    A*., *Lillicrap T*. *P*., *Harley T*., *Silver D*., *Kavukcuoglu K*., *arXiv:1602.01783
    [cs.LG]* for further information) are based on a single network that outputs both
    the softmax policy (whose actions are generally proportional to their probability)
    and the value. Instead of employing an explicitly *ε*-greedy soft policy, it''s
    possible to add a *maximum-entropy constraint* to the global cost function:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂的深度学习模型（如异步A3C；参见*异步深度强化学习方法*，Mnih V.，Puigdomènech Badia A.，Mirza M.，Graves
    A.，Lillicrap T. P.，Harley T.，Silver D.，Kavukcuoglu K.，arXiv:1602.01783 [cs.LG]*以获取更多信息）基于一个网络，该网络输出softmax策略（其动作通常与概率成正比）和值。而不是使用显式的*ε*-greedy软策略，可以在全局成本函数中添加一个*最大熵约束*：
- en: '![](img/21653b7e-8e87-40d4-9800-65b36bdbe1ba.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/21653b7e-8e87-40d4-9800-65b36bdbe1ba.png)'
- en: As the entropy is at the maximum when all of the actions have the same probability,
    this constraint (with an appropriate weight) forces the algorithm to increase
    the exploration probability until an action becomes dominant and there's no more
    need to avoid a greedy selection. This is a sound and easy way to employ an *adaptive ε-greedy
    policy*, because as the model works with each state separately, the states where
    the uncertainty is very low can become greedy; it's possible to automatically
    keep a high entropy whenever it's necessary to continue the exploration, in order
    to maximize the reward.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有动作具有相同概率时，熵达到最大值，这个约束（带有适当的权重）迫使算法增加探索概率，直到某个动作变得主导，不再需要避免贪婪选择。这是一种合理且简单的方法来应用*自适应ε-greedy策略*，因为模型与每个状态分别工作，不确定性非常低的状态可以变得贪婪；在需要继续探索时，可以自动保持高熵，以最大化奖励。
- en: The effect of double correction, together with a maximum-entropy constraint,
    improves the convergence speed of the model, encourages the exploration during
    the initial iterations, and yields very high final accuracy. I invite the reader
    to implement this variant with other scenarios and algorithms. In particular,
    at the end of this chapter, we are going to experiment with an algorithm based
    on a neural network. As the example is pretty simple, I suggest using Tensorflow
    to create a small network based on the Actor-Critic approach. The reader can employ
    a *mean squared error* loss for the value and softmax cross entropy for the policy.
    Once the models work successfully with our toy examples, it will be possible to
    start working with more complex scenarios (like the ones proposed in OpenAI Gym
    at [https://gym.openai.com/](https://gym.openai.com/)).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 双重校正的效果，加上最大熵约束，提高了模型的收敛速度，鼓励在初始迭代中进行探索，并产生非常高的最终精度。我邀请读者在其他场景和算法中实现这个变体。特别是，在本章的结尾，我们将尝试一个基于神经网络的算法。由于示例相当简单，我建议使用Tensorflow根据Actor-Critic方法创建一个小型网络。读者可以为价值使用*均方误差*损失，为策略使用softmax交叉熵。一旦模型成功应用于我们的玩具示例，就可以开始处理更复杂的情况（如OpenAI
    Gym中提出的[https://gym.openai.com/](https://gym.openai.com/)）。
- en: SARSA algorithm
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA算法
- en: '**SARSA** (whose name is derived from the sequence *state-action-reward-state-action*)
    is a natural extension of TD(0) to the estimation of the *Q* function. Its standard
    formulation (which is sometimes called one-step SARSA, or SARSA(0), for the same
    reasons explained in the previous chapter) is based on a single next reward, *r[t+1]*,
    which is obtained by executing the action *a[t]* in the state *s[t]*. The temporal
    difference computation is based on the following update rule:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**SARSA**（其名称来源于序列*状态-动作-奖励-状态-动作*）是TD(0)的自然扩展，用于估计*Q*函数。其标准公式（有时称为一步SARSA或SARSA(0)，原因与上一章中解释的相同）基于单个下一个奖励*r[t+1]*，该奖励是通过在状态*s[t]*中执行动作*a[t]*获得的。时间差计算基于以下更新规则：'
- en: '![](img/539d7ddc-fad5-4e37-bd39-5d52684fa197.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/539d7ddc-fad5-4e37-bd39-5d52684fa197.png)'
- en: 'The equation is equivalent to TD(0), and if the policy is chosen to be GLIE,
    it has been proven (in *Convergence Results for Single-Step On-Policy Reinforcement-Learning
    Algorithms*, *Singh S.*,* Jaakkola T.*,* Littman M. L.*,* Szepesvári C.*,* Machine
    Learning*, *39/2000*) that SARSA converges to an optimal policy, *π^(opt)(s)*,
    with the probability 1, when all couples (state, action) are experienced an infinite
    number of times. This means that if the policy is updated to be greedy with respect
    to the current value function induced by *Q*, it holds that:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 该方程等价于TD(0)，如果策略选择为GLIE，根据*Convergence Results for Single-Step On-Policy Reinforcement-Learning
    Algorithms*（*Singh S.*，*Jaakkola T.*，*Littman M. L.*，*Szepesvári C.*，*Machine
    Learning*，*39/2000*）中的证明，SARSA在所有（状态，动作）对都被无限次体验的情况下，以概率1收敛到最优策略*π^(opt)(s)*。这意味着如果策略更新为相对于由*Q*引起的当前价值函数的贪婪策略，则成立：
- en: '![](img/2ba03afc-b4f4-4b09-8055-2e3910d0588a.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2ba03afc-b4f4-4b09-8055-2e3910d0588a.png)'
- en: 'The same result is valid for the *Q* function. In particular, the most important
    conditions required by the proof are:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*Q*函数，同样的结果也是有效的。特别是，证明中所需的最重要条件是：
- en: The learning rate, *α ∈ [0, 1]*, with the constraints *Σ**α = ∞* and *Σα² <
    ∞*
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率，*α ∈ [0, 1]*，满足约束*Σα = ∞*和*Σα² < ∞*
- en: The variance of the rewards must be finite
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励的方差必须是有限的
- en: The first condition is particularly important when *α* is a function of the
    state and the time step; however, in many cases, it is a constant bounded between
    0 and 1, and hence, *Σα² = ∞*. A common way to solve this problem (above all when
    a large number of iterations are required) is to let the learning rate decay (in
    other words, exponentially) during the training process. Instead, to mitigate
    the effect of very large rewards, it's possible to clip them in a suitable range
    (*[-1, 1]*). In many cases, it's not necessary to employ these strategies, but
    in more complex scenarios, they can become crucial in order to ensure the convergence
    of the algorithm. Moreover, as pointed out in the previous chapter, these kinds
    of algorithms need a long exploration phase before starting to stabilize the policy.
    The most common strategy is to employ a *ε*-greedy policy, with a temporal decay
    of the exploration factor. During the first iterations, the agent must explore
    without caring about the returns of the actions. In this way, it's possible to
    assess the actual values before the beginning of a final refining phase characterized
    by a purely greedy exploration, based on a more precise approximation of *V(s)*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 当*α*是状态和时间步的函数时，第一个条件尤为重要；然而，在许多情况下，它是一个介于0和1之间的常数，因此，*Σα² = ∞*。解决这个问题的常见方法（尤其是当需要大量迭代时）是在训练过程中让学习率衰减（换句话说，指数衰减）。相反，为了减轻非常大的奖励的影响，可以将它们剪辑到合适的范围内（*[-1,
    1]*）。在许多情况下，不需要采用这些策略，但在更复杂的场景中，它们可能变得至关重要，以确保算法的收敛。此外，正如前一章所指出的，这类算法在开始稳定策略之前需要一个长的探索阶段。最常用的策略是采用*ε*-贪婪策略，探索因子的时序衰减。在最初的迭代中，智能体必须探索，而不关心动作的回报。这样，就可以在最终精炼阶段开始之前评估实际值，该阶段的特点是纯粹的贪婪探索，基于对*V(s)*的更精确近似。
- en: 'The complete SARSA(0) algorithm (with an optional forced termination of the
    episode) is:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的SARSA(0)算法（带有可选的强制终止剧集）是：
- en: Set an initial deterministic random policy, *π(s)*
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个初始的确定性随机策略，*π(s)*
- en: Set the initial value array, *Q(s, a) = 0 ∀ s ∈ S* and *∀ a ∈ A*
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置初始值数组，*Q(s, a) = 0 ∀ s ∈ S* 和 *∀ a ∈ A*
- en: Set the number of episodes, *N*[*episodes*]
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置剧集数量，*N[episodes*]
- en: Set a maximum number of steps per episode, *N*[*max*]
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置每剧集的最大步数，*N[max*]
- en: Set a constant, *α* (*α = 0.1*)
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置常数，*α* (*α = 0.1*)
- en: Set a constant, *γ* (*γ** = 0.9*)
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置常数，*γ* (*γ** = 0.9*)
- en: Set an initial exploration factor, *ε^((0))* (*ε^((0)) = 1.0*)
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置初始探索因子，*ε^((0)) = 1.0*
- en: Define a policy to let the exploration factor *ε* decay (linear or exponential)
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个策略，让探索因子*ε*衰减（线性或指数）
- en: Set a counter, *e = 0*
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置计数器，*e = 0*
- en: 'For *i = 1* to *N[episodes]*:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*i = 1*到*N[episodes]*：
- en: Observe the initial state, *s*[*i*]
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察初始状态，*s[i*]
- en: 'While *s[j]* is non-terminal and *e < N[max]*:'
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当*s[j]*非终止且*e < N[max]*时：
- en: '*e += 1*'
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*e += 1*'
- en: Select the action, *a[t] = **π(s[i])*, with an exploration factor *ε*^(*(e)*)
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作，*a[t] = **π(s[i**)，带有探索因子*ε*^(*e*)*
- en: Observe the transition, *(a[t], s[i]) → (s[j], r[ij])*
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察转换，*(a[t], s[i]) → (s[j], r[ij])*
- en: Select the action, *a[t+1] = π(s[j]**)*, with an exploration factor *ε*^(*(e)*)
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作，*a[t+1] = π(s[j]*)，带有探索因子*ε*^(*e*)*
- en: Update the *Q(s[t], a[t])* function (if *s[j]* is terminal, set *Q(s[t+1], a[t+1]**)
    = 0*)
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新*Q(s[t], a[t])*函数（如果*s[j]*是终止状态，则设置*Q(s[t+1], a[t+1]*) = 0*)
- en: Set *s[i] = s*[*j*]
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置*s[i] = s[j*]
- en: The concept of eligibility trace can also be extended to SARSA (and other TD
    methods); however, that is beyond the scope of this book. A reader who is interested
    can find all of the algorithms (together with their mathematical formulations)
    in *Sutton R. S.,‎ Barto A. G., Reinforcement Learning, A Bradford Book*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可选性跟踪的概念也可以扩展到SARSA（以及其他TD方法）；然而，这超出了本书的范围。对感兴趣的读者来说，可以在*Sutton R. S.，Barto
    A. G.，强化学习，布拉德福德图书*中找到所有算法（包括它们的数学公式）。
- en: SARSA in the checkerboard environment
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 棋盘环境中的SARSA
- en: 'We can now test the SARSA algorithm in the original tunnel environment (all
    of the elements that are not redefined are the same as the previous chapter).
    The first step is defining the *Q(s, a)* array and the constants employed in the
    training process:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在原始隧道环境中测试SARSA算法（所有未重新定义的元素与上一章相同）。第一步是定义*Q(s, a)*数组以及在训练过程中使用的常数：
- en: '[PRE9]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As we want to employ a *ε*-greedy policy, we can set the starting point to
    `(0, 0)`, forcing the agent to reach the positive final state. We can now define
    the functions needed to perform a training step:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想采用*ε*-贪婪策略，我们可以将起点设置为 `(0, 0)`，迫使代理达到积极最终状态。我们现在可以定义执行训练步骤所需的函数：
- en: '[PRE10]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `select_action()` function has been designed to select a random action
    with the probability *ε*, and a greedy one with respect to *Q(s, a)*, with the
    probability *1 - **ε*. The `sarsa_step()` function is straightforward, and executes
    a complete episode updating the *Q(s, a)* (that''s why this is an online algorithm).
    At this point, it''s possible to train the model for 20,000 episodes and employ
    a linear decay for *ε* during the first 15,000 episodes (when t > 15,000, *ε*
    is set equal to 0 in order to employ a purely greedy policy):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`select_action()` 函数被设计为以概率*ε*选择随机动作，以概率*1 - **ε*选择基于*Q(s, a)*的贪婪动作。`sarsa_step()`
    函数很简单，执行一个完整的剧集更新*Q(s, a)*（这就是为什么这是一个在线算法）。在这个阶段，我们可以对模型进行20,000个剧集的训练，并在前15,000个剧集期间使用*ε*的线性衰减（当t
    > 15,000时，*ε*设置为0，以便采用纯贪婪策略）：'
- en: '[PRE11]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As usual, let''s check the learned values (considering that the policy is greedy,
    we''re going to plot *V(s)* *= max[a] Q(s, a)*):'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如同往常，让我们检查学习到的值（考虑到策略是贪婪的，我们将绘制*V(s)* = max[a] Q(s, a)*）：
- en: '![](img/1f28d7a1-70b6-4a99-973d-2e12cf37714f.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1f28d7a1-70b6-4a99-973d-2e12cf37714f.png)'
- en: Final value matrix (as *V(s) = max[a] Q(s, a)*)
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最终值矩阵（作为*V(s) = max[a] Q(s, a)*）
- en: 'As expected, the Q function has been learned in a consistent way, and we can
    get a confirmation plotting the resulting policy:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，Q函数已经以一致的方式学习，我们可以通过绘制结果策略来得到证实：
- en: '![](img/7736ad03-4864-455a-b93d-832fb9048459.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7736ad03-4864-455a-b93d-832fb9048459.png)'
- en: Final policy
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最终策略
- en: The policy is coherent with the initial objective, and the agent avoids all
    negative absorbing states, always trying to move towards the final positive state.
    However, some paths seem longer than expected. As an exercise, I invite the reader
    to retrain the model for a larger number of iterations, adjusting the exploration
    period. Moreover, *is it possible to improve the model by increasing (or decreasing)
    the discount factor γ?* Remember that *γ → 0* leads to a short-sighted agent,
    which is able to select actions only considering the immediate reward, while*γ →
    1* forces the agent to take into account a larger number of future rewards. This
    particular example is based on a long environment, because the agent always starts
    from *(0, 0)* and must reach the farthest point; therefore, all intermediate states
    have less importance, and it's helpful to look at the future to pick the optimal
    actions. Using random starts can surely improve the policy for all initial states,
    but it's interesting to investigate how different *γ* values can affect the decisions;
    hence, I suggest repeating the experiment in order to evaluate the various configurations
    and increase awareness about the different factors that are involved in a TD algorithm.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 政策与初始目标一致，代理避免了所有负面吸收状态，始终试图向最终积极状态移动。然而，一些路径似乎比预期的要长。作为一个练习，我邀请读者重新训练模型，进行更多的迭代次数，调整探索期。此外，*是否可以通过增加（或减少）折扣因子γ来改进模型？*
    记住，*γ → 0* 导致短视的代理，只能根据即时奖励选择动作，而*γ → 1* 则迫使代理考虑更多的未来奖励。这个特定例子基于一个长期环境，因为代理始终从
    *(0, 0)* 开始，必须到达最远点；因此，所有中间状态的重要性都较低，展望未来以选择最佳动作是有帮助的。使用随机起点可以无疑地改善所有初始状态的政策，但研究不同的*γ*值如何影响决策是有趣的；因此，我建议重复实验以评估不同的配置并提高对涉及TD算法的不同因素的意识。
- en: Q-learning
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: 'This algorithm was proposed by Watkins (in *Learning from delayed rewards*,
    *Watkins C.I.C.H.*,* Ph.D. Thesis*, *University of Cambridge*, *1989*; and further
    analyzed in *Watkins C.I.C.H.*, *Dayan P.*, *Technical Note Q-Learning*, *Machine
    Learning 8*, *1992*) as a more efficient alternative to SARSA. The main feature
    of *Q-learning* is that the TD update rule is immediately greedy with respect
    to the *Q(s[t+1], a)* function:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这个算法由Watkins（在*延迟奖励学习*，*Watkins C.I.C.H.*，*博士论文*，*剑桥大学*，*1989*；并在*Watkins C.I.C.H.*，*Dayan
    P.*，*技术笔记Q-Learning*，*机器学习8*，*1992*）提出，作为SARSA的更有效替代方案。*Q-learning*的主要特点是TD更新规则立即对*Q(s[t+1],
    a)*函数贪婪：
- en: '![](img/d568224d-83a3-42bf-85c1-5b5ae44789d1.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d568224d-83a3-42bf-85c1-5b5ae44789d1.png)'
- en: 'The key idea is to compare the current *Q(s[t], a[t])* value with the maximum
    *Q* value achievable when the agent is in the successor state. In fact, as the
    policy must be GLIE, the convergence speed can be increased by avoiding wrong
    estimations due to the selection of a *Q* value that won''t be associated with
    the final action. By choosing the maximum *Q* value, the algorithm will move towards
    the optimal solution faster than SARSA, and also, the convergence proof is less
    restrictive. In fact, Watkins and Dayan (in the aforementioned papers) proved
    that, if *|r[i]| < R*, the learning rate *α ∈ [0, 1[* (in this case, α must be
    always smaller than 1) with the same constraints imposed for SARSA (*Σα = ∞* and *Σα² <
    ∞*), then the estimated *Q* function converges with probability 1 to the optimal
    one:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 关键思想是比较当前 *Q(s[t], a[t])* 值与代理处于后续状态时可以达到的最大 *Q* 值。实际上，由于策略必须是GLIE，可以通过避免选择不会与最终动作关联的
    *Q* 值而导致的错误估计来提高收敛速度。通过选择最大 *Q* 值，算法将比SARSA更快地趋向于最优解，并且，收敛证明也更加宽松。事实上，Watkins和Dayan（在上述论文中）证明了，如果
    *|r[i]| < R*，则学习率 *α ∈ [0, 1[* （在这种情况下， α 必须始终小于1）并且对SARSA施加相同的约束（*Σα = ∞* 和 *Σα² <
    ∞*），则估计的 *Q* 函数以概率1收敛到最优值：
- en: '![](img/825eda8d-d7a9-4362-815d-f39440a16f79.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/825eda8d-d7a9-4362-815d-f39440a16f79.png)'
- en: As discussed for SARSA, the conditions on the rewards and the learning rate
    can be managed by employing a clipping function and a temporal decay, respectively.
    In almost all deep Q-learning applications, these are extremely important factors
    to guarantee the convergence; therefore, I invite the reader to consider them
    whenever the training process isn't able to converge to an acceptable solution.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如同SARSA所讨论的，可以通过使用裁剪函数和时间衰减来管理奖励和学习率的条件。在几乎所有的深度Q学习应用中，这些是保证收敛的极其重要的因素；因此，我邀请读者在训练过程无法收敛到可接受的解决方案时考虑它们。
- en: 'The complete Q-learning algorithm (with an optional forced termination of the
    episode) is:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 Q-learning 算法（带有可选的强制终止回合）是：
- en: Set an initial deterministic random policy, *π(s)*
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个初始的确定性随机策略，*π(s)*
- en: Set the initial value array, *Q(s, a) = 0 ∀ s ∈ S* and *∀ a ∈ A*
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置初始值数组，*Q(s, a) = 0 ∀ s ∈ S* 和 *∀ a ∈ A*
- en: Set the number of episodes, *N*[*episodes*]
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置回合数，*N*[*episodes*]
- en: Set a maximum number of steps per episode, *N*[*max*]
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置每个回合的最大步数，*N*[*max*]
- en: Set a constant, *α* (*α = 0.1*)
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个常数，*α* (*α = 0.1*)
- en: Set a constant, *γ* (*γ** = 0.9*)
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个常数，*γ* (*γ** = 0.9*)
- en: Set an initial exploration factor, *ε^((0))* (*ε^((0)) = 1.0*)
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个初始探索因子， *ε^((0))* (*ε^((0)) = 1.0*)
- en: Define a policy to let the exploration factor *ε* decay (linear or exponential)
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个策略，让探索因子 *ε*衰减（线性或指数）
- en: Set a counter, *e = 0*
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置一个计数器，*e = 0*
- en: 'For *i = 1* to *N[episodes]*:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '对于 *i = 1* 到 *N[episodes]*:'
- en: Observe the initial state, *s*[*i*]
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察初始状态， *s*[*i*]
- en: 'While *s[j]* is non-terminal and *e < N[max]*:'
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当 *s[j]* 是非终止状态且 *e < N[max]* 时：
- en: '*e += 1*'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*e += 1*'
- en: Select the action, *a[t] = **π(s[i])*, with an exploration factor *ε*^(*(e)*)
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作，*a[t] = **π(s[i])*, 带有探索因子 *ε*^(*(e)*)
- en: Observe the transition (*a[t], s[i]*) → (*s[j], r[ij]*)
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察转换 (*a[t], s[i]*) → (*s[j], r[ij]*)
- en: Select the action, *a[t+1] = π(s[j]**)*, with an exploration factor *ε*^(*(e)*)
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择动作，*a[t+1] = π(s[j]**)*，带有探索因子 *ε*^(*(e)*)
- en: Update the *Q(s[t], a[t])* function (if *s[j]* is terminal, set *Q(s[t+1], a[t+1]**)
    = 0*) using *max[a] Q(s[t+1], a)*
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *max[a] Q(s[t+1], a)* 更新 *Q(s[t], a[t])* 函数（如果 *s[j]* 是终止状态，则设置 *Q(s[t+1],
    a[t+1]**) = 0*）
- en: Set *s[i] = s*[*j*]
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置 *s[i] = s*[*j*]
- en: Q-learning in the checkerboard environment
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 棋盘环境中的Q学习
- en: 'Let''s repeat the previous experiment with the Q-learning algorithm. As all
    of the constants are the same (as well as the choice of a *ε*-greedy policy and
    the starting point set to *(0, 0)*), we can directly define the function that
    implements the training for a single episode:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用Q学习算法重复之前的实验。由于所有常数都相同（以及选择*ε*-贪婪策略和起始点设置为*(0, 0)*），我们可以直接定义实现单个回合训练的函数：
- en: '[PRE12]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can now train the model for 5,000 iterations, with 3,500 explorative ones:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以训练模型5,000次迭代，其中3,500次是探索性的：
- en: '[PRE13]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The resulting value matrix (defined as in the SARSA experiment) is:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的值矩阵（定义为SARSA实验中）是：
- en: '![](img/b87746f0-69ad-4765-9ce2-70d810465dcc.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b87746f0-69ad-4765-9ce2-70d810465dcc.png)'
- en: Final value matrix
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 最终值矩阵
- en: 'Again, the learned *Q* function (and obviously, also the greedy *V(s)*) is
    coherent with the initial objective (in particular, considering the starting point
    set to *(0, 0)*), and the resulting policy can immediately confirm this result:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，学到的*Q*函数（以及显然的，贪婪的*V(s)*）与初始目标一致（特别是考虑到起始点设置为*(0, 0)*），并且由此产生的策略可以立即证实这一结果：
- en: '![](img/098a6096-5934-4fb8-9601-78c6f2bbec35.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/098a6096-5934-4fb8-9601-78c6f2bbec35.png)'
- en: Final policy
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 最终策略
- en: The behavior of Q-learning is not very different from SARSA (even if the convergence
    is faster), and some initial states are not perfectly managed. This is a consequence
    of our choice; therefore, I invite the reader to repeat the exercise using random
    starts and comparing the training speed of Q-learning and SARSA.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的表现与SARSA（即使收敛速度更快）没有太大区别，并且一些初始状态管理并不完美。这是我们的选择的结果；因此，我邀请读者使用随机起始重复练习，并比较Q学习和SARSA的训练速度。
- en: Q-learning using a neural network
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络的Q学习
- en: 'Now, we want to test the Q-learning algorithm using a smaller checkerboard
    environment and a neural network (with Keras). The main difference from the previous
    examples is that now, the state is represented by a screenshot of the current
    configuration; hence, the model has to learn how to associate a value with each
    input image and action. This isn''t actual deep Q-learning (which is based on
    Deep Convolutional Networks, and requires more complex environments that we cannot
    discuss in this book), but it shows how such a model can learn an optimal policy
    with the same input provided to a human being. In order to reduce the training
    time, we are considering a square checkerboard environment, with four negative
    absorbing states and a positive final one:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想使用较小的棋盘环境和神经网络（使用Keras）来测试Q学习算法。与前例的主要区别在于，现在状态由当前配置的截图表示；因此，模型必须学会将值与每个输入图像和动作关联起来。这不是实际的深度Q学习（它基于深度卷积网络，需要更复杂的环境，我们无法在本书中讨论），但它展示了这样的模型如何使用与人类相同的输入学习最优策略。为了减少训练时间，我们正在考虑一个正方形的棋盘环境，有四个负吸收状态和一个正最终状态：
- en: '[PRE14]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A graphical representation of the rewards is shown in the following figure:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了奖励的图形表示：
- en: '![](img/8795c029-0797-4dd0-ba56-81f67b20ff0c.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8795c029-0797-4dd0-ba56-81f67b20ff0c.png)'
- en: Rewards in the smaller checkerboard environment
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 较小棋盘环境中的奖励
- en: 'As we want to provide the network with a graphical input, we need to define
    a function to create a matrix representing the tunnel:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望向网络提供图形输入，我们需要定义一个函数来创建表示隧道的矩阵：
- en: '[PRE15]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `reset_tunnel()` function sets all values equal to 0, except for (which
    is marked with `-1`) and the final state (defined by `0.5`). The position of the
    agent (defined with the value `1`) is directly managed by the training function.
    At this point, we can create and compile our neural network. As the problem is
    not very complex, we are employing an MLP:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`reset_tunnel()`函数将所有值设置为0，除了（用`-1`标记）和最终状态（由`0.5`定义）。代理的位置（用值`1`定义）直接由训练函数管理。在此阶段，我们可以创建和编译我们的神经网络。由于问题不太复杂，我们正在使用MLP：'
- en: '[PRE16]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The input is a flattened array, while the output is the *Q* function (all of
    the values corresponding to each action). The network is trained using RMSprop
    and a mean squared error loss function (our goal is to reduce the MSE between
    the actual value and the prediction). In order to train and query the network,
    it''s helpful to create two dedicated functions:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是一个展平的数组，而输出是*Q*函数（所有对应于每个动作的值）。网络使用RMSprop和均方误差损失函数进行训练（我们的目标是减少实际值和预测值之间的均方误差）。为了训练和查询网络，创建两个专用函数是有帮助的：
- en: '[PRE17]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The behavior of these functions is straightforward. The only element that may
    be new to the reader is the use of the `train_on_batch()` method. Contrary to
    `fit()`, this function allows us to perform a single training step, given a batch
    of input-output couples (in our case, we always have a single couple). As our
    goal is finding an optimal path to the final state, starting from every possible
    cell, we are going to employ random starts:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数的行为很简单。对读者可能陌生的唯一元素是使用`train_on_batch()`方法。与`fit()`不同，此函数允许我们执行单个训练步骤，给定一批输入-输出对（在我们的情况下，我们始终只有一个对）。由于我们的目标是找到从每个可能的单元格开始的到达最终状态的最优路径，我们将使用随机起始：
- en: '[PRE18]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now, we can define the functions needed to perform a single training step:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义执行单个训练步骤所需的函数：
- en: '[PRE19]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The `q_step_neural_network()` function is very similar to the one defined in
    the previous example. The only difference is the management of the visual state.
    Every time there''s a transition, the value `1.0` (denoting the agent) is moved
    from the old position to the new one, and the value of the previous cell is reset
    to its default (saved in the `prev_value` variable). Another secondary difference
    is the absence of *α* because there''s already a learning rate set in the SGD
    algorithm, and it doesn''t make sense to add another parameter to the model. We
    can now train the model for 10,000 iterations, with 7,500 explorative ones:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '`q_step_neural_network()`函数与前面示例中定义的函数非常相似。唯一的区别是视觉状态的管理。每次发生转换时，值`1.0`（表示代理）从旧位置移动到新位置，前一个单元格的值重置为其默认值（保存在`prev_value`变量中）。另一个次要区别是缺少*α*，因为SGD算法中已经设置了学习率，因此没有必要在模型中添加另一个参数。现在我们可以用10,000次迭代训练模型，其中7,500次用于探索：'
- en: '[PRE20]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'When the training process has finished, we can analyze the total rewards, in
    order to understand whether the network has successfully learned the *Q* functions:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练过程完成后，我们可以分析总奖励，以了解网络是否成功学习了*Q*函数：
- en: '![](img/ae1981ff-5be7-44eb-a44e-cce3327b5944.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ae1981ff-5be7-44eb-a44e-cce3327b5944.png)'
- en: Total rewards obtained by the neural network Q-learning algorithm
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络Q学习算法获得的总奖励
- en: 'It''s clear that the model is working well, because after the exploration period,
    the total reward becomes stationary around `4`, with small oscillations due to
    the different path lengths (however, the final plot can be different because of
    the internal random state employed by Keras). To see a confirmation, let''s generate
    the trajectories for all of the possible initial states, using the greedy policy
    (equivalent to *ε = 0*):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，模型运行良好，因为经过探索期后，总奖励值稳定在`4`左右，由于路径长度不同而出现小幅波动（然而，由于Keras使用的内部随机状态，最终图表可能会有所不同）。为了确认这一点，让我们使用贪婪策略（相当于*ε
    = 0*）生成所有可能初始状态下的轨迹：
- en: '[PRE21]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Twelve random trajectories are shown in the following figure:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了12条随机轨迹：
- en: '![](img/e1324c22-d2fc-442e-9242-04366531da33.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e1324c22-d2fc-442e-9242-04366531da33.png)'
- en: Twelve trajectories generated using the greedy policy
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用贪婪策略生成的12条轨迹
- en: 'The agent always follows the optimal policy, independent from the initial state,
    and never ends up in a well. Even if the example is quite simple, it''s helpful
    to introduce the reader to the concept of deep Q-learning (for further details,
    the reader can check the introductory paper, *Deep Reinforcement Learning*: *An
    Overview*, *Li Y.*, *arXiv:1701.07274 [cs.LG]*).'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 代理始终遵循最优策略，不受初始状态的影响，并且永远不会陷入陷阱。即使示例相当简单，也有助于向读者介绍深度Q学习（有关更多细节，读者可以查看介绍性论文，*深度强化学习*：*概述*，*Li
    Y.*，*arXiv:1701.07274 [cs.LG]*）。
- en: In a general case, the environment can be a more complex game (like Atari or
    Sega), and the number of possible actions is very limited. Moreover, there's no
    possibility to employ random starts, but it's generally a good practice to skip
    a number of initial frames, in order to avoid a bias to the estimator. Clearly,
    the network must be more complex (involving convolutions to better learn the geometric
    dependencies), and the number of iterations must be extremely large. Many other
    tricks and specific algorithms can be employed in order to speed up the convergence,
    but due to a lack of space, they are beyond the scope of this book.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，环境可以是一个更复杂的游戏（如Atari或Sega），可能采取的动作数量非常有限。此外，没有随机开始的可能性，但通常一个好的做法是跳过一些初始帧，以避免对估计器的偏差。显然，网络必须更复杂（涉及卷积以更好地学习几何依赖关系），并且迭代次数必须非常大。为了加快收敛速度，可以采用许多其他技巧和特定算法，但由于篇幅限制，这些内容超出了本书的范围。
- en: However, the general process and its logic are almost the same, and it's not
    difficult to understand why some strategies are preferable, and how the accuracy
    can be improved. As an exercise, I invite the reader to create more complex environments,
    with or without checkpoints and stochastic rewards. It's not surprising to see
    how the model will be able to easily learn the dynamics with a sufficiently large
    number of episodes. Moreover, as suggested in the Actor-Critic section, it's a
    good idea to use Tensorflow to implement such a model, comparing the performances
    with Q-learning.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一般过程及其逻辑几乎是相同的，理解为什么某些策略更可取以及如何提高准确性并不困难。作为一个练习，我邀请读者创建更复杂的环境，带有或不带有检查点和随机奖励。看到模型能够轻松地通过足够多的回合学习动态，这并不令人惊讶。此外，正如在Actor-Critic部分所建议的，使用Tensorflow实现这样一个模型是一个好主意，并将性能与Q-learning进行比较。
- en: Summary
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we presented the natural evolution of TD(0), based on an average
    of backups with different lengths. The algorithm, called TD(λ), is extremely powerful,
    and it assures a faster convergence than TD(0), with only a few (non-restrictive)
    conditions. We also showed how to implement the Actor-Critic method with TD(0),
    in order to learn about both a stochastic policy and a value function.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了基于不同长度备份的平均的TD(0)的自然演变。被称为TD(λ)的算法非常强大，它保证了比TD(0)更快的收敛速度，只需满足少数（非限制性）条件。我们还展示了如何使用TD(0)实现Actor-Critic方法，以便学习随机策略和值函数。
- en: 'In further sections, we discussed two methods based on the estimation of the
    *Q* function: SARSA and Q-learning. They are very similar, but the latter has
    a greedy approach, and its performance (in particular, the training speed) results
    in it being superior to SARSA. The Q-learning algorithm is one of the most important
    models for the latest developments. In fact, it was the first RL approach employed
    with a Deep Convolutional Network to solve complex environments (like Atari games).
    For this reason, we also presented a simple example, based on an MLP that processes
    a visual input and outputs the *Q* values for each action.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们讨论了基于估计*Q*函数的两种方法：SARSA和Q-learning。它们非常相似，但后者采用贪婪方法，其性能（特别是训练速度）使其优于SARSA。Q-learning算法是最新发展中最重要的模型之一。事实上，它是第一个与深度卷积网络结合使用的强化学习方法，用于解决复杂环境（如Atari游戏）。因此，我们也提供了一个基于MLP的简单示例，该MLP处理视觉输入并输出每个动作的*Q*值。
- en: The world of RL is extremely fascinating, and hundreds of researchers work every
    day to improve algorithms and solve more and more complex problems. I invite the
    reader to check the references in order to find useful resources that can be exploited
    to obtain a deeper understanding of the models and their developments. Moreover,
    I suggest reading the blog posts written by the Google DeepMind team, which is
    one of the pioneers in the field of deep RL. I also suggest searching for the
    papers freely available on *arXiv*.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的世界极其迷人，每天都有数百名研究人员致力于改进算法和解决越来越复杂的问题。我邀请读者查阅参考文献，以找到可以利用的有用资源，从而更深入地理解模型及其发展。此外，我建议阅读由谷歌DeepMind团队撰写的博客文章，该团队是深度强化学习领域的先驱之一。我还建议搜索在*arXiv*上免费提供的论文。
- en: I'm happy to end this book with this topic, because I believe that RL can provide
    new and more powerful tools that will dramatically change our lives!
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我很高兴以这个主题结束这本书，因为我相信强化学习可以提供新的、更强大的工具，这将极大地改变我们的生活！
