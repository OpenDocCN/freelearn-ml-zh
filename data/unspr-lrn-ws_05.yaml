- en: 4\. Dimensionality Reduction Techniques and PCA
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4. 降维技术与PCA
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will apply dimension reduction techniques and describe the
    concepts behind principal components and dimensionality reduction. We will apply
    **Principal Component Analysis** (**PCA**) when solving problems using scikit-learn.
    We will also compare manual PCA versus scikit-learn. By the end of this chapter,
    you will be able to reduce the size of a dataset by extracting only the most important
    components of variance within the data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将应用降维技术，并描述主成分和降维背后的概念。在使用scikit-learn解决问题时，我们将应用**主成分分析**（**PCA**）。我们还将比较手动PCA与scikit-learn的实现。到本章结束时，你将能够通过提取数据中最重要的方差成分，来减小数据集的规模。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the previous chapter, we discussed clustering algorithms and how they can
    be helpful to find underlying meaning in large volumes of data. This chapter investigates
    the use of different feature sets (or spaces) in our unsupervised learning algorithms,
    and we will start with a discussion regarding dimensionality reduction, specifically,
    **Principal Component Analysis** (**PCA**). We will then extend our understanding
    of the benefits of the different feature spaces through an exploration of two
    independently powerful machine learning architectures in neural network-based
    autoencoders. Neural networks certainly have a well-deserved reputation for being
    powerful models in supervised learning problems. Furthermore, through the use
    of an autoencoder stage, neural networks have been shown to be sufficiently flexible
    for their application to unsupervised learning problems. Finally, we will build
    on our neural network implementation and dimensionality reduction as we cover
    t-distributed nearest neighbors in *Chapter 6*, *t-Distributed Stochastic Neighbor
    Embedding*. These techniques will prove helpful when dealing with high-dimensional
    data, such as image processing or datasets with many features. One strong business
    benefit of some types of dimension reduction is that it helps to remove features
    that do not have much impact on final outputs. This creates opportunities to make
    your algorithms more efficient without any loss in performance.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们讨论了聚类算法及其在从大量数据中提取潜在意义方面的应用。本章将探讨在无监督学习算法中使用不同特征集（或特征空间），我们将从讨论降维开始，特别是**主成分分析**（**PCA**）。接着，我们将通过探索两种独立强大的机器学习架构——基于神经网络的自动编码器，来扩展我们对不同特征空间优势的理解。神经网络无疑在监督学习问题中享有强大的声誉。此外，通过使用自动编码器阶段，神经网络已经被证明在无监督学习问题中具有足够的灵活性。最后，我们将基于神经网络实现和降维进行扩展，介绍在*第六章*中涵盖的t-分布最近邻方法，*t-分布随机邻居嵌入*。这些技术在处理高维数据时非常有用，如图像处理或包含多个特征的数据集。一些降维技术的一个重要商业优势是，它有助于去除那些对最终输出影响不大的特征。这为提升算法效率创造了机会，而不损失性能。
- en: What Is Dimensionality Reduction?
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是降维？
- en: 'Dimensionality reduction is an important tool in any data scientist''s toolkit,
    and due to its wide variety of use cases, is essentially assumed knowledge within
    the field. So, before we can consider reducing the dimensionality and why we would
    want to reduce it, we must first have a good understanding of what dimensionality
    is. To put it simply, dimensionality is the number of dimensions, features, or
    variables associated with a sample of data. Often, this can be thought of as a
    number of columns in a spreadsheet, where each sample is on a new row, and each
    column describes an attribute of the sample. The following table is an example:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 降维是数据科学家工具箱中的一项重要工具，由于其广泛的应用场景，几乎已成为该领域的基本知识。因此，在我们考虑降维及其降维的原因之前，我们必须首先清楚理解“维度”是什么。简单来说，维度是与数据样本相关的维度、特征或变量的数量。通常，可以将其视为电子表格中的列数，其中每个样本占一行，每列描述样本的一个属性。以下表格就是一个例子：
- en: '![Figure 4.1: Two samples of data with three different features'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.1：具有三种不同特征的两组数据样本'
- en: '](img/B15923_04_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_01.jpg)'
- en: 'Figure 4.1: Two samples of data with three different features'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：具有三种不同特征的两组数据样本
- en: 'In the preceding table, we have two samples of data, each with three independent
    features or dimensions. Depending on the problem being solved, or the origin of
    this dataset, we may want to reduce the number of dimensions per sample without
    losing the provided information. This is where dimensionality reduction can be
    helpful. But how exactly can dimensionality reduction help us to solve problems?
    We will cover the applications in more detail in the following section. However,
    let''s say that we had a very large dataset of time series data, such as echocardiogram
    or ECG (also known as an EKG in some countries) signals, as shown in the following
    diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上表中，我们有两个数据样本，每个样本有三个独立的特征或维度。根据正在解决的问题或数据集的来源，我们可能希望减少每个样本的维度数量，而不丢失提供的信息。这就是降维可以帮助我们的地方。但是降维究竟如何帮助我们解决问题呢？我们将在接下来的部分详细介绍其应用。然而，假设我们有一个非常大的时间序列数据集，比如心电图或心电图（在某些国家也称为心电图），如下图所示：
- en: '![Figure 4.2: Electrocardiogram (ECG or EKG)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.2：心电图（ECG或EKG）'
- en: '](img/B15923_04_02.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_02.jpg)'
- en: 'Figure 4.2: Electrocardiogram (ECG or EKG)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：心电图（ECG或EKG）
- en: 'These signals were captured from your company''s new model of watch, and we
    need to look for signs of a heart attack or stroke. After looking through the
    dataset, we can make a few observations:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信号是从贵公司新款手表中捕获的，我们需要寻找心脏病发作或中风的迹象。在查看数据集后，我们可以得出一些观察结果：
- en: Most of the individual heartbeats are very similar.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数单独的心跳非常相似。
- en: There is some noise in the data from the recording system or from the patient
    moving during the recording.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中存在来自记录系统或患者在记录期间移动时的一些噪声。
- en: Despite the noise, the heartbeat signals are still visible.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管存在噪声，心跳信号仍然可见。
- en: There is a lot of data – too much to be able to process using the hardware available
    on the watch.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据量非常大 - 使用手表上可用的硬件无法处理太多数据。
- en: It is in such a situation that dimensionality reduction really shines. By using
    dimensionality reduction, we are able to remove much of the noise from the signal,
    which, in turn, will assist with the performance of the algorithms that are applied
    to the data as well as reduce the size of the dataset to allow for reduced hardware
    requirements. The techniques that we are going to discuss in this chapter, in
    particular, PCA and autoencoders, have been well applied in research and industry
    to effectively process, cluster, and classify such datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这种情况下，降维技术真正发挥了作用。通过使用降维，我们能够从信号中去除大部分噪声，这反过来将有助于算法对数据的性能以及减小数据集大小以满足更低的硬件要求。本章中要讨论的技术，特别是PCA和自编码器，已在研究和工业中得到了有效地应用，以有效地处理、聚类和分类这类数据集。
- en: Applications of Dimensionality Reduction
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维的应用场景
- en: 'Before we start a detailed investigation of dimensionality reduction and PCA,
    we will discuss some of the common applications for these techniques:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始详细研究降维和PCA之前，我们将讨论这些技术的一些常见应用：
- en: '**Preprocessing/feature engineering**: One of the most common applications
    is in the preprocessing or feature engineering stages of developing a machine
    learning solution. The quality of the information provided during the algorithm
    development, as well as the correlation between the input data and the desired
    result, is critical in order for a high-performing solution to be designed. In
    this situation, PCA can provide assistance, as we are able to isolate the most
    important components of information from the data and provide this to the model
    so that only the most relevant information is being provided. This can also have
    a secondary benefit in that we have reduced the number of features being provided
    to the model, so there can be a corresponding reduction in the number of calculations
    to be completed. This can reduce the overall training time for the system. An
    example use case of this feature engineering would be predicting whether a transaction
    is at risk of credit card theft. In this scenario, you may be presented with millions
    of transactions that each have tens or hundreds of features. This would be resource-intensive
    or even impossible to run a predictive algorithm on in real time; however, by
    using feature preprocessing, we can distill the many features down to just the
    top 3-4 most important ones, thereby reducing runtime.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理/特征工程**：最常见的应用之一是在开发机器学习解决方案的预处理或特征工程阶段。在算法开发过程中提供的信息质量，以及输入数据与期望结果之间的相关性，对于设计出高性能的解决方案至关重要。在这种情况下，PCA可以提供帮助，因为我们能够从数据中分离出最重要的信息成分，并将其提供给模型，以便仅提供最相关的信息。这也有一个次要的好处，即我们减少了提供给模型的特征数量，从而可以减少需要完成的计算量。这可以减少系统的整体训练时间。这个特征工程的一个典型应用案例是预测某笔交易是否存在信用卡盗刷风险。在这种情况下，您可能会面临数百万笔交易，每笔交易有几十个甚至上百个特征。这将是资源密集型的，甚至在实时运行预测算法时都几乎不可能；然而，通过使用特征预处理，我们可以将许多特征提炼成仅仅是最重要的3-4个特征，从而减少运行时间。'
- en: '**Noise reduction**: Dimensionality reduction can also be used as an effective
    noise reduction/filtering technique. It is expected that the noise within a signal
    or dataset does not comprise a large component of the variation within the data.
    Thus, we can remove some of the noise from the signal by removing the smaller
    components of variation and then restoring the data back to the original dataspace.
    In the following example, the image on the left has been filtered to the first
    20 most significant sources of data, which gives us the image on the right. We
    can see that the quality of the image has been reduced, but the critical information
    is still there:![Figure 4.3: An image filtered with dimensionality reduction'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**降噪**：降维也可以作为一种有效的降噪/过滤技术。预期信号或数据集中的噪音并不占据数据变化的主要部分。因此，我们可以通过去除较小的变化成分来去除信号中的一些噪音，然后将数据恢复到原始的数据空间。在下面的示例中，左侧的图像已经过滤到前20个最重要的数据源，这为我们提供了右侧的图像。我们可以看到图像的质量有所降低，但关键信息仍然存在：![图4.3：使用降维过滤的图像](img/B15923_04_03.jpg)'
- en: '](img/B15923_04_03.jpg)'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_03.jpg)'
- en: 'Figure 4.3: An image filtered with dimensionality reduction'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：使用降维过滤的图像
- en: Note
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This photograph was taken by Arthur Brognoli from Pexels and is available for
    free use under [https://www.pexels.com/photo-license/](https://www.pexels.com/photo-license/).
    In this case, we have the original image on the left and the filtered image on
    the right.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这张照片由Arthur Brognoli拍摄，来自Pexels，并根据[https://www.pexels.com/photo-license/](https://www.pexels.com/photo-license/)提供免费下载。在这种情况下，左侧是原始图像，右侧是经过过滤的图像。
- en: '**Generating plausible artificial datasets**: As PCA divides the dataset into
    the components of information (or variation), we can investigate the effects of
    each component or generate new dataset samples by adjusting the ratios between
    the eigenvalues. We will cover more on eigenvalues later on in this chapter. We
    can scale these components, which, in effect, increases or decreases the importance
    of that specific component. This is also referred to as **statistical shape modeling**,
    as one common method is to use it to create plausible variants of shapes. It is
    also used to detect facial landmarks in images in the process of **active shape
    modeling**.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成合理的人工数据集**：由于主成分分析（PCA）将数据集分解为信息（或变化）的组件，我们可以研究每个组件的影响，或者通过调整特征值之间的比例来生成新的数据集样本。本章稍后会详细介绍特征值。我们可以对这些组件进行缩放，这实际上是增加或减少该特定组件的重要性。这也被称为**统计形状建模**，因为一个常见的方法是使用它来创建合理的形状变体。它还用于在**主动形状建模**过程中检测图像中的面部标志点。'
- en: '**Financial modeling/risk analysis**: Dimensionality reduction provides a useful
    toolkit for the finance industry, since being able to consolidate a large number
    of individual market metrics or signals into a smaller number of components allows
    for faster, and more efficient, computations. Similarly, the components can be
    used to highlight those higher-risk products/companies.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**金融建模/风险分析**：降维为金融行业提供了一个有用的工具包，因为能够将大量的市场指标或信号合并成较少的几个组件，可以加快计算速度并提高效率。类似地，这些组件可以用于突出那些风险较高的产品/公司。'
- en: The Curse of Dimensionality
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 高维灾难
- en: 'Before we can understand the benefits of using dimensionality reduction techniques,
    we must first understand why the dimensionality of feature sets needs to be reduced
    at all. The **curse of dimensionality** is a phrase commonly used to describe
    issues that arise when working with data that has a high number of dimensions
    in the feature space; for example, the number of attributes that are collected
    for each sample. Consider a dataset of point locations within a game of *Pac-Man*.
    Your character, Pac-Man, occupies a position within the virtual world defined
    by two dimensions or coordinates (*x*, *y*). Let''s say that we are creating a
    new computer enemy: an AI-driven ghost to play against, and that it requires some
    information regarding our character to make its own game logic decisions. For
    the bot to be effective, we require the player''s position (*x*, *y*) and their
    velocity in each of the directions (*vx*, *vy*) in addition to the players last
    five (*x*, *y*) positions, the number of remaining hearts, and the number of remaining
    power pellets in the maze (power pellets temporarily allow Pac-Man to eat ghosts).
    Now, for each moment in time, our bot requires 16 individual features (or dimensions)
    to make its decisions. These 16 features correspond to 5 previous positions times
    the 2 *x* and *y* coordinates + the 2 *x* and *y* coordinates of the player''s
    current position + the 2 *x* and *y* coordinates of player''s velocity + 1 feature
    for the number of hearts + 1 feature for the power pellets = 16\. This is clearly
    a lot more than just the two dimensions as provided by the position:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们理解使用降维技术的好处之前，我们必须首先理解为什么需要减少特征集的维度。**高维灾难**是一个常用术语，用来描述在处理具有高维特征空间的数据时所遇到的问题；例如，为每个样本收集的属性数量。考虑一下《吃豆人》游戏中的点位置数据集。你的角色，吃豆人，处于一个由两个维度或坐标（*x*,
    *y*）定义的虚拟世界中的某个位置。假设我们正在创建一个新的电脑敌人：一个由人工智能驱动的鬼怪，用来对抗玩家，它需要一些关于玩家角色的信息来做出自己的游戏逻辑决策。为了让这个机器人有效，我们需要玩家的位置信息（*x*,
    *y*）和他们在各个方向上的速度（*vx*, *vy*），以及玩家最近五个（*x*, *y*）位置的数据，剩余的心数和迷宫中剩余的能量豆数量（能量豆暂时允许吃豆人吃掉鬼怪）。因此，在每个时刻，我们的机器人需要16个单独的特征（或维度）来做出决策。这16个特征对应于5个先前的位置数据乘以2个*x*和*y*坐标
    + 当前玩家位置的2个*x*和*y*坐标 + 玩家速度的2个*x*和*y*坐标 + 1个心数特征 + 1个能量豆特征 = 16。这显然比单纯由位置提供的两个维度要多得多：
- en: '![Figure 4.4: Dimensions in a PacMan game'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.4：吃豆人游戏中的维度'
- en: '](img/B15923_04_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_04.jpg)'
- en: 'Figure 4.4: Dimensions in a PacMan game'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4：吃豆人游戏中的维度
- en: 'To explain the concept of dimensionality reduction, we will consider a fictional
    dataset (see *Figure 4.5*) of *x* and *y* coordinates as features, giving two
    dimensions in the feature space. It should be noted that this example is by no
    means a mathematical proof but is rather intended to provide a means of visualizing
    the effect of increased dimensionality. In this dataset, we have six individual
    samples (or points), and we can visualize the currently occupied volume within
    the feature space of approximately *(3 – 1) x (4 – 2) = 2 x 2 = 4* squared units:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释降维的概念，我们将考虑一个虚构的数据集（参见*图4.5*），其中*x*和*y*坐标作为特征，构成了特征空间中的两个维度。需要注意的是，这个例子绝不是数学证明，而是旨在提供一种可视化增加维度效果的方法。在这个数据集中，我们有六个单独的样本（或点），我们可以在特征空间中可视化目前占据的体积，约为*(3
    – 1) x (4 – 2) = 2 x 2 = 4*平方单位：
- en: '![Figure 4.5: Data in a 2D feature space'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.5：二维特征空间中的数据'
- en: '](img/B15923_04_05.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_05.jpg)'
- en: 'Figure 4.5: Data in a 2D feature space'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：二维特征空间中的数据
- en: 'Suppose the dataset comprises the same number of points, but with an additional
    feature (the *z* coordinate) to each sample. The occupied data volume is now approximately
    *2 x 2 x 2 = 8* cubed units. So, we now have the same number of samples, but the
    space enclosing the dataset is now larger. As such, the data takes up less relative
    volume in the available space and is now sparser. This is the curse of dimensionality;
    as we increase the number of available features, we increase the sparsity of the
    data, and, in turn, make statistically valid correlations more difficult. Looking
    back to our example of creating a video game bot to play against a human player,
    we have 16 features that are a mix of different feature types: positions, velocity,
    power-ups, and hearts. Depending on the range of possible values for each of these
    features and the variance to the dataset provided by each feature, the data could
    be extremely sparse. Even within the constrained world of Pac-Man, the potential
    variance of each of the features could be quite large, some much larger than others.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据集包含相同数量的点，但每个样本增加了一个特征（*z*坐标）。现在，占据的数据体积大约是*2 x 2 x 2 = 8*立方单位。因此，我们现在有相同数量的样本，但数据集所包围的空间变大了。因此，数据在可用空间中的相对体积变小，变得更加稀疏。这就是维度的诅咒；随着我们增加可用特征的数量，数据的稀疏性增加，从而使得统计相关性更难以识别。回到我们创建一个视频游戏机器人与人类玩家对战的例子，我们有16个特征，这些特征是不同类型的混合：位置、速度、能量道具和生命值。根据每个特征的可能取值范围以及每个特征对数据集的方差，这些数据可能会变得非常稀疏。即使在受限的吃豆人世界中，每个特征的潜在方差也可能非常大，有些特征的方差甚至比其他特征大得多。
- en: So, without dealing with the sparsity of the dataset, we have more information
    with the additional feature(s), but may not be able to improve the performance
    of our machine learning model, as the statistical correlations are more difficult.
    What we would like to do is to keep the useful information provided by the extra
    features but minimize the negative effect of sparsity. This is exactly what dimensionality
    reduction techniques are designed to do and these can be extremely powerful in
    increasing the performance of your machine learning model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在不处理数据集稀疏性的情况下，额外的特征提供了更多的信息，但可能无法提高我们机器学习模型的性能，因为统计相关性更难以识别。我们希望做的是保留额外特征提供的有用信息，同时最小化稀疏性的负面影响。这正是降维技术的设计目的，这些技术在提高机器学习模型性能方面可以非常强大。
- en: Throughout this chapter, we will discuss a number of different dimensionality
    reduction techniques and will cover one of the most important and useful methods,
    PCA, in greater detail with an example.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论多种不同的降维技术，并将更加详细地介绍其中最重要和最有用的方法——主成分分析（PCA），并提供一个例子。
- en: Overview of Dimensionality Reduction Techniques
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降维技术概述
- en: 'The goal of any dimensionality reduction technique is to manage the sparsity
    of the dataset while keeping any useful information that is provided. In our case
    of classification, dimensionality reduction is typically used as an important
    preprocessing step used before the actual classification. Most dimensionality
    reduction techniques aim to complete this task using a process of **feature projection**,
    which adjusts the data from the higher-dimensional space into a space with fewer
    dimensions to remove the sparsity from the data. Again, as a means of visualizing
    the projection process, consider a sphere in a 3D space. We can project the sphere
    into a lower 2D space into a circle with some information loss (the value for
    the *z* coordinate), but retaining much of the information that describes its
    original shape. We still know the origin, radius, and manifold (outline) of the
    shape, and it is still very clear that it is a circle. So, depending on the problem
    that we are trying to solve, we may have reduced the dimensionality while retaining
    the important information:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 任何降维技术的目标都是管理数据集的稀疏性，同时保留其中提供的有用信息。在我们的分类案例中，降维通常作为一个重要的预处理步骤，在实际分类之前进行。大多数降维技术的目标是通过**特征投影**的过程来完成这一任务，将数据从高维空间调整到低维空间，以去除数据的稀疏性。同样，为了可视化投影过程，考虑一个三维空间中的球体。我们可以将球体投影到低维的二维空间，得到一个圆形，尽管丢失了一些信息（*z*坐标的值），但保留了描述其原始形状的大部分信息。我们仍然知道球体的原点、半径和流形（轮廓），并且仍然非常清楚它是一个圆。因此，根据我们要解决的问题，我们可能已经在保留重要信息的同时降低了维度：
- en: '![Figure 4.6: A projection of a 3D sphere into a 2D space'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.6：3D 球体投影到 2D 空间'
- en: '](img/B15923_04_06.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_06.jpg)'
- en: 'Figure 4.6: A projection of a 3D sphere into a 2D space'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：3D 球体投影到 2D 空间
- en: The secondary benefit that can be obtained by preprocessing the dataset with
    a dimensionality reduction stage is the improved computational performance that
    can be achieved. As the data has been projected into a lower-dimensional space,
    it will contain fewer, but potentially more powerful, features. The fact that
    there are fewer features means that, during later classification or regression
    stages, the size of the dataset being processed is significantly smaller. This
    will potentially reduce the required system resources and processing time for
    classification/regression, and, in some cases, the dimensionality reduction technique
    can also be used directly to complete the analysis.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数据集上进行降维预处理，可以获得的第二个好处是改进的计算性能。由于数据已经被投影到低维空间，它将包含更少的但可能更强大的特征。特征较少意味着，在后续的分类或回归阶段，处理的数据集规模显著更小。这可能会减少分类/回归所需的系统资源和处理时间，并且在某些情况下，降维技术还可以直接用于完成分析。
- en: This analogy also introduces one of the important considerations of dimensionality
    reduction. We are always trying to balance the information loss resulting from
    the projection into lower dimensional space by reducing the sparsity of the data.
    Depending on the nature of the problem and the dataset being used, the correct
    balance could present itself and be relatively straightforward. In some applications,
    this decision may rely on the outcome of additional validation methods, such as
    cross-validation (particularly in supervised learning problems) or the assessment
    of experts in your problem domain. In this scenario, cross-validation refers to
    the practice of partitioning a rolling section of the data to test on, with the
    inverse serving as the train set until all parts of the dataset are used. This
    approach allows for the reduction of bias within a machine learning problem.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类比还引入了降维中的一个重要考虑因素。我们总是在尝试平衡由于投影到低维空间而导致的信息丢失，同时减少数据的稀疏性。根据问题的性质和使用的数据集，正确的平衡可能会显现出来，并且相对简单。在一些应用中，这个决策可能依赖于额外验证方法的结果，比如交叉验证（特别是在监督学习问题中）或领域专家的评估。在这种情况下，交叉验证指的是将数据集的滚动部分划分出来进行测试，而其余部分作为训练集，直到数据集的所有部分都被使用。此方法有助于减少机器学习问题中的偏差。
- en: One way we like to think about this trade-off in dimensionality reduction is
    to consider compressing a file or image on a computer for transfer. Dimensionality
    reduction techniques, such as PCA, are essentially methods of compressing information
    into a smaller size for transfer, and, in many compression methods, some losses
    occur as a result of the compression process. Sometimes, these losses are acceptable;
    if we are transferring a 50 MB image and need to shrink it to 5 MB for transfer,
    we can expect to still be able to see the main subject of the image, but perhaps
    some smaller background features will become too blurry to see. We would also
    not expect to be able to restore the original image to a pixel-perfect representation
    from the compressed version, but we could expect to restore it with some additional
    artifacts, such as blurring.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们喜欢以一种方式思考降维中的权衡：考虑在计算机上传输文件或图像时进行压缩。降维技术，如PCA，本质上是将信息压缩到更小的尺寸以便传输的方法，在许多压缩方法中，压缩过程中会发生一些损失。有时，这些损失是可以接受的；例如，如果我们需要将一个50
    MB的图像缩小到5 MB以进行传输，我们仍然可以看到图像的主要内容，但可能一些较小的背景细节会变得模糊不清。我们也不会期望能够从压缩后的版本恢复出原始图像的像素完美表示，但我们可以期待恢复时会有一些额外的伪影，例如模糊。
- en: Dimensionality Reduction
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 降维
- en: 'Dimensionality reduction techniques have many uses in machine learning, as
    the ability to extract the useful information of a dataset can provide performance
    boosts in many machine learning problems. They can be particularly useful in unsupervised
    as opposed to supervised learning methods because the dataset does not contain
    any ground truth labels or targets to achieve. In unsupervised learning, the training
    environment is being used to organize the data in a way that is appropriate for
    the problem being solved (for example, classification via clustering), which is
    typically based on the most important information in the dataset. Dimensionality
    reduction provides an effective means of extracting important information, and,
    as there are a number of different methods that we could use, it is beneficial
    to review some of the available options:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 降维技术在机器学习中有许多应用，因为提取数据集中的有用信息能够在许多机器学习问题中提供性能提升。它们在无监督学习中尤为有用，因为无监督学习方法的数据集不包含任何真实标签或目标。无监督学习中，训练环境被用来以适合解决问题的方式组织数据（例如，通过聚类进行分类），这通常是基于数据集中的最重要信息。降维提供了一种有效的提取重要信息的方法，并且由于我们可以使用多种不同的方法，因此回顾一些可用的选项是有益的：
- en: '**Linear Discriminant Analysis** (**LDA**): This is a particularly handy technique
    that can be used for both classification as well as dimensionality reduction.
    LDA will be covered in more detail in *Chapter 7*, *Topic Modeling*.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析** (**LDA**)：这是一种特别实用的技术，既可以用于分类，也可以用于降维。LDA将在*第7章*，*主题建模*中进行详细介绍。'
- en: '**Non-negative matrix factorization** (**NMF**): Like many of the dimensionality
    reduction techniques, this relies on the properties of linear algebra to reduce
    the number of features in the dataset. NMF will also be covered in more detail
    in *Chapter 7*, *Topic Modeling*.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非负矩阵分解** (**NMF**)：像许多降维技术一样，这依赖于线性代数的性质来减少数据集中的特征数。NMF也将在*第7章*，*主题建模*中进行详细介绍。'
- en: '**Singular Value Decomposition** (**SVD**): This is somewhat related to PCA
    (which is covered in more detail in this chapter) and is also a matrix decomposition
    process not too dissimilar to NMF.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解** (**SVD**)：这与PCA（本章将详细介绍）有些相关，也是一个矩阵分解过程，与NMF没有太大区别。'
- en: '**Independent Component Analysis** (**ICA**): This also shares some similarities
    to SVD and PCA, but relaxing the assumption of the data being a Gaussian distribution
    allows for non-Gaussian data to be separated.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立成分分析** (**ICA**)：这与SVD和PCA有一些相似之处，但通过放宽数据为高斯分布的假设，使得非高斯数据也能被分离出来。'
- en: Each of the methods described so far all use linear transformation to reduce
    the sparsity of the data in their original implementation. Some of these methods
    also have variants that use non-linear kernel functions in the separation process,
    providing the ability to reduce the sparsity in a non-linear fashion. Depending
    on the dataset being used, a non-linear kernel may be more effective at extracting
    the most useful information from the signal.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止所描述的每种方法都使用线性变换来减少数据在原始实现中的稀疏性。这些方法中的一些还有使用非线性核函数进行分离过程的变体，提供了以非线性方式减少稀疏性的能力。根据所使用的数据集，非线性核可能在从信号中提取最有用的信息时更为有效。
- en: Principal Component Analysis
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: As described previously, PCA is a commonly used and very effective dimensionality
    reduction technique, which often forms a preprocessing stage for a number of machine
    learning models and techniques. For this reason, we will dedicate this section
    of the book to looking at PCA in more detail than any of the other methods. PCA
    reduces the sparsity in the dataset by separating the data into a series of components
    where each component represents a source of information within the data. As its
    name suggests, the first component produced in PCA, the **principal component**,
    comprises the majority of information or variance within the data. The principal
    component can often be thought of as contributing the most amount of interesting
    information in addition to the mean. With each subsequent component, less information,
    but more subtlety, is contributed to the compressed data. If we consider all of
    these components together, there will be no benefit of using PCA, as the original
    dataset will be returned. To clarify this process and the information returned
    by PCA, we will use a worked example, completing the PCA calculations by hand.
    But first, we must review some foundational statistical concepts, which are required
    to execute the PCA calculations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，PCA是一种常用且非常有效的降维技术，它通常是许多机器学习模型和技术的预处理阶段。因此，我们将把本书的这一部分专门用来详细探讨PCA，而不仅仅是其他方法。PCA通过将数据分离成一系列组件来减少数据集的稀疏性，其中每个组件代表数据中的一个信息源。正如其名称所示，PCA生成的第一个组件，即**主成分**，包含了数据中大部分的信息或方差。主成分通常可以被认为是在均值之外，贡献最多的有趣信息。随着每个后续组件的生成，贡献的信息虽然减少，但压缩数据中的微妙之处也增多。如果我们将所有这些组件放在一起使用，那么PCA将没有任何好处，因为它会还原回原始数据集。为了澄清这个过程以及PCA返回的信息，我们将通过一个实际的例子，手动完成PCA的计算。但首先，我们必须复习一些执行PCA计算所需的基础统计概念。
- en: Mean
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 均值
- en: The mean, or the average value, is simply the addition of all values divided
    by the number of values in the set.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 均值，或称平均值，就是将所有值相加并除以数据集中的值的数量。
- en: Standard Deviation
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标准差
- en: Often referred to as the spread of the data and related to the variance, the
    standard deviation is a measure of how much of the data lies within proximity
    to the mean. In a normally distributed dataset, approximately 68% of the dataset
    lies within one standard deviation of the mean, (that is, between (mean - 1*std)
    to (mean + 1*std), you can find 68% of the data if it is normally distributed.)
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常被称为数据的分布，并与方差相关，标准差是衡量数据有多少接近均值的一个指标。在正态分布的数据集中，约68%的数据位于均值的一个标准差内（即在（均值 -
    1*std）到（均值 + 1*std）之间，如果数据是正态分布的，你可以找到68%的数据）。
- en: The relationship between the variance and standard deviation is quite a simple
    one – the variance is the standard deviation squared.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 方差和标准差之间的关系相当简单——方差是标准差的平方。
- en: Covariance
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协方差
- en: Where standard deviation or variance is the spread of the data calculated on
    a single dimension, the covariance is the variance of one dimension (or feature)
    against another. When the covariance of a dimension is computed against itself,
    the result is the same as simply calculating the variance for the dimension.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 当标准差或方差是基于单一维度计算的数据的分布时，协方差则是一个维度（或特征）相对于另一个维度的方差。当计算某一维度相对于自身的协方差时，结果与简单计算该维度的方差相同。
- en: Covariance Matrix
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: 'A covariance matrix is a matrix representation of the possible covariance values
    that can be computed for a dataset. Other than being particularly useful in data
    exploration, covariance matrices are also required to execute the PCA of a dataset.
    To determine the variance of one feature with respect to another, we simply look
    up the corresponding value in the covariance matrix. In the following diagram,
    we can see that, in column 1, row 2, the value is the variance of feature or dataset
    *Y* with respect to *X* (*cov(Y, X))*. We can also see that there is a diagonal
    column of covariance values computed against the same feature or dataset; for
    example, *cov(X, X)*. In this situation, the value is simply the variance of *X*:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵是数据集可能计算出的协方差值的矩阵表示。除了在数据探索中非常有用，协方差矩阵在执行数据集的主成分分析（PCA）时也是必需的。要确定一个特征相对于另一个特征的方差，我们只需在协方差矩阵中查找对应的值。在下图中，我们可以看到，在第
    1 列第 2 行的值是特征或数据集 *Y* 相对于 *X* 的方差（*cov(Y, X))*）。我们还可以看到，协方差矩阵的对角线列包含了对相同特征或数据集计算的协方差值；例如，*cov(X,
    X)*。在这种情况下，值就是 *X* 的方差：
- en: '![Figure 4.7: The covariance matrix'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.7：协方差矩阵'
- en: '](img/B15923_04_07.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_07.jpg)'
- en: 'Figure 4.7: The covariance matrix'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：协方差矩阵
- en: Typically, the exact values of each of the covariances are not as interesting
    as looking at the magnitude and relative size of each of the covariances within
    the matrix. A large value of the covariance of one feature against another would
    suggest that one feature changes significantly with respect to the other, while
    a value close to zero would signify very little change. The other interesting
    aspect of the covariance to look for is the sign associated with the covariance;
    a positive value indicates that as one feature increases or decreases, then so
    does the other, while a negative covariance indicates that the two features diverge
    from one another, with one increasing as the other decreases or vice versa.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，每个协方差的确切值并不像查看协方差矩阵中每个协方差的大小和相对大小那样有趣。一个特征相对于另一个特征的协方差值很大，意味着一个特征相对于另一个特征的变化显著，而接近零的值则意味着变化很小。协方差中另一个值得注意的方面是其符号；正值表示随着一个特征的增加或减少，另一个特征也会增加或减少，而负协方差则表示两个特征相互背离，一个特征增加时另一个特征减少，反之亦然。
- en: Thankfully, `numpy` and `scipy` provide functions to efficiently perform these
    calculations for you. In the next exercise, we will compute these values in Python.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`numpy` 和 `scipy` 提供了高效执行这些计算的函数。在接下来的练习中，我们将在 Python 中计算这些值。
- en: 'Exercise 4.01: Computing Mean, Standard Deviation, and Variance Using the pandas
    Library'
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.01：使用 pandas 库计算均值、标准差和方差
- en: 'In this exercise, we will briefly review how to compute some of the foundational
    statistical concepts using both the `numpy` and `pandas` Python packages. In this
    exercise, we will use a dataset of the measurements of seeds from different varieties
    of wheat, created using X-ray imaging. The dataset, which can be found in the
    accompanying source code, comprises seven individual measurements (`area A`, `perimeter
    P`, `compactness C`, `length of kernel LK`, `width of kernel WK`, `asymmetry coefficient`
    `A_Coef`, and `length of kernel groove LKG`) of three different wheat varieties:
    Kama, Rosa, and Canadian.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将简要回顾如何使用 `numpy` 和 `pandas` Python 库计算一些基础的统计概念。在本练习中，我们将使用一个包含不同品种小麦种子测量数据的数据库，该数据集是通过
    X 射线成像创建的。这个数据集可以在附带的源代码中找到，包含了来自三种不同小麦品种：Kama、Rosa 和 Canadian 的七个独立测量值（`area
    A`、`perimeter P`、`compactness C`、`length of kernel LK`、`width of kernel WK`、`asymmetry
    coefficient` `A_Coef` 和 `length of kernel groove LKG`）。
- en: Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seed](https://archive.ics.uci.edu/ml/datasets/seed)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/2RjpDxk](https://packt.live/2RjpDxk).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集来源于 [https://archive.ics.uci.edu/ml/datasets/seed](https://archive.ics.uci.edu/ml/datasets/seed)（UCI
    机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]）。加利福尼亚大学尔湾分校信息与计算机科学学院。引用：贡献者感激地感谢波兰科学院农物理研究所（Institute
    of Agrophysics of the Polish Academy of Sciences in Lublin）对其工作的支持。数据集也可以从 [https://packt.live/2RjpDxk](https://packt.live/2RjpDxk)
    下载。
- en: 'The steps to be performed are as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行的步骤如下：
- en: 'Import the `pandas`, `numpy`, and `matplotlib` packages for use:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`和`matplotlib`包以供使用：
- en: '[PRE0]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Load the dataset and preview the first five lines of data:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集并预览前五行数据：
- en: '[PRE1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.8: The head of the data'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.8：数据头'
- en: '](img/B15923_04_08.jpg)'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_08.jpg)'
- en: 'Figure 4.8: The head of the data'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.8：数据头
- en: 'We only require the area, `A`, and the length of the kernel `LK` features,
    so remove the other columns:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们只需要面积`A`和内核长度`LK`特征，因此删除其他列：
- en: '[PRE2]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.9: The head after cleaning the data'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.9：清洗数据后的数据头'
- en: '](img/B15923_04_09.jpg)'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_09.jpg)'
- en: 'Figure 4.9: The head after cleaning the data'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.9：清洗数据后的数据头
- en: 'Visualize the dataset by plotting the `A` versus `LK` values:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制`A`与`LK`的值来可视化数据集：
- en: '[PRE3]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output is as follows:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.10: Plot of the data'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.10：数据的绘图'
- en: '](img/B15923_04_10.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_10.jpg)'
- en: 'Figure 4.10: Plot of the data'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.10：数据的绘图
- en: 'Compute the mean value using the `pandas` method:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`方法计算均值：
- en: '[PRE4]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The output is as follows:'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE5]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Compute the mean value using the `numpy` method:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`numpy`方法计算均值：
- en: '[PRE6]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output is as follows:'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE7]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Compute the standard deviation value using the `pandas` method:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`方法计算标准差值：
- en: '[PRE8]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Compute the standard deviation value using the `numpy` method:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`numpy`方法计算标准差值：
- en: '[PRE10]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE11]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Compute the variance values using the `pandas` method:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`方法计算方差值：
- en: '[PRE12]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE13]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Compute the variance values using the `numpy` method:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`numpy`方法计算方差值：
- en: '[PRE14]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE15]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Compute the covariance matrix using the `pandas` method:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pandas`方法计算协方差矩阵：
- en: '[PRE16]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.11: Covariance matrix using the pandas method'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.11：使用pandas方法的协方差矩阵'
- en: '](img/B15923_04_11.jpg)'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_11.jpg)'
- en: 'Figure 4.11: Covariance matrix using the pandas method'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.11：使用pandas方法的协方差矩阵
- en: 'Compute the covariance matrix using the `numpy` method:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`numpy`方法计算协方差矩阵：
- en: '[PRE17]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE18]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now that we know how to compute the foundational statistic values, we will turn
    our attention to the remaining components of PCA.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何计算基础的统计值，我们将把注意力转向PCA的其他组成部分。
- en: Note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2BHiLFz](https://packt.live/2BHiLFz).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看本节的源代码，请参考[https://packt.live/2BHiLFz](https://packt.live/2BHiLFz)。
- en: You can also run this example online at [https://packt.live/2O80UtW](https://packt.live/2O80UtW).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，访问[https://packt.live/2O80UtW](https://packt.live/2O80UtW)。
- en: Eigenvalues and Eigenvectors
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征值和特征向量
- en: 'The mathematical concept of eigenvalues and eigenvectors is a very important
    one in the fields of physics and engineering, and they also form the final steps
    in computing the principal components of a dataset. The exact mathematical definition
    of eigenvalues and eigenvectors is outside the scope of this book, as it is quite
    involved and requires a reasonable understanding of linear algebra. Any square
    matrix *A* of dimensions *n x n* has a vector, *x*, of shape *n x 1* in such a
    way that it satisfies the following relation:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值和特征向量是物理学和工程学领域中非常重要的数学概念，它们也是计算数据集主成分的最后步骤。特征值和特征向量的准确数学定义超出了本书的范围，因为它涉及较多的内容，并且需要对线性代数有一定的理解。任何一个*大小为n
    x n*的方阵*A*，都存在一个形状为*n x 1*的向量*x*，使得它满足以下关系：
- en: '![Figure 4.12: Equation representing PCA'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.12：表示PCA的方程'
- en: '](img/B15923_04_12.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_12.jpg)'
- en: 'Figure 4.12: Equation representing PCA'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.12：表示PCA的方程
- en: 'Here, the term ![C:\Users\user\Downloads\B15923_04_Formula_01.PNG](img/B15923_04_Formula_01.png)
    is a numerical value and denotes the eigenvalue, whereas *x* denotes the corresponding
    eigenvector. *N* denotes the order of the matrix, *A*. There will be exactly *n*
    eigenvalue and eigenvectors for matrix *A*. Without diving into the mathematical
    details of PCA, let''s take a look at another way of representing the preceding
    equation as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，术语![C:\Users\user\Downloads\B15923_04_Formula_01.PNG](img/B15923_04_Formula_01.png)是一个数值，表示特征值，而*x*表示相应的特征向量。*N*表示矩阵*A*的阶数。矩阵*A*将有*n*个特征值和特征向量。我们不深入探讨PCA的数学细节，接下来让我们看一下另一种表示前述方程的方式，如下所示：
- en: '![Figure 4.13: Alternative equation representing PCA'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.13：表示PCA的替代方程式'
- en: '](img/B15923_04_13.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_13.jpg)'
- en: 'Figure 4.13: Alternative equation representing PCA'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：表示PCA的替代方程式
- en: 'Putting this simply into the context of PCA, we can derive the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，将其应用于PCA，我们可以推导出以下结论：
- en: '**Covariance Matrix** (*A*): As discussed in the preceding section, matrix
    *A* should be a square matrix before it can undergo eigenvalue decomposition.
    Since, in the case of our dataset, it has rows greater than the number of columns
    (let the shape of dataset be *m x n* where *m* is the number of rows and *n* is
    the number of columns). Therefore, we cannot perform eigenvalue decomposition
    directly. To perform eigenvalue decomposition on a rectangular matrix, it is first
    converted to a square matrix by computing its covariance matrix. A covariance
    matrix has a shape of *n x n*, that is, it is a square matrix of order ''*n*''.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**协方差矩阵** (*A*)：如前一节所述，在进行特征值分解之前，矩阵*A*应该是方阵。由于在我们的数据集中，行数大于列数（假设数据集的形状是*m
    x n*，其中*m*是行数，*n*是列数），因此我们无法直接进行特征值分解。为了对矩形矩阵执行特征值分解，首先需要通过计算其协方差矩阵将其转换为方阵。协方差矩阵的形状为*n
    x n*，即它是一个*n*阶的方阵。'
- en: '**Eigenvectors** (*U*) are the components contributing information to the dataset
    as described in the first paragraph of this section on principal components called
    eigenvectors. Each eigenvector describes some amount of variability within the
    dataset. This variability is indicated by the corresponding eigenvalue. The larger
    the eigenvalue, the greater its contribution. An eigenvectors matrix has a shape
    of *n x n*.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征向量** (*U*) 是对数据集贡献信息的组成部分，如本节第一段中所述的主成分称为特征向量。每个特征向量描述数据集中的某些变化性。这种变化性由相应的特征值表示。特征值越大，贡献越大。特征向量矩阵的形状为*n
    x n*。'
- en: '**Eigenvalues** (![C:\Users\user\Downloads\B15923_04_Formula_02.PNG](img/B15923_04_Formula_02.png))
    are the individual values that describe how much contribution each eigenvector
    provides to the dataset. As described previously, the single eigenvector that
    describes the largest contribution is referred to as the principal component,
    and, as such, will have the largest eigenvalue. Accordingly, the eigenvector with
    the smallest eigenvalue contributes the least amount of variance or information
    to the data. Eigenvalues are a diagonal matrix, which has the diagonal elements
    representing eigenvalues.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征值** (![C:\Users\user\Downloads\B15923_04_Formula_02.PNG](img/B15923_04_Formula_02.png))
    是描述每个特征向量对数据集贡献大小的单个值。如前所述，描述最大贡献的特征向量称为主成分，因此具有最大的特征值。相应地，具有最小特征值的特征向量对数据的方差或信息贡献最小。特征值是对角矩阵，其中对角元素表示特征值。'
- en: Please note that even the SVD of a covariance matrix of data produces eigenvalue
    decomposition, which we will see in *Exercise 4.04*, *scikit-learn PCA*. However,
    SVD uses a different process for the decomposition of the matrix. Remember that
    eigenvalue decomposition can be done for a square matrix only, whereas SVD can
    be done for a rectangular matrix as well.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使是数据的协方差矩阵的SVD也会产生特征值分解，我们将在*练习4.04*中看到，*scikit-learn PCA*。然而，SVD使用不同的过程来分解矩阵。请记住，特征值分解仅适用于方阵，而SVD也可以应用于矩阵。
- en: Note
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '**Square matrix**: A square matrix has the same number of rows and columns.
    The number of rows in a square matrix is called the order of the matrix. A matrix
    that has an unequal number of rows and columns is known as a rectangular matrix.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**方阵**：方阵的行数和列数相等。方阵的行数被称为矩阵的阶数。行列数不等的矩阵称为矩形矩阵。'
- en: '**Diagonal matrix**: A diagonal matrix has all non-diagonal elements as zero.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '**对角矩阵**：对角矩阵的非对角元素全为零。'
- en: 'Exercise 4.02: Computing Eigenvalues and Eigenvectors'
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.02：计算特征值和特征向量
- en: 'As discussed previously, deriving and computing the eigenvalues and eigenvectors
    manually is a little involved and is not within the scope of this book. Thankfully,
    `numpy` provides all the functionality for us to compute these values. Again,
    we will use the Seeds dataset for this example:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，手动推导和计算特征值和特征向量有些复杂，超出了本书的范围。幸运的是，`numpy`为我们提供了计算这些值的所有功能。再次，我们将使用Seeds数据集作为示例：
- en: Note
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/34gOQ0B](https://packt.live/34gOQ0B).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。（UCI
    机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州尔湾：加利福尼亚大学信息与计算机科学学院。）引用：贡献者由衷感谢波兰科学院农业物理研究所（位于卢布林）对其工作的支持。该数据集还可以从[https://packt.live/34gOQ0B](https://packt.live/34gOQ0B)下载。
- en: 'Import the `pandas` and `numpy` packages:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas` 和 `numpy` 包：
- en: '[PRE19]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Load the dataset:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE20]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.14: The first five rows of the dataset'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.14：数据集的前五行'
- en: '](img/B15923_04_14.jpg)'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_14.jpg)'
- en: 'Figure 4.14: The first five rows of the dataset'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.14：数据集的前五行
- en: 'Again, we only require the `A` and `LK` features, so remove the other columns:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次，我们只需要 `A` 和 `LK` 特征，所以删除其他列：
- en: '[PRE21]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.15: The area and length of the kernel features'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.15：核特征的面积和长度'
- en: '](img/B15923_04_15.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_15.jpg)'
- en: 'Figure 4.15: The area and length of the kernel features'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.15：核特征的面积和长度
- en: 'From the linear algebra module of `numpy`, use the `eig` function to compute
    the `eigenvalues` and `eigenvectors` characteristic vectors. Note the use of the
    covariance matrix of data here:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `numpy` 的线性代数模块中，使用 `eig` 函数计算 `eigenvalues` 和 `eigenvectors` 特征向量。注意此处使用了数据的协方差矩阵：
- en: '[PRE22]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The `numpy` function, `cov`, can be used to calculate the covariance matrix
    of data. It produces a square matrix of order equal to the number of features
    of data.
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`numpy` 函数 `cov` 可以用来计算数据的协方差矩阵。它产生一个与数据特征数相等的方阵。'
- en: 'Look at the eigenvalues; we can see that the first value is the largest, so
    the first eigenvector contributes the most information:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看一下特征值；我们可以看到第一个值是最大的，所以第一个特征向量贡献了最多的信息：
- en: '[PRE23]'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is handy to look at eigenvalues as a percentage of the total variance within
    the dataset. We will use a cumulative sum function to do this:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察特征值作为数据集总方差百分比是很方便的。我们将使用一个累积和函数来实现这一点：
- en: '[PRE25]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE26]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Divide by the last or maximum value to convert eigenvalues into a percentage:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 除以最后一个或最大值，将特征值转换为百分比：
- en: '[PRE27]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We can see here that the first (or principal) component comprises 99% of the
    variation within the data, and, therefore, most of the information.
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们可以看到，第一个（或主）成分包含了数据中99%的变化，因此，包含了大部分信息。
- en: 'Now, let''s take a look at `eigenvectors`:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来看看 `eigenvectors`：
- en: '[PRE29]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'A section of the output is as follows:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的部分如下：
- en: '[PRE30]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Confirm that the shape of the eigenvector matrix is in (`n x n`) format; that
    is, `2` x `2`:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确认特征向量矩阵的形状是（`n x n`）格式；即 `2` x `2`：
- en: '[PRE31]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'So, from the eigenvalues, we saw that the principal component was the first
    eigenvector. Look at the values for the first eigenvector:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所以，从特征值中，我们看到主成分是第一个特征向量。看看第一个特征向量的值：
- en: '[PRE33]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: We have decomposed the dataset down into the principal components, and, using
    the eigenvectors, we can further reduce the dimensionality of the available data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将数据集分解为主成分，并通过特征向量进一步降低了数据的维度。
- en: Note
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3e5x3N3](https://packt.live/3e5x3N3).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/3e5x3N3](https://packt.live/3e5x3N3)。
- en: You can also run this example online at [https://packt.live/3f5Skrk](https://packt.live/3f5Skrk).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个例子，访问[https://packt.live/3f5Skrk](https://packt.live/3f5Skrk)。
- en: In later examples, we will consider PCA and apply this technique to an example
    dataset.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续示例中，我们将考虑 PCA，并将此技术应用于一个示例数据集。
- en: The Process of PCA
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 过程
- en: Now, we have all of the pieces ready to complete PCA in order to reduce the
    number of dimensions in a dataset.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好所有的部分来完成 PCA，以减少数据集中的维度。
- en: 'The overall algorithm for completing PCA is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 完成 PCA 的整体算法如下：
- en: Import the required Python packages (`numpy` and `pandas`).
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的 Python 包（`numpy` 和 `pandas`）。
- en: Load the entire dataset.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载整个数据集。
- en: From the available data, select the features that you wish to use in dimensionality
    reduction.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从可用的数据中，选择您希望在降维中使用的特征。
- en: Note
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: If there is a significant difference in the scale between the features of the
    dataset; for example, one feature ranges in values between 0 and 1, and another
    between 100 and 1,000, you may need to normalize one of the features, as such
    differences in magnitude can eliminate the effect of the smaller features. In
    such a situation, you may need to divide the larger feature by its maximum value.
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果数据集中的特征之间存在显著的规模差异；例如，一个特征的值范围在 0 和 1 之间，另一个特征的值范围在 100 和 1000 之间，您可能需要对其中一个特征进行归一化，因为这种量级差异可能会消除较小特征的影响。在这种情况下，您可能需要将较大的特征除以其最大值。
- en: 'As an example, take a look at this:'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 作为示例，看看这个：
- en: '`x1 = [0.1, 0.23, 0.54, 0.76, 0.78]`'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x1 = [0.1, 0.23, 0.54, 0.76, 0.78]`'
- en: '`x2 = [121, 125, 167, 104, 192]`'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x2 = [121, 125, 167, 104, 192]`'
- en: '`# Normalise x2 to be between 0 and 1`'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`# 将 x2 归一化到 0 到 1 之间`'
- en: '`x2 = (x2-np.min(x2)) / (np.max(x2)-np.min(x2))`'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`x2 = (x2-np.min(x2)) / (np.max(x2)-np.min(x2))`'
- en: Compute the `covariance` matrix of the selected (and possibly normalized) data.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所选（并可能归一化后的）数据的`协方差`矩阵。
- en: Compute the eigenvalues and eigenvectors of the `covariance` matrix.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`协方差`矩阵的特征值和特征向量。
- en: Sort the eigenvalues (and corresponding eigenvectors) from the highest to the
    lowest.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将特征值（及其对应的特征向量）从大到小排序。
- en: Compute the eigenvalues as a percentage of the total variance within the dataset.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算特征值占数据集中总方差的百分比。
- en: Select the number of eigenvalues and corresponding eigenvectors. They will be
    required to comprise a predetermined value of a minimum composition variance.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择特征值和对应特征向量的数量。它们将被要求组成一个预定值的最小成分方差。
- en: Note
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: At this stage, the sorted eigenvalues represent a percentage of the total variance
    within the dataset. As such, we can use these values to select the number of eigenvectors
    required, either for the problem being solved or to sufficiently reduce the size
    of the dataset being applied to the model. For example, say that we required at
    least 90% of the variance to be accounted for within the output of PCA. We would
    then select the number of eigenvalues (and corresponding eigenvectors) that comprise
    at least 90% of the variance.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在此阶段，排序后的特征值表示数据集总方差的百分比。因此，我们可以使用这些值来选择所需的特征向量数量，无论是用于解决问题，还是充分减少应用于模型的数据集的大小。例如，假设我们要求在
    PCA 的输出中至少占有 90% 的方差。我们就会选择那些至少占有 90% 方差的特征值（及其对应的特征向量）。
- en: Multiply the dataset by the selected eigenvectors and you have completed a PCA,
    thereby reducing the number of features representing the data.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集与选定的特征向量相乘，您就完成了 PCA，从而减少了表示数据的特征数量。
- en: Plot the result.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制结果。
- en: Before moving on to the next exercise, note that **transpose** is a term from
    linear algebra that means to swap the rows with the columns and vice versa. Let's
    say we
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行下一个练习之前，请注意，**转置**是线性代数中的一个术语，指的是将矩阵的行和列互换。假设我们
- en: have a matrix of `X=[1, 2, 3]`, then, the transpose of *X* would be ![C:\Users\user\Downloads\B15923_04_Formula_03.PNG](img/B15923_04_Formula_03.png).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个矩阵`X=[1, 2, 3]`，那么，*X*的转置就是![C:\Users\user\Downloads\B15923_04_Formula_03.PNG](img/B15923_04_Formula_03.png)。
- en: 'Exercise 4.03: Manually Executing PCA'
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.03：手动执行 PCA
- en: 'For this exercise, we will be completing PCA manually, again using the Seeds
    dataset. For this example, we want to sufficiently reduce the number of dimensions
    within the dataset to comprise at least 75% of the available variance:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本练习，我们将手动完成 PCA，再次使用 Seeds 数据集。对于这个例子，我们希望足够减少数据集中的维度，以包含至少 75% 的可用方差：
- en: Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/2Xe7cxO](https://packt.live/2Xe7cxO).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。
    （UCI 机器学习数据集 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学，信息与计算机科学学院。）引用：贡献者感谢波兰科学院农物理研究所对其工作的支持。该数据集也可以从[https://packt.live/2Xe7cxO](https://packt.live/2Xe7cxO)下载。
- en: 'Import the `pandas` and `numpy` packages:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`numpy`包：
- en: '[PRE35]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Load the dataset:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE36]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.16: The first five rows of the dataset'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.16：数据集的前五行'
- en: '](img/B15923_04_16.jpg)'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_16.jpg)'
- en: 'Figure 4.16: The first five rows of the dataset'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.16：数据集的前五行
- en: 'Again, we only require the `A` and `LK` features, so remove the other columns.
    In this example, we are not normalizing the selected dataset:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们只需要`A`和`LK`特征，因此删除其他列。在此示例中，我们没有对所选数据集进行归一化：
- en: '[PRE37]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.17: The area and length of the kernel features'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.17：核特征的面积和长度'
- en: '](img/B15923_04_17.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_17.jpg)'
- en: 'Figure 4.17: The area and length of the kernel features'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.17：核特征的面积和长度
- en: 'Compute the `covariance` matrix for the selected data. Note that we need to
    take the transpose of the `covariance` matrix to ensure that it is based on the
    number of features (2) and not samples (150):'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所选数据的`协方差`矩阵。请注意，我们需要对`协方差`矩阵进行转置，以确保其基于特征数量（2），而不是样本数量（150）：
- en: '[PRE38]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE39]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Compute the eigenvectors and eigenvalues for the covariance matrix, Again,
    use the `full_matrices` function argument:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算协方差矩阵的特征向量和特征值。同样，使用`full_matrices`函数参数：
- en: '[PRE40]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Eigenvalues are returned, sorted from the highest value to the lowest:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征值按从大到小排序返回：
- en: '[PRE41]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The output is as follows:'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE42]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Eigenvectors are returned as a matrix:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征向量作为矩阵返回：
- en: '[PRE43]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows:'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE44]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Compute the eigenvalues as a percentage of the variance within the dataset:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据集内方差占比的特征值：
- en: '[PRE45]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE46]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'As per the introduction to the exercise, we need to describe the data with
    at least 75% of the available variance. As per *Step 7*, the principal component
    comprises 99% of the available variance. As such, we require only the principal
    component from the dataset. What are the principal components? Let''s take a look:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据练习的介绍，我们需要描述至少包含 75% 可用方差的数据。如*步骤7*所述，主成分包含了 99% 的可用方差。因此，我们只需要数据集中的主成分。什么是主成分？让我们来看看：
- en: '[PRE47]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE48]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now, we can apply the dimensionality reduction process. Execute a matrix multiplication
    of the principal component with the transpose of the dataset.
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们可以应用降维过程。执行主成分与数据集转置的矩阵乘法。
- en: Note
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The dimensionality reduction process is a matrix multiplication of the selected
    eigenvectors and the data to be transformed.
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 降维过程是将所选特征向量与需要转换的数据进行矩阵乘法。
- en: 'Without taking the transpose of the `df.values` matrix, multiplication could
    not occur:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果不对`df.values`矩阵进行转置，将无法进行乘法运算：
- en: '[PRE49]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'A section of the output is as follows:'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![Figure 4.18: The result of matrix multiplication'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.18：矩阵乘法的结果'
- en: '](img/B15923_04_18.jpg)'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_18.jpg)'
- en: 'Figure 4.18: The result of matrix multiplication'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.18：矩阵乘法的结果
- en: Note
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The transpose of the dataset is required to execute matrix multiplication, as
    the **inner dimensions of the matrix must be the same** for matrix multiplication
    to occur. For **A** ("A dot B") to be valid, **A** must have the shape of *m x
    n*, and **B** must have the shape of *n x p*. In this example, the inner dimensions
    of **A** and **B** are both *n*. The resulting matrix would have dimensions of
    *m x p*.
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了执行矩阵乘法，需要对数据集进行转置，因为**矩阵的内维度必须相同**才能进行矩阵乘法。对于**A**（“A 点 B”）有效，**A**必须具有*m
    x n*的形状，**B**必须具有*n x p*的形状。在此示例中，**A**和**B**的内维度均为*n*。得到的矩阵将具有*m x p*的维度。
- en: In the following example, the output of the PCA is a single-column, 210-sample
    dataset. As such, we have just reduced the size of the initial dataset by half,
    comprising approximately 99% of the variance within the data.
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下示例中，PCA的输出是一个单列的210样本数据集。因此，我们已将初始数据集的大小减少了一半，涵盖了大约99%的数据方差。
- en: 'Plot the values of the principal component:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制主成分的值：
- en: '[PRE50]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows, and shows the new component values of the 210-sample
    dataset, as seen printed in the preceding step:'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下，显示了210样本数据集的新组件值，正如前一步中打印出来的所示：
- en: '![Figure 4.19: The Seeds dataset transformed using a manual PCA'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.19：使用手动PCA转换的Seeds数据集'
- en: '](img/B15923_04_19.jpg)'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_19.jpg)'
- en: 'Figure 4.19: The Seeds dataset transformed using a manual PCA'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.19：使用手动PCA转换的Seeds数据集
- en: In this exercise, we simply computed the covariance matrix of the dataset without
    applying any transformations to the dataset beforehand. If the two features have
    roughly the same mean and standard deviation, this is perfectly fine. However,
    if one feature is much larger in value (and has a somewhat different mean) than
    the other, then this feature may dominate the other when decomposing into components.
    This could have the effect of removing the information provided by the smaller
    feature altogether. One simple normalization technique before computing the covariance
    matrix would be to subtract the respective means from the features, thus centering
    the dataset around zero. We will demonstrate this in *Exercise 4.05*, *Visualizing
    Variance Reduction with Manual PCA*.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们仅计算了数据集的协方差矩阵，而未事先对数据集进行任何变换。如果两个特征的均值和标准差大致相同，这是完全可以的。然而，如果一个特征的值远大于另一个特征（并且均值有所不同），那么在分解成组件时，该特征可能会主导另一个特征。这可能会导致小特征所提供的信息完全丢失。一个简单的归一化技术是在计算协方差矩阵之前从特征中减去各自的均值，从而使数据集以零为中心。我们将在*练习
    4.05*，*使用手动PCA可视化方差减少*中演示这一点。
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3fa8X57](https://packt.live/3fa8X57).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参考[https://packt.live/3fa8X57](https://packt.live/3fa8X57)。
- en: You can also run this example online at [https://packt.live/3iOvg2P](https://packt.live/3iOvg2P).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3iOvg2P](https://packt.live/3iOvg2P)上在线运行此示例。
- en: 'Exercise 4.04: scikit-learn PCA'
  id: totrans-291
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.04：scikit-learn PCA
- en: 'Typically, we will not complete PCA manually, especially when scikit-learn
    provides an optimized API with convenient methods that allow us to easily transform
    the data to and from the reduced-dimensional space. In this exercise, we will
    look at using a scikit-learn PCA on the Seeds dataset in more detail:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不会手动完成PCA，特别是当scikit-learn提供了一个优化过的API，包含方便的方法，可以让我们轻松地将数据转换到降维空间并从中还原。在本练习中，我们将更详细地使用scikit-learn的PCA来处理Seeds数据集：
- en: Note
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    also can be downloaded from [https://packt.live/2Ri6VGk](https://packt.live/2Ri6VGk).'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。（UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学欧文分校，信息与计算机科学学院。）引用：贡献者在此感谢波兰科学院农物理研究所（位于卢布林）对其工作的支持。该数据集也可以从[https://packt.live/2Ri6VGk](https://packt.live/2Ri6VGk)下载。
- en: 'Import the `pandas`, `numpy`, and `PCA` modules from the `sklearn` packages:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn`包中导入`pandas`、`numpy`和`PCA`模块：
- en: '[PRE51]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Load the dataset:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE52]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.20: The first five rows of the dataset'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.20：数据集的前五行'
- en: '](img/B15923_04_20.jpg)'
  id: totrans-301
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_20.jpg)'
- en: 'Figure 4.20: The first five rows of the dataset'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.20：数据集的前五行
- en: 'Again, we only require the `A` and `LK` features, so remove the other columns.
    In this example, we are not normalizing the selected dataset:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次说明，我们只需要`A`和`LK`特征，因此需要删除其他列。在此示例中，我们不会对选择的数据集进行归一化：
- en: '[PRE53]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output is as follows:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.21: The area and length of the kernel features'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.21：种子特征的面积和长度'
- en: '](img/B15923_04_21.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_21.jpg)'
- en: 'Figure 4.21: The area and length of the kernel features'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.21：种子特征的面积和长度
- en: 'Fit the data to a scikit-learn PCA model of the covariance data. Using the
    default values, as we have here, produces the maximum number of eigenvalues and
    eigenvectors possible for the dataset:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据拟合到scikit-learn的PCA模型中，使用的是协方差数据。使用我们在此处的默认值，将为数据集生成最大数量的特征值和特征向量：
- en: '[PRE54]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE55]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Here, `copy` indicates that the data fit within the model is copied before any
    calculations are applied. If `copy` was set to `False`, data passed to PCA is
    overwritten. `iterated_power` shows that the `A` and `LK` features are the number
    of principal components to keep. The default value is `None`, which selects the
    number of components as one less than the minimum of either the number of samples
    or the number of features. `random_state` allows the user to specify a seed for
    the random number generator used by the SVD solver. `svd_solver` specifies the
    SVD solver to be used during PCA. `tol` is the tolerance value used by the SVD
    solver. With `whiten`, the component vectors are multiplied by the square root
    of the number of samples. This will remove some information but can improve the
    performance of some downstream estimators.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`copy`表示在应用任何计算之前，数据适配到模型中时会被复制。如果将`copy`设置为`False`，则传递给PCA的数据将被覆盖。`iterated_power`表示`A`和`LK`特征是要保留的主成分的数量。默认值为`None`，它选择的组件数为样本数和特征数中较小值减一。`random_state`允许用户为SVD求解器使用的随机数生成器指定种子。`svd_solver`指定在PCA过程中使用的SVD求解器。`tol`是SVD求解器使用的容忍度值。使用`whiten`时，组件向量会乘以样本数的平方根。这会移除一些信息，但可以提高某些下游估计器的性能。
- en: 'The percentage of variance described by the components (eigenvalues) is contained
    within the `explained_variance_ratio_` property. Display the values for `explained_variance_ratio_`:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由组件（特征值）描述的方差百分比包含在`explained_variance_ratio_`属性中。显示`explained_variance_ratio_`的值：
- en: '[PRE56]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE57]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Display the eigenvectors via the `components_` property:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过`components_`属性显示特征向量：
- en: '[PRE58]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE59]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'In this exercise, we will again only use the primary component, so we will
    create a new `PCA` model, this time specifying the number of components (eigenvectors/eigenvalues)
    to be `1`:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将仅使用主成分，因此我们将创建一个新的`PCA`模型，这次指定组件数（特征向量/特征值）为`1`：
- en: '[PRE60]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Use the `fit` method to fit the `covariance` matrix to the `PCA` model and
    generate the corresponding eigenvalues/eigenvectors:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`fit`方法将`covariance`矩阵拟合到`PCA`模型中，并生成相应的特征值/特征向量：
- en: '[PRE61]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'The output is as follows:'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE62]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The model is fitted using a number of default parameters, as listed in the preceding
    output. `copy = True` is the data provided to the `fit` method, which is copied
    before PCA is applied. `iterated_power='auto'` is used to define the number of
    iterations by the internal SVD solver. `n_components=1` specifies that the PCA
    model is to return only the principal component. `random_state=None` specifies
    the random number generator to be used by the internal SVD solver if required.
    `svd_solver='auto'` is the type of SVD solver used. `tol=0.0` is the tolerance
    value for the SVD solver. `whiten=False` specifies that the eigenvectors are not
    to be modified. If set to `True`, whitening modifies the components further by
    multiplying by the square root of the number of samples and dividing by the singular
    values. This can help to improve the performance of later algorithm steps.
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型是使用多个默认参数拟合的，如前面输出所示。`copy = True`表示传递给`fit`方法的数据会在应用PCA之前被复制。`iterated_power='auto'`用于定义内部SVD求解器的迭代次数。`n_components=1`指定PCA模型只返回一个主成分。`random_state=None`指定在需要时由内部SVD求解器使用的随机数生成器。`svd_solver='auto'`指定使用的SVD求解器类型。`tol=0.0`是SVD求解器的容忍度值。`whiten=False`表示特征向量不会被修改。如果设置为`True`，则白化会通过乘以样本数的平方根并除以奇异值进一步修改组件。这有助于提高后续算法步骤的性能。
- en: Typically, you will not need to worry about adjusting any of these parameters,
    other than the number of components (`n_components`), which you can pass while
    declaring the PCA object as `model = PCA(n_components=1)`.
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常，除了`n_components`（组件数量）外，您无需担心调整任何其他参数，`n_components`可以在声明PCA对象时传递，例如`model
    = PCA(n_components=1)`。
- en: 'Display the eigenvectors using the `components_` property:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`components_`属性显示特征向量：
- en: '[PRE63]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output is as follows:'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE64]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Transform the Seeds dataset into the lower space by using the `fit_transform`
    method of the model on the dataset. Assign the transformed values to the `data_t`
    variable:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型的`fit_transform`方法将Seeds数据集转换到低维空间。将转换后的值赋给`data_t`变量：
- en: '[PRE65]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Plot the transformed values to visualize the result:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制转换后的值以可视化结果：
- en: '[PRE66]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output is as follows:'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.22: The seeds dataset transformed using the scikit-learn PCA'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.22：使用scikit-learn PCA转换后的seeds数据集'
- en: '](img/B15923_04_22.jpg)'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_22.jpg)'
- en: 'Figure 4.22: The seeds dataset transformed using the scikit-learn PCA'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.22：使用scikit-learn PCA转换后的seeds数据集
- en: You have just reduced the dimensionality of the Seeds dataset using manual PCA,
    along with the scikit-learn API. But before we celebrate too early, compare *Figure
    4.19* and *Figure 4.22*; these plots should be identical, shouldn't they? We used
    two separate methods to complete a PCA on the same dataset and selected the principal
    component for both. In the next activity, we will investigate why there are differences
    between the two.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚使用手动PCA和scikit-learn API将Seeds数据集的维度降低了。但是，在我们过早庆祝之前，请比较*图4.19*和*图4.22*；这两张图应该是相同的，对吧？我们使用了两种不同的方法在相同的数据集上完成PCA，并且都选择了主成分。在接下来的活动中，我们将探讨为什么这两者之间会有差异。
- en: Note
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZQV85c](https://packt.live/2ZQV85c).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定章节的源代码，请参考[https://packt.live/2ZQV85c](https://packt.live/2ZQV85c)。
- en: You can also run this example online at [https://packt.live/2VSG99R](https://packt.live/2VSG99R).
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2VSG99R](https://packt.live/2VSG99R)上在线运行此示例。
- en: 'Activity 4.01: Manual PCA versus scikit-learn'
  id: totrans-346
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动4.01：手动PCA与scikit-learn
- en: Suppose that you have been asked to port some legacy code from an older application
    executing PCA manually to a newer application that uses scikit-learn. During the
    porting process, you observe some differences between the output of the manual
    PCA and that of your port. Why is there a difference between the output of our
    manual PCA and scikit-learn? Compare the results of the two approaches on the
    Seeds dataset. What are the differences between them?
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你被要求将一个旧应用程序中手动执行PCA的遗留代码移植到一个使用scikit-learn的新应用程序中。在移植过程中，你注意到手动PCA和移植后输出之间有一些差异。为什么我们的手动PCA和scikit-learn输出之间会有差异？比较两种方法在Seeds数据集上的结果。它们之间的差异是什么？
- en: 'The aim of this activity is to truly dive into understanding how PCA works
    by doing it from scratch, and then comparing your implementation against the one
    included in scikit-learn to see whether there are any major differences:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目的是通过从头开始实现PCA，深入理解其工作原理，然后将你的实现与scikit-learn中包含的实现进行比较，以查看是否存在重大差异：
- en: Note
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/2JIH1qT](https://packt.live/2JIH1qT).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。
    (UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学欧文分校信息与计算机科学学院)。引用：贡献者感谢波兰科学院农物理研究所（位于卢布林）对他们工作的支持。该数据集也可以从[https://packt.live/2JIH1qT](https://packt.live/2JIH1qT)下载。
- en: Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the scikit-learn
    `PCA` model.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`和`matplotlib`绘图库，以及scikit-learn的`PCA`模型。
- en: Load the dataset and select only the kernel features as per the previous exercises.
    Display the first five rows of the data.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集，并根据之前的练习选择仅包含核心特征的数据。显示数据的前五行。
- en: Compute the `covariance` matrix for the data.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据的`covariance`矩阵。
- en: Transform the data using the scikit-learn API and only the first principal component.
    Store the transformed data in the `sklearn_pca` variable.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用scikit-learn API并仅使用第一个主成分转换数据。将转换后的数据存储在`sklearn_pca`变量中。
- en: Transform the data using the manual PCA and only the first principal component.
    Store the transformed data in the `manual_pca` variable.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用手动PCA并仅使用第一个主成分来转换数据。将转换后的数据存储在`manual_pca`变量中。
- en: Plot the `sklearn_pca` and `manual_pca` values on the same plot to visualize
    the difference.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that the two plots look almost identical, but with some key differences.
    What are these differences?
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，这两张图几乎看起来相同，但有一些关键区别。这些区别是什么？
- en: See whether you can modify the output of the manual PCA process to bring it
    in line with the scikit-learn version.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看是否可以修改手动PCA过程的输出，使其与scikit-learn版本一致。
- en: Note
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这张图将展示两种方法完成的降维结果实际上是相同的。
- en: 'Hint: The scikit-learn API subtracts the mean of the data prior to the transform.'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提示：scikit-learn的API在变换前会减去数据的均值。
- en: '**Expected output**: By the end of this activity, you will have transformed
    the dataset using both the manual and scikit-learn PCA methods. You will have
    produced a plot demonstrating that the two reduced datasets are, in fact, identical,
    and you should have an understanding of why they initially looked quite different.
    The final plot should look similar to the following:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sklearn_pca`和`manual_pca`的值绘制在同一图表上，以可视化差异。
- en: '![Figure 4.23: The expected final plot'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '![图4.23：预期的最终图](img/B15923_04_23.jpg)'
- en: '](img/B15923_04_23.jpg)'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这两张图几乎完全相同，但有一些关键的区别。这些区别是什么？
- en: 'Figure 4.23: The expected final plot'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.23：预期的最终图
- en: This plot will demonstrate that the dimensionality reduction completed by the
    two methods are, in fact, the same.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图将演示两种方法完成的降维结果实际上是相同的。
- en: Note
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 437.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第437页找到。
- en: Restoring the Compressed Dataset
  id: totrans-368
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恢复压缩后的数据集
- en: Now that we have covered a few different examples of transforming a dataset
    into a lower-dimensional space, we should consider what practical effect this
    transformation has had on the data. Using PCA as a preprocessing step to condense
    the number of features in the data will result in some of the variance being discarded.
    The following exercise will walk us through this process so that we can see how
    much information has been discarded by the transformation.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了将数据集转换为低维空间的几个不同示例，我们应该考虑这种转换对数据产生了什么实际影响。使用PCA作为预处理步骤来减少数据中的特征数量将导致部分方差被丢弃。以下练习将引导我们完成这一过程，让我们看到转换丢弃了多少信息。
- en: 'Exercise 4.05: Visualizing Variance Reduction with Manual PCA'
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习4.05：使用手动PCA可视化方差减少
- en: 'One of the most important aspects of dimensionality reduction is understanding
    how much information has been removed from the dataset as a result of the dimensionality
    reduction process. Removing too much information will add additional challenges
    to later processing, while not removing enough defeats the purpose of PCA or other
    techniques. In this exercise, we will visualize the amount of information that
    has been removed from the Seeds dataset as a result of PCA:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的一个最重要方面是理解由于降维过程从数据集中去除了多少信息。去除过多的信息将给后续处理带来额外的挑战，而去除的信息不足则违背了PCA或其他技术的目的。在本练习中，我们将可视化通过PCA从Seeds数据集中去除的信息量：
- en: Note
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    also can be downloaded from [https://packt.live/2RhnDFS](https://packt.live/2RhnDFS).'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.24：核特征
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`、`numpy`和`matplotlib`绘图库：
- en: '[PRE67]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Read in the `wheat kernel` features from the Seeds dataset:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Seeds数据集中读取`wheat kernel`特征：
- en: '[PRE68]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'The output is as follows:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.24: Kernel features'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.24：核特征](img/B15923_04_24.jpg)'
- en: '](img/B15923_04_24.jpg)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。
    （UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学尔湾分校信息与计算机科学学院。）引用：贡献者感谢波兰科学院农业物理研究所（位于卢布林）对他们工作的支持。该数据集也可以从[https://packt.live/2RhnDFS](https://packt.live/2RhnDFS)下载。
- en: 'Figure 4.24: Kernel features'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**预期输出**：在本活动结束时，您将使用手动PCA方法和scikit-learn PCA方法分别转换数据集。您将生成一个图表，展示这两个降维后的数据集实际上是相同的，并且您应该能够理解它们最初看起来非常不同的原因。最终图应类似于以下内容：'
- en: Center the dataset around zero by subtracting the respective means.
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过减去各自的均值，将数据集居中于零。
- en: '[PRE69]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'The output is as follows:'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE70]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'To calculate the data and print the results, use the following code:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算数据并打印结果，使用以下代码：
- en: '[PRE71]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'A section of the output is as follows:'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![Figure 4.25: Section of the output'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.25：输出的一部分'
- en: '](img/B15923_04_25.jpg)'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_25.jpg)'
- en: 'Figure 4.25: Section of the output'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.25：输出的一部分
- en: 'Use manual PCA to transform the data on the basis of the first principal component:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用手动PCA对数据进行转换，基于第一个主成分：
- en: '[PRE72]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output is as follows:'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE73]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Transform the data into the lower-dimensional space by doing a dot product
    of the preceding `P` with a transposed version of the data matrix:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将之前的`P`与数据矩阵的转置版本进行点积，将数据转换为低维空间：
- en: '[PRE74]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Reshape the principal components for later use:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新塑造主成分以供后续使用：
- en: '[PRE75]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'To compute the inverse transform of the reduced dataset, we need to restore
    the selected eigenvectors to the higher-dimensional space. To do this, we will
    invert the matrix. Matrix inversion is another linear algebra technique that we
    will only cover very briefly. A square matrix, *A*, is said to be invertible if
    there is another square matrix, *B*, and if *AB=BA=I*, where *I* is a special
    matrix known as an identity matrix, consisting of values of `1` only through the
    center diagonal:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要计算降维数据集的逆转换，我们需要将选定的特征向量恢复到高维空间。为此，我们将对矩阵进行求逆。矩阵求逆是另一种线性代数技术，我们将只简要介绍。一个方阵，*A*，如果存在另一个方阵*B*，并且*AB=BA=I*，其中*I*是一个特殊的矩阵，称为单位矩阵，它的对角线只有值`1`：
- en: '[PRE76]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'The output is as follows:'
  id: totrans-402
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE77]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Prepare the transformed data for use in the matrix multiplication:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备转换后的数据以供矩阵乘法使用：
- en: '[PRE78]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算降维数据的逆转换并绘制结果，以可视化去除数据方差的效果：
- en: '[PRE79]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'A section of the output is as follows:'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![Figure 4.26: The inverse transform of the reduced data'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.26：降维数据的逆转换'
- en: '](img/B15923_04_26.jpg)'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_26.jpg)'
- en: 'Figure 4.26: The inverse transform of the reduced data'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.26：降维数据的逆转换
- en: 'Add the `means` array back to the transformed data:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`means`数组添加回转换后的数据：
- en: '[PRE80]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Visualize the result by plotting the original and the transformed datasets:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过绘制原始数据集和转换后数据集的图形来可视化结果：
- en: '[PRE81]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'The output is as follows:'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.27: The inverse transform after removing variance'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.27：去除方差后的逆转换'
- en: '](img/B15923_04_27.jpg)'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_27.jpg)'
- en: 'Figure 4.27: The inverse transform after removing variance'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.27：去除方差后的逆转换
- en: 'There are only two components of variation in this dataset. If we do not remove
    any of the components, what will be the result of the inverse transform? Again,
    transform the data into the lower-dimensional space, but this time, use all of
    the eigenvectors:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该数据集只有两个变异成分。如果我们不移除任何成分，逆转换的结果会是什么？再次将数据转换为低维空间，但这次使用所有的特征向量：
- en: '[PRE82]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Transpose `data_transformed` to put it in the correct shape for matrix multiplication:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转置`data_transformed`，以便将其放入适合矩阵乘法的正确形状：
- en: '[PRE83]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Now, restore the data back to the higher-dimensional space:'
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将数据恢复到高维空间：
- en: '[PRE84]'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'A section of the output is as follows:'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![Figure 4.28: The restored data'
  id: totrans-427
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.28：恢复的数据'
- en: '](img/B15923_04_28.jpg)'
  id: totrans-428
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_28.jpg)'
- en: 'Figure 4.28: The restored data'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.28：恢复的数据
- en: 'Add the means back to the restored data:'
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将均值添加回恢复的数据：
- en: '[PRE85]'
  id: totrans-431
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Visualize the restored data in the context of the original dataset:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在原始数据集的背景下可视化恢复的数据：
- en: '[PRE86]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-434
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.29: The inverse transform after removing the variance'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图4.29：去除方差后的逆转换'
- en: '](img/B15923_04_29.jpg)'
  id: totrans-436
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_29.jpg)'
- en: 'Figure 4.29: The inverse transform after removing the variance'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.29：去除方差后的逆转换
- en: If we compare the two plots produced in this exercise, we can see that the PCA
    went down, and the restored dataset is essentially a negative linear trend line
    between the two feature sets. We can compare this to the dataset restored from
    all of the available components, where we have recreated the original dataset
    as a whole.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们比较在本次练习中生成的两个图，我们可以看到PCA降维后的结果，恢复的数据集基本上是两组特征之间的负线性趋势线。我们可以将其与从所有可用成分恢复的数据集进行比较，在该数据集中，我们已经整体重建了原始数据集。
- en: Note
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/38EztBu](https://packt.live/38EztBu).
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/38EztBu](https://packt.live/38EztBu)。
- en: You can also run this example online at [https://packt.live/3f8LDVC](https://packt.live/3f8LDVC).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在[https://packt.live/3f8LDVC](https://packt.live/3f8LDVC)在线运行这个示例。
- en: 'Exercise 4.06: Visualizing Variance Reduction with scikit-learn'
  id: totrans-442
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.06：使用 scikit-learn 可视化方差减少
- en: 'In this exercise, we will again visualize the effect of reducing the dimensionality
    of the dataset; however, this time, we will be using the scikit-learn API. This
    is this method that you will commonly use in practical applications due to the
    power and simplicity of the scikit-learn model:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将再次可视化数据集降维的效果；然而，这次我们将使用 scikit-learn API。由于 scikit-learn 模型的强大和简便性，这是你在实际应用中常用的方法：
- en: Note
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    can also be downloaded from [https://packt.live/3bVlJm4](https://packt.live/3bVlJm4).'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。
    (UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。美国加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。)
    引用：贡献者衷心感谢波兰科学院农业物理研究所（位于卢布林）对其工作的支持。该数据集也可以从[https://packt.live/3bVlJm4](https://packt.live/3bVlJm4)下载。
- en: 'Import the `pandas`, `numpy`, and `matplotlib` plotting libraries and the `PCA`
    model from scikit-learn:'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pandas`、`numpy`、`matplotlib` 绘图库和 scikit-learn 中的 `PCA` 模型：
- en: '[PRE87]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Read in the `Wheat Kernel` features from the Seeds dataset:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 Seeds 数据集中读取 `Wheat Kernel` 特征：
- en: '[PRE88]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'The output is as follows:'
  id: totrans-450
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.30: The Wheat Kernel features from the Seeds dataset'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.30：来自 Seeds 数据集的小麦种子特征'
- en: '](img/B15923_04_30.jpg)'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_30.jpg)'
- en: 'Figure 4.30: The Wheat Kernel features from the Seeds dataset'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.30：来自 Seeds 数据集的小麦种子特征
- en: 'Use the scikit-learn API to transform the data on the basis of the first principal
    component:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn API 基于第一个主成分对数据进行变换：
- en: '[PRE89]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Compute the inverse transform of the reduced data and plot the result to visualize
    the effect of removing the variance from the data:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算降维数据的逆变换并绘制结果，以可视化从数据中去除方差的效果：
- en: '[PRE90]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The output is as follows:'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.31: The inverse transform after removing the variance'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.31：去除方差后的逆变换'
- en: '](img/B15923_04_31.jpg)'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_31.jpg)'
- en: 'Figure 4.31: The inverse transform after removing the variance'
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.31：去除方差后的逆变换
- en: 'There are only two components of variation in this dataset. If we do not remove
    any of the components, what will the result of the inverse transform be? Let''s
    find out by computing the inverse transform and seeing how the results change
    without removing any components:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该数据集中只有两个变异成分。如果我们不移除任何成分，那么逆变换的结果会是什么？我们通过计算逆变换并观察在不移除任何成分的情况下结果如何变化来找出答案：
- en: '[PRE91]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'The output is as follows:'
  id: totrans-464
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 4.32: The inverse transform after removing the variance'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.32：去除方差后的逆变换'
- en: '](img/B15923_04_29.jpg)'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_29.jpg)'
- en: 'Figure 4.32: The inverse transform after removing the variance'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.32：去除方差后的逆变换
- en: As we can see here, if we don't remove any of the components in the PCA, it
    will recreate the original data when performing inverse transform. We have demonstrated
    the effect of removing information from the dataset and the ability to recreate
    the original data using all of the available eigenvectors.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在这里看到的，如果我们不移除 PCA 中的任何成分，进行逆变换时它将重建原始数据。我们已经演示了从数据集中去除信息的效果，并展示了使用所有可用特征向量重建原始数据的能力。
- en: Note
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2O362zv](https://packt.live/2O362zv).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2O362zv](https://packt.live/2O362zv)。
- en: You can also run this example online at [https://packt.live/3fdWYDU](https://packt.live/3fdWYDU).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在[https://packt.live/3fdWYDU](https://packt.live/3fdWYDU)在线运行这个示例。
- en: The previous exercises specified the reduction in dimensionality using PCA to
    two dimensions, partly to allow the results to be visualized easily. We can, however,
    use PCA to reduce the dimensions to any value less than that of the original set.
    The following example demonstrates how PCA can be used to reduce a dataset to
    three dimensions, thereby allowing visualizations.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的练习指定了使用PCA将维度减少到两维，部分目的是为了便于结果的可视化。然而，我们可以使用PCA将维度减少到小于原始数据集的任何值。以下示例演示了如何使用PCA将数据集减少到三维，从而实现可视化。
- en: 'Exercise 4.07: Plotting 3D Plots in Matplotlib'
  id: totrans-473
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 4.07：在Matplotlib中绘制3D图
- en: 'Creating 3D scatter plots in `matplotlib` is unfortunately not as simple as
    providing a series of (*x*, *y*, *z*) coordinates to a scatter plot. In this exercise,
    we will work through a simple 3D plotting example using the Seeds dataset:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 在`matplotlib`中创建3D散点图，遗憾的是不像提供一系列(*x*, *y*, *z*)坐标那样简单。在本练习中，我们将使用种子数据集进行一个简单的3D绘图示例：
- en: Note
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集来自于[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。
    （UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。美国加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。）
- en: 'Citation: Contributors gratefully acknowledge the support of their work by
    the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The
    dataset can also be downloaded from [https://packt.live/3c2tAhT](https://packt.live/3c2tAhT).'
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：贡献者感谢波兰科学院农物理研究所（位于卢布林）对他们工作的支持。该数据集也可以从[https://packt.live/3c2tAhT](https://packt.live/3c2tAhT)下载。
- en: 'Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`:'
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`matplotlib`。为了启用3D绘图，您还需要导入`Axes3D`：
- en: '[PRE92]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Read in the dataset and select the `A`, `LK`, and `C` columns:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据集并选择`A`、`LK`和`C`列：
- en: '[PRE93]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The output is as follows:'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 4.33: The first five rows of the data'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.33：数据的前五行'
- en: '](img/B15923_04_33.jpg)'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_33.jpg)'
- en: 'Figure 4.33: The first five rows of the data'
  id: totrans-485
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.33：数据的前五行
- en: 'Plot the data in three dimensions and use the `projection=''3d''` argument
    with the `add_subplot` method to create the 3D plot:'
  id: totrans-486
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在三维空间中绘制数据，并使用`projection='3d'`参数与`add_subplot`方法一起创建3D图：
- en: '[PRE94]'
  id: totrans-487
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The plot will appear as follows:'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图将如下所示：
- en: '![Figure 4.34: The expanded Seeds dataset'
  id: totrans-489
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 4.34：扩展的种子数据集'
- en: '](img/B15923_04_34.jpg)'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_04_34.jpg)'
- en: 'Figure 4.34: The expanded Seeds dataset'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.34：扩展的种子数据集
- en: Note
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: While the `Axes3D` library was imported but not directly used, it is required
    for configuring the plot window in three dimensions. If the import of `Axes3D`
    was omitted, the `projection='3d'` argument would return an `AttributeError` exception.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`Axes3D`库已导入但未直接使用，它对于配置三维绘图窗口是必需的。如果省略了`Axes3D`的导入，`projection='3d'`参数将会返回一个`AttributeError`异常。
- en: To access the source code for this specific section, please refer to [https://packt.live/3gPM1J9](https://packt.live/3gPM1J9).
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3gPM1J9](https://packt.live/3gPM1J9)。
- en: You can also run this example online at [https://packt.live/2AFVXFr](https://packt.live/2AFVXFr).
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/2AFVXFr](https://packt.live/2AFVXFr)上在线运行此示例。
- en: 'Activity 4.02: PCA Using the Expanded Seeds Dataset'
  id: totrans-496
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 4.02：使用扩展的种子数据集进行PCA
- en: 'In this activity, we are going to use the complete Seeds dataset to look at
    the effect of selecting a differing number of components in the PCA decomposition.
    This activity aims to simulate the process that is typically completed in a real-world
    problem as we try to determine the optimum number of components to select, attempting
    to balance the extent of dimensionality reduction and information loss. Therefore,
    we will be using the scikit-learn PCA model:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，我们将使用完整的种子数据集，观察在PCA分解中选择不同数量的组件的影响。此活动旨在模拟在实际问题中通常完成的过程，我们尝试确定选择的最佳组件数量，平衡维度减少和信息丢失的程度。因此，我们将使用scikit-learn的PCA模型：
- en: Note
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds).
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science.)
    Citation: Contributors gratefully acknowledge the support of their work by the
    Institute of Agrophysics of the Polish Academy of Sciences in Lublin. The dataset
    also can be downloaded from [https://packt.live/3aPY0nj](https://packt.live/3aPY0nj).'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来自[https://archive.ics.uci.edu/ml/datasets/seeds](https://archive.ics.uci.edu/ml/datasets/seeds)。
    （UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院。）引用：贡献者感激波兰科学院农业物理研究所（位于卢布林）对其工作的支持。该数据集也可以从[https://packt.live/3aPY0nj](https://packt.live/3aPY0nj)下载。
- en: 'The following steps will help you to complete the activity:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤将帮助你完成该活动：
- en: Import `pandas` and `matplotlib`. To enable 3D plotting, you will also need
    to import `Axes3D`.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pandas`和`matplotlib`。为了启用三维绘图，你还需要导入`Axes3D`。
- en: Read in the dataset and select the `Area of Kernel`, `Length of Kernel`, and
    `Compactness of Kernel` columns.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取数据集并选择`Area of Kernel`、`Length of Kernel`和`Compactness of Kernel`列。
- en: Plot the data in three dimensions.
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在三维中绘制数据。
- en: Create a `PCA` model without specifying the number of components.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个不指定成分数的`PCA`模型。
- en: Fit the model to the dataset.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型拟合到数据集。
- en: Display the eigenvalues or `explained_variance_ratio_`.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示特征值或`explained_variance_ratio_`。
- en: We want to reduce the dimensionality of the dataset but still keep at least
    90% of the variance. What is the minimum number of components required to keep
    90% of the variance?
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望减少数据集的维度，但仍保持至少 90% 的方差。为了保持 90% 方差，所需的最小成分数是多少？
- en: Create a new `PCA` model, this time specifying the number of components required
    to keep at least 90% of the variance.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的`PCA`模型，这次指定需要保持至少 90% 方差的成分数。
- en: Transform the data using the new model.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新模型转换数据。
- en: Plot the transformed data.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制转换后的数据。
- en: Restore the transformed data to the original dataspace.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将转换后的数据恢复到原始数据空间。
- en: 'Plot the restored data in three dimensions in one subplot and the original
    data in a second subplot to visualize the effect of removing some of the variance:'
  id: totrans-512
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个子图中绘制恢复后的三维数据，在第二个子图中绘制原始数据，以可视化去除部分方差的效果：
- en: '[PRE95]'
  id: totrans-513
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: '**Expected output**: The final plot will appear as follows:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**预期输出**：最终图将如下所示：'
- en: '![Figure 4.35: Expected plots'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 4.35：预期图'
- en: '](img/B15923_04_35.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_04_35.jpg)'
- en: 'Figure 4.35: Expected plots'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.35：预期图
- en: Note
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 443.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 443 页找到。
- en: Summary
  id: totrans-520
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we covered the process of dimensionality reduction and PCA.
    We completed a number of exercises and developed the skills to reduce the size
    of a dataset by extracting only the most important components of variance within
    the data, using both a manual PCA process and the model provided by scikit-learn.
    During this chapter, we also returned the reduced datasets back to the original
    dataspace and observed the effect of removing the variance on the original data.
    Finally, we discussed a number of potential applications for PCA and other dimensionality
    reduction processes. In our next chapter, we will introduce neural network-based
    autoencoders and use the Keras package to implement them.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们介绍了降维和 PCA 的过程。我们完成了一些练习，并掌握了通过提取数据中最重要的方差成分来减少数据集大小的技巧，使用了手动的 PCA 过程和 scikit-learn
    提供的模型。在本章中，我们还将降维后的数据集恢复到原始数据空间，并观察去除方差对原始数据的影响。最后，我们讨论了 PCA 和其他降维过程的多种潜在应用。在下一章中，我们将介绍基于神经网络的自动编码器，并使用
    Keras 包实现它们。
