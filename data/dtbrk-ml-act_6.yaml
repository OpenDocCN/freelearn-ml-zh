- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Searching for a Signal
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找信号
- en: In this chapter, we’ll cover how to use data science to search for a signal
    hidden in the noise of data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍如何使用数据科学在数据的噪声中寻找隐藏的信号。
- en: We will leverage the features we created within the Databricks platform during
    the previous chapter. We start by using **automated machine learning** (**AutoML**)
    for a basic modeling approach, which provides autogenerated code and quickly enables
    data scientists to establish a baseline model to beat. When searching for a signal,
    we experiment with different features, hyperparameters, and models. Historically,
    tracking these configurations and their corresponding evaluation metrics is a
    time-consuming project in and of itself. A low-overhead tracking mechanism, such
    as the tracking provided by MLflow, an open source platform for managing data
    science projects and supporting **ML operations** (**MLOps**) will reduce the
    burden of manually capturing configurations. More specifically, we’ll introduce
    MLflow Tracking, an MLflow component that significantly improves tracking each
    permutation’s many outputs. However, that is only the beginning. As data science
    teams are being pressed to leverage **generative artificial intelligence** (**GenAI**),
    we will also showcase how to leverage a **large language model** (**LLM**) to
    create a SQL bot and a **deep learning** (**DL**) model utilizing PyTorch. These
    examples demonstrate that in addition to building our own solutions, we can integrate
    external innovative solutions into our workflows. This openness allows us to pick
    from the best of both worlds.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在前一章中在Databricks平台上创建的特征的基础上进行操作。我们首先使用**自动机器学习**（**AutoML**）进行基本建模方法，这提供了自动生成的代码，并迅速使数据科学家能够建立基线模型以超越。在寻找信号时，我们尝试不同的特征、超参数和模型。从历史上看，跟踪这些配置及其相应的评估指标本身就是一项耗时的工作。MLflow提供的低开销跟踪机制，作为一个开源平台，用于管理数据科学项目和支撑**机器学习操作**（**MLOps**），将减轻手动捕获配置的负担。更具体地说，我们将介绍MLflow
    Tracking，这是一个显著提高跟踪每个排列的许多输出的MLflow组件。然而，这仅仅是开始。随着数据科学团队被要求利用**生成式人工智能**（**GenAI**），我们还将展示如何利用**大型语言模型**（**LLM**）创建SQL机器人以及使用PyTorch的**深度学习**（**DL**）模型。这些示例表明，除了构建我们自己的解决方案外，我们还可以将外部创新解决方案集成到我们的工作流程中。这种开放性使我们能够从两个世界的最佳之处进行选择。
- en: 'Here is what you will learn as part of this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的一部分，你将学习以下内容：
- en: Baselining with AutoML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AutoML进行基线
- en: Classifying beyond the basic
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超越基本分类
- en: Applying our learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Here are the technical requirements needed to complete the hands-on examples
    in this chapter:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是需要完成本章动手实验的技术要求：
- en: 'Databricks ML Runtime:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Databricks ML Runtime：
- en: AutoML
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AutoML
- en: MLflow
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLflow
- en: '`Pandas`'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pandas`'
- en: '`Sklearn`'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sklearn`'
- en: '`Torch`'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Torch`'
- en: For our LLM model, we will integrate with the **ChatGPT** model from **OpenAI**
    ([https://openai.com/](https://openai.com/))
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们的LLM模型，我们将与来自**OpenAI**的**ChatGPT**模型（[https://openai.com/](https://openai.com/)）集成
- en: We will use the **PyTorch Lightning AI** Python package ([https://www.pytorchlightning.ai/index.html](https://www.pytorchlightning.ai/index.html))
    and **TorchMetrics** ([https://torchmetrics.readthedocs.io/en/stable/](https://torchmetrics.readthedocs.io/en/stable/))
    while building the classification model for the Parkinson’s **Freezing of Gait**
    (**FOG**) problem
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在构建用于帕金森病**冻结步态**（**FOG**）问题的分类模型时，我们将使用**PyTorch Lightning AI** Python包（[https://www.pytorchlightning.ai/index.html](https://www.pytorchlightning.ai/index.html)）和**TorchMetrics**（[https://torchmetrics.readthedocs.io/en/stable/](https://torchmetrics.readthedocs.io/en/stable/)）
- en: Baselining with AutoML
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AutoML进行基线
- en: A baseline model is a simple model used as a starting point for ML. Data scientists
    often use a baseline model to compare the performance of more complex models.
    Baseline models are typically simple or common algorithms, such as the majority
    class classifier or a random forest.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型是一个简单的模型，用作机器学习的起点。数据科学家经常使用基线模型来比较更复杂模型的性能。基线模型通常是简单的或常见的算法，例如多数类分类器或随机森林。
- en: 'Baseline models are valuable for several reasons, some of which are listed
    here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 基线模型有几个价值，其中一些如下列出：
- en: They can help you understand the difficulty of finding a signal given your current
    dataset. If even the best baseline model performs poorly, it may indicate that
    more complex models will also struggle to find useful patterns (that is, garbage
    data in, garbage models out).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以帮助你理解在当前数据集下寻找信号的难度。如果即使是最好的基线模型表现不佳，这也可能表明更复杂的模型在寻找有用的模式（即垃圾数据输入，垃圾模型输出）时也会遇到困难。
- en: Baseline models can help you to identify features that are most important for
    the ML task. If a baseline model performs well, it may be because it can learn
    from the most salient features.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准模型可以帮助你识别对机器学习任务最重要的特征。如果一个基准模型表现良好，那可能是因为它可以从最显著的特征中学习。
- en: Baseline models can help you avoid overfitting. Overfitting is a frequent problem
    with more complex models. It occurs when a model learns the training data too
    well and cannot generalize to new data. You can determine whether the more complex
    model is overfitting by comparing its performance to the baseline model. If the
    complex model performs better than the baseline on training data but worse on
    unseen test data, you know you’ve overfit.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准模型可以帮助你避免过拟合。过拟合是更复杂模型中常见的问题。当模型对训练数据学习得太好，无法推广到新数据时，就会发生过拟合。你可以通过将更复杂模型的性能与基准模型进行比较来确定该模型是否过拟合。如果复杂模型在训练数据上的表现优于基准模型，但在未见过的测试数据上表现较差，那么你就知道你已经过拟合了。
- en: There are multiple ways to create a baseline model. One straightforward approach
    is to use a random model. A random model is created by randomly assigning labels
    to the data. This type of model helps you evaluate how other models perform compared
    to random guessing. Another common approach is to use the majority class classifier.
    The majority class classifier always predicts the most common class in the training
    data and gives you another simplistic algorithm against which you can compare
    more complex models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 创建基准模型有多种方法。一种简单直接的方法是使用随机模型。随机模型是通过随机分配标签到数据创建的。这类模型帮助你评估其他模型与随机猜测相比的表现。另一种常见的方法是使用多数类分类器。多数类分类器总是预测训练数据中最常见的类别，为你提供了一个可以与更复杂模型比较的简单算法。
- en: In the lakehouse, we have AutoML, which is another straightforward way to get
    a baseline. This is our personal favorite way to start an ML task because it gives
    us a head start on model selection compared to the simpler baseline options. Recall
    that we used AutoML in [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180) to explore
    the *Favorita sales* data. While generating that exploration notebook, AutoML
    also generated model experiments, which is what we are focusing on now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据湖中，我们有AutoML，这是获取基准的另一种简单直接的方法。这是我们最喜欢的开始机器学习任务的方法，因为它在模型选择上比更简单的基准选项给我们带来了先发优势。回想一下，我们在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中使用了AutoML来探索*Favorita销售*数据。在生成那个探索笔记本的同时，AutoML还生成了模型实验，这是我们目前关注的重点。
- en: AutoML rapidly explores many model/hyperparameter permutations to find the best
    baseline model for your data, along with evaluation metrics and actual code. Once
    you have created a baseline model, you can evaluate its performance using accuracy,
    precision, recall, confusion matrices, **receiver operating characteristic** (**ROC**)
    curves, and more to choose the best experiment. However, it is essential to remember
    that AutoML is not a magic bullet. It can remove some of the overhead of coding
    multiple algorithms for experimentation, but you are still in charge of where
    to go next. Luckily, AutoML automatically tracks these model artifacts in MLflow,
    which is where MLflow shines. Tracking the many features, models, hyperparameters,
    and evaluation metrics is a real headache. Using MLflow to track everything natively
    is a lifesaver, so let’s explore that next.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML可以快速探索许多模型/超参数排列，以找到最适合你数据的基准模型，以及评估指标和实际代码。一旦你创建了基准模型，你可以使用准确率、精确率、召回率、混淆矩阵、**接收者操作特征**（**ROC**）曲线等来评估其性能，从而选择最佳的实验。然而，重要的是要记住，AutoML并不是万能的。它可以减少为实验编写多个算法的一些开销，但你仍然负责决定下一步的方向。幸运的是，AutoML会自动在MLflow中跟踪这些模型工件，这正是MLflow的亮点。跟踪许多特征、模型、超参数和评估指标确实是个头疼的问题。使用MLflow原生地跟踪所有这些内容可以救命，所以让我们继续探讨这一点。
- en: Tracking experiments with MLflow
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用MLflow跟踪实验
- en: 'MLflow is an open source platform developed by Databricks for managing data
    science projects through the entire ML life cycle, from the experimentation phase
    to packaging code to model deployment in production. We’ll cover deployment in
    a later chapter. For now, let’s focus on the **Tracking** component of MLflow
    ([https://mlflow.org/docs/latest/tracking.html#tracking](https://mlflow.org/docs/latest/tracking.html#tracking)).
    Before we had MLflow Tracking, ML experiments required a lot of work outside of
    actual experimentation. MLflow Tracking handles the overhead of capturing configurations
    of features, algorithms, and hyperparameters during testing. Additionally, using
    MLflow in the lakehouse gives you access to Managed MLflow, which is built on
    top of MLflow. Databricks notebooks have built-in integrations that make it easy
    to manage and compare experiment results both programmatically via Mlflow’s lightweight
    APIs or through **user interfaces** (**UIs**). For example, *Figure 6**.1* shows
    how we can view our experiment runs while still in our notebook:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow是一个开源平台，由Databricks开发，用于通过整个机器学习生命周期管理数据科学项目，从实验阶段到打包代码再到生产环境中的模型部署。我们将在后面的章节中介绍部署。现在，让我们专注于MLflow的**跟踪**组件([https://mlflow.org/docs/latest/tracking.html#tracking](https://mlflow.org/docs/latest/tracking.html#tracking))。在我们有MLflow
    Tracking之前，ML实验需要在实际实验之外做很多工作。MLflow Tracking处理了在测试期间捕获特征、算法和超参数配置的开销。此外，在lakehouse中使用MLflow，您将能够访问基于MLflow的托管MLflow。Databricks笔记本内置了集成，使得通过Mlflow的轻量级API或通过**用户界面**（**UIs**）程序化地管理和比较实验结果变得容易。例如，*图6.1*显示了我们在笔记本中如何查看我们的实验运行：
- en: '![Figure 6.1 – The in-notebook UI for viewing the experiment runs](img/B16865_06_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 查看实验运行的笔记本内UI](img/B16865_06_01.jpg)'
- en: Figure 6.1 – The in-notebook UI for viewing the experiment runs
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 查看实验运行的笔记本内UI
- en: 'In MLflow Tracking lingo, the execution of data science code is called a **run**.
    Each run can record the following:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLflow Tracking术语中，数据科学代码的执行被称为**运行**。每个运行可以记录以下内容：
- en: '**Parameters**: Key-value pairs of input parameters, such as the features used
    for a given run or the number of trees in a random forest.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数**：输入参数的关键值对，例如给定运行使用的特征或随机森林中的树的数量。'
- en: '**Metrics**: Evaluation metrics such as **Root Mean Squared Error** (**RMSE**)
    or **Area Under the ROC** **Curve** (**AUC**).'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指标**：评估指标，如**均方根误差**（**RMSE**）或**ROC曲线下面积**（**AUC**）。'
- en: '**Artifacts**: Arbitrary output files in any format. This can include images,
    pickled models, and data files.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工件**：任何格式的任意输出文件。这可以包括图像、pickle模型和数据文件。'
- en: '**Source**: The code that originally ran the experiment and a reference to
    the exact version of the data used for training.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**来源**：运行实验的原始代码以及用于训练的数据的确切版本。'
- en: 'When you train models in the notebook, model training information is automatically
    tracked with MLflow Tracking. To customize the auto-logging configuration, call
    `mlflow.autolog()` before your training code. Please note that although many common
    libraries have auto-logging support (such as Scikit-learn, XGBoost, and Keras),
    check the documentation for a full list: [https://mlflow.org/docs/latest/models.html#built-in-model-flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors).'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在笔记本中训练模型时，模型训练信息会自动通过MLflow Tracking进行跟踪。要自定义自动记录配置，请在训练代码之前调用`mlflow.autolog()`。请注意，尽管许多常见库都有自动记录支持（例如Scikit-learn、XGBoost和Keras），但请查阅文档以获取完整列表：[https://mlflow.org/docs/latest/models.html#built-in-model-flavors](https://mlflow.org/docs/latest/models.html#built-in-model-flavors)。
- en: When working on a particular ML task, it’s helpful to group your runs into “experiments.”
    This is an easy way to compare runs, either programmatically or via the Databricks
    Experiments UI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在特定的ML任务上工作时，将你的运行分组到“实验”中是有帮助的。这是一种比较运行的好方法，无论是通过程序化方式还是通过Databricks实验UI。
- en: In [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180) and [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244),
    we used AutoML for traditional regression and classification models. In the next
    session, we will lean into more advanced classification techniques for more complex
    business problems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16865_04.xhtml#_idTextAnchor180)和[*第5章*](B16865_05.xhtml#_idTextAnchor244)中，我们使用了AutoML来构建传统的回归和分类模型。在下一节中，我们将深入探讨更高级的分类技术，以解决更复杂的商业问题。
- en: Classifying beyond the basic
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越基本分类
- en: The Databricks AutoML product is a solid starting point for classification,
    regression, and forecasting models. There are more advanced classification techniques
    beyond tree-based models, gradient boost models, and logistic regression that
    you can use with the lakehouse, as it is designed to work with virtually any open
    source ML model.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks AutoML 产品是分类、回归和预测模型的良好起点。除了基于树的模型、梯度提升模型和逻辑回归之外，还有更多高级的分类技术可以使用
    lakehouse，因为它旨在与几乎任何开源机器学习模型一起工作。
- en: The Databricks ML runtimes include pre-built DL infrastructure and libraries
    such as PyTorch, TensorFlow, and Hugging Face transformers. DL models are computationally
    intensive, and **distributed DL** (**DDL**) frameworks such as Horovod also work
    in conjunction with these DL libraries for more efficient DDL. Be sure to check
    out the new PyTorch on Databricks! There is a *PyTorch on Databricks – Introducing
    the Spark PyTorch Distributor* blog that is useful if you are working with PyTorch
    ([https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html)).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Databricks ML 运行时包括预构建的深度学习基础设施和库，如 PyTorch、TensorFlow 和 Hugging Face 转换器。深度学习模型计算密集，**分布式深度学习**（**DDL**）框架如
    Horovod 也与这些深度学习库协同工作，以实现更高效的 DDL。务必查看 Databricks 上的新 PyTorch！有一个名为 *PyTorch on
    Databricks – Introducing the Spark PyTorch Distributor* 的博客，如果你在使用 PyTorch，这个博客会很有用（[https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html)）。
- en: 'Another exciting type of ML is **generative adversarial networks** (**GANs**).
    A quick introduction for those not familiar: GANs are a type of generative model
    that consists of two **neural networks** (**NNs**) – a generator and a discriminator.
    The generator network learns to generate synthetic data, such as images or text,
    that is similar to the real data, while the discriminator network tries to distinguish
    between the real and synthetic data. GANs have been used for image synthesis,
    data augmentation, and generating realistic deepfake videos. We used GANs in the
    past to thwart image classification algorithms. The goal was to alter an image
    just enough to confuse the DL algorithms but not so much that the human eye would
    recognize the image was altered. To see other applications of GANs, watch this
    awesome talk from *Data + AI Summit 2023*: *Generative AI at Scale Using GAN and
    Stable* *Diffusion* ([https://www.youtube.com/watch?v=YsWZDCsM9aE](https://www.youtube.com/watch?v=YsWZDCsM9aE)).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种令人兴奋的机器学习类型是**生成对抗网络**（**GANs**）。对于那些不熟悉的人来说，GANs 是一种由两个**神经网络**（**NNs**）组成的生成模型——生成器和判别器。生成器网络学习生成与真实数据相似的人工数据，如图像或文本，而判别器网络试图区分真实数据和人工数据。GANs
    已被用于图像合成、数据增强和生成逼真的深度伪造视频。我们过去曾使用 GANs 来挫败图像分类算法。目标是仅对图像进行足够的修改以混淆深度学习算法，但又不至于让人的眼睛能识别出图像已被修改。要了解
    GANs 的其他应用，请观看来自 *Data + AI Summit 2023* 的这个精彩演讲：*Generative AI at Scale Using
    GAN and Stable* *Diffusion* ([https://www.youtube.com/watch?v=YsWZDCsM9aE](https://www.youtube.com/watch?v=YsWZDCsM9aE))。
- en: Integrating innovation
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成创新
- en: The world of data science and ML moves very fast. You will likely come across
    projects that will benefit from innovations outside the standard ML libraries.
    For example, if you want to work on a project using text data, you will want to
    explore LLMs. LLMs are a type of advanced language model trained using DL techniques
    on massive amounts of text data. Fortunately, the Databricks platform makes it
    easy to integrate with projects such as OpenAI’s ChatGPT and other available options
    from Hugging Face.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学和机器学习的世界发展非常迅速。你可能会遇到一些可以从标准机器学习库之外的创新中受益的项目。例如，如果你想在一个使用文本数据的项目上工作，你将想要探索大型语言模型（LLMs）。LLMs
    是一种在大量文本数据上使用深度学习技术训练的高级语言模型。幸运的是，Databricks 平台使得与 OpenAI 的 ChatGPT 和 Hugging
    Face 提供的其他选项等项目的集成变得容易。
- en: Next, let’s look at an example of using an LLM to help business users or analysts
    get information from their tables without knowing SQL. We will build a chatbot
    using OpenAI’s **Generative Pre-trained Transformer 4** (**GPT-4**) as a data
    analyst. In this example, you create instructions on how it can ask for a list
    of tables, get information from those tables, and sample data from the tables.
    The chatbot is able to build a SQL query and then interpret the results. To run
    these notebooks in the example, you will need an account with OpenAI at the OpenAI
    developer site ([https://platform.openai.com](https://platform.openai.com)) and
    must request a key for the OpenAI API.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看使用LLM（大型语言模型）帮助商业用户或分析师从他们的表中获取信息而不需要知道SQL的一个示例。我们将使用OpenAI的**生成式预训练变换器4**（**GPT-4**）作为数据分析师来构建聊天机器人。在这个例子中，您将创建指令，说明它如何请求表列表、从那些表中获取信息以及从表中采样数据。聊天机器人能够构建SQL查询并解释结果。要在示例中运行这些笔记本，您需要在OpenAI开发者网站（[https://platform.openai.com](https://platform.openai.com)）上有一个账户，并且必须为OpenAI
    API请求一个密钥。
- en: In the first notebook, `sql_resource`, we will create instructions and references
    for the chatbot.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一本笔记本中，`sql_resource`，我们将为聊天机器人创建指令和参考。
- en: 'The notebook starts with commented text with tips on responses and response
    format, as shown here:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本从带有关于响应和响应格式的提示的注释文本开始，如下所示：
- en: '![Figure 6.2 – Instruction text for response and response format for this chatbot](img/B16865_06_02.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 聊天机器人的响应文本和响应格式](img/B16865_06_02.jpg)'
- en: Figure 6.2 – Instruction text for response and response format for this chatbot
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 聊天机器人的响应文本和响应格式
- en: 'The next lines are where you create text for invalid responses and the response
    format for your chatbot to share its output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的行是您创建无效响应文本和聊天机器人响应格式的位置：
- en: '![Figure 6.3 – Text for invalid responses and the chatbot response format](img/B16865_06_03.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 无效响应的文本和聊天机器人的响应格式](img/B16865_06_03.jpg)'
- en: Figure 6.3 – Text for invalid responses and the chatbot response format
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 无效响应的文本和聊天机器人的响应格式
- en: 'For your chatbot to understand the landscape of data, you will need to create
    a catalog and define functions to identify your list of tables, table definitions,
    and schemas as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您的聊天机器人理解数据环境，您需要创建一个目录并定义函数以识别您的表列表、表定义和模式，如下所示：
- en: '![Figure 6.4 – Defining tables and table locations for the chatbot](img/B16865_06_04.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – 为聊天机器人定义表和表位置](img/B16865_06_04.jpg)'
- en: Figure 6.4 – Defining tables and table locations for the chatbot
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 为聊天机器人定义表和表位置
- en: 'Now, your chatbot knows where to get information. To communicate with your
    chatbot, we need to teach it how to have a conversation. To do this, we define
    a log for a conversation and conversation function and the function to send the
    conversation to the OpenAI GPT-4 model. This is also where you can change which
    model your chatbot uses:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的聊天机器人知道在哪里获取信息。为了与您的聊天机器人进行交流，我们需要教它如何进行对话。为此，我们定义了一个对话日志、对话函数以及将对话发送到OpenAI
    GPT-4模型的函数。这也是您可以更改聊天机器人使用的模型的地方：
- en: '![Figure 6.5 – Defining function to submit conversation to OpenAI API](img/B16865_06_05.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 定义提交对话到OpenAI API的函数](img/B16865_06_05.jpg)'
- en: Figure 6.5 – Defining function to submit conversation to OpenAI API
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 定义提交对话到OpenAI API的函数
- en: 'We want our chatbot to build SQL queries to get data from our tables, so we
    create a function to teach it how to build a Spark SQL query:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望我们的聊天机器人构建SQL查询以从我们的表中获取数据，因此我们创建一个函数来教它如何构建Spark SQL查询：
- en: '![Figure 6.6 – Function to process SQL](img/B16865_06_06.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 处理SQL的函数](img/B16865_06_06.jpg)'
- en: Figure 6.6 – Function to process SQL
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 处理SQL的函数
- en: 'The function we created in *Figure 6**.6* is just a few lines of code, but
    it enables the chatbot to effectively build SQL queries against the tables defined
    in *Figure 6**.4*. Now, we need to tie it all together by defining how to process
    the conversation and create a response:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在图6.6中创建的函数只有几行代码，但它使聊天机器人能够有效地针对图6.4中定义的表构建SQL查询。现在，我们需要通过定义如何处理对话和创建响应来将这些功能全部结合起来：
- en: '![Figure 6.7 – Function to process request and response](img/B16865_06_07.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 处理请求和响应的函数](img/B16865_06_07.jpg)'
- en: Figure 6.7 – Function to process request and response
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 处理请求和响应的函数
- en: We have now constructed the chatbot, created the initial language for the chatbot
    to interact with prompts, designated the data and tables available, and showed
    it how to assemble queries and respond to prompts. In the next section, we start
    working with the chatbot.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经构建了聊天机器人，创建了聊天机器人与提示交互的初始语言，指定了可用的数据和表格，并展示了如何组装查询和响应提示。在下一节中，我们将开始与聊天机器人一起工作。
- en: 'Our next notebook is where we interact with the chatbot, and it starts by installing
    the OpenAI library:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一个笔记本是我们与聊天机器人交互的地方，它从安装OpenAI库开始：
- en: '![Figure 6.8 – Installing OpenAI library](img/B16865_06_08.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图6.8 – 安装OpenAI库](img/B16865_06_08.jpg)'
- en: Figure 6.8 – Installing OpenAI library
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 安装OpenAI库
- en: 'Next, we will pull in the functions that we defined in our `sql_resource` file:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从我们的`sql_resource`文件中调用我们定义的函数：
- en: '![Figure 6.9 – Importing functions from sql_resources](img/B16865_06_09.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图6.9 – 从sql_resources导入函数](img/B16865_06_09.jpg)'
- en: Figure 6.9 – Importing functions from sql_resources
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 从sql_resources导入函数
- en: 'With the library installed and the functions loaded, we have all of the parts
    assembled that are needed to interact. We start by using the `startConversation()`
    function to initiate a conversation with our chatbot:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装了库并加载了函数之后，我们已经组装了所有需要的部分来交互。我们首先使用`startConversation()`函数来与我们的聊天机器人开始对话：
- en: '![Figure 6.10 – Starting a conversation with the chatbot](img/B16865_06_10.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图6.10 – 与聊天机器人开始对话](img/B16865_06_10.jpg)'
- en: Figure 6.10 – Starting a conversation with the chatbot
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 与聊天机器人开始对话
- en: 'One thing that we have all experienced when interacting with chatbots is they
    don’t always give you the information you want the first time, so with our chatbot,
    we can have a back-and-forth conversation. In the preceding conversation, we wanted
    to know which customer ordered the most, but we don’t know how many orders the
    customer ordered, so in *Figure 6**.11*, we ask the question in a different way:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在与聊天机器人交互时都经历过的一件事是，它们并不总是第一次就提供您想要的信息，因此，在我们的聊天机器人中，我们可以进行一来一回的对话。在前面的对话中，我们想知道哪个客户订购得最多，但我们不知道客户订购了多少订单，所以在*图6*.11中，我们以不同的方式提出问题：
- en: '![Figure 6.11 – Continuing the conversation with the chatbot](img/B16865_06_11.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11 – 与聊天机器人继续对话](img/B16865_06_11.jpg)'
- en: Figure 6.11 – Continuing the conversation with the chatbot
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 与聊天机器人继续对话
- en: As new versions of OpenAI’s GPT model are released, the results and behavior
    of your chatbot may change. In this case, GPT-3.5 asked more questions than the
    GPT-4 version, but the GPT-4 version was better at using the commands to list
    tables and request table definitions. As new models and approaches become available,
    it is good practice to test them and see how the changes impact your work and
    the results of your chatbot. Leveraging MLflow with your chatbot experiment will
    help you track and compare different features and configurations and assist in
    your production process.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着OpenAI的GPT模型新版本的发布，您的聊天机器人的结果和行为可能会发生变化。在这种情况下，GPT-3.5版本比GPT-4版本提出了更多的问题，但GPT-4版本在利用命令列出表格和请求表格定义方面表现得更好。随着新的模型和方法变得可用，测试它们并观察这些变化如何影响您的工作和聊天机器人的结果是一种良好的实践。利用MLflow与您的聊天机器人实验相结合将帮助您跟踪和比较不同的功能和配置，并协助您的生产过程。
- en: In this next section, we will combine the features created in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244)
    to create models for our different datasets.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将结合在第[*第5章*](B16865_05.xhtml#_idTextAnchor244)中创建的功能来为我们的不同数据集创建模型。
- en: Applying our learning
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用我们的学习
- en: In this chapter, we have learned how to create baseline models using AutoML,
    tracking our MLOps with MLflow, and even using more advanced language models in
    order to extract more information and ultimately business value from our data.
    Now, let’s take what we have learned and apply it to our datasets that we cleaned
    in [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180) and featurized in [*Chapter
    5*](B16865_05.xhtml#_idTextAnchor244).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用AutoML创建基线模型，使用MLflow跟踪我们的MLOps，甚至使用更高级的语言模型从我们的数据中提取更多信息，并最终获得商业价值。现在，让我们将我们学到的知识应用到我们在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中清理并在[*第5章*](B16865_05.xhtml#_idTextAnchor244)中特征化的数据集上。
- en: We will start with creating and training a classification model for our Parkinson’s
    data so that, ultimately, we can classify hesitation using the patients’ tracking
    data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先为我们的帕金森病数据创建和训练一个分类模型，这样我们最终可以使用患者的跟踪数据来对犹豫进行分类。
- en: Parkinson’s FOG
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 帕金森病FOG
- en: 'As mentioned in the *Technical requirements* section, we are using PyTorch.
    To use this, either install the packages in your notebook using `pip` or add it
    to your cluster configuration under `libraries`:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *技术要求* 部分所述，我们正在使用 PyTorch。要使用此库，要么使用 `pip` 在笔记本中安装包，要么将其添加到集群配置下的 `libraries`：
- en: '![Figure 6.12 – Installing the PyTorch library](img/B16865_06_12.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.12 – 安装 PyTorch 库](img/B16865_06_12.jpg)'
- en: Figure 6.12 – Installing the PyTorch library
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 – 安装 PyTorch 库
- en: 'Once you have your libraries loaded, we import all the libraries we use:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦加载了库，我们就导入所有我们使用的库：
- en: '![Figure 6.13 – Importing libraries](img/B16865_06_13.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.13 – 导入库](img/B16865_06_13.jpg)'
- en: Figure 6.13 – Importing libraries
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.13 – 导入库
- en: 'For simplicity, we create a model focused on one target label, namely `StartHesitation`.
    For ease of reuse, define feature and target variables:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们创建了一个专注于一个目标标签的模型，即 `StartHesitation`。为了便于重用，定义特征和目标变量：
- en: '![Figure 6.14 – Defining measures and target variable](img/B16865_06_14.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.14 – 定义指标和目标变量](img/B16865_06_14.jpg)'
- en: Figure 6.14 – Defining measures and target variable
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 – 定义指标和目标变量
- en: 'Next, we create a custom `FogDataset` class. The class is used by PyTorch and
    requires three specific class methods: `__init__`, `__len__`, `__getitem__`.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个自定义的 `FogDataset` 类。该类由 PyTorch 使用，并需要三个特定的类方法：`__init__`、`__len__`、`__getitem__`。
- en: 'For the `LightningModel` class, as earlier, the `__init__` class method sets
    the labels and converts the features to tensors. The `__len__` class method returns
    the total amount of samples in your dataset. The `__getitem__` class method returns,
    given an index, the i-th sample and label:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `LightningModel` 类，正如之前所述，`__init__` 类方法设置标签并将特征转换为张量。`__len__` 类方法返回数据集中样本的总数。`__getitem__`
    类方法返回给定索引的 i-第样本和标签：
- en: '![Figure 6.15 – Creating custom FogDataset class](img/B16865_06_15.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.15 – 创建自定义 FogDataset 类](img/B16865_06_15.jpg)'
- en: Figure 6.15 – Creating custom FogDataset class
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.15 – 创建自定义 FogDataset 类
- en: 'Next, we are going to define the PyTorch model with functions. In the PyTorch
    model definitions, we call `self.log`, defining a `forward` and `test set()` function
    to surface scalars in TensorBoard. This will then be directly usable with PyTorch
    Lightning to make a lightning-fast model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用函数定义 PyTorch 模型。在 PyTorch 模型定义中，我们调用 `self.log`，定义一个 `forward` 和 `test
    set()` 函数以在 TensorBoard 中显示标量。然后，这将可以直接与 PyTorch Lightning 一起使用，以创建一个快速模型：
- en: '![Figure 6.16 – Defining the PyTorch model up to the training step definition](img/B16865_06_16.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.16 – 定义 PyTorch 模型直到训练步骤定义](img/B16865_06_16.jpg)'
- en: Figure 6.16 – Defining the PyTorch model up to the training step definition
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.16 – 定义 PyTorch 模型直到训练步骤定义
- en: '*Figure 6**.16* defines the PyTorch model, the forward feed, and the training
    step details. In the second half of the model code, we define test and validation
    steps, as shown here:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 6.16* 定义了 PyTorch 模型、前向传递和训练步骤的详细信息。在模型代码的第二部分，我们定义了测试和验证步骤，如下所示：'
- en: '![Figure 6.17 – The second half of the PyTorch model definition](img/B16865_06_17.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.17 – PyTorch 模型定义的第二部分](img/B16865_06_17.jpg)'
- en: Figure 6.17 – The second half of the PyTorch model definition
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.17 – PyTorch 模型定义的第二部分
- en: 'We are creating training data using SQL to join the `tdcsfog` data and `tdcsfog_metadata`.
    We check the label count and then convert the training data to Pandas to prepare
    it for the Sklearn library:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 SQL 创建训练数据，将 `tdcsfog` 数据和 `tdcsfog_metadata` 连接起来。我们检查标签计数，然后将训练数据转换为
    Pandas 以准备 Sklearn 库：
- en: '![Figure 6.18 – Creating Parkinson’s training dataset](img/B16865_06_18.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.18 – 创建帕金森病训练数据集](img/B16865_06_18.jpg)'
- en: Figure 6.18 – Creating Parkinson’s training dataset
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.18 – 创建帕金森病训练数据集
- en: 'We will stratify the training data by subject, printing the label distribution
    to look for the most representative train/test split:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按主题对训练数据进行分层，打印标签分布以寻找最具代表性的训练/测试分割：
- en: '![Figure 6.19 – Stratifying the training dataset](img/B16865_06_19.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.19 – 分层训练数据集](img/B16865_06_19.jpg)'
- en: Figure 6.19 – Stratifying the training dataset
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.19 – 分层训练数据集
- en: 'The following is a snippet of the output to illustrate why examining the folds
    is a necessary step in the process. There are some folds where there is a notable
    difference between the label distribution in the test and training data:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个输出片段，说明为什么检查折是过程中的一个必要步骤。有些折在测试和训练数据中的标签分布之间存在显著差异：
- en: '![Figure 6.20 – Reviewing the test and train labels of the folds](img/B16865_06_20.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图6.20 – 查看折的测试和训练标签](img/B16865_06_20.jpg)'
- en: Figure 6.20 – Reviewing the test and train labels of the folds
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 – 查看折的测试和训练标签
- en: 'We now implement our splits with fold `0`:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用折`0`实现拆分：
- en: '![Figure 6.21 – Implementing the splits at fold 3](img/B16865_06_21.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图6.21 – 在第3折中实现拆分](img/B16865_06_21.jpg)'
- en: Figure 6.21 – Implementing the splits at fold 3
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 – 在第3折中实现拆分
- en: 'Now that we have train test indices, we can clean our DataFrames by resetting
    the indices:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了训练和测试索引，我们可以通过重置索引来清理我们的DataFrames：
- en: '![Figure 6.22 – Resetting the indices after training](img/B16865_06_22.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图6.22 – 训练后重置索引](img/B16865_06_22.jpg)'
- en: Figure 6.22 – Resetting the indices after training
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 – 训练后重置索引
- en: 'Improperly indexed DataFrames cause issues with the `__getitem__` method in
    our `FogDataset` class. Now, we create custom datasets:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不正确索引的DataFrames会导致我们的`FogDataset`类中的`__getitem__`方法出现问题。现在，我们创建自定义数据集：
- en: '![Figure 6.23 – Creating customer train, test, and validation datasets](img/B16865_06_23.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图6.23 – 创建客户训练、测试和验证数据集](img/B16865_06_23.jpg)'
- en: Figure 6.23 – Creating customer train, test, and validation datasets
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 – 创建客户训练、测试和验证数据集
- en: 'Now, we build the model with the custom datasets we created and train using
    PyTorch’s `Trainer`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用我们创建的自定义数据集构建模型，并使用PyTorch的`Trainer`进行训练：
- en: "![Figure 6.24 – Building and\uFEFF training the model](img/B16865_06_24.jpg)"
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图6.24 – 构建和训练模型](img/B16865_06_24.jpg)'
- en: Figure 6.24 – Building and training the model
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 – 构建和训练模型
- en: We have now used our Parkinson’s FOG data to build and train a classification
    PyTorch model to predict hesitation in our dataset.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用我们的帕金森病FOG数据构建和训练了一个用于预测数据集中犹豫不决的分类PyTorch模型。
- en: Forecasting Favorita sales
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预测Favorita销售
- en: In [*Chapter 4*](B16865_04.xhtml#_idTextAnchor180), we used AutoML to jump-start
    our **exploratory data analysis** (**EDA**). Now, we’ll use AutoML to create a
    baseline model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第4章*](B16865_04.xhtml#_idTextAnchor180)中，我们使用AutoML来启动我们的**探索性数据分析**（**EDA**）。现在，我们将使用AutoML创建一个基线模型。
- en: 'To get started, we create an aggregated table to feed into AutoML. This is
    not required but is an easy way to get started:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们创建一个汇总表以供AutoML使用。这不是必需的，但这是一个简单的开始方法：
- en: '![Figure 6.25 – Creating a table of aggregated sales data](img/B16865_06_25.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图6.25 – 创建汇总销售数据表](img/B16865_06_25.jpg)'
- en: Figure 6.25 – Creating a table of aggregated sales data
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 – 创建汇总销售数据表
- en: Note that the code creates a `favorita_autoML_agg` table, which includes the
    lag features we created in [*Chapter 5*](B16865_05.xhtml#_idTextAnchor244).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，代码创建了一个`favorita_autoML_agg`表，其中包含我们在[*第5章*](B16865_05.xhtml#_idTextAnchor244)中创建的滞后特征。
- en: 'We create our AutoML experiment similarly to our previous one. See the experiment
    configurations in *Figure 6**.1*:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建我们的AutoML实验与之前类似。参见实验配置在*图6.1*：
- en: '![Figure 6.26 – AutoML experiment configuration for the Favorita forecasting
    sales example](img/B16865_06_26.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图6.26 – 为Favorita预测销售示例的AutoML实验配置](img/B16865_06_26.jpg)'
- en: Figure 6.26 – AutoML experiment configuration for the Favorita forecasting sales
    example
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 – 为Favorita预测销售示例的AutoML实验配置
- en: 'Notice that during this experiment, we are treating the forecasting problem
    like a regression problem by selecting the ML problem type as `date` variable
    in the `time` column; see *Figure 6**.27*:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这次实验中，我们通过在`time`列中将机器学习问题类型选择为`date`变量，将预测问题视为回归问题；参见*图6.27*：
- en: '![Figure 6.27 – AutoML advanced configuration for the Favorita forecasting
    sales example](img/B16865_06_27.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图6.27 – 为Favorita预测销售示例的高级AutoML配置](img/B16865_06_27.jpg)'
- en: Figure 6.27 – AutoML advanced configuration for the Favorita forecasting sales
    example
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 – 为Favorita预测销售示例的高级AutoML配置
- en: 'The experiment created about 100 runs before reaching the point where it was
    no longer making progress against the metric of choice – in our case, **R-squared**,
    as shown in *Figure 6**.27*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该实验在达到不再对所选指标（在我们的案例中为**R-squared**）取得进展的点之前进行了大约100次运行，如图*图6.27*所示：
- en: '![Figure 6.28 – AutoML advanced configuration for the Favorita forecasting
    sales example](img/B16865_06_28.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图6.28 – 为Favorita预测销售示例的高级AutoML配置](img/B16865_06_28.jpg)'
- en: Figure 6.28 – AutoML advanced configuration for the Favorita forecasting sales
    example
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28 – 为Favorita预测销售示例的高级AutoML配置
- en: Out of the 100 combinations, only 6 have an R-squared value of 0.85 or higher.
    Using AutoML is saving us considerable time and effort. During the experiment,
    MLflow tried many model types and hyperparameter tuning utilizing **Hyperopt**.
    This experiment is also distributed with the power of Spark, meaning we have a
    solid model that was tuned efficiently. We have a baseline model, and we are sure
    a signal can be found. From here forward, we want to aim to beat the model. Beating
    the model at this point is done by brute force. We improve performance by creating
    new features, gathering more data points, or enriching the dataset. This point
    is brute force. We improve performance by creating new features, gathering more
    data points, or enriching the dataset.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在100种组合中，只有6种具有0.85或更高的R平方值。使用AutoML为我们节省了大量的时间和精力。在实验过程中，MLflow尝试了许多模型类型和**Hyperopt**超参数调整。这个实验也利用Spark的分布式能力，这意味着我们有一个经过有效调整的稳定模型。我们有一个基线模型，并且我们确信可以找到信号。从现在开始，我们希望努力超越模型。在这个阶段，超越模型是通过暴力方法实现的。我们通过创建新特征、收集更多数据点或丰富数据集来提高性能。这一点是暴力方法。我们通过创建新特征、收集更多数据点或丰富数据集来提高性能。
- en: Summary
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed quick ways to create a baseline model and demonstrated
    how that increases productivity.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了创建基线模型的快速方法，并展示了这种方法如何提高生产力。
- en: We demonstrated MLflow functionality that supports MLOps and helps track model
    training and tuning. We also covered more complex classification frameworks that
    can be used in the lakehouse. Access to these frameworks made it possible to implement
    a DL model in PyTorch for the Parkinson’s FOG example. The openness of Databricks
    opens the doors for open source and proprietary innovations with API integrations,
    as shown by the SQL bot LLM. This integration saved time by not recreating the
    wheel and putting the SQL tool in the hands of our analysts sooner.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了支持MLOps并帮助跟踪模型训练和调整的MLflow功能。我们还介绍了可以在湖屋中使用的更复杂的分类框架。这些框架的访问使我们能够在PyTorch中实现帕金森病FOG示例的深度学习模型。Databricks的开放性为开源和专有创新打开了大门，如SQL机器人LLM所示。这种集成通过避免重复造轮子，并更快地将SQL工具交到我们的分析师手中，节省了时间。
- en: The next chapter will focus on moving our models into production.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将重点介绍将我们的模型投入生产。
- en: Questions
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'The following questions solidify key points to remember and tie the content
    back to your experience:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 以下问题有助于巩固需要记住的关键点，并将内容与您的经验联系起来：
- en: Why would you use a baseline model?
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么你会使用基线模型？
- en: What are examples of more advanced classification techniques?
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更高级的分类技术有哪些例子？
- en: When would you use LLM models, such as OpenAI’s ChatGPT or **Dolly**, in your
    lakehouse?
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你会在何时在您的数据湖中使用LLM模型，例如OpenAI的ChatGPT或**Dolly**？
- en: Answers
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 答案
- en: 'After putting thought into the questions, compare your answers to ours:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在思考过这些问题后，比较您的答案与我们的答案：
- en: Use a baseline model to have a simple model as a starting point to compare later
    and more complex models.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基线模型作为起点，以便稍后比较更复杂和更复杂的模型。
- en: Some examples of more advanced classification techniques include DL and GANs.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些更高级的分类技术示例包括深度学习（DL）和生成对抗网络（GANs）。
- en: I would use an LLM model in my lakehouse if I needed to have more advanced language
    techniques with my data, such as a chatbot.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我需要在我的数据湖中实现更高级的语言技术，例如聊天机器人，我会使用LLM模型。
- en: Further reading
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'In this chapter, we pointed out specific technologies, technical features,
    and options. Please take a look at these resources to get deeper into areas that
    interest you most:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们指出了具体的技术、技术特性和选项。请查看这些资源，深入了解您最感兴趣的区域：
- en: '*Introducing AI Functions: Integrating Large Language Models with Databricks*
    *SQL*: [https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html](https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html)'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*介绍AI功能：将大型语言模型与Databricks集成* *SQL*：[https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html](https://www.databricks.com/blog/2023/04/18/introducing-ai-functions-integrating-large-language-models-databricks-sql.html)'
- en: '*PyTorch on Databricks – Introducing the Spark PyTorch* *Distributor*:[https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*在Databricks上使用PyTorch – 介绍Spark PyTorch* *分发器*：[https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html](https://www.databricks.com/blog/2023/04/20/pytorch-databricks-introducing-spark-pytorch-distributor.html)'
- en: '*Free Dolly: Introducing the World’s First Truly Open Instruction-Tuned* *LLM*:
    [https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*免费Dolly：介绍世界上第一个真正开放的指令调整* *LLM*：[https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)'
- en: '*Ray 2.3 release (**PyPI)*: [https://pypi.org/project/ray/](https://pypi.org/project/ray/)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ray 2.3版本（**PyPI**）：[https://pypi.org/project/ray/](https://pypi.org/project/ray/)'
- en: '*Ray on Spark Databricks* *docs*: [https://docs.databricks.com/machine-learning/ray-integration.html](https://docs.databricks.com/machine-learning/ray-integration.html)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Spark Databricks上的Ray* 文档：[https://docs.databricks.com/machine-learning/ray-integration.html](https://docs.databricks.com/machine-learning/ray-integration.html)'
- en: '*Announcing Ray support on Databricks and Apache Spark* *Clusters*: [https://www.databricks.com/blog/2023/02/28/announcing-ray-support-databricks-and-apache-spark-clusters.html](https://www.databricks.com/blog/2023/02/28/announcing-ray-support-databricks-and-apache-spark-clusters.html)'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*宣布在Databricks和Apache Spark* *集群*上支持Ray：[https://www.databricks.com/blog/2023/02/28/announcing-ray-support-databricks-and-apache-spark-clusters.html](https://www.databricks.com/blog/2023/02/28/announcing-ray-support-databricks-and-apache-spark-clusters.html)'
- en: '*Ray* *docs*: [https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#deploying-on-spark-standalone-cluster](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#deploying-on-spark-standalone-cluster)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ray* 文档：[https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#deploying-on-spark-standalone-cluster](https://docs.ray.io/en/latest/cluster/vms/user-guides/community/spark.html#deploying-on-spark-standalone-cluster)'
