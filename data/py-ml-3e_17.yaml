- en: '17'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '17'
- en: Generative Adversarial Networks for Synthesizing New Data
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于合成新数据的生成对抗网络
- en: In the previous chapter, we focused on **recurrent neural networks** for modeling
    sequences. In this chapter, we will explore **generative adversarial networks**
    (**GANs**) and see their application in synthesizing new data samples. GANs are
    considered to be the most important breakthrough in deep learning, allowing computers
    to generate new data (such as new images).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，我们重点介绍了用于建模序列的**循环神经网络**。在本章中，我们将探索**生成对抗网络**（**GANs**）并了解其在合成新数据样本中的应用。GANs被认为是深度学习中的最重要突破，它允许计算机生成新数据（例如新图像）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Introducing generative models for synthesizing new data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入用于合成新数据的生成模型
- en: Autoencoders, **variational autoencoders** (**VAEs**), and their relationship
    to GANs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码器、**变分自编码器**（**VAEs**）及其与GAN的关系
- en: Understanding the building blocks of GANs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解GAN的构建模块
- en: Implementing a simple GAN model to generate handwritten digits
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个简单的GAN模型来生成手写数字
- en: Understanding transposed convolution and **batch normalization** (**BatchNorm**
    or **BN**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解转置卷积和**批归一化**（**BatchNorm**或**BN**）
- en: 'Improving GANs: deep convolutional GANs and GANs using the Wasserstein distance'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进GAN：深度卷积GAN和使用Wasserstein距离的GAN
- en: Introducing generative adversarial networks
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入生成对抗网络
- en: Let's first look at the foundations of GAN models. The overall objective of
    a GAN is to synthesize new data that has the same distribution as its training
    dataset. Therefore, GANs, in their original form, are considered to be in the
    unsupervised learning category of machine learning tasks, since no labeled data
    is required. It is worth noting, however, that extensions made to the original
    GAN can lie in both semi-supervised and supervised tasks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先来看一下GAN模型的基础。GAN的总体目标是合成具有与训练数据集相同分布的新数据。因此，GAN在其原始形式下被认为是无监督学习类别中的机器学习任务，因为不需要标注数据。然而，值得注意的是，对原始GAN的扩展可以同时应用于半监督和监督任务。
- en: The general GAN concept was first proposed in 2014 by Ian Goodfellow and his
    colleagues as a method for synthesizing new images using deep neural networks
    (NNs) (Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D.,
    Ozair, S., Courville, A. and Bengio, Y., *Generative Adversarial Nets*, in *Advances
    in Neural Information Processing Systems*, pp. 2672-2680, 2014). While the initial
    GAN architecture proposed in this paper was based on fully connected layers, similar
    to multilayer perceptron architectures, and trained to generate low-resolution
    MNIST-like handwritten digits, it served more as a proof of concept to demonstrate
    the feasibility of this new approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Ian Goodfellow及其同事首次提出了通用的GAN概念，作为一种使用深度神经网络（NNs）合成新图像的方法（Goodfellow,
    I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,
    A.和Bengio, Y.，《生成对抗网络》，发表于《神经信息处理系统进展》，第2672-2680页，2014年）。虽然这篇论文中提出的最初GAN架构是基于完全连接的层，类似于多层感知机架构，并且训练生成低分辨率的MNIST风格手写数字，但它更多的是作为一个概念验证，展示了这种新方法的可行性。
- en: However, since its introduction, the original authors, as well as many other
    researchers, have proposed numerous improvements and various applications in different
    fields of engineering and science; for example, in computer vision, GANs are used
    for image-to-image translation (learning how to map an input image to an output
    image), image super-resolution (making a high-resolution image from a low-resolution
    version), image inpainting (learning how to reconstruct the missing parts of an
    image), and many more applications. For instance, recent advances in GAN research
    have led to models that are able to generate new, high-resolution face images.
    Examples of such high-resolution images can be found on [https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/),
    which showcases synthetic face images generated by a GAN.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，自从它的提出以来，原作者以及许多其他研究人员提出了众多改进和不同领域的应用。例如，在计算机视觉中，GANs被用于图像到图像的转换（学习如何将输入图像映射到输出图像）、图像超分辨率（从低分辨率版本生成高分辨率图像）、图像修复（学习如何重建图像中缺失的部分）等多个应用。比如，最近GAN研究的进展已经导致了能够生成新高分辨率人脸图像的模型。此类高分辨率图像的示例可以在[https://www.thispersondoesnotexist.com/](https://www.thispersondoesnotexist.com/)上找到，该网站展示了由GAN生成的合成面部图像。
- en: Starting with autoencoders
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从自编码器开始
- en: Before we discuss how GANs work, we will first start with autoencoders, which
    can compress and decompress training data. While standard autoencoders cannot
    generate new data, understanding their function will help you to navigate GANs
    in the next section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论GAN如何工作之前，我们首先从自动编码器开始，自动编码器可以压缩和解压训练数据。虽然标准的自动编码器无法生成新数据，但理解它们的功能将帮助你在下一节中理解GAN。
- en: 'Autoencoders are composed of two networks concatenated together: an **encoder**
    network and a **decoder** network. The encoder network receives a *d*-dimensional
    input feature vector associated with example *x* (that is, ![](img/B13208_17_001.png))
    and encodes it into a *p*-dimensional vector, *z* (that is, ![](img/B13208_17_002.png)).
    In other words, the role of the encoder is to learn how to model the function
    ![](img/B13208_17_003.png). The encoded vector, *z*, is also called the latent
    vector, or the latent feature representation. Typically, the dimensionality of
    the latent vector is less than that of the input examples; in other words, *p*
    < *d*. Hence, we can say that the encoder acts as a data compression function.
    Then, the decoder decompresses ![](img/B13208_17_004.png) from the lower-dimensional
    latent vector, *z*, where we can think of the decoder as a function, ![](img/B13208_17_005.png).
    A simple autoencoder architecture is shown in the following figure, where the
    encoder and decoder parts consist of only one fully connected layer each:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器由两个网络连接在一起组成：**编码器**网络和**解码器**网络。编码器网络接收一个与示例 *x* 相关的 *d* 维输入特征向量（即，![](img/B13208_17_001.png)），并将其编码成一个
    *p* 维向量 *z*（即，![](img/B13208_17_002.png)）。换句话说，编码器的作用是学习如何建模函数 ![](img/B13208_17_003.png)。编码后的向量
    *z* 也叫做潜在向量，或潜在特征表示。通常，潜在向量的维度小于输入示例的维度；换句话说，*p* < *d*。因此，我们可以说编码器充当了数据压缩函数的角色。然后，解码器从低维的潜在向量
    *z* 解压出 ![](img/B13208_17_004.png)，我们可以把解码器看作是一个函数，![](img/B13208_17_005.png)。下图展示了一个简单的自动编码器架构，其中编码器和解码器部分各自只包含一个全连接层：
- en: '![](img/B13208_17_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_01.png)'
- en: '**The connection between autoencoders and dimensionality reduction**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动编码器与降维之间的关系**'
- en: In *Chapter 5*, *Compressing Data via Dimensionality Reduction*, you learned
    about dimensionality reduction techniques, such as principal component analysis
    (PCA) and linear discriminant analysis (LDA). Autoencoders can be used as a dimensionality
    reduction technique as well. In fact, when there is no nonlinearity in either
    of the two subnetworks (encoder and decoder), then the autoencoder approach is
    *almost identical* to PCA.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第5章*，*通过降维压缩数据*，你学习了降维技术，如主成分分析（PCA）和线性判别分析（LDA）。自动编码器也可以作为一种降维技术。事实上，当两个子网络（编码器和解码器）中都没有非线性时，自动编码器方法*几乎与PCA相同*。
- en: 'In this case, if we assume the weights of a single-layer encoder (no hidden
    layer and no nonlinear activation function) are denoted by the matrix *U*, then
    the encoder models ![](img/B13208_17_006.png). Similarly, a single-layer linear
    decoder models ![](img/B13208_17_007.png). Putting these two components together,
    we have ![](img/B13208_17_008.png). This is exactly what PCA does, with the exception
    that PCA has an additional orthonormal constraint: ![](img/B13208_17_009.png).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，如果我们假设单层编码器（没有隐藏层和非线性激活函数）的权重用矩阵 *U* 表示，那么编码器建模 ![](img/B13208_17_006.png)。类似地，单层线性解码器建模
    ![](img/B13208_17_007.png)。将这两个组件结合起来，我们得到了 ![](img/B13208_17_008.png)。这正是PCA所做的，唯一的区别是PCA有一个附加的正交归一约束：![](img/B13208_17_009.png)。
- en: While the previous figure depicts an autoencoder without hidden layers within
    the encoder and decoder, we can, of course, add multiple hidden layers with nonlinearities
    (as in a multilayer NN) to construct a deep autoencoder that can learn more effective
    data compression and reconstruction functions. Also, note that the autoencoder
    mentioned in this section uses fully connected layers. When we work with images,
    however, we can replace the fully connected layers with convolutional layers,
    as you learned in *Chapter 15*, *Classifying Images with Deep Convolutional Neural
    Networks*.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的图展示了一个没有隐藏层的编码器和解码器的自动编码器，但我们当然可以添加多个带有非线性的隐藏层（如多层神经网络）来构建一个深度自动编码器，它能够学习更有效的数据压缩和重建功能。此外，注意到这一节中提到的自动编码器使用的是全连接层。当我们处理图像时，我们可以用卷积层替代全连接层，正如你在*第15章*，*使用深度卷积神经网络分类图像*中学到的那样。
- en: '**Other types of autoencoders based on the size of latent space**'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于潜在空间大小的其他类型的自动编码器**'
- en: As previously mentioned, the dimensionality of an autoencoder's latent space
    is typically lower than the dimensionality of the inputs (*p* < *d*), which makes
    autoencoders suitable for dimensionality reduction. For this reason, the latent
    vector is also often referred to as the "bottleneck," and this particular configuration
    of an autoencoder is also called *undercomplete*. However, there is a different
    category of autoencoders, called *overcomplete*, where the dimensionality of the
    latent vector, *z*, is, in fact, greater than the dimensionality of the input
    examples (*p* > *d*).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，自编码器的潜在空间维度通常低于输入的维度（*p* < *d*），这使得自编码器适用于降维。因此，潜在向量通常也被称为“瓶颈”，这种特定配置的自编码器被称为*欠完备*。然而，还有一种不同类型的自编码器，称为*过完备*，在这种情况下，潜在向量的维度，*z*，实际上大于输入示例的维度（*p*
    > *d*）。
- en: When training an overcomplete autoencoder, there is a trivial solution where
    the encoder and the decoder can simply learn to copy (memorize) the input features
    to their output layer. Obviously, this solution is not very useful. However, with
    some modifications to the training procedure, overcomplete autoencoders can be
    used for *noise reduction*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过完备自编码器时，会出现一个简单的解决方案，其中编码器和解码器可以仅通过学习复制（记忆）输入特征到其输出层。显然，这个解决方案并不是很有用。然而，通过对训练过程进行一些修改，过完备自编码器可以用于*降噪*。
- en: In this case, during training, random noise, ![](img/B13208_17_010.png), is
    added to the input examples and the network learns to reconstruct the clean example,
    *x*, from the noisy signal, ![](img/B13208_17_011.png). Then, at evaluation time,
    we provide the new examples that are naturally noisy (that is, noise is already
    present such that no additional artificial noise, ![](img/B13208_17_012.png),
    is added) in order to remove the existing noise from these examples. This particular
    autoencoder architecture and training method is referred to as a *denoising autoencoder*.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，在训练过程中，会向输入示例中添加随机噪声，![](img/B13208_17_010.png)，网络学习从噪声信号中重建干净的示例，*x*，![](img/B13208_17_011.png)。然后，在评估时，我们提供自然带有噪声的新示例（即，噪声已经存在，因此不再添加额外的人工噪声，![](img/B13208_17_012.png)），以从这些示例中去除已有的噪声。这种特定的自编码器架构和训练方法被称为*去噪自编码器*。
- en: 'If you are interested, you can learn more about it in the research article
    *Stacked denoising autoencoders: Learning useful representations in a deep network
    with a local denoising criterion* by Vincent et al., which is freely available
    at [http://www.jmlr.org/papers/v11/vincent10a.html](http://www.jmlr.org/papers/v11/vincent10a.html).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您感兴趣，可以通过Vincent等人的研究文章《*堆叠去噪自编码器：通过局部去噪标准在深度网络中学习有用的表示*》进一步了解，该文章可以免费访问：[http://www.jmlr.org/papers/v11/vincent10a.html](http://www.jmlr.org/papers/v11/vincent10a.html)。
- en: Generative models for synthesizing new data
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于合成新数据的生成模型
- en: Autoencoders are deterministic models, which means that after an autoencoder
    is trained, given an input, *x*, it will be able to reconstruct the input from
    its compressed version in a lower-dimensional space. Therefore, it cannot generate
    new data beyond reconstructing its input through the transformation of the compressed
    representation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是确定性模型，这意味着在自编码器训练完成后，给定输入，*x*，它将能够从其在低维空间中的压缩版本中重建输入。因此，它无法生成超出通过压缩表示的转换重建其输入的新数据。
- en: A generative model, on the other hand, can generate a new example, ![](img/B13208_17_139.png),
    from a random vector, *z* (corresponding to the latent representation). A schematic
    representation of a generative model is shown in the following figure. The random
    vector, *z*, comes from a simple distribution with fully known characteristics,
    so we can easily sample from such a distribution. For example, each element of
    *z* may come from the uniform distribution in the range [–1, 1] (for which we
    write ![](img/B13208_17_013.png)) or from a standard normal distribution (in which
    case, we write ![](img/B13208_17_014.png)).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，生成模型可以从一个随机向量，*z*（对应于潜在表示），生成一个新的示例，![](img/B13208_17_139.png)。生成模型的示意图如下所示。随机向量，*z*，来自一个简单的分布，具有完全已知的特征，因此我们可以轻松地从该分布中采样。例如，*z*
    的每个元素可以来自范围[–1, 1]内的均匀分布（我们可以写作![](img/B13208_17_013.png)），或者来自标准正态分布（在这种情况下，我们写作![](img/B13208_17_014.png)）。
- en: '![](img/B13208_17_02.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_02.png)'
- en: As we have shifted our attention from autoencoders to generative models, you
    may have noticed that the decoder component of an autoencoder has some similarities
    with a generative model. In particular, they both receive a latent vector, *z*,
    as input and return an output in the same space as *x*. (For the autoencoder,
    ![](img/B13208_17_015.png) is the reconstruction of an input, *x*, and for the
    generative model, ![](img/B13208_17_016.png) is a synthesized sample.)
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将注意力从自编码器转移到生成模型时，你可能已经注意到自编码器的解码器组件与生成模型有些相似。特别是，它们都接收潜在向量*z*作为输入，并返回与*x*相同空间的输出。（对于自编码器，![](img/B13208_17_015.png)是输入*x*的重构，对于生成模型，![](img/B13208_17_016.png)是合成的样本。）
- en: However, the major difference between the two is that we do not know the distribution
    of *z* in the autoencoder, while in a generative model, the distribution of *z*
    is fully characterizable. It is possible to generalize an autoencoder into a generative
    model, though. One approach is **VAEs**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，两者之间的主要区别在于我们不知道自编码器中*z*的分布，而在生成模型中，*z*的分布是完全可以表征的。不过，将自编码器推广为生成模型是可能的。一种方法是**变分自编码器（VAEs）**。
- en: 'In a VAE receiving an input example, *x*, the encoder network is modified in
    such a way that it computes two moments of the distribution of the latent vector:
    the mean, ![](img/B13208_17_017.png), and variance, ![](img/B13208_17_018.png).
    During the training of a VAE, the network is forced to match these moments with
    those of a standard normal distribution (that is, zero mean and unit variance).
    Then, after the VAE model is trained, the encoder is discarded, and we can use
    the decoder network to generate new examples, ![](img/B13208_17_016.png), by feeding
    random *z* vectors from the "learned" Gaussian distribution.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在VAE中，接收到输入示例*x*时，编码器网络被修改成这样一种形式，使其计算潜在向量的两个时刻：均值，![](img/B13208_17_017.png)，和方差，![](img/B13208_17_018.png)。在VAE的训练过程中，网络被迫使这些时刻与标准正态分布的时刻匹配（即均值为零，方差为单位）。然后，在VAE模型训练完成后，编码器被丢弃，我们可以使用解码器网络通过输入来自“学习”高斯分布的随机*z*向量来生成新的示例，![](img/B13208_17_016.png)。
- en: Besides VAEs, there are other types of generative models, for example, *autoregressive
    models* and *normalizing flow models*. However, in this chapter, we are only going
    to focus on GAN models, which are among the most recent and most popular types
    of generative models in deep learning.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了变分自编码器（VAEs），还有其他类型的生成模型，例如*自回归模型*和*正则化流模型*。然而，在本章中，我们只会专注于GAN模型，后者是深度学习中最现代且最流行的生成模型之一。
- en: '**What is a generative model?**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是生成模型？**'
- en: Note that generative models are traditionally defined as algorithms that model
    data input distributions, *p*(*x*), or the joint distributions of the input data
    and associated targets, *p*(*x*, *y*). By definition, these models are also capable
    of sampling from some feature, ![](img/B13208_17_020.png), conditioned on another
    feature, ![](img/B13208_17_021.png), which is known as *conditional inference*.
    In the context of deep learning, however, the term *generative model* is typically
    used to refer to models that generate realistic-looking data. This means that
    we can sample from input distributions, *p*(*x*), but we are not necessarily able
    to perform conditional inference.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成模型通常定义为模拟数据输入分布的算法，*p*(*x*)，或者输入数据与相关目标的联合分布，*p*(*x*, *y*)。按照定义，这些模型也能从某些特征中采样，![](img/B13208_17_020.png)，并且条件于另一个特征，![](img/B13208_17_021.png)，这称为*条件推理*。然而，在深度学习的背景下，*生成模型*这个术语通常指的是生成逼真数据的模型。这意味着我们可以从输入分布*p*(*x*)中采样，但不一定能够进行条件推理。
- en: Generating new samples with GANs
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GAN生成新样本
- en: To understand what GANs do in a nutshell, let's first assume we have a network
    that receives a random vector, *z*, sampled from a known distribution and generates
    an output image, *x*. We will call this network **generator** (*G*) and use the
    notation ![](img/B13208_17_022.png) to refer to the generated output. Assume our
    goal is to generate some images, for example, face images, images of buildings,
    images of animals, or even handwritten digits such as MNIST.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，为了理解GAN的作用，我们首先假设有一个网络，它接收一个从已知分布中采样的随机向量*z*并生成输出图像*x*。我们将这个网络称为**生成器**（*G*），并使用符号![](img/B13208_17_022.png)表示生成的输出。假设我们的目标是生成一些图像，例如人脸图像、建筑物图像、动物图像，甚至是像MNIST这样的手写数字。
- en: As always, we will initialize this network with random weights. Therefore, the
    first output images, before these weights are adjusted, will look like white noise.
    Now, imagine there is a function that can assess the quality of images (let's
    call it an *assessor function*).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，我们将用随机权重初始化这个网络。因此，在这些权重调整之前，第一次输出的图像看起来像白噪声。现在，假设存在一个可以评估图像质量的函数（我们称之为
    *评估函数*）。
- en: If such a function exists, we can use the feedback from that function to tell
    our generator network how to adjust its weights in order to improve the quality
    of the generated images. This way, we can train the generator based on the feedback
    from that assessor function, such that the generator learns to improve its output
    toward producing realistic-looking images.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果存在这样的函数，我们可以利用该函数的反馈来告诉生成器网络如何调整其权重，以提高生成图像的质量。通过这种方式，我们可以基于评估函数的反馈训练生成器，使其学习改善输出，朝着生成真实感图像的方向努力。
- en: 'While an assessor function, as described in the previous paragraph, would make
    the image generation task very easy, the question is whether such a universal
    function to assess the quality of images exists and, if so, how it is defined.
    Obviously, as humans, we can easily assess the quality of output images when we
    observe the outputs of the network; although, we cannot (yet) backpropagate the
    result from our brain to the network. Now, if our brain can assess the quality
    of synthesized images, can we design an NN model to do the same thing? In fact,
    that''s the general idea of a GAN. As shown in the following figure, a GAN model
    consists of an additional NN called **discriminator** (*D*), which is a classifier
    that learns to detect a synthesized image, ![](img/B13208_17_023.png), from a
    real image, *x*:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前述的评估函数会使图像生成任务变得非常简单，但问题在于是否存在这样一个通用函数来评估图像质量，如果存在，它又是如何定义的。显然，作为人类，我们可以轻松地评估当我们观察网络输出时图像的质量；尽管我们目前还不能（暂时）将这一结果从大脑反向传播到网络。那么，如果我们的脑袋能够评估合成图像的质量，我们能否设计一个神经网络模型来做同样的事情？实际上，这正是
    GAN 的基本思想。如以下图所示，GAN 模型包含一个额外的神经网络，称为 **判别器** (*D*)，它是一个分类器，学习区分合成图像 ![](img/B13208_17_023.png)
    和真实图像 *x*：
- en: '![](img/B13208_17_03.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_03.png)'
- en: In a GAN model, the two networks, generator and discriminator, are trained together.
    At first, after initializing the model weights, the generator creates images that
    do not look realistic. Similarly, the discriminator does a poor job of distinguishing
    between real images and images synthesized by the generator. But over time (that
    is, through training), both networks become better as they interact with each
    other. In fact, the two networks play an adversarial game, where the generator
    learns to improve its output to be able to fool the discriminator. At the same
    time, the discriminator becomes better at detecting the synthesized images.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAN 模型中，生成器和判别器两个网络是一起训练的。最初，在初始化模型权重后，生成器创建的图像看起来不真实。类似地，判别器也难以区分真实图像和生成器合成的图像。但随着时间的推移（即通过训练），两个网络会随着相互作用变得越来越好。实际上，这两个网络玩的是一种对抗性游戏，生成器学习提高其输出，以便能够欺骗判别器。同时，判别器变得更擅长检测合成图像。
- en: Understanding the loss functions of the generator and discriminator networks
    in a GAN model
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 GAN 模型中生成器和判别器网络的损失函数
- en: 'The objective function of GANs, as described in the original paper *Generative
    Adversarial Nets* by Goodfellow et al. ([https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf)),
    is as follows:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GANs 的目标函数，如 Goodfellow 等人在原始论文 *Generative Adversarial Nets* 中所描述的 ([https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf](https://papers.nips.cc/paper/5423-generative-adversarial-nets.pdf))，如下所示：
- en: '![](img/B13208_17_024.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_024.png)'
- en: 'Here, ![](img/B13208_17_025.png) is called the *value function*, which can
    be interpreted as a payoff: we want to maximize its value with respect to the
    discriminator (*D*), while minimizing its value with respect to the generator
    (*G*), that is, ![](img/B13208_17_026.png). *D*(*x*) is the probability that indicates
    whether the input example, *x*, is real or fake (that is, generated). The expression
    ![](img/B13208_17_027.png) refers to the expected value of the quantity in brackets
    with respect to the examples from the data distribution (distribution of the real
    examples); ![](img/B13208_17_028.png) refers to the expected value of the quantity
    with respect to the distribution of the input, *z*, vectors.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，！[](img/B13208_17_025.png)被称为*价值函数*，它可以解释为一种收益：我们希望相对于判别器（*D*）最大化其值，同时相对于生成器（*G*）最小化其值，即！[](img/B13208_17_026.png)。*D*（*x*）是一个概率，表示输入样本*x*是否为真实的（即，是否为生成的）。表达式！[](img/B13208_17_027.png)指的是相对于数据分布（真实样本的分布）的期望值；！[](img/B13208_17_028.png)指的是相对于输入*z*向量分布的期望值。
- en: 'One training step of a GAN model with such a value function requires two optimization
    steps: (1) maximizing the payoff for the discriminator and (2) minimizing the
    payoff for the generator. A practical way of training GANs is to alternate between
    these two optimization steps: (1) fix (freeze) the parameters of one network and
    optimize the weights of the other one, and (2) fix the second network and optimize
    the first one. This process should be repeated at each training iteration. Let''s
    assume that the generator network is fixed, and we want to optimize the discriminator.
    Both terms in the value function ![](img/B13208_17_029.png) contribute to optimizing
    the discriminator, where the first term corresponds to the loss associated with
    the real examples, and the second term is the loss for the fake examples. Therefore,
    when *G* is fixed, our objective is to *maximize* ![](img/B13208_17_030.png),
    which means making the discriminator better at distinguishing between real and
    generated images.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一个包含这种价值函数的GAN模型训练步骤需要两次优化： (1) 最大化判别器的收益，(2) 最小化生成器的收益。训练GAN的一个实用方法是交替进行这两次优化步骤：
    (1) 固定（冻结）一个网络的参数，优化另一个网络的权重，(2) 固定第二个网络并优化第一个网络。这个过程应在每次训练迭代时重复。假设生成器网络被固定，我们想要优化判别器。价值函数中的两项！[](img/B13208_17_029.png)都会对优化判别器产生贡献，其中第一项对应于真实样本的损失，第二项则是假样本的损失。因此，当*G*被固定时，我们的目标是*最大化*！[](img/B13208_17_030.png)，即使判别器更好地区分真实图像和生成图像。
- en: After optimizing the discriminator using the loss terms for real and fake samples,
    we then fix the discriminator and optimize the generator. In this case, only the
    second term in ![](img/B13208_17_031.png) contributes to the gradients of the
    generator. As a result, when *D* is fixed, our objective is to *minimize* ![](img/B13208_17_032.png),
    which can be written as ![](img/B13208_17_033.png). As was mentioned in the original
    GAN paper by Goodfellow et al., this function, ![](img/B13208_17_034.png), suffers
    from vanishing gradients in the early training stages. The reason for this is
    that the outputs, *G*(*z*), early in the learning process, look nothing like real
    examples, and therefore *D*(*G*(*z*)) will be close to zero with high confidence.
    This phenomenon is called *saturation*. To resolve this issue, we can reformulate
    the minimization objective, ![](img/B13208_17_035.png), by rewriting it as ![](img/B13208_17_036.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用真实样本和假样本的损失项优化判别器后，我们接着固定判别器并优化生成器。在这种情况下，只有公式中第二项！[](img/B13208_17_031.png)会对生成器的梯度产生贡献。因此，当*D*被固定时，我们的目标是*最小化*！[](img/B13208_17_032.png)，其表达式可以写为！[](img/B13208_17_033.png)。正如Goodfellow等人在原始GAN论文中提到的，这个函数！[](img/B13208_17_034.png)在训练初期会出现梯度消失问题。其原因在于，在学习过程的初期，*G*（*z*）的输出与真实样本差异很大，因此*D*（*G*（*z*））的值将以很高的置信度接近零。这个现象称为*饱和*。为了解决这个问题，我们可以通过将最小化目标！[](img/B13208_17_035.png)重新写为！[](img/B13208_17_036.png)来重新构造。
- en: This replacement means that for training the generator, we can swap the labels
    of real and fake examples and carry out a regular function minimization. In other
    words, even though the examples synthesized by the generator are fake and are
    therefore labeled 0, we can flip the labels by assigning label 1 to these examples,
    and *minimize* the binary cross-entropy loss with these new labels instead of
    maximizing ![](img/B13208_17_036.png).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这一替换意味着在训练生成器时，我们可以交换真实和伪造样本的标签，并执行常规的函数最小化。换句话说，尽管生成器合成的示例是假的，因此标记为0，但我们可以通过将标签设置为1来翻转标签，并*最小化*这些新标签下的二元交叉熵损失，而不是最大化！[](img/B13208_17_036.png)。
- en: 'Now that we have covered the general optimization procedure for training GAN
    models, let''s explore the various data labels that we can use when training GANs.
    Given that the discriminator is a binary classifier (the class labels are 0 and
    1 for fake and real images, respectively), we can use the binary cross-entropy
    loss function. Therefore, we can determine the ground truth labels for the discriminator
    loss as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了训练GAN模型的常规优化过程，接下来我们来探讨在训练GAN时可以使用的各种数据标签。由于判别器是一个二元分类器（类标签分别为0和1，表示伪造和真实图像），因此我们可以使用二元交叉熵损失函数。因此，我们可以按如下方式确定判别器损失的地面真相标签：
- en: '![](img/B13208_17_037_.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_037_.png)'
- en: What about the labels to train the generator? As we want the generator to synthesize
    realistic images, we want to penalize the generator when its outputs are not classified
    as real by the discriminator. This means that we will assume the ground truth
    labels for the outputs of the generator to be 1 when computing the loss function
    for the generator.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 那么训练生成器的标签应该是什么呢？因为我们希望生成器合成真实的图像，所以当生成器的输出没有被判别器分类为真实时，我们希望对生成器进行惩罚。这意味着我们在计算生成器的损失函数时，将假定生成器输出的地面真相标签为1。
- en: 'Putting all of this together, the following figure displays the individual
    steps in a simple GAN model:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有内容汇总，以下图展示了一个简单GAN模型中的各个步骤：
- en: '![](img/B13208_17_04.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_04.png)'
- en: In the following section, we will implement a GAN from scratch to generate new
    handwritten digits.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将从零开始实现一个GAN，生成新的手写数字。
- en: Implementing a GAN from scratch
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始实现GAN
- en: In this section, we will cover how to implement and train a GAN model to generate
    new images such as MNIST digits. Since the training on a normal central processing
    unit (CPU) may take a long time, in the following subsection, we will cover how
    to set up the Google Colab environment, which will allow us to run the computations
    on graphics processing units (GPUs).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍如何实现和训练一个GAN模型来生成新的图像，比如MNIST数字。由于在普通中央处理单元（CPU）上训练可能需要很长时间，在接下来的子节中，我们将介绍如何设置Google
    Colab环境，这样我们就可以在图形处理单元（GPU）上运行计算。
- en: Training GAN models on Google Colab
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Google Colab上训练GAN模型
- en: Some of the code examples in this chapter may require extensive computational
    resources that go beyond a commercial laptop or a workstation without a GPU. If
    you already have an NVIDIA GPU-enabled computing machine available, with CUDA
    and cuDNN libraries installed, you can use that to speed up the computations.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的一些代码示例可能需要大量计算资源，这些资源超出了普通笔记本电脑或没有GPU的工作站的能力。如果您已经有一台配备NVIDIA GPU的计算机，并且已安装CUDA和cuDNN库，那么您可以使用它来加速计算。
- en: However, since many of us do not have access to high-performance computing resources,
    we will use the Google Colaboratory environment (often referred to as Google Colab),
    which is a free cloud computing service (available in most countries).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于我们很多人没有高性能的计算资源，我们将使用Google Colaboratory环境（通常称为Google Colab），它是一个免费的云计算服务（在大多数国家/地区都可用）。
- en: Google Colab provides Jupyter Notebook instances that run on the cloud; the
    notebooks can be saved on Google Drive or GitHub. While the platform provides
    various different computing resources, such as CPUs, GPUs, and even tensor processing
    units (TPUs), it is important to highlight that the execution time is currently
    limited to 12 hours. Therefore, any notebook running longer than 12 hours will
    be interrupted.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Google Colab提供了基于云运行的Jupyter Notebook实例；这些笔记本可以保存在Google Drive或GitHub上。尽管该平台提供了多种不同的计算资源，如CPU、GPU甚至张量处理单元（TPU），但需要强调的是，执行时间目前限制为12小时。因此，任何运行超过12小时的笔记本将会被中断。
- en: The code blocks in this chapter will need a maximum computing time of two to
    three hours, so this will not be an issue. However, if you decide to use Google
    Colab for other projects that take longer than 12 hours, be sure to use checkpointing
    and save intermediate checkpoints.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的代码块最大需要的计算时间为两到三小时，因此这不会成为问题。然而，如果你决定在其他项目中使用 Google Colab 并且这些项目的运行时间超过
    12 小时，请务必使用检查点并保存中间检查点。
- en: '**Jupyter Notebook**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**Jupyter Notebook**'
- en: Jupyter Notebook is a graphical user interface (GUI) for running code interactively
    and interleaving it with text documentation and figures. Due to its versatility
    and ease of use, it has become one of the most popular tools in data science.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook 是一个图形用户界面（GUI），用于交互式运行代码，并将代码与文本文档和图形交织在一起。由于其多功能性和易用性，它已成为数据科学中最受欢迎的工具之一。
- en: For more information about the general Jupyter Notebook GUI, please view the
    official documentation at [https://jupyter-notebook.readthedocs.io/en/stable/](https://jupyter-notebook.readthedocs.io/en/stable/).
    All the code in this book is also available in the form of Jupyter notebooks,
    and a short introduction can be found in the code directory of the first chapter
    at [https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch01#pythonjupyter-notebook](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch01#pythonjupyter-notebook).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如需了解有关 Jupyter Notebook 图形用户界面的更多信息，请查看官方文档：[https://jupyter-notebook.readthedocs.io/en/stable/](https://jupyter-notebook.readthedocs.io/en/stable/)。本书中的所有代码也以
    Jupyter 笔记本的形式提供，简短的介绍可以在第一章的代码目录中找到：[https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch01#pythonjupyter-notebook](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch01#pythonjupyter-notebook)。
- en: Lastly, we highly recommend Adam Rule et al.'s article *Ten simple rules for
    writing and sharing computational analyses in Jupyter Notebooks* on using Jupyter
    Notebook effectively in scientific research projects, which is freely available
    at [https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们强烈推荐 Adam Rule 等人撰写的文章 *Ten simple rules for writing and sharing computational
    analyses in Jupyter Notebooks*，该文章介绍了如何在科学研究项目中有效使用 Jupyter Notebook，文章可以在[https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007](https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1007007)免费下载。
- en: Accessing Google Colab is very straightforward. You can visit [https://colab.research.google.com](https://colab.research.google.com),
    which automatically takes you to a prompt window where you can see your existing
    Jupyter notebooks. From this prompt window, click the **GOOGLE DRIVE** tab, as
    shown in the following figure. This is where you will save the notebook on your
    Google Drive.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Google Colab 非常简单。你可以访问 [https://colab.research.google.com](https://colab.research.google.com)，该网址会自动带你进入一个提示窗口，在这里你可以看到现有的
    Jupyter 笔记本。在这个提示窗口中，点击**GOOGLE DRIVE**标签，如下图所示。这是你将保存笔记本的地方。
- en: 'Then, to create a new notebook, click on the link **NEW PYTHON 3 NOTEBOOK**
    at the bottom of the prompt window:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，点击提示窗口底部的链接**NEW PYTHON 3 NOTEBOOK**以创建一个新的笔记本：
- en: '![](img/B13208_17_05.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_05.png)'
- en: This will create and open a new notebook for you. All the code examples you
    write in this notebook will be automatically saved, and you can later access the
    notebook from your Google Drive in a directory called Colab Notebooks.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你创建并打开一个新的笔记本。你在此笔记本中编写的所有代码示例将自动保存，你以后可以通过 Google Drive 中名为 Colab Notebooks
    的目录访问该笔记本。
- en: 'In the next step, we want to utilize GPUs to run the code examples in this
    notebook. To do this, from the **Runtime** option in the menu bar of this notebook,
    click on **Change runtime type**and select **GPU**, as shown in the following
    figure:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将利用 GPU 运行本笔记本中的代码示例。为此，在本笔记本的菜单栏中，从**运行时**选项中点击**更改运行时类型**，并选择**GPU**，如图所示：
- en: '![](img/B13208_17_06.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_06.png)'
- en: 'In the last step, we just need to install the Python packages that we will
    need for this chapter. The Colab Notebooks environment already comes with certain
    packages, such as NumPy, SciPy, and the latest stable version of TensorFlow. However,
    at the time of writing, the latest stable version on Google Colab is TensorFlow
    1.15.0, but we want to use TensorFlow 2.0\. Therefore, first we need to install
    TensorFlow 2.0 with GPU support by executing the following command in a new cell
    of this notebook:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，我们只需要安装本章所需的 Python 包。Colab Notebooks 环境已经预装了某些包，如 NumPy、SciPy 和最新的稳定版本的
    TensorFlow。然而，在撰写本文时，Google Colab 上的最新稳定版本是 TensorFlow 1.15.0，但我们希望使用 TensorFlow
    2.0。因此，首先，我们需要通过在 notebook 的新单元格中执行以下命令来安装带 GPU 支持的 TensorFlow 2.0：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: (In a Jupyter notebook, a cell starting with an exclamation mark will be interpreted
    as a Linux shell command.)
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: （在 Jupyter Notebook 中，以感叹号开头的单元格将被解释为 Linux shell 命令。）
- en: 'Now, we can test the installation and verify that the GPU is available using
    the following code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以通过以下代码来测试安装并验证 GPU 是否可用：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Furthermore, if you want to save the model to your personal Google Drive, or
    transfer or upload other files, you need to mount the Google Drive. To do this,
    execute the following in a new cell of the notebook:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果你想将模型保存到个人的 Google Drive，或者传输或上传其他文件，你需要挂载 Google Drive。为此，请在 notebook
    中的新单元格中执行以下操作：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This will provide a link to authenticate the Colab Notebook accessing your Google
    Drive. After following the instructions for authentication, it will provide an
    authentication code that you need to copy and paste into the designated input
    field below the cell you have just executed. Then, your Google Drive will be mounted
    and available at `/content/drive/My Drive`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将提供一个链接，用于验证 Colab Notebook 访问你的 Google Drive。在按照验证步骤操作后，它会提供一个认证代码，你需要将其复制并粘贴到刚才执行的单元格下方的指定输入框中。然后，你的
    Google Drive 将被挂载，并可以在 `/content/drive/My Drive` 位置访问。
- en: Implementing the generator and the discriminator networks
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现生成器和判别器网络
- en: We will start the implementation of our first GAN model with a generator and
    a discriminator as two fully connected networks with one or more hidden layers
    (see the following figure).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过实现一个生成器和判别器的第一版 GAN 模型开始，其中生成器和判别器是两个完全连接的网络，包含一个或多个隐藏层（见下图）。
- en: This is the original GAN version, which we will refer to as *vanilla GAN*.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是原始的 GAN 版本，我们将其称为 *原生 GAN*。
- en: In this model, for each hidden layer, we will apply the leaky ReLU activation
    function. The use of ReLU results in sparse gradients, which may not be suitable
    when we want to have the gradients for the full range of input values. In the
    discriminator network, each hidden layer is also followed by a dropout layer.
    Furthermore, the output layer in the generator uses the hyperbolic tangent (tanh)
    activation function. (Using tanh activation is recommended for the generator network
    since it helps with the learning.)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个模型中，对于每个隐藏层，我们将应用带泄漏的 ReLU 激活函数。ReLU 的使用会导致稀疏梯度，这在我们希望对所有输入值范围的梯度进行计算时可能不太合适。在判别器网络中，每个隐藏层后面还会接一个
    dropout 层。此外，生成器中的输出层使用双曲正切（tanh）激活函数。（推荐在生成器网络中使用 tanh 激活函数，因为它有助于学习过程。）
- en: 'The output layer in the discriminator has no activation function (that is,
    linear activation) to get the logits. Alternatively, we can use the sigmoid activation
    function to get probabilities as output:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器中的输出层没有激活函数（即线性激活）来获取 logits。或者，我们可以使用 sigmoid 激活函数来获得概率作为输出：
- en: '![](img/B13208_17_08.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_08.png)'
- en: '**Leaky rectified linear unit (ReLU) activation function**'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '**带泄漏的修正线性单元（ReLU）激活函数**'
- en: In *Chapter 13*, *Parallelizing Neural Network Training with TensorFlow*, we
    covered different nonlinear activation functions that can be used in an NN model.
    If you recall, the ReLU activation function was defined as ![](img/B13208_17_038_.png),
    which suppresses the negative (preactivation) inputs; that is, negative inputs
    are set to zero. As a consequence, using the ReLU activation function may result
    in sparse gradients during backpropagation. Sparse gradients are not always detrimental
    and can even benefit models for classification. However, in certain applications,
    such as GANs, it can be beneficial to obtain the gradients for the full range
    of input values, which we can achieve by making a slight modification to the ReLU
    function such that it outputs small values for negative inputs. This modified
    version of the ReLU function is also known as *leaky ReLU*. In short, the leaky
    ReLU activation function permits non-zero gradients for negative inputs as well,
    and as a result, it makes the networks more expressive overall.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第 13 章*，*使用 TensorFlow 并行化神经网络训练*，我们讨论了在神经网络模型中可以使用的不同非线性激活函数。如果你还记得，ReLU
    激活函数定义为 ![](img/B13208_17_038_.png)，它会抑制负的（预激活）输入；也就是说，负输入会被设为零。因此，使用 ReLU 激活函数可能会导致反向传播时梯度稀疏。稀疏梯度并不总是有害的，甚至可以对分类模型有益。然而，在某些应用中，例如
    GANs，获取完整输入值范围的梯度是有益的，我们可以通过对 ReLU 函数做小幅修改来实现这一点，使其对负输入也输出小的值。这个修改版本的 ReLU 函数也被称为*泄漏
    ReLU*。简而言之，泄漏 ReLU 激活函数允许负输入也产生非零梯度，因此，它使网络整体上更具表现力。
- en: 'The leaky ReLU activation function is defined as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 泄漏 ReLU 激活函数定义如下：
- en: '![](img/B13208_17_07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_07.png)'
- en: Here, ![](img/B13208_17_039.png) determines the slope for the negative (preactivation)
    inputs.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_17_039.png) 确定了负（预激活）输入的斜率。
- en: 'We will define two helper functions for each of the two networks, instantiate
    a model from the Keras `Sequential` class, and add the layers as described. The
    code is as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将为每个网络定义两个辅助函数，从 Keras `Sequential` 类实例化一个模型，并按描述添加各层。代码如下：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will specify the training settings for the model. As you will remember
    from previous chapters, the image size in the MNIST dataset is ![](img/B13208_17_040.png)
    pixels. (That is only one color channel because MNIST contains only grayscale
    images.) We will further specify the size of the input vector, *z*, to be 20,
    and we will use a random uniform distribution to initialize the model weights.
    Since we are implementing a very simple GAN model for illustration purposes only
    and using fully connected layers, we will only use a single hidden layer with
    100 units in each network. In the following code, we will specify and initialize
    the two networks, and print their summary information:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将为模型指定训练设置。如你所记得，MNIST 数据集中的图像大小是 ![](img/B13208_17_040.png) 像素。（因为 MNIST
    只包含灰度图像，所以只有一个颜色通道。）我们还将进一步指定输入向量 *z* 的大小为 20，并使用随机均匀分布来初始化模型权重。由于我们仅为说明目的实现了一个非常简单的
    GAN 模型，并且使用的是全连接层，所以我们将在每个网络中只使用一个包含 100 个单元的隐藏层。在下面的代码中，我们将指定并初始化这两个网络，并打印它们的摘要信息：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Defining the training dataset
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义训练数据集
- en: 'In the next step, we will load the MNIST dataset and apply the necessary preprocessing
    steps. Since the output layer of the generator is using the tanh activation function,
    the pixel values of the synthesized images will be in the range (–1, 1). However,
    the input pixels of the MNIST images are within the range [0, 255] (with a TensorFlow
    data type `tf.uint8`). Thus, in the preprocessing steps, we will use the `tf.image.convert_image_dtype`
    function to convert the `dtype` of the input image tensors from `tf.uint8` to
    `tf.float32`. As a result, besides changing the `dtype`, calling this function
    will also change the range of input pixel intensities to [0, 1]. Then, we can
    scale them by a factor of 2 and shift them by –1 such that the pixel intensities
    will be rescaled to be in the range [–1, 1]. Furthermore, we will also create
    a random vector, *z*, based on the desired random distribution (in this code example,
    uniform or normal, which are the most common choices), and return both the preprocessed
    image and the random vector in a tuple:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将加载 MNIST 数据集并应用必要的预处理步骤。由于生成器的输出层使用了 tanh 激活函数，合成图像的像素值将在（–1, 1）范围内。然而，MNIST
    图像的输入像素值范围是 [0, 255]（使用 TensorFlow 数据类型 `tf.uint8`）。因此，在预处理步骤中，我们将使用 `tf.image.convert_image_dtype`
    函数将输入图像张量的 `dtype` 从 `tf.uint8` 转换为 `tf.float32`。结果，除了改变 `dtype`，调用此函数还会将输入像素的强度范围更改为
    [0, 1]。然后，我们可以通过一个因子 2 来缩放它们，并将其偏移 –1，使得像素强度重新缩放到 [–1, 1] 范围内。此外，我们还将根据所需的随机分布（在这个代码示例中是均匀分布或正态分布，它们是最常见的选择）创建一个随机向量
    *z*，并返回预处理后的图像和随机向量作为一个元组：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that, here, we returned both the input vector, *z*, and the image to fetch
    the training data conveniently during model fitting. However, this does not imply
    that the vector, *z*, is by any means related to the image—the input image comes
    from the dataset, while vector *z* is generated randomly. In each training iteration,
    the randomly generated vector, *z*, represents the input that the generator receives
    for synthesizing a new image, and the images (the real ones as well as the synthesized
    ones) are the inputs to the discriminator.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这里我们返回了输入向量 *z* 和图像，以便在模型训练过程中方便地获取训练数据。然而，这并不意味着向量 *z* 与图像有任何关系——输入图像来自数据集，而向量
    *z* 是随机生成的。在每次训练迭代中，随机生成的向量 *z* 代表了生成器接收到的输入，用于合成新图像，而图像（无论是真实图像还是合成图像）则是判别器的输入。
- en: Let's inspect the dataset object that we created. In the following code, we
    will take one batch of examples and print the array shapes of this sample of input
    vectors and images. Furthermore, in order to understand the overall data flow
    of our GAN model, in the following code, we will process a forward pass for our
    generator and discriminator.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查一下我们创建的数据集对象。在以下代码中，我们将取一批样本，并打印这一批输入向量和图像的数组形状。此外，为了理解 GAN 模型的整体数据流，在以下代码中，我们将处理一次生成器和判别器的前向传播。
- en: 'First, we will feed the batch of input, *z*, vectors to the generator and get
    its output, `g_output`. This will be a batch of fake examples, which will be fed
    to the discriminator model to get the logits for the batch of fake examples, `d_logits_fake`.
    Furthermore, the processed images that we get from the dataset object will be
    fed to the discriminator model, which will result in the logits for the real examples,
    `d_logits_real`. The code is as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将输入 *z* 向量批次喂入生成器，得到它的输出 `g_output`。这将是一批假样本，将被输入到判别器模型中，以获取这批假样本的 logits，即
    `d_logits_fake`。此外，我们从数据集对象中获取的处理后的图像将被输入到判别器模型中，从而得到真实样本的 logits，即 `d_logits_real`。代码如下：
- en: '[PRE6]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The two logits, `d_logits_fake` and `d_logits_real`, will be used to compute
    the loss functions for training the model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 两个 logits，`d_logits_fake` 和 `d_logits_real`，将用于计算模型训练的损失函数。
- en: Training the GAN model
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练 GAN 模型
- en: 'As the next step, we will create an instance of `BinaryCrossentropy` as our
    loss function and use that to calculate the loss for the generator and discriminator
    associated with the batches that we just processed. To do this, we also need the
    ground truth labels for each output. For the generator, we will create a vector
    of 1s with the same shape as the vector containing the predicted logits for the
    generated images, `d_logits_fake`. For the discriminator loss, we have two terms:
    the loss for detecting the fake examples involving `d_logits_fake` and the loss
    for detecting the real examples based on `d_logits_real`.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步，我们将创建一个`BinaryCrossentropy`的实例作为我们的损失函数，并用它来计算刚刚处理的批次中生成器和鉴别器的损失。为此，我们还需要每个输出的地面真实标签。对于生成器，我们将创建一个与包含生成图像预测logits的向量`d_logits_fake`形状相同的1向量。对于鉴别器损失，我们有两个术语：涉及`d_logits_fake`检测伪例的损失和基于`d_logits_real`检测真实例的损失。
- en: 'The ground truth labels for the fake term will be a vector of 0s that we can
    generate via the `tf.zeros()`(or `tf.zeros_like()`) function. Similarly, we can
    generate the ground truth values for the real images via the `tf.ones()`(or `tf.ones_like()`)
    function, which creates a vector of 1s:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 伪造术语的真实标签将是一个由`tf.zeros()`（或`tf.zeros_like()`）函数生成的零向量。类似地，我们可以通过`tf.ones()`（或`tf.ones_like()`）函数为真实图像生成地面真实值，该函数创建一个由1组成的向量：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The previous code example shows the step-by-step calculation of the different
    loss terms for the purpose of understanding the overall concept behind training
    a GAN model. The following code will set up the GAN model and implement the training
    loop, where we will include these calculations in a `for` loop.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码示例展示了逐步计算不同损失项的过程，以便理解训练GAN模型背后的整体概念。接下来的代码将设置GAN模型并实现训练循环，我们将在`for`循环中包含这些计算。
- en: 'In addition, we will use `tf.GradientTape()` to compute the loss gradients
    with respect to the model weights and optimize the parameters of the generator
    and discriminator using two separate Adam optimizers. As you will see in the following
    code, for alternating between the training of the generator and the discriminator
    in TensorFlow, we explicitly provide the parameters of each network and apply
    the gradients of each network separately to the respective designated optimizer:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，我们将使用`tf.GradientTape()`来计算相对于模型权重的损失梯度，并使用两个单独的Adam优化器来优化生成器和鉴别器的参数。正如您将在以下代码中看到的那样，为了在TensorFlow中交替训练生成器和鉴别器，我们明确地提供了每个网络的参数，并将每个网络的梯度分别应用于各自指定的优化器：
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Using a GPU, the training process that we implemented in the previous code block
    should be completed in less than an hour on Google Colab. (It may even be faster
    on your personal computer if you have a recent and capable CPU and a GPU.) After
    the model training has completed, it is often helpful to plot the discriminator
    and generator losses to analyze the behavior of both subnetworks and assess whether
    they converged.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GPU，在Google Colab上我们实现的训练过程应该在一个小时内完成。（如果您有一台最近和能力强大的CPU和GPU，您的个人电脑上可能会更快。）模型训练完成后，通常有助于绘制鉴别器和生成器损失，以分析两个子网络的行为并评估它们是否收敛。
- en: 'It is also helpful to plot the average probabilities of the batches of real
    and fake examples as computed by the discriminator in each iteration. We expect
    these probabilities to be around 0.5, which means that the discriminator is not
    able to confidently distinguish between real and fake images:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 还有助于绘制鉴别器在每次迭代中计算的真实和伪例子批次的平均概率。我们预计这些概率约为0.5，这意味着鉴别器不能确信地区分真实和伪造图像。
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The following figure shows the results:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了结果：
- en: '![](img/B13208_17_09.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_09.png)'
- en: Note that the discriminator model outputs logits, but for this visualization,
    we already stored the probabilities computed via the sigmoid function before calculating
    the averages for each batch.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，鉴别器模型输出logits，但为了这个可视化，我们已经存储了通过sigmoid函数计算的概率，在计算每批次的平均值之前。
- en: As you can see from the discriminator outputs in the previous figure, during
    the early stages of the training, the discriminator was able to quickly learn
    to distinguish quite accurately between the real and fake examples, that is, the
    fake examples had probabilities close to 0, and the real examples had probabilities
    close to 1\. The reason for that was that the fake examples were nothing like
    the real ones; therefore, distinguishing between real and fake was rather easy.
    As the training proceeds further, the generator will become better at synthesizing
    realistic images, which will result in probabilities of both real and fake examples
    that are close to 0.5.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从前面的判别器输出中看到的，在训练的早期阶段，判别器能够迅速并准确地区分真实和伪造样本，即伪造样本的概率接近0，真实样本的概率接近1。原因是伪造样本与真实样本差异很大，因此区分它们相对容易。随着训练的继续，生成器会越来越擅长合成逼真的图像，这将导致真实和伪造样本的概率都接近0.5。
- en: 'Furthermore, we can also see how the outputs of the generator, that is, the
    synthesized images, change during training. After each epoch, we generated some
    examples by calling the `create_samples()` function and stored them in a Python
    list. In the following code, we will visualize some of the images produced by
    the generator for a selection of epochs:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还可以看到生成器输出（即合成图像）在训练过程中如何变化。在每个周期之后，我们通过调用`create_samples()`函数生成一些样本，并将它们存储在Python列表中。在以下代码中，我们将可视化在不同周期生成器产生的部分图像：
- en: '[PRE10]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following figure shows the produced images:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了生成的图像：
- en: '![](img/B13208_17_10.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_10.png)'
- en: As you can see from the previous figure, the generator network produced more
    and more realistic images as the training progressed. However, even after 100
    epochs, the produced images still look very different to the handwritten digits
    contained in the MNIST dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从前面的图示中看到的，生成器网络随着训练的进行，生成的图像变得越来越真实。然而，即便训练了100个周期，生成的图像与MNIST数据集中包含的手写数字仍然有很大的不同。
- en: In this section, we designed a very simple GAN model with only a single fully
    connected hidden layer for both the generator and discriminator. After training
    the GAN model on the MNIST dataset, we were able to achieve promising, although
    not yet satisfactory, results with the new handwritten digits. As we learned in
    *Chapter 15*, *Classifying Images with Deep Convolutional Neural Networks*, NN
    architectures with convolutional layers have several advantages over fully connected
    layers when it comes to image classification. In a similar sense, adding convolutional
    layers to our GAN model to work with image data might improve the outcome. In
    the next section, we will implement a **deep convolutional GAN** (**DCGAN**),
    which uses convolutional layers for both the generator and the discriminator networks.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们设计了一个非常简单的GAN模型，生成器和判别器都有一个单独的全连接隐藏层。经过在MNIST数据集上的训练，我们能够取得一些有希望的结果，虽然还未达到理想状态，生成了新的手写数字。如我们在*第15章*《*使用深度卷积神经网络进行图像分类*》中学到的，卷积层的神经网络架构在图像分类方面，相比全连接层有许多优势。类似地，向我们的GAN模型中添加卷积层来处理图像数据，可能会改善结果。在下一节中，我们将实现一个**深度卷积GAN**（**DCGAN**），该模型为生成器和判别器网络都使用卷积层。
- en: Improving the quality of synthesized images using a convolutional and Wasserstein
    GAN
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积和Wasserstein GAN提高合成图像的质量
- en: In this section, we will implement a DCGAN, which will enable us to improve
    the performance we saw in the previous GAN example. Additionally, we will employ
    several extra key techniques and implement a **Wasserstein GAN** (**WGAN**).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现一个DCGAN，这将使我们能够提高在前一个GAN示例中看到的性能。此外，我们还将采用一些额外的关键技术，并实现一个**Wasserstein
    GAN**（**WGAN**）。
- en: 'The techniques that we will cover in this section will include the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中，我们将介绍以下技术：
- en: Transposed convolution
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转置卷积
- en: BatchNorm
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量归一化（BatchNorm）
- en: WGAN
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserstein GAN（WGAN）
- en: Gradient penalty
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度惩罚
- en: The DCGAN was proposed in 2016 by A. Radford, L. Metz, and S. Chintala in their
    article *Unsupervised representation learning with deep convolutional generative
    adversarial networks*, which is freely available at [https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf).
    In this article, the researchers proposed using convolutional layers for both
    the generator and discriminator networks. Starting from a random vector, *z*,
    the DCGAN first uses a fully connected layer to project *z* into a new vector
    with a proper size so that it can be reshaped into a spatial convolution representation
    (![](img/B13208_17_041.png)), which is smaller than the output image size. Then,
    a series of convolutional layers, known as **transposed convolution**, are used
    to upsample the feature maps to the desired output image size.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: DCGAN于2016年由A. Radford、L. Metz和S. Chintala在其文章《*无监督表示学习与深度卷积生成对抗网络*》中提出，文章可以在[https://arxiv.org/pdf/1511.06434.pdf](https://arxiv.org/pdf/1511.06434.pdf)免费获取。在这篇文章中，研究人员提出在生成器和判别器网络中都使用卷积层。从随机向量*z*开始，DCGAN首先使用全连接层将*z*投影到一个新的向量，使其具有适当的大小，以便可以将其重塑为空间卷积表示(![](img/B13208_17_041.png))，该表示比输出图像大小要小。然后，使用一系列卷积层，称为**转置卷积**，来将特征图上采样到所需的输出图像大小。
- en: Transposed convolution
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 转置卷积
- en: In *Chapter 15*, *Classifying Images with Deep Convolutional Neural Networks*,
    you learned about the convolution operation in one- and two-dimensional spaces.
    In particular, we looked at how the choices for the padding and strides change
    the output feature maps. While a convolution operation is usually used to downsample
    the feature space (for example, by setting the stride to 2, or by adding a pooling
    layer after a convolutional layer), a *transposed convolution* operation is usually
    used for *upsampling* the feature space.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第15章*，*使用深度卷积神经网络进行图像分类*中，你学习了在一维和二维空间中的卷积操作。特别地，我们探讨了填充和步幅的选择如何改变输出特征图。虽然卷积操作通常用于对特征空间进行下采样（例如，通过将步幅设置为2，或在卷积层后添加池化层），但*转置卷积*操作通常用于*上采样*特征空间。
- en: 'To understand the transposed convolution operation, let''s go through a simple
    thought experiment. Assume that we have an input feature map of size ![](img/B13208_17_042.png).
    Then, we apply a 2D convolution operation with certain padding and stride parameters
    to this ![](img/B13208_17_043.png) input, resulting in an output feature map of
    size ![](img/B13208_17_044.png). Now, the question is, how we can apply another
    convolution operation to obtain a feature map with the initial dimension ![](img/B13208_17_045.png)
    from this ![](img/B13208_17_046.png) output feature map while maintaining the
    connectivity patterns between the input and output? Note that only the shape of
    the ![](img/B13208_17_047.png) input matrix is recovered and not the actual matrix
    values. This is what transposed convolution does, as shown in the following figure:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解转置卷积操作，让我们通过一个简单的思想实验。假设我们有一个大小为![](img/B13208_17_042.png)的输入特征图。然后，我们对这个![](img/B13208_17_043.png)输入应用一个带有特定填充和步幅参数的二维卷积操作，得到一个大小为![](img/B13208_17_044.png)的输出特征图。现在，问题是，我们如何应用另一个卷积操作，从这个![](img/B13208_17_046.png)输出特征图中获得具有初始维度![](img/B13208_17_045.png)的特征图，同时保持输入和输出之间的连接模式？请注意，只有![](img/B13208_17_047.png)输入矩阵的形状被恢复，而不是实际的矩阵值。这正是转置卷积的作用，如下图所示：
- en: '![](img/B13208_17_11.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_11.png)'
- en: '**Transposed convolution versus deconvolution**'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '**转置卷积与反卷积**'
- en: Transposed convolution is also called *fractionally strided convolution*. In
    deep learning literature, another common term that is used to refer to transposed
    convolution is *deconvolution*. However, note that deconvolution was originally
    defined as the inverse of a convolution operation, *f*, on a feature map, *x*,
    with weight parameters, *w*, producing feature map ![](img/B13208_17_048.png),
    ![](img/B13208_17_049.png). A deconvolution function, ![](img/B13208_17_050.png),
    can then be defined as ![](img/B13208_17_051.png). However, note that the transposed
    convolution is merely focused on recovering the dimensionality of the feature
    space and not the actual values.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 转置卷积也叫做*分数步幅卷积*。在深度学习文献中，另一个常用的术语是*反卷积*，用于指代转置卷积。然而，值得注意的是，反卷积最初被定义为卷积操作*f*的逆操作，它作用于特征图*x*，并与权重参数*w*结合，产生特征图![](img/B13208_17_048.png)，![](img/B13208_17_049.png)。然后，反卷积函数![](img/B13208_17_050.png)可以定义为![](img/B13208_17_051.png)。然而，转置卷积仅关注恢复特征空间的维度，而非实际的数值。
- en: 'Upsampling feature maps using transposed convolution works by inserting 0s
    between the elements of the input feature maps. The following illustration shows
    an example of applying transposed convolution to an input of size ![](img/B13208_17_052.png),
    with a stride of ![](img/B13208_17_053.png) and kernel size of ![](img/B13208_17_054.png).
    The matrix of size ![](img/B13208_17_055.png) in the center shows the results
    after inserting such 0s into the input feature map. Then, performing a normal
    convolution using the ![](img/B13208_17_056.png) kernel with a stride of 1 results
    in an output of size ![](img/B13208_17_057.png). We can verify the backward direction
    by performing a regular convolution on the output with a stride of 2, which results
    in an output feature map of size ![](img/B13208_17_058.png), which is the same
    as the original input size:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 使用转置卷积进行特征图上采样，通过在输入特征图的元素之间插入0来工作。下图显示了应用转置卷积于大小为 ![](img/B13208_17_052.png)
    的输入的示例，步幅为 ![](img/B13208_17_053.png)，卷积核大小为 ![](img/B13208_17_054.png)。中间的 ![](img/B13208_17_055.png)
    大小矩阵显示了将0插入输入特征图后的结果。然后，使用步幅为1的 ![](img/B13208_17_056.png) 卷积核进行常规卷积，得到大小为 ![](img/B13208_17_057.png)
    的输出。我们可以通过对输出进行步幅为2的常规卷积来验证反向方向，从而得到大小为 ![](img/B13208_17_058.png) 的输出特征图，这与原始输入大小相同：
- en: '![](img/B13208_17_12.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_12.png)'
- en: The preceding illustration shows how transposed convolution works in general.
    There are various cases in which input size, kernel size, strides, and padding
    variations can change the output. If you want to learn more about all these different
    cases, refer to the tutorial *A Guide to Convolution Arithmetic for Deep Learning*
    by Vincent Dumoulin and Francesco Visin, which is freely available at [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 上述插图展示了转置卷积的一般工作原理。在各种情况下，输入大小、卷积核大小、步幅和填充变化可能会改变输出。如果您想了解所有这些不同情况的更多信息，请参考
    Vincent Dumoulin 和 Francesco Visin 撰写的教程 *A Guide to Convolution Arithmetic for
    Deep Learning*，该教程可以在 [https://arxiv.org/pdf/1603.07285.pdf](https://arxiv.org/pdf/1603.07285.pdf)
    上免费获得。
- en: Batch normalization
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 批量归一化
- en: '**BatchNorm** was introduced in 2015 by Sergey Ioffe and Christian Szegedy
    in the article *Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift*, which you can access via arXiv at [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf).
    One of the main ideas behind BatchNorm is normalizing the layer inputs and preventing
    changes in their distribution during training, which enables faster and better
    convergence.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**BatchNorm** 是由 Sergey Ioffe 和 Christian Szegedy 于2015年在文章 *Batch Normalization:
    Accelerating Deep Network Training by Reducing Internal Covariate Shift* 中提出的，您可以通过
    arXiv 在 [https://arxiv.org/pdf/1502.03167.pdf](https://arxiv.org/pdf/1502.03167.pdf)
    访问这篇文章。BatchNorm 背后的主要思想之一是对层的输入进行归一化，并防止训练过程中其分布的变化，这使得训练更快且收敛效果更好。'
- en: 'BatchNorm transforms a mini-batch of features based on its computed statistics.
    Assume that we have the net preactivation feature maps obtained after a convolutional
    layer in a four-dimensional tensor, *Z*, with the shape ![](img/B13208_17_059.png),
    where *m* is the number of examples in the batch (i.e., batch size), ![](img/B13208_17_060.png)
    is the spatial dimension of the feature maps, and *c* is the number of channels.
    BatchNorm can be summarized in three steps, as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: BatchNorm 根据计算出的统计数据转换一个小批次的特征。假设我们有一个四维张量 *Z*，它是在卷积层之后得到的净激活特征图，其形状为 ![](img/B13208_17_059.png)，其中
    *m* 是批次中的样本数（即批次大小），![](img/B13208_17_060.png) 是特征图的空间维度，*c* 是通道数。BatchNorm 可以总结为三个步骤，如下所示：
- en: 'Compute the mean and standard deviation of the net inputs for each mini-batch:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个小批次的净输入的均值和标准差：
- en: '![](img/B13208_17_061.png), ![](img/B13208_17_062.png), where ![](img/B13208_17_063.png)
    and ![](img/B13208_17_064.png) both have size *c*.'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![](img/B13208_17_061.png)，![](img/B13208_17_062.png)，其中 ![](img/B13208_17_063.png)
    和 ![](img/B13208_17_064.png) 都的大小为 *c*。'
- en: 'Standardize the net inputs for all examples in the batch: ![](img/B13208_17_065.png),
    where ![](img/B13208_17_066.png) is a small number for numerical stability (that
    is, to avoid division by zero).'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化批次中所有样本的净输入：![](img/B13208_17_065.png)，其中 ![](img/B13208_17_066.png) 是一个小数值，用于数值稳定性（即避免除以零）。
- en: 'Scale and shift the normalized net inputs using two learnable parameter vectors,
    ![](img/B13208_17_067.png) and ![](img/B13208_17_068.png), of size *c* (number
    of channels): ![](img/B13208_17_069.png).'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用两个可学习的参数向量 ![](img/B13208_17_067.png) 和 ![](img/B13208_17_068.png) 对归一化后的净输入进行缩放和平移，它们的大小为
    *c*（通道数）：![](img/B13208_17_069.png)。
- en: 'The following figure illustrates the process:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示说明了这个过程：
- en: '![](img/B13208_17_13.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_13.png)'
- en: In the first step of BatchNorm, the mean, ![](img/B13208_17_070.png), and standard
    deviation, ![](img/B13208_17_071.png), of the mini-batch are computed. Both ![](img/B13208_17_072.png)
    and ![](img/B13208_17_073.png) are vectors of size *c* (where *c* is the number
    of channels). Then, these statistics are used in step 2 to scale the examples
    in each mini-batch via z-score normalization (standardization), resulting in standardized
    net inputs, ![](img/B13208_17_074.png). As a consequence, these net inputs are
    mean-centered and have *unit variance*, which is generally a useful property for
    gradient descent-based optimization. On the other hand, always normalizing the
    net inputs such that they have the same properties across the different mini-batches,
    which can be diverse, can severely impact the representational capacity of NNs.
    This can be understood by considering a feature, ![](img/B13208_17_075.png), which,
    after sigmoid activation to ![](img/B13208_17_076.png), results in a linear region
    for values close to 0\. Therefore, in step 3, the learnable parameters, ![](img/B13208_17_077.png)
    and ![](img/B13208_17_078.png), which are vectors of size *c* (number of channels),
    allow BatchNorm to control the shift and spread of the normalized features.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在 BatchNorm 的第一步中，会计算小批量的均值，![](img/B13208_17_070.png)，和标准差，![](img/B13208_17_071.png)。这两个值，![](img/B13208_17_072.png)
    和 ![](img/B13208_17_073.png)，都是大小为 *c* 的向量（其中 *c* 是通道数）。接着，在第二步中，这些统计量用于通过 z-score
    标准化（标准化）来缩放每个小批量中的示例，从而得到标准化后的网络输入，![](img/B13208_17_074.png)。因此，这些网络输入是均值中心化的，并且具有
    *单位方差*，这通常是基于梯度下降优化的有用特性。另一方面，始终将网络输入标准化，使得它们在不同的小批量中具有相同的特性，而这些小批量可能是多样化的，这可能会严重影响神经网络的表征能力。这可以通过考虑一个特征，![](img/B13208_17_075.png)，它经过
    sigmoid 激活后变成 ![](img/B13208_17_076.png)，导致对于接近 0 的值出现线性区域来理解。因此，在第三步中，可学习的参数，![](img/B13208_17_077.png)
    和 ![](img/B13208_17_078.png)，它们是大小为 *c* 的向量（通道数），使得 BatchNorm 可以控制归一化特征的偏移和扩展。
- en: During training, the running averages, ![](img/B13208_17_079.png), and running
    variance, ![](img/B13208_17_080.png), are computed, which are used along with
    the tuned parameters, ![](img/B13208_17_081.png) and ![](img/B13208_17_082.png),
    to normalize the test example(s) at evaluation.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，会计算运行平均值，![](img/B13208_17_079.png)，和运行方差，![](img/B13208_17_080.png)，这些值将与调优后的参数，![](img/B13208_17_081.png)
    和 ![](img/B13208_17_082.png)，一起用于在评估时归一化测试示例。
- en: '**Why does BatchNorm help optimization?**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么 BatchNorm 有助于优化？**'
- en: Initially, BatchNorm was developed to reduce the so-called *internal covariance
    shift*, which is defined as the changes that occur in the distribution of a layer's
    activations due to the updated network parameters during training.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，BatchNorm 是为了减少所谓的 *内部协方差偏移*，即由于训练过程中更新的网络参数而导致的某一层激活值分布的变化。
- en: To explain this with a simple example, consider a fixed batch that passes through
    the network at epoch 1\. We record the activations of each layer for this batch.
    After iterating through the whole training dataset and updating the model parameters,
    we start the second epoch, where the previously fixed batch passes through the
    network. Then, we compare the layer activations from the first and second epochs.
    Since the network parameters have changed, we observe that the activations have
    also changed. This phenomenon is called the internal covariance shift, which was
    believed to decelerate NN training.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个简单的例子来解释，考虑一个固定的批次，它在第 1 轮通过网络。我们记录这个批次在每一层的激活值。经过遍历整个训练数据集并更新模型参数后，我们开始第二轮训练，这时之前固定的批次再次通过网络。然后，我们将第一轮和第二轮的层激活值进行比较。由于网络参数发生了变化，我们观察到激活值也发生了变化。这种现象被称为内部协方差偏移，曾被认为会减缓神经网络的训练。
- en: However, in 2018, S. Santurkar, D. Tsipras, A. Ilyas, and A. Madry further investigated
    what makes BatchNorm so effective. In their study, the researchers observed that
    the effect of BatchNorm on the internal covariance shift is marginal. Based on
    the outcome of their experiments, they hypothesized that the effectiveness of
    BatchNorm is, instead, based on a smoother surface of the loss function, which
    makes the non-convex optimization more robust.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在 2018 年，S. Santurkar、D. Tsipras、A. Ilyas 和 A. Madry 进一步研究了 BatchNorm 为什么如此有效。在他们的研究中，研究人员观察到
    BatchNorm 对内部协方差偏移的影响是微不足道的。根据实验结果，他们假设 BatchNorm 的有效性实际上是基于损失函数的平滑表面，这使得非凸优化更加稳健。
- en: If you are interested in learning more about these results, read through the
    original paper, *How Does Batch Normalization Help Optimization?*, which is freely
    available at [http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf](http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对进一步了解这些结果感兴趣，可以阅读原始论文《*Batch Normalization如何帮助优化？*》，该论文可以在[http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf](http://papers.nips.cc/paper/7515-how-does-batch-normalization-help-optimization.pdf)免费获取。
- en: The TensorFlow Keras API provides a class, `tf.keras.layers.BatchNormalization()`,
    that we can use as a layer when defining our models; it will perform all of the
    steps that we described for BatchNorm. Note that the behavior for updating the
    learnable parameters, ![](img/B13208_17_083.png) and ![](img/B13208_17_068.png),
    depends on whether `training=False` or `training=True`, which can be used to ensure
    that these parameters are learned only during training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Keras API提供了一个类`tf.keras.layers.BatchNormalization()`，我们可以在定义模型时作为一层使用；它将执行我们描述的所有BatchNorm步骤。请注意，更新可学习参数![](img/B13208_17_083.png)和![](img/B13208_17_068.png)的行为取决于`training=False`还是`training=True`，这可以确保这些参数只在训练期间进行学习。
- en: Implementing the generator and discriminator
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现生成器和判别器
- en: At this point, we have covered the main components of a DCGAN model, which we
    will now implement. The architectures of the generator and discriminator networks
    are summarized in the following two figures.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了DCGAN模型的主要组件，接下来我们将实现这些组件。生成器和判别器网络的架构总结在以下两个图中。
- en: 'The generator takes a vector, *z*, of size 20 as input, applies a fully connected
    (dense) layer to increase its size to 6,272 and then reshapes it into a rank-3
    tensor of shape ![](img/B13208_17_085.png) (spatial dimension ![](img/B13208_17_086.png)
    and 128 channels). Then, a series of transposed convolutions using `tf.keras.layers.Conv2DTransposed()`
    upsamples the feature maps until the spatial dimension of the resulting feature
    maps reaches ![](img/B13208_17_087.png). The number of channels is reduced by
    half after each transposed convolutional layer, except the last one, which uses
    only one output filter to generate a grayscale image. Each transposed convolutional
    layer is followed by BatchNorm and leaky ReLU activation functions, except the
    last one, which uses tanh activation (without BatchNorm). The architecture for
    the generator (the feature maps after each layer) is shown in the following figure:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器以一个大小为20的向量*z*作为输入，应用全连接（密集）层将其大小增加到6,272，然后将其重塑为一个形状为![](img/B13208_17_085.png)（空间维度![](img/B13208_17_086.png)和128个通道）的3阶张量。接着，使用`tf.keras.layers.Conv2DTransposed()`进行一系列转置卷积操作，直到生成的特征图的空间维度达到![](img/B13208_17_087.png)。每个转置卷积层后，通道数减半，除了最后一层，它仅使用一个输出滤波器生成灰度图像。每个转置卷积层后跟随BatchNorm和leaky
    ReLU激活函数，除了最后一层，它使用tanh激活函数（不使用BatchNorm）。生成器的架构（每层之后的特征图）如下图所示：
- en: '![](img/B13208_17_14.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_14.png)'
- en: 'The discriminator receives images of size ![](img/B13208_17_088.png), which
    are passed through four convolutional layers. The first three convolutional layers
    reduce the spatial dimensionality by 4 while increasing the number of channels
    of the feature maps. Each convolutional layer is also followed by BatchNorm, leaky
    ReLU activation, and a dropout layer with `rate=0.3` (drop probability). The last
    convolutional layer uses kernels of size ![](img/B13208_17_089.png) and a single
    filter to reduce the spatial dimensionality of the output to ![](img/B13208_17_090.png):'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器接收大小为![](img/B13208_17_088.png)的图像，并通过四个卷积层进行处理。前三个卷积层将空间维度降低4倍，同时增加特征图的通道数。每个卷积层后面也跟着BatchNorm、leaky
    ReLU激活函数和一个丢弃层，丢弃率为`rate=0.3`（丢弃概率）。最后一个卷积层使用大小为![](img/B13208_17_089.png)的卷积核和一个滤波器，将输出的空间维度减少到![](img/B13208_17_090.png)：
- en: '![](img/B13208_17_15.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_15.png)'
- en: '**Architecture design considerations for convolutional GANs**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积GAN的架构设计考虑**'
- en: Notice that the number of feature maps follows different trends between the
    generator and the discriminator. In the generator, we start with a large number
    of feature maps and decrease them as we progress toward the last layer. On the
    other hand, in the discriminator, we start with a small number of channels and
    increase it toward the last layer. This is an important point for designing CNNs
    with the number of feature maps and the spatial size of the feature maps in reverse
    order. When the spatial size of the feature maps increases, the number of feature
    maps decreases and vice versa.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，生成器和判别器之间的特征图数量趋势是不同的。在生成器中，我们从大量的特征图开始，并随着接近最后一层时逐渐减少。而在判别器中，我们从少量的通道开始，并向最后一层逐步增加。这是设计CNN时一个重要的要点，特征图数量和特征图的空间大小是反向排列的。当特征图的空间大小增大时，特征图的数量减少，反之亦然。
- en: In addition, note that it's usually not recommended to use bias units in the
    layer that follows a BatchNorm layer. Using bias units would be redundant in this
    case, since BatchNorm already has a shift parameter, ![](img/B13208_17_081.png).
    You can omit the bias units for a given layer by setting `use_bias=False` in t`f.keras.layers.Dense`
    or `tf.keras.layers.Conv2D`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，注意通常不建议在BatchNorm层之后的层中使用偏置单元。使用偏置单元在这种情况下是多余的，因为BatchNorm已经有一个平移参数，![](img/B13208_17_081.png)。你可以通过在`tf.keras.layers.Dense`或`tf.keras.layers.Conv2D`中设置`use_bias=False`来省略给定层的偏置单元。
- en: 'The code for two helper functions to make the generator and discriminator networks
    is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 用于创建生成器和判别器网络的两个辅助函数的代码如下：
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: With these two helper functions, you can build a DCGAN model and train it by
    using the same MNIST dataset object we initialized in the previous section when
    we implemented the simple, fully connected GAN. Also, we can use the same loss
    functions and training procedure as before.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这两个辅助函数，你可以构建一个DCGAN模型，并使用我们在上一节中初始化的相同MNIST数据集对象来训练它，当时我们实现了简单的全连接GAN。此外，我们可以像之前一样使用相同的损失函数和训练过程。
- en: 'We will be making a few additional modifications to the DCGAN model in the
    remaining sections of this chapter. Note that the `preprocess()` function for
    transforming the dataset must change to output an image tensor instead of flattening
    the image to a vector. The following code shows the necessary modifications to
    build the dataset, as well as creating the new generator and discriminator networks:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将对DCGAN模型进行一些额外的修改。请注意，`preprocess()`函数用于转换数据集时，必须更改为输出图像张量，而不是将图像展平为向量。以下代码显示了构建数据集所需的修改，以及创建新的生成器和判别器网络：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can create the generator networks using the helper function, `make_dcgan_generator()`,
    and print its architecture as follows:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用辅助函数`make_dcgan_generator()`来创建生成器网络，并按如下方式打印其架构：
- en: '[PRE13]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Similarly, we can generate the discriminator network and see its architecture:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以生成判别器网络并查看其架构：
- en: '[PRE14]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Notice that the number of parameters for the BatchNorm layers is indeed four
    times the number of channels (![](img/B13208_17_091.png)). Remember that the BatchNorm
    parameters, ![](img/B13208_17_070.png) and ![](img/B13208_17_093.png), represent
    the (non-trainable parameters) mean and standard deviation for each feature value
    inferred from a given batch; ![](img/B13208_17_094.png) and ![](img/B13208_17_068.png)
    are the trainable BN parameters.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，BatchNorm层的参数数量确实是通道数的四倍（![](img/B13208_17_091.png)）。记住，BatchNorm参数，![](img/B13208_17_070.png)
    和 ![](img/B13208_17_093.png)，表示从给定批次推断出的每个特征值的（非可训练参数）均值和标准差；![](img/B13208_17_094.png)
    和 ![](img/B13208_17_068.png) 是可训练的BN参数。
- en: Note that this particular architecture would not perform very well when using
    cross-entropy as a loss function.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这种特定的架构在使用交叉熵作为损失函数时表现并不好。
- en: In the next subsection, we will cover WGAN, which uses a modified loss function
    based on the so-called Wasserstein-1 (or earth mover's) distance between the distributions
    of real and fake images for improving the training performance.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将介绍WGAN，它使用基于所谓的Wasserstein-1（或地球移动者）距离的修改版损失函数，来改进真实图像和假图像分布之间的训练性能。
- en: Dissimilarity measures between two distributions
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两个分布之间的相异度度量
- en: We will first see different measures for computing the divergence between two
    distributions. Then, we will see which one of these measures is already embedded
    in the original GAN model. Finally, switching this measure in GANs will lead us
    to the implementation of a WGAN.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先看到几种计算两个分布之间散度的方法。接着，我们将看到这些方法中哪一种已经嵌入到原始的GAN模型中。最后，在GAN中切换这种度量将引导我们实现WGAN。
- en: As mentioned at the beginning of this chapter, the goal of a generative model
    is to learn how to synthesize new samples that have the same distribution as the
    distribution of the training dataset. Let *P*(*x*) and *Q*(*x*) represent the
    distribution of a random variable, *x*, as shown in the following figure.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所述，生成模型的目标是学习如何合成新的样本，这些样本与训练数据集的分布相同。让*P*(*x*)和*Q*(*x*)表示随机变量*x*的分布，如下图所示。
- en: 'First, let''s look at some ways, shown in the following figure, that we can
    use to measure the dissimilarity between two distributions, *P* and *Q*:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，看看下图所示的几种方法，我们可以用来衡量两个分布之间的差异，*P*和*Q*：
- en: '![](img/B13208_17_16.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_16.png)'
- en: 'The function supremum, sup(*S*), used in the total variation (TV) measure,
    refers to the smallest value that is greater than all elements of *S*. In other
    words, sup(*S*) is the least upper bound for *S*. Vice versa, the infimum function,
    inf(*S*), which is used in EM distance, refers to the largest value that is smaller
    than all elements of *S* (the greatest lower bound). Let''s gain an understanding
    of these measures by briefly stating what they are trying to accomplish in simple
    words:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在总变差（TV）度量中使用的上确界函数，sup(*S*)，指的是大于集合*S*中所有元素的最小值。换句话说，sup(*S*)是*S*的最小上界。反之，EM距离中使用的下确界函数，inf(*S*)，指的是小于集合*S*中所有元素的最大值（即最大下界）。让我们通过简单的语言简要了解这些度量方法的作用：
- en: The first one, TV distance, measures the largest difference between the two
    distributions at each point.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种，TV距离，衡量的是两个分布在每个点上的最大差异。
- en: The EM distance can be interpreted as the minimal amount of work needed to transform
    one distribution into the other. The infimum function in the EM distance is taken
    over ![](img/B13208_17_096.png), which is the collection of all joint distributions
    whose marginals are *P* or *Q*. Then, ![](img/B13208_17_097.png) is a transfer
    plan, which indicates how we redistribute the earth from location *u* to *v*,
    subject to some constraints for maintaining valid distributions after such transfers.
    Computing EM distance is an optimization problem by itself, which is to find the
    optimal transfer plan, ![](img/B13208_17_098.png).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EM距离可以理解为将一个分布转化为另一个分布所需的最小工作量。EM距离中的下确界函数是在！[](img/B13208_17_096.png)上取值的，这个集合包含了所有其边际分布为*P*或*Q*的联合分布。然后，！[](img/B13208_17_097.png)是一个传输计划，表示我们如何在地理位置*u*和*v*之间重新分配地球资源，同时确保在这种转移后分布仍然有效。计算EM距离本身是一个优化问题，目标是找到最优的传输计划，！[](img/B13208_17_098.png)。
- en: The Kullback-Leibler (KL) and Jensen-Shannon (JS) divergence measures come from
    the field of information theory. Note that KL divergence is not symmetric, that
    is, ![](img/B13208_17_099.png) in contrast to JS divergence.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kullback-Leibler（KL）散度和Jensen-Shannon（JS）散度源自信息论领域。请注意，KL散度不是对称的，即与JS散度不同，！[](img/B13208_17_099.png)。
- en: 'The dissimilarity equations provided in the previous figure correspond to continuous
    distributions but can be extended for discrete cases. An example of calculating
    these different dissimilarity measures with two simple discrete distributions
    is illustrated in the following figure:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上一图中提供的差异度量公式适用于连续分布，但也可以扩展到离散情况。以下图展示了计算这几种不同差异度量方法的例子，使用了两个简单的离散分布：
- en: '![](img/B13208_17_17.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_17.png)'
- en: Note that, in the case of the EM distance, for this simple example, we can see
    that *Q*(*x*) at *x* = 2 has the excess value of ![](img/B13208_17_100.png), while
    the value of *Q* at the other two *x*'s is below 1/3\. Therefore, the minimal
    amount of work is when we transfer the extra value at *x* = 2 to *x* = 1 and *x*
    = 3, as shown in the previous figure. For this simple example, it's easy to see
    that these transfers will result in the minimal amount of work out of all possible
    transfers. However, this may be infeasible to do for more complex cases.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在EM距离的情况下，对于这个简单的例子，我们可以看到在 *x* = 2 时，*Q*(*x*) 的过剩值为 ![](img/B13208_17_100.png)，而其他两个
    *x* 的 *Q* 值低于1/3\. 因此，最小的工作量是在 *x* = 2 时将额外的值转移到 *x* = 1 和 *x* = 3，如前图所示。对于这个简单的例子，很容易看出这些转移将导致所有可能转移中的最小工作量。然而，对于更复杂的情况，这种方法可能不可行。
- en: '**The relationship between KL divergence and cross-entropy**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**KL散度与交叉熵之间的关系**'
- en: KL divergence, ![](img/B13208_17_101.png), measures the relative entropy of
    the distribution, *P*, with respect to a reference distribution, *Q*. The formulation
    for KL divergence can be extended as
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度，![](img/B13208_17_101.png)，衡量分布 *P* 相对于参考分布 *Q* 的相对熵。KL散度的公式可以扩展为
- en: '![](img/B13208_17_102.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_102.png)'
- en: Moreover, for discrete distributions, KL divergence can be written as
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，对于离散分布，KL散度可以写成
- en: '![](img/B13208_17_103.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_103.png)'
- en: which can be similarly extended as
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 可以类似地扩展为
- en: '![](img/B13208_17_104.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_104.png)'
- en: Based on the extended formulation (either discrete or continuous), KL divergence
    is viewed as the cross-entropy between *P* and *Q* (the first term in the preceding
    equation) subtracted by the (self-) entropy of *P* (second term), that is, ![](img/B13208_17_105.png).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 基于扩展的公式（无论是离散的还是连续的），KL散度被视为 *P* 和 *Q* 之间的交叉熵（前面公式中的第一项），减去 *P* 的（自）熵（第二项），即
    ![](img/B13208_17_105.png)。
- en: Now, going back to our discussion of GANs, let's see how these different distance
    measures are related to the loss function for GANs. It can be mathematically shown
    that the loss function in the original GAN indeed *minimizes the JS divergence
    between the distribution of real and fake examples*. But, as discussed in an article
    by Martin Arjovsky et al. (*Wasserstein Generative Adversarial Networks*, [http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf](http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf)),
    JS divergence has problems training a GAN model, and therefore, in order to improve
    the training, the researchers proposed using the EM distance as a measure of dissimilarity
    between the distribution of real and fake examples.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，回到我们对GAN的讨论，让我们看看这些不同的距离度量如何与GAN的损失函数相关。可以通过数学证明，原始GAN中的损失函数确实*最小化了真实样本和假样本分布之间的JS散度*。但是，正如Martin
    Arjovsky等人（*Wasserstein Generative Adversarial Networks*， [http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf](http://proceedings.mlr.press/v70/arjovsky17a/arjovsky17a.pdf)）在一篇文章中讨论的那样，JS散度在训练GAN模型时存在问题，因此，为了改善训练，研究人员提出使用EM距离作为衡量真实样本和假样本分布之间相异性的度量。
- en: '**What is the advantage of using EM distance?**'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用EM距离的优势是什么？**'
- en: To answer this question, we can consider an example that was given in the article
    by Martin Arjovsky et al, titled *Wasserstein GAN*. To put it in simple words,
    assume we have two distributions, *P* and *Q*, which are two parallel lines. One
    line is fixed at *x* = 0 and the other line can move across the *x*-axis but is
    initially located at ![](img/B13208_17_106.png), where ![](img/B13208_17_107.png).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回答这个问题，我们可以考虑Martin Arjovsky等人文章中给出的一个例子，标题为 *Wasserstein GAN*。简单来说，假设我们有两个分布
    *P* 和 *Q*，它们是两条平行线。一条线固定在 *x* = 0，另一条线可以沿 *x*-轴移动，但最初位于 ![](img/B13208_17_106.png)，其中
    ![](img/B13208_17_107.png)。
- en: It can be shown that the KL, TV, and JS dissimilarity measures are ![](img/B13208_17_108.png),
    ![](img/B13208_17_109.png), and ![](img/B13208_17_110.png). None of these dissimilarity
    measures are a function of the parameter ![](img/B13208_17_111.png), and therefore,
    they cannot be differentiated with respect to ![](img/B13208_17_112.png) toward
    making the distributions, *P* and *Q*, become similar to each other. On the other
    hand, the EM distance is ![](img/B13208_17_113.png), whose gradient with respect
    to ![](img/B13208_17_114.png) exists and can push *Q* toward *P*.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 可以证明，KL、TV和JS不相似度度量是 ![](img/B13208_17_108.png)、![](img/B13208_17_109.png) 和
    ![](img/B13208_17_110.png)。这些不相似度度量都不是参数 ![](img/B13208_17_111.png) 的函数，因此，它们无法对
    ![](img/B13208_17_112.png) 进行求导来使得分布 *P* 和 *Q* 变得相似。另一方面，EM距离是 ![](img/B13208_17_113.png)，其关于
    ![](img/B13208_17_114.png) 的梯度存在，并且可以推动 *Q* 向 *P* 靠近。
- en: 'Now, let''s focus our attention on how EM distance can be used to train a GAN
    model. Let''s assume ![](img/B13208_17_115.png) is the distribution of the real
    examples and ![](img/B13208_17_116.png) denotes the distributions of fake (generated)
    examples. ![](img/B13208_17_117.png) and ![](img/B13208_17_118.png) replace *P*
    and *Q* in the EM distance equation. As was mentioned earlier, computing the EM
    distance is an optimization problem by itself; therefore, this becomes computationally
    intractable, especially if we want to repeat this computation in each iteration
    of the GAN training loop. Fortunately, though, the computation of the EM distance
    can be simplified using a theorem called *Kantorovich-Rubinstein duality*, as
    follows:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们集中注意力在EM距离如何用于训练GAN模型上。假设 ![](img/B13208_17_115.png) 是真实样本的分布，而 ![](img/B13208_17_116.png)
    表示假（生成）样本的分布。 ![](img/B13208_17_117.png) 和 ![](img/B13208_17_118.png) 代替了EM距离公式中的
    *P* 和 *Q*。如前所述，计算EM距离本身就是一个优化问题；因此，如果我们想在GAN训练循环的每次迭代中重复计算它，这会变得计算上不可行。幸运的是，EM距离的计算可以通过一个叫做
    *Kantorovich-Rubinstein 对偶性* 的定理来简化，如下所示：
- en: '![](img/B13208_17_119.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_119.png)'
- en: Here, the supremum is taken over all the *1-Lipschitz* continuous functions
    denoted by ![](img/B13208_17_120.png).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，supremum是取所有*1-Lipschitz*连续函数的上确界，记作 ![](img/B13208_17_120.png)。
- en: '**Lipschitz continuity**'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**Lipschitz 连续性**'
- en: 'Based on 1-Lipschitz continuity, the function, *f*, must satisfy the following
    property:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 基于1-Lipschitz连续性，函数 *f* 必须满足以下性质：
- en: '![](img/B13208_17_121.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_121.png)'
- en: Furthermore, a real function, ![](img/B13208_17_122.png), that satisfies the
    property
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，满足该性质的实函数为 ![](img/B13208_17_122.png)。
- en: '![](img/B13208_17_123.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_123.png)'
- en: is called K-Lipschitz continuous.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为K-Lipschitz连续。
- en: Using EM distance in practice for GANs
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在实践中使用EM距离来训练GAN
- en: Now, the question is, how do we find such a 1-Lipschitz continuous function
    to compute the Wasserstein distance between the distribution of real (![](img/B13208_17_124.png))
    and fake (![](img/B13208_17_125.png)) outputs for a GAN? While the theoretical
    concepts behind the WGAN approach may seem complicated at first, the answer to
    this question is simpler than it may appear. Recall that we consider deep NNs
    to be universal function approximators. This means that we can simply train an
    NN model to approximate the Wasserstein distance function. As you saw in the previous
    section, the simple GAN uses a discriminator in the form of a classifier. For
    WGAN, the discriminator can be changed to behave as a *critic*, which returns
    a scalar score instead of a probability value. We can interpret this score as
    how realistic the input images are (like an art critic giving scores to artworks
    in a gallery).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是，如何找到这样的1-Lipschitz连续函数来计算真实样本分布（![](img/B13208_17_124.png)）和假样本分布（![](img/B13208_17_125.png)）之间的Wasserstein距离？虽然WGAN方法背后的理论概念一开始可能看起来很复杂，但这个问题的答案其实比它看起来要简单。回想一下，我们认为深度神经网络是通用的函数逼近器。这意味着我们可以简单地训练一个神经网络模型来逼近Wasserstein距离函数。正如你在上一部分看到的，简单的GAN使用了一个分类器形式的鉴别器。对于WGAN，鉴别器可以改为作为一个
    *评论家* 来工作，它返回一个标量分数，而不是一个概率值。我们可以将这个分数解释为输入图像的真实度（就像艺术评论家给画廊中的艺术作品打分一样）。
- en: 'To train a GAN using the Wasserstein distance, the losses for the discriminator,
    *D*, and generator, *G*, are defined as follows. The critic (that is, the discriminator
    network) returns its outputs for the batch of real image examples and the batch
    of synthesized examples. We use the notations *D*(*x*) and *D*(*G*(*z*)), respectively.
    Then, the following loss terms can be defined:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使用 Wasserstein 距离训练 GAN，定义了鉴别器 *D* 和生成器 *G* 的损失如下。鉴别器（即鉴别网络）返回其对于真实图像样本批次和生成样本批次的输出。我们分别使用符号
    *D*(*x*) 和 *D*(*G*(*z*))。然后，可以定义以下损失项：
- en: 'The real component of the discriminator''s loss: ![](img/B13208_17_126.png)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器损失的真实部分：![](img/B13208_17_126.png)
- en: 'The fake component of the discriminator''s loss: ![](img/B13208_17_127.png)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鉴别器损失的伪造部分：![](img/B13208_17_127.png)
- en: 'The loss for the generator: ![](img/B13208_17_128.png)'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器的损失：![](img/B13208_17_128.png)
- en: That will be all for the WGAN, except that we need to ensure that the 1-Lipschitz
    property of the critic function is preserved during training. For this purpose,
    the WGAN paper proposes clamping the weights to a small region, for example, [–0.01,
    0.01].
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: WGAN 的所有内容就到此为止，除了我们需要确保在训练过程中鉴别器函数的 1-Lipschitz 性质得到保持。为此，WGAN 论文建议将权重限制在一个小区域内，例如
    [–0.01, 0.01]。
- en: Gradient penalty
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度惩罚
- en: In the paper by Arjovsky et al., weight clipping is suggested for the 1-Lipschitz
    property of the discriminator (or critic). However, in another paper titled *Improved
    Training of Wasserstein GANs*, which is freely available at [https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf),
    Ishaan Gulrajani et al. showed that clipping the weights can lead to exploding
    and vanishing gradients. Furthermore, weight clipping can also lead to capacity
    underuse, which means that the critic network is limited to learning only some
    simple functions, as opposed to more complex functions. Therefore, rather than
    clipping the weights, Ishaan Guljarani et al. proposed **gradient penalty** (**GP**)
    as an alternative solution. The result is the **WGAN with gradient penalty** (**WGAN-GP**).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Arjovsky 等人的论文中，建议通过权重裁剪来保持鉴别器（或鉴别网络）的 1-Lipschitz 性质。然而，在另一篇题为 *改进的 Wasserstein
    GAN 训练* 的论文中，该论文可以在 [https://arxiv.org/pdf/1704.00028.pdf](https://arxiv.org/pdf/1704.00028.pdf)
    免费获取，Ishaan Gulrajani 等人展示了裁剪权重可能导致梯度爆炸和梯度消失。此外，权重裁剪还可能导致模型容量不足，这意味着鉴别器网络只能学习一些简单的函数，而无法学习更复杂的函数。因此，Ishaan
    Gulrajani 等人提出了 **梯度惩罚** (**GP**) 作为替代方案。最终结果就是 **带有梯度惩罚的 WGAN** (**WGAN-GP**)。
- en: 'The procedure for the GP that is added in each iteration can be summarized
    by the following sequence of steps:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代中加入的 GP 过程可以通过以下步骤总结：
- en: For each pair of real and fake examples ![](img/B13208_17_129.png) in a given
    batch, choose a random number, ![](img/B13208_17_130.png), sampled from a uniform
    distribution, that is, ![](img/B13208_17_131.png).
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定批次中的每对真实和伪造样本 ![](img/B13208_17_129.png)，选择一个随机数，![](img/B13208_17_130.png)，从均匀分布中采样，即
    ![](img/B13208_17_131.png)。
- en: 'Calculate an interpolation between the real and fake examples: ![](img/B13208_17_132.png),
    resulting in a batch of interpolated examples.'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算真实和伪造样本之间的插值：![](img/B13208_17_132.png)，得到一批插值样本。
- en: Compute the discriminator (critic) output for all the interpolated examples,
    ![](img/B13208_17_133.png).
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有插值样本的鉴别器（鉴别器）输出，![](img/B13208_17_133.png)。
- en: Calculate the gradients of the critic's output with respect to each interpolated
    example, that is, ![](img/B13208_17_134.png).
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算鉴别器输出相对于每个插值样本的梯度，即：![](img/B13208_17_134.png)。
- en: Compute the GP as ![](img/B13208_17_135.png).
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度惩罚为：![](img/B13208_17_135.png)。
- en: 'The total loss for the discriminator is then as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴别器的总损失如下所示：
- en: '![](img/B13208_17_136.png),'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B13208_17_136.png)，'
- en: Here, ![](img/B13208_17_137.png) is a tunable hyperparameter.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_17_137.png) 是一个可调的超参数。
- en: Implementing WGAN-GP to train the DCGAN model
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 WGAN-GP 来训练 DCGAN 模型
- en: 'We have already defined the helper functions that create the generator and
    discriminator networks for DCGAN (`make_dcgan_generator()` and `make_dcgan_discriminator()`).
    The code to build the DCGAN model is as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了创建 DCGAN 生成器和鉴别器网络的辅助函数（`make_dcgan_generator()` 和 `make_dcgan_discriminator()`）。构建
    DCGAN 模型的代码如下：
- en: '[PRE15]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we can train the model. Note that, typically, the RMSprop optimizer is
    recommended for WGAN (without the GP), whereas the Adam optimizer is used for
    WGAN-GP. The code is as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以开始训练模型。请注意，通常建议使用 RMSprop 优化器来训练 WGAN（不带 GP），而使用 Adam 优化器来训练 WGAN-GP。代码如下：
- en: '[PRE16]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, let''s visualize the saved examples at some epochs to see how the
    model is learning and how the quality of synthesized examples changes over the
    course of learning:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们可视化在一些训练周期保存的样本，看看模型是如何学习的，以及合成样本的质量如何随着学习过程而变化：
- en: '[PRE17]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The following figure shows the results:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了结果：
- en: '![](img/B13208_17_18.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_18.png)'
- en: We used the same code to visualize the results as in the section on vanilla
    GAN. Comparing the new examples shows that DCGAN (with Wasserstein and GP) can
    generate images of a much higher quality.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与香草 GAN 部分相同的代码来可视化结果。比较新的示例可以看到，DCGAN（结合 Wasserstein 和 GP）能够生成质量更高的图像。
- en: Mode collapse
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模式崩溃
- en: Due to the adversarial nature of GAN models, it is notoriously hard to train
    them. One common cause of failure in training GANs is when the generator gets
    stuck in a small subspace and learns to generate similar samples. This is called
    **mode collapse**, and an example is shown in the following figure.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 GAN 模型的对抗性特性，它们的训练非常困难。训练 GAN 失败的一个常见原因是生成器卡在一个小子空间中，并学会生成相似的样本。这被称为**模式崩溃**，以下图展示了一个例子。
- en: 'The synthesized examples in this figure are not cherry-picked. This shows that
    the generator has failed to learn the entire data distribution, and instead, has
    taken a lazy approach focusing on a subspace:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图中的合成样本并非精心挑选的。这表明生成器未能学会整个数据分布，而是采取了一种懒惰的方法，专注于一个子空间：
- en: '![](img/B13208_17_19.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_17_19.png)'
- en: Besides the vanishing and exploding gradient problems that we saw previously,
    there are some further aspects that can also make training GAN models difficult
    (indeed, it is an art). Here are a few suggested tricks from GAN artists.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前看到的梯度消失和梯度爆炸问题外，还有一些其他方面也可能使得 GAN 模型的训练变得困难（事实上，这是一门艺术）。这里有一些来自 GAN 艺术家的建议技巧。
- en: One approach is called *mini-batch discrimination*, which is based on the fact
    that batches consisting of only real or fake examples are fed separately to the
    discriminator. In mini-batch discrimination, we let the discriminator compare
    examples across these batches to see whether a batch is real or fake. The diversity
    of a batch consisting of only real examples is most likely higher than the diversity
    of a fake batch if a model suffers from mode collapse.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法叫做*小批量判别*，它基于以下事实：由真实或伪造样本组成的小批量数据分别输入判别器。在小批量判别中，我们让判别器比较这些批次中的样本，看看一个批次是来自真实数据还是伪造数据。如果模型发生模式崩溃，那么仅由真实样本组成的批次的多样性通常会高于伪造批次的多样性。
- en: Another technique that is commonly used for stabilizing GAN training is *feature
    matching*. In feature matching, we make a slight modification to the objective
    function of the generator by adding an extra term that minimizes the difference
    between the original and synthesized images based on intermediate representations
    (feature maps) of the discriminator. We encourage you to read more about this
    technique in the original article by Ting-Chun Wang et al., titled *High Resolution
    Image Synthesis and Semantic Manipulation with Conditional GANs,* which is freely
    available at [https://arxiv.org/pdf/1711.11585.pdf](https://arxiv.org/pdf/1711.11585.pdf).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常用来稳定 GAN 训练的技术是*特征匹配*。在特征匹配中，我们通过增加一个额外的项来稍微修改生成器的目标函数，该项最小化基于判别器中间表示（特征图）之间的原始图像和合成图像的差异。我们鼓励您阅读
    Ting-Chun Wang 等人撰写的原始文章，标题为*使用条件 GAN 进行高分辨率图像合成和语义操控*，该文章可以免费访问，链接：[https://arxiv.org/pdf/1711.11585.pdf](https://arxiv.org/pdf/1711.11585.pdf)。
- en: During the training, a GAN model can also get stuck in several modes and just
    hop between them. To avoid this behavior, you can store some old examples and
    feed them to the discriminator to prevent the generator from revisiting previous
    modes. This technique is referred to as *experience replay*. Furthermore, you
    can train multiple GANs with different random seeds so that the combination of
    all of them covers a larger part of the data distribution than any single one
    of them.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，GAN 模型也可能会卡在几个模式之间，只是在这些模式之间跳来跳去。为了避免这种情况，您可以存储一些旧的样本并将其输入判别器，以防止生成器重新访问先前的模式。这种技术被称为*经验回放*。此外，您还可以用不同的随机种子训练多个
    GAN，这样所有 GAN 的组合覆盖的数据分布范围要比单一模型更广。
- en: Other GAN applications
  id: totrans-249
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他 GAN 应用
- en: In this chapter, we mainly focused on generating examples using GANs and looked
    at a few tricks and techniques to improve the quality of synthesized outputs.
    The applications of GANs are expanding rapidly, including in computer vision,
    machine learning, and even other domains of science and engineering. A nice list
    of different GAN models and application areas can be found at [https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要集中于使用GAN生成示例，并探讨了一些技巧和技术，以提高合成输出的质量。GAN的应用正在迅速扩展，包括计算机视觉、机器学习，甚至其他科学和工程领域。在[https://github.com/hindupuravinash/the-gan-zoo](https://github.com/hindupuravinash/the-gan-zoo)上可以找到一份有关不同GAN模型和应用领域的良好列表。
- en: It is worth mentioning that we covered GANs in an unsupervised fashion, that
    is, no class label information was used in the models that were covered in this
    chapter. However, the GAN approach can be generalized to semi-supervised and supervised
    tasks, as well. For example, the conditional GAN (cGAN) proposed by Mehdi Mirza
    and Simon Osindero in the paper *Conditional Generative Adversarial Nets* ([https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf))
    uses the class label information and learns to synthesize new images conditioned
    on the provided label, that is, ![](img/B13208_17_138.png)—applied to MNIST. This
    allows us to generate different digits in the range 0-9 selectively. Furthermore,
    conditional GANs allows us to do image-to-image translation, which is to learn
    how to convert a given image from a specific domain to another. In this context,
    one interesting work is the Pix2Pix algorithm, published in the paper *Image-to-Image
    Translation with Conditional Adversarial Networks* by Philip Isola et al. ([https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf)).
    It is worth mentioning that in the Pix2Pix algorithm, the discriminator provides
    the real/fake predictions for multiple patches across the image as opposed to
    a single prediction for an entire image.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 值得一提的是，我们在本章中以无监督的方式介绍了GAN，也就是说，在本章所覆盖的模型中并未使用类别标签信息。然而，GAN方法也可以推广到半监督和监督任务。例如，Mehdi
    Mirza和Simon Osindero在论文*Conditional Generative Adversarial Nets*中提出的条件GAN（cGAN）（[https://arxiv.org/pdf/1411.1784.pdf](https://arxiv.org/pdf/1411.1784.pdf)）使用类别标签信息，并学习在提供的标签条件下合成新的图像，即！[](img/B13208_17_138.png)—应用于MNIST数据集。这使得我们可以有选择性地生成0-9范围内的不同数字。此外，条件GAN还允许我们进行图像到图像的转换，即学习如何将特定领域的给定图像转换到另一个领域。在这个背景下，一个有趣的工作是Pix2Pix算法，它发表在Philip
    Isola等人的论文*Image-to-Image Translation with Conditional Adversarial Networks*中（[https://arxiv.org/pdf/1611.07004.pdf](https://arxiv.org/pdf/1611.07004.pdf)）。值得一提的是，在Pix2Pix算法中，判别器提供多个图像区域的真/假预测，而不是对整个图像的单一预测。
- en: CycleGAN is another interesting GAN model built on top of the cGAN, also for
    image-to-image translation. However, note that in CycleGAN, the training examples
    from the two domains are unpaired, meaning that there is no one-to-one correspondence
    between inputs and outputs. For example, using a CycleGAN, we could change the
    season of a picture taken in summer to winter. In the paper *Unpaired Image-to-Image
    Translation Using Cycle-Consistent Adversarial Networks* by Jun-Yan Zhu et al.
    ([https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)),
    an impressive example shows horses converted into zebras.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: CycleGAN是另一个基于cGAN的有趣GAN模型，同样用于图像到图像的转换。然而，请注意，在CycleGAN中，来自两个领域的训练示例是非配对的，这意味着输入和输出之间没有一一对应关系。例如，使用CycleGAN，我们可以将一张夏季拍摄的图片的季节转换为冬季。在Jun-Yan
    Zhu等人的论文*Unpaired Image-to-Image Translation Using Cycle-Consistent Adversarial
    Networks*中（[https://arxiv.org/pdf/1703.10593.pdf](https://arxiv.org/pdf/1703.10593.pdf)），一个令人印象深刻的示例展示了将马转化为斑马。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 小结
- en: 'In this chapter, you first learned about generative models in deep learning
    and their overall objective: synthesizing new data. We then covered how GAN models
    use a generator network and a discriminator network, which compete with each other
    in an adversarial training setting to improve each other. Next, we implemented
    a simple GAN model using only fully connected layers for both the generator and
    the discriminator.'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你首先了解了深度学习中的生成模型及其总体目标：合成新数据。接着我们介绍了GAN模型如何使用生成网络和判别网络，在对抗训练设置中互相竞争，从而彼此改进。随后，我们实现了一个简单的GAN模型，其中生成器和判别器都只使用了全连接层。
- en: 'We also covered how GAN models can be improved. First, you saw a DCGAN, which
    uses deep convolutional networks for both the generator and the discriminator.
    Along the way, you also learned about two new concepts: transposed convolution
    (for upsampling the spatial dimensionality of feature maps) and BatchNorm (for
    improving convergence during training).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了如何改进GAN模型。首先，你了解了DCGAN，它为生成器和判别器都使用了深度卷积网络。在这个过程中，你还学习了两个新概念：转置卷积（用于上采样特征图的空间维度）和BatchNorm（用于提高训练过程中的收敛性）。
- en: We then looked at a WGAN, which uses the EM distance to measure the distance
    between the distributions of real and fake samples. Finally, we talked about the
    WGAN with GP to maintain the 1-Lipschitz property instead of clipping the weights.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们讨论了WGAN，它使用EM距离来衡量真实样本和伪造样本之间的分布距离。最后，我们讲解了带有GP的WGAN，它通过保持1-Lipschitz性质来代替对权重的裁剪。
- en: In the next chapter, we will look at reinforcement learning, which is a completely
    different category of machine learning compared to what we have covered so far
    in this book.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论强化学习，这是与本书目前内容完全不同的机器学习类别。
