- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Applying Active Learning to Computer Vision
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将主动学习应用于计算机视觉
- en: In this chapter, we will dive into using active learning techniques for computer
    vision tasks. Computer vision involves analyzing visual data such as images and
    videos to extract useful information. It relies heavily on machine learning models
    such as convolutional neural networks. However, these models require large labeled
    training sets, which can be expensive and time-consuming to obtain. Active ML
    provides a solution by interactively querying the user to label only the most
    informative examples. This chapter demonstrates how to implement uncertainty sampling
    for diverse computer vision tasks. By the end, you will have the tools to efficiently
    train computer vision models with optimized labeling effort. The active ML methods
    presented open up new possibilities for building robust vision systems with fewer
    data requirements.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将深入探讨使用主动学习技术进行计算机视觉任务。计算机视觉涉及分析图像和视频等视觉数据以提取有用信息。它严重依赖于卷积神经网络等机器学习模型。然而，这些模型需要大量的标记训练集，这可能既昂贵又耗时。主动机器学习通过交互式查询用户仅对最有信息量的示例进行标记来提供解决方案。本章演示了如何为各种计算机视觉任务实现不确定性采样。到本章结束时，您将拥有使用优化标记努力高效训练计算机视觉模型的工具。所提出的主动机器学习方法为在更少的数据需求下构建鲁棒的视觉系统开辟了新的可能性。
- en: 'By the end of this chapter, you will be able to do the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够做到以下事项：
- en: Implementing active ML for an image classification project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在图像分类项目中实现主动机器学习
- en: Applying active ML to an object detection project
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将主动机器学习应用于目标检测项目
- en: Using active ML for an instance segmentation project
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用主动机器学习进行实例分割项目
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, you will need to install the Ultralytics, PyYAML, and Roboflow
    packages.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要安装Ultralytics、PyYAML和Roboflow包。
- en: Ultralytics is a popular open source Python library for building high-performance
    computer vision and deep learning models. It provides implementations of state-of-the-art
    object detection and image segmentation models including YOLO that can be trained
    on custom datasets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Ultralytics是一个流行的开源Python库，用于构建高性能计算机视觉和深度学习模型。它提供了包括YOLO在内的最先进的对象检测和图像分割模型的实现，这些模型可以在自定义数据集上训练。
- en: PyYAML is a Python library used for reading and writing YAML files. YAML is
    a human-readable data serialization format. PyYAML allows loading YAML data from
    files or strings into Python data types such as dictionaries and lists. It can
    also dump Python objects back into YAML strings.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: PyYAML是一个Python库，用于读取和写入YAML文件。YAML是一种人类可读的数据序列化格式。PyYAML允许将YAML数据从文件或字符串加载到Python数据类型，如字典和列表。它还可以将Python对象转储回YAML字符串。
- en: Roboflow, as presented in earlier chapters, is a platform that helps with preparing
    and managing datasets for computer vision models. It provides tools to annotate
    images, create training/test splits, and export labeled datasets in formats that
    are usable by deep learning frameworks such as PyTorch. Roboflow also integrates
    with libraries such as Ultralytics to streamline training pipelines. The main
    goal is to simplify the dataset management aspects of developing CV models.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，Roboflow是一个帮助准备和管理计算机视觉模型数据集的平台。它提供了注释图像、创建训练/测试分割以及以PyTorch等深度学习框架可用的格式导出标记数据集的工具。Roboflow还与Ultralytics等库集成，以简化训练流程。主要目标是简化开发CV模型的数据集管理方面。
- en: 'To install these packages, we can run the following code:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装这些包，我们可以运行以下代码：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You will also need the following imports:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要以下导入：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Additionally, you will need a Roboflow account in order to get a Roboflow API
    key. You can create an account here: [https://app.roboflow.com/](https://app.roboflow.com/).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还需要一个Roboflow账户以获取Roboflow API密钥。您可以在以下链接创建账户：[https://app.roboflow.com/](https://app.roboflow.com/).
- en: Implementing active ML for an image classification project
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在图像分类项目中实现主动机器学习
- en: In this section, we will guide you through the implementation of active ML techniques
    for an image classification project. Image classification has various applications
    in computer vision, ranging from identifying products for an e-commerce website
    to detecting patterns of deforestation on geospatial tiles. However, creating
    accurate image classifiers requires extensive datasets of labeled images, which
    can be expensive and time-consuming to gather, as mentioned in [*Chapter 1*](B21789_01.xhtml#_idTextAnchor015),
    *Introducing Active Machine Learning*. Active ML offers a solution to this labeling
    bottleneck by interactively requesting the oracle to label only the most informative
    examples.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将指导您实现图像分类项目的主动机器学习技术。图像分类在计算机视觉领域有各种应用，从为电子商务网站识别产品到检测地理空间瓦片上的森林砍伐模式。然而，创建准确的图像分类器需要大量的标记图像数据集，这可能会非常昂贵且耗时，如[第1章](B21789_01.xhtml#_idTextAnchor015)中所述，*介绍主动机器学习*。主动机器学习通过交互式请求仅对最有信息量的示例进行标记，为这一标记瓶颈提供了解决方案。
- en: We will build an image classification model that will be capable of accurately
    classifying various images obtained from the CIFAR-10 dataset. This dataset is
    widely recognized in the field of computer vision and contains a diverse collection
    of 60,000 images, each belonging to one of 10 different classes. We will start
    with a small *labeled* set of only 2,000 images from CIFAR-10, then employ an
    active ML strategy to select the best images to present to an oracle for labeling.
    Specifically, we will use uncertainty sampling to query the examples the model
    is least certain about. We use uncertainty sampling here as it is simpler and
    less computationally expensive than other methods we have discussed previously.
    For instance, query-by-committee requires training multiple models, which is computationally
    expensive.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个图像分类模型，该模型能够准确地对从 CIFAR-10 数据集中获得的各类图像进行分类。这个数据集在计算机视觉领域广为人知，包含了一个多样化的60,000张图像集合，每张图像属于10个不同的类别之一。我们将从CIFAR-10数据集中的2,000张小批量的标记图像开始，然后采用主动机器学习策略来选择最佳图像展示给标记者进行标记。具体来说，我们将使用不确定性采样来查询模型最不确定的示例。我们在这里使用不确定性采样，因为它比我们之前讨论的其他方法更简单且计算成本更低。例如，委员会查询需要训练多个模型，这计算成本较高。
- en: As more labels are acquired, model accuracy improves with fewer training examples.
    This demonstrates how active learning can create high-performing computer vision
    models with significantly lower data requirements.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 随着获取的标签越来越多，模型精度随着训练样本数量的减少而提高。这展示了主动学习如何以显著较低的数据需求创建高性能的计算机视觉模型。
- en: Building a CNN for the CIFAR dataset
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为 CIFAR 数据集构建 CNN
- en: The implementation will cover initializing a **convolutional neural network**
    (**CNN**) classifier, training our model with a small labeled set, selecting unlabeled
    images for the next labeling step using active ML, acquiring new labels, retraining
    the model, and tracking model performance.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 实现将包括初始化一个**卷积神经网络**（**CNN**）分类器，使用小批量的标记数据集训练我们的模型，使用主动机器学习选择下一标记步骤的未标记图像，获取新的标签，重新训练模型，并跟踪模型性能。
- en: Quick reminder
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 快速提醒
- en: A CNN classifier takes an image as input, extracts feature maps using convolutions,
    integrates the features in fully connected layers, and outputs predicted class
    probabilities based on what it learned during training. The convolutions allow
    it to automatically learn relevant visual patterns, making CNNs very effective
    for image classification tasks. You can find the PyTorch official tutorial on
    building a neural network model at [https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: CNN 分类器以图像为输入，使用卷积提取特征图，在完全连接层中整合特征，并根据训练期间学习到的内容输出预测的类别概率。卷积允许它自动学习相关的视觉模式，这使得
    CNN 对于图像分类任务非常有效。您可以在 PyTorch 官方教程中找到构建神经网络模型的说明，请参阅[https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#sphx-glr-beginner-blitz-neural-networks-tutorial-py)。
- en: 'Let’s create a simple image classification model:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的图像分类模型：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we use this model because it is a small CNN that runs quickly and
    efficiently. This is helpful for running simple proofs of concept. However, next,
    we could use one of the pretrained models (such as `torchvision`, as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用这个模型是因为它是一个运行快速且高效的较小CNN。这对于运行简单的概念验证很有帮助。然而，接下来，我们可以使用预训练模型之一（例如`torchvision`，如下所示：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can find all the `torchvision` pretrained models on the library’s model
    page: [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html).'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在库的模型页面上找到所有`torchvision`预训练模型：[https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)。
- en: 'Now, we load the CIFAR-10 dataset with the appropriate transform function.
    The transform function defines a series of data processing and augmentation operations
    that are automatically applied when fetching samples from a PyTorch dataset. In
    the following code, we convert the images to tensors and normalize them:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用适当的转换函数加载CIFAR-10数据集。转换函数定义了一系列在从PyTorch数据集中获取样本时自动应用的数据处理和增强操作。在下面的代码中，我们将图像转换为张量并对它们进行归一化：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This print shows us that the length of the full training dataset is 50,000 images.
    We are using the CIFAR-10 train dataset because we set the Boolean value of `train=True`.
    Later on, we will use the test set from CIFAR-10 and will then set `train=False`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个打印结果显示，完整训练数据集的长度是50,000张图像。我们使用CIFAR-10训练数据集是因为我们将`train=True`的布尔值设置为True。稍后，我们将使用CIFAR-10的测试集，并将`train=False`设置为False。
- en: 'Now, we will create a small dataset of 2,000 labeled images. The purpose here
    is to simulate the existence of a small labeled set of images, while the remaining
    images are unlabeled. Our objective is to identify and select the most informative
    images for labeling next with active ML:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个包含2,000个标记图像的小型数据集。这里的目的是模拟存在一个小型标记图像集，而其余图像未标记。我们的目标是使用主动机器学习识别和选择下一个标记的最具信息量的图像：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So, we have created a small labeled dataset and now need to initialize our
    training PyTorch data loader. A **PyTorch data loader** is used to load and iterate
    over datasets for training neural networks. It takes the dataset that contains
    the actual images and labels and is responsible for batching up these samples
    and feeding them to the model. The data loader allows you to specify a batch size,
    which determines how many samples are batched together – this is usually set to
    something like 64 or 128\. Additionally, the data loader will shuffle the data
    by default if you are using it for a training set. This randomization of the order
    of samples helps the model generalize better during training:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经创建了一个小型标记数据集，现在需要初始化我们的训练PyTorch数据加载器。**PyTorch数据加载器**用于加载和迭代用于训练神经网络的数据集。它接受包含实际图像和标签的数据集，并负责将这些样本分批并馈送到模型。数据加载器允许您指定批大小，这决定了多少个样本被一起分批
    – 这通常设置为64或128等。此外，如果您使用它作为训练集，数据加载器默认会打乱数据。这种样本顺序的随机化有助于模型在训练期间更好地泛化：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The next step is to initialize our model. We know that CIFAR-10 has 10 classes:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是初始化我们的模型。我们知道CIFAR-10有10个类别：
- en: '[PRE7]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'A good practice is to visualize the data with which we are working:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的做法是可视化我们正在处理的数据：
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '*Figure 4**.1* depicts a sample of CIFAR-10 dataset images.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4.1*展示了CIFAR-10数据集图像的一个样本。'
- en: '![Figure 4.1 – A random visualization of some CIFAR-10 dataset images](img/B21789_04_1.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 一些CIFAR-10数据集图像的随机可视化](img/B21789_04_1.jpg)'
- en: Figure 4.1 – A random visualization of some CIFAR-10 dataset images
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 – 一些CIFAR-10数据集图像的随机可视化
- en: 'It is also good to take a look at the labels, so let’s print the first five
    labels:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 最好也看一下这些标签，所以让我们打印前五个标签：
- en: '[PRE9]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code returns the following list of labels as the output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码返回以下标签列表作为输出：
- en: '[PRE10]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We can see that this is correct when cross-referencing these labels with the
    first five images in *Figure 4**.1*.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将这些标签与*图4.1*中的前五张图像进行交叉引用来验证这是否正确。
- en: What is unnormalizing?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是反归一化？
- en: '**Unnormalizing** an image means reversing any normalization that was previously
    applied to the image pixel values in order to restore the original pixel value
    distribution (from the 0–1 range to the original 0–255 range).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**反归一化**一张图像意味着反转之前应用于图像像素值的任何归一化，以便恢复原始像素值分布（从0-1范围到原始0-255范围）。'
- en: 'Now that we have our data loader, we can start the training loop; we first
    define our loss and optimizer. The **loss function** measures how well the model’s
    predictions match the true labels for a batch of images. It calculates the error
    between the predicted and true labels. Common loss functions for classification
    include cross-entropy loss and negative log-likelihood loss. These loss functions
    will output a high number if the model predicts incorrect labels, and a low number
    if the predictions are accurate. The goal during training is to minimize the loss
    by updating the model parameters. A good resource for learning about the loss
    functions available for use in PyTorch can be found here: [https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions).'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了数据加载器，我们可以开始训练循环；我们首先定义我们的损失函数和优化器。**损失函数**衡量模型预测与一批图像的真实标签匹配的程度。它计算预测标签和真实标签之间的误差。常见的分类损失函数包括交叉熵损失和负对数似然损失。如果模型预测了错误的标签，这些损失函数将输出一个高数值，如果预测准确，则输出一个低数值。训练的目标是通过更新模型参数来最小化损失。有关PyTorch中可用的损失函数的更多信息，可以在此处找到：[https://pytorch.org/docs/stable/nn.html#loss-functions](https://pytorch.org/docs/stable/nn.html#loss-functions)。
- en: 'The **optimizer** is responsible for this parameter updating. It uses the loss
    value to perform backpropagation and update the model’s weights and biases to
    reduce the loss. **Stochastic gradient descent** (**SGD**) is a popular optimization
    algorithm, where the parameters are updated proportionally to the gradient of
    the loss function. The learning rate controls the size of the updates. Other optimizers
    such as **Adam** and **RMSProp** are also commonly used for deep learning models
    (to learn about the optimizer functions available for use in PyTorch, you can
    visit this link: [https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**优化器**负责参数更新。它使用损失值执行反向传播并更新模型的权重和偏差以减少损失。**随机梯度下降**（**SGD**）是一种流行的优化算法，其中参数的更新与损失函数的梯度成比例。学习率控制更新的大小。其他如**Adam**和**RMSProp**等优化器也常用于深度学习模型（有关PyTorch中可用的优化函数的更多信息，您可以访问此链接：[https://pytorch.org/docs/stable/optim.html](https://pytorch.org/docs/stable/optim.html)）：'
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will train our model for 100 epochs. Epochs represent the number of passes
    through the full training dataset during the training of the model. We define
    a `train` function as follows to run our training:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练我们的模型100个epoch。Epoch表示在模型训练过程中通过完整训练数据集的次数。我们定义一个`train`函数如下以运行我们的训练：
- en: '[PRE12]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Then, we run the training:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始训练：
- en: '[PRE13]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have now an initial trained model on our small dataset and we want to use
    it to select the next images to label. But first, let’s evaluate this model on
    the CIFAR-10 test set. We define an evaluation function:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在我们的小型数据集上有一个初始训练好的模型，我们想用它来选择下一个要标记的图像。但首先，让我们在CIFAR-10测试集上评估这个模型。我们定义一个评估函数：
- en: '[PRE14]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can then use this function with our trained model once we define our test
    set as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义测试集后使用此函数与我们的训练好的模型：
- en: '[PRE15]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The test set’s length is 10,000.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 测试集的长度是10,000。
- en: 'Let’s use our evaluation function with this test set:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的评估函数与这个测试集：
- en: '[PRE16]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This gives us the following result:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下结果：
- en: '[PRE17]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So we have now tested our first trained model, which was trained on the 2,000
    images of our initial small labeled set. The model’s accuracy on the test set
    is 40.08%. We aim to improve this accuracy by labeling more images. This is where
    our active ML selection strategy comes into play.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在已经测试了我们第一个训练好的模型，该模型是在我们的初始小型标记集的2,000个图像上训练的。模型在测试集上的准确率为40.08%。我们的目标是通过标记更多图像来提高这个准确率。这就是我们的主动机器学习选择策略发挥作用的地方。
- en: Applying uncertainty sampling to improve classification performance
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用不确定性采样以改进分类性能
- en: We will choose the most informative images to label next from our dataset –
    namely, the frames where the **model is least confident**, a method discussed
    in [*Chapter 2*](B21789_02.xhtml#_idTextAnchor027), *Designing Query* *Strategy
    Frameworks*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从我们的数据集中选择最具信息量的图像进行标记，即模型最不自信的帧，这是在第[*第2章*](B21789_02.xhtml#_idTextAnchor027)，*设计查询策略框架*中讨论的方法。
- en: 'We first define a function to get the model’s uncertainty scores:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个函数来获取模型的不确定性分数：
- en: '[PRE18]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Then, we define our data loader for the unlabeled set. We will use a batch
    size of 1 as we will loop through all the images to get the uncertainty scores:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们为未标记集定义我们的数据加载器。由于我们将遍历所有图像以获取不确定性分数，我们将使用批大小为1：
- en: '[PRE19]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We collect the confidence scores for our set of **unlabeled** images:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了我们一组**未标记**图像的置信度分数：
- en: '[PRE20]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This returns the following:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了以下结果：
- en: '[PRE21]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'These values represent the **least confidence scores** of the model’s predictions.
    The higher the scores, the less confident the model is. Therefore, next, we want
    to know the indices of the images where the scores are highest. We decide that
    we want to select 200 images (queries):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值代表模型预测的**最低置信度分数**。分数越高，模型越不自信。因此，接下来，我们想知道分数最高的图像索引。我们决定要选择200张图像（查询）：
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we sort by uncertainty:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按不确定性排序：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We get the original indices of the most uncertain samples and print the results:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获取最不确定样本的原始索引并打印结果：
- en: '[PRE24]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This returns the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了以下结果：
- en: '[PRE25]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now we have the indices of the images selected using our active ML least-confident
    strategy. These are the images that would be sent to our oracles to be labeled
    and then used to train the model again.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了使用我们的主动ML最不自信策略选择的图像索引。这些是会被发送到我们的预言机进行标记，然后用于再次训练模型的图像。
- en: 'Let’s take a look at five of these selected images:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这五张选定的图像：
- en: '[PRE26]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Figure 4.2 – Five of the chosen images to be labeled next](img/B21789_04_2.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 下一个要标记的五张选择的图像](img/B21789_04_2.jpg)'
- en: Figure 4.2 – Five of the chosen images to be labeled next
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 – 下一个要标记的五张选择的图像
- en: 'We have the images that now need to be labeled. Since this is a demo, we already
    have the labels, so let’s retrain our model with these newly labeled images. First,
    we need to add those to our labeled set:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了需要标记的图像。由于这是一个演示，我们已经有标签了，所以让我们用这些新标记的图像重新训练我们的模型。首先，我们需要将这些图像添加到我们的标记集中：
- en: '[PRE27]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This returns 2,200, which is correct because we first selected 2,000 images
    from our dataset and then queried 200 with our active ML sampling.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了2,200，这是正确的，因为我们首先从我们的数据集中选择了2,000张图像，然后使用我们的主动ML采样查询了200张。
- en: 'Let’s start our training from scratch again for 100 epochs:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从100个epoch开始重新从头开始训练：
- en: '[PRE28]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Then, run the evaluation on the test set:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在测试集上运行评估：
- en: '[PRE29]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This returns the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了以下结果：
- en: '[PRE30]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We have improved the accuracy on the test set from 40.08% to 41.54% by adding
    images selected with our active ML strategy to the training dataset. We could
    also fine-tune the model that was originally trained as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将使用我们的主动ML策略选择的图像添加到训练数据集中，我们将测试集上的准确率从40.08%提高到了41.54%。我们还可以按照以下方式微调最初训练的模型：
- en: '[PRE31]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This gives us the following:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下结果：
- en: '[PRE32]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We have an interesting result here: the fine-tuned model is performing worse
    than the model trained from scratch using the larger dataset. Overall, the model’s
    performance improves when the selected images chosen by the active ML are added.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个有趣的结果：微调模型的表现不如从头开始使用更大数据集训练的模型。总体而言，当添加由主动ML选择的图像时，模型的表现有所提高。
- en: This approach can be applied to real-world problems. It is important to note,
    however, that this is a basic demonstration of how to use the least confident
    sampling method for classification. In a real project, you would need to have
    oracles label the selected images. Additionally, you would likely need to query
    more than 200 images and use a larger pretrained model, as mentioned earlier.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以应用于现实世界的问题。然而，需要注意的是，这只是一个如何使用最不自信采样方法进行分类的基本演示。在实际项目中，你需要有预言机对所选图像进行标记。此外，你可能会需要查询超过200张图像，并使用一个更大的预训练模型，如前所述。
- en: While the previous example demonstrated active ML for image classification,
    the same principles can be applied to other computer vision tasks such as object
    detection, as we’ll see next.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然前面的例子展示了主动ML在图像分类中的应用，但相同的原理也可以应用于其他计算机视觉任务，例如目标检测，我们将在下面看到。
- en: Applying active ML to an object detection project
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将主动ML应用于目标检测项目
- en: In this section, we will guide you through the implementation of active ML techniques
    for an object detection project. An object detection project refers to developing
    a computer vision model to detect and localize objects within images or videos.
    The dataset is a collection of images (video frames) containing examples of the
    objects you want to detect, among other things. The dataset needs to have labels
    in the form of bounding boxes around the objects. Popular datasets for this purpose
    include **COCO** ([https://cocodataset.org/](https://cocodataset.org/)), **PASCAL
    VOC** ([http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/)),
    and **OpenImages** ([https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html)).
    The model architecture uses a neural network designed for object detection such
    as Faster R-CNN, YOLO, and so on. This type of architecture can automatically
    identify and localize real-world objects within visual data. The end result is
    a model that can detect and draw boxes around objects such as cars, people, furniture,
    and so on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将指导您实现针对目标检测项目的主动机器学习技术。目标检测项目指的是开发一个计算机视觉模型，用于在图像或视频中检测和定位对象。数据集是一组包含您想要检测的对象示例的图像（视频帧），以及其他内容。数据集需要以对象周围的边界框形式包含标签。用于此目的的流行数据集包括**COCO**
    ([https://cocodataset.org/](https://cocodataset.org/))、**PASCAL VOC** ([http://host.robots.ox.ac.uk/pascal/VOC/](http://host.robots.ox.ac.uk/pascal/VOC/))和**OpenImages**
    ([https://storage.googleapis.com/openimages/web/index.html](https://storage.googleapis.com/openimages/web/index.html))。模型架构使用专为对象检测设计的神经网络，如Faster
    R-CNN、YOLO等。这种架构可以自动在视觉数据中识别和定位现实世界中的对象。最终结果是能够检测并在对象周围绘制边界框的模型，例如汽车、人、家具等。
- en: 'The object detection project faces the same problem as classification projects:
    creating datasets is difficult and time-consuming. In fact, it is even more challenging
    for object detection tasks because it involves manually drawing bounding boxes
    around the objects. Once again, active ML provides a solution to this labeling
    bottleneck by sending the most informative images to the oracles for labeling.
    We will build an object detection model that will be capable of localizing brain
    tumors. This dataset we will use is from Roboflow Universe ([https://universe.roboflow.com/](https://universe.roboflow.com/))
    and is called *Brain Tumor Computer Vision Project*. To download this dataset,
    we use the Roboflow API:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测项目面临与分类项目相同的问题：创建数据集既困难又耗时。实际上，对于目标检测任务来说，这更具挑战性，因为它涉及到手动在对象周围绘制边界框。再次强调，主动机器学习通过将最有信息量的图像发送到仲裁者进行标记，为这个标记瓶颈提供了解决方案。我们将构建一个能够定位脑肿瘤的目标检测模型。我们将使用的数据集来自Roboflow
    Universe ([https://universe.roboflow.com/](https://universe.roboflow.com/))，被称为*脑肿瘤计算机视觉项目*。为了下载这个数据集，我们使用Roboflow
    API：
- en: '[PRE33]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: This downloads the dataset locally. The dataset is downloaded as a folder with
    the structure shown in *Figure 4**.3*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在本地下载数据集。数据集以*图4.3*中所示的结构作为一个文件夹下载。
- en: '![Figure 4.3 – Folder structure of the Roboflow Universe dataset, brain-tumor-m2pbp](img/B21789_04_3.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – Roboflow Universe数据集的文件夹结构，brain-tumor-m2pbp](img/B21789_04_3.jpg)'
- en: Figure 4.3 – Folder structure of the Roboflow Universe dataset, brain-tumor-m2pbp
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 – Roboflow Universe数据集的文件夹结构，brain-tumor-m2pbp
- en: Preparing and training our model
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备和训练我们的模型
- en: 'Next, we need to fix the `data.yaml` file to work properly in Google Colab
    and organize the data for our active ML demo. The `data.yaml` file is used in
    the `ultralytics` training to specify where the different sets (`train`, `valid`,
    and `test`) are placed. We assume here that the original training set is our unlabeled
    set of images, the validation set is our testing data, and the test set is our
    labeled data because it has the fewest examples. So, first, we define a function
    to rename the folders accordingly:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要修复`data.yaml`文件，以便在Google Colab中正常工作，并为我们的主动机器学习演示组织数据。`data.yaml`文件在`ultralytics`训练中用于指定不同的集合（`train`、`valid`和`test`）的位置。我们假设原始训练集是我们的未标记图像集，验证集是我们的测试数据，测试集是我们的标记数据，因为它包含的示例最少。因此，首先，我们定义一个函数来相应地重命名文件夹：
- en: '[PRE34]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![Figure 4.4 – Structure of the dataset after renaming the subfolders for our
    demo](img/B21789_04_4.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图4.4 – 重命名子文件夹后数据集的结构](img/B21789_04_4.jpg)'
- en: Figure 4.4 – Structure of the dataset after renaming the subfolders for our
    demo
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 – 重命名子文件夹后数据集的结构
- en: '*Figure 4**.4* shows the structure that we now have in our brain tumor dataset
    after renaming the folders for our demo. We then modify the `data.yaml` file:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*图4**.4*显示了我们在对演示中的文件夹进行重命名后，现在在我们脑肿瘤数据集中拥有的结构。然后我们修改`data.yaml`文件：'
- en: '[PRE35]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Here, we renamed the subfolder paths in the `data.yaml` file, which is the file
    that we will use for our training. We do not want to use the `val` folder for
    now in our training.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们在`data.yaml`文件中重命名了子文件夹路径，这是我们用于训练的文件。我们目前不想在训练中使用`val`文件夹。
- en: 'Now let’s take a look at our subfolders to determine the number of images in
    one of them:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看我们的子文件夹，以确定其中一个文件夹中的图片数量：
- en: '[PRE36]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The preceding code returns the following:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回以下内容：
- en: '[PRE37]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can now begin our initial training on our `labeled` dataset. For this training,
    we will utilize a widely used Python computer vision package called `ultralytics`
    ([https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics))
    and employ the `yolov8` model. The `yolov8` model is capable of performing tasks
    such as detection and tracking, instance segmentation, image classification, and
    pose estimation. We will train our model for 10 epochs only for our demo purposes.
    We use the `detect` task type because we want to train the model for object detection:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始在我们的`labeled`数据集上进行初始训练。为此训练，我们将使用一个广泛使用的Python计算机视觉包，称为`ultralytics`
    ([https://github.com/ultralytics/ultralytics](https://github.com/ultralytics/ultralytics))，并使用`yolov8`模型。`yolov8`模型能够执行检测和跟踪、实例分割、图像分类和姿态估计等任务。为了演示目的，我们将只训练我们的模型10个epoch。我们使用`detect`任务类型，因为我们想训练模型进行目标检测：
- en: '[PRE38]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Analyzing the evaluation metrics
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析评估指标
- en: 'Once the training is done, we evaluate it on the test set. Here is how we can
    evaluate the model:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们就在测试集上对其进行评估。以下是评估模型的方法：
- en: '[PRE39]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The preceding returns the following output:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码返回以下输出：
- en: '[PRE40]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Let’s analyze these metrics:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这些指标：
- en: '`precision(B)` measures how many of the predicted bounding boxes are correct.
    A value of 0.60 means 60% of the predicted boxes match the ground truth boxes.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`precision(B)`衡量预测的边界框中有多少是正确的。0.60的值意味着60%的预测框与真实框匹配。'
- en: '`recall(B)` measures how many of the ground truth boxes were correctly detected.
    A value of 0.48 means the model detected 48% of the true boxes.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`recall(B)`衡量有多少真实框被正确检测到。0.48的值意味着模型检测到了48%的真实框。'
- en: '`mAP50(B)` is the mean average precision at the **intersection over union**
    (**IoU**) threshold of 50%, which measures the model’s precision across different
    confidence thresholds. A prediction is considered correct if the IoU with ground
    truth is at least 50%. A value of 0.50 means the model has 50% mAP at this IoU
    threshold.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mAP50(B)`是在**交并比**（**IoU**）阈值为50%时的平均精度，它衡量了模型在不同置信度阈值下的精度。如果预测与真实框的IoU至少为50%，则认为预测是正确的。0.50的值意味着模型在这个IoU阈值下有50%的mAP。'
- en: '`mAP50-95(B)` is the mean average precision at IoU thresholds between 50% and
    95% and is a more strict metric that expects higher overlap with the ground truth
    to be considered correct. The lower value of 0.23 indicates performance drops
    at higher IoU thresholds.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mAP50-95(B)`是在50%到95%的IoU阈值之间的平均精度，这是一个更严格的指标，它期望与真实框有更高的重叠才能被认为是正确的。0.23的较低值表明在更高的IoU阈值下性能下降。'
- en: '`fitness` combines precision and recall. A model that scores high on precision
    but low on recall would have poor fitness. Similarly, high recall but low precision
    also results in low fitness. A high fitness score requires strong performance
    on both precision and recall metrics. This encourages the model to improve both
    the accuracy and completeness of detections during training. The specific fitness
    value of 0.25 here indicates there is significant room for improvement in precision,
    recall, or both.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fitness`结合了精度和召回率。一个在精度上得分高但在召回率上得分低的模型会有较差的适应性。同样，高召回率但低精度也会导致适应性差。高适应性分数需要在精度和召回率指标上都有良好的表现。这鼓励模型在训练过程中提高检测的准确性和完整性。这里0.25的具体适应性值表明在精度、召回率或两者方面都有很大的改进空间。'
- en: The metrics indicate reasonably good precision but lower recall, meaning the
    model struggles to detect all ground truth boxes. The high precision but lower
    mAP shows many detections are offset from the ground truth. Overall, the metrics
    show room for improvement in the alignment and completeness of detections.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 指标显示精度相当好，但召回率较低，这意味着模型难以检测到所有真实框。高精度但低mAP表明许多检测偏离了真实框。总体而言，指标显示在检测的对齐和完整性方面仍有改进空间。
- en: The next step is thus to select the most informative images to label using our
    active ML approach.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用我们的主动机器学习（ML）方法选择最具有信息量的图像进行标注。
- en: Implementing an active ML strategy
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施主动机器学习策略
- en: We will use the `ultralytics` package, which is highly useful for enabling the
    selection of informative images. This can help us improve the metrics we just
    discussed. This package provides the confidence score for each bounding box prediction,
    which we will utilize here.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`ultralytics`包，这个包非常有用，可以帮助我们选择具有信息量的图像。这可以帮助我们提高我们刚才讨论的指标。此包为每个边界框预测提供置信度分数，我们将在这里使用它。
- en: 'We apply the model to each image in the unlabeled set using a confidence threshold
    of `0.15`. This means that any predictions with a confidence score below 0.15
    will be discarded. You can choose this value based on your specific needs and
    use case. It is important to keep in mind that choosing a low confidence score
    threshold allows for the selection of images where the model lacks confidence:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用置信度阈值`0.15`将模型应用于未标注集中的每个图像。这意味着任何置信度分数低于0.15的预测将被丢弃。你可以根据你的具体需求和用例选择这个值。重要的是要记住，选择一个低的置信度分数阈值允许选择模型缺乏信心的图像：
- en: '[PRE41]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Let’s take a look at some of these images and the predicted bounding boxes:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一些这些图像和预测到的边界框：
- en: '[PRE42]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![Figure 4.5 – Samples of model predictions on images from the unlabeled set](img/B21789_04_5.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – 未标注集图像上的模型预测样本](img/B21789_04_5.jpg)'
- en: Figure 4.5 – Samples of model predictions on images from the unlabeled set
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 – 未标注集图像上的模型预测样本
- en: We can see in *Figure 4**.5* that the model is detecting tumors in the unlabeled
    brain images.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图4.5**.5*中看到模型正在检测未标注的脑部图像中的肿瘤。
- en: 'Let’s collect all the confidence scores of the predicted bounding boxes:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们收集所有预测边界框的置信度分数：
- en: '[PRE43]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We''ll only keep the minimum confidence value for each image. If there is no
    predicted bounding box, we add a confidence score of `10` (a high value to put
    these images at the end of the list of potential images). Confidence scores are
    values ranging from 0 to 1, with 1 being high:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只为每张图像保留最低的置信度值。如果没有预测到的边界框，我们添加一个置信度分数为`10`（一个高值，将这些图像放在潜在图像列表的末尾）。置信度分数是介于0到1之间的值，其中1表示高：
- en: '[PRE44]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We have 6,930 confidence scores, which is correct because we have 6,930 unlabeled
    files.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有6,930个置信度分数，这是正确的，因为我们有6,930个未标注的文件。
- en: 'Next, we select 500 images where the confidence scores are the lowest:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们选择500张置信度分数最低的图像：
- en: '[PRE45]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This returns the following:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下结果：
- en: '[PRE46]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We now get the selected images with the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了以下选定的图像：
- en: '[PRE47]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We move these selected images (and their corresponding label files) to our
    labeled set – this mimics the step where we would have an oracle label these images:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些选定的图像（及其相应的标签文件）移动到我们的标注集中 – 这模拟了我们将有神谕标注这些图像的步骤：
- en: '[PRE48]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let’s check that we have correctly moved the images and label files:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查我们是否正确地移动了图像和标签文件：
- en: '[PRE49]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: These two `print` commands both return 1,490, which is what we expected because
    we had 990 labeled images and then added 500 new image/label pairs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个`print`命令都返回1,490，这是我们预期的，因为我们有990个标注图像，然后添加了500个新的图像/标签对。
- en: 'We can train our model again with this updated dataset:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个更新后的数据集再次训练我们的模型：
- en: '[PRE50]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Then, we evaluate this model on the test set:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在测试集上评估这个模型：
- en: '[PRE51]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Now we get the following metrics:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们得到了以下指标：
- en: '[PRE52]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Comparing these metrics with the previous metrics we got, we can see that the
    precision improved from 0.60 to 0.65, the recall from 0.48 to 0.51, the mAP50
    from 0.50 to 0.54, the mAP50-95 from 0.22 to 0.27, and the fitness from 0.25 to
    0.29\. So, adding the 500 most informative images to our labeled set improved
    our metrics across the board.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些指标与我们之前得到的指标进行比较，我们可以看到，精确度从0.60提高到0.65，召回率从0.48提高到0.51，mAP50从0.50提高到0.54，mAP50-95从0.22提高到0.27，适应性从0.25提高到0.29。因此，将500张最具信息量的图像添加到我们的标注集中，提高了我们各项指标。
- en: We can use a similar method for instance segmentation, which we will cover in
    the next section.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用类似的方法进行实例分割，我们将在下一节中介绍。
- en: Using active ML for a segmentation project
  id: totrans-174
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用主动机器学习进行分割项目
- en: In this section, we will reuse what we did for the object detection task, but
    instead of using an object detection dataset, we will use an instance segmentation
    dataset with the `segment` task of `yolov8`.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重复我们在对象检测任务中做的事情，但我们将使用`yolov8`的`segment`任务的实例分割数据集，而不是使用对象检测数据集。
- en: '`person`, `car`, and so on. Instance segmentation separates individual instances
    of those classes – person 1 versus person 2, or car 1 versus car 2.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`person`、`car`等。实例分割将那些类别的单个实例分开 – 1号人 versus 2号人，或1号车 versus 2号车。'
- en: The combination of localization, classification, and separation of instances
    enables precise analysis of images down to the pixel level. This makes instance
    segmentation useful for applications such as autonomous driving, medical imaging,
    and robotics, where understanding scenes at a fine-grained level is required.
    Some popular instance segmentation algorithms and models are **Mask R-CNN** ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870)),
    **Panoptic FPN** ([https://arxiv.org/abs/1901.02446](https://arxiv.org/abs/1901.02446)),
    and **YOLACT** ([https://arxiv.org/abs/1904.02689](https://arxiv.org/abs/1904.02689)).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 定位、分类和实例分离的组合使得可以精确分析图像直到像素级别。这使得实例分割在需要精细级别理解场景的应用中非常有用，例如自动驾驶、医学成像和机器人技术。一些流行的实例分割算法和模型包括**Mask
    R-CNN** ([https://arxiv.org/abs/1703.06870](https://arxiv.org/abs/1703.06870))、**Panoptic
    FPN** ([https://arxiv.org/abs/1901.02446](https://arxiv.org/abs/1901.02446))和**YOLACT**
    ([https://arxiv.org/abs/1904.02689](https://arxiv.org/abs/1904.02689))。
- en: Let’s download the `strawberry` dataset from Roboflow Universe
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从Roboflow宇宙下载`strawberry`数据集
- en: '[PRE53]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Then, we work through the same steps as we followed for the object detection
    dataset in the preceding section. We rename the subfolders for our demo use case
    and update the YAML file. We end up with the structure shown in *Figure 4**.6*.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按照前面章节中针对对象检测数据集所遵循的相同步骤进行操作。我们为我们的演示用例重命名子文件夹并更新YAML文件。最终我们得到了*图4**.6*中所示的架构。
- en: '![Figure 4.6 – Structure of the strawberry dataset after renaming the subfolders
    for our demo](img/B21789_04_6.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图4.6 – 重命名子文件夹后的草莓数据集结构，用于我们的演示](img/B21789_04_6.jpg)'
- en: Figure 4.6 – Structure of the strawberry dataset after renaming the subfolders
    for our demo
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 – 重命名子文件夹后的草莓数据集结构，用于我们的演示
- en: 'For this dataset, after renaming the folders and updating the YAML file, the
    code returns the following:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，在重命名文件夹并更新YAML文件后，代码返回以下内容：
- en: '[PRE54]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'As we are now training for instance segmentation, we update the training code
    as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在正在训练实例分割，我们更新训练代码如下：
- en: '[PRE55]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Once the training is completed, we evaluate the model using the same code as
    in the previous project and obtain the following metrics:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们使用与上一个项目相同的代码评估模型，并得到以下指标：
- en: '[PRE56]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: The metrics with `(B)` represent the metrics for object detection, while the
    metrics with `(M)` refer to instance segmentation, where `M` stands for masks.
    The metrics are the same between the two types; the only difference is that the
    `M` metrics are computed on the pixels from the masks rather than those from the
    bounding boxes.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 带有`(B)`的指标代表目标检测的指标，而带有`(M)`的指标指的是实例分割，其中`M`代表蒙版。两种类型的指标相同；唯一的区别是`M`指标是在蒙版的像素上计算的，而不是在边界框的像素上。
- en: Following the same logic, we then select the images to label in order to improve
    our metrics.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 按照相同的逻辑，我们然后选择要标记的图像以提高我们的指标。
- en: 'The code is slightly different this time when we run the model on each image
    in the unlabeled set:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在未标记集中的每张图像上运行模型时，代码略有不同：
- en: '[PRE57]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: We have to specify that we are performing a segmentation task and choose a higher
    confidence threshold to avoid memory issues in Colab.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指定我们正在进行分割任务，并选择一个更高的置信度阈值，以避免在Colab中发生内存问题。
- en: 'Let’s take a look at the model’s predictions on some of the unlabeled images:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模型对一些未标记图像的预测：
- en: '[PRE58]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: This returns the image depicted in *Figure 4**.7*.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回了*图4**.7*中所示的照片。
- en: '![Figure 4.7 – Samples of model predictions on images from the unlabeled set](img/B21789_04_7.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图4.7 – 从未标记集图像上的模型预测样本](img/B21789_04_7.jpg)'
- en: Figure 4.7 – Samples of model predictions on images from the unlabeled set
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 – 从未标记集图像上的模型预测样本
- en: In *Figure 4**.7*, we can see that the model is correctly detecting most of
    the strawberries. The object detection is represented by the green bounding boxes
    in the images, while the segmentation is indicated by the white overlaying masks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在**图4**.7中，我们可以看到模型正确地检测到了大部分的草莓。图像中的绿色边界框表示目标检测，而白色叠加的蒙版表示分割。
- en: 'We then follow the steps discussed in the preceding section on object detection,
    where we selected the 500 images to label next, and we get the following result:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遵循前面章节中讨论的对象检测步骤，选择了500张图像进行标记，并得到以下结果：
- en: '[PRE59]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'We move these images to our labeled set, and thus go from 184 images in the
    labeled set to 684\. We run the training on the updated labeled set, followed
    by the evaluation, and obtain these metrics:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些图像移动到我们的标记集中，因此从标记集中的184张图像增加到684张。我们在更新后的标记集上运行训练，然后进行评估，并得到以下指标：
- en: '[PRE60]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Let’s compare those to the metrics we had before the addition of the 500 most
    informative images:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下在添加了500张最有信息量的图像之前的指标：
- en: '[PRE61]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We can observe that all metrics have improved.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到所有指标都有所提高。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In conclusion, this chapter has demonstrated how active ML can be applied to
    optimize the training of computer vision models. As we have seen, computer vision
    tasks such as image classification, object detection, and instance segmentation
    require large labeled datasets to train **convolutional neural networks** (**CNNs**).
    Manually collecting and labeling this much data is expensive and time-consuming.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章展示了如何将主动机器学习应用于优化计算机视觉模型的训练。正如我们所见，图像分类、目标检测和实例分割等计算机视觉任务需要大量的标记数据集来训练**卷积神经网络**（**CNNs**）。手动收集和标记如此大量的数据既昂贵又耗时。
- en: Active ML provides a solution to this challenge by intelligently selecting the
    most informative examples to be labeled by a human oracle. Strategies such as
    uncertainty sampling query the model to find the data points it is least certain
    about. By labeling only these useful data points, we can train our models with
    significantly less data-labeling effort required.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 主动机器学习通过智能选择最具有信息量的示例由人类专家进行标记，从而为这一挑战提供了解决方案。例如，不确定性采样策略会查询模型以找到它最不确定的数据点。通过仅标记这些有用的数据点，我们可以用显著减少的数据标记工作来训练我们的模型。
- en: In this chapter, we covered implementing active ML approach for diverse computer
    vision applications. By interactively querying the model and refining the training
    data, we can rapidly improve model performance at a fraction of the labeling cost.
    These techniques make it feasible to develop computer vision systems even with
    limited data.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了为各种计算机视觉应用实现主动机器学习方法的步骤。通过交互式查询模型和细化训练数据，我们可以以标记成本的一小部分快速提高模型性能。这些技术使得即使在有限数据的情况下开发计算机视觉系统也成为可能。
- en: The active ML implementations presented offer new possibilities for building
    performant and robust computer vision models without needing massive datasets.
    With these strategies, you can optimize and target the data collection and training
    efforts for efficient results. Going forward, active ML will become an essential
    tool for developing real-world computer vision systems.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中提出的主动机器学习实现提供了在无需大量数据集的情况下构建高性能和鲁棒的计算机视觉模型的新可能性。通过这些策略，您可以优化和针对数据收集和训练工作以实现高效的结果。展望未来，主动机器学习将成为开发现实世界计算机视觉系统的一个基本工具。
- en: In the next chapter, we will explore how to leverage active ML for big data
    projects that use large amounts of data, such as videos.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨如何利用主动机器学习来处理使用大量数据的大数据项目，例如视频。
