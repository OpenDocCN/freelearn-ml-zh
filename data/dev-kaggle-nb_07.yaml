- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Text Analysis Is All You Need
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析一切所需
- en: In this chapter, we will learn how to analyze text data and create machine learning
    models to help us. We will use the *Jigsaw Unintended Bias in Toxicity Classification*
    dataset (see *Reference 1*). This competition had the objective of building models
    that detect toxicity and reduce unwanted bias toward minorities that might be
    wrongly associated with toxic comments. With this competition, we introduce the
    field of **Natural Language Processing** (**NLP**).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '在本章中，我们将学习如何分析文本数据并创建机器学习模型来帮助我们。我们将使用*Jigsaw无意中在毒性分类中的偏差*数据集（参见*参考文献1*）。这个竞赛的目标是构建检测毒性和减少可能错误地与毒性评论相关联的少数族裔的不当偏见的模型。通过这个竞赛，我们引入了**自然语言处理**（**NLP**）领域。 '
- en: The data used in the competition originates from the Civil Comments platform,
    which was founded by Aja Bogdanoff and Christa Mrgan in 2015 (see *Reference 2*)
    with the aim of solving the problem of civility in online discussions. When the
    platform was closed in 2017, they chose to keep around 2 million comments for
    researchers who want to understand and improve civility in online conversations.
    Jigsaw was the organization that sponsored this effort and then started a competition
    for language toxicity classification. In this chapter, we’re going to transform
    pure text into meaningful, model-ready numbers to be able to classify them into
    groups according to the toxicity of the comments.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 竞赛中使用的数据来源于由Aja Bogdanoff和Christa Mrgan于2015年创立的Civil Comments平台（参见*参考文献2*），旨在解决在线讨论中的礼貌问题。当平台于2017年关闭时，他们选择保留约200万条评论，供研究人员理解并改善在线对话中的礼貌。Jigsaw组织赞助了这项工作，并随后开始了一场语言毒性分类竞赛。在本章中，我们将把纯文本转换为有意义的、模型准备好的数字，以便根据评论的毒性将它们分类到不同的组别。
- en: 'In a nutshell, this chapter will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，本章将涵盖以下主题：
- en: Data exploration of the *Jigsaw Unintended Bias in Toxicity Classification*
    competition dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对*Jigsaw无意中在毒性分类中的偏差*竞赛数据集的数据进行探索
- en: Introduction to NLP-specific processing and analysis techniques, including word
    frequency, tokenization, part-of-speech tagging, named entity recognition, and
    word embeddings
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍NLP特定的处理和分析技术，包括词频、分词、词性标注、命名实体识别和词嵌入
- en: The iterative refinement of the preprocessing of text data to prepare a model
    baseline
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对文本数据的预处理进行迭代优化以准备模型基线
- en: A model baseline for this text classification competition
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为这次文本分类竞赛创建一个模型基线
- en: What is in the data?
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据中有什么？
- en: 'The data from the *Jigsaw Unintended Bias in Toxicity Classification* competition
    dataset contains 1.8 million rows in the training set and 97,300 rows in the test
    set. The test data contains only a **comment** column and does not contain a target
    (the value to predict) column. Training data contains, besides the **comment**
    column, another 43 columns, including the target feature. The target is a number
    between 0 and 1, which represents the annotation that is the objective of the
    prediction for this competition. This target value represents the degree of toxicity
    of a comment (`0` means zero/no toxicity and `1` means maximum toxicity), and
    the other 42 columns are flags related to the presence of certain sensitive topics
    in the comments. The topic is related to five categories: race and ethnicity,
    gender, sexual orientation, religion, and disability. In more detail, these are
    the flags per each of the five categories:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 来自*Jigsaw无意中在毒性分类中的偏差*竞赛数据集的数据包含训练集中的180万行和测试集中的97,300行。测试数据仅包含一个**评论**列，不包含目标（预测值）列。训练数据除了**评论**列外，还包括另外43列，包括目标特征。目标是介于0和1之间的数字，代表预测此竞赛的目标注释。此目标值表示评论的毒性程度（`0`表示零/无毒性，`1`表示最大毒性），其他42列是与评论中存在某些敏感主题相关的标志。主题与五个类别相关：种族和民族、性别、性取向、宗教和残疾。更详细地说，以下是每个类别的标志：
- en: '**Race and ethnicity**: `asian`, `black`, `jewish`, `latino`, `other_race_or_ethnicity`,
    and `white`'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**种族和民族**：`亚洲人`、`黑人`、`犹太人`、`拉丁美洲人`、`其他种族或民族`和`白人`'
- en: '**Gender**: `female`, `male`, `transgender`, and `other_gender`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**：`女性`、`男性`、`跨性别`和`其他性别`'
- en: '**Sexual orientation**: `bisexual`, `heterosexual`, `homosexual_gay_or_lesbian`,
    and `other_sexual_orientation`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性取向**：`双性恋`、`异性恋`、`同性恋/女同性恋`和`其他性取向`'
- en: '**Religion**: `atheist`, `buddhist`, `christian`, `hindu`, `muslim`, and `other_religion`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**宗教**：`无神论者`、`佛教徒`、`基督徒`、`印度教徒`、`穆斯林`和`其他宗教`'
- en: '**Disability**: `intellectual_or_learning_disability`, `other_disability`,
    `physical_disability`, and `psychiatric_or_mental_illness`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残疾**：`智力或学习障碍`、`其他残疾`、`身体残疾`和`精神或心理疾病`'
- en: 'There are also a few features (a.k.a. columns in the dataset) that serve for
    identifying the comment: `created_data`, `publication_id`, `parent_id`, and `article_id`.
    Also provided are several user feedback information features associated with the
    comments: `rating`, `funny`, `wow`, `sad`, `likes`, `disagree`, and `sexual_explicit`.
    Finally, there are also two fields relative to annotations: `identity_annotator_count`
    and `toxicity_annotator_count`.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些特征（即数据集中的列）用于识别评论：`created_data`、`publication_id`、`parent_id`和`article_id`。还提供了与评论相关的一些用户反馈信息特征：`rating`、`funny`、`wow`、`sad`、`likes`、`disagree`和`sexual_explicit`。最后，还有两个与注释相关的字段：`identity_annotator_count`和`toxicity_annotator_count`。
- en: Let’s start with a quick analysis of the target feature and the sensitive features.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从对目标特征和敏感特征的快速分析开始。
- en: Target feature
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标特征
- en: 'We would like to look first at the distribution of the target feature. Let’s
    look at the histogram for these values’ distribution in *Figure 7.1*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先想看看目标特征的分布。让我们看看*图7.1*中这些值的分布直方图：
- en: '![A picture containing text, plot, line, diagram  Description automatically
    generated](img/B20963_07_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、图表、线条、图表的图片，自动生成描述](img/B20963_07_01.png)'
- en: 'Figure 7.1: Distribution of target values (training data, 1.9 million columns)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：目标值分布（训练数据，190万列）
- en: 'For this histogram, we’ve used a logarithmic scale on the *y* axis; the reason
    behind this is that we want to see the skewed distribution of values toward 0\.
    As we do this, we observe that we have a bimodal distribution: peak values at
    around 0.1 intervals, decreasing in amplitude, and less frequent values with a
    slowly rising trend, superposed. Most of the target values (above 1 million) are
    `0`.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个直方图，我们在*Y*轴上使用了对数刻度；这样做的原因是我们想看到值向0的偏斜分布。当我们这样做时，我们观察到我们有一个双峰分布：峰值在约0.1的间隔处，振幅减小，并且出现频率较低，趋势缓慢上升，叠加。大部分的目标值（超过100万）都是`0`。
- en: Sensitive features
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敏感特征
- en: We will look at the distribution of sensitive features as listed earlier (race
    and ethnicity, gender, sexual orientation, religion, and disability). We will
    again use a logarithmic scale on the *y* axis due to the skewness of the distribution
    (similar to the target, we have a concentration at `0`).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看之前列出的敏感特征的分布（种族和民族、性别、性取向、宗教和残疾）。由于分布的偏斜（与目标类似，我们在`0`处有集中），我们将在*Y*轴上再次使用对数刻度。
- en: '*Figure 7.2* shows the distribution of race and ethnicity feature values. These
    look discontinuous and very discrete, with the histogram showing a few separate
    peaks:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.2*展示了种族和民族特征值的分布。这些值看起来不连续且非常离散，直方图显示了几个分离的峰值：'
- en: '![A graph with numbers and lines  Description automatically generated with
    medium confidence](img/B20963_07_02.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![包含数字和线条的图表，中等置信度自动生成描述](img/B20963_07_02.png)'
- en: 'Figure 7.2: Distribution of race and ethnicity feature values'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：种族和民族特征值的分布
- en: 'We can observe a similar distribution for the gender feature values in *Figure
    7.3*:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图7.3*中观察到性别特征值有类似的分布：
- en: '![A picture containing text, screenshot, plot, line  Description automatically
    generated](img/B20963_07_03.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、线条的图片，自动生成描述](img/B20963_07_03.png)'
- en: 'Figure 7.3: Distribution of gender feature values'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：性别特征值分布
- en: 'In *Figure 7.4*, we show the distribution of the additional toxicity features
    (`severe_toxicity`, `obscene`, `identity_attack`, `insult`, or `threat`) values.
    As you can see, the distribution is more even, and with an increasing trend for
    `insult`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.4*中，我们展示了额外毒性特征值（`severe_toxicity`、`obscene`、`identity_attack`、`insult`或`threat`）的分布。如您所见，分布更加均匀，并且`insult`有增加的趋势：
- en: '![A picture containing screenshot, text, plot, line  Description automatically
    generated](img/B20963_07_04.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![包含截图、文本、图表的图片，自动生成描述](img/B20963_07_04.png)'
- en: 'Figure 7.4: Distribution of additional toxicity feature values'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：额外毒性特征值的分布
- en: 'Let us also look at the correlation between the target values and the race
    or ethnicity, gender, sexual orientation, religion, and disability feature values.
    We are not showing here the correlation matrix for all the features, but you can
    inspect it in the notebook associated with this chapter. Here, we only show the
    first 16 features correlated with the target, ordered by correlation factor:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也看看目标值与种族或民族、性别、性取向、宗教和残疾特征值之间的相关性。我们在此不展示所有特征的相关矩阵，但您可以在与本章节相关的笔记本中检查它。在这里，我们只展示了与目标相关的第一个16个特征，按相关性因素排序：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let’s look at the top 15 features ordered by correlation factor with the target
    in *Figure 7.5*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看*图7.5*中按相关性因素与目标排序的前15个特征：
- en: '![A screen shot of a computer code  Description automatically generated](img/B20963_07_05.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![计算机代码的屏幕截图  自动生成的描述](img/B20963_07_05.png)'
- en: 'Figure 7.5: Top 15 correlation factors of other features with the target feature'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：其他特征与目标特征的前15个相关因素
- en: 'Next, in *Figure 7.6*, we represent the correlation matrix for these selected
    features and the target feature:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在*图7.6*中，我们展示了这些选定特征与目标特征之间的相关矩阵：
- en: '![A chart with numbers and a red line  Description automatically generated
    with medium confidence](img/B20963_07_06.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![带有数字和红色线的图表  中度置信度自动生成的描述](img/B20963_07_06.png)'
- en: 'Figure 7.6: Correlation matrix between the target and 15 features with the
    highest correlation with it'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：目标与相关性最高的15个特征之间的相关矩阵
- en: We can observe that `target` is highly correlated with `insult` (0.93), `obscene`
    (0.49), and `identity_attack` (0.45). Also, `severe_toxicity` is correlated positively
    with `insult` and `obscene`. `identity_attack` has a small correlation with being
    `white`, `black`, `muslim`, and `homosexual_gay_or_lesbian`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到`target`与`insult`（0.93）、`obscene`（0.49）和`identity_attack`（0.45）高度相关。此外，`severe_toxicity`与`insult`和`obscene`呈正相关。`identity_attack`与`white`、`black`、`muslim`和`homosexual_gay_or_lesbian`有轻微的相关性。
- en: 'We investigated the distribution of the `target` feature (feature to predict)
    and of the sensitive features. Now we will move to the main topic of analysis
    for this chapter: the comments text. We will apply several NLP-specific analysis
    techniques.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们研究了`target`特征（预测特征）和敏感特征的分布。现在我们将转向本章分析的主要主题：评论文本。我们将应用几种NLP特定的分析技术。
- en: Analyzing the comments text
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分析评论文本
- en: 'NLP is a field of AI that involves the use of computational techniques to enable
    computers to understand, interpret, transform, and even generate human language.
    NLP uses several techniques, algorithms, and models to process and analyze large
    datasets of text. Among these techniques, we can mention:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: NLP是人工智能的一个领域，它涉及使用计算技术使计算机能够理解、解释、转换甚至生成人类语言。NLP使用多种技术、算法和模型来处理和分析大量文本数据集。在这些技术中，我们可以提到：
- en: '**Tokenization**: Breaks down text into smaller units, like words, parts of
    words, or characters'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分词**：将文本分解成更小的单元，如单词、词的一部分或字符'
- en: '**Lemmatization or stemming**: Reduces the words to dictionary form or removes
    the last few characters to get to a common form (stem)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词形还原或词干提取**：将单词还原为词典形式或去除最后几个字符以得到一个共同形式（词干）'
- en: '**Part-of-Speech** (**POS**) **tagging**: Assigns a grammatical category (for
    example, nouns, verbs, proper nouns, and adjectives) to each word in a sequence'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词性标注**（**POS**）**标记**：将语法类别（例如，名词、动词、专有名词和形容词）分配给序列中的每个单词'
- en: '**Named Entity Recognition** (**NER**): Identifies and classifies entities
    (for example, names of people, organizations, and places)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**命名实体识别**（**NER**）：识别和分类实体（例如，人名、组织名和地名）'
- en: '**Word embeddings**: Use a high-dimensional space to represent the words, a
    space in which the position of each word is determined by its relationship with
    other words'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词嵌入**：使用高维空间来表示单词，在这个空间中，每个单词的位置由其与其他单词的关系决定'
- en: '**Machine learning models**: Train models on annotated datasets to learn patterns
    and relationships in language data'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习模型**：在标注数据集上训练模型以学习语言数据中的模式和关系'
- en: NLP applications can include sentiment analysis, machine translation, question
    answering, text summarization, and text classification.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: NLP应用可以包括情感分析、机器翻译、问答、文本摘要和文本分类。
- en: 'With that quick introduction to NLP, let us inspect the actual comment text.
    We will build a few word cloud graphs (using a 20,000-comment subset of the entire
    dataset). We will look first at the overall word distribution (see the notebook
    associated with this chapter), then at the distribution of words with target values
    above 0.75 and below 0.25:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在对NLP进行快速介绍之后，让我们检查实际的评论文本。我们将构建几个词云图（使用整个数据集的20,000条评论子集）。我们首先查看整体单词分布（参见本章相关的笔记本），然后查看目标值高于0.75和低于0.25的单词分布：
- en: '![](img/B20963_07_07.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_07.png)'
- en: 'Figure 7.7: Prevalent words (1-gram) in comments with low target score < 0.25
    (left) and high target score > 0.75 (right)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：低目标分数<0.25（左）和高目标分数>0.75（右）的评论中的常见单词（1-gram）
- en: '`target` is very highly correlated with `insult`, and we expect to see a rather
    close distribution of words in the word clouds for the two features. This hypothesis
    is confirmed, and *Figure 7.8* illustrates this very well (both for low score
    and high score):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '`目标`与`侮辱`高度相关，我们预计在两个特征的词云中会看到相当接近的单词分布。这一假设得到了证实，*图7.8*很好地说明了这一点（无论是低分还是高分）：'
- en: '![](img/B20963_07_08.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_08.png)'
- en: 'Figure 7.8: Prevalent words (1-gram) in comments with low insult score < 0.25
    (left) and high insult score > 0.75 (right)'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：低侮辱分数<0.25（左）和高侮辱分数>0.75（右）的评论中的常见单词（1-gram）
- en: As you can see, distributions show similar words with high frequency for both
    low scores and high scores for **target** and **insult**.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，分布显示了**目标**和**侮辱**的低分和高分中高频相似单词。
- en: More word clouds are available in the associated notebook for `threat`, `obscene`,
    and other features. These word clouds give us a good initial intuition for the
    most frequent words. We will perform a more detailed analysis of word frequency
    in the entire corpus vocabulary in the *Building the vocabulary* and *Checking
    vocabulary coverage* sections. For now, we can observe that the analysis we performed
    is limited to individual word frequency, without capturing how these words are
    grouped over the entire corpus – in other words, how various words are used together
    and, based on this, identifying the main themes in the corpus. Such processing,
    aimed to reveal the underlying semantic structure of the entire corpus, is called
    **topic modeling**. The analysis of co-occurrence patterns of words in this approach
    allows us to reveal the latent topics existent in the text.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关的笔记本中还有更多关于`威胁`、`淫秽`和其他特征的词云。这些词云为我们提供了对最频繁单词的良好初始直觉。我们将在*构建词汇表*和*检查词汇覆盖范围*部分对整个语料库词汇中的单词频率进行更详细的分析。目前，我们可以观察到，我们进行的分析仅限于单个单词的频率，而没有捕捉到这些单词在整个语料库中的分组情况——换句话说，各种单词是如何一起使用的，以及基于此，如何识别语料库中的主要主题。这种旨在揭示整个语料库潜在语义结构的处理称为**主题建模**。在此方法中，对单词共现模式的分析使我们能够揭示文本中存在的潜在主题。
- en: The inspiration for the implementation of the topic modeling approach in the
    associated notebook is from a set of articles and tutorials on topic modeling
    using latent Dirichlet allocation (see *References* *5*–*10*).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 相关笔记本中主题建模实现灵感的来源是一系列关于使用潜在狄利克雷分配进行主题建模的文章和教程（见*参考文献*5-10）。
- en: Topic modeling
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题建模
- en: 'We start by preprocessing the comments text, using the `gensim` library to
    eliminate special characters, frequently used words, connection words (or stopwords),
    and words with lengths less than 2:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过预处理评论文本，使用`gensim`库来消除特殊字符、常用词、连接词（或停用词）以及长度小于2的单词：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following code applies the defined `preprocess` function to all the comments:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将定义的`preprocess`函数应用于所有评论：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we create a dictionary of words using `dictionary` from `gensim`/`corpora`.
    We also filter extremes, to eliminate less frequent words and limit the size of
    the vocabulary:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`gensim`/`corpora`中的`dictionary`创建一个单词字典。我们还过滤极端值，以消除不常见的单词并限制词汇表的大小：
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With these restrictions, we go then to the next step and generate a *bag of
    words* (`bow`) corpus from the dictionary. Then we apply **TF-IDF** (**Term Frequency-Inverse
    Document Frequency**) to this corpus, which provides a numerical representation
    of the importance of a word within a document in a collection or corpus of documents.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些限制下，我们接下来生成一个从字典中生成的*词袋*（`bow`）语料库。然后，我们对这个语料库应用**TF-IDF**（**词频-逆文档频率**），它为文档在文档集合或语料库中的重要性提供了一个数值表示。
- en: 'The `tf` component measures how frequently a word appears in a document. The
    `idf` component shows the significance of a word across the entire corpus of documents
    (in our case, over the full set of comments). This factor decreases with a higher
    occurrence of a term in the documents. Therefore, after the `tfidf` transform,
    the coefficient for one word and one document is larger for words that are infrequent
    at the corpus level and appear with higher frequency inside the current document:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf`组件衡量一个单词在文档中出现的频率。`idf`组件显示一个单词在整个文档语料库中的重要性（在我们的案例中，是整个评论集合）。这个因素随着一个术语在文档中出现的频率增加而降低。因此，在`tfidf`转换之后，一个单词和一个文档的系数对于在语料库级别不常见但在当前文档中频率较高的单词来说更大：'
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then apply *Latent Dirichlet Allocation* (`lda`), a topic model that generates
    topics based on word frequency on this corpus, using the `gensim` implementation
    for parallel processing (`LdaMulticore`):'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应用*潜在狄利克雷分配*（`lda`），这是一种基于该语料库中单词频率生成主题的主题模型，使用`gensim`的并行处理实现（`LdaMulticore`）：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s represent the first 10 topics, with 5 words for each of them:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用5个单词来表示前10个主题：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The topic words are shown with the associated relative weight in the topic,
    as shown here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 主题单词以与主题相关的相对权重显示，如下所示：
- en: '![](img/B20963_07_09.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_09.png)'
- en: 'Figure 7.9: Top 10 topics, with 5 words (most relevant) selected per topic'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：前10个主题，每个主题选择5个（最相关）单词
- en: 'Once we extract the topics, we can go through the documents and identify which
    topics are present in the current document (in our case, comment). In *Figure
    7.10*, we show the dominant topics (with the relative weights) for one document
    (the following is the code used to generate the list of topics for a selected
    comment):'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们提取了主题，我们就可以遍历文档并识别当前文档（在我们的案例中，是评论）中包含哪些主题。在*图7.10*中，我们展示了单个文档的占主导地位的主题（以及相对权重）（以下是用以生成选定评论的主题列表的代码）：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![](img/B20963_07_10.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_10.png)'
- en: 'Figure 7.10: Topics associated (each with relative importance) with one comment'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：与一个评论相关联（每个都有相对重要性）的主题
- en: We prefer the **pyLDAvis** visualization tool to represent the topics. In *Figure
    7.11*, we show a screenshot of this tool (in the notebook, we generated 20 topics
    for train data and, separately, for test data). The dashboard in *Figure 7.11*
    displays the **Intertopic Distance Map**. Here, the topic’s relative dimension
    (or influence) is represented by the size of the disks and the topic’s relative
    distance by their mutual distance. On the right side, for the currently selected
    topic (in the left-side panel), we can see the top 30 most relevant terms per
    topic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们更喜欢使用**pyLDAvis**可视化工具来表示主题。在*图7.11*中，我们展示了该工具的屏幕截图（在笔记本中，我们为训练数据生成了20个主题，并为测试数据单独生成了主题）。*图7.11*中的仪表板显示了**主题间距离图**。在这里，主题的相对维度（或影响力）由圆盘的大小表示，主题的相对距离由它们的相互距离表示。在右侧，对于当前选定的主题（在左侧面板中），我们可以看到每个主题前30个最相关的术语。
- en: The disks with a light color (blue in the notebook) represent the overall word
    frequency. The darker colored disks (red in the notebook) represent the estimated
    word frequency within the selected topic. We can use a slide as well to adjust
    the relevance metric (in the picture, this is set to `1`). We can further refine
    this analysis by improving the preprocessing step (for example, we can add more
    stopwords, specific to this corpus), adjusting the parameters for the dictionary
    formation, and controlling the parameters for `tfidf` and `lda`. Due to the complexity
    of the LDA procedure, we also reduced the size of the corpus, by subsampling the
    train data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 色彩较浅的圆盘（笔记本中的蓝色）代表整体单词频率。颜色较深的圆盘（笔记本中的红色）代表所选主题内的估计单词频率。我们可以使用滑块来调整相关性指标（在图片中，这设置为`1`）。我们可以通过改进预处理步骤（例如，我们可以添加更多特定于该语料库的停用词）、调整字典形成的参数以及控制`tfidf`和`lda`的参数来进一步细化此分析。由于LDA过程的复杂性，我们还通过从训练数据中子采样来减小语料库的大小。
- en: 'As shown in the following screenshot, on the left panel of the topic modeling
    dashboard generated using the pyLDAvis tool, we see the **Intertopic Distance
    Map** – with relative dimension of topic influence in the corpus and the relative
    topic’s distance:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下屏幕截图所示，在pyLDAvis工具生成的主题建模仪表板的左侧面板中，我们看到**主题间距离图**——在语料库中主题影响力的相对维度和相对主题的距离：
- en: '![](img/B20963_07_11.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_11.png)'
- en: 'Figure 7.11: Topic modeling dashboard generated using the pyLDAvis tool (left
    panel)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：使用pyLDAvis工具生成的主题建模仪表板（左侧面板）
- en: 'On the right-side panel of the topic modeling dashboard generated using the
    pyLDAvis tool, for the selected topic, we see the top 30 most relevant terms per
    topic, with blue for the overall term frequency in the corpus and red for the
    estimated term frequency within the selected topic:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用pyLDAvis工具生成的主题建模仪表板的右侧面板中，对于选定的主题，我们可以看到每个主题中最相关的30个术语，蓝色代表语料库中的整体词频，红色代表所选主题内的估计词频：
- en: '![](img/B20963_07_12.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_07_12.png)'
- en: 'Figure 7.12: Topic modeling dashboard generated using the pyLDAvis tool (right
    panel)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：使用pyLDAvis工具生成的主题建模仪表板（右侧面板）
- en: We can repeat the analysis over the entire corpus, but this will require more
    computational resources than are available on Kaggle.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对整个语料库进行重复分析，但这将需要比Kaggle上可用的计算资源更多的资源。
- en: And with that, we have explored the topics in the comments text corpus, using
    the `lda` method. With this procedure, we revealed one of the hidden (or latent)
    structures in the corpus of the text. Now we can better understand not only the
    frequency of the words but also how words are associated in comments, to form
    topics discussed by the commentators. Let’s continue to explore the corpus, from
    a different perspective. We will take every comment and analyze, using NER, what
    types of concepts are present in the text. Then, we will start to look at the
    syntactic elements and use POS tagging to extract the nouns, verbs, adjectives,
    and other parts of speech.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以上内容，我们已经使用`lda`方法探索了评论文本语料库中的主题。通过这个程序，我们揭示了文本语料库中的一个隐藏（或潜在）结构。现在我们可以更好地理解不仅单词的频率，而且单词在评论中的关联方式，从而形成评论者讨论的主题。让我们继续从不同的角度探索语料库。我们将对每个评论进行分析，使用NER来确定文本中存在哪些类型的概念。然后，我们将开始关注句法元素，并使用POS标记来提取名词、动词、形容词和其他词性。
- en: The reason we review these NLP techniques is twofold. First, we want to give
    you a glimpse of the richness of the tools and techniques available in NLP. Second,
    for more complex machine learning models, you can include features derived using
    these methods. For example, you can add, besides other features extracted from
    the text, features obtained by the use of NER or POS tagging.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾这些NLP技术的原因有两个。首先，我们希望让你一窥NLP中可用的工具和技术之丰富。其次，对于更复杂的机器学习模型，你可以包括使用这些方法导出的特征。例如，除了从文本中提取的其他特征外，你还可以添加通过NER或POS标记获得的特征。
- en: Named entity recognition
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 命名实体识别
- en: Let’s perform NER on a selection of comments. NER is an information extraction
    task that aims to identify and extract named entities in unstructured data (text).
    Named entities are people, organizations, geographical places, dates and times,
    amounts, and currencies. There are several available methods to identify and extract
    named entities, with the most frequently used being `spacy` and `transformers`.
    In our case, we will use `spacy` to perform NER. We prefer `spacy` because it
    requires fewer resources compared with transformers and yet gives good results.
    Something to note here is that `spacy` is also available in 23 languages, including
    English, Portuguese, Spanish, Russian, and Chinese.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在一组评论上执行NER。NER是一种信息提取任务，旨在识别和提取非结构化数据（文本）中的命名实体。命名实体包括人名、组织、地理位置、日期和时间、数量和货币。有几种可用的方法来识别和提取命名实体，其中最常用的是`spacy`和`transformers`。在我们的案例中，我们将使用`spacy`来执行NER。我们更喜欢`spacy`，因为它与transformers相比需要的资源更少，但仍然能给出良好的结果。在此需要注意的是，`spacy`也支持包括英语、葡萄牙语、西班牙语、俄语和中文在内的23种语言。
- en: 'First, we initialize an `nlp` object using the `spacy.load` function:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`spacy.load`函数初始化一个`nlp`对象：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will load the `'en_core_web_sm'` (`sm` stands for small) `spacy` pipeline,
    which includes the `tok2vec`, `tagger`, `parser`, `senter`, `ner`, `attribute_ruler`,
    and `lemmatizer` components. We will not use all the functionality provided by
    this pipeline; we are interested in the `nlp` component.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将加载`'en_core_web_sm'`（`sm`代表小型）`spacy`管道，该管道包括`tok2vec`、`tagger`、`parser`、`senter`、`ner`、`attribute_ruler`和`lemmatizer`组件。我们不会使用此管道提供的所有功能；我们感兴趣的是`nlp`组件。
- en: 'Then, we create a selection of comments. We filter documents that contain either
    the name `Obama` or the name `Trump` and have less than 100 characters. For the
    purpose of this demonstration, we do not want to manipulate large sentences; it
    will be easier to follow the demonstrations if we operate with smaller sentences.
    The next code fragment will perform the selection:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建了一个评论选择。我们筛选出包含名称 `奥巴马` 或 `特朗普` 且字符数少于 100 的文档。出于演示目的，我们不希望操作大型句子；如果我们使用较小的句子进行操作，将更容易跟随演示。下一个代码片段将执行选择：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can visualize the result of applying NER in two ways. One way is to print
    out the text’s start and end characters and the entity label for each entity identified
    in the current comment. An alternative way is to use `displacy` rendering from
    `spacy`, which will decorate each entity with a selected color and add the entity
    name beside the entity text (see *Figure 7.13*).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过两种方式可视化应用 NER 的结果。一种方式是打印出文本中每个识别实体的起始和结束字符以及实体标签。另一种方式是使用来自 `spacy` 的
    `displacy` 渲染，它将为每个实体添加选定的颜色，并在实体文本旁边添加实体名称（见 *图 7.13*）。
- en: 'The following code is for the extraction of entities using `nlp` and the preparation
    of visualization using `displacy`. Before showing the annotated text using `displacy`,
    we are printing each entity text, followed by its position (start and end character
    positions) and the entity label:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于使用 `nlp` 提取实体以及使用 `displacy` 准备可视化。在显示使用 `displacy` 注释的文本之前，我们打印出每个实体文本，然后是其位置（起始和结束字符位置）和实体标签：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'There are multiple labels predefined in `spacy` `nlp`. We can extract the meaning
    of each one with a simple piece of code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`spacy` `nlp` 中预定义了多个标签。我们可以用一段简单的代码提取每个标签的含义：'
- en: '[PRE11]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Here is the resulting list of labels and the meaning of each one:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是标签列表及其含义：
- en: '**CARDINAL**: Numerals that do not fall under another type'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CARDINAL**: 不属于其他类型的数字'
- en: '**DATE**: Absolute or relative dates or periods'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DATE**: 绝对或相对日期或时间段'
- en: '**EVENT**: Named hurricanes, battles, wars, sports events, and so on'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**EVENT**: 命名的飓风、战役、战争、体育赛事等'
- en: '**FAC**: Buildings, airports, highways, bridges, and so on'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**FAC**: 建筑物、机场、高速公路、桥梁等'
- en: '**GPE**: Countries, cities, or states'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPE**: 国家、城市或州'
- en: '**LANGUAGE**: Any named language'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LANGUAGE**: 任何命名语言'
- en: '**LAW**: Named documents made into laws'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LAW**: 被制成法律的命名文件'
- en: '**LOC**: Non-GPE locations, mountain ranges, or bodies of water'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LOC**: 非GPE地点、山脉或水体'
- en: '**MONEY**: Monetary values, including units'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MONEY**: 货币价值，包括单位'
- en: '**NORP**: Nationalities or religious or political groups'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NORP**: 国籍或宗教或政治团体'
- en: '**ORDINAL**: `first`, `second`, and so on'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ORDINAL**: `第一`、`第二`等'
- en: '**ORG**: Companies, agencies, institutions, and so on'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ORG**: 公司、机构、机构等'
- en: '**PERCENT**: Percentage, including `%`'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PERCENT**: 百分比，包括 `%`'
- en: '**PERSON**: People, including fictional'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PERSON**: 人，包括虚构人物'
- en: '**PRODUCT**: Objects, vehicles, foods, and so on (not services)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PRODUCT**: 物体、车辆、食品等（不包括服务）'
- en: '**QUANTITY**: Measurements, such as weight or distance'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**QUANTITY**: 重量或距离等度量'
- en: '**TIME**: Times less than a day'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TIME**: 低于一天的时间'
- en: '**WORK_OF_ART**: Titles of books, songs, and so on'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**WORK_OF_ART**: 书籍、歌曲等的标题'
- en: '![A picture containing text, screenshot, font, line  Description automatically
    generated](img/B20963_07_13.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、字体、行描述自动生成](img/B20963_07_13.png)'
- en: 'Figure 7.13: NER using spacy and displacy for visualization of NER results'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13：使用 spacy 和 displacy 进行 NER 结果的可视化
- en: In the top example of the preceding screenshot, Bernie (Sanders) is recognized
    correctly as a person (**PERSON**), while (Donald) Trump is identified as an organization
    (**ORG**). This might be because former president Trump frequently used his name
    as part of the name of several of the organizations he founded while being a businessperson.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的截图顶部示例中，Bernie（Sanders）被正确识别为人物（**PERSON**），而（Donald）Trump 被识别为组织（**ORG**）。这可能是因为前总统
    Trump 在作为商人时，经常将他的名字作为他创立的几个组织的名称的一部分。
- en: In the bottom example, Obama (also a former president and a frequent topic in
    disputed political debates) is recognized correctly as **PERSON**. In both cases,
    we are also showing the list of extracted entities, complemented with the starting
    and ending positions of each identified entity.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在底部示例中，奥巴马（也是一位前总统，在争议性政治辩论中经常成为话题）被正确识别为**人物**。在两种情况下，我们还展示了提取的实体列表，包括每个识别实体的起始和结束位置。
- en: POS tagging
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词性标注
- en: With NER analysis, we identified names specific to various entities like people,
    organizations, places, and so on. These help us to associate various terms with
    a certain semantic group. We can go further and explore the comments text so that
    we understand what POS (like noun or verb) each word is, and understand the syntax
    of each phrase.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 通过NER分析，我们识别了各种实体（如人、组织、地点等）的特定名称。这些帮助我们将各种术语与特定的语义组关联起来。我们可以进一步探索评论文本，以便了解每个单词的POS（如名词或动词），并理解每个短语的句法。
- en: 'Let’s start with using `nltk` (an alternative `nlp` library) to extract parts
    of speech from the same small selection of phrases we used for NER experiments.
    We chose `nltk` here because, as well as being even less resource-hungry than
    spacy, it provides good-quality results. We also want to be able to compare the
    results of both (`spacy` and `nltk`):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用`nltk`（一个替代的`nlp`库）从我们在NER实验中使用的相同短语小集中提取词性。我们在这里选择`nltk`是因为，除了比spacy更节省资源外，它还提供了高质量的结果。我们还想能够比较两者的结果（`spacy`和`nltk`）：
- en: '[PRE12]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The results will be as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '![](img/B20963_07_14.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_14.png)'
- en: 'Figure 7.14: POS tagging using nltk'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：使用nltk进行POS标记
- en: 'We can perform the same analysis using spacy as well:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用spacy执行相同的分析：
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The results will be as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将如下所示：
- en: '![](img/B20963_07_15.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_15.png)'
- en: 'Figure 7.15: POS tagging using spacy'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：使用spacy进行POS标记
- en: Let’s compare the two outputs in *Figures 7.14* and *7.15*. The two libraries
    generate slightly different POS results. Some of the differences are due to the
    different mapping of actual parts of speech on categories. For `nltk`, the word
    “is” represents an `AUX` (auxiliary), while the same “is” for `spacy` is a verb.
    `spacy` distinguishes between proper nouns (names of persons, places, and so on)
    and regular nouns (`NOUN`), whereas `nltk` does not differentiate.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较一下*图7.14*和*图7.15*中的两个输出。这两个库生成的POS结果略有不同。其中一些差异是由于实际词性在类别上的映射不同。对于`nltk`，单词“is”代表一个`AUX`（辅助），而`spacy`中的相同“is”则是一个动词。`spacy`区分专有名词（人名、地名等）和普通名词（`NOUN`），而`nltk`则不进行区分。
- en: With some phrases having a non-standard structure, both outputs wrongly identify
    the verb “Go” as a noun (`nltk`) and a proper noun (`spacy`). In the case of `spacy`,
    it is somewhat expected, since “Go” is written in uppercase after a comma. spacy
    differentiates between a coordinating conjunction (**CCONJ**) and a subordinating
    conjunction (**SCONJ**), while `nltk` will only recognize that there are conjunctions
    (**CONJ**).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一些具有非标准结构的短语，两个输出都将动词“Go”错误地识别为名词（`nltk`）和专有名词（`spacy`）。在`spacy`的情况下，这是可以预料的，因为“Go”在逗号后面被大写。spacy区分了并列连词（**CCONJ**）和从属连词（**SCONJ**），而`nltk`只会识别出存在连词（**CONJ**）。
- en: With the same library extension for spacy that we used to highlight NER in the
    previous subsection, we can also represent the syntactic structure of phrases
    and paragraphs. In *Figure 7.16*, we show one example of such a representation.
    In the notebook, we show all the comment (set of phrases) visualizations using
    `displacy` with the “dep” (dependency) flag.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用与我们在前一小节中用于突出NER的相同spacy库扩展，我们也可以表示短语和段落的句法结构。在*图7.16*中，我们展示了一个这样的表示示例。在笔记本中，我们使用带有“dep”（依存）标志的`displacy`展示了所有注释（短语集）的可视化。
- en: '![A picture containing diagram, sketch  Description automatically generated](img/B20963_07_16.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表、草图  自动生成的描述](img/B20963_07_16.png)'
- en: 'Figure 7.16: POS tagging using spacy and dependency to show phrase structure
    with the dependency between the parts of speech'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16：使用spacy进行POS标记，并使用依存关系显示词性之间的短语结构
- en: We saw how we can use dependency to show entities and the category for each
    of them using dependency, and also how we can use the same function to show both
    the parts of speech and the phrase structure. With inspiration from *Reference
    11*, we extended the code sample given there (and transitioned from using `nltk`
    to `spacy` for POS extraction, given that nltk is not fully aligned with spacy)
    so that we can show the parts of speech highlighted in the same way as we represented
    the named entities.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何使用依存关系来显示实体及其各自的类别，以及如何使用相同的函数来显示词性和短语结构。从*参考文献11*中汲取灵感，我们扩展了那里给出的代码示例（并从使用`nltk`转换为使用`spacy`进行POS提取，因为nltk与spacy不完全兼容）以便我们可以以与表示命名实体相同的方式显示词性。
- en: 'The modified code (including some minor bug fixing, besides the changes mentioned
    already) from *Reference 11* is given here (code explanation follows after the
    code block):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这里给出了从*参考文献11*修改后的代码（代码解释将在代码块之后进行）：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Let’s understand the preceding code better. In the function `visualise_pos`,
    we first define a mapping between parts of speech and colors (how a part of speech
    will be highlighted). Then, we define the parts of speech that we will consider.
    Then, we correct a bug existing in the original code (from *Reference 11*) using
    some replacements for special characters. We also use a spacy tokenizer and add,
    in the `tags` list, the text and part of speech for each `pos` extracted using
    `nlp` from `spacy`. Then, we calculate the position of each `pos` identified and
    create a dictionary with the `pos` tokens and their position in the text, to be
    able to highlight them with different colors. At the end, we render the document
    with all `pos` highlighted using `displacy`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更好地理解前面的代码。在`visualise_pos`函数中，我们首先定义了词性与颜色之间的映射（词性如何被突出显示）。然后，我们定义了我们将考虑的词性。接着，我们使用一些特殊字符的替换来纠正原始代码（来自*参考文献11*）中存在的一个错误。我们还使用spacy分词器，并在`tags`列表中添加了使用`nlp`从`spacy`提取的每个`pos`的文本和词性。然后，我们计算每个`pos`的位置，并创建一个字典，包含`pos`标记和它们在文本中的位置，以便能够用不同的颜色突出显示它们。最后，我们使用`displacy`渲染所有突出显示的`pos`的文档。
- en: In *Figure 7.17*, we show the result of applying this procedure to our sample
    of comments. We can now see some of the errors of spacy easier. In the second
    comment, it misinterprets the second “Go” as a proper noun (**PROPN**). This is
    somewhat explainable, since normally, after a comma, only proper nouns will be
    written in uppercase in English.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.17*中，我们展示了将此程序应用于我们的样本评论的结果。现在我们可以更容易地看到spacy的一些错误。在第二条评论中，它错误地将第二个“Go”解释为专有名词（**PROPN**）。这在某种程度上是可以解释的，因为通常在逗号之后，只有专有名词才会用大写字母书写。
- en: '![](img/B20963_07_17.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_17.png)'
- en: 'Figure 7.17: POS tagging using spacy and dependency with the procedure modified
    from Reference 8 to show POSs highlighted inline in text'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17：使用spacy进行词性标注和依赖关系，并使用从参考文献8修改的程序来在文本中突出显示词性
- en: We can observe other errors as well. In the first comment, “Trump” appears as
    **NOUN** – that is, a simple noun. The term “republicans” is categorized as **PROPN**,
    which is likely accurate in the context of U.S. politics where “Republicans” is
    treated as a proper noun. However, in our context, this is inaccurate, as it represents
    a simple noun in plural form, identifying a group of individuals advocating for
    a republican government.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以观察到其他错误。在第一条评论中，“Trump”被标记为**名词**——即一个简单的名词。术语“republicans”被分类为**PROPN**，这在美国政治背景下可能是准确的，因为在美国政治中，“Republicans”被视为专有名词。然而，在我们的上下文中，这是不准确的，因为它代表了一个复数形式的简单名词，指代一群倡导共和政府的人。
- en: We reviewed several NLP techniques that helped us to get a better understanding
    of the word distribution, topics, POS, and concepts present in the text. Optionally,
    we can also use these techniques to generate features to include in a machine
    learning model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们回顾了几种自然语言处理（NLP）技术，这些技术帮助我们更好地理解文本中存在的词语分布、主题、词性（POS）和概念。可选地，我们也可以使用这些技术来生成特征，以便包含在机器学习模型中。
- en: In the next section, we will start the analysis targeted at preparing a supervised
    NLP model for the classification of comments.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将开始分析，目的是为评论分类准备一个监督式NLP模型。
- en: Preparing the model
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备模型
- en: The model preparation, depending on the method we will implement, might be more
    or less complex. In our case, we opt to start the first baseline model with a
    simple deep learning architecture (which was the standard approach at the time
    of the competition), including a word embeddings layer (using pretrained word
    embeddings) and one or more bidirectional LSTM layers. This architecture was a
    common choice at the time when this competition took place, and it is still a
    good option for a baseline for a text classification problem. **LSTM** stands
    for **Long Short-Term Memory**. It is a type of recurrent neural network architecture
    designed to capture and remember long-term dependencies in sequential data. It
    is particularly effective for text classification problems due to its ability
    to handle and model intricate relationships and dependencies in sequences of text.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 模型准备，根据我们将要实施的方法，可能更复杂或更简单。在我们的情况下，我们选择从简单的深度学习架构开始构建第一个基线模型（这是比赛时的标准方法），包括一个词嵌入层（使用预训练的词嵌入）和一个或多个双向长短期记忆（LSTM）层。这种架构在这次比赛发生时是一个常见的选择，对于文本分类问题的基线来说仍然是一个好的选择。**LSTM**代表**长短期记忆**。它是一种循环神经网络架构，旨在捕捉和记住序列数据中的长期依赖关系。它特别适用于文本分类问题，因为它能够处理和模拟文本序列中的复杂关系和依赖关系。
- en: For this, we will need to perform some comment data preprocessing (we also performed
    preprocessing when preparing to build the topic modeling model). This time, we
    will perform the preprocessing steps gradually, and monitor how these steps are
    affecting the result not of the model, but of one prerequisite of a well-performing
    language model, which is the vocabulary coverage of the word embeddings.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要对评论数据进行一些预处理（在准备构建主题建模模型时我们也进行了预处理）。这次，我们将逐步执行预处理步骤，并监控这些步骤如何影响结果，不是模型的结果，而是表现良好的语言模型的一个先决条件，即词嵌入的词汇覆盖范围。
- en: We will then use word embeddings in the first baseline model to extend the generalization
    power of our model so that the words that are not present in the train set but
    are in the test set would benefit from the vicinity of words that exist in the
    word embeddings. Finally, to ensure that our approach will be effective, we will
    need the pretrained word embeddings to have as large a vocabulary coverage as
    possible. Thus, we will also measure the vocabulary coverage and suggest methods
    for improving it.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用第一个基线模型中的词嵌入来扩展我们模型的泛化能力，以便不在训练集中但在测试集中的单词能够从存在于词嵌入中的单词的邻近性中受益。最后，为了确保我们的方法有效，我们需要预训练的词嵌入具有尽可能大的词汇覆盖范围。因此，我们还将测量词汇覆盖范围并建议改进它的方法。
- en: For now, we start by building the initial vocabulary.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们首先构建初始词汇表。
- en: Building the vocabulary
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建词汇表
- en: We performed the earlier experiments with word frequency, word distribution
    associated with various values of the target and other features, topic modeling,
    NER, and POS tagging with a subset of the entire comments corpus. For the following
    experiment, we will start using the entire dataset. We will use word embeddings
    with an embedding size of 300.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前使用单词频率、与目标值相关的单词分布以及其他特征、主题建模、命名实体识别（NER）和词性标注（POS tagging）对整个评论语料库的一个子集进行了实验。在接下来的实验中，我们将开始使用整个数据集。我们将使用具有300个嵌入大小的词嵌入。
- en: Word embeddings are numerical representations of a word. They map words to vectors.
    The embedding size refers to the number of components (or dimensions) of these
    vectors. This procedure enables computers to understand and compare relationships
    between words. Because all the words are first transformed using word embeddings
    (and in the word embeddings space, the relationship between words is represented
    by relationships between vectors), words with similar meanings will be represented
    by vectors aligned in the word embeddings space.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 词嵌入是单词的数值表示。它们将单词映射到向量。嵌入大小指的是这些向量的组成部分（或维度）的数量。这个过程使计算机能够理解和比较单词之间的关系。因为所有单词首先都使用词嵌入进行转换（在词嵌入空间中，单词之间的关系由向量之间的关系表示），所以具有相似意义的单词将在词嵌入空间中由对齐的向量表示。
- en: At testing time, new words, not present in the training data, will be represented
    in the word embeddings space as well, and their relationship with other words,
    present in the training data, will be exploited by the algorithm. The effect will
    be to enhance the algorithm we are using for text classification.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，新词，不在训练数据中出现的词，也将被表示在词嵌入空间中，并且算法将利用它们与训练数据中存在的其他词的关系。这将增强我们用于文本分类的算法。
- en: Additionally, we will set the number of characters (or length of comments) to
    a fixed number; we chose this dimension to be 220\. For shorter comments, we will
    pad the comment sequence (that is, add spaces), and for larger comment sequences,
    we will truncate them (to 220 characters). This procedure will ensure we will
    have inputs for the machine learning model with the same dimension. Let’s first
    define a function for building the vocabulary. For building these functions, we
    used sources from *References 12* and *13*.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将字符数（或注释长度）设置为固定值；我们选择了220这个维度。对于较短的注释，我们将填充注释序列（即添加空格），而对于较长的注释序列，我们将截断它们（到220个字符）。这个程序将确保我们会有相同维度的机器学习模型的输入。让我们首先定义一个用于构建词汇表的函数。在构建这些函数时，我们使用了来自*参考文献12*和*13*的来源。
- en: 'The following code is used to build the vocabulary (that is, the corpus of
    words present in the comments). We apply a split on each comment and gather all
    data in a list of sentences. We then parse all these sentences to create a dictionary
    with the vocabulary. Each time a word parsed is found as a key in the dictionary,
    we increment the value associated with the key. What we obtain is a vocabulary
    dictionary with the count (or overall frequency) of each word in the vocabulary:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码用于构建词汇表（即注释中存在的单词的语料库）。我们对每个注释进行分割，并将所有数据收集到一个句子列表中。然后我们解析所有这些句子以创建一个包含词汇的字典。每次解析到的单词如果在字典中作为键存在，我们就增加与该键关联的值。我们得到的是一个包含词汇表中每个单词计数（或总体频率）的词汇表字典：
- en: '[PRE15]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'We create the overall vocabulary by concatenating `train` and `test`:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过连接`train`和`test`创建整体词汇表：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can check the first 10 elements in the vocabulary to have an intuition of
    what this vocabulary looks like:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查词汇表中的前10个元素，以了解这个词汇表的样子：
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The following image shows the result of running the preceding code. It shows
    the most frequent words in the text. As expected, the most used words are some
    of the most frequently used words in the English language.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了运行前面代码的结果。它显示了文本中最常用的单词。正如预期的那样，最常用的单词是英语中最常用的一些单词。
- en: '![](img/B20963_07_18.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_07_18.png)'
- en: 'Figure 7.18: Vocabulary without any preprocessing – uppercase and lowercase
    words, and possibly wrongly spelled expressions'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18：未经任何预处理的词汇表 – 大写和小写单词，以及可能拼写错误的表达
- en: We will use the earlier introduced function, `build_vocabulary`, repeatedly
    every time we perform an additional (sometimes repeated) text transformation.
    We perform successive text transformations to ensure that, while using pretrained
    word embeddings, we have good coverage with the words in the pretrained word embeddings
    of the vocabulary in the comments. With a larger coverage, we ensure a better
    accuracy of the model that we are building. Let’s continue by loading some pretrained
    word embeddings.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在每次执行额外的（有时重复的）文本转换时重复使用之前引入的`build_vocabulary`函数。我们执行连续的文本转换，以确保在使用预训练的词嵌入时，我们对注释中词汇表的预训练词嵌入中的单词有良好的覆盖。更大的覆盖范围将确保我们构建的模型有更高的准确性。让我们继续加载一些预训练的词嵌入。
- en: Embedding index and embedding matrix
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 嵌入索引和嵌入矩阵
- en: We will now build a dictionary with the words in word embeddings as keys and
    the arrays of their embedding representations as values. We call this dictionary
    the embedding index. We will then also build the embedding matrix, which is a
    matrix representation of embeddings. We will use GloVe’s pretrained embeddings
    (with 300 dimensions) for our experiments. **GloVe** stands for **Global Vectors
    for Word Representation** and is an unsupervised algorithm that produces word
    embeddings. It works by analyzing global text statistics over a very large text
    corpus to create vector representations and capture semantic relationships between
    words.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将构建一个字典，以词嵌入中的单词作为键，以它们的嵌入表示的数组作为值。我们称这个字典为嵌入索引。然后我们也将构建嵌入矩阵，它是嵌入的矩阵表示。我们将使用GloVe的预训练嵌入（300维）进行我们的实验。**GloVe**代表**全局词表示向量**，是一种无监督算法，它产生词嵌入。它通过分析非常大的文本语料库中的全局文本统计来创建向量表示，并捕捉单词之间的语义关系。
- en: 'The following code loads the pretrained word embeddings:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码加载预训练的词嵌入：
- en: '[PRE18]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The size of the embedding structure obtained is 2.19 million items. Next, we
    create the embedding matrix using the word index and the embedding index we just
    created:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 获得的嵌入结构大小为2.19百万项。接下来，我们将使用我们刚刚创建的词索引和嵌入索引来创建嵌入矩阵：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: We use the parameter `MAX_FEATURES` to limit the dimension of the embedding
    matrix.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`MAX_FEATURES`参数来限制嵌入矩阵的维度。
- en: Checking vocabulary coverage
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查词汇覆盖率
- en: We introduced the functions to read the word embeddings and compute the embedding
    matrix. Now we will continue with introducing the functions to evaluate the vocabulary
    coverage with words from the word embeddings. The larger the vocabulary coverage,
    the better the accuracy of the model we are building.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了读取词嵌入和计算嵌入矩阵的功能。现在我们将继续介绍用于评估词嵌入中词汇覆盖率的函数。词汇覆盖率越大，我们构建的模型精度越好。
- en: 'To check the coverage of the vocabulary by the embeddings, we are going to
    use the following function:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查嵌入对词汇的覆盖率，我们将使用以下函数：
- en: '[PRE20]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code browses through all the vocabulary items (that is, the words
    present in the comments text) and counts the unknown words (that is, words in
    the text but not in the list of embeddings words). Then, it calculates the percentage
    of words in the vocabulary that exist in the word embeddings index. This percentage
    is calculated in two ways: with each word in the vocabulary unweighted and with
    words weighted by their frequency in the text.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码遍历所有词汇项（即评论文本中存在的单词）并计算未知单词（即文本中但不在嵌入单词列表中的单词）。然后，它计算词汇中存在于词嵌入索引中的单词百分比。这个百分比以两种方式计算：对词汇中的每个单词进行无权重计算，以及根据文本中的单词频率进行加权计算。
- en: We will apply this function repeatedly to check the vocabulary coverage after
    each step of preprocessing. Let’s start with checking the vocabulary coverage
    for the initial vocabulary, where we haven’t applied any preprocessing to the
    comments text yet.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将反复应用此函数，在每个预处理步骤之后检查词汇覆盖率。让我们从检查初始词汇的词汇覆盖率开始，此时我们还没有对评论文本进行任何预处理。
- en: Iteratively improving vocabulary coverage
  id: totrans-189
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐步提高词汇覆盖率
- en: 'We apply the function `check_coverage` to check the vocabulary coverage, passing
    the two parameters: vocabulary and embedding matrix. In the following notation,
    **oov** stands for **out of vocabulary**:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应用`check_coverage`函数来检查词汇覆盖率，传递两个参数：词汇和嵌入矩阵。在以下符号中，**oov**代表**词汇外**：
- en: '[PRE21]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The result of the first iteration is not great. Although we have almost 90%
    of the text covered, only 15.5% of the words in the vocabulary are covered by
    the word embeddings:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次迭代的成果并不理想。尽管我们覆盖了几乎90%的文本，但词汇表中只有15.5%的单词被词嵌入覆盖：
- en: '![](img/B20963_07_19.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B20963_07_19.png)'
- en: 'Figure 7.19: Vocabulary coverage – first iteration'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19：词汇覆盖率 – 第一次迭代
- en: 'We can also look at the list of not covered terms. Because, in `oov_glove`,
    we stored the not covered terms in descending order by the number of appearances
    in the corpus, we can see, by selecting the first terms in this list, the most
    important words not included in the word embeddings. In *Figure 7.20*, we show
    the first 10 terms in this list – the top 10 words not covered. Here, *not covered*
    refers to words that appear in the vocabulary (are present in the comments texts)
    but not in the word embeddings index (are not present in the pretrained word embeddings):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看未覆盖术语的列表。因为在`oov_glove`中，我们按在语料库中出现的次数降序存储了未覆盖的术语，因此，通过选择此列表中的前几个术语，我们可以看到未包含在词嵌入中的最重要的单词。在*图7.20*中，我们显示了此列表中的前10个术语——前10个未覆盖的单词。在这里，“未覆盖”指的是出现在词汇表中的单词（存在于评论文本中），但不在词嵌入索引中（不在预训练的词嵌入中）：
- en: '![](img/B20963_07_20.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B20963_07_20.png)'
- en: 'Figure 7.20: Most frequent 10 words from the vocabulary not covered by the
    word embeddings in the first iteration'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20：第一轮迭代中，词汇表中未覆盖的词汇中最常见的10个单词
- en: By quickly inspecting the list in *Figure 7.20*, we see that some of the frequent
    words are either contracted, or colloquial, non-standard forms of spoken English.
    It is normal to see such forms in online comments. We will perform several steps
    of preprocessing to try to improve the vocabulary coverage by correcting the issues
    we find. After each step, we will also measure the vocabulary coverage again.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 通过快速检查*图7.20*中的列表，我们看到一些常用词要么是缩写词，要么是非标准口语英语的口语化形式。在网上评论中看到这样的形式是正常的。我们将执行几个预处理步骤，试图通过纠正我们发现的问题来提高词汇覆盖率。在每一步之后，我们还将再次测量词汇覆盖率。
- en: Transforming to lowercase
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转换为小写
- en: 'We will start by converting all text to lowercase and adding it to the vocabulary.
    In word embeddings, the words will be all lowercase:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将所有文本转换为小写，并将其添加到词汇表中。在词嵌入中，单词将全部为小写：
- en: '[PRE22]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We apply this lowercase transformation to both the train and test sets and
    then we rebuild the vocabulary and calculate the vocabulary coverage:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将此小写转换应用于训练集和测试集，然后我们重新构建词汇表并计算词汇覆盖率：
- en: '[PRE23]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '*Figure 7.21* shows the new vocabulary coverage after we applied the lowercase
    transformation:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.21*显示了应用小写转换后新的词汇覆盖率：'
- en: '![A number with black text  Description automatically generated with medium
    confidence](img/B20963_07_21.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![黑色文字的数字  描述自动生成，置信度中等](img/B20963_07_21.png)'
- en: 'Figure 7.21: Vocabulary coverage – second iteration with lowercase of all words'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21：词汇覆盖率——第二次迭代，所有单词的小写
- en: We can observe a few small improvements in the word percentage and text percentage
    coverage. Let’s continue by removing contractions in the comments text.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在单词百分比和文本百分比覆盖率上观察到一些小的改进。让我们继续通过从注释文本中移除缩写词来继续操作。
- en: Removing contractions
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除缩写词
- en: 'Next, we will remove contractions. These are modified forms of words and expressions.
    We will use a predefined dictionary of usually encountered contractions. These
    will be mapped on words that exist in embeddings. Because of limited space, we
    are just including here a few examples of items in the contractions dictionary,
    but the entire resource is available in the notebook associated with this chapter:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将移除缩写词。这些是单词和表达式的修改形式。我们将使用一个预定义的通常遇到的缩写词字典。这些将映射到存在于嵌入中的单词上。由于空间有限，我们在这里只包括缩写词字典中的一些项目示例，但整个资源可以在与本章相关的笔记本中找到：
- en: '[PRE24]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'With the following function, we can get the list of known contractions in GloVe
    embeddings:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下函数，我们可以获取GloVe嵌入中已知缩写词的列表：
- en: '[PRE25]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We can use the next function to clean the known contractions from the vocabulary
    – that is, replace them by using the contractions dictionary:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用下一个函数从词汇表中清除已知的缩写词——即，使用缩写词字典来替换它们：
- en: '[PRE26]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'After we apply `clean_contractions` to both the train and test sets and again
    apply the function to build the vocabulary and measure vocabulary coverage, we
    get the new stats about the vocabulary coverage:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对训练集和测试集应用`clean_contractions`并再次应用构建词汇表和测量词汇覆盖率的函数后，我们得到了关于词汇覆盖率的新统计数据：
- en: '![A number with numbers on it  Description automatically generated with medium
    confidence](img/B20963_07_22.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![带有数字的数字  描述自动生成，置信度中等](img/B20963_07_22.png)'
- en: 'Figure 7.22: Vocabulary coverage – third iteration, after replacing contractions
    using the contractions dictionary'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22：词汇覆盖率——第三轮迭代，使用缩写词字典替换缩写词后的结果
- en: Further refinement of the contractions dictionary is possible by inspecting
    expressions without coverage and enhancing it to equate not covered expressions
    in the corpus with words or groups of words where each word is represented in
    the embedding vector.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 通过检查没有覆盖的表达式并增强它，使其等于语料库中未覆盖的表达式与单词或单词组相等，可以进一步细化缩写字典。每个单词都表示在嵌入向量中。
- en: Removing punctuation and special characters
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除标点和特殊字符
- en: 'Next, we will remove punctuation and special characters. The following lists
    and functions are useful for this step. First, we list the unknown punctuation:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将移除标点和特殊字符。以下列表和函数对此步骤很有用。首先，我们列出未知标点：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Then we clean the special characters and punctuation:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们清理特殊字符和标点：
- en: '[PRE28]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Let’s check the vocabulary coverage again in *Figure 7.23*. This time, we increased
    the word vocabulary coverage by word embeddings from around 15% to 54%. Additionally,
    text coverage increased from 90% to 99.7%.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在*图7.23*中再次检查词汇覆盖率。这次，通过词嵌入，我们将词汇覆盖率从大约15%提高到54%。此外，文本覆盖率从90%提高到99.7%。
- en: '![A close-up of numbers  Description automatically generated](img/B20963_07_23.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![数字的特写  自动生成的描述](img/B20963_07_23.png)'
- en: 'Figure 7.23: Vocabulary coverage – fourth iteration, after cleaning punctuation
    and special characters'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23：词汇覆盖率 – 第四次迭代，在清理标点和特殊字符之后
- en: 'Looking at the top 20 words not covered, we see that we have small words with
    accents, special characters, and idiomatic expressions. We extend the punctuation
    dictionary to include the most frequent special characters, and after we run `build_vocabulary`
    and `check_coverage` again, we get a new status of the vocabulary coverage:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 查看未覆盖的前20个单词，我们发现有一些带重音的小词、特殊字符和习语。我们将标点字典扩展到包括最频繁的特殊字符，并在我们再次运行`build_vocabulary`和`check_coverage`之后，我们得到了词汇覆盖率的新状态：
- en: '[PRE29]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: There is a trivial improvement this time, but we can continue with addressing
    either frequent expressions or frequent special character replacements until we
    get a significant improvement.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这次有一个微小的改进，但我们可以继续处理频繁的表达式或频繁的特殊字符替换，直到我们得到显著的改进。
- en: An alternative way to further improve the comments corpus vocabulary coverage
    by the embeddings is to add an additional embedding source to current pretrained
    embeddings. Let’s try this. We used pretrained embeddings from `GloVe`. We can
    also use `FastText` from Facebook. `FastText` is a very practical industry-standard
    library that is commonly used in search and recommendation engines in several
    companies daily. Let us load the embeddings and recreate the embeddings index
    with the combined embeddings vectors.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加额外的嵌入源到当前的预训练嵌入中，可以进一步改善注释语料库的词汇覆盖率。让我们试试这个。我们使用了来自`GloVe`的预训练嵌入。我们也可以使用Facebook的`FastText`。`FastText`是一个非常实用的行业标准库，在许多公司的日常搜索和推荐引擎中广泛使用。让我们加载嵌入并使用组合嵌入向量重新创建嵌入索引。
- en: After we merge both word embedding dictionaries, with dimensions of 2.19 million
    and 2.0 million entries (both with a vector dimension of 300), we obtain a dictionary
    with a dimension of 2.8 million entries (due to many common words in the two dictionaries).
    We then recalculate the vocabulary coverage. In *Figure 7.24*, we show the result
    of this operation.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将两个词嵌入字典合并后，一个包含2.19百万个条目，另一个包含2.0百万个条目（两者都具有300维的向量维度），我们得到了一个包含2.8百万个条目的字典（由于两个字典中有很多共同词汇）。然后我们重新计算了词汇覆盖率。在*图7.24*中，我们展示了这一操作的结果。
- en: '![A close-up of numbers  Description automatically generated](img/B20963_07_24.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![数字的特写  自动生成的描述](img/B20963_07_24.png)'
- en: 'Figure 7.24: Vocabulary coverage – fifth iteration, after adding the FastText
    pretrained word embeddings to the initial GloVe embedding dictionary'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24：词汇覆盖率 – 第五次迭代，在将FastText预训练词嵌入添加到初始GloVe嵌入字典之后
- en: To summarize our process here, our intention was to build a baseline solution
    based on the use of pretrained word embeddings. We introduced two pretrained word
    embedding algorithms, `GloVe` and `FastText`. Pretrained means that we used the
    already trained algorithms; we didn’t calculate the word embeddings from the corpus
    of comments in our dataset. To be effective, we need to ensure that we have good
    coverage with these word embeddings of the comments text vocabulary. Initially,
    the coverage was rather poor (15% of the vocabulary and 86% of the entire text).
    We improved these statistics gradually by transforming to lowercase, removing
    contractions, removing punctuation, and replacing special characters. In the last
    step, we extended the embeddings dictionary by adding pretrained embeddings from
    an alternative source. In the end, we were able to ensure a 56% coverage of the
    vocabulary and 99.75% of the entire text.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下我们的过程，我们的意图是构建一个基于预训练词嵌入的基线解决方案。我们介绍了两种预训练词嵌入算法，`GloVe` 和 `FastText`。预训练意味着我们使用了已经训练好的算法；我们没有从我们的数据集的评论语料库中计算词嵌入。为了有效，我们需要确保我们用这些词嵌入覆盖了评论文本词汇表的良好范围。最初，覆盖率相当低（词汇表的
    15% 和整个文本的 86%）。通过转换为小写、删除缩写、删除标点符号和替换特殊字符，我们逐渐提高了这些统计数据。在最后一步，我们通过添加来自替代源的预训练嵌入来扩展嵌入字典。最终，我们能够确保词汇表的
    56% 覆盖率和整个文本的 99.75% 覆盖率。
- en: The next step is to go ahead and create a baseline model in a separate notebook.
    We will only reuse a part of the functions we created for the experiments in the
    current notebook.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建一个基线模型，并在单独的笔记本中完成。我们只会重用当前笔记本中实验的一部分函数。
- en: Building a baseline model
  id: totrans-236
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基线模型
- en: These days, everybody will build a baseline model by at least fine-tuning a
    Transformer architecture. Since the 2017 paper *Attention Is All You Need* (*Reference
    14*), the performance of these solutions has continuously improved, and for competitions
    like *Jigsaw Unintended Bias in Toxicity Classification*, a recent Transformer-based
    solution will probably take you easily into the gold zone.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这些天，每个人至少都会通过微调 Transformer 架构来构建一个基线模型。自从 2017 年的论文《Attention Is All You Need》（参考文献
    14）以来，这些解决方案的性能一直在持续提升，对于像 *Jigsaw Unintended Bias in Toxicity Classification*
    这样的比赛，一个基于 Transformer 的最新解决方案可能会轻易地将你带入金牌区域。
- en: 'In this exercise, we will start with a more classical baseline. The core of
    this solution is based on contributions from Christof Henkel (Kaggle nickname:
    Dieter), Ane Berasategi (Kaggle nickname: Ane), Andrew Lukyanenko (Kaggle nickname:
    Artgor), Thousandvoices (Kaggle nickname), and Tanrei (Kaggle nickname); see *References*
    *12*, *13*, *15*, *16*, *17*, and *18*.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将从一个更经典的基线开始。这个解决方案的核心是基于 Christof Henkel（Kaggle 昵称：Dieter）、Ane Berasategi（Kaggle
    昵称：Ane）、Andrew Lukyanenko（Kaggle 昵称：Artgor）、Thousandvoices（Kaggle 昵称）和 Tanrei（Kaggle
    昵称）的贡献；参见参考文献 *12*、*13*、*15*、*16*、*17* 和 *18*。
- en: The solution includes four steps. In the first step, we load the train and test
    data as `pandas` datasets and then we perform preprocessing on the two datasets.
    The preprocessing is largely based on the preprocessing steps we performed before,
    and hence, we won’t repeat those steps here.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 该解决方案包括四个步骤。在第一步，我们将训练数据和测试数据加载为 `pandas` 数据集，然后对这两个数据集进行预处理。预处理主要基于我们之前执行的预处理步骤，因此我们在这里不会重复这些步骤。
- en: 'In the second step, we perform tokenization and prepare the data to present
    it to the model. The tokenization is performed as shown in the following code
    excerpt (we are not showing the entire procedure here):'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步，我们执行分词并准备数据以呈现给模型。分词过程如下所示（我们这里不展示整个过程）：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We used a basic tokenizer here from `keras.preprocessing.text`. After tokenization,
    each input sequence is padded with a predefined `MAX_LEN`, which was selected
    as an optimum considering the average/median length of sequences for the entire
    comments corpus and also considering the available memory and runtime constraints.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了一个基本的分词器来自 `keras.preprocessing.text`。分词后，每个输入序列都会用预定义的 `MAX_LEN` 进行填充，这个长度是根据整个评论语料库的平均/中位长度以及可用的内存和运行时限制选定的。
- en: 'In the third step, we build the embedding matrix and the model structure. The
    code for building the embedding matrix is largely based on the procedures we already
    presented in the previous sections. Here, we just systematize it:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步，我们构建嵌入矩阵和模型结构。构建嵌入矩阵的代码主要基于我们在前几节中已经介绍过的过程。在这里，我们只是将其系统化：
- en: '[PRE31]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The model is a deep learning architecture with a word embeddings layer, a `SpatialDropout1D`
    layer, two bidirectional LSTM layers, a concatenation of `GlobalMaxPooling1D`
    with a `GlobalAveragePooling1D`, two dense layers with `'relu'` activation, and
    one dense layer with `'sigmoid'` activation for the target output.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型是一个具有词嵌入层、`SpatialDropout1D`层、两个双向LSTM层、`GlobalMaxPooling1D`与`GlobalAveragePooling1D`的拼接、两个具有`'relu'`激活的密集层，以及一个具有`'sigmoid'`激活的目标输出的密集层的深度学习架构。
- en: 'In the word embedding layer, the input is transformed so that each word is
    represented by its corresponding vector. After this transformation, the information
    about the semantic distance between words in the input is captured by the model.
    The `SpatialDropout1D` layer helps prevent overfitting by randomly deactivating
    neurons during training (the coefficient gives the percentage of neurons deactivated
    each epoch). The bidirectional LSTM layer’s role is to process the input sequences
    in both forward and backward directions, enhancing contextual understanding for
    better predictions. The role of the `GlobalAveragePooling1D` layer is to compute
    the average of each feature across the entire sequence, reducing the dimensionality
    while retaining essential information in the 1D (sequential) data. This amounts
    to revealing a latent representation of the sequences. The dense layers’ output
    is the prediction of the model. See *References 17* and *18* for more details
    regarding the implementation:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在词嵌入层中，输入被转换，使得每个词都由其对应的向量表示。经过这种转换后，模型捕捉到了输入中词语之间的语义距离信息。`SpatialDropout1D`层通过在训练过程中随机停用神经元来帮助防止过拟合（系数给出了每个epoch停用神经元的百分比）。双向LSTM层的作用是处理输入序列的前向和反向，增强上下文理解以获得更好的预测。`GlobalAveragePooling1D`层的作用是计算整个序列中每个特征的均值，在1D（顺序）数据中降低维度同时保留关键信息。这相当于揭示了序列的潜在表示。密集层的输出是模型的预测。有关实现的更多细节，请参阅*参考文献17*和*参考文献18*：
- en: '[PRE32]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In the fourth step, we run the training, prepare the submission, and submit.
    To reduce the memory used during runtime, we are using temporary storage and performing
    garbage collection after deleting non-used allocated data. We run the model twice
    for a specified number of `NUM_EPOCHS` (representing one complete pass of training
    data through the algorithm) and then average the test predictions using variable
    weights. Then we submit the predictions:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在第四步，我们进行训练，准备提交，并提交。为了减少运行时使用的内存，我们使用临时存储，并在删除未使用的分配数据后进行垃圾回收。我们运行模型两次，指定数量的`NUM_EPOCHS`（代表训练数据通过算法的一个完整遍历），然后使用可变权重平均测试预测。然后我们提交预测：
- en: '[PRE33]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With this solution (for the full code, see *Reference 16*), we can obtain, via
    a late submission, a core of 0.9328 and, consequently, a ranking in the upper
    half of the private leaderboard. Next, we will show how, by using a Transformer-based
    solution, we can obtain a higher score, in the upper silver medal or even gold
    medal zone for this competition.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种解决方案（完整代码请见*参考文献16*），我们可以通过延迟提交获得核心得分0.9328，从而在私有排行榜的上半部分获得排名。接下来，我们将展示如何通过使用基于Transformer的解决方案，我们可以获得更高的分数，进入银牌甚至金牌区域。
- en: Transformer-based solution
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于Transformer的解决方案
- en: At the time of the competition, BERT and some other Transformer models were
    already available and a few solutions with high scores were provided. Here, we
    will not attempt to replicate them but we will just point out the most accessible
    implementations.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在竞赛期间，BERT和一些其他Transformer模型已经可用，并提供了几个高分解决方案。在这里，我们不会尝试复制它们，但我们会指出最易于实现的实现。
- en: In *Reference 20*, Qishen Ha combines a few solutions, including BERT-Small
    V2, BERT-Large V2, XLNet, and GPT-2 (fine-tuned models using competition data
    included as datasets) to obtain a 0.94656 private leaderboard score (late submission),
    which would put you in the top 10 (both gold medal and prize area for this competition).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在*参考文献20*中，齐申·哈结合了几种解决方案，包括BERT-Small V2、BERT-Large V2、XLNet和GPT-2（使用竞赛数据作为数据集进行微调的模型）来获得0.94656的私有排行榜得分（延迟提交），这将使你进入前10名（包括金牌和奖项区域）。
- en: A solution with only the BERT-Small model (see *Reference 21*) will yield a
    private leaderboard score of 0.94295\. Using the BERT-Large model (see *Reference
    22*) will result in a private leaderboard score of 0.94388\. Both these solutions
    will be in the silver medal zone (around places 130 and 80, respectively, in the
    private leaderboard, as late submissions).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用BERT-Small模型（参见*参考文献21*）的解决方案将产生0.94295的私有排行榜分数。使用BERT-Large模型（参见*参考文献22*）将导致0.94388的私有排行榜分数。这两个解决方案都将位于银牌区域（在私有排行榜中分别位于130和80左右，作为晚提交）。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to work with text data, using various approaches
    to explore this type of data. We started by analyzing our target and text data,
    preprocessing text data to include it in a machine learning model. We also explored
    various NLP tools and techniques, including topic modeling, NER, and POS tagging,
    and then prepared the text to build a baseline model, passing through an iterative
    process to gradually improve the data quality for the objective set (in this case,
    the objective being to improve the coverage of word embeddings for the vocabulary
    in the corpus of text from the competition dataset).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何处理文本数据，使用各种方法来探索这类数据。我们首先分析了我们的目标和文本数据，对文本数据进行预处理以便将其包含在机器学习模型中。我们还探讨了各种NLP工具和技术，包括主题建模、命名实体识别（NER）和词性标注（POS
    tagging），然后准备文本以构建基线模型，通过迭代过程逐步提高目标集（在这种情况下，目标是提高竞赛数据集中文本语料库中词汇的词嵌入覆盖范围）的数据质量。
- en: We introduced and discussed a baseline model (based on the work of several Kaggle
    contributors). This baseline model architecture includes a word embedding layer
    and bidirectional LSTM layers. Finally, we looked at some of the most advanced
    solutions available, based on Transformer architectures, either as single models
    or combined, to get a late submission with a score in the upper part of the leaderboard
    (silver and gold zones).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们介绍了并讨论了一个基线模型（基于几位Kaggle贡献者的工作）。这个基线模型架构包括一个词嵌入层和双向LSTM层。最后，我们查看了一些基于Transformer架构的最先进解决方案，无论是作为单一模型还是组合使用，以获得排行榜上部的分数（银牌和金牌区域）。
- en: In the next chapter, we will start working with signal data. We will introduce
    data formats specific to various signal modalities (sound, image, video, experimental,
    or sensor data). We will analyze the data from the *LANL Earthquake Prediction*
    Kaggle competition.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始处理信号数据。我们将介绍各种信号模态（声音、图像、视频、实验或传感器数据）特定的数据格式。我们将分析来自*LANL地震预测* Kaggle竞赛的数据。
- en: References
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Jigsaw Unintended Bias in Toxicity Classification, Kaggle competition dataset:
    [https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/
    )'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Jigsaw毒性分类中的无意偏差，Kaggle竞赛数据集：[https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/](https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/)
- en: 'Aja Bogdanoff, Saying goodbye to Civil Comments, Medium: [https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d](mailto:https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d)'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Aja Bogdanoff，告别文明评论，Medium：[https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d](mailto:https://medium.com/@aja_15265/saying-goodbye-to-civil-comments-41859d3a2b1d)
- en: 'Gabriel Preda, Jigsaw Comments Text Exploration: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb)'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gabriel Preda，Jigsaw评论文本探索：[https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-comments-text-exploration.ipynb)
- en: 'Gabriel Preda, Jigsaw Simple Baseline: [https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb)'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Gabriel Preda，Jigsaw简单基线：[https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb](https://github.com/PacktPublishing/Developing-Kaggle-Notebooks/blob/develop/Chapter-07/jigsaw-simple-baseline.ipynb)
- en: 'Susan Li, Topic Modeling and Latent Dirichlet Allocation (LDA) in Python: [https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)'
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Susan Li, Python中的主题建模和潜在狄利克雷分配（LDA）：[https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24](https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24)
- en: 'Aneesha Bakharia, Improving the Interpretation of Topic Models: [https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d](https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d
    )'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Aneesha Bakharia, 提高主题模型的解释能力：[https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d](https://towardsdatascience.com/improving-the-interpretation-of-topic-models-87fd2ee3847d)
- en: 'Carson Sievert, Kenneth Shirley, LDAvis: A method for visualizing and interpreting
    topics: [https://www.aclweb.org/anthology/W14-3110](https://www.aclweb.org/anthology/W14-3110)'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Carson Sievert, Kenneth Shirley, LDAvis：一种可视化和解释主题的方法：[https://www.aclweb.org/anthology/W14-3110](https://www.aclweb.org/anthology/W14-3110)
- en: 'Lucia Dosin, Experiments on Topic Modeling – PyLDAvis: [https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/](https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/)'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Lucia Dosin, 主题建模实验 – PyLDAvis：[https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/](https://www.objectorientedsubject.net/2018/08/experiments-on-topic-modeling-pyldavis/)
- en: 'Renato Aranha, Topic Modelling (LDA) on Elon Tweets: [https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets](https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets)'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Renato Aranha, 在Elon推文上的主题建模（LDA）：[https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets](https://www.kaggle.com/errearanhas/topic-modelling-lda-on-elon-tweets)
- en: 'Latent Dirichlet Allocation, Wikipedia: [https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配，维基百科：[https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation)
- en: 'Leonie Monigatti, Visualizing Part-of-Speech Tags with NLTK and SpaCy: [https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e](https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e)'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Leonie Monigatti, 使用NLTK和SpaCy可视化词性标签：[https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e](https://towardsdatascience.com/visualizing-part-of-speech-tags-with-nltk-and-spacy-42056fcd777e)
- en: 'Ane, Quora preprocessing + model: [https://www.kaggle.com/anebzt/quora-preprocessing-model](https://www.kaggle.com/anebzt/quora-preprocessing-model)'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ane, Quora预处理 + 模型：[https://www.kaggle.com/anebzt/quora-preprocessing-model](https://www.kaggle.com/anebzt/quora-preprocessing-model)
- en: 'Christof Henkel (Dieter), How to: Preprocessing when using embeddings: [https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Christof Henkel (Dieter), 如何：在使用嵌入时进行预处理：[https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings](https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings)
- en: 'Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser, Illia Polosukhin, Attention Is All You Need: [https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan
    N. Gomez, Lukasz Kaiser, Illia Polosukhin, 注意力即是全部：[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)
- en: 'Christof Henkel (Dieter), keras baseline lstm + attention 5-fold: [https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold
    )'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Christof Henkel (Dieter), keras基线lstm + attention 5折：[https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold](https://www.kaggle.com/christofhenkel/keras-baseline-lstm-attention-5-fold)
- en: 'Andrew Lukyanenko, CNN in keras on folds: [https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds](https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds)'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Andrew Lukyanenko, 在keras上使用CNN的折叠：[https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds](https://www.kaggle.com/code/artgor/cnn-in-keras-on-folds)
- en: 'Thousandvoices, Simple LSTM: [https://www.kaggle.com/code/thousandvoices/simple-lstm/s](https://www.kaggle.com/code/thousandvoices/simple-lstm/s)'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Thousandvoices, 简单LSTM：[https://www.kaggle.com/code/thousandvoices/simple-lstm/s](https://www.kaggle.com/code/thousandvoices/simple-lstm/s)
- en: 'Tanrei, Simple LSTM using Identity Parameters Solution: [https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution](https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution)'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tanrei, 使用身份参数的简单LSTM解决方案：[https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution](https://www.kaggle.com/code/tanreinama/simple-lstm-using-identity-parameters-solution)
- en: 'Gabriel Preda, Jigsaw Simple Baseline: [https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline](https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline)'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gabriel Preda, Jigsaw Simple Baseline: [https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline](https://www.kaggle.com/code/gpreda/jigsaw-simple-baseline)'
- en: 'Qishen Ha, Jigsaw_predict: [https://www.kaggle.com/code/haqishen/jigsaw-predict/](https://www.kaggle.com/code/haqishen/jigsaw-predict/)'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Qishen Ha, Jigsaw_predict: [https://www.kaggle.com/code/haqishen/jigsaw-predict/](https://www.kaggle.com/code/haqishen/jigsaw-predict/)'
- en: 'Gabriel Preda, Jigsaw_predict_BERT_small: [https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small](https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small)'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gabriel Preda, Jigsaw_predict_BERT_small: [https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small](https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-small)'
- en: 'Gabriel Preda, Jigsaw_predict_BERT_large: [https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large](https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large)'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Gabriel Preda, Jigsaw_predict_BERT_large: [https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large](https://www.kaggle.com/code/gpreda/jigsaw-predict-bert-large)'
- en: Join our book’s Discord space
  id: totrans-282
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并在以下地点与超过 5000 名成员一起学习：
- en: '[https://packt.link/kaggle](https://packt.link/kaggle)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/kaggle](https://packt.link/kaggle)'
- en: '![](img/QR_Code9220780366773140.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code9220780366773140.png)'
