- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Introducing Existing Federated Learning Frameworks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍现有的联邦学习框架
- en: The objective of this chapter is to introduce existing **federated learning**
    (**FL**) frameworks and platforms, applying each to federated learning scenarios
    involving toy **machine learning** (**ML**) problems. The platforms focused on
    in this chapter are Flower, TensorFlow Federated, OpenFL, IBM FL, and STADLE –
    the idea behind this selection was to help you by covering a breadth of existing
    FL platforms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是介绍现有的**联邦学习**（**FL**）框架和平台，将每个平台应用于涉及玩具**机器学习**（**ML**）问题的联邦学习场景。本章关注的平台是Flower、TensorFlow
    Federated、OpenFL、IBM FL和STADLE——选择这些平台背后的想法是通过涵盖现有的FL平台范围来帮助你。
- en: By the end of this chapter, you should have a basic understanding of how to
    use each platform for FL, and you should be able to choose a platform based on
    its associated strengths and weaknesses for an FL application.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你应该对如何使用每个平台进行联邦学习有一个基本的了解，并且你应该能够根据其相关的优势和劣势选择一个平台用于联邦学习应用。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Introduction to existing FL frameworks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现有FL框架的介绍
- en: Implementations of an example NLP FL task on movie review dataset, using existing
    frameworks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有框架在电影评论数据集上实现示例NLP FL任务
- en: Implementations of example computer vision FL task with non-IID datasets, using
    existing frameworks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有框架实现示例计算机视觉FL任务，使用非-IID数据集
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find the supplemental code files for this chapter in the book’s GitHub
    repository:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的GitHub仓库中找到本章的补充代码文件：
- en: https://github.com/PacktPublishing/Federated-Learning-with-Python
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: https://github.com/PacktPublishing/Federated-Learning-with-Python
- en: Important note
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: You can use the code files for personal or educational purposes. Please note
    that we will not support deployments for commercial use and will not be responsible
    for any errors, issues, or damage caused by using the code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用代码文件进行个人或教育目的。请注意，我们不会支持商业部署，并且不会对使用代码造成的任何错误、问题或损害负责。
- en: Each implementation example in this chapter was run on an x64 machine running
    Ubuntu 20.04.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的每个实现示例都是在运行Ubuntu 20.04的x64机器上运行的。
- en: 'The implementation of the training code for the NLP example requires the following
    libraries to run:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: NLP示例的训练代码实现需要以下库来运行：
- en: Python 3 (version ≥ 3.8)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3 (版本 ≥ 3.8)
- en: NumPy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: TensorFlow (version ≥ 2.9.1)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow（版本 ≥ 2.9.1）
- en: TensorFlow Hub (`pip` `install tensorflow-hub`)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Hub (`pip` `install tensorflow-hub`)
- en: TensorFlow Datasets (`pip` `install tensorflow-datasets`)
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Datasets (`pip` `install tensorflow-datasets`)
- en: TensorFlow Text (`pip` `install tensorflow-text`)
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Text (`pip` `install tensorflow-text`)
- en: Using a GPU with the appropriate TensorFlow installation is recommended to save
    training time for the NLP example, due to the size of the model.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型的大小，建议使用带有适当TensorFlow安装的GPU来节省NLP示例的训练时间。
- en: 'The implementation of the training code for the **non-IID** (**non-independent
    and identical distribution**) computer vision example requires the following libraries
    to run:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练非-IID（非独立同分布）计算机视觉示例的代码实现需要以下库来运行：
- en: Python 3 (version ≥ 3.8)
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 3（版本 ≥ 3.8）
- en: NumPy
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumPy
- en: PyTorch (version ≥ 1.9)
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PyTorch（版本 ≥ 1.9）
- en: Torchvision (version ≥ 0.10.0, tied to PyTorch version)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Torchvision（版本 ≥ 0.10.0，与PyTorch版本相关联）
- en: The installation instructions for each FL framework are listed in the following
    subsections.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 每个FL框架的安装说明列在以下子节中。
- en: TensorFlow Federated
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Federated
- en: 'You can install the following libraries to use TFF:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以安装以下库来使用TFF：
- en: '`tensorflow_federated` (using the `pip install` `tensorflow_federated` command)'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow_federated`（使用`pip install tensorflow_federated`命令）'
- en: '`nest_asyncio` (using the `pip install` `nest_asyncio` command)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nest_asyncio`（使用`pip install nest_asyncio`命令）'
- en: OpenFL
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenFL
- en: You can install OpenFL using `pip` `install openfl`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`pip install openfl`命令安装OpenFL。
- en: 'Alternatively, you can build from source with the following commands:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以使用以下命令从源代码构建：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: IBM FL
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM FL
- en: 'Installing the locally hosted version of IBM FL requires the wheel installation
    file located in the code repository. To perform this installation, run the following
    commands:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 安装IBM FL的本地版本需要位于代码仓库中的wheel安装文件。要执行此安装，请运行以下命令：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Flower
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flower
- en: You can install Flower using the `pip install` `flwr` command.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`pip install flwr`命令安装Flower。
- en: STADLE
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: STADLE
- en: You can install the STADLE client-side library using the `pip install` `stadle-client`
    command.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `pip install stadle-client` 命令安装 STADLE 客户端库。
- en: Introduction to FL frameworks
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 联邦学习框架简介
- en: First, we introduce the FL frameworks and platforms to be used in the subsequent
    implementation-focused sections.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们介绍后续实现重点章节中将要使用的联邦学习框架和平台。
- en: Flower
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Flower
- en: Flower ([https://flower.dev/](https://flower.dev/)) is an open source and ML
    framework-agnostic FL framework that aims to be accessible to users. Flower follows
    a standard client-server architecture, in which the clients are set up to receive
    the model parameters from the server, train on local data, and send the new local
    model parameters back to the server.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Flower ([https://flower.dev/](https://flower.dev/)) 是一个开源且与机器学习框架无关的联邦学习框架，旨在让用户易于使用。Flower
    采用标准的客户端-服务器架构，其中客户端被设置为从服务器接收模型参数，在本地数据上训练，并将新的本地模型参数发送回服务器。
- en: The high-level orchestration of the federated learning process is dictated by
    what Flower calls strategies, used by the server for aspects such as client selection
    and parameter aggregation.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 联邦学习过程的高级编排由 Flower 所称的策略决定，服务器使用这些策略来处理客户端选择和参数聚合等方面。
- en: Flower uses **Remote Procedure Calls** (**RPCs**) in order to perform said orchestration
    through client-side execution from messages sent by the server. The extensibility
    of the framework allows researchers to experiment with novel approaches such as
    new aggregation algorithms and communication methods (such as model compression).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Flower 使用 **远程过程调用** (**RPCs**) 来通过客户端执行从服务器发送的消息以执行所述编排。框架的可扩展性允许研究人员尝试新的方法，例如新的聚合算法和通信方法（如模型压缩）。
- en: TensorFlow Federated (TFF)
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Federated (TFF)
- en: TFF ([https://www.tensorflow.org/federated](https://www.tensorflow.org/federated))
    is an open source FL/computation framework built on top of TensorFlow that aims
    to allow researchers to easily simulate federated learning with existing TensorFlow/Keras
    models and training pipelines. It consists of the Federated Core layer, which
    allows for the implementation of general federated computations, and the Federated
    Learning layer, which is built on top and provides interfaces for FL-specific
    processes.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: TFF ([https://www.tensorflow.org/federated](https://www.tensorflow.org/federated))
    是一个基于 TensorFlow 的开源 FL/计算框架，旨在允许研究人员轻松地使用现有的 TensorFlow/Keras 模型和训练管道模拟联邦学习。它包括联邦核心层，允许实现通用联邦计算，以及联邦学习层，它建立在核心之上，并为
    FL 特定过程提供接口。
- en: TFF focuses on single-machine local simulations of FL, using wrappers to create
    TFF-specific datasets, models, and federated computations (core client and server
    computation performed during the FL process) from the standard TensorFlow equivalents.
    The focus on building everything from general federated computations allows researchers
    to implement each step as desired, allowing experimentation to be supported.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: TFF 专注于 FL 的单机本地模拟，使用包装器从标准的 TensorFlow 等价物创建 TFF 特定的数据集、模型和联邦计算（FL 过程中执行的核心客户端和服务器计算）。从通用联邦计算构建一切的关注使研究人员能够按需实现每个步骤，从而支持实验。
- en: OpenFL
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: OpenFL
- en: OpenFL ([https://github.com/intel/openfl](https://github.com/intel/openfl))
    is an open source FL framework developed by Intel, focused on allowing cross-silo
    privacy-preserving ML to be performed. OpenFL allows for two different workflows
    depending on the desired lifespan of the federation (where federation refers to
    the entire FL system).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: OpenFL ([https://github.com/intel/openfl](https://github.com/intel/openfl))
    是英特尔开发的开源联邦学习框架，专注于允许跨隔离区隐私保护机器学习。OpenFL 允许根据联盟（指整个 FL 系统）的预期生命周期选择两种不同的工作流程。
- en: 'In the aggregator-based workflow, a single experiment and associated federated
    learning plan are sent from the aggregator to the participating *collaborators*
    (agents) to be run as the local training step of the FL process—the federation
    is stopped after the experiment is complete. In the director-based workflow, long-lived
    components are instead used to allow for experiments to be run on demand. The
    following diagram depicts the architecture and users for the director-based workflow:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于聚合器的流程中，单个实验及其相关的联邦学习计划从聚合器发送到参与 *协作者*（代理）以作为 FL 流程的本地训练步骤运行——实验完成后，联盟停止。在基于导演的流程中，使用持久组件而不是短生命周期的组件，以便按需运行实验。以下图表描述了基于导演的流程的架构和用户：
- en: '![Figure 8.1 – Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.1 – 基于总监的工作流程架构（改编自 https://openfl.readthedocs.io/en/latest/source/openfl/components.html）'
- en: '](img/B18369_08_01.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18369_08_01.jpg)'
- en: Figure 8.1 – Architecture of director-based workflow (adapted from https://openfl.readthedocs.io/en/latest/source/openfl/components.html)
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 基于总监的工作流程架构（改编自 https://openfl.readthedocs.io/en/latest/source/openfl/components.html）
- en: '**Director Manager** oversees the running of experiments, working with long-lived
    **Envoy** components residing on the collaborator nodes to manage the short-lived
    components (collaborators + aggregator) for each experiment. In targeting the
    cross-silo data scenario, OpenFL applies a unique focus on managing data shards,
    including cases where data representations differ across silos.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**总监经理**负责实验的运行，与位于协作节点上的长期**信使**组件合作，管理每个实验的短期组件（协作者+聚合者）。在针对跨数据孤岛的场景时，OpenFL对管理数据分片给予了独特的关注，包括数据表示在不同孤岛中不同的情况。'
- en: IBM FL
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: IBM FL
- en: IBM FL is a framework that also focuses on enterprise FL. It follows a straightforward
    aggregator-party design, where some number of parties with local data collaborate
    with other parties by sending incremental model training results to the aggregator
    and working with the produced aggregate models (following standard client-server
    FL architecture). IBM FL has official support for a number of fusion (aggregation)
    algorithms and certain fairness techniques aimed at combatting bias—the details
    of these algorithms can be found at the repository located at https://github.com/IBM/federated-learning-lib.
    One specific goal of IBM FL is to be highly extensible, allowing users to easily
    make necessary modifications if specific features are desired. It also supports
    a Jupyter-Notebook-based dashboard to aid in orchestrating FL experiments.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: IBM FL是一个也专注于企业联邦学习的框架。它遵循简单的聚合者-参与者设计，其中一些拥有本地数据的参与者通过向聚合者发送增量模型训练结果并与生成的聚合模型（遵循标准的客户端-服务器联邦学习架构）合作，与其他参与者协作。IBM
    FL对多种融合（聚合）算法和旨在对抗偏见的某些公平技术提供官方支持——这些算法的详细信息可以在位于https://github.com/IBM/federated-learning-lib的存储库中找到。IBM
    FL的一个具体目标是高度可扩展，使用户能够轻松地进行必要的修改，以满足特定的功能需求。它还支持基于Jupyter-Notebook的仪表板，以帮助协调联邦学习实验。
- en: STADLE
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: STADLE
- en: Unlike the previous frameworks, STADLE ([https://stadle.ai/](https://stadle.ai/))
    is an ML-framework-agnostic FL and distributed learning SaaS platform that aims
    to allow for the seamless integration of FL into production-ready applications
    and ML pipelines. The goal of STADLE is to minimize the amount of FL-specific
    code necessary for integration, making FL accessible to newcomers while still
    providing flexibility to those looking to experiment.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的框架不同，STADLE ([https://stadle.ai/](https://stadle.ai/)) 是一个与机器学习框架无关的联邦学习和分布式学习SaaS平台，旨在允许无缝地将联邦学习集成到生产就绪的应用程序和机器学习管道中。STADLE的目标是最大限度地减少集成所需的特定于联邦学习的代码量，使联邦学习对新手来说易于访问，同时仍然为那些想要进行实验的人提供灵活性。
- en: 'With the STADLE SaaS platform, users of varying technical abilities can collaborate
    on FL projects at all scales. Performance tracking and model management functionalities
    allow users to produce validated federated models with strong performance, while
    an intuitive configuration panel allows for detailed control over the federated
    learning process. STADLE uses a two-level component hierarchy that allows for
    multiple aggregators to operate in parallel, scaling to match demand. The following
    figure depicts the high-level architecture:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 使用STADLE SaaS平台，不同技术能力的用户可以在所有规模上协作进行联邦学习项目。性能跟踪和模型管理功能使用户能够生成具有强大性能的验证联邦模型，而直观的配置面板允许对联邦学习过程进行详细控制。STADLE使用两级组件层次结构，允许多个聚合器并行操作，以匹配需求。以下图展示了高级架构：
- en: '![Figure 8.2 – STADLE multi-aggregator architecture'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.2 – STADLE多聚合器架构'
- en: '](img/B18369_08_02.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18369_08_02.jpg)'
- en: Figure 8.2 – STADLE multi-aggregator architecture
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 – STADLE多聚合器架构
- en: Development of STADLE clients is streamlined with `pip` installation and an
    easy-to-understand configuration file, with several examples made publicly available
    for use as a reference on the different ways STADLE can be integrated into existing
    ML code.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: STADLE客户端的开发通过`pip`安装和易于理解的配置文件简化，同时公开提供了一些示例，供用户参考STADLE如何集成到现有的机器学习代码中的不同方式。
- en: PySyft
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySyft
- en: While PySyft ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft))
    implementations are not included in this chapter due to ongoing changes in the
    codebase, it is still a major player in the privacy-preserving deep learning space.
    The core principle behind PySyft is to allow for the ability to perform computations
    over data stored on a machine without direct access to said data ever being given.
    This is accomplished by adding an intermediate layer between the user and the
    data location that sends computation requests to participating worker machines,
    returning the computed result to the user while maintaining the privacy of the
    data stored and used by each worker to perform the computation.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管由于代码库的持续变化，PySyft ([https://github.com/OpenMined/PySyft](https://github.com/OpenMined/PySyft))
    的实现不包括在本章中，但它仍然是隐私保护深度学习空间中的主要参与者。PySyft背后的核心原则是允许在不对数据进行直接访问的情况下对存储在机器上的数据进行计算。这是通过在用户和数据位置之间添加一个中间层来实现的，该层向参与工作的机器发送计算请求，将计算结果返回给用户，同时保持每个工人存储和使用的用于执行计算的数据的隐私。
- en: This general capability directly extends itself to FL, reworking each step of
    a normal deep learning training flow to be a computation over the model parameters
    and data stored at each worker (agent) participating in FL. To accomplish this,
    PySyft utilizes hooks that encapsulate the standard PyTorch/TensorFlow libraries,
    modifying the requisite internal functions in order to allow model training and
    testing to be supported as PySyft privacy-preserving computations.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用能力直接扩展到FL，重新设计正常深度学习训练流程的每一步，使其成为对每个参与FL的工人（代理）存储的模型参数和数据的计算。为了实现这一点，PySyft利用钩子封装标准的PyTorch/TensorFlow库，修改必要的内部函数，以便支持模型训练和测试作为PySyft隐私保护计算。
- en: Now that the high-level ideas behind the FL frameworks have been explained,
    we move to the implementation-level details for their practical usage in two example
    scenarios. First, we look at how to modify the existing centralized training code
    for an NLP model to use FL.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经解释了FL框架背后的高级思想，我们将转向其实际应用中的实现级细节，以两个示例场景为例。首先，我们来看如何修改现有的用于NLP模型的集中式训练代码，使其能够使用FL。
- en: Example – the federated training of an NLP model
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 - NLP模型的联邦训练
- en: The first ML problem that will be converted into an FL scenario through each
    of the aforementioned FL frameworks will be a classification problem within the
    domain of NLP. At a high level, NLP refers to the intersection of computational
    linguistics and ML with an overarching goal of allowing computers to achieve some
    level of *understanding* from human language – the details of this understanding
    vary widely based on the specific problem being targeted.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过上述每个FL框架将第一个ML问题转换为FL场景的将是NLP领域的分类问题。从高层次来看，NLP是指计算语言学和ML的交集，其总体目标是使计算机能够从人类语言中达到某种程度的*理解*
    - 这种理解的细节根据要解决的具体问题而大相径庭。
- en: For this example, we will be performing sentiment analysis on movie reviews,
    classifying them as positive or negative. The dataset we will be using is the
    SST-2 dataset (https://nlp.stanford.edu/sentiment/), containing movie reviews
    in a string format and the associated binary labels 0/1 representing negative
    and positive sentiment, respectively.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将对电影评论进行情感分析，将它们分类为正面或负面。我们将使用的数据集是SST-2数据集 (https://nlp.stanford.edu/sentiment/)，包含以字符串格式表示的电影评论和相关的二进制标签0/1，分别代表负面和正面情感。
- en: The model we will use to perform binary classification is a pretrained BERT
    model with a custom classification head. The BERT model allows us to encode a
    sentence into a high-dimensional numerical vector, which can then be passed to
    the classification head to output the binary label prediction; more information
    on the BERT model can be found at https://huggingface.co/blog/bert-101\. We choose
    to use a pretrained model that has already learned how to produce general encodings
    for sentences after a significant amount of training, as opposed to performing
    said training from scratch. This allows us to focus training on the classification
    head to fine-tune the model on the SST-2 dataset, saving time while maintaining
    performance.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用进行二元分类的模型是一个带有自定义分类头的预训练BERT模型。BERT模型允许我们将句子编码成一个高维数值向量，然后将其传递到分类头以输出二元标签预测；有关BERT模型的更多信息，请参阅https://huggingface.co/blog/bert-101。我们选择使用一个预训练模型，该模型在大量训练后已经学会了如何生成句子的通用编码，而不是从头开始进行训练。这允许我们将训练集中在分类头上，以微调模型在SST-2数据集上的性能，从而节省时间并保持性能。
- en: We will now go through the local (centralized) training code that will be used
    as a base when showing how to use each of the FL frameworks, starting with the
    Keras model definition and dataset loader.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过本地（集中式）训练代码，该代码将作为展示如何使用每个FL框架的基础，从Keras模型定义和数据集加载器开始。
- en: Defining the sentiment analysis model
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义情感分析模型
- en: The `SSTModel` object defined in the `sst_model.py` file is the Keras model
    we will be using for this example.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在`sst_model.py`文件中定义的`SSTModel`对象是我们将在这个示例中使用的Keras模型。
- en: 'First, we import the requisite libraries:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的库：
- en: '[PRE2]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: TensorFlow Hub is used to easily download the pretrained BERT weights into a
    Keras layer. TensorFlow Text is used when loading in the BERT weights from TensorFlow
    Hub. TensorFlow Datasets will allow us to download and cache the SST-2 dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Hub用于轻松下载预训练的BERT权重到Keras层。当从TensorFlow Hub加载BERT权重时使用TensorFlow
    Text。TensorFlow Datasets将允许我们下载和缓存SST-2数据集。
- en: 'Next, we define the model and initialize the model layer objects:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义模型并初始化模型层对象：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `preprocessor` object takes the raw sentence input batches and converts
    them into the format used by the BERT model. We load the preprocessor and BERT
    layers from TensorFlow Hub, then initialize the dense layers that make up the
    classification head. We use the sigmoid activation function at the end to squash
    the output into the interval (0,1), allowing for comparison with the true labels.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocessor`对象将原始句子输入批次转换为BERT模型使用的格式。我们从TensorFlow Hub加载预处理器和BERT层，然后初始化构成分类头的密集层。我们使用sigmoid激活函数在最后将输出压缩到区间（0,1），以便与真实标签进行比较。'
- en: 'We can then define the forward pass of the model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以定义模型的正向传递：
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We apply leaky ReLU to the BERT output to add non-linearity before passing the
    output to the classification head layers.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将leaky ReLU应用于BERT输出，在传递到分类头层之前添加非线性。
- en: Creating the data loader
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据加载器
- en: 'We also implement a function to load in the SST-2 dataset using the TensorFlow
    Datasets library. First, the training data is loaded and converted into a NumPy
    array for use during training:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了一个函数，使用TensorFlow Datasets库加载SST-2数据集。首先，加载训练数据并将其转换为NumPy数组，以便在训练期间使用：
- en: '[PRE5]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We load the test data in a similar manner:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以类似的方式加载测试数据：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'If `client_idx` and `num_clients` are specified, we return the respective partition
    of the training dataset – this will be used for performing FL:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果指定了`client_idx`和`num_clients`，我们返回训练数据集的相应分区——这将用于执行联邦学习：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Next, we examine the code to perform local training, located in `local_training.py`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们检查位于`local_training.py`中的执行本地训练的代码。
- en: Training the model
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'We first import the requisite libraries:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入必要的库：
- en: '[PRE8]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can then use the previously defined dataset loader (without splitting) to
    load in the train and test splits:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用之前定义的数据集加载器（不进行拆分）来加载训练和测试分割：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can now compile the model and begin training:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以编译模型并开始训练：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we evaluate the model on the test split:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在测试分割上评估模型：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The model should reach around 82% test accuracy after three epochs of training.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 经过三个训练周期后，模型应该达到大约82%的测试准确率。
- en: Now that we have gone through the local training code, we can examine how the
    code can be modified to use FL with each of the aforementioned FL frameworks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经通过了本地训练代码，我们可以检查如何修改代码以使用上述每个FL框架进行联邦学习。
- en: Adopting an FL training approach
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采用FL训练方法
- en: To demonstrate how FL can be applied to the SST model training scenario, we
    have to first split the original SST-2 dataset into disjoint subsets representing
    the local datasets in an FL application. To keep things simple, we will examine
    the case of three agents each training on separate thirds of the dataset.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示如何将FL应用于SST模型训练场景，我们首先需要将原始SST-2数据集拆分成不相交的子集，这些子集代表FL应用中的本地数据集。为了简化问题，我们将研究三个代理各自在数据集的不同三分之一上训练的情况。
- en: For now, these subsets are randomly sampled without replacement from the dataset
    – in the next section, *Federated training of an image classification model on
    non-IID data*, we examine the case where the local datasets are created from a
    biased sampling of the original dataset. Instead of locally training for three
    epochs, we will perform three rounds of FL with each local training phase training
    for one epoch on the local data. FedAvg will be used to aggregate the locally
    trained models at the end of each round. After these three rounds, the aforementioned
    validation metrics will be computed using the final aggregate model, allowing
    for comparisons to be drawn between the local training cases and the FL case.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，这些子集是从数据集中随机采样而不重复的 – 在下一节“在非-IID数据上对图像分类模型进行联邦训练”中，我们将研究本地数据集是从原始数据集的有偏采样中创建的情况。我们不会在本地训练三个epoch，而是将进行三轮FL，每轮本地训练阶段在本地数据上训练一个epoch。FedAvg将在每一轮结束时用于聚合本地训练的模型。在这三轮之后，将使用最终的聚合模型计算上述验证指标，从而允许比较本地训练案例和FL案例。
- en: Integrating TensorFlow Federated for SST-2
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成TensorFlow Federated用于SST-2
- en: As previously mentioned, the **TensorFlow Federated** (**TFF**) framework was
    built on top of the TensorFlow and Keras deep learning libraries. The model implementation
    was done using Keras; as a result, the integration of TFF into the local training
    code is relatively straightforward.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，**TensorFlow Federated**（**TFF**）框架是在TensorFlow和Keras深度学习库之上构建的。模型实现是使用Keras完成的；因此，将TFF集成到本地训练代码中相对简单。
- en: 'The first step is to add the TFF-specific imports and FL-specific parameters
    prior to loading the dataset:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在加载数据集之前添加TFF特定的导入和FL特定的参数：
- en: '[PRE12]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'TFF allows us to simulate some number of agents by passing the appropriate
    number of datasets (local datasets) to the FL process. To split the SST-2 dataset
    into thirds after preprocessing, we can use the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: TFF允许我们通过向FL过程传递适当数量的数据集（本地数据集）来模拟一定数量的代理。为了在预处理后将SST-2数据集分成三份，我们可以使用以下代码：
- en: '[PRE13]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Next, we have to wrap the Keras model using a TFF API function to easily create
    the respective `tff.learning.Model` object. We create a function that initializes
    the SST model and passes it along with the input spec (information on the size
    of each data element) to this API function, returning the result – TFF will use
    this function internally to create the model during the FL process:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须使用TFF API函数包装Keras模型，以便轻松创建相应的`tff.learning.Model`对象。我们创建一个函数，初始化SST模型，并将其与输入规范（关于每个数据元素大小的信息）一起传递给这个API函数，返回结果
    – TFF将在FL过程中内部使用此函数来创建模型：
- en: '[PRE14]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The TFF FedAvg process can then be created, using the `sst_model_fn` function
    along with the optimizers used to update the local models and the aggregate model.
    Using a learning rate of 1.0 for the server optimizer function allows for the
    new aggregate model to replace the old one at the end of each round (as opposed
    to computing a weighted average of the old and new models):'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sst_model_fn`函数以及用于更新本地模型和聚合模型的优化器，可以创建TFF FedAvg过程。对于服务器优化器函数使用1.0的学习率，允许在每一轮结束时用新的聚合模型替换旧的模型（而不是计算旧模型和新模型的加权平均值）：
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we initialize and run the federated learning process for 10 rounds.
    Each `fed_avg_process.next()` call simulates one round by performing local training
    with three models on the client datasets followed by aggregation using FedAvg.
    The resulting state after the first round is passed to the next call as the starting
    FL state for the round:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们初始化并运行联邦学习过程10轮。每次`fed_avg_process.next()`调用通过在客户端数据集上使用三个模型进行本地训练，然后使用FedAvg进行聚合来模拟一轮。第一轮后的状态被传递到下一次调用，作为该轮的起始FL状态：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After the FL process is completed, we convert the final aggregate `tff.learning.Model`
    object back into the original Keras model format in order to compute the validation
    metrics:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: FL过程完成后，我们将最终的聚合 `tff.learning.Model` 对象转换回原始的Keras模型格式，以便计算验证指标：
- en: '[PRE17]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The final accuracy of the aggregate model should be around 82%.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合模型的最终准确率应约为82%。
- en: From this, it should be clear that the TFF FedAvg results are nearly identical
    to those of the local training scenario.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点来看，应该很清楚TFF FedAvg的结果几乎与本地训练场景的结果相同。
- en: Integrating OpenFL for SST-2
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成OpenFL用于SST-2
- en: 'Recall that OpenFL supports two different workflows: the aggregator-based workflow
    and the director-based workflow. This example will use the director-based workflow,
    involving long-living components that can conduct FL task requests as they come
    in. This was chosen due to the desirability of having a persistent FL setup for
    deploying multiple projects; however, both workflows conduct the same core FL
    process and thus demonstrate similar performance.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，OpenFL支持两种不同的工作流程：基于聚合器的工作流程和基于导演的工作流程。本例将使用基于导演的工作流程，涉及长期存在的组件，可以处理传入的FL任务请求。这选择是因为希望有一个持久的FL设置来部署多个项目；然而，两种工作流程都执行相同的核心FL过程，因此表现出类似的表现。
- en: 'To help with model serialization in this case, we only aggregate the classification
    head weights, reconstructing the full model at runtime during training and validation
    (TensorFlow Hub caches the downloaded layers, so the download process only occurs
    once). We include the following functions in `sst_model.py` to aid with this modification:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助在此情况下进行模型序列化，我们只聚合分类头权重，在训练和验证时运行时重建完整模型（TensorFlow Hub 缓存下载的层，因此下载过程只发生一次）。我们在
    `sst_model.py` 中包含以下函数以帮助进行此修改：
- en: '[PRE18]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Because OpenFL focuses on addressing the data silo case, the creation of the
    local datasets from the SST-2 data is slightly more involved than the TFF case.
    The objects needed to create the dataset will be implemented in a separate file
    named `sst_fl_dataset.py`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 由于OpenFL专注于解决数据孤岛问题，从SST-2数据创建本地数据集比TFF情况稍微复杂一些。创建数据集所需的对象将在名为 `sst_fl_dataset.py`
    的单独文件中实现。
- en: 'First, we include the necessary imports. The two OpenFL-specific objects we
    import are the `ShardDescriptor` object, which handles the dataset loading and
    sharding, and the `DataInterface` object, which handles access to the datasets:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们包括必要的导入。我们导入的两个OpenFL特定对象是处理数据集加载和分片的 `ShardDescriptor` 对象，以及处理数据集访问的 `DataInterface`
    对象：
- en: '[PRE19]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Implementing ShardDescriptor
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现ShardDescriptor
- en: 'We first implement the `SSTShardDescriptor` class. When this shard descriptor
    is created, we save the `rank` (client number) and `worldsize` (total number of
    clients) values, then load the training and validation datasets:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实现了 `SSTShardDescriptor` 类。当创建此分片描述符时，我们保存 `rank`（客户端编号）和 `worldsize`（客户端总数）值，然后加载训练和验证数据集：
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We implement the `ShardDescriptor` class functions to get the available dataset
    types (training and validation in this case) and the respective dataset/shard
    based on the rank of the client:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实现了 `ShardDescriptor` 类函数以获取可用的数据集类型（在这种情况下为训练和验证）以及基于客户端排名的相应数据集/分片：
- en: '[PRE21]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We also specify the properties of the specific dataset being used. Note that
    the sample shape is set to `1`. The preprocessor layer of the `SSTModel` allows
    us to pass in strings as input, which are treated as input vectors of type `tf.string`
    and length `1`:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了正在使用的数据集的具体属性。请注意，样本形状设置为 `1`。`SSTModel` 的预处理层允许我们传入字符串作为输入，这些字符串被视为类型为
    `tf.string` 且长度为 `1` 的输入向量：
- en: '[PRE22]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With this, the `SSTShardDescriptor` implementation is completed.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，`SSTShardDescriptor` 的实现就完成了。
- en: Implementing DataInterface
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现数据接口
- en: 'Next, we implement the `SSTFedDataset` class as a subclass of `DataInterface`.
    This is done by implementing the shard descriptor getter and setter methods, with
    the setter method preparing the data to be provided to the training/validation
    FL tasks:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将 `SSTFedDataset` 类实现为 `DataInterface` 的子类。这是通过实现分片描述符获取器和设置器方法来完成的，设置器方法准备要提供给训练/验证FL任务的数据：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We also implement the API functions to grant dataset access and dataset size
    information (used during aggregation):'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了API函数以授予数据集访问和数据集大小信息（用于聚合）：
- en: '[PRE24]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With this, the local SST-2 datasets can be constructed and used.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，就可以构建并使用本地SST-2数据集了。
- en: Creating FLExperiment
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建FLExperiment
- en: 'We now focus on the actual implementation of the FL process within a new file,
    `fl_sim.py`. First, we import the necessary libraries – from OpenFL, we import
    the following:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们专注于在新的文件`fl_sim.py`中实现FL过程的实际实现。首先，我们导入必要的库——从OpenFL中，我们导入以下内容：
- en: '`TaskInterface`: Allows us to define our FL training and validation tasks for
    the model; the registered tasks are what the director instructs each envoy to
    conduct'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TaskInterface`：允许我们为模型定义FL训练和验证任务；注册的任务是director指示每个envoy执行的任务'
- en: '`ModelInterface`: Allows us to convert our Keras model into the format used
    by OpenFL in the registered tasks'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ModelInterface`：允许我们将我们的Keras模型转换为OpenFL在注册任务中使用的格式'
- en: '`Federation`: Manages information relating to the connection with the director'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Federation`：管理与director连接相关的信息'
- en: '`FLExperiment`: Uses the `TaskInterface`, `ModelInterface`, and `Federation`
    objects to conduct the FL process'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FLExperiment`：使用`TaskInterface`、`ModelInterface`和`Federation`对象来执行FL过程'
- en: 'The requisite imports are done as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 必要的导入如下所示：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we create the `Federation` object using the default `director` connection
    information:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用默认的`director`连接信息创建`Federation`对象：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then initialize the model with the associated optimizer and loss function
    – these objects are used by the OpenFL `KerasAdapter` to create the `ModelInterface`
    object. We call the model on a dummy Keras input in order to initialize all of
    the weights before passing the model to `ModelInterface`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用相关的优化器和损失函数初始化模型——这些对象被OpenFL的`KerasAdapter`用于创建`ModelInterface`对象。我们在一个虚拟的Keras输入上调用模型，以便在将模型传递给`ModelInterface`之前初始化所有权重：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, we create a `TaskInterface` object and use it to register the training
    task. Note that including the optimizer in the decorator function of a task will
    result in the training dataset being passed to the task; otherwise, the validation
    dataset will be passed to the task:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个`TaskInterface`对象，并使用它来注册训练任务。请注意，将优化器包含在任务的装饰器函数中会导致训练数据集被传递给任务；否则，验证数据集将被传递给任务：
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Similarly, we register the validation task using the `TaskInterface` object.
    Note that we can collect the metrics generated by the `evaluate` function and
    return the values as a means of tracking performance:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们使用`TaskInterface`对象注册验证任务。请注意，我们可以收集由`evaluate`函数生成的指标，并将值作为跟踪性能的手段：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We can now load in the dataset using the `SSTFedDataset` class implemented
    earlier and create and start a new `FLExperiment` using the created `ModelInterface`,
    `TaskInterface`, and `SSTFedDatasets` objects:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用之前实现的`SSTFedDataset`类加载数据集，并使用创建的`ModelInterface`、`TaskInterface`和`SSTFedDatasets`对象创建并启动一个新的`FLExperiment`：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Defining the configuration files
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义配置文件
- en: 'The last step is to create the configuration files used by `director` and `envoys`
    in order to actually load the data and start the FL process. First, we create
    `director_config` containing the following information:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建由`director`和`envoys`使用的配置文件，以便实际加载数据并启动FL过程。首先，我们创建包含以下信息的`director_config`：
- en: '[PRE31]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This is saved in `director/director_config.yaml`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这被保存在`director/director_config.yaml`中。
- en: 'We then create the three `envoy` configuration files. The first file (`envoy_config_1.yaml`)
    contains the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后创建了三个`envoy`配置文件。第一个文件（`envoy_config_1.yaml`）包含以下内容：
- en: '[PRE32]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The second and third `envoy` config files are the same, except with the values
    `rank_worldsize: 2, 3` and `rank_worldsize: 3, 3`, respectively. These config
    files, alongside all of the code files, are stored in the experiment directory.
    The directory structure should look like the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个和第三个`envoy`配置文件与第一个相同，只是`rank_worldsize`的值分别为`2, 3`和`3, 3`。这些配置文件以及所有代码文件都存储在实验目录中。目录结构应如下所示：
- en: '`director`'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`director`'
- en: '`director_config.yaml`'
  id: totrans-173
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`director_config.yaml`'
- en: '`experiment`'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`experiment`'
- en: '`envoy_config_1.yaml`'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`envoy_config_1.yaml`'
- en: '`envoy_config_2.yaml`'
  id: totrans-176
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`envoy_config_2.yaml`'
- en: '`envoy_config_3.yaml`'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`envoy_config_3.yaml`'
- en: '`sst_fl_dataset.py`'
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sst_fl_dataset.py`'
- en: '`sst_model.py`'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sst_model.py`'
- en: '`fl_sim.py (file with` `FLExperiment creation)`'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fl_sim.py（包含`FLExperiment`创建的文件）`'
- en: With everything set up, we can now perform FL with OpenFL.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪后，我们现在可以使用OpenFL执行FL。
- en: Running the OpenFL example
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行OpenFL示例
- en: 'First, start the director by running the following command from within the
    `director` folder (make sure OpenFL is installed in the working environment):'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从`director`文件夹中运行以下命令以启动director（确保OpenFL已安装在工作环境中）：
- en: '[PRE33]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, run the following commands in separate terminals from the experiment
    directory:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在实验目录中分别在不同的终端运行以下命令：
- en: '[PRE34]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Finally, start `FLExperiment` by running the `fl_sim.py` script. After the three
    rounds are completed, the aggregate model should achieve a validation accuracy
    of around 82%. Once again, the performance is nearly identical to the local training
    scenario.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过运行 `fl_sim.py` 脚本来启动 `FLExperiment`。完成三轮后，聚合模型应该达到大约 82% 的验证准确率。再次强调，性能几乎与本地训练场景相同。
- en: Integrating IBM FL for SST-2
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 IBM FL 用于 SST-2
- en: 'IBM FL uses a saved version of the model when performing FL. The following
    code (`create_saved_model.py`) initializes a model (calling the model on a dummy
    input to initialize the parameters) and then saves the model in the Keras `SavedModel`
    format for IBM FL to use:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: IBM FL 在执行联邦学习时使用保存的模型版本。以下代码（`create_saved_model.py`）初始化一个模型（在虚拟输入上调用模型以初始化参数）然后以
    Keras `SavedModel` 格式保存模型供 IBM FL 使用：
- en: '[PRE35]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Run this once to save the model into the folder named `sst_model_save_dir` –
    we will point IBM FL to load in the model saved in this directory.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令一次以将模型保存到名为 `sst_model_save_dir` 的文件夹中 – 我们将指示 IBM FL 从此目录加载保存的模型。
- en: Creating DataHandler
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 DataHandler
- en: 'Next, we create a subclass of the IBM FL `DataHandler` class in charge of providing
    the training and validation data to the model – this subclass will load, preprocess,
    and store the SST datasets as class attributes. We first import the necessary
    libraries:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个 IBM FL `DataHandler` 类的子类，该类负责向模型提供训练和验证数据 – 这个子类将加载、预处理并存储 SST 数据集作为类属性。我们首先导入必要的库：
- en: '[PRE36]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The `init` function of this class loads the data info parameters, which are
    then used to load the correct SST-2 data partition:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类的 `init` 函数加载数据信息参数，然后使用这些参数来加载正确的数据集分片 SST-2：
- en: '[PRE37]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We also implement the API function that returns the loaded datasets for use
    during training/validation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还实现了返回用于训练/验证期间使用的加载数据集的 API 函数：
- en: '[PRE38]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Defining the configuration files
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义配置文件
- en: 'The next step is to create the configuration JSON files used when starting
    the aggregator and initializing the parties. The aggregation config first specifies
    the connection information it will use to communicate with the parties:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建在启动聚合器和初始化聚会时使用的配置 JSON 文件。聚合配置首先指定它将用于与聚会通信的连接信息：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we specify the fusion handler used for aggregation:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们指定用于聚合的融合处理器：
- en: '[PRE40]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We also specify the hyperparameters related to both local training and aggregation.
    `perc_quorum` refers to the percentage of parties that must participate before
    aggregation can begin:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还指定了与本地训练和聚合相关的超参数。`perc_quorum` 指的是在聚合开始之前必须参与聚会的比例：
- en: '[PRE41]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we specify the IBM FL protocol handler to use:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指定要使用的 IBM FL 协议处理器：
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: This configuration is saved in `agg_config.json`.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 此配置保存在 `agg_config.json` 文件中。
- en: 'We also create the base party configuration file used to conduct FL with the
    local data. We first specify the connection information of the aggregator and
    the party:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还创建了用于使用本地数据进行联邦学习的基聚会配置文件。我们首先指定聚合器和聚会的连接信息：
- en: '[PRE43]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We then specify the data handler and the local training handler to use – this
    component trains the SST model using the model information and the local data:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们指定要使用的数据处理器和本地训练处理器 – 此组件使用模型信息和本地数据训练 SST 模型：
- en: '[PRE44]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The model format and information is then specified – this is where we point
    to the saved model created earlier:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 指定模型格式和信息 – 这是我们指向之前创建的保存模型的地方：
- en: '[PRE45]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, we specify the protocol handler:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们指定协议处理器：
- en: '[PRE46]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Creating IBM FL party
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 IBM FL 聚会
- en: 'With this, all that is left is the code that starts each party, saved in `fl_sim.py`.
    We first import the necessary libraries:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方式，剩下的只是启动每个聚会的代码，保存在 `fl_sim.py` 文件中。我们首先导入必要的库：
- en: '[PRE47]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We include an `argparse` argument that allows for the party number to be specified
    – this is used to modify the base party configuration file in order to allow for
    distinct parties to be started from the same file:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们包含一个 `argparse` 参数，允许指定聚会编号 – 这用于修改基本聚会配置文件，以便从同一文件启动不同的聚会：
- en: '[PRE48]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Finally, we create and start a new `Party` object with the modified configuration
    information:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用修改后的配置信息创建并启动一个新的 `Party` 对象：
- en: '[PRE49]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: With this, we can now begin performing FL using IBM FL.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种方式，我们现在可以开始使用 IBM FL 进行联邦学习。
- en: Running the IBM FL example
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 IBM FL 示例
- en: 'First, start `aggregator` by running the following command:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，通过运行以下命令来启动 `aggregator`：
- en: '[PRE50]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'After the aggregator is finished setting up, type `START` and press *Enter*
    key to open the aggregator to receive incoming connections. You can then start
    three parties using the following commands in separate terminals:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合器完成设置后，输入 `START` 并按 *Enter* 键以打开聚合器以接收传入的连接。然后，你可以在单独的终端中使用以下命令启动三个参与者：
- en: '[PRE51]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Finally, type `TRAIN` into the aggregator window and press *Enter* key to begin
    the FL process. When three rounds are completed, you can type `SAVE` into the
    same window to save the latest aggregate model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在聚合器窗口中输入 `TRAIN` 并按 *Enter* 键开始 FL 流程。当完成三轮后，你可以在同一窗口中输入 `SAVE` 以保存最新的聚合模型。
- en: Integrating Flower for SST-2
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将 Flower 集成到 SST-2 中
- en: The two main Flower components that must be incorporated on top of the existing
    local training code are the client and strategy subclass implementations. The
    client subclass implementation allows us to interface with Flower, with API functions
    that allow for model parameters to be passed between the clients and the server.
    The strategy subclass implementation allows us to specify the details of the aggregation
    approach performed by the server.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在现有本地训练代码之上集成的两个主要 Flower 组件是客户端和策略子类实现。客户端子类实现允许我们与 Flower 接口，API 函数允许在客户端和服务器之间传递模型参数。策略子类实现允许我们指定服务器执行的聚合方法的细节。
- en: 'We begin by writing the code to implement and start a client (stored in `fl_sim.py`).
    First, the necessary libraries are imported:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先编写代码来实现并启动客户端（存储在 `fl_sim.py` 中）。首先，导入必要的库：
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'We add a command-line argument specifying the client ID in order to allow for
    the same client script to be reused for all three agents:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加一个命令行参数来指定客户端 ID，以便允许相同的客户端脚本被所有三个代理重用：
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We then load in the SST-2 datasets:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们加载 SST-2 数据集：
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Note that we use the client ID to get the respective shard from the training
    dataset.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用客户端 ID 从训练数据集中获取相应的分片。
- en: 'Next, we create the model and the associated optimizer and loss objects, making
    sure to call the model on a dummy input to initialize the weights:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建模型和相关优化器以及损失对象，确保在哑输入上调用模型以初始化权重：
- en: '[PRE55]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Implementing the Flower client
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现 Flower 客户端
- en: 'We can now implement the Flower client object that will pass model parameters
    to and from the server. To implement a client subclass, we have to define three
    functions:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以实现 Flower 客户端对象，该对象将在服务器之间传递模型参数。要实现客户端子类，我们必须定义三个函数：
- en: '`get_parameters(self, config)`: Returns the model parameter values'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_parameters(self, config)`: 返回模型参数值'
- en: '`fit(self, parameters, config)`: Sets the weights of the local model to the
    received parameters, performs local training, and returns the new model parameters
    alongside the dataset size and training metrics'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fit(self, parameters, config)`: 将本地模型的权重设置为接收到的参数，执行本地训练，并返回新的模型参数以及数据集大小和训练指标'
- en: '`evaluate(self, parameters, config)`: Sets the weights of the local model to
    the received parameters, then evaluates the model on validation/test data and
    returns the performance metrics'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evaluate(self, parameters, config)`: 将本地模型的权重设置为接收到的参数，然后在验证/测试数据上评估模型，并返回性能指标'
- en: 'Using `fl.client.NumPyClient` as the superclass allows us to take advantage
    of the Keras model `get_weights` and `set_weights` functions that convert the
    model parameters into lists of NumPy arrays:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `fl.client.NumPyClient` 作为超类允许我们利用 Keras 模型的 `get_weights` 和 `set_weights`
    函数，这些函数将模型参数转换为 NumPy 数组的列表：
- en: '[PRE56]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The `evaluate` function is also defined:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '`evaluate` 函数也被定义：'
- en: '[PRE57]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'With this client implementation, we can finally start the client using the
    default connection information with the following line:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此客户端实现，我们最终可以使用以下行使用默认连接信息启动客户端：
- en: '[PRE58]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Creating the Flower server
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 Flower 服务器
- en: 'The final step before running Flower is to create the script (`server.py`)
    that will start the Flower server. We begin with the necessary imports and the
    `MAX_ROUNDS` parameter:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行 Flower 之前，我们需要创建一个脚本（`server.py`），该脚本将启动 Flower 服务器。我们开始导入必要的库和 `MAX_ROUNDS`
    参数：
- en: '[PRE59]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Because we want to save the model after performing federated learning, we create
    a subclass of the flower FedAvg strategy and add a final step that saves the model
    at the last round during the aggregation phase:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们希望在执行联邦学习后保存模型，所以我们创建了一个 flower FedAvg 策略的子类，并在聚合阶段的最后一步添加了一个保存模型的步骤：
- en: '[PRE60]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'With this strategy, we can run the following line to start the server (passing
    the `MAX_ROUNDS` parameter through the `config` argument):'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，我们可以运行以下行来启动服务器（通过 `config` 参数传递 `MAX_ROUNDS` 参数）：
- en: '[PRE61]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: We can now start the server and clients, allowing for FL to be performed using
    Flower.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以启动服务器和客户端，允许使用 Flower 进行 FL。
- en: Running the Flower example
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行 Flower 示例
- en: To start the server, first run the `server.py` script.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动服务器，首先运行 `server.py` 脚本。
- en: 'Each of the three clients can then be started by running the following commands
    in separate terminal windows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 每个客户端都可以通过在单独的终端窗口中运行以下命令来启动：
- en: '[PRE62]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The final aggregate model after FL will be saved in the `final_agg_sst_model`
    directory as a `SavedModel` object.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: FL 最终的聚合模型将被保存在 `final_agg_sst_model` 目录中，作为一个 `SavedModel` 对象。
- en: Integrating STADLE for SST-2
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 STADLE 用于 SST-2
- en: STADLE differs from the previously examined FL frameworks by providing a cloud-based
    platform (STADLE Ops) to handle the deployment of aggregators and management of
    the FL process. Because the deployment of the server side can be done through
    the platform, the client-side implementation is all that needs to be implemented
    for performing FL with STADLE. This integration is done by creating a client object
    that occasionally sends the local model and returns the aggregate model from the
    previous round. To do this, we need to create the agent configuration file and
    modify the local training code to interface with STADLE.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: STADLE 与之前考察的 FL 框架不同，它提供了一个基于云的平台（STADLE Ops），用于处理聚合器的部署和 FL 流程的管理。因为服务器端的部署可以通过该平台完成，所以使用
    STADLE 进行 FL 所需实现的只是客户端的实现。这种集成是通过创建一个客户端对象来完成的，该对象偶尔发送本地模型，并从上一轮返回聚合模型。为此，我们需要创建代理配置文件，并修改本地训练代码以与
    STADLE 接口。
- en: 'First, we create the configuration file for the agent as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建代理的配置文件，如下所示：
- en: '[PRE63]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Information on these parameters can be found at https://stadle-documentation.readthedocs.io/en/latest/documentation.html#configuration-of-agent.
    Note that the aggregator IP and registration port values listed here are placeholders
    and will be modified when connecting to the STADLE Ops platform.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这些参数的详细信息可以在 https://stadle-documentation.readthedocs.io/en/latest/documentation.html#configuration-of-agent
    找到。请注意，这里列出的聚合器 IP 和注册端口号是占位符，在连接到 STADLE Ops 平台时将被修改。
- en: 'Next, we modify the local training code to work with STADLE. We first import
    the requisite libraries:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们修改本地训练代码以与 STADLE 一起工作。我们首先导入所需的库：
- en: '[PRE64]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Once again, we add a command-line argument to specify which partition of the
    training data the agent should receive:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们添加一个命令行参数来指定代理应接收的训练数据分区：
- en: '[PRE65]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we instantiate a `BasicClient` object – this is the STADLE client component
    that handles communication between the local training process and the aggregators
    on the server side. We use the configuration file defined earlier to create this
    client:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化一个 `BasicClient` 对象——这是 STADLE 客户端组件，用于处理本地训练过程与服务器端聚合器之间的通信。我们使用之前定义的配置文件来创建此客户端：
- en: '[PRE66]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Finally, we implement the FL training loop. In each round, the client gets
    the aggregate model from the previous round (starting with the base model) and
    trains it further on the local data before sending it back to the aggregator through
    the client:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们实现 FL 训练循环。在每一轮中，客户端从上一轮（从基础模型开始）获取聚合模型，并在本地数据上进一步训练，然后再通过客户端将其发送回聚合器：
- en: '[PRE67]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The `wait_for_sg_model` function returns the latest aggregate model from the
    server, and the `send_trained_model` function sends the locally trained model
    with the desired performance metrics to the server. More information on these
    integration steps can be found at https://stadle-documentation.readthedocs.io/en/latest/usage.html#client-side-stadle-integration.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`wait_for_sg_model` 函数从服务器返回最新的聚合模型，而 `send_trained_model` 函数将具有所需性能指标的本地训练模型发送到服务器。有关这些集成步骤的更多信息，请参阅
    https://stadle-documentation.readthedocs.io/en/latest/usage.html#client-side-stadle-integration。'
- en: Now that the client side has been implemented, we can use the STADLE Ops platform
    to start an aggregator and start an FL process.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在客户端实现完成后，我们可以使用 STADLE Ops 平台启动一个聚合器并启动一个 FL 流程。
- en: Creating a STADLE Ops project
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 STADLE Ops 项目
- en: 'First, go to stadle.ai and create a new account. Once you are logged in, you
    should be directed to the project information page in STADLE Ops:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，访问 stadle.ai 并创建一个新账户。一旦登录，你应该会被引导到 STADLE Ops 的项目信息页面：
- en: '![Figure 8.3 – Project information page in STADLE Ops'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 8.3 – STADLE Ops 中的项目信息页面'
- en: '](img/B18369_08_03.jpg)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18369_08_03.jpg]'
- en: Figure 8.3 – Project information page in STADLE Ops
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 – STADLE Ops中的项目信息页面
- en: 'Click on **Create New Project**, then fill in the project information and click
    **Create Project**. The project information page should have changed to show the
    following:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**创建新项目**，然后填写项目信息并点击**创建项目**。项目信息页面应已更改以显示以下内容：
- en: '![Figure 8.4 – New project added to the project information page'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.4 – 新项目添加到项目信息页面'
- en: '](img/B18369_08_04.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18369_08_04.jpg]'
- en: Figure 8.4 – New project added to the project information page
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 – 新项目添加到项目信息页面
- en: 'Click on the plus icon under **Initiate Aggregator** to start a new aggregator
    for the project, then click **OK** on the confirmation prompt. You can now navigate
    to the **Dashboard** page on the left side, resulting in a page that looks like
    the following:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 点击**启动聚合器**下方的加号图标以启动项目的新聚合器，然后在确认提示中点击**确定**。现在您可以导航到左侧的**仪表板**页面，页面看起来如下所示：
- en: '![Figure 8.5 – Dashboard page of STADLE Ops'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图8.5 – STADLE Ops仪表板页面'
- en: '](img/B18369_08_05.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18369_08_05.jpg]'
- en: Figure 8.5 – Dashboard page of STADLE Ops
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – STADLE Ops仪表板页面
- en: Replace the `aggr_ip` and `reg_port` placeholder parameter values in the `config_agent.json`
    file with the values under **IP Address to Connect** and **Port to** **Connect**,
    respectively.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 将`config_agent.json`文件中的`aggr_ip`和`reg_port`占位符参数值分别替换为**连接IP地址**和**连接端口**下的值。
- en: With this, we are now ready to begin the FL training process.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们现在就可以开始FL训练过程了。
- en: Running the STADLE example
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 运行STADLE示例
- en: 'The first step is to send the base model object to the server, allowing it
    to in turn distribute the model to the training agents. This is done with the
    following command:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是将基础模型对象发送到服务器，使其能够反过来将模型分发给训练代理。这可以通过以下命令完成：
- en: '[PRE68]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Once the command successfully runs, the **Base Model Info** section on the
    STADLE Ops dashboard should update to show the model information. We can now start
    the three agents by running the following commands:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦命令成功运行，STADLE Ops仪表板上的**基础模型信息**部分应更新以显示模型信息。现在我们可以通过运行以下命令来启动三个代理：
- en: '[PRE69]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: After three rounds, the agents will terminate and the final aggregate model
    will be displayed in the project dashboard, available for download in the Keras
    SavedModel format. The user guide located at [https://stadle.ai/user_guide/guide](https://stadle.ai/user_guide/guide)
    is recommended for more information on the various functionalities of the STADLE
    Ops platform.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 经过三轮后，代理将终止，最终的聚合模型将在项目仪表板上显示，并以Keras SavedModel格式可供下载。建议查阅位于[https://stadle.ai/user_guide/guide](https://stadle.ai/user_guide/guide)的用户指南，以获取有关STADLE
    Ops平台各种功能的更多信息。
- en: Evaluating the resulting aggregate models produced by each FL framework results
    in the same conclusion—the performance of the aggregate model essentially matches
    that of the centralized training model. As explained in the *Dataset distributions*
    section of [*Chapter 7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation*,
    this is generally the expected result. The natural question to ask is how the
    performance is affected when the local datasets are not IID—this is the focal
    point of the next section.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 评估每个联邦学习框架产生的结果聚合模型，得出的结论相同——聚合模型的性能基本上与集中式训练模型的性能相匹配。正如在[*第7章*](B18369_07.xhtml#_idTextAnchor176)的“数据集分布”部分所解释的，*模型聚合*，这通常是预期的结果。自然要问的是，当本地数据集不是独立同分布（IID）时，性能会受到怎样的影响——这是下一节的重点。
- en: Example – the federated training of an image classification model on non-IID
    data
  id: totrans-303
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 在非IID数据上对图像分类模型进行联邦训练
- en: In the previous example, we examined how a centralized deep learning problem
    could be converted into an FL analog by training multiple clients on disjoint
    subsets of the original training dataset (the *local datasets*) in an FL process.
    One key point of this local dataset creation was that the subsets were created
    by random sampling, leading to local datasets that were all IID under the same
    distribution as the original dataset. As a result, the similar performance of
    FedAvg compared to the local training scenario was expected – each client’s model
    essentially had the same set of local minima to move toward during training, making
    all local training beneficial for the global objective.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们考察了如何通过在联邦学习过程中在原始训练数据集（本地数据集）的不相交子集上训练多个客户端来将集中式深度学习问题转换为联邦学习的类似问题。这个本地数据集创建的一个关键点是，子集是通过随机采样创建的，导致所有本地数据集在原始数据集相同的分布下都是独立同分布的。因此，FedAvg与本地训练场景相似的性能是可以预期的——每个客户端的模型在训练过程中本质上都有相同的局部最小值集合要移动，这使得所有本地训练都对全局目标有益。
- en: Recall that in [*Chapter 7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation*,
    we explored how FedAvg was susceptible to the divergence in training objectives
    induced by severely non-IID local datasets. To explore the performance of FedAvg
    on varying non-IID severities, this example trains the VGG-16 model (a simple
    deep-learning-based image classification model) on constructed non-IID local datasets
    sampled from the CIFAR-10 dataset (located at [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
    CIFAR-10 is a well-known simple image classification dataset containing 60,000
    images separated into 10 different classes; the goal of models trained on CIFAR-10
    is to correctly predict the class associated with an input image. The relatively
    low complexity and ubiquity as a benchmark dataset make CIFAR-10 ideal for exploring
    the response of FedAvg to non-IID data.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在[*第7章*](B18369_07.xhtml#_idTextAnchor176)“模型聚合”中，我们探讨了FedAvg如何容易受到严重非独立同分布的本地数据集引起的训练目标发散的影响。为了探索FedAvg在变化非独立同分布严重程度上的性能，本例在从CIFAR-10数据集（位于[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)）中采样的构建的非独立同分布的本地数据集上训练了VGG-16模型（一个基于简单深度学习的图像分类模型）。CIFAR-10是一个著名的简单图像分类数据集，包含60,000张图像，分为10个不同的类别；在CIFAR-10上训练的模型的目标是正确预测与输入图像相关联的类别。相对较低复杂性和作为基准数据集的普遍性使CIFAR-10成为探索FedAvg对非独立同分布数据的响应的理想选择。
- en: Important note
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: To avoid including redundant code samples, this section focuses on the key lines
    of code that allow FL to be performed on PyTorch models using non-IID local datasets.
    It is recommended that you go through the examples within the *Example – the federated
    training of an NLP model* section in this chapter prior to reading this section
    in order to understand the core components needed for each FL framework. The implementations
    for this example can be found in full at this book’s GitHub repository ([https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    tree/main/ch8/cv_code), for use as a reference.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免包含冗余的代码示例，本节重点介绍允许在PyTorch模型上使用非独立同分布的本地数据集执行联邦学习的关键代码行。建议在阅读本节之前，先阅读本章中“示例
    – NLP模型的联邦训练”部分中的示例，以便了解每个联邦学习框架所需的核心组件。本例的实现可以在本书的GitHub仓库中找到，完整内容位于[https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)树/main/ch8/cv_code)，供参考使用。
- en: The key point of this example is determining how the non-IID datasets should
    be constructed. We will change the class label distributions of each local dataset
    by changing the number of images of each class included in the training dataset. For
    example, a dataset skewed toward cars and birds might have 5,000 images of cars,
    5,000 images of birds, and 500 images for every other class. By creating three
    disjointed subsets of the 10 classes and constructing local datasets skewed toward
    these classes, we produce three local datasets with non-IID severity proportional
    to the number of images included from the classes not selected.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 本例的关键点是确定如何构建非独立同分布（non-IID）数据集。我们将通过改变训练数据集中每个类别的图像数量来改变每个本地数据集的类别标签分布。例如，一个偏向于汽车和鸟类的数据集可能包含5,000张汽车的图像，5,000张鸟类的图像，以及每个其他类别500张图像。通过创建10个类别的三个不相交子集，并构建偏向这些类别的本地数据集，我们产生了三个本地数据集，其非独立同分布的严重程度与从未选择的类别中包含的图像数量成比例。
- en: Skewing the CIFAR-10 dataset
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏斜CIFAR-10数据集
- en: 'We first map the three class subsets to client IDs, and set the proportion
    of images to be taken from the original dataset for selected classes (`sel_count`)
    and the other classes (`del_count`):'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将三个类别子集映射到客户端 ID，并设置从原始数据集中选取的类别（`sel_count`）和其他类别（`del_count`）的图像比例：
- en: '[PRE70]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We then sample the appropriate number of images from the original dataset,
    using the indices of the images in the dataset to construct the skewed CIFAR-10
    subset:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们从原始数据集中采样适当数量的图像，使用数据集中图像的索引来构建有偏的 CIFAR-10 子集：
- en: '[PRE71]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The skewed trainset is then used to create the skewed `trainloader` for local
    training. When we refer to biasing the training data going forward, this is the
    code that is run.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用有偏的训练集创建用于本地训练的有偏 `trainloader`。当我们提到对未来的训练数据进行偏差时，这就是运行的代码。
- en: We will now demonstrate how to use different FL frameworks to run this non-IID
    FL process. Please refer to the installation instructions and framework-specific
    implementations in the previous section, *Example – the federated training of
    an NLP model*, for the explanations of the basics omitted in this section.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将演示如何使用不同的 FL 框架来运行这个非-IID FL 流程。请参阅上一节 *示例 - NLP 模型的联邦训练* 中的安装说明和框架特定实现，以了解本节中省略的基本概念。
- en: Integrating OpenFL for CIFAR-10
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 OpenFL 用于 CIFAR-10
- en: Similar to the Keras NLP example, we first create the `ShardDescriptor` and
    `DataInterface` subclasses for the non-IID CIFAR-10 datasets in `cifar_fl_dataset.py`.
    Only a few changes need to be made in order to accommodate the new dataset.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Keras NLP 示例类似，我们首先在 `cifar_fl_dataset.py` 中为非-IID 的 CIFAR-10 数据集创建 `ShardDescriptor`
    和 `DataInterface` 子类。为了适应新的数据集，只需要进行少数几个更改。
- en: 'First, we modify the `self.data_by_type` dictionary to instead store the modified
    CIFAR datasets:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们修改 `self.data_by_type` 字典，以便存储修改后的 CIFAR 数据集：
- en: '[PRE72]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The `load_cifar_data` function loads in the training and test data using `torchvision`,
    then biases the training data based on the rank passed to the object.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_cifar_data` 函数使用 `torchvision` 加载训练和测试数据，然后根据传递给对象的排名对训练数据进行偏差。'
- en: 'Because the dimensions of a data element are now known (the size of CIFAR-10
    image), we also modify the shape properties with fixed values:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据元素的维度现在已知（CIFAR-10 图像的大小），我们还使用固定值修改了形状属性：
- en: '[PRE73]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We then implement the `CifarFedDataset` subclass of the `DataInterface` class.
    No significant modifications are needed for this implementation; thus, we can
    now use the biased CIFAR-10 dataset with OpenFL.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们实现 `CifarFedDataset` 类，它是 `DataInterface` 类的子类。对于这个实现不需要进行重大修改；因此，我们现在可以使用带有
    OpenFL 的有偏 CIFAR-10 数据集。
- en: 'We now move to the actual FL process implementation (`fl_sim.py`). One key
    difference is the framework adapter that must be used to create the `ModelInterface`
    object from a PyTorch model:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们转向实际的 FL 流程实现 (`fl_sim.py`)。一个关键的区别是必须使用框架适配器来从 PyTorch 模型创建 `ModelInterface`
    对象：
- en: '[PRE74]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: The only other major change is modifying the train and validation functions
    passed to the `TaskInterface` object to mirror the PyTorch implementations of
    these functions from the local training code.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 唯一的另一个主要更改是修改传递给 `TaskInterface` 对象的培训和验证函数，以反映本地训练代码中这些函数的 PyTorch 实现。
- en: 'The last step is to create the configuration files used by the director and
    envoys. The only necessary change in the director config is the updated `sample_shape`
    and `target_shape` for the CIFAR-10 data:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是创建导演和使节使用的配置文件。导演配置中唯一必要的更改是更新 CIFAR-10 数据的 `sample_shape` 和 `target_shape`：
- en: '[PRE75]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: This is saved in `director/director_config.yaml`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 这个文件保存在 `director/director_config.yaml` 中。
- en: 'The envoy configuration files require no changes outside of updating the object
    and filenames – the directory structure should look like the following:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 使节配置文件除了更新对象和文件名之外不需要任何更改——目录结构应该如下所示：
- en: '`director`'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`director`'
- en: '`director_config.yaml`'
  id: totrans-332
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`director_config.yaml`'
- en: '`experiment`'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`experiment`'
- en: '`envoy_config_1.yaml`'
  id: totrans-334
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`envoy_config_1.yaml`'
- en: '`envoy_config_2.yaml`'
  id: totrans-335
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`envoy_config_2.yaml`'
- en: '`envoy_config_3.yaml`'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`envoy_config_3.yaml`'
- en: '`cifar_fl_dataset.py`'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cifar_fl_dataset.py`'
- en: '`fl_sim.py`'
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fl_sim.py`'
- en: You can refer to *Running the OpenFL example* in the *Integrating OpenFL for
    SST-2* section to run this example.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考 *在 *集成 OpenFL 用于 SST-2* 部分的 *运行 OpenFL 示例* 来运行此示例。
- en: Integrating IBM FL for CIFAR-10
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成 IBM FL 用于 CIFAR-10
- en: 'Recall that IBM FL requires a saved version of the model used during training.
    We first run the following code in `create_saved_model.py` to create the saved
    VGG-16 PyTorch model:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，IBM FL 需要保存训练过程中使用的模型版本。我们首先在 `create_saved_model.py` 中运行以下代码以创建保存的 VGG-16
    PyTorch 模型：
- en: '[PRE76]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: Next, we create the `DataHandler` subclass for the skewed CIFAR-10 datasets.
    The only core change is the modification of the `load_and_preprocess_data` function
    to instead load in the CIFAR-10 data and bias the training set.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为倾斜的CIFAR-10数据集创建`DataHandler`子类。唯一的核心更改是修改`load_and_preprocess_data`函数，以加载CIFAR-10数据并对训练集进行偏差。
- en: 'The next step is to create the configuration JSON files used when starting
    the aggregator and initializing the parties. No significant changes to the aggregator
    config (`agg_config.json`) are necessary, and the only core change in the party
    config is the modification of the model information to work with PyTorch:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建启动聚合器和初始化各方时使用的配置JSON文件。聚合器配置（`agg_config.json`）无需进行重大更改，而各方配置的核心更改仅是修改模型信息以与PyTorch兼容：
- en: '[PRE77]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: The code in `fl_sim.py` responsible for starting up the parties can essentially
    remain unmodified due to the extensive use of the configuration files.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 由于广泛使用配置文件，`fl_sim.py`中负责启动各方代码基本上无需修改。
- en: You can refer to *Running the IBM FL example* in the *Integrating IBM FL for
    SST-2* section to run this example.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考*在SST-2中集成IBM FL*部分的*运行IBM FL示例*来运行此示例。
- en: Integrating Flower for CIFAR-10
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成Flower用于CIFAR-10
- en: 'After loading in the CIFAR-10 data and biasing the training data, the core
    change needed for the Flower implementation is the `NumPyClient` subclass. Unlike
    the Keras example, the `get_parameters` and `set_parameters` methods rely on the
    PyTorch model state dictionaries and are a bit more involved:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载CIFAR-10数据并对训练数据进行偏差后，Flower实现所需的核心更改是`NumPyClient`子类。与Keras示例不同，`get_parameters`和`set_parameters`方法依赖于PyTorch模型状态字典，并且更为复杂：
- en: '[PRE78]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: We modify the `fit` function to mirror the training code in the local training
    example and modify the evaluate function to similarly mirror the local training
    evaluation code. Note that we call `self.set_parameters(parameters)` in order
    to update the local model instance with the most recent weights.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们修改`fit`函数，使其与本地训练示例中的训练代码相匹配，并修改`evaluate`函数，使其与本地训练评估代码相类似。请注意，我们调用`self.set_parameters(parameters)`来更新本地模型实例的最新权重。
- en: 'We also set the `grpc_max_message_length` parameter to 1 GB when starting the
    Flower client and server to accommodate the larger VGG16 model size. The client
    initialization function is now the following:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在启动Flower客户端和服务器时将`grpc_max_message_length`参数设置为1 GB，以适应更大的VGG16模型大小。客户端初始化函数现在是以下内容：
- en: '[PRE79]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Finally, we modify the aggregator code in `server.py` – the custom strategy
    we used previously to save the aggregate model at the end of the last round needs
    to be modified to work with PyTorch models:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们修改了`server.py`中的聚合器代码——我们之前用于在最后一轮结束时保存聚合模型的自定义策略需要修改以与PyTorch模型兼容：
- en: '[PRE80]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'With this strategy, we can run the following line to start the server (adding
    the `grpc_max_message_length` parameter here as well):'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此策略，我们可以运行以下行来启动服务器（在此处也添加了`grpc_max_message_length`参数）：
- en: '[PRE81]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Refer to *Running the Flower example* in the *Integrating Flower for SST-2*
    section to run this example.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*在SST-2中集成Flower*部分的*运行Flower示例*来运行此示例。
- en: Integrating STADLE for CIFAR-10
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成STADLE用于CIFAR-10
- en: 'We first modify the `config_agent.json` config file to use the VGG16 model
    from the `torchvision` library:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先修改`config_agent.json`配置文件，以使用`torchvision`库中的VGG16模型：
- en: '[PRE82]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'To integrate STADLE into the local training code, we initialize the `BasicClient`
    object and modify the training loop to send the local model every two local training
    epochs and wait for the new aggregate model:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 要将STADLE集成到本地训练代码中，我们初始化`BasicClient`对象，并修改训练循环，每两个本地训练轮次发送一次本地模型，并等待新的聚合模型：
- en: '[PRE83]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Note
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code located at [https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    contains the full implementation of this integration example for reference. To
    start an aggregator and perform FL with the CIFAR-10 STADLE example, please refer
    to *Creating a STADLE Ops project* and *Running the STADLE example* in the *Integrating
    STADLE for* *SST-2* subsection.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 位于[https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)的代码包含此集成示例的完整实现，供参考。要启动聚合器并使用CIFAR-10
    STADLE示例进行联邦学习，请参考*创建STADLE Ops项目*和*在SST-2中运行STADLE示例*部分。
- en: Testing different levels of bias in the constructed local datasets should lead
    to the same conclusion stated in the *Dataset distributions* section of [*Chapter
    7*](B18369_07.xhtml#_idTextAnchor176), *Model Aggregation* for non-IID cases—as
    the non-IID severity increases, the convergence speed and model performance decrease.
    The goal of this section was to build off of the understanding of each FL framework
    from the SST-2 example, highlighting the key changes necessary to work with a
    PyTorch model on a modified dataset. Using this section alongside the code examples
    in [https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)
    should help in understanding this example integration.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 测试构建的局部数据集中不同水平的偏差，应得出与[*第7章*](B18369_07.xhtml#_idTextAnchor176)中“数据集分布”部分所陈述的相同结论——*模型聚合*对于非独立同分布情况——随着非独立同分布严重程度的增加，收敛速度和模型性能降低。本节的目标是在SST-2示例中理解每个联邦学习框架的基础上，突出与修改后的数据集上使用PyTorch模型所需的关键变化。结合本节和[https://github.com/PacktPublishing/Federated-Learning-with-Python](https://github.com/PacktPublishing/Federated-Learning-with-Python)中的代码示例，应有助于理解此示例集成。
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered several FL frameworks through the context of two
    different examples. From the first example, you learned how a traditional centralized
    ML problem can be converted into the analogous FL scenario by separating the data
    into disjointed subsets. It is now clear that random sampling leads to local datasets
    that are IID, allowing FedAvg to reach the same level of performance as the centralized
    equivalent with any of the FL frameworks.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过两个不同示例的背景，介绍了几个联邦学习（FL）框架。从第一个示例中，你学习了如何通过将数据分割成互不重叠的子集，将传统的集中式机器学习（ML）问题转化为类似的联邦学习场景。现在很清楚，随机采样会导致局部数据集是独立同分布（IID），这使得FedAvg能够达到与集中式等效的任何联邦学习框架相同的性能水平。
- en: In the second example, you learned one of the many ways a group of datasets
    can be non-IID (different class label distributions) and observed how different
    severities of non-IID datasets affect the performance of FedAvg. We encourage
    you to explore how alternative aggregation methods can improve on FedAvg in these
    cases.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个示例中，你了解到了一组数据集可以是非独立同分布（不同类别标签分布）的许多方法之一，并观察到了不同严重程度的非独立同分布数据集如何影响FedAvg的性能。我们鼓励你探索如何通过替代聚合方法在这些情况下改进FedAvg。
- en: Both examples also should have given you a solid understanding of the general
    trends when working with different FL frameworks; while the specific implementation-level
    details may change (due to the rapidly changing field), the core concepts and
    implementation details will remain fundamentals.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个示例也应该让你对不同联邦学习框架工作时的一般趋势有了坚实的理解；虽然具体的实现级细节可能会改变（由于该领域的快速变化），但核心概念和实现细节将仍然是基础。
- en: In the next chapter, we continue our transition to the business application
    side of FL by taking a look at several case studies involving the application
    of FL to specific domains.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续转向联邦学习的商业应用方面，通过研究涉及联邦学习在特定领域应用的几个案例研究。
