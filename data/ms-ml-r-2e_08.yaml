- en: Cluster Analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类分析
- en: '"Quickly bring me a beaker of wine, so that I may wet my mind and say something
    clever."'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “快给我拿一杯酒来，让我润润嗓子，好说些聪明话。”
- en: '- Aristophanes, Athenian Playwright'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '- 阿里斯托芬，雅典剧作家'
- en: In the earlier chapters, we focused on trying to learn the best algorithm in
    order to solve an outcome or response, for example, a breast cancer diagnosis
    or level of Prostate Specific Antigen. In all these cases, we had *y,* and that
    *y* is a function of *x,* or *y = f(x)*. In our data, we had the actual *y* values
    and we could train the *x* accordingly. This is referred to as **supervised learning**.
    However, there are many situations where we try to learn something from our data
    and either we do not have the *y* or we actually choose to ignore it. If so, we
    enter the world of **unsupervised learning**. In this world, we build and select
    our algorithm based on how well it addresses our business needs versus how accurate
    it is.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们专注于尝试学习最佳算法来解决结果或响应，例如，乳腺癌诊断或前列腺特异性抗原水平。在这些所有情况下，我们都有*y*，而*y*是*x*的函数，或者说*y
    = f(x)*。在我们的数据中，我们有实际的*y*值，我们可以相应地训练*x*。这被称为**监督学习**。然而，有许多情况下，我们试图从我们的数据中学习，要么我们没有*y*，要么我们实际上选择忽略它。如果是这样，我们就进入了**无监督学习**的世界。在这个世界里，我们根据算法如何解决我们的业务需求以及其准确性来构建和选择我们的算法。
- en: Why would we try and learn without supervision? First of all, unsupervised learning
    can help you understand and identify patterns in your data, which may be valuable.
    Second, you can use it to transform your data in order to improve your supervised
    learning techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要尝试无监督学习呢？首先，无监督学习可以帮助你理解和识别数据中的模式，这可能很有价值。其次，你可以用它来转换你的数据，以改进你的监督学习技术。
- en: This chapter will focus on the former and the next chapter on the latter.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点关注前者，下一章将关注后者。
- en: 'So, let''s begin by tackling a popular and powerful technique known as **cluster
    analysis**. With cluster analysis, the goal is to group the observations into
    a number of groups (k-groups), where the members in a group are as similar as
    possible while the members between groups are as different as possible. There
    are many examples of how this can help an organization; here are just a few:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从解决一种流行且强大的技术开始，这种技术被称为**聚类分析**。在聚类分析中，目标是把观测值分成若干组（k组），其中组内成员尽可能相似，而组间成员尽可能不同。有许多例子说明这如何帮助一个组织；这里只列举几个：
- en: The creation of customer types or segments
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建客户类型或细分市场
- en: The detection of high-crime areas in a geography
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在地理区域中检测高犯罪区域
- en: Image and facial recognition
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像和面部识别
- en: Genetic sequencing and transcription
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基因测序和转录
- en: Petroleum and geological exploration
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 石油和地质勘探
- en: 'There are many uses of cluster analysis but there are also many techniques.
    We will focus on the two most common: **hierarchical** and **k-means**. They are
    both effective clustering methods, but may not always be appropriate for the large
    and varied datasets that you may be called upon to analyze. Therefore, we will
    also examine **Partitioning Around Medoids** (**PAM**) using a **Gower-based**
    metric dissimilarity matrix as the input.  Finally, we will examine a new methodology
    I recently learned and applied using **Random Forest** to transform your data.
     The transformed data can then be used as an input to unsupervised learning.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析有许多用途，但也有许多技术。我们将重点关注两种最常见的方法：**层次聚类**和**k-means聚类**。它们都是有效的聚类方法，但可能并不总是适用于你可能需要分析的庞大且多样化的数据集。因此，我们还将使用基于**Gower**距离度量的**PAM（基于中位数聚类**）方法作为输入进行考察。最后，我们将考察一种我最近学习并应用的新方法，即使用**随机森林**来转换你的数据。转换后的数据可以随后用作无监督学习的输入。
- en: 'A final comment before moving on. You may be asked if these techniques are
    more art than science as the learning is unsupervised. I think the clear answer
    is, *it depends*. In early 2016, I presented the methods here at a meeting of
    the Indianapolis, Indiana R-User Group. To a person, we all agreed that it is
    the judgment of the analysts and the business users that makes unsupervised learning
    meaningful and determines whether you have, say, three versus four clusters in
    your final algorithm.  This quote sums it up nicely:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我想说一句。你可能会被问及这些技术是否更像艺术而非科学，因为学习是无监督的。我认为明确的答案是，*这取决于*。在2016年初，我在印第安纳波利斯R-用户组的会议上介绍了这些方法。我们一致认为，分析师和业务用户的判断使得无监督学习变得有意义，并决定了你最终算法中有三个还是四个聚类。这句话很好地总结了这一点：
- en: '"The major obstacle is the difficulty in evaluating a clustering algorithm
    without taking into account the context: why does the user cluster his data in
    the first place, and what does he want to do with the clustering afterwards? We
    argue that clustering should not be treated as an application-independent mathematical
    problem, but should always be studied in the context of its end-use."'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “主要障碍在于难以在没有考虑上下文的情况下评估聚类算法：用户最初为什么要聚类他的数据，他打算用聚类做什么？我们认为聚类不应被视为一个独立于应用数学问题，而应始终在其最终使用的上下文中进行研究。”
- en: '- Luxburg et al. (2012)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '- Luxburg et al. (2012)'
- en: Hierarchical clustering
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: The hierarchical clustering algorithm is based on a dissimilarity measure between
    observations. A common measure, and what we will use, is **Euclidean distance**.
    Other distance measures are also available.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法基于观测值之间的相似性度量。一个常见的度量，也是我们将使用的，是**欧几里得距离**。还有其他距离度量可供选择。
- en: Hierarchical clustering is an agglomerative or bottom-up technique. By this,
    we mean that all observations are their own cluster. From there, the algorithm
    proceeds iteratively by searching all the pairwise points and finding the two
    clusters that are the most similar. So, after the first iteration, there are *n-1*
    clusters, and after the second iteration, there are *n-2* clusters, and so forth.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种聚合或自下而上的技术。这意味着所有观测值都是它们自己的聚类。从那里开始，算法通过迭代地搜索所有成对点并找到最相似的两组聚类来继续进行。因此，第一次迭代后，有
    *n-1* 个聚类，第二次迭代后有 *n-2* 个聚类，依此类推。
- en: As the iterations continue, it is important to understand that in addition to
    the distance measure, we need to specify the linkage between the groups of observations.
    Different types of data will demand that you use different cluster linkages. As
    you experiment with the linkages, you may find that some may create highly unbalanced
    numbers of observations in one or more clusters. For example, if you have 30 observations,
    one technique may create a cluster of just one observation, regardless of how
    many total clusters that you specify. In this situation, your judgment will likely
    be needed to select the most appropriate linkage as it relates to the data and
    business case.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 随着迭代的进行，重要的是要理解，除了距离度量之外，我们还需要指定观测值组之间的链接。不同类型的数据将要求你使用不同的聚类链接。当你尝试不同的链接时，你可能会发现某些链接可能会在一个或多个聚类中创建高度不平衡的观测值数量。例如，如果你有30个观测值，一种技术可能会创建一个只有一个观测值的聚类，无论你指定多少个总聚类。在这种情况下，你可能会需要做出判断，选择与数据和业务案例最合适的链接。
- en: 'The following table lists the types of common linkages, but note that there
    are others:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了常见的链接类型，但请注意，还有其他类型：
- en: '| **Linkage** | **Description** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **链接** | **描述** |'
- en: '| Ward | This minimizes the total within-cluster variance as measured by the
    sum of squared errors from the cluster points to its centroid |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 瓦德（Ward） | 这通过从聚类点到其重心的平方误差之和来最小化总聚类内方差 |'
- en: '| Complete | The distance between two clusters is the maximum distance between
    an observation in one cluster and an observation in the other cluster |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 完全 | 两个聚类之间的距离是一个聚类中的观测值与另一个聚类中的观测值之间的最大距离 |'
- en: '| Single | The distance between two clusters is the minimum distance between
    an observation in one cluster and an observation in the other cluster |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 单个 | 两个聚类之间的距离是一个聚类中的观测值与另一个聚类中的观测值之间的最小距离 |'
- en: '| Average | The distance between two clusters is the mean distance between
    an observation in one cluster and an observation in the other cluster |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 平均 | 两个聚类之间的距离是一个聚类中的观测值与另一个聚类中的观测值之间的平均距离 |'
- en: '| Centroid | The distance between two clusters is the distance between the
    cluster centroids |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 重心 | 两个聚类之间的距离是聚类重心的距离 |'
- en: The output of hierarchical clustering will be a **dendrogram**, which is a tree-like
    diagram that shows the arrangement of the various clusters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的输出将是一个**树状图**，它是一个类似树的图表，显示了各种聚类的排列。
- en: As we will see, it can often be difficult to identify a clear-cut breakpoint
    in the selection of the number of clusters. Once again, your decision should be
    iterative in nature and focused on the context of the business decision.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们将看到的，在确定聚类数量时，往往很难找到一个明确的断点。再次强调，你的决策应该是迭代的，并且要关注业务决策的背景。
- en: Distance calculations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 距离计算
- en: As mentioned previously, Euclidean distance is commonly used to build the input
    for hierarchical clustering. Let's look at a simple example of how to calculate
    it with two observations and two variables/features.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，欧几里得距离通常用于构建层次聚类的输入。让我们看看如何使用两个观察结果和两个变量/特征来计算它。
- en: 'Let''s say that observation *A* costs $5.00 and weighs 3 pounds. Further, observation
    *B* costs $3.00 and weighs 5 pounds. We can place these values in the distance
    formula: *distance between A and B is equal to the square root of the sum of the
    squared differences*, which in our example would be as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 假设观察结果 *A* 的成本为 $5.00，重量为 3 磅。进一步，观察结果 *B* 的成本为 $3.00，重量为 5 磅。我们可以将这些值放入距离公式中：*A
    和 B 之间的距离等于平方差的和的平方根*，在我们的例子中如下所示：
- en: '*d(A, B) = square root((5 - 3)² + (3 - 5)²) *, which is equal to *2.83*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*d(A, B) = 平方根((5 - 3)² + (3 - 5)²)*，等于 *2.83*'
- en: The value of 2.83 is not a meaningful value in and of itself, but is important
    in the context of the other pairwise distances. This calculation is the default
    in R for the `dist()` function. You can specify other distance calculations (maximum,
    manhattan, canberra, binary, and minkowski) in the function. We will avoid going
    in to detail on why or where you would choose these over Euclidean distance. This
    can get rather domain-specific, for example, a situation where Euclidean distance
    may be inadequate is where your data suffers from high-dimensionality, such as
    in a genomic study. It will take domain knowledge and/or trial and error on your
    part to determine the proper distance measure.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 2.83 的值本身并没有意义，但在其他成对距离的上下文中很重要。这个计算是 R 中 `dist()` 函数的默认值。你可以在函数中指定其他距离计算（最大值、曼哈顿距离、坎berra
    距离、二元距离和 Minkowski 距离）。我们不会深入探讨为什么或在哪里选择这些而不是欧几里得距离。这可能会非常特定于领域，例如，在欧几里得距离可能不足够的情况下，例如在基因组研究中。这将需要领域知识以及/或试错来确定适当的距离度量。
- en: One final note is to scale your data with a mean of zero and standard deviation
    of one so that the distance calculations are comparable. If not, any variable
    with a larger scale will have a larger effect on distances.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点是要将你的数据缩放，使其均值为零，标准差为之一，以便距离计算具有可比性。如果不是这样，任何规模较大的变量将对距离产生更大的影响。
- en: K-means clustering
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: With k-means, we will need to specify the exact number of clusters that we want.
    The algorithm will then iterate until each observation belongs to just one of
    the k-clusters. The algorithm's goal is to minimize the within-cluster variation
    as defined by the squared Euclidean distances. So, the kth-cluster variation is
    the sum of the squared Euclidean distances for all the pairwise observations divided
    by the number of observations in the cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 k-means 中，我们需要指定我们想要的簇的确切数量。然后算法将迭代，直到每个观察结果只属于 k 个簇中的一个。算法的目标是最小化由平方欧几里得距离定义的簇内变异。因此，第
    k 个簇的变异是所有成对观察结果的平方欧几里得距离之和除以簇中观察结果的数量。
- en: 'Due to the iteration process that is involved, one k-means result can differ
    greatly from another result even if you specify the same number of clusters. Let''s
    see how this algorithm plays out:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于涉及到的迭代过程，即使指定了相同的簇数量，一个 k-means 的结果也可能与另一个结果大相径庭。让我们看看这个算法是如何发挥作用的：
- en: '**Specify** the exact number of clusters you desire (k).'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指定**你想要的簇的确切数量（k）。'
- en: '**Initialize** K observations are randomly selected as the initial *means.*'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化** K 个观察值被随机选择作为初始的**均值**。'
- en: '**Iterate**:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代**：'
- en: K clusters are created by assigning each observation to its closest cluster
    center (minimizing within-cluster sum of squares)
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将每个观察结果分配给最近的簇中心（最小化簇内平方和）来创建 K 个簇
- en: The centroid of each cluster becomes the new *mean*
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个簇的重心成为新的**均值**
- en: This is repeated until convergence, that is the cluster centroids do not change
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这将重复进行，直到收敛，即簇重心不再改变
- en: As you can see, the final result will vary because of the initial assignment
    in step 1\. Therefore, it is important to run multiple initial starts and let
    the software identify the best solution. In R, this can be a simple process as
    we will see.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，最终结果会因步骤 1 中的初始分配而变化。因此，运行多次初始启动并让软件识别最佳解决方案非常重要。在 R 中，这可以是一个简单的过程，我们将看到。
- en: Gower and partitioning around medoids
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gower 和基于中位数划分
- en: As you conduct clustering analysis in real life, one of the things that can
    quickly become apparent is the fact that neither hierarchical nor k-means is specifically
    designed to handle mixed datasets. By mixed data, I mean both quantitative and
    qualitative or, more specifically, nominal, ordinal, and interval/ratio data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在你进行实际聚类分析时，可以迅速显现的一个事实是，层次聚类和 k-means 都不是专门设计来处理混合数据集的。混合数据，我指的是定量和定性数据，或者更具体地说，是名义、有序和区间/比率数据。
- en: The reality of most datasets that you will use is that they will probably contain
    mixed data. There are a number of ways to handle this, such as doing **Principal
    Components Analysis** (**PCA**) first in order to create latent variables, then
    using them as input in clustering or using different dissimilarity calculations.
    We will discuss PCA in the next chapter.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用的多数数据集的现实情况是，它们可能包含混合数据。有几种处理方法，例如首先进行 **主成分分析**（PCA），以创建潜在变量，然后使用它们作为聚类的输入，或者使用不同的差异计算。我们将在下一章讨论
    PCA。
- en: With the power and simplicity of R, you can use the **Gower** **dissimilarity
    coefficient** to turn mixed data to the proper feature space. In this method,
    you can even include factors as input variables. Additionally, instead of k-means,
    I recommend using the **PAM clustering algorithm**.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 利用 R 的强大和简洁性，你可以使用 **Gower** **距离系数**将混合数据转换为适当的特征空间。在此方法中，你甚至可以将因素作为输入变量。此外，与其使用
    k-means，我建议使用 **PAM 聚类算法**。
- en: 'PAM is very similar to k-means but offers a couple of advantages. They are
    listed as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PAM 与 k-means 非常相似，但提供了一些优势。以下列出如下：
- en: First, PAM accepts a dissimilarity matrix, which allows the inclusion of mixed
    data
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，PAM 接受一个距离矩阵，这允许包含混合数据
- en: Second, it is more robust to outliers and skewed data because it minimizes a
    sum of dissimilarities instead of a sum of squared Euclidean distances (Reynolds,
    1992)
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其次，它对异常值和偏斜数据更稳健，因为它最小化差异之和而不是欧几里得距离平方之和（Reynolds，1992）
- en: This is not to say that you must use Gower and PAM together. If you choose,
    you can use the Gower coefficients with hierarchical and I've seen arguments for
    and against using it in the context of k-means. Additionally, PAM can accept other
    linkages. However, when paired they make an effective method to handle mixed data.
    Let's take a quick look at both of these concepts before moving on.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说你必须同时使用 Gower 和 PAM。如果你选择，你可以使用 Gower 系数进行层次聚类，我也看到过关于在 k-means 上下文中使用它的支持与反对的论点。此外，PAM
    可以接受其他链接。然而，当它们配对时，它们成为处理混合数据的有效方法。在继续之前，让我们快速看一下这两个概念。
- en: Gower
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gower
- en: 'The Gower coefficient compares cases pairwise and calculates a dissimilarity
    between them, which is essentially the weighted mean of the contributions of each
    variable. It is defined for two cases called *i* and *j* as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Gower 系数比较案例成对，并计算它们之间的差异，这实际上是每个变量贡献的加权平均值。它被定义为两个称为 *i* 和 *j* 的案例，如下所示：
- en: '![](img/B06473_08_01.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06473_08_01.png)'
- en: Here, *S[ijk]* is the contribution provided by the *k*[th] variable, and *W[ijk]*
    is 1 if the *k*[th] variable is valid, or else *0*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*S[ijk]* 是 *k* 个变量提供的贡献，而 *W[ijk]* 如果 *k* 个变量有效则为 1，否则为 *0*。
- en: For ordinal and continuous variables, *S[ijk] = 1 - (absolute value of x[ij]
    - x[ik]) / r[k]*, where *r[k]* is the range of values for the *k*[th] variable.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 对于有序和连续变量，*S[ijk] = 1 - (x[ij] - x[ik] 的绝对值 / r[k])*，其中 *r[k]* 是 *k* 个变量的值域。
- en: For nominal variables, *S[ijk] = 1* if *x[ij] = x[jk]*, or else *0*.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 对于名义变量，如果 *x[ij] = x[jk]*，则 *S[ijk] = 1*，否则为 *0*。
- en: 'For binary variables, *S[ijk]* is calculated based on whether an attribute
    is present (+) or not present (-), as shown in the following table:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元变量，*S[ijk]* 根据属性是否存在（+）或不存在（-）来计算，如下表所示：
- en: '| **Variables** | **Value of attribute *k*** |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| **变量** | **属性 *k* 的值** |'
- en: '| Case *i* | + | + | - | - |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 案例i | + | + | - | - |'
- en: '| Case *j* | + | - | + | - |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 案例j | + | - | + | - |'
- en: '| Sijk | 1 | 0 | 0 | 0 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| Sijk | 1 | 0 | 0 | 0 |'
- en: '| Wijk | 1 | 1 | 1 | 0 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| Wijk | 1 | 1 | 1 | 0 |'
- en: PAM
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PAM
- en: For **Partitioning Around Medoids**, let's first define a **medoid**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 **围绕中位数分区**，让我们首先定义一个 **中位数**。
- en: A medoid is an observation of a cluster that minimizes the dissimilarity (in
    our case, calculated using the Gower metric) between the other observations in
    that cluster. So, similar to k-means, if you specify five clusters, you will have
    five partitions of the data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 中位数是集群中其他观测值之间差异最小化的观测值（在我们的情况下，使用 Gower 度量计算）。因此，与 k-means 类似，如果你指定五个集群，你将拥有五个数据分区。
- en: 'With the objective of minimizing the dissimilarity of all the observations
    to the nearest medoid, the PAM algorithm iterates over the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化所有观测值与最近聚类中心的差异，PAM算法迭代以下步骤：
- en: Randomly select k observations as the initial medoid.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择k个观测值作为初始聚类中心。
- en: Assign each observation to the closest medoid.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个观测值分配给最近的聚类中心。
- en: Swap each medoid and non-medoid observation, computing the dissimilarity cost.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交换每个聚类中心和非聚类中心观测值，计算差异成本。
- en: Select the configuration that minimizes the total dissimilarity.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择使总差异度最小的配置。
- en: Repeat steps 2 through 4 until there is no change in the medoids.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2至4，直到聚类中心（medoids）没有变化。
- en: Both Gower and PAM can be called using the `cluster` package in R. For Gower,
    we will use the `daisy()` function in order to calculate the dissimilarity matrix
    and the `pam()` function for the actual partitioning. With this, let's get started
    with putting these methods to the test.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: Gower和PAM都可以通过R中的`cluster`包调用。对于Gower，我们将使用`daisy()`函数来计算差异矩阵，并使用`pam()`函数进行实际分区。有了这些，让我们开始对这些方法进行测试。
- en: Random forest
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Like our motivation with the use of the Gower metric in handling mixed, in
    fact, *messy* data, we can apply random forest in an unsupervised fashion.  Selection
    of this method has some advantages:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在处理混合、实际上*杂乱*数据时使用Gower度量一样，我们也可以以无监督的方式应用随机森林。选择这种方法有一些优点：
- en: Robust against outliers and highly skewed variables
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对异常值和高度偏斜的变量具有鲁棒性
- en: No need to transform or scale the data
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无需转换或缩放数据
- en: Handles mixed data (numeric and factors)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理混合数据（数值和因子）
- en: Can accommodate missing data
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以容纳缺失数据
- en: Can be used on data with a large number of variables, in fact, it can be used
    to eliminate useless features by examining variable importance
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用于具有大量变量的数据，实际上，可以通过检查变量重要性来消除无用特征
- en: The dissimilarity matrix produced serves as an input to the other techniques
    discussed earlier (hierarchical, k-means, and PAM)
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的差异矩阵作为其他讨论过的技术（层次聚类、k均值和PAM）的输入。
- en: A couple words of caution.  It may take some trial and error to properly tune
    the Random Forest with respect to the number of variables sampled at each tree
    split (`mtry = ?` in the function) and the number of trees grown.  Studies done
    show that the more trees grown, up to a point, provide better results, and a good
    starting point is to grow 2,000 trees (Shi, T. & Horvath, S., 2006).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 有几点注意事项。调整随机森林以适应每个树分裂中采样的变量数量（函数中的`mtry = ?`）和生长的树的数量可能需要一些尝试和错误。研究表明，生长的树越多，在一定范围内，结果越好，一个好的起点是生长2,000棵树（Shi,
    T. & Horvath, S., 2006）。
- en: 'This is how the algorithm works, given a data set with no labels:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是算法的工作原理，给定一个没有标签的数据集：
- en: The current observed data is labeled as class 1
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前观察到的数据被标记为类别1
- en: A second (synthetic) set of observations are created of the same size as the
    observed data; this is created by randomly sampling from each of the features
    from the observed data, so if you have 20 observed features, you will have 20
    synthetic features
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个与观察数据大小相同的第二（合成）观测值集；这是通过从观察数据的每个特征中随机采样创建的，所以如果你有20个观察特征，你将会有20个合成特征
- en: The synthetic portion of the data is labeled as class 2, which facilitates using
    Random Forest as an artificial classification problem
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的合成部分被标记为类别2，这有助于将随机森林用作人工分类问题
- en: Create a Random Forest model to distinguish between the two classes
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个随机森林模型来区分两个类别
- en: Turn the model's proximity measures of just the observed data (the synthetic
    data is now discarded) into a dissimilarity matrix
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型仅针对观察数据的临近度度量（合成数据现在被丢弃）转换为差异矩阵
- en: Utilize the dissimilarity matrix as the clustering input features
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用差异矩阵作为聚类输入特征
- en: So what exactly are these proximity measures?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 那么这些临近度度量到底是什么呢？
- en: Proximity measure is a pairwise measure between all the observations. If two
    observations end up in the same terminal node of a tree, their proximity score
    is equal to one, otherwise zero.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 临近度度量是所有观测值之间的成对度量。如果两个观测值最终落在树的同一个终端节点，它们的临近度得分等于一，否则为零。
- en: At the termination of the Random Forest run, the proximity scores for the observed
    data are normalized by dividing by the total number of trees.  The resulting NxN
    matrix contains scores between zero and one, naturally with the diagonal values
    all being one.  That's all there is to it.  An effective technique that I believe
    is underutilized and one that I wish I had learned years ago.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林运行结束时，通过除以总树数来对观测数据的接近度得分进行归一化。得到的 NxN 矩阵包含介于零和一之间的得分，自然地，对角线值都是一。这就是全部内容。这是一个我认为被低估的有效技术，我希望我几年前就能学到。
- en: Business understanding
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业理解
- en: Until a couple of weeks ago, I was unaware that there were less than 300 certified
    Master Sommeliers in the entire world. The exam, administered by the Court of
    Master Sommeliers, is notorious for its demands and high failure rate.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 直到几周前，我才知道全世界不到 300 名认证的高级品酒师。由高级品酒师法庭管理的考试以其要求严格和高失败率而闻名。
- en: The trials, tribulations, and rewards of several individuals pursuing the certification
    are detailed in the critically-acclaimed documentary, **Somm**. So, for this exercise,
    we will try and help a hypothetical individual struggling to become a Master Sommelier
    find a latent structure in Italian wines.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 几位追求认证的个人所经历的试验、磨难和回报在备受赞誉的纪录片《Somm》中有详细描述。因此，为了这次练习，我们将尝试帮助一个试图成为高级品酒师的人找到意大利葡萄酒中的潜在结构。
- en: Data understanding and preparation
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据理解和准备
- en: 'Let''s start with loading the R packages that we will need for this chapter.
    As always, make sure that you have installed them first:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载本章所需的 R 包开始。和往常一样，请确保您已经安装了它们：
- en: '[PRE0]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset is in the `HDclassif` package, which we installed. So, we can load
    the data and examine the structure with the `str()` function:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在 `HDclassif` 包中，这是我们安装的。因此，我们可以使用 `str()` 函数加载数据并检查其结构：
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The data consists of `178` wines with 13 variables of the chemical composition
    and one variable `Class`, the label, for the cultivar or plant variety. We won''t
    use this in the clustering but as a test of model performance. The variables,
    `V1` through `V13`, are the measures of the chemical composition as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据包括 `178` 种葡萄酒，其中包含 13 个化学成分变量和一个变量 `Class`，即品种或植物种类的标签。我们不会在聚类中使用这个变量，而是将其作为模型性能的测试。变量
    `V1` 到 `V13` 是化学成分的测量值，如下所示：
- en: '`V1`: alcohol'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V1`: 酒精'
- en: '`V2`: malic acid'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V2`: 苹果酸'
- en: '`V3`: ash'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V3`: 灰分'
- en: '`V4`: alkalinity of ash'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V4`: 灰分碱度'
- en: '`V5`: magnesium'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V5`: 镁'
- en: '`V6`: total phenols'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V6`: 总酚'
- en: '`V7`: flavonoids'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V7`: 黄酮类化合物'
- en: '`V8`: non-flavonoid phenols'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V8`: 非黄酮酚'
- en: '`V9`: proanthocyanins'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V9`: 前花青素'
- en: '`V10`: color intensity'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V10`: 颜色强度'
- en: '`V11`: hue'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V11`: 色调'
- en: '`V12`: OD280/OD315'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V12`: OD280/OD315'
- en: '`V13`: proline'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`V13`: 精氨酸'
- en: 'The variables are all quantitative. We should rename them to something meaningful
    for our analysis. This is easily done with the `names()` function:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 变量都是定量变量。我们应该将它们重命名为对我们分析有意义的名称。这可以通过 `names()` 函数轻松完成：
- en: '[PRE2]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As the variables are not scaled, we will need to do this using the `scale()`
    function. This will first center the data where the column mean is subtracted
    from each individual in the column. Then the centered values will be divided by
    the corresponding column''s standard deviation. We can also use this transformation
    to make sure that we only include columns 2 through 14, dropping class and putting
    it in a data frame. This can all be done with one line of code:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变量没有缩放，我们需要使用 `scale()` 函数进行缩放。这将首先将数据中心化，即从列中的每个个体中减去列的均值。然后，中心化值将除以相应列的标准差。我们还可以使用这种转换来确保我们只包括列
    2 到 14，排除类别并将其放入数据框中。这都可以用一行代码完成：
- en: '[PRE3]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, check the structure to make sure that it all worked according to plan:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，检查结构以确保一切按计划进行：
- en: '[PRE4]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Before moving on, let''s do a quick table to see the distribution of the cultivars
    or `Class`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，让我们快速制作一个表格，以查看品种或 `Class` 的分布情况：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We can now move on to the modeling step of the process.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以继续到过程的建模步骤。
- en: Modeling and evaluation
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模与评估
- en: Having created our data frame, `df`, we can begin to develop the clustering
    algorithms. We will start with hierarchical and then try our hand at k-means.
    After this, we will need to manipulate our data a little bit to demonstrate how
    to incorporate mixed data with Gower and Random Forest.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建我们的数据框 `df` 之后，我们可以开始开发聚类算法。我们将从层次聚类开始，然后尝试 k-means 聚类。之后，我们需要稍微调整我们的数据，以展示如何将混合数据与
    Gower 和随机森林结合。
- en: Hierarchical clustering
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: To build a hierarchical cluster model in R, you can utilize the `hclust()` function
    in the base `stats` package. The two primary inputs needed for the function are
    a distance matrix and the clustering method. The distance matrix is easily done
    with the `dist()` function. For the distance, we will use Euclidean distance.
    A number of clustering methods are available, and the default for `hclust()` is
    the complete linkage.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要在R中构建层次聚类模型，你可以利用基础`stats`包中的`hclust()`函数。该函数的两个主要输入是一个距离矩阵和聚类方法。距离矩阵可以通过`dist()`函数轻松完成。对于距离，我们将使用欧几里得距离。有几种聚类方法可供选择，`hclust()`的默认方法是完全链接。
- en: We will try this, but I also recommend Ward's linkage method. Ward's method
    tends to produce clusters with a similar number of observations.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试这种方法，但我还推荐Ward的链接方法。Ward的方法倾向于产生具有相似观察数量的聚类。
- en: The complete linkage method results in the distance between any two clusters
    that is the maximum distance between any one observation in a cluster and any
    one observation in the other cluster. Ward's linkage method seeks to cluster the
    observations in order to minimize the within-cluster sum of squares.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 完全链接方法导致任意两个聚类之间的距离是聚类中任意一个观察值与其他聚类中任意一个观察值之间的最大距离。Ward的链接方法试图聚类观察值，以最小化聚类内的平方和。
- en: It is noteworthy that the R method `ward.D2` uses the squared Euclidean distance,
    which is indeed Ward's linkage method. In R, `ward.D` is available but requires
    your distance matrix to be squared values. As we will be building a distance matrix
    of non-squared values, we will require `ward.D2`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，R中的`ward.D2`方法使用的是平方欧几里得距离，这确实是Ward的链接方法。在R中，`ward.D`是可用的，但需要你的距离矩阵是平方值。由于我们将构建一个非平方值的距离矩阵，我们需要使用`ward.D2`。
- en: Now, the big question is how many clusters should we create? As stated in the
    introduction, the short, and probably not very satisfying answer is that it depends.
    Even though there are cluster validity measures to help with this dilemma--which
    we will look at--it really requires an intimate knowledge of the business context,
    underlying data, and, quite frankly, trial and error. As our sommelier partner
    is fictional, we will have to rely on the validity measures. However, that is
    no panacea to selecting the numbers of clusters as there are several dozen validity
    measures.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，最大的问题是我们应该创建多少个聚类？正如引言中所述，简短且可能并不令人满意的答案是这取决于。尽管有聚类有效性度量可以帮助解决这个困境——我们将会探讨——但这实际上需要深入了解业务背景、基础数据和，坦白说，试错。由于我们的品酒师伙伴是虚构的，我们不得不依赖有效性度量。然而，这并不是选择聚类数量的万能药，因为存在几十种有效性度量。
- en: As exploring the positives and negatives of the vast array of cluster validity
    measures is way outside the scope of this chapter, we can turn to a couple of
    papers and even R itself to simplify this problem for us. A paper by Miligan and
    Cooper, 1985, explored the performance of 30 different measures/indices on simulated
    data. The top five performers were CH index, Duda Index, Cindex, Gamma, and Beale
    Index. Another well-known method to determine the number of clusters is the **gap
    statistic** (Tibshirani, Walther, and Hastie, 2001). These are two good papers
    for you to explore if your cluster validity curiosity gets the better of you.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 由于探索大量聚类有效性度量方法的正负方面远远超出了本章的范围，我们可以转向几篇论文，甚至直接使用R来简化这个问题。Miligan和Cooper在1985年发表的一篇论文探讨了30种不同的度量/指标在模拟数据上的表现。表现最好的前五种分别是CH指数、Duda指数、C指数、Gamma指数和Beale指数。确定聚类数量的另一种著名方法是**gap统计量**（Tibshirani,
    Walther, and Hastie, 2001）。如果你对聚类有效性感到好奇，这两篇论文是很好的参考资料。
- en: 'With R, one can use the `NbClust()` function in the `NbClust` package to pull
    results on 23 indices, including the top five from Miligan and Cooper and the
    gap statistic. You can see a list of all the available indices in the help file
    for the package. There are two ways to approach this process: one is to pick your
    favorite index or indices and call them with R, the other way is to include all
    of them in the analysis and go with the majority rules method, which the function
    summarizes for you nicely. The function will also produce a couple of plots as
    well.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用R，你可以使用`NbClust`包中的`NbClust()`函数来获取23个指标的结果，包括Miligan和Cooper的前五种和gap统计量。你可以在包的帮助文件中查看所有可用的指标列表。有两种方法可以处理这个过程：一种是你选择你喜欢的指标或指标集，并用R调用它们；另一种方法是将所有这些指标都包含在分析中，并采用多数规则方法，该函数会为你很好地总结。该函数还会生成一些图表。
- en: 'With the stage set, let''s walk through the example of using the complete linkage
    method. When using the function, you will need to specify the minimum and maximum
    number of clusters, distance measures, and indices in addition to the linkage.
    As you can see in the following code, we will create an object called `numComplete`.
    The function specifications are for Euclidean distance, minimum number of clusters
    two, maximum number of clusters six, complete linkage, and all indices. When you
    run the command, the function will automatically produce an output similar to
    what you can see here--a discussion on both the graphical methods and majority
    rules conclusion:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 阶段设置完毕后，让我们通过使用完全连接方法来举例说明。当使用该函数时，你将需要指定最小和最大簇数、距离度量以及索引，除了连接方式。正如你可以在下面的代码中看到的那样，我们将创建一个名为`numComplete`的对象。函数规范是欧几里得距离，最小簇数为两个，最大簇数为六个，完全连接，以及所有索引。当你运行命令时，该函数将自动生成一个类似于你在这里看到的输出--对图形方法和多数规则结论的讨论：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Going with the majority rules method, we would select three clusters as the
    optimal solution, at least for hierarchical clustering. The two plots that are
    produced contain two graphs each. As the preceding output states, you are looking
    for a significant knee in the plot (the graph on the left-hand side) and the peak
    of the graph on the right-hand side. This is the **Hubert Index** plot:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 按照多数规则方法，我们会选择三个簇作为最佳解决方案，至少对于层次聚类来说是这样。产生的两个图表各包含两个图形。正如前面的输出所述，你正在寻找图（左侧的图形）中的显著拐点和右侧图形的峰值。这是**Hubert指数**图：
- en: '![](img/image_08_03.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_08_03.png)'
- en: 'You can see that the bend or knee is at three clusters in the graph on the
    left-hand side. Additionally, the graph on the right-hand side has its peak at
    three clusters. The following **Dindex plot** provides the same information:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在左侧的图表中看到弯曲或膝盖出现在三个簇中。此外，右侧的图表在其峰值处也有三个簇。下面的**Dindex图**提供了相同的信息：
- en: '![](img/image_08_04.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_08_04.png)'
- en: 'There are a number of values that you can call with the function and there
    is one that I would like to show. This output is the best number of clusters for
    each index and the index value for that corresponding number of clusters. This
    is done with `$Best.nc`. I''ve abbreviated the output to the first nine indices:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用该函数调用多个值，其中有一个我想展示。这个输出是每个索引的最佳簇数以及对应簇数的索引值。这是通过`$Best.nc`完成的。我已经将输出简化为前九个索引：
- en: '[PRE7]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can see that the first index, (`KL`), has the optimal number of clusters
    as five and the next index, (`CH`), has it as three.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，第一个索引（`KL`）的最佳簇数是五个，下一个索引（`CH`）是三个。
- en: 'With three clusters as the recommended selection, we will now compute the distance
    matrix and build our hierarchical cluster object. This code will build the distance
    matrix:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以三个簇作为推荐选择，我们现在将计算距离矩阵并构建我们的层次聚类对象。此代码将构建距离矩阵：
- en: '[PRE8]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we will use this matrix as the input for the actual clustering with `hclust()`:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将使用这个矩阵作为`hclust()`实际聚类的输入：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The common way to visualize hierarchical clustering is to plot a **dendrogram**.
    We will do this with the plot function. Note that `hang = -1` puts the observations
    across the bottom of the diagram:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 展示层次聚类的一种常见方式是绘制**树状图**。我们将使用绘图函数来完成这个任务。请注意，`hang = -1`将观测值放置在图的最底部：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/image_08_05.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_08_05.png)'
- en: The dendrogram is a tree diagram that shows you how the individual observations
    are clustered together. The arrangement of the connections (branches, if you will)
    tells us which observations are similar. The height of the branches indicates
    how much the observations are similar or dissimilar to each other from the distance
    matrix. Note that I specified `labels = FALSE`. This was done to aid in the interpretation
    because of the number of observations. In a smaller dataset of, say, no more than
    40 observations, the row names can be displayed.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图是一种树形图，展示了单个观测值是如何聚在一起的。连接（分支，如果你愿意这样称呼）的排列告诉我们哪些观测值是相似的。分支的高度表示观测值之间的相似度或差异度。请注意，我指定了`labels
    = FALSE`。这样做是为了帮助解释，因为观测值的数量。在一个较小的数据集中，比如说不超过40个观测值，行名可以被显示。
- en: 'To aid in visualizing the clusters, you can produce a colored dendrogram using
    the `sparcl` package. To color the appropriate number of clusters, you need to
    cut the dendrogram tree to the proper number of clusters using the `cutree()`
    function. This will also create the cluster label for each of the observations:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助可视化聚类，你可以使用`sparcl`包生成彩色树状图。为了给适当数量的聚类着色，你需要使用`cutree()`函数将树状图切割到正确的聚类数量。这也会为每个观测值创建聚类标签：
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, the `comp3` object is used in the function to build the colored dendrogram:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`comp3`对象在函数中用于构建彩色树状图：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/image_08_06.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_06.png)'
- en: 'Note that I used `branchlength = 50`. This value will vary based on your own
    data. As we have the cluster labels, let''s build a table that shows the count
    per cluster:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我使用了`branchlength = 50`。这个值将根据你的数据而变化。既然我们有聚类标签，让我们构建一个显示每个聚类计数的表格：
- en: '[PRE13]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Out of curiosity, let''s go ahead and compare how this clustering algorithm
    compares to the **cultivar** labels:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 出于好奇，让我们继续比较这种聚类算法与品种标签的对比：
- en: '[PRE14]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In this table, the rows are the clusters and columns are the cultivars. This
    method matched the cultivar labels at an 84 percent rate. Note that we are not
    trying to use the clusters to predict a cultivar, and in this example, we have
    no a priori reason to match clusters to the cultivars.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个表格中，行是聚类，列是品种。这种方法在84%的比率上匹配了品种标签。请注意，我们并不是试图使用聚类来预测品种，在这个例子中，我们没有先验理由将聚类与品种匹配。
- en: 'We will now try Ward''s linkage. This is the same code as before; it first
    starts with trying to identify the number of clusters, which means that we will
    need to change the method to `Ward.D2`:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将尝试Ward的连接。这与之前的代码相同；它首先尝试确定聚类数量，这意味着我们需要将方法更改为`Ward.D2`：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This time around also, the majority rules were for a three cluster solution.
    Looking at the Hubert Index, the best solution is a three cluster as well:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，大多数规则也是针对三个聚类解决方案的。查看Hubert指数，最佳解决方案也是三个聚类：
- en: '![](img/image_08_07.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_07.png)'
- en: 'The Dindex adds further support to the three cluster solution:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Dindex进一步支持三个聚类解决方案：
- en: '![](img/image_08_08.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_08.png)'
- en: 'Let''s move on to the actual clustering and production of the dendrogram for
    Ward''s linkage:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行实际的聚类和Ward连接的树状图生成：
- en: '[PRE16]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/image_08_09.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_09.png)'
- en: 'The plot shows three pretty distinct clusters that are roughly equal in size.
    Let''s get a count of the cluster size and show it in relation to the cultivar
    labels:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图表显示了三个大小大致相等的明显不同的聚类。让我们统计一下聚类的大小，并将其与品种标签相关联：
- en: '[PRE17]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: So, cluster one has 64 observations, cluster two has `58`, and cluster three
    has 56\. This method matches the cultivar categories closer than using complete
    linkage.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，第一个聚类有64个观测值，第二个聚类有`58`个，第三个聚类有56个。这种方法比使用完全连接法更接近品种分类。
- en: 'With another table, we can compare how the two methods match observations:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 通过另一个表格，我们可以比较两种方法如何匹配观测值：
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'While cluster three for each method is pretty close, the other two are not.
    The question now is how do we identify what the differences are for the interpretation?
    In many examples, the datasets are very small and you can look at the labels for
    each cluster. In the real world, this is often impossible. A good way to compare
    is to use the `aggregate()` function, summarizing on a statistic such as the `mean`
    or median. Additionally, instead of doing it on the scaled data, let''s try it
    on the original data. In the function, you will need to specify the dataset, what
    you are aggregating it by, and the summary statistic:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每种方法中的第三个聚类非常接近，但其他两个则不然。现在的问题是，我们如何识别这些差异以进行解释？在许多例子中，数据集非常小，你可以查看每个聚类的标签。在现实世界中，这通常是不可能的。一种好的比较方法是使用`aggregate()`函数，对统计量如`mean`或中位数进行汇总。此外，我们不是在缩放数据上操作，而是尝试在原始数据上操作。在函数中，你需要指定数据集、按什么进行聚合以及汇总统计量：
- en: '[PRE19]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This gives us the `mean` by the cluster for each of the 13 variables in the
    data. With complete linkage done, let''s give Ward a try:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了每个数据中的13个变量的聚类平均数。完成完全连接后，让我们尝试Ward的方法：
- en: '[PRE20]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The numbers are very close. The cluster one for Ward's method does have slightly
    higher values for all the variables. For cluster two of Ward's method, the mean
    values are smaller except for Hue. This would be something to share with someone
    who has the domain expertise to assist in the interpretation. We can help this
    effort by plotting the values for the variables by the cluster for the two methods.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 数字非常接近。Ward 方法的第一聚类的所有变量值都略有偏高。对于 Ward 方法的第二个聚类，除了色调外，平均值都较小。这将是与具有领域专业知识的人分享以协助解释的内容。我们可以通过绘制两种方法的变量值按聚类来帮助这一努力。
- en: A nice plot to compare distributions is the **boxplot**. The boxplot will show
    us the minimum, first quartile, median, third quartile, maximum, and potential
    outliers.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于比较分布的好图是 **箱线图**。箱线图将显示最小值、第一四分位数、中位数、第三四分位数、最大值和潜在的异常值。
- en: 'Let''s build a comparison plot with two boxplot graphs with the assumption
    that we are curious about the `Proline` values for each clustering method. The
    first thing to do is to prepare our plot area in order to display the graphs side
    by side. This is done with the `par()` function:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设我们对每种聚类方法的 `Proline` 值感兴趣，构建一个包含两个箱线图的比较图。首先要做的事情是为显示并排的图形准备我们的绘图区域。这是通过
    `par()` 函数完成的：
- en: '[PRE21]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here, we specified that we wanted one row and two columns with `mfrow = c(1,
    2))`. If you want it as two rows and one column, then it would have been `mfrow
    = c(2, 1))`. In the `boxplot()` function, we will need to specify that the *y*
    axis values are a function of the *x* axis values with the tilde `~` symbol:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们指定了想要一行两列，使用 `mfrow = c(1, 2))`。如果你想要两行一列，那么应该是 `mfrow = c(2, 1))`。在 `boxplot()`
    函数中，我们需要指定 *y* 轴的值是 *x* 轴值的函数，使用波浪号 `~` 符号：
- en: '[PRE22]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![](img/image_08_10.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_10.png)'
- en: Looking at the boxplot, the thick boxes represent the first quartile, median
    (the thick horizontal line in the box), and the third quartile, which is the **interquartile
    range**. The ends of the dotted lines, commonly referred to as **whiskers** represent
    the minimum and maximum values. You can see that cluster two in complete linkage
    has five small circles above the maximum. These are known as **suspected outliers**
    and are calculated as greater than plus or minus 1.5 times the interquartile range.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 观察箱线图，粗箱代表第一四分位数、中位数（箱中的粗横线），以及第三四分位数，这是 **四分位距**。虚线线的两端，通常被称为 **胡须**，代表最小值和最大值。你可以看到在完全连接的第二个聚类中，最大值上方有五个小圆圈。这些被称为
    **疑似异常值**，计算结果大于或等于四分位距加减 1.5 倍。
- en: Any value that is greater than plus or minus three times the interquartile range
    are deemed outliers and are represented as solid black circles. For what it's
    worth, clusters one and two of Ward's linkage have tighter interquartile ranges
    with no suspected outliers.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 任何大于或等于四分位距加减三倍的值都被认为是异常值，并以实心黑色圆圈表示。就其本身而言，Ward 连接的第一个和第二个聚类的四分位距更紧密，没有疑似异常值。
- en: Looking at the boxplots for each of the variables could help you, and a domain
    expert can determine the best hierarchical clustering method to accept. With this
    in mind, let's move on to k-means clustering.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 观察每个变量的箱线图可能有助于你，领域专家可以确定最佳层次聚类方法。考虑到这一点，让我们继续到 k-means 聚类。
- en: K-means clustering
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means 聚类
- en: 'As we did with hierarchical clustering, we can also use `NbClust()` to determine
    the optimum number of clusters for k-means. All you need to do is specify `kmeans`
    as the method in the function. Let''s also loosen up the maximum number of clusters
    to `15`. I''ve abbreviated the following output to just the majority rules portion:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在层次聚类中所做的那样，我们也可以使用 `NbClust()` 来确定 k-means 的最佳聚类数量。你只需要在函数中指定 `kmeans`
    作为方法。让我们也将最大聚类数量放宽到 `15`。以下输出已被简化为仅包含多数规则部分：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Once again, three clusters appear to be the optimum solution. Here is the Hubert
    plot, which confirms this:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，三个聚类似乎是最优解。这是 Hubert 图，它证实了这一点：
- en: '![](img/image_08_11.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/image_08_11.png)'
- en: 'In R, we can use the `kmeans()` function to do this analysis. In addition to
    the input data, we have to specify the number of clusters we are solving for and
    a value for random assignments, the `nstart` argument. We will also need to specify
    a random seed:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在 R 中，我们可以使用 `kmeans()` 函数来进行这项分析。除了输入数据外，我们还需要指定我们正在解决的聚类数量以及随机分配的值，即 `nstart`
    参数。我们还需要指定一个随机种子：
- en: '[PRE24]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Creating a table of the clusters gives us a sense of the distribution of the
    observations between them:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 创建簇的表格让我们对观测值在它们之间的分布有一个概念：
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The number of observations per cluster is well-balanced. I have seen on a number
    of occasions with larger datasets and many more variables that no number of k-means
    yields a promising and compelling result. Another way to analyze the clustering
    is to look at a matrix of the cluster centers for each variable in each cluster:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 每个簇的观测数是均衡的。我在许多情况下看到，在更大的数据集和更多的变量中，没有任何数量的k-means算法能产生有希望和令人信服的结果。分析聚类的另一种方法是查看每个簇中每个变量的簇中心矩阵：
- en: '[PRE26]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Note that cluster one has, on average, a higher alcohol content. Let''s produce
    a boxplot to look at the distribution of alcohol content in the same manner as
    we did before and also compare it to Ward''s:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，簇一的平均酒精含量较高。让我们制作一个箱线图，以查看酒精含量的分布情况，就像我们之前做的那样，并与之比较Ward的：
- en: '[PRE27]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](img/image_08_12.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_08_12.png)'
- en: 'The alcohol content for each cluster is almost exactly the same. On the surface,
    this tells me that three clusters is the proper latent structure for the wines
    and there is little difference between using k-means or hierarchical clustering.
    Finally, let''s do the comparison of the k-means clusters versus the cultivars:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 每个簇的酒精含量几乎完全相同。从表面上看，这告诉我三个簇是葡萄酒的适当潜在结构，使用k-means或层次聚类之间几乎没有差异。最后，让我们比较k-means簇与品种：
- en: '[PRE28]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: This is very similar to the distribution produced by Ward's method, and either
    one would probably be acceptable to our hypothetical sommelier.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这与Ward方法产生的分布非常相似，任何一个都可能被我们的假设品酒师接受。
- en: However, to demonstrate how you can cluster on data with both numeric and non-numeric
    values, let's work through some more examples.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了演示如何在具有数值和非数值值的数据上聚类，让我们再看一些例子。
- en: Gower and PAM
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Gower和PAM
- en: 'To begin this step, we will need to wrangle our data a little bit. As this
    method can take variables that are factors, we will convert alcohol to either
    high or low content. It also takes only one line of code utilizing the `ifelse()`
    function to change the variable to a factor. What this will accomplish is if alcohol
    is greater than zero, it will be `High`, otherwise, it will be `Low`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 开始这一步之前，我们需要稍微整理一下数据。由于这种方法可以接受因子变量，我们将酒精含量转换为高或低含量。这只需要一行代码，利用`ifelse()`函数将变量转换为因子。这将实现的是，如果酒精含量大于零，它将被标记为`High`，否则为`Low`：
- en: '[PRE29]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We are now ready to create the dissimilarity matrix using the `daisy()` function
    from the `cluster` package and specifying the method as `gower`:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备使用`cluster`包中的`daisy()`函数创建相似性矩阵，并指定方法为`gower`：
- en: '[PRE30]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The creation of the cluster object--let''s call it `pamFit`--is done with the
    `pam()` function, which is a part of the `cluster` package. We will create three
    clusters in this example and create a table of the cluster size:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 创建簇对象——让我们称它为`pamFit`——是通过`cluster`包中的`pam()`函数完成的。在这个例子中，我们将创建三个簇并创建一个簇大小的表格：
- en: '[PRE31]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Now, let''s see how it does compared to the cultivar labels:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看它与品种标签相比的表现如何：
- en: '[PRE32]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let''s take this solution and build a descriptive statistics table using the
    power of the `compareGroups` package. In base R, creating presentation-worthy
    tables can be quite difficult and this package offers an excellent solution. The
    first step is to create an object of the descriptive statistics by the cluster
    with the `compareGroups()` function of the package. Then, using `createTable()`,
    we will turn the statistics to an easy-to-export table, which we will do as a
    .csv. If you want, you can also export the table as a PDF, HTML, or the LaTeX
    format:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用`compareGroups`包的力量，将这个解决方案构建成一个描述性统计表。在基础R中，创建令人印象深刻的表格可能相当困难，而这个包提供了一个出色的解决方案。第一步是使用包中的`compareGroups()`函数创建一个按簇的描述性统计对象。然后，使用`createTable()`，我们将统计信息转换为一个易于导出的表格，我们将以.csv格式完成。如果您愿意，也可以将表格导出为PDF、HTML或LaTeX格式：
- en: '[PRE33]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This table shows the proportion of the factor levels by the cluster, and for
    the numeric variables, the mean and standard deviation are displayed in parentheses.
    To export the table to a `.csv` file, just use the `export2csv()` function:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此表显示了每个簇中因子水平的比例，对于数值变量，均值和标准差显示在括号中。要将表格导出为`.csv`文件，只需使用`export2csv()`函数：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If you open this file, you will get this table, which is conducive to further
    analysis and can be easily manipulated for presentation purposes:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你打开这个文件，你会得到这个表格，它有利于进一步分析，并且可以很容易地用于演示目的：
- en: '![](img/image_08_001.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/image_08_001.jpg)'
- en: Finally, we'll create a dissimilarity matrix with Random Forest and create three
    clusters with PAM.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用随机森林创建一个相似性矩阵，并使用PAM创建三个聚类。
- en: Random Forest and PAM
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林和PAM
- en: 'To perform this method in R, you can use the `randomForest()` function.  After
    seeding the random seed, simply create the model object.  In the following code,
    I specify the number of trees as `2000` and set proximity measure to `TRUE`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 要在R中执行此方法，你可以使用`randomForest()`函数。在设置随机种子后，只需创建模型对象。在以下代码中，我指定了树的数量为`2000`并将邻近度度量设置为`TRUE`：
- en: '[PRE35]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'As you can see, placing a call to `rf` did not provide any meaningful output
    other than the variables sampled at each split (`mtry`).  Let''s examine the first
    five rows and first five columns of the *N x N* matrix:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，调用`rf`并没有提供任何有意义的输出，除了在每个分割中采样的变量（`mtry`）。让我们检查矩阵的前五行和前五行：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'One way to think of the values is that they are the percentage of times those
    two observations show up in the same terminal nodes! Looking at variable importance
    we see that the transformed Alcohol input could be dropped. We will keep it for
    simplicity:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 想象这些值的一种方式是，它们是这两个观察值出现在相同终端节点中的百分比！查看变量重要性，我们发现转换后的酒精输入可以删除。我们将为了简单起见保留它：
- en: '[PRE37]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'It is now just a matter of creating the dissimilarity matrix, which transforms
    the proximity values (*square root(1 - proximity)*) as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在只是创建相似性矩阵的问题，它将邻近度值（*平方根（1 - 邻近度）*）转换为以下形式：
- en: '[PRE38]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We now have our input features, so let''s run a PAM clustering as we did earlier:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了输入特征，让我们像之前一样运行PAM聚类：
- en: '[PRE39]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: These results are comparable to the other techniques applied. Can you improve
    the results by tuning the Random Forest?
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与其他应用的技术相当。你能通过调整随机森林来提高结果吗？
- en: If you have messy data for a clustering problem, consider using Random Forest.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个聚类问题的杂乱数据，考虑使用随机森林。
- en: Summary
  id: totrans-237
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we started exploring unsupervised learning techniques. We focused
    on cluster analysis to both provide data reduction and data understanding of the
    observations.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们开始探索无监督学习技术。我们专注于聚类分析，旨在提供数据降维和对观察值的数据理解。
- en: 'Four methods were introduced: the traditional hierarchical and k-means clustering
    algorithms, along with PAM, incorporating two different inputs (Gower and Random
    Forest). We applied these four methods to find a structure in Italian wines coming
    from three different cultivars and examined the results.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 介绍了四种方法：传统的层次聚类和k-means聚类算法，以及PAM，结合了两种不同的输入（Gower和随机森林）。我们将这四种方法应用于来自三个不同品种的意大利葡萄酒的结构，并检查了结果。
- en: In the next chapter, we will continue exploring unsupervised learning, but instead
    of finding structure among the observations, we will focus on finding structure
    among the variables in order to create new features that can be used in a supervised
    learning problem.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续探索无监督学习，但我们将专注于在变量之间寻找结构，而不是在观察值之间寻找结构，以便创建可用于监督学习问题的新特征。
