- en: 7\. Topic Modeling
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 主题建模
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will perform basic cleaning techniques for textual data
    and then model the cleaned data to derive relevant topics. You will evaluate **Latent
    Dirichlet Allocation** (**LDA**) models and execute **non-negative matrix factorization**
    (**NMF**) models. Finally, you will interpret the results of topic models and
    identify the best topic model for the given scenario. We will see how topic modeling
    provides insights into the underlying structure of documents. By the end of this
    chapter, you will be able to build fully functioning topic models to derive value
    and insights for your business.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将对文本数据进行基本的清洗技术，然后对清洗后的数据进行建模，以推导出相关主题。你将评估**潜在狄利克雷分配**（**LDA**）模型，并执行**非负矩阵分解**（**NMF**）模型。最后，你将解释主题模型的结果，并为给定场景识别最佳的主题模型。我们将看到主题建模如何为文档的潜在结构提供洞察力。在本章结束时，你将能够构建完整的主题模型，为你的业务提供价值和洞察。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In the last chapter, the discussion focused on preparing data for modeling using
    dimensionality reduction and autoencoding. Large feature sets can be problematic
    when it comes to modeling because of multicollinearity and extensive computation
    and can thereby hinder real-time prediction. Dimensionality reduction using principal
    component analysis is one antidote to that problem. Similarly, autoencoders seek
    to find optimal feature encodings. You can think of autoencoders as a means of
    identifying quality interaction terms for the dataset. Let's now move past dimensionality
    reduction and look at some real-world modeling techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，讨论重点是使用降维和自编码技术为建模准备数据。大规模特征集在建模时可能会带来问题，因为多重共线性和大量计算可能会阻碍实时预测。使用主成分分析的降维方法是解决这一问题的一种方法。同样，自编码器旨在找到最佳的特征编码。你可以把自编码器看作是识别数据集中优质交互项的一种手段。现在，让我们超越降维，看看一些实际的建模技术。
- en: Topic modeling is one facet of **Natural Language Processing** (**NLP**), the
    field of computer science exploring the syntactic and semantic analysis of natural
    language, which has been increasing in popularity with the increased availability
    of textual datasets. NLP can deal with language in almost any form, including
    text, speech, and images. Besides topic modeling, sentiment analysis, entity recognition,
    and object character recognition are noteworthy NLP applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 主题建模是**自然语言处理**（**NLP**）的一部分，NLP是计算机科学领域，研究自然语言的句法和语义分析，随着文本数据集的增加而越来越受欢迎。NLP几乎可以处理任何形式的语言，包括文本、语音和图像。除了主题建模之外，情感分析、实体识别和对象字符识别也是值得注意的NLP应用。
- en: Nowadays, the data being collected and analyzed comes less frequently in standard
    tabular forms and more often in less structured forms, such as documents, images,
    and audio files. As such, successful data science practitioners need to be fluent
    in the methodologies used to handle these diverse datasets.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，收集和分析的数据不再是以标准表格形式出现，而是更多以不太结构化的形式，如文档、图像和音频文件。因此，成功的数据科学从业者需要精通处理这些多样化数据集的方法。
- en: 'Here is a demonstration of identifying words in a text and assigning them to
    topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个识别文本中的单词并将其分配到主题的示范：
- en: '![Figure 7.1: Example of identifying words in a text and assigning them to
    topics'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.1：在文本中识别单词并将其分配到主题的示例'
- en: '](img/B15923_07_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_01.jpg)'
- en: 'Figure 7.1: Example of identifying words in a text and assigning them to topics'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1：在文本中识别单词并将其分配到主题的示例
- en: Your immediate question is probably *what are topics?* Let's answer that question
    with an example. You could imagine, or perhaps have noticed, that on days when
    major events take place (such as national elections, natural disasters, or sports
    championships), the posts on social media websites tend to focus on those events.
    Posts generally reflect, in some way, the day's events, and they will do so in
    varying ways. Posts can, and will, have a number of divergent viewpoints that
    can be clustered into high-level topics. If we had tweets about the World Cup
    final, the topics of those tweets could cover divergent viewpoints, ranging from
    the quality of the refereeing to fan behavior. In the United States, the president
    delivers an annual speech in mid to late January called the State of the Union.
    With sufficient numbers of social media posts, we would be able to infer or predict
    high-level reactions (topics) to the speech from the social media community by
    grouping posts using the specific keywords contained in them. Topic models are
    important because they serve the same role for textual data that classic summary
    statistics serve for numeric data. That is, they provide a meaningful summarization
    of data. Let's return to the State of the Union example. The quick look here would
    be ascertaining the major points of the speech that either resonate with or miss
    the viewership.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你立刻想到的问题可能是*什么是主题？* 让我们通过一个例子来回答这个问题。你可以想象，或者可能已经注意到，在发生重大事件的日子里（例如国家选举、自然灾害或体育赛事冠军），社交媒体网站上的帖子通常会集中讨论这些事件。帖子通常以某种方式反映当天的事件，并且会以不同的方式呈现。这些帖子可以并且会有多个不同的观点，这些观点可以被归类为高层次的主题。如果我们有关于世界杯决赛的推文，这些推文的主题可能涵盖从裁判质量到球迷行为的不同观点。在美国，总统会在每年1月中下旬发表一次国情咨文演讲。通过足够数量的社交媒体帖子，我们可以通过使用帖子中的特定关键词来对社交媒体社区对演讲的高层次反应（主题）进行推断或预测。主题模型之所以重要，是因为它们在文本数据中起到的作用类似于经典的统计汇总在数值数据中的作用。也就是说，它们提供了数据的有意义总结。让我们回到国情咨文的例子。快速浏览的目标是确认演讲中的主要观点，这些观点要么引起了观众的共鸣，要么被观众忽视。
- en: Topic Models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主题模型
- en: Topic models fall into the unsupervised learning bucket because, almost always,
    the topics being identified are not known in advance. So, no target exists on
    which we can perform regression or classification modeling. In terms of unsupervised
    learning, topic models most resemble clustering algorithms, specifically k-means
    clustering. You'll recall that, in k-means clustering, the number of clusters
    is established first, and then the model assigns each data point to one of the
    predetermined number of clusters. The same is generally true of topic models.
    We select the number of topics at the start, and then the model isolates the words
    that form that number of topics. This is a great jumping-off point for a high-level
    topic modeling overview.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型属于无监督学习范畴，因为几乎总是，所识别的主题在事先是未知的。因此，没有目标可以进行回归或分类建模。从无监督学习的角度来看，主题模型与聚类算法最为相似，特别是k均值聚类。你可能还记得，在k均值聚类中，首先确定聚类的数量，然后模型将每个数据点分配到预定数量的聚类中。主题模型通常也是如此。我们在开始时选择主题的数量，然后模型会隔离出形成这些主题的词汇。这是进行高层次主题建模概述的一个很好的起点。
- en: 'Before that, let''s check that the correct environment and libraries are installed
    and ready for use. The following table lists the required libraries and their
    main purposes:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之前，让我们检查一下是否已安装并准备好使用正确的环境和库。下表列出了所需的库及其主要用途：
- en: '![Figure 7.2: Table showing different libraries and their use'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2：显示不同库及其用途的表格'
- en: '](img/B15923_07_02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_02.jpg)'
- en: 'Figure 7.2: Table showing different libraries and their use'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2：显示不同库及其用途的表格
- en: If any or all of these libraries are not currently installed, install the required
    packages via the command line using `pip`; for example, `pip install langdetect`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果当前未安装这些库中的任何一个或全部，请通过命令行使用`pip`安装所需的包；例如，`pip install langdetect`。
- en: '*Step 3* of the forthcoming exercise covers the installation of word dictionaries
    from the `nltk` package. Word dictionaries are simply collections of words that
    are curated for a specific use. The stop words word dictionary, installed below,
    contains the common words in the English language that do not clarify context,
    meaning, or intention. These common words could include *the*, *an*, *a*, and
    *in*. The word net word dictionary provides word mappings that help in the lemmatization
    process – explained below. The word mappings link words such as *run*, *running*,
    and *ran* together as all essentially meaning the same thing. At a high level,
    word dictionaries provide data scientists with a means of preparing text data
    for analysis without having an in-depth knowledge of linguistics or spending an
    enormous amount of time defining word lists or word mappings.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 即将进行的练习的*第 3 步*涵盖了从`nltk`包安装词典。词典只是为特定用途整理的单词集合。下面安装的停用词词典包含了英语中常见的词，这些词无法澄清上下文、含义或意图。这些常见词可能包括*the*、*an*、*a*和*in*等。WordNet
    词典提供了帮助词形还原过程的单词映射——如下所述。这些单词映射将诸如*run*、*running*和*ran*等词汇联系在一起，所有这些词基本上意味着相同的意思。从高层次来看，词典为数据科学家提供了一种准备文本数据以供分析的方式，无需深入了解语言学或花费大量时间定义单词列表或单词映射。
- en: Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In the exercises and activities below, the results can differ slightly from
    what is shown because of the optimization algorithms that support both Latent
    Dirichlet Allocation and Non-negative Matrix Factorization. Many of the functions
    do not have a seed setting capability.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的练习和活动中，由于支持拉普拉斯·狄利希雷分配（Latent Dirichlet Allocation）和非负矩阵分解（Non-negative
    Matrix Factorization）的优化算法，结果可能会与显示的略有不同。许多函数没有设置种子功能。
- en: 'Exercise 7.01: Setting up the Environment'
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.01：环境设置
- en: 'To check whether the environment is ready for topic modeling, we will perform
    several steps. The first of these involves loading all the libraries that will
    be needed in this chapter:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查环境是否准备好进行主题建模，我们将执行几个步骤。其中第一步是加载本章所需的所有库：
- en: Open a new Jupyter notebook.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个新的 Jupyter 笔记本。
- en: 'Import the requisite libraries:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE0]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that not all of these packages are used for cleaning the data; some of
    them are used in the actual modeling. But it is useful to import all of the required
    libraries at once, so let's take care of all library importing now.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，并非所有这些包都是用于清理数据的；其中一些包是在实际建模过程中使用的。但一次性导入所有需要的库是很有用的，因此我们现在就来处理所有库的导入。
- en: 'Libraries not yet installed will return the following error:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 尚未安装的库将返回以下错误：
- en: '![Figure 7.3: Library not installed error'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.3：未安装库错误'
- en: '](img/B15923_07_03.jpg)'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_03.jpg)'
- en: 'Figure 7.3: Library not installed error'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.3：未安装库错误
- en: If this error is returned, install the relevant libraries via the command line
    as previously discussed. Once successfully installed, rerun the library import
    process using `import`.
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果返回此错误，请按之前讨论的方式通过命令行安装相关库。安装成功后，使用`import`重新执行库导入过程。
- en: 'Certain textual data cleaning and preprocessing processes require word dictionaries.
    Here, we''ll install two of these dictionaries. If the `nltk` library is imported,
    execute the following code:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 某些文本数据清理和预处理过程需要词典。在这里，我们将安装两个这样的词典。如果已导入`nltk`库，请执行以下代码：
- en: '[PRE1]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The output is as follows:'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.4: Importing libraries and downloading dictionaries'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.4：导入库和下载词典'
- en: '](img/B15923_07_04.jpg)'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_04.jpg)'
- en: 'Figure 7.4: Importing libraries and downloading dictionaries'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.4：导入库和下载词典
- en: 'Run `matplotlib` and specify inline so that the plots print inside the notebook:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`matplotlib`并指定内联，以便图形可以显示在笔记本中：
- en: '[PRE2]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The notebook and environment are now set and ready for data loading.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本和环境现在已经设置好，可以开始加载数据了。
- en: Note
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此部分的源代码，请参考[https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3fbWQES](https://packt.live/3fbWQES)上在线运行这个示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 必须执行整个笔记本才能获得预期的结果。
- en: A High-Level Overview of Topic Models
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主题模型的高层概述
- en: When it comes to analyzing large volumes of potentially related text data, topic
    models are one go-to approach. By 'related', we mean that the documents describe
    similar topics. To run any topic model, the only data required are the documents
    themselves. No additional data (meta or otherwise) is required.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析大量潜在相关的文本数据时，主题模型是一个常用的方法。这里所说的“相关”是指文档描述的是相似的主题。运行任何主题模型所需的唯一数据就是文档本身。不需要额外的数据（无论是元数据还是其他数据）。
- en: In the simplest terms, topic models identify the abstract topics (also known
    as themes) in a collection of documents (referred to as a **corpus**), using the
    words contained in the documents. That is, if a sentence contains the words *salary*,
    *employee*, and *meeting*, it would be safe to assume that that sentence is about,
    or that its topic is, *work*. It is of note that the documents making up the corpus
    need not be documents as traditionally defined – think letters or contracts. A
    document could be anything containing text, including tweets, news headlines,
    or transcribed speech.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，主题模型通过使用文档中的单词，识别出文档集合（称为**语料库**）中的抽象主题（也称为主题）。也就是说，如果一个句子包含单词*薪水*、*员工*和*会议*，可以合理推测该句子的主题是*工作*。值得注意的是，构成语料库的文档不必是传统意义上的文档——可以是信件或合同。文档可以是任何包含文本的内容，包括推文、新闻标题或转录的语音。
- en: 'Topic models assume that words in the same document are related and use that
    assumption to define abstract topics by finding groups of words that repeatedly
    appear in close proximity. In this way, these models are classic pattern recognition
    algorithms in which the detected patterns are made up of words. The general topic
    modeling algorithm has four main steps:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型假设同一文档中的单词是相关的，并利用这一假设通过寻找反复出现在相近位置的单词群体来定义抽象主题。通过这种方式，这些模型是经典的模式识别算法，其中检测到的模式由单词组成。通用的主题建模算法有四个主要步骤：
- en: Determine the number of topics.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定主题的数量。
- en: Scan the documents and identify co-occurring words or phrases.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 扫描文档并识别共现的单词或短语。
- en: Auto-learn groups (or clusters) of words characterizing the documents.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 自动学习描述文档的单词群体（或聚类）。
- en: Output abstract topics characterizing the corpus as word groupings.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出描述语料库的抽象主题，作为单词群体。
- en: As *Step 1* notes, the number of topics needs to be selected before fitting
    the model. Selecting an appropriate number of topics can be tricky, but, as is
    the case with most machine learning models, this parameter can be optimized by
    fitting several models using different numbers of topics and selecting the best
    model based on a performance metric. We'll dive into this process again later.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 正如*步骤 1*所述，主题的数量需要在拟合模型之前选择。选择合适的主题数量可能有点棘手，但就像大多数机器学习模型一样，可以通过使用不同主题数量拟合多个模型并基于性能指标选择最佳模型来优化此参数。我们稍后会再次深入探讨这个过程。
- en: 'The following is the generic topic modeling workflow:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是通用主题建模的工作流：
- en: '![Figure 7.5: The generic topic modeling workflow'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.5：通用主题建模工作流'
- en: '](img/B15923_07_05.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_05.jpg)'
- en: 'Figure 7.5: The generic topic modeling workflow'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：通用主题建模工作流
- en: It is important to optimize the number of topics parameter, as this parameter
    can majorly impact topic coherence. This is because the model finds groups of
    words that best fit the corpus under the constraint of a predefined number of
    topics. If the number of topics is too high, the topics become inappropriately
    narrow. Overly specific topics are referred to as **over-cooked**. Likewise, if
    the number of topics is too low, the topics become generic and vague. These types
    of topics are considered **under-cooked**. Over-cooked and under-cooked topics
    can sometimes be fixed by decreasing or increasing the number of topics, respectively.
    In practice, a frequent and unavoidable result of topic models is that, frequently,
    at least one topic will be problematic.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 优化主题数量的参数非常重要，因为这个参数会极大影响主题的连贯性。因为模型在预定义的主题数量约束下，找到最适合语料库的单词群体。如果主题数量过高，主题就会变得不适当的狭窄。过于具体的主题称为**过度处理**。同样，如果主题数量过低，主题就会变得泛化和模糊。这些类型的主题被认为是**欠处理**。过度处理和欠处理的主题有时可以通过分别减少或增加主题数量来解决。实际上，主题模型的一个常见且不可避免的结果是，通常至少会有一个主题存在问题。
- en: 'A key aspect of topic models is that they do not produce specific one-word
    or one-phrase topics, but rather collections of words, each of which represents
    an abstract topic. Recall the imaginary sentence about *work* from before. The
    topic model built to identify the topics of some hypothetical corpus to which
    that sentence belongs would not return the word *work* as a topic. It would instead
    return a collection of words, such as *paycheck*, *employee*, and *boss*—words
    that describe the topic and from which the one-word or one-phrase topic could
    be inferred. This is because topic models understand word **proximity**, not context.
    The model has no idea what *paycheck*, *employee*, and *boss* mean; it only knows
    that these words, generally, whenever they appear, appear in close proximity to
    one another:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型的一个关键方面是，它们不会生成具体的单词或短语作为主题，而是生成一组词，每个词代表一个抽象主题。回想之前关于*工作*的假想句子。构建的主题模型旨在识别该句子所属的假设语料库中的主题时，并不会返回*工作*这个词作为主题。它会返回一组词，例如*薪水单*、*员工*和*老板*——这些词描述了该主题，可以从中推断出单词或短语主题。这是因为主题模型理解词的**接近性**，而不是上下文。模型并不知道*薪水单*、*员工*和*老板*的含义，它只知道这些词通常在出现时，彼此之间会出现在接近的位置：
- en: '![Figure 7.6: Inferring topics from word groupings'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.6：从词组推断主题'
- en: '](img/B15923_07_06.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_06.jpg)'
- en: 'Figure 7.6: Inferring topics from word groupings'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：从词组推断主题
- en: Topic models can be used to predict the topic(s) belonging to unseen documents,
    but if you are going to make predictions, it is important to recognize that topic
    models only know the words used to train them. That is, if the unseen documents
    have words that were not in the training data, the model will not be able to process
    those words even if they link to one of the topics identified in the training
    data. Because of this fact, topic models tend to be used more for exploratory
    analysis and inference than for prediction.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 主题模型可以用来预测未知文档所属的主题，但如果你打算进行预测，重要的是要认识到，主题模型只知道用来训练它们的词汇。也就是说，如果未知文档中有训练数据中没有的词，模型将无法处理这些词，即使它们与训练数据中已识别的某个主题相关联。由于这一事实，主题模型往往更多地用于探索性分析和推理，而非预测。
- en: Each topic model outputs two matrices. The first matrix contains words against
    topics. It lists each word related to each topic with some quantification of the
    relationship. Given the number of words being considered by the model, each topic
    is only going to be described by a relatively small number of words.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 每个主题模型会输出两个矩阵。第一个矩阵包含了词和主题的关系。它列出了与每个主题相关的每个词，并对关系进行了量化。鉴于模型所考虑的词汇数量，每个主题只会用相对较少的词来描述。
- en: Words can either be assigned to one topic or to multiple topics with differing
    quantifications. Whether words are assigned to one or multiple topics depends
    on the algorithm. Similarly, the second matrix contains documents against topics.
    It maps each document to each topic by some quantification of the relationship
    of each document topic combination.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 词语可以被分配到一个主题，也可以分配到多个主题，并赋予不同的量化值。词语是否被分配到一个或多个主题取决于算法。类似地，第二个矩阵包含了文档和主题的关系。它通过量化文档与主题组合的关系，将每个文档映射到每个主题。
- en: When discussing topic modeling, it is important to continually reinforce the
    fact that the word groups representing topics are not related conceptually; they
    are related only by proximity. The frequent proximity of certain words in the
    documents is enough to define topics because of an assumption stated previously—that
    all words in the same document are related.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论主题建模时，必须不断强调这样一个事实：代表主题的词组在概念上并不相关，它们仅仅是通过接近性相关。文档中某些词的频繁接近足以定义主题，因为之前提到的假设——同一文档中的所有词都是相关的。
- en: However, this assumption may either not be true or the words may be too generic
    to form coherent topics. Interpreting abstract topics involves balancing the innate
    characteristics of text data with the generated word groupings. Text data, and
    language in general, is highly variable, complex, and contextual, which means
    any generalized result needs to be consumed cautiously.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个假设可能并不成立，或者这些词可能过于通用，无法形成连贯的主题。解释抽象主题涉及平衡文本数据的内在特性与生成的词组。文本数据和语言通常具有高度的变异性、复杂性和上下文性，这意味着任何泛化的结果都需要谨慎对待。
- en: This is not to downplay or invalidate the results of the model. Given thoroughly
    cleaned documents and an appropriate number of topics, word groupings, as we will
    see, can be a good guide as to what is contained in a corpus and can effectively
    be incorporated into larger data systems.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是贬低或无效化模型的结果。在彻底清洗过的文档和适当数量的主题下，词汇组（正如我们将看到的）可以很好地指引我们理解语料库中的内容，并能有效地纳入更大的数据系统。
- en: We have discussed some of the limitations of topic models already, but there
    are some additional points to consider. The noisy nature of text data can lead
    topic models to assign words unrelated to one of the topics to that topic.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了一些主题模型的局限性，但仍有一些额外的要点需要考虑。文本数据的噪声特性可能导致主题模型将与某个主题无关的词语错误地分配到该主题。
- en: Again, consider the sentence about *work* from before. The word *meeting* could
    appear in the word grouping representing the topic *work*. It is also possible
    that the word *long* could be in that group, but the word *long* is not directly
    related to *work*. *Long* may be in the group because it frequently appears in
    close proximity to the word *meeting*. Therefore, *long* would probably be considered
    to be falsely (or spuriously) correlated to *work* and should probably be removed
    from the topic grouping, if possible. Spuriously correlated words in word groupings
    can cause significant problems when analyzing the data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 再次考虑之前关于*工作*的句子。词语*meeting*可能会出现在表示*工作*主题的词汇组中。也有可能词语*long*出现在这个组中，但*long*并不直接与*工作*相关。*Long*之所以出现在该组中，可能是因为它经常与词语*meeting*相近。因此，*long*可能被错误地（或虚假地）认为与*工作*相关，并且应该尽可能从主题词汇组中移除。词汇组中的虚假相关词语可能会在分析数据时造成重大问题。
- en: This is not necessarily a flaw in the model. It is, instead, a characteristic
    that, given noisy data, the model could extract quirks from the data that might
    negatively impact the results. Spurious correlations could be the result of how,
    where, or when the data was collected. If the documents were collected only in
    some specific geographic region, words associated with that region could be incorrectly,
    albeit accidentally, linked to one or many of the word groupings output from the
    model.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这不一定是模型的缺陷。相反，这是一个特性，考虑到数据中的噪声，模型可能会从数据中提取出一些特殊性，这可能会对结果产生负面影响。虚假的相关性可能是由于数据的收集方式、地点或时间所导致的。如果文档仅在某个特定的地理区域收集，那么与该区域相关的词语可能会不正确地（尽管是偶然的）与模型输出的一个或多个词汇组关联起来。
- en: Note that, with additional words in the word group, we could be attaching more
    documents to that topic than should be attached. It should be straightforward
    that, if we shrink the number of words belonging to a topic, then that topic will
    be assigned to fewer documents. Keep in mind that this is not a bad thing. We
    want each word grouping to contain only words that make sense so that we assign
    the appropriate topics to the appropriate documents.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，随着词汇组中词语的增加，我们可能会将更多文档错误地附加到该主题。应该很容易理解的是，如果我们减少属于某个主题的词语数量，那么该主题将会被分配到更少的文档中。请记住，这并不是坏事。我们希望每个词汇组只包含那些合适的词语，以便将适当的主题分配给适当的文档。
- en: There are many topic modeling algorithms, but perhaps the two best known are
    **Latent Dirichlet Allocation** (**LDA**) and **Non-Negative Matrix Factorization**
    (**NMF**). We will discuss both in detail later on.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多主题建模算法，但也许最著名的两种是**潜在狄利克雷分配**（**LDA**）和**非负矩阵分解**（**NMF**）。我们稍后会详细讨论这两种方法。
- en: Business Applications
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业应用
- en: Despite its limitations, topic modeling can provide actionable insights that
    drive business value if used correctly and in the appropriate context. Let's now
    review some of the biggest applications of topic models.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在一些局限性，主题建模仍然可以提供有助于推动商业价值的可操作性洞察，如果正确使用并在合适的情境下应用。现在，让我们回顾一下主题模型的一些最大应用。
- en: One of the use cases is exploratory data analysis on new text data where the
    underlying structure of the dataset is unknown. This is the equivalent to plotting
    and computing summary statistics for an unseen dataset featuring numeric and categorical
    variables whose characteristics need to be understood before more sophisticated
    analyses can be reasonably performed. With the results of topic modeling, the
    usability of this dataset in future modeling exercises is ascertainable. For example,
    if the topic model returns clear and distinct topics, then that dataset would
    be a great candidate for further clustering-type analyses.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个使用场景是在处理新文本数据时进行探索性数据分析，这些数据集的潜在结构尚不清楚。这相当于为一个未见过的数据集绘制图表并计算摘要统计量，其中包括需要理解其特征的数值和分类变量，在进一步的复杂分析能够合理进行之前。这些主题建模的结果可以帮助我们评估该数据集在未来建模工作中的可用性。例如，如果主题模型返回清晰且明确的主题，那么该数据集将是进一步进行聚类类分析的理想候选者。
- en: Determining topics creates an additional variable that can be used to sort,
    categorize, and/or chunk data. If our topic model returns cars, farming, and electronics
    as abstract topics, we could filter our large text dataset down to just the documents
    with farming as a topic. Once filtered, we could perform further analyses, including
    sentiment analysis, another round of topic modeling, or any other analysis we
    could think up. Beyond defining the topics present in a corpus, topic modeling
    returns a lot of other information indirectly that could be used to further break
    a large dataset down and understand its characteristics.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 确定主题会创建一个额外的变量，可以用来对数据进行排序、分类和/或分块。如果我们的主题模型返回“汽车”、“农业”和“电子产品”作为抽象主题，我们可以将大规模文本数据集筛选至仅包含“农业”作为主题的文档。筛选后，我们可以进行进一步的分析，包括情感分析、再一次的主题建模，或任何我们能想到的其他分析。除了定义语料库中存在的主题外，主题建模还间接返回了许多其他信息，这些信息可以用来进一步分解大型数据集并理解其特征。
- en: Among those characteristics is topic prevalence. Think about performing an analysis
    on an open response survey that is designed to gauge the response to a product.
    We could imagine the topic model returning topics in the form of sentiment. One
    group of words might be *good*, *excellent*, *recommend*, and *quality*, while
    the other might be *garbage*, *broken*, *poor*, and *disappointing*.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个特征是主题的普遍性。想象一下，在一个旨在衡量对某个产品反应的开放式问卷调查中进行分析。我们可以设想，主题模型返回的主题形式为情感。一组词可能是*好*、*优秀*、*推荐*和*质量*，而另一组则可能是*垃圾*、*坏掉*、*差劲*和*失望*。
- en: Given this style of survey, the topics themselves may not be that surprising,
    but what would be interesting is that we could count the number of documents containing
    each topic and glean useful insights from it. From the counts, we could say things
    like x-percent of the survey respondents had a positive reaction to the product,
    while only y-percent of the respondents had a negative reaction. Essentially,
    what we would have created is a rough version of a sentiment analysis.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这种调查方式，主题本身可能并不令人惊讶，但有趣的是，我们可以统计包含每个主题的文档数量，并从中获取有用的见解。通过这些统计数据，我们可以得出这样的结论：例如，x%
    的调查参与者对产品持积极反应，而只有 y% 的参与者持消极反应。实际上，我们所做的就是创建了一个粗略版本的情感分析。
- en: Currently, the most frequent use of a topic model is as a component of a recommendation
    engine. The emphasis today is on personalization—delivering products to consumers
    that are specifically designed and curated for those individuals. Take websites,
    news or otherwise, devoted to the propagation of articles. Companies such as Yahoo
    and Medium need customers to keep reading in order to stay in business, and one
    way to keep customers reading is to feed them articles that they would be more
    inclined to read. This is where topic modeling comes in. Using a corpus made up
    of articles previously read by an individual, a topic model would essentially
    tell us what types of articles said subscriber likes to read. The company could
    then go to its inventory and find articles with similar topics and send them to
    the individual via their account page or email. This is custom curation to facilitate
    simplicity and ease of use while also maintaining engagement.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当前，主题模型最常见的用途是作为推荐引擎的一部分。今天的重点是个性化——向消费者提供专门为他们设计和策划的产品。以网站为例，无论是新闻网站还是其他，致力于传播文章的公司，例如雅虎和Medium，需要让客户继续阅读才能保持运营。保持客户阅读的一种方式是向他们推送他们更有可能阅读的文章。这就是主题建模的作用所在。通过使用由个体之前阅读的文章组成的语料库，主题模型基本上可以告诉我们该订阅者喜欢阅读哪些类型的文章。然后，公司可以访问其库存，找到具有相似主题的文章，并通过该用户的账户页面或电子邮件将它们发送给该用户。这是为了简化使用并保持用户参与的定制策划。
- en: Before we get into prepping data for our model, let's quickly load and explore
    the data.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始准备数据以供模型使用之前，让我们快速加载并探索数据。
- en: 'Exercise 7.02: Data Loading'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '练习 7.02: 数据加载'
- en: In this exercise, we will load the data and format it. We will execute this
    exercise in the same notebook that we executed in *Exercise 7.01*, *Setting up
    the Environment*. It is incredibly important to understand as thoroughly as possible
    the dataset with which we are going to work. That process of understanding starts
    with knowing what the data looks like at a high level, how big the data is, what
    columns are present, and identifying what aspects of the dataset might be helpful
    in solving the problem we've been tasked with solving. We answer these basic questions
    below.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将加载并格式化数据。我们将在与*练习 7.01*，*设置环境*相同的笔记本中执行此练习。尽可能彻底地理解我们将要处理的数据集非常重要。理解的过程从了解数据的大致样貌、数据的大小、存在的列以及识别哪些数据集的方面可能对解决我们要解决的问题有帮助开始。我们将在下面回答这些基本问题。
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'This data is downloaded from [https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于[https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)（UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学欧文分校信息与计算机科学学院）。
- en: 'Citation: Nuno Moniz and Luís Torgo. "Multi-Source Social Feedback of Online
    News Feeds".CoRR [arXiv:1801.07055 [cs.SI]] (2018).'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 引用：Nuno Moniz 和 Luís Torgo. "在线新闻源的多源社交反馈".CoRR [arXiv:1801.07055 [cs.SI]] (2018)。
- en: The dataset can also be downloaded from [https://packt.live/2Xin2HC](https://packt.live/2Xin2HC).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集也可以从[https://packt.live/2Xin2HC](https://packt.live/2Xin2HC)下载。
- en: This is the only file that is required for this exercise. Once downloaded and
    saved locally, the data can be loaded into the notebook.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本练习所需的唯一文件。下载并保存到本地后，数据可以加载到笔记本中。
- en: 'Define the path to the data and load it using `pandas`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据路径并使用`pandas`加载数据：
- en: '[PRE3]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Add the file to the same folder where you have opened your notebook.
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将文件添加到与您打开笔记本的相同文件夹中。
- en: 'Examine the data briefly by executing the following code:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下代码简要检查数据：
- en: '[PRE4]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This user-defined function returns the shape of the data (the number of rows
    and columns), the column names, and the first two rows of the data:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个用户定义的函数返回数据的形状（行数和列数）、列名以及数据的前两行：
- en: '![Figure 7.7: Raw data'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.7: 原始数据'
- en: '](img/B15923_07_07.jpg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_07.jpg)'
- en: 'Figure 7.7: Raw data'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 7.7: 原始数据'
- en: This is a much larger dataset in terms of features than is needed to run the
    topic models.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从特征上讲，这是一个比运行主题模型所需的更大的数据集。
- en: 'Notice that one of the columns, named `Topic`, actually contains the information
    that any topic model would try to ascertain. Briefly look at the topic data provided,
    so that when you finally generate your own topics, the results can be compared
    directly. Run the following line to print the unique topic values and their number
    of occurrences:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，其中一列名为`Topic`，实际上包含了任何主题模型试图确定的信息。简要查看提供的主题数据，这样当你最终生成自己的主题时，结果可以直接进行比较。运行以下代码打印唯一的主题值及其出现次数：
- en: '[PRE5]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE6]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now, extract the headline data and transform the extracted data into a list
    object. Print the first five elements of the list and the list length to confirm
    that the extraction was successful:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，提取标题数据并将提取的数据转换为列表对象。打印列表的前五个元素以及列表的长度，以确认提取是否成功：
- en: '[PRE7]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.8: A list of headlines'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.8：标题列表'
- en: '](img/B15923_07_08.jpg)'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_08.jpg)'
- en: 'Figure 7.8: A list of headlines'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：标题列表
- en: With the data now loaded and correctly formatted, let's talk about textual data
    cleaning and then jump into some actual cleaning and preprocessing. For instructional
    purposes, the cleaning process will initially be built and executed on only one
    headline. Once we have established the process and tested it on the example headline,
    we will go back and run the process on every headline.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已加载并正确格式化，我们来谈谈文本数据清洗，然后进行一些实际的清洗和预处理。出于教学目的，清洗过程最初将在单个标题上进行构建和执行。一旦我们建立了清洗过程并在示例标题上进行了测试，我们将返回并对每个标题运行该过程。
- en: Note
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见[https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，访问[https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个Notebook才能得到期望的结果。
- en: Cleaning Text Data
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗文本数据
- en: 'A key component of all successful modeling exercises is a clean dataset that
    has been appropriately and sufficiently preprocessed for the specific data type
    and analysis being performed. Text data is no exception, as it is virtually unusable
    in its raw form. It does not matter what algorithm is being run: if the data isn''t
    properly prepared, the results will be at best meaningless and at worst misleading.
    As the saying goes, *garbage in, garbage out.* For topic modeling, the goal of
    data cleaning is to isolate the words in each document that could be relevant
    by removing everything that could be obstructive.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所有成功建模练习的一个关键组成部分是一个经过适当和充分预处理的干净数据集，专门为特定的数据类型和分析任务进行预处理。文本数据也不例外，因为它在原始形式下几乎无法使用。无论运行什么算法：如果数据没有经过适当准备，结果最好的情况下是没有意义的，最坏的情况下是误导性的。正如谚语所说，*垃圾进，垃圾出*。对于主题建模，数据清洗的目标是通过去除所有可能干扰的内容，来孤立每个文档中可能相关的词汇。
- en: Data cleaning and preprocessing is almost always specific to the dataset, meaning
    that each dataset will require a unique set of cleaning and preprocessing steps
    selected to specifically handle the issues in it. With text data, cleaning and
    preprocessing steps can include language filtering, removing URLs and screen names,
    lemmatizing, and stop word removal, among others. We will explore these in detail
    in the upcoming sections and implement these ideas in the forthcoming exercises,
    where a dataset featuring news headlines will be cleaned for topic modeling.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗和预处理几乎总是特定于数据集的，这意味着每个数据集都需要一组独特的清洗和预处理步骤，专门用于处理其中的问题。对于文本数据，清洗和预处理步骤可能包括语言过滤、移除网址和屏幕名称、词形还原以及停用词移除等。我们将在接下来的章节中详细探讨这些步骤，并在即将进行的练习中实施这些思想，届时一个包含新闻标题的数据集将被清理用于主题建模。
- en: Data Cleaning Techniques
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据清洗技术
- en: To reiterate a previous point, the goal of cleaning text for topic modeling
    is to isolate the words in each document that could be relevant to finding the
    abstract topics of the corpus. This means removing common words, short words (generally
    more common), numbers, and punctuation. No hard and fast process exists for cleaning
    data, so it is important to understand the typical problem points in the type
    of data being cleaned and do extensive exploratory work.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 重申一下之前的观点，清洗文本以进行主题建模的目标是从每个文档中提取可能与发现语料库抽象主题相关的单词。这意味着需要去除常见词、短词（通常更常见）、数字和标点符号。清洗数据没有固定的流程，因此理解所清洗数据类型中的典型问题点并进行广泛的探索性工作非常重要。
- en: Let's now discuss some of the text data cleaning techniques that we will employ.
    One of the first things that needs to be done when doing any modeling task involving
    text is to determine the language(s) of the text. In this dataset, most of the
    headlines are English, so we will remove the non-English headlines for simplicity.
    Building models on non-English text data requires additional skill sets, the least
    of which is fluency in the language being modeled.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论一些我们将在数据清洗中使用的文本清洗技巧。进行任何涉及文本的建模任务时，首先需要做的事情之一是确定文本的语言。在这个数据集中，大多数标题是英文的，因此为了简便起见，我们将删除非英文的标题。构建非英文文本数据的模型需要额外的技能，其中最基本的是对所建模语言的流利掌握。
- en: The next crucial step in data cleaning is to remove all elements of the documents
    that are either not relevant to word-based models or are potential sources of
    noise that could obscure the results. Elements needing removal could include website
    addresses, punctuation, numbers, and stop words. **Stop words** are basically
    simple, commonly used words (including *we*, *are*, and *the*). It is important
    to note that there is no definitive dictionary of stop words; instead, every dictionary
    varies slightly. Despite the differences, each dictionary contains a number of
    common words that are assumed to be topic agnostic. Topic models try to identify
    words that are both frequent and infrequent enough to be descriptive of an abstract
    topic.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗的下一个关键步骤是移除文档中所有与基于单词的模型无关的元素，或者可能成为噪声来源、掩盖结果的元素。需要移除的元素可能包括网站地址、标点符号、数字和停用词。**停用词**基本上是一些简单的、常用的词（包括*we*、*are*和*the*）。需要注意的是，并没有一个权威的停用词词典；每个词典略有不同。尽管如此，每个词典都包含一些常见词，这些词被认为与主题无关。主题模型试图识别那些既频繁又不那么频繁的词，这些词足以描述一个抽象的主题。
- en: The removal of website addresses has a similar motivation. Specific website
    addresses will appear very rarely, but even if one specific website address appears
    enough to be linked to a topic, website addresses are not interpretable in the
    same way as words. Removing irrelevant information from the documents reduces
    the amount of noise that could either prevent model convergence or obscure results.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 移除网站地址有类似的动机。特定的网站地址出现的频率非常低，但即使某个特定网站地址足够多次出现在文档中并且能与某个主题相关联，网站地址的解释方式却不同于单词。去除文档中无关的信息，可以减少那些可能妨碍模型收敛或掩盖结果的噪声。
- en: '**Lemmatization**, like language detection, is an important component of all
    modeling activities involving text. It is the process of reducing words to their
    base form as a way to group words that should all be the same but are not because
    of various changes in the tense or the part of speech. Consider the words *running*,
    *runs*, and *ran*. All three of these words have the base form of *run*. A great
    aspect of lemmatizing is that it looks at all the words in a sentence (in other
    words, it considers the context), before determining how to alter each word. Lemmatization,
    like most of the preceding cleaning techniques, simply reduces the amount of noise
    in the data, so that we can identify clean and interpretable topics.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**词形还原**，像语言检测一样，是所有涉及文本的建模活动中的一个重要组成部分。它是将单词还原为其基本形式的过程，目的是将应该相同但因时态或词性变化而不同的单词归为一类。考虑单词*running*、*runs*和*ran*。这三个单词的基本形式是*run*。词形还原的一个很好的方面是，它会查看句子中的所有单词（换句话说，它会考虑上下文），然后决定如何改变每个单词。词形还原，像大多数前述的清洗技巧一样，简单地减少了数据中的噪声，使我们能够识别出干净且易于解释的主题。'
- en: Now, with a basic knowledge of textual cleaning techniques, let's apply these
    techniques to real-world data.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，拥有了基本的文本清洗技巧知识，让我们将这些技巧应用于实际数据中。
- en: 'Exercise 7.03: Cleaning Data Step by Step'
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.03：逐步清洗数据
- en: 'In this exercise, we will learn how to implement some key techniques for cleaning
    text data. Each technique will be explained as we work through the exercise. After
    every cleaning step, the example headline is output using `print`, so we can watch
    the evolution from raw data to model-ready data:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将学习如何实现一些清理文本数据的关键技术。每个技术将在我们进行练习时进行解释。每一步清理后，都会使用`print`输出示例标题，以便我们观察从原始数据到模型数据的演变：
- en: 'Select the sixth headline as the example on which we will build and test the
    cleaning process. The sixth headline is not a random choice; it was selected because
    it contains specific problems that will be addressed during the cleaning process:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择第六个标题作为我们构建并测试清理过程的示例。第六个标题并不是随机选择的，它是因为包含了在清理过程中将要处理的特定问题：
- en: '[PRE8]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.9: The sixth headline'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.9：第六个标题'
- en: '](img/B15923_07_09.jpg)'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_09.jpg)'
- en: 'Figure 7.9: The sixth headline'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.9：第六个标题
- en: 'Use the `langdetect` library to detect the language of each headline. If the
    language is anything other than English (`en`), remove that headline from the
    dataset. The `detect` function simply detects the language of the text that is
    passed into it. When the function fails to detect a language, which it periodically
    does, simply set the language to `none` for removal later on:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `langdetect` 库来检测每个标题的语言。如果语言不是英语（`en`），则从数据集中删除该标题。`detect` 函数仅仅是检测传入文本的语言。当该函数无法检测出语言时（偶尔会发生），只需将语言设置为
    `none`，以便稍后删除：
- en: '[PRE9]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE10]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Split the string containing the headline into pieces, called **tokens**, using
    the white spaces. The returned object is a list of words and numbers that make
    up the headline. Breaking the headline string into tokens makes the cleaning and
    preprocessing process simpler. There are multiple types of tokenizers available.
    Note that NLTK itself provides various types of tokenizers. Each of the tokenizers
    considers different ways to split the sentence into tokens. The simplest one is
    splitting the text based on white spaces.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用空格将包含标题的字符串拆分成片段，称为**标记**。返回的对象是由构成标题的单词和数字组成的列表。将标题字符串拆分成标记，使清理和预处理过程更加简单。市场上有多种类型的标记器。请注意，NLTK
    本身提供了多种类型的标记器。每个标记器考虑了将句子拆分成标记的不同方式。最简单的一种是基于空格拆分文本。
- en: '[PRE11]'
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.10: String split using white spaces'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.10：使用空格拆分字符串'
- en: '](img/B15923_07_10.jpg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_10.jpg)'
- en: 'Figure 7.10: String split using white spaces'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.10：使用空格拆分字符串
- en: 'Identify all URLs using a regular expression search for tokens containing `http://`
    or `https://`. Replace the URLs with the `''URL''` string:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式搜索包含 `http://` 或 `https://` 的标记来识别所有 URL。将 URL 替换为 `'URL'` 字符串：
- en: '[PRE12]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.11: URLs replaced with the URL string'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.11：将 URL 替换为 URL 字符串'
- en: '](img/B15923_07_11.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_11.jpg)'
- en: 'Figure 7.11: URLs replaced with the URL string'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.11：将 URL 替换为 URL 字符串
- en: 'Replace all punctuation and newline symbols (`\n`) with empty strings using
    regular expressions:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式将所有标点符号和换行符号（`\n`）替换为空字符串：
- en: '[PRE13]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.12: Punctuation replaced with empty strings using regular expressions'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.12：使用正则表达式将标点符号替换为空字符串'
- en: '](img/B15923_07_12.jpg)'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_12.jpg)'
- en: 'Figure 7.12: Punctuation replaced with empty strings using regular expressions'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.12：使用正则表达式将标点符号替换为空字符串
- en: 'Replace all numbers with empty strings using regular expressions:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式将所有数字替换为空字符串：
- en: '[PRE14]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.13: Numbers replaced with empty strings'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.13：将数字替换为空字符串'
- en: '](img/B15923_07_13.jpg)'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_13.jpg)'
- en: 'Figure 7.13: Numbers replaced with empty strings'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.13：将数字替换为空字符串
- en: 'Change all uppercase letters to lowercase. Converting everything to lowercase
    is not a mandatory step, but it does help reduce complexity. With everything lowercase,
    there is less to keep track of and therefore less chance of error:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有大写字母转换为小写字母。虽然将所有内容转换为小写字母不是强制步骤，但它有助于简化复杂性。将所有内容转换为小写字母后，跟踪的内容较少，因此出错的机会也较小：
- en: '[PRE15]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.14: Uppercase letters converted to lowercase'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.14：将大写字母转换为小写字母'
- en: '](img/B15923_07_14.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_14.jpg)'
- en: 'Figure 7.14: Uppercase letters converted to lowercase'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.14：将大写字母转换为小写字母
- en: 'Remove the `''URL''` string that was added as a placeholder in *Step 4*. The
    previously added `''URL''` string is not actually needed for modeling. If it seems
    harmless to leave it in, consider that the `''URL''` string could appear naturally
    in a headline and we do not want to artificially boost its number of appearances.
    Also, the `''URL''` string does not appear in every headline, so by leaving it
    in, we could be unintentionally creating a connection between the `''URL''` strings
    and a topic:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除在 *步骤 4* 中添加的 `'URL'` 字符串作为占位符。先前添加的 `'URL'` 字符串实际上在建模中并不需要。如果它似乎无害，留着它也无妨，但要考虑到
    `'URL'` 字符串可能自然出现在标题中，我们不希望人为地增加它的出现频率。此外，`'URL'` 字符串并非出现在每个标题中，因此，留下它可能会无意间在
    `'URL'` 字符串和某些主题之间建立联系：
- en: '[PRE16]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.15: String URL removed'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.15：字符串 URL 已移除'
- en: '](img/B15923_07_15.jpg)'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_15.jpg)'
- en: 'Figure 7.15: String URL removed'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.15：字符串 URL 已移除
- en: 'Load in the `stopwords` dictionary from `nltk` and print it:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `nltk` 加载 `stopwords` 字典并打印出来：
- en: '[PRE17]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.16: List of stop words'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.16：停用词列表'
- en: '](img/B15923_07_16.jpg)'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_16.jpg)'
- en: 'Figure 7.16: List of stop words'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.16：停用词列表
- en: Before using the dictionary, it is important to reformat the words to match
    the formatting of our headlines. That involves confirming that everything is lowercase
    and without punctuation.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在使用字典之前，重要的是要重新格式化单词，使其与我们标题的格式匹配。这包括确认所有内容都是小写且没有标点符号。
- en: 'Now that we have correctly formatted the `stopwords` dictionary, use it to
    remove all stop words from the headline:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经正确格式化了 `stopwords` 字典，使用它从标题中移除所有停用词：
- en: '[PRE18]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.17: Stop words removed from the headline'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.17：从标题中移除停用词'
- en: '](img/B15923_07_17.jpg)'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_17.jpg)'
- en: 'Figure 7.17: Stop words removed from the headline'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.17：从标题中移除停用词
- en: 'Perform lemmatization by defining a function that can be applied to each headline
    individually. Lemmatizing requires the `wordnet` dictionary to be loaded. The
    `morphy` function takes each individual word in a text and returns its standard
    form if it recognizes it. For example, if the word input is *running* or *ran*,
    the `morphy` function would return *run*:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过定义一个可以应用于每个标题的函数来执行词形还原。词形还原需要加载 `wordnet` 字典。`morphy` 函数会处理文本中的每个单词，并返回其标准形式（如果识别到的话）。例如，如果输入的单词是
    *running* 或 *ran*，`morphy` 函数将返回 *run*：
- en: '[PRE19]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.18: Output after performing lemmatization'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.18：执行词形还原后的输出'
- en: '](img/B15923_07_18.jpg)'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_18.jpg)'
- en: 'Figure 7.18: Output after performing lemmatization'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.18：执行词形还原后的输出
- en: Remove all words with a length of four or less from the list of tokens. The
    assumption around this step is that short words are, in general, more common and
    therefore will not drive the types of insights we are looking to extract from
    the topic models. Note that removing words of certain lengths is not a technique
    that should be used all the time; it is for specific cases only. For example,
    short words can sometimes be very indicative of topics such as in the case of
    identifying animals (for example, dog, cat, bird).
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从词元列表中移除所有长度为四个字符或更少的单词。这个步骤的假设是，短单词通常更为常见，因此不会为我们从主题模型中提取的洞察提供帮助。请注意，移除某些长度的单词并不是一种适用于所有情况的技巧；它仅适用于特定情况。例如，短单词有时可能非常指示某些主题，如识别动物（例如：dog，cat，bird）。
- en: '[PRE20]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.19: Headline number six post-cleaning'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.19：第六个标题清理后的结果'
- en: '](img/B15923_07_19.jpg)'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_19.jpg)'
- en: 'Figure 7.19: Headline number six post-cleaning'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19：第六个标题清理后的结果
- en: Now that we have worked through the cleaning and preprocessing steps individually
    on one headline, we need to apply those steps to every one of the nearly 100,000
    headlines. The most efficient way to do that is to write a function that contains
    all the steps outlined above and apply that function to every document in the
    corpus in some iterative fashion. That process is undertaken in the next exercise.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经逐一完成了清理和预处理步骤，接下来需要将这些步骤应用到接近 100,000 个标题上。最有效的方法是编写一个包含上述所有步骤的函数，并以某种迭代方式将该函数应用于语料库中的每个文档。这个过程将在下一个练习中进行。
- en: Note
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考 [https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3fbWQES](https://packt.live/3fbWQES)在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须执行整个Notebook才能获得所需的结果。
- en: 'Exercise 7.04: Complete Data Cleaning'
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习7.04：完整数据清洗
- en: 'In this exercise, we will consolidate *Steps 2* to *12* from *Exercise 7.03*,
    *Cleaning Data Step by Step*, into one function that we can apply to every headline.
    The function will take one headline in string format as an input and the output
    will be a cleaned headline as a list of tokens. The topic models require that
    documents be formatted as strings instead of as lists of tokens, so in *Step 4*,
    the lists of tokens are converted back into strings:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将把*步骤2*到*步骤12*从*练习7.03*《逐步清洗数据》整合为一个函数，应用于每个标题。该函数将以字符串格式的标题作为输入，输出将是一个清洗后的标题列表（tokens）。主题模型要求文档格式为字符串，而不是tokens的列表，因此在*步骤4*中，tokens列表将被转换回字符串：
- en: 'Define a function that contains all the individual steps of the cleaning process
    from *Exercise 7.03*, *Cleaning Data Step by step*:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，包含*练习7.03*《逐步清洗数据》中的所有独立步骤：
- en: '[PRE21]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Execute the function on each headline. The `map` function in Python is a nice
    way to apply a user-defined function to each element of a list. Convert the `map`
    object to a list and assign it to the `clean` variable. The `clean` variable is
    a list of lists:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个标题上执行该函数。Python中的`map`函数是一种很好的方式，可以将用户定义的函数应用于列表中的每个元素。将`map`对象转换为列表，并将其分配给`clean`变量。`clean`变量是一个列表的列表：
- en: '[PRE22]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In `do_headline_cleaning`, `None` is returned if the language of the headline
    is detected as being any language other than English. The elements of the final
    cleaned list should only be lists, not `None`, so remove all `None` types. Use
    `print` to display the first five cleaned headlines and the length of the `clean`
    variable:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`do_headline_cleaning`中，如果检测到标题的语言不是英语，则返回`None`。最终清洗后的列表中的元素应仅为列表，而非`None`，因此需要去除所有`None`类型。使用`print`显示前五个清洗后的标题以及`clean`变量的长度：
- en: '[PRE23]'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.20: Example headlines and the length of the headline list'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.20：示例标题及标题列表的长度'
- en: '](img/B15923_07_20.jpg)'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_20.jpg)'
- en: 'Figure 7.20: Example headlines and the length of the headline list'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.20：示例标题及标题列表的长度
- en: 'For every individual headline, concatenate the tokens using a white space separator.
    The headlines should now be an unstructured collection of words, nonsensical to
    the human reader, but ideal for topic modeling:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个单独的标题，使用空格分隔符连接tokens。现在这些标题将变成一个无结构的单词集合，对于人类阅读者来说没有意义，但对于主题建模来说是理想的：
- en: '[PRE24]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The cleaned headlines should resemble the following:'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 清洗后的标题应类似于以下内容：
- en: '![Figure 7.21: Headlines cleaned for modeling'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.21：为建模清洗后的标题'
- en: '](img/B15923_07_21.jpg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_21.jpg)'
- en: 'Figure 7.21: Headlines cleaned for modeling'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21：为建模清洗后的标题
- en: Note
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3fbWQES](https://packt.live/3fbWQES)在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须执行整个Notebook才能获得所需的结果。
- en: To recap, what the cleaning and preprocessing work effectively does is strip
    out the noise from the data so that the model can hone in on elements of the data
    that could actually drive insights. For example, words that are agnostic to any
    topic should not be informing topics, but by accident alone, if left in, could
    be.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，清洗和预处理的工作实际上是剔除数据中的噪音，以便模型能够专注于数据中可能推动洞察的元素。例如，任何与主题无关的词语不应影响主题，但如果不小心留下这些词，它们可能会干扰。
- en: In an effort to avoid what we could call *fake signal*, we remove those words.
    Likewise, since topic models cannot discern context, punctuation is irrelevant
    and is therefore removed. Even if the model could find the topics without removing
    the noise from the data, the uncleaned data could have thousands to millions of
    extra words and random characters to parse (depending on the number of documents
    in the corpus), which could significantly increase the computational demands.
    So, data cleaning is an integral part of topic modeling. You will practice this
    in the following activity.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免我们可以称之为*假信号*的内容，我们会移除这些词语。同样，由于主题模型无法辨别上下文，标点符号是无关的，因此会被移除。即便模型可以在不清洗数据的情况下找到主题，未经清洗的数据可能包含成千上万甚至百万个多余的单词和随机字符（取决于语料库中的文档数量），这可能显著增加计算需求。因此，数据清洗是主题建模的一个重要部分。你将在接下来的活动中练习这个过程。
- en: 'Activity 7.01: Loading and Cleaning Twitter Data'
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动7.01：加载和清洗Twitter数据
- en: In this activity, we will load and clean Twitter data for modeling to be done
    in subsequent activities. Our usage of the headline data is ongoing, so let's
    complete this activity in a separate Jupyter notebook, but with all the same requirements
    and imported libraries.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将加载并清洗Twitter数据，以便在后续活动中进行建模。我们对头条数据的使用是持续进行的，因此让我们在一个单独的Jupyter笔记本中完成此活动，但所有的要求和导入的库保持一致。
- en: The goal is to take the raw tweet data, clean it, and produce the same output
    that we did in *Step 4* of the previous exercise. The output should be a list
    whose length is similar to the number of rows in the raw data file, but potentially
    not equal to it. This is because tweets can get dropped in the cleaning process
    for many reasons, such as the tweet being written in a language other than English.
    Each element of the list should represent one tweet and should contain just the
    words in the tweet that might be relevant to topic formation.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是处理原始推文数据，清洗它，并生成与前一个练习中*第4步*相同的输出。输出应该是一个列表，其长度应该与原始数据文件中的行数相似，但可能不完全相同。这是因为推文在清洗过程中可能会被丢弃，原因可能有很多，比如推文使用了非英语语言。列表中的每个元素应该代表一条推文，并且只包含可能与主题形成相关的推文内容。
- en: 'Here are the steps to complete the activity:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成此活动的步骤：
- en: Import the necessary libraries.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Load the LA Times health Twitter data (`latimeshealth.txt`) from [https://packt.live/2Xje5xF](https://packt.live/2Xje5xF).
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从[https://packt.live/2Xje5xF](https://packt.live/2Xje5xF)加载LA Times健康Twitter数据(`latimeshealth.txt`)。
- en: Note
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'This dataset is sourced from [https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter](https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter)
    (UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science).'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 数据集来源于[https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter](https://archive.ics.uci.edu/ml/datasets/Health+News+in+Twitter)（UCI机器学习库[[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚大学欧文分校信息与计算机科学学院）。
- en: 'Citation: Karami, A., Gangopadhyay, A., Zhou, B., & Kharrazi, H. (2017). Fuzzy
    approach topic discovery in health and medical corpora. International Journal
    of Fuzzy Systems, 1-12.'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 引用：Karami, A., Gangopadhyay, A., Zhou, B., & Kharrazi, H.（2017）。健康和医学语料库中的模糊方法主题发现。《国际模糊系统杂志》，1-12。
- en: It is also available on GitHub at [https://packt.live/2Xje5xF](https://packt.live/2Xje5xF).
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它也可以在GitHub上找到，[https://packt.live/2Xje5xF](https://packt.live/2Xje5xF)。
- en: Run a quick exploratory analysis to ascertain data size and structure.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进行快速的探索性分析，确定数据的大小和结构。
- en: Extract the tweet text and convert it to a list object.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取推文文本并将其转换为列表对象。
- en: Write a function to perform language detection and tokenization on white spaces,
    and then replace the screen names and URLs with `SCREENNAME` and `URL`, respectively.
    The function should also remove punctuation, numbers, and the `SCREENNAME` and
    `URL` replacements. Convert everything to lowercase, except `SCREENNAME` and `URL`.
    It should remove all stop words, perform lemmatization, and keep words with five
    or more letters only.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写一个函数，执行语言检测和基于空格的分词，然后分别用`SCREENNAME`和`URL`替换屏幕名称和网址。该函数还应移除标点符号、数字以及`SCREENNAME`和`URL`的替换内容。将所有内容转换为小写字母，除了`SCREENNAME`和`URL`。它应移除所有停用词，执行词形还原，并且只保留长度为五个字母或以上的单词。
- en: Apply the function defined in *Step 5* to every tweet.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*第5步*中定义的函数应用于每一条推文。
- en: Remove elements of the output list equal to `None`.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除输出列表中值为`None`的元素。
- en: Turn the elements of each tweet back into a string. Concatenate using white
    space.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每条推文的元素重新转换为字符串。使用空格进行连接。
- en: Keep the notebook open for future activities.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持笔记本打开，以便进行未来的活动。
- en: Note
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: All the activities in this chapter need to be performed in the same notebook.
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本章中的所有活动需要在同一个笔记本中执行。
- en: 'The output will be as follows:'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.22: Tweets cleaned for modeling'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.22：已清洗的推文，用于建模'
- en: '](img/B15923_07_22.jpg)'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_22.jpg)'
- en: 'Figure 7.22: Tweets cleaned for modeling'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22：已清洗的推文，用于建模
- en: Note
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 478.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第478页找到。
- en: Latent Dirichlet Allocation
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 潜在狄利克雷分配（Latent Dirichlet Allocation）
- en: In 2003, David Blei, Andrew Ng, and Michael Jordan published their article on
    the topic modeling algorithm known as **Latent Dirichlet Allocation** (**LDA**).
    LDA is a generative probabilistic model. This means that the modeling process
    starts with the text and works backward through the process that is assumed to
    have generated it in order to identify the parameters of interest. In this case,
    it is the topics that generated the data that are of interest. The process discussed
    here is the most basic form of LDA, but for learning, it is also the most comprehensible.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 2003年，David Blei、Andrew Ng 和 Michael Jordan 发表了他们关于主题建模算法**潜在狄利克雷分配**（**LDA**）的文章。LDA是一种生成概率模型。这意味着建模过程从文本开始，反向工作，通过假设生成它的过程，以识别感兴趣的参数。在这种情况下，感兴趣的是生成数据的主题。这里讨论的过程是LDA的最基本形式，但对于学习来说，它也是最容易理解的。
- en: There are M documents available for topic modeling within the corpus. Each document
    can be considered as the sequence of *N* words, i.e., a sequence (*w*1,*w*2… *w*N).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 语料库中有M个可用于主题建模的文档。每个文档可以视为*N*个单词的序列，即序列（*w*1，*w*2… *w*N）。
- en: 'For each document in the corpus, the assumed generative process is:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 对于语料库中的每个文档，假设的生成过程是：
- en: Select ![A picture containing tableware
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择![一个包含餐具的图片
- en: Description automatically generated](img/B15923_07_Formula_01.png), where *N*
    is the number of words and λ is the parameter controlling the Poisson distribution.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B15923_07_Formula_01.png)，其中*N*是单词的数量，λ是控制泊松分布的参数。
- en: Select ![A drawing of a face
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择![面部插图
- en: Description automatically generated](img/B15923_07_Formula_02.png), where ![
    distribution o](img/B15923_07_Formula_03.png) is the distribution of topics.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B15923_07_Formula_02.png)，其中![ 主题分布](img/B15923_07_Formula_03.png)是主题的分布。
- en: For each *N* words, *W*n, select topic ![A drawing of a face
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个*N*个单词，*W*n，选择主题![ 面部插图
- en: Description automatically generated](img/B15923_07_Formula_04.png), and select
    word *W*n from ![A picture containing clipart
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B15923_07_Formula_04.png)，并从中选择单词*W*n，来自![一个包含剪贴画的图片
- en: Description automatically generated](img/B15923_07_Formula_05.png).
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B15923_07_Formula_05.png)。
- en: Let's go through the generative process in a bit more detail. The preceding
    three steps repeat for every document in the corpus. The initial step is to choose
    the number of words in the document by sampling from, in most cases, the *Poisson*
    distribution. It is important to note that, because N is independent of the other
    variables, the randomness associated with its generation is mostly ignored in
    the derivation of the algorithm.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一下生成过程。前面提到的三个步骤会对语料库中的每个文档重复。初始步骤是通过从大多数情况下的*泊松*分布中采样来选择文档中的单词数。需要注意的是，由于N与其他变量是独立的，因此与其生成相关的随机性在算法推导中大多被忽略。
- en: 'Coming after the selection of *N* is the generation of the topic mixture or
    distribution of topics, unique to each document. Think of this as a per-document
    list of topics with probabilities representing the amount of the document represented
    by each topic. Consider three topics: A, B, and C. An example document could be
    100% topic A, 75% topic B and 25% topic C, or an infinite number of other combinations.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择*N*之后，接下来是生成主题混合或主题分布，这对每个文档来说是独特的。可以将其视为每个文档的主题列表，概率表示每个主题所代表的文档部分。考虑三个主题：A、B
    和 C。一个示例文档可能是100%的主题A，75%的主题B和25%的主题C，或者是无数其他的组合。
- en: Lastly, the specific words in the document are selected via a probability statement
    conditioned on the selected topic and the distribution of words for that topic.
    Note that documents are not really generated in this way, but it is a reasonable
    proxy.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，文档中的特定单词是通过概率语句从所选主题及该主题的单词分布中选择的。请注意，文档并不真正以这种方式生成，但它是一个合理的代理方法。
- en: This process can be thought of as a distribution over distributions. A document
    is selected from the collection (distribution) of documents, and one topic is
    selected (via the multinomial distribution) from the probability distribution
    of topics for that document, generated by the Dirichlet distribution.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程可以被看作是一个分布上的分布。一个文档从文档集合（分布）中选择出来，然后从该文档的主题概率分布中选择一个主题（通过多项式分布），该分布由Dirichlet分布生成。
- en: '![Figure 7.23: Graphical representation of LDA'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.23：LDA的图形表示'
- en: '](img/B15923_07_23.jpg)'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_23.jpg)'
- en: 'Figure 7.23: Graphical representation of LDA'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23：LDA的图形表示
- en: The most straightforward way to build the formula representing the LDA solution
    is through a graphical representation. This particular representation is referred
    to as a plate notation graphical model, as it uses plates to represent the two
    iterative steps in the process.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 构建表示LDA解法的公式最直接的方法是通过图形表示。这个特定的表示方法被称为板符号图形模型，因为它使用板块来表示过程中的两个迭代步骤。
- en: You will recall that the generative process was executed for every document
    in the corpus, so the outermost plate (labeled *M*) represents iterating over
    each document. Similarly, the iteration over words in *Step 3* is represented
    by the innermost plate of the diagram, labeled *N*.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得生成过程是针对语料库中的每个文档执行的，因此最外层的板块（标记为*M*）表示对每个文档的迭代。类似地，*步骤3*中对词汇的迭代通过图中的最内层板块表示，标记为*N*。
- en: 'The circles represent the parameters, distributions, and results. The shaded
    circle, labeled *w*, is the selected word, which is the only known piece of data
    and, as such, is used to reverse-engineer the generative process. Besides *w*,
    the other four variables in the diagram are defined as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 圆圈代表参数、分布和结果。阴影部分的圆圈，标记为*w*，是选定的词汇，这是唯一已知的数据，因此用于反向推导生成过程。除了*w*，图中的其他四个变量定义如下：
- en: '![A picture containing scissors, tool'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![包含剪刀、工具的图片'
- en: 'Description automatically generated](img/B15923_07_Formula_06.png): Hyperparameter
    for the topic-document Dirichlet distribution.'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_06.png)：主题-文档Dirichlet分布的超参数。
- en: '![A picture containing furniture'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![包含家具的图片'
- en: 'Description automatically generated](img/B15923_07_Formula_07.png): Distribution
    of words for each topic.'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_07.png)：每个主题的词汇分布。
- en: '![Description automatically generated](img/B15923_07_Formula_08.png): This
    is the latent variable for the topic.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![描述自动生成](img/B15923_07_Formula_08.png)：这是主题的潜在变量。'
- en: '![ This is the latent variable for the ](img/B15923_07_Formula_09.png): This
    is the latent variable for the distribution of topics for each document.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![这是潜在变量](img/B15923_07_Formula_09.png)：这是每个文档主题分布的潜在变量。'
- en: '![A picture containing scissors, tool'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '![包含剪刀、工具的图片'
- en: Description automatically generated](img/B15923_07_Formula_10.png) and ![A picture
    containing furniture
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_10.png) 和![包含家具的图片
- en: Description automatically generated](img/B15923_07_Formula_11.png) control the
    frequency of topics in documents and the frequency of words in topics. If ![A
    picture containing scissors, tool
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_11.png) 控制文档中主题的频率和主题中词汇的频率。如果![包含剪刀、工具的图片
- en: Description automatically generated](img/B15923_07_Formula_12.png) increases,
    the documents become increasingly similar as the number of topics in each document
    increases. On the other hand, if ![A picture containing scissors, tool
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_12.png) 增加时，文档变得越来越相似，因为每个文档中的主题数量增加。另一方面，如果![包含剪刀、工具的图片
- en: Description automatically generated](img/B15923_07_Formula_13.png) decreases,
    the documents become increasingly dissimilar as the number of topics in each document
    decreases. The ![A picture containing furniture
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_13.png) 减少时，文档之间的相似度逐渐降低，因为每个文档中的主题数量减少。 ![包含家具的图片
- en: Description automatically generated](img/B15923_07_Formula_14.png) parameter
    behaves similarly. If ![A picture containing furniture
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_14.png) 参数表现类似。如果![包含家具的图片
- en: Description automatically generated](img/B15923_07_Formula_15.png) increases,
    more words from the document are used to model a topic while a lower ![A picture
    containing furniture
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/B15923_07_Formula_15.png) 增加时，文档中使用的词汇更多，用来建模一个主题，而较低的![包含家具的图片
- en: Description automatically generated](img/B15923_07_Formula_16.png) causes a
    smaller number of words to be used for a topic. Given the complexity of the distributions
    in LDA, there is no direct solution, so some sort of approximation algorithm is
    required to generate the results. The standard approximation algorithm for LDA
    is discussed in the next section.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 自动生成的描述](img/B15923_07_Formula_16.png)导致每个主题所使用的词汇数量较少。鉴于 LDA 中分布的复杂性，没有直接的解决方案，因此需要某种近似算法来生成结果。LDA
    的标准近似算法将在下一节中讨论。
- en: Variational Inference
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变分推断
- en: The big issue with LDA is that the evaluation of the conditional probabilities
    (the distributions) is unmanageable, so instead of computing them directly, the
    probabilities are approximated. Variational inference is one of the simpler approximation
    algorithms, but it has an extensive derivation that requires significant knowledge
    of probability. In order to spend more time on the application of LDA, this section
    will give some high-level details on how variational inference is applied in this
    context but will not fully explore the algorithm.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 的一个主要问题是条件概率（分布）的评估难以管理，因此，概率不是直接计算，而是通过近似来得到。变分推断是其中一种较为简单的近似算法，但它有一个广泛的推导过程，需要对概率有深入的理解。为了更多地关注
    LDA 的应用，本节将简要介绍变分推断在该背景下的应用，但不会深入探讨该算法。
- en: Let's take a moment to work through the variational inference algorithm intuitively.
    Start by randomly assigning each word in each document in the corpus to one of
    the topics. Then, for each document and each word in each document separately,
    calculate two proportions. Those proportions would be the proportion of words
    in the document that are currently assigned to the topic, *P(Topic|Document)*
    and the proportion of assignments across all documents of a specific word to the
    topic, *P(Word|Topic)*. Multiply the two proportions and use the resulting proportion
    to assign the word to a new topic. Repeat this process until a steady state is
    reached where topic assignments are not changing significantly. These assignments
    are then used to estimate the within-document topic mixture and the within-topic
    word mixture.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间直观地理解变分推断算法。从随机地将语料库中每篇文档中的每个单词分配到一个主题开始。然后，分别为每个文档和每个文档中的每个单词计算两个比例。这些比例分别是当前分配给该主题的文档中单词的比例，*P(Topic|Document)*，以及特定单词在所有文档中分配到该主题的比例，*P(Word|Topic)*。将这两个比例相乘，使用得到的比例将该单词分配到一个新的主题。重复这个过程，直到达到一个稳定状态，在这个状态下，主题分配不会发生显著变化。然后，使用这些分配来估计文档内部的主题混合和主题内的单词混合。
- en: '![Figure 7.24: The variational inference process'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.24：变分推断过程'
- en: '](img/B15923_07_24.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_24.jpg)'
- en: 'Figure 7.24: The variational inference process'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24：变分推断过程
- en: The thought process behind variational inference is that, if the actual distribution
    is intractable, then a simpler distribution, let's call it the variational distribution,
    very close to true distribution, which is tractable, should be found so that inference
    becomes possible. In other words, since inferring the actual distribution is impossible
    due to the complexity of the actual distribution, we try instead to find a simpler
    distribution that is an excellent approximation of the actual distribution.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 变分推断的思路是，如果实际分布是不可处理的，那么应找到一个更简单的分布，称为变分分布，它非常接近真实分布且是可处理的，从而使得推断成为可能。换句话说，由于由于实际分布的复杂性，推断实际分布是不可能的，我们试图找到一个更简单的分布，它能够很好地近似实际分布。
- en: Let's take a momentary break from the theory for an example. Variational inference
    is like trying to view animals at a crowded zoo. The animals at the zoo are in
    an enclosed habitat, which, in this example, is the posterior distribution. Visitors
    cannot actually get into the habitat, so the visitors have to settle for viewing
    the habitat from the closest possible position, which is the posterior approximation
    (i.e. the best approximation of the habitat). If there are a lot of people at
    the zoo, it can be difficult to get to that optimal vantage point. People generally
    start at the back of the crowd and strategically move their way toward that optimal
    vantage point. The path the visitors follow to move from the back of the crowd
    to the optimal vantage point is the optimization path. Variational inference is
    simply the process of getting as close to the desired point as possible knowing
    that the desired point cannot actually be reached.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时从理论中休息一下，来看一个例子。变分推断就像是在拥挤的动物园中观察动物。动物园里的动物处于一个封闭的栖息地，在这个例子中，栖息地就是后验分布。游客无法实际进入栖息地，因此他们必须尽可能靠近栖息地观察，这就是后验近似（即栖息地的最佳近似）。如果动物园里有很多人，可能很难找到那个最佳的观察点。人们通常从人群的后面开始，逐步朝着最佳观察点移动。游客从人群后面移动到最佳观察点的路径就是优化路径。变分推断实际上就是在知道无法真正到达期望点的情况下，尽可能接近期望点的过程。
- en: 'To start, select a family of distributions (i.e. binomial, gaussian, exponential,
    and so on), *q*, conditioned on new variational parameters. The parameters are
    optimized so that the original distribution, which is actually the posterior distribution
    for those people familiar with Bayesian statistics, and the variational distribution
    are as close as possible. The variational distribution will be close enough to
    the original posterior distribution to be used as a proxy, making any inference
    done on it applicable to the original posterior distribution. The generic formula
    for the family of distributions, *q*, is as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择一个分布族（即二项分布、高斯分布、指数分布等），*q*，并根据新的变分参数进行条件化。这些参数经过优化，使得原始分布（实际上是后验分布，对于熟悉贝叶斯统计的人来说）和变分分布尽可能接近。变分分布会足够接近原始的后验分布，因此可以作为代理，基于它进行的任何推断都适用于原始后验分布。分布族
    *q* 的通用公式如下：
- en: '![Figure 7.25: Formula for the family of distributions, q'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.25：分布族的公式，q'
- en: '](img/B15923_07_25.jpg)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_25.jpg)'
- en: 'Figure 7.25: Formula for the family of distributions, q'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25：分布族的公式，q
- en: There is a large collection of potential variational distributions that can
    be used as an approximation for the posterior distribution. An initial variational
    distribution is selected from the collection, which acts as the starting point
    for an optimization process that iteratively moves closer and closer to the optimal
    distribution. The optimal parameters are the parameters of the distribution that
    best approximate the posterior. The similarity of the two distributions is measured
    using **Kullback-Leibler** (**KL**) divergence. KL divergence represents the expected
    amount of error generated if we approximate one distribution with another. The
    distribution with optimal parameters will have the smallest KL divergence when
    measured against the true distribution.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 有一大堆潜在的变分分布可以用作后验分布的近似。从这些分布中选择一个初始的变分分布，作为优化过程的起点，该过程会不断接近最佳分布。最佳参数是指最能近似后验分布的分布参数。使用**Kullback-Leibler**（**KL**）散度来衡量这两个分布的相似性。KL
    散度表示如果我们用一个分布来近似另一个分布时，所产生的预期误差量。具有最佳参数的分布将具有最小的 KL 散度，并且与真实分布相比。
- en: Once the optimal distribution has been identified, which means the optimal parameters
    have been identified, it can be leveraged to produce the output matrices and execute
    any required inference.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了最佳分布，也就意味着确定了最佳参数，可以利用它来生成输出矩阵并执行任何需要的推断。
- en: Bag of Words
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 词袋模型
- en: Text cannot be passed directly into any machine learning algorithm; it first
    needs to be encoded numerically. A straightforward way of working with text in
    machine learning is via a bag-of-words model, which removes all information regarding
    the order of the words and focuses strictly on the degree of presence (meaning
    the count or frequency) of each word.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 文本不能直接传递给任何机器学习算法；它首先需要被数值编码。在机器学习中处理文本的一个直接方法是使用词袋模型，它移除了关于单词顺序的所有信息，专注于每个单词的出现程度（即计数或频率）。
- en: The Python `sklearn` library can be leveraged to transform the cleaned vector
    created in the previous exercise into the structure that the LDA model requires.
    Since LDA is a probabilistic model, we do not want to do any scaling or weighting
    of the word occurrences; instead, we opt to input just the raw counts.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`sklearn`库可以用来将前一个练习中创建的清洗后的向量转换为LDA模型所需的结构。由于LDA是一个概率模型，我们不希望对单词出现频率进行任何缩放或加权；相反，我们选择仅输入原始计数。
- en: The input to the bag-of-words model will be the list of cleaned strings that
    were returned from *Exercise 7.04*, *Complete Data Cleaning*. The output will
    be the document number, the word as its numeric encoding, and a count of the number
    of times that word appears in that document. These three items will be presented
    as a tuple and an integer.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 词袋模型的输入将是*练习 7.04*中返回的清洗字符串列表，即*完整数据清理*。输出将是文档编号、单词的数值编码以及该单词在文档中出现的次数。这三个项目将以元组和整数的形式呈现。
- en: The tuple will be something like (0, 325), where 0 is the document number and
    325 is the numerically encoded word. Note that 325 will be the encoding of that
    word across all documents. The integer would then be the count. The bag-of-words
    models we will be running in this chapter are from `sklearn` and are called `CountVectorizer`
    and `TfIdfVectorizer`. The first model returns the raw counts and the second returns
    a scaled value, which we will discuss a bit later.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 元组将类似于(0, 325)，其中0是文档编号，325是数值编码的单词。请注意，325将是该单词在所有文档中的编码。整数部分将是计数。我们将在本章中运行的词袋模型来自`sklearn`，分别称为`CountVectorizer`和`TfIdfVectorizer`。第一个模型返回原始计数，第二个返回一个缩放值，我们稍后将讨论这一点。
- en: A critical note is that the results of both topic models being covered in this
    chapter can vary from run to run, even when the data is the same, because of randomness.
    Neither the probabilities in LDA nor the optimization algorithms are deterministic,
    so do not be surprised if your results differ slightly from the results shown
    from here on out. In the next exercise, we will run the count vectorizer to numerically
    encode our documents, so that we can continue on to topic modeling using LDA.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的注意事项是，本章涵盖的两种主题模型的结果可能会因运行而异，即使数据相同，这也是由于随机性所导致。LDA中的概率和优化算法都不是确定性的，因此不要惊讶于你的结果与接下来展示的结果有所不同。在下一个练习中，我们将运行计数向量化器，以数值方式编码我们的文档，以便能够继续使用LDA进行主题建模。
- en: 'Exercise 7.05: Creating a Bag-of-Words Model Using the Count Vectorizer'
  id: totrans-308
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.05：使用计数向量化器创建词袋模型
- en: 'In this exercise, we will run the `CountVectorizer` in `sklearn` to convert
    our previously created cleaned vector of headlines into a bag-of-words data structure.
    In addition, we will define some variables that will be used throughout the modeling
    process:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将运行`sklearn`中的`CountVectorizer`，将之前创建的清洗后的标题向量转换为词袋数据结构。此外，我们还将定义一些将在建模过程中使用的变量：
- en: 'Define `number_words`, `number_docs`, and `number_features`. The first two
    variables control the visualization of the LDA results. The `number_features`
    variable controls the number of words that will be kept in the feature space:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`number_words`、`number_docs`和`number_features`。前两个变量控制LDA结果的可视化。`number_features`变量控制将在特征空间中保留的词汇数量：
- en: '[PRE25]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Run the count vectorizer and print the output. There are three crucial inputs,
    which are `max_df`, `min_df`, and `max_features`. These parameters further filter
    the number of words in the corpus down to those that will most likely influence
    the model.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行计数向量化器并打印输出。这里有三个关键输入参数：`max_df`、`min_df`和`max_features`。这些参数进一步筛选出语料库中最可能影响模型的词汇。
- en: Words that only appear in a small number of documents are too rare to be attributable
    to any topic, so `min_df` is used to throw away words that appear in fewer than
    the specified number of documents. Words that appear in too many documents are
    not specific enough to be linked to specific topics, so `max_df` is used to throw
    away words that appear in more than the specified percentage of documents.
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在少数文档中出现的词汇太过稀有，无法归因于任何特定主题，因此使用`min_df`来丢弃在指定文档数量以下出现的词汇。出现在过多文档中的词汇不够具体，无法与特定主题相关联，因此使用`max_df`来丢弃在超过指定百分比文档中出现的词汇。
- en: 'Lastly, we do not want to overfit the model, so the number of words used to
    fit the model is limited to the most frequently occurring specified number (`max_features`)
    of words:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，我们不希望模型出现过拟合，因此用于拟合模型的词汇数量被限制为最频繁出现的指定数量（`max_features`）的词汇：
- en: '[PRE26]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.26: The bag-of-words data structure'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.26：词袋数据结构'
- en: '](img/B15923_07_26.jpg)'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_26.jpg)'
- en: 'Figure 7.26: The bag-of-words data structure'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.26：词袋数据结构
- en: 'Extract the feature names and the words from the vectorizer. The model is only
    fed the numerical encodings of the words, so having the feature names vector merge
    with the results will make interpretation easier:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从向量化器中提取特征名称和单词。模型只接收单词的数值编码，因此将特征名称向量与结果合并，将使得解释过程更加容易：
- en: '[PRE27]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This exercise involved the enumeration of the documents for use in the LDA model.
    The required format is a bag of words. That is, a bag-of-words model is simply
    a listing of all the words that appear in each document with a count of the number
    of times each word appears in each specific document. Having accomplished this
    task using `sklearn`, it is time to explore the process of evaluating LDA models.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习涉及文档的枚举，用于LDA模型。所需的格式是词袋模型。也就是说，词袋模型仅仅是列出每个文档中出现的所有词汇，并计算每个词在每个文档中出现的次数。通过使用`sklearn`完成这一任务后，接下来是探索LDA模型评估过程。
- en: Note
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在网上运行此示例，访问[https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个Notebook才能获得所需的结果。
- en: Perplexity
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 困惑度
- en: Models generally have metrics that can be leveraged to evaluate their performance.
    Topic models are no different, although performance, in this case, has a slightly
    different definition. In regression and classification, predicted values can be
    compared to actual values from which clear measures of performance can be calculated.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通常具有可用于评估其性能的指标。主题模型也不例外，尽管在这种情况下，性能的定义稍有不同。在回归和分类中，预测值可以与实际值进行比较，从中可以计算出明确的性能度量。
- en: With topic models, prediction is less reliable, because the model only knows
    the words it was trained on and new documents may not contain any of those words,
    despite featuring the same topics. Due to that difference, topic models are evaluated
    using a metric specific to language models, called **perplexity**.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 对于主题模型，预测的可靠性较低，因为模型仅了解它所训练过的词汇，而新文档可能并未包含这些词汇，尽管它们可能包含相同的主题。由于这一差异，主题模型的评估使用了专门针对语言模型的度量指标，称为**困惑度**。
- en: 'Perplexity, abbreviated to PP, measures the number of different equally most
    probable words that can follow any given word on average. Let''s consider two
    words as an example: *the* and *announce*. The word *the* can preface an enormous
    number of equally most probable words, while the number of equally most probable
    words that can follow the word *announce* is significantly less—albeit still a
    large number.'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 困惑度（Perplexity，缩写为PP）衡量的是在任何给定词语后平均可以跟随的不同且同样最可能的词汇数量。我们以两个词为例：*the*和*announce*。词*the*可以引出大量同样最可能的词汇，而词*announce*后面可以跟随的同样最可能的词汇数量则明显较少——尽管仍然是一个很大的数字。
- en: 'The idea is that words that, on average, can be followed by a smaller number
    of equally most probable words are more specific and can be more tightly tied
    to topics. As such, lower perplexity scores imply better language models. Perplexity
    is very similar to entropy, but perplexity is typically used because it is easier
    to interpret. As we will see momentarily, it can be used to select the optimal
    number of topics. With *m* being the number of words in the sequence of words,
    perplexity is defined as:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 其思想是，平均而言，后面能够跟随更少数量的同样最可能出现的单词的单词，越具体，越能够紧密地与主题联系。因此，较低的困惑度分数意味着更好的语言模型。困惑度与熵非常相似，但通常使用困惑度，因为它更容易解释。正如我们稍后将看到的，它可以用于选择最佳的主题数量。假设*m*是单词序列中的单词数，困惑度定义为：
- en: '![Figure 7.27: Formula of perplexity'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.27：困惑度公式](img/B15923_07_27.jpg)'
- en: '](img/B15923_07_27.jpg)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_27.jpg)'
- en: 'Figure 7.27: Formula of perplexity'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.27：困惑度公式
- en: In this formula, *w*1*, …, w*m are the words making up some document in the
    test dataset. The joint probability of those words, *P(w*1*, …, w*m*)*, is a measure
    of how well the test document fits in the existing model. Higher probabilities
    suggest stronger models. The probability is raised to the *-1/m* power to normalize
    the score by the number of words in each document and to make lower values more
    optimal. Both these changes increase the interpretability of the score. The perplexity
    score, like root mean squared error, is not very meaningful as a standalone metric.
    It tends to be used as a comparison metric. That is, several models are built
    for which perplexity scores are calculated and compared to identify the best model
    with which to move forward.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*w*1*，…，w*m 是构成测试数据集中某文档的单词。这些单词的联合概率，*P(w*1*，…，w*m)*，衡量了测试文档与现有模型的契合度。较高的概率意味着模型更强。概率会被提升到*-1/m*的幂，以根据每个文档中的单词数量对分数进行标准化，并使较低的值更优。两者的变化都增加了分数的可解释性。困惑度分数，类似于均方根误差，作为单独的指标意义不大。它通常作为一个比较指标使用。即，构建几个模型，计算它们的困惑度分数并进行比较，以确定最佳的模型，从而继续前进。
- en: As stated previously, LDA has two required inputs. The first is the documents
    themselves, and the second is the number of topics. Selecting an appropriate number
    of topics can be very tricky. One approach to finding the optimal number of topics
    is to search over several numbers of topics and select the number of topics that
    corresponds to the smallest perplexity score. In machine learning, this approach
    is referred to as grid search. In the next exercise, we will put grid search to
    work to find the optimal number of topics.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LDA有两个必需的输入。第一个是文档本身，第二个是主题数量。选择合适的主题数量可能非常棘手。找到最佳主题数量的一种方法是对多个主题数量进行搜索，并选择与最小困惑度分数对应的主题数量。在机器学习中，这种方法被称为网格搜索。接下来的练习中，我们将使用网格搜索来找到最佳主题数量。
- en: 'Exercise 7.06: Selecting the Number of Topics'
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.06：选择主题数量
- en: 'In this exercise, we use the perplexity scores for LDA models fit on varying
    numbers of topics to determine the number of topics with which to move forward.
    Keep in mind that the original dataset had the headlines sorted into four topics.
    Let''s see whether this approach returns four topics:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们使用适配不同主题数量的LDA模型的困惑度分数来确定应该继续使用的主题数量。请记住，原始数据集中的标题已经被分成了四个主题。让我们看看这种方法是否能得到四个主题：
- en: 'Define a function that fits an LDA model on various numbers of topics and computes
    the perplexity score. Return two items: a DataFrame that has the number of topics
    with its perplexity score and the number of topics with the minimum perplexity
    score as an integer:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，适配不同主题数量的LDA模型并计算困惑度分数。返回两个项：一个数据框，包含主题数量及其困惑度分数，和具有最小困惑度分数的主题数量，作为整数：
- en: '[PRE28]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Execute the function defined in *Step 1*. The `ntopics` input is a list of
    numbers of topics that can be of any length and contain any values. Print out
    the DataFrame:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行在*步骤 1*中定义的函数。`ntopics`输入是一个包含主题数量的数字列表，列表的长度和数值均可变。打印出数据框：
- en: '[PRE29]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.28: DataFrame containing the number of topics and perplexity score'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.28：包含主题数量和困惑度分数的数据框](img/B15923_07_28.jpg)'
- en: '](img/B15923_07_28.jpg)'
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_28.jpg)'
- en: 'Figure 7.28: DataFrame containing the number of topics and perplexity score'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.28：包含主题数量和困惑度分数的数据框
- en: 'Plot the perplexity scores as a function of the number of topics. This is just
    another way to view the results contained in the DataFrame from *Step 2*:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制困惑度分数作为主题数的函数。这只是查看 *步骤 2* 中 DataFrame 中结果的另一种方式：
- en: '[PRE30]'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The plot appears as follows:'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图结果如下：
- en: '![Figure 7.29: Line plot view of perplexity as a function of the number of
    topics'
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 7.29: 主题数与困惑度的线图视图'
- en: '](img/B15923_07_29.jpg)'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![B15923_07_29.jpg](img/B15923_07_29.jpg)'
- en: 'Figure 7.29: Line plot view of perplexity as a function of the number of topics'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29：主题数与困惑度的线图视图
- en: As the DataFrame and plot show, the optimal number of topics using perplexity
    is three. Having the number of topics set to four yielded the second-lowest perplexity.
    Thus, while the results did not exactly match the information contained in the
    original dataset, the results are close enough to engender confidence in the grid
    search approach to identify the optimal number of topics. There could be several
    reasons that the grid search returned three instead of four, which we will dig
    into in an upcoming exercise.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 DataFrame 和绘图所示，使用困惑度得出的最佳主题数为三。将主题数设为四产生了第二低的困惑度。因此，虽然结果与原始数据集中包含的信息并不完全匹配，但这些结果足以让我们对网格搜索方法识别最佳主题数感到满意。关于为何网格搜索返回三个而不是四个结果，我们将在即将进行的练习中深入探讨。
- en: Note
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 注
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参阅 [https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在 [https://packt.live/3fbWQES](https://packt.live/3fbWQES) 上线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须执行整个笔记本才能获得所需的结果。
- en: Now that we've selected the optimal number of topics, we will use that number
    of topics to build our official LDA model. That model will then be used to create
    visualizations and define the list of topics present in the corpus.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了最佳主题数，将使用该主题数构建我们的官方 LDA 模型。然后，该模型将用于创建可视化效果，并定义语料库中存在的主题列表。
- en: 'Exercise 7.07: Running LDA'
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.07：运行 LDA
- en: 'In this exercise, we''ll implement LDA and examine the results. LDA outputs
    two matrices. The first is the topic-document matrix and the second is the word-topic
    matrix. We will look at these matrices as returned from the model and as nicely
    formatted tables that are easier to digest:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实施 LDA 并检查结果。LDA 输出两个矩阵。第一个是主题-文档矩阵，第二个是词-主题矩阵。我们将查看这些矩阵，这些矩阵是模型返回的，并且格式化为更易于理解的表格：
- en: 'Fit an LDA model using the optimal number of topics found in *Exercise 7.06*,
    *Selecting the Number of Topics*:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用在 *练习 7.06*，*选择主题数* 中找到的最佳主题数拟合 LDA 模型：
- en: '[PRE31]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.30: The LDA model'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![Figure 7.30: LDA 模型'
- en: '](img/B15923_07_30.jpg)'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![B15923_07_30.jpg](img/B15923_07_30.jpg)'
- en: 'Figure 7.30: The LDA model'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.30：LDA 模型
- en: 'Output the topic-document matrix and its shape to confirm that it aligns with
    the number of topics and the number of documents. Each row of the matrix is the
    per-document distribution of topics:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出主题-文档矩阵及其形状，以确认其与主题数和文档数的对齐情况。矩阵的每一行是主题的文档分布：
- en: '[PRE32]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE33]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Output the word-topic matrix and its shape to confirm that it aligns with the
    number of features (words) specified in *Exercise 7.05*, *Creating a Bag-of-Words
    Model Using the Count Vectorizer*, and the number of topics input. Each row is
    basically the prevalence of assignments to that topic of each word. The prevalence
    score can be transformed into the per-topic distribution of words:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出词-主题矩阵及其形状，以确认其与 *练习 7.05*，*使用计数向量化器创建词袋模型* 中指定的特征数（词）和输入的主题数的对齐情况。每一行基本上是每个单词分配给该主题的流行度。流行度分数可以转换为每个主题的词分布：
- en: '[PRE34]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The output is as follows:'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE35]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Define a function that formats the two output matrices into easy-to-read tables:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，将两个输出矩阵格式化为易于阅读的表格：
- en: '[PRE36]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: The function may be tricky to navigate, so let's walk through it. Start by creating
    the *W* and *H* matrices, which includes converting the assignment counts of *W*
    into the per-topic distribution of words. Then, iterate over the topics. Inside
    each iteration, identify the top words and documents associated with each topic.
    Convert the results into two DataFrames.
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该函数可能有些难以操作，所以让我们一起逐步分析。首先创建*W*和*H*矩阵，包括将*W*的分配计数转换为每个主题的词汇分布。然后，遍历每个主题。在每次遍历中，识别与每个主题相关的前几个词汇和文档。最后，将结果转换为两个数据框。
- en: 'Execute the function defined in *Step 4*:'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行在*步骤4*中定义的函数：
- en: '[PRE37]'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Print out the word-topic DataFrame. It shows the top 10 words (by distribution
    value) that are associated with each topic. From this DataFrame, we can identify
    the abstract topics that the word groupings represent. More on abstract topics
    will follow:'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出词汇-主题数据框。它展示了与每个主题相关的前10个词汇（按分布值排序）。通过这个数据框，我们可以识别出词汇分组所代表的抽象主题。关于抽象主题的更多内容将在后续介绍：
- en: '[PRE38]'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.31: Word-topic table'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.31：词汇-主题表'
- en: '](img/B15923_07_31.jpg)'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_31.jpg)'
- en: 'Figure 7.31: Word-topic table'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.31：词汇-主题表
- en: 'Print out the topic-document DataFrame. This shows the 10 documents to which
    each topic is most closely related. The values are from the per-document distribution
    of topics:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出主题-文档数据框。这显示了与每个主题最密切相关的10篇文档。其值来自每篇文档的主题分布：
- en: '[PRE39]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.32: Topic-document table'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.32：主题-文档表'
- en: '](img/B15923_07_32.jpg)'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_32.jpg)'
- en: 'Figure 7.32: Topic-document table'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.32：主题-文档表
- en: The results of the word-topic DataFrame show that the abstract topics are Barack
    Obama, the economy, and Microsoft. What is interesting is that the word grouping
    describing the economy contains references to Palestine. All four topics specified
    in the original dataset are represented in the word-topic DataFrame output, but
    not in the fully distinct manner expected. We could be facing one of two problems.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇-主题数据框的结果显示，抽象主题包括巴拉克·奥巴马、经济和微软。有趣的是，描述经济的词汇分组中包含了对巴勒斯坦的提及。原始数据集中的四个主题都在词汇-主题数据框的输出中得到了体现，但并没有以预期的完全独立的方式展现出来。我们可能面临两种问题。
- en: First, the topic referencing both the economy and Palestine could be under-cooked,
    which means increasing the number of topics may fix the issue. The other potential
    problem is that LDA does not handle correlated topics well. In *Exercise 7.09*,
    *Trying Four Topics*, we will try expanding the number of topics, which will give
    us a better idea of why one of the word groupings is seemingly a mixture of topics.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，引用经济和巴勒斯坦的主题可能还不够成熟，这意味着增加主题的数量可能会解决这个问题。另一个潜在的问题是LDA在处理相关主题时效果不佳。在*练习7.09*中，*尝试四个主题*，我们将尝试扩展主题的数量，这将帮助我们更好地理解为什么其中一个词汇分组似乎是多个主题的混合。
- en: Note
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问[https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个Notebook才能得到预期的结果。
- en: Visualization
  id: totrans-398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化
- en: The output of LDA models in Python using `sklearn` can be difficult to interpret
    in raw form. As is the case in most modeling exercises, visualizations can be
    a great benefit when it comes to interpreting and communicating model results.
    One Python library, `pyLDAvis`, integrates directly with the `sklearn` model object
    to produce straightforward graphics. This visualization tool returns a histogram
    showing the words that are the most closely related to each topic and a biplot,
    frequently used in PCA, where each circle corresponds to a topic. From the biplot,
    we know how prevalent each topic is across the entire corpus, which is reflected
    by the area of the circle, and the similarity of the topics, which is reflected
    by the closeness of the circles.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`sklearn`的LDA模型在Python中的输出可能难以直接解读。与大多数建模工作一样，数据可视化在解读和传达模型结果时有很大帮助。一种Python库`pyLDAvis`直接与`sklearn`模型对象集成，生成直观的图形。这个可视化工具返回一个直方图，展示与每个主题最紧密相关的词汇，以及一个双变量图（PCA中常用），每个圆圈代表一个主题。通过双变量图，我们可以了解每个主题在整个语料库中的普遍性，这通过圆圈的面积来反映，以及主题之间的相似性，这通过圆圈的接近程度来体现。
- en: The ideal scenario is to have the circles spread throughout the plot and be
    of reasonable and consistent size. That is, we want the topics to be distinct
    and to appear uniformly across the corpus. In addition to the `pyLDAvis` graphics,
    we will leverage the t-SNE model, discussed in a prior chapter, to produce a two-dimensional
    representation of the topic-document matrix, a matrix where each row represents
    one document and each column represents the probability of that topic describing
    the document.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 理想的情况是图中的圆圈应均匀分布，且大小合理一致。也就是说，我们希望主题清晰区分，并在语料库中均匀分布。除了 `pyLDAvis` 图形外，我们还将利用前一章节讨论的
    t-SNE 模型，生成主题-文档矩阵的二维表示，这个矩阵的每一行表示一个文档，每一列表示该主题描述该文档的概率。
- en: Having completed the LDA model fitting, let's create some graphics to help us
    dig into the results.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成 LDA 模型拟合后，让我们创建一些图形，帮助我们深入理解结果。
- en: 'Exercise 7.08: Visualizing LDA'
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.08：可视化 LDA
- en: 'Visualization is a helpful tool for exploring the results of topic models.
    In this exercise, we will look at three different visualizations. Those visualizations
    are basic histograms and specialty visualizations using t-SNE and PCA:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是探索主题模型结果的有力工具。在本练习中，我们将观察三种不同的可视化方式。它们分别是基本的直方图和使用 t-SNE 及 PCA 的专业可视化：
- en: 'Run and display `pyLDAvis`. This plot is interactive. Clicking on each circle
    updates the histogram to show the top words related to that specific topic. The
    following is one view of this interactive plot:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行并显示 `pyLDAvis`。此图是交互式的。点击每个圆圈时，直方图会更新，显示与该特定主题相关的顶部词汇。以下是此交互式图的一种视图：
- en: '[PRE40]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The plot appears as follows:'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图结果如下：
- en: '![Figure 7.33: A histogram and biplot for the LDA model'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.33：LDA 模型的直方图和双变量图'
- en: '](img/B15923_07_33.jpg)'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_33.jpg)'
- en: 'Figure 7.33: A histogram and biplot for the LDA model'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.33：LDA 模型的直方图和双变量图
- en: 'Define a function that fits a t-SNE model and then plots the results. After
    defining it, the pieces of the function will be described in detail, so that the
    steps are clear:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个拟合 t-SNE 模型并绘制结果的函数。定义完该函数后，将详细描述函数的各个部分，以便步骤清晰：
- en: '[PRE41]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '**Step 1**: The function starts by filtering down the topic-document matrix
    using an input threshold value. There are tens of thousands of headlines, and
    any plot incorporating all the headlines is going to be difficult to read and
    therefore not helpful. So, this function only plots a document if one of the distribution
    values is greater than or equal to the input threshold value:'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**步骤 1**：该函数首先通过输入的阈值过滤主题-文档矩阵。由于有成千上万的标题，包含所有标题的图形会难以阅读，因此不具有帮助性。因此，只有当分布值大于或等于输入阈值时，函数才会绘制该文档：'
- en: '[PRE42]'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '**Step 2**: Once the data is filtered down, run t-SNE, where the number of
    components is two, so that we can plot the results in two dimensions:'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**步骤 2**：数据过滤完成后，运行 t-SNE，其中组件数量为 2，以便我们能够在二维中绘制结果：'
- en: '[PRE43]'
  id: totrans-415
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '**Step 3**: Create a vector with an indicator of which topic is most related
    to each document. This vector will be used to color-code the plot by topic:'
  id: totrans-416
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**步骤 3**：创建一个向量，用来标示每个文档最相关的主题。该向量将用于根据主题为绘图着色：'
- en: '[PRE44]'
  id: totrans-417
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Step 4**: To understand the distribution of topics across the corpus and
    the impact of threshold filtering, the function returns the length of the topic
    vector as well as the topics themselves with the number of documents to which
    that topic has the largest distribution value:'
  id: totrans-418
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**步骤 4**：为了了解主题在语料库中的分布以及阈值筛选的影响，该函数返回主题向量的长度，并给出每个主题及其分布值最大文档数：'
- en: '[PRE45]'
  id: totrans-419
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '**Step 5**: Create and return the plot:'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**步骤 5**：创建并返回绘图：'
- en: '[PRE46]'
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Execute the function:'
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行函数：
- en: '[PRE47]'
  id: totrans-423
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-424
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.34: t-SNE plot with metrics around the distribution of the topics
    across the corpus'
  id: totrans-425
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.34：t-SNE 绘图，展示了主题在语料库中的分布指标'
- en: '](img/B15923_07_34.jpg)'
  id: totrans-426
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_34.jpg)'
- en: 'Figure 7.34: t-SNE plot with metrics around the distribution of the topics
    across the corpus'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.34：t-SNE 绘图，展示了主题在语料库中的分布指标
- en: The visualizations show that the LDA model with three topics is producing good
    results overall. In the biplot, the circles are of a medium size, which suggests
    that the topics appear consistently across the corpus and the circles also have
    good spacing. The t-SNE plot shows clear clusters supporting the separation between
    the circles represented in the biplot. The only glaring issue, which was previously
    discussed, is that one of the topics has words that do not seem to belong to that
    topic.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化结果显示，使用三个主题的 LDA 模型整体产生了良好的结果。在双图中，圆圈的大小适中，表明这些主题在语料库中呈现一致性，且圆圈之间的间隔较好。t-SNE
    图显示出明显的聚类，支持双图中圆圈之间的分离。唯一的明显问题，之前已经讨论过，就是其中一个主题包含了似乎与该主题不太相关的词汇。
- en: Note
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅 [https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址：[https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 必须执行整个 Notebook 才能获得预期的结果。
- en: In the next exercise, let's rerun the LDA using four topics.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个练习中，让我们使用四个主题重新运行 LDA 模型。
- en: 'Exercise 7.09: Trying Four Topics'
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.09：尝试四个主题
- en: 'In this exercise, LDA is run with the number of topics set to four. The motivation
    for doing this is to try and solve what might be an under-cooked topic from the
    three-topic LDA model that has words related to both Palestine and the economy.
    We will run through the steps first and then explore the results at the end:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，LDA 模型的主题数量设置为四。这样做的动机是尝试解决三主题 LDA 模型中可能存在的一个问题，该主题包含与巴勒斯坦和经济相关的词汇。我们首先会执行这些步骤，然后在最后查看结果：
- en: 'Run an LDA model with the number of topics equal to four:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行一个主题数量为四的 LDA 模型：
- en: '[PRE48]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-438
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.35: The LDA model'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.35：LDA 模型'
- en: '](img/B15923_07_35.jpg)'
  id: totrans-440
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_35.jpg)'
- en: 'Figure 7.35: The LDA model'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.35：LDA 模型
- en: 'Execute the `get_topics` function defined earlier to produce the more readable
    word-topic and topic-document tables:'
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行之前定义的 `get_topics` 函数，生成更易读的词汇-主题和主题-文档表：
- en: '[PRE49]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Print the word-topic table:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印词汇-主题表：
- en: '[PRE50]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-446
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.36: The word-topic table using the four-topic LDA model'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.36：使用四主题 LDA 模型的词汇-主题表'
- en: '](img/B15923_07_36.jpg)'
  id: totrans-448
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_36.jpg)'
- en: 'Figure 7.36: The word-topic table using the four-topic LDA model'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.36：使用四主题 LDA 模型的词汇-主题表
- en: 'Print the document-topic table:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印文档-主题表：
- en: '[PRE51]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The output is as follows:'
  id: totrans-452
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.37: The document-topic table using the four-topic LDA model'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.37：使用四主题 LDA 模型的文档-主题表'
- en: '](img/B15923_07_37.jpg)'
  id: totrans-454
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_37.jpg)'
- en: 'Figure 7.37: The document-topic table using the four-topic LDA model'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.37：使用四主题 LDA 模型的文档-主题表
- en: 'Display the results of the LDA model using `pyLDAvis`:'
  id: totrans-456
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `pyLDAvis` 显示 LDA 模型的结果：
- en: '[PRE52]'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The plot is as follows:'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图像如下：
- en: '![Figure 7.38: A histogram and biplot describing the four-topic LDA model'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.38：描述四主题 LDA 模型的直方图和双图'
- en: '](img/B15923_07_38.jpg)'
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_38.jpg)'
- en: 'Figure 7.38: A histogram and biplot describing the four-topic LDA model'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.38：描述四主题 LDA 模型的直方图和双图
- en: Looking at the word-topic table, we see that the four topics found by this model
    align with the four topics specified in the original dataset. Those topics are
    Barack Obama, Palestine, Microsoft, and the economy. The question now is, why
    did the model built using four topics have a higher perplexity score than the
    model with three topics? That answer comes from the visualization produced in
    *Step 5*.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 查看词汇-主题表，我们可以看到这个模型找到的四个主题与原始数据集中的四个主题一致。这些主题分别是巴拉克·奥巴马、巴勒斯坦、微软和经济。现在的问题是，为什么使用四个主题构建的模型具有比使用三个主题的模型更高的困惑度得分？这个答案可以从*步骤
    5* 生成的可视化结果中找到。
- en: The biplot has circles of reasonable size, but two of those circles are quite
    close together, which suggests that those two topics (Microsoft and the economy)
    are very similar. In this case, the similarity actually makes intuitive sense.
    Microsoft is a major global company that impacts and is impacted by the economy.
    The next step, if we were to make one, would be to run the t-SNE plot to check
    whether the clusters in the t-SNE plot overlap.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 双变量图有合理大小的圆圈，但其中两个圆圈相距非常近，这表明这两个主题（微软和经济）非常相似。在这种情况下，相似性实际上是直观上有道理的。微软是一家全球性的大公司，影响并受经济的影响。如果我们要进行下一步，那就是运行
    t-SNE 图，以检查 t-SNE 图中的簇是否有重叠。
- en: Note
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参见 [https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个例子，访问 [https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 必须执行整个笔记本才能获得期望的结果。
- en: Let's now apply our knowledge of LDA to another dataset.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将 LDA 的知识应用于另一个数据集。
- en: 'Activity 7.02: LDA and Health Tweets'
  id: totrans-469
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 7.02：LDA 和健康推文
- en: In this activity, we'll apply LDA to the health tweets data loaded and cleaned
    in *Activity 7.01*, *Loading and Cleaning Twitter Data*. Remember to use the same
    notebook used in that activity. Once the steps have been executed, discuss the
    results of the model. Do these word groupings make sense?
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，我们将应用 LDA 于 *活动 7.01* 中加载和清理过的健康推文数据，*加载并清理 Twitter 数据*。记得使用该活动中使用的同一笔记本。一旦步骤执行完毕，讨论模型的结果。这些单词分组有意义吗？
- en: For this activity, let's imagine that we are interested in acquiring a high-level
    understanding of the major public health topics. That is, what people are talking
    about in the world of health. We have collected some data that could shed light
    on this inquiry. The easiest way to identify the major topics in the dataset,
    as we have discussed, is topic modeling.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本活动，让我们假设我们有兴趣获得对主要公共卫生话题的高层次理解。也就是说，了解人们在健康领域谈论的内容。我们已经收集了一些数据，可能会揭示这一问题的答案。正如我们所讨论的，识别数据集中主要话题的最简单方法是主题建模。
- en: 'Here are the steps to complete the activity:'
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成该活动的步骤：
- en: Specify the `number_words`, `number_docs`, and `number_features` variables.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定 `number_words`、`number_docs` 和 `number_features` 变量。
- en: Create a bag-of-words model and assign the feature names to another variable
    for use later on.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个词袋模型，并将特征名称分配给另一个变量，以便以后使用。
- en: Identify the optimal number of topics.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定最佳的主题数量。
- en: Fit the LDA model using the optimal number of topics.
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳的主题数量来拟合 LDA 模型。
- en: Create and print the word-topic table.
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并打印出词-主题表。
- en: Print the document-topic table.
  id: totrans-478
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出文档-主题表。
- en: Create a biplot visualization.
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个双变量图可视化。
- en: Keep the notebook open for future modeling.
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持笔记本打开，以便以后进行建模。
- en: 'The output will be as follows:'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将如下：
- en: '![Figure 7.39: A histogram and biplot for the LDA model trained on health tweets'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.39：在健康推文上训练的 LDA 模型的直方图和双变量图'
- en: '](img/B15923_07_39.jpg)'
  id: totrans-483
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_39.jpg)'
- en: 'Figure 7.39: A histogram and biplot for the LDA model trained on health tweets'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.39：在健康推文上训练的 LDA 模型的直方图和双变量图
- en: Note
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 482.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 482 页找到。
- en: 'Exercise 7.10: Creating a Bag-of-Words Model Using TF-IDF'
  id: totrans-487
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.10：使用 TF-IDF 创建词袋模型
- en: 'In this exercise, we will create a bag-of-words model using TF-IDF:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 TF-IDF 创建一个词袋模型：
- en: 'Run the TF-IDF vectorizer and print out the first few rows:'
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 TF-IDF 向量化器并打印出前几行：
- en: '[PRE53]'
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output is as follows:'
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.40: Output of the TF-IDF vectorizer'
  id: totrans-492
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.40：TF-IDF 向量化器的输出'
- en: '](img/B15923_07_40.jpg)'
  id: totrans-493
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_40.jpg)'
- en: 'Figure 7.40: Output of the TF-IDF vectorizer'
  id: totrans-494
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.40：TF-IDF 向量化器的输出
- en: 'Return the feature names (the actual words in the corpus dictionary) to use
    when analyzing the output. You will recall that we did the same thing when we
    ran `CountVectorizer` in *Exercise7.05*, *Creating a Bag-of-Words Model Using
    the Count Vectorizer*:'
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回用于分析输出的特征名称（语料库字典中的实际单词）。你会记得我们在执行 `CountVectorizer` 时也做过同样的事情，出现在 *练习7.05*，*使用
    Count Vectorizer 创建词袋模型*：
- en: '[PRE54]'
  id: totrans-496
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'A section of the output is as follows:'
  id: totrans-497
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一个部分如下：
- en: '[PRE55]'
  id: totrans-498
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: In this exercise, we summarized the corpus in the form of a bag-of-words model.
    Weights were computed for each document word combination. This bag of words output
    will return later on during the fitting on our next topic model. The next section
    will introduce NMF.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们以词袋模型的形式总结了语料库。为每个文档-词组合计算了权重。这个词袋输出将在我们下一步的主题模型拟合中再次使用。下一节将介绍NMF。
- en: Note
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参阅[https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/3fbWQES](https://packt.live/3fbWQES)上在线运行此示例。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个笔记本才能获得预期结果。
- en: Non-Negative Matrix Factorization
  id: totrans-504
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非负矩阵分解
- en: Unlike LDA, **Non-Negative Matrix Factorization** (**NMF**) is not a probabilistic
    model. instead, it is, as the name implies, an approach involving linear algebra.
    Using matrix factorization as an approach to topic modeling was introduced by
    Daniel D. Lee and H. Sebastian Seung in 1999\. The approach falls into the decomposition
    family of models that includes PCA, the modeling technique introduced in *Chapter
    4*, *Introduction to Dimensionality Reduction and PCA*.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 与LDA不同，**非负矩阵分解**（**NMF**）不是一个概率模型。相反，正如其名称所示，它是一种涉及线性代数的方法。将矩阵分解作为主题建模的方法由Daniel
    D. Lee和H. Sebastian Seung于1999年提出。该方法属于模型的分解类，包括PCA，这是一种在*第4章*中介绍的建模技术，*降维与PCA简介*。
- en: The major differences between PCA and NMF are that PCA requires components to
    be perpendicular while allowing them to be either positive or negative. NMF requires
    that matrix components be non-negative, which should make sense if you think of
    this requirement in the context of the data. Topics cannot be negatively related
    to documents, and words cannot be negatively related to topics.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和NMF之间的主要区别在于，PCA要求组件是垂直的，但允许它们是正数或负数。而NMF要求矩阵组件是非负的，如果你从数据的角度思考这一要求，这应该是有道理的。主题与文档之间不能是负相关的，词汇与主题之间也不能是负相关的。
- en: If you are not convinced, try to interpret a negative weight associating a topic
    with a document. It would be something like, topic T makes up -30% of document
    D; but what does that even mean? It is nonsensical, so NMF has non-negative requirements
    for every part of the matrix factorization.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不信服，试着解释将一个负权重与主题和文档关联起来。这就像，主题T占文档D的-30%；但这是什么意思呢？这毫无意义，因此NMF对矩阵分解的每个部分都有非负的要求。
- en: Let's define the matrix to be factorized, *X*, as a term-document matrix where
    the rows are words and the columns are documents. Each element of matrix *X* is
    either the number of occurrences of word *i* (the row) in document *j* (the column)
    or some other quantification of the relationship between word *i* and document
    *j*. The matrix, *X*, is naturally a sparse matrix as most elements in the term-document
    matrix will be zero, since each document only contains a limited number of words.
    There will be more on creating this matrix and deriving the quantifications later.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义要分解的矩阵*X*为术语-文档矩阵，其中行是词汇，列是文档。矩阵*X*的每个元素要么是词* i*（行）在文档*j*（列）中的出现次数，要么是词*
    i*与文档*j*之间关系的其他量化。矩阵*X*自然是一个稀疏矩阵，因为术语-文档矩阵中的大多数元素将为零，因为每个文档只包含有限数量的词汇。稍后会讲到如何创建这个矩阵并推导量化方法。
- en: '![Figure 7.41: The matrix factorization'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.41: 矩阵分解'
- en: '](img/B15923_07_41.jpg)'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_41.jpg)'
- en: 'Figure 7.41: The matrix factorization'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '图 7.41: 矩阵分解'
- en: The matrix factorization takes the form ![where the two ](img/B15923_07_Formula_17.png)
    , where the two component matrices, *W* and *H*, represent the topics as collections
    of words and the topic weights for each document, respectively. More specifically,
    *W*nxk is a word by topic matrix, while *H*kxm is a topic by document matrix and,
    as stated earlier, *X*nxm is a word by document matrix.
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵分解的形式为 ![其中两个 ](img/B15923_07_Formula_17.png)，其中两个组件矩阵，*W*和*H*，分别表示主题词集合和每个文档的主题权重。更具体地说，*W*nxk是一个词对主题的矩阵，而*H*kxm是一个主题对文档的矩阵，正如前面所述，*X*nxm是一个词对文档的矩阵。
- en: A nice way to think of this factorization is as a weighted sum of word groupings
    defining abstract topics. The equivalency symbol in the formula for the matrix
    factorization is an indicator that the factorization *WH* is an approximation,
    and thus, the product of those two matrices will not reproduce the original term-document
    matrix exactly.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 思考这个因式分解的一种好方式是将其看作是定义抽象主题的加权词组的总和。矩阵因式分解公式中的等价符号表明，因式分解*WH*是一个近似值，因此这两个矩阵的乘积不会完全重现原始的术语-文档矩阵。
- en: The goal, as it was with LDA, is to find the approximation that is closest to
    the original matrix. Like *X*, both *W* and *H* are sparse matrices as each topic
    is only related to a few words, and each document is a mixture of only a small
    number of topics—one topic in many cases.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 目标和LDA一样，是找到最接近原始矩阵的近似值。像*X*一样，*W*和*H*也是稀疏矩阵，因为每个主题只与少数几个词相关，每个文档仅由少数几个主题组成——在许多情况下是一个主题。
- en: The Frobenius Norm
  id: totrans-515
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Frobenius范数
- en: 'The goal of solving NMF is the same as that of LDA: find the best approximation.
    To measure the distance between the input matrix and the approximation, NMF can
    use virtually any distance measure, but the standard is the Frobenius norm, also
    known as the Euclidean norm. The Frobenius norm is the sum of the element-wise
    squared'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 解决NMF的目标与LDA相同：找到最佳近似值。为了衡量输入矩阵与近似值之间的距离，NMF可以使用几乎任何距离度量，但标准是Frobenius范数，也称为欧几里得范数。Frobenius范数是元素平方和的总和。
- en: errors mathematically expressed as ![C:\Users\user\Downloads\B15923_07_Formula_18.png](img/B15923_07_Formula_18.png).
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 数学上表示为错误 ![C:\Users\user\Downloads\B15923_07_Formula_18.png](img/B15923_07_Formula_18.png)。
- en: With the measure of distance selected, the next step is to define the objective
    function. The minimization of the Frobenius norm will return the best approximation
    of the original term-document matrix and, thus, the most reasonable topics. Note
    that the objective function is minimized with respect to *W* and *H* so that both
    matrices
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 选择好距离度量后，下一步是定义目标函数。最小化Frobenius范数将返回最好的原始术语-文档矩阵的近似值，从而得到最合理的主题。请注意，目标函数是相对于*W*和*H*最小化的，以使两个矩阵都
- en: are non-negative. It is expressed as ![C:\Users\user\Downloads\B15923_07_Formula_19.png](img/B15923_07_Formula_19.png).
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 是非负的。它的表达式为 ![C:\Users\user\Downloads\B15923_07_Formula_19.png](img/B15923_07_Formula_19.png)。
- en: The Multiplicative Update Algorithm
  id: totrans-520
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 乘法更新算法
- en: The optimization algorithm used to solve NMF by Lee and Seung in their 1999
    paper is the Multiplicative Update algorithm, and it is still one of the most
    commonly used solutions. It will be implemented in the exercises and activities
    later in the chapter.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 1999年Lee和Seung在他们的论文中用于解决NMF的优化算法是乘法更新算法，它仍然是最常用的解决方案之一。在本章后面的练习和活动中将会实现该算法。
- en: 'The update rules, for both *W* and *H*, are derived by expanding the objective
    function and taking the partial derivatives with respect to *W* and *H*. The derivatives
    are not difficult but do, require fairly extensive linear algebra knowledge, and
    are time-consuming, so let''s skip the derivatives and just state the updates.
    Note that, in the update rules, *i* is the current iteration and *T* means the
    transpose of the matrix. The first update rule is as follows:'
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: '*W*和*H*的更新规则是通过展开目标函数并对*W*和*H*分别取偏导数得到的。导数并不难，但需要相当广泛的线性代数知识，而且时间较长，所以我们跳过导数，直接给出更新规则。请注意，在更新规则中，*i*是当前的迭代次数，*T*表示矩阵的转置。第一个更新规则如下：'
- en: '![Figure 7.42: First update rule'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.42：第一个更新规则'
- en: '](img/B15923_07_42.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_42.jpg)'
- en: 'Figure 7.42: First update rule'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.42：第一个更新规则
- en: 'The second update rule is as follows:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个更新规则如下：
- en: '![Figure 7.43: Second update rule'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.43：第二个更新规则'
- en: '](img/B15923_07_43.jpg)'
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_07_43.jpg)'
- en: 'Figure 7.43: Second update rule'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.43：第二个更新规则
- en: '*W* and *H* are updated iteratively until the algorithm converges. The objective
    function can also be shown to be non-increasing; that is, with each iterative
    update of *W* and *H*, the objective function gets closer to the minimum. Note
    that the multiplicative update optimizer, if the update rules are reorganized,
    is a rescaled gradient descent algorithm.'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '*W*和*H*会迭代更新，直到算法收敛。目标函数也可以被证明是非递减的；即，在每次迭代更新*W*和*H*时，目标函数会更接近最小值。请注意，乘法更新优化器，如果更新规则重新组织，是一种重新缩放的梯度下降算法。'
- en: The final component of building a successful NMF algorithm is initializing the
    *W* and *H* component matrices so that the multiplicative update works quickly.
    A popular approach to initializing matrices is **Singular Value Decomposition**
    (**SVD**), which is a generalization of Eigen decomposition.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 构建成功的NMF算法的最后一个组成部分是初始化*W*和*H*矩阵，以确保乘法更新能够快速工作。一种流行的初始化矩阵的方法是**奇异值分解**（**SVD**），它是特征分解的推广。
- en: In the implementation of NMF undertaken in the forthcoming exercises, the matrices
    are initialized via non-negative Double Singular Value Decomposition, which is
    basically a more advanced version of SVD that is strictly non-negative. The full
    details of these initialization algorithms are not important for understanding
    NMF. Just note that initialization algorithms are used as a starting point for
    the optimization algorithms and can drastically speed up convergence.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的练习中实现的NMF方法中，矩阵通过非负双奇异值分解进行初始化，基本上这是SVD的一个更高级版本，严格要求非负。关于这些初始化算法的详细信息，对于理解NMF并不重要。只需注意，初始化算法是优化算法的起点，能够显著加速收敛过程。
- en: 'Exercise 7.11: Non-negative Matrix Factorization'
  id: totrans-533
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.11：非负矩阵分解
- en: In this exercise, we'll fit the NMF algorithm and output the same two result
    tables we previously did with LDA. Those tables are the word-topic table, which
    shows the top 10 words associated with each topic, and the document-topic table,
    which shows the top 10 documents associated with each topic.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将拟合NMF算法，并输出与之前使用LDA时相同的两个结果表。这些表是词-主题表，显示与每个主题相关的前10个词，和文档-主题表，显示与每个主题相关的前10个文档。
- en: 'There are two additional parameters in the NMF algorithm function that we have
    not previously discussed, which are `alpha` and `l1_ratio`. If an overfit model
    is of concern, these parameters control how (`l1_ratio`) and the extent to which
    (`alpha`) regularization is applied to the objective function:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: NMF算法函数中有两个我们之前没有讨论过的额外参数，分别是`alpha`和`l1_ratio`。如果存在过拟合模型的风险，这些参数控制正则化在目标函数中的应用方式（`l1_ratio`）和程度（`alpha`）：
- en: Note
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: More details can be found in the documentation for the scikit-learn library
    ([https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)).
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 更多细节可以在scikit-learn库的文档中找到（[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.NMF.html)）。
- en: 'Define the NMF model and call the `fit` function using the output of the TF-IDF
    vectorizer:'
  id: totrans-538
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义NMF模型并使用TF-IDF向量化器的输出调用`fit`函数：
- en: '[PRE56]'
  id: totrans-539
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-540
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.44: Defining the NMF model'
  id: totrans-541
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.44：定义NMF模型'
- en: '](img/B15923_07_44.jpg)'
  id: totrans-542
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_44.jpg)'
- en: 'Figure 7.44: Defining the NMF model'
  id: totrans-543
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.44：定义NMF模型
- en: 'Run the `get_topics` functions to produce the two output tables:'
  id: totrans-544
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行`get_topics`函数以生成两个输出表：
- en: '[PRE57]'
  id: totrans-545
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Print the `W` table:'
  id: totrans-546
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`W`表：
- en: '[PRE58]'
  id: totrans-547
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-548
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.45: The word-topic table containing probabilities'
  id: totrans-549
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.45：包含概率的词-主题表'
- en: '](img/B15923_07_45.jpg)'
  id: totrans-550
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_45.jpg)'
- en: 'Figure 7.45: The word-topic table containing probabilities'
  id: totrans-551
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.45：包含概率的词-主题表
- en: 'Print the `H` table:'
  id: totrans-552
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印`H`表：
- en: '[PRE59]'
  id: totrans-553
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The output is as follows:'
  id: totrans-554
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 7.46: The document-topic table containing probabilities'
  id: totrans-555
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.46：包含概率的文档-主题表'
- en: '](img/B15923_07_46.jpg)'
  id: totrans-556
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_46.jpg)'
- en: 'Figure 7.46: The document-topic table containing probabilities'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.46：包含概率的文档-主题表
- en: The word-topic table contains word groupings that suggest the same abstract
    topics that the four-topic LDA model produced in *Exercise 7.09*, *Trying Four
    Topics*. However, the interesting part of the comparison is that some of the individual
    words contained in these groupings are new or in a new place in the grouping.
    This is not surprising given that the methodologies are distinct. Given the alignment
    with the topics specified in the original dataset, we have shown that both of
    these methodologies are effective tools for extracting the underlying topic structure
    of the corpus.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 词-主题表包含词语分组，表明与四主题LDA模型在*练习 7.09*《尝试四个主题》中生成的抽象主题相同。然而，比较中有趣的是，这些分组中包含的一些个别词语是新的，或者它们在分组中的位置发生了变化。考虑到这两种方法学是不同的，这并不令人惊讶。鉴于与原始数据集中指定的主题一致性，我们已经证明这两种方法都是提取语料库潜在主题结构的有效工具。
- en: As we did with our previously fit LDA model, we will visualize the results of
    our NMF model.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前对 LDA 模型的拟合一样，我们将可视化我们的 NMF 模型的结果。
- en: Note
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问该部分的源代码，请参考 [https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问链接 [https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个 Notebook 才能获得期望的结果。
- en: 'Exercise 7.12: Visualizing NMF'
  id: totrans-564
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 7.12：可视化 NMF
- en: 'The purpose of this exercise is to visualize the results of NMF. Visualizing
    the results gives insight into the distinctness of the topics and the prevalence
    of each topic in the corpus. In this exercise, we''ll do the visualizing using
    t-SNE, which was discussed fully in *Chapter 6*, *t-Distributed Stochastic Neighbor
    Embedding*:'
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习的目的是可视化 NMF 的结果。通过可视化结果，可以深入了解主题的独特性以及每个主题在语料库中的流行度。在这个练习中，我们将使用 t-SNE 来进行可视化，t-SNE
    在*第 6 章*中有详细讨论，*t-分布随机邻域嵌入*：
- en: 'Run `transform` on the cleaned data to get the topic-document allocations.
    Print both the shape and an example of the data:'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在清理后的数据上运行 `transform`，以获取主题-文档分配。打印数据的形状和一个示例：
- en: '[PRE60]'
  id: totrans-567
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-568
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE61]'
  id: totrans-569
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Run the `plot_tsne` function to fit a t-SNE model and plot the results:'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 `plot_tsne` 函数来拟合 t-SNE 模型并绘制结果：
- en: '[PRE62]'
  id: totrans-571
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The plot appears as follows:'
  id: totrans-572
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图形显示如下：
- en: '![Figure 7.47: t-SNE plot with metrics summarizing the topic distribution across
    the corpus'
  id: totrans-573
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.47：带有度量的 t-SNE 图，汇总了语料库中的主题分布'
- en: '](img/B15923_07_47.jpg)'
  id: totrans-574
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_47.jpg)'
- en: 'Figure 7.47: t-SNE plot with metrics summarizing the topic distribution across
    the corpus'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.47：带有度量的 t-SNE 图，汇总了语料库中的主题分布
- en: Note
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The results can differ slightly because of the optimization algorithms that
    support both LDA and NMF. Many of the functions do not have a seed setting capability.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能略有不同，因为支持 LDA 和 NMF 的优化算法有所不同。许多函数没有设定种子值的功能。
- en: To access the source code for this specific section, please refer to [https://packt.live/34gLGKa](https://packt.live/34gLGKa).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问该部分的源代码，请参考 [https://packt.live/34gLGKa](https://packt.live/34gLGKa)。
- en: You can also run this example online at [https://packt.live/3fbWQES](https://packt.live/3fbWQES).
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行这个示例，访问链接 [https://packt.live/3fbWQES](https://packt.live/3fbWQES)。
- en: You must execute the entire Notebook in order to get the desired result.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须执行整个 Notebook 才能获得期望的结果。
- en: The t-SNE plot, with no threshold specified, shows some topic overlap and a
    clear discrepancy in the topic frequency across the corpus. These two facts explain
    why, when using perplexity, the optimal number of topics was three. There seems
    to be some correlation between topics that the model can't fully accommodate.
    Even with the correlation between topics, the model is finding the topics it should
    when the number of topics is set to four.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE 图没有指定阈值，显示了一些主题重叠，并且语料库中的主题频率存在明显差异。这两点解释了为何在使用困惑度时，最佳的主题数量为三个。似乎存在某些主题之间的关联，模型无法完全处理。即使存在主题之间的关联，当主题数设置为四时，模型仍能找到正确的主题。
- en: To recap, NMF is a non-probabilistic topic model that seeks to answer the same
    question LDA is trying to answer. It uses a popular concept of linear algebra
    known as matrix factorization, which is the process of breaking a large and intractable
    matrix down into smaller and more easily interpretable matrices that can be leveraged
    to answer many questions about the data. Remember that the non-negative requirement
    is not rooted in mathematics, but in the data itself. It does not make sense for
    the components of any document to be negative.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，NMF 是一种非概率主题模型，旨在回答与 LDA 相同的问题。它使用线性代数中的一种常用概念——矩阵分解，即将一个庞大且难以处理的矩阵分解为较小、更易解释的矩阵，从而帮助回答许多与数据相关的问题。请记住，非负性要求并不是数学上的要求，而是数据本身的要求。任何文档的组件不可能为负数。
- en: In many cases, NMF does not perform as well as LDA, because LDA incorporates
    prior distributions that add an extra layer of information to help inform the
    topic word groupings. However, we know that there are cases, especially when the
    topics are highly correlated, when NMF is the better performer. One of those cases
    was the headline data on which all the exercises were based.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，NMF的表现不如LDA，因为LDA包含先验分布，这为主题词组提供了额外的信息层。然而，我们知道在某些情况下，尤其是当主题高度相关时，NMF的表现更好。正是这种情况发生在所有练习所依据的标题数据上。
- en: Let's now try to apply our new knowledge of NMF to the Twitter dataset used
    in the previous activities.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试将新学到的NMF知识应用到前面活动中使用的Twitter数据集。
- en: 'Activity 7.03: Non-negative Matrix Factorization'
  id: totrans-585
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动7.03：非负矩阵分解
- en: This activity is the summation of the topic modeling analysis done on the health
    Twitter data loaded and cleaned in *Activity 7.01*, *Loading and Cleaning Twitter
    Data*, and on which LDA was done in *Activity 7.02*, *LDA and Health Tweets*.
    The execution of NMF is straightforward and requires limited coding. We can take
    this opportunity to play with the parameters of the model while thinking about
    the limitations and benefits of NMF.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动总结了在*活动7.01*，*加载与清理Twitter数据*中加载并清理的健康Twitter数据上的主题建模分析，以及在*活动7.02*，*LDA与健康推文*中进行的LDA分析。执行NMF非常简单，所需代码有限。我们可以借此机会在思考NMF的局限性和优势时调整模型参数。
- en: 'Here are the steps to complete the activity:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成本活动的步骤：
- en: Create the appropriate bag-of-words model and output the feature names as another
    variable.
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建适当的词袋模型，并将特征名称输出为另一个变量。
- en: Define and fit the NMF algorithm using the number of topics (`n_components`)
    value from *Activity 7.02*, *LDA and Health Tweets*.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*活动7.02*，*LDA与健康推文*中的主题数量（`n_components`）值，定义并拟合NMF算法。
- en: Get the topic-document and word-topic tables. Take a few minutes to explore
    the word groupings and try to define the abstract topics. Can you quantify the
    meanings of the word groupings? Do the word groupings make sense? Are the results
    similar to those produced using LDA?
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取主题-文档和词-主题表格。花几分钟探索词组，并尝试定义抽象的主题。你能量化这些词组的含义吗？这些词组合理吗？与使用LDA产生的结果是否相似？
- en: Adjust the model parameters and rerun *Step 3* and *Step 4*. How do the results
    change?
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调整模型参数并重新运行*步骤3*和*步骤4*。结果如何变化？
- en: 'The output will be as follows:'
  id: totrans-592
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 7.48: The word-topic table with probabilities'
  id: totrans-593
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图7.48：带有概率的词-主题表格'
- en: '](img/B15923_07_48.jpg)'
  id: totrans-594
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_07_48.jpg)'
- en: 'Figure 7.48: The word-topic table with probabilities'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.48：带有概率的词-主题表格
- en: Note
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 487.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第487页找到。
- en: Summary
  id: totrans-598
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: When faced with the task of extracting information from an as yet unseen large
    collection of documents, topic modeling is a great approach, as it provides insights
    into the underlying structure of the documents. That is, topic models find word
    groupings using proximity, not context.
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对从尚未看到的大量文档中提取信息的任务时，主题建模是一个很好的方法，因为它可以提供有关文档潜在结构的洞察。也就是说，主题模型通过接近性而非语境来寻找词组。
- en: 'In this chapter, we have learned how to apply two of the most common and most
    effective topic modeling algorithms: latent Dirichlet allocation and non-negative
    matrix factorization. You should now feel comfortable cleaning raw text documents
    using several different techniques; techniques that can be utilized in many other
    modeling scenarios. We continued by learning how to convert the cleaned corpus
    into the appropriate data structure of per-document raw word counts or word weights
    by applying bag-of-words models.'
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何应用两种最常见且最有效的主题建模算法：潜在Dirichlet分配（LDA）和非负矩阵分解（NMF）。现在你应该能够熟练使用几种不同的技术清理原始文本文档，这些技术可以在许多其他建模场景中使用。接着，我们学习了如何通过应用词袋模型，将清理过的语料库转换为适当的数据结构，即每个文档的原始词频或词权重。
- en: The main focus of the chapter was fitting the two topic models, including optimizing
    the number of topics, converting the output to easy-to-interpret tables, and visualizing
    the results. With this information, you should be able to apply fully functioning
    topic models to derive value and insights for your business.
  id: totrans-601
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容是拟合这两种主题模型，包括优化主题数量、将输出转换为易于理解的表格，并可视化结果。有了这些信息，你应该能够应用完全功能的主题模型，为你的业务提取价值和洞察。
- en: In the next chapter, we will change direction entirely. We will deep dive into
    market basket analysis.
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将完全改变方向。我们将深入探讨市场篮子分析。
