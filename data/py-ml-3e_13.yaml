- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Parallelizing Neural Network Training with TensorFlow
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 并行化神经网络训练
- en: In this chapter, we will move on from the mathematical foundations of machine
    learning and deep learning to focus on TensorFlow. TensorFlow is one of the most
    popular deep learning libraries currently available, and it lets us implement
    neural networks (NNs) much more efficiently than any of our previous NumPy implementations.
    In this chapter, we will start using TensorFlow and see how it brings significant
    benefits to training performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从机器学习和深度学习的数学基础转向 TensorFlow。TensorFlow 是目前最流行的深度学习库之一，它让我们比之前任何的 NumPy
    实现更高效地实现神经网络（NN）。在本章中，我们将开始使用 TensorFlow，并看到它如何显著提升训练性能。
- en: 'This chapter will begin the next stage of our journey into machine learning
    and deep learning, and we will explore the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将开始我们进入机器学习和深度学习的新阶段，我们将探索以下主题：
- en: How TensorFlow improves training performance
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 如何提高训练性能
- en: Working with TensorFlow's `Dataset` API (`tf.data`) to build input pipelines
    and efficient model training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 的 `Dataset` API（`tf.data`）构建输入管道和高效的模型训练
- en: Working with TensorFlow to write optimized machine learning code
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 编写优化的机器学习代码
- en: Using TensorFlow high-level APIs to build a multilayer NN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 高级 API 构建多层神经网络
- en: Choosing activation functions for artificial NNs
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择人工神经网络的激活函数
- en: Introducing Keras (`tf.keras`), a high-level wrapper around TensorFlow that
    can be used to implement common deep learning architectures conveniently
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Keras（`tf.keras`），这是一个高层次的 TensorFlow 封装器，可方便地实现常见的深度学习架构
- en: TensorFlow and training performance
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 和训练性能
- en: TensorFlow can speed up our machine learning tasks significantly. To understand
    how it can do this, let's begin by discussing some of the performance challenges
    we typically run into when we run expensive calculations on our hardware. Then,
    we will take a high-level look at what TensorFlow is and what our learning approach
    will be in this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 可以显著加速我们的机器学习任务。为了理解它是如何做到这一点的，让我们首先讨论在硬件上进行昂贵计算时通常遇到的一些性能挑战。然后，我们将高层次地了解
    TensorFlow 是什么，以及我们在本章中的学习方法是什么。
- en: Performance challenges
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能挑战
- en: The performance of computer processors has, of course, been continuously improving
    in recent years, and that allows us to train more powerful and complex learning
    systems, which means that we can improve the predictive performance of our machine
    learning models. Even the cheapest desktop computer hardware that's available
    right now comes with processing units that have multiple cores.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，近年来计算机处理器的性能不断提高，这使我们能够训练更强大和复杂的学习系统，这意味着我们可以提高机器学习模型的预测性能。即使是目前最便宜的桌面计算机硬件，也配备了多个核心的处理单元。
- en: In the previous chapters, we saw that many functions in scikit-learn allow us
    to spread those computations over multiple processing units. However, by default,
    Python is limited to execution on one core due to the **global interpreter lock**
    (**GIL**). So, although we, indeed, take advantage of Python's multiprocessing
    library to distribute our computations over multiple cores, we still have to consider
    that the most advanced desktop hardware rarely comes with more than eight or 16
    such cores.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到 scikit-learn 中的许多函数允许我们将计算分散到多个处理单元上。然而，默认情况下，由于**全局解释器锁**（**GIL**），Python
    限制了只能在一个核心上执行。因此，尽管我们确实利用了 Python 的多进程库将计算分布到多个核心上，但我们仍然需要考虑到，最先进的桌面硬件通常只有八个或十六个这样的核心。
- en: You will recall from *Chapter 12*, *Implementing a Multilayer Artificial Neural
    Network from Scratch*, that we implemented a very simple multilayer perceptron
    (MLP) with only one hidden layer consisting of 100 units. We had to optimize approximately
    80,000 weight parameters ([784*100 + 100] + [100 * 10] + 10 = 79,510) to learn
    a model for a very simple image classification task. The images in MNIST are rather
    small (![](img/B13208_13_001.png)), and we can only imagine the explosion in the
    number of parameters if we wanted to add additional hidden layers or work with
    images that have higher pixel densities. Such a task would quickly become unfeasible
    for a single processing unit. The question then becomes, how can we tackle such
    problems more effectively?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得在*第12章*，*从零开始实现多层人工神经网络*中，我们实现了一个非常简单的多层感知器（MLP），只有一个包含100个单元的隐藏层。我们需要优化大约80,000个权重参数（[784*100
    + 100] + [100 * 10] + 10 = 79,510）来学习一个非常简单的图像分类模型。MNIST中的图像相当小（![](img/B13208_13_001.png)），我们可以想象如果想要添加更多的隐藏层或处理更高像素密度的图像，参数的数量会爆炸式增长。这样的任务对于单一的处理单元来说很快就变得不可行。那么问题就来了，如何更有效地处理这样的任务呢？
- en: 'The obvious solution to this problem is to use graphics processing units (GPUs),
    which are real work horses. You can think of a graphics card as a small computer
    cluster inside your machine. Another advantage is that modern GPUs are relatively
    cheap compared to the state-of-the-art central processing units (CPUs), as you
    can see in the following overview:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这一问题的显而易见的方法是使用图形处理单元（GPU），它们是真正的工作马。你可以将显卡看作是你机器内部的一个小型计算机集群。另一个优势是，与最先进的中央处理单元（CPU）相比，现代GPU的价格相对便宜，正如以下概览所示：
- en: '![](img/B13208_13_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_01.png)'
- en: 'The sources for the information in the table are the following websites (Date:
    October 2019):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中信息的来源是以下网站（日期：2019年10月）：
- en: '[https://ark.intel.com/content/www/us/en/ark/products/189123/intel-core-i9-9960x-x-series-processor-22m-cache-up-to-4-50-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/189123/intel-core-i9-9960x-x-series-processor-22m-cache-up-to-4-50-ghz.html)'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://ark.intel.com/content/www/us/en/ark/products/189123/intel-core-i9-9960x-x-series-processor-22m-cache-up-to-4-50-ghz.html](https://ark.intel.com/content/www/us/en/ark/products/189123/intel-core-i9-9960x-x-series-processor-22m-cache-up-to-4-50-ghz.html)'
- en: '[https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/](https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/)'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/](https://www.nvidia.com/en-us/geforce/graphics-cards/rtx-2080-ti/)'
- en: At 65 percent of the price of a modern CPU, we can get a GPU that has 272 times
    more cores and is capable of around 10 times more floating-point calculations
    per second. So, what is holding us back from utilizing GPUs for our machine learning
    tasks? The challenge is that writing code to target GPUs is not as simple as executing
    Python code in our interpreter. There are special packages, such as CUDA and OpenCL,
    that allow us to target the GPU. However, writing code in CUDA or OpenCL is probably
    not the most convenient environment for implementing and running machine learning
    algorithms. The good news is that this is what TensorFlow was developed for!
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 以现代CPU价格的65%为代价，我们可以获得一款GPU，其核心数是现代CPU的272倍，且每秒能够进行约10倍更多的浮点计算。那么，是什么阻碍我们利用GPU来完成机器学习任务呢？挑战在于，编写针对GPU的代码并不像在解释器中执行Python代码那样简单。我们需要使用特殊的包，如CUDA和OpenCL，来利用GPU。然而，使用CUDA或OpenCL编写代码可能不是实现和运行机器学习算法的最便捷环境。好消息是，这正是TensorFlow的开发初衷！
- en: What is TensorFlow?
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是TensorFlow？
- en: TensorFlow is a scalable and multiplatform programming interface for implementing
    and running machine learning algorithms, including convenience wrappers for deep
    learning. TensorFlow was developed by the researchers and engineers from the Google
    Brain team. While the main development is led by a team of researchers and software
    engineers at Google, its development also involves many contributions from the
    open source community. TensorFlow was initially built for internal use at Google,
    but it was subsequently released in November 2015 under a permissive open source
    license. Many machine learning researchers and practitioners from academia and
    industry have adapted TensorFlow to develop deep learning solutions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是一个可扩展的多平台编程接口，用于实现和运行机器学习算法，包括深度学习的便捷封装。TensorFlow 是由 Google Brain
    团队的研究人员和工程师开发的。虽然主要的开发工作由 Google 的研究人员和软件工程师团队领导，但其开发也得到了开源社区的许多贡献。TensorFlow
    最初是为 Google 内部使用而构建的，但随后于 2015 年 11 月在宽松的开源许可证下发布。许多来自学术界和行业的机器学习研究人员和从业者已将 TensorFlow
    应用于开发深度学习解决方案。
- en: To improve the performance of training machine learning models, TensorFlow allows
    execution on both CPUs and GPUs. However, its greatest performance capabilities
    can be discovered when using GPUs. TensorFlow supports CUDA-enabled GPUs officially.
    Support for OpenCL-enabled devices is still experimental. However, OpenCL will
    likely be officially supported in the near future. TensorFlow currently supports
    frontend interfaces for a number of programming languages.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高训练机器学习模型的性能，TensorFlow 允许在 CPU 和 GPU 上执行。然而，当使用 GPU 时，其最大的性能优势可以得到体现。TensorFlow
    官方支持 CUDA 启用的 GPU。对 OpenCL 启用的设备的支持仍处于实验阶段。然而，OpenCL 很可能会在不久的将来得到官方支持。TensorFlow
    当前支持多种编程语言的前端接口。
- en: Luckily for us as Python users, TensorFlow's Python API is currently the most
    complete API, thereby it attracts many machine learning and deep learning practitioners.
    Furthermore, TensorFlow has an official API in C++. In addition, new tools based
    on TensorFlow have been released, TensorFlow.js and TensorFlow Lite, that focus
    on running and deploying machine learning models in a web browser and on mobile
    and Internet of Things (IoT) devices. The APIs in other languages, such as Java,
    Haskell, Node.js, and Go, are not stable yet, but the open source community and
    TensorFlow developers are constantly improving them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们作为 Python 用户来说，TensorFlow 的 Python API 当前是最完整的 API，因此它吸引了许多机器学习和深度学习从业者。此外，TensorFlow
    还提供了官方的 C++ API。此外，基于 TensorFlow 的新工具 TensorFlow.js 和 TensorFlow Lite 已经发布，专注于在
    Web 浏览器和移动设备及物联网（IoT）设备上运行和部署机器学习模型。其他语言的 API，如 Java、Haskell、Node.js 和 Go，仍不稳定，但开源社区和
    TensorFlow 开发人员正在不断改进它们。
- en: TensorFlow is built around a computation graph composed of a set of nodes. Each
    node represents an operation that may have zero or more input or output. A tensor
    is created as a symbolic handle to refer to the input and output of these operations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是围绕一个由一组节点组成的计算图构建的。每个节点代表一个操作，该操作可能有零个或多个输入和输出。张量是作为一个符号句柄来引用这些操作的输入和输出。
- en: Mathematically, tensors can be understood as a generalization of scalars, vectors,
    matrices, and so on. More concretely, a scalar can be defined as a rank-0 tensor,
    a vector can be defined as a rank-1 tensor, a matrix can be defined as a rank-2
    tensor, and matrices stacked in a third dimension can be defined as rank-3 tensors.
    But note that in TensorFlow, the values are stored in NumPy arrays, and the tensors
    provide references to these arrays.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学角度来看，张量可以理解为标量、向量、矩阵等的推广。更具体地说，标量可以定义为一个零阶张量，向量可以定义为一个一阶张量，矩阵可以定义为一个二阶张量，堆叠在第三维的矩阵可以定义为三阶张量。但需要注意的是，在
    TensorFlow 中，值是存储在 NumPy 数组中的，而张量提供对这些数组的引用。
- en: 'To make the concept of a tensor clearer, consider the following figure, which
    represents tensors of ranks 0 and 1 in the first row, and tensors of ranks 2 and
    3 in the second row:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地理解张量的概念，考虑下图，它在第一行表示零阶和一阶张量，在第二行表示二阶和三阶张量：
- en: '![](img/B13208_13_02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_02.png)'
- en: In the original TensorFlow release, TensorFlow computations relied on constructing
    a static, directed graph to represent the data flow. As the use of static computation
    graphs proved to be a major friction point for many users, the TensorFlow library
    recently received a major overhaul with its 2.0 version, which makes building
    and training NN models a lot simpler. While TensorFlow 2.0 still supports static
    computation graphs, it now uses dynamic computation graphs, which allows for more
    flexibility.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在原版 TensorFlow 发布时，TensorFlow 的计算依赖于构建一个静态的有向图来表示数据流。由于静态计算图的使用对于许多用户来说是一个主要的障碍，TensorFlow
    库最近在 2.0 版本中进行了大规模的改进，使得构建和训练神经网络模型变得更加简单。尽管 TensorFlow 2.0 仍然支持静态计算图，但它现在使用动态计算图，这使得操作更加灵活。
- en: How we will learn TensorFlow
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们将如何学习 TensorFlow
- en: First, we are going to cover TensorFlow's programming model, in particular,
    creating and manipulating tensors. Then, we will see how to load data and utilize
    TensorFlow `Dataset` objects, which will allow us to iterate through a dataset
    efficiently. In addition, we will discuss the existing, ready-to-use datasets
    in the `tensorflow_datasets` submodule and learn how to use them.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将介绍 TensorFlow 的编程模型，特别是如何创建和操作张量。然后，我们将学习如何加载数据并利用 TensorFlow `Dataset`
    对象，这将使我们能够高效地迭代数据集。此外，我们还将讨论 `tensorflow_datasets` 子模块中现有的、可直接使用的数据集，并学习如何使用它们。
- en: After learning about these basics, the `tf.keras` API will be introduced and
    we will move forward to building machine learning models, learn how to compile
    and train the models, and learn how to save the trained models on disk for future
    evaluation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 学习了这些基础知识后，`tf.keras` API 将被介绍，我们将继续构建机器学习模型，学习如何编译和训练模型，并学习如何将训练好的模型保存在磁盘上，以便将来评估。
- en: First steps with TensorFlow
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 的第一步
- en: In this section, we will take our first steps in using the low-level TensorFlow
    API. After installing TensorFlow, we will cover how to create tensors in TensorFlow
    and different ways of manipulating them, such as changing their shape, data type,
    and so on.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将迈出使用低级 TensorFlow API 的第一步。安装 TensorFlow 后，我们将介绍如何在 TensorFlow 中创建张量，以及如何操作它们的不同方式，例如改变它们的形状、数据类型等。
- en: Installing TensorFlow
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安装 TensorFlow
- en: 'Depending on how your system is set up, you can typically just use Python''s
    `pip` installer and install TensorFlow from PyPI by executing the following from
    your terminal:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 根据系统设置的不同，通常你只需使用 Python 的 `pip` 安装器，通过在终端中执行以下命令从 PyPI 安装 TensorFlow：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will install the latest *stable* version, which is 2.0.0 at the time of
    writing. In order to ensure that the code presented in this chapter can be executed
    as expected, it is recommended that you use TensorFlow 2.0.0, which can be installed
    by specifying the version explicitly:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将安装最新的 *稳定* 版本，即本文撰写时的 2.0.0 版本。为了确保本章中展示的代码能够按预期执行，建议使用 TensorFlow 2.0.0 版本，可以通过明确指定版本来安装：
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In case you want to use GPUs (recommended), you need a compatible NVIDIA graphics
    card, along with the CUDA Toolkit and the NVIDIA cuDNN library to be installed.
    If your machine satisfies these requirements, you can install TensorFlow with
    GPU support, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用 GPU（推荐），你需要一块兼容的 NVIDIA 显卡，以及安装 CUDA Toolkit 和 NVIDIA cuDNN 库。如果你的机器满足这些要求，你可以按照以下步骤安装支持
    GPU 的 TensorFlow：
- en: '[PRE2]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For more information about the installation and setup process, please see the
    official recommendations at [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 有关安装和设置过程的更多信息，请参阅官方推荐的[https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)。
- en: 'Note that TensorFlow is still under active development; therefore, every couple
    of months, newer versions are released with significant changes. At the time of
    writing this chapter, the latest TensorFlow version is 2.0\. You can verify your
    TensorFlow version from your terminal, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，TensorFlow 仍在积极开发中；因此，每隔几个月会发布新版本，并进行重要的变更。在撰写本章时，最新的 TensorFlow 版本是 2.0。你可以通过终端来验证你的
    TensorFlow 版本，如下所示：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Troubleshooting your installation of TensorFlow**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**解决 TensorFlow 安装问题**'
- en: 'If you experience problems with the installation procedure, read more about
    system- and platform-specific recommendations that are provided at [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/).
    Note that all the code in this chapter can be run on your CPU; using a GPU is
    entirely optional but recommended if you want to fully enjoy the benefits of TensorFlow.
    For example, while training some NN models on CPU could take a week, the same
    models could be trained in just a few hours on a modern GPU. If you have a graphics
    card, refer to the installation page to set it up appropriately. In addition,
    you may find this TensorFlow-GPU setup guide helpful, which explains how to install
    the NVIDIA graphics card drivers, CUDA, and cuDNN on Ubuntu (not required but
    recommended requirements for running TensorFlow on a GPU): [https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf).
    Furthermore, as you will see in *Chapter 17*, *Generative Adversarial Networks
    for Synthesizing New Data*, you can also train your models using a GPU for free
    via Google Colab.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在安装过程中遇到问题，请查看 [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)
    上提供的系统和平台特定的建议。请注意，本章中的所有代码都可以在 CPU 上运行；使用 GPU 是完全可选的，但如果你希望充分利用 TensorFlow 的优势，推荐使用
    GPU。例如，使用 CPU 训练一些神经网络模型可能需要一周时间，而在现代 GPU 上训练相同的模型只需要几个小时。如果你有显卡，请参考安装页面以正确设置它。此外，你可能会发现这个
    TensorFlow-GPU 安装指南很有用，指导你如何在 Ubuntu 上安装 NVIDIA 显卡驱动程序、CUDA 和 cuDNN（这些不是必需的，但如果你想在
    GPU 上运行 TensorFlow，它们是推荐的要求）：[https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_h_cloud-computing.pdf)。此外，正如你将在
    *第17章*《*生成对抗网络用于合成新数据*》中看到的，你还可以通过 Google Colab 免费使用 GPU 来训练模型。
- en: Creating tensors in TensorFlow
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中创建张量
- en: 'Now, let''s consider a few different ways of creating tensors, and then see
    some of their properties and how to manipulate them. Firstly, we can simply create
    a tensor from a list or a NumPy array using the `tf.convert_to_tensor` function
    as follows:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考虑几种不同的创建张量的方式，并查看它们的一些属性以及如何操作它们。首先，我们可以通过 `tf.convert_to_tensor` 函数从列表或
    NumPy 数组简单地创建一个张量，如下所示：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This resulted in tensors `t_a` and `t_b`, with their properties, `shape=(3,)`
    and `dtype=int32,` adopted from their source. Similar to NumPy arrays, we can
    further see these properties:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了张量 `t_a` 和 `t_b`，它们的属性为 `shape=(3,)` 和 `dtype=int32`，这些属性来自它们的源。类似于 NumPy
    数组，我们还可以查看这些属性：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get access to the values that a tensor refers to, we can simply call the
    `.numpy()` method on a tensor:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问张量所引用的值，我们只需调用张量的 `.numpy()` 方法：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, creating a tensor of constant values can be done as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建一个常量值的张量可以通过以下方式完成：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Manipulating the data type and shape of a tensor
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操作张量的数据类型和形状
- en: Learning ways to manipulate tensors is necessary to make them compatible for
    input to a model or an operation. In this section, you will learn how to manipulate
    tensor data types and shapes via several TensorFlow functions that cast, reshape,
    transpose, and squeeze.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 学习如何操作张量是非常必要的，以使它们能够兼容模型或操作的输入。在本节中，你将学习如何通过几个 TensorFlow 函数操作张量的数据类型和形状，这些函数包括转换、重塑、转置和压缩。
- en: 'The `tf.cast()` function can be used to change the data type of a tensor to
    a desired type:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.cast()` 函数可用于将张量的数据类型转换为所需的类型：'
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you will see in upcoming chapters, certain operations require that the input
    tensors have a certain number of dimensions (that is, rank) associated with a
    certain number of elements (shape). Thus, we might need to change the shape of
    a tensor, add a new dimension, or squeeze an unnecessary dimension. TensorFlow
    provides useful functions (or operations) to achieve this, such as `tf.transpose()`,
    `tf.reshape()`, and `tf.squeeze()`. Let''s take a look at some examples:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在后续章节中看到的，某些操作要求输入张量具有特定的维度（即秩）和特定数量的元素（形状）。因此，我们可能需要更改张量的形状、添加新维度或压缩不必要的维度。TensorFlow
    提供了有用的函数（或操作）来实现这一点，例如 `tf.transpose()`、`tf.reshape()` 和 `tf.squeeze()`。让我们来看一些例子：
- en: '**Transposing a tensor**:'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转置一个张量**：'
- en: '[PRE9]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**Reshaping a tensor (for example, from a 1D vector to a 2D array)**:'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重塑一个张量（例如，从 1D 向量到 2D 数组）**：'
- en: '[PRE10]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Removing the unnecessary dimensions (dimensions that have size 1, which are
    not needed)**:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**去除不必要的维度（大小为 1 的维度，通常是不需要的）**：'
- en: '[PRE11]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Applying mathematical operations to tensors
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对张量应用数学操作
- en: Applying mathematical operations, in particular linear algebra operations, is
    necessary for building most machine learning models. In this subsection, we will
    cover some widely used linear algebra operations, such as element-wise product,
    matrix multiplication, and computing the norm of a tensor.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 应用数学运算，特别是线性代数运算，对于构建大多数机器学习模型是必需的。在本小节中，我们将介绍一些常用的线性代数操作，如元素级乘积、矩阵乘法和计算张量的范数。
- en: 'First, let''s instantiate two random tensors, one with uniform distribution
    in the range [–1, 1) and the other with a standard normal distribution:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们实例化两个随机张量，一个是在区间 [–1, 1) 内均匀分布的张量，另一个是标准正态分布的张量：
- en: '[PRE12]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Notice that `t1` and `t2` have the same shape. Now, to compute the element-wise
    product of `t1` and `t2`, we can use the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`t1` 和 `t2` 具有相同的形状。现在，为了计算 `t1` 和 `t2` 的元素级乘积，我们可以使用以下方法：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To compute the mean, sum, and standard deviation along a certain axis (or axes),
    we can use `tf.math.reduce_mean()`, `tf.math.reduce_sum()`, and `tf.math.reduce_std()`.
    For example, the mean of each column in `t1` can be computed as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要沿某一轴（或多个轴）计算均值、和与标准差，我们可以使用 `tf.math.reduce_mean()`、`tf.math.reduce_sum()`
    和 `tf.math.reduce_std()`。例如，可以按照如下方式计算 `t1` 每一列的均值：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The matrix-matrix product between `t1` and `t2` (that is, ![](img/B13208_13_002.png),
    where the superscript T is for transpose) can be computed by using the `tf.linalg.matmul()`
    function as follows:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '`t1` 和 `t2` 之间的矩阵乘积（即 ![](img/B13208_13_002.png)，其中上标 T 表示转置）可以通过使用 `tf.linalg.matmul()`
    函数按如下方式计算：'
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'On the other hand, computing ![](img/B13208_13_003.png) is performed by transposing
    `t1`, resulting in an array of size ![](img/B13208_13_004.png):'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，计算 ![](img/B13208_13_003.png) 是通过转置 `t1` 来完成的，结果是一个大小为 ![](img/B13208_13_004.png)
    的数组：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Finally, the `tf.norm()` function is useful for computing the ![](img/B13208_13_005.png)
    norm of a tensor. For example, we can calculate the ![](img/B13208_13_006.png)
    norm of `t1` as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`tf.norm()` 函数对于计算张量的 ![](img/B13208_13_005.png) 范数非常有用。例如，我们可以按照如下方式计算 `t1`
    的 ![](img/B13208_13_006.png) 范数：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Split, stack, and concatenate tensors
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拆分、堆叠和连接张量
- en: 'In this subsection, we will cover TensorFlow operations for splitting a tensor
    into multiple tensors, or the reverse: stacking and concatenating multiple tensors
    into a single one.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍 TensorFlow 中的操作，用于将一个张量拆分为多个张量，或执行相反的操作：将多个张量堆叠和连接成一个张量。
- en: 'Assume that we have a single tensor and we want to split it into two or more
    tensors. For this, TensorFlow provides a convenient `tf.split()` function, which
    divides an input tensor into a list of equally-sized tensors. We can determine
    the desired number of splits as an integer using the argument `num_or_size_splits`
    to split a tensor along a desired dimension specified by the `axis` argument.
    In this case, the total size of the input tensor along the specified dimension
    must be divisible by the desired number of splits. Alternatively, we can provide
    the desired sizes in a list. Let''s have a look at an example of both these options:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个单一的张量，并且我们想将其拆分成两个或更多张量。为此，TensorFlow 提供了一个便捷的 `tf.split()` 函数，它将输入张量分割成一个等大小的张量列表。我们可以使用参数
    `num_or_size_splits` 来确定期望的拆分数量，沿着指定的维度（由 `axis` 参数指定）拆分张量。在这种情况下，输入张量在指定维度上的总大小必须能够被期望的拆分数量整除。或者，我们可以提供一个包含期望大小的列表。让我们看一下这两种选项的示例：
- en: '**Providing the number of splits (must be divisible)**:'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供拆分的数量（必须是可整除的）**：'
- en: '[PRE18]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this example, a tensor of size 6 was divided into a list of three tensors
    each with size 2.
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这个例子中，一个大小为 6 的张量被分成了一个由三个大小为 2 的张量组成的列表。
- en: '**Providing the sizes of different splits**:'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供不同拆分的大小**：'
- en: 'Alternatively, instead of defining the number of splits, we can also specify
    the sizes of the output tensors directly. Here, we are splitting a tensor of size
    `5` into tensors of sizes `3` and `2`:'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，除了定义拆分的数量外，我们还可以直接指定输出张量的大小。在这里，我们将一个大小为 `5` 的张量拆分成大小为 `3` 和 `2` 的张量：
- en: '[PRE19]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Sometimes, we are working with multiple tensors and need to concatenate or
    stack them to create a single tensor. In this case, TensorFlow functions such
    as `tf.stack()` and `tf.concat()` come in handy. For example, let''s create a
    1D tensor, `A`, containing 1s with size `3` and a 1D tensor, `B`, containing 0s
    with size `2` and concatenate them into a 1D tensor, `C`, of size `5`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们需要处理多个张量，并将它们连接或堆叠以创建一个单一的张量。在这种情况下，TensorFlow 的函数如 `tf.stack()` 和 `tf.concat()`
    很有用。例如，让我们创建一个大小为 `3` 的包含 1 的一维张量 `A` 和一个大小为 `2` 的包含 0 的一维张量 `B`，并将它们连接成一个大小为
    `5` 的一维张量 `C`：
- en: '[PRE20]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'If we create 1D tensors `A` and `B`, both with size `3`, then we can stack
    them together to form a 2D tensor, `S`:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们创建了大小为`3`的1D张量`A`和`B`，那么我们可以将它们堆叠在一起形成一个2D张量`S`：
- en: '[PRE21]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The TensorFlow API has many operations that you can use for building a model,
    processing your data, and more. However, covering every function is outside the
    scope of this book, where we will focus on the most essential ones. For the full
    list of operations and functions, you can refer to the documentation page of TensorFlow
    at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow API提供了许多操作，可以用于构建模型、处理数据等。然而，涵盖所有的函数超出了本书的范围，本书将重点介绍最基本的操作。有关所有操作和函数的完整列表，可以参考TensorFlow文档页面：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf)。
- en: Building input pipelines using tf.data – the TensorFlow Dataset API
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用tf.data构建输入管道——TensorFlow数据集API
- en: When we are training a deep NN model, we usually train the model incrementally
    using an iterative optimization algorithm such as stochastic gradient descent,
    as we have seen in previous chapters.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练深度神经网络（NN）模型时，我们通常使用迭代优化算法（如随机梯度下降）逐步训练模型，正如我们在前面的章节中所看到的那样。
- en: As mentioned at the beginning of this chapter, the Keras API is a wrapper around
    TensorFlow for building NN models. The Keras API provides a method, `.fit()`,
    for training the models. In cases where the training dataset is rather small and
    can be loaded as a tensor into the memory, TensorFlow models (that are built with
    the Keras API) can directly use this tensor via their `.fit()` method for training.
    In typical use cases, however, when the dataset is too large to fit into the computer
    memory, we will need to load the data from the main storage device (for example,
    the hard drive or solid-state drive) in chunks, that is, batch by batch (note
    the use of the term "batch" instead of "mini-batch" in this chapter to stay close
    to the TensorFlow terminology). In addition, we may need to construct a data-processing
    pipeline to apply certain transformations and preprocessing steps to our data,
    such as mean centering, scaling, or adding noise to augment the training procedure
    and to prevent overfitting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本章开头提到的，Keras API是一个围绕TensorFlow构建神经网络模型的包装器。Keras API提供了一个方法`.fit()`来训练模型。在训练数据集较小并且可以作为张量加载到内存中的情况下，使用Keras
    API构建的TensorFlow模型可以直接通过`.fit()`方法使用这个张量进行训练。然而，在典型的使用场景中，当数据集太大无法完全载入计算机内存时，我们需要从主存储设备（例如硬盘或固态硬盘）按批次加载数据（注意本章使用“批次”而非“迷你批次”以更接近TensorFlow术语）。此外，我们可能需要构建一个数据处理管道，应用某些转换和预处理步骤，如均值中心化、缩放或添加噪声，以增强训练过程并防止过拟合。
- en: Applying preprocessing functions manually every time can be quite cumbersome.
    Luckily, TensorFlow provides a special class for constructing efficient and convenient
    preprocessing pipelines. In this section, we will see an overview of different
    methods for constructing a TensorFlow `Dataset`, including dataset transformations
    and common preprocessing steps.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 每次手动应用预处理函数可能会非常繁琐。幸运的是，TensorFlow提供了一个专门的类来构建高效且便捷的预处理管道。在这一节中，我们将概述不同的构建TensorFlow
    `Dataset`的方法，包括数据集转换和常见的预处理步骤。
- en: Creating a TensorFlow Dataset from existing tensors
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从现有的张量创建一个TensorFlow数据集
- en: 'If the data already exists in the form of a tensor object, a Python list, or
    a NumPy array, we can easily create a dataset using the `tf.data.Dataset.from_tensor_slices()`
    function. This function returns an object of class `Dataset`, which we can use
    to iterate through the individual elements in the input dataset. As a simple example,
    consider the following code, which creates a dataset from a list of values:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据已经以张量对象、Python列表或NumPy数组的形式存在，我们可以使用`tf.data.Dataset.from_tensor_slices()`函数轻松地创建数据集。该函数返回一个`Dataset`类的对象，我们可以用它来逐一遍历输入数据集中的元素。作为一个简单的例子，考虑以下代码，它从一个数值列表中创建数据集：
- en: '[PRE22]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can easily iterate through a dataset entry by entry as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过如下方式轻松地逐条遍历数据集中的每一项：
- en: '[PRE23]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'If we want to create batches from this dataset, with a desired batch size of
    `3`, we can do this as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想从这个数据集中创建批次，设定每个批次的大小为`3`，可以通过以下方式实现：
- en: '[PRE24]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This will create two batches from this dataset, where the first three elements
    go into batch #1, and the remaining elements go into batch #2\. The `.batch()`
    method has an optional argument, `drop_remainder`, which is useful for cases when
    the number of elements in the tensor is not divisible by the desired batch size.
    The default for `drop_remainder` is `False`. We will see more examples illustrating
    the behavior of this method later in the subsection *Shuffle, batch, and repeat*.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从该数据集中创建两个批次，其中前三个元素进入批次#1，剩余的元素进入批次#2。`.batch()`方法有一个可选参数`drop_remainder`，当张量中的元素数量不能被期望的批次大小整除时，`drop_remainder`非常有用。`drop_remainder`的默认值是`False`。我们将在后面的*打乱、批处理和重复*小节中看到更多示例，展示该方法的行为。
- en: Combining two tensors into a joint dataset
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将两个张量组合成一个联合数据集
- en: Often, we may have the data in two (or possibly more) tensors. For example,
    we could have a tensor for features and a tensor for labels. In such cases, we
    need to build a dataset that combines these tensors together, which will allow
    us to retrieve the elements of these tensors in tuples.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们可能会将数据存储在两个（或可能更多）张量中。例如，我们可能会有一个存储特征的张量和一个存储标签的张量。在这种情况下，我们需要构建一个数据集，将这些张量组合在一起，这样我们就可以以元组的形式获取这些张量的元素。
- en: 'Assume that we have two tensors, `t_x` and `t_y`. Tensor `t_x` holds our feature
    values, each of size `3`, and `t_y` stores the class labels. For this example,
    we first create these two tensors as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个张量，`t_x`和`t_y`。张量`t_x`存储我们的特征值，每个大小为`3`，而`t_y`存储类别标签。对于这个例子，我们首先按如下方式创建这两个张量：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, we want to create a joint dataset from these two tensors. Note that there
    is a required one-to-one correspondence between the elements of these two tensors:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想要从这两个张量中创建一个联合数据集。请注意，这两个张量的元素之间需要一一对应：
- en: '[PRE26]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here, we first created two separate datasets, namely `ds_x` and `ds_y`. We
    then used the `zip` function to form a joint dataset. Alternatively, we can create
    the joint dataset using `tf.data.Dataset.from_tensor_slices()` as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们首先创建了两个独立的数据集，分别是`ds_x`和`ds_y`。然后，我们使用`zip`函数将它们组合成一个联合数据集。或者，我们也可以通过`tf.data.Dataset.from_tensor_slices()`来创建联合数据集，方法如下：
- en: '[PRE27]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: which results in the same output.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生相同的输出。
- en: Note that a common source of error could be that the element-wise correspondence
    between the original features (*x*) and labels (*y*) might be lost (for example,
    if the two datasets are shuffled separately). However, once they are merged into
    one dataset, it is safe to apply these operations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，一个常见的错误来源是，原始特征（*x*）和标签（*y*）之间的逐元素对应关系可能会丢失（例如，如果两个数据集分别被打乱）。但是，一旦它们合并为一个数据集，就可以安全地应用这些操作。
- en: 'Next, we will see how to apply transformations to each individual element of
    a dataset. For this, we will use the previous `ds_joint` dataset and apply feature-scaling
    to scale the values to the range [-1, 1), as currently the values of `t_x` are
    in the range [0, 1) based on a random uniform distribution:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何对数据集中的每个元素应用转换。为此，我们将使用之前的`ds_joint`数据集，并应用特征缩放，将值缩放到范围[-1, 1)，因为目前`t_x`的值基于随机均匀分布在[0,
    1)范围内：
- en: '[PRE28]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Applying this sort of transformation can be used for a user-defined function.
    For example, if we have a dataset created from the list of image filenames on
    disk, we can define a function to load the images from these filenames and apply
    that function by calling the `.map()` method. You will see an example of applying
    multiple transformations to a dataset later in this chapter.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 应用这种转换可以用于用户自定义的函数。例如，如果我们有一个由磁盘上图像文件名列表创建的数据集，我们可以定义一个函数从这些文件名中加载图像，并通过调用`.map()`方法应用该函数。你将在本章后面看到一个示例，展示如何对数据集应用多重转换。
- en: Shuffle, batch, and repeat
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 打乱、批处理和重复
- en: As was mentioned in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification,* to train an NN model using stochastic gradient descent optimization,
    it is important to feed training data as randomly shuffled batches. You have already
    seen how to create batches by calling the `.batch()` method of a dataset object.
    Now, in addition to creating batches, you will see how to shuffle and reiterate
    over the datasets. We will continue working with the previous `ds_joint` dataset.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如*第2章：训练简单的机器学习分类算法*中所述，要使用随机梯度下降优化训练神经网络模型，重要的是要将训练数据作为随机打乱的批次输入。你已经看到如何通过调用数据集对象的`.batch()`方法来创建批次。现在，除了创建批次之外，你将看到如何对数据集进行打乱并重复迭代。我们将继续使用之前的`ds_joint`数据集。
- en: 'First, let''s create a shuffled version from the `ds_joint` dataset:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从`ds_joint`数据集中创建一个洗牌版本：
- en: '[PRE29]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: where the rows are shuffled without losing the one-to-one correspondence between
    the entries in `x` and `y`. The `.shuffle()` method requires an argument called
    `buffer_size`, which determines how many elements in the dataset are grouped together
    before shuffling. The elements in the buffer are randomly retrieved and their
    place in the buffer is given to the next elements in the original (unshuffled)
    dataset. Therefore, if we choose a small `buffer_size`, we may not shuffle the
    dataset perfectly.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，行被洗牌，但不失去`x`和`y`之间的一对一对应关系。`.shuffle()`方法需要一个叫做`buffer_size`的参数，该参数决定在洗牌之前，数据集中有多少个元素被组合在一起。缓冲区中的元素会被随机提取，并且其在缓冲区中的位置会被赋给原始（未洗牌）数据集中下一个元素的位置。因此，如果选择一个较小的`buffer_size`，我们可能无法完美地洗牌数据集。
- en: If the dataset is small, choosing a relatively small `buffer_size` may negatively
    affect the predictive performance of the NN as the dataset may not be completely
    randomized. In practice, however, it usually does not have a noticeable effect
    when working with relatively large datasets, which is common in deep learning.
    Alternatively, to ensure complete randomization during each epoch, we can simply
    choose a buffer size that is equal to the number of the training examples, as
    in the preceding code (`buffer_size=len(t_x)`).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集较小，选择一个相对较小的`buffer_size`可能会对神经网络的预测性能产生负面影响，因为数据集可能无法完全随机化。然而，实际上，当处理相对较大的数据集时（深度学习中常见的情况），通常不会产生明显的影响。或者，为了确保在每个epoch期间完全随机化，我们可以选择一个等于训练样本数量的缓冲区大小，就像前面的代码一样（`buffer_size=len(t_x)`）。
- en: 'You will recall that dividing a dataset into batches for model training is
    done by calling the `.batch()` method. Now, let''s create such batches from the
    `ds_joint` dataset and take a look at what a batch looks like:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，为了进行模型训练，将数据集划分为批次是通过调用`.batch()`方法来完成的。现在，让我们从`ds_joint`数据集中创建这样的批次，并看看一个批次是什么样的：
- en: '[PRE30]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In addition, when training a model for multiple epochs, we need to shuffle
    and iterate over the dataset by the desired number of epochs. So, let''s repeat
    the batched dataset twice:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当对模型进行多次训练时，我们需要根据所需的epoch次数来洗牌并迭代数据集。因此，让我们将批处理数据集重复两次：
- en: '[PRE31]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This results in two copies of each batch. If we change the order of these two
    operations, that is, first batch and then repeat, the results will be different:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致每个批次有两份副本。如果我们改变这两个操作的顺序，也就是先批处理然后重复，结果会不同：
- en: '[PRE32]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Notice the difference between the batches. When we first batch and then repeat,
    we get four batches. On the other hand, when repeat is performed first, three
    batches are created.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意批次之间的差异。当我们首先进行批处理然后重复时，会得到四个批次。另一方面，当先进行重复操作时，则会创建三个批次。
- en: 'Finally, to get a better understanding of how these three operations (batch,
    shuffle, and repeat) behave, let''s experiment with them in different orders.
    First, we will combine the operations in the following order: (1) shuffle, (2)
    batch, and (3) repeat:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了更好地理解这三种操作（批处理、洗牌和重复）如何表现，让我们尝试以不同的顺序进行实验。首先，我们将按以下顺序组合这些操作：(1) 洗牌，(2)
    批处理，(3) 重复：
- en: '[PRE33]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now, let''s try a different order: (2) batch, (1) shuffle, and (3) repeat:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试另一种顺序：(2) 批处理，(1) 洗牌，(3) 重复：
- en: '[PRE34]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'While the first code example (shuffle, batch, repeat) appears to have shuffled
    the dataset as expected, we can see that in the second case (batch, shuffle, repeat),
    the elements within a batch were not shuffled at all. We can observe this lack
    of shuffling by taking a closer look at the tensor containing the target values,
    `y`. All batches contain either the pair of values `[y=0, y=1]` or the remaining
    pair of values `[y=2, y=3]`; we do not observe the other possible permutations:
    `[y=2, y=0]`, `[y=1, y=3]`, and so forth. Note that in order to ensure these results
    are not coincidental, you may want to repeat this with a higher number than 3\.
    For example, try it with `.repeat(20)`.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一个代码示例（洗牌、批处理、重复）似乎按预期洗牌了数据集，但我们可以看到在第二种情况下（批处理、洗牌、重复），批次内的元素根本没有被洗牌。通过仔细查看包含目标值`y`的张量，我们可以观察到这种没有洗牌的情况。所有批次要么包含一对值`[y=0,
    y=1]`，要么包含另一对值`[y=2, y=3]`；我们没有观察到其他可能的排列：`[y=2, y=0]`、`[y=1, y=3]`，依此类推。请注意，为了确保这些结果不是巧合，您可能希望使用大于3的重复次数。例如，尝试`.repeat(20)`。
- en: Now, can you predict what will happen if we use the shuffle operation after
    repeat, for example, (2) batch, (3) repeat, (1) shuffle? Give it a try.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你能预测在使用重复操作之后，如果我们使用 shuffle 操作会发生什么吗？例如，(2) 批量处理，(3) 重复操作，(1) 打乱顺序？试试看。
- en: One common source of error is to call `.batch()` twice in a row on a given dataset.
    By doing this, retrieving items from the resulting dataset will create a batch
    of batches of examples. Basically, each time you call `.batch()` on a dataset,
    it will increase the rank of the retrieved tensors by one.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的错误来源是对给定数据集连续调用 `.batch()` 两次。这样做会导致从结果数据集中提取项目时，生成批量的批量示例。基本上，每次对数据集调用
    `.batch()` 时，它都会将提取的张量的维度增加一。
- en: Creating a dataset from files on your local storage disk
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从本地存储磁盘上的文件创建数据集
- en: In this section, we will build a dataset from image files stored on disk. There
    is an image folder associated with the online content of this chapter. After downloading
    the folder, you should be able to see six images of cats and dogs in JPEG format.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从存储在磁盘上的图像文件构建数据集。本章的在线内容包含一个图像文件夹。下载该文件夹后，你应该能够看到六张猫和狗的 JPEG 格式图像。
- en: 'This small dataset will show how building a dataset from stored files generally
    works. To accomplish this, we are going to use two additional modules in TensorFlow:
    `tf.io` to read the image file contents, and `tf.image` to decode the raw contents
    and image resizing.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个小数据集将展示如何从存储的文件中构建数据集的一般过程。为了实现这一点，我们将使用 TensorFlow 中的两个额外模块：`tf.io` 用于读取图像文件内容，`tf.image`
    用于解码原始内容并进行图像大小调整。
- en: '**tf.io and tf.image modules**'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '**tf.io 和 tf.image 模块**'
- en: 'The `tf.io` and `tf.image` modules provide a lot of additional and useful functions,
    which are beyond the scope of the book. You are encouraged to browse through the
    official documentation to learn more about these functions:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.io` 和 `tf.image` 模块提供了许多额外且有用的功能，超出了本书的范围。建议你浏览官方文档，了解更多关于这些功能的内容：'
- en: '[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io)
    for `tf.io`'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/io)
    了解 `tf.io`'
- en: '[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image)
    for `tf.image`'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/image)
    了解 `tf.image`'
- en: 'Before we start, let''s take a look at the content of these files. We will
    use the `pathlib` library to generate a list of image files:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们先看看这些文件的内容。我们将使用 `pathlib` 库生成一个图像文件列表：
- en: '[PRE35]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we will visualize these image examples using Matplotlib:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 Matplotlib 可视化这些图像示例：
- en: '[PRE36]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The following figure shows the example images:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了示例图像：
- en: '![](img/B13208_13_03.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_03.png)'
- en: 'Just from this visualization and the printed image shapes, we can already see
    that the images have different aspect ratios. If you print the aspect ratios (or
    data array shapes) of these images, you will see that some images are 900 pixels
    high and 1200 pixels wide (![](img/B13208_13_008.png)), some are ![](img/B13208_13_009.png),
    and one is ![](img/B13208_13_010.png). Later, we will preprocess these images
    to a consistent size. Another point to consider is that the labels for these images
    are provided within their filenames. So, we extract these labels from the list
    of filenames, assigning label `1` to dogs and label `0` to cats:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 仅凭这个可视化和打印的图像形状，我们已经可以看到图像具有不同的纵横比。如果你打印这些图像的纵横比（或数据数组形状），你会看到一些图像高 900 像素，宽
    1200 像素（![](img/B13208_13_008.png)），有些是 ![](img/B13208_13_009.png)，还有一个是 ![](img/B13208_13_010.png)。稍后，我们将把这些图像预处理为统一的尺寸。另一个需要考虑的点是，这些图像的标签包含在其文件名中。因此，我们将从文件名列表中提取这些标签，将标签
    `1` 分配给狗，将标签 `0` 分配给猫：
- en: '[PRE37]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we have two lists: a list of filenames (or paths of each image) and a
    list of their labels. In the previous section, you already learned two ways of
    creating a joint dataset from two tensors. Here, we will use the second approach
    as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有两个列表：一个是文件名列表（或者每个图像的路径），另一个是它们的标签。在上一节中，你已经学会了从两个张量创建联合数据集的两种方法。这里我们将使用第二种方法，如下所示：
- en: '[PRE38]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We have called this dataset `ds_files_labels`, since it has filenames and labels.
    Next, we need to apply transformations to this dataset: load the image content
    from its file path, decode the raw content, and resize it to a desired size, for
    example, ![](img/B13208_13_011.png). Previously, we saw how to apply a lambda
    function using the `.map()` method. However, since we need to apply multiple preprocessing
    steps this time, we are going to write a helper function instead and use it when
    calling the `.map()` method:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这个数据集命名为 `ds_files_labels`，因为它包含文件名和标签。接下来，我们需要对这个数据集应用转换：从文件路径加载图像内容，解码原始内容，并调整到所需大小，例如，![](img/B13208_13_011.png)。之前，我们已经看过如何使用
    `.map()` 方法应用一个 lambda 函数。然而，由于这次我们需要应用多个预处理步骤，我们将编写一个辅助函数，并在调用 `.map()` 方法时使用它：
- en: '[PRE39]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This results in the following visualization of the retrieved example images,
    along with their labels:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成以下可视化效果，显示检索到的示例图像及其标签：
- en: '![](img/B13208_13_04.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_04.png)'
- en: The `load_and_preprocess()` function wraps all four steps into a single function,
    including the loading of the raw content, decoding it, and resizing the images.
    The function then returns a dataset that we can iterate over and apply other operations
    that we learned about in the previous sections.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '`load_and_preprocess()` 函数将所有四个步骤封装成一个函数，包括加载原始内容、解码和调整图像大小。该函数随后返回一个可以遍历的数据集，并可以应用我们在前面章节中学习的其他操作。'
- en: Fetching available datasets from the tensorflow_datasets library
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 tensorflow_datasets 库中获取可用数据集
- en: The `tensorflow_datasets` library provides a nice collection of freely available
    datasets for training or evaluating deep learning models. The datasets are nicely
    formatted and come with informative descriptions, including the format of features
    and labels and their type and dimensionality, as well as the citation of the original
    paper that introduced the dataset in BibTeX format. Another advantage is that
    these datasets are all prepared and ready to use as `tf.data.Dataset` objects,
    so all the functions we covered in the previous sections can be used directly.
    So, let's see how to use these datasets in action.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`tensorflow_datasets` 库提供了一系列免费可用的数据集，适用于训练或评估深度学习模型。这些数据集格式规范，并附带有信息性描述，包括特征和标签的格式、类型和维度，以及原始论文的引用，引用格式为
    BibTeX。另外一个优点是，这些数据集已经准备好并可以作为 `tf.data.Dataset` 对象直接使用，因此我们在前面章节中介绍的所有函数都可以直接使用。那么，让我们看看如何在实际中使用这些数据集。'
- en: 'First, we need to install the `tensorflow_datasets` library via `pip` from
    the command line:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过命令行使用 `pip` 安装 `tensorflow_datasets` 库：
- en: '[PRE40]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now, let''s import this module and take a look at the list of available datasets:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们导入这个模块，看看可用数据集的列表：
- en: '[PRE41]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The preceding code indicates that there are currently 101 datasets available
    (101 datasets at the time of writing this chapter, but this number will likely
    increase)—we printed the first five datasets to the command line. There are two
    ways of fetching a dataset, which we will cover in the following paragraphs by
    fetching two different datasets: CelebA (`celeb_a`) and the MNIST digit dataset.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码表明，目前有 101 个可用数据集（截至写本章时有 101 个数据集，但这个数字可能会增加）——我们将前五个数据集打印到了命令行。有两种获取数据集的方式，我们将在接下来的段落中介绍，通过获取两个不同的数据集：CelebA（`celeb_a`）和
    MNIST 数字数据集。
- en: 'The first approach consists of three steps:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法包含三个步骤：
- en: Calling the dataset builder function
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用数据集构建函数
- en: Executing the `download_and_prepare()` method
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行 `download_and_prepare()` 方法
- en: Calling the `as_dataset()` method
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `as_dataset()` 方法
- en: 'Let''s work with the first step for the CelebA dataset and print the associated
    description that is provided within the library:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 CelebA 数据集的第一步开始，并打印出库中提供的相关描述：
- en: '[PRE42]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This provides some useful information to understand the structure of this dataset.
    The features are stored as a dictionary with three keys: `''image''`, `''landmarks''`,
    and `''attributes''`.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了一些有用的信息，帮助我们理解该数据集的结构。特征以字典的形式存储，包含三个键：`'image'`、`'landmarks'` 和 `'attributes'`。
- en: The `'image'` entry refers to the face image of a celebrity; `'landmarks'` refers
    to the dictionary of extracted facial points, such as the position of the eyes,
    nose, and so on; and `'attributes'` is a dictionary of 40 facial attributes for
    the person in the image, like facial expression, makeup, hair properties, and
    so on.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`''image''`条目表示名人的面部图像；`''landmarks''`表示提取的面部特征点字典，例如眼睛、鼻子的位置信息等；`''attributes''`是该图像中人物的40个面部特征属性字典，如面部表情、化妆、发型特征等。'
- en: 'Next, we will call the `download_and_prepare()` method. This will download
    the data and store it on disk in a designated folder for all TensorFlow Datasets.
    If you have already done this once, it will simply check whether the data is already
    downloaded so that it does not re-download it if it already exists in the designated
    location:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将调用 `download_and_prepare()` 方法。这将下载数据并将其存储在所有 TensorFlow 数据集的指定文件夹中。如果您已经执行过此操作，它只会检查数据是否已下载，以便在指定位置已经存在数据时不会重新下载：
- en: '[PRE43]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we will instantiate the datasets as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将按如下方式实例化数据集：
- en: '[PRE44]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This dataset is already split into train, test, and validation datasets. In
    order to see what the image examples look like, we can execute the following code:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已经被拆分为训练集、测试集和验证集。为了查看图像示例的样子，我们可以执行以下代码：
- en: '[PRE45]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Note that the elements of this dataset come in a dictionary. If we want to
    pass this dataset to a supervised deep learning model during training, we have
    to reformat it as a tuple of `(features, label)`. For the label, we will use the
    `''Male''` category from the attributes. We will do this by applying a transformation
    via `map()`:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该数据集的元素以字典的形式出现。如果我们想在训练过程中将此数据集传递给一个监督学习的深度学习模型，我们必须将其重新格式化为 `(features,
    label)` 的元组。对于标签，我们将使用来自属性中的 `'Male'` 类别。我们将通过 `map()` 应用转换来完成这一操作：
- en: '[PRE46]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Finally, let''s batch the dataset and take a batch of 18 examples from it to
    visualize them with their labels:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们对数据集进行批处理，并从中提取18个示例批次，以便使用其标签可视化它们：
- en: '[PRE47]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The examples and their labels that are retrieved from `ds_train` are shown
    in the following figure:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从`ds_train`中提取的示例及其标签如下图所示：
- en: '![](img/B13208_13_05.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_05.png)'
- en: This was all we needed to do to fetch and use the CelebA image dataset.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们需要做的所有事情，以获取并使用 CelebA 图像数据集。
- en: 'Next, we will proceed with the second approach for fetching a dataset from
    `tensorflow_datasets`. There is a wrapper function called `load()` that combines
    the three steps for fetching a dataset in one. Let''s see how it can be used to
    fetch the MNIST digit dataset:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将继续使用第二种方法，从 `tensorflow_datasets` 获取数据集。有一个叫做 `load()` 的包装函数，它将获取数据集的三个步骤合并为一个。让我们看看它如何用于获取
    MNIST 数字数据集：
- en: '[PRE48]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As we can see, the MNIST dataset is split into two partitions. Now, we can
    take the train partition, apply a transformation to convert the elements from
    a dictionary to a tuple, and visualize 10 examples:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，MNIST 数据集被拆分为两个部分。现在，我们可以获取训练部分，应用转换将元素从字典转换为元组，并可视化 10 个示例：
- en: '[PRE49]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'The retrieved example handwritten digits from this dataset are shown as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 从该数据集中提取的手写数字示例如下所示：
- en: '![](img/B13208_13_06.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_06.png)'
- en: This concludes our coverage of building and manipulating datasets and fetching
    datasets from the `tensorflow_datasets` library. Next, we will see how to build
    NN models in TensorFlow.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们关于构建和操作数据集以及从 `tensorflow_datasets` 库获取数据集的介绍。接下来，我们将学习如何在 TensorFlow
    中构建神经网络模型。
- en: '**TensorFlow style guide**'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**TensorFlow 风格指南**'
- en: Note that the official TensorFlow style guide ([https://www.tensorflow.org/community/style_guide](https://www.tensorflow.org/community/style_guide))
    recommends using two-character spacing for code indents. However, this book uses
    four characters for indents as it is more consistent with the official Python
    style guide and also helps with displaying the code syntax highlighting in many
    text editors correctly, as well as the accompanying Jupyter code notebooks at
    [https://github.com/rasbt/python-machine-learning-book-3rd-edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，官方 TensorFlow 风格指南（[https://www.tensorflow.org/community/style_guide](https://www.tensorflow.org/community/style_guide)）建议使用两个字符的缩进。然而，本书采用四个字符的缩进，因为它与官方
    Python 风格指南更加一致，并且有助于在许多文本编辑器中正确显示代码语法高亮，以及在[https://github.com/rasbt/python-machine-learning-book-3rd-edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition)上的
    Jupyter 代码笔记本。
- en: Building an NN model in TensorFlow
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中构建神经网络模型
- en: So far in this chapter, you have learned about the basic utility components
    of TensorFlow for manipulating tensors and organizing data into formats that we
    can iterate over during training. In this section, we will finally implement our
    first predictive model in TensorFlow. As TensorFlow is a bit more flexible but
    also more complex than machine learning libraries such as scikit-learn, we will
    start with a simple linear regression model.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，你已经了解了 TensorFlow 中用于操作张量和将数据组织为可以在训练过程中迭代的格式的基本工具组件。在本节中，我们将最终在
    TensorFlow 中实现我们的第一个预测模型。由于 TensorFlow 比如 scikit-learn 等机器学习库更具灵活性，但也更复杂，因此我们将从一个简单的线性回归模型开始。
- en: The TensorFlow Keras API (tf.keras)
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TensorFlow Keras API（tf.keras）
- en: Keras is a high-level NN API and was originally developed to run on top of other
    libraries such as TensorFlow and Theano. Keras provides a user-friendly and modular
    programming interface that allows easy prototyping and the building of complex
    models in just a few lines of code. Keras can be installed independently from
    PyPI and then configured to use TensorFlow as its backend engine. Keras is tightly
    integrated into TensorFlow and its modules are accessible through `tf.keras`.
    In TensorFlow 2.0, `tf.keras` has become the primary and recommended approach
    for implementing models. This has the advantage that it supports TensorFlow-specific
    functionalities, such as dataset pipelines using `tf.data`, which you learned
    about in the previous section. In this book, we will use the `tf.keras` module
    to build NN models.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 是一个高级神经网络（NN）API，最初是为了在其他库（如 TensorFlow 和 Theano）之上运行而开发的。Keras 提供了一个用户友好且模块化的编程接口，允许用户在几行代码内轻松地进行原型设计并构建复杂的模型。Keras
    可以独立安装于 PyPI 中，然后配置为使用 TensorFlow 作为其后端引擎。Keras 与 TensorFlow 紧密集成，其模块可以通过 `tf.keras`
    进行访问。在 TensorFlow 2.0 中，`tf.keras` 已成为实现模型的主要且推荐的方式。这种方式的优点在于它支持 TensorFlow 特有的功能，比如使用
    `tf.data` 构建数据集流水线，正如你在前一节中所学到的那样。在本书中，我们将使用 `tf.keras` 模块来构建神经网络模型。
- en: As you will see in the following subsections, the Keras API (`tf.keras`) makes
    building an NN model extremely easy. The most commonly used approach for building
    an NN in TensorFlow is through `tf.keras.Sequential()`, which allows stacking
    layers to form a network. A stack of layers can be given in a Python list to a
    model defined as `tf.keras.Sequential()`. Alternatively, the layers can be added
    one by one using the `.add()` method.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在以下小节中看到的，Keras API（`tf.keras`）使得构建神经网络模型变得异常简单。构建神经网络的最常用方法是通过 `tf.keras.Sequential()`，该方法允许通过堆叠层来形成网络。层的堆叠可以通过将
    Python 列表传递给定义为 `tf.keras.Sequential()` 的模型来实现。或者，也可以使用 `.add()` 方法逐一添加层。
- en: Furthermore, `tf.keras` allows us to define a model by subclassing `tf.keras.Model`.
    This gives us more control over the forward pass by defining the `call()` method
    for our model class to specify the forward pass explicitly. We will see examples
    of both of these approaches for building an NN model using the `tf.keras` API.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`tf.keras` 允许我们通过继承 `tf.keras.Model` 来定义模型。这使我们可以通过为模型类定义 `call()` 方法，明确地指定前向传播，从而对前向传播过程有更多的控制。我们将看到如何使用这两种方法，通过
    `tf.keras` API 来构建神经网络模型的例子。
- en: Finally, as you will see in the following subsections, models built using the
    `tf.keras` API can be compiled and trained via the `.compile()` and `.fit()` methods.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，正如你将在以下小节中看到的，通过 `tf.keras` API 构建的模型可以通过 `.compile()` 和 `.fit()` 方法进行编译和训练。
- en: Building a linear regression model
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个线性回归模型
- en: 'In this subsection, we will build a simple model to solve a linear regression
    problem. First, let''s create a toy dataset in NumPy and visualize it:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一小节中，我们将构建一个简单的模型来解决线性回归问题。首先，让我们在 NumPy 中创建一个玩具数据集并进行可视化：
- en: '[PRE50]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As a result, the training examples will be shown in a scatterplot as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，训练样本将显示在如下所示的散点图中：
- en: '![](img/B13208_13_07.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_07.png)'
- en: 'Next, we will standardize the features (mean centering and dividing by the
    standard deviation) and create a TensorFlow `Dataset`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对特征进行标准化（均值中心化并除以标准差），并创建一个 TensorFlow `Dataset`：
- en: '[PRE51]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Now, we can define our model for linear regression as ![](img/B13208_13_012.png).
    Here, we are going to use the Keras API. `tf.keras` provides predefined layers
    for building complex NN models, but to start, you will learn how to define a model
    from scratch. Later in this chapter, you will see how to use those predefined
    layers.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以为线性回归定义模型，如下图所示：![](img/B13208_13_012.png)。在这里，我们将使用 Keras API。`tf.keras`
    提供了用于构建复杂神经网络模型的预定义层，但为了开始，你将学习如何从头开始定义模型。在本章稍后的部分，你将看到如何使用那些预定义层。
- en: 'For this regression problem, we will define a new class derived from the `tf.keras.Model`
    class. Subclassing `tf.keras.Model` allows us to use the Keras tools for exploring
    a model, training, and evaluation. In the constructor of our class, we will define
    the parameters of our model, `w` and `b`, which correspond to the weight and the
    bias parameters, respectively. Finally, we will define the `call()` method to
    determine how this model uses the input data to generate its output:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个回归问题，我们将定义一个新的类，继承自`tf.keras.Model`类。通过子类化`tf.keras.Model`，我们可以使用Keras工具来探索模型、训练和评估。在我们类的构造函数中，我们将定义模型的参数`w`和`b`，它们分别对应权重和偏置参数。最后，我们将定义`call()`方法，以确定模型如何使用输入数据生成输出：
- en: '[PRE52]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we will instantiate a new model from the `MyModel()` class that we can
    train based on the training data. The TensorFlow Keras API provides a method named
    `.summary()` for models that are instantiated from `tf.keras.Model`, which allows
    us to get a summary of the model components layer by layer and the number of parameters
    in each layer. Since we have sub-classed our model from `tf.keras.Model`, the
    `.summary()` method is also available to us. But, in order to be able to call
    `model.summary()`, we first need to specify the dimensionality of the input (the
    number of features) to this model. We can do this by calling `model.build()` with
    the expected shape of the input data:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从`MyModel()`类实例化一个新模型，我们可以根据训练数据对其进行训练。TensorFlow Keras API提供了一个名为`.summary()`的方法，用于从`tf.keras.Model`实例化的模型，这允许我们逐层查看模型组件的摘要以及每层的参数数量。由于我们已经将模型从`tf.keras.Model`子类化，因此`.summary()`方法对我们也是可用的。但是，为了能够调用`model.summary()`，我们首先需要指定此模型的输入维度（特征数量）。我们可以通过调用`model.build()`并传递期望的输入数据形状来做到这一点：
- en: '[PRE53]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note that we used `None` as a placeholder for the first dimension of the expected
    input tensor via `model.build()`, which allows us to use an arbitrary batch size.
    However, the number of features is fixed (here `1`) as it directly corresponds
    to the number of weight parameters of the model. Building model layers and parameters
    after instantiation by calling the `.build()` method is called *late variable
    creation*. For this simple model, we already created the model parameters in the
    constructor; therefore, specifying the `input_shape` via `build()` has no further
    effect on our parameters, but still it is needed if we want to call `model.summary()`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过`model.build()`将`None`用作期望输入张量的第一个维度的占位符，这使得我们可以使用任意批次大小。然而，特征数量是固定的（这里为`1`），因为它直接对应于模型的权重参数数量。通过调用`.build()`方法在实例化后构建模型层和参数被称为*延迟变量创建*。对于这个简单的模型，我们已经在构造函数中创建了模型参数；因此，通过`build()`指定`input_shape`对我们的参数没有进一步的影响，但如果我们想调用`model.summary()`，仍然是必要的。
- en: After defining the model, we can define the cost function that we want to minimize
    to find the optimal model weights. Here, we will choose the **mean squared error**
    (**MSE**) as our cost function. Furthermore, to learn the weight parameters of
    the model, we will use stochastic gradient descent. In this subsection, we will
    implement this training via the stochastic gradient descent procedure by ourselves,
    but in the next subsection, we will use the Keras methods `compile()` and `fit()`
    to do the same thing.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 定义模型后，我们可以定义我们想要最小化的代价函数，以找到最优的模型权重。在这里，我们将选择**均方误差**（**MSE**）作为我们的代价函数。此外，为了学习模型的权重参数，我们将使用随机梯度下降。在这一小节中，我们将通过随机梯度下降过程自己实现这个训练，但在下一小节中，我们将使用Keras方法`compile()`和`fit()`来做相同的事情。
- en: 'To implement the stochastic gradient descent algorithm, we need to compute
    the gradients. Rather than manually computing the gradients, we will use the TensorFlow
    API `tf.GradientTape`. We will cover `tf.GradientTape` and its different behaviors
    in *Chapter 14*, *Going Deeper – The Mechanics of TensorFlow*. The code is as
    follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现随机梯度下降算法，我们需要计算梯度。我们将使用TensorFlow API中的`tf.GradientTape`来计算梯度，而不是手动计算梯度。我们将在*第14章*、*深入探讨——TensorFlow的原理*中讲解`tf.GradientTape`及其不同的行为。代码如下：
- en: '[PRE54]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Now, we can set the hyperparameters and train the model for 200 epochs. We
    will create a batched version of the dataset and repeat the dataset with `count=None`,
    which will result in an infinitely repeated dataset:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以设置超参数并训练模型200个周期。我们将创建一个批处理版本的数据集，并通过`count=None`重复数据集，这将导致数据集无限重复：
- en: '[PRE55]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Let''s look at the trained model and plot it. For test data, we will create
    a NumPy array of values evenly spaced between 0 to 9\. Since we trained our model
    with standardized features, we will also apply the same standardization to the
    test data:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看训练后的模型并对其进行绘制。对于测试数据，我们将创建一个 NumPy 数组，包含从 0 到 9 之间均匀间隔的值。由于我们用标准化特征训练了模型，因此我们也会对测试数据应用相同的标准化：
- en: '[PRE56]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The following figure shows the scatterplot of the training examples and the
    trained linear regression model, as well as the convergence history of the weight,
    *w*, and the bias unit, *b*:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了训练样本的散点图以及训练后的线性回归模型，同时还展示了权重 *w* 和偏置单位 *b* 的收敛历史：
- en: '![](img/B13208_13_08.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_08.png)'
- en: Model training via the .compile() and .fit() methods
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 .compile() 和 .fit() 方法训练模型
- en: 'In the previous example, we saw how to train a model by writing a custom function,
    `train()`, and applied stochastic gradient descent optimization. However, writing
    the `train()` function can be a repeatable task across different projects. The
    TensorFlow Keras API provides a convenient `.fit()` method that can be called
    on an instantiated model. To show how this works, let''s create a new model and
    compile it by selecting the optimizer, loss function, and evaluation metrics:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个示例中，我们看到了如何通过编写自定义函数 `train()` 来训练模型，并应用随机梯度下降优化方法。然而，编写 `train()` 函数在不同项目中可能是一个重复的任务。TensorFlow
    Keras API 提供了一个便捷的 `.fit()` 方法，可以在实例化的模型上调用。为了展示这一点，让我们创建一个新模型并通过选择优化器、损失函数和评估指标来编译它：
- en: '[PRE57]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we can simply call the `fit()` method to train the model. We can pass
    a batched dataset (like `ds_train`, which was created in the previous example).
    However, this time you will see that we can pass the NumPy arrays for `x` and
    `y` directly, without needing to create a dataset:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以直接调用 `fit()` 方法来训练模型。我们可以传入一个批量数据集（例如在前一个示例中创建的 `ds_train`）。然而，这一次你会看到，我们可以直接传入
    NumPy 数组作为 `x` 和 `y`，无需再创建数据集：
- en: '[PRE58]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: After the model is trained, visualize the results and make sure that they are
    similar to the results of the previous method.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，进行可视化并确保结果与前一种方法的结果相似。
- en: Building a multilayer perceptron for classifying flowers in the Iris dataset
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建一个多层感知机，用于分类 Iris 数据集中的花卉
- en: 'In the previous example, you saw how to build a model from scratch. We trained
    this model using stochastic gradient descent optimization. While we started our
    journey based on the simplest possible example, you can see that defining the
    model from scratch, even for such a simple case, is neither appealing nor a good
    practice. TensorFlow instead provides already defined layers through `tf.keras.layers`
    that can be readily used as the building blocks of an NN model. In this section,
    you will learn how to use these layers to solve a classification task using the
    Iris flower dataset and build a two-layer perceptron using the Keras API. First,
    let''s get the data from `tensorflow_datasets`:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个示例中，你看到了如何从零开始构建一个模型。我们使用随机梯度下降优化方法训练了这个模型。虽然我们从最简单的例子入手，但你可以看到，即使是如此简单的情况，完全从零定义模型既不吸引人也不具备良好的实践性。相反，TensorFlow
    提供了通过 `tf.keras.layers` 预定义的层，可以直接作为神经网络模型的构建模块。在这一部分，你将学习如何使用这些层来解决 Iris 花卉数据集的分类任务，并使用
    Keras API 构建一个两层感知机。首先，让我们从 `tensorflow_datasets` 获取数据：
- en: '[PRE59]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This prints some information about this dataset (not printed here to save space).
    However, you will notice in the shown information that this dataset comes with
    only one partition, so we have to split the dataset into training and testing
    partitions (and validation for proper machine learning practice) on our own. Let's
    assume that we want to use two-thirds of the dataset for training and keep the
    remaining examples for testing. The `tensorflow_datasets` library provides a convenient
    tool that allows us to determine slices and splits via the `DatasetBuilder` object
    prior to loading a dataset. You can find out more about splits at [https://www.tensorflow.org/datasets/splits](https://www.tensorflow.org/datasets/splits).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印一些关于数据集的信息（为节省空间，本文未打印）。然而，你会在显示的信息中注意到该数据集只有一个分区，因此我们必须自行将数据集拆分为训练集和测试集（并且为符合机器学习的最佳实践，也需要有验证集）。假设我们希望使用数据集的三分之二进行训练，其余部分用于测试。`tensorflow_datasets`
    库提供了一个便捷的工具，允许我们在加载数据集之前通过 `DatasetBuilder` 对象来确定数据集的切片和拆分。你可以在 [https://www.tensorflow.org/datasets/splits](https://www.tensorflow.org/datasets/splits)
    了解更多有关数据拆分的信息。
- en: 'An alternative approach is to load the entire dataset first and then use `.take()`
    and `.skip()` to split the dataset to two partitions. If the dataset is not shuffled
    at first, we can also shuffle the dataset. However, we need to be very careful
    with this because it can lead to mixing the train/test examples, which is not
    acceptable in machine learning. To avoid this, we have to set an argument, `reshuffle_each_iteration=False`,
    in the `.shuffle()` method. The code for splitting the dataset into train/test
    is as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是先加载整个数据集，然后使用 `.take()` 和 `.skip()` 将数据集分成两个部分。如果数据集开始时没有进行洗牌，我们也可以对数据集进行洗牌。然而，我们需要非常小心，因为这可能导致训练集和测试集混合，这在机器学习中是不可接受的。为了避免这种情况，我们必须在
    `.shuffle()` 方法中设置一个参数，`reshuffle_each_iteration=False`。将数据集拆分为训练集/测试集的代码如下：
- en: '[PRE60]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Next, as you have already seen in the previous sections, we need to apply a
    transformation via the `.map()` method to convert the dictionary to a tuple:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，正如你在前面的部分中已经看到的，我们需要通过 `.map()` 方法应用一个转换，将字典转换为元组：
- en: '[PRE61]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: Now, we are ready to use the Keras API to build a model efficiently. In particular,
    using the `tf.keras.Sequential` class, we can stack a few Keras layers and build
    an NN. You can see the list of all the Keras layers that are already available
    at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers).
    For this problem, we are going to use the `Dense` layer (`tf.keras.layers.Dense`),
    which is also known as a fully connected (FC) layer or linear layer, and can be
    best represented by ![](img/B13208_13_013.png), where *x* is the input features,
    *w* and *b* are the weight matrix and the bias vector, and *f* is the activation
    function.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好使用 Keras API 高效地构建模型。特别是，使用 `tf.keras.Sequential` 类，我们可以堆叠一些 Keras
    层并构建一个神经网络。你可以在[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers)查看所有已存在的
    Keras 层的列表。对于这个问题，我们将使用 `Dense` 层（`tf.keras.layers.Dense`），它也被称为全连接（FC）层或线性层，可以通过
    ![](img/B13208_13_013.png) 最好地表示，其中 *x* 是输入特征，*w* 和 *b* 是权重矩阵和偏置向量，*f* 是激活函数。
- en: 'If you think of the layers in an NN, each layer receives its inputs from the
    preceding layer; therefore, its dimensionality (rank and shape) is fixed. Typically,
    we need to concern ourselves with the dimensionality of output only when we design
    an NN architecture. (Note: the first layer is the exception, but TensorFlow/Keras
    allows us to decide the input dimensionality of the first layer after defining
    the model through *late variable creation*.) Here, we want to define a model with
    two hidden layers. The first one receives an input of four features and projects
    them to 16 neurons. The second layer receives the output of the previous layer
    (which has size `16`) and projects them to three output neurons, since we have
    three class labels. This can be done using the `Sequential` class and the `Dense`
    layer in Keras as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑神经网络中的每一层，每一层都会从前一层接收输入；因此，它的维度（秩和形状）是固定的。通常，我们只在设计神经网络架构时才需要关心输出的维度。（注意：第一层是例外，但
    TensorFlow/Keras 允许我们在定义模型后通过*延迟变量创建*来决定第一层的输入维度。）在这里，我们希望定义一个包含两层隐藏层的模型。第一层接收四个特征的输入，并将其投影到16个神经元上。第二层接收前一层的输出（其大小为`16`），并将其投影到三个输出神经元上，因为我们有三个类别标签。这可以通过
    Keras 中的 `Sequential` 类和 `Dense` 层来实现，如下所示：
- en: '[PRE62]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: Notice that we determined the input shape for the first layer via `input_shape=(4,)`,
    and therefore, we did not have to call `.build()` anymore in order to use `iris_model.summary()`.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们通过 `input_shape=(4,)` 确定了第一层的输入形状，因此我们不再需要调用 `.build()` 来使用 `iris_model.summary()`。
- en: The printed model summary indicates that the first layer (`fc1`) has 80 parameters,
    and the second layer has 51 parameters. You can verify that by ![](img/B13208_13_014.png),
    where ![](img/B13208_13_015.png) is the number of input units, and ![](img/B13208_13_016.png)
    is the number of output units. Recall that for a fully (densely) connected layer,
    the learnable parameters are the weight matrix of size ![](img/B13208_13_017.png)
    and the bias vector of size ![](img/B13208_13_016.png). Furthermore, notice that
    we used the sigmoid activation function for the first layer and softmax activation
    for the last (output) layer. Softmax activation in the last layer is used to support
    multi-class classification, since here we have three class labels (which is why
    we have three neurons at the output layer). We will discuss the different activation
    functions and their applications later in this chapter.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的模型摘要表明，第一个层（`fc1`）有80个参数，第二个层有51个参数。你可以通过查看![](img/B13208_13_014.png)来验证这一点，其中![](img/B13208_13_015.png)是输入单元的数量，![](img/B13208_13_016.png)是输出单元的数量。回想一下，对于全连接层（密集连接层），可学习的参数是大小为![](img/B13208_13_017.png)的权重矩阵和大小为![](img/B13208_13_016.png)的偏置向量。此外，请注意，我们为第一个层使用了sigmoid激活函数，并为最后一层（输出层）使用了softmax激活函数。最后一层的softmax激活函数用于支持多类分类，因为在这里我们有三个类别标签（这也是为什么输出层有三个神经元）。我们将在本章稍后讨论不同的激活函数及其应用。
- en: 'Next, we will compile this model to specify the loss function, the optimizer,
    and the metrics for evaluation:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译此模型以指定损失函数、优化器和评估指标：
- en: '[PRE63]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Now, we can train the model. We will specify the number of epochs to be `100`
    and the batch size to be `2`. In the following code, we will build an infinitely
    repeating dataset, which will be passed to the `fit()` method for training the
    model. In this case, in order for the `fit()` method to be able to keep track
    of the epochs, it needs to know the number of steps for each epoch.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练模型。我们将指定训练的epoch数量为`100`，batch大小为`2`。在以下代码中，我们将构建一个无限重复的数据集，并将其传递给`fit()`方法来训练模型。在这种情况下，为了让`fit()`方法能够跟踪epoch数量，它需要知道每个epoch的步数。
- en: 'Given the size of our training data (here, `100`) and the batch size (`batch_size`),
    we can determine the number of steps in each epoch, `steps_per_epoch`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的训练数据的大小（这里是`100`）和batch大小（`batch_size`），我们可以确定每个epoch中的步数`steps_per_epoch`：
- en: '[PRE64]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The returned variable history keeps the training loss and the training accuracy
    (since they were specified as metrics to `iris_model.compile()`) after each epoch.
    We can use this to visualize the learning curves as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的变量history保存了每个epoch之后的训练损失和训练准确度（因为它们在`iris_model.compile()`中被指定为评估指标）。我们可以使用它来可视化学习曲线，如下所示：
- en: '[PRE65]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The learning curves (training loss and training accuracy) are as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 学习曲线（训练损失和训练准确度）如下：
- en: '![](img/B13208_13_09.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_09.png)'
- en: Evaluating the trained model on the test dataset
  id: totrans-257
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在测试数据集上评估训练过的模型
- en: 'Since we specified `''accuracy''` as our evaluation metric in `iris_model.compile()`,
    we can now directly evaluate the model on the test dataset:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在`iris_model.compile()`中指定了`'accuracy'`作为评估指标，现在我们可以直接在测试数据集上评估模型：
- en: '[PRE66]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Notice that we have to batch the test dataset as well, to ensure that the input
    to the model has the correct dimension (rank). As we discussed earlier, calling
    `.batch()` will increase the rank of the retrieved tensors by 1\. The input data
    for `.evaluate()` must have one designated dimension for the batch, although here
    (for evaluation), the size of the batch does not matter. Therefore, if we pass
    `ds_batch.batch(50)` to the `.evaluate()` method, the entire test dataset will
    be processed in one batch of size 50, but if we pass `ds_batch.batch(1)`, 50 batches
    of size 1 will be processed.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们也需要对测试数据集进行batch处理，以确保输入到模型的数据具有正确的维度（秩）。正如我们之前讨论的，调用`.batch()`会将获取的张量的秩增加1。`.evaluate()`方法的输入数据必须具有指定的批次维度，尽管在这里（评估时）批次大小并不重要。因此，如果我们将`ds_batch.batch(50)`传递给`.evaluate()`方法，整个测试数据集将在一个大小为50的批次中处理，但如果我们传递`ds_batch.batch(1)`，则会处理50个大小为1的批次。
- en: Saving and reloading the trained model
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保存和重新加载训练好的模型
- en: 'Trained models can be saved on disk for future use. This can be done as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型可以保存到磁盘以供将来使用。可以通过以下方式进行保存：
- en: '[PRE67]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: The first option is the filename. Calling `iris_model.save()` will save both
    the model architecture and all the learned parameters. However, if you want to
    save only the architecture, you can use the `iris_model.to_json()` method, which
    saves the model configuration in JSON format. Or if you want to save only the
    model weights, you can do that by calling `iris_model.save_weights()`. The `save_format`
    can be specified to be either `'h5'` for the HDF5 format or `'tf'` for TensorFlow
    format.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个选项是文件名。调用`iris_model.save()`将保存模型架构和所有学习到的参数。然而，如果你只想保存架构，可以使用`iris_model.to_json()`方法，这将以JSON格式保存模型配置。如果你只想保存模型权重，可以通过调用`iris_model.save_weights()`来实现。`save_format`可以指定为'h5'（HDF5格式）或'tf'（TensorFlow格式）。
- en: 'Now, let''s reload the saved model. Since we have saved both the model architecture
    and the weights, we can easily rebuild and reload the parameters in just one line:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们重新加载已保存的模型。由于我们已经保存了模型架构和权重，我们可以通过一行代码轻松重建和重新加载参数：
- en: '[PRE68]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: Try to verify the model architecture by calling `iris_model_new.summary()`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试通过调用`iris_model_new.summary()`来验证模型架构。
- en: 'Finally, let''s evaluate this new model that is reloaded on the test dataset
    to verify that the results are the same as before:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们评估这个新模型，并在测试数据集上重新加载它，以验证结果是否与之前相同：
- en: '[PRE69]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Choosing activation functions for multilayer neural networks
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为多层神经网络选择激活函数
- en: For simplicity, we have only discussed the sigmoid activation function in the
    context of multilayer feedforward NNs so far; we used it in the hidden layer as
    well as the output layer in the MLP implementation in *Chapter 12*, *Implementing
    a Multilayer Artificial Neural Network from Scratch*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简便起见，我们到目前为止只讨论了在多层前馈神经网络中的sigmoid激活函数；我们在*第12章*中的多层感知机实现中，既在隐藏层也在输出层使用了它（*从头开始实现多层人工神经网络*）。
- en: Note that in this book, the sigmoidal logistic function, ![](img/B13208_13_033.png),
    is referred to as the *sigmoid* function for brevity, which is common in machine
    learning literature. In the following subsections, you will learn more about alternative
    nonlinear functions that are useful for implementing multilayer NNs.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在本书中，S形逻辑函数（![](img/B13208_13_033.png)）为了简洁起见被称为*sigmoid*函数，这在机器学习文献中是很常见的术语。在接下来的各小节中，您将了解更多关于实现多层神经网络（NN）时有用的其他非线性函数。
- en: Technically, we can use any function as an activation function in multilayer
    NNs as long as it is differentiable. We can even use linear activation functions,
    such as in Adaline (*Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*). However, in practice, it would not be very useful to use
    linear activation functions for both hidden and output layers, since we want to
    introduce nonlinearity in a typical artificial NN to be able to tackle complex
    problems. The sum of linear functions yields a linear function after all.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，只要激活函数是可微分的，我们可以在多层神经网络中使用任何函数。我们甚至可以使用线性激活函数，比如在Adaline中（*第2章*，*训练简单的机器学习算法进行分类*）。然而，实际上，如果我们在隐藏层和输出层都使用线性激活函数，这并不是很有用，因为在典型的人工神经网络中，我们希望引入非线性，以便能够处理复杂的问题。毕竟，线性函数的和仍然是线性函数。
- en: The logistic (sigmoid) activation function that we used in *Chapter 12* probably
    mimics the concept of a neuron in a brain most closely—we can think of it as the
    probability of whether a neuron fires. However, the logistic (sigmoid) activation
    function can be problematic if we have highly negative input, since the output
    of the sigmoid function will be close to zero in this case. If the sigmoid function
    returns output that is close to zero, the NN will learn very slowly, and it will
    be more likely to get trapped in the local minima during training. This is why
    people often prefer a hyperbolic tangent as an activation function in hidden layers.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第12章*中使用的逻辑（sigmoid）激活函数可能最接近模仿大脑中神经元的概念——我们可以将其视为神经元是否发火的概率。然而，如果输入值非常负，逻辑（sigmoid）激活函数可能会出现问题，因为此时sigmoid函数的输出将接近零。如果sigmoid函数返回接近零的输出，神经网络的学习速度将非常慢，并且在训练过程中更容易陷入局部最小值。这就是为什么人们通常更倾向于在隐藏层使用双曲正切作为激活函数的原因。
- en: Before we discuss what a hyperbolic tangent looks like, let’s briefly recapitulate
    some of the basics of the logistic function and look at a generalization that
    makes it more useful for multilabel classification problems.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论双曲正切的具体形态之前，让我们简要回顾一下逻辑函数的一些基本内容，并看看它的推广形式，看看它如何在多标签分类问题中更有用。
- en: Logistic function recap
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑函数回顾
- en: As was mentioned in the introduction to this section, the logistic function
    is, in fact, a special case of a sigmoid function. You will recall from the section
    on logistic regression in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*, that we can use a logistic function to model the probability
    that sample *x* belongs to the positive class (class `1`) in a binary classification
    task.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节介绍中所提到的，Logistic函数实际上是Sigmoid函数的一个特例。你会从 *第3章*，《使用scikit-learn的机器学习分类器巡礼》中关于Logistic回归的部分回忆起，我们可以使用Logistic函数来建模样本
    *x* 在二分类任务中属于正类（类别 `1`）的概率。
- en: 'The given net input, *z*, is shown in the following equation:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 给定的净输入 *z* 如下方公式所示：
- en: '![](img/B13208_13_034.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_034.png)'
- en: 'The logistic (sigmoid) function will compute the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: Logistic（Sigmoid）函数将计算以下内容：
- en: '![](img/B13208_13_035.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_035.png)'
- en: 'Note that ![](img/B13208_13_036.png) is the bias unit (*y*-axis intercept,
    which means ![](img/B13208_13_037.png)). To provide a more concrete example, let’s
    take a model for a two-dimensional data point, *x*, and a model with the following
    weight coefficients assigned to the *w* vector:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，![](img/B13208_13_036.png)是偏置单元（*y*-轴截距，这意味着![](img/B13208_13_037.png)）。为了提供一个更具体的例子，假设我们有一个二维数据点
    *x* 的模型，并且该模型分配给 *w* 向量以下的权重系数：
- en: '[PRE70]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: If we calculate the net input (*z*) and use it to activate a logistic neuron
    with those particular feature values and weight coefficients, we get a value of
    `0.888`, which we can interpret as an 88.8 percent probability that this particular
    sample, *x*, belongs to the positive class.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算净输入（*z*）并用它来激活具有这些特定特征值和权重系数的Logistic神经元，我们得到一个`0.888`的值，我们可以将其解释为该特定样本
    *x* 属于正类的88.8%概率。
- en: 'In *Chapter 12*, we used the one-hot encoding technique to represent multiclass
    ground truth labels and designed the output layer consisting of multiple logistic
    activation units. However, as will be demonstrated by the following code example,
    an output layer consisting of multiple logistic activation units does not produce
    meaningful, interpretable probability values:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *第12章* 中，我们使用了独热编码技术来表示多类的真实标签，并设计了由多个Logistic激活单元组成的输出层。然而，正如以下代码示例所演示的，包含多个Logistic激活单元的输出层并没有生成有意义的、可解释的概率值：
- en: '[PRE71]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'As you can see in the output, the resulting values cannot be interpreted as
    probabilities for a three-class problem. The reason for this is that they do not
    sum up to 1\. However, this is, in fact, not a big concern if we use our model
    to predict only the class labels and not the class membership probabilities. One
    way to predict the class label from the output units obtained earlier is to use
    the maximum value:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在输出中所看到的，得到的值不能解释为三类问题的概率。原因是这些值的总和不等于1。然而，如果我们仅使用我们的模型来预测类别标签，而不是类别成员概率，这实际上并不是一个大问题。从先前得到的输出单元中预测类别标签的一种方法是使用最大值：
- en: '[PRE72]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: In certain contexts, it can be useful to compute meaningful class probabilities
    for multiclass predictions. In the next section, we will take a look at a generalization
    of the logistic function, the `softmax` function, which can help us with this
    task.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，计算多类预测的有意义类别概率是有用的。在接下来的部分，我们将介绍Logistic函数的推广版，即`softmax`函数，它可以帮助我们完成这一任务。
- en: Estimating class probabilities in multiclass classification via the softmax
    function
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过softmax函数估计多类分类中的类别概率
- en: In the previous section, you saw how we can obtain a class label using the `argmax`
    function. Previously, in the section *Building a multilayer perceptron for classifying
    flowers in the Iris dataset*, we determined `activation=’softmax’` in the last
    layer of the MLP model. The `softmax` function is a soft form of the `argmax`
    function; instead of giving a single class index, it provides the probability
    of each class. Therefore, it allows us to compute meaningful class probabilities
    in multiclass settings (multinomial logistic regression).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一部分，你看到我们如何使用`argmax`函数获得类别标签。之前，在《为鸢尾花数据集构建多层感知机进行分类》部分中，我们确定了MLP模型最后一层的`activation='softmax'`。`softmax`函数是`argmax`函数的一种软形式；它不是给出一个单一的类别索引，而是提供每个类别的概率。因此，它允许我们在多类设置中计算有意义的类别概率（多项式Logistic回归）。
- en: 'In `softmax`, the probability of a particular sample with net input *z* belonging
    to the *i*th class can be computed with a normalization term in the denominator,
    that is, the sum of the exponentially weighted linear functions:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在`softmax`中，某个样本的净输入*z*属于第*i*类的概率可以通过分母中的归一化项计算，即所有指数加权线性函数的和：
- en: '![](img/B13208_13_038.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_038.png)'
- en: 'To see `softmax` in action, let’s code it up in Python:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看`softmax`的实际应用，我们可以用Python来实现：
- en: '[PRE73]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: As you can see, the predicted class probabilities now sum up to 1, as we would
    expect. It is also notable that the predicted class label is the same as when
    we applied the `argmax` function to the logistic output.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，预测的类别概率现在加起来为1，这正是我们所期望的。值得注意的是，预测的类别标签与我们对逻辑输出应用`argmax`函数时得到的结果相同。
- en: 'It may help to think of the result of the `softmax` function as a *normalized*
    output that is useful for obtaining meaningful class-membership predictions in
    multiclass settings. Therefore, when we build a multiclass classification model
    in TensorFlow, we can use the `tf.keras.activations.softmax()` function to estimate
    the probabilities of each class membership for an input batch of examples. To
    see how we can use the `softmax` activation function in TensorFlow, in the following
    code, we will convert `Z` to a tensor, with an additional dimension reserved for
    the batch size:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将`softmax`函数的结果看作是一个*归一化*输出，这对于在多分类设置中获取有意义的类别成员预测非常有用。因此，当我们在TensorFlow中构建多分类模型时，我们可以使用`tf.keras.activations.softmax()`函数来估计输入批次样本每个类别的概率。为了展示如何在TensorFlow中使用`softmax`激活函数，在以下代码中，我们将`Z`转换为张量，并为批次大小预留额外的维度：
- en: '[PRE74]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Broadening the output spectrum using a hyperbolic tangent
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用双曲正切扩展输出谱
- en: 'Another sigmoidal function that is often used in the hidden layers of artificial
    NNs is the **hyperbolic tangent** (commonly known as **tanh**), which can be interpreted
    as a rescaled version of the logistic function:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用于人工神经网络隐藏层的S形函数是**双曲正切**（通常称为**tanh**），它可以被解释为逻辑函数的缩放版本：
- en: '![](img/B13208_13_039.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_039.png)'
- en: 'The advantage of the hyperbolic tangent over the logistic function is that
    it has a broader output spectrum ranging in the open interval (–1, 1), which can
    improve the convergence of the back-propagation algorithm (*Neural Networks for
    Pattern Recognition*, *C. M. Bishop*, *Oxford University Press*, pages: 500-501,
    *1995*).'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数相较于逻辑函数的优势在于，它有一个更广泛的输出谱，范围在开区间（–1, 1）内，这可以改善反向传播算法的收敛性（*模式识别中的神经网络*，*C.
    M. Bishop*，*牛津大学出版社*，第500-501页，*1995*）。
- en: 'In contrast, the logistic function returns an output signal ranging in the
    open interval (0, 1). For a simple comparison of the logistic function and the
    hyperbolic tangent, let’s plot the two sigmoidal functions:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，逻辑函数的输出信号范围在开区间(0, 1)内。为了简单对比逻辑函数和双曲正切函数，我们可以绘制这两个S形函数：
- en: '[PRE75]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As you can see, the shapes of the two sigmoidal curves look very similar; however,
    the `tanh` function has double the output space of the `logistic` function:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这两个S形曲线的形状非常相似；然而，`tanh`函数的输出空间是`logistic`函数的两倍：
- en: '![](img/B13208_13_10.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_10.png)'
- en: Note that we previously implemented the `logistic` and `tanh` functions verbosely
    for the purpose of illustration. In practice, we can use NumPy’s `tanh` function.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们之前为了演示目的详细实现了`logistic`和`tanh`函数。实际上，我们可以使用NumPy的`tanh`函数。
- en: 'Alternatively, when building an NN model, we can use `tf.keras.activations.tanh()`
    in TensorFlow to achieve the same results:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在构建神经网络模型时，我们可以在TensorFlow中使用`tf.keras.activations.tanh()`来实现相同的效果：
- en: '[PRE76]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'In addition, the logistic function is available in SciPy’s `special` module:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，逻辑函数可以在SciPy的`special`模块中找到：
- en: '[PRE77]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Similarly, we can use the `tf.keras.activations.sigmoid()` function in TensorFlow
    to do the same computation, as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以在TensorFlow中使用`tf.keras.activations.sigmoid()`函数进行相同的计算，代码如下：
- en: '[PRE78]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Rectified linear unit activation
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 整流线性单元激活
- en: '**Rectified linear unit** (**ReLU**) is another activation function that is
    often used in deep NNs. Before we delve into ReLU, we should step back and understand
    the vanishing gradient problem of tanh and logistic activations.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '**整流线性单元**（**ReLU**）是另一个常用于深度神经网络中的激活函数。在深入研究ReLU之前，我们需要回顾一下tanh和逻辑激活函数的梯度消失问题。'
- en: To understand this problem, let’s assume that we initially have the net input
    ![](img/B13208_13_040.png), which changes to ![](img/B13208_13_041.png). Computing
    the tanh activation, we get ![](img/B13208_13_042.png) and ![](img/B13208_13_043.png),
    which shows no change in the output (due to the asymptotic behavior of the tanh
    function and numerical errors).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个问题，我们假设最初我们有净输入 ![](img/B13208_13_040.png)，它变化为 ![](img/B13208_13_041.png)。计算
    tanh 激活函数后，我们得到 ![](img/B13208_13_042.png) 和 ![](img/B13208_13_043.png)，显示输出没有变化（由于
    tanh 函数的渐近行为和数值误差）。
- en: 'This means that the derivative of activations with respect to the net input
    diminishes as *z* becomes large. As a result, learning weights during the training
    phase becomes very slow because the gradient terms may be very close to zero.
    ReLU activation addresses this issue. Mathematically, ReLU is defined as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，当 *z* 变得很大时，激活函数对净输入的导数会减小。因此，在训练阶段，学习权重变得非常缓慢，因为梯度项可能非常接近零。ReLU 激活函数解决了这个问题。从数学上讲，ReLU
    定义如下：
- en: '![](img/B13208_13_044.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_044.png)'
- en: 'ReLU is still a nonlinear function that is good for learning complex functions
    with NNs. Besides this, the derivative of ReLU, with respect to its input, is
    always 1 for positive input values. Therefore, it solves the problem of vanishing
    gradients, making it suitable for deep NNs. In TensorFlow, we can apply the ReLU
    activation as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU 仍然是一个非线性函数，适合用神经网络学习复杂的函数。除此之外，ReLU 对其输入的导数对于正输入值始终为 1。因此，它解决了梯度消失的问题，使其适合深度神经网络。在
    TensorFlow 中，我们可以如下应用 ReLU 激活函数：
- en: '[PRE79]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: We will use the ReLU activation function in the next chapter as an activation
    function for multilayer convolutional NNs.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将使用 ReLU 激活函数作为多层卷积神经网络的激活函数。
- en: 'Now that we know more about the different activation functions that are commonly
    used in artificial NNs, let’s conclude this section with an overview of the different
    activation functions that we encountered in this book:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们了解了更多关于常用的人工神经网络激活函数，让我们通过概述本书中遇到的不同激活函数来总结这一部分：
- en: '![](img/B13208_13_11.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_13_11.png)'
- en: You can find the list of all activation functions available in the Keras API
    at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/activations)
    查找 Keras API 中所有可用的激活函数列表。
- en: Summary
  id: totrans-325
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned how to use TensorFlow, an open source library for
    numerical computations, with a special focus on deep learning. While TensorFlow
    is more inconvenient to use than NumPy, due to its additional complexity to support
    GPUs, it allows us to define and train large, multilayer NNs very efficiently.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用 TensorFlow —— 一个开源的数值计算库，特别关注深度学习。虽然 TensorFlow 比 NumPy 更不方便使用，因为它需要额外的复杂性来支持
    GPU，但它允许我们非常高效地定义和训练大型多层神经网络。
- en: Also, you learned about using the TensorFlow Keras API to build complex machine
    learning and NN models and run them efficiently. We explored model building in
    TensorFlow by defining a model from scratch via subclassing the `tf.keras.Model`
    class. Implementing models can be tedious when we have to program at the level
    of matrix-vector multiplications and define every detail of each operation. However,
    the advantage is that this allows us, as developers, to combine such basic operations
    and build more complex models. We then explored `tf.keras.layers`, which makes
    building NN models a lot easier than implementing them from scratch.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还学习了如何使用 TensorFlow Keras API 来构建复杂的机器学习和神经网络模型并高效运行它们。我们通过继承 `tf.keras.Model`
    类从零开始定义模型，探索了 TensorFlow 中的模型构建。当我们必须在矩阵-向量乘法的层面编程并定义每个操作的细节时，模型的实现可能会很繁琐。然而，优点是，这使得我们作为开发人员能够将这些基础操作结合起来，构建更复杂的模型。接着，我们探索了
    `tf.keras.layers`，它使得构建神经网络模型比从头实现它们要容易得多。
- en: Finally, you learned about different activation functions and understood their
    behaviors and applications. Specifically, in this chapter, we covered tanh, softmax,
    and ReLU.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你学习了不同的激活函数，并理解了它们的行为和应用。具体来说，在本章中，我们讨论了 tanh、softmax 和 ReLU。
- en: In the next chapter, we’ll continue our journey and dive deeper into TensorFlow,
    where we’ll find ourselves working with TensorFlow function decoration and TensorFlow
    Estimators. Along the way, you’ll learn many new concepts, such as variables and
    feature columns.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将继续我们的旅程，深入探讨 TensorFlow，在那里我们将与 TensorFlow 函数装饰器和 TensorFlow Estimators
    一起工作。在这个过程中，你将学习到许多新概念，例如变量和特征列。
