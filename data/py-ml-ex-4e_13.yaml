- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Advancing Language Understanding and Generation with the Transformer Models
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过Transformer模型推进语言理解和生成
- en: In the previous chapter, we focused on RNNs and used them to deal with sequence
    learning tasks. However, RNNs may easily suffer from the vanishing gradient problem.
    In this chapter, we will explore the Transformer neural network architecture,
    which is designed for sequence-to-sequence tasks and is particularly well suited
    for **Natural Language Processing** (**NLP**). The key innovation is the self-attention
    mechanism, allowing the model to weigh different parts of the input sequence differently,
    and enabling it to capture long-range dependencies more effectively than RNNs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章节中，我们专注于RNN，并使用它们处理序列学习任务。然而，RNN容易受到梯度消失问题的影响。在本章节中，我们将探讨Transformer神经网络架构，它是为序列到序列任务设计的，特别适用于**自然语言处理**（**NLP**）。其关键创新是自注意力机制，使得模型能够对输入序列的不同部分进行不同权重的加权，从而比RNN更有效地捕捉长程依赖关系。
- en: We will learn two cutting-edge models utilizing the Transformer architecture
    and delve into their practical applications, such as sentiment analysis and text
    generation. Expect enhanced performance on tasks previously covered in the preceding
    chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习两种利用Transformer架构的前沿模型，并深入探讨它们的实际应用，如情感分析和文本生成。预计在前一章节所涉及的任务中，性能将得到提升。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节将涵盖以下主题：
- en: Understanding self-attention
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解自注意力
- en: Exploring the Transformer’s architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Transformer架构
- en: Improving sentiment analysis with **Bidirectional Encoder Representations from
    Transformers** (**BERT**) and Transformers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**双向编码器表示从变换器**（**BERT**）和Transformer改善情感分析
- en: Generating text using **Generative Pre-trained Transformers** (**GPT**)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成文本使用**生成预训练变换器**（**GPT**）
- en: Understanding self-attention
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解自注意力
- en: The Transformer neural network architecture revolves around the self-attention
    mechanism. So, let’s first kick off the chapter by looking at this. **Self-attention**
    is a mechanism used in machine learning, particularly in NLP and computer vision.
    It allows a model to weigh the importance of different parts of the input sequence.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer神经网络架构围绕自注意力机制展开。那么，让我们首先通过这一点来开启本章节。**自注意力**是一种用于机器学习的机制，特别是在NLP和计算机视觉领域。它允许模型对输入序列的不同部分的重要性进行加权。
- en: Self-attention is a specific type of attention mechanism. In traditional attention
    mechanisms, the importance weights are between two different sets of input data.
    For example, an attention-based English-to-French translation model may focus
    on specific parts (e.g., nouns, verbs) of the English source sentences that are
    relevant to the current French target word being generated. However, in self-attention,
    the importance weighting operates between any two elements within the same input
    sequence. It focuses on how different parts in the same sequence relate to each
    other. Used for English-to-French translation, the self-attention model analyzes
    how each English word interacts with every other English word. By understanding
    these relationships, the model can generate a more nuanced and accurate French
    translation.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是一种特定类型的注意力机制。在传统的注意力机制中，重要性权重存在于两个不同输入数据集之间。例如，基于注意力的英法翻译模型可能会聚焦于与当前生成的法语目标单词相关的英文源句子的特定部分（例如，名词、动词）。然而，在自注意力中，重要性加权发生在同一输入序列中的任意两个元素之间。它关注同一序列中的不同部分是如何相互关联的。在英法翻译中，自注意力模型分析每个英语单词与其他所有英语单词的相互作用。通过理解这些关系，模型能够生成更加细致和准确的法语翻译。
- en: In the context of NLP, traditional RNNs process input sequences sequentially.
    Because of this sequential processing style, RNNs can only handle shorter sequences
    well, and capture shorter-range dependencies among tokens. On the contrary, a
    self-attention-powered model can simultaneously process all input tokens in a
    sequence. For a given token, the model assigns different attention weights to
    different tokens based on their relevance to the given token, regardless of their
    positions. As a result, it can capture the relationships between different tokens
    in the input, and their long-range dependencies. Self-attention-based models outperform
    RNNs in several sequence-to-sequence tasks such as machine translation, text summarization,
    and query answering.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在自然语言处理的背景下，传统的循环神经网络按顺序处理输入序列。由于这种顺序处理的方式，RNN只能很好地处理较短的序列，并捕捉标记之间的较短范围依赖关系。相反，基于自注意力的模型可以同时处理序列中的所有输入标记。对于给定的标记，模型根据它们与给定标记的相关性分配不同的注意力权重，而不考虑它们的位置。因此，它可以捕捉输入中不同标记之间的关系及其长距离依赖关系。基于自注意力的模型在多个序列到序列的任务（如机器翻译、文本摘要和查询回答）中优于RNN。
- en: 'Let’s discuss how **self-attention** plays a key role in sequence learning
    tasks in the following examples:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论自注意力在以下示例中在序列学习任务中的关键作用：
- en: “I read *Python Machine Learning by Example* by Hayden Liu and it is indeed
    a great book.” Apparently, *it* here refers to *Python Machine Learning by Example*.
    When the Transformer model processes this sentence, self-attention will associate
    *it* with *Python Machine Learning by Example*.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: “我读过Hayden Liu的*Python机器学习实例*，确实是一本好书。”显然，这里的*it*指的是*Python机器学习实例*。当Transformer模型处理这个句子时，自注意力将把*it*与*Python机器学习实例*关联起来。
- en: We can use a self-attention-based model to summarize a document (e.g., this
    chapter). Self-attention isn’t limited by the order of sentences, unlike sequential
    learning RNNs, and it can identify relationships between sentences (even distant
    ones), which ensures the summary reflects the overall information.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用基于自注意力的模型来总结文档（例如本章）。与顺序学习的循环神经网络不同，自注意力不受句子顺序的限制，可以识别句子之间的关系（即使是远距离的关系），这确保了摘要反映了整体信息。
- en: Given a token in an input sequence, self-attention allows the model to look
    at the other tokens in the sequence at different attention levels. In the next
    section, we’ll look at a more detailed explanation of how the self-attention mechanism
    works.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 给定输入序列中的一个标记，自注意力机制允许模型以不同的注意力水平查看序列中的其他标记。在接下来的部分，我们将详细解释自注意力机制的工作原理。
- en: Key, value, and query representations
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 键（Key）、值（Value）和查询（Query）的表示
- en: 'The self-attention mechanism is applied to each token in a sequence. Its goal
    is to represent every token by an embedding vector that captures long-range context.
    The embedding vector of an input token is composed of three vectors: key, value,
    and query. For a given token, a self-attention-based model learns these three
    vectors in order to compute the embedding vector.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力机制应用于序列中的每个标记。其目标是通过捕捉长距离上下文的嵌入向量来表示每个标记。输入标记的嵌入向量由三个向量组成：键（key）、值（value）和查询（query）。对于给定的标记，基于自注意力的模型学习这三个向量以计算嵌入向量。
- en: 'The following describes the meaning of each of the three vectors. To aid comprehension,
    we use an analogy that likens a model’s understanding of a sequence to a detective
    investigating a crime scene with a bunch of clues. To solve the case (understand
    the meaning of the sequence), the detective needs to figure out which clues (tokens)
    are most important and how clues are connected (how tokens relate to each other):'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 以下描述了三个向量的含义。为了帮助理解，我们使用一个类比，将模型对序列的理解比作一名侦探调查带有大量线索的犯罪现场。为了解决案件（理解序列的含义），侦探需要弄清楚哪些线索（标记）最重要以及线索如何连接（标记如何相互关联）。
- en: The key vector, *K*, represents the core information of a token. It captures
    the key but not the detailed information a token holds. In our detective analogy,
    the key vector of a clue might contain information about a witness who saw the
    crime, but not the details they saw.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 键向量*K*表示标记的核心信息。它捕捉标记所持有的关键信息，但不包含详细信息。在我们的侦探类比中，一个线索的键向量可能包含见证者看到犯罪的信息，但不包含他们看到的细节。
- en: The value vector, *V*, holds the full information of a token. In our detective
    example, the value vector of a clue could be the detailed statement from the witness.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值向量*V*包含标记的完整信息。在我们的侦探示例中，一个线索的值向量可以是来自证人的详细陈述。
- en: Finally, the query vector, *Q*, represents the importance of understanding a
    given token in the context of the whole sequence. It is a question about a token’s
    relevance to the current task. During the detective’s investigation, their focus
    can change depending on what they are currently looking for. It can be the weapon
    used, the victim, the motivation, or something else. The query vector represents
    the detective’s current focus in the investigation.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，查询向量 *Q* 代表了在整个序列上下文中理解给定 token 的重要性。它是一个关于 token 与当前任务相关性的问题。在侦探的调查过程中，他们的关注点会根据当前寻找的目标而改变。可能是使用的武器、受害者、动机，或者其他一些因素。查询向量代表了侦探在调查中的当前关注点。
- en: 'These three vectors are derived from the input token’s embeddings. Let’s discuss
    how they work together using the detective example again:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个向量是从输入 token 的嵌入（embedding）中派生出来的。我们再用侦探的例子来讨论它们是如何协同工作的：
- en: First, we calculate the attention scores based on the key and query vector.
    Based on a query vector, *Q*, the model analyses each token and computes the relevance
    score between its key vector, *K*, and the query vector, *Q*. A high score indicates
    a high importance of the token to the context. In the detective example, they
    try to figure out how relevant a clue is to the current investigation focus. For
    instance, the detective would think clue A about the building is highly relevant
    when they are looking into the crime scene location. Note that the detective doesn’t
    look at the details of clue A yet, just like the attention scores are computed
    based on the key vector, not the value vector. The model (the detective) will
    use the value vector (details of a clue) in the next stage – embedding vector
    generation.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们基于 key 和 query 向量计算注意力得分。基于查询向量 *Q*，模型分析每个 token，并计算其 key 向量 *K* 和查询向量
    *Q* 之间的相关性得分。高得分表示 token 对上下文的重要性较高。在侦探的例子中，他们试图弄清楚一个线索与当前调查重点的相关性。例如，当侦探在调查犯罪现场位置时，他们会认为关于建筑的线索
    A 与此高度相关。注意，侦探还没有查看线索 A 的详细信息，就像注意力得分是基于 key 向量计算的，而不是基于 value 向量。模型（侦探）将在下一个阶段——嵌入向量生成中使用
    value 向量（线索的详细信息）。
- en: Next, we generate the embedding vector for a token using the value vectors,
    *V*, and the attention weights. The detective has decided how much weight (attention
    scores) is assigned to each clue, and now they will combine the details (value
    vector) of each clue to create a comprehensive understanding (embedding vector)
    of the crime scene.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 value 向量 *V* 和注意力权重生成 token 的嵌入向量。侦探已经决定了为每个线索分配多少权重（注意力得分），现在他们将结合每个线索的详细信息（value
    向量），以创建对犯罪现场的全面理解（嵌入向量）。
- en: The terminology, “`query`,” “`key`,” and “`value`,” is inspired by the information
    retrieval systems.
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 术语“`query`”，“`key`”和“`value`”灵感来自信息检索系统。
- en: Let’s illustrate using a search engine example. Given a query, the search engine
    undergoes a matching process against the key of each document candidate and comes
    up with a ranking score individually. Based on the detailed information and the
    ranking score of a document, the search engine creates a search result page with
    the retrieval of specific associated values.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们用搜索引擎的例子来说明。给定一个查询，搜索引擎会对每个文档候选项的 key 进行匹配过程，并单独给出排名得分。基于文档的详细信息和排名得分，搜索引擎创建一个搜索结果页面，并检索到与之相关的具体值。
- en: We’ve discussed what key, value, and query vectors are in self-attention mechanisms,
    and how they work together to allow the model to capture important information
    from an input sequence. The generated context-aware embedding vector encapsulates
    the relationships between tokens in the sequence. I hope the detective’s crime
    scene analogy helped you gain a better understanding. In the next section, we
    will see how the context-aware embedding vector is generated mathematically.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了 self-attention 机制中的 key、value 和 query 向量，以及它们如何协同工作，使得模型能够从输入序列中捕捉到重要信息。生成的上下文感知嵌入向量封装了序列中
    token 之间的关系。希望侦探的犯罪现场类比能帮助你更好地理解。接下来的章节中，我们将看到上下文感知嵌入向量是如何在数学上生成的。
- en: Attention score calculation and embedding vector generation
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力得分计算与嵌入向量生成
- en: We have a sequence of tokens (*x*[1], *x*[2], …*x*[i], …*x*[n]). Here, *n* is
    the length of the sequence.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个 token 序列（*x*[1]，*x*[2]，…*x*[i]，…*x*[n]）。这里，*n* 是序列的长度。
- en: 'For a given token, the calculation of attention score begins by computing the
    similarity score between each token in the sequence and the token in question.
    The similarity score is computed by taking the dot product of the query vector
    of the current token and the key vector of the other tokens:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的令牌，注意力分数的计算从计算序列中每个令牌与该令牌之间的相似度分数开始。相似度分数通过计算当前令牌的查询向量与其他令牌的键向量的点积来得到：
- en: '*s*[ij]=*q*[i]∙*k*[j]'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*s*[ij]=*q*[i]∙*k*[j]'
- en: Here, *s*[ij] is the similarity score between token *x*[i] and *x*[j], *q*[i]
    is the query vector of *x*[i], and *k*[j] is the query vector of *x*[j]. The similarity
    score measures how relevant a token *x*[j] is to the current token *x*[i].
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*s*[ij]是令牌*x*[i]和*x*[j]之间的相似度分数，*q*[i]是*x*[i]的查询向量，*k*[j]是*x*[j]的查询向量。相似度分数衡量了令牌*x*[j]与当前令牌*x*[i]的相关性。
- en: 'You may notice that the raw similarity scores do not directly reflect relative
    relevance (they can be negative or greater than 1). Recall that the `softmax`
    function normalizes raw scores, converting them into probabilities that sum up
    to 1\. Hence, we need to normalize them using a `softmax` function. The normalized
    scores are the attention scores we are looking for:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，原始的相似度分数并不直接反映相对相关性（它们可能为负数或大于1）。回想一下，`softmax`函数对原始分数进行归一化，将它们转化为概率，使得它们的总和为1。因此，我们需要使用`softmax`函数对它们进行归一化。归一化后的分数就是我们所需要的注意力分数：
- en: '![](img/B21047_13_001.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_001.png)'
- en: Here, *a*[ij] is the similarity score between token *x*[i] and *x*[j], d is
    the dimension of the key vector, and the division of ![](img/B21047_13_002.png)
    is for scaling. The attention weights (*a*[i][1],*a*[i][2],…,*a*[i][n]) convey
    the probability distribution over all other tokens in the sequence with respect
    to the current token.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*a*[ij]是令牌*x*[i]和*x*[j]之间的相似度分数，d是键向量的维度，而![](img/B21047_13_002.png)的除法是用于缩放。注意力权重(*a*[i][1],*a*[i][2],…,*a*[i][n])表示当前令牌对序列中所有其他令牌的概率分布。
- en: 'With the normalized attention weights available, we can now compute the embedding
    vector for the current token. The embedding vector *z*[i] is a weighted sum of
    the value vectors *v* of all tokens in the sequence, where each weight is the
    attention score *a*[ij] between the current token *x*[i] and the respective token
    *x*[j]:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用归一化后的注意力权重，我们现在可以计算当前令牌的嵌入向量。嵌入向量*z*[i]是序列中所有令牌的值向量*v*的加权和，其中每个权重是当前令牌*x*[i]与相应令牌*x*[j]之间的注意力分数*a*[ij]：
- en: '![](img/B21047_13_003.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_003.png)'
- en: This weighted sum vector is considered the context-aware representation of the
    current token, taking into account its relationship with all other tokens.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这个加权和向量被视为当前令牌的上下文感知表示，考虑到它与所有其他令牌的关系。
- en: 'Take the sequence *python machine learning by example* as an example; we take
    the following steps to calculate the self-attention embedding vector for the first
    word, *python*:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以序列*python machine learning by example*为例，我们按以下步骤计算第一个单词*python*的自注意力嵌入向量：
- en: We calculate the dot products between each word in the sequence and the word
    *python*. They are *q*[1]∙*k*[1], *q*[1]∙*k*[2], *q*[1]∙*k*[3], *q*[1]∙*k*[4],
    and *q*[1]∙*k*[5]. Here, *q*[1] is the query vector for the first word, and *k*[1]
    to *k*[5] are the key vectors for the five words, respectively.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算序列中每个单词与单词*python*之间的点积。它们分别是*q*[1]∙*k*[1]，*q*[1]∙*k*[2]，*q*[1]∙*k*[3]，*q*[1]∙*k*[4]，和*q*[1]∙*k*[5]。在这里，*q*[1]是第一个单词的查询向量，而*k*[1]到*k*[5]分别是五个单词的键向量。
- en: 'We normalize the resulting dot products with division and `softmax` activation
    to find the attention weights:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用除法和`softmax`激活函数对结果的点积进行归一化，从而找到注意力权重：
- en: '![](img/B21047_13_004.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_004.png)'
- en: '![](img/B21047_13_005.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_005.png)'
- en: '![](img/B21047_13_006.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_006.png)'
- en: '![](img/B21047_13_007.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_007.png)'
- en: '![](img/B21047_13_008.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_008.png)'
- en: 'Then, we multiply the resulting attention weights by the value vectors, *v*[1],
    *v*[2], *v*[3], *v*[4], *v*[5], and add up the results:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将得到的注意力权重与值向量相乘，*v*[1]，*v*[2]，*v*[3]，*v*[4]，*v*[5]，并将结果加起来：
- en: '*z*[1] = *a*[11].*v*[1]+*a*[12].*v*[2]+*a*[13].*v*[3]+*a*[14].*v*[4]+*a*[15].*v*[5]'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*[1] = *a*[11].*v*[1]+*a*[12].*v*[2]+*a*[13].*v*[3]+*a*[14].*v*[4]+*a*[15].*v*[5]'
- en: '*z*[1] is the context-aware embedding vector for the first word, *python*,
    in the sequence. We repeat this process for each remaining word in the sequence
    to obtain its context-aware embedding.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*z*[1]是序列中第一个单词*python*的上下文感知嵌入向量。我们对序列中的每个剩余单词重复这一过程，以获得其上下文感知嵌入。'
- en: 'During training in a self-attention mechanism, the key, query, and value vectors
    are created by three weight matrices, *W*[k], *W*[q], and *W*[v], using a linear
    transformation:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在自注意力机制的训练过程中，键（key）、查询（query）和值（value）向量是通过三个权重矩阵*W*[k]、*W*[q]和*W*[v]使用线性变换创建的：
- en: '![](img/B21047_13_009.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_009.png)'
- en: '![](img/B21047_13_010.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_010.png)'
- en: '![](img/B21047_13_011.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_011.png)'
- en: Here, *W*[k] is the weight matrix for the key transformation, *W*[q] is the
    weight matrix for the query transformation, and *W*[v] is the weight matrix for
    the value transformation. These three weight matrices are learnable parameters.
    During the model training process, they get updated typically using gradient-based
    optimization algorithms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*W*[k]是键变换的权重矩阵，*W*[q]是查询变换的权重矩阵，*W*[v]是值变换的权重矩阵。这三个权重矩阵是可学习的参数。在模型训练过程中，它们通常通过基于梯度的优化算法进行更新。
- en: 'Now, let’s see how we can simulate the calculation of *z*[1] in PyTorch:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在PyTorch中模拟计算*z*[1]：
- en: 'First, assume we have the following integer representation mapping for the
    tokens in the input `python machine learning by example`:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，假设我们有以下整数表示映射，表示输入`python machine learning by example`中的标记：
- en: '[PRE0]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Each integer corresponds to the index of the respective token in the vocabulary.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 每个整数对应于词汇中相应标记的索引。
- en: 'We also assume we have embeddings ready to use for our simulated vocabulary:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还假设已经有了可以使用的嵌入向量来模拟我们的词汇：
- en: '[PRE1]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, our simulated vocabulary has 10 tokens, and the embedding size is 16\.
    `detach()` is used to create a new tensor that shares the same underlying data
    as the original tensor but is detached from the computation graph. Also, note
    that you may get different embedding results due to some non-deterministic operations.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们的模拟词汇有10个标记，嵌入维度是16。`detach()`用于创建一个新的张量，该张量与原始张量共享相同的底层数据，但与计算图断开连接。另外，注意你可能会得到不同的嵌入结果，因为某些操作是非确定性的。
- en: 'Next, we assume we have the following three weight matrices, *W*[k], *W*[q],
    and *W*[v]:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们假设有以下三组权重矩阵*W*[k]、*W*[q]和*W*[v]：
- en: '[PRE2]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For matrix operations, it is essential that vectors *Q* and *K* share the same
    dimensions to ensure they operate within a consistent feature space. However,
    vector *V* is allowed to have different dimensions. In this example, we will maintain
    uniform dimensions for all three vectors for the sake of simplicity. So, we choose
    16 as the common dimension.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 对于矩阵操作，确保向量*Q*和*K*具有相同的维度是至关重要的，以确保它们在一致的特征空间内操作。然而，向量*V*允许具有不同的维度。在这个示例中，为了简化起见，我们将保持三个向量的维度一致。所以，我们选择16作为共同维度。
- en: 'Now, we can compute the key vector *k*[1], query vector *q*[1], and value vector
    *v*[1] for the *python* token accordingly:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以相应地计算*python*标记的键向量*k*[1]、查询向量*q*[1]和值向量*v*[1]：
- en: '[PRE3]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Take a look at *k*[1]:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 看看*k*[1]：
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can also directly compute the key matrix (composed of key vectors for individual
    tokens) as follows:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以直接计算键矩阵（由各个标记的键向量组成）如下：
- en: '[PRE5]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Similarly, the value matrix can be directly computed as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，值矩阵可以直接计算如下：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'With the key matrix and the query vector *q*[1], we obtain the attention weight
    vector *a*[1]=(*a*[11], *a*[12], *a*[13], *a*[14], *a*[15]):'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过键矩阵和查询向量*q*[1]，我们可以得到注意力权重向量*a*[1]=(*a*[11]、*a*[12]、*a*[13]、*a*[14]、*a*[15])：
- en: '[PRE7]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we multiply the resulting attention weights by the value vectors to
    obtain the context-aware embedding vector for the first token:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将得到的注意力权重与值向量相乘，以获得第一个标记的上下文感知嵌入向量：
- en: '[PRE8]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This is the self-attention version of the embedding vector for the *python*
    token based on the three toy weight matrices, *W*[k], *W*[q], and *W*[v].
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于三个玩具权重矩阵*W*[k]、*W*[q]和*W*[v]的*python*标记的自注意力版本嵌入向量。
- en: In practice, we typically employ more than one set of trainable weight matrices,
    *W*[k], *W*[q], and *W*[v]. That is why self-attention is often called **multi-head
    self-attention**. Each attention head has its own set of learnable parameters
    for the key, query, and value transformations. Using multiple attention heads
    can capture different aspects of relationships within a sequence. Let’s dig into
    this next.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们通常使用不止一组可训练的权重矩阵*W*[k]、*W*[q]和*W*[v]。这就是为什么自注意力通常被称为**多头自注意力**。每个注意力头都有自己的键、查询和值变换的可学习参数。使用多个注意力头可以捕捉序列中不同方面的关系。接下来我们来深入了解这一点。
- en: Multi-head attention
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力
- en: The single-head attention mechanism is effective but may not capture diverse
    relationships within the sequence. Multi-head attention extends this by employing
    multiple sets of query, key, and value matrices (multiple “heads”). Each head
    operates **independently** and can attend to different parts of the input sequence
    **in parallel**. This allows the model to capture diverse relationships simultaneously.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 单头注意力机制虽然有效，但可能无法捕捉序列内的多样化关系。多头注意力通过使用多个查询、键和值矩阵（多个“头”）来扩展这一机制。每个头部**独立**操作，可以**并行**关注输入序列的不同部分。这使得模型能够同时捕捉多样化的关系。
- en: Using the previous example sequence, *python machine learning by example*, one
    attention head might focus on local dependencies, identifying “`machine learning`"
    as a noun phrase; another attention head might emphasize semantic relationships,
    inferring that the “`examples`" are about “`machine learning`.” It’s like having
    multiple analysts examining the same sentence. Each analyst focuses on a different
    aspect (for instance, one on grammar, one on word order, and another on sentiment).
    By combining their insights, you get a more comprehensive understanding of the
    sentence.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 以之前的示例序列 *python machine learning by example* 为例，一个注意力头可能会专注于局部依赖，识别出“`machine
    learning`”作为一个名词短语；另一个注意力头可能会强调语义关系，推测“`examples`”是关于“`machine learning`”的。这就像是多个分析师在检查同一句话。每个分析师专注于不同的方面（例如，一个分析语法，一个分析词序，另一个分析情感）。通过结合他们的见解，你能获得对句子的更全面理解。
- en: Finally, the outputs from all attention heads are concatenated and linearly
    transformed to produce the final attention output.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，来自所有注意力头的输出被拼接并线性变换，生成最终的注意力输出。
- en: In this section, we presented a self-attention mechanism featuring trainable
    parameters. In the upcoming section, we will delve into the Transformer architecture,
    which centers around the self-attention mechanism.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们介绍了一个具有可训练参数的自注意力机制。在接下来的章节中，我们将深入探讨围绕自注意力机制构建的 Transformer 架构。
- en: Exploring the Transformer’s architecture
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索 Transformer 的架构
- en: The Transformer architecture was proposed as an alternative to RNNs for sequence-to-sequence
    tasks. It heavily relies on the self-attention mechanism to process both input
    and output sequences.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer 架构被提出作为 RNNs 在序列到序列任务中的替代方案。它大量依赖自注意力机制来处理输入和输出序列。
- en: 'We’ll start by looking at the high-level architecture of the Transformer model
    (image based on that in the paper *Attention Is All You Need*, by Vaswani et al.):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从查看 Transformer 模型的高层次架构开始（该图像基于论文 *Attention Is All You Need*，由 Vaswani
    等人提供）：
- en: '![](img/B21047_13_01.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_01.png)'
- en: 'Figure 13.1: Transformer architecture'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.1：Transformer 架构
- en: 'As you can see, the Transformer consists of two parts: the **encoder** (the
    big rectangle on the left-hand side) and the **decoder** (the big rectangle on
    the right-hand side). The encoder encrypts the input sequence. It has a **multi-head
    attention layer** and a regular feedforward layer. On the other hand, the decoder
    generates the output sequence. It has a masked multi-head attention (we will talk
    about this in detail later) layer, along with a multi-head attention layer and
    a regular feedforward layer.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，Transformer 由两部分组成：**编码器**（左侧的大矩形）和**解码器**（右侧的大矩形）。编码器对输入序列进行加密。它具有**多头注意力层**和常规的前馈层。另一方面，解码器生成输出序列。它具有一个掩蔽的多头注意力层（我们稍后会详细讨论），以及一个多头注意力层和常规的前馈层。
- en: At step *t*, the Transformer model takes in input steps *x*[1], *x*[2], …, *x*[t]
    and output steps *y*[1], *y*[2], …, *y*[t][−1]. It then predicts *y*[t]. This
    is no different from the many-to-many RNN model. In the next section, we will
    explore the important elements of the Transformer that set it apart from RNNs,
    including the encoder-decoder structure, positional encoding, and layer normalization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 *t* 时，Transformer 模型接收输入步骤 *x*[1], *x*[2], …, *x*[t] 和输出步骤 *y*[1], *y*[2],
    …, *y*[t][−1]，然后预测 *y*[t]。这与多对多的 RNN 模型没有不同。在下一节中，我们将探索 Transformer 中使其与 RNN 区别开来的重要元素，包括编码器-解码器结构、位置编码和层归一化。
- en: The encoder-decoder structure
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码器-解码器结构
- en: The encoder-decoder structure is the key element in the Transformer architecture.
    It leverages the model’s ability to handle sequence-to-sequence tasks. Below is
    a breakdown of the encoder component and the decoder component with an analogy
    to help you understand.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器结构是 Transformer 架构中的关键元素。它利用模型处理序列到序列任务的能力。以下是编码器组件和解码器组件的详细拆解，并通过类比帮助你理解。
- en: The **encoder** component processes the input sequence and creates a context
    representation. Typically, the encoder component is a stack of encoders. Each
    encoder consists of a self-attention layer and a feedforward neural network. We’ve
    covered that the self-attention allows each token to attend to other tokens in
    the sequence. Unlike sequential models like RNNs, relations between tokens (even
    the distant ones) are captured in the Transformer, and the feedforward neural
    network adds non-linearity to the model’s learning capacity. We’ve seen this in
    deep neural networks before.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**编码器**组件处理输入序列并创建上下文表示。通常，编码器组件是由多个编码器堆叠而成。每个编码器由一个自注意力层和一个前馈神经网络组成。我们已经介绍了自注意力允许每个标记关注序列中的其他标记。与像RNN这样的顺序模型不同，Transformer能够捕捉标记之间的关系（即使是远距离的关系），而前馈神经网络为模型的学习能力增加了非线性。我们在深度神经网络中也见过这种情况。'
- en: Imagine you want to order a meal at a restaurant. The encoder in the Transformer
    works in a similar way as you reading the menu and generating your own understanding.
    The encoder takes in the menu (input sequence of words). It analyzes the words
    using self-attention, just like you read the descriptions of each dish and their
    ingredients (relationships between words). After the encoder (it could be a stack
    of encoders) digests the information from the menu, it creates a condensed representation,
    the encoded context that captures the essence of the menu. This output of the
    encoder is like your own comprehension of the menu.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想在餐厅点餐。Transformer 中的编码器工作原理类似于你阅读菜单并生成自己的理解。编码器接收菜单（输入的单词序列），它通过自注意力分析这些单词，就像你阅读每道菜肴的描述和其配料（单词之间的关系）。在编码器（可能是多个编码器堆叠在一起）消化了菜单的信息后，它创建一个简化的表示，编码的上下文，捕捉菜单的精髓。这个编码器的输出就像你对菜单的理解。
- en: For the encoder component composed of multiple identical encoder blocks, the
    output from each encoder serves as the input for the subsequent block in the stacked
    structure. This stacked approach offers a powerful way to capture more complex
    relationships and create a richer understanding of the input sequence. We’ve seen
    a similar approach in RNNs. In case you are wondering, six encoder blocks were
    employed in the original design in *Attention Is All You Need*. The number of
    encoders is not magical and is subject to experimentation.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 对于由多个相同编码器块组成的编码器组件，每个编码器的输出作为后续块的输入，构成了堆叠结构。这种堆叠方法提供了一种强大的方式来捕捉更复杂的关系，并创建对输入序列更丰富的理解。在RNN中我们看到了类似的方法。如果你有疑问，原始设计中在*Attention
    Is All You Need*中使用了六个编码器块。编码器的数量并非神奇数字，而是需要通过实验来确定。
- en: The **decoder** component utilizes the context representation provided by the
    encoder to generate the output sequence. Similar to the encoder, the decoder component
    also consists of multiple stacked decoder blocks. Similarly, each decoder block
    contains a self-attention layer and a feedforward neural network. However, the
    self-attention in the decoder is slightly different from the encoder one. It attends
    to the output sequence but it only considers the context it has already built.
    This means for a given token, the decoder self-attention only considers the relationships
    between previously processed tokens and the current tokens. Recall that the self-attention
    in the encoder can attend to the whole input sequence at once. Hence, we call
    the self-attention in the decoder **masked self-attention**.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '**解码器**组件利用编码器提供的上下文表示生成输出序列。与编码器类似，解码器组件也由多个堆叠的解码器块组成。同样，每个解码器块包含一个自注意力层和一个前馈神经网络。然而，解码器中的自注意力与编码器中的自注意力略有不同。它关注的是输出序列，但只考虑它已经构建的上下文。这意味着，对于给定的标记，解码器自注意力仅考虑之前处理过的标记和当前标记之间的关系。回想一下，编码器中的自注意力可以一次性关注整个输入序列。因此，我们称解码器中的自注意力为**掩蔽自注意力**。'
- en: Besides a masked self-attention layer and a feedforward neural network, the
    decoder block has an additional attention layer called **encoder-decoder attention**.
    It attends to the context representation provided by the encoder so that the generated
    output sequence is relevant to the encoded context.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除了一个掩蔽自注意力层和一个前馈神经网络，解码器块还具有一个额外的注意力层，称为**编码器-解码器注意力**。它关注编码器提供的上下文表示，以确保生成的输出序列与编码的上下文相关。
- en: Going back to the restaurant ordering analogy. Now, you (the decoder) want to
    place an order (generate the output sequence). The decoder uses encoder-decoder
    self-attention to consider the encoded context (your understanding of the menu).
    Encoder-decoder self-attention ensures the output generation is based on your
    comprehension of the menu, not someone else’s. The decoder uses **masked self-attention**
    to generate the output word by word (your order of dishes). Masked self-attention
    ensures you don’t “peek” at future dishes (words) you haven’t “ordered” (generated)
    yet. With each generated word (the dish you order), the decoder can refine its
    understanding of the desired output sequence (meal) based on the encoded context.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 回到餐厅点餐的类比。现在，你（解码器）想要下单（生成输出序列）。解码器使用编码器-解码器自注意力来考虑编码后的上下文（你对菜单的理解）。编码器-解码器自注意力确保输出的生成是基于你对菜单的理解，而不是其他人的。解码器使用**掩蔽自注意力**来逐字生成输出（你点的菜肴）。掩蔽自注意力确保你不会“偷看”还未“点餐”（生成）的未来菜肴（单词）。随着每个生成的词（你点的菜肴），解码器可以基于编码后的上下文来调整对所需输出序列（餐点）的理解。
- en: Similar to the encoder, in the stacked decoder component, the output from each
    decoder block serves as the input for the subsequent block. Due to the encoder-decoder
    self-attention, the number of decoder blocks is usually the same as the encoder
    blocks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于编码器，在堆叠的解码器组件中，每个解码器块的输出作为下一个解码器块的输入。由于编码器-解码器自注意力机制，解码器块的数量通常与编码器块的数量相同。
- en: During training, the model is provided with both the input and the target output
    sequences. It learns to generate the target output sequence by minimizing the
    difference between its predictions and the actual target sequence.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，模型会同时接收输入序列和目标输出序列。它通过最小化预测结果与实际目标序列之间的差异来学习生成目标输出序列。
- en: In this section, we’ve delved into the Transformer’s encoder-decoder structure.
    The encoder stack extracts a context representation of the input sequence. The
    decoder generates the output sequence one token at a time, attending to both the
    encoded context and previously generated tokens.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了 Transformer 的编码器-解码器结构。编码器堆栈提取输入序列的上下文表示。解码器则逐个生成输出序列的标记，关注编码后的上下文以及之前生成的标记。
- en: Positional encoding
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 位置编码
- en: While powerful, self-attention struggles to differentiate between the importance
    of elements based solely on their content. For instance, given the sentence, “The
    white fox jumps over the brown dog,” self-attention might assign similar importance
    to “`fox`" and “`dog`" simply because they share similar grammatical roles (nouns).
    To address this limitation, **positional encoding** is introduced to inject positional
    information into self-attention.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然强大，自注意力机制在仅基于内容区分元素的重要性方面存在困难。例如，给定句子“白色狐狸跳过棕色狗”，自注意力可能会因为“fox”和“dog”在语法角色（名词）上相似，而给它们分配相似的重要性。为了解决这个局限性，引入了**位置编码**来将位置信息注入自注意力机制。
- en: 'The positional encoding is a fixed-size vector that contains positional information
    of the token in the sequence. It is typically calculated based on mathematical
    functions. One common approach is to use a combination of sine and cosine functions
    as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是一个固定大小的向量，包含序列中标记的位置信息。它通常是基于数学函数计算的。一种常见的方法是使用正弦和余弦函数的组合，如下所示：
- en: '![](img/B21047_13_012.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_012.png)'
- en: '![](img/B21047_13_013.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_013.png)'
- en: Here, *i* is the dimension index, *pos* is the position of the token, and *d*
    is the dimension of the embedding vector. *PE*(*pos*, 2*i*) denotes the 2*i*^(th)
    dimension of the positional encoding for position *pos*; *PE*(*pos*, 2*i*+1) represents
    the 2*i*+1^(th) dimension of the positional encoding for position *pos*. ![](img/B21047_13_014.png)
    introduces different frequencies for different dimensions.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*i* 是维度索引，*pos* 是标记的位置，*d* 是嵌入向量的维度。*PE*(*pos*, 2*i*) 表示位置 *pos* 的位置编码中第
    2*i* 维的值；*PE*(*pos*, 2*i*+1) 表示位置 *pos* 的位置编码中第 2*i*+1 维的值。![](img/B21047_13_014.png)
    为不同维度引入了不同的频率。
- en: 'Let’s try to encode the positions of the words in a simple sentence, “`Python
    machine learning`" using a four-dimensional vector. For the first word, “Python,”
    we have the following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用四维向量对简单句子“`Python machine learning`”中的单词位置进行编码。对于第一个单词“Python”，我们有以下内容：
- en: '![](img/B21047_13_015.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_015.png)'
- en: '![](img/B21047_13_016.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_016.png)'
- en: '![](img/B21047_13_017.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_017.png)'
- en: '![](img/B21047_13_018.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_018.png)'
- en: 'For the second word “machine,” we have the following:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个词“machine”，我们得到以下结果：
- en: '![](img/B21047_13_019.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_019.png)'
- en: '![](img/B21047_13_020.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_020.png)'
- en: '![](img/B21047_13_021.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_021.png)'
- en: '![](img/B21047_13_022.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_022.png)'
- en: So, we have positional encoding `[0, 1, 0, 1]` for “Python,” and `[0.8, 0.5,
    0, 1]` for “machine.” We will leave the third word as an exercise for you.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们为“Python”提供了位置编码 `[0, 1, 0, 1]`，为“machine”提供了位置编码 `[0.8, 0.5, 0, 1]`。我们将留给你一个练习，自己完成第三个词的位置编码。
- en: After the positional encoding vectors are computed for each position, they are
    then added to the embeddings of the corresponding tokens. As you can see in the
    Transformer architecture in *Figure 13.1*, the positional encoding is added to
    the input embedding before the input embedding is fed into the encoder component.
    Similarly, the positional encoding is added to the output embedding before the
    output embedding is fed into the decoder component. Now, when self-attention learns
    the relationships between tokens, it considers both the content (token themselves)
    and their positional information.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在为每个位置计算位置编码向量之后，它们会被加到相应标记的嵌入表示中。如图 *13.1* 所示，位置编码在输入嵌入进入编码器组件之前被加到输入嵌入中。同样，位置编码会在输出嵌入进入解码器组件之前加到输出嵌入中。现在，当自注意力学习标记之间的关系时，它会同时考虑内容（标记本身）和它们的位置相关信息。
- en: Positional encoding is an important complement to self-attention in Transformers.
    Since the Transformer doesn’t inherently understand the sequential order of tokens
    like RNNs do, the additional positional encoding allows it to capture sequential
    dependencies effectively. In the next section, we will look at another crucial
    component in Transformers, layer normalization.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 位置编码是 Transformer 中自注意力的重要补充。由于 Transformer 本身不像 RNN 那样固有地理解标记的顺序，因此额外的位置信息使得它能够有效地捕捉序列依赖关系。在下一部分，我们将研究
    Transformer 中另一个关键组件——层归一化。
- en: Layer normalization
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层归一化
- en: A Transformer has many layers (blocks of encoders and decoders that are composed
    of multi-head self-attention), and it can suffer from exploding or vanishing gradients.
    This makes it difficult for the network to learn effectively during training.
    **Layer normalization** helps address this by normalizing the outputs of each
    layer.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 Transformer 由多个层（由多头自注意力组成的编码器和解码器块）构成，它可能会遇到梯度爆炸或梯度消失的问题。这使得网络在训练过程中很难有效学习。**层归一化**通过对每一层的输出进行归一化，帮助解决了这一问题。
- en: Normalization is applied independently to each layer’s activations, including
    the self-attention layer and the feedforward network. This means that each layer’s
    outputs are kept within a specific range to prevent gradients from becoming too
    large or too small. As a result, layer normalization can stabilize the training
    process and improve the Transformer’s learning efficiency.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 归一化是独立应用于每一层的激活，包括自注意力层和前馈网络。这意味着每一层的输出都会保持在一个特定的范围内，以防止梯度过大或过小。因此，层归一化能够稳定训练过程，并提高
    Transformer 的学习效率。
- en: We’ve gained a deep understanding of the Transformer architecture and its components,
    including the encoder, decoder, multi-head self-attention, masked self-attention,
    positional encoding, and layer normalization. Next, we will learn about models,
    BERT and GPT, that are based on the Transformer architecture, and will work on
    their applications, including sentiment analysis and text generation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经深入理解了 Transformer 架构及其组成部分，包括编码器、解码器、多头自注意力、掩蔽自注意力、位置编码和层归一化。接下来，我们将学习基于
    Transformer 架构的模型 BERT 和 GPT，并探讨它们的应用，包括情感分析和文本生成。
- en: Improving sentiment analysis with BERT and Transformers
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 BERT 和 Transformer 改进情感分析
- en: '**BERT** ([https://arxiv.org/abs/1810.04805v2](https://arxiv.org/abs/1810.04805v2))
    is a model based on the Transformer architecture. It has achieved significant
    success in various language understanding tasks in recent years.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT** ([https://arxiv.org/abs/1810.04805v2](https://arxiv.org/abs/1810.04805v2))
    是一个基于 Transformer 架构的模型。近年来，它在各种语言理解任务中取得了显著的成功。'
- en: As its name implies, bidirectional is one significant difference between BERT
    and earlier Transformer models. Traditional models often process sequence in a
    unidirectional manner, but BERT processes the entire context bidirectionally.
    This bidirectional context understanding makes the model more effective in capturing
    nuanced relationships in a sequence.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名称所示，双向性是BERT与早期Transformer模型之间的一个显著区别。传统模型通常以单向方式处理序列，而BERT则以双向方式处理整个上下文。这种双向上下文理解使得模型在捕捉序列中的细微关系时更加有效。
- en: BERT is basically a stack of trained Transformer’s encoders. It is pre-trained
    on large amounts of unlabeled text data in a self-supervised manner. During pre-training,
    it focuses on understanding the meaning of text in context. After pre-training,
    BERT can be fine-tuned for specific downstream tasks.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: BERT基本上是一个训练过的Transformer编码器堆叠。它在大量未标记的文本数据上进行自监督预训练。在预训练过程中，BERT专注于理解文本的上下文意义。预训练完成后，BERT可以针对特定的下游任务进行微调。
- en: Let’s first talk about the pre-training works.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来讨论一下预训练工作。
- en: Pre-training BERT
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练BERT
- en: 'The goal of pre-training BERT is to capture rich contextualized representations
    of words. It involves training the model on **a large corpus** of unlabeled text
    data in a self-supervised manner. The pre-training process consists of two main
    tasks: the **Masked Language Model** (**MLM**) task and the **Next Sentence Prediction**
    (**NSP**) task. Here is the breakdown.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的预训练目标是捕捉丰富的上下文化单词表示。它通过在**大量语料**上进行自监督训练，处理未标记的文本数据。预训练过程包括两个主要任务：**掩蔽语言模型**（**MLM**）任务和**下一句预测**（**NSP**）任务。以下是具体分解。
- en: MLM
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLM
- en: In MLM, random words in a sentence are replaced with a special token `[MASK]`.
    BERT takes the modified sentence as input and tries to predict the original masked
    word based on the surrounding words. In this fill-in-the-blank game, BERT is trained
    to understand the meaning and context of words.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在MLM任务中，句子中的随机单词被替换为特殊标记`[MASK]`。BERT将修改后的句子作为输入，并尝试基于周围的单词预测原始被掩蔽的单词。在这个填空游戏中，BERT被训练去理解单词的意义和上下文。
- en: It is worth noting that during the MLM task, the model is trained using the
    **bidirectional** context—both the left and right context of each masked word.
    This improves the masked word prediction accuracy. As a result, by going through
    a large number of training examples, BERT gets better at understanding word meanings
    and capturing relationships in different contexts.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在MLM任务中，模型使用**双向**上下文进行训练——即每个掩蔽单词的左右上下文。这提高了掩蔽单词预测的准确性。因此，通过处理大量的训练样本，BERT在理解单词意义和捕捉不同上下文中的关系方面变得更为出色。
- en: NSP
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NSP
- en: 'The NSP task helps the model understand relationships between sentences and
    discourse-level information. During training, pairs of sentences are randomly
    sampled from the training corpus. For each pair, there is a 50% chance that the
    second sentence follows the first in the original text and a 50% chance that it
    doesn’t. Two sentences concatenated together to form a training sample. Special
    `[CLS]` and `[SEP]` tokens are used to format the training sample:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: NSP任务帮助模型理解句子之间的关系和话语级别的信息。在训练过程中，句子对会从训练语料库中随机抽取。对于每一对句子，第二个句子有50%的概率跟随第一个句子出现，也有50%的概率不跟随第一个句子。将两个句子连接起来形成一个训练样本。特殊的`[CLS]`和`[SEP]`标记用于格式化训练样本：
- en: The `[CLS]` (classification) token is added at the beginning of the training
    sample. The output corresponding to the `[CLS]` token is used to represent the
    entire input sequence for classification tasks.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[CLS]`（分类）标记被添加到训练样本的开始位置。对应于`[CLS]`标记的输出用于表示整个输入序列，用于分类任务。'
- en: The `[SEP]` (separator) token separates two concatenated input sentences.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`[SEP]`（分隔符）标记用于分隔两个连接的输入句子。'
- en: 'BERT’s pre-training process is depicted in the following diagram (note that
    “C” in the diagram is short for “Class”):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: BERT的预训练过程如下图所示（请注意，图中的“C”是“Class”的缩写）：
- en: '![](img/B21047_13_02.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_13_02.png)'
- en: 'Figure 13.2: BERT pre-training'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：BERT预训练
- en: As you can see in the pre-training diagram, the model is trained on concatenated
    sentences to predict whether the second sentence follows the first one. It receives
    the correct label (is next sentence or not) as feedback and adjusts its parameters
    to improve its prediction accuracy. Through this sentence matchmaking task, BERT
    learns to understand how sentences relate to each other and how ideas flow coherently
    within a text.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在预训练图中所见，模型在连接的句子上进行训练，以预测第二个句子是否跟随第一个句子。它会接收正确的标签（是否是下一个句子）作为反馈，并调整其参数以提高预测准确性。通过这种句子配对任务，BERT学会了理解句子之间的关系以及思想在文本中的连贯流动。
- en: Interestingly, the diagram shows that BERT’s pre-training combines MLM and NSP
    tasks simultaneously. The model predicts masked tokens within a sentence while
    also determining whether a sentence pair follows sequentially.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，图表显示BERT的预训练同时结合了MLM和NSP任务。模型在句子中预测被遮盖的标记，同时还判断句子对是否是连续的。
- en: While the MLM task focuses on word-level contextualization, the NSP task contributes
    to BERT’s broader understanding of sentence relationships. After pre-training,
    BERT can be **fine-tuned** for specific downstream NLP tasks. Let’s see how we
    do this next.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MLM任务侧重于词汇级的上下文化，NSP任务有助于BERT更广泛地理解句子之间的关系。预训练后，BERT可以**微调**以适应特定的下游NLP任务。接下来我们来看如何进行微调。
- en: Fine-tuning of BERT
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT的微调
- en: 'BERT is usually fine-tuned for targeted tasks such as sentiment analysis or
    named entity recognition. Task-specific layers are added on top of the pre-trained
    model, and the model is trained on labeled data relevant to the task. Here’s the
    step-by-step process of fine-tuning:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: BERT通常会针对目标任务（如情感分析或命名实体识别）进行微调。任务特定的层被添加到预训练模型之上，并在与任务相关的标注数据上训练模型。以下是微调的逐步过程：
- en: We first gather labeled data specific to the downstream task.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先收集特定于下游任务的标注数据。
- en: Next, we need to select a pre-trained BERT model. Various options are available,
    such as BERT-base and BERT-large. You should use the one suitable for the downstream
    task and computational capabilities.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要选择一个预训练的BERT模型。有多种选择，例如BERT-base和BERT-large。你应该选择适合下游任务和计算能力的模型。
- en: Based on the selected BERT model, we use the corresponding tokenizer to tokenize
    the input. Various tokenizers are available in the Hugging Face tokenizers package
    ([https://huggingface.co/docs/tokenizers/python/latest/index.html](https://huggingface.co/docs/tokenizers/python/latest/index.html)).
    But you should use the one that matches the model.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据所选的BERT模型，我们使用相应的分词器来对输入进行分词。Hugging Face的tokenizers包中提供了多种分词器（[https://huggingface.co/docs/tokenizers/python/latest/index.html](https://huggingface.co/docs/tokenizers/python/latest/index.html)）。但你应该使用与模型匹配的分词器。
- en: Here comes the fun part – architecture modification. We can add task-specific
    layers on top of the pre-trained BERT model. For instance, you can add a single
    neuron with a sigmoid activation for sentiment analysis.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有趣的部分来了——架构修改。我们可以在预训练的BERT模型上添加任务特定的层。例如，你可以为情感分析添加一个带有sigmoid激活的单神经元。
- en: We then define the task-specific loss function and objective for training. Use
    the sentiment analysis example again; you can use the binary cross-entropy loss
    function.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接着定义任务特定的损失函数和目标，以进行训练。再次以情感分析为例，你可以使用二元交叉熵损失函数。
- en: Finally, we train the modified BERT model on the labeled data.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在标注数据上训练修改后的BERT模型。
- en: We usually perform hyperparameter tuning to find the optimal model configurations
    including learning rate, batch size, and regularization.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通常会执行超参数调优，以找到最佳的模型配置，包括学习率、批次大小和正则化。
- en: Fine-tuning BERT leverages the knowledge acquired during pre-training and adapts
    it to a specific task. With this transfer learning strategy, BERT doesn’t need
    to start from scratch, so it learns new things faster and needs less data. In
    the next section, we’ll utilize BERT to improve the sentiment prediction on movie
    reviews.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 微调BERT利用了预训练期间获得的知识，并将其适应于特定任务。通过这种迁移学习策略，BERT无需从头开始，因此能够更快学习新知识，并且需要更少的数据。在接下来的部分，我们将使用BERT提升电影评论的情感预测。
- en: Fine-tuning a pre-trained BERT model for sentiment analysis
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对预训练BERT模型进行情感分析的微调
- en: 'In *Chapter 12**, Making Predictions with Sequences Using Recurrent Neural
    Networks*, we developed an LSTM model for movie review sentiment prediction. We
    will fine-tune a pre-trained BERT model for the same task in the following steps:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第12章*《使用循环神经网络进行序列预测》中，我们开发了一个LSTM模型用于电影评论情感预测。在接下来的步骤中，我们将对同一任务微调预训练的BERT模型：
- en: 'First, we read the IMDb review data from the PyTorch built-in datasets:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从PyTorch内置数据集中读取IMDb评论数据：
- en: '[PRE9]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We just load 25,000 training samples and 25,000 test samples.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了25,000个训练样本和25,000个测试样本。
- en: 'Then, we separate the raw data into text and label data, as we will need to
    tokenize the text data:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将原始数据分为文本和标签数据，因为我们需要对文本数据进行分词：
- en: '[PRE10]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We have finished data preparation and move on to the tokenization step.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了数据准备，并进入分词步骤。
- en: Now, we need to pick a suitable pre-trained model and the corresponding tokenizer.
    We choose the `distilbert-base-uncased` model given that we have limited computational
    resources. Think of it as a smaller (“distilled”), faster version of BERT. It
    keeps most of the power of BERT but with fewer parameters. The “uncased” part
    just means that the model was trained on lowercase text. To make things work smoothly,
    we’ll use the `distilbert-base-uncased` tokenizer that matches the model.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要选择一个合适的预训练模型和相应的分词器。考虑到我们有限的计算资源，我们选择了`distilbert-base-uncased`模型。可以将其视为BERT的一个更小（“蒸馏”）、更快的版本。它保留了BERT的大部分功能，但参数更少。“uncased”部分意味着该模型是基于小写文本训练的。为了确保一切顺利，我们将使用与模型匹配的`distilbert-base-uncased`分词器。
- en: 'If you have not installed Hugging Face’s transformers package yet, you can
    do so in the command line as follows:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你还没有安装Hugging Face的transformers包，可以通过命令行按如下方式安装：
- en: '[PRE11]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Or, with the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，使用以下方式：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As of the current writing, we are using version `4.32.1`. Feel free to replicate
    the process using this version, as the transformer package undergoes frequent
    updates.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 截至目前，我们使用的是`4.32.1`版本。可以自由地使用此版本进行复制，因为transformers包会经常更新。
- en: 'Pre-trained models from Hugging Face can be loaded and then downloaded and
    cached locally. We load the `distilbert-base-uncased` tokenizer as follows:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hugging Face的预训练模型可以加载并在本地下载和缓存。我们按如下方式加载`distilbert-base-uncased`分词器：
- en: '[PRE13]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can also manually download a `transformers` model ahead of time, and read
    it from the specified local path. To fetch the `distilbert-base-uncased` pre-trained
    model and tokenizer, you can search `distilbert-base-uncased` at [https://huggingface.co/models](https://huggingface.co/models),
    and go to the clickable link of `distilbert-base-uncased`. Model and tokenizer
    files can be found in the **Files** tab or at [https://huggingface.co/distilbert-base-uncased/tree/main](https://huggingface.co/distilbert-base-uncased/tree/main)
    directly. Download all files **except** `flax_model.msgpack`, `model.safetensors`,
    `rust_model.ot`, and `tf_model.h5` (as we only need the PyTorch model), and put
    them in the folder called `distilbert-base-uncased`. Finally, we will be able
    to load the tokenizer from the `distilbert-base-uncased` path as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以提前手动下载一个`transformers`模型，并从指定的本地路径读取。要获取`distilbert-base-uncased`预训练模型和分词器，可以在[https://huggingface.co/models](https://huggingface.co/models)搜索`distilbert-base-uncased`，并点击`distilbert-base-uncased`的链接。模型和分词器文件可以在**Files**选项卡中找到，或者直接在[https://huggingface.co/distilbert-base-uncased/tree/main](https://huggingface.co/distilbert-base-uncased/tree/main)查看。下载所有文件**除了**`flax_model.msgpack`、`model.safetensors`、`rust_model.ot`和`tf_model.h5`（因为我们只需要PyTorch模型），并将它们放在名为`distilbert-base-uncased`的文件夹中。最后，我们将能够按如下方式从`distilbert-base-uncased`路径加载分词器：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we tokenize the input text from the train and test datasets:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们对训练集和测试集中的输入文本进行分词：
- en: '[PRE15]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Take a look at the encoding result of the first train sample:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下第一个训练样本的编码结果：
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Here, the resulting encoding object can only hold up to 512 tokens. If the original
    text is longer, it will be truncated. Attributes are a list of information associated
    with the encoding process, including `ids` (representing the token IDs) and `attention_mask`
    (specifying which tokens should be attended to and which should be ignored).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，生成的编码对象最多只能容纳512个标记。如果原始文本更长，它将被截断。属性是与编码过程相关的信息列表，包括`ids`（表示标记ID）和`attention_mask`（指定哪些标记应被关注，哪些应被忽略）。
- en: 'Next, we encapsulate all data fields, including the labels within a `Dataset`
    class:'
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将所有数据字段，包括标签，封装在一个`Dataset`类中：
- en: '[PRE17]'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we transform the positive label (denoted by “`2`") and the negative
    label (denoted by “`1`") into the formats `[0, 1]` and `[1, 0]` respectively.
    We make this adjustment to align with the labeling format required by DistilBERT.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们将正标签（表示为“`2`”）和负标签（表示为“`1`”）转换为格式`[0, 1]`和`[1, 0]`。我们做出这个调整是为了与DistilBERT要求的标签格式对齐。
- en: 'We then generate the custom `Dataset` objects for the train and test data:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们为训练数据和测试数据生成自定义的`Dataset`对象：
- en: '[PRE18]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Based on the resulting datasets, we create batch data loaders and get ready
    for model fine-tuning and evaluation:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据生成的数据集，我们创建批量数据加载器，并为模型微调和评估做好准备：
- en: '[PRE19]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After completing the data preparation and tokenization, the next step is loading
    the pre-trained model and fine-tuning it with the dataset we’ve just prepared.
    The code for loading the pre-trained model is provided here:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成数据准备和分词后，下一步是加载预训练模型并使用我们刚刚准备好的数据集进行微调。加载预训练模型的代码如下：
- en: '[PRE20]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We load the pre-trained `distilbert-base-uncased` model as mentioned earlier.
    We also ensure the model is placed on the specified computing device (GPU highly
    recommended if available) for training and inference.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们加载了前面提到的预训练`distilbert-base-uncased`模型。同时，我们确保模型被放置在指定的计算设备上（如果有GPU，强烈推荐使用GPU）进行训练和推理。
- en: The `transformers` package offers a variety of pre-trained models. You can explore
    them at [https://huggingface.co/docs/transformers/index#supported-models-and-frameworks](https://huggingface.co/docs/transformers/index#supported-models-and-frameworks).
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '`transformers`包提供了多种预训练模型。你可以在[https://huggingface.co/docs/transformers/index#supported-models-and-frameworks](https://huggingface.co/docs/transformers/index#supported-models-and-frameworks)上浏览它们。'
- en: 'We set the corresponding `Adam` optimizer with a learning rate of `0.00005`
    as follows:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了相应的`Adam`优化器，学习率为`0.00005`，如下所示：
- en: '[PRE21]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, we define a training function responsible for training (fine-tuning) the
    model for one iteration:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们定义了一个训练函数，负责训练（微调）模型一次：
- en: '[PRE22]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is similar to the train function we employed, except BERT needs both token
    IDs and `attention_mask` as inputs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们使用的训练函数类似，只不过BERT需要同时输入token ID和`attention_mask`。
- en: 'Similarly, we define the evaluation function responsible for evaluating the
    model accuracy:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们定义了评估函数，用于评估模型的准确率：
- en: '[PRE23]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We then train the model for one iteration and display the train loss and accuracy
    at the end of it:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们训练模型进行一次迭代，并在结束时显示训练损失和准确率：
- en: '[PRE24]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The training process takes a while and the training accuracy is 96%. You may
    train with more iterations if resources and time allow.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程需要一些时间，训练准确率为96%。如果资源和时间允许，你可以进行更多的迭代训练。
- en: 'Finally, we evaluate the performance on the test set:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在测试集上评估模型性能：
- en: '[PRE25]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We obtained a test accuracy of 93% with just one epoch in fine-tuning the pre-trained
    DistilBERT model. This marks a significant enhancement compared to the 86% test
    accuracy attained with LSTM in *Chapter 12*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过对预训练的DistilBERT模型进行一次epoch的微调，获得了93%的测试准确率。这相比于在*第12章*中使用LSTM得到的86%测试准确率，取得了显著提升。
- en: '**Best practice**'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: If you are dealing with large models or datasets on GPU, it is recommended to
    monitor GPU memory usage to avoid running out of GPU memory.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理大型模型或数据集并使用GPU，建议监控GPU内存使用情况，以避免GPU内存不足。
- en: In PyTorch, you can use `torch.cuda.mem_get_info()` to check the GPU memory
    usage. It tells you information about the available and allocated GPU memory.
    Another trick is `torch.cuda.empty_cache()`. It attempts to release all unused
    cached memory held by the GPU memory allocator back to the system. Finally, if
    you’re done with a model, you can run `del model` to free up the memory it was
    using.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，你可以使用`torch.cuda.mem_get_info()`来检查GPU内存使用情况。它会告诉你关于可用和已分配GPU内存的信息。另一个小技巧是`torch.cuda.empty_cache()`，它会尝试释放GPU内存分配器未使用的缓存内存回系统。最后，如果你不再使用某个模型，可以运行`del
    model`来释放它占用的内存。
- en: Using the Trainer API to train Transformer models
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Trainer API训练Transformer模型
- en: The Trainer API included in Hugging Face is a shortcut for training Transformer-based
    models. It lets you fine-tune those pre-trained models on your own tasks with
    an easy high-level interface. No more wrestling with tons of training code like
    we did in the previous section.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face中包含的Trainer API是训练基于Transformer模型的快捷方式。它让你能够轻松地在自己的任务上微调这些预训练模型，提供了一个简单的高级接口。无需再像之前那样与大量的训练代码作斗争。
- en: 'We will fine-tune the BERT model more conveniently using the `Trainer` API
    in the following steps:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的步骤中，我们将更方便地使用`Trainer` API对BERT模型进行微调：
- en: 'Load the pre-trained model again and create the corresponding optimizer:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次加载预训练模型并创建相应的优化器：
- en: '[PRE26]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'To execute the `Trainer` scripts, it is necessary to have the accelerate package
    installed. Use the following command to install `accelerate`:'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要执行`Trainer`脚本，需要安装accelerate包。使用以下命令来安装`accelerate`：
- en: '[PRE27]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: or
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 或者
- en: '[PRE28]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we prepare the necessary configurations and initialize a `Trainer` object
    for training the model:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们准备必要的配置并初始化一个`Trainer`对象来训练模型：
- en: '[PRE29]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, the `TrainingArguments` configuration defines the number of training epochs,
    the batch size for training, and the number of steps between each logging of training
    metrics. We also tell the Trainer what model to use, which data to train on, and
    what optimizer to use – the second element (`None`) means there’s no learning
    rate scheduler this time.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`TrainingArguments`配置定义了训练的轮数、训练批次大小以及每次记录训练指标的步骤数。我们还告诉Trainer使用哪个模型，训练的数据是什么，以及使用什么优化器——第二个元素（`None`）表示这次没有学习率调度器。
- en: 'You may notice that the `Trainer` initialization in the previous step does
    not involve evaluation metrics and test datasets. Let’s add them and rewrite the
    initialization:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能会注意到，前一步`Trainer`初始化时并没有涉及评估指标和测试数据集。我们来添加这些并重写初始化：
- en: '[PRE30]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `compute_metrics` function calculates the accuracy based on the predicted
    and true labels. The `Trainer` will use this metric to measure how well the model
    performs on the specified test set (`test_encoded_dataset`).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute_metrics`函数根据预测标签和真实标签计算准确率。`Trainer`将使用这个指标来衡量模型在指定测试集（`test_encoded_dataset`）上的表现。'
- en: 'Now, we train the model with just one line of code:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们只用一行代码来训练模型：
- en: '[PRE31]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The model was just trained for one epoch as we specified, and training loss
    was displayed for every 50 steps.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 模型仅根据我们的指定训练了一个epoch，且每50步就会显示训练损失。
- en: 'With another line of code, we can evaluate the trained model on the test dataset:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只需再写一行代码，我们就可以在测试数据集上评估训练好的模型：
- en: '[PRE32]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: We obtained the same test accuracy of 93% utilizing the Trainer API, and it
    required significantly less code.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们利用Trainer API得到了相同的93%的测试准确率，并且所需的代码明显更少。
- en: '**Best practice**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Here are some best practices for fine-tuning BERT:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是微调BERT的一些最佳实践：
- en: '**Data is king**: You should prioritize high-quality and well-labeled data.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据至关重要**：你应该优先使用高质量且标注良好的数据。'
- en: '**Start small**: You can begin with smaller pre-trained models like BERT-base
    or DistilBERT. They’re less demanding on your computational power compared to
    larger models like BERT-large.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从小做起**：你可以从较小的预训练模型开始，比如BERT-base或DistilBERT。它们相比于较大的模型（如BERT-large）对计算资源的需求更低。'
- en: '**Automate hyperparameter tuning**: You may utilize automated hyperparameter
    tuning libraries (e.g., Hyperopt, Optuna) to search for optimal hyperparameters.
    This can save you time and let your computer do the heavy lifting.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化超参数调优**：你可以利用自动化超参数调优库（例如Hyperopt、Optuna）来搜索最佳超参数。这可以节省你的时间，并让计算机进行繁重的计算工作。'
- en: '**Implement early stopping**: You should monitor validation loss during training.
    If it stops getting better after a while, hit the brakes. This early stopping
    strategy can prevent unnecessary training iterations. Remember, fine-tuning BERT
    can take some time and resources.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实施提前停止**：在训练过程中，你应该监控验证损失。如果验证损失在一段时间后不再变得更好，那么就应该停止训练。这种提前停止策略可以防止不必要的训练迭代。记住，微调BERT可能需要一些时间和资源。'
- en: In this section, we discussed BERT, a model based on a Transformer encoder,
    and leveraged it to enhance sentiment analysis. In the following section, we will
    explore another Transformer-based mode, **GPT**.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们讨论了基于Transformer编码器的BERT模型，并利用它来增强情感分析。接下来的部分，我们将探讨另一种基于Transformer的模型，**GPT**。
- en: Generating text using GPT
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用GPT生成文本
- en: BERT and GPT are both state-of-the-art NLP models based on the Transformer architecture.
    However, they differ in their architectures, training objectives, and use cases.
    We will first learn more about GPT and then generate our own version of *War and
    Peace* with a fine-tuned GPT model.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和GPT都是基于Transformer架构的最先进的自然语言处理模型。然而，它们在架构、训练目标和使用场景上有所不同。我们将首先了解更多关于GPT的内容，然后用微调后的GPT模型生成我们自己的*战争与和平*。
- en: Pre-training of GPT and autoregressive generation
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GPT的预训练和自回归生成
- en: GPT (*Improving Language Understanding by Generative Pre-training* by Alec Radford
    et al. 2018) is a **decoder-only** Transformer architecture, while BERT is encoder
    only. This means GPT utilizes masked self-attention in the decoders and emphasizes
    predicting the next token in a sequence.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: GPT（*通过生成预训练改善语言理解*，作者：Alec Radford等，2018）是一个**仅解码器**的Transformer架构，而BERT是仅编码器。这意味着GPT在解码器中使用了屏蔽自注意力，并且强调预测序列中的下一个标记。
- en: Think of BERT like a super detective. It gets a sentence with some words hidden
    (masked) and has to guess what they are based on the clues (surrounding words)
    in both directions, like looking at a crime scene from all angles. GPT, on the
    other hand, is more like a creative storyteller. It is pre-trained using an **autoregressive**
    language model objective. It starts with a beginning word and keeps adding words
    one by one, using the previous words as inspiration, similar to how we write a
    story. This process repeats until the desired sequence length is reached.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将BERT看作是一位超级侦探。它接收到一个句子，其中一些单词被隐藏（屏蔽），并且必须根据线索（周围的单词）在两个方向上推测出这些单词是什么，就像从各个角度观察犯罪现场。而GPT则更像是一位创造性的故事讲述者。它使用**自回归**语言模型目标进行预训练。它从一个起始词开始，逐个添加单词，利用之前的单词作为灵感，类似于我们写故事的方式。这个过程会一直重复，直到达到所需的序列长度。
- en: The word “autoregressive” means it generates text one token at a time in a sequential
    manner.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: “自回归”一词意味着它以顺序的方式逐个标记地生成文本。
- en: Both BERT and GPT are pre-trained on large-scale datasets. However, due to their
    training methods, they have different strengths and use cases for fine-tuning.
    BERT is a master at grasping how words and sentences connect. It can be fine-tuned
    for tasks like sentiment analysis and text classification. On the other hand,
    GPT is better at creating grammatically correct and smooth-flowing text. This
    makes it ideal for tasks like text generation, machine translation, and summarization.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: BERT和GPT都在大规模数据集上进行了预训练。然而，由于它们的训练方法不同，它们在微调时具有不同的优势和应用场景。BERT擅长理解单词和句子之间的关系。它可以用于情感分析和文本分类等任务。另一方面，GPT更擅长生成语法正确且流畅的文本。这使得它在文本生成、机器翻译和摘要等任务中表现更好。
- en: In the next section, we will write our own version of *War and Peace* as we
    did in *Chapter 12**, Making Predictions with Sequences Using Recurrent Neural
    Networks*, but by fine-tuning a GPT model this time.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将像在*第12章*《使用循环神经网络进行序列预测》中那样写我们自己的*《战争与和平》*版本，但这次我们将通过微调GPT模型来实现。
- en: Writing your own version of War and Peace with GPT
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用GPT写你自己的*《战争与和平》*版本
- en: 'For illustrative purposes, we’ll employ GPT-2, a model that is more potent
    than GPT-1 yet smaller in size than GPT-3, and open source, to generate our own
    version of *War and Peace* in the following steps:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明，我们将使用GPT-2，这是一个比GPT-1更强大但比GPT-3更小的开源模型，来生成我们自己的*《战争与和平》*版本，以下是具体步骤：
- en: 'Before we start, let’s quickly look at how to generate text using the GPT-2
    model:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们开始之前，让我们快速看看如何使用GPT-2模型生成文本：
- en: '[PRE33]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To generate our own version of *War and Peace*, we need to fine-tune the GPT-2
    model based on the original *War and Peace* text. We first need to load the GPT-2
    based tokenizer:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成我们自己的*《战争与和平》*版本，我们需要基于原始的*《战争与和平》*文本微调GPT-2模型。我们首先需要加载基于GPT-2的标记器：
- en: '[PRE35]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Next, we create a `Dataset` object for the tokenized *War and Peace* text:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们为经过标记化的*《战争与和平》*文本创建一个`Dataset`对象：
- en: '[PRE36]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We prepare a dataset (`text_dataset`) by tokenizing the text from the `'warpeace_input.txt`'
    file (which we used in *Chapter 12*). We set the `tokenizer` parameter to the
    previously created GPT-2 tokenizer, and `block_size` to `128`, specifying the
    maximum length of each sequence of tokens.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过标记化`'warpeace_input.txt'`文件中的文本（我们在*第12章*中使用的文件）准备了一个数据集（`text_dataset`）。我们将`tokenizer`参数设置为之前创建的GPT-2标记器，并将`block_size`设置为`128`，指定每个标记序列的最大长度。
- en: '[PRE37]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We generated `6176` training samples based on the original text.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于原始文本生成了`6176`个训练样本。
- en: 'Recall in *Chapter 12*, we had to create the training data manually. Thankfully,
    this time we utilize the `DataCollatorForLanguageModeling` class from Hugging
    Face to automatically generate the input sequence and output token. This data
    collator is specifically designed for language modeling tasks, where we’re trying
    to predict masked tokens. Let’s see how to create a data collator as follows:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回顾一下*第12章*，我们必须手动创建训练数据。幸运的是，这次我们使用了Hugging Face的`DataCollatorForLanguageModeling`类来自动生成输入序列和输出标记。这个数据聚合器是专为语言建模任务设计的，我们试图预测被屏蔽的标记。让我们看看如何创建数据聚合器，具体如下：
- en: '[PRE38]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The data collator helps organize and batch the input data for training the language
    model. We set the `mlm` parameter to False. If set to `True`, it would enable
    masked language modeling, where tokens are randomly masked for the model to predict
    as we did in BERT. In our case here, it’s turned off, meaning that the model is
    trained using an autoregressive approach. This approach is ideal for tasks like
    text generation, where the model learns to create new text one word at a time.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理器帮助组织和批处理输入数据，以训练语言模型。我们将`mlm`参数设置为False。如果设置为`True`，它将启用掩码语言建模，其中tokens会被随机掩盖，模型需要预测这些被掩盖的部分，就像我们在BERT中做的那样。在我们这里，它被关闭，意味着模型是使用自回归方法进行训练的。这种方法非常适合文本生成任务，因为模型学习逐字生成新文本。
- en: 'After completing the tokenization and data collation, the next step is loading
    the pre-trained GPT-2 model and creating the corresponding optimizer:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成分词和数据整理后，下一步是加载预训练的GPT-2模型并创建相应的优化器：
- en: '[PRE39]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: We use `GPT2LMHeadModel` for text generation. It predicts the probability distribution
    of the next token in a sequence.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`GPT2LMHeadModel`进行文本生成。它预测序列中下一个token的概率分布。
- en: 'Next, we prepare the necessary configurations and initialize a `Trainer` object
    for training the model:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们准备必要的配置并初始化一个`Trainer`对象来训练模型：
- en: '[PRE40]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The model will be trained on the `text_dataset` dataset divided into `16` sample
    batches for `20` epochs. We also set the limit on the total number of checkpoints
    to save. In this case, only the latest checkpoint will be saved. This is to reduce
    space consumption. In the `trainer`, we provide the `DataCollatorForLanguageModeling`
    instance to organize the data for training.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将在`text_dataset`数据集上进行训练，该数据集分成`16`个样本批次，进行`20`轮训练。我们还设置了保存的检查点的总数限制。在这种情况下，只有最新的检查点会被保存。这是为了减少空间消耗。在`trainer`中，我们提供了`DataCollatorForLanguageModeling`实例来组织数据进行训练。
- en: 'Now, we are ready to train the model:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们准备好训练模型了：
- en: '[PRE41]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'After the GPT-2 model is trained based on *War and Peace*, we finally use it
    to generate our own version. We first develop the following function to generate
    text based on a given model and prompt text:'
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于*《战争与和平》*训练GPT-2模型后，我们最终使用它生成了我们自己的版本。我们首先开发了以下函数，用于根据给定的模型和提示文本生成文本：
- en: '[PRE42]'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: In the generation process, we first tokenize the input prompt text using the
    tokenizer and convert the tokenized sequence to a PyTorch tensor. Then, we generate
    text based on the tokenized input using the given model. Here, `no_repeat_ngram_size`
    prevents repeating the same n-gram phrases to keep things fresh. Another interesting
    setting, `top_p`, controls the diversity of the generated text. It considers the
    tokens that are most probable instead of only the most probable one. Finally,
    we decode the generated response using the tokenizer, translating it back to human
    language.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成过程中，我们首先使用分词器对输入的提示文本进行分词，并将分词后的序列转换为PyTorch张量。然后，我们基于分词后的输入使用给定的模型生成文本。在这里，`no_repeat_ngram_size`防止重复相同的n-gram短语，以保持文本的新鲜感。另一个有趣的设置是`top_p`，它控制生成文本的多样性。它考虑最可能的多个token，而不仅仅是最可能的一个。最后，我们使用分词器解码生成的响应，将其转换回人类语言。
- en: 'We use the same prompt, “`the emperor`,” and here is our version of *War and
    Peace* in 100 words:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用相同的提示文本“`the emperor`”，以下是我们版本的*《战争与和平》*，共100个字：
- en: '[PRE43]'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We’ve successfully generated our own version of *War and Peace* using the fine-tuned
    GPT-2 model. It reads better than the LSTM version in *Chapter 12*.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功地使用微调后的GPT-2模型生成了我们自己的*《战争与和平》*版本。它比LSTM版本的*第12章*阅读体验要好。
- en: GPT is a decoder-only Transformer architecture. It’s all about making things
    up on the fly, one token at a time. Unlike some other Transformer-based models,
    it doesn’t need a separate step (encoding) to understand what you feed it. This
    lets it focus on generating text that flows naturally, like a creative writer.
    Once GPT is trained on a massive pile of text, we can fine-tune it for specific
    tasks with smaller datasets. Think of it like teaching a master storyteller to
    write about a specific topic, like history or science fiction. In our example,
    we generated our own version of *War and Peace* by fine-tuning a GPT-2 model.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: GPT是一种仅解码的Transformer架构。它的核心是一次生成一个token。与一些其他基于Transformer的模型不同，它不需要额外的步骤（编码）来理解输入的内容。这使得它能够专注于生成自然流畅的文本，像一个创造性的作家。一旦GPT在大量文本数据上进行训练，我们可以通过小型数据集对其进行微调，适应特定任务。可以把它想象成教一个大师级的故事讲述者，去写关于特定主题的内容，比如历史或科幻。在我们的例子中，我们通过微调GPT-2模型生成了自己的*《战争与和平》*版本。
- en: 'While BERT focuses on bidirectional pre-training, GPT is autoregressive and
    predicts the next word in a sequence. You may wonder whether there is a model
    that combines aspects of both bidirectional understanding and auto-regressive
    generation. The answer is yes – **Bidirectional and Auto-Regressive Transformers**
    (**BART**). It was introduced by Facebook AI (*BART: Bidirectional and Autoregressive
    Transformers for Sequence-to-Sequence Learning* by Lewis et al., 2019), and designed
    to combine both strengths.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然BERT专注于双向预训练，GPT是自回归的，预测序列中的下一个单词。你可能会想，是否存在一种结合了双向理解和自回归生成的模型？答案是有的——**双向和自回归Transformer**（**BART**）。它由Facebook
    AI推出（《BART: 双向和自回归Transformer用于序列到序列学习》，Lewis等，2019年），旨在结合两者的优势。'
- en: Summary
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: This chapter was all about Transformer, a powerful neural network architecture
    designed for sequence-to-sequence tasks. Its key ingredient, self-attention, lets
    the model focus on the most important parts of the information it’s looking at
    in a sequence.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要讲解了Transformer，这是一种强大的神经网络架构，专为序列到序列任务设计。其关键成分——自注意力机制，使模型能够专注于序列中最重要的信息部分。
- en: 'We worked on two NLP projects: sentiment analysis and text generation using
    two state-of-the-art Transformer models, BERT and GPT. We observed an elevated
    performance compared to what we did in the last chapter. We also learned how to
    fine-tune these Transformers with the Hugging Face library, a one-stop shop for
    loading pre-trained models, performing different NLP tasks, and fine-tuning models
    on your own data. Plus, it throws in some bonus tools for chopping up text, checking
    how well the model did, and even generating some text of its own.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行了两个NLP项目：情感分析和文本生成，使用了两种最先进的Transformer模型——BERT和GPT。与上一章的工作相比，我们观察到了更高的性能。我们还学习了如何使用Hugging
    Face库微调这些Transformer模型，该库是加载预训练模型、执行不同NLP任务以及在你自己的数据上微调模型的一站式平台。除此之外，它还提供了一些额外工具，帮助切割文本、检查模型表现，甚至生成一些文本。
- en: In the next chapter, we will focus on another OpenAI cutting-edge model, CLIP,
    and will implement natural language-based image search.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将聚焦于另一个OpenAI的前沿模型——CLIP，并实现基于自然语言的图像搜索。
- en: Exercises
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you compute the positional encoding for the third word “`learning`" in the
    example sentence “`python machine learning`" using a four-dimensional vector?
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能计算出示例句子“`python machine learning`”中第三个单词“`learning`”的位置信息编码吗？请使用一个四维向量。
- en: Can you fine-tune a BERT model for topic classification? You can take the newsgroups
    dataset as an example.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能为主题分类微调一个BERT模型吗？你可以以新闻组数据集为例。
- en: Can you fine-tune a BART model ([https://huggingface.co/facebook/bart-base](https://huggingface.co/facebook/bart-base))
    to write your own version of *War and Peace*?
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能微调一个BART模型（[https://huggingface.co/facebook/bart-base](https://huggingface.co/facebook/bart-base)）来编写你自己的《战争与和平》版本吗？
- en: Join our book’s Discord space
  id: totrans-281
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
