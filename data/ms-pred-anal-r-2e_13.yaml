- en: Chapter 13. Scaling Up
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。扩展
- en: Up until now, we have reviewed a steady stream of pertinent topics concerning
    *statistics* and specifically, *predictive analytics*. In this chapter, we look
    to provide a tutorial dedicated to applying those concepts and practices to very
    large datasets. First, we'll begin by defining the phrase very large – at least
    as it is used to describe data defined (that we want to train our predictive models
    on or run our statistical algorithms against). Next, we will review the list of
    the challenges imposed by using bigger data sources, and finally, we will offer
    some ideas for meeting these challenges.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经回顾了一系列与*统计学*和特别是*预测分析*相关的重要主题。在本章中，我们将提供一篇教程，专门介绍如何将这些概念和实践应用于非常大的数据集。首先，我们将从定义“非常大”这个短语开始——至少就其用于描述数据定义（我们希望用它来训练我们的预测模型或运行我们的统计算法）而言。接下来，我们将回顾使用更大数据源带来的挑战列表，最后，我们将提出一些应对这些挑战的想法。
- en: 'Our chapter is broken down into the following sections:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本章分为以下部分：
- en: Getting started
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始
- en: The phases of an analytics project
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析项目的阶段
- en: Experience and data of scale
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 经验和数据规模
- en: The characteristics of big data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据的特征
- en: Training models at scale
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模化训练模型
- en: The specific challenges (of big data)
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定挑战（大数据）
- en: A path forward
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前进的道路
- en: Starting the project
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始项目
- en: The phases of a general purpose predictive analytics project may be straightforward
    and perhaps easy (it's the practice of carrying out each of these phases effectively
    that is challenging).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 通用预测分析项目的阶段可能很简单，也许很容易（真正具有挑战性的是有效地执行每个阶段）。
- en: '![Starting the project](img/00212.jpeg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![开始项目](img/00212.jpeg)'
- en: The Phases of a predictive analytics project
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分析项目的阶段
- en: 'These phases are:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这些阶段包括：
- en: '**Define** (the data).'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**定义**（数据）。'
- en: '**Profile & Prepare** (the data).'
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**概要和准备**（数据）。'
- en: '**Determine the Question** (what to predict).'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**确定问题**（要预测什么）。'
- en: '**Choose** the algorithm.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择**算法。'
- en: '**Apply** the model.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用**模型。'
- en: Data definition
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据定义
- en: 'An interesting thought:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个有趣的思考：
- en: '"…Once you have enough data, you start to see patterns," he said. "You can
    build a model of how these data work. Once you build a model, you can predict…"'
  id: totrans-22
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “……一旦你有了足够的数据，你就会开始看到模式，”他说。“你可以建立一个这些数据如何工作的模型。一旦你建立了模型，你就可以预测……”
- en: ''
  id: totrans-23
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: – Bertolucci, 2013
  id: totrans-24
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: – 贝托鲁奇，2013
- en: 'At the beginning of any (and every) analytics project, data is defined – reviewed
    and analyzed: source, format, state, interval, and so on (some refer to this as
    the process of investigating the breadth and depth of available data).'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何（以及每一个）分析项目的开始阶段，数据被定义——审查和分析：来源、格式、状态、间隔等（有些人将此称为调查可用数据的广度和深度的过程）。
- en: One exercise demanded is to perform what is referred to as profiling the data
    source, or to establishing your data's profile by determining its characteristics,
    relationships, and patterns (and context). This process will, hopefully, produce
    a clearer view of the content and quality the data to be used in the project –
    that is, the data profile.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 一个要求进行的练习是执行所谓的数据源概要分析，或者通过确定其特征、关系和模式（以及上下文）来建立你的数据概要。这个过程有望产生对将要用于项目的数据的内容和质量的一个更清晰的看法——即数据概要。
- en: Then, after the exercise of profiling is completed, one would most likely proceed
    with performing some form of data scrubbing (this is also sometimes referred to
    as cleansing or in some cases preparing) in an effort to improve its level of
    quality. During the process of cleansing or scrubbing your data, you would most
    likely perform tasks such as aggregation, appending, merging, reformatting fields,
    changing variable types or adding missing values, and so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在完成数据概要分析之后，人们很可能会进行某种形式的数据清洗（这有时也被称为净化或在某些情况下准备），以努力提高其质量水平。在清洗或清洗数据的过程中，你很可能会执行诸如聚合、追加、合并、重新格式化字段、更改变量类型或添加缺失值等任务。
- en: Note
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Data profiling techniques can include specific analysis types such as a *univariate
    analysis* which involves frequency analysis for categorical variables and understanding
    distribution and summary statistics for continuous variables. This aids in missing
    value treatment, understanding distribution, and outlier treatment.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据概要技术可以包括特定的分析类型，例如*单变量分析*，它涉及对分类变量的频率分析以及对连续变量的分布和汇总统计的理解。这有助于处理缺失值、理解分布和异常值处理。
- en: Experience
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 经验
- en: When soliciting advice from a **subject matter expert** (**SME**), one would
    probably likely agree that an individual with more experience most likely will
    be able to provide a better service. With predictive analytics projects, the objective
    is not what the data can tell us, but what the data can tell us about an objective
    or problem, therefore, the size or amount of the data source (the amount of experience)
    available for the project becomes much more important. Typically, the more the
    data, the better.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当向**主题专家**（SME）寻求建议时，很可能会同意一个经验更丰富的人更有可能提供更好的服务。在预测分析项目中，目标不是数据能告诉我们什么，而是数据能告诉我们关于目标或问题的什么，因此，可用于项目的数据源的大小或数量（经验量）变得更为重要。通常情况下，数据越多，越好。
- en: So, at what point is it acceptable to say you have enough data for your predictive
    project? The politically correct answer to this question is that it depends. Some
    types of data science and predictive analysis projects require more specific data
    requirements than others will, effectively setting what the minimal data volumes
    might be.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，在什么情况下可以说你的预测项目已经有了足够的数据？对这个问题的政治正确答案是这取决于具体情况。某些类型的数据科学和预测分析项目可能需要比其他项目更具体的数据要求，从而实际上确定了可能的最小数据量。
- en: In an extreme case, predicting may require data spanning many years or even
    many decades – as larger amounts of data can yield a breadth of patterns surrounding
    behaviors and decisions, and so on. Why? Because typically, analyzing (or training
    a model with) more data develops a more comprehensive understanding or a **better
    prediction**.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在极端情况下，预测可能需要跨越多年甚至数十年的数据——因为更多的数据可以产生围绕行为和决策等方面的广泛模式。为什么？因为通常情况下，使用更多数据进行分析（或用数据训练模型）可以发展出更全面的理解或**更好的预测**。
- en: With this in mind, perhaps a general rule of thumb is to collect as much data
    as possible (depending upon the objective or type of application). Some experts
    might suggest collecting at least three years ', and preferably five years', worth
    of data before beginning any predictive analysis project. Of course, years may
    not be the appropriate measure depending upon the type of application. For example,
    cases might be more appropriate or lines of text, and so on.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，可能的一个一般性规则是尽可能收集尽可能多的数据（取决于目标或应用类型）。一些专家可能会建议在开始任何预测分析项目之前收集至少三年的数据，最好是五年的数据。当然，根据应用类型，年可能不是合适的度量单位。例如，案例可能更适合或文本行，等等。
- en: Note
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: In practice, if an application was built around hospital visits, the more patient
    cases (typically millions) the better; a word predictor application would want
    to have as many text sentences or word phrases (tens of millions) as it could
    (to be effective).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，如果一个应用是基于医院访问构建的，那么患者案例（通常是数百万）越多越好；一个单词预测应用希望拥有尽可能多的文本句子或单词短语（数千万）以有效（使用）。
- en: Another predictive analytics data controversy might be understanding the idea
    of *sufficient* versus *enough*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个预测分析数据的争议可能是理解**足够**与**充足**的概念。
- en: In some cases, given a shortage of volume or *quantity*, the wise data scientist
    would always focus on the *quality* or suitability of the data. This means that
    even though the volume of data is less than hoped for, the quality of the data,
    based upon the objective of the project, is deemed sufficient.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，如果数据量或**数量**不足，明智的数据科学家会始终关注数据的**质量**或适用性。这意味着尽管数据量少于预期，但根据项目的目标，数据的质量被认为是足够的。
- en: Given an understanding of all of the preceding points, it is important to gauge
    your data to determine if the volume of your data has reached the tipping point
    – that point where typical analytical activities begin to become onerous to perform.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解了所有上述要点之后，评估你的数据以确定你的数据量是否已经达到临界点——即典型分析活动开始变得难以执行的那个点——是很重要的。
- en: In the next section, we will cover how to establish that data volume tipping
    point as it is always better to understand and expect challenges before you begin
    your heavy model training rather than finding out the hard way, after you've already
    begun.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何确定数据量的临界点，因为在开始重型模型训练之前理解和预期挑战总是比在已经开始之后发现困难的方法要好。
- en: Data of scale – big data
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规模数据——大数据
- en: When we use the phrase, data of scale we are not referring to the statistical
    measurement scales of interval, ordinal, nominal, and dichotomous. We are using
    the phrase loosely to convey the size, volume, or complexity of the data source
    to be used in your analytics project.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用“数据规模”这个短语时，我们并不是指区间、顺序、名义和二分的统计测量尺度。我们在这里使用这个短语是松散的，以传达在您的分析项目中将要使用的数据源的大小、量或复杂性。
- en: The, by now, well-known buzz word, **big data** might (loosely) fit here, so
    let us take pause here to define how we are using the term big data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，众所周知的时髦词**大数据**可能（松散地）适用于这里，所以让我们在这里停下来，定义我们是如何使用大数据这个术语的。
- en: A large assemblage of data, datasets that are so large or complex that traditional
    data processing applications are inadequate, and data about every aspect of our
    lives have all been used to define or refer to big data.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 大量的数据集合，如此之大或复杂以至于传统的数据处理应用不足，以及关于我们生活各个方面的数据都被用来定义或指代大数据。
- en: 'The following diagram illustrates big data''s three v''s:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了大数据的三个V：
- en: '![Data of scale – big data](img/00213.jpeg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![数据规模——大数据](img/00213.jpeg)'
- en: 'In 2001, then Gartner analyst *Doug Laney* introduced the 3Vs concept to describe
    the occurrence of big data. The 3Vs, according to *Laney*, are volume, variety,
    and velocity. The Vs make up the dimensionality of big data: volume (or the measurable
    amount of data), variety (meaning the number of types of data), and velocity (referring
    to the speed of processing or dealing with that data).'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 2001年，当时的Gartner分析师**道格·兰尼**提出了3Vs概念来描述大数据的发生。根据兰尼的说法，3Vs是量、种类和速度。Vs构成了大数据的维度：量（或可测量的数据量）、种类（意味着数据类型的数量）和速度（指处理或处理该数据的速度）。
- en: Note
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Laney''s explanation can be reviewed here: [http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf)).'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 兰尼的解释可以在这里查看：[http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf](http://blogs.gartner.com/doug-laney/files/2012/01/ad949-3D-Data-Management-Controlling-Data-Volume-Velocity-and-Variety.pdf))。
- en: Using the *volume*, *variety*, and *velocity* concept, it is easier to foresee
    how a *big data* source can be or quickly become increasingly challenging to work
    with, and as these dimensions' increase or expand they will only encumber the
    ability to effectively train predictive models on the data further.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*量*、*种类*和*速度*的概念，可以更容易地预见一个*大数据*源如何变得或迅速变得难以处理，并且随着这些维度的增加或扩展，它们将只会阻碍在数据上有效训练预测模型的能力。
- en: Using Excel to gauge your data
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Excel衡量你的数据
- en: Microsoft Excel is not a tool to be used to determine if your data qualifies
    as big data.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 微软Excel不是一个用来确定你的数据是否符合大数据标准的工具。
- en: '**If your data is too big for Microsoft Excel it still really doesn''t necessarily
    qualify as big data.** In fact, gigabytes of data, still manageable with various
    techniques, enterprise, and even open source tools, especially with the lower
    cost of storage today.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**如果你的数据太大以至于无法使用微软Excel处理，这并不意味着它一定符合大数据的标准。** 事实上，即使是几吉字节的数据，仍然可以通过各种技术、企业级甚至开源工具来管理，尤其是在今天存储成本较低的情况下。'
- en: It is important to be able to realistically size or scale the data technology
    (keeping in mind expected data growth rates) you will be using in your predictive
    project before selecting an approach or even beginning any profiling or preparing
    work effort. This time is well spent as it will save time later that may be lost
    due to performance bottlenecks or rewriting scripts to use a different approach
    (one that can handle bigger data sources).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择方法或开始任何配置文件或准备工作之前，能够现实地评估或调整你将在预测项目中使用的数据技术（考虑到预期的数据增长速度）是很重要的。这段时间花得很值得，因为它将节省以后可能因性能瓶颈或重写脚本以使用不同方法（可以处理更大数据源的方法）而失去的时间。
- en: So, the question becomes, how do you gauge your data – is it really big data?
    Is it manageable? Or does it fall into that category that will require special
    handling or pre-processing before it can be effectively used for your predictive
    analytics objective?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题变成了，你如何衡量你的数据——它真的是大数据吗？它是可管理的吗？还是它属于需要特殊处理或预处理才能有效用于预测分析目标的那一类？
- en: Characteristics of big data
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据的特征
- en: 'For you to determine if your data source qualifies as big data or as needing
    special handling, you can start by examining your data source in the following
    areas:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定你的数据源是否属于大数据或需要特殊处理，你可以从以下方面开始检查你的数据源：
- en: The volume (amount) of data.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据的体积（数量）。
- en: The variety of data.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据的多样性。
- en: The number of different sources and spans of the data.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不同数据源的数量和数据跨度。
- en: Let's examine each of these areas.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一考察这些领域。
- en: Volume
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 体积
- en: If you are talking about the number of rows or records, then most likely your
    data source is not a big data source since big data is typically measured in gigabytes,
    terabytes, and petabytes. However, space doesn't always mean big, as these size
    measurements can vary greatly in terms of both volume and functionality. Additionally,
    data sources of several million records may qualify as big data, given their structure
    (or lack of structure).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你谈论的是行数或记录数，那么你的数据源很可能不是大数据源，因为大数据通常以千兆字节、太字节和拍字节来衡量。然而，空间并不总是意味着大数据，因为这些尺寸测量在体积和功能方面可能有很大的差异。此外，具有数百万条记录的数据源，如果其结构（或缺乏结构）符合条件，也可能被视为大数据。
- en: Varieties
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多样性
- en: Data used in predictive models may be structured or unstructured (or both) and
    include transactions from databases, survey results, website logs, application
    messages, and so on (by using a data source consisting of a higher variety of
    data, you are usually able to cover a broader context for the analytics you derive
    from it). Variety, much like volume, is considered a normal qualifier for big
    data.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 用于预测模型的数据可能是结构化的、非结构化的（或两者兼有），包括来自数据库的交易、调查结果、网站日志、应用程序消息等（通过使用包含更多样化数据的数据源，你通常能够覆盖更广泛的上下文，从而从其中获得的分析）。多样性与体积一样，被视为大数据的正常标准。
- en: Sources and spans
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据源和跨度
- en: If the data source for your predictive analytics project is the result of integrating
    several sources, you most likely hit on both criteria of volume and variety and
    your data qualifies as big data. If your project uses data that is affected by
    governmental mandates, consumer requests is a historical analysis, you are almost
    certainty using big data. Government regulations usually require that certain
    types of data need to be stored for several years. Products can be consumer driven
    over the lifetime of the product and with today's trends, historical analysis
    data is usually available for more than five years. Again, all examples of big
    data sources.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的预测分析项目数据源是整合了多个来源的结果，你很可能同时满足了体积和多样性的标准，你的数据可以被视为大数据。如果你的项目使用受政府法规影响的数据，如消费者请求的历史分析，你几乎可以确定正在使用大数据。政府法规通常要求某些类型的数据需要存储数年。产品在其生命周期内可能由消费者驱动，并且根据今天的趋势，历史分析数据通常可用超过五年。再次强调，这些都是大数据来源的例子。
- en: Structure
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结构
- en: 'You will often find that data sources typically fall into one of the following
    three categories:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会发现数据源通常属于以下三个类别之一：
- en: Sources with little or no structure in the data (such as simple text files).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据结构化程度低或没有结构的数据源（如简单的文本文件）。
- en: Sources containing both structured and unstructured data (like data that is
    sourced from document management systems or various websites, and so on).
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含结构化和非结构化数据（如来自文档管理系统或各种网站的来源）的数据源。
- en: Sources containing highly structured data (like transactional data stored in
    a relational database example).
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含高度结构化数据（如存储在关系型数据库中的交易数据）的数据源。
- en: how your data source is categorized will determine how you prepare and work
    with your data in each phase of your predictive analytics project.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据源如何分类将决定你在预测分析项目的每个阶段如何准备和操作你的数据。
- en: Although data sources with structure can obviously still fall into the category
    of big data, it's data containing both structured and unstructured data (and of
    course totally unstructured data) that fit as big data and will require special
    handling and or pre-processing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然具有结构的数据源显然仍然可以归入大数据类别，但包含结构化和非结构化数据（以及当然完全非结构化数据）的数据源才符合大数据的定义，并且需要特殊处理或预处理。
- en: Statistical noise
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计噪声
- en: Finally, we should take a note here that other factors (other than those discussed
    already in the chapter) can qualify your project data source as being unwieldy,
    overly complex, or a big data source.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们应该注意，除了本章中已经讨论的因素之外，其他因素也可以使你的项目数据源被视为难以处理、过于复杂或大数据源。
- en: 'These include (but are not limited to):'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括（但不限于）：
- en: Statistical noise (a term for recognized amounts of unexplained variations within
    the data)
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计噪声（一个术语，用于描述数据中未解释的变异量）
- en: Data suffering from mismatched understandings (the differences in interpretations
    of the data by communities, cultures, practices, and so on)
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存在理解不匹配（社区、文化、实践等对数据的解释差异）
- en: And others
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及其他
- en: Once you have determined that the data source that you will be using in your
    predictive analytics project seems to qualify as big (again as we are using the
    term here) then you can proceed with the process of deciding how to manage and
    manipulate that data source, based upon the known challenges this type of data
    demands, so as to be most effective.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定你将在预测分析项目中使用的数据源似乎符合大的标准（再次强调，我们在这里使用这个术语），那么你可以继续决定如何管理和操作这个数据源的过程，基于这类数据所要求的已知挑战，以便最有效地进行。
- en: In the next section, we will review some of these common problems, before we
    go on to offer useable solutions.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将在继续提供可用的解决方案之前，回顾一些这些常见问题。
- en: Training models at scale
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规模化训练模型
- en: In an earlier section of this chapter, we listed and studied what the industry
    experts agree on as the most common phases of any predictive analytics project.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的早期部分，我们列出并研究了行业专家一致认为的任何预测分析项目最常见阶段的内容。
- en: 'To recall, they are as follows:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，它们如下：
- en: Defining the data source
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义数据源
- en: Profiling and preparation of the data source
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据源的配置文件和准备
- en: Determining the question(s) that you want to ask your data
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定你想向你的数据提出的问题（们）
- en: Choosing an algorithm to train on the data source
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择在数据源上训练的算法
- en: Application of a predictive model
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用预测模型
- en: In a predictive analytics project using big data, those same phases are present,
    but may be slightly varied and require some supplementary efforts.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用大数据的预测分析项目中，这些相同的阶段都存在，但可能略有变化，并需要一些额外的努力。
- en: Pain by phase
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 阶段性疼痛
- en: In the initial phase of a project, once you've chosen a source for your data
    (determined the data source), the data must be attained. Some industry experts
    describe this as the acquisition and recording of data. In a predictive project
    that involves a more common data source, access to the data might be as straightforward
    as opening a file on your local disk; with a big data source, it's a bit more
    difficult. For example, suppose your project sources data from a combination of
    devices (multiple servers and many mobile devices, that is, the Internet of Things
    data).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在项目的初期阶段，一旦你选择了数据源（确定了数据源），就必须获取数据。一些行业专家将此描述为数据的获取和记录。在一个涉及更常见数据源的预测项目中，访问数据可能就像在你的本地磁盘上打开一个文件一样简单；而对于大数据源来说，则要复杂得多。例如，假设你的项目从多种设备（多个服务器和许多移动设备，即物联网数据）的组合中获取数据。
- en: This activity-generated data might include a combination of website tracking
    information, application logs, sensor data – among other machine-generated content
    – perfect for your analysis. You can see how the effort to access this information
    as a single data source for your project would take some effort (and expertise!).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种活动生成数据可能包括网站跟踪信息、应用程序日志、传感器数据——以及其他机器生成内容——非常适合你的分析。你可以看到，将获取这些信息作为你项目单一数据源的努力需要一些努力（以及专业知识！）。
- en: 'In the profiling and preparation phase, data is extracted, cleaned, and annotated.
    Typically, any analytics project will require this pre-processing of the data:
    setting context, identifying operational definitions and statistical types, and
    so on. This step is critical as this is the phase where we establish an understanding
    of the data challenges so that later surprises can be minimized. This phase usually
    involves time spent querying and re-querying the data, creating visualizations
    to validate findings, and then performing updates to the data to address areas
    of concern. Big data inhibits these activities since it includes either more data
    to process, is perhaps inconsistent in format, and could be changing rapidly.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置文件和准备阶段，数据被提取、清理和注释。通常，任何分析项目都将需要这种数据预处理：设置上下文、确定操作定义和统计类型等。这一步至关重要，因为这是我们建立对数据挑战理解的过程，以便以后可以最小化意外。这一阶段通常涉及花费时间查询和重新查询数据、创建可视化以验证发现，然后更新数据以解决关注领域。大数据阻碍了这些活动，因为它可能包括更多需要处理的数据，格式可能不一致，并且可能变化迅速。
- en: In the phase where question determination takes place, data integration, aggregation,
    and the representation of the data must be considered so that the proper questions
    to ask the data can be identified. This phase may be divided into three steps;
    preparation, integration, and determination (of questions). The prep step involves
    assembling the data, identification of unique keys, aggregation/duplication, scrubbing
    as required, format manipulation, and perhaps mapping of values. The integration
    step involves merging data, testing, and reconciliation. Finally, project questions
    are established. Once again, big data's volumes, varieties, and velocities can
    slow this phase down considerably.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在问题确定阶段，必须考虑数据集成、聚合和数据表示，以便可以确定向数据提出的问题。这一阶段可以分为三个步骤；准备、集成和问题确定。准备步骤涉及组装数据、识别唯一键、聚合/重复、按要求进行清理、格式操作，也许还有值的映射。集成步骤涉及合并数据、测试和协调。最后，确定项目问题。再次强调，大数据的量、种类和速度可能会显著减缓这一阶段。
- en: Choosing an algorithm and application of a predictive model are the phases where
    there is analysis, modeling, and interpretation of the data. Considering the volumes,
    varieties, and velocities of a big data source, selecting the appropriate algorithm
    to be used to train data can be much more involved. An example would be the idea
    that predictive modeling works best given the lowest level of granularity possible
    and, in the previous phase, perhaps the sheer volume of a big data source required
    that extensive aggregation be done, thus potentially burying anomalies and variations
    that exist within the data.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 选择算法和预测模型的应用是分析、建模和解释数据的阶段。考虑到大数据源的量、种类和速度，选择用于训练数据的适当算法可能更加复杂。一个例子是预测建模在可能的最小粒度下效果最好，在前一阶段，大数据源的
    sheer volume 可能需要大量聚合，从而可能埋没了数据中存在的异常和变化。
- en: Specific challenges
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 具体挑战
- en: 'Let''s take a few moments to address some very specific challenges brought
    on by big data. Among these top topics are:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花几分钟时间来讨论一下大数据带来的具体挑战。其中一些主要话题包括：
- en: Heterogeneity
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异质性
- en: Scale
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 规模
- en: Location
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置
- en: Timeliness
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 及时性
- en: Privacy
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐私
- en: Collaborations
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 合作
- en: Reproducibility
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可重复性
- en: Heterogeneity
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 异质性
- en: By variety, we usually need to consider the heterogeneity of data types, representation,
    and semantic interpretation. Efforts to correctly review and understand these
    variations in big data sources can be time consuming and complex. Interestingly,
    an element may be homogeneous (more uniform) on a larger scale, compared to being
    heterogeneous (less uniform) on a smaller scale. This means that your approach
    to addressing a big data source may cause very different results!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过多样性，我们通常需要考虑数据类型、表示和语义解释的异质性。正确审查和理解大数据源中这些变化的工作可能既耗时又复杂。有趣的是，一个元素在较大尺度上可能是同质的（更均匀），而与较小尺度上的异质（不均匀）相比。这意味着你处理大数据源的方法可能会导致非常不同的结果！
- en: Scale
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 规模
- en: We've already touched on the idea of scale – typically scale refers to the sheer
    size of the data source, but could also refer to its complexities.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了规模的概念——通常规模指的是数据源的 sheer size，但也可能指的是其复杂性。
- en: Location
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 位置
- en: Typically, you'll see that when you decide to use a big data source, it is not
    located all in one place, but spread throughout electronic space. This means that
    any process (manual or automated) will have to consolidate the data – physically
    or virtually before it can be properly used in a project.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当你决定使用大数据源时，你会发现它并不位于一个地方，而是分散在电子空间中。这意味着任何过程（手动或自动化）都必须在数据能够被项目正确使用之前，对数据进行物理或虚拟的整合。
- en: Timeliness
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 及时性
- en: The bigger the data, the more time it will take to analyze. However, it is not
    just this time that is meant when one speaks of velocity in the context of big
    data. Rather, there is the challenge of the acquisition rate of the data. In other
    words, with data piling up or being updated continuously within the data source,
    when (or how often) is the correct snapshot established? In addition, scanning
    the entire data source to find a suitable sample pertinent to a particular predictive
    analytics objective is obviously impractical.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 数据量越大，分析所需的时间就越长。然而，当人们在大数据背景下谈论速度时，并不仅仅是指这个时间。相反，数据获取率是一个挑战。换句话说，当数据源中的数据不断累积或更新时，何时（或多长时间）才能建立正确的快照？此外，扫描整个数据源以找到与特定预测分析目标相关的合适样本显然是不切实际的。
- en: Privacy
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐私
- en: Data privacy should be a consideration when using any data source, and one that
    increases in complexity in the context of big data. The most well-known example
    of this is with electronic health records – which have strict laws governing them.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用任何数据源时，都应该考虑数据隐私，在大数据背景下，这种考虑会变得更加复杂。最著名的例子是电子健康记录——它们受到严格的法律法规约束。
- en: Suppose, for example having the requirement to pre-process a big data source
    that is over a terabyte in size to hide both a user's identify and location information?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，需要预处理一个超过一太字节大小的数据源，以隐藏用户的身份和位置信息？
- en: Collaborations
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 协作
- en: One might think that in this day and age, analytics and predictive models are
    entirely computational (especially when you hear the term machine learning), however,
    no matter how advanced a predicative algorithm or model proclaims to be, there
    remain many patterns in data that humans can simply detect, but computer algorithms
    no matter how complex in their logic, have a hard time finding.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 人们可能会认为，在这个时代，分析和预测模型完全是计算性的（尤其是当你听到机器学习这个术语时），然而，无论预测算法或模型声称多么先进，数据中仍然存在许多人类可以轻易察觉但计算机算法在逻辑上难以找到的模式。
- en: Note
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There is a new following in analytics that may be considered a sub-field of
    *visual* analytics that utilizes SME input, at least with respect to the modelling
    and analysis phase of a predictive project.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析领域出现了一种新的趋势，这可能被视为*视觉分析*的一个子领域，它利用了领域专家的知识，至少在预测项目的建模和分析阶段是这样。
- en: To include a subject matter expert in a predictive analytics project might not
    be a huge issue but, with a big data source, it often takes multiple experts from
    different domains to really understand what is going on with the data and to share
    their respective exploration of results and advice.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测分析项目中包含一个领域专家可能不是一个大问题，但面对大数据源时，通常需要来自不同领域的多位专家才能真正理解数据的情况，并分享他们各自的结果探索和建议。
- en: These multiple experts may be separated in space and time and be difficult to
    assemble in one location at one time. Again, causing additional time and effort
    to be spent.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这些多位专家可能在空间和时间上分散，难以在某一时间聚集在同一个地点。这再次导致需要花费额外的时间和精力。
- en: Reproducibility
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可重复性
- en: Believe it or not, most predictive analytics projects are repeated for a variety
    of reasons. For example, if the results are in question for any reason or if the
    data is suspect, all of the phases of the project may be repeated. Reproduction
    of a big data analytics project is seldom reasonable. In most cases, all that
    can be done is that the bad data in a big data resource will be found and flagged
    as such.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，大多数预测分析项目因各种原因而重复进行。例如，如果结果因任何原因受到质疑或数据存在疑点，项目的所有阶段都可能需要重复。大数据分析项目的重复很少是合理的。在大多数情况下，所能做的就是找到大数据资源中的不良数据并将其标记为不良数据。
- en: A path forward
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前进的道路
- en: So, the inkling of having more than enough data for training a model seems very
    appealing.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，拥有足够多的数据来训练模型的想法似乎非常吸引人。
- en: Big data sources would appear to answer this desire, however in practice, a
    big data source is not often (if ever) analyzed in its entirety. You can pretty
    much count on performing a sweeping filtering process aimed to reduce the big
    data into small(er) data (more on this in the next section).
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据源似乎能够满足这一需求，然而在实践中，大数据源很少（如果有的话）被完全分析。你可以相当肯定地执行一个广泛的过滤过程，旨在将大数据减少到小（一些）数据（更多内容将在下一节中介绍）。
- en: In the following section, we will review various approaches to addressing the
    various challenges of using big data as a source for your predictive analytics
    project.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾各种方法，以解决将大数据作为预测分析项目数据源所面临的挑战。
- en: Opportunities
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机会
- en: In this section, we offer a few recommendations for handling big data sources
    in predictive analytic projects using R. Also, we'll offer some practical use
    case examples.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们提供了一些关于在预测分析项目中使用R处理大数据源的推荐方法。此外，我们还将提供一些实际用例示例。
- en: Bigger data, bigger hardware
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更大的数据，更大的硬件
- en: We are starting with the most obvious option first.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从最明显的选项开始。
- en: To be clear, R keeps all of its objects in memory, which is a limitation if
    the data source gets too large. One of the easiest ways to deal with big data
    in R is simply to increase the machine's memory.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确，R将所有对象都保存在内存中，如果数据源太大，这会成为一个限制。在R中处理大数据的最简单方法之一就是增加机器的内存。
- en: At the time of writing, R can use 8 TB of RAM if it runs on a 64-bit machine
    (compared to only 2 GB addressable RAM on 32-bit machines). Most machines used
    for predictive analytics projects are (at least should be) 64-bit already, so
    you just need to add RAM.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，如果R在64位机器上运行，它可以使用8 TB的RAM（相比之下，32位机器上只有2 GB可寻址RAM）。用于预测分析项目的大多数机器（至少应该是）已经是64位，所以你只需要添加RAM。
- en: Note
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: There are both 32-bit and 64-bit versions of R. Do yourself a favor and use
    the 64-bit version!
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: R有32位和64位版本。请自己方便，使用64位版本！
- en: If you know your data source well and have added appropriate amounts of memory
    to your machine, then you'll most likely be OK to work with a big data source
    efficiently, especially if you use one of the approaches outlined in the following
    sections of this chapter.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对你的数据源非常了解，并且已经为你的机器添加了适当的内存，那么你很可能能够有效地处理大数据源，特别是如果你使用本章以下部分概述的方法之一。
- en: Breaking up
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分割
- en: One of the most straightforward and proven approaches to taming a big data source
    with R (or any language for that matter) is to create workable subsets of data
    prepared from the big data resource.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用R（或任何语言）驯服大数据源的最直接和最有效的方法之一是从大数据资源中创建可工作的数据子集。
- en: For example, suppose we have patient health records making up a current big
    data source. There are literally trillions of patient case records in the data
    with more added almost every minute. These cases record both the basics (sex,
    age, height, weight, and so on) as well as specifics around the patient's background
    (such as if the patient is a smoker, drinker, currently on medications, has ever
    been operated on, and so on). Luckily, our file does not contain any information
    that can be used to identify the patient (such as name or social security number)
    so we won't be in violation of any laws.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个由患者健康记录组成的当前大数据源。数据中实际上有数万亿的患者病例记录，几乎每分钟都在增加。这些病例记录了基本信息（性别、年龄、身高、体重等）以及患者背景的详细信息（例如，患者是否吸烟、饮酒、目前正在服用药物、是否曾经接受过手术等）。幸运的是，我们的文件不包含任何可以用来识别患者的个人信息（如姓名或社会保障号码），所以我们不会违反任何法律。
- en: The data source is fed by hospitals and doctors' offices all over the country.
    Our predictive project is one that is looking to determine relationships between
    a patient's health and the state that they live in. Rather than attempting to
    train on all of the data (a mostly impractical effort), we can use some logic
    to prepare our series of smaller, more workable subsets. For example, we could
    simply separate out our overall data source into 50 smaller files – one for each
    state. This would help, but the smaller files may still be massive, so with a
    little profiling of the data, we may be able to identify other measurements that
    we can use to divide our data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 数据源由全国各地的医院和诊所提供。我们的预测项目旨在确定患者健康状况与他们居住状态之间的关系。我们不是试图在所有数据上训练（这通常是一项不切实际的尝试），而是可以使用一些逻辑来准备一系列更小、更易于处理的数据子集。例如，我们可以简单地将整体数据源分成50个更小的文件——每个州一个。这将有所帮助，但较小的文件可能仍然很大，所以通过对数据进行一点分析，我们可能能够识别出我们可以用来划分数据的其他度量。
- en: 'The process of data discovery and separation might look pretty close to the
    following steps:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 数据发现和分离的过程可能看起来非常接近以下步骤：
- en: 'Since we are dealing with a big data source and are not sure of the number
    of cases or records within the file, we can start by creating an R data object
    from our comma separated file and restricting the number of records to be read:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们正在处理大数据源，并且不确定文件中的案例或记录数，我们可以从创建一个R数据对象开始，并限制要读取的记录数：
- en: '[PRE0]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`x` now contains 150 records that we can review looking for interesting measures
    we may be able to use to logically split our data on. You can also utilize the
    summary function to evaluate variables within the data source. For example, we
    see that column 9 is the patients'' home state, column 5 is the patients'' current
    body weight, and column 79 indicates the patients'' weight 1 year ago:![Breaking
    up](img/00214.jpeg)'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`x`现在包含150条记录，我们可以从中查找可能用于逻辑分割数据的有趣度量。您还可以使用summary函数评估数据源中的变量。例如，我们看到第9列是患者的家庭州，第5列是患者的当前体重，而第79列表示患者一年前的体重：![分割](img/00214.jpeg)'
- en: Now, we can perhaps create a series of smaller subsets where there are 50 state
    files, but each containing only cases that have patients who have gained more
    than five pounds in the past year:![Breaking up](img/00215.jpeg)
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们或许可以创建一系列较小的子集，其中包含50个州文件，但每个文件只包含在过去一年中体重增加超过五磅的患者案例：![分割](img/00215.jpeg)
- en: We do end up with 50 files, but each file should be much smaller and easier
    to work with then a single, large big data source. This is also a simple example
    and in practice, you may (and probably will) end up rerunning the split code and
    stitching together multiple state files.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终确实得到了50个文件，但每个文件应该比单个大型大数据源小得多，更容易处理。这只是一个简单的例子，在实践中，你可能（并且很可能）需要重新运行分割代码，并将多个州文件拼接在一起。
- en: The preceding is one example of how big data research typically works—by constructing
    smaller datasets that can be efficiently analyzed!
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是大数据研究通常工作方式的一个例子——通过构建可以高效分析的较小数据集！
- en: Sampling
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 采样
- en: Another method for dealing with the volume of a big data source is with population
    sampling.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理大数据源体积的方法是使用总体抽样。
- en: Sampling is a selection of or a subset of cases from within a statistical population
    intended to estimate or represent characteristics of the whole population. The
    net-net is the size of the data to be trained on, is reduced.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 采样是从统计总体中选择或选择子集的案例，目的是估计或代表整个群体的特征。简而言之，要训练的数据量减少了。
- en: There is some concern that sampling may decrease the performance (not as in
    processing time, but in the accuracy of results generated) of a model. This may
    be somewhat true as typically the more data the model is trained on, the better
    the result, but depending upon the objective, the decrease in performance can
    be negligible.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些担忧认为采样可能会降低模型的性能（不是指处理时间，而是指生成的结果的准确性）。这可能是部分正确的，因为通常模型训练的数据越多，结果越好，但根据目标的不同，性能的下降可能是可以忽略不计的。
- en: Overall, it is safe to say that if sampling can be avoided, it is recommendable
    to use another big data strategy. But if you find that sampling is necessary,
    it still can lead to satisfying models.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 总的来说，可以说如果可以避免采样，使用另一种大数据策略是可取的。但如果发现采样是必要的，它仍然可以导致令人满意的模型。
- en: When you use sampling as a big data predictive strategy, you should try to keep
    the sample as big as you can, consider carefully the size of the sample in proportion
    to the full population and ensure as best you can that the sample is not biased.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用采样作为大数据预测策略时，你应该尽量使样本尽可能大，仔细考虑样本大小与整个总体之间的比例，并尽可能确保样本没有偏差。
- en: One of the easiest methods for creating a sample is with the R function sample.
    Sample takes a sample of the specified size from the elements of `x` using either
    with or without replacement.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 创建样本的最简单方法之一是使用R函数sample。Sample从`x`的元素中抽取指定大小的样本，可以使用或不使用替换。
- en: 'The following lines of R code are a simple example of creating a random sample
    of 500 cases from our original data. Notice the row counts (indicated by using
    the R function `nrow`):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 以下R代码行是一个从原始数据中创建500个随机样本的简单示例。注意行数（通过使用R函数`nrow`表示）：
- en: '![Sampling](img/00216.jpeg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![采样](img/00216.jpeg)'
- en: Aggregation
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合
- en: Another method for reducing the size of a big data source (again depending on
    your projects' objectives) is by statistical aggregation of the data. In other
    words, you simply may not require the level of granularity in the data that is
    available.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少大数据源大小（再次取决于你的项目目标）的方法是通过数据的统计聚合。换句话说，你可能根本不需要数据中可用的粒度级别。
- en: In statistical data aggregation, the data can be combined from several measurements.
    This means that groups of observations are replaced with summary statistics based
    on those observations. Aggregation is used a lot in descriptive analytics, but
    can also be used to prepare data for a predictive project.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在统计数据聚合中，可以从多个测量值中组合数据。这意味着观察组的组合被基于这些观察值的汇总统计所取代。聚合在描述性分析中应用广泛，但也可以用于为预测项目准备数据。
- en: For larger and especially disparately located big data sources one might use
    a Hadoop and Hive (or similar technology) solution to aggregate the data. If the
    data is in a transactional database, you may even be able to use native SQL. In
    an all-natural R solution, you have more work to do.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更大且特别是分布不均的大数据源，可以使用 Hadoop 和 Hive（或类似技术）解决方案来聚合数据。如果数据在事务型数据库中，甚至可能使用原生 SQL。在纯
    R 解决方案中，你需要做更多的工作。
- en: R provides a convenient function named `aggregate` that can be used for big
    data aggregation, once you have determined how you want to (or need to) use the
    data in your project.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: R 提供了一个名为 `aggregate` 的方便函数，可用于大数据聚合，一旦你确定如何在你的项目中使用（或需要）数据。
- en: 'For example, the following code shows applying the function to the original
    data (stored in the data object named `x`) being aggregated on the 3 variables
    (patient `sex`):'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下代码展示了将函数应用于原始数据（存储在名为 `x` 的数据对象中）并按 3 个变量（患者 `sex`）进行聚合：
- en: '[PRE1]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Going back to an earlier section, to our example of splitting data into 50
    state files, we could potentially instead use the R code shown as follows to aggregate
    and generate summary statistics by state. Notice that the original case count
    was 5,994 and after aggregating the data, we have a case count of 50 (one summary
    record for each state):'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前面的部分，关于将数据拆分成 50 个州文件的例子，我们可能可以使用以下 R 代码来聚合并按州生成汇总统计。注意，原始案例数为 5,994，在聚合数据后，我们有
    50 个案例（每个州一个汇总记录）：
- en: '![Aggregation](img/00217.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![聚合](img/00217.jpeg)'
- en: Dimensional reduction
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低
- en: In [Chapter 8](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 8. Dimensionality Reduction"), *Dimensionality Reduction*, we introduced
    the process of dimensional reduction, which (as we pointed out then) allows the
    data scientist to minimize the data's dimensionality, but can also reduce the
    overall volume of a big data source, thereby reducing the amount of time and memory
    required for processing the data, allowing it to be more easily visualized, and
    eliminating features irrelevant to the model's purpose, reduce model noise, and
    so on.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 8 章](part0069_split_000.html#21PMQ2-c6198d576bbb4f42b630392bd61137d7 "第
    8 章。维度降低")，*维度降低*中，我们介绍了维度降低的过程，这个过程（正如我们当时所指出的）允许数据科学家最小化数据的维度，但也可以减少大数据源的整体体积，从而减少处理数据所需的时间和内存，使其更容易可视化，并消除与模型目的无关的特征，减少模型噪声等。
- en: Like breaking up the data into smaller more manageable files, using dimensional
    reduction will help, but takes a good understanding of the data as well as perhaps
    plenty of processing steps to eventually produce a workable data population.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 就像将数据拆分成更小的、更易于管理的文件一样，使用维度降低会有所帮助，但这也需要良好的数据理解以及可能的大量处理步骤，最终产生一个可工作的数据集。
- en: Alternatives
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 替代方案
- en: Since R is an in-memory language, it sometimes has a reputation of not being
    able to handle big data. However, using some creativity and strategic thinking,
    you can use big data in your predictive analytics projects quite successfully.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 R 是内存语言，它有时被认为无法处理大数据。然而，通过一些创造性和战略性的思考，你可以在预测分析项目中相当成功地使用大数据。
- en: 'In addition to the preceding approaches, there are currently a number of alternative
    approaches you may wish to research, such as:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述方法之外，目前还有许多其他替代方法你可能希望研究，例如：
- en: Chunking
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分块处理
- en: 'There are packages available that avoid storing data in memory. Instead, objects
    are stored on hard disk and analyzed in chunks. As a side effect, the chunking
    also leads naturally to parallelization, if the algorithms allow parallel analysis
    of the chunks in principle. You can search: Revolution R Enterprise for some background
    on the topic.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些包可以避免在内存中存储数据。相反，对象存储在硬盘上，并以块的形式进行分析。作为副作用，如果算法允许对块进行并行分析，分块也会自然地导致并行化。你可以搜索：Revolution
    R Enterprise 了解该主题的一些背景信息。
- en: Alternative language integrations
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替代语言集成
- en: Integrating higher performing programming languages is becoming a popular alternative
    to dealing with big data sources in R. This concept takes portions of R code and
    moves them to another language that may be better suited to carry out the logic
    or work than R is. This blends the best of R while avoiding performance bottlenecks.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中集成性能更高的编程语言正成为处理大数据源的流行替代方案。这个概念将R代码的部分内容移动到另一种可能更适合执行逻辑或工作的语言中。这样做既融合了R的优点，又避免了性能瓶颈。
- en: This outsourcing of code chunks from R to another language can easily be hidden
    in functions. In this case, proficiency in other programming languages is mandatory
    for the developers, but not for the users of these functions.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 将R中的代码块外包给另一种语言可以很容易地隐藏在函数中。在这种情况下，开发者必须精通其他编程语言，但用户不需要。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we broke out a typical predict analytics project into phases
    and explained that the first phase is where you define what data is to be used.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将典型的预测分析项目分解为阶段，并解释了第一阶段是定义要使用哪些数据的地方。
- en: Typically, the more the data, there is better the performance (or results) of
    a predictive model, but at some point (as in the case of a big data source) there
    may be too much data, at least to effectively deal with.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，数据越多，预测模型的性能（或结果）就越好，但在某个时候（例如在大数据源的情况下），数据可能太多，至少难以有效处理。
- en: After reviewing the reasons why big data is so challenging, we instructed on
    how to gauge your data source, to qualify it as a big source, and then offered
    various proven techniques for addressing the common challenges of using big data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了大数据之所以如此具有挑战性的原因之后，我们指导了如何评估你的数据源，将其认定为大数据源，并提供了各种经过验证的技术来解决使用大数据的常见挑战。
