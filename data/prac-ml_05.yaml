- en: Chapter 5. Decision Tree based learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。基于决策树的学习
- en: Starting this chapter, we will take a deep dive into each of the Machine learning
    algorithms. We begin with a non-parametric supervised learning method, Decision
    trees, and advanced techniques, used for classification and regression. We will
    outline a business problem that can be addressed by building a Decision tree-based
    model and learn how it can be implemented in Apache Mahout, R, Julia, Apache Spark,
    and Python.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从本章开始，我们将深入研究每种机器学习算法。我们从一个非参数监督学习方法——决策树及其用于分类和回归的高级技术开始。我们将概述一个可以通过构建基于决策树的模型来解决的问题，并学习如何在Apache
    Mahout、R、Julia、Apache Spark和Python中实现它。
- en: 'The following topics are covered in depth in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章深入探讨了以下主题：
- en: 'Decision trees: definition, terminology, the need, advantages, and limitations.'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树：定义、术语、需求、优点和局限性。
- en: The basics of constructing and understanding Decision trees and some key aspects
    such as Information gain and Entropy. You will also learn to build regression,
    the classification of trees and measuring errors.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建和理解决策树的基本知识以及一些关键方面，如信息增益和熵。你还将学习构建回归树、树的分类以及测量误差。
- en: Understanding some common problems with Decision trees, need for pruning Decision
    trees, and techniques for pruning.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解决策树的一些常见问题、修剪决策树的需求以及修剪技术。
- en: You will learn Decision tree algorithms such as CART, C4.5, C5.0 and so on;
    and specialized trees such as Random forests, Oblique trees, Evolutionary and
    Hellinger trees.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学习决策树算法，如CART、C4.5、C5.0等；以及专门的树，如随机森林、斜树、进化树和Hellinger树。
- en: Understanding a business use case for classification and regression trees, and
    an implementation of the same using Apache Mahout, R, Apache Spark, and Julia
    and Python (scikit-learn) libraries and modules.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分类和回归树在业务用例中的应用，以及使用Apache Mahout、R、Apache Spark和Julia以及Python（scikit-learn）库和模块的实现。
- en: Decision trees
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees are known to be one of the most powerful and widely used modeling
    techniques in the field of Machine learning.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树被认为是机器学习领域中最为强大和广泛使用的建模技术之一。
- en: 'Decision trees naturally induce rules that can be used in data classification
    and prediction. Following is an example of a rule definition derived from building
    a Decision tree:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树自然地诱导出可用于数据分类和预测的规则。以下是从构建决策树中得出的规则定义示例：
- en: If (laptop model is *x*) and (manufactured by *y*) and (is *z* years old) and
    (with some owners being *k*) then (the battery life is *n* hours).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 如果（笔记本电脑型号是**x**）并且（由**y**制造）并且（**z**年）并且（某些所有者是**k**）那么（电池寿命是**n**小时）。
- en: When closely observed, these rules are expressed in simple, human readable,
    and comprehensible formats. Additionally, these rules can be stored for later
    reference in a data store. The following concept map depicts various characteristics
    and attributes of Decision trees that will be covered in the following sections.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当仔细观察时，这些规则以简单、可读和易于理解的形式表达。此外，这些规则可以存储在数据存储中，以便以后参考。以下的概念图展示了将在以下章节中涵盖的决策树的各种特性和属性。
- en: '![Decision trees](img/B03980_05_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![决策树](img/B03980_05_01.jpg)'
- en: Terminology
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 术语
- en: Decision trees classify instances by representing in a tree structure starting
    from the root to a leaf. Most importantly, at a high level, there are two representations
    of a Decision tree—a node and an arc that connects nodes. To make a decision,
    the flow starts at the root nodes, navigates to the arcs until it has reached
    a leaf node, and then makes a decision. Each node of the tree denotes testing
    of an attribute, and the branches denote the possible values that the attribute
    can take.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过从根节点到叶节点的树结构来表示实例的分类。最重要的是，在高级别上，决策树有两种表示形式——节点和连接节点的弧。为了做出决策，流程从根节点开始，导航到弧，直到达到叶节点，然后做出决策。树中的每个节点表示属性的测试，分支表示属性可能采取的可能值。
- en: 'Following are some of the characteristics of a Decision tree representation:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些决策树表示的特性：
- en: Every non-leaf node (for example, a decision node) denotes a representation
    of the attribute value
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个非叶节点（例如，决策节点）表示属性值的表示
- en: Every branch denotes the rest of the value representation
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分支表示值表示的其余部分
- en: Every leaf (or terminal) node represents the value of the target attribute
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个叶节点（或终端节点）代表目标属性的值
- en: The starting node is called the root node
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起始节点被称为根节点
- en: 'The following figure is a representation of the same:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表示了相同的内容：
- en: '![Terminology](img/B03980_05_02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![术语](img/B03980_05_02.jpg)'
- en: Purpose and uses
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目的和用途
- en: 'Decision trees are used for classification and regression. Two types of trees
    are used in this context:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树用于分类和回归。在此背景下使用两种类型的树：
- en: Classification trees
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树
- en: Regression trees
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归树
- en: Classification trees are used to classify the given data set into categories.
    To use classification trees, the response of the target variable needs to be a
    categorical value such as yes/no, true/false. On the other hand, regression trees
    are used to address prediction requirements and are always used when the target
    or response variable is a numeric or discrete value such as stock value, commodity
    price, and so on.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树用于将给定的数据集分类到类别中。要使用分类树，目标变量的响应需要是分类值，如是/否、真/假。另一方面，回归树用于解决预测需求，并且当目标或响应变量是数值或离散值（如股票价值、商品价格等）时总是使用。
- en: 'The next figure depicts the purpose of the Decision tree and relevant tree
    category as the classification or regression tree:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个图展示了决策树的目的以及相关的树类别，即分类树或回归树：
- en: '![Purpose and uses](img/B03980_05_03.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![目的和用途](img/B03980_05_03.jpg)'
- en: Constructing a Decision tree
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树
- en: Decision trees can be learned best by taking a simple example and constructing
    a Decision tree by hand. In this section, let's look at a simple example; the
    following table shows the dataset on hand. Our target is to predict whether a
    customer will accept a loan or not, given their demographics. Clearly, it will
    be most useful for the business user if we can come out with a rule as a model
    for this dataset.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个简单的例子和手动构建决策树，可以最好地学习决策树。在本节中，让我们看一个简单的例子；以下表格显示了手头的数据集。我们的目标是预测客户是否会接受贷款，给定他们的人口统计信息。显然，如果我们可以为这个数据集制定一个规则作为模型，这将最有用。
- en: '![Constructing a Decision tree](img/B03980_05_04.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![构建决策树](img/B03980_05_04.jpg)'
- en: From the previous table, since age and experience are highly correlated, we
    can choose to ignore one of the attributes. This aids the feature selection implicitly.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从前一个表中，由于年龄和经验高度相关，我们可以选择忽略其中一个属性。这隐式地帮助了特征选择。
- en: 'Case 1: Let''s start building the Decision tree. To start with, we will choose
    to split by CCAvg (the average credit card balance).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 情况 1：让我们开始构建决策树。首先，我们将选择根据 CCAvg（平均信用卡余额）进行分割。
- en: '![Constructing a Decision tree](img/B03980_05_05.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![构建决策树](img/B03980_05_05.jpg)'
- en: 'With this Decision tree, we now have two very explicit rules:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个决策树，我们现在有两个非常明确的规则：
- en: '*If CCAvg is medium then loan = accept* or *if CCAvg is high then loan = accept*'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果 CCAvg 中等则贷款 = 接受* 或 *如果 CCAvg 高则贷款 = 接受*'
- en: 'For more clarity in the rules, let''s add the income attribute. We have two
    more rules:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清晰地说明规则，让我们添加收入属性。我们还有两个额外的规则：
- en: '*If CCAvg is low and income is low, then loan is not accept*'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果 CCAvg 低且收入低，则贷款不接受*'
- en: '*If CCAvg is low and income is high, then loan is accept*'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果 CCAvg 低且收入高，则贷款接受*'
- en: 'By combining the second rule here and the first two rules, we can derive the
    following rule:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合这里的第二个规则和前两个规则，我们可以推导出以下规则：
- en: '*If (CCAvg is medium) or (CCAvg is high) or (CCAvg is low, and income is high)
    then loan = accept*'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*如果 (CCAvg 中等) 或 (CCAvg 高) 或 (CCAvg 低且收入高)，则贷款 = 接受*'
- en: 'Case 2: Let''s start building the Decision tree using Family:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 情况 2：让我们使用家庭作为开始构建决策树。
- en: '![Constructing a Decision tree](img/B03980_05_06.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![构建决策树](img/B03980_05_06.jpg)'
- en: In this case, there is just one rule that it is not giving an accurate result
    as it has only two data points.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，只有一个规则，因为它只有两个数据点，所以结果并不准确。
- en: 'So, choosing a valid attribute to start the tree makes a difference to the
    accuracy of the model. From the previous example, let''s list out some core rules
    for building Decision trees:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择一个有效的属性来开始构建树对模型的准确性有影响。从先前的例子中，让我们列出构建决策树的一些核心规则：
- en: We usually start building Decision trees with one attribute, split the data
    based on the attribute, and continue with the same process for other attributes.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通常从一个属性开始构建决策树，根据该属性分割数据，然后对其他属性继续同样的过程。
- en: There can be many Decision trees for the given problem.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于给定的问题，可能有多个决策树。
- en: The depth of the tree is directly proportional to the number of attributes chosen.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的深度与所选属性的数目成正比。
- en: There needs to be a Termination Criteria that will determine when to stop further
    building the tree. In the case of no termination criteria, the model will result
    in the over-fitting of the data.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要有一个终止标准来确定何时停止进一步构建树。在没有终止标准的情况下，模型可能会导致数据过拟合。
- en: Finally, the output is always in the form of simple rule(s) that can be stored
    and applied to different datasets for classification and/or prediction.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，输出总是以简单规则的形式呈现，这些规则可以存储并应用于不同的数据集进行分类和/或预测。
- en: One of the reasons why Decision trees are preferred in the field of Machine
    learning is because of their robustness to errors; they can be used when there
    are some unknown values in the training datasets too (for example, the data for
    income is not available for all the records).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在机器学习领域被优先选择的原因之一是它们对错误的鲁棒性；当训练数据集中存在一些未知值时，它们也可以被使用（例如，收入数据并非所有记录都有）。
- en: Handling missing values
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: One of the interesting ways of assigning values to some unknowns is to see that
    the most common value in terms of occurrence is assigned and in some cases they
    can belong to the same class, if possible we should bring it closer to accuracy.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为一些未知值分配值的一个有趣的方法是观察最常见的值被分配，在某些情况下，如果可能的话，它们可以属于同一类，如果可能的话，我们应该将其与准确性更接近。
- en: 'There is another probabilistic way of doing this where the prediction is distributed
    proportionately:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这还有另一种概率方法，预测是按比例分配的：
- en: Assign a probability *pi* for every value *vi* of *x*.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为 *x* 的每个值 *vi* 分配一个概率 *pi*。
- en: Now, assign the fraction *pi* of *x* to each of the descendants. These probabilities
    can be estimated again based on the observed frequencies of the various values
    for A, among the examples at node *n*.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将 *x* 的分数 *pi* 分配给每个后代。这些概率可以根据节点 *n* 中 A 的各种值的观察频率再次估计。
- en: For example, let's consider a Boolean attribute *A*. Let there be 10 values
    for *A* out of which three have a value of True and the rest 7 have a value of
    False. So, the probability of *A(x) = True* is 0.3, and the probability that *A(x)
    = False* is 0.7.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑一个布尔属性 *A*。假设 *A* 有 10 个值，其中三个值为 True，其余 7 个值为 False。因此，*A(x) = True*
    的概率是 0.3，而 *A(x) = False* 的概率是 0.7。
- en: A fractional 0.3 of this is distributed down the branch for *A = True*, and
    a fractional 0.7 is distributed down the other. These probability values are used
    for computing the information gain, and can be used if a second missing attribute
    value needs to be tested. The same methodology can be applied in the case of learning
    when we need to fill any unknowns for the new branches. The C4.5 algorithm uses
    this mechanism for filling the missing values.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 0.3 的分数沿着 *A = True* 的分支分布，0.7 的分数沿着另一个分支分布。这些概率值用于计算信息增益，如果需要测试第二个缺失的属性值，则可以使用这些值。同样的方法可以在学习时应用，当我们需要为新分支填充任何未知值时。C4.5
    算法使用这种机制来填充缺失值。
- en: Considerations for constructing Decision trees
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建决策树的考虑因素
- en: 'The key to constructing Decision trees is knowing where to split them. To do
    this, we need to be clear on the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 构建决策树的关键在于知道如何对它们进行分割。为此，我们需要明确以下内容：
- en: Which attribute to start and which attribute to apply subsequently?
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从哪个属性开始，以及随后的属性应用哪个？
- en: When do we stop building the Decision tree (that is avoid over-fitting)?
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们何时停止构建决策树（即避免过拟合）？
- en: Choosing the appropriate attribute(s)
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 选择合适的属性（或属性组合）。
- en: 'There are three different ways to identify the best-suited attributes:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同的方法来识别最适合的属性：
- en: Information Gain and Entropy
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信息增益和熵
- en: Gini index
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉尼指数
- en: Gain ratio
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收益率
- en: Information gain and Entropy
  id: totrans-69
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 信息增益和熵
- en: This entity is used in an algorithm known as C4.5\. Entropy is a measure of
    uncertainty in the data. Let us take an intuitive approach to understand the concepts
    of Information gain and Entropy.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实体在名为 C4.5 的算法中使用。熵是数据不确定性的度量。让我们用一个直观的方法来理解信息增益和熵的概念。
- en: 'For example, consider a coin is being tossed, and there are five coins with
    a probability for heads as 0, 0.25, 0.5, 0.75, and 1 respectively. So, if we think
    which one has the highest and which one has the lowest uncertainty, then the case
    of 0 or 1 will be the lowest certain one and highest would be when it is 0.5\.
    The following figure depicts the representation of the same:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个硬币正在被抛掷，有五个硬币，正面朝上的概率分别为 0, 0.25, 0.5, 0.75 和 1。所以，如果我们考虑哪个具有最高的不确定性和哪个具有最低的不确定性，那么
    0 或 1 的情况将是最低的确定性，而最高的是当它是 0.5 时。以下图展示了相同的表示：
- en: '![Information gain and Entropy](img/B03980_05_07.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_07.jpg)'
- en: 'A mathematical representation is shown here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了数学表示：
- en: H = -∑p[i]log2p[i]
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: H = -∑p[i]log2p[i]
- en: Here, p[i] is the probability of a specific state.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，p[i] 是特定状态的概率。
- en: 'If a system has four events with probabilities 1/2, 1/4, 1/5, and 1/8 indicate
    the total Entropy of the system as shown here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个系统有四个事件，概率分别为 1/2, 1/4, 1/5 和 1/8，则表示系统的总熵如下：
- en: H = -1/2 log2(1/2)-1/4log2(1/4)-1/5log2(1/5)-1/8log2(1/8)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: H = -1/2 log2(1/2)-1/4log2(1/4)-1/5log2(1/5)-1/8log2(1/8)
- en: In the original version of the C5.0 and C4.5 algorithms (ID3), a root node was
    chosen on the basis of how much of the total Entropy was reduced if this node
    was chosen. This is called information gain.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在 C5.0 和 C4.5 算法的原始版本（ID3）中，根节点是根据如果选择此节点将减少多少总熵来选择的。这被称为信息增益。
- en: Information gain = Entropy of the system before split - Entropy of the system
    after split
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益 = 分割前系统的熵 - 分割后系统的熵
- en: 'Entropy in the system before split is shown as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 分割前的系统熵如下：
- en: '![Information gain and Entropy](img/B03980_05_08.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_08.jpg)'
- en: 'Entropy after using *A* to split *D* into *v* partitions to classify *D*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 *A* 将 *D* 划分为 *v* 个分区以分类 *D* 后的熵：
- en: '![Information gain and Entropy](img/B03980_05_09.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_09.jpg)'
- en: 'Information gained by branching on an attribute is:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在属性上分支获得的信息增益：
- en: 'Let''s now compute the information gained from our data:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来计算从我们的数据中获得的信息增益：
- en: '![Information gain and Entropy](img/B03980_05_10.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_10.jpg)'
- en: Class P accepts the loan = yes/ 1\. Class N accepts the loan = no / 0
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 类 P 接受贷款 = 是/ 1. 类 N 接受贷款 = 否 / 0
- en: 'Entropy before split is as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 分割前的熵如下：
- en: '![Information gain and Entropy](img/B03980_05_11.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_11.jpg)'
- en: This is obvious and expected as we have almost a fifty-fifty split of the data.
    Let's now see which attribute gives the best information gain.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是很明显且预期的，因为我们几乎有一半的数据是平分的。现在让我们看看哪个属性能给出最佳的信息增益。
- en: In case the split is based on CCAvg and Family, the Entropy computations can
    be shown as follows. The total Entropy is weighted as the sum of the Entropies
    of each of the nodes that were created.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分割是基于 CCAvg 和 Family，则熵的计算可以表示如下。总熵是作为创建的每个节点的熵之和的加权总和。
- en: '![Information gain and Entropy](img/B03980_05_12.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_12.jpg)'
- en: 'The Entropy after its split is shown here:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 其分割后的熵如下：
- en: '![Information gain and Entropy](img/B03980_05_13.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_13.jpg)'
- en: 'The information gain is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益如下：
- en: '![Information gain and Entropy](img/B03980_05_14.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![信息增益与熵](img/B03980_05_14.jpg)'
- en: This methodology is applied to compute the information gain for all other attributes.
    It chooses the one with the highest information gain. This is tested at each node
    to select the best node.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被应用于计算所有其他属性的信息增益。它选择信息增益最高的一个。这将在每个节点上进行测试以选择最佳节点。
- en: Gini index
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Gini 指数
- en: 'Gini index is a general splitting criterion. It is named after an Italian statistician
    and economist—Corrado Gini. Gini Index is used to measure the probability of two
    random items belonging to the same class. In the case of a real dataset, this
    probability value is 1\. The Gini measure of a node is the sum of the squares
    of the proportions of the classes. A node with two classes each has a score of
    *0.52 + 0.52 = 0.5*. This is because the probability of picking the same class
    at random is 1 out of 2\. Now, if we apply Gini index for the data set we get
    the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Gini 指数是一个通用的分割标准。它以意大利统计学家和经济学家 Corrado Gini 命名。Gini 指数用于衡量两个随机项目属于同一类的概率。在真实数据集的情况下，这个概率值是
    1。一个节点的 Gini 测量值是类比例的平方和。具有两个类的节点得分为 *0.52 + 0.52 = 0.5*。这是因为随机选择相同类的概率是 2 中的
    1。现在，如果我们对数据集应用 Gini 指数，我们得到以下结果：
- en: The original Gini Index = ![Gini index](img/B03980_05_15.jpg) = 0.502959
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 原始基尼系数 = ![基尼系数](img/B03980_05_15.jpg) = 0.502959
- en: 'When split with CCAvg and Family, the Gini Index changes to the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当与CCAvg和Family分割时，基尼系数变为以下：
- en: '![Gini index](img/B03980_05_16.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![基尼系数](img/B03980_05_16.jpg)'
- en: Gain ratio
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增益率
- en: Another improvement in C4.5 compared to ID3 is that the factor that decides
    the attribute is the gain ratio. The gain ratio is the ratio of information gain
    and information content. The attribute that gives the maximum amount of gain ratio
    is the attribute that is used to split it.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 与ID3相比，C4.5的另一个改进是决定属性的因子是增益率。增益率是信息增益和信息含量的比率。提供最大增益率的属性是用于分割的属性。
- en: 'Let''s do some calculations with an extremely simple example to highlight why
    the gain ratio is a better attribute than the information gain:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个极其简单的例子来做一些计算，以突出为什么增益率比信息增益是一个更好的属性：
- en: '![Gain ratio](img/B03980_05_17.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![增益率](img/B03980_05_17.jpg)'
- en: The dependent variable is whether they are married under a specific circumstance.
    Let's assume that in this case, no man is married. Whereas all women, except the
    last one (60 women), are married.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因变量是他们在特定情况下是否已婚。让我们假设在这种情况下，没有男人已婚。而所有女人（除了最后一位60位女性）都是已婚的。
- en: 'So, intuitively the rule has to be as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，直观上规则必须如下：
- en: If it is a man, then he is unmarried
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是男性，那么他是未婚的
- en: If it is a woman then she is married (the only isolated case where she is not
    married must be noise).
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果是女性，那么她是已婚的（唯一一个她未婚的孤立案例必须是噪声）。
- en: Let's systematically solve this problem to gain insights into various parameters.
    First let's split the data into two halves as training and testing data. So, our
    training set consists of the last 20 males (all insensible and aged between 21-40),
    and the last 30 females (all married and aged between 71-99, except the last one).
    Testing contains the other half where all the women are married.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们系统地解决这个问题，以深入了解各种参数。首先，让我们将数据分成两半作为训练数据和测试数据。因此，我们的训练集包括最后20个男性（所有不可感知且年龄在21-40岁之间），以及最后30个女性（所有已婚且年龄在71-99岁之间，除了最后一位）。测试数据包含另一半，其中所有女性都是已婚的。
- en: The gain ratio requires measure for **Information content**.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 增益率需要衡量**信息含量**。
- en: Information content is defined as *-f[i] log[2] f[i]*. Note that here, we do
    not take the value of the dependent variable into account. We only want to know
    the fraction of the members in a state divided by the total members.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 信息含量定义为 *-f[i] log[2] f[i]*。请注意，在这里，我们不考虑因变量的值。我们只想知道一个状态中成员的分数除以总成员数。
- en: The information content of gender is that it has only two states; males are
    20 and females are 30\. So, the information content for the gender is *2/5*LOG(2/5,2)-3/5*LOG(3/5,2)=0.9709*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 性别的信息含量是指它只有两种状态；男性是20，女性是30。因此，性别信息含量为 *2/5*LOG(2/5,2)-3/5*LOG(3/5,2)=0.9709*。
- en: The information content of age is that there is a total of 49 states for the
    age. For the states that have only one data point, the information content is
    *-(1/50)*log(1/50,2) = 0.1129*.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄的信息含量是指年龄共有49个状态。对于只有一个数据点的状态，信息含量是 *-(1/50)*log(1/50,2) = 0.1129*。
- en: There are 48 such states with a single data point. So, their information content
    is (0.1129*48), 5.4192\. In the last state, there are two data points. So, its
    information content is *-(2/50 * LOG(2/50,2)) = 0.1857*. The total information
    content for the age is 5.6039.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有48个这样的状态只有一个数据点。因此，它们的信息含量是 (0.1129*48), 5.4192。在最后一个状态中，有两个数据点。因此，它的信息含量是
    *-(2/50 * LOG(2/50,2)) = 0.1857*。年龄的总信息含量是5.6039。
- en: The gain ratio for the gender = Information gain for gender / Information content
    for gender = 0.8549/0.9709 = 0.8805.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 性别的增益率 = 性别的信息增益 / 性别的信息含量 = 0.8549/0.9709 = 0.8805。
- en: The gain ratio for the age = 0.1680
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 年龄的增益率 = 0.1680
- en: So, if we consider the gain ratio, we get that the gender is a more suitable
    measure. This aligns with the intuition. Let's now say that we used the gain ratio
    and built the tree. Our rule is if the gender is male, the person is unmarried
    and if it is female, the person is married.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，如果我们考虑增益率，我们会得到性别是一个更合适的度量。这与直觉相符。现在让我们假设我们使用了增益率并构建了树。我们的规则是如果性别是男性，那么这个人未婚，如果性别是女性，那么这个人已婚。
- en: Termination Criteria / Pruning Decision trees
  id: totrans-120
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 终止标准/剪枝决策树
- en: Each branch is grown deeply enough to classify the training examples perfectly
    by the Decision tree algorithm. This can turn out to be an acceptable approach
    and most of the times results in problems when there is some noise in the data.
    In case the training dataset is too small and cannot represent the true picture
    of the actual data set the Decision tree might end up over-fitting the training
    examples.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分支都足够深入地生长，以便通过决策树算法完美地分类训练示例。这可能会成为一种可接受的方法，但在数据中存在噪声时，通常会导致问题。如果训练数据集太小，无法代表实际数据集的真实情况，决策树可能会最终过拟合训练示例。
- en: 'There are many ways of avoiding over-fitting in Decision tree learning. Following
    are the two different cases:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树学习中避免过拟合有许多方法。以下是两种不同的情况：
- en: One case where the Decision tree is terminated for growth way before a perfect
    classification of the training data is done
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种情况是决策树在完成训练数据的完美分类之前就终止了生长
- en: Another case where the over-fitting of data is done and then the tree is pruned
    to recover
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一种情况是数据过拟合后，然后剪枝以恢复
- en: Though the first case might seem to be more direct, the second case of post-pruning
    the over-fitting trees is more successful in reality. The reason is the difficulty
    to know when to stop growing the tree. Irrespective of the approach taken, it
    is more important to identify the criterion to determine the final, appropriate
    tree size.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然第一种情况看起来可能更直接，但第二种情况，即对过拟合树进行后剪枝，在现实中更成功。原因是难以知道何时停止树的生长。无论采取何种方法，确定最终、适当的树大小的标准更为重要。
- en: 'Following are a couple of approaches to find the correct tree size:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些寻找正确树大小的方法：
- en: Identify a separate and different dataset to that of the target training data
    set to be used, and evaluate the correctness of post-pruning nodes in the tree.
    This is a common approach and is called training and validation set approach.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别一个与目标训练数据集不同的独立数据集，并评估树中后剪枝节点的正确性。这是一个常见的方法，被称为训练和验证集方法。
- en: Instead of having a subset of data in the training set, use up all the data
    in the training set, and apply probabilistic methods to check if pruning a particular
    node has any likelihood to produce any improvement over and above the training
    dataset. Use all the available data for training. For example, the chi-square
    test can be used to check this probability.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集中不使用数据子集，而是使用训练集中的所有数据，并应用概率方法来检查剪枝特定节点是否有任何可能产生超过训练数据集的改进。使用所有可用数据进行训练。例如，可以使用卡方检验来检查这个概率。
- en: 'Reduced-Error-Pruning (D): We prune at a node by removing the subtree that
    is rooted at the node. We make that node a leaf (with the majority label of associated
    examples); algorithm is shown as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 减少错误剪枝（D）：通过移除以节点为根的子树来剪枝。我们将该节点变为叶节点（具有相关示例的大多数标签）；算法如下所示：
- en: '![Termination Criteria / Pruning Decision trees](img/B03980_05_18.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![终止标准/剪枝决策树](img/B03980_05_18.jpg)'
- en: Rule post-pruning is a more commonly used method and is a highly accurate hypotheses
    technique. A variation of this pruning method is used in C4.5.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 规则后剪枝是一种更常用的方法，是一种高度准确的理论技术。C4.5中使用的是这种剪枝方法的变体。
- en: 'Following are the steps of the Rule Post-Pruning process:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是规则后剪枝过程的步骤：
- en: Construct a Decision tree from the training set by growing it until there is
    an obvious over-fitting seen.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过生长直到出现明显的过拟合来从训练集中构建决策树。
- en: Generate rules from the constructed Decision tree with every path, starting
    from the root node to a particular leaf node mapping to a rule.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从根节点到特定叶节点的每一条路径生成决策树中的规则，映射到一个规则。
- en: Apply pruning to each rule for removing identified preconditions and help improve
    the probabilistic accuracy.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个规则应用剪枝，以去除已识别的先决条件，并有助于提高概率准确性。
- en: Next, use the pruned rules in the order of their increased accuracy on the subsequent
    instances.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，按照它们在后续实例中增加的准确度顺序使用剪枝规则。
- en: 'Following are the advantages of rule-based pruning and its need for converting
    into rules:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是基于规则的剪枝的优点及其转换为规则的需求：
- en: Improving the readability of the rules
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高规则的易读性
- en: A consistent testing can be done at both the root and leaf level nodes
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以在根节点和叶节点级别进行一致的测试。
- en: There is a clear decision that can be made of either removing the decision node
    or retaining it
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以清楚地决定是移除决策节点还是保留它
- en: Decision trees in a graphical representation
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树图形表示
- en: Until now, we have seen how Decision trees are described by dividing the data
    at the node and comparing the value with a constant. Another way of representing
    Decision trees is to visualize and have graphical representation. For example,
    we can choose two input attributes in two dimensions, then compare the value of
    one attribute with constant and show the split on the data to a parallel axis.
    We can also compare two attributes with one another along with a linear combination
    of attributes, instead of a hyperplane that is not parallel to an axis.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到决策树是如何通过在节点处划分数据并与常数比较来描述的。另一种表示决策树的方法是可视化和图形表示。例如，我们可以在两个维度中选择两个输入属性，然后比较一个属性的值与常数，并在平行轴上显示数据分割。我们还可以比较两个属性，包括属性的线性组合，而不是与轴不平行的超平面。
- en: '![Decision trees in a graphical representation](img/B03980_05_19.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![决策树图形表示](img/B03980_05_19.jpg)'
- en: 'Constructing multiple Decision trees for the given data is possible. The process
    of identifying the smallest and a perfect tree is called a minimum consistent
    hypothesis. Let''s use two arguments to see why this is the best Decision tree:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的数据，可以构建多个决策树。识别最小和完美树的过程称为最小一致假设。让我们用两个论点来看为什么这是最好的决策树：
- en: Occam's Razor is simple; when there are two ways to solve a problem and both
    give the same result, the simplest of them prevails.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 奥卡姆剃刀原理很简单；当有两种方法可以解决问题并且两者都给出相同的结果时，最简单的那一个占上风。
- en: In data mining analysis, one is likely to fall into the trap of complex methods
    and large computations. So, it is essential to internalize the line of reasoning
    adopted by Occam. Always choose a Decision tree that has an optimum combination
    of size and errors.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据挖掘分析中，人们可能会陷入复杂方法和大量计算陷阱。因此，内化奥卡姆的推理路线至关重要。始终选择一个大小和误差最佳组合的决策树。
- en: Inducing Decision trees – Decision tree algorithms
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 诱导决策树 – 决策树算法
- en: There are many Decision tree inducing methods. Among all the methods, C4.5 and
    CART are the most adopted or popular ones. In this section, we will cover these
    methods in depth and list a brief on other methods.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多诱导决策树的方法。在所有方法中，C4.5和CART是最被采用或最受欢迎的。在本节中，我们将深入探讨这些方法，并简要介绍其他方法。
- en: CART
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: CART
- en: CART stands for Classification and Regression Trees (Breiman et al., 1984).
    CART creates binary trees. This means there are always two branches that can emerge
    from a given node. The philosophy of the CART algorithm is to follow a *goodness*
    criterion, which is all about choosing the best possible partition. Moreover,
    as the tree grows, a cost-complexity pruning mechanism is adopted. CART uses the
    Gini index to select appropriate attributes or the splitting criteria.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: CART代表分类和回归树（Breiman等，1984）。CART创建二叉树。这意味着从给定的节点总是有两个分支可以产生。CART算法的哲学是遵循一个*良好性*标准，这关乎选择最佳可能的分区。此外，随着树的成长，采用成本复杂度剪枝机制。CART使用基尼指数来选择适当的属性或分割标准。
- en: Using CART, the prior probability distribution can be provided. We can generate
    Regression trees using CART that in turn help in predicting real numbers against
    a class. The prediction is done by applying the weighted mean for the node. CART
    identifies splits that minimize the prediction squared error (that is, the least-squared
    deviation).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CART，可以提供先验概率分布。我们可以使用CART生成回归树，这反过来又有助于预测一个类别对实数的预测。预测是通过应用节点的加权平均值来完成的。CART识别出最小化预测平方误差的分割（即最小二乘偏差）。
- en: 'The depiction in the following figure of CART is for the same example referred
    in the previous section, where Decision tree construction is demonstrated:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 下图中CART的描述是针对前一小节中提到的相同示例，其中展示了决策树构建过程：
- en: '![CART](img/B03980_05_20.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![CART](img/B03980_05_20.jpg)'
- en: C4.5
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: C4.5
- en: Similar to CART, C4.5 is a Decision tree algorithm with a primary difference
    that it can generate more than binary trees, which means support for multiway
    splits. For attribute selection, C4.5 uses the information gain measure. As explained
    in the previous section, an attribute with the largest information gain (or the
    lowest Entropy reduction) value helps to achieve closer to accurate classification
    with the least quantity of data. One of the key drawbacks of C4.5 is the need
    for large memory and CPU capacity for generating rules. The C5.0 algorithm is
    a commercial version of C4.5 that was presented in 1997.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 与CART类似，C4.5是一种决策树算法，其主要区别在于它可以生成超过二叉树，这意味着支持多路分割。在属性选择上，C4.5使用信息增益度量。正如前一部分所解释的，具有最大信息增益（或最低熵减少）值的属性有助于以最少的数量数据实现更接近准确的分类。C4.5的一个主要缺点是需要大量的内存和CPU容量来生成规则。C5.0算法是C4.5的商业版本，于1997年推出。
- en: C4.5 is an evolution of the ID3 algorithm. The gain ratio measure is used for
    identifying the splitting criteria. The splitting process stops when the number
    of splits reaches a boundary condition definition that acts as a threshold. Post
    this growing phase of the tree, pruning is done, and an error-based pruning method
    is followed.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: C4.5是ID3算法的演变。使用增益率度量来识别分割标准。分割过程在分割数量达到边界条件定义的阈值时停止。在树的这个生长阶段之后，进行剪枝，并遵循基于错误的剪枝方法。
- en: 'Here is a representation of the C4.5 way of constructing the Decision tree
    for the same example used in the previous section:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是C4.5构建决策树的表示，用于与上一节中使用的相同示例：
- en: '![C4.5](img/B03980_05_21.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![C4.5](img/B03980_05_21.jpg)'
- en: '| Tree Induction method | How does it work? |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 树形归纳法 | 它是如何工作的？ |'
- en: '| --- | --- |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ID3 | The ID3 (**Iterative Dichotomiser 3**) algorithm is considered the
    simplest among the Decision tree algorithms. The information gain method is used
    as splitting criteria; the splitting is done until the best information gain is
    not greater than zero. There is no specific pruning done with ID3\. It cannot
    handle numeric attributes and missing values. |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ID3 | ID3（**迭代二分器3**）算法被认为是决策树算法中最简单的一种。它使用信息增益方法作为分割标准；分割会一直进行，直到最佳信息增益不再大于零。ID3没有进行特定的剪枝操作。它无法处理数值属性和缺失值。
    |'
- en: '| CHAID | **CHAID** (**Chi-squared Automatic Interaction Detection**) was built
    to support only nominal attributes. For every attribute, a value is chosen in
    such a way that it is the closest to the target attribute. There is an additional
    statistical measure, depending on the type of the target attribute that differentiates
    this algorithm.F test for a continuous target attribute, Pearson chi-squared test
    for nominal target attribute, and likelihood–ratio test for an ordinal target
    attribute is used. CHAID checks a condition to merge that can have a threshold
    and moves for a next check for merging. This process is repeated until no matching
    pairs are found.CHAID addresses missing values in a simple way, and it operates
    on the assumption that all values belong to a single valid category. No pruning
    is done in this process. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| CHAID | **CHAID**（**卡方自动交互检测**）是为了仅支持名义属性而构建的。对于每个属性，选择一个值，使其与目标属性最接近。根据目标属性的类型，有一个额外的统计度量，区分了该算法。对于连续目标属性使用F检验，对于名义目标属性使用皮尔逊卡方检验，对于有序目标属性使用似然比检验。CHAID检查一个条件以合并，这可能有一个阈值，并移动到下一个合并检查。这个过程会重复进行，直到找不到匹配的对。CHAID以简单的方式处理缺失值，并假设所有值都属于一个单一的合法类别。在这个过程中没有进行剪枝操作。
    |'
- en: '| QUEST | The acronym QUEST stands for Quick, Unbiased, Efficient, and Statistical
    Tree.This algorithm supports univariate and linear combination splits. ANOVA F-test
    or Pearson''s chi-square or two-means clustering methods are used to compute the
    relationship between each input attribute and the target attribute, depending
    on the type of the attribute. Splitting is applied on attributes that have stronger
    association with the target attribute. To ensure that there is an optimal splitting
    point achieved, **Quadratic Discriminant Analysis** (**QDA**) is applied. Again,
    QUEST achieves binary trees and for pruning 10-fold cross-validation is used.
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| QUEST | QUEST 的缩写代表快速、无偏、高效和统计树。此算法支持单变量和线性组合分割。根据属性类型，使用 ANOVA F 检验或皮尔逊卡方检验或双均值聚类方法来计算每个输入属性与目标属性之间的关系。分割应用于与目标属性关联更强的属性。为了确保达到最优分割点，应用**二次判别分析**（**QDA**）。再次，QUEST
    实现了二叉树，并且为了剪枝使用了10折交叉验证。|'
- en: '| CAL5 | This works with numerical attributes. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| CAL5 | 这与数值属性一起工作。|'
- en: '| FACT | This algorithm is an earlier version of QUEST that uses statistical
    methods followed by discriminant analysis for attribute selection. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| FACT | 此算法是 QUEST 的早期版本，使用统计方法，随后进行判别分析以进行属性选择。|'
- en: '| LMDT | This uses a multivariate testing mechanism to build Decision trees.
    |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| LMDT | 这使用多元测试机制来构建决策树。|'
- en: '| MARS | A multiple regression function is approximated using linear splines
    and their tensor products. |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| MARS | 使用线性样条及其张量积来近似多个回归函数。|'
- en: Greedy Decision trees
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贪婪决策树
- en: A vital characteristic of Decision trees is that they are *Greedy!* A greedy
    algorithm targets achieving optimal solutions globally by achieving local optimums
    at every stage. Though the global optimum is not always guaranteed, the local
    optimums help in achieving global optimum to a maximum extent.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个关键特征是它们是**贪婪的**！贪婪算法通过在每个阶段实现局部最优来全局地实现最优解。虽然全局最优并不总是有保证，但局部最优有助于最大限度地实现全局最优。
- en: Every node is greedily searched to reach the local optimum, and the possibility
    of getting stuck at achieving local optima is high. Most of the time, targeting
    local optima might help in providing a good enough solution.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 每个节点都会贪婪地搜索以达到局部最优，并且陷入局部最优的可能性很高。大多数时候，针对局部最优可能有助于提供足够好的解决方案。
- en: Benefits of Decision trees
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决策树的好处
- en: 'Some of the advantages of using Decision trees are listed here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树的一些优点如下：
- en: Decision trees are fast and easy to build and require little experimentation
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树构建快速且简单，且需要很少的实验
- en: They are robust
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是健壮的
- en: They are easy to understand and interpret
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于理解和解释
- en: Decision trees do not require complex data preparation
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树不需要复杂的数据准备
- en: They can handle both categorical and numerical data
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理分类数据和数值数据
- en: They are supported using statistical models for validation
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们通过使用统计模型进行验证来得到支持
- en: They can handle highly dimensional data and also operate large datasets
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理高维数据，并且可以操作大型数据集
- en: Specialized trees
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 专用树
- en: In this section, we will explore some important special situations we face and
    special types of Decision trees. These become handy while solving special kinds
    of problems.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些重要的特殊情况以及特殊的决策树类型。在解决特殊类型的问题时，这些情况非常有用。
- en: Oblique trees
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 倾斜树
- en: 'Oblique trees are used in cases where the data is extremely complex. If the
    attributes are *x1, x2, AND x3…xn*, then the C4.5 and CART tests the criteria
    as *x1>some value* or *x2< some other value*, and so on. The goal in such cases
    is to find an attribute to test at each node. These are graphically parallel axis
    splits as shown in the following figure:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据极其复杂的情况下使用倾斜树。如果属性是 *x1, x2, AND x3…xn*，那么 C4.5 和 CART 测试标准为 *x1>某个值* 或 *x2<另一个值*，依此类推。在这种情况下，目标是找到每个节点要测试的属性。这些是如图所示图形上的平行轴分割：
- en: '![Oblique trees](img/B03980_05_22.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![倾斜树](img/B03980_05_22.jpg)'
- en: Clearly, we need to construct enormous trees. At this point, let's learn a data
    mining jargon called hyperplanes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们需要构建巨大的树。在这个时候，让我们学习一个称为超平面的数据挖掘术语。
- en: 'In a **1 D** problem, a point classifies the space. In **2 D**, a line (straight
    or curved) classifies the space. In a **3 D** problem, a plane (linear or curved)
    classifies the space. In higher dimensional space, we imagine a plane like a thing
    splitting and classifying the space, calling it **hyperplane**. This is shown
    in the following figure:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在**一维**问题中，一个点对空间进行分类。在**二维**中，一条直线（直线或曲线）对空间进行分类。在**三维**问题中，一个平面（线性或曲线）对空间进行分类。在更高维的空间中，我们想象一个平面像东西一样分裂并对空间进行分类，称之为**超平面**。以下图表展示了这一点：
- en: '![Oblique trees](img/B03980_05_23.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![倾斜树木](img/B03980_05_23.jpg)'
- en: 'So, the traditional Decision tree algorithms produce axis parallel hyperplanes
    that split the data. These can be cumbersome if the data is complex. If we can
    construct oblique planes, the explicability may come down, but we might reduce
    the tree size substantially. So, the idea is to change the testing conditions
    from the following:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，传统的决策树算法产生的是与轴平行的超平面来分割数据。如果数据复杂，这可能会变得很繁琐。如果我们能构建斜面，虽然可解释性可能会降低，但我们可能会大幅减少树的大小。所以，我们的想法是将测试条件从以下内容改变：
- en: xi > K or < K to a1x1+ a2x2+ … + c > K or < K
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: xi > K 或 < K 到 a1x1+ a2x2+ … + c > K 或 < K
- en: 'These oblique hyperplanes can at times drastically reduce the length of the
    tree. The same data shown in figure 2 is classified using oblique planes in the
    figure here:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这些斜面超平面有时可以极大地减少树的大小。图2中所示的数据在这里使用斜面进行分类：
- en: '![Oblique trees](img/B03980_05_24.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![倾斜树木](img/B03980_05_24.jpg)'
- en: Random forests
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林
- en: These specialized trees are used when there are too many dimensions. We have
    learned about curse of dimensionality in the Machine learning introduction chapter.
    The basic premise of the curse of dimensionality is that high dimensional data
    brings in complexity. With more dimensions and features, the possibility of errors
    is also high. Before we take a deep dive into Random forests, let's understand
    the concept of Boosting. More details on boosting methods are covered as a part
    of [Chapter 13](ch13.html "Chapter 13. Ensemble learning"), *Ensemble learning*.
    In the case of Random forests, the application of boosting is about how single
    tree methods are brought together to see a boost in the result regarding accuracy.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 当维度过多时，我们会使用这些专门的树木。我们在机器学习简介章节中学习了维度的诅咒。维度诅咒的基本前提是高维数据带来了复杂性。随着维度和特征的增多，错误的可能性也相应提高。在我们深入探讨随机森林之前，让我们先了解提升法的概念。关于提升法的更多细节可以在[第13章](ch13.html
    "第13章. 集成学习") *集成学习*中找到。在随机森林的情况下，提升法的应用主要涉及如何将单个决策树方法结合起来，以提升结果的准确性。
- en: '![Random forests](img/B03980_05_25.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B03980_05_25.jpg)'
- en: 'A Random forest extends Decision trees by including more number of Decision
    trees. These Decision trees are built by a combination of random selection of
    data (samples) and a random selection of a subset of attributes. The following
    diagram depicts the random selection of datasets to build each of the Decision
    trees:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林通过包含更多的决策树来扩展决策树。这些决策树是通过随机选择数据（样本）和随机选择属性子集的组合来构建的。以下图表展示了构建每个决策树时数据集的随机选择：
- en: '![Random forests](img/B03980_05_26.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B03980_05_26.jpg)'
- en: 'Another variable input required for the making of multiple Decision trees are
    random subsets of the attributes, which is represented in the diagram here:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 制作多个决策树所需的另一个变量输入是属性的随机子集，这在以下图表中有所表示：
- en: '![Random forests](img/B03980_05_27.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B03980_05_27.jpg)'
- en: Since each tree is built using random dataset and random variable set, these
    trees are called Random trees. Moreover, many such Random trees define a Random
    forest.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每棵树都是使用随机数据集和随机变量集构建的，因此这些树被称为随机树。此外，许多这样的随机树定义了一个随机森林。
- en: The result of a Random tree is based on two radical beliefs. One is that each
    of the trees make an accurate prediction for maximum part of the data. Second,
    mistakes are encountered at different places. So, on an average, a poll of results
    is taken across the Decision trees to conclude a result.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 随机树的结果基于两个根本的信念。一是每棵树都能对大部分数据做出准确的预测。二是错误会在不同的地方发生。因此，通常会对决策树的结果进行平均，以得出结论。
- en: There are not enough observations to get good estimates, which leads to sparsity
    issues. There are two important causes for exponential increase on spatial density,
    one, is increase in dimensionality and the other is increase in the equidistant
    points in data. Most of the data is in the tails.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据不足，无法得到良好的估计，这导致了稀疏性问题。空间密度指数的指数增长有两个重要原因，一是维度增加，二是数据中等距点的增加。大部分数据都在尾部。
- en: 'To estimate the density of a given accuracy, the following table shows how
    the sample size increases with dimensionality. The subsequent computations table
    shows how the mean square error of an estimate of multivariate normal distribution
    increases with an increase in dimensionality (as demonstrated by Silverman and
    computed by the formula given here):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 为了估计给定精度的密度，以下表格显示了样本大小如何随着维度的增加而增加。随后的计算表格显示了多元正态分布估计的均方误差如何随着维度的增加而增加（如Silverman所展示，并使用此处给出的公式计算）：
- en: '![Random forests](img/B03980_05_31.jpg)![Random forests](img/B03980_05_28.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![随机森林](img/B03980_05_31.jpg)![随机森林](img/B03980_05_28.jpg)'
- en: Random forests are a vital extension of the Decision trees that are very simple
    to understand and are extremely efficient, particularly when one is dealing with
    high dimensional spaces. When the original data has many dimensions, we randomly
    pick a small subset of the dimensions (columns) and construct a tree. We let it
    grow all the way without pruning. Now, we iterate this process and construct hundreds
    of trees with a different set of attributes each time.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是决策树的一个重要扩展，非常容易理解，效率极高，尤其是在处理高维空间时。当原始数据有许多维度时，我们随机选择维度（列）的小子集并构建一棵树。我们让它无修剪地生长。现在，我们迭代这个过程，每次构建具有不同属性集的数百棵树。
- en: For prediction, a new sample is pushed down the tree. A new label of the training
    sample is assigned to the terminal node, where it ends up. This procedure is iterated
    over all the trees in the group, and the average vote of all trees is reported
    as the Random forest prediction.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测，一个新的样本被推入树中。训练样本的新标签被分配给终端节点，它最终结束的地方。这个程序在组中的所有树上迭代，所有树的平均投票结果被报告为随机森林预测。
- en: Evolutionary trees
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进化树
- en: When achieving the global optima seems almost impossible, Evolutionary trees
    are used. As you learned, Decision trees are greedy. So sometimes, we may be constructing
    much bigger trees just because we are stuck in local optima. So, if your tree
    length is just too much, try oblique trees or evolutionary trees.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当达到全局最优解似乎几乎不可能时，会使用进化树。正如你所学的，决策树是贪婪的。因此，有时我们可能只是因为陷入了局部最优解，而构建出更大的树。所以，如果你的树长度实在太大，可以尝试斜树或进化树。
- en: The concept of evolutionary trees is originated from a very exciting concept
    called genetic algorithms. You will learn about it in detail in a different course.
    Let us only look at the essence.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 进化树的概念起源于一个非常激动人心的概念，即遗传算法。你将在另一门课程中详细了解它。让我们只看看其本质。
- en: Instead of mathematically computing the best attribute at every node, an Evolutionary
    tree randomly picks a node at each point and creates a tree. It then iterates
    and creates a collection of trees (forest). Now, it identifies the best trees
    in the forest for the data. It then creates the next generation of the forest
    by combining these trees randomly.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 与在每一个节点数学上计算最佳属性不同，进化树在每个点随机选择一个节点并创建一棵树。然后它迭代并创建一系列树（森林）。现在，它识别森林中最佳树木用于数据。然后通过随机组合这些树来创建森林的下一代。
- en: Evolutionary trees, on the other hand, choose a radically different top node
    and produce a much shorter tree, which has the same efficiency. Evolutionary algorithms
    take more time to compute.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，进化树选择一个截然不同的顶级节点，并生成一个更短的树，具有相同的效率。进化算法的计算时间更长。
- en: Hellinger trees
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 希尔林格树
- en: There have been attempts to identify impurity measures that are less sensitive
    to the distribution of dependent variable values than Entropy or Gini index. A
    very recent paper suggested Hellinger distance as a measure of impurity that does
    not depend on the distribution of the target variable.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 已有尝试识别比熵或基尼指数对依赖变量值分布更不敏感的纯度度量。一篇最新的论文建议使用希尔林格距离作为不依赖于目标变量分布的纯度度量。
- en: '![Hellinger trees](img/B03980_05_29.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![希尔林格树](img/B03980_05_29.jpg)'
- en: Essentially, *P(Y+|X)* is the probability of finding *Y+* for each attribute
    and similarly, *P(Y-|X)* for each attribute is computed.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 实质上，*P(Y+|X)* 是对于每个属性找到 *Y+* 的概率，同样地，对于每个属性的 *P(Y-|X)* 也是计算出来的。
- en: '![Hellinger trees](img/B03980_05_30.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![Hellinger trees](img/B03980_05_30.jpg)'
- en: From the previous image, for a **High** value of the first attribute, only a
    **High** value of the second attribute results in a probability value of 1\. This
    brings the total distance value to *sqrt(2)*.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 从上一张图中可以看出，对于第一个属性的**高**值，只有第二个属性的**高**值才能得到概率值为1。这使得总距离值为 *sqrt(2)*。
- en: Implementing Decision trees
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现决策树
- en: Refer to the source code provided for this chapter for implementing Decision
    Trees and Random Forests (source code path `.../chapter5/...` under each of the
    folder for the technology).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考本章提供的源代码以实现决策树和随机森林（源代码路径 `.../chapter5/...` 在每个文件夹下的技术中）。
- en: Using Mahout
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Mahout
- en: Refer to the folder `.../mahout/chapter5/decisiontreeexample/`.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../mahout/chapter5/decisiontreeexample/`。
- en: Refer to the folder`.../mahout/chapter5/randomforestexample/`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../mahout/chapter5/randomforestexample/`。
- en: Using R
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 R
- en: Refer to the folder `.../r/chapter5/decisiontreeexample/`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../r/chapter5/decisiontreeexample/`。
- en: Refer to the folder `.../r/chapter5/randomforestexample/`.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../r/chapter5/randomforestexample/`。
- en: Using Spark
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Spark
- en: Refer to the folder `.../spark/chapter5/decisiontreeexample/`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../spark/chapter5/decisiontreeexample/`。
- en: Refer to the folder `.../spark/chapter5/randomforestexample/`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../spark/chapter5/randomforestexample/`。
- en: Using Python (scikit-learn)
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Python (scikit-learn)
- en: Refer to the folder `.../python scikit-learn/chapter5/decisiontreeexample/`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../python scikit-learn/chapter5/decisiontreeexample/`。
- en: Refer to the folder `.../python scikit-learn/chapter5/randomforestexample/`.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../python scikit-learn/chapter5/randomforestexample/`。
- en: Using Julia
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Julia
- en: Refer to the folder `.../julia/chapter5/decisiontreeexample/`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../julia/chapter5/decisiontreeexample/`。
- en: Refer to the folder `.../julia/chapter5/randomforestexample/`.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考文件夹 `.../julia/chapter5/randomforestexample/`。
- en: Summary
  id: totrans-234
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned a supervised learning technique with Decision trees
    to solve classification and regression problems. We also covered methods to select
    attributes, split the tree, and prune the tree. Among all other Decision tree
    algorithms, we have explored the CART and C4.5 algorithms. For a special requirement
    or a problem, you have also learned how to implement Decision tree-based models
    using MLib of Spark, R, and Julia. In the next chapter, we will cover **Nearest
    Neighbour** and **SVM** (**Support Vector Machines**) to solve supervised and
    unsupervised learning problems.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了使用决策树进行监督学习的技术，以解决分类和回归问题。我们还介绍了选择属性、分割树和剪枝树的方法。在所有其他决策树算法中，我们还探讨了CART和C4.5算法。对于特殊需求或问题，你还学习了如何使用Spark、R和Julia的MLib实现基于决策树的模型。在下一章中，我们将介绍**最近邻**和**支持向量机（SVM**）来解决监督学习和无监督学习问题。
