- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Understanding AutoML Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解AutoML算法
- en: All ML algorithms have a foundation in **computational statistics**. Computational
    statistics is the combination of statistics and computer science where computers
    are used to compute complex mathematics. This computation is the ML algorithm
    and the results that we get from it are the predictions. As engineers and scientists
    working in the field of ML, we are often expected to know the basic logic of ML
    algorithms. There are plenty of ML algorithms in the AI domain. All of them aim
    to solve different types of prediction problems. All of them also have their own
    set of pros and cons. Thus, it became the job of engineers and scientists to find
    the best ML algorithms that can solve a given prediction problem within the required
    constraints. This job, however, was eased with the invention of AutoML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 所有机器学习算法都以**计算统计学**为基础。计算统计学是统计学与计算机科学的结合，其中计算机用于计算复杂的数学。这种计算就是机器学习算法，而我们从中得到的预测结果。作为机器学习领域的工程师和科学家，我们通常需要了解机器学习算法的基本逻辑。人工智能领域有大量的机器学习算法。它们都旨在解决不同类型的预测问题。它们各自也有自己的优缺点。因此，工程师和科学家的工作就是找到能够解决给定预测问题且在所需约束条件下表现最佳的机器学习算法。然而，随着AutoML的发明，这项工作变得容易多了。
- en: Despite AutoML taking over this huge responsibility of finding the best ML algorithm,
    it is still our job as engineers and scientists to verify and justify the selection
    of these algorithms. And to do that, a basic understanding of the ML algorithms
    is a must.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AutoML承担了寻找最佳机器学习算法的巨大责任，但作为工程师和科学家，我们仍然需要验证和证明这些算法的选择。为此，对机器学习算法的基本理解是必不可少的。
- en: In this chapter, we shall explore and understand the various ML algorithms that
    H2O AutoML uses to train models. As mentioned previously, all ML algorithms have
    a heavy foundation in statistics. Statistics itself is a huge branch of mathematics
    and is too large to cover in a single chapter. Hence, for the sake of having a
    basic understanding of how ML algorithms work, we shall explore their inner workings
    conceptually with basic statistics rather than diving deep into the math.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨和理解H2O AutoML用于训练模型的各种机器学习算法。如前所述，所有机器学习算法都有深厚的统计学基础。统计学本身是数学的一个庞大分支，内容太多，无法在一章中涵盖。因此，为了对机器学习算法的工作原理有一个基本的理解，我们将从基本统计学概念出发，从概念上探讨它们的内部运作，而不是深入数学。
- en: Tip
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'If you are interested in gaining more knowledge in the field of statistics,
    then the following link should be a good place to start: [https://online.stanford.edu/courses/xfds110-introduction-statistics](https://online.stanford.edu/courses/xfds110-introduction-statistics).'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要在统计学领域获得更多知识，那么以下链接应该是一个不错的起点：[https://online.stanford.edu/courses/xfds110-introduction-statistics](https://online.stanford.edu/courses/xfds110-introduction-statistics)。
- en: First, we shall understand what the different types of ML algorithms are and
    then learn about the workings of these algorithms. We will do this by breaking
    them down into individual concepts, understanding them, and then building the
    algorithm back up to understand the big picture.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将了解不同类型的机器学习算法，然后学习这些算法的工作原理。我们将通过将它们分解为单个概念，理解它们，然后再将这些概念组合起来以理解整体来做到这一点。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Understanding the different types of ML algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解不同类型的机器学习算法
- en: Understanding the Generalized Linear Model algorithm
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解广义线性模型算法
- en: Understanding the Distributed Random Forest algorithm
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解分布式随机森林算法
- en: Understanding the Gradient Boosting Machine algorithm
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解梯度提升机算法
- en: Understanding what is Deep Learning
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解深度学习是什么
- en: So, let’s begin our journey by understanding the different types of ML algorithms.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从理解不同类型的机器学习算法开始我们的旅程。
- en: Understanding the different types of ML algorithms
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解不同类型的机器学习算法
- en: ML algorithms are designed to solve a specific prediction problem. These prediction
    problems can be anything that can provide value if predicted accurately. The differentiating
    factor between various prediction problems is what value is to be predicted. Is
    it a simple yes or no value, a range of numbers, or a specific value from a list
    of potential values, probabilities, or semantics of a text? The field of ML is
    vast enough to cover the majority, if not all, of such problems in a wide variety
    of ways.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法旨在解决特定的预测问题。这些预测问题可以是任何在预测准确的情况下能提供价值的问题。不同预测问题之间的区别在于要预测的价值是什么。是简单的“是”或“否”值，一个数值范围，还是从潜在值列表中选择的一个特定值，或者是文本的概率或语义？机器学习领域足够广泛，可以以多种方式涵盖大多数，如果不是所有这样的问题。
- en: 'So, let’s start with understanding the different categories of prediction problems.
    They are as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们从了解预测问题的不同类别开始。它们如下：
- en: '**Regression**: Regression analysis is a statistical process that aims to find
    the relationship between independent variables, also called features, and dependent
    variables, also called label or response variables, and use that relationship
    to predict future values.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回归**：回归分析是一种统计过程，旨在找到独立变量（也称为特征）和因变量（也称为标签或响应变量）之间的关系，并使用这种关系来预测未来的值。'
- en: Regression problems are problems that aim to predict certain continuous numerical
    values – for example, predicting the price of a car given the car’s brand name,
    engine size, economy, and electronic features. In such a scenario, the car’s brand
    name, engine size, economy, and electronic features are the independent variables
    as their presence is independent of other values, while the car price is the dependent
    variable whose value is dependent on the other features. Also, the price of the
    car is a continuous value as it can numerically range anywhere from 0 to 100 million
    in dollars or any other currency.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回归问题是指旨在预测某些连续数值的问题——例如，根据汽车的品牌名称、发动机大小、经济性和电子特性来预测汽车的价格。在这种情况下，汽车的品牌名称、发动机大小、经济性和电子特性是独立变量，因为它们的存在与其他值无关，而汽车价格是因变量，其值取决于其他特性。此外，汽车的价格是一个连续值，因为它可以在0到1亿美元或任何其他货币的数值范围内。
- en: '**Classification**: Classification is a statistical process that aims to categorize
    the label values depending on their relationship to the features into certain
    classes or categories.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类**：分类是一种统计过程，旨在根据标签值与特征之间的关系将它们分类到某些类别或类别中。'
- en: Classification problems are problems that aim to predict a certain set of values
    – for example, predicting if a person is likely to face heart disease, depending
    on their cholesterol level, weight, exercise levels, heart rate, and family history.
    Another example would be predicting the rating of a restaurant on Google reviews
    that ranges from 1-5 stars, depending on the location, food, ambience, and price.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 分类问题是指旨在预测一组特定值的问题——例如，根据一个人的胆固醇水平、体重、锻炼水平、心率和家庭病史来预测这个人是否可能面临心脏病。另一个例子是预测餐厅在谷歌评论上的评分，评分范围从1到5星，这取决于位置、食物、环境和价格。
- en: 'As you can see from these examples, classification problems can be either a
    *yes* or *no*, *true* or *false*, or *1* or *0* type of classification, or a specific
    set of classification values, such as those in the Google Review example, where
    the values can either be 1, 2, 3, 4, or 5\. Thus, classification problems can
    be further divided, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从这些例子中看到的，分类问题可以是“是”或“否”，“真”或“假”，“1”或“0”类型的分类，或者是一组特定的分类值，例如在谷歌评论示例中，值可以是1、2、3、4或5。因此，分类问题可以进一步分为以下几类：
- en: '**Binary Classification**: In this type of classification problem, the predicted
    values are binary, meaning they have only two values – that is, *yes* or *no*,
    *true* or *false*, *1* or *0*.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元分类**：在这种分类问题中，预测值是二元的，这意味着它们只有两个值——也就是说，“是”或“否”，“真”或“假”，“1”或“0”。'
- en: '**Multiclass/Polynomial Classification**: In this type of classification problem,
    the predicted values are non-binary, also called polynomial, in nature, meaning
    they have more than two sets of values. For example, classification by age, which
    involves whole numbers from 1 to 100, or classification by primary colors, which
    can be red, yellow, or blue.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多类/多项式分类**：在这种分类问题中，预测值是非二元的，也称为多项式，这意味着它们有多于两组的值。例如，按年龄分类，涉及从1到100的整数，或者按基本颜色分类，可以是红色、黄色或蓝色。'
- en: '**Clustering**: Clustering is a statistical process that aims to group or divide
    certain data points in such a way that data points in a single group have similar
    characteristics that are different from data points in other groups.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：聚类是一种统计过程，旨在以某种方式将某些数据点分组或划分，使得单个组内的数据点具有与其他组数据点不同的相似特征。'
- en: Clustering problems are problems that aim to understand similarities within
    a set of values.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类问题**：聚类问题是旨在理解一组值内部相似性的问题。'
- en: 'For example, given a set of people who play video games with certain details,
    such as the hardware they use, the different games they play, and time spent playing
    those video games, you can categorize people by their favorite game genre. Clustering
    can be further divided as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，给定一组具有某些细节（如他们使用的硬件、他们玩的不同游戏以及他们玩这些视频游戏的时间）玩视频游戏的人，你可以根据他们最喜欢的游戏类型对人们进行分类。聚类可以进一步细分为以下几种：
- en: '**Hard Clustering**: In this type of clustering, all data points either belong
    to one or another cluster; they are mutually exclusive.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬聚类**：在这种类型的聚类中，所有数据点要么属于一个簇，要么属于另一个簇；它们是互斥的。'
- en: '**Soft Clustering**: In this type of clustering, rather than assigning a data
    point to a cluster, the probability that a data point might belong to a certain
    cluster is calculated. This opens the likelihood that a data point might belong
    to multiple clusters at the same time.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**软聚类**：在这种类型的聚类中，不是将数据点分配给一个簇，而是计算数据点可能属于某个簇的概率。这增加了数据点可能同时属于多个簇的可能性。'
- en: '**Association**: Association is a statistical process that aims to find the
    probability that if event A happened, what is the likelihood that event B will
    happen too? Association problems are based on association rules, which are if-then
    statements that show the probability of a relationship between different data
    points.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联**：关联是一种统计过程，旨在找出如果事件A发生了，事件B发生的可能性有多大？关联问题基于关联规则，这些规则是如果-那么陈述，显示了不同数据点之间关系的概率。'
- en: The most common example of the association problem is **Market Basket Analysis**.
    Market Basket Analysis is a prediction problem where, given a user buys a certain
    product A from the market, what is the probability of the user buying product
    B, which is related to product A?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 关联问题的最常见例子是**市场篮子分析**。市场篮子分析是一个预测问题，给定一个用户在市场上购买了一定产品A，那么用户购买与产品A相关的产品B的概率是多少？
- en: '**Optimization/Control**: **Control Theory**, **Optimal Control Theory**, or
    **Optimization Problems** is a branch of mathematics that deals with finding a
    certain combination of values that collectively optimize a dynamic system. **Machine
    Learning Control** (**MLC**) is a subfield in ML that aims to solve the Optimization
    Problem using ML. A good example of MLC is the implementation of ML to optimize
    traffic on roads using automated cars.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化/控制**：**控制理论**、**最优控制理论**或**优化问题**是数学的一个分支，它处理寻找一组值的组合，以优化动态系统。**机器学习控制**（**MLC**）是机器学习中的一个子领域，旨在使用机器学习来解决优化问题。MLC的一个好例子是将机器学习应用于优化道路上的交通，使用自动汽车。'
- en: 'Now that we understand the different types of prediction problems, let’s dive
    into understanding the different types of ML algorithms. The different types of
    ML algorithms are categorized as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同类型的预测问题，让我们深入了解不同类型的机器学习算法。不同类型的机器学习算法被分类如下：
- en: '**Supervised Learning**: Supervised learning is the ML task of mapping the
    relationship between the independent variables and dependent variables based on
    previously existing values that are labeled. Labeled data is data that contains
    information about which of its features are dependent and which features are independent.
    In supervised learning, we know which feature we want to predict and tag that
    feature as a label. The ML algorithm will use this information to map the relationships.
    Using this mapping, we predict the output for new input values. Another way of
    understanding this problem is that the previously existing values supervise the
    ML algorithm''s learning task.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：监督学习是根据先前存在的、已标记的值来映射自变量和因变量之间关系的机器学习任务。标记数据是包含有关其特征哪些是依赖的、哪些是独立的的信息的数据。在监督学习中，我们知道我们想要预测的特征，并将该特征标记为标签。机器学习算法将使用这些信息来映射关系。使用这种映射，我们预测新输入值的输出。另一种理解这个问题的方式是，先前存在的值监督机器学习算法的学习任务。'
- en: Supervised learning algorithms are often used to solve regression and classification
    problems.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法通常用于解决回归和分类问题。
- en: Some examples of supervised learning algorithms are **decision trees**, **linear
    regression**, and **neural networks**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习算法的一些例子包括**决策树**、**线性回归**和**神经网络**。
- en: '**Unsupervised learning**: As mentioned previously, supervised learning is
    the ML task of finding patterns and behaviors from data that is not tagged. In
    this case, we don’t know which feature we want to predict, or the feature we want
    to predict may not even be a part of the dataset. Unsupervised learning helps
    us predict potential repeating patterns and categorize the set of data using those
    patterns. Another way of understanding this problem is that there are no labeled
    values to supervise the ML algorithm learning task; the algorithm learns the patterns
    and behaviors on its own.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：如前所述，监督学习是从未标记的数据中寻找模式和行为的机器学习任务。在这种情况下，我们不知道我们想要预测哪个特征，或者我们想要预测的特征甚至可能不是数据集的一部分。无监督学习帮助我们预测潜在的重复模式，并使用这些模式对数据集进行分类。另一种理解这个问题的方式是，没有标记的值来监督机器学习算法的学习任务；算法独立地学习模式和行为了。'
- en: Unsupervised learning algorithms are often used to solve clustering and association
    problems.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法通常用于解决聚类和关联问题。
- en: Some examples of unsupervised learning algorithms are **K-means clustering**
    and **association rule learning**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法的一些例子包括**K-means聚类**和**关联规则学习**。
- en: '**Semi-supervised learning**: Semi-supervised learning falls between supervised
    learning and unsupervised learning. It is the ML task of performing learning on
    a dataset that is partially labeled. It is used in scenarios where you have a
    small dataset that is labeled along with a large unlabeled dataset. In real-world
    scenarios, labeling large amounts of data is an expensive task as it requires
    a lot of experimentation and contextual information that is manually interpreted,
    while unlabeled data is relatively cheap to acquire. Semi-supervised learning
    often proves efficient in this case as it is good at assuming expected label values
    from unlabeled datasets while working as efficiently as any supervised learning
    algorithm.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**半监督学习**：半监督学习介于监督学习和无监督学习之间。它是执行部分标记数据集上的机器学习任务的机器学习任务。它用于你有一个小部分标记的数据集和大量未标记数据集的场景。在现实场景中，标记大量数据是一个昂贵的任务，因为它需要大量的实验和需要人工解释的上下文信息，而未标记数据相对容易获取。在这种情况下，半监督学习通常证明是有效的，因为它擅长从未标记数据集中假设预期的标签值，同时像任何监督学习算法一样高效地工作。'
- en: Unsupervised learning algorithms are often used to solve clustering and classification
    problems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习算法通常用于解决聚类和分类问题。
- en: Some examples of semi-supervised learning algorithms are **generative models**
    and **Laplacian regularization**.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 半监督学习算法的一些例子包括**生成模型**和**拉普拉斯正则化**。
- en: '**Reinforcement learning**: Reinforcement learning is an ML task that aims
    to identify the next correct logical action to take in a given environment to
    maximize the cumulative reward. In this type of learning, the accuracy of the
    prediction is calculated after the prediction is made using positive and/or negative
    reinforcement, which is again fed to the algorithm. This continuous learning of
    the environment eventually helps the algorithm find the best sequence of steps
    to take to maximize the reward, thus making the most accurate decision.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**：强化学习是一个旨在在给定环境中识别下一步正确逻辑动作以最大化累积奖励的机器学习任务。在这种学习中，预测的准确性是在使用正强化和/或负强化后计算的，这些强化再次被输入到算法中。这种对环境的持续学习最终帮助算法找到最佳步骤序列以最大化奖励，从而做出最准确的决策。'
- en: Reinforcement learning is often used to solve a mix of regression, classification,
    and optimization problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习通常用于解决回归、分类和优化问题的混合。
- en: Some examples of reinforcement learning algorithms are **Monte Carlo Methods**,
    **Q-Learning**, and **Deep Q Network**.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法的一些例子包括**蒙特卡洛方法**、**Q学习**和**深度Q网络**。
- en: 'The AutoML technology, despite being mature enough to be used commercially,
    is still in its infancy compared to the vast developments in the field of ML.
    AutoML may be able to train the best predictive models in the shortest time using
    little to no human intervention, but its potential is currently limited to only
    supervised learning. The following diagram summarizes the various types of ML
    algorithms categorized under the different ML tasks:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管AutoML技术已经足够成熟，可以用于商业用途，但与机器学习领域的广泛发展相比，它仍然处于起步阶段。AutoML可能能够在最短的时间内使用最少或没有人为干预来训练最佳的预测模型，但它的潜力目前仅限于监督学习。以下图表总结了不同机器学习任务下分类的各种机器学习算法：
- en: '![Figure 5.1 – Types of ML problems and algorithms ](img/B17298_05_001.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 机器学习问题的类型和算法](img/B17298_05_001.jpg)'
- en: Figure 5.1 – Types of ML problems and algorithms
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 机器学习问题的类型和算法
- en: Similarly, H2O’s AutoML also focuses on supervised learning and as such, you
    are often expected to have labeled data that you can feed to it.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，H2O的AutoML也专注于监督学习，因此你通常需要提供标签数据来供其使用。
- en: ML algorithms that perform unsupervised learning are often quite sophisticated
    compared to supervised learning algorithms as there is no ground truth to measure
    the performance of the model. This goes against the very nature of AutoML, which
    is very reliant on model performance measurements to automate training and hyperparameter
    tuning.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习算法相比，执行无监督学习的机器学习算法通常更为复杂。因为在这种情况下，没有真实数据来衡量模型的性能。这与AutoML的本质相悖，AutoML非常依赖于模型性能的测量来自动化训练和超参数调整。
- en: So, accordingly, H2O AutoML falls in the domain of supervised ML algorithms,
    where it trains several supervised ML algorithms to solve regression and classification
    problems and ranks them based on their performance. In this chapter, we shall
    focus on these ML algorithms and understand their functionality so that we are
    well equipped to understand, select, and justify the different models that H2O
    AutoML trains for a given prediction problem.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，相应地，H2O AutoML属于监督机器学习算法的范畴，它训练多个监督机器学习算法来解决回归和分类问题，并根据它们的性能进行排名。在本章中，我们将关注这些机器学习算法，了解它们的功能，以便我们能够充分理解、选择和证明H2O
    AutoML为特定预测问题训练的不同模型。
- en: 'With this understanding, let’s start with the first ML algorithm: Generalized
    Linear Model.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种理解，让我们从第一个机器学习算法：广义线性模型开始。
- en: Understanding the Generalized Linear Model algorithm
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解广义线性模型算法
- en: '**Generalized Linear Model** (**GLM**), as its name suggests, is a flexible
    way of generalizing linear models. It was formulated by *John Nelder* and *Robert
    Wedderburn* as a way of combining various regression models into a single analysis
    with considerations given to different probability distributions. You can find
    their detailed paper (Nelder, J.A. and Wedderburn, R.W., 1972\. *Generalized linear
    models. Journal of the Royal Statistical Society: Series A (General), 135(3),
    pp.370-384*.) at [https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614](https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614).'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '**广义线性模型**（**GLM**），正如其名所示，是一种灵活的推广线性模型的方法。它是由*约翰·尼尔*和*罗伯特·韦德伯恩*提出的，作为一种将各种回归模型组合成一个分析的方法，同时考虑到不同的概率分布。你可以在[https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614](https://rss.onlinelibrary.wiley.com/doi/abs/10.2307/2344614)找到他们详细的论文（Nelder,
    J.A. and Wedderburn, R.W., 1972\. *广义线性模型. 皇家统计学会会刊：A系列（一般），135(3)，pp.370-384*）。'
- en: Now, you may be wondering what linear models are. Why do we need to generalize
    them? What benefit does it provide? These are relevant questions indeed and they
    are pretty easy to understand without diving too deep into the mathematics. Once
    we break down the logic, you will notice that the concept of GLM is pretty easy
    to understand.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可能想知道什么是线性模型。为什么我们需要对它们进行推广？它提供了什么好处？这些问题确实很重要，而且不需要深入数学知识就可以很容易地理解。一旦我们分解了逻辑，你会发现广义线性模型的概念非常容易理解。
- en: So, let’s start by understanding the basics of linear regression.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们先从理解线性回归的基本概念开始。
- en: Introduction to linear regression
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归简介
- en: '**Linear regression** is probably one of the oldest statistical models, dating
    back to 200 years ago. It is an approach that maps the relationship between the
    dependent and independent variables linearly on a graph. What that means is that
    the relationship between the two variables can be completely explained by a straight
    line.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性回归**可能是最古老的统计模型之一，其历史可以追溯到200年前。它是一种在图上线性映射因变量和自变量之间关系的方法。这意味着两个变量之间的关系可以完全由一条直线来解释。'
- en: 'Consider the following example:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下例子：
- en: '![Figure 5.2 – Linear regression ](img/B17298_05_002.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 线性回归](img/B17298_05_002.jpg)'
- en: Figure 5.2 – Linear regression
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 线性回归
- en: This example demonstrates the relationship between two variables. The height
    of a person, H, is an independent variable, while the weight of a person, W, is
    a dependent variable. The relationship between these two variables can easily
    be explained by a straight red line. The taller a person is, the more likely he
    or she will weigh more. Easy enough to understand.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了两个变量之间的关系。一个人的身高H是自变量，而一个人的体重W是因变量。这两个变量之间的关系可以很容易地用一条红色的直线来解释。一个人越高，他或她的体重越有可能更重。这很容易理解。
- en: 'Statistically, the general equation for any straight line, also called the
    **linear equation**, is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 统计上，任何直线的通用方程，也称为**线性方程**，如下所示：
- en: '![](img/Formula_B17298__05_001.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17298__05_001.png)'
- en: 'Here, we have the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有以下内容：
- en: '*y* is a point on the *Y*-axis and indicates the dependent variable.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是 *Y* 轴上的一个点，表示因变量。'
- en: '*x* is a point on the *X*-axis and indicates the independent variable.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x* 是 *X* 轴上的一个点，表示自变量。'
- en: '*b**1* is the slope of the line, also called the gradient, and indicates how
    steep the line is. The bigger the gradient, the steeper the line.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**1* 是直线的斜率，也称为梯度，表示直线的陡峭程度。梯度越大，直线越陡。'
- en: '*b**0* is a constant that indicates the point at which the line crosses the
    *Y*-axis.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b**0* 是一个常数，表示直线与 *Y* 轴相交的点。'
- en: During linear regression, the machine will map all the data points of the two
    variables on the graph and randomly place the line on the graph. Then, it will
    calculate the values of *y* by inserting the value of *x* from the data points
    in the graph into the linear equation and comparing the result with the respective
    *y* values from the data points. After that, it will calculate the magnitude of
    the error between the *y* value it calculated and the actual *y* value. This difference
    in values is what we call a **residual**.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归过程中，机器将在图上映射两个变量的所有数据点，并在图上随机放置直线。然后，它将通过将图中的数据点的 *x* 值插入线性方程中来计算 *y* 的值，并将结果与数据点的相应
    *y* 值进行比较。之后，它将计算它计算的 *y* 值与实际 *y* 值之间的误差大小。这种值之间的差异就是我们所说的**残差**。
- en: 'The following diagram should help you understand what residuals are:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表应有助于你理解什么是残差：
- en: '![Figure 5.3 – Residuals in linear regression ](img/B17298_05_003.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图5.3 – 线性回归中的残差](img/B17298_05_003.jpg)'
- en: Figure 5.3 – Residuals in linear regression
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 线性回归中的残差
- en: The machine will do this for all the data points and make a note of all the
    errors. It will then try to tweak the line by changing the values of b1 and b0,
    meaning changing the angle and position of the line on the graph, and repeating
    the process. It will do this until it minimizes the error.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 机器将为所有数据点做这件事，并记录所有误差。然后，它将通过改变b1和b0的值来尝试调整直线，这意味着改变图上直线的角度和位置，并重复这个过程。它将这样做，直到最小化误差。
- en: The values of b1 and b0 that generate the least amount of error are the most
    accurate linear relationship between the two variables. The equation with these
    values for b1 and b0 is the linear model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 产生最少误差的b1和b0的值是两个变量之间最准确的线性关系。具有这些b1和b0值的方程是线性模型。
- en: Now, say you want to predict how much a person would weigh if they were 180
    cm tall. Then, you use this same linear model equation with the b1 and b0 values,
    set *x* to 180, and calculate *y*, which will be the expected weight.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设你想预测一个身高180厘米的人的体重。然后，你使用这个相同的线性模型方程，使用b1和b0的值，将 *x* 设置为180，并计算 *y*，这将是你预期的体重。
- en: 'Congratulations, you just performed ML in your mind without any computers and
    made predictions too! Actual ML works the same way, albeit with added complexities
    from complex algorithms. Linear regression doesn’t need to be restricted to just
    two variables – it can also work on multiple variables where there’s more than
    one independent variable. Such linear regression is called multiple or curvilinear
    regression. The equation of such a linear regression expands as follows:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你，你刚刚在心中进行了机器学习，没有任何计算机的帮助，并且做出了预测！实际的机器学习工作方式相同，尽管增加了复杂算法的复杂性。线性回归不仅限于两个变量——它还可以处理存在多个自变量的多个变量。这种线性回归称为多元或曲线回归。这种线性回归的方程如下扩展：
- en: '![](img/Formula_B17298__05_002.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17298__05_002.png)'
- en: In this equation, the additional variables – x1, x2, x3, and so on – are added
    with their own coefficients – b1, b2, and b3, respectively.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，附加的变量——x1、x2、x3等等——分别与它们自己的系数——b1、b2和b3相加。
- en: Feel free to explore these algorithms and the mathematics behind them if you
    are interested in the inner workings of linear regression.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对线性回归的内部工作原理感兴趣，不妨探索这些算法及其背后的数学。
- en: Understanding the assumptions of linear regression
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解线性回归的假设
- en: Linear regression, when training a model on a given dataset, works on certain
    assumptions about the data. One of these assumptions is the **normality of errors**.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当在给定的数据集上训练模型时，线性回归基于对数据的某些假设。这些假设之一是**误差的正态性**。
- en: 'Before we understand what the normality of errors is, let’s quickly understand
    the concept of the **probability density function**. This is a mathematical expression
    that defines the probability distribution of discrete values – in other words,
    it is a mathematical expression that shows the probability of a sample value occurring
    from a given sample space. To understand this, refer to the following diagram:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们了解误差的正态性之前，让我们快速了解**概率密度函数**的概念。这是一个数学表达式，它定义了离散值的概率分布——换句话说，它是一个数学表达式，显示了从给定的样本空间中发生样本值的概率。为了理解这一点，请参考以下图表：
- en: '![Figure 5.4 – Probability distribution of values for two dice ](img/B17298_05_004.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图5.4 – 两个骰子数值的概率分布](img/B17298_05_004.jpg)'
- en: Figure 5.4 – Probability distribution of values for two dice
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – 两个骰子数值的概率分布
- en: 'The preceding diagram shows the distribution of probabilities of all the values
    that can occur when a pair of six-sided dice are thrown fairly and independently.
    There are different kinds of distributions. Some examples of commonly occurring
    distributions are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了当公平且独立地掷一对六面骰子时，所有可能出现的数值的概率分布。存在不同类型的分布。以下是一些常见分布的例子：
- en: '![Figure 5.5 – Different types of distribution ](img/B17298_05_005.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图5.5 – 不同类型的分布](img/B17298_05_005.jpg)'
- en: Figure 5.5 – Different types of distribution
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – 不同类型的分布
- en: 'The normality of errors states that the residuals of the data must be normally
    distributed. A **normal distribution**, also called **Gaussian distribution**,
    is a probability density function that is symmetrical about the mean, where the
    values closest to the mean occur frequently, while those far from the mean rarely
    occur. The following diagram shows a normal distribution:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 误差的正态性表明，数据残差必须是正态分布的。**正态分布**，也称为**高斯分布**，是一个关于均值对称的概率密度函数，其中最接近均值的值出现频率较高，而远离均值的值很少出现。以下图表显示了正态分布：
- en: '![Figure 5.6 – Normal distribution  ](img/B17298_05_006.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 正态分布](img/B17298_05_006.jpg)'
- en: Figure 5.6 – Normal distribution
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 正态分布
- en: Linear regression expects the residuals that get calculated to fall within a
    normal distribution. In our previous example of the expected weight for a height,
    there is bound to be some error between the predicted weight and the actual weight
    of a person with a certain height. However, the residuals or errors from the prediction
    will most likely fall within a normal distribution as there cannot be too many
    occurrences of people with an extreme difference between the expected weight and
    the predicted weight.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归期望计算出的残差落在正态分布范围内。在我们之前的关于预期体重与一定身高的人实际体重之间误差的例子中，预测体重和实际体重之间必然存在一些误差。然而，由于预期体重和预测体重之间极端差异的人出现的次数不会太多，因此预测的残差或误差很可能会落在正态分布范围内。
- en: 'Consider a scenario of people claiming health insurance payouts. The following
    diagram shows a sample of the linear regression graph for that dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个人们申请健康保险赔付的场景。以下图表显示了该数据集的线性回归图样本：
- en: '![Figure 5.7 – Health insurance payout ](img/B17298_05_007.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.7 – 健康保险赔付](img/B17298_05_007.jpg)'
- en: Figure 5.7 – Health insurance payout
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – 健康保险赔付
- en: In the preceding diagram, you can see that the majority of people from various
    age groups did not claim health insurance. Some of them did and the cost of claims
    varied a lot. Some had minor issues costing *little*, while some had serious injuries
    and had to go through expensive surgeries.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，你可以看到来自各个年龄段的大多数人没有申请健康保险。其中一些人申请了，索赔费用差异很大。有些人有轻微的问题，花费*很少*，而有些人遭受了严重的伤害，不得不进行昂贵的手术。
- en: 'If you plot a linear regression line through this dataset, it will look as
    follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过这个数据集绘制线性回归线，它看起来如下所示：
- en: '![Figure 5.8 – Linear regression on health insurance payouts ](img/B17298_05_008.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.8 – 健康保险赔付的线性回归](img/B17298_05_008.jpg)'
- en: Figure 5.8 – Linear regression on health insurance payouts
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 健康保险赔付的线性回归
- en: 'But now, if you calculate the residual errors from the expected and predicted
    value for all the data points, then the probability distribution of these residuals
    will not fall into a normal distribution. It will look as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，现在如果你从所有数据点的预期值和预测值中计算残差误差，那么这些残差的概率分布将不会落入正态分布。它看起来如下所示：
- en: '![Figure 5.9 – Residual distribution of health insurance payouts ](img/B17298_05_009.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9 – 健康保险赔付的残差分布](img/B17298_05_009.jpg)'
- en: Figure 5.9 – Residual distribution of health insurance payouts
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 健康保险赔付的残差分布
- en: This is an inaccurate model as the expected value and the predicted values are
    not even close enough to round off or correct. So, what do you do for such a scenario,
    where the normality of errors assumption fails for the dataset? What if the distribution
    of the residuals is, say, Poisson instead of normal? How will the machine correct
    that?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个不精确的模型，因为预期值和预测值甚至不够接近，以至于可以四舍五入或纠正。那么，对于这种错误假设对于数据集不成立的情况，你该怎么办？如果残差的分布是泊松分布而不是正态分布，机器将如何纠正？
- en: Well, the answer to this is that the distribution of residuals depends on the
    distribution of the dataset itself. If the values of the dependent variables are
    normally distributed, then the distribution of the residuals will also be normal.
    So, once we have identified which probability density function fits the dataset,
    we can use that function to train our linear model.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，这个问题的答案是残差的分布取决于数据集本身的分布。如果因变量的值呈正态分布，那么残差的分布也将是正态的。因此，一旦我们确定了哪个概率密度函数适合数据集，我们就可以使用该函数来训练我们的线性模型。
- en: Depending on this function, there are specialized linear regression methods
    for every probability density function. If your distribution is **Poisson**, then
    you can use **Poisson regression**. If your data distribution is negative **binomial**,
    then you can use **negative binomial regression**.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这个函数，为每个概率密度函数有专门的线性回归方法。如果你的分布是**泊松**，那么你可以使用**泊松回归**。如果你的数据分布是负**二项式**，那么你可以使用**负二项式回归**。
- en: Working with a Generalized Linear Model
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用广义线性模型（GLM）进行工作
- en: Now that we have covered the basics, let’s focus on understanding what GLM is.
    GLM is a way of pointing to all the regression methods that are specific to the
    type of probability distribution of the data. Technically, all the regression
    models are GLM, including our ordinary simple linear model. GLM just encapsulates
    them together and trains the appropriate regression model based on the probability
    distribution function.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经涵盖了基础知识，让我们来了解什么是GLM。GLM是一种指向所有特定于数据类型概率分布的回归方法的方式。技术上讲，所有回归模型都是GLM，包括我们的普通简单线性模型。GLM只是将它们封装在一起，并根据概率分布函数训练适当的回归模型。
- en: The way GLM works is by using something called a link function in conjunction
    with a systematic component and the random variable.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: GLM的工作方式是使用所谓的连接函数，结合系统成分和随机变量。
- en: 'These are three components of GLM:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是广义线性模型（GLM）的三个组成部分：
- en: '**Systematic component**: Going back to the multi-variate linear equation,
    we have the following:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系统成分**：回到多元线性方程，我们有以下内容：'
- en: '![](img/Formula_B17298__05_003.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B17298__05_003.png)'
- en: Here, b1x1+ b2x2 + b3x3 + ……. + b0 is the systematic component. This is the
    function that links our data, also called predictors, with our predictions.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，b1x1+ b2x2 + b3x3 + …… + b0 是系统成分。这是将我们的数据（也称为预测变量）与我们的预测相联系的功能。
- en: '**Random Component**: This component refers to the probability distribution
    of the response variable. This will be whether the response variable is normally
    distributed or binomially distributed or any other form of distribution.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机成分**：这个成分指的是响应变量的概率分布。这将是响应变量是正态分布、二项分布还是其他任何形式的分布。'
- en: '**Link function**: A link function is a function that maps the non-linear relationship
    of data to a linear one. In other words, it bends the line of linear regression
    to represent the relationship of non-linear data more accurately. It is a link
    between the random and the systematic components. We can explain the equation
    with a link function mathematically as *Y = f*n*( b*1*x*1*+ b*2*x*2 *+ b*3*x*3
    *+ ……. + b*0 *)*, where *f*n is the link function that changes as per the distribution
    of the response variable.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**链接函数**：链接函数是一个将数据的非线性关系映射到线性关系上的函数。换句话说，它弯曲线性回归的线，以更准确地表示非线性数据的关系。它是随机成分和系统成分之间的链接。我们可以用链接函数数学上解释方程式为
    *Y = f*n*( b*1*x*1*+ b*2*x*2 *+ b*3*x*3 *+ …… + b*0 *)*，其中 *f*n 是随响应变量分布变化的链接函数。'
- en: 'The link function is different for different distributions. The following table
    shows the different link functions for different distributions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 链接函数对于不同的分布是不同的。下表显示了不同分布的不同链接函数：
- en: '| **Distribution Type** | **Link Function** | **Name of Algorithm** |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| **分布类型** | **链接函数** | **算法名称** |'
- en: '| Normal | ![](img/Formula_B17298__05_004.png) | Linear model |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 正态 | ![](img/Formula_B17298__05_004.png) | 线性模型 |'
- en: '| Binomial | ![](img/Formula_B17298__05_005.png) | Logistic regression |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 二项分布 | ![](img/Formula_B17298__05_005.png) | 逻辑回归 |'
- en: '| Poisson | ![](img/Formula_B17298__05_006.png) | Poisson regression |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 泊松 | ![](img/Formula_B17298__05_006.png) | 泊松回归 |'
- en: '| Gamma | ![](img/Formula_B17298__05_007.png) | Gamma regression |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 伽马 | ![](img/Formula_B17298__05_007.png) | 伽马回归 |'
- en: Figure 5.10 – Link functions for different distribution types
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 不同分布类型的链接函数
- en: When training GLM models, you have the option of selecting the value for the
    family hyperparameter. The family option specifies the probability distribution
    of your response column and the GLM training algorithm uses the appropriate link
    function during training.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练GLM模型时，你可以选择家庭超参数的值。家庭选项指定了响应列的概率分布，GLM训练算法在训练期间使用适当的链接函数。
- en: 'The values for the family hyperparameter are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 家庭超参数的值如下：
- en: '**gaussian**: You should select this option if the response is a real integer
    number.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯分布**：如果响应是一个实整数，你应该选择此选项。'
- en: '**binomial**: You should select this option if the response is categorical
    with two classes or binaries that could be either enums or integers.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二项分布**：如果响应是具有两个类别的分类响应或二进制，可以是枚举或整数，你应该选择此选项。'
- en: '**fractionalbinomial**: You should select this option if the response is numeric
    between 0 and 1.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分数二项分布**：如果响应是介于0和1之间的数值，你应该选择此选项。'
- en: '**ordinal**: You should select this option if the response is a categorical
    response with three or more classes.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有序**：如果响应是一个具有三个或更多类别的分类响应，你应该选择此选项。'
- en: '**quasibinomial**: You should select this option if the response is numeric.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准二项分布**：如果响应是数值，你应该选择此选项。'
- en: '**multinomial**: You should select this option if the response is a categorical
    response with three or more classes that are of enum types.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式分布**：如果响应是一个具有三个或更多类别的分类响应，且这些类别为枚举类型，你应该选择此选项。'
- en: '**poisson**: You should select this option if the response is numeric and contains
    non-negative integers.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泊松**：如果响应是数值且包含非负整数，你应该选择此选项。'
- en: '**gamma**: You should select this option if the response is numeric and continuous
    and contains positive real integers.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**伽马分布**：如果响应是数值且连续，并包含正实整数，你应该选择此选项。'
- en: '**tweedie**: You should select this option if the response is numeric and contains
    continuous real values and non-negative values.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tweedie**：如果响应是数值且包含连续的实数和非负值，你应该选择此选项。'
- en: '**negativebinomial**: You should select this option if the response is numeric
    and contains a non-negative integer.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**负二项分布**：如果响应是数值且包含非负整数，你应该选择此选项。'
- en: '**AUTO**: This determines the family automatically for the user.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AUTO**：这会自动为用户确定家族。'
- en: As you may have guessed, H2O’s AutoML selects AUTO as the family type when training
    GLM models. The AutoML process handles this case of selecting the correct distribution
    family by understanding the distribution of the response variable in the dataset
    and applying the correct link function to train the GLM model.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，H2O的AutoML在训练GLM模型时将AUTO选为家族类型。AutoML过程通过理解数据集中响应变量的分布并应用正确的链接函数来训练GLM模型，来处理选择正确分布家族的情况。
- en: Congratulations, we have just looked into how the GLM algorithm works! GLM is
    a very powerful and flexible algorithm and H2O AutoML expertly configures its
    training so that it trains the most accurate and high-performance GLM model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，我们刚刚了解了GLM算法的工作原理！GLM是一个非常强大且灵活的算法，H2O AutoML专家地配置了其训练，以便训练最准确和性能最高的GLM模型。
- en: 'Now, let’s move on to the next ML algorithm that H2O trains: **Distributed
    Random Forest** (**DRF**).'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续了解H2O训练的下一个机器学习算法：**分布式随机森林**（**DRF**）。
- en: Understanding the Distributed Random Forest algorithm
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解分布式随机森林算法
- en: '**DRF**, simply called **Random Forest**, is a very powerful supervised learning
    technique often used for classification and regression. The foundation of the
    DRF learning technique is based on **decision trees**, where a large number of
    decision trees are randomly created and used for predictions and their results
    are combined to get the final output. This randomness is used to minimize the
    bias and variance of all the individual decision trees. All the decision trees
    are collectively combined and called a forest, hence the name Random Forest.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**DRF**，简称为**随机森林**，是一种非常强大的监督学习技术，常用于分类和回归。DRF学习技术的基础是基于**决策树**，其中随机创建大量决策树用于预测，并将它们的预测结果结合起来得到最终输出。这种随机性用于最小化所有单个决策树的偏差和方差。所有决策树共同组合在一起，称为森林，因此得名随机森林。'
- en: To get a deeper conceptual understanding of DRF, we need to understand the basic
    building block of DRF – that is, a decision tree.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 要对DRF有更深入的概念理解，我们需要了解DRF的基本构建块——那就是决策树。
- en: Introduction to decision trees
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树简介
- en: 'In very simple terms, a decision tree is just a set of *IF* conditions that
    either return a yes or a no answer based on data passed to it. The following diagram
    shows a simple example of a decision tree:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 用非常简单的话来说，决策树只是一组*IF*条件，根据传递给它的数据返回是或否的答案。以下图展示了决策树的一个简单示例：
- en: '![Figure 5.11 – Simple decision tree ](img/B17298_05_010.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – 简单决策树](img/B17298_05_010.jpg)'
- en: Figure 5.11 – Simple decision tree
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 简单决策树
- en: 'The preceding diagram shows a basic decision tree. A decision tree consists
    of the following components:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 上一图展示了基本的决策树。决策树由以下组件组成：
- en: '**Nodes**: Nodes are basically *IF* conditions that split the decision tree
    based on whether the condition was met or not.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**节点**：节点基本上是*IF*条件，根据条件是否满足来分割决策树。'
- en: '**Root Node**: The node on the top of the decision tree is called the root
    node.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**根节点**：决策树顶部的节点被称为根节点。'
- en: '**Leaf Node**: The nodes of the decision tree that do not branch out further
    are called leaf nodes, or simply leaves. The condition, in this case, is if the
    value of the data that’s passed to it is numeric, then the answer is the data
    is a number; if the data that’s passed to it is not numeric, then the answer will
    be the data is non-numeric. This is simple enough to understand.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**叶节点**：决策树中不再分支的节点被称为叶节点，或简称为叶子。在这种情况下，条件是如果传递给它的数据值是数值型的，则答案是数据是一个数字；如果传递给它的数据不是数值型的，则答案是数据是非数值型。这很简单，容易理解。'
- en: 'As seen in *Figure 5.11*, the decision tree is based on a simple true or false
    question. Decision trees can also be based on mathematical conditions on numeric
    data. The following example shows a decision tree on numeric conditions:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如图5.11所示，决策树基于一个简单的真或假问题。决策树也可以基于数值数据的数学条件。以下示例展示了基于数值条件的决策树：
- en: '![Figure 5.12 – Numerical decision tree ](img/B17298_05_011.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 – 数值决策树](img/B17298_05_011.jpg)'
- en: Figure 5.12 – Numerical decision tree
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – 数值决策树
- en: In this example, the root node computes whether the IQ number is greater than
    300 and decides if it is artificial intelligence or human intelligence.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，根节点计算智商数是否大于300，并决定它是人工智能还是人类智能。
- en: 'Decision trees can be combined as well. They can form a complex set of decision-making
    conditions that rely on the results of previous decisions. Refer to the following
    example for a complex decision tree:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树也可以组合使用。它们可以形成依赖于先前决策结果的复杂决策条件集。请参考以下示例中的复杂决策树：
- en: '![Figure 5.13 – Complex decision tree ](img/B17298_05_012.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13 – 复杂决策树](img/B17298_05_012.jpg)'
- en: Figure 5.13 – Complex decision tree
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 复杂决策树
- en: In the preceding example, we are trying to calculate if *you can go play outside*
    or *finish your ML studies*. This decision tree combines numeric data as well
    as the classification of data. When making predictions, the decision tree will
    start at the top and work its way down, making decisions on whether the data satisfies
    the conditions or not. The leaf nodes are the final potential results of the decision
    tree.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们试图计算你是否可以“出去玩”或“完成你的机器学习研究”。这个决策树结合了数值数据和数据的分类。在做出预测时，决策树将从顶部开始，逐层向下，根据数据是否满足条件做出决策。叶节点是决策树最终的潜在结果。
- en: 'With this knowledge in mind, let’s create a decision tree on a sample dataset.
    Refer to the following table for the sample dataset:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个知识的基础上，让我们在一个示例数据集上创建一个决策树。请参考以下表格中的示例数据集：
- en: '![Figure 5.14 – Sample dataset for creating a decision tree ](img/B17298_05_013.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14 – 创建决策树的示例数据集](img/B17298_05_013.jpg)'
- en: Figure 5.14 – Sample dataset for creating a decision tree
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – 创建决策树的示例数据集
- en: 'The content of the aforementioned dataset is as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 上述数据集的内容如下：
- en: '**Chest Pain**: This column indicates if a patient suffers from chest pain.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**胸痛**：此列表示患者是否患有胸痛。'
- en: '**Good Blood Circulation**: This column indicates if a patient has good blood
    circulation.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**良好血液循环**：此列表示患者是否有良好的血液循环。'
- en: '**Blocked Arteries**: This column indicates if a patient has any blocked arteries.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**阻塞动脉**：此列表示患者是否有阻塞的动脉。'
- en: '**Heart Disease**: This column indicates if the patient suffers from heart
    disease.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心脏病**：此列表示患者是否患有心脏病。'
- en: For this scenario, we want to create a decision tree that uses Chest Pain, Good
    Blood Circulation, and Blocked Arteries features to predict whether a patient
    has heart disease. Now, when forming a decision tree, the first thing that we
    need to do is find the root node. So, what feature should we place at the top
    of the decision tree?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个场景，我们想要创建一个决策树，该决策树使用胸痛、良好血液循环和阻塞动脉特征来预测患者是否患有心脏病。现在，在形成决策树时，我们首先需要做的是找到根节点。那么，我们应该将哪个特征放在决策树的顶部？
- en: 'We start by looking at how the Chest Pain feature alone fairs when predicting
    heart disease. We shall go through all the values in the dataset and map them
    to this decision tree while comparing the values in the Chest Pain column with
    those of heart disease. We shall keep track of these relationships in the decision
    tree. The decision tree for Chest Pain as the root node will be as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先来看一下，仅凭“胸痛”特征在预测心脏病方面的表现。我们将遍历数据集中的所有值，并将它们映射到这个决策树上，同时比较“胸痛”列的值与心脏病列的值。我们将在决策树中跟踪这些关系。以“胸痛”为根节点的决策树如下：
- en: '![Figure 5.15 – Decision tree for the Chest Pain feature ](img/B17298_05_014.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 胸痛特征的决策树](img/B17298_05_014.jpg)'
- en: Figure 5.15 – Decision tree for the Chest Pain feature
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 胸痛特征的决策树
- en: Now, we do this for all the other features in the dataset. We create a decision
    tree for Good Blood Circulation and see how it fairs alone when making predictions
    for Heart Disease and keep a track of the comparison, repeating the same process
    for the Blocked Arteries status as well. If there are any missing values in the
    dataset, then we skip them. Ideally, you should not work with datasets that have
    missing values. We can use the techniques we learned about in [*Chapter 3*](B17298_03.xhtml#_idTextAnchor066),
    *Understanding Data Processing*, where we impute and handle missing dataset values.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对数据集中的其他所有特征都进行同样的操作。我们为“良好血液循环”创建一个决策树，并查看它在预测心脏病时的表现，同时记录比较结果，重复同样的过程来跟踪“动脉阻塞”状态。如果数据集中有任何缺失值，则跳过它们。理想情况下，你不应该使用包含缺失值的数据集。我们可以使用我们在[*第
    3 章*](B17298_03.xhtml#_idTextAnchor066)，“理解数据处理”中学到的技术，其中我们插补和处理缺失数据集值。
- en: 'Refer to the following diagram, which shows the two decision trees that were
    created – one for **Patient Has Blocked Arteries** and another for **Patient Has
    Good Blood Circulation**:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表，它显示了创建的两个决策树 – 一个用于**患者有阻塞的动脉**，另一个用于**患者有良好的血液循环**：
- en: '![Figure 5.16 – Decision tree for the Blocked Arteries and Good Blood Circulation
    features ](img/B17298_05_015.jpg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 阻塞性动脉和良好血液循环特征的决策树](img/B17298_05_015.jpg)'
- en: Figure 5.16 – Decision tree for the Blocked Arteries and Good Blood Circulation
    features
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 – 阻塞性动脉和良好血液循环特征的决策树
- en: Now that we have created a decision tree for all the features in the dataset,
    we can compare their results to find the pure feature. In the context of decision
    trees, a feature is said to be 100% impure when a node is split evenly, 50/50,
    and 100% pure when all of its data belongs to a single class. In our scenario,
    we don’t have any feature that is 100% pure. All of our features are impure to
    some degree. So, we need to find some way of finding the feature that is the purest.
    For that, we need a metric that can measure the purity of a decision tree.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为数据集中的所有特征创建了一个决策树，我们可以比较它们的结果以找到纯度最高的特征。在决策树的情况下，如果一个节点被平均分割，即 50/50，则称一个特征为
    100% 不纯，如果所有数据都属于单个类别，则称其为 100% 纯。在我们的场景中，我们没有 100% 纯的特征。所有特征都有一定程度的不纯。因此，我们需要找到一种方法来找到最纯的特征。为此，我们需要一个可以衡量决策树纯度的指标。
- en: There are plenty of ways by which data scientists and engineers measure purity.
    The most common metric to measure impurity in decision trees is **Gini Impurity**.
    Gini Impurity is the measure of the likelihood that a new random instance of data
    will be incorrectly classified during classification.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和工程师有很多种方法来衡量纯度。在决策树中衡量不纯度最常用的指标是**Gini 不纯度**。Gini 不纯度是衡量新随机数据实例在分类过程中被错误分类的可能性的度量。
- en: 'Gini Impurity is calculated as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Gini 不纯度的计算如下：
- en: '![](img/Formula_B17298__05_008.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B17298__05_008](img/Formula_B17298__05_008.png)'
- en: 'Here, p1, p2 , p3 , p4 … are the probabilities of the various classifications
    for Heart Disease. In our scenario, we only have two classifications – either
    a yes or a no. Thus, for our scenario, the measure of impurity is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，p1, p2, p3, p4…是心脏病各种分类的概率。在我们的场景中，我们只有两种分类 – 要或不要。因此，在我们的场景中，不纯度的度量如下：
- en: '![](img/Formula_B17298__05_009.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B17298__05_009](img/Formula_B17298__05_009.png)'
- en: 'So, let’s calculate the Gini Impurity of all the decision trees we just created
    so that we can find the feature that is the purest. Gini Impurity for a decision
    tree with multiple leaf nodes is calculated by calculating the Gini Impurity of
    individual leaf nodes and then calculating the weighted average of all the impurity
    values to get the Gini Impurity of the decision tree as a whole. So, let’s start
    by calculating the Gini Impurity of the left leaf node of the Chest Pain decision
    tree and repeat this for the right leaf node:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们计算我们刚刚创建的所有决策树的 Gini 不纯度，以便我们可以找到最纯的特征。具有多个叶子节点的决策树的 Gini 不纯度是通过计算单个叶子节点的
    Gini 不纯度，然后计算所有不纯度值的加权平均来得到整个决策树的 Gini 不纯度。因此，让我们首先计算胸痛决策树的左叶子节点的 Gini 不纯度，并重复此操作以计算右叶子节点：
- en: '![](img/Formula_B17298__05_010.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.16 – 阻塞性动脉和良好血液循环特征的决策树](img/B17298_05_015.jpg)'
- en: '![](img/Formula_B17298__05_011.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B17298__05_011](img/Formula_B17298__05_011.png)'
- en: 'The reason why we calculate the weighted average of the Gini Impurities is
    because the representation of the data is not equally divided between the two
    branches of the decision tree. The weighted average helps us offset this unequal
    distribution of the data values. Thus, we can calculate the Gini Impurity of the
    whole Chest Pain decision tree as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算 Gini 不纯度的加权平均的原因是因为数据在决策树的两个分支中的表示并不是平均分配的。加权平均帮助我们抵消这种数据值的不均匀分布。因此，我们可以按照以下方式计算整个胸痛决策树的
    Gini 不纯度：
- en: '*Gini Impurity (Chest Pain) = weighted average of the Gini Impurities of the
    leaf nodes*'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gini 不纯度 (胸痛) = 叶子节点的 Gini 不纯度的加权平均*'
- en: '*Gini Impurity (Chest Pain) =*'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*Gini 不纯度 (胸痛) =*'
- en: '*(Total number of data inputs in the left leaf node / total number of rows)
    x Gini Impurity of the left leaf node*'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '*(左叶子节点中的数据输入总数 / 总行数) x 左叶子节点的 Gini 不纯度*'
- en: '*+*'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*+*'
- en: '*(Total number of data inputs in the right leaf node / total number of rows)
    x Gini Impurity of the right leaf node*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*(右叶子节点中的数据输入总数 / 总行数) x 右叶子节点的 Gini 不纯度*'
- en: '*= (144 / (144 + 159)) x 0.395 + (159 / (144 + 159)) x 0.364*'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '*= (144 / (144 + 159)) x 0.395 + (159 / (144 + 159)) x 0.364*'
- en: '*= 0.364*'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '*= 0.364*'
- en: The Gini Impurity of the Chest Pain decision tree is 0.364.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 胸痛决策树的基尼不纯度为 0.364。
- en: 'We repeat this process for all the other feature decision trees as well. We
    should get the following results:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样对其他特征决策树重复这个过程。我们应该得到以下结果：
- en: The Gini Impurity of the Chest Pain decision tree is 0.364
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 胸痛决策树的基尼不纯度为 0.364
- en: The Gini Impurity of the Good Blood Circulation tree is 0.360
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好血液循环树的基尼不纯度为 0.360
- en: The Gini Impurity of the Blocked Arteries tree is 0.381
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阻塞动脉树的基尼不纯度为 0.381
- en: Comparing these values, we can infer that the Gini Impurity of the Good Blood
    Circulation feature has the lowest Gini Impurity, making it the purest feature
    in the dataset. So, we will use it as the root of our decision tree.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这些值，我们可以推断出良好血液循环特征的基尼不纯度最低，使其成为数据集中最纯净的特征。因此，我们将用它作为决策树的根。
- en: Referring to *Figures 5.12* and *5.13*, when we divided the patients by using
    the Good Blood Circulation feature, we were left with an impure distribution of
    the results on the left and right leaf nodes. So, each leaf node had a mix of
    results that showed with and without Heart Disease. Now, we need to figure out
    a way to separate the mix of results from the Good Blood Circulation feature using
    the remaining features – Chest Pain and Blocked Arteries.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 参考*图 5.12*和*图 5.13*，当我们使用良好血液循环特征对患者进行划分时，我们在左右叶节点上留下了一个不纯的结果分布。因此，每个叶节点都有显示有和无心脏病的结果混合。现在，我们需要找出一种方法，使用剩余的特征（胸痛和阻塞动脉）从良好血液循环特征中分离出结果混合。
- en: So, just as how we did previously, we shall use these mixed results and separate
    them using the other features and calculate the Gini Impurity value of those features.
    We shall choose the feature that is the purest and replace it at the given node
    for further classification.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，就像我们之前做的那样，我们将使用这些混合结果，并使用其他特征将它们分开，计算这些特征的基尼不纯度值。我们将选择最纯净的特征，并在给定的节点上进行替换以进行进一步分类。
- en: 'We shall repeat this process for the right branch as well. So, to simplify
    the selection of the decision tree nodes, we must do the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样需要为右分支重复这个过程。为了简化决策树节点的选择，我们必须做以下事情：
- en: Calculate the Gini Impurity score of all the remaining features for that node
    using the mixed results.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混合结果计算该节点所有剩余特征的基尼不纯度得分。
- en: Choose the one with the lowest impurity and replace it with the node.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择不纯度最低的一个，并用它替换节点。
- en: Repeat the same process further down the decision tree with the remaining features.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在决策树的其余部分使用剩余特征重复相同的过程。
- en: Continue replacing the nodes, so long as the classification lowers the Gini
    Impurity; otherwise, leave it as a leaf node.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续替换节点，只要分类降低了基尼不纯度，否则将其保留为叶节点。
- en: 'So, your final decision tree will be as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你的最终决策树将如下所示：
- en: '![Figure 5.17 – The final decision tree ](img/B17298_05_016.jpg)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 最终决策树](img/B17298_05_016.jpg)'
- en: Figure 5.17 – The final decision tree
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 最终决策树
- en: This decision tree is good for classification with true or false values. What
    if you had numerical data instead?
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策树适用于对真假值进行分类。如果你有数值数据会怎样呢？
- en: 'Creating decision trees with numerical data is very easy and has almost the
    same steps as we do for true/false data. Consider Weight as a new feature; the
    data for the **Weight** column is as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数值数据创建决策树非常容易，几乎与我们对真假数据的处理步骤相同。以“重量”作为一个新特征；**重量**列的数据如下：
- en: '![Figure 5.18 – Dataset with a new feature, Weight, in kilograms ](img/B17298_05_017.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – 包含新特征“重量”，单位为千克的样本数据](img/B17298_05_017.jpg)'
- en: Figure 5.18 – Dataset with a new feature, Weight, in kilograms
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – 包含新特征“重量”，单位为千克的样本数据
- en: 'For this scenario, we must follow these steps:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个场景，我们必须遵循以下步骤：
- en: Sort the data in ascending order. In our scenario, we shall sort the rows of
    the dataset with the Weight column from highest to lowest.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按升序排序数据。在我们的场景中，我们将按重量列从高到低对数据集的行进行排序。
- en: 'Calculate the average weights for all the adjacent rows:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有相邻行的平均重量：
- en: '![Figure 5.19 – Calculating the average of the subsequent row values ](img/B17298_05_018.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.19 – 计算后续行值的平均值](img/B17298_05_018.jpg)'
- en: Figure 5.19 – Calculating the average of the subsequent row values
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 – 计算后续行值的平均值
- en: 'Calculate the Gini Impurity of all the averages we calculated:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算我们计算的所有平均值的基尼不纯度：
- en: '![Figure 5.20 – Calculating the Gini Impurity of all the averages ](img/B17298_05_019.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.20 – 计算所有平均值的基尼不纯度](img/B17298_05_019.jpg)'
- en: Figure 5.20 – Calculating the Gini Impurity of all the averages
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 - 计算所有平均值的Gini不纯度
- en: Identify and select the average feature value that gives us the least Gini Impurity
    value.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别并选择给我们带来最小Gini不纯度值的平均特征值。
- en: Use the selected feature value as a decision node in the decision tree.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所选特征值作为决策树中的决策节点。
- en: Making predictions using decision trees is very easy. You will have data with
    values for chest pain, blocked arteries, good circulation, and weight and you
    will feed it to the decision tree model. The model will filter the values down
    the decision tree while calculating the node conditions and eventually arriving
    at the leaf node with the prediction value.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树进行预测非常简单。你将有一组具有胸痛、阻塞的动脉、良好的循环和体重等值的数值数据，并将这些数据输入到决策树模型中。模型将根据节点条件过滤决策树中的值，最终到达具有预测值的叶节点。
- en: Congratulations – you have just understood the concept of decision trees! Despite
    decision trees being easy to understand and implement, they are not that good
    at solving real-life ML problems.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你——你刚刚理解了决策树的概念！尽管决策树易于理解和实现，但它们在解决现实生活中的机器学习问题方面并不出色。
- en: 'There are certain drawbacks to using decision trees:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 使用决策树存在某些缺点：
- en: Decision trees are very unstable. Any minor changes in the dataset can drastically
    alter the performance of the model and prediction results.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树非常不稳定。数据集的任何微小变化都会极大地改变模型的性能和预测结果。
- en: They are inaccurate.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是不准确的。
- en: They can get very complex for large datasets with a large number of features.
    Imagine a dataset with 1,000 features – the decision tree for this dataset will
    have a tree whose depth will be very large and its computation will be very resource-intensive.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有大量特征的大数据集，它们可能会变得非常复杂。想象一下具有1,000个特征的数据库——该数据集的决策树将具有非常深的深度，其计算将非常资源密集。
- en: 'To mitigate all these drawbacks, the Random Forest algorithm was developed,
    which builds on top of decision trees. With this knowledge, let’s move on to the
    next concept: Random Forest.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻所有这些缺点，开发了随机森林算法，该算法建立在决策树之上。有了这些知识，让我们继续下一个概念：随机森林。
- en: Introduction to Random Forest
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机森林简介
- en: '**Random Forest**, also called **Random Decision Forest**, is an ML method
    that builds a large number of decision trees during learning and groups, or ensembles,
    the results of the individual decision trees to make predictions. Random Forest
    is used to solve both classification and regression problems. For classification
    problems, the class value predicted by the majority of the decision trees is the
    predicted value. For regression problems, the mean or average prediction of the
    individual trees is calculated and returned as the prediction value.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '**随机森林**，也称为**随机决策森林**，是一种机器学习方法，在学习过程中构建大量决策树，并将单个决策树的结果分组或集成，以进行预测。随机森林用于解决分类和回归问题。对于分类问题，预测值是多数决策树预测的类别值。对于回归问题，计算单个树的平均或预测值，并将其作为预测值返回。'
- en: 'The Random Forest algorithm follows these steps for learning during training:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法在训练过程中遵循以下步骤进行学习：
- en: Create a bootstrapped dataset from the original dataset.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从原始数据集中创建一个bootstrap数据集。
- en: Randomly select a subset of the data features.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择数据特征的一个子集。
- en: Start creating a decision tree using the selected subset of features, where
    the feature that splits the data the best is chosen as the root node.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所选特征子集开始创建决策树，其中将最佳分割数据的特征选为根节点。
- en: Select a random subset of the other remaining features to further split the
    decision tree.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择其他剩余特征的一个随机子集以进一步分割决策树。
- en: Let’s understand this concept of Random Forest by creating one.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建一个来理解随机森林的概念。
- en: We shall use the same dataset that we used to make our complex decision tree
    in *Figure 5.17*. The dataset is the same one we used to make our decision trees.
    To create a Random Forest, we need to create a bootstrapped version of the dataset.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与我们在*图5.17*中创建复杂决策树相同的同一个数据集。该数据集是我们创建决策树时使用的同一个数据集。要创建随机森林，我们需要创建数据集的bootstrap版本。
- en: A bootstrapped dataset is a dataset that is created from the original dataset
    by randomly selecting rows from the dataset. The bootstrapped dataset is the same
    size as the original dataset and can also contain duplicate rows from the original
    dataset. There are plenty of inbuilt functions for creating a bootstrapped dataset
    and you can use any of them to create one.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 自助采样数据集是从原始数据集中随机选择行创建的数据集。自助采样数据集与原始数据集大小相同，也可以包含原始数据集中的重复行。有许多内置函数可以创建自助采样数据集，您可以使用其中的任何一个来创建一个。
- en: 'Consider the following bootstrapped dataset:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下自助采样数据集：
- en: '![Figure 5.21 – Bootstrapping dataset ](img/B17298_05_020.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图5.21 – 自助采样数据集](img/B17298_05_020.jpg)'
- en: Figure 5.21 – Bootstrapping dataset
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 – 自助采样数据集
- en: The next step is to create a decision tree from the bootstrapped dataset but
    using only a subset of the feature columns at each step. So, selecting all the
    features to be considered for the decision tree only lets you go with Good Blood
    Circulation and Blocked Arteries as features for the decision tree.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是从自助采样数据集中创建决策树，但在每个步骤中只使用特征列的子集。因此，选择所有要考虑的特征仅允许您使用良好的血液循环和阻塞动脉作为决策树的特性。
- en: We shall follow the same purity identification criteria to determine the root
    of the node. Let’s assume that for our experiment, Good Blood Circulation is the
    purest. Setting that as the root node, we shall now consider the remaining features
    to fill the next level of decision nodes. Just like we did previously, we shall
    randomly select two features from the remaining features and decide which feature
    should fit in the next decision node. We will build the tree as usual while considering
    the random subset of remaining variables at each step.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循相同的纯度识别标准来确定节点的根。让我们假设在我们的实验中，良好的血液循环是最纯的。将其设置为根节点后，我们现在将考虑剩余的特征以填充决策节点的下一级。就像我们之前做的那样，我们将从剩余的特征中随机选择两个特征，并决定哪个特征应该适合下一个决策节点。我们将像往常一样构建树，同时考虑每个步骤中剩余变量的随机子集。
- en: 'Here is the tree we just made:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这里就是我们刚刚创建的树：
- en: '![Figure 5.22 – First decision tree from the bootstrapped dataset ](img/B17298_05_021.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图5.22 – 来自自助采样数据集的第一个决策树](img/B17298_05_021.jpg)'
- en: Figure 5.22 – First decision tree from the bootstrapped dataset
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.22 – 来自自助采样数据集的第一个决策树
- en: 'Now, we repeat the same process while creating multiple decision trees and
    bootstrapping and selecting features from random trees. An ideal Random Forest
    will create hundreds of decision trees, as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在创建多个决策树的同时，进行自助采样并从随机树中选择特征。一个理想的随机森林将创建数百个决策树，如下所示：
- en: '![Figure 5.23 – Multiple decision trees from different bootstrapped datasets
    ](img/B17298_05_022.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图5.23 – 来自不同自助采样数据集的多个决策树](img/B17298_05_022.jpg)'
- en: Figure 5.23 – Multiple decision trees from different bootstrapped datasets
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.23 – 来自不同自助采样数据集的多个决策树
- en: This large variety of decision trees that were created with randomized implementation
    is what makes Random Forest more effective than a single decision tree.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通过随机实现创建的决策树的大量多样性使得随机森林比单个决策树更有效。
- en: Now that we have created our Random Forest, let’s see how we can use it to make
    predictions. To make predictions, you will have a row that contains data values
    for the different features and you want to predict whether that person has heart
    disease.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了我们的随机森林，让我们看看我们如何使用它来进行预测。为了进行预测，您将有一行包含不同特征的数据值，并且您想要预测这个人是否有心脏病。
- en: 'You will pass this data down an individual decision tree in the Random Forest:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 您将把这个数据传递给随机森林中的单个决策树：
- en: '![Figure 5.24 – Predictions from the first decision tree in the Random Forest
    ](img/B17298_05_023.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图5.24 – 随机森林中第一个决策树的预测](img/B17298_05_023.jpg)'
- en: Figure 5.24 – Predictions from the first decision tree in the Random Forest
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.24 – 随机森林中第一个决策树的预测
- en: 'The decision tree will predict the results based on its structure. We shall
    keep a track of the prediction made by this tree and continue passing the data
    down to the other trees, noting their predictions as well:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树将根据其结构预测结果。我们将跟踪此树所做的预测，并将数据继续传递给其他树，同时记录它们的预测：
- en: '![Figure 5.25 – Predictions from the other individual trees in the Random Forest
    ](img/B17298_05_024.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图5.25 – 随机森林中其他单个树所做的预测](img/B17298_05_024.jpg)'
- en: Figure 5.25 – Predictions from the other individual trees in the Random Forest
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.25 – 随机森林中其他单个树所做的预测
- en: Once we get predictions from all the individual trees, we can find out which
    value got the most votes from all the decision trees. The prediction value with
    the most votes concludes the prediction for the Random Forest.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们从所有单个树中得到预测，我们可以找出哪个值从所有决策树中获得最多的投票。获得最多投票的预测值即为随机森林的预测结果。
- en: Bootstrapping the dataset and aggregating the prediction values of all the decision
    trees to make a decision is called **bagging**.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集进行重采样并汇总所有决策树的预测值以做出决策的过程称为**袋装法**。
- en: Congratulations – you have just understood the concept of Random Forest! Random
    Forest, despite being a very good ML algorithm with low bias and variance, still
    suffers from high computation requirements. Hence, H2O AutoML, instead of training
    Random Forest, trains an alternate version of Random Forest called **Extremely
    Randomized Trees** (**XRT**).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜——你刚刚理解了随机森林的概念！尽管随机森林是一个非常优秀的ML算法，具有低偏差和方差，但它仍然面临着高计算需求的问题。因此，H2O AutoML不是训练随机森林，而是训练随机森林的另一种版本，称为**极度随机树**（**XRT**）。
- en: Understanding Extremely Randomized Trees
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解极度随机树
- en: 'The **XRT** algorithm, also called **ExtraTrees**, is just like the ordinary
    Random Forest algorithm. However, there are two key differences between Random
    Forest and XRT, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**XRT**算法，也称为**ExtraTrees**，与普通的随机森林算法类似。然而，随机森林和XRT之间有两个关键的区别，如下所示：'
- en: In Random Forest, we use a bootstrapped dataset to train the individual decision
    trees. In XRT, we use the whole dataset to train the individual decision trees.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在随机森林中，我们使用重采样的数据集来训练单个决策树。在XRT中，我们使用整个数据集来训练单个决策树。
- en: In Random Forest, the decision nodes are split based on certain selection criteria
    such as the impurity metric or error rate when building the individual decision
    tree. In XRT, this process is completely randomized and the one with the best
    results is chosen.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在随机森林中，决策节点是根据某些选择标准（如构建单个决策树时的不纯度指标或错误率）进行分裂的。在XRT中，这个过程是完全随机的，选择结果最好的进行选择。
- en: Let’s consider the same example we used to understand Random Forest to understand
    XRT. We have a dataset, as shown in *Figure 5.17*. Instead of bootstrapping the
    data, as we did in *Figure 5.20*, we shall use the dataset as-is.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑之前用来理解随机森林的相同例子，以理解XRT。我们有一个数据集，如图5.17所示。与图5.20中我们进行数据重采样不同，我们将直接使用原始数据集。
- en: Then, we start creating our decision trees by randomly selecting a subset of
    the features. In Random Forest, we used the purity criteria to decide which feature
    should be set as the root node of the decision tree. However, for XRT, we shall
    set the root node as well as the decision nodes of the decision tree randomly.
    Similarly, we shall create multiple decision trees like these with all the features
    randomly selected. This added randomness allows the algorithm to further reduce
    the variance of the model, at the expense of a slight increase in bias.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始通过随机选择特征子集来创建我们的决策树。在随机森林中，我们使用纯度标准来决定哪个特征应被设置为决策树的根节点。然而，对于XRT，我们将随机设置决策树的根节点以及决策节点。同样，我们将创建具有所有特征随机选择的多个类似决策树。这种增加的随机性允许算法进一步减少模型的方差，但代价是略微增加偏差。
- en: Congratulations! We have just investigated how the XRT algorithm uses an extremely
    randomized forest of decision trees to make accurate regressions and classification
    predictions. Now, let’s understand how the GBM algorithm trains a classification
    model to classify data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们刚刚研究了XRT算法如何使用极度随机的决策树来做出准确的回归和分类预测。现在，让我们了解GBM算法如何训练一个分类模型来对数据进行分类。
- en: Understanding the Gradient Boosting Machine algorithm
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解梯度提升机算法
- en: '**Gradient Boosting Machine** (**GBM**) is a forward learning ensemble ML algorithm
    that works on both classification as well as regression. The GBM model is an ensemble
    model just like the DRF algorithm in the sense that the GBM model, as a whole,
    is a combination of multiple weak learner models whose results are aggregated
    and presented as a GBM prediction. GBM works similarly to DRF in that it consists
    of multiple decision trees that are built in a sequence that sequentially minimizes
    the error.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度提升机**（**GBM**）是一种前向学习集成ML算法，它适用于分类和回归。GBM模型与DRF算法类似，是一个集成模型，因为GBM模型整体上是多个弱学习器模型的组合，其结果被汇总并作为GBM预测呈现。GBM的工作方式与DRF类似，它由多个决策树组成，这些树按顺序构建，以逐步最小化误差。'
- en: GBM can be used to predict continuous numerical values, as well as to classify
    data. If GBM is used to predict continuous numerical values, we say that we are
    using GBM for regression. If we are using GBM to classify data, then we say we
    are using GBM for classification.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: GBM可以用来预测连续数值，也可以用来分类数据。如果我们使用GBM来预测连续数值，我们说我们在使用GBM进行回归。如果我们使用GBM来分类数据，那么我们说我们在使用GBM进行分类。
- en: The GBM algorithm has a foundation on decision trees, just like DRF. However,
    how the decision trees are built is different compared to DRF.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: GBM算法与DRF一样，建立在决策树的基础上。然而，与DRF相比，决策树的构建方式不同。
- en: Let’s try to understand how the GBM algorithm works for regression.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试理解GBM算法在回归中的工作原理。
- en: Building a Gradient Boosting Machine
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建梯度提升机
- en: 'We shall use the following sample dataset and understand how GBM works as we
    conceptually build the model. The following table contains a sample of the dataset:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下样本数据集，并在概念上构建模型的同时理解GBM的工作原理。以下表格包含数据集的一个样本：
- en: '![Figure 5.26 – Sample dataset for GBM ](img/B17298_05_025.jpg)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图5.26 – GBM的样本数据集](img/B17298_05_025.jpg)'
- en: Figure 5.26 – Sample dataset for GBM
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.26 – GBM的样本数据集
- en: 'This is an arbitrary dataset that we are using just for the sake of understanding
    how GBM will build its ML model. The contents of the dataset are as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个我们仅为了理解GBM如何构建其机器学习模型而使用的任意数据集。数据集的内容如下：
- en: '**Height**: This column indicates the height of the person in centimeters.'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**身高**：这一列表示人的身高（厘米）。'
- en: '**Gender**: This column indicates the gender of the person.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**：这一列表示人的性别。'
- en: '**Age**: This column indicates the age of the person.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**年龄**：这一列表示人的年龄。'
- en: '**Weight**: This column indicates the weight of the person.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：这一列表示人的权重。'
- en: 'GBM, unlike DRF, starts creating its weak learner decision trees from leaf
    nodes instead of root nodes. The very first leaf node that it will create will
    be the average of all the values of the response variable. So, accordingly, the
    GBM algorithm will create the leaf node, as shown in the following diagram:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 与DRF不同，GBM从叶节点而不是根节点开始创建其弱学习决策树。它将创建的第一个叶节点将是响应变量所有值的平均值。因此，相应地，GBM算法将创建叶节点，如下面的图表所示：
- en: '![Figure 5.27 – Calculating the leaf node using the column average ](img/B17298_05_026.jpg)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图5.27 – 使用列平均值计算叶节点](img/B17298_05_026.jpg)'
- en: Figure 5.27 – Calculating the leaf node using the column average
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.27 – 使用列平均值计算叶节点
- en: This leaf node alone can also be considered a decision tree. It acts like a
    prediction model that only predicts a constant value for any kind of input data.
    In this case, it’s the average value that we get from the response column. This
    is, as we expect, an incorrect way of making any predictions, but it is just the
    first step for GBM.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这个叶节点本身也可以被视为一个决策树。它像一个预测模型，只为任何类型的输入数据预测一个常数值。在这种情况下，它是我们从响应列得到的平均值。正如我们所预期的那样，这是一种错误的预测方式，但它只是GBM的第一步。
- en: The next thing GBM will do is create another decision tree based on the errors
    it observed from its initial leaf node predictions on the dataset. An error, as
    we discussed previously, is nothing but the difference between the observed weight
    and the predicted weight and is also called the residual. However, these residuals
    are different from the actual residuals that we will get from the complete GBM
    model. The residuals that we get from the weak learner decision trees of GBM are
    called **pseudo-residuals**, while those of the GBM model are the actual residuals.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: GBM接下来要做的事情是创建另一个基于它在数据集上对初始叶节点预测观察到的错误的决策树。正如我们之前讨论的，错误不过是观察到的权重与预测的权重之间的差异，也称为残差。然而，这些残差与我们最终从完整的GBM模型中得到的实际残差不同。我们从GBM的弱学习决策树得到的残差称为**伪残差**，而GBM模型的残差是实际残差。
- en: So, as mentioned previously, the GBM algorithm will calculate the pseudo-residuals
    of the first leaf node for all the data values in the dataset and create a special
    column that keeps track of these pseudo-residual values.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，正如之前提到的，GBM算法将为数据集中的所有数据值计算第一个叶节点的伪残差，并创建一个特殊列来跟踪这些伪残差值。
- en: 'Refer to the following diagram for a better understanding:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 参考以下图表以获得更好的理解：
- en: '![Figure 5.28 – Dataset with pseudo-residuals ](img/B17298_05_027.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图5.28 – 带有伪残差的数据集](img/B17298_05_027.jpg)'
- en: Figure 5.28 – Dataset with pseudo-residuals
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.28 – 带有伪残差的数据集
- en: 'Using these pseudo-residual values, the GBM algorithm then builds a decision
    tree using all the remaining features – that is, Height, Favorite Color, and Gender.
    The decision tree will look as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些伪残差值，GBM 算法然后使用所有剩余的特征（即身高、最喜欢的颜色和性别）构建一个决策树。决策树将如下所示：
- en: '![Figure 5.29 – Decision tree using pseudo-residual values ](img/B17298_05_028.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.29 – 使用伪残差值的决策树](img/B17298_05_028.jpg)'
- en: Figure 5.29 – Decision tree using pseudo-residual values
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.29 – 使用伪残差值的决策树
- en: As you can see, this decision tree only has four leaf nodes, while the pseudo-residual
    values in the algorithm generated from the first tree are way more than four.
    This is because the GBM algorithm restricts the size of the decision trees it
    makes. For this scenario, we are only using four leaf nodes. Data scientists can
    control the size of the trees by passing the right hyperparameters when configuring
    the GBM algorithm. Ideally, for large datasets, you often use 8 to 32 leaf nodes.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个决策树只有四个叶节点，而算法从第一个树生成的伪残差值远远超过四个。这是因为 GBM 算法限制了它所制作的决策树的大小。对于这种情况，我们只使用四个叶节点。数据科学家可以通过在配置
    GBM 算法时传递正确的超参数来控制树的大小。理想情况下，对于大型数据集，您通常使用 8 到 32 个叶节点。
- en: 'Due to the restriction of the leaf nodes in the decision trees, the decision
    tree ends up with multiple pseudo-residual values in the same leaf nodes. So,
    the GBM algorithm replaces them with their average to get one concrete number
    for a single leaf node. Accordingly, after calculating the averages, we will end
    up with a decision tree that looks as follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树中叶节点的限制，决策树最终在同一个叶节点中有多个伪残差值。因此，GBM 算法用它们的平均值替换它们，以得到一个单一叶节点的具体数字。相应地，在计算平均值后，我们将得到如下所示的决策树：
- en: '![Figure 5.30 – Decision tree using averaged pseudo-residual values ](img/B17298_05_029.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.30 – 使用平均伪残差值的决策树](img/B17298_05_029.jpg)'
- en: Figure 5.30 – Decision tree using averaged pseudo-residual values
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.30 – 使用平均伪残差值的决策树
- en: Now, the algorithm combines the original leaf node with this new decision tree
    to make predictions on it. So, now, we have a value of 71.2 from the initial leaf
    node prediction. Then, after running the data down the decision tree, we get 16.8\.
    So, the predicted weight is the summation of both the predictions, which is 88\.
    This is also the observed weight.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，算法将原始叶节点与这个新的决策树结合起来，对其做出预测。因此，现在，我们从初始叶节点预测中得到一个值为 71.2。然后，在将数据运行通过决策树后，我们得到
    16.8。所以，预测的权重是这两个预测的总和，即 88。这也是观察到的权重。
- en: This is not correct as this is a case of overfitting. **Overfitting** is a modeling
    error where the model function is too fine-tuned to predict only the data values
    available in the dataset and not any other values outside the dataset. As a result,
    the model becomes useless for predicting any values that fall outside of the dataset.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这是不正确的，因为这是一个过拟合的例子。**过拟合**是一种建模错误，其中模型函数过于精细地调整，只能预测数据集中可用的数据值，而不能预测数据集外的任何其他值。结果，模型对于预测数据集外的任何值都变得无用。
- en: 'So, to correct this, the GBM algorithm assigns a learning rate to all the weak
    learner decision trees that it trains. The **learning rate** is a hyperparameter
    that tunes the rate at which the model learns new information that can override
    the old information. The value of the learning rate ranges from 0 to 1\. By adding
    this learning rate to the predictions from the decision trees, the algorithm controls
    the influence of the decision tree’s predictions and slowly moves toward minimizing
    the error step by step. For our example, let’s assume that the learning rate is
    0.1\. So, accordingly, the predicted weight can be calculated as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了纠正这一点，GBM 算法为其训练的所有弱学习决策树分配一个学习率。**学习率**是一个超参数，它调整模型学习新信息（可以覆盖旧信息）的速度。学习率的值范围从
    0 到 1。通过将这个学习率添加到决策树的预测中，算法控制决策树预测的影响，并逐步缓慢地朝着最小化误差的方向移动。在我们的例子中，假设学习率为 0.1。因此，相应地，预测的权重可以计算如下：
- en: '![Figure 5.31 – Calculating the predicted weight ](img/B17298_05_030.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.31 – 计算预测权重](img/B17298_05_030.jpg)'
- en: Figure 5.31 – Calculating the predicted weight
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.31 – 计算预测权重
- en: Thus, the algorithm will plug in the learning rate for the predictions made
    by the decision tree and then calculate the predicted weight. Now, the predicted
    weight will be *62.1 + (0.1 x -14.2) = 60.68*.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，算法将为决策树做出的预测插入学习率，然后计算预测权重。现在，预测权重将是*62.1 + (0.1 x -14.2) = 60.68*。
- en: 60.68 is not a very good prediction but it is still a better prediction than
    62.68, which is what the initial leaf node predicted. The incremental steps to
    minimize the errors are the right way to maintain low variance in predictions.
    A correct balance of learning rate is also important as too high a learning rate
    will offshoot the correction in the opposite direction, while too low a learning
    rate will lead to long computation time as the algorithm will take very small
    correction steps to reach the minimum error.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 60.68不是一个很好的预测，但它仍然比初始叶节点预测的62.68要好。最小化误差的增量步骤是保持预测低方差的方法。正确的学习率平衡也同样重要，因为过高的学习率会导致校正方向相反，而过低的学习率会导致计算时间过长，因为算法将采取非常小的校正步骤以达到最小误差。
- en: To further correct the prediction value and minimize the error, the GBM algorithm
    will create another decision tree. For this, it will calculate the new pseudo-residual
    values from predictions made with the leaf node and the first decision tree and
    use these values to build the second decision tree.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步校正预测值并最小化误差，GBM算法将创建另一棵决策树。为此，它将从叶节点和第一棵决策树做出的预测中计算新的伪残差值，并使用这些值来构建第二棵决策树。
- en: 'The following diagram shows how new pseudo-residual values are calculated:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了如何计算新的伪残差值：
- en: '![Figure 5.32 – Calculating new pseudo-residual values ](img/B17298_05_031.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![图5.32 – 计算新的伪残差值](img/B17298_05_031.jpg)'
- en: Figure 5.32 – Calculating new pseudo-residual values
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.32 – 计算新的伪残差值
- en: You will notice that the new pseudo-residual values that were generated are
    a lot closer to the actual values compared to the first pseudo-residual values.
    This indicates that the GBM model is slowly minimizing errors and improving its
    accuracy.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，生成的新伪残差值与第一组伪残差值相比，更接近实际值。这表明GBM模型正在逐渐减少误差并提高其准确性。
- en: Moving on with the second decision tree, the algorithm uses the new pseudo residual
    values to create the second decision tree. Once created, it aggregates the tree,
    along with the learning rate, to the already existing leaf node and the first
    decision tree.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用第二棵决策树，算法使用新的伪残差值来创建第二棵决策树。一旦创建，它将树和学习率聚合到已经存在的叶节点和第一棵决策树中。
- en: 'The decision trees can be different each time the GBM algorithm creates one.
    However, the learning rate stays common for all the trees. So, now, the prediction
    values will be the summation of the three components – the initial leaf node prediction
    value, the scaled value of the first decision tree prediction, and the scaled
    value of the second decision tree prediction. So, the prediction values will be
    as follows:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树每次GBM算法创建时可能都不同。然而，学习率对所有树都是通用的。因此，现在预测值将是三个组成部分的总和——初始叶节点预测值、第一棵决策树预测值的缩放值和第二棵决策树预测值的缩放值。因此，预测值如下所示：
- en: '![Figure 5.33 – GBM model with a second boosted decision tree ](img/B17298_05_032.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![图5.33 – 带有第二棵提升决策树的GBM模型](img/B17298_05_032.jpg)'
- en: Figure 5.33 – GBM model with a second boosted decision tree
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.33 – 带有第二棵提升决策树的GBM模型
- en: 'The GBM algorithm will repeat the same process, creating decision trees up
    to the specified number of trees or until adding decision trees stops improving
    the predictions. So, eventually, the GBM model will look as follows:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: GBM算法将重复相同的过程，创建决策树，直到达到指定的树的数量或直到添加决策树不再提高预测为止。因此，GBM模型最终将如下所示：
- en: '![Figure 5.34 – Complete GBM model ](img/B17298_05_033.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图5.34 – 完整的GBM模型](img/B17298_05_033.jpg)'
- en: Figure 5.34 – Complete GBM model
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.34 – 完整的GBM模型
- en: Congratulations! we have just explored how the GBM algorithm uses an ensemble
    of weak decision tree learners to make accurate regressions predictions.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！我们刚刚探讨了GBM算法如何使用一系列弱决策树学习器来做出准确的回归预测。
- en: Another algorithm that H2O AutoML uses, which builds on top of GBM, is the XGBoost
    algorithm. XGBoost stands for Extreme Gradient Boosting and implements a process
    called boosting that sometimes helps in training better-performing models. It
    is one of the most widely used ML algorithms in Kaggle competitions and has proven
    to be an amazing ML algorithm that can be used for both classification and regression.
    The mathematics behind how XGBoost works can be slightly difficult for users not
    well versed with statistics. However, it is highly recommended that you take the
    time and learn more about this algorithm. You can find more information about
    how H2O performs XGBoost training at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: H2O AutoML使用的另一种算法，它建立在GBM之上，是XGBoost算法。XGBoost代表极端梯度提升（Extreme Gradient Boosting），实现了一种称为提升（boosting）的过程，有时有助于训练性能更好的模型。它是Kaggle竞赛中最广泛使用的机器学习算法之一，并且已被证明是一种出色的机器学习算法，可用于分类和回归。XGBoost背后的数学对于不熟悉统计学的用户来说可能有些困难。然而，强烈建议您花时间学习更多关于这个算法的知识。您可以在[https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/xgboost.xhtml)找到有关H2O如何执行XGBoost训练的更多信息。
- en: Tip
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Ensemble ML is a method of combining multiple ML models to obtain better prediction
    results compared to the performance of the models individually – just like how
    a combination of decision trees creates the Random Forest algorithm using bagging
    and how the GBM algorithm uses a combination of weak learners to minimize errors.
    Ensemble models take things one step further by finding the best combinations
    of prediction algorithms and using their combined performance to train a meta-learner
    that provides improved performance. This is done using a process called stacking.
    You can find more information about how H2O trains these stacked ensemble models
    at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 集成机器学习（Ensemble ML）是一种将多个机器学习模型组合起来以获得比单个模型性能更好的预测结果的方法——就像决策树组合成随机森林算法一样，使用袋装法，以及GBM算法使用弱学习者的组合来最小化错误。集成模型通过找到最佳的预测算法组合，并使用它们的组合性能来训练一个提供改进性能的元学习器，从而更进一步。这是通过称为堆叠的过程来完成的。您可以在[https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/stacked-ensembles.xhtml#stacked-ensembles)找到有关H2O如何训练这些堆叠集成模型的更多信息。
- en: Now, let’s learn how deep learning works and understand neural networks.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们学习深度学习的工作原理，并理解神经网络。
- en: Understanding what is Deep Learning
  id: totrans-320
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解什么是深度学习
- en: '**Deep Learning** (**DL**) is a branch of ML that develops prediction models
    using **Artificial Neural Networks** (**ANNs**). ANNs, simply called **Neural
    Networks** (**NNs**), are computations that are loosely based on how human brains
    with neurons work to process information. ANNs consist of neurons, which are types
    of nodes that are interconnected with other neurons. These neurons transmit information
    among themselves; this gets processed down the NN to eventually arrive at a result.'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度学习**（**DL**）是机器学习的一个分支，它使用**人工神经网络**（**ANNs**）来开发预测模型。人工神经网络（ANNs），简称为**神经网络**（**NNs**），是基于人类大脑中神经元如何处理信息而松散构建的计算。人工神经网络由神经元组成，这些神经元是相互连接的其他神经元类型的节点。这些神经元在彼此之间传输信息；这些信息在神经网络中处理，最终得到一个结果。'
- en: DL is one of the most powerful ML techniques and is used to train models that
    are highly configurable and can support predictions for large and complicated
    datasets. DL models can be supervised, semi-supervised, or unsupervised, depending
    on their configuration.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是最强大的机器学习（ML）技术之一，用于训练高度可配置的模型，这些模型可以支持对大型和复杂数据集的预测。深度学习模型可以是监督的、半监督的或无监督的，这取决于它们的配置。
- en: 'There are various types of ANNs:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs有多种类型：
- en: '**Recurrent Neural Network** (**RNN**): RNN is a type of NN where the connections
    between the various neurons of the NN can form a directed or undirected graph.
    This type of network is cyclic since the outputs of the network are fed back to
    the start of the network and contribute to the next cycle of predictions. The
    following diagram shows an example of an RNN:'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）：RNN是一种神经网络，其中神经网络中各个神经元之间的连接可以形成一个有向或无向图。这种类型的网络是循环的，因为网络的输出被送回到网络的起点，并有助于下一个预测周期的下一个循环。以下图显示了RNN的一个示例：'
- en: '![Figure 5.35 – RNN ](img/B17298_05_034.jpg)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![图5.35 – RNN](img/B17298_05_034.jpg)'
- en: Figure 5.35 – RNN
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.35 – RNN
- en: As you can see, the values from the last nodes in the NN are fed to the starting
    nodes of the network as inputs.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，神经网络中最后节点的值被作为输入馈送到网络的起始节点。
- en: '**Feedforward NN**: A feedforward neural network is similar to an RNN, with
    the only difference being that the network of nodes does not form a cycle. The
    following diagram shows an example of a feedforward neural network:'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前馈神经网络**：前馈神经网络与RNN相似，唯一的区别是节点的网络不形成一个循环。以下图表显示了一个前馈神经网络的示例：'
- en: '![Figure 5.36 – Feedforward neural network ](img/B17298_05_035.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图5.36 – 前馈神经网络](img/B17298_05_035.jpg)'
- en: Figure 5.36 – Feedforward neural network
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.36 – 前馈神经网络
- en: As you can see, this type of NN is unidirectional. This is the simplest type
    of ANN. A feedforward neural network is also called **Deep Neural Network** (**DNN**).
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这种类型的神经网络是单向的。这是最简单的ANN类型。前馈神经网络也称为**深度神经网络**（**DNN**）。
- en: 'H2O’s DL is based on a multi-layer free forward ANN. It is trained on **stochastic
    gradient descent** using **backpropagation**. There are plenty of different types
    of DNNs that H2O can train. They are as follows:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: H2O的深度学习（DL）基于多层前馈神经网络。它使用**随机梯度下降**和**反向传播**进行训练。H2O可以训练多种不同类型的DNN。具体如下：
- en: '**Multi-Layer Perceptron** (**MLP**): These types of DNNs are best suited for
    tabular data.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）：这类DNN最适合表格数据。'
- en: '**Convolutional Neural Networks** (**CNNs**): These types of DNNs are best
    suited for image data.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNN**）：这类DNN最适合图像数据。'
- en: '**Recurrent Neural Networks** (**RNNs**): These types of DNNs are best suited
    for sequential data such as voice data or time series data.'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNN**）：这类DNN最适合序列数据，如语音数据或时间序列数据。'
- en: It is recommended to use the default DNNs that H2O provides out of the box as
    configuring a DNN can be very difficult for non-experts. H2O has already preconfigured
    its implementation of DL to use the best type of DNNs for the given cases.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 建议使用H2O提供的默认DNN，因为对于非专家来说，配置DNN可能非常困难。H2O已经预先配置了其深度学习实现，以使用针对给定情况的最佳类型的DNN。
- en: With these basics in mind, let’s dive deeper into understanding how DL works.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解这些基础知识后，让我们更深入地了解深度学习是如何工作的。
- en: Understanding neural networks
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解神经网络
- en: '**NNs** form the basis of DL. The workings of a NN are easy to understand:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: '**NN**是深度学习的基础。神经网络的工作原理易于理解：'
- en: You feed the data into the input layer of the NN.
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您将数据输入到神经网络的输入层。
- en: The nodes in the NN train themselves to recognize patterns and behaviors from
    the input data.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络中的节点通过自身训练来识别输入数据中的模式和行。
- en: The NN then makes predictions based on the patterns and behaviors it learns
    during training.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 神经网络根据在训练期间学习的模式和行进行预测。
- en: 'The structure of an NN looks as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的架构如下：
- en: '![Figure 5.37 – Structure of an NN ](img/B17298_05_036.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![图5.37 – 神经网络的架构](img/B17298_05_036.jpg)'
- en: Figure 5.37 – Structure of an NN
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.37 – 神经网络的架构
- en: 'There are three essential components of an NN:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络（NN）有三个基本组成部分：
- en: '**The input layer**: The input layer consists of multiple sets of neurons.
    These neurons are connected to the next layer of neurons, which reside in the
    hidden layer.'
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层**：输入层由多个神经元集组成。这些神经元连接到下一层的神经元，这些神经元位于隐藏层。'
- en: '**The hidden layer**: Within the hidden layer, there can be multiple layers
    of neurons, all of which are interconnected layer by layer.'
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**隐藏层**：在隐藏层中，可以有多个神经元层，这些神经元层逐层相互连接。'
- en: '**The output layer**: The output layer is the final layer in the NN that makes
    the final calculations to compute the final prediction values in terms of probability.'
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出层**：输出层是神经网络中的最后一层，它进行最终计算以计算最终预测值（以概率形式）。'
- en: 'The learning process of the NN can be broken down into two components:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习过程可以分为两个部分：
- en: '**Forward propagation**: As the name suggests, forward propagation is where
    the information flows from the input layer to the output layer through the middle
    layer. The following diagram shows forward propagation in action:'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向传播**：正如其名所示，前向传播是信息从输入层通过中间层流向输出层的过程。以下图表显示了前向传播的实际操作：'
- en: '![Figure 5.38 – Forward propagation in an NN ](img/B17298_05_037.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图5.38 – 神经网络中的前向传播](img/B17298_05_037.jpg)'
- en: Figure 5.38 – Forward propagation in an NN
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.38 – 神经网络中的前向传播
- en: The neurons within the middle layer are connected via **channels**. These channels
    are assigned numerical values called **weights**. Weights determine how important
    the neuron is in terms of its value contributing to the overall prediction. The
    higher the value of the weight, the more important that node is when making predictions.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 中间层中的神经元通过**通道**连接。这些通道被分配了称为**权重**的数值。权重决定了神经元在贡献整体预测中的重要性。权重值越高，该节点在预测时的重要性就越大。
- en: The input values from the input layer are multiplied by these weights as they
    pass through the channels and their sum is sent as inputs to the neurons in the
    hidden layer. Each neuron in the hidden layer is associated with a numerical value
    called a **bias,** which is added to the input sum.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层的输入值在通过通道时乘以这些权重，它们的总和作为输入发送到隐藏层中的神经元。隐藏层中的每个神经元都与一个称为**偏置**的数值相关联，该数值被添加到输入总和。
- en: This weighted value is then passed to a non-linear function called the activation
    function. The activation function is a function that decides if the particular
    neuron can pass its calculated weight value onto the next layer of the neuron
    or not, depending on the equation of the non-linear function. The bias is a scalar
    value that shifts the activation function either to the left or right of the graph
    for corrections.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 这个加权值随后传递到一个称为激活函数的非线性函数。激活函数是一个函数，它根据非线性函数的方程决定特定的神经元是否可以将计算出的权重值传递到下一层的神经元。偏置是一个标量值，它将激活函数向左或向右移动以进行校正。
- en: This flow of information continues to the next layer of neurons in the hidden
    layer, following the same process of multiplying the weight of the channels and
    passing the input to the next activation function of the node.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 这种信息流继续传递到隐藏层的下一层神经元，遵循相同的乘以通道权重并将输入传递到节点的下一个激活函数的过程。
- en: Finally, in the output layer, the neuron with the highest value determines what
    the prediction value is, which is a form of probability.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在输出层，具有最高值的神经元决定了预测值，这是一种概率的形式。
- en: '**Backpropagation**: Backpropagation works the same way as forward propagation
    except that it works in the reverse direction. Information is passed from the
    output layer to the input layer through the hidden layer in a reverse manner.
    The following diagram will give you a better understanding of this:'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**：反向传播的工作方式与正向传播相同，只是它的工作方向相反。信息以相反的方式从输出层通过隐藏层传递到输入层。以下图表将更好地帮助您理解这一点：'
- en: '![Figure 5.39 – Backpropagation in NN ](img/B17298_05_038.jpg)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![图5.39 – 神经网络中的反向传播](img/B17298_05_038.jpg)'
- en: Figure 5.39 – Backpropagation in NN
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.39 – 神经网络中的反向传播
- en: It may be counterintuitive to understand how backpropagation can work since
    it works in a reverse manner from the output to the input, but it is a concept
    that makes DL so powerful for ML. It is through backpropagation that NNs can learn
    by themselves.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 由于反向传播从输出到输入以相反的方式工作，理解其工作原理可能有些反直觉，但这是一个使深度学习对机器学习如此强大的概念。正是通过反向传播，神经网络能够自主学习。
- en: The way it does this is pretty simple. In backpropagation learning, the NN will
    calculate the magnitude of error between the expected value and the predicted
    value and evaluate its performance by mapping it onto a **loss function**. The
    loss function is a function that calculates the deviance between the predicted
    and expected values. This deviation value is the information that helps the NN
    adjust its biases and weights in the hidden layer to improve its performance and
    make better predictions.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 它这样做的方式相当简单。在反向传播学习中，神经网络将计算预期值和预测值之间的误差幅度，并通过将其映射到一个**损失函数**来评估其性能。损失函数是一个计算预测值和预期值之间偏差的函数。这个偏差值是帮助神经网络调整其隐藏层中的偏置和权重以改进性能和做出更好预测的信息。
- en: Congratulations – we have just gotten a basic glimpse of how DL trains ML models!
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您 – 我们刚刚对深度学习如何训练机器学习模型有了基本的了解！
- en: Tip
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: DL is one of the most sophisticated fields in ML, as well as AI as a whole.
    It’s a vast area of ML specialization where data scientists spend a lot of time
    researching and understanding the problem statement and the data they are working
    with so that they can correctly tune their DL NN. The mathematics behind it is
    also very complex and as such deserves a dedicated book. So, if you are interested
    in mathematics and want to excel in the art of DL, feel free to explore the ML
    algorithm in depth as every step in understanding it will make you that much more
    of an expert ML engineer. You can find more information about how H2O performs
    DL training at [https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是机器学习中最复杂的领域之一，也是人工智能作为一个整体。这是一个庞大的机器学习专业化领域，数据科学家在这里花费大量时间研究并理解他们正在处理的问题陈述和数据，以便他们可以正确调整他们的深度学习神经网络（NN）。其背后的数学也非常复杂，因此值得有一本专门的书籍。所以，如果你对数学感兴趣并想在深度学习艺术上脱颖而出，请自由地深入研究机器学习算法，因为理解它的每一步都会使你成为一个更加专业的机器学习工程师。你可以在[https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml](https://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/deep-learning.xhtml)找到更多关于H2O如何执行深度学习训练的信息。
- en: Summary
  id: totrans-367
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we understood the different types of prediction problems and
    how various algorithms aim to solve them. Then, we understood how the different
    ML algorithms are categorized into supervised, unsupervised, semi-supervised,
    and reinforcement based on their method of learning from data. Once we had an
    understanding of the overall problem domain of ML, we understood that H2O AutoML
    trains only supervised learning ML algorithms and can solve prediction problems
    in this domain specifically.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们了解了不同类型的预测问题以及各种算法如何试图解决这些问题。然后，我们了解了不同的机器学习算法是如何根据它们从数据中学习的方法被分类为监督学习、无监督学习、半监督学习和基于强化学习的。一旦我们对机器学习的整体问题领域有了理解，我们就明白H2O
    AutoML只训练监督学习的机器学习算法，并且可以专门解决这个领域的预测问题。
- en: Then, we understood which algorithms H2O AutoML trains starting with GLM. To
    understand GLM, we understood what linear regression is and how it works and what
    assumptions about the normal distribution of data it has to make to be effective.
    With these basics in mind, we understood how GLM is generalized to be effective,
    even if these assumptions of linear regression are met, which is a common case
    in real life.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们了解了H2O AutoML从GLM开始训练的算法。为了理解GLM，我们了解了线性回归是什么以及它是如何工作的，以及它必须对数据的正态分布做出哪些假设才能有效。带着这些基础知识，我们理解了GLM是如何被推广以有效性的，即使这些线性回归的假设得到满足，这在现实生活中是一个常见的情况。
- en: Then, we learned about DRF. To understand DRF, we understood what decision trees
    are – that is, the basic building blocks of DRF. Then, we learned that multiple
    decision trees with their ensembled learnings are better ML models than a normal
    decision tree – that is how Random Forest works. Building on top of this, we learned
    how DRF adds more randomization in the form of XRT to make the algorithm all the
    more effective with low variance and bias.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们学习了随机森林（DRF）。为了理解DRF，我们了解了决策树是什么——即DRF的基本构建块。然后，我们了解到，具有集成学习的多个决策树比普通的决策树是更好的机器学习模型——这就是随机森林的工作原理。在此基础上，我们学习了DRF如何通过XRT的形式增加更多的随机化，使算法在低方差和偏差的情况下更加有效。
- en: After that, we learned about GBM. We learned how GBM is similar to DRFs but
    that it has a slightly different way of learning. We understood how GBM sequentially
    builds decision trees and slowly minimizes error by learning from its residuals
    from previous decision tree prediction aggregates.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们学习了梯度提升机（GBM）。我们了解到GBM与DRF相似，但它有稍微不同的学习方法。我们理解了GBM是如何顺序地构建决策树，并通过从先前决策树预测的残差中学习来逐渐最小化错误的。
- en: Finally, we learned what DL is. We understood how NNs are the building blocks
    of DL and their different types. We also understood how NNs perform backpropagation
    learning from its results and self-learn and improve the model by adjusting the
    weights and biases of the neurons in the middle layer.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们学习了什么是深度学习。我们理解了神经网络是深度学习的基础构建块以及它们的类型。我们还理解了神经网络如何通过其结果进行反向传播学习，并通过调整中间层神经元的权重和偏差来自我学习和改进模型。
- en: This chapter gave you a brief conceptual understanding of how the various ML
    algorithms are trained by H2O AutoML without diving too deep into the mathematics.
    However, ML enthusiasts who want to become experts in the field of ML and wish
    to work on complex ML problems are strongly encouraged to understand the math
    behind the wonderful world of ML algorithms. It is the culmination of years of
    research and effort by scientists and enthusiasts such as yourselves that we have
    the capability today to potentially predict the future with the help of machines.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为您提供了对H2O AutoML如何训练各种机器学习算法的简要概念性理解，而没有深入到数学层面。然而，对于那些想要成为机器学习领域专家并希望解决复杂机器学习问题的机器学习爱好者来说，强烈建议您理解机器学习算法奇妙世界背后的数学原理。正是由于科学家和像您这样的爱好者多年的研究和努力，我们今天才有可能在机器的帮助下预测未来。
- en: In the next chapter, we shall dive deep into understanding how you can understand
    if an ML model is performing optimally or not using different statistical measurements
    and other metrics that explain more about ML model performance.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将深入探讨如何使用不同的统计测量和其他指标来理解机器学习模型是否表现最优。这些指标能更详细地解释机器学习模型的表现。
