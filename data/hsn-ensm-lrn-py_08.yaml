- en: Bagging
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 装袋法（Bagging）
- en: Bagging, or bootstrap aggregating, is the first generative ensemble learning
    technique that this book will present. It can be a useful tool to reduce variance
    as it creates a number of base learners by sub-sampling the original train set.
    In this chapter, we will discuss the statistical method on which bagging is based,
    bootstrapping. Next, we will present bagging, along with its strengths and weaknesses.
    Finally, we will implement the method in Python, as well as use the scikit-learn
    implementation, to solve regression and classification problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 装袋法，或称为自助聚合（Bootstrap Aggregating），是本书介绍的第一个生成性集成学习技术。它可以作为减少方差的有用工具，通过对原始训练集进行子抽样来创建多个基础学习器。在本章中，我们将讨论装袋法所基于的统计方法——自助法。接下来，我们将介绍装袋法的优缺点，并最终用
    Python 实现该方法，同时使用 scikit-learn 实现解决回归和分类问题。
- en: 'The main topics covered in this chapter are as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主要内容如下：
- en: The bootstrapping method from computational statistics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算统计学中的自助法方法
- en: How bagging works
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 装袋法的工作原理
- en: Strengths and weaknesses of bagging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 装袋法的优缺点
- en: Implementing a custom bagging ensemble
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现自定义的装袋集成方法
- en: Using the scikit-learn implementation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 实现
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will require basic knowledge of machine learning techniques and algorithms.
    Furthermore, a knowledge of python conventions and syntax is required. Finally,
    familiarity with the NumPy library will greatly help the reader to understand
    some custom algorithm implementations.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要具备基本的机器学习技术和算法知识。此外，还需要了解 Python 的规范和语法。最后，熟悉 NumPy 库将极大地帮助读者理解一些自定义算法实现。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可以在 GitHub 上找到：
- en: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter05)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter05](https://github.com/PacktPublishing/Hands-On-Ensemble-Learning-with-Python/tree/master/Chapter05)'
- en: Check out the following video to see the Code in Action: [http://bit.ly/2JKcokD](http://bit.ly/2JKcokD).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，查看代码的实际操作：[http://bit.ly/2JKcokD](http://bit.ly/2JKcokD)。
- en: Bootstrapping
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自助法（Bootstrapping）
- en: Bootstrapping is a resampling method. In statistics, resampling entails the
    use of many samples, generated from an original sample. In machine learning terms,
    the sample is our training data. The main idea is to use the original sample as
    the population (the whole domain of our problem) and the generated sub-samples
    as samples.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法是一种重抽样方法。在统计学中，重抽样是指使用从原始样本生成的多个样本。在机器学习术语中，样本即为我们的训练数据。其主要思想是将原始样本视为总体（问题的整个领域），而将生成的子样本视为样本。
- en: 'In essence, we are simulating how a statistic would behave if we collected
    many samples from the original population, as shown in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本质上，我们在模拟如果我们从原始总体中收集多个样本，统计量将如何表现，正如以下图示所示：
- en: '![](img/eea515ec-c607-48f5-9db1-8eb8b6159a9f.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eea515ec-c607-48f5-9db1-8eb8b6159a9f.png)'
- en: A representation of how resampling works
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 自助法如何工作的示意图
- en: Creating bootstrap samples
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自助样本
- en: In order to create bootstrap samples, we resample with replacement (each instance
    may be selected multiple times) from our original sample. This means that a single
    instance can be selected multiple times. Suppose we have data for 100 individuals.
    The data contains the weight and height of each individual. If we generate random
    numbers from 1 to 100 and add the corresponding data to a new dataset, we have
    essentially created a bootstrap sample.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建自助样本，我们使用有放回抽样（每个实例可能被多次选择）从原始样本中抽取数据。这意味着一个实例可以被多次选择。假设我们有 100 个人的数据，数据中包含每个人的体重和身高。如果我们从
    1 到 100 生成随机数字，并将对应的数据添加到一个新数据集中，那么我们基本上就创建了一个自助样本。
- en: 'In Python, we can use `numpy.random.choice`to create a sub-sample of a given
    size. We can try to create bootstrap samples and estimates about the mean and
    standard deviation of the diabetes dataset. First, we load the dataset and libraries
    and print the statistics of our sample, as in the following example:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们可以使用 `numpy.random.choice` 来创建给定大小的子样本。我们可以尝试创建自助样本并估算糖尿病数据集的均值和标准差。首先，我们加载数据集和库，并打印样本的统计信息，如下例所示：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We then create the bootstrap samples and statistics and store them in `bootstrap_stats`.
    We could store the whole bootstrap samples, but it is not memory-efficient to
    do so. Furthermore, we only care about the statistics, so it makes sense only
    to store them. Here, we create 10,000 bootstrap samples and statistics:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建自助样本和统计量，并将其存储在`bootstrap_stats`中。我们本可以存储整个自助样本，但这样做会消耗过多内存。而且，我们只关心统计量，因此只存储它们更有意义。在这里，我们创建了10,000个自助样本和统计量：
- en: '[PRE1]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can now plot the histograms of the mean and standard deviation, as well
    as calculate the standard error (that is, the standard deviation of the statistic''s
    distributions) for each:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制平均值和标准差的直方图，并计算每个值的标准误差（即统计量分布的标准差）：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the output shown in the following diagram:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到如下图所示的输出：
- en: '![](img/8114287f-2280-4cd4-8aa7-fcd3b478a8dd.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8114287f-2280-4cd4-8aa7-fcd3b478a8dd.png)'
- en: Bootstrap distributions for mean and standard deviation
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 平均值和标准差的自助分布
- en: Note that due to the inherent randomness of the process (for which instances
    will be selected for each bootstrap sample), the results may vary each time the
    procedure is executed. A higher number of bootstrap samples will help to stabilize
    the results. Nonetheless, it is a useful technique to calculate the standard error,
    confidence intervals, and other statistics without making any assumptions about
    the underlying distribution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于该过程的固有随机性（每个自助样本将选择哪些实例），每次执行时结果可能会有所不同。增加自助样本的数量有助于稳定结果。尽管如此，这仍然是一种非常有用的技术，可以在不做假设的情况下计算标准误差、置信区间和其他统计量，而无需假设底层分布。
- en: Bagging
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成法（Bagging）
- en: Bagging makes use of bootstrap samples in order to train an array of base learners.
    It then combines their predictions using voting. The motivation behind this method
    is to produce diverse base learners by diversifying the train sets. In this section,
    we discuss the motivation, strengths, and weaknesses of this method.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 集成法利用自助采样（bootstrap sampling）训练一系列基础学习器，然后通过投票方式合并它们的预测结果。这种方法的动机是通过多样化训练集，产生多样化的基础学习器。在本节中，我们讨论这种方法的动机、优势与劣势。
- en: Creating base learners
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建基础学习器
- en: 'Bagging applies bootstrap sampling to the train set, creating a number of *N*
    bootstrap samples. It then creates the same number *N* of base learners, using
    the same machine learning algorithm. Each base learner is trained on the corresponding
    train set and all base learners are combined by voting (hard voting for classification,
    and averaging for regression). The procedure is depicted as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 集成法对训练集应用自助采样，创建多个*N*个自助样本。接着，使用相同的机器学习算法创建相同数量*N*的基础学习器。每个基础学习器都在相应的训练集上进行训练，所有基础学习器通过投票合并（分类时使用硬投票，回归时使用平均值）。该过程如下所示：
- en: '![](img/7a27a871-ef8e-4465-9e56-2e53afe717f4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a27a871-ef8e-4465-9e56-2e53afe717f4.png)'
- en: Creating base learners through bagging
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通过集成法创建基础学习器
- en: By using bootstrap samples with the same size as the original train set, each
    instance has a probability of 0.632 of appearing in any given bootstrap sample.
    Thus, in many cases, this type of bootstrap estimate is referred to as the 0.632
    bootstrap estimate. In our case, this means that we can use the remaining 36.8%
    of the original train set in order to estimate the individual base learner's performance.
    This is called the **out**-**of**-**bag score**, and the 36.8% of instances are
    called **out**-**of**-**bag instances**.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用与原始训练集大小相同的自助样本，每个实例在任何给定的自助样本中出现的概率为0.632。因此，在许多情况下，这种自助估计被称为0.632自助估计。在我们的案例中，这意味着我们可以使用原始训练集中剩余的36.8%实例来估算单个基础学习器的性能。这被称为**袋外得分**（**out**-**of**-**bag
    score**），而这36.8%的实例则被称为**袋外实例**（**out**-**of**-**bag instances**）。
- en: Strengths and weaknesses
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优势与劣势
- en: Bagging is usually utilized with decision trees as its base learners, but it
    can be used with any machine learning algorithm. Bagging reduces variance greatly
    and it has been proved that it is most effective when unstable base learners are
    used. Unstable learners generate models with great inter-model variance, even
    when the respective train sets vary only slightly. Furthermore, bagging converges
    as the number of base learners grows. Similar to estimating a bootstrap statistic,
    by increasing the number of base learners, we also increase the number of bootstrap
    samples. Finally, bagging allows for easy parallelization, as each model is trained
    independently.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 通常使用决策树作为基本学习器，但它可以与任何机器学习算法一起使用。Bagging 大大减少了方差，并且已被证明在使用不稳定的基本学习器时最为有效。不稳定的学习器生成的模型具有较大的模型间方差，即使训练集仅略微变化。此外，随着基本学习器数量的增加，bagging
    会收敛。类似于估计自助法统计量，通过增加基本学习器的数量，我们也增加了自助样本的数量。最后，bagging 允许轻松并行化，因为每个模型都是独立训练的。
- en: The main disadvantage of bagging is the loss of interpretability and transparency
    of our models. For example, using a single decision tree allows for great interpretability,
    as the decision of each node is readily available. Using a bagging ensemble of
    100 trees makes the individual decisions less important, while the collective
    predictions define the ensemble's final output.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging 的主要缺点是模型的可解释性和透明度的丧失。例如，使用单个决策树可以提供很好的可解释性，因为每个节点的决策都是可直接获取的。使用 100
    棵树的 bagging 集成模型会使得单个决策变得不那么重要，而是集体预测定义了集成模型的最终输出。
- en: Python implementation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python 实现
- en: To better understand the process of creating the ensemble, as well as its merits,
    we will implement it in Python using decision trees. In this example, we will
    try to classify the MNIST dataset of handwritten digits. Although we have used
    the cancer dataset for classification examples up until now, it contains only
    two classes, while the number of examples is relatively small for effective bootstrapping.
    The digits dataset contains a considerable number of examples and is also more
    complex, as there is a total of 10 classes.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解集成模型的创建过程及其优点，我们将使用决策树在 Python 中实现它。在这个示例中，我们将尝试对手写数字的 MNIST 数据集进行分类。虽然我们之前一直使用癌症数据集作为分类示例，但它只有两个类别，并且样本数量相对较少，不适合有效的自助法。数字数据集包含大量样本，且更加复杂，因为它总共有
    10 个类别。
- en: Implementation
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: 'For this example, we will use 1500 instances as the train set, and the remaining
    297 as the test set. We will generate 10 bootstrap samples, and consequently 10
    decision-tree models. We will then combine the base predictions using hard voting:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用 1500 个实例作为训练集，剩余的 297 个作为测试集。我们将生成 10 个自助样本，因此会得到 10 个决策树模型。接着，我们将通过硬投票将基本预测结果结合起来：
- en: 'We load the libraries and data as shown in the following example:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载库和数据，如下所示：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then create our bootstrap samples and train the corresponding models. Note,
    that we do not use `np.random.choice`. Instead, we generate an array of indices
    with `np.random.randint(0, train_size, size=train_size)`, as this will enable
    us to choose both the features and the corresponding targets for each bootstrap
    sample. We store each base learner in the `base_learners` list, for ease of access
    later on:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们创建自助样本并训练相应的模型。请注意，我们没有使用 `np.random.choice`，而是使用 `np.random.randint(0,
    train_size, size=train_size)` 生成一个索引数组，这样我们可以为每个自助样本选择特征和相应的目标。为了后续方便访问，我们将每个基本学习器存储在
    `base_learners` 列表中：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we predict the targets of the test set with each base learner and store
    their predictions as well as their evaluated accuracy, as shown in the following
    code block:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用每个基本学习器预测测试集的目标，并存储它们的预测结果以及评估后的准确性，如下方代码块所示：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that we have each base learner''s predictions in `base_predictions`, we
    can combine them with hard voting, as we did in [Chapter 3](ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml), *Voting*,
    for individual base learners. Furthermore, we evaluate the ensemble''s accuracy:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经在 `base_predictions` 中得到了每个基本学习器的预测，我们可以像在[第3章](ad9aa66b-7b30-4779-8914-0ff58140b3e8.xhtml)中做的那样，使用硬投票将它们结合起来，*投票*，用于个体基本学习器的预测。此外，我们还评估了集成模型的准确性：
- en: '[PRE6]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we print the accuracy of each base learner, as well as the ensemble''s
    accuracy, sorted in ascending order:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们打印每个基本学习器的准确性以及集成模型的准确性，并按升序排序：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The final output is shown in the following example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最终输出如下所示：
- en: '[PRE8]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It is evident that the ensemble's accuracy is almost 10% higher than the best-performing
    base model. This is a considerable improvement, especially if we take into account
    that this ensemble consists of identical base learners (considering the machine
    learning method used).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，集成模型的准确率比表现最佳的基模型高出近 10%。这是一个相当大的改进，特别是如果我们考虑到该集成模型由相同的基学习器组成（考虑到所使用的机器学习方法）。
- en: Parallelizing the implementation
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 并行化实现
- en: 'We can easily parallelize our bagging implementation using `from concurrent.futures
    import ProcessPoolExecutor`. This executor allows the user to spawn a number of
    tasks to be executed and executes them in parallel processes. It only needs to
    be passed a target function and its parameters. In our example, we only need to
    create functions out of code sections (sections 2 and 3):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过 `from concurrent.futures import ProcessPoolExecutor` 来轻松并行化我们的袋装实现。这个执行器允许用户生成多个任务并在并行进程中执行。它只需要传入一个目标函数及其参数。在我们的例子中，我们只需要将代码块（第
    2 和第 3 部分）封装成函数：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, in the original sections 2 and 3, we modify the code as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，在原始的第 2 和第 3 部分中，我们将代码修改如下：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `executor` returns an object (in our case `future`), which contains the
    results of our function. The rest of the code remains unchanged with the exception
    that it is enclosed in `if __name__ == '__main__' `guard, as each new process
    will import the whole script. This guard prevents them from re-executing the rest
    of the code. As our example is small, with six processes available, we need to
    have at least 1,000 base learners to see any considerable speedup in the execution
    times. For a fully working version, please refer to `'bagging_custom_parallel.py'`
    from the provided codebase.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`executor` 返回一个对象（在我们的例子中是 `future`），其中包含了我们函数的结果。其余代码保持不变，唯一的变化是它被封装在 `if
    __name__ == ''__main__''` 的保护代码块中，因为每个新进程都会导入整个脚本。这个保护代码块防止它们重新执行其余的代码。由于我们的示例较小，且有六个进程可用，因此我们需要至少
    1,000 个基学习器才能看到执行时间的显著加速。有关完整的工作版本，请参考提供的代码库中的 `''bagging_custom_parallel.py''`。'
- en: Using scikit-learn
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn
- en: Scikit-learn has a great implementation of bagging for both regression and classification
    problems. In this section, we will go through the process of using the provided
    implementations to create ensembles for the digits and diabetes datasets.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 为回归和分类问题提供了出色的袋装（bagging）实现。在本节中，我们将通过使用提供的实现，创建数字和糖尿病数据集的集成模型。
- en: Bagging for classification
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类问题的袋装（Bagging）
- en: Scikit-learn's implementation of bagging lies in the `sklearn.ensemble` package.
    `BaggingClassifier` is the corresponding class for classification problems. It
    has a number of interesting parameters, allowing for greater flexibility. It can
    use any scikit-learn estimator by specifying it with `base_estimator`. Furthermore,
    `n_estimators` dictates the ensemble's size (and, consequently, the number of
    bootstrap samples that will be generated), while `n_jobs` dictates how many jobs
    (processes) will be used to train and predict with each base learner. Finally,
    if set to `True`, `oob_score` calculates the out-of-bag score for the base learners.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 的袋装实现位于 `sklearn.ensemble` 包中。`BaggingClassifier` 是分类问题的相应类。它有许多有趣的参数，提供了更大的灵活性。通过指定
    `base_estimator`，它可以使用任何 scikit-learn 估计器。此外，`n_estimators` 决定了集成模型的大小（也就是说，决定了生成的自助样本数量），而
    `n_jobs` 则决定了在训练和预测每个基学习器时将使用多少个作业（进程）。最后，如果设置为 `True`，`oob_score` 会计算基学习器的袋外得分。
- en: 'Using the actual classifier is straightforward and similar to all other scikit-learn
    estimators. First, we load the required data and libraries, as shown in the following
    example:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 使用实际的分类器是非常简单的，与所有其他 scikit-learn 估计器类似。首先，我们加载所需的数据和库，如以下示例所示：
- en: '[PRE11]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We then create, train, and evaluate the estimator:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建、训练并评估估计器：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The final achieved accuracy is 88%, the same as our own implementation. Furthermore,
    we can access the out-of-bag score through `ensemble.oob_score_`, which in our
    case is equal to 89.6%. Generally, the out-of-bag score slightly overestimates
    the out-of-sample predictive capability of the ensemble, which is what we observe
    in this example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的准确率为 88%，与我们自己的实现相同。此外，我们可以通过 `ensemble.oob_score_` 访问袋外得分，在我们的例子中，它等于 89.6%。通常情况下，袋外得分略微高估了集成模型的样本外预测能力，这在这个示例中得到了体现。
- en: 'In our examples, we chose an `ensemble_size` of `10`. Suppose we would like
    to test how different ensemble sizes affect the ensemble''s performance. Given
    that the bagging classifier accepts the size as a constructor''s parameter, we
    can use validation curves from [Chapter 2](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml), *Getting
    Started with Ensemble Learning*, to conduct the test. We test 1 to 39 base learners,
    with a step of 2\. We observe an initial decrease in bias and variance. For ensembles
    with more than 20 base learners, there seems to be zero benefit in increasing
    the ensemble’s size. The results are depicted in the following diagram:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的示例中，我们选择了`ensemble_size`为`10`。假设我们希望测试不同集成模型大小如何影响集成模型的表现。由于bagging分类器接受该大小作为构造函数的参数，我们可以使用[第二章](d7921006-351e-4c21-ab54-f1dc834557dc.xhtml)中的验证曲线，*开始使用集成学习*，来进行该测试。我们测试了1到39个基础学习器，步长为2。我们观察到偏差和方差的初始下降。对于具有超过20个基础学习器的集成模型，似乎增加集成模型的大小并没有带来任何好处。结果在下图中显示：
- en: '![](img/1366c425-06ba-4e76-9523-a11d125fe96f.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1366c425-06ba-4e76-9523-a11d125fe96f.png)'
- en: Validation curves for 1 to 39 base learners
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 1到39个基础学习器的验证曲线
- en: Bagging for regression
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于回归的Bagging
- en: 'For regression purposes, we will use the `BaggingRegressor` class from the
    same `sklearn.ensemble` package. We will also instantiate a single `DecisionTreeRegressor `to
    compare the results. We start by loading the libraries and data, as usual:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归目的，我们将使用来自相同`sklearn.ensemble`包的`BaggingRegressor`类。我们还将实例化一个单独的`DecisionTreeRegressor`以比较结果。我们按惯例开始加载库和数据：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We instantiate the single decision tree and the ensemble. Note that we allow
    for a relatively deep decision tree, by specifying `max_depth=6`. This allows
    the creation of diverse and unstable models, which greatly benefits bagging. If
    we restrict the maximum depth to 2 or 3 levels, we will see that bagging does
    not perform better than a single model. Training and evaluating the ensemble and
    the model follows the standard procedure:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实例化了单个决策树和集成模型。请注意，我们通过指定`max_depth=6`来允许相对较深的决策树。这允许创建多样化且不稳定的模型，极大地有利于bagging。如果我们将最大深度限制为2或3层，我们会看到bagging的表现不比单一模型更好。训练和评估集成模型和单一模型的过程遵循标准程序：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The ensemble can greatly outperform the single model, by producing both higher
    R-squared and lower **mean squared error** (**MSE**). As mentioned earlier, this
    is due to the fact that the base learners are allowed to create deep and unstable
    models. The actual results of the two models are provided in the following output:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型可以显著优于单一模型，通过产生更高的R平方值和更低的**均方误差**（**MSE**）。如前所述，这是因为基础学习器可以创建深度和不稳定的模型。以下是两个模型的实际结果：
- en: '[PRE15]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we presented the main concept of creating bootstrap samples
    and estimating bootstrap statistics. Building on this foundation, we introduced
    bootstrap aggregating, or bagging, which uses a number of bootstrap samples to
    train many base learners that utilize the same machine learning algorithm. Later,
    we provided a custom implementation of bagging for classification, as well as
    the means to parallelize it. Finally, we showcased the use of scikit-learn's own
    implementation of bagging for regression and classification problems.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了创建自助法样本和估算自助法统计量的主要概念。在此基础上，我们介绍了自助法聚合（或称为bagging），它使用多个自助法样本来训练许多基础学习器，这些学习器使用相同的机器学习算法。随后，我们提供了用于分类的自定义bagging实现，并介绍了如何并行化它。最后，我们展示了scikit-learn自身实现的bagging在回归和分类问题中的应用。
- en: The chapter can be summarized as follows. **Bootstrap samples** are created
    by resampling with replacement from the original dataset. The main idea is to
    treat the original sample as the population, and each subsample as an original
    sample. If the original dataset and the bootstrap dataset have the same size,
    each instance has a probability of  **63.2%** of being included in the bootstrap
    dataset (sample). Bootstrap methods are useful for calculating statistics such
    as confidence intervals and standard error, **without making assumptions** about
    the underlying distribution. **Bagging** generates a number of bootstrap samples
    to train each individual base learner. Bagging benefits **unstable learners**,
    where small variations in the train set induce great variations in the generated
    model. Bagging is a suitable ensemble learning method to reduce **variance**.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本章可以总结如下：**自助法样本**是通过从原始数据集进行有放回的重采样来创建的。其主要思想是将原始样本视为总体，将每个子样本视为原始样本。如果原始数据集和自助法数据集的大小相同，则每个实例有**63.2%**的概率被包含在自助法数据集中（样本）。自助法方法对于计算统计量（如置信区间和标准误差）非常有用，**无需对潜在的分布做假设**。**集成自助法（Bagging）**通过生成多个自助法样本来训练每个独立的基学习器。集成自助法对于**不稳定学习器**很有帮助，因为训练集中的小变化可能会导致生成的模型发生较大变化。集成自助法是减少**方差**的适合的集成学习方法。
- en: Bagging allows for easy **parallelization**, as each bootstrap sample and base
    learner can be generated, trained, and tested individually. As with all ensemble
    learning methods, using bagging reduces the **interpretability** and motivation
    behind individual predictions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 集成自助法支持**并行化**，因为每个自助法样本和基学习器都可以独立生成、训练和测试。与所有集成学习方法一样，使用集成自助法会降低单个预测的**可解释性**和动机。
- en: In the next chapter, we will introduce the second generative method, Boosting.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将介绍第二种生成性方法——提升法（Boosting）。
