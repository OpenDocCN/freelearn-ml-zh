- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Categorizing Images of Clothing with Convolutional Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用卷积神经网络对服装图像进行分类
- en: The previous chapter wrapped up our coverage of the best practices for general
    machine learning. Starting from this chapter, we will dive into the more advanced
    topics of deep learning and reinforcement learning.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章总结了我们对一般机器学习最佳实践的讲解。从本章开始，我们将深入探讨更高级的深度学习和强化学习主题。
- en: When we deal with image classification, we usually flatten the images, get vectors
    of pixels, and feed them to a neural network (or another model). Although this
    might do the job, we lose critical spatial information. In this chapter, we will
    use **Convolutional Neural Networks** (**CNNs**) to extract rich and distinguishable
    representations from images. You will see how CNN representations make a “9” a
    “9”, a “4” a “4”, a cat a cat, or a dog a dog.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行图像分类时，通常会将图像展开成像素向量，然后输入到神经网络（或其他模型）中。尽管这样做可能能完成任务，但我们会丢失重要的空间信息。在本章中，我们将使用**卷积神经网络**（**CNNs**）从图像中提取丰富且可区分的表示。你将看到，CNN的表示能够将“9”识别为“9”，将“4”识别为“4”，将猫识别为猫，或者将狗识别为狗。
- en: We will start by exploring individual building blocks in the CNN architecture.
    Then, we will develop a CNN classifier in PyTorch to categorize clothing images
    and demystify the convolutional mechanism. Finally, we will introduce data augmentation
    to boost the performance of CNN models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从探索CNN架构中的各个构建模块开始。接着，我们将开发一个CNN分类器，在PyTorch中对服装图像进行分类，并揭示卷积机制。最后，我们将介绍数据增强技术，以提升CNN模型的性能。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Getting started with CNN building blocks
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始了解CNN构建模块
- en: Architecting a CNN for classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分类设计CNN架构
- en: Exploring the clothing image dataset
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索服装图像数据集
- en: Classifying clothing images with CNNs
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用CNN对服装图像进行分类
- en: Boosting the CNN classifier with data augmentation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据增强提升CNN分类器的性能
- en: Advancing the CNN classifier with transfer learning
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用迁移学习提升CNN分类器性能
- en: Getting started with CNN building blocks
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始了解CNN构建模块
- en: Although regular hidden layers (the fully connected layers we have seen so far)
    do a good job of extracting features from data at certain levels, these representations
    might not be useful in differentiating images of different classes. CNNs can be
    used to extract richer, more distinguishable representations that, for example,
    make a car a car, a plane a plane, or the handwritten letters “y” and “z” recognizably
    a “y” and a “z,” and so on. CNNs are a type of neural network that is biologically
    inspired by the human visual cortex. To demystify CNNs, I will start by introducing
    the components of a typical CNN, including the convolutional layer, the non-linear
    layer, and the pooling layer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管常规的隐藏层（我们至今看到的全连接层）在某些层次上提取数据特征时表现良好，但这些表示可能不足以区分不同类别的图像。CNN可以用来提取更丰富、更具区分性的表示，例如，使汽车成为汽车，使飞机成为飞机，或者使手写字母“y”和“z”可识别为“y”和“z”，等等。CNN是一种受人类视觉皮层生物学启发的神经网络。为了揭开CNN的神秘面纱，我将从介绍典型CNN的组成部分开始，包括卷积层、非线性层和池化层。
- en: The convolutional layer
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层
- en: The **convolutional layer** is the first layer in a CNN, or the first few layers
    in a CNN if it has multiple convolutional layers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积层**是CNN中的第一层，或者是如果CNN有多个卷积层的话，前几层。'
- en: 'CNNs, specifically their convolutional layers, mimic the way our visual cells
    work, as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: CNN，特别是它们的卷积层，模仿了我们视觉细胞的工作方式，具体如下：
- en: Our visual cortex has a set of complex neuronal cells that are sensitive to
    specific sub-regions of the visual field and that are called **receptive fields**.
    For instance, some cells only respond in the presence of vertical edges; some
    cells fire only when they are exposed to horizontal edges; some react more strongly
    when they are shown edges of a certain orientation. These cells are organized
    together to produce the entire visual perception, with each cell being specialized
    in a specific component. A convolutional layer in a CNN is composed of a set of
    filters that act like those cells in humans’ visual cortexes.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的视觉皮层中有一组复杂的神经细胞，这些细胞对视觉场的特定子区域非常敏感，称为**感受野**。例如，一些细胞只有在垂直边缘出现时才会响应；一些细胞仅在暴露于水平边缘时才会激活；一些细胞对特定方向的边缘反应更强。这些细胞组合在一起，产生完整的视觉感知，每个细胞专门处理特定的组成部分。CNN中的卷积层由一组过滤器组成，作用类似于人类视觉皮层中的这些细胞。
- en: A simple cell only responds when edge-like patterns are presented within its
    receptive sub-regions. A more complex cell is sensitive to larger sub-regions,
    and as a result, can respond to edge-like patterns across the entire visual field.
    A stack of convolutional layers is a bunch of complex cells that can detect patterns
    in a bigger scope.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的细胞只在其感受域内出现边缘类模式时作出反应。更复杂的细胞对更大的子区域敏感，因此可以对整个视野内的边缘类模式作出反应。一堆卷积层则是一些复杂的细胞，可以在更大的范围内检测模式。
- en: 'The convolutional layer processes input images or matrices and mimics how neural
    cells react to the particular regions they are attuned to by employing a convolutional
    operation on the input. Mathematically, it computes the **dot product** between
    the nodes of the convolutional layer and individual small regions in the input
    layer. The small region is the receptive field, and the nodes of the convolutional
    layer can be viewed as the values on a filter. As the filter moves along on the
    input layer, the dot product between the filter and the current receptive field
    (sub-region) is computed. A new layer called the **feature map** is obtained after
    the filter has convolved over all the sub-regions. Let’s look at a simple example,
    as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层处理输入图像或矩阵，并通过在输入上执行卷积操作，模拟神经细胞如何对它们调适的特定区域作出反应。从数学上讲，它计算卷积层节点与输入层中单个小区域的**点积**。这个小区域就是感受野，而卷积层的节点可以看作滤波器上的值。当滤波器沿着输入层滑动时，会计算滤波器与当前感受野（子区域）之间的点积。经过滤波器卷积过所有子区域后，会得到一个新的层，称为**特征图**。让我们看一个简单的例子，如下所示：
- en: '![A diagram of a network  Description automatically generated with low confidence](img/B21047_11_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![网络示意图  描述自动生成，信心较低](img/B21047_11_01.png)'
- en: 'Figure 11.1: How a feature map is generated'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：特征图是如何生成的
- en: In this example, layer *l* has 5 nodes and the filter is composed of 3 nodes
    [*w*[1], *w*[2], *w*[3]]. We first compute the dot product between the filter
    and the first three nodes in layer *l* and obtain the first node in the output
    feature map; then, we compute the dot product between the filter and the middle
    three nodes and generate the second node in the output feature map; finally, the
    third node is generated from the convolution on the last three nodes in layer
    *l*.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，层* l *包含5个节点，滤波器由3个节点组成[*w*[1], *w*[2], *w*[3]]。我们首先计算滤波器与层* l *中前3个节点的点积，得到输出特征图中的第一个节点；接着，计算滤波器与中间3个节点的点积，生成输出特征图中的第二个节点；最后，第三个节点是通过对层*
    l *中最后3个节点进行卷积得到的。
- en: 'Now, we’ll take a closer look at how convolution works in the following example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过以下示例更详细地了解卷积是如何工作的：
- en: '![](img/B21047_11_02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_11_02.png)'
- en: 'Figure 11.2: How convolution works'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.2：卷积是如何工作的
- en: 'In this example, a 3*3 filter is sliding around a 5*5 input matrix from the
    top-left sub-region to the bottom-right sub-region. For each sub-region, the dot
    product is computed using the filter. Take the top-left sub-region (in the orange
    rectangle) as an example: we have 1 * 1 + 1 * 0 + 1 * 1 = 2, therefore the top-left
    node (in the upper-left orange rectangle) in the feature map is of value 2\. For
    the next leftmost sub-region (in the blue dash rectangle), we calculate the convolution
    as 1 * 1 + 1 * 1 + 1 * 1 = 3, so the value of the next node (in the upper-middle
    blue dash rectangle) in the resulting feature map becomes 3\. At the end, a 3*3
    feature map is generated as a result.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，一个3*3的滤波器在一个5*5的输入矩阵上滑动，从左上角的子区域到右下角的子区域。对于每个子区域，使用滤波器计算点积。以左上角的子区域（橙色矩形）为例：我们有1
    * 1 + 1 * 0 + 1 * 1 = 2，因此特征图中的左上角节点（橙色矩形）值为2。对于下一个最左边的子区域（蓝色虚线矩形），我们计算卷积为1 *
    1 + 1 * 1 + 1 * 1 = 3，所以结果特征图中下一个节点（蓝色虚线矩形）值为3。最后，生成一个3*3的特征图。
- en: So what do we use convolutional layers for? They are actually used to extract
    features such as edges and curves. The pixel in the output feature map will be
    of high value if the corresponding receptive field contains an edge or curve that
    is recognized by the filter. For instance, in the preceding example, the filter
    portrays a backslash-shape “\” diagonal edge; the receptive field in the blue
    dash rectangle contains a similar curve and hence the highest intensity 3 is created.
    However, the receptive field in the top-right corner does not contain such a backslash
    shape, hence it results in a pixel of value 0 in the output feature map. The convolutional
    layer acts as a curve detector or a shape detector.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们使用卷积层来做什么呢？它们实际上是用来提取特征的，比如边缘和曲线。如果输出特征图中的像素值较高，则说明对应的感受野包含了滤波器识别的边缘或曲线。例如，在前面的例子中，滤波器描绘了一个反斜线形状“\”的对角边缘；蓝色虚线矩形中的感受野包含了类似的曲线，因此生成了最高强度值3。然而，右上角的感受野没有包含这样的反斜线形状，因此它在输出特征图中产生了值为0的像素。卷积层的作用就像是一个曲线检测器或形状检测器。
- en: Also, a convolutional layer usually has multiple filters detecting different
    curves and shapes. In the simple preceding example, we only apply one filter and
    generate one feature map, which indicates how well the shape in the input image
    resembles the curve represented in the filter. In order to detect more patterns
    from the input data, we can employ more filters, such as horizontal, vertical
    curve, 30-degree, and right-angle shape.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，卷积层通常会有多个滤波器，检测不同的曲线和形状。在前面的简单示例中，我们只使用了一个滤波器并生成了一个特征图，它表示输入图像中的形状与滤波器所表示的曲线的相似度。为了从输入数据中检测更多的模式，我们可以使用更多的滤波器，比如水平、垂直曲线、30度角和直角形状。
- en: Additionally, we can stack several convolutional layers to produce higher-level
    representations such as the overall shape and contour. Chaining more layers will
    result in larger receptive fields that are able to capture more global patterns.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以堆叠多个卷积层来生成更高级的表示，例如整体形状和轮廓。链式连接更多的层将产生更大的感受野，能够捕捉到更多的全局模式。
- en: Right after each convolutional layer, we often apply a non-linear layer.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 每个卷积层之后，我们通常会应用一个非线性层。
- en: The non-linear layer
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性层
- en: The non-linear layer is basically the activation layer we saw in *Chapter 6*,
    *Predicting Stock Prices with Artificial Neural Networks*. It is used to introduce
    non-linearity, obviously. Recall that in the convolutional layer, we only perform
    linear operations (multiplication and addition). And no matter how many linear
    hidden layers a neural network has, it will just behave as a single-layer perceptron.
    Hence, we need a non-linear activation right after the convolutional layer. Again,
    ReLU is the most popular candidate for the non-linear layer in deep neural networks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性层基本上就是我们在*第六章*《使用人工神经网络预测股票价格》中看到的激活层。它的作用显然是引入非线性。回想一下，在卷积层中，我们只执行线性操作（乘法和加法）。无论神经网络有多少个线性隐藏层，它都只能表现得像一个单层感知器。因此，我们需要在卷积层后加一个非线性激活层。再次强调，ReLU是深度神经网络中最常用的非线性层候选。
- en: The pooling layer
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: Normally after one or more convolutional layers (along with non-linear activation),
    we can directly use the derived features for classification. For example, we can
    apply a softmax layer in the multiclass classification case. But let’s do some
    math first.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在一个或多个卷积层（及非线性激活层）之后，我们可以直接利用提取的特征进行分类。例如，在多类分类问题中，我们可以应用Softmax层。但我们先做一些数学运算。
- en: Given 28 * 28 input images, supposing that we apply 20 5 * 5 filters in the
    first convolutional layer, we will obtain 20 output feature maps and each feature
    map layer will be of size (28 – 5 + 1) * (28 – 5 + 1) = 24 * 24 = 576\. This means
    that the number of features as inputs for the next layer increases to 11,520 (20
    * 576) from 784 (28 * 28). We then apply 50 5 * 5 filters in the second convolutional
    layer. The size of the output grows to 50 * 20 * (24 – 5 + 1) * (24 – 5 + 1) =
    400,000\. This is a lot higher than our initial size of 784\. We can see that
    the dimensionality increases dramatically with every convolutional layer before
    the final softmax layer. This can be problematic as it leads to overfitting easily,
    not to mention the cost of training such a large number of weights.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对 28 * 28 的输入图像应用 20 个 5 * 5 的滤波器在第一层卷积中，那么我们将得到 20 个输出特征图，每个特征图的大小为 (28
    – 5 + 1) * (28 – 5 + 1) = 24 * 24 = 576。这意味着下一层的输入特征数将从 784 (28 * 28) 增加到 11,520
    (20 * 576)。然后我们在第二层卷积中应用 50 个 5 * 5 的滤波器。输出的大小变为 50 * 20 * (24 – 5 + 1) * (24
    – 5 + 1) = 400,000。这比我们最初的 784 要高得多。我们可以看到，在最后的 softmax 层之前，每经过一层卷积层，维度都会急剧增加。这可能会引发过拟合问题，更不用说训练如此大量权重的成本了。
- en: 'To address the issue of drastically growing dimensionality, we often employ
    a **pooling layer** after the convolutional and non-linear layers. The pooling
    layer is also called the **downsampling layer**. As you can imagine, it reduces
    the dimensions of the feature maps. This is done by aggregating the statistics
    of features over sub-regions. Typical pooling methods include:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决维度急剧增加的问题，我们通常会在卷积层和非线性层之后使用**池化层**。池化层也叫做**下采样层**。正如你所想，它通过对子区域中的特征进行聚合来减少特征图的维度。典型的池化方法包括：
- en: Max pooling, which takes the max values over all non-overlapping sub-regions
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大池化，取所有不重叠子区域的最大值
- en: Mean pooling, which takes the mean values over all non-overlapping sub-regions
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值池化，取所有不重叠子区域的均值
- en: 'In the following example, we apply a 2 * 2 max-pooling filter on a 4 * 4 feature
    map and output a 2 * 2 one:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们对 4 * 4 的特征图应用 2 * 2 的最大池化滤波器，并输出一个 2 * 2 的结果：
- en: '![A picture containing screenshot, number  Description automatically generated](img/B21047_11_03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![包含屏幕截图、数字的图片  描述自动生成](img/B21047_11_03.png)'
- en: 'Figure 11.3: How max pooling works'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：最大池化的工作原理
- en: 'Besides dimensionality reduction, the pooling layer has another advantage:
    translation invariance. This means that its output doesn’t change even if the
    input matrix undergoes a small amount of translation. For example, if we shift
    the input image a couple of pixels to the left or right, as long as the highest
    pixels remain the same in the sub-regions, the output of the max-pooling layer
    will still be the same. In other words, the prediction becomes less position-sensitive
    with pooling layers. The following example illustrates how max pooling achieves
    translation invariance.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 除了维度减小，池化层还有另一个优点：平移不变性。这意味着即使输入矩阵经历了小幅度的平移，它的输出也不会改变。例如，如果我们将输入图像向左或向右移动几个像素，只要子区域中的最高像素保持不变，最大池化层的输出仍然会保持相同。换句话说，池化层使得预测对位置的敏感度降低。以下示例说明了最大池化如何实现平移不变性。
- en: 'Here is the 4 * 4 original image, along with the output from max pooling with
    a 2 * 2 filter:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 4 * 4 的原始图像，以及使用 2 * 2 滤波器进行最大池化后的输出：
- en: '![A picture containing screenshot, diagram, number  Description automatically
    generated](img/B21047_11_04.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![包含屏幕截图、图示、数字的图片  描述自动生成](img/B21047_11_04.png)'
- en: 'Figure 11.4: The original image and the output from max pooling'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：原始图像和最大池化输出
- en: 'And if we shift the image 1 pixel to the right, we have the following shifted
    image and the corresponding output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将图像向右移动 1 个像素，得到以下移动后的图像和相应的输出：
- en: '![A picture containing text, diagram, number, screenshot  Description automatically
    generated](img/B21047_11_05.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、图示、数字、屏幕截图的图片  描述自动生成](img/B21047_11_05.png)'
- en: 'Figure 11.5: The shifted image and the output'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：移动后的图像和输出
- en: We have the same output even if we horizontally move the input image. Pooling
    layers increase the robustness of image translation.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们水平移动输入图像，输出也是相同的。池化层增加了图像平移的鲁棒性。
- en: You’ve now learned about all of the components of a CNN. It was easier than
    you thought, right? Let’s see how they compose a CNN next.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了 CNN 的所有组件了。比你想象的要简单，对吧？接下来我们来看看它们是如何构成一个 CNN 的。
- en: Architecting a CNN for classification
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建用于分类的卷积神经网络（CNN）
- en: 'Putting the three types of convolutional-related layers together, along with
    the fully connected layer(s), we can structure the CNN model for classification
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 将三种类型的卷积相关层与全连接层（们）结合在一起，我们可以按如下方式构建CNN模型进行分类：
- en: '![A picture containing text, screenshot, diagram, plot  Description automatically
    generated](img/B21047_11_06.png)Figure 11.6: CNN architecture'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![A picture containing text, screenshot, diagram, plot  Description automatically
    generated](img/B21047_11_06.png)图11.6：CNN架构'
- en: In this example, the input images are first fed into a convolutional layer (with
    ReLU activation) composed of a bunch of filters. The coefficients of the convolutional
    filters are trainable. A well-trained initial convolutional layer is able to derive
    good low-level representations of the input images, which will be critical to
    downstream convolutional layers if there are any, and also downstream classification
    tasks. Each resulting feature map is then downsampled by the pooling layer.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，输入图像首先被传入一个卷积层（带有ReLU激活函数），该层由一组滤波器组成。卷积滤波器的系数是可训练的。一个训练良好的初始卷积层能够提取输入图像的良好低级特征表示，这对于后续的卷积层（如果有的话）以及后续的分类任务是至关重要的。然后，每个生成的特征图会通过池化层进行下采样。
- en: Next, the aggregated feature maps are fed into the second convolutional layer.
    Similarly, the second pooling layer reduces the size of the output feature maps.
    You can chain as many pairs of convolutional and pooling layers as you want. The
    second (or more, if any) convolutional layer tries to compose high-level representations,
    such as the overall shape and contour, through a series of low-level representations
    derived from previous layers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将聚合的特征图输入到第二个卷积层。类似地，第二个池化层减少了输出特征图的尺寸。你可以根据需要链式连接任意数量的卷积层和池化层。第二个（或更多，如果有的话）卷积层试图通过一系列从前面层派生的低级表示，构建高级表示，比如整体形状和轮廓。
- en: Up until this point, the feature maps are matrices. We need to flatten them
    into a vector before performing any downstream classification. The flattened features
    are just treated as the input to one or more fully connected hidden layers. We
    can think of a CNN as a hierarchical feature extractor on top of a regular neural
    network. CNNs are well suited to exploiting strong and unique features that differentiate
    images.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，特征图都是矩阵。我们需要将它们展平为一个向量，然后才能进行后续的分类。展平后的特征就像是输入到一个或多个全连接的隐藏层。我们可以将CNN看作是一个常规神经网络之上的分层特征提取器。CNN特别适合于利用强大而独特的特征来区分图像。
- en: The network ends up with a logistic function if we deal with a binary classification
    problem, a softmax function for a multiclass case, or a set of logistic functions
    for multi-label cases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们处理的是二分类问题，网络的输出将是一个逻辑函数；如果是多类问题，则是一个softmax函数；如果是多标签问题，则是多个逻辑函数的集合。
- en: By now, you should have a good understanding of CNNs and should be ready to
    solve the clothing image classification problem. Let’s start by exploring the
    dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该对CNN有了较好的理解，并且应该准备好解决服装图像分类问题。让我们从探索数据集开始。
- en: Exploring the clothing image dataset
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索服装图像数据集
- en: 'The clothing dataset Fashion-MNIST ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))
    is a dataset of images from Zalando (Europe’s biggest online fashion retailer).
    It consists of 60,000 training samples and 10,000 test samples. Each sample is
    a 28 * 28 grayscale image, associated with a label from the following 10 classes,
    each representing articles of clothing:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 服装数据集Fashion-MNIST ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist))是来自Zalando（欧洲最大的在线时尚零售商）的图像数据集。它包含60,000个训练样本和10,000个测试样本。每个样本都是一个28
    * 28的灰度图像，并带有以下10个类别的标签，每个类别代表一种服装：
- en: '0: T-shirt/top'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0: T恤/上衣'
- en: '1: Trouser'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '1: 长裤'
- en: '2: Pullover'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '2: 套头衫'
- en: '3: Dress'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '3: 连衣裙'
- en: '4: Coat'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '4: 外套'
- en: '5: Sandal'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '5: 凉鞋'
- en: '6: Shirt'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '6: 衬衫'
- en: '7: Sneaker'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '7: 运动鞋'
- en: '8: Bag'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '8: 包'
- en: '9: Ankle boot'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '9: 踝靴'
- en: Zalando aims to make the dataset as popular as the handwritten digits MNIST
    dataset for benchmarking algorithms and hence calls it Fashion-MNIST.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Zalando旨在使该数据集像手写数字MNIST数据集一样流行，用于基准测试算法，因此称其为Fashion-MNIST。
- en: 'You can download the dataset from the direct links in the *Get the data* section
    using the GitHub link or simply import it from PyTorch, which already includes
    the dataset and its data loader API. We will take the latter approach, as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过 *获取数据* 部分中的 GitHub 链接直接下载数据集，或者直接从 PyTorch 导入，它已经包含了数据集及其数据加载器 API。我们将采用后者的方法，如下所示：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We just import `torchvision`, a package in PyTorch that provides access to datasets,
    model architectures, and various image transformation utilities for computer vision
    tasks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚导入了 `torchvision`，这是 PyTorch 中一个包，提供了访问数据集、模型架构以及用于计算机视觉任务的各种图像转换工具。
- en: 'The `torchvision` library includes the following key components:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision` 库包含以下关键组件：'
- en: '**Datasets and data loaders**: `torchvision.datasets` provides access to standard
    datasets for tasks like image classification, object detection, semantic segmentation,
    etc. Examples include MNIST, CIFAR-10, ImageNet, `FashionMNIST`, etc. `torch.utils.data.DataLoader`
    helps in creating data loaders to efficiently load and preprocess batches of data
    from datasets.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集和数据加载器**：`torchvision.datasets` 提供了用于图像分类、目标检测、语义分割等任务的标准数据集。示例包括 MNIST、CIFAR-10、ImageNet、`FashionMNIST`
    等。`torch.utils.data.DataLoader` 帮助创建数据加载器，从数据集中高效加载和预处理数据批次。'
- en: '**Transformations**: `torchvision.transforms` offers a variety of image transformations
    for data augmentation, normalization, and preprocessing. Common transformations
    include resizing, cropping, normalization, and more.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：`torchvision.transforms` 提供了多种图像转换工具，用于数据增强、标准化和预处理。常见的转换包括调整大小、裁剪、标准化等。'
- en: '**Model architectures**: `torchvision.models` provides pre-trained model architectures
    for various computer vision tasks.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型架构**：`torchvision.models` 提供了多种计算机视觉任务的预训练模型架构。'
- en: '**Utilities**: `torchvision.utils` includes utility functions for visualizing
    images, converting images into different formats, and more.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**工具**：`torchvision.utils` 包含用于图像可视化、将图像转换为不同格式等的实用函数。'
- en: 'The Fashion-MNIST dataset we just loaded comes with a pre-specified training
    and test dataset partitioning scheme. The training set is stored at `image_path`.
    Then we convert them into Tensor format. Output these two dataset objects to obtain
    additional details:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚加载的 Fashion-MNIST 数据集带有预设的训练集和测试集分区方案。训练集存储在 `image_path` 中。然后，我们将它们转换为
    Tensor 格式。输出这两个数据集对象以获取更多细节：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see, there are 60,000 training samples and 10,000 test samples.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，共有 60,000 个训练样本和 10,000 个测试样本。
- en: 'Next, we load the training set into batches of 64 samples, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练集加载为每批 64 个样本，如下所示：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In PyTorch, `DataLoader` is a utility that provides an efficient way to load
    and preprocess data from a dataset during training or evaluation of machine learning
    models. It essentially wraps around a dataset and provides methods to iterate
    over batches of data. This is particularly useful when working with large datasets
    that do not fit entirely in memory.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PyTorch 中，`DataLoader` 是一个工具，用于在训练或评估机器学习模型时高效加载和预处理数据集中的数据。它本质上是对数据集的包装，提供了遍历数据批次的方法。这对于处理大型数据集（无法完全加载到内存中）特别有用。
- en: 'Key features of DataLoader:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: DataLoader 的关键特性：
- en: '**Batching**: It automatically divides the dataset into batches of specified
    size, allowing you to work with mini-batches of data during training.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理**：它会自动将数据集分成指定大小的批次，从而在训练过程中使用小批量数据。'
- en: '**Shuffling**: You can set the `shuffle` parameter to `True` to shuffle the
    data before each epoch, which helps in reducing bias and improving convergence.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洗牌**：你可以将 `shuffle` 参数设置为 `True`，以在每个训练周期之前对数据进行洗牌，这有助于减少偏差并提高收敛性。'
- en: 'Feel free to inspect the image samples and their labels from the first batch,
    for example:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 随时检查第一批数据中的图像样本及其标签，例如：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The label arrays do not include class names. Hence, we define them as follows
    and will use them for plotting later on:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 标签数组不包含类别名称。因此，我们将它们定义如下，并稍后用于绘图：
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Take a look at the format of the image data as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 查看图像数据的格式，如下所示：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Each image is represented as 28 * 28 pixels, whose values are in the range `[0,
    1]`.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 每张图像表示为 28 * 28 像素，其值的范围是 `[0, 1]`。
- en: 'Let’s now display an image as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们展示一张图像，如下所示：
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In PyTorch, `np.transpose(npimg, (1, 2, 0))` is used when visualizing images
    using `matplotlib`. `(1, 2, 0)` is a tuple representing the new order of dimensions.
    In PyTorch, images are represented in the format `(channels, height, width)`.
    However, matplotlib expects images to be in the format `(height, width, channels)`.
    `np.transpose(npimg, (1, 2, 0))` is used to rearrange the dimensions of the image
    array to match the format that matplotlib expects.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在PyTorch中，`np.transpose(npimg, (1, 2, 0))`用于通过`matplotlib`可视化图像。`(1, 2, 0)`是表示维度新顺序的元组。在PyTorch中，图像采用`(channels,
    height, width)`格式表示，而matplotlib期望图像采用`(height, width, channels)`格式。因此，使用`np.transpose(npimg,
    (1, 2, 0))`重新排列图像数组的维度，以匹配matplotlib期望的格式。
- en: 'Refer to the following sneaker image – the end result:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下面的运动鞋图像——最终结果：
- en: '![A green and blue color scheme  Description automatically generated with medium
    confidence](img/B21047_11_07.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![绿色和蓝色配色方案  描述自动生成，信心中等](img/B21047_11_07.png)'
- en: 'Figure 11.7: A training sample from Fashion-MNIST'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.7：Fashion-MNIST中的一个训练样本
- en: 'Similarly, we display the first 16 training samples, as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们展示前16个训练样本，如下所示：
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Refer to the following image for the result:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考下面的图像以查看结果：
- en: '![A collection of different clothes  Description automatically generated](img/B21047_11_08.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![一组不同的衣服  描述自动生成](img/B21047_11_08.png)'
- en: 'Figure 11.8: 16 training samples from Fashion-MNIST'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8：Fashion-MNIST中的16个训练样本
- en: In the next section, we will be building our CNN model to classify these clothing
    images.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一部分，我们将构建我们的CNN模型来分类这些服装图像。
- en: Classifying clothing images with CNNs
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN对服装图像进行分类
- en: 'As mentioned, the CNN model has two main components: the feature extractor
    composed of a set of convolutional and pooling layers, and the classifier backend,
    similar to a regular neural network.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，CNN模型有两个主要组件：由一组卷积层和池化层组成的特征提取器，以及类似常规神经网络的分类器后端。
- en: Let’s start this project by architecting the CNN model.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从构建CNN模型开始这个项目。
- en: Architecting the CNN model
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建CNN模型
- en: 'We import the necessary module and initialize a Sequential-based model:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入必要的模块并初始化一个基于Sequential的模型：
- en: '[PRE8]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For the convolutional extractor, we are going to use three convolutional layers.
    We start with the first convolutional layer with 32 small-sized 3 * 3 filters.
    This is implemented with the following code:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积提取器，我们将使用三个卷积层。我们从第一个卷积层开始，使用32个小尺寸的3 * 3滤波器。这是通过以下代码实现的：
- en: '[PRE9]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that we use ReLU as the activation function.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用ReLU作为激活函数。
- en: 'The convolutional layer is followed by a max-pooling layer with a 2 * 2 filter:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层后跟一个2 * 2滤波器的最大池化层：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here comes the second convolutional layer. It has 64 3 * 3 filters and comes
    with a ReLU activation function as well:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是第二个卷积层。它有64个3 * 3滤波器，并且也带有ReLU激活函数：
- en: '[PRE11]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The second convolutional layer is followed by another max-pooling layer with
    a 2 * 2 filter:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个卷积层后跟另一个带有2 * 2滤波器的最大池化层：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We continue adding the third convolutional layer. It has 128 3 * 3 filters
    at this time:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续添加第三个卷积层。此时它有128个3 * 3滤波器：
- en: '[PRE13]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s take a pause here and see what the resulting filter maps are. We feed
    a random batch (of 64 samples) into the model we have built so far:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，看看生成的滤波器映射是什么。我们将一批随机样本（64个样本）输入到到目前为止构建的模型中：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: By providing the input shape as `(64, 1, 28, 28)`, which means 64 images within
    the batch, and image size 28 * 28, the output has a shape of `(64, 128, 3, 3)`,
    indicating feature maps with 128 channels and a spatial size of 3 * 3.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过提供输入形状`(64, 1, 28, 28)`，表示批量中的64张图像，图像大小为28 * 28，输出的形状为`(64, 128, 3, 3)`，表示具有128个通道和3
    * 3空间大小的特征图。
- en: 'Next, we need to flatten these small 128 * 3 * 3 spatial representations to
    provide features to the downstream classifier backend:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将这些小的128 * 3 * 3空间表示展平，以提供特征给下游的分类器后端：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'As a result, we have a flattened output of shape `(64, 1152)`, as computed
    by the following code:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个形状为`(64, 1152)`的展平输出，通过以下代码计算得出：
- en: '[PRE16]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For the classifier backend, we just use one hidden layer with 64 nodes:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类器后端，我们只使用一个包含64个节点的隐藏层：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The hidden layer here is the regular fully connected dense layer, with ReLU
    as the activation function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的隐藏层是常规的全连接层，使用ReLU作为激活函数。
- en: 'Finally, the output layer has 10 nodes representing 10 different classes in
    our case, along with a softmax activation:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，输出层有10个节点，代表我们案例中的10个不同类别，并带有softmax激活函数：
- en: '[PRE18]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Let’s take a look at the model architecture, as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下模型架构，如下所示：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If you want to display each layer in detail, including the shape of its output,
    and the number of its trainable parameters, you can use the `torchsummary` library.
    You can install it via `pip` and use it as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想详细显示每一层，包括其输出的形状和可训练参数的数量，可以使用`torchsummary`库。你可以通过`pip`安装它，并按如下方式使用：
- en: '[PRE20]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As you may notice, the output from a convolutional layer is three-dimensional,
    where the first two are the dimensions of the feature maps and the third is the
    number of filters used in the convolutional layer. The size (the first two dimensions)
    of the max-pooling output is half of its input feature map in the example. Feature
    maps are downsampled by the pooling layer. You may want to see how many parameters
    there would be to be trained if you took out all the pooling layers. Actually,
    it is 4,058,314! So, the benefits of applying pooling are obvious: avoiding overfitting
    and reducing training costs.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，卷积层的输出是三维的，其中前两个是特征图的维度，第三个是卷积层中使用的滤波器数量。在示例中，最大池化输出的大小（前两个维度）是其输入特征图的一半。特征图被池化层下采样。你可能想知道，如果去掉所有池化层，训练的参数会有多少。实际上，是4,058,314个！所以，应用池化的好处显而易见：避免过拟合并减少训练成本。
- en: You may wonder why the number of convolutional filters keeps increasing over
    the layers. Recall that each convolutional layer attempts to capture patterns
    of a specific hierarchy. The first convolutional layer captures low-level patterns,
    such as edges, dots, and curves. Then, the subsequent layers combine those patterns
    extracted in previous layers to form high-level patterns, such as shapes and contours.
    As we move forward in these convolutional layers, there are more and more combinations
    of patterns to capture in most cases. As a result, we need to keep increasing
    (or at least not decreasing) the number of filters in the convolutional layers.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道为什么卷积滤波器的数量在每一层中不断增加。回想一下，每个卷积层都试图捕捉特定层次结构的模式。第一层卷积捕捉低级模式，例如边缘、点和曲线。然后，后续的层将这些在前几层中提取的模式组合起来，形成更高级的模式，例如形状和轮廓。随着我们在卷积层中向前推进，在大多数情况下，捕捉的模式组合会越来越多。因此，我们需要不断增加（或至少不减少）卷积层中滤波器的数量。
- en: Fitting the CNN model
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟合CNN模型
- en: Now it’s time to train the model we just built.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候训练我们刚刚构建的模型了。
- en: 'First, we compile the model with Adam as the optimizer, cross-entropy as the
    loss function, and classification accuracy as the metric:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用Adam作为优化器，交叉熵作为损失函数，分类准确率作为评估指标来编译模型：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here, we employ the GPU for training, so we run `torch.device("cuda:0")` to
    specify the GPU device (the first device, with index 0) and allocate tensors on
    it. Opting for the CPU is a working, but comparatively slower option.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用GPU进行训练，因此我们运行`torch.device("cuda:0")`来指定GPU设备（第一个设备，索引为0）并在其上分配张量。选择CPU也是一种可行的选项，但相对较慢。
- en: 'Next, we train the model by defining the following function:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过定义以下函数来训练模型：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We will train the CNN model for 30 iterations and monitor the learning progress:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练CNN模型进行30次迭代并监控学习进度：
- en: '[PRE23]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We are able to achieve an accuracy of around 94% on the training set. If you
    want to check the performance on the test set, you can do the following:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够在训练集上达到约94%的准确率。如果你想查看在测试集上的表现，可以执行如下操作：
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The model achieves an accuracy of 90% on the test dataset. Note that this result
    may vary due to factors like differences in hidden layer initializations, or non-deterministic
    operations in GPUs.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在测试数据集上达到了90%的准确率。请注意，由于隐藏层初始化的差异，或GPU中的非确定性操作等因素，这一结果可能会有所不同。
- en: '**Best practice**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Operations that are better suited to execution on a GPU compared to a CPU typically
    involve parallelizable tasks that benefit from the massive parallelism and computational
    power offered by GPU architectures. Here are some examples:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 与CPU相比，更适合在GPU上执行的操作通常涉及可以并行化的任务，这些任务能够受益于GPU架构提供的大规模并行性和计算能力。以下是一些例子：
- en: Matrix and convolutional operations
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵和卷积操作
- en: Processing large batches of data simultaneously. Tasks that involve batch processing,
    such as training and inference on mini-batches in machine learning models, benefit
    from the parallel processing capabilities of GPUs.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时处理大量数据批次。涉及批处理的任务，如机器学习模型中小批次的训练和推理，受益于GPU的并行处理能力。
- en: Forward and backward propagation in neural networks, which are typically faster
    on GPUs due to hardware acceleration.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的前向传播和反向传播，通常由于硬件加速，在GPU上更快。
- en: '**Best practice**'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Operations that are better suited to execution on a CPU compared to a GPU typically
    involve less parallelizable tasks and require more sequential processing or small
    data sizes. Here are some examples:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 相比于GPU，适合在CPU上执行的操作通常涉及较少的可并行化任务，需要更多的顺序处理或小数据集。以下是一些示例：
- en: Preprocessing such as data loading, feature extraction, and data augmentation.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理工作，如数据加载、特征提取和数据增强。
- en: Inference on small models. For small models or inference tasks with low computational
    requirements, performing operations on the CPU can be more cost-effective.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小模型推理。对于小模型或计算要求较低的推理任务，在CPU上执行操作可能更具成本效益。
- en: Control flow operations. Operations involving conditional statements or loops
    are generally more efficient on the CPU due to its sequential processing nature.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制流操作。涉及条件语句或循环的操作通常在CPU上更高效，因为CPU具有顺序处理的特点。
- en: You have seen how the trained model performs, and you may wonder what the convolutional
    filters look like. You will find out in the next section.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了训练模型的表现，你可能会想知道卷积滤波器的样子。你将在下一节中了解。
- en: Visualizing the convolutional filters
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化卷积滤波器
- en: We extract the convolutional filters from the trained model and visualize them
    with the following steps.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从训练好的模型中提取卷积滤波器，并通过以下步骤进行可视化。
- en: 'From the model summary, we know that the layers of `conv1`, `conv2`, and `conv3`
    in the model are convolutional layers. Using the third convolutional layer as
    an example, we obtain its filters as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型摘要中，我们知道模型中的`conv1`、`conv2`和`conv3`层是卷积层。以第三个卷积层为例，我们获取它的滤波器如下：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It’s apparent that there are 128 filters, where each filter possesses dimensions
    of 3x3 and contains 64 channels.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，有128个滤波器，每个滤波器的尺寸为3x3，并且包含64个通道。
- en: 'Next, for simplification, we visualize only the first channel from the first
    16 filters in four rows and four columns:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，为了简化，我们仅可视化前16个滤波器中的第一个通道，排列为四行四列：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下截图以获取最终结果：
- en: '![A picture containing square, pattern, rectangle, symmetry  Description automatically
    generated](img/B21047_11_09.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含方形、图案、矩形、对称的图片 说明自动生成](img/B21047_11_09.png)'
- en: 'Figure 11.9: Trained convolutional filters'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：训练后的卷积滤波器
- en: In a convolutional filter, the dark squares represent small weights and the
    white squares indicate large weights. Based on this intuition, we can see that
    the second filter in the second row detects the vertical line in a receptive field,
    while the third filter in the first row detects a gradient from light at the bottom
    right to dark at the top left.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积滤波器中，深色方块代表较小的权重，白色方块表示较大的权重。根据这一直觉，我们可以看到第二行第二个滤波器检测到了接收域中的垂直线，而第一行第三个滤波器则检测到了从右下角的亮色到左上角的暗色的梯度。
- en: In the previous example, we trained the clothing image classifier with 60,000
    labeled samples. However, it is not easy to gather such a big labeled dataset
    in reality. Specifically, image labeling is expensive and time-consuming. How
    can we effectively train an image classifier with a limited number of samples?
    One solution is data augmentation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，我们用60,000个标注样本训练了服装图像分类器。然而，在实际中收集这么大的标注数据集并不容易。具体来说，图像标注既昂贵又耗时。如何在样本数量有限的情况下有效地训练图像分类器？一种解决方案是数据增强。
- en: Boosting the CNN classifier with data augmentation
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用数据增强提升CNN分类器性能
- en: '**Data augmentation** means expanding the size of an existing training dataset
    in order to improve the generalization performance. It overcomes the cost involved
    in collecting and labeling more data. In PyTorch, we use the `torchvision.transforms`
    module to implement image augmentation in real time.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据增强**意味着扩展现有训练数据集的大小，以提高泛化性能。它克服了收集和标注更多数据的成本。在PyTorch中，我们使用`torchvision.transforms`模块来实时实现图像增强。'
- en: Flipping for data augmentation
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于数据增强的翻转操作
- en: 'There are many ways to augment image data. The simplest one is probably flipping
    an image horizontally or vertically. For instance, we will have a new image if
    we flip an existing image horizontally. To create a horizontally flipped image,
    we utilize `transforms.functional.hflip`, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 增强图像数据的方法有很多，最简单的可能是水平或垂直翻转图像。例如，如果我们水平翻转一张现有图像，就会得到一张新图像。为了创建水平翻转的图像，我们使用`transforms.functional.hflip`，如下所示：
- en: '[PRE27]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Let’s take a look at the flipped image:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看翻转后的图像：
- en: '[PRE28]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下截图以查看最终结果：
- en: '![A screenshot of a computer screen  Description automatically generated](img/B21047_11_10.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图  描述自动生成](img/B21047_11_10.png)'
- en: 'Figure 11.10: Horizontally flipped image for data augmentation'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10：用于数据增强的水平翻转图像
- en: 'In training using data augmentation, we will create manipulated images using
    a random generator. For horizontal flipping, we will use `transforms.RandomHorizontalFlip`,
    which randomly flips images horizontally with a 50% chance, effectively augmenting
    the dataset. Let’s see three output samples:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据增强进行训练时，我们将使用随机生成器创建处理过的图像。对于水平翻转，我们将使用`transforms.RandomHorizontalFlip`，它以50%的概率随机水平翻转图像，从而有效地增强数据集。让我们看三个输出样本：
- en: '[PRE29]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下截图以查看最终结果：
- en: '![A close-up of a pair of shoes  Description automatically generated](img/B21047_11_11.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![一双鞋子的特写镜头  描述自动生成](img/B21047_11_11.png)'
- en: 'Figure 11.11: Randomly horizontally flipped images for data augmentation'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11：用于数据增强的随机水平翻转图像
- en: As you can see, the generated images are either horizontally flipped or not
    flipped.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，生成的图像要么是水平翻转的，要么没有翻转。
- en: In general, the horizontally flipped images convey the same message as the original
    ones. Vertically flipped images are not frequently seen, although you can generate
    them using `transforms.RandomVerticalFlip`. It is also worth noting that flipping
    only works in orientation-insensitive cases, such as classifying cats and dogs
    or recognizing parts of cars. On the contrary, it is dangerous to do so in cases
    where orientation matters, such as classifying between right and left turn signs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，水平翻转的图像传达的信息与原始图像相同。垂直翻转的图像不常见，尽管你可以使用`transforms.RandomVerticalFlip`来生成它们。值得注意的是，翻转仅适用于与方向无关的情况，比如分类猫和狗或识别汽车部件。相反，在方向很重要的情况下进行翻转是危险的，比如区分左转标志和右转标志。
- en: Rotation for data augmentation
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于数据增强的旋转
- en: 'Instead of rotating every 90 degrees as in horizontal or vertical flipping,
    a small-to-medium degree rotation can also be applied in image data augmentation.
    Let’s look at random rotation using `transforms`. We use `RandomRotation` in the
    following example:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与水平或垂直翻转每次旋转90度不同，可以在图像数据增强中应用小到中度的旋转。让我们看看使用`transforms`进行的随机旋转。在以下示例中，我们使用`RandomRotation`：
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下截图以查看最终结果：
- en: '![A close-up of a pair of images  Description automatically generated](img/B21047_11_12.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![一双鞋子的特写镜头  描述自动生成](img/B21047_11_12.png)'
- en: 'Figure 11.12: Rotated images for data augmentation'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12：用于数据增强的旋转图像
- en: In the preceding example, the image is rotated by any degree ranging from -20
    (counterclockwise) to 20 (clockwise).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的示例中，图像被旋转了从-20（逆时针方向）到20（顺时针方向）的任意角度。
- en: Cropping for data augmentation
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用于数据增强的裁剪
- en: Cropping is another commonly used augmentation method. It generates new images
    by selecting a segment of the original image. Typically, this process is accompanied
    by resizing the cropped area to a predetermined output size to ensure uniform
    dimensions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 裁剪是另一种常用的增强方法。它通过选择原始图像的一部分生成新图像。通常，这一过程伴随裁剪区域被调整为预定的输出尺寸，以确保尺寸一致。
- en: 'Now, let’s explore how to utilize `transforms.RandomResizedCrop` to randomly
    select the aspect ratio of the cropped section and subsequently resize the result
    to match the original dimensions:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索如何利用`transforms.RandomResizedCrop`随机选择裁剪区域的宽高比，并随后调整结果的大小以匹配原始尺寸：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, `size` specifies the size of the output image after cropping and resizing;
    `scale` defines the range of scaling for cropping. If set to (`min_scale`, `max_scale`),
    the crop area’s size will be randomly chosen to be between `min_scale` and `max_scale`
    times the original image’s size.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`size`指定裁剪和调整大小后输出图像的尺寸；`scale`定义了裁剪的缩放范围。如果设置为（`min_scale`，`max_scale`），则裁剪区域的大小会随机选择，范围在`min_scale`和`max_scale`倍的原始图像大小之间。
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下截图查看最终结果：
- en: '![A close-up of a pair of shoes  Description automatically generated](img/B21047_11_13.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![一双鞋的特写描述 自动生成](img/B21047_11_13.png)'
- en: 'Figure 11.13: Cropped images for data augmentation'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.13：用于数据增强的裁剪图像
- en: As you can see, `scale=(0.7, 1.0)` indicates that the crop area’s size can vary
    between 70% and 100% of the original image’s size.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`scale=(0.7, 1.0)`表示裁剪区域的大小可以在原始图像大小的70%到100%之间变化。
- en: Improving the clothing image classifier with data augmentation
  id: totrans-211
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过数据增强改进服装图像分类器
- en: 'Armed with several common augmentation methods, we will now apply them to train
    our image classifier on a small dataset in the following steps:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了几种常见的数据增强方法后，我们将在以下步骤中应用它们来训练我们的图像分类器，使用一个小型数据集：
- en: 'We start by constructing the transform function by combining all the data augmentation
    techniques we just discussed:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过结合刚才讨论的所有数据增强技术来构建转换函数：
- en: '[PRE32]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, we employ horizontal flip, rotation of up to 10 degrees, and cropping,
    with dimensions ranging from 90% to 100% of the original size.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用水平翻转、最多10度的旋转和裁剪，裁剪的尺寸范围在原始图像的90%到100%之间。
- en: 'We reload the training dataset with this transform function and only use 500
    samples for training:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重新加载训练数据集，使用这个转换函数，并仅使用500个样本进行训练：
- en: '[PRE33]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: We will see how data augmentation improves generalization and performance with
    a very small training set available.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看到数据增强如何在可用的非常小的训练集上提高泛化能力和性能。
- en: 'Load this small but augmented training set into batches of 64 samples as we
    did previously:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这个小型但增强过的训练集按64个样本一批加载，像我们之前做的那样：
- en: '[PRE34]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Note that even for the same original image, iterating using this data loader
    will produce different augmented images, which could be flipped, rotated, or cropped
    within the specified ranges.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，即使是同一原始图像，通过这个数据加载器进行迭代时，会生成不同的增强图像，可能会被翻转、旋转或在指定范围内裁剪。
- en: 'Next, we initialize the CNN model using the same architecture we used previously
    and the optimizer accordingly:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前相同的架构初始化CNN模型，并相应地初始化优化器：
- en: '[PRE35]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we train the model on the augmented small dataset:'
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们在增强后的小型数据集上训练模型：
- en: '[PRE36]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We train the model for 1,000 iterations.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对模型进行了1,000次迭代的训练。
- en: 'Let’s see how it performs on the test set:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看它在测试集上的表现如何：
- en: '[PRE37]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The model with data augmentation has a classification accuracy of 79.24% on
    the test set. Note that this result may vary.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据增强的模型在测试集上的分类准确率为79.24%。请注意，这个结果可能会有所不同。
- en: We also experimented with training without data augmentation, resulting in a
    test set accuracy of approximately 76%. When employing data augmentation, the
    accuracy improved to 79%. As always, feel free to fine-tune the hyperparameters
    as we did in *Chapter 6*, *Predicting Stock Prices with Artificial Neural Networks*,
    and see if you can further improve the classification performance.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还尝试了不使用数据增强进行训练，结果测试集的准确率大约为76%。当使用数据增强时，准确率提高到了79%。像往常一样，您可以像我们在*第6章*《使用人工神经网络预测股票价格》中所做的那样，随意调整超参数，看看能否进一步提高分类性能。
- en: Transfer learning is an alternative method to enhance the performance of a CNN
    classifier. Let’s proceed to the following section.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是提升CNN分类器性能的另一种方法。让我们继续下一部分。
- en: Advancing the CNN classifier with transfer learning
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过迁移学习推进CNN分类器
- en: '**Transfer learning** is a machine learning technique where a model trained
    on one task is adapted or fine-tuned for a second related task. In transfer learning,
    the knowledge acquired during the training of the first task (source task) is
    leveraged to improve the learning of the second task (target task). This can be
    particularly useful when you have limited data for the target task because it
    allows you to transfer knowledge from a larger or more diverse dataset.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**迁移学习**是一种机器学习技术，其中在一个任务上训练好的模型会被适配或微调用于第二个相关任务。在迁移学习中，第一次任务（源任务）训练期间获得的知识被用来改进第二次任务（目标任务）的学习。当目标任务的数据有限时，这尤其有用，因为它可以将来自更大或更具多样性的数据集的知识转移过来。'
- en: 'The typical workflow of transfer learning involves:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习的典型工作流程包括：
- en: '**Pretrained model**: Start with a pretrained model that has already been trained
    on a large and relevant dataset for a different but related task. This model is
    often a deep neural network, such as a CNN model for image tasks.'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预训练模型**：从一个已经在一个大规模且相关的数据集上针对不同但相关任务训练好的预训练模型开始。这个模型通常是一个深度神经网络，例如用于图像任务的CNN模型。'
- en: '**Feature Extraction**: Use the pretrained model as a feature extractor. Remove
    the final classification layers (if they exist) and use the output of one of the
    intermediate layers as a feature representation for your data. These features
    can capture high-level patterns and information from the source task.'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征提取**：使用预训练模型作为特征提取器。移除最后的分类层（如果存在），并使用其中一个中间层的输出作为数据的特征表示。这些特征可以捕捉到源任务中的高级模式和信息。'
- en: '**Fine-Tuning**: Add new layers to the feature extractor. These new layers
    are specific to your target task and are typically randomly initialized. You then
    train the entire model, including the feature extractor and the new layers, on
    your target dataset. Fine-tuning allows the model to adapt to the specifics of
    the target task.'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**微调**：在特征提取器上添加新的层。这些新层是针对目标任务的，通常会随机初始化。然后，您将整个模型，包括特征提取器和新层，在目标数据集上进行训练。微调使得模型能够适应目标任务的具体要求。'
- en: Prior to implementing transfer learning for our clothing image classification
    task, let’s begin by exploring the evolution of CNN architectures and pretrained
    models. Even the early CNN architecture is still actively used today! The key
    point here is that all of these architectures are valuable tools in the modern
    DL toolbox, particularly when employed for transfer learning tasks.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现迁移学习进行我们的服装图像分类任务之前，让我们先来探索CNN架构和预训练模型的发展历程。即使是早期的CNN架构，今天仍然在积极使用！这里的关键是，所有这些架构都是现代深度学习工具箱中的宝贵工具，特别是在进行迁移学习任务时。
- en: Development of CNN architectures and pretrained models
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: CNN架构和预训练模型的发展
- en: The concept of CNNs for image processing dates back to the 1990s. Early architectures
    like **LeNet-5** (1998) demonstrated the potential of deep neural networks for
    image classification. LeNet-5 consists of two sets of convolutional layers, followed
    by two fully connected layers and one output layer. Each convolutional layer uses
    5x5 kernels. LeNet-5 played a significant role in demonstrating the effectiveness
    of deep learning for image classification tasks. It was able to achieve high accuracy
    on the MNIST dataset, a widely used benchmark dataset for handwritten digit recognition.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 用于图像处理的CNN概念可以追溯到1990年代。早期的架构如**LeNet-5**（1998年）展示了深度神经网络在图像分类中的潜力。LeNet-5由两组卷积层组成，后跟两层全连接层和一层输出层。每个卷积层使用5x5的卷积核。LeNet-5在图像分类任务中起到了重要作用，展示了深度学习的有效性。它在MNIST数据集上取得了高精度，MNIST是一个广泛使用的手写数字识别基准数据集。
- en: The achievements of LeNet-5 paved the way for the creation of more complex architectures,
    such as **AlexNet** (2012). It consists of eight layers – five sets of convolutional
    layers followed by three fully connected layers. It used a ReLU activation function
    for the first time in a deep CNN and utilized dropout in the fully connected layers
    to prevent overfitting. Data augmentation techniques, such as random cropping
    and horizontal flipping, were employed to improve the model’s generalization.
    The success of AlexNet triggered a renewed interest in neural networks and led
    to the development of even deeper and more complex architectures, including VGGNet,
    GoogLeNet, and ResNet, which have become foundational in computer vision.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet-5的成就为更复杂架构的创建铺平了道路，例如**AlexNet**（2012年）。它由八层组成——五组卷积层后接三层全连接层。它首次在深度CNN中使用了ReLU激活函数，并在全连接层中采用了dropout技术以防止过拟合。使用了数据增强技术，如随机裁剪和水平翻转，来提高模型的泛化能力。AlexNet的成功引发了对神经网络的重新关注，并促成了更深层次和更复杂架构的发展，包括VGGNet、GoogLeNet和ResNet，这些架构在计算机视觉中成为了基础。
- en: '**VGGNet** was introduced by the Visual Geometry Group at the University of
    Oxford in 2014\. VGGNet follows a straightforward and uniform architecture. It
    consists of a series of convolutional layers, followed by max-pooling layers,
    with a stack of fully connected layers at the end. It primarily uses 3x3 convolutional
    filters, allowing the network to capture fine-grained spatial information. The
    most commonly used versions are VGG16 and VGG19, which have 16 and 19 layers in
    the network. They are often used as a starting point for transfer learning in
    various computer vision tasks.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**VGGNet**由牛津大学的视觉几何小组于2014年提出。VGGNet遵循一个简单且统一的架构，由一系列卷积层组成，后面跟着最大池化层，最后是堆叠的全连接层。它主要使用3x3卷积滤波器，能够捕捉细粒度的空间信息。最常用的版本是VGG16和VGG19，它们分别有16层和19层，常作为各种计算机视觉任务中的迁移学习起点。'
- en: In the same year, **GoogLeNet**, better known as **Inception**, was developed
    by Google. The hallmark of GoogLeNet is the inception module. Instead of using
    a single convolutional layer with a fixed filter size, inception modules use multiple
    filter sizes (1x1, 3x3, 5x5) in parallel. These parallel operations capture features
    at different scales and provide richer representations. Similar to VGGNet, pretrained
    GoogLeNet comes in different versions, such as InceptionV1, InceptionV2, InceptionV3,
    and Inception V4, each with variations and improvements in architecture.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 同年，**GoogLeNet**，更广为人知的**Inception**，由谷歌开发。GoogLeNet的标志性特点是Inception模块。与使用固定滤波器大小的单个卷积层不同，Inception模块并行使用多种滤波器大小（1x1、3x3、5x5）。这些并行操作可以在不同的尺度上捕捉特征，从而提供更丰富的表示。与VGGNet类似，预训练的GoogLeNet也有多个版本，例如InceptionV1、InceptionV2、InceptionV3和InceptionV4，每个版本在架构上有所变化和改进。
- en: '**ResNet**, short for **Residual Network**, was introduced by Kaiming He et
    al. in 2015, to address the vanishing gradient problem – gradients of the loss
    function becoming extremely small in CNNs. Its core innovation is the use of residual
    connections. These blocks allow the network to skip certain layers during training.
    Instead of directly learning the desired mapping from input to output, residual
    blocks learn a residual mapping, which is added to the original input. Deeper
    networks were made possible this way. Its pretrained models come in various versions,
    such as ResNet-18, ResNet-34, ResNet-50, ResNet-101, and the extremely deep ResNet-152\.
    Again, the numbers denote the depth of the network.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '**ResNet**，即**残差网络**，由Kaiming He等人在2015年提出，旨在解决消失梯度问题——在CNN中，损失函数的梯度变得极其微小。其核心创新是使用残差连接。这些模块允许网络在训练过程中跳过某些层。残差模块不是直接学习从输入到输出的目标映射，而是学习一个残差映射，该映射与原始输入相加。通过这种方式，深层网络成为可能。它的预训练模型有多个版本，例如ResNet-18、ResNet-34、ResNet-50、ResNet-101以及极深的ResNet-152。再次强调，数字表示网络的深度。'
- en: The development of CNN architectures and pretrained models continues with innovations
    like EfficientNet, MobileNet, and custom architectures for specific tasks. For
    instance, **MobileNet** models are designed to be highly efficient in terms of
    computational resources and memory usage. They are tailored for deployment on
    devices with limited hardware capabilities, such as smartphones, IoT devices,
    and edge devices.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构和预训练模型的开发仍在继续，随着像EfficientNet、MobileNet等创新的出现，以及针对特定任务的定制架构。例如，**MobileNet**模型设计时注重高效的计算资源和内存使用。它们专门为在硬件能力有限的设备上部署而优化，如智能手机、物联网设备和边缘设备。
- en: 'You can see all available pretrained models in PyTorch on this page: https://pytorch.org/vision/stable/models.html#classification.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此页面查看所有可用的预训练模型：[PyTorch预训练模型](https://pytorch.org/vision/stable/models.html#classification)。
- en: The evolution of CNN architectures and the availability of pretrained models
    have revolutionized computer vision tasks. They have significantly improved the
    state of the art in image classification, object detection, segmentation, and
    many other applications.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: CNN架构的演变以及预训练模型的出现彻底改变了计算机视觉任务。它们显著提高了图像分类、目标检测、分割以及许多其他应用的技术水平。
- en: Now, let’s explore using a pretrained model to enhance our clothing image classifier.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索使用预训练模型来增强我们的服装图像分类器。
- en: Improving the clothing image classifier by fine-tuning ResNets
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过微调ResNet来改进服装图像分类器
- en: 'We will use the pre-trained ResNets, ResNet-18 to be specific, for transfer
    learning in the following steps:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的步骤中使用预训练的ResNet，具体是ResNet-18进行迁移学习：
- en: 'We start by importing the pretrained ResNet-18 model from `torchvision`:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从`torchvision`导入预训练的ResNet-18模型：
- en: '[PRE38]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Here, `IMAGENET1K` refers to the pretrained model that was trained on the ImageNet-1K
    dataset ([https://www.image-net.org/download.php](https://www.image-net.org/download.php))
    and `V1` refers to version 1 of the pretrained model.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`IMAGENET1K`指的是在ImageNet-1K数据集上训练的预训练模型（[https://www.image-net.org/download.php](https://www.image-net.org/download.php)），而`V1`指的是预训练模型的版本1。
- en: This is the pretrained model step.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预训练模型步骤。
- en: 'Since the ImageNet-1K dataset comprises RGB images, the first convolutional
    layer in the original `ResNet` is designed for three-dimensional inputs. However,
    our `FashionMNIST` dataset contains grayscale images, so we need to modify it
    to accept one-dimensional inputs:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于ImageNet-1K数据集包含RGB图像，原始`ResNet`中的第一卷积层设计用于三维输入。然而，我们的`FashionMNIST`数据集包含灰度图像，因此我们需要修改它以接受一维输入：
- en: '[PRE39]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We just change the first argument, the input dimension, from 3 to 1 in the
    original definition of the first convolutional:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只是将原始定义中第一个卷积层的第一个参数——输入维度——从3改为1：
- en: '[PRE40]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Change the output layer to output 10 classes from 1,000 classes:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输出层改为输出10个类别而不是1,000个类别：
- en: '[PRE41]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Here, we only update the output size of the output layer.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只更新输出层的输出大小。
- en: Steps 2 and 3 prepare for the **fine-tuning** process.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步和第3步为**微调**过程做准备。
- en: 'Finally, we **fine-tune** the adapted pretrained model by training it on the
    full training set:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过在完整训练集上训练，**微调**了已适配的预训练模型：
- en: '[PRE42]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: After only 10 iterations, an accuracy of 94% is achieved with the fine-tuned
    ResNet model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 仅经过10次迭代，经过微调的ResNet模型就达到了94%的准确率。
- en: 'How about its performance on the test set? Let’s see the following:'
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它在测试集上的表现如何？让我们看一下以下结果：
- en: '[PRE43]'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We are able to boost the accuracy on the test set from 90% to 91%, with only
    10 training iterations.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过仅进行10次训练迭代，我们能够将测试集的准确率从90%提升到91%。
- en: Transfer learning with CNNs is a powerful technique that allows you to leverage
    pretrained models and adapt them for your specific image classification tasks.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CNN进行迁移学习是一种强大的技术，可以让你利用预训练模型，并将其调整为特定的图像分类任务。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we worked on classifying clothing images using CNNs. We started
    with a detailed explanation of the individual components of a CNN model and learned
    how CNNs are inspired by the way our visual cells work. We then developed a CNN
    model to categorize fashion-MNIST clothing images from Zalando. We also talked
    about data augmentation and several popular image augmentation methods. We practiced
    transfer learning with ResNets, after discussing the evolution of CNN architectures
    and pretrained models.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们使用CNN对服装图像进行分类。我们从详细解释CNN模型的各个组成部分开始，学习了CNN是如何受到我们视觉细胞工作方式的启发。然后，我们开发了一个CNN模型，用于分类Zalando的Fashion-MNIST服装图像。我们还讨论了数据增强和几种流行的图像增强方法。在讨论了CNN架构和预训练模型的演变之后，我们通过ResNets进行了迁移学习的实践。
- en: 'In the next chapter, we will focus on another type of deep learning network:
    **Recurrent Neural Networks** (**RNNs**). CNNs and RNNs are the two most powerful
    deep neural networks that make deep learning so popular nowadays.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将重点介绍另一种类型的深度学习网络：**循环神经网络**（**RNNs**）。CNN和RNN是目前最强大的两种深度神经网络，使得深度学习在如今如此受欢迎。
- en: Exercises
  id: totrans-273
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: As mentioned before, can you try to fine-tune the CNN image classifier and see
    if you can beat what we have achieved?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，你能尝试微调CNN图像分类器并看看能否超越我们所取得的成绩吗？
- en: Can you also employ the dropout technique to improve the CNN model?
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你也能使用dropout技术来改进CNN模型吗？
- en: 'Can you experiment with using the pretrained Vision Transformer model: [https://huggingface.co/google/vit-base-patch16-224?](https://huggingface.co/google/vit-base-patch16-224?)'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能尝试使用预训练的视觉Transformer模型吗：[https://huggingface.co/google/vit-base-patch16-224?](https://huggingface.co/google/vit-base-patch16-224?)
- en: Join our book’s Discord space
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者一起讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
