- en: '*Chapter 6*: AWS Services for Data Processing'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：用于数据处理的服务'
- en: In the previous chapter, we learned about several ways of storing data in AWS.
    In this chapter, we will explore the techniques for using that data and gaining
    some insight from the data. There are use cases where you have to process your
    data or load the data to a hive data warehouse to query and analyze the data.
    If you are on AWS and your data is in S3, then you have to create a table in hive
    on AWS EMR to query them. To provide the same as a managed service, AWS has a
    product called Athena, where you have to create a data catalog and query your
    data on S3\. If you need to transform the data, then AWS Glue is the best option
    to transform and restore it to S3\. Let's imagine a use case where we need to
    stream the data and create analytical reports on that data. For such scenarios,
    we can opt for AWS Kinesis Data Streams to stream data and store it in S3\. Using
    Glue, the same data can be copied to Redshift for further analytical utilization.
    Now, let's learn about them and we will cover the following in brief.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了在AWS中存储数据的几种方法。在本章中，我们将探讨使用这些数据并从数据中获得一些洞察的方法。在某些用例中，您必须处理数据或将数据加载到Hive数据仓库以查询和分析数据。如果您在AWS上，并且数据存储在S3中，那么您必须在AWS
    EMR上创建一个hive表来查询它们。为了提供相同的管理服务，AWS有一个名为Athena的产品，您需要创建一个数据目录并在S3上查询数据。如果您需要转换数据，那么AWS
    Glue是转换并恢复到S3的最佳选择。让我们设想一个需要流式传输数据并在此数据上创建分析报告的用例。对于此类场景，我们可以选择AWS Kinesis Data
    Streams来流式传输数据并将其存储在S3中。使用Glue，相同的数据可以被复制到Redshift以进行进一步的分析利用。现在，让我们了解它们，我们将简要介绍以下内容。
- en: Using Glue for designing ETL jobs
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Glue设计ETL作业
- en: Querying S3 data using Athena
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Athena查询S3数据
- en: Streaming data through AWS Kinesis Data Streams and storing it using Kinesis
    Firehose
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过AWS Kinesis Data Streams流式传输数据并使用Kinesis Firehose进行存储
- en: Ingesting data from on-premises locations to AWS
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从本地位置摄取数据到AWS
- en: Migrating data to AWS and extending on-premises data centers to AWS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据迁移到AWS并扩展本地数据中心到AWS
- en: Processing data on AWS
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在AWS上处理数据
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can download the data used in the examples from GitHub, available here:
    [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6).'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从GitHub下载示例中使用的数据，链接如下：[https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6)。
- en: Creating ETL jobs on AWS Glue
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS Glue上创建ETL作业
- en: 'In a modern data pipeline, there are multiple stages, such as Generate Data,
    Collect Data, Store Data, Perform ETL, Analyze, and Visualize. In this section,
    we will cover each of these at a high level and understand the **ETL** (**extract**,
    **transform**, **load**) part in-depth:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在现代数据管道中，存在多个阶段，例如生成数据、收集数据、存储数据、执行ETL、分析和可视化。在本节中，我们将从高层次概述这些阶段，并深入理解**ETL**（**提取**、**转换**、**加载**）部分：
- en: Data can be generated from several devices, including mobile devices or IoT,
    weblogs, social media, transactional data, online games, and many more besides.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以由多个设备生成，包括移动设备或物联网设备、网络日志、社交媒体、交易数据、在线游戏等。
- en: This huge amount of generated data can be collected by using polling services
    or through API gateways integrated with AWS Lambda to collect the data, or via
    streams such as AWS Kinesis or AWS-managed Kafka or Kinesis Firehose. If you have
    an on-premises database and you want to collect that data to AWS, then you choose
    AWS DMS for that. You can sync your on-premises data to Amazon S3, Amazon EFS,
    or Amazon FSx via AWS DataSync. AWS Snowball is used to collect/transfer data
    into and out of AWS.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这么大的数据量可以通过使用轮询服务或通过集成AWS Lambda的API网关来收集数据，或者通过AWS Kinesis或AWS管理的Kafka或Kinesis
    Firehose等流来收集。如果您有一个本地数据库，并且希望将数据收集到AWS中，那么您可以选择AWS DMS来完成这项工作。您可以通过AWS DataSync将本地数据同步到Amazon
    S3、Amazon EFS或Amazon FSx。AWS Snowball用于收集/传输数据到和从AWS中。
- en: The next step involves storing data, and we have learned some of the services
    in the previous chapter, such as AWS S3, EBS, EFS, Amazon RDS, Amazon Redshift,
    and DynamoDB.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一步涉及存储数据，我们在上一章中学习了一些服务，例如AWS S3、EBS、EFS、Amazon RDS、Amazon Redshift和DynamoDB。
- en: Once we know our data storage, an ETL job can be designed to extract-transform-load
    or extract-load-transform our structured or unstructured data into our desired
    format for further analysis. For example, we can think of AWS Lambda to transform
    the data on the fly and store the transformed data into S3, or we can run a Spark
    application on an EMR cluster to transform the data and store it in S3 or Redshift
    or RDS.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦我们了解了我们的数据存储，就可以设计一个ETL作业来提取-转换-加载或提取-加载-转换我们的结构化或非结构化数据，以便进行进一步分析。例如，我们可以考虑使用AWS
    Lambda在实时转换数据并将其存储到S3中，或者我们可以在EMR集群上运行Spark应用程序来转换数据，并将其存储在S3、Redshift或RDS中。
- en: There are many services available in AWS for performing an analysis on transformed
    data; for example, Amazon EMR, Amazon Athena, Amazon Redshift, Amazon Redshift
    Spectrum, and Kinesis Analytics.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS中提供了许多服务用于对转换后的数据进行分析；例如，亚马逊EMR、亚马逊雅典娜、亚马逊Redshift、亚马逊Redshift Spectrum和Kinesis
    Analytics。
- en: Once the data is analyzed, you can visualize the data using AWS Quicksight to
    understand the pattern or trend. Data scientists or machine learning professionals
    would love to apply statistical analysis to understand data distribution in a
    better way. Business users use it to prepare reports. We have already learned
    various ways to present and visualize data in [*Chapter 4*](B16735_04_Final_VK_ePub.xhtml#_idTextAnchor082),
    *Understanding and Visualizing Data*.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据分析完成后，您可以使用AWS Quicksight可视化数据，以了解模式或趋势。数据科学家或机器学习专业人员会喜欢应用统计分析来更好地理解数据分布。商业用户使用它来准备报告。我们已经在[*第4章*](B16735_04_Final_VK_ePub.xhtml#_idTextAnchor082)，“理解和可视化数据”中学习了各种展示和可视化数据的方法。
- en: What we understood from the traditional data pipeline is that ETL is all about
    coding and maintaining code on the servers to run smoothly. If the data format
    changes in any way, then the code needs to be changed, and that results in a change
    to the target schema. If the data source changes, then the code must be able to
    handle that too and it's an overhead. *Should we write code to recognize these
    changes in data sources? Should we need a system to adapt to the change and discover
    the data for us?* The answer is **AWS Glue**. Now, let's understand why AWS Glue
    is so famous.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 从传统的数据管道中，我们了解到ETL完全是关于在服务器上编码和维护代码以确保其顺利运行。如果数据格式有任何变化，那么代码就需要进行更改，这会导致目标模式的变化。如果数据源发生变化，代码也必须能够处理这种情况，这会带来额外的开销。*我们应该编写代码来识别数据源中的这些变化吗？我们需要一个系统来自适应变化并为我们发现数据吗？*
    答案是**AWS Glue**。现在，让我们了解为什么AWS Glue如此著名。
- en: Features of AWS Glue
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Glue的特性
- en: 'AWS Glue is a completely managed serverless ETL service on AWS. It has the
    following features:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue是AWS上完全托管的无服务器ETL服务。它具有以下特性：
- en: It automatically discovers and categorizes your data by connecting to the data
    sources and generates a data catalog.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过连接到数据源自动发现和分类您的数据，并生成数据目录。
- en: Services such as Amazon Athena, Amazon Redshift, and Amazon EMR can use the
    data catalog to query the data.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊雅典娜、亚马逊Redshift和亚马逊EMR等服务可以使用数据目录来查询数据。
- en: AWS Glue generates the ETL code, which is an extension to Spark in Python or
    Scala, which can be modified, too.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Glue生成ETL代码，这是Spark在Python或Scala中的扩展，也可以进行修改。
- en: It scales out automatically to match your Spark application requirements for
    running the ETL job and loading the data into the destination.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它会自动扩展以匹配运行ETL作业和将数据加载到目标位置所需的Spark应用程序要求。
- en: 'AWS Glue has **Data Catalog**, and that''s the secret of its success. It helps
    to discover the data from data sources and lets us understand a bit about it:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue拥有**数据目录**，这是其成功的关键。它有助于从数据源中发现数据，并让我们对其有所了解：
- en: Data Catalog automatically discovers new data and extracts schema definitions.
    It detects schema changes and version tables. It detects Apache Hive-style partitions
    on Amazon S3.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录自动发现新数据并提取模式定义。它检测模式更改和版本表。它检测Amazon S3上的Apache Hive风格的分区。
- en: Data Catalog comes with built-in classifiers for popular data types. Custom
    classifiers can be written using **Grok expressions**. The classifiers help to
    detect the schema.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据目录自带用于流行数据类型的内置分类器。可以使用**Grok表达式**编写自定义分类器。分类器有助于检测模式。
- en: Glue crawlers can be run ad hoc or in a scheduled fashion to update the metadata
    in Glue Data Catalog. Glue crawlers must be associated with an IAM role with sufficient
    access to read the data sources, such as Amazon RDS, Amazon Redshift, and Amazon
    S3.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Glue爬虫可以按需或按计划运行以更新Glue数据目录中的元数据。Glue爬虫必须与一个IAM角色关联，该角色有足够的权限读取数据源，例如Amazon
    RDS、Amazon Redshift和Amazon S3。
- en: As we now have a brief idea of AWS Glue, let's run the following example to
    get our hands dirty.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们对AWS Glue有了简要的了解，让我们运行以下示例来亲自动手。
- en: Getting hands-on with AWS Glue data catalog components
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熟练使用AWS Glue数据目录组件
- en: 'In this example, we will create a job to copy data from S3 to Redshift by using
    AWS Glue. All my components are created in the `us-east-1` region. Let''s start
    by creating a bucket:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将创建一个作业，使用AWS Glue将数据从S3复制到Redshift。我所有的组件都创建在`us-east-1`区域。让我们先创建一个存储桶：
- en: Navigate to AWS S3 Console and create a bucket. I have named the bucket `aws-glue-example-01`.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到AWS S3控制台并创建一个存储桶。我已将该存储桶命名为`aws-glue-example-01`。
- en: Click on `input-data`.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`input-data`。
- en: 'Navigate inside the folder and click on the `sales-records.csv` dataset. The
    data is available in the following GitHub location: [https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data).'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在文件夹内导航并点击`sales-records.csv`数据集。数据可在以下GitHub位置找到：[https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data](https://github.com/PacktPublishing/AWS-Certified-Machine-Learning-Specialty-MLS-C01-Certification-Guide/tree/master/Chapter-6/AWS-Glue-Demo/input-data)。
- en: As we have the data uploaded in the S3 bucket, let's create a VPC in which we
    will create our Redshift cluster.
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 由于我们已经将数据上传到S3存储桶，让我们创建一个VPC，在其中我们将创建我们的Redshift集群。
- en: Navigate to the VPC console by accessing the [https://console.aws.amazon.com/vpc/home?region=us-east-1#](https://console.aws.amazon.com/vpc/home?region=us-east-1#)
    URL and click on `AWS services`
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过访问[https://console.aws.amazon.com/vpc/home?region=us-east-1#](https://console.aws.amazon.com/vpc/home?region=us-east-1#)
    URL导航到VPC控制台并点击`AWS services`。
- en: b) `com.amazonaws.us-east-1.s3` (Gateway type)
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `com.amazonaws.us-east-1.s3` (网关类型)
- en: c) `Select the Default VPC` (we will use this default VPC in which our Redshift
    cluster will be created)
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `选择默认VPC`（我们将使用此默认VPC创建Redshift集群）
- en: Leave the other fields as-is and click on **Create Endpoint**.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持其他字段不变并点击**创建端点**。
- en: Click on `redshift-self`, and choose the default VPC drop-down menu. Provide
    an appropriate description of `Redshift Security Group`. Click on **Create security
    group**.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`redshift-self`，并选择默认VPC下拉菜单。为`Redshift安全组`提供一个适当的描述。点击**创建安全组**。
- en: Click on the `All traffic`
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`所有流量`
- en: b) `Custom`
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `自定义`
- en: c) In the search field, select the same security group `(redshift-self)`
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在搜索字段中，选择相同的`安全组`(redshift-self)
- en: Click on **Save Rules**.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存规则**。
- en: Now, let's create our Redshift cluster.
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的Redshift集群。
- en: Navigate to the Amazon Redshift console. Click on **Create Cluster** and complete
    the highlighted fields, as shown in *Figure 6.1*:![Figure 6.1 – A screenshot of
    Amazon Redshift's Create cluster
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到Amazon Redshift控制台。点击**创建集群**并填写高亮字段，如图*图6.1*所示：![图6.1 – Amazon Redshift创建集群的屏幕截图
- en: '](img/B16735_06_01.jpg)'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16735_06_01.jpg)'
- en: Figure 6.1 – A screenshot of Amazon Redshift's Create cluster
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.1 – Amazon Redshift创建集群的屏幕截图
- en: Scroll down and fill in the highlighted fields, as shown in *Figure 6.2*:![Figure
    6.2 – A screenshot of Amazon Redshift Cluster's Database configurations section
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并填写高亮字段，如图*图6.2*所示：![图6.2 – Amazon Redshift集群数据库配置部分的屏幕截图
- en: '](img/B16735_06_02.jpg)'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16735_06_02.jpg)'
- en: Figure 6.2 – A screenshot of Amazon Redshift Cluster's Database configurations
    section
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.2 – Amazon Redshift集群数据库配置部分的屏幕截图
- en: Scroll down and change the **Additional configurations** field, as shown in
    *Figure 6.3*:![Figure 6.3 – A screenshot of Amazon Redshift Cluster's Additional
    configurations section
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并更改**附加配置**字段，如图*图6.3*所示：![图6.3 – Amazon Redshift集群附加配置部分的屏幕截图
- en: '](img/B16735_06_03.jpg)'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16735_06_03.jpg)'
- en: Figure 6.3 – A screenshot of Amazon Redshift Cluster's Additional configurations
    section
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.3 – Amazon Redshift集群附加配置部分的屏幕截图
- en: Change the IAM permissions, too, as shown in *Figure 6.4*:![Figure 6.4 – A screenshot
    of Amazon Redshift Cluster's Cluster permissions section
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还要更改IAM权限，如图*图6.4*所示：![图6.4 – Amazon Redshift集群集群权限部分的屏幕截图
- en: '](img/B16735_06_04.jpg)'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16735_06_04.jpg)'
- en: Figure 6.4 – A screenshot of Amazon Redshift Cluster's Cluster permissions section
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.4 – Amazon Redshift 集群的集群权限部分截图
- en: Scroll down and click on **Create Cluster**. It will take a minute or two to
    get the cluster in the available state.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 滚动并点击 **创建集群**。集群将在一分钟或两分钟内变为可用状态。
- en: Next, we will create an IAM role.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个 IAM 角色。
- en: Navigate to the AWS IAM console and select **Roles** in the **Access Management**
    section on the screen.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 AWS IAM 控制台，并在屏幕上的 **访问管理** 部分选择 **角色**。
- en: 'Click on the **Create role** button and choose **Glue** from the services.
    Click on the **Next: permissions** button to navigate to the next page.'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **创建角色** 按钮，并从服务中选择 **Glue**。点击 **下一步：权限** 按钮跳转到下一页。
- en: 'Search **AmazonS3FullAccess** and select. Then, search **AWSGlueServiceRole**
    and select. As we are writing our data to Redshift as part of this example, select
    **AmazonRedshiftFullAccess**. Click on **Next: Tags**, followed by the **Next:
    Review** button.'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搜索 **AmazonS3FullAccess** 并选择。然后，搜索 **AWSGlueServiceRole** 并选择。由于我们在这个示例中将数据写入
    Redshift，因此选择 **AmazonRedshiftFullAccess**。点击 **下一步：标签**，然后点击 **下一步：审查** 按钮。
- en: Provide a name, `Glue-IAM-Role`, and then click on the **Create role** button.
    The role appears as shown in *Figure 6.5*:![Figure 6.5 – A screenshot of the IAM
    role
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提供一个名称，`Glue-IAM-Role`，然后点击 **创建角色** 按钮。角色将如图 6.5 所示出现：![图 6.5 – IAM 角色截图
- en: '](img/B16735_06_05.jpg)'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16735_06_05.jpg)'
- en: Figure 6.5 – A screenshot of the IAM role
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 6.5 – IAM 角色截图
- en: Now, we have the input data source and the output data storage handy. The next
    step is to create the Glue crawler from the AWS Glue console.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，我们已经有了输入数据源和输出数据存储，下一步是从 AWS Glue 控制台创建 Glue 爬虫。
- en: Select `glue-redshift-connection`
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `glue-redshift-connection`
- en: b) `Amazon Redshift`
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `Amazon Redshift`
- en: Click on `redshift-glue-example`
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `redshift-glue-example`
- en: b) `glue-dev`
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `glue-dev`
- en: c) `awsuser`
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `awsuser`
- en: d) `********` (enter the value as created in *step 10*)
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `********`（输入在第 10 步中创建的值）
- en: Click on `Glue-IAM-Role` for the IAM role section, and then click **Test Connection**.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 IAM 角色部分的 `Glue-IAM-Role`，然后点击 **测试连接**。
- en: Go to `s3-glue-crawler`, and then click **Next**. In the **Specify crawler source
    type** page, leave everything as their default settings and then click **Next**.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往 `s3-glue-crawler`，然后点击 **下一步**。在 **指定爬虫源类型** 页面上，保持所有默认设置，然后点击 **下一步**。
- en: On `s3://aws-glue-example-01/input-data/sales-records.csv`.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `s3://aws-glue-example-01/input-data/sales-records.csv`。
- en: Click **Next**.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **下一步**。
- en: '`No`. Click **Next**.'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`No`。点击 **下一步**。'
- en: '`Glue-IAM-Role`. Then click **Next**.'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Glue-IAM-Role`。然后点击 **下一步**。'
- en: '`Run on demand`. Click **Next**.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Run on demand`。点击 **下一步**。'
- en: There's no database created, so click on `s3-data`, click **Next**, and then
    click **Finish**.
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 没有创建数据库，因此点击 `s3-data`，点击 **下一步**，然后点击 **完成**。
- en: Select the crawler, `s3-glue-crawler`, and then click on `1` in the `s3-data`,
    has been created, as mentioned in the previous step, and a table is added. Click
    on `sales_records_csv`. You can see that the schema has been discovered now. You
    can change the data type if the inferred data type does not meet your requirements.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择爬虫 `s3-glue-crawler`，然后点击 `s3-data` 中的 `1`，如前所述，已创建，并添加了一个表。点击 `sales_records_csv`。现在可以看到已经发现了模式。如果推断的数据类型不符合您的需求，您可以更改数据类型。
- en: In this hands-on section, we learned about database tables, database connections,
    crawlers on S3, and creation of the Redshift cluster. In the next hands-on section,
    we will learn about creating ETL jobs using Glue.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节动手实践中，我们学习了数据库表、数据库连接、S3 上的爬虫以及 Redshift 集群的创建。在下一节动手实践中，我们将学习如何使用 Glue 创建
    ETL 作业。
- en: Getting hands-on with AWS Glue ETL components
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Glue ETL 组件的动手实践
- en: 'In this section, we will use the Data Catalog components created earlier to
    build our job. Let''s start by creating jobs:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用之前创建的数据目录组件来构建我们的作业。让我们先创建作业：
- en: Navigate to the AWS Glue console and click on **Jobs** under the **ETL** section.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到 AWS Glue 控制台，然后在 **ETL** 部分的 **作业** 下点击。
- en: Click on the `s3-glue-redshift`
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 `s3-glue-redshift`
- en: '`Glue-IAM-Role` (this is the same role we created in the previous section)'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Glue-IAM-Role`（这是我们在上一节中创建的角色）'
- en: '`Spark`'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Spark`'
- en: '`Spark 2.4, Python 3 with improved job start up times (Glue version 2.0)`'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`Spark 2.4, Python 3 with improved job start up times (Glue version 2.0)`'
- en: Leave the other fields as they are and then click on **Next**.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 保持其他字段不变，然后点击 **下一步**。
- en: Select `sales_records_csv` and click on **Next**.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 `sales_records_csv` 并点击 **下一步**。
- en: Select **Change Schema** by default and then click **Next** (at the time of
    writing this book, machine learning transformations are not supported for Glue
    2.0).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认选择**更改模式**，然后点击**下一步**（在撰写本书时，Glue 2.0不支持机器学习转换）。
- en: Select `JDBC` as the data store and `glue-redshift-connection` as the connection.
    Provide `glue-dev` as the database name (as created in the previous section) and
    then click **Next**.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 默认选择`JDBC`作为数据存储，并选择`glue-redshift-connection`作为连接。提供`glue-dev`作为数据库名称（如前节中创建的），然后点击**下一步**。
- en: Next comes the **Output Schema Definition** page, where you can choose the desired
    columns to be removed from the target schema. Scroll down and click on **Save
    job and edit script**.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来是**输出模式定义**页面，你可以选择要从目标模式中删除的所需列。向下滚动并点击**保存作业并编辑脚本**。
- en: You can now see the pipeline being created on the left-hand side of the screen
    and the suggested code on the right-hand side, as shown in *Figure 6.6*. You can
    modify the code based on your requirements. Click on the **Run job** button. A
    pop-up window appears, asking you to edit any details that you wish to change.
    This is optional. Then, click on the **Run job** button:![Figure 6.6 – A screenshot
    of the AWS Glue ETL job
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在可以看到屏幕左侧正在创建的管道和右侧的建议代码，如图6.6所示。你可以根据你的需求修改代码。点击**运行作业**按钮。会出现一个弹出窗口，要求你编辑任何你希望更改的细节。这是可选的。然后，点击**运行作业**按钮：![图6.6
    – AWS Glue ETL作业的截图
- en: '](img/B16735_06_06.jpg)'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![img/B16735_06_06.jpg]'
- en: Figure 6.6 – A screenshot of the AWS Glue ETL job
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.6 – AWS Glue ETL作业的截图
- en: Once the job is successful, navigate to Amazon Redshift and click on **Query
    editor**.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业成功后，导航到Amazon Redshift并点击**查询编辑器**。
- en: Set the database name as `glue-dev` and then provide the username and password
    to create a connection.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据库名称设置为`glue-dev`，然后提供用户名和密码以创建连接。
- en: 'Select the `public` schema and now you can query the table to see the records,
    as shown in *Figure 6.7*:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`public`模式，现在你可以查询表以查看记录，如图6.7所示：
- en: '![Figure 6.7 – A screenshot of Amazon Redshift''s Query editor'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – Amazon Redshift的查询编辑器截图'
- en: '](img/B16735_06_07.jpg)'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16735_06_07.jpg]'
- en: Figure 6.7 – A screenshot of Amazon Redshift's Query editor
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – Amazon Redshift的查询编辑器截图
- en: We now understand how to create an ETL job using AWS Glue to copy the data from
    the S3 bucket to Amazon Redshift. We also queried the data in Amazon Redshift
    using the query editor from the UI console. It is recommended to delete the Redshift
    cluster and AWS Glue job if you have completed the steps successfully. AWS creates
    two buckets in your account to store the AWS Glue scripts and AWS Glue temporary
    results. Therefore, delete these as well to save costs. We will use the data catalog
    created on S3 data in the next section.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在了解了如何使用AWS Glue创建ETL作业，将数据从S3存储桶复制到Amazon Redshift。我们还使用UI控制台中的查询编辑器查询了Amazon
    Redshift中的数据。如果你已成功完成这些步骤，建议删除Redshift集群和AWS Glue作业。AWS会在你的账户中创建两个存储桶来存储AWS Glue脚本和AWS
    Glue临时结果。因此，也请删除这些以节省成本。我们将在下一节中使用S3数据上创建的数据目录。
- en: In the following section, we will learn about querying the S3 data using Athena.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用Athena查询S3数据。
- en: Querying S3 data using Athena
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Athena查询S3数据
- en: 'Athena is a serverless service designed for querying data stored in S3\. It
    is serverless because the client doesn''t manage the servers that are used for
    computation:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Athena是一个无服务器服务，用于查询存储在S3中的数据。它是无服务器的，因为客户端不管理用于计算的服务器：
- en: Athena uses a schema to present the results against the query on the data stored
    in S3\. You define how you want your data to appear in the form of a schema and
    Athena reads the raw data from S3 to show the results as per the defined schema.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Athena使用模式来呈现对存储在S3中的数据的查询结果。你定义你希望数据以模式的形式出现，Athena从S3读取原始数据，根据定义的模式显示结果。
- en: The output can be used by other services for visualization, storing, or various
    analytics purposes. The source data in S3 can be in any of the following structured,
    semi-structured, and unstructured data formats, including XML, JSON, CSV/TSV,
    AVRO, Parquet, ORC, and more. CloudTrail, ELB Logs, and VPC flow logs can also
    be stored in S3 and analyzed by Athena.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出可以被其他服务用于可视化、存储或各种分析目的。S3中的源数据可以是以下任何一种结构化、半结构化和非结构化数据格式，包括XML、JSON、CSV/TSV、AVRO、Parquet、ORC等。CloudTrail、ELB日志和VPC流量日志也可以存储在S3中，并由Athena进行分析。
- en: This follows the schema-on-read technique. Unlike traditional techniques, tables
    are defined in advance in a data catalog, and data is projected when it reads.
    SQL-like queries are allowed on data without transforming the source data.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这遵循了读取时定义模式的技术。与传统技术不同，表在数据目录中预先定义，数据在读取时进行投影。允许在数据上执行类似SQL的查询，而无需转换源数据。
- en: 'Now, let''s understand this with the help of an example, where we will use
    **AWSDataCatlog** created in AWS Glue on the S3 data and query them using Athena:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过一个示例来理解这一点，我们将使用在AWS Glue上创建的**AWSDataCatlog**对S3数据进行查询：
- en: Navigate to the AWS Athena console. Select `AWSDataCatalog` from `sampledb`
    database will be created with a table, `elb_logs`, in the AWS Glue data catalog).
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到AWS Athena控制台。从`sampledb`数据库中选择`AWSDataCatalog`（将在AWS Glue数据目录中创建一个表，名为`elb_logs`）。
- en: Select `s3-data` as the database.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择`s3-data`作为数据库。
- en: Click on **Settings** from the top-right corner and fill in the details as shown
    in *Figure 6.8* (I have used the same bucket as in the previous example and a
    different folder):![Figure 6.8 – A screenshot of Amazon Athena's settings
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击右上角的**设置**并填写如图*图6.8*所示的详细信息（我使用了与上一个示例相同的存储桶和不同的文件夹）：![图6.8 – Amazon Athena的设置截图
- en: '](img/B16735_06_08.jpg)'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16735_06_08.jpg)'
- en: Figure 6.8 – A screenshot of Amazon Athena's settings
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.8 – Amazon Athena的设置截图
- en: The next step is to write your query in the query editor and execute it. Once
    your execution is complete, please delete your S3 buckets and AWS Glue data catalogs.
    This will save you money.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是在查询编辑器中编写您的查询并执行它。一旦执行完成，请删除您的S3存储桶和AWS Glue数据目录。这将为您节省费用。
- en: In this section, we learned about querying S3 data using Amazon Athena through
    the AWS data catalog. You can also create your schema and query the data from
    S3\. In the next section, we will learn about Amazon Kinesis Data Streams.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何通过AWS数据目录使用Amazon Athena查询S3数据。您也可以创建自己的模式并从S3查询数据。在下一节中，我们将学习Amazon
    Kinesis Data Streams。
- en: Processing real-time data using Kinesis data streams
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kinesis数据流处理实时数据
- en: 'Kinesis is Amazon''s streaming service and can be scaled based on requirements.
    It is highly available in a region. It has a level of persistence that retains
    data for 24 hours by default or optionally up to 365 days. Kinesis data streams
    are used for large-scale data ingestion, analytics, and monitoring:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis是亚马逊的流服务，可以根据需求进行扩展。它在区域中具有高度可用性。它默认保留数据24小时，或可选地最多保留365天。Kinesis数据流用于大规模数据摄取、分析和监控：
- en: Kinesis can be ingested by multiple producers and multiple consumers can also
    read data from the stream. Let's understand this by means of an example in real
    time. Suppose you have a producer ingesting data to a Kinesis stream and the default
    retention period is 24 hours, which means the data ingested at 05:00:00 A.M. today
    will be available in the stream until 04:59:59 A.M. tomorrow. This data won't
    be available beyond that point and ideally, this should be consumed before it
    expires or can be stored somewhere if it's critical. The retention period can
    be extended to a maximum of 365 days at an extra cost.
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis可以被多个生产者摄取，多个消费者也可以从流中读取数据。让我们通过一个实时示例来理解这一点。假设您有一个生产者将数据摄取到Kinesis流中，默认保留期为24小时，这意味着今天早上5:00:00摄取的数据将在明天早上4:59:59之前在流中可用。这些数据在此之后将不可用，理想情况下，应该在过期之前消费，或者如果它很重要，可以存储在某个地方。保留期可以额外付费延长至最多365天。
- en: Kinesis can be used for real-time analytics or dashboard visualization. Producers
    can be imagined as a piece of code pushing data into the Kinesis stream, and it
    can be an EC2 instance running the code, a Lambda function, an IoT device, on-premises
    servers, mobile applications or devices, and so on.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis可用于实时分析或仪表板可视化。生产者可以想象成一段将数据推入Kinesis流的代码，它可以是一个运行代码的EC2实例、Lambda函数、物联网设备、本地服务器、移动应用程序或设备等。
- en: Similarly, the consumer can also be a piece of code running on an EC2 instance,
    Lambda function, or on-premises servers that know how to connect to a kinesis
    stream, read the data, and apply some action to the data. AWS provides triggers
    to invoke a lambda consumer as soon as data arrives at the kinesis stream.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，消费者也可以是一段在EC2实例、Lambda函数或本地服务器上运行的代码，这些代码知道如何连接到Kinesis流、读取数据并对数据进行一些操作。AWS提供触发器，在数据到达Kinesis流时立即调用Lambda消费者。
- en: Kinesis is scalable due to its shard architecture, which is the fundamental
    throughput unit of a kinesis stream. *What is a shard?* A shard is a logical structure
    that partitions the data based on a partition key. A shard supports a writing
    capacity of *1 MB/sec* and a reading capacity of *2 MB/sec*. *1,000* `PUT` records
    per second are supported by a single shard. If you have created a stream with
    *3 shards*, then *3 MB/sec write throughput* and *6 MB/sec read throughput* can
    be achieved and this allows *3,000* `PUT` records. So, with more shards, you have
    to pay an extra amount to get higher performance.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kinesis因其分片架构而具有可扩展性，这是Kinesis流的基本吞吐量单元。*什么是分片？* 分片是一种逻辑结构，根据分区键对数据进行分区。一个分片支持每秒*1
    MB*的写入容量和每秒*2 MB*的读取容量。单个分片每秒支持*1,000*条`PUT`记录。如果你创建了一个包含*3*个分片的流，那么可以实现*3 MB/sec*的写入吞吐量和*6
    MB/sec*的读取吞吐量，这允许*3,000*条`PUT`记录。因此，随着分片数量的增加，你需要支付额外的费用以获得更高的性能。
- en: The data in a shard is stored via a Kinesis data record and can be a maximum
    of 1 MB. Kinesis data records are stored across the shard based on the partition
    key. It also has a sequence number. A sequence number is assigned by kinesis as
    soon as a `putRecord` or `putRecords` API operation is performed so as to uniquely
    identify a record. The partition key is specified by the producer while adding
    the data to the Kinesis data stream, and the partition key is responsible for
    segregating and routing the record to different shards in the stream to balance
    the load.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片中的数据通过Kinesis数据记录存储，最大可达1 MB。Kinesis数据记录根据分区键跨分片存储。它还有一个序列号。序列号在执行`putRecord`或`putRecords`
    API操作时由Kinesis分配，以便唯一标识一个记录。分区键由生产者在将数据添加到Kinesis数据流时指定，分区键负责将记录隔离和路由到流中的不同分片以平衡负载。
- en: 'There are two ways to encrypt the data in a kinesis stream: server-side encryption
    and client-side encryption. Client-side encryption is hard to implement and manage
    the keys because the client has to encrypt the data before putting it into the
    stream and decrypt the data after reading from the stream. With server-side encryption
    enabled via **AWS/KMS**, the data is automatically encrypted and decrypted as
    you put the data and get it from a stream.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Kinesis流中加密数据有两种方式：服务器端加密和客户端加密。客户端加密难以实现和管理密钥，因为客户端必须在将数据放入流之前加密数据，并在从流中读取数据之后解密数据。通过**AWS/KMS**启用的服务器端加密，数据在放入流和从流中获取时自动加密和解密。
- en: Note
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Amazon Kinesis shouldn't be confused with Amazon SQS. Amazon SQS supports one
    production group and one consumption group. If your use case demands multiple
    users sending data and receiving data, then Kinesis is the solution.
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 不要将Amazon Kinesis与Amazon SQS混淆。Amazon SQS支持一个生产组和一组消费组。如果你的用例需要多个用户发送和接收数据，那么Kinesis是解决方案。
- en: For decoupling and asynchronous communications, SQS is the solution, because
    the sender and receiver do not need to be aware of one another.
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对于解耦和异步通信，SQS是解决方案，因为发送者和接收者不需要相互了解。
- en: In SQS, there is no concept of persistence. Once the message is read, the next
    step is deletion. There's no concept of the retention time window for Amazon SQS.
    If your use case demands large-scale ingestion, then Kinesis should be used.
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在SQS中，没有持久性的概念。一旦读取了消息，下一步就是删除。Amazon SQS没有保留时间窗口的概念。如果你的用例需要大规模摄取，那么应该使用Kinesis。
- en: In the next section, we will learn about storing the streamed data for further
    analysis.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何存储流式数据以进行进一步分析。
- en: Storing and transforming real-time data using Kinesis Data Firehose
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kinesis Data Firehose存储和转换实时数据
- en: 'There are a lot of use cases demanding the data to be streamed and stored for
    future analytics purposes. To overcome such problems, you can write a kinesis
    consumer to read the Kinesis stream and store the data in S3\. This solution needs
    an instance or a machine to run the code with the required access to read from
    the stream and write to S3\. The other possible option would be to run a Lambda
    function that gets triggered on the `putRecord` or `putRecords` API made to the
    stream and reads the data from the stream to store in the S3 bucket:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多用例需要将数据流式传输并存储以供未来分析。为了克服这些问题，你可以编写一个Kinesis消费者来读取Kinesis流并将数据存储在S3中。这个解决方案需要一个实例或机器来运行代码，并具有从流中读取和写入S3所需的访问权限。另一个可能的选项是运行一个Lambda函数，该函数在向流发送`putRecord`或`putRecords`
    API时被触发，并从流中读取数据以存储在S3桶中：
- en: To make this easy, Amazon provides a separate service called Kinesis Data Firehose.
    This can easily be plugged into a Kinesis data stream and it will require essential
    IAM roles to write data into S3\. This is a fully managed service to reduce the
    load of managing servers and code. It also supports loading the streamed data
    into Amazon Redshift, Amazon Elasticsearch Service, and Splunk. Kinesis Data Firehose
    scales automatically to match the throughput of the data.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使这个过程变得简单，Amazon 提供了一个名为 Kinesis Data Firehose 的独立服务。它可以轻松地连接到 Kinesis 数据流，并且需要基本的
    IAM 角色将数据写入 S3。这是一个完全托管的服务，用于减少管理服务器和代码的负担。它还支持将流式数据加载到 Amazon Redshift、Amazon
    Elasticsearch 服务和 Splunk。Kinesis Data Firehose 可以自动扩展以匹配数据的吞吐量。
- en: Data can be transformed via an AWS Lambda function before storing or delivering
    it to the destination. If you want to build a raw data lake with the untransformed
    data, then by enabling source record backup, you can store it in another S3 bucket
    prior to the transformation.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以在存储或发送到目的地之前通过 AWS Lambda 函数进行转换。如果您想使用未转换的数据构建原始数据湖，那么通过启用源记录备份，您可以在转换之前将其存储在另一个
    S3 桶中。
- en: With the help of AWS/KMS, data can be encrypted following delivery to the S3
    bucket. It has to be enabled while creating the delivery stream. Data can also
    be compressed in supported formats such as GZIP, ZIP, and SNAPPY compression formats.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 AWS/KMS 的帮助，数据可以在发送到 S3 桶后进行加密。必须在创建交付流时启用。数据还可以以支持的格式（如 GZIP、ZIP 和 SNAPPY
    压缩格式）进行压缩。
- en: In the next section, we will learn about different AWS services used for ingesting
    data from on-premises servers to AWS.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解用于从本地服务器将数据导入 AWS 的不同 AWS 服务。
- en: Different ways of ingesting data from on-premises into AWS
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从本地服务器导入 AWS 的不同数据方式
- en: With the increasing demand for data-driven use cases, managing data on on-premises
    servers is pretty tough at the moment. Taking backups is not easy when you deal
    with a huge amount of data. This data in data lakes is being used to build deep
    neural networks, to create a data warehouse to extract meaningful information
    from it, to run analytics, and to generate reports.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对数据驱动用例需求的增加，目前管理本地服务器上的数据相当困难。当您处理大量数据时，备份并不容易。这些数据在数据湖中正被用于构建深度神经网络，创建数据仓库以从中提取有意义的信息，运行分析，以及生成报告。
- en: Now, if we look at the available options to migrate data into AWS, then it comes
    with various challenges, too. For example, if you want to send data to S3, then
    you have to write a few lines of code to send your data to AWS. You will have
    to manage the code and servers to run the code. It has to be ensured that the
    data is commuting via the HTTPS network. You need to verify whether the data transfer
    was successful. This adds complexity as well as time and effort challenges to
    the process. To avoid such scenarios, AWS provides services to match or solve
    your use cases by designing hybrid infrastructure that allows data sharing between
    the on-premises data centers and AWS. Let's learn about these in the following
    sections.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们看看将数据迁移到 AWS 的可用选项，那么也会带来各种挑战。例如，如果您想将数据发送到 S3，那么您必须编写几行代码将您的数据发送到 AWS。您将需要管理代码和服务器以运行代码。必须确保数据通过
    HTTPS 网络传输。您需要验证数据传输是否成功。这增加了过程的复杂性，同时也带来了时间和精力上的挑战。为了避免此类情况，AWS 提供服务来匹配或解决您的用例，通过设计混合基础设施允许本地数据中心和
    AWS 之间的数据共享。让我们在接下来的章节中了解这些内容。
- en: AWS Storage Gateway
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS 存储网关
- en: 'Storage Gateway is a hybrid storage virtual appliance. It can run in three
    different modes – **File Gateway**, **Tape Gateway**, and **Volume Gateway**.
    It can be used for extension, migration, and backups of the on-premises data center
    to AWS:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 存储网关是一种混合存储虚拟设备。它可以在三种不同的模式下运行——**文件网关**、**磁带网关**和**卷网关**。它可以用于扩展、迁移和备份本地数据中心到
    AWS：
- en: In Tape Gateway mode, the storage gateway stores virtual tapes on S3, and when
    ejected and archived, the tapes are moved from S3 to Glacier. Active tapes are
    stored in S3 for storage and retrieval. Archived or exported tapes are stored
    in **Virtual Tape Shelf** (**VTS**) in Glacier. Virtual tapes can be created and
    can range in size from 100 GiB to 5 TiB. A total of 1 petabyte of storage can
    be configured locally and an unlimited number of tapes can be archived to Glacier.
    This is ideal for an existing backup system on tape and where there is a need
    to migrate backup data into AWS. You can decommission the physical tape hardware
    later.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在磁带网关模式下，存储网关将虚拟磁带存储在S3上，当磁带被弹出并归档时，磁带将从S3移动到Glacier。活动磁带存储在S3上进行存储和检索。归档或导出的磁带存储在Glacier的**虚拟磁带架（VTS）**中。虚拟磁带可以创建，大小从100
    GiB到5 TiB不等。总共可以配置1PB的本地存储，并且可以将无限数量的磁带归档到Glacier。这对于现有的磁带备份系统以及需要将备份数据迁移到AWS的场景非常理想。您可以在以后退役物理磁带硬件。
- en: In File Gateway mode, the storage gateway maps files onto S3 objects, which
    can be stored using one of the available storage classes. This helps you to extend
    the data center into AWS. You can load more files to your File Gateway and these
    are stored as S3 objects. It can run on your on-premises virtual server, which
    connects to various devices using **Server Message Block (SMB)** or **Network
    File System (NFS)**. The File Gateway connects to AWS using an HTTPS public endpoint
    to store the data on S3 objects. Life cycle policies can be applied to those S3
    objects. You can easily integrate your **Active** **Directory** (**AD**) with
    File Gateway to control access to the files on the file share.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文件网关模式下，存储网关将文件映射到S3对象上，可以使用可用的存储类之一进行存储。这有助于您将数据中心扩展到AWS。您可以将更多文件加载到文件网关中，这些文件将作为S3对象存储。它可以在您的本地虚拟服务器上运行，通过**服务器消息块（SMB）**或**网络文件系统（NFS）**连接到各种设备。文件网关通过HTTPS公共端点连接到AWS，以在S3对象上存储数据。可以对这些S3对象应用生命周期策略。您可以将您的**活动目录（AD**）轻松集成到文件网关中，以控制对文件共享中文件的访问。
- en: 'In Volume Gateway mode, the storage gateway presents block storage. There are
    two ways of using this, one is **Gateway Cached**, and the other is **Gateway
    Stored**:'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在卷网关模式下，存储网关提供块存储。使用这种存储有两种方式，一种为**网关缓存**，另一种为**网关存储**：
- en: '**Gateway Stored** is a volume storage gateway running locally on-premises
    and it has local storage and an upload buffer. A total of 32 volumes can be created
    and each volume can be up to 16 TB in size for a total capacity of 512 TB. Primary
    data is stored on-premises and backup data is asynchronously replicated to AWS
    in the background. Volumes are made available via **Internet Small Computer Systems
    Interface (iSCSI)** for network-based servers to access. It connects to **Storage
    Gateway Endpoint** via an HTTPS public endpoint and creates EBS snapshots from
    backup data. These snapshots can be used to create standard EBS volumes. This
    option is ideal for migration to AWS or disaster recovery or business continuity.
    The local system will still use the local volume, but the EBS snapshots are in
    AWS, which can be used instead of backups. It''s not the best option for data
    center extension because you require a huge amount of local storage.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网关存储**是一种本地运行的卷存储网关，它具有本地存储和上传缓冲区。总共可以创建32个卷，每个卷的大小可达16 TB，总容量为512 TB。主数据存储在本地，备份数据在后台异步复制到AWS。卷通过**互联网小型计算机系统接口（iSCSI**）提供，以便基于网络的服务器访问。它通过HTTPS公共端点连接到**存储网关端点**，并从备份数据创建EBS快照。这些快照可用于创建标准的EBS卷。此选项非常适合迁移到AWS或灾难恢复或业务连续性。本地系统将继续使用本地卷，但EBS快照存储在AWS中，可以用作备份的替代品。这不是数据中心扩展的最佳选择，因为您需要大量的本地存储。'
- en: '**Gateway Cached** is a volume storage gateway running locally on-premises
    and it has cache storage and an upload buffer. The difference is that the data
    that is added to the storage gateway is not local but uploaded to AWS. Primary
    data is stored in AWS. Frequently accessed data is cached locally. This is an
    ideal option for extending the on-premises data center to AWS. It connects to
    **Storage Gateway Endpoint** via an HTTPS public endpoint and creates **S3 Backed
    Volume (AWS-managed bucket)** snapshots that are stored as standard EBS snapshots.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Gateway Cached**是一种在本地运行的卷存储网关，它具有缓存存储和上传缓冲区。不同之处在于，添加到存储网关的数据不是本地的，而是上传到AWS。主数据存储在AWS。频繁访问的数据在本地缓存。这是将本地数据中心扩展到AWS的理想选择。它通过HTTPS公共端点连接到**存储网关端点**，并创建存储为标准EBS快照的**S3支持卷（AWS管理的存储桶）**快照。'
- en: Snowball, Snowball Edge, and Snowmobile
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Snowball、Snowball Edge和Snowmobile
- en: 'They belong to the same product clan for the physical transfer of data between
    business operating locations and AWS. For moving a large amount of data into and
    out of AWS, you can use any of the three:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 它们属于同一产品家族，用于在业务运营地点和AWS之间进行物理数据传输。对于在AWS中移动大量数据，您可以使用以下三者中的任何一个：
- en: '**Snowball**: This can be ordered from AWS by logging a job. AWS delivers the
    device to load your data and send it back. Data in Snowball is encrypted using
    KMS. It comes with two capacity ranges, one 50 TB and the other 80 TB. It is economical
    to order one or more Snowball devices for data between 10 TB and 10 PB. The device
    can be sent to different premises. It does not have any compute capability; it
    only comes with storage capability.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowball**：您可以通过提交一个工作订单从AWS订购此设备。AWS将设备交付给您用于加载数据并发送回来。Snowball中的数据使用KMS进行加密。它提供两种容量范围，一种是50
    TB，另一种是80 TB。对于10 TB到10 PB之间的数据，订购一个或多个Snowball设备是经济实惠的。设备可以被发送到不同的地点。它没有计算能力；它只提供存储能力。'
- en: '**Snowball Edge**: This is like Snowball, but it comes with both storage and
    computes capability. It has a larger capacity than Snowball. It offers fastened
    networking, such as 10 Gbps over RJ45, 10/25 Gb over SFP28, and 40/100 Gb+ over
    QSFP+ copper. This is ideal for the secure and quick transfer of terabytes to
    petabytes of data into AWS.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowball Edge**：这与Snowball类似，但它同时具备存储和计算能力。它的容量比Snowball大。它提供快速网络连接，例如通过RJ45的10
    Gbps、10/25 Gb通过SFP28，以及40/100 Gb+通过QSFP+铜线。这对于将千兆到太字节的数据安全快速地传输到AWS是非常理想的。'
- en: '**Snowmobile**: This is a portable data center within a shipping container
    on a truck. This allows you to move exabytes of data from on-premises to AWS.
    If your data size exceeds 10 PB, then Snowmobile is preferred. Essentially, upon
    requesting Snowmobile, a truck is driven to your location, you plug your data
    center into the truck, and transfer the data. If you have multiple sites, choosing
    Snowmobile for data transfer is not an ideal option.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Snowmobile**：这是一个位于卡车上的运输集装箱内的便携式数据中心。这允许您将艾字节的数据从本地迁移到AWS。如果您的数据量超过10 PB，则Snowmobile是首选。本质上，在请求Snowmobile后，一辆卡车将被开到您的位置，您将数据中心连接到卡车上，并传输数据。如果您有多个地点，选择Snowmobile进行数据传输不是理想的选择。'
- en: AWS DataSync
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS DataSync
- en: 'AWS DataSync is designed to move data from on-premises storage to AWS, or vice
    versa:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: AWS DataSync旨在将数据从本地存储迁移到AWS，或反之亦然：
- en: It is an ideal product from AWS for data processing transfers, archival or cost-effective
    storage, disaster recovery, business continuity, and data migrations.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是AWS为数据处理传输、归档或成本效益存储、灾难恢复、业务连续性和数据迁移提供的理想产品。
- en: It has a special data validation feature that verifies the original data with
    the data in AWS, as soon as the data arrives in AWS. In other words, it checks
    the integrity of the data.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有特殊的数据验证功能，可以在数据到达AWS后立即验证原始数据与AWS中的数据，换句话说，它检查数据的完整性。
- en: Let's understand this product in depth by considering an example of an on-premises
    data center that has SAN/NAS storage. When we run the AWS DataSync agent on a
    VMWare platform, this agent is capable of communicating with the NAS/SAN storage
    via an NFS/SMB protocol. Once it is on, it communicates with the AWS DataSync
    endpoint and, from there, it can connect with several different types of location,
    including various S3 storage classes or VPC-based resources, such as the **Elastic**
    **File** **System** (**EFS**) and FSx for Windows Server.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过考虑一个具有SAN/NAS存储的本地数据中心示例，让我们深入理解这个产品。当我们在一个VMWare平台上运行AWS DataSync代理时，这个代理能够通过NFS/SMB协议与NAS/SAN存储进行通信。一旦启动，它就会与AWS
    DataSync端点通信，从那里，它可以连接到多种不同类型的地点，包括各种S3存储类别或基于VPC的资源，例如**弹性文件系统**（**EFS**）和FSx
    for Windows Server。
- en: It allows you to schedule data transfers during specific periods. By configuring
    the built-in bandwidth throttle, you can limit the amount of network bandwidth
    that DataSync uses.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许您在特定时间段内安排数据传输。通过配置内置的带宽节流，您可以限制DataSync使用的网络带宽量。
- en: Processing stored data on AWS
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在AWS上处理存储数据
- en: There are several services for processing the data stored in AWS. We will go
    through AWS Batch and AWS EMR (Elastic MapReduce) in this section. EMR is a product
    from AWS that primarily runs MapReduce jobs and Spark applications in a managed
    way. AWS Batch is used for long-running, compute-heavy workloads.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: AWS中有几个用于处理存储在AWS中的数据的服务。在本节中，我们将介绍AWS Batch和AWS EMR（弹性MapReduce）。EMR是AWS提供的一项服务，主要用于以管理方式运行MapReduce作业和Spark应用程序。AWS
    Batch用于长时间运行、计算密集型的工作负载。
- en: AWS EMR
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS EMR
- en: 'EMR is a managed implementation of Apache Hadoop provided as a service by AWS.
    It includes other components of the Hadoop ecosystem, such as Spark, HBase, Flink,
    Presto, Hive, Pig, and many more. We will not cover these in detail for the certification
    exam:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: EMR是AWS提供的一项托管Apache Hadoop实现，它还包括Hadoop生态系统的其他组件，如Spark、HBase、Flink、Presto、Hive、Pig等。我们不会在认证考试中详细讨论这些：
- en: EMR clusters can be launched from the AWS console or via the AWS CLI with a
    specific number of nodes. The cluster can be a long-term cluster or an ad hoc
    cluster. If you have a long-running traditional cluster, then you have to configure
    the machines and manage them yourself. If you have jobs to be executed faster,
    then you need to manually add a cluster. In the case of EMR, these admin overheads
    are gone. You can request any number of nodes to EMR and it manages and launches
    the nodes for you. If you have autoscaling enabled on the cluster, then with the
    increased requirement, EMR launches new nodes in the cluster and decommissions
    the nodes once the load is reduced.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR集群可以从AWS控制台或通过AWS CLI以特定数量的节点启动。集群可以是长期集群或临时集群。如果您有一个长期运行的传统集群，那么您必须自行配置机器并管理它们。如果您有需要更快执行的工作，那么您需要手动添加一个集群。在EMR的情况下，这些管理开销消失了。您可以向EMR请求任意数量的节点，并且它将为您管理和启动节点。如果您在集群上启用了自动扩展，那么在需求增加时，EMR将在集群中启动新的节点，一旦负载减少，就会退役这些节点。
- en: EMR uses EC2 instances in the background and runs in one availability zone in
    a VPC. This enables faster network speeds between the nodes. AWS Glue uses EMR
    clusters in the background, where users do not need to worry about the operational
    understanding of AWS EMR.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR在后台使用EC2实例运行，并在VPC中的一个可用区运行。这使节点之间的网络速度更快。AWS Glue在后台使用EMR集群，用户无需担心AWS EMR的操作理解。
- en: From a use case standpoint, EMR can be used for processing or transforming the
    data stored in S3 and outputs data to be stored in S3\. EMR uses nodes (EC2 instances)
    as the computing units for data processing. EMR nodes come in different variants,
    including Master node, Core node, and Task node.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从用例的角度来看，EMR可以用于处理或转换存储在S3中的数据，并将输出数据存储在S3中。EMR使用节点（EC2实例）作为数据处理的基本单元。EMR节点有多种变体，包括主节点、核心节点和任务节点。
- en: The EMR master node acts as a Hadoop namenode and manages the cluster and its
    health. It is responsible for distributing the job workload among the other core
    nodes and task nodes. If you have SSH enabled, then you can connect to the master
    node instance and access the cluster.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR主节点充当Hadoop namenode，并管理集群及其健康状态。它负责在其他核心节点和任务节点之间分配作业工作负载。如果您启用了SSH，则可以连接到主节点实例并访问集群。
- en: The EMR cluster can have one or more core nodes. If you relate to the Hadoop
    ecosystem, then core nodes are similar to Hadoop data nodes for HDFS and they
    are responsible for running tasks therein.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR集群可以有一个或多个核心节点。如果您与Hadoop生态系统相关联，那么核心节点类似于Hadoop数据节点，它们负责在其中运行任务。
- en: Task nodes are optional and they don't have HDFS storage. They are responsible
    for running tasks. If a task node fails for some reason, then this does not impact
    HDFS storage, but a core node failure causes HDFS storage interruptions.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务节点是可选的，它们没有HDFS存储。它们负责运行任务。如果某个原因导致任务节点失败，那么这不会影响HDFS存储，但核心节点故障会导致HDFS存储中断。
- en: EMR has its filesystem called EMRFS. It is backed by S3 and this feature makes
    it regionally resilient. If a core node fails, the data is still safe in S3\.
    HDFS is efficient in terms of I/O and faster than EMRFS.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EMR有其文件系统EMRFS。它由S3支持，这使得它具有区域弹性。如果核心节点失败，数据仍然安全在S3中。HDFS在I/O效率方面表现良好，比EMRFS更快。
- en: In the following section, we will learn about AWS Batch, which is a managed
    batch processing compute service that can be used for long-running services.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将了解AWS Batch，它是一个可管理的批量处理计算服务，可用于长时间运行的服务。
- en: AWS Batch
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: AWS Batch
- en: 'This is a managed batch processing product. If you are using AWS Batch, then
    jobs can be run without end user interaction or can be scheduled to run:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个可管理的批量处理产品。如果您使用AWS Batch，则作业可以在没有最终用户交互的情况下运行，或者可以安排运行：
- en: Imagine an event-driven application that launches a Lambda function to process
    the data stored in S3\. If the processing time goes beyond 15 minutes, then Lambda
    has the execution time limit. For such scenarios, AWS Batch is a better solution,
    where computation-heavy workloads can be scheduled or driven through API events.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想象一个事件驱动的应用程序，它启动Lambda函数来处理存储在S3中的数据。如果处理时间超过15分钟，那么Lambda有执行时间限制。在这种情况下，AWS
    Batch是一个更好的解决方案，其中计算密集型工作负载可以通过API事件进行调度或驱动。
- en: AWS Batch is a good fit for use cases where a longer processing time is required
    or more computation resources are needed.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Batch非常适合需要更长时间处理时间或更多计算资源的使用场景。
- en: AWS Batch runs a job that can be a script or an executable. One job can depend
    on another job. A job needs a definition, such as who can run a job (with IAM
    permissions), where the job can be run (resources to be used), mount points, and
    other metadata.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Batch运行一个可以是脚本或可执行文件的作业。一个作业可以依赖于另一个作业。作业需要一个定义，例如谁可以运行作业（使用IAM权限）、作业可以在哪里运行（要使用的资源）、挂载点和其他元数据。
- en: Jobs are submitted to queues where they wait for compute environment capacity.
    These queues are associated with one or more compute environments.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务被提交到队列中，在那里它们等待计算环境容量。这些队列与一个或多个计算环境相关联。
- en: Compute environments do the actual work of executing the jobs. These can be
    ECS or EC2 instances, or any computing resources. You can define their sizes and
    capacities too.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算环境执行实际的工作，即执行作业。这些可以是ECS或EC2实例，或任何计算资源。您还可以定义它们的大小和容量。
- en: Environments receive jobs from the queues based on their priority and execute
    the jobs. They can be managed or unmanaged compute environments.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境根据其优先级从队列中接收作业并执行作业。它们可以是管理的或非管理的计算环境。
- en: AWS Batch can store the metadata in DynamoDB for further use and can also store
    the output to the S3 bucket.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS Batch可以将元数据存储在DynamoDB中以供进一步使用，也可以将输出存储到S3存储桶中。
- en: Note
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: If you get a question in exams on an event-style workload that requires flexible
    compute, a higher disk space, no time limit (more than 15 minutes), or an effective
    resource limit, then choose AWS Batch.
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你在考试中遇到一个需要灵活计算、更高磁盘空间、无时间限制（超过15分钟）或有效资源限制的事件式工作负载问题，那么请选择AWS Batch。
- en: Summary
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about different ways of processing data in AWS.
    We also learned the capabilities in terms of extending our data centers to AWS,
    migrating data to AWS, and the ingestion process. We also learned the various
    ways of using the data to process it and make it ready for analysis in our way.
    We understood the magic of having a data catalog that helps us to query our data
    via AWS Glue and Athena.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了在AWS中处理数据的不同方式。我们还学习了将数据中心扩展到AWS、迁移数据到AWS以及摄取过程的能力。我们还学习了使用数据以我们自己的方式处理数据并使其准备好分析的多种方法。我们了解了数据目录的魔力，它帮助我们通过AWS
    Glue和Athena查询我们的数据。
- en: In the next chapter, we will learn about various machine learning algorithms
    and their usages.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习各种机器学习算法及其用法。
- en: Questions
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题
- en: If you have a large number of IoT devices sending data to AWS to be consumed
    by a large number of mobile devices, which of the following should you choose?
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您有大量物联网设备向AWS发送数据，由大量移动设备消费，以下哪个选项应该选择？
- en: A. SQS standard queue
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. SQS标准队列
- en: B. SQS FIFO queue
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. SQS FIFO队列
- en: C. Kinesis stream
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. Kinesis流
- en: If you have a requirement to decouple a high-volume application, which of the
    following should you choose?
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您需要解耦一个高流量应用程序，以下哪个选项应该选择？
- en: A. SQS standard queue
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. SQS标准队列
- en: B. SQS FIFO queue
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. SQS FIFO队列
- en: C. Kinesis stream
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. Kinesis流
- en: Which of the following do I need to change to improve the performance of a Kinesis
    stream?
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了提高Kinesis流的性能，我需要更改以下哪个设置？
- en: A. The read capacity unit of a stream
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 流的读取容量单位
- en: B. The write capacity unit of a stream
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 流的写入容量单位
- en: C. Shards of a stream
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 流的分区
- en: D. The region of a stream
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. 流的区域
- en: Which of the following ensures data integrity?
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪个确保数据完整性？
- en: A. AWS Kinesis
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. AWS Kinesis
- en: B. AWS DataSync
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. AWS DataSync
- en: C. AWS EMR
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. AWS EMR
- en: D. Snowmobile
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. Snowmobile
- en: Which storage gateway mode can replace a tape drive with S3 storage?
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪种存储网关模式可以用S3存储替换磁带驱动器？
- en: A. Volume Gateway Stored
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 体积网关存储
- en: B. Volume Gateway Cached
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 体积网关缓存
- en: C. File
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 文件
- en: D. VTL
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. VTL
- en: Which storage gateway mode can be used to present storage over SMB to clients?
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪种存储网关模式可以用于将SMB存储呈现给客户端？
- en: A. Volume Gateway Stored
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 体积网关存储
- en: B. Volume Gateway Cached
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 体积网关缓存
- en: C. File
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 文件
- en: D. VTL
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. VTL
- en: Which storage gateway mode is ideal for data center extension to AWS?
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪种存储网关模式适合数据中心扩展到AWS？
- en: A. Volume Gateway Stored
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 体积网关存储
- en: B. Volume Gateway Cached
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 体积网关缓存
- en: C. File
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 文件
- en: D. VTL
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. VTL
- en: What storage product in AWS can be used for Windows environments for shared
    storage?
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AWS中哪种存储产品可用于Windows环境的共享存储？
- en: A. S3
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. S3
- en: B. FSx
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. FSx
- en: C. EBS
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. EBS
- en: D. EFS
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. EFS
- en: Which node within an EMR cluster handles operations?
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在EMR集群中，哪个节点处理操作？
- en: A. Master node
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 主节点
- en: B. Core node
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 核心节点
- en: C. Task node
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 任务节点
- en: D. Primary node
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. 主节点
- en: Which nodes in an EMR cluster are good for spot instances?
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在EMR集群中，哪些节点适合spot实例？
- en: A. Master node
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 主节点
- en: B. Core node
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 核心节点
- en: C. Task node
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. 任务节点
- en: D. Primary node
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. 主节点
- en: If you have large quantities of streaming data and add it to Redshift, which
    services would you use (choose three)?
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您有大量流数据并添加到Redshift，您将使用哪些服务（选择三个）？
- en: A. Kinesis Data Streams
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. Kinesis Data Streams
- en: B. Kinesis Data Firehose
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. Kinesis Data Firehose
- en: C. S3
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. S3
- en: D. SQS
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. SQS
- en: E. Kinesis Analytics
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: E. Kinesis Analytics
- en: Kinesis Firehose supports data transformation using Lambda.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kinesis Firehose支持使用Lambda进行数据转换。
- en: A. True
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. 正确
- en: B. False
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. 错误
- en: Which of the following are valid destinations for Kinesis Data Firehose (choose
    five)?
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下哪些是Kinesis Data Firehose的有效目的地（选择五个）？
- en: A. HTTP
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: A. HTTP
- en: B. Splunk
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: B. Splunk
- en: C. Redshift
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: C. Redshift
- en: D. S3
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: D. S3
- en: E. Elastic Search
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: E. Elastic Search
- en: F. EC2
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F. EC2
- en: G. SQS
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: G. SQS
- en: Answers
  id: totrans-251
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 答案
- en: 1\. C
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 1\. C
- en: 2\. A
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 2\. A
- en: 3\. C
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 3\. C
- en: 4\. B
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 4\. B
- en: 5\. D
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 5\. D
- en: 6\. C
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 6\. C
- en: 7\. B
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 7\. B
- en: 8\. B
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 8\. B
- en: 9\. A
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 9\. A
- en: 10\. C
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 10\. C
- en: 11\. A, B, C
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 11\. A, B, C
- en: 12\. A
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 12\. A
- en: 13\. A, B, C, D, E
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 13\. A, B, C, D, E
