- en: '2'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '2'
- en: Training Simple Machine Learning Algorithms for Classification
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练简单的机器学习分类算法
- en: 'In this chapter, we will make use of two of the first algorithmically described
    machine learning algorithms for classification: the perceptron and adaptive linear
    neurons. We will start by implementing a perceptron step by step in Python and
    training it to classify different flower species in the Iris dataset. This will
    help us to understand the concept of machine learning algorithms for classification
    and how they can be efficiently implemented in Python.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用两种最早算法描述的机器学习分类算法：感知机和自适应线性神经元。我们将从头开始一步步实现感知机，并训练其在鸢尾花数据集上分类不同的花卉种类。这将帮助我们理解机器学习分类算法的概念，以及如何高效地在Python中实现它们。
- en: Discussing the basics of optimization using adaptive linear neurons will then
    lay the groundwork for using more sophisticated classifiers via the scikit-learn
    machine learning library in *Chapter 3*, *A Tour of Machine Learning Classifiers
    Using scikit-learn*.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用自适应线性神经元讨论优化基础，将为在*第3章*《使用scikit-learn的机器学习分类器之旅》中使用更复杂的分类器奠定基础。
- en: 'The topics that we will cover in this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖的主题如下：
- en: Building an understanding of machine learning algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建立对机器学习算法的理解
- en: Using pandas, NumPy, and Matplotlib to read in, process, and visualize data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用pandas、NumPy和Matplotlib读取、处理和可视化数据
- en: Implementing linear classification algorithms in Python
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中实现线性分类算法
- en: Artificial neurons – a brief glimpse into the early history of machine learning
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元——机器学习早期历史的简要回顾
- en: 'Before we discuss the perceptron and related algorithms in more detail, let''s
    take a brief tour of the beginnings of machine learning. Trying to understand
    how the biological brain works, in order to design artificial intelligence (AI),
    Warren McCulloch and Walter Pitts published the first concept of a simplified
    brain cell, the so-called **McCulloch-Pitts** (**MCP**) neuron, in 1943 (*A Logical
    Calculus of the Ideas Immanent in Nervous Activity*, *W. S. McCulloch* and *W.
    Pitts*, *Bulletin of Mathematical Biophysics*, 5(4): 115-133, *1943*). Biological
    neurons are interconnected nerve cells in the brain that are involved in the processing
    and transmitting of chemical and electrical signals, which is illustrated in the
    following figure:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们更详细地讨论感知机及相关算法之前，让我们简要回顾一下机器学习的起源。为了理解生物大脑的工作原理，以设计人工智能（AI），沃伦·麦卡洛克和沃尔特·皮茨于1943年首次提出了简化版脑细胞的概念，即所谓的**麦卡洛克-皮茨**（**MCP**）神经元（《神经活动中固有思想的逻辑演算》，*W.
    S. McCulloch* 和 *W. Pitts*，*数学生物物理学公报*，5(4)：115-133，*1943*）。生物神经元是大脑中互相连接的神经细胞，参与处理和传递化学和电信号，如下图所示：
- en: '![](img/B13208_02_01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_01.png)'
- en: McCulloch and Pitts described such a nerve cell as a simple logic gate with
    binary outputs; multiple signals arrive at the dendrites, they are then integrated
    into the cell body, and, if the accumulated signal exceeds a certain threshold,
    an output signal is generated that will be passed on by the axon.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 麦卡洛克和皮茨将这种神经细胞描述为一个具有二进制输出的简单逻辑门；多个信号通过树突到达细胞体，然后它们被整合，如果累积的信号超过某个阈值，则会生成一个输出信号，并通过轴突传递出去。
- en: 'Only a few years later, Frank Rosenblatt published the first concept of the
    perceptron learning rule based on the MCP neuron model (*The Perceptron: A Perceiving
    and Recognizing Automaton*, *F. Rosenblatt*, *Cornell Aeronautical Laboratory*,
    *1957*). With his perceptron rule, Rosenblatt proposed an algorithm that would
    automatically learn the optimal weight coefficients that would then be multiplied
    with the input features in order to make the decision of whether a neuron fires
    (transmits a signal) or not. In the context of supervised learning and classification,
    such an algorithm could then be used to predict whether a new data point belongs
    to one class or the other.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 几年后，弗兰克·罗森布拉特基于MCP神经元模型提出了感知机学习规则的第一个概念（《感知机：一个感知和识别的自动机》，*F. Rosenblatt*，*康奈尔航空实验室*，*1957*）。通过他的感知机规则，罗森布拉特提出了一种算法，能够自动学习最优的权重系数，这些权重系数随后会与输入特征相乘，以决定神经元是否发火（传递信号）。在监督学习和分类的背景下，这样的算法可以用来预测一个新的数据点是否属于某一类别。
- en: The formal definition of an artificial neuron
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经元的正式定义
- en: 'More formally, we can put the idea behind **artificial neurons** into the context
    of a binary classification task where we refer to our two classes as *1* (positive
    class) and *–1* (negative class) for simplicity. We can then define a decision
    function (![](img/B13208_02_001.png)) that takes a linear combination of certain
    input values, *x*, and a corresponding weight vector, *w*, where *z* is the so-called
    net input ![](img/B13208_02_002.png):'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，我们可以将**人工神经元**的概念置于二分类任务的背景下，简化为我们称之为*1*（正类）和*–1*（负类）的两个类别。然后我们可以定义一个决策函数（！[](img/B13208_02_001.png)），它取某些输入值*x*和相应的权重向量*w*的线性组合，其中*z*是所谓的净输入！[](img/B13208_02_002.png)：
- en: '![](img/B13208_02_003.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_003.png)'
- en: 'Now, if the net input of a particular example, ![](img/B13208_02_004.png),
    is greater than a defined threshold, ![](img/B13208_02_005.png), we predict class
    *1*, and class *–1* otherwise. In the perceptron algorithm, the decision function,
    ![](img/B13208_02_006.png), is a variant of a **unit step function**:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果某个特定示例的净输入！[](img/B13208_02_004.png)大于定义的阈值！[](img/B13208_02_005.png)，我们预测类别*1*，否则预测类别*–1*。在感知机算法中，决策函数！[](img/B13208_02_006.png)是一个**单位阶跃函数**的变体：
- en: '![](img/B13208_02_007.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_007.png)'
- en: 'For simplicity, we can bring the threshold, ![](img/B13208_02_008.png), to
    the left side of the equation and define a weight-zero as ![](img/B13208_02_009.png)
    and ![](img/B13208_02_010.png) so that we write *z* in a more compact form:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们可以将阈值！[](img/B13208_02_008.png)移到方程的左侧，并定义一个权重零为！[](img/B13208_02_009.png)和！[](img/B13208_02_010.png)，这样我们可以用更简洁的形式表示*z*：
- en: '![](img/B13208_02_011.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_011.png)'
- en: 'And:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以及：
- en: '![](img/B13208_02_012.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_012.png)'
- en: In machine learning literature, the negative threshold, or weight, ![](img/B13208_02_013.png),
    is usually called the **bias unit**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习文献中，负阈值或权重！[](img/B13208_02_013.png)通常称为**偏置单元**。
- en: '**Linear algebra basics: dot product and matrix transpose**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性代数基础：点积和矩阵转置**'
- en: 'In the following sections, we will often make use of basic notations from linear
    algebra. For example, we will abbreviate the sum of the products of the values
    in *x* and *w* using a vector dot product, whereas superscript *T* stands for
    transpose, which is an operation that transforms a column vector into a row vector
    and vice versa:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将经常使用线性代数中的基本符号。例如，我们将通过向量点积来简化*x*和*w*值的乘积之和，而上标*T*表示转置，这是一个将列向量转换为行向量，反之亦然的操作：
- en: '![](img/B13208_02_014.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_014.png)'
- en: 'For example:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如：
- en: '![](img/B13208_02_015.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_015.png)'
- en: 'Furthermore, the transpose operation can also be applied to matrices to reflect
    it over its diagonal, for example:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，转置操作也可以应用于矩阵，以沿其对角线进行反射，例如：
- en: '![](img/B13208_02_016.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_016.png)'
- en: Please note that the transpose operation is strictly only defined for matrices;
    however, in the context of machine learning, we refer to ![](img/B13208_02_017.png)
    or ![](img/B13208_02_018.png) matrices when we use the term "vector."
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，转置操作严格来说只对矩阵定义；然而，在机器学习的上下文中，当我们使用“向量”一词时，我们指的是！[](img/B13208_02_017.png)或！[](img/B13208_02_018.png)矩阵。
- en: In this book, we will only use very basic concepts from linear algebra; however,
    if you need a quick refresher, please take a look at Zico Kolter's excellent *Linear
    Algebra Review and Reference*, which is freely available at [http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将只使用线性代数中的非常基础的概念；但是，如果你需要快速回顾，请查看Zico Kolter的优秀作品《线性代数复习与参考》，该书可以在[http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf](http://www.cs.cmu.edu/~zkolter/course/linalg/linalg_notes.pdf)免费获取。
- en: 'The following figure illustrates how the net input ![](img/B13208_02_019.png)
    is squashed into a binary output (–1 or 1) by the decision function of the perceptron
    (left subfigure) and how it can be used to discriminate between **two linearly
    separable classes** (right subfigure):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了净输入！[](img/B13208_02_019.png)是如何通过感知机的决策函数（左图）被压缩成二进制输出（–1 或 1），以及它如何用于区分**两个线性可分的类别**（右图）：
- en: '![](img/B13208_02_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_02.png)'
- en: The perceptron learning rule
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机学习规则
- en: 'The whole idea behind the MCP neuron and Rosenblatt''s *thresholded* perceptron
    model is to use a reductionist approach to mimic how a single neuron in the brain
    works: it either *fires* or it doesn''t. Thus, Rosenblatt''s initial perceptron
    rule is fairly simple, and the perceptron algorithm can be summarized by the following
    steps:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: MCP 神经元和 Rosenblatt 的 *阈值* 感知机模型的核心思想是采用还原主义的方法来模拟大脑中单个神经元的工作方式：它要么 *发放* 信号，要么不发放。因此，Rosenblatt
    最初的感知机规则相当简单，感知机算法可以通过以下步骤总结：
- en: Initialize the weights to 0 or small random numbers.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化权重为 0 或小的随机数。
- en: 'For each training example, ![](img/B13208_02_020.png):'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个训练样本，![](img/B13208_02_020.png)：
- en: Compute the output value, ![](img/B13208_02_021.png).
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出值，![](img/B13208_02_021.png)。
- en: Update the weights.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新权重。
- en: 'Here, the output value is the class label predicted by the unit step function
    that we defined earlier, and the simultaneous update of each weight, ![](img/B13208_02_022.png),
    in the weight vector, *w*, can be more formally written as:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，输出值是由我们之前定义的单位阶跃函数预测的类别标签，同时，权重向量中的每个权重（![](img/B13208_02_022.png)）也会更新，*w*
    的更新可以更正式地写作：
- en: '![](img/B13208_02_023.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_023.png)'
- en: 'The update value for ![](img/B13208_02_024.png) (or change in ![](img/B13208_02_025.png)),
    which we refer to as ![](img/B13208_02_026.png), is calculated by the perceptron
    learning rule as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B13208_02_024.png)（或 ![](img/B13208_02_025.png) 的变化）的更新值，我们称之为 ![](img/B13208_02_026.png)，是通过感知机学习规则计算的，如下所示：'
- en: '![](img/B13208_02_027.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_027.png)'
- en: 'Where ![](img/B13208_02_028.png) is the **learning rate** (typically a constant
    between 0.0 and 1.0), ![](img/B13208_02_029.png) is the **true class label** of
    the *i*th training example, and ![](img/B13208_02_030.png) is the **predicted
    class label**. It is important to note that all weights in the weight vector are
    being updated simultaneously, which means that we don''t recompute the predicted
    label, ![](img/B13208_02_031.png), before all of the weights are updated via the
    respective update values, ![](img/B13208_02_032.png). Concretely, for a two-dimensional
    dataset, we would write the update as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，![](img/B13208_02_028.png) 是 **学习率**（通常是介于 0.0 和 1.0 之间的常数），![](img/B13208_02_029.png)
    是第 *i* 个训练样本的 **真实类别标签**，而 ![](img/B13208_02_030.png) 是 **预测的类别标签**。需要注意的是，权重向量中的所有权重是同时更新的，这意味着在通过各自的更新值（![](img/B13208_02_032.png)）更新所有权重之前，我们不会重新计算预测标签（![](img/B13208_02_031.png)）。具体来说，对于一个二维数据集，我们将这样写出更新公式：
- en: '![](img/B13208_02_033.png)![](img/B13208_02_034.png)![](img/B13208_02_035.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_033.png)![](img/B13208_02_034.png)![](img/B13208_02_035.png)'
- en: 'Before we implement the perceptron rule in Python, let''s go through a simple
    thought experiment to illustrate how beautifully simple this learning rule really
    is. In the two scenarios where the perceptron predicts the class label correctly,
    the weights remain unchanged, since the update values are 0:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现感知机规则之前，让我们通过一个简单的思想实验，来阐明这个学习规则的简洁性。在感知机正确预测类别标签的两个场景中，由于更新值为 0，权重保持不变：
- en: (1) ![](img/B13208_02_036.png)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: (1) ![](img/B13208_02_036.png)
- en: (2) ![](img/B13208_02_037.png)
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: (2) ![](img/B13208_02_037.png)
- en: 'However, in the case of a wrong prediction, the weights are being pushed toward
    the direction of the positive or negative target class:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在预测错误的情况下，权重会朝着正类或负类目标的方向调整：
- en: (3) ![](img/B13208_02_038.png)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: (3) ![](img/B13208_02_038.png)
- en: (4) ![](img/B13208_02_039.png)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: (4) ![](img/B13208_02_039.png)
- en: 'To get a better understanding of the multiplicative factor, ![](img/B13208_02_040.png),
    let''s go through another simple example, where:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解乘法因子，![](img/B13208_02_040.png)，让我们通过另一个简单的例子来分析，其中：
- en: '![](img/B13208_02_041.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_041.png)'
- en: 'Let''s assume that ![](img/B13208_02_042.png), and we misclassify this example
    as *–1*. In this case, we would increase the corresponding weight by 1 so that
    the net input, ![](img/B13208_02_043.png), would be more positive the next time
    we encounter this example, and thus be more likely to be above the threshold of
    the unit step function to classify the example as *+1*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 ![](img/B13208_02_042.png)，并且我们错误地将该样本分类为 *–1*。在这种情况下，我们会将相应的权重增加 1，以便下次遇到该样本时，净输入（![](img/B13208_02_043.png)）会更为正向，从而更有可能超过单位阶跃函数的阈值，预测该样本为
    *+1*：
- en: '![](img/B13208_02_044.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_044.png)'
- en: 'The weight update is proportional to the value of ![](img/B13208_02_045.png).
    For instance, if we have another example, ![](img/B13208_02_046.png), that is
    incorrectly classified as *–1*, we will push the decision boundary by an even
    larger extent to classify this example correctly the next time:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 权重更新与![](img/B13208_02_045.png)的值成正比。例如，如果我们有另一个示例![](img/B13208_02_046.png)，它被错误地分类为
    *–1*，我们将会推动决策边界进一步调整，以便下次正确地分类此示例：
- en: '![](img/B13208_02_047.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_047.png)'
- en: 'It is important to note that the convergence of the perceptron is only guaranteed
    if the two classes are linearly separable and the learning rate is sufficiently
    small (interested readers can find the mathematical proof in my lecture notes:
    [https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf).).
    If the two classes can''t be separated by a linear decision boundary, we can set
    a maximum number of passes over the training dataset (**epochs**) and/or a threshold
    for the number of tolerated misclassifications—the perceptron would never stop
    updating the weights otherwise:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，感知机的收敛性只有在两个类别线性可分并且学习率足够小的情况下才能得到保证（感兴趣的读者可以在我的讲义中找到数学证明：[https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf)）。如果两个类别不能通过线性决策边界分开，我们可以设置一个最大遍历次数（**epochs**）和/或一个容忍误分类的阈值——否则，感知机将永远不会停止更新权重：
- en: '![](img/B13208_02_03.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_03.png)'
- en: '**Downloading the example code**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**下载示例代码**'
- en: If you bought this book directly from Packt, you can download the example code
    files from your account at [http://www.packtpub.com](http://www.packtpub.com).
    If you purchased this book elsewhere, you can download all code examples and datasets
    directly from [https://github.com/rasbt/python-machine-learning-book-3rd-edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是直接从Packt购买这本书，你可以从你的账户下载示例代码文件，访问[http://www.packtpub.com](http://www.packtpub.com)。如果你是从其他地方购买的这本书，你可以直接从[https://github.com/rasbt/python-machine-learning-book-3rd-edition](https://github.com/rasbt/python-machine-learning-book-3rd-edition)下载所有代码示例和数据集。
- en: 'Now, before we jump into the implementation in the next section, what you just
    learned can be summarized in a simple diagram that illustrates the general concept
    of the perceptron:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们进入下一节的实现之前，你刚刚学到的内容可以通过一个简单的图示来总结，这个图示展示了感知机的基本概念：
- en: '![](img/B13208_02_04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_04.png)'
- en: The preceding diagram illustrates how the perceptron receives the inputs of
    an example, **x**, and combines them with the weights, **w**, to compute the net
    input. The net input is then passed on to the threshold function, which generates
    a binary output of –1 or +1—the predicted class label of the example. During the
    learning phase, this output is used to calculate the error of the prediction and
    update the weights.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的图示说明了感知机如何接收一个示例的输入**x**，并将其与权重**w**结合来计算净输入。净输入然后传递给阈值函数，该函数生成一个二进制输出——–1
    或 +1——即该示例的预测类别标签。在学习阶段，这个输出用于计算预测的误差，并更新权重。
- en: Implementing a perceptron learning algorithm in Python
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在Python中实现感知机学习算法
- en: In the previous section, we learned how Rosenblatt's perceptron rule works;
    let's now implement it in Python and apply it to the Iris dataset that we introduced
    in *Chapter 1*, *Giving Computers the Ability to Learn from Data*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了Rosenblatt感知机规则的工作原理；现在让我们在Python中实现它，并将其应用于我们在*第1章*《让计算机从数据中学习》介绍的Iris数据集。
- en: An object-oriented perceptron API
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 面向对象的感知机API
- en: We will take an object-oriented approach to defining the perceptron interface
    as a Python class, which will allow us to initialize new `Perceptron` objects
    that can learn from data via a `fit` method, and make predictions via a separate
    `predict` method. As a convention, we append an underscore (`_`) to attributes
    that are not created upon the initialization of the object, but we do this by
    calling the object's other methods, for example, `self.w_`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用面向对象的方法来定义感知机接口作为Python类，这将允许我们初始化新的`Perceptron`对象，这些对象可以通过`fit`方法从数据中学习，并通过单独的`predict`方法进行预测。作为一种约定，我们将下划线（`_`）附加到那些在对象初始化时没有创建但通过调用对象的其他方法来创建的属性上，例如`self.w_`。
- en: '**Additional resources for Python''s scientific computing stack**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python科学计算栈的额外资源**'
- en: 'If you are not yet familiar with Python''s scientific libraries or need a refresher,
    please see the following resources:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还不熟悉 Python 的科学库，或者需要复习一下，请参考以下资源：
- en: '**NumPy**: [https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf)'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NumPy**: [https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf](https://sebastianraschka.com/pdf/books/dlb/appendix_f_numpy-intro.pdf)'
- en: '**pandas**: [https://pandas.pydata.org/pandas-docs/stable/10min.html](https://pandas.pydata.org/pandas-docs/stable/10min.html)'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**pandas**: [https://pandas.pydata.org/pandas-docs/stable/10min.html](https://pandas.pydata.org/pandas-docs/stable/10min.html)'
- en: '**Matplotlib**: [https://matplotlib.org/tutorials/introductory/usage.html](https://matplotlib.org/tutorials/introductory/usage.html)'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Matplotlib**: [https://matplotlib.org/tutorials/introductory/usage.html](https://matplotlib.org/tutorials/introductory/usage.html)'
- en: 'The following is the implementation of a perceptron in Python:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个感知机在 Python 中的实现：
- en: '[PRE0]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Using this perceptron implementation, we can now initialize new `Perceptron`
    objects with a given learning rate, `eta`, and the number of epochs, `n_iter`
    (passes over the training dataset).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个感知机实现，我们现在可以初始化具有给定学习率 `eta` 和迭代次数 `n_iter`（训练数据集的遍历次数）的新 `Perceptron` 对象。
- en: Via the `fit` method, we initialize the weights in `self.w_` to a vector, ![](img/B13208_02_048.png),
    where *m* stands for the number of dimensions (features) in the dataset, and we
    add *1* for the first element in this vector that represents the bias unit. Remember
    that the first element in this vector, `self.w_[0]`, represents the so-called
    bias unit that we discussed earlier.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 `fit` 方法，我们将 `self.w_` 中的权重初始化为一个向量，![](img/B13208_02_048.png)，其中 *m* 代表数据集中的维度（特征）数量，我们为该向量中的第一个元素添加
    *1*，表示偏置单元。记住，这个向量中的第一个元素 `self.w_[0]` 代表我们之前讨论过的所谓偏置单元。
- en: Also notice that this vector contains small random numbers drawn from a normal
    distribution with standard deviation 0.01 via `rgen.normal(loc=0.0, scale=0.01,
    size=1 + X.shape[1])`, where `rgen` is a NumPy random number generator that we
    seeded with a user-specified random seed so that we can reproduce previous results
    if desired.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 另外请注意，这个向量包含从正态分布中抽取的小随机数，标准差为 0.01，通过`rgen.normal(loc=0.0, scale=0.01, size=1
    + X.shape[1])`生成，其中 `rgen` 是一个 NumPy 随机数生成器，我们为其指定了一个用户定义的随机种子，以便在需要时能够重现之前的结果。
- en: 'It is important to keep in mind that we don''t initialize the weights to zero
    because the learning rate, ![](img/B13208_02_049.png) (`eta`), only has an effect
    on the classification outcome if the weights are initialized to non-zero values.
    If all the weights are initialized to zero, the learning rate parameter, `eta`,
    affects only the scale of the weight vector, not the direction. If you are familiar
    with trigonometry, consider a vector, ![](img/B13208_02_050.png), where the angle
    between ![](img/B13208_02_051.png) and a vector, ![](img/B13208_02_052.png), would
    be exactly zero, as demonstrated by the following code snippet:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的是，我们不将权重初始化为零，因为只有当权重初始化为非零值时，学习率 ![](img/B13208_02_049.png) (`eta`) 才会影响分类结果。如果所有权重都初始化为零，那么学习率参数
    `eta` 只会影响权重向量的规模，而不是方向。如果你熟悉三角学，考虑一个向量 ![](img/B13208_02_050.png)，其中 ![](img/B13208_02_051.png)
    和一个向量 ![](img/B13208_02_052.png) 之间的夹角将恰好为零，以下代码片段演示了这一点：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, `np.arccos` is the trigonometric inverse cosine, and `np.linalg.norm`
    is a function that computes the length of a vector (our decision to draw the random
    numbers from a random normal distribution—for example, instead of from a uniform
    distribution—and to use a standard deviation of `0.01` was arbitrary; remember,
    we are just interested in small random values to avoid the properties of all-zero
    vectors, as discussed earlier).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`np.arccos` 是三角函数的反余弦，`np.linalg.norm` 是一个计算向量长度的函数（我们选择从正态分布中抽取随机数，而不是从均匀分布中抽取，且标准差为
    `0.01` 是任意的；记住，我们只是希望得到小的随机值，以避免前面讨论的全零向量的特性）。
- en: '**NumPy array indexing**'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy 数组索引**'
- en: NumPy indexing for one-dimensional arrays works similarly to Python lists using
    the square-bracket (`[]`) notation. For two-dimensional arrays, the first indexer
    refers to the row number and the second indexer to the column number. For example,
    we would use `X[2, 3]` to select the third row and fourth column of a two-dimensional
    array, `X`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 对一维数组的索引方式类似于 Python 列表，使用方括号 (`[]`) 表示法。对于二维数组，第一个索引表示行号，第二个索引表示列号。例如，我们可以使用
    `X[2, 3]` 来选择二维数组 `X` 的第三行和第四列。
- en: After the weights have been initialized, the `fit` method loops over all individual
    examples in the training dataset and updates the weights according to the perceptron
    learning rule that we discussed in the previous section.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化权重之后，`fit`方法会遍历训练数据集中的所有单个样本，并根据我们在上一节中讨论的感知机学习规则更新权重。
- en: The class labels are predicted by the `predict` method, which is called in the
    `fit` method during training to get the class label for the weight update; but
    `predict` can also be used to predict the class labels of new data after we have
    fitted our model. Furthermore, we also collect the number of misclassifications
    during each epoch in the `self.errors_` list so that we can later analyze how
    well our perceptron performed during the training. The `np.dot` function that
    is used in the `net_input` method simply calculates the vector dot product, ![](img/B13208_02_053.png).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 类别标签由`predict`方法预测，该方法在训练过程中通过`fit`方法调用，以获取权重更新的类别标签；但是，`predict`也可以在我们拟合模型后用来预测新数据的类别标签。此外，我们还在`self.errors_`列表中收集每个时代的误分类数，以便稍后分析我们的感知机在训练过程中的表现。`net_input`方法中使用的`np.dot`函数简单地计算向量点积，![](img/B13208_02_053.png)。
- en: Instead of using NumPy to calculate the vector dot product between two arrays,
    `a` and `b`, via `a.dot(b)` or `np.dot(a, b)`, we could also perform the calculation
    in pure Python via `sum([i * j for i, j in zip(a, b)])`. However, the advantage
    of using NumPy over classic Python for loop structures is that its arithmetic
    operations are vectorized. **Vectorization** means that an elemental arithmetic
    operation is automatically applied to all elements in an array. By formulating
    our arithmetic operations as a sequence of instructions on an array, rather than
    performing a set of operations for each element at a time, we can make better
    use of our modern central processing unit (CPU) architectures with **single instruction,
    multiple data** (**SIMD**) support. Furthermore, NumPy uses highly optimized linear
    algebra libraries, such as **Basic Linear Algebra Subprograms** (**BLAS**) and
    **Linear Algebra Package** (**LAPACK**), that have been written in C or Fortran.
    Lastly, NumPy also allows us to write our code in a more compact and intuitive
    way using the basics of linear algebra, such as vector and matrix dot products.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用纯Python通过`sum([i * j for i, j in zip(a, b)])`来计算两个数组`a`和`b`的点积，而不是使用NumPy的`a.dot(b)`或`np.dot(a,
    b)`。然而，使用NumPy而不是经典的Python for循环结构的优势在于它的算术运算是矢量化的。**矢量化**意味着一个元素级别的算术运算会自动应用到数组中的所有元素。通过将我们的算术运算表示为数组上的一系列指令，而不是对每个元素执行一组操作，我们可以更好地利用现代中央处理单元（CPU）架构，尤其是支持**单指令、多数据**（**SIMD**）的架构。此外，NumPy还使用了高度优化的线性代数库，如**基本线性代数子程序**（**BLAS**）和**线性代数包**（**LAPACK**），这些库是用C或Fortran编写的。最后，NumPy还允许我们通过使用线性代数的基础知识，如向量和矩阵点积，编写更紧凑和直观的代码。
- en: Training a perceptron model on the Iris dataset
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在鸢尾花数据集上训练感知机模型
- en: To test our perceptron implementation, we will restrict the following analyses
    and examples in the remainder of this chapter to two feature variables (dimensions).
    Although the perceptron rule is not restricted to two dimensions, considering
    only two features, sepal length and petal length, will allow us to visualize the
    decision regions of the trained model in a scatter plot for learning purposes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的感知机实现，接下来的分析和示例将仅限于使用两个特征变量（维度）。尽管感知机规则不限于二维，考虑仅使用花萼长度和花瓣长度这两个特征将帮助我们在学习过程中通过散点图可视化训练模型的决策区域。
- en: Note that we will also only consider two flower classes, Setosa and Versicolor,
    from the Iris dataset for practical reasons—remember, the perceptron is a binary
    classifier. However, the perceptron algorithm can be extended to multi-class classification—for
    example, the **one-vs.-all** (**OvA**) technique.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，出于实际考虑，我们将只考虑鸢尾花数据集中的两类花卉——Setosa和Versicolor——记住，感知机是一个二分类器。然而，感知机算法可以扩展到多分类问题——例如，**一对多**（**OvA**）技术。
- en: '**The OvA method for multi-class classification**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**多分类的OvA方法**'
- en: OvA, which is sometimes also called **one-vs.-rest** (**OvR**), is a technique
    that allows us to extend any binary classifier to multi-class problems. Using
    OvA, we can train one classifier per class, where the particular class is treated
    as the positive class and the examples from all other classes are considered negative
    classes. If we were to classify a new, unlabeled data instance, we would use our
    *n* classifiers, where *n* is the number of class labels, and assign the class
    label with the highest confidence to the particular instance we want to classify.
    In the case of the perceptron, we would use OvA to choose the class label that
    is associated with the largest absolute net input value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: OvA，有时也被称为**一对多**（**OvR**），是一种技术，允许我们将任何二分类器扩展到多类问题。使用OvA，我们可以为每个类训练一个分类器，其中特定类被视为正类，所有其他类的样本被视为负类。如果我们要对一个新的、未标记的数据实例进行分类，我们将使用我们的*n*个分类器，其中*n*是类标签的数量，并将具有最高置信度的类标签分配给我们要分类的特定实例。在感知机的情况下，我们将使用OvA选择与最大绝对净输入值关联的类标签。
- en: 'First, we will use the `pandas` library to load the Iris dataset directly from
    the *UCI Machine Learning Repository* into a `DataFrame` object and print the
    last five lines via the `tail` method to check that the data was loaded correctly:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用`pandas`库直接从*UCI机器学习库*加载Iris数据集到一个`DataFrame`对象中，并通过`tail`方法打印最后五行，以检查数据是否正确加载：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/B13208_02_05.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_05.png)'
- en: '**Loading the Iris dataset**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**加载Iris数据集**'
- en: You can find a copy of the Iris dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)
    is temporarily unavailable. For instance, to load the Iris dataset from a local
    directory, you can replace this line,
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中找到一份Iris数据集的副本（以及本书中使用的所有其他数据集），如果你在离线工作或者UCI服务器（[https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data](https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data)）暂时无法访问时，可以使用该副本。例如，要从本地目录加载Iris数据集，你可以将这一行替换为：
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'with the following one:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码：
- en: '[PRE4]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we extract the first 100 class labels that correspond to the 50 Iris-setosa
    and 50 Iris-versicolor flowers and convert the class labels into the two integer
    class labels, `1` (versicolor) and `-1` (setosa), that we assign to a vector,
    `y`, where the `values` method of a pandas `DataFrame` yields the corresponding
    NumPy representation.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取前100个类标签，这些标签对应50个Iris-setosa花和50个Iris-versicolor花，并将类标签转换为两个整数类标签`1`（versicolor）和`-1`（setosa），我们将它们赋值给向量`y`，其中pandas
    `DataFrame`的`values`方法返回相应的NumPy表示。
- en: 'Similarly, we extract the first feature column (sepal length) and the third
    feature column (petal length) of those 100 training examples and assign them to
    a feature matrix, `X`, which we can visualize via a two-dimensional scatterplot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们提取这100个训练样本的第一个特征列（花萼长度）和第三个特征列（花瓣长度），并将它们赋值给特征矩阵`X`，我们可以通过二维散点图进行可视化：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'After executing the preceding code example, we should now see the following
    scatterplot:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码示例后，我们现在应该看到以下散点图：
- en: '![](img/B13208_02_06.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_06.png)'
- en: 'The preceding scatterplot shows the distribution of flower examples in the
    Iris dataset along the two feature axes: petal length and sepal length (measured
    in centimeters). In this two-dimensional feature subspace, we can see that a linear
    decision boundary should be sufficient to separate Setosa from Versicolor flowers.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的散点图显示了Iris数据集中花卉样本沿两个特征轴的分布：花瓣长度和花萼长度（以厘米为单位）。在这个二维特征子空间中，我们可以看到，线性决策边界应该足以将Setosa花与Versicolor花分开。
- en: Thus, a linear classifier such as the perceptron should be able to classify
    the flowers in this dataset perfectly.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，像感知机这样的线性分类器应该能够完美地对该数据集中的花进行分类。
- en: 'Now, it''s time to train our perceptron algorithm on the Iris data subset that
    we just extracted. Also, we will plot the misclassification error for each epoch
    to check whether the algorithm converged and found a decision boundary that separates
    the two Iris flower classes:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，到了训练我们的感知机算法的时候了，我们将使用刚刚提取的Iris数据子集。此外，我们还将绘制每个epoch的误分类错误，以检查算法是否收敛，并找到分离两个Iris花类的决策边界：
- en: '[PRE6]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After executing the preceding code, we should see the plot of the misclassification
    errors versus the number of epochs, as shown in the following graph:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述代码后，我们应该能看到误分类错误与迭代次数的关系图，如下图所示：
- en: '![](img/B13208_02_07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_07.png)'
- en: 'As we can see in the preceding plot, our perceptron converged after the sixth
    epoch and should now be able to classify the training examples perfectly. Let''s
    implement a small convenience function to visualize the decision boundaries for
    two-dimensional datasets:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在前面的图中所见，我们的感知机在第六次迭代后收敛，现在应该能够完美地分类训练样本。让我们实现一个小的便捷函数，来可视化二维数据集的决策边界：
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: First, we define a number of `colors` and `markers` and create a colormap from
    the list of colors via `ListedColormap`. Then, we determine the minimum and maximum
    values for the two features and use those feature vectors to create a pair of
    grid arrays, `xx1` and `xx2`, via the NumPy `meshgrid` function. Since we trained
    our perceptron classifier on two feature dimensions, we need to flatten the grid
    arrays and create a matrix that has the same number of columns as the Iris training
    subset so that we can use the `predict` method to predict the class labels, `Z`,
    of the corresponding grid points.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义一些`colors`和`markers`，并通过`ListedColormap`从颜色列表中创建一个颜色映射。然后，我们确定两个特征的最小值和最大值，并利用这些特征向量通过NumPy的`meshgrid`函数创建一对网格数组`xx1`和`xx2`。由于我们在两个特征维度上训练了感知机分类器，我们需要将网格数组展平，并创建一个具有与鸢尾花训练子集相同列数的矩阵，以便我们可以使用`predict`方法预测相应网格点的类别标签`Z`。
- en: 'After reshaping the predicted class labels, `Z`, into a grid with the same
    dimensions as `xx1` and `xx2`, we can now draw a contour plot via Matplotlib''s
    `contourf` function, which maps the different decision regions to different colors
    for each predicted class in the grid array:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在将预测的类别标签`Z`重新塑形为与`xx1`和`xx2`相同维度的网格后，我们可以通过Matplotlib的`contourf`函数绘制一个等高线图，将不同的决策区域映射到网格数组中每个预测类别的不同颜色：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After executing the preceding code example, we should now see a plot of the
    decision regions, as shown in the following figure:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 执行上述代码示例后，我们现在应该能够看到决策区域的图示，如下图所示：
- en: '![](img/B13208_02_08.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_08.png)'
- en: As we can see in the plot, the perceptron learned a decision boundary that is
    able to classify all flower examples in the Iris training subset perfectly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在图中所见，感知机学到了一个决策边界，能够完美地分类所有鸢尾花训练子集中的样本。
- en: '**Perceptron convergence**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机收敛性**'
- en: Although the perceptron classified the two Iris flower classes perfectly, convergence
    is one of the biggest problems of the perceptron. Rosenblatt proved mathematically
    that the perceptron learning rule converges if the two classes can be separated
    by a linear hyperplane. However, if the classes cannot be separated perfectly
    by such a linear decision boundary, the weights will never stop updating unless
    we set a maximum number of epochs. Interested readers can find a summary of the
    proof in my lecture notes at [https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管感知机能够完美地分类两个鸢尾花类别，但收敛性是感知机面临的最大问题之一。Rosenblatt通过数学证明，如果两个类别可以通过线性超平面分隔，感知机学习规则是会收敛的。然而，如果这些类别无法通过这样的线性决策边界完美分隔，权重将永远不会停止更新，除非我们设置最大迭代次数。感兴趣的读者可以在我的讲义中找到该证明的摘要，地址是[https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf](https://sebastianraschka.com/pdf/lecture-notes/stat479ss19/L03_perceptron_slides.pdf)。
- en: Adaptive linear neurons and the convergence of learning
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自适应线性神经元和学习的收敛性
- en: 'In this section, we will take a look at another type of single-layer neural
    network (NN): **ADAptive LInear NEuron** (**Adaline**). Adaline was published
    by Bernard Widrow and his doctoral student Tedd Hoff only a few years after Rosenblatt''s
    perceptron algorithm, and it can be considered an improvement on the latter (*An
    Adaptive "Adaline" Neuron Using Chemical "Memistors"*, *Technical Report Number
    1553-2, B. Widrow and others*, *Stanford Electron Labs*, Stanford, CA, *October
    1960*).'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将了解另一种类型的单层神经网络（NN）：**自适应线性神经元**（**Adaline**）。Adaline是由Bernard Widrow和他的博士生Tedd
    Hoff在Rosenblatt的感知机算法发布几年后提出的，可以看作是对感知机的改进（*An Adaptive "Adaline" Neuron Using
    Chemical "Memistors"*, *技术报告编号1553-2, B. Widrow及其他人*, *斯坦福电子实验室*, 斯坦福，加利福尼亚州，*1960年10月*）。
- en: The Adaline algorithm is particularly interesting because it illustrates the
    key concepts of defining and minimizing continuous cost functions. This lays the
    groundwork for understanding more advanced machine learning algorithms for classification,
    such as logistic regression, support vector machines, and regression models, which
    we will discuss in future chapters.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline 算法特别有趣，因为它展示了定义和最小化连续成本函数的关键概念。这为理解更高级的机器学习分类算法奠定了基础，如逻辑回归、支持向量机和回归模型，我们将在未来的章节中讨论这些内容。
- en: 'The key difference between the Adaline rule (also known as the **Widrow-Hoff
    rule**) and Rosenblatt''s perceptron is that the weights are updated based on
    a linear activation function rather than a unit step function like in the perceptron.
    In Adaline, this linear activation function, ![](img/B13208_02_054.png), is simply
    the identity function of the net input, so that:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Adaline 规则（也称为**Widrow-Hoff 规则**）与 Rosenblatt 的感知机之间的关键区别在于，权重是基于线性激活函数更新的，而不是像感知机那样基于单位阶跃函数。在
    Adaline 中，这个线性激活函数，![](img/B13208_02_054.png)，仅仅是净输入的恒等函数，因此：
- en: '![](img/B13208_02_055.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_055.png)'
- en: While the linear activation function is used for learning the weights, we still
    use a threshold function to make the final prediction, which is similar to the
    unit step function that we covered earlier.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然线性激活函数用于学习权重，但我们仍然使用阈值函数进行最终预测，这与我们之前讨论的单位阶跃函数相似。
- en: 'The main differences between the perceptron and Adaline algorithm are highlighted
    in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 感知机与 Adaline 算法之间的主要区别在下图中得到了突出显示：
- en: '![](img/B13208_02_09.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_09.png)'
- en: As the illustration indicates, the Adaline algorithm compares the true class
    labels with the linear activation function's continuous valued output to compute
    the model error and update the weights. In contrast, the perceptron compares the
    true class labels to the predicted class labels.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如图所示，Adaline 算法将真实类别标签与线性激活函数的连续值输出进行比较，以计算模型误差并更新权重。相比之下，感知机将真实类别标签与预测的类别标签进行比较。
- en: Minimizing cost functions with gradient descent
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用梯度下降最小化成本函数
- en: 'One of the key ingredients of supervised machine learning algorithms is a defined
    **objective function** that is to be optimized during the learning process. This
    objective function is often a cost function that we want to minimize. In the case
    of Adaline, we can define the cost function, *J*, to learn the weights as the
    **sum of squared errors** (**SSE**) between the calculated outcome and the true
    class label:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 监督式机器学习算法的关键组成部分之一是一个定义好的**目标函数**，该函数将在学习过程中进行优化。这个目标函数通常是我们希望最小化的成本函数。在 Adaline
    的情况下，我们可以定义成本函数 *J*，以学习权重为目标，作为计算结果与真实类别标签之间的**平方误差之和**（**SSE**）：
- en: '![](img/B13208_02_056.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_056.png)'
- en: The term ![](img/B13208_02_057.png) is just added for our convenience and will
    make it easier to derive the gradient of the cost or loss function with respect
    to the weight parameters, as we will see in the following paragraphs. The main
    advantage of this continuous linear activation function, in contrast to the unit
    step function, is that the cost function becomes differentiable. Another nice
    property of this cost function is that it is convex; thus, we can use a very simple
    yet powerful optimization algorithm called **gradient descent** to find the weights
    that minimize our cost function to classify the examples in the Iris dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 术语 ![](img/B13208_02_057.png) 仅为我们的方便而加上，它将使我们更容易推导出成本或损失函数关于权重参数的梯度，正如我们将在下面的段落中看到的那样。与单位阶跃函数相比，这个连续的线性激活函数的主要优点是成本函数变得可微分。这个成本函数的另一个优良性质是它是凸的；因此，我们可以使用一种非常简单却强大的优化算法，称为**梯度下降**，来找到最小化我们成本函数的权重，从而对
    Iris 数据集中的样本进行分类。
- en: 'As illustrated in the following figure, we can describe the main idea behind
    gradient descent as *climbing down a hill* until a local or global cost minimum
    is reached. In each iteration, we take a step in the opposite direction of the
    gradient, where the step size is determined by the value of the learning rate,
    as well as the slope of the gradient:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，我们可以将梯度下降的主要思想描述为*爬山*，直到达到局部或全局成本最小值。在每次迭代中，我们都会朝着梯度的相反方向迈出一步，其中步长由学习率的值以及梯度的坡度决定：
- en: '![](img/B13208_02_10.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_10.png)'
- en: 'Using gradient descent, we can now update the weights by taking a step in the
    opposite direction of the gradient, ![](img/B13208_02_058.png), of our cost function,
    ![](img/B13208_02_059.png):'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度下降法，我们现在可以通过在成本函数的梯度！[](img/B13208_02_058.png)的相反方向上采取一步来更新权重！[](img/B13208_02_059.png)：
- en: '![](img/B13208_02_060.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_060.png)'
- en: 'The weight change, ![](img/B13208_02_061.png), is defined as the negative gradient
    multiplied by the learning rate, ![](img/B13208_02_062.png):'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 权重变化！[](img/B13208_02_061.png)定义为负梯度乘以学习率！[](img/B13208_02_062.png)：
- en: '![](img/B13208_02_063.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_063.png)'
- en: 'To compute the gradient of the cost function, we need to compute the partial
    derivative of the cost function with respect to each weight, ![](img/B13208_02_064.png):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算成本函数的梯度，我们需要计算成本函数相对于每个权重的偏导数！[](img/B13208_02_064.png)：
- en: '![](img/B13208_02_065.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_065.png)'
- en: 'So we can write the update of weight ![](img/B13208_02_066.png) as:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以写出权重！[](img/B13208_02_066.png)的更新公式：
- en: '![](img/B13208_02_067.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_067.png)'
- en: 'Since we update all weights simultaneously, our Adaline learning rule becomes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们同时更新所有权重，我们的Adaline学习规则变成了：
- en: '![](img/B13208_02_068.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_068.png)'
- en: '**The squared error derivative**'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '**平方误差导数**'
- en: 'If you are familiar with calculus, the partial derivative of the SSE cost function
    with respect to the *j*th weight can be obtained as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉微积分，SSE成本函数相对于第*j*个权重的偏导数可以通过以下方式得到：
- en: '![](img/B13208_02_069.png)![](img/B13208_02_070.png)![](img/B13208_02_071.png)![](img/B13208_02_072.png)![](img/B13208_02_073.png)![](img/B13208_02_074.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_069.png)![](img/B13208_02_070.png)![](img/B13208_02_071.png)![](img/B13208_02_072.png)![](img/B13208_02_073.png)![](img/B13208_02_074.png)'
- en: Although the Adaline learning rule looks identical to the perceptron rule, we
    should note that ![](img/B13208_02_075.png) with ![](img/B13208_02_076.png) is
    a real number and not an integer class label. Furthermore, the weight update is
    calculated based on all examples in the training dataset (instead of updating
    the weights incrementally after each training example), which is why this approach
    is also referred to as **batch gradient descent**.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Adaline学习规则看起来与感知机规则相同，我们应该注意到！[](img/B13208_02_075.png)与！[](img/B13208_02_076.png)是实数而非整数类别标签。此外，权重更新是基于训练数据集中的所有样本计算的（而不是在每个训练样本后增量更新权重），这就是为什么这种方法也被称为**批量梯度下降**。
- en: Implementing Adaline in Python
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现Adaline
- en: 'Since the perceptron rule and Adaline are very similar, we will take the perceptron
    implementation that we defined earlier and change the `fit` method so that the
    weights are updated by minimizing the cost function via gradient descent:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于感知机规则和Adaline非常相似，我们将采用之前定义的感知机实现，并修改`fit`方法，使得权重通过梯度下降法最小化成本函数来更新：
- en: '[PRE9]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Instead of updating the weights after evaluating each individual training example,
    as in the perceptron, we calculate the gradient based on the whole training dataset
    via `self.eta * errors.sum()` for the bias unit (zero-weight), and via `self.eta
    * X.T.dot(errors)` for the weights 1 to *m*, where `X.T.dot(errors)` is a matrix-vector
    multiplication between our feature matrix and the error vector.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 不像感知机在评估每个训练样本后更新权重，我们通过`self.eta * errors.sum()`来计算偏置单元（零权重）的梯度，使用`self.eta
    * X.T.dot(errors)`计算权重1到*m*的梯度，其中`X.T.dot(errors)`是特征矩阵与误差向量的矩阵-向量乘法。
- en: 'Please note that the `activation` method has no effect in the code since it
    is simply an identity function. Here, we added the activation function (computed
    via the `activation` method) to illustrate the general concept with regard to
    how information flows through a single-layer NN: features from the input data,
    net input, activation, and output.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`activation`方法在代码中没有效果，因为它只是一个恒等函数。在这里，我们添加了激活函数（通过`activation`方法计算）以说明单层神经网络中信息流动的一般概念：从输入数据的特征、净输入、激活到输出。
- en: In the next chapter, we will learn about a logistic regression classifier that
    uses a non-identity, nonlinear activation function. We will see that a logistic
    regression model is closely related to Adaline, with the only difference being
    its activation and cost function.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习使用非恒等、非线性激活函数的逻辑回归分类器。我们将看到逻辑回归模型与Adaline密切相关，唯一的区别是它的激活函数和成本函数。
- en: Now, similar to the previous perceptron implementation, we collect the cost
    values in a `self.cost_` list to check whether the algorithm converged after training.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，与之前的感知机实现类似，我们将成本值收集到 `self.cost_` 列表中，以检查算法在训练后是否已经收敛。
- en: '**Matrix multiplication**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**矩阵乘法**'
- en: 'Performing a matrix multiplication is similar to calculating a vector dot-product
    where each row in the matrix is treated as a single row vector. This vectorized
    approach represents a more compact notation and results in a more efficient computation
    using NumPy. For example:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 执行矩阵乘法类似于计算向量点积，其中矩阵中的每一行都被视为一个单独的行向量。这种向量化的方法代表了更紧凑的符号，并且使用 NumPy 可以实现更高效的计算。例如：
- en: '![](img/B13208_02_077.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_077.png)'
- en: Please note that in the preceding equation, we are multiplying a matrix with
    a vector, which is mathematically not defined. However, remember that we use the
    convention that this preceding vector is regarded as a ![](img/B13208_02_078.png)
    matrix.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的方程中，我们正在将矩阵与向量相乘，这在数学上是未定义的。然而，请记住，我们使用的约定是，将前面的向量视为一个 ![](img/B13208_02_078.png)
    矩阵。
- en: In practice, it often requires some experimentation to find a good learning
    rate, ![](img/B13208_02_079.png), for optimal convergence. So, let's choose two
    different learning rates, ![](img/B13208_02_080.png) and ![](img/B13208_02_081.png),
    to start with and plot the cost functions versus the number of epochs to see how
    well the Adaline implementation learns from the training data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，通常需要一些实验来找到一个合适的学习率，![](img/B13208_02_079.png)，以实现最佳的收敛。因此，我们先选择两个不同的学习率，![](img/B13208_02_080.png)
    和 ![](img/B13208_02_081.png)，并绘制成本函数与迭代次数的关系图，以观察 Adaline 实现从训练数据中学习的效果。
- en: '**Perceptron hyperparameters**'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '**感知机超参数**'
- en: The learning rate, ![](img/B13208_02_082.png), (`eta`), as well as the number
    of epochs (`n_iter`), are the so-called hyperparameters (or tuning parameters)
    of the perceptron and Adaline learning algorithms. In *Chapter 6*, *Learning Best
    Practices for Model Evaluation and Hyperparameter Tuning*, we will take a look
    at different techniques to automatically find the values of different hyperparameters
    that yield optimal performance of the classification model.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 ![](img/B13208_02_082.png)（`eta`），以及迭代次数（`n_iter`）是感知机和 Adaline 学习算法的超参数（或调节参数）。在
    *第六章*，*模型评估和超参数调优的最佳实践* 中，我们将探讨不同的技术，以自动找到能够实现分类模型最佳性能的超参数值。
- en: 'Let''s now plot the cost against the number of epochs for the two different
    learning rates:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制两个不同学习率的成本与迭代次数的关系图：
- en: '[PRE10]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see in the resulting cost-function plots, we encountered two different
    types of problem. The left chart shows what could happen if we choose a learning
    rate that is too large. Instead of minimizing the cost function, the error becomes
    larger in every epoch, because we *overshoot* the global minimum. On the other
    hand, we can see that the cost decreases on the right plot, but the chosen learning
    rate, ![](img/B13208_02_083.png), is so small that the algorithm would require
    a very large number of epochs to converge to the global cost minimum:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在成本函数图中看到的那样，我们遇到了两种不同类型的问题。左侧图表显示了如果我们选择一个过大的学习率可能会发生的情况。由于我们 *越过* 了全局最小值，每个迭代中的误差变得更大，而不是最小化成本函数。另一方面，我们可以看到右侧图表中的成本在下降，但所选择的学习率
    ![](img/B13208_02_083.png) 太小，以至于算法需要大量的迭代才能收敛到全局成本最小值。
- en: '![](img/B13208_02_11.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_11.png)'
- en: The following figure illustrates what might happen if we change the value of
    a particular weight parameter to minimize the cost function, *J*. The left subfigure
    illustrates the case of a well-chosen learning rate, where the cost decreases
    gradually, moving in the direction of the global minimum.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下图说明了如果我们改变某个特定权重参数的值来最小化成本函数 *J* 可能发生的情况。左侧子图展示了选择合适学习率的情况，其中成本逐渐下降，朝着全局最小值的方向移动。
- en: 'The subfigure on the right, however, illustrates what happens if we choose
    a learning rate that is too large—we overshoot the global minimum:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，右侧子图说明了如果我们选择一个过大的学习率会发生什么——我们会越过全局最小值：
- en: '![](img/B13208_02_12.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_12.png)'
- en: Improving gradient descent through feature scaling
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过特征缩放来改进梯度下降
- en: Many machine learning algorithms that we will encounter throughout this book
    require some sort of feature scaling for optimal performance, which we will discuss
    in more detail in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*,
    and *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中我们将遇到的许多机器学习算法需要某种特征缩放来实现最佳性能，关于这一点我们将在*第3章*《使用scikit-learn的机器学习分类器巡礼》和*第4章*《构建良好的训练数据集——数据预处理》中详细讨论。
- en: 'Gradient descent is one of the many algorithms that benefit from feature scaling.
    In this section, we will use a feature scaling method called **standardization**,
    which gives our data the properties of a standard normal distribution: zero-mean
    and unit variance. This normalization procedure helps gradient descent learning
    to converge more quickly; however, it does not make the original dataset normally
    distributed. Standardization shifts the mean of each feature so that it is centered
    at zero and each feature has a standard deviation of 1 (unit variance). For instance,
    to standardize the *j*th feature, we can simply subtract the sample mean, ![](img/B13208_02_084.png),
    from every training example and divide it by its standard deviation, ![](img/B13208_02_085.png):'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是许多从特征缩放中受益的算法之一。在本节中，我们将使用一种名为**标准化**的特征缩放方法，它赋予我们的数据标准正态分布的特性：零均值和单位方差。这个归一化过程有助于梯度下降学习更快地收敛；然而，它并不会使原始数据集变为正态分布。标准化将每个特征的均值平移，使其居中于零，并且每个特征的标准差为1（单位方差）。例如，为了标准化第*j*个特征，我们可以简单地从每个训练样本中减去样本均值![](img/B13208_02_084.png)，并将其除以标准差![](img/B13208_02_085.png)：
- en: '![](img/B13208_02_086.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_086.png)'
- en: Here, ![](img/B13208_02_087.png) is a vector consisting of the *j*th feature
    values of all training examples, *n*, and this standardization technique is applied
    to each feature, *j*, in our dataset.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_02_087.png)是一个由所有训练示例的第*j*个特征值构成的向量，*n*，该标准化技术应用于数据集中的每个特征*j*。
- en: 'One of the reasons why standardization helps with gradient descent learning
    is that the optimizer has to go through fewer steps to find a good or optimal
    solution (the global cost minimum), as illustrated in the following figure, where
    the subfigures represent the cost surface as a function of two model weights in
    a two-dimensional classification problem:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化有助于梯度下降学习的原因之一是，优化器需要经过更少的步骤来找到一个好的或最优的解决方案（全局成本最小值），如下图所示，其中子图表示在一个二维分类问题中，成本面作为两个模型权重的函数：
- en: '![](img/B13208_02_13.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_13.png)'
- en: 'Standardization can easily be achieved by using the built-in NumPy methods
    `mean` and `std`:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化可以通过使用内置的NumPy方法`mean`和`std`轻松实现：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'After standardization, we will train Adaline again and we will see that it
    now converges after a small number of epochs using a learning rate of ![](img/B13208_02_088.png):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化之后，我们将再次训练Adaline，并会看到它在少数几个epoch后使用学习率![](img/B13208_02_088.png)就收敛了：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After executing this code, we should see a figure of the decision regions,
    as well as a plot of the declining cost, as shown in the following figure:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们应该看到决策区域的图形，以及下降成本的图表，如下图所示：
- en: '![](img/B13208_02_14.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_14.png)'
- en: As we can see in the plots, Adaline has now converged after training on the
    standardized features using a learning rate of ![](img/B13208_02_089.png). However,
    note that the SSE remains non-zero even though all flower examples were classified
    correctly.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在图表中看到的，Adaline在使用学习率![](img/B13208_02_089.png)对标准化特征进行训练后已收敛。然而，注意尽管所有的花卉示例都已正确分类，SSE仍然不为零。
- en: Large-scale machine learning and stochastic gradient descent
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大规模机器学习和随机梯度下降
- en: In the previous section, we learned how to minimize a cost function by taking
    a step in the opposite direction of a cost gradient that is calculated from the
    whole training dataset; this is why this approach is sometimes also referred to
    as **batch gradient descent**. Now imagine that we have a very large dataset with
    millions of data points, which is not uncommon in many machine learning applications.
    Running batch gradient descent can be computationally quite costly in such scenarios,
    since we need to reevaluate the whole training dataset each time that we take
    one step toward the global minimum.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一部分中，我们学习了如何通过朝着代价梯度的相反方向迈出一步来最小化代价函数，该代价梯度是从整个训练数据集中计算得出的；因此，这种方法有时也被称为**批量梯度下降**。现在假设我们有一个非常大的数据集，其中包含数百万个数据点，这在许多机器学习应用中并不罕见。在这种情况下，运行批量梯度下降的计算成本可能会非常高，因为每次向全局最小值迈出一步时，我们都需要重新评估整个训练数据集。
- en: 'A popular alternative to the batch gradient descent algorithm is **stochastic
    gradient descent (SGD)**, which is sometimes also called iterative or online gradient
    descent. Instead of updating the weights based on the sum of the accumulated errors
    over all training examples, ![](img/B13208_02_090.png):'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降算法的一个流行替代方法是**随机梯度下降（SGD）**，有时也被称为迭代或在线梯度下降。它不是基于所有训练样本的累积误差总和来更新权重，而是基于每个训练样本的误差来更新权重！[](img/B13208_02_090.png)：
- en: '![](img/B13208_02_091.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_091.png)'
- en: 'we update the weights incrementally for each training example:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对每个训练样本逐步更新权重：
- en: '![](img/B13208_02_092.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_092.png)'
- en: Although SGD can be considered as an approximation of gradient descent, it typically
    reaches convergence much faster because of the more frequent weight updates. Since
    each gradient is calculated based on a single training example, the error surface
    is noisier than in gradient descent, which can also have the advantage that SGD
    can escape shallow local minima more readily if we are working with nonlinear
    cost functions, as we will see later in *Chapter 12*, *Implementing a Multilayer
    Artificial Neural Network from Scratch*. To obtain satisfying results via SGD,
    it is important to present training data in a random order; also, we want to shuffle
    the training dataset for every epoch to prevent cycles.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SGD可以看作是梯度下降的一种近似方法，但由于更频繁的权重更新，它通常能更快地达到收敛。由于每个梯度是基于单个训练样本计算的，因此误差面比梯度下降更加嘈杂，这也带来了一个优势，即如果我们处理的是非线性代价函数，SGD可以更容易地逃脱浅层局部最小值，正如我们将在*第12章*中看到的，*从头开始实现多层人工神经网络*。为了通过SGD获得令人满意的结果，重要的是将训练数据按随机顺序呈现；此外，我们还希望在每个周期后对训练数据集进行洗牌，以防止出现周期性问题。
- en: '**Adjusting the learning rate during training**'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '**在训练过程中调整学习率**'
- en: 'In SGD implementations, the fixed learning rate, ![](img/B13208_02_093.png),
    is often replaced by an adaptive learning rate that decreases over time, for example:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在SGD的实现中，固定的学习率，![](img/B13208_02_093.png)，通常会被一个随着时间减少的自适应学习率替代，例如：
- en: '![](img/B13208_02_094.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_094.png)'
- en: where ![](img/B13208_02_095.png) and ![](img/B13208_02_096.png) are constants.
    Note that SGD does not reach the global minimum but an area very close to it.
    And using an adaptive learning rate, we can achieve further annealing to the cost
    minimum.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其中![](img/B13208_02_095.png)和![](img/B13208_02_096.png)是常数。请注意，SGD并未达到全局最小值，而是达到了一个非常接近全局最小值的区域。通过使用自适应学习率，我们可以进一步进行退火，以便更好地接近代价最小值。
- en: Another advantage of SGD is that we can use it for **online learning**. In online
    learning, our model is trained on the fly as new training data arrives. This is
    especially useful if we are accumulating large amounts of data, for example, customer
    data in web applications. Using online learning, the system can immediately adapt
    to changes, and the training data can be discarded after updating the model if
    storage space is an issue.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降（SGD）的另一个优点是，我们可以用它进行**在线学习**。在在线学习中，随着新训练数据的到来，我们的模型会实时进行训练。这在我们积累大量数据时尤其有用，例如，Web应用中的客户数据。使用在线学习，系统可以立即适应变化，并且如果存储空间有限，更新模型后可以丢弃训练数据。
- en: '**Mini-batch gradient descent**'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '**小批量梯度下降**'
- en: A compromise between batch gradient descent and SGD is so-called **mini-batch
    learning**. Mini-batch learning can be understood as applying batch gradient descent
    to smaller subsets of the training data, for example, 32 training examples at
    a time. The advantage over batch gradient descent is that convergence is reached
    faster via mini-batches because of the more frequent weight updates. Furthermore,
    mini-batch learning allows us to replace the `for` loop over the training examples
    in SGD with vectorized operations leveraging concepts from linear algebra (for
    example, implementing a weighted sum via a dot product), which can further improve
    the computational efficiency of our learning algorithm.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降和 SGD 之间的折衷方法是所谓的 **小批量学习**。小批量学习可以理解为将批量梯度下降应用于较小的训练数据子集，例如一次处理 32 个训练样本。与批量梯度下降相比，小批量学习的优势在于，通过更频繁的权重更新，小批量方法能够更快地收敛。此外，小批量学习还允许我们将
    SGD 中对训练样本的 `for` 循环替换为利用线性代数概念（例如，通过点积实现加权和）的向量化操作，从而进一步提高学习算法的计算效率。
- en: 'Since we already implemented the Adaline learning rule using gradient descent,
    we only need to make a few adjustments to modify the learning algorithm to update
    the weights via SGD. Inside the `fit` method, we will now update the weights after
    each training example. Furthermore, we will implement an additional `partial_fit`
    method, which does not reinitialize the weights, for online learning. In order
    to check whether our algorithm converged after training, we will calculate the
    cost as the average cost of the training examples in each epoch. Furthermore,
    we will add an option to shuffle the training data before each epoch to avoid
    repetitive cycles when we are optimizing the cost function; via the `random_state`
    parameter, we allow the specification of a random seed for reproducibility:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经使用梯度下降实现了 Adaline 学习规则，我们只需要做一些调整，就可以修改学习算法通过 SGD 更新权重。在 `fit` 方法中，我们现在将在每个训练样本之后更新权重。此外，我们将实现一个额外的
    `partial_fit` 方法，用于在线学习，该方法不会重新初始化权重。为了检查我们的算法在训练后是否已收敛，我们将计算成本，即每个周期中训练样本的平均成本。此外，我们将添加一个选项，在每个周期之前打乱训练数据，以避免在优化成本函数时出现重复的周期；通过
    `random_state` 参数，我们允许指定随机种子以确保可复现性：
- en: '[PRE13]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `_shuffle` method that we are now using in the `AdalineSGD` classifier
    works as follows: via the `permutation` function in `np.random`, we generate a
    random sequence of unique numbers in the range 0 to 100\. Those numbers can then
    be used as indices to shuffle our feature matrix and class label vector.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在 `AdalineSGD` 分类器中使用的 `_shuffle` 方法如下工作：通过 `np.random` 中的 `permutation`
    函数，我们生成一个从 0 到 100 的唯一数字的随机序列。然后可以将这些数字用作索引来打乱我们的特征矩阵和类别标签向量。
- en: 'We can then use the `fit` method to train the `AdalineSGD` classifier and use
    our `plot_decision_regions` to plot our training results:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `fit` 方法训练 `AdalineSGD` 分类器，并使用我们的 `plot_decision_regions` 绘制训练结果：
- en: '[PRE14]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The two plots that we obtain from executing the preceding code example are
    shown in the following figure:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码示例后，我们获得的两个图表如下所示：
- en: '![](img/B13208_02_15.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_02_15.png)'
- en: As you can see, the average cost goes down pretty quickly, and the final decision
    boundary after 15 epochs looks similar to the batch gradient descent Adaline.
    If we want to update our model, for example, in an online learning scenario with
    streaming data, we could simply call the `partial_fit` method on individual training
    examples—for instance `ada_sgd.partial_fit(X_std[0, :], y[0])`.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，平均成本下降得非常快，经过 15 个周期后的最终决策边界与批量梯度下降的 Adaline 相似。如果我们想要更新模型，例如在流式数据的在线学习场景中，我们可以简单地对单个训练样本调用
    `partial_fit` 方法——例如 `ada_sgd.partial_fit(X_std[0, :], y[0])`。
- en: Summary
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we gained a good understanding of the basic concepts of linear
    classifiers for supervised learning. After we implemented a perceptron, we saw
    how we can train adaptive linear neurons efficiently via a vectorized implementation
    of gradient descent and online learning via SGD.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对监督学习中的线性分类器的基本概念有了较为深入的了解。在实现感知器之后，我们看到了如何通过梯度下降的向量化实现和通过 SGD 的在线学习高效训练自适应线性神经元。
- en: Now that we have seen how to implement simple classifiers in Python, we are
    ready to move on to the next chapter, where we will use the Python scikit-learn
    machine learning library to get access to more advanced and powerful machine learning
    classifiers, which are commonly used in academia as well as in industry.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在Python中实现简单的分类器，接下来我们准备进入下一章，在那里我们将使用Python的scikit-learn机器学习库，获取更高级和更强大的机器学习分类器，这些分类器在学术界和工业界都有广泛应用。
- en: 'The object-oriented approach that we used to implement the perceptron and Adaline
    algorithms will help with understanding the scikit-learn API, which is implemented
    based on the same core concepts that we used in this chapter: the `fit` and `predict`
    methods. Based on these core concepts, we will learn about logistic regression
    for modeling class probabilities and support vector machines for working with
    nonlinear decision boundaries. In addition, we will introduce a different class
    of supervised learning algorithms, tree-based algorithms, which are commonly combined
    into robust ensemble classifiers.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用于实现感知机和Adaline算法的面向对象方法将有助于理解scikit-learn API，它基于我们在本章中使用的相同核心概念实现：`fit`和`predict`方法。基于这些核心概念，我们将学习用于建模类别概率的逻辑回归以及用于处理非线性决策边界的支持向量机。此外，我们还将介绍一种不同类别的监督学习算法——基于树的算法，这些算法通常会结合成强大的集成分类器。
