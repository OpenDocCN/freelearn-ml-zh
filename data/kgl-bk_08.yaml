- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Designing Good Validation
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设计良好的验证
- en: In a Kaggle competition, in the heat of modeling and submitting results, it
    may seem enough to take at face value the results you get back from the leaderboard.
    In the end, you may think that what counts in a competition is your ranking. This
    is a common error that is made repeatedly in competitions. In actual fact, you
    won’t know what the actual leaderboard (the private one) looks like until after
    the competition has closed, and trusting the public part of it is not advisable
    because it is quite often misleading.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kaggle 竞赛中，在建模和提交结果的紧张时刻，可能会觉得只从排行榜上得到的结果就足够了。最终，你可能会认为在比赛中重要的是你的排名。这是一个在比赛中反复犯下的常见错误。实际上，直到比赛结束后，你才知道实际的排行榜（私有部分）是什么样的，而且信任它的公开部分是不明智的，因为它往往具有误导性。
- en: 'In this chapter, we will introduce you to the importance of **validation**
    in data competitions. You will learn about:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将向您介绍在数据竞赛中**验证**的重要性。您将了解：
- en: What overfitting is and how a public leaderboard can be misleading
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合是什么以及公开排行榜可能具有误导性
- en: The dreadful shake-ups
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恐怖的震动
- en: The different kinds of validation strategies
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的验证策略
- en: Adversarial validation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性验证
- en: How to spot and leverage leakages
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何发现并利用泄露
- en: What your strategies should be when choosing your final submissions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当选择最终提交时，你的策略应该是什么
- en: Monitoring your performances when modeling and distinguishing when overfitting
    happens is a key competency not only in data science competitions but in all data
    science projects. Validating your models properly is one of the most important
    skills that you can learn from a Kaggle competition and that you can resell in
    the professional world.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模时监控你的表现，以及在何时发生过拟合时进行区分，这不仅是在数据科学竞赛中，而且在所有数据科学项目中的一项关键能力。正确验证你的模型是你可以从 Kaggle
    竞赛中学到并能在专业世界中再次出售的最重要技能之一。
- en: Snooping on the leaderboard
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 排行榜上的窥探
- en: As we previously described, in each competition, Kaggle divides the test set
    into a **public part**, which is visualized on the ongoing leaderboard, and a
    **private part**, which will be used to calculate the final scores. These test
    parts are usually randomly determined (although in time series competitions, they
    are determined based on time) and the entire test set is released without any
    distinction made between public and private.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，在每一场比赛中，Kaggle 将测试集分为一个**公开部分**，这部分在实时排行榜上可视化，以及一个**私有部分**，这部分将用于计算最终分数。这些测试部分通常是随机确定的（尽管在时间序列比赛中，它们是基于时间确定的）并且整个测试集发布时不会区分公开和私有。
- en: Recently, in order to avoid the scrutinizing of test data in certain competitions,
    Kaggle has even held back the test data, providing only some examples of it and
    replacing them with the real test set when the submission is made. These are called
    **Code** competitions because you are not actually providing the predictions themselves,
    but a Notebook containing the code to generate them.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，为了防止某些比赛中对测试数据的审查，Kaggle 甚至推迟了测试数据的发布，只提供一些示例，并在提交时用真实的测试集替换它们。这些被称为**代码**竞赛，因为你实际上并没有提供预测本身，而是一个包含生成这些预测的代码的笔记本。
- en: Therefore, a submission derived from a model will cover the entire test set,
    but only the public part will immediately be scored, leaving the scoring of the
    private part until after the competition has closed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从模型中得出的提交将覆盖整个测试集，但只有公开部分会立即评分，而私有部分的评分将留待比赛结束后进行。
- en: 'Given this, three considerations arise:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，出现了三个考虑因素：
- en: In order for a competition to work properly, training data and test data should
    be from **the same distribution**. Moreover, the private and public parts of the
    test data should resemble each other in terms of distribution.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使比赛能够正常进行，训练数据和测试数据应来自**相同的分布**。此外，测试数据的公开和私有部分在分布上应该相似。
- en: Even if the training and test data are apparently from the same distribution,
    the **lack of sufficient examples** in either set could make it difficult to obtain
    aligned results between the training data and the public and private test data.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使训练数据和测试数据显然来自相同的分布，但任一集中**示例不足**可能会使得在训练数据和公开以及私有测试数据之间获得一致的结果变得困难。
- en: 'The public test data should be regarded as a holdout test in a data science
    project: to be used only for final validation. Hence, it should not be queried
    much in order to avoid what is called **adaptive overfitting**, which implies
    a model that works well on a specific test set but underperforms on others.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公共测试数据应被视为数据科学项目中的一个保留测试：仅用于最终验证。因此，不应过多查询，以避免所谓的**适应性过拟合**，这意味着模型在特定测试集上表现良好，但在其他测试集上表现不佳。
- en: Keeping in mind these three considerations is paramount to understanding the
    dynamics of a competition. In most competitions, there are always quite a few
    questions in the discussion forums about how the training, public, and private
    test data relate to each other, and it is quite common to see submissions of hundreds
    of solutions that have only been evaluated based on their efficacy on the public
    leaderboard.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑这三个因素对于理解比赛的动态至关重要。在大多数比赛中，讨论论坛中总是有很多关于训练、公共和私有测试数据之间如何相互关联的问题，而且看到基于公共排行榜有效性评估的数百个解决方案的提交是非常常见的。
- en: 'It is also common to hear discussions about **shake-ups** that revolutionize
    the rankings. They are, in fact, a rearranging of the final rankings that can
    disappoint many who previously held better positions on the public leaderboard.
    Anecdotally, shake-ups are commonly attributed to differences between the training
    and test set or between the private and public parts of the test data. They are
    measured *ex ante* based on how competitors have seen their expected local scores
    correlate with the leaderboard feedback and *ex post* by a series of analyses
    based on two figures:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人们也常常听到关于**动荡**的讨论，这些讨论可能会彻底改变排名。实际上，这些动荡是对最终排名的一种重新排列，可能会让许多之前在公共排行榜上排名更好的人感到失望。据传闻，动荡通常归因于训练集和测试集之间或测试数据的私有部分和公共部分之间的差异。它们是根据竞争者如何看到他们的预期局部得分与排行榜反馈的相关性进行**事前**评估的，并通过一系列基于两个数字的分析进行**事后**评估：
- en: A general shake-up figure based on `mean(abs(private_rank-public_rank)/number_of_teams)`
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于平均（绝对值（私有排名-公共排名）/队伍数量）的总体动荡数字
- en: A top leaderboard shake-up figure, taking into account only the top 10% of public
    ranks
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名前列的动荡人物，仅考虑公共排名的前10%
- en: These *ex post* figures were first devised by *Steve Donoho* ([https://www.kaggle.com/breakfastpirate](https://www.kaggle.com/breakfastpirate))
    who compiled a ranking of the worst Kaggle shake-ups (see [https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831)).
    They are nowadays easily available, recreated by many Notebooks based on the Meta
    Kaggle dataset we discussed in *Chapter 5*, *Competition Tasks and Metrics* (see
    [https://www.kaggle.com/jtrotman/meta-kaggle-competition-shake-up](https://www.kaggle.com/jtrotman/meta-kaggle-competition-shake-up)).
    For instance, by consulting these figures, you may find out how dreadful the *RSNA
    Intracranial Hemorrhage Detection* competition was for many because of its shake-ups,
    especially in the top positions.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这些**事后**数字最初是由**Steve Donoho**（[https://www.kaggle.com/breakfastpirate](https://www.kaggle.com/breakfastpirate)）提出的，他编制了一个最糟糕的Kaggle动荡排名（见[https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831](https://www.kaggle.com/c/recruit-restaurant-visitor-forecasting/discussion/49106#278831)）。如今，这些数字很容易通过基于我们在第5章“比赛任务和度量”中讨论的Meta
    Kaggle数据集的许多Notebooks重新创建。例如，通过查阅这些数字，你可能会发现由于动荡，特别是顶尖位置上的动荡，*RSNA颅内出血检测*比赛对许多人来说是多么可怕。
- en: 'However, aside from an *ex post* evaluation, there are quite a few lessons
    that we can get from previous shake-ups that can help you in your Kaggle competitions.
    A few researchers from UC Berkeley think so too. In their paper presented at NIPS
    2019, Roelofs, Fridovich-Keil et al. study in detail a few thousand Kaggle competitions
    to gain insight into the public-private leaderboard dynamics in Kaggle competitions.
    Although they focus on a limited subset of competitions (120, above a certain
    number of participants, focused on binary classification), they obtained some
    interesting findings:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，除了**事后**评估之外，我们还可以从之前的动荡中获得很多教训，这些教训可以帮助你在Kaggle比赛中取得成功。加州大学伯克利分校的一些研究人员也这样认为。在2019年NIPS会议上，Roelofs、Fridovich-Keil等人详细研究了数千个Kaggle比赛，以深入了解Kaggle比赛中的公共-私有排行榜动态。尽管他们关注的是有限的比赛子集（120个以上参与者，专注于二元分类），但他们获得了一些有趣的发现：
- en: There is little adaptive overfitting; in other words, public standings usually
    do hold in the unveiled private leaderboard.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 很少有自适应过拟合；换句话说，公共排名通常在未公开的私有排行榜中是成立的。
- en: Most shake-ups are due to random fluctuations and overcrowded rankings where
    competitors are too near to each other, and any slight change in the performance
    in the private test sets causes major changes in the rankings.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数动荡都是由于随机波动和过度拥挤的排名造成的，竞争对手彼此过于接近，任何在私有测试集中的性能的微小变化都会导致排名的重大变化。
- en: Shake-ups happen when the training set is very small or the training data is
    not **independent and identically distributed** (**i.i.d.**).
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练集非常小或训练数据不是**独立同分布**（**i.i.d.**）时，就会发生动荡。
- en: 'The full paper, Roelofs, R., Fridovich-Keil, S. et al. *A meta-analysis of
    overfitting in machine learning*. Proceedings of the 33^(rd) International Conference
    on Neural Information Processing Systems. 2019, can be found at this link: [https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf](https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: Roelofs, R.，Fridovich-Keil, S.等人撰写的关于机器学习过拟合的元分析全文，可在以下链接找到：[https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf](https://papers.nips.cc/paper/2019/file/ee39e503b6bedf0c98c388b7e8589aca-Paper.pdf)。
- en: 'In our long experience of Kaggle competitions, however, we have seen quite
    a lot of problems with adaptive overfitting since the beginning. For instance,
    you can read *Greg Park*’s analysis of one of the first competitions we ever took
    part in: [http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/](http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/).
    Since this is quite a common and persistent problem for many Kagglers, we suggest
    a strategy that is a bit more sophisticated than simply following what happens
    on the public leaderboard:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的Kaggle比赛长期经验中，我们从一开始就看到了许多自适应过拟合的问题。例如，你可以阅读*Greg Park*对我们最初参加的其中一场比赛的分析：[http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/](http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/)。由于这是许多Kagglers的相当常见且持久的问题，我们建议一种比简单地跟随公共排行榜发生的事情更为复杂的策略：
- en: Always build reliable cross-validation systems for local scoring.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是构建可靠的交叉验证系统进行本地评分。
- en: Always try to control non-i.i.d distributions using the best validation scheme
    dictated by the situation. Unless clearly stated in the description of the competition,
    it is not an easy task to spot non-i.i.d. distributions, but you can get hints
    from discussion or by experimenting using stratified validation schemes (when
    stratifying according to a certain feature, the results improve decisively, for
    instance).
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总是尝试使用最佳验证方案控制非-i.i.d.分布，该方案由具体情况决定。除非在比赛描述中明确说明，否则识别非-i.i.d.分布并不容易，但你可以通过讨论或通过使用分层验证方案（例如，根据某个特征进行分层时，结果会显著改善）来获得提示。
- en: Correlate local scoring with the public leaderboard in order to figure out whether
    or not they go in the same direction.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将本地评分与公共排行榜相关联，以确定它们是否朝同一方向移动。
- en: Test using adversarial validation, revealing whether or not the test distribution
    is similar to the training data.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用对抗验证进行测试，以揭示测试分布是否与训练数据相似。
- en: Make your solutions more robust using ensembling, especially if you are working
    with small datasets.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用集成使你的解决方案更加鲁棒，特别是如果你正在处理小型数据集。
- en: In the following sections, we are going to explore each of these ideas (except
    for ensembling, which is the topic of a future chapter) and provide you with all
    the best tools and strategies to obtain the best results, especially on the private
    dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨这些想法（除了集成，这是未来章节的主题）并为你提供所有最佳的工具和策略，以获得最佳结果，尤其是在私有数据集上。
- en: The importance of validation in competitions
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在比赛中验证的重要性
- en: If you think about a competition carefully, you can imagine it as a huge system
    of experiments. Whoever can create the most systematic and efficient way to run
    these experiments wins.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细思考一场比赛，你可以将其想象为一个巨大的实验系统。能够创造出最系统、最有效运行这些实验方式的人将获胜。
- en: In fact, in spite of all your theoretical knowledge, you will be in competition
    with the hundreds or thousands of data professionals who have more or less the
    same competencies as you.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，尽管你拥有所有的理论知识，你仍将与其他数百或数千名数据专业人士竞争，他们的能力与你或多或少相当。
- en: In addition, they will be using exactly the same data as you and roughly the
    same tools for learning from the data (TensorFlow, PyTorch, Scikit-learn, and
    so on). Some will surely have better access to computational resources, although
    the availability of Kaggle Notebooks and generally decreasing cloud computing
    prices mean the gap is no longer so wide. Consequently, if you look at differences
    in knowledge, data, models, and available computers, you won’t find many discriminating
    factors between you and the other competitors that could explain huge performance
    differences in a competition. Yet, some participants consistently outperform others,
    implying there is some underlying success factor.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，他们将与您使用完全相同的数据，以及大致相同的工具来从数据中学习（TensorFlow、PyTorch、Scikit-learn等）。有些人肯定能更好地获取计算资源，尽管Kaggle
    Notebooks的可用性和云计算价格的普遍下降意味着差距不再那么大。因此，如果您观察知识、数据、模型和可用计算机之间的差异，您不会在您和其他竞争对手之间找到许多区分因素，这些因素可以解释比赛中巨大的性能差异。然而，一些参与者持续优于其他人，这表明存在一些潜在的成功的因素。
- en: In interviews and meet-ups, some Kagglers describe this success factor as “grit,”
    some others as “trying everything,” some others again as a “willingness to put
    everything you have into a competition.” These may sound a bit obscure and magic.
    Instead, we call it **systematic experimentation**. In our opinion, the key to
    successful participation resides in the number of experiments you conduct and
    the way you run all of them. The more experiments you undertake, the more chances
    you will have to crack the problem better than other participants. This number
    certainly depends on a few factors, such as the time you have available, your
    computing resources (the faster the better, but as we previously mentioned, this
    is not such a strong differentiator *per se*), your team size, and their involvement
    in the task. This aligns with the commonly reported grit and engagement as keys
    for success.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在访谈和聚会中，一些Kagglers将这个成功因素描述为“毅力”，其他人则将其描述为“尝试一切”，还有一些人将其描述为“愿意把所有东西都投入到比赛中”。这些可能听起来有点模糊和神奇。相反，我们称之为**系统实验**。在我们看来，成功参与的关键在于您进行的实验数量以及您运行所有实验的方式。您进行的实验越多，您就越有可能比其他参与者更好地解决该问题。这个数量当然取决于几个因素，例如您可用的空闲时间、您的计算资源（越快越好，但正如我们之前提到的，这本身并不是一个如此强烈的区分因素），您团队的大小以及他们在任务中的参与度。这与通常报道的毅力和参与度作为成功的关键是一致的。
- en: However, these are not the only factors affecting the result. You have to take
    into account that the way you run your experiments also has an impact. *Fail fast
    and learn from it* is an important factor in a competition. Of course, you need
    to reflect carefully both when you fail and when you succeed in order to learn
    something from your experiences, or your competition will just turn into a random
    sequence of attempts in the hope of picking the right solution.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些并不是影响结果唯一因素。您必须考虑到您进行实验的方式也会产生影响。“快速失败并从中学习”是竞争中一个重要的因素。当然，您在失败和成功时都需要仔细反思，以便从经验中学习，否则您的比赛可能只是变成了一系列随机的尝试，希望找到正确的解决方案。
- en: Therefore, *ceteris paribus*, having a proper **validation strategy** is the
    great discriminator between successful Kaggle competitors and those who just overfit
    the leaderboard and end up in lower-than-expected rankings after a competition.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在所有其他条件相同的情况下，拥有一个合适的**验证策略**是成功Kaggle竞争者和那些只是过度拟合排行榜、比赛后排名低于预期的人之间的重要区分因素。
- en: '**Validation** is the method you use to correctly evaluate the errors that
    your model produces and to measure how its performance improves or decreases based
    on your experiments.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**验证**是您用来正确评估模型产生的错误以及根据您的实验来衡量其性能如何提高或降低的方法。'
- en: Generally, the impact of choosing proper validation is too often overlooked
    in favor of more quantitative factors, such as having the latest, most powerful
    GPU or a larger team producing submissions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，选择合适的验证方法的影响往往被忽视，而更倾向于考虑更多量化因素，例如拥有最新、最强大的GPU或一个能提交更多作品的更大团队。
- en: Nevertheless, if you count only on the firepower of experiments and their results
    on the leaderboard, it will be like “throwing mud at the wall and hoping something
    will stick” (see [http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/](http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/)).
    Sometimes such a strategy will work, but most often it won’t, because you will
    miss important opportunities to experiment in the right direction, and you won’t
    even be able to see the shining gem you managed to produce in the middle of all
    that mud. For instance, if you concentrate too much on trying your luck on the
    public leaderboard using a random, unsystematic strategy, even if you produce
    great solutions, you may end up not choosing your final submission correctly and
    missing the best scoring one on the private leaderboard.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你只依赖实验的力量以及它们在排行榜上的结果，那就好比“向墙上扔泥巴，希望有什么东西能粘上去”（参见 [http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/](http://gregpark.io/blog/Kaggle-Psychopathy-Postmortem/)）。有时这样的策略会奏效，但大多数时候不会，因为你将错过在正确方向上进行实验的重要机会，甚至无法看到你在所有泥巴中找到的闪亮的宝石。例如，如果你过于专注于使用随机、非系统的策略在公共排行榜上碰运气，即使你产生了伟大的解决方案，你也可能最终无法正确选择你的最终提交，并错过私有排行榜上得分最高的一个。
- en: Having a proper validation strategy can help you decide which of your models
    should be submitted for ranking on the private test set. Though the temptation
    to submit your top public leaderboard models may be high, *always consider your
    own validation scores*. For your final submissions, depending on the situation
    and whether or not you trust the leaderboard, choose your best model based on
    the leaderboard and your best based on your local validation results. If you don’t
    trust the leaderboard (especially when the training sample is small or the examples
    are non-i.i.d.), submit models that have two of the best validation scores, picking
    two very different models or ensembles. In this way, you will reduce the risk
    of choosing solutions that won’t perform on the private test set.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个合适的验证策略可以帮助你决定哪些模型应该提交到私有测试集进行排名。尽管提交你最高公共排行榜模型的诱惑可能很大，但*始终考虑你自己的验证分数*。对于你的最终提交，根据具体情况以及你是否信任排行榜，基于排行榜选择你的最佳模型，基于你的本地验证结果选择你的最佳模型。如果你不信任排行榜（尤其是当训练样本很小或示例是非独立同分布时），提交具有两个最佳验证分数的模型，选择两个非常不同的模型或集成。这样，你将降低选择在私有测试集上无法表现良好的解决方案的风险。
- en: 'Having pointed out the importance of having a method of experimenting, what
    is left is all a matter of the practicalities of validation. In fact, when you
    model a solution, you take a series of interrelated decisions:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 指出了拥有实验方法的重要性之后，剩下的所有问题都变成了验证的实用性问题。实际上，当你对一个解决方案进行建模时，你需要做出一系列相互关联的决定：
- en: How to process your data
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何处理你的数据
- en: What model to apply
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应该应用哪种模型
- en: How to change the model’s architecture (especially true for deep learning models)
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何更改模型的架构（尤其是对于深度学习模型）
- en: How to set the model’s hyperparameters
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何设置模型的超参数
- en: How to post-process the predictions
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如何后处理预测
- en: Even if the public leaderboard is perfectly correlated with the private one,
    the limited number of daily submissions (a limitation present in all competitions)
    prevents you from even scratching the surface of possible tests that you could
    do in all the aforementioned areas. Having a proper validation system tells you
    beforehand if what you are doing could work on the leaderboard.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 即使公共排行榜与私有排行榜完全相关，每天提交数量的限制（所有比赛中都存在的限制）也阻止你触及上述所有可能测试的表面。拥有一个合适的验证系统会提前告诉你你所做的是否能在排行榜上工作。
- en: '![](img/Dmitry_Larko.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![Dmitry Larko](img/Dmitry_Larko.png)'
- en: Dmitry Larko
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Dmitry Larko
- en: '[https://www.kaggle.com/dmitrylarko](https://www.kaggle.com/dmitrylarko)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/dmitrylarko](https://www.kaggle.com/dmitrylarko)'
- en: '**Dmitry Larko** is a Kaggle Competition Grandmaster and the chief data scientist
    at H2O.ai. He has over a decade of experience in ML and data science. He discovered
    Kaggle in December 2012 and participated in his first competition a few months
    later. He is a strong advocate of validation in Kaggle competitions, as he told
    us in his interview.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**Dmitry Larko** 是 Kaggle 竞赛的大师级选手，同时也是 H2O.ai 的首席数据科学家。他在机器学习和数据科学领域拥有超过十年的经验。他在2012年12月发现了
    Kaggle，并在几个月后参加了他的第一个比赛。他是 Kaggle 竞赛中验证的强烈倡导者，正如他在采访中告诉我们的。'
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的比赛类型是什么？为什么？在Kaggle上，你在技术和解决方法方面有什么专长？
- en: '*I have mostly participated in competitions for tabular datasets but also enjoy
    competitions for computer vision.*'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*我主要参加了表格数据集的比赛，但也喜欢参加计算机视觉的比赛。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle比赛的？这种方法与你在日常工作中所做的方法有何不同？
- en: '*I always try to start simple and build a submission pipeline for smaller/simpler
    models first. A major step here is to create a proper validation scheme so you
    can validate your ideas in a robust way. Also, it is always a good idea to spend
    as much time as you can looking at the data and analyzing it.*'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*我总是尝试从简单开始，为较小的/较简单的模型首先构建一个提交流程。这里的一个重要步骤是创建一个适当的验证方案，这样你可以以稳健的方式验证你的想法。此外，花尽可能多的时间查看和分析数据始终是一个好主意。*'
- en: '*In my day-to-day work, I am building an AutoML platform, so a lot of things
    I try on Kaggle end up being implemented as a part of this platform.*'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我的日常工作中，我正在构建一个AutoML平台，所以我在Kaggle上尝试的许多事情最终都成为了这个平台的一部分。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们你参加的一个特别具有挑战性的比赛，以及你使用了哪些见解来应对这项任务。
- en: '*Nothing comes to my mind, and it doesn’t matter, because what is technically
    challenging for me could be a piece of cake for somebody else. Technical challenges
    are not that important; what’s important is to remember that a competition is
    somewhat like a marathon, not a sprint. Or you can see it as a marathon of sprints
    if you like. So, it is important not to get exhausted, sleep well, exercise, and
    take a walk in a park to regenerate your brain for new ideas. To win a Kaggle
    competition, you will need all your creativity and expertise and sometimes even
    a bit of luck.*'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '*我脑海中没有想出什么，这无关紧要，因为对我来说技术上具有挑战性的事情可能对其他人来说是小菜一碟。技术挑战并不那么重要；重要的是要记住，比赛在一定程度上类似于马拉松，而不是短跑。或者如果你喜欢，你可以将其视为一系列的短跑。因此，重要的是不要过度疲劳，保证充足的睡眠，进行锻炼，并在公园散步以恢复大脑，激发新想法。要赢得Kaggle比赛，你需要所有的创造力和专业知识，有时甚至需要一点运气。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进步？如果是的话，是如何帮助的？
- en: '*I got my current job thanks to the fact I was a Kaggle Competition Grandmaster.
    For my current employer, this fact was evidence enough of my expertise in the
    field.*'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*我之所以得到现在的工作，是因为我是Kaggle竞赛的大师。对我现在的雇主来说，这一点已经足以证明我在该领域的专业知识。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，没有经验的Kagglers通常忽略了什么？你现在知道什么，而当你刚开始时希望知道的呢？
- en: '*Mostly they overlook the right validation scheme and follow the feedback from
    the public leaderboard. That ends badly in most cases, leading to something known
    as a “shake-up” on Kaggle.*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*他们通常忽略了正确的验证方案，并跟随公共排行榜的反馈。在大多数情况下，这会导致所谓的Kaggle上的“震动”。*'
- en: '*Also, they rush to skip exploratory data analysis and build models right away,
    which leads to simplistic solutions and mediocre leaderboard scores.*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*此外，他们急于跳过探索性数据分析，直接构建模型，这导致了解决方案简单化以及排行榜分数平庸。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*My main mistake is really the same that an inexperienced person will make
    – following the leaderboard score and not my internal validation. Every time I
    decided to do so, it cost me several places on the leaderboard.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*我犯的主要错误实际上与一个没有经验的人会犯的错误是一样的——跟随排行榜分数而不是我的内部验证。每次我决定这样做，都会让我在排行榜上失去几个位置。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你有没有推荐用于数据分析或机器学习的特定工具或库？
- en: '*That would be the usual suspects. For tabular data: LightGBM, XGBoost, CatBoost;
    for deep learning: PyTorch, PyTorch-Lightning, timm; and Scikit-learn for everyone.*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*那通常是常见的几种方法。对于表格数据：LightGBM、XGBoost、CatBoost；对于深度学习：PyTorch、PyTorch-Lightning、timm；以及Scikit-learn适用于所有人。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加比赛时，他们应该记住或做哪件事是最重要的？
- en: '*Start simple, always validate; believe in your validation score and not the
    leaderboard score.*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*从简单开始，始终进行验证；相信你的验证分数，而不是排行榜分数。*'
- en: Bias and variance
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 偏差和方差
- en: A good validation system helps you with metrics that are more reliable than
    the error measures you get from your training set. In fact, metrics obtained on
    the training set are affected by the capacity and complexity of each model. You
    can think of the **capacity** of a model as its memory that it can use to learn
    from data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的验证系统会帮助你获得比从训练集中得到的错误度量更可靠的指标。实际上，从训练集中获得的指标会受到每个模型容量和复杂性的影响。你可以将模型的**容量**视为其可以用来从数据中学习的内存。
- en: Each model has a set of internal parameters that help the model to record the
    patterns taken from the data. Every model has its own skills for acquiring patterns,
    and some models will spot certain rules or associations whereas others will spot
    others. As a model extracts patterns from data, it records them in its “memory.”
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 每个模型都有一组内部参数，帮助模型记录从数据中提取的模式。每个模型都有其获取模式的能力，一些模型会注意到某些规则或关联，而其他模型会注意到其他规则。当模型从数据中提取模式时，它会将其记录在“记忆”中。
- en: 'You also hear about the capacity or expressiveness of a model as a matter of
    **bias and variance**. In this case, the bias and variance of a model refer to
    the predictions, but the underlying principle is strictly related to the expressiveness
    of a model. Models can be reduced to mathematical functions that map an input
    (the observed data) to a result (the predictions). Some mathematical functions
    are more complex than others, in the number of internal parameters they have and
    in the ways they use them:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你也会听到关于模型的容量或表达性，作为**偏差和方差**的问题。在这种情况下，模型的偏差和方差指的是预测，但基本原理严格与模型的表达性相关。模型可以被简化为将输入（观察到的数据）映射到结果（预测）的数学函数。一些数学函数比其他函数更复杂，这体现在它们内部参数的数量以及它们使用参数的方式：
- en: If the mathematical function of a model is not complex or expressive enough
    to capture the complexity of the problem you are trying to solve, we talk of **bias**,
    because your predictions will be limited (“biased”) by the limits of the model
    itself.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型的数学函数不够复杂或表达性不足以捕捉你试图解决的问题的复杂性，我们谈论的是**偏差**，因为你的预测将受到模型自身限制（“偏差”）的影响。
- en: If the mathematical function at the core of a model is too complex for the problem
    at hand, we have a **variance** problem, because the model will record more details
    and noise in the training data than needed and its predictions will be deeply
    influenced by them and become erratic.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型核心的数学函数对于当前问题来说过于复杂，我们就遇到了**方差**问题，因为模型会记录比所需更多的训练数据细节和噪声，其预测将深受其影响并变得不可预测。
- en: Nowadays, given the advances in machine learning and the available computation
    resources, the problem is always due to variance, since deep neural networks and
    gradient boosting, the most commonly used solutions, often have a mathematical
    expressiveness that exceeds what most of the problems you will face need in order
    to be solved.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到机器学习的进步和可用的计算资源，问题总是由于方差，因为深度神经网络和梯度提升，最常用的解决方案，通常具有超过大多数你将面临的问题解决所需数学表达性。
- en: When all the useful patterns that a certain model can extract have been captured,
    if the model has not exhausted its capacity, it will then start memorizing data
    characteristics and signals that are unrelated to the prediction (usually referred
    to as **noise**). While the initially extracted patterns will help the model to
    generalize to a test dataset and predict more correctly, not everything that it
    learns specifically about the training set will help; instead, it may damage its
    performance. The process of learning elements of the training set that have no
    generalization value is commonly called **overfitting**.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当某个模型能够提取的所有有用模式都被捕捉到后，如果模型尚未耗尽其容量，它将开始记忆与预测无关的数据特征和信号（通常称为**噪声**）。虽然最初提取的模式将帮助模型泛化到测试数据集并更准确地预测，但并非它从训练集中学习到的所有特定内容都有帮助；相反，它可能会损害其性能。学习没有泛化价值的训练集元素的过程通常被称为**过拟合**。
- en: The core purpose of validation is to explicitly define a score or loss value
    that separates the generalizable part of that value from that due to overfitting
    the training set characteristics.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 验证的核心目的是明确定义一个分数或损失值，将这个值中可泛化的部分与由于过度拟合训练集特征而产生的部分分开。
- en: 'This is the **validation loss**. You can see the situation visualized in the
    following figure of learning curves:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是**验证损失**。你可以在以下学习曲线图中看到这种情况的可视化：
- en: '![](img/B17574_06_01.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_06_01.png)'
- en: 'Figure 6.1: Learning more from the training data does not always mean learning
    to predict'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1：从训练数据中学习更多并不总是意味着学习预测
- en: If you graph the loss measure on the *y*-axis against some measure of learning
    effort of the model (this could be epochs for neural networks, or rounds for gradient
    boosting) on the *x*-axis, you will notice that learning always seems to happen
    on the training dataset, but this is not always true on other data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将损失度量在y轴上与模型学习努力的某个度量（这可能是神经网络的epoch，或梯度提升的round）在x轴上作图，你会注意到学习似乎总是在训练数据集上发生，但这种情况并不总是适用于其他数据。
- en: The same thing happens even if you change the hyperparameters, process the data,
    or decide on a different model altogether. The curves will change shape, but you’ll
    always have a sweet point where overfitting starts. That point can be different
    across models and between the various choices that you make in your modeling efforts.
    If you have properly computed the point when overfitting starts thanks to a correct
    validation strategy, your model’s performance will surely correlate with the leaderboard
    results (both public and private), and your validation metrics will provide you
    with a proxy to evaluate your work without making any submissions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你更改超参数、处理数据或完全决定使用不同的模型，这种情况也会发生。曲线的形状会改变，但你总会找到一个甜蜜点，即过拟合开始的地方。这个点在不同的模型和你在建模过程中所做的各种选择之间可能不同。如果你通过正确的验证策略正确地计算了过拟合开始的时间点，你的模型性能将肯定与排行榜结果（公开和私人）相关联，并且你的验证指标将为你提供一个评估你工作的代理，而无需提交任何内容。
- en: 'You can hear about overfitting at various levels:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在各个层面上听到过拟合：
- en: At the level of the training data, when you use a model that is too complex
    for the problem
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练数据层面，当你使用一个对于问题过于复杂的模型时
- en: At the level of the validation set itself, when you tune your model too much
    with respect to a specific validation set
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在验证集本身层面，当你过度调整你的模型以适应特定的验证集时
- en: At the level of the public leaderboard, when your results are far from what
    you would expect from your training
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在公开排行榜层面，当你的结果远远低于你从训练中预期的结果时
- en: At the level of the private leaderboard, when in spite of the good results on
    the public leaderboard, your private scores will be disappointing
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在私人排行榜层面，尽管在公开排行榜上取得了好成绩，但你的私人分数可能会令人失望。
- en: Though slightly different in meaning, they all equally imply that your model
    is not generalizable, as we have described in this section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在意义上略有不同，但它们都同样意味着你的模型不可泛化，正如我们在本节中描述的那样。
- en: Trying different splitting strategies
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尝试不同的分割策略
- en: As previously discussed, the validation loss is based on a data sample that
    is not part of the training set. It is an empirical measure that tells you how
    good your model is at predicting, and a more correct one than the score you get
    from your training, which will tell you mostly how much your model has memorized
    the training data patterns. Correctly choosing the data sample you use for validation
    constitutes your validation strategy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，验证损失是基于一个不属于训练集的数据样本。这是一个经验度量，告诉你你的模型在预测方面有多好，比从训练中得到的分数更准确，后者主要告诉你你的模型有多少是记忆了训练数据模式。正确选择用于验证的数据样本构成了你的验证策略。
- en: 'To summarize the strategies for validating your model and measuring its performance
    correctly, you have a couple of choices:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结验证模型和正确衡量其性能的策略，你有几个选择：
- en: The first choice is to **work with a holdout system**, incurring the risk of
    not properly choosing a representative sample of the data or overfitting to your
    validation holdout.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一种选择是**使用保留系统**，这可能会带来风险，即未能正确选择数据的代表性样本或过度拟合到你的验证保留数据。
- en: The second option is to **use a probabilistic approach** and rely on a series
    of samples to draw your conclusions on your models. Among the probabilistic approaches,
    you have cross-validation, **leave-one-out** (**LOO**), and bootstrap. Among the
    cross-validation strategies, there are different nuances depending on the sampling
    strategies you take based on the characteristic of your data (simple random sampling,
    stratified sampling, sampling by groups, time sampling).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二种选择是**使用概率方法**并依赖于一系列样本来得出你对模型的结论。在概率方法中，有交叉验证、**留一法**（LOO）和自助法。在交叉验证策略中，根据你基于数据特征（简单随机抽样、分层抽样、分组抽样、时间抽样）采取的抽样策略，有不同的细微差别。
- en: What all these strategies have in common is that they are **sampling strategies**.
    It means that they help you to infer a general measure (the performance of your
    model) based on a small part of your data, randomly selected. Sampling is at the
    root of statistics and it is not an exact procedure because, based on your sampling
    method, your available data, and the randomness of picking up certain cases as
    part of your sample, you will experience a certain degree of error.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些策略的共同之处在于它们都是**采样策略**。这意味着它们帮助你根据你数据的一小部分（随机选择的）来推断一个总体指标（你模型的性能）。采样是统计学的根本，它不是一个精确的过程，因为基于你的采样方法、可用数据和选择某些案例作为样本部分的随机性，你将经历一定程度的误差。
- en: For instance, if you rely on a biased sample, your evaluation metric may be
    estimated incorrectly (over- or under-estimated). However, if properly designed
    and implemented, sampling strategies generally provide you with a good estimate
    of your general measure.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你依赖于有偏见的样本，你的评估指标可能会被错误地估计（高估或低估）。然而，如果设计得当并实施得当，采样策略通常能为你提供对你总体指标的较好估计。
- en: The other aspect that all these strategies have in common is that they are **partitions**,
    which divide cases in an exclusive way as either part of the training or part
    of the validation. In fact, as we discussed, since most models have a certain
    memorization capability, using the same cases in both training and validation
    leads to inflated estimates because it allows the model to demonstrate its memorization
    abilities; instead, we want it to be evaluated on its ability to derive patterns
    and functions that work on *unseen* examples.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些策略的另一个共同之处在于它们都是**分割**，它们以排他的方式将案例分为训练部分或验证部分。实际上，正如我们讨论的，由于大多数模型都有一定的记忆能力，在训练和验证中使用相同的案例会导致估计过高，因为这允许模型展示其记忆能力；相反，我们希望它能够被评估其从*未见*示例中推导出模式和函数的能力。
- en: The basic train-test split
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基本的训练-测试分割
- en: The first strategy that we will analyze is the **train-test split**. In this
    strategy, you sample a portion of your training set (also known as the **holdout**)
    and you use it as a test set for all the models that you train using the remaining
    part of the data.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要分析的第一种策略是**训练-测试分割**。在这种策略中，你从你的训练集中采样一部分（也称为**保留集**），并使用它作为所有使用剩余数据部分训练的模型的测试集。
- en: 'The great advantage of this strategy is that it is very simple: you pick up
    a part of your data and you check your work on that part. You usually split the
    data 80/20 in favor of the training partition. In Scikit-learn, it is implemented
    in the `train_test_split` function. We’ll draw your attention to a couple of aspects
    of the method:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的巨大优势在于它非常简单：你选择你数据的一部分，并在那部分上检查你的工作。你通常将数据分成80/20，以利于训练部分。在Scikit-learn中，这是在`train_test_split`函数中实现的。我们将关注该方法的一些方面：
- en: When you have large amounts of data, you can expect that the test data you extract
    is similar to (representative of) the original distribution on the entire dataset.
    However, since the extraction process is based on randomness, you always have
    the chance of extracting a non-representative sample. In particular, the chance
    increases if the training sample you start from is small. Comparing the extracted
    holdout partition using **adversarial validation** (more about this in a few sections)
    can help you to make sure you are evaluating your efforts in a correct way.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你拥有大量数据时，你可以预期你提取的测试数据与整个数据集上的原始分布相似（具有代表性）。然而，由于提取过程基于随机性，你总是有可能提取一个不具有代表性的样本。特别是，如果起始的训练样本很小，这种可能性会增加。使用**对抗验证**（在接下来的几节中会详细介绍）比较提取的保留部分可以帮助你确保你以正确的方式评估你的努力。
- en: In addition, to ensure that your test sampling is representative, especially
    with regard to how the training data relates to the target variable, you can use
    **stratification**, which ensures that the proportions of certain features are
    respected in the sampled data. You can use the `stratify` parameter in the `train_test_split`
    function and provide an array containing the class distribution to preserve.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，为确保你的测试样本具有代表性，特别是关于训练数据与目标变量之间的关系，你可以使用**分层**，这确保了在样本数据中某些特征的占比得到尊重。你可以在`train_test_split`函数中使用`stratify`参数，并提供一个包含类别分布的数组以保留。
- en: We have to remark that, even if you have a representative holdout available,
    sometimes a simple train-test split is not enough for ensuring a correct tracking
    of your efforts in a competition.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须指出，即使你有可用的代表性保留集，有时简单的训练-测试分割不足以确保在比赛中正确跟踪你的努力。
- en: In fact, as you keep checking on this test set, you may drive your choices to
    some kind of adaptation overfitting (in other words, erroneously picking up the
    noise of the training set as signals), as happens when you frequently evaluate
    on the public leaderboard. For this reason, a probabilistic evaluation, though
    more computationally expensive, is more suited for a competition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，随着你继续检查这个测试集，你可能会将你的选择推向某种适应过拟合（换句话说，错误地将训练集的噪声作为信号），就像你在公共排行榜上频繁评估时发生的那样。因此，尽管概率评估在计算上更昂贵，但它更适合比赛。
- en: Probabilistic evaluation methods
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率评估方法
- en: Probabilistic evaluation of the performance of a machine learning model is based
    on the statistical properties of a sample from a distribution. By sampling, you
    create a smaller set of your original data that is expected to have the same characteristics.
    In addition, what is left untouched from the sampling constitutes a sample in
    itself, and it is also expected to have the same characteristics as the original
    data. By training and testing your model on this sampled data and repeating this
    procedure a large number of times, you are basically creating a statistical estimator
    measuring the performance of your model. Every sample may have some “error” in
    it; that is, it may not be fully representative of the true distribution of the
    original data. However, as you sample more, the mean of your estimators on these
    multiple samples will converge to the true mean of the measure you are estimating
    (this is an observed outcome that, in probability, is explained by a theorem called
    the *Law of Large Numbers*).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 概率评估机器学习模型的性能基于分布样本的统计特性。通过采样，你创建了一个较小的原始数据集，它预期具有相同的特征。此外，采样中未被触及的部分本身也构成一个样本，并且预期具有与原始数据相同的特征。通过在采样的数据上训练和测试你的模型，并重复这一过程多次，你基本上创建了一个统计估计器，用于衡量你模型的性能。每个样本可能都存在一些“误差”，也就是说，它可能并不完全代表原始数据的真实分布。然而，随着你采样的增加，这些多个样本上的估计器的平均值将收敛到你正在估计的度量量的真实平均值（这是一个观察到的结果，在概率上由称为**大数定律**的定理解释）。
- en: 'Probabilistic estimators naturally require more computations than a simple
    train-test split, but they offer more confidence that you are correctly estimating
    the right measure: the general performance of your model.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 概率估计器自然需要比简单的训练-测试分割更多的计算，但它们提供了更多的信心，确保你正确地估计了正确的度量：你模型的总体性能。
- en: k-fold cross-validation
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k折交叉验证
- en: The most used probabilistic validation method is **k****-fold cross-validation**,
    which is recognized as having the ability to correctly estimate the performance
    of your model on unseen test data drawn from the same distribution.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的概率验证方法是**k**-折交叉验证，这种方法被认为能够正确估计你的模型在未见过的、来自同一分布的测试数据上的性能。
- en: 'This is clearly explained in the paper Bates, S., Hastie, T., and Tibshirani,
    R.; *Cross-validation: what does it estimate and how well does it do it?* arXiv
    preprint arX­iv:2104.00673, 2021 ([https://arxiv.org/pdf/2104.00673.pdf](https://arxiv.org/pdf/2104.00673.pdf)).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这在Bates, S., Hastie, T., 和 Tibshirani, R. 的论文中有明确的解释；*交叉验证：它估计了什么以及它做得怎么样？*
    arXiv预印本 arXiv:2104.00673，2021 ([https://arxiv.org/pdf/2104.00673.pdf](https://arxiv.org/pdf/2104.00673.pdf))。
- en: '*k*-fold cross-validation can be successfully used to compare predictive models,
    as well as when selecting the hyperparameters for your model that will perform
    the best on the test set.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**k**-折交叉验证可以成功地用于比较预测模型，以及在选择模型的最佳超参数时。'
- en: There are quite a few different variations of *k*-fold cross-validation, but
    the simplest one, which is implemented in the `KFold` function in Scikit-learn,
    is based on the splitting of your available training data into *k* partitions.
    After that, for *k* iterations, one of the *k* partitions is taken as a test set
    while the others are used for the training of the model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**k**-折交叉验证有相当多的不同变体，但最简单的一种，即在Scikit-learn中的`KFold`函数中实现的，是基于将你的可用训练数据分成**k**个分区。之后，在**k**次迭代中，其中一个**k**个分区被用作测试集，而其他分区用于模型的训练。'
- en: 'The *k* validation scores are then averaged and that averaged score value is
    the *k*-fold validation score, which will tell you the estimated average model
    performance on any unseen data. The standard deviation of the scores will inform
    you about the uncertainty of the estimate. *Figure 6.2* demonstrates how 5-fold
    cross-validation is structured:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 验证分数随后被平均，这个平均分数值就是 *k*-折验证分数，它将告诉你模型在任意未见数据上的估计平均性能。分数的标准差将告诉你估计的不确定性。*图6.2*
    展示了5折交叉验证的结构：'
- en: '![](img/B17574_06_02.png)Figure 6.2: How a 5-fold validation scheme is structured'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.2](img/B17574_06_02.png)图6.2：5折验证方案的结构'
- en: One important aspect of the *k*-fold cross-validation score you have to keep
    in mind is that it estimates the average score of a model trained on the same
    quantity of data as *k - 1* folds. If, afterward, you train your model on all
    your data, the previous validation estimate no longer holds. As *k* approaches
    the number *n* of examples, you have an increasingly correct estimate of the model
    derived on the full training set, yet, due to the growing correlation between
    the estimates you obtain from each fold, you will lose all the probabilistic estimates
    of the validation. In this case, you’ll end up having a number showing you the
    performance of your model on your training data (which is still a useful estimate
    for comparison reasons, but it won’t help you in correctly estimating the generalization
    power of your model).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *k*-折交叉验证分数中，你必须记住的一个重要方面是，它估计的是在 *k - 1* 折相同数据量上训练的模型的平均分数。如果你之后在所有数据上训练你的模型，之前的验证估计就不再适用了。当
    *k* 接近例子数量 *n* 时，你对在完整训练集上得到的模型的估计将越来越准确，然而，由于从每个折中获得的估计之间的相关性越来越大，你将失去所有验证的概率估计。在这种情况下，你最终会得到一个数字，显示你的模型在训练数据上的性能（这仍然是一个有用的估计，用于比较目的，但它不会帮助你正确估计模型的泛化能力）。
- en: When you reach *k = n*, you have the LOO validation method, which is useful
    when you have a few cases available. The method is mostly an unbiased fitting
    measure since it uses almost all the available data for training and just one
    example for testing. Yet it is not a good estimate of the expected performance
    on unseen data. Its repeated tests over the whole dataset are highly correlated
    with each other and the resulting LOO metric represents more the performance of
    the model on the dataset itself than the performance the model would have on unknown
    data.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当你达到 *k = n* 时，你有了LOO验证方法，这在你有少量案例可用时很有用。这种方法主要是一个无偏拟合度量，因为它几乎使用了所有可用数据来训练，只用一个例子来测试。然而，它并不是对未见数据预期性能的良好估计。它在整个数据集上的重复测试高度相关，并且产生的LOO指标更多地代表了模型在数据集本身的性能，而不是模型在未知数据上的性能。
- en: 'The correct *k* number of partitions to choose is decided based on a few aspects
    relative to the data you have available:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正确的 *k* 分区数量选择是基于一些与你的可用数据相关的方面来决定的：
- en: 'The smaller the *k* (the minimum is 2), the smaller each fold will be, and
    consequently, the more bias in learning there will be for a model trained on *k
    - 1* folds: your model validated on a smaller *k* will be less well-performing
    with respect to a model trained on a larger *k*.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 越小（最小为2），每个折越小，因此，在 *k - 1* 折上训练的模型的学习偏差将越大：你的在较小 *k* 上验证的模型相对于在较大 *k*
    上训练的模型将表现得更差。'
- en: 'The higher the *k*, the more the data, yet the more correlated your validation
    estimates: you will lose the interesting properties of *k*-fold cross-validation
    in estimating the performance on unseen data.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 越高，数据越多，但你的验证估计的相关性也越高：你将失去 *k*-折交叉验证在估计未见数据性能中的有趣特性。'
- en: Commonly, *k* is set to 5, 7, or 10, more seldom to 20 folds. We usually regard
    *k* = 5 or *k* = 10 as a good choice for a competition, with the latter using
    more data for each training (90% of the available data), and hence being more
    suitable for figuring out the performance of your model when you retrain on the
    full dataset.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，*k* 被设置为5、7或10，更少的情况下设置为20折。我们通常认为 *k* = 5 或 *k* = 10 是比赛中的好选择，后者使用更多的数据来训练（90%的可用数据），因此更适合在重新在完整数据集上训练时确定你模型的性能。
- en: When deciding upon what *k* to choose for a specific dataset in a competition,
    we find it useful to reflect on two perspectives.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当决定在比赛中为特定数据集选择什么 *k* 时，我们发现从两个角度进行反思是有用的。
- en: 'Firstly, the choice of the number of folds should reflect your goals:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，折数的数量选择应该反映你的目标：
- en: If your purpose is performance estimation, you need models with low bias estimates
    (which means no systematic distortion of estimates). You can achieve this by using
    a higher number of folds, usually between 10 and 20.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的目的是性能估计，你需要具有低偏差估计的模型（这意味着没有系统性的估计扭曲）。你可以通过使用更多的折数来实现这一点，通常在10到20之间。
- en: If your aim is parameter tuning, you need a mix of bias and variance, so it
    is advisable to use a medium number of folds, usually between 5 and 7.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的目标是参数调整，你需要权衡偏差和方差，因此建议使用中等数量的折数，通常在5到7之间。
- en: Finally, if your purpose is just to apply variable selection and simplify your
    dataset, you need models with low variance estimates (or you will have disagreement).
    Hence, a lower number of folds will suffice, usually between 3 and 5.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，如果你的目的是仅仅应用变量选择和简化你的数据集，你需要具有低方差估计的模型（否则你将会有不一致性）。因此，较低的折数就足够了，通常在3到5之间。
- en: When the size of the available data is quite large, you can safely stay on the
    lower side of the suggested bands.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当可用数据的规模相当大时，你可以安全地保持在建议范围的较低端。
- en: Secondly, if you are just aiming for performance estimation, consider that the
    more folds you use, the fewer cases you will have in your validation set, so the
    more the estimates of each fold will be correlated. Beyond a certain point, increasing
    *k* renders your cross-validation estimates less predictive of unseen test sets
    and more representative of an estimate of how well-performing your model is on
    your training set. This also means that, with more folds, you can get the perfect
    out-of-fold prediction for stacking purposes, as we will explain in detail in
    *Chapter 9*, *Ensembling with Blending and Stacking Solutions*.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，如果你只是想要进行性能估计，请注意，你使用的折数越多，你的验证集中的案例就越少，因此每个折的估计值之间的相关性就越高。超过某个点后，增加*k*会使你的交叉验证估计对未见过的测试集的预测能力降低，而对模型在训练集上的性能估计更加具有代表性。这也意味着，随着折数的增加，你可以得到用于堆叠的完美出折预测，正如我们将在第9章“使用混合和堆叠解决方案的集成”中详细解释的那样。
- en: In Kaggle competitions, *k*-fold cross-validation is often applied not only
    for validating your solution approach and figuring out the performance of your
    model, but to produce your prediction. When you cross-validate, you are subsampling,
    and averaging the results of multiple models built on subsamples of the data is
    an effective strategy for fighting against variance, and often more effective
    than training on all the data available (we will discuss this more in *Chapter
    9*). Hence, many Kaggle competitors use the models built during cross-validation
    to provide a series of predictions on the test set that, averaged, will provide
    them with the solution.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，*k*-折交叉验证不仅用于验证你的解决方案方法和确定模型的性能，还用于生成预测。当你进行交叉验证时，你是在进行子采样，并且平均多个基于数据子样本构建的模型的预测结果是一种有效的对抗方差的策略，通常比在所有可用数据上训练更有效（我们将在第9章中进一步讨论这一点）。因此，许多Kaggle竞争者使用交叉验证期间构建的模型在测试集上提供一系列预测，平均起来将为他们提供解决方案。
- en: k-fold variations
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: k折变化
- en: 'Since it is based on random sampling, *k*-fold can provide unsuitable splits
    when:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它基于随机采样，*k*-折在以下情况下可能会提供不合适的分割：
- en: You have to preserve the proportion of small classes, both at a target level
    and at the level of features. This is typical when your target is highly imbalanced.
    Typical examples are spam datasets (because spam is a small fraction of the normal
    email volume) or any credit risk dataset where you have to predict the not-so-frequent
    event of a defaulted loan.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须保持小类别的比例，无论是在目标级别还是在特征级别。这在你目标高度不平衡时很典型。典型的例子是垃圾邮件数据集（因为垃圾邮件只是正常电子邮件量的一个小部分）或任何信用风险数据集，你必须预测不太频繁的违约贷款事件。
- en: You have to preserve the distribution of a numeric variable, both at a target
    level and at the level of features. This is typical of regression problems where
    the distribution is quite skewed or you have heavy, long tails. A common example
    is house price prediction, where you have a consistent small portion of houses
    on sale that will cost much more than the average house.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你必须保持一个数值变量的分布，无论是在目标级别还是在特征级别。这通常是回归问题，其中分布非常偏斜或尾部很重。一个常见的例子是房价预测，其中有一小部分待售房屋的价格会远高于平均房价。
- en: Your cases are non-i.i.d, in particular when dealing with time series forecasting.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的案例是非独立同分布的，特别是在处理时间序列预测时。
- en: In the first two scenarios, the solution is the **stratified** **k****-fold**,
    where the sampling is done in a controlled way that preserves the distribution
    you want to preserve. If you need to preserve the distribution of a single class,
    you can use `StratifiedKFold` from Scikit-learn, using a stratification variable,
    usually your target variable but also any other feature whose distribution you
    need to preserve. The function will produce a set of indexes that will help you
    to partition your data accordingly. You can also obtain the same result with a
    numeric variable, after having discretized it, using `pandas.cut` or Scikit-learn’s
    `KBinsDiscretizer`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两种情况下，解决方案是**分层****k****-fold**，其中采样是以受控的方式进行，以保留你想要保留的分布。如果你需要保留单个类的分布，你可以使用Scikit-learn的`StratifiedKFold`，使用分层变量，通常是你的目标变量，也可以是任何其他你需要保留其分布的特征。该函数将生成一组索引，帮助你相应地划分你的数据。你还可以使用`pandas.cut`或Scikit-learn的`KBinsDiscretizer`对数值变量进行离散化后获得相同的结果。
- en: It is a bit more complicated when you have to stratify based on multiple variables
    or overlapping labels, such as in multi-label classification.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当你必须根据多个变量或重叠标签进行分层时，比如在多标签分类中，这会变得稍微复杂一些。
- en: 'You can find a solution in the **Scikit-multilearn** package ([http://scikit.ml/](http://scikit.ml/)),
    in particular, the `IterativeStratification` command that helps you to control
    the order (the number of combined proportions of multiple variables) that you
    want to preserve ([http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html](http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html)).
    It implements the algorithm explained by the following papers:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在**Scikit-multilearn**包（[http://scikit.ml/](http://scikit.ml/))中找到一个解决方案，特别是`IterativeStratification`命令，它可以帮助你控制你想要保留的顺序（多个变量的组合比例数）。该命令实现了以下论文中解释的算法：[http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html](http://scikit.ml/api/skmultilearn.model_selection.iterative_stratification.html)。
- en: Sechidis, K., Tsoumakas, G., and Vlahavas, I. (2011). *On the stratification
    of multi-label data*. *Machine Learning and Knowledge Discovery in Databases,
    145-158*. [http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf](http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sechidis, K., Tsoumakas, G., and Vlahavas, I. (2011). *关于多标签数据的分层*. *机器学习与数据库知识发现，145-158*.
    [http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf](http://lpis.csd.auth.gr/publications/sechidis-ecmlpkdd-2011.pdf)
- en: 'Szymański, P. and Kajdanowicz, T.; *Proceedings of the First International
    Workshop on Learning with Imbalanced Domains*: *Theory and Applications*, PMLR
    74:22-35, 2017\. [http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html](http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szymański, P. and Kajdanowicz, T.; *第一国际不平衡领域学习研讨会论文集*：*理论与应用*，PMLR 74:22-35，2017\.
    [http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html](http://proceedings.mlr.press/v74/szyma%C5%84ski17a.html)
- en: You can actually make good use of stratification even when your problem is not
    a classification, but a regression. Using stratification in regression problems
    helps your regressor to fit during cross-validation on a similar distribution
    of the target (or of the predictors) to the one found in the entire sample. In
    these cases, in order to have `StratifiedKFold` working correctly, you have to
    use a discrete proxy for your target instead of your continuous target.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，即使你的问题不是分类问题，而是回归问题，你仍然可以很好地利用分层。在回归问题中使用分层可以帮助你的回归器在交叉验证期间拟合到与整个样本中找到的类似的目标（或预测因子）分布。在这些情况下，为了使`StratifiedKFold`正常工作，你必须使用你的连续目标的一个离散代理而不是你的连续目标。
- en: 'The first, simplest way of achieving this is to use the pandas `cut` function
    and divide your target into a large enough number of bins, such as 10 or 20:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的第一种，最简单的方法是使用pandas的`cut`函数，并将你的目标划分为足够多的箱，例如10或20：
- en: '[PRE0]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In order to determine the number of bins to be used, *Abhishek Thakur* prefers
    to use **Sturges’ rule** based on the number of examples available, and provide
    that number to the pandas `cut` function (see [https://www.kaggle.com/abhishek/step-1-create-folds](https://www.kaggle.com/abhishek/step-1-create-folds)):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定要使用的箱数，*Abhishek Thakur*更喜欢使用基于可用示例数量的**Sturges规则**，并将该数字提供给pandas的`cut`函数（见[https://www.kaggle.com/abhishek/step-1-create-folds](https://www.kaggle.com/abhishek/step-1-create-folds))）：
- en: '[PRE1]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: An alternative approach is to focus on the distributions of the features in
    the training set and aim to reproduce them. This requires the use of **cluster
    analysis** (an unsupervised approach) on the features of the training set, thus
    excluding the target variable and any identifiers, and then using the predicted
    clusters as strata. You can see an example in this Notebook ([https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way](https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way)),
    where first a PCA (principal component analysis) is performed to remove correlations,
    and then a *k*-means cluster analysis is performed. You can decide on the number
    of clusters to use by running empirical tests.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是关注训练集中特征的分部，并试图重现它们。这需要在对训练集的特征进行**聚类分析**（一种无监督方法）时使用，从而排除目标变量和任何标识符，然后使用预测的聚类作为层。您可以在本笔记本中看到一个示例（[https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way](https://www.kaggle.com/lucamassaron/are-you-doing-cross-validation-the-best-way))，其中首先执行PCA（主成分分析）以消除相关性，然后执行*k*-means聚类分析。您可以通过运行经验测试来决定使用多少个聚类。
- en: 'Proceeding with our discussion of the cases where *k*-fold can provide unsuitable
    splits, things get tricky in the third scenario, when you have non-i.i.d. data,
    such as in the case of some grouping happening among examples. The problem with
    non-i.i.d. examples is that the features and target are correlated between the
    examples (hence it is easier to predict all the examples if you know just one
    example among them). In fact, if you happen to have the same group divided between
    training and testing, your model may learn to distinguish the groups and not the
    target itself, producing a good validation score but very bad results on the leaderboard.
    The solution here is to use `GroupKFold`: by providing a grouping variable, you
    will have the assurance that each group will be placed either in the training
    folds or in the validation ones, but never split between the two.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续讨论*k*-fold可能提供不合适的划分的情况时，第三种情况，即当您有非独立同分布（non-i.i.d.）的数据时，例如在某些示例之间发生分组的情况，事情变得复杂。非独立同分布示例的问题在于示例之间的特征和目标是相关的（因此，如果您知道其中的一个示例，那么预测所有示例就更容易）。实际上，如果您碰巧将同一组数据在训练集和测试集中划分，那么您的模型可能会学会区分组而不是目标本身，从而产生良好的验证分数，但在排行榜上的结果却非常糟糕。这里的解决方案是使用`GroupKFold`：通过提供一个分组变量，您可以确保每个组要么被放置在训练折中，要么被放置在验证折中，但永远不会在两者之间分割。
- en: Discovering groupings in the data that render your data non-i.i.d. is actually
    not an easy task to accomplish. Unless stated by the competition problem, you
    will have to rely on your ability to investigate the data (using unsupervised
    learning techniques, such as cluster analysis) and the domain of the problem.
    For instance, if your data is about mobile telephone usage, you may realize that
    some examples are from the same user by noticing sequences of similar values in
    the features.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中发现分组，使得数据非独立同分布（non-i.i.d.），实际上并不是一个容易完成的任务。除非竞赛问题中明确指出，否则您将不得不依靠您调查数据的能力（使用无监督学习技术，如聚类分析）和问题的领域。例如，如果您的数据是关于移动电话使用情况，您可能会通过注意到特征中相似值的序列，意识到一些示例来自同一用户。
- en: 'Time series analysis presents the same problem, and since data is non-i.i.d.,
    you cannot validate by random sampling because you will mix different time frames
    and later time frames could bear traces of the previous ones (a characteristic
    called **auto-correlation** in statistics). In the most basic approach to validation
    in time series, you can use a training and validation split based on time, as
    illustrated by *Figure 6.3*:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列分析也面临同样的问题，由于数据非独立同分布，您不能通过随机抽样进行验证，因为您将混合不同的时间框架，而后期的时间框架可能会留下前期的时间痕迹（在统计学中称为**自相关**的特征）。在时间序列验证的最基本方法中，您可以使用基于时间的训练集和验证集划分，如图*6.3*所示：
- en: '![](img/B17574_06_03.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3](img/B17574_06_03.png)'
- en: 'Figure 6.3: Training and validation splits are based on time'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3：训练集和验证集的划分基于时间
- en: Your validation capabilities will be limited, however, since your validation
    will be anchored to a specific time. For a more complex approach, you can use
    time split validation, `TimeSeriesSplit`, as provided by the Scikit-learn package
    (`sklearn.model_selection.TimeSeriesSplit`). `TimeSeriesSplit` can help you set
    the timeframe of your training and testing portions of the time series.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，你的验证能力将会有限，因为你的验证将锚定在特定的时间点上。对于更复杂的方法，你可以使用Scikit-learn包提供的`TimeSeriesSplit`（`sklearn.model_selection.TimeSeriesSplit`）中的时间分割验证。`TimeSeriesSplit`可以帮助你设置时间序列的训练和测试部分的时间框架。
- en: In the case of the training timeframe, the `TimeSeriesSplit` function can help
    you to set your training data so it involves all the past data before the test
    timeframe, or limit it to a fixed period lookback (for instance, always using
    the data from three months before the test timeframe for training).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练时间框架的情况下，`TimeSeriesSplit`函数可以帮助你设置你的训练数据，使其涉及测试时间框架之前的所有过去数据，或者限制它到一个固定的回溯期（例如，始终使用测试时间框架前三个月的数据进行训练）。
- en: 'In *Figure 6.4*, you can see the structure of a time-based validation strategy
    involving a growing training set and a moving validation set:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.4*中，你可以看到涉及增长训练集和移动验证集的时间验证策略的结构：
- en: '![](img/B17574_06_04.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_06_04.png)'
- en: 'Figure 6.4: The training set is growing over time'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4：训练集随时间增长
- en: 'In *Figure 6.5*, you can instead see how the strategy changes if you stipulate
    that the training set has a fixed lookback:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.5*中，你可以看到如果你规定训练集有一个固定的回溯期，策略是如何变化的：
- en: '![](img/B17574_06_05.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_06_05.png)'
- en: 'Figure 6.5: Training and validation splits are moving over time'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5：训练和验证分割随时间移动
- en: In our experience, going by a fixed lookback helps to provide a fairer evaluation
    of time series models since you are always counting on the same training set size.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，按照固定的回溯期可以帮助提供对时间序列模型更公平的评估，因为你总是在同一个训练集大小上计数。
- en: By instead using a growing training set size over time, you confuse the effects
    of your model performance across time slices with the decreasing bias in your
    model (since more examples mean less bias).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通过相反地使用随时间增长的训练集大小，你混淆了模型性能随时间切片的影响和模型中减少的偏差（因为更多的例子意味着更少的偏差）。
- en: Finally, remember that `TimeSeriesSplit` can be set to keep a pre-defined gap
    between your training and test time. This is extremely useful when you are told
    that the test set is a certain amount of time in the future (for instance, a month
    after the training data) and you want to test if your model is robust enough to
    predict that far into the future.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，记住`TimeSeriesSplit`可以设置为在训练和测试时间之间保持一个预定义的间隔。当你被告知测试集在未来某个时间（例如，训练数据后一个月）时，这非常有用，并且你想测试你的模型是否足够鲁棒，能够预测那么远的未来。
- en: Nested cross-validation
  id: totrans-167
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 嵌套交叉验证
- en: At this point, it is important to introduce **nested cross-validation**. Up
    to now, we have only discussed testing models with respect to their final performance,
    but often you also need to test their intermediate performance when tuning their
    hyperparameters. In fact, you cannot test how certain model parameters work on
    your test set and then use the same data in order to evaluate the final performance.
    Since you have specifically found the best parameters that work on the test set,
    your evaluation measure on the same test set will be too optimistic; on a different
    test set, you will probably not obtain the exact same result. In this case, you
    have to distinguish between a **validation set**, which is used to evaluate the
    performance of various models and hyperparameters, and a **test set**, which will
    help you to estimate the final performance of the model.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，介绍**嵌套交叉验证**非常重要。到目前为止，我们只讨论了根据最终性能测试模型，但通常在调整超参数时，还需要测试它们的中间性能。实际上，你不能测试某些模型参数在你的测试集上的效果，然后使用相同的数据来评估最终性能。因为你已经专门找到了在测试集上工作的最佳参数，你在同一测试集上的评估指标将过于乐观；在另一个测试集上，你可能不会得到完全相同的结果。在这种情况下，你必须区分**验证集**和**测试集**，验证集用于评估各种模型和超参数的性能，而测试集将帮助你估计模型的最终性能。
- en: If you are using a test-train split, this is achieved by splitting the test
    part into two new parts. The usual split is 70/20/10 for training, validation,
    and testing, respectively (but you can decide differently). If you are using cross-validation,
    you need nested cross-validation; that is, you do cross-validation based on the
    split of another cross-validation. Essentially, you run your usual cross-validation,
    but when you have to evaluate different models or different parameters, you run
    cross-validation based on the fold split.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用测试-训练分割，这是通过将测试部分分割成两个新的部分来实现的。通常的分割是70/20/10，分别用于训练、验证和测试（但你也可以决定不同）。如果你使用交叉验证，你需要嵌套交叉验证；也就是说，你基于另一个交叉验证的分割进行交叉验证。本质上，你运行常规的交叉验证，但在你需要评估不同的模型或不同的参数时，你基于折叠分割进行交叉验证。
- en: 'The example in *Figure 6.6* demonstrates this internal and external cross-validation
    structure. Within the external part, you determine the portion of the data used
    to test your evaluation metric. Within the internal part, which is fed by the
    training data from the external part, you arrange training/validation splits in
    order to evaluate and optimize specific model choices, such as deciding which
    model or hyperparameter values to pick:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.6*中的例子展示了这种内部和外部交叉验证结构。在外部部分，你确定用于测试评估指标的数据部分。在内部部分，它由外部部分提供的训练数据组成，你安排训练/验证分割以评估和优化特定的模型选择，例如决定选择哪个模型或超参数值：'
- en: '![](img/B17574_06_06.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_06_06.png)'
- en: 'Figure 6.6: How nested cross-validation is structured in an external and an
    internal loop'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6：嵌套交叉验证在外部和内部循环中的结构
- en: 'This approach has the advantage of making your test and parameter search fully
    reliable, but in doing so you incur a couple of problems:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优势在于使你的测试和参数搜索完全可靠，但在这样做的同时，你会遇到几个问题：
- en: A reduced training set, since you first split by cross-validation, and then
    you split again
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少的训练集，因为你在第一次通过交叉验证分割后，又再次分割
- en: 'More importantly, it requires a huge amount of model building: if you run two
    nested 10-fold cross-validations, you’ll need to run 100 models'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更重要的是，它需要大量的模型构建：如果你运行两个嵌套的10折交叉验证，你需要运行100个模型
- en: Especially for this last reason, some Kagglers tend to ignore nested cross-validation
    and risk some adaptive fitting by using the same cross-validation for both model/parameter
    search and performance evaluation, or using a fixed test sample for the final
    evaluation. In our experience, this approach can work as well, though it may result
    in overestimating model performance and overfitting if you are generating out-of-fold
    predictions to be used for successive modeling (something we are going to discuss
    in the next section). We always suggest you try the most suitable methodology
    for testing your models. If your aim is to correctly estimate your model’s performance
    and reuse its predictions in other models, remember that using nested cross-validation,
    whenever possible, can provide you with a less overfitting solution and could
    make the difference in certain competitions.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是因为最后一个原因，一些Kagglers倾向于忽略嵌套交叉验证，并使用相同的交叉验证进行模型/参数搜索和性能评估，或者使用固定的测试样本进行最终评估，从而承担一些适应性拟合的风险。根据我们的经验，这种方法可能同样有效，尽管如果你生成折叠外预测用于后续建模（我们将在下一节讨论），可能会导致过度估计模型性能和过拟合。我们始终建议你尝试最适合测试你模型的方
    法。如果你的目标是正确估计你的模型性能并在其他模型中重用其预测，请记住，在可能的情况下使用嵌套交叉验证可以为你提供一个更少过拟合的解决方案，并在某些比赛中产生差异。
- en: Producing out-of-fold predictions (OOF)
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 生成折叠外预测（OOF）
- en: 'An interesting application of cross-validation, besides estimating your evaluation
    metric performance, is producing test predictions and out-of-fold predictions.
    In fact, as you train on portions of your training data and predict on the remaining
    ones, you can:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 除了估计你的评估指标性能之外，交叉验证的一个有趣应用是生成测试预测和折叠外预测。实际上，当你训练你的训练数据的一部分并预测剩余的部分时，你可以：
- en: '**Predict on the test set**: The average of all the predictions is often more
    effective than re-training the same model on all the data: this is an ensembling
    technique related to blending, which will be dealt with in *Chapter 9*, *Ensembling
    with Blending and Stacking Solutions*.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在测试集上预测**：所有预测的平均值通常比在所有数据上重新训练相同的模型更有效：这是一种与混合相关的集成技术，将在*第9章*，*使用混合和堆叠解决方案进行集成*中讨论。'
- en: '**Predict on the validation set**: In the end, you will have predictions for
    the entire training set and can re-order them in the same order as the original
    training data. These predictions are commonly referred to as **out-of-fold** (**OOF**)
    **predictions** and they can be extremely useful.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在验证集上进行预测**：最后，你将对整个训练集进行预测，并可以按照原始训练数据的顺序重新排序这些预测。这些预测通常被称为**出卷预测**（**OOF**）**预测**，它们可以非常有用。'
- en: The first use of OOF predictions is to estimate your performance since you can
    compute your evaluation metric directly on the OOF predictions. The performance
    obtained is different from the cross-validated estimates (based on sampling);
    it doesn’t have the same probabilistic characteristics, so it is not a valid way
    to measure generalization performance, but it can inform you about the performance
    of your model on the specific set you are training on.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: OOF预测的第一个用途是估计你的性能，因为你可以直接在OOF预测上计算你的评估指标。获得的表现与交叉验证估计（基于采样）不同；它没有相同的概率特性，因此它不是衡量泛化性能的有效方式，但它可以告诉你你的模型在特定训练集上的表现。
- en: A second use is to produce a plot and visualize the predictions against the
    ground truth values or against other predictions obtained from different models.
    This will help you in understanding how each model works and if their predictions
    are correlated.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个用途是生成一个图表，将预测值与真实值或其他来自不同模型的预测值进行可视化。这将帮助你理解每个模型的工作原理以及它们的预测是否相关。
- en: The last use is to create meta-features or meta-predictors. This will also be
    fully explored in *Chapter 9*, but it is important to remark on now, as OOF predictions
    are a byproduct of cross-validation and they work because, during cross-validation,
    your model is always predicting on examples that it has not seen during training
    time.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个用途是创建元特征或元预测器。这将在*第9章*中完全探讨，但重要的是现在就强调这一点，因为OOF预测是交叉验证的副产品，并且它们之所以有效，是因为在交叉验证期间，你的模型始终在预测训练时间未看到的示例。
- en: Since every prediction in your OOF predictions has been generated by a model
    trained on different data, these predictions are unbiased and you can use them
    without any fear of overfitting (though there are some caveats that will be discussed
    in the next chapter).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的OOF预测中的每一个预测都是由在不同数据上训练的模型生成的，因此这些预测是无偏的，你可以放心使用，无需担心过拟合（尽管下一章将讨论一些注意事项）。
- en: 'Generating OOF predictions can be done in two ways:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 生成OOF预测有两种方式：
- en: By coding a procedure that stores the validation predictions into a prediction
    vector, taking care to arrange them in the same index position as the examples
    in the training data
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过编写一个将验证预测存储到预测向量中的程序，同时注意将它们按照训练数据中的示例索引位置排列
- en: By using the Scikit-learn function `cross_val_predict`, which will automatically
    generate the OOF predictions for you
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用Scikit-learn函数`cross_val_predict`，该函数将自动为你生成OOF预测
- en: We will be seeing this second technique in action when we look at adversarial
    validation later in this chapter.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在本章后面查看对抗验证时，我们将看到这种第二种技术的实际应用。
- en: Subsampling
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 子采样
- en: There are other validation strategies aside from *k*-fold cross-validation,
    but they do not have the same generalization properties. We have already discussed
    LOO, which is the case when *k = n* (where *n* is the number of examples). Another
    choice is **subsampling**. Subsampling is similar to *k*-fold, but you do not
    have fixed folds; you use as many as you think are necessary (in other words,
    take an educated guess). You repetitively subsample your data, each time using
    the data that you sampled as training data and the data that has been left unsampled
    for your validation. By averaging the evaluation metrics of all the subsamples,
    you will get a validation estimate of the performances of your model.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*k*-折交叉验证之外，还有其他验证策略，但它们不具有相同的泛化特性。我们已经讨论了LOO，这是*k = n*（其中*n*是示例数量）的情况。另一个选择是**子采样**。子采样类似于*k*-折，但你没有固定的折数；你使用你认为必要的数量（换句话说，做出一个有根据的猜测）。你重复地子采样你的数据，每次使用你采样的数据作为训练数据，而未采样的数据作为验证数据。通过平均所有子样本的评估指标，你将得到模型性能的验证估计。
- en: Since you are systematically testing all your examples, as in *k*-fold, you
    actually need quite a lot of trials to have a good chance of testing all of them.
    For the same reason, some cases may be tested more than others if you do not apply
    enough subsamples. You can run this sort of validation using `ShuffleSplit` from
    Scikit-learn.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你正在系统地测试所有示例，就像在*k*-fold中一样，你需要进行相当多的试验才能有很好的机会测试它们。同样，如果你没有应用足够的子样本，某些案例可能比其他案例被测试的次数更多。你可以使用Scikit-learn的`ShuffleSplit`运行这种验证。
- en: The bootstrap
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重抽样
- en: Finally, another option is to try the **bootstrap**, which has been devised
    in statistics to conclude the error distribution of an estimate; for the same
    reasons, it can be used for performance estimation. The bootstrap requires you
    to draw a sample, *with replacement*, that is the same size as the available data.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一个选择是尝试**重抽样**，这在统计学中被设计用来推断估计值的误差分布；出于同样的原因，它也可以用于性能估计。重抽样要求你抽取一个样本，**有放回地**，其大小与可用数据相同。
- en: 'At this point, you can use the bootstrap in two different ways:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，你可以以两种不同的方式使用重抽样：
- en: As in statistics, you can bootstrap multiple times, train your model on the
    samples, and compute your evaluation metric on the training data itself. The average
    of the bootstraps will provide your final evaluation.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就像在统计学中一样，你可以多次进行重抽样，在样本上训练你的模型，并在训练数据本身上计算你的评估指标。重抽样的平均值将提供你的最终评估。
- en: Otherwise, as in subsampling, you can use the bootstrapped sample for your training
    and what is left not sampled from the data as your test set.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，就像在子抽样中一样，你可以使用重抽样样本进行训练，并将未从数据中抽样的部分作为测试集。
- en: 'In our experience, the first method of calculating the evaluation metric on
    the bootstrapped training data, often used in statistics for linear models in
    order to estimate the value of the model’s coefficients and their error distributions,
    is much less useful in machine learning. This is because many machine learning
    algorithms tend to overfit the training data, hence you can never have a valid
    metric evaluation on your training data, even if you bootstrap it. For this reason,
    Efron and Tibshirani (see *Efron*, *B. and Tibshirani, R*. *Improvements on cross-validation:
    the 632+ bootstrap method.* Journal of the American Statistical Association 92.438
    (1997): 548-560.) proposed **the 632+ estimator** as a final validation metric.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '根据我们的经验，在重抽样训练数据上计算评估指标的第一种方法，通常在统计学中用于线性模型的系数值及其误差分布的估计，在机器学习中作用不大。这是因为许多机器学习算法倾向于过度拟合训练数据，因此即使进行重抽样，你也无法在训练数据上得到有效的指标评估。因此，Efron和Tibshirani（参见*Efron*,
    *B. and Tibshirani, R*. *Improvements on cross-validation: the 632+ bootstrap
    method.* Journal of the American Statistical Association 92.438 (1997): 548-560.）提出了**632+估计器**作为最终的验证指标。'
- en: 'At first, they proposed a simple version, called the 632 bootstrap:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，他们提出了一个简单版本，称为632重抽样：
- en: '![](img/B17574_06_001.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_06_001.png)'
- en: 'In this formula, given your evaluation metric *err*, *err*[fit] is your metric
    computed on the training data and *err*[bootstrap] is the metric computed on the
    bootstrapped data. However, in the case of an overfitted training model, *err*[fit]
    would tend to zero, rendering the estimator not very useful. Therefore, they developed
    a second version of the 632+ bootstrap:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，给定你的评估指标*err*，*err*[fit]是在训练数据上计算的指标，*err*[bootstrap]是在重抽样数据上计算的指标。然而，在训练模型过度拟合的情况下，*err*[fit]往往会接近零，使得估计器不太有用。因此，他们开发了632+重抽样的第二个版本：
- en: '![](img/B17574_06_002.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_06_002.png)'
- en: 'Where *w* is:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*w*是：
- en: '![](img/B17574_06_003.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_06_003.png)'
- en: '![](img/B17574_06_004.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_06_004.png)'
- en: Here you have a new parameter, ![](img/B17574_06_005.png), which is the **no-information
    error rate**, estimated by evaluating the prediction model on all possible combinations
    of targets and predictors. Calculating ![](img/B17574_06_005.png) is indeed intractable,
    as discussed by the developers of Scikit-learn ([https://github.com/scikit-learn/scikit-learn/issues/9153](https://github.com/scikit-learn/scikit-learn/issues/9153)).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个新的参数，![图片](img/B17574_06_005.png)，这是**无信息误差率**，通过评估预测模型在所有可能的响应变量和预测变量组合上来估计。正如Scikit-learn的开发者所讨论的（[https://github.com/scikit-learn/scikit-learn/issues/9153](https://github.com/scikit-learn/scikit-learn/issues/9153)），计算![图片](img/B17574_06_005.png)实际上是不可行的。
- en: Given the limits and intractability of using the bootstrap as in classical statistics
    for machine learning applications, you can instead use the second method, getting
    your evaluation from the examples left not sampled by the bootstrap.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用自助法作为经典统计学在机器学习应用中的局限性和不可行性，你可以使用第二种方法，从自助法未采样的例子中获得你的评估。
- en: In this form, the bootstrap is an alternative to cross-validation, but as with
    subsampling, it requires building many more models and testing them than for cross-validation.
    However, it makes sense to know about such alternatives in case your cross-validation
    is showing too high a variance in the evaluation metric and you need more intensive
    checking through testing and re-testing.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种形式下，自助法是交叉验证的替代方案，但与子采样一样，它需要构建更多的模型并进行测试，比交叉验证要多。然而，了解这些替代方案是有意义的，以防你的交叉验证在评估指标上显示出过高的方差，你需要通过测试和重新测试进行更深入的检查。
- en: 'Previously, this method has been implemented in Scikit-learn ([https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613](https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613))
    but was then removed. Since you cannot find the bootstrap anymore on Scikit-learn
    and it bootstrapped even the test data, you can use our own implementation. Here
    is our example:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 以前，这种方法已经在Scikit-learn中实现（[https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613](https://github.com/scikit-learn/scikit-learn/blob/0.16.X/sklearn/cross_validation.py#L613)），但后来被移除了。由于你无法在Scikit-learn中找到自助法，而且它甚至对测试数据也进行了自助，你可以使用我们自己的实现。以下是我们的示例：
- en: '[PRE2]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: In conclusion, the bootstrap is indeed an alternative to cross-validation. It
    is certainly more widely used in statistics and finance. In machine learning,
    the golden rule is to use the *k*-fold cross-validation approach. However, we
    suggest not forgetting about the bootstrap in all those situations where, due
    to outliers or a few examples that are too heterogeneous, you have a large standard
    error of the evaluation metric in cross-validation. In these cases, the bootstrap
    will prove much more useful in validating your models.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，自助法确实是交叉验证的替代方案。它在统计学和金融学中确实被更广泛地使用。在机器学习中，黄金法则是要使用*k*-折交叉验证方法。然而，我们建议不要忘记在所有那些情况下使用自助法，因为这些情况是由于异常值或一些过于异质化的例子，你在交叉验证中评估指标的标准误差很大。在这些情况下，自助法将证明在验证你的模型方面非常有用。
- en: '![](img/Ryan_Chesler.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![Ryan Chesler](img/Ryan_Chesler.png)'
- en: Ryan Chesler
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Ryan Chesler
- en: '[https://www.kaggle.com/ryches](https://www.kaggle.com/ryches)'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/ryches](https://www.kaggle.com/ryches)'
- en: Our second interview of the chapter is with Ryan Chesler, a Discussions Grandmaster
    and Notebooks and Competitions Master. He is a Data Scientist at H2O.ai and one
    of the organizers of the San Diego Machine Learning group on Meetup ([https://www.meetup.com/San-Diego-Machine-Learning/](https://www.meetup.com/San-Diego-Machine-Learning/)).
    The importance of validation came up in a few of his answers.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二次采访是与Ryan Chesler，一个讨论大师和笔记本及竞赛大师。他是H2O.ai的数据科学家，也是Meetup上圣地亚哥机器学习小组的组织者之一（[https://www.meetup.com/San-Diego-Machine-Learning/](https://www.meetup.com/San-Diego-Machine-Learning/))。在他的回答中，验证的重要性被提到了几次。
- en: What’s your favourite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*I tend to dabble in all kinds of competitions. It is more interesting to sample
    varied problems than specialize in a specific niche like computer vision or natural
    language processing. The ones I find most interesting are the ones where there
    are deep insights that can be derived from the data and error of predictions.
    For me, error analysis is one of the most illuminating processes; understanding
    where the model is failing and trying to find some way to improve the model or
    input data representation to address the weakness.*'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '*我倾向于涉猎各种竞赛。与专注于特定领域，如计算机视觉或自然语言处理相比，尝试各种各样的问题更有趣。我最感兴趣的是那些可以从数据和分析预测错误中得出深刻见解的问题。对我来说，错误分析是最有启发性的过程之一；理解模型失败的地方，并试图找到改进模型或输入数据表示的方法来克服弱点。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle竞赛的？这种方法和你在日常工作中所做的工作有何不同？
- en: '*My approach is similar in both cases. Many people seem to favor exploratory
    data analysis before any modeling efforts, but I find that the process of preparing
    the data for modeling is usually sufficient. My typical approach is to manually
    view the data and make some preliminary decisions about how I think I can best
    model the data and different options to explore. After this, I build the model
    and evaluate performance, and then focus on analysing errors and reason about
    the next modeling steps based on where I see the model making errors.*'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '*在这两种情况下，我的方法都是相似的。很多人似乎更喜欢在建模之前进行探索性数据分析，但我发现为建模准备数据的过程通常就足够了。我典型的做法是手动查看数据，并就我认为如何最好地建模数据和探索不同选项做出一些初步决定。然后，我构建模型并评估性能，然后专注于分析错误，并根据我在模型出错的地方推理出下一步的建模步骤。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你在你的职业生涯中？如果是，那又是如何帮助的？
- en: '*Yes, it helped me get my current job. I work at H2O and they greatly value
    Kaggle achievements. My previous job also liked that I performed well in competitions.*'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的，这帮助我得到了我现在的工作。我在H2O工作，他们非常重视Kaggle的成就。我的上一份工作也喜欢我在比赛中表现良好。*'
- en: You are also the organizer of a meetup in San Diego with over two thousand participants.
    Is this related to your experience with Kaggle?
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 你也是圣地亚哥一个有两千多名参与者的Meetup的组织者。这与你在Kaggle的经验有关吗？
- en: '*Yes, it is absolutely related. I started from very little knowledge and tried
    out a Kaggle competition without much success at first. I went to a local meetup
    and found people to team up with and learn from. At the time, I got to work with
    people of a much higher skill level than me and we did really well in a competition,
    3rd/4500+ teams.*'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '*是的，这绝对有关。我从几乎一无所知开始，尝试了一个Kaggle比赛，一开始并没有取得太大的成功。我参加了一个当地的Meetup，找到了可以一起学习和合作的人。当时，我能够与比我技能水平高得多的人一起工作，我们在比赛中表现得很好，排名第三/4500+个团队。*'
- en: '*After this, the group stopped being as consistent and I wanted to keep the
    community going, so I made my own group and started organizing my own events.
    I’ve been doing that for almost 4 years and I get to be on the opposite side of
    the table teaching people and helping them get started. We originally just focused
    on Kaggle competitions and trying to form teams, but have slowly started branching
    off to doing book clubs and lectures on various topics of interest. I attribute
    a lot of my success to having this dedicated weekly time to study and think about
    machine learning.*'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '*之后，这个团队不再那么一致，我想让这个社区继续下去，所以我组建了自己的团队，开始组织自己的活动。我差不多已经做了4年，我得以站在桌子的另一边教人们，帮助他们开始。最初，我们只专注于Kaggle比赛和尝试组建团队，但慢慢地开始扩展到做读书俱乐部和关于各种感兴趣主题的讲座。我把我的很多成功归因于有这个专门的每周时间来学习和思考机器学习。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kagglers通常忽略了什么？你现在知道的事情，你希望在你刚开始的时候就知道？
- en: '*In my experience, a lot of people overstate the importance of bias-variance
    trade-off and overfitting. This is something I have seen people consistently worry
    about too much. The focus should not be making training and validation performance
    close, but make validation performance as good as possible.*'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我的经验中，很多人过分强调偏差-方差权衡和过拟合的重要性。这是我一直看到人们过分担心的事情。重点不应该是使训练和验证性能接近，而应该是使验证性能尽可能好。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 你在过去比赛中犯过哪些错误？
- en: '*My consistent mistake is not exploring enough. Sometimes I have ideas that
    I discount too early and turn out to be important for improving performance. Very
    often I can get close to competitive performance on the first try, but iterating
    and continuing to improve as I try new things takes a slightly different skill
    that I am still working on mastering.*'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的一个持续错误就是探索得不够。有时候我过早地否定了自己的想法，结果这些想法对于提高表现非常重要。我经常可以在第一次尝试时就接近竞争水平，但当我尝试新事物并持续改进时，需要一种我仍在努力掌握的不同技能。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些特定的工具或库来进行数据分析或机器学习？
- en: '*I use a lot of the standard tools: XGBoost, LightGBM, Pytorch, TensorFlow,
    Scikit-learn. I don’t have any strong affinity for a specific tool or library,
    just whatever is relevant to the problem.*'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '*我使用了很多标准工具：XGBoost、LightGBM、Pytorch、TensorFlow、Scikit-learn。我没有对任何特定的工具或库有强烈的偏好，只是与问题相关的任何工具。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加竞赛时，他们应该牢记或做什么是最重要的？
- en: '*I think the most important thing people have to keep in mind is good validation.
    Very often I see people fooling themselves thinking their performance is improving
    but then submitting to the leaderboard and realizing it didn’t actually go how
    they expected. It is an important skill to understand how to match assumptions
    with your new unseen data and build a model that is robust to new conditions.*'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '*我认为人们必须牢记的最重要的事情是良好的验证。我经常看到人们自欺欺人，以为他们的性能在提高，但提交到排行榜后，才发现实际情况并没有他们预期的那么好。理解如何将假设与你的新未见数据相匹配，并构建一个对新条件具有鲁棒性的模型是一项重要的技能。*'
- en: Tuning your model validation system
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整你的模型验证系统
- en: At this point, you should have a complete overview of all possible validation
    strategies. When you approach a competition, you devise your validation strategy
    and you implement it. Then, you test if the strategy you have chosen is correct.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，你应该对所有可能的验证策略有一个全面的了解。当你参加竞赛时，制定你的验证策略并实施它。然后，测试你选择的策略是否正确。
- en: As a golden rule, be guided in devising your validation strategy by the idea
    that you have to replicate the same approach used by the organizers of the competition
    to split the data into training, private, and public test sets. Ask yourself how
    the organizers have arranged those splits. Did they draw a random sample? Did
    they try to preserve some specific distribution in the data? Are the test sets
    actually drawn from the same distribution as the training data?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一条黄金法则，在制定你的验证策略时，要以你必须复制竞赛组织者用于将数据分割为训练、私有和公共测试集的方法为指导思想。问问自己组织者是如何安排这些分割的。他们是随机抽取样本吗？他们试图保留数据中的某些特定分布吗？测试集实际上是从与训练数据相同的分布中抽取的吗？
- en: These are not the questions you would ask yourself in a real-world project.
    Contrary to a real-world project where you have to generalize at all costs, a
    competition has a much narrower focus on having a model that performs on the given
    test set (especially the private one). If you focus on this idea from the beginning,
    you will have more of a chance of finding out the best validation strategy, which
    will help you rank more highly in the competition.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不是你在现实世界项目中会问自己的问题。与必须不惜一切代价进行泛化的现实世界项目相反，竞赛有一个更狭窄的焦点，即拥有在给定测试集（特别是私有测试集）上表现良好的模型。如果你从一开始就关注这个想法，你就有更大的机会找到最佳的验证策略，这将有助于你在竞赛中排名更高。
- en: 'Since this is a trial-and-error process, as you try to find the best validation
    strategy for the competition, you can systematically apply the following two consistency
    checks in order to figure out if you are on the right path:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个试错过程，当你试图为竞赛找到最佳的验证策略时，你可以系统地应用以下两个一致性检查，以确定你是否走上了正确的道路：
- en: First, you have to check if your local tests are consistent, that is, that the
    single cross-validation fold errors are not so different from each other or, when
    you opt for a simple train-test split, that the same results are reproducible
    using different train-test splits.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你必须检查你的本地测试是否一致，也就是说，单个交叉验证折的误差彼此之间差异不大，或者当你选择简单的训练-测试分割时，使用不同的训练-测试分割可以重现相同的结果。
- en: Then, you have to check if your local validation error is consistent with the
    results on the public leaderboard.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你必须检查你的本地验证误差是否与公共排行榜上的结果一致。
- en: 'If you’re failing the first check, you have a few options depending on the
    following possible origins of the problem:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你第一次检查失败了，你可以根据以下可能的问题来源选择几个选项：
- en: You don’t have much training data
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你没有多少训练数据
- en: The data is too diverse and every training partition is very different from
    every other (for instance, if you have too many **high cardinality** features,
    that is, features with too many levels – like zip codes – or if you have multivariate
    outliers)
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据过于多样，每个训练分区都与每个其他分区非常不同（例如，如果你有太多的**高基数**特征，即具有太多级别的特征——如邮政编码——或者如果你有多元异常值）
- en: In both cases, the point is that you lack data with respect to the model you
    want to implement. Even when the problem just appears to be that the data is too
    diverse, plotting learning curves will make it evident to you that your model
    needs more data.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，关键是你缺乏你想要实施的模型所需的数据。即使问题只是数据过于多样，绘制学习曲线也会让你明显看出你的模型需要更多的数据。
- en: 'In this case, unless you find out that moving to a simpler algorithm works
    on the evaluation metric (in which case trading variance for bias may worsen your
    model’s performance, but not always), your best choice is to use an extensive
    validation approach. This can be implemented by:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，除非你发现转向一个更简单的算法在评估指标上有效（在这种情况下，以方差换取偏差可能会降低你的模型性能，但并不总是如此），否则你的最佳选择是使用广泛的验证方法。这可以通过以下方式实现：
- en: Using larger *k* values (thus approaching LOO where *k = n*). Your validation
    results will be less about the capability of your model to perform on unseen data,
    but by using larger training portions, you will have the advantage of more stable
    evaluations.
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更大的 *k* 值（从而接近LOO，其中 *k = n*）。你的验证结果将更多地关于你的模型在未见数据上的表现能力，但通过使用更大的训练部分，你将拥有更稳定评估的优势。
- en: Averaging the results of multiple *k*-fold validations (based on different data
    partitions picked by different random seed initializations).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均多个 *k*-折验证的结果（基于不同随机种子初始化选择的不同数据分区）。
- en: Using repetitive bootstrapping.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用重复的引导启动。
- en: Keep in mind that when you find unstable local validation results, you won’t
    be the only one to suffer from the problem. Usually, this is a common problem
    due to the data’s origin and characteristics. By keeping tuned in to the discussion
    forums, you may get hints at possible solutions. For instance, a good solution
    for high cardinality features is target encoding; stratification can help with
    outliers; and so on.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当你发现不稳定的局部验证结果时，你不会是唯一一个受此问题困扰的人。通常，这是一个由于数据来源和特征而普遍存在的问题。通过关注讨论论坛，你可能会得到一些可能的解决方案的提示。例如，对于高基数特征，目标编码是一个好的解决方案；分层可以帮助处理异常值；等等。
- en: The situation is different when you’ve passed the first check but failed the
    second; your local cross-validation is consistent but you find that it doesn’t
    hold on the leaderboard. In order to realize this problem exists, you have to
    keep diligent note of all your experiments, validation test types, random seeds
    used, and leaderboard results if you submitted the resulting predictions. In this
    way, you can draw a simple scatterplot and try fitting a linear regression or,
    even simpler, compute a correlation between your local results and the associated
    public leaderboard scores. It costs some time and patience to annotate and analyze
    all of these, but it is the most important meta-analysis of your competition performances
    that you can keep track of.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当你通过了第一项检查但失败了第二项时，情况就不同了；你的局部交叉验证是一致的，但你发现它在排行榜上并不成立。为了意识到这个问题存在，你必须仔细记录你所有的实验、验证测试类型、使用的随机种子以及如果你提交了预测结果，排行榜结果。这样，你可以绘制一个简单的散点图并尝试拟合线性回归，或者更简单的是，计算你的局部结果与相关公开排行榜分数之间的相关性。注释和分析所有这些需要一些时间和耐心，但这是你可以跟踪的最重要的比赛表现元分析。
- en: When the mismatch is because your validation score is systematically lower or
    higher than the leaderboard score, you actually have a strong signal that something
    is missing from your validation strategy, but this problem does not prevent you
    from improving your model. In fact, you can keep on working on your model and
    expect improvements to be reflected on the leaderboard, though not in a proportional
    way. However, systematic differences are always a red flag, implying something
    is different between what you are doing and what the organizers have arranged
    for testing the model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当不匹配是因为你的验证分数系统地低于或高于排行榜分数时，你实际上有一个强烈的信号表明你的验证策略中缺少某些东西，但这并不妨碍你改进你的模型。实际上，你可以继续改进你的模型，并期待排行榜上的改进，尽管不是成比例的。然而，系统性的差异始终是一个红旗，意味着你所做的事情与组织者安排的测试模型之间存在差异。
- en: 'An even worse scenario occurs when your local cross-validation scores do not
    correlate at all with the leaderboard feedback. This is really a red flag. When
    you realize this is the case, you should immediately run a series of tests and
    investigations in order to figure out why, because, regardless of whether it is
    a common problem or not, the situation poses a serious threat to your final rankings.
    There are a few possibilities in such a scenario:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的局部交叉验证分数与排行榜反馈完全不相关时，情况甚至更糟。这确实是一个红旗。当你意识到这种情况时，你应该立即运行一系列测试和调查，以找出原因，因为无论这是一个普遍问题还是不是，这种情况对你的最终排名构成了严重威胁。在这种情况下，有几种可能性：
- en: You figure out that the test set is drawn from a different distribution to the
    training set. The adversarial validation test (that we will discuss in the next
    section) is the method that can enlighten you in such a situation.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你发现测试集是从与训练集不同的分布中抽取的。敌对验证测试（我们将在下一节讨论）是在这种情况下可以启发你的方法。
- en: 'The data is non-i.i.d. but this is not explicit. For instance, in *The Nature
    Conservancy Fisheries Monitoring* competition ([https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring)),
    images in the training set were taken from similar situations (fishing boats).
    You had to figure out by yourself how to arrange them in order to avoid the model
    learning to identify the target rather than the context of the images (see, for
    instance, this work by *Anokas*: [https://www.kaggle.com/anokas/finding-boatids](https://www.kaggle.com/anokas/finding-boatids)).'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是非独立同分布的，但这并不明确。例如，在*《自然保护基金会渔业监测》*竞赛([https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring](https://www.kaggle.com/c/the-nature-conservancy-fisheries-monitoring))中，训练集中的图像是从类似情况（渔船）中拍摄的。你必须自己想出如何安排它们，以避免模型学会识别目标而不是图像的上下文（例如，参见*Anokas*的这项工作：[https://www.kaggle.com/anokas/finding-boatids](https://www.kaggle.com/anokas/finding-boatids))。
- en: The multivariate distribution of the features is the same, but some groups are
    distributed differently in the test set. If you can figure out the differences,
    you can set your training set and your validation accordingly and gain an edge.
    You need to probe the public leaderboard to work this out.
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征的多变量分布是相同的，但在测试集中某些组的分布不同。如果你能找出这些差异，你可以相应地设置你的训练集和验证集，从而获得优势。你需要调查公共排行榜来解决这个问题。
- en: The test data is drifted or trended, which is usually the case in time series
    predictions. Again, you need to probe the public leaderboard to get some insight
    about some possible post-processing that could help your score, for instance,
    applying a multiplier to your predictions, thus mimicking a decreasing or increasing
    trend in the test data.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据发生了漂移或趋势，这在时间序列预测中通常是情况。再次强调，你需要调查公共排行榜来获取一些关于可能有助于提高分数的后处理方法的见解，例如，将乘数应用于你的预测，从而模拟测试数据中的下降或上升趋势。
- en: As we’ve discussed before, probing the leaderboard is the act of making specifically
    devised submissions in order to get insights about the composition of the public
    test set. It works particularly well if the private test set is similar to the
    public one. There are no general methods for probing, so you have to devise a
    probing methodology according to the type of competition and problem.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，调查排行榜是通过专门设计的提交来获取关于公共测试集组成的洞察。如果私有测试集与公共测试集相似，这种方法特别有效。没有通用的调查方法，因此你必须根据竞赛和问题的类型设计调查方法。
- en: For instance, in the paper *Climbing the Kaggle Leaderboard by Exploiting the
    Log-Loss Oracle* ([https://export.arxiv.org/pdf/1707.01825](https://export.arxiv.org/pdf/1707.01825)),
    Jacob explains how to get fourth position in a competition without even downloading
    the training data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在论文*《通过利用对数损失算子攀登Kaggle排行榜》*([https://export.arxiv.org/pdf/1707.01825](https://export.arxiv.org/pdf/1707.01825))中，Jacob解释了如何在没有下载训练数据的情况下在竞赛中获得第四名。
- en: 'With regard to regression problems, in the recent *30 Days of ML* organized
    by Kaggle, *Hung Khoi* explained how probing the leaderboard helped him to understand
    the differences in the mean and standard deviation of the target column between
    the training dataset and the public test data (see: [https://www.kaggle.com/c/30-days-of-ml/discussion/269541](https://www.kaggle.com/c/30-days-of-ml/discussion/269541)).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 关于回归问题，在Kaggle最近组织的*“30天机器学习”*活动中，*Hung Khoi*解释了如何通过调查排行榜帮助他理解训练数据集和公共测试数据中目标列的均值和标准差之间的差异（参见：[https://www.kaggle.com/c/30-days-of-ml/discussion/269541](https://www.kaggle.com/c/30-days-of-ml/discussion/269541))。
- en: 'He used the following equation:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 他使用了以下方程：
- en: '![](img/B17574_06_007.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_06_007.png)'
- en: Essentially, you need just two submissions to solve for the mean and variance
    of the test target, since there are two unknown terms – variance and mean.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，你只需要提交两次来求解测试目标的均值和方差，因为有两个未知项——方差和均值。
- en: You can also get some other ideas about leaderboard probing from *Chris Deotte*
    ([https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)) from this post,
    [https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place](https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place),
    relevant to the *Don’t Overfit II competition* ([https://www.kaggle.com/c/dont-overfit-ii](https://www.kaggle.com/c/dont-overfit-ii)).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以从*克里斯·德奥特*（[https://www.kaggle.com/cdeotte](https://www.kaggle.com/cdeotte)）的这篇帖子中获得一些关于排行榜探测的其他想法，[https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place](https://www.kaggle.com/cdeotte/lb-probing-strategies-0-890-2nd-place)，与*不要过度拟合II竞赛*（[https://www.kaggle.com/c/dont-overfit-ii](https://www.kaggle.com/c/dont-overfit-ii)）相关。
- en: 'If you want to get a feeling about how probing information from the leaderboard
    is a double-edged sword, you can read about how *Zahar Chikishev* managed to probe
    information from the *LANL Earthquake Prediction* competition, ending up in 87^(th)
    place in the private leaderboard after leading in the public one: [https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe](https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe)'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要了解从排行榜中获取信息是如何一把双刃剑，你可以阅读一下*扎哈·奇基舍夫*是如何在*LANL地震预测*竞赛中获取信息，最终在公开排行榜上领先，但在私人排行榜上排名87位：[https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe](https://towardsdatascience.com/how-to-lb-probe-on-kaggle-c0aa21458bfe)
- en: Using adversarial validation
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用对抗验证
- en: As we have discussed, cross-validation allows you to test your model’s ability
    to generalize to unseen datasets coming from the same distribution as your training
    data. Hopefully, since in a Kaggle competition you are asked to create a model
    that can predict on the public and private datasets, you should expect that such
    test data is from the same distribution as the training data. In reality, this
    is not always the case.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，交叉验证允许你测试你的模型将泛化到来自与你的训练数据相同分布的未见数据集的能力。希望你在Kaggle竞赛中需要创建一个可以预测公共和私有数据集的模型，你应该期待这样的测试数据与训练数据来自相同的分布。在现实中，这并不总是如此。
- en: Even if you do not overfit to the test data because you have based your decision
    not only on the leaderboard results but also considered your cross-validation,
    you may still be surprised by the results. This could happen in the event that
    the test set is even slightly different from the training set on which you have
    based your model. In fact, the target probability and its distribution, as well
    as how the predictive variables relate to it, inform your model during training
    about certain expectations that cannot be satisfied if the test data is different
    from the training data.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你没有因为你的决策不仅基于排行榜结果，还考虑了交叉验证而过度拟合测试数据，你仍然可能会对结果感到惊讶。这种情况可能发生在测试集与你的模型基于的训练集略有不同的情况下。实际上，目标概率及其分布，以及预测变量如何与之相关，在训练过程中向你的模型传达了某些期望，如果测试数据与训练数据不同，这些期望是无法满足的。
- en: Hence, it is not enough to avoid overfitting to the leaderboard as we have discussed
    up to now, but, in the first place, it is also advisable to find out if your test
    data is comparable to the training data. Then, if they differ, you have to figure
    out if there is any chance that you can mitigate the different distributions between
    training and test data and build a model that performs on that test set.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仅仅避免过度拟合到排行榜，就像我们之前讨论的那样，是不够的，首先，还建议找出你的测试数据是否与训练数据可比。然后，如果它们不同，你必须弄清楚是否有机会减轻训练数据和测试数据之间的不同分布，并构建一个在该测试集上表现良好的模型。
- en: '**Adversarial validation** has been developed just for this purpose. It is
    a technique allowing you to easily estimate the degree of difference between your
    training and test data. This technique was long rumored among Kaggle participants
    and transmitted from team to team until it emerged publicly thanks to a post by
    *Zygmunt Zając* ([https://www.kaggle.com/zygmunt](https://www.kaggle.com/zygmunt))
    on his FastML blog.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '**对抗验证**就是为了这个目的而开发的。这是一种允许你轻松估计你的训练数据和测试数据之间差异程度的技巧。这项技术长期以来在Kaggle参与者中流传，从团队到团队传递，直到*齐格蒙特·扎尼亚克*（[https://www.kaggle.com/zygmunt](https://www.kaggle.com/zygmunt)）在他的FastML博客上发布文章后公开。'
- en: 'The idea is simple: take your training data, remove the target, assemble your
    training data together with your test data, and create a new binary classification
    target where the positive label is assigned to the test data. At this point, run
    a machine learning classifier and evaluate for the ROC-AUC evaluation metric (we
    discussed this metric in the previous chapter on *Detailing Competition Tasks
    and Metrics*).'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法很简单：取你的训练数据，移除目标，将你的训练数据和测试数据一起组装，并创建一个新的二元分类目标，其中正标签分配给测试数据。在这个时候，运行机器学习分类器，并评估ROC-AUC评估指标（我们在上一章*详细说明竞赛任务和指标*中讨论了此指标）。
- en: 'If your ROC-AUC is around 0.5, it means that the training and test data are
    not easily distinguishable and are apparently from the same distribution. ROC-AUC
    values higher than 0.5 and nearing 1.0 signal that it is easy for the algorithm
    to figure out what is from the training set and what is from the test set: in
    such a case, don’t expect to be able to easily generalize to the test set because
    it clearly comes from a different distribution.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的ROC-AUC值在0.5左右，这意味着训练数据和测试数据不容易区分，并且显然来自相同的分布。高于0.5且接近1.0的ROC-AUC值表明算法很容易区分来自训练集和测试集的数据：在这种情况下，不要期望能够轻易地将结果推广到测试集，因为它显然来自不同的分布。
- en: 'You can find an example Notebook written for the *Sberbank Russian Housing
    Market* competition ([https://www.kaggle.com/c/sberbank-russian-housing-market](https://www.kaggle.com/c/sberbank-russian-housing-market))
    that demonstrates a practical example of adversarial validation and its usage
    in a competition here: [https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在为*Sberbank俄罗斯住房市场*竞赛编写的示例Notebook中找到示例（[https://www.kaggle.com/c/sberbank-russian-housing-market](https://www.kaggle.com/c/sberbank-russian-housing-market)），它展示了对抗验证的实际示例及其在竞赛中的应用：[https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms](https://www.kaggle.com/konradb/adversarial-validation-and-other-scary-terms)。
- en: 'Since your data may be of different types (numeric or string labels) and you
    may have missing cases, you’ll need some data processing before being able to
    successfully run the classifier. Our suggestion is to use the random forest classifier
    because:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你的数据可能具有不同的类型（数值或字符串标签）并且你可能存在缺失情况，在成功运行分类器之前，你需要进行一些数据处理。我们的建议是使用随机森林分类器，因为：
- en: It doesn’t output true probabilities but its results are intended as simply
    ordinal, which is a perfect fit for an ROC-AUC score.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不输出真正的概率，但其结果仅作为序数，这对于ROC-AUC分数来说是一个完美的匹配。
- en: The random forest is a flexible algorithm based on decision trees that can do
    feature selection by itself and operate on different types of features without
    any pre-processing, while rendering all the data numeric. It is also quite robust
    to overfitting and you don’t have to think too much about fixing its hyperparameters.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林是一种基于决策树的灵活算法，它可以自行进行特征选择，并且可以在不进行任何预处理的情况下操作不同类型的特征，同时将所有数据转换为数值。它对过拟合也相当稳健，你不必过多考虑调整其超参数。
- en: You don’t need much data processing because of its tree-based nature. For missing
    data, you can simply replace the values with an improbable negative value such
    as -999, and you can deal with string variables by converting their strings into
    numbers (for instance, using the Scikit-learn label encoder, `sklearn.preprocessing.LabelEncoder`).
    As a solution, it performs less well than one-hot encoding, but it is very speedy
    and it will work properly for the problem.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其基于树的本质，你不需要太多的数据处理。对于缺失数据，你可以简单地用不可能的负值（如-999）替换这些值，并且你可以通过将字符串转换为数字（例如，使用Scikit-learn标签编码器，`sklearn.preprocessing.LabelEncoder`）来处理字符串变量。作为一个解决方案，它的性能不如独热编码，但它非常快速，并且对于这个问题可以正常工作。
- en: Although building a classification model is the most direct way to adversarially
    validate your test set, you can also use other approaches. One approach is to
    map both training and test data into a lower-dimensional space, as in this post
    ([https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data](https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data))
    by *NanoMathias* ([https://www.kaggle.com/nanomathias](https://www.kaggle.com/nanomathias)).
    Although requiring more tuning work, such an approach based on t-SNE and PCA has
    the great advantage of being graphically representable in an appealing and understandable
    way.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然构建分类模型是直接对抗验证测试集的最直接方法，但你也可以使用其他方法。一种方法是将训练数据和测试数据映射到一个低维空间，就像NanoMathias在这篇帖子中做的那样
    ([https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data](https://www.kaggle.com/nanomathias/distribution-of-test-vs-training-data))。虽然需要更多的调整工作，但基于t-SNE和PCA的这种方法具有将结果以吸引人且易于理解的方式图形化的巨大优势。
- en: Don’t forget that our brains are more adept at spotting patterns in visual representations
    than numeric ones (for an articulate discussion about our visual abilities, see
    [https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480](https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480)).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记，我们的大脑在识别视觉表示中的模式方面比识别数值模式更擅长（关于我们视觉能力的详细讨论，请参阅[https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480](https://onlinelibrary.wiley.com/doi/full/10.1002/qua.24480))。
- en: PCA and t-SNE are not the only tools that can help you to reduce the dimensionality
    of your data and allow you to visualize it. UMAP ([https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap))
    can often provide a faster low dimensionality solution with clear and distinct
    data clusters. Variational auto-encoders (discussed in *Chapter 7*, *Modeling
    for Tabular Competitions*) can instead deal with non-linear dimensionality reduction
    and offer a more useful representation than PCA; they are more complicated to
    set up and tune, however.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: PCA和t-SNE不是唯一可以帮助你降低数据维度并使其可视化的工具。UMAP ([https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap))通常可以提供更快的低维度解决方案，具有清晰且明显的数据簇。变分自动编码器（在第7章*表格竞赛建模*中讨论）可以处理非线性维度缩减，并提供比PCA更有用的表示；然而，它们设置和调整起来更复杂。
- en: Example implementation
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 示例实现
- en: While you can find examples of adversarial validation in the original article
    by Zygmunt and the Notebook we linked, we have created a fresh example for you,
    based on the Playground competition *Tabular Playground Series – Jan 2021* ([https://www.kaggle.com/c/tabular-playground-series-jan-2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021)).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以在Zygmunt的原始文章和我们所链接的笔记本中找到对抗验证的例子，但我们为你创建了一个全新的例子，这个例子基于Playground竞赛的*Tabular
    Playground Series – Jan 2021* ([https://www.kaggle.com/c/tabular-playground-series-jan-2021](https://www.kaggle.com/c/tabular-playground-series-jan-2021))。
- en: 'You start by importing some Python packages and getting the training and test
    data from the competition:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先导入一些Python包，并从竞赛中获取训练数据和测试数据：
- en: '[PRE3]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Data preparation is short and to the point. Since all features are numeric,
    you won’t need any label encoding, but you do have to fill any missing values
    with a negative number (-1 usually works fine), and drop the target and also any
    identifiers; when the identifier is progressive, the adversarial validation may
    return a high ROC-AUC score:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备简短而直接。由于所有特征都是数值型的，你不需要任何标签编码，但你必须用负数（通常-1就足够了）填充任何缺失值，并删除目标和任何标识符；当标识符是递增的，对抗验证可能会返回一个高的ROC-AUC分数：
- en: '[PRE4]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'At this point, you just need to generate `RandomForestClassifier` predictions
    for your data using the `cross_val_predict` function, which automatically creates
    a cross-validation scheme and stores the predictions on the validation fold:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你只需要使用`cross_val_predict`函数为你数据生成`RandomForestClassifier`预测，该函数自动创建交叉验证方案并将预测存储在验证折上：
- en: '[PRE5]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As a result, you obtain predictions that are unbiased (they are not overfit
    as you did not predict on what you trained) and that can be used for error estimation.
    Please note that `cross_val_predict` won’t fit your instantiated model, so you
    won’t get any information from it, such as what the important features used by
    the model are. If you need such information, you just need to fit it first by
    calling `model.fit(X, y)`.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你获得的预测是无偏的（它们没有过拟合，因为你没有在你训练的数据上进行预测），并且可以用于误差估计。请注意，`cross_val_predict`不会拟合你的实例化模型，所以你不会从它那里得到任何信息，例如模型使用的重要特征是什么。如果你需要此类信息，你只需先通过调用`model.fit(X,
    y)`来拟合它。
- en: 'Finally, you can query the ROC-AUC score for your predictions:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以查询你预测的ROC-AUC分数：
- en: '[PRE6]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: You should obtain a value of around 0.49-0.50 (`cross_val_predict` won’t be
    deterministic unless you use cross-validation with a fixed `random_seed`). This
    means that you cannot easily distinguish training from test data. Hence, they
    come from the same distribution.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该得到大约0.49-0.50的值（除非你使用具有固定`random_seed`的交叉验证，否则`cross_val_predict`不会是确定性的）。这意味着你无法轻易地区分训练数据和测试数据。因此，它们来自相同的分布。
- en: Handling different distributions of training and test data
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理训练数据和测试数据的不同分布
- en: 'ROC-AUC scores of 0.8 or more would alert you that the test set is peculiar
    and quite distinguishable from the training data. In these cases, what can you
    do? You actually have a few strategies at hand:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ROC-AUC分数为0.8或更高会提醒你测试集是特殊的，并且与训练数据相当不同。在这些情况下，你可以采取哪些策略？实际上，你手头有几个策略：
- en: Suppression
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抑制
- en: Training on cases most similar to the test set
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在与测试集最相似的案例上进行训练
- en: Validating by mimicking the test set
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过模拟测试集进行验证
- en: With **suppression**, you remove the variables that most influence the result
    in the adversarial test set until the distributions are the same again. To do
    so, you require an iterative approach. This time, you fit your model to all your
    data, and then you check the importance measures (provided, for instance, by the
    `feature_importances_` method in the Scikit-learn `RandomForest` classifier) and
    the ROC-AUC fit score. At this point, you remove the most important variable for
    the model from your data and run everything again. You repeat this cycle where
    you train, measure the ROC-AUC fit, and drop the most important variable from
    your data until the fitted ROC-AUC score decreases to around 0.5\.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过**抑制**，你移除在对抗测试集中对结果影响最大的变量，直到分布再次相同。为此，你需要一个迭代的方法。这次，你将你的模型拟合到所有数据上，然后检查重要性度量（例如，由Scikit-learn的`RandomForest`分类器的`feature_importances_`方法提供）和ROC-AUC拟合分数。在这个时候，你从数据中移除对模型最重要的变量，然后重新运行一切。你重复这个周期，其中你训练、测量ROC-AUC拟合，并从数据中移除最重要的变量，直到拟合的ROC-AUC分数下降到大约0.5。
- en: The only problem with this method is that you may actually be forced to remove
    the majority of important variables from your data, and any model you then build
    on such variable censored data won’t be able to predict sufficiently correctly
    due to the lack of informative features.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的唯一问题是，你可能实际上被迫从数据中移除大多数重要的变量，并且任何基于这种变量缺失数据的模型将无法由于缺乏信息特征而足够准确地预测。
- en: 'When you **train on the examples most similar to the test set**, you instead
    take a different approach, focusing not on the *variables* but on the *samples*
    you are using for training. In this case, you pick up from the training set only
    the samples that fit the test distribution. Any trained model then suits the testing
    distribution (but it won’t be generalizable to anything else), which should allow
    you to test the best on the competition problem. The limitation of this approach
    is that you are cutting down the size of your dataset and, depending on the number
    of samples that fit the test distribution, you may suffer from a very biased resulting
    model due to the lack of training examples. In our previous example, picking up
    just the adversarial predictions on the training data that exceed a probability
    of 0.5 and summing them results in picking only 1,495 cases (the number is so
    small because the test set is not very different from the training set):'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在与测试集最相似的例子上进行训练时，你采取不同的方法，专注于训练中使用的**变量**而不是**样本**。在这种情况下，你只从训练集中选择符合测试分布的样本。任何训练好的模型都适合测试分布（但它不会推广到其他任何东西），这应该允许你在竞赛问题上有最好的测试。这种方法的局限性在于你正在减少数据集的大小，并且根据符合测试分布的样本数量，你可能会由于缺乏训练示例而得到一个非常偏颇的结果模型。在我们的前一个例子中，只选择训练数据中概率超过0.5的对抗预测并将它们相加，结果只选择了1,495个案例（这个数字如此之小，因为测试集与训练集没有很大不同）：
- en: '[PRE7]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Finally, with the strategy of **validating by mimicking the test set**, you
    keep on training on all the data, but for validation purposes, you pick your examples
    only from the adversarial predictions on the training set that exceed a probability
    of 0.5 (or an even higher threshold such as 0.9).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用**通过模拟测试集进行验证**的策略，你继续在所有数据上训练，但为了验证目的，你只从训练集的对抗预测中选择概率超过0.5（或更高的阈值，如0.9）的例子。
- en: Having a validation set tuned to the test set will allow you to pick all the
    possible hyperparameters and model choices that will favor a better result on
    the leaderboard.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 将验证集调整到测试集将允许你选择所有可能超参数和模型选择，这将有利于在排行榜上获得更好的结果。
- en: 'In our example, we can figure out that `feature_19` and `feature_54` appear
    the most different between the training/test split from the output of the following
    code:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们可以从以下代码的输出中找出`feature_19`和`feature_54`在训练/测试分割中出现的差异最大：
- en: '[PRE8]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To conclude, we have a few more remarks on adversarial validation. First, using
    it will generally help you to perform better in competitions, but not always.
    Kaggle’s Code competitions, and other competitions where you cannot fully access
    the test set, cannot be inspected by adversarial validation. In addition, adversarial
    validation can inform you about the test data as a whole, but it cannot advise
    you on the split between the private and the public test data, which is the cause
    of the most common form of public leaderboard overfitting and consequent shake-up.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，我们对对抗性验证还有一些额外的评论。首先，使用它通常会帮助你更好地在竞赛中表现，但并不总是如此。Kaggle的代码竞赛以及其他你无法完全访问测试集的竞赛，不能通过对抗性验证来检查。此外，对抗性验证可以告诉你关于整个测试数据的信息，但它不能就私有和公共测试数据之间的分割提供建议，这是最常见的公开排行榜过拟合和随之而来的波动的原因。
- en: 'Finally, adversarial validation, though a very specific method devised for
    competitions, has quite a few practical use cases in the real world: how often
    have you picked the wrong test set to validate your models? The method we have
    presented here can enlighten you about whether you are using the test data, and
    any validation data, in your projects properly. Moreover, data changes and models
    in production may be affected by such changes and produce bad predictions if you
    don’t retrain them. This is called **concept drift**, and by using adversarial
    validation, you can immediately understand if you have to retrain new models to
    put into production or if you can leave the previous ones in operation.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，虽然对抗性验证是一种为竞赛特别设计的非常具体的方法，但在现实世界中它有许多实际的应用场景：你有多少次选择了错误的测试集来验证你的模型？我们在这里提出的方法可以让你明白你是否在项目中正确地使用了测试数据和任何验证数据。此外，数据变化和生产中的模型可能会受到这些变化的影响，如果你不重新训练它们，可能会产生不良预测。这被称为**概念漂移**，通过使用对抗性验证，你可以立即了解是否需要重新训练新的模型以投入生产，或者是否可以继续使用之前的模型。
- en: '![](img/Giuliano_Janson.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![Giuliano_Janson](img/Giuliano_Janson.png)'
- en: Giuliano Janson
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 吉尔亚诺·贾森
- en: '[https://www.kaggle.com/adjgiulio](https://www.kaggle.com/adjgiulio)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/adjgiulio](https://www.kaggle.com/adjgiulio)'
- en: Giuliano Janson is a Competitions Grandmaster and senior applied scientist for
    ML and NLP at Zillow Group. He spoke to us about his competition wins, the importance
    of cross-validation, and data leakages, the subject of the upcoming section.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 吉尔亚诺·贾森是一位竞赛大师，同时也是Zillow集团机器学习和自然语言处理的高级应用科学家。他向我们讲述了他的竞赛胜利、交叉验证的重要性以及数据泄露，这是下一节的主题。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*My perfect competition is made up of a) an interesting problem to solve, b)
    a mid-size dataset that is small enough to fit in memory but not too small to
    become an overfitting headache, and c) an opportunity to be creative from a feature
    engineering perspective. The combination of those three dimensions is where I’m
    at my best in competitive ML because I feel I have the means to use rigor and
    creativity without having to worry about engineering constraints.*'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*我理想的竞赛由以下三个要素组成：a) 一个有趣的问题需要解决，b) 一个中等大小的数据集，足够小以适应内存，但又不至于太小而成为过拟合的烦恼，c)
    从特征工程的角度来看，有机会发挥创造力。这三个维度的结合是我擅长竞争机器学习的地方，因为我感觉我有能力在不担心工程约束的情况下使用严谨和创造力。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle竞赛的？这种方法与你在日常工作中所做的工作有何不同？
- en: '*A Kaggle competition is a marathon. Going into a competition, I know I can
    get 90 to 95% of my best final score with a couple of days of work. The rest is
    a slow grind. The only success metric is your score; nothing else matters.*'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle竞赛是一场马拉松。进入竞赛时，我知道我可以通过几天的工作获得我最好最终得分的90%到95%。其余的都是缓慢的磨砺。唯一的成功指标是你的分数；其他什么都不重要。*'
- en: '*My daily work looks more like a series of sprints. Model performance is only
    a small portion of what I need to consider. A go-live date might be just as important,
    or other aspects such as interpretability, scalability, and maintainability could
    tip the scale in a totally different direction. After each sprint, priorities
    are reassessed and the end product might look totally different from what was
    originally envisioned. Also, modeling is a small part of my day. I spend far more
    time talking to people, managing priorities, building use cases, scrubbing data,
    and thinking about everything that it takes to make a prototype model a successful
    production solution.*'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的日常工作更像是一系列冲刺。模型性能只是我需要考虑的一小部分。上线日期可能同样重要，或者其他方面，如可解释性、可扩展性和可维护性可能会完全改变方向。每次冲刺后，优先级都会重新评估，最终产品可能看起来与最初设想的大相径庭。此外，建模只是我一天中的一部分。我花更多的时间与人交谈，管理优先级，构建用例，清洗数据，以及思考使原型模型成为成功的生产解决方案所需的一切。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们你参加的一个特别具有挑战性的比赛，以及你使用了哪些见解来应对这项任务。
- en: '*One of the two competitions I won, the Genentech Cancer competition, was a
    Masters-only competition. The data provided was raw transactional data. There
    was no nice tabular dataset to start from. This is the type of work I love because
    feature engineering is actually one of my favorite parts of ML. Since I had worked
    in healthcare for a decade at the time of the competition, I had business and
    clinical insights on the data, but most of all, I had engineering insights on
    the complexity of correctly handling this type of data and about all the things
    that can go wrong when this type of transactional raw data is not handled carefully.
    That turned out to be key to winning, as one of the initial hypotheses regarding
    a possible source of leakage turned out to be true, and provided a “golden feature”
    that gave the final boost to our model. The insight from the competition is to
    always be extra careful when doing feature engineering or setting up validation
    approaches. Leakage can be very hard to detect and the usual train/validation/test
    approach to model validation will provide no help in identifying leakage in most
    cases, thus putting a model at risk of underperforming in production.*'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '*我赢得的两个比赛中之一，基因泰克癌症比赛，是一个仅限硕士生的比赛。提供的数据是原始交易数据。没有从表格数据集开始。这种类型的工作是我喜欢的，因为特征工程实际上是机器学习我最喜欢的部分之一。由于我在比赛时已经在医疗保健行业工作了十年，我对数据有商业和临床见解，但最重要的是，我对正确处理这种数据复杂性的工程见解，以及当这种交易原始数据处理不当时可能出现的所有问题。这最终证明是获胜的关键，因为关于可能泄漏来源的初步假设之一最终被证明是正确的，并为我们的模型提供了“黄金特征”，从而为我们的模型提供了最后的推动力。从比赛中得到的启示是在进行特征工程或设置验证方法时始终格外小心。泄漏可能很难检测，而通常的模型验证方法在大多数情况下无法帮助识别泄漏，从而将模型置于在生产中表现不佳的风险之中。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进展？如果是的话，是如何帮助的？
- en: '*Kaggle has helped me in two ways. First, it provided a low barrier entry point
    to modern ML, a ton of exposure to cutting-edge modeling techniques, and forced
    me to truly understand the art and science of professional-grade model validation
    techniques. Second, Kaggle provided access to some of the brightest minds in applied
    ML. What I learned teaming up with some of the top Kaggle participants are lessons
    I cherish and try to share with my teammates every day.*'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle以两种方式帮助了我。首先，它提供了一个低门槛的入门点，让我接触到现代机器学习，大量的前沿建模技术，并迫使我真正理解专业级模型验证技术的艺术和科学。其次，Kaggle让我有机会接触到应用机器学习领域的一些最聪明的大脑。我与一些顶级Kaggle参与者合作学到的教训是我珍视的，并且我每天都在努力与我的队友分享这些教训。*'
- en: How have you built up your portfolio thanks to Kaggle?
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何通过Kaggle建立起你的作品集的？
- en: '*My professional career hasn’t been directly impacted much by my Kaggle résumé.
    By that, I mean I haven’t got job offers or interviews as a result of my Kaggle
    standings. I started Kaggle when I was already in a senior data science role,
    albeit with not much of an ML focus. Thanks to what I learned on Kaggle, I was
    able to better advocate a change in my career to move into an ML-focused job.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的职业生涯并没有因为我的Kaggle简历而有太大的影响。我的意思是，我没有因为我的Kaggle排名而得到工作机会或面试。我是在已经担任高级数据科学角色时开始使用Kaggle的，尽管当时对机器学习的关注不多。多亏了在Kaggle上学到的知识，我能够更好地倡导我的职业转变，转向专注于机器学习的职位。*'
- en: '*To this date, many folks I work with enjoy chatting about competitive ML and
    are curious about tips and tricks from my Kaggle experience, but it is also true
    that a large portion of the ML community might not even know what Kaggle is.*'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*截至目前，我工作中很多人喜欢讨论竞争性机器学习，并对我的Kaggle经验中的技巧和窍门表示好奇，但事实也是，机器学习社区中很大一部分人甚至可能都不知道Kaggle是什么。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kagglers通常忽略了什么？你现在知道什么，而你在最初开始时希望知道的呢？
- en: '*The importance of proper cross-validation is easily overlooked by participants
    new to competitive ML. A solid cross-validation framework allows you to measure
    improvement reliably and objectively. And in a competition that might be as long
    as six months, the best models do not usually come from those who have the best
    initial ideas, but from those who are willing to iterate and adjust based on empirical
    feedback from the data. A great validation framework is at the foundation of it
    all.*'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '*对于竞争性机器学习的新手来说，交叉验证的重要性很容易被忽视。一个稳固的交叉验证框架可以让你可靠和客观地衡量改进。在一个可能长达六个月的比赛中，最好的模型通常不是来自那些有最好初始想法的人，而是来自那些愿意根据数据经验反馈进行迭代和调整的人。一个出色的验证框架是这一切的基础。*'
- en: What mistakes have you made in competitions in the past?
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的比赛中，你犯过哪些错误？
- en: '*One of the lessons learned that I always share with people new to ML is to
    “never get over-enamored with overly complex ideas.” When facing a new complex
    problem, it is easy to be tempted to build complex solutions. Complex solutions
    usually require time to develop. But the main issue is that complex solutions
    are often of marginal value, conditional on robust baselines. For example, imagine
    you want to model the outcome of an election and start thinking about a series
    of features to capture complex conditional relationships among observable and
    latent geographic, socio-economic, and temporal features. You could spend weeks
    developing these features, under the assumption that because they are so well
    thought out, they will be impactful.*'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '*我总是与机器学习新入门者分享的一个教训是“永远不要对过于复杂的思想过分迷恋。”面对一个新复杂问题时，很容易被诱惑去构建复杂的解决方案。复杂的解决方案通常需要时间来开发。但主要问题是，复杂的解决方案通常价值不大，基于稳健的基线。例如，想象你想模拟选举结果，并开始思考一系列特征来捕捉可观察和潜在地理、社会经济和时间特征之间的复杂条件关系。你可能会花费数周时间开发这些特征，假设由于它们考虑得非常周到，它们将产生重大影响。*'
- en: '*The mistake is that while often those complex features could be very powerful
    on their own, conditional on a series of simple features and on a model that can
    already build highly optimized, data-driven deep interaction, all of a sudden,
    the complex features we built with time and effort may lead to little to no marginal
    improvement. My advice is to stick to Occam’s razor and try easy things before
    being tempted by more complex approaches.*'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '*错误在于，尽管这些复杂特征本身可能非常强大，但基于一系列简单特征和能够构建高度优化、数据驱动的深度交互的模型，我们花费时间和精力构建的复杂特征可能会突然导致边际改进很小甚至没有。我的建议是坚持奥卡姆剃刀原则，在诱惑更复杂的方法之前先尝试简单的事情。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis or machine learning?
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些特定的工具或库来进行数据分析或机器学习？
- en: '*I’m a pandas and Scikit-learn person. I love how pandas enables easy data
    manipulation and exploration and how I can quickly prototype models using Scikit-learn
    in a matter of minutes. Most of my prototype work is done using these two libraries.
    That said, my final models are often based on XGBoost. For deep learning, I love
    using Keras.*'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '*我是一个pandas和Scikit-learn的用户。我喜欢pandas如何使数据操作和探索变得简单，以及我如何能在几分钟内快速使用Scikit-learn原型化模型。我的大部分原型工作都是使用这两个库完成的。尽管如此，我的最终模型通常基于XGBoost。对于深度学习，我喜欢使用Keras。*'
- en: Handling leakage
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理泄露
- en: A common issue in Kaggle competitions that can affect the outcome of the challenge
    is data leakage. **Data leakage**, often mentioned simply as **leakage** or with
    other fancy names (such as *golden features*), involves information in the training
    phase that won’t be available at prediction time. The presence of such information
    (leakage) will make your model over-perform in training and testing, allowing
    you to rank highly in the competition, but will render unusable or at best suboptimal
    any solution based on it from the sponsor’s point of view.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，一个常见的问题可能会影响挑战的结果，那就是数据泄露。**数据泄露**，通常简单地被称为**泄露**或使用其他花哨的名称（如*黄金特征*），涉及在训练阶段存在但在预测时不可用的信息。这种信息（泄露）的存在会使你的模型在训练和测试中表现过度，让你在比赛中排名很高，但从赞助商的角度来看，任何基于它的解决方案都将变得不可用或至多次优。
- en: We can define leakage as “when information concerning the ground truth is artificially
    and unintentionally introduced within the training feature data, or training metadata”
    as stated by *Michael Kim* ([https://www.kaggle.com/mikeskim](https://www.kaggle.com/mikeskim))
    in his presentation at *Kaggle Days San Francisco* in 2019.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义泄露为“当与真实情况相关的信息被人为且无意地引入训练特征数据或训练元数据中”时，正如*迈克尔·金*（[https://www.kaggle.com/mikeskim](https://www.kaggle.com/mikeskim)）在2019年*Kaggle
    Days San Francisco*的演讲中所说。
- en: Leakage is often found in Kaggle competitions, despite careful checking from
    both the sponsor and the Kaggle team. Such situations are due to the subtle and
    sneaky nature of leakage, which can unexpectedly appear due to the intense searching
    undertaken by Kagglers, who are always looking for any way to score better in
    a competition.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管赞助商和Kaggle团队都进行了仔细的检查，但在Kaggle竞赛中仍然经常发现泄露。这种情况是由于泄露的微妙和隐蔽性质，它可能会由于Kagglers进行的激烈搜索而意外出现，他们总是在寻找任何在比赛中得分更高的方法。
- en: Don’t confuse data leakage with a leaky validation strategy. In a leaky validation
    strategy, the problem is that you have arranged your validation strategy in a
    way that favors better validation scores because some information leaks from the
    training data. It has nothing to do with the competition itself, but it relates
    to how you are handling your validation. It occurs if you run any pre-processing
    modifying your data (normalization, dimensionality reduction, missing value imputation)
    before separating training and validation or test data.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将数据泄露与泄露的验证策略混淆。在泄露的验证策略中，问题在于你安排的验证策略有利于更好的验证分数，因为训练数据中存在一些信息泄露。这与比赛本身无关，但它与你处理验证的方式有关。如果你在将训练数据和验证或测试数据分开之前对数据进行任何预处理（归一化、降维、缺失值插补），就会发生这种情况。
- en: In order to prevent leaky validation, if you are using Scikit-learn to manipulate
    and process your data, you absolutely have to exclude your validation data from
    any fitting operation. Fitting operations tend to create leakage if applied to
    any data you use for validation. The best way to avoid this is to use Scikit-learn
    pipelines ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)),
    which will enclose both your data processing and model together, thereby avoiding
    any risk of inadvertently applying any leaking transformation to your data.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止泄露的验证，如果你使用Scikit-learn来处理和操作你的数据，你绝对必须将你的验证数据排除在任何拟合操作之外。拟合操作倾向于在应用于任何用于验证的数据时创建泄露。避免这种情况的最佳方法是使用Scikit-learn管道（[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)），它将你的数据处理和模型一起封装，从而避免无意中将对数据应用任何泄露转换的风险。
- en: Data leakage instead is therefore something that is not strictly related to
    validation operations, though it affects them deeply. Even though this chapter
    is principally devoted to validation strategies, at this point we consider it
    necessary to discuss data leakage, since this issue can profoundly affect how
    you evaluate your models and their ability to generalize beyond the competition
    test sets.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，数据泄露并不是严格与验证操作相关，尽管它深深地影响着它们。尽管这一章主要致力于验证策略，但在此阶段我们认为讨论数据泄露是必要的，因为这个问题可能会深刻地影响你评估模型及其在比赛测试集之外泛化能力的方式。
- en: 'Generally speaking, leakage can originate at a feature or example level. **Feature
    leakage** is by far the most common. It can be caused by the existence of a proxy
    for the target, or by a feature that is posterior to the target itself. A target
    proxy could be anything derived from processing the label itself or from the test
    split process; for instance, when defining identifiers, specific identifiers (a
    numeration arc, for instance) may be associated with certain target responses,
    making it easier for a model to guess if properly fed with the information processed
    in the right way. A more subtle way in which data processing can cause leakage
    is when the competition organizers have processed the training and test set together
    before splitting it. Historically, leakages in Kaggle competitions have been found
    in:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，泄露可能起源于特征或示例级别。**特征泄露**是最常见的。它可能是由目标代理的存在或由目标本身之后的特征引起的。目标代理可以是来自处理标签本身或来自测试分割过程的任何东西；例如，在定义标识符时，特定的标识符（例如，一个编号弧）可能与某些目标响应相关联，使得模型在正确地提供经过适当处理的信息时更容易猜测。数据处理可能导致泄露的更微妙的方式是，当竞赛组织者在分割之前一起处理训练集和测试集时。历史上，Kaggle竞赛中的泄露通常发现于：
- en: Mishandled data preparation from organizers, especially when they operate on
    a combination of training and test data (for example, in *Loan Default Prediction*
    ([https://www.kaggle.com/c/loan-default-prediction](https://www.kaggle.com/c/loan-default-prediction)),
    organizers initially used features with aggregated historical data that leaked
    future information).
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组织者处理数据准备不当，尤其是在他们操作训练数据和测试数据的组合时（例如，在 *Loan Default Prediction* ([https://www.kaggle.com/c/loan-default-prediction](https://www.kaggle.com/c/loan-default-prediction))
    中，组织者最初使用了包含聚合历史数据的特征，这些特征泄露了未来的信息）。
- en: Row order when it is connected to a time index or to specific data groups (for
    instance, in *Telstra Network Disruptions* ([https://www.kaggle.com/c/telstra-recruiting-network](https://www.kaggle.com/c/telstra-recruiting-network)),
    the order of records in a feature hinted at proxy information, the location, which
    was not present in the data and which was very predictive).
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当与时间索引或特定数据组相关联时，行的顺序（例如，在 *Telstra Network Disruptions* ([https://www.kaggle.com/c/telstra-recruiting-network](https://www.kaggle.com/c/telstra-recruiting-network))
    中，特征中记录的顺序暗示了代理信息，即位置信息，该信息在数据中不存在，但具有很高的预测性）。
- en: Column order when it is connected to a time index (you get hints by using the
    columns as rows).
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当与时间索引相关联时，列的顺序（您可以通过将列用作行来获得提示）。
- en: Feature duplication in consecutive rows because it can hint at examples with
    correlated responses, such as in *Bosch Production Line Performance* (see the
    first-place solution by *Beluga* at [https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434](https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434)).
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连续行中的特征重复，因为这可能会暗示具有相关响应的示例，例如在 *Bosch Production Line Performance*（参见 *Beluga*
    在 [https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434](https://www.kaggle.com/c/bosch-production-line-performance/discussion/25434)
    提出的第一名解决方案）。
- en: 'Image metadata (as in *Two Sigma Connect: Rental Listing Inquiries* ([https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries))).'
  id: totrans-340
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '图像元数据（例如在 *Two Sigma Connect: Rental Listing Inquiries* ([https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries))）中）。'
- en: Hashes or other easily crackable anonymization practices of encodings and identifiers.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编码和标识符的散列或其他容易被破解的匿名化实践。
- en: 'The trouble with posterior information originates from the way we deal with
    information when we do not consider the effects of time and of the sequence of
    cause and effect that spans across time. Since we are looking back at the past,
    we often forget that certain variables that make sense at the present moment do
    not have value in the past. For instance, if you have to calculate a credit score
    for a loan to a new company, knowing that payments of the borrowed money are often
    late is a great indicator of the lower reliability and higher risk represented
    by the debtor, but you cannot know this before you have lent out the money. This
    is also a problem that you will commonly find when analyzing company databases
    in your projects: your query data will represent present situations, not past
    ones. Reconstructing past information can also be a difficult task if you cannot
    specify that you wish to retrieve only the information that was present at a certain
    time. For this reason, great effort has to be spent on finding these leaking features
    and excluding or adjusting them before building any model.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 后验信息的问题源于我们在不考虑时间和跨越时间因果序列的影响时处理信息的方式。由于我们是在回顾过去，我们常常忘记某些在当下有意义的变量在过去并没有价值。例如，如果你必须为新公司计算贷款的信用评分，知道借款的支付通常延迟是债务人较低可靠性和较高风险的强烈指标，但你不能在借钱之前知道这一点。这也是你在项目分析公司数据库时常见的问题：你的查询数据将代表当前情况，而不是过去的情况。如果不能指定你希望检索特定时间点存在的信息，重建过去信息也可能是一项困难的任务。因此，在构建任何模型之前，必须付出巨大的努力来寻找这些泄漏特征，并在构建模型之前排除或调整它们。
- en: Similar problems are also common in Kaggle competitions based on the same kind
    of data (banking or insurance, for instance), though, since much care is put into
    the preparation of the data for the competition, they appear in more subtle ways
    and forms. In general, it is easy to spot these leaking features since they strongly
    correlate with the target, and a domain expert can figure out why (for instance,
    knowing at what stage the data is recorded in the databases). Therefore, in competitions,
    you never find such obvious features, but derivatives of them, often transformed
    or processed features that have slipped away from the control of the sponsor.
    Since the features are often anonymized to preserve the sponsor’s business, they
    end up lurking among the others. This has given rise to a series of hunts for
    the golden/magic features, a search to combine existing features in the dataset
    in order to have the leakage emerge.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 类似的问题在基于相同类型数据（例如银行或保险）的Kaggle竞赛中也普遍存在，尽管由于对竞赛数据的准备投入了大量的精力，它们以更微妙的方式和形式出现。一般来说，这些泄漏特征很容易被发现，因为它们与目标变量有很强的相关性，并且领域专家可以找出原因（例如，知道数据在数据库中记录的阶段）。因此，在竞赛中，你永远不会找到如此明显的特征，而是它们的衍生特征，通常是已经从赞助商控制中溜走的经过转换或处理过的特征。由于特征通常被匿名化以保护赞助商的业务，它们最终隐藏在其他特征中。这引发了一系列寻找金色/魔法特征的比赛，这是一种结合数据集中现有特征的搜索，以便泄漏出现。
- en: 'You can read an enlightening post by *Corey Levison* here: [https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/](https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/).
    It tells the story of how the *Santander Customer Transaction Prediction* competition
    turned into a hunt for magic features for his team.'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里阅读Corey Levison的一篇启发性的文章：[https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/](https://www.linkedin.com/pulse/winning-13th-place-kaggles-magic-competition-corey-levinson/)。这篇文章讲述了Santander客户交易预测竞赛如何变成寻找团队魔法特征的过程。
- en: 'Another good example is provided by *dune_dweller* here: [https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766](https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766).
    By looking at how the data was ordered, dune_dweller found out that the data was
    likely in time order. Putting this information in a new feature increased the
    score.'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个很好的例子是由*dune_dweller*提供的：[https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766](https://www.kaggle.com/c/telstra-recruiting-network/discussion/19239#109766)。通过观察数据的排序方式，dune_dweller发现数据很可能是按时间顺序排列的。将这一信息放入一个新的特征中提高了分数。
- en: The other way in which leakage can occur is by **training example leakage**.
    This happens especially with non-i.i.d. data. This means that some cases correlate
    between themselves because they are from the same period (or from contiguous ones)
    or the same group. If such cases are not all together either in the training or
    test data, but separated between them, there is a high chance that the machine
    learning algorithm will learn how to spot the cases (and derive the predictions)
    rather than using general rules. An often-cited example of such a situation involves
    the team of *Prof. Andrew Ng* (see [https://twitter.com/nizkroberts/status/931121395748270080](https://twitter.com/nizkroberts/status/931121395748270080)).
    In 2017, they wrote a paper using a dataset of 100,000 x-rays from 30,000 patients.
    They used a random split in order to separate training and test data, not realizing
    that the x-rays of the same patient could end up partly in the training set and
    partly in the test set. Practitioners such as Nick Roberts spotted this fact,
    pointing out a possible leakage that could have inflated the performances of the
    model and that led to a substantial revision of the paper itself.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 泄露可能发生的另一种方式是通过**训练示例泄露**。这种情况在非独立同分布数据中尤为常见。这意味着某些案例因为它们来自同一时期（或连续的时期）或同一组而相互关联。如果这些案例在训练或测试数据中不是全部在一起，而是被分开，那么机器学习算法有很大可能会学会如何识别这些案例（并推导出预测），而不是使用一般规则。一个经常引用的此类情况的例子涉及*安德鲁·吴教授*的团队（见[https://twitter.com/nizkroberts/status/931121395748270080](https://twitter.com/nizkroberts/status/931121395748270080))。2017年，他们使用了一个包含30,000名患者100,000张X光片的数据库。他们使用随机分割来分离训练和测试数据，没有意识到同一患者的X光片可能会部分出现在训练集和测试集中。像Nick
    Roberts这样的从业者注意到了这一点，指出可能存在的泄露可能会夸大模型的性能，并导致论文本身的重大修订。
- en: 'What happens when there is a data leakage in a Kaggle competition? Kaggle has
    clear policies about it and will either:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kaggle竞赛中出现数据泄露时会发生什么？Kaggle对此有明确的规定，它将：
- en: Let the competition continue as is (especially if the leakage only has a small
    impact)
  id: totrans-348
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让竞赛照常进行（特别是如果泄露只有轻微的影响）
- en: Remove the leakage from the set and relaunch the competition
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从集合中移除泄露并重新启动竞赛
- en: Generate a new test set that does not have the leakage present
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成一个不包含泄露的新测试集
- en: In particular, Kaggle recommends making any leakage found public, though this
    is not compulsory or sanctioned if it doesn’t happen. However, in our experience,
    if there is any leakage in a competition, it will soon become very apparent and
    the discussion forums will start lighting up with a discussion about magic stuff
    and the like. You will soon know, if you are attentive to what is being said in
    the forums and able to put together all the hints provided by different Kagglers.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，Kaggle建议将发现的任何泄露公之于众，尽管如果没有发生，这并非强制性的或受到制裁。然而，根据我们的经验，如果在竞赛中发生任何泄露，它很快就会变得非常明显，讨论论坛将开始充满关于神奇事物等的讨论。如果你留心论坛中的言论，并能将不同Kagglers提供的所有线索综合起来，你很快就会知道。
- en: 'However, please beware that some players may even use discussions about magic
    features to distract other competitors from serious modeling. For instance, in
    *Santander Customer Transaction Prediction*, there was a famous situation involving
    some Kagglers who fueled in other participants an interest in magic features that
    weren’t actually so magic, directing their efforts in the wrong direction (see
    the discussion here: [https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362)).'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，一些玩家甚至可能利用关于神奇功能的讨论来分散其他竞争对手对严肃建模的注意力。例如，在*Santander Customer Transaction
    Prediction*竞赛中，有一个著名的涉及一些Kagglers的情况，他们激发了其他参与者对并非真正神奇的神奇功能的兴趣，将他们的努力引向了错误的方向（见此讨论：[https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/87057#502362))。
- en: Our suggestion is to carefully read the discussions around leakage and magic
    features that arise in the competition’s forum, and decide whether to pursue the
    research and use any leakage found based on your own interest and motivations
    for participating in the competition.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '我们的建议是仔细阅读竞赛论坛中关于泄露和神奇功能的讨论，并根据自己的兴趣和参与竞赛的动机来决定是否继续研究并使用发现的任何泄露。 '
- en: Not exploiting any leakage may really damage your final rankings, though it
    will surely spoil your learning experience (because leakage is a distortion and
    you cannot claim anything about the models using it). If you are not participating
    in a competition in order to gain a reputation or to later approach the sponsor
    for an opportunity to be hired, it is perfectly fine to use any leakage you come
    across. Otherwise, just ignore it and keep on working hard on your models (who
    knows; maybe Kaggle will reset or fix the competition by the end, rendering the
    leakage ineffective to the great disappointment of the many who used it).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 不利用任何泄露可能会真正损害您的最终排名，尽管这肯定会破坏您的学习体验（因为泄露是一种扭曲，您不能对使用它的模型做出任何声明）。如果您参加比赛不是为了获得声誉或以后向赞助商寻求被雇佣的机会，那么使用您遇到的任何泄露都是完全可以接受的。否则，只需忽略它，继续努力工作在您的模型上（谁知道呢；也许Kaggle会在比赛结束时重置或修复比赛，使泄露变得无效，让许多使用它的人感到非常失望）。
- en: 'Leakages are very different from competition to competition. If you want to
    get an idea of a few real leakages that have happened in Kaggle competitions,
    you can have a look at these three memorable ones:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 泄露在不同的比赛中非常不同。如果您想了解一些在Kaggle比赛中发生过的真实泄露，您可以看看这三个难忘的例子：
- en: '[https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807](https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807)
    from *Predicting Red Hat Business Value* ([https://www.kaggle.com/c/predicting-red-hat-business-value](https://www.kaggle.com/c/predicting-red-hat-business-value))
    where the problem arose because of an imperfect train/test split methodology of
    the competition.'
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807](https://www.kaggle.com/c/predicting-red-hat-business-value/discussion/22807)
    来自 *Predicting Red Hat Business Value* ([https://www.kaggle.com/c/predicting-red-hat-business-value](https://www.kaggle.com/c/predicting-red-hat-business-value))，问题产生是因为比赛训练/测试分割方法不完善。'
- en: '[https://www.kaggle.com/c/talkingdata-mobile-user-demographics /discussion/23403](https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/23403)
    from *TalkingData Mobile User Demographics* ([https://www.kaggle.com/c/talkingdata-mobile-user-demographics](https://www.kaggle.com/c/talkingdata-mobile-user-demographics))
    where a series of problems and non-i.i.d cases affected the correct train/test
    split of the competition.'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/c/talkingdata-mobile-user-demographics /discussion/23403](https://www.kaggle.com/c/talkingdata-mobile-user-demographics/discussion/23403)
    来自 *TalkingData Mobile User Demographics* ([https://www.kaggle.com/c/talkingdata-mobile-user-demographics](https://www.kaggle.com/c/talkingdata-mobile-user-demographics))，其中一系列问题和非独立同分布的情况影响了比赛的正确训练/测试分割。'
- en: '[https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870)
    from *Two Sigma Connect: Rental Listing Inquiries* ([https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries))
    where metadata (the creation time of each folder) did the trick.'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries/discussion/31870)
    来自 *Two Sigma Connect: Rental Listing Inquiries* ([https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries](https://www.kaggle.com/c/two-sigma-connect-rental-listing-inquiries))，其中元数据（每个文件夹的创建时间）起到了作用。'
- en: Summary
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Having arrived at the end of the chapter, we will summarize the advice we have
    discussed along the way so you can organize your validation strategy and reach
    the end of a competition with a few suitable models to submit.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 到达本章的结尾，我们将总结一路上讨论的建议，以便您可以组织您的验证策略，并在比赛中提交几个合适的模型。
- en: In this chapter, we first analyzed the dynamics of the public leaderboard, exploring
    problems such as adaptive overfitting and shake-ups. We then discussed the importance
    of validation in a data science competition, building a reliable system, tuning
    it to the leaderboard, and then keeping track of your efforts.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先分析了公共排行榜的动态，探讨了自适应过拟合和动荡等问题。然后我们讨论了在数据科学竞赛中验证的重要性，构建一个可靠的系统，将其调整到排行榜上，并跟踪您的努力。
- en: Having discussed the various validation strategies, we also saw the best way
    of tuning your hyperparameters and checking your test data or validation partitions
    by using adversarial validation. We concluded by discussing some of the various
    leakages that have been experienced in Kaggle competitions and we provided advice
    about how to deal with them.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论了各种验证策略后，我们还看到了通过使用对抗验证来调整超参数和检查测试数据或验证分区最佳的方式。我们通过讨论在 Kaggle 比赛中遇到的各种泄漏情况，并提供了如何处理它们的建议。
- en: 'Here are our closing suggestions:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的总结建议：
- en: Always spend the first part of the competition building a reliable validation
    scheme, favoring more a *k*-fold over a train-test split, given its probabilistic
    nature and ability to generalize to unseen data.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在比赛中，始终将比赛的第一部分用于构建一个可靠的验证方案，鉴于其概率性质和泛化到未见数据的能力，更倾向于使用 *k*-fold 而不是训练-测试分割。
- en: If your validation scheme is unstable, use more folds or run it multiple times
    with different data partitions. Always check your test set using adversarial validation.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的验证方案不稳定，请使用更多的折数或多次运行它，使用不同的数据分区。始终使用对抗验证检查您的测试集。
- en: Keep track of results based on both your validation scheme and the leaderboard.
    For the exploration of possible optimizations and breakthroughs (such as magic
    features or leakages), trust your validation score more.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据您的验证方案和排行榜跟踪结果。对于探索可能的优化和突破（如神奇特征或泄漏），更信任您的验证分数。
- en: As we explained at the beginning of the chapter, use your validation scores
    when deciding your final submissions to the competition. For your final submissions,
    depending on the situation and whether or not you trust the leaderboard, choose
    among your best local cross-validated models and good-scoring submissions on the
    leaderboard, favoring the first over the second.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正如我们在本章开头所解释的，在决定提交给比赛的最终作品时，使用您的验证分数。对于最终提交，根据情况以及您是否信任排行榜，从您最好的本地交叉验证模型和排行榜上得分良好的提交中选择，优先考虑前者。
- en: 'At this point of our journey, we are ready to discuss how to tackle competitions
    using tabular data, which is numeric or categorical data arranged in matrices
    (with rows representing the examples and columns the features). In the next chapter,
    we discuss the Tabular Playground Series, a monthly contest organized by Kaggle
    using tabular data (organized by *Inversion*: [https://www.kaggle.com/inversion](https://www.kaggle.com/inversion)).'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们旅程的这个阶段，我们准备讨论如何使用表格数据来应对比赛，表格数据是按矩阵排列的数值或分类数据（行代表示例，列代表特征）。在下一章中，我们将讨论 Tabular
    Playground Series，这是 Kaggle 使用表格数据组织的每月比赛（由 *Inversion* 组织：[https://www.kaggle.com/inversion](https://www.kaggle.com/inversion))。
- en: In addition, we will introduce you to some specific techniques to help you shine
    in these competitions, such as feature engineering, target encoding, denoising
    autoencoders, and some neural networks for tabular data, as an alternative to
    the recognized state-of-the-art learning algorithms in tabular data problems (the
    gradient boosting algorithms such as XGBoost, LightGBM, or CatBoost).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将向您介绍一些具体的技术，帮助您在这些比赛中脱颖而出，例如特征工程、目标编码、降噪自编码器，以及一些用于表格数据的神经网络，作为表格数据问题中公认的顶级学习算法（如
    XGBoost、LightGBM 或 CatBoost）的替代方案。
- en: Join our book’s Discord space
  id: totrans-370
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 加入书籍的 Discord 工作空间，参加每月的“问我任何问题”活动，与作者交流：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-373
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code40480600921811704671.png)'
