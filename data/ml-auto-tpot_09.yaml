- en: '*Chapter 6*: Getting Started with Deep Learning: Crash Course in Neural Networks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：深度学习入门：神经网络快速课程'
- en: In this chapter, you'll learn the basics of deep learning and artificial neural
    networks. You'll discover the basic idea and theory behind these topics and how
    to train simple neural network models with Python. The chapter will serve as an
    excellent primer for the upcoming chapters, where the ideas of pipeline optimization
    and neural networks are combined.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习深度学习和人工神经网络的基础知识。你将了解这些主题背后的基本思想和理论，以及如何使用Python训练简单的神经网络模型。本章将为后续章节提供一个极好的入门，在这些章节中，管道优化和神经网络的理念将结合在一起。
- en: We'll cover the essential topics and ideas behind deep learning, why it has
    gained popularity in the last few years, and the cases in which neural networks
    work better than traditional machine learning algorithms. You'll also get hands-on
    experience in coding your own neural networks, both from scratch and through pre-made
    libraries.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖深度学习背后的基本主题和思想，为什么它在过去几年中变得流行，以及神经网络比传统机器学习算法表现更好的案例。你还将获得实际操作经验，从零开始编写自己的神经网络，以及通过预制的库来实现。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of deep learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习概述
- en: Introducing artificial neural networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍人工神经网络
- en: Using neural networks to classify handwritten digits
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络对手写数字进行分类
- en: Comparing neural networks in regression and classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较回归和分类中的神经网络
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: No prior experience with deep learning and neural networks is necessary. You
    should be able to understand the basics from this chapter alone. Previous experience
    is helpful, as deep learning isn't something you can learn in one sitting.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习和神经网络方面没有先前的经验是必要的。你应该能够仅从本章中理解基础知识。先前的经验是有帮助的，因为深度学习不是一次就能学会的。
- en: 'You can download the source code and dataset for this chapter here: [https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在此处下载本章的源代码和数据集：[https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06](https://github.com/PacktPublishing/Machine-Learning-Automation-with-TPOT/tree/main/Chapter06)。
- en: Overview of deep learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习概述
- en: Deep learning is a subfield of machine learning that focuses on neural networks.
    Neural networks aren't that new as a concept – they were introduced back in the
    1940s but didn't gain much in popularity until they started winning data science
    competitions (somewhere around 2010).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子领域，它专注于神经网络。神经网络作为一个概念并不新鲜——它们在20世纪40年代就被引入了，但直到它们开始在数据科学竞赛中获胜（大约在2010年左右），才获得了更多的流行。
- en: Potentially the biggest year for deep learning and AI was 2016, all due to a
    single event. *AlphaGo*, a computer program that plays the board game Go, defeated
    the highest-ranking player in the world. Before this event, Go was considered
    to be a game that computers couldn't master, as there are so many potential board
    configurations.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习和人工智能可能最有成效的一年是2016年，这一切都归功于一个单一事件。*AlphaGo*，一个玩围棋的计算机程序，击败了世界排名第一的选手。在此事件之前，围棋被认为是一种计算机无法掌握的游戏，因为存在如此多的潜在棋盘配置。
- en: As mentioned before, deep learning is based on neural networks. You can think
    of neural networks as **directed acyclic graphs** – a graph consisting of vertices
    (nodes) and edges (connections). The input layer (the first layer, on the far
    left side) takes in the raw data from your datasets, passes it through one or
    multiple hidden layers, and constructs an output.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，深度学习基于神经网络。你可以将神经网络想象成**有向无环图** – 由顶点（节点）和边（连接）组成的图。输入层（最左侧的第一层）接收来自数据集的原始数据，通过一个或多个隐藏层传递，并构建输出。
- en: 'You can see an example architecture of a neural network in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下图表中看到神经网络的一个示例架构：
- en: '![Figure 6.1 – An example neural network architecture'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 一个示例神经网络架构'
- en: '](img/B16954_06_1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_1.jpg)'
- en: Figure 6.1 – An example neural network architecture
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 一个示例神经网络架构
- en: The small black nodes on the far left side represent the input data – data that
    comes directly from your dataset. These values are then connected with the hidden
    layers, with their respective weights and biases. A common way to refer to these
    weights and biases is by using the term **tunable parameters**. We'll address
    this term and show how to calculate them in the next section.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最左侧的小黑点表示输入数据——直接来自你的数据集的数据。然后这些值通过各自的权重和偏差与隐藏层连接。这些权重和偏差的常见称呼是**可调参数**。我们将在下一节中讨论这个术语并展示如何计算它们。
- en: 'Every node of a neural network is called a **neuron**. Let''s take a look at
    the architecture of an individual neuron:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的每个节点都称为**神经元**。让我们看看单个神经元的架构：
- en: '![Figure 6.2 – Individual neuron'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – 单个神经元'
- en: '](img/B16954_06_2.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_2.jpg)'
- en: Figure 6.2 – Individual neuron
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – 单个神经元
- en: The X's correspond to the values either from the input layer or from the previous
    hidden layer. These values are multiplied together (*x1 * w1*, *x2 * w2*) and
    then added together (*x1w1 + x2w2*). After the summation, a bias term is added,
    and finally, everything is passed through an **activation function**. This function
    determines if the neuron will "fire" or not. It's something like an on-off switch,
    in the simplest terms.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: X 值对应于输入层或前一隐藏层的值。这些值相乘（*x1 * w1*，*x2 * w2*）然后相加（*x1w1 + x2w2*）。在求和之后，添加一个偏差项，最后，所有内容都通过一个**激活函数**。这个函数决定神经元是否会“激活”。用最简单的话说，就像一个开关。
- en: 'A brief explanation of weights and biases is shown here:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了权重和偏差的简要说明：
- en: 'Weights:'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重：
- en: a) Multiplied with values from the previous layer
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 与前一层的值相乘
- en: b) Can change the magnitude or entirely flip the value from positive to negative
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 可以改变幅度或完全将值从正变为负
- en: c) In function terms – adjusting the weight changes the slope of the function
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 在函数术语中——调整权重会改变函数的斜率
- en: 'Biases:'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差：
- en: a) Interpreted as an offset of the function
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 解释为函数的偏移
- en: b) An increase in bias leads to an upward shift of a function
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 偏差的增加会导致函数向上移动
- en: c) A decrease in bias leads to a downward shift of a function
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 偏差的减少会导致函数向下移动
- en: 'There are many types of neural network architectures besides artificial neural
    networks, and they are discussed here:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了人工神经网络之外，还有许多类型的神经网络架构，这里将讨论它们：
- en: '**Convolutional neural networks** (**CNNs**) – a type of neural network most
    commonly applied to analyzing images. They are based on the convolution operation
    – an operation between two matrices in which the second one slides (convolves)
    over the first one and computes element-wise multiplication. The goal of this
    operation is to find a sliding matrix (kernel) that can extract the correct features
    from the input image and hence make image classification tasks easy.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）——一种最常见的神经网络类型，用于分析图像。它们基于卷积操作——两个矩阵之间的操作，其中一个矩阵在另一个矩阵上滑动（卷积）并计算逐元素乘积。这个操作的目标是找到一个滑动的矩阵（核）可以从输入图像中提取正确的特征，从而简化图像分类任务。'
- en: '**Recurrent neural networks** (**RNNs**) – a type of neural network most commonly
    used on sequence data. Today these networks are applied in many tasks, such as
    handwriting recognition, speech recognition, machine translation, and time series
    forecasting. The RNN model processes a single element in the sequence at a time.
    After processing, the new updated unit''s state is passed down to the next time
    step. Imagine predicting a single character based on the previous *n* characters;
    that''s the general gist.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）——一种最常见的神经网络类型，用于处理序列数据。如今，这些网络被应用于许多任务，如手写识别、语音识别、机器翻译和时间序列预测。RNN
    模型一次处理序列中的单个元素。处理完毕后，新的更新单元的状态传递到下一个时间步。想象一下根据前 *n* 个字符预测单个字符；这就是一般的概念。'
- en: '**Generative adversarial networks** (**GANs**) – a type of neural network most
    commonly used to create new samples after learning from real data. The GAN architecture
    comprises two separate models – generators and discriminators. The job of a generator
    model is to make fake images and send them to the discriminator. The discriminator
    works like a judge and tries to tell whether an image is fake or not.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）——一种最常见的神经网络类型，用于在从真实数据学习后创建新的样本。GAN 架构由两个独立的模型组成——生成器和判别器。生成器模型的任务是生成假图像并将其发送到判别器。判别器的工作就像一个法官，试图判断图像是否为假。'
- en: '**Autoencoders** – unsupervised learning techniques, designed to learn a low-dimensional
    representation of a high-dimensional dataset. In a way, they work similarly to
    **Principal Component Analysis** (**PCA**).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自编码器** – 无监督学习技术，旨在学习高维数据集的低维表示。从某种意义上说，它们的工作方式类似于**主成分分析**（**PCA**）。'
- en: These four deep learning concepts won't be covered in this book. We'll focus
    only on artificial neural networks, but it's good to know they exist in case you
    want to dive deeper on your own.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个深度学习概念不会在本书中介绍。我们将只关注人工神经网络，但了解它们的存在是有好处的，以防你想要自己深入研究。
- en: The next section looks at artificial neural networks and shows you how to implement
    them in Python, both from scratch and with data science libraries.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨人工神经网络，并展示如何在 Python 中实现它们，无论是从头开始还是使用数据科学库。
- en: Introducing artificial neural networks
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍人工神经网络
- en: The fundamental building block of an artificial neural network is the neuron.
    By itself, a single neuron is useless, but it can have strong predictive power
    when combined into a more complex network.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络的基本构建块是神经元。单独来看，一个神经元是无用的，但当它组合成一个更复杂的网络时，它可以具有强大的预测能力。
- en: If you can't reason why, think about your brain and how it works for a minute.
    Just like artificial neural networks, it is also made from millions of neurons,
    which function only when there's communication between them. Since artificial
    neural networks try to imitate the human brain, they need to somehow replicate
    neurons in the brain and connections between them (weights). This association
    will be made less abstract throughout this section.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不能理解其中的原因，想想你的大脑以及它是如何工作的。就像人工神经网络一样，它也是由数百万个神经元组成的，只有当它们之间有通信时才会工作。由于人工神经网络试图模仿人脑，它们需要以某种方式复制大脑中的神经元及其之间的连接（权重）。这一关联将在本节中逐步变得不那么抽象。
- en: Today, artificial neural networks can be used to tackle any problem that regular
    machine learning algorithms can. In a nutshell, if you can solve a problem with
    linear or logistic regression, you can solve it with neural networks.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 今天，人工神经网络可以用来解决常规机器学习算法可以解决的任何问题。简而言之，如果你可以用线性或逻辑回归解决问题，你也可以用神经网络解决。
- en: Before we can explore the complexity and inner workings of an entire network,
    we have to start simple – with the theory of a single neuron.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够探索整个网络的复杂性和内部工作原理之前，我们必须从简单开始——从单个神经元的理论开始。
- en: Theory of a single neuron
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单个神经元的理论
- en: 'Modeling a single neuron is easy with Python. For example, let''s say a neuron
    receives values from five other neurons (inputs, or X''s). Let''s examine this
    behavior visually before implementing it in code. The following diagram shows
    how a single neuron looks when receiving values from five neurons in the previous
    layers (we''re modeling the neuron on the right):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Python 模拟单个神经元很容易。例如，假设一个神经元从五个其他神经元（输入，或 X）接收值。在将其实现为代码之前，我们先从视觉上考察这种行为。以下图表显示了单个神经元在接收来自前一层五个神经元的值时的样子（我们正在模拟右侧的神经元）：
- en: '![Figure 6.3 – Modeling a single neuron'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.3 – 模拟单个神经元'
- en: '](img/B16954_06_3.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_3.jpg)'
- en: Figure 6.3 – Modeling a single neuron
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 – 模拟单个神经元
- en: The X's represent input features, either from the raw data or from the previous
    hidden layer. Each input feature has a weight assigned to it, denoted with W's.
    Corresponding input values and weights are multiplied and summed, and then the
    bias term (b) is added on top of the result.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: X 代表输入特征，这些特征可能来自原始数据或前一个隐藏层。每个输入特征都分配有一个权重，用 W 表示。相应的输入值和权重相乘并求和，然后在结果上加上偏置项（b）。
- en: 'The formula for calculating the output value of our neuron is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 计算我们神经元输出值的公式如下：
- en: '![](img/Formula_06_001.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_001.jpg)'
- en: 'Let''s work with concrete values to get this concept a bit clearer. The following
    diagram looks identical to Figure 6.3, but has actual numbers instead of variables:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用具体的数值来使这个概念更清晰。以下图表看起来与图 6.3 相同，但用实际的数字代替了变量：
- en: '![Figure 6.4 – Neuron value calculation'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.4 – 神经元值计算'
- en: '](img/B16954_06_4.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_4.jpg)'
- en: Figure 6.4 – Neuron value calculation
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 – 神经元值计算
- en: 'We can plug the values directly into the preceding formula to calculate the
    value:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接将这些值代入前面的公式来计算值：
- en: '![](img/Formula_06_002.jpg)![](img/Formula_06_003.jpg)![](img/Formula_06_004.jpg)![](img/Formula_06_005.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_06_002.jpg)![](img/Formula_06_003.jpg)![](img/Formula_06_004.jpg)![](img/Formula_06_005.jpg)'
- en: In reality, single neurons get their value from potentially thousands of neurons
    in the previous layers, so calculating values manually and expressing visually
    isn't practical.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，单个神经元可以从前一层可能成千上万的神经元中获得其值，因此手动计算值并直观表达是不切实际的。
- en: Even if you decide to do so, that's only a single forward pass. Neural networks
    learn during the backward pass, which is much more complicated to calculate by
    hand.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你决定这样做，那也只是一个前向传递。神经网络在反向传递中学习，手动计算这个传递要复杂得多。
- en: Coding a single neuron
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码单个神经元
- en: 'Next, let''s see how you can semi-automate neuron value calculation with Python:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何使用Python半自动化地计算神经元值：
- en: 'To start, let''s declare input values, their respective weights, and a value
    for the bias term. The first two are lists, and the bias is just a number:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们声明输入值、它们各自的权重以及偏置项的值。前两者是列表，而偏置项只是一个数字：
- en: '[PRE0]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: That's all you need to calculate the output value. Let's examine what your options
    are next.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这就是计算输出值所需的所有内容。接下来，让我们看看你的选择有哪些。
- en: There are three simple methods for calculating neuron output values. The first
    one is the most manual, and that is to explicitly multiply corresponding inputs
    and weights, adding them together with the bias.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神经元输出值有三种简单的方法。第一种是最手动的方法，即明确地乘以相应的输入和权重，然后将它们与偏置项相加。
- en: 'Here''s a Python implementation:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是一个Python实现：
- en: '[PRE1]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see a value of `11.6` printed out after executing this code. To be
    more precise, the value should be `11.600000000000001`, but don't worry about
    this calculation error.
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行此代码后，你应该会看到一个`11.6`的值打印出来。更精确地说，这个值应该是`11.600000000000001`，但不用担心这个计算误差。
- en: 'The next method is a bit more scalable, and it boils down to iterating through
    inputs and weights at the same time and incrementing the variable declared earlier
    for the output. After the loop finishes, the bias term is added. Here''s how to
    implement this calculation method:'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下一种方法稍微更可扩展一些，它归结为同时迭代输入和权重，并递增之前声明的输出变量。循环结束后，添加偏置项。下面是如何实现这种计算方法的：
- en: '[PRE2]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output is still identical, but you can immediately see how much more scalable
    this option is. Just imagine using the first option if the previous network layer
    had 1,000 neurons – it's not even remotely convenient.
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出仍然是相同的，但你可以立即看到这个选项的可扩展性有多高。想象一下，如果前一个网络层有1,000个神经元，使用第一个选项——这甚至都不方便。
- en: 'The third and preferred method is to use a scientific computing library, such
    as NumPy. With it, you can calculate the vector dot product and add the bias term.
    Here''s how:'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第三种也是首选的方法是使用科学计算库，例如NumPy。有了它，你可以计算向量点积并添加偏置项。下面是如何操作的：
- en: '[PRE3]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This option is the fastest, both to write and to execute, so it's the preferred
    one.
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个选项既易于编写也易于执行，所以是首选的。
- en: You now know how to code a single neuron – but neural networks employ layers
    of neurons. You'll learn more about layers next.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道如何编码单个神经元——但神经网络使用的是神经元层。你将在下一节中了解更多关于层的内容。
- en: Theory of a single layer
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单层的理论
- en: To make things simpler, think of layers as vectors or simple groups. Layers
    aren't some complicated or abstract data structure. In code terms, you can think
    of them as lists. They contain a number of neurons.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，可以将层想象成向量或简单的组。层并不是一些复杂或抽象的数据结构。在代码术语中，你可以将它们视为列表。它们包含一定数量的神经元。
- en: Coding a single layer of neurons is quite similar to coding a single neuron.
    We still have the same inputs, as they are coming either from a previous hidden
    layer or an input layer. What changes are weights and biases. In code terms, weights
    aren't treated as a list anymore, but as a list of lists instead (or a matrix).
    Similarly, bias is now a list instead of a scalar value.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 编码单个神经层与编码单个神经元相当相似。我们仍然有相同的输入，因为它们要么来自前一个隐藏层，要么来自输入层。变化的是权重和偏置。在代码术语中，权重不再被视为列表，而是列表的列表（或矩阵）。同样，偏置现在是一个列表而不是标量值。
- en: 'Put simply, your matrix of weights will have as many rows as there are neurons
    in the new layer and as many columns as there are neurons in the previous layer.
    Let''s take a look at a sample diagram to make this concept a bit less abstract:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，你的权重矩阵将会有与新层中神经元数量一样多的行，以及与前一层的神经元数量一样多的列。让我们通过一个示例图来使这个概念更加具体化：
- en: '![Figure 6.5 – Layer of neurons'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – 神经层'
- en: '](img/B16954_06_5.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_5.jpg)'
- en: Figure 6.5 – Layer of neurons
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 神经层
- en: 'Weight values deliberately weren''t placed on the previous diagram, as it would
    look messy. To implement this layer in code, you''ll need to have the following
    structures:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 权重值故意没有放在之前的图中，因为它看起来会很杂乱。要在代码中实现这一层，你需要以下结构：
- en: Vector of inputs (1 row, 5 columns)
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入向量（1行，5列）
- en: Matrix of weights (2 rows, 5 columns)
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重矩阵（2行，5列）
- en: Vector of biases (1 row, 2 columns)
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置向量（1行，2列）
- en: A matrix multiplication rule from linear algebra states that two matrices need
    to be of shapes (m, n) and (n, p) in order to produce an (m, p) matrix after multiplication.
    Bearing that in mind, you could easily perform matrix multiplication by transposing
    the matrix of weights.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 线性代数中的一个矩阵乘法规则指出，两个矩阵需要具有形状（m, n）和（n, p），以便在乘法后产生一个（m, p）矩阵。考虑到这一点，你可以通过转置权重矩阵轻松地进行矩阵乘法。
- en: 'Mathematically, here''s the formula you can use to calculate the values of
    the output layer:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，这是你可以用来计算输出层值的公式：
- en: '![](img/Formula_06_006.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![公式 06_006.jpg](img/Formula_06_006.jpg)'
- en: 'Here, the following applies:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，以下适用：
- en: '![](img/Formula_06_007.png) is the vector of inputs.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式 06_007.png](img/Formula_06_007.png)是输入向量。'
- en: '![](img/Formula_06_008.png) is the matrix of weights.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式 06_008.png](img/Formula_06_008.png)是权重矩阵。'
- en: '![](img/Formula_06_009.png) is the vector of biases.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![公式 06_009.png](img/Formula_06_009.png)是偏置向量。'
- en: 'Let''s declare values for all of these and see how to calculate the values
    for an output layer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为所有这些声明值，并看看如何计算输出层的值：
- en: '![](img/Formula_06_010.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![公式 06_010](img/Formula_06_010.jpg)'
- en: 'The previously mentioned formula can now be used to calculate the values of
    the output layer:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 之前提到的公式现在可以用来计算输出层的值：
- en: '![](img/Formula_06_011.jpg)![](img/Formula_06_012.jpg)![](img/Formula_06_013.jpg)![](img/Formula_06_014.jpg)![](img/Formula_06_015.jpg)![](img/Formula_06_016.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![公式 06_011](img/Formula_06_011.jpg)![公式 06_012](img/Formula_06_012.jpg)![公式
    06_013](img/Formula_06_013.jpg)![公式 06_014](img/Formula_06_014.jpg)![公式 06_015](img/Formula_06_015.jpg)![公式
    06_016](img/Formula_06_016.jpg)'
- en: And that's essentially how you can calculate outputs for an entire layer. The
    calculations will grow in size for actual neural networks, as there are thousands
    of neurons per layer, but the logic behind the math is identical.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上就是你可以计算整个层的输出的方法。对于实际的神经网络，计算量会增长，因为每一层有数千个神经元，但数学背后的逻辑是相同的。
- en: You can see how tedious it is to calculate layer outputs manually. You'll learn
    how to calculate the values in Python next.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到手动计算层输出是多么繁琐。你将在下一节中学习如何在Python中计算这些值。
- en: Coding a single layer
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编码单层
- en: Let's now examine three ways in which you could calculate the output values
    for a single layer. As with single neurons, we'll start with the manual approach
    and finish with a NumPy one-liner.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨三种计算单层输出值的方法。与单个神经元一样，我们将从手动方法开始，并以NumPy的一行代码结束。
- en: 'You''ll have to declare values for inputs, weights, and biases first, so here''s
    how to do that:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须首先声明输入、权重和偏置的值，所以下面是如何做的：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let''s proceed with calculating the values of the output layer:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续计算输出层的值：
- en: Let's start with the manual approach. No, we won't do the same procedure as
    with neurons. You could, of course, but it would look too messy and impractical.
    Instead, we'll immediately use the `zip()` function to iterate over the `weights`
    matrix and `biases` array and calculate the value of a single output neuron.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从手动方法开始。不，我们不会像处理神经元那样进行相同的程序。当然，你可以这样做，但看起来会太杂乱且不实用。相反，我们将立即使用`zip()`函数遍历`weights`矩阵和`biases`数组，并计算单个输出神经元的值。
- en: This procedure is repeated for however many neurons there are, and each output
    neuron is appended to a list that represents the output layer.
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个过程会重复进行，直到有那么多神经元，每个输出神经元都会被添加到一个表示输出层的列表中。
- en: 'Here''s the entire code snippet:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是整个代码片段：
- en: '[PRE5]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The result is a list with the value `[11.6, 6.8]`, which are the same results
    we got from the manual calculation earlier.
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果是一个包含值`[11.6, 6.8]`的列表，这与我们之前手动计算得到的结果相同。
- en: While this approach works, it's still not optimal. Let's see how to improve
    next.
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然这种方法可行，但仍然不是最优的。让我们看看如何改进。
- en: You'll now calculate the values of the output layer by taking the vector dot
    product between input values and every row of the `weights` matrix. The bias term
    will be added after this operation is completed.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在将通过将输入值与`weights`矩阵的每一行进行向量点积来计算输出层的值。在完成此操作后，将添加偏置项。
- en: 'Let''s see how it works in action:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让我们看看它是如何实际工作的：
- en: '[PRE6]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The layer values are still identical – `[11.6, 6.8]`, and this approach is a
    bit more scalable than the previous one. It can still be improved upon. Let's
    see how next.
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 层的值仍然是相同的——`[11.6, 6.8]`，这种方法比之前的方法稍微可扩展一些。它还可以进一步改进。让我们看看下一步如何操作。
- en: 'You can perform a matrix multiplication between inputs and transposed weights
    and add the corresponding biases with a single line of Python code. Here''s how:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用一行Python代码在输入和转置权重之间执行矩阵乘法，并添加相应的偏差。下面是如何操作的：
- en: '[PRE7]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: That's the recommended way if, for some reason, you want to calculate outputs
    manually. NumPy handles it completely, so it's the fastest one at the same time.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果出于某种原因，你想手动计算输出，这是推荐的方法。NumPy可以完全处理它，因此它也是最快的。
- en: You now know how to calculate output values both for a single neuron and for
    a single layer of a neural network. So far, we haven't covered a crucial idea
    in neural networks that decides whether a neuron will "fire" or "activate" or
    not. These are called activation functions, and we'll cover them next.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经知道了如何计算单个神经元和神经网络单层的输出值。到目前为止，我们还没有涵盖神经网络中的一个关键概念，它决定了神经元是否会“触发”或“激活”。这些被称为激活函数，我们将在下一节中介绍。
- en: Activation functions
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: Activation functions are essential for the output of neural networks, and hence
    to the output of the deep learning model. They are nothing but mathematical equations,
    and relatively simple ones to be precise. Activation functions are those that
    determine whether the neuron should be "activated" or not.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数对于神经网络输出，以及深度学习模型的输出至关重要。它们不过是数学方程，而且相对简单。激活函数是决定神经元是否“激活”的函数。
- en: Another way to think about the activation function is as a sort of gate that
    stands between the input coming into the current neuron and its output, which
    goes to the next layer. Activation function can be as simple as a step function
    (turns neurons on or off), or a bit more complicated and non-linear. It's the
    non-linear functions that prove useful in learning complex data and providing
    accurate predictions.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考激活函数的方式是将其视为一种门控，它位于当前神经元接收到的输入和其输出（传递到下一层）之间。激活函数可以像阶跃函数（开启或关闭神经元）那样简单，或者稍微复杂一些且非线性。在学习和提供准确预测的复杂数据中，非线性函数非常有用。
- en: We'll go over a couple of the most common activation functions next.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍一些最常用的激活函数。
- en: Step function
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 阶跃函数
- en: The step function is based on a threshold. If the value coming in is above the
    threshold, the neuron is activated. That's why we can say the step function serves
    as an on-off switch – there are no values in between.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 阶跃函数基于一个阈值。如果输入的值高于阈值，则神经元被激活。这就是为什么我们可以说阶跃函数充当开/关开关——中间没有值。
- en: 'You can easily use Python and NumPy to declare and visualize a basic step function.
    The procedure is shown here:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Python和NumPy轻松地声明和可视化一个基本的阶跃函数。过程如下：
- en: To start, you'll have to define a step function. The typical threshold value
    is 0, so the neuron will activate if and only if the value passed in to the function
    is greater than 0 (the input value being the sum of the previous inputs multiplied
    by the weights and added bias).
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你必须定义一个阶跃函数。典型的阈值是0，因此只有当传递给函数的值大于0（输入值是前一个输入的总和乘以权重并加上偏差）时，神经元才会激活。
- en: 'This kind of logic is trivial to implement in Python:'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这种逻辑在Python中实现起来非常简单：
- en: '[PRE8]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply `step_function()` to this list. Here''s an example:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以声明一个值列表，这些值将作为该函数的输入，然后对列表应用`step_function()`。以下是一个示例：
- en: '[PRE9]'
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你只需两行代码就可以使用Matplotlib库可视化该函数：
- en: '[PRE10]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can see how the function works visually in the following diagram:'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在以下图表中直观地看到函数的工作方式：
- en: '![Figure 6.6 – Step activation function'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – 阶跃激活函数'
- en: '](img/B16954_06_6.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_6.jpg)'
- en: Figure 6.6 – Step activation function
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 阶跃激活函数
- en: The biggest problem of the step function is that it doesn't allow multiple outputs
    – only two. We'll dive into a set of non-linear functions next, and you'll see
    what makes them different.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 阶跃函数的最大问题是它不允许有多个输出——只有两个。接下来，我们将深入研究一系列非线性函数，你将看到它们的不同之处。
- en: Sigmoid function
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: The sigmoid activation function is frequently referred to as the logistic function.
    It is a very popular function in the realm of neural networks and deep learning.
    It essentially transforms the input into a value between 0 and 1.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数通常被称为逻辑函数。它在神经网络和深度学习的领域中非常受欢迎。它本质上将输入转换为一个介于0和1之间的值。
- en: You'll see how the function works later, and you'll immediately notice an advantage
    over the step function – the gradient is smooth, so there are no jumps in the
    output values. For example, you wouldn't get a jump from 0 to 1 if the value changed
    slightly (for example, from -0.000001 to 0.0000001).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在后面看到函数的工作方式，你将立即注意到它相对于阶梯函数的优势 – 梯度是平滑的，所以输出值中没有跳跃。例如，如果值略有变化（例如，从-0.000001到0.0000001），你不会从0跳到1。
- en: The sigmoid function does suffer from a common problem in deep learning – **vanishing
    gradient**. It is a problem that often occurs during backpropagation (a process
    of learning in neural networks, way beyond this chapter's scope). Put simply,
    the gradient "vanishes" during the backward pass, making it impossible for the
    network to learn (tweak weights and biases), as the suggested tweaks are too close
    to zero.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid函数在深度学习中确实存在一个常见问题 – **梯度消失**。这是一个在反向传播（神经网络学习过程中的一个过程，远超本章范围）中经常出现的问题。简单来说，梯度在反向传递过程中“消失”，使得网络无法学习（调整权重和偏差），因为建议的调整太接近于零。
- en: 'You can use Python and NumPy to easily declare and visualize the sigmoid function.
    The procedure is shown here:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用Python和NumPy轻松声明和可视化Sigmoid函数。过程如下：
- en: 'To start, you''ll have to define the sigmoid function. Its formula is pretty
    well established: *(1 / (1 + exp(-x)))*, where *x* is the input value.'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要定义Sigmoid函数。其公式已经相当成熟：*(1 / (1 + exp(-x)))*，其中*x*是输入值。
- en: 'Here''s how to implement this formula in Python:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是如何在Python中实现此公式的示例：
- en: '[PRE11]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply `sigmoid_function()` to this list. Here''s an example:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以声明一个值列表，该列表将作为此函数的输入，然后应用`sigmoid_function()`到这个列表上。以下是一个示例：
- en: '[PRE12]'
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你只需两行代码就可以使用Matplotlib库可视化该函数：
- en: '[PRE13]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You can see how the function works visually in the following diagram:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在以下图表中看到函数的视觉工作方式：
- en: '![Figure 6.7 – Sigmoid activation function'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.7 – Sigmoid激活函数'
- en: '](img/B16954_06_7.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B16954_06_7.jpg](img/B16954_06_7.jpg)'
- en: Figure 6.7 – Sigmoid activation function
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – Sigmoid激活函数
- en: One big disadvantage is that the values returned by the sigmoid function are
    not centered around zero. This is a problem because modeling inputs that are highly
    negative or highly positive gets harder. The hyperbolic tangent function fixes
    this problem.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很大的缺点是sigmoid函数返回的值不是围绕零中心分布的。这是一个问题，因为对高度负值或高度正值输入的建模变得更加困难。双曲正切函数解决了这个问题。
- en: Hyperbolic tangent function
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 双曲正切函数
- en: Hyperbolic tangent function (or TanH) is closely related to the sigmoid function.
    It's also a type of activation function that suffers from the vanishing gradient
    issue, but its outputs are centered around zero – as the function ranges from
    -1 to +1\.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 双曲正切函数（或TanH）与Sigmoid函数密切相关。它也是一种激活函数，也受到梯度消失问题的影响，但其输出值围绕零中心分布 – 函数的范围从-1到+1。
- en: 'This makes it much easier to model inputs that are highly negative or highly
    positive. You can use Python and NumPy to easily declare and visualize the hyperbolic
    tangent function. The procedure is shown here:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得对高度负值或高度正值输入的建模变得容易得多。你可以使用Python和NumPy轻松声明和可视化双曲正切函数。过程如下：
- en: To start, you'll have to define the hyperbolic tangent function. You can use
    the `tanh()` function from NumPy for the implementation.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要定义双曲正切函数。你可以使用NumPy的`tanh()`函数来实现。
- en: 'Here''s how to implement it in Python:'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是如何在Python中实现它的示例：
- en: '[PRE14]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply the `tanh_function()` to this list. Here''s an example:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你可以声明一个值列表，该列表将作为此函数的输入，然后应用`tanh_function()`到这个列表上。以下是一个示例：
- en: '[PRE15]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你只需两行代码就可以使用Matplotlib库可视化该函数：
- en: '[PRE16]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'You can see how the function works visually in the following diagram:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在以下图表中看到函数的视觉工作方式：
- en: '![Figure 6.8 – Hyperbolic tangent activation function'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 6.8 – 双曲正切激活函数'
- en: '](img/B16954_06_8.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16954_06_8.jpg](img/B16954_06_8.jpg)'
- en: Figure 6.8 – Hyperbolic tangent activation function
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 双曲正切激活函数
- en: To train and optimize neural networks efficiently, you need an activation function
    that acts as a linear function but is non-linear in nature, allowing the network
    to learn the complex relationships in the data. That's where the last activation
    function in this section comes in.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地训练和优化神经网络，您需要一个充当线性函数但本质上是非线性的激活函数，以便网络能够学习数据中的复杂关系。这就是本节最后一个激活函数的作用所在。
- en: Rectified linear unit function
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 矩形线性单元函数
- en: The rectified linear unit (or ReLU) function is an activation function you can
    see in most modern-day deep learning architectures. Put simply, it returns the
    larger of two values between 0 and x, where x is the input value.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 矩形线性单元（或ReLU）函数是您在大多数现代深度学习架构中都能看到的激活函数。简单来说，它返回0和x之间的两个值中的较大值，其中x是输入值。
- en: 'ReLU is one of the most computationally efficient functions, and it allows
    the relatively quick finding of the convergence point. You''ll see how to implement
    it in Python next:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU是最具计算效率的函数之一，它允许相对快速地找到收敛点。您将在下一节中看到如何在Python中实现它：
- en: To start, you'll have to define the ReLU function. This can be done entirely
    from scratch or with NumPy, as you only have to find the larger values of the
    two (0 and x).
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您必须定义ReLU函数。这可以从头开始完成，也可以使用NumPy完成，因为您只需要找到两个值（0和x）中的较大值。
- en: 'Here''s how to implement ReLU in Python:'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是Python中实现ReLU的方法：
- en: '[PRE17]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can now declare a list of values that will serve as an input to this function,
    and then apply `relu_function()` to this list. Here''s an example:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在可以声明一个值列表，该列表将作为此函数的输入，然后应用`relu_function()`到此列表。以下是一个示例：
- en: '[PRE18]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, you can visualize the function with the help of the Matplotlib library
    in just two lines of code:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以使用Matplotlib库仅用两行代码来可视化该函数：
- en: '[PRE19]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can see how the function works visually in the following diagram:'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在以下图表中直观地看到该函数的工作原理：
- en: '![Figure 6.9 – ReLU activation function'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.9 – ReLU激活函数'
- en: '](img/B16954_06_9.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B16954_06_9.jpg](img/B16954_06_9.jpg)'
- en: Figure 6.9 – ReLU activation function
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – ReLU激活函数
- en: And that's ReLU in a nutshell. You can use the default version or any of the
    variations (for example, leaky ReLU or parametric ReLU), depending on the use
    case.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是ReLU的概述。您可以使用默认版本或任何变体（例如，泄漏ReLU或参数ReLU），具体取决于用例。
- en: You now know enough of the theory to code a basic neural network with Python.
    We haven't covered all of the theoretical topics, so terms such as loss, gradient
    descent, backpropagation, and others may still feel abstract. We'll try to demystify
    them in the hands-on example that's coming up next.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经了解了足够的理论，可以使用Python编写一个基本的神经网络。我们还没有涵盖所有的理论主题，所以像损失、梯度下降、反向传播等术语可能仍然感觉抽象。我们将在接下来的动手示例中尝试解开这些谜团。
- en: Using neural networks to classify handwritten digits
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络对手写数字进行分类
- en: The "hello world" of deep learning is training a model that can classify handwritten
    digits. That's just what you'll do in this section. It will only require a couple
    of lines of code to implement with the TensorFlow library.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的“hello world”是训练一个能够对手写数字进行分类的模型。这正是本节要做的。使用TensorFlow库实现它只需要几行代码。
- en: 'Before you can proceed, you''ll have to install TensorFlow. The process is
    a bit different depending on whether you''re on Windows, macOS, or Linux, and
    whether you have a CUDA-compatible GPU or not. You can refer to the official installation
    instructions: [https://www.tensorflow.org/install](https://www.tensorflow.org/install).
    The rest of this section assumes you have TensorFlow 2.x installed. Here are the
    steps to follow:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在您继续之前，您必须安装TensorFlow。根据您是在Windows、macOS还是Linux上，以及您是否有CUDA兼容的GPU，安装过程会有所不同。您可以参考官方安装说明：[https://www.tensorflow.org/install](https://www.tensorflow.org/install)。本节其余部分假设您已安装TensorFlow
    2.x。以下是需要遵循的步骤：
- en: To start, you'll have to import the TensorFlow library along with some additional
    modules. The `datasets` module enables you to download data straight from the
    notebook. The `layers` and `models` modules will be used later to design the architecture
    of the neural network.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您必须导入TensorFlow库以及一些额外的模块。`datasets`模块使您能够直接从笔记本中下载数据。`layers`和`models`模块将在以后用于设计神经网络的架构。
- en: 'Here''s the code snippet for the imports:'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是导入代码片段：
- en: '[PRE20]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: You can now proceed with data gathering and preparation. A call to `datasets.mnist.load_data()`
    will download train and test images alongside the train and test labels. The images
    are grayscale and 28x28 pixels in size. This means you'll have a bunch of 28x28
    matrices with values ranging from 0 (black) to 255 (white).
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您可以继续进行数据收集和准备。调用`datasets.mnist.load_data()`将下载训练和测试图像以及相应的训练和测试标签。这些图像是灰度的，大小为28x28像素。这意味着您将有一系列28x28的矩阵，其值范围从0（黑色）到255（白色）。
- en: 'You can then further prepare the dataset by rescaling the images – dividing
    the values by 255 to bring everything into a zero-to-one range:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，您可以通过重新缩放图像来进一步准备数据集 – 将值除以255，将所有内容转换为0到1的范围：
- en: '[PRE21]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Here''s what you should see in your notebook:'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在您的笔记本中应该看到以下内容：
- en: '![Figure 6.10 – Downloading the MNIST dataset'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.10 – 下载MNIST数据集'
- en: '](img/B16954_06_10.jpg)'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16954_06_10.jpg)'
- en: Figure 6.10 – Downloading the MNIST dataset
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.10 – 下载MNIST数据集
- en: Furthermore, you can inspect the matrix values for one of the images to see
    if you can spot the pattern inside.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您还可以检查图像中的一个矩阵值，看看您是否能在其中找到模式。
- en: 'The following line of code makes it easy to inspect matrices – it prints them
    and rounds all floating-point numbers to a single decimal point:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码行使检查矩阵变得容易 – 它打印它们并将所有浮点数四舍五入到一个小数点：
- en: '[PRE22]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下截图所示：
- en: '![Figure 6.11 – Inspecting a single image matrix'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.11 – 检查单个图像矩阵'
- en: '](img/B16954_06_11.jpg)'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16954_06_11.jpg)'
- en: Figure 6.11 – Inspecting a single image matrix
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.11 – 检查单个图像矩阵
- en: Do you notice how easy it is to spot a 5 in the image? You can execute `train_labels[0]`
    to verify.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您是否注意到在图像中很容易找到数字5？您可以通过执行`train_labels[0]`来验证。
- en: You can continue with laying out the neural network architecture next. As mentioned
    earlier, the input images are 28x28 pixels in size. Artificial neural networks
    can't process a matrix directly, so you'll have to convert this matrix to a vector.
    This process is known as `layers.Dense()` to construct a layer.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以继续布局神经网络架构。如前所述，输入图像的大小为28x28像素。人工神经网络不能直接处理矩阵，因此您必须将这个矩阵转换为向量。这个过程被称为`layers.Dense()`来构建一个层。
- en: This hidden layer will also need an activation function, so you can use ReLU.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个隐藏层也需要一个激活函数，因此可以使用ReLU。
- en: Finally, you can add the final (output) layer, which needs to have as many neurons
    as there are distinct classes – 10 in this case.
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后，您可以添加最终的（输出）层，该层需要与不同类别的数量一样多的神经元 – 在这个例子中是10个。
- en: 'Here''s the entire code snippet for the network architecture:'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是网络架构的整个代码片段：
- en: '[PRE23]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The `models.Sequential` function allows you to stack layers one after the other,
    and, well, to make a network out of the individual layers.
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`models.Sequential`函数允许您将层一个接一个地堆叠起来，并且，嗯，从单个层中构建一个网络。'
- en: 'You can view the architecture of your model by calling the `summary()` method
    on it:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以通过在模型上调用`summary()`方法来查看模型的架构：
- en: '[PRE24]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下截图所示：
- en: '![Figure 6.12 – Neural network architecture'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.12 – 神经网络架构'
- en: '](img/B16954_06_12.jpg)'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16954_06_12.jpg)'
- en: Figure 6.12 – Neural network architecture
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.12 – 神经网络架构
- en: There's still one thing you need to do before model training, and that is to
    compile the model. During the compilation, you'll have to specify values for the
    optimizer, loss, and the optimization metrics.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在模型训练之前，您还需要做一件事，那就是编译模型。在编译过程中，您需要指定优化器、损失函数和优化指标的具体值。
- en: 'These haven''t been covered in this chapter, but a brief explanation of each
    follows:'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些内容在本章中尚未涉及，但下面将简要解释每个部分：
- en: '*Optimizers* – algorithms used to change the attributes of the neural networks
    to reduce the loss. These attributes include weights, learning rates, and so on.'
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*优化器* – 用于改变神经网络属性以减少损失值的算法。这些属性包括权重、学习率等。'
- en: '*Loss* – a method used to calculate gradients, which are then used to update
    the weights in the neural network.'
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*损失* – 一种用于计算梯度的方法，然后使用这些梯度来更新神经网络中的权重。'
- en: '*Metrics* – the metric(s) you''re optimizing for (for example, accuracy).'
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*指标* – 您正在优化的指标（例如，准确率）。'
- en: Going deeper into any of these topics is beyond the scope of this book. There
    are plenty of resources for discovering the theory behind deep learning. This
    chapter only aims to cover the essential basics.
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 深入探讨这些主题超出了本书的范围。有很多资源可以帮助您了解深度学习背后的理论。本章仅旨在介绍基本概念。
- en: 'You can compile your neural network by executing the following code:'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以通过执行以下代码来编译你的神经网络：
- en: '[PRE25]'
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now you're ready to train the model. The training subset will be used to train
    the network, and the testing subset will be used for the evaluation. The network
    will be trained for 10 epochs (10 complete passes through the entire training
    data).
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，你已经准备好训练模型了。训练子集将被用来训练网络，测试子集将被用来评估。网络将训练10个epoch（即10次完整遍历整个训练数据）。
- en: 'You can use the following code snippet to train the model:'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用以下代码片段来训练模型：
- en: '[PRE26]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Executing the preceding code will start the training process. How long it will
    take depends on the hardware you have and whether you''re using a GPU or CPU.
    You should see something similar to the following screenshot:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 执行前面的代码将启动训练过程。所需时间取决于你的硬件配置以及你是否使用了GPU或CPU。你应该会看到以下类似的屏幕截图：
- en: '![Figure 6.13 – MNIST model training'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图6.13 – MNIST模型训练'
- en: '](img/B16954_06_13.jpg)'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B16954_06_13.jpg)'
- en: Figure 6.13 – MNIST model training
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.13 – MNIST模型训练
- en: After 10 epochs, the accuracy on the validation set was 97.7% – excellent if
    we consider that regular neural networks don't work too well with images.
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 经过10个epoch后，验证集上的准确率达到了97.7%——如果我们考虑到常规神经网络在图像处理上表现不佳的话，这已经是非常出色的了。
- en: To test your model on a new instance, you can use the `predict()` method. It
    returns an array that tells you how likely it is that the prediction for a given
    class is correct. There will be 10 items in this array, as there were 10 classes.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要在新的实例上测试你的模型，你可以使用`predict()`方法。它返回一个数组，告诉你对于给定类别的预测是否正确。这个数组将有10个项，因为共有10个类别。
- en: 'You can then call `np.argmax()` to get the item with the highest value:'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，你可以调用`np.argmax()`来获取值最高的项：
- en: '[PRE27]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The results are shown in the following screenshot:'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果将在下面的屏幕截图中展示：
- en: '![Figure 6.14 – Testing the MNIST model'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.14 – 测试MNIST模型'
- en: '](img/B16954_06_14.jpg)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B16954_06_14.jpg)'
- en: Figure 6.14 – Testing the MNIST model
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 – 测试MNIST模型
- en: As you can see, the prediction is correct.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，预测是正确的。
- en: And that's how easy it is to train neural networks with libraries such as TensorFlow.
    Keep in mind that this way of handling image classification isn't recommended
    in the real world, as we've flattened a 28x28 image and immediately lost all two-dimensional
    information. CNNs would be a better approach for image classification, as they
    can extract useful features from two-dimensional data. Our artifical neural network
    worked well here because MNIST is a simple and clean dataset – not something you'll
    get a whole lot of in your job.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 就是这样简单，使用TensorFlow等库训练神经网络。请记住，在现实世界中，我们不建议使用这种方式处理图像分类，因为我们已经将28x28的图像展平，立即丢失了所有的二维信息。对于图像分类，CNN（卷积神经网络）会是一个更好的方法，因为它们可以从二维数据中提取有用的特征。我们的人工神经网络在这里表现良好，因为MNIST是一个简单且干净的数据库集——这不是你在工作中会遇到的很多情况。
- en: In the next section, you'll learn the differences in approaching classification
    and regression tasks with neural networks.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解使用神经网络处理分类和回归任务的差异。
- en: Neural networks in regression versus classification
  id: totrans-247
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归与分类中的神经网络
- en: If you've done any machine learning with scikit-learn, you know there are dedicated
    classes and models for regression and classification datasets. For example, if
    you would like to apply a decision tree algorithm to a classification dataset,
    you would use the `DecisionTreeClassifier` class. Likewise, you would use the
    `DecisionTreeRegressor` class for regression tasks.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用过scikit-learn进行过任何机器学习，你就会知道，对于回归和分类数据集，都有专门的类和模型。例如，如果你想将决策树算法应用于分类数据集，你会使用`DecisionTreeClassifier`类。同样，对于回归任务，你会使用`DecisionTreeRegressor`类。
- en: But what do you do with neural networks? There are no dedicated classes or layers
    for classification and regression tasks.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 但你该如何使用神经网络呢？没有专门用于分类和回归任务的类或层。
- en: Instead, you can accommodate by tweaking the number of neurons in the output
    layer. Put simply, if you're dealing with regression tasks, there has to be a
    single neuron in the output layer. If you're dealing with classification tasks,
    there will be as many neurons in the output layer as there are distinct classes
    in your target variable.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，你可以通过调整输出层中的神经元数量来适应。简单来说，如果你处理的是回归任务，输出层中必须只有一个神经元。如果你处理的是分类任务，输出层中的神经元数量将与目标变量中的不同类别数量相同。
- en: For example, you saw how the neural network in the previous section had 10 neurons
    in the output layer. The reason is that there are 10 distinct digits, from zero
    to nine. If you were instead predicting the price of something (regression), there
    would be only a single neuron in the output layer.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你看到了上一节中的神经网络在输出层有10个神经元。原因是存在从零到九的10个不同的数字。如果你预测的是某物的价格（回归），输出层将只有一个神经元。
- en: The task of the neural network is to learn the adequate parameter values (weights
    and biases) to produce the best output value, irrespective of the type of problem
    you're solving.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的任务是学习适当的参数值（权重和偏置），以产生最佳输出值，无论你正在解决什么类型的问题。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter might be hard to process if this was your first encounter with
    deep learning and neural networks. Going over the materials a couple of times
    could help, but it won't be enough to understand the topic fully. Entire books
    have been written on deep learning, and even on small subsets of deep learning.
    Hence, covering everything in a single chapter isn't possible.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次接触深度学习和神经网络，那么这一章可能难以理解。反复阅读材料可能会有所帮助，但这还不足以完全理解这个主题。关于深度学习，甚至关于深度学习的小子集，已经写出了整本书。因此，在一章中涵盖所有内容是不可能的。
- en: Still, you should have the basic theory behind the concepts of neurons, layers,
    and activation functions, and you can always learn more on your own. The following
    chapter, [*Chapter 7*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086)*, Neural
    Network Classifier with TPOT*, will show you how to connect neural networks and
    pipeline optimization, so you can build state-of-the-art models in a completely
    automated fashion.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，您应该了解神经元、层和激活函数概念背后的基本理论，并且您可以随时自学更多。下一章，[*第7章*](B16954_07_Final_SK_ePub.xhtml#_idTextAnchor086)*，TPOT神经网络分类器*，将向您展示如何连接神经网络和管道优化，这样您就可以完全自动地构建最先进的模型。
- en: As always, please feel free to explore the theory and practice of deep learning
    and neural networks on your own. It is definitely a field of study worth exploring
    further.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，请随时探索深度学习和神经网络的理沦和实践。这绝对是一个值得进一步研究的学术领域。
- en: Q&A
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q&A
- en: How would you define the term "deep learning"?
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将如何定义“深度学习”这个术语？
- en: What is the difference between traditional machine learning algorithms and algorithms
    used in deep learning?
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 传统机器学习算法与深度学习中使用的算法有什么区别？
- en: List and briefly describe five types of neural networks.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出并简要描述五种类型的神经网络。
- en: Can you figure out how to calculate the number of trainable parameters in a
    network given the number of neurons per layer? For example, a neural network with
    the architecture [10, 8, 8, 2] has in total 178 trainable parameters (160 weights
    and 18 biases).
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能想出如何根据每层的神经元数量来计算网络中的可训练参数数量吗？例如，具有架构[10, 8, 8, 2]的神经网络总共有178个可训练参数（160个权重和18个偏置）。
- en: Name four different activation functions and briefly explain them.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 列出四种不同的激活函数，并简要解释它们。
- en: In your own words, describe *loss* in neural networks.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用你自己的话描述神经网络中的*损失*。
- en: Explain why modeling imagine classification models with regular artificial neural
    networks isn't a good idea.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释为什么用常规人工神经网络来建模图像分类模型不是一个好主意。
