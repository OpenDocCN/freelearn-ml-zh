- en: Chapter 8. Ensemble Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 集成学习
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Classifying data with the bagging method
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Bagging方法分类数据
- en: Performing cross-validation with the bagging method
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Bagging方法进行交叉验证
- en: Classifying data with the boosting method
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Boosting方法分类数据
- en: Performing cross-validation with the boosting method
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Boosting方法进行交叉验证
- en: Classifying data with gradient boosting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度提升分类数据
- en: Calculating the margins of a classifier
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算分类器的边缘
- en: Calculating the error evolution of the ensemble method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算集成方法的误差演变
- en: Classifying the data with random forest
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林分类数据
- en: Estimating the prediction errors of different classifiers
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计不同分类器的预测误差
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Ensemble learning is a method to combine results produced by different learners
    into one format, with the aim of producing better classification results and regression
    results. In previous chapters, we discussed several classification methods. These
    methods take different approaches but they all have the same goal, that is, finding
    an optimum classification model. However, a single classifier may be imperfect,
    which may misclassify data in certain categories. As not all classifiers are imperfect,
    a better approach is to average the results by voting. In other words, if we average
    the prediction results of every classifier with the same input, we may create
    a superior model compared to using an individual method.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习是一种将不同学习器产生的结果组合成一种格式的方法，目的是产生更好的分类结果和回归结果。在前面的章节中，我们讨论了几种分类方法。这些方法采取不同的方法，但它们都有相同的目标，即找到最优的分类模型。然而，单个分类器可能是不完美的，它可能在某些类别中错误地分类数据。由于并非所有分类器都是不完美的，因此更好的方法是通过对结果进行平均投票。换句话说，如果我们对每个分类器相同的输入的预测结果进行平均，我们可能创建一个比使用单个方法更好的模型。
- en: 'In ensemble learning, bagging, boosting, and random forest are the three most
    common methods:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在集成学习中，Bagging、Boosting和随机森林是三种最常用的方法：
- en: Bagging is a voting method, which first uses Bootstrap to generate a different
    training set, and then uses the training set to make different base learners.
    The bagging method employs a combination of base learners to make a better prediction.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging是一种投票方法，它首先使用Bootstrap生成不同的训练集，然后使用该训练集来制作不同的基础学习器。Bagging方法采用基础学习器的组合来做出更好的预测。
- en: Boosting is similar to the bagging method. However, what makes boosting different
    is that it first constructs the base learning in sequence, where each successive
    learner is built for the prediction residuals of the preceding learner. With the
    means to create a complementary learner, it uses the mistakes made by previous
    learners to train the next base learner.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boosting与Bagging方法类似。然而，使Boosting与众不同的地方在于它首先按顺序构建基础学习器，其中每个后续学习器都是为了构建前一个学习器的预测残差。通过创建互补学习者的手段，它使用先前学习者的错误来训练下一个基础学习器。
- en: Random forest uses the classification results voted from many classification
    trees. The idea is simple; a single classification tree will obtain a single classification
    result with a single input vector. However, a random forest grows many classification
    trees, obtaining multiple results from a single input. Therefore, a random forest
    will use the majority of votes from all the decision trees to classify data or
    use an average output for regression.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林使用来自许多分类树的投票结果。这个想法很简单；单个分类树将使用单个输入向量获得单个分类结果。然而，随机森林生长了许多分类树，从单个输入中获得多个结果。因此，随机森林将使用所有决策树中的多数投票来分类数据或使用平均输出进行回归。
- en: In the following recipes, we will discuss how to use bagging and boosting to
    classify data. We can then perform cross-validation to estimate the error rate
    of each classifier. In addition to this, we'll introduce the use of a margin to
    measure the certainty of a model. Next, we cover random forests, similar to the
    bagging and boosting methods, and introduce how to train the model to classify
    data and use margins to estimate the model certainty. Lastly, we'll demonstrate
    how to estimate the error rate of each classifier, and use the error rate to compare
    the performance of different classifiers.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的菜谱中，我们将讨论如何使用Bagging和Boosting来分类数据。然后我们可以进行交叉验证来估计每个分类器的错误率。除此之外，我们还将介绍使用边缘来衡量模型确定性的方法。接下来，我们将介绍类似于Bagging和Boosting方法的随机森林，并介绍如何训练模型来分类数据以及如何使用边缘来估计模型确定性。最后，我们将演示如何估计每个分类器的错误率，并使用错误率来比较不同分类器的性能。
- en: Classifying data with the bagging method
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用bagging方法对数据进行分类
- en: The `adabag` package implements both boosting and bagging methods. For the bagging
    method, the package implements Breiman's Bagging algorithm, which first generates
    multiple versions of classifiers, and then obtains an aggregated classifier. In
    this recipe, we will illustrate how to use the bagging method from `adabag` to
    generate a classification model using the telecom `churn` dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`adabag`包实现了提升和bagging方法。对于bagging方法，该包实现了Breiman的Bagging算法，该算法首先生成多个分类器的多个版本，然后获得一个聚合分类器。在这个菜谱中，我们将说明如何使用`adabag`中的bagging方法来使用电信`churn`数据集生成分类模型。'
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source for the bagging method. For those who have not prepared the dataset,
    please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for detailed information.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们继续使用电信`churn`数据集作为bagging方法的输入数据源。对于那些尚未准备数据集的人，请参阅[第5章](part0060_split_000.html#page
    "第5章. 分类（I） – 树、懒惰和概率分类"), *分类（I） – 树、懒惰和概率分类*，获取详细信息。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to generate a classification model for the telecom
    `churn` dataset:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以生成电信`churn`数据集的分类模型：
- en: 'First, you need to install and load the `adabag` package (it might take a while
    to install `adabag`):'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你需要安装并加载`adabag`包（安装`adabag`可能需要一段时间）：
- en: '[PRE0]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, you can use the `bagging` function to train a training dataset (the result
    may vary during the training process):'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`bagging`函数来训练一个训练数据集（训练过程中结果可能会有所不同）：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Access the variable importance from the bagging result:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从bagging结果中获取变量重要性：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After generating the classification model, you can use the predicted results
    from the testing dataset:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在生成分类模型后，你可以使用测试数据集的预测结果：
- en: '[PRE3]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the predicted results, you can obtain a classification table:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从预测结果中，你可以获得一个分类表：
- en: '[PRE4]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, you can retrieve the average error of the bagging result:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以检索bagging结果的平均误差：
- en: '[PRE5]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: How it works...
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何工作...
- en: 'Bagging is derived from the name Bootstrap aggregating, which is a stable,
    accurate, and easy to implement model for data classification and regression.
    The definition of bagging is as follows: given a training dataset of size *n*,
    bagging performs Bootstrap sampling and generates *m* new training sets, *Di*,
    each of size *n*. Finally, we can fit *m* Bootstrap samples to *m* models and
    combine the result by averaging the output (for regression) or voting (for classification):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging来源于Bootstrap aggregating（自助聚合）的名字，这是一种稳定、准确且易于实现的数据分类和回归模型。Bagging的定义如下：给定一个大小为*n*的训练数据集，bagging执行Bootstrap抽样并生成*m*个新的训练集，*Di*，每个大小为*n*。最后，我们可以将*m*个Bootstrap样本拟合到*m*个模型中，并通过平均输出（对于回归）或投票（对于分类）来组合结果：
- en: '![How it works...](img/00132.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00132.jpeg)'
- en: An illustration of bagging method
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: bagging方法的示意图
- en: The advantage of using bagging is that it is a powerful learning method, which
    is easy to understand and implement. However, the main drawback of this technique
    is that it is hard to analyze the result.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用bagging的优点是它是一种强大的学习方法，易于理解和实现。然而，这种技术的缺点是结果难以分析。
- en: In this recipe, we use the boosting method from `adabag` to classify the telecom
    churn data. Similar to other classification methods discussed in previous chapters,
    you can train a boosting classifier with a formula and a training dataset. Additionally,
    you can set the number of iterations to 10 in the `mfinal` argument. Once the
    classification model is built, you can examine the importance of each attribute.
    Ranking the attributes by importance reveals that the number of customer service
    calls play a crucial role in the classification model.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用`adabag`中的提升方法对电信客户流失数据进行分类。类似于前面章节中讨论的其他分类方法，你可以使用公式和训练数据集来训练一个提升分类器。此外，你可以在`mfinal`参数中将迭代次数设置为10。一旦构建了分类模型，你可以检查每个属性的重要性。按重要性对属性进行排序显示，客户服务电话的数量在分类模型中起着至关重要的作用。
- en: Next, with a fitted model, you can apply the `predict.bagging` function to predict
    the labels of the testing dataset. Therefore, you can use the labels of the testing
    dataset and predicted results to generate a classification table and obtain the
    average error, which is 0.045 in this example.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用拟合好的模型，您可以应用`predict.bagging`函数来预测测试数据集的标签。因此，您可以使用测试数据集的标签和预测结果来生成分类表并获取平均误差，在这个例子中为0.045。
- en: There's more...
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Besides `adabag`, the `ipred` package provides a bagging method for a classification
    tree. We demonstrate here how to use the bagging method of the `ipred` package
    to train a classification model:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`adabag`，`ipred`包还提供了一个用于分类树的袋装方法。在这里，我们演示如何使用`ipred`包的袋装方法来训练一个分类模型：
- en: 'First, you need to install and load the `ipred` package:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，您需要安装并加载`ipred`包：
- en: '[PRE6]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can then use the `bagging` method to fit the classification method:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用`bagging`方法来拟合分类方法：
- en: '[PRE7]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Obtain an out of bag estimate of misclassification of the errors:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取错误分类的出袋估计：
- en: '[PRE8]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You can then use the `predict` function to obtain the predicted labels of the
    testing dataset:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用`predict`函数来获取测试数据集的预测标签：
- en: '[PRE9]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Obtain the classification table from the labels of the testing dataset and
    prediction result:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从测试数据集的标签和预测结果中获取分类表：
- en: '[PRE10]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Performing cross-validation with the bagging method
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用袋装方法进行交叉验证
- en: To assess the prediction power of a classifier, you can run a cross-validation
    method to test the robustness of the classification model. In this recipe, we
    will introduce how to use `bagging.cv` to perform cross-validation with the bagging
    method.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估分类器的预测能力，您可以运行交叉验证方法来测试分类模型的鲁棒性。在这个菜谱中，我们将介绍如何使用`bagging.cv`来执行带有袋装方法的交叉验证。
- en: Getting ready
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source to perform a k-fold cross-validation with the bagging method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们继续使用telecom `churn`数据集作为输入数据源，使用袋装方法进行k折交叉验证。
- en: How to do it...
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to retrieve the minimum estimation errors by performing
    cross-validation with the bagging method:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用袋装方法进行交叉验证来执行以下步骤以检索最小估计误差：
- en: 'First, we use `bagging.cv` to make a 10-fold classification on the training
    dataset with 10 iterations:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们使用`bagging.cv`在训练数据集上进行10次迭代的10折分类：
- en: '[PRE11]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'You can then obtain the confusion matrix from the cross-validation results:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以从交叉验证结果中获取混淆矩阵：
- en: '[PRE12]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Lastly, you can retrieve the minimum estimation errors from the cross-validation
    results:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以从交叉验证结果中检索最小估计误差：
- en: '[PRE13]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: How it works...
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The `adabag` package provides a function to perform the k-fold validation with
    either the bagging or boosting method. In this example, we use `bagging.cv` to
    make the k-fold cross-validation with the bagging method. We first perform a 10-fold
    cross validation with 10 iterations by specifying `v=10` and `mfinal=10`. Please
    note that this is quite time consuming due to the number of iterations. After
    the cross-validation process is complete, we can obtain the confusion matrix and
    average errors (0.058 in this case) from the cross-validation results.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`adabag`包提供了一个函数来执行带有袋装或提升方法的k折验证。在这个例子中，我们使用`bagging.cv`来使用袋装方法进行k折交叉验证。我们首先通过指定`v=10`和`mfinal=10`进行10次迭代的10折交叉验证。请注意，由于迭代次数较多，这相当耗时。交叉验证过程完成后，我们可以从交叉验证结果中获得混淆矩阵和平均误差（在这种情况下为0.058）。'
- en: See also
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For those interested in tuning the parameters of `bagging.cv`, please view
    the `bagging.cv` document by using the `help` function:'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于那些想要调整`bagging.cv`参数的人来说，请使用`help`函数查看`bagging.cv`文档：
- en: '[PRE14]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Classifying data with the boosting method
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用提升方法进行数据分类
- en: Similar to the bagging method, boosting starts with a simple or weak classifier
    and gradually improves it by reweighting the misclassified samples. Thus, the
    new classifier can learn from previous classifiers. The `adabag` package provides
    implementation of the **AdaBoost.M1** and **SAMME** algorithms. Therefore, one
    can use the boosting method in `adabag` to perform ensemble learning. In this
    recipe, we will use the boosting method in `adabag` to classify the telecom `churn`
    dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装法类似，提升法从简单或弱分类器开始，通过重新加权误分类样本逐渐改进它。因此，新的分类器可以从前一个分类器中学习。`adabag`包提供了**AdaBoost.M1**和**SAMME**算法的实现。因此，可以使用`adabag`中的提升方法进行集成学习。在本菜谱中，我们将使用`adabag`中的提升方法对电信`churn`数据集进行分类。
- en: Getting ready
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will continue to use the telecom churn dataset as the input
    data source to perform classifications with the boosting method. Also, you need
    to have the `adabag` package loaded in R before commencing the recipe.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本菜谱中，我们将继续使用电信`churn`数据集作为输入数据源，使用提升法进行分类。此外，在开始菜谱之前，您需要将`adabag`包加载到R中。
- en: How to do it...
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to classify the telecom `churn` dataset with the
    boosting method:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤，使用提升法对电信`churn`数据集进行分类：
- en: 'You can use the boosting function from the `adabag` package to train the classification
    model:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您可以使用`adabag`包中的提升函数来训练分类模型：
- en: '[PRE15]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You can then make a prediction based on the boosted model and testing dataset:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以根据提升模型和测试数据集进行预测：
- en: '[PRE16]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, you can retrieve the classification table from the predicted results:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，您可以从预测结果中检索分类表：
- en: '[PRE17]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, you can obtain the average errors from the predicted results:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，您可以从预测结果中获得平均误差：
- en: '[PRE18]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: How it works...
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The idea of boosting is to "boost" weak learners (for example, a single decision
    tree) into strong learners. Assuming that we have *n* points in our training dataset,
    we can assign a weight, *Wi* (0 <= i <n), for each point. Then, during the iterative
    learning process (we assume the number of iterations is *m*), we can reweigh each
    point in accordance with the classification result in each iteration. If the point
    is correctly classified, we should decrease the weight. Otherwise, we increase
    the weight of the point. When the iteration process is finished, we can then obtain
    the *m* fitted model, *f[i](x)* (0 <= i <n). Finally, we can obtain the final
    prediction through the weighted average of each tree''s prediction, where the
    weight, b, is based on the quality of each tree:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法的思想是将弱学习器（例如单个决策树）提升为强学习器。假设我们的训练数据集中有*n*个点，我们可以为每个点分配一个权重，*Wi*（0 <= i <
    n）。然后，在迭代学习过程中（我们假设迭代次数为*m*），我们可以根据每次迭代的分类结果重新加权每个点。如果点被正确分类，我们应该减少权重。否则，我们增加该点的权重。当迭代过程完成后，我们可以获得*m*个拟合模型，*f[i](x)*（0
    <= i < n）。最后，我们可以通过每个树的预测的加权平均获得最终预测，其中权重b基于每个树的质量：
- en: '![How it works...](img/00133.jpeg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00133.jpeg)'
- en: An illustration of boosting method
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法的示意图
- en: Both bagging and boosting are ensemble methods, which combine the prediction
    power of each single learner into a strong learner. The difference between bagging
    and boosting is that the bagging method combines independent models, but boosting
    performs an iterative process to reduce the errors of preceding models by predicting
    them with successive models.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是袋装法（bagging）还是提升法（boosting），它们都是集成方法，将每个单个学习器的预测能力结合成一个强大的学习器。袋装法和提升法之间的区别在于，袋装法结合了独立的模型，而提升法通过迭代过程，通过后续模型预测先前模型的错误来减少错误。
- en: In this recipe, we demonstrate how to fit a classification model within the
    boosting method. Similar to bagging, one has to specify the formula and the training
    dataset used to train the classification model. In addition, one can specify parameters,
    such as the number of iterations (`mfinal`), the weight update coefficient (`coeflearn`),
    the weight of how each observation is used (`boos`), and the control for `rpart`
    (a single decision tree). In this recipe, we set the iteration to 10, using `Freund`
    (the AdaBoost.M1 algorithm implemented method) as `coeflearn`, `boos` set to false
    and max depth set to `3` for `rpart` configuration.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何在提升方法内拟合分类模型。类似于袋装法，必须指定用于训练分类模型的公式和训练数据集。此外，还可以指定参数，例如迭代次数（`mfinal`）、权重更新系数（`coeflearn`）、每个观测值的权重（`boos`）以及`rpart`（单个决策树）的控制。在这个菜谱中，我们将迭代次数设置为10，使用`Freund`（Adaboost.M1算法实现的方法）作为`coeflearn`，将`boos`设置为false，并将`rpart`配置的最大深度设置为`3`。
- en: We use the boosting method to fit the classification model and then save it
    in `churn.boost`. We can then obtain predicted labels using the `prediction` function.
    Furthermore, we can use the `table` function to retrieve a classification table
    based on the predicted labels and testing the dataset labels. Lastly, we can get
    the average errors of the predicted results.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用提升方法拟合分类模型，并将其保存在`churn.boost`中。然后，我们可以使用`prediction`函数获取预测标签。此外，我们可以使用`table`函数根据预测标签和测试数据集标签检索分类表。最后，我们可以获取预测结果的平均误差。
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更多...
- en: 'In addition to using the boosting function in the `adabag` package, one can
    also use the `caret` package to perform a classification with the boosting method:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在`adabag`包中使用提升函数外，还可以使用`caret`包通过提升方法进行分类：
- en: 'First, load the `mboost` and `pROC` package:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，加载`mboost`和`pROC`包：
- en: '[PRE19]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can then set the training control with the `trainControl` function and use
    the `train` function to train the classification model with adaboost:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`trainControl`函数设置训练控制，并使用`train`函数使用adaboost训练分类模型：
- en: '[PRE20]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Use the `summary` function to obtain the details of the classification model:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary`函数获取分类模型的详细信息：
- en: '[PRE21]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Use the `plot` function to plot the ROC curve within different iterations:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`plot`函数在不同迭代中绘制ROC曲线：
- en: '[PRE22]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![There''s more...](img/00134.jpeg)'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![更多...](img/00134.jpeg)'
- en: The repeated cross validation plot
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重复交叉验证图
- en: 'Finally, we can make predictions using the `predict` function and view the
    classification table:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`predict`函数进行预测，并查看分类表：
- en: '[PRE23]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Performing cross-validation with the boosting method
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用提升方法进行交叉验证
- en: Similar to the `bagging` function, `adabag` provides a cross-validation function
    for the boosting method, named `boosting.cv`. In this recipe, we will demonstrate
    how to perform cross-validation using `boosting.cv` from the package, `adabag`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`bagging`函数，`adabag`为提升方法提供了一个交叉验证函数，名为`boosting.cv`。在这个菜谱中，我们将演示如何使用`adabag`包中的`boosting.cv`进行交叉验证。
- en: Getting ready
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source to perform a k-fold cross-validation with the `boosting` method.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们继续使用电信`churn`数据集作为输入数据源，使用`boosting`方法进行k折交叉验证。
- en: How to do it...
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to retrieve the minimum estimation errors via cross-validation
    with the `boosting` method:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`boosting`方法进行交叉验证，执行以下步骤以检索最小估计误差：
- en: 'First, you can use `boosting.cv` to cross-validate the training dataset:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你可以使用`boosting.cv`对训练数据集进行交叉验证：
- en: '[PRE24]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'You can then obtain the confusion matrix from the boosting results:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以从提升结果中获得混淆矩阵：
- en: '[PRE25]'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, you can retrieve the average errors of the boosting method:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以检索提升方法的平均误差：
- en: '[PRE26]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: Similar to `bagging.cv`, we can perform cross-validation with the boosting method
    using `boosting.cv`. If `v` is set to `10` and `mfinal` is set to `5`, the `boosting`
    method will perform 10-fold cross-validations with five iterations. Also, one
    can set the control of the `rpart` fit within the parameter. We can set the complexity
    parameter to 0.01 in this example. Once the training is complete, the confusion
    matrix and average errors of the boosted results will be obtained.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`bagging.cv`，我们可以使用`boosting.cv`在提升方法中进行交叉验证。如果将`v`设置为`10`，将`mfinal`设置为`5`，则提升方法将执行10次交叉验证，每次迭代5次。此外，还可以在参数内设置`rpart`拟合的控制。在这个例子中，我们可以将复杂性参数设置为0.01。一旦训练完成，将获得提升结果的混淆矩阵和平均误差。
- en: See also
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'For those who require more information on tuning the parameters of `boosting.cv`,
    please view the `boosting.cv` document by using the `help` function:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于需要更多关于调整`boosting.cv`参数信息的人，请使用`help`函数查看`boosting.cv`文档：
- en: '[PRE27]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Classifying data with gradient boosting
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用梯度提升法分类数据
- en: Gradient boosting ensembles weak learners and creates a new base learner that
    maximally correlates with the negative gradient of the loss function. One may
    apply this method on either regression or classification problems, and it will
    perform well in different datasets. In this recipe, we will introduce how to use
    `gbm` to classify a telecom `churn` dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升集成弱学习器并创建一个与损失函数的负梯度最大相关的新基学习器。可以将此方法应用于回归或分类问题，并且在不同数据集上表现良好。在本菜谱中，我们将介绍如何使用`gbm`对电信`churn`数据集进行分类。
- en: Getting ready
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we continue to use the telecom `churn` dataset as the input
    data source for the `bagging` method. For those who have not prepared the dataset,
    please refer to [Chapter 5](part0060_split_000.html#page "Chapter 5. Classification
    (I) – Tree, Lazy, and Probabilistic"), *Classification (I) – Tree, Lazy, and Probabilistic*,
    for detailed information.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们继续使用电信`churn`数据集作为`bagging`方法的输入数据源。对于那些尚未准备数据集的人，请参阅[第5章](part0060_split_000.html#page
    "第5章。分类（I）-树、懒惰和概率性")，*分类（I）-树、懒惰和概率性*，以获取详细信息。
- en: How to do it...
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做
- en: 'Perform the following steps to calculate and classify data with the gradient
    boosting method:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用梯度提升法计算和分类数据：
- en: 'First, install and load the package, `gbm`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`gbm`包：
- en: '[PRE28]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The `gbm` function only uses responses ranging from `0` to `1`; therefore,
    you should transform yes/no responses to numeric responses (0/1):'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`gbm`函数只使用从`0`到`1`的响应；因此，你应该将是/否响应转换为数值响应（0/1）：'
- en: '[PRE29]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, you can use the `gbm` function to train a training dataset:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`gbm`函数来训练一个训练数据集：
- en: '[PRE30]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Then, you can obtain the summary information from the fitted model:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以从拟合模型中获取摘要信息：
- en: '[PRE31]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![How to do it...](img/00135.jpeg)'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00135.jpeg)'
- en: Relative influence plot of fitted model
  id: totrans-139
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拟合模型的相对影响图
- en: 'You can obtain the best iteration using cross-validation:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用交叉验证获得最佳迭代次数：
- en: '[PRE32]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![How to do it...](img/00136.jpeg)'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00136.jpeg)'
- en: The performance measurement plot
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 性能测量图
- en: 'Then, you can retrieve the odd value of the log returned from the Bernoulli
    loss function:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以检索伯努利损失函数返回的对数中的奇数值：
- en: '[PRE33]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, you can plot the ROC curve and get the best cut off that will have the
    maximum accuracy:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以绘制ROC曲线并获取具有最大准确性的最佳截止值：
- en: '[PRE34]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '![How to do it...](img/00137.jpeg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00137.jpeg)'
- en: The ROC curve of fitted model
  id: totrans-149
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 拟合模型的ROC曲线
- en: 'You can retrieve the best cut off with the `coords` function and use this cut
    off to obtain the predicted label:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`coords`函数检索最佳截止值并使用此截止值来获取预测标签：
- en: '[PRE35]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Lastly, you can obtain the classification table from the predicted results:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，你可以从预测结果中获得分类表：
- en: '[PRE36]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: How it works...
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The algorithm of gradient boosting involves, first, the process computes the
    deviation of residuals for each partition, and then, determines the best data
    partitioning in each stage. Next, the successive model will fit the residuals
    from the previous stage and build a new model to reduce the residual variance
    (an error). The reduction of the residual variance follows the functional gradient
    descent technique, in which it minimizes the residual variance by going down its
    derivative, as show here:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升算法涉及的过程，首先计算每个分区的残差偏差，然后确定每个阶段的最佳数据分区。接下来，连续的模型将拟合前一阶段的残差并构建一个新的模型以减少残差方差（一个误差）。残差方差的减少遵循功能梯度下降技术，通过沿着其导数下降来最小化残差方差，如下所示：
- en: '![How it works...](img/00138.jpeg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的...](img/00138.jpeg)'
- en: Gradient descent method
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法
- en: In this recipe, we use the gradient boosting method from `gbm` to classify the
    telecom churn dataset. To begin the classification, we first install and load
    the `gbm` package. Then, we use the `gbm` function to train the classification
    model. Here, as our prediction target is the `churn` attribute, which is a binary
    outcome, we therefore set the distribution as `bernoulli` in the `distribution`
    argument. Also, we set the 1000 trees to fit in the `n.tree` argument, the maximum
    depth of the variable interaction to `7` in `interaction.depth`, the learning
    rate of the step size reduction to 0.01 in `shrinkage`, and the number of cross-validations
    to `3` in `cv.folds`. After the model is fitted, we can use the summary function
    to obtain the relative influence information of each variable in the table and
    figure. The relative influence shows the reduction attributable to each variable
    in the sum of the square error. Here, we can find `total_day_minutes` is the most
    influential one in reducing the loss function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用`gbm`中的梯度提升方法对电信客户流失数据集进行分类。为了开始分类，我们首先安装并加载`gbm`包。然后，我们使用`gbm`函数来训练分类模型。在这里，由于我们的预测目标是`churn`属性，这是一个二元结果，因此我们在`distribution`参数中将分布设置为`bernoulli`。此外，我们在`n.tree`参数中将1000棵树设置为拟合，在`interaction.depth`中将变量交互的最大深度设置为`7`，在`shrinkage`中将步长减少的学习率设置为0.01，在`cv.folds`中将交叉验证的次数设置为`3`。模型拟合后，我们可以使用`summary`函数从表中获取每个变量的相对影响信息，并从图表中获取。相对影响显示了每个变量在平方误差和中的减少量。在这里，我们可以找到`total_day_minutes`是减少损失函数的最有影响力的一个变量。
- en: Next, we use the `gbm.perf` function to find the optimum iteration. Here, we
    estimate the optimum number with cross-validation by specifying the `method` argument
    to `cv`. The function further generates two plots, where the black line plots
    the training error and the green one plots the validation error. The error measurement
    here is a `bernoulli` distribution, which we have defined earlier in the training
    stage. The blue dash line on the plot shows where the optimum iteration is.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用`gbm.perf`函数找到最佳迭代次数。在这里，我们通过指定`method`参数为`cv`来进行交叉验证，以估计最佳迭代次数。该函数进一步生成两个图表，其中黑色线条表示训练误差，绿色线条表示验证误差。这里的误差测量是一个`bernoulli`分布，我们已经在训练阶段定义了它。图表上的蓝色虚线显示了最佳迭代的位置。
- en: Then, we use the `predict` function to obtain the odd value of a log in each
    testing case returned from the Bernoulli loss function. In order to get the best
    prediction result, one can set the `n.trees` argument to an optimum iteration
    number. However, as the returned value is an odd value log, we still have to determine
    the best cut off to determine the label. Therefore, we use the `roc` function
    to generate an ROC curve and get the cut off with the maximum accuracy.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用`predict`函数从伯努利损失函数返回的每个测试案例中获取对数概率值。为了获得最佳的预测结果，可以将`n.trees`参数设置为最佳迭代次数。然而，由于返回值是对数概率值的奇数，我们仍然需要确定最佳的截止点来确定标签。因此，我们使用`roc`函数生成ROC曲线，并获取具有最大准确率的截止点。
- en: Finally, we can use the function, `coords`, to retrieve the best cut off threshold
    and use the `ifelse` function to determine the class label from the odd value
    of the log. Now, we can use the `table` function to generate the classification
    table and see how accurate the classification model is.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`coords`函数检索最佳截止阈值，并使用`ifelse`函数从对数概率值中确定类别标签。现在，我们可以使用`table`函数生成分类表，以查看分类模型的准确性。
- en: There's more...
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'In addition to using the boosting function in the `gbm` package, one can also
    use the `mboost` package to perform classifications with the gradient boosting
    method:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在`gbm`包中使用提升函数外，还可以使用`mboost`包通过梯度提升方法进行分类：
- en: 'First, install and load the `mboost` package:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`mboost`包：
- en: '[PRE37]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The `mboost` function only uses numeric responses; therefore, you should transform
    yes/no responses to numeric responses (0/1):'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`mboost`函数仅使用数值响应；因此，应将是/否响应转换为数值响应（0/1）：'
- en: '[PRE38]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Also, you should remove nonnumerical attributes, such as `voice_mail_plan`
    and `international_plan`:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，还应删除非数值属性，例如`voice_mail_plan`和`international_plan`：
- en: '[PRE39]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We can then use `mboost` to train the classification model:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以使用`mboost`来训练分类模型：
- en: '[PRE40]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Use the `summary` function to obtain the details of the classification model:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`summary`函数获取分类模型的详细信息：
- en: '[PRE41]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Lastly, use the `plot` function to draw a partial contribution plot of each
    attribute:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`plot`函数绘制每个属性的局部贡献图：
- en: '[PRE42]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '![There''s more...](img/00139.jpeg)'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![还有更多...](img/00139.jpeg)'
- en: The partial contribution plot of important attributes
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 重要属性的局部贡献图
- en: Calculating the margins of a classifier
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算分类器的边缘
- en: A margin is a measure of the certainty of classification. This method calculates
    the difference between the support of a correct class and the maximum support
    of an incorrect class. In this recipe, we will demonstrate how to calculate the
    margins of the generated classifiers.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘是分类确定性的度量。此方法计算正确类别的支持与错误类别的最大支持之间的差异。在本配方中，我们将演示如何计算生成的分类器的边缘。
- en: Getting ready
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by storing a fitted bagging model
    in the variables, `churn.bagging` and `churn.predbagging`. Also, put the fitted
    boosting classifier in both `churn.boost` and `churn.boost.pred`.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成前面的配方，将拟合的 bagging 模型存储在变量 `churn.bagging` 和 `churn.predbagging` 中。同时，将拟合的提升分类器放入
    `churn.boost` 和 `churn.boost.pred` 中。
- en: How to do it...
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to calculate the margin of each ensemble learner:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以计算每个集成学习者的边缘：
- en: 'First, use the `margins` function to calculate the margins of the boosting
    classifiers:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用 `margins` 函数计算提升分类器的边缘：
- en: '[PRE43]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'You can then use the `plot` function to plot a marginal cumulative distribution
    graph of the boosting classifiers:'
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用 `plot` 函数绘制提升分类器的边缘累积分布图：
- en: '[PRE44]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '![How to do it...](img/00140.jpeg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00140.jpeg)'
- en: The margin cumulative distribution graph of using the boosting method
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用提升方法的边缘累积分布图
- en: 'You can then calculate the percentage of negative margin matches training errors
    and the percentage of negative margin matches test errors:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以计算负边缘匹配训练错误的百分比和负边缘匹配测试错误的百分比：
- en: '[PRE45]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Also, you can calculate the margins of the bagging classifiers. You might see
    the warning message showing "`no non-missing argument to min`". The message simply
    indicates that the min/max function is applied to the numeric of the 0 length
    argument:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，您还可以计算 bagging 分类器的边缘。您可能会看到显示 "`no non-missing argument to min`" 的警告信息。该信息仅表示
    min/max 函数应用于长度为 0 的参数的数值：
- en: '[PRE46]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'You can then use the `plot` function to plot a margin cumulative distribution
    graph of the bagging classifiers:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，您可以使用 `plot` 函数绘制 bagging 分类器的边缘累积分布图：
- en: '[PRE47]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![How to do it...](img/00141.jpeg)'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00141.jpeg)'
- en: The margin cumulative distribution graph of the bagging method
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: bagging 方法的边缘累积分布图
- en: 'Finally, you can then compute the percentage of negative margin matches training
    errors and the percentage of negative margin matches test errors:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，然后您可以计算负边缘匹配训练错误的百分比和负边缘匹配测试错误的百分比：
- en: '[PRE48]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: How it works...
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'A margin is the measurement of certainty of the classification; it is computed
    by the support of the correct class and the maximum support of the incorrect class.
    The formula of margins can be formulated as:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘是分类确定性的度量；它是通过正确类别的支持与错误类别的最大支持来计算的。边缘的公式可以表示为：
- en: '![How it works...](img/00142.jpeg)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/00142.jpeg)'
- en: Here, the margin of the xi sample equals the support of a correctly classified
    sample (*c* denotes the correct class) minus the maximum support of a sample that
    is classified to class *j* (where *j≠c* and *j=1…k*). Therefore, correctly classified
    examples will have positive margins and misclassified examples will have negative
    margins. If the margin value is close to one, it means that correctly classified
    examples have a high degree of confidence. On the other hand, examples of uncertain
    classifications will have small margins.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，xi 样本的边缘等于正确分类样本的支持（*c* 表示正确类别）减去被分类到类别 *j* 的样本的最大支持（其中 *j≠c* 且 *j=1…k*）。因此，正确分类的示例将具有正边缘，错误分类的示例将具有负边缘。如果边缘值接近
    1，则表示正确分类的示例具有高度的置信度。另一方面，不确定分类的示例将具有较小的边缘。
- en: The `margins` function calculates the margins of AdaBoost.M1, AdaBoost-SAMME,
    or the bagging classifier, which returns a vector of a margin. To visualize the
    margin distribution, one can use a margin cumulative distribution graph. In these
    graphs, the x-axis shows the margin and the y-axis shows the percentage of observations
    where the margin is less than or equal to the margin value of the x-axis. If every
    observation is correctly classified, the graph will show a vertical line at the
    margin equal to 1 (where margin = 1).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`margins`函数计算AdaBoost.M1、AdaBoost-SAMME或袋装分类器的边缘，并返回一个边缘向量。为了可视化边缘分布，可以使用边缘累积分布图。在这些图中，x轴表示边缘，y轴表示边缘小于或等于x轴边缘值的观测值的百分比。如果每个观测值都被正确分类，则图表将在边缘等于1的位置显示一条垂直线（其中边缘=1）。'
- en: For the margin cumulative distribution graph of the boosting classifiers, we
    can see that there are two lines plotted on the graph, in which the green line
    denotes the margin of the testing dataset, and the blue line denotes the margin
    of the training set. The figure shows about 6.39 percent of negative margins match
    the training error, and 6.26 percent of negative margins match the test error.
    On the other hand, we can find that 17.33% of negative margins match the training
    error and 4.3 percent of negative margins match the test error in the margin cumulative
    distribution graph of the bagging classifiers. Normally, the percentage of negative
    margins matching the training error should be close to the percentage of negative
    margins that match the test error. As a result of this, we should then examine
    the reason why the percentage of negative margins that match the training error
    is much higher than the negative margins that match the test error.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提升分类器的边缘累积分布图，我们可以看到图上有两条线，其中绿色线表示测试数据集的边缘，蓝色线表示训练集的边缘。图显示大约6.39%的负边缘与训练错误匹配，6.26%的负边缘与测试错误匹配。另一方面，我们可以在袋装分类器的边缘累积分布图中找到，17.33%的负边缘与训练错误匹配，4.3%的负边缘与测试错误匹配。通常，匹配训练错误的负边缘百分比应该接近匹配测试错误的负边缘百分比。因此，我们应该检查为什么匹配训练错误的负边缘百分比远高于匹配测试错误的负边缘百分比。
- en: See also
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参考内容
- en: 'If you are interested in more details on margin distribution graphs, please
    refer to the following source: *Kuncheva LI (2004)*, *Combining Pattern Classifiers:
    Methods and Algorithms*, *John Wiley & Sons*.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '如果您对边缘分布图有更多细节感兴趣，请参阅以下来源：*Kuncheva LI (2004)*, *Combining Pattern Classifiers:
    Methods and Algorithms*, *John Wiley & Sons*。'
- en: Calculating the error evolution of the ensemble method
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算集成方法的错误演化
- en: The `adabag` package provides the `errorevol` function for a user to estimate
    the ensemble method errors in accordance with the number of iterations. In this
    recipe, we will demonstrate how to use `errorevol` to show the evolution of errors
    of each ensemble classifier.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`adabag`包提供了一个`errorevol`函数，用户可以根据迭代次数估计集成方法错误。在这个配方中，我们将演示如何使用`errorevol`来展示每个集成分类器错误的演化。'
- en: Getting ready
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: You need to have completed the previous recipe by storing the fitted bagging
    model in the variable, `churn.bagging`. Also, put the fitted boosting classifier
    in `churn.boost`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要完成上一个配方，将拟合的袋装模型存储在变量`churn.bagging`中。同时，将拟合的提升分类器放入`churn.boost`。
- en: How to do it...
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Perform the following steps to calculate the error evolution of each ensemble
    learner:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来计算每个集成学习者的错误演化：
- en: 'First, use the `errorevol` function to calculate the error evolution of the
    boosting classifiers:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`errorevol`函数来计算提升分类器的错误演化：
- en: '[PRE49]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![How to do it...](img/00143.jpeg)'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00143.jpeg)'
- en: Boosting error versus number of trees
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 提升错误与树的数量对比
- en: 'Next, use the `errorevol` function to calculate the error evolution of the
    bagging classifiers:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，使用`errorevol`函数来计算袋装分类器的错误演化：
- en: '[PRE50]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![How to do it...](img/00144.jpeg)'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何做...](img/00144.jpeg)'
- en: Bagging error versus number of trees
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 袋装错误与树的数量对比
- en: How it works...
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: The `errorest` function calculates the error evolution of AdaBoost.M1, AdaBoost-SAMME,
    or the bagging classifiers and returns a vector of error evolutions. In this recipe,
    we use the boosting and bagging models to generate error evolution vectors and
    graph the error versus number of trees.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: '`errorest`函数计算AdaBoost.M1、AdaBoost-SAMME或袋装分类器的错误演化，并返回一个错误演化的向量。在这个配方中，我们使用提升和袋装模型生成错误演化向量，并绘制错误与树的数量对比图。'
- en: The resulting graph reveals the error rate of each iteration. The trend of the
    error rate can help measure how fast the errors reduce, while the number of iterations
    increases. In addition to this, the graphs may show whether the model is over-fitted.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表揭示了每次迭代的错误率。错误率的变化趋势可以帮助衡量错误减少的速度，同时迭代次数增加。此外，图表可能显示模型是否过拟合。
- en: See also
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'If the ensemble model is over-fitted, you can use the `predict.bagging` and
    `predict.boosting` functions to prune the ensemble model. For more information,
    please use the help function to refer to `predict.bagging` and `predict.boosting`:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果集成模型过拟合，你可以使用`predict.bagging`和`predict.boosting`函数来修剪集成模型。更多信息，请使用帮助函数参考`predict.bagging`和`predict.boosting`：
- en: '[PRE51]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Classifying data with random forest
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林进行数据分类
- en: Random forest is another useful ensemble learning method that grows multiple
    decision trees during the training process. Each decision tree will output its
    own prediction results corresponding to the input. The forest will use the voting
    mechanism to select the most voted class as the prediction result. In this recipe,
    we will illustrate how to classify data using the `randomForest` package.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是另一种有用的集成学习方法，在训练过程中会生成多个决策树。每个决策树将输出其对应的输入预测结果。森林将使用投票机制来选择得票最多的类别作为预测结果。在本教程中，我们将说明如何使用`randomForest`包进行数据分类。
- en: Getting ready
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to perform classifications with the random forest method.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在本教程中，我们将继续使用电信`churn`数据集作为输入数据源，使用随机森林方法进行分类。
- en: How to do it...
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to classify data with random forest:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤以使用随机森林进行数据分类：
- en: First, you have to install and load the `randomForest` package;
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，你必须安装并加载`randomForest`包；
- en: '[PRE52]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'You can then fit the random forest classifier with a training set:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用训练集拟合随机森林分类器：
- en: '[PRE53]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, make predictions based on the fitted model and testing dataset:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，根据拟合模型和测试数据集进行预测：
- en: '[PRE54]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Similar to other classification methods, you can obtain the classification
    table:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与其他分类方法类似，你可以获得分类表：
- en: '[PRE55]'
  id: totrans-241
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You can use the `plot` function to plot the mean square error of the forest
    object:'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以使用`plot`函数绘制森林对象的均方误差：
- en: '[PRE56]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![How to do it...](img/00145.jpeg)'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00145.jpeg)'
- en: The mean square error of the random forest
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机森林的均方误差
- en: 'You can then examine the importance of each attribute within the fitted classifier:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以检查拟合分类器中每个属性的重要性：
- en: '[PRE57]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Next, you can use the `varImpPlot` function to obtain the plot of variable
    importance:'
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，你可以使用`varImpPlot`函数获取变量重要性的绘图：
- en: '[PRE58]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '![How to do it...](img/00146.jpeg)'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00146.jpeg)'
- en: The visualization of variable importance
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 变量重要性的可视化
- en: 'You can also use the `margin` function to calculate the margins and plot the
    margin cumulative distribution:'
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以使用`margin`函数来计算边缘并绘制边缘累积分布图：
- en: '[PRE59]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '![How to do it...](img/00147.jpeg)'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00147.jpeg)'
- en: The margin cumulative distribution graph for the random forest method
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机森林方法的边缘累积分布图
- en: 'Furthermore, you can use a histogram to visualize the margin distribution of
    the random forest:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，你可以使用直方图来可视化随机森林的边缘分布：
- en: '[PRE60]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '![How to do it...](img/00148.jpeg)'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00148.jpeg)'
- en: The histogram of margin distribution
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 边缘分布的直方图
- en: 'You can also use `boxplot` to visualize the margins of the random forest by
    class:'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你还可以使用`boxplot`通过类别可视化随机森林的边缘：
- en: '[PRE61]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '![How to do it...](img/00149.jpeg)'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何操作...](img/00149.jpeg)'
- en: Margins of the random forest by class
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 随机森林的类别边缘
- en: How it works...
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'The purpose of random forest is to ensemble weak learners (for example, a single
    decision tree) into a strong learner. The process of developing a random forest
    is very similar to the bagging method, assuming that we have a training set containing
    *N* samples with *M* features. The process first performs bootstrap sampling,
    which samples *N* cases at random, with the replacement as the training dataset
    of each single decision tree. Next, in each node, the process first randomly selects
    *m* variables (where *m << M*), then finds the predictor variable that provides
    the best split among m variables. Next, the process grows the full tree without
    pruning. In the end, we can obtain the predicted result of an example from each
    single tree. As a result, we can get the prediction result by taking an average
    or weighted average (for regression) of an output or taking a majority vote (for
    classification):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的目的是将弱学习器（例如单个决策树）集成到强学习器中。开发随机森林的过程与袋装法非常相似，假设我们有一个包含*N*个样本和*M*个特征的训练集。这个过程首先进行自助采样，随机抽取*N*个案例，作为每个单个决策树的训练数据集。接下来，在每个节点中，过程首先随机选择*m*个变量（其中*m
    << M*），然后找到在m个变量中提供最佳分割的预测变量。接下来，过程在不剪枝的情况下生长完整的树。最后，我们可以从每棵树中获得示例的预测结果。因此，我们可以通过对输出取平均或加权平均（对于回归）或取多数投票（对于分类）来获得预测结果：
- en: '![How it works...](img/00132.jpeg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/00132.jpeg)'
- en: 'A random forest uses two parameters: **ntree** (the number of trees) and **mtry**
    (the number of features used to find the best feature), while the bagging method
    only uses ntree as a parameter. Therefore, if we set mtry equal to the number
    of features within a training dataset, then the random forest is equal to the
    bagging method.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林使用两个参数：**ntree**（树的数量）和**mtry**（用于找到最佳特征的特性数量），而袋装法只使用ntree作为参数。因此，如果我们设置mtry等于训练数据集中的特性数量，那么随机森林就等同于袋装法。
- en: The main advantages of random forest are that it is easy to compute, it can
    efficiently process data, and is fault tolerant to missing or unbalanced data.
    The main disadvantage of random forest is that it cannot predict the value beyond
    the range of a training dataset. Also, it is prone to over-fitting of noisy data.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林的主要优点是它易于计算，可以高效地处理数据，并且对缺失或不平衡数据具有容错性。随机森林的主要缺点是它不能预测超出训练数据集范围的值。此外，它容易对噪声数据进行过拟合。
- en: In this recipe, we employ the random forest method adapted from the `randomForest`
    package to fit a classification model. First, we install and load `randomForest`
    into an R session. We then use the random forest method to train a classification
    model. We set `importance = T`, which will ensure that the importance of the predictor
    is assessed.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用来自`randomForest`包的随机森林方法来拟合分类模型。首先，我们在R会话中安装并加载`randomForest`。然后，我们使用随机森林方法训练分类模型。我们设置`importance
    = T`，这将确保评估预测变量的重要性。
- en: Similar to the bagging and boosting methods, once the model is fitted, one can
    perform predictions using a fitted model on the testing dataset, and furthermore,
    obtain the classification table.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 与袋装法和提升法类似，一旦模型拟合完成，就可以在测试数据集上使用拟合的模型进行预测，并且还可以获得分类表。
- en: In order to assess the importance of each attribute, the `randomForest` package
    provides the importance and `varImpPlot` functions to either list the importance
    of each attribute in the fitted model or visualize the importance using either
    mean decrease accuracy or mean decrease `gini`.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估每个属性的重要性，`randomForest`包提供了重要性和`varImpPlot`函数，既可以列出拟合模型中每个属性的重要性，也可以使用平均降低准确度或平均降低`gini`来可视化重要性。
- en: Similar to `adabag`, which contains a method to calculate the margins of the
    bagging and boosting methods, `randomForest` provides the `margin` function to
    calculate the margins of the forest object. With the `plot`, `hist`, and `boxplot`
    functions, you can visualize the margins in different aspects to the proportion
    of correctly classified observations.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 与包含计算袋装法和提升法边缘方法的`adabag`类似，`randomForest`提供了`margin`函数来计算森林对象的边缘。使用`plot`、`hist`和`boxplot`函数，你可以从不同方面可视化正确分类观察值的比例。
- en: There's more...
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Apart from the `randomForest` package, the `party` package also provides an
    implementation of random forest. In the following steps, we illustrate how to
    use the `cforest` function within the `party` package to perform classifications:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`randomForest`包之外，`party`包还提供了一个随机森林的实现。在以下步骤中，我们将展示如何在`party`包中使用`cforest`函数来进行分类：
- en: 'First, install and load the `party` package:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，安装并加载`party`包：
- en: '[PRE62]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'You can then use the `cforest` function to fit the classification model:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以使用`cforest`函数来拟合分类模型：
- en: '[PRE63]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'You can make predictions based on the built model and the testing dataset:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以根据构建的模型和测试数据集进行预测：
- en: '[PRE64]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, obtain the classification table from the predicted labels and the
    labels of the testing dataset:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，从预测标签和测试数据集的标签中获取分类表：
- en: '[PRE65]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Estimating the prediction errors of different classifiers
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计不同分类器的预测误差
- en: At the beginning of this chapter, we discussed why we use ensemble learning
    and how it can improve the prediction performance compared to using just a single
    classifier. We now validate whether the ensemble model performs better than a
    single decision tree by comparing the performance of each method. In order to
    compare the different classifiers, we can perform a 10-fold cross-validation on
    each classification method to estimate test errors using `erroreset` from the
    `ipred` package.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们讨论了为什么使用集成学习以及它如何与仅使用单个分类器相比提高预测性能。我们现在通过比较每种方法的性能来验证集成模型是否比单个决策树表现更好。为了比较不同的分类器，我们可以对每种分类方法进行10折交叉验证，使用`ipred`包中的`erroreset`来估计测试误差。
- en: Getting ready
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: In this recipe, we will continue to use the telecom `churn` dataset as the input
    data source to estimate the prediction errors of the different classifiers.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们将继续使用电信`churn`数据集作为输入数据源来估计不同分类器的预测误差。
- en: How to do it...
  id: totrans-287
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Perform the following steps to estimate the prediction errors of each classification
    method:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下步骤来估计每种分类方法的预测误差：
- en: 'You can estimate the error rate of the bagging model:'
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以估计袋装模型的错误率：
- en: '[PRE66]'
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'You can then estimate the error rate of the boosting method:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以估计提升方法的错误率：
- en: '[PRE67]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, estimate the error rate of the random forest model:'
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，估计随机森林模型的错误率：
- en: '[PRE68]'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Finally, make a prediction function using `churn.predict`, and then use the
    function to estimate the error rate of the single decision tree:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用`churn.predict`创建一个预测函数，然后使用该函数估计单个决策树的错误率：
- en: '[PRE69]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: How it works...
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: In this recipe, we estimate the error rates of four different classifiers using
    the `errorest` function from the `ipred` package. We compare the boosting, bagging,
    and random forest methods, and the single decision tree classifier. The `errorest`
    function performs a 10-fold cross-validation on each classifier and calculates
    the misclassification error. The estimation results from the four chosen models
    reveal that the boosting method performs the best with the lowest error rate (0.0475).
    The random forest method has the second lowest error rate (0.051), while the bagging
    method has an error rate of 0.0583\. The single decision tree classifier, `rpart`,
    performs the worst among the four methods with an error rate equal to 0.0674\.
    These results show that all three ensemble learning methods, boosting, bagging,
    and random forest, outperform a single decision tree classifier.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们使用`ipred`包中的`errorest`函数来估计四种不同分类器的错误率。我们比较了提升、袋装和随机森林方法，以及单个决策树分类器。`errorest`函数对每个分类器进行10折交叉验证，并计算误分类误差。四个选择模型的估计结果显示，提升方法具有最低的错误率（0.0475）。随机森林方法具有第二低的错误率（0.051），而袋装方法的错误率为0.0583。单个决策树分类器`rpart`在四种方法中表现最差，错误率等于0.0674。这些结果表明，提升、袋装和随机森林这三种集成学习方法都优于单个决策树分类器。
- en: See also
  id: totrans-299
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关内容
- en: 'In this recipe we mentioned the `ada` package, which contains a method to perform
    stochastic boosting. For those interested in this package, please refer to: *Additive
    Logistic Regression: A Statistical View of Boosting by Friedman*, *et al. (2000)*.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们提到了`ada`包，它包含一个执行随机提升的方法。对那些对这个包感兴趣的人，请参阅：*Friedman等（2000）的《加性逻辑回归：提升的统计视角》*。
