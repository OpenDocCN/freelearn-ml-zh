- en: '*Chapter 16*: K-Means and DBSCAN Clustering'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第16章*：K-Means和DBSCAN聚类'
- en: Data clustering allows us to organize unlabeled data into groups of observations
    with more in common with other members of the group than with observations outside
    of the group. There are a surprisingly large number of applications for clustering,
    either as the final model of a machine learning pipeline or as input for another
    model. This includes market research, image processing, and document classification.
    We sometimes also use clustering to improve exploratory data analysis or to create
    more meaningful visualizations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据聚类使我们能够将未标记的数据组织成具有更多共同点的观察组，这些共同点比组外的观察点更多。聚类有许多令人惊讶的应用，无论是作为机器学习管道的最终模型，还是作为其他模型的输入。这包括市场研究、图像处理和文档分类。我们有时也使用聚类来改进探索性数据分析或创建更有意义的可视化。
- en: K-means and **density-based spatial clustering of applications with noise**
    (**DBSCAN**) clustering, like **principal component analysis** (**PCA**), are
    unsupervised learning algorithms. There are no labels to use as the basis for
    predictions. The purpose of the algorithm is to identify instances that hang together
    based on their features. Instances that are in close proximity to each other,
    and further away from other instances, can be considered to be in a cluster. There
    are a number of ways to gauge proximity. **Partition-based clustering**, such
    as k-means, and **density-based clustering**, such as DBSCAN, are two of the more
    popular approaches. We will explore those approaches in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: K-means和**基于密度的应用噪声聚类**（**DBSCAN**），就像**主成分分析**（**PCA**）一样，是无监督学习算法。没有标签可以用作预测的基础。算法的目的是根据特征识别出相互关联的实例。彼此靠近且与其他实例距离较远的实例可以被认为是处于一个簇中。有几种方法可以衡量邻近程度。**基于划分的聚类**，如k-means，和**基于密度的聚类**，如DBSCAN，是两种更受欢迎的方法。我们将在本章中探讨这些方法。
- en: 'Specifically, we will go over the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将讨论以下主题：
- en: The key concepts of k-means and DBSCAN clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means和DBSCAN聚类的关键概念
- en: Implementing k-means clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现k-means聚类
- en: Implementing DBSCAN clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现DBSCAN聚类
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: We will mainly stick to the pandas, NumPy, and scikit-learn libraries in this
    chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要使用pandas、NumPy和scikit-learn库。
- en: The key concepts of k-means and DBSCAN clustering
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means和DBSCAN聚类的关键概念
- en: With k-means clustering, we identify *k* clusters, each with a center, or **centroid**.
    The centroid is the point that minimizes the total squared distance between it
    and the other data points in the cluster.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means聚类中，我们识别*k*个簇，每个簇都有一个中心，或**质心**。质心是使它与簇中其他数据点的总平方距离最小的点。
- en: An example with made-up data should help here. The data points in *Figure 16.1*
    seem to be in three clusters. (It is not usually that easy to visualize the number
    of clusters, *k*.)
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一个使用虚构数据的例子应该能有所帮助。*图16.1*中的数据点似乎在三个簇中。（通常并不那么容易可视化簇的数量，*k*。）
- en: '![Figure 16.1 – Data points with three discernible clusters ](img/B17978_16_001.jpg)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图16.1 – 具有三个可识别簇的数据点](img/B17978_16_001.jpg)'
- en: Figure 16.1 – Data points with three discernible clusters
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 – 具有三个可识别簇的数据点
- en: 'We perform the following steps to construct the clusters:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们执行以下步骤来构建簇：
- en: Assign a random point as the center of each cluster.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将一个随机点分配为每个簇的中心。
- en: Calculate the distance of each point from the center of each cluster.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个点到每个簇中心的距离。
- en: Assign data points to a cluster based on their proximity to the center point.
    These first three steps are illustrated in *Figure 16.2*. The points with an **X**
    are the randomly chosen cluster centers (with *k* set at 3). Data points that
    are closer to the cluster center point than to other cluster center points get
    assigned to that cluster.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据数据点到中心点的邻近程度将数据点分配到簇中。这三个步骤在*图16.2*中进行了说明。带有**X**的点是被随机选择的簇中心（将*k*设置为3）。比其他簇中心点更接近簇中心点的数据点被分配到该簇。
- en: '![Figure 16.2 – Random points assigned as the center of the cluster ](img/B17978_16_002.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图16.2 – 随机分配为簇中心的点](img/B17978_16_002.jpg)'
- en: Figure 16.2 – Random points assigned as the center of the cluster
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 – 随机分配为簇中心的点
- en: Calculate a new center point for the new cluster. This is illustrated in *Figure
    16.3*.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为新簇计算一个新的中心点。这如图16.3所示。
- en: '![Figure 16.3 – New cluster centers calculated ](img/B17978_16_003.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图16.3 – 新计算的簇中心](img/B17978_16_003.jpg)'
- en: Figure 16.3 – New cluster centers calculated
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 – 新的聚类中心计算
- en: Repeat steps 2 through 4 until there is not much change in the centers.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2到4，直到中心的变化不大。
- en: K-means clustering is a very popular algorithm for clustering for several reasons.
    It is quite intuitive and typically quite fast. It does have some disadvantages,
    however. It processes every data point as part of a cluster, so the clusters can
    be yanked around by extreme values. It also assumes clusters will have spherical
    shapes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类是一种非常流行的聚类算法，原因有几个。它相当直观，通常也相当快。然而，它确实有一些缺点。它将每个数据点都处理为聚类的一部分，因此聚类可能会被极端值拉扯。它还假设聚类将具有球形形状。
- en: 'The evaluation of unsupervised models is less clear than with supervised models,
    as we do not have a target with which to compare our predictions. A fairly common
    metric for clustering models is the **silhouette score**. The silhouette score
    is the mean silhouette coefficient for all instances. The **silhouette coefficient**
    is as follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督模型的评估不如监督模型清晰，因为我们没有目标来比较我们的预测。聚类模型的一个相当常见的指标是**轮廓分数**。轮廓分数是所有实例的平均轮廓系数。**轮廓系数**如下：
- en: '![](img/B17978_16_0011.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17978_16_0011.jpg)'
- en: Here, ![](img/B17978_16_002.png) is the mean distance to all instances of the
    next closest cluster for the ith instance, and ![](img/B17978_16_003.png) is the
    mean distance to the instances of the assigned cluster. This coefficient ranges
    from -1 to 1, with scores near 1 meaning that the instance is well within the
    assigned cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B17978_16_002.png)是第i个实例到下一个最近簇的所有实例的平均距离，而![](img/B17978_16_003.png)是到分配簇的实例的平均距离。这个系数的范围从-1到1，分数接近1意味着实例很好地位于分配的簇内。
- en: Another metric to evaluate our clusters is the **inertia score**. This is the
    sum of squared distances between each instance and its centroid. This distance
    will decrease as we increase the number of clusters but there are eventually diminishing
    marginal returns from increasing the number of clusters. The change in inertia
    score with *k* is often visualized using an **elbow plot**. This plot is called
    an elbow plot because the slope gets much closer to 0 as we increase *k*, so close
    that it resembles an elbow. This is shown in *Figure 16.4*. In this case, we would
    choose a value of *k* near the elbow.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 评估我们的聚类的一个另一个指标是**惯性分数**。这是每个实例与其质心之间平方距离的总和。随着我们增加聚类数量，这个距离会减小，但最终增加聚类数量会带来边际收益的递减。通常使用**肘图**来可视化k值与惯性分数的变化。这个图被称为肘图，因为随着k的增加，斜率会接近0，接近到它类似于一个肘部。这如图*图16.4*所示。在这种情况下，我们会选择一个接近肘部的k值。
- en: '![Figure 16.4 – An elbow plot with inertia and k ](img/B17978_16_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图16.4 – 惯性和k的肘图](img/B17978_16_004.jpg)'
- en: Figure 16.4 – An elbow plot with inertia and k
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4 – 惯性和k的肘图
- en: Another metric often used when evaluating a clustering model is the **Rand index**.
    The Rand index tells us how frequently two clusterings have assigned the same
    cluster to instances. Values for the Rand index will range between 0 and 1\. We
    typically use an adjusted Rand index, which corrects for chance in the similarity
    calculation. Values for the adjusted Rand index can sometimes be negative.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 评估聚类模型时，经常使用的另一个指标是**Rand指数**。Rand指数告诉我们两个聚类如何频繁地将相同的簇分配给实例。Rand指数的值将在0到1之间。我们通常使用调整后的Rand指数，它纠正了相似度计算中的偶然性。调整后的Rand指数的值有时可能是负数。
- en: '**DBSCAN** takes a different approach to clustering. For each instance, it
    counts the number of instances within a specified distance of that instance. All
    instances within ɛ of an instance are said to be in that instance’s ɛ-neighborhood.
    When the number of instances in an ɛ-neighborhood equals or exceeds the minimum
    samples value that we specify, that instance is considered a core instance and
    the ɛ-neighborhood is considered a cluster. Any instance that is more than ɛ from
    another instance is considered noise. This is illustrated in *Figure 16.5*.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**DBSCAN**采用了一种不同的聚类方法。对于每个实例，它计算该实例指定距离内的实例数量。所有在ɛ距离内的实例都被认为是该实例的ɛ-邻域。当ɛ-邻域中的实例数量等于或超过我们指定的最小样本值时，该实例被认为是核心实例，ɛ-邻域被认为是聚类。任何与另一个实例距离超过ɛ的实例被认为是噪声。这如图*图16.5*所示。'
- en: '![Figure 16.5 – DBSCAN clustering with minimum samples = five ](img/B17978_16_005.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图16.5 – 最小样本数=五的DBSCAN聚类](img/B17978_16_005.jpg)'
- en: Figure 16.5 – DBSCAN clustering with minimum samples = five
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 – 最小样本数=五的DBSCAN聚类
- en: 'There are several advantages of this density-based approach. The clusters do
    not need to be spherical. They can take any shape. We do not need to guess at
    the number of clusters, though we do need to provide a value for ɛ. Outliers are
    just interpreted as noise and so do not impact the clusters. (This last point
    hints at another useful application of DBSCAN: identifying anomalies.)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基于密度的方法有几个优点。簇不需要是球形的，它们可以采取任何形状。虽然我们不需要猜测簇的数量，但我们需要提供一个ɛ的值。异常值只是被解释为噪声，因此不会影响簇。（这一点暗示了DBSCAN的另一个有用应用：识别异常。）
- en: We will use DBSCAN for clustering later in this chapter. First, we will examine
    how to do clustering with k-means, including how to choose a good value for k.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面使用DBSCAN进行聚类。首先，我们将检查如何使用k-means进行聚类，包括如何选择一个好的k值。
- en: Implementing k-means clustering
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现k-means聚类
- en: We can use k-means with some of the same data that we used with the supervised
    learning models that we developed in earlier chapters. The difference is that
    there is no longer a target for us to predict. Rather, we are interested in how
    certain instances hang together. Think of how people arrange themselves in groups
    during a stereotypical high school lunch break and you kind of get a general idea.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用与我们在前几章中开发的监督学习模型相同的某些数据来使用k-means。区别在于我们不再有一个预测的目标。相反，我们感兴趣的是某些实例是如何聚集在一起的。想想典型的中学午餐休息时间人们如何分组，你就能得到一个大致的概念。
- en: We also need to do much of the same preprocessing work that we did with supervised
    learning models. We will start with that in this section. We will work with data
    on income gaps between women and men, labor force participation rates, educational
    attainment, teenage birth frequency, and female participation in politics at the
    highest level.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要做很多与监督学习模型相同的预处理工作。我们将在本节开始这部分。我们将处理关于女性和男性之间的收入差距、劳动力参与率、教育成就、青少年出生频率以及女性在最高级别参与政治的数据。
- en: Note
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The income gap dataset is made available for public use by the *United Nations
    Development Program* at [https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development).
    There is one record per country with aggregate employment, income, and education
    data by gender for 2015.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 收入差距数据集由联合国开发计划署在[https://www.kaggle.com/datasets/undp/human-development](https://www.kaggle.com/datasets/undp/human-development)上提供供公众使用。每个国家都有一个记录，包含2015年按性别汇总的就业、收入和教育数据。
- en: 'Let’s build a k-means clustering model:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个k-means聚类模型：
- en: 'We load the familiar libraries. We also load the `KMeans` and `silhouette_score`
    modules. Recall that the silhouette score is often used to evaluate how good a
    job our model has done of clustering. We also load `rand_score`, which will allow
    us to compute the Rand index of similarity between different clusterings:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了熟悉的库。我们还加载了`KMeans`和`silhouette_score`模块。回想一下，轮廓分数通常用于评估我们的模型在聚类方面做得有多好。我们还加载了`rand_score`，这将允许我们计算不同聚类之间的相似性指数：
- en: '[PRE0]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we load the income gap data:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载收入差距数据：
- en: '[PRE1]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s look at some descriptive statistics:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看一些描述性统计：
- en: '[PRE2]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We should also look at some correlations. The education ratio (the ratio of
    female educational level to male educational level) and the human development
    ratio are highly correlated, as are gender inequality and the adolescent birth
    rate, as well as the income ratio and the labor force participation ratio:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还应该查看一些相关性。教育比率（女性教育水平与男性教育水平的比率）和人类发展比率高度相关，性别不平等和青少年出生率也是如此，以及收入比率和劳动力参与率：
- en: '[PRE3]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This produces the following plot:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 16.6 – A heat map of the correlation matrix ](img/B17978_16_006.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图16.6 – 相关矩阵的热图](img/B17978_16_006.jpg)'
- en: Figure 16.6 – A heat map of the correlation matrix
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 – 相关矩阵的热图
- en: 'We need to scale the data before running our model. We also use **KNN imputation**
    to handle the missing values:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在运行我们的模型之前，我们需要对数据进行缩放。我们还使用**KNN插补**来处理缺失值：
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we are ready to run the k-means clustering. We specify a value for the
    number of clusters.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行k-means聚类。我们为簇的数量指定一个值。
- en: 'After fitting the model, we can generate a silhouette score. Our silhouette
    score is not great. This suggests that our clusters are not very far apart. Later,
    we will look to see whether we can get a better score with more or fewer clusters:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合模型后，我们可以生成一个影子分数。我们的影子分数并不高。这表明我们的聚类之间并没有很远。稍后，我们将看看是否可以通过更多或更少的聚类来获得更好的分数：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s take a closer look at the clusters. We can use the `labels_` attribute
    to get the clusters:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们更仔细地观察聚类。我们可以使用`labels_`属性来获取聚类：
- en: '[PRE6]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We could have used the `fit_predict` method instead to get the clusters, like
    so:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们本可以使用`fit_predict`方法来获取聚类，如下所示：
- en: '[PRE7]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It is helpful to examine how the clusters differ in terms of the values of
    their features. Cluster 0 countries have much higher maternal mortality and adolescent
    birth rate values than countries in the other clusters. Cluster 1 countries have
    very low maternal mortality and high income per capita. Cluster 2 countries have
    very low labor force participation ratios (the ratio of female labor force participation
    to male labor force participation) and income ratios. Recall that we have scaled
    the data:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有助于检查聚类在特征值方面的差异。聚类0的国家在孕产妇死亡率和青少年出生率方面比其他聚类的国家要高得多。聚类1的国家孕产妇死亡率非常低，人均收入很高。聚类2的国家劳动力参与率（女性劳动力参与率与男性劳动力参与率的比率）和收入比率非常低。回想一下，我们已经对数据进行归一化处理：
- en: '[PRE8]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can use the `cluster_centers_` attribute to get the center of each cluster.
    There are nine values representing the center for each of the three clusters,
    since we used nine features for the clustering:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`cluster_centers_`属性来获取每个聚类的中心。由于我们使用了九个特征进行聚类，因此有九个值代表三个聚类的中心：
- en: '[PRE9]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We plot the clusters by some of their features, as well as the center. We place
    the number for the cluster at the centroid for that cluster:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过一些特征绘制聚类，以及中心。我们将该聚类的数字放置在该聚类的质心处：
- en: '[PRE10]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following plot:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![Figure 16.7 – A 3D scatter plot of three clusters  ](img/B17978_16_007.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图16.7 – 三个聚类的3D散点图](img/B17978_16_007.jpg)'
- en: Figure 16.7 – A 3D scatter plot of three clusters
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 – 三个聚类的3D散点图
- en: We can see here that the cluster 0 countries have higher maternal mortality
    and higher adolescent birth rates. Cluster 0 countries have lower income ratios.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，聚类0的国家孕产妇死亡率和青少年出生率较高。聚类0国家的收入比率较低。
- en: So far, we have assumed that the best number of clusters to use for our model
    is three. Let’s build a five-cluster model and see how those results look.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设用于我们模型的最佳聚类数量是三个。让我们构建一个五聚类模型，看看那些结果如何。
- en: 'The silhouette score has declined from the three-cluster model. That could
    be an indicator that at least some of the clusters are very close together:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 影子分数从三聚类模型中下降。这可能表明至少有一些聚类非常接近：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s plot the new clusters to get a better sense of where they are:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制新的聚类，以更好地了解它们的位置：
- en: '[PRE12]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This produces the following plot:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![Figure 16.8 – A 3D scatter plot of five clusters ](img/B17978_16_008.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图16.8 – 五个聚类的3D散点图](img/B17978_16_008.jpg)'
- en: Figure 16.8 – A 3D scatter plot of five clusters
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8 – 五个聚类的3D散点图
- en: 'We can use a statistic called the Rand index to measure the similarity between
    the clusters:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用一个称为Rand指数的统计量来衡量聚类之间的相似性：
- en: '[PRE13]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We tried three-cluster and five-cluster models, but were either of those a
    good choice? Let’s look at scores for a range of *k* values:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们尝试了三聚类和五聚类模型，但那些是否是好的选择？让我们查看一系列*k*值的分数：
- en: '[PRE14]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s plot the inertia scores with an elbow plot:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们绘制惯性分数与肘图：
- en: '[PRE15]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This produces the following plot:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![Figure 16.9 – An elbow plot of inertia scores ](img/B17978_16_009.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图16.9 – 惯性分数的肘图](img/B17978_16_009.jpg)'
- en: Figure 16.9 – An elbow plot of inertia scores
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9 – 惯性分数的肘图
- en: 'We also create a plot of the silhouette scores:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还创建了一个影子分数的图形：
- en: '[PRE16]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This produces the following plot:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图形：
- en: '![Figure 16.10 – An plot of silhouette scores ](img/B17978_16_010.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图16.10 – 影子分数图](img/B17978_16_010.jpg)'
- en: Figure 16.10 – An plot of silhouette scores
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10 – 影子分数图
- en: The elbow plot suggests that a value of *k* around 6 or 7 would be best. We
    start to get diminishing returns in inertia at *k* values above that. The silhouette
    score plot suggests a smaller *k*, as there is a sharp decline in silhouette scores
    after that.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 肘图表明，大约6或7的*k*值会是最优的。当*k*值超过这个值时，惯性开始减少。轮廓分数图表明*k*值更小，因为在那之后轮廓分数急剧下降。
- en: K-means clustering helped us make sense of our data on the gap between women
    and men in terms of income, education, and employment by country. We can now see
    how certain features hang together, in a way that the simple correlations we did
    earlier did not reveal. This largely assumed, however, that our clusters have
    a spherical shape, and we had to do some work to confirm that our value of *k*
    was the best. We will not have any of the same issues with DBSCAN clustering,
    so we will try that in the next section.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类帮助我们理解了关于收入、教育和就业方面的男女差距数据。我们现在可以看到某些特征是如何相互关联的，这是之前简单的相关性分析所没有揭示的。然而，这很大程度上假设我们的聚类具有球形形状，我们不得不做一些工作来确认我们的*k*值是最好的。在DBSCAN聚类中，我们不会遇到任何相同的问题，所以我们将尝试在下一节中这样做。
- en: Implementing DBSCAN clustering
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现DBSCAN聚类
- en: DBSCAN is a very flexible approach to clustering. We just need to specify a
    value for ɛ, also referred to as **eps**. As we have discussed, the ɛ value determines
    the size of the ɛ-neighborhood around an instance. The minimum samples hyperparameter
    indicates how many instances around an instance are needed for it to be considered
    a core instance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是一种非常灵活的聚类方法。我们只需要指定一个值用于ɛ，也称为**eps**。正如我们之前讨论的，ɛ值决定了实例周围的ɛ-邻域的大小。最小样本超参数表示围绕一个实例需要多少个实例才能将其视为核心实例。
- en: Note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: We use DBSCAN to cluster the same income gap data that we worked with in the
    previous section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用DBSCAN聚类我们在上一节中使用的相同收入差距数据。
- en: 'Let’s build a DBSCAN clustering model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个DBSCAN聚类模型：
- en: 'We start by loading familiar libraries, plus the `DBSCAN` module:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载熟悉的库，以及`DBSCAN`模块：
- en: '[PRE17]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We import the code to load and preprocess the wage income data that we worked
    with in the previous section. Since that code is unchanged, there is no need to
    repeat it here:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入代码来加载和预处理我们在上一节中使用的工资收入数据。由于该代码没有变化，这里不需要重复：
- en: '[PRE18]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We are now ready to preprocess the data and fit a DBSCAN model . We have chosen
    an eps value of 0.35 here largely through trial and error. We could have also
    looped over a range of eps values and compared silhouette score:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备好预处理数据并拟合一个DBSCAN模型。我们在这里选择ɛ值为0.35主要是通过试错。我们也可以遍历一系列ɛ值并比较轮廓分数：
- en: '[PRE19]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can use the `labels_` attribute to see the clusters. We have 17 noise instances,
    those with a cluster of -1\. The remaining observations are in one of two clusters:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`labels_`属性来查看聚类。我们有17个噪声实例，那些聚类为-1的实例。其余的观测值在一个或两个聚类中：
- en: '[PRE20]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let’s take a closer look at which features are associated with each cluster.
    Cluster 1 countries are very different from cluster 0 countries in `maternalmortality`,
    `adolescentbirthrate`, and `genderinequality`. These were important features with
    the k-means clustering as well, but there is one fewer cluster with DBSCAN and
    the overwhelming majority of instances fall into one cluster:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看哪些特征与每个聚类相关联。聚类1的国家在`maternalmortality`、`adolescentbirthrate`和`genderinequality`方面与聚类0的国家非常不同。这些特征在k-means聚类中也很重要，但DBSCAN中聚类数量更少，绝大多数实例都落入一个聚类中：
- en: '[PRE21]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s visualize the clusters:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化聚类：
- en: '[PRE22]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This produces the following plot:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 16.11 – A 3D scatter plot of the cluster for each country ](img/B17978_16_011.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图16.11 – 每个国家的聚类3D散点图](img/B17978_16_011.jpg)'
- en: Figure 16.11 – A 3D scatter plot of the cluster for each country
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11 – 每个国家的聚类3D散点图
- en: DBSCAN is an excellent tool for clustering, particularly when the characteristics
    of our data mean that k-means clustering is not a good option; for example, when
    the clusters are not spherical. It also has the advantage of not being influenced
    by outliers.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN是聚类的一个优秀工具，尤其是当我们的数据特征意味着k-means聚类不是一个好选择时；例如，当聚类不是球形时。它还有的优点是不受异常值的影响。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We sometimes need to organize our instances into groups with similar characteristics.
    This can be useful even when there is no target to predict. We can use the clusters
    created for visualizations, as we did in this chapter. Since the clusters are
    easy to interpret, we can use them to hypothesize why some features move together.
    We can also use the clustering results in subsequent analysis.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有时需要将具有相似特征的实例组织成组。即使没有预测目标，这也有用。我们可以使用为可视化创建的聚类，就像我们在本章中所做的那样。由于聚类易于解释，我们可以利用它们来假设为什么某些特征会一起移动。我们还可以在后续分析中使用聚类结果。
- en: This chapter explored two popular clustering techniques, k-means and DBSCAN.
    Both techniques are intuitive, efficient, and handle clustering reliably.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了两种流行的聚类技术：k-means和DBSCAN。这两种技术都直观、高效，并且能够可靠地处理聚类。
