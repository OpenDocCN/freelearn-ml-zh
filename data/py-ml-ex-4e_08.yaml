- en: '8'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '8'
- en: Discovering Underlying Topics in the Newsgroups Dataset with Clustering and
    Topic Modeling
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过聚类和主题建模发现新闻组数据集中的潜在主题
- en: In the previous chapter, we went through a text visualization using t-SNE. t-SNE,
    or any dimensionality reduction algorithm, is a type of unsupervised learning.
    In this chapter, we will be continuing our unsupervised learning journey, specifically
    focusing on clustering and topic modeling. We will start with how unsupervised
    learning learns without guidance and how it is good at discovering hidden information
    underneath data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 t-SNE 进行了文本可视化。t-SNE 或任何降维算法都是一种无监督学习方法。在本章中，我们将继续我们的无监督学习之旅，特别关注聚类和主题建模。我们将从无监督学习如何在没有指导的情况下进行学习以及它如何擅长发现数据中隐藏的信息开始。
- en: Next, we will talk about clustering as an important branch of unsupervised learning,
    which identifies different groups of observations from data. For instance, clustering
    is useful for market segmentation, where consumers of similar behaviors are grouped
    into one segment for marketing purposes. We will perform clustering on the 20
    newsgroups text dataset and see what clusters will be produced.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论聚类作为无监督学习的重要分支，它从数据中识别不同的观察组。例如，聚类在市场细分中非常有用，可以将具有相似行为的消费者分为一个组进行营销。我们将对
    20 个新闻组文本数据集进行聚类，并看看会产生哪些聚类。
- en: Another unsupervised learning route we will take is topic modeling, which is
    the process of extracting themes hidden in the dataset. You will be amused by
    how many interesting themes we are able to mine from the 20 newsgroups dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将采用的另一种无监督学习方法是主题建模，它是从数据集中提取隐藏主题的过程。你会对从 20 个新闻组数据集中挖掘出的许多有趣主题感到惊讶。
- en: 'We will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖以下主题：
- en: Leaning without guidance – unsupervised learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无指导的学习——无监督学习
- en: Getting started with k-means clustering
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开始使用 k-means 聚类
- en: Clustering newsgroups data
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类新闻组数据
- en: Discovering underlying topics in newsgroups
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现新闻组中的潜在主题
- en: Learning without guidance – unsupervised learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无指导的学习——无监督学习
- en: In the previous chapter, we applied t-SNE to visualize the newsgroup text data,
    reduced to two dimensions. t-SNE, or dimensionality reduction in general, is a
    type of **unsupervised learning**. Instead of being guided by predefined labels
    or categories, such as a class or membership (classification), and a continuous
    value (regression), unsupervised learning identifies inherent structures or commonalities
    in the input data. Since there is no guidance in unsupervised learning, there
    is no clear answer on what is a right or wrong result. Unsupervised learning has
    the freedom to discover hidden information underneath input data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们应用了 t-SNE 来可视化新闻组文本数据，并将其降维到二维。t-SNE 或一般的降维技术是一种**无监督学习**。与监督学习不同，无监督学习没有预定义的标签或类别指导（如类别或成员资格（分类）和连续值（回归））。无监督学习识别输入数据中的内在结构或共性。由于无监督学习没有指导，因此没有明确的正确或错误的结果。无监督学习有自由去发现输入数据下面隐藏的信息。
- en: 'An easy way to understand unsupervised learning is to think of going through
    many practice questions for an exam. In supervised learning, you are given answers
    to those practice questions. You basically figure out the relationship between
    the questions and answers and learn how to map the questions to the answers. Hopefully,
    you will do well in the actual exam in the end by giving the correct answers.
    However, in unsupervised learning, you are not provided with the answers to those
    practice questions. What you might do in this instance could include the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 理解无监督学习的一种简单方法是将其比作通过做大量的模拟考试题目来准备考试。在监督学习中，你会得到这些模拟题的答案。你基本上是找出问题和答案之间的关系，学会如何将问题映射到答案上。希望你最终能够通过正确回答问题，顺利通过实际考试。然而，在无监督学习中，你并没有得到这些模拟题的答案。在这种情况下，你可能会做以下几件事：
- en: Grouping similar practice questions so that you can later study related questions
    together at one time
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相似的练习题归类，以便你可以一次性学习相关的题目
- en: Finding questions that are highly repetitive so that you don’t have to waste
    time working out the answer for each one individually
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出那些高度重复的问题，以便你不必为每一个问题单独花费时间去解答
- en: Spotting rare questions so that you can be better prepared for them
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出稀有的问题，以便你能更好地为它们做好准备
- en: Extracting the key chunk of each question by removing boilerplate text so you
    can cut to the point
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过去除模板化文本提取每个问题的关键部分，这样你可以直达要点。
- en: You will notice that the outcomes of all these tasks are pretty open-ended.
    They are correct as long as they are able to describe the commonality and the
    structure underneath the data.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，这些任务的结果都非常开放。只要能够描述数据下的共同性和结构，它们就是正确的。
- en: Practice questions are the **features** in machine learning, which are also
    often called **attributes**, **observations**, or **predictive variables**. Answers
    to questions are the labels in machine learning, which are also called **targets**
    or **target variables**. Practice questions with answers provided are called **labeled
    data**, while practice questions without answers are called **unlabeled data**.
    Unsupervised learning works with unlabeled data and acts on that information without
    guidance.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中的**特征**通常也称为**属性**、**观测值**或**预测变量**。问题的答案则是机器学习中的标签，也叫**目标**或**目标变量**。提供答案的练习问题称为**有标签数据**，而没有答案的练习问题则是**无标签数据**。无监督学习适用于无标签数据，并在没有指导的情况下对这些信息进行处理。
- en: 'Unsupervised learning can include the following types:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习可以包括以下几种类型：
- en: '**Clustering**: This means grouping data based on commonality, which is often
    used for exploratory data analysis. Grouping similar practice questions, as mentioned
    earlier, is an example of clustering. Clustering techniques are widely used in
    customer segmentation or for grouping similar online behaviors for a marketing
    campaign. We will learn more about the popular algorithm k-means clustering in
    this chapter.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类**：这意味着根据共同点对数据进行分组，通常用于探索性数据分析。将相似的练习问题分组，正如前面提到的，便是聚类的一个例子。聚类技术广泛应用于客户细分或为营销活动分组相似的在线行为。本章我们将学习流行的k-means聚类算法。'
- en: '**Association**: This explores the co-occurrence of particular values of two
    or more features. Outlier detection (also called anomaly detection) is a typical
    case, where rare observations are identified. Spotting rare questions in the preceding
    example can be achieved using outlier detection techniques.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联**：这探索两个或多个特征的特定值的共现。异常检测（也称为离群值检测）是一个典型案例，通过它可以识别稀有的观测值。通过异常检测技术可以在前面的例子中识别出稀有问题。'
- en: '**Projection**: This maps the original feature space to a reduced dimensional
    space retaining or extracting a set of principal variables. Extracting the key
    chunk of practice questions is an example projection or, specifically, a dimensionality
    reduction. The t-SNE we learned about previously is a good example.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投影**：将原始特征空间映射到一个降维空间，保留或提取一组主变量。提取练习问题的关键部分是一个投影的例子，或者更具体地说，是一个降维过程。我们之前学习过的t-SNE就是一个很好的例子。'
- en: Unsupervised learning is extensively employed in the area of NLP mainly because
    of the difficulty of obtaining labeled text data. Unlike numerical data (such
    as house prices, stock data, and online click streams), labeling text can sometimes
    be subjective, manual, and tedious. Unsupervised learning algorithms that do not
    require labels become effective when it comes to mining text data.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习在自然语言处理领域广泛应用，主要是因为获取标注文本数据的困难。与数值数据（如房价、股票数据和在线点击流）不同，标注文本有时是主观的、手动的且繁琐的。在挖掘文本数据时，不需要标签的无监督学习算法显得尤为有效。
- en: In *Chapter 7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*,
    you experienced using t-SNE to reduce the dimensionality of text data. Now, let’s
    explore text mining with clustering algorithms and topic modeling techniques.
    We will start with clustering the newsgroups data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*，*使用文本分析技术挖掘20个新闻组数据集*，你体验了使用t-SNE来减少文本数据的维度。现在，让我们通过聚类算法和主题建模技术来探索文本挖掘。我们将从对新闻组数据进行聚类开始。
- en: Getting started with k-means clustering
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习k-means聚类
- en: 'The newsgroups data comes with labels, which are the categories of the newsgroups,
    and a number of categories that are closely related or even overlapping, for instance,
    the five computer groups: `comp.graphics`, `comp.os.ms-windows.misc`, `comp.sys.ibm.pc.hardware`,
    `comp.sys.mac.hardware`, and `comp.windows.x`, and the two religion-related ones:
    `alt.atheism` and `talk.religion.misc`.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 新sgroups数据自带标签，这些标签是新闻组的类别，其中有一些类别之间紧密相关甚至重叠。例如，五个计算机相关的新闻组：`comp.graphics`、`comp.os.ms-windows.misc`、`comp.sys.ibm.pc.hardware`、`comp.sys.mac.hardware`、`comp.windows.x`，以及两个与宗教相关的新闻组：`alt.atheism`和`talk.religion.misc`。
- en: Let’s now pretend we don’t know those labels or they don’t exist. Will samples
    from related topics be clustered together? We will now resort to the k-means clustering
    algorithm.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们不知道这些标签，或者它们不存在。相关主题的样本是否会被聚类到一起？我们将使用 k-means 聚类算法来验证。
- en: How does k-means clustering work?
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 聚类是如何工作的？
- en: 'The goal of the k-means algorithm is to partition the data into k groups based
    on feature similarities. *k* is a predefined property of a *k*-means clustering
    model. Each of the *k* clusters is specified by a centroid (center of a cluster)
    and each data sample belongs to the cluster with the nearest centroid. During
    training, the algorithm iteratively updates the *k* centroids based on the data
    provided. Specifically, it involves the following steps:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法的目标是根据特征相似性将数据划分为 k 个组。*k* 是 *k* 均值聚类模型的预定义属性。每个 *k* 个聚类由一个质心（聚类的中心）指定，每个数据样本属于与其最接近的质心对应的聚类。在训练过程中，算法根据提供的数据反复更新
    *k* 个质心。具体步骤如下：
- en: '**Specifying k**: The algorithm needs to know how many clusters to generate
    as an end result.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指定 k**：算法需要知道最终要生成多少个聚类。'
- en: '**Initializing centroids**: The algorithm starts with randomly selecting k
    samples from the dataset as centroids.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**初始化质心**：算法首先从数据集中随机选择 k 个样本作为质心。'
- en: '**Assigning clusters**: Now that we have *k* centroids, samples that share
    the same closest centroid constitute one cluster. *k* clusters are created as
    a result. Note that closeness is usually measured by the **Euclidean distance**.
    Other metrics can also be used, such as the **Manhattan distance** and **Chebyshev
    distance**, which are listed in the following table:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**分配聚类**：现在我们有了 *k* 个质心，共享相同最近质心的样本构成一个聚类。因此，创建了 *k* 个聚类。请注意，距离通常通过**欧几里得距离**来衡量。也可以使用其他度量标准，如**曼哈顿距离**和**切比雪夫距离**，它们列在下表中：'
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_08_01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本的图片，截图，字体，数字  描述自动生成](img/B21047_08_01.png)'
- en: 'Figure 8.1: Distance metrics'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1：距离度量
- en: '**Updating centroids**: For each cluster, we need to recalculate its center
    point, which is the mean of all the samples in the cluster. *k* centroids are
    updated to be the means of corresponding clusters. This is why the algorithm is
    called **k-means**.'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**更新质心**：对于每个聚类，我们需要重新计算其中心点，即该聚类中所有样本的均值。*k* 个质心将更新为相应聚类的均值。这就是算法被称为**k-means**的原因。'
- en: '**Repeating steps 3 and 4**: We keep repeating assigning clusters and updating
    centroids until the model converges when no or a small enough update of centroids
    can be done, or enough iterations have been completed.'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**重复步骤 3 和 4**：我们不断重复分配聚类和更新质心的过程，直到模型收敛，即无法再更新质心或质心更新足够小，或者完成了足够多的迭代。'
- en: 'The outputs of a trained k-means clustering model include the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后的 k-means 聚类模型的输出包括以下内容：
- en: The cluster ID of each training sample, ranging from 1 to *k*
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个训练样本的聚类 ID，范围从 1 到 *k*
- en: '*k* centroids, which can be used to cluster new samples—a new sample will belong
    to the cluster of the closest centroid'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 个质心，可以用来对新样本进行聚类——新样本将归属于最近的质心对应的聚类。'
- en: It is easy to understand the k-means clustering algorithm and its implementation
    is also straightforward, as you will discover next.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类算法很容易理解，其实现也很直接，正如你接下来将发现的那样。
- en: Implementing k-means from scratch
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从零实现 k-means 算法
- en: 'We will use the `iris` dataset from scikit-learn as an example. Let’s first
    load the data and visualize it. We herein only use two features out of the original
    four for simplicity:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将以 scikit-learn 中的 `iris` 数据集为例。首先加载数据并进行可视化。为简化起见，我们这里只使用原始数据集中的两个特征：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Since the dataset contains three iris classes, we plot it in three different
    colors, as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集包含三种鸢尾花类别，我们将其用三种不同的颜色表示，如下所示：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give us the following output for the original data plot:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为原始数据图生成如下输出：
- en: '![A chart of different colored dots  Description automatically generated](img/B21047_08_02.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![一张不同颜色的点图  描述自动生成](img/B21047_08_02.png)'
- en: 'Figure 8.2: Plot of the original iris dataset'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2：原始鸢尾花数据集的绘图
- en: 'Assuming we know nothing about the label *y*, we try to cluster the data into
    three groups, as there seem to be three clusters in the preceding plot (or you
    might say two, which we will come back to later). Let’s perform *step 1*, *specifying
    k*, and *step 2*, *initializing centroids*, by randomly selecting three samples
    as the initial centroids:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们对标签 *y* 一无所知，我们尝试将数据分成三个组，因为前面的图中似乎有三个簇（或者你可能说有两个，稍后我们会回来讨论）。让我们执行*步骤 1*，*指定
    k*，以及*步骤 2*，*初始化质心*，通过随机选择三个样本作为初始质心：
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We visualize the data (without labels anymore) along with the initial random
    centroids:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可视化数据（不再带标签）以及初始的随机质心：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Refer to the following screenshot for the data, along with the initial random
    centroids:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见以下截图，查看数据及初始随机质心：
- en: '![A diagram of a line of dots  Description automatically generated](img/B21047_08_03.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的点线图示意图](img/B21047_08_03.png)'
- en: 'Figure 8.3: Data points with random centroids'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3：具有随机质心的数据点
- en: 'Now we perform *step 3*, which entails assigning clusters based on the nearest
    centroids. First, we need to define a function calculating distance, which is
    measured by the Euclidean distance, as demonstrated here:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们执行*步骤 3*，这包括基于最近的质心分配簇。首先，我们需要定义一个计算距离的函数，该距离通过欧几里得距离来度量，如下所示：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we develop a function that assigns a sample to the cluster of the nearest
    centroid:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开发一个函数，将样本分配给最近的质心簇：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'With the clusters assigned, we perform *step 4*, which involves updating the
    centroids to the mean of all samples in the individual clusters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 分配完簇后，我们执行*步骤 4*，即将质心更新为各个簇中所有样本的均值：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Finally, we have *step 5*, which involves repeating *step 3* and *step 4* until
    the model converges and whichever of the following occurs:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有*步骤 5*，这涉及重复*步骤 3*和*步骤 4*，直到模型收敛并且出现以下任一情况：
- en: Centroids move less than the pre-specified threshold
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心移动小于预设的阈值
- en: Sufficient iterations have been taken
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已经进行了足够的迭代
- en: 'We set the tolerance of the first condition and the maximum number of iterations
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们设置了第一个条件的容差和最大迭代次数，如下所示：
- en: '[PRE7]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Initialize the clusters’ starting values, along with the starting clusters
    for all samples, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化簇的起始值，并为所有样本初始化簇，如下所示：
- en: '[PRE8]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'With all the components ready, we can train the model iteration by iteration
    where it first checks convergence before performing *steps 3* and *4*, and then
    visualizes the latest centroids:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所有组件准备好后，我们可以逐轮训练模型，其中首先检查收敛性，然后执行*步骤 3*和*步骤 4*，最后可视化最新的质心：
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Let’s look at the following outputs generated from the preceding commands:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从前面的命令生成的以下输出：
- en: '**Iteration 1**: Take a look at the following output of iteration 1:'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代 1**：请查看迭代 1 的以下输出：'
- en: '[PRE10]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The plot of centroids after iteration 1 is as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代 1 后的质心图如下：
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_04.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的地图示意图](img/B21047_08_04.png)'
- en: 'Figure 8.4: k-means clustering result after the first round'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4：第一次迭代后的 k-means 聚类结果
- en: '**Iteration 2**: Take a look at the following output of iteration 2:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代 2**：请查看迭代 2 的以下输出：'
- en: '[PRE11]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The plot of centroids after iteration 2 is as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代 2 后的质心图如下：
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_05.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的地图示意图](img/B21047_08_05.png)'
- en: 'Figure 8.5: k-means clustering result after the second round'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5：第二轮迭代后的 k-means 聚类结果
- en: '**Iteration 6**: Take a look at the following output of iteration 6 (we herein
    skip iterations 3 to 5 to avoid tedium):'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代 6**：请查看迭代 6 的以下输出（在此我们跳过迭代 3 到 5，以避免冗长）：'
- en: '[PRE12]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The plot of centroids after iteration 6 is as follows:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代 6 后的质心图如下：
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_06.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的地图示意图](img/B21047_08_06.png)'
- en: 'Figure 8.6: k-means clustering result after the sixth round'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.6：第六轮迭代后的 k-means 聚类结果
- en: '**Iteration 7**: Take a look at the following output of iteration 7:'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代 7**：请查看迭代 7 的以下输出：'
- en: '[PRE13]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The plot of centroids after iteration 7 is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代 7 后的质心图如下：
- en: '![A diagram of a map  Description automatically generated](img/B21047_08_07.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![自动生成的地图示意图](img/B21047_08_07.png)'
- en: 'Figure 8.7: k-means clustering result after the seventh round'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.7：第七轮迭代后的 k-means 聚类结果
- en: 'The model converges after seven iterations. The resulting centroids look promising,
    and we can also plot the clusters:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在七次迭代后收敛。结果的聚类中心看起来很有希望，我们也可以绘制聚类：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Refer to the following screenshot for the end result:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅以下截图获取最终结果：
- en: '![A diagram of different colored dots  Description automatically generated](img/B21047_08_08.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色的点的示意图，描述自动生成](img/B21047_08_08.png)'
- en: 'Figure 8.8: Data samples along with learned cluster centroids'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8：数据样本及其学习到的聚类中心
- en: As you can see, samples around the same centroid form a cluster. After seven
    iterations (you might see slightly more or fewer iterations in your case if you
    change the random seed in `np.random.seed(0)`), the model converges and the centroids
    will no longer be updated.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，围绕相同聚类中心的样本形成了一个聚类。经过七次迭代（如果更改 `np.random.seed(0)` 中的随机种子，您可能会看到稍多或稍少的迭代次数），模型收敛，并且聚类中心不再更新。
- en: Implementing k-means with scikit-learn
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 实现 k-means
- en: 'Having developed our own k-means clustering model, we will now discuss how
    to use scikit-learn for a quicker solution by performing the following steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发了我们自己的 k-means 聚类模型后，我们将讨论如何通过执行以下步骤，使用 scikit-learn 进行更快速的解决方案：
- en: 'First, import the `KMeans` class and initialize a model with three clusters,
    as follows:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入 `KMeans` 类，并初始化一个包含三个聚类的模型，如下所示：
- en: '[PRE15]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The `KMeans` class takes in the following important parameters:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '`KMeans` 类接受以下重要参数：'
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **构造器参数** | **默认值** | **示例值** | **描述** |'
- en: '| `n_clusters` | `8` | `3, 5, 10` | *k* clusters |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| `n_clusters` | `8` | `3, 5, 10` | *k* 个聚类 |'
- en: '| `max_iter` | `300` | `10, 100, 500` | Maximum number of iterations |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| `max_iter` | `300` | `10, 100, 500` | 最大迭代次数 |'
- en: '| `tol` | `1e-4` | `1e-5, 1e-8` | Tolerance to declare convergence |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| `tol` | `1e-4` | `1e-5, 1e-8` | 声明收敛的容忍度 |'
- en: '| `random_state` | `None` | `0, 42` | Random seed for program reproducibility
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| `random_state` | `None` | `0, 42` | 用于程序可重复性的随机种子 |'
- en: 'Table 8.1: Parameters of the KMeans class'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1：KMeans 类的参数
- en: 'We then fit the model on the data:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将模型拟合到数据上：
- en: '[PRE16]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'After that, we can obtain the clustering results, including the clusters for
    data samples and centroids of individual clusters:'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们可以获得聚类结果，包括数据样本的聚类和各个聚类的中心：
- en: '[PRE17]'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Similarly, we plot the clusters along with the centroids:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，我们绘制聚类及其中心：
- en: '[PRE18]'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'This will result in the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![A diagram of different colored dots  Description automatically generated](img/B21047_08_09.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色的点的示意图，描述自动生成](img/B21047_08_09.png)'
- en: 'Figure 8.9: Data samples along with learned cluster centroids using scikit-learn'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.9：使用 scikit-learn 学到的聚类中心及数据样本
- en: We get a similar result to the previous one using the model we implemented from
    scratch.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们从头实现的模型，我们得到与之前相似的结果。
- en: Choosing the value of k
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择 k 的值
- en: Let’s return to our earlier discussion on what the right value for *k* is. In
    the preceding example, it is more intuitive to set it to `3` since we know there
    are three classes in total. However, in most cases, we don’t know how many groups
    are sufficient or efficient, and meanwhile, the algorithm needs a specific value
    of *k* to start with. So, how can we choose the value of *k*? There is a famous
    heuristic approach called the **elbow method**.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到之前讨论的 *k* 值的问题。在前面的例子中，设置为 `3` 更直观，因为我们知道总共有三个类。然而，在大多数情况下，我们并不知道多少个群组是足够的或高效的，同时算法需要一个特定的
    *k* 值来开始。那么，我们该如何选择 *k* 的值呢？有一种著名的启发式方法，叫做 **肘部法则**。
- en: In the elbow method, different values of *k* are chosen and corresponding models
    are trained; for each trained model, the **sum of squared errors**, or **SSE**
    (also called the **sum of within-cluster distances**), of centroids is calculated
    and is plotted against *k*. Note that for one cluster, the squared error (or the
    within-cluster distance) is computed as the sum of the squared distances from
    individual samples in the cluster to the centroid. The optimal *k* is chosen where
    the marginal drop of SSE starts to decrease dramatically. This means that further
    clustering does not provide any substantial gain.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在肘部法则中，选择不同的 *k* 值并训练相应的模型；对于每个训练好的模型，计算 **平方误差和**（**SSE**，也称为 **聚类内距离和**），并将其与
    *k* 绘制在一起。请注意，对于一个聚类，平方误差（或聚类内距离）是通过计算每个样本到聚类中心的平方距离之和来得出的。选择最优的 *k* 值是在 SSE 的边际下降开始急剧减小时，这意味着进一步的聚类不再提供任何实质性的提升。
- en: 'Let’s apply the elbow method to the example we covered in the previous section
    (learning by example is what this book is all about). We perform k-means clustering
    under different values of *k* on the `iris` data:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将肘部法则应用于我们在前一节中介绍的示例（通过示例学习正是本书的重点）。我们在`iris`数据上对不同的*k*值执行k-means聚类：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We use the whole feature space and `k` ranges from `1` to `6`. Then, we train
    individual models and record the resulting SSE, respectively:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用完整的特征空间，并将`k`值范围从`1`到`6`。然后，我们训练各个模型，并分别记录结果的SSE：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, we plot the SSE versus the various `k` ranges, as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们绘制SSE与不同`k`范围的关系图，如下所示：
- en: '[PRE21]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will result in the following output:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '![A picture containing screenshot, line, rectangle, plot  Description automatically
    generated](img/B21047_08_10.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含屏幕截图、线条、矩形、图表的图片  描述自动生成](img/B21047_08_10.png)'
- en: 'Figure 8.10: k-means elbow – SSE versus k'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10：k-means肘部法则 – SSE与k的关系
- en: '**Best practice**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Choosing the right similarity measure for distance calculation in k-means clustering
    depends on the nature of your data and the specific goals of your analysis. Some
    common similarity measures include the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 选择适当的相似度度量来计算k-means聚类中的距离，取决于数据的性质和分析的具体目标。一些常见的相似度度量包括以下几种：
- en: '**Euclidean distance**: This default measure is suitable for continuous data
    where the difference between feature values matters.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欧几里得距离**：这是默认的度量方法，适用于连续数据，其中特征值之间的差异很重要。'
- en: '**Manhattan distance (also known as L1 norm)**: This calculates the sum of
    the absolute differences between the coordinates of two points. It is suitable
    for high-dimensional data and when the dimensions are not directly comparable.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿距离（也称为L1范数）**：它计算两点坐标之间绝对差的总和。适用于高维数据以及维度之间不可直接比较的情况。'
- en: '**Cosine similarity**: This is useful for text data or data represented as
    vectors where the magnitude of the vectors is less important than the orientation.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦相似度**：对于文本数据或表示为向量的数据，余弦相似度非常有用，因为在这种情况下，向量的大小不如方向重要。'
- en: '**Jaccard similarity**: This measures the similarity between two sets by comparing
    their intersection to their union. It is commonly used for binary or categorical
    data.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**杰卡德相似度**：通过比较两个集合的交集与并集来衡量它们之间的相似度。通常用于二元或类别数据。'
- en: Apparently, the elbow point is `k=3`, since the drop in SSE slows down dramatically
    right after `3`. Hence, `k=3` is an optimal solution in this case, which is consistent
    with the fact that there are three classes of flowers.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，肘部点是`k=3`，因为在`3`之后，SSE的下降速度显著减缓。因此，`k=3`是此情况的最优解，这与有三类花朵的事实一致。
- en: Clustering newsgroups dataset
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类新闻组数据集
- en: You should now be very familiar with k-means clustering. Next, let’s see what
    we are able to mine from the newsgroups dataset using this algorithm. We will
    use all the data from four categories, `'alt.atheism'`, `'talk.religion.misc'`,
    `'comp.graphics'`, and `'sci.space'`, as an example. We will then use ChatGPT
    to describe the generated newsgroup clusters. ChatGPT can generate natural language
    descriptions of the clusters formed by k-means clustering. This can help in understanding
    the characteristics and themes of each cluster.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你应该对k-means聚类非常熟悉了。接下来，让我们看看使用该算法能够从新闻组数据集中挖掘出什么。我们将以`'alt.atheism'`、`'talk.religion.misc'`、`'comp.graphics'`和`'sci.space'`四个类别的所有数据为例。然后，我们将使用ChatGPT来描述生成的新闻组聚类。ChatGPT可以生成关于k-means聚类形成的聚类的自然语言描述。这有助于理解每个聚类的特征和主题。
- en: Clustering newsgroups data using k-means
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用k-means对新闻组数据进行聚类
- en: 'We first load the data from those newsgroups and preprocess it as we did in
    *Chapter 7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从这些新闻组加载数据，并按*第7章*中所示进行预处理，*使用文本分析技术挖掘20个新闻组数据集*：
- en: '[PRE22]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We then convert the cleaned text data into count vectors using `CountVectorizer`
    from scikit-learn:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用scikit-learn中的`CountVectorizer`将清理后的文本数据转换为计数向量：
- en: '[PRE23]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that the vectorizer we use here does not limit the number of features (word
    tokens), but the minimum and maximum document frequency (`min_df` and `max_df`),
    which are 2% and 50% of the dataset, respectively. The **document frequency**
    of a word is measured by the fraction of documents (samples) in the dataset that
    contain this word. This helps filter out rare or spurious terms that may not be
    relevant to the analysis.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们在这里使用的向量化器并不限制特征（词项）数量，而是限制最小和最大文档频率（`min_df`和`max_df`），它们分别是数据集的2%和50%。一个词的**文档频率**是通过计算数据集中包含该词的文档（样本）的比例来衡量的。这有助于过滤掉那些稀有或无关的术语。
- en: 'With the input data ready, we will now try to cluster them into four groups
    as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据准备好后，我们现在将尝试将其聚类为四个组，具体如下：
- en: '[PRE24]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Let’s do a quick check on the sizes of the resulting clusters:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速检查一下结果聚类的大小：
- en: '[PRE25]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The clusters don’t look absolutely correct, with most samples (`3360` samples)
    congested in one big cluster (cluster 3). What could have gone wrong? It turns
    out that our count-based features are not sufficiently representative. A better
    numerical representation for text data is the **term frequency-inverse document
    frequency** (**tf-idf**). Instead of simply using the token count, or the so-called
    **term frequency** (**tf**), it assigns each term frequency a weighting factor
    that is inversely proportional to the document frequency. In practice, the **idf**
    factor of a term *t* in documents *D* is calculated as follows:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类看起来并不完全正确，大多数样本（`3360`个样本）聚集在一个大聚类（聚类3）中。可能出了什么问题？事实证明，我们基于计数的特征不够具有代表性。对于文本数据，更好的数值表示是**词频-逆文档频率**（**tf-idf**）。与其简单地使用词项计数，或者所谓的**词频**（**tf**），它为每个词项频率分配一个与文档频率成反比的加权因子。实际上，术语*t*在文档*D*中的**idf**因子计算如下：
- en: '![](img/B21047_08_001.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_08_001.png)'
- en: Here, *n*[D] is the total number of documents, *n*[t] is the number of documents
    containing the term *t*, and *1* is added to avoid division by 0.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*n*[D]是文档的总数，*n*[t]是包含术语*t*的文档数，*1*是为了避免除以0而加上的常数。
- en: With the `idf` factor incorporated, the `tf-idf` representation diminishes the
    weight of common terms (such as *get* and *make*) and emphasizes terms that rarely
    occur but convey an important meaning.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在加入`idf`因子后，`tf-idf`表示法减少了常见术语（例如*get*和*make*）的权重，强调那些很少出现但传达重要意义的术语。
- en: 'To use the `tf-idf` representation, we just need to replace `CountVectorizer`
    with `TfidfVectorizer` from scikit-learn as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`tf-idf`表示法，我们只需将`CountVectorizer`替换为scikit-learn中的`TfidfVectorizer`，如下所示：
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The parameter `max_df` is used to ignore terms that have a document frequency
    higher than the given threshold. In this case, terms that appear in more than
    50% of the documents will be ignored during the vectorization process. `min_df`
    specifies the minimum document frequency required for a term to be included in
    the output. Terms that appear in fewer than two documents will be ignored.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`max_df`用于忽略那些文档频率高于给定阈值的术语。在本例中，出现在超过50%的文档中的术语将在向量化过程中被忽略。`min_df`指定了一个术语被包含在输出中的最小文档频率要求。那些出现在少于两个文档中的术语将被忽略。
- en: 'Now, redo feature extraction using the `tf-idf` vectorizer and the k-means
    clustering algorithm on the resulting feature space:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用`tf-idf`向量化器和k-means聚类算法在结果特征空间中重新进行特征提取：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The clustering result becomes more reasonable.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类结果变得更加合理。
- en: 'We also take a closer look at the clusters by examining what they contain and
    the top 10 terms (the terms with the 10 highest tf-idf scores) representing each
    cluster:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还通过检查每个聚类包含的内容以及表示每个聚类的前10个术语（具有最高tf-idf得分的10个术语）来更深入地观察聚类结果：
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'From what we observe in the preceding results:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们在前面的结果中观察到的情况来看：
- en: '`cluster_0` is obviously about space and includes almost all `sci.space` samples
    and related terms such as `orbit`, `moon`, `nasa`, `launch`, `shuttle`, and `space`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster_0`显然是关于太空的，并且包含几乎所有的`sci.space`样本以及相关术语，如`orbit`，`moon`，`nasa`，`launch`，`shuttle`和`space`。'
- en: '`cluster_1` is more of a generic topic'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster_1`是更为通用的主题。'
- en: '`cluster_2` is more about computer graphics and related terms, such as `computer`,
    `program`, `file`, `graphic`, and `image`'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster_2`更多的是关于计算机图形学和相关术语，例如`computer`，`program`，`file`，`graphic`和`image`。'
- en: '`cluster_3` is an interesting one, which successfully brings together two overlapping
    topics, atheism and religion, with key terms including `bible`, `believe`, `jesus`,
    `christian`, and `god`'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cluster_3` 是一个有趣的簇，成功地将两个重叠的主题——无神论和宗教——汇集在一起，关键术语包括`bible`、`believe`、`jesus`、`christian`和`god`'
- en: Feel free to try different values of `k`, or use the elbow method to find the
    optimal one (this is actually an exercise later in this chapter).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 随时尝试不同的 `k` 值，或使用肘部法找到最佳值（这实际上是本章后面的一个练习）。
- en: It is quite interesting to find key terms for each text group via clustering.
    It will be more fun if we can describe each cluster based on its key terms. Let’s
    see how we do so with ChatGPT in the next section.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过聚类找到每个文本组的关键术语非常有趣。如果我们能够根据其关键术语描述每个簇，那将更有趣。让我们看看如何在下一节中通过 ChatGPT 进行此操作。
- en: Describing the clusters using GPT
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 GPT 描述簇
- en: '**ChatGPT** ([https://chat.openai.com/](https://chat.openai.com/)) is an AI
    language model developed by **OpenAI** ([https://openai.com/](https://openai.com/)).
    It is part of the **Generative Pre-trained Transformer** (**GPT**) family of models,
    specifically based on GPT-3.5 (GPT-4 is in beta at the time of writing) architecture.
    ChatGPT is designed to engage in natural language conversations with users and
    provide human-like responses.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**ChatGPT** ([https://chat.openai.com/](https://chat.openai.com/)) 是由 **OpenAI**
    ([https://openai.com/](https://openai.com/)) 开发的 AI 语言模型，是 **生成预训练变换器**（**GPT**）系列模型的一部分，具体基于
    GPT-3.5（在撰写时 GPT-4 处于测试阶段）架构。ChatGPT 设计用于与用户进行自然语言对话，并提供类似人类的响应。'
- en: The model is trained on a vast amount of diverse text data from the internet,
    allowing it to understand and generate human-like text across a wide range of
    topics and contexts. ChatGPT can comprehend questions, prompts, and instructions
    given by users and generate coherent responses based on its training.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在大量来自互联网的多样文本数据上进行了训练，使其能够理解并生成各种主题和背景下类似人类的文本。ChatGPT 能够理解用户提出的问题、提示和指令，并根据其训练生成连贯的响应。
- en: ChatGPT has been used in various applications, including chatbots, virtual assistants,
    content generation, language translation, and more. Users interact with ChatGPT
    through API calls or interactive interfaces, and the model generates responses
    in real time. However, it is essential to note that while ChatGPT can produce
    impressive and contextually relevant responses, it may also occasionally generate
    incorrect or nonsensical answers due to the limitations of current language models.
    ChatGPT responses should be sense-checked to improve the quality and reliability
    of the generated text and minimize the risk of misinformation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 已在各种应用中使用，包括聊天机器人、虚拟助手、内容生成、语言翻译等等。用户通过 API 调用或交互界面与 ChatGPT 进行交互，模型实时生成响应。然而，需要注意的是，虽然
    ChatGPT 能够生成印象深刻且与上下文相关的响应，但由于当前语言模型的限制，它偶尔也可能生成不正确或无意义的答案。应对 ChatGPT 的响应应进行意义检查，以提高生成文本的质量和可靠性，并尽量减少误信息的风险。
- en: We will ask ChatGPT to describe the clusters we just generated in the following
    steps.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将请求 ChatGPT 描述我们刚生成的簇的步骤如下。
- en: 'First, we obtain the top 100 terms as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们如下获取前 100 个术语：
- en: '[PRE29]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'After signing up (or logging in if you have an account) at [https://chat.openai.com](https://chat.openai.com),
    we ask ChatGPT to describe the topic based on these keywords using the prompt
    `Describe a common topic based on the following keywords:`. Refer to the following
    screenshot for the entire question and answer:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [https://chat.openai.com](https://chat.openai.com) 注册（或登录如果您已有账户）后，我们通过使用提示`基于以下关键词描述一个共同的主题：`要求
    ChatGPT 描述主题。请参考下图以获取完整的问题和答案：
- en: '![A screenshot of a computer  Description automatically generated](img/B21047_08_11.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图 说明已自动生成](img/B21047_08_11.png)'
- en: 'Figure 8.11: Asking ChatGPT to describe the topic of cluster 0'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '图 8.11: 要求 ChatGPT 描述第 0 簇的主题'
- en: 'As ChatGPT pointed out correctly, `"the common topic revolves around the various
    aspects of space exploration, research, technology, and missions, with mentions
    of key players and celestial bodies in the field."` Feel free to repeat the same
    process for other clusters. You can also achieve the same using the ChatGPT API
    in Python by following these steps:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 正如 ChatGPT 正确指出的那样，`“共同的主题围绕着太空探索、研究、技术和任务的各个方面，提到该领域的关键人物和天体。”` 随时可以对其他簇重复相同的过程。您还可以通过遵循以下步骤在
    Python 中使用 ChatGPT API 实现相同效果：
- en: 'Install the OpenAI library with `pip`:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`pip`安装OpenAI库：
- en: '[PRE30]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'You can also do this with `conda`:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用`conda`进行此操作：
- en: '[PRE31]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Generate an API key at [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys).
    Note that you will need to log in or sign up to do this.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 [https://platform.openai.com/account/api-keys](https://platform.openai.com/account/api-keys)
    生成一个 API 密钥。请注意，你需要先登录或注册才能进行此操作。
- en: 'Import the library and set your API key:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并设置你的 API 密钥：
- en: '[PRE32]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Create a function that allows you to obtain a response from ChatGPT:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个函数，允许你从 ChatGPT 获取响应：
- en: '[PRE33]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Here, we use the `text-davinci-003` model. Check out the page at [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)
    for more information on the various models available.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用 `text-davinci-003` 模型。欲了解更多有关不同模型的信息，请访问 [https://platform.openai.com/docs/models](https://platform.openai.com/docs/models)。
- en: 'Query the API:'
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查询 API：
- en: '[PRE34]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: This will yield a response similar to what you previously read in the web interface.
    Note that the API call is subject to your plan quota.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回一个类似于你在网页界面中看到的响应。请注意，API 调用受到你所选计划配额的限制。
- en: Up to this point, we have produced topical keywords by first grouping documents
    into clusters and subsequently extracting the top terms within each cluster. **Topic
    modeling** is another approach to produce topical keywords but in a much more
    direct way. It does not simply search for the key terms in individual clusters
    generated beforehand. What it does is directly extract collections of key terms
    from documents. You will see how this works in the next section.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们通过首先将文档分组为聚类，然后提取每个聚类中的顶级术语，来生成主题关键词。**主题建模**是另一种生成主题关键词的方法，但它以一种更加直接的方式进行。它不仅仅是搜索预先生成的单独聚类中的关键术语。它的做法是直接从文档中提取关键术语的集合。你将在下一节中看到它是如何工作的。
- en: '**Density-Based Spatial Clustering of Applications with Noise** (**DBSCAN**)
    is another popular clustering algorithm used for identifying clusters in spatial
    data. Unlike centroid-based algorithms like k-means, DBSCAN does not require specifying
    the number of clusters in advance and can discover clusters of arbitrary shapes.
    It works by partitioning the dataset into clusters of contiguous high-density
    regions, separated by regions of low density, while also identifying outliers
    as noise.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于密度的空间聚类应用与噪声** (**DBSCAN**) 是另一种流行的聚类算法，用于识别空间数据中的聚类。与像 k-means 这样的基于质心的算法不同，DBSCAN
    不需要提前指定聚类的数量，并且可以发现任意形状的聚类。它通过将数据集划分为连续的高密度区域的聚类，这些区域由低密度区域分隔，同时将异常值标记为噪声。'
- en: 'The algorithm requires two parameters: epsilon (![](img/B21047_08_002.png)),
    which defines the maximum distance between two samples for them to be considered
    as part of the same neighborhood, and `min_samples`, which specifies the minimum
    number of samples required to form a dense region.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法需要两个参数：epsilon (![](img/B21047_08_002.png))，定义了两个样本之间的最大距离，使其被视为同一邻域的一部分，以及
    `min_samples`，它指定了形成密集区域所需的最小样本数。
- en: DBSCAN starts by randomly selecting a point and expanding its neighborhood to
    find all reachable points within ε distance. If the number of reachable points
    exceeds `min_samples`, the point is labeled as a core point and a new cluster
    is formed. The process is repeated recursively for all core points and their neighborhoods
    until all points are assigned to a cluster or labeled as noise.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 从随机选择一个点开始，扩展其邻域，找到所有在 ε 距离内可达的点。如果可达点的数量超过 `min_samples`，该点被标记为核心点，并形成一个新的聚类。这个过程会对所有核心点及其邻域递归执行，直到所有点都被分配到某个聚类中或标记为噪声。
- en: Discovering underlying topics in newsgroups
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现新闻组中的潜在主题
- en: A **topic model** is a type of statistical model for discovering the probability
    distributions of words linked to the topic. The topic in topic modeling does not
    exactly match the dictionary definition but corresponds to a nebulous statistical
    concept, which is an abstraction that occurs in a collection of documents.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题模型**是一种用于发现与主题相关的词汇概率分布的统计模型。在主题建模中，"主题"的定义并不完全符合字典中的解释，而是对应于一个模糊的统计概念，它是文档集合中出现的一个抽象。'
- en: 'When we read a document, we expect certain words appearing in the title or
    the body of the text to capture the semantic context of the document. An article
    about Python programming might have words such as *class* and *function*, while
    a story about snakes might have words such as *eggs* and *afraid*. Documents usually
    have multiple topics; for instance, this section is about three things: topic
    modeling, non-negative matrix factorization, and latent Dirichlet allocation,
    which we will discuss shortly. We can therefore define an additive model for topics
    by assigning different weights to topics.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们阅读一篇文档时，我们期望某些出现在标题或正文中的词汇能够捕捉文档的语义上下文。例如，一篇关于Python编程的文章可能会包含*class*和*function*等词汇，而一篇关于蛇的故事可能会包含*eggs*和*afraid*等词汇。文档通常涉及多个主题；例如，本节讨论的内容包括三项内容：主题建模、非负矩阵分解和潜在狄利克雷分配，我们将很快讨论这些内容。因此，我们可以通过为每个主题分配不同的权重来定义一个加法模型。
- en: '**Topic modeling** is widely used for mining hidden semantic structures in
    given text data. There are two popular topic modeling algorithms—**non-negative
    matrix factorization** (**NMF**) and **latent Dirichlet allocation** (**LDA**).
    We will go through both of these in the next two sections.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**主题建模**广泛应用于挖掘给定文本数据中的潜在语义结构。有两种流行的主题建模算法——**非负矩阵分解**（**NMF**）和**潜在狄利克雷分配**（**LDA**）。我们将在接下来的两节中介绍这两种算法。'
- en: Topic modeling using NMF
  id: totrans-201
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用NMF进行主题建模
- en: '**Non-negative matrix factorization** (**NMF**) is a dimensionality reduction
    technique used for feature extraction and data representation. It factorizes a
    non-negative input matrix, **V**, into a product of two smaller matrices, **W**
    and **H**, in such a way that these three matrices have no negative values. These
    two lower-dimensional matrices represent features and their associated coefficients.
    In the context of NLP, these three matrices have the following meanings:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '**非负矩阵分解**（**NMF**）是一种用于特征提取和数据表示的降维技术。它将一个非负输入矩阵**V**分解为两个较小矩阵**W**和**H**的乘积，使得这三个矩阵都不含负值。这两个低维矩阵表示特征及其关联的系数。在自然语言处理的背景下，这三个矩阵具有以下含义：'
- en: The input matrix **V** is the term count or tf-idf matrix of size *n* * *m*,
    where *n* is the number of documents or samples, and *m* is the number of terms.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入矩阵**V**是一个大小为*n* × *m*的词项计数或tf-idf矩阵，其中*n*是文档或样本的数量，*m*是词项的数量。
- en: The first decomposition output matrix **W** is the feature matrix of size *t*
    * *m*, where *t* is the number of topics specified. Each row of **W** represents
    a topic with each element in the row representing the rank of a term in the topic.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个分解输出矩阵**W**是一个大小为*t* × *m*的特征矩阵，其中*t*是指定的主题数量。**W**的每一行表示一个主题，每行中的每个元素表示该主题中某个词项的排名。
- en: The second decomposition output matrix **H** is the coefficient matrix of size
    *n* * *t*. Each row of **H** represents a document, with each element in the row
    representing the weight of a topic within the document.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个分解输出矩阵**H**是一个大小为*n* × *t*的系数矩阵。**H**的每一行表示一个文档，每个元素表示该文档中某个主题的权重。
- en: 'How to derive the computation of **W** and **H** is beyond the scope of this
    book. However, you can refer to the following example to get a better sense of
    how NMF works:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如何推导**W**和**H**的计算超出了本书的范围。然而，你可以参考以下示例，以更好地理解NMF是如何工作的：
- en: '![A picture containing text, diagram, screenshot, line  Description automatically
    generated](img/B21047_08_12.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, diagram, screenshot, line  Description automatically
    generated](img/B21047_08_12.png)'
- en: 'Figure 8.12: Example of matrix W and matrix H derived from an input matrix
    V'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12：从输入矩阵V中得到的矩阵W和矩阵H示例
- en: If you are interested in reading more about NMF, feel free to check out the
    original paper *Generalized Nonnegative Matrix Approximations with Bregman Divergences*,
    by Inderjit S. Dhillon and Suvrit Sra, in NIPS 2005.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对阅读更多关于NMF的内容感兴趣，可以查看Inderjit S. Dhillon和Suvrit Sra在NIPS 2005中发布的原始论文《*Generalized
    Nonnegative Matrix Approximations with Bregman Divergences*》。
- en: 'Let’s now apply NMF to our newsgroups data. Scikit-learn has a nice module
    for decomposition that includes NMF:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将NMF应用到我们的新闻组数据上。Scikit-learn有一个很好的分解模块，其中包含NMF：
- en: '[PRE35]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We specify 20 topics (`n_components`) as an example. Important parameters of
    the model are included in the following table:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了20个主题（`n_components`）作为示例。模型的主要参数包括在下表中：
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| **构造器参数** | **默认值** | **示例值** | **描述** |'
- en: '| `n_components` | `None` | `5`, `10`, `20` | Number of components—in the context
    of topic modeling, this corresponds to the number of topics. If `None`, it becomes
    the number of input features. |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| `n_components` | `None` | `5`、`10`、`20` | 组件数量——在话题建模中，这对应于话题的数量。如果是 `None`，则为输入特征的数量。
    |'
- en: '| `max_iter` | `200` | `100`, `200` | Maximum number of iterations. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| `max_iter` | `200` | `100`、`200` | 最大迭代次数。 |'
- en: '| `tol` | `1e-4` | `1e-5`, `1e-8` | Tolerance to declare convergence. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| `tol` | `1e-4` | `1e-5`, `1e-8` | 声明收敛的容差。 |'
- en: 'Table 8.2: Parameters of the NMF class'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2：NMF 类的参数
- en: 'We used the term matrix as input to the NMF model, but you could also use the
    `tf-idf` one instead. Now, fit the NMF model, `nmf`, on the term matrix, `data_cv`:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将词条矩阵作为输入传递给 NMF 模型，但你也可以使用 `tf-idf` 矩阵。现在，拟合 NMF 模型 `nmf`，基于词条矩阵 `data_cv`：
- en: '[PRE36]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can obtain the resulting topic feature rank **W** after the model is trained:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在模型训练完成后获得结果话题特征排名 **W**：
- en: '[PRE37]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'For each topic, we display the top 10 terms based on their ranks:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个话题，我们根据其排名显示前 10 个词条：
- en: '[PRE38]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'There are a number of interesting topics, for instance:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多有趣的话题，例如：
- en: Computer graphics-related topics, such as `0`, `2`, `6`, and `8`
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与计算机图形学相关的主题，如 `0`、`2`、`6` 和 `8`
- en: Space-related ones, such as `3`, `4`, and `9`
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与太空相关的主题，如 `3`、`4` 和 `9`
- en: Religion-related ones, such as `5`, `7`, and `13`
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与宗教相关的，如 `5`、`7` 和 `13`
- en: Some topics, such as `1` and `12`, are hard to interpret. This is totally fine
    since topic modeling is a kind of free-form learning.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 一些话题，例如 `1` 和 `12`，很难解释。这完全正常，因为话题建模是一种自由形式的学习方法。
- en: Topic modeling using LDA
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 LDA 进行话题建模
- en: Let’s explore another popular topic modeling algorithm, **Latent Dirichlet Allocation**
    (**LDA**). LDA is a generative probabilistic graphical model that explains each
    input document by means of a mixture of topics with certain probabilities. It
    assumes that each document is a mixture of multiple topics, and each topic is
    characterized by a specific word probability distribution. The algorithm iteratively
    assigns words in documents to topics and updates the topic distributions based
    on the observed word co-occurrences. Again, **topic** in topic modeling means
    a collection of words with a certain connection. In other words, LDA basically
    deals with two probability values, *P*(*term* V *topic*) and *P*(*topic* V *document*).
    This can be difficult to understand at the beginning. So, let’s start from the
    bottom, the end result of an LDA model.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索另一个流行的话题建模算法——**潜在狄利克雷分配**（**LDA**）。LDA 是一种生成式概率图模型，通过一定概率的主题混合来解释每个输入文档。它假设每个文档是多个话题的混合，每个话题由特定的词概率分布表示。该算法通过迭代将文档中的词分配给话题，并根据观察到的词共现更新话题分布。再说一次，话题建模中的**话题**是指具有某种联系的词的集合。换句话说，LDA
    基本上处理的是两个概率值，*P*(*term* V *topic*) 和 *P*(*topic* V *document*)。刚开始时这可能很难理解。所以，让我们从最基础的地方开始，LDA
    模型的最终结果。
- en: 'Let’s take a look at the following set of documents:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下以下文档集：
- en: '[PRE39]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Now, let’s say we want two topics. The topics derived from these documents
    may appear as follows:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想要两个话题。由这些文档派生的话题可能如下所示：
- en: '[PRE40]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Therefore, we find how each document is represented by these two topics:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们找到每个文档如何通过这两个话题来表示：
- en: '[PRE41]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'After seeing a toy example, we come back to its learning procedure:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到一个玩具示例后，我们回到它的学习过程：
- en: Specify the number of topics, *T*. Now we have topics 1, 2, …, and *T*.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定话题的数量 *T*。现在我们有话题 1、2、……、*T*。
- en: For each document, randomly assign one of the topics to each term in the document.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档，随机为文档中的每个词分配一个话题。
- en: For each document, calculate *P*(*topic* = *t* V *document*), which is the proportion
    of terms in the document that are assigned to the topic *t*.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个文档，计算 *P*(*topic* = *t* V *document*)，即文档中分配给话题 *t* 的词条所占比例。
- en: For each topic, calculate *P*(*term* = *w* V *topic*), which is the proportion
    of term *w* among all terms that are assigned to the topic.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个话题，计算 *P*(*term* = *w* V *topic*)，即词条 *w* 在所有分配给该话题的词条中的比例。
- en: For each term *w*, reassign its topic based on the latest probabilities *P*(*topic*
    = *t* V *document*) and *P*(*term* = *w* V *topic* = *t*).
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个词条 *w*，根据最新的概率 *P*(*topic* = *t* V *document*) 和 *P*(*term* = *w* V *topic*
    = *t*) 重新分配其话题。
- en: Repeat *steps 3* to *5* under the latest topic distributions for each iteration.
    The training stops if the model converges or reaches the maximum number of iterations.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，重复 *步骤 3* 到 *步骤 5*，并基于最新的话题分布进行训练。如果模型收敛或达到最大迭代次数，则停止训练。
- en: LDA is trained in a generative manner, where it tries to abstract from the documents
    a set of hidden topics that are likely to generate a certain collection of words.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: LDA是以生成方式进行训练的，它试图从文档中抽象出一组隐藏的主题，这些主题可能会生成某些特定的单词集合。
- en: 'With all this in mind, let’s see LDA in action. The LDA model is also included
    in scikit-learn:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，LDA模型在实际应用中的表现。LDA模型也包括在scikit-learn库中：
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Again, we specify 20 topics (`n_components`). The key parameters of the model
    are included in the following table:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们指定了20个主题（`n_components`）。模型的关键参数包括在下表中：
- en: '| **Constructor parameter** | **Default value** | **Example values** | **Description**
    |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| **构造函数参数** | **默认值** | **示例值** | **描述** |'
- en: '| `n_components` | `10` | `5, 10, 20` | Number of components—in the context
    of topic modeling, this corresponds to the number of topics. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| `n_components` | `10` | `5, 10, 20` | 组件数——在主题建模的上下文中，这对应于主题的数量。 |'
- en: '| `learning_method` | `"batch"` | `"online", "batch"` | In `batch` mode, all
    training data is used for each update. In `online` mode, a mini-batch of training
    data is used for each update. In general, if the data size is large, the `online`
    mode is faster. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| `learning_method` | `"batch"` | `"online", "batch"` | 在`batch`模式下，每次更新都使用所有训练数据。在`online`模式下，每次更新都使用一个小批量的训练数据。一般来说，如果数据量大，`online`模式更快。
    |'
- en: '| `max_iter` | `10` | `10, 20` | Maximum number of iterations. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| `max_iter` | `10` | `10, 20` | 最大迭代次数。 |'
- en: '| `randome_state` | `None` | `0, 42` | Seed used by the random number generator.
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| `randome_state` | `None` | `0, 42` | 随机数生成器使用的种子。 |'
- en: 'Table 8.3: Parameters of the LatentDirichletAllocation class'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3：LatentDirichletAllocation类的参数
- en: 'For the input data to LDA, remember that LDA only takes in term counts as it
    is a probabilistic graphical model. This is unlike NMF, which can work with both
    the term count matrix and the tf-idf matrix as long as they are non-negative data.
    Again, we use the term matrix defined previously as input to the LDA model. Now,
    we fit the LDA model on the term matrix, `data_cv`:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 对于LDA的输入数据，请记住LDA只接受词频计数，因为它是一个概率图模型。这不同于NMF，后者可以使用词频矩阵或tf-idf矩阵，只要它们是非负数据。再次，我们使用之前定义的词矩阵作为LDA模型的输入。现在，我们在词矩阵`data_cv`上拟合LDA模型：
- en: '[PRE43]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'We can obtain the resulting topic term rank after the model is trained:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在模型训练完成后获得结果的主题词排名：
- en: '[PRE44]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Similarly, for each topic, we display the top 10 terms based on their ranks
    as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，对于每个主题，我们根据它们的排名显示前10个词，如下所示：
- en: '[PRE45]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'There are a number of interesting topics that we just mined, for instance:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚挖掘出了一些有趣的话题，例如：
- en: Computer graphics-related topics, such as `2`, `5`, `6`, `8`, and `19`
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与计算机图形学相关的话题，如`2`、`5`、`6`、`8`和`19`
- en: Space-related ones, such as `10`, `11`, `12`, and `15`
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与空间相关的话题，如`10`、`11`、`12`和`15`
- en: Religion-related ones, such as `0` and `13`
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与宗教相关的话题，如`0`和`13`
- en: There are also topics involving noise, for example, `9` and `16`, which may
    require some imagination to interpret. Once more, this observation is entirely
    expected, given that LDA or topic modeling, as mentioned before, falls under the
    category of free-form learning.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一些涉及噪声的话题，例如`9`和`16`，这些可能需要一些想象力来进行解释。再一次，考虑到LDA或主题建模，如前所述，属于自由形式学习的范畴，这种观察是完全可以预期的。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The project in this chapter was about finding hidden similarities underneath
    newsgroups data, be it semantic groups, themes, or word clouds. We started with
    what unsupervised learning does and the typical types of unsupervised learning
    algorithms. We then introduced unsupervised learning clustering and studied a
    popular clustering algorithm, k-means, in detail. We also explored using ChatGPT
    to describe the topics of individual clusters based on their keywords.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的项目是关于在新闻组数据中找到隐藏的相似性，无论是语义组、主题还是词云。我们从无监督学习的功能和典型的无监督学习算法类型开始。接着，我们介绍了无监督学习中的聚类，并详细研究了一种流行的聚类算法——k均值算法。我们还探讨了如何使用ChatGPT根据关键词描述各个聚类的主题。
- en: We also talked about tf-idf as a more efficient feature extraction tool for
    text data. After that, we performed k-means clustering on the newsgroups data
    and obtained four meaningful clusters. After examining the key terms in each resulting
    cluster, we went straight to extracting representative terms among original documents
    using topic modeling techniques. Two powerful topic modeling approaches, NMF and
    LDA, were discussed and implemented. Finally, we had some fun interpreting the
    topics we obtained from both methods.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了 tf-idf 作为文本数据的更高效的特征提取工具。之后，我们对新闻组数据进行了 k-means 聚类，并得到了四个有意义的聚类。通过检查每个聚类中的关键术语，我们直接使用主题建模技术从原始文档中提取代表性术语。我们讨论并实现了两种强大的主题建模方法——NMF
    和 LDA。最后，我们通过解释这两种方法得到的主题，玩得不亦乐乎。
- en: So far, we have covered all the main categories of unsupervised learning, including
    dimensionality reduction, clustering, and topic modeling, which is also dimensionality
    reduction in a way.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了所有主要的无监督学习类别，包括降维、聚类和主题建模，后者在某种程度上也是一种降维方法。
- en: In the next chapter, we will talk about **Support Vector Machines** (**SVMs**)
    for face recognition. SVM is a popular choice for a wide range of classification
    and regression tasks, especially when dealing with complex decision boundaries.
    We will also cover another dimensionality reduction technique called principal
    component analysis.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将讨论 **支持向量机** (**SVMs**) 在人脸识别中的应用。SVM 是一种广泛应用于各类分类和回归任务的流行选择，尤其是在处理复杂决策边界时。我们还将介绍另一种降维技术——主成分分析。
- en: Exercises
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Ask ChatGPT to describe other clusters we generated through k-means clustering.
    You may experiment with various prompts and discover intriguing information within
    these clusters.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请让 ChatGPT 描述我们通过 k-means 聚类生成的其他聚类。你可以尝试不同的提示词，发现这些聚类中的有趣信息。
- en: Perform k-means clustering on newsgroups data using different values of *k*,
    or use the elbow method to find the optimal one. See if you get better grouping
    results.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对新闻组数据执行 k-means 聚类，使用不同的 *k* 值，或者使用肘部法则来找到最佳的 k 值。看看是否能得到更好的分组结果。
- en: Try different numbers of topics in either NMF or LDA and see which one produces
    more meaningful topics in the end. This should be a fun exercise.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在 NMF 或 LDA 中使用不同的主题数，看看哪一个能产生更有意义的主题。这个练习应该会很有趣。
- en: Can you experiment with NMF or LDA on the entire 20 groups of newsgroups data?
    Are the resulting topics full of noise or gems?
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在整个 20 个新闻组数据上尝试 NMF 或 LDA 吗？最终的主题是充满噪音还是有价值的发现？
- en: Join our book’s Discord space
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 社区
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
