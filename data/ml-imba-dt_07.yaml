- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Data-Level Deep Learning Methods
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据级别深度学习方法
- en: You learned about various sampling methods in the previous chapters. Collectively,
    we call these methods *data-level methods* in this book. These methods include
    random undersampling, random oversampling, NearMiss, and SMOTE. We also explored
    how these methods work with classical machine learning algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你已经了解了各种采样方法。在这本书中，我们将这些方法统称为*数据级别方法*。这些方法包括随机欠采样、随机过采样、NearMiss 和 SMOTE。我们还探讨了这些方法如何与经典机器学习算法协同工作。
- en: In this chapter, we’ll explore how to apply familiar sampling methods to deep
    learning models. Deep learning offers unique opportunities to enhance these methods
    further. We’ll delve into elegant techniques to combine deep learning with oversampling
    and undersampling. Additionally, we’ll learn how to implement various sampling
    methods with a basic neural network. We’ll also cover dynamic sampling, which
    involves adjusting the data sample across multiple training iterations, using
    varying balancing ratios for each iteration. Then, we will learn to use some data
    augmentation techniques for both images and text. We’ll end the chapter by highlighting
    key takeaways from a variety of other data-level techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何将熟悉的采样方法应用于深度学习模型。深度学习为这些方法提供了进一步改进的独特机会。我们将深入研究将深度学习与过采样和欠采样相结合的优雅技术。此外，我们还将学习如何使用基本神经网络实现各种采样方法。我们还将介绍动态采样，这涉及到在多个训练迭代中调整数据样本，并为每个迭代使用不同的平衡比率。然后，我们将学习使用一些数据增强技术来处理图像和文本。我们将通过强调来自各种其他数据级别技术的关键要点来结束本章。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Preparing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据
- en: Sampling techniques for deep learning models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习模型的采样技术
- en: Data-level techniques for text classification
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类的数据级别技术
- en: A discussion of other data-level deep learning methods and their key ideas
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关于其他数据级别深度学习方法和其关键思想的讨论
- en: It is not always straightforward to port methods to handle data imbalance, which
    worked well on classical ML models, into the deep learning world. The challenges
    and opportunities of deep learning models differ from the classical ML models
    primarily because of the difference in the type of data these models have to deal
    with. While classical ML models mostly deal with tabular and structured data,
    deep learning models typically deal with unstructured data, such as images, text,
    audio, and video, which is fundamentally different from tabular data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 将处理数据不平衡的方法从在经典机器学习模型上表现良好的方法迁移到深度学习领域并不总是直接的。深度学习模型与经典机器学习模型的主要挑战和机遇不同，主要是因为这些模型必须处理的数据类型不同。虽然经典机器学习模型主要处理表格和结构化数据，但深度学习模型通常处理非结构化数据，如图像、文本、音频和视频，这与表格数据有根本性的不同。
- en: We will discuss various techniques to deal with imbalance problems in computer
    vision. In the first part of the chapter, we will focus on various techniques,
    such as sampling and data augmentation, to handle class imbalance when training
    convolutional neural networks on image and text data.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论各种处理计算机视觉中不平衡问题的技术。在本章的第一部分，我们将重点关注各种技术，例如采样和数据增强，以处理在图像和文本数据上训练卷积神经网络时的类别不平衡问题。
- en: In the latter half of the chapter, we will discuss common data-level techniques
    that can be applied to text problems. A lot of computer vision techniques can
    be successfully applied to NLP problems, too.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们将讨论可以应用于文本问题的常见数据级别技术。许多计算机视觉技术也可以成功地应用于自然语言处理问题。
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: Similar to prior chapters, we will continue to utilize common libraries such
    as `torch`, `torchvision`, `numpy`, and `scikit-learn`. We will also use `nlpaug`
    for NLP-related functionalities. The code and notebooks for this chapter are available
    on GitHub at [https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07).
    You can open the GitHub notebooks using Google Colab by clicking on the **Open
    in Colab** icon at the top of the chapter’s notebook, or by launching it from
    [https://colab.research.google.com](https://colab.research.google.com), using
    the GitHub URL of the notebook.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章类似，我们将继续使用常见的库，如`torch`、`torchvision`、`numpy`和`scikit-learn`。我们还将使用`nlpaug`来实现NLP相关的功能。本章的代码和笔记本可在GitHub上找到，网址为[https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07](https://github.com/PacktPublishing/Machine-Learning-for-Imbalanced-Data/tree/main/chapter07)。您可以通过点击章节笔记本顶部的**在Colab中打开**图标或通过从[https://colab.research.google.com](https://colab.research.google.com)启动它，使用笔记本的GitHub
    URL来打开GitHub笔记本。
- en: Preparing the data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据
- en: In this chapter, we are going to use the classic MNIST dataset. This dataset
    contains 28-pixel x 28-pixel images of handwritten digits. The task for the model
    is to take an image as input and identify the digit in the image. We will use
    `PyTorch`, a popular deep-learning library, to demonstrate the algorithms. Let’s
    prepare the data now.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用经典的MNIST数据集。该数据集包含28像素x 28像素的手写数字图像。对于模型的任务是接收一个图像作为输入并识别图像中的数字。我们将使用流行的深度学习库`PyTorch`来展示算法。现在让我们准备数据。
- en: The first step in the process will be to import the libraries. We will need
    NumPy (as we deal with `numpy` arrays), `torchvision` (to load MNIST data), `torch`,
    `random`, and `copy` libraries.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的第一步将是导入库。我们需要NumPy（因为我们处理`numpy`数组）、`torchvision`（用于加载MNIST数据）、`torch`、`random`和`copy`库。
- en: 'Next, we can download the MNIST data from `torchvision.datasets`. The `torchvision`
    library is a part of the `PyTorch` framework, which contains datasets, models,
    and common image transformers for computer vision tasks. The following code will
    download the MNIST dataset from this library:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以从`torchvision.datasets`下载MNIST数据。`torchvision`库是`PyTorch`框架的一部分，它包含用于计算机视觉任务的数据库、模型和常见图像转换器。以下代码将从该库下载MNIST数据集：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once the data is downloaded from `torchvision`, we can load it into the `Dataloader`
    utility of `PyTorch`, which creates batches of data and provides us with a Python-style
    iterator over the batches. The following code does exactly that. Here, we are
    creating batches of size 64 for `train_loader`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从`torchvision`下载了数据，我们就可以将其加载到`PyTorch`的`Dataloader`实用工具中，它创建数据批次并提供对批次的Python风格迭代器。以下代码正是这样做的。在这里，我们为`train_loader`创建大小为64的批次。
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Since we are interested in imbalanced datasets, we will convert our MNIST dataset,
    which is a balanced dataset, into a long-tailed version of itself by deleting
    examples from various classes. We are omitting that implementation here to save
    space; you can refer to the chapter’s GitHub repository for details. We assume
    that you have the `imbalanced_train_loader` class created from the imbalanced
    trainset.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们对不平衡数据集感兴趣，我们将把我们的MNIST数据集，一个平衡数据集，通过删除各个类别的示例转换为它的长尾版本。我们在这里省略了该实现以节省空间；您可以参考章节的GitHub仓库以获取详细信息。我们假设您已经创建了由不平衡训练集生成的`imbalanced_train_loader`类。
- en: '*Figure 7**.1* shows the distribution of samples in the imbalanced MNIST dataset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.1*显示了不平衡MNIST数据集中样本的分布：'
- en: '![](img/B17259_07_01.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_01.jpg)'
- en: Figure 7.1 – A bar chart of the counts of examples from each digit class
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 每个数字类示例计数的条形图
- en: Next, we will learn to create a training loop in `PyTorch`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将学习如何在`PyTorch`中创建训练循环。
- en: Creating the training loop
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建训练循环
- en: Before creating the training loop, we should import the `torch.nn` and `torch.optim`
    packages. The `torch.nn` package provides all the building blocks to create a
    neural network graph, while the `torch.optim` package provides us with most of
    the common optimization algorithms.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建训练循环之前，我们应该导入`torch.nn`和`torch.optim`包。`torch.nn`包提供了创建神经网络图的全部构建块，而`torch.optim`包为我们提供了大多数常见的优化算法。
- en: '[PRE2]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Since we will need some hyperparameters, let’s define them:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要一些超参数，让我们定义它们：
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: After setting up the hyperparameters, we can define the `train` function, which
    will take PyTorch’s data loader as input and return a model fitted to the data.
    To create a trained model, we will need a model, a loss criterion, and an optimizer.
    We will use a single-layer linear neural network as the model here. You can design
    your own neural network architecture based on your requirements. For the loss
    criterion, we will use `CrossEntropyLoss`, and we will use **Stochastic Gradient
    Descent** (**SGD**) as the optimizer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好超参数之后，我们可以定义`train`函数，该函数将接受PyTorch的数据加载器作为输入，并返回一个拟合数据的模型。为了创建一个训练好的模型，我们需要一个模型、一个损失准则和一个优化器。在这里，我们将使用单层线性神经网络作为模型。你可以根据你的需求设计自己的神经网络架构。对于损失准则，我们将使用`CrossEntropyLoss`，并且我们将使用**随机梯度下降**（**SGD**）作为优化器。
- en: 'We will train the model for `num_epochs` epochs. We will discuss how the model
    is trained during a single epoch in the next paragraph. For now, we will abstract
    that part out in the `run_epoch` function:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练模型`num_epochs`个epochs。我们将在下一段讨论模型在单个epoch中的训练过程。现在，我们将在`run_epoch`函数中抽象出这部分：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'During every epoch, we will train our model over the whole training data once.
    As discussed earlier, dataloader divides the data into multiple batches. First,
    we will have to match the shape of the images in the batch with the input dimension
    of our model. We will take the current batch and do a forward pass over the model,
    calculating the predictions and loss over the predictions in one go. Then, we
    will backpropagate the loss to update the model weights:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个epoch期间，我们将对整个训练数据进行一次模型训练。如前所述，数据加载器将数据分成多个批次。首先，我们必须将批次中图像的形状与模型的输入维度相匹配。我们将对当前批次进行前向传递，一次计算预测和预测的损失。然后，我们将损失反向传播以更新模型权重：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To get a trained model, we can now send the data loader to the `train` function:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得一个训练好的模型，我们现在可以将数据加载器发送到`train`函数：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For all image-related methods in this chapter, we’ll employ the model code
    detailed next. We will create a `PyTorch` neural network named `Net` that features
    two convolutional layers, a dropout mechanism, and a pair of fully connected layers.
    Through the `forward` function, the model seamlessly integrates these layers using
    `ReLU` activations and max-pooling, manipulating the input, `x`. The result is
    `log_softmax` of the computed output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的所有与图像相关的方法中，我们将使用下面详细说明的模型代码。我们将创建一个名为`Net`的`PyTorch`神经网络，它具有两个卷积层、一个dropout机制和一对全连接层。通过`forward`函数，模型使用`ReLU`激活和最大池化无缝集成这些层，操作输入`x`。结果是计算输出的`log_softmax`：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s break down some of these terms:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们分解一些这些术语：
- en: '`Net` class, there are two such layers – `conv1` and `conv2`. The numbers `(1,
    10)` and `(10, 20)` are simply the input and output channels. The term `kernel_size=5`
    means that a 5 x 5 grid (or filter) is used to scan the input.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在`Net`类中，有两个这样的层——`conv1`和`conv2`。数字`(1, 10)`和`(10, 20)`仅仅是输入和输出通道。术语`kernel_size=5`意味着使用一个5
    x 5的网格（或过滤器）来扫描输入。
- en: '`conv2_drop` is a dropout layer of type `Dropout2d`, specifically designed
    for 2D data (such as images).'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conv2_drop`是一个类型为`Dropout2d`的dropout层，专门设计用于2D数据（如图像）。'
- en: '`fc1` and `fc2`, which further process the patterns recognized by the convolutional
    layers to make predictions.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fc1`和`fc2`，它们进一步处理卷积层识别出的模式，以进行预测。'
- en: '`F.relu` is an activation function that introduces non-linearity to the model,
    enabling it to learn complex patterns'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F.relu`是一个激活函数，它向模型引入非线性，使其能够学习复杂的模式'
- en: '`F.max_pool2d` is a pooling function that reduces the spatial dimensions of
    the data while retaining important features'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`F.max_pool2d`是一个池化函数，它在保留重要特征的同时减少了数据的空间维度'
- en: Finally, `F.log_softmax` is an activation function commonly used for classification
    tasks to produce probabilities for each class
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，`F.log_softmax`是一个常用于分类任务的激活函数，用于为每个类别生成概率。
- en: In essence, the `Net` class defines a neural network that first detects patterns
    in data using convolutional layers, reduces overfitting using dropout, and then
    makes predictions using fully connected layers. The forward method is a sequence
    of operations that define how data flows through this network.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，`Net`类定义了一个神经网络，它首先使用卷积层在数据中检测模式，使用dropout减少过拟合，然后使用全连接层进行预测。前向方法是一系列操作，定义了数据如何通过这个网络流动。
- en: In the next section, we will learn how to use the `train` function with oversampling
    methods.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何使用oversampling方法与`train`函数一起使用。
- en: Sampling techniques for deep learning models
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习模型的采样技术
- en: In this section, we’ll explore some sampling methods, such as random oversampling
    and weighted sampling, for deep learning models. We’ll then transition into data
    augmentation techniques, which bolster model robustness and mitigate dataset limitations.
    While large datasets are ideal for deep learning, real-world constraints often
    make them hard to obtain. We will also look at some advanced augmentations, such
    as CutMix and MixUp. We’ll start with standard methods before discussing these
    advanced techniques.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一些采样方法，例如随机过采样和加权采样，用于深度学习模型。然后我们将过渡到数据增强技术，这些技术可以增强模型鲁棒性并减轻数据集的限制。虽然大型数据集对于深度学习来说是理想的，但现实世界的限制往往使得它们难以获得。我们还将探讨一些高级增强技术，如CutMix和MixUp。我们将从标准方法开始，然后讨论这些高级技术。
- en: Random oversampling
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随机过采样
- en: Here, we will apply the plain old random oversampling we learned in [*Chapter
    2*](B17259_02.xhtml#_idTextAnchor042), *Oversampling Methods*, but using image
    data as input to a neural network. The basic idea is to duplicate samples from
    the minority classes randomly until we end up with an equal number of samples
    from each class. This technique often performs better than no sampling.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将应用我们在[*第2章*](B17259_02.xhtml#_idTextAnchor042)，“过采样方法”中学习的简单随机过采样，但使用图像数据作为神经网络的输入。基本思想是随机复制少数类的样本，直到每个类别的样本数量相等。这种技术通常比不采样表现得更好。
- en: Tip
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: Make sure to train the model for enough epochs so that it has fully been fitted
    to the data. Under-training will likely lead to suboptimal model performance.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 确保训练模型足够多的轮次，以便它已经完全拟合到数据上。欠拟合可能会导致模型性能不佳。
- en: 'Let’s spend some time working with the code. There are a few simple steps we
    need to follow. First, we need to convert data from the data loaders into tensors.
    Our `RandomOverSampler` API from `imbalanced-learn` doesn’t work directly with
    data loaders:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花些时间来处理代码。我们需要遵循几个简单的步骤。首先，我们需要将数据加载器中的数据转换为张量。我们的`imbalanced-learn`中的`RandomOverSampler`
    API不能直接与数据加载器一起使用：
- en: '[PRE8]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We also need to reshape the `X` tensor for `RandomOverSampler` to work with
    two-dimensional inputs, as each of our images is a 28 x 28 matrix:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要重塑`X`张量，以便`RandomOverSampler`可以处理二维输入，因为我们的每个图像都是一个28 x 28的矩阵：
- en: '[PRE9]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Now, we can import the `RandomOverSampler` class from the `imbalanced-learn`
    library, define an `oversampler` object, and resample our data using it:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以从`imbalanced-learn`库中导入`RandomOverSampler`类，定义一个`oversampler`对象，并使用它重新采样我们的数据：
- en: '[PRE10]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'After resampling the data, we need to reshape it again back to the original
    form:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在重新采样数据后，我们需要将其再次重塑回原始形式：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can now create a new data loader using the oversampled data:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用过采样数据创建一个新的数据加载器：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we can train our model using the new data loader. For this step, we
    can use the `train` function defined in the previous section:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用新的数据加载器来训练我们的模型。对于这一步，我们可以使用上一节中定义的`train`函数：
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: That’s all we need to do to use the random oversampling technique with deep
    learning models.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们使用深度学习模型中的随机过采样技术所需做的所有事情。
- en: A similar strategy can be used for `RandomUnderSampling` from the `imbalanced-learn`
    library.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用类似的策略从`imbalanced-learn`库中的`RandomUnderSampling`进行采样。
- en: '`PyTorch` provides a `WeightedRandomSampler` API, which is similar to the `sample_weight`
    parameter from `scikit-learn` (found in many of the fit methods in `scikit-learn`
    estimators (such as `RandomForestClassifier` and `LogisticRegression`) and serves
    a similar purpose of assigning a weight to each sample of the training dataset.
    We had a detailed discussion of the differences between `class_weight` and `sample_weight`
    in [*Chapter 5*](B17259_05.xhtml#_idTextAnchor151), *Cost-Sensitive Learning*.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTorch`提供了一个`WeightedRandomSampler` API，它与`scikit-learn`中的`sample_weight`参数类似（在`scikit-learn`估计器的许多fit方法中找到，例如`RandomForestClassifier`和`LogisticRegression`），具有为训练数据集中的每个样本分配权重的类似目的。我们在[*第5章*](B17259_05.xhtml#_idTextAnchor151)，“成本敏感学习”中对`class_weight`和`sample_weight`之间的差异进行了详细讨论。'
- en: 'We can specify `weights` as a parameter to `WeightedRamdomSampler` so that
    it can automatically weigh the examples in the batch, according to the weight
    of each sample. The `weights` parameter values are typically the inverse of the
    frequency of various classes in the dataset:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将`weights`参数指定给`WeightedRamdomSampler`，以便它可以根据每个样本的权重自动对批次的示例进行加权。`weights`参数的值通常是数据集中各种类别的频率的倒数：
- en: '[PRE14]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`class_weights` is more for the minority class labels than the majority class
    labels. Let’s compute the `weightedRamdomSampler` values:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`class_weights` 对于少数类标签比多数类标签更重要。让我们计算`weightedRamdomSampler`的值：'
- en: '[PRE15]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the next section, we will learn how to sample data dynamically.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习如何动态采样数据。
- en: Dynamic sampling
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动态采样
- en: Dynamic sampling [1] is an advanced technique that can self-adjust the sampling
    rate as training progresses. It promises to adapt according to the problem’s complexity
    and class imbalance, with almost no hyperparameter tuning. It is just one more
    tool in your arsenal to try on your dataset, especially when you have imbalanced
    image data at hand, and see whether it gives a better performance than the other
    techniques we’ve discussed so far in this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 动态采样 [1] 是一种高级技术，可以在训练过程中自我调整采样率。它承诺根据问题的复杂性和类别不平衡进行适应，几乎不需要调整超参数。它是你武器库中另一个可以尝试的工具，尤其是在你手头有不平衡的图像数据时，看看它是否比我们在本章中讨论的其他技术给出更好的性能。
- en: The basic idea of dynamic sampling is to dynamically adjust the sampling rate
    for various classes, depending on whether they are doing well or worse in a particular
    training iteration when compared to the prior iteration. If a class is performing
    comparatively poorly, then the class is oversampled in the next iteration, and
    vice versa.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 动态采样的基本思想是根据它们在特定训练迭代中与先前迭代相比的表现好坏，动态调整各种类别的采样率。如果一个类别的表现相对较差，那么在下一个迭代中该类别将被过度采样，反之亦然。
- en: The details of the algorithm
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算法的细节
- en: 'These are the core components of dynamic sampling:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是动态采样的核心组件：
- en: '**Real-time data augmentation**: Apply various kinds of image transformations
    to the images of each training batch. These transformations can be rotation, flipping,
    adjusting brightness, translation, adjusting contrast/color, noise injection,
    and so on. As discussed earlier, this step helps to reduce model overfitting and
    improves generalization.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时数据增强**：将各种图像变换应用于每个训练批次的图像。这些变换可以是旋转、翻转、调整亮度、平移、调整对比度/颜色、噪声注入等。如前所述，这一步骤有助于减少模型过拟合并提高泛化能力。'
- en: '**Dynamic sampling method**: In each iteration, a sample size (given by a certain
    formula) is chosen, and a model is trained with that sample size. The classes
    with lower F1 scores are sampled at a higher rate in the next iteration, forcing
    the model to focus more on previously misclassified examples. The number of images,
    c j, for the next iteration is updated according to the following formula:'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态采样方法**：在每次迭代中，选择一个样本大小（由某个公式给出），并使用该样本大小训练一个模型。在下一个迭代中，具有较低F1分数的类别将以更高的比率进行采样，迫使模型更多地关注先前错误分类的示例。下一个迭代中图像的数量，c j，根据以下公式更新：'
- en: UpdateSampleSize( F1 i, c j) =  1 − f1 i, j  _ ∑ c k∈ C 1 − f1 i,k  × N
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: UpdateSampleSize( F1 i, c j) =  1 − f1 i, j  _ ∑ c k∈ C 1 − f1 i,k  × N
- en: 'Here:'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里：
- en: f1 i, j is the F1-score of class c j in iteration i
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: f1 i, j 是第i次迭代中类c j的F1分数
- en: N = the average number of samples in all classes
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: N = 所有类别中样本数量的平均值
- en: 'For a particular training epoch, let’s say we got the following F1 score for
    each of the three classes, **A**, **B**, and **C**, on our validation dataset:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的训练周期，假设我们在验证数据集上对三个类别，**A**、**B**和**C**，得到了以下F1分数：
- en: '| **Class** | **F1 score** |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **F1分数** |'
- en: '| A | 0.1 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| A | 0.1 |'
- en: '| B | 0.2 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| B | 0.2 |'
- en: '| C | 0.3 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| C | 0.3 |'
- en: Table 7.1 – Sample F1-scores of each class after some epoch
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 – 某个周期后每个类别的样本F1分数
- en: 'Here is how we would compute the weight of each class for the next epoch of
    training:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们如何计算每个类别的权重以用于下一个训练周期的计算方法：
- en: Weight (class A) =  N * (1 − f1 a)  ______________________  (1 − f1 a)+ (1 −
    f1 b)+ (1 − f1 c)  =  N * 0.9  _ 0.9 + 0.8 + 0.7
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 权重（类A）=  N * (1 − f1 a)  ______________________  (1 − f1 a)+ (1 − f1 b)+ (1
    − f1 c)  =  N * 0.9  _ 0.9 + 0.8 + 0.7
- en: = N * 0.375
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: = N * 0.375
- en: '*Weight (class B) = N* 0.8/2.4 =* *N*0.33*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*（类B的）权重 = N* 0.8/2.4 =* *N*0.33*'
- en: '*Weight (class C) = N* 0.7/2.4 =* *N* 0.29*'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*（类C的）权重 = N* 0.7/2.4 =* *N* 0.29*'
- en: This means that we will sample class A at a higher rate than class B and class
    C, which makes sense because the performance on class A was weaker than that on
    classes B and C.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们将以比类B和类C更高的比率采样类A，这是有意义的，因为类A的性能比类B和类C弱。
- en: A second model is trained through transfer learning, without sampling, to prevent
    the minority classes from overfitting. At inference time, the model output is
    a function of both models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 通过迁移学习训练第二个模型，不进行采样，以防止少数类过拟合。在推理时间，模型的输出是两个模型输出的函数。
- en: There are additional details about the **DynamicSampling** algorithm that we
    have omitted here due to space constraints. You can find the complete implementation
    code in the corresponding GitHub repository for this chapter.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，我们在此省略了关于**动态采样**算法的更多细节。您可以在本章对应的GitHub仓库中找到完整的实现代码。
- en: '![](img/B17259_07_02.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_02.jpg)'
- en: Figure 7.2 – An overall model accuracy comparison of various sampling techniques
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 使用各种采样技术的整体模型准确率比较
- en: '*Table 7.2* shows the per-class model accuracy using various sampling techniques,
    including the baseline, where no sampling is done.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.2*显示了使用各种采样技术（包括基线，其中不进行采样）的每类模型准确率。'
- en: '| **Class** | **Baseline** | **Weighted** **random sampler** | **Dynamic**
    **sampler** | **Random** **oversampling** | **Random** **undersampling** |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| **类别** | **基线** | **加权随机采样器** | **动态采样器** | **随机过采样** | **随机欠采样** |'
- en: '| 0 | **99.9** | 99.0 | 92.4 | 99.1 | 97.2 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 0 | **99.9** | 99.0 | 92.4 | 99.1 | 97.2 |'
- en: '| 1 | **99.7** | 99.2 | 96.8 | 99.2 | 90.7 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 1 | **99.7** | 99.2 | 96.8 | 99.2 | 90.7 |'
- en: '| 2 | **98.5** | 98.3 | 93.5 | **98.5** | 70.8 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 2 | **98.5** | 98.3 | 93.5 | **98.5** | 70.8 |'
- en: '| 3 | 97.3 | 97.4 | 96.8 | **98.3** | 74.4 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 97.3 | 97.4 | 96.8 | **98.3** | 74.4 |'
- en: '| 4 | 98.3 | 98.0 | 91.9 | **98.6** | 79.6 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 98.3 | 98.0 | 91.9 | **98.6** | 79.6 |'
- en: '| 5 | 96.2 | 96.0 | 97.3 | **98.1** | 52.8 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 96.2 | 96.0 | 97.3 | **98.1** | 52.8 |'
- en: '| 6 | 94.5 | 97.6 | **98.7** | 97.3 | 77.6 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 94.5 | 97.6 | **98.7** | 97.3 | 77.6 |'
- en: '| 7 | 89.7 | 94.7 | **96.5** | 94.1 | 81.1 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 89.7 | 94.7 | **96.5** | 94.1 | 81.1 |'
- en: '| 8 | 63.3 | 91.5 | **96.9** | 93.0 | 60.2 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 63.3 | 91.5 | **96.9** | 93.0 | 60.2 |'
- en: '| 9 | 50.7 | 92.6 | **97.7** | 91.8 | 56.5 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 50.7 | 92.6 | **97.7** | 91.8 | 56.5 |'
- en: Table 7.2 – A per-class model accuracy comparison of various sampling techniques
    (the highest value for a class is in bold)
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 – 使用各种采样技术的每类模型准确率比较（类别的最高值用粗体表示）
- en: 'Here are some insights from the results:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一些从结果中得到的见解：
- en: In terms of overall performance, **Random OverSampling** (**ROS**) performed
    the best, while **Random Undersampling** (**RUS**) did the worst.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整体性能方面，**随机过采样**（**ROS**）表现最佳，而**随机欠采样**（**RUS**）表现最差。
- en: Although ROS did the best, it can be computationally very expensive due to data
    cloning, making it less suitable for large datasets and industrial settings.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然ROS表现最佳，但由于数据克隆，它可能计算成本非常高，这使得它不太适合大型数据集和工业环境。
- en: Dynamic sampling did a little worse than ROS; it did best on the minority classes
    6–9 and would be our preferred choice here. However, due to its increased complexity,
    our second choice will be the weighted random sampler.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动态采样比ROS略差；它在少数类别6-9上表现最佳，将是我们的首选选择。然而，由于其复杂性增加，我们的第二选择将是加权随机采样器。
- en: The baseline and weighted random sampler techniques are stable across classes;
    RUS is notably variable and performs poorly on most classes.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线和加权随机采样器技术在类别间表现稳定；RUS在大多数类别上表现明显不稳定且表现不佳。
- en: Similarly, we can apply SMOTE in the same way as `RandomOverSampler`. Please
    note that while SMOTE can be applied to images, its use of a linear subspace of
    the original data is often limiting.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以像使用`RandomOverSampler`一样应用SMOTE。请注意，虽然SMOTE可以应用于图像，但其对原始数据线性子空间的利用通常受到限制。
- en: This ends our discussion of the various sampling techniques. In the next section,
    we will focus on data augmentation techniques specifically designed for images
    to achieve more effective oversampling.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对各种采样技术的讨论。在下一节中，我们将专注于专门为图像设计的用于实现更有效过采样的数据增强技术。
- en: Data augmentation techniques for vision
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 视觉数据增强技术
- en: Today, a variety of custom augmentation techniques are used for various kinds
    of data, such as images, audio, video, and even text data. In the vision realm,
    this includes techniques such as rotating, scaling, cropping, blurring, adding
    noise to an image, and a host of other techniques, including combining those techniques
    all at once in some appropriate sequence. Image data augmentation is not really
    a recent innovation. Some image augmentation techniques can also be found in the
    LeNet-5 model paper [2] from 1998, for example. Similarly, the AlexNet model [3]
    from 2012 uses random cropping, flipping, changing the color intensity of RGB
    channels, and so on to reduce errors during model training.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，各种定制的增强技术被用于各种类型的数据，如图像、音频、视频，甚至文本数据。在视觉领域，这包括旋转、缩放、裁剪、模糊、向图像添加噪声以及其他许多技术，包括将这些技术一次性以适当的顺序组合在一起。图像数据增强并不是一项最近才出现的创新。例如，一些图像增强技术也可以在1998年的LeNet-5模型论文[2]中找到。同样，2012年的AlexNet模型[3]使用随机裁剪、翻转、改变RGB通道的颜色强度等方法来减少模型训练过程中的错误。
- en: 'Let’s discuss why data augmentation can often be helpful:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论一下为什么数据增强通常很有帮助：
- en: In problems where we have limited data or imbalanced data, it may not always
    be possible to gather more data. This could be because either gathering more data
    is difficult in the first place (for example, waiting for more fraud to occur
    when dealing with credit card fraud, or gathering satellite images where we have
    to pay satellite operators, which can be quite expensive) or labeling the data
    is difficult or expensive (for example, to label medical image datasets, we need
    domain experts).
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们数据有限或不平衡的问题中，并不总是能够收集到更多的数据。这可能是因为收集更多数据本身就很困难（例如，在处理信用卡欺诈时等待更多欺诈发生，或者收集卫星图像，我们必须支付卫星运营商的费用，这可能相当昂贵）或者标记数据很困难或昂贵（例如，为了标记医学图像数据集，我们需要领域专家）。
- en: Data augmentation can help reduce overfitting and improve the overall performance
    of a model. One motivation for this practice is that attributes such as lighting,
    noise, color, scale, and focus in the training set may not align with those in
    the real-world images on which we run inference. Additionally, augmentation diversifies
    a dataset to help the model generalize better. For example, if the model is trained
    only on images of cats facing right, it may not perform well on images where the
    cat faces left. Therefore, it’s advisable to always apply valid transformations
    to augment image datasets, as most models gain performance with more diverse data.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强可以帮助减少过拟合并提高模型的总体性能。这种做法的一个动机是，训练集中的属性如光照、噪声、颜色、比例和焦点可能与我们运行推理的真实世界图像中的属性不一致。此外，增强多样化数据集可以帮助模型更好地泛化。例如，如果模型仅在面向右的猫的图像上训练，它可能在面向左的猫的图像上表现不佳。因此，始终应用有效的变换来增强图像数据集是明智的，因为大多数模型在更多样化的数据上都能获得性能提升。
- en: Data augmentation is widely used for computer vision tasks such as object detection,
    classification, and segmentation. It can be very useful for NLP tasks as well.
    In the computer vision world, there are lots of open source libraries that help
    standardize the various image augmentation techniques, while the NLP tools for
    data augmentation space have yet to mature.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强在计算机视觉任务中如目标检测、分类和分割等方面被广泛使用。它对于自然语言处理任务也非常有用。在计算机视觉领域，有许多开源库帮助标准化各种图像增强技术，而数据增强在自然语言处理工具方面尚未成熟。
- en: While there are several popular open source libraries for image augmentation,
    such as `imgaug`, Facebook’s `AugLy`, and `Albumentations`, we will use `torchvision`
    in this book. As a part of the PyTorch ecosystem, it offers seamless integration
    with PyTorch workflows, a range of common image transformations, as well as pre-trained
    models and datasets, making it a convenient and comprehensive choice for computer
    vision tasks. If you need more advanced augmentations, or if speed is a concern,
    `Albumentations` may be a better choice.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有几个流行的开源库用于图像增强，如 `imgaug`、Facebook的 `AugLy` 和 `Albumentations`，但在这本书中我们将使用
    `torchvision`。作为PyTorch生态系统的一部分，它提供了与PyTorch工作流程的无缝集成、一系列常见的图像变换、以及预训练模型和数据集，使其成为计算机视觉任务的方便且全面的选项。如果您需要更高级的增强，或者如果速度是一个关注点，`Albumentations`
    可能是一个更好的选择。
- en: 'We can use `torchvision.transforms.Pad` to add some padding to the image boundaries:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `torchvision.transforms.Pad` 来向图像边界添加一些填充：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/B17259_07_03.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_07_03.jpg)'
- en: Figure 7.3 – The results of applying the Pad function to the image
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 应用Pad函数到图像的结果
- en: 'The `torchvision.transforms.FiveCrop` class transforms and crops the given
    image into four corners and the central crop:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.FiveCrop`类将给定的图像转换并裁剪成四个角落和中央部分：'
- en: '[PRE17]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/B17259_07_04.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_04.jpg)'
- en: Figure 7.4 – The results of applying the FiveCrop function on the image
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 应用FiveCrop函数到图像的结果
- en: '`torchvision.transforms.CenterCrop` is a similar class to crop images from
    the center.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.CenterCrop`是一个类似的类，用于从中心裁剪图像。'
- en: 'The `torchvision.transforms.ColorJitter` class changes the brightness, saturation,
    and other similar properties of the image:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.ColorJitter`类会改变图像的亮度、饱和度和其他类似属性：'
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/B17259_07_05.jpg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_05.jpg)'
- en: Figure 7.5 – The results of applying the ColorJitter function on the image
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 应用ColorJitter函数到图像的结果
- en: '`GaussianBlur` can add some blurring to the images:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`GaussianBlur`可以为图像添加一些模糊效果：'
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/B17259_07_06.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_06.jpg)'
- en: Figure 7.6 – The results of applying the GaussianBlur function on the image
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 应用高斯模糊函数到图像的结果
- en: 'The `torchvision.transforms.RandomRotation` class transform rotates an image
    at a random angle:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision.transforms.RandomRotation`类可以将图像随机旋转一个角度：'
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](img/B17259_07_07.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_07.jpg)'
- en: Figure 7.7 – The results of random rotation on the original image (leftmost)
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 对原始图像（最左侧）进行随机旋转的结果
- en: Consider exploring the other image transformation functionalities supported
    by the `torchvision.transforms` class that we didn’t discuss here.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑探索`torchvision.transforms`类支持的其它图像变换功能，这些功能我们在这里没有讨论。
- en: Cutout masks out random square regions of input images during training. While
    it may seem like this technique removes unnecessary portions of the image, it’s
    important to note that the areas to be masked are typically selected at random.
    The primary aim is to force the neural network to generalize better by ensuring
    it does not overly rely on any specific set of pixels within a given image.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，Cutout会随机遮盖输入图像的随机正方形区域。虽然这种技术看起来像是移除了图像中不必要的部分，但重要的是要注意，要被遮盖的区域通常是随机选择的。主要目的是通过确保神经网络不过度依赖给定图像中的任何特定像素集，来强迫神经网络更好地泛化。
- en: '![](img/B17259_07_08.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_08.jpg)'
- en: Figure 7.8 – The result of applying the cutout function on an image
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 应用cutout函数到图像的结果
- en: 🚀 Deep learning data-level techniques in production at Etsy/Booking/Wayfair
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Etsy/Booking/Wayfair在生产中应用深度学习数据级技术
- en: '**🎯** **Problem** **being solved:**'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎯** **问题解决：**'
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair leveraged user behavior
    to enhance personalization. Etsy focused on item recommendations based on browsing
    history [4], [Booking.com](http://Booking.com) tailored search results to boost
    bookings [5], and Wayfair optimized product image angles to improve CTRs [6].
    All aimed to utilize data-driven strategies for better user experience and performance.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Etsy、[Booking.com](http://Booking.com)和Wayfair利用用户行为来增强个性化。Etsy专注于基于浏览历史的商品推荐[4]，[Booking.com](http://Booking.com)定制搜索结果以增加预订[5]，而Wayfair优化产品图像角度以提高点击率[6]。所有这些都是为了利用数据驱动策略，以改善用户体验和性能。
- en: '**⚖️** **Data** **imbalance issue:**'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '**⚖️** **数据不平衡问题：**'
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair each grappled with data
    imbalance issues in their machine learning projects. Etsy faced a power law distribution
    in user sessions, where most users interacted with only a few listings within
    a one-hour window. [Booking.com](http://Booking.com) dealt with imbalanced classes
    in hotel images, as photos of bedrooms and bathrooms vastly outnumbered those
    of other facilities such as saunas or table tennis. Wayfair encountered an imbalance
    in real-world images of furniture, with a majority of images showing the “front”
    angle, leading to poor performance for other angles. All three companies had to
    address these imbalances to improve the performance and fairness of their models.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: Etsy、[Booking.com](http://Booking.com)和Wayfair在他们的机器学习项目中都遇到了数据不平衡问题。Etsy面临用户会话的幂律分布，其中大多数用户在一小时窗口内只与几个商品列表互动。[Booking.com](http://Booking.com)处理酒店图像中的不平衡类别，卧室和浴室的照片远远多于其他设施如桑拿或乒乓球的照片。Wayfair遇到了家具真实世界图像的不平衡，大多数图像显示“正面”角度，导致其他角度的性能不佳。这三家公司都必须解决这些不平衡问题，以提高模型性能和公平性。
- en: '**🎨** **Data** **augmentation strategy:**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎨** **数据增强策略：**'
- en: Etsy, [Booking.com](http://Booking.com), and Wayfair each had unique data augmentation
    strategies to address their specific challenges. Etsy used image random rotation,
    translation, zoom, and color contrast transformation to augment their dataset.
    [Booking.com](http://Booking.com) employed a variety of techniques, including
    mirroring, random cropping, affine transformation, aspect ratio distortion, color
    manipulation, and contrast enhancement. They increased their labeled data by 10
    times through these methods, applying distortions on the fly during training.
    Wayfair took a different approach by creating synthetic data with 3D models, generating
    100 views for each 3D model of chairs and sofas, thus providing granular angle
    information for training.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Etsy、[Booking.com](http://Booking.com)和Wayfair各自采用了独特的数据增强策略来解决他们各自的挑战。Etsy使用了图像随机旋转、平移、缩放和颜色对比度变换来增强他们的数据集。[Booking.com](http://Booking.com)采用了包括镜像、随机裁剪、仿射变换、纵横比扭曲、颜色操作和对比度增强在内的各种技术。他们通过这些方法将标记数据增加了10倍，在训练过程中实时应用扭曲。Wayfair通过创建3D模型生成的合成数据采取了不同的方法，为椅子和沙发的每个3D模型生成100个视图，从而为训练提供了细粒度的角度信息。
- en: Next, let’s look at some of the more advanced techniques, such as CutMix, MixUp,
    and AugMix, which are types of **Mixed Sample Data Augmentation** (**MSDA**) techniques.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一些更高级的技术，例如CutMix、MixUp和AugMix，这些都是**混合样本数据增强**（**MSDA**）技术的类型。
- en: MSDA is a set of techniques that involve mixing data samples to produce an augmented
    dataset, used to train a model (*Figure 7**.9*).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: MSDA是一组涉及混合数据样本以生成增强数据集的技术，用于训练模型（*图7**.9*）。
- en: '![](img/B17259_07_09.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_09.jpg)'
- en: Figure 7.9 – Common MSDA techniques
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 – 常见的MSDA技术
- en: CutMix
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CutMix
- en: 'CutMix [7] is an image data augmentation technique where patches are cut and
    pasted among training images. Specifically, a portion of an image is replaced
    by a portion of another image, as shown here:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: CutMix [7]是一种图像数据增强技术，其中在训练图像之间剪切和粘贴补丁。具体来说，图像的一部分被另一图像的一部分所取代，如下所示：
- en: '![](img/B17259_07_10.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_10.jpg)'
- en: Figure 7.10 – The result of applying the CutMix function to the images (image
    1 and image 2)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 – 将CutMix函数应用于图像的结果（图像1和图像2）
- en: It’s designed to encourage a model to make more localized, fine-grained predictions,
    thus improving overall generalization. CutMix also enforces consistent predictions
    outside the mixed regions, further enhancing model robustness.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 它旨在鼓励模型做出更多局部化、细粒度的预测，从而提高整体泛化能力。CutMix还强制在混合区域外进行一致的预测，进一步增强了模型的鲁棒性。
- en: '`torchvision` also offers an in-built API for CutMix, `torchvision.transforms.v2.CutMix`,
    so we don’t have to implement it from scratch.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`torchvision`还提供了内置的CutMix API，`torchvision.transforms.v2.CutMix`，因此我们不必从头实现它。'
- en: The full notebook with CutMix implementation from scratch can be found in the
    GitHub repo.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始实现的CutMix完整笔记本可以在GitHub仓库中找到。
- en: CutMix often shows improvements over traditional augmentation techniques on
    benchmark datasets, such as CIFAR-10, CIFAR-100, and ImageNet [7].
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: CutMix在基准数据集（如CIFAR-10、CIFAR-100和ImageNet [7]）上通常比传统的增强技术有改进。
- en: MixUp
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MixUp
- en: MixUp [8] creates virtual training examples by forming combinations of pairs
    of inputs and their corresponding labels.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: MixUp [8]通过形成输入及其对应标签的成对组合来创建虚拟训练示例。
- en: 'If (x i, y i) and (x j, y j) is an arbitrary pair of images in dataset D, where
    x is the image while y is its label, a mixed sample  ~ x ,  ~ y  can be generated
    using the following equations:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(x i, y i)和(x j, y j)是数据集D中的任意一对图像，其中x是图像，y是其标签，则可以使用以下方程生成混合样本~x ,  ~y ：
- en: ~ x  = λ x i + (1 − λ) x j
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ~ x  = λ x i + (1 − λ) x j
- en: ~ y  = λ y i + (1 − λ) y j
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ~ y  = λ y i + (1 − λ) y j
- en: where λ is the mixing factor sampled from the beta distribution. The Beta distribution
    is a flexible, continuous probability distribution defined on the interval [0,
    1].
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其中λ是从Beta分布中采样的混合因子。Beta分布是一种灵活的、在区间[0, 1]上定义的连续概率分布。
- en: 'MixUp acts as a regularizer, preventing overfitting and enhancing the generalization
    capabilities of models. The following implementation shuffles the data and targets,
    and then combines them using a weighted average, based on a value sampled from
    the Beta distribution, creating mixed data and targets for augmentation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: MixUp充当正则化器，防止过拟合并增强模型的泛化能力。以下实现通过从Beta分布中采样的值进行数据集和目标的洗牌，然后使用加权平均结合它们，从而创建用于增强的混合数据和目标：
- en: '[PRE21]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](img/B17259_07_11.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_11.jpg)'
- en: Figure 7.11 – The result of applying MixUp on the images (image 1 and image
    2)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 – 应用MixUp于图像（图像1和图像2）的结果
- en: On datasets such as CIFAR-100, MixUp has been found to provide significant gains
    in test accuracy compared to models trained without MixUp [8].
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在CIFAR-100等数据集上，MixUp被发现与未使用MixUp训练的模型相比，在测试精度上提供了显著的提升[8]。
- en: Similar to CutMix, `torchvision` provides a built-in API called `torchvision.transforms.v2.MixUp`,
    eliminating the need for manual implementation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 与CutMix类似，`torchvision`提供了一个名为`torchvision.transforms.v2.MixUp`的内置API，消除了手动实现的需求。
- en: AugMix
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AugMix
- en: The augmentation techniques that we have studied so far have all been fixed
    augmentations, but deep learning models can memorize them [9] and their performance
    can plateau. This is where AugMix [10] can be helpful, as it produces a diverse
    set of augmented images by performing several random augmentations in a sequence.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止研究过的增强技术都是固定的增强，但深度学习模型可以记住它们[9]，并且它们的性能可能会达到平台期。这就是AugMix[10]可以发挥作用的地方，因为它通过执行一系列随机增强来生成一组多样化的增强图像。
- en: AugMix improves model robustness and uncertainty without requiring any changes
    to the model architecture. The full AugMix algorithm also uses a special kind
    of loss function, but we will skip that for simplicity.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: AugMix在不改变模型架构的情况下提高了模型的鲁棒性和不确定性。完整的AugMix算法还使用了一种特殊的损失函数，但为了简单起见，我们将跳过这一点。
- en: 'The following code presents a simplified version of AugMix’s core logic:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了AugMix核心逻辑的简化版本：
- en: '[PRE22]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: At the end of the function, we combine images by using equal weight for each
    of the four transformed pictures. The actual AugMix implementation uses a Dirichlet
    distribution function to combine the images. A Dirichlet distribution is a generalization
    of the beta distribution that we saw in the MixUp technique.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在函数的末尾，我们通过为四个转换图像中的每一个使用相等权重来组合图像。实际的AugMix实现使用Dirichlet分布函数来组合图像。Dirichlet分布是我们在MixUp技术中看到的贝塔分布的推广。
- en: '![](img/B17259_07_12.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_07_12.jpg)'
- en: Figure 7.12 – The result of applying the Augmix function to four different images
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 – 将Augmix函数应用于四幅不同图像的结果
- en: In *Figure 7**.12*, the top row shows the original image, while the bottom row
    shows the result of applying AugMix. Images 1 and 3 don’t seem to have changed,
    but images 2 and 4 have noticeable changes.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7**.12中，顶部行显示原始图像，而底部行显示应用AugMix的结果。图像1和3似乎没有变化，但图像2和4有明显的改变。
- en: According to the AugMix paper [10], in experiments with ImageNet and CIFAR,
    AugMix achieved reduced test errors while providing improved robustness against
    corruption.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 根据AugMix论文[10]的描述，在ImageNet和CIFAR的实验中，AugMix实现了减少测试误差的同时，提高了对损坏的鲁棒性。
- en: We don’t need to create AugMix from scratch, as `torchvision` provides a built-in
    API called `torchvision.transforms.AugMix` for this purpose.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要从头开始创建AugMix，因为`torchvision`提供了一个名为`torchvision.transforms.AugMix`的内置API来完成这个目的。
- en: Remix
  id: totrans-196
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Remix
- en: Standard data augmentation techniques such as MixUp and CutMix may not be sufficient
    to handle class imbalances, as they do not take the distribution of class labels
    into account. Remix [11] addresses the challenges of training deep learning models
    on imbalanced datasets.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 标准数据增强技术，如MixUp和CutMix，可能不足以处理类别不平衡，因为它们没有考虑类别标签的分布。Remix[11]解决了在类别不平衡数据集上训练深度学习模型的挑战。
- en: MixUp and CutMix utilize the same mixing factor to combine samples in both the
    feature space and the label space. In the context of imbalanced data, the authors
    of the Remix paper [11] argued that this approach may not be optimal. Therefore,
    they proposed to separate the mixing factors, allowing for more flexibility in
    their application. By doing so, greater weight can be assigned to the minority
    class, enabling the creation of labels that are more favorable to the underrepresented
    class.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: MixUp和CutMix使用相同的混合因子在特征空间和标签空间中结合样本。在处理不平衡数据的情况下，Remix论文[11]的作者们认为这种方法可能不是最优的。因此，他们提出了将混合因子分开，以便在应用中提供更大的灵活性。通过这样做，可以给少数类分配更大的权重，从而创建出更有利于代表性不足的类的标签。
- en: 'If (xi, yi; xi, yj) is an arbitrary pair of images in dataset D, a mixed sample
    xRM, y RM can be generated using the following equations:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果(xi, yi; xi, yj)是数据集D中的任意一对图像，可以使用以下方程生成混合样本xRM，y RM：
- en: x RM = λxxi + (1 − λx)xj
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: x RM = λxxi + (1 − λx)xj
- en: y RM = λyyi + (1 − λy)yj
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: y RM = λyyi + (1 − λy)yj
- en: λx and λy are the mixing factors sampled from the beta distribution.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: λx和λy是从贝塔分布中采样的混合因子。
- en: 'Here is a simplified implementation:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个简化的实现：
- en: '[PRE23]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Combining previous techniques
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合先前技术
- en: 'It’s possible to combine these methods to introduce even more diversity to
    the training data, such as the following:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将这些方法结合起来，为训练数据引入更多的多样性，例如以下内容：
- en: '**CutMix and MixUp**: These can be alternated or used in tandem, creating regions
    in images that are replaced with parts of other images while also blending images
    pixel-wise'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**CutMix和MixUp**：这些可以交替使用或同时使用，在图像中创建被其他图像的部分替换的区域，同时像素级混合图像'
- en: '**Sequential**: You could sequentially apply these techniques (e.g., use MixUp
    first and then CutMix) to further diversify the augmented dataset'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序**：您可以顺序应用这些技术（例如，首先使用MixUp然后使用CutMix）以进一步多样化增强数据集'
- en: When combining these methods, it’s important to carefully manage the probabilities
    and strengths of each method, thus avoiding introducing too much noise or making
    the training data too divergent from the original distribution.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 当结合这些方法时，重要的是要仔细管理每种方法的概率和强度，从而避免引入过多的噪声或使训练数据与原始分布差异过大。
- en: Also, while combining these methods might improve model robustness and generalization
    in certain scenarios, it can also make training more computationally intensive
    and complex. It’s essential to balance the benefits against the potential trade-offs.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，虽然结合这些方法可能在某些情况下提高模型的鲁棒性和泛化能力，但它也可能使训练更加计算密集和复杂。权衡利弊是至关重要的。
- en: Remember, always validate the effectiveness of combined augmentations on a validation
    set to ensure they are beneficial for the task at hand.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，始终在验证集上验证结合增强的有效性，以确保它们对当前任务有益。
- en: Let’s use a long-tailed version of a different dataset called Fashion-MNIST
    for the techniques we just discussed (*Figure 7**.13*). Fashion-MNIST is another
    MNIST variant, consisting of 60,000 training and 10,000 testing images of 10 different
    clothing items, such as shoes, shirts, and dresses, each represented in a grayscale
    image of 28x28 pixels.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用名为Fashion-MNIST的不同数据集的尾部版本来讨论我们刚才讨论的技术（*图7.13*）。Fashion-MNIST是MNIST的另一个变体，包含60,000个训练图像和10,000个测试图像，共有10种不同的服装项目，如鞋子、衬衫和连衣裙，每个项目都以28x28像素的灰度图像表示。
- en: '![](img/B17259_07_13.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_13.jpg)'
- en: Figure 7.13 – Imbalanced FashionMNIST
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 – 不平衡的FashionMNIST
- en: '*Figure 7**.14* shows the overall model accuracy when trained using CutMix,
    MixUp, a combination of both, and Remix on the imbalanced FashionMNIST dataset.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*图7.14*显示了在FashionMNIST不平衡数据集上使用CutMix、MixUp、两者组合以及Remix训练时的整体模型准确率。'
- en: '![](img/B17259_07_14.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_14.jpg)'
- en: Figure 7.14 – Overall model accuracy (FashionMNIST)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 – 整体模型准确率（FashionMNIST）
- en: The difference in the performance of these techniques is more apparent when
    looking at the class-wise accuracy numbers (*Figure 7**.15*).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看按类别准确率数字时，这些技术的性能差异更为明显（*图7.15*）。
- en: '![](img/B17259_07_15.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_15.jpg)'
- en: Figure 7.15 – Class-wise model accuracy (the FashionMNIST dataset)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 – 按类别模型准确率（FashionMNIST数据集）
- en: 'Based on the given data, here are some insightful conclusions that can be drawn
    for the various techniques, especially in the context of imbalanced data:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 根据给定数据，以下是一些关于各种技术的见解，特别是在不平衡数据的情况下：
- en: '**Overall performance**: **Cutmix** and **Remix** generally offer the highest
    performance across most classes, followed closely by **Mixup** and **Cutmix+Mixup**.
    **Baseline** seems to be the least effective in general.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**整体性能**：**Cutmix**和**Remix**在大多数类别中通常提供最高的性能，其次是**Mixup**和**Cutmix+Mixup**。**基线**似乎在总体上效果最差。'
- en: '**Performance on minority classes**: For the minority class labeled as “6,”
    all techniques show relatively low performance compared to other classes. However,
    **Mixup** and **Cutmix+Mixup** offer a slight improvement over the baseline.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**少数类别的性能**：对于标记为“6”的少数类别，所有技术与其他类别相比都表现出相对较低的性能。然而，**Mixup**和**Cutmix+Mixup**在基线之上提供了一点点改进。'
- en: '**Consistency across classes**: **Cutmix** and **Mixup** are more consistent
    across different classes, excluding class “6,” where they are only marginally
    better. **Baseline**, on the other hand, shows significant variability, performing
    extremely well on some classes (such as “0” and “5”) but poorly on others (such
    as “6”).'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别间的一致性**：**Cutmix**和**Mixup**在不同类别间更为一致，除了类别“6”，在那里它们仅略有改进。另一方面，**基线**显示出显著的变异性，在某些类别（如“0”和“5”）上表现极好，但在其他类别（如“6”）上表现较差。'
- en: '**Techniques suited for specific classes**: **Cutmix** performs exceptionally
    well for the classes labeled “1” and “8,” where it outperforms all other techniques.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**适用于特定类别的技术**：**Cutmix**在标记为“1”和“8”的类别中表现出色，在这些类别中它优于所有其他技术。'
- en: '**Remix** is particularly strong for the class labeled “2,” where it outshines
    all other techniques.'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Remix**在标记为“2”的类别中特别强大，它超越了所有其他技术。'
- en: '**Complexity versus benefit**: **Cutmix+Mixup** does not offer a significant
    improvement over **Cutmix** or **Mixup** individually, raising questions about
    whether the additional computational complexity is justified.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂性与收益**：**Cutmix+Mixup**与**Cutmix**或**Mixup**单独使用相比，并没有带来显著的改进，这引发了额外的计算复杂性是否合理的疑问。'
- en: '**Generalizability**: **Cutmix** and **Mixup** appear to be the most robust
    techniques, showing high performance across most classes. These techniques would
    likely perform well on unseen data and are potentially good choices for imbalanced
    datasets.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**泛化性**：**Cutmix**和**Mixup**似乎是最稳健的技术，在大多数类别中表现出高性能。这些技术在未见过的数据上可能表现良好，并且对于不平衡数据集可能是好的选择。'
- en: '**Trade-offs**: **Cutmix** offers high performance but may not be the best
    for minority classes. **Mixup**, although slightly less effective overall, offers
    more balanced performance across classes, including minority ones.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权衡**：**Cutmix**提供了高性能，但可能不是对少数类别最好的选择。**Mixup**虽然整体上略逊一筹，但在各个类别，包括少数类别中提供了更平衡的性能。'
- en: 'The following are some points to be careful about while performing these image
    augmentations:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行这些图像增强时需要注意以下一些点：
- en: We must ensure that data augmentations preserve the original labels. For instance,
    rotating digits such as 6 and 9 can be problematic in digit recognition tasks.
    Similarly, cropping an image could invalidate its label, such as removing a cat
    from a “cat” image. This is especially crucial in complex tasks, such as image
    segmentation in self-driving cars, where augmentations can alter output labels
    or masks.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须确保数据增强保留原始标签。例如，旋转数字如6和9在数字识别任务中可能会出现问题。同样，裁剪图像可能会使其标签无效，例如从“猫”图像中移除猫。这在复杂任务中尤为重要，例如在自动驾驶汽车中的图像分割任务，增强可能会改变输出标签或掩码。
- en: While geometric and color transformations often increase memory usage and training
    time, they are not inherently problematic. Although color alterations can sometimes
    remove important details and affect label integrity, smart manipulation can also
    be beneficial. For instance, tweaking the color space to mimic different lighting
    or camera lenses can improve model performance.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然几何和颜色变换通常会增加内存使用和训练时间，但它们本身并不是问题。尽管颜色变化有时会去除重要细节并影响标签完整性，但智能操作也可能有益。例如，调整颜色空间以模拟不同的光照或相机镜头可以提高模型性能。
- en: To overcome some of the increased memory, time, and cost issues, as mentioned,
    a technique called `AutoAugment` is available in `PyTorch`, which can automatically
    search for the best augmentation policies on the dataset being used.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服一些增加的内存、时间和成本问题，如前所述，PyTorch中有一个名为`AutoAugment`的技术，可以在使用的数据集上自动搜索最佳的增强策略。
- en: 🚀 Deep learning data-level techniques in production at Grab
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Grab在生产中使用的深度学习数据级技术
- en: '**🎯** **Problem** **being solved:**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎯** **解决的问题**：'
- en: Grab, a ride-hailing and food delivery company in South-East Asia, faced the
    primary challenge of anonymizing faces and license plates in images collected
    for their geotagged imagery platform, KartaView [12]. This was essential to ensure
    user privacy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: Grab是一家位于东南亚的打车和食品配送公司，面临着匿名化用于其地理标记图像平台KartaView [12]中收集的图像中的人脸和车牌的主要挑战。这是确保用户隐私所必需的。
- en: '**⚖️** **Data** **imbalance issue**:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**⚖️** **数据不平衡问题**：'
- en: The dataset used by Grab was imbalanced, particularly in terms of the object
    sizes. Larger regions of interest, such as close-ups of faces or license plates,
    were underrepresented. This skewed distribution led to poor model performance
    in detecting these larger objects.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: Grab使用的数据集不平衡，特别是在物体大小方面。较大的感兴趣区域，如面部特写或车牌，代表性不足。这种分布偏差导致模型在检测这些较大物体时性能不佳。
- en: '**🎨** **Data** **augmentation strategy:**'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎨** **数据增强策略**：'
- en: 'Grab employed a multi-pronged data augmentation approach to address the imbalance:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Grab采用了多角度的数据增强方法来解决不平衡问题：
- en: '• **Offline augmentation**: One key method they used was the “image view splitting,”
    where each original image is divided into multiple “views” with predefined properties.
    This was crucial to accommodate different types of images such as perspective,
    wide field of view, and 360-degree equirectangular images. Each “view” was treated
    as a separate image with its tags, which helped the model generalize better. They
    also implemented oversampling for images with larger tags, addressing the imbalance
    in their dataset. This was vital for their anchor-based object detection model,
    as the imbalance was affecting the model’s performance in identifying larger objects.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: • **离线增强**：他们使用的关键方法之一是“图像视图分割”，即将每个原始图像分割成多个具有预定义属性的“视图”。这对于适应不同类型的图像，如透视、宽视野和360度等距图像至关重要。每个“视图”都被视为一个单独的图像，具有其标签，这有助于模型更好地泛化。他们还实施了过采样，以解决具有较大标签的图像数据集中的不平衡。这对于他们的基于锚点的目标检测模型至关重要，因为不平衡正在影响模型识别较大对象的表现。
- en: '• **Online augmentation**: They used YOLOv4 for object detection, which allowed
    for a variety of online augmentations, such as saturation, exposure, hue, flip,
    and mosaic.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: • **在线增强**：他们使用了YOLOv4进行目标检测，这允许进行各种在线增强，如饱和度、曝光、色调、翻转和马赛克。
- en: Modern techniques such as autoencoders and adversarial networks, specifically
    **Generative Adversarial Networks** (**GANs**), have recently gained traction
    in creating synthetic data to enhance image datasets. A GAN comprises two neural
    networks – the generator, which produces synthetic data, and the discriminator,
    which evaluates the authenticity of this data. Together, they work to create realistic
    and high-quality synthetic samples. GANs have also been applied to generate synthetic
    tabular data. For example, they’ve been used to create synthetic medical images
    that significantly improve diagnostic models. We’ll explore these cutting-edge
    techniques in greater detail toward the end of the chapter. In the next section,
    we will learn about applying data-level techniques to NLP problems.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 近期，现代技术如自动编码器和对抗网络，特别是**生成对抗网络**（**GANs**），在创建合成数据以增强图像数据集方面取得了显著进展。GAN由两个神经网络组成——生成器，它生成合成数据，和判别器，它评估这些数据的真实性。它们共同工作以创建逼真且高质量的合成样本。GANs也被应用于生成合成表格数据。例如，它们被用于创建合成医学图像，这显著提高了诊断模型。我们将在本章末尾更详细地探讨这些尖端技术。在下一节中，我们将学习如何将数据级别技术应用于NLP问题。
- en: Data-level techniques for text classification
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分类的数据级别技术
- en: Data imbalance, wherein certain classes in a dataset are underrepresented, is
    not just an issue confined to image or structured data domains. In NLP, imbalanced
    datasets can lead to biased models that might perform well on the majority class
    but are likely to misclassify underrepresented ones. To address this challenge,
    numerous strategies have been devised.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 数据不平衡，即数据集中某些类别代表性不足，不仅限于图像或结构化数据领域的问题。在NLP中，不平衡的数据集可能导致模型在大多数类别上表现良好，但很可能错误分类代表性不足的类别。为了应对这一挑战，已经设计了多种策略。
- en: 'In NLP, data augmentation can boost model performance, especially with limited
    training data. *Table 7.3* categorizes the various data augmentation techniques
    for text data:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在NLP中，数据增强可以提高模型性能，尤其是在训练数据有限的情况下。*表7.3*对文本数据的不同数据增强技术进行了分类：
- en: '| **Level** | **Method** | **Description** | **Example techniques** |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| **级别** | **方法** | **描述** | **示例技术** |'
- en: '| Character level | Noise | Introducing randomness at the character level |
    Jumbling characters |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 字符级别 | 噪声 | 在字符级别引入随机性 | 字符打乱 |'
- en: '| Rule-based | Transformations based on predefined rules | Capitalization |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 基于规则的 | 基于预定义规则的转换 | 大小写 |'
- en: '| Word level | Noise | Random word changes | “cat” to “dog” |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 词级别 | 噪声 | 随机更改单词 | “cat”到“dog” |'
- en: '| Synonyms | Replacing words with their synonyms | “happy” to “joyful” |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 同义词 | 用同义词替换单词 | “happy”到“joyful” |'
- en: '| Embeddings | Using word embeddings for replacement | “king” to “monarch”
    |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 嵌入 | 使用词嵌入进行替换 | “king”到“monarch” |'
- en: '| Language models | Leveraging advanced language models for word replacement
    | BERT |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 语言模型 | 利用高级语言模型进行单词替换 | BERT |'
- en: '| Phrase level | Structure | Altering the structure of phrases | Changing word
    order |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 短语级别 | 结构 | 改变短语的结构 | 改变单词顺序 |'
- en: '| Interpolation | Merging features of two phrases | “The cat sat” + “The dog
    barked” = “The cat barked” |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 插值 | 合并两个短语的特性 | “The cat sat” + “The dog barked” = “The cat barked” |'
- en: '| Document level | Translation | Translating the document to another language
    and back | English to French to English |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 文档级别 | 翻译 | 将文档翻译成另一种语言再翻译回原文 | 英文到法文再到英文 |'
- en: '| Generative | Using models to generate new content | GPT-3 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 生成式 | 使用模型生成新内容 | GPT-3 |'
- en: Table 7.3 – The categorization of different data augmentation methods (adapted
    from [13])
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3 – 不同数据增强方法的分类（改编自[13]）
- en: Data augmentation techniques for text can be categorized into character, word,
    phrase, and document levels. Techniques vary from jumbling characters to using
    models such as BERT and GPT-3\. This taxonomy guides us through NLP data augmentation
    methods.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 文本数据增强技术可以根据字符、单词、短语和文档级别进行分类。这些技术从字符打乱到使用BERT和GPT-3等模型，这一分类法引导我们了解NLP数据增强方法。
- en: '*Table 7.3* shows various data augmentation methods used in NLP. We will break
    down the methods based on the level at which the data is manipulated – character,
    word, phrase, and document. Each level has its unique set of methods, such as
    introducing “noise” at the character level or leveraging “language models” at
    the word level. These methods are not just random transformations; they are often
    carefully designed to preserve the semantic meaning of the text while introducing
    variability.'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '*表7.3* 展示了在NLP中使用的各种数据增强方法。我们将根据数据操作的水平来分解这些方法——字符、单词、短语和文档。每个级别都有其独特的方法集，例如在字符级别引入“噪声”或在单词级别利用“语言模型”。这些方法不仅仅是随机变换；它们通常被精心设计，以在引入可变性同时保留文本的语义意义。'
- en: What sets this categorization apart is its multilayered approach, which allows
    a more targeted application of data augmentation methods. For instance, if you’re
    dealing with short text snippets, methods at the character or word level may be
    more appropriate. On the other hand, if you’re working with longer documents or
    need to generate entirely new content, then methods at the document level, such
    as “generative” techniques, come into play.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 使这种分类与众不同的特点是它的多层次方法，这允许更精确地应用数据增强方法。例如，如果你处理的是短文本片段，字符或单词级别的方法可能更合适。另一方面，如果你处理的是较长的文档或需要生成全新的内容，那么文档级别的“生成”技术等方法就派上用场。
- en: In the subsequent sections, we will explore a text classification dataset that
    is imbalanced and illustrate various data augmentation techniques using it. These
    methodologies are designed to synthesize additional data, thereby enhancing a
    model’s ability to learn and generalize from the imbalanced information.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨一个不平衡的文本分类数据集，并使用它来展示各种数据增强技术。这些方法旨在合成额外的数据，从而增强模型从不平衡信息中学习和泛化的能力。
- en: Dataset and baseline model
  id: totrans-263
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集和基线模型
- en: Let’s take the spam text message classification dataset available on Kaggle
    ([https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification)).
    This dataset, primarily used to distinguish spam from legitimate messages, presents
    an imbalance with a majority of “ham” (legitimate) messages and a minority of
    “spam” messages. We are skipping the code here to save space. You can find the
    notebook in the GitHub repo with the name `Data_level_techniques_NLP.ipynb`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以Kaggle上可用的垃圾邮件文本消息分类数据集为例（[https://www.kaggle.com/datasets/team-ai/spam-text-message-classification](https://www.kaggle.com/datasets/team-ai/spam-text-message-classification)）。这个数据集主要用于区分垃圾邮件和合法消息，存在不平衡，大多数是“ham”（合法）消息，少数是“spam”（垃圾）消息。这里省略代码以节省空间。你可以在GitHub仓库中找到名为`Data_level_techniques_NLP.ipynb`的笔记本。
- en: 'With a baseline model, we have the following results:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 使用基线模型，我们得到了以下结果：
- en: '[PRE24]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Random oversampling
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机过采样
- en: 'One basic technique to handle data imbalance is random oversampling, where
    instances of the minority class are replicated to balance out the class distribution.
    While this technique is easy to implement and often shows improved performance,
    it’s essential to be wary of overfitting:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据不平衡的一个基本技术是随机过采样，即复制少数类的实例以平衡类别分布。虽然这种方法易于实现并且通常显示出改进的性能，但必须警惕过拟合：
- en: '[PRE25]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Random oversampling shows a slight improvement in overall accuracy, rising from
    0.97 to 0.98\. The most notable gain is in the recall for the `spam` class, which
    increased from 0.80 to 0.91, indicating better identification of spam messages.
    However, the precision for `spam` dropped a bit from 0.97 to 0.93.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 随机过采样在整体准确率上略有提升，从0.97上升到0.98。最显著的提升是在`spam`类别的召回率上，从0.80上升到0.91，表明对垃圾邮件消息的识别能力更好。然而，`spam`的精确度略有下降，从0.97下降到0.93。
- en: The macro average F1-score also improved from 0.93 to 0.95, suggesting that
    the model is now better at handling both classes (`ham` and `spam`) more equally.
    The weighted average metrics remain strong, reinforcing that the model’s overall
    performance has improved without sacrificing its ability to correctly classify
    the majority class (`ham`).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 宏观平均F1分数也从0.93提高到0.95，这表明模型现在在处理两个类别（`ham`和`spam`）方面更加均衡。加权平均指标仍然强劲，进一步证实了模型的整体性能得到了提升，而没有牺牲其正确分类大多数类别（`ham`）的能力。
- en: Similarly, undersampling can be useful to reduce the size of the majority class,
    particularly by eliminating exact duplicate sentences. For example, you might
    not need 500 copies of “Thanks very much!” However, sentences with similar semantic
    meaning but different wording, such as “Thanks very much!” and “Thanks so much!”,
    should generally be retained. Exact duplicates can be identified, using methods
    such as string matching, while sentences with similar meanings can be detected
    using cosine similarity or the Jaccard similarity of sentence embeddings.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，欠采样可以用来减少大多数类别的规模，特别是通过消除完全重复的句子。例如，你可能不需要500个“非常感谢！”的副本。然而，具有相似语义但措辞不同的句子，如“非常感谢！”和“非常感谢！”，通常应该保留。可以使用字符串匹配等方法识别完全重复的句子，而具有相似意义的句子可以使用余弦相似度或句子嵌入的Jaccard相似度来检测。
- en: 🚀 Deep learning data-level techniques in production at Cloudflare
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 🚀 Cloudflare在生产环境中应用深度学习数据级技术
- en: '**🎯** **Problem** **being solved**:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎯** **待解决的问题**:'
- en: Cloudflare [14] aimed to enhance its **Web Application Firewall** (**WAF**)
    to better identify malicious HTTP requests and protect against common attacks,
    such as SQL injection and **cross-site** **scripting** (**XSS**).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: Cloudflare [14] 旨在增强其 **Web应用防火墙**（**WAF**），以更好地识别恶意HTTP请求并防范常见的攻击，如SQL注入和**跨站脚本**（**XSS**）。
- en: '**⚖️** **Data** **imbalance issue**:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '**⚖️** **数据不平衡问题**:'
- en: Creating a quality dataset to train the WAF model was difficult, due to strict
    privacy regulations and the absence of labeled data for malicious HTTP requests.
    The heterogeneity of samples also presented challenges as the requests came in
    various formats and encodings. There was a significant lack of samples for specific
    types of attacks, making the dataset imbalanced and leading to the risk of false
    positives or negatives.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 由于严格的隐私法规和缺乏恶意HTTP请求的标记数据，创建高质量的训练WAF模型的数据库很困难。样本的异质性也带来了挑战，因为请求以各种格式和编码方式到来。对于特定类型的攻击，样本严重不足，导致数据集不平衡，增加了假阳性或假阴性的风险。
- en: '**🎨** **Data** **augmentation strategy**:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**🎨** **数据增强策略**:'
- en: To tackle this, Cloudflare employed a combination of data augmentation and generation
    techniques. These included mutating benign content in various ways, generating
    pseudo-random noise samples, and using language models for synthetic data creation.
    The focus was on increasing the diversity of negative samples while maintaining
    the integrity of the content, thereby forcing the model to consider a broader
    spectrum of structural, semantic, and statistical properties for better classification.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，Cloudflare采用了数据增强和生成技术的组合。这包括以各种方式变异良性内容，生成伪随机噪声样本，以及使用语言模型进行合成数据创建。重点是增加负样本的多样性，同时保持内容的完整性，从而迫使模型考虑更广泛的结构、语义和统计特性，以实现更好的分类。
- en: '🚀 **Model deployment**:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '🚀 **模型部署**:'
- en: The model that they used significantly improved after employing these data augmentation
    techniques, with a remarkable F1 score of 0.99 after augmentation compared to
    0.61 before. The model has been validated against Cloudflare’s signature-based
    WAF and was found to perform comparably, making it production-ready.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 他们使用的模型在采用这些数据增强技术后显著改进，增强后的F1分数达到了0.99，而增强前的F1分数为0.61。该模型已通过Cloudflare基于签名的WAF进行验证，发现其性能相当，因此可以用于生产。
- en: Document-level augmentation
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 文档级增强
- en: In document-level augmentation, entire documents are modified to create new
    examples, in order to preserve the broader semantic context or narrative flow
    of the document. One such technique is back translation.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在文档级增强中，整个文档被修改以创建新的示例，以保留文档的更广泛语义上下文或叙事流程。其中一种技术是反向翻译。
- en: Back translation
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向翻译
- en: Back translation involves translating a sentence to a different language and
    then reverting it back to the original (*Figure 7**.16*). This produces sentences
    that are syntactically different but semantically similar to the original text,
    providing a form of augmentation.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 反向翻译涉及将一个句子翻译成另一种语言，然后再将其翻译回原文（*图7**.16*）。这会产生与原文在句法上不同但语义上相似的句子，提供了一种增强形式。
- en: '![](img/B17259_07_16.jpg)'
  id: totrans-286
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17259_07_16.jpg)'
- en: Figure 7.16 – Demonstrating the back translation technique
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 – 反向翻译技术的演示
- en: We generate the back-translated text and append it to the original dataset.
    Then, we use the full dataset to train the logistic regression model. Note that
    this can be a time-consuming process, since the translation model binaries are
    resource-intensive. It may also introduce errors, since some words may not be
    exactly translatable across languages. In the GitHub notebook, we used the `BackTranslationAug`
    API from the `nlpaug` library [15].
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成反向翻译的文本并将其附加到原始数据集上。然后，我们使用完整的数据集来训练逻辑回归模型。请注意，这可能是一个耗时的过程，因为翻译模型的二进制文件资源密集。它还可能引入错误，因为某些单词可能无法在语言之间精确翻译。在GitHub笔记本中，我们使用了`nlpaug`库中的`BackTranslationAug`
    API [15]。
- en: 'The following results show the classification metrics on the test set. The
    precision of the spam class shows improvement over the random oversampling technique,
    while recall is a bit worse:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 以下结果展示了测试集上的分类指标。垃圾邮件类别的精确度相较于随机过采样技术有所提升，而召回率略有下降：
- en: '[PRE26]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Back translation maintains an overall accuracy of 0.98, similar to random oversampling.
    It slightly improves `spam` precision to 0.96 but lowers recall to 0.86\. Both
    methods outperform the baseline, with back translation favoring precision over
    recall for the `spam` class.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 反向翻译保持了整体准确率0.98，与随机过采样相似。它略微提高了`垃圾邮件`的精确度至0.96，但将召回率降低至0.86。两种方法都优于基线，反向翻译在`垃圾邮件`类别中更倾向于精确度而非召回率。
- en: Character and word-level augmentation
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 字符和词级增强
- en: Let’s briefly go over a few character and word-level augmentation techniques
    that can be applied to NLP problems.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下可以应用于NLP问题的几个字符和词级增强技术。
- en: Easy Data Augmentation techniques
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简易数据增强技术
- en: '**Easy Data Augmentation** (**EDA**) is a suite of data augmentation techniques
    specific to text data. It includes simple operations such as synonym replacement,
    random insertion, random swap, and random deletion. These operations, being simple,
    ensure that the augmented data remains meaningful. The following table shows various
    metrics when using EDA on the dataset:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**简易数据增强**（**EDA**）是一套针对文本数据特定的数据增强技术。它包括诸如同义词替换、随机插入、随机交换和随机删除等简单操作。这些操作简单，确保增强后的数据仍然具有意义。以下表格展示了在数据集上使用EDA时的各种指标：'
- en: '[PRE27]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: After applying EDA, the model retains an overall accuracy of 0.98, consistent
    with both random oversampling and back translation. The precision for `spam` is
    high at 0.96, similar to back translation, while the recall is slightly better
    at 0.88 compared to 0.86 with Back Translation. The macro and weighted averages
    remain robust at 0.95 and 0.98, respectively.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 应用EDA后，模型保持了整体准确率0.98，与随机过采样和反向翻译一致。`垃圾邮件`的精确度很高，为0.96，与反向翻译相似，而召回率略好于反向翻译的0.86。宏平均和加权平均分别保持在0.95和0.98。
- en: EDA offers a balanced improvement in both precision and recall for the `spam`
    class, making it a strong contender among the data augmentation techniques we’ve
    tried.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: EDA在`垃圾邮件`类别的精确度和召回率上提供了平衡的改进，使其成为我们尝试过的数据增强技术中的有力竞争者。
- en: '|  | **Precision** | **Recall** | **F1-Score** | **Accuracy** |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确度** | **召回率** | **F1分数** | **准确度** |'
- en: '| Baseline model | **0.97** | 0.80 | 0.88 | 0.97 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 基线模型 | **0.97** | 0.80 | 0.88 | 0.97 |'
- en: '| Random oversampling | 0.93 | **0.91** | **0.92** | **0.98** |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 随机过采样 | 0.93 | **0.91** | **0.92** | **0.98** |'
- en: '| Back translation | 0.96 | 0.86 | 0.91 | **0.98** |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 反向翻译 | 0.96 | 0.86 | 0.91 | **0.98** |'
- en: '| EDA | 0.96 | 0.88 | 0.91 | **0.98** |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| EDA | 0.96 | 0.88 | 0.91 | **0.98** |'
- en: Table 7.4 – Comparing the results of the various NLP data-level techniques for
    the spam class (max per metric in bold)
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.4 – 比较各种NLP数据级技术对垃圾邮件类别的结果（每个指标的最大值以粗体显示）
- en: Overall, as we can see in *Table 7.4*, for our dataset, random oversampling
    excels in recall for `spam` but slightly lowers precision. Back translation boosts
    precision at a minor recall trade-off. EDA offers a balanced improvement in both.
    It’s important to note that these results are empirical and specific to the dataset
    used for this analysis. Data augmentation techniques can yield different outcomes,
    depending on the nature of the data, its distribution, and the problem being addressed.
    Therefore, while these techniques show promise in this context, their effectiveness
    may vary when applied to different datasets or NLP tasks.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，如*表7.4*所示，对于我们的数据集，随机过采样在`垃圾邮件`的召回率方面表现优异，但略微降低了精确度。反向翻译在略微牺牲召回率的同时提高了精确度。EDA在两者方面都提供了平衡的改进。需要注意的是，这些结果是经验性的，并且特定于用于此分析的数据集。数据增强技术可能会产生不同的结果，这取决于数据的性质、其分布以及所解决的问题。因此，虽然这些技术在当前背景下显示出希望，但它们在不同数据集或NLP任务中的应用效果可能会有所不同。
- en: We will not be covering phrase-level augmentation techniques in this book due
    to space constraints, but we recommend exploring them on your own.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 由于篇幅限制，本书将不会涵盖短语级增强技术，但我们建议您自行探索。
- en: Next, we will look at some miscellaneous data-level deep learning techniques
    at a high level.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从高层次上查看一些其他的数据级深度学习技术。
- en: Discussion of other data-level deep learning methods and their key ideas
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他数据级深度学习方法和其关键思想的讨论
- en: In addition to the methods previously discussed, there is a rich array of other
    techniques specifically designed to address imbalanced data challenges. This section
    provides a high-level overview of these alternative approaches, each offering
    unique insights and potential advantages. While we will only touch upon their
    key ideas, we encourage you to delve deeper into the literature and explore them
    further if you find these techniques intriguing.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 除了之前讨论的方法之外，还有一系列专门设计来处理不平衡数据挑战的其他技术。本节提供了这些替代方法的概述，每个方法都提供了独特的见解和潜在优势。虽然我们只会触及它们的关键思想，但我们鼓励您深入研究文献，并在发现这些技术有趣时进一步探索。
- en: Two-phase learning
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 两阶段学习
- en: 'Two-phase learning [16][17] is a technique designed to enhance the performance
    of minority classes in multi-class classification problems, without compromising
    the performance of majority classes. The process involves two training phases:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 两阶段学习[16][17]是一种旨在在不损害多数类性能的情况下，提高多类分类问题中少数类性能的技术。该过程涉及两个训练阶段：
- en: In the first phase, a deep learning model is first trained on the dataset, which
    is balanced with respect to each class. Balancing can be done using sampling techniques
    such as random oversampling or undersampling.
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一阶段，首先在平衡每个类别的数据集上训练一个深度学习模型。平衡可以通过使用随机过采样或欠采样等技术来实现。
- en: In the second phase, we freeze all the layers except the last one, and then
    the model is fine-tuned using the entire dataset.
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第二阶段，我们冻结所有层除了最后一层，然后使用整个数据集对模型进行微调。
- en: The first phase ensures that all layers are trained on a balanced dataset. The
    second phase calibrates the output probabilities by retraining the last layer
    with the entire dataset, reflecting the original imbalanced class distribution.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 第一阶段确保所有层都在平衡数据集上训练。第二阶段通过使用整个数据集重新训练最后一层来校准输出概率，反映了原始不平衡的类别分布。
- en: The order of the two phases can be reversed – that is, the first model is trained
    on the full imbalanced data and then fine-tuned on a balanced dataset in the second
    phase. This is called **deferred sampling**, since sampling is done later.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 两个阶段的顺序可以颠倒——也就是说，第一个模型在完整的不平衡数据上训练，然后在第二阶段在平衡数据集上进行微调。这被称为**延迟采样**，因为采样是在之后进行的。
- en: Expansive Over-Sampling
  id: totrans-316
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展过采样
- en: Introduced in a paper by Damien Dablain et al. [18], **Expansive Over-Sampling**
    (**EOS**) is another data augmentation technique used within a three-phase CNN
    training framework, designed for imbalanced data. It can be considered to incorporate
    both two-phase learning and data augmentation techniques.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 由Damien Dablain等人[18]在论文中引入的**扩展过采样**（**EOS**）是另一种在三个阶段的CNN训练框架中使用的数据增强技术，旨在处理不平衡数据。它可以被认为是结合了两阶段学习和数据增强技术。
- en: EOS works by creating synthetic training instances as combinations between the
    minority class samples and their nearest “enemies” in the embedded space. The
    term “nearest enemies” refers to instances of other classes that are closest in
    the feature space to a given instance. By creating synthetic instances in this
    way, EOS aims to reduce the generalization gap, which is wider for minority classes.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: EOS通过在嵌入空间中少数类样本及其最近的“敌人”之间创建合成训练实例来工作。术语“最近的敌人”指的是在特征空间中与给定实例最接近的其他类别的实例。通过这种方式创建合成实例，EOS旨在减少泛化差距，对于少数类来说，这个差距更大。
- en: The paper’s authors [18] claimed that this method improves accuracy and efficiency
    over common imbalanced learning techniques, requiring fewer parameters and less
    training time.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 论文的作者[18]声称，这种方法比常见的平衡学习技术提高了准确性和效率，需要更少的参数和更少的训练时间。
- en: Using generative models for oversampling
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用生成模型进行过采样
- en: Generative models, including GANs, **Variational AutoEncoders** (**VAEs**),
    diffusion models, and their derivatives, such as StyleGAN, StyleGAN2, and GPT-based
    models, have become prominent tools for producing data points that resemble training
    data.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 生成模型，包括GANs、**变分自编码器**（**VAEs**）、扩散模型及其衍生品，如StyleGAN、StyleGAN2和基于GPT的模型，已成为生成类似于训练数据的数据点的突出工具。
- en: VAEs, a specific type of generative model, consist of an encoder and decoder
    that work together to create new instances of data, such as realistic images,
    and can be used to balance imbalanced datasets. On the long-tailed version of
    MNIST, we got a decent performance improvement by using a VAE-augmented model
    when compared to the baseline model on the most imbalanced classes. *Figure 7**.17*
    shows the performance comparison after 50 epochs. You can find the notebook in
    the corresponding chapter of the GitHub repo.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs（变分自编码器），一种特定的生成模型，由一个编码器和一个解码器组成，它们协同工作以创建新的数据实例，例如逼真的图像，并且可以用于平衡不平衡的数据集。在MNIST长尾版本上，我们通过使用增强的VAE模型与基线模型相比，在最不平衡的类别上获得了相当的性能提升。*图7.17*显示了50个epoch后的性能比较。您可以在GitHub仓库的相应章节中找到笔记本。
- en: '![](img/B17259_07_17.jpg)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_07_17.jpg)'
- en: Figure 7.17 – VAE-augmented model performance on the long-tailed MNIST dataset
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 – VAE增强模型在长尾MNIST数据集上的性能
- en: Diffusion models operate by progressively corrupting an image with noise and
    then reconstructing it, with applications in areas such as medical imaging. Examples
    include DALLE-2 and the open source stable diffusion model.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 扩散模型通过逐步用噪声破坏图像并随后重建它来运行，在医学成像等领域有应用。例如包括DALLE-2和开源的稳定扩散模型。
- en: Recent studies [19] highlight the utility of synthetic data in enhancing zero-shot
    and few-shot image classification tasks. Specifically, text-to-image generation
    models, when used in conjunction with large-scale pre-trained models such as DALL-E
    and Stable Diffusion, significantly improve performance in scenarios where real-world
    data is sparse or unavailable. These generative models have gained prominence
    for their ability to create high-quality images based on natural language prompts,
    offering a potential solution for imbalanced datasets. For example, if there is
    a scarcity of images featuring a monkey seated in a car, these models can generate
    hundreds or even thousands of such images to augment training datasets. However,
    it’s worth noting that models trained solely on synthetic data may still underperform
    compared to those trained on real data.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 近期研究[19]强调了合成数据在增强零样本和少样本图像分类任务中的实用性。具体来说，当与大型预训练模型如DALL-E和稳定扩散结合使用时，文本到图像生成模型在现实世界数据稀少或不可用的情况下显著提高了性能。这些生成模型因其能够根据自然语言提示创建高质量图像的能力而备受关注，为不平衡数据集提供了一种潜在的解决方案。例如，如果缺少猴子坐在车里的图像，这些模型可以生成数百甚至数千张这样的图像来扩充训练数据集。然而，值得注意的是，仅用合成数据进行训练的模型可能仍然比那些用真实数据进行训练的模型表现不佳。
- en: These models often require significant computational resources, making them
    time-consuming and expensive to scale up, especially for vast datasets. Diffusion
    models, in particular, are computationally intensive, and potential overfitting
    can compromise model generalizability. Therefore, it is crucial to balance the
    benefits of data augmentation with the computational cost and potential challenges
    when employing these advanced generative models.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型通常需要大量的计算资源，这使得它们在扩展时既耗时又昂贵，尤其是在大规模数据集的情况下。特别是扩散模型，计算密集，潜在的过度拟合可能会损害模型的泛化能力。因此，在采用这些高级生成模型时，平衡数据增强的好处与计算成本和潜在挑战至关重要。
- en: DeepSMOTE
  id: totrans-328
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DeepSMOTE
- en: 'The **Deep Synthetic Minority Oversampling** (**DeepSMOTE**) technique [20]
    is essentially SMOTE adapted for deep learning models using an encoder-decoder
    architecture, with minor tweaks for image data. DeepSMOTE consists of three major
    components:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度合成少数类过采样**（**DeepSMOTE**）技术[20]本质上是对SMOTE进行了适配，使其适用于深度学习模型，使用编码器-解码器架构，并对图像数据进行了细微调整。DeepSMOTE由三个主要组件组成：'
- en: '**An encoder/decoder framework to handle complex and high-dimensional data**:
    An encoder/decoder framework is used to learn a compact feature representation
    of the image data. It is trained to reconstruct the original images from this
    compact form, ensuring that essential features are captured.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个用于处理复杂和高维数据的编码器/解码器框架**：使用编码器/解码器框架来学习图像数据的紧凑特征表示。它被训练从这种紧凑形式重建原始图像，确保捕获了基本特征。'
- en: '**SMOTE-based oversampling for generating synthetic instances**: Once the feature
    representation is learned, SMOTE is applied in this feature space to generate
    synthetic instances of the minority class. This is particularly useful for image
    data where the raw data is high-dimensional and complex. SMOTE creates these synthetic
    instances by finding the *k*-nearest neighbors in the feature space and generating
    new instances that are interpolations between the instance under consideration
    and its neighbors.'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于SMOTE的过采样生成合成实例**：一旦学习到特征表示，SMOTE就应用于这个特征空间以生成少数类的合成实例。这在原始数据高维且复杂的情况下，如图像数据，尤其有用。SMOTE通过在特征空间中找到最近的*
    k*个邻居，并生成介于待考虑实例及其邻居之间的插值新实例来创建这些合成实例。'
- en: '**A dedicated loss function**: DeepSMOTE introduces a specialized loss function
    that not only focuses on the reconstruction error (how well the decoder can reconstruct
    the original image from the encoded form) but also includes a penalty term, ensuring
    that the synthetic instances are useful for the classification task.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一个专门的损失函数**：DeepSMOTE引入了一个专门的损失函数，它不仅关注重建误差（解码器从编码形式重建原始图像的能力），还包括一个惩罚项，确保合成实例对分类任务有用。'
- en: Unlike GAN-based oversampling, DeepSMOTE does not require a discriminator. It
    claims to generate high-quality, information-rich synthetic images that can be
    visually inspected.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于GAN的过采样不同，DeepSMOTE不需要判别器。它声称可以生成高质量、信息丰富的合成图像，这些图像可以进行视觉检查。
- en: '![](img/B17259_07_18.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_07_18.jpg)'
- en: Figure 7.18 – Demonstrating the DeepSMOTE technique (adapted from [20])
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 – 展示DeepSMOTE技术（改编自[20]）
- en: Neural style transfer
  id: totrans-336
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经风格迁移
- en: Neural style transfer is a technique in deep learning that artistically blends
    the content of one image with the style of another (*Figure 7**.19*). While its
    primary application is in art and image processing, the concept of generating
    synthetic data samples can be adapted to address data imbalance in machine learning.
    By drawing inspiration from style transfer, one could potentially generate synthetic
    samples for the minority class, blend features of different classes, or adapt
    domain-specific knowledge. However, care must be taken to ensure that synthetic
    data authentically represents real-world scenarios to avoid overfitting and poor
    generalization of real data.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 神经风格迁移是深度学习中的一个技术，它艺术性地将一张图像的内容与另一张图像的风格相结合（*图7**.19*）。虽然其主要应用在艺术和图像处理中，但生成合成数据样本的概念可以适应解决机器学习中的数据不平衡问题。通过从风格迁移中汲取灵感，可以潜在地生成少数类的合成样本，混合不同类的特征，或适应特定领域的知识。然而，必须小心确保合成数据真实地代表现实世界场景，以避免过度拟合和真实数据的泛化能力差。
- en: '![](img/B17259_07_19.jpg)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17259_07_19.jpg)'
- en: Figure 7.19 – Demonstrating the neural style transfer technique
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 – 展示神经风格迁移技术
- en: We hope that this provides a thorough understanding of data-level deep learning
    methods to address imbalanced data, including oversampling, data augmentation,
    and various other strategies.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望这能为您提供对解决不平衡数据的数据级别深度学习方法的全面理解，包括过采样、数据增强以及其他各种策略。
- en: Summary
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The transition of methods to handle data imbalance from classical machine learning
    models to deep learning models can pose unique challenges, primarily due to the
    distinct types of data that these models have to work with. Classical machine
    learning models typically deal with structured, tabular data, whereas deep learning
    models often grapple with unstructured data, such as images, text, audio, and
    video. This chapter explored how to adapt sampling techniques to work with deep
    learning models. To facilitate this, we used an imbalanced version of the MNIST
    dataset to train a model, which is then employed in conjunction with various oversampling
    methods.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 从经典机器学习模型到深度学习模型处理数据不平衡的方法的转变可能会带来独特的挑战，这主要是因为这些模型必须处理的数据类型不同。经典机器学习模型通常处理结构化、表格数据，而深度学习模型通常处理非结构化数据，如图像、文本、音频和视频。本章探讨了如何调整采样技术以与深度学习模型一起工作。为此，我们使用MNIST数据集的不平衡版本来训练一个模型，然后将其与各种过采样方法结合使用。
- en: Incorporating random oversampling with deep learning models involves duplicating
    samples from minority classes randomly, until each class has an equal number of
    samples. This is usually performed using APIs from libraries such as imbalanced-learn,
    Keras, TensorFlow, or PyTorch, which work together seamlessly for this purpose.
    Once data is oversampled, it can be sent for model training in PyTorch or TensorFlow.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 将随机过采样与深度学习模型结合使用涉及随机复制少数类的样本，直到每个类别的样本数量相等。这通常使用imbalanced-learn、Keras、TensorFlow或PyTorch等库的API来完成，这些库可以无缝地协同工作。一旦数据过采样，就可以将其发送到PyTorch或TensorFlow进行模型训练。
- en: The chapter also delved into different data augmentation techniques, which can
    be especially beneficial when dealing with limited or imbalanced data. Augmentation
    techniques include rotating, scaling, cropping, blurring, and adding noise, among
    other advanced techniques such as AugMix, CutMix, and MixUp. However, care must
    be taken to ensure these augmentations preserve the original labels and do not
    inadvertently alter vital information in the data. We discussed other methods,
    such as two-phase learning and dynamic sampling, as potential strategies to improve
    model performance on imbalanced data. We also learned about some data-level techniques
    applicable to text, such as back translation and EDA, while running them on a
    spam/ham dataset.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还深入探讨了不同的数据增强技术，这些技术在处理有限或不平衡数据时特别有益。增强技术包括旋转、缩放、裁剪、模糊和添加噪声，以及其他高级技术，如AugMix、CutMix和MixUp。然而，必须小心确保这些增强不会改变原始标签，并且不会无意中改变数据中的关键信息。我们还讨论了其他方法，如两阶段学习和动态采样，作为提高不平衡数据模型性能的潜在策略。同时，我们还了解了一些适用于文本的数据级别技术，如回译和EDA，这些技术是在垃圾邮件/非垃圾邮件数据集上运行的。
- en: In the next chapter, we will look at some algorithm-based methods to deal with
    imbalanced datasets.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨一些基于算法的方法来处理不平衡数据集。
- en: Questions
  id: totrans-346
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: 'Apply Mixup interpolation to the Kaggle spam detection NLP dataset used in
    the chapter. See if Mixup helps to improve the model performance. You can refer
    to the paper *Augmenting Data with Mixup for Sentence Classification: An Empirical
    Study* by Guo et al. ([https://arxiv.org/pdf/1905.08941.pdf](https://arxiv.org/pdf/1905.08941.pdf))
    for further reading.'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Mixup插值应用于本章中使用的Kaggle垃圾邮件检测NLP数据集。看看Mixup是否有助于提高模型性能。您可以参考Guo等人撰写的论文《使用Mixup增强数据以进行句子分类：一项实证研究》以获取更多信息。[《使用Mixup增强数据以进行句子分类：一项实证研究》论文链接](https://arxiv.org/pdf/1905.08941.pdf)
- en: Refer to the FMix paper [21] and implement the FMix augmentation technique.
    Apply it to the Caltech101 dataset. See whether model performance improves by
    using FMix over the baseline model performance.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 参考FMix论文[21]并实现FMix增强技术。将其应用于Caltech101数据集。看看使用FMix是否比基线模型性能有所提高。
- en: Apply the EOS technique described in the chapter to the CIFAR-10-LT (the long-tailed
    version of CIFAR-10) dataset, and see whether the model performance improves for
    the most imbalanced classes.
  id: totrans-349
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本章中描述的EOS技术应用于CIFAR-10-LT（CIFAR-10的长尾版本）数据集，并查看模型性能是否对最不平衡的类别有所提高。
- en: Apply the MDSA techniques we studied in this chapter to the CIFAR-10-LT dataset,
    and see whether the model performance improves for the most imbalanced classes.
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本章中学习的MDSA技术应用于CIFAR-10-LT数据集，并查看模型性能是否对最不平衡的类别有所提高。
- en: References
  id: totrans-351
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent
    Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, and Mei-Ling
    Shyu. 2018\. *Dynamic Sampling in Convolutional Neural Networks for Imbalanced
    Data Classification*. In 2018 IEEE Conference on Multimedia Information Processing
    and Retrieval (MIPR), pages 112–117, Miami, FL, April. IEEE.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Samira Pouyanfar, Yudong Tao, Anup Mohan, Haiman Tian, Ahmed S. Kaseb, Kent
    Gauen, Ryan Dailey, Sarah Aghajanzadeh, Yung-Hsiang Lu, Shu-Ching Chen, 和 Mei-Ling
    Shyu. 2018\. *卷积神经网络在不平衡数据分类中的动态采样*. 载于2018年IEEE多媒体信息处理与检索会议（MIPR），第112–117页，佛罗里达州迈阿密，4月。IEEE。
- en: 'LeNet-5 paper, *Gradient-based learning applied to document* *classification*:
    [http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf).'
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: LeNet-5 论文，*基于梯度的学习应用于文档分类*：[http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf](http://vision.stanford.edu/cs598_spring07/papers/Lecun98.pdf)。
- en: 'AlexNet paper, *ImageNet Classification with Deep Convolutional Neural* *Networks*:
    [https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html).'
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AlexNet 论文，*使用深度卷积神经网络进行ImageNet分类*：[https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)。
- en: '*Leveraging Real-Time User Actions to Personalize Etsy Ads* (2023): [https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads](https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads).'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*利用实时用户行为来个性化Etsy广告（2023）: [https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads](https://www.etsy.com/codeascraft/leveraging-real-time-user-actions-to-personalize-etsy-ads)。'
- en: 'Automated image tagging at [Booking.com](http://Booking.com) (2017): [https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b](https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b).'
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '在[Booking.com](http://Booking.com)上的自动图像标记（2017）: [https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b](https://booking.ai/automated-image-tagging-at-booking-com-7704f27dcc8b)。'
- en: '*Shot Angle Prediction: Estimating Pose Angle with Deep Learning for Furniture
    Items Using Images Generated from 3D Models (**2020)*: [https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models](https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models).'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*通过使用从3D模型生成的图像进行深度学习来估计家具物品的拍摄角度（**2020**）: [https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models](https://www.aboutwayfair.com/tech-innovation/shot-angle-prediction-estimating-pose-angle-with-deep-learning-for-furniture-items-using-images-generated-from-3d-models)。'
- en: 'S. Yun, D. Han, S. Chun, S. J. Oh, Y. Yoo, and J. Choe, “*CutMix: Regularization
    Strategy to Train Strong Classifiers With Localizable Features*,” in 2019 IEEE/CVF
    International Conference on Computer Vision (ICCV), Seoul, Korea (South): IEEE,
    Oct. 2019, pp. 6022–6031\. doi: 10.1109/ICCV.2019.00612.'
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'S. Yun, D. Han, S. Chun, S. J. Oh, Y. Yoo, and J. Choe, “*CutMix: 用于训练具有可定位特征的强大分类器的正则化策略*，”载于2019年IEEE/CVF国际计算机视觉会议（ICCV），韩国首尔：IEEE，2019年10月，第6022–6031页。doi:
    10.1109/ICCV.2019.00612。'
- en: 'H. Zhang, M. Cisse, Y. N. Dauphin, and D. Lopez-Paz, “*mixup: Beyond Empirical
    Risk Minimization*.” arXiv, Apr. 27, 2018\. Accessed: Feb. 11, 2023\. [Online].
    Available: [http://arxiv.org/abs/1710.09412](http://arxiv.org/abs/1710.09412).'
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H. Zhang, M. Cisse, Y. N. Dauphin, 和 D. Lopez-Paz, “*mixup: 超越经验风险最小化*。” arXiv，2018年4月27日。访问日期：2023年2月11日。[在线]。可获取：[http://arxiv.org/abs/1710.09412](http://arxiv.org/abs/1710.09412)。'
- en: R. Geirhos, C. R. M. Temme, J. Rauber, H. H. Schütt, M. Bethge, and F. A. Wichmann,
    “*Generalisation in humans and deep* *neural networks*.”
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. Geirhos, C. R. M. Temme, J. Rauber, H. H. Schütt, M. Bethge, 和 F. A. Wichmann,
    “*人类和深度神经网络的泛化*。”
- en: 'D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, and B. Lakshminarayanan,
    “*AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty*.”
    arXiv, Feb. 17, 2020\. Accessed: Aug. 01, 2023\. [Online]. Available: [http://arxiv.org/abs/1912.02781](http://arxiv.org/abs/1912.02781).'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'D. Hendrycks, N. Mu, E. D. Cubuk, B. Zoph, J. Gilmer, 和 B. Lakshminarayanan,
    “*AugMix: 一种简单的数据处理方法，以提高鲁棒性和不确定性*。” arXiv，2020年2月17日。访问日期：2023年8月1日。[在线]。可获取：[http://arxiv.org/abs/1912.02781](http://arxiv.org/abs/1912.02781)。'
- en: 'H.-P. Chou, S.-C. Chang, J.-Y. Pan, W. Wei, and D.-C. Juan, “*Remix: Rebalanced
    Mixup*.” arXiv, Nov. 19, 2020\. Accessed: Aug. 15, 2023\. [Online]. Available:
    [http://arxiv.org/abs/2007.03943](http://arxiv.org/abs/2007.03943).'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H.-P. Chou, S.-C. Chang, J.-Y. Pan, W. Wei, 和 D.-C. Juan，"Remix: Rebalanced
    Mixup"，arXiv，2020年11月19日。访问日期：2023年8月15日。[在线]。可获取：[http://arxiv.org/abs/2007.03943](http://arxiv.org/abs/2007.03943).'
- en: '*Protecting Personal Data in Grab’s Imagery* (2021): [https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Grab的图像中保护个人数据* (2021)：[https://engineering.grab.com/protecting-personal-data-in-grabs-imagery](https://engineering.grab.com/protecting-personal-data-in-grabs-imagery).'
- en: 'M. Bayer, M.-A. Kaufhold, and C. Reuter, “*A Survey on Data Augmentation for
    Text Classification*,” ACM Comput. Surv., vol. 55, no. 7, pp. 1–39, Jul. 2023,
    doi: 10.1145/3544558.'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'M. Bayer, M.-A. Kaufhold, 和 C. Reuter，"文本分类的数据增强综述"，ACM Comput. Surv.，第55卷，第7期，第1-39页，2023年7月，doi:
    10.1145/3544558.'
- en: '*Improving the accuracy of our machine learning WAF using data augmentation
    and sampling* (2022), Vikram Grover: [https://blog.cloudflare.com/data-generation-and-sampling-strategies/](https://blog.cloudflare.com/data-generation-and-sampling-strategies/).'
  id: totrans-365
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用数据增强和采样提高我们机器学习WAF的准确性* (2022), Vikram Grover: [https://blog.cloudflare.com/data-generation-and-sampling-strategies/](https://blog.cloudflare.com/data-generation-and-sampling-strategies/).'
- en: '*Data augmentation for* *NLP*: [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*NLP的数据增强*: [https://github.com/makcedward/nlpaug](https://github.com/makcedward/nlpaug).'
- en: 'B. Kang *et al.*, “*Decoupling Representation and Classifier for Long-Tailed
    Recognition*.” arXiv, Feb. 19, 2020\. Accessed: Dec. 15, 2022\. [Online]. Available:
    [http://arxiv.org/abs/1910.09217](http://arxiv.org/abs/1910.09217).'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: B. Kang *et al.*，"解耦表示和分类器以实现长尾识别"，arXiv，2020年2月19日。访问日期：2022年12月15日。[在线]。可获取：[http://arxiv.org/abs/1910.09217](http://arxiv.org/abs/1910.09217).
- en: 'K. Cao, C. Wei, A. Gaidon, N. Arechiga, and T. Ma, “*Learning Imbalanced Datasets
    with Label-Distribution-Aware Margin Loss*”, [Online]. Available: [https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Cao, C. Wei, A. Gaidon, N. Arechiga, 和 T. Ma，"使用标签分布感知边缘损失的平衡数据集学习"，[在线]。可获取：[https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf](https://proceedings.neurips.cc/paper/2019/file/621461af90cadfdaf0e8d4cc25129f91-Paper.pdf).
- en: 'D. Dablain, C. Bellinger, B. Krawczyk, and N. Chawla, “*Efficient Augmentation
    for Imbalanced Deep Learning*.” arXiv, Oct. 17, 2022\. Accessed: Jul. 23, 2023\.
    [Online]. Available: [http://arxiv.org/abs/2207.06080](http://arxiv.org/abs/2207.06080).'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: D. Dablain, C. Bellinger, B. Krawczyk, 和 N. Chawla，"不平衡深度学习的有效增强"，arXiv，2022年10月17日。访问日期：2023年7月23日。[在线]。可获取：[http://arxiv.org/abs/2207.06080](http://arxiv.org/abs/2207.06080).
- en: 'R. He *et al.*, “*Is synthetic data from generative models ready for image
    recognition?*” arXiv, Feb. 15, 2023\. Accessed: Aug. 06, 2023\. [Online]. Available:
    [http://arxiv.org/abs/2210.07574](http://arxiv.org/abs/2210.07574).'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: R. He *et al.*，"生成模型合成的合成数据是否适用于图像识别？" arXiv，2023年2月15日。访问日期：2023年8月6日。[在线]。可获取：[http://arxiv.org/abs/2210.07574](http://arxiv.org/abs/2210.07574).
- en: 'D. Dablain, B. Krawczyk, and N. V. Chawla, “*DeepSMOTE: Fusing Deep Learning
    and SMOTE for Imbalanced Data*,” IEEE Transactions on Neural Networks and Learning
    Systems, pp. 1–15, 2022, doi: 10.1109/TNNLS.2021.3136503.'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'D. Dablain, B. Krawczyk, 和 N. V. Chawla，"DeepSMOTE:融合深度学习和SMOTE的平衡数据"，IEEE
    Transactions on Neural Networks and Learning Systems，第33卷，第1-15页，2022年，doi: 10.1109/TNNLS.2021.3136503.'
- en: 'E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel-Bennett, and J. Hare,
    “*FMix: Enhancing Mixed Sample Data Augmentation*.” arXiv, Feb. 28, 2021\. Accessed:
    Aug. 08, 2023\. [Online]. Available: [http://arxiv.org/abs/2002.12047](http://arxiv.org/abs/2002.12047).'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: E. Harris, A. Marcu, M. Painter, M. Niranjan, A. Prügel-Bennett, 和 J. Hare，"FMix:增强混合样本数据增强"，arXiv，2021年2月28日。访问日期：2023年8月8日。[在线]。可获取：[http://arxiv.org/abs/2002.12047](http://arxiv.org/abs/2002.12047).
