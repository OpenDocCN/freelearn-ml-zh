- en: Recommender System – Getting to Know Their Taste
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统 – 了解用户的口味
- en: A layperson might not know about the sophisticated machine learning algorithms
    controlling the high-frequency transactions taking place in the stock exchange.
    They may also not know about the algorithms detecting online crimes and controlling
    missions to outer space. Yet, they interact with recommendation engines every
    day. They are daily witnesses of the recommendation engines picking books for
    them to read on Amazon, selecting which movies they should watch next on Netflix,
    and influencing the news articles they read every day. The prevalence of recommendation
    engines in many businesses requires different flavors of recommendation algorithms.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 一个外行可能不知道那些控制股票交易所高频交易的复杂机器学习算法。他们也许不了解那些检测在线犯罪和控制外太空任务的算法。然而，他们每天都会与推荐引擎互动。他们是推荐引擎每天为他们挑选亚马逊上的书籍、在
    Netflix 上选择他们下一部应该观看的电影、以及影响他们每天阅读新闻文章的见证者。推荐引擎在许多行业中的普及要求采用不同版本的推荐算法。
- en: 'In this chapter, we will learn about the different approaches used by recommender
    systems. We will mainly use a sister library to scikit-learn called Surprise.
    Surprise is a toolkit that implements different collaborative filtering algorithms.
    So, we will start by learning the differences between the *c**ollaborative filtering*
    algorithms and the *content-based filtering* algorithms used in a recommendation
    engine. We will also learn how to package our trained models to be used by other
    software without the need for retraining. The following topics will be discussed
    here:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习推荐系统使用的不同方法。我们将主要使用一个与 scikit-learn 相关的库——Surprise。Surprise 是一个实现不同协同过滤算法的工具包。因此，我们将从学习*协同过滤*算法和*基于内容的过滤*算法在推荐引擎中的区别开始。我们还将学习如何将训练好的模型打包，以便其他软件使用而无需重新训练。以下是本章将讨论的主题：
- en: The different recommendation paradigms
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的推荐范式
- en: Downloading Surprise and the dataset
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载 Surprise 和数据集
- en: Using KNN-inspired algorithms
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于 KNN 的算法
- en: Using baseline algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基准算法
- en: Using singular value decomposition
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用奇异值分解
- en: Deploying machine learning models in production
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中部署机器学习模型
- en: The different recommendation paradigms
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同的推荐范式
- en: 'In a recommendation task, you have a set of users interacting with a set of
    items and your job is to figure out which items are suitable for which users.
    You may know a thing or two about each user: where they live, how much they earn,
    whether they are logged in via their phone or their tablet, and more. Similarly,
    for an item—say, a movie—you know its genre, its production year, and how many
    Academy Awards it has won. Clearly, this looks like a classification problem.
    You can combine the user features with the item features and build a classifier
    for each user-item pair, and then try to predict whether the user will like the
    item or not. This approach is known as **content-based filtering**. As its name
    suggests, it is as good as the content or the features extracted from each user
    and each item. In practice, you may only know basic information about each user.
    A user''s location or gender may reveal enough about their tastes. This approach
    is also hard to generalize. Say we decided to expand our recommendation engine
    to recommend TV series as well. The number of Academy Awards may not be relevant,
    then, and we may need to replace this feature with the number of Golden Globe
    nominations instead. What if we expand it to music later? It makes sense to think
    of a different approach that is content-agnostic instead.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在推荐任务中，你有一组用户与一组物品互动，你的任务是弄清楚哪些物品适合哪些用户。你可能了解每个用户的一些信息：他们住在哪里、他们赚多少钱、他们是通过手机还是平板登录的，等等。类似地，对于一个物品——比如说一部电影——你知道它的类型、制作年份以及它获得了多少项奥斯卡奖。显然，这看起来像一个分类问题。你可以将用户特征与物品特征结合起来，为每个用户-物品对构建一个分类器，然后尝试预测用户是否会喜欢该物品。这种方法被称为**基于内容的过滤**。顾名思义，它的效果取决于从每个用户和每个物品中提取的内容或特征。在实践中，你可能只知道每个用户的一些基本信息。用户的位置信息或性别可能足以揭示他们的口味。这种方法也很难推广。例如，如果我们决定扩展推荐引擎，推荐电视连续剧。那时奥斯卡奖的数量可能就不相关了，我们可能需要将此特征替换为金球奖提名的数量。如果我们后来将其扩展到音乐呢？因此，考虑采用一种与内容无关的不同方法似乎更为合理。
- en: '**Collaborative filtering**, on the other hand, doesn''t care much about the
    user or the item features. Rather, it assumes that users who are already interested
    in some items will probably have the same interests in the future. To make a recommendation
    for you, it basically recruits other users who are similar to you and uses the
    decisions they make to suggest items to you in the future. One obvious problem
    here is the cold-start problem. When a new user joins, it is hard to know which
    users are similar to them right away. Also, for a new item, it will take a while
    for some users to discover it, and only then will the system be able to recommend
    it to other users.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**协同过滤**则不太关注用户或物品特征。相反，它假设那些已经对某些物品感兴趣的用户，将来可能对相同的物品也有兴趣。为了给您推荐物品，它基本上是通过招募与您相似的其他用户，并利用他们的决策来向您推荐物品。这里一个明显的问题是冷启动问题。当新用户加入时，很难立刻知道哪些用户与他们相似。此外，对于一个新物品，可能需要一段时间，直到某些用户发现它，系统才有可能将其推荐给其他用户。'
- en: Since each approach has its shortcomings, a hybrid approach of the two can be
    used. In its simplest form, we can just recommend to the new users the most popular
    items on the platform. Once these new users consume enough items for us to know
    their taste, we can start incorporating a more collaborative filtering approach
    to tailor their recommendations for them.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 由于每种方法都有其局限性，因此可以使用两者的混合方法。在最简单的形式中，我们可以向新用户推荐平台上最受欢迎的物品。一旦这些新用户消费了足够的物品，以便我们了解他们的兴趣，我们就可以开始结合更具协同过滤的方法，为他们量身定制推荐。
- en: In this chapter, we are going to focus on the *collaborative filtering* paradigm.
    It is the more common approach, and we already learned in previous chapters how
    to build the classifiers needed for the *content-based filtering* approach. We
    will be using a library called Surprise to demonstrate the differentcollaborative
    filtering algorithms. In the next section, we are going to install Surprise and
    download the data needed for the rest of the chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注*协同过滤*范式。它是更常见的方法，我们在前面的章节中已经学会了如何构建为*基于内容的过滤*方法所需的分类器。我们将使用一个名为
    Surprise 的库来演示不同的协同过滤算法。在接下来的部分，我们将安装 Surprise 并下载本章其余部分所需的数据。
- en: Downloading surprise and the dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载 surprise 库和数据集
- en: Nicolas Hug created Surprise [[http://surpriselib.com](http://surpriselib.com)],
    which implements a number of the*collaborative filtering algorithms we will use
    here. I am using version 1.1.0 of the library. To download the same version of
    the library via `pip`, you can run the following command in your terminal:*
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Nicolas Hug 创建了 Surprise [[http://surpriselib.com](http://surpriselib.com)]，它实现了我们在这里将使用的一些*协同过滤算法*。我正在使用该库的
    1.1.0 版本。要通过 `pip` 下载相同版本的库，您可以在终端中运行以下命令：
- en: '*[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE0]'
- en: Before using the library, we also need to download the dataset used in this
    chapter.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用该库之前，我们还需要下载本章使用的数据集。
- en: Downloading the KDD Cup 2012 dataset
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 下载 KDD Cup 2012 数据集
- en: 'We are going to use the same dataset that we used in [Chapter 10](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=32&action=edit)*,
    Imbalanced Learning – Not Even 1% Win the Lottery*. The data is published on the
    **OpenML** platform. It contains a list of records. In each record, a user has
    seen an online advertisement, and there is an additional column stating whether
    the user clicked on the advertisement. In the aforementioned chapter, we built
    a classifier to predict whether the user clicked on the advertisement. We used
    the provided features for the advertisements and the visiting users in our classifier.
    In this chapter, we are going to frame the problem as a collaborative filtering
    problem. So, we will only use the IDs of the users and the advertisements. All
    the other features will be ignored, and this time, the target label will be the
    user rating. Here, we will download the data and put it into a data frame:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与[第 10 章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=32&action=edit)*不平衡学习
    – 甚至不到 1%的人赢得彩票*中使用的相同数据集。数据发布在 **OpenML** 平台上。它包含了一系列记录。在每条记录中，一个用户看到了在线广告，并且有一列额外的信息说明该用户是否点击了广告。在前面提到的章节中，我们构建了一个分类器来预测用户是否会点击广告。我们在分类器中使用了广告和访问用户提供的特征。在本章中，我们将把这个问题框架化为协同过滤问题。因此，我们将只使用用户和广告的
    ID，其他所有特征将被忽略，这次的目标标签将是用户评分。在这里，我们将下载数据并将其放入数据框中：
- en: '[PRE1]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We converted all the columns into integers. The rating column takes binary
    values, where `1` indicates a click or a positive rating. We can see that only`16.8%`of
    the records lead to a positive rating. We can check this by printing the mean
    of the `user_rating`column, as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将所有的列转换为整数。评级列采用二进制值，`1`表示点击或正向评分。我们可以看到，只有`16.8%`的记录导致了正向评分。我们可以通过打印`user_rating`列的均值来验证这一点，如下所示：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also display the first four rows of the dataset. Here, you can see the
    IDs of the users and the advertisements, as well as the given ratings:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示数据集的前四行。在这里，您可以看到用户和广告的 ID 以及给出的评分：
- en: '![](img/6206c67f-cdb9-4636-ba2a-63dc3c8bc68e.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6206c67f-cdb9-4636-ba2a-63dc3c8bc68e.png)'
- en: The Surprise library expects the data columns to be in this exact order. So,
    no more data manipulations are required for now. In the next section, we are going
    to see how to load this data frame into the library and split it into training
    and test sets.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Surprise 库期望数据列的顺序完全按照这个格式。所以，目前不需要进行更多的数据处理。在接下来的章节中，我们将看到如何将这个数据框加载到库中，并将其分割成训练集和测试集。
- en: Processing and splitting the dataset
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理和拆分数据集
- en: 'In its simplest form, two users are similar, from acollaborative filtering
    point of view, if they give the same ratings to the same items. It is hard to
    see this in the current data format. It would be better to put the data into a
    user-item rating matrix. Each row in this matrix represents a user, each column
    represents an item, and the values in each cell represent the rating given by
    each user to the corresponding item. We can use the`pivot`method in `pandas` to
    create this matrix. Here, I have created the matrix for the first 10 records of
    our dataset:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 从协同过滤的角度来看，两个用户如果对相同的物品给出相同的评分，则认为他们是相似的。在当前的数据格式中很难看到这一点。将数据转换为用户-物品评分矩阵会更好。这个矩阵的每一行表示一个用户，每一列表示一个物品，每个单元格中的值表示该用户给对应物品的评分。我们可以使用
    `pandas` 的 `pivot` 方法来创建这个矩阵。在这里，我为数据集的前 10 条记录创建了矩阵：
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here is the resulting `10` users by `10` items matrix:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是得到的 `10` 个用户与 `10` 个物品的矩阵：
- en: '![](img/8d73ec36-11fd-47a7-9d7b-2d3c5acd48a5.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d73ec36-11fd-47a7-9d7b-2d3c5acd48a5.png)'
- en: 'Doing this ourselves using data frames is not the most efficient approach.
    The Surprise library stores the data in a more efficient way. So, we will use
    the library''s `Dataset` module instead. Before loading the data, we need to specify
    the scale of the ratings given. Here, we will use the`Reader`module to specify
    that our ratings take binary values. Then, we will load the data frame using the
    `load_from_df` method of the dataset. This method takes our data frame as well
    as an instance of the aforementioned reader:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据框自己实现这一点并不是最高效的做法。Surprise 库以更高效的方式存储数据。所以，我们将改用该库的 `Dataset` 模块。在加载数据之前，我们需要指定评分的尺度。在这里，我们将使用
    `Reader` 模块来指定我们的评分是二进制值。然后，我们将使用数据集的 `load_from_df` 方法加载数据框。该方法需要我们的数据框以及前述的读取器实例：
- en: '[PRE4]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The collaborative filtering algorithm is not considered a supervised learning
    algorithm due to the absence of concepts such as features and targets. Nevertheless,
    users give ratings to the item and we try to predict those ratings. This means
    that we can still evaluate our algorithm by comparing the actual ratings to the
    predicted ones. That''s why it is common to split the data into training and test
    sets and use metrics to evaluate our predictions. Surprise has a similar function
    to scikit-learn''s `train_test_split` function. We will use it here to split the
    data into 75% training versus 25% test sets:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 协同过滤算法不被认为是监督学习算法，因为缺乏特征和目标等概念。尽管如此，用户会对物品进行评分，我们尝试预测这些评分。这意味着我们仍然可以通过比较实际评分和预测评分来评估我们的算法。这就是为什么通常将数据分割为训练集和测试集，并使用评估指标来检验我们的预测。Surprise
    提供了一个类似于 scikit-learn 中 `train_test_split` 函数的功能。我们将在这里使用它，将数据分割为 75% 的训练集和 25%
    的测试集：
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In addition to the train-test split, we can also perform **K-Fold cross-validation**.
    We will use the**Mean Absolute Error** (**MAE**) and the **Root Mean Squared Error**
    (**RMSE**) to compare the predicted ratings to the actual ones. The following
    code uses 4-fold cross-validation and prints the average MAE and RMSE for the
    four folds. To make it easier to apply to different algorithms, I created a `predict_evaluate`function,
    which takes an instance of the algorithm we want to use. It also takes the entire
    dataset, and the name of the algorithm is used to print it alongside the results
    at the end. It then uses the`cross_validate`**module od `surprise` to calculate
    the expected errors and print their averages:**
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 除了训练-测试划分外，我们还可以进行**K折交叉验证**。我们将使用**平均绝对误差**（**MAE**）和**均方根误差**（**RMSE**）来比较预测评分和实际评分。以下代码使用4折交叉验证，并打印四个折叠的平均MAE和RMSE。为了方便不同算法的应用，我创建了一个`predict_evaluate`函数，它接受我们想使用的算法的实例。它还接受整个数据集，并且算法的名称会与结果一起打印出来。然后它使用`surprise`的`cross_validate`模块来计算期望误差并打印它们的平均值：
- en: '**[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE6]**'
- en: We will be using this function in the following sections. Before learning about
    the different algorithms, we need to create a reference algorithm—a line in the
    sand with which to compare the remaining algorithms. In the next section, we are
    going to create a recommendation system that gives random results. This will be
    our reference algorithm further down the road.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中使用这个函数。在了解不同算法之前，我们需要创建一个参考算法—一条用来与其他算法进行比较的标准。在下一节中，我们将创建一个给出随机结果的推荐系统。这个系统将是我们之后的参考算法。
- en: Creating a random recommender
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个随机推荐系统
- en: We know that 16.8% of the records lead to positive ratings. Thus, a recommender
    that randomly gives positive ratings to 16.8% of the cases seems like a good reference
    to compare the other algorithms. By the way, I am deliberately avoiding the term
    *baseline* here and using terms such as *reference* instead, since one of the
    algorithms used here is called *baseline*. Anyway, we can create our reference
    algorithm by creating a `RandomRating`classthat inherits from the Surprise library's`AlgoBase`class.
    All the algorithms in the library are driven from the `AlgoBase` base class and
    they are expected to implement an estimate method.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道16.8%的记录会导致正向评分。因此，一个随机给16.8%情况赋予正向评分的推荐系统，似乎是一个很好的参考，能够用来与其他算法进行比较。顺便说一句，我特意避免在这里使用*基准*这个术语，而是使用*参考*这样的词汇，因为这里使用的算法之一被称为*基准*。无论如何，我们可以通过创建一个继承自Surprise库中`AlgoBase`类的`RandomRating`类来创建我们的参考算法。库中的所有算法都继承自`AlgoBase`基础类，预计它们都需要实现一个估算方法。
- en: 'This method is called with each user-item pair and it is expected to return
    the predicted rating for this particular user-item pair. Since we are returning
    random ratings here, we will use NumPy''s `random` module. Here, we set `n=1`
    in the binomial method, which turns it into a Bernoulli distribution. The value
    given to `p` during the class initialization specifies the probability of returning
    ones. By default, 50% of the user-item pairs will get a rating of `1` and 50%
    of them will get a rating of `0`. We will override this default and set it to
    16.8% when using the class later on. Here is the code for the newly created method:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法会针对每一对用户-项目对进行调用，并期望返回该特定用户-项目对的预测评分。由于我们这里返回的是随机评分，我们将使用NumPy的`random`模块。在这里，我们将二项分布方法中的`n`设置为1，这使得它变成了伯努利分布。在类初始化时，赋值给`p`的值指定了返回1的概率。默认情况下，50%的用户-项目对会得到`1`的评分，而50%的用户-项目对会得到`0`的评分。我们将覆盖这个默认值，并在稍后的使用中将其设置为16.8%。以下是新创建方法的代码：
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We need to change the default value of `p` to `16.8%`. We can then pass the
    `RandomRating` instance to `predict_evaluate` to get the estimated errors:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将`p`的默认值更改为`16.8%`。然后，我们可以将`RandomRating`实例传递给`predict_evaluate`，以获得预测误差：
- en: '[PRE8]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The previous code gives us an average MAE of `0.28` and an average RMSE of `0.53`.
    Remember, we are using K-fold cross-validation. So, we calculate the average of
    the average errors returned for each fold. Keep these error numbers in mind as
    we expect more advanced algorithms to give lower errors. In the next section,
    we will meet the most basic family of the collaborative filtering algorithms,
    inspired by the**K-Nearest Neighbors** (**KNN**) algorithms.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码给出了`0.28`的平均MAE和`0.53`的平均RMSE。请记住，我们使用的是K折交叉验证。因此，我们计算每一折返回的平均误差的平均值。记住这些误差数字，因为我们预计更先进的算法会给出更低的误差。在接下来的章节中，我们将介绍最基础的协同过滤算法系列，其灵感来源于**K-近邻**（**KNN**）算法。
- en: Using KNN-inspired algorithms
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于KNN的算法
- en: We have encountered enough variants of the KNN**algorithm for it be our first
    choice for solving the recommendation problem. In the user-item rating matrix
    from the previous section, each row represents a user and each column represents
    an item. Thus, similar rows represent users who have similar tastes and identical
    columns represent items liked by the same users. Therefore, if we want to estimate
    the rating (*r[u,i]*),given by the user (*u*) to the item (*i*), we can get the
    KNNs to the user (*u*), find their ratings for the item (*i*), and calculate the
    average of their rating as an estimate for (*r[u,i]*). Nevertheless, since some
    of these neighbors are more similar to the user (*u*) than others, we may need
    to use a weighted average instead. Ratings given by more similar users should
    be given more weight than the others. Here is a formula where a similarity score
    is used to weigh the ratings given by the user's neighbors:**
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经遇到过足够多的KNN**算法变种，因此它是我们解决推荐问题时的首选算法。在上一节中的用户-项评分矩阵中，每一行代表一个用户，每一列代表一个项。因此，相似的行代表口味相似的用户，相同的列代表喜欢相同项的用户。因此，如果我们想估算用户（*u*）对项（*i*）的评分（*r[u,i]*），我们可以获取与用户（*u*）最相似的KNN，找到他们对项（*i*）的评分，然后计算他们评分的平均值，作为对（*r[u,i]*）的估算。然而，由于某些邻居比其他邻居与用户（*u*）更相似，我们可能需要使用加权平均值。与用户（*u*）更相似的邻居给出的评分应该比其他邻居的评分权重大。以下是一个公式，其中相似度得分用于加权用户邻居给出的评分：**
- en: '**![](img/f8c3f918-d8f1-4121-9414-2c2e147ba36e.png)'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/f8c3f918-d8f1-4121-9414-2c2e147ba36e.png)**'
- en: We refer to the neighbors of *u* with the term *v*. Therefore, *r[v,i]*is the
    rating given by each of them to the item (*i*). Conversely, we can base our estimation
    on *item similarities* rather than *user similarities.* Then, the expected rating
    (*r[u,i]*) would be the weighted average of the ratings given by the user (*u*)
    to their most similar items (*i*).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用术语*v*来表示*u*的邻居。因此，*r[v,i]*是每个邻居给项（*i*）的评分。相反，我们也可以根据*项相似度*而不是*用户相似度*来进行估算。然后，预期的评分（*r[u,i]*）将是用户（*u*）对其最相似项（*i*）评分的加权平均值。
- en: 'You may be wondering whether we can nowset the number of neighbors and whether
    there are multiple similarity metrics to choose from. The answer to both questions
    is yes. We will dig deeper into the algorithm''s hyperparameters in a bit, but
    for now, let''s use it with its default values. Once `KNNBasic` is initialized,
    we can pass it to the `predict_evaluate` function, the same way we passed the
    `RandomRating` estimator to it in the previous section. Make sure you have enough
    memory on your computer before running the following code:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想，我们现在是否可以设置邻居的数量，是否有多个相似度度量可以选择。两个问题的答案都是肯定的。我们稍后会深入探讨算法的超参数，但现在先使用它的默认值。一旦`KNNBasic`初始化完成，我们可以像在上一节中将`RandomRating`估算器传递给`predict_evaluate`函数那样，将其传递给`predict_evaluate`函数。运行以下代码之前，请确保计算机有足够的内存。
- en: '[PRE9]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We get an average MAE of `0.28` and an average RMSE of `0.38` this time. The
    improvement in the squared error is expected, given that the `RandomRating` estimator
    was blindly making random predictions, while`KNNBasic`bases its decision on users'
    similarities.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次我们得到了`0.28`的平均MAE和`0.38`的平均RMSE。考虑到`RandomRating`估算器是在盲目地做随机预测，而`KNNBasic`是基于用户相似性做决策的，平方误差的改善是预期之中的。
- en: The ratings in the dataset used here are binary values. In some other scenarios,
    users may be allowed to give 5-star ratings, or even give scores from 0 to 100\.
    In those scenarios, one user may be more generous with their numbers than another.
    We both may have the same taste, but for me, a 5-star rating signals the movie
    is great, while you never give a 5-star rating yourself, and your favorite movies
    get 4-star rating tops. The `KNNWithMeans`algorithm deals with this problem. It
    is an almost identical algorithm to`KNNBasic`, except for the fact that it initially
    normalizes the ratings given by each user to make them comparable.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用的数据集中的评分是二进制值。在其他一些场景中，用户可能允许给出5星评分，甚至给出从0到100的分数。在这些场景中，一个用户可能比另一个用户更慷慨地给出分数。我们两个人可能有相同的口味，但对我来说，5星评分意味着电影非常好，而你自己从不给5星评分，你最喜欢的电影顶多获得4星评分。`KNNWithMeans`算法解决了这个问题。它是与`KNNBasic`几乎相同的算法，不同之处在于它最初会对每个用户给出的评分进行归一化，使得评分可以进行比较。
- en: 'As stated earlier, we can choose the number for `K`, as well as the similarity
    score used. Additionally, we can decide whether we want to base our estimation
    on user similarities or on item similarities. Here, we set the number of neighbors
    to `20`, use cosine similarity, and base our estimation on item similarities:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以选择`K`的数值以及使用的相似度得分。此外，我们还可以决定是否基于用户相似性或物品相似性来进行估计。在这里，我们将邻居数量设置为`20`，使用余弦相似度，并基于物品相似性来进行估计：
- en: '[PRE10]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting errors are worse than before. We get an average MAE of `0.29`
    and an average RMSE of `0.39`. Clearly, we need to try different hyperparameters
    until we get the best results. Luckily, Surprise provides a `GridSearchCV` helper
    for tuning the algorithm''s hyperparameters. We basically provide a list of the
    hyperparameter values and specify the measures we need to use to evaluate the
    algorithms. In the following code snippet, we set the measures to `rmse` and `mae`.
    We use 4-fold cross-validation and use all the available processors in our machines
    when running the grid search. You probably know by now that KNN algorithms are
    slow with their prediction time. So, to speed up this process, I only ran the
    search on a subset of our dataset, as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果错误比之前更严重。我们得到的平均MAE为`0.29`，平均RMSE为`0.39`。显然，我们需要尝试不同的超参数，直到得到最佳结果。幸运的是，Surprise提供了一个`GridSearchCV`助手来调节算法的超参数。我们基本上提供一个超参数值的列表，并指定我们需要用来评估算法的衡量标准。在下面的代码片段中，我们将衡量标准设置为`rmse`和`mae`。我们使用4折交叉验证，并在运行网格搜索时使用机器上的所有可用处理器。你现在可能已经知道，KNN算法的预测时间较慢。因此，为了加速这一过程，我只在我们的数据集的一个子集上运行了搜索，如下所示：
- en: '[PRE11]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We get an average MAE of `0.28` and an average RMSE of `0.38`. These are the
    same results as with the default hyperparameters. However,`GridSearchCV`chose
    a `K` value of `20` versus the default of `40`. It also chose the **Pearson correlation
    coefficien**t as its similarity measure.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的平均MAE为`0.28`，平均RMSE为`0.38`。这些结果与使用默认超参数时相同。然而，`GridSearchCV`选择了`K`值为`20`，而默认值为`40`。它还选择了**皮尔逊相关系数**作为相似度衡量标准。
- en: The KNN algorithm is slow and did not give the best performance for our dataset.
    Therefore, in the next section, we are going to try a non-instance-based learner
    instead.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: KNN算法较慢，并没有为我们的数据集提供最佳的性能。因此，在下一部分中，我们将尝试使用非实例基础的学习器。
- en: Using baseline algorithms
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基准算法
- en: 'The simplicity of the nearest neighbors algorithm is a double-edged sword.
    On the one hand, it is easier to grasp, but on the other hand, it lacks an objective
    function that we can optimize during training. This also means that the majority
    of its computation is performed during prediction time. To overcome these problems,
    Yehuda Koren formulated the recommendation problem in terms of an optimization
    task. Still, for each user-item pair, we need to estimate a rating (*r[u,i]*).
    The expected rating this time is the summation of the following triplet:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻算法的简单性是一把双刃剑。一方面，它更容易掌握，但另一方面，它缺乏一个可以在训练过程中优化的目标函数。这也意味着它的大部分计算是在预测时进行的。为了解决这些问题，Yehuda
    Koren将推荐问题表述为一个优化任务。然而，对于每个用户-物品对，我们仍然需要估计一个评分（*r[u,i]*）。这次期望的评分是以下三元组的总和：
- en: '![](img/a8e42ccd-0aab-45e7-9757-acf0bea746cb.png): The overall average rating
    given by all users to all items'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/a8e42ccd-0aab-45e7-9757-acf0bea746cb.png)：所有用户对所有物品的总体平均评分'
- en: '*b[u]*: A term for how the user (*u*) deviates from the overall average rating'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b[u]*：表示用户(*u*)与总体平均评分的偏差'
- en: '*b[i]*: A term for how the item (*i*) deviates from the average rating'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b[i]*：表示项目(*i*)偏离平均评分的术语'
- en: 'Here is the formula for the expected ratings:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这是期望评分的公式：
- en: '![](img/3804eba7-13fb-4efc-9233-ebc145f874b5.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3804eba7-13fb-4efc-9233-ebc145f874b5.png)'
- en: For each user-item pair in our training set, we know its actual rating (*r[u,i]*),
    and all we need to do now is to figure out the optimal values of *b[u]* and *b[i]*.
    We are after values that minimize the difference between the actual rating (*r[u,i]*)
    and the *expected rating* (*r[u,i]*) from the aforementioned formula. In other
    words, we need a solver to learn the values of the terms when given the training
    data. In practice, the baseline algorithm tries to minimize the average squared
    difference between the actual and the expected ratings. It also adds a regularization
    term that penalizes (*b[u]*) and (*b[i]*) to avoid overfitting. Please refer to
    [Chapter 3](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=26&action=edit)*,
    Making Decisions with Linear Equations*, for a better understanding of the concept
    of regularization.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练集中的每一对用户-项目，我们知道其实际评分(*r[u,i]*)，现在我们要做的就是找出最优的*b[u]*和*b[i]*值。我们的目标是最小化实际评分(*r[u,i]*)与上述公式中*期望评分*(*r[u,i]*)之间的差异。换句话说，我们需要一个求解器在给定训练数据时学习这些项的值。实际上，基准算法尝试最小化实际评分与期望评分之间的平均平方差。它还添加了一个正则化项，用来惩罚(*b[u]*)和(*b[i]*)，以避免过拟合。更多关于正则化概念的理解，请参见[第3章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=26&action=edit)*，用线性方程做决策*。
- en: The learned coefficients (*b[u]* and *b[i]*)are vectors describing each user
    and each item. At prediction time, if a new user is encountered, *b[u]* is set
    to `0`. Similarly, if a new item that wasn't seen in the training set is encountered,
    *b[i]* is set to `0`.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 学到的系数(*b[u]*和*b[i]*)是描述每个用户和每个项目的向量。在预测时，如果遇到新用户，*b[u]*将设置为`0`。类似地，如果遇到在训练集中未出现过的新项目，*b[i]*将设置为`0`。
- en: 'Two solvers are available for solving this optimization problem: **Stochastic
    Gradient Descent** (**SGD**) and **Alternating Least Squares** (**ALS**). ALS
    is used by default. Each of the two solvers has its own settings, such as the
    maximum number of epochs and the learning rate. Moreover, you can also tune the
    regularization parameters.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个求解器可用于解决此优化问题：**随机梯度下降** (**SGD**) 和 **交替最小二乘法** (**ALS**)。默认使用ALS。每个求解器都有自己的设置，如最大迭代次数和学习率。此外，您还可以调整正则化参数。
- en: 'Here is how the model is used with its default hyperparameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用其默认超参数的模型：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This time, we get an average MAE of `0.27` and an average RMSE of `0.37`. Once
    more, `GridSearchCV` can be used to tune the model''s hyperparameters. I will
    leave the parameter tuning for you to try. Now, it is time to move on to our third
    algorithm: **Singular Value Decomposition** (**SVD**).'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们得到了平均MAE为`0.27`，平均RMSE为`0.37`。同样，`GridSearchCV`可以用来调整模型的超参数。参数调优部分留给你去尝试。现在，我们进入第三个算法：**奇异值分解**
    (**SVD**)。
- en: Using singular value decomposition
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用奇异值分解
- en: 'The user-item rating matrix is usually a huge matrix. The one we got from our
    dataset here comprises 30,114 rows and 19,228 columns, and most of the values
    in this matrix (99.999%) are zeros. This is expected. Say you own a streaming
    service with thousands of movies in your library. It is very unlikely that a user
    will watch more than a few dozen of them. This sparsity creates another problem.
    If one user watched the movie *The Hangover: Part 1* and another user watched
    *The Hangover: Part 2,* from the matrix''s point of view, they watched two different
    movies. We already know that collaborative filtering algorithms don''t use users
    or item features. Thus, it is not aware of the fact that the two parts of *The
    Hangover* movie belong to the same franchise, let alone knowing that they both
    are comedies. To deal with this shortcoming, we need to transform our user-item
    rating matrix. We want the new matrix, or matrices, to be smaller and to capture
    the similarities between the users and the items better.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 用户-项目评分矩阵通常是一个巨大的矩阵。我们从数据集中得到的矩阵包含30,114行和19,228列，其中大多数值（99.999%）都是零。这是预期中的情况。假设你拥有一个包含数千部电影的流媒体服务库。用户观看的电影数量很少，因此矩阵中的零值非常多。这种稀疏性带来了另一个问题。如果一个用户观看了电影*宿醉：第一部*，而另一个用户观看了*宿醉：第二部*，从矩阵的角度来看，他们看的是两部不同的电影。我们已经知道，协同过滤算法不会使用用户或项目的特征。因此，它无法意识到*宿醉*的两部作品属于同一系列，更不用说它们都是喜剧片了。为了解决这个问题，我们需要转换我们的用户-项目评分矩阵。我们希望新的矩阵，或者多个矩阵，能够更小并更好地捕捉用户和项目之间的相似性。
- en: The **SVD** is a matrix factorization algorithm that is used for dimensionality
    reduction. It is very similar to **Principal Component Analysis**(**PCA**), which
    we looked at in [Chapter 5](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit)*,
    Image Processing with Nearest Neighbors*. The resulting singular values, as opposed
    to the principal components in PCA, capture latent information about the users
    and the item in the user-item rating matrix. Don't worry if the previous sentence
    is not clear yet. In the next section, we will understand the algorithm better
    via an example.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVD**是一种用于降维的矩阵分解算法，它与我们在[第5章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit)*《使用最近邻的图像处理》*中讨论的**主成分分析**（**PCA**）非常相似。与PCA中的主成分不同，得到的奇异值捕捉了用户-项目评分矩阵中用户和项目的潜在信息。如果之前的句子不太清楚也不用担心。在接下来的章节中，我们将通过一个例子更好地理解这个算法。'
- en: Extracting latent information via SVD
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过SVD提取潜在信息
- en: 'Nothing spells taste like music. Let''s take the following dataset. Here, we
    have six users, each voting for the musicians they like:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么能比音乐更能体现品味了。让我们来看一下下面这个数据集。在这里，我们有六个用户，每个用户都投票选出了自己喜欢的音乐人：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can put these ratings into a data frame and convert it into a user-item
    rating matrix, using the data frame''s `pivot` method, as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些评分放入数据框，并使用数据框的`pivot`方法将其转换为用户-项目评分矩阵，具体方法如下：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Here is the resulting matrix. I used `pandas` styling to give the different
    ratings different colors for clarity:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这是得到的矩阵。我使用了`pandas`样式，将不同的评分用不同的颜色表示，以便清晰区分：
- en: '![](img/69a9ab17-90b1-41d6-b515-269105890a30.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69a9ab17-90b1-41d6-b515-269105890a30.png)'
- en: 'Clearly, users 1, 2, and 5 like metal music, while users 3, 4, and 6 like trance
    music. We can see this despite the fact that user 5 only shares one band with
    users 1 and 2\. We could perhaps also see this because we are aware of these musicians
    and because we have a holistic view of the matrix instead of focusing on individual
    pairs. We can use scikit-learn''s `TruncatedSVD` function to reduce the dimensionality
    of the matrix and represent each user and musician via *N* components (single
    vectors). The following snippet calculates `TruncatedSVD` with two *single vectors*.
    Then, the `transform` function returns a new matrix, where each row represents
    one of the six users, and each of its two columns corresponds to one of the two
    single vectors:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，用户1、2和5喜欢金属音乐，而用户3、4和6喜欢迷幻音乐。尽管用户5只与用户1和2共享一个乐队，但我们仍然可以看出这一点。也许我们之所以能够看到这一点，是因为我们了解这些音乐人，并且我们从整体的角度来看待矩阵，而不是专注于单独的用户对。我们可以使用scikit-learn的`TruncatedSVD`函数来降低矩阵的维度，并通过*N*个组件（单一向量）来表示每个用户和音乐人。以下代码片段计算了带有两个*单一向量*的`TruncatedSVD`。然后，`transform`函数返回一个新的矩阵，其中每一行代表六个用户中的一个，每一列对应两个单一向量之一：
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Once more, I put the resulting matrix into a data frame and used its styling
    to color the cells according to their values. Here is the code for that:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我将结果矩阵放入数据框，并使用其样式根据值给单元格上色。以下是实现这一点的代码：
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This is the resulting data frame:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是结果数据框：
- en: '![](img/c51d9ed9-ea3a-4f77-8d37-b8ac40ac745c.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c51d9ed9-ea3a-4f77-8d37-b8ac40ac745c.png)'
- en: You can treat each of the two components as a music genre. It is clear that
    the smaller matrix was able to capture the user's taste in terms of genres. Users
    1, 2, and 5 are brought closer to each other now, as are users 3, 4, and 6, who
    are closer to each other than they were in the original matrix. We will use the
    cosine similarity score to show this more clearly in the next section.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这两个组件视为一种音乐风格。很明显，较小的矩阵能够捕捉到用户在风格上的偏好。用户1、2和5现在更加接近彼此，用户3、4和6也是如此，他们彼此之间比原始矩阵中的更为接近。我们将在下一节中使用余弦相似度得分来更清楚地展示这一点。
- en: The concept used here is also used with textual data. Words such as `search`,
    `find`, and `forage` carry similar meanings. Thus, the `TruncatedSVD` transformer
    can be used to compress a***Vector Space Model** (**VSM**) into a lower space
    before using it in a supervised or an unsupervised learning algorithm. When used
    in that context, it is known as**Latent Semantic Analysis** (**LSA**).*
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用的概念也适用于文本数据。诸如`search`、`find`和`forage`等词汇具有相似的意义。因此，`TruncatedSVD`变换器可以用来将**向量空间模型**（**VSM**）压缩到一个较低的空间，然后再将其用于有监督或无监督的学习算法。在这种背景下，它被称为**潜在语义分析**（**LSA**）。
- en: '*This compression not only captures the latent information that is not clear
    in the bigger matrix, but also helps with distance calculations. We already know
    that algorithms such as KNN work best with lower dimensions. Don''t take my word
    for it. In the next section, we will compare the cosine distances when calculated
    based on the original user-item rating matrix versus the two-dimensional one.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*这种压缩不仅捕捉到了较大矩阵中不明显的潜在信息，还帮助了距离计算。我们已经知道，像KNN这样的算法在低维度下效果最好。不要仅仅相信我的话，在下一节中，我们将比较基于原始用户-物品评分矩阵与二维矩阵计算的余弦距离。'
- en: Comparing the similarity measures for the two matrices
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较两个矩阵的相似度度量
- en: 'We can calculate the cosine similarities between all users. We will start with
    the original user-item rating matrix. After calculating pairwise cosine similarities
    for users 1, 2, 3, and 5, we put the results into a data frame and apply some
    styling for clarity:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以计算所有用户之间的余弦相似度。我们将从原始的用户-物品评分矩阵开始。在计算用户1、2、3和5的配对余弦相似度后，我们将结果放入数据框并应用一些样式以便于查看：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Here are the resulting pairwise similarities between the four users:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是四个用户之间的配对相似度结果：
- en: '![](img/1524d18a-9d9d-4df8-9de5-2bc635683bbc.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1524d18a-9d9d-4df8-9de5-2bc635683bbc.png)'
- en: 'Indeed, user 5 is more similar to users 1 and 2, compared to user 3\. However,
    they are not as similar as we expected them to be. Let''s now calculate the same
    similarities by using `TruncatedSVD` this time:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，用户5比用户3更像用户1和用户2。然而，他们之间的相似度并没有我们预期的那么高。现在我们来通过使用`TruncatedSVD`来计算相同的相似度：
- en: '[PRE18]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The new calculations capture the latent similarities between the musicians
    this time and incorporate this when comparing the users. Here is the new similarity
    matrix:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 新的计算方法这次捕捉了音乐家之间的潜在相似性，并在比较用户时考虑了这一点。以下是新的相似度矩阵：
- en: '![](img/931b2f30-502d-49d9-bbff-9142f321f25f.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/931b2f30-502d-49d9-bbff-9142f321f25f.png)'
- en: Clearly, user 5 is more similar to users 1 and 2 than before. Ignore the negative
    signs before some of the zeros here. This is because of Python's implementation
    of the **IEEE** (**Institute of Electrical and Electronics Engineers**) standard
    for floating-point arithmetic.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，用户5比以前更像用户1和用户2。忽略这里某些零前的负号，这是因为Python实现了**IEEE**（**电气和电子工程师协会**）标准的浮点运算。
- en: Naturally, we can also represent the musicians in terms of their genres (single
    vectors). This other matrix can be retrieved via `svd.components_`. Then, we can
    calculate the similarities between the different musicians. This transformation
    is also advised as a preliminary step before clusters sparse data.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 自然，我们也可以根据音乐家的风格（单一向量）来表示他们。这种矩阵可以通过`svd.components_`检索。然后，我们可以计算不同音乐家之间的相似度。这种转换也建议作为稀疏数据聚类的初步步骤。
- en: Now that this version of `SVD` is clear, in practice, when dealing with large
    datasets, more scalable factorization algorithms are usually used. **Probabilistic
    Matrix Factorization** (**P****MF***)*scales linearly with the number of observations
    and performs well on sparse and imbalanced datasets. We are going to use Surprise's
    implementation of PMF in the next section.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这个版本的`SVD`已经清晰了，在实践中，当处理大数据集时，通常会使用更具可扩展性的矩阵分解算法。**概率矩阵分解** (**P****MF***)*与观测数量成线性比例，并且在稀疏和不平衡数据集上表现良好。在下一节中，我们将使用Surprise的PMF实现。
- en: Click prediction using SVD
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SVD进行点击预测
- en: 'We can now use Surprise''s `SVD` algorithm to predict the clicks in our dataset.
    Let''s start with the algorithm''s default parameters, and then explain it later
    on:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用Surprise的`SVD`算法来预测我们数据集中的点击。让我们从算法的默认参数开始，然后稍后再解释：
- en: '[PRE19]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This time, we get an average MAE of `0.27` and an average RMSE of `0.37`. These
    are similar results to the baseline algorithm used earlier. In fact, Surprise''s
    implementation of `SVD` is a combination of the baseline algorithm and `SVD`.
    It expresses the user-item ratings using the following formula:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们得到的平均MAE为`0.27`，平均RMSE为`0.37`。这些结果与之前使用的基准算法类似。事实上，Surprise的`SVD`实现是基准算法和`SVD`的结合。它使用以下公式表示用户-项目评分：
- en: '![](img/f29915ef-89b9-4866-be27-9a1b9d6880ae.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f29915ef-89b9-4866-be27-9a1b9d6880ae.png)'
- en: The first three terms of the equation (![](img/a8e42ccd-0aab-45e7-9757-acf0bea746cb.png),
    *b[u]*, and *b[i]*) are the same as in the baseline algorithm. The fourth term
    represents the product of two similar matrices to the ones we got from `TruncatedSVD`.
    The *q[i]*matrixexpresses each item as a number of single vectors. Similarly,
    the*p[u]*matrixexpresses each user as a number of single vectors. The item matrix
    is transposed, hence the letter *T* on top of it. The algorithm then uses**SGD**
    to minimize the squared difference between the expected ratings and the actual
    ones. Similar to the baseline model, it also regularizes the coefficients of the
    expected rating (*b[u], b[i], q[i],* and *p[u]*) to avoid overfitting.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的前三项（(![](img/a8e42ccd-0aab-45e7-9757-acf0bea746cb.png)，*b[u]* 和 *b[i]*）与基准算法相同。第四项表示两个相似矩阵的乘积，这些矩阵与我们从`TruncatedSVD`得到的矩阵类似。*q[i]*矩阵将每个项目表示为多个单一向量。类似地，*p[u]*矩阵将每个用户表示为多个单一向量。项目矩阵被转置，因此上面有一个*T*字母。然后，算法使用**SGD**来最小化预期评分与实际评分之间的平方差。与基准模型类似，它还对预期评分的系数（*b[u],
    b[i], q[i],* 和 *p[u]*）进行正则化，以避免过拟合。
- en: We can ignore the baseline part of the equation—that is, remove the first three
    coefficients of it (![](img/a8e42ccd-0aab-45e7-9757-acf0bea746cb.png), *b[u]*,
    and *b[i]*) by setting `biased=False`. The number of single vectors to use is
    set using the `n_factors`hyperparameter. We can also control the number of epochs
    for `SGD` via `n_epochs`. Furthermore, there are additional hyperparameters for
    setting the algorithm's learning rate, regularization, and the initial values
    of its coefficients. You can find the best mix for these parameters using the
    parameter-tuning helpers provided by`surprise`—that is, `GridSearchCV`or `RandomizedSearchCV`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以忽略方程的基准部分——即通过设置`biased=False`来移除前面三个系数（(![](img/a8e42ccd-0aab-45e7-9757-acf0bea746cb.png)，*b[u]*
    和 *b[i]*）。使用的单一向量数量由`n_factors`超参数设置。我们还可以通过`n_epochs`控制`SGD`的迭代次数。此外，还有其他超参数用于设置算法的学习率、正则化以及系数的初始值。你可以使用`surprise`提供的参数调优助手来找到这些参数的最佳组合——即`GridSearchCV`或`RandomizedSearchCV`。
- en: Our discussion of the recommender systems, along with their various algorithms,
    marks an end to the machine learning topics discussed in this book. Like all the
    other algorithms discussed here, they are only useful when putting in production
    for others to use them. In the next section, we are going to see how we can deploy
    a trained algorithm and make it accessible to others.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对推荐系统及其各种算法的讨论标志着本书中机器学习话题的结束。与这里讨论的其他所有算法一样，只有在将其投入生产环境并供他人使用时，它们才有意义。在下一节中，我们将看到如何部署一个训练好的算法并让其他人使用它。
- en: Deploying machine learning models in production
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在生产环境中部署机器学习模型
- en: 'There are two main modes of using machine learning models:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用机器学习模型有两种主要模式：
- en: '**Batch predictions**: In this mode, you load a bunch of data records after
    a certain period—for example, every night or every month. You then make predictions
    for this data. Usually, latency is not an issue here, and you can afford to put
    your training and prediction code into single batch jobs. One exception to this
    is if you need to run your job too frequently that you do not have enough time
    to retrain the model every time the job runs. Then, it makes sense to train the
    model once, store it somewhere, and load it each time new batch predictions are
    to be made.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量预测**：在这种模式下，你在一段时间后加载一批数据记录——例如，每晚或每月一次。然后，你对这些数据进行预测。通常，在这里延迟不是问题，你可以将训练和预测代码放入单个批处理作业中。一个例外情况是，如果你需要过于频繁地运行作业，以至于每次作业运行时都没有足够的时间重新训练模型。那么，训练一次模型，将其存储在某处，并在每次进行新的批量预测时加载它是有意义的。'
- en: '**Online** **predictions**: In this model, your model is usually deployed behind
    an**Application Programming Interface** (**API**). Your API is usually called
    with a single data record each time, and it is supposed to make predictions for
    this single record and return it. Having low latency is paramount here and it
    is typically advised to train the model once, store it somewhere, and use the
    pre-trained model whenever a new API call is made.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线** **预测**：在这个模型中，你的模型通常被部署在**应用程序编程接口**（**API**）后面。每次调用API时，通常会传入一条数据记录，API需要为这条记录做出预测并返回结果。这里低延迟是至关重要的，通常建议训练模型一次，将其存储在某处，并在每次新的API调用时使用预训练模型。'
- en: 'As you can see, in both cases, we may need to separate the code used during
    the model''s training from the one used at prediction time. Whether it is a supervised
    learning algorithm or an unsupervised learning one, besides the lines of code
    it is written in, a fitted model also depends on the coefficients and parameters
    learned from the data. Thus, we need a way to store the code and the learned parameters
    as one unit. This single unit can be saved after training and then used later
    on at prediction time. To be able to store functions or objects in files or share
    them over the internet, we need to convert them into a standard format or protocol.
    This process is known as serialization. `pickle`is one of the most commonly used
    serialization protocols in Python. The Python standard library provides tools
    for pickling objects; however,`joblib`is a more efficient option when dealing
    with NumPy arrays. To be able to use the library, you need to install it via`pip`by
    running the following in your terminal:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这两种情况下，我们可能需要将模型训练过程中使用的代码与预测时使用的代码分开。无论是监督学习算法还是无监督学习算法，除了编写代码的行数外，拟合的模型还依赖于从数据中学习到的系数和参数。因此，我们需要一种方法来将代码和学习到的参数作为一个单元存储。这个单元可以在训练后保存，并在预测时使用。为了能够将函数或对象存储在文件中或通过互联网共享，我们需要将它们转换为标准格式或协议。这个过程被称为序列化。`pickle`是Python中最常用的序列化协议之一。Python标准库提供了序列化对象的工具；然而，`joblib`在处理NumPy数组时是一个更高效的选择。为了能够使用这个库，你需要通过`pip`在终端中运行以下命令来安装它：
- en: '[PRE20]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Once installed, you can use `joblib` to save anything onto a file on disk.
    For example, after fitting a baseline algorithm, we can store the fitted object
    using the `joblib` function''s `dump` method. The method expects, along with the
    model''s object, the name of the file to save the object in. We usually use a
    `.pkl` extension to refer to `pickle` files:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 安装完成后，你可以使用`joblib`将任何东西保存到磁盘文件中。例如，在拟合基线算法后，我们可以使用`joblib`函数的`dump`方法来存储拟合的对象。该方法除了模型的对象外，还需要提供一个保存对象的文件名。我们通常使用`.pkl`扩展名来表示`pickle`文件：
- en: '[PRE21]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Once saved to a disk, any other Python code can load the same model again and
    use it right away without the need for refitting. Here, we load the pickled algorithm
    and use it to make predictions for the test set:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦保存到磁盘，任何其他Python代码都可以再次加载相同的模型，并立即使用它，而无需重新拟合。在这里，我们加载已序列化的算法，并使用它对测试集进行预测：
- en: '[PRE22]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: A `surprise` estimator was used here since this is the library we used throughout
    this chapter. Nevertheless, any Python object can be pickled and loaded in the
    same way. Any of the estimators used in the previous chapters can be used the
    same way. Furthermore, you can also write your own classes, instantiate them,
    and pickle the resulting objects.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这里使用了一个`surprise`估算器，因为这是我们在本章中一直使用的库。然而，任何Python对象都可以以相同的方式被序列化并加载。前几章中使用的任何估算器也可以以相同的方式使用。此外，你还可以编写自己的类，实例化它们，并序列化生成的对象。
- en: To deploy your model as an API, you may need to use a web framework, such as
    **Flask** or **CherryPy**. Developing web applications is beyond the scope of
    this book, but once you know how to build them, loading pickled models should
    be straightforward. It's advised to load the pickled object when the web application
    is starting. This way, you do not introduce any additional latency if you reload
    the objects each time you receive a new request.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 若要将你的模型部署为API，你可能需要使用如**Flask**或**CherryPy**之类的Web框架。开发Web应用超出了本书的范围，但一旦你学会如何构建它们，加载已保存的模型应该会很简单。建议在Web应用启动时加载已保存的对象。这样，如果每次收到新请求时都重新加载对象，你就不会引入额外的延迟。
- en: Summary
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter marks the end of this book. I hope all the concepts discussed here
    are clear by now. I also hope the mixture of the theoretical background of each
    algorithm and its practical use paved the way for you to adapt the solutions offered
    here for the different problems you meet in practice in real life. Obviously,
    no book can be conclusive, and new algorithms and tools will be available to you
    in the future. Nevertheless, Pedro Domingos groups the machine learning algorithms
    into five tribes. Except for the evolutionary algorithms, we have met algorithms
    that belong to four out of Domingos' five tribes. Thus, I hope the various algorithms
    discussed here, each with their own approach, will serve as a good foundation
    when dealing with any new machine learning solutions in the future.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 本章标志着本书的结束。我希望到现在为止，这里讨论的所有概念都已经清晰明了。我也希望每个算法的理论背景与实际应用的结合，能够为你提供解决实际生活中不同问题的途径。显然，没有任何一本书能够得出最终结论，未来你会接触到新的算法和工具。不过，Pedro
    Domingos将机器学习算法分为五个类别。除了进化算法外，我们已经学习了属于Domingos五个类别中的四个类别的算法。因此，我希望这里讨论的各种算法，每种都有其独特的方法，在未来处理任何新的机器学习问题时，能够为你提供坚实的基础。
- en: All books are a work in progress. Their value is not only in their content but
    goes beyond that to include the value that comes from the future discussions they
    spark. Be assured that you will make the author of any book happy each time you
    share something you built based on the knowledge you gained from that book. You
    will make them equally happy each time you quote them, share new and better ways
    to explain things in their books, or even correct mistakes they made. I, too,
    am looking forward to such invaluable contributions from you.******
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的书籍都是一个不断完善的过程。它们的价值不仅体现在内容上，还包括它们激发的未来讨论所带来的价值。每次你分享基于书籍中获得的知识所构建的内容时，作者都会感到高兴。每次你引用他们的观点，分享更好、更清晰的解释方式，甚至纠正他们的错误时，他们也会同样高兴。我也期待着你为此做出的宝贵贡献。******
