- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: AutoML with LightGBM and FLAML
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LightGBM 和 FLAML 进行 AutoML
- en: In the previous chapter, we discussed two case studies that showed end-to-end
    examples of how to approach data science problems. Of the steps involved in the
    typical data science life cycle, often, the most time-consuming tasks are preparing
    the data, finding the correct models, and tuning the models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了两个案例研究，展示了如何处理数据科学问题的端到端示例。在典型的数据科学生命周期中，通常最耗时的工作是准备数据、寻找正确的模型以及调整模型。
- en: This chapter looks at the concept of automated machine learning. Automated machine
    learning systems seek to automate some or all parts of the machine learning life
    cycle. We will look at **FLAML**, a library that automates the process’s model
    selection and tuning steps using efficient hyperparameter optimization algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了自动机器学习的概念。自动机器学习系统旨在自动化机器学习生命周期的某些或全部部分。我们将探讨 **FLAML**，这是一个库，使用高效的超参数优化算法来自动化模型选择和调优步骤。
- en: Lastly, we will present a case study using FLAML and another open source tool
    called Featuretools. Practical usage of FLAML will be discussed and shown. We
    will also show FLAML’s zero-shot AutoML functionality, which bypasses tuning altogether.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将通过一个案例研究来展示如何使用 FLAML 和另一个名为 Featuretools 的开源工具。我们将讨论和展示 FLAML 的实际应用，并展示
    FLAML 的零样本 AutoML 功能，该功能完全绕过了调优过程。
- en: 'The main topics of this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容包括：
- en: An introduction to automatic machine learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动机器学习简介
- en: FLAML for AutoML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FLAML 用于 AutoML
- en: Case study – using FLAML with LightGBM
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究 – 使用 FLAML 和 LightGBM
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The chapter includes examples and code excerpts showcasing using FLAML with
    LightGBM for AutoML use cases. Complete examples and instructions for setting
    up a suitable environment for this chapter are available at [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-7](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-7).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含了一些示例和代码片段，展示了如何使用 FLAML 和 LightGBM 进行 AutoML 应用。关于设置本章所需环境的完整示例和说明可在[https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-7](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-7)找到。
- en: Automated machine learning
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动机器学习
- en: '**Automated machine learning** (**AutoML**) is a burgeoning field that aims
    to automate complex aspects of ML workflows, allowing for more efficient and accessible
    deployment of ML models. The advent of AutoML reflects the increasing sophistication
    of artificial intelligence and ML technologies and their permeation into various
    industry and research sectors. It aims to alleviate some of the complex, time-consuming
    aspects of the data science process, allowing for broader usage and more accessible
    application of ML technologies.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**自动机器学习**（**AutoML**）是一个新兴领域，旨在自动化机器学习工作流程的复杂方面，从而更高效、更易于部署机器学习模型。AutoML 的出现反映了人工智能和机器学习技术的日益复杂化，以及它们在各个行业和研究领域的渗透。它旨在减轻数据科学过程中的某些复杂、耗时的工作，使机器学习技术的应用更加广泛和易于获取。'
- en: For software engineers well versed in ML and data science processes, the increasing
    complexity of ML models and the expanding universe of algorithms can pose a significant
    challenge. Building a robust, high-performing model requires substantial expertise,
    time, and computational resources to select suitable algorithms, tune hyperparameters,
    and conduct in-depth comparisons. AutoML has emerged as a solution to these challenges,
    aiming to automate these complex, frequently labor-intensive tasks.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉机器学习和数据科学流程的软件工程师来说，机器学习模型的日益复杂性和算法宇宙的不断扩大可能构成重大挑战。构建一个稳健、高性能的模型需要大量的专业知识、时间和计算资源来选择合适的算法、调整超参数以及进行深入的比较。AutoML
    作为解决这些挑战的解决方案应运而生，旨在自动化这些复杂且劳动密集型的工作。
- en: AutoML also serves to democratize the field of ML. By abstracting away some
    of the complexities of data engineering and model building and tuning, AutoML
    makes it possible for individuals and organizations with less experience in ML
    to leverage these powerful technologies. As a result, ML can be effective in a
    broader array of contexts, with more individuals and organizations capable of
    deploying ML solutions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML 还有助于民主化机器学习领域。通过抽象化数据工程和模型构建及调优的一些复杂性，AutoML 使得那些在机器学习方面经验较少的个人和组织能够利用这些强大的技术。因此，机器学习可以在更广泛的背景下发挥作用，更多个人和组织能够部署机器学习解决方案。
- en: AutoML systems are available at various levels of complexity. Although all AutoML
    systems aim to simplify the ML workflow, most systems and tools only focus on
    parts of the workflow. Commonly, steps such as data preprocessing, feature engineering,
    model selection, and hyperparameter tuning are automated. Such automation saves
    time and can improve the robustness and performance of ML models by systematically
    exploring a more comprehensive array of options that may be overlooked or unexplored
    due to human biases or time constraints.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML系统在复杂度方面各不相同。尽管所有AutoML系统都旨在简化机器学习（ML）工作流程，但大多数系统和工具仅关注工作流程的一部分。通常，数据预处理、特征工程、模型选择和超参数调整等步骤被自动化。这种自动化可以节省时间，并通过系统地探索更全面的选项集来提高机器学习模型的鲁棒性和性能，这些选项可能由于人为偏见或时间限制而被忽视或未探索。
- en: Automating feature engineering
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化特征工程
- en: As discussed in [*Chapter 6*](B16690_06.xhtml#_idTextAnchor094), *Solving Real-World
    Data Science Problems with LightGBM*, data cleaning and feature engineering are
    critical parts of the ML workflow. They involve dealing with unusable data, handling
    missing values, and creating meaningful features that can be fed into the model.
    Manual feature engineering can be particularly challenging and time-consuming.
    AutoML systems aim to handle these tasks effectively, enabling automated feature
    extraction and transformation, leading to more robust models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如在第[*第6章*](B16690_06.xhtml#_idTextAnchor094)《使用LightGBM解决现实世界数据科学问题》中所述，数据清洗和特征工程是机器学习工作流程的关键部分。它们涉及处理不可用数据、处理缺失值以及创建可以输入到模型中的有意义的特征。手动特征工程可能特别具有挑战性和耗时。AutoML系统旨在有效地处理这些任务，实现自动特征提取和转换，从而产生更鲁棒的模型。
- en: 'Data cleaning automation is typically achieved by following specific well-known
    techniques for dealing with problems such as outliers and missing values. In previous
    chapters, we applied some of these techniques manually: outliers can be tested
    statistically and capped or truncated. Missing values are typically imputed using
    descriptive statistics such as the mean or the mode. AutoML systems either use
    heuristic algorithms and tests to select the best techniques to clean the data
    or take multiple approaches and tests that work best by training models against
    the data.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据清洗自动化通常是通过遵循处理异常值和缺失值等问题的特定知名技术来实现的。在之前的章节中，我们手动应用了一些这些技术：异常值可以通过统计测试进行测试，并截断或截顶。缺失值通常使用描述性统计量，如平均值或众数进行插补。AutoML系统要么使用启发式算法和测试来选择最佳的数据清洗技术，要么通过训练模型来采取多种最佳方法和测试。
- en: 'The methods for automating feature engineering are often similar: many possible
    transformations are applied to all existing features, and the usefulness of the
    generated features is tested after modeling. Examples of how transformations generate
    features can be found in the following case study.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化特征工程的方法通常相似：将许多可能的转换应用于所有现有特征，并在建模后测试生成的特征的有用性。以下案例研究中可以找到转换如何生成特征的示例。
- en: Another method for automating feature engineering is extracting features using
    rules based on the data type of the features. For example, the day, week, month,
    and year could be extracted from a date field. Or the number of characters, lemmas,
    stems, or embeddings of word or sentence features could be calculated.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自动化特征工程的另一种方法是使用基于特征数据类型的规则提取特征。例如，可以从日期字段中提取日期、周、月和年。或者可以计算单词或句子特征的字符数、词元、词干或嵌入。
- en: 'As you may notice, the application of feature engineering automation techniques
    relies on technical information about the features at hand, typically the data
    type, alongside correlation and relationships with other features. Importantly,
    there is no domain knowledge applied when creating new features. This highlights
    one of the shortcomings of AutoML: it cannot handle specific, domain-driven decisions
    that require human expertise. For example, consider a diabetes dataset with a
    feature for fasting blood sugar. A medical professional (domain expert) knows
    that an individual with a fasting blood sugar of 100 to 125 mg/dL is considered
    prediabetic, and any higher is considered diabetic. This continuous feature can
    be engineered to specific classes: normal, prediabetic, and diabetic, simplifying
    the data for modeling. A transformation like this is not possible to achieve with
    AutoML systems.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能注意到的，特征工程自动化技术的应用依赖于手头特征的技术信息，通常包括数据类型，以及与其他特征的关联和关系。重要的是，在创建新特征时没有应用领域知识。这突显了AutoML的一个缺点：它无法处理需要人类专业知识的特定、领域驱动型决策。例如，考虑一个包含空腹血糖特征的糖尿病数据集。医学专业人士（领域专家）知道，空腹血糖在100到125
    mg/dL之间的人被认为是糖尿病前期，任何更高都被认为是糖尿病患者。这个连续特征可以被工程化为特定的类别：正常、糖尿病前期和糖尿病患者，从而简化了建模数据。这种类型的转换是无法通过AutoML系统实现的。
- en: Automating model selection and tuning
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动化模型选择和调整
- en: Areas where AutoML is particularly useful are model selection and hyperparameter
    tuning. Given the plethora of algorithms available, choosing the best one for
    a particular dataset and problem can be daunting. AutoML systems use various techniques,
    including Bayesian optimization and meta-learning, to select the best model. They
    also automate the tuning of hyperparameters to maximize model performance.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML特别有用的领域包括模型选择和超参数调整。鉴于可用的算法众多，为特定数据集和问题选择最佳算法可能会令人望而却步。AutoML系统使用各种技术，包括贝叶斯优化和元学习，来选择最佳模型。它们还自动调整超参数，以最大化模型性能。
- en: AutoML systems can provide automated cross-validation, reducing the risk of
    overfitting and ensuring the model’s generalizability to unseen data. Once the
    optimal model is selected and trained, many AutoML tools can also help with deployment,
    making the model available for inference on new data.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML系统可以提供自动交叉验证，降低过拟合的风险，并确保模型对未见数据的泛化能力。一旦选定了最佳模型并进行了训练，许多AutoML工具还可以帮助部署模型，使其可用于对新数据的推理。
- en: Beyond the initial model deployment, some AutoML solutions also provide value
    in ongoing model monitoring and maintenance. As real-world data evolves, models
    may suffer from drift, and their performance can degrade. AutoML can help monitor
    model performance and retrain the model as needed, ensuring that your ML system
    remains effective in the long run.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 除了初始模型部署之外，一些AutoML解决方案在持续模型监控和维护方面也提供了价值。随着现实世界数据的发展，模型可能会出现漂移，其性能可能会下降。AutoML可以帮助监控模型性能，并在需要时重新训练模型，确保您的机器学习系统在长期内保持有效。
- en: Risks of using AutoML systems
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用AutoML系统的风险
- en: As mentioned previously, AutoML systems typically use no domain knowledge to
    aid in feature engineering, model selection, or other automation. Instead, a brute-force
    or scattershot approach of try and see is used.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，AutoML系统通常不使用领域知识来辅助特征工程、模型选择或其他自动化任务。相反，它们采用一种试错的方法。
- en: The “black box” nature of some AutoML systems can also make it challenging to
    interpret decisions made by the system, making it less suitable for applications
    that require high levels of explainability.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一些AutoML系统的“黑盒”特性也可能使得解释系统做出的决策变得具有挑战性，这使得它们对于需要高解释性的应用不太适合。
- en: Therefore, it’s still essential to have a data scientist or domain expert in
    the loop, working alongside the AutoML system, to identify and act on opportunities
    where domain knowledge can lead to better models. However, AutoML systems sometimes
    hinder the data scientist instead of enabling them by creating one extra layer
    between the scientist and the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，仍然非常重要的是要有一个数据科学家或领域专家参与其中，与AutoML系统协同工作，以识别并利用领域知识可以带来更好模型的机遇。然而，AutoML系统有时会阻碍数据科学家，而不是通过在科学家和数据之间增加一个额外的层次来使他们能够发挥作用。
- en: 'We’ve already seen one AutoML framework in action. Optuna, which we discussed
    in [*Chapter 5*](B16690_05.xhtml#_idTextAnchor083), *LightGBM Parameter Optimization
    with Optuna*, is an example of an AutoML framework focusing on hyperparameter
    tuning. In the next section, we discuss another AutoML framework: FLAML.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了一个自动化机器学习框架的实际应用。在[*第五章*](B16690_05.xhtml#_idTextAnchor083)中讨论的Optuna，*使用Optuna进行LightGBM参数优化*，是一个专注于超参数调整的自动化机器学习框架的例子。在下一节中，我们将讨论另一个自动化机器学习框架：FLAML。
- en: Introducing FLAML
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍FLAML
- en: '**FLAML** (**Fast and Lightweight AutoML**) is a Python library developed by
    Microsoft Research *[1]*. It is designed to produce high-quality ML models with
    low computational cost automatically. The primary aim of FLAML is to minimize
    the resources required to tune hyperparameters and identify optimal ML models,
    making AutoML more accessible and cost-effective, particularly for users with
    budget constraints.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**FLAML**（**快速轻量级自动化机器学习**）是由微软研究院开发的Python库*[1]*。它旨在自动生成高质量的机器学习模型，降低计算成本。FLAML的主要目标是最大限度地减少调整超参数和识别最佳机器学习模型所需的资源，使自动化机器学习更加易于访问和成本效益，尤其是对于预算有限的用户。'
- en: FLAML offers several key features that set it apart. One of these is its efficiency.
    It provides a fast and lightweight solution for ML tasks, minimizing the time
    and computational resources needed. It achieves this without compromising the
    quality of the models it produces. FLAML also emphasizes its versatility across
    various ML algorithms and various application domains.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML提供了一些关键特性，使其与众不同。其中之一是它的效率。它为机器学习任务提供了一种快速轻量级的解决方案，最大限度地减少了所需的时间和计算资源。它在不影响所产生模型质量的情况下实现了这一点。FLAML还强调其在各种机器学习算法和不同应用领域中的多功能性。
- en: The core of FLAML’s efficiency lies in its novel, cost-effective search algorithms.
    These algorithms intelligently explore the hyperparameter space, initially focusing
    on “cheap” configurations. It gradually explores more “expensive” configurations
    as it gains more insights into the search space. This ensures a balanced exploration
    and exploitation, delivering optimized models within user-specified time and resource
    budgets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML的高效核心在于其新颖且成本效益高的搜索算法。这些算法智能地探索超参数空间，最初专注于“低成本”配置。随着对搜索空间的深入了解，它逐渐探索更多“高成本”配置。这确保了平衡的探索和利用，在用户指定的时间和资源预算内提供优化的模型。
- en: FLAML also excels at the model selection process. It supports various ML algorithms,
    including XGBoost, LightGBM, CatBoost, RandomForest, and various linear models.
    The library can automatically choose the best algorithm for a given dataset and
    optimize its hyperparameters, providing users with an optimal model without extensive
    manual intervention.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML在模型选择过程中也表现出色。它支持各种机器学习算法，包括XGBoost、LightGBM、CatBoost、随机森林和各种线性模型。该库可以自动为给定数据集选择最佳算法并优化其超参数，为用户提供一个无需大量手动干预的优化模型。
- en: FLAML provides a straightforward, intuitive API that integrates seamlessly with
    existing Python-based data science and ML workflows. Users specify the dataset,
    a time budget (in seconds), and the optimization task, and FLAML handles the rest.
    This user-friendliness and efficiency make it a practical choice for ML beginners
    and seasoned practitioners looking to expedite their workflows.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML提供了一个简单直观的API，可以无缝集成到现有的基于Python的数据科学和机器学习工作流程中。用户指定数据集、时间预算（以秒为单位）和优化任务，FLAML处理其余部分。这种用户友好性和效率使其成为机器学习初学者和希望加快工作流程的资深从业者的一项实用选择。
- en: 'The novelty behind FLAML’s efficiency comes from its **hyperparameter optimization**
    (**HPO**) algorithms. FLAML provides two HPO algorithms: **Cost Frugal Optimization**
    and **BlendSearch**.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML效率背后的新意来自于其**超参数优化**（HPO）算法。FLAML提供了两种HPO算法：**成本节约优化**和**BlendSearch**。
- en: Cost Frugal Optimization
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 成本节约优化
- en: '**Cost Frugal Optimization** (**CFO**) is a local search method that leverages
    random direct search to explore the hyperparameter space *[2]*. The CFO algorithm
    starts with a low-cost hyperparameter configuration (for LightGBM, a low-cost
    configuration would, for instance, have few boosted trees). It takes randomized
    steps for a fixed number of iterations in the hyperparameter space toward higher
    cost parameter regions.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**成本节约优化**（CFO）是一种局部搜索方法，利用随机直接搜索来探索超参数空间*[2]*。CFO算法从一个低成本的超参数配置开始（例如，对于LightGBM，一个低成本配置可能只有少数提升树）。它在超参数空间中随机移动固定次数的迭代，朝着更高成本的参数区域前进。'
- en: The CFO step size is adaptive, meaning the algorithm lowers the step size if
    there is no improvement for several iterations. Doing so means large step sizes
    aren’t taken in unpromising directions with high cost.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: CFO的步长是自适应的，这意味着如果连续几次迭代没有改进，算法会降低步长。这样做意味着不会在成本高昂的方向上采取大步长。
- en: CFO also utilizes random restarts. As a local search algorithm, CFO can get
    stuck in local optima. If no progress is made and the step size is already small,
    the algorithm restarts at a random point.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: CFO还利用随机重启。作为一个局部搜索算法，CFO可能会陷入局部最优。如果没有任何进展且步长已经很小，算法会在一个随机点重新启动。
- en: In summary, CFO quickly (with large step sizes) attempts to reach more promising
    regions in the search space, using as little of the optimization budget as possible
    (by starting in low-cost regions). CFO continues the search while the optimization
    budget allows, restarting in random areas if stagnation occurs. FLAML allows the
    user to set the optimization budget in seconds.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，CFO快速（使用大步长）尝试在搜索空间中达到更有希望的领域，尽可能少地使用优化预算（通过在低成本区域开始）。当优化预算允许时，CFO会继续搜索，如果出现停滞，会在随机区域重新启动。FLAML允许用户以秒为单位设置优化预算。
- en: BlendSearch
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BlendSearch
- en: FLAML provides an alternative to the CFO algorithm in BlendSearch. BlendSearch
    differs from CFO by running both a global and local search process using a multithreaded
    approach *[3]*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML为BlendSearch中的CFO算法提供了一种替代方案。BlendSearch与CFO的不同之处在于，它使用多线程方法*[3]*同时运行全局和局部搜索过程。
- en: Similar to CFO, BlendSearch starts with a low-cost configuration and proceeds
    with a local search. However, unlike CFO, BlendSearch does not wait for the local
    search to stagnate before exploring new regions. Instead, a global search algorithm
    (such as Bayesian optimization) continually suggests new starting points. Starting
    points are filtered based on their distance to existing points and prioritized
    in terms of cost.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与CFO类似，BlendSearch从一个低成本配置开始，并继续进行局部搜索。然而，与CFO不同，BlendSearch不会等待局部搜索停滞后再探索新的区域。相反，全局搜索算法（如贝叶斯优化）会不断提出新的起始点。起始点会根据它们与现有点的距离进行过滤，并按成本优先排序。
- en: Each iteration of BlendSearch then chooses whether to continue a local search
    or start at a new global search point based on the performance in the previous
    iteration. Like CFO, configurations proposed by the global search method are validated
    for viability.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: BlendSearch的每次迭代都会根据前一次迭代的表现来决定是继续进行局部搜索还是从新的全局搜索点开始。与CFO类似，全局搜索方法提出的配置会经过可行性验证。
- en: As BlendSearch uses global optimization, BlendSearch is less prone to getting
    stuck in local minima. BlendSearch is recommended over CFO if the hyperparameter
    search space is highly complex. It’s often a good idea to try CFO first and only
    switch to BlendSearch if CFO is struggling.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于BlendSearch使用全局优化，因此它不太可能陷入局部最小值。如果超参数搜索空间非常复杂，建议使用BlendSearch而不是CFO。通常，先尝试CFO，如果CFO遇到困难，再切换到BlendSearch是个不错的主意。
- en: FLAML limitations
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FLAML局限性
- en: Despite its advantages, FLAML also has its limitations. The library’s automated
    processes may not consistently outperform manual tuning by experts, particularly
    for complex, domain-specific tasks. Also, as with other AutoML solutions, the
    interpretability of the produced models can be challenging, especially when dealing
    with models such as boosted trees or neural networks.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管FLAML有其优势，但也存在局限性。该库的自动化流程可能不会始终优于专家的手动调整，尤其是在处理复杂、特定领域的任务时。此外，与其他AutoML解决方案一样，生成的模型的可解释性可能具有挑战性，尤其是在处理如提升树或神经网络等模型时。
- en: FLAML only performs the model selection and tuning part of the ML process. These
    are some of the most time-consuming parts of model development, but FLAML does
    not provide the functionality to perform feature engineering or data preparation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML只执行ML过程中的模型选择和调整部分。这些是模型开发中最耗时的部分之一，但FLAML不提供执行特征工程或数据准备的功能。
- en: The following section presents a case study of using FLAML with LightGBM, showcasing
    everyday use cases, different optimization algorithms, and FLAML’s zero-shot AutoML.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下部分展示了使用FLAML与LightGBM的案例研究，展示了日常用例、不同的优化算法以及FLAML的零样本AutoML。
- en: Case study – using FLAML with LightGBM
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 - 使用FLAML与LightGBM
- en: We will use the Wind Turbine dataset from the previous chapter for the case
    study. The dataset is cleaned as before, imputing missing values and capping outliers
    to appropriate ranges. However, we take a different approach to feature engineering.
    To further explore AutoML, we use an open source framework called Featuretools.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用前一章中的风力涡轮机数据集作为案例研究。数据集像以前一样进行了清理，填补了缺失值，并将异常值限制在适当的范围内。然而，我们在特征工程上采取了不同的方法。为了进一步探索AutoML，我们使用了一个名为Featuretools的开源框架。
- en: Feature engineering
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程
- en: '**Featuretools** ([https://featuretools.alteryx.com/en/stable/#](https://featuretools.alteryx.com/en/stable/#))
    is an open source framework for automated feature engineering. Specifically, Featuretools
    is well suited to transforming relational datasets and temporal data.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Featuretools** ([https://featuretools.alteryx.com/en/stable/#](https://featuretools.alteryx.com/en/stable/#))
    是一个用于自动特征工程的开源框架。具体来说，Featuretools非常适合转换关系数据集和时间序列数据。'
- en: As discussed in the previous section, automated feature engineering tools typically
    use combinatorial transformations of features to generate new features for the
    dataset. Featuretools supports feature transformations through their **Deep Feature
    Synthesis** (**DFS**) process.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，自动特征工程工具通常使用特征的组合转换来为数据集生成新特征。Featuretools通过其**深度特征合成**（**DFS**）过程支持特征转换。
- en: 'As an example, consider a dataset of online customer web sessions. Typical
    features that could be useful in such a dataset are the total sessions a customer
    visited the site for, or the month a customer signed up. Using Featuretools and
    DFS, this can be achieved using the following code (courtesy of [https://featuretools.alteryx.com/](https://featuretools.alteryx.com/)):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个在线客户网络会话的数据集。此类数据集中可能有用的典型特征包括客户访问网站的会话总数，或客户注册的月份。使用Featuretools和DFS，可以通过以下代码实现（感谢[https://featuretools.alteryx.com/](https://featuretools.alteryx.com/)）：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Two transformations are being applied here: a transformation for “`month`”
    and an aggregation for “`count`”. With these transformations, the month would
    be automatically extracted from any dates present for a customer (such as the
    join date), and the count aggregations would be calculated for each customer (such
    as the number of sessions or transactions). Featuretools has a rich set of transformations
    and aggregations available. [A complete list is available at https://featuretools.alteryx.](https://featuretools.alteryx.com/en/stable/api_reference.xhtml)com/en/stable/api_reference.xhtml.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这里应用了两种转换：一个是“`month`”的转换，另一个是“`count`”的聚合。通过这些转换，会自动从客户的任何日期（如加入日期）中提取月份，并为每个客户（如会话数或交易数）计算聚合计数。Featuretools提供了一套丰富的转换和聚合功能。[完整的列表可在https://featuretools.alteryx.](https://featuretools.alteryx.com/en/stable/api_reference.xhtml)com/en/stable/api_reference.xhtml上找到。
- en: Let’s see how we can use Featuretools to engineer the features for the Wind
    Turbine dataset.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用Featuretools为风力涡轮机数据集的特征进行工程。
- en: Using Featuretools with the Wind Turbine dataset
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Featuretools和风力涡轮机数据集
- en: 'We must perform two feature engineering tasks for our dataset: engineer features
    for the datetime field and encode categorical features. To get started, we create
    an `EntitySet` for our data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须为我们数据集执行两个特征工程任务：为日期时间字段生成特征，并对分类特征进行编码。为了开始，我们为我们的数据创建一个`EntitySet`：
- en: '[PRE1]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`EntitySet` tells the Featuretools framework the entities and relationships
    we work with within the data. Customer is an example of an entity in the earlier
    example; for this case, it’s Wind Turbines. We then pass the data frame and the
    column to be used as an index.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '`EntitySet`告诉Featuretools框架我们在数据中处理的数据实体和关系。在先前的例子中，客户是一个实体示例；对于这个案例，它是风力涡轮机。然后我们传递数据框和用作索引的列。'
- en: 'We then apply `dfs` and `encode_features` to engineer the features for our
    dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们应用`dfs`和`encode_features`来为我们数据集的特征进行工程：
- en: '[PRE2]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The preceding code extracts the day, year, month, and weekday for each of our
    wind turbine measurements. The feature encoding then automatically one-hot encodes
    the categorical features for our dataset, including the new datefields.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码提取了每个风力涡轮机测量值的日、年、月和星期几。特征编码随后自动将数据集中的分类特征进行one-hot编码，包括新的日期字段。
- en: 'The following is an excerpt from the dataset column list showing some of the
    columns created by Featuretools:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的内容是从数据集列列表的摘录，显示了Featuretools创建的一些列：
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Note the one-hot encoding of categorical features: each value is now split
    into a separate column. This includes columns for unknown values (for example,
    `YEAR(datetime) is unknown`), which illustrates another way of dealing with missing
    values in categorical features. Instead of imputing a value by using something
    such as the mode, we have a column that signals to the model (`true` or `false`)
    that the value is missing.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意分类特征的独热编码：每个值现在都分割成单独的列。这包括未知值的列（例如，`YEAR(datetime) is unknown`），这说明了处理分类特征中缺失值的另一种方法。我们不是通过使用诸如众数之类的值来填充值，而是有一个列向模型（`true`或`false`）发出信号，表示该值缺失。
- en: 'The automated feature engineering has increased our column count from 22 columns
    to 66 columns. This illustrates another general caveat with automated feature
    engineering and AutoML: automation may lead to overcomplicated datasets. In [*Chapter
    6*](B16690_06.xhtml#_idTextAnchor094), *Solving Real-World Data Science Problems
    with LightGBM*, we could encode features selectively based on our understanding
    of the learning algorithm. LightGBM can automatically handle categorical features;
    therefore, one-hot encoding is superfluous if LightGBM is the only learning algorithm
    used.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 自动特征工程将我们的列数从22列增加到66列。这说明了自动特征工程和AutoML的另一个一般性警告：自动化可能导致数据集过于复杂。在[*第6章*](B16690_06.xhtml#_idTextAnchor094)《使用LightGBM解决现实世界数据科学问题》中，我们可以根据对学习算法的理解有选择性地编码特征。LightGBM可以自动处理分类特征；因此，如果只使用LightGBM作为学习算法，则独热编码是多余的。
- en: Additionally, the datefields could be handled numerically. By applying our knowledge
    of the problem and algorithm, we can reduce the dimensionality of the learning
    problem, thereby simplifying it. The ease of use of automated systems has to be
    balanced with the manual effort of expert feature engineering, which could save
    time later on.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，日期字段可以以数值方式处理。通过应用我们对问题和算法的了解，我们可以降低学习问题的维度，从而简化它。自动系统的易用性必须与专家特征工程的手动工作相平衡，这可能在以后节省时间。
- en: The dataset is now ready for model development; we’ve completed the feature
    engineering for our dataset using only two lines of code.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集现在已准备好进行模型开发；我们仅用两行代码就完成了数据集的特征工程。
- en: FLAML AutoML
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FLAML AutoML
- en: 'We will now look at model selection and tuning. We will use FLAML to compare
    five different models: LightGBM, RandomForest, XGBoost, ExtraTrees, and a limited-depth
    version of XGBoost. We would also like to find optimal parameters for the best
    model. This entire process is possible in two lines of code with FLAML:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将探讨模型选择和调优。我们将使用FLAML比较五种不同的模型：LightGBM、RandomForest、XGBoost、ExtraTrees以及XGBoost的有限深度版本。我们还想找到最佳模型的最佳参数。整个流程只需两行代码即可使用FLAML完成：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The preceding code fits an optimal regression model within a time budget of
    60 seconds. FLAML automatically uses a holdout set to calculate validation results
    and then proceeds with optimization, using CFO as the tuner by default.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码在60秒的时间预算内拟合了一个最优回归模型。FLAML自动使用保留集来计算验证结果，然后使用默认的CFO作为调优器进行优化。
- en: 'The AutoML class provides “task-oriented AutoML.” The user sets the learning
    task, and FLAML does the rest. Among others, the following tasks are supported:
    classification, regression, time-series forecasting and time-series classification,
    ranking, and NLP-related tasks such as summarization and word token classification.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML类提供“面向任务的AutoML”。用户设置学习任务，FLAML完成剩余工作。支持的任务包括：分类、回归、时间序列预测和时间序列分类、排序以及与NLP相关的任务，如摘要和词元分类。
- en: 'The call to `fit` is customizable. For example, we could customize it as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`函数的调用是可定制的。例如，我们可以这样定制它：'
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we customize the hyperparameter search space by explicitly declaring the
    learning rate as a log-scaled uniform variable within a range. Other options for
    setting the search space for parameters are uniform sampling, random integer sampling,
    and choice-based sampling for categorical parameters.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们通过显式声明学习率作为一个在范围内对数缩放的均匀变量来自定义超参数搜索空间。设置参数搜索空间的其它选项包括均匀抽样、随机整数抽样以及用于分类参数的选择性抽样。
- en: 'Furthermore, we set the estimator list to focus on only three modeling algorithms:
    LightGBM, Random Forest, and XGBoost. Lastly, we can customize the HPO algorithm,
    and here, we set it to BlendSearch, which uses the multithreaded optimization
    approach discussed earlier.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将估计器列表设置为仅关注三种建模算法：LightGBM、随机森林和 XGBoost。最后，我们可以自定义 HPO 算法，在这里我们将其设置为
    BlendSearch，它使用之前讨论过的多线程优化方法。
- en: A complete list of customizations is available at https://microsoft.github.io/FLAML/docs/reference/automl/automl/#automl-objects.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的自定义列表可在 https://microsoft.github.io/FLAML/docs/reference/automl/automl/#automl-objects
    找到。
- en: Once `fit` has been called, we can use the AutoML-trained model as we would
    any other. FLAML provides a scikit-learn-style API for prediction and probability-based
    prediction (for classification problems).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦调用 `fit`，我们可以像使用任何其他模型一样使用 AutoML 训练的模型。FLAML 提供了类似 scikit-learn 的 API 用于预测和基于概率的预测（用于分类问题）。
- en: 'The following code creates predictions from the given data and calculates metrics
    and feature importance:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码从给定数据创建预测并计算指标和特征重要性：
- en: '[PRE6]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can also get the best hyperparameter configuration for the winning model
    and for each of the models trialed by calling the following:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过调用以下代码来获取获胜模型和每个试验模型的最佳超参数配置：
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: A final notable feature of FLAML is zero-shot AutoML, which bypasses the need
    for model tuning entirely.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML 的一个最终显著特点是零样本 AutoML，它完全绕过了模型调整的需求。
- en: Zero-shot AutoML
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本 AutoML
- en: '**Zero-shot AutoML** is a FLAML feature where hyperparameter optimization is
    not performed. Instead, suitable hyperparameter configurations are determined
    offline by analyzing the performance of an algorithm on a wide variety of datasets.
    The process can be described as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**零样本 AutoML** 是 FLAML 的一个功能，其中不执行超参数优化。相反，通过分析算法在广泛数据集上的性能，离线确定合适的超参数配置。该过程可以描述如下：'
- en: 'Before building a model:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在构建模型之前：
- en: Train models on many datasets using AutoML
  id: totrans-93
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 AutoML 在许多数据集上训练模型
- en: Store all datasets’ hyperparameter configurations, evaluation results, and metadata
    as a zero-shot solution
  id: totrans-94
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有数据集的超参数配置、评估结果和元数据存储为零样本解决方案。
- en: 'When building a model for a new problem:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当为新的问题构建模型时：
- en: Use FLAML to analyze the new dataset against the zero-shot solution results
    and determine suitable hyperparameters
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 FLAML 分析新数据集与零样本解决方案结果，以确定合适的超参数。
- en: Train a model on the new dataset using the hyperparameters
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用超参数在新数据集上训练模型。
- en: The first step is performed only once for a given model type (such as LightGBM).
    Thereafter, a new model can be built for any new problem without tuning. The solution
    is “zero-shot” because suitable parameters are used on the first fit for a new
    dataset.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步对于给定的模型类型（例如 LightGBM）只执行一次。之后，对于任何新的问题都可以构建新的模型，无需调整。解决方案是“零样本”，因为对于新的数据集，在第一次拟合时使用了合适的参数。
- en: 'FLAML’s zero-shot AutoML approach has many advantages:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML 的零样本 AutoML 方法具有许多优点：
- en: As mentioned, no tuning is involved, sparing much computational effort and time
    when solving a new problem
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，不涉及调整，在解决新问题时节省了大量计算努力和时间。
- en: Since no tuning is required, a validation dataset is not required either, and
    more of the data may be used for training
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于不需要调整，因此也不需要验证数据集，更多的数据可以用于训练。
- en: Even less involvement is required by the user
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户需要的参与更少。
- en: Often, no code changes are required, as we’ll see next
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，不需要更改代码，正如我们接下来将要看到的。
- en: Of course, creating the zero-shot solution for a model type is still arduous,
    requiring varied datasets and much computation to train many models. Fortunately,
    FLAML provides pretrained zero-shot solutions for many popular models, including
    LightGBM, XGBoost, and scikit-learn’s random forests.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，为模型类型创建零样本解决方案仍然很困难，需要各种数据集和大量计算来训练许多模型。幸运的是，FLAML 为许多流行的模型提供了预训练的零样本解决方案，包括
    LightGBM、XGBoost 和 scikit-learn 的随机森林。
- en: 'To use a zero-shot solution, replace the regular LightGBM import with the FLAML-wrapped
    version:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用零样本解决方案，将常规的 LightGBM 导入替换为 FLAML 包装的版本：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Calling `fit` analyzes the data in `X`, selects suitable parameters, and trains
    the model using those parameters. Training is performed only once, and no tuning
    is done.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `fit` 会分析 `X` 中的数据，选择合适的参数，并使用这些参数训练模型。训练只执行一次，不进行任何调整。
- en: This concludes our case study of FLAML. As we have seen, FLAML provides an intuitive
    API for sophisticated model selection and tuning functionality, which could spare
    much effort when working on an ML problem.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着FLAML案例研究的结束。正如我们所见，FLAML提供了一个直观的API，用于复杂的模型选择和调优功能，这在处理ML问题时可以节省很多精力。
- en: Summary
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In summary, this chapter discussed AutoML systems and their uses. Typical approaches
    to automating feature engineering, model selection, and tuning were discussed.
    We also mentioned the risks and caveats associated with using these systems.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本章讨论了AutoML系统和它们的用途。我们讨论了自动化特征工程、模型选择和调优的典型方法。我们还提到了使用这些系统可能存在的风险和注意事项。
- en: The chapter also introduced FLAML, a library for AutoML that provides tools
    for automating the model selection and tuning process. We also presented CFO and
    BlendSearch, two efficient hyperparameter optimization algorithms provided by
    FLAML.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还介绍了FLAML，这是一个提供自动化模型选择和调优工具的AutoML库。我们还介绍了FLAML提供的两个高效的超参数优化算法：CFO和BlendSearch。
- en: The practicalities of applying FLAML were shown in the form of a case study.
    In addition to FLAML, we showcased an open source tool called Featuretools, which
    provides functionality to automate feature engineering. We showed how to develop
    optimized models in fixed-time budgets using FLAML. Finally, we provided examples
    of using FLAML’s zero-shot AutoML functionality, which analyzes datasets against
    configurations for known problems to determine suitable hyperparameters, eliminating
    the need for model tuning.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: FLAML的实际应用通过案例研究的形式展示。除了FLAML，我们还展示了一个开源工具Featuretools，它提供自动化特征工程的功能。我们展示了如何使用FLAML在固定时间内开发优化模型。最后，我们提供了使用FLAML零样本AutoML功能示例，该功能通过分析数据集与已知问题的配置来确定合适的超参数，从而消除了模型调优的需要。
- en: The next chapter discusses building ML pipelines around LightGBM models, focusing
    on exporting, packaging, and deploying LightGBM models for production.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论围绕LightGBM模型构建ML管道，重点关注导出、打包和部署LightGBM模型以用于生产。
- en: References
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| *[**1]* | *C. Wang, Q. Wu, M. Weimer, and E. Zhu, “FLAML: A Fast and Lightweight
    AutoML Library,” in* *MLSys, 2021.* |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| *[**1]* | *王晨，吴强，魏梅，朱易，“FLAML：一个快速且轻量级的AutoML库”，发表于* *MLSys，2021.* |'
- en: '| *[**2]* | *Q. Wu, C. Wang and S. Huang, Frugal Optimization for Cost-related*
    *Hyperparameters, 2020.* |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| *[**2]* | *吴强，王晨，黄思，关于成本相关超参数的节约优化，2020.* |'
- en: '| *[**3]* | *C. Wang, Q. Wu, S. Huang, and A. Saied, “Economical Hyperparameter
    Optimization With Blended Search Strategy,” in* *ICLR, 2021.* |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| *[**3]* | *王晨，吴强，黄思，赛义德，“使用混合搜索策略进行经济超参数优化”，发表于* *ICLR，2021.* |'
- en: 'Part 3: Production-ready Machine Learning with LightGBM'
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：使用LightGBM的生产就绪机器学习
- en: In Part 3, we will delve into the practical applications of ML solutions in
    production environments. We will uncover the intricacies of machine learning pipelines,
    ensuring systematic data processing and model building for consistent results.
    MLOps, a confluence of DevOps and ML, takes center stage, highlighting the importance
    of deploying and maintaining robust ML systems in real-world scenarios. Through
    hands-on examples, we will explore the deployment of ML pipelines on platforms
    (like Google Cloud, Amazon SageMaker, and the innovative PostgresML) emphasizing
    the unique advantages each offers. Lastly, distributed computing and GPU-based
    training will be explored, showcasing methods to expedite training processes and
    manage larger datasets efficiently. This concluding part will emphasize the seamless
    integration of ML into practical, production-ready solutions, equipping readers
    with the knowledge to bring their models to life in dynamic environments.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3部分，我们将深入探讨机器学习解决方案在生产环境中的实际应用。我们将揭示机器学习管道的复杂性，确保系统性地处理数据并构建模型以获得一致的结果。MLOps，DevOps和ML的结合，成为焦点，突出了在现实场景中部署和维护强大ML系统的重要性。通过实际案例，我们将探讨在平台（如Google
    Cloud、Amazon SageMaker和创新的PostgresML）上部署ML管道，强调每个平台提供的独特优势。最后，我们将探讨分布式计算和基于GPU的训练，展示加速训练过程和管理大数据集的有效方法。本部分将强调将ML无缝集成到实际、生产就绪的解决方案中，为读者提供将他们的模型在动态环境中实现的知识。
- en: 'This part will include the following chapters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分将包括以下章节：
- en: '[*Chapter 8*](B16690_08.xhtml#_idTextAnchor134)*, Machine Learning Pipelines
    and MLOps with LightGBM*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B16690_08.xhtml#_idTextAnchor134)*，使用LightGBM的机器学习管道和MLOps*'
- en: '[*Chapter 9*](B16690_09.xhtml#_idTextAnchor146)*, LightGBM MLOps with AWS SageMaker*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B16690_09.xhtml#_idTextAnchor146)*，使用AWS SageMaker进行LightGBM MLOps*'
- en: '[*Chapter 10*](B16690_10.xhtml#_idTextAnchor162)*, LightGBM* *M**odels with
    PostgresML*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B16690_10.xhtml#_idTextAnchor162)*，PostgresML的LightGBM* *模型*'
- en: '[*Chapter 11*](B16690_11.xhtml#_idTextAnchor177)*, Distributed and GPU-based
    Learning with LightGBM*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B16690_11.xhtml#_idTextAnchor177)*，基于分布式和GPU的LightGBM学习*'
