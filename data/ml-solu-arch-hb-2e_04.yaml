- en: '4'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '4'
- en: Data Management for ML
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的数据管理
- en: As an ML solutions architecture practitioner, I often receive requests for guidance
    on designing data management platforms for ML workloads. Although data management
    platform architecture is typically treated as a separate technical discipline,
    it plays a crucial role in ML workloads. To create a comprehensive ML platform,
    ML solutions architects must understand the essential data architecture considerations
    for ML and be familiar with the technical design of a data management platform
    that caters to the needs of data scientists and automated ML pipelines.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器学习解决方案架构师，我经常收到关于设计机器学习工作负载的数据管理平台的指导请求。尽管数据管理平台架构通常被视为一个独立的技术学科，但它对机器学习工作负载起着至关重要的作用。为了创建一个全面的机器学习平台，机器学习解决方案架构师必须了解机器学习的基本数据架构考虑因素，并熟悉满足数据科学家和自动化机器学习管道需求的数据管理平台的技术设计。
- en: In this chapter, we will explore the intersection of data management and ML,
    discussing key considerations for designing a data management platform specifically
    tailored for ML. We will delve into the core architectural components of such
    a platform and examine relevant AWS technologies and services that can be used
    to build it.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨数据管理和机器学习（ML）的交汇点，讨论为机器学习量身定制的数据管理平台的关键考虑因素。我们将深入研究此类平台的核心架构组件，并检查可用于构建该平台的相关AWS技术和服务。
- en: 'The following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 以下内容将涵盖：
- en: Data management considerations for ML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的数据管理考虑因素
- en: Data management architecture for ML
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的数据管理架构
- en: Hands-on exercise – data management for ML
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实践练习 – 机器学习的数据管理
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, you will need access to an AWS account and AWS services such
    as **Amazon** **S3**, **Amazon** **Lake Formation**, **AWS** **Glue**, and **AWS**
    **Lambda**. If you do not have an AWS account, follow the official AWS website’s
    instructions to create an account.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您需要访问AWS账户以及AWS服务，如**Amazon S3**、**Amazon Lake Formation**、**AWS Glue**和**AWS
    Lambda**。如果您没有AWS账户，请按照AWS官方网站的说明创建账户。
- en: Data management considerations for ML
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的数据管理考虑因素
- en: Data management is a broad and complex topic. Many organizations have dedicated
    data management teams and organizations to manage and govern the various aspects
    of a data platform. Historically, data management primarily revolved around fulfilling
    the requirements of transactional systems and analytics systems. However, as ML
    solutions gain prominence, there are now additional business and technological
    factors to consider when it comes to data management platforms. The advent of
    ML introduces new requirements and challenges that necessitate an evolution in
    data management practices to effectively support these advanced solutions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管理是一个广泛且复杂的话题。许多组织都设有专门的数据管理团队和组织来管理和治理数据平台的各种方面。从历史上看，数据管理主要围绕满足事务系统和分析系统的需求展开。然而，随着机器学习解决方案的兴起，在考虑数据管理平台时，现在还需要考虑额外的商业和技术因素。机器学习的出现引入了新的需求和挑战，这需要数据管理实践的发展，以有效地支持这些高级解决方案。
- en: 'To understand where data management intersects with the ML workflow, let’s
    bring back the ML lifecycle, as illustrated in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解数据管理如何与机器学习工作流程相交，让我们回顾一下以下图中所示的机器学习生命周期：
- en: '![Figure 4.1 – Intersection of data management and the ML life cycle ](img/B20836_04_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图4.1 – 数据管理和机器学习生命周期的交汇](img/B20836_04_01.png)'
- en: 'Figure 4.1: Intersection of data management and the ML lifecycle'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1：数据管理和机器学习生命周期的交汇
- en: 'At a high level, data management intersects with the ML lifecycle in three
    stages: *data understanding and preparation*, *model training and evaluation*,
    and *model deployment*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，数据管理在三个阶段与机器学习生命周期相交：*数据理解和准备*、*模型训练和评估*以及*模型部署*。
- en: 'During the *data understanding and preparation* stage, data scientists undertake
    several essential tasks. They begin by identifying relevant data sources that
    contain datasets suitable for their modeling tasks. Exploratory data analysis
    is then performed to gain insights into the dataset, including data statistics,
    correlations between features, and data sample distributions. Additionally, data
    preparation for model training and validation is crucial, involving a series of
    steps that typically include the following:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在**数据理解和准备**阶段，数据科学家需要执行多项基本任务。他们首先识别与建模任务相关的合适数据集的数据来源。然后进行探索性数据分析，以了解数据集，包括数据统计、特征之间的相关性以及数据样本分布。此外，为模型训练和验证准备数据至关重要，通常包括以下步骤：
- en: '**Data validation**: The data is checked for errors and anomalies to ensure
    its quality. This includes verifying the data range, distribution, and data types
    and identifying missing or null values.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据验证**：检查数据中的错误和异常，以确保其质量。这包括验证数据范围、分布和数据类型，并识别缺失或空值。'
- en: '**Data cleaning**: Any identified data errors are fixed or corrected to ensure
    the accuracy and consistency of the dataset. This may involve removing duplicates,
    handling missing values, or resolving inconsistencies.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：任何识别出的数据错误都需被修正或更正，以确保数据集的准确性和一致性。这可能包括删除重复项、处理缺失值或解决不一致性。'
- en: '**Data enrichment**: Additional value is derived from the data through techniques
    like joining different datasets or transforming the data. This helps generate
    new signals and insights that can enhance the modeling process.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据丰富**：通过合并不同数据集或转换数据等技术从数据中提取额外价值。这有助于生成新的信号和洞察，从而增强建模过程。'
- en: '**Data labeling**: For supervised ML model training, training and testing datasets
    need to be labeled by human annotators or the ML model accurately. This critical
    step is necessary to guarantee the development and validation of high-quality
    models.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据标注**：对于监督式机器学习模型训练，训练和测试数据集需要由人工标注员或机器学习模型准确标注。这一关键步骤对于确保高质量模型的发展和验证是必要的。'
- en: 'The data management capabilities needed during this stage encompass the following
    aspects:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一阶段所需的数据管理能力包括以下方面：
- en: '**Dataset discovery**: The capability to search and locate curated datasets
    using relevant metadata like dataset name, description, field name, and data owner.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集发现**：能够使用相关元数据（如数据集名称、描述、字段名称和数据所有者）搜索和定位经过整理的数据集。'
- en: '**Data access**: The ability to access both raw and processed datasets to perform
    exploratory data analysis. This ensures data scientists can explore and analyze
    the data effectively.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问**：能够访问原始和经过处理的数据集以执行探索性数据分析。这确保数据科学家可以有效地探索和分析数据。'
- en: '**Querying and retrieval**: The capability to run queries against selected
    datasets to obtain details such as statistical information, data quality metrics,
    and data samples. Additionally, it includes the ability to retrieve data from
    the data management platform to a data science environment for further processing
    and feature engineering.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**查询和检索**：运行查询以针对选定数据集获取详细信息的能力，例如统计信息、数据质量指标和数据样本。此外，还包括将数据从数据管理平台检索到数据科学环境进行进一步处理和特征工程的能力。'
- en: '**Scalable data processing**: The ability to execute data processing operations
    on large datasets efficiently. This ensures that data scientists can handle and
    process substantial amounts of data during model development and experimentation.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可扩展的数据处理**：在大型数据集上高效执行数据处理操作的能力。这确保数据科学家在模型开发和实验过程中可以处理和加工大量数据。'
- en: 'During the stage of model training and validation, data scientists are responsible
    for generating a training and validation dataset to conduct formal model training.
    To facilitate this process, the following data management capabilities are essential:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在**模型训练和验证**阶段，数据科学家负责生成训练和验证数据集以进行正式的模型训练。为了促进这一过程，以下数据管理能力是必不可少的：
- en: '**Data processing and automated workflows**: A data management platform should
    provide robust data processing capabilities along with automated workflows. This
    enables the conversion of raw or curated datasets into training and validation
    datasets in various formats suitable for model training.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理和自动化工作流**：数据管理平台应提供强大的数据处理能力以及自动化工作流。这使将原始或整理好的数据集转换为适合模型训练的多种格式的训练和验证数据集成为可能。'
- en: '**Data repository and versioning**: An efficient data management platform should
    offer a dedicated data repository to store and manage the training and validation
    datasets. Additionally, it should support versioning, allowing data scientists
    to keep track of different iterations and modifications made to the datasets,
    along with the versions of the code and trained ML models.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据存储库和版本控制**：一个高效的数据管理平台应提供专门的数据存储库来存储和管理训练和验证数据集。此外，它应支持版本控制，使数据科学家能够跟踪对数据集进行的不同迭代和修改，以及代码和训练好的机器学习模型的版本。'
- en: '**Data labeling**: For supervised ML model training, training and testing datasets
    need to be labeled by human annotators or the ML model accurately. This critical
    step is necessary to guarantee the development and validation of high-quality
    models. This is a highly labor-intensive task, requiring purpose-built software
    tools to do it at scale.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据标注**：对于监督式机器学习模型训练，训练和测试数据集需要由人工标注员或机器学习模型准确标注。这一关键步骤对于确保高质量模型的发展和验证至关重要。这是一项劳动密集型任务，需要专门构建的软件工具来大规模完成。'
- en: '**ML features/embeddings generation and storage**: Some ML features/embeddings
    (e.g., averages, sums, and text embeddings) need to be pre-computed for one or
    more downstream model training tasks. These features/embeddings often need to
    be managed using purpose-built tools for efficient access and reuse.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器学习特征/嵌入生成和存储**：一些机器学习特征/嵌入（例如，平均值、总和和文本嵌入）需要为一个或多个下游模型训练任务预先计算。这些特征/嵌入通常需要使用专门构建的工具进行管理，以实现高效的访问和重用。'
- en: '**Dataset provisioning for model training**: The platform should provide mechanisms
    to serve the training and validation datasets to the model training infrastructure.
    This ensures that the datasets are accessible by the training environment, allowing
    data scientists to train models effectively.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为模型训练提供数据集**：平台应提供机制，将训练和验证数据集提供给模型训练基础设施。这确保了数据集可由训练环境访问，使数据科学家能够有效地训练模型。'
- en: 'During the stage of model deployment, the focus shifts toward utilizing the
    trained models to serve predictions. To support this stage effectively, the following
    data management capabilities are crucial:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型部署阶段，重点转向利用训练好的模型进行预测。为了有效地支持这一阶段，以下数据管理能力至关重要：
- en: '**Serving data for feature processing**: The data management platform should
    be capable of serving the data required for feature processing as part of the
    input data when invoking the deployed models. This ensures that the models receive
    the relevant data inputs required for generating predictions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**为特征处理提供数据**：数据管理平台应能够在调用部署的模型时，作为输入数据的一部分提供用于特征处理的数据。这确保了模型接收到生成预测所需的相关数据输入。'
- en: '**Serving pre-computed features/embeddings**: In some cases, pre-computed features/embeddings
    are utilized as inputs when invoking the deployed models. The data management
    platform should have the capability to serve these pre-computed features seamlessly,
    allowing the models to incorporate them into the prediction process.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提供预计算的特性和嵌入**：在某些情况下，在调用部署的模型时，会使用预计算的特性和嵌入作为输入。数据管理平台应具备无缝提供这些预计算特性的能力，使模型能够将它们纳入预测过程。'
- en: In contrast to traditional data access patterns for transactional or business
    intelligence solutions, where developers can utilize non-production data in lower
    environments for development purposes, data scientists typically require access
    to production data for model development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的事务或商业智能解决方案的数据访问模式不同，在这些模式中，开发者可以在较低的环境中利用非生产数据进行开发，数据科学家通常需要访问生产数据以进行模型开发。
- en: Having explored the considerations for ML data management, we will now delve
    deeper into the data management architecture specifically designed for ML. It
    is important to understand that effective data management is crucial for success
    in applied ML. Organizations fail with ML not just due to poor algorithms or inaccurate
    models, but also due to problems with real-world data and production systems.
    Data management shortcomings can sink ML projects despite brilliant modeling.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在探讨了机器学习数据管理的考虑因素之后，我们将更深入地探讨专门为机器学习设计的机器学习数据管理架构。重要的是要理解，有效的数据管理对于应用机器学习的成功至关重要。组织在机器学习上失败的原因不仅仅是算法差或模型不准确，还可能是由于现实世界数据和生产系统的问题。数据管理的不足可能会使即使建模出色的人工智能项目失败。
- en: Data management architecture for ML
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的数据管理架构
- en: Depending on the scale of your ML initiatives, it is important to consider different
    data management architecture patterns to effectively support them. The right architecture
    depends on the scale and scope of the ML initiatives within an organization in
    order to balance the business needs with engineering efforts.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的ML倡议的规模，考虑不同的数据管理架构模式以有效支持它们是很重要的。正确的架构取决于组织内ML倡议的规模和范围，以便在业务需求和工程努力之间取得平衡。
- en: 'For *small-scale ML projects* characterized by limited data scope, a small
    team size, and minimal cross-functional dependencies, a purpose-built data pipeline
    tailored to meet specific project requirements can be a suitable approach. For
    instance, if your project involves working with structured data sourced from an
    existing data warehouse and a publicly available dataset, you can consider developing
    a straightforward data pipeline. This pipeline would extract the necessary data
    from the data warehouse and public domain and store it in a dedicated storage
    location owned by the project team. This data extraction process can be scheduled
    as needed to facilitate further analysis and processing. The following diagram
    illustrates a simplified data management flow designed to support a small-scale
    ML project:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有有限数据范围、小型团队规模和最小跨职能依赖性的*小型ML项目*，一个专门构建的、满足特定项目需求的数据管道可能是一个合适的方法。例如，如果你的项目涉及从现有的数据仓库和公开可用的数据集中获取结构化数据，你可以考虑开发一个简单的数据管道。这个管道将从数据仓库和公共领域提取必要的数据，并将其存储在项目团队拥有的专用存储位置。此数据提取过程可以根据需要安排，以方便进一步的分析和处理。以下图表展示了一个简化的数据管理流程，旨在支持小型ML项目：
- en: '![Figure 4.2 – Data architecture for an ML project with limited scope ](img/B20836_04_02.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图4.2 – 有限范围ML项目的数据架构](img/B20836_04_02.png)'
- en: 'Figure 4.2: Data architecture for an ML project with limited scope'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2：有限范围ML项目的数据架构
- en: For *large-scale ML initiatives* at the enterprise level, the data management
    architecture closely resembles that of enterprise analytics. Both require robust
    support for data ingestion from diverse sources and centralized management of
    data for various processing and access requirements. While analytics data management
    primarily deals with structured data and often relies on an enterprise data warehouse
    as its core backend, ML data management needs to handle structured, semi-structured,
    and unstructured data for different ML tasks. Consequently, a data lake architecture
    is commonly adopted. ML data management is typically an integral part of the broader
    enterprise data management strategy, encompassing both analytics and ML initiatives.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业层面的*大规模ML倡议*，数据管理架构与企业分析非常相似。两者都需要对来自不同来源的数据摄取提供强大的支持，以及针对各种处理和访问需求的数据集中管理。虽然分析数据管理主要处理结构化数据，并且通常依赖于企业数据仓库作为其核心后端，但ML数据管理需要处理结构化、半结构化和非结构化数据以执行不同的ML任务。因此，通常采用数据湖架构。ML数据管理通常是更广泛的企业数据管理策略的一部分，包括分析和ML倡议。
- en: 'The following diagram illustrates a logical enterprise data management architecture
    comprising key components such as data ingestion, data storage, data processing,
    data catalog, data security, and data access:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了一个包含关键组件如数据摄取、数据存储、数据处理、数据目录、数据安全和数据访问的逻辑企业数据管理架构：
- en: '![Figure 4.3 – Enterprise data management ](img/B20836_04_03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3 – 企业数据管理](img/B20836_04_03.png)'
- en: 'Figure 4.3: Enterprise data management'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3：企业数据管理
- en: In the following sections, we will delve into a detailed analysis of each key
    component of enterprise data management, providing an in-depth understanding of
    their functionalities and implications within a data management architecture built
    using AWS native services in the cloud. By exploring the specific characteristics
    and capabilities of these components, we will gain valuable insights into the
    overall structure and mechanics of an AWS-based data management architecture.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将深入分析企业数据管理的每个关键组件，提供对它们在云中使用AWS原生服务构建的数据管理架构中的功能和影响的深入了解。通过探索这些组件的具体特性和能力，我们将获得关于基于AWS的数据管理架构的整体结构和机制的有价值见解。
- en: Data storage and management
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储和管理
- en: Data storage and management is a fundamental component of the overall ML data
    management architecture. ML workloads often require data from diverse sources
    and in various formats, and the sheer volume of data can be substantial, particularly
    when dealing with unstructured data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储和管理是整体机器学习（ML）数据管理架构的基本组成部分。ML工作负载通常需要来自不同来源和多种格式的数据，尤其是处理非结构化数据时，数据量可能非常庞大。
- en: To address these requirements, cloud object data storage solutions like Amazon
    S3 are commonly employed as the underlying storage medium. Conceptually, cloud
    object storage can be likened to a file storage system that accommodates files
    of different formats. Moreover, the storage system allows for the organization
    of files using prefixes, which serve as virtual folders for enhanced object management.
    It is important to note that these prefixes do not correspond to physical folder
    structures. The term “object storage” stems from the fact that each file is treated
    as an independent object, bundled with metadata, and assigned a unique identifier.
    Object storage boasts features such as virtually unlimited storage capacity, robust
    object analytics based on metadata, API-based access, and cost-effectiveness.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了满足这些需求，云对象数据存储解决方案，如Amazon S3，通常被用作底层存储介质。从概念上讲，云对象存储可以类比为一种能够容纳不同格式文件的文件存储系统。此外，存储系统允许使用前缀来组织文件，这些前缀作为虚拟文件夹，以增强对象管理。需要注意的是，这些前缀并不对应于物理文件夹结构。术语“对象存储”源于每个文件都被视为一个独立的对象，附带元数据，并分配一个唯一的标识符。对象存储具有诸如几乎无限的存储容量、基于元数据的强大对象分析、基于API的访问和成本效益等特性。
- en: To efficiently handle the vast quantities of data stored in cloud object storage,
    it is advisable to implement a data lake architecture that leverages this storage
    medium. A data lake, tailored to encompass the entire enterprise or a specific
    line of business, acts as a centralized hub for data management and access. Designed
    to accommodate limitless data volumes, the data lake facilitates the organization
    of data across various lifecycle stages, including raw, transformed, curated,
    and ML feature data. Its primary purpose is to consolidate disparate data silos
    into a singular repository that enables centralized management and access for
    both analytics and ML requirements. Notably, a data lake can house diverse data
    formats, such as structured data from databases, unstructured data like documents,
    semi-structured data in JSON and XML formats, as well as binary formats encompassing
    images, videos, and audio files. This capability proves particularly invaluable
    for ML workloads, as ML often involves working with data in multiple formats.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地处理存储在云对象存储中的大量数据，建议实施一个利用这种存储介质的数据湖架构。一个针对整个企业或特定业务线量身定制的数据湖，充当数据管理和访问的中心枢纽。设计用于容纳无限数据量，数据湖促进了数据在各个生命周期阶段（包括原始、转换、精选和ML特征数据）的组织。其主要目的是将分散的数据孤岛合并到一个单一的存储库中，以便为分析和ML需求提供集中式管理和访问。值得注意的是，数据湖可以容纳多种数据格式，如来自数据库的结构化数据、文档等非结构化数据、JSON和XML格式的半结构化数据，以及包含图像、视频和音频文件的二进制格式。这种能力对于ML工作负载尤其有价值，因为ML通常涉及处理多种格式的数据。
- en: The data lake should be organized into different zones. For example, a *landing
    zone* should be established as the target for the initial data ingestion from
    different sources. After data preprocessing and data quality management processing,
    the data can be moved to the raw data zone. Data in the *raw data zone* can be
    further transformed and processed to meet different business and downstream consumption
    needs. To further ensure the reliability of the dataset for usage, the data can
    be curated and stored in the *curated data zone*. For ML tasks, ML features often
    need to be pre-computed and stored in an ML feature zone for reuse purposes.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖应组织成不同的区域。例如，应建立一个*着陆区*作为来自不同来源的初始数据摄入的目标。在数据预处理和数据质量管理处理之后，数据可以被移动到原始数据区。*原始数据区*中的数据可以进一步转换和处理，以满足不同的业务和下游消费需求。为了进一步确保数据集的使用可靠性，数据可以被精选并存储在*精选数据区*中。对于ML任务，ML特征通常需要预先计算并存储在ML特征区中，以便于重复使用。
- en: AWS Lake Formation
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Lake Formation
- en: 'AWS Lake Formation is a comprehensive data management service offered by AWS,
    which streamlines the process of building and maintaining a data lake on the AWS
    platform. The following figure illustrates the core components of AWS Lake Formation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lake Formation 是 AWS 提供的全面数据管理服务，它简化了在 AWS 平台上构建和维护数据湖的过程。以下图示说明了 AWS Lake
    Formation 的核心组件：
- en: '![Figure 4.4 – AWS Lake Formation ](img/B20836_04_04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.4 – AWS Lake Formation](img/B20836_04_04.png)'
- en: 'Figure 4.4: AWS Lake Formation'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4：AWS Lake Formation
- en: 'Overall, AWS Lake Formation offers four fundamental capabilities to enhance
    data lake management:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，AWS Lake Formation 提供了四个基本功能来增强数据湖管理：
- en: '**Data source crawler**: This functionality automatically examines data files
    within the data lake to infer their underlying structure, enabling efficient organization
    and categorization of the data.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据源爬虫**：此功能自动检查数据湖中的数据文件，以推断其底层结构，从而实现数据的有效组织和分类。'
- en: '**Data catalog creation and maintenance**: AWS Lake Formation facilitates the
    creation and ongoing management of a data catalog, providing a centralized repository
    for metadata, enabling easy data discovery and exploration within the data lake.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据目录创建和维护**：AWS Lake Formation 促进了数据目录的创建和持续管理，提供了一个集中式元数据存储库，便于在数据湖中进行数据发现和探索。'
- en: '**Data transformation processing**: With built-in data transformation capabilities,
    the service allows for the processing and transformation of data stored in the
    data lake, enabling data scientists and analysts to work with refined and optimized
    datasets.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换处理**：凭借内置的数据转换功能，该服务允许处理和转换存储在数据湖中的数据，使数据科学家和分析人员能够使用精炼和优化的数据集进行工作。'
- en: '**Data security and access control**: AWS Lake Formation ensures robust data
    security by providing comprehensive access control mechanisms and enabling fine-grained
    permissions management, ensuring that data is accessed only by authorized individuals
    and teams.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据安全和访问控制**：AWS Lake Formation 通过提供全面的访问控制机制和启用细粒度权限管理，确保数据仅由授权的个人和团队访问，从而确保数据安全。'
- en: Lake Formation integrates with AWS Glue, a serverless **Extract, Transform,
    Load** (**ETL**) and data catalog service, to provide data catalog management
    and data ETL processing functionality. We will cover ETL and data catalog components
    separately in later sections.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: Lake Formation 与 AWS Glue 集成，AWS Glue 是一个无服务器 **提取、转换、加载**（**ETL**）和数据目录服务，以提供数据目录管理和数据
    ETL 处理功能。我们将在后面的章节中分别介绍 ETL 和数据目录组件。
- en: Lake Formation provides a centralized data access management capability for
    managing data access permissions for databases, tables, or different registered
    S3 locations. For databases and tables, the permission can be granularly assigned
    to individual tables and columns and database functions, such as creating tables
    and inserting records.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Lake Formation 提供了集中式数据访问管理功能，用于管理数据库、表或不同注册的 S3 位置的数据访问权限。对于数据库和表，权限可以细粒度地分配给单个表、列和数据库功能，例如创建表和插入记录。
- en: Data ingestion
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Data ingestion is the bridge between data sources and data storage. It plays
    a crucial role in acquiring data from diverse sources, including structured, semi-structured,
    and unstructured formats, such as databases, knowledge graphs, social media, file
    storage, and IoT devices. Its primary responsibility is to store this data persistently
    in various storage solutions like object data storage (e.g., Amazon S3), data
    warehouses, or other data stores. Effective data ingestion patterns should incorporate
    both real-time streaming and batch ingestion mechanisms to cater to different
    types of data sources and ensure timely and efficient data acquisition.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取是数据源和数据存储之间的桥梁。它在从各种来源获取数据方面发挥着关键作用，包括结构化、半结构化和非结构化格式，如数据库、知识图谱、社交媒体、文件存储和物联网设备。其主要职责是将这些数据持久地存储在各种存储解决方案中，如对象数据存储（例如，Amazon
    S3）、数据仓库或其他数据存储。有效的数据摄取模式应结合实时流式传输和批量摄取机制，以满足不同类型的数据源，并确保及时高效的数据获取。
- en: Various data ingestion technologies and tools cater to different ingestion patterns.
    For streaming data ingestion, popular choices include Apache Kafka, Apache Spark
    Streaming, and Amazon Kinesis/Kinesis Firehose. These tools enable real-time data
    ingestion and processing. On the other hand, for batch-oriented data ingestion,
    tools like **Secure File Transfer Protocol** (**SFTP**) and AWS Glue are commonly
    used. AWS Glue, in particular, offers support for a wide range of data sources
    and targets, including Amazon RDS, MongoDB, Kafka, Amazon DocumentDB, S3, and
    any databases that support JDBC connections. This flexibility allows for seamless
    ingestion of data from various sources into the desired data storage or processing
    systems.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的数据摄入技术和工具满足不同的摄入模式。对于流数据摄入，流行的选择包括Apache Kafka、Apache Spark Streaming和Amazon
    Kinesis/Kinesis Firehose。这些工具能够实现实时数据摄入和处理。另一方面，对于面向批次的摄入，常用的工具有**安全文件传输协议**（**SFTP**）和AWS
    Glue。特别是AWS Glue，它为广泛的源和目标提供支持，包括Amazon RDS、MongoDB、Kafka、Amazon DocumentDB、S3以及任何支持JDBC连接的数据库。这种灵活性使得从各种来源无缝摄入数据到所需的数据存储或处理系统成为可能。
- en: 'When making decisions on which tools to use for data ingestion, it is important
    to assess the tools and technologies based on practical needs. The following are
    some of the considerations when deciding on data ingestion tools:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在决定使用哪些工具进行数据摄入时，根据实际需求评估工具和技术非常重要。以下是在决定数据摄入工具时的一些考虑因素：
- en: '**Data format, size, and scalability**: Take into account the various data
    formats, data size, and scalability needs. ML projects could be using data from
    different sources and different formats (e.g., **CSV**, **Parquet**, JSON/XML,
    documents, or image/audio/video files). Determine whether the infrastructure can
    handle large data volumes efficiently when necessary and scale down to reduce
    costs during periods of low volume.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据格式、大小和可扩展性**：考虑各种数据格式、数据大小和可扩展性需求。机器学习项目可能使用来自不同来源和不同格式的数据（例如，**CSV**、**Parquet**、JSON/XML、文档或图像/音频/视频文件）。确定在必要时基础设施是否能够高效地处理大量数据，并在低量期间缩减规模以降低成本。'
- en: '**Ingestion patterns**: Consider the different data ingestion patterns that
    need to be supported. The tool or combination of several tools should support
    both batch ingestion patterns (transferring bulk data at specific time intervals)
    and real-time streaming (processing data such as sensor data or website clickstreams
    in real time).'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**摄入模式**：考虑需要支持的不同数据摄入模式。工具或几个工具的组合应支持批量摄入模式（在特定时间间隔传输大量数据）和实时流（实时处理如传感器数据或网站点击流）。'
- en: '**Data preprocessing capability**: Evaluate whether the ingested data needs
    to be preprocessed before it is stored in the target data repository. Look for
    tools that offer built-in processing capability or seamless integration with external
    processing tools.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据预处理能力**：评估在数据存储到目标数据存储库之前，是否需要对摄入的数据进行预处理。寻找提供内置处理能力或与外部处理工具无缝集成的工具。'
- en: '**Security**: Ensure that the selected tools provide robust security mechanisms
    for authentication and authorization to protect sensitive data.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性**：确保所选工具提供强大的安全机制，用于身份验证和授权，以保护敏感数据。'
- en: '**Reliability**: Verify that the tools offer failure recovery mechanisms to
    prevent critical data loss during the ingestion process. If recovery capability
    is lacking, ensure there is an option to rerun ingestion jobs from the source.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可靠性**：验证工具是否提供故障恢复机制，以防止在摄入过程中发生关键数据丢失。如果缺乏恢复能力，确保有选项可以从源重新运行摄入作业。'
- en: '**Support for different data sources and targets**: The chosen ingestion tools
    should be compatible with a wide range of data sources, including databases, files,
    and streaming sources. Additionally, they should provide an API for easy data
    ingestion.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持不同的数据源和目标**：所选的摄入工具应与广泛的源兼容，包括数据库、文件和流源。此外，它们应提供API以实现轻松的数据摄入。'
- en: '**Manageability**: Another important factor to consider is the level of manageability.
    Does the tool require self-management, or is it a fully managed solution? Consider
    the trade-offs between cost and operational complexity before making a decision.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可管理性**：另一个需要考虑的重要因素是可管理性水平。该工具是否需要自我管理，或者它是否是一个完全管理的解决方案？在做出决定之前，考虑成本和操作复杂性的权衡。'
- en: AWS provides several services for data ingestion into a data lake on their platform.
    These services include Kinesis Data Streams, Kinesis Firehose, AWS Managed Streaming
    for Kafka, and AWS Glue Streaming, which cater to streaming data requirements.
    For batch ingestion, options such as AWS Glue, SFTP, and AWS **Data Migration
    Service** (**DMS**) are available. In the upcoming section, we will delve into
    the usage of Kinesis Firehose and AWS Glue to manage data ingestion processes
    for data lakes. We will also discuss AWS Lambda, a serverless compute service,
    for a simple and lightweight data ingestion alternative.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: AWS在其平台上提供了多种服务，用于将数据导入其数据湖。这些服务包括Kinesis数据流、Kinesis Firehose、AWS管理的Kafka流和AWS
    Glue流，它们满足流数据的需要。对于批量导入，可用的选项包括AWS Glue、SFTP和AWS **数据迁移服务**（**DMS**）。在下一节中，我们将深入探讨如何使用Kinesis
    Firehose和AWS Glue来管理数据湖的数据导入过程。我们还将讨论AWS Lambda，这是一种无服务器计算服务，提供简单轻量级的数据导入替代方案。
- en: Kinesis Firehose
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kinesis Firehose
- en: Kinesis Firehose is a service that streamlines the process of loading streaming
    data into a data lake. It is a fully managed solution, meaning you don’t have
    to worry about managing the underlying infrastructure. Instead, you can interact
    with the service’s API to handle the ingestion, processing, and delivery of your
    data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Firehose是一种简化将流数据加载到数据湖中的过程的工具。它是一个完全托管解决方案，这意味着您无需担心管理底层基础设施。相反，您可以通过与服务的API交互来处理数据的导入、处理和交付。
- en: 'Kinesis Firehose provides comprehensive support for various scalable data ingestion
    requirements, including:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis Firehose为各种可扩展的数据导入需求提供全面支持，包括：
- en: Seamless integration with diverse data sources such as websites, IoT devices,
    and video cameras. This is achieved using an ingestion agent or ingestion API.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与各种数据源的无缝集成，如网站、物联网设备和视频摄像头。这是通过使用导入代理或导入API实现的。
- en: Versatility in delivering data to multiple destinations, including Amazon S3,
    Amazon Redshift (an AWS data warehouse service), Amazon OpenSearch (a managed
    search engine), and Splunk (a log aggregation and analysis product).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在将数据发送到多个目的地方面具有多功能性，包括Amazon S3、Amazon Redshift（一个AWS数据仓库服务）、Amazon OpenSearch（一个托管搜索引擎）和Splunk（一个日志聚合和分析产品）。
- en: Seamless integration with AWS Lambda and Kinesis Data Analytics, offering advanced
    data processing capabilities. With AWS Lambda, you can leverage serverless computing
    to execute custom functions written in languages like Python, Java, Node.js, Go,
    C#, and Ruby. For more comprehensive information on the functionality of Lambda,
    please refer to the official AWS documentation.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与AWS Lambda和Kinesis Data Analytics的无缝集成，提供高级数据处理能力。使用AWS Lambda，您可以利用无服务器计算来执行用Python、Java、Node.js、Go、C#和Ruby等语言编写的自定义函数。有关Lambda功能的更详细信息，请参阅AWS官方文档。
- en: 'The following figure illustrates the data flow with Kinesis Firehose:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了使用Kinesis Firehose的数据流：
- en: '![Figure 4.5 – Kinesis Firehose data flow ](img/B20836_04_05.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图4.5 – Kinesis Firehose数据流](img/B20836_04_05.png)'
- en: 'Figure 4.5: Kinesis Firehose data flow'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5：Kinesis Firehose数据流
- en: Kinesis operates by establishing delivery streams, which are the foundational
    components in the Firehose architecture responsible for receiving streaming data
    from data producers. These delivery streams can be configured with various delivery
    destinations, such as S3 and Redshift. To accommodate the data volume generated
    by the producers, you can adjust the throughput of the data stream by specifying
    the number of shards. Each shard has the capacity to ingest 1 MB/sec of data and
    can support data reading at a rate of 2 MB/sec. Additionally, Kinesis Firehose
    offers APIs for increasing the number of shards and merging them when needed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: Kinesis通过建立交付流来运行，这些流是Firehose架构中的基础组件，负责从数据生产者接收流数据。这些交付流可以配置各种交付目的地，如S3和Redshift。为了适应生产者生成的大量数据，您可以通过指定分片数量来调整数据流的吞吐量。每个分片具有每秒1
    MB的数据导入能力，并可以以每秒2 MB的速度支持数据读取。此外，Kinesis Firehose还提供API来增加分片数量并在需要时合并它们。
- en: AWS Glue
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Glue
- en: AWS Glue is a comprehensive serverless ETL service that helps manage the data
    integration and ingestion process for data lakes. It seamlessly connects with
    various data sources, including transactional databases, data warehouses, and
    NoSQL databases, facilitating the movement of data to different destinations,
    such as Amazon S3\. This movement can be scheduled or triggered by events. Additionally,
    AWS Glue offers the capability to process and transform data before delivering
    it to the target. It provides a range of processing options, such as the Python
    shell for executing Python scripts and Apache Spark for Spark-based data processing
    tasks. With AWS Glue, you can efficiently integrate and ingest data into your
    data lake, benefiting from its fully managed and serverless nature.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue 是一种全面的、无服务器的 ETL 服务，它帮助管理数据湖的数据集成和摄取过程。它可以无缝连接到各种数据源，包括事务型数据库、数据仓库和无服务器数据库，促进数据移动到不同的目的地，例如
    Amazon S3。这种移动可以是计划或由事件触发的。此外，AWS Glue 还提供了在交付目标之前处理和转换数据的能力。它提供了一系列处理选项，例如用于执行
    Python 脚本的 Python shell 和用于基于 Spark 的数据处理任务的 Apache Spark。借助 AWS Glue，您可以高效地将数据集成和摄取到您的数据湖中，并从中受益于其完全托管和无服务器的特性。
- en: AWS Lambda
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Lambda
- en: AWS Lambda is AWS’s serverless computing platform. It seamlessly integrates
    with various AWS services, including Amazon S3\. By leveraging Lambda, you can
    trigger the execution of functions in response to events, such as the creation
    of a new file in S3\. These Lambda functions can be developed to move data from
    different sources, such as copying data from a source S3 bucket to a target landing
    bucket in a data lake.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda 是 AWS 的无服务器计算平台。它可以无缝集成到各种 AWS 服务中，包括 Amazon S3。通过利用 Lambda，您可以在响应事件时触发函数的执行，例如在
    S3 中创建新文件。这些 Lambda 函数可以开发成从不同来源移动数据，例如将数据从源 S3 存储桶复制到数据湖中的目标着陆桶。
- en: It’s important to note that AWS Lambda is not specifically designed for large-scale
    data movement or processing tasks, due to limitations such as memory size and
    maximum execution time allowed. However, for simpler data ingestion and processing
    jobs, it proves to be a highly efficient tool.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，AWS Lambda 并非专门设计用于大规模数据移动或处理任务，因为其内存大小和允许的最大执行时间有限制。然而，对于简单的数据摄取和处理作业，它证明是一个高度有效的工具。
- en: Data cataloging
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据目录
- en: 'A data catalog plays a crucial role in enabling data analysts and scientists
    to discover and access data stored in a central data storage. It becomes particularly
    important during the data understanding and exploration phase of the ML lifecycle
    when scientists need to search and comprehend available data for their ML projects.
    When evaluating a data catalog tool, consider the following key factors:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录在使数据分析师和科学家发现和访问存储在中央数据存储中的数据方面发挥着至关重要的作用。在机器学习生命周期中的数据理解和探索阶段，当科学家需要为他们的机器学习项目搜索和理解可用数据时，这一点尤为重要。在评估数据目录工具时，请考虑以下关键因素：
- en: '**Metadata catalog**: The technology should support a central data catalog
    for effective management of data lake metadata. This involves handling metadata
    such as database names, table schemas, and table tags. The Hive metastore catalog
    is a popular standard for managing metadata catalogs.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据目录**：该技术应支持一个中心数据目录，以有效管理数据湖的元数据。这包括处理数据库名称、表模式和表标签等元数据。Hive 元数据目录是管理元数据目录的流行标准。'
- en: '**Automated data cataloging**: The technology should have the capability to
    automatically discover and catalog datasets, as well as to infer data schemas
    from various data sources like Amazon S3, relational databases, NoSQL databases,
    and logs. Typically, this functionality is implemented through a crawler that
    scans data sources, identifies metadata elements (e.g., column names, data types),
    and adds them to the catalog.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动数据目录**：该技术应具备自动发现和目录化数据集的能力，以及从各种数据源（如 Amazon S3、关系型数据库、NoSQL 数据库和日志）推断数据模式的能力。通常，此功能通过一个爬虫实现，该爬虫扫描数据源，识别元数据元素（例如，列名、数据类型），并将它们添加到目录中。'
- en: '**Tagging flexibility**: The technology should have the ability to assign custom
    attributes or tags to metadata entities like databases, tables, and fields. This
    flexibility supports enhanced data search and discovery capabilities within the
    catalog.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**标签灵活性**：该技术应能够为数据库、表和字段等元数据实体分配自定义属性或标签。这种灵活性支持在目录中增强数据搜索和发现能力。'
- en: '**Integration with other tools**: The technology should allow seamless integration
    of the data catalog with a wide range of data processing tools, enabling easy
    access to the underlying data. Additionally, native integration with data lake
    management platforms is advantageous.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与其他工具的集成**：该技术应允许数据目录与广泛的数据处理工具无缝集成，从而便于访问底层数据。此外，与数据湖管理平台的本地集成具有优势。'
- en: '**Search functionality**: The technology should have a robust search capability
    across diverse metadata attributes within the catalog. This includes searching
    by database, table, and field names, custom tags or descriptions, and data types.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索功能**：该技术应在目录中的各种元数据属性上具有强大的搜索能力。这包括按数据库、表和字段名称、自定义标签或描述以及数据类型进行搜索。'
- en: When it comes to building data catalogs, there are various technical options
    available. In this section, we first explore how AWS Glue can be utilized for
    data cataloging purposes. We will also discuss a **Do-It-Yourself** (**DIY**)
    option for a data catalog using standard AWS services such as Lambda and OpenSearch.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当谈到构建数据目录时，有各种技术选项可用。在本节中，我们首先探讨AWS Glue如何用于数据目录目的。我们还将讨论使用标准AWS服务（如Lambda和OpenSearch）的**DIY（自行制作）**数据目录选项。
- en: AWS Glue Data Catalog
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AWS Glue数据目录
- en: 'AWS Glue offers a comprehensive solution for data cataloging, integrating seamlessly
    with AWS Lake Formation and other AWS services. The AWS Glue Data Catalog can
    be a drop-in replacement for the Hive metastore catalog, so any Hive metastore-compatible
    applications can work with the AWS Glue Data Catalog. With AWS Glue, you can automatically
    discover, catalog, and organize your data assets, making them easily searchable
    and accessible to data analysts and scientists. Here are some key features and
    benefits of using AWS Glue for data cataloging:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue为数据目录提供了全面的解决方案，与AWS Lake Formation和其他AWS服务无缝集成。AWS Glue数据目录可以作为Hive元存储目录的直接替代品，因此任何与Hive元存储兼容的应用程序都可以与AWS
    Glue数据目录一起工作。使用AWS Glue，您可以自动发现、目录化和组织您的数据资产，使它们易于搜索和访问数据分析师和科学家。以下是使用AWS Glue进行数据目录的关键功能和优势：
- en: '**Automated data discovery**: AWS Glue provides automated data discovery capabilities.
    By using data crawlers, Glue can scan and analyze data from diverse structured
    and semi-structured sources such as Amazon S3, relational databases, NoSQL databases,
    and more. It identifies metadata information, including table schemas, column
    names, and data types, that is stored in the AWS Glue Data Catalog.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动化数据发现**：AWS Glue提供了自动化数据发现功能。通过使用数据爬虫，Glue可以扫描和分析来自各种结构化和半结构化数据源（如Amazon
    S3、关系型数据库、NoSQL数据库等）的数据。它识别存储在AWS Glue数据目录中的元数据信息，包括表模式、列名和数据类型。'
- en: '**Centralized metadata repository**: The AWS Glue Data Catalog serves as a
    centralized metadata repository for your data assets. It provides a unified view
    of your data, making it easier to search, query, and understand the available
    datasets.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集中式元数据仓库**：AWS Glue数据目录充当您数据资产集中式元数据仓库。它提供了一个统一的数据视图，使得搜索、查询和理解可用数据集变得更加容易。'
- en: '**Metadata management**: AWS Glue allows you to manage and maintain metadata
    associated with your data assets. You can define custom tags, add descriptions,
    and organize your data using databases, tables, and partitions within the Data
    Catalog.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**元数据管理**：AWS Glue允许您管理和维护与您的数据资产相关的元数据。您可以在数据目录中使用数据库、表和分区定义自定义标签、添加描述并组织您的数据。'
- en: The metadata hierarchy of the AWS Glue Data Catalog is organized using databases
    and tables. Databases serve as containers for tables, which hold the actual data.
    Like traditional databases, a single database can house multiple tables, which
    can be sourced from various data stores. However, each table is exclusively associated
    with a single database. To query these databases and tables, one can utilize Hive
    metastore-compatible tools such as Amazon Athena to execute SQL queries. When
    collaborating with AWS Lake Formation, access permissions to the catalog’s databases
    and tables can be controlled through the Lake Formation entitlement layer.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Glue数据目录的元数据层次结构是使用数据库和表组织的。数据库作为表的容器，实际数据存储在其中。像传统数据库一样，单个数据库可以包含多个表，这些表可以来自不同的数据存储。然而，每个表仅与单个数据库相关联。要查询这些数据库和表，可以使用与Hive元存储兼容的工具（如Amazon
    Athena）执行SQL查询。当与AWS Lake Formation协作时，可以通过Lake Formation权益层控制对目录数据库和表的访问权限。
- en: Custom data catalog solution
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义数据目录解决方案
- en: 'Another option for building a data catalog is to create your own with a set
    of AWS services. Consider this option when you have specific requirements that
    are not met by the purpose-built products. The architecture for this DIY approach
    involves leveraging services like DynamoDB and Lambda, as depicted in the accompanying
    diagram:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 构建数据目录的另一种选择是使用一组 AWS 服务创建自己的数据目录。当您有特定需求而专用产品无法满足时，请考虑此选项。这种 DIY 方法涉及利用 DynamoDB
    和 Lambda 等服务，如图所示：
- en: '![         Comprehensive data catalog using AWS Lambda, DynamoDB,            and
    Amazon OpenSearch Service                ](img/B20836_04_06.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![使用 AWS Lambda、DynamoDB 和 Amazon OpenSearch Service 的综合数据目录](img/B20836_04_06.png)'
- en: 'Figure 4.6: Custom data catalog solution'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6：自定义数据目录解决方案
- en: At a high level, AWS Lambda triggers are used to populate DynamoDB tables with
    object names and metadata when those objects are put into S3; Amazon OpenSearch
    Service is used to search for specific assets, related metadata, and data classifications.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，AWS Lambda 触发器用于在对象被放入 S3 时，将对象名称和元数据填充到 DynamoDB 表中；Amazon OpenSearch
    服务用于搜索特定资产、相关元数据和数据分类。
- en: Data processing
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理
- en: 'The data processing functionality of a data lake encompasses the frameworks
    and compute resources necessary for various data processing tasks, such as data
    correction, transformation, merging, splitting, and ML feature engineering. This
    component is a key step in the ML lifecycle as it helps prepare the data for downstream
    model training and inference steps. Common data processing frameworks include
    Python shell scripts using libraries such as pandas, NumPy, and Apache Spark.
    The essential requirements for data processing technology are as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖的数据处理功能包括执行各种数据处理任务所需的框架和计算资源，例如数据校正、转换、合并、拆分和机器学习特征工程。该组件是机器学习生命周期中的关键步骤，因为它有助于准备数据以供下游模型训练和推理步骤使用。数据处理技术的必要要求如下：
- en: '**Integration and compatibility with the underlying storage technology**: The
    ability to seamlessly work with the native storage system simplifies data access
    and movement between the storage and processing layers.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与底层存储技术的集成和兼容性**：能够无缝地与原生存储系统协同工作，简化了数据在存储和处理层之间的访问和移动。'
- en: '**Integration with the data catalog**: The capability to interact with the
    data catalog’s metastore to query databases and tables within the catalog.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**与数据目录的集成**：能够与数据目录的元存储进行交互，查询目录中的数据库和表。'
- en: '**Scalability**: The capacity to scale compute resources up or down to accommodate
    changing data volumes and processing velocity requirements.'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可伸缩性**：根据不断变化的数据量和处理速度要求，能够扩展或缩减计算资源的能力。'
- en: '**Language and framework support**: Support for popular data processing libraries
    and frameworks, such as Python and Spark.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语言和框架支持**：支持流行的数据处理库和框架，如 Python 和 Spark。'
- en: '**Batch and real-time processing capabilities**: The capability to handle both
    real-time data streams and bulk data processing in batch mode.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批处理和实时处理能力**：能够处理实时数据流和批量数据处理的批处理模式。'
- en: 'Now, let’s examine a selection of AWS services that offer data processing capabilities
    within a data lake architecture:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们考察一些 AWS 服务，它们在数据湖架构内提供数据处理能力：
- en: '**AWS Glue ETL**: In addition to supporting data movement and data catalogs,
    the ETL features of AWS Glue can be used for ETL and general-purpose data processing.
    AWS Glue ETL provides several built-in functions for data transformation, such
    as dropping the `NULL` field (the `NULL` field represents new data) and data filtering.
    It also provides general processing frameworks for Python and Spark to run Python
    scripts and Spark jobs. Glue ETL works natively with the AWS Glue Data Catalog
    to access the databases and tables in the catalog. Glue ETL can also access the
    Amazon S3 storage directly.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Glue ETL**：除了支持数据移动和数据目录外，AWS Glue 的 ETL 功能还可以用于 ETL 和通用数据处理。AWS Glue
    ETL 提供了用于数据转换的内置函数，例如删除 `NULL` 字段（`NULL` 字段表示新数据）和数据过滤。它还提供了 Python 和 Spark 的通用处理框架，以运行
    Python 脚本和 Spark 作业。Glue ETL 与 AWS Glue 数据目录原生集成，以访问目录中的数据库和表。Glue ETL 还可以直接访问
    Amazon S3 存储。'
- en: '**Amazon Elastic MapReduce (EMR)**: **Amazon** **EMR** is a fully managed big
    data processing platform on AWS. It is designed for large-scale data processing
    using the Spark framework and other Apache tools, such as **Apache** **Hive**,
    **Apache** **Hudi**, and **Presto**. It integrates with the Glue Data Catalog
    and Lake Formation natively to access databases and tables in Lake Formation.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**亚马逊弹性映射减少（EMR）**：**亚马逊** **EMR** 是 AWS 上的一个完全托管的大数据处理平台。它设计用于使用 Spark 框架和其他
    Apache 工具（如 **Apache** **Hive**、**Apache** **Hudi** 和 **Presto**）进行大规模数据处理。它能够与
    Glue 数据目录和 Lake Formation 原生集成，以访问 Lake Formation 中的数据库和表。'
- en: '**AWS Lambda**: AWS Lambda can be used for lightweight data processing tasks
    or as part of a larger data processing pipeline within the data lake architecture.
    Lambda can be triggered by real-time events, so it is a good option for real-time
    data processing.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Lambda**：AWS Lambda 可用于轻量级数据处理任务，或作为数据湖架构中更大数据处理管道的一部分。Lambda 可以由实时事件触发，因此它是实时数据处理的好选择。'
- en: While efficient data processing prepares raw data for model training and consumption,
    robust data management must also ensure ML teams can track data provenance and
    access historical versions as needed through capabilities like data versioning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然高效的数据处理为模型训练和消费准备原始数据，但稳健的数据管理还必须确保机器学习团队能够跟踪数据来源，并通过数据版本化等能力在需要时访问历史版本。
- en: ML data versioning
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML数据版本控制
- en: To establish a lineage for model training across training data and ML models,
    it is crucial to implement version control for the training, validation, and testing
    datasets. Data versioning control presents challenges as it necessitates the use
    of appropriate tools and adherence to best practices by individuals. During the
    model building process, it is common for data scientists to obtain a copy of a
    dataset, perform cleansing and transformations specific to their needs, and save
    the modified data as a new version. This poses significant challenges in terms
    of data management, including duplication and establishing links between the data
    and its various upstream and downstream tasks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在训练数据和机器学习模型之间建立模型训练的谱系，实施训练、验证和测试数据集的版本控制至关重要。数据版本控制面临挑战，因为它需要使用适当的工具并遵守个人最佳实践。在模型构建过程中，数据科学家通常获取数据集的副本，执行针对其需求的特定清洁和转换，并将修改后的数据保存为新版本。这在数据管理方面带来了重大挑战，包括数据重复以及建立数据与其各种上游和下游任务之间的联系。
- en: Data versioning for the entire data lake is out of the scope of this book. Instead,
    we will focus on discussing a few architectural options specifically related to
    versioning control for training datasets.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据湖的数据版本控制不在此书的范围之内。相反，我们将专注于讨论一些与训练数据集版本控制相关的特定架构选项。
- en: S3 partitions
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: S3 分区
- en: 'In this approach, each newly created or updated dataset is stored in a separate
    S3 partition with a unique prefix, typically derived from the name of the S3 folder.
    While this method can lead to data duplication, it offers a clear and simple approach
    to differentiate between different datasets intended for model training. To maintain
    data integrity, it is advisable to generate datasets through a controlled processing
    pipeline that enforces naming standards. The processing pipeline should also track
    data provenance and record the processing scripts used for data manipulation and
    feature engineering. Furthermore, the datasets should be configured as read-only
    for downstream applications, ensuring their immutability. The following example
    showcases an S3 partition structure, illustrating multiple versions of a training
    dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，每个新创建或更新的数据集都存储在一个具有唯一前缀的单独 S3 分区中，通常从前缀的名称中派生。虽然这种方法可能导致数据重复，但它为区分不同数据集提供了一个清晰且简单的方法，这些数据集旨在用于模型训练。为了维护数据完整性，建议通过强制命名标准的受控处理管道生成数据集。处理管道还应跟踪数据来源并记录用于数据操作和特征工程的处理脚本。此外，应将数据集配置为只读，以确保其不可变性。以下示例展示了
    S3 分区结构，说明了训练数据集的多个版本：
- en: '[PRE0]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this instance, the two versions of the dataset are segregated using distinct
    S3 prefixes. To effectively track these training files, it is recommended to employ
    a database for storing metadata pertaining to these training files. When utilizing
    these files, it is crucial to establish links between the training datasets, ML
    training jobs, ML training scripts, and the resulting ML models to establish a
    comprehensive lineage.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据集的两个版本使用不同的S3前缀进行隔离。为了有效地跟踪这些训练文件，建议使用数据库来存储与这些训练文件相关的元数据。当使用这些文件时，建立训练数据集、机器学习训练作业、机器学习训练脚本和生成的机器学习模型之间的链接至关重要，以建立全面的血缘关系。
- en: Versioned S3 buckets
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 版本化S3存储桶
- en: Amazon S3 offers versioning support for S3 buckets, which can be leveraged to
    manage different versions of training datasets when enabled. With this approach,
    each newly created or updated dataset is assigned a unique version ID at the S3
    object level. Additionally, it is recommended to utilize a database to store all
    relevant metadata associated with each version of the training dataset. This enables
    the establishment of lineage, tracking the journey from data processing to ML
    model training. The metadata should capture essential information to facilitate
    comprehensive tracking and analysis.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon S3为S3存储桶提供版本控制支持，当启用时，可以用来管理训练数据集的不同版本。采用这种方法，每个新创建或更新的数据集都会在S3对象级别分配一个唯一的版本ID。此外，建议使用数据库来存储与每个版本的训练数据集相关的所有相关元数据。这有助于建立血缘关系，跟踪从数据处理到机器学习模型训练的过程。元数据应捕获必要信息，以促进全面的跟踪和分析。
- en: Purpose-built data version tools
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专门的数据版本工具
- en: 'Instead of developing custom solutions for data version control, there are
    purpose-built tools available for efficient data version management. For example,
    these tools can be used to track and store different versions of ML training and
    validation datasets, which are important for repeatable experimentations and model
    training tasks. Here are a few notable options:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是为数据版本控制开发定制解决方案，现在有专门的工具可用于高效的数据版本管理。例如，这些工具可以用来跟踪和存储机器学习训练和验证数据集的不同版本，这对于可重复的实验和模型训练任务非常重要。以下是一些值得注意的选项：
- en: '**Git LFS (Large File Storage)**: Git LFS extends Git’s capabilities to handle
    large files, including datasets. It stores these files outside the Git repository
    while retaining versioning information. Git LFS seamlessly integrates with Git
    and is commonly used to version large files in data-centric projects.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Git LFS（大型文件存储）**：Git LFS扩展了Git的功能，以处理大型文件，包括数据集。它将这些文件存储在Git存储库之外，同时保留版本信息。Git
    LFS与Git无缝集成，常用于在以数据为中心的项目中版本化大型文件。'
- en: '**DataVersionControl (DVC)**: DVC is an open-source tool designed specifically
    for data versioning and management. It integrates with Git and provides features
    for tracking and managing large datasets. DVC enables lightweight links to actual
    data files stored in remote storage, such as Amazon S3 or a shared file system.
    This approach maintains a history of changes and allows easy switching between
    different dataset versions, eliminating the need for data duplication.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据版本控制（DVC）**：DVC是一个专门为数据版本控制和管理工作设计的开源工具。它与Git集成，并提供跟踪和管理大型数据集的功能。DVC允许对存储在远程存储（如Amazon
    S3或共享文件系统）中的实际数据文件进行轻量级链接。这种方法维护了变更的历史记录，并允许轻松地在不同的数据集版本之间切换，消除了数据复制的需要。'
- en: '**Pachyderm**: Pachyderm is an open-source data versioning and data lineage
    tool. It offers version control for data pipelines, enabling tracking of changes
    to data, code, and configuration files. Pachyderm supports distributed data processing
    frameworks like Apache Spark and provides features like reproducibility, data
    lineage, and data lineage-based branching.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pachyderm**：Pachyderm是一个开源的数据版本控制和数据血缘工具。它为数据管道提供版本控制，能够跟踪数据、代码和配置文件的变更。Pachyderm支持分布式数据处理框架，如Apache
    Spark，并提供可重复性、数据血缘和数据血缘分支等功能。'
- en: These purpose-built tools streamline the process of data versioning, ensuring
    efficient tracking and management of datasets.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些专门设计的工具简化了数据版本控制的过程，确保了数据集的高效跟踪和管理。
- en: ML feature stores
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ML特征存储
- en: In large enterprises, it is beneficial to centrally manage common reusable ML
    features like curated customer profile data and standardized product sales data.
    This practice helps reduce the ML project lifecycle, particularly during the data
    understanding and data preparation stages. To achieve this, many organizations
    have built central ML feature stores, an architectural component for storing common
    reusable ML features, as part of the ML development architecture to meet the downstream
    model development, training, and model inference needs. Depending on the specific
    requirements, there are two main options for managing these reusable ML features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型企业中，集中管理常见的可重用机器学习特征，如精心整理的客户档案数据和标准化的产品销售数据，是有益的。这种做法有助于缩短机器学习项目生命周期，尤其是在数据理解和数据准备阶段。为了实现这一点，许多组织已经构建了中心机器学习功能存储，这是一个用于存储常见可重用机器学习特征的架构组件，作为机器学习开发架构的一部分，以满足下游模型开发、训练和模型推理的需求。根据具体要求，管理这些可重用机器学习特征有两种主要选项。
- en: Firstly, you can build custom feature stores that fulfill the fundamental requirements
    of inserting and looking up organized features for ML model training. These custom
    feature stores can be tailored to meet the specific needs of the organization.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您可以构建定制的功能存储，以满足机器学习模型训练中插入和查找组织化特征的基本要求。这些定制的功能存储可以根据组织的具体需求进行定制。
- en: Alternatively, you can opt for commercial-grade feature store products, such
    as Amazon SageMaker Feature Store, a ML service offered by AWS, which we will
    delve into in later chapters. It provides advanced capabilities such as online
    and offline functionality for training and inference, metadata tagging, feature
    versioning, and advanced search. These features enable efficient management and
    utilization of ML features in production-grade scenarios.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以选择商业级功能存储产品，例如 AWS 提供的机器学习服务 Amazon SageMaker Feature Store，我们将在后续章节中深入探讨。它提供了高级功能，如在线和离线功能用于训练和推理、元数据标记、特征版本控制和高级搜索。这些功能使得在生产级场景中高效管理和利用机器学习特征成为可能。
- en: Data serving for client consumption
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为客户端消费的数据服务
- en: The central data management platform should offer various methods, such as APIs
    or Hive metastore-based approaches, to facilitate online access to the data for
    downstream tasks such as data discovery and model training. Additionally, it is
    important to consider data transfer tools that support the movement of data from
    the central data management platform to other data-consuming environments, catering
    to different data consumption patterns such as local access to the data in the
    consuming environment. It is advantageous to explore tools that either have built-in
    data serving capabilities or can be seamlessly integrated with external data serving
    tools, as building custom data serving features could be a challenging engineering
    undertaking.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 中心数据管理平台应提供各种方法，例如 API 或基于 Hive 元数据存储的方法，以促进对数据的在线访问，用于下游任务，如数据发现和模型训练。此外，考虑支持将数据从中心数据管理平台转移到其他数据消费环境的数据传输工具也很重要，以满足不同的数据消费模式，例如在消费环境中本地访问数据。探索具有内置数据服务功能或可以无缝集成到外部数据服务工具中的工具是有利的，因为构建定制的数据服务功能可能是一项具有挑战性的工程任务。
- en: When supplying data to data science environments, there are multiple data serving
    patterns to consider. In the following discussion, we will explore two prominent
    data access patterns and their characteristics.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在向数据科学环境提供数据时，需要考虑多种数据服务模式。在接下来的讨论中，我们将探讨两种突出的数据访问模式及其特点。
- en: Consumption via API
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 API 消费
- en: In this data serving pattern, consumption environments and applications have
    the capability to directly access data from the data lake. This can be achieved
    using Hive metastore-compliant tools or through direct access to S3, the underlying
    storage of the data lake. Amazon provides various services that facilitate this
    pattern, such as Amazon Athena, a powerful big data query tool, Amazon EMR, a
    robust big data processing tool, and Amazon Redshift Spectrum, a feature of Amazon
    Redshift.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种数据服务模式中，消费环境和应用程序具有直接从数据湖访问数据的能力。这可以通过使用符合 Hive 元数据存储的工具或直接访问数据湖的基础存储 S3
    来实现。Amazon 提供了各种服务来促进这种模式，例如 Amazon Athena，一个强大的大数据查询工具，Amazon EMR，一个强大的大数据处理工具，以及
    Amazon Redshift Spectrum，Amazon Redshift 的一个功能。
- en: By leveraging these services, data lake data indexed in Glue catalogs can be
    queried without the need to make a separate copy of the data. This pattern is
    particularly suitable when only a subset of the data is required for downstream
    data processing tasks. It offers the advantage of avoiding data duplication while
    enabling efficient selection and processing of specific data subsets as part of
    the overall data workflow.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 通过利用这些服务，Glue 目录中索引的数据湖数据可以查询，无需单独复制数据。这种模式特别适合只需要数据子集用于下游数据处理任务的情况。它提供了避免数据重复的同时，能够高效选择和处理特定数据子集作为整体数据工作流程一部分的优势。
- en: Consumption via data copy
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过数据复制进行消费
- en: In this data serving pattern, a specific portion of the data stored in the data
    lake is replicated or copied to the storage of the consumption environment. This
    replication allows for tailored processing and consumption based on specific needs.
    For instance, the latest or most relevant data can be loaded into a data analytics
    environment such as Amazon Redshift. Similarly, it can be delivered to S3 buckets
    owned by a data science environment, enabling efficient access and utilization
    for data science tasks. By replicating the required data subsets, this pattern
    provides flexibility and optimized performance for different processing and consumption
    requirements in various environments.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种数据服务模式中，数据湖中存储的特定数据部分被复制或复制到消费环境的存储中。这种复制允许根据特定需求进行定制处理和消费。例如，最新或最相关的数据可以加载到数据分析环境，如Amazon
    Redshift。同样，它也可以被发送到数据科学环境拥有的S3存储桶中，从而实现数据科学任务的便捷访问和利用。通过复制所需的数据子集，这种模式为不同环境中的不同处理和消费需求提供了灵活性和优化性能。
- en: Special databases for ML
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适用于ML的专用数据库
- en: Considering emerging ML paradigms like graph neural networks and generative
    AI, specialized databases have been developed to cater to ML-specific tasks such
    as link prediction, cluster classification, and retrieval-augmented generation.
    In the following section, we will delve into two types of databases—vector databases
    and graph databases—and examine how they are utilized in ML tasks. We will explore
    their unique characteristics and applications in the context of ML.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到图神经网络和生成式人工智能等新兴的ML范式，已经开发了专门的数据库来满足ML特定的任务，如链接预测、聚类分类和检索增强生成。在下一节中，我们将深入探讨两种类型的数据库——向量数据库和图数据库——并检查它们在ML任务中的应用。我们将探讨它们在ML背景下的独特特性和应用。
- en: Vector databases
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 向量数据库
- en: Vector databases, also known as vector similarity search engines or vector stores,
    are specialized databases designed to efficiently store, index, and query high-dimensional
    vectors. Examples of high-dimensional vectors include numerical vectors’ representation
    of images or text. These databases are particularly well suited for ML applications
    that rely on vector-based computations.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库，也称为向量相似性搜索引擎或向量存储，是专门设计用于高效存储、索引和查询高维向量的数据库。高维向量的例子包括表示图像或文本的数值向量。这些数据库特别适合依赖于基于向量计算的应用程序。
- en: In ML, vectors are commonly used to represent data points, embeddings, or feature
    representations. These vectors capture essential information about the underlying
    data, enabling similarity search, clustering, classification, and other ML tasks.
    Vector databases provide powerful tools for handling these vector-based operations
    at scale.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在ML中，向量通常用于表示数据点、嵌入或特征表示。这些向量捕捉了底层数据的基本信息，使得相似性搜索、聚类、分类和其他ML任务成为可能。向量数据库提供了处理这些基于向量操作的大规模强大工具。
- en: One of the key features of vector databases is their ability to perform fast
    similarity searches, allowing efficient retrieval of vectors that are most similar
    to a given query vector. This capability is essential in various ML use cases,
    such as recommender systems, content-based search, and anomaly detection.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 向量数据库的一个关键特性是它们能够执行快速的相似性搜索，允许高效检索与给定查询向量最相似的向量。这种能力在各种机器学习（ML）用例中至关重要，例如推荐系统、基于内容的搜索和异常检测。
- en: 'There are several vector database providers on the market, each offering its
    own unique features and capabilities. Some of the prominent ones include:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 市场上有多家向量数据库提供商，每个都提供其独特的功能和能力。其中一些突出的包括：
- en: '**Facebook AI Similarity Search (FAISS)**: Developed by **Facebook AI Research**
    (**FAIR**), FAISS is an open-source library for efficient similarity search and
    clustering of dense vectors. It provides highly optimized algorithms and data
    structures for fast and scalable vector search.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Facebook AI Similarity Search (FAISS)**：由**Facebook AI Research**（**FAIR**）开发，FAISS
    是一个用于高效相似性搜索和密集向量聚类的开源库。它提供高度优化的算法和数据结构，以实现快速和可扩展的向量搜索。'
- en: '**Milvus**: Milvus is an open-source vector database designed for managing
    and serving large-scale vector datasets. It offers efficient similarity search,
    supports multiple similarity metrics, and provides scalability through distributed
    computing.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Milvus**：Milvus 是一个开源的向量数据库，旨在管理和提供大规模向量数据集。它提供高效的相似性搜索，支持多种相似性度量，并通过分布式计算提供可扩展性。'
- en: '**Pinecone**: Pinecone is a cloud-native vector database service that specializes
    in high-performance similarity search and recommendation systems. It offers real-time
    indexing and retrieval of vectors with low latency and high throughput.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pinecone**：Pinecone 是一个专注于高性能相似性搜索和推荐系统的云原生向量数据库服务。它提供实时索引和检索向量，具有低延迟和高吞吐量。'
- en: '**Elasticsearch**: Although primarily known as a full-text search and analytics
    engine, Elasticsearch also provides vector similarity search capabilities using
    plugins for efficient vector indexing and querying.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Elasticsearch**：尽管Elasticsearch主要以其全文搜索和分析引擎而闻名，但它也通过插件提供向量相似性搜索功能，用于高效的向量索引和查询。'
- en: '**Weaviate**: Weaviate is an open-source vector database. It allows you to
    store data objects and vector embeddings from your favorite ML models, and scale
    seamlessly into billions of data objects.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weaviate**：Weaviate 是一个开源的向量数据库。它允许您存储来自您最喜欢的机器学习模型的数据对象和向量嵌入，并无缝扩展到数十亿数据对象。'
- en: These are just a few examples of vector database providers, and the landscape
    is continuously evolving with new solutions and advancements in the field. When
    choosing a vector database provider, it’s important to consider factors such as
    performance, scalability, ease of integration, and the specific requirements of
    your ML use case.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是向量数据库提供商的几个例子，该领域的格局正在随着新的解决方案和该领域的发展而不断演变。在选择向量数据库提供商时，重要的是要考虑性能、可扩展性、集成简便性和您机器学习用例的具体要求。
- en: Graph databases
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图数据库
- en: Graph databases are specialized databases designed to store, manage, and query
    graph-structured data. In a graph database, data is represented as nodes (entities)
    and edges (relationships) connecting these nodes, forming a graph-like structure.
    Graph databases excel at capturing and processing complex relationships and dependencies
    between entities, making them highly relevant for ML tasks.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据库是专门设计的数据库，用于存储、管理和查询图结构化数据。在图数据库中，数据以节点（实体）和连接这些节点的边（关系）的形式表示，形成一个类似图的结构。图数据库擅长捕捉和处理实体之间的复杂关系和依赖，这使得它们对于机器学习任务高度相关。
- en: Graph databases offer a powerful way to model and analyze data in domains where
    relationships play a crucial role, such as social networks, recommendation systems,
    fraud detection, knowledge graphs, and network analysis. They enable efficient
    traversal of the graph, allowing for queries that explore connections and patterns
    within the data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据库提供了一种强大的方式来建模和分析在关系起关键作用的数据领域，例如社交网络、推荐系统、欺诈检测、知识图谱和网络分析。它们能够高效地遍历图，允许查询探索数据中的连接和模式。
- en: In the context of ML, graph databases have multiple applications. One key use
    case is graph-based feature engineering, where graphs are used to represent relationships
    between entities, and the graph structure is leveraged to derive features that
    can enhance the performance of ML models. For example, in a recommendation system,
    a graph database can represent user-item interactions and graph-based features
    can be derived to capture user similarities, item similarities, or collaborative
    filtering patterns.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的背景下，图数据库有多个应用。一个关键用例是基于图的特性工程，其中使用图来表示实体之间的关系，并利用图结构来推导出可以增强机器学习模型性能的特征。例如，在推荐系统中，图数据库可以表示用户-项目交互，并可以推导出基于图的特性来捕捉用户相似性、项目相似性或协同过滤模式。
- en: Graph databases also enable graph-based algorithms, such as **graph convolutional
    networks** (**GCNs**), for tasks like node classification, link prediction, and
    graph clustering. These algorithms leverage the graph structure to propagate information
    across nodes and capture complex patterns in the data.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图数据库还支持基于图的算法，如 **图卷积网络**（**GCNs**），用于节点分类、链接预测和图聚类等任务。这些算法利用图结构在节点之间传播信息，并捕获数据中的复杂模式。
- en: Furthermore, graph databases can be used to store and query graph embeddings,
    which are low-dimensional vector representations of nodes or edges. These embeddings
    capture the structural and semantic information of the graph and can be input
    to ML models for downstream tasks, such as node classification or recommendation.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，图数据库可以用于存储和查询图嵌入，这些嵌入是节点或边的低维向量表示。这些嵌入捕获了图的结构和语义信息，可以输入到机器学习模型中，用于下游任务，如节点分类或推荐。
- en: Some of the notable graph databases include **Neo4j**, a popular and widely
    used graph database that allows for efficient storage, retrieval, and querying
    of graph-structured data, and Amazon Neptune, a fully managed graph database service
    provided by AWS.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 一些值得注意的图数据库包括 **Neo4j**，这是一个流行且广泛使用的图数据库，允许高效地存储、检索和查询图结构化数据，以及 Amazon Neptune，这是
    AWS 提供的完全托管的图数据库服务。
- en: Data pipelines
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管道
- en: 'Data pipelines streamline the flow of data by automating tasks such as data
    ingestion, validation, transformation, and feature engineering. These pipelines
    ensure data quality and facilitate the creation of training and validation datasets
    for ML models. Numerous workflow tools are available for constructing data pipelines,
    and many data management tools offer built-in capabilities for building and managing
    these pipelines:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道通过自动化数据摄取、验证、转换和特征工程等任务，简化了数据的流动。这些管道确保了数据质量，并促进了为机器学习模型创建训练和验证数据集。有许多工作流程工具可用于构建数据管道，许多数据管理工具也提供了构建和管理这些管道的内置功能：
- en: '**AWS Glue workflows**: AWS Glue workflows provide a native workflow management
    feature within AWS Glue, enabling the orchestration of various Glue jobs like
    data ingestion, processing, and feature engineering. Comprised of trigger and
    node components, a Glue workflow incorporates schedule triggers, event triggers,
    and on-demand triggers. Nodes within the workflow can be either crawler jobs or
    ETL jobs. Triggers initiate workflow runs, while event triggers are emitted after
    the completion of crawler or ETL jobs. By structuring a series of triggers and
    jobs, workflows facilitate the seamless execution of data pipelines within AWS
    Glue.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Glue 工作流程**：AWS Glue 工作流程在 AWS Glue 中提供原生的工作流程管理功能，允许编排各种 Glue 任务，如数据摄取、处理和特征工程。由触发器和节点组件组成，Glue
    工作流程包含计划触发器、事件触发器和按需触发器。工作流程中的节点可以是爬虫作业或 ETL 作业。触发器启动工作流程运行，而事件触发器在爬虫或 ETL 作业完成后发出。通过结构化一系列触发器和作业，工作流程促进了
    AWS Glue 内数据管道的无缝执行。'
- en: '**AWS Step Functions**: AWS Step Functions is a powerful workflow orchestration
    tool that seamlessly integrates with various AWS data processing services like
    AWS Glue and Amazon EMR. It enables the creation of robust workflows to execute
    diverse steps within a data pipeline, such as data ingestion, data processing,
    and feature engineering, ensuring smooth coordination and execution of these tasks.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS Step Functions**：AWS Step Functions 是一个强大的工作流程编排工具，可以无缝集成到各种 AWS 数据处理服务中，如
    AWS Glue 和 Amazon EMR。它允许创建强大的工作流程，以执行数据管道中的各种步骤，例如数据摄取、数据处理和特征工程，确保这些任务的协调和执行顺畅。'
- en: '**AWS Managed Workflows for Apache Airflow**: AWS **Managed Workflows for Apache
    Airflow** (**MWAA**) is a fully managed service that simplifies the deployment,
    configuration, and management of Apache Airflow, an open-source platform for orchestrating
    and scheduling data workflows. This service offers scalability, reliability, and
    easy integration with other AWS services, making it an efficient solution for
    managing complex data workflows in the cloud.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AWS 管理的 Apache Airflow 工作流程**：AWS **管理的 Apache Airflow 工作流程**（**MWAA**）是一个完全托管的服务的，简化了
    Apache Airflow 的部署、配置和管理，Apache Airflow 是一个开源平台，用于编排和调度数据工作流程。此服务提供可伸缩性、可靠性和与其他
    AWS 服务的轻松集成，使其成为管理云中复杂数据工作流程的高效解决方案。'
- en: Having explored the fundamental elements of ML data management architecture,
    the subsequent sections will delve into subjects related to security and governance.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 探索了机器学习数据管理架构的基本要素之后，接下来的章节将深入探讨与安全和治理相关的内容。
- en: Authentication and authorization
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认证和授权
- en: Authentication and authorization are crucial for ensuring secure access to a
    data lake. Federated authentication, such as AWS **Identity and Access Management**
    (**IAM**), verifies user identities for administration and data consumption purposes.
    AWS Lake Formation combines the built-in Lake Formation access control with AWS
    IAM to govern access to data catalog resources and underlying data storage.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 认证和授权对于确保数据湖的安全访问至关重要。联合认证，例如 AWS **身份和访问管理**（**IAM**），用于验证用户身份以进行管理和数据消费。AWS
    Lake Formation结合了内置的Lake Formation访问控制与AWS IAM来管理对数据目录资源和底层数据存储的访问。
- en: The built-in Lake Formation permission model utilizes commands like grant and
    revoke to control access to resources such as databases and tables, as well as
    actions like table creation. When a user requests access to a resource, both IAM
    policies and Lake Formation permissions are evaluated to verify and enforce access
    before granting it. This multi-layered approach enhances data lake security and
    governance.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 内置的Lake Formation权限模型使用诸如grant和revoke等命令来控制对数据库和表等资源以及如表创建等操作的访问。当用户请求访问资源时，会评估IAM策略和Lake
    Formation权限，以验证并强制执行访问权限，然后再授予访问权限。这种多层次的方法增强了数据湖的安全性和治理。
- en: 'There are several personas involved in the administration of the data lake
    and consumption of the data lake resources, including:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖的管理和数据湖资源的消费涉及多个角色，包括：
- en: '**Lake Formation administrator**: A Lake Formation administrator has permission
    to manage all aspects of a Lake Formation data lake in an AWS account. Examples
    include granting/revoking permissions to access data lake resources for other
    users, registering data stores in S3, and creating/deleting databases. When setting
    up Lake Formation, you will need to register as an administrator. An administrator
    can be an AWS IAM user or IAM role. You can add more than one administrator to
    a Lake Formation data lake.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lake Formation管理员**：Lake Formation管理员有权管理AWS账户中Lake Formation数据湖的所有方面。例如，包括为其他用户授予/撤销访问数据湖资源的权限、在S3中注册数据存储以及创建/删除数据库。在设置Lake
    Formation时，您需要注册为管理员。管理员可以是AWS IAM用户或IAM角色。您可以为Lake Formation数据湖添加多个管理员。'
- en: '**Lake Formation database creator**: A Lake Formation database creator is granted
    permission to create databases in Lake Formation. A database creator can be an
    IAM user or IAM role.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lake Formation数据库创建者**：Lake Formation数据库创建者被授予在Lake Formation中创建数据库的权限。数据库创建者可以是IAM用户或IAM角色。'
- en: '**Lake Formation database user**: A Lake Formation database user can be granted
    permission to perform different actions against a database. Example permissions
    include create table, drop table, describe table, and alter table. A database
    user can be an IAM user or IAM role.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lake Formation数据库用户**：Lake Formation数据库用户可以授予对数据库执行不同操作的权限。例如权限包括创建表、删除表、描述表和修改表。数据库用户可以是IAM用户或IAM角色。'
- en: '**Lake Formation data user**: A Lake Formation data user can be granted permission
    to perform different actions against database tables and columns. Example permissions
    include insert, select, describe, delete, alter, and drop. A data user can be
    an IAM user or an IAM role.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Lake Formation数据用户**：Lake Formation数据用户可以授予对数据库表和列执行不同操作的权限。例如权限包括插入、选择、描述、删除、修改和删除。数据用户可以是IAM用户或IAM角色。'
- en: Accessing and querying the database and tables in Lake Formation is facilitated
    through compatible AWS services like Amazon Athena and Amazon EMR. When performing
    queries using these services, Lake Formation verifies the principals (IAM users,
    groups, and roles) associated with them to ensure they have the necessary access
    permissions for the database, tables, and corresponding S3 data location. If access
    is granted, Lake Formation issues a temporary credential to the service, enabling
    it to execute the query securely and efficiently. This process ensures that only
    authorized services can interact with Lake Formation and perform queries on the
    data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过兼容的AWS服务，如Amazon Athena和Amazon EMR，简化了在Lake Formation中访问和查询数据库和表。当使用这些服务进行查询时，Lake
    Formation会验证与之关联的实体（IAM用户、组和角色），以确保它们具有对数据库、表和相应的S3数据位置的必要访问权限。如果授予访问权限，Lake Formation会向服务颁发临时凭证，使其能够安全高效地执行查询。此过程确保只有授权的服务才能与Lake
    Formation交互并查询数据。
- en: Data governance
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据治理
- en: Having secure access to trustworthy data is essential to the success of an ML
    initiative. Data governance encompasses essential practices to ensure the reliability,
    security, and accountability of data assets. Trustworthy data is achieved through
    the identification and documentation of data flows, as well as the measurement
    and reporting of data quality. Data protection and security involve classifying
    data and applying appropriate access permissions to safeguard its confidentiality
    and integrity. To maintain visibility of data activities, monitoring and auditing
    mechanisms should be implemented, allowing organizations to track and analyze
    actions performed on data, ensuring transparency and accountability in data management.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 获取可信赖的数据对机器学习项目的成功至关重要。数据治理包括确保数据资产可靠性、安全性和责任的基本实践。通过识别和记录数据流以及测量和报告数据质量，可以实现可信赖的数据。数据保护和安全涉及对数据进行分类并应用适当的访问权限，以保护其机密性和完整性。为了保持数据活动的可见性，应实施监控和审计机制，使组织能够跟踪和分析对数据执行的操作，确保数据管理中的透明度和问责制。
- en: A data catalog is one of the most important components of data governance. On
    AWS, the Glue Data Catalog is a fully managed service for data catalog management.
    You also have the option to build custom data catalogs using different foundational
    building blocks. For example, you can follow the reference architecture at [https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html](https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html)
    for building a custom data catalog on AWS.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 数据目录是数据治理最重要的组成部分之一。在AWS上，Glue数据目录是一个用于数据目录管理的完全托管服务。您还可以选择使用不同的基础构建块来构建自定义数据目录。例如，您可以参考[https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html](https://docs.aws.amazon.com/whitepapers/latest/enterprise-data-governance-catalog/implementation-reference-architecture-diagrams.html)中的参考架构，在AWS上构建自定义数据目录。
- en: Data lineage
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据血缘
- en: 'To establish and document data lineage during the ingestion and processing
    of data across different zones, it is important to capture specific data points.
    When utilizing data ingestion and processing tools like AWS Glue, AWS EMR, or
    AWS Lambda in a data pipeline, the following information can be captured to establish
    comprehensive data lineage:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在跨不同区域的数据摄取和处理过程中建立和记录数据血缘时，捕获特定数据点非常重要。当在数据管道中使用AWS Glue、AWS EMR或AWS Lambda等数据摄取和处理工具时，可以捕获以下信息以建立全面的数据血缘：
- en: '**Data source details**: Include the name of the data source, its location,
    and ownership information to identify the origin of the data.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据源详细信息**：包括数据源名称、其位置和所有权信息，以识别数据的来源。'
- en: '**Data processing job history**: Capture the history and details of the data
    processing jobs involved in the pipeline. This includes information such as the
    job name, unique **identifier** (**ID**), associated processing script, and owner
    of the job.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据处理作业历史记录**：捕获管道中涉及的数据处理作业的历史和详细信息。这包括作业名称、唯一的**标识符**（**ID**）、相关的处理脚本和作业所有者。'
- en: '**Generated artifacts**: Document the artifacts generated because of the data
    processing jobs. For example, record the S3 URI or other storage location for
    the target data produced by the pipeline.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成工件**：记录由于数据处理作业生成的工作件。例如，记录由管道生成的目标数据的S3 URI或其他存储位置。'
- en: '**Data metrics**: Track relevant metrics at different stages of data processing.
    This can include the number of records, data size, data schema, and feature statistics
    to provide insights into the processed data.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据指标**：在数据处理的不同阶段跟踪相关指标。这可能包括记录数、数据大小、数据模式以及特征统计，以提供对处理数据的洞察。'
- en: To store and manage data lineage information and processing metrics, it is recommended
    to establish a central data operational data store. AWS DynamoDB, a fully managed
    NoSQL database, is an excellent technology choice for this purpose. With its capabilities
    optimized for low latency and high transaction access, DynamoDB provides efficient
    storage and retrieval of data lineage records and processing metrics. By capturing
    and documenting these data points, organizations can establish a comprehensive
    data lineage that provides a clear understanding of the data’s journey from its
    source through various processing stages. This documentation enables traceability,
    auditability, and better management of the data as it moves through the pipeline.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储和管理数据血缘信息和处理指标，建议建立一个中央数据操作数据存储库。AWS DynamoDB，一个完全托管的NoSQL数据库，是此目的的一个优秀技术选择。DynamoDB具有针对低延迟和高事务访问优化的功能，能够高效地存储和检索数据血缘记录和处理指标。通过捕获和记录这些数据点，组织可以建立全面的数据血缘，从而清晰地了解数据从源头经过各种处理阶段的过程。此文档使得可追溯性、可审计性和数据在管道中移动时的更好管理成为可能。
- en: Other data governance measures
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 其他数据治理措施
- en: 'In addition to managing data lineage, there are several other important measures
    for effective data governance, including:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 除了管理数据血缘之外，还有其他几个重要的措施对于有效的数据治理至关重要，包括：
- en: '**Data quality**: Automated data quality checks should be implemented at different
    stages, and quality metrics should be reported. For example, after the source
    data is ingested into the landing zone, an AWS Glue quality check job can run
    to check the data quality using tools such as the open-source `Deequ` library.
    Data quality metrics (such as counts, schema validation, missing data, the wrong
    data type, or statistical deviations from the baseline) and reports can be generated
    for reviews. Optionally, manual or automated operational data cleansing processes
    should be established to correct data quality issues.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据质量**：应在不同阶段实施自动化的数据质量检查，并报告质量指标。例如，在源数据被导入到着陆区之后，可以使用如开源的`Deequ`库等工具运行AWS
    Glue质量检查作业来检查数据质量。可以生成数据质量指标（如计数、模式验证、缺失数据、错误的数据类型或与基线统计偏差）和报告以供审查。可选地，应建立手动或自动的操作数据清理流程来纠正数据质量问题。'
- en: '**Data cataloging**: Create a central data catalog and run Glue crawlers on
    datasets in the data lake to automatically create an inventory of data and populate
    the central data catalog. Enrich the catalogs with additional metadata to track
    other information to support discovery and data audits, such as the business owner,
    data classification, and data refresh date. For ML workloads, data science teams
    also generate new datasets (for example, new ML features) from the existing datasets
    in the data lake for model training purposes. These datasets should also be registered
    and tracked in a data catalog, and different versions of the data should be retained
    and archived for audit purposes.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据编目**：创建一个中央数据编目，并在数据湖中的数据集上运行Glue爬虫以自动创建数据清单并填充中央数据编目。通过添加额外的元数据来丰富编目，以跟踪其他信息以支持发现和数据审计，例如业务所有者、数据分类和数据刷新日期。对于机器学习工作负载，数据科学团队也会从数据湖中的现有数据集中生成新的数据集（例如，新的机器学习特征）用于模型训练。这些数据集也应注册并跟踪在数据编目中，并且为了审计目的，应保留和归档数据的不同版本。'
- en: '**Data access provisioning**: A formal process should be established for requesting
    and granting access to datasets and Lake Formation databases and tables. An external
    ticketing system can be used to manage the workflow for requesting access and
    granting access.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据访问授权**：应建立一个正式的过程来请求和授权对数据集和Lake Formation数据库和表的访问。可以使用外部票务系统来管理请求访问和授权访问的工作流程。'
- en: '**Monitoring and auditing**: Data access should be monitored, and access history
    should be maintained. Amazon S3 server access logging can be enabled to track
    access to all S3 objects directly. AWS Lake Formation also records all accesses
    to Lake Formation datasets in **AWS** **CloudTrail** (AWS CloudTrail provides
    event history in an AWS account to enable governance, compliance, and operational
    auditing). With Lake Formation auditing, you can get details such as event source,
    event name, SQL queries, and data output location.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控和审计**：应监控数据访问，并保留访问历史。可以启用 Amazon S3 服务器访问日志来直接跟踪所有 S3 对象的访问。AWS Lake Formation
    还会记录 AWS CloudTrail 中 Lake Formation 数据集的所有访问（AWS CloudTrail 在 AWS 账户中提供事件历史，以实现治理、合规性和运营审计）。通过
    Lake Formation 审计，您可以获取有关事件源、事件名称、SQL 查询和数据输出位置等详细信息。'
- en: By implementing these key data governance measures, organizations can establish
    a strong foundation for data management, security, and compliance, enabling them
    to maximize the value of their data assets while mitigating risks.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过实施这些关键数据治理措施，组织可以建立强大的数据管理、安全和合规性基础，使他们能够最大化其数据资产的价值，同时降低风险。
- en: Hands-on exercise – data management for ML
  id: totrans-200
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践练习 – 机器学习的数据管理
- en: In this hands-on exercise, you will go through the process of constructing a
    simple data management platform for a fictional retail bank. This platform will
    serve as the foundation for an ML workflow, and we will leverage different AWS
    technologies to build it. If you don’t have an AWS account, you can easily create
    one by following the instructions at [https://aws.amazon.com/console/](https://aws.amazon.com/console/).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实践练习中，您将经历构建一个虚构零售银行简单数据管理平台的过程。这个平台将作为机器学习工作流程的基础，我们将利用不同的 AWS 技术来构建它。如果您没有
    AWS 账户，可以按照以下说明轻松创建一个：[https://aws.amazon.com/console/](https://aws.amazon.com/console/)。
- en: 'The data management platform we create will have the following key components:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建的数据管理平台将包含以下关键组件：
- en: A data lake environment for data management using Lake Formation
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Lake Formation 进行数据管理的数据湖环境
- en: A data ingestion component for ingesting files to the data lake using Lambda
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Lambda 将文件导入数据湖的数据摄取组件
- en: A data catalog component using the Glue Data Catalog
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Glue 数据目录的数据目录组件
- en: A data discovery and query component using the Glue Data Catalog and Athena
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Glue 数据目录和 Athena 的数据发现和查询组件
- en: A data processing component using Glue ETL
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Glue ETL 的数据处理组件
- en: A data pipeline component using a Glue pipeline
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Glue 管道的数据管道组件
- en: 'The following diagram shows the data management architecture we will build
    in this exercise:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了我们将在此练习中构建的数据管理架构：
- en: '![Figure 4.6 – Data management architecture for the hands-on exercise ](img/B20836_04_07.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图 4.6 – 实践练习的数据管理架构](img/B20836_04_07.png)'
- en: 'Figure 4.7: Data management architecture for the hands-on exercise'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.7：实践练习的数据管理架构
- en: Let’s get started with building out this architecture on AWS.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建这个架构在 AWS 上的实施。
- en: Creating a data lake using Lake Formation
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Lake Formation 创建数据湖
- en: 'We will build the data lake architecture using AWS Lake Formation; it is the
    primary service for building data lakes on AWS. After you log on to the **AWS**
    **Management Console**, create an S3 bucket called `MLSA-DataLake-<your initials>`.
    We will use this bucket as the storage for the data lake. If you get a message
    that the bucket name is already in use, try adding some random characters to the
    name to make it unique. If you are not familiar with how to create S3 buckets,
    follow the instructions at the following link: [https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 AWS Lake Formation 构建数据湖架构；它是 AWS 上构建数据湖的主要服务。登录到 **AWS** **管理控制台**后，创建一个名为
    `MLSA-DataLake-<您的首字母缩写>` 的 S3 存储桶。我们将使用此存储桶作为数据湖的存储。如果您收到存储桶名称已被使用的消息，请尝试在名称中添加一些随机字符以使其唯一。如果您不熟悉如何创建
    S3 存储桶，请按照以下链接中的说明操作：[https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html](https://docs.aws.amazon.com/AmazonS3/latest/user-guide/create-bucket.html)
- en: 'After the bucket is created, follow these steps to get started with creating
    a data lake:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建存储桶后，按照以下步骤开始创建数据湖：
- en: '**Register Lake Formation administrators**: We need to add Lake Formation administrators
    to the data lake. The administrators will have full permission to manage all aspects
    of the data lake. To do this, navigate to the Lake Formation management console,
    click on the **Administrative roles and tasks** link, and you should be prompted
    to add an administrator. Select **Add myself** and click on the **Get started**
    button.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注册 Lake Formation 管理员**：我们需要将 Lake Formation 管理员添加到数据湖中。管理员将拥有管理数据湖所有方面的完全权限。为此，导航到
    Lake Formation 管理控制台，点击 **管理角色和任务** 链接，你应该会被提示添加管理员。选择 **添加我自己** 并点击 **开始** 按钮。'
- en: '**Register S3 storage**: Next, we need to register the S3 bucket (`MLSA-DataLake-<your
    initials>`) you created earlier in Lake Formation, so it will be managed and accessible
    through Lake Formation. To do this, click on the **Dashboard** link, expand **Data
    lake setup**, and then click on **Register Location**. Browse and select the bucket
    you created and click on **Register Location**. This S3 bucket will be used by
    Lake Formation to store data for the databases and manage its access permissions.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**注册 S3 存储**：接下来，我们需要在 Lake Formation 中注册您之前创建的 S3 存储桶 (`MLSA-DataLake-<your
    initials>`)，以便它可以通过 Lake Formation 进行管理和访问。为此，点击 **仪表板** 链接，展开 **数据湖设置**，然后点击
    **注册位置**。浏览并选择您创建的存储桶，然后点击 **注册位置**。这个 S3 存储桶将由 Lake Formation 用于存储数据库的数据并管理其访问权限。'
- en: '**Create database**: Now, we are ready to set up a database called `bank_customer_db`
    for managing retail customers. Before we register the database, let’s first create
    a folder called the `bank_customer_db` folder under the `MLSA-DataLake-<your initials>`
    bucket. This folder will be used to store data files associated with the database.
    To do this, click on the **Create database** button on the Lake Formation dashboard
    and follow the instructions on the screen to create the database.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建数据库**：现在，我们准备设置一个名为 `bank_customer_db` 的数据库，用于管理零售客户。在我们注册数据库之前，首先在 `MLSA-DataLake-<your
    initials>` 存储桶下创建一个名为 `bank_customer_db` 的文件夹。这个文件夹将用于存储与数据库相关的数据文件。为此，点击 Lake
    Formation 控制台上的 **创建数据库** 按钮，并按照屏幕上的说明创建数据库。'
- en: You have now successfully created a data lake powered by Lake Formation and
    created a database for data management. With this data lake created, we are now
    ready to build additional data management components. Next, we will create a data
    ingestion pipeline to move files into the data lake.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已成功创建了一个由 Lake Formation 驱动的数据湖，并创建了一个数据库用于数据管理。有了这个数据湖，我们现在准备构建额外的数据管理组件。接下来，我们将创建一个数据导入管道，将文件移动到数据湖中。
- en: Creating a data ingestion pipeline
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据导入管道
- en: 'Now that the database is prepared, we can proceed to ingest data into this
    newly created database. As mentioned earlier, there are various data sources available,
    including databases like Amazon RDS, streaming platforms like social media feeds,
    and logs such as CloudTrail. Additionally, AWS offers a range of services for
    building data ingestion pipelines, such as AWS Glue, Amazon Kinesis, and AWS Lambda.
    In this phase of the exercise, we will focus on creating an AWS Lambda function
    job that will facilitate the ingestion of data from other S3 buckets into our
    target database. As mentioned earlier, Lambda functions can be used for lightweight
    data ingestion and processing tasks:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据库已经准备好了，我们可以将数据导入到这个新创建的数据库中。如前所述，有各种数据源可供选择，包括像 Amazon RDS 这样的数据库，社交媒体流这样的流平台，以及
    CloudTrail 这样的日志。此外，AWS 提供了一系列用于构建数据导入管道的服务，例如 AWS Glue、Amazon Kinesis 和 AWS Lambda。在这个练习阶段，我们将专注于创建一个
    AWS Lambda 函数作业，它将促进从其他 S3 存储桶到我们的目标数据库的数据导入。如前所述，Lambda 函数可用于轻量级的数据导入和处理任务：
- en: '**Create a source S3 bucket and download data files**: Let’s create another
    S3 bucket, called `customer-data-source`, to represent the data source where we
    will ingest the data from.'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建源 S3 存储桶并下载数据文件**：让我们创建另一个名为 `customer-data-source` 的 S3 存储桶，以表示我们将从中导入数据的源。'
- en: '**Create a Lambda function**: Now, we will create the Lambda function that
    ingests data from the `customer-data-source` bucket to the `MLSA-DataLake-<your
    initials>` bucket:'
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建 Lambda 函数**：现在，我们将创建一个 Lambda 函数，从 `customer-data-source` 存储桶导入数据到 `MLSA-DataLake-<your
    initials>` 存储桶：'
- en: To get started, navigate to the AWS Lambda management console, click on the
    **Functions** link in the left pane, and click on the **Create Function** button
    in the right pane. Choose **Author from scratch**, then enter `datalake-s3-ingest`
    for the function name, and select the latest Python version (e.g., 3.10) as the
    runtime. Keep the default for the execution role, which will create a new IAM
    role for this Lambda function. Click on **Create function** to continue.
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始，导航到AWS Lambda管理控制台，在左侧面板中点击**函数**链接，然后在右侧面板中点击**创建函数**按钮。选择**从头开始编写**，然后输入`datalake-s3-ingest`作为函数名称，并选择最新的Python版本（例如，3.10）作为运行时。保持默认的执行角色，这将为此Lambda函数创建一个新的IAM角色。点击**创建函数**继续。
- en: On the next screen, click on **Add trigger**, select **S3** as the trigger,
    and select the `customer-data-source` bucket as the source. For **Event Type**,
    choose the **Put** event and click on the **Add** button to complete the step.
    This trigger will allow the Lambda function to be invoked when there is an S3
    bucket event, such as saving a file into the bucket.
  id: totrans-225
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏中，点击**添加触发器**，选择**S3**作为触发器，并选择`customer-data-source`存储桶作为源。对于**事件类型**，选择**PUT**事件，然后点击**添加**按钮完成此步骤。此触发器将允许Lambda函数在发生S3存储桶事件时被调用，例如将文件保存到存储桶中。
- en: 'After you add the trigger, you will be brought back to the `Lambda->function->
    datalake-s3-ingest` screen. Next, let’s create the function by replacing the default
    function template with the following code block. Replace the `desBucket` variable
    with the name of the actual bucket:'
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加触发器后，您将返回到`Lambda->function->datalake-s3-ingest`屏幕。接下来，让我们通过替换以下代码块来创建函数，替换默认函数模板。将`desBucket`变量替换为实际存储桶的名称：
- en: '[PRE1]'
  id: totrans-227
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The new function will also need S3 permission to copy files (*objects*) from
    one bucket to another. For simplicity, just add the `AmazonS3FullAccess` policy
    to the **execution IAM role** associated with the function. You can find the IAM
    role by clicking on the **Permission** tab for the Lambda function.
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 新函数还需要S3权限从另一个存储桶复制文件（*对象*）。为了简单起见，只需将`AmazonS3FullAccess`策略添加到与函数关联的**执行IAM角色**。您可以通过点击Lambda函数的**权限**选项卡来找到IAM角色。
- en: '**Trigger data ingestion**: Now, download the sample data files from the following
    link: [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip)'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**触发数据摄取**：现在，从以下链接下载示例数据文件：[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-Handbook/tree/main/Chapter04/Archive.zip)'
- en: Then, save the file to your local machine. Extract the archived files. There
    should be two files (`customer_data.csv` and `churn_list.csv`).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将文件保存到您的本地计算机。解压缩归档文件。应该有两个文件（`customer_data.csv`和`churn_list.csv`）。
- en: You can now trigger the data ingestion process by uploading the `customer_detail.csv`
    and `churn_list.csv` files to the `customer-data-source` bucket and verify the
    process completion by checking the `MLSA-DataLake-<your initials>/bank_customer_db`
    folder for the two files.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以通过将`customer_detail.csv`和`churn_list.csv`文件上传到`customer-data-source`存储桶来触发数据摄取过程，并通过检查`MLSA-DataLake-<your
    initials>/bank_customer_db`文件夹中的两个文件来验证过程完成。
- en: You have now successfully created an AWS Lambda-based data ingestion pipeline
    to automatically move data from a source S3 bucket to a target S3 bucket. With
    this simple ingestion pipeline created and data moved, we are now ready to implement
    components to support the discovery of these data files. Next, let’s create an
    AWS Glue Data Catalog using the Glue crawler.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已成功创建了一个基于AWS Lambda的数据摄取管道，用于自动将数据从源S3存储桶移动到目标S3存储桶。通过创建此简单的摄取管道并移动数据，我们现在准备实现支持这些数据文件发现的组件。接下来，让我们使用Glue爬虫创建AWS
    Glue数据目录。
- en: Creating a Glue Data Catalog
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建Glue数据目录
- en: To allow discovery and querying of the data in the `bank_customer_db` database,
    we need to create a data catalog. As discussed earlier, Glue Data Catalog is a
    managed data catalog on AWS. It comes with a utility called an AWS Glue crawler
    that can help discover data and populate the catalog.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 要允许发现和查询`bank_customer_db`数据库中的数据，我们需要创建一个数据目录。如前所述，Glue数据目录是AWS上的托管数据目录。它附带一个名为AWS
    Glue爬虫的实用工具，可以帮助发现数据并填充目录。
- en: 'Here, we will use an AWS Glue crawler to crawl the files in the `bank_customer_db`
    S3 folder and generate the catalog:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用AWS Glue爬虫爬取`bank_customer_db` S3文件夹中的文件并生成目录：
- en: '**Grant permission for Glue**:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**授权 Glue 权限**：'
- en: 'First, let’s grant permission for AWS Glue to access the `bank_customer_db`
    database. We will create a new IAM role for the Glue service to assume on your
    behalf. To do this, create a new IAM service role called `AWSGlueServiceRole_data_lake`,
    and attach the `AWSGlueServiceRole` and `AmazonS3FullAccess` IAM-managed policies
    to it. Make sure you select **Glue** as the service when you create the role.
    If you are not familiar with how to create a role and attach a policy, follow
    the instructions at the following link: [https://docs.aws.amazon.com/IAM/latest/UserGuide](https://docs.aws.amazon.com/IAM/latest/UserGuide)'
  id: totrans-237
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们授权 AWS Glue 访问 `bank_customer_db` 数据库。我们将为您创建一个新的 IAM 服务角色，名为 `AWSGlueServiceRole_data_lake`，并将其附加到
    `AWSGlueServiceRole` 和 `AmazonS3FullAccess` IAM 管理策略上。确保在创建角色时选择**Glue**作为服务。如果您不熟悉如何创建角色和附加策略，请遵循以下链接中的说明：[https://docs.aws.amazon.com/IAM/latest/UserGuide](https://docs.aws.amazon.com/IAM/latest/UserGuide)
- en: After the role is created, click on **Data lake permission** in the left pane
    of the Lake Formation management console and then click the **Grant** button in
    the right pane.
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 角色创建后，点击 Lake Formation 管理控制台左侧面板中的**数据湖权限**，然后在右侧面板中点击**授权**按钮。
- en: On the next screen, select `AWSGlueServiceRole_data_lake` for **IAM users and
    role**and `bank_customer_db` under **Named data catalog resources**, choose **Super**
    for both **Database permissions** and **Grantable permissions**, and finally click
    on **Grant**. The **Super** permission allows the service role to have access
    to create databases and grant permission as part of the automation. `AWSGlueServiceRole_data_lake`
    will be used later to configure the Glue crawler job.
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在下一屏，为**IAM 用户和角色**选择 `AWSGlueServiceRole_data_lake`，在**命名数据目录资源**下选择**bank_customer_db**，为**数据库权限**和**可授权权限**都选择**超级**，最后点击**授权**。**超级**权限允许服务角色访问创建数据库和作为自动化的一部分授权权限。`AWSGlueServiceRole_data_lake`
    将用于稍后配置 Glue 爬虫作业。
- en: '**Configure the Glue crawler job**:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**配置 Glue 爬虫作业**：'
- en: Launch the Glue crawler by clicking on the **Crawler** link in the Lake Formation
    management console. A new browser tab for Glue will open. Click on the **Create
    Crawler** button to get started. Enter `bank_customer_db_crawler` as the name
    of the crawler. Click on the **Add a data source** button, select **S3**, and
    enter `s3://MLSA-DataLake-<your initials>/bank_customer_db/churn_list/` for the
    **include path** field.
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过点击 Lake Formation 管理控制台中的**爬虫**链接来启动 Glue 爬虫。一个新的 Glue 浏览器标签页将打开。点击**创建爬虫**按钮开始。将爬虫的名称输入为
    `bank_customer_db_crawler`。点击**添加数据源**按钮，选择**S3**，并在**包含路径**字段中输入 `s3://MLSA-DataLake-<你的首字母>/bank_customer_db/churn_list/`。
- en: Click on the **Add another data source** button again. This time, enter `s3://MLSA-DataLake-<your
    initials>/bank_customer_db/customer_data/`.
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次点击**添加另一个数据源**按钮。这次，输入 `s3://MLSA-DataLake-<你的首字母>/bank_customer_db/customer_data/`。
- en: 'On the next screen, **Configure security settings**, select `AWSGlueServiceRole_data_lake`
    for the existing IAM role, which you used earlier:'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一屏，**配置安全设置**，为之前使用的现有 IAM 角色**选择 `AWSGlueServiceRole_data_lake`**：
- en: On the next **Set output and scheduling** screen, select `bank_customer_db`
    as the target database, and choose **on demand** as the frequency for the crawler
    schedule.
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个**设置输出和调度**屏幕上，将目标数据库选择为**bank_customer_db**，并将爬虫调度的频率选择为**按需**。
- en: On the next **Review and create** screen, select **Finish** on the final screen
    to complete the setup.
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个**审查和创建**屏幕上，在最终屏幕上选择**完成**以完成设置。
- en: On the **Crawler** screen, select the `bank_customer_db_crawler` job you just
    created, click on **Run crawler**, and wait for the status to say **Ready**.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**爬虫**屏幕上，选择您刚刚创建的 `bank_customer_db_crawler` 作业，点击**运行爬虫**，等待状态显示为**就绪**。
- en: Navigate back to the Lake Formation management console and click on the **Tables**
    link. You will now see two new tables created (`churn_list` and `customer_data`).
  id: totrans-247
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回 Lake Formation 管理控制台并点击**表**链接。您现在将看到创建了两个新表（`churn_list` 和 `customer_data`）。
- en: You have now successfully configured an AWS Glue crawler that automatically
    discovers table schemas from data files and creates data catalogs for the new
    data.
  id: totrans-248
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您现在已成功配置了一个 AWS Glue 爬虫，该爬虫可自动从数据文件中发现表模式并为新数据创建数据目录。
- en: You have successfully created the Glue Data Catalog for the newly ingested data.
    With that, we now have the proper component to support data discovery and query.
    Next, we will use Lake Formation and Athena to discover and query the data in
    the data lake.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功创建了用于新导入数据的 Glue 数据目录。有了这个，我们现在有了支持数据发现和查询的正确组件。接下来，我们将使用 Lake Formation
    和 Athena 来发现和查询数据湖中的数据。
- en: Discovering and querying data in the data lake
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在数据湖中发现和查询数据
- en: To facilitate the data discovery and data understanding phase of the ML workflow,
    it is essential to incorporate data discovery and data query capabilities within
    the data lake.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于机器学习工作流程中的数据发现和数据理解阶段，在数据湖中集成数据发现和数据查询功能是至关重要的。
- en: 'By default, Lake Formation already provides a list of tags, such as data type
    classification (for example, CSV), for searching tables in the database. Let’s
    add a few more tags for each table to make it more discoverable:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Lake Formation 已经提供了一系列标签列表，例如数据类型分类（例如，CSV），用于在数据库中搜索表。让我们为每个表添加更多标签，使其更容易被发现：
- en: Grant permission to edit the database tables by granting your current user ID
    **Super** permission for both the `customer_data` and `churn_list` tables.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过授予您的当前用户 ID 对 `customer_data` 和 `churn_list` 表的 **Super** 权限来编辑数据库表。
- en: 'Let’s add some metadata to the table fields. Select the `customer_data` table,
    click on **Edit Schema**, select the `creditscore` field, click on **Edit** and
    **Add** to add a column property, and enter the following, where `description`
    is the key and the actual text is the value:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在表字段中添加一些元数据。选择 `customer_data` 表，点击 **编辑模式**，选择 `creditscore` 字段，点击 **编辑**
    和 **添加** 以添加列属性，并输入以下内容，其中 `description` 是键，实际文本是值：
- en: '[PRE2]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Follow the same previous steps and add the following column property for the
    `exited` field in the `churn_list` table:'
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照相同的步骤，为 `churn_list` 表中的 `exited` 字段添加以下列属性：
- en: '[PRE3]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to do some searches using metadata inside the Lake Formation
    management console. Try typing the following words separately in the text box
    for **Find table by properties** to search for tables and see what’s returned:'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以准备在 Lake Formation 管理控制台中利用元数据进行一些搜索了。尝试在“查找表属性”文本框中单独输入以下单词来搜索表，并查看返回结果：
- en: '`FICO`'
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FICO`'
- en: '`csv`'
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`csv`'
- en: '`churn flag`'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`churn flag`'
- en: '`creditscore`'
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`creditscore`'
- en: '`customerid`'
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`customerid`'
- en: 'Now that you have found the table you are looking for, let’s query the table
    and see the actual data to learn how to query the data interactively, which is
    an important task performed by data scientists for data exploration and understanding.
    Select the table you want to query and click on the **View data** button in the
    **Actions** drop-down menu. This should bring you to the **Amazon Athena** screen.
    You should see a **Query** tab already created, and the query already executed.
    The results are displayed at the bottom of the screen. If you get a warning message
    stating that you need to provide an output location, select the **Settings** tab,
    and then click on the **Manage** button to provide an S3 location as the output
    location. You can run any other SQL query to explore the data further, such as
    joining the `customer_data` and `churn_list` tables with the `customerid` field:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经找到了所需的表，让我们查询该表并查看实际数据，以学习如何交互式地查询数据，这是数据科学家在数据探索和理解过程中执行的重要任务。选择您想要查询的表，然后在
    **操作** 下拉菜单中点击 **查看数据** 按钮。这应该会带您到 **Amazon Athena** 屏幕。您应该看到一个已经创建的 **查询** 选项卡，并且查询已经执行。结果显示在屏幕底部。如果您收到一个警告消息，表明您需要提供一个输出位置，请选择
    **设置** 选项卡，然后点击 **管理** 按钮以提供 S3 位置作为输出位置。您可以运行任何其他 SQL 查询来进一步探索数据，例如使用 `customerid`
    字段将 `customer_data` 和 `churn_list` 表连接起来：
- en: '[PRE4]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You have now learned how to discover the data in Lake Formation and run queries
    against the data in a Lake Formation database and tables. Next, let’s run a data
    processing job using the Amazon Glue ETL service to make the data ready for ML
    tasks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经学会了如何在 Lake Formation 中发现数据，并在 Lake Formation 数据库和表中运行查询。接下来，让我们使用 Amazon
    Glue ETL 服务运行一个数据处理作业，以便为机器学习任务准备数据。
- en: Creating an Amazon Glue ETL job to process data for ML
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建 Amazon Glue ETL 作业以处理机器学习数据
- en: 'The `customer_data` and `churn_list` tables contain features that are useful
    for ML. However, they need to be joined and processed so they can be used to train
    ML models. One option is for the data scientists to download these datasets and
    process them in a Jupyter notebook for model training. Another option is to process
    the data using a separate processing engine so that the data scientists can work
    with the processed data directly. Here, we will set up an AWS Glue job to process
    the data in the `customer_data` and `churn_list` tables and transform them into
    new ML features that are ready for model training directly:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`customer_data`和`churn_list`表包含对机器学习有用的特征。但是，它们需要被连接和处理，以便可以用于训练机器学习模型。一个选项是数据科学家下载这些数据集并在Jupyter笔记本中处理它们以进行模型训练。另一个选项是使用单独的处理引擎处理数据，以便数据科学家可以直接处理处理后的数据。在这里，我们将设置一个AWS
    Glue作业来处理`customer_data`和`churn_list`表中的数据，并将它们转换成可以直接用于模型训练的新机器学习特征：'
- en: First, create a new S3 bucket called `MLSA-DataLake-Serving-<your initials>`.
    We will use this bucket to store the output training datasets from the Glue job.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，创建一个新的名为`MLSA-DataLake-Serving-<your initials>`的S3存储桶。我们将使用此存储桶来存储Glue作业的输出训练数据集。
- en: Using the Lake Formation console, grant `AWSGlueService_Role` **Super** access
    to the `customer_data` and `churn_list` tables. We will use this role to run the
    Glue job.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Lake Formation控制台，授予`AWSGlueService_Role`对`customer_data`和`churn_list`表的**超级**访问权限。我们将使用此角色来运行Glue作业。
- en: To start creating the Glue job, go to the Glue console and click on the **ETL
    Jobs** link on the Glue console. Click on **Script editor** and then click on
    the **Create script** button.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要开始创建Glue作业，请转到Glue控制台，并点击Glue控制台上的**ETL作业**链接。点击**脚本编辑器**，然后点击**创建脚本**按钮。
- en: On the script editor screen, change the job name from **Untitled job** to `customer_churn_process`
    for easy tracking.
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在脚本编辑器屏幕上，将作业名称从**未命名作业**更改为`customer_churn_process`以便于跟踪。
- en: On the **Job details** tab, select `AWSGlueService_Role` as the IAM role. Add
    a new `Job` parameter called `target_bucket` under `Advanced Properties` and enter
    the value of your target bucket for the output files.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**作业详情**选项卡上，选择`AWSGlueService_Role`作为IAM角色。在**高级属性**下添加一个新的`Job`参数`target_bucket`，并输入目标存储桶的值以输出文件。
- en: On the **Script tab** screen, copy the following code blocks to the code section.
    Make sure to replace `default_bucket` with your own bucket in the code. The following
    code block first joins the `churn_list` and `customer_data` tables using the `customerid`
    column as the key, then transforms the `gender` and `geo` columns with an index,
    creates a new DataFrame with only the relevant columns, and finally saves the
    output file to an S3 location using the date and generated version ID as partitions.
    The code uses default values for the target bucket and prefix variables and generates
    a date partition and version partition for the S3 location. The job can also accept
    input arguments for these parameters.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在**脚本标签**屏幕上，将以下代码块复制到代码部分。确保在代码中将`default_bucket`替换为您自己的存储桶。以下代码块首先使用`customerid`列作为键将`churn_list`和`customer_data`表连接起来，然后使用索引转换`gender`和`geo`列，创建一个新的只包含相关列的DataFrame，并最终使用日期和生成的版本ID作为分区将输出文件保存到S3位置。代码使用默认的目标存储桶和前缀变量，并为S3位置生成日期分区和版本分区。作业还可以接受这些参数的输入参数。
- en: 'The following code block sets up default configurations, such as `SparkContext`
    and a default bucket:'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码块设置了默认配置，例如`SparkContext`和默认存储桶：
- en: '[PRE5]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following code joins the `customer_data` and `churn_list` tables into a
    single table using the `customerid` column as the key:'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码使用`customerid`列作为键将`customer_data`和`churn_list`表合并为一个单一表：
- en: '[PRE6]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The following code block transforms several data columns from string labels
    to label indices and writes the final file to an output location in S3:'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下代码块将几个数据列从字符串标签转换为标签索引，并将最终文件写入S3的输出位置：
- en: '[PRE7]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Click on **Save** and then the **Run job** button to run the job. Check the
    job running status by clicking on the **ETL jobs** link in the Glue console, and
    then click on **Job run monitoring**.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击**保存**然后点击**运行作业**按钮来运行作业。通过在Glue控制台中点击**ETL作业**链接，检查作业运行状态，然后点击**作业运行监控**。
- en: After the job completes, check the `s3://MLSA-DataLake-Serving-<your initials>/ml-customer-churn/<date>/<guid>/`
    location in S3 and see whether a new CSV file was generated. Open the file and
    see whether you see the new processed dataset in the file.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业完成后，检查 S3 中的 `s3://MLSA-DataLake-Serving-<your initials>/ml-customer-churn/<date>/<guid>/`
    位置，看看是否生成了新的 CSV 文件。打开文件，查看是否在文件中看到了新的处理后的数据集。
- en: You have now successfully built an AWS Glue job for data processing and feature
    engineering for ML. With this, you can automate data processing and feature engineering,
    which is critical to achieve reproducibility and governance. Try creating a crawler
    to crawl the newly processed data in the `MLSA-DataLake-Serving-<your initials>`
    bucket to make it available in the Glue catalog and run some queries against it.
    You should see a new table created with multiple partitions (for example, `ml-customer-churn`,
    `date`, and `GUID`) for the different training datasets. You can query the data
    by using the `GUID` partition as a query condition.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已成功构建了一个 AWS Glue 作业，用于数据处理和机器学习特征工程。有了这个，您可以自动化数据处理和特征工程，这对于实现可重复性和治理至关重要。尝试创建一个爬虫来爬取
    `MLSA-DataLake-Serving-<your initials>` 存储桶中的新处理数据，使其在 Glue 目录中可用，并对它执行一些查询。您应该会看到一个新表被创建，具有多个分区（例如，`ml-customer-churn`、`date`
    和 `GUID`），用于不同的训练数据集。您可以通过使用 `GUID` 分区作为查询条件来查询数据。
- en: Building a data pipeline using Glue workflows
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Glue 工作流程构建数据管道
- en: 'Next, we will construct a pipeline that executes a data ingestion job, followed
    by the creation of a database catalog for the data. Finally, a data processing
    job will be initiated to generate the training dataset. This pipeline will automate
    the flow of data from the source to the desired format, ensuring seamless and
    efficient data processing for ML model training:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建一个管道，执行数据摄取作业，随后创建数据数据库目录。最后，将启动一个数据处理作业以生成训练数据集。此管道将自动化数据从源到所需格式的流动，确保为机器学习模型训练提供无缝且高效的数据处理：
- en: To start, click on the **Workflows (orchestration)** link in the left pane of
    the Gluemanagement console.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在 Gluemanagement 控制台的左侧面板中点击 **工作流程（编排）** 链接。
- en: Click on **Add workflow** and enter a name for your workflow on the next screen.
    Then, click on the **Create workflow** button.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击 **添加工作流程** 并在下一屏幕上为您的流程输入一个名称。然后，点击 **创建工作流程** 按钮。
- en: Select the workflow you just created and click on **Add trigger**. Select the
    **Add New** tab, and then enter a name for the trigger and select the `on-demand`
    trigger type.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择您刚刚创建的工作流程，并点击 **添加触发器**。选择 **添加新** 选项卡，然后为触发器输入一个名称，并选择 `on-demand` 触发器类型。
- en: On the workflow UI designer, you will see a new **Add Node** icon show up. Click
    on the **Add Node** icon, select the **Crawler** tab, and select `bank_customer_db_crawler`,
    then click on **Add**.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在工作流程 UI 设计器中，您将看到一个新的 **添加节点** 图标出现。点击 **添加节点** 图标，选择 **爬虫** 选项卡，并选择 `bank_customer_db_crawler`，然后点击
    **添加**。
- en: On the workflow UI designer, click on the **Crawler** icon, and you will see
    a new **Add Trigger** icon show up. Click on the **Add Trigger** icon, select
    the **Add new** tab, and select **Start after ANY event** as the trigger logic,
    and then click on **Add**.
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在工作流程 UI 设计器中，点击 **爬虫** 图标，您将看到一个新的 **添加触发器** 图标出现。点击 **添加触发器** 图标，选择 **添加新**
    选项卡，并选择 **在任意事件后启动** 作为触发逻辑，然后点击 **添加**。
- en: On the workflow UI designer, click on the **Add Node** icon, select the **Jobs**
    tab, and select the `customer_churn_process` job.
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在工作流程 UI 设计器中，点击 **添加节点** 图标，选择 **作业** 选项卡，并选择 `customer_churn_process` 作业。
- en: On the workflow UI designer, the final workflow should look like the following
    diagram:![Figure 4.7 – Glue data flow design ](img/B20836_04_08.png)
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在工作流程 UI 设计器中，最终的流程应类似于以下图表：![图 4.7 – Glue 数据流设计](img/B20836_04_08.png)
- en: 'Figure 4.8: Glue data flow design'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.8：Glue 数据流设计
- en: Now, you are ready to run the workflow. Select the workflow and select **Run**
    from the **Actions** dropdown. You can monitor the running status by selecting
    the **Run ID** and clicking on **View run details**. You should see something
    similar to the following screenshot:![Figure 4.8 – Glue workflow execution ](img/Image2513.jpg)
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，您已准备好运行工作流程。选择工作流程，并从 **操作** 下拉菜单中选择 **运行**。您可以通过选择 **运行 ID** 并点击 **查看运行详情**
    来监控运行状态。您应该会看到类似于以下截图的内容：![图 4.8 – Glue 工作流程执行](img/Image2513.jpg)
- en: 'Figure 4.9: Glue workflow execution'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4.9：Glue 工作流程执行
- en: Try deleting the `customer_data` and `churn_list` tables and re-run the workflow.
    See whether the new tables are created again. Check the `s3://MLSA-DataLake-Serving-<your
    initials>/ml-customer-churn/<date>/` S3 location to verify a new folder is created
    with a new dataset.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试删除`customer_data`和`churn_list`表，并重新运行工作流程。查看新表是否再次创建。检查`s3://MLSA-DataLake-Serving-<your
    initials>/ml-customer-churn/<date>/` S3位置，以验证是否创建了一个包含新数据集的新文件夹。
- en: Congratulations! You have completed the hands-on lab and learned how to build
    a simple data lake and its supporting components to allow data cataloging, data
    querying, and data processing. You should now be able to apply some of the skills
    learned to real-world design and the implementation of a data management platform
    on AWS to support the ML development lifecycle.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了动手实验室，并学习了如何构建一个简单的数据湖及其支持组件，以允许数据编目、数据查询和数据处理。你现在应该能够将所学的一些技能应用到现实世界的设计和AWS上数据管理平台的实施中，以支持机器学习开发生命周期。
- en: Summary
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into the considerations for managing data in the
    context of ML and explored the architecture of an enterprise data management platform
    for ML. We examined the intersection of data management with the ML lifecycle
    and learned how to design a data lake architecture on AWS. To apply these concepts,
    we went through the process of building a data lake using AWS Lake Formation.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了在机器学习背景下管理数据的考虑因素，并探讨了企业数据管理平台在机器学习中的架构。我们研究了数据管理与机器学习生命周期的交集，并学习了如何在AWS上设计数据湖架构。为了应用这些概念，我们通过使用AWS
    Lake Formation构建数据湖的过程进行了实践。
- en: Through hands-on experience, we practiced data ingestion, processing, and cataloging
    for data discovery, querying, and ML tasks. Additionally, we gained proficiency
    in using AWS data management tools such as AWS Glue, AWS Lambda, and Amazon Athena.
    In the next chapter, our focus will shift to the architecture and technologies
    involved in constructing data science environments using open-source tools.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 通过动手实践，我们练习了数据摄取、处理和编目，以进行数据发现、查询和机器学习任务。此外，我们还熟练掌握了使用AWS数据管理工具，如AWS Glue、AWS
    Lambda和Amazon Athena。在下一章中，我们的重点将转向使用开源工具构建数据科学环境所涉及的架构和技术。
- en: Leave a review!
  id: totrans-301
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢这本书吗？通过留下亚马逊评论来帮助像你这样的读者。扫描下面的二维码，获取你选择的免费电子书。
- en: '![](img/Review_Copy.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Review_Copy.png)'
- en: '**Limited Offer*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '**限时优惠*'
