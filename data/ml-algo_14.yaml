- en: A Brief Introduction to Deep Learning and TensorFlow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度学习和TensorFlow简介
- en: In this chapter, we're going to briefly introduce deep learning with some examples
    based on TensorFlow. This topic is quite complex and needs dedicated books; however,
    our goal is to allow the reader to understand some basic concepts that can be
    useful before starting a complete course. In the first section, we're presenting
    the structure of artificial neural networks and how they can be transformed in
    a complex computational graph with several different layers. In the second one,
    instead, we're going to introduce the basic concepts concerning TensorFlow and
    we'll show some examples based on algorithms already discussed in previous chapters.
    In the last section, we briefly present Keras, a high-level deep learning framework
    and we build an example of image classification using a convolutional neural network.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过基于TensorFlow的一些示例简要介绍深度学习。这个主题相当复杂，需要专门的书籍；然而，我们的目标是让读者理解一些在开始完整课程之前可能有用的基本概念。在第一部分，我们介绍了人工神经网络的架构以及它们如何通过几个不同的层转换成复杂的计算图。在第二部分，我们将介绍与TensorFlow有关的基本概念，并展示一些基于之前章节中讨论过的算法的示例。在最后一部分，我们简要介绍了Keras，这是一个高级深度学习框架，并构建了一个使用卷积神经网络的图像分类示例。
- en: Deep learning at a glance
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述深度学习
- en: Deep learning has become very famous in the last few decades, thanks to hundreds
    of applications that are changing the way we interact with many electronic (and
    non-electronic) systems. Speech, text, and image recognition; autonomous vehicles;
    and intelligent bots (just to name a few) are common applications normally based
    on deep learning models and have outperformed any previous classical approach.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几十年中，深度学习因其数百个改变我们与许多电子（和非电子）系统交互方式的应用而变得非常著名。语音、文本和图像识别；自动驾驶汽车；以及智能机器人（仅举几例）都是通常基于深度学习模型的应用，并且优于任何先前的经典方法。
- en: To better understand what a deep architecture is (considering that this is only
    a brief introduction), we need to step back and talk about standard artificial
    neural networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解深度架构是什么（考虑到这只是一个简要介绍），我们需要回顾并讨论标准的人工神经网络。
- en: Artificial neural networks
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: 'An **artificial neural network** (**ANN**) or simply neural network is a directed
    structure that connects an input layer with an output one. Normally, all operations
    are differentiable and the overall vectorial function can be easily written as:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）或简称为神经网络，是一种将输入层与输出层连接的定向结构。通常，所有操作都是可微分的，整体向量函数可以很容易地写成：'
- en: '![](img/e7c98e70-61e6-42ad-b597-10f44309e086.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7c98e70-61e6-42ad-b597-10f44309e086.png)'
- en: 'Here:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里：
- en: '![](img/f927c9b8-7a07-41c1-82c4-c9c3f783da73.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f927c9b8-7a07-41c1-82c4-c9c3f783da73.png)'
- en: 'The adjective "neural" comes from two important elements: the internal structure
    of a basic computational unit and the interconnections among them. Let''s start
    with the former. In the following figure, there''s a schematic representation
    of an artificial neuron:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 形容词“神经”来源于两个重要元素：基本计算单元的内部结构和它们之间的相互连接。让我们先从前者开始。在下面的图中，有一个人工神经元的示意图：
- en: '![](img/36f956de-358a-4cd5-97e8-d62ac8b2a1af.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36f956de-358a-4cd5-97e8-d62ac8b2a1af.png)'
- en: 'A neuron core is connected with *n* input channels, each of them characterized
    by a synaptic weight *w[i]*. The input is split into its components and they are
    multiplied by the corresponding weight and summed. An optional bias can be added
    to this sum (it works like another weight connected to a unitary input). The resulting
    sum is filtered by an activation function *f[a]* (for example a sigmoid, if you
    recall how a logistic regression works) and the output is therefore produced.
    In [Chapter 5](9d0c9c1c-e5b3-46a1-b331-c9689a687edf.xhtml), *Logistic Regression*,
    we also discussed perceptrons (the first artificial neural networks), which correspond
    exactly to this architecture with a binary-step activation function. On the other
    hand, even a logistic regression can be represented as a single neuron neural
    network, where *f[a](x)* is a sigmoid. The main problem with this architecture
    is that it''s intrinsically linear because the output is always a function of
    the dot product between the input vector and the weight one. You already know
    all the limitations that such a system has; therefore it''s necessary to step
    forward and create the first **Multi-layer Perceptron** (**MLP**). In the following
    figure, there''s a schematic representation of an MLP with an n-dimensional input,
    *p* hidden neurons, and a *k*-dimensional output:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元核心与*n*个输入通道相连，每个通道由一个突触权重*w[i]*表征。输入被分成其组成部分，然后它们与相应的权重相乘并求和。可以添加一个可选的偏差到这个和（它就像另一个连接到单位输入的权重）。这个和通过一个激活函数*f[a]*（例如，如果你记得逻辑回归是如何工作的，可以使用Sigmoid函数）过滤，因此输出因此产生。在[第5章](9d0c9c1c-e5b3-46a1-b331-c9689a687edf.xhtml)“逻辑回归”中，我们也讨论了感知器（第一个人工神经网络），它正好对应这种具有二元步激活函数的架构。另一方面，逻辑回归也可以表示为一个单神经元神经网络，其中*f[a](x)*是Sigmoid函数。这个架构的主要问题是它本质上是线性的，因为输出总是输入向量和权重向量的点积的函数。你已经知道这样一个系统所有的局限性；因此，有必要向前迈进并创建第一个**多层感知器**（**MLP**）。在下面的图中，有一个具有n维输入、*p*个隐藏神经元和*k*维输出的MLP的示意图：
- en: '![](img/535b5780-b621-46d8-b69f-e5175b4e8b4f.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/535b5780-b621-46d8-b69f-e5175b4e8b4f.png)'
- en: 'There are three layers (even though the number can be larger): the input layer,
    which receives the input vectors; a hidden layer; and the output one, which is
    responsible for producing the output. As you can see, every neuron is connected
    to all the neurons belonging the next layer and now we have two weight matrices, *W
    = (w[ij])*and *H = (h[jk])*, using the convention that the first index is referred
    to the previous layer and the second to the following one.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 有三层（尽管数量可以更大）：输入层，它接收输入向量；一个隐藏层；以及输出层，它负责产生输出。正如你所见，每个神经元都与下一层的所有神经元相连，现在我们有两个权重矩阵，*W
    = (w[ij])*和*H = (h[jk])*，按照惯例，第一个索引指的是前一层，第二个索引指的是下一层。
- en: 'Therefore, the net input to each hidden neuron and the corresponding output
    is:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个隐藏神经元的净输入及其相应的输出是：
- en: '![](img/b2ac7fde-a8f3-4379-a6d2-16798579d9bb.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b2ac7fde-a8f3-4379-a6d2-16798579d9bb.png)'
- en: 'In the same way, we can compute the network output:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们可以计算网络输出：
- en: '![](img/5cc3888c-04f9-4496-a850-bf4db9162ca7.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5cc3888c-04f9-4496-a850-bf4db9162ca7.png)'
- en: As you can see, the network has become highly non-linear and this feature allows
    us to model complex scenarios that were impossible to manage with linear methods.
    But how can we determine the values for all synaptic weights and biases? The most
    famous algorithm is called **back-propagation** and it works in a very simple
    way (the only important assumption is that both *f[a](x)* must be differentiable).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，网络已经变得高度非线性，这一特性使我们能够模拟那些用线性方法无法管理的复杂场景。但是，我们如何确定所有突触权重和偏差的值呢？最著名的算法被称为**反向传播**，它的工作方式非常简单（唯一重要的假设是两个
    *f[a](x)* 都必须是可微的）。
- en: 'First of all, we need to define an error (loss) function; for many classification
    tasks, it can be the total squared error:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一个误差（损失）函数；对于许多分类任务，它可以是总平方误差：
- en: '![](img/f8837b97-25d9-4e7d-8fa8-e72caba6d0ae.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8837b97-25d9-4e7d-8fa8-e72caba6d0ae.png)'
- en: 'Here we have assumed to have *N* input samples. Expanding it, we obtain:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们假设有*N*个输入样本。展开它，我们得到：
- en: '![](img/06d71310-d0c2-4823-be4a-728e2ddbcf32.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/06d71310-d0c2-4823-be4a-728e2ddbcf32.png)'
- en: 'This function depends on all variables (weights and biases), but we can start
    from the bottom and consider first only *h*[*jk* ](for simplicity I''m not considering
    the biases as normal weights); therefore we can compute the gradients and update
    the weights:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数依赖于所有变量（权重和偏差），但我们可以从底部开始，首先只考虑*h*[*jk* ](为了简单起见，我没有将偏差视为正常权重）；因此，我们可以计算梯度并更新权重：
- en: '**[![](img/ac32ae69-659c-462f-abd5-92a88181bd25.png) ]**'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**[![](img/ac32ae69-659c-462f-abd5-92a88181bd25.png) ]**'
- en: 'In the same way, we can derive the gradient with respect to *w[ij]*:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们可以推导出与*w[ij]*相关的梯度：
- en: '![](img/4d120e71-d46d-4944-ae88-f7eb430334c6.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4d120e71-d46d-4944-ae88-f7eb430334c6.png)'
- en: 'As you can see, the term alpha (which is proportional to the error delta) is
    back-propagated from the output layer to the hidden one. If there are many hidden
    layers, this procedure should be repeated recursively until the first layer. The
    algorithm adopts the gradient descent method; therefore it updates the weights
    iteratively until convergence:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，术语alpha（与误差delta成正比）从输出层反向传播到隐藏层。如果有许多隐藏层，则此过程应递归重复，直到第一层。该算法采用梯度下降法；因此，它迭代更新权重，直到收敛：
- en: '![](img/5173acce-f8f1-41f0-8077-137731ad6016.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5173acce-f8f1-41f0-8077-137731ad6016.png)'
- en: Here, the parameter `eta` (Greek letter in the formula) is the learning rate.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，参数`eta`（公式中的希腊字母）是学习率。
- en: In many real problems, the stochastic gradient descent method is adopted (read
    [https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent), for
    further information), which works with batches of input samples, instead of considering
    the entire dataset. Moreover, many optimizations can be employed to speed up the
    convergence, but they are beyond the scope of this book. In Goodfellow I., Bengio
    Y., Courville A., *Deep Learning*, MIT Press*,* the reader can find all the details
    about the majority of them. For our purposes, it's important to know that we can
    build a complex network and, after defining a global loss function, optimize all
    the weights with a standard procedure. In the section dedicated to TensorFlow,
    we're going to show an example of MLP, but we're not implementing the learning
    algorithm because, luckily, all optimizers have already been built and can be
    applied to every architecture.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际问题中，采用的是随机梯度下降法（阅读[https://en.wikipedia.org/wiki/Stochastic_gradient_descent](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)，获取更多信息），该方法使用输入样本的批次，而不是考虑整个数据集。此外，可以采用许多优化方法来加速收敛，但这些内容超出了本书的范围。在Goodfellow
    I.，Bengio Y.，Courville A.的《深度学习》，MIT Press*中，读者可以找到大多数这些优化的详细信息。就我们的目的而言，重要的是要知道我们可以构建一个复杂的网络，并在定义全局损失函数后，使用标准程序优化所有权重。在TensorFlow的章节中，我们将展示一个MLP的示例，但我们不会实现学习算法，因为幸运的是，所有优化器都已经构建好，并且可以应用于任何架构。
- en: Deep architectures
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度架构
- en: MLPs are powerful, but their expressiveness is limited by the number and the
    nature of the layers. Deep learning architectures, on the other side, are based
    on a sequence of heterogeneous layers which perform different operations organized
    in a computational graph. The output of a layer, correctly reshaped, is fed into
    the following one, until the output, which is normally associated with a loss
    function to optimize. The most interesting applications have been possible thanks
    to this stacking strategy, where the number of variable elements (weights and
    biases) can easily reach over 10 million; therefore, the ability to capture small
    details and generalize them exceeds any expectations. In the following section,
    I'm going to introduce briefly the most important layer types.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MLPs功能强大，但它们的表达能力受层数量和性质的限制。另一方面，深度学习架构基于一系列异构层，这些层在计算图中执行不同的操作。正确重塑的层输出被送入下一层，直到输出，这通常与一个用于优化的损失函数相关。最有趣的应用得益于这种堆叠策略，其中可变元素（权重和偏差）的数量可以轻松超过1000万；因此，捕捉细节和推广的能力超出了任何预期。在下一节中，我将简要介绍最重要的层类型。
- en: Fully connected layers
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 全连接层
- en: 'A fully connected (sometimes called dense) layer is made up of n neurons and
    each of them receives all the output values coming from the previous layer (like
    the hidden layer in a MLP). It can be characterized by a weight matrix, a bias
    vector, and an activation function:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个全连接层（有时称为密集层）由n个神经元组成，每个神经元接收来自前一层的所有输出值（如MLP中的隐藏层）。它可以由一个权重矩阵、一个偏差向量和一个激活函数来表征：
- en: '![](img/647ecc9f-9608-4cee-bfd3-5b1c2f8ec657.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/647ecc9f-9608-4cee-bfd3-5b1c2f8ec657.png)'
- en: 'They are normally used as intermediate or output layers, in particular when
    it''s necessary to represent a probability distribution. For example, a deep architecture
    could be employed for an image classification with *m* output classes. In this
    case, the *softmax* activation function allows having an output vector where each
    element is the probability of a class (and the sum of all outputs is always normalized
    to 1.0). In this case, the argument is considered as a **logit** or the logarithm
    of a probability:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 它们通常用作中间层或输出层，特别是在需要表示概率分布时。例如，可以使用深度架构进行具有*m*个输出类别的图像分类。在这种情况下，*softmax*激活函数允许有一个输出向量，其中每个元素是某个类别的概率（并且所有输出的总和总是归一化到1.0）。在这种情况下，参数被视为**logit**或概率的对数：
- en: '![](img/7898100d-7f45-46c5-8c3e-aa550424f86b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7898100d-7f45-46c5-8c3e-aa550424f86b.png)'
- en: '*W[i]* is the i-th row of *W***. **The probability of a class *y**[i]*** is
    obtained by applying the *softmax* function to each *logit*:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*W[i]*是*W***的i行。**类别*y**[i]***的概率是通过将*softmax*函数应用于每个*logit*来获得的：'
- en: '![](img/76e5ef7e-e437-44d0-8258-7215cffb1605.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/76e5ef7e-e437-44d0-8258-7215cffb1605.png)'
- en: This type of output can easily be trained using a cross-entropy loss function,
    as already discussed for logistic regression.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的输出可以很容易地使用交叉熵损失函数进行训练，正如已经讨论过的逻辑回归。
- en: Convolutional layers
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'Convolutional layers are normally applied to bidimensional inputs (even though
    they can be used for vectors and 3D matrices) and they became particularly famous
    thanks to their extraordinary performance in image classification tasks. They
    are based on the discrete convolution of a small kernel *k* with a bidimensional
    input (which can be the output of another convolutional layer):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层通常应用于二维输入（尽管它们也可以用于向量和3D矩阵），并且由于它们在图像分类任务中表现出色，因此变得特别著名。它们基于一个小核*k*与二维输入的离散卷积（可以是另一个卷积层的输出）：
- en: '![](img/3d8eb5e0-ef8d-42c1-bdba-0342cd94c16f.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3d8eb5e0-ef8d-42c1-bdba-0342cd94c16f.png)'
- en: A layer is normally made up of n fixed-size kernels, and their values are considered
    as weights to learn using a back-propagation algorithm. A convolutional architecture,
    in most cases, starts with layers with few larger kernels (for example, 16 (8
    x 8) matrices) and feeds their output to other layers with a higher number of
    smaller kernels (32 (5 x 5), 128 (4 x 4), and 256 (3 x 3)). In this way, the first
    layers should learn to capture more generic features (such as orientation), while
    the following ones will be trained to capture smaller and smaller elements (such
    as the position of eyes, nose, and mouth in a face). The output of the last convolutional
    layer is normally flattened (transformed into a 1D vector) and used as input for
    one or more fully connected layers.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 一层通常由n个固定大小的核组成，它们的值被视为使用反向传播算法进行学习的权重。在大多数情况下，卷积架构从具有较少较大核的层开始（例如，16（8 x 8）矩阵），并将它们的输出馈送到具有更多较小核的更高层（32（5
    x 5），128（4 x 4），和256（3 x 3））。这样，第一层应该学会捕获更通用的特征（例如，方向），而后续的层将被训练以捕获越来越小的元素（例如，脸部眼睛、鼻子和嘴巴的位置）。最后卷积层的输出通常会被展平（转换为1D向量）并用作一个或多个全连接层的输入。
- en: 'In the following figure, there''s a schematic representation of a convolution
    over a picture:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，展示了一个对图片进行卷积的示意图：
- en: '![](img/3f6d3092-71f1-456f-989a-500996f968b2.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3f6d3092-71f1-456f-989a-500996f968b2.png)'
- en: Each square set of 3 x 3 pixels is convoluted with a Laplacian kernel and transformed
    into a single value, which corresponds to the sum of upper, lower, left, and right
    pixels (considering the centre) minus four times the central one. We're going
    to see a complete example using this kernel in the following section.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 每个由3 x 3像素组成的正方形组与拉普拉斯核进行卷积，并转换为单个值，该值对应于上、下、左、右像素（考虑中心）的总和减去中心像素的四倍。我们将在下一节中看到一个使用此核的完整示例。
- en: 'To reduce the complexity when the number of convolutions is very high, one
    or more **pooling layers** can be employed. Their task is to transform each group
    of input points (pixels in an image) into a single value using a predefined strategy.
    The most common pooling layers are:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在卷积数量非常高时减少复杂性，可以采用一个或多个**池化层**。它们的作用是使用预定义的策略将每个输入点组（图像中的像素）转换为单个值。最常见的池化层有：
- en: '**Max pooling**: Every bidimensional group of (*m* x *n*) pixels is transformed
    into a single pixel whose value is the greatest in the group.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大池化**：每个二维的(*m* x *n*)像素组被转换成一个像素，其值是该组中的最大值。'
- en: '**Average pooling**: Every bidimensional group of (*m* x *n*) pixels is transformed
    into a single pixel whose value is the average of the group.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均池化**：每个二维的(*m* x *n*)像素组被转换成一个像素，其值是该组的平均值。'
- en: In this way, the dimensionality of the original matrix can be reduced with a
    loss of information, but that can often be discarded (in particular in the first
    layers where the granularity of the features is coarse). Another important category
    of layers are the **zero-padding** ones. They work by adding null values (0) before
    and after the input (1D) or at the left, right, top and bottom side of 2D input.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，原始矩阵的维度可以减少，但会损失一些信息，但这些信息通常可以丢弃（特别是在第一层，特征粒度较粗）。另一个重要的层类别是**零填充**层。它们通过在输入（1D）之前和之后或2D输入的左侧、右侧、顶部和底部添加空值（0）来工作。
- en: Dropout layers
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout层
- en: A dropout layer is used to prevent overfitting of the network by randomly setting
    a fixed number of input elements to 0\. This layer is adopted during the training
    phase, but it's normally deactivated during test, validation, and production phases.
    Dropout networks can exploit higher learning rates, moving in different directions
    on the loss surface (setting to zero a few random input values in the hidden layers
    is equivalent to training different sub-models) and excluding all the error-surface
    areas that don't lead to a consistent optimization. Dropout is very useful in
    very big models, where it increases the overall performance and reduces the risk
    of freezing some weights and overfitting the model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout层用于通过随机将固定数量的输入元素设置为0来防止网络过拟合。这个层在训练阶段被采用，但在测试、验证和生产阶段通常被禁用。Dropout网络可以利用更高的学习率，在损失表面上移动不同的方向（在隐藏层中设置一些随机输入值为零相当于训练不同的子模型）并排除所有不导致一致优化的错误表面区域。Dropout在非常大的模型中非常有用，它可以提高整体性能并降低某些权重冻结和模型过拟合的风险。
- en: Recurrent neural networks
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: A recurrent layer is made up of particular neurons that present recurrent connections
    so as to bind the state at time *t* to its previous values (in general, only one).
    This category of computational cells is particularly useful when it's necessary
    to capture the temporal dynamics of an input sequence. In many situations, in
    fact, we expect an output value that must be correlated with the history of the
    corresponding inputs. But an MLP, as well as the other models that we've discussed,
    are stateless. Therefore, their output is determined only by the current input.
    RNNs overcome this problem by providing an internal memory which can capture short-term
    and long-term dependencies.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 循环层由特定的神经元组成，这些神经元具有循环连接，以便将时间*t*的状态绑定到其前一个值（通常只有一个）。这类计算单元在需要捕捉输入序列的时间动态时特别有用。实际上，在许多情况下，我们期望的输出值必须与相应输入的历史相关。但是，MLP以及我们讨论的其他模型都是无状态的。因此，它们的输出仅由当前输入决定。RNN通过提供内部记忆来克服这个问题，该记忆可以捕捉短期和长期依赖关系。
- en: The most common cells are **Long Short-Term Memory** (**LSTM**) and **Gated
    Recurrent Unit** (**GRU**) and they can both be trained using a standard back-propagation
    approach. As this is only an introduction, I cannot go deeper (RNN mathematical
    complexity is non-trivial); however, it's useful to remember that whenever a temporal
    dimension must be included in a deep model, RNNs offer stable and powerful support.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的单元是**长短期记忆**（**LSTM**）和**门控循环单元**（**GRU**），它们都可以使用标准的反向传播方法进行训练。由于这只是一个简介，我无法深入探讨（RNN的数学复杂性非同小可）；然而，记住这一点是有用的，即每当需要在深度模型中包含时间维度时，RNNs提供稳定且强大的支持。
- en: A brief introduction to TensorFlow
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow简介
- en: TensorFlow is a computational framework created by Google and has become one
    of the most diffused deep-learning toolkits. It can work with both CPUs and GPUs
    and already implements most of the operations and structures required to build
    and train a complex model. TensorFlow can be installed as a Python package on
    Linux, Mac, and Windows (with or without GPU support); however, I suggest you
    follow the instructions provided on the website (the link can be found in the
    infobox at the end of this chapter) to avoid common mistakes.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 是由 Google 创建的计算框架，已经成为最广泛使用的深度学习工具包之一。它可以与 CPU 和 GPU 一起工作，并且已经实现了构建和训练复杂模型所需的大部分操作和结构。TensorFlow
    可以作为 Python 包安装在 Linux、Mac 和 Windows 上（带或不带 GPU 支持）；然而，我建议您遵循网站上的说明（链接可在本章末尾的信息框中找到），以避免常见的错误。
- en: 'The main concept behind TensorFlow is the computational graph, or a set of
    subsequent operations that transform an input batch into the desired output. In
    the following figure, there''s a schematic representation of a graph:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 的主要概念是计算图，或者是一系列后续操作，这些操作将输入批次转换成所需的输出。在下面的图中，有一个图的示意图：
- en: '![](img/ec9a5336-23d5-4470-acb6-735806039a53.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ec9a5336-23d5-4470-acb6-735806039a53.png)'
- en: Starting from the bottom, we have two input nodes (**a** and **b**), a transpose
    operation (that works on **b**), a matrix multiplication and a mean reduction.
    The **init** block is a separate operation, which is formally part of the graph,
    but it's not directly connected to any other node; therefore it's autonomous (indeed,
    it's a global initializer).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 从底部开始，我们有两个输入节点（**a** 和 **b**），一个转置操作（作用于 **b**），一个矩阵乘法和均值减少。**init** 块是一个独立的操作，它正式是图的一部分，但它没有直接连接到任何其他节点；因此它是自主的（实际上，它是一个全局初始化器）。
- en: 'As this one is only a brief introduction, it''s useful to list all of the most
    important strategic elements needed to work with TensorFlow so as to be able to
    build a few simple examples that can show the enormous potential of this framework:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这只是一个简要的介绍，列出所有与 TensorFlow 一起工作所需的最重要战略元素是有用的，以便能够构建几个简单的示例，以展示这个框架的巨大潜力：
- en: '**Graph**: This represents the computational structure that connects a generic
    input batch with the output tensors through a directed network made of operations.
    It''s defined as a `tf.Graph()` instance and normally used with a Python context
    manager.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图**：这代表通过由操作组成的定向网络连接一个通用输入批次与输出张量的计算结构。它定义为 `tf.Graph()` 实例，通常与 Python 上下文管理器一起使用。'
- en: '**Placeholder**: This is a reference to an external variable, which must be
    explicitly supplied when it''s requested for the output of an operation that uses
    it directly or indirectly. For example, a placeholder can represent a variable
    `x`, which is first transformed into its squared value and then summed to a constant
    value. The output is then `x²+c`, which is materialized by passing a concrete
    value for `x`. It''s defined as a `tf.placeholder()` instance.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**占位符**：这是一个对外部变量的引用，当需要输出使用它直接或间接进行的操作时，必须显式提供。例如，占位符可以代表一个变量 `x`，它首先被转换为其平方值，然后与一个常数值相加。输出结果是
    `x²+c`，通过传递一个具体的 `x` 值来实现。它定义为 `tf.placeholder()` 实例。'
- en: '**Variable**: An internal variable used to store values which are updated by
    the algorithm. For example, a variable can be a vector containing the weights
    of a logistic regression. It''s normally initialized before a training process
    and automatically modified by the built-in optimizers. It''s defined as a `tf.Variable()`
    instance. A variable can also be used to store elements which must not be considered
    during training processes; in this case, it must be declared with the parameter
    `trainable=False`.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变量**：一个内部变量，用于存储由算法更新的值。例如，一个变量可以是一个包含逻辑回归权重的向量。它通常在训练过程之前初始化，并由内置优化器自动修改。它定义为
    `tf.Variable()` 实例。变量也可以用来存储在训练过程中不应考虑的元素；在这种情况下，它必须使用参数 `trainable=False` 声明。'
- en: '**Constant**: A constant value defined as a `tf.constant()` instance.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**常数**：定义为 `tf.constant()` 实例的常数值。'
- en: '**Operation**: A mathematical operation that can work with placeholders, variables,
    and constants. For example, the multiplication of two matrices is an operation
    defined as `tf.matmul(A, B)`. Among all operations, gradient calculation is one
    of the most important. TensorFlow allows determining the gradients starting from
    a determined point in the computational graph, until the origin or another point
    that must be logically before it. We''re going to see an example of this operation.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：一种可以与占位符、变量和常量一起工作的数学操作。例如，两个矩阵的乘法是一个定义为 `tf.matmul(A, B)` 的操作。在所有操作中，梯度计算是最重要的之一。TensorFlow
    允许从计算图中的某个确定点开始确定梯度，直到原点或逻辑上必须在其之前的另一个点。我们将看到这个操作的示例。'
- en: '**Session**: This is a sort of wrapper-interface between TensorFlow and our
    working environment (for example, Python or C++). When the evaluation of a graph
    is needed, this macro-operation will be managed by a session, which must be fed
    with all placeholder values and will produce the required outputs using the requested
    devices. For our purposes, it''s not necessary to go deeper into this concept;
    however, I invite the reader to retrieve further information from the website
    or from one of the resources listed at the end of this chapter. It''s declared
    as an instance of `tf.Session()` or, as we''re going to do, an instance of `tf.InteractiveSession()`.
    This type of session is particularly useful when working with notebooks or shell
    commands, because it places itself automatically as the default one.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**会话**：这是 TensorFlow 和我们的工作环境（例如 Python 或 C++）之间的一种包装接口。当需要评估图时，这个宏操作将由会话管理，会话必须提供所有占位符的值，并使用请求的设备生成所需的输出。对于我们的目的，没有必要深入研究这个概念；然而，我邀请读者从网站或本章末尾列出的资源中获取更多信息。它声明为
    `tf.Session()` 的实例，或者，正如我们即将做的，`tf.InteractiveSession()` 的实例。这种会话在处理笔记本或 shell
    命令时特别有用，因为它会自动将其设置为默认会话。'
- en: '**Device**: A physical computational device, such as a CPU or a GPU. It''s
    declared explicitly through an instance of the class `tf.device()` and used with
    a context manager. When the architecture contains more computational devices,
    it''s possible to split the jobs so as to parallelize many operations. If no device
    is specified, TensorFlow will use the default one (which is the main CPU or a
    suitable GPU if all the necessary components are installed).'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**设备**：一个物理计算设备，例如 CPU 或 GPU。它通过 `tf.device()` 类的实例显式声明，并使用上下文管理器使用。当架构包含多个计算设备时，可以将工作拆分以便并行化许多操作。如果没有指定设备，TensorFlow
    将使用默认设备（这是主要 CPU 或如果安装了所有必要的组件，则是一个合适的 GPU）。'
- en: We can now analyze some simple examples using these concepts.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用这些概念分析一些简单的示例。
- en: Computing gradients
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 计算梯度
- en: 'The option to compute the gradients of all output tensors with respect to any
    connected input or node is one of the most interesting features of TensorFlow,
    because it allows us to create learning algorithms without worrying about the
    complexity of all transformations. In this example, we first define a linear dataset
    representing the function *f(x) = x* in the range (-100, 100):'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 计算所有输出张量相对于任何连接的输入或节点的梯度的选项是 TensorFlow 最有趣的功能之一，因为它允许我们创建学习算法，而无需担心所有转换的复杂性。在这个例子中，我们首先定义一个线性数据集，表示范围在
    (-100, 100) 内的函数 *f(x) = x*：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The corresponding plot is shown in the following figure:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的图表如下所示：
- en: '![](img/f6bdb853-a204-4bfa-87e6-ad1790513c93.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f6bdb853-a204-4bfa-87e6-ad1790513c93.png)'
- en: 'Now we want to use TensorFlow to compute:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想使用 TensorFlow 来计算：
- en: '![](img/e7d221f9-5369-4dd5-b9b2-d1f40ae217d2.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e7d221f9-5369-4dd5-b9b2-d1f40ae217d2.png)'
- en: 'The first step is defining a graph:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义一个图：
- en: '[PRE1]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Within the context of this graph, we can define our input placeholder and other
    operations:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图的上下文中，我们可以定义我们的输入占位符和其他操作：
- en: '[PRE2]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A placeholder is generally defined with a type (first parameter), a shape, and
    an optional name. We've decided to use a `tf.float32` type because this is the
    only type also supported by GPUs. Selecting `shape=(None, 1)` means that it's
    possible to use any bidimensional vectors with the second dimension equal to 1.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 占位符通常使用类型（第一个参数）、形状和可选名称定义。我们决定使用 `tf.float32` 类型，因为这是唯一由 GPU 也支持的类型。选择 `shape=(None,
    1)` 意味着可以使用任何二维向量，其第二维等于 1。
- en: The first operation computes the third power if `Xt` is working on all elements.
    The second operation computes all the gradients of `Y` with respect to the input
    placeholder `Xt`. The last operation will repeat the gradient computation, but
    in this case, it uses `Yd`, which is the output of the first gradient operation.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作计算`Xt`在所有元素上的三次幂。第二个操作计算`Y`相对于输入占位符`Xt`的所有梯度。最后一个操作将重复梯度计算，但在这个情况下，它使用`Yd`，这是第一个梯度操作的输出。
- en: 'We can now pass some concrete data to see the results. The first thing to do
    is create a session connected with this graph:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以传递一些具体的数据来查看结果。首先要做的事情是创建一个与该图连接的会话：
- en: '[PRE3]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'By using this session, we ask any computation using the method `run()`. All
    the input parameters must be supplied through a feed-dictionary, where the key
    is the placeholder, while the value is the actual array:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这个会话，我们要求使用`run()`方法进行的任何计算。所有输入参数都必须通过feed字典提供，其中键是占位符，值是实际的数组：
- en: '[PRE4]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We needed to reshape our array to be compliant with the placeholder. The first
    argument of `run()` is a list of tensors that we want to be computed. In this
    case, we need all operation outputs. The plot of each of them is shown in the
    following figure:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要调整我们的数组以符合占位符。`run()`的第一个参数是我们想要计算的张量列表。在这种情况下，我们需要所有操作输出。每个输出的图如下所示：
- en: '![](img/5184642f-a567-4058-8295-14625558a3dd.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5184642f-a567-4058-8295-14625558a3dd.png)'
- en: 'As expected, they represent respectively: *x³*, *3x²*, and *6x*.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，它们分别代表：*x³*，*3x²*，和*6x*。
- en: Logistic regression
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Now we can try a more complex example implementing a logistic regression algorithm.
    The first step, as usual, is creating a dummy dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试一个更复杂的例子，实现逻辑回归算法。第一步，像往常一样，是创建一个虚拟数据集：
- en: '[PRE5]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The dataset is shown in the following figure:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集在以下图中显示：
- en: '![](img/a090ec57-21aa-417d-8bee-4aeee20ed613.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a090ec57-21aa-417d-8bee-4aeee20ed613.png)'
- en: 'At this point, we can create the graph and all placeholders, variables, and
    operations:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们可以创建图和所有占位符、变量和操作：
- en: '[PRE6]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The placeholder `Xt` is needed for the points, while `Yt` represents the labels.
    At this point, we need to involve a couple of variables: if you remember, they
    store values that are updated by the training algorithm. In this case, we need
    a weight vector `W` (with two elements) and a single `bias`. When a variable is
    declared, its initial value must be provided; we''ve decided to set both to zero
    using the function `tf.zeros()`, which accepts as argument the shape of the desired
    tensor.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 需要占位符`Xt`来表示点，而`Yt`代表标签。在这个阶段，我们需要涉及几个变量：如果你记得，它们存储的是由训练算法更新的值。在这种情况下，我们需要一个权重向量`W`（包含两个元素）和一个单一的`bias`。当声明一个变量时，必须提供其初始值；我们决定使用`tf.zeros()`函数将它们都设置为零，该函数接受作为参数的期望张量的形状。
- en: 'Now we can compute the output (if you don''t remember how logistic regression
    works, please step back to [Chapter 5](9d0c9c1c-e5b3-46a1-b331-c9689a687edf.xhtml),
    *Logistic Regression*) in two steps: first the sigmoid exponent `Ye` and then
    the actual binary output `Yc`, which is obtained by rounding the sigmoid value.
    The training algorithm for a logistic regression minimizes the negative log-likelihood,
    which corresponds to the cross-entropy between the real distribution `Y` and `Yc`.
    It''s easy to implement this loss function; however, the function `tf.log()` is
    numerically unstable (when its value becomes close to zero, it tends to negative
    infinity and yields a `NaN` value); therefore, TensorFlow has implemented a more
    robust function, `tf.nn.sigmoid_cross_entropy_with_logits()`, which computes the
    cross-entropy assuming the output is produced by a sigmoid. It takes two parameters,
    the `logits` (which corresponds to the exponent `Ye`) and the target `labels`,
    that are stored in `Yt`.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以分两步计算输出（如果你不记得逻辑回归是如何工作的，请回顾[第5章](9d0c9c1c-e5b3-46a1-b331-c9689a687edf.xhtml)，*逻辑回归*）：首先计算sigmoid指数`Ye`，然后通过四舍五入sigmoid值得到实际的二进制输出`Yc`。逻辑回归的训练算法最小化负对数似然，这对应于真实分布`Y`和`Yc`之间的交叉熵。实现这个损失函数很容易；然而，函数`tf.log()`在数值上是不稳定的（当其值接近零时，它趋向于负无穷大并产生一个`NaN`值）；因此，TensorFlow实现了一个更健壮的函数，`tf.nn.sigmoid_cross_entropy_with_logits()`，它假设输出是由sigmoid产生的来计算交叉熵。它接受两个参数，`logits`（对应于指数`Ye`）和目标`labels`，它们存储在`Yt`中。
- en: 'Now we can work with one of the most powerful TensorFlow features: the training
    optimizers. After defining a loss function, it will be dependent on placeholders,
    constants, and variables. A training optimizer (such as `tf.train.GradientDescentOptimizer()`),
    through its method `minimize()`, accepts the loss function to optimize. Internally,
    according to every specific algorithm, it will compute the gradients of the loss
    function with respect to all trainable variables and will apply the corresponding
    corrections to the values. The parameter passed to the optimizer is the learning
    rate.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用TensorFlow最强大的功能之一：训练优化器。在定义损失函数后，它将依赖于占位符、常量和变量。训练优化器（如`tf.train.GradientDescentOptimizer()`），通过其`minimize()`方法，接受要优化的损失函数。内部，根据每个特定算法，它将计算损失函数相对于所有可训练变量的梯度，并将相应的校正应用于值。传递给优化器的参数是学习率。
- en: Therefore, we have defined an extra operation called `training_step`, which
    corresponds to a single stateful update step. It doesn't matter how complex the
    graph is; all trainable variables involved in a loss function will be optimized
    with a single instruction.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们定义了一个额外的操作，称为`training_step`，它对应于一个单一的状态更新步骤。无论图有多复杂，所有涉及损失函数的可训练变量都将通过单一指令进行优化。
- en: 'Now it''s time to train our logistic regression. The first thing to do is to
    ask TensorFlow to initialize all variables so that they are ready when the operations
    have to work with them:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候训练我们的逻辑回归了。首先要做的是让TensorFlow初始化所有变量，以便在操作需要使用它们时它们已经准备好了：
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'At this point, we can create a simple training loop (it should be stopped when
    the loss stops decreasing; however, we have a fixed number of iterations):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，我们可以创建一个简单的训练循环（应该在损失停止减少时停止；然而，我们有一个固定的迭代次数）：
- en: '[PRE8]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As you can see, at each iteration we ask TensorFlow to compute the loss function
    and a training step, and we always pass the same dictionary containing `X` and
    `Y`. At the end of this loop, the loss function is stable and we can check the
    quality of this logistic regression by plotting the separating hyperplane:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在每次迭代中，我们要求TensorFlow计算损失函数和训练步骤，并且我们总是传递包含`X`和`Y`的相同字典。在这个循环结束时，损失函数稳定了，我们可以通过绘制分离超平面来检查这个逻辑回归的质量：
- en: '![](img/61e3843b-4a4d-418f-9db2-7ae07aa0b78d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61e3843b-4a4d-418f-9db2-7ae07aa0b78d.png)'
- en: 'The result is approximately equivalent to the one obtained with the scikit-learn
    implementation. If we want to know the values of both coefficients (weights) and
    intercept (bias), we can ask TensorFlow to retrieve them by calling the method
    `eval()` on each variable:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 结果大约等同于使用scikit-learn实现得到的结果。如果我们想知道两个系数（权重）和截距（偏差）的值，我们可以通过在每个变量上调用`eval()`方法来让TensorFlow检索它们：
- en: '[PRE9]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Classification with a multi-layer perceptron
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器的分类
- en: 'We can now build an architecture with two dense layers and train a classifier
    for a more complex dataset. Let''s start by creating it:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以构建一个具有两个密集层的架构，并训练一个用于更复杂数据集的分类器。让我们先创建它：
- en: '[PRE10]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Even if we have only two classes, the dataset has three features and three
    clusters per class; therefore it''s almost impossible that a linear classifier
    can separate it with very high accuracy. A plot of the dataset is shown in the
    following figure:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 即使只有两个类别，数据集有三个特征，每个类别有三个簇；因此，线性分类器几乎不可能以非常高的精度将其分离。以下图表显示了数据集的图示：
- en: '![](img/76c390a4-fc1c-4d6b-95ad-be58f677d82b.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/76c390a4-fc1c-4d6b-95ad-be58f677d82b.png)'
- en: 'For benchmarking purposes, it''s useful to test a logistic regression:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基准测试的目的，测试一个逻辑回归是有用的：
- en: '[PRE11]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The score computed on the test set is about 71%, which is not really bad but
    below an acceptable threshold. Let''s try with an MLP with 50 hidden neurons (with
    hyperbolic tangent activation) and 1 sigmoid output neuron. The hyperbolic tangent
    is:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上计算的分数大约是71%，这并不算太差，但低于可接受的阈值。让我们尝试使用具有50个隐藏神经元（具有双曲正切激活）和1个sigmoid输出神经元的MLP。双曲正切是：
- en: '![](img/301cace9-c3c1-40e4-acb8-315d536b31f9.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/301cace9-c3c1-40e4-acb8-315d536b31f9.png)'
- en: And it's bounded asymptotically between -1.0 and 1.0.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 它的值在-1.0和1.0之间渐近有界。
- en: 'We are not going to implement each layer manually, but we''re using the built-in
    class `tf.contrib.layers.fully_connected()`. It accepts the input tensor or placeholder as
    the first argument and the number of layer-output neurons as the second one. The
    activation function can be specified using the attribute `activation_fn`:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会手动实现每一层，而是使用内置类`tf.contrib.layers.fully_connected()`。它接受输入张量或占位符作为第一个参数，并将层输出神经元的数量作为第二个参数。可以通过属性`activation_fn`指定激活函数：
- en: '[PRE12]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As in the previous example, we have defined two placeholders, `Xt` and `Yt`,
    and two fully connected layers. The first one accepts as input `Xt` and has 50
    output neurons (with `tanh` activation), while the second accepts as input the
    output of the previous layer (`layer_1`) and has only one sigmoid neuron, representing
    the class. The rounded output is provided by `Yo`, while the loss function is
    the total squared error, and it's implemented using the function `tf.nn.l2_loss()` computed
    on the difference between the output of the network (`layer_2`) and the target
    class placeholder `Yt`. The training step is implemented using a standard gradient
    descent optimizer, as for the logistic regression example.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个示例所示，我们定义了两个占位符`Xt`和`Yt`，以及两个全连接层。第一个接受`Xt`作为输入，有50个输出神经元（使用`tanh`激活），而第二个接受前一个层（`layer_1`）的输出，只有一个sigmoid神经元，代表类别。四舍五入的输出由`Yo`提供，损失函数是总平方误差，它通过`tf.nn.l2_loss()`函数实现，该函数计算网络输出（`layer_2`）与目标类别占位符`Yt`之间的差异。训练步骤使用标准的梯度下降优化器实现，就像逻辑回归示例中那样。
- en: 'We can now implement a training loop, splitting our dataset into a fixed number
    of batches (the number of samples is defined in the variable `batch_size`) and
    repeating a complete cycle for `nb_epochs` epochs:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以实施一个训练循环，将我们的数据集分成固定数量的批次（样本数量定义在变量`batch_size`中），并重复一个完整的周期`nb_epochs`个epoch：
- en: '[PRE13]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As it's possible to see, without particular attention to all details, the accuracy
    computed on the test set is 94%. This is an acceptable value, considering the
    structure of the dataset. In Goodfellow I., Bengio Y., Courville A., *Deep Learning*,
    MIT Press*, *the reader will find details of many important concepts that can
    still improve the performance and speed up the convergence process.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，如果不特别关注所有细节，测试集上计算出的准确率是94%。考虑到数据集的结构，这是一个可接受的价值。在Goodfellow I.，Bengio
    Y.，Courville A.的《深度学习》，MIT Press中，读者将找到许多重要概念的详细信息，这些信息仍然可以改善性能并加快收敛过程。
- en: Image convolution
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像卷积
- en: 'Even if we''re not building a complete deep learning model, we can test how
    convolution works with a simple example. The input image we''re using is already
    provided by `SciPy`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 即使我们没有构建完整的深度学习模型，我们也可以通过一个简单的示例测试卷积是如何工作的。我们使用的输入图像已经由`SciPy`提供：
- en: '[PRE14]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The original picture is shown here:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图片在此显示：
- en: '![](img/22e0ad00-5f44-4708-acc4-f8e0addbcdde.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22e0ad00-5f44-4708-acc4-f8e0addbcdde.png)'
- en: 'We''re going to apply a Laplacian filter, which emphasizes the boundary of
    each shape:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用拉普拉斯滤波器，它强调每个形状的边界：
- en: '[PRE15]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The kernel must be repeated twice because the TensorFlow convolution function
    `tf.nn.conv2d` expects an input and an output filter. We can now build the graph
    and test it:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因为TensorFlow卷积函数`tf.nn.conv2d`期望一个输入和一个输出滤波器，所以内核必须重复两次。我们现在可以构建图并测试它：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The parameters `strides` is a four-dimensional vector (each value corresponds
    to the input dimensions, so the first is the batch and the last one is the number
    of channels) that specifies how many pixels the sliding window must shift. In
    this case, we want to cover all the image shifting pixel to pixel. The parameter
    `padding` determines how the new dimensions must be computed and whether it's
    necessary to apply a zero padding. In our case, we're using the value `SAME`,
    which computes the dimensions by rounding off to the next integer the original
    dimensions divided by the corresponding strides value (as the latter are both
    1.0, the resulting image size will be exactly like the original one).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`strides`是一个四维向量（每个值对应输入维度，因此第一个是批次，最后一个是对应通道的数量），它指定滑动窗口必须移动多少像素。在这种情况下，我们想要覆盖所有图像的像素对像素的移动。参数`padding`确定如何计算新维度以及是否需要应用零填充。在我们的情况下，我们使用值`SAME`，它通过将原始维度除以相应的步长值并四舍五入到下一个整数来计算维度（因为这两个步长值都是1.0，所以结果图像大小将正好与原始图像相同）。
- en: 'The output image is shown here:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出图像在此显示：
- en: '![](img/dfb8bb16-70eb-4c94-acb1-1e047180c38d.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dfb8bb16-70eb-4c94-acb1-1e047180c38d.png)'
- en: The installation instructions for every operating system can be found on [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 每个操作系统的安装说明可以在 [https://www.tensorflow.org/install/](https://www.tensorflow.org/install/)
    找到。
- en: A quick glimpse inside Keras
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 快速浏览 Keras 内部结构
- en: 'Keras ([https://keras.io](https://keras.io)) is a high-level deep learning
    framework that works seamlessly with low-level backends like TensorFlow, Theano
    or CNTK. In Keras a model is like a sequence of layers where each output is fed
    into the following computational block until the final layer is reached. The generic
    structure of a model is:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Keras ([https://keras.io](https://keras.io)) 是一个高级深度学习框架，可以无缝地与 TensorFlow、Theano
    或 CNTK 等低级后端一起工作。在 Keras 中，一个模型就像一系列层，每个输出都被送入下一个计算块，直到达到最终层。模型的通用结构如下：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The class `Sequential` defines a generic empty model, that already implements
    all the methods needed to `add` layers, `compile` the model according to the underlying
    framework, to `fit` and `evaluate` the model and to `predict` the output given
    an input. All the most common layers are already implemented, including:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`Sequential` 类定义了一个通用的空模型，它已经实现了添加层、根据底层框架编译模型、`fit` 和 `evaluate` 模型以及给定输入预测输出的所有所需方法。所有最常用的层都已经实现，包括：'
- en: Dense, Dropout and Flattening layers
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集、Dropout 和展平层
- en: Convolutional (1D, 2D and 3D) layers
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积（1D、2D 和 3D）层
- en: Pooling layers
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 池化层
- en: Zero padding layers
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 零填充层
- en: RNN layers
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RNN 层
- en: 'A model can be compiled using several loss functions (like MSE or cross-entropy)
    and all the most diffused Stochastic Gradient Descent optimization algorithms
    (like RMSProp or Adam). For further details about the mathematical foundation
    of these methods, please refer to Goodfellow I., Bengio Y., Courville A., *Deep
    Learning*, MIT Press. As it''s impossible to discuss all important elements in
    such a short space, I prefer to create a complete example of image classification
    based on a convolutional network. The dataset we''re going to use is the CIFAR-10
    ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    which is made up of 60000 small RGB images (32 x 32) belonging to 10 different
    categories (airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck).
    In the following figure, a subset of images is shown:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以使用多个损失函数（如均方误差或交叉熵）和所有最常用的随机梯度下降优化算法（如 RMSProp 或 Adam）进行编译。有关这些方法的数学基础的更多详细信息，请参阅
    Goodfellow I.，Bengio Y.，Courville A. 的《深度学习》，MIT 出版社。由于篇幅有限，不可能讨论所有重要元素，我更喜欢创建一个基于卷积网络的图像分类的完整示例。我们将使用的数据集是
    CIFAR-10 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))，它由
    60000 张小 RGB 图像（32 x 32）组成，属于 10 个不同的类别（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船、卡车）。在下图中，显示了图像的一个子集：
- en: '![](img/e758d067-bd32-4d83-86aa-eaa275fed875.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e758d067-bd32-4d83-86aa-eaa275fed875.png)'
- en: Since the last release, Keras allows us to download this dataset using a built-in
    function; therefore, no further actions are required to use it.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 自从上次发布以来，Keras 允许我们使用内置函数下载这个数据集；因此，无需采取进一步行动即可使用它。
- en: 'The first step is loading the dataset and splitting it into training and test
    subsets:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是加载数据集并将其分为训练集和测试集：
- en: '[PRE18]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The training dataset contains 50000 images, while the test set 10000\. Now it's
    possible to build the model. We want to use a few convolutional layers to capture
    the specific elements of each category. As explained in the previous section,
    these particular layers can learn to identify specific geometric properties and
    generalize in an excellent way. In our small architecture, we start with a (5
    x 5) filter size to capture all the low-level features (like the orientation)
    and proceed by increasing the number of filters and reducing their size. In this
    way, the high-level features (like the shape of a wheel or the relative position
    of eyes, nose, and mouth) can also be captured.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集包含 50000 张图片，而测试集 10000 张。现在可以构建模型了。我们想使用几个卷积层来捕捉每个类别的特定元素。正如前文所述，这些特定的层可以学会识别特定的几何属性并以优秀的方式泛化。在我们的小型架构中，我们从
    (5 x 5) 的过滤器大小开始，以捕捉所有低级特征（如方向），然后通过增加过滤器数量并减小其大小来继续。这样，高级特征（如车轮的形状或眼睛、鼻子和嘴巴的相对位置）也可以被捕捉。
- en: '[PRE19]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The first instruction creates a new empty model. At this point, we can all
    the layers we want to include in the computational graph. The most common parameters
    of a convolutional layer are:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 第一条指令创建了一个新的空模型。在这个时候，我们可以添加所有我们想要包含在计算图中的层。卷积层最常见的参数包括：
- en: '**The number of filters**'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**滤波器数量**'
- en: '**Kernel size** (as tuple)'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**核大小**（作为元组）'
- en: '**Strides** (the default value is [1, 1]). This parameter specifies how many
    pixels the sliding window must consider when shifting on the image. [1, 1] means
    that no pixels are discarded. [2, 2] means that every horizontal and vertical
    shift will have a width of 2 pixels and so forth.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**步长**（默认值是[1, 1]）。此参数指定滑动窗口在图像上移动时必须考虑多少像素。[1, 1]表示不丢弃任何像素。[2, 2]表示每次水平和垂直移动都将有2像素的宽度，依此类推。'
- en: '**Activation** (the default value is None, meaning that the identity function
    will be used)'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活**（默认值为None，表示将使用恒等函数）'
- en: '**Input shape** (only for the first layer is this parameter mandatory)'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入形状**（仅对于第一层此参数是强制性的）'
- en: 'Our first layer has 32 (5 x 5) filters with a **ReLU** (**Rectified Linear
    Unit**) activation. This function is defined as:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一层有32个(5 x 5)滤波器，并使用**ReLU**（**归一化线性单元**）激活。此函数定义为：
- en: '![](img/09fbf182-d47a-4c52-ac17-d45f21dc2afd.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](img/09fbf182-d47a-4c52-ac17-d45f21dc2afd.png)'
- en: The second layer reduces the dimensionality with a max pooling considering (2
    x 2) blocks. Then we apply another convolution with 64 (4 x 4) filters followed
    by a zero padding (1 pixel at the top, bottom, left and right side of the input)
    and finally, we have the third convolutional layer with 128 (3 x 3) filters followed
    by a max pooling and a zero padding.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 第二层通过考虑(2 x 2)块的最大池化来降低维度。然后我们应用另一个有64个(4 x 4)滤波器的卷积，随后是零填充（在输入的顶部、底部、左侧和右侧各1像素），最后，我们得到第三个有128个(3
    x 3)滤波器的卷积层，随后是最大池化和零填充。
- en: 'At this point, we need to flatten the output of the last layer, so to work
    like in a MLP:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们需要展平最后一层的输出，以便像在MLP（多层感知器）中一样工作。
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'A dropout (with a probability of 0.2) is applied to the output of the last
    zero-padding layer; then this multidimensional value is flattened and transformed
    in a vector. This value is fed into a fully-connected layer with 128 neurons and
    ReLU activation. Another dropout is applied to the output (to prevent the overfitting)
    and, finally, this vector is fed into another fully connected layer with 10 neurons
    with a *softmax* activation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一个零填充层的输出上应用了一个dropout（概率为0.2）；然后这个多维值被展平并转换为一个向量。这个值被输入到一个有128个神经元的全连接层，并使用ReLU激活。然后对输出应用另一个dropout（以防止过拟合），最后，这个向量被输入到另一个有10个神经元的全连接层，并使用*softmax*激活：
- en: '![](img/c01a96a0-0c0e-400d-a23e-003f70d8b2e5.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c01a96a0-0c0e-400d-a23e-003f70d8b2e5.png)'
- en: In this way, the output of the model represents a discrete probability distribution
    (each value is the probability of the corresponding class).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，模型的输出代表了一个离散的概率分布（每个值是对应类别的概率）。
- en: 'The last step before training the model is compiling it:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前的最后一步是编译它：
- en: '[PRE21]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Keras will transform the high-level description into low-level operations (like
    the ones we have discussed in the previous section) with a categorical cross-entropy
    loss function (see the example of TensorFlow logistic regression) and the Adam
    optimizer. Moreover, it will apply an accuracy metric to dynamically evaluate
    the performance.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: Keras将使用具有分类交叉熵损失函数（参见TensorFlow逻辑回归的示例）和Adam优化器的低级操作（如我们在上一节中讨论的）将高级描述转换为低级操作。此外，它将应用一个准确度指标以动态评估性能。
- en: 'At this point, the model can be trained. We need only two preliminary operations:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，模型可以进行训练。我们只需要两个初步操作：
- en: Normalizing the images so they have values between 0 and 1
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将图像归一化，使它们的值介于0和1之间
- en: Applying the one-hot encoding to the integer label
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将整数标签应用one-hot编码
- en: 'The first operation can be simply performed by dividing the dataset by 255,
    while the second can be easily carried out using the built-in function `to_categorical()`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作可以通过将数据集除以255来简单地执行，而第二个可以通过使用内置函数`to_categorical()`轻松完成：
- en: '[PRE22]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We want to train with batches made up of 32 images and for a period of 15 epochs.
    The reader is free to change all these values to compare the results. The output
    provided by Keras shows the progress in the learning phase:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望以32张图像组成的批次进行训练，持续15个epoch。读者可以自由更改所有这些值以比较结果。Keras提供的输出显示了学习阶段的进度：
- en: '[PRE23]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'At the end of the 15th epoch, the accuracy on the training set is about 84%
    (a very good result). The final operation is evaluating the model with the test
    set:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在第15个epoch结束时，训练集上的准确率约为84%（一个非常好的结果）。最后的操作是使用测试集评估模型：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The final validation accuracy is lower (about 72%) than the one achieved during
    the training phase. This is a normal behavior for deep models, therefore, when
    optimizing the algorithm, it's always a good practice to use the cross validation
    or a well-defined test set (with the same distribution of the training set and
    25-30% of total samples).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 最终验证准确率（约 72%）低于训练阶段达到的准确率。这对于深度模型来说是正常行为，因此，在优化算法时，始终使用交叉验证或一个定义良好的测试集（与训练集具有相同的分布，包含
    25-30% 的总样本）是一个好习惯。
- en: Of course, we have presented a very simple architecture, but the reader can
    go deeper into these topics and create more complex models (Keras also contains
    some very famous pre-trained architectures like VGG16/19 and Inception V3 that
    can also be used to perform image classifications with 1000 categories).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们提出的是一个非常简单的架构，但读者可以深入研究这些主题并创建更复杂的模型（Keras 还包含一些非常著名的预训练架构，如 VGG16/19 和
    Inception V3，这些架构也可以用于执行具有 1000 个类别的图像分类）。
- en: 'All the information needed to install Keras with different backends, and the
    official documentation can be found on the website:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在网站上可以找到安装 Keras 所需的所有信息，包括不同后端的使用和官方文档：
- en: '[https://keras.io](https://keras.io)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://keras.io](https://keras.io)'
- en: References
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Goodfellow I., Bengio Y., Courville A., *Deep Learning*, MIT Press
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Goodfellow I.，Bengio Y.，Courville A.，*《深度学习》*，MIT Press
- en: 'Abrahams S., Hafner D., *TensorFlow for Machine Intelligence: A Hands-On Introduction
    to Learning Algorithms*, Bleeding Edge Press'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Abrahams S.，Hafner D.，*《机器智能的 TensorFlow：学习算法的实践入门》*，Bleeding Edge Press
- en: Bonaccorso G., *Neural Artistic Style Transfer with Keras*, [https://github.com/giuseppebonaccorso/Neural_Artistic_Style_Transfer](https://github.com/giuseppebonaccorso/Neural_Artistic_Style_Transfer)
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bonaccorso G.，*使用 Keras 进行神经艺术风格迁移*，[https://github.com/giuseppebonaccorso/Neural_Artistic_Style_Transfer](https://github.com/giuseppebonaccorso/Neural_Artistic_Style_Transfer)
- en: Krizhevsky A, Learning Multiple Layers of Features from Tiny Images, 2009 ([https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf))
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky A，从微图像中学习多层特征，2009 ([https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf))
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have briefly discussed some basic deep learning concepts,
    and the reader should now understand what a computational graph is and how it
    can be modeled using TensorFlow. A deep architecture, in fact, can be seen as
    a sequence of layers connected to each other. They can have different characteristics
    and purposes, but the overall graph is always a directed structure that associates
    input values with a final output layer. Therefore, it's possible to derive a global
    loss function that will be optimized by a training algorithm. We also saw how
    TensorFlow computes the gradients of an output tensor with respect to any previous
    connected layer and therefore how it's possible to implement the standard back-propagation
    strategy seamlessly to deep architectures. We did not discuss actual deep learning
    problems and methods because they require much more space; however, the reader
    can easily find many valid resources to continue his/her exploration in this fascinating
    field.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们简要讨论了一些基本的深度学习概念，读者现在应该理解什么是计算图以及如何使用 TensorFlow 来建模。实际上，深度架构可以看作是一系列相互连接的层。它们可以具有不同的特性和目的，但整体图始终是一个有向结构，将输入值与最终输出层关联起来。因此，可以推导出一个全局损失函数，该函数将由训练算法进行优化。我们还看到了
    TensorFlow 如何计算输出张量相对于任何先前连接层的梯度，以及如何无缝地将标准反向传播策略应用于深度架构。我们没有讨论实际的深度学习问题和方法，因为它们需要更多的空间；然而，读者可以轻松找到许多有效的资源，以继续在这个迷人的领域的探索。
- en: In the next chapter, we're going to summarize many of the concepts previously
    discussed in order to create complex machine learning architectures.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将总结之前讨论的许多概念，以便创建复杂的机器学习架构。
