- en: Learning Object Tracking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习物体跟踪
- en: In the previous chapter, we learned about video surveillance, background modeling,
    and morphological image processing. We discussed how we can use different morphological
    operators to apply cool visual effects to input images. In this chapter, we are
    going to learn how to track an object in a live video. We will discuss the different
    characteristics of an object that can be used to track it. We will also learn
    about different methods and techniques for object tracking. Object tracking is
    used extensively in robotics, self-driving cars, vehicle tracking, player tracking
    in sports, and video compression.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了视频监控、背景建模和形态学图像处理。我们讨论了如何使用不同的形态学算子将酷炫的视觉效果应用到输入图像上。在本章中，我们将学习如何在实时视频中跟踪一个物体。我们将讨论可用于跟踪物体的不同物体特性。我们还将了解不同的物体跟踪方法和技巧。物体跟踪在机器人学、自动驾驶汽车、车辆跟踪、体育中的运动员跟踪和视频压缩等领域得到了广泛的应用。
- en: 'By the end of this chapter, you will know the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解以下内容：
- en: How to track objects of a specific color
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何跟踪特定颜色的物体
- en: How to build an interactive object tracker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建一个交互式物体跟踪器
- en: What a corner detector is
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 角点检测器是什么
- en: How to detect good features to track
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何检测用于跟踪的良好特征
- en: How to build an optical flow-based feature tracker
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建基于光流的特征跟踪器
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter requires familiarity with the basics of the C++ programming language.
    All the code used in this chapter can be downloaded from the following GitHub
    link: [https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09). The
    code can be executed on any operating system, though it is only tested on Ubuntu.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章需要熟悉C++编程语言的基础知识。本章中使用的所有代码都可以从以下GitHub链接下载：[https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09](https://github.com/PacktPublishing/Learn-OpenCV-4-By-Building-Projects-Second-Edition/tree/master/Chapter_09)。代码可以在任何操作系统上执行，尽管它仅在Ubuntu上进行了测试。
- en: 'Check out the following video to see the Code in Action:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下视频，了解代码的实际应用：
- en: '[http://bit.ly/2SidbMc](http://bit.ly/2SidbMc)'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2SidbMc](http://bit.ly/2SidbMc)'
- en: Tracking objects of a specific color
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 跟踪特定颜色的物体
- en: In order to build a good object tracker, we need to understand what characteristics
    can be used to make our tracking robust and accurate. So, let's take a baby step
    in that direction and see whether we can use colorspace information to come up
    with a good visual tracker. One thing to keep in mind is that color information
    is sensitive to lighting conditions. In real-world applications, you will have
    to do some preprocessing to take care of that. But for now, let's assume that
    somebody else is doing that and we are getting clean color images.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建一个好的物体跟踪器，我们需要了解哪些特性可以用来使我们的跟踪既稳健又准确。因此，让我们迈出第一步，看看我们是否可以使用颜色空间信息来提出一个好的视觉跟踪器。需要注意的是，颜色信息对光照条件敏感。在实际应用中，你将不得不进行一些预处理来处理这个问题。但就目前而言，让我们假设有人正在做这件事，而我们正在获得干净的彩色图像。
- en: There are many different colorspaces, and picking a good one will depend on
    the different applications that a user is using. While RGB is the native representation
    on a computer screen, it's not necessarily ideal for humans. When it comes to
    humans, we give names to colors more naturally based on their hue, which is why
    **hue saturation value** (**HSV**) is probably one of the most informative colorspaces.
    It closely aligns with how we perceive colors. Hue refers to the color spectrum,
    saturation refers to the intensity of a particular color, and value refers to
    the brightness of that pixel. This is actually represented in a cylindrical format.
    You can find a simple explanation at [http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html](http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html).
    We can take the pixels of an image to the HSV colorspace and then use this colorspace
    to measure distances in this colorspace and threshold in this space thresholding
    to track a given object.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多不同的色彩空间，选择一个合适的色彩空间将取决于用户所使用的不同应用。虽然RGB是计算机屏幕上的原生表示，但它对于人类来说并不一定是理想的。对于人类来说，我们更自然地根据它们的色调来命名颜色，这就是为什么**色调饱和度值**（**HSV**）可能是最具有信息量的色彩空间之一。它与我们的颜色感知非常接近。色调指的是颜色光谱，饱和度指的是特定颜色的强度，而亮度指的是该像素的亮度。这实际上是以圆柱格式表示的。你可以在[http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html](http://infohost.nmt.edu/tcc/help/pubs/colortheory/web/hsv.html)找到简单的解释。我们可以将图像的像素转换到HSV色彩空间，然后使用这个色彩空间来测量这个空间中的距离和阈值，以跟踪特定的对象。
- en: 'Consider the following frame in the video:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑视频中的以下框架：
- en: '![](img/1faadcdb-ac4b-42d9-a9e6-912196cc317b.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1faadcdb-ac4b-42d9-a9e6-912196cc317b.png)'
- en: 'If you run it through the colorspace filter and track the object, you will
    see something like this:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过色彩空间过滤器运行它并跟踪对象，你会看到类似这样的结果：
- en: '![](img/37d137d4-7ba2-4630-8f26-36b464dc3c5e.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37d137d4-7ba2-4630-8f26-36b464dc3c5e.png)'
- en: 'As we can see here, our tracker recognizes a particular object in the video
    based on the color characteristics. In order to use this tracker, we need to know
    the color distribution of our target object. Here is the code to track a colored
    object, which selects only pixels that have a certain given hue. The code is well-commented,
    so read the explanation about each term to see what''s happening:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，我们的跟踪器根据颜色特征在视频中识别出特定的对象。为了使用这个跟踪器，我们需要知道目标对象的颜色分布。以下是跟踪彩色对象的代码，它只选择具有特定给定色调的像素。代码有很好的注释，所以阅读每个术语的解释，以了解发生了什么：
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Building an interactive object tracker
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建交互式对象跟踪器
- en: A colorspace-based tracker gives us the freedom to track a colored object, but
    we are also constrained to a predefined color. What if we just want to pick an
    object at random? How do we build an object tracker that can learn the characteristics
    of the selected object and just track it automatically? This is where the **c****ontinuously-adaptive
    meanshift** (**CAMShift**) algorithm comes into picture. It's basically an improved
    version of the meanshift algorithm.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 基于色彩空间的跟踪器为我们提供了跟踪彩色对象的自由，但我们也被限制在预定义的色彩上。如果我们只想随机选择一个对象怎么办？我们如何构建一个可以学习所选对象特征并自动跟踪它的对象跟踪器？这就是**连续自适应均值漂移**（**CAMShift**）算法出现的地方。它基本上是均值漂移算法的改进版本。
- en: The concept of meanshift is actually nice and simple. Let's say we select a
    region of interest and we want our object tracker to track that object. In this
    region, we select a bunch of points based on the color histogram and we compute
    the centroid of spatial points. If the centroid lies at the center of this region,
    we know that the object hasn't moved. But if the centroid is not at the center
    of this region, then we know that the object is moving in some direction. The
    movement of the centroid controls the direction in which the object is moving.
    So, we move the bounding box of the object to a new location so that the new centroid
    becomes the center of this bounding box. Hence, this algorithm is called meanshift,
    because the mean (the centroid) is shifting. This way, we keep ourselves updated
    with the current location of the object.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: MeanShift的概念实际上很棒且简单。假设我们选择一个感兴趣的区域，并希望我们的物体追踪器追踪该物体。在这个区域中，我们根据颜色直方图选择一些点，并计算空间点的质心。如果质心位于这个区域的中心，我们知道物体没有移动。但如果质心不在这个区域的中心，那么我们知道物体正在某个方向上移动。质心的移动控制着物体移动的方向。因此，我们将物体的边界框移动到新的位置，使新的质心成为这个边界框的中心。因此，这个算法被称为mean
    shift，因为均值（质心）在移动。这样，我们就能保持对物体当前位置的更新。
- en: But the problem with meanshift is that the size of the bounding box is not allowed
    to change. When you move the object away from the camera, the object will appear
    smaller to the human eye, but meanshift will not take that into account. The size
    of the bounding box will remain the same throughout the tracking session. Hence,
    we need to use CAMShift. The advantage of CAMShift is that it can adapt the size
    of the bounding box to the size of the object. Along with that, it can also keep
    track of the orientation of the object.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 但mean shift的问题在于边界框的大小不允许改变。当你将物体移离相机时，物体在人的眼中会显得更小，但mean shift不会考虑这一点。边界框的大小在整个追踪过程中保持不变。因此，我们需要使用CAMShift。CAMShift的优势在于它可以调整边界框的大小以适应物体的大小。除此之外，它还可以追踪物体的方向。
- en: 'Let''s consider the following frame, in which the object is highlighted:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下帧，其中物体被突出显示：
- en: '![](img/2a4365f9-5811-42a5-ba22-35ed50858e29.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2a4365f9-5811-42a5-ba22-35ed50858e29.png)'
- en: 'Now that we have selected the object, the algorithm computes the histogram
    backprojection and extracts all the information. What is histogram backprojection?
    It''s just a way of identifying how well the image fits into our histogram model.
    We compute the histogram model of a particular thing and then use this model to
    find that thing in an image. Let''s move the object and see how it''s getting
    tracked:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了物体，算法计算直方图反向投影并提取所有信息。什么是直方图反向投影？它只是识别图像如何适合我们的直方图模型的一种方法。我们计算特定事物的直方图模型，然后使用这个模型在图像中找到该事物。让我们移动物体，看看它是如何被追踪的：
- en: '![](img/2d8da352-b1c1-435f-b36a-a9caa1d56c82.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2d8da352-b1c1-435f-b36a-a9caa1d56c82.png)'
- en: 'It looks like the object is getting tracked fairly well. Let''s change the
    orientation and see whether the tracking is maintained:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来物体被追踪得相当好。让我们改变物体的方向，看看追踪是否还能维持：
- en: '![](img/a2d6e845-f927-4510-bd49-5784687e6d03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a2d6e845-f927-4510-bd49-5784687e6d03.png)'
- en: 'As we can see, the bounding ellipse has changed its location as well as orientation.
    Let''s change the perspective of the object and see whether it''s still able to
    track it:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，边界椭圆已经改变了其位置和方向。让我们改变物体的透视，看看它是否还能追踪到它：
- en: '![](img/967cea66-cf87-4a11-bb7c-b2dd25b1d4fa.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/967cea66-cf87-4a11-bb7c-b2dd25b1d4fa.png)'
- en: 'We''re still good! The bounding ellipse has changed the aspect ratio to reflect
    the fact that the object looks skewed now (because of the perspective transformation).
    Let''s look at the user interface functionality in the code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然做得很好！边界椭圆已经改变了长宽比，以反映物体现在看起来是倾斜的（因为透视变换）。让我们看看代码中的用户界面功能：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This function basically captures the coordinates of the rectangle that was selected
    in the window. The user just needs to click and drag with the mouse. There are
    a set of built-in functions in OpenCV that help us to detect these different mouse
    events.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数基本上捕获了在窗口中选择的矩形的坐标。用户只需要用鼠标点击并拖动。OpenCV中有一系列内置函数帮助我们检测这些不同的鼠标事件。
- en: 'Here is the code for performing object tracking based on CAMShift:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是基于CAMShift进行物体追踪的代码：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We now have the HSV image waiting to be processed. Let''s go ahead and see
    how we can use our thresholds to process this image:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个等待处理的 HSV 图像。让我们看看我们如何使用我们的阈值来处理这个图像：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As we can see here, we use the HSV image to compute the histogram of the region.
    We use our thresholds to locate the required color in the HSV spectrum and then
    filter out the image based on that. Let''s go ahead and see how we can compute
    the histogram backprojection:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们使用 HSV 图像来计算该区域的直方图。我们使用我们的阈值在 HSV 光谱中定位所需颜色，然后根据该颜色过滤图像。让我们看看我们如何计算直方图反向投影：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We are now ready to display the results. Using the rotated rectangle, let''s
    draw an ellipse around our region of interest:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备显示结果。使用旋转矩形，让我们在我们的感兴趣区域周围画一个椭圆：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Detecting points using the Harris corner detector
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Harris 角点检测器检测点
- en: Corner detection is a technique used to detect points of interest in an image.
    These interest points are also called feature points, or simply features, in computer
    vision terminology. A corner is basically an intersection of two edges. An interest
    point is basically something that can be uniquely detected in an image. A corner
    is a particular case of an interest point. These interest points help us characterize
    an image. These points are used extensively in applications such as object tracking,
    image classification, and visual search. Since we know that the corners are interesting,
    let's see how can detect them.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 角点检测是一种用于检测图像中感兴趣点的技术。这些感兴趣点在计算机视觉术语中也被称作特征点，或者简单地称为特征。一个角基本上是两条边的交点。一个感兴趣点基本上是在图像中可以唯一检测到的东西。一个角是感兴趣点的特例。这些感兴趣点帮助我们描述图像。这些点在诸如目标跟踪、图像分类和视觉搜索等应用中被广泛使用。既然我们知道角是有趣的，让我们看看如何检测它们。
- en: In computer vision, there is a popular corner detection technique called the
    Harris corner detector. We basically construct a 2 x 2 matrix based on partial
    derivatives of the grayscale image, and then analyze the eigenvalues. What does
    that even mean? Well, let's dissect it so that we can understand it better. Let's
    consider a small patch in the image. Our goal is to identify whether this patch
    has a corner in it. So, we consider all the neighboring patches and compute the
    intensity difference between our patch and all those neighboring patches. If the
    difference is high in all directions, then we know that our patch has a corner
    in it. This is an oversimplification of the actual algorithm, but it covers the
    gist. If you want to understand the underlying mathematical details, you can check
    out the original paper by *Harris* and *Stephens* at [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf).
    A corner is a point with strong intensity differences along two directions.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，有一个流行的角点检测技术叫做 Harris 角点检测器。我们基本上基于灰度图像的偏导数构建一个 2x2 矩阵，然后分析特征值。这到底是什么意思呢？好吧，让我们剖析一下，以便我们更好地理解它。让我们考虑图像中的一个小的区域。我们的目标是确定这个区域中是否有角点。因此，我们考虑所有相邻的区域，并计算我们的区域与所有这些相邻区域之间的强度差异。如果在所有方向上差异都很大，那么我们知道我们的区域中有一个角点。这是实际算法的过度简化，但它涵盖了要点。如果你想要了解背后的数学细节，你可以查看
    *Harris* 和 *Stephens* 在 [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)
    发表的原始论文。一个角是沿两个方向具有强烈强度差异的点。
- en: 'If we run the Harris corner detector, it will look like this:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们运行 Harris 角点检测器，它看起来会是这样：
- en: '![](img/8825804c-b5d9-4478-938c-c4e8b1a17e37.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8825804c-b5d9-4478-938c-c4e8b1a17e37.png)'
- en: 'As we can see, the green circles on the TV remote are the detected corners.
    This will change based on the parameters you choose for the detector. If you modify
    the parameters, you can see that more points might get detected. If you make it
    strict, you might not be able to detect soft corners. Let''s look at the code
    to detect Harris corners:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，电视遥控器上的绿色圆圈是检测到的角点。这会根据你为检测器选择的参数而改变。如果你修改参数，你可能会看到更多点被检测到。如果你让它更严格，你可能无法检测到软角。让我们看看检测
    Harris 角点的代码：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We converted the image to grayscale and detected corners using our parameters.
    You can find the full code in the `.cpp` files. These parameters play an important
    role in the number of points that will be detected. You can check out the OpenCV
    documentation of `cornerHarris()` at [https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将图像转换为灰度，并使用我们的参数检测角。您可以在`.cpp`文件中找到完整的代码。这些参数在检测到的点数中起着重要作用。您可以在[https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345](https://docs.opencv.org/master/dd/d1a/group__imgproc__feature.html#gac1fc3598018010880e370e2f709b4345)查看`cornerHarris()`的OpenCV文档。
- en: 'We now have all the information we need. Let''s go ahead and draw circles around
    our corners to display the results:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了所有需要的信息。让我们继续在角周围画圆圈以显示结果：
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, this code takes an input argument: `blockSize`. Depending on
    the size you choose, the performance will vary. Start with a value of four and
    play around with it to see what happens.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，此代码接受一个输入参数：`blockSize`。根据您选择的大小，性能会有所不同。从四个开始，并尝试不同的值以查看会发生什么。
- en: Good features to track
  id: totrans-55
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '**Good features to track**'
- en: 'Harris corner detector performs well in many cases, but it can still be improved.
    Around six years after the original paper by *Harris* and *Stephens*, *Shi* and
    *Tomasi* came up with something better and they called it *Good Features to Track*.
    You can read the original paper here: [http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf).
    They used a different scoring function to improve the overall quality. Using this
    method, we can find the N strongest corners in the given image. This is very useful
    when we don''t want to use every single corner to extract information from the
    image. As we discussed, a good interest point detector is very useful in applications
    such as object tracking, object recognition, and image search.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 哈里斯角检测器在许多情况下表现良好，但仍有改进空间。在哈里斯和斯蒂芬斯发表原始论文后的六年左右，石和托马西提出了更好的方法，并将其命名为**Good Features
    to Track**。您可以在以下链接中阅读原始论文：[http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf)。他们使用不同的评分函数来提高整体质量。使用这种方法，我们可以在给定的图像中找到N个最强的角。当我们不想使用图像中的每一个角来提取信息时，这非常有用。正如我们讨论的那样，一个好的兴趣点检测器在目标跟踪、目标识别和图像搜索等应用中非常有用。
- en: 'If you apply the Shi-Tomasi corner detector to an image, you will see something
    like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将石-托马西角检测器应用于图像，您将看到类似这样的结果：
- en: '![](img/f397c6a1-aa49-41f4-923a-ff22ed465243.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f397c6a1-aa49-41f4-923a-ff22ed465243.png)'
- en: 'As we can see here, all the important points in the frame are captured. Let''s
    look at the code to track these features:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，框架中的所有重要点都被捕捉到了。让我们看看代码来跟踪这些特征：
- en: '[PRE8]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'As we can see, we extracted the frame and used `goodFeaturesToTrack` to detect
    the corners. It''s important to understand that the number of corners detected
    will depend on our choice of parameters. You can find a detailed explanation at [http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack).
    Let''s go ahead and draw circles on these points to display the output image:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们提取了帧，并使用`goodFeaturesToTrack`来检测角。重要的是要理解检测到的角的数量将取决于我们的参数选择。您可以在[http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack](http://docs.opencv.org/2.4/modules/imgproc/doc/feature_detection.html?highlight=goodfeaturestotrack#goodfeaturestotrack)找到详细的解释。让我们继续在这些点上画圆圈以显示输出图像：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This program takes an input argument: `numCorners`. This value indicates the
    maximum number of corners you want to track. Start with a value of `100` and play
    around with it to see what happens. If you increase this value, you will see more
    feature points getting detected.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 此程序接受一个输入参数：`numCorners`。此值表示您想要跟踪的最大角数。从`100`开始，并尝试不同的值以查看会发生什么。如果您增加此值，您将看到更多特征点被检测到。
- en: Feature-based tracking
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于特征的跟踪
- en: Feature-based tracking refers to tracking individual feature points across successive
    frames in the video. The advantage here is that we don't have to detect feature
    points in every single frame. We can just detect them once and keep tracking them
    after that. This is more efficient than running the detector on every frame. We
    use a technique called optical flow to track these features. Optical flow is one
    of the most popular techniques in computer vision. We choose a bunch of feature
    points and track them through the video stream. When we detect the feature points,
    we compute the displacement vectors and show the motion of those keypoints between
    consecutive frames. These vectors are called motion vectors. A motion vector for
    a particular point is basically just a directional line indicating where that
    point has moved, as compared to the previous frame. Different methods are used
    to detect these motion vectors. The two most popular algorithms are the **Lucas-Kanade**
    method and the **Farneback** algorithm.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 基于特征点的跟踪是指在视频的连续帧之间跟踪单个特征点。这里的优势是我们不必在每一帧中检测特征点。我们只需检测一次，然后继续跟踪。这比在每一帧上运行检测器更有效。我们使用一种称为光流的技术来跟踪这些特征。光流是计算机视觉中最流行的技术之一。我们选择一些特征点，并通过视频流跟踪它们。当我们检测到特征点时，我们计算位移矢量，并显示这些关键点在连续帧之间的运动。这些矢量被称为运动矢量。特定点的运动矢量基本上就是一个指示该点相对于前一帧移动方向的直线。不同的方法被用来检测这些运动矢量。最流行的两种算法是**Lucas-Kanade**方法和**Farneback**算法。
- en: Lucas-Kanade method
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lucas-Kanade方法
- en: 'The Lucas-Kanade method is used for sparse optical flow tracking. By sparse,
    we mean that the number of feature points is relatively low. You can refer to
    their original paper here: [http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf).
    We start the process by extracting the feature points. For each feature point,
    we create 3 x 3 patches with the feature point at the center. The assumption here
    is that all the points within each patch will have a similar motion. We can adjust
    the size of this window depending on the problem at hand.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Lucas-Kanade方法用于稀疏光流跟踪。这里的稀疏意味着特征点的数量相对较低。你可以在这里参考他们的原始论文：[http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf](http://cseweb.ucsd.edu/classes/sp02/cse252/lucaskanade81.pdf)。我们通过提取特征点开始这个过程。对于每个特征点，我们以特征点为中心创建3
    x 3的块。这里的假设是每个块内的所有点将具有相似的运动。我们可以根据问题调整这个窗口的大小。
- en: For each feature point in the current frame, we take the surrounding 3 x 3 patch
    as our reference point. For this patch, we look in its neighborhood in the previous
    frame to get the best match. This neighborhood is usually bigger than 3 x 3 because
    we want to get the patch that's closest to the patch under consideration. Now,
    the path from the center pixel of the matched patch in the previous frame to the
    center pixel of the patch under consideration in the current frame will become
    the motion vector. We do that for all the feature points and extract all the motion
    vectors.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于当前帧中的每个特征点，我们将其周围的3 x 3区域作为参考点。对于这个区域，我们在前一帧的邻域中寻找最佳匹配。这个邻域通常比3 x 3大，因为我们想要找到与当前考虑的块最接近的块。现在，从前一帧匹配块的中心像素到当前帧考虑的块的中心的路径将成为运动矢量。我们对所有特征点都这样做，并提取所有运动矢量。
- en: 'Let''s consider the following frame:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下框架：
- en: '![](img/77fb918e-4022-4492-9af6-58024173479e.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/77fb918e-4022-4492-9af6-58024173479e.png)'
- en: 'We need to add some points that we want to track. Just go ahead and click on
    a bunch of points on this window with your mouse:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要添加一些我们想要跟踪的点。只需用鼠标点击这个窗口上的多个点即可：
- en: '![](img/81a8a130-e510-4376-b3fe-3c3ce81a862a.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/81a8a130-e510-4376-b3fe-3c3ce81a862a.png)'
- en: 'If I move into a different position, you will see that the points are still
    being tracked correctly within a small margin of error:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我移动到不同的位置，你将看到点仍然在很小的误差范围内被正确跟踪：
- en: '![](img/0715e6df-ef45-4dea-bf7b-dbda0b2822fb.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0715e6df-ef45-4dea-bf7b-dbda0b2822fb.png)'
- en: 'Let''s add a lot of points and see what happens:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加很多点，看看会发生什么：
- en: '![](img/b375668e-563f-46b1-956a-e54925b634ae.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b375668e-563f-46b1-956a-e54925b634ae.png)'
- en: As we can see, it will keep tracking those points. But, you will notice that
    some of the points will be dropped because of factors such as prominence or speed
    of movement. If you want to play around with it, you can just keep adding more
    points to it. You can also let the user select a region of interest in the input
    video. You can then extract feature points from this region of interest and track
    the object by drawing a bounding box. It will be a fun exercise!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，它将一直跟踪这些点。但，你会注意到由于突出度或移动速度等因素，一些点将会被丢弃。如果你想玩转它，你只需继续添加更多点即可。你也可以让用户在输入视频中选择感兴趣的区域。然后你可以从这个感兴趣的区域提取特征点，并通过绘制边界框来跟踪对象。这将是一个有趣的练习！
- en: 'Here is the code to do Lucas-Kanade-based tracking:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用Lucas-Kanade进行跟踪的代码：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We use the current image and the previous image to compute the optical flow
    information. Needless to say, the quality of the output will depend on the parameters
    chosen. You can find more details about the parameters at [http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk](http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk).
    To increase quality and robustness, we need to filter out the points that are
    very close to each other because they''re not adding new information. Let''s go
    ahead and do that:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用当前图像和前一个图像来计算光流信息。不用说，输出的质量将取决于所选择的参数。你可以在[http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk](http://docs.opencv.org/2.4/modules/video/doc/motion_analysis_and_object_tracking.html#calcopticalflowpyrlk)找到更多关于参数的详细信息。为了提高质量和鲁棒性，我们需要过滤掉彼此非常接近的点，因为它们并没有增加新的信息。让我们继续这样做：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We now have the tracking points. The next step is to refine the location of
    those points. What exactly does **refine** mean in this context? To increase the
    speed of computation, there is some level of quantization involved. In layman''s
    terms, you can think of it as rounding off. Now that we have the approximate region,
    we can refine the location of the point within that region to get a more accurate
    outcome. Let''s go ahead and do that:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了跟踪点。下一步是精炼这些点的位置。在这个上下文中，“精炼”究竟是什么意思？为了提高计算速度，涉及到了一定程度的量化。用通俗易懂的话来说，你可以把它想象成四舍五入。现在我们已经有了大致的区域，我们可以在该区域内精炼点的位置，以获得更准确的结果。让我们继续这样做：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Farneback algorithm
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Farneback算法
- en: 'Gunnar Farneback proposed this optical flow algorithm and it''s used for dense
    tracking. Dense tracking is used extensively in robotics, augmented reality, and
    3D mapping. You can check out the original paper here: [http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf).
    The Lucas-Kanade method is a sparse technique, which means that we only need to
    process some pixels in the entire image. The Farneback algorithm, on the other
    hand, is a dense technique that requires us to process all the pixels in the given
    image. So, obviously, there is a trade-off here. Dense techniques are more accurate,
    but they are slower. Sparse techniques are less accurate, but they are faster.
    For real-time applications, people tend to prefer sparse techniques. For applications
    where time and complexity are not a factor, people tend to prefer dense techniques
    to extract finer details.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Gunnar Farneback提出了这个光流算法，并且它被用于密集跟踪。密集跟踪在机器人技术、增强现实和3D制图中被广泛使用。你可以在以下链接查看原始论文：[http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf](http://www.diva-portal.org/smash/get/diva2:273847/FULLTEXT01.pdf)。Lucas-Kanade方法是一种稀疏技术，这意味着我们只需要处理整个图像中的一些像素。而Farneback算法则是一种密集技术，它要求我们处理给定图像中的所有像素。因此，显然这里有一个权衡。密集技术更准确，但速度较慢。稀疏技术准确性较低，但速度较快。对于实时应用，人们往往更倾向于使用稀疏技术。对于时间和复杂度不是主要因素的场合，人们往往更倾向于使用密集技术来提取更精细的细节。
- en: In his paper, Farneback describes a method for dense optical-flow estimation
    based on polynomial expansion for two frames. Our goal is to estimate the motion
    between these two frames, which is basically a three-step process. In the first
    step, each neighborhood in both frames is approximated by polynomials. In this
    case, we are only interested in quadratic polynomials. The next step is to construct
    a new signal by global displacement. Now that each neighborhood is approximated
    by a polynomial, we need to see what happens if this polynomial undergoes an ideal
    translation. The last step is to compute the global displacement by equating the
    coefficients in the yields of these quadratic polynomials.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的论文中，Farneback描述了一种基于多项式展开的密集光流估计方法。我们的目标是估计这两帧之间的运动，这基本上是一个三步过程。在第一步中，两个帧中的每个邻域都通过多项式进行近似。在这种情况下，我们只对二次多项式感兴趣。下一步是通过全局位移构建一个新的信号。现在，每个邻域都通过一个多项式进行近似，我们需要看看如果这个多项式经历一个理想的平移会发生什么。最后一步是通过将二次多项式的系数相等来计算全局位移。
- en: Now, how this is feasible? If you think about it, we are assuming that an entire
    signal is a single polynomial and there is a global translation relating the two
    signals. This is not a realistic scenario! So, what are we looking for? Well,
    our goal is to find out whether these errors are small enough so that we can build
    a useful algorithm that can track the features.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，这是如何可行的呢？如果你这么想，我们假设整个信号是一个单一的多项式，并且有两个信号之间的全局平移关系。这不是一个现实的情况！那么，我们在寻找什么？好吧，我们的目标是找出这些误差是否足够小，以至于我们可以构建一个有用的算法来跟踪特征。
- en: 'Let''s look at a static image:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个静态图像：
- en: '![](img/312e61da-2099-4efb-ae7c-e6224558a3fe.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/312e61da-2099-4efb-ae7c-e6224558a3fe.png)'
- en: 'If I move sideways, we can see that the motion vectors are pointing in a horizontal
    direction. It is simply tracking the movement of my head:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我向侧面移动，我们可以看到运动向量是指向水平方向的。这只是在跟踪我的头部运动：
- en: '![](img/d82a7c3d-8a54-4447-9f85-6d329df90574.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d82a7c3d-8a54-4447-9f85-6d329df90574.png)'
- en: 'If I move away from the webcam, you can see that the motion vectors are pointing
    in a direction perpendicular to the image plane:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我远离摄像头，你可以看到运动向量是指向与图像平面垂直的方向：
- en: '![](img/22a60ecd-e148-4c97-b659-bb2075c9664b.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/22a60ecd-e148-4c97-b659-bb2075c9664b.png)'
- en: 'Here is the code to do optical-flow-based tracking using the Farneback algorithm:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用Farneback算法进行基于光流跟踪的代码：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'As we can see, we use the Farneback algorithm to compute the optical flow vectors.
    The input parameters to `calcOpticalFlowFarneback` are important when it comes
    to the quality of tracking. You can find details about those parameters at [http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html).
    Let''s go ahead and draw those vectors on the output image:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们使用Farneback算法来计算光流向量。`calcOpticalFlowFarneback`的输入参数对于跟踪质量非常重要。你可以在[http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html](http://docs.opencv.org/3.0-beta/modules/video/doc/motion_analysis_and_object_tracking.html)找到有关这些参数的详细信息。让我们继续在输出图像上绘制这些向量：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We used a function called `drawOpticalFlow` to draw those optical flow vectors.
    These vectors indicate the direction of motion. Let''s look at the function to
    see how we draw those vectors:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了一个名为 `drawOpticalFlow` 的函数来绘制那些光流向量。这些向量指示运动的方向。让我们看看这个函数，看看我们是如何绘制这些向量的：
- en: '[PRE15]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about object tracking. We learned how to use HSV
    colorspace to track objects of a specific color. We discussed clustering techniques
    for object tracking and how we can build an interactive object tracker using the
    CAMShift algorithm. We looked at corner detectors and how we can track corners
    in a live video. We discussed how to track features in a video using optical flow.
    Finally, we understood the concepts behind the Lucas-Kanade and Farneback algorithms
    and then implemented them.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了对象跟踪。我们学习了如何使用HSV颜色空间跟踪特定颜色的对象。我们讨论了对象跟踪的聚类技术以及如何使用CAMShift算法构建一个交互式对象跟踪器。我们研究了角点检测以及如何在实时视频中跟踪角点。我们讨论了如何使用光流在视频中跟踪特征。最后，我们理解了Lucas-Kanade和Farneback算法背后的概念，然后实现了它们。
- en: In the next chapter, we are going to discuss segmentation algorithms and how
    we can use them for text recognition.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论分割算法以及我们如何使用它们进行文本识别。
