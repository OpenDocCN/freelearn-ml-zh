- en: Chapter 9. Ensembling Regression Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章. 集成回归模型
- en: '[Chapters 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, to [Chapters 8](part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee
    "Chapter 8. Ensemble Diagnostics"), *Ensemble Diagnostics*, were devoted to learning
    different types of ensembling methods. The discussion was largely based on the
    classification problem. If the regressand/output of the supervised learning problem
    is a numeric variable, then we have a regression problem, which will be addressed
    here. The housing price problem is selected for demonstration purposes throughout
    the chapter, and the dataset is chosen from a Kaggle competition: [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).
    The data consists of numerous variables, including as many as 79 independent variables,
    with the price of the house as the output/dependent variable. The dataset needs
    some pre-processing as some variables have missing dates, some variables have
    lots of levels, with a few of them only occurring very rarely, and some variables
    have missing data in more than 20% of observations.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee "第3章.
    袋装"), *袋装*, 到 [第8章](part0057_split_000.html#1MBG21-2006c10fab20488594398dc4871637ee
    "第8章. 集成诊断"), *集成诊断*，都致力于学习不同类型的集成方法。讨论主要基于分类问题。如果监督学习问题的回归变量/输出是一个数值变量，那么我们就有了一个回归问题，这将在本章中解决。为了演示目的，本章选择了房价问题，数据集来自Kaggle竞赛：[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/)。数据包括许多变量，包括多达79个独立变量，房价作为输出/依赖变量。数据集需要一些预处理，因为一些变量有缺失日期，一些变量有大量级别，其中一些变量只出现得非常少，还有一些变量在超过20%的观测值中缺失数据。'
- en: 'The pre-processing techniques will be succeeded by variable reduction methods
    and then we will fit important regression models: linear regression, neural network,
    and regression trees. An ensemble extension of the regression tree will first
    be provided, and then we will apply the bagging and random forest methods. Various
    boosting methods will be used to improve the prediction. Stacked ensemble methods
    will be applied in the concluding section.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理技术将被变量减少方法所继替，然后我们将拟合重要的回归模型：线性回归、神经网络和回归树。首先将提供回归树的集成扩展，然后我们将应用袋装和随机森林方法。将使用各种提升方法来提高预测。在结论部分将应用堆叠集成方法。
- en: 'In this chapter, we will cover the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Data pre-processing and visualization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理和可视化
- en: Variable reduction techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量减少技术
- en: Regression models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型
- en: Bagging and Random Forests for the regression data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归数据的袋装和随机森林
- en: Boosting regression models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升回归模型
- en: Stacked ensemble methods for regression data
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归数据的堆叠集成方法
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will need the following R packages for this chapter:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将需要以下R包来完成本章内容：
- en: '`adabag`'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`adabag`'
- en: '`caret`'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caret`'
- en: '`caretEnsemble`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caretEnsemble`'
- en: '`ClustofVar`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ClustofVar`'
- en: '`FactoMinR`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FactoMinR`'
- en: '`gbm`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gbm`'
- en: '`ipred`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ipred`'
- en: '`missForest`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missForest`'
- en: '`nnet`'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nnet`'
- en: '`NeuralNetTools`'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NeuralNetTools`'
- en: '`plyr`'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plyr`'
- en: '`rpart`'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rpart`'
- en: '`RSADBE`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RSADBE`'
- en: Pre-processing the housing data
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理房价数据
- en: 'The dataset was selected from [www.kaggle.com](http://www.kaggle.com) and the
    title of the project is **House Prices: Advanced Regression Techniques**. The
    main files we will be using are `test.csv` and `train.csv`, and the files are
    available in the companion bundle package. A description of the variables can
    be found in the `data_description.txt` file. Further details, of course, can be
    obtained at [https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/).
    The train dataset contains 1460 observations, while the test dataset contains
    1459 observations. The price of the property is known only in the train dataset
    and are not available for those in the test dataset. We will use the train dataset
    for model development only. The datasets are first loaded into an R session and
    a beginning inspection is done using the `read.csv`, `dim`, `names`, and `str`
    functions:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集是从[www.kaggle.com](http://www.kaggle.com)选择的，项目的标题是**房价：高级回归技术**。我们将使用的主要文件是`test.csv`和`train.csv`，这些文件可在配套的捆绑包中找到。变量的描述可以在`data_description.txt`文件中找到。更详细的信息当然可以在[https://www.kaggle.com/c/house-prices-advanced-regression-techniques/](https://www.kaggle.com/c/house-prices-advanced-regression-techniques/)获得。训练数据集包含1460个观测值，而测试数据集包含1459个观测值。房产价格只在训练数据集中已知，在测试数据集中不可用。我们只将使用训练数据集进行模型开发。首先使用`read.csv`、`dim`、`names`和`str`函数将数据集加载到R会话中，并进行初步检查：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The dimensions of the data frames give the number of variables and the number
    of observations. The details of all the variables can be found in the `data_description.txt`
    file. It can be seen that what we have on hand is a comprehensive dataset. Now,
    we ran the option of `na.strings = "NA"` in the `read.csv` import function, and
    quite naturally, this implied that we have missing data. When we have missing
    data in both the training and test data partitions, the author recommends combining
    the covariates in the partitions and then examining them further. The covariates
    are first combined and then we find the number of missing observations for each
    of the variables:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框的维度表示变量的数量和观测值的数量。所有变量的详细信息可以在`data_description.txt`文件中找到。可以看出，我们手头上的是一个综合性的数据集。现在，我们在`read.csv`导入函数中运行了`na.strings
    = "NA"`选项，并且很自然地，这暗示我们存在缺失数据。当训练数据和测试数据分区中都有缺失数据时，作者建议将分区的协变量合并，然后进一步检查。首先将协变量合并，然后我们找出每个变量的缺失观测值数量：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `rbind` function combines the data in the training and testing datasets.
    The `is.na(x)` code inspects the absence of the values for every element of `x`,
    and the `sum` applied tells us the number of missing observations for the variable.
    The function is then applied for every variable of `housing` using the `sapply`
    function. The count of missing observations for the variables is sorted in descending
    order using the `sort` function with the argument `dec=TRUE`, and hence it enables
    us to find the variables with the most missing numbers in the beginning.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`rbind`函数将训练和测试数据集中的数据合并。`is.na(x)`代码检查`x`中每个元素的值是否存在，应用`sum`函数后告诉我们变量的缺失观测值数量。然后使用`sapply`函数对`housing`的每个变量应用此函数。使用带有`dec=TRUE`参数的`sort`函数按降序对变量的缺失观测值计数进行排序，因此它使我们能够在开始时找到缺失值最多的变量。'
- en: 'The reader might be wondering about the rationale behind the collation of the
    observations. The intuitive reasoning behind the collation is that while some
    variables might have missing data, more in the training data than in the test
    data, or the other way around, it is important that the overall missing percentage
    does not exceed a certain threshold of the observations. Although we have missing
    data imputation techniques, using them when the missing data percentage is too
    high might cause us to miss out on the important patterns of the features. Consequently,
    we arbitrarily make a choice of restricting the variables if more than 10% of
    the values are missing. If the missing percentage of any variable exceeds 10%,
    we will avoid analyzing that variable further. First, we identify the variables
    that exceed 10%, and then we remove them from the master data frame. The following
    R code block gives us the desired result:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 读者可能会想知道关于观察结果整理背后的理由。整理观察结果的直观推理是，虽然某些变量可能有缺失数据，在训练数据中比在测试数据中多，或者相反，重要的是整体缺失百分比不要超过观测值的某个特定阈值。尽管我们有缺失数据插补技术，但在缺失数据百分比过高时使用它们可能会使我们错过特征的重要模式。因此，我们任意选择限制变量，如果超过10%的值缺失。如果任何变量的缺失百分比超过10%，我们将避免进一步分析该变量。首先，我们识别出超过10%的变量，然后从主数据框中移除它们。下面的R代码块给出了我们想要的结果：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The variables that have more than 10% missing observations are first identified
    and then stored in the `miss_variables` character vector, and we have 11 variables
    that meet this criterion. Such variables are eliminated with the `NULL` assignment
    for them.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 首先识别出超过10%缺失观测值的变量，然后存储在`miss_variables`字符向量中，我们有11个变量符合这一标准。这些变量通过`NULL`赋值被消除。
- en: 'Next, we find the number of levels (distinct) of factor variables. We define
    a function, `find_df`, which will find the number of levels of a factor variable.
    For numeric and integer variables, it will return `1`. The purpose of this exercise
    will become clear soon enough. The `find_df` function is created in the next block:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们找到因子变量的水平数（不同的）。我们定义了一个函数，`find_df`，它将找到因子变量的水平数。对于数值和整数变量，它将返回`1`。这个练习的目的很快就会变得清楚。`find_df`函数将在下一个代码块中创建：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We need to inspect `67` variables, following the elimination of `11` variables
    with more than 10% missing observations. Some of these might not be factor variables.
    The `find_df` function shows that, for factor variables, the number of levels
    varies from 2-25\. A quick problem now arises for the `Condition2` and `Exterior1st`
    variables:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要检查`67`个变量，在消除`11`个具有超过10%缺失观测值的变量之后。其中一些可能不是因子变量。`find_df`函数显示，对于因子变量，水平数从2到25不等。现在对于`Condition2`和`Exterior1st`变量出现了一个快速问题：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In many practical problems, it appears that there will be **factor variables**
    that have some levels that occur very infrequently. Now, if we have new levels
    in the test/validation partition, it is not possible to make predictions. From
    a statistical perspective, we have a technical problem: losing too many **degrees
    of freedom**. A rudimentary approach is pursued here, and we will simply put together
    all the observations in the `Others` umbrella. A `Truncate_Factor` function is
    created, and this has two arguments: `x` and `alpha`. The `x` object is the variable
    to be given to the function, and `alpha` is the specified fraction below which
    any variable frequency would be pooled to obtain `Others`.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际问题中，似乎存在一些**因子变量**，它们的一些水平出现频率非常低。现在，如果我们测试/验证分区中有新的水平，我们就无法进行预测。从统计学的角度来看，我们遇到了一个技术问题：失去了太多的**自由度**。这里采取了一种基本的方法，我们只是简单地将所有观察结果汇总到`Others`这个总类别中。创建了一个`Truncate_Factor`函数，它有两个参数：`x`和`alpha`。`x`对象是要传递给函数的变量，而`alpha`是任何变量频率会被汇总到`Others`中的指定比例。
- en: Note
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: If there are certain levels of a factor that are new in the test dataset, no
    analytical method will be able to incorporate the influence. Thus, in cases where
    we have too many infrequent levels, the chances of some levels not being included
    in the training dataset will be high and the prediction will not yield the output
    for the test observations.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果因子变量在测试数据集中有新的水平，就没有分析方法是能够包含其影响的。因此，在我们有太多不常见水平的情况下，某些水平没有被包括在训练数据集中的可能性很高，预测结果将不会为测试观察结果提供输出。
- en: 'The `Truncate_Factor` function is now created:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建`Truncate_Factor`函数：
- en: '[PRE6]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: We can now see that the `Others` level is more frequent and if we randomly create
    partitions, it is very likely that the problem of unknown levels will not occur.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，“其他”级别出现的频率更高，如果我们随机创建分区，未知级别的出现问题很可能不会发生。
- en: 'You may recollect that we have eliminated the variables that have excessive
    missing observations thus far. This does not mean that we are free of missing
    data, as can be quickly noticed:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，我们之前已经消除了具有过多缺失观测值的变量。这并不意味着我们没有缺失数据，这一点可以很快地发现：
- en: '[PRE7]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The `474` values can''t be ignored. Missing data imputation is an important
    way of filling the missing values. Although the EM algorithm is a popular method
    to achieve that, we will apply the Random Forests technique to simulate the missing
    observations. The `missForest` package was introduced in [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*, and an example was used to simulate
    the missing values. We will apply this function to the housing data frame. Since
    the default number of variables chosen in this function is `mtry=5` and we have
    68 variables in housing, the number of variables chosen for splitting a node is
    changed to about p/3 and hence the option of `mtry=20` is seen in the next R block.
    On a machine with 8 GB of RAM, the next single-line code takes several hours to
    run. Next, we will apply the `missForest` function, save the imputed object for
    future reference, and create the test and training dataset with imputed values:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`474`个值不能被忽视。缺失数据插补是填充缺失值的重要方法。尽管EM算法是实现这一目标的一种流行方法，但我们将应用随机森林技术来模拟缺失观测值。`missForest`包在第4章中介绍，即*随机森林*，并使用一个示例来模拟缺失值。我们将应用此函数到房价数据框上。由于此函数中默认选择的变量数是`mtry=5`，而我们房价中有68个变量，因此用于分割节点的变量数将更改为大约p/3，因此在下一个R块中可以看到`mtry=20`的选项。在8GB
    RAM的机器上，下一行代码需要运行几个小时。接下来，我们将应用`missForest`函数，保存插补对象以供将来参考，并使用插补值创建测试和训练数据集：'
- en: '[PRE8]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The reader should certainly run the `missForest` code line on their local machine.
    However, to save time, the reader can also skip the line and then load the `ht_imp`
    and `htest_imp` objects from the code bundle. The next section will show a way
    of visualizing a large dataset and two data reduction methods.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 读者当然应该在他们的本地机器上运行`missForest`代码行。然而，为了节省时间，读者也可以跳过这一行，然后从代码包中加载`ht_imp`和`htest_imp`对象。下一节将展示一种可视化大数据集和两种数据降维方法的方式。
- en: Visualization and variable reduction
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化和变量降维
- en: 'In the previous section, the housing data underwent a lot of analytical pre-processing,
    and we are now ready to further analyze this. First, we begin with visualization.
    Since we have a lot of variables, the visualization on the R visual device is
    slightly difficult. As seen in earlier chapters, to visualize the random forests
    and other large, complex structures, we will initiate a PDF device and store the
    graphs in it. In the housing dataset, the main variable is the housing price and
    so we will first name the output variable `SalePrice`. We need to visualize the
    data in a way that facilitates the relationship between the numerous variables
    and the `SalePrice`. The independent variables can be either numeric or categorical.
    If the variables are numeric, a scatterplot will indicate the kind of relationship
    between the variable and the `SalePrice` regressand. If the independent variable
    is categorical/factor, we will visualize the boxplot at each level of the factor.
    The `pdf`, `plot`, and `boxplot` functions will help in generating the required
    plots:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，房价数据经历了大量的分析预处理，我们现在准备进一步分析它。首先，我们从可视化开始。由于我们有大量的变量，R可视化设备上的可视化稍微有些困难。正如前几章所见，为了可视化随机森林和其他大型、复杂结构，我们将初始化一个PDF设备并将图表存储在其中。在房价数据集中，主要变量是房价，因此我们将首先将输出变量命名为`SalePrice`。我们需要以方便展示众多变量与`SalePrice`之间的关系的方式来可视化数据。自变量可以是数值型或分类型。如果变量是数值型，散点图将表明变量与`SalePrice`回归量之间的关系。如果自变量是分类型/因子型，我们将对因子的每个级别可视化箱线图。`pdf`、`plot`和`boxplot`函数将有助于生成所需的图表：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `ht_imp` object is loaded from the `ht_imp_author.Rdata` file. Note that,
    if you run the `missForest` function on your own and work on that file, then the
    results will be different from `ht_imp_author.Rdata`. The `pdf` function is known
    to initiate a file of the same name, as seen many times earlier. For the numeric
    variable, the `if` condition is checked and a scatter plot is displayed with the
    `xlab` taking the actual name of the variable as a name for the label along the
    *x* axis. The `title` function slaps the output of the `paste` function, and the
    `paste` function ensures that we have a suitable title for the generated plot.
    Similar conditions are tested for the factor variables. We will now look at some
    of the interesting plots. The first plot of `SalePrice` with `MSSubClass (`see
    the `Visualizing_Housing_Data.pdf` file) is the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '`ht_imp`对象是从`ht_imp_author.Rdata`文件中加载的。注意，如果你在自己的机器上运行`missForest`函数并在此文件上工作，那么结果将与`ht_imp_author.Rdata`不同。`pdf`函数已知可以启动一个同名文件，如之前多次所见。对于数值变量，检查`if`条件，并显示散点图，其中`xlab`使用变量的实际名称作为*x*轴上的标签名称。`title`函数将`paste`函数的输出贴上，而`paste`函数确保我们有一个适合生成的图的标题。对于因子变量，也测试了类似的条件。我们现在将查看一些有趣的图表。`SalePrice`与`MSSubClass`的第一个图表（见`Visualizing_Housing_Data.pdf`文件）如下：'
- en: '![Visualization and variable reduction](img/00372.jpeg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![可视化与变量减少](img/00372.jpeg)'
- en: 'Figure 1: Scatter Plot of Sales Price against MSSubClass'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：销售价格与MSSubClass的散点图
- en: Note
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note here that although we specified the `MSSubClass` variable as a numeric
    variable, the scatterplot does not give the same impression. Here, the values
    of the `MSSubClass` variable are cluttered around a specific point and then the
    scale jumps to the next value.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，尽管我们指定了`MSSubClass`变量为数值变量，但散点图并没有给出同样的印象。在这里，`MSSubClass`变量的值围绕一个特定的点杂乱无章，然后尺度跳到下一个值。
- en: 'In short, it does not appear to be a continuous variable and this can be easily
    verified using the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，它似乎不是一个连续变量，这可以通过以下方式轻松验证：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '**Exercise**: The reader should convert the `MSSubClass` variable to a factor
    and then apply `Truncate_Factor` to reduce the noise. Identify other numeric variables
    exhibiting this property in the `Visualizing_Housing_Data.pdf` file.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：读者应将`MSSubClass`变量转换为因子，然后应用`Truncate_Factor`以减少噪声。在`Visualizing_Housing_Data.pdf`文件中识别其他表现出此特性的数值变量。'
- en: 'Let''s now look at the boxplot for the `MSZoning` factor variable:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看`MSZoning`因子变量的箱线图：
- en: '![Visualization and variable reduction](img/00373.jpeg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![可视化与变量减少](img/00373.jpeg)'
- en: 'Figure 2: Box plots of Sales Price at Three Levels of MSZoning'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：MSZoning三个级别的销售价格箱线图
- en: The points beyond the whiskers indicate the presence of outliers. However, with
    complex problems, the interpretation is also likely to go awfully wrong. The notches
    are a useful trick in the display of boxplots. If the notches do not overlap for
    two levels of variables, it means that the levels are significant and the information
    is therefore useful, as seen in the display of the boxplot of `SalePrice` against
    the `MSZoning` levels.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 超出胡须的点表示存在异常值。然而，对于复杂问题，解释也可能非常错误。缺口是箱线图显示中的有用技巧。如果两个变量级别的缺口不重叠，则表示级别是显著的，因此信息是有用的，如`SalePrice`与`MSZoning`级别箱线图的显示所示。
- en: 'The next display of the scatterplot of `SalePrice` against `LotArea` is taken
    up:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来展示的是`SalePrice`与`LotArea`的散点图：
- en: '![Visualization and variable reduction](img/00374.jpeg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![可视化与变量减少](img/00374.jpeg)'
- en: 'Figure 3: Scatter Plot of Sales Price against Lot Area'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：销售价格与Lot Area的散点图
- en: 'Clearly, the scatterplot shows that there is no meaningful relationship between
    the two variables `SalePrice` and `LotArea`. A different type of display is seen
    between `SalePrice` and `TotalBsmtSF` in the following figure:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 明显地，散点图显示这两个变量`SalePrice`和`LotArea`之间没有有意义的关系。在下面的图中，`SalePrice`与`TotalBsmtSF`之间可以看到不同类型的显示：
- en: '![Visualization and variable reduction](img/00375.jpeg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![可视化与变量减少](img/00375.jpeg)'
- en: 'Figure 4: Scatter Plot of Sales Price against TotalBsmtSF'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：销售价格与TotalBsmtSF的散点图
- en: 'We can clearly see an outlier in the `TotalBsmtSF` value at the extreme right
    of the figure. There is also a cluttering of values at 0 with `TotalBsmtSF`, which
    might be controlled by some other variable. Alternatively, it may be discovered
    that there is a zero-inflation of the variable and it therefore could be a mixture
    variable. Similarly, all other plots can be interpreted. The correlation between
    the `SalePrice` and other numeric variables is obtained next:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以清楚地看到图的最右侧`TotalBsmtSF`值存在一个异常值。同时，`TotalBsmtSF`在0值处也存在值的杂乱，这可能是其他某个变量的控制结果。或者，可能会发现该变量存在零膨胀，因此它可能是一个混合变量。同样，所有其他图表也可以进行解释。接下来获得`SalePrice`与其他数值变量之间的相关性：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Exercise**: Interpret all the relationships in the `Visualizing_Housing_Data.pdf`
    file and sort the correlations by their absolute value in the preceding R code.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：解释`Visualizing_Housing_Data.pdf`文件中的所有关系，并按照其绝对值在前面R代码中对相关性进行排序。'
- en: We made use of the variable of interest for the visualization, and in turn this
    led to useful insights. As previously stated, *p = 68* is a lot of covariates/independent
    variables. With big data, the complexity will increase in the north direction,
    and it is known that for many practical applications we have thousands of independent
    variables. While most visualization techniques are insightful, a shortcoming is
    that we seldom get insights into higher order relationships. For instance, when
    it comes to three or more variables, a relationship is seldom richly brought out
    in graphical displays. It is then important to deploy methods that will reduce
    the number of variables without being at the expense of information. The two methods
    of data reduction to be discussed here are **principal component analysis** and
    **variable clustering**.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了感兴趣的变量进行可视化，这进而导致了有价值的见解。正如之前所述，*p = 68*意味着有很多协变量/独立变量。在大数据中，复杂性将在北方方向增加，而且众所周知，对于许多实际应用，我们拥有数千个独立变量。虽然大多数可视化技术都有洞察力，但一个缺点是我们很少能深入了解高阶关系。例如，当涉及到三个或更多变量时，在图形显示中很少能充分展现它们之间的关系。因此，部署那些在不牺牲信息的情况下减少变量数量的方法是重要的。这里将要讨论的两种数据降维方法是**主成分分析**和**变量聚类**。
- en: '**Principal Component Analysis** (**PCA**) is a method drawn from the larger
    pool of **multivariate statistics**. This is useful in data reduction as, given
    the original number of variables, it tries to give a new set of variables that
    covers most of the variance of the original data in as few new variables as possible.
    A brief explanation of PCA is given here.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是从**多元统计**的更广泛领域抽取的一种方法。在数据降维方面，这种方法很有用，因为它在给定原始变量数量的情况下，试图用尽可能少的新变量来覆盖原始数据的大部分方差。这里简要介绍了PCA。'
- en: Suppose we have a random vector of observations ![Visualization and variable
    reduction](img/00376.jpeg). Given the random vector ![Visualization and variable
    reduction](img/00377.jpeg), PCA finds a new vector of *principal components* ![Visualization
    and variable reduction](img/00378.jpeg) such that each *Yi* is a linear combination
    of ![Visualization and variable reduction](img/00379.jpeg). Furthermore, the principal
    components are such that the variance of *Y1* is higher than the variance of *Y2*
    and both are uncorrelated; the variance of *Y2* is higher than the variance of
    *Y3* and *Y1; Y2* and *Y3* are uncorrelated, and so forth. This relates to ![Visualization
    and variable reduction](img/00380.jpeg), none of which are correlated with each
    other. The principal components are set up so that most of the variance of ![Visualization
    and variable reduction](img/00381.jpeg) is accumulated in the first few principal
    components (see Chapter 15 of Tattar, et al. (2016) for more information on this).
    As a result, we can achieve a lot of data reduction. However, the fundamental
    premise of PCA is that ![Visualization and variable reduction](img/00382.jpeg)
    is a vector of continuous random variables. In our dataset, we also have factor
    variables. Consequently, we can't use PCA for our purposes. A crude method is
    to ignore the factor variables and simply run the data reduction on the continuous
    variables. Instead, we would use `factor analysis for mixed data`, and the software
    functions to carry this out are available in the `FactoMineR` package.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个观测值的随机向量 ![可视化与变量降维](img/00376.jpeg)。给定随机向量 ![可视化与变量降维](img/00377.jpeg)，主成分分析（PCA）找到一个新的主成分向量
    ![可视化与变量降维](img/00378.jpeg)，使得每个 *Yi* 都是 ![可视化与变量降维](img/00379.jpeg) 的线性组合。此外，主成分满足
    *Y1* 的方差高于 *Y2* 的方差，且两者不相关；*Y2* 的方差高于 *Y3* 和 *Y1* 的方差，*Y2* 和 *Y3* 不相关，以此类推。这与
    ![可视化与变量降维](img/00380.jpeg) 有关，它们之间都不相关。主成分被设置成使得 ![可视化与变量降维](img/00381.jpeg)
    的大部分方差累积在前几个主成分中（关于此信息的更多信息，请参阅 Tattar 等人（2016）的第15章）。因此，我们可以实现大量的数据降维。然而，PCA的基本前提是
    ![可视化与变量降维](img/00382.jpeg) 是一个连续随机变量的向量。在我们的数据集中，我们也有因子变量。因此，我们不能为了我们的目的使用PCA。一种粗略的方法是忽略因子变量，并在连续变量上简单地执行数据降维。相反，我们会使用
    `混合数据的因子分析`，并且 `FactoMineR` 软件包中提供了执行此操作的软件功能。
- en: 'Since data reduction only needs to be performed on the covariates and we do
    not have longitudinal data, the data reduction is applied on the entire set of
    observations available, and not only on the training dataset. The rationale for
    carrying out the data reduction on the entire dataset is the same as for truncating
    the number of levels of a factor variable. The `housing_impute` data frame is
    available in `housing_covariates_impute.Rdata`. We will first load it and then
    apply the `FAMD` function to carry out the factor analysis for mixed data:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据降维只需要在协变量上执行，而我们没有纵向数据，因此数据降维应用于所有可用的观测值集，而不仅仅是训练数据集。在整份数据集上执行数据降维的理由与截断因子变量级别的数量相同。`housing_impute`
    数据框可在 `housing_covariates_impute.Rdata` 中找到。我们首先将其加载，然后应用 `FAMD` 函数来执行混合数据的因子分析：
- en: '[PRE12]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the `FAMD` function, the `ncp` option is set as equal to 68, since that
    is the number of variables we have. We would also like to look at how the principal
    components respond to the dataset. If the `graph=TRUE` option is selected, the
    function will display the related graphs. The `colnames` of `housing_cov_famd$eig`
    is changed as the default names don''t do justice to the output it generates.
    We can see from the Eigen value analysis that the overall 68 components do not
    complete with the entire variation available in the data. Furthermore, even for
    the 50% of variance explained by the components, we need to pick 26 of them. As
    a consequence, the data reduction here does not seem to be productive. However,
    this does not mean that performance will be poor in the next set of analyses.
    When applying the `pareto.chart` function from the quality control package `qcc,
    on frequency data` gives a Pareto chart. As demonstrated by the percentages, it
    is clear that if we need 90% of the variance in the original variables to be explained
    by the principal components, then we will need nearly 60 principal components.
    Consequently, the number of variables reduced is only 8 and the interpretation
    is also an additional complexity. This is not good news. However, we will still
    save the data of principal components:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在`FAMD`函数中，`ncp`选项被设置为68，因为这是我们拥有的变量数量。我们还想看看主成分如何响应数据集。如果选择`graph=TRUE`选项，函数将显示相关图表。`housing_cov_famd$eig`的`colnames`被更改为默认名称，因为这些名称并不能公正地反映它生成的输出。我们可以从特征值分析中看到，总共68个成分并没有完全覆盖数据中可用的全部变异。此外，即使是解释了50%变异的成分，我们也需要从中选择26个。因此，这里的变量减少似乎并不富有成效。然而，这并不意味着在下一组分析中性能会差。当使用质量控制包`qcc`中的`pareto.chart`函数对频率数据进行处理时，会得到一个帕累托图。正如百分比所展示的，很明显，如果我们需要原始变量中90%的变异由主成分解释，那么我们需要近60个主成分。因此，减少的变量数量仅为8，解释也增加了一个复杂性。这并不是好消息。然而，我们仍然会保存主成分的数据：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![Visualization and variable reduction](img/00383.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![可视化与变量减少](img/00383.jpeg)'
- en: 'Figure 5: Pareto Chart for Contribution of Principal Components'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：主成分贡献的帕累托图
- en: '**Exercise**: Explore the use of the `PCAmix` function from the R package `PCAmix`
    to reduce the number of variables through principal component analysis.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：探索使用来自R包`PCAmix`的`PCAmix`函数，通过主成分分析来减少变量的数量。'
- en: Variable clustering
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 变量聚类
- en: 'Variables can be grouped together as we do with observations. To achieve this,
    we will use the `kmeansvar` function from the `ClustOfVar` package. The variable
    clustering package needs to be specified as the quantitative (numeric) variable
    separately, and the qualitative (factor) variables also need to be specified separately.
    In addition, we need to specify how many variable clusters we need. The `init`
    option helps in its specification here. The `is.numeric` and `is.factor` functions
    are used to identify the numeric and factor variables and the variable clusters
    are set up:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 变量可以像我们对观测值那样进行分组。为了实现这一点，我们将使用来自`ClustOfVar`包的`kmeansvar`函数。变量聚类包需要分别指定定量（数值）变量，定性（因子）变量也需要分别指定。此外，我们还需要指定需要多少个变量簇。`init`选项有助于在这里进行指定。`is.numeric`和`is.factor`函数用于识别数值和因子变量，并设置变量簇：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Oops! It is an error. It is important to recollect that all infrequent levels
    of factor variables have been labeled as Others. It might be the case that there
    are other levels that have the same name across variables, which is a very common
    label choice in survey data, including options such as Very Dissatisfied < Dissatisfied
    < OK < Good < Excellent. This choice of variable levels can be the same across
    multiple questions. However, we need the names of the levels to be distinct across
    all variables. A manual renaming of labels will be futile and an excessive waste
    of time. Consequently, we will approach the problem with a set of names that will
    be unique across the variables, namely the variable names themselves. The variable
    names will be concatenated with the variable levels and thus we will have distinct
    factor levels throughout. Using the `paste0` function and `mapvalues` from the
    `plyr` package, we will carry out the level renaming manipulation first and then
    apply `kmeansvar` again:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！这是一个错误。重要的是要记住，所有不常见的因子变量级别都被标记为“其他”。可能存在其他级别在多个变量中具有相同的名称，这在调查数据中是一个非常常见的标签选择，包括如“非常不满意
    < 不满意 < 一般 < 好 < 优秀”这样的选项。这种变量级别的选择可以在多个问题中相同。然而，我们需要所有变量的级别名称在所有变量中都是独特的。手动重命名标签将是徒劳的，并且是时间上的极大浪费。因此，我们将使用一组将在变量中唯一的名称来解决这个问题，即变量名称本身。变量名称将与变量级别连接，因此我们将在整个变量中拥有独特的因子级别。使用`paste0`函数和`plyr`包中的`mapvalues`，我们首先执行级别重命名操作，然后再次应用`kmeansvar`：
- en: '[PRE15]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The important question is that although we have marked the variables in groups,
    how do we use them? The answer is provided in the coefficients of the variables
    within each group. To display the coefficients, run `$coef` in adjunct to the
    `clustvar` object `Housing_VarClust`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的问题是，尽管我们已经将变量分组标记，但我们如何使用它们？答案是每个组内变量的系数。要显示系数，请在`clustvar`对象`Housing_VarClust`旁边运行`$coef`：
- en: '[PRE16]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Now, for the observations in the data, the corresponding variables are multiplied
    by the coefficients of the variable clusters to obtain a single vector for that
    variable cluster. Consequently, we will then have reduced the 68 variables to
    4 variables.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于数据中的观测值，相应的变量将与变量聚类的系数相乘，以获得该变量聚类的单个向量。因此，我们将68个变量减少到4个变量。
- en: '**Exercise**: Obtain the cluster variables for the `housing_covariates` data
    frame using the coefficients displayed previously.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：使用之前显示的系数，获取`housing_covariates`数据框的聚类变量。'
- en: The data pre-processing for the housing problem is now complete. In the next
    section, we will build the base learners for the regression data.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 房地产问题的数据预处理现在已完成。在下一节中，我们将为回归数据构建基础学习器。
- en: Regression models
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归模型
- en: Sir Francis Galton invented the simple linear regression model near the end
    of the nineteenth century. The example used looked at how a parent's height influences
    the height of their child. This study used data and laid the basis of regression
    analysis. The correlation between the height of parents and children is well known,
    and using data on 928 pairs of height measurements, a linear regression was developed
    by Galton. In an equivalent form, however, the method might have been in informal
    use before Galton officially invented it. The simple linear regression model consists
    of a single input (independent) variable and the output is also a single output.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 弗朗西斯·高尔顿爵士在十九世纪末发明了简单的线性回归模型。所用的例子是研究父母的身高如何影响孩子的身高。这项研究使用了数据，并奠定了回归分析的基础。父母和孩子的身高之间的相关性是众所周知的，高尔顿利用928对身高测量数据，开发了线性回归模型。然而，在伽尔顿正式发明之前，这种方法可能已经在非正式地使用了。简单的线性回归模型由一个单一输入（自变量）和一个单一输出组成。
- en: In this supervised learning method, the target variable/output/dependent variable
    is a continuous variable, and it can also take values in intervals, including
    non-negative and real numbers. The input/independent variable has no restrictions
    and as such it can be numeric, categorical, or in any other form we used earlier
    for the classification problem. Interestingly though, linear regression models
    started much earlier than classification regression models such as logistic regression
    models. Machine learning problems are more often conceptualized based on the classification
    problem, and the ensemble methods, especially boosting, have been developed by
    using classification as the motive. The primary reason for this is that the error
    improvisation gives a nice intuition, and the secondary reason might be due to
    famous machine learning examples such as digit recognition, spam classification,
    and so on.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种监督学习方法中，目标变量/输出/因变量是一个连续变量，它也可以取区间值，包括非负数和实数。输入/自变量没有限制，因此它可以取数值、分类或其他我们之前用于分类问题的任何形式。有趣的是，线性回归模型的出现时间比分类回归模型（如逻辑回归模型）要早得多。机器学习问题更常基于分类问题进行概念化，而集成方法，特别是提升方法，是通过将分类作为动机来开发的。主要原因在于误差改进提供了良好的直觉，次要原因可能是因为著名的机器学习示例，如数字识别、垃圾邮件分类等。
- en: The simple linear regression extension is the multiple linear regression where
    we allow more than one independent variable. We will drop the convention of simple
    and multiple regression altogether and adhere simply to regression. As a base
    learner, the linear regression model is introduced first. Interesting datasets
    will be used to kick-start the linear regression model.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 简单线性回归的扩展是多元线性回归，其中我们允许有多个独立变量。我们将完全放弃简单和多元回归的惯例，并简单地遵循回归。作为基学习器，线性回归模型首先被介绍。有趣的数据库将被用来启动线性回归模型。
- en: Linear regression model
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性回归模型
- en: 'In more formal terms, let ![Linear regression model](img/00384.jpeg) be a set
    of *p* independent variables, and *Y* be the variable of interest. We need to
    understand the regressand *Y* in terms of the regressors ![Linear regression model](img/00385.jpeg).
    The linear regression model is given by:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，设![线性回归模型](img/00384.jpeg)为包含*p*个独立变量的集合，而*Y*是感兴趣的变量。我们需要从回归变量![线性回归模型](img/00385.jpeg)的角度来理解回归变量*Y*。线性回归模型由以下公式给出：
- en: '![Linear regression model](img/00386.jpeg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归模型](img/00386.jpeg)'
- en: The relationship between *Y* and the regressors is in linear form; ![Linear
    regression model](img/00387.jpeg) is the intercept term; ![Linear regression model](img/00388.jpeg)
    are the regression coefficients; and ![Linear regression model](img/00389.jpeg)
    is the error term. It needs to be mentioned that the linearity is in terms of
    the regression coefficients. It is also important to note that the regressors
    can come in any form and can sometimes be taken as other forms, including log,
    exponential, and quadratic. The error term ![Linear regression model](img/00390.jpeg)
    is often assumed to follow the normal distribution with unknown variance and zero
    mean. More details about the linear regression model can be found in Draper and
    Smith (1999), Chatterjee and Hadi (2012), and Montgomery, et al. (2005). For information
    on the implementation of this technique using R software, see Chapter 12 of Tattar,
    et al. (2016) or Chapter 6 of Tattar (2017).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* 与回归变量之间的关系是线性的；![线性回归模型](img/00387.jpeg)是截距项；![线性回归模型](img/00388.jpeg)是回归系数；而![线性回归模型](img/00389.jpeg)是误差项。需要指出的是，这里的线性是指回归系数的线性。同时，需要注意的是，回归变量可以采取任何形式，有时也可以被看作是其他形式，包括对数、指数和二次形式。误差项![线性回归模型](img/00390.jpeg)通常假设服从具有未知方差和零均值的正态分布。关于线性回归模型的更多细节，可以参考Draper和Smith
    (1999)、Chatterjee和Hadi (2012)以及Montgomery等人(2005)的著作。关于使用R软件实现此技术的信息，请参阅Tattar等人(2016)的第12章或Tattar(2017)的第6章。'
- en: 'First, we will explain the core notions of the linear regression model using
    the Galton dataset. The data is loaded from the `RSADBE` package and, using the
    `lm` function, we can build the model:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将使用Galton数据集来解释线性回归模型的核心概念。数据是从`RSADBE`包中加载的，使用`lm`函数，我们可以构建模型：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![Linear regression model](img/00391.jpeg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归模型](img/00391.jpeg)'
- en: 'Figure 6: Height of Child against Height of Parent - Scatterplot'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：儿童身高与父母身高对比 - 散点图
- en: What does this code block tell us? First, we will load the galton data from
    the RSADBE package and then look at the `cor` correlation between the height of
    parent and child. The correlation is `0.46`, which seems to be a strong, positive
    correlation. The plot scatterplot indicates the positive correlation too, and
    consequently we proceed to build the linear regression model of the height of
    the child as a function of the height of the parent. It is advisable to look at
    the p-value associated with the model first, which in this case is given in the
    last line of `summary(cp_lm)` as `<2e-16`. The smaller p-value means that we reject
    the null hypothesis of the model being insignificant, and hence the current fitted
    model is useful. The p-values associated with the intercept and variable term
    are both `<2e-16`, and that again means that the terms are significant. The regression
    coefficient of `0.6463` implies that if a parent is an inch taller, the child's
    height would increase by a magnitude of the regression coefficient.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码块告诉我们什么？首先，我们将从RSADBE包中加载galton数据，然后查看父母和孩子的`cor`相关系数。相关系数是`0.46`，这似乎是一个强烈的正相关。散点图也表明了这种正相关，因此我们继续构建以父母高度为函数的孩子的线性回归模型。建议首先查看与模型相关的p值，在这种情况下，它在`summary(cp_lm)`的最后行给出，为`<2e-16`。较小的p值意味着我们拒绝模型无意义的零假设，因此当前的拟合模型是有用的。与截距项和变量项相关的p值都是`<2e-16`，这也意味着这些项是显著的。回归系数`0.6463`意味着如果父母身高增加一英寸，孩子的身高将增加回归系数的量级。
- en: The value of `Multiple R-squared` (technically simple R-squared) and `Adjusted
    R-squared` are both `0.21`, which is expected as we have a single variable in
    the model. The interpretation of R-squared is that if we multiply it by 100 (so
    21%, in this case), the resulting number is the percentage of variation in the
    data (height of the child) as explained by the fitted value. The higher the value
    of this metric, the better the model is. In this example, it means that the height
    of the parent explains only about 21% of the variation of the child's height.
    This means that we need to consider other variables. In this case, one starting
    point might be to consider the height of both parents. The multiple R-square value
    will keep on increasing if you add more variables, and hence it is preferable
    to use the more robust adjusted R-square value. Is it possible to obtain a perfect
    R-square, for example 1, or 100%?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '`Multiple R-squared`（技术上简单的R平方）和`Adjusted R-squared`的值都是`0.21`，这是预期的，因为我们模型中只有一个变量。R平方的解释是，如果我们将其乘以100（在这种情况下是21%），那么得到的数字就是数据（孩子的高度）变化中由拟合值解释的百分比。这个指标的值越高，模型就越好。在这个例子中，这意味着父母的高度只能解释孩子身高变化的约21%。这意味着我们需要考虑其他变量。在这种情况下，一个起点可能是考虑父母双方的高度。如果你添加更多的变量，多重R平方值将会持续增加，因此更倾向于使用更稳健的调整R平方值。是否有可能获得完美的R平方，例如1或100%？'
- en: 'A dataset named `Mantel` is available online in the bundle package, and we
    will build a linear regression model to check for its R-square. To do this, we
    import the dataset and run the `lm` function over it:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为`Mantel`的数据集在捆绑包中在线可用，我们将构建一个线性回归模型来检查其R平方。为此，我们导入数据集并对其运行`lm`函数：
- en: '[PRE18]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, we can see that the R-square is perfect. Let's have some fun before we
    embark on the serious task of analyzing the housing price data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到R平方是完美的。在我们开始分析房价数据的严肃任务之前，让我们先玩一会儿吧。
- en: 'For the `galton` dataset, we will add a new variable called `frankenstein`,
    and this variable will be the residuals from the fitted model `cp_lm`. A new dataset
    will be created, which will augment the `galton` dataset with the residuals; the
    linear model will then be fitted using the `lm` function and its R-square will
    be checked:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`galton`数据集，我们将添加一个新的变量，称为`frankenstein`，这个变量将是拟合模型`cp_lm`的残差。将创建一个新的数据集，它将使用残差来增强`galton`数据集；然后，将使用`lm`函数拟合线性模型，并检查其R平方：
- en: '[PRE19]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Don''t ignore the warning function. You may recall that such a warning function
    was not displayed for the `Mantel` dataset. This is because this warning can be
    eliminated by adding a little noise to the `frankenstein` variable, consequently
    making him more monstrous:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忽略警告函数。你可能还记得，对于`Mantel`数据集没有显示这样的警告函数。这是因为通过向`frankenstein`变量添加一点噪声，可以消除这个警告，从而使它更加可怕：
- en: '[PRE20]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We have thus mastered the art of obtaining a perfect R-square. Playtime is
    over now; let''s move on to the housing dataset. We previously saved the housing
    dataset for the train and test blocks as the `ht_imp.Rdata` and `htest_imp.Rdata`
    files. The author''s filename version has been modified by renaming the filenames
    as `_author` to make things clearer. We then separate the training block into
    training and testing ones. Then, we use the `load` function to import the data,
    partition it with the `sample` function, and then use the `lm` function to build
    the regression model:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们已经掌握了获得完美R平方的艺术。玩耍的时间已经结束，让我们继续到住房数据集。我们之前已经将住房数据集保存为训练和测试块，分别命名为`ht_imp.Rdata`和`htest_imp.Rdata`文件。作者将文件名版本修改为`_author`，以便更清晰地表示。然后，我们将训练块分为训练和测试两部分。然后，我们使用`load`函数导入数据，使用`sample`函数对其进行分区，然后使用`lm`函数构建回归模型：
- en: '[PRE21]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The accuracy assessment of the fitted linear model will be carried out after
    fitting three more base learners. The adjusted R-square value is about 87%. However,
    we have 68 variables, and we can see from the p-value of the previous summary
    that a lot of variables don't have p-values less than either 0.05 or 0.1\. Consequently,
    we need to get rid of the insignificant variables. The step function can be slapped
    on many fitted regression models to eliminate the insignificant variables while
    retaining most of the model characteristics.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在拟合三个更多基础学习器之后，将进行拟合线性模型的准确性评估。调整后的R平方值约为87%。然而，我们有68个变量，我们可以从之前的总结中的p值看到，许多变量没有p值小于0.05或0.1。因此，我们需要消除不显著的变量。`step`函数可以应用于许多拟合的回归模型，以消除不显著的变量，同时保留模型的大部分特征。
- en: 'Running the step function in the R session leads to a huge display of output
    in the console. The initial output is lost to the space restrictions. Consequently,
    the author ran the script with the option of **Compile Report from R Script in
    RStudio**, chose the option of MS Word as the report output format, and saved
    that file. An abbreviated version of the results from that file is given here:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在R会话中运行`step`函数会导致控制台中出现大量的输出。初始输出因空间限制而丢失。因此，作者使用RStudio中的**从R脚本编译报告**选项运行脚本，选择MS
    Word作为报告输出格式，并保存该文件。以下是该文件结果的简略版：
- en: '[PRE22]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The `model` is summarized as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`model`的总结如下：'
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The small module covering the `step` function is available in the `Housing_Step_LM.R`
    file and the output generated by using R Markdown is saved in the file named `Housing_Step_LM.docx`.
    The output of the `step` function runs over forty-three pages, but we don't have
    to inspect the variables left out at each step. It suffices to say that a lot
    of insignificant variables have been eliminated without losing the traits of the
    model. The accuracy assessment of the validated partition will be seen later.
    Next, we will extend the linear regression model to the nonlinear model and work
    out the neural networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 覆盖`step`函数的小模块可以在`Housing_Step_LM.R`文件中找到，使用R Markdown生成的输出保存在名为`Housing_Step_LM.docx`的文件中。`step`函数的输出超过四十三页，但我们不必检查每一步中省略的变量。只需说，已经消除了许多不显著的变量，而没有丢失模型的特征。验证分区的准确性评估将在稍后看到。接下来，我们将扩展线性回归模型到非线性模型，并解决神经网络问题。
- en: '**Exercise**: Build linear regression models using the principal component
    and variable cluster variables. Does the accuracy – the R-square – with the set
    of relevant variables improve the linear regression model?'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：使用主成分和变量聚类变量构建线性回归模型。使用相关变量集的准确性（R平方）是否提高了线性回归模型？'
- en: Neural networks
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络
- en: The neural network architecture was introduced in the *Statistical/machine learning
    models* section of [Chapter 1](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "Chapter 1. Introduction to Ensemble Techniques"), *Introduction to Ensemble Techniques*.
    Neural networks are capable of handling nonlinear relationships, the choice of
    the number of hidden neurons, the choice of transfer functions, and the learning
    rate (or decay rate) provides a great flexibility in building useful regression
    models. Haykin (2009) and Ripley (1996) provide two detailed explanations of the
    theory of neural networks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络架构在[第1章](part0012_split_000.html#BE6O2-2006c10fab20488594398dc4871637ee
    "第1章。集成技术介绍")的*统计/机器学习模型*部分中介绍，*集成技术介绍*。神经网络能够处理非线性关系，隐藏神经元数量的选择、传递函数的选择以及学习率（或衰减率）的选择在构建有用的回归模型时提供了很大的灵活性。Haykin
    (2009)和Ripley (1996)提供了神经网络理论的两个详细解释。
- en: 'We have looked at the use of neural networks for classification problems and
    have also seen the stack ensemble models in action. For the regression model,
    we need to tell the `nnet` function that the output/dependent variable is a continuous
    variable through the `linout=TRUE` option. Here, we will build a neural network
    with five hidden neurons, `size=5`, and run the function for a maximum of 100
    iterations, `maxit=100`:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探讨了神经网络在分类问题中的应用，并看到了堆叠集成模型的实际操作。对于回归模型，我们需要通过`linout=TRUE`选项告诉`nnet`函数输出/因变量是连续变量。在这里，我们将构建一个具有五个隐藏神经元的神经网络，`size=5`，并运行函数最多100次迭代，`maxit=100`：
- en: '[PRE24]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the neural network architecture is not very useful. However, sometimes
    we are asked to display what we have built. Thus, we will use the `plotnet` function
    from the `NeuralNetTools` package to generate the network. Since there are too
    many variables (68 in this case), we save the plot to the `Housing_NN.pdf` PDF
    file and the reader can open it and zoom into the plot to inspect it:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，神经网络架构并不非常实用。然而，有时我们被要求展示我们所构建的内容。因此，我们将使用来自`NeuralNetTools`包的`plotnet`函数来生成网络。由于变量太多（本例中为68个），我们将绘图保存到`Housing_NN.pdf`
    PDF 文件中，读者可以打开并放大查看：
- en: '[PRE25]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The prediction of the neural network will be performed shortly.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的预测将很快进行。
- en: '**Exercise 1**: Build neural networks with different decay options; the default
    is 0\. Vary the decay value in the range of 0-0.2, with increments of 0.01, 0.05,
    and so on.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**：使用不同的衰减选项构建神经网络；默认值为0。在0-0.2的范围内变化衰减值，增量分别为0.01、0.05等。'
- en: '**Exercise 2**: Improve the neural network fit using `reltol` values, decay
    values, and a combination of these variables.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**：使用`reltol`值、衰减值以及这些变量的组合来改进神经网络拟合。'
- en: Regression tree
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归树
- en: The regression tree forms the third base learner for the housing dataset and
    provides the decision tree structure for the regression problems. The advantages
    of the decision tree naturally get carried over to the regression tree. As seen
    in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, the options for many decision trees are also
    available for the regression tree.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 回归树构成了住房数据集的第三个基学习器，并为回归问题提供了决策树结构。决策树的自然优势也自然地转移到回归树上。如[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "第3章。Bagging")中所述，*Bagging*，许多决策树的选项也适用于回归树。
- en: 'We will use the `rpart` function from the `rpart` library with the default
    settings to build the regression tree. Using the plot and text functions, we set
    up the regression tree:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`rpart`库中的`rpart`函数和默认设置来构建回归树。使用绘图和文本函数，我们设置了回归树：
- en: '[PRE26]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![Regression tree](img/00392.jpeg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![回归树](img/00392.jpeg)'
- en: 'Figure 7: Regression Tree for the Sales Price of Houses'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：房屋销售价格的回归树
- en: 'Which variables are important here? The answer to this is provided by the variable
    importance metric. We extract the variable importance from `HT_rtree` and the
    variable with the highest bar length is the most important of all the variables.
    We will now use the `barplot` function for the `HT_rtree`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 哪些变量在这里很重要？这个答案由变量重要性指标提供。我们从`HT_rtree`中提取变量重要性，条形长度最高的变量是所有变量中最重要的。现在我们将使用`barplot`函数对`HT_rtree`进行绘图：
- en: '[PRE27]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![Regression tree](img/00393.jpeg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![回归树](img/00393.jpeg)'
- en: 'Figure 8: Variable Importance of the Regression Tree of Housing Model'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：住房模型回归树的变量重要性
- en: '**Exercise**: Explore the pruning options for a regression tree.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：探索回归树的剪枝选项。'
- en: Next, we will look at the performance of the three base learners for the validation
    dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看三个基础学习器在验证数据集上的性能。
- en: Prediction for regression models
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归模型的预测
- en: 'We separated the housing training dataset into two sections: train and validate.
    Now we will use the built models and check how well they are performing. We will
    do this by looking at the MAPE metric : `|Actual-Predicted|/Actual`. Using the
    `predict` function with the `newdata` option, the predictions are first obtained,
    and then the MAPE is calculated for the observations in the validated section
    of the data:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将房价训练数据集分为两个部分：训练和验证。现在我们将使用构建的模型并检查它们的性能如何。我们将通过查看MAPE指标（`|实际-预测|/实际`）来完成这项工作。使用带有`newdata`选项的`predict`函数，首先获得预测值，然后计算数据验证部分观测值的MAPE：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The linear regression model `HT_LM_01` and the most efficient linear model (by
    AIC) `HT_LM_Final` both give the same accuracy (up to two digits) and the MAPE
    is `0.11` for these two models. The neural network model `HT_NN` (with five hidden
    neurons) results in a MAPE of `0.37`, which is a bad result. This again reinforces
    the well-known fact that complexity does not necessarily mean accuracy. The accuracy
    of the regression tree `HT_rtree` is `0.17`.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型`HT_LM_01`和最有效的线性模型（根据AIC）`HT_LM_Final`都给出了相同的准确度（精确到两位数），这两个模型的MAPE为`0.11`。具有五个隐藏神经元的神经网络模型`HT_NN`的MAPE为`0.37`，这是一个不好的结果。这再次证实了众所周知的事实：复杂性并不一定意味着准确性。回归树`HT_rtree`的准确度为`0.17`。
- en: 'The predicted prices are visualized in the following program:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 预测的价格在以下程序中进行了可视化：
- en: '[PRE29]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '![Prediction for regression models](img/00394.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![回归模型的预测](img/00394.jpeg)'
- en: 'Figure 9: Predicting Housing Sales Prices'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：预测房价
- en: Now we have set up the base learners, it is time to build the ensembles out
    of them. We will now build ensemble models based on the homogeneous base learner
    of the decision tree.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了基础学习器，是时候构建它们的集成模型了。我们将基于决策树的同质基础学习器构建集成模型。
- en: Bagging and Random Forests
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Bagging和随机森林
- en: '[Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, and [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*, demonstrate how to improve the
    stability and accuracy of the basic decision tree. In this section, we will primarily
    use the decision tree as base learners and create an ensemble of trees in the
    same way that we did in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, and [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee "第3章。Bagging")，*Bagging*，和[第4章](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "第4章。随机森林*")，*随机森林*，展示了如何提高基本决策树的稳定性和准确性。在本节中，我们将主要使用决策树作为基础学习器，并以与[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "第3章。Bagging")和[第4章](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "第4章。随机森林*")中相同的方式创建树集成。'
- en: 'The `split` function is the primary difference between bagging and random forest
    algorithms for classification and regression trees. Thus, unsurprisingly, we can
    continue to use the same functions and packages for the regression problem as
    the counterparts that were used in the classification problem. We will first use
    the `bagging` function from the `ipred` package to set up the bagging algorithm
    for the housing data:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`split`函数是Bagging和随机森林算法在分类和回归树中的主要区别。因此，不出所料，我们可以继续使用与分类问题中使用的相同函数和包来处理回归问题。我们将首先使用`ipred`包中的`bagging`函数来为房价数据设置Bagging算法：'
- en: '[PRE30]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The trees in the bagging object can be saved to a PDF file in the same way
    as in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 与[第3章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee "第3章。Bagging")中的相同方式一样，Bagging对象中的树可以被保存到PDF文件中：*Bagging*：
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Since variable importance is not given directly by the `ipred` package, and
    it is always an important measure to know which variables are important, we run
    a similar loop and program to what was used in [Chapter 3](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "Chapter 3. Bagging"), *Bagging*, to get the variable importance plot:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `ipred` 包没有直接给出变量重要性，而了解哪些变量重要始终是一个重要的衡量标准，因此我们运行了一个与 [第 3 章](part0027_split_000.html#PNV61-2006c10fab20488594398dc4871637ee
    "第 3 章。Bagging") 中使用的类似循环和程序，即 *Bagging*，以获取变量重要性图：
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![Bagging and Random Forests](img/00395.jpeg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging 和随机森林](img/00395.jpeg)'
- en: 'Figure 10: Variable Importance Plot of the Bagging Algorithm for the Housing
    Data'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10：住房数据 bagging 算法的变量重要性图
- en: '**Exercise**: Compare *Figure 10* with *Figure 8* to decide whether we have
    an overfitting problem in the regression tree.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：** 比较 *图 10* 和 *图 8*，以决定回归树中是否存在过拟合问题。'
- en: 'Did bagging improve the prediction performance? This is the important criterion
    that we need to evaluate. Using the `predict` function with the `newdata` option,
    we again calculate the MAPE as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: bagging 是否提高了预测性能？这是我们需要评估的重要标准。使用带有 `newdata` 选项的 `predict` 函数，我们再次计算 MAPE，如下所示：
- en: '[PRE33]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The simple regression tree had a MAPE of 17%, and now it is down to 13%. This
    leads us into the next exercise.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 简单回归树的 MAPE 为 17%，现在降至 13%。这引出了下一个练习。
- en: '**Exercise:** Use some of the pruning options with `rpart.control` to improve
    the performance of bagging.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：** 使用 `rpart.control` 中的某些剪枝选项来提高 bagging 的性能。'
- en: 'The next step following bagging is the random forest. We will use the `randomForest`
    function from the package of the same name. Here, we explore 500 trees for this
    forest. For the regression data, the default setting for the number of covariates
    to be randomly sampled for splitting a node is `mtry = p/3`, where `p` is the
    number of covariates. We will use the default choice. The `randomForest` function
    is used to set up the tree ensemble and then `plot_rf`, defined in [Chapter 4](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "Chapter 4. Random Forests"), *Random Forests*, is used to save the trees of the
    forest to a PDF file:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 bagging 之后，下一步是随机森林。我们将使用同名的 `randomForest` 函数。在这里，我们探索了 500 棵树来构建这个森林。对于回归数据，节点分裂时随机采样的协变量数量的默认设置是
    `mtry = p/3`，其中 `p` 是协变量的数量。我们将使用默认选择。`randomForest` 函数用于设置树集成，然后使用在 [第 4 章](part0033_split_000.html#VF2I1-2006c10fab20488594398dc4871637ee
    "第 4 章。随机森林") 中定义的 `plot_rf` 函数，即 *随机森林*，将森林的树保存到 PDF 文件中：
- en: '[PRE34]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The variable importance plot for the random forest is given next:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 下图给出了随机森林的变量重要性图：
- en: '![Bagging and Random Forests](img/00396.jpeg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![Bagging 和随机森林](img/00396.jpeg)'
- en: 'Figure 11: Variable Importance of the Random Forest for the Housing Data'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11：住房数据的随机森林变量重要性
- en: '**Exercise**: Find the difference between the two variable importance plots
    `%lncMSE` and `IncNodePurity`. Also, compare the variable importance plot of the
    random forest with the bagging plot and comment on this.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：** 找出两个变量重要性图 `%lncMSE` 和 `IncNodePurity` 之间的差异。同时，比较随机森林的变量重要性图和 bagging
    图，并对此进行评论。'
- en: 'How accurate is our forest? Using the `predict` function, we will get our answer:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的森林有多准确？使用 `predict` 函数，我们将得到答案：
- en: '[PRE35]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This is simply brilliant stuff, and the random forest has significantly improved
    the accuracy by drastically reducing the MAPE from `0.17` to `0.038`. This is
    the outright winner of all the models built thus far.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 这真是太棒了，随机森林通过大幅降低 MAPE 从 `0.17` 到 `0.038` 显著提高了精度。这是迄今为止构建的所有模型中的绝对赢家。
- en: '**Exercise**: In spite of the increased accuracy, try to build forests based
    on pruned trees and calculate the accuracy.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：** 尽管精度有所提高，但尝试基于剪枝树构建森林，并计算精度。'
- en: Let's see how boosting changes the performance of the trees next.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看提升如何改变树的性能。
- en: Boosting regression models
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升回归模型
- en: '[Chapter 5](part0042_split_000.html#181NK1-2006c10fab20488594398dc4871637ee
    "Chapter 5. The Bare Bones Boosting Algorithms"), *Boosting*, introduced the boosting
    method for trees when we had a categorical variable of interest. The adaptation
    of boosting to the regression problem requires lot of computational changes. For
    more information, refer to papers by Zemel and Pitassi (2001), [http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf](http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf)
    , or Ridgeway, et al. (1999), [http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf](http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](part0042_split_000.html#181NK1-2006c10fab20488594398dc4871637ee "第5章。裸骨提升算法")《提升》介绍了当有感兴趣的分类变量时，提升法对树的提升方法。将提升法应用于回归问题需要大量的计算改变。更多信息，请参阅Zemel和Pitassi（2001）的论文，[http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf](http://papers.nips.cc/paper/1797-a-gradient-based-boosting-algorithm-for-regression-problems.pdf)，或Ridgeway等人（1999）的论文，[http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf](http://dimacs.rutgers.edu/Research/MMS/PAPERS/BNBR.pdf)。'
- en: 'The `gbm` function from the `gbm` library will be used to boost the weak learners
    generated by using random forests. We generate a thousand trees, `n.trees=1e3`,
    and use the `shrinkage` factor of `0.05`, and then boost the regression trees
    using the gradient boosting algorithm for regression data:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用`gbm`库中的`gbm`函数来提升由随机森林生成的弱学习器。我们生成了一千棵树，`n.trees=1e3`，并使用`shrinkage`因子为`0.05`，然后使用梯度提升算法对回归数据进行提升：
- en: '[PRE36]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This summary gives the variable importance in descending order. The performance
    of the boosting can be looked into using the `gbm.perf` function and since our
    goal was always to generate a technique that performs well on new data, the out-of-bag
    curve is also laid over as follows:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 本总结按降序给出了变量的重要性。可以通过`gbm.perf`函数查看提升法的性能，由于我们的目标始终是生成对新数据表现良好的技术，因此还叠加了以下出袋曲线：
- en: '[PRE37]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '![Boosting regression models](img/00397.jpeg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![提升回归模型](img/00397.jpeg)'
- en: 'Figure 12: Boosting Convergence for the Housing Data'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：住房数据的提升收敛
- en: 'The boosting method has converged at iteration **137**. Next, we look at the
    performance of the boosting procedure on the validated data:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提升法在第**137**次迭代时收敛。接下来，我们查看提升法在验证数据上的性能：
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The MAPE has decreased from 17% to 11%. However, the random forest continues
    to be the most accurate model thus far.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: MAPE已从17%降至11%。然而，随机森林仍然是迄今为止最准确的模型。
- en: Stacking methods for regression models
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归模型的堆叠方法
- en: 'Linear regression models, neural networks, and regression trees are the three
    methods that will be stacked here. We will require the `caret` and `caretEnsemble`
    packages to do this task. The stacked ensemble methods have been introduced in
    detail in [Chapter 7](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee
    "Chapter 7. The General Ensemble Technique"), *The General Ensemble Technique*.
    First, we specify the control parameters for the training task, specify the list
    of algorithms, and create the stacked ensemble:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归模型、神经网络和回归树是这里将要堆叠的三个方法。我们将需要`caret`和`caretEnsemble`包来完成这项任务。堆叠集成方法已在[第7章](part0051_split_000.html#1GKCM1-2006c10fab20488594398dc4871637ee
    "第7章。一般集成技术")《一般集成技术》中详细介绍。首先，我们指定训练任务的控制参数，指定算法列表，并创建堆叠集成：
- en: '[PRE39]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The neural network is specified through `caretModelSpec`. `Emodels` needs to
    be resampled for further analysis:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络通过`caretModelSpec`指定。`Emodels`需要重新采样以进行进一步分析：
- en: '[PRE40]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The `dotplot` is displayed next:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来显示`dotplot`：
- en: '![Stacking methods for regression models](img/00398.jpeg)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![回归模型的堆叠方法](img/00398.jpeg)'
- en: 'Figure 13: R-square, MAE, and RMSE for Housing Data'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图13：住房数据的R-square、MAE和RMSE
- en: 'We can see from *Figure 13* that the R-square is similar for the three models,
    although MAE and RMSE are significantly different across the three models. The
    model correlations can be found using the `modelCor` function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从*图13*中看到，三个模型的R-square相似，尽管三个模型的MAE和RMSE有显著差异。可以使用`modelCor`函数找到模型的相关性：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We now apply the ensemble method to the validation data:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将集成方法应用于验证数据：
- en: '[PRE42]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the results from the neural network are default and we did not specify
    the size of the hidden layers. The MAPE of 16% is not desirable and we are better
    off using the random forest ensemble.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，神经网络的默认结果，我们没有指定隐藏层的大小。16%的MAPE并不理想，我们最好使用随机森林集成。
- en: '**Exercise**: Perform the stacked ensemble methods on the principal components
    and variable cluster data.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习**：对主成分和变量聚类数据进行堆叠集成方法的操作。'
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we extended most of the models and methods learned earlier
    in the book. The chapter began with a detailed example of housing data, and we
    carried out the visualization and pre-processing. The principal component method
    helps in reducing data, and the variable clustering method also helps with the
    same task. Linear regression models, neural networks, and the regression tree
    were then introduced as methods that will serve as base learners. Bagging, boosting,
    and random forest algorithms are some methods that helped to improve the models.
    These methods are based on homogeneous ensemble methods. This chapter then closed
    with the stacking ensemble method for the three heterogeneous base learners.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们扩展了书中早期学习的大多数模型和方法。本章从住房数据的详细示例开始，我们进行了可视化和预处理。主成分方法有助于减少数据，变量聚类方法也有助于完成同样的任务。随后介绍了线性回归模型、神经网络和回归树作为基学习器的方法。袋装、提升和随机森林算法是一些有助于改进模型的方法。这些方法基于同质集成方法。本章最后以堆叠集成方法为三个异质基学习器进行了总结。
- en: A different data structure of censored observations will be the topic of the
    next chapter. Such data is referred to as survival data, and it commonly appears
    in the study of clinical trials.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将讨论不同结构的有删失观测数据，这种数据被称为生存数据，它通常出现在临床试验的研究中。
