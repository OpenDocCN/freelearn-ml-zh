- en: '*Chapter 9*: XGBoost Kaggle Masters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第9章*：XGBoost Kaggle大师'
- en: In this chapter, you will learn valuable tips and tricks from `VotingClassifier`
    and `VotingRegressor` to build non-correlated machine learning ensembles, and
    the advantages of **stacking** a final model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将学习从`VotingClassifier`和`VotingRegressor`中获得的宝贵技巧和窍门，以构建非相关的机器学习集成模型，并了解**堆叠**最终模型的优势。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要内容：
- en: Exploring Kaggle competitions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索Kaggle竞赛
- en: Engineering new columns of data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建新的数据列
- en: Building non-correlated ensembles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建非相关集成模型
- en: Stacking final models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠最终模型
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter09)找到。
- en: Exploring Kaggle competitions
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索Kaggle竞赛
- en: '"I used only XGBoost (tried others but none of them performed well enough to
    end up in my ensemble)."'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '"我只用了XGBoost（尝试过其他的，但没有一个能达到足够的表现以最终加入我的集成模型中）。"'
- en: – *Qingchen Wang, Kaggle Winner*
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: – *Qingchen Wang，Kaggle获胜者*
- en: ([https://www.cnblogs.com/yymn/p/4847130.html](https://www.cnblogs.com/yymn/p/4847130.html))
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://www.cnblogs.com/yymn/p/4847130.html](https://www.cnblogs.com/yymn/p/4847130.html))
- en: In this section, we will investigate Kaggle competitions by looking at a brief
    history of Kaggle competitions, how they are structured, and the importance of
    a hold-out/test set as distinguished from a validation/test set.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过回顾Kaggle竞赛的简短历史、它们的结构以及区分验证/测试集与保留/测试集的重要性，来探讨Kaggle竞赛。
- en: XGBoost in Kaggle competitions
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBoost在Kaggle竞赛中的表现
- en: XGBoost built its reputation as the leading machine learning algorithm on account
    of its unparalleled success in winning Kaggle competitions. XGBoost often appeared
    in winning ensembles along with deep learning models such as **neural networks**,
    in addition to winning outright. A sample list of XGBoost Kaggle competition winners
    appears on the *Distributed (Deep) Machine Learning Community* web page at [https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions).
    For a list of more XGBoost Kaggle competition winners, it's possible to sort through
    *Winning solutions of Kaggle competitions* ([https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions))
    to research the winning models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost因其在赢得Kaggle竞赛中的无与伦比的成功而建立了作为领先机器学习算法的声誉。XGBoost常常与深度学习模型如**神经网络**一起出现在获胜的集成模型中，除了单独获胜之外。一个XGBoost
    Kaggle竞赛获胜者的样例列表出现在*分布式（深度）机器学习社区*的网页上，[https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions](https://github.com/dmlc/xgboost/tree/master/demo#machine-learning-challenge-winning-solutions)。要查看更多XGBoost
    Kaggle竞赛获胜者，可以通过*Winning solutions of Kaggle competitions* ([https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions](https://www.kaggle.com/sudalairajkumar/winning-solutions-of-kaggle-competitions))来研究获胜模型。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意事项
- en: While XGBoost is regularly featured among the winners, other machine learning
    models make appearances as well.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然XGBoost经常出现在获胜者中，但其他机器学习模型也有出现。
- en: As mentioned in [*Chapter 5*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117),
    *XGBoost Unveiled*, Kaggle competitions are machine learning competitions where
    machine learning practitioners compete against one another to obtain the best
    possible score and win cash prizes. When XGBoost exploded onto the scene in 2014
    during the *Higgs Boson Machine Learning Challenge*, it immediately jumped the
    leaderboard and became one of the most preferred machine learning algorithms in
    Kaggle competitions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如[*第5章*](B15551_05_Final_NM_ePUB.xhtml#_idTextAnchor117)《XGBoost揭秘》中提到的，Kaggle竞赛是机器学习竞赛，机器学习从业者相互竞争，争取获得最佳分数并赢得现金奖励。当XGBoost在2014年参加*希格斯玻色子机器学习挑战*时，它迅速跃升至排行榜顶端，并成为Kaggle竞赛中最受欢迎的机器学习算法之一。
- en: Between 2014 and 2018, XGBoost consistently outperformed the competition on
    tabular data—data organized in rows and columns as contrasted with unstructured
    data such as images or text, where neural networks had an edge. With the emergence
    of **LightGBM** in 2017, a lightning-fast Microsoft version of gradient boosting,
    XGBoost finally had some real competition with tabular data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在2014年到2018年间，XGBoost 一直在表格数据（以行和列组织的数据，相对于图像或文本等非结构化数据，神经网络在这些领域占有优势）上表现出色。随着
    **LightGBM** 在2017年问世，这款由微软推出的快速梯度提升算法，XGBoost 在表格数据上终于遇到了真正的竞争者。
- en: 'The following introductory paper, *LightGBM: A Highly Efficient Gradient Boosting
    Decision Tree*, written by eight authors, is recommended for an introduction to
    LightGBM: [https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '以下是由八位作者编写的入门论文，*LightGBM: A Highly Efficient Gradient Boosting Decision Tree*，推荐用作了解
    LightGBM 的入门材料：[https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf](https://papers.nips.cc/paper/6907-lightgbm-a-highly-efficient-gradient-boosting-decision-tree.pdf)。'
- en: Implementing a great machine algorithm such as XGBoost or LightGBM in Kaggle
    competitions isn't enough. Similarly, fine-tuning a model's hyperparameters often
    isn't enough. While individual model predictions are important, it's equally important
    to engineer new data and to combine optimal models to attain higher scores.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kaggle 竞赛中实现一个优秀的机器学习算法，如 XGBoost 或 LightGBM，仅仅做到这一点还不够。同样，调优模型的超参数通常也不足够。尽管单个模型的预测很重要，但工程化新的数据并结合最优模型以获得更高的分数同样至关重要。
- en: The structure of Kaggle competitions
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Kaggle 竞赛的结构
- en: It's worth understanding the structure of Kaggle competitions to gain insights
    into why techniques such as non-correlated ensemble building and stacking are
    widespread. Furthermore, exploring the structure of Kaggle competitions will give
    you confidence in entering Kaggle competitions down the road if you choose to
    pursue that route.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 理解 Kaggle 竞赛的结构是很有价值的，这有助于你理解为什么像无相关的集成构建和堆叠技术在竞赛中如此广泛应用。此外，探索 Kaggle 竞赛的结构也会让你在未来如果选择参与此类竞赛时更有信心。
- en: Tip
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: 'Kaggle recommends *Housing Prices: Advanced Regression Techniques*, [https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques),
    for machine learning students looking to transition beyond the basics to advanced
    competitions. This is one of many knowledge-based competitions that do not offer
    cash prizes.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kaggle 推荐了 *Housing Prices: Advanced Regression Techniques*，[https://www.kaggle.com/c/house-prices-advanced-regression-techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)，适合那些希望从基础过渡到高级竞赛的机器学习学生。这是许多基于知识的竞赛之一，尽管它们不提供现金奖励。'
- en: 'Kaggle competitions exist on the Kaggle website. Here is the website from *Avito
    Context Ad Clicks* from 2015 won by XGBoost user Owen Zhang: [https://www.kaggle.com/c/avito-context-ad-clicks/overview](https://www.kaggle.com/c/avito-context-ad-clicks/overview).
    Several XGBoost Kaggle competition winners, Owen Zhang included, are from 2015,
    indicating XGBoost''s circulation before Tianqi Chin''s landmark paper, *XGBoost:
    A Scalable Tree Boosting System* published in 2016: [https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 'Kaggle 竞赛存在于 Kaggle 网站上。以下是 *Avito Context Ad Clicks* 竞赛网站的链接，这场比赛于2015年由 XGBoost
    用户 Owen Zhang 获胜：[https://www.kaggle.com/c/avito-context-ad-clicks/overview](https://www.kaggle.com/c/avito-context-ad-clicks/overview)。许多
    XGBoost Kaggle 竞赛的获胜者，包括 Owen Zhang，在2015年就已获得奖项，这表明 XGBoost 在 Tianqi Chin 2016年发表的里程碑论文
    *XGBoost: A Scalable Tree Boosting System* 之前就已广泛传播：[https://arxiv.org/pdf/1603.02754.pdf](https://arxiv.org/pdf/1603.02754.pdf)。'
- en: 'Here is the top of the *Avito Context Ad Clicks* website:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 *Avito Context Ad Clicks* 网站的顶部：
- en: '![Figure 9.1 – Avito Context Ad Clicks Kaggle competition website](img/B15551_09_01.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – Avito Context Ad Clicks Kaggle 竞赛网站](img/B15551_09_01.jpg)'
- en: Figure 9.1 – Avito Context Ad Clicks Kaggle competition website
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – Avito Context Ad Clicks Kaggle 竞赛网站
- en: 'This overview page explains the competition as follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 该概览页面对竞赛的解释如下：
- en: Additional links next to **Overview** (highlighted in blue) include **Data**,
    where you access data for the competition.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 **概览**（以蓝色高亮显示）旁边的附加链接包括 **数据**，在这里你可以访问竞赛的数据。
- en: '**Notebooks**, where Kagglers post solutions and starter notebooks.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**，Kagglers 发布解决方案和起始笔记本的地方。'
- en: '**Discussion**, where Kagglers post and answer questions.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**讨论区**，Kagglers 在这里发布和回答问题。'
- en: '**Leaderboard**, where the top scores are displayed.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排行榜**，展示最高分的地方。'
- en: '**Rules**, which explains how the competition works.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规则**，解释了竞赛的运作方式。'
- en: Additionally, note the **Late Submission** link on the far-right side, which
    indicates that submissions are still acceptable even though the competition is
    over, a general Kaggle policy.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另外，请注意右侧的**延迟提交**链接，表示即使竞赛已经结束，提交仍然是被接受的，这是 Kaggle 的一项常规政策。
- en: To download the data, you need to enter the competition by signing up for a
    free account. The data is typically split into two datasets, `training.csv`, the
    training set used to build a model, and `test.csv`, the test set used to score
    the model. After submitting a model, you earn a score on the public leaderboard.
    At the competition's end, a final model is submitted against a private test set
    to determine the winning solution.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 若要下载数据，你需要通过注册一个免费的账户来参加竞赛。数据通常被分为两个数据集，`training.csv` 是用于构建模型的训练集，`test.csv`
    是用于评估模型的测试集。提交模型后，你会在公共排行榜上获得一个分数。竞赛结束时，最终模型会提交给一个私有测试集，以确定获胜的解决方案。
- en: Hold-out sets
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 保留集
- en: It's important to make the distinction between building machine learning models
    for Kaggle competitions and building them on your own. Up to this point, we have
    split datasets into training and test sets to ensure that our models generalize
    well. In Kaggle competitions, however, models must be tested in a competitive
    environment. For that reason, data from the test set remains hidden.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建机器学习模型时，区分在 Kaggle 竞赛中构建模型与独立构建模型非常重要。到目前为止，我们已经将数据集分为训练集和测试集，以确保我们的模型能够很好地泛化。然而，在
    Kaggle 竞赛中，模型必须在竞争环境中进行测试。因此，测试集的数据会保持隐藏。
- en: 'Here are the differences between Kaggle''s training set and test set:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是 Kaggle 的训练集和测试集之间的区别：
- en: '`training.csv`: This is where you train and score models on your own. This
    training set should be split into its own training and test sets using `train_test_split`
    or `cross_val_score` to build models that generalize well to new data. The test
    sets used during training are often referred to as **validation sets** since they
    validate the models.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`training.csv`：这是你自己训练和评分模型的地方。这个训练集应该使用 `train_test_split` 或 `cross_val_score`
    将其划分为自己的训练集和测试集，从而构建能够很好泛化到新数据的模型。在训练过程中使用的测试集通常被称为**验证集**，因为它们用来验证模型。'
- en: '`test.csv`: This is a separate hold-out set. You don''t use the test set until
    you have a final model ready to test on data it has never seen before. The purpose
    of the hidden test set is to maintain the integrity of the competition. The test
    data is hidden from participants and the results are only revealed after participants
    submit a model.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`test.csv`：这是一个独立的保留集。在模型准备好并可以在它从未见过的数据上进行测试之前，你不会使用测试集。隐藏测试集的目的是保持竞赛的公正性。测试数据对参与者是隐藏的，结果只会在参与者提交模型之后才会公开。'
- en: It's always good practice to keep a test set aside when building a model for
    research or industry. When a model is tested using data it has already seen, the
    model risks overfitting the test set, a possibility that often arises in Kaggle
    competitions when competitors obsess over improving their position in the public
    leaderboard by few thousandths of a percent.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建研究或行业模型时，将测试集留存一旁始终是一个良好的做法。当一个模型在已经见过的数据上进行测试时，模型有过拟合测试集的风险，这种情况通常出现在 Kaggle
    竞赛中，参赛者通过千分之一的微小差异来过度优化自己的成绩，从而在公共排行榜上提升名次。
- en: Kaggle competitions intersect with the real world regarding this hold-out set.
    The purpose of building machine learning models is to make accurate predictions
    using unknown data. For example, if a model gives 100% accuracy on the training
    set, but only gives 50% accuracy on unknown data, the model is basically worthless.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 竞赛与现实世界在保留集的使用上有所交集。构建机器学习模型的目的是使用未知数据进行准确的预测。例如，如果一个模型在训练集上达到了 100%
    的准确率，但在未知数据上只有 50% 的准确率，那么这个模型基本上是没有价值的。
- en: This distinction, between validating models on test sets and testing models
    on hold-out sets, is very important.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集上验证模型与在保留集上测试模型之间的区别非常重要。
- en: 'Here is a general approach for validating and testing machine learning models
    on your own:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是验证和测试机器学习模型的一般方法：
- en: '**Split data into a training set and a hold-out set**: Keep the hold-out set
    away and resist the temptation to look at it.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将数据划分为训练集和保留集**：将保留集隔离开，并抵制查看它的诱惑。'
- en: '**Split the training set into a training and test set or use cross-validation**:
    Fit new models on the training set and validate the model, going back and forth
    to improve scores.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**将训练集划分为训练集和测试集，或使用交叉验证**：在训练集上拟合新模型，并验证模型，一来一回地改进得分。'
- en: '**After obtaining a final model, test it on the hold-out set**: This is the
    real test of the model. If the score is below expectations, return to *step 2*
    and repeat. Do not—and this is important—use the hold-out set as the new validation
    set, going back and forth adjusting hyperparameters. When this happens, the model
    is adjusting itself to match the hold-out set, which defeats the purpose of a
    hold-out set in the first place.'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**在获得最终模型后，在保留集上进行测试**：这是对模型的真正考验。如果得分低于预期，返回到*第2步*并重复。切记——这一点很重要——不要将保留集用作新的验证集，一来一回地调整超参数。这样做会导致模型根据保留集进行调整，这违背了保留集的初衷。'
- en: In Kaggle competitions, adjusting the machine learning model too closely to
    the test set will not work. Kaggle often splits test sets into an additional public
    and private component. The public test set gives participants a chance to score
    their models and work on improvements, adjusting and resubmitting along the way.
    The private test set is not revealed until the last day of the competition. Although
    rankings are displayed for the public test set, competition winners are announced
    based on the results of the unseen test set.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，过于将机器学习模型与测试集紧密结合是行不通的。Kaggle通常将测试集拆分为公共和私有两个部分。公共测试集让参赛者有机会评估他们的模型，并进行改进，一来一回地调整并重新提交。私有测试集直到竞赛最后一天才会揭晓。虽然公共测试集的排名会显示，但竞赛的胜者是基于未见测试集的结果宣布的。
- en: Winning a Kaggle competition requires getting the best possible score on the
    private test set. In Kaggle competitions, every percentage point matters. The
    need for this kind of precision, sometimes scoffed at by the industry, has led
    to innovative machine learning practices to improve scores. Understanding these
    techniques, as presented in this chapter, can lead to stronger models and a deeper
    understanding of machine learning overall.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 赢得Kaggle竞赛需要在私有测试集上获得尽可能高的分数。在Kaggle竞赛中，每一个百分点都至关重要。业界有时对这种精确度嗤之以鼻，但它促使了创新的机器学习实践来提高得分。理解本章所介绍的这些技术，可以让我们构建更强的模型，并更深入地理解整体机器学习。
- en: Engineering new columns
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开发新列
- en: '"Almost always I can find open source code for what I want to do, and my time
    is much better spent doing research and feature engineering."'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '"几乎总是，我都能找到我想做的事情的开源代码，我的时间应该更多地用在研究和特征工程上。"'
- en: – *Owen Zhang, Kaggle Winner*
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: – *Owen Zhang，Kaggle冠军*
- en: ([https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13](https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13](https://medium.com/kaggle-blog/profiling-top-kagglers-owen-zhang-currently-1-in-the-world-805b941dbb13))
- en: Many Kagglers and data scientists have confessed to spending considerable time
    on research and feature engineering. In this section, we will use `pandas` to
    engineer new columns of data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Kaggle参赛者和数据科学家都承认，他们花了相当多的时间在研究和特征工程上。在本节中，我们将使用`pandas`来开发新的数据列。
- en: What is feature engineering?
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是特征工程？
- en: Machine learning models are as good as the data that they train on. When data
    is insufficient, building a robust machine learning model is impossible.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的效果取决于它们训练所用的数据。当数据不足时，构建一个强大的机器学习模型几乎是不可能的。
- en: A more revealing question is whether the data can be improved. When new data
    is extracted from other columns, these new columns of data are said to be *engineered*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更具启发性的问题是，数据是否可以改进。当从其他列中提取新数据时，这些新列的数据被称为*工程化*数据。
- en: Feature engineering is the process of developing new columns of data from the
    original columns. The question is not whether you should implement feature engineering,
    but how much feature engineering you should implement.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是从原始列中开发新数据列的过程。问题不是你是否应该实施特征工程，而是你应该实施多少特征工程。
- en: Let's practice feature engineering on a dataset predicting the cab fare of **Uber**
    and **Lyft** rides.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在预测**Uber**和**Lyft**打车费的数据集上进行特征工程实践。
- en: Uber and Lyft data
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Uber和Lyft数据
- en: 'In addition to hosting competitions, Kaggle hosts a large number of datasets
    that include public datasets such as the following one, which predicts Uber and
    Lyft cab prices: [https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices](https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 除了举办竞赛，Kaggle 还主办了大量数据集，其中包括如下公开数据集，该数据集预测 Uber 和 Lyft 的出租车价格：[https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices](https://www.kaggle.com/ravi72munde/uber-lyft-cab-prices)：
- en: 'To get started, first import all the libraries and modules needed for this
    section and silence the warnings:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入本节所需的所有库和模块，并禁止警告：
- en: '[PRE0]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, load the `''cab_rides.csv''` CSV file and view the first five rows. Limit
    `nrows` to `10000` to expedite computations. There are 600,000+ rows in total:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，加载`'cab_rides.csv'` CSV 文件并查看前五行。限制`nrows`为`10000`，以加快计算速度。数据总共有超过 60 万行：
- en: '[PRE1]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the expected output:'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是预期输出：
- en: '![Figure 9.2 – Cab rides dataset](img/B15551_09_02.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 出租车数据集](img/B15551_09_02.jpg)'
- en: Figure 9.2 – Cab rides dataset
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 出租车数据集
- en: This display reveals a wide range of columns, including categorical features
    and a timestamp.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 此展示显示了多种列，包括类别特征和时间戳。
- en: Null values
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 空值
- en: 'As always, check for null values before making any computations:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，进行任何计算之前要检查空值：
- en: 'Recall that `df.info()` also provides information about column types:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记得`df.info()`也提供了关于列类型的信息：
- en: '[PRE2]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE3]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As you can see from the output, null values exist in the `price` column since
    there are less than `10,000` non-null floats.
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从输出结果可以看到，`price`列中存在空值，因为非空浮动数值少于`10,000`。
- en: 'It''s worth checking the null values to see whether more information can be
    gained about the data:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查空值是值得的，以查看是否可以获得更多关于数据的信息：
- en: '[PRE4]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here are the first five rows of the output:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是输出的前五行：
- en: '![Figure 9.3 – Null values in the cab rides dataset](img/B15551_09_03.jpg)'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 9.3 – 出租车数据集中的空值](img/B15551_09_03.jpg)'
- en: Figure 9.3 – Null values in the cab rides dataset
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.3 – 出租车数据集中的空值
- en: As you can see, there is nothing particularly glaring about these rows. It could
    be that the price of the ride was never recorded.
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如你所见，这些行没有什么特别明显的问题。可能是因为这次乘车的价格从未被记录。
- en: 'Since `price` is the target column, these rows can be deleted with `dropna`
    using the `inplace=True` parameter to ensure that drops occur within the DataFrame:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于`price`是目标列，可以使用`dropna`删除这些行，并使用`inplace=True`参数确保删除操作发生在 DataFrame 中：
- en: '[PRE5]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can verify that no null values are present by using `df.na()` or `df.info()`
    one more time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`df.na()`或`df.info()`再检查一次，验证没有空值。
- en: Feature engineering time columns
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程时间列
- en: '**Timestamp** columns often represent **Unix time**, which is the number of
    milliseconds since January 1st, 1970\. Specific time data can be extracted from
    the timestamp column that may help predict cab fares, such as the month, hour
    of the day, whether it is rush hour, and so on:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**时间戳**列通常表示**Unix 时间**，即自1970年1月1日以来的毫秒数。可以从时间戳列中提取特定时间数据，帮助预测出租车费用，如月份、小时、是否为高峰时段等：'
- en: 'First, convert the timestamp column into a time object using `pd.to_datetime`,
    and then view the first five rows:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，使用`pd.to_datetime`将时间戳列转换为时间对象，然后查看前五行：
- en: '[PRE6]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here is the expected output:'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是预期输出：
- en: '![Figure 9.4 – The cab rides dataset after time_stamp conversion](img/B15551_09_04.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 9.4 – 时间戳转换后的出租车数据集](img/B15551_09_04.jpg)'
- en: Figure 9.4 – The cab rides dataset after time_stamp conversion
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.4 – 时间戳转换后的出租车数据集
- en: Something is wrong with this data. It doesn't take much domain expertise to
    know that Lyft and Uber were not around in 1970\. The extra decimal places are
    a clue that the conversion is incorrect.
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个数据有问题。稍微具备领域知识的人就能知道 Lyft 和 Uber 在1970年并不存在。额外的小数位是转换不正确的线索。
- en: 'After trying several multipliers to make an appropriate conversion, I discovered
    that `10**6` gives the appropriate results:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试了几个乘数以进行适当的转换后，我发现`10**6`给出了合适的结果：
- en: '[PRE7]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Here is the expected output:'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是预期输出：
- en: '![Figure 9.5 – The cab rides dataset after ''date'' conversion](img/B15551_09_05.jpg)'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 9.5 – `''date''`转换后的出租车数据集](img/B15551_09_05.jpg)'
- en: Figure 9.5 – The cab rides dataset after 'date' conversion
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.5 – `'date'`转换后的出租车数据集
- en: 'With a datetime column, you can extract new columns, such as `month`, `hour`,
    and `day of week`, after importing `datetime`, as follows:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于一个日期时间列，你可以在导入`datetime`后提取新列，如`month`、`hour`和`day of week`，如下所示：
- en: '[PRE8]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, you can use these columns to feature engineer more columns, such as whether
    it's the weekend or rush hour.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在，你可以使用这些列来进行特征工程，创建更多的列，例如判断是否是周末或高峰时段。
- en: 'The following function determines whether a day of the week is a weekend by
    checking whether `''dayofweek''` is equivalent to `5` or `6`, which represent
    Saturday or Sunday, according to the official documentation: [https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html):'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下函数通过检查 `'dayofweek'` 是否等于 `5` 或 `6` 来确定一周中的某天是否为周末，这两个值分别代表星期六或星期天，具体参见官方文档：[https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.dt.weekday.html)：
- en: '[PRE9]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, apply the function to the DataFrame as a new column, `df[''weekend'']`,
    as follows:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，将该函数应用于 DataFrame 作为新列 `df['weekend']`，如下所示：
- en: '[PRE10]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The same strategy can be implemented to create a rush hour column by seeing
    whether the hour is between 6–10 AM (hours `6–10`) and 3–7 PM (hours `15–19`):'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 相同的策略可以用来创建一个高峰时段列，通过判断小时是否在早上 6-10 点（小时 `6-10`）和下午 3-7 点（小时 `15-19`）之间：
- en: '[PRE11]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, apply the function to a new `''rush_hour''` column:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将该函数应用于新的 `'rush_hour'` 列：
- en: '[PRE12]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The last five rows show variation in the new columns, as `df.tail()` reveals:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后一行显示了新列的变化，正如`df.tail()`所揭示的：
- en: '[PRE13]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is an excerpt from the output revealing the new columns:'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是输出的摘录，展示了新列：
- en: '![Figure 9.6 – The last five rows of the cab rides dataset after feature engineering](img/B15551_09_06.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 特征工程后出租车乘车数据集的最后五行](img/B15551_09_06.jpg)'
- en: Figure 9.6 – The last five rows of the cab rides dataset after feature engineering
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 特征工程后出租车乘车数据集的最后五行
- en: The process of extracting and engineering new time columns can continue.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 提取和工程化新时间列的过程可以继续进行。
- en: Note
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When engineering a lot of new columns, it's worth checking to see whether new
    features are strongly correlated. The correlation of data will be explored later
    in this chapter.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行大量新列的工程化时，值得检查是否有新的特征高度相关。数据的相关性将在本章稍后讨论。
- en: Now that you understand the practice of feature engineering time columns, let's
    feature engineer categorical columns.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你理解了时间列特征工程的实践，让我们来进行类别列的特征工程。
- en: Feature engineering categorical columns
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 类别列特征工程
- en: Previously, we used `pd.get_dummies` to convert categorical columns into numerical
    columns. Scikit-learn's `OneHotEncoder` feature is another option designed to
    transform categorical data into 0s and 1s using sparse matrices, a technique that
    you will apply in [*Chapter 10*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230),
    *XGBoost Model Deployment*. While converting categorical data into numerical data
    using either of these options is standard, alternatives exist.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们使用 `pd.get_dummies` 将类别列转换为数值列。Scikit-learn 的 `OneHotEncoder` 特性是另一种选择，它使用稀疏矩阵将类别数据转换为
    0 和 1，这种技术将在 [*第10章*](B15551_10_Final_NM_ePUB.xhtml#_idTextAnchor230) *XGBoost
    模型部署* 中应用。虽然使用这两种方法将类别数据转换为数值数据是常规做法，但也存在其他的替代方法。
- en: Although 0s and 1s make sense as numerical values for categorical columns, since
    0 indicates absence and 1 indicates presence, it's possible that other values
    may deliver better results.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 0 和 1 作为类别列的数值表示是有意义的，因为 0 表示缺失，1 表示存在，但也有可能其他值能提供更好的结果。
- en: One strategy would be to convert categorical columns into their frequencies,
    which equates to the percentage of times each category appears within the given
    column. So, instead of a column of categories, each category is converted into
    its percentage within the column.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 一种策略是将类别列转换为它们的频率，这相当于每个类别在给定列中出现的百分比。因此，列中的每个类别都被转换为它在该列中的百分比，而不是一个类别列。
- en: Let's view the steps to convert categorical values into numerical values next.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们查看将类别值转换为数值值的步骤。
- en: Engineering frequency columns
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工程化频率列
- en: 'To engineer a categorical column, such as `''cab_type''`, first view the number
    of values for each category:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要对类别列进行工程化，例如 `'cab_type'`，首先查看每个类别的值的数量：
- en: 'Use the `.value_counts()` method to see the frequency of types:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `.value_counts()` 方法查看各类型的频率：
- en: '[PRE14]'
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The result is as follows:'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE15]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use `groupby` to place the counts in a new column. `df.groupby(column_name)`
    is `groupby`, while `[column_name].transform` specifies the column to be transformed
    followed by the aggregate in parentheses:'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `groupby` 将计数放入新列。`df.groupby(column_name)` 是 `groupby`，而 `[column_name].transform`
    指定要转换的列，后面跟着括号内的聚合操作：
- en: '[PRE16]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Divide the new column by the total number of rows to obtain the frequency:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新列除以总行数以获得频率：
- en: '[PRE17]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Verify that changes have been made as expected:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证更改是否按预期进行：
- en: '[PRE18]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is an excerpt from the output showing the new columns:'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是显示新列的输出摘录：
- en: '![Figure 9.7 – The cab rides dataset after engineering the frequency of cabs](img/B15551_09_07.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 经出租车频率工程处理后的出租车乘车数据集](img/B15551_09_07.jpg)'
- en: Figure 9.7 – The cab rides dataset after engineering the frequency of cabs
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 经出租车频率工程处理后的出租车乘车数据集
- en: The cab frequency now displays the expected output.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，出租车频率显示出预期的结果。
- en: Kaggle tip – mean encoding
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Kaggle小贴士 – 均值编码
- en: We will conclude this section with a competition-tested approach to feature
    engineering called **mean encoding** or **target encoding**.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个经过竞赛验证的特征工程方法来结束这一部分，称为**均值编码**或**目标编码**。
- en: Mean encoding transforms categorical columns into numerical columns based on
    the mean target variable. For instance, if the color orange led to seven target
    values of 1 and three target values of 0, the mean encoded column would be 7/10
    = 0.7\. Since there is data leakage while using the target values, additional
    regularization techniques are required.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 均值编码将类别列转换为基于目标变量均值的数值列。例如，如果颜色橙色对应的七个目标值为1，三个目标值为0，那么均值编码后的列将是7/10 = 0.7。由于在使用目标值时存在数据泄漏，因此需要额外的正则化技术。
- en: '**Data leakage** occurs when information between training and test sets, or
    predictor and target columns, are shared. The risk here is that the target column
    is being directly used to influence the predictor columns, which is generally
    a bad idea in machine learning. Nevertheless, mean encoding has been shown to
    produce outstanding results. It can work when datasets are deep, and the distribution
    of mean values are approximately the same for incoming data. Regularization is
    an extra precaution taken to reduce the possibility of overfitting.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据泄漏**发生在训练集和测试集之间，或者预测列和目标列之间共享信息时。这里的风险是目标列被直接用来影响预测列，这在机器学习中通常是个坏主意。不过，均值编码已被证明能产生出色的结果。当数据集很深，并且均值分布对于输入数据大致相同时，它仍然有效。正则化是减少过拟合可能性的一项额外预防措施。'
- en: 'Fortunately, scikit-learn provides `TargetEncoder` to handle mean conversions
    for you:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn提供了`TargetEncoder`来帮助你处理均值转换：
- en: 'First, import `TargetEndoder` from `category_encoders`. If this does not work,
    install `category_encoders` using the following code:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从`category_encoders`导入`TargetEncoder`。如果无法工作，可以使用以下代码安装`category_encoders`：
- en: '[PRE19]'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, initialize `encoder`, as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，初始化`encoder`，如下所示：
- en: '[PRE20]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Finally, introduce a new column and apply mean encoding using the `fit_transform`
    method on the encoder. Include the column that is being changed and the target
    column as parameters:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，介绍一个新列，并使用编码器的`fit_transform`方法应用均值编码。将要更改的列和目标列作为参数传入：
- en: '[PRE21]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, verify that the changes are as expected:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，验证更改是否按预期进行：
- en: '[PRE22]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is an excerpt of the output with the new column in view:'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 下面是显示新列的输出摘录：
- en: '![Figure 9.8 – The cab rides dataset after mean encoding](img/B15551_09_08.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – 经均值编码后的出租车乘车数据集](img/B15551_09_08.jpg)'
- en: Figure 9.8 – The cab rides dataset after mean encoding
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – 经均值编码后的出租车乘车数据集
- en: The far-right column, `cab_type_mean`, is as expected.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最右侧的列`cab_type_mean`符合预期。
- en: 'For more information on mean encoding, refer to this Kaggle study: [https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study).'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 有关均值编码的更多信息，请参考这篇Kaggle研究：[https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study](https://www.kaggle.com/vprokopev/mean-likelihood-encodings-a-comprehensive-study)。
- en: The idea here is not to say that mean encoding is better than one-hot encoding,
    but rather that mean encoding is a proven technique that has done well in Kaggle
    competitions and may be worth implementing to try and improve scores.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的观点并不是说均值编码比独热编码更好，而是说明均值编码是一种经过验证的技术，在Kaggle竞赛中表现优异，可能值得实施来尝试提高得分。
- en: More feature engineering
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 更多特征工程
- en: There is no reason to stop here. More feature engineering may include statistical
    measures on other columns using `groupby` and additional encoders. Other categorical
    columns, such as the destination and arrival columns, may be converted to latitude
    and longitude and then to new measures of distance, such as the taxicab distance
    or the **Vincenty** distance, which takes spherical geometry into account.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 没有理由止步于此。更多的特征工程可能包括对其他列进行统计度量，使用`groupby`和附加编码器。其他类别型列，比如目的地和到达列，可以转换为纬度和经度，然后转换为新的距离度量方式，例如出租车距离或**Vincenty**距离，它考虑了球面几何。
- en: In Kaggle competitions, participants may engineer thousands of new columns hoping
    to gain a few extra decimal places of accuracy. If you have a high number of engineered
    columns, you can select the most significant ones using `.feature_importances_`,
    as outlined in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*. You can also eliminate highly correlated columns (explained
    in the next section, *Building non-correlated ensembles*).
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛中，参与者可能会进行数千列新的特征工程，希望能获得几位小数的准确度。如果你有大量的工程化列，可以使用`.feature_importances_`选择最重要的列，正如在[*第二章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入剖析》中所述。你还可以去除高度相关的列（将在下一节“构建无相关性的集成模型”中解释）。
- en: For this particular cab rides dataset, there is an additional CSV file that
    includes the weather. But what if there wasn't a weather file? You could always
    research the weather data from the provided dates and include the weather data
    on your own.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的出租车乘车数据集，还附带了一个包含天气数据的CSV文件。但如果没有天气文件该怎么办呢？你可以自行查找提供日期的天气数据，并将其添加到数据集中。
- en: Feature engineering is an essential skill for any data scientist to build robust
    models. The strategies covered here are only a fraction of the options that exist.
    Feature engineering involves research, experimentation, domain expertise, standardizing
    columns, feedback on the machine learning performance of new columns, and narrowing
    down the final columns at the end.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是任何数据科学家构建鲁棒模型的必要技能。这里讲解的策略只是现存选项的一部分。特征工程涉及研究、实验、领域专业知识、标准化列、对新列的机器学习性能反馈，并最终缩小最终列的范围。
- en: Now that you understand the various strategies for feature engineering, let's
    move on to building non-correlated ensembles.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了各种特征工程策略，让我们继续讨论构建无相关性的集成模型。
- en: Building non-correlated ensembles
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建无相关性的集成模型
- en: '"In our final model, we had XGBoost as an ensemble model, which included 20
    XGBoost models, 5 random forests, 6 randomized decision tree models, 3 regularized
    greedy forests, 3 logistic regression models, 5 ANN models, 3 elastic net models
    and 1 SVM model."'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: “在我们的最终模型中，我们使用了XGBoost作为集成模型，其中包含了20个XGBoost模型，5个随机森林，6个随机化决策树模型，3个正则化贪婪森林，3个逻辑回归模型，5个ANN模型，3个弹性网模型和1个SVM模型。”
- en: – *Song, Kaggle Winner*
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: – *Song, Kaggle获胜者*
- en: ([https://hunch243.rssing.com/chan-68612493/all_p1.html](https://hunch243.rssing.com/chan-68612493/all_p1.html))
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://hunch243.rssing.com/chan-68612493/all_p1.html](https://hunch243.rssing.com/chan-68612493/all_p1.html))
- en: The winning models of Kaggle competitions are rarely individual models; they
    are almost always ensembles. By ensembles, I do not mean boosting or bagging models,
    such as random forests or XGBoost, but pure ensembles that include any distinct
    models, including XGBoost, random forests, and others.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle竞赛的获胜模型很少是单一模型；它们几乎总是集成模型。这里所说的集成模型，并不是指提升（boosting）或袋装（bagging）模型，如随机森林（random
    forests）或XGBoost，而是纯粹的集成模型，包含任何不同的模型，包括XGBoost、随机森林等。
- en: In this section, we will combine machine learning models into non-correlated
    ensembles to gain accuracy and reduce overfitting.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将结合机器学习模型，构建无相关性的集成模型，以提高准确性并减少过拟合。
- en: Range of models
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型范围
- en: The Wisconsin Breast Cancer dataset, used to predict whether a patient has breast
    cancer, has 569 rows and 30 columns, and can be viewed at [https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 威斯康星州乳腺癌数据集用于预测患者是否患有乳腺癌，包含569行和30列数据，可以在[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html?highlight=load_breast_cancer)查看。
- en: 'Here are the steps to prepare and score the dataset using several classifiers:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是使用几种分类器准备和评分数据集的步骤：
- en: 'Import the `load_breast_cancer` dataset from scikit-learn so that we can quickly
    start building models:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 导入 `load_breast_cancer` 数据集，以便我们能快速开始构建模型：
- en: '[PRE23]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Assign the predictor columns to `X` and the target column to `y` by setting
    the `return_X_y=True` parameter:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过设置 `return_X_y=True` 参数，将预测变量列赋值给 `X`，将目标变量列赋值给 `y`：
- en: '[PRE24]'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Prepare 5-fold cross-validation using `StratifiedKFold` for consistency:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `StratifiedKFold` 准备 5 折交叉验证以确保一致性：
- en: '[PRE25]'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now, build a simple classification function that takes a model as input and
    returns the mean cross-validation score as output:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，构建一个简单的分类函数，该函数接收一个模型作为输入，并返回交叉验证的平均得分作为输出：
- en: '[PRE26]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Get the scores of several default classifiers, including XGBoost, along with
    its alternative base learners, a random forest, and logistic regression:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取几个默认分类器的得分，包括 XGBoost 及其替代基础学习器、随机森林和逻辑回归：
- en: 'a) Score with XGBoost:'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) 使用 XGBoost 进行评分：
- en: '[PRE27]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The score is as follows:'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE28]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'b) Score with `gblinear`:'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) 使用 `gblinear` 进行评分：
- en: '[PRE29]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The score is as follows:'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'c) Score with `dart`:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) 使用 `dart` 进行评分：
- en: '[PRE31]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The score is as follows:'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE32]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Note that for the dart booster, we set `one_drop=True` to ensure that trees
    are actually dropped.
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，对于 dart 增强器，我们设置 `one_drop=True` 以确保树木确实被丢弃。
- en: 'd) Score with `RandomForestClassifier`:'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) 使用 `RandomForestClassifier` 进行评分：
- en: '[PRE33]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The score is as follows:'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE34]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'e) Score with `LogisticRegression`:'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: e) 使用 `LogisticRegression` 进行评分：
- en: '[PRE35]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The score is as follows:'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE36]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Most models perform respectably, with the XGBoost classifier obtaining the highest
    score. The `gblinear` base learner did not perform particularly well, however,
    so we will not use it going forward.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数模型的表现都很不错，其中 XGBoost 分类器获得了最高分。然而，`gblinear` 基础学习器的表现不太好，因此我们以后将不再使用它。
- en: 'In practice, each of these models should be tuned. Since we have already covered
    hyperparameter tuning in multiple chapters, that option is not pursued here. Nevertheless,
    knowledge of hyperparameters can give confidence in trying a quick model with
    some adjusted values. For instance, as done in the following code, lowering `max_depth`
    to `2`, increasing `n_estimators` to `500`, and making sure that `learning_rate`
    is set to `0.1` may be attempted on XGBoost:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，应该对每个模型进行调整。由于我们在多个章节中已经介绍了超参数调整，因此在这里不再讨论该选项。然而，了解超参数的知识可以增加尝试快速模型并调整一些参数值的信心。例如，正如以下代码所示，可以尝试将
    XGBoost 的 `max_depth` 降至 `2`，将 `n_estimators` 增加到 `500`，并确保将 `learning_rate` 设置为
    `0.1`：
- en: '[PRE37]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The score is as follows:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE38]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This is a very good score. Although it's not the highest, it may be of value
    in our ensemble.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常不错的得分，尽管它不是最高的，但在我们的集成模型中可能仍然有价值。
- en: Now that we have a variety of models, let's learn about the correlations between
    them.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了多种模型，让我们了解它们之间的相关性。
- en: Correlation
  id: totrans-213
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性
- en: The purpose of this section is not to select all models for the ensemble, but
    rather to select the non-correlated models.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目的是选择非相关的模型，而不是选择所有模型进行集成。
- en: First, let's understand what **correlation** represents.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们理解**相关性**代表什么。
- en: Correlation is a statistical measure between `-1` and `1` that indicates the
    strength of the linear relationship between two sets of points. A correlation
    of `1` is a perfectly straight line, while a correlation of `0` shows no linear
    relationship whatsoever.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性是一个统计度量，范围从 `-1` 到 `1`，表示两组数据点之间线性关系的强度。相关性为 `1` 表示完全的直线关系，而相关性为 `0` 表示没有任何线性关系。
- en: 'Some visuals on correlation should make things clear. The following visuals
    are taken from Wikipedia''s *Correlation and Dependence* page at [https://en.wikipedia.org/wiki/Correlation_and_dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 一些关于相关性的可视化图表可以使事情变得更加清晰。以下图表来自维基百科的 *Correlation and Dependence* 页面，[https://en.wikipedia.org/wiki/Correlation_and_dependence](https://en.wikipedia.org/wiki/Correlation_and_dependence)：
- en: 'Scatter plots with listed correlations look as follows:'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 显示相关性的散点图如下所示：
- en: '![Figure 9.9 – Listed Correlations](img/B15551_09_09.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.9 – 列出相关性](img/B15551_09_09.jpg)'
- en: Figure 9.9 – Listed Correlations
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 – 列出相关性
- en: License information
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证信息
- en: By DenisBoigelot, the original uploader was Imagecreator – own work, CC0, [https://commons.wikimedia.org/w/index.php?curid=15165296](https://commons.wikimedia.org/w/index.php?curid=15165296).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 由 DenisBoigelot 上传，原上传者是 Imagecreator – 自制作品，CC0，[https://commons.wikimedia.org/w/index.php?curid=15165296](https://commons.wikimedia.org/w/index.php?curid=15165296)。
- en: 'Anscombe''s quartet – four scatter plots with a correlation of **0.816** –
    looks as follows:'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anscombe 四重奏 – 四个相关性为 **0.816** 的散点图如下所示：
- en: '![Figure 9.10 – Correlation of 0.816](img/B15551_09_10.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图9.10 – 相关性为0.816](img/B15551_09_10.jpg)'
- en: Figure 9.10 – Correlation of 0.816
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10 – 相关性为0.816
- en: License information
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 许可证信息
- en: 'By Anscombe.svg: Schutz (label using subscripts): Avenue – Anscombe.svg, CC
    BY-SA 3.0, [https://commons.wikimedia.org/w/index.php?curid=9838454](https://commons.wikimedia.org/w/index.php?curid=9838454)'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由Anscombe.svg提供：Schutz（使用下标标记）：Avenue – Anscombe.svg，CC BY-SA 3.0，[https://commons.wikimedia.org/w/index.php?curid=9838454](https://commons.wikimedia.org/w/index.php?curid=9838454)
- en: The first example shows that the higher the correlation, the closer the dots
    generally are to a straight line. The second example shows that data points of
    the same correlation can differ widely. In other words, correlation provides valuable
    information, but it doesn't tell the whole story.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个示例表明，相关性越高，点通常越接近直线。第二个示例表明，相同相关性的数据显示点可能会有较大差异。换句话说，相关性提供了有价值的信息，但它不能完全说明问题。
- en: Now that you understand what correlation means, let's apply correlation to building
    machine learning ensembles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你理解了相关性是什么意思，接下来让我们将相关性应用于构建机器学习集成。
- en: Correlation in machine learning ensembles
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习集成中的相关性
- en: Now we choose which models to include in our ensemble.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们选择要包括在集成学习中的模型。
- en: A high correlation between machine learning models is undesirable in an ensemble.But
    why?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型之间的高相关性在集成学习中是不可取的。那为什么呢？
- en: Consider the case of two classifiers with 1,000 predictions each. If these classifiers
    all make the same predictions, no new information is gained from the second classifier,
    making it superfluous.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑两个分类器每个有1,000个预测的情况。如果这两个分类器做出了相同的预测，那么从第二个分类器中没有获得新信息，使得它变得多余。
- en: Using a *majority rules* implementation, a prediction is only wrong if the majority
    of classifiers get it wrong. It's desirable, therefore, to have a diversity of
    models that score well but give different predictions. If most models give the
    same predictions, the correlation is high, and there is little value in adding
    the new model to the ensemble. Finding differences in predictions where a strong
    model may be wrong gives the ensemble the chance to produce better results. Predictions
    will be different when the models are non-correlated.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*多数规则*实现时，只有在大多数分类器预测错误时，预测才算错误。因此，拥有表现良好但给出不同预测的多样化模型是可取的。如果大多数模型给出了相同的预测，相关性就很高，那么将新模型加入集成学习的价值就不大了。找到模型预测的差异，尤其是强模型可能错误的地方，为集成学习提供了产生更好结果的机会。当模型不相关时，预测结果会有所不同。
- en: To compute correlations between machine learning models, we first need data
    points to compare. The different data points that machine learning models produce
    are their predictions. After obtaining predictions, we concatenate them into a
    DataFrame, and then apply the `.corr` method to obtain all correlations at once.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算机器学习模型之间的相关性，我们首先需要用来比较的数据点。机器学习模型生成的不同数据点是它们的预测结果。获得预测结果后，我们将它们连接成一个数据框，然后应用`.corr`方法一次性获取所有相关性。
- en: 'Here are the steps to find correlations between machine learning models:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是找到机器学习模型之间相关性的步骤：
- en: 'Define a function that returns predictions for each machine learning model:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个函数，返回每个机器学习模型的预测结果：
- en: '[PRE39]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Prepare the data for one-fold predictions using `train_test_split`:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`train_test_split`准备数据进行一次预测：
- en: '[PRE40]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Obtain the predictions of all classifier candidates using the previously defined
    function:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用之前定义的函数获取所有分类器候选的预测结果：
- en: 'a) `XGBClassifier` uses the following:'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) `XGBClassifier`使用以下方法：
- en: '[PRE41]'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The accuracy score is as follows:'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率得分如下：
- en: '[PRE42]'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'b) `XGBClassifier` with `dart` uses the following:'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `XGBClassifier`使用`dart`，采用以下方法：
- en: '[PRE43]'
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The accuracy score is as follows:'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率得分如下：
- en: '[PRE44]'
  id: totrans-249
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'c) `RandomForestClassifier` uses the following:'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: c) `RandomForestClassifier`使用以下方法：
- en: '[PRE45]'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The accuracy score is as follows:'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率得分如下：
- en: '[PRE46]'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'd) `LogisticRegression` uses the following:'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: d) `LogisticRegression`使用以下方法：
- en: '[PRE47]'
  id: totrans-255
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The accuracy score is as follows:'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率得分如下：
- en: '[PRE48]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The accuracy score is as follows:'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率得分如下：
- en: '[PRE49]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Concatenate the predictions into a new DataFrame using `np.c` (the `c` is short
    for concatenation):'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`np.c`（`c`代表连接）将预测结果连接成一个新的数据框：
- en: '[PRE50]'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Run correlations on the DataFrame using the `.corr()` method:'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`.corr()`方法在数据框上运行相关性计算：
- en: '[PRE51]'
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'You should see the following output:'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你应该看到以下输出：
- en: '![Figure 9.11 – Correlations between various machine learning models](img/B15551_09_11.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![图9.11 – 各种机器学习模型之间的相关性](img/B15551_09_11.jpg)'
- en: Figure 9.11 – Correlations between various machine learning models
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 – 各种机器学习模型之间的相关性
- en: As you can see, all correlations on the diagonal are `1.0` because the correlation
    between the model and itself must be perfectly linear. All other values are reasonably
    high.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，所有对角线上的相关性都是`1.0`，因为模型与自身之间的相关性必须是完全线性的。其他所有值也相当高。
- en: There is no clear cut-off to obtain a non-correlated threshold. It ultimately
    depends on the values of correlation and the number of models to choose from.
    For this example, we could pick the next two least correlated models with our
    best model, `xgb`, which are the random forest and logistic regression.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 没有明确的截断值来确定非相关性的阈值。最终选择依赖于相关性值和可选模型的数量。对于这个例子，我们可以选择与最佳模型`xgb`相关性最小的下两个模型，分别是随机森林和逻辑回归。
- en: Now that we have chosen our models, we will combine them into a single ensemble
    using the `VotingClassifier` ensemble, introduced next.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了模型，接下来我们将它们组合成一个集成模型，使用`VotingClassifier`集成，如下所述。
- en: The VotingClassifier ensemble
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: VotingClassifier集成
- en: Scikit-learn's `VotingClassifier` ensemble is designed to combine multiple classification
    models and select the output for each prediction using majority rules. Note that
    scikit-learn also comes with `VotingRegressor`, which combines multiple regression
    models by taking the average of each one.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn的`VotingClassifier`集成旨在结合多个分类模型，并使用多数规则选择每次预测的输出。请注意，scikit-learn还包含`VotingRegressor`，它通过取每个回归模型的平均值来结合多个回归模型。
- en: 'Here are the steps to create an ensemble in scikit-learn:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在scikit-learn中创建集成模型的步骤：
- en: 'Initialize an empty list:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空列表：
- en: '[PRE52]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Initialize the first model:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化第一个模型：
- en: '[PRE53]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Append the model to the list as a tuple in the form `(model_name, model)`:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型作为元组`(model_name, model)`追加到列表中：
- en: '[PRE54]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Repeat *steps 2* and *3* as many times as desired:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要重复*步骤2*和*步骤3*：
- en: '[PRE55]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Initialize `VotingClassifier` (or `VotingRegressor`) using the list of models
    as input:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型列表作为输入初始化`VotingClassifier`（或`VotingRegressor`）：
- en: '[PRE56]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Score the classifier using `cross_val_score`:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cross_val_score`评分分类器：
- en: '[PRE57]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'The score is as follows:'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE58]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: As you can see, the score has improved.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，得分已经提高。
- en: Now that you understand the purpose and technique of building non-correlated
    machine learning ensembles, let's move on to a similar but potentially advantageous
    technique called stacking.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了构建无相关性机器学习集成模型的目的和技术，让我们继续探讨一种类似但可能更有优势的技术——堆叠。
- en: Stacking models
  id: totrans-289
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 堆叠模型
- en: '"For stacking and boosting I use xgboost, again primarily due to familiarity
    and its proven results."'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: “对于堆叠和提升方法，我使用xgboost，主要是由于对它的熟悉以及它验证过的优异结果。”
- en: – *David Austin, Kaggle Winner*
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: – *David Austin, Kaggle冠军*
- en: ([https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/](https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/))
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ([https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/](https://www.pyimagesearch.com/2018/03/26/interview-david-austin-1st-place-25000-kaggles-popular-competition/))
- en: In this final section, we will examine one of the most powerful tricks frequently
    used by Kaggle winners, called stacking.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节最后，我们将探讨Kaggle获奖者经常使用的最强大技巧之一——堆叠。
- en: What is stacking?
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是堆叠？
- en: 'Stacking combines machine learning models at two different levels: the base
    level, whose models make predictions on all the data, and the meta level, which
    takes the predictions of the base models as input and uses them to generate final
    predictions.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠将机器学习模型结合在两个不同的层次：基础层，模型对所有数据进行预测；元层，将基础层模型的预测作为输入，并用它们生成最终预测。
- en: In other words, the final model in stacking does not take the original data
    as input, but rather takes the predictions of the base machine learning models
    as input.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，堆叠中的最终模型并不直接使用原始数据作为输入，而是将基础机器学习模型的预测作为输入。
- en: Stacked models have found huge success in Kaggle competitions. Most Kaggle competitions
    have merger deadlines, where individuals and teams can join together. These mergers
    can lead to greater success as teams rather than individuals because competitors
    can build larger ensembles and stack their models together.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠模型在Kaggle比赛中取得了巨大的成功。大多数Kaggle比赛都有合并截止日期，个人和团队可以在此期间合并。通过合并，作为团队而非个人竞争可以获得更大的成功，因为参赛者可以构建更大的集成模型并将其堆叠在一起。
- en: Note that stacking is distinct from a standard ensemble on account of the meta-model
    that combines predictions at the end. Since the meta-model takes predictive values
    as the input, it's generally advised to use a simple meta-model, such as linear
    regression for regression and logistic regression for classification.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，堆叠与标准集成方法不同，因为它有一个在最后进行预测组合的元模型。由于元模型将预测值作为输入，因此通常建议使用一个简单的元模型，比如回归任务中的线性回归和分类任务中的逻辑回归。
- en: Now that you have an idea of what stacking is, let's apply stacking with scikit-learn.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你对堆叠有所了解，让我们使用scikit-learn应用堆叠。
- en: Stacking in scikit-learn
  id: totrans-300
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在scikit-learn中的堆叠
- en: Fortunately, scikit-learn comes with a stacking regressor and classifier that
    makes the process fairly straightforward. The general idea is very similar to
    the ensemble model in the last section. A variety of base models are chosen, and
    then linear regression or logistic regression is chosen for the meta-model.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，scikit-learn提供了一个堆叠回归器和分类器，使得这一过程相当简单。其基本思路与上一节中的集成模型非常相似。选择多种基础模型，然后为元模型选择线性回归或逻辑回归。
- en: 'Here are the steps to use stacking with scikit-learn:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在scikit-learn中使用堆叠的步骤：
- en: 'Create an empty list of base models:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个空的基础模型列表：
- en: '[PRE59]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Append all base models to the base model list as tuples using the syntax `(name,
    model)`:'
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用语法`(name, model)`将所有基础模型作为元组附加到基础模型列表中：
- en: '[PRE60]'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: More models may be chosen when stacking since there are no majority rules limitations
    and linear weights adjust more easily to new data. An optimal approach is to use
    non-correlation as loose a guideline and to experiment with different combinations.
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在堆叠中可以选择更多的模型，因为没有多数规则的限制，并且线性权重能更容易地调整到新数据。一个最佳方法是使用非相关性作为松散的指导原则，并尝试不同的组合。
- en: 'Choose a meta model, preferably linear regression for regression and logistic
    regression for classification:'
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个元模型，最好是回归任务中的线性回归和分类任务中的逻辑回归：
- en: '[PRE61]'
  id: totrans-309
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Initialize `StackingClassifier` (or `StackingRegressor`) using `base_models`
    for `estimators` and `meta_model` for `final_estimator`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`base_models`作为`estimators`，`meta_model`作为`final_estimator`来初始化`StackingClassifier`（或`StackingRegressor`）：
- en: '[PRE62]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Validate the stacked model using `cross_val_score` or any other scoring method:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`cross_val_score`或任何其他评分方法来验证堆叠模型：
- en: '[PRE63]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The score is as follows:'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分如下：
- en: '[PRE64]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: This is the strongest result yet.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这是迄今为止最强的结果。
- en: As you can see, stacking is an incredibly powerful method and outperformed the
    non-correlated ensemble from the previous section.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，堆叠是一种非常强大的方法，它超越了上一节中的非相关集成模型。
- en: Summary
  id: totrans-318
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you learned some of the well-tested tips and tricks from the
    winners of Kaggle competitions. In addition to exploring Kaggle competitions and
    understanding the importance of a hold-out set, you gained essential practice
    in feature engineering time columns, feature engineering categorical columns,
    mean encoding, building non-correlated ensembles, and stacking. These advanced
    techniques are widespread among elite Kagglers, and they can give you an edge
    when developing machine learning models for research, competition, and industry.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，你学习了一些来自Kaggle竞赛获胜者的经过验证的技巧和窍门。除了探索Kaggle竞赛并理解保留集的重要性外，你还获得了在时间列特征工程、类别列特征工程、均值编码、构建非相关集成模型以及堆叠方面的基本实践。这些高级技术在精英Kaggler中广泛使用，它们能在你开发用于研究、竞赛和行业的机器学习模型时，提供优势。
- en: In the next and final chapter, we will shift gears from the competitive world
    to the tech world, where we will build an XGBoost model from beginning to end
    using transformers and pipelines to complete a model ready for industry deployment.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也是最后一章，我们将从竞争世界转向技术世界，在这里我们将使用转换器和管道从头到尾构建一个XGBoost模型，完成一个适合行业部署的模型。
