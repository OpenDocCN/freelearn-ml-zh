- en: Clustering – Making Sense of Unlabeled Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类 – 理解无标签数据
- en: Clustering is the poster child of unsupervised learning methods. It is usually
    our first choice when we need to add meaning to unlabeled data. In an e-commerce
    website, the marketing team may ask you to put your users into a few buckets so
    that they can tailor the messages they send to each group of them. If no one has
    labeled those millions of users for you, then clustering is your only way to put
    these users into buckets. When dealing with a large number of documents, videos,
    or web pages, and there are no categories assigned to this content, and you are
    not willing to ask *Marie Kondo* for help, then clustering is your only way to
    declutter this mess.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是无监督学习方法的代表。它通常是我们在需要为无标签数据添加意义时的首选。在一个电子商务网站中，营销团队可能会要求你将用户划分为几个类别，以便他们能够为每个群体定制信息。如果没有人给这些数百万用户打标签，那么聚类就是你将这些用户分组的唯一方法。当处理大量文档、视频或网页，并且这些内容没有被分配类别，而且你又不愿意求助于*Marie
    Kondo*，那么聚类就是你整理这一堆混乱数据的唯一途径。
- en: Since this is our first chapter about supervised learning algorithms, we will
    start with some theoretical background about clustering. Then, we will have a
    look at three commonly used clustering algorithms, in addition to the methods
    used for evaluating them.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们关于监督学习算法的第一章，我们将首先介绍一些关于聚类的理论背景。然后，我们将研究三种常用的聚类算法，并介绍用于评估这些算法的方法。
- en: 'In this chapter, we are going to cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论以下主题：
- en: Understanding clustering
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解聚类
- en: K-means clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K均值聚类
- en: Agglomerative clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合聚类
- en: DBSCAN
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN
- en: Let's get started!
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Understanding clustering
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解聚类
- en: Machine learning algorithms can be seen as optimization problems. They take
    data samples, and an objective function, and try to optimize this function. In
    the case of supervised learning, the objective function is based on the labels
    given to it. We try to minimize the differences between our predictions and the
    actual labels. In the case of unsupervised learning, things are different due
    to the lack of labels. Clustering algorithms, in essence, try to put the data
    samples into clusters so that it minimizes the intracluster distances and maximizes
    the intercluster distances. In other words, we want samples that are in the same
    cluster to be as similar as possible, and samples from different clusters to be
    as different as possible.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法可以看作是优化问题。它们获取数据样本和目标函数，并尝试优化该函数。在监督学习的情况下，目标函数基于所提供的标签。我们试图最小化预测值和实际标签之间的差异。在无监督学习的情况下，由于缺乏标签，情况有所不同。聚类算法本质上是试图将数据样本划分到不同的聚类中，从而最小化聚类内的距离并最大化聚类间的距离。换句话说，我们希望同一聚类中的样本尽可能相似，而来自不同聚类的样本则尽可能不同。
- en: Nevertheless, there is one trivial solution to this optimization problem. If
    we treat each sample as its own cluster, then the intracluster distances are all
    zeros and the intercluster distances are at their maximum. Obviously, this is
    not what we want from our clustering algorithm. Thus, to avoid this trivial solution,
    we usually add a constraint to our optimization function. For example, we may
    predefine the number of clusters we need to make sure the aforementioned trivial
    solution is avoided. One other possible constraint involves setting the minimum
    number of samples per cluster. We will see those constraints in practice when
    we discuss each of the different clustering algorithms in this chapter.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，解决这个优化问题有一个显而易见的解决方案。如果我们将每个样本视为其自身的聚类，那么聚类内的距离都为零，而聚类间的距离则为最大值。显然，这不是我们希望从聚类算法中得到的结果。因此，为了避免这个显而易见的解决方案，我们通常会在优化函数中添加约束。例如，我们可能会预定义需要的聚类数量，以确保避免上述显而易见的解决方案。另一个可能的约束是设置每个聚类的最小样本数。在本章讨论不同的聚类算法时，我们将看到这些约束在实际中的应用。
- en: The lack of labels also dictates the different metrics for evaluating how good
    the resulting clusters are. That's why I decided to emphasize the objective function
    of clustering algorithms here, since understanding the objective of an algorithm
    makes it easier to understand its evaluation metrics. We will come across a couple
    of evaluation metrics throughout this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 标签的缺失还决定了评估结果聚类好坏的不同度量标准。这就是为什么我决定在这里强调聚类算法的目标函数，因为理解算法的目标有助于更容易理解其评估度量标准。在本章中，我们将遇到几个评估度量标准。
- en: One way to measure the intracluster distances is to calculate the distances
    between each point in the cluster and the cluster's centroid. The concept of the
    centroid should be familiar to you by now since we discussed the **nearest centroid**
    algorithm in [Chapter 5](b95b628d-5913-477e-8897-989ce2afb974.xhtml), *Image Processing
    with Nearest Neighbors*. The centroid is basically the mean of all the samples
    in the clusters. Furthermore, the average Euclidean distance between some samples
    and their mean has another name that we all learned about in primary school –
    **standard deviation**. The very same distance measure can be used to measure
    the dissimilarity between the clusters' centroids.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量簇内距离的一种方式是计算簇中每个点与簇中心的距离。簇中心的概念你应该已经很熟悉，因为我们在[第五章](b95b628d-5913-477e-8897-989ce2afb974.xhtml)中讨论过**最近邻中心**算法，*图像处理与最近邻*。簇中心基本上是簇中所有样本的均值。此外，某些样本与其均值之间的平均欧几里得距离还有一个名字，这是我们在小学时学过的——**标准差**。相同的距离度量可以用于衡量簇中心之间的差异。
- en: At this point, we are ready to explore our first algorithm, known as **K-means**.
    However, we need to create some sample data first so that we can use it to demonstrate
    our algorithms. In the next section, after explaining the algorithm, we are going
    to create the needed data and use the K-means algorithm to cluster it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们准备好探索第一个算法——**K均值**。然而，我们需要先创建一些样本数据，这样才能用来演示算法。在接下来的部分，解释完算法之后，我们将创建所需的数据，并使用K均值算法进行聚类。
- en: K-means clustering
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K均值聚类
- en: '"We all know we are unique individuals, but we tend to see others as representatives
    of groups."'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: “我们都知道自己是独一无二的个体，但我们往往把他人看作是群体的代表。”
- en: '- Deborah Tannen'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '- Deborah Tannen'
- en: 'In the previous section, we discussed the constraint we put on our objective
    function by specifying the number of clusters we need. This is what the *K* stands
    for: the number of clusters. We also discussed the cluster''s centroid, hence
    the word means. The algorithm works as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了通过指定所需簇的数量来对目标函数进行约束。这就是*K*的含义：簇的数量。我们还讨论了簇的中心，因此“均值”这个词也可以理解。算法的工作方式如下：
- en: It starts by picking *K* random points and setting them as the cluster centroids.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它首先随机选择*K*个点，并将其设置为簇中心。
- en: Then, it assigns each data point to the nearest centroid to it to form *K* clusters.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它将每个数据点分配给最近的簇中心，形成*K*个簇。
- en: Then, it calculates a new centroid for the newly formed clusters.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它会为新形成的簇计算新的簇中心。
- en: Since the centroids have been updated, we need to go back to *step 2* to reassign
    the samples to their new clusters based on the updated centroids. However, if
    the centroids didn't move much, we know that the algorithm has converged, and
    we stop.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于簇中心已经更新，我们需要回到*步骤2*，根据更新后的簇中心重新分配样本到新的簇中。然而，如果簇中心没有太大变化，我们就知道算法已经收敛，可以停止。
- en: As you can see, this is an iterative algorithm. It keeps iterating until it
    converges, but we can limit the number of iterations by setting its `max_iter`hyperparameter.
    Additionally, we may decide to tolerate bigger centroid movements and stop earlier
    by setting the `tol`*hyperparameter to a larger value. The different choices regarding
    the initial cluster centroids may lead to different results. Setting the algorithm's`init`
    hyperparameter to`k-means++`makes sure the initial centroids are distant from
    each other. This usually leads to better results than random initialization. The
    choice of *K* is also given using the`n_clusters` hyperparameter. To demonstrate
    the usage of this algorithm and its hyperparameters, let's start by creating a
    sample dataset.*
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这是一个迭代算法。它会不断迭代直到收敛，但我们可以通过设置其`max_iter`超参数来限制迭代次数。此外，我们可以通过将`tol`*超参数设置为更大的值来容忍更大的中心移动，从而提前停止。关于初始簇中心的不同选择可能会导致不同的结果。将算法的`init`超参数设置为`k-means++`可以确保初始簇中心彼此远离。这通常比随机初始化能得到更好的结果。*K*的选择也是通过`n_clusters`超参数来指定的。为了演示该算法及其超参数的使用，我们先从创建一个示例数据集开始。
- en: '*## Creating a blob-shaped dataset'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*## 创建一个球形数据集'
- en: We usually visualize clusters as rounded blobs of scattered data points. This
    sort of shape is also known as a convex cluster and is one of the easiest shapes
    for algorithms to deal with. Later on, we will generate harder-to-cluster datasets,
    but let's start with the easy blobs for now.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将聚类数据可视化为圆形的散点数据点。这种形状也称为凸聚类，是算法最容易处理的形状之一。稍后我们将生成更难以聚类的数据集，但现在我们先从简单的blob开始。
- en: 'The `make_blobs` function helps us create a blob-shaped dataset. Here, we set
    the number of samples to `100` and divide them into four clusters. Each data point
    only has two features. This will make it easier for us to visualize the data later
    on. The clusters have different standard deviations; that is, some clusters are
    more dispersed than the others. The function also returns labels. We will keep
    the labels aside to validate our algorithm later on. Finally, we put the `x`''s
    and the `y`''s into a DataFrame and call it `df_blobs`:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_blobs`函数帮助我们创建一个blob形状的数据集。在这里，我们将样本数量设置为`100`，并将它们分成四个聚类。每个数据点只有两个特征。这将使我们后续更容易可视化数据。这些聚类有不同的标准差；也就是说，有些聚类比其他聚类更分散。该函数还返回标签。我们将标签放在一边，稍后用于验证我们的算法。最后，我们将`x`和`y`放入一个DataFrame，并将其命名为`df_blobs`：'
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: To make sure you get the exact same data I did, set the `random_state` parameter
    of the data generating function to a specific random seed. Now that the data is
    ready, we need to create a function to visualize this data.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保你得到和我一样的数据，请将数据生成函数的`random_state`参数设置为一个特定的随机种子。数据准备好后，我们需要创建一个函数来可视化这些数据。
- en: Visualizing our sample data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化我们的示例数据
- en: 'We are going to use the following function throughout this chapter. It takes
    the 2D *x*''s and *y* labels and plots them into the given Matplotlib axis, *ax*.
    In real-life scenarios, no labels are given, but still, we can give this function
    the labels predicted by the clustering algorithm instead. The resulting plot gets
    a title, along with the number of clusters that have been deduced from the cardinality
    of the given *y*:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用以下函数。它接受二维的 *x* 和 *y* 标签，并将它们绘制到给定的Matplotlib轴 *ax* 上。在实际场景中，通常不会给出标签，但我们仍然可以将聚类算法预测的标签传递给这个函数。生成的图形会带上一个标题，并显示从给定
    *y* 的基数推断出的聚类数量：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can use the new `plot_2d_clusters()` function as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式使用新的`plot_2d_clusters()`函数：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give us the following diagram:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下图示：
- en: '![](img/46e06437-e455-4edb-9f80-2a5f0e3fa3a8.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/46e06437-e455-4edb-9f80-2a5f0e3fa3a8.png)'
- en: Each data point is marked according to its given label. Now, we will pretend
    those labels haven't been given to us and see whether the K-means algorithm will
    be able to predict them or not.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个数据点都会根据其给定的标签进行标记。现在，我们将假设这些标签没有被提供，并观察K-means算法是否能够预测这些标签。
- en: Clustering with K-means
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用K-means进行聚类
- en: 'Now that we''re pretending that no labels have been given, how can we tell
    what value to use for *K*, that is, the`n_clusters`hyperparameter? We can''t.
    We will just pick any number for now; later on, we will learn how to find the
    best value for `n_clusters`. Let''s set it to five for now. We will keep all the
    other hyperparameters at their default values. Once the algorithm is initialized,
    we can use its `fit_predict` method, as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们假装没有给定标签，我们该如何确定用于*K*的值，也就是`n_clusters`超参数的值呢？我们无法确定。现在我们只好随便选一个数值，稍后我们将学习如何找到`n_clusters`的最佳值。暂时我们将其设为五。其余的超参数将保持默认值。一旦算法初始化完成，我们可以使用它的`fit_predict`方法，如下所示：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that the concept of fitting on a training set and predicting a test date
    seldom makes sense here. We usually fit and predict on the same dataset. We also
    don't pass any labels to the `fit` or`fit_predict` methods.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在训练集上进行拟合并在测试集上进行预测的概念在这里通常没有意义。我们通常在同一数据集上进行拟合和预测。我们也不会向`fit`或`fit_predict`方法传递任何标签。
- en: 'Now that we''ve predicted the new labels, we can use the `plot_2d_clusters()`
    function to compare our predictions to the original labels, as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经预测了新的标签，我们可以使用`plot_2d_clusters()`函数来将我们的预测与原始标签进行比较，如下所示：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'I prepended the words `Actuals` and `KMeans` to their corresponding figure
    titles. The resulting clusters are shown in the following screenshot:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我在对应的图形标题前加上了`Actuals`和`KMeans`两个词。生成的聚类如下截图所示：
- en: '![](img/5ee215f8-1d25-4616-9a9f-af712af33559.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5ee215f8-1d25-4616-9a9f-af712af33559.png)'
- en: One of the original four clusters has been split into two since we set *K* to
    five. Other than that, the predictions for the other clusters make sense. The
    labels that have been given to the clusters are arbitrary. The original cluster
    with label one was called three by the algorithm. This should not bother us at
    all, as long as the clusters have the exact same members. This should not bother
    the clustering evaluation metrics either. They usually take this fact into account
    and ignore the label names when evaluating a clustering algorithm.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将*K*设置为五，原来的四个聚类中的一个被拆分成了两个。除此之外，其他聚类的预测结果是合理的。给聚类分配的标签是随意的。原来标签为一的聚类在算法中被称为三。只要聚类的成员完全相同，这一点应该不会让我们困扰。这一点对于聚类评估指标也没有影响。它们通常会考虑到这一事实，并在评估聚类算法时忽略标签名称。
- en: 'Still, how do we determine the value of *K*? We have no other choice but to
    run the algorithm multiple times with different numbers of clusters and pick the
    best one. In the following code snippet, we''re looping over three different values
    for `n_clusters`. We also have access to the final centroids, which are calculated
    for each cluster after the algorithm converges. Seeing these centroids clarifies
    how the algorithm assigned each data point to its own cluster. The last line in
    our code snippet uses a triangular marker to plot the centroids in each of the
    three graphs:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们如何确定*K*的值呢？我们别无选择，只能多次运行算法，使用不同数量的聚类并选择最佳结果。在以下的代码片段中，我们正在遍历三个不同的`n_clusters`值。我们还可以访问最终的质心，这些质心是在算法收敛后为每个聚类计算得出的。查看这些质心有助于理解算法如何将每个数据点分配到它自己的聚类中。代码片段的最后一行使用三角形标记在三个图形中绘制了质心：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Here are the results of the three choices, side by side:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这是三个选择的结果，横向排列：
- en: '![](img/0b987078-8fc4-4e29-aaab-d2ad7f166a07.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0b987078-8fc4-4e29-aaab-d2ad7f166a07.png)'
- en: A visual investigation of the three graphs tells us that the choice of four
    clusters was the right choice. Nevertheless, we have to remember that we are dealing
    with 2D data points here. The same visual investigation would have been much harder
    if our data samples contained more than two features. In the next section, we
    are going to learn about the silhouette score and use it to pick the optimum number
    of clusters, without the need for visual aid.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对三个图形进行视觉检查告诉我们，选择四个聚类是正确的选择。不过，我们必须记住，我们这里处理的是二维数据点。如果我们的数据样本包含两个以上的特征，同样的视觉检查就会变得更加困难。在接下来的部分中，我们将学习轮廓系数，并利用它来选择最佳的聚类数，而不依赖于视觉辅助。
- en: The silhouette score
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 轮廓系数
- en: 'The **silhouette score** is a measure of how similar a sample is to its own
    cluster compared to the samples in the other clusters. For each sample, we will
    calculate the average distance between this sample and all the other samples in
    the same cluster. Let''s call this mean distance *A*. Then, we calculate the average
    distance between the same sample and all the other samples in the nearest cluster.
    Let''s call this other mean distance *B*. Now, we can define the silhouette score,
    as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**轮廓系数**是衡量一个样本与其自身聚类中其他样本相比的相似度的指标。对于每个样本，我们将计算该样本与同一聚类中所有其他样本之间的平均距离。我们称这个平均距离为*A*。然后，我们计算该样本与最近聚类中所有其他样本之间的平均距离。我们称这个平均距离为*B*。现在，我们可以定义轮廓系数，如下所示：'
- en: '![](img/31c4243e-df21-4e17-adc4-552606e7e508.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/31c4243e-df21-4e17-adc4-552606e7e508.png)'
- en: 'Now, rather than performing a visual investigation of the clusters, we are
    going to loop over multiple values for `n_clusters` and store the silhouette score
    after each iteration. As you can see, `silhouette_score` takes two parameters
    – the data points (`x`) and the predicted cluster labels (`y_pred`):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不再通过视觉检查聚类，而是将遍历多个`n_clusters`的值，并在每次迭代后存储轮廓系数。如你所见，`silhouette_score`接受两个参数——数据点（`x`）和预测的聚类标签（`y_pred`）：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can just pick the `n_clusters` value that gives the best score. Here, we
    put the calculated scores into a DataFrame and use a bar chart to compare them:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接选择提供最佳得分的`n_clusters`值。在这里，我们将计算出的得分放入一个DataFrame中，并使用柱状图进行比较：
- en: '[PRE7]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting scores confirm our initial decision that four is the best choice
    for the number of clusters:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 结果得分确认了我们最初的决定，四是最适合的聚类数：
- en: '![](img/6e3a0b1b-2b2c-4241-b818-5c0dc80941ec.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6e3a0b1b-2b2c-4241-b818-5c0dc80941ec.png)'
- en: In addition to picking the number of clusters, the choice of the algorithm's
    initial centroid also affects its accuracy. A bad choice may lead the K-means
    algorithm to converge at an undesirable local minimum. In the next section, we
    are going to witness how the initial centroids may affect the algorithm's final
    decision.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择聚类的数量外，算法初始质心的选择也会影响其准确性。错误的选择可能会导致 K-means 算法收敛到一个不理想的局部最小值。在下一节中，我们将看到初始质心如何影响算法的最终决策。
- en: Choosing the initial centroids
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择初始质心
- en: 'By default, the K-means implementation of scikit-learn picks random initial
    centroids that are further apart from each other. It also tries multiple initial
    centroids and picks the one that gives the best results. Having said that, we
    can also set the initial centroids by hand. In the following code snippet, we
    will compare two initial settings to see their effect on the final results. We
    will then print the two outcomes side by side:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，scikit-learn 的 K-means 实现会选择相互之间距离较远的随机初始质心。它还会尝试多个初始质心，并选择产生最佳结果的那个。话虽如此，我们也可以手动设置初始质心。在以下代码片段中，我们将比较两种初始设置，看看它们对最终结果的影响。然后我们将并排打印这两种结果：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following graphs show the resulting clusters after the algorithm converges.
    Parts of the styling code were omitted for brevity:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了算法收敛后的聚类结果。部分样式代码为了简洁被省略：
- en: '![](img/aba5995c-2526-458d-adcf-3f7a3eb00ffa.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aba5995c-2526-458d-adcf-3f7a3eb00ffa.png)'
- en: Clearly, the first initial setup helped the algorithm, while the second one
    led it to bad results. Thus, we have to be aware of the algorithm's initialization
    since its results are nondeterministic.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，第一个初始设置帮助了算法，而第二个设置则导致了不好的结果。因此，我们必须注意算法的初始化，因为它的结果是非确定性的。
- en: In the field of machine learning, the term transfer learning refers to the set
    of problems where we need to repurpose the knowledge gained while solving one
    problem and apply it to a slightly different problem. Humans also need transfer
    learning. The K-means algorithm has a `fit_transform` method. If our data (*x*)
    is made of *N* samples and *M* features, the method will transform it into *N*
    samples and *K* columns instead. The values in the columns are based on the predicted
    clusters. Usually, *K* is much smaller than *N*. Thus, you can repurpose your
    K-means clustering**algorithm so that it can be used as a dimensionality reduction
    step, before feeding its transformed output to a simple classifier or regressor.
    Similarly, in a**multi-class** classification problem, a clustering algorithm
    can be used to reduce the cardinality of the targets.**
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习领域，迁移学习指的是我们需要重新利用在解决一个问题时获得的知识，并将其应用于稍微不同的另一个问题。人类也需要迁移学习。K-means 算法有一个
    `fit_transform` 方法。如果我们的数据（*x*）由 *N* 个样本和 *M* 个特征组成，方法将把它转换成 *N* 个样本和 *K* 列。列中的值基于预测的聚类。通常，*K*
    要远小于 *N*。因此，您可以重新利用 K-means 聚类**算法**，使其可以作为降维步骤，在将其转换后的输出传递给简单分类器或回归器之前。类似地，在**多分类**问题中，可以使用聚类算法来减少目标的基数。**
- en: '**In contrast to the K-means algorithm, **agglomerative clustering** is another
    algorithm whose results are deterministic. It doesn''t rely on any initial choices
    since it approaches the clustering problem from a different angle. Agglomerative
    clustering is the topic of the next section.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**与 K-means 算法相对，**层次聚类**是另一种结果是确定性的算法。它不依赖任何初始选择，因为它从不同的角度来解决聚类问题。层次聚类是下一节的主题。'
- en: Agglomerative clustering
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: '"The most populous city is but an agglomeration of wildernesses."'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '"人口最多的城市不过是荒野的集合。"'
- en: '- Aldous Huxley'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '- 奥尔杜斯·赫胥黎'
- en: In the K-means clustering algorithm, we had our *K* cluster from day one. With
    each iteration, some samples may change their allegiances and some clusters may
    change their centroids, but in the end, the clusters are defined from the very
    beginning. Conversely, in agglomerative clustering, no clusters exist at the beginning.
    Initially, each sample belongs to its own cluster. We have as many clusters in
    the beginning as there are data samples. Then, we find the two closest samples
    and aggregate them into one cluster. After that, we keep iterating by combining
    the next closest two samples, two clusters, or the next closest sample and a cluster.
    As you can see, with each iteration, the number of clusters decreases by one until
    all our samples join a single cluster. Putting all the samples into one cluster
    sounds unintuitive. Thus, we have the option to stop the algorithm at any iteration,
    depending on the final number of clusters we need.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在K-means聚类算法中，我们从一开始就有了我们的*K*个聚类。在每次迭代中，某些样本可能会改变其归属，某些聚类的质心可能会改变，但最终，聚类从一开始就已定义。相反，在凝聚层次聚类中，开始时并不存在聚类。最初，每个样本都属于它自己的聚类。开始时的聚类数量与数据样本的数量相同。然后，我们找到两个最接近的样本，并将它们合并为一个聚类。之后，我们继续迭代，通过合并下一个最接近的两个样本、两个聚类，或者下一个最接近的样本和一个聚类。正如你所看到的，每次迭代时，聚类数量都会减少一个，直到所有样本都加入一个聚类。将所有样本放入一个聚类听起来不太直观。因此，我们可以选择在任何迭代中停止算法，具体取决于我们需要的最终聚类数量。
- en: 'With that, let''s learn how to use the agglomerative clustering algorithm.
    All you have to do for the algorithm to prematurely abort its agglomeration mission
    is to let it know the final number of clusters we need via its `n_clusters`hyperparameter.
    Obviously, since I mentioned that the algorithm combines the closed clusters,
    we need to dive into how intercluster distances are being calculated, but let''s
    ignore this for now – we will get to it in a bit. Here is how the algorithm is
    used when the number of clusters has been set to `4`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，让我们来学习如何使用凝聚层次聚类算法。要让算法提前终止其聚合任务，你需要通过它的`n_clusters`超参数告知它我们需要的最终聚类数量。显然，既然我提到算法会合并已关闭的聚类，我们需要深入了解聚类间的距离是如何计算的，但暂时我们可以忽略这一点——稍后我们会讲到。以下是当聚类数量设置为`4`时，算法的使用方法：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Since we set the number of clusters to `4`, the predicted `y_pred` will have
    values from zero to three.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将聚类数量设置为`4`，预测的`y_pred`将会有从零到三的值。
- en: In fact, the agglomerative clustering algorithm did not stop when the number
    of clusters was four. It continued to aggregate the clusters and kept track of
    which clusters are members of which bigger clusters using an internal tree structure.
    When we specified that we just needed four clusters, it revisited this internal
    tree and inferred the clusters' labels accordingly. In the next section, we are
    going to learn how to access the algorithm's internal hierarchy and trace the
    tree it builds.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，凝聚层次聚类算法并没有在聚类数量为四时停止。它继续合并聚类，并使用内部树结构跟踪哪些聚类是哪些更大聚类的成员。当我们指定只需要四个聚类时，它重新访问这个内部树，并相应地推断聚类的标签。在下一节中，我们将学习如何访问算法的内部层级，并追踪它构建的树。
- en: Tracing the agglomerative clustering's children
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 跟踪凝聚层次聚类的子节点
- en: 'As we mentioned previously, each sample or cluster becomes a member of another
    cluster, which, in turn, becomes a member of a bigger cluster, and so forth. This
    hierarchy is stored in the algorithm''s `children_` attribute. This attribute
    is in the form of a list of lists. The outer list has as many members as the number
    of data samples, minus one. Each of the member lists is made up of two numbers.
    We can list the last five members of the `children_` attribute as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，每个样本或聚类都会成为另一个聚类的成员，而这个聚类又成为更大聚类的成员，依此类推。这个层次结构被存储在算法的`children_`属性中。这个属性的形式是一个列表的列表。外部列表的成员数量等于数据样本数量减去一。每个成员列表由两个数字组成。我们可以列出`children_`属性的最后五个成员，如下所示：
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will give us the following list:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给我们以下列表：
- en: '[PRE11]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The very last element of the list is the root of the tree. It has two children,
    `196` and `197`. Those are the IDs of the children of this root node. An ID that
    is greater than or equal to the number of data samples is a cluster ID, while
    the smaller IDs refer to individual samples. If you subtract the number of data
    samples from a cluster ID, it will give you the location in the children list
    where you can get the members of this cluster. From this information, we can build
    the following recursive function, which takes a list of children and the number
    of data samples and returns the nested tree of all the clusters and their members,
    as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表中的最后一个元素是树的根节点。它有两个子节点，`196`和`197`。这些是根节点的子节点的ID。大于或等于数据样本数量的ID是聚类ID，而较小的ID则表示单个样本。如果你从聚类ID中减去数据样本的数量，就可以得到子节点列表中的位置，从而获得该聚类的成员。根据这些信息，我们可以构建以下递归函数，它接受一个子节点列表和数据样本的数量，并返回所有聚类及其成员的嵌套树，如下所示：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can call the function we''ve just created like so:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像这样调用我们刚刚创建的函数：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'At this point, `tree[0]` and `tree[1]` contain the IDs of the samples in the
    left-hand side and right-hand side of the tree – these are the members of the
    two biggest clusters. If our aim is to divide our samples into four clusters instead
    of two, we can use `tree[0][0]`, `tree[0][1]`, `tree[1][0]`, and `tree[1][1]`.
    Here is what `tree[0][0]` looks like:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，`tree[0]`和`tree[1]`包含树的左右两侧样本的ID——这些是两个最大聚类的成员。如果我们的目标是将样本分成四个聚类，而不是两个，我们可以使用`tree[0][0]`、`tree[0][1]`、`tree[1][0]`和`tree[1][1]`。以下是`tree[0][0]`的样子：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This nestedness allows us to set how deep we want our clusters to be and retrieve
    their members accordingly. Nevertheless, we can flatten this list using the following
    code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种嵌套性使我们能够设置我们希望聚类的深度，并相应地获取其成员。尽管如此，我们可以使用以下代码将这个列表展平：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can get a member of`tree[0][0]`, as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以获取`tree[0][0]`的成员，如下所示：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can also mimic the output of `fit_predict` and build our own predicted labels
    using the following code snippet. It will assign the labels from zero to three
    to the members of the different branches of the tree we built. Let''s call our
    predicted labels `y_pred_dash`:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以模拟`fit_predict`的输出，并使用以下代码片段构建我们自己的预测标签。它将为我们构建的树的不同分支中的成员分配从零到三的标签。我们将我们的预测标签命名为`y_pred_dash`：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: To make sure our code works as expected, the values in `y_pred_dash` should
    match those in `y_pred` from the previous section. Nonetheless, nothing says whether
    the`tree[0][0]`part of the tree should be given the label `0`, `1`, `2`, or `3`.
    Our choice of labels is arbitrary. Therefore, we need a scoring function that
    compares the two predictions while taking into account that the label names may
    vary. That's the job of the adjusted Rand index, which is going to be the topic
    of the next section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们的代码按预期工作，`y_pred_dash`中的值应该与上一节中的`y_pred`匹配。然而，`tree[0][0]`部分的树是否应被分配标签`0`、`1`、`2`或`3`并没有明确的规定。我们选择标签是任意的。因此，我们需要一个评分函数来比较这两个预测，同时考虑到标签名称可能会有所不同。这就是调整后的兰德指数（adjusted
    Rand index）的作用，接下来我们将讨论它。
- en: The adjusted Rand index
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整后的兰德指数
- en: 'The **adjusted Rand index** is very similar to the accuracy score in terms
    of its classification. It calculates the level of agreement between two lists
    of labels, yet it accounts for the following issues that the accuracy score cannot
    deal with:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**调整后的兰德指数**在分类中的计算方式与准确率评分非常相似。它计算两个标签列表之间的一致性，但它考虑了准确率评分无法处理的以下问题：'
- en: The adjusted rand index doesn't care much about the actual labels, as long as
    the members of one cluster here are the same members of the cluster there.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整后的兰德指数并不关心实际的标签，只要这里的一个聚类的成员和那里聚类的成员是相同的。
- en: Unlike in classification, we may end up having too many clusters. In the extreme
    case of having each sample as its own cluster, any two lists of clusters will
    agree with each other if we ignore the names of the labels. Thus, the adjusted
    rand index discounts the possibility of the two clusters agreeing by chance.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与分类不同，我们可能会得到太多的聚类。在极端情况下，如果每个样本都是一个独立的聚类，忽略标签名称的情况下，任何两个聚类列表都会一致。因此，调整后的兰德指数会减小两个聚类偶然一致的可能性。
- en: 'The best-adjusted rand index is`1` when the two predictions match. Thus, we
    can use it to compare`y_pred` with our `y_pred_dash`. The score is symmetric,
    so the order of its parameters doesn''t matter when calling the scoring function,
    as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当两个预测结果匹配时，最佳调整的兰德指数为`1`。因此，我们可以用它来比较`y_pred`和我们的`y_pred_dash`。该得分是对称的，因此在调用评分函数时，参数的顺序并不重要，如下所示：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Since we get an adjusted rand index of `1`, we can rest assured that our code
    for inferring the cluster memberships from the children tree is correct.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们得到了`1`的调整兰德指数，我们可以放心，推断子树中簇的成员资格的代码是正确的。
- en: I quickly mentioned that, in each iteration, the algorithm combines the two
    closest clusters. It is easy to imagine how distances are calculated between the
    two samples. They are basically two points and we have already used different
    distance measures, such as the Euclidean distance and Manhattan distance, before.
    However, a cluster is not a point. Where exactly should we measure the distances
    from? Shall we use the cluster's centroid? Shall we pick a specific data point
    within each cluster to calculate the distance from it? All these choices can be
    specified using the **linkage** hyperparameter. In the next section, we are going
    to see its different options.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前简要提到过，在每次迭代中，算法会合并两个最接近的簇。很容易想象，如何计算两个样本之间的距离。它们基本上是两个点，我们之前已经使用了不同的距离度量，例如欧氏距离和曼哈顿距离。然而，簇并不是一个点。我们应该从哪里开始测量距离呢？是使用簇的质心吗？还是在每个簇中选择一个特定的数据点来计算距离？所有这些选择都可以通过**连接**超参数来指定。在下一节中，我们将看到它的不同选项。
- en: Choosing the cluster linkage
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择聚类连接
- en: By default, the **Euclidean** distance is used to decide which cluster pairs
    are closest to each other. This default metric can be changed using the **affinity**
    hyperparameter. Please refer to [Chapter 5](b95b628d-5913-477e-8897-989ce2afb974.xhtml)*,
    Image Processing with Nearest Neighbors*, if you want to know more about the different
    distance metrics, such as the **c****osine** and**Manhattan** distance. When calculating
    the distance between two clusters, the **linkage** criterion decides how the distances
    can be measured, given the fact that a cluster usually contains more than one
    data point. In a *complete* linkage, the maximum distance between all the data
    points in the two clusters is used. Conversely, in a *single* linkage, the minimum
    distance is used. Clearly, the *average* linkage takes the average of all the
    distances between all sample pairs. In a *ward* linkage, two clusters are merged
    if the average Euclidean distances between each data point in the two clusters
    and the centroid of the merging cluster are at their minimum. Only Euclidean distances
    can be used with ward linkage.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，使用**欧氏**距离来决定哪些簇对彼此最接近。这个默认度量可以通过**亲和度**超参数进行更改。如果你想了解更多不同的距离度量，例如**余弦**和**曼哈顿**距离，请参考[第5章](b95b628d-5913-477e-8897-989ce2afb974.xhtml)*，最近邻的图像处理*。在计算两个簇之间的距离时，**连接**准则决定了如何测量这些距离，因为簇通常包含不止一个数据点。在*完全*连接中，使用两个簇中所有数据点之间的最大距离。相反，在*单一*连接中，使用最小距离。显然，*平均*连接取所有样本对之间所有距离的平均值。在*沃德*连接中，如果两个簇的每个数据点与合并簇的质心之间的平均欧氏距离最小，则这两个簇会合并。沃德连接仅能使用欧氏距离。
- en: 'To be able to compare the aforementioned linkage methods, we need to create
    a new dataset. The data points will be arranged in the form of two concentric
    circles. The smaller circle is enclaved into the bigger one, like Lesotho and
    South Africa. The `make_circles` function specifies the number of samples to generate
    (`n_samples`), how far apart the two circles are (`factor`), and how noisy the
    data is (`noise`):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够比较上述连接方法，我们需要创建一个新的数据集。数据点将以两个同心圆的形式排列。较小的圆嵌套在较大的圆内，就像莱索托和南非一样。`make_circles`函数指定了生成样本的数量（`n_samples`）、两个圆之间的距离（`factor`）以及数据的噪声大小（`noise`）：
- en: '[PRE19]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'I will display the resulting dataset in a bit, but first, let''s use the agglomerative
    algorithm to cluster the new data samples. I will run the algorithm twice: first
    with a complete linkage and then with a single linkage. I will be using Manhattan
    distance this time:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我稍后会显示生成的数据集，但首先，我们先使用凝聚算法对新的数据样本进行聚类。我将运行两次算法：第一次使用完全连接，第二次使用单一连接。这次我将使用曼哈顿距离：
- en: '[PRE20]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here are the results of the two linkage methods side by side:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是两种连接方法并排显示的结果：
- en: '![](img/c84230e2-77e0-4616-b3a2-40c162c83714.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c84230e2-77e0-4616-b3a2-40c162c83714.png)'
- en: 'When a single linkage is used, the shortest distance between each cluster pair
    is considered. This allows it to identify the circular strip where the data points
    have been arranged. The compete linkage considers the longest distances between
    the clusters. This resulted in more biased results. Clearly, the single linkage
    had the best results here. Nevertheless, it is subject to noise due to its variance.
    To demonstrate this, we can regenerate the circular samples once more after increasing
    the noise from `0.05` to `0.08`, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用单链聚合时，考虑每对聚类之间的最短距离。这使得它能够识别出数据点排列成的圆形带状区域。完全链聚合考虑聚类之间的最长距离，导致结果偏差较大。显然，单链聚合在这里获得了最佳结果。然而，由于其方差，它容易受到噪声的影响。为了证明这一点，我们可以在将噪声从`0.05`增加到`0.08`后，重新生成圆形样本，如下所示：
- en: '[PRE21]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Running the same clustering algorithm on the new samples will give us the following
    results:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在新样本上运行相同的聚类算法将给我们以下结果：
- en: '![](img/805f7f0e-cd92-42a4-bb30-501e56fa5dd2.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/805f7f0e-cd92-42a4-bb30-501e56fa5dd2.png)'
- en: The noisy data confused our single linkage this time, while the outcome of the
    complete linkage did not vary much. In the single linkage, a noisy point that
    falls between two clusters may cause them to merge. The average linkage can be
    seen as a middle ground between the single and the complete linkage criteria.
    Due to the iterative nature of these algorithms, the three linkage methods cause
    the bigger clusters to grow even bigger. This may result in uneven cluster sizes.
    If having imbalanced clusters must be avoided, then the ward linkage should be
    favored over the other three linkage methods.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这次噪声数据干扰了我们的单链聚合，而完全链聚合的结果变化不大。在单链聚合中，一个落在两个聚类之间的噪声点可能会导致它们合并。平均链聚合可以看作是单链和完全链聚合标准之间的中间地带。由于这些算法的迭代特性，三种链聚合方法会导致较大的聚类变得更大。这可能导致聚类大小不均。如果必须避免不平衡的聚类，那么应该优先选择沃德链聚合，而不是其他三种链聚合方法。
- en: So far, the desired number of clusters had to be predefined for the K-means
    and agglomerative clustering algorithms. Agglomerative**clustering**is computationally
    expensive compared to the K-means algorithm, while the K-means algorithm cannot
    deal with non-convex data. In the next section, we are going to see a third algorithm
    that doesn't require the number of clusters to be predefined.****
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，K-means 和层次聚类算法需要预先定义期望的聚类数量。与 K-means 算法相比，层次聚类**计算量大**，而 K-means 算法无法处理非凸数据。在下一节中，我们将看到第三种不需要预先定义聚类数量的算法。****
- en: '****# DBSCAN'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '****# DBSCAN'
- en: '"You never really understand a person until you consider things from his point
    of view."'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '"你永远无法真正理解一个人，除非你从他的角度考虑问题。"'
- en: '- Harper Lee'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '- 哈珀·李'
- en: The acronym **DBSCAN** stands for **density-based spatial clustering of applications
    with noise**. It sees clusters as areas of high density separated by areas of
    low density. This allows it to deal with clusters of any shape. This is in contrast
    to the K-means algorithm, which assumes clusters to be convex; that is, data blobs
    with centroids. The DBSCAN**algorithm starts by identifying the core samples.
    These are points that have at least `min_samples` around them within a distance
    of `eps` (***ε***). Initially, a cluster is built out of its core samples. Once
    a core sample has been identified, its neighbors are also examined and added to
    the cluster if they meet the core sample criteria. Then, the cluster is expanded
    so that we can add non-core samples to it. These are samples that can be reached
    directly from the core samples within a distance of `eps` but are not core samples
    themselves. Once all the clusters have been identified, along with their core
    and non-core samples, the remaining samples are considered noise.**
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 缩写**DBSCAN**代表**基于密度的噪声应用空间聚类**。它将聚类视为高密度区域，之间由低密度区域分隔。这使得它能够处理任何形状的聚类。这与假设聚类为凸形的
    K-means 算法不同；即数据簇与质心。DBSCAN**算法首先通过识别核心样本来开始。这些是周围至少有 `min_samples` 点，且距离在 `eps`
    (***ε***) 内的点。最初，一个聚类由其核心样本组成。一旦识别出核心样本，它的邻居也会被检查，并且如果符合核心样本标准，就将其添加到聚类中。接着，聚类将被扩展，以便我们可以将非核心样本添加到其中。这些样本是可以直接从核心样本通过
    `eps` 距离到达的点，但它们本身不是核心样本。一旦所有的聚类被识别出来，包括核心样本和非核心样本，剩余的样本将被视为噪声。**
- en: '**It is clear that the `min_samples` and `eps` hyperparameters play a big role
    in the final predictions. Here, we''re setting `min_samples` to `3` and trying
    a different setting for `eps`***:***'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**很明显，`min_samples`和`eps`超参数在最终预测中起着重要作用。这里，我们将`min_samples`设置为`3`并尝试不同的`eps`设置***:***'
- en: '[PRE22]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The resulting clusters for the blobs dataset help us identify the effect of
    the `eps`**hyperparameter:**
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 对于blobs数据集，结果聚类帮助我们识别`eps`**超参数的影响:**
- en: '**![](img/df4122fc-30e5-4c42-a0f8-bbda069507d7.png)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**![](img/df4122fc-30e5-4c42-a0f8-bbda069507d7.png)'
- en: A very small `eps` does not allow any core samples to form. When `eps` was set
    to `0.1`, almost all the points were treated as noise. The core points started
    to form as we increased the value of `eps`. However, at some point, when `eps`**was
    set to `0.5`, two clusters were mistakenly merged.**
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常小的`eps`值不允许任何核心样本形成。当`eps`被设置为`0.1`时，几乎所有的点都被当作噪声处理。当我们增加`eps`值时，核心点开始形成。然而，在某个时刻，当`eps`**设置为`0.5`时，两个簇错误地合并在一起。**
- en: '**Similarly, the value of `min_samples` can make or break our clustering algorithm.
    Here, we''re going to try different values of `min_samples`**for our concentric
    data points:**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**同样，`min_samples`的值可以决定我们的聚类算法是否成功。这里，我们将尝试不同的`min_samples`值**来对我们的同心数据点进行处理:**'
- en: '**[PRE23]'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE23]'
- en: 'Here, we can see the effect of `min_samples` on our clustering results:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`min_samples`对聚类结果的影响：
- en: '![](img/00022b95-11a4-44a4-9c8e-4de20b8a90a9.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](img/00022b95-11a4-44a4-9c8e-4de20b8a90a9.png)'
- en: Once more, a careful choice of `min_samples` gave the best results. In contrast
    to `eps`, the bigger the value of `min_samples`, the harder it is for the core
    samples to form.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，仔细选择`min_samples`给出了最好的结果。与`eps`不同，`min_samples`值越大，核心样本越难以形成。**
- en: In addition to the aforementioned hyperparameters, we can also change the distance
    metric used by the algorithm. Usually, `min_samples` takes values above three.
    Setting `min_samples` to one means that each sample will be its own cluster, while
    setting it to two will give similar results to the agglomerative clustering algorithm,
    but with a single linkage. You may start by setting the `min_samples` value to
    double the dimensionality of your data; that is, twice the number of features.
    Then, you may increase it if your data is known to be noisy and decrease it otherwise.
    As for `eps`, we can use the following **k-distance graph**.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 除了上述超参数外，我们还可以更改算法使用的距离度量。通常，`min_samples`取值大于三。将`min_samples`设置为一意味着每个样本将成为自己的簇，而将其设置为二将给出类似于层次聚类算法的结果，但采用单链接法。你可以从将`min_samples`值设置为数据维度的两倍开始；也就是说，设置为特征数量的两倍。然后，如果数据已知是噪声较多的，你可以增加该值，否则可以减少它。至于`eps`，我们可以使用以下**k距离图**。
- en: 'In the concentric dataset, we set `min_samples` to three. Now, for each sample,
    we want to see how far its two neighbors are. The following code snippet calculates
    the distance between each point and its closest two neighbors:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在同心数据集上，我们将`min_samples`设置为三。现在，对于每个样本，我们想要查看它的两个邻居有多远。以下代码片段计算每个点与其最接近的两个邻居之间的距离：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If `min_samples` was set to any other number, we would have wanted to get as
    many neighbors as that number, minus one. Now, we can focus on the farthest neighbor
    of the two for each sample and plot all the resulting distances, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`min_samples`设置为其他任何数值，我们希望得到与该数值相等的邻居数量，减去一。现在，我们可以关注每个样本的两个邻居中最远的一个，并绘制所有结果的距离，如下所示：
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The resulting graph will look as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图形将如下所示：
- en: '![](img/6264955d-c1b2-4dbc-b366-674508dc0573.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6264955d-c1b2-4dbc-b366-674508dc0573.png)'
- en: The point where the graph changes its slope dramatically gives us a rough estimate
    for our `eps` value. Here, when`min_samples` was set to three, an `eps` value
    of `0.2` sounded quite right. Furthermore, we can try different values for these
    two numbers and use the silhouette score or any other clustering metric to fine-tune
    our hyperparameters.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图表剧烈改变斜率的点为我们提供了`eps`值的大致估计。在这里，当`min_samples`设置为三时，`eps`值为`0.2`似乎非常合适。此外，我们可以尝试这些两个数值的不同组合，并使用轮廓系数或其他聚类度量来微调我们的超参数。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: The British historian Arnold Toynbee once said, "*n**o tool is omnicompetent"*.
    In this chapter, we used three tools for clustering. Each of the three algorithms
    we discussed here approaches the problem from a different angle. The K-means clustering
    algorithm tries to find points that summarize the clusters and the centroids and
    builds its clusters around them. The agglomerative clustering approach is more
    of a bottom-up approach, while the DBSCAN clustering algorithm introduces new
    concepts such as core points and density. This chapter is the first of three chapters
    to deal with unsupervised learning problems. The lack of labels here forced us
    to learn about newer evaluation metrics, such as the adjusted rand index and the
    silhouette score.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 英国历史学家阿诺德·汤因比曾说过，"*没有哪种工具是全能的*。" 在本章中，我们使用了三种工具来进行聚类。我们在这里讨论的三种算法从不同的角度处理问题。K-means
    聚类算法试图找到总结簇和质心的点，并围绕它们构建聚类。凝聚层次聚类方法则更倾向于自下而上的方式，而 DBSCAN 聚类算法则引入了核心点和密度等新概念。本章是三章关于无监督学习问题的第一章。由于缺乏标签，我们被迫学习了新的评估指标，如调整兰德指数和轮廓系数。
- en: 'In the next chapter, we are going to deal with our second unsupervised learning
    problem: **anomaly detection**. Luckily, the concepts discussed here, as well
    as the ones from[Chapter 5](b95b628d-5913-477e-8897-989ce2afb974.xhtml), *Image
    Processing with Nearest Neighbors,*about nearest neighbors and nearest centroid
    algorithms will help us in the next chapter. Once more, we will be given unlabeled
    data samples, and we will be tasked with picking the odd samples out.***************'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将处理第二个无监督学习问题：**异常检测**。幸运的是，本章中讨论的概念，以及[第五章](b95b628d-5913-477e-8897-989ce2afb974.xhtml)中关于最近邻和最近质心算法的内容，将帮助我们在下一章中进一步理解。再一次，我们将得到未标记的数据样本，我们的任务是挑出其中的异常样本。
