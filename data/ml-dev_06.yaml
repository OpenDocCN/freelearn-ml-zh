- en: Convolutional Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Well, now things are getting fun! Our models can now learn more complex functions,
    and we are now ready for a wonderful tour around the more contemporary and surprisingly
    effective models
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，现在事情变得有趣了！我们的模型现在可以学习更复杂的函数，我们现在准备进行一次关于更现代且出人意料的有效的模型的奇妙之旅
- en: After piling layers of neurons became the most popular solution to improving
    models, new ideas for richer nodes appeared, starting with models based on human
    vision. They started as little more than research themes, and after the image
    datasets and more processing power became available, they allowed researchers
    to reach almost human accuracy in classification challenges, and we are now ready
    to leverage that power in our projects.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在堆叠层神经元成为提高模型的最流行解决方案之后，出现了更丰富节点的创新想法，始于基于人类视觉的模型。它们最初只是研究主题，随着图像数据集和更多处理能力的出现，它们使研究人员在分类挑战中几乎达到人类的准确性，我们现在准备在我们的项目中利用这种力量。
- en: 'The topics we will cover in this chapter are as follows:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖以下主题：
- en: Origins of convolutional neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络的起源
- en: Simple implementation of discrete convolution
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散卷积的简单实现
- en: 'Other operation types: pooling, dropout'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他操作类型：池化、dropout
- en: Transfer learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习
- en: Origin of convolutional neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络的起源
- en: '**Convolutional neural networks** (**CNNs**) have a remote origin. They developed
    while **multi-layer perceptrons** were perfected, and the first concrete example
    is the **neocognitron**.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）有一个遥远的起源。它们在多层感知器得到完善的同时发展起来，第一个具体的例子是**新认知机**。'
- en: 'The neocognitron is a hierarchical, multilayered **Artificial Neural Network** (**ANN**),
    and was introduced in a 1980 paper by Prof. Fukushima and has the following principal
    features:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 新认知机是一个分层、多层的**人工神经网络**（**ANN**），由福岛教授在1980年的一篇论文中引入，并具有以下主要特点：
- en: Self-organizing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自组织
- en: Tolerant to shifts and deformation in the input
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对输入的偏移和变形具有容忍性
- en: '![](img/c29e144d-13cd-46c1-a2fd-a92cc2b6aa99.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c29e144d-13cd-46c1-a2fd-a92cc2b6aa99.png)'
- en: This original idea appeared again in 1986 in the book version of the original
    backpropagation paper, and was also employed in 1988 for temporal signals in speech
    recognition.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个原始想法在1986年的原始反向传播论文的书籍版本中再次出现，并在1988年被用于语音识别中的时间信号。
- en: The design was improved in 1998, with a paper from Ian LeCun*,* *Gradient-Based
    Learning Aapplied to Document Recognition*, presenting the LeNet-5 network, an
    architecture used to classify handwritten digits. The model showed increased performance
    compared to other existing models, especially over several variations of SVM,
    one of the most performant operations in the year of publication.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 设计在1998年得到了改进，一篇由Ian LeCun*、* *梯度学习应用于文档识别*的论文提出了LeNet-5网络，这是一种用于分类手写数字的架构。该模型与当时其他现有模型相比，性能有所提高，特别是在SVM（当时最有效的操作之一）的几个变体上。
- en: Then a generalization of that paper came in 2003, with "*Hierarchical Neural
    Networks for Image Interpretation*". But in general, almost all kernels followed
    the original idea, until now.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，2003年出现了该论文的推广，“*用于图像解释的分层神经网络*”。但总的来说，几乎所有核都遵循原始想法，直到现在。
- en: Getting started with convolution
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积入门
- en: In order to understand convolution, we will start by studying the origin of
    the convolution operator, and then we will explain how this concept is applied
    to the information.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解卷积，我们将从研究卷积算子的起源开始，然后我们将解释这个概念是如何应用于信息的。
- en: Convolution is basically an operation between two functions, continuous or discrete,
    and in practice, it has the effect of filtering one of them by another.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积基本上是两个函数之间的操作，连续或离散，在实践中，它通过另一个函数过滤其中一个的效果。
- en: It has many uses across diverse fields, especially in digital signal processing,
    where it is the preferred tool for shaping and filtering audio, and images, and
    it is even used in probabilistic theory, where it represents the sum of two independent
    random variables.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 它在众多领域中有许多用途，特别是在数字信号处理中，它是塑造和过滤音频、图像的首选工具，甚至在概率论中也有应用，它代表两个独立随机变量的和。
- en: And what do these filtering capabilities have to do with machine learning? The
    answer is that with filters, we will be able to build network nodes that can emphasize
    or hide certain characteristics of our inputs (by the definition of the filters)
    so we can build automatic custom detectors for all the features, which can be
    used to detect a determinate pattern. We will cover more of this in the following
    sections; now, let's review the formal definition of the operation, and a summary
    of how it is calculated.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这些滤波能力与机器学习有什么关系呢？答案是，通过滤波器，我们将能够构建能够强调或隐藏输入中某些特征的网络节点（根据滤波器的定义），这样我们就可以构建用于检测所有特征的自动定制检测器，这些检测器可以用来检测特定的模式。我们将在接下来的章节中更详细地介绍这一点；现在，让我们回顾该操作的正式定义及其计算摘要。
- en: Continuous convolution
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续卷积
- en: 'Convolution as an operation was first created during the 18 century by d''Alembert during
    the initial developments of differential calculus. The common definition of the
    operation is as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积作为运算最早是在18世纪由d'Alembert在微分学初步发展期间创造的。该操作的常见定义如下：
- en: '![](img/cfa53da3-92fd-4150-919e-5949eecac3ba.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cfa53da3-92fd-4150-919e-5949eecac3ba.png)'
- en: 'If we try to describe the steps needed to apply the operation, and how it combines
    two functions, we can express the mathematical operation involved in the following
    detail:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试描述应用该操作所需的步骤以及它是如何结合两个函数的，我们可以以下列详细方式表达涉及的数学运算：
- en: 'Flip the signal: This is the (*-τ*) part of the variable'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 翻转信号：这是变量的 (*-τ*) 部分
- en: 'Shift it: This is given by the *t* summing factor for *g(τ)*'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平移它：这是由 *t* 的求和因子给出的 *g(τ)*
- en: 'Multiply it: The product of *f* and **g**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘以它：*f* 和 **g** 的乘积
- en: 'Integrate the resulting curve: This is the less intuitive part, because each
    instantaneous value is the result of an integral'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对结果曲线进行积分：这是不太直观的部分，因为每个瞬时值都是积分的结果
- en: 'In order to understand all the steps involved, let''s visually represent all
    the steps involved for the calculation of the convolution between two functions,
    *f* and *g*, at a determinate point *t*[*0*]:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解所有涉及的步骤，让我们直观地表示出两个函数在确定点 *t*[*0*] 之间计算卷积的所有步骤，*f* 和 *g*：
- en: '![](img/1827d5c2-accf-4d2f-93bc-713e522cd3eb.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1827d5c2-accf-4d2f-93bc-713e522cd3eb.png)'
- en: This intuitive approximation to the rules of the convolution also applies to
    the discrete field of functions, and it is the real realm in which we will be
    working. So, let's start by defining it.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对卷积规则的直观近似也适用于函数的离散领域，这是我们实际工作的真实领域。因此，让我们首先定义它。
- en: Discrete convolution
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 离散卷积
- en: 'Even when useful, the realm in which we work is eminently digital, so we need
    to translate this operation to the discrete domain. The convolution operation
    for two discrete functions *f* and *g* is the translation of the original integral
    to an equivalent summation in the following form:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在有用的情况下，我们工作的领域无疑是数字化的，因此我们需要将这个操作转换到离散域。两个离散函数 *f* 和 *g* 的卷积操作是将原始积分转换为以下形式的等效求和：
- en: '![](img/c2080076-17fb-4e9f-ab49-c12ada720b31.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c2080076-17fb-4e9f-ab49-c12ada720b31.png)'
- en: This original definition can be applied to functions of an arbitrary number
    of dimensions. In particular, we will work on 2D images, because this is the realm
    of a large number of applications, which we will describe further in this chapter.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '这个原始定义可以应用于任意数量的维度上的函数。特别是，我们将处理二维图像，因为这是许多应用领域的领域，我们将在本章中进一步描述。 '
- en: Now it's time to learn the way in which we normally apply the convolution operator,
    which is through kernels.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是学习我们通常应用卷积算子方式的时候了，这种方式是通过核来实现的。
- en: Kernels and convolutions
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 核和卷积
- en: When solving practical problems in the discrete domain, we normally have 2D
    functions of finite dimensions (which could be an image, for example) that we
    want to filter through another image. The discipline of filter development studies
    the effects of different kinds of filters when applied via convolution to a variety
    of classes. The most common types of function applied are of two to five elements
    per dimension, and of 0 value on the remaining elements. These little matrices,
    representing filtering functions, are called **kernels**.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决离散域中的实际问题时，我们通常有有限维度的二维函数（例如，可能是一个图像），我们希望通过另一个图像对其进行过滤。滤波器开发学科研究将不同类型的滤波器通过卷积应用于各种类别时产生的影响。最常用的函数类型是每维两个到五个元素，其余元素为0值。这些代表滤波函数的小矩阵被称为**核**。
- en: The convolution operation starts with the first element of an *n*-dimensional
    matrix (usually a 2D matrix representing an image) with all the elements of a
    kernel, applying the center element of the matrix to the specific value we are
    multiplying, and applying the remaining factors following the kernel's dimensions.
    The final result is, in the case of an image, an equivalent image in which certain
    elements of it (for example, lines and edges) have been highlighted, and others
    (for example, in the case of blurring) have been hidden.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积操作从*n*维矩阵（通常是一个表示图像的2D矩阵）的第一个元素开始，与核的所有元素相乘，将矩阵的中心元素应用于我们正在乘以的特定值，然后按照核的维度应用剩余的因子。在图像的情况下，最终结果是一个等效图像，其中某些元素（例如，线条和边缘）被突出显示，而其他元素（例如，在模糊的情况下）则被隐藏。
- en: 'In the following example, you will see how a particular 3 x 3 kernel is applied
    to a particular image element. This is repeated in a scan pattern to all elements
    of the matrix:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，您将看到如何将特定的3 x 3核应用于特定的图像元素。这以扫描模式重复应用于矩阵的所有元素：
- en: '![](img/8732d877-f163-44ff-8548-701da33e19ad.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8732d877-f163-44ff-8548-701da33e19ad.png)'
- en: Kernels also have a couple of extra elements to consider when applying them,
    specifically stride and padding, which complete the specification for accommodating
    special application cases. Let's have a look at stride and padding.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用核时，还需要考虑一些额外的元素，特别是步长和填充，这些元素完成了适应特殊应用案例的规格。让我们来看看步长和填充。
- en: Stride and padding
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 步长和填充
- en: 'When applying the convolution operation, one of the variations that can be
    applied to the process is to change the displacement units for the kernels. This
    parameter, which can be specified per dimension, is called **stride**. In the
    following image, we show a couple of examples of how stride is applied. In the
    third case, we see an incompatible stride because the kernel can''t be applied
    to the last step. Depending on the library, this type of warning can be dismissed:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用卷积操作时，可以对过程应用的一种变化是改变核的位移单位。这个参数可以按维度指定，称为**步长**。在以下图像中，我们展示了步长应用的一些示例。在第三种情况下，我们看到一个不兼容的步长，因为核不能应用于最后一步。根据库的不同，这种类型的警告可以被忽略：
- en: '![](img/9998091b-8d00-42e9-a034-11f3ebf7f49d.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9998091b-8d00-42e9-a034-11f3ebf7f49d.png)'
- en: 'The other important fact when applying a kernel is that the bigger the kernel,
    the more units there are on the border of the image/matrix that won''t receive
    an answer because we need to cover the entire kernel. In order to cope with that,
    the **padding** parameters will add a border of the specified width to the image
    to allow the kernel to be able to apply evenly to the edge pixels/elements. Here,
    you have a graphical depiction of the padding parameter:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用核时，另一个重要的事实是，核越大，图像/矩阵边缘不接收答案的单位就越多，因为我们需要覆盖整个核。为了应对这种情况，**填充**参数将在图像上添加一个指定宽度的边框，以便核能够均匀地应用于边缘像素/元素。这里是对填充参数的图形描述：
- en: '![](img/e8261782-72b9-4c56-8e97-c62b33b19db0.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e8261782-72b9-4c56-8e97-c62b33b19db0.png)'
- en: After describing the fundamental concepts of convolution, let's implement convolution
    in a practical example to see it applied to real image and get an intuitive idea
    of its effects.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在描述了卷积的基本概念之后，让我们通过一个实际例子来实现卷积，看看它如何应用于真实图像，并对其效果有一个直观的认识。
- en: Implementing the 2D discrete convolution operation in an example
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在一个例子中实现2D离散卷积操作
- en: 'In order to understand the mechanism of the discrete convolution operation,
    let''s do a simple intuitive implementation of this concept and apply it to a
    sample image with different types of kernel. Let''s import the required libraries.
    As we will implement the algorithms in the clearest possible way, we will just
    use the minimum necessary ones, such as NumPy:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解离散卷积操作的机制，让我们对这个概念进行简单的直观实现，并将其应用于具有不同类型核的样本图像。让我们导入所需的库。由于我们将以尽可能清晰的方式实现算法，我们将只使用最必要的库，例如NumPy：
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Using the `imread` method of the `imageio` package, let''s read the image (imported
    as three equal channels, as it is grayscale). We then slice the first channel,
    convert it to a floating point, and show it using matplotlib:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`imageio`包的`imread`方法，让我们读取图像（作为三个相等的通道导入，因为它为灰度图）。然后我们切分第一个通道，将其转换为浮点数，并使用matplotlib显示：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](img/e65c77d4-672a-4da5-b364-a6c8dd099a5a.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e65c77d4-672a-4da5-b364-a6c8dd099a5a.png)'
- en: 'Now it''s time to define the kernel convolution operation. As we did previously,
    we will simplify the operation on a 3 x 3 kernel in order to better understand
    the border conditions.  `apply3x3kernel` will apply the kernel over all the elements
    of the image, returning a new equivalent image. Note that we are restricting the
    kernels to 3 x 3 for simplicity, and so the 1 pixel border of the image won''t
    have a new value because we are not taking padding into consideration:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是定义内核卷积操作的时候了。正如我们之前所做的那样，我们将简化3 x 3内核上的操作，以便更好地理解边界条件。`apply3x3kernel`将在图像的所有元素上应用内核，返回一个等效的新图像。请注意，为了简单起见，我们将内核限制为3
    x 3，因此图像的1像素边界不会有一个新值，因为我们没有考虑填充：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As we saw in the previous sections, the different kernel configurations highlight
    different elements and properties of the original image, building filters that
    in conjunction can specialize in very high-level features after many epochs of
    training, such as eyes, ears, and doors. Here, we will generate a dictionary of
    kernels with a name as the key, and the coefficients of the kernel arranged in
    a 3 x 3 array. The `Blur` filter is equivalent to calculating the average of the
    3 x 3 point neighborhood, `Identity` simply returns the pixel value as is, `Laplacian`
    is a classic derivative filter that highlights borders, and then the two `Sobel`
    filters will mark horizontal edges in the first case, and vertical ones in the
    second case:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中看到的，不同的内核配置突出了原始图像的不同元素和属性，构建的过滤器在经过许多个训练周期后可以专门处理非常高级的特征，例如眼睛、耳朵和门。在这里，我们将生成一个以名称为键、内核系数按3
    x 3数组排列的内核字典。`模糊`过滤器相当于计算3 x 3点邻域的平均值，`恒等`简单地返回像素值，`拉普拉斯`是一个经典的导数过滤器，突出显示边界，然后两个`索贝尔`过滤器将在第一种情况下标记水平边缘，在第二种情况下标记垂直边缘：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s generate a `ConvolutionalOperation` object and generate a comparative
    kernel graphical chart to see how they compare:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成一个`ConvolutionalOperation`对象，并生成一个比较内核图形图表，以查看它们如何比较：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the final image you can clearly see how our kernel has detected several
    high-detail features on the image—in the first one, you see the unchanged image
    because we used the unit kernel, then the Laplacian edge finder, the left border
    detector, the upper border detector, and then the blur operator:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终图像中，你可以清楚地看到我们的内核如何在图像上检测到几个高细节特征——在第一个中，你看到的是未更改的图像，因为我们使用了单位内核，然后是拉普拉斯边缘检测器、左侧边界检测器、顶部边界检测器，然后是模糊操作器：
- en: '![](img/fabe713d-8ade-4606-a804-df420bfbfae5.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fabe713d-8ade-4606-a804-df420bfbfae5.png)'
- en: Having reviewed the main characteristics of the convolution operation for the
    continuous and discrete fields, we can conclude by saying that, basically, convolution
    kernels highlight or hide patterns. Depending on the trained or (in our example)
    manually set parameters, we can begin to discover many elements in the image,
    such as orientation and edges in different dimensions. We may also cover some
    unwanted details or outliers by blurring kernels, for example. Additionally, by
    piling layers of convolutions, we can even highlight higher-order composite elements,
    such as eyes or ears.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在回顾了连续和离散域上卷积操作的主要特征之后，我们可以得出结论，基本上，卷积内核突出或隐藏模式。根据训练的或（在我们的例子中）手动设置的参数，我们可以开始发现图像中的许多元素，例如不同维度的方向和边缘。我们还可以通过模糊内核覆盖一些不需要的细节或异常值，例如。此外，通过堆叠卷积层，我们甚至可以突出更高阶的复合元素，如眼睛或耳朵。
- en: 'This characteristic of convolutional neural networks is their main advantage
    over previous data-processing techniques: we can determine with great flexibility
    the primary components of a certain dataset, and represent further samples as
    a combination of these basic building blocks.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种卷积神经网络的特征是它们相对于先前数据处理技术的优势：我们可以非常灵活地确定某个数据集的主要组成部分，并将进一步样本表示为这些基本构建块的组合。
- en: Now it's time to look at another type of layer that is commonly used in combination
    with the former—the pooling layer.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是查看另一种通常与前者结合使用的层的时刻——池化层。
- en: Subsampling operation (pooling)
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 子采样操作（池化）
- en: The subsampling operation consists of applying a kernel (of varying dimensions)
    and reducing the extension of the input dimensions by dividing the image into *mxn*
    blocks and taking one element representing that block, thus reducing the image
    resolution by some determinate factor. In the case of a 2 x 2 kernel, the image
    size will be reduced by half. The most well-known operations are maximum (max
    pool), average (avg pool), and minimum (min pool).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 子采样操作包括应用一个核（尺寸可变）并通过将图像分成*m*n块并取代表该块的元素来减少输入维度的扩展，从而通过某个确定的因子降低图像分辨率。在2 x 2核的情况下，图像大小将减半。最著名的操作是最大（max
    pool）、平均（avg pool）和最小（min pool）池化。
- en: 'The following image gives you an idea of how to apply a 2 x 2 `maxpool` kernel,
    applied to a one-channel 16 x 16 matrix. It just maintains the maximum value of
    the internal zone it covers:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像展示了如何应用一个2 x 2 `maxpool`核，应用于一个单通道16 x 16矩阵。它只是保持它所覆盖的内部区域的最大值：
- en: '![](img/854251eb-1ea4-4f3a-aa9b-6e3aa8afaf0a.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/854251eb-1ea4-4f3a-aa9b-6e3aa8afaf0a.png)'
- en: 'Now that we have seen this simple mechanism, let''s ask ourselves, what''s
    the main purpose of it? The main purpose of subsampling layers is related to the
    convolutional layers: to reduce the quantity and complexity of information while
    retaining the most important information elements. In other word, they build a
    ***compact representation*** of the underlying information.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了这个简单的机制，让我们问问自己，它的主要目的是什么？子采样层的主要目的与卷积层相关：在保留最重要的信息元素的同时，减少信息和复杂性的数量和复杂性。换句话说，它们构建了底层信息的**紧凑表示**。
- en: 'Now it''s time to write a simple pooling operator. It''s much easier and more
    direct to write than a convolutional operator, and in this case we will only be
    implementing max pooling, which chooses the brightest pixel in the 4 x 4 vicinity
    and projects it to the final image:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候编写一个简单的池化算子了。与卷积算子相比，编写起来要简单得多，而且更直接，在这种情况下，我们只将实现最大池化，它选择4 x 4邻域中最亮的像素并将其投影到最终图像：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let''s apply the newly created pooling operation, and as you can see, the final
    image resolution is much more blocky, and the details, in general, are brighter:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用新创建的池化操作，正如你所见，最终图像的分辨率要粗糙得多，而且总体上细节更亮：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here you can see the differences, even though they are subtle. The final image
    is of lower precision, and the chosen pixels, being the maximum of the environment,
    produce a brighter image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到差异，尽管它们很微妙。最终图像的精度较低，所选像素作为环境中的最大值，产生了一个更亮的图像：
- en: '![](img/c37f3486-e074-4d1f-96d4-b1adbd85a921.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c37f3486-e074-4d1f-96d4-b1adbd85a921.png)'
- en: Improving efficiency with the dropout operation
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用dropout操作提高效率
- en: As we have observed in the previous chapters, overfitting is a potential problem
    for every model. This is also the case for neural networks, where data can do
    very well on the training set but not on the test set, which renders it useless
    for generalization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中观察到的，过拟合是每个模型的潜在问题。这种情况也适用于神经网络，数据可以在训练集上做得很好，但在测试集上却不行，这使得它对泛化没有用。
- en: 'For this reason, in 2012, a team led by Geoffrey Hinton published a paper in
    which the dropout operation was described. Its operation is simple:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在2012年，由Geoffrey Hinton领导的一个团队发表了一篇论文，其中描述了dropout操作。其操作很简单：
- en: A random number of nodes is chosen (the ratio of the chosen node from the total
    is a parameter)
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机的节点数量（所选节点与总数之比是一个参数）
- en: The values of the chosen weights are reviews to zero, invalidating their previously
    connected peers at the subsequent layers
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所选权重的值被重置为零，从而在后续层中使之前连接的同伴无效
- en: Advantages of the dropout layers
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: dropout层的优势
- en: The main advantage of this method is that it prevents all neurons in a layer
    from synchronously optimizing their weights. This adaptation, made in random groups,
    prevents all the neurons from converging to the same goal, thus decorrelating
    the weights.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要优势是它防止了同一层中所有神经元同步优化它们的权重。这种在随机组中进行的适应，防止了所有神经元收敛到同一个目标，从而解耦了权重。
- en: A second property discovered for the application of dropout is that the activations
    of the hidden units become sparse, which is also a desirable characteristic.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 对于dropout的应用，还发现了一个第二性质，即隐藏单元的激活变得稀疏，这也是一个期望的特性。
- en: 'In the following diagram, we have a representation of an original, fully-connected
    multi-layer neural network, and the associated network with dropout:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们展示了原始的、全连接的多层神经网络及其相关的具有dropout功能的网络：
- en: '>![](img/000dfeda-4caa-4272-b4a2-28903b1d4044.png)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '>![](img/000dfeda-4caa-4272-b4a2-28903b1d4044.png)'
- en: Deep neural networks
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度神经网络
- en: Now that we have a rich number of layers, it's time to start a tour of how the
    neural architectures have evolved over time. Starting in 2012, a rapid succession
    of new and increasingly powerful combinations of layers began, and it has been
    unstoppable. This new set of architectures adopted the term **deep learning**,
    and we can approximately define them as complex neural architectures that involve
    at least three layers. They also tend to include more advanced layers than the
    **Single Layer Perceptrons**, like convolutional ones.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了丰富的层数，是时候开始了解神经网络架构随时间演变的历程了。从2012年开始，一系列新的、越来越强大的层组合迅速出现，并且势头不可阻挡。这一套架构采用了**深度学习**这一术语，我们可以大致将其定义为至少包含三个层的复杂神经网络架构。它们还倾向于包含比**单层感知器**更先进的层，如卷积层。
- en: Deep convolutional network architectures through time
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随时间演变的深度卷积网络架构
- en: Deep learning architectures date from 20 years ago and have evolved, guided
    for the most part by the challenge of solving the human vision problem. Let's
    have a look at the main deep learning architectures and their principal building
    blocks, which we can then reuse for our own purposes.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习架构可以追溯到20年前，并在很大程度上由解决人类视觉问题的挑战所引导。让我们来看看主要的深度学习架构及其主要构建块，然后我们可以为我们的目的重用它们。
- en: Lenet 5
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Lenet 5
- en: As we saw in the historical introduction of the convolutional neural networks,
    convolutional layers were discovered during the 1980s. But the available technology
    wasn't powerful enough to build complex combinations of layers until the end of
    the 1990s.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在卷积神经网络的历史介绍中所见，卷积层是在20世纪80年代发现的。但直到1990年代末，可用的技术还不够强大，无法构建复杂的层组合。
- en: Around 1998, in Bell Labs, during research around the decodification of handwritten
    digits, Ian LeCun formed a new approach—a mix of convolutional, pooling, and fully
    connected layers—to solve the problem of recognizing handwritten digits.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 大约在1998年，在贝尔实验室，在研究手写数字解码的过程中，伊恩·李飞飞提出了一种新的方法——结合卷积、池化和全连接层来解决识别手写数字的问题。
- en: At this time, SVM and other much more mathematically defined problems were used
    more or less successfully, and the fundamental paper on CNNs shows that neural
    networks could perform comparatively well with the then state-of-the-art methods.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个时候，SVM和其他更多数学定义的问题被或多或少地成功应用，CNNs的基本论文表明神经网络可以与当时最先进的方法相比拟地表现良好。
- en: 'In the following diagram, there is a representation of all the layers of this
    architecture, which received a grayscale 28 x 28 image as input and returned a
    10-element vector, with the probability for each character as the output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，展示了该架构的所有层，它接收一个28 x 28像素的灰度图像作为输入，并返回一个包含每个字符概率的10元素向量：
- en: '![](img/564b343b-c5aa-444e-b00d-38f719b0c32a.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/564b343b-c5aa-444e-b00d-38f719b0c32a.png)'
- en: Alexnet
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Alexnet
- en: After some more years of hiatus (even though Lecun was applying his networks
    to other tasks, such as face and object recognition), the exponential growth of
    both available structured data and raw processing power allowed the teams to grow
    and tune the models to an extent that could have been considered impossible just
    a few years before.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 几年后，尽管李飞飞将神经网络应用于其他任务，如人脸和物体识别，但可用结构化数据和原始处理能力的指数级增长使得团队得以扩大规模，并将模型调整到几年前被认为不可能的程度。
- en: One of the elements that fostered innovation in the field was the availability
    of an image recognition benchmark called **Imagenet**, consisting of millions
    of images of objects organized into categories.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 促进该领域创新的一个因素是图像识别基准**Imagenet**的可用性，它包含数百万张按类别组织的物体图像。
- en: Starting in 2012, the **Large Scale Visual Recognition Challenge (LSVRC)** ran
    every year and helped researchers to innovate in network configurations, obtaining
    better and better results every year.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从2012年开始，每年都会举办**大规模视觉识别挑战赛（LSVRC**），这有助于研究人员在网络配置上进行创新，每年都取得了更好的成果。
- en: '**Alexnet**, developed by Alex Krizhevsky, was the first deep convolutional
    network that won this challenge, and set a precedent for years to come. It consisted
    of a model similar in structure to Lenet-5, but the convolutional layers of which
    had a depth of hundreds of units, and the total number of parameters was in the
    tens of millions.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由 Alex Krizhevsky 开发的 **AlexNet** 是第一个赢得这一挑战的深度卷积网络，并为未来的几年设定了先例。它由一个与 Lenet-5
    结构相似的模型组成，但其卷积层的深度达到数百个单位，总参数数量达到数百万。
- en: The following challenges saw the appearance of a powerful contender, the **Visual
    Geometry Group (VGG)** from Oxford University, with its VGG model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的挑战中，牛津大学的 **Visual Geometry Group (VGG)** 出现了一个强大的竞争者，其 VGG 模型。
- en: The VGG model
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: VGG 模型
- en: The main characteristic of the VGG network architecture is that it reduced the
    size of the convolutional filters to a simple 3 x 3 matrix and combined them in
    sequences, which was different to previous contenders, which had large kernel
    dimensions (up to 11 x 11).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: VGG 网络架构的主要特点是它将卷积核的大小减少到简单的 3 x 3 矩阵，并将它们按序列组合，这与之前的竞争者不同，它们的核尺寸较大（高达 11 x
    11）。
- en: Paradoxically, the series of small convolutional weights amounted to a really
    large number of parameters (in the order of many millions), and so it had to be
    limited by a number of measures.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 反讽的是，一系列小的卷积权重加起来却是一个非常大的参数数量（达到数百万级别），因此它必须通过多种措施进行限制。
- en: '![](img/7aee123c-c553-4558-a73d-a924b6c642a7.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![批归一化的 Inception V2 和 V3](img/7aee123c-c553-4558-a73d-a924b6c642a7.png)'
- en: GoogLenet and the Inception model
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GoogLeNet 和 Inception 模型
- en: '**GoogLenet** was the neural network architecture that won the LSVRC in 2014,
    and was the first really successful attempt by one of the big IT companies in
    the series, which has been won mostly by corporations with giant budgets since
    2014.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**GoogLeNet** 是在 2014 年赢得 LSVRC 的神经网络架构，是大型 IT 公司系列中第一个真正成功的尝试，自 2014 年以来，这一系列主要被拥有巨额预算的公司赢得。'
- en: 'GoogLenet is basically a deep composition of nine chained Inception modules,
    with little or no modification. Each one of these Inception modules is represented
    in the following figure, and it''s a mix of tiny convolutional blocks, intermixed
    with a 3 x 3 max pooling node:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: GoogLeNet 基本上是九个连锁 Inception 模块的深度组合，几乎没有修改。以下图示中展示了这些 Inception 模块，它们是由微小的卷积块组成，并穿插着一个
    3 x 3 的最大池化节点：
- en: '![](img/5c3db9cd-2c4e-47d0-a033-6fd7357c77be.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![VGG 模型](img/5c3db9cd-2c4e-47d0-a033-6fd7357c77be.png)'
- en: Even with such complexity, GoogLenet managed to reduce the required parameter
    number (11 millon compared to 60 millon), and increased the accuracy (6.7% error
    compared to 16.4%) compared to Alexnet, which was released just two years previously.
    Additionally, the reuse of the Inception module allowed agile experimentation.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 即使具有这样的复杂性，GoogLeNet 仍然设法减少了所需的参数数量（与两年前发布的 AlexNet 相比，从 6000 万减少到 1100 万），并提高了准确性（与
    AlexNet 相比，误差从 16.4% 降至 6.7%）。此外，Inception 模块的重复使用允许敏捷的实验。
- en: But it wasn't the last version of this architecture; soon enough, a second version
    of the Inception module was created, with the following characteristics.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但这并不是这个架构的最后一个版本；不久之后，就创建了一个 Inception 模块的第二个版本，具有以下特点。
- en: Batch-normalized inception V2 and V3
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批归一化的 Inception V2 和 V3
- en: In December 2015, with the paper *Rethinking the Inception Architecture for
    Computer Vision*, Google Research released a new iteration of the Inception architecture.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 2015 年 12 月，随着论文 *Rethinking the Inception Architecture for Computer Vision*
    的发布，谷歌研究发布了 Inception 架构的新版本。
- en: '**The internal covariance shift problem**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**内部协方差偏移问题**'
- en: One of the main problems of the original GoogLenet was training instability.
    As we saw earlier, input normalization consisted basically of centering all the
    input values on zero and dividing its value by the standard deviation in order
    to get a good baseline for the gradients of the backpropagations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 原始 GoogLeNet 的一个主要问题是训练不稳定。正如我们之前看到的，输入归一化基本上是将所有输入值中心化，并将其值除以标准差，以便为反向传播的梯度提供一个良好的基线。
- en: What occurs during the training of really large datasets is that after a number
    of training examples, the different value osculations begin to amplify the mean
    parameter value, like in a resonance phenomenon. This phenomenon is called **covariance
    shift**.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模数据集的训练过程中发生的情况是，在多个训练示例之后，不同的值振荡开始放大均值参数值，就像在共振现象中一样。这种现象被称为**协方差偏移**。
- en: To mitigate this, the solution was to apply normalization not only to the original
    input values, but also to the output values at each layer, avoiding the instabilities
    appearing between layers before they begin to drift from the mean.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这一点，解决方案是对原始输入值以及每一层的输出值进行归一化，避免在它们开始偏离平均值之前出现在层之间的不稳定性。
- en: 'Apart from batch normalization, there were a number of additions proposed incrementally
    to V2:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了批量归一化之外，还有许多增量添加到V2中：
- en: Reduce the number of convolutions to maximum of 3 x 3
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将卷积的数量减少到最大3 x 3
- en: Increase the general depth of the networks
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加网络的总体深度
- en: Use the width increase technique on each layer to improve feature combination
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每一层上使用宽度增加技术来改善特征组合
- en: Factorization of convolutions
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积分解
- en: Inception V3 basically implements all the proposed innovations on the same architecture,
    and adds batch normalization to auxiliary classifiers of the networks.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Inception V3基本上在相同的架构上实现了所有提出的创新，并添加了批量归一化到网络的辅助分类器中。
- en: 'In the following diagram, we represent the new architecture. Note the reduced
    size of the convolutional units:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们表示新的架构。注意卷积单元的减小尺寸：
- en: '![](img/ca0f4608-6d08-40a2-ab45-364692d956a2.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ca0f4608-6d08-40a2-ab45-364692d956a2.png)'
- en: At the end of 2015, the last fundamental improvement in this series of architectures
    came from another company, Microsoft, in the form of **ResNets**.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 到了2015年底，这一系列架构的最后一次基本改进来自另一家公司，微软，以**ResNets**的形式。
- en: Residual Networks (ResNet)
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 残差网络（ResNet）
- en: This new architecture appeared in December 2015 (more or less the same time
    as Inception V3), and it had a simple but novel idea—not only should the output
    of each constitutional layer be used but the architecture should also combine
    the output of the layer with the original input.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这种新的架构出现在2015年12月（或多或少与Inception V3同时），它有一个简单但新颖的想法——不仅应该使用每个构成层的输出，而且架构还应该将层的输出与原始输入相结合。
- en: 'In the following diagram, we observe a simplified view of one of the ResNet
    modules. It clearly shows the sum operation at the end of the convolutional series,
    and a final ReLU operation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，我们观察到ResNet模块的一个简化视图。它清楚地显示了卷积系列结束时的求和操作，以及最后的ReLU操作：
- en: '![](img/b626c89d-0474-4e33-aab3-01fdf6cc9f19.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b626c89d-0474-4e33-aab3-01fdf6cc9f19.png)'
- en: The convolutional part of the module includes a feature reduction from 256 to
    64 values, a 3 x 3 filter layer maintaining the feature numbers, and a feature
    augmenting the 1 x 1 layer from 64 x 256 values. Originally, it spanned more than
    100 layers, but in recent developments, ResNet is also used in a depth of fewer
    than 30 layers, but with a wider distribution.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 模块的卷积部分包括从256个值减少到64个值的特征减少，一个3 x 3的滤波层保持特征数量，以及一个将1 x 1层从64 x 256个值增加特征的层。最初，它跨越了100多个层，但在最近的发展中，ResNet也被用于少于30层的深度，但具有更宽的分布。
- en: Now that we have seen a general overview of the main developments of recent
    years, let's go directly to the main types of application that researchers have
    discovered for CNNs.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经看到了近年来主要发展的概述，让我们直接进入研究人员为CNNs发现的几种主要应用类型。
- en: Types of problem solved by deep layers of CNNs
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CNN深层解决的问题类型
- en: 'CNNs have been employed to solve a wide variety of problems in the past. Here
    is a review of the main problem types, and a short reference to the architecture:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: CNN在过去被用来解决各种各样的问题。以下是主要问题类型的回顾，以及对其架构的简要参考：
- en: Classification
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Detection
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测
- en: Segmentation
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割
- en: Classification
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类
- en: Classification models, as we have seen previously, take an image or other type
    of input as a parameter and return one array with as many elements as the number
    of the possible classes, with a corresponding probability for each one.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所见，分类模型将图像或其他类型的输入作为参数，返回一个数组，其元素数量与可能类别的数量相同，每个都有一个相应的概率。
- en: The normal architecture for this type of solution is a complex combination of
    convolutional and pooling layers with a logistic layer at the end, showing the
    probability any of the pretrained classes.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这种解决方案的正常架构是一个复杂的卷积和池化层的组合，最后是一个逻辑层，显示任何预训练类别的概率。
- en: Detection
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测
- en: Detection adds a level of complexity because it requires guessing the location
    of one or more elements pertaining to the image, and then trying to classify each
    of these elements of information.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 检测增加了一层复杂性，因为它需要猜测图像中一个或多个相关元素的位置，然后尝试对每个这些信息元素进行分类。
- en: For this task, a common strategy for the individual localization problem is
    to combine a classification and regression problem—one (classification) for the
    class of the object, and the remaining one (regression) for determining the coordinates
    of the detected object—and then combining the losses into a common one.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，解决个体本地化问题的常见策略是将分类和回归问题结合起来——一个（分类）用于对象的类别，剩下的一个（回归）用于确定检测到的对象的坐标——然后将损失合并为一个共同的损失。
- en: For multiple elements, the first step is to determine a number of regions of
    interest, searching for places in the image that are statistically showing blobs
    of information belonging to the same object and then only applying the classification
    algorithms to the detected regions, looking for positive cases with high probabilities.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多个元素，第一步是确定感兴趣区域的数量，寻找图像中统计上显示出属于同一对象的信息块的地方，然后只对检测到的区域应用分类算法，寻找概率高的阳性案例。
- en: Segmentation
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割
- en: 'Segmentation adds an additional layer of complexity to the mix because the
    model has to locate the elements in an image and mark the exact shapes of all
    the located objects, as in the following illustration:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 分割增加了额外的复杂性，因为模型必须在图像中定位元素并标记所有定位对象的精确形状，如下面的插图所示：
- en: '![](img/9650e822-2c45-41e6-b22f-cf1082db369a.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9650e822-2c45-41e6-b22f-cf1082db369a.png)'
- en: One of the most common approaches for this task is to implement sequential downsampling
    and upsampling operations, recovering a high-resolution image with only a certain
    number of possible outcomes per pixel that mark the class number for that element.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，最常见的方法是实现顺序下采样和上采样操作，仅使用每个像素的可能结果数量恢复高分辨率图像，这些结果标记了该元素的类别编号。
- en: Deploying a deep neural network with Keras
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Keras部署深度神经网络
- en: 'In this exercise, we will generate an instance of the previously described
    Inception model, provided by the Keras application library. First of all, we will
    import all the required libraries, including the Keras model handling, the image
    preprocessing library, the gradient descent used to optimize the variables, and
    several Inception utilities. Additionally, we will use OpenCV libraries to adjust
    the new input images, and the common NumPy and matplotlib libraries:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将生成Keras应用程序库提供的先前描述的Inception模型的实例。首先，我们将导入所有必需的库，包括Keras模型处理、图像预处理库、用于优化变量的梯度下降以及几个Inception实用工具。此外，我们将使用OpenCV库调整新的输入图像，以及常见的NumPy和matplotlib库：
- en: '[PRE7]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Keras makes it really simple to load a model. You just have to invoke a new
    instance of the `InceptionV3` class, and then we will assign an optimizer based
    on **stochastic gradient descent**, and the categorical cross-entropy for the
    loss, which is very suitable for image classification problems:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Keras使得加载模型变得非常简单。你只需要调用`InceptionV3`类的新实例，然后我们将基于**随机梯度下降**分配一个优化器，以及用于损失的类别交叉熵，这对于图像分类问题非常适合：
- en: '[PRE8]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that the model is loaded into memory, it''s time to load and adjust the
    photo using the `cv` library, and then we call the preprocess input of the Keras
    application, which will normalize the values:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已加载到内存中，是时候使用`cv`库加载和调整照片了，然后我们调用Keras应用程序的预处理输入，这将归一化值：
- en: '[PRE9]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This is what the image looks like after it''s normalized—note how our structural
    understanding of the image has changed, but from the point of view of the model,
    this is the best way of allowing the model to converge:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这是图像归一化后的样子——注意我们的图像结构理解是如何改变的，但从模型的角度来看，这是让模型收敛的最佳方式：
- en: '![](img/4adcfca4-f4b1-4e89-98ba-bee9896d17da.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4adcfca4-f4b1-4e89-98ba-bee9896d17da.png)'
- en: 'Now we will invoke the `predict` method of the model, which will show the results
    of the last layer of the neural network, an array of probabilities for each of
    the categories. The `decode_predictions` method reads a dictionary with all the
    category numbers as indexes, and the category name as the value, and so it provides
    the name of the detected item classification, instead of the number:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将调用模型的`predict`方法，这将显示神经网络最后一层的输出结果，一个包含每个类别概率的数组。`decode_predictions`方法读取一个字典，其中所有类别编号作为索引，类别名称作为值，因此它提供了检测到的项目分类的名称，而不是数字：
- en: '[PRE10]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As you can see, with this simple approach we have received a very approximate
    prediction from a list of similar birds. Additional tuning of the input images
    and the model itself could lead to more precise answers because the blue jay is
    a category included in the 1,000 possible classes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过这种方法，我们从一系列类似鸟类中得到了一个非常近似的预测。对输入图像和模型本身的进一步调整可能导致更精确的答案，因为蓝松鸦是包含在 1,000
    个可能类别中的类别之一。
- en: Exploring a convolutional model with Quiver
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Quiver 探索卷积模型
- en: In this practical example, we will load one of the models we have previously
    studied (in this case, `Vgg19`) with the help of the Keras library and Quiver.
    Then we will observe the different stages of the architecture, and how the different
    layers work, with a certain input.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个实际例子中，我们将借助 Keras 库和 Quiver 加载我们之前研究过的一个模型（在这个例子中，是 `Vgg19`）。然后我们将观察架构的不同阶段，以及不同层如何通过一定的输入工作。
- en: Exploring a convolutional network with Quiver
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Quiver 探索卷积网络
- en: '**Quiver** ([https://github.com/keplr-io/quiver](https://github.com/keplr-io/quiver))
    is a recent and very convenient tool used to explore models with the help of Keras.
    It creates a server that can be accessed by a contemporary web browser and allows
    the visualization of a model''s structure and the evaluation of input images from
    the input layers until the final predictions.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**Quiver** ([https://github.com/keplr-io/quiver](https://github.com/keplr-io/quiver))
    是一个最近且非常方便的工具，用于在 Keras 的帮助下探索模型。它创建了一个可以被现代网络浏览器访问的服务器，并允许可视化模型的结构以及从输入层到最终预测的输入图像的评估。'
- en: 'With the following code snippet, we will create an instance of the `VGG16`
    model and then we will allow Quiver to read all the images sitting on the current
    directory and start a web application that will allow us to interact with our
    model and its parameters:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段将创建一个 `VGG16` 模型实例，然后我们将允许 Quiver 读取当前目录中的所有图像，并启动一个网络应用，允许我们与模型及其参数进行交互：
- en: '[PRE11]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The script will then download the `VGG16` model weights (you need a good connection
    because it weighs in the hundreds of megabytes). Then it loads the model in memory
    and creates a server listening on port 5000.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 然后脚本将下载 `VGG16` 模型权重（你需要一个好的连接，因为它有几百兆字节）。然后它将模型加载到内存中，并创建一个监听 5000 端口的服务器。
- en: The model weights that the Keras library downloads have been previously trained
    thoroughly with Imagenet, so it is ready to get very good accuracy on the 1,000
    categories in our dataset.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 库下载的模型权重已经用 Imagenet 进行了彻底的训练，因此它已经准备好在我们的数据集的 1,000 个类别上获得非常好的准确率。
- en: In the following screenshot, we see the first screen we will see after loading
    the index page of the web application. On the left, an interactive graphical representation
    of the network architecture is shown. On the center right, you can select one
    of the pictures in your current directory, and the application will automatically
    feed it as input, printing the five most likely outcomes of the input.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的截图中，我们看到加载网络应用索引页面后的第一个屏幕。在左侧，显示了网络架构的交互式图形表示。在中心右侧，你可以选择当前目录中的一张图片，应用将自动将其作为输入，并打印出输入的最可能五种结果。
- en: 'The screenshot also shows the first network layer, which basically consists
    of three matrices representing the red, green, and blue components of the original
    image:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 截图还显示了第一个网络层，它基本上由三个矩阵组成，代表原始图像的红、绿和蓝成分：
- en: '![](img/7f08fd1e-24e3-484d-b270-fcf57f39f584.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7f08fd1e-24e3-484d-b270-fcf57f39f584.png)'
- en: 'Then, as we advance into the model layers, we have the first convolutional
    layer. Here we can see that this stage highlights mainly high-level features,
    like the ones we set up with our 3 x 3 filters, such as different types of border,
    brightness, and contrast:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，当我们深入到模型层时，我们遇到了第一个卷积层。在这里，我们可以看到这个阶段主要突出高级特征，就像我们用 3x3 滤波器设置的那样，例如不同的边缘类型、亮度和对比度：
- en: '![](img/d6515d78-8381-4126-884f-38a674246a5d.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d6515d78-8381-4126-884f-38a674246a5d.png)'
- en: 'Let''s advance a bit more. We now can see an intermediate layer that isn''t
    focused on global features. Instead, we see that it has trained for intermediate
    features, such as different sets of textures, angles, or sets of features, such
    as eyes and noses:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再前进一点。现在我们可以看到一个不专注于全局特征的中间层。相反，我们看到它已经训练了中间特征，例如不同的纹理集、角度或特征集，如眼睛和鼻子：
- en: '![](img/c7883909-d0bb-40fa-83f0-55f4d1fb386d.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c7883909-d0bb-40fa-83f0-55f4d1fb386d.png)'
- en: 'When arriving at the last convolutional layers, really abstract concepts are
    appearing. This stage shows how incredibly powerful the models we are now training
    are, because now we are seeing highlighted elements without any useful (for us)
    meaning. These new abstract categories will lead, after some fully connected layers,
    to the final solution, which is a 1,000-element array with a float probability
    value, the probability value for each category in ImageNet:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 当到达最后的卷积层时，真正抽象的概念出现了。这一阶段展示了我们现在训练的模型是多么的强大，因为我们现在看到的是没有任何有用（对我们来说）意义的突出元素。这些新的抽象类别将在一些全连接层之后，导致最终的解决方案，这是一个包含1,000个元素的数组，具有浮点概率值，是ImageNet中每个类别的概率值：
- en: '![](img/c0ecddc0-353d-4f3b-b020-8a36010b61e2.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c0ecddc0-353d-4f3b-b020-8a36010b61e2.png)'
- en: We hope you can explore different examples and the layers' outputs, and try
    to discover how they highlight the different features for different categories
    of images.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望您能够探索不同的示例和层的输出，并尝试发现它们如何突出不同类别图像的不同特征。
- en: Now it's time to work on a new type of machine learning, which consists of applying
    previously trained networks to work on new types of problem. This is called **transfer
    learning**.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候着手研究一种新的机器学习类型了，这种类型包括将先前训练好的网络应用于解决新的问题类型。这被称为**迁移学习**。
- en: Implementing transfer learning
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现迁移学习
- en: 'In this example, we will implement one of the previously seen examples, replacing
    the last stages of a pretrained convolutional neural network and training the
    last stages for a new set of elements, applying it to classification. It has the
    following advantages:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将实现之前看到的一个例子，用预训练卷积神经网络的最后阶段替换，并为一组新元素训练最后阶段，应用于分类。它有以下优点：
- en: It builds upon models with proved efficiency for image classification tasks
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它建立在图像分类任务中已证明有效的模型之上
- en: It reduces the training time because we can reuse coefficients with an accuracy
    that could take weeks of computing power to reach
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它减少了训练时间，因为我们能够以可能需要数周计算能力才能达到的精度重用系数。
- en: The dataset classes will be two different flower types from the flower17 dataset.
    It is a 17-category flower dataset with 80 images for each class. The flowers
    chosen are some common flowers in the UK. The images have large scale, pose, and
    light variations, and there are also classes with large variations of images within
    the class and close similarity to other classes. In this case, we will gather
    the first two classes (daffodil and coltsfoot), and build a classifier on top
    of the pretrained VGG16 network.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集类别将是来自flower17数据集的两种不同的花卉类型。这是一个包含每个类别80个图像的17类别花卉数据集。选择的花是英国的一些常见花卉。图像具有大范围、姿态和光照变化，而且同一类别内也存在图像的大范围变化和与其他类别的相似性。在这种情况下，我们将收集前两个类别（水仙花和紫菀），并在预训练的VGG16网络上构建一个分类器。
- en: 'First, we will do image data augmentation, because the quantity of images may
    not be enough to abstract all the elements of each species. Let''s start by importing
    all the required libraries, including applications, preprocessing, the checkpoint
    model, and the associated object, to allow us to save the intermediate steps,
    and the `cv2` and `NumPy` libraries for image processing and numerical base operations:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将进行图像数据增强，因为图像的数量可能不足以抽象出每个物种的所有元素。让我们首先导入所有必需的库，包括应用、预处理、检查点模型和相关对象，以便我们保存中间步骤，以及`cv2`和`NumPy`库用于图像处理和数值基础操作：
- en: '[PRE12]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In this section, we will define all the variables affecting the input, data
    sources, and training parameters:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将定义所有影响输入、数据源和训练参数的变量：
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now we will invoke the VGG16 pretrained model, not including the top flattening
    layers:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将调用VGG16预训练模型，不包括顶部的平坦层：
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now it''s time to compile the model and create the image data augmentation
    object for the training and testing dataset:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候编译模型并为训练和测试数据集创建图像数据增强对象了：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we will generate the new augmented data:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将生成新的增强数据：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'It''s time to fit the new final layers for the model:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候为模型拟合新的最终层了：
- en: '[PRE17]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now let''s try this with a daffodil image. Let''s test the output of the classifier,
    which should output an array close to `[1.,0.]`, indicating that the probability
    for the first option is very high:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们用一朵水仙花图像来试一试。让我们测试分类器的输出，它应该输出一个接近`[1.,0.]`的数组，表示第一个选项的概率非常高：
- en: '[PRE18]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: So, we have a very definitive answer for this kind of flower. You can play with
    new images and test the model with clipped or distorted images, even with related
    classes, to test the level of accuracy.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于这类花朵，我们有一个非常明确的答案。你可以用新的图像进行实验，并用裁剪或扭曲的图像，甚至是相关类别的图像来测试模型，以检验其准确度水平。
- en: References
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Fukushima, Kunihiko, and Sei Miyake, *Neocognitron: A Self-Organizing Neural
    Network Model for a Mechanism of Visual Pattern Recognition.* Competition and
    cooperation in neural nets. Springer, Berlin, Heidelberg, 1982\. 267-285.'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fukushima, Kunihiko 和 Sei Miyake, *新认知机：一种用于视觉模式识别机制的自我组织神经网络模型.* 神经网络中的竞争与合作。Springer,
    Berlin, Heidelberg, 1982. 267-285.
- en: 'LeCun, Yann, et al. *Gradient-based learning applied to document recognition.* Proceedings
    of the IEEE 86.11 (1998): 2278-2324.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LeCun, Yann 等人. *将基于梯度的学习方法应用于文档识别.* IEEE 86.11 (1998): 2278-2324.'
- en: Krizhevsky, Alex, Ilya Sutskever, and Geoffrey E. Hinton, *ImageNet Classification
    with Deep Convolutional Neural Networks.* Advances in neural information processing
    systems. 2012.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Krizhevsky, Alex，Ilya Sutskever 和 Geoffrey E. Hinton，*使用深度卷积神经网络进行ImageNet分类.*
    神经信息处理系统进展，2012.
- en: Hinton, Geoffrey E., et al, *Improving Neural Networks by Preventing Co-Adaptation
    of Feature Detectors.* arXiv preprint arXiv:1207.0580 (2012).
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hinton, Geoffrey E. 等人, *通过防止特征检测器的共适应来改进神经网络.* arXiv预印本 arXiv:1207.0580 (2012).
- en: Simonyan, Karen, and Andrew Zisserman, *Very Deep Convolutional Networks for
    Large-Scale Image Recognition*. arXiv preprint arXiv:1409.1556 (2014).
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Simonyan, Karen 和 Andrew Zisserman, *用于大规模图像识别的非常深的卷积神经网络.* arXiv预印本 arXiv:1409.1556
    (2014).
- en: 'Srivastava, Nitish, et al. *Dropout: A Simple Way to Prevent Neural Networks
    from Overfitting.* Journal of machine learning research15.1 (2014): 1929-1958.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Srivastava, Nitish 等人. *Dropout：一种防止神经网络过拟合的简单方法.* 机器学习研究杂志15.1 (2014): 1929-1958.'
- en: Szegedy, Christian, et al, *Rethinking the Inception Architecture for Computer
    Vision.* Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition.
    2016.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Szegedy, Christian 等人, *重新思考计算机视觉中的Inception架构.* IEEE计算机视觉和模式识别会议论文集，2016.
- en: He, Kaiming, et al, *Deep Residual Learning for Image Recognition.* Proceedings
    of the IEEE conference on computer vision and pattern recognition. 2016.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: He, Kaiming 等人, *用于图像识别的深度残差学习.* IEEE计算机视觉和模式识别会议论文集，2016.
- en: 'Chollet, François, *Xception: Deep Learning with Depthwise Separable Convolutions.* arXiv
    preprint arXiv:1610.02357 (2016).'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Chollet, François, *Xception: 深度学习中的深度可分离卷积.* arXiv预印本 arXiv:1610.02357 (2016).'
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter provides important insights into one of the technologies responsible
    for the amazing new applications you see in the media every day. Also, with the
    practical example provided, you will even be able to create new customized solutions.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了对媒体每天展示的令人惊叹的新应用中负责的一项技术的深刻见解。此外，通过提供的实际示例，你甚至能够创建新的定制解决方案。
- en: As our models won't be enough to solve very complex problems, in the following
    chapter, our scope will expand even more, adding the important dimension of time
    to the set of elements included in our generalization.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的模型不足以解决非常复杂的问题，在下一章中，我们的范围将进一步扩大，将时间维度添加到我们泛化所包含的元素集中。
