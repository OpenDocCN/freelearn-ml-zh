- en: '*Chapter 7*: Hosting ML Models in the Cloud: Best Practices'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第七章*：云中托管机器学习模型：最佳实践'
- en: After you've successfully trained a model, you want to make the model available
    for inference, don't you? ML models are often the product of a business that is
    ML-driven. Your customers consume the ML prediction from your model, not your
    training jobs or processed data. How do you provide a satisfying customer experience,
    starting with a good experience with your ML models?
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在您成功训练了一个模型之后，您希望使模型可用于推理，不是吗？机器学习模型通常是ML驱动型企业的产品。您的客户消费的是您模型中的ML预测，而不是您的训练作业或处理后的数据。您如何提供令人满意的客户体验，从您ML模型的良好体验开始？
- en: SageMaker has several options for ML hosting and inferencing, depending on your
    use case. Options are welcomed in many aspects of life, but it can be difficult
    to find the best option. This chapter will help you understand how to host models
    for batch inference and for online real-time inference, how to use multi-model
    endpoints to save costs, and how to conduct resource optimization for your inference
    needs.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的用例，SageMaker提供了多种机器学习托管和推理选项。在生活的许多方面，选项都是受欢迎的，但找到最佳选项可能很困难。本章将帮助您了解如何托管用于批量推理和在线实时推理的模型，如何使用多模型端点来节省成本，以及如何针对您的推理需求进行资源优化。
- en: 'In this chapter, we will be covering the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Deploying models in the cloud after training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练后云中部署模型
- en: Inferencing in batches with batch transform
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量转换中的推理
- en: Hosting real-time endpoints
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 托管实时端点
- en: Optimizing your model deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化您的模型部署
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: For this chapter, you need to access the code at [https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07).
    If you did not run the notebooks in the previous chapter, please run the [chapter05/02-tensorflow_sentiment_analysis.ipynb](http://chapter05/02-tensorflow_sentiment_analysis.ipynb)
    file from the repository before proceeding.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，您需要访问[https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07](https://github.com/PacktPublishing/Getting-Started-with-Amazon-SageMaker-Studio/tree/main/chapter07)中的代码。如果您在上一章中没有运行笔记本，请在继续之前，从存储库中运行[chapter05/02-tensorflow_sentiment_analysis.ipynb](http://chapter05/02-tensorflow_sentiment_analysis.ipynb)文件。
- en: Deploying models in the cloud after training
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练后云中部署模型
- en: ML models can primarily be consumed in the cloud in two ways, **batch inference**
    and **live inference**. Batch inference refers to model inference performed on
    data that is in batches, often large batches, and asynchronous in nature. It fits
    use cases that collect data infrequently, that focus on group statistics rather
    than individual inference, and that do not need to have inference results right
    away for downstream processes. Projects that are research oriented, for example,
    do not require model inference to be returned for a data point right away. Researchers
    often collect a chunk of data for testing and evaluation purposes and care about
    overall statistics and performance rather than individual predictions. They can
    conduct the inference in batches and wait for the prediction for the whole batch
    to complete before they move on.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型主要可以通过两种方式在云中消费，**批量推理**和**实时推理**。批量推理指的是对批量数据进行模型推理，通常是大型批量，并且具有异步性质。它适合那些收集数据不频繁、关注群体统计而不是个体推理、并且不需要立即获得推理结果用于下游流程的场景。例如，研究导向的项目不需要立即返回数据点的模型推理。研究人员通常为了测试和评估目的收集数据块，他们更关注整体统计和性能，而不是个体预测。他们可以在批量中进行推理，并在整个批次的预测完成之前继续进行。
- en: Live inference, on the other hand, refers to model inference performed in real
    time. It is expected that the inference result for an incoming data point is returned
    immediately so that it can be used for subsequent decision-making processes. For
    example, an interactive chatbot would require a live inference capability to support
    such a service. No one would want to wait until the end of the conversation to
    get responses from the chatbot model, nor would people want to wait for more than
    even a couple of seconds. Companies looking to provide the best customer experience
    would want an inference to be made and returned to the customer instantly.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，实时推理指的是在实时进行的模型推理。预期对于传入的数据点的推理结果将立即返回，以便用于后续的决策过程。例如，一个交互式聊天机器人需要实时推理能力来支持此类服务。没有人愿意等到对话结束时才从聊天机器人模型那里得到回应，人们也不愿意等待超过几秒钟。希望提供最佳客户体验的公司希望推理和结果能够立即返回给客户。
- en: Given the different requirements, the architecture and deployment choices also
    differ between batch inference and live inference. Amazon SageMaker has it covered
    as it provides various fully managed options for your inference use cases. **SageMaker
    batch transform** is designed to perform batch inference at scale and is cost-effective
    as the compute infrastructure is fully managed and is de-provisioned when your
    inference job is complete. **SageMaker real-time endpoints** aim to provide a
    robust live hosting option for your ML use cases. Both the SageMaker hosting options
    are fully managed, meaning you do not have to worry much about the cloud infrastructure.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 由于有不同的要求，批量推理和实时推理之间的架构和部署选择也有所不同。Amazon SageMaker 提供了各种完全管理的选项来满足您的推理用例。**SageMaker
    批量转换**旨在进行大规模的批量推理，并且由于计算基础设施是完全管理的，因此在推理作业完成后会自动取消分配，因此具有成本效益。**SageMaker 实时端点**旨在为您的机器学习用例提供强大的实时托管选项。SageMaker
    的这两种托管选项都是完全管理的，这意味着您不必过多担心云基础设施。
- en: Let's first take a look at SageMaker batch transform, how it works, and when
    to use it.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看 SageMaker 批量转换，它是如何工作的，以及在什么情况下使用它。
- en: Inferencing in batches with batch transform
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量转换的推理
- en: SageMaker batch transform is designed to provide offline inference for large
    datasets. Depending on how you organize the data, SageMaker batch transform can
    split a single large text file in S3 by lines into a small and manageable size
    (mini-batch) that would fit into the memory before making inference against the
    model; it can also distribute the files by S3 key into compute instances for efficient
    computation. For example, it could send `test1.csv` to instance 1 and `test2.csv`
    to instance 2.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 批量转换旨在为大型数据集提供离线推理。根据您如何组织数据，SageMaker 批量转换可以将单个大文本文件在 S3 中按行分割成小而可管理的尺寸（迷你批次），以便在针对模型进行推理之前放入内存中；它还可以通过
    S3 键将文件分配到计算实例中进行高效计算。例如，它可以将 `test1.csv` 发送到实例 1，将 `test2.csv` 发送到实例 2。
- en: 'To demonstrate SageMaker batch transform, we can pick up from our training
    example in the previous chapter. In [*Chapter 6*](B17447_06_ePub_RK.xhtml#_idTextAnchor090),
    *Detecting ML Bias and Explaining Models with SageMaker Clarify*, we showed you
    how to train a TensorFlow model using SageMaker managed training for a movie review
    sentiment prediction use case in `Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb.`
    We can deploy the trained model to make a batch inference using SageMaker batch
    transform in the following steps:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示 SageMaker 批量转换，我们可以从上一章的训练示例开始。在 [*第 6 章*](B17447_06_ePub_RK.xhtml#_idTextAnchor090)，“使用
    SageMaker Clarify 检测机器学习偏差和解释模型”，我们向您展示了如何使用 SageMaker 管理训练在 `Getting-Started-with-Amazon-SageMaker-Studio/chapter05/02-tensorflow_sentiment_analysis.ipynb.`
    中对电影评论情感预测用例进行 TensorFlow 模型的训练。我们可以按照以下步骤将训练好的模型部署到 SageMaker 批量转换中进行批量推理：
- en: Please open the `Getting-Started-with-Amazon-SageMaker-Studio/chapter07/01-tensorflow_sentiment_analysis_batch_transform.ipynb`
    notebook and use the **Python 3** (**TensorFlow 2.3 Python 3.7 CPU Optimized**)
    kernel.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请打开 `Getting-Started-with-Amazon-SageMaker-Studio/chapter07/01-tensorflow_sentiment_analysis_batch_transform.ipynb`
    笔记本，并使用 **Python 3** （**TensorFlow 2.3 Python 3.7 CPU 优化**） 内核。
- en: Run the first three cells to set up the SageMaker SDK, import the libraries,
    and prepare the test dataset. There are 25,000 documents in the test dataset.
    We save the test data as a CSV file and upload the CSV file to our S3 bucket.
    The file is 27 MB.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行前三个单元格以设置 SageMaker SDK，导入库，并准备测试数据集。测试数据集中有 25,000 个文档。我们将测试数据保存为 CSV 文件，并将
    CSV 文件上传到我们的 S3 桶中。文件大小为 27 MB。
- en: Note
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: SageMaker batch transform expects the input CSV files to *not* contain headers.
    That is, the first row of the CSV should be the first data point.
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: SageMaker 批量转换期望输入 CSV 文件不包含标题。也就是说，CSV 的第一行应该是第一个数据点。
- en: We retrieve the training TensorFlow estimator from a training job we did in
    [*Chapter 6*](B17447_06_ePub_RK.xhtml#_idTextAnchor090), *Detecting ML Bias and
    Explaining Models with SageMaker Clarify*. We need to grab the training job name
    for the `TensorFlow.attach()` method. You can find it in **Experiments and trials**
    in the left sidebar, as shown in *Figure 7.1*, thanks to the experiments we used
    when training. In **Experiments and trials**, left-click on **imdb-sentiment-analysis**
    and you should see your training job as a trial in the list.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从[*第6章*](B17447_06_ePub_RK.xhtml#_idTextAnchor090)中提到的训练作业中检索了训练 TensorFlow
    估计器，该作业是关于*检测机器学习偏差和用 SageMaker Clarify 解释模型*。我们需要获取用于 `TensorFlow.attach()` 方法的训练作业名称。您可以在左侧侧边栏的**实验和试验**中找到它，如图*图7.1*所示，这是由于我们在训练时使用的实验。在**实验和试验**中，左键单击**imdb-sentiment-analysis**，您应该会在列表中看到您的训练作业作为一个试验。
- en: '![Figure 7.1 – Obtaining training job name in Experiments and trials'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.1 – 在实验和试验中获取训练作业名称'
- en: '](img/B17447_07_01.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_07_01.jpg]'
- en: Figure 7.1 – Obtaining training job name in Experiments and trials
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 7.1 – 在实验和试验中获取训练作业名称
- en: 'You should replace `training_job_name` in the following code with your own:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在以下代码中将 `training_job_name` 替换为您自己的：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Once you have replaced `training_job_name` and attached it to reload `estimator`,
    you should see the history of the job printed in the output.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您替换了 `training_job_name` 并将其附加到重新加载 `estimator`，您应该会在输出中看到作业的历史记录。
- en: 'To run SageMaker batch transform, you only need two lines of SageMaker API
    code:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要运行 SageMaker 批量转换，您只需要两行 SageMaker API 代码：
- en: '[PRE1]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `estimator.transformer()` method creates a `Transformer` object with the
    compute resource desired for the inference. Here we request one `ml.c5.xlarge`
    instance for predicting 25,000 movie reviews. The `max_payload` argument allows
    us to control the size of each mini-batch that SageMaker Batch Transform is splitting.
    The `accept` argument determines the output type. SageMaker managed Tensorflow
    serving container supports '`application/json`', and 'a`pplication/jsonlines`'.
    `assemble_with` controls how you assemble the inference results that are in mini-batches.
    Then we provide the S3 location of the test data (`test_data_s3`) in the `transformer.transform()`,
    and indicate that the input content type to be of '`text/csv`' as the file is
    of CSV format. `split_type` determines how the input files will be split by SageMaker
    Batch Transform into mini-batch. We put in a unique job name and SageMaker Experiments
    configuration so that we can track the inference to the associated training job
    in the same trial. The Batch Transform job would take around 5 minutes to complete.
    Like a training job, SageMaker manages the provisioning, computation, and de-provisioning
    of the instances once the job finishes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`estimator.transformer()` 方法创建一个具有所需推理计算资源的 `Transformer` 对象。在这里，我们请求一个 `ml.c5.xlarge`
    实例来预测 25,000 部电影评论。`max_payload` 参数允许我们控制 SageMaker 批量转换分割的每个 mini-batch 的大小。`accept`
    参数确定输出类型。SageMaker 管理的 Tensorflow serving 容器支持 ''`application/json`'' 和 ''a`pplication/jsonlines`''。`assemble_with`
    控制如何组装 mini-batch 中的推理结果。然后我们在 `transformer.transform()` 中提供测试数据的 S3 位置（`test_data_s3`），并指示输入内容类型为
    ''`text/csv`''，因为文件是 CSV 格式。`split_type` 确定SageMaker 批量转换如何将输入文件分割成 mini-batch。我们输入一个唯一的作业名称和
    SageMaker Experiments 配置，以便我们可以跟踪推理到同一试验中相关的训练作业。批量转换作业大约需要 5 分钟才能完成。像训练作业一样，一旦作业完成，SageMaker
    会管理实例的配置、计算和取消配置。'
- en: 'After the job completes, we should take a look at the result. SageMaker batch
    transform saves the results after assembly to the specified S3 location with `.out`
    appended to the input filename. You can access the full S3 path in `transformer.output_path`
    attribute. SageMaker uses TensorFlow Serving, a model serving framework developed
    by TensorFlow, for model serving, the model output is written in JSON format.
    The output has the sentiment probabilities in an array with predictions as the
    JSON key. We can inspect the batch transform results with the following code:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作业完成后，我们应该查看结果。SageMaker 批量转换在组装后将结果保存到指定的 S3 位置，并在输入文件名后附加 `.out`。您可以在 `transformer.output_path`
    属性中访问完整的 S3 路径。SageMaker 使用 TensorFlow Serving，这是一个由 TensorFlow 开发的模型服务框架，用于模型服务，模型输出以
    JSON 格式编写。输出包含一个数组，其中包含预测作为 JSON 键的情感概率。我们可以使用以下代码检查批量转换结果：
- en: '[PRE2]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then collect all 25,000 predictions into a `results` variable:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将所有25,000个预测收集到一个`results`变量中：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The rest of the notebook displays one original movie review, the predicted sentiment,
    and the corresponding ground truth sentiment. The model returns the probabilities
    of the reviews being positive or negative. We take a `0.5` threshold and mark
    probabilities over the threshold to be positive and below `0.5` to be negative.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 笔记本本的其余部分显示了一篇原始电影评论、预测的情感和相应的真实情感。模型返回评论为正面或负面的概率。我们采用`0.5`阈值，将超过阈值的概率标记为正面，低于`0.5`的标记为负面。
- en: As we logged the batch transform job in the same trial as the training job,
    we can find it easily in **Experiments and trials** in the left sidebar, as shown
    in *Figure 7.2*. You can see more information about this batch transform job in
    this entry.![Figure 7.2 – The batch transform job is logged as a trial component
    alongside the training component
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们在与训练作业相同的试验中记录了批转换作业，我们可以在左侧侧边栏的**实验和试验**中轻松找到它，如图7.2所示。您可以在本条目中查看有关此批转换作业的更多信息。![图7.2
    – 批转换作业作为试验组件与训练组件一起记录
- en: '](img/B17447_07_02.jpg)'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17447_07_02.jpg)'
- en: Figure 7.2 – The batch transform job is logged as a trial component alongside
    the training component
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 批转换作业作为试验组件与训练组件一起记录
- en: 'That''s how easy it is to make use of SageMaker batch transform to generate
    inferences on a large dataset. You may wonder, why can''t I just use the notebook
    to make inferences? What''s the benefit of using SageMaker batch transform? Yes,
    you can use the notebook for quick analysis. The advantages of SageMaker batch
    transform are as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用SageMaker批转换在大型数据集上生成推理是多么容易。您可能会想，为什么我不能直接使用笔记本进行推理？使用SageMaker批转换的好处是什么？是的，您可以使用笔记本进行快速分析。SageMaker批转换的优势如下：
- en: Fully managed mini-batching helps make inferences on a large dataset efficiently.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 完全管理的迷你批处理有助于高效地对大数据集进行推理。
- en: You can use a separate SageMaker-managed compute infrastructure that is different
    from your notebook instance. You can easily run prediction with a cluster of instances
    for faster prediction.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用一个独立的SageMaker管理的计算基础设施，这与您的笔记本实例不同。您可以使用实例集群轻松运行预测，以实现更快的预测。
- en: You only pay for the runtime of a batch transform job, even with a much larger
    compute cluster.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使使用更大的计算集群，您也只需为批转换作业的运行时间付费。
- en: You can schedule and kick off a model prediction independently in the cloud
    with SageMaker batch transform. It is not necessary to use a Python notebook in
    SageMaker Studio to start a prediction job.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用SageMaker批转换在云中独立安排和启动模型预测。无需在SageMaker Studio中使用Python笔记本启动预测作业。
- en: Next, let's see how we can host ML models in the cloud for real-time use cases.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看我们如何在云中托管ML模型以用于实时用例。
- en: Hosting real-time endpoints
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 托管实时端点
- en: 'SageMaker real-time inference is a fully managed feature for hosting your model(s)
    on compute instance(s) for real-time low-latency inference. The deployment process
    consists of the following steps:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker实时推理是一个完全管理的功能，用于在计算实例上托管您的模型（或多个模型）以实现实时低延迟推理。部署过程包括以下步骤：
- en: Create a model, container, and associated inference code in SageMaker. The model
    refers to the training artifact, `model.tar.gz`. The container is the runtime
    environment for the code and the model.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在SageMaker中创建一个模型、容器和相关的推理代码。模型指的是训练工件，`model.tar.gz`。容器是代码和模型的运行时环境。
- en: Create an HTTPS endpoint configuration. This configuration carries information
    about compute instance type and quantity, models, and traffic patterns to model
    variants.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个HTTPS端点配置。此配置包含有关计算实例类型和数量、模型和流量模式到模型变体的信息。
- en: Create ML instances and an HTTPS endpoint. SageMaker creates a fleet of ML instances
    and an HTTPS endpoint that handles the traffic and authentication. The final step
    is to put everything together for a working HTTPS endpoint that can interact with
    client-side requests.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建ML实例和HTTPS端点。SageMaker创建了一支ML实例和HTTPS端点，用于处理流量和身份验证。最后一步是将所有内容组合起来，以创建一个可以与客户端请求交互的工作HTTPS端点。
- en: 'Hosting a real-time endpoint faces one particular challenge that is common
    when hosting a website or a web application: it can be difficult to scale your
    compute instances when you have a spike in traffic to your endpoint. You may have
    1,000 customers visiting your website per minute in a particular hour and then
    have 100,000 customers in the next hour. If you only deploy one instance behind
    your endpoint that is capable of handling 5,000 requests per minute, it would
    work well in the first hour and would struggle in the next. Autoscaling is a technique
    in the cloud to help you scale out instances automatically when certain criteria
    are met so that your application can handle the load at any time.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 运行实时端点面临的一个特定挑战是，当你的端点流量激增时，这通常在托管网站或Web应用程序时很常见：你可能在一小时内每分钟有1,000名客户访问你的网站，而在下一小时有10万名客户。如果你只在端点后面部署一个能够每分钟处理5,000个请求的实例，那么在第一个小时内它会运行良好，但在下一个小时内会陷入困境。自动扩展是云中的一种技术，可以帮助你在满足某些条件时自动扩展实例，以便你的应用程序在任何时候都能处理负载。
- en: 'Let''s walk through a SageMaker real-time endpoint example. Like the batch
    transform example, we continue the ML use case in [*Chapter 5*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)*,
    Building and Training ML Models with SageMaker Studio IDE* and 05/02-tensorflow_sentiment_analysis.
    ipynb. Please open the notebook in `Getting-Started-with-Amazon-SageMaker-Studio/chapter07/02-tensorflow_sentiment_analysis_inference.ipynb`
    and use the **Python 3** (**TensorFlow 2.3 Python 3.7 CPU Optimized**) kernel.
    We will deploy a trained model to SageMaker as a real-time endpoint, make some
    predictions as an example, and finally apply an autoscaling policy to help scale
    the compute instances behind the endpoint. Please follow these steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个SageMaker实时端点示例来了解一下。就像批量转换示例一样，我们继续在[*第五章*](B17447_05_ePub_RK.xhtml#_idTextAnchor077)*，使用SageMaker
    Studio IDE构建和训练ML模型*和05/02-tensorflow_sentiment_analysis.ipynb中继续ML用例。请打开`Getting-Started-with-Amazon-SageMaker-Studio/chapter07/02-tensorflow_sentiment_analysis_inference.ipynb`中的笔记本，并使用**Python
    3**（**TensorFlow 2.3 Python 3.7 CPU Optimized**）内核。我们将部署一个训练好的模型到SageMaker作为一个实时端点，进行一些预测作为示例，并最终应用自动扩展策略来帮助扩展端点后面的计算实例。请按照以下步骤操作：
- en: In the first four cells, we set up the SageMaker session, load the Python libraries,
    load the test data that we created in `01-tensorflow_sentiment_analysis_batch_transform.ipynb`,
    and retrieve the training job that we trained previously using its name.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在前四个单元格中，我们设置了SageMaker会话，加载Python库，加载我们在`01-tensorflow_sentiment_analysis_batch_transform.ipynb`中创建的测试数据，并使用其名称检索我们之前训练的训练作业。
- en: 'We then deploy the model to an endpoint:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将模型部署到端点：
- en: '[PRE4]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, we choose `ml.c5.xlarge` for `instance_type` argument. `initial_instance_
    count` argument refers to the number of ML instances behind the endpoint when
    we make this call. Later, we will show you how to use the autoscaling feature,
    which is designed to help us scale out the instance fleet when the initial settings
    become insufficient. The deployment process takes about 5 minutes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们选择`ml.c5.xlarge`作为`instance_type`参数。`initial_instance_count`参数指的是我们调用此参数时端点后面的ML实例数量。稍后，我们将向您展示如何使用自动扩展功能，该功能旨在帮助我们扩展实例群，当初始设置不足时。部署过程大约需要5分钟。
- en: 'We can test the endpoint with some sample data. The TensorFlow Serving framework
    in the container handles the data interface and takes the NumPy array as input
    so we can pass an entry into the model directly. We can get a response from the
    endpoint in JSON format, which gets converted to a dictionary in Python in the
    `prediction` variable:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以用一些样本数据测试端点。容器中的TensorFlow Serving框架处理数据接口，并接受NumPy数组作为输入，因此我们可以直接将条目传递给模型。我们可以从端点获得JSON格式的响应，该响应在Python中的`prediction`变量中被转换为字典：
- en: '[PRE5]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The next two cells retrieve the review in text and print out the ground truth
    sentiment and the predicted sentiment with a threshold of 0.5, just like in the
    batch transform example.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个单元格检索文本评论，并打印出带有0.5阈值的真实情感和预测情感，就像在批量转换示例中一样。
- en: '(Optional) You may be wondering: Can I ask the endpoint to predict the entire
    `x_test` of 25,000 data points? To find out, feel free to try out the following
    line:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: (可选) 你可能想知道：我能否让端点预测25,000个数据点的整个`x_test`？为了找到答案，请随意尝试以下行：
- en: '[PRE6]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This line will run for a couple of seconds and eventually fail. This is because
    a SageMaker endpoint is designed to take on requests that are 6 MB in size one
    at a time. You can request inferences for multiple data points, for example, `x_test[:100]`,
    but not 25,000 all in one call. In contrast, batch transform does the data splitting
    (mini-batching) automatically and is better suited to handle large datasets.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码将运行几秒钟，最终失败。这是因为 SageMaker 端点设计为一次处理 6 MB 大小的请求。例如，您可以请求多个数据点的推理，例如 `x_test[:100]`，但不能在一次调用中请求
    25,000 个。相比之下，批量转换会自动进行数据拆分（迷你批处理），更适合处理大型数据集。
- en: 'Next, we can apply SageMaker''s autoscaling feature to this endpoint using
    the `application-autoscaling` client from the `boto3` SDK:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用 `boto3` SDK 中的 `application-autoscaling` 客户端将 SageMaker 的自动缩放功能应用于此端点：
- en: '[PRE7]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'It is a two-step process to configure autoscaling for computing instances in
    AWS. First, we run `autoscaling_client.register_scalable_target()` to register
    the target with the desired minimum/maximum capacity for our SageMaker endpoint:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 AWS 中为计算实例配置自动缩放是一个两步过程。首先，我们运行 `autoscaling_client.register_scalable_target()`
    以将目标与我们的 SageMaker 端点所需的期望最小/最大容量注册：
- en: '[PRE8]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our target, the SageMaker real-time endpoint, is denoted with `resource_id`.
    We set the minimum capacity to `1` and the maximum to `4`, meaning that when the
    load is at the lowest, there will be at least one instance running behind the
    endpoint. Our endpoint is capable of scaling out to four instances at the most.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标，SageMaker 实时端点，用 `resource_id` 表示。我们将最小容量设置为 `1`，最大容量设置为 `4`，这意味着当负载最低时，至少有一个实例在端点后面运行。我们的端点最多可以扩展到四个实例。
- en: 'Then we run `autoscaling_client.put_scaling_policy()` to instruct *how* we
    want to autoscale:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们运行 `autoscaling_client.put_scaling_policy()` 来指示我们想要如何自动缩放：
- en: '[PRE9]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this example, we employ a scaling strategy called `SageMakerVariantInvocationsPerInstance`)
    in this configuration to make sure each instance can share 4,000 requests per
    minute before scaling out another instance. `ScaleInCooldown` and `ScaleOutCooldown`
    refer to the period of time in seconds after the last scaling activity before
    autoscaling can scale in and out again. With our configuration, SageMaker will
    not scale in (remove an instance) within 600 seconds of the last scale-in activity,
    and will not scale out (add an instance) within 300 seconds of the last scale-out
    activity.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们采用了一种名为 `SageMakerVariantInvocationsPerInstance` 的缩放策略，在此配置中确保每个实例在扩展另一个实例之前可以每分钟共享
    4,000 个请求。`ScaleInCooldown` 和 `ScaleOutCooldown` 指的是在自动缩放可以再次进行缩放之前，最后一次缩放活动后的秒数。根据我们的配置，SageMaker
    不会在最后一次缩放活动后的 600 秒内进行缩放（移除实例），也不会在最后一次缩放活动后的 300 秒内进行扩展（添加实例）。
- en: Note
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'There are two commonly used advanced scaling strategies for `PolicyType`: **step
    scaling** and **scheduled scaling**. In step scaling, you can define the number
    of instances to scale in/out based on the size of the alarm breaches of a certain
    metric. Read more about step scaling at [https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html).
    In scheduled scaling, you can set up the scaling based on the schedule. This is
    particularly useful if the traffic is predictable or has some seasonality. Read
    more about scheduled scaling at [https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 `PolicyType`，有两种常用的高级缩放策略：**步进缩放**和**计划缩放**。在步进缩放中，您可以根据某个指标的警报越界大小定义要缩放/缩出的实例数量。有关步进缩放的更多信息，请参阅
    [https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/as-scaling-simple-step.html)。在计划缩放中，您可以根据计划设置缩放。如果流量可预测或具有某种季节性，这特别有用。有关计划缩放的更多信息，请参阅
    [https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html](https://docs.aws.amazon.com/autoscaling/ec2/userguide/schedule_time.html)。
- en: 'We can verify the configuration of the autoscaling policy with the following
    code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码验证自动缩放策略的配置：
- en: '[PRE10]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In **Amazon SageMaker Studio**, you can easily find the details of an endpoint
    in the **Endpoints** registry in the left sidebar, as shown in *Figure 7.3*. If
    you double-click on an endpoint, you can see more information in the main working
    area:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 **Amazon SageMaker Studio** 中，您可以在左侧侧边栏的 **Endpoints** 注册表中轻松找到端点的详细信息，如图
    *图 7.3* 所示。如果您双击一个端点，您可以在主工作区域看到更多信息：
- en: '![Figure 7.3 – Discovering endpoints in SageMaker Studio'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – 在 SageMaker Studio 中发现端点'
- en: '](img/B17447_07_03.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_07_03.jpg)'
- en: Figure 7.3 – Discovering endpoints in SageMaker Studio
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 在SageMaker Studio中查找端点
- en: The purpose of hosting an endpoint is to serve the ML models in the cloud so
    that you can integrate ML as a microservice into your applications or websites.
    Your model has to be available at all times as long as your main product or service
    is available. You can imagine that there is a great opportunity and incentive
    for you to optimize the deployment to minimize the cost while maintaining performance.
    We just learned how to deploy an ML model in the cloud; we should also learn how
    to optimize the deployment.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 运行端点的目的是在云中提供机器学习模型，以便您可以将机器学习作为微服务集成到您的应用程序或网站上。只要您的核心产品或服务可用，您的模型就必须始终可用。您可以想象，优化部署以最小化成本同时保持性能，这对您来说是一个巨大的机会和激励。我们刚刚学习了如何在云中部署机器学习模型；我们也应该学习如何优化部署。
- en: Optimizing your model deployment
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化您的模型部署
- en: Optimizing model deployment is a critical topic for businesses. No one wants
    to be spending a dime more than they need to. Because deployed endpoints are being
    used continuously, and incurring charges continuously, making sure that the deployment
    is optimized in terms of cost and runtime performance can save you a lot of money.
    SageMaker has several options to help you reduce costs while optimizing the runtime
    performance. In this section, we will be discussing multi-model endpoint deployment
    and how to choose the instance type and autoscaling policy for your use case.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 优化模型部署是企业的一个关键话题。没有人愿意多花一分钱。因为部署的端点正在持续使用，并且持续产生费用，确保部署在成本和运行时性能方面得到优化可以为您节省大量资金。SageMaker有几个选项可以帮助您在优化运行时性能的同时降低成本。在本节中，我们将讨论多模型端点部署以及如何为您的用例选择实例类型和自动扩展策略。
- en: Hosting multi-model endpoints to save costs
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 运行多模型端点以节省成本
- en: A multi-model endpoint is a type of real-time endpoint in SageMaker that allows
    multiple models to be deployed behind the same endpoint. There are many use cases
    in which you would build models for each customer or for each geographic area,
    and depending on the characteristics of the incoming data point, you would apply
    the corresponding ML model. Take the telecommunications churn prediction use case
    that we tackled in [*Chapter 3*](B17447_03_ePub_RK.xhtml#_idTextAnchor043), *Data
    Preparation with SageMaker Data Wrangler,* as an example. We may get more accurate
    ML models if we train them by state because there may be regional differences
    in terms of competition among local telecommunication providers. And if we do
    train ML models for each US state, you can also easily imagine that the utilization
    of each model might not be completely equal. Actually, quite the contrary.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 多模型端点是SageMaker中的一种实时端点，允许在同一个端点后面部署多个模型。有许多用例，您会为每个客户或每个地理区域构建模型，并且根据传入数据点的特征，您将应用相应的机器学习模型。以我们在[*第3章*](B17447_03_ePub_RK.xhtml#_idTextAnchor043)中解决的电信流失预测用例为例，*使用SageMaker
    Data Wrangler进行数据准备*。如果我们按州训练它们，可能会得到更准确的机器学习模型，因为当地电信提供商之间的竞争可能存在地区差异。如果我们为每个美国州训练机器学习模型，您也可以很容易地想象到每个模型的利用率可能并不完全相等。实际上，恰恰相反。
- en: 'Model utilization is inevitably proportional to the population of each state.
    Your New York model is going to be used more frequently than your Alaska model.
    In this scenario, if you host an endpoint for each state, you will have to pay
    for instances, even for the least utilized endpoint. With multi-model endpoints,
    SageMaker helps you reduce costs by reducing the number of endpoints needed for
    your use case. Let''s take a look at how it works with the telecommunications
    churn prediction use case. Please open the `Getting-Started-with-Amazon-SageMaker-Studio/chapter07/03-multimodel-endpoint.ipynb`
    notebook with the Python 3 (Data Science) kernel and follow the next steps:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 模型利用率不可避免地与每个州的居民人数成比例。您的纽约模型将比您的阿拉斯加模型使用得更频繁。在这种情况下，如果您为每个州运行一个端点，您将不得不为最少使用的端点付费。使用多模型端点，SageMaker可以帮助您通过减少您用例所需的端点数量来降低成本。让我们看看它是如何与电信流失预测用例一起工作的。请使用Python
    3（数据科学）内核打开`Getting-Started-with-Amazon-SageMaker-Studio/chapter07/03-multimodel-endpoint.ipynb`笔记本，并按照以下步骤操作：
- en: We define the SageMaker session, load up the Python libraries, and load the
    churn dataset in the first three cells.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义了SageMaker会话，在前三个单元中加载Python库，并加载流失数据集。
- en: 'We do minimal preprocessing to convert the binary columns from strings to `0`
    and `1`:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们进行最小化预处理，将二进制列从字符串转换为`0`和`1`：
- en: '[PRE11]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We leave out 10% of the data for ML inference later on:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为后续的 ML 推理留出了 10% 的数据：
- en: '[PRE12]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After the data is prepared, we set up our state-wise model training process
    in the function `launch_training_job()` with SageMaker Experiments integrated.
    The training algorithm we use is SageMaker''s built-in XGBoost algorithm, which
    is fast and accurate for structural data like this. For binary classification,
    we use a `binary:logtistic` objective with `num_round` set to `20`:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据准备完成后，我们使用 SageMaker Experiments 集成在函数 `launch_training_job()` 中设置我们的状态模型训练过程。我们使用的训练算法是
    SageMaker 内置的 XGBoost 算法，对于这种结构化数据来说，它既快又准确。对于二元分类，我们使用 `binary:logtistic` 目标，并将
    `num_round` 设置为 `20`：
- en: '[PRE13]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'With `launch_training_job()`, we could easily create multiple training jobs
    in a `for` loop for states. For demonstration purposes, we only train five states
    in this example:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `launch_training_job()`，我们可以通过 `for` 循环轻松地为状态创建多个训练作业。为了演示目的，我们在这个例子中只训练了五个状态：
- en: '[PRE14]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Each training job should take no more than 5 minutes. We will wait for all of
    them to complete before proceeding to use the `wait_for_training_job_to_complete()`
    function.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 每个训练作业不应超过 5 分钟。我们将在使用 `wait_for_training_job_to_complete()` 函数之前等待所有作业完成。
- en: 'After the training is done, we finally deploy our multi-model endpoint. It''s
    a bit different to deploying a single model to an endpoint from a trained estimator
    object. We use the `sagemaker.multidatamodel.MultiDataModel` class for deployment:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，我们最终部署我们的多模型端点。与从训练估计器对象部署单个模型到端点相比，这有点不同。我们使用 `sagemaker.multidatamodel.MultiDataModel`
    类进行部署：
- en: '[PRE15]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '`MultiDataModel` initialization needs to understand the common model configuration,
    such as the container image and the network configurations, to configure the endpoint
    configuration. We pass in the model for `PA`. Afterward, we deploy the model to
    one `ml.c5.xlarge` instance and configure the `serializer` and `deserializer`
    to take CSV as input and produce JSON as output, respectively:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiDataModel` 初始化需要了解常见的模型配置，例如容器镜像和网络配置，以配置端点配置。我们传递模型给 `PA`。之后，我们将模型部署到一个
    `ml.c5.xlarge` 实例上，并配置 `serializer` 和 `deserializer` 分别以 CSV 作为输入，以 JSON 作为输出：'
- en: '[PRE16]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can then dynamically add models to the endpoint. Note that at this time,
    there is no model deployed behind an endpoint:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以动态地向端点添加模型。请注意，在此时刻，端点后面还没有部署任何模型：
- en: '[PRE17]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'That''s it. We can verify that there are five models associated with this endpoint:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。我们可以验证与该端点关联有五个模型：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We can test out the endpoint with some data points from each state. You can
    specify which model to make inference with using the `target_model` argument in
    `predictor.predict()`:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用每个状态的一些数据点来测试端点。您可以使用 `predictor.predict()` 中的 `target_model` 参数指定要用于推理的模型：
- en: '[PRE19]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this cell and onwards, we also set up a timer to measure the time it takes
    models for other states to respond in order to illustrate the nature of dynamic
    loading of the model from S3 to the endpoint. When the endpoint is first created,
    there is no model located behind the endpoint. With `add_model()`, it merely upload
    the models to an S3 location, `model_data_prefix`. When a model is first requested,
    SageMaker dynamically downloads the requested model from S3 to the ML instance
    and loads it into the inference container. This process has a longer response
    time when we first run the prediction for each of the state models, up to 1,000
    milliseconds. But once the model is loaded into the memory in the container behind
    the endpoint, the response time is greatly reduced, to around 20 milliseconds.
    When a model is loaded, it is persisted in the container until the memory of the
    instance is exhausted by having too many models loaded at once. Then SageMaker
    unloads models that are not being used anymore from memory while still keeping
    `model.tar.gz` on disk in the instance for the next request to avoid downloading
    it from S3.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个单元格以及之后，我们还设置了一个计时器来测量其他状态模型响应所需的时间，以便说明模型从 S3 到端点的动态加载特性。当端点首次创建时，端点后面没有模型。使用
    `add_model()`，它仅仅将模型上传到 S3 位置，`model_data_prefix`。当首次请求模型时，SageMaker 会动态地从 S3
    下载请求的模型到 ML 实例，并将其加载到推理容器中。当我们首次为每个状态模型进行预测时，这个过程有较长的响应时间，高达 1,000 毫秒。但一旦模型被加载到端点后面的容器内存中，响应时间会大大减少，大约为
    20 毫秒。当模型被加载时，它会被保存在容器中，直到实例的内存因一次性加载太多模型而被耗尽。然后 SageMaker 会从内存中卸载不再使用的模型，同时在实例的磁盘上保留
    `model.tar.gz`，以便下次请求避免从 S3 下载。
- en: In this example, we showed how to host a SageMaker multi-model endpoint that
    is flexible and cost-effective because it drastically reduces the number of endpoints
    needed for your use case. So, instead of hosting and paying for five endpoints,
    we would only host and pay for one endpoint. That's an easy 80% cost saving. With
    hosting models trained for 50 US states in 1 endpoint instead of 50, that's a
    98% cost saving!
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们展示了如何托管一个灵活且经济的 SageMaker 多模型端点，因为它大大减少了您用例所需的端点数量。因此，我们不会托管和支付五个端点的费用，而只需托管和支付一个端点的费用。这将节省
    80% 的成本。在 1 个端点中托管为 50 个美国州训练的模型，而不是 50 个，这将节省 98% 的成本！
- en: With SageMaker multi-model endpoints, you can host as many models as you can
    in an S3 bucket location. The number of simultaneous models you can load in an
    endpoint depends on the memory footprint of your models and the amount of RAM
    on the compute instance. Multi-model endpoints are suitable for use cases where
    you have models that are built in the same framework (XGBoost in this example),
    and where it is tolerable to have latency on less frequently used models.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SageMaker 多模型端点，您可以在 S3 存储桶位置托管尽可能多的模型。您可以在端点中加载的模型数量取决于您模型的内存占用和计算实例上的 RAM
    量。多模型端点适用于您有在相同框架（本例中为 XGBoost）中构建的模型，并且可以容忍较少使用模型的延迟的情况。
- en: Note
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If you have models built from different ML frameworks, for example, a mix of
    TensorFlow, PyTorch, and XGBoost models, you can use a multi-container endpoint,
    which allows hosting up to 15 distinct framework containers. Another benefit of
    multi-container endpoints is that they do not have latency penalties as all containers
    are running at the same time. Find out more at [https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有由不同的机器学习框架构建的模型，例如 TensorFlow、PyTorch 和 XGBoost 模型的混合，您可以使用多容器端点，这允许托管多达
    15 个不同的框架容器。多容器端点的另一个好处是，由于所有容器同时运行，它们没有延迟惩罚。更多信息请参阅[https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/multi-container-endpoints.html)。
- en: The other optimization approach is using a technique called load testing to
    help us choose the instance and autoscaling policy.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种优化方法是使用称为压力测试的技术来帮助我们选择实例和自动扩展策略。
- en: Optimizing instance type and autoscaling with load testing
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用压力测试优化实例类型和自动扩展
- en: Load testing is a technique that allows us to understand how our ML model hosted
    in an endpoint with a compute resource configuration responds to online traffic.
    There are factors such as model size, ML framework, number of CPUs, amount of
    RAM, autoscaling policy, and traffic size that affect how your ML model performs
    in the cloud. Understandably, it's not easy to predict how many requests can come
    to an endpoint over time. It is prudent to understand how your model and endpoint
    behave in this complex situation. Load testing creates artificial traffic and
    requests to your endpoint and stress tests how your model and endpoint respond
    in terms of model latency, instance CPU utilization, memory footprint, and so
    on.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 压力测试是一种技术，它使我们能够了解我们的机器学习模型在具有计算资源配置的端点上如何响应在线流量。模型大小、机器学习框架、CPU 数量、RAM 量、自动扩展策略和流量大小等因素会影响您的机器学习模型在云中的表现。显然，预测一段时间内可以到达端点的请求数量并不容易。理解模型和端点在这种复杂情况下的行为是明智的。压力测试为您的端点创建人工流量和请求，并测试模型和端点在模型延迟、实例
    CPU 利用率、内存占用等方面的响应。
- en: In this section, let's run some load testing against the endpoint we created
    in `chapter07/02-tensorflow_sentiment_analysis_inference.ipynb` with some scenarios.
    In the example, we hosted a TensorFlow-based model to an `ml.c5.xlarge` instance,
    which has 4 vCPUs and 8 GiB of memory.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对在 `chapter07/02-tensorflow_sentiment_analysis_inference.ipynb` 中创建的端点进行一些场景的压力测试。在示例中，我们将一个基于
    TensorFlow 的模型托管到具有 4 个 vCPU 和 8 GiB 内存 `ml.c5.xlarge` 实例上。
- en: First of all, we need to understand the model's latency and capacity as a function
    of the type of instance and the number of instances before an endpoint becomes
    unavailable. Then we vary the instance configuration and autoscaling configuration
    until the desired latency and traffic capacity has been reached.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要在端点不可用之前理解模型的延迟和容量作为实例类型和实例数量的函数。然后我们调整实例配置和自动扩展配置，直到达到所需的延迟和流量容量。
- en: 'Please open the `Getting-Started-with-Amazon-SageMaker-Studio/chapter07/04-load_testing.ipynb`
    notebook with the `ml.t3.xlarge` instance and follow these steps:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 请使用 `ml.t3.xlarge` 实例打开 `Getting-Started-with-Amazon-SageMaker-Studio/chapter07/04-load_testing.ipynb`
    笔记本，并按照以下步骤操作：
- en: We use a Python load testing framework called **locust** to perform the load
    testing in SageMaker Studio. Let's download the library first in the notebook.
    You can read more about the library at [https://docs.locust.io/en/stable/index.html](https://docs.locust.io/en/stable/index.html).
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在 SageMaker Studio 中使用一个名为 **locust** 的 Python 压力测试框架来进行负载测试。首先在笔记本中下载这个库。你可以在
    [https://docs.locust.io/en/stable/index.html](https://docs.locust.io/en/stable/index.html)
    上了解更多关于这个库的信息。
- en: As usual, we set up the SageMaker session in the second cell.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同往常，我们在第二个单元中设置 SageMaker 会话。
- en: Create a load testing configuration script, `load_testing/locustfile.py`, which
    is required by locust. The script is also provided within the repository. This
    cell overwrites the file. In this configuration, we instruct locust to create
    simulated users (the `SMLoadTestUser` class) to run model inference against a
    SageMaker endpoint (the `test_endpoint` class function) provided by the environment
    variable with a data point loaded from `imdb_data/test/test.csv`. Here, the response
    time, `total_time`, is measured in **milliseconds** (**ms**).
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个负载测试配置脚本，`load_testing/locustfile.py`，这是 locust 所必需的。该脚本也包含在存储库中。这个单元会覆盖文件。在这个配置中，我们指示
    locust 创建模拟用户（`SMLoadTestUser` 类）来对 SageMaker 端点（由环境变量提供的 `test_endpoint` 类函数）进行模型推理，数据点从
    `imdb_data/test/test.csv` 加载。在这里，响应时间 `total_time` 以 **毫秒**（**ms**）为单位进行测量。
- en: 'In the next cell, we start our first load testing job on our already-deployed
    SageMaker endpoint with an `ml.c5.xlarge` instance. Remember we applied the autoscaling
    policy in `chapter07/02-tensorflow_sentiment_analysis_inference`? Let''s first
    reverse the policy by setting `MaxCapacity` to `1` to make sure the endpoint does
    not scale out to multiple instances during our first test:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个单元中，我们使用 `ml.c5.xlarge` 实例在我们已经部署的 SageMaker 端点上进行第一次负载测试工作。记得我们在 `chapter07/02-tensorflow_sentiment_analysis_inference`
    中应用了自动扩展策略吗？让我们首先通过将 `MaxCapacity` 设置为 `1` 来反转策略，以确保在第一次测试期间端点不会扩展到多个实例：
- en: '[PRE20]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then we test the endpoint with locust. We set up two-worker distributed load
    testing on two CPU cores in the following snippet. We instruct `locust` to create
    10 users (the `-r 10` argument) per second up to 500 online users (`-u 500`),
    each making calls to our endpoint for 60 seconds (`-t 60s`). Please replace the
    `ENDPOINT_NAME` string with your SageMaker endpoint name. You can find the endpoint
    name in the **Endpoints** registry, as shown in *Figure 7.3*:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们使用 locust 测试端点。在下面的代码片段中，我们设置了在两个 CPU 核心上进行的两个工作者的分布式负载测试。我们指示 `locust`
    每秒创建 10 个用户（`-r 10` 参数），最多 500 个在线用户（`-u 500`），每个用户对端点进行 60 秒（`-t 60s`）的调用。请将
    `ENDPOINT_NAME` 字符串替换为你的 SageMaker 端点名称。你可以在 **Endpoints** 注册表中找到端点名称，如图 7.3 所示：
- en: '[PRE21]'
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: "As it is running, let's navigate to the `<endpoint-with-ml.c5-xlarge-instance>`\
    \ with your endpoint name and replace the region if you use a region other than\
    \ us-west-2: \L`https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#metricsV2:graph=~(metrics~(~(~'AWS*2fSageMaker~'InvocationsPerInstance~'EndpointName~'<endpoint-with-ml.c5-xlarge-instance>~'VariantName~'AllTraffic)~(~'.~'ModelLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invocations~'.~'.~'.~'.)~(~``'.~'OverheadLatency~'.~'.~'.~'.~(stat~'Average))~(~'.~'Invoca\
    \ tion5XXErrors~'.~'.~'.~'.)~(~'.~'Invocation4XXErrors~'.~'.~'.~'.))~view~'timeSeries~stacked~false~region~'us-west-2~stat~'Sum~period~60~start~'-PT3H~end~'P0D\
    \ );query=~'*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20<endpoint-with-ml.c5-xlarge-instance>`"
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '当它正在运行时，让我们导航到 `<endpoint-with-ml.c5-xlarge-instance>` 并用你的端点名称替换它，如果你使用的是除
    us-west-2 之外的区域：     `https://us-west-2.console.aws.amazon.com/cloudwatch/home?region=us-west-2#metricsV2:graph=~(metrics~(~(~''AWS*2fSageMaker~''InvocationsPerInstance~''EndpointName~''<endpoint-with-ml.c5-xlarge-instance>~''VariantName~''AllTraffic)~(~''.~''ModelLatency~''.~''.~''.~''.~(stat~''Average''))~(~''.~''Invocations~''.~''.~''.~''.)~(~``''.~''OverheadLatency~''.~''.~''.~''.~(stat~''Average''))~(~''.~''Invoca
    tion5XXErrors~''.~''.~''.~''.)~(~''.~''Invocation4XXErrors~''.~''.~''.~''.))~view~''timeSeries~stacked~false~region~''us-west-2~stat~''Sum~period~60~start~''-PT3H~end~''P0D
    );query=~''*7bAWS*2fSageMaker*2cEndpointName*2cVariantName*7d*20<endpoint-with-ml.c5-xlarge-instance>`'
- en: You can see a dashboard in *Figure 7.4*. The dashboard has captured the most
    important metrics regarding our SageMaker endpoint's health and status. **Invocations**
    and **InvocationsPerInstance** show the total number of invocations and per-instance
    counts. **Invocation5XXErrors** and **Invocation4XXErrors** are error counts with
    HTTP codes 5XX and 4XX respectively. **ModelLatency** (in microseconds) is the
    time taken by a model inside the container behind a SageMaker endpoint to return
    a response. **OverheadLatency** (in microseconds) is the time taken for our SageMaker
    endpoint to transmit a request and a response. Total latency for a request is
    **ModelLatency** plus **OverheadLatency**. These metrics are emitted by our SageMaker
    endpoint to Amazon CloudWatch.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在*图7.4*中看到一个仪表板。仪表板捕获了我们SageMaker端点健康和状态的最重要指标。**Invocations**和**InvocationsPerInstance**显示了总的调用次数和每个实例的计数。**Invocation5XXErrors**和**Invocation4XXErrors**分别表示带有HTTP代码5XX和4XX的错误计数。**ModelLatency**（以微秒为单位）是SageMaker端点后面容器中的模型返回响应所需的时间。**OverheadLatency**（以微秒为单位）是我们SageMaker端点传输请求和响应所需的时间。请求的总延迟是**ModelLatency**加上**OverheadLatency**。这些指标是由我们的SageMaker端点发送到Amazon
    CloudWatch的。
- en: '![Figure 7.4 – Viewing load testing results on one ml.c5.xlarge instance in
    Amazon CloudWatch'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.4 – 在Amazon CloudWatch中查看单个ml.c5.xlarge实例的负载测试结果'
- en: '](img/B17447_07_04.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_07_04.jpg]'
- en: Figure 7.4 – Viewing load testing results on one ml.c5.xlarge instance in Amazon
    CloudWatch
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 在Amazon CloudWatch中查看单个ml.c5.xlarge实例的负载测试结果
- en: In the first load test (*Figure 7.4*), we can see that there are around 8,221
    invocations per minute, 0 errors, with an average **ModelLatency** of **53,825**
    microseconds, or 53.8 milliseconds.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一次负载测试（*图7.4*）中，我们可以看到每分钟大约有8,221次调用，0个错误，平均**ModelLatency**为**53,825**微秒，即53.8毫秒。
- en: With these numbers in mind as a baseline, let's scale up the instance, that
    is, let's use a larger instance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些数字作为基准的情况下，让我们扩展实例，也就是说，让我们使用更大的实例。
- en: 'We load up the previous IMDb sentiment analysis training job and deploy the
    TensorFlow model to another endpoint with one `ml.c5.2xlarge` instance, which
    has 8 vCPU and 16 GiB of memory, twice the capacity of `ml.c5.xlarge`:'
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载了之前的IMDb情感分析训练作业，并将TensorFlow模型部署到另一个端点，该端点有一个`ml.c5.2xlarge`实例，该实例有8个vCPU和16
    GiB的内存，是`ml.c5.xlarge`的两倍：
- en: '[PRE22]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The deployment process takes a couple of minutes. Then we retrieve the endpoint
    name with the next cell, `predictor_c5_2xl.endpoint_name`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 部署过程需要几分钟。然后我们使用下一个单元格`predictor_c5_2xl.endpoint_name`检索端点名称。
- en: 'Replace `ENDPOINT_NAME` with the output of `predictor_c5_2xl.endpoint_name`
    and run the cell to kick off another load test against the new endpoint:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ENDPOINT_NAME`替换为`predictor_c5_2xl.endpoint_name`的输出，并运行单元格以启动针对新端点的另一个负载测试：
- en: '[PRE23]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'In Amazon CloudWatch (replacing `<endpoint-with-ml.c5-xlarge- instance>` in
    the long URL in *step 4* or clicking the hyperlink generated in the next cell
    in the notebook), we can see how the endpoint responds to traffic in *Figure 7.5*:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Amazon CloudWatch（在*步骤4*中的长URL中替换`<endpoint-with-ml.c5-xlarge-instance>`或点击笔记本中下一个单元格生成的超链接），我们可以在*图7.5*中看到端点对流量的响应：
- en: '![Figure 7.5 – Viewing load testing results on one ml.c5.2xlarge instance in
    Amazon CloudWatch'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.5 – 在Amazon CloudWatch中查看单个ml.c5.2xlarge实例的负载测试结果'
- en: '](img/B17447_07_05.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_07_05.jpg]'
- en: Figure 7.5 – Viewing load testing results on one ml.c5.2xlarge instance in Amazon
    CloudWatch
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 在Amazon CloudWatch中查看单个ml.c5.2xlarge实例的负载测试结果
- en: Similarly, the traffic that locust was able to generate is around 8,000 invocations
    per minute (`ml.c5.xlarge` instance.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，locust能够生成的流量大约是每分钟8,000次调用（`ml.c5.xlarge`实例）。
- en: 'Next, we deploy the same model to an `ml.g4dn.xlarge` instance, which is a
    GPU instance dedicated to model inference use cases. G4dn instances are equipped
    with NVIDIA T4 GPUs and are cost-effective for ML inference and small neural network
    training jobs:'
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将相同的模型部署到`ml.g4dn.xlarge`实例上，这是一个专门用于模型推理用例的GPU实例。G4dn实例配备了NVIDIA T4 GPU，对于ML推理和小型神经网络训练作业来说性价比很高：
- en: '[PRE24]'
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We set up a load testing job similar to the previous ones. The result can also
    be found on the Amazon CloudWatch dashboard by replacing `<endpoint-with-ml.c5-xlarge-
    instance>` in the long URL in *step 4* or clicking the hyperlink generated in
    the next cell in the notebook. As shown in *Figure 7.6*, with around 6,000 invocations
    per minute, the average `ml.g4dn.xlarge` instance making inference much faster.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了一个类似于之前的负载测试任务。结果也可以在Amazon CloudWatch仪表板上找到，只需在*步骤4*中将长URL中的`<endpoint-with-ml.c5-xlarge-instance>`替换，或者点击笔记本中下一个单元格生成的超链接。如图*图7.6*所示，每分钟大约有6,000次调用，平均的`ml.g4dn.xlarge`实例进行推理的速度更快。
- en: '![Figure 7.6 – Viewing the load test results on one ml.g4dn.xlarge instance
    in Amazon CloudWatch'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.6 – 在Amazon CloudWatch中查看单个ml.g4dn.xlarge实例的负载测试结果'
- en: '](img/B17447_07_06.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_07_06.jpg]'
- en: Figure 7.6 – Viewing the load test results on one ml.g4dn.xlarge instance in
    Amazon CloudWatch
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 7.6 – 在Amazon CloudWatch中查看单个ml.g4dn.xlarge实例的负载测试结果
- en: 'The last approach we should try is autoscaling. Autoscaling allows us to spread
    the load across instances, which in turns helps improve the CPU utilization and
    model latency. We once again set the autoscaling to `MaxCapacity=4` with the following
    cell:'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该尝试的最后一种方法是自动扩展。自动扩展允许我们将负载分散到实例上，这反过来有助于提高CPU利用率和模型延迟。我们再次将自动扩展设置为`MaxCapacity=4`，如下单元格所示：
- en: '[PRE25]'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: You can confirm the scaling policy attached with the next cell in the notebook.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过笔记本中下一个单元格附加的扩展策略来确认。
- en: We are ready to perform our last load testing experiment. Replace `ENDPOINT_NAME`
    with `<endpoint-with-ml.c5-xlarge-instance>`, and run the next cell to kick off
    the load test against the endpoint that is now able to scale out up to four instances.
    This load test needs to run longer in order to see the effect of autoscaling.
    This is because SageMaker first needs to observe the number of invocations to
    decide how many new instances are based on our target metric, `SageMakerVariantInvocationsPerInstance=4000`.
    With our traffic at around 8,000 invocations per minute, SageMaker will spin up
    one additional instance to have a per-instance invocation at the desired value,
    4,000\. Spinning up new instances takes around 5 minutes to complete.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们准备进行最后的负载测试实验。将`ENDPOINT_NAME`替换为`<endpoint-with-ml.c5-xlarge-instance>`，并运行下一个单元格以启动针对现在能够扩展到四个实例的端点的负载测试。这个负载测试需要运行更长的时间，以便看到自动扩展的效果。这是因为SageMaker首先需要观察调用次数，然后根据我们的目标指标`SageMakerVariantInvocationsPerInstance=4000`来决定需要多少新实例。在我们的流量大约每分钟8,000次调用的情况下，SageMaker将启动一个额外的实例，以实现每个实例的调用次数达到期望值，即4,000。启动新实例需要大约5分钟才能完成。
- en: '![Figure 7.7 – Viewing load testing results on an ml.c5.xlarge instance with
    autoscaling in Amazon CloudWatch'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 7.7 – 在Amazon CloudWatch中查看具有自动扩展的ml.c5.xlarge实例的负载测试结果'
- en: '](img/B17447_07_07.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B17447_07_07.jpg]'
- en: Figure 7.7 – Viewing load testing results on an ml.c5.xlarge instance with autoscaling
    in Amazon CloudWatch
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 7.7 – 在Amazon CloudWatch中查看具有自动扩展的ml.c5.xlarge实例的负载测试结果
- en: We can see the load test result on the Amazon CloudWatch dashboard, as shown
    in *Figure 7.7*. We can see an interesting pattern in the chart. We can clearly
    see something happened between `18:48` and `18:49`. The `33,839` microseconds
    (33.8 milliseconds). And the `SageMakerVariantInvocationsPerInstance=4000` and
    splits the traffic into two instances. A lower **ModelLatency** is the preferred
    outcome of having multiple instances to share the load.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Amazon CloudWatch仪表板上看到负载测试结果，如图*图7.7*所示。我们可以在图表中看到有趣的模式。我们可以清楚地看到在`18:48`和`18:49`之间发生了某些事情。`33,839`微秒（33.8毫秒）。并且`SageMakerVariantInvocationsPerInstance=4000`将流量分成两个实例。较低的**模型延迟**是多个实例分担负载的理想结果。
- en: 'After the four load testing experiments, we can conclude that at a load of
    around 6,000 to 8,000 invocations per minute, the following takes place:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 经过四次负载测试实验后，我们可以得出结论，在每分钟大约6,000到8,000次调用的负载下，以下情况发生：
- en: Single-instance performance is measured by average `ml.g4dn.xlarge` with 1 GPU
    and 4 vCPUs gives the smallest `ml.c5.2xlarge` instance with 8 vCPUs at 45.8 milliseconds.
    Last is the `ml.c5.xlarge` instance with 4 vCPUs at 53.8 milliseconds.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单实例性能是通过平均`ml.g4dn.xlarge`（1个GPU和4个vCPU）来衡量的，它给出了最小的`ml.c5.2xlarge`实例（8个vCPU）在45.8毫秒。最后是`ml.c5.xlarge`实例（4个vCPU）在53.8毫秒。
- en: With autoscaling, two `ml.c5.xlarge` instances with 8 vCPUs achieves 33.8 milliseconds'
    `ml.c5.2xlarge` with the same number of vCPUs.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自动扩展，两个具有8个vCPU的`ml.c5.xlarge`实例实现了与相同数量的vCPU的`ml.c5.2xlarge`在33.8毫秒的`ml.c5.2xlarge`。
- en: If we consider another dimension, the cost of the instance(s), we can come to
    an even more interesting situation, as shown in *Figure 7.8*. In the table, we
    create a simple compound metric to measure the cost-performance efficiency of
    a configuration by multiplying **ModelLatency** by the price per hour of the instance
    configuration.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑另一个维度，即实例的成本，我们可以遇到一个更有趣的情况，如图*图7.8*所示。在表中，我们创建了一个简单的复合指标，通过将**模型延迟**乘以实例配置的每小时价格来衡量配置的成本性能效率。
- en: '![Figure 7.8 – Cost-performance comparisons'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '![图7.8 – 成本性能比较'
- en: '](img/B17447_07_08_table.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17447_07_08_table.jpg)'
- en: Figure 7.8 – Cost-performance comparisons
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 成本性能比较
- en: If we are constrained by cost, we should consider using the last configuration
    (row d), where the monthly cost is the lowest yet with the second-best cost-performance
    efficiency while sacrificing some model latency. If we need a model latency of
    around 40 milliseconds or lower, by paying the same monthly cost, we would get
    even more bang for our buck and lower latency with the third configuration (row
    c) than the second configuration (row b). The first configuration (row a) gives
    the best model latency and the best cost-performance efficiency. But it is also
    the most expensive option. Unless there is a strict single-digit model latency
    requirement, we might not want to use this option.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们受限于成本，我们应该考虑使用最后一个配置（行d），其中月度成本最低，同时牺牲一些模型延迟，但成本性能效率位居第二。如果我们需要大约40毫秒或更低的模型延迟，通过支付相同的月度成本，我们使用第三个配置（行c）比第二个配置（行b）可以获得更多的性价比和更低的延迟。第一个配置（行a）提供最佳的模型延迟和最佳的成本性能效率。但这也是最昂贵的选项。除非有严格的个位数模型延迟要求，我们可能不想使用这个选项。
- en: To reduce cost, when you complete the examples, make sure to uncomment and run
    the last cells in `02-tensorflow_sentiment_analysis_inference.ipynb`, `03-multimodel-endpoint.ipynb`,
    and `04-load_testing.ipynb` to delete the endpoints in order to stop incurring
    charges to your AWS account.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低成本，当你完成示例后，确保取消注释并运行`02-tensorflow_sentiment_analysis_inference.ipynb`、`03-multimodel-endpoint.ipynb`和`04-load_testing.ipynb`中的最后几个单元格，以删除端点，从而停止向您的AWS账户收取费用。
- en: This discussion is based on the example we used, which assumes many factors,
    such as model framework, traffic pattern, and instance types. You should follow
    the best practices we introduced for your use case and test out more instance
    types and autoscaling policies to find the optimal solution for your use case.
    You can find the full list of instances, specifications, and prices per hour in
    the **real-time inference** tab at [https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/)
    to come up with your own cost-performance efficiency analysis.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这场讨论基于我们使用的示例，它假设了许多因素，例如模型框架、流量模式和实例类型。你应该遵循我们为你的用例介绍的最佳实践，并测试更多实例类型和自动扩展策略，以找到最适合你用例的解决方案。你可以在[https://aws.amazon.com/sagemaker/pricing/](https://aws.amazon.com/sagemaker/pricing/)的**实时推理**标签页中找到完整的实例列表、规格和每小时价格，以进行你自己的成本性能效率分析。
- en: There are other optimization features in SageMaker that help you reduce latency,
    such as Amazon Elastic Inference, SageMaker Neo, and Amazon EC2 Inf1 instances.
    **Elastic Inference** ([https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html))
    attaches fractional GPUs to a SageMaker hosted endpoint. It increases the inference
    throughput and decreases the model latency for your deep learning models that
    can benefit from GPU acceleration. **SageMaker Neo** ([https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html))
    optimizes an ML model for inference in the cloud and supported devices at the
    edge with no loss in accuracy. SageMaker Neo speeds up prediction and reduces
    cost with a compiled model and optimized container in SageMaker hosted endpoint.
    **Amazon EC2 Inf1 instances** ([https://aws.amazon.com/ec2/instance-types/inf1/](https://aws.amazon.com/ec2/instance-types/inf1/))
    provide high performance and low cost in the cloud with **AWS Inferentia** chips
    designed and built by AWS for ML inference purposes. You can compile supported
    ML models using SageMaker Neo and select Inf1 instances to deploy the compiled
    model in a SageMaker hosted endpoint.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 中还有其他优化功能可以帮助您降低延迟，例如 Amazon Elastic Inference、SageMaker Neo 和 Amazon
    EC2 Inf1 实例。**弹性推理（Elastic Inference**）([https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html](https://docs.aws.amazon.com/sagemaker/latest/dg/ei-endpoints.html))
    将部分 GPU 连接到 SageMaker 托管端点。它增加了深度学习模型的推理吞吐量并降低了模型延迟，这些模型可以从 GPU 加速中受益。**SageMaker
    Neo** ([https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html](https://docs.aws.amazon.com/sagemaker/latest/dg/neo.html))
    在云和边缘支持的设备上优化机器学习模型进行推理，而不会损失精度。SageMaker Neo 通过在 SageMaker 托管端点中使用编译的模型和优化的容器来加速预测并降低成本。**Amazon
    EC2 Inf1 实例**([https://aws.amazon.com/ec2/instance-types/inf1/](https://aws.amazon.com/ec2/instance-types/inf1/))
    在云中提供高性能和低成本，使用的是 AWS 为机器学习推理目的设计和构建的 **AWS Inferentia** 芯片。您可以使用 SageMaker Neo
    编译支持的机器学习模型，并选择 Inf1 实例在 SageMaker 托管端点中部署编译后的模型。
- en: Summary
  id: totrans-165
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned how to efficiently make ML inferences in the cloud
    using Amazon SageMaker. We followed up with what we trained in the previous chapter—an
    IMDb movie review sentiment prediction—to demonstrate SageMaker's batch transform
    and real-time hosting. More importantly, we learned how to optimize for cost and
    model latency with load testing. We also learned about another great cost-saving
    opportunity by hosting multiple ML models in one single endpoint using SageMaker
    multi-model endpoints. Once you have selected the best inference option and instance
    types for your use case, SageMaker makes deploying your models straightforward.
    With these step-by-step instructions and this discussion, you will be able to
    translate what you've learned to your own ML use cases.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用 Amazon SageMaker 在云中高效地进行机器学习推理。我们接着学习了上一章中训练的内容——IMDb 电影评论情感预测，以展示
    SageMaker 的批量转换和实时托管。更重要的是，我们学习了如何通过负载测试来优化成本和模型延迟。我们还了解了通过使用 SageMaker 多模型端点在单个端点托管多个机器学习模型来节省成本的机会。一旦您为您的用例选择了最佳推理选项和实例类型，SageMaker
    就会使模型部署变得简单直接。有了这些逐步说明和讨论，您将能够将所学知识应用到您自己的机器学习用例中。
- en: In the next chapter, we will take a different route to learn how we can use
    SageMaker's JumpStart and Autopilot to quick-start your ML journey. SageMaker
    JumpStart offers solutions to help you see how best practices and ML use cases
    are tackled. JumpStart model zoos collect numerous pre-trained deep learning models
    for natural language processing and computer vision use cases. SageMaker Autopilot
    is an autoML feature that crunches data and trains a performant model without
    you worrying about data, coding, or modeling. After we have learned the fundamentals
    of SageMaker—fully managed model training and model hosting—we can better understand
    how SageMaker JumpStart and Autopilot work.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将采取不同的路径来学习如何使用 SageMaker 的 JumpStart 和 Autopilot 快速启动您的机器学习之旅。SageMaker
    JumpStart 提供解决方案，帮助您了解如何应对最佳实践和机器学习用例。JumpStart 模型动物园收集了众多预训练的深度学习模型，用于自然语言处理和计算机视觉用例。SageMaker
    Autopilot 是一个自动机器学习（autoML）功能，它可以处理数据并训练性能良好的模型，而无需您担心数据、编码或建模。在我们学习了 SageMaker
    的基础知识——完全托管模型训练和模型托管之后，我们可以更好地理解 SageMaker JumpStart 和 Autopilot 的工作原理。
