- en: Building Models with Distance Metrics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用距离度量构建模型
- en: 'This chapter will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍以下配方：
- en: Using k-means to cluster data
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 k-means 对数据进行聚类
- en: Optimizing the number of centroids
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化质心数量
- en: Assessing cluster correctness
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估聚类的正确性
- en: Using MiniBatch k-means to handle more data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 MiniBatch k-means 处理更多数据
- en: Quantizing an image with k-means clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 k-means 聚类对图像进行量化
- en: Finding the closest objects in the feature space
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特征空间中找到最接近的对象
- en: Probabilistic clustering with Gaussian Mixture Models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高斯混合模型进行概率聚类
- en: Using k-means for outlier detection
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 k-means 进行异常值检测
- en: Using KNN for regression
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 KNN 进行回归
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we'll cover clustering. Clustering is often grouped with unsupervised
    techniques. These techniques assume that we do not know the outcome variable.
    This leads to ambiguity in outcomes and objectives in practice, but nevertheless,
    clustering can be useful. As we'll see, we can use clustering to localize our
    estimates in a supervised setting. This is perhaps why clustering is so effective;
    it can handle a wide range of situations, and often the results are, for the lack
    of a better term, sane.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将讨论聚类。聚类通常与无监督技术一起使用。这些技术假设我们不知道结果变量。这会导致实践中的结果和目标出现模糊，但尽管如此，聚类仍然很有用。正如我们将看到的那样，我们可以在监督环境中使用聚类来定位我们的估计。这或许是聚类如此有效的原因；它可以处理各种情况，且结果通常是，在没有更好术语的情况下，可以说是合理的。
- en: We'll walk through a wide variety of applications in this chapter, from image
    processing to regression and outlier detection. Clustering is related to classification
    of categories. You have a finite set of blobs or categories. Unlike classification,
    you do not know the categories in advance. Additionally, clustering can often
    be viewed through a continuous and probabilistic or optimization lens.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将涵盖各种应用，从图像处理到回归和异常值检测。聚类与类别的分类相关。你有一个有限的簇或类别集。与分类不同，你事先并不知道这些类别。此外，聚类通常可以通过连续的、概率性的或优化的视角来看待。
- en: Different interpretations lead to various trade-offs. We'll walk through how
    to fit the models here so that you'll have the tools to try out many models when
    faced with a clustering problem.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的解释会导致不同的权衡。我们将讨论如何在这里拟合模型，这样当你遇到聚类问题时，你就有工具可以尝试多种模型。
- en: Using k-means to cluster data
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k-means 对数据进行聚类
- en: In a dataset, we observe sets of points gathered together. With k-means, we
    will categorize all the points into groups, or clusters.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，我们观察到一些点聚集在一起。使用 k-means，我们将把所有点分到不同的组或簇中。
- en: Getting ready
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'First, let''s walk through some simple clustering; then we''ll talk about how
    k-means works:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们演示一些简单的聚类操作，然后我们将讨论 k-means 如何工作：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Also, since we''ll be doing some plotting, import `matplotlib` as shown:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们将进行一些绘图操作，按如下方式导入 `matplotlib`：
- en: '[PRE1]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: How to do it…
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何进行……
- en: 'We are going to walk through a simple example that clusters blobs of fake data.
    Then we''ll talk a little bit about how k-means works to find the optimal number
    of blobs:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过一个简单的示例，演示如何对虚拟数据的簇进行聚类。然后，我们将简要介绍 k-means 如何工作，以寻找最佳的簇数：
- en: 'Looking at our blobs, we can see that there are three distinct clusters:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 观察我们的簇，我们可以看到有三个明显的聚类：
- en: '[PRE2]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![](img/c3a55fba-8576-4348-98e5-b471123d27d2.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c3a55fba-8576-4348-98e5-b471123d27d2.png)'
- en: Now we can use k-means to find the centers of these clusters.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用 k-means 找到这些簇的中心。
- en: 'In the first example, we''ll pretend we know that there are three centers:'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第一个示例中，我们假设我们知道有三个中心：
- en: '[PRE3]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The following screenshot shows the output:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下截图显示了输出结果：
- en: '![](img/887c0b1a-c85f-42f3-bb92-ac9ab324c15d.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](img/887c0b1a-c85f-42f3-bb92-ac9ab324c15d.png)'
- en: 'Other attributes are useful too. For instance, the `labels_` attribute will
    produce the expected label for each point:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其他属性也很有用。例如，`labels_` 属性将为每个点生成预期的标签：
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can check whether `kmean.labels_` is the same as the classes, but because
    k-means has no knowledge of the classes going in, it cannot assign the sample
    index values to both classes:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查 `kmean.labels_` 是否与类别相同，但由于 k-means 在开始时并不知道类别，它无法将样本索引值分配给两个类别：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Feel free to swap `1` and `0` in classes to check whether it matches up with
    `labels_`. The transform function is quite useful in the sense that it will output
    the distance between each point and the centroid:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 随时交换类别中的 `1` 和 `0`，检查它们是否与 `labels_` 匹配。变换函数非常有用，因为它会输出每个点与质心之间的距离：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: How it works...
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: k-means is actually a very simple algorithm that works to minimize the within-cluster
    sum of squares of distances from the mean. We'll be minimizing the sum of squares
    yet again!
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: k-means实际上是一个非常简单的算法，旨在最小化群内距离均值的平方和。我们将再次最小化平方和！
- en: 'It does this by first setting a prespecified number of clusters, K, and then
    alternating between the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 它首先设置一个预设的簇数K，然后在以下步骤之间交替进行：
- en: Assigning each observation to the nearest cluster
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个观察值分配给最近的簇
- en: Updating each centroid by calculating the mean of each observation assigned
    to this cluster
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过计算分配给该簇的每个观察值的均值来更新每个质心
- en: This happens until some specified criterion is met. Centroids are difficult
    to interpret, and it can also be very difficult to determine whether we have the
    correct number of centroids. It's important to understand whether your data is
    unlabeled or not as this will directly influence the evaluation measures you can
    use.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这一过程会一直持续，直到满足某个特定的标准。质心很难解释，而且确定我们是否有正确数量的质心也非常困难。理解数据是否标记过是很重要的，因为这将直接影响你可以使用的评估方法。
- en: Optimizing the number of centroids
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化质心数量
- en: When doing k-means clustering, we really do not know the right number of clusters
    in advance, so finding this out is an important step. Once we know (or estimate)
    the number of centroids, the problem will start to look more like a classification
    one as our knowledge to work with will have increased substantially.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行k-means聚类时，我们实际上无法事先知道正确的簇数，因此找出这一点是一个重要步骤。一旦我们知道（或估计）了质心的数量，问题就开始更像一个分类问题，因为我们可以使用的知识量大大增加了。
- en: Getting ready
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备开始
- en: Evaluating the model performance for unsupervised techniques is a challenge.
    Consequently, `sklearn` has several methods for evaluating clustering when a ground
    truth is known, and very few for when it isn't.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对无监督技术的模型性能进行评估是一项挑战。因此，`sklearn`提供了几种在已知真实标签的情况下评估聚类的方法，而在未知真实标签的情况下，方法则非常有限。
- en: We'll start with a single cluster model and evaluate its similarity. This is
    more for the purpose of mechanics as measuring the similarity of one cluster count
    is clearly not useful in finding the ground truth number of clusters.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从单一簇模型开始并评估其相似度。这更多是为了操作上的需要，因为测量单一簇数量的相似度显然对于找到真实的簇数并没有什么用处。
- en: How to do it...
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'To get started, we''ll create several blobs that can be used to simulate clusters
    of data:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了开始，我们将创建几个簇状数据，用于模拟数据的聚类：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'First, we''ll look at the silhouette distance. Silhouette distance is the ratio
    of the difference between the in-cluster dissimilarity and the closest out-of-cluster
    dissimilarity, and the maximum of these two values. It can be thought of as a
    measure of how separate the clusters are. Let''s look at the distribution of distances
    from the points to the cluster centers; it''s useful to understand silhouette
    distances:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们来看轮廓距离。轮廓距离是群内不相似度与最接近的群外不相似度之间的差值与这两个值的最大值之比。可以将其视为衡量簇与簇之间分离度的标准。让我们看看从点到簇中心的距离分布；这对于理解轮廓距离很有帮助：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following is part of the output:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是部分输出：
- en: '![](img/b217e764-86a9-4ccc-adf8-e51a507c9209.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b217e764-86a9-4ccc-adf8-e51a507c9209.png)'
- en: Notice that generally, the higher the number of coefficients close to 1 (which
    is good), the better the score.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，通常情况下，接近1的系数越多（这很好），得分越高。
- en: How it works...
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'The average of the silhouette coefficients is often used to describe the entire
    model''s fit:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数的平均值通常用于描述整个模型的拟合度：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It''s very common; in fact, the metrics module exposes a function to arrive
    at the value we just got:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这是非常常见的，实际上，metrics模块暴露了一个函数，可以得到我们刚刚得到的值：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The following is the output:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出：
- en: '![](img/7c24ff17-ff8d-4555-ad8c-89dd97bac11f.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7c24ff17-ff8d-4555-ad8c-89dd97bac11f.png)'
- en: This plot shows that the silhouette averages as the number of centroids increase.
    We can see that the optimum number, according to the data generating process,
    is 3; but here it looks like it's around 7 or 8\. This is the reality of clustering;
    quite often, we won't get the correct number of clusters. We can only really hope
    to estimate the number of clusters to some approximation.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图显示了随着质心数量的增加，轮廓系数的平均值变化。我们可以看到，根据数据生成过程，最佳的簇数是3；但在这里，看起来它大约是7或8。这就是聚类的现实；通常，我们无法获得正确的簇数。我们只能希望对簇数进行某种程度的估算。
- en: Assessing cluster correctness
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估簇的正确性
- en: We talked a little bit about assessing clusters when the ground truth is not
    known. However, we have not yet talked about assessing k-means when the cluster
    is known. In a lot of cases, this isn't knowable; however, if there is outside
    annotation, we will know the ground truth or at least the proxy sometimes.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍微谈了一下当真实标签未知时如何评估聚类。然而，我们还没有讨论如何在已知簇的情况下评估 k-means。在很多情况下，这个是不可知的；然而，如果有外部标注，我们将知道真实标签或至少知道某种代理标签。
- en: Getting ready
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好了
- en: So, let's assume a world where we have an outside agent supplying us with the
    ground truth.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，让我们假设一个世界，假设有一个外部代理为我们提供真实标签。
- en: 'We''ll create a simple dataset, evaluate the measures of correctness against
    the ground truth in several ways, and then discuss them:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个简单的数据集，以几种方式评估与真实标签的正确性度量，然后进行讨论：
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: How to do it...
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Before we walk through the metrics, let''s take a look at the dataset:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在讲解度量标准之前，我们先来看一下数据集：
- en: '[PRE12]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](img/fc26c78b-07e5-41d5-a23c-15fea61431ae.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fc26c78b-07e5-41d5-a23c-15fea61431ae.png)'
- en: 'In order to fit a k-means model, we''ll create a `KMeans` object from the cluster
    module:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了拟合一个 k-means 模型，我们将从聚类模块创建一个 `KMeans` 对象：
- en: '[PRE13]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we''ve fit the model, let''s have a look at the cluster centroids:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经拟合了模型，让我们看看聚类中心：
- en: '[PRE14]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The following is the output:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/a43c8911-edf8-4152-a51c-17a20fd4c660.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a43c8911-edf8-4152-a51c-17a20fd4c660.png)'
- en: 'Now that we can view the clustering performance as a classification exercise,
    the metrics that are useful in its context are also useful here:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将聚类性能视为分类练习，其相关的度量标准在这里也同样适用：
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Clearly, we have some backward clusters. So, let''s get this straightened out
    first, and then we''ll look at the accuracy:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，我们有一些反向的簇。所以，让我们先解决这个问题，然后再看看准确性：
- en: '[PRE16]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'So, we''re roughly correct 90% of the time. The second measure of similarity
    we''ll look at is the mutual information score:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，我们大约有 90% 的时间是正确的。我们将看的第二个相似性度量是互信息得分：
- en: '[PRE17]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As the score tends to be 0, the label assignments are probably not generated
    through similar processes; however, a score closer to 1 means that there is a
    large amount of agreement between the two labels.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 当得分趋向 0 时，标签分配可能不是通过相似的过程生成的；然而，接近 1 的得分意味着两个标签之间有很大的相似性。
- en: 'For example, let''s look at what happens when the mutual information score
    itself:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看当互信息得分本身时会发生什么：
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Given the name, we can tell that there is probably an unnormalized `mutual_info_score`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从名称上看，我们可以推测这可能是一个未归一化的 `mutual_info_score`：
- en: '[PRE19]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: These are very close; however, normalized mutual information is the mutual information
    divided by the root of the product of the entropy of each set truth and assigned
    label.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些非常接近；然而，归一化互信息是互信息除以每个集合真实标签和分配标签的熵乘积的平方根。
- en: There's more...
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多内容...
- en: One cluster metric we haven't talked about yet, and one that is not reliant
    on the ground truth, is inertia. It is not very well documented as a metric at
    the moment. However, it is a metric that k-means minimizes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有讨论过的一个聚类度量，并且这个度量不依赖于真实标签的是惯性。它目前作为度量并没有得到很好的文档化。然而，它是 k-means 最小化的一个度量。
- en: 'Inertia is the sum of the squared difference between each point and its assigned
    cluster. We can use a bite of NumPy to determine this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 惯性是每个点与其分配的簇之间的平方差之和。我们可以通过一点 NumPy 来确定这一点：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Using MiniBatch k-means to handle more data
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 MiniBatch k-means 处理更多数据
- en: K-means is a nice method to use; however, it is not ideal for a lot of data.
    This is due to the complexity of k-means. This said, we can get approximate solutions
    with much better algorithmic complexity using MiniBatch k-means.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: K-means 是一种不错的方法；然而，它对于大量数据并不理想。这是由于 k-means 的复杂性。话虽如此，我们可以通过 MiniBatch k-means
    以更好的算法复杂度获得近似解。
- en: Getting ready
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备好了
- en: MiniBatch k-means is a faster implementation of k-means. K-means is computationally
    very expensive; the problem is NP-hard.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: MiniBatch k-means 是 k-means 的更快实现。K-means 在计算上非常昂贵；该问题是 NP 难问题。
- en: However, using MiniBatch k-means, we can speed up k-means by orders of magnitude.
    This is achieved by taking many subsamples that are called MiniBatches. Given
    the convergence properties of subsampling, a close approximation to regular k-means
    is achieved provided there are good initial conditions.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用 MiniBatch k-means，我们可以通过数量级加速 k-means。这是通过使用许多叫做 MiniBatches 的子样本来实现的。考虑到子抽样的收敛性，提供了良好的初始条件时，可以实现对常规
    k-means 的接近近似。
- en: How to do it...
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'Let''s do some very high-level profiling of MiniBatch clustering. First, we''ll
    look at the overall speed difference, and then we''ll look at the errors in the
    estimates:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们对MiniBatch聚类进行一些非常高层次的分析。首先，我们将看一下整体速度差异，然后再看一下估计误差：
- en: '[PRE21]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Understand that these metrics are meant to expose the issue. Therefore, great
    care is taken to ensure the highest accuracy of the benchmarks. There is a lot
    of information available on this topic; if you really want to get to the heart
    of why MiniBatch k-means is better at scaling, it will be a good idea to review
    what's available.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这些指标的目的是为了揭示问题。因此，在确保基准的最高准确性方面非常小心。关于这个主题有很多信息可供参考；如果你真的想深入了解为什么MiniBatch
    k-means在扩展性方面更好，建议查看现有的资料。
- en: 'Now that the setup is complete, we can measure the time difference:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在设置完成了，我们可以测量时间差异：
- en: '[PRE22]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'There''s a large difference in CPU times. The difference in the clustering
    performance is shown as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: CPU时间上有很大的差异。聚类性能的差异如下所示：
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Look at the two arrays; the located centers are ordered the same. This is random—the
    clusters do not have to be ordered the same. Look at the distance between the
    first cluster centers:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看这两个数组；找到的中心点是按照相同顺序排列的。这是随机的—聚类不必按相同顺序排列。查看第一个聚类中心之间的距离：
- en: '[PRE24]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This seems to be very close. The diagonals will contain the cluster center
    differences:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这似乎非常接近。对角线将包含聚类中心的差异：
- en: '[PRE25]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: How it works...
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 工作原理...
- en: The batches here are key. Batches are iterated through to find the batch mean;
    for the next iteration, the prior batch mean is updated in relation to the current
    iteration. There are several options that dictate the general k-means behavior
    and parameters that determine how MiniBatch k-means gets updated.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的批次很关键。通过批次迭代以找到批次均值；对于下一次迭代，相对于当前迭代更新前一批次均值。有几个选项决定了一般k-means的行为和参数，这些参数决定了MiniBatch
    k-means的更新方式。
- en: 'The `batch_size` parameter determines how large the batches should be. Just
    for fun, let''s run MiniBatch; however, this time we set the batch size to be
    the same as the dataset size:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`batch_size`参数决定了批处理的大小。仅供娱乐，让我们来运行MiniBatch；但是这次我们将批处理大小设置为与数据集大小相同：'
- en: '[PRE26]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Clearly, this is against the spirit of the problem, but it does illustrate an
    important point. Choosing poor initial conditions can affect how well models,
    particularly clustering models, converge. With MiniBatch k-means, there is no
    guarantee that the global optimum will be achieved.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这违背了问题的本意，但它确实说明了一个重要的观点。选择不良的初始条件可能会影响模型的收敛效果，特别是聚类模型。对于MiniBatch k-means，不能保证会达到全局最优解。
- en: There are many powerful lessons in MiniBatch k-means. It uses the power of many
    random samples, similar to bootstrapping. When creating an algorithm for big data,
    you can use many random samples on many machines processing in parallel.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: MiniBatch k-means中有许多有力的教训。它利用了许多随机样本的力量，类似于自助法。在创建大数据的算法时，您可以在许多机器上并行处理许多随机样本。
- en: Quantizing an image with k-means clustering
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means聚类量化图像
- en: Image processing is an important topic in which clustering has some application.
    It's worth pointing out that there are several very good image processing libraries
    in Python. `scikit-image` is a sister project of scikit-learn. It's worth taking
    a look at if you want to do anything complicated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理是一个重要的话题，聚类在其中有一些应用。值得指出的是，Python中有几个非常好的图像处理库。`scikit-image`是scikit-learn的姊妹项目。如果您想要做一些复杂的事情，不妨看看它。
- en: A big point of this chapter is that images are data as well and clustering can
    be used to try to guess where some objects in an image are. Clustering can be
    part of an image processing pipeline.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的重点之一是图像也是数据，聚类可以用来尝试猜测图像中某些物体的位置。聚类可以是图像处理流程的一部分。
- en: Getting ready
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We will have some fun in this recipe. The goal is to use a cluster to blur an
    image. First, we'll make use of SciPy to read the image. The image is translated
    in a three-dimensional array; the `x` and `y` coordinates describe the height
    and width, and the third dimension represents the RGB values for each image.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中我们将会有一些乐趣。目标是使用一个聚类方法来对图像进行模糊处理。首先，我们将使用SciPy来读取图像。图像被转换为一个三维数组；`x`和`y`坐标描述了高度和宽度，第三维表示每个像素的RGB值。
- en: Begin by downloading or moving an `.jpg` image to the folder where your IPython
    notebook is located. You can use a picture of yours. I use a picture of myself
    named `headshot.jpg`.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，下载或将一个 `.jpg` 图像移动到你的 IPython 笔记本所在的文件夹。你可以使用你自己的照片。我使用的是一张名为 `headshot.jpg`
    的自己的照片。
- en: How do it…
  id: totrans-126
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 怎么做……
- en: 'Now, let''s read the image in Python:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们用 Python 读取图像：
- en: '[PRE27]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The following image is seen:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 看到以下图片：
- en: '![](img/e2fd4a5a-4121-49f4-9ccc-15c8cb68a495.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2fd4a5a-4121-49f4-9ccc-15c8cb68a495.png)'
- en: 'That''s me! Now that we have the image, let''s check its dimensions:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就是我！现在我们有了图像，让我们检查一下它的维度：
- en: '[PRE28]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: To actually quantize the image, we need to convert it into a two-dimensional
    array, with the length being 379 x 337 and the width being the RGB values. A better
    way to think about this is to have a bunch of data points in three-dimensional
    space and cluster the points to reduce the number of distant colors in the image—a
    simple way to do quantization.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 要实际对图像进行量化，我们需要将其转换为二维数组，长度为 379 x 337，宽度为 RGB 值。更好的理解方式是将其看作是三维空间中的一组数据点，接着对这些点进行聚类，以减少图像中远离的颜色——这是一种简单的量化方法。
- en: 'First, let''s reshape our array; it is a NumPy array, and thus simple to work
    with:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们重新整理一下我们的数组；它是一个 NumPy 数组，因此操作起来很简单：
- en: '[PRE29]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now we can start the clustering process. First, let''s import the cluster module
    and create a k-means object. We''ll pass `n_clusters=5` so that we have five clusters,
    or really, five distinct colors. This will be a good recipe to practice using
    silhouette distance, which we reviewed in the *Optimizing the number of centroids*
    recipe:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以开始聚类过程。首先，让我们导入聚类模块并创建一个 k-means 对象。我们将传递 `n_clusters=5`，这样我们就有五个聚类，或者说，五种不同的颜色。这将是一个很好的练习，帮助我们使用轮廓距离，这个在*优化质心数量*的练习中已经讲解过：
- en: '[PRE30]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: How it works…
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'Now that we have the centers, the next thing we need is the labels. This will
    tell us which points should be associated with which clusters:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了中心点，接下来我们需要的是标签。这将告诉我们哪些点应该与哪些聚类相关联：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'At this point, we require the simplest of NumPy array manipulation followed
    by a bit of reshaping, and we''ll have the new image:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，我们只需进行最简单的 NumPy 数组操作，然后稍作调整，就可以得到新的图像：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The following is the resultant image:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是结果图像：
- en: '![](img/eb03efc6-0bd1-4c20-9351-d62bdde53ca8.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/eb03efc6-0bd1-4c20-9351-d62bdde53ca8.png)'
- en: The clustering separated the image into a few regions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类将图像分成了几个区域。
- en: Finding the closest object in the feature space
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在特征空间中寻找最接近的对象
- en: Sometimes, the easiest thing to do is to find the distance between two objects.
    We just need to find some distance metric, compute the pairwise distances, and
    compare the outcomes with what is expected.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，最简单的方法是找出两个对象之间的距离。我们只需要找出一个距离度量，计算成对距离，并将结果与预期的进行比较。
- en: Getting ready
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: A lower level utility in scikit-learn is `sklearn.metrics.pairwise`. It contains
    server functions used to compute distances between vectors in a matrix X or between
    vectors in X and Y easily. This can be useful for information retrieval. For example,
    given a set of customers with attributes of X, we might want to take a reference
    customer and find the closest customers to this customer.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 中的一个底层工具是 `sklearn.metrics.pairwise`。它包含了用于计算矩阵 X 中向量之间或 X 和 Y
    中向量之间距离的服务器函数。这对于信息检索很有帮助。例如，给定一组具有属性 X 的客户，我们可能想选择一个参考客户，找到与该客户最相似的客户。
- en: In fact, we might want to rank customers by the notion of similarity measured
    by a distance function. The quality of similarity depends upon the feature space
    selection as well as any transformation we might do on the space. We'll walk through
    several different scenarios of measuring distance.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们可能想根据相似度来对客户进行排名，而这种相似度是通过距离函数来衡量的。相似度的质量取决于特征空间的选择以及我们对空间进行的任何变换。我们将通过几种不同的距离衡量场景来进行讲解。
- en: How to do it...
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做……
- en: 'We will use the `pairwise_distances` function to determine the closeness of
    objects. Remember that the closeness is just similarity, which we grade using
    our distance function:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 `pairwise_distances` 函数来确定对象之间的相似性。请记住，相似性本质上就是通过我们定义的距离函数来衡量的：
- en: 'First, let''s import the pairwise distance function from the metrics module
    and create a dataset to play with:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们从 metrics 模块中导入成对距离函数，并创建一个数据集进行操作：
- en: '[PRE33]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The simplest way to check the distances is `pairwise_distances`:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查距离的最简单方法是 `pairwise_distances`：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '`distances` is an N x N matrix with 0s along the diagonals. In the simplest
    case, let''s see the distances between each point and the first point:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`distances`是一个N x N矩阵，对角线上的值为0。在最简单的情况下，让我们查看每个点与第一个点之间的距离：'
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Ranking the points by closeness is very easy with np.argsort:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照接近程度对点进行排名非常简单，使用np.argsort：
- en: '[PRE36]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The great thing about `argsort` is that now we can sort our `points` matrix
    to get the actual points:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`argsort`的一个优点是，现在我们可以对`points`矩阵进行排序，从而获得实际的点：'
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: It's useful to see what the closest points look like as follows. The chosen
    point, points [0], is colored green. The closest points are colored red (except
    for the chosen point).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 看到最接近的点是什么样子是很有用的，如下所示。选定的点[0]被涂成绿色。最接近的点被涂成红色（除了选定的点）。
- en: 'Note that other than some assurances, this works as intended:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，除了某些保证之外，这个过程按预期工作：
- en: '[PRE38]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '![](img/99cb8345-f0b7-419b-be50-5e78e4f0da9d.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99cb8345-f0b7-419b-be50-5e78e4f0da9d.png)'
- en: How it works...
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Given some distance function, each point is measured in a pairwise function.
    Consider two points represented as vectors in N-dimensional space with components
    *p[i]* and *q[i]*; the default is the Euclidean distance, which is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 给定某个距离函数，每个点通过成对的函数进行测量。考虑两个点，表示为N维空间中的向量，具有分量*p[i]*和*q[i]*；默认情况下是欧几里得距离，其公式如下：
- en: '![](img/015397b7-7363-4a0b-b2f5-b324b967fe94.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/015397b7-7363-4a0b-b2f5-b324b967fe94.png)'
- en: Verbally, this takes the difference between each component of the two vectors,
    squares these differences, sums them all, and then finds the square root. This
    looks very familiar as we used something very similar when looking at the mean
    squared error. If we take the square root, we have the same thing. In fact, a
    metric used often is root mean square deviation (RMSE), which is just the applied
    distance function.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 口头上，这个过程是计算两个向量每个分量之间的差值，平方这些差异，求和所有差异，然后求平方根。这个过程看起来非常熟悉，因为我们在研究均方误差时用过类似的东西。如果我们取平方根，我们就得到了相同的结果。事实上，常用的一个度量是均方根偏差（RMSE），它就是应用的距离函数。
- en: 'In Python, this looks like the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，这看起来如下所示：
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'There are several other functions available in scikit-learn, but scikit-learn
    will also use distance functions of SciPy. At the time of writing this book, the
    scikit-learn distance functions support sparse matrixes. Check out the SciPy documentation
    for more information on the distance functions:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中还有其他一些函数，但scikit-learn还将使用SciPy的距离函数。在写这本书时，scikit-learn的距离函数支持稀疏矩阵。有关距离函数的更多信息，请查阅SciPy文档：
- en: '`cityblock`'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cityblock`'
- en: '`cosine`'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cosine`'
- en: '`euclidean`'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`euclidean`'
- en: '`l1`'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1`'
- en: '`l2`'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2`'
- en: '`manhattan`'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`manhattan`'
- en: We can now solve problems. For example, if we were standing on a grid at the
    origin and the lines were the streets, how far would we have to travel to get
    to point *(5, 5)*?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以解决问题了。例如，如果我们站在网格的原点上，而这些线是街道，我们需要走多远才能到达点*(5, 5)*？
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: There's more...
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Using pairwise distances, we can find the similarity between bit vectors. For
    N-dimensional vectors *p* and *q*, it''s a matter of finding the hamming distance,
    which is defined as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 使用成对距离，我们可以找到比特向量之间的相似度。对于N维向量*p*和*q*，问题就变成了计算汉明距离，其定义如下：
- en: '![](img/afd3edd0-ade2-460c-9b74-0916c44bb4ae.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afd3edd0-ade2-460c-9b74-0916c44bb4ae.png)'
- en: 'Use the following command:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令：
- en: '[PRE41]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Note that scikit-learn's `hamming` metric returns the hamming distance divided
    by the length of the vectors, `4` in this case.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，scikit-learn的`hamming`度量返回的是汉明距离除以向量的长度，在此情况下为`4`。
- en: Probabilistic clustering with Gaussian mixture models
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用高斯混合模型进行概率聚类
- en: In k-means, we assume that the variance of the clusters is equal. This leads
    to a subdivision of space that determines how the clusters are assigned; but what
    about a situation where the variances are not equal and each cluster point has
    some probabilistic association with it?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means中，我们假设簇的方差是相等的。这导致了一种空间的划分，决定了簇的分配方式；但是，如果方差不相等，并且每个簇点与其有某种概率关联，应该如何处理呢？
- en: Getting ready
  id: totrans-190
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: There's a more probabilistic way of looking at k-means clustering. Hard k-means
    clustering is the same as applying a Gaussian mixture model with a covariance
    matrix, `S`, which can be factored to the error times of the identity matrix.
    This is the same covariance structure for each cluster. It leads to spherical
    clusters. However, if we allow S to vary, a GMM can be estimated and used for
    prediction. We'll look at how this works in a univariate sense and then expand
    to more dimensions.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种更具概率性的方式来看待 k-means 聚类。硬性 k-means 聚类等同于应用一个具有协方差矩阵 `S` 的高斯混合模型（GMM），其中协方差矩阵
    `S` 可以分解为误差和单位矩阵的乘积。这对于每个聚类来说都是相同的协方差结构，导致聚类呈球形。然而，如果我们允许 S 变化，则可以估计 GMM 并用于预测。我们将首先在一维空间中看看这种方法如何运作，然后扩展到更多维度。
- en: How to do it...
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'First, we need to create some data. For example, let''s simulate heights of
    both women and men. We''ll use this example throughout this recipe. It''s a simple
    example, but hopefully it will illustrate what we''re trying to accomplish in
    an N-dimensional space, which is a little easier to visualize:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要创建一些数据。例如，我们可以模拟女性和男性的身高。在整个例子中，我们将使用这个例子。它是一个简单的示例，但希望它能够展示我们在 N 维空间中试图实现的目标，这样稍微更容易进行可视化：
- en: '[PRE42]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'This is the output:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '![](img/4b0ba254-044d-48b7-9e75-3e3e53127334.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b0ba254-044d-48b7-9e75-3e3e53127334.png)'
- en: 'Next, we might be interested in subsampling the group, fitting the distribution,
    and then predicting the remaining groups:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可能有兴趣对子集进行抽样，拟合分布，然后预测剩余的组：
- en: '[PRE43]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Now we need to get the empirical distribution of the heights of both men and
    women based on the training set:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们需要根据训练集获取男性和女性身高的经验分布：
- en: '[PRE44]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: For the test set, we will calculate based on the likelihood that the data point
    was generated from either distribution, and the most likely distribution will
    get the appropriate label assigned.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 对于测试集，我们将根据数据点来自某个分布的似然度进行计算，最有可能的分布将被分配到相应的标签。
- en: 'We will, of course, look at how accurate we were:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们当然会查看我们的准确度：
- en: '[PRE45]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Notice the difference in likelihoods. Assume that we guess situations when
    the men''s probability is higher, but we overwrite them if the women''s probability
    is higher:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意似然度的差异。假设我们在男性概率较高时进行猜测，但如果女性的概率更高时，我们会覆盖这些猜测：
- en: '[PRE46]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Obviously, the question is how accurate we are. Since `guesses_m` will be *1*
    if we are correct and *0* if we aren''t, we take the mean of the vector and get
    the accuracy:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显然，问题是我们有多准确。由于如果我们猜对了，`guesses_m` 将是 *1*，如果猜错了，则为 *0*，我们可以取向量的均值来计算准确度：
- en: '[PRE47]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Not too bad! Now, to see how well we did with the women''s group, we use the
    following commands:'
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还不错！现在，为了查看我们在女性组上的表现，我们使用以下命令：
- en: '[PRE48]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Let''s allow the variance to differ between groups. First, create some new
    data:'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们允许不同组之间的方差有所不同。首先，创建一些新的数据：
- en: '[PRE49]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Then, create a training set:'
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，创建一个训练集：
- en: '[PRE50]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '![](img/2ae342ce-0c92-4060-a6de-dd66b8de5179.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2ae342ce-0c92-4060-a6de-dd66b8de5179.png)'
- en: 'Now we can create the same PDFs:'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以创建相同的概率密度函数（PDFs）：
- en: '[PRE51]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The following is the output:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/6f93504d-a8a5-4578-8ba0-0f545236ba50.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f93504d-a8a5-4578-8ba0-0f545236ba50.png)'
- en: 'You can imagine this in a multidimensional space:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在一个多维空间中想象这个过程：
- en: '[PRE52]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '![](img/550cf069-4940-4f13-9035-935e2ae9d0bb.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/550cf069-4940-4f13-9035-935e2ae9d0bb.png)'
- en: How it works...
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: 'Okay, so now that we''ve looked at how we can classify points based on distribution,
    let''s look at how we can do this in scikit-learn:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们已经看过如何根据分布来分类数据点，接下来我们看看如何在 scikit-learn 中实现这一点：
- en: '[PRE53]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Since we''re conscientious data scientists, we''ll create a training set:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们是认真的数据科学家，我们将创建一个训练集：
- en: '[PRE54]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Fitting and predicting is done in the same way as fitting is done for many
    of the other objects in scikit-learn:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 拟合和预测的方式与在 scikit-learn 中拟合其他对象时相同：
- en: '[PRE55]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: There are other methods worth looking at now that the model has been fit. For
    example, using `score_samples`, we can actually get the per-sample likelihood
    for each label.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型拟合之后，还有其他方法值得关注。例如，使用 `score_samples`，我们可以获得每个标签的每个样本的似然度。
- en: Using k-means for outlier detection
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 k-means 进行异常值检测
- en: In this recipe, we'll look at both the debate and mechanics of k-means for outlier
    detection. It can be useful to isolate some types of errors, but care should be
    taken when using it.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将讨论 k-means 在异常值检测中的辩论和机制。它在隔离某些类型的错误时可能有用，但使用时需要小心。
- en: Getting ready
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: We'll use k-means to do outlier detection on a cluster of points. It's important
    to note that there are many camps when it comes to outliers and outlier detection.
    On one hand, we're potentially removing points that were generated by the data-generating
    process by removing outliers. On the other hand, outliers can be due to a measurement
    error or some other outside factor.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用k-means对一组点进行异常值检测。值得注意的是，在异常值和异常值检测方面有许多不同的观点。一方面，我们可能通过移除异常值去除了一些由数据生成过程产生的点；另一方面，异常值可能是由于测量误差或其他外部因素导致的。
- en: This is the most credence we'll give to the debate. The rest of this recipe
    is about finding outliers; we'll work under the assumption that our choice to
    remove outliers is justified. The act of outlier detection is a matter of finding
    the centroids of the clusters and then identifying points that are potential outliers
    by their distances from the centroid.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们对这个辩论给出的最大信任。接下来的部分将关注识别异常值；我们将假设移除异常值的选择是合理的。异常值检测的过程是找到簇的质心，然后通过计算与质心的距离来识别潜在的异常值点。
- en: How to do it...
  id: totrans-235
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何操作……
- en: 'First, we''ll generate a single blob of 100 points, and then we''ll identify
    the five points that are furthest from the centroid. These are the potential outliers:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将生成一个包含100个点的单一簇，然后识别出距离质心最远的五个点。这些点可能是异常值：
- en: '[PRE56]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'It''s important that the k-means cluster has a single center. This idea is
    similar to a one-class SVM that is used for outlier detection:'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: k-means聚类必须具有一个单一的质心，这是很重要的。这个概念类似于用于异常值检测的单类SVM：
- en: '[PRE57]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, let''s look at the plot. Those playing along at home, try to guess which
    points will be identified as one of the five outliers:'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看这个图。正在家里一起操作的朋友们，试着猜猜哪些点会被识别为五个异常值之一：
- en: '[PRE58]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The following is the output:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/327feae1-f9d0-414f-9203-c236df182253.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](img/327feae1-f9d0-414f-9203-c236df182253.png)'
- en: 'Now, let''s identify the five closest points:'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们识别出最接近的五个点：
- en: '[PRE59]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let''s see which plots are the farthest away:'
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看哪些点离得最远：
- en: '[PRE60]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The following is the output:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/2912168d-0c4d-4153-8eeb-df2038428a22.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2912168d-0c4d-4153-8eeb-df2038428a22.png)'
- en: 'It''s easy to remove these points if we like:'
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们愿意，去除这些点是很简单的：
- en: '[PRE61]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Also, the centroid clearly changes with the removal of these points:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着这些点的移除，质心明显发生了变化：
- en: '[PRE62]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Let''s visualize the difference between the old and new centroids:'
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们可视化一下旧质心和新质心之间的差异：
- en: '[PRE63]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The following is the output:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是输出结果：
- en: '![](img/497f4845-bb5c-42b0-9a6a-67cc2ae16000.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](img/497f4845-bb5c-42b0-9a6a-67cc2ae16000.png)'
- en: Clearly, the centroid hasn't moved much, which is to be expected only when removing
    the five most extreme values. This process can be repeated until we're satisfied
    that the data is representative of the process.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，质心几乎没有移动，这在移除五个最极端的值时是可以预期的。这个过程可以重复进行，直到我们满意数据能代表这个过程。
- en: How it works...
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的……
- en: 'As we''ve already seen, there is a fundamental connection between the Gaussian
    distribution and the k-means clustering. Let''s create an empirical Gaussian based
    on the centroid and sample covariance matrix and look at the probability of each
    point—theoretically, the five points we removed. This just shows that we have,
    in fact, removed the values with the least likelihood. This idea between distances
    and likelihoods is very important and will come around quite often in your machine
    learning training. Use the following command to create an empirical Gaussian:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经看到的，高斯分布和k-means聚类之间有着根本的联系。让我们基于质心和样本协方差矩阵创建一个经验高斯分布，并查看每个点的概率——理论上，这就是我们移除的那五个点。这实际上表明我们已经移除了最不可能出现的值。距离和可能性之间的这种关系非常重要，在你的机器学习训练中会经常遇到。使用以下命令创建经验高斯分布：
- en: '[PRE64]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Using KNN for regression
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用KNN进行回归
- en: Regression is covered elsewhere in the book, but we might also want to run a
    regression on pockets of the feature space. We can think that our dataset is subject
    to several data processes. If this is true, only training on similar data points
    is a good idea.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 回归在本书的其他部分已经涉及，但我们也可能想对特征空间中的小范围进行回归。我们可以认为我们的数据集受到多个数据处理过程的影响。如果这是真的，只有在相似的数据点上进行训练才是明智的选择。
- en: Getting ready
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: Our old friend, regression, can be used in the context of clustering. Regression
    is obviously a supervised technique, so we'll use **K-Nearest Neighbors** (**KNN**)
    clustering rather than k-means. For KNN regression, we'll use the K closest points
    in the feature space to build the regression rather than using the entire space
    as in regular regression.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的老朋友——回归，可以在聚类的背景下使用。回归显然是一种监督学习技术，因此我们将使用**K-最近邻**（**KNN**）聚类，而不是K均值聚类。对于KNN回归，我们将使用特征空间中离测试点最近的K个点来构建回归模型，而不是像常规回归那样使用整个特征空间。
- en: How to do it…
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何实现…
- en: 'For this recipe, we''ll use the `iris` dataset. If we want to predict something
    such as the petal width for each flower, clustering by iris species can potentially
    give us better results. The KNN regression won''t cluster by the species, but
    we''ll work under the assumption that the Xs will be close for the same species,
    in this case, the petal length:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用`iris`数据集。如果我们想预测每朵花的花瓣宽度，通过鸢尾花种类进行聚类可能会为我们带来更好的结果。KNN回归不会根据种类进行聚类，但我们会假设相同种类的样本在特征空间中会较为接近，在这个例子中，即花瓣长度：
- en: 'We''ll use the `iris` dataset for this recipe:'
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`iris`数据集进行这个示例：
- en: '[PRE65]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'We''ll try to predict the petal length based on the sepal length and width.
    We''ll also fit a regular linear regression to see how well the KNN regression
    does in comparison:'
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将尝试基于萼片的长度和宽度预测花瓣长度。我们还将拟合一个常规的线性回归，看看KNN回归与它相比表现如何：
- en: '[PRE66]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, for the KNN regression, use the following code:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，对于KNN回归，使用以下代码：
- en: '[PRE67]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Let''s look at what the KNN regression does when we tell it to use the 10 closest
    points for regression:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看当我们让KNN回归使用离测试点最近的10个点时，结果会是怎样：
- en: '[PRE68]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '![](img/8fd74776-d305-48be-bb6d-549028459772.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8fd74776-d305-48be-bb6d-549028459772.png)'
- en: 'It might be completely clear that the predictions are close for the most part,
    but let''s look at the predictions for the Setosa species as compared to the actuals:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测结果大部分是接近的，这可能是显而易见的，但让我们看看Setosa种类的预测结果与实际值的对比：
- en: '[PRE69]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: Looking at the plots again, we see that the setosa species (upper-left cluster)
    is largely overestimated by linear regression, and KNN is fairly close to the
    actual values.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次查看图表，我们看到Setosa种类（左上角的聚类）在常规线性回归中被大大高估，而KNN则非常接近实际值。
- en: How it works..
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的..
- en: 'KNN regression is very simple to calculate by taking the average of the *K*
    closest points to the point being tested. Let''s manually predict a single point:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: KNN回归非常简单，通过取离测试点最近的*K*个点的平均值来计算。让我们手动预测一个点：
- en: '[PRE70]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Now, we need to get the 10 closest points to our `example_point`:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要找到离我们的`example_point`最近的10个点：
- en: '[PRE71]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: We can see that this is very close to what was expected.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，结果非常接近预期。
