- en: Learning to Classify and Localize Objects
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习如何分类和定位物体
- en: So far, we have studied a range of algorithms and approaches where you have
    learned how to solve real-world problems with the help of computer vision. In
    recent years, in parallel with the considerable hardware computational power that
    is provided with devices such as **Graphical Processing Units **(**GPUs**), a
    lot of algorithms arose that utilized this power and achieved state-of-the-art
    results in computer vision tasks. Usually, these algorithms are based on neural
    networks, which enable the creator of the algorithm to squeeze quite a lot of
    meaningful information from data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了一系列算法和方法，你学习了如何借助计算机视觉解决现实世界的问题。近年来，随着**图形处理单元**（**GPU**）等设备提供的强大硬件计算能力的出现，许多算法应运而生，这些算法利用了这种能力，在计算机视觉任务中实现了最先进的结果。通常，这些算法基于神经网络，这使得算法的创造者能够从数据中提取大量有意义的信息。
- en: Meanwhile, in contrast to the classical approaches, this information is often
    quite hard to interpret. From that point of view, you might say that we are getting
    closer to artificial intelligence—that is, we are giving a computer an approach
    and it figures out how to do the rest. In order for all of this to not appear
    so mysterious, let's learn about deep learning models in this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，与经典方法相比，这些信息通常很难解释。从这个角度来看，你可能会说我们正在接近人工智能——也就是说，我们正在给计算机一个方法，然后它自己找出如何完成剩下的工作。为了不让这一切显得如此神秘，让我们在本章中学习关于深度学习模型的知识。
- en: As you have already seen, a few of the classical problems in computer vision
    include object detection and localization. Let's look at how to classify and localize
    objects with the help of deep learning models in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你已经看到的，计算机视觉中的一些经典问题包括目标检测和定位。让我们看看在本章中如何使用深度学习模型来分类和定位物体。
- en: 'The goal of this chapter is to learn important deep learning concepts such
    as transfer learning and how to apply them to build your own object classifier
    and localizer. Specifically, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是学习重要的深度学习概念，如迁移学习，以及如何将它们应用到构建你自己的物体分类器和定位器中。具体来说，我们将涵盖以下主题：
- en: Preparing a large dataset for training a deep learning model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于训练深度学习模型的大型数据集
- en: Understanding **Convolutional Neural Networks** (**CNNs**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解**卷积神经网络**（**CNNs**）
- en: Classifying and localizing with CNNs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络进行分类和定位
- en: Learning about transfer learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习迁移学习
- en: Implementing activation functions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现激活函数
- en: Understanding backpropagation
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: We will start by preparing a dataset for training. Then, we will go on to understand
    how to use a pretrained model for creating a new classifier. Once you have understood
    how it is done, we will move forward and build more complex architectures that
    will perform localization.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先准备一个用于训练的数据集。然后，我们将继续了解如何使用预训练模型来创建一个新的分类器。一旦你明白了这个过程，我们将继续前进，构建更复杂的架构，这些架构将执行定位。
- en: During these steps, we will use the **Oxford-IIIT-Pet** dataset. Finally, we
    will run the app that will use our trained localizer network for inference. Although
    the network will be trained only using the bounding boxes of the heads of pets,
    you will see that it will also be good for localization of the human head position.
    The latter will show the power of generalization of our model.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些步骤中，我们将使用**牛津-IIIT-Pet**数据集。最后，我们将运行一个应用，该应用将使用我们训练好的定位网络进行推理。尽管这个网络仅使用宠物的头部边界框进行训练，但你将看到它对于定位人类头部位置也非常有效。后者将展示我们模型泛化的能力。
- en: Learning about these concepts of deep learning and seeing them in action will
    be very useful in the future when you make your own applications using deep learning
    models or when you start to work on completely new deep learning architectures.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 了解深度学习的这些概念并在实际应用中看到它们的效果，在你使用深度学习模型创建自己的应用或开始研究全新的深度学习架构时，将会非常有用。
- en: Getting started
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习
- en: As we have mentioned in all of the chapters of this book, you will need to have
    OpenCV installed. Besides that, you will need to install TensorFlow.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在这本书的所有章节中提到的，你需要安装OpenCV。除此之外，你还需要安装TensorFlow。
- en: The Oxford-IIIT-Pet dataset is available for download at [https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/), along
    with our dataset preparation script, which will be downloaded automatically for
    you.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 牛津-IIIT-Pet数据集可在[https://www.robots.ox.ac.uk/~vgg/data/pets/](https://www.robots.ox.ac.uk/~vgg/data/pets/)下载，以及我们的数据集准备脚本，该脚本将自动为您下载。
- en: You can find the code that we present in this chapter (from the GitHub repository)
    at [https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition). You
    can also use the Docker files available in the repository to run the code in the
    chapter. Refer to the [Appendix B](c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml),
    *Setting Up a Docker Container*, for more information on the Docker files.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在本章节（GitHub仓库）中找到我们提供的代码（[https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition](https://github.com/PacktPublishing/OpenCV-4-with-Python-Blueprints-Second-Edition)）。您还可以使用仓库中可用的Docker文件在本章中运行代码。有关Docker文件的更多信息，请参阅[附录B](c86bca68-4b4a-4be6-8edd-67b1d43f0bfa.xhtml)，*设置Docker容器*。
- en: Planning the app
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 规划应用程序
- en: 'The final app will consist of modules to prepare the dataset, train the models,
    and run an inference with the models using input from your camera. This will require
    the following components:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的应用程序将包括准备数据集、训练模型以及使用来自相机的输入运行模型推理的模块。这需要以下组件：
- en: '`main.py`: This is the main script for starting the application and localizing
    the head (of the pets) in real time.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`main.py`：这是启动应用程序并在实时中定位宠物头部的主要脚本。'
- en: '`data.py`: This is a module to download and prepare the dataset for training.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`data.py`：这是一个模块，用于下载和准备训练数据集。'
- en: '`classification.py`: This is a script to train a classifier network.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`classification.py`：这是一个用于训练分类网络的脚本。'
- en: '`localization.py`: This is a script to train and save a localization network.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`localization.py`：这是一个用于训练和保存定位网络的脚本。'
- en: 'After preparing the dataset for training, we will do the following to complete
    our app:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备训练数据集后，我们将执行以下操作以完成我们的应用：
- en: We will first train a classification network using transfer learning.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先使用迁移学习训练一个分类网络。
- en: Next, we will train an object localization network, again using transfer learning.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用迁移学习再次训练一个目标定位网络。
- en: After we create and train our localization network, we will run our `main.py`
    script to localize the heads in real time.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们创建和训练定位网络后，我们将运行`main.py`脚本以实时定位头部。
- en: Let's start by learning how to prepare the inference script that will run our
    app. The script will connect to your camera, find a head position in each frame
    of the video stream using the localization model that we will create, and then
    illustrate the results in real time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从学习如何准备将要运行我们应用的推理脚本开始。该脚本将连接到您的相机，使用我们将创建的定位模型在视频流的每一帧中找到头部位置，并在实时中展示结果。
- en: Preparing an inference script
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备推理脚本
- en: 'Our inference script is quite simple. It will first prepare a drawing function,
    then load the model and connect it to the camera. Then, it will loop over the
    frames from the video stream. In the loop, for each frame of the stream, it will
    use the imported model to make an inference and the drawing function to display
    the results. Let''s create a complete script using the following steps:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的推理脚本相当简单。它将首先准备一个绘图函数，然后加载模型并将其连接到相机。然后，它将遍历视频流的帧。在循环中，对于流中的每一帧，它将使用导入的模型进行推理，并使用绘图函数显示结果。让我们按照以下步骤创建一个完整的脚本：
- en: 'First, we import the required modules:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的模块：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code, besides importing NumPy and OpenCV, we have also imported **Keras**.
    We are going to use Keras to make predictions in this script; additionally, we
    will use it to create and train our models throughout the chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，除了导入NumPy和OpenCV外，我们还导入了**Keras**。我们将在此脚本中使用Keras进行预测；此外，我们将在本章中用它来创建和训练我们的模型。
- en: 'Then, we define a function to draw localization bounding boxes on a frame:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们定义一个函数在帧上绘制定位边界框：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The preceding `draw_box` function accepts `frame` and the normalized coordinates
    of the two corners of a bounding box as an array of four numbers. The function
    first reshapes the one-dimensional array of the box into a two-dimensional array,
    where the first index represents the point and the second represents the *x* and
    *y* coordinates. Then, it transforms the normalized coordinates to the coordinates
    of the image by multiplying them with an array composed of the width and height
    of the image and translates the result into integer values in the same line. Finally,
    it draws the bounding box with the color green using the `cv2.rectangle` function
    and returns `frame`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的 `draw_box` 函数接受 `frame` 和一个边界框两个角的归一化坐标作为四个数字的数组。该函数首先将框的一维数组重塑为二维数组，其中第一个索引表示点，第二个索引表示
    *x* 和 *y* 坐标。然后，它通过乘以一个由图像宽度和高度组成的数组将归一化坐标转换为图像坐标，并将结果转换为同一行的整数值。最后，它使用 `cv2.rectangle`
    函数以绿色绘制边界框，并返回 `frame`。
- en: 'Then, we import the model that we will prepare throughout the chapter and connect
    to the camera:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将导入本章中准备好的模型并将其连接到摄像头：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`model` will be stored in a binary file, which is imported using a convenient
    function from Keras.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '`model` 将存储在一个二进制文件中，可以使用 Keras 中的一个方便的函数来导入。'
- en: 'After that, we iterate over the frames from the camera, resize each `frame`
    to a standard size (that is, the default image size for the models that we will
    create), and convert `frame` to the **RGB** (**red**, **green**, **blue**) color
    space as we will train our models on RGB images:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们遍历来自摄像头的帧，将每个 `frame` 调整到标准大小（即我们将创建的模型的默认图像大小），并将 `frame` 转换为 **RGB**（**红色**、**绿色**、**蓝色**）颜色空间，因为我们将在
    RGB 图像上训练我们的模型：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the same loop, we normalize the image and add one to the shape of the frame
    as the model accepts batches of images. Then, we pass the result to `model` for
    inference:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在相同的循环中，我们将图像归一化，并将一个添加到帧的形状中，因为模型接受图像批次。然后，我们将结果传递给 `model` 进行推理：
- en: '[PRE4]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We continue the loop by drawing the predicted bounding box using the previously
    defined function, show the results, and then set the termination criteria:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过使用先前定义的函数绘制预测的边界框、显示结果，然后设置终止条件来继续循环：
- en: '[PRE5]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we have our inference script ready, let's start the journey of creating
    our model by first preparing the dataset in the next section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了推理脚本，让我们开始创建我们自己的模型的旅程，首先在下一节中准备数据集。
- en: Preparing the dataset
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'As mentioned previously, in this chapter, we are going to use the Oxford-IIIT-Pet
    dataset. It will be a good idea to encapsulate the preparation of the dataset
    in a separate `data.py` script, which can then be used throughout the chapter.
    As with any other script, first of all, we have to import all the required modules,
    as shown in the following code snippet:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在本章中，我们将使用牛津-IIIT-Pet 数据集。将数据集的准备封装在一个单独的 `data.py` 脚本中是一个好主意，这样就可以在本章中重复使用。与任何其他脚本一样，首先，我们必须导入所有必需的模块，如下面的代码片段所示：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In order to prepare our dataset for use, we will first download and parse the
    dataset into memory. Then, out of the parsed data, we will create a TensorFlow
    dataset, which allows us to work with a dataset in a convenient manner as well
    as prepare the data in the background so that the preparation of the data does
    not interrupt the neural network training process. So, let's move on to download
    and parse the dataset in the next section.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了准备我们的数据集以供使用，我们首先将数据集下载并解析到内存中。然后，从解析的数据中，我们将创建一个 TensorFlow 数据集，这使我们能够以方便的方式处理数据集，并在后台准备数据，以便数据准备不会中断神经网络训练过程。因此，让我们继续在下一节中下载和解析数据集。
- en: Downloading and parsing the dataset
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 下载并解析数据集
- en: 'In this section, we first download the dataset from the official website and
    then parse it into a convenient format. During this stage, we will leave out the
    images, which occupy quite a lot of memory. We cover this procedure in the following
    steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们首先从官方网站下载数据集，然后将其解析为方便的格式。在这个阶段，我们将省略图像，因为它们占用相当多的内存。我们将在以下步骤中介绍这个程序：
- en: 'Define where we want to store our pets dataset and download it using a convenient `get_file` function
    in Keras:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们想要存储宠物数据集的位置，并使用 Keras 中的方便的 `get_file` 函数下载它：
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As our dataset resides in an archive, we have also extracted it by passing `untar=True`.
    We also pointed `cache_dir` to the current directory. Once the files are saved,
    consequent executions of the `get_file` function will result in no action.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集位于存档中，我们还通过传递`untar=True`提取了它。我们还把`cache_dir`指向当前目录。一旦文件保存，`get_file`函数的后续执行将不会采取任何行动。
- en: The dataset weighs more than half a gigabyte, and, on the first run, you will
    need a stable internet connection with good bandwidth.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集超过半吉字节，在第一次运行时，您需要一个稳定且带宽良好的互联网连接。
- en: 'Once we have downloaded and extracted our dataset, let''s define constants
    for the dataset and annotation folders and set the image size that we want to
    resize our images to:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并提取我们的数据集后，让我们定义数据集和注释文件夹的常量，并设置我们想要将图像调整大小到的大小：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Size `224` is often the default size on which image classification networks
    are trained. Hence, it's a good idea to keep to that size for better accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 大小`224`通常是图像分类网络训练的默认大小。因此，保持该大小可以获得更好的准确性。
- en: 'Annotations of this dataset contain information about the image in XML format.
    Before parsing the XML, let''s first define what data we want to have:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该数据集的注释包含有关图像的XML格式信息。在解析XML之前，让我们首先定义我们想要的数据：
- en: '[PRE9]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`namedtuple` is an extension of a standard tuple in Python and allows you to
    refer to an element of a tuple by its name. The names that we have defined correspond
    to the data elements that we are interested in. Namely, those are the image itself
    (`image`), the head bounding box of the pet ( `box`), the image size, `type` (cat
    or dog), and `breed` (there are 37 breeds).'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '`namedtuple`是Python中标准元组的扩展，允许您通过名称引用元组中的元素。我们定义的名称对应于我们感兴趣的数据元素。具体来说，这些是图像本身（`image`）、宠物的头部边界框（`box`）、图像大小、`type`（猫或狗）和`breed`（共有37个品种）。'
- en: '`breeds` and `types` are strings in the annotation; what we want are numbers
    corresponding to `breeds`. For that purpose, we define two dictionaries:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`breeds`和`types`在注释中是字符串；我们想要的是与`breed`对应的数字。为此，我们定义了两个字典：'
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`defaultdict` is a dictionary that returns default values for the undefined
    keys. Here, it will return the next number starting from zero when requested.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '`defaultdict`是一个返回默认值的字典。在这里，当请求时，它将从零开始返回下一个数字。'
- en: 'Next, we define a function that, given a path to an XML file, will return an
    instance of our data:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个函数，给定一个XML文件的路径，将返回我们的数据实例：
- en: '[PRE11]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The previously defined function covers the following steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 之前定义的函数涵盖了以下步骤：
- en: 'Open the XML file and parse it:'
  id: totrans-69
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开XML文件并解析它：
- en: '[PRE12]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The contents of the XML file are parsed using the `ElementTree` module, which
    represents the XML in a format that is convenient to navigate through.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: XML文件的内容使用`ElementTree`模块解析，该模块以方便导航的格式表示XML。
- en: 'Then, get the name of the corresponding image and extract the name of the breed:'
  id: totrans-72
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，获取对应图像的名称并提取品种名称：
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After that, convert the breed to a number using `breeds` that was previously
    defined, which assigns the next number for each undefined key:'
  id: totrans-74
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，使用之前定义的`breeds`将品种转换为数字，它为每个未定义的键分配下一个数字：
- en: '[PRE14]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Similarly, get the ID of `types`:'
  id: totrans-76
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 同样，获取`types`的ID：
- en: '[PRE15]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Then, extract the bounding box and normalize it:'
  id: totrans-78
  prefs:
  - PREF_UL
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，提取边界框并归一化它：
- en: '[PRE16]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Return the results as an instance of `Data`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 将结果作为`Data`实例返回：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that we have downloaded the dataset and prepared a parser, let''s go on
    to parse the dataset:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经下载了数据集并准备了解析器，让我们继续解析数据集：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have also sorted the paths so that they appear in the same order in different
    runtime environments.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还排序了路径，以便在不同的运行环境中以相同的顺序出现。
- en: 'As we have parsed our dataset, we might want to print out available breeds
    and types for illustration:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经解析了我们的数据集，我们可能想打印出可用的品种和类型以供说明：
- en: '[PRE19]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The previous code snippet outputs two types, namely `cat` and `dog`, and their `breeds`:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码片段输出了两种类型，即`cat`和`dog`及其`breed`：
- en: '[PRE20]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Later on in this chapter, we will have to split the dataset on training and
    test sets. In order to perform a good split, we should randomly pick data elements
    from the dataset in order to have a proportional number of `breeds` in the train
    and test sets.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将不得不将数据集分为训练集和测试集。为了进行良好的分割，我们应该从数据集中随机选择数据元素，以便在训练集和测试集中有比例的`breed`数量。
- en: 'We can mix the dataset now so that we don''t have to worry about it later,
    as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以混合数据集，这样我们就不必担心以后的问题，如下所示：
- en: '[PRE21]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The previous code first sets a random seed, which is required to get the same
    result every time we execute the code. The `seed` method accepts one argument,
    which is a number specifying a random sequence.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码首先设置了一个随机种子，这是每次执行代码时获得相同结果所必需的。`seed`方法接受一个参数，即指定随机序列的数字。
- en: Once the `seed` method is set, we have the same sequence of random numbers in
    functions that use random numbers. Such numbers are called **pseudorandom**. This
    means that, although they look random, they are predefined. In our case, we use
    the `shuffle` method, which mixes the order of elements in the `parsed` array.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了`seed`方法，在所有使用随机数的函数中，我们都会有相同的随机数序列。这些数字被称为**伪随机数**。这意味着，尽管它们看起来是随机的，但它们是预先定义的。在我们的例子中，我们使用`shuffle`方法，它打乱了`parsed`数组中元素的位置。
- en: Now that we have parsed our dataset into a convenient NumPy array, let's move
    on and create a TensorFlow dataset out of it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将数据集解析为方便的NumPy数组，让我们继续并创建一个TensorFlow数据集。
- en: Creating a TensorFlow dataset
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建TensorFlow数据集
- en: We are going to use the TensorFlow dataset adapter in order to train our models.
    Of course, we could create a NumPy array from our dataset, but imagine how much
    memory it would require to keep all the images in the memory.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用TensorFlow数据集适配器来训练我们的模型。当然，我们可以从我们的数据集中创建一个NumPy数组，但想象一下，要保留所有图像在内存中需要多少内存。
- en: 'In contrast, the dataset adapter allows you to load the data into memory when
    required. Moreover, the data is loaded and prepared in the background so that
    it will not be a bottleneck in our training process. We transform our parsed array
    as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，数据集适配器允许你在需要时将数据加载到内存中。此外，数据在后台加载和准备，这样它就不会成为我们训练过程中的瓶颈。我们按照以下方式转换我们的解析数组：
- en: '[PRE22]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: From the previous code snippet, `from_tensor_slices` creates `Dataset` whose
    elements are slices of the given tensors. In our case, the tensors are NumPy arrays
    of labels (box, breed, image location, and more).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的代码片段中，`from_tensor_slices`创建了一个`Dataset`，其元素是给定张量的切片。在我们的例子中，这些张量是标签的NumPy数组（包括盒子、品种、图像位置等）。
- en: 'Under the hood, it is a similar concept to the Python `zip` function. First,
    we have prepared the input accordingly. Let''s now print one element from the
    dataset to see how it looks:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，它与Python的`zip`函数有类似的概念。首先，我们已经相应地准备好了输入。现在让我们打印数据集中的一个元素，看看它是什么样子：
- en: '[PRE23]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This gives the following output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE24]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'It is the TensorFlow—`tensor` that contains all the information that we have
    parsed from a single XML file. Given the dataset, we can check whether all our
    bounding boxes are correct:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这是TensorFlow中的`tensor`，它包含了我们从单个XML文件中解析出的所有信息。给定数据集，我们可以检查我们的所有边界框是否正确：
- en: '[PRE25]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: As we have normalized the boxes, they should be in the range of `[0,1]`. Additionally,
    we make sure that the coordinates of the first point of the box are less than
    or equal to the coordinates of the second point.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经归一化了盒子，它们应该在`[0,1]`的范围内。此外，我们确保盒子的第一个点的坐标小于或等于第二个点的坐标。
- en: 'Now, we define a function that will transform our data element so that we can
    feed it into a neural network:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们定义一个函数，将我们的数据元素转换，以便我们可以将其输入到神经网络中：
- en: '[PRE26]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The function first loads the corresponding image and resizes it to the standard
    size and normalizes it to `[0,1]`. Then, it creates a `one_hot` vector out of `types`
    and `breeds` using the `tf.one_hot` method and returns the result as an instance
    of `Data`.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先加载相应的图像，将其调整到标准大小，并将其归一化到`[0,1]`。然后，它使用`tf.one_hot`方法从`types`和`breeds`创建一个`one_hot`向量，并将结果作为`Data`实例返回。
- en: 'Now what remains is to `map` our dataset with the function, and we are ready
    to go:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的是使用函数`map`我们的数据集，然后我们就可以开始了：
- en: '[PRE27]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We have also called the `prefetch` method, which makes sure that some amount
    of data is prefetched so that our networks will not have to wait for the data
    to be loaded from the hard drive.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还调用了`prefetch`方法，确保预取一定量的数据，这样我们的网络就不必等待从硬盘加载数据。
- en: 'If we are running the data preparation script directly, it might be a good
    idea to illustrate some samples of the data. First, we create a function that
    creates an illustration image when it is given a sample of data:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们直接运行数据准备脚本，展示一些数据样本可能是个好主意。首先，我们创建一个函数，当给定数据样本时，它会创建一个说明图像：
- en: '[PRE28]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The function converts the `breed` one-hot vector back to a number, finds the
    name of the breed in the `breeds` dictionary, and plots the bounding box of the
    head together with the breed name.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将`breed`独热向量转换回一个数字，在`breeds`字典中找到品种的名称，并绘制头部边界框以及品种名称。
- en: 'Now, we concatenate several such illustrations and show the resulting image:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将几个这样的插图连接起来，并展示结果图像：
- en: '[PRE29]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The result is shown in the following screenshot:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下一张截图：
- en: '![](img/b8a17387-fbe6-4919-8f02-5520a8e469c8.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b8a17387-fbe6-4919-8f02-5520a8e469c8.png)'
- en: The preceding screenshot shows nice pets with the bounding boxes around their
    heads as expected. Note that, although we have used random numbers to mix the
    dataset in our script, you obtain the same result as illustrated previously. So,
    you can now see the power of pseudorandom numbers.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 上一张截图显示了预期中头部周围有边界框的可爱宠物。请注意，尽管我们在脚本中使用随机数来混合数据集，但你获得的结果与之前展示的相同。因此，你现在可以看到伪随机数的威力。
- en: Now that we have prepared the dataset, let's move on to creating and training
    classifiers in the next section. We will build two classifiers—one for the pet
    type and the other for the breed.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据集，接下来让我们进入下一节，创建和训练分类器。我们将构建两个分类器——一个用于宠物类型，另一个用于品种。
- en: Classifying with CNNs
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用CNN进行分类
- en: 'To start with the classification, first of all, we have to import the required
    modules:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始分类，首先，我们必须导入所需的模块：
- en: '[PRE30]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We have to import our prepared dataset and Keras, which we will use to build
    our classifier.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须导入我们准备好的数据集和Keras，我们将使用它们来构建我们的分类器。
- en: However, before we build our classifier, let's first learn about convolutional
    networks, as we are going to use them to build our classifier.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们构建我们的分类器之前，让我们首先了解卷积网络，因为我们将要使用它们来构建我们的分类器。
- en: Understanding CNNs
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解CNN
- en: In [Chapter 1](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml), *Fun with Filters*, you
    learned about filters and convolution. In particular, you learned how filters
    can be used to create a pencil sketch image. In the pencil sketch, you could see
    the points in the image that had a sharp change in value, that is, they were darker
    than those that had a smooth change.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml)，“*玩转滤波器*”，你学习了关于滤波器和卷积的知识。特别是，你学习了如何使用滤波器创建铅笔素描图像。在铅笔素描中，你可以看到图像中那些值发生急剧变化的点，也就是说，它们比那些值变化平滑的点要暗。
- en: From that point of view, the filters that we applied can be thought of as filters
    for edge detection. In other words, the filters act as a feature detector, where
    the feature is an edge. Alternatively, you could compose a different filter that
    is activated on the corners or that is activated when there is no change in the
    color value.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 从那个角度来看，我们应用的滤波器可以被视为边缘检测滤波器。换句话说，滤波器充当特征检测器，其中特征是边缘。或者，你也可以组合一个不同的滤波器，它在角上激活，或者在颜色值没有变化时激活。
- en: The filters that we have used act on a single-channel image and have two dimensions;
    however, we can extend the filter with the third dimension, which can then be
    applied to a multichannel image. For example, if a single-channel filter has size
    *3 x 3*, the corresponding 3-channel (for example, RGB) filter will have size
    *3 x 3 x 3*, where the last value is the depth of the filter.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的滤波器作用于单通道图像，并且有两个维度；然而，我们可以通过第三个维度扩展滤波器，这样它就可以应用于多通道图像。例如，如果一个单通道滤波器的大小是*3
    x 3*，那么相应的3通道（例如，RGB）滤波器的大小将是*3 x 3 x 3*，其中最后一个值是滤波器的深度。
- en: Such filters can already be used for more complex features. For example, you
    might think of a filter that works with the color green, meanwhile ignoring the
    values in red and blue by setting zeros in the corresponding elements of the filter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的滤波器已经可以用于更复杂的功能。例如，你可能想到一个与绿色颜色一起工作，同时通过在滤波器中相应元素设置零来忽略红色和蓝色值的滤波器。
- en: Once you come up with a good set of filters, you can apply them to the original
    image and then stack them into a new multichannel image. For example, if we apply
    100 filters on an image, we will obtain 100 single-channel images, which will
    result in a 100-channel image after stacking. Hence, we have built a layer that
    accepts 3 channels and outputs 100 channels.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到了一组好的滤波器，你就可以将它们应用到原始图像上，然后将它们堆叠成一个新的多通道图像。例如，如果我们对一个图像应用100个滤波器，我们将获得100个单通道图像，堆叠后将会得到一个100通道的图像。因此，我们构建了一个接受3个通道并输出100个通道的层。
- en: Next, we can compose new filters that have a depth of 100 and act on the composed
    100-channel image. These filters can also be activated on more complex features.
    For example, if there were filters in previous layers that are activated on edges,
    we can compose a filter that is activated on an intersection of edges.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以组合新的滤波器，这些滤波器的深度为 100，并作用于组合的 100 通道图像。这些滤波器也可以在更复杂的功能上被激活。例如，如果前面的层中有激活在边缘上的滤波器，我们可以组合一个在边缘交点处激活的滤波器。
- en: 'After a range of layers, we might see filters that are activated, for example,
    on the noses of people, heads, the wheels of vehicles, or so on. That is actually
    how the convolutional network works. Surely, a question arises: how do we compose
    those filters? The answer is we don''t, because they are learned.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一系列层之后，我们可能会看到激活的滤波器，例如，在人们的鼻子上、头部、车辆的轮子上等等。这正是卷积网络的工作方式。当然，一个问题随之而来：我们如何组合这些滤波器？答案是，我们不需要，因为它们是学习得到的。
- en: We provide the data and the network learns which filters it needs to make good
    predictions. Another difference between the convolutional filters that you have
    used is that, besides the learnable parameter of the filters, there is one more
    learnable value called, which is a constant term added to the output of a filter.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们提供数据，网络学习它需要的滤波器以做出良好的预测。与您使用的卷积滤波器之间的另一个区别是，除了滤波器的可学习参数之外，还有一个额外的可学习值，称为，它是一个添加到滤波器输出的常数项。
- en: Besides that, after the convolutional filters in each layer, usually, a nonlinear
    function is applied to the output of the filters called the **activation function**.
    As a result of the nonlinearity, the network represents quite a wider class of
    functions so that there is a relatively higher chance of building a good model.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在每个卷积滤波器之后，通常会对滤波器的输出应用一个非线性函数，称为 **激活函数**。由于非线性，网络可以表示更广泛的函数类别，因此构建良好模型的机会相对较高。
- en: Now that we have some understanding of how a convolutional network works, let's
    start by building a classifier. While building the networks in this chapter, you
    will see how the convolutional layers are built and used. As mentioned previously,
    we use a pretrained model for our new models, or, in other words, we use transfer
    learning. Let's understand what this is in the next section.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对卷积网络的工作原理有了些了解，让我们先从构建分类器开始。在构建本章中的网络时，您将看到卷积层是如何构建和使用的。如前所述，我们使用预训练模型来构建我们的新模型，换句话说，我们使用迁移学习。让我们在下一节中了解这是什么。
- en: Learning about transfer learning
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习迁移学习
- en: Usually, a CNN has millions of parameters. Let's make an estimation to find
    out where all of those parameters come from.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个卷积神经网络有数百万个参数。让我们做一个估计，找出所有这些参数的来源。
- en: Suppose we have a 10-layer network and each layer has 100 filters of size *3
    x 3*. These numbers are quite low and networks that have good performance usually
    have dozens of layers and hundreds of filters in each layer. For our case, each
    filter has a depth of 100.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个 10 层的网络，每层有 100 个大小为 *3 x 3* 的滤波器。这些数字相当低，通常表现良好的网络有数十个层和每层数百个滤波器。在我们的情况下，每个滤波器的深度为
    100。
- en: 'Hence, each filter has 3 x 3 x 3 = 900 parameters (excluding biases, the number
    of which is 100), which results in *900 x 100* parameters for each layer and,
    therefore, about 900,000 parameters for the complete network. To learn so many
    parameters from scratch without overfitting would require quite a large annotated
    dataset. A question arises: what can we do instead?'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个滤波器有 3 x 3 x 3 = 900 个参数（不包括偏差，偏差的数量为 100），这意味着每个层的参数为 *900 x 100*，因此整个网络的参数约为
    900,000 个。要从头开始学习这么多参数而不发生过拟合，需要相当大的标注数据集。一个问题随之而来：我们还能做什么？
- en: You have learned that layers of a network act as feature extractors. Besides
    this, natural images have quite a lot in common. Therefore, it would be a good
    idea to use the feature extractors of a network that was trained on a large dataset
    to achieve good performance on a different, smaller dataset. This technique is
    called **transfer learning**.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解到，网络层充当特征提取器。除此之外，自然图像有很多共同之处。因此，使用在大数据集上训练的网络的特征提取器来在不同的、较小的数据集上实现良好的性能是一个好主意。这种技术被称为
    **迁移学习**。
- en: 'Let''s pick a pretrained model as our base model, which is a single line of
    code with Keras:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择一个预训练模型作为我们的基础模型，这是 Keras 的一行代码：
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Here, we use the `MobileNetV2` pretrained network, which is a robust and lightweight
    network. Of course, you can use other available models instead, which can be found
    on the Keras website or by simply listing them with `dir(K.applications)`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是预训练的`MobileNetV2`网络，这是一个健壮且轻量级的网络。当然，你也可以使用其他可用的模型，这些模型可以在Keras网站上找到，或者通过简单地使用`dir(K.applications)`列出它们。
- en: We have taken the version of the network that excludes the top layers responsible
    for classification by passing in `include_top=False`, as we are going to build
    a new classifier on top of it. But still, the network includes all the other layers
    that were trained on **ImageNet**. ImageNet is a dataset that includes millions
    of images and each of the images is annotated with one of 1,000 classes of the
    dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过传递`include_top=False`来排除负责分类的顶层版本的网络，因为我们将在其之上构建一个新的分类器。但仍然，网络包括所有在**ImageNet**上训练的其他层。ImageNet是一个包含数百万图像的数据集，每个图像都标注了数据集中1,000个类别中的一个。
- en: 'Let''s take a look at the shape of the output of our base model:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们基础模型输出的形状：
- en: '[PRE32]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The result is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE33]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The first number is undefined and denotes the batch size or, in other words,
    the number of input images. Suppose we simultaneously pass a stack of 10 images
    to the network; then, the output here would have a shape of `(10,7,7,1280)` and
    the first dimension of the tensor will correspond to the input image number.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个数字是未定义的，表示批处理大小，换句话说，输入图像的数量。假设我们同时向网络传递10张图像的堆栈；那么，这里的输出将具有形状`(10,7,7,1280)`，张量的第一个维度将对应于输入图像编号。
- en: The next two indexes are the size of the output shape and the last is the number
    of channels. In the original model, this output represents features from the input
    images that are later used to classify the images of the ImageNet dataset.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两个索引是输出形状的大小，最后一个是通道数。在原始模型中，这个输出代表了从输入图像中提取的特征，这些特征后来用于对ImageNet数据集的图像进行分类。
- en: Therefore, they are quite a good representation of all the images so that the
    network can classify the images of ImageNet based on them. Let's try to use these
    features to classify the types and breeds of our pets. In order to do this, let's
    first prepare a classifier in the next section.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，它们很好地代表了所有图像，以便网络可以根据它们对ImageNet的图像进行分类。让我们尝试使用这些特征来分类我们宠物的类型和品种。为了做到这一点，让我们在下一节中首先准备一个分类器。
- en: Preparing the pet type and breed classifier
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备宠物类型和品种分类器
- en: 'As we are going to use the features as they are, let''s first freeze the weights
    of the network layers so that they don''t update during the training process:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将直接使用这些特征，让我们首先冻结网络层的权重，这样它们在训练过程中就不会更新：
- en: '[PRE34]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In general, each location of an activation map specifies whether there is a
    feature of the corresponding type in that location. As we work on the last layers
    of the network, we can suppose that different locations on the activation map
    contain similar information and reduce the dimensionality of our features by averaging
    the activation maps:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，激活图中的每个位置都指定了在该位置是否存在对应类型的特征。当我们处理网络的最后一层时，我们可以假设激活图上的不同位置包含相似的信息，并通过平均激活图来减少我们特征的维度：
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The operation is called `AveragePooling2D`—we pool the average of the tensor
    in two dimensions of our feature tensor. You can see the results by printing the
    shapes of the input and output of the operation:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个操作被称为`AveragePooling2D`——我们在特征张量的两个维度上池化张量的平均值。你可以通过打印操作输入和输出的形状来查看结果：
- en: '[PRE36]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'This shows the following output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了以下输出：
- en: '[PRE37]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now that we have just `1280` features per image, let''s add the classification
    layer right away and prepare our dataset for training either on the types or the
    breeds:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为每张图像有了`1280`个特征，让我们立即添加分类层，并准备我们的数据集以在类型或品种上进行训练：
- en: '[PRE38]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Training on the types and `breeds` differs only by the number of output neurons
    and the labels. In the case of `breeds`, the number of labels is `37`, and, in
    the case of types, this is `2` (namely cat or dog), which you can see in the code.
    A dense layer represents densely connected neurons. The latter means that each
    neuron in the layer is connected to all 1,280 inputs to the layer.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在类型和品种上训练的不同之处仅在于输出神经元的数量和标签。对于品种，标签的数量是`37`，而对于类型，这是`2`（即猫或狗），你可以在代码中看到。密集层代表密集连接的神经元。这意味着层中的每个神经元都连接到层的所有1,280个输入。
- en: Hence, each neuron has *1280 + 1* learnable parameters, where 1 is for the bias.
    Mathematically, for the complete layer, the weights of the kernel are represented
    with a matrix that has a size (1,280 for the number of classes) and a column of
    height 1280.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，每个神经元有 *1280 + 1* 个可学习的参数，其中 1 是用于偏置的。从数学上讲，对于完整的层，核的权重用一个大小为 (1,280，即类别的数量)
    的矩阵表示，并且有一个高度为 1280 的列。
- en: 'The linear part of the layer can be written as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 层的线性部分可以写成如下形式：
- en: '![](img/d26cdbef-7778-47fc-941b-c1661e2aa0d5.png),'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/d26cdbef-7778-47fc-941b-c1661e2aa0d5.png)'
- en: Here, **x** is the output of the previous layer (1,280 averaged features, in
    our case), **a** is the matrix, and **b** is the column.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**x** 是前一层（在我们的情况下是 1,280 个平均特征）的输出，**a** 是矩阵，**b** 是列。
- en: 'Also, we have set a **softmax** function as the activation, which is a good
    choice with classification tasks. The latter is defined as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还设置了一个 **softmax** 函数作为激活函数，这对于分类任务是一个很好的选择。后者定义如下：
- en: '![](img/e2b6cfaf-9977-473b-87ee-011f3fc1f660.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e2b6cfaf-9977-473b-87ee-011f3fc1f660.png)'
- en: Here, **x** is the input to the activation (output of the linear part).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**x** 是激活函数的输入（线性部分的输出）。
- en: You can see that it sums up to one across all outputs; hence, the output can
    be thought of as the probability of the corresponding class.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到所有输出加起来等于一；因此，输出可以被认为是相应类别的概率。
- en: The mapping that we defined on the dataset will set the image as data and the
    breeds or types as the label.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据集上定义的映射将图像作为数据，品种或类型作为标签。
- en: 'Now we are ready to define our model:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好定义我们的模型：
- en: '[PRE39]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Here, you can see that the input of the network is our base model and the output
    is our classifier layer. Hence, we have successfully built our classification
    network.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到网络的输入是我们的基础模型，输出是我们的分类层。因此，我们已经成功构建了我们的分类网络。
- en: 'So, now that we have prepared our classifier network, let''s train and evaluate
    it in the next section:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经准备好了我们的分类网络，让我们在下一节中对其进行训练和评估：
- en: Training and evaluating the classifier
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练和评估分类器
- en: 'In order to train the classifier, we have to configure it for training. We
    have to specify an objective function (the `loss` function) and a training method.
    Additionally, we might want to specify some metrics in order to see how the model
    performs. We can configure the classifier using the `compile` method of the model:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练分类器，我们必须为它配置训练。我们必须指定一个目标函数（损失函数）和一个训练方法。此外，我们可能还想指定一些指标，以便查看模型的性能。我们可以使用模型的
    `compile` 方法来配置分类器：
- en: '[PRE40]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We have passed `metrics` as `categorical_accuracy`, which will show which part
    of the dataset is classified with the right class. Besides this, we have passed
    one more metric called `top_k_categorical_accuracy`, which shows which part of
    the dataset is correct in the top `k` prediction of the network.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 `metrics` 传递为 `categorical_accuracy`，这将显示数据集的哪一部分被正确分类。除此之外，我们还传递了一个名为 `top_k_categorical_accuracy`
    的额外指标，它显示了数据集的哪一部分在网络的前 `k` 预测中是正确的。
- en: The default value of `k` is five, so the metric shows which part of the dataset
    is in the most probable five classes predicted by the neural network. We have
    also passed `optimizer="adam"`, which forces the model to use **Adam Optimizer **as
    a training algorithm. You will learn how neural networks are usually trained in
    the *Understanding backpropagation* section.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`k` 的默认值是五，因此该指标显示了数据集中最有可能被神经网络预测的前五个类别。我们还传递了 `optimizer="adam"`，这强制模型使用
    **Adam Optimizer** 作为训练算法。你将在 *理解反向传播* 部分学习神经网络通常是如何训练的。'
- en: 'Before training, we also split the dataset into training and test sets in order
    to see how the network performs on unseen data:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们还把数据集分成训练集和测试集，以便查看网络在未见数据上的表现：
- en: '[PRE41]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Here, we take the first `1000` elements of the dataset for test purposes. And
    the remaining part is used for training.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们取数据集的前 `1000` 个元素用于测试目的。其余部分用于训练。
- en: 'The training part is mixed by calling the `shuffle` method, which will make
    sure that we have a different order of the data in each epoch of training. Finally,
    we train our network by calling the `fit` method of the dataset and then evaluate
    this on the validation set:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调用 `shuffle` 方法，训练部分是混合的，这将确保我们在每个训练周期中都有不同的数据顺序。最后，我们通过调用数据集的 `fit` 方法来训练我们的网络，并在验证集上评估它：
- en: '[PRE42]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: First, the `fit` method accepts the dataset itself, which we pass with batches
    of `32`. The latter means that, on each step of the training process, `32` images
    from the dataset will be used.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`fit` 方法接受数据集本身，我们通过`32`批次的批量传递。这意味着，在训练过程的每一步，将使用数据集中的`32`张图片。
- en: 'We have also passed a number of `epochs`, which means that our dataset will
    be iterated for `4` times until the training procedure stops. The output of the
    last `epoch` looks as follows:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还传递了多个`epochs`，这意味着我们的数据集将被迭代`4`次，直到训练过程停止。最后一个`epoch`的输出如下：
- en: '[PRE43]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Our categorical accuracy on the train set is more than 97%. So, we are pretty
    good at differentiating between cats and dogs. Of course, the **top-K accuracy**
    will be 100 percent as we have just two classes. Now, let's see how we are performing
    on the validation set.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上的分类准确率超过 97%。因此，我们在区分猫和狗方面做得相当不错。当然，**top-K 准确率**将是 100%，因为我们只有两个类别。现在，让我们看看我们在验证集上的表现。
- en: 'After training, the model is evaluated and you should obtain results similar
    to the test set:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，模型将被评估，你应该获得与测试集相似的结果：
- en: '[PRE44]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is given as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE45]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: We again get the categorical accuracy of more than 97%. Therefore, our model
    does not overfit and performs well on the test set.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次获得了超过 97% 的分类准确率。因此，我们的模型没有过拟合，在测试集上的表现良好。
- en: 'If we train on breeds, the same output for training looks as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在品种上进行训练，训练输出的结果如下：
- en: '[PRE46]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Meanwhile, the output for testing looks like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，测试输出的结果如下所示：
- en: '[PRE47]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: For breeds, we get worse results, which is expected as it is much more difficult
    to differentiate a breed than just state whether it is a cat or a dog. In any
    case, the model does not perform too badly. Its first-attempt guess is more than
    80 percent right, and we can also be about 99 percent sure that it will guess
    the breed if it has 5 attempts.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于品种，我们得到了更差的结果，这是预期的，因为区分一个品种比仅仅判断它是猫还是狗要困难得多。无论如何，模型的表现并不太差。它的第一次尝试猜测有超过 80%
    是正确的，我们也可以有大约 99% 的把握，如果它有 5 次尝试，它将猜对品种。
- en: In this section, we have learned how to use a pretrained classifier network
    to build a new classifier. In the next section, let's move ahead with our deep
    learning journey and create an object localization network using the same base
    model—a task that the base model was never trained to accomplish.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了如何使用预训练分类器网络构建一个新的分类器。在下一节中，让我们继续我们的深度学习之旅，并使用相同的基础模型创建一个对象定位网络——这是一个基础模型从未训练过的任务。
- en: Localizing with CNNs
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 CNN 进行定位
- en: Being able to create your own localizer is a good way to acquire intuition on
    how an object detection network might work. This is because the only conceptual
    difference between object detection and localization networks is that a localization
    network predicts a single bounding box, while an object detection network predicts
    multiple boxes. Also, it is a good way to start understanding how to build a neural
    network that accomplishes other regression tasks.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 能够创建自己的定位器是了解对象检测网络可能如何工作的一种好方法。这是因为对象检测网络和定位网络之间唯一的理念差异在于，定位网络预测单个边界框，而对象检测网络预测多个边界框。此外，这也是开始理解如何构建一个能够完成其他回归任务的神经网络的好方法。
- en: 'In this section, we are going to use the same pretrained classifier network, `MobileNetV2`,
    as the previous section. However, this time we are going to use the network for
    localizing objects instead of classifying. Let''s import the required modules
    and the base model in the same way that we did in the previous section—although,
    this time, we are not going to freeze the layers of the base model:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用与上一节相同的预训练分类器网络，`MobileNetV2`。然而，这次我们将使用该网络进行对象定位而不是分类。让我们以与上一节相同的方式导入所需的模块和基础模型——尽管这次，我们不会冻结基础模型中的层：
- en: '[PRE48]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Now that we have everything ready, let's go on to prepare our localizer model.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了所有东西，让我们继续准备我们的定位器模型。
- en: Preparing the model
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备模型
- en: First, let's think about how we can make a localizer using the output of the
    base model.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们思考一下我们如何使用基础模型的输出制作一个定位器。
- en: As mentioned previously, the output tensor of the base model has a shape of `(None,
    7, 7, 1280)`. The output tensor represents features obtained using a convolutional
    network. We can suppose that some spatial information is encoded in the spatial
    indexes *(7,7)*.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，基础模型的输出张量形状为`(None, 7, 7, 1280)`。输出张量表示使用卷积网络获得的特征。我们可以假设一些空间信息被编码在空间索引*(7,7)*中。
- en: Let's try to reduce the dimensionality of our feature map using a couple of
    convolutional layers and create a regressor that should predict the corner coordinates
    of the pets' head bounding boxes provided by the dataset.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用几个卷积层来降低特征图的维度，并创建一个回归器，该回归器应该预测数据集中提供的宠物头部边界框的角坐标。
- en: 'Our convolutional layers will have several options that are the same:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的卷积层将有几个相同的选项：
- en: '[PRE49]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: First of all, they will both use the **Rectified Linear Unit** (**ReLU**) as
    an activation function. The latter is a simple function, which is zero when the
    input is less than zero and is equal to the input when the input is greater than
    or equal to zero.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，它们都将使用**ReLU**（修正线性单元）作为激活函数。后者是一个简单的函数，当输入小于零时为零，当输入大于或等于零时等于输入。
- en: '`padding=same` specifies that we do not want the convolution operation to reduce
    the size of the feature map. The feature maps will be padded with zeros such that
    the feature maps do not reduce size. This is in contrast to `padding=''valid''`,
    which applies the convolutional kernel only up to the margins of the feature maps.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '`padding=same`指定我们不想卷积操作减小特征图的大小。特征图将通过填充零来填充，这样特征图就不会减小大小。这与`padding=''valid''`形成对比，后者只将卷积核应用于特征图的边缘。'
- en: It is often a good idea to regularize trained parameters, normalize them, or
    do both. The latter often allows you to train easier, faster, and generalize better.
    Regularizers allow you to apply penalties on layer parameters during optimization.
    These penalties are incorporated in the loss function that the network optimizes.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 经常来说，正则化训练参数、归一化它们或两者都做是一个好主意。后者通常允许你更容易、更快地训练，并且泛化得更好。正则化器允许你在优化期间对层参数应用惩罚。这些惩罚被纳入网络优化的损失函数中。
- en: In our case, we use the `l2` kernel regularizer, which regularizes the **Euclidian**
    norm of the convolutional kernel weights. The regularization is accomplished by
    adding the ![](img/36015bb2-55c9-49d9-aded-2922fb7f138e.png) term to the loss
    function (the objective function). Here, ![](img/41764ce8-db3a-4c8c-9556-f1ba8a681902.png) is
    a small constant and ![](img/1fafba1e-c15e-42e9-aec3-821c0ef3eb13.png) is the
    *L2* norm, which is equal to the square root of the sum of squares of the parameters
    of the layer.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们使用`l2`核正则化器，该正则化器正则化了卷积核权重的**欧几里得**范数。正则化是通过将![](img/36015bb2-55c9-49d9-aded-2922fb7f138e.png)项添加到损失函数（目标函数）中实现的。在这里，![](img/41764ce8-db3a-4c8c-9556-f1ba8a681902.png)是一个小的常数，而![](img/1fafba1e-c15e-42e9-aec3-821c0ef3eb13.png)是**L2**范数，它等于层参数平方和的平方根。
- en: 'This is one of the most widely used regularization terms. Now we are ready
    to define our convolutional layers. The first layer is shown as follows:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最常用的正则化项之一。现在我们准备好定义我们的卷积层。第一层如下所示：
- en: '[PRE50]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Here, the first parameter is the number of output channels, which is also the
    number of convolutional filters. The second parameter describes the size of the
    convolutional filters. At first glance, it might seem that a single-pixel convolutional
    kernel does not make much sense as it cannot encode the contextual information
    of a feature map.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，第一个参数是输出通道数，也就是卷积核的数量。第二个参数描述了卷积核的大小。乍一看，单个像素的卷积核可能没有太多意义，因为它不能编码特征图的上下文信息。
- en: That is surely correct; however, in this case, it is used for a different purpose.
    It is a fast operation that allows encoding the depth of the input feature maps
    in a lower dimensionality. The depth is reduced from 1280 to `256`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这无疑是正确的；然而，在这种情况下，它被用于不同的目的。这是一个快速操作，允许在较低维度中编码输入特征图的深度。深度从1280减少到`256`。
- en: 'The next layer looks as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个层看起来如下：
- en: '[PRE51]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Here, besides the default options that we use, we use strides, which specify
    the number of pixels shifts over the input. In [Chapter 1](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml),
    *Fun with Filters,* the convolutional operation was applied in each location,
    which means the filter was moved one pixel at a time and is equivalent to strides
    equal to one.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，除了我们使用的默认选项之外，我们还使用了步长，它指定了输入上的像素偏移量。在[第1章](2e878463-75f1-40a5-b263-0c5aa9627328.xhtml)，“与过滤器一起玩乐”，卷积操作在每个位置应用，这意味着过滤器每次移动一个像素，相当于步长等于1。
- en: When the `strides` option is `2`, then we move the filters by two pixels at
    each step. The option is in plural form as we might want to have different strides
    in different directions, which can be done by passing a tuple of numbers. The
    application of a `stride` with a value greater than 1 is a means to reduce the
    size of the activation map without losing spatial information.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当`strides`选项为`2`时，我们每次移动过滤器两个像素。这个选项以复数形式出现，因为我们可能希望在不同方向上有不同的步长，这可以通过传递一个数字元组来实现。使用大于1的`stride`值是一种在不丢失空间信息的情况下减小激活图大小的手段。
- en: Of course, there are other operations that can reduce the size of the activation
    maps. For example, an operation called **max pooling** can be used, which is one
    of the most widely used operations in modern convolutional networks. The latter
    takes a small window size (for example, *2 x 2*), picks a single maximal value
    from that window, moves by a specified number of pixels (for example, 2), and
    repeats the procedure throughout the activation map. Therefore, as a result of
    this procedure, the size of the activation map will be reduced by a factor of
    2.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，还有其他可以减小激活图大小的操作。例如，可以使用称为**最大池化**的操作，这是现代卷积网络中最广泛使用的操作之一。后者采用较小的窗口大小（例如，*2
    x 2*），从这个窗口中选取一个最大值，然后移动指定数量的像素（例如，2），并在整个激活图上重复此过程。因此，通过这个过程，激活图的大小将减少2倍。
- en: In contrast to the approach with strides, the max-pooling operation is more
    suitable for tasks where we are not very interested in spatial information. Such
    tasks are, for example, classification tasks, in which we are not interested where
    an object is exactly but are simply interested in what it is. The loss of the
    spatial information in max pooling happens when we simply take the maximal value
    in a window without considering its position in the window.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 与使用步长的方法相比，最大池化操作更适合我们不太关心空间信息的任务。例如，这些任务包括分类任务，我们对其中的对象确切位置不感兴趣，而只是对其是什么感兴趣。在最大池化中丢失空间信息发生在我们简单地从一个窗口中取最大值而不考虑其在窗口中的位置时。
- en: 'The last thing that we want to do is to connect a dense layer of four neurons
    to the convolutional layer, which will be regressed to the two corner coordinates
    of the bounding boxes (`(x,y)` for each corner):'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后想要做的是将一个包含四个神经元的密集层连接到卷积层，这将回归到边界框的两个角坐标（每个角为`(x,y)`）：
- en: '[PRE52]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As the coordinates of bounding boxes are normalized, it's a good idea to use
    an activation function, which has values in the range of `(0,1)` such as a `sigmoid`
    function, in our case.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 由于边界框的坐标是归一化的，因此使用一个值在`(0,1)`范围内的激活函数，例如`sigmoid`函数，是个好主意。
- en: 'All the required layers are ready. Now, let''s define the model with the new
    layers and compile it for training:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 所有必要的层都已准备就绪。现在，让我们使用新层定义模型，并编译它以进行训练：
- en: '[PRE53]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: We use the **Mean Squared Error** (**MSE**) as a `loss` function, which is the
    squared difference between the ground truth and the predicted value. During training,
    this value will be minimized; hence, the model is supposed to predict the corner
    coordinates after training.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**均方误差**（**MSE**）作为`loss`函数，它是真实值和预测值之间平方差的函数。在训练过程中，这个值将被最小化；因此，模型在训练后应该预测角坐标。
- en: The regularization terms that we have added to the convolutional layers are
    also added to `loss` as discussed. The latter is done automatically by Keras.
    Also, we use the **Root of MSE** (**RMSE**) along with the **Mean Absolute Error**
    (**MAE**), which measures the average magnitude of the errors, as our metrics.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加到卷积层的正则化项也如讨论的那样添加到`loss`中。这由Keras自动完成。此外，我们还使用**均方根误差**（**RMSE**）和**平均绝对误差**（**MAE**），后者衡量误差的平均幅度，作为我们的指标。
- en: 'Let''s now split the dataset, in the same way that we did in the previous section:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们以与上一节相同的方式分割数据集：
- en: '[PRE54]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: What is left to do is to train our model, just like we did in the previous section.
    However, before proceeding with our training, you might be interested in learning
    how exactly the training of our new layers is accomplished. In multilayer neural
    networks, training is usually done using the **backpropagation** **algorithm**,
    so let's first learn about that in the next section.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作就是训练我们的模型，就像我们在上一节中所做的那样。然而，在我们继续训练之前，你可能对了解我们新层的训练是如何完成的感兴趣。在多层神经网络中，训练通常使用**反向传播**算法进行，所以让我们在下一节中首先了解这一点。
- en: Understanding backpropagation
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解反向传播
- en: A neural network is considered to be trained when we have some optimal weights
    of the network so that the network makes good predictions on our data. So, the
    question is how do we reach these optimal weights? Neural networks are usually
    trained using a **gradient descent** algorithm. This might be either the pure
    gradient descent algorithm or some improved optimization method such as **Adam
    optimizer**, which is again based on computing the gradient.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一些网络的优化权重，使得网络对我们的数据做出良好的预测时，我们认为神经网络已经训练好了。所以，问题是我们是怎样达到这些优化权重的？神经网络通常使用**梯度下降**算法进行训练。这可能纯粹是梯度下降算法，或者是一些改进的优化方法，如**Adam优化器**，它再次基于计算梯度。
- en: In all of these algorithms, we need to compute the gradient of the loss function
    relative to all the weights. As a neural network is a complex function, it might
    not appear to be straightforward. This is where the backpropagation algorithm
    jumps in, which allows us to calculate the gradients easily in complex networks
    and understand what the gradient looks like. Let's dive into the details of the
    algorithm.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些算法中，我们需要计算损失函数相对于所有权重的梯度。由于神经网络是一个复杂的函数，这可能看起来并不直接。这就是反向传播算法介入的地方，它使我们能够在复杂的网络中轻松地计算梯度，并了解梯度看起来是什么样子。让我们深入了解算法的细节。
- en: 'Suppose we have a neural network consisting of an *N* sequential layer. Generally
    speaking, the *i^(th)* layer in such a network is a function that can be defined
    as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个由*N*个连续层组成的神经网络。一般来说，这样的网络中的*i^(th)*层是一个可以定义为以下函数：
- en: '![](img/5fb2f53d-17c4-497c-999c-22c2343ef5ab.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5fb2f53d-17c4-497c-999c-22c2343ef5ab.png)'
- en: Here, ![](img/6b337984-58aa-48f5-843f-ebfdf60098d1.png) is the weight of the
    layer and ![](img/3b4433fc-eda7-4fd8-b358-e97534e0dd42.png) is the function corresponding
    to the previous layer.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![图片](img/6b337984-58aa-48f5-843f-ebfdf60098d1.png)是层的权重，![图片](img/3b4433fc-eda7-4fd8-b358-e97534e0dd42.png)是前一层对应的函数。
- en: We can define ![](img/0eedcb66-349e-4f28-9927-52467d336169.png) to be the input
    of our network so that the formula holds for the complete neural network including
    the first layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将![图片](img/0eedcb66-349e-4f28-9927-52467d336169.png)定义为网络的输入，这样公式就适用于包括第一层的完整神经网络。
- en: We can also define ![](img/16351790-0066-4bbe-8664-ebab0df88d75.png) to be our
    loss function so that the formula defines not only all the layers but also the
    loss function. Of course, such a generalization excludes the weight normalization
    term that we have already used. However, this is a simple term that just adds
    up to the loss and hence can be omitted for simplicity.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以定义![图片](img/16351790-0066-4bbe-8664-ebab0df88d75.png)为我们的损失函数，这样公式不仅定义了所有层，还定义了损失函数。当然，这种泛化排除了我们已使用的权重归一化项。然而，这是一个简单的项，它只是加到损失上，因此为了简单起见可以省略。
- en: 'We can compute the gradient of the loss function by setting ![](img/cecf9042-e73b-4400-b08b-c297da168364.png) and using
    the chain rule as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置![图片](img/cecf9042-e73b-4400-b08b-c297da168364.png)并使用链式法则来计算损失函数的梯度：
- en: '![](img/577e2914-0581-45de-9448-52b995b0d006.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/577e2914-0581-45de-9448-52b995b0d006.png)'
- en: According to our definition, this formula holds not only for the loss function,
    but it is also general for all of the layers. In this formula, we can see that
    the partial derivative of a certain layer with respect to all the weight in the
    previous layer including the current layer is expressed in terms of the same derivative
    of the previous layer, which is the ![](img/207667cb-a836-4e3f-b064-8ce5f4d66ed0.png) term
    in the formula, and terms that can be calculated using only the current layer,
    namely, ![](img/c9dea053-54e9-4b00-abaf-4b7c0aa70465.png) and ![](img/6b7913d5-3b59-4749-9fd1-e1beec1d0088.png).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的定义，这个公式不仅适用于损失函数，而且适用于所有层。在这个公式中，我们可以看到，某一层相对于前一层中所有权重的偏导数（包括当前层）是用前一层的相同导数表示的，即公式中的 ![](img/207667cb-a836-4e3f-b064-8ce5f4d66ed0.png) 项，以及只能使用当前层计算出的项，即 ![](img/c9dea053-54e9-4b00-abaf-4b7c0aa70465.png) 和 ![](img/6b7913d5-3b59-4749-9fd1-e1beec1d0088.png)。
- en: 'Using the formula, we can now numerically compute the gradient. In order to
    do that, we first define a variable representing an error signal and assign its
    initial value to one. It should be clear in a moment why it represents an error
    signal. Then, we start from the last layer (the loss function, in our case) and
    repeat the following steps until we reach the input of the network:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用公式，我们现在可以数值地计算梯度。为了做到这一点，我们首先定义一个代表误差信号的变量，并将其初始值设为1。很快就会清楚为什么它代表误差信号。然后，我们从最后一层（在我们的例子中是损失函数）开始，重复以下步骤，直到我们到达网络的输入层：
- en: Compute the partial derivative of the current layer with respect to its weights
    and multiply by the error signal. This will be the part of the gradient corresponding
    to the weights of the current layer.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算当前层相对于其权重的偏导数，并乘以误差信号。这将对应于当前层权重的梯度部分。
- en: Compute the partial derivative with respect to the previous layer, multiply
    by the error signal, and then update the error signal with the resulting value.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对前一层的偏导数进行计算，乘以误差信号，然后用得到的结果更新误差信号。
- en: If the input of the network is not reached, move to the previous layer and repeat
    the steps.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果网络的输入没有被到达，就移动到前一层并重复这些步骤。
- en: Once we reach the input, we have all the partial derivatives of the loss with
    respect to the learnable weights; therefore, we have the gradient of our loss
    function. Now we can note that this is the partial derivative of a layer with
    respect to the previous layer that propagates backward throughout the network
    during the gradient computation process.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们到达输入层，我们就有了损失函数相对于可学习权重的所有偏导数；因此，我们得到了损失函数的梯度。现在我们可以注意到，这是在梯度计算过程中通过网络传播的层的偏导数。
- en: That is a **propagating signal**, which influences the contribution of each
    layer to the gradient of the loss function. For example, if it becomes all zero
    somewhere during the propagation, then the contribution of all the remaining layers
    to the gradient will also be zero. Such a phenomenon is called a **vanishing-gradient
    problem**. This algorithm can be generalized to acyclic networks with different
    kinds of branches.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 那是一个**传播信号**，它影响每个层对损失函数梯度的贡献。例如，如果在传播过程中某处变为全零，那么所有剩余层对梯度的贡献也将为零。这种现象称为**梯度消失问题**。此算法可以推广到具有不同类型分支的循环网络。
- en: In order to train our network, all that is left to do is to update our weight
    in the direction of the gradient and repeat the procedure until convergence. If
    the pure gradient descent algorithm is used, we simply subtract the gradient multiplied
    by some small constant from the weights; however, usually, more advanced optimization
    algorithms are used such as Adam optimizer.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练我们的网络，我们剩下的唯一要做的事情就是沿着梯度的方向更新我们的权重，并重复这个过程直到收敛。如果使用纯梯度下降算法，我们只需从权重中减去乘以某个小常数的梯度；然而，通常情况下，我们会使用更高级的优化算法，例如Adam优化器。
- en: The problem with the pure gradient descent algorithm is that, first, we should
    find some optimal value for the small constant so that the update of the weights
    is neither too small, which will result in slow learning, or too large, as too
    large a value results in instability. Another problem is that once we have found
    an optimal value, we have to start to decrease it once the network starts to converge.
    What is more important, it's often wise to update different weights with different
    factors as different weights might be at different distances from their optimal
    values.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 纯梯度下降算法的问题在于，首先，我们应该找到一个最优的小常数值，以便权重的更新既不会太小，导致学习速度慢，也不会太大，因为太大的值会导致不稳定性。另一个问题是，一旦我们找到了一个最优值，一旦网络开始收敛，我们就必须开始减小它。更重要的是，通常情况下，使用不同的因子更新不同的权重是明智的，因为不同的权重可能距离它们的最优值有不同距离。
- en: These are some of the reasons why we might want to use more advanced optimization
    techniques such as Adam optimizer or **RMSProp**, which take some or all of these
    mentioned issues, and even some unmentioned issues, into account. Meanwhile, while
    creating your networks, you should note that there is still ongoing research in
    the field of optimization algorithms and that one of the existing optimizers might
    be better in some cases than others, although the Adam optimizer should be a good
    choice for many tasks.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们可能想要使用更高级优化技术（如Adam优化器或**RMSProp**）的一些原因，这些技术考虑了这些提到的问题，甚至考虑了一些未提到的问题。同时，在创建你的网络时，你应该注意，优化算法领域的研究仍在进行中，尽管Adam优化器对于许多任务来说应该是一个不错的选择，但现有的某些优化器在某些情况下可能比其他优化器更好。
- en: You might also note that in the algorithm, we did not mention exactly how the
    partial derivatives in a layer can be computed. Of course, they can be numerically
    computed by varying the values and measuring the response as is done with numerical
    methods for computing a derivative. The problem is that such computations would
    be heavy and error-prone. A better way to do it is to define a symbolic representation
    for each operation used and then, again, use the chain rule as in the backpropagation.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到，在算法中，我们没有具体说明如何计算层中的偏导数。当然，可以通过改变值并测量响应来数值计算它们，就像使用数值方法计算导数一样。问题是这样的计算既费时又容易出错。更好的方法是定义每个操作的符号表示，然后再次使用链式法则，就像在反向传播中做的那样。
- en: So, we now understand how the complete gradient is calculated. Actually, most
    of the modern deep learning frameworks do the differentiation for you. You usually
    don't need to worry about how exactly it is accomplished, but understanding the
    backgrounds of the computation might be very helpful if you are planning to work
    on new, that is, your own, models.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们现在理解了完整梯度的计算方法。实际上，大多数现代深度学习框架都会为你完成微分。你通常不需要担心它是如何实现的，但如果计划开发新的，即你自己的模型，了解计算的背景可能非常有帮助。
- en: But for now, let's train our prepared model in the next section and see how
    it performs.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，让我们在下一节中训练我们准备好的模型，看看它的表现如何。
- en: Training the model
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练模型
- en: 'Before we proceed with the actual training, it is a good idea to have some
    means to save the model with the best weights. For that purpose, we will use a
    callback from Keras:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行实际训练之前，有一个保存最佳权重的模型的好方法。为此，我们将使用Keras的回调：
- en: '[PRE55]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The callback will be called after each epoch of training; it will calculate
    the `root_mean_square_error` metric of predictions on the validation data and
    will save the model to `localization.h5` if the metric has improved.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 训练回调将在每次训练周期后调用；它将计算验证数据上预测的`root_mean_square_error`指标，如果指标有所改进，则会将模型保存到`localization.h5`。
- en: 'Now, we train our model in the same way that we did with classification:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们以与分类相同的方式进行模型训练：
- en: '[PRE56]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Here, the difference is that we train with more `epochs` this time, as well
    as pass our callbacks and the validation dataset.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，不同之处在于我们这次使用更多的`epochs`进行训练，以及传递我们的回调和验证数据集。
- en: During training, you will first see a gradual decrease in the loss and metrics,
    both on the train and validation data. After several `epochs`, you might see that
    the metrics on the validation data increase. The latter might be thought of as
    a sign of overfitting, but after more `epochs`, you might see that the metrics
    on `validation_data` suddenly drop. The latter phenomenon is because the model
    switches to a better minimum metric during the optimization process.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，你首先会看到损失和指标在训练和验证数据上都有所下降。经过几个`epochs`后，你可能会看到验证数据上的指标有所上升。后者可能被认为是过拟合的迹象，但在更多的`epochs`之后，你可能会看到`validation_data`上的指标突然下降。后者的现象是因为模型在优化过程中切换到了更好的最小指标。
- en: 'Here is the result of the lowest value of the monitored metric:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是监控指标最低值的结果：
- en: '[PRE57]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: You can note that, in this case, it was the eighth `epoch` that performed best
    on the validation data. You can note that the RMSE deviation on the validation
    data is about 6 percent. The MAE is less than 6 percent. We can interpret this
    result as follows—given an image from the validation data, the corner coordinates
    of the bounding box are usually shifted by a factor of 1/20 of the size of the
    image, which is not a bad result as the size of the bounding box is comparable
    with the size of the image.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以注意到，在这种情况下，是第八个`epoch`在验证数据上表现最好。你可以注意到，在验证数据上的RMSE偏差大约是6%。MAE小于6%。我们可以这样解释这个结果——给定一个验证数据集中的图像，边界框的角坐标通常会被图像大小的1/20所偏移，由于边界框的大小与图像大小相当，所以这不是一个坏的结果。
- en: You might also want to try to train the model with the frozen layers of the
    base models. If you do so, you will notice a far worse performance than with an
    unfrozen model. It will perform about twice as badly on the validation dataset
    according to the metrics. Given these numbers, we can conclude that the layers
    of the base model were able to learn on the dataset so that our model performs
    better on the localization task.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可能想尝试使用基础模型的冻结层来训练模型。如果你这样做，你将注意到性能远比未冻结模型差。根据指标，它在验证数据集上的表现大约差两倍。考虑到这些数字，我们可以得出结论，基础模型的层能够在数据集上学习，从而使我们的模型在定位任务上表现更好。
- en: So, now that we have our model ready, let's use our inference script to see
    what it can do in the next section.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们的模型已经准备好了，让我们在下一节使用我们的推理脚本来看看它能做什么。
- en: Seeing inference in action
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 观察推理的实际应用
- en: 'Once we run our inference script, it will connect to the camera and localize
    a box on each frame, as depicted in the following photograph:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们运行推理脚本，它将连接到相机并在每一帧上定位一个框，如下面的照片所示：
- en: '![](img/a6a24176-0086-4809-8cc9-bc69254f672d.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a6a24176-0086-4809-8cc9-bc69254f672d.png)'
- en: Although the model was trained on the location of heads of pets, we can see
    that it's quite good at localizing the head of a person. This is where you can
    note the power of generalization of the model.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管模型是在宠物头部位置上训练的，但我们可以看到它在定位人的头部方面相当出色。这就是你可以注意到模型泛化能力的时刻。
- en: When you create your own deep learning apps, you might discover that you have
    a lack of data for your particular application. However, if you relate your specific
    case to other available datasets, you might be able to find some applicable dataset
    that, although different, might allow you to successfully train your model.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 当你创建自己的深度学习应用时，你可能会发现你为特定应用缺乏数据。然而，如果你将你的特定案例与其他可用的数据集联系起来，你可能会找到一些适用的数据集，尽管它们不同，但可能允许你成功训练你的模型。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Throughout this chapter, we have created and trained classification and localization
    models using the Oxford-IIIT-Pet dataset. We have learned how to create deep learning
    classifiers and localizers using transfer learning.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Oxford-IIIT-Pet数据集创建和训练了分类和定位模型。我们学习了如何使用迁移学习创建深度学习分类器和定位器。
- en: You have started to understand how to solve real-world problems using deep learning.
    You have understood how CNNs work and you know how to create a new CNN using a
    base model.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经开始理解如何使用深度学习解决现实世界的问题。你已经理解了CNN是如何工作的，并且知道如何使用基础模型创建一个新的CNN。
- en: We have also covered the backpropagation algorithm for computing gradients.
    Understanding this algorithm will allow you to make wiser decisions on the architecture
    of models that you might want to build in the future.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了用于计算梯度的反向传播算法。理解这个算法将使你能够对未来可能想要构建的模型架构做出更明智的决定。
- en: In the next chapter, we will continue our deep learning journey. We will create
    an application that will detect and track objects with high accuracy.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续我们的深度学习之旅。我们将创建一个能够以高精度检测和跟踪物体的应用程序。
- en: Dataset attribution
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集归属
- en: '**Oxford-IIIT-Pet dataset**: *Cats and Dogs*, O. M. Parkhi, A. Vedaldi, A.
    Zisserman, C. V. Jawahar in IEEE Conference on Computer Vision and Pattern Recognition,
    2012.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**牛津-IIIT-Pet 数据集**：*猫和狗*，O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar
    在 IEEE 计算机视觉与模式识别会议，2012年。'
