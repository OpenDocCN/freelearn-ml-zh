- en: Algorithms/Techniques Related to Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习相关的算法/技术
- en: There are a few algorithms and techniques, related to the machine learning examples
    in this book, that we were not able to go into much detail about in the preceding
    chapters. We will address that here.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们未能对与本书中机器学习示例相关的某些算法和技术进行详细讨论。我们将在本节中解决这个问题。
- en: Gradient descent
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度下降
- en: In multiple examples (including those in [Chapter 4](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml),
    *Regression* and [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)*, Classification*),
    we took advantage of an optimization technique called **gradient descent**. There
    are multiple variants of the gradient descent method, and, in general, you will
    see them pretty much everywhere in the machine learning world. Most prominently,
    they are utilized in the determination of optimal coefficients for algorithms
    such as linear or logistic regression, and thus, they often also play a role in
    more complicated techniques at least partially based on linear/logistic regression
    (such as neural networks).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在多个例子中（包括[第4章](c5c610c4-4e25-4e09-9150-b25c4b69720e.xhtml)“回归”和[第5章](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)“分类”中的例子），我们利用了一种称为**梯度下降**的优化技术。梯度下降方法有多种变体，通常在机器学习世界的各个地方都能看到它们。最显著的是，它们被用于确定线性或逻辑回归等算法的最佳系数，因此，它们在至少部分基于线性/逻辑回归的更复杂技术中也经常发挥作用（如神经网络）。
- en: The general idea of gradient descent methods is to determine a direction and
    magnitude of change in some parameters that will move you in the right direction
    to optimize some measure (such as error). Think about standing on some landscape.
    To move toward lower elevations, you need to take steps in the downward direction.
    This is basically what gradient descent is doing algorithmically when it is optimizing
    parameters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降方法的一般思想是确定某些参数的变化方向和幅度，这将使你朝着正确的方向移动以优化某些度量（如误差）。想象一下站在某个景观上。为了向更低的地势移动，你需要朝下走。这就是梯度下降在优化参数时算法上所做的事情。
- en: 'Let''s gain some more intuition about this process by looking at so-called
    **Stochastic Gradient Descent** (**SGD**), which is an incremental kind of gradient
    descent. If you remember, we actually utilized SGD in our implementation of logistic
    regression in [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*.
    In that example, we implemented the training or fitting of our logistic regression
    parameters as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 通过观察所谓的**随机梯度下降**（**SGD**），我们可以进一步理解这个过程，它是一种增量式的梯度下降方法。如果你还记得，我们在[第5章](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)“分类”中对逻辑回归的实现中实际上使用了SGD。在那个例子中，我们是这样实现逻辑回归参数的训练或拟合的：
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The loop under the `// Iteratively optimize the weights` comment implements
    SGD to optimize the logistic regression parameters. Let's pick apart this loop
    to determine what exactly is happening.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在`// 递归优化权重`注释下的循环实现了SGD来优化逻辑回归参数。让我们分析这个循环，以确定到底发生了什么。
- en: 'First, we calculate the output of our model with the current weights and the
    difference between our prediction and the ideal value (the actual observation,
    that is):'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用当前权重计算模型的输出，并计算我们的预测值与理想值（即实际观察值）之间的差异：
- en: '[PRE1]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then, according to SGD, we are going to calculate an update to our parameters
    (in this case `weights`) according to the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，根据SGD，我们将根据以下公式计算参数（在这种情况下为`权重`）的更新：
- en: '![](img/9585b444-6017-4174-95fd-103c4941f2f9.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9585b444-6017-4174-95fd-103c4941f2f9.png)'
- en: The **gradient** is the mathematical gradient of the cost function in your problem.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '**梯度**是问题中成本函数的数学梯度。'
- en: 'More detailed mathematical information about this quantity can be found here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个量的更详细的数学信息可以在这里找到：
- en: '[http://mathworld.wolfram.com/Gradient.html](http://mathworld.wolfram.com/Gradient.html)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://mathworld.wolfram.com/Gradient.html](http://mathworld.wolfram.com/Gradient.html)'
- en: 'The update can then be applied to the parameters as follows:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 更新可以应用于参数，如下所示：
- en: '![](img/2a207315-7ebb-4ca5-824f-5794482733b1.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2a207315-7ebb-4ca5-824f-5794482733b1.png)'
- en: 'In the case of our logistic regression model, this works out to have the following
    form:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的逻辑回归模型中，这可以表示为以下形式：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This type of SGD is pretty widely used in machine learning. However, in some
    cases, this kind of gradient descent can lead to overfitting or getting stuck
    in local minimums/maximums (rather than finding the global optimum).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的SGD在机器学习中相当广泛地使用。然而，在某些情况下，这种梯度下降可能导致过拟合或陷入局部最小值/最大值（而不是找到全局最优解）。
- en: To address some of these issues, you can utilize a variant of gradient descent
    called **batch gradient descent**. In batch gradient descent, you calculate each
    of the parameter updates based on gradients in all of the training dataset, as
    opposed to a gradient for a particular observation or row of the dataset. This
    helps you prevent overfitting, but it can also be rather slow and have memory
    issues because you need to calculate gradient with respect to a whole dataset
    for each parameter. **Mini-batch** **gradient descent**, which is another variant,
    attempts to maintain some of the benefits of batch gradient descent while being
    more computationally tractable. In mini-batch gradient descent, the gradients
    are calculated on subsets of the training dataset rather than the whole training
    dataset.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，你可以利用一种称为**批量梯度下降**的梯度下降的变体。在批量梯度下降中，你根据整个训练数据集中的梯度来计算每个参数的更新，而不是根据特定观察或数据集的行来计算梯度。这有助于你防止过拟合，但它也可能相当慢，并可能存在内存问题，因为你需要为每个参数计算整个数据集的梯度。**小批量**
    **梯度下降**，这是另一种变体，试图保持批量梯度下降的一些好处，同时更具计算上的可处理性。在小批量梯度下降中，梯度是在训练数据集的子集上计算的，而不是整个训练数据集。
- en: In the case of logistic regression, you may see the use of gradient ascent or
    descent, where gradient ascent is the same thing as gradient descent except that
    it is applied to the negative of the cost function. The logistic cost function
    gives you both of these options as long as you are consistent. This is further
    discussed at [https://stats.stackexchange.com/questions/261573/using-gradient-ascent-instead-of-gradient-descent-for-logistic-regression](https://stats.stackexchange.com/questions/261573/using-gradient-ascent-instead-of-gradient-descent-for-logistic-regression).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归的情况下，你可能会看到梯度上升或下降的使用，其中梯度上升与梯度下降相同，只是它应用于成本函数的负值。逻辑成本函数只要你保持一致，就会给你这两个选项。这进一步讨论在[https://stats.stackexchange.com/questions/261573/using-gradient-ascent-instead-of-gradient-descent-for-logistic-regression](https://stats.stackexchange.com/questions/261573/using-gradient-ascent-instead-of-gradient-descent-for-logistic-regression)。
- en: 'Gradient descent methods are also already implemented by the gonum team in
    `gonum.org/v1/gonum/optimize`. See these docs for more information:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降方法已经被gonum团队在`gonum.org/v1/gonum/optimize`中实现。有关更多信息，请参阅这些文档：
- en: '[https://godoc.org/gonum.org/v1/gonum/optimize#GradientDescent](https://godoc.org/gonum.org/v1/gonum/optimize#GradientDescent)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://godoc.org/gonum.org/v1/gonum/optimize#GradientDescent](https://godoc.org/gonum.org/v1/gonum/optimize#GradientDescent)'
- en: Entropy, information gain, and related methods
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 熵、信息增益和相关方法
- en: In [Chapter 5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*,
    we explored decision tree methods in which models consisted of a tree of if/then
    statements. These if/then portions of the decision tree split the prediction logic
    based on one of the features of the training set. In an example where we were
    trying to classify medical patients into unhealthy or healthy categories, a decision
    tree might first split based on a gender feature, then based on an age feature,
    then based on a weight feature, and so on, eventually landing on healthy or unhealthy.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)，*分类*中，我们探讨了决策树方法，其中模型由一个if/then语句的树组成。决策树的这些if/then部分根据训练集的一个特征来分割预测逻辑。在一个我们试图将医疗患者分类为不健康或健康类别的例子中，决策树可能会首先根据性别特征进行分割，然后根据年龄特征，然后根据体重特征，依此类推，最终落在健康或不健康上。
- en: How does the algorithm choose which features to use first in the decision tree?
    In the preceding example, we could split on gender first, or weight first, and
    any other feature first. We need a way to arrange our splits in an optimal way,
    such that our model makes the best predictions that it can make.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 算法是如何在决策树中首先选择使用哪些特征的？在先前的例子中，我们可以首先根据性别进行分割，或者首先根据权重，然后是任何其他特征。我们需要一种方法来以最佳方式安排我们的分割，以便我们的模型做出最好的预测。
- en: 'Many decision tree model implementations, including the one we used in [Chapter
    5](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml), *Classification*, use a quantity
    called **entropy** and an analysis of **information gain** to build up decision
    trees. To illustrate this process, let''s consider an example. Assume that you
    have the following data about numbers of healthy people versus various characteristics
    of those people:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 许多决策树模型实现，包括我们在[第5章](f0ffd10e-d2c4-41d7-8f26-95c05a30d818.xhtml)“分类”中使用的，使用一个称为**熵**的量和一个**信息增益**的分析来构建决策树。为了说明这个过程，让我们考虑一个例子。假设你有一些关于健康人数与这些人各种特征的数据：
- en: '|  | **Healthy** | **Unhealthy** |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '|  | **健康** | **不健康** |'
- en: '| **Vegan Diet** | 5 | 2 |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **素食饮食** | 5 | 2 |'
- en: '| **Vegetarian Diet** | 4 | 1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **素食饮食** | 4 | 1 |'
- en: '| **Meat Eating Diet** | 3 | 4 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **肉食饮食** | 3 | 4 |'
- en: '|  | **Healthy** | **Unhealthy** |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|  | **健康** | **不健康** |'
- en: '| **Age 40+** | 3 | 5 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **40岁及以上** | 3 | 5 |'
- en: '| **Age < 40** | 9 | 2 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **40岁以下** | 9 | 2 |'
- en: Here, we have two features in our data, Diet and Age, and we would like to build
    a decision tree to predict if people are healthy or not based on Diet and Age.
    To do this, we need to decide whether we should split our decision tree first
    on Age or first on Diet. Notice that we also have a total of 12 healthy people
    and 7 unhealthy people represented in the data.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们数据中有两个特征，饮食和年龄，我们希望构建一个决策树，根据饮食和年龄预测人们是否健康。为此，我们需要决定是否应该首先在年龄上还是饮食上分割我们的决策树。请注意，数据中还有总共12个健康人和7个不健康人。
- en: 'To begin, we will calculate the overall or total entropy of the classes in
    our data. This is defined as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将计算我们数据中类别的整体或总熵。这定义如下：
- en: '![](img/0d24303f-5049-4827-8236-938d7f26a921.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d24303f-5049-4827-8236-938d7f26a921.png)'
- en: 'Here, *p[1]*, *p[2],* and so on, are the probabilities of a first category,
    a second category, and so on. In our particular case (because we have 12 healthy
    people and 7 unhealthy people), our total entropy is as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*p[1]*，*p[2]*，等等，是第一类，第二类等的概率。在我们特定的案例中（因为我们有12个健康人和7个不健康人），我们的总熵如下：
- en: '![](img/4ce3e27c-05ed-47d1-bcf9-44a96c149cce.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4ce3e27c-05ed-47d1-bcf9-44a96c149cce.png)'
- en: This *0.95* measure represents the homogeneity of our health data. It goes between
    0 and 1, with high values corresponding to less homogeneous data.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个*0.95*的度量代表了我们的健康数据的同质性。它介于0和1之间，高值对应于同质性较低的数据。
- en: To determine whether we should split our tree first on Age or first on Diet,
    we will calculate which of these features gives us the most information gain.
    Simply put, we will find the feature that gives us the most homogeneity after
    splitting on that feature, as measured by the preceding entropy. This decrease
    in entropy is called **information gain**.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要确定我们首先应该在年龄上还是饮食上分割我们的树，我们将计算这些特征中哪一个给我们带来最大的信息增益。简单来说，我们将找到在分割该特征后给我们带来最大同质性的特征，这是通过之前的熵来衡量的。这种熵的减少被称为**信息增益**。
- en: 'The information gain for a certain feature in our example is defined as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们例子中某个特征的**信息增益**定义如下：
- en: '![](img/3158bf53-063b-4137-8163-579a8b5b2fea.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3158bf53-063b-4137-8163-579a8b5b2fea.png)'
- en: 'Here, *E(Health, Feature)* is a second measure of entropy with respect to the
    given feature (*Age* or *Diet*). For Diet, this second measure would be calculated
    as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*E(Health, Feature)*是关于给定特征（*年龄*或*饮食*）的熵的第二个度量。对于饮食，这个第二个度量可以这样计算：
- en: '![](img/5c8cb814-8e7c-4157-8b5f-7dea151e83fb.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c8cb814-8e7c-4157-8b5f-7dea151e83fb.png)'
- en: The quantities *p[40+]* and *p[<40]* are the probabilities of having an age
    of *40*+ or *<40* (8/19 and 11/19, respectively). The quantities *E(Health,40+)*
    and *E(Health,<40)* are the health entropies (as defined in the preceding formula)
    but only using the counts corresponding to those Age *40+* and Age *<40*, respectively.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 量*p[40+]*和*p[<40]*是年龄为*40*+或*<40*（8/19和11/19，分别）的概率。量*E(Health,40+)*和*E(Health,<40)*是健康熵（如前公式定义），但只使用与那些年龄*40+*和年龄*<40*对应的计数。
- en: For our example data, the information gain for the Age feature comes out to
    *0.152* and the information gain for the Diet feature comes out to *0.079*. Thus,
    we would choose to split our decision tree on the Age feature first because it
    increases the overall homogeneity of our data the most.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据，年龄特征的信息增益为*0.152*，饮食特征的信息增益为*0.079*。因此，我们会选择首先在年龄特征上分割我们的决策树，因为它最大限度地增加了我们数据的整体同质性。
- en: You can find out more about building decision trees based on entropy at [http://www.saedsayad.com/decision_tree.htm](http://www.saedsayad.com/decision_tree.htm),
    and you can see an example implementation in Go at [https://github.com/sjwhitworth/golearn/blob/master/trees/entropy.go](https://github.com/sjwhitworth/golearn/blob/master/trees/entropy.go).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在[http://www.saedsayad.com/decision_tree.htm](http://www.saedsayad.com/decision_tree.htm)了解更多关于基于熵构建决策树的信息，你还可以在Go语言中看到一个示例实现[https://github.com/sjwhitworth/golearn/blob/master/trees/entropy.go](https://github.com/sjwhitworth/golearn/blob/master/trees/entropy.go)。
- en: Backpropagation
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播
- en: '[Chapter 8](3e51d99f-2f6a-4d4b-876f-3b44f74b9a20.xhtml), *Neural Networks and
    Deep Learning*, included an example of a neural network built from scratch in
    Go. This neural network included an implementation of the backpropagation method
    to train neural networks, which can be found in almost any neural network code.
    We discussed some details in that chapter. However, this method is utilized so
    often that we wanted to go through it step by step here.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '[第8章](3e51d99f-2f6a-4d4b-876f-3b44f74b9a20.xhtml)，*神经网络与深度学习*，包含了一个从头开始构建的神经网络示例。这个神经网络包含了一个反向传播方法的实现，用于训练神经网络，这几乎可以在任何神经网络代码中找到。我们在那一章讨论了一些细节。然而，这个方法被如此频繁地使用，所以我们想在这里一步一步地介绍它。'
- en: 'To train a neural network with backpropagation, we do the following for each
    of a series of epochs:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用反向传播训练神经网络，我们为一系列的每个epoch执行以下操作：
- en: Feed the training data through the neural network to produce output.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据通过神经网络产生输出。
- en: Calculate an error between the expected output and the predicted output.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算期望输出和预测输出之间的误差。
- en: Based on the error, calculate updates for the neural network weights and biases.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据误差，计算神经网络权重和偏置的更新。
- en: Propagate these updates back into the network.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些更新反向传播到网络中。
- en: 'As a reminder, our implementation of this procedure for a network with a single
    hidden layer looked like the following (where `wHidden` and `wOut` are our hidden
    layer and output layer weights and `bHidden` and `bOut` are our hidden layer and
    output layer biases):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，我们对于具有单个隐藏层的网络实现这个过程的代码如下（其中`wHidden`和`wOut`是我们的隐藏层和输出层权重，而`bHidden`和`bOut`是我们的隐藏层和输出层偏置）：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's pick this implementation apart in detail to understand exactly what's
    happening.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细分析这个实现，以了解到底发生了什么。
- en: 'The feed forward process that produces our output does the following:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 产生我们输出的前向过程执行以下操作：
- en: Multiplies the input data by the hidden layer weights, adds the hidden layer
    biases, and applies the sigmoid activation function to calculate the output of
    the hidden layer, `hiddenLayerActivations` (lines 112 to 120 in the preceding
    snippet).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入数据乘以隐藏层权重，加上隐藏层偏置，然后应用sigmoid激活函数来计算隐藏层的输出，即`hiddenLayerActivations`（前一个代码片段的第112到120行）。
- en: Multiples the `hiddenLayerActivations` by the output layer weights, then adds
    the output layer biases, and applies the sigmoid activation function to calculate
    the `output` (lines 122 to 126).
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`hiddenLayerActivations`乘以输出层权重，然后加上输出层偏置，并应用sigmoid激活函数来计算`output`（第122到126行）。
- en: Notice that in the feed forward process, we are starting with the input data
    at the input layer and working our way forward through the hidden layer until
    we reach the output.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前向过程中，我们是始于输入层的输入数据，然后通过隐藏层逐步向前，直到达到输出。
- en: After the feed forward process, we need to calculate optimal updates to our
    weights and biases. As you might expect after going through the gradient descent
    portion of this Appendix, gradient descent is a perfect fit to find these weights
    and biases. Lines 129 through 144 in the preceding snippet implement SGD.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向过程之后，我们需要计算权重和偏置的最佳更新。正如你可能预期的那样，在附录的这一部分经过梯度下降部分之后，梯度下降是找到这些权重和偏置的完美选择。前一个代码片段的第129到144行实现了SGD。
- en: 'Finally, we need to apply these updates backward through the network in lines
    147 through 169\. This is the backward propagation of updates that gives backpropagation
    its name. There isn''t anything too special about this process, we just perform
    the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们需要将这些更新反向应用到网络的第147到169行。这是反向传播更新，使得反向传播得名。这个过程并没有什么特别之处，我们只是执行以下操作：
- en: Apply the calculated updates to the output weights and biases (lines 147 to
    157).
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计算出的更新应用到输出层的权重和偏置（第147到157行）。
- en: Apply the calculated updates to the hidden layer weights and biases (lines 159
    to 169).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将计算出的更新应用到隐藏层权重和偏置（第159到169行）。
- en: Notice how we start at the output and work our way back to the input applying
    the changes.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我们是如何从输出开始，逐步回溯到输入，应用这些变化的。
- en: 'You can find a very detailed discussion of backpropagation, including mathematical
    proofs, here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这里找到关于反向传播的非常详细的讨论，包括数学证明：
- en: '[http://neuralnetworksanddeeplearning.com/chap2.html](http://neuralnetworksanddeeplearning.com/chap2.html)'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '[神经网络与深度学习](http://neuralnetworksanddeeplearning.com/chap2.html)'
- en: Backpropagation started to be widely utilized after a 1986 paper by David Rumelhart,
    Geoffrey Hinton, and Ronald Williams. Although the method is utilized across the
    industry in neural networks, Geoffrey Hinton recently came out to say that he
    is *deeply suspicious* of backpropagation and suggests that we need to work hard
    to find an alternative.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播在1986年David Rumelhart、Geoffrey Hinton和Ronald Williams发表的一篇论文之后开始被广泛使用。尽管这种方法在神经网络行业中得到了广泛应用，但Geoffrey
    Hinton最近公开表示他对反向传播持**深深怀疑**的态度，并建议我们需要努力寻找替代方案。
