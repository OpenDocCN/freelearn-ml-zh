- en: '*Chapter 6*: XGBoost Hyperparameters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*：XGBoost超参数'
- en: XGBoost has many hyperparameters. XGBoost base learner hyperparameters incorporate
    all decision tree hyperparameters as a starting point. There are gradient boosting
    hyperparameters, since XGBoost is an enhanced version of gradient boosting. Hyperparameters
    unique to XGBoost are designed to improve upon accuracy and speed. However, trying
    to tackle all XGBoost hyperparameters at once can be dizzying.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost有许多超参数。XGBoost的基础学习器超参数包含所有决策树的超参数作为起点。由于XGBoost是梯度提升的增强版，因此也有梯度提升的超参数。XGBoost特有的超参数旨在提升准确性和速度。然而，一次性尝试解决所有XGBoost超参数可能会让人感到头晕。
- en: In [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision
    Trees in Depth*, we reviewed and applied base learner hyperparameters such as
    `max_depth`, while in [*Chapter 4*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093),
    *From Gradient Boosting to XGBoost*, we applied important XGBoost hyperparameters,
    including `n_estimators` and `learning_rate`. We will revisit these hyperparameters
    in this chapter in the context of XGBoost. Additionally, we will also learn about
    novel XGBoost hyperparameters such as `gamma` and a technique called **early stopping**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)中，*决策树深入剖析*，我们回顾并应用了基础学习器超参数，如`max_depth`，而在[*第4章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)中，*从梯度提升到XGBoost*，我们应用了重要的XGBoost超参数，包括`n_estimators`和`learning_rate`。我们将在本章中再次回顾这些超参数，并介绍一些新的XGBoost超参数，如`gamma`，以及一种叫做**早停法**的技术。
- en: 'In this chapter, to gain proficiency in fine-tuning XGBoost hyperparameters,
    we will cover the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，为了提高XGBoost超参数微调的熟练度，我们将讨论以下主要主题：
- en: Preparing data and base models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据和基础模型
- en: Tuning core XGBoost hyperparameters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整核心XGBoost超参数
- en: Applying early stopping
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用早停法
- en: Putting it all together
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: Technical requirements
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06)找到。
- en: Preparing data and base models
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据和基础模型
- en: 'Before introducing and applying XGBoost hyperparameters, let''s prepare by
    doing the following:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍和应用XGBoost超参数之前，让我们做好以下准备：
- en: Getting the **heart disease dataset**
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取**心脏病数据集**
- en: Building an `XGBClassifier` model
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建`XGBClassifier`模型
- en: Implementing `StratifiedKFold`
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现`StratifiedKFold`
- en: Scoring a **baseline XGBoost model**
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对**基准XGBoost模型**进行评分
- en: Combining `GridSearchCV` with `RandomizedSearchCV` to form one powerful function
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将`GridSearchCV`与`RandomizedSearchCV`结合，形成一个强大的函数
- en: Good preparation is essential for gaining accuracy, consistency, and speed when
    fine-tuning hyperparameters.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的准备对于在微调超参数时获得准确性、一致性和速度至关重要。
- en: The heart disease dataset
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 心脏病数据集
- en: 'The dataset used throughout this chapter is the heart disease dataset originally
    presented in [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision
    Trees in Depth*. We have chosen the same dataset to maximize the time spent doing
    hyperparameter fine-tuning, and to minimize the time spent on data analysis. Let''s
    begin the process:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章使用的数据集是最初在[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)中提出的心脏病数据集，*决策树深入剖析*。我们选择相同的数据集，以最大化超参数微调的时间，并最小化数据分析的时间。让我们开始这个过程：
- en: 'Go to [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06)
    to load `heart_disease.csv` into a DataFrame and display the first five rows.
    Here is the code:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter06)，加载`heart_disease.csv`到DataFrame中，并显示前五行。以下是代码：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The result should look as follows:'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果应如下所示：
- en: '![Figure 6.1 – The first five rows](img/B15551_06_01.jpg)'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图6.1 - 前五行数据](img/B15551_06_01.jpg)'
- en: Figure 6.1 – The first five rows
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图6.1 - 前五行数据
- en: The last column, **target**, is the target column, where **1** indicates presence,
    meaning the patient has a heart disease, and **2** indicates absence. For detailed
    information on the other columns, visit [https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)
    at the UCI Machine Learning Repository, or see [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最后一列，**target**，是目标列，**1**表示存在，表示患者患有心脏病，**2**表示不存在。有关其他列的详细信息，请访问[https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)上的UCI机器学习库，或参见[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入剖析》。
- en: 'Now, check `df.info()` to ensure that the data is all numerical with no null
    values:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，检查`df.info()`以确保数据全部为数值型且没有空值：
- en: '[PRE1]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Here is the output:'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这是输出结果：
- en: '[PRE2]'
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Since all data points are non-null and numerical, the data is machine learning-ready.
    It's time to build a classifier.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有数据点都是非空且数值型的，数据已经准备好用于机器学习。现在是时候构建一个分类器了。
- en: XGBClassifier
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: XGBClassifier
- en: Before tuning hyperparameters, let's build a classifier so that we can obtain
    a baseline score as a starting point.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整超参数之前，我们先构建一个分类器，以便获得一个基准评分作为起点。
- en: 'To build an XGBoost classifier, follow these steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 构建XGBoost分类器的步骤如下：
- en: 'Download `XGBClassifier` and `accuracy_score` from their respective libraries.
    The code is as follows:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从各自的库中下载`XGBClassifier`和`accuracy_score`。代码如下：
- en: '[PRE3]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Declare `X` as the predictor columns and `y` as the target column, where the
    last row is the target column:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明`X`为预测列，`y`为目标列，其中最后一行是目标列：
- en: '[PRE4]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Initialize `XGBClassifier` with the `booster=''gbtree''` and `objective=''binary:logistic''`
    defaults along with `random_state=2`:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`booster='gbtree'`和`objective='binary:logistic'`的默认设置初始化`XGBClassifier`，并将`random_state=2`：
- en: '[PRE5]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The `'gbtree'` booster, the base learner, is a gradient boosted tree. The `'binary:logistic'`
    objective is standard for binary classification in determining the loss function.
    Although `XGBClassifier` includes these values by default, we include them here
    to gain familiarity in preparation of modifying them in later chapters.
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`''gbtree''`提升器，即基本学习器，是一个梯度提升树。`''binary:logistic''`目标是二分类中常用的标准目标，用于确定损失函数。虽然`XGBClassifier`默认包含这些值，但我们在此明确指出，以便熟悉这些设置，并为后续章节的修改做准备。'
- en: 'To score the baseline model, import `cross_val_score` and `numpy` to fit, score,
    and display results:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了评估基准模型，导入`cross_val_score`和`numpy`，以便进行拟合、评分并显示结果：
- en: '[PRE6]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The accuracy score is as follows:'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率如下：
- en: '[PRE7]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: An accuracy score of 81% is an excellent starting point, considerably higher
    than the 76% cross-validation obtained by `DecisionTreeClassifier` in [*Chapter
    2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047), *Decision Trees in Depth*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 81%的准确率是一个非常好的起点，远高于在[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入剖析》中，`DecisionTreeClassifier`通过交叉验证获得的76%。
- en: We used `cross_val_score` here, and we will use `GridSearchCV` to tune hyperparameters.
    Next, let's find a way to ensure that the test folds are the same using `StratifiedKFold`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了`cross_val_score`，并将使用`GridSearchCV`来调整超参数。接下来，让我们找到一种方法，确保使用`StratifiedKFold`时测试折叠保持一致。
- en: StratifiedKFold
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: StratifiedKFold
- en: When fine-tuning hyperparameters, `GridSearchCV` and `RandomizedSearchCV` are
    the standard options. An issue from [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047),
    *Decision Trees in Depth*, is that `cross_val_score` and `GridSearchCV`/`RandomizedSearchCV`
    do not split data the same way.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在调整超参数时，`GridSearchCV`和`RandomizedSearchCV`是标准选项。来自[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《决策树深入剖析》中的一个问题是，`cross_val_score`和`GridSearchCV`/`RandomizedSearchCV`在划分数据时方式不同。
- en: One solution is to use `StratifiedKFold` whenever cross-validation is used.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是在使用交叉验证时使用`StratifiedKFold`。
- en: A stratified fold includes the same percentage of target values in each fold.
    If a dataset contains 60% 1s and 40% 0s in the target column, each stratified
    test set contains 60% 1s and 40% 0s. When folds are random, it's possible that
    one test set contains a 70-30 split while another contains a 50-50 split of target
    values.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分层折叠法在每一折中都包含相同百分比的目标值。如果数据集中目标列中包含60%的1和40%的0，则每个分层测试集都包含60%的1和40%的0。当折叠是随机的时，可能会出现一个测试集包含70%-30%的划分，而另一个测试集包含50%-50%的目标值划分。
- en: Tip
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When using `train_test_split`, the shuffle and stratify parameters use defaults
    to stratify the data for you. See [https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)
    for general information.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`train_test_split`时，shuffle和stratify参数使用默认设置，以帮助你对数据进行分层抽样。有关一般信息，请参见[https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)。
- en: 'To use `StratifiedKFold`, do the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`StratifiedKFold`时，执行以下操作：
- en: 'Implement `StratifiedKFold` from `sklearn.model_selection`:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.model_selection`中实现`StratifiedKFold`：
- en: '[PRE8]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, define the number of folds as `kfold` by selecting `n_splits=5`, `shuffle=True`,
    and `random_state=2` as the `StratifiedKFold` parameters. Note that `random_state`
    provides a consistent ordering of indices, while `shuffle=True` allows rows to
    be initially shuffled:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，通过选择`n_splits=5`、`shuffle=True`和`random_state=2`作为`StratifiedKFold`的参数来定义折叠数为`kfold`。请注意，`random_state`提供一致的索引排序，而`shuffle=True`允许初始时对行进行随机排序：
- en: '[PRE9]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `kfold` variable can now be used inside `cross_val_score`, `GridSeachCV`,
    and `RandomizedSearchCV` to ensure consistent results.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在可以在`cross_val_score`、`GridSearchCV`和`RandomizedSearchCV`中使用`kfold`变量，以确保结果的一致性。
- en: Now, let's return to `cross_val_score` using `kfold` so that we have an appropriate
    baseline for comparison.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到`cross_val_score`，使用`kfold`，这样我们就有了一个适当的基准进行比较。
- en: Baseline model
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基准模型
- en: 'Now that we have a method for obtaining consistent folds, it''s time to score
    an official baseline model using `cv=kfold` inside `cross_val_score`. The code
    is as follows:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了一种获取一致折叠的方法，是时候在`cross_val_score`中使用`cv=kfold`来评分一个正式的基准模型。代码如下：
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The accuracy score is as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率如下：
- en: '[PRE11]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The score has gone down. What does this mean?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 分数下降了。这意味着什么？
- en: It's important not to become too invested in obtaining the highest possible
    score. In this case, we trained the same `XGBClassifier` model on different folds
    and obtained different scores. This shows the importance of being consistent with
    test folds when training models, and why the score is not necessarily the most
    important thing. Although when choosing between models, obtaining the best possible
    score is an optimal strategy, the difference in scores here reveals that the model
    is not necessarily better. In this case, the two models have the same hyperparameters,
    and the difference in scores is attributed to the different folds.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是不要过于执着于获得最高的评分。在这种情况下，我们在不同的折叠上训练了相同的`XGBClassifier`模型，并获得了不同的分数。这显示了在训练模型时，保持测试折叠的一致性的重要性，也说明了为什么分数不一定是最重要的。虽然在选择模型时，获得最好的评分是一个最佳策略，但这里的分数差异表明模型并不一定更好。在这种情况下，两个模型的超参数相同，分数差异归因于不同的折叠。
- en: The point here is to use the same folds to obtain new scores when fine-tuning
    hyperparameters with `GridSearchCV` and `RandomizedSearchCV` so that the comparison
    of scores is fair.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键是，当使用`GridSearchCV`和`RandomizedSearchCV`微调超参数时，使用相同的折叠来获得新的分数，以确保分数的比较是公平的。
- en: Combining GridSearchCV and RandomizedSearchCV
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合`GridSearchCV`和`RandomizedSearchCV`
- en: '`GridSearchCV` searches all possible combinations in a hyperparameter grid
    to find the best results. `RandomizedSearchCV` selects 10 random hyperparameter
    combinations by default. `RandomizedSearchCV` is typically used when `GridSearchCV`
    becomes unwieldy because there are too many hyperparameter combinations to exhaustively
    check each one.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV`在超参数网格中搜索所有可能的组合以找到最佳结果。`RandomizedSearchCV`默认选择10个随机超参数组合。通常，当`GridSearchCV`变得繁琐，无法逐一检查所有超参数组合时，`RandomizedSearchCV`就会被使用。'
- en: 'Instead of writing two separate functions for `GridSearchCV` and `RandomizedSearchCV`,
    we will combine them into one streamlined function with the following steps:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`GridSearchCV`和`RandomizedSearchCV`合并成一个精简的函数，而不是为它们写两个单独的函数，步骤如下：
- en: 'Import `GridSearchCV` and `RandomizedSearchCV` from `sklearn.model_selection`:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`sklearn.model_selection`导入`GridSearchCV`和`RandomizedSearchCV`：
- en: '[PRE12]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Define a `grid_search` function with the `params` dictionary as input, along
    with `random=False`:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个`grid_search`函数，以`params`字典作为输入，`random=False`：
- en: '[PRE13]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Initialize an XGBoost classifier using the standard defaults:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用标准默认值初始化XGBoost分类器：
- en: '[PRE14]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If `random=True`, initialize `RandomizedSearchCV` with `xgb` and the `params`
    dictionary. Set `n_iter=20` to allow 20 random combinations instead of 10\. Otherwise,
    initialize `GridSearchCV` with the same inputs. Make sure to set `cv=kfold` for
    consistent results:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`random=True`，则用`xgb`和`params`字典初始化`RandomizedSearchCV`。设置`n_iter=20`，以允许20个随机组合，而不是10个。否则，用相同的输入初始化`GridSearchCV`。确保设置`cv=kfold`以确保结果的一致性：
- en: '[PRE15]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Fit `X` and `y` to the `grid` model:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`X`和`y`拟合到`grid`模型：
- en: '[PRE16]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Obtain and print `best_params_`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取并打印`best_params_`：
- en: '[PRE17]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Obtain and print `best_score_`:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取并打印`best_score_`：
- en: '[PRE18]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The `grid_search` function can now be used to fine-tune all hyperparameters.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用`grid_search`函数来微调所有超参数。
- en: Tuning XGBoost hyperparameters
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整XGBoost超参数
- en: There are many XGBoost hyperparameters, some of which have been introduced in
    previous chapters. The following table summarizes key XGBoost hyperparameters,
    most of which we cover in this book.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost有许多超参数，其中一些在前几章已经介绍。下表总结了关键的XGBoost超参数，我们在本书中大部分进行了讨论。
- en: Note
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The XGBoost hyperparameters presented here are not meant to be exhaustive, but
    they are meant to be comprehensive. For a complete list of hyperparameters, read
    the official documentation, *XGBoost Parameters*, at [https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 此处展示的XGBoost超参数并非详尽无遗，而是力求全面。要查看完整的超参数列表，请阅读官方文档中的*XGBoost参数*，[https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)。
- en: 'Following the table, further explanations and examples are provided:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 紧随表格后，提供了进一步的解释和示例：
- en: '![Figure 6.2 – XGBoost hyperparameter table](img/B15551_06_02.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – XGBoost超参数表](img/B15551_06_02.jpg)'
- en: Figure 6.2 – XGBoost hyperparameter table
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – XGBoost超参数表
- en: Now that the key XGBoost hyperparameters have been presented, let's get to know
    them better by tuning them one at a time.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 既然已经展示了关键的XGBoost超参数，让我们逐一调优它们，进一步了解它们的作用。
- en: Applying XGBoost hyperparameters
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用XGBoost超参数
- en: The XGBoost hyperparameters presented in this section are frequently fine-tuned
    by machine learning practitioners. After a brief explanation of each hyperparameter,
    we will test standard variations using the `grid_search` function defined in the
    previous section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中展示的XGBoost超参数通常由机器学习从业者进行微调。每个超参数简要解释后，我们将使用前面定义的`grid_search`函数测试标准的变动。
- en: n_estimators
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: n_estimators
- en: Recall that `n_estimators` provides the number of trees in the ensemble. In
    the case of XGBoost, `n_estimators` is the number of trees trained on the residuals.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，`n_estimators`表示集成中树的数量。在XGBoost中，`n_estimators`是训练残差的树的数量。
- en: 'Initialize a grid search of `n_estimators` with the default of `100`, then
    double the number of trees through `800` as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用默认的`100`初始化`n_estimators`的网格搜索，然后将树的数量翻倍至`800`，如下所示：
- en: '[PRE19]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE20]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Since our dataset is small, increasing `n_estimators` did not produce better
    results. One strategy for finding an ideal value of `n_estimators` is discussed
    in the *Applying early stopping* section in this chapter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的数据集较小，增加`n_estimators`并没有带来更好的结果。在本章的*应用早期停止*部分中讨论了寻找理想`n_estimators`值的策略。
- en: learning_rate
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: learning_rate
- en: '`learning_rate` shrinks the weights of trees for each round of boosting. By
    lowering `learning_rate`, more trees are required to produce better scores. Lowering
    `learning_rate` prevents overfitting because the size of the weights carried forward
    is smaller.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`learning_rate`会缩小每一轮提升中树的权重。通过降低`learning_rate`，需要更多的树来获得更好的得分。降低`learning_rate`能防止过拟合，因为传递下来的权重较小。'
- en: 'A default value of `0.3` is used, though previous versions of scikit-learn
    have used `0.1`. Here is a starting range for `learning_rate` as placed inside
    our `grid_search` function:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值为`0.3`，尽管以前版本的scikit-learn使用的是`0.1`。这里是`learning_rate`的一个起始范围，已放入我们的`grid_search`函数中：
- en: '[PRE21]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The output is as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE22]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Changing the learning rate has resulted in a slight increase. As described in
    [*Chapter 4*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093), *From Gradient
    Boosting to XGBoost*, lowering `learning_rate` may be advantageous when `n_estimators`
    goes up.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 改变学习率导致了轻微的增加。如在[*第4章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)中所述，*从梯度提升到XGBoost*，当`n_estimators`增大时，降低`learning_rate`可能会带来好处。
- en: max_depth
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: max_depth
- en: '`max_depth` determines the length of the tree, equivalent to the number of
    rounds of splitting. Limiting `max_depth` prevents overfitting because the individual
    trees can only grow as far as `max_depth` allows. XGBoost provides a default `max_depth`
    value of six:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`决定了树的长度，相当于分裂的轮次。限制`max_depth`可以防止过拟合，因为单棵树的生长受到`max_depth`的限制。XGBoost默认的`max_depth`值为六：'
- en: '[PRE23]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Changing `max_depth` from `6` to `2` gave a better score. The lower value for
    `max_depth` means variance has been reduced.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将`max_depth`从`6`调整到`2`后得到了更好的分数。较低的`max_depth`值意味着方差已被减少。
- en: gamma
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: gamma
- en: 'Known as a `gamma` provides a threshold that nodes must surpass before making
    further splits according to the loss function. There is no upper limit to the
    value of `gamma`. The default is `0`, and anything over `10` is considered very
    high. Increasing `gamma` results in a more conservative model:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 被称为`gamma`的值为节点提供了一个阈值，只有超过这个阈值后，节点才会根据损失函数进行进一步的分裂。`gamma`没有上限，默认值为`0`，而大于`10`的值通常认为非常高。增大`gamma`会使模型变得更加保守：
- en: '[PRE25]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Changing `gamma` from `0` to `0.5` has resulted in a slight improvement.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将`gamma`从`0`调整到`0.5`带来了轻微的改善。
- en: min_child_weight
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: min_child_weight
- en: '`min_child_weight` refers to the minimum sum of weights required for a node
    to split into a child. If the sum of the weights is less than the value of `min_child_weight`,
    no further splits are made. `min_child_weight` reduces overfitting by increasing
    its value:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_child_weight`表示一个节点进行分裂成子节点所需的最小权重和。如果权重和小于`min_child_weight`的值，则不会进行进一步的分裂。通过增大`min_child_weight`的值，可以减少过拟合：'
- en: '[PRE27]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: A slight adjustment to `min_child_weight` gives the best results yet.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 对`min_child_weight`的微调给出了最佳结果。
- en: subsample
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: subsample
- en: 'The `subsample` hyperparameter limits the percentage of training instances
    (rows) for each boosting round. Decreasing `subsample` from 100% reduces overfitting:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`subsample`超参数限制了每次提升轮次的训练实例（行）百分比。将`subsample`从100%降低有助于减少过拟合：'
- en: '[PRE29]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The output is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The score has improved by a slight amount once again, indicating a small presence
    of overfitting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 分数再次略有提高，表明存在轻微的过拟合现象。
- en: colsample_bytree
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: colsample_bytree
- en: 'Similar to `subsample`, `colsample_bytree` randomly selects particular columns
    according to the given percentage. `colsample_bytree` is useful for limiting the
    influence of columns and reducing variance. Note that `colsample_bytree` takes
    a percentage as input, not the number of columns:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`subsample`，`colsample_bytree`根据给定的百分比随机选择特定的列。`colsample_bytree`有助于限制列的影响并减少方差。请注意，`colsample_bytree`接受的是百分比作为输入，而不是列的数量：
- en: '[PRE31]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE32]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Gains here are minimal at best. You are encouraged to try `colsample_bylevel`
    and `colsample_bynode` on your own. `colsample_bylevel` randomly selects columns
    for each tree depth, and `colsample_bynode` randomly selects columns when evaluating
    each tree split.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里的增益最多也只是微乎其微。建议你尝试自行使用`colsample_bylevel`和`colsample_bynode`。`colsample_bylevel`在每一层深度随机选择列，而`colsample_bynode`则在评估每个树的分裂时随机选择列。
- en: Fine-tuning hyperparameters is an art and a science. As with both disciplines,
    varied approaches work. Next, we will look into early stopping as a specific strategy
    for fine-tuning `n_estimators`.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 微调超参数既是一门艺术，也是一门科学。与这两种学科一样，采取不同的策略都会有效。接下来，我们将探讨早停法作为微调`n_estimators`的一种特定策略。
- en: Applying early stopping
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用早停法
- en: Early stopping is a general method to limit the number of training rounds in
    iterative machine learning algorithms. In this section, we look at `eval_set`,
    `eval_metric`, and `early_stopping_rounds` to apply early stopping.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 早停法是一种通用方法，用于限制迭代机器学习算法的训练轮次。在本节中，我们将探讨如何通过`eval_set`、`eval_metric`和`early_stopping_rounds`来应用早停法。
- en: What is early stopping?
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是早停法？
- en: Early stopping provides a limit to the number of rounds that iterative machine
    learning algorithms train on. Instead of predefining the number of training rounds,
    early stopping allows training to continue until *n* consecutive rounds fail to
    produce any gains, where *n* is a number decided by the user.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 早停法为迭代机器学习算法的训练轮次提供了限制。与预定义训练轮次不同，早停法允许训练继续，直到*n*次连续的训练轮次未能带来任何增益，其中*n*是由用户决定的数字。
- en: It doesn't make sense to only choose multiples of 100 when looking for `n_estimators`.
    It's possible that the best value is 737 instead of 700\. Finding a value this
    precise manually can be tiring, especially when hyperparameter adjustments may
    require changes down the road.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 仅选择`n_estimators`的100的倍数是没有意义的。也许最佳值是737而不是700。手动找到这么精确的值可能会很累人，特别是当超参数调整可能需要在后续进行更改时。
- en: With XGBoost, a score may be determined after each boosting round. Although
    scores go up and down, eventually scores will level off or move in the wrong direction.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在XGBoost中，每轮增强后可能会确定一个得分。尽管得分会上下波动，但最终得分将趋于稳定或朝错误方向移动。
- en: A peak score is reached when all subsequent scores fail to provide any gains.
    You determine the peak after 10, 20, or 100 training rounds fail to improve upon
    the score. You choose the number of rounds.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 当所有后续得分未提供任何增益时，达到峰值分数。在经过10、20或100个训练轮次未能改进得分后，您会确定峰值。您可以选择轮次数。
- en: In early stopping, it's important to give the model sufficient time to fail.
    If the model stops too early, say, after five rounds of no improvement, the model
    may miss general patterns that it could pick up on later. As with deep learning,
    where early stopping is used frequently, gradient boosting needs sufficient time
    to find intricate patterns within data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在早停策略中，给予模型足够的失败时间是很重要的。如果模型停止得太早，比如在连续五轮没有改进后停止，那么模型可能会错过稍后可以捕捉到的一般模式。与深度学习类似，早停策略经常被使用，梯度提升需要足够的时间来在数据中找到复杂的模式。
- en: For XGBoost, `early_stopping_rounds` is the key parameter for applying early
    stopping. If `early_stopping_rounds=10`, the model will stop training after 10
    consecutive training rounds fail to improve the model. Similarly, if `early_stopping_rounds=100`,
    training continues until 100 consecutive rounds fail to improve the model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于XGBoost，`early_stopping_rounds`是应用早停策略的关键参数。如果`early_stopping_rounds=10`，则模型将在连续10个训练轮次未能改进模型后停止训练。类似地，如果`early_stopping_rounds=100`，则训练将持续直到连续100轮未能改进模型。
- en: Now that you understand what early stopping is, let's take a look at `eval_set`
    and `eval_metric`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您了解了什么是早停策略后，让我们来看看`eval_set`和`eval_metric`。
- en: eval_set and eval_metric
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: eval_set和eval_metric
- en: '`early_stopping_rounds` is not a hyperparameter, but a strategy for optimizing
    the `n_estimators` hyperparameter.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`不是一个超参数，而是优化`n_estimators`超参数的策略。'
- en: Normally when choosing hyperparameters, a test score is given after all boosting
    rounds are complete. To use early stopping, we need a test score after each round.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 通常在选择超参数时，会在所有增强轮次完成后给出测试分数。要使用早停策略，我们需要在每一轮后得到一个测试分数。
- en: '`eval_metric` and `eval_set` may be used as parameters for `.fit` to generate
    test scores for each training round. `eval_metric` provides the scoring method,
    commonly `''error''` for classification, and `''rmse''` for regression. `eval_set`
    provides the test to be evaluated, commonly `X_test` and `y_test`.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 可以将`eval_metric`和`eval_set`用作`.fit`的参数，以生成每个训练轮次的测试分数。`eval_metric`提供评分方法，通常为分类时的`'error'`和回归时的`'rmse'`。`eval_set`提供要评估的测试集，通常为`X_test`和`y_test`。
- en: 'The following six steps display an evaluation metric for each round of training
    with the default `n_estimators=100`:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 以下六个步骤显示了每轮训练的评估指标，其中默认`n_estimators=100`：
- en: 'Split the data into training and test sets:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集：
- en: '[PRE33]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Initialize the model:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化模型：
- en: '[PRE34]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Declare `eval_set`:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明`eval_set`：
- en: '[PRE35]'
  id: totrans-161
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Declare `eval_metric`:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 声明`eval_metric`：
- en: '[PRE36]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Fit the model with `eval_metric` and `eval_set`:'
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`eval_metric`和`eval_set`拟合模型：
- en: '[PRE37]'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Check the final score:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查最终得分：
- en: '[PRE38]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is the truncated output:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是截断的输出：
- en: '[PRE39]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Do not get too excited about the score as we have not used cross-validation.
    In fact, we know that `StratifiedKFold` cross-validation gives a mean accuracy
    of 78% when `n_estimators=100`. The disparity in scores comes from the difference
    in test sets.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 不要对得分过于激动，因为我们尚未使用交叉验证。事实上，我们知道当`n_estimators=100`时，`StratifiedKFold`交叉验证给出的平均准确率为78%。得分差异来自于测试集的不同。
- en: early_stopping_rounds
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: early_stopping_rounds
- en: '`early_stopping_rounds` is an optional parameter to include with `eval_metric`
    and `eval_set` when fitting a model.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`early_stopping_rounds`是在拟合模型时与`eval_metric`和`eval_set`一起使用的可选参数。'
- en: Let's try `early_stopping_rounds=10`.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试`early_stopping_rounds=10`。
- en: 'The previous code is repeated with `early_stopping_rounds=10` added in:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以添加`early_stopping_rounds=10`的方式重复了上述代码：
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output is as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '[PRE41]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The result may come as a surprise. Early stopping reveals that `n_estimators=2`
    gives the best result, which may be an account of the test fold.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能会让人惊讶。早停止显示`n_estimators=2`给出了最佳结果，这可能是测试折叠的原因。
- en: Why only two trees? By only giving the model 10 rounds to improve upon accuracy,
    it's possible that patterns within the data have not yet been discovered. However,
    the dataset is very small, so it's possible that two boosting rounds gives the
    best possible result.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么只有两棵树？只给模型10轮来提高准确性，可能数据中的模式尚未被发现。然而，数据集非常小，因此两轮提升可能给出了最佳结果。
- en: A more thorough approach is to use larger values, say, `n_estimators = 5000`
    and `early_stopping_rounds=100`.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 一个更彻底的方法是使用更大的值，比如`n_estimators = 5000`和`early_stopping_rounds=100`。
- en: By setting `early_stopping_rounds=100`, you are guaranteed to reach the default
    of `100` boosted trees presented by XGBoost.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置`early_stopping_rounds=100`，您将确保达到XGBoost提供的默认`100`个提升树。
- en: 'Here is the code that gives a maximum of 5,000 trees and that will stop after
    100 consecutive rounds fail to find any improvement:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一段代码，最多生成5,000棵树，并在连续100轮未找到任何改进时停止：
- en: '[PRE42]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Here is the truncated output:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是截断的输出：
- en: '[PRE43]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: After 100 rounds of boosting, the score provided by two trees remains the best.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在100轮提升后，两棵树提供的分数仍然是最佳的。
- en: As a final note, consider that early stopping is particularly useful for large
    datasets when it's unclear how high you should aim.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点要注意的是，早停止在大型数据集中特别有用，当不清楚应该瞄准多高时。
- en: Now, let's use the results from early stopping with all the hyperparameters
    previously tuned to generate the best possible model.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用早停止的结果，以及之前调整的所有超参数来生成最佳模型。
- en: Combining hyperparameters
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结合超参数
- en: It's time to combine all the components of this chapter to improve upon the
    78% score obtained through cross-validation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将本章的所有组件结合起来，以提高通过交叉验证获得的78%分数。
- en: As you know, there is no one-size-fits-all approach to hyperparameter fine-tuning.
    One approach is to input all hyperparameter ranges with `RandomizedSearchCV`.
    A more systematic approach is to tackle hyperparameters one at a time, using the
    best results for subsequent iterations. All approaches have advantages and limitations.
    Regardless of strategy, it's essential to try multiple variations and make adjustments
    when the data comes in.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您所知，没有一种适合所有情况的超参数微调方法。一种方法是使用`RandomizedSearchCV`输入所有超参数范围。更系统化的方法是逐个处理超参数，使用最佳结果进行后续迭代。所有方法都有优势和局限性。无论采取何种策略，尝试多种变化并在数据到手时进行调整是至关重要的。
- en: One hyperparameter at a time
  id: totrans-192
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一次调整一个超参数
- en: Using a systematic approach, we add one hyperparameter at a time, aggregating
    results along the way.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 使用系统化的方法，我们一次添加一个超参数，并在途中聚合结果。
- en: n_estimators
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: n_estimators
- en: 'Even though the `n_estimators` value of `2` gave the best result, it''s worth
    trying a range on the `grid_search` function, which uses cross-validation:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`n_estimators`值为`2`给出了最佳结果，但值得在`grid_search`函数上尝试一系列值，该函数使用交叉验证：
- en: '[PRE44]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE45]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: It's no surprise that `n_estimators=50`, between the previous best value of
    2, and the default of 100, gives the best result. Since cross-validation was not
    used in early stopping, the results here are different.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么奇怪的，`n_estimators=50`，介于先前最佳值2和默认值100之间，给出了最佳结果。由于早停止没有使用交叉验证，这里的结果是不同的。
- en: max_depth
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最大深度
- en: 'The `max_depth` hyperparameter determines the length of each tree. Here is
    a nice range:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`超参数确定每棵树的长度。这里是一个不错的范围：'
- en: '[PRE46]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The output is as follows:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE47]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: This is a very substanial gain. A tree with a depth of 1 is called a **decision
    tree stump**. We have gained four percentage points from our baseline model by
    adjusting just two hyperparameters.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常可观的收益。深度为1的树称为**决策树桩**。通过调整只有两个超参数，我们从基线模型中获得了四个百分点的增长。
- en: 'A limitation with the approach of keeping the top values is that we may miss
    out on better combinations. Perhaps `n_estimators=2` or `n_esimtators=100` gives
    better results in conjunction with `max_depth`. Let''s find out:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 保留顶级值的方法的局限性在于我们可能会错过更好的组合。也许`n_estimators=2`或`n_estimators=100`与`max_depth`结合会给出更好的结果。让我们看看：
- en: '[PRE48]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The output is as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE49]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '`n_estimators=50` and `max_depth=1` still give the best results, so we will
    use them going forward, returning to our early stopping analysis later.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '`n_estimators=50`和`max_depth=1`仍然给出了最佳结果，因此我们将继续使用它们，并稍后回到我们的早停止分析。'
- en: learning_rate
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习率
- en: 'Since `n_esimtators` is reasonably low, adjusting `learning_rate` may improve
    results. Here is a standard range:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`n_estimators`合理较低，调整`learning_rate`可能会改善结果。以下是一个标准范围：
- en: '[PRE50]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE51]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This is the same score as previously obtained. Note that a `learning_rate` value
    of 0.3 is the default value provided by XGBoost.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是与之前获得的得分相同的结果。请注意，`learning_rate`值为0.3是XGBoost提供的默认值。
- en: min_child_weight
  id: totrans-217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: min_child_weight
- en: 'Let''s see whether adjusting the sum of weights required to split into child
    nodes increases the score:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看调整分裂为子节点所需的权重总和是否能提高得分：
- en: '[PRE52]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE53]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: In this case, the best score is the same. Note that 1 is the default for `min_child_weight`.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，最佳得分保持不变。请注意，`min_child_weight`的默认值是1。
- en: subsample
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: subsample
- en: 'If reducing variance is beneficial, `subsample` may work by limiting the percentage
    of samples. In this case, however, there are only 303 samples to begin with, and
    a small number of samples makes it difficult to adjust hyperparameters to improve
    scores. Here is the code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果减少方差是有益的，`subsample`可能通过限制样本的百分比来起作用。然而，在此情况下，初始只有303个样本，样本量较少使得调整超参数以提高得分变得困难。以下是代码：
- en: '[PRE54]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The output is as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE55]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Still no gains. At this point, you may be wondering whether new gains would
    have continued with `n_esimtators=2`.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然没有提升。此时，你可能会想，若使用`n_estimators=2`，是否会继续带来新的提升。
- en: Let's find out by using a comprehensive grid search of the values used thus
    far.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过使用到目前为止所使用的值进行全面的网格搜索，找出结果。
- en: '[PRE56]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'The output is as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE57]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: It's not surprising that a classifier with only two trees performs worse. Even
    though the initial scores were better, it does not go through enough iterations
    for the hyperparameters to make significant adjustments.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一台仅有两棵树的分类器表现较差并不令人意外。尽管初始得分较高，但它没有经过足够的迭代，导致超参数没有得到显著调整。
- en: Hyperparameter adjustments
  id: totrans-234
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数调整
- en: When shifting directions with hyperparameters, `RandomizedSearchCV` is useful
    due to the extensive range of inputs.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当调整超参数方向时，`RandomizedSearchCV`非常有用，因为它涵盖了广泛的输入范围。
- en: 'Here is a range of hyperparameter values combining new inputs with previous
    knowledge. Limiting ranges with `RandomizedSearchCV` increases the odds of finding
    the best combination. Recall that `RandomizedSearchCV` is useful when the total
    number of combinations is too time-consuming for a grid search. There are 4,500
    possible combinations with the following options:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个将新输入与先前知识结合的超参数值范围。通过`RandomizedSearchCV`限制范围增加了找到最佳组合的机会。回想一下，`RandomizedSearchCV`在总组合数量过多以至于网格搜索太耗时时特别有用。以下选项共有4,500种可能的组合：
- en: '[PRE58]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE59]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: This is interesting. Different values are obtaining good results.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣。不同的值取得了不错的结果。
- en: We use the hyperparameters from the best score going forward.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用最佳得分对应的超参数继续调整。
- en: Colsample
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Colsample
- en: Now, let's try `colsample_bytree`, `colsample_bylevel`, and `colsample_bynode`,
    in that order.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们按顺序尝试`colsample_bytree`、`colsample_bylevel`和`colsample_bynode`。
- en: colsample_bytree
  id: totrans-244
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: colsample_bytree
- en: 'Let''s start with `colsample_bytree`:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`colsample_bytree`开始：
- en: '[PRE60]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'The output is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE61]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The score has not improved. Next, try `colsample_bylevel`.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 得分没有改善。接下来，尝试`colsample_bylevel`。
- en: colsample_bylevel
  id: totrans-250
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: colsample_bylevel
- en: 'Use the following code to try out `colsample_bylevel`:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码尝试`colsample_bylevel`：
- en: '[PRE62]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE63]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Still no gain.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然没有提升。
- en: It seems that we are peaking out with the shallow dataset. Let's try a different
    approach. Instead of using `colsample_bynode` alone, let's tune all colsamples
    together.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 看来我们在浅层数据集上已经达到峰值。让我们尝试不同的方法。与其单独使用`colsample_bynode`，不如一起调整所有colsamples。
- en: colsample_bynode
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: colsample_bynode
- en: 'Try the following code:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试以下代码：
- en: '[PRE64]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output is as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE65]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Outstanding. Working together, the colsamples have combined to deliver the highest
    score yet, 5 percentage points higher than the original.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 非常好。通过共同调整，colsamples组合起来取得了目前为止的最高得分，比原始得分高出了5个百分点。
- en: gamma
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gamma
- en: 'The last hyperparameter that we will attempt to fine-tune is `gamma`. Here
    is a range of `gamma` values designed to reduce overfitting:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试微调的最后一个超参数是`gamma`。这里是一个旨在减少过拟合的`gamma`值范围：
- en: '[PRE66]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The output is as follows:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE67]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '`gamma` remains at the default value of `0`.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`gamma`仍然保持在默认值`0`。'
- en: Since our best score is over five percentage points higher than the original,
    no small feat with XGBoost, we will stop here.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的最佳得分比原始得分高出超过五个百分点，这在XGBoost中是一个不小的成就，我们将在这里停止。
- en: Summary
  id: totrans-270
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you prepared for hyperparameter fine-tuning by establishing
    a baseline XGBoost model using `StratifiedKFold`. Then, you combined `GridSearchCV`
    and `RandomizedSearchCV` to form one powerful function. You learned the standard
    definitions, ranges, and applications of key XGBoost hyperparameters, in addition
    to a new technique called early stopping. You synthesized all functions, hyperparameters,
    and techniques to fine-tune the heart disease dataset, gaining an impressive five
    percentage points from the default XGBoost classifier.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，你通过使用 `StratifiedKFold` 建立了一个基准的 XGBoost 模型，为超参数调优做了准备。接着，你将 `GridSearchCV`
    和 `RandomizedSearchCV` 结合成一个强大的功能。你学习了 XGBoost 关键超参数的标准定义、范围和应用，并且掌握了一种名为早停（early
    stopping）的新技巧。你将所有功能、超参数和技巧进行了综合应用，成功地对心脏病数据集进行了调优，从默认的 XGBoost 分类器中提升了令人印象深刻的五个百分点。
- en: XGBoost hyperparameter fine-tuning takes time to master, and you are well on
    your way. Fine-tuning hyperparameters is a key skill that separates machine learning
    experts from machine learning novices. Knowledge of XGBoost hyperparameters is
    not just useful, it's essential to get the most out of the machine learning models
    that you build.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost 超参数调优需要时间来掌握，而你已经走在了正确的道路上。调优超参数是一项关键技能，它将机器学习专家与初学者区分开来。了解 XGBoost
    的超参数不仅有用，而且对于最大化机器学习模型的性能至关重要。
- en: Congratulations on completing this important chapter.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你完成了这一重要章节。
- en: Next, we present a case study of XGBoost regression from beginning to end, highlighting
    the power, range, and applications of `XGBClassifier`.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示了一个 XGBoost 回归的案例研究，从头到尾，突出展示了 `XGBClassifier` 的强大功能、适用范围和应用。
