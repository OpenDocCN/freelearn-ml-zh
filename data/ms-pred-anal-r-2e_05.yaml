- en: Chapter 5. Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。神经网络
- en: So far, we've looked at two of the most well-known methods used for predictive
    modeling. Linear regression is probably the most typical starting point for problems
    where the goal is to predict a numerical quantity. The model is based on a linear
    combination of input features. Logistic regression uses a nonlinear transformation
    of this linear feature combination in order to restrict the range of the output
    in the interval [0,1]. In so doing, it predicts the probability that the output
    belongs to one of two classes. Thus, it is a very well-known technique for classification.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探讨了用于预测建模的两种最知名的方法。线性回归可能是预测数值量的目标问题的最典型起点。该模型基于输入特征的线性组合。逻辑回归使用非线性变换来限制线性特征组合的输出范围在[0,1]区间内。通过这种方式，它预测输出属于两个类别中的一个的概率。因此，它是一种非常著名的分类技术。
- en: Both methods share the disadvantage that they are not robust when dealing with
    many input features. In addition, logistic regression is typically used for binary
    classification problems. In this chapter, we will introduce the concept of **neural
    networks**, a nonlinear approach to solving both regression and classification
    problems. They are significantly more robust when dealing with a higher dimensional
    input feature space, and for classification, they possess a natural way to handle
    more than two output classes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法都存在一个缺点，那就是在处理许多输入特征时不够稳健。此外，逻辑回归通常用于二元分类问题。在本章中，我们将介绍**神经网络**的概念，这是一种解决回归和分类问题的非线性方法。它们在处理高维输入特征空间时显著更加稳健，并且在分类方面，它们拥有一种自然的方式来处理超过两个输出类别。
- en: Neural networks are a biologically inspired model, the origins of which date
    back to the 1940s. Interest in neural networks has fluctuated greatly over the
    years as the first models proved to be quite limited compared to the expectations
    at the time. Additionally, training a large neural network requires substantial
    computational resources. Recently, there has been a huge surge in interest in
    neural networks as distributed on-demand computing resources are now widespread
    and an important area of machine learning, known as **deep learning**, is already
    showing great promise. For this reason, it is a great time to be learning about
    this type of model.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一种生物启发模型，其起源可以追溯到20世纪40年代。多年来，对神经网络的研究兴趣波动很大，因为最初模型与当时的期望相比相当有限。此外，训练大型神经网络需要大量的计算资源。最近，由于分布式按需计算资源现在广泛存在，以及机器学习的一个重要领域——**深度学习**已经显示出巨大的潜力，因此对神经网络的研究兴趣激增。因此，现在是学习这类模型的好时机。
- en: The biological neuron
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生物神经元
- en: 'Neural network models draw their analogy from the organization of neurons in
    the human brain, and for this reason they are also often referred to as **artificial
    neural networks** (**ANNs**) to distinguish them from their biological counterparts.
    The key parallel is that a single biological neuron acts as a simple computational
    unit, but when a large number of these are combined together, the result is an
    extremely powerful and massively distributed processing machine capable of complex
    learning, known more commonly as the human brain. To get an idea of how neurons
    are connected in the brain, the following image shows a simplified picture of
    a human neural cell:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络模型借鉴了人类大脑中神经元的组织结构，因此它们也常被称为**人工神经网络**（**ANNs**）以区别于它们的生物对应物。关键平行之处在于，单个生物神经元作为一个简单的计算单元，但当大量这些单元组合在一起时，结果是一个极其强大且广泛分布的处理机器，能够进行复杂的学习，通常被称为人脑。为了了解大脑中神经元是如何连接的，以下图像展示了一个简化的人神经细胞图：
- en: '![The biological neuron](img/00079.jpeg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![生物神经元](img/00079.jpeg)'
- en: In a nutshell, we can think of a human neuron as a computational unit that takes
    in a series of parallel electrical signal inputs known as **synaptic neurotransmitters**
    coming in from the **dendrites**. The dendrites transmit signal chemicals to the
    **soma** or body of the neuron in response to the received synaptic neurotransmitters.
    This conversion of an external input signal to a local signal can be thought of
    as a process in which the dendrites apply a **weight** (which can be negative
    or positive depending on whether the chemicals produced are **inhibitors** or
    **activators,** respectively) to their inputs.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们可以将人类神经元视为一个计算单元，它接收一系列平行的电信号输入，这些信号被称为**突触神经递质**，它们从**树突**传入。树突在接收到突触神经递质后，将信号化学物质传输到神经元的**胞体**或身体。这种将外部输入信号转换为局部信号的过程可以被视为树突对其输入应用**权重**（根据产生的化学物质是**抑制剂**还是**激活剂**，权重可以是负的或正的）的过程。
- en: The soma of the neuron, which houses the **nucleus** or central processor, mixes
    these input signals in a process that can be thought of as summing up all the
    signals. Consequently, the original dendrite inputs are basically transformed
    into a single linear weighted sum. This sum is sent to the **axon** of the neuron,
    which is the transmitter of the neuron. The weighted sum of electrical inputs
    creates an electric potential in the neuron, and this potential is processed in
    the axon by means of an **activation function**, which determines whether the
    neuron will fire.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的胞体，其中包含**核**或中央处理器，将这些输入信号混合在一起，这个过程可以被视为对所有信号求和。因此，原始的树突输入基本上被转换成一个单一的线性加权总和。这个总和被发送到神经元的**轴突**，它是神经元的传输器。电输入的加权总和在神经元中产生一个电势，这个电势通过轴突中的**激活函数**进行处理，该函数决定了神经元是否会放电。
- en: Typically, the activation function is modeled as a switch that requires a minimum
    electrical potential, known as the **bias**, to be reached before it is turned
    on. Thus, the activation function essentially determines whether the neuron will
    output an electrical signal or not, and if so, the signal is transported through
    the axon and propagated to other neurons through the **axon terminals**. These,
    in turn, connect to the dendrites of neighboring neurons and the electrical signal
    output becomes an input to subsequent neural processing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，激活函数被建模为一个开关，它需要达到一个最小电势，称为**偏置**，才能被打开。因此，激活函数本质上决定了神经元是否会输出电信号，如果是的话，信号将通过轴突传输，并通过**轴突末端**传播到其他神经元。这些末端反过来连接到邻近神经元的树突，电信号输出成为后续神经处理的一个输入。
- en: This description is, of course, a simplification of what happens in our neurons,
    but the goal here is to explain what aspects of the biological process have been
    used to inspire the computational model of a neural network.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个描述是对我们神经元中发生的事情的简化，但这里的目的是解释生物过程中哪些方面被用来启发神经网络计算模型。
- en: The artificial neuron
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经元
- en: 'Using our biological analogy, we can construct a model of a computational neuron,
    and this model is known as the **McCulloch-Pitts model** of a neuron:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的生物类比，我们可以构建一个计算神经元的模型，这个模型被称为神经元的**麦库洛奇-皮茨模型**：
- en: '![The artificial neuron](img/00080.jpeg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![人工神经元](img/00080.jpeg)'
- en: Note
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '*Warren McCulloch* and *Walter Pitts* proposed this model of a neural network
    as a computing machine in a paper titled *A logical calculus of the ideas immanent
    in nervous activity*, published by the *Bulletin of Mathematical Biophysics* in
    1943.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*沃伦·麦库洛奇*和*沃尔特·皮茨*在1943年由《数学生物物理学通报》发表的论文《神经活动中内在思想的逻辑演算》中提出了这个神经网络模型作为计算机器。'
- en: 'This computational neuron is the simplest example of a neural network. We can
    construct the output function, *y*, of our neural network directly from following
    our diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这个计算神经元是神经网络最简单的例子。我们可以直接从以下图表中构建我们神经网络的输出函数，*y*：
- en: '![The artificial neuron](img/00081.jpeg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![人工神经元](img/00081.jpeg)'
- en: 'The function `g()` in our neural network is the activation function. Here,
    the specific activation function that is chosen is the **step function**:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们神经网络中的函数`g()`是激活函数。在这里，选择的特定激活函数是**阶跃函数**：
- en: '![The artificial neuron](img/00082.jpeg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![人工神经元](img/00082.jpeg)'
- en: 'When the linear weighted sum of inputs exceeds zero, the step function outputs
    1, and when it does not, the function outputs -1\. It is customary to create a
    dummy input feature *x[0]* which is always taken to be 1, in order to merge the
    bias or threshold *w[0]* into the main sum as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入的线性加权总和超过零时，步函数输出1，当它不等于零时，函数输出-1。通常，我们会创建一个虚拟输入特征*x[0]*，它始终被取为1，以便将偏差或阈值*w[0]*合并到主要求和中，如下所示：
- en: '![The artificial neuron](img/00083.jpeg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![人工神经元](img/00083.jpeg)'
- en: Using our experience with logistic regression, it should be very easy to conclude
    that we could construct a simple classifier using this setup for the binary classification
    problem. The only difference is that in logistic regression, we would choose the
    logistic function as the activation function. In fact, in 1957, *Frank Rosenblatt*
    proposed a supervised learning algorithm for training the *McCulloch-Pitts* model
    of neurons to perform binary classification, and this algorithm along with the
    learning model produced is known as the **Rosenblatt perceptron**.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们对逻辑回归的经验，我们可以很容易地得出结论，我们可以使用这个设置构建一个简单的分类器来解决二元分类问题。唯一的区别在于，在逻辑回归中，我们会选择逻辑函数作为激活函数。事实上，在1957年，*弗兰克·罗森布拉特*提出了一种监督学习算法，用于训练神经元的*麦库洛奇-皮茨*模型以执行二元分类，这个算法以及产生的学习模型被称为**罗森布拉特感知器**。
- en: We've thus far presented linear and logistic regression as models that can solve
    supervised learning problems and showed the criteria that are used to train them
    without actually going into the optimization details of the training algorithms
    involved. This was done intentionally to allow us to focus our attention on understanding
    the models themselves, and how to apply them in R.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已将线性回归和逻辑回归作为可以解决监督学习问题的模型进行介绍，并展示了用于训练它们的准则，而没有深入到涉及训练算法的优化细节。这样做是有意为之，以便我们能够专注于理解模型本身，以及如何在R中应用它们。
- en: Now that we have built up some experience with classification and regression,
    this chapter is going to be different, in that we will look at some of the details
    of how predictive models are trained, as this too is an important process that
    adds to our overall understanding of a model. In addition, neural networks differ
    substantially from previous models we have seen so far, in that training a neural
    network is often more time consuming and involves adjusting a number of parameters,
    many of which arise from the optimization procedure itself. Thus, it helps to
    understand the role these parameters play during training and how they can affect
    the final model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经积累了一些关于分类和回归的经验，这一章将有所不同，我们将探讨预测模型训练的一些细节，因为这也是一个重要的过程，有助于我们全面理解模型。此外，神经网络与之前我们所见的模型有显著不同，训练神经网络通常耗时更长，并涉及调整大量参数，其中许多参数源于优化过程本身。因此，了解这些参数在训练期间的作用以及它们如何影响最终模型是有帮助的。
- en: Before we present a training algorithm for the perceptron, we'll first have
    to learn one of the most fundamental techniques used in solving optimization problems.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们介绍感知器训练算法之前，我们首先需要学习解决优化问题中最基本的技术之一。
- en: Stochastic gradient descent
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: 'In the models we''ve seen so far, such as linear regression, we''ve talked
    about a criterion or objective function that the model must minimize while it
    is being trained. This criterion is also sometimes known as the **cost function**.
    For example, the least squares cost function for a model can be expressed as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前看到的模型中，例如线性回归，我们讨论了模型在训练过程中必须最小化的准则或目标函数。这个准则有时也被称为**损失函数**。例如，模型的平方损失函数可以表示为：
- en: '![Stochastic gradient descent](img/00084.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/00084.jpeg)'
- en: 'We''ve added a constant term of ½ in front of this for reasons that will become
    apparent shortly. We know from basic differentiation that when we are minimizing
    a function, multiplying the function by a constant factor does not alter the value
    of the minimum value of the function. In linear regression, just as with our perceptron
    model, our model''s predicted ![Stochastic gradient descent](img/00085.jpeg)are
    just the sum of a linear weighted combination of the input features. If we assume
    that our data is fixed and that the weights are variable and must be chosen so
    as to minimize our criterion, we can treat the cost function as being a function
    of the weights:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这个公式前添加了一个常数项½，原因将在稍后变得明显。从基本的微分知识我们知道，当我们最小化一个函数时，将函数乘以一个常数因子不会改变函数最小值。在线性回归中，正如我们的感知器模型一样，我们的模型预测![随机梯度下降](img/00085.jpeg)仅仅是输入特征的线性加权组合的总和。如果我们假设我们的数据是固定的，而权重是可变的，并且必须选择以最小化我们的标准，那么我们可以将成本函数视为权重的函数：
- en: '![Stochastic gradient descent](img/00086.jpeg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/00086.jpeg)'
- en: 'We have used the letter *w* to represent the model weights here for the more
    general case, though in linear regression we''ve seen that it is customary to
    use the Greek letter *β* instead. As our model variables are the weights, we can
    consider that our function is a function of a weight vector ![Stochastic gradient
    descent](img/00087.jpeg). To find the minimum of this function, we just need to
    take the partial derivative of our cost function with respect to this weight vector.
    For a specific weight *w[k]*, this partial derivative is given by:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用字母*w*来表示模型权重，以表示更一般的情况，尽管在线性回归中，我们通常使用希腊字母*β*。由于我们的模型变量是权重，我们可以认为我们的函数是权重向量![随机梯度下降](img/00087.jpeg)的函数。为了找到这个函数的最小值，我们只需要对成本函数关于这个权重向量求偏导。对于特定的权重*w[k]*，这个偏导数由以下给出：
- en: '![Stochastic gradient descent](img/00088.jpeg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/00088.jpeg)'
- en: 'Note that the coefficient of one half has usefully cancelled out the *2* from
    the derivative. We now have three different subscripts, so it is a good idea to
    take a step back and try to understand this equation. The innermost sum is still
    computing, which is the model''s predicted output. Let''s replace this in the
    equation to simplify things a bit:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，一半的系数已经有效地抵消了导数中的*2*。我们现在有三个不同的下标，所以退一步理解这个方程是个好主意。最内层的求和仍在进行，这是模型的预测输出。让我们将这个替换到方程中以简化事情：
- en: '![Stochastic gradient descent](img/00089.jpeg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/00089.jpeg)'
- en: Now we should be in a better position to understand this equation. It says that
    the partial derivative of the cost function that we are trying to minimize for
    a specific weight, *w[k]*, in our model is just the difference between the predicted
    output of the model and the actual labeled output, multiplied by *x[ik]* (for
    the *i^(th)* observation, the value of the input feature that corresponds to our
    weight *w[k]*), and averaged over all the *n* observations in our dataset.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该更有能力理解这个方程了。它表明，我们试图最小化的成本函数的偏导数，针对我们模型中的特定权重*w[k]*，仅仅是模型预测输出与实际标记输出的差，乘以*x[ik]*（对于第*i*个观察值，对应于我们的权重*w[k]*的输入特征值），然后对所有数据集中的*n*个观察值进行平均。
- en: Tip
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you are not familiar with partial differentiation, but are familiar with
    differentiation, you already know everything you need to in order to understand
    this equation. We use partial differentiation to explicitly identify the variable
    that we will be differentiating with respect to an equation that has more than
    one variable. When we do this, we treat all other variables as constants and the
    differentiation is carried out normally.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉微分但不熟悉偏微分，你已经知道理解这个方程所需的一切。我们使用偏微分来明确识别我们将相对于一个具有多个变量的方程进行微分变量的变量。当我们这样做时，我们将所有其他变量视为常数，并正常进行微分。
- en: To find the optimal weights, we need to solve this equation for every weight
    in our weight vector. Note that through the predicted output term, all the weights
    in the model appear in the partial derivative of every individual weight. Put
    differently, this produces a complete system of linear equations that is often
    very large, so solving this directly is often prohibitively expensive, computationally
    speaking.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '为了找到最优权重，我们需要为权重向量中的每个权重求解此方程。注意，通过预测输出项，模型中的所有权重都出现在每个单个权重的偏导数中。换句话说，这产生了一个完整的线性方程组，通常非常大，因此直接求解通常成本过高，从计算角度来看。 '
- en: 'Instead, many model implementations use iterative optimization procedures that
    are designed to gradually approach the correct solution. One such method is **gradient
    descent**. For a particular value of the weight vector, gradient descent finds
    the direction in which the gradient of the cost function is steepest, and adjusts
    the weights in that direction by a small amount, which is determined by a parameter
    known as the **learning rate**. Thus, the updated equation is:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，许多模型实现使用迭代优化过程，这些过程旨在逐步接近正确解。其中一种方法是**梯度下降法**。对于权重向量的特定值，梯度下降法找到成本函数梯度最陡的方向，并通过一个称为**学习率**的参数以小量调整该方向上的权重。因此，更新后的方程是：
- en: '![Stochastic gradient descent](img/00090.jpeg)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/00090.jpeg)'
- en: In the previous equation, the learning rate is denoted by the Greek letter *η*.
    Setting the learning rate to an appropriate value is a very important aspect of
    optimizing with gradient descent. If we choose a value that is too small, the
    algorithm will update the weights by a very small amount each time, and thus it
    will take too long to finish. If we use a value that is too large, we may cause
    the weights to change too drastically, oscillating between values, and so again
    the learning algorithm will either take too long to converge or oscillate continuously.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的方程中，学习率用希腊字母*η*表示。将学习率设置为一个适当的值是使用梯度下降进行优化的一个非常重要的方面。如果我们选择一个过小的值，算法每次将权重更新一个非常小的量，因此它将花费太长时间来完成。如果我们使用一个过大的值，我们可能会使权重变化过于剧烈，在值之间振荡，因此学习算法要么需要太长时间才能收敛，要么持续振荡。
- en: There are various sophisticated methods to estimate an appropriate learning
    rate, the details of which we won't discuss here. Instead, we'll try to find an
    appropriate learning rate through trial and error, and this often works just fine
    in practice. One way to keep track of whether our chosen learning rate is decent
    is to plot the cost function we are trying to minimize versus time (represented
    by the number of iterations made through the dataset). We should be seeing a decreasing
    (or at least non-increasing) change in the cost function over time if we have
    chosen a good value for the learning rate.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 有各种复杂的方法来估计适当的学习率，其细节我们在此不讨论。相反，我们将尝试通过试错法找到一个适当的学习率，这在实践中通常效果很好。跟踪我们选择的 learning
    rate 是否合适的一种方法是将我们试图最小化的成本函数与时间（通过通过数据集进行的迭代次数表示）绘制出来。如果我们选择了好的学习率值，我们应该会看到成本函数随时间逐渐减少（或者至少是非增加的）。
- en: A variant of the gradient descent method is **stochastic gradient descent**,
    which does a similar computation, but takes the observations one at a time instead
    of all together. The key idea is that, on average, the gradient of the cost function
    computed for a particular observation will equal that of the gradient computed
    across all observations. This is, of course, an approximation, but it does mean
    that we can process individual observations one at a time, which is very useful,
    especially if we want to perform online learning.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法的一种变体是**随机梯度下降法**，它执行类似的计算，但一次只处理一个观察值而不是全部一起。关键思想是，平均而言，为特定观察值计算的成本函数的梯度将等于在整个观察值上计算出的梯度的平均值。这当然是一个近似，但它确实意味着我们可以一次处理一个单独的观察值，这在实践中非常有用，特别是如果我们想进行在线学习。
- en: 'Stochastic gradient descent updates a particular weight, *w[k]*, when processing
    the *i^(th)* observation in the dataset according to the following equation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降在处理数据集中的第*i*个观察值时更新特定的权重*w[k]*，根据以下方程：
- en: '![Stochastic gradient descent](img/00091.jpeg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![随机梯度下降](img/00091.jpeg)'
- en: Note
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: An excellent resource for some of the tricks that are useful when training a
    model with stochastic gradient descent is a book chapter by *Leo Bottou*, titled
    *Stochastic Gradient Descent Tricks*. A version of this can be found online at
    [http://research.microsoft.com/pubs/192769/tricks-2012.pdf](http://research.microsoft.com/pubs/192769/tricks-2012.pdf).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在随机梯度下降训练模型时有用的技巧，一个很好的资源是 *Leo Bottou* 的一个章节，标题为 *Stochastic Gradient Descent
    Tricks*。这本书的版本可以在网上找到，链接为 [http://research.microsoft.com/pubs/192769/tricks-2012.pdf](http://research.microsoft.com/pubs/192769/tricks-2012.pdf)。
- en: Gradient descent and local minima
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度下降和局部最小值
- en: 'Gradient descent methods rely on the idea that the cost function that is being
    minimized is a **convex function**. We''ll skip the mathematical details of this
    and just say that a convex function is a function that has, at most, a single
    global minimum. Let''s look at an example of a non-convex cost function in terms
    of a single weight *w*:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降方法依赖于这样一个观点：正在最小化的成本函数是一个**凸函数**。我们将跳过这个数学细节，只说凸函数是一个最多只有一个全局最小值的函数。让我们来看一个关于单个权重
    *w* 的非凸成本函数的例子：
- en: '![Gradient descent and local minima](img/00092.jpeg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![梯度下降和局部最小值](img/00092.jpeg)'
- en: The global minimum of this function is the first trough on the left for a value
    of *w*, close to 4.5\. If our initial guess for the weight *w* is 1, the gradient
    of the cost function points towards the global minimum, and we will progressively
    approach it until we reach it. If our initial guess of the weight is 12, then
    the gradient of the cost function will point downwards towards the trough near
    the value 10.5\. Once we reach the second trough, the gradient of the cost function
    will be 0 and consequently, we will not be able to make any progress towards our
    global minimum because we have landed in a local minimum.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的全局最小值是在 *w* 接近 4.5 的值时左侧的第一个凹槽。如果我们对权重 *w* 的初始猜测是 1，则成本函数的梯度指向全局最小值，我们将逐步接近它，直到达到它。如果我们对权重
    *w* 的初始猜测是 12，那么成本函数的梯度将指向接近 10.5 的凹槽下方。一旦我们达到第二个凹槽，成本函数的梯度将为 0，因此，我们将无法向全局最小值前进，因为我们已经陷入局部最小值。
- en: Detecting and avoiding local minima can be very tricky, especially if there
    are many of them. One way to do this is to repeat the optimization with different
    starting points and then pick the weights that produce the lowest value of the
    cost function across the different times the optimization is run. This procedure
    works well if the number of local minima is small and they are not too close together.
    Thankfully, the squared error cost function that we saw in the previous section
    is a convex function and so gradient descent methods are guaranteed to find the
    global minimum, but it is good to be aware that there are other examples of cost
    functions that we will encounter that are non-convex.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 检测和避免局部最小值可能非常棘手，尤其是如果有很多局部最小值。一种方法是使用不同的起始点重复优化，然后选择在优化运行的不同时间产生成本函数最低值的权重。如果局部最小值数量很少且彼此之间不是很接近，这个程序效果很好。幸运的是，我们在上一节中看到的平方误差成本函数是一个凸函数，因此梯度下降方法保证找到全局最小值，但了解我们还将遇到其他非凸成本函数的例子是好的。
- en: The perceptron algorithm
  id: totrans-53
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 感知机算法
- en: Without further ado, we'll present our first training algorithm for classification
    with neural networks. This is a variation of the perceptron learning algorithm
    and is known as the **pocket perceptron algorithm**.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 不再拖延，我们将介绍第一个用于神经网络分类的训练算法。这是感知机学习算法的一种变体，被称为**口袋感知机算法**。
- en: '**Inputs:**'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入：**'
- en: '`x`: A two-dimensional matrix, where the rows are the observations and the
    columns are the input features.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`x`：一个二维矩阵，其中行是观测值，列是输入特征。'
- en: '`y`: A vector with the class label (-1 or 1) for all the observations in *x*.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y`：一个向量，包含 *x* 中所有观测值的类别标签（-1 或 1）。'
- en: '`learning_rate`: A number that controls the learning rate of the algorithm.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`learning_rate`：一个控制算法学习率的数字。'
- en: '`max_iterations`: The maximum number of cycles through our data that our algorithm
    is allowed to perform while learning.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_iterations`：算法在学习过程中允许执行的最大数据循环次数。'
- en: '**Outputs:**'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出：**'
- en: '`w`: The learned weights of the perceptron.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`w`：感知机的学习权重。'
- en: '`converged`: Whether the algorithm converged (true or false).'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`converged`：算法是否收敛（真或假）。'
- en: '`iterations`: The actual number of iterations through the data performed during
    learning.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterations`：学习过程中实际执行的数据循环次数。'
- en: '**Method:**'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法：**'
- en: Randomly initialize the weights *w*.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化权重 *w*。
- en: Select an observation in *x*, and call it *xi*.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 *x* 中选择一个观测值，并将其称为 *xi*。
- en: Compute the predicted class,![The perceptron algorithm](img/00093.jpeg), using
    the current values of the weights *w* and the equation for the output of the perceptron.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用当前权重 *w* 的值和感知器输出的方程计算预测类别，![感知器算法](img/00093.jpeg)。
- en: If the predicted class,![The perceptron algorithm](img/00093.jpeg) is not the
    same as the actual class, `yi`, then update the weights vector using stochastic
    gradient descent.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果预测类别，![感知器算法](img/00093.jpeg)与实际类别 `yi` 不相同，则使用随机梯度下降更新权重向量。
- en: Repeat steps 2–4 for all the observations in our dataset and count the number
    of errors made.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对数据集中的所有观测值重复步骤 2–4，并计算犯下的错误数量。
- en: If the number of errors is zero, we have converged and the algorithm terminates.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果错误数量为零，则我们已收敛，算法终止。
- en: If the number of errors made in the current iteration was less than the lowest
    numbers of errors ever made, store the weights vector as the best weights vector
    seen so far.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前迭代中犯下的错误数量少于以前犯下的最低错误数量，则将权重向量存储为迄今为止看到的最佳权重向量。
- en: If we have reached the maximum number of iterations, stop and return the value
    of the best weights vector. Otherwise, begin a new iteration over the dataset
    at step 2.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们达到了最大迭代次数，停止并返回最佳权重向量的值。否则，从步骤 2 开始在数据集上开始新的迭代。
- en: 'We''ll see the R code for this directly and discuss the steps in detail:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将直接展示 R 代码，并详细讨论这些步骤：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The first function we define is the step function, which we know will produce
    either the value `-1` or the value `1` corresponding to the two classes in our
    dataset. We then define our main function, which we call `pocket_perceptron()`.
    The job of this function is to learn the weights for our perceptron so that our
    model classifies our training data correctly.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的第一个函数是步进函数，我们知道它将产生 `-1` 或 `1` 的值，对应于数据集中的两个类别。然后我们定义我们的主要函数，我们称之为 `pocket_perceptron()`。这个函数的目的是学习感知器的权重，以便我们的模型能够正确地分类训练数据。
- en: Note that we have not introduced any regularization in our algorithm to keep
    things simple, and so we will likely end up with a model that will overfit our
    data, as we are shooting for 100 percent training accuracy. Proceeding with our
    algorithm description, we begin our function by initializing the weights vector
    to small randomly generated numbers. In practice, it is a good idea to make sure
    that weights are not set to `0` and are not symmetric, and this method is a good
    way to avoid this.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们没有在我们的算法中引入任何正则化，以保持简单，因此我们可能会得到一个过度拟合数据的模型，因为我们追求的是 100% 的训练准确率。继续我们的算法描述，我们首先初始化权重向量为小的随机生成的数字。在实践中，确保权重不是设置为
    `0` 并且不是对称的，这是一个避免这种情况的好方法。
- en: We will also set our starting best guess of the weights to be our initial vector
    and our starting best error rate to be the total number of observations, which
    is the worst possible error rate on a dataset.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将起始的最佳权重猜测设置为我们的初始向量，并将起始的最佳错误率设置为观测值的总数，这是数据集上最坏可能的错误率。
- en: The main `while` loop of the function controls the number of iterations over
    which our algorithm will run. We will only begin a new iteration when we have
    not converged and when we have not hit our maximum number of iterations. Inside
    the `while` loop, we use a `for` loop to iterate over the observations in our
    dataset and classify these using the current version of our weight vector.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的主要 `while` 循环控制算法将运行的迭代次数。只有在我们没有收敛且未达到最大迭代次数时，我们才会开始新的迭代。在 `while` 循环内部，我们使用
    `for` 循环遍历数据集中的观测值，并使用我们权重向量的当前版本对这些观测值进行分类。
- en: Every time we make a mistake in classification, we update our error rate, note
    that we have not converged in this iteration, and update our weight vector according
    to the stochastic gradient descent update rule for least squares that we saw in
    the previous section. Although the cost function for the perceptron is not differentiable
    because of the step function used to threshold the output, it turns out that we
    can, in fact, still use the same update rule for the weights.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 每当我们分类错误时，我们都会更新我们的错误率，注意我们在这个迭代中尚未收敛，并根据我们在上一节中看到的平方最小化随机梯度下降更新规则更新我们的权重向量。尽管由于用于阈值输出的步进函数，感知器的成本函数不可导，但事实证明，我们实际上仍然可以使用相同的权重更新规则。
- en: At the end of a complete iteration through our dataset, also known as an **epoch**,
    we check whether we need to update our best weights vector and update the number
    of iterations. We update our best weights vector only if the performance in the
    current iteration on the training data was the best performance we have seen thus
    far across all completed iterations. When the algorithm terminates, we return
    the best weights we found, whether or not we converged, and the total number of
    completed iterations.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在完整地遍历我们的数据集之后，也称为一个**时代**，我们检查是否需要更新我们的最佳权重向量并更新迭代次数。只有当当前迭代在训练数据上的性能是我们迄今为止在所有完成的迭代中看到的最佳性能时，我们才更新我们的最佳权重向量。当算法终止时，无论是否收敛，我们都返回找到的最佳权重，以及完成的迭代总数。
- en: Note
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The definitive textbook on neural networks, and one that explains perceptron
    learning in more detail, including proof of why the algorithm works, is *Neural
    Networks and Learning Machines 3rd Edition*, *Simon Haykin*, *Prentice Hall.*
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 关于神经网络的决定性教科书，以及一本更详细解释感知机学习的书籍，是《神经网络与学习机器 第3版》，作者西蒙·海金，出版社：普伦蒂斯·霍尔。
- en: 'We can put our model to the test by generating some artificial data. We''ll
    do this by sampling values from two uniform distributions in order to create two
    input features: *x[1]* and *x[2]*. We''ll then separate these data points into
    two different classes according to a linear decision boundary that we''ve chosen
    randomly:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过生成一些人工数据来测试我们的模型。我们将通过从两个均匀分布中采样值来创建两个输入特征：*x[1]* 和 *x[2]*。然后，我们将根据我们随机选择的线性决策边界将这些数据点分为两个不同的类别：
- en: '![The perceptron algorithm](img/00094.jpeg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![感知机算法](img/00094.jpeg)'
- en: 'Once we have the data and the computed class labels, we can run our perceptron
    algorithm on it. The following code generates the test data and builds our model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据和计算出的类别标签，我们就可以在它们上运行感知机算法。以下代码生成测试数据和构建我们的模型：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can see that after 32 iterations, our perceptron algorithm has converged.
    If we divide our weights vector by `2` (this does not alter our decision boundary),
    we can see more clearly that we have a decision boundary that is very close to
    the one that was used when classifying the data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，经过32次迭代后，我们的感知机算法已经收敛。如果我们将权重向量除以`2`（这不会改变我们的决策边界），我们可以更清楚地看到我们有一个非常接近用于分类数据的决策边界的决策边界：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The following plot shows that the model's decision boundary is virtually indistinguishable
    from the population line. For our artificially generated dataset, this is because
    the two classes are so close together. If the classes were further apart, we would
    more likely see a noticeable difference between the population decision boundary
    and the model's decision boundary.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示显示，该模型的决策边界几乎与种群线无法区分。对于我们的人工生成数据集，这是因为两个类别非常接近。如果类别之间距离更远，我们更有可能看到种群决策边界和模型决策边界之间的明显差异。
- en: This is because the space of possible lines (or planes when we are dealing with
    more than two features) that can separate the data would be larger.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为可能分离数据的线条（或当我们处理超过两个特征时的平面）的空间会更大。
- en: '![The perceptron algorithm](img/00095.jpeg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![感知机算法](img/00095.jpeg)'
- en: Linear separation
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性分离
- en: The data that we generated had a particular property that ensured that the perceptron
    algorithm would converge--it was **linearly separable**. When two classes are
    linearly separable in terms of a set of features, it means that it is possible
    to find a linear combination of these features as a decision boundary that will
    allow us to classify the two classes with 100 percent accuracy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们生成的数据具有特定的属性，确保感知机算法会收敛——它是**线性可分**的。当两个类别在一系列特征上线性可分时，这意味着可以找到这些特征的线性组合作为决策边界，这将允许我们以100%的准确率对两个类别进行分类。
- en: If we consider plotting the data points belonging to the two classes in the
    *p*-dimensional feature space, then linear separation means that there is a plane
    (or line for two dimensions, as we saw in our example) that can be drawn to separate
    the two classes. There is a theorem, known as the **perceptron convergence theorem**,
    which states that for linearly separable classes, the perceptron learning algorithm
    will always converge to a solution that correctly classifies all the data given
    enough time.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑在 *p*-维特征空间中绘制属于两个类别的数据点，那么线性分离意味着可以画出一个平面（或线，如我们在示例中看到的）来分离这两个类别。有一个称为**感知器收敛定理**的定理，它表明对于线性可分类别，如果给定足够的时间，感知器学习算法将始终收敛到一个正确分类所有给定数据的解。
- en: The logistic neuron
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑神经元
- en: The perceptron is also known as a **binary threshold neuron**. We can create
    different types of neurons by changing the activation function. For example, if
    we remove the threshold function completely, we end up with a **linear neuron**,
    which essentially performs the same task as linear regression. By changing the
    activation function to a logistic function, we can create a **logistic neuron**.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器也被称为**二元阈值神经元**。我们可以通过改变激活函数来创建不同类型的神经元。例如，如果我们完全移除阈值函数，我们最终会得到一个**线性神经元**，它本质上执行与线性回归相同的任务。通过将激活函数更改为逻辑函数，我们可以创建一个**逻辑神经元**。
- en: 'A logistic neuron performs the same task as logistic regression, by taking
    a linear combination of inputs and applying the logistic function to predict a
    value in the interval [0,1]. Stochastic gradient descent can be applied in order
    to learn the weights of linear neurons as well as logistic neurons. Hence, it
    can also be applied to learn the weights for logistic and linear regression. The
    general form of the stochastic gradient descent weight update rule is:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一个逻辑神经元执行的任务与逻辑回归相同，通过将输入进行线性组合并应用逻辑函数来预测区间 [0,1] 内的值。可以使用随机梯度下降来学习线性神经元以及逻辑神经元的权重。因此，它也可以用于学习逻辑回归和线性回归的权重。随机梯度下降权重更新规则的一般形式是：
- en: '![The logistic neuron](img/00096.jpeg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑神经元](img/00096.jpeg)'
- en: 'Here, the derivative is computing the gradient of the cost function at the
    particular observation. We saw the simple form for linear regression and the linear
    neuron in the previous section. If we perform differentiation on the cost function
    for logistic regression, we will discover that the update rule for stochastic
    gradient descent for the logistic neuron appears to be exactly the same as with
    the linear neuron:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，导数是计算成本函数在特定观察点处的梯度。我们在上一节中看到了线性回归和线性神经元的简单形式。如果我们对逻辑回归的成本函数进行微分，我们会发现逻辑神经元的随机梯度下降更新规则似乎与线性神经元完全相同：
- en: '![The logistic neuron](img/00097.jpeg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑神经元](img/00097.jpeg)'
- en: The subtle difference here is that the form of ![The logistic neuron](img/00093.jpeg)
    is completely different as it now includes the weights inside the logistic function,
    whereas this was not the case in linear regression. Logistic neurons are very
    important because they are the most common type of neuron used when building networks
    of many neurons connected together. As we'll see in the next section, we generally
    build neural networks in layers. The layer containing the neurons that produce
    our outputs is known as the **output layer**. The **input layer** is comprised
    of our data features that are the inputs to network.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的微妙区别在于，![逻辑神经元](img/00093.jpeg) 的形式完全不同，因为它现在将权重包含在逻辑函数中，而线性回归中并非如此。逻辑神经元非常重要，因为它们是构建由许多相互连接的神经元组成的网络时最常用的神经元类型。正如我们将在下一节中看到的，我们通常按层构建神经网络。产生我们输出的神经元所在的层被称为**输出层**。**输入层**由我们的数据特征组成，这些特征是网络的输入。
- en: Layers in between the input and output layers are known as **hidden layers**.
    Logistic neurons are the most common hidden layer neuron. Additionally, we use
    logistic neurons as output layer neurons when our task is classification, and
    linear neurons when our task is regression.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层之间的层被称为**隐藏层**。逻辑神经元是最常见的隐藏层神经元。此外，当我们的任务是分类时，我们使用逻辑神经元作为输出层神经元，当我们的任务是回归时，我们使用线性神经元。
- en: Multilayer perceptron networks
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多层感知器网络
- en: Multilayer neural networks are models that chain many neurons in order to create
    a neural architecture. Individually, neurons are very basic units, but when organized
    together, we can create a model significantly more powerful than the individual
    neurons.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 多层神经网络是连接许多神经元以创建神经架构的模型。单个神经元是非常基本的单元，但当我们组织在一起时，我们可以创建一个比单个神经元强大得多的模型。
- en: 'As touched upon in the previous section, we build neural networks in layers
    and we distinguish between different kinds of neural networks primarily on the
    basis of the connections that exist between these layers and the types of neurons
    used. The following diagram shows the general structure of a **multilayer perceptron**
    (**MLP**) neural network, shown here for two hidden layers:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们按层构建神经网络，我们主要根据这些层之间的连接和使用的神经元类型来区分不同类型的神经网络。以下图显示了**多层感知器**（**MLP**）神经网络的一般结构，这里展示了两个隐藏层：
- en: '![Multilayer perceptron networks](img/00098.jpeg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器网络](img/00098.jpeg)'
- en: The first characteristic of the MLP network is that the information flows in
    a single direction from input layer to output layer. Thus, it is known as a **feedforward
    neural network**. This is in contrast to other neural network types, in which
    there are cycles that allow information to flow back to earlier neurons in the
    network as a feedback signal. These networks are known as **feedback neural networks**
    or **recurrent neural** networks. Recurrent neural networks are generally very
    difficult to train and often do not scale well with the number of inputs. Nonetheless,
    they do find a number of applications, in particular with problems involving a
    time component such as forecasting and signal processing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: MLP 网络的第一个特征是信息从输入层流向输出层，方向单一。因此，它被称为**前馈神经网络**。这与其他神经网络类型形成对比，其他神经网络类型中存在循环，允许信息作为反馈信号流回网络中的早期神经元。这些网络被称为**反馈神经网络**或**循环神经网络**。循环神经网络通常很难训练，并且通常不随着输入数量的增加而很好地扩展。尽管如此，它们仍然找到了许多应用，特别是在涉及时间成分的问题中，如预测和信号处理。
- en: Returning to the MLP architecture shown in the diagram, we note that the first
    group of neurons on the left are known as the input neurons and form the input
    layer. We always have as many input neurons as there are input features. The input
    neurons are said to produce the values of our input features as outputs. For this
    reason, we often don't refer to them as input neurons, but rather as input sources
    or input nodes. At the far right of the diagram, we have the output layer with
    the output neurons. We usually have as many output neurons as outputs that we
    are modeling. Thus, our neural network can naturally learn to predict more than
    one thing at a time. One exception to this rule is that when we are modeling a
    multiclass classification problem, we usually have one binary output neuron for
    every class. In this case, all the output neurons are a dummy encoding of a single
    multiclass factor output.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 回到图中所示的 MLP 架构，我们注意到左侧的第一组神经元被称为输入神经元，形成了输入层。我们总是有与输入特征数量一样多的输入神经元。输入神经元被认为产生我们输入特征的值作为输出。因此，我们通常不称它们为输入神经元，而是称它们为输入源或输入节点。在图的最右侧，我们有输出层和输出神经元。我们通常有与我们要建模的输出数量一样多的输出神经元。因此，我们的神经网络可以自然地学习一次预测多个事物。这个规则的例外之一是当我们建模多类分类问题时，我们通常为每个类别有一个二进制输出神经元。在这种情况下，所有输出神经元都是单个多类因素输出的虚拟编码。
- en: Between the input and output layers, we have the hidden layers. Neurons are
    organized into layers depending on how many neurons are between them and an input
    neuron. For example, neurons in the first hidden layer are directly connected
    to at least one neuron in the input layer, whereas neurons in the second hidden
    layer are directly connected to one or more neurons in the first hidden layer.
    Our diagram is an example of a 4-4 architecture, which means that there are two
    hidden layers with four neurons each. Even though they are not neurons themselves,
    the diagram explicitly shows the bias units for all the neurons. We saw in our
    equation for the output of a single neuron that we can treat the bias unit as
    a dummy input feature with a value of 1 that has a weight on it that corresponds
    to the bias or threshold.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入层和输出层之间，我们有隐藏层。神经元根据它们之间以及与输入神经元之间的神经元数量被组织成层。例如，第一隐藏层中的神经元直接连接到输入层中至少一个神经元，而第二隐藏层中的神经元直接连接到第一隐藏层中的一个或多个神经元。我们的图是一个4-4架构的例子，这意味着有两个每个有四个神经元的隐藏层。尽管它们本身不是神经元，但该图明确显示了所有神经元的偏差单元。我们在单个神经元输出的方程中看到，我们可以将偏差单元视为具有值为1的虚拟输入特征，并且它有一个与之对应的偏差或阈值权重的权重。
- en: Not all the neurons in the architecture are assumed to have the same activation
    function. In general, we pick the activation function for the neurons in the hidden
    layers separately from that of the output layer. The activation function for the
    output layer we've already seen is chosen based on what type of output we would
    like, which in turn depends on whether we are performing regression or classification.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 并不是架构中的所有神经元都假设具有相同的激活函数。通常，我们为隐藏层中的神经元选择激活函数时，会与输出层的激活函数分开考虑。我们已经看到的输出层激活函数的选择是基于我们希望得到哪种类型的输出，这反过来又取决于我们是进行回归还是分类。
- en: The activation function for the hidden layer neurons is generally nonlinear,
    because chaining together linear neurons can be algebraically simplified to a
    single linear neuron with different weights and so this does not add any power
    to the network. The most common activation function is the logistic function,
    but others such as the hyperbolic tangent function are also used.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层神经元的激活函数通常是非线性函数，因为将线性神经元链式连接起来可以从代数上简化为一个具有不同权重的单个线性神经元，因此这并不会给网络增加任何能力。最常用的激活函数是对数逻辑函数，但其他如双曲正切函数也被使用。
- en: The output of the neural network can be calculated by successively computing
    the outputs of the neurons of each layer. The output of the units of the first
    hidden layer can be computed using the equations for the output of a neuron that
    we have seen thus far. These outputs become inputs to the neurons of the second
    hidden layer and thus, are effectively the new features with respect to that layer.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出可以通过依次计算每一层神经元的输出来计算。第一隐藏层单元的输出可以使用我们迄今为止看到的神经元输出方程来计算。这些输出成为第二隐藏层神经元的输入，因此，它们实际上是相对于该层的新特征。
- en: One of the strengths of neural networks is this power to learn new features
    through the learning of weights in the hidden layers. This process repeats for
    every layer in the neural network until the final layer, where we obtain the output
    of the neural network as a whole. This process of propagating the signals from
    the input to the output layer is known as **forward propagation**.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个优势是这种通过学习隐藏层中的权重来学习新特征的能力。这个过程在神经网络的每一层重复进行，直到最终层，在那里我们获得整个神经网络的输出。从输入层到输出层的信号传播过程被称为**正向传播**。
- en: Training multilayer perceptron networks
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练多层感知器网络
- en: Multilayer perceptron networks are more complicated to train than a single perceptron.
    The famous algorithm used to train them--which has been around since the 1980s--is
    known as the **backpropagation algorithm**. We'll give a sketch of how this algorithm
    works here, but the reader interested in neural networks is strongly encouraged
    to read up on this algorithm in more depth.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 多层感知器网络比单个感知器更难训练。用于训练它们的著名算法——自20世纪80年代以来一直存在——被称为**反向传播算法**。在这里，我们将简要介绍这个算法的工作原理，但强烈建议对神经网络感兴趣的读者深入了解这个算法。
- en: There are two very important insights to understand this algorithm. The first
    is that for every observation, it proceeds in two steps. The forward propagation
    step begins at the input layer and ends at the output layer, and computes the
    predicted output of the network for this observation. This is relatively straightforward
    to do using the equation for the output of each neuron, which is just the application
    of its activation function on the linear weighted sum of its inputs.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 理解这个算法有两个非常重要的洞察。第一个是，对于每一个观察，它分为两个步骤进行。正向传播步骤从输入层开始，到输出层结束，并计算网络对于这个观察的预测输出。这相对简单，可以使用每个神经元的输出方程来完成，这仅仅是将其输入的线性加权和应用其激活函数。
- en: The backward propagation step is designed to modify the weights of the network
    when the predicted output does not match the desired output. This step begins
    at the output layer, computing the error on the output nodes and the necessary
    updates to the weights of the output neurons. Then, it moves backwards through
    the network, updating the weights of each hidden layer in reverse until it reaches
    the first hidden layer, which is processed last. Thus, there is a forward pass
    through the network, followed by a backward pass.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播步骤是为了在预测输出与期望输出不匹配时修改网络的权重。这一步骤从输出层开始，计算输出节点的误差以及输出神经元权重的必要更新。然后，它通过网络反向移动，反向更新每个隐藏层的权重，直到达到最后一个隐藏层，它被最后处理。因此，网络中有一个正向传递，然后是一个反向传递。
- en: The second important insight to understand is that updating the weights of the
    neurons in the hidden layer is substantially trickier than updating the weights
    in the output layer. To see this, consider that when we want to update the weights
    of neurons in the output layer, we know precisely what the desired output for
    that neuron should be for a given input.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要的洞察是，更新隐藏层中神经元的权重比更新输出层的权重要复杂得多。为了看到这一点，考虑当我们想要更新输出层中神经元的权重时，我们知道对于给定的输入，该神经元应该有什么期望输出。
- en: This is because the desired outputs of the output neurons are the outputs of
    the network itself, which are available to us in our training data. By contrast,
    at first glance, we don't actually know what the right output of a neuron in a
    hidden layer should be for a particular input. Additionally, this output is distributed
    to all the neurons of the next layer in the network and hence impacts all of their
    outputs as well.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为输出神经元的期望输出就是网络本身的输出，这些输出在我们的训练数据中都是可用的。相比之下，乍一看，我们实际上并不知道对于一个特定的输入，隐藏层中一个神经元的正确输出应该是什么。此外，这个输出被分布到网络中下一层的所有神经元，因此影响了它们的输出。
- en: 'The key insight here is that we propagate the error made in the output neurons
    back to the neurons in the hidden layers. We do this by finding the gradient of
    the cost function to adjust the weights of the neurons in the direction of the
    greatest error reduction and apply the chain rule of differentiation to express
    this gradient in terms of the output of the individual neuron we are interested
    in. This process results in a general formula for updating the weights of any
    neuron in the network, known as the **delta update rule**:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的关键洞察是，我们将输出神经元中犯的错误传播回隐藏层中的神经元。我们通过找到成本函数的梯度来调整神经元的权重，以减少最大误差的方向，并应用微分链式法则将这个梯度用我们感兴趣的个别神经元的输出来表示。这个过程导致了一个更新网络中任何神经元权重的通用公式，称为**delta更新规则**：
- en: '![Training multilayer perceptron networks](img/00099.jpeg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![训练多层感知器网络](img/00099.jpeg)'
- en: Let's understand this equation by assuming that we are currently processing
    the weights for all the neurons in layer l. This equation tells us how to update
    the weight between the *j^(th)* neuron in layer l and the *i^(th)* neuron in the
    layer before it (layer l-1). The (*n*) superscripts all denote the fact that we
    are currently updating the weight as a result of processing the *n^(th)* observation
    in our dataset. We will drop these from now on, and assume they are implied.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过假设我们目前正在处理层l中所有神经元的权重来理解这个方程。这个方程告诉我们如何更新层l中第*j*个神经元和它之前一层（层l-1）中第*i*个神经元之间的权重。所有的(*n*)上标都表示我们目前正在更新权重，这是由于处理数据集中第*n*个观察的结果。从现在起，我们将省略这些上标，并假设它们是隐含的。
- en: 'In a nutshell, the delta rule tells us that to obtain the new value of the
    neuron weight; we must add a product of three terms to the old value. The first
    of these terms is the learning rate *η*. The second is known as the local gradient,
    *δ[j]*, and is the product of the error, *e[j]*, of neuron *j* and the gradient
    of its activation function, *g()*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，delta规则告诉我们，为了获得神经元权重的新的值，我们必须将三个项的乘积加到旧值上。这三个项中的第一个是学习率 *η*。第二个被称为局部梯度，*δ[j]*，它是神经元
    *j* 的误差，*e[j]*，与其激活函数的梯度，*g()* 的乘积：
- en: '![Training multilayer perceptron networks](img/00100.jpeg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![训练多层感知器网络](img/00100.jpeg)'
- en: 'Here, we denote the output of neuron *j* before applying its activation function
    by *z[j]*, so that the following relation holds:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们用 *z[j]* 表示应用其激活函数之前神经元 *j* 的输出，以便以下关系成立：
- en: '![Training multilayer perceptron networks](img/00101.jpeg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![训练多层感知器网络](img/00101.jpeg)'
- en: 'It turns out that the local gradient is also the gradient of the cost function
    of the network computed with respect to *z[j]*. Finally, the third term in the
    delta update rule is the input to neuron *j* from neuron *i*, which is just the
    output of neuron *i*, *y[i]*. The only term that differs between output layer
    neurons and hidden layer neurons is the local gradient term. We''ll see an illustrative
    example for neural networks that perform classification using logistic neurons
    throughout. When neuron *j* is an output neuron, the local gradient is given by:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，局部梯度也是网络成本函数相对于 *z[j]* 计算的梯度。最后，delta更新规则中的第三项是来自神经元 *i* 的神经元 *j* 的输入，这仅仅是神经元
    *i* 的输出，*y[i]*。输出层神经元和隐藏层神经元之间唯一不同的项是局部梯度项。我们将通过一个示例来说明使用逻辑神经元进行分类的神经网络。当神经元 *j*
    是输出神经元时，局部梯度由以下公式给出：
- en: '![Training multilayer perceptron networks](img/00102.jpeg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![训练多层感知器网络](img/00102.jpeg)'
- en: 'The first term in brackets is just the known error of the output neuron, this
    being the difference between the target output, *t[j]*, and the actual output,
    *y[j]*. The other two terms arise from the differentiation of the logistic activation
    function. When neuron *j* is a hidden layer neuron, the gradient of the logistic
    activation function is the same, but the error term is computed as the weighted
    sum of the local gradients of the *k* neurons in the next layer that receive input
    from neuron *j*:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 方括号中的第一个项是输出神经元的已知误差，这是目标输出，*t[j]*，与实际输出，*y[j]* 之间的差异。其他两个项来自逻辑激活函数的微分。当神经元
    *j* 是隐藏层神经元时，逻辑激活函数的梯度相同，但误差项是下一个层中接收来自神经元 *j* 输入的 *k* 个神经元的局部梯度的加权总和：
- en: '![Training multilayer perceptron networks](img/00103.jpeg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![训练多层感知器网络](img/00103.jpeg)'
- en: The back propagation algorithm
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 反向传播算法
- en: The backward propagation of errors, or simply backpropagation, is another somewhat
    common method for training artificial neural networks and it is used in combination
    with an optimization method (such as gradient descent, which is described later
    in this chapter).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 错误反向传播，或简称反向传播，是另一种用于训练人工神经网络的常见方法，它通常与一种优化方法（如本章后面将要描述的梯度下降法）结合使用。
- en: The goal of backpropagation is to *optimize the weights* so that the neural
    network model can learn how to correctly map arbitrary inputs to outputs. In other
    words, when using back propagation, the initial system output is continually compared
    to the desired output, and the system is adjusted until the difference between
    the two is minimized.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的目标是 *优化权重*，以便神经网络模型能够学习如何正确地将任意输入映射到输出。换句话说，当使用反向传播时，初始系统输出会持续与期望输出进行比较，系统会进行调整，直到两者之间的差异最小化。
- en: Predicting the energy efficiency of buildings
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测建筑物的能源效率
- en: In this section, we will investigate how neural networks can be used to solve
    a real-world regression problem. Once again, we turn to the UCI Machine Learning
    Repository for our dataset. We've chosen to try out the *energy efficiency dataset*
    available at [http://archive.ics.uci.edu/ml/datasets/Energy+efficiency](http://archive.ics.uci.edu/ml/datasets/Energy+efficiency).
    The prediction task is to use various building characteristics, such as surface
    area and roof area, in order to predict the energy efficiency of a building, which
    is expressed in the form of two different metrics--heating load and cooling load.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将研究如何使用神经网络解决一个实际的回归问题。再次，我们转向UCI机器学习仓库以获取我们的数据集。我们选择尝试位于[http://archive.ics.uci.edu/ml/datasets/Energy+efficiency](http://archive.ics.uci.edu/ml/datasets/Energy+efficiency)的*能源效率数据集*。预测任务是使用各种建筑特征，如表面积和屋顶面积，来预测建筑的能源效率，该效率以两种不同的指标形式表达——加热负荷和冷却负荷。
- en: 'This is a good example for us to try out as we can demonstrate how neural networks
    can be used to predict two different outputs with a single network. The full attribute
    description of the dataset is given in the following table:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个很好的例子，我们可以用它来展示如何使用单个神经网络预测两个不同的输出。数据集的完整属性描述如下表所示：
- en: '| Column name | Type | Definition |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| 列名 | 类型 | 定义 |'
- en: '| --- | --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `relCompactness` | Numerical | Relative compactness |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| `relCompactness` | 数值 | 相对紧凑度 |'
- en: '| `surfArea` | Numerical | Surface area |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `surfArea` | 数值 | 表面积 |'
- en: '| `wallArea` | Numerical | Wall area |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `wallArea` | 数值 | 墙面积 |'
- en: '| `roofArea` | Numerical | Roof area |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `roofArea` | 数值 | 屋顶面积 |'
- en: '| `height` | Numerical | Overall height |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `height` | 数值 | 总高度 |'
- en: '| `orientation` | Numerical | Building orientation (factor) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `orientation` | 数值 | 建筑朝向（因子） |'
- en: '| `glazArea` | Numerical | Glazing area |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `glazArea` | 数值 | 玻璃面积 |'
- en: '| `glazAreaDist` | Numerical | Glazing area distribution (factor) |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `glazAreaDist` | 数值 | 玻璃面积分布（因子） |'
- en: '| `heatLoad` | Numerical | Heating load (first output) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `heatLoad` | 数值 | 加热负荷（第一个输出） |'
- en: '| `coolLoad` | Numerical | Cooling load (second output) |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `coolLoad` | 数值 | 冷却负荷（第二个输出） |'
- en: The data was generated using a simulator called *Ecotect*. Each observation
    in the dataset corresponds to a simulated building. All the buildings have the
    same volume, but other attributes that impact their energy efficiency, such as
    their glazing area, are modified.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 数据是使用名为*Ecotect*的模拟器生成的。数据集中的每个观测值对应一个模拟建筑。所有建筑具有相同的体积，但影响其能效的其他属性，如玻璃面积，则进行了修改。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: This dataset is described in the paper *Accurate quantitative estimation of
    energy performance of residential buildings using statistical machine learning
    tools*, *Athanasios Tsanas* and *Angeliki Xifara*, published in *Energy and Buildings*,
    Vol. 49, in 2012.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集在2012年发表的论文《使用统计机器学习工具准确估计住宅建筑能效》中进行了描述，该论文由*Athanasios Tsanas*和*Angeliki
    Xifara*撰写，发表于*Energy and Buildings*，第49卷。
- en: 'The data on the website comes in Microsoft Excel format. To load this into
    R, we can use the R package `xlsx`, which can read and understand Microsoft Excel
    files:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 网站上的数据以Microsoft Excel格式提供。要将这些数据加载到R中，我们可以使用R包`xlsx`，它可以读取和理解Microsoft Excel文件：
- en: '[PRE3]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The import adds a number of empty observations at the end of the data frame,
    so the last line removes these. Now, by referring to the paper in which the dataset
    was presented, we discover that two of our attributes are actually factors. In
    order for our neural network to work with these, we will need to convert them
    into dummy variables. To do this, we will use the `dummyVars()` function from
    the `caret` package:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 导入操作在数据框的末尾添加了一些空观测值，因此最后一行删除了这些值。现在，通过参考介绍数据集的论文，我们发现我们的两个属性实际上是因子。为了让我们的神经网络能够使用这些属性，我们需要将它们转换为虚拟变量。为此，我们将使用`caret`包中的`dummyVars()`函数：
- en: '[PRE4]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `dummyVars()` function takes in a formula and a data frame. From these,
    it identifies the input features and performs dummy encoding on those that are
    factors in order to produce new binary columns. There are as many columns created
    for a factor as there are levels in that factor. Just as with the `preProcess()`
    function that we''ve been using, we actually obtain the columns themselves after
    using the `predict()` function. Next, we''ll do an 80-20 split between the training
    and test data:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: '`dummyVars()`函数接受一个公式和一个数据框。从这些中，它识别输入特征并对那些是因子的特征进行虚拟编码，以产生新的二进制列。为因子创建的列数与该因子的级别数相同。就像我们之前使用的`preProcess()`函数一样，我们实际上是在使用`predict()`函数后获得这些列。接下来，我们将在训练数据和测试数据之间进行80-20的分割：'
- en: '[PRE5]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: One of the most important preprocessing steps to perform when training neural
    networks is to scale input features and outputs. One good reason to perform input
    scaling is in order to avoid **saturation,** which occurs when the optimization
    procedure reaches a point where the gradient of the error function is very small
    in absolute value. This is usually the result of very large or very small inputs
    to the nonlinear neuron activation functions. Saturation causes the optimization
    procedure to terminate, thinking we have converged.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练神经网络时，执行最重要的预处理步骤之一是缩放输入特征和输出。执行输入缩放的一个好理由是为了避免**饱和**，这发生在优化过程达到一个点，其中误差函数的梯度绝对值非常小。这通常是由于非线性神经元激活函数的输入非常大或非常小。饱和会导致优化过程终止，认为我们已经收敛。
- en: 'Depending on the particular neural network implementation, for regression tasks
    it may also make sense to scale the outputs as some implementations of linear
    neurons are designed to produce an output in the interval [-1,1]. Scaling can
    also help convergence. Consequently, we will use `caret` to scale all our data
    dimensions to the unit interval, noting that this has no effect on the binary
    columns produced earlier:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特定的神经网络实现，对于回归任务，也可能有理由对输出进行缩放，因为一些线性神经元的实现被设计为在区间[-1,1]内产生输出。缩放也有助于收敛。因此，我们将使用`caret`将所有数据维度缩放到单位区间，注意这不会影响之前产生的二进制列：
- en: '[PRE6]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Several different packages implement neural networks in R, each with their
    various merits and strengths. For this reason, it helps to be familiar with more
    than one package and in this chapter we will investigate three of these, the first
    being `neuralnet`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 几个不同的包在R中实现了神经网络，每个包都有其各自的优点和优势。因此，熟悉多个包是有帮助的，在本章中，我们将研究这三个包，首先是`neuralnet`：
- en: '[PRE7]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `neuralnet()` function trains a neural network based on the information
    provided in its arguments. The first argument we provide is a formula and the
    format is similar to the formulae that we've seen with the `lm()` and `glm()`
    functions in previous chapters. One interesting difference here is that we have
    specified two outputs, `heatLoad` and `coolLoad`. Another difference is that currently
    we are unable to use the dot (`.`) notation to imply that all the remaining columns
    in our data frame can be used as features, so we need to specify them explicitly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet()`函数根据其参数中提供的信息训练一个神经网络。我们提供的第一个参数是一个公式，其格式与我们在前几章中看到的`lm()`和`glm()`函数中的公式类似。这里的一个有趣差异是我们指定了两个输出，`heatLoad`和`coolLoad`。另一个差异是，目前我们无法使用点(`.`)表示法来暗示我们数据框中剩余的所有列都可以用作特征，因此我们需要明确指定它们。'
- en: Note that with the formula, we have effectively defined the input and output
    layers of the neural network and so what remains to be specified is the structure
    of the hidden layers. This is specified with the `hidden` parameter, which either
    takes in a scalar for a single layer, or a vector of scalars that specify the
    number of hidden units in each layer, starting from the layer just after the input
    layer, and ending with the layer just before the output layer.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用公式后，我们已经有效地定义了神经网络的输入层和输出层，因此需要指定的是隐藏层的结构。这通过`hidden`参数来指定，它可以是单个标量表示单层，或者是一个标量向量，指定了从输入层之后的每一层到输出层之前的每一层的隐藏单元数量。
- en: 'In the example we saw earlier, we''ve used a single layer with 10 nodes. We
    can actually visualize our neural network as the package provides us with the
    ability to plot the model directly (the numbered circles are dummy bias neurons):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前看到的例子中，我们使用了一个包含10个节点的单层。实际上，我们可以将我们的神经网络可视化，因为该包提供了直接绘制模型的能力（编号的圆圈是虚拟偏置神经元）：
- en: '![Predicting the energy efficiency of buildings](img/00104.jpeg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![预测建筑物的能源效率](img/00104.jpeg)'
- en: The call to `neuralnet()` also allows us to specify what type of activation
    function we would like to use for our neurons through the parameter string `act.fct`.
    By default, this is set to the logistic activation function and so we have not
    changed this. Another very important parameter is `linear.output`, which can be
    either `TRUE` or `FALSE`. This specifies whether we should apply the activation
    function to the neurons in the output layer. The default value of `TRUE` that
    we used means that we do not apply the activation function and so we can observe
    a linear output. For regression type problems, this is what is appropriate. This
    is because; if we were to apply a logistic activation function our output would
    be bounded in the interval [0,1]. Finally, we can specify a differentiable error
    function through the `err.fct` parameter to use as part of our optimization strategy.
    As we are doing regression, we use the default value of `sse`, which corresponds
    to the sum of squared error.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 对`neuralnet()`的调用还允许我们通过参数字符串`act.fct`指定我们希望为神经元使用的激活函数类型。默认情况下，这被设置为逻辑激活函数，所以我们没有更改它。另一个非常重要的参数是`linear.output`，它可以设置为`TRUE`或`FALSE`。这指定了我们应该是否将激活函数应用于输出层的神经元。我们使用的默认值`TRUE`意味着我们不应用激活函数，因此我们可以观察到线性输出。对于回归类型问题，这是合适的。这是因为；如果我们应用逻辑激活函数，我们的输出将限制在区间[0,1]内。最后，我们可以通过`err.fct`参数指定一个可微分的误差函数，作为我们优化策略的一部分。由于我们正在进行回归，我们使用默认值`sse`，它对应于平方误差之和。
- en: 'As there is a random component in neural network training, namely the initialization
    of the weights, we may want to specify that we should retrain the same model a
    number of times in order for us to pick the best possible model that we get (using
    criteria such as the SSE to rank these). This can be done by specifying an integer
    value for the `rep` parameter. Let''s rewrite our original call to explicitly
    show the default values we are using:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络训练中存在随机成分，即权重的初始化，我们可能希望指定我们应该多次重新训练同一个模型，以便我们能够选择最佳可能的模型（使用SSE等标准对这些进行排名）。这可以通过指定`rep`参数的整数值来完成。让我们重写我们原始的调用，以明确显示我们正在使用的默认值：
- en: '[PRE8]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model's output provides us with some information about the performance of
    the neural network and what is shown depends on its configuration. As we have
    specified the SSE as our error metric, the error shown is the SSE that was obtained.
    The threshold figure is just the value of the partial derivative of the error
    function when the model stopped training. Essentially, instead of terminating
    when the gradient is 0 exactly, we specify a very small value below which the
    error gradient needs to fall before the algorithm terminates. The default value
    for this is 0.01 and it can be changed by supplying a number for the threshold
    parameter in the `neuralnet()` function. Reducing this value will generally result
    in longer training times. The model output also shows us the number of training
    steps that were performed. Finally, if we had used the `rep` parameter to repeat
    this process multiple times, we would see a row for each model trained. Our output
    shows us that we trained only one model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的输出为我们提供了关于神经网络性能的一些信息，显示的内容取决于其配置。由于我们已指定SSE作为我们的误差度量，显示的误差就是所获得的SSE。阈值数字仅仅是当模型停止训练时误差函数偏导数的值。本质上，我们不是在梯度精确为0时终止，而是指定一个非常小的值，误差梯度需要下降到这个值以下算法才会终止。这个值的默认值是0.01，可以通过在`neuralnet()`函数中提供一个数字来改变阈值参数。降低这个值通常会导致更长的训练时间。模型输出还显示了执行的训练步骤数量。最后，如果我们使用了`rep`参数重复这个过程多次，我们会看到每个训练的模型都有一行。我们的输出显示我们只训练了一个模型。
- en: Tip
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: As the neural network contains a random component in the form of the initialization
    of weight vectors, reproducing our code will likely not give the exact same results.
    If, when running the examples, R outputs a message that the model has not converged,
    try running the code again.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络包含一个以权重向量初始化形式存在的随机成分，重新运行我们的代码可能不会给出完全相同的结果。如果在运行示例时，R输出一个消息表示模型没有收敛，请尝试再次运行代码。
- en: Evaluating multilayer perceptrons for regression
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估用于回归的多层感知器
- en: 'The package `neuralnet` provides us with a neat way to use our model to perform
    predictions through the `compute()` function. Essentially, it provides us with
    not only the predicted output for a data frame of observations, but also shows
    us the output values of all the neurons in the model''s architecture. To evaluate
    the performance of the model, we are interested in the outputs of the neural network
    on our test set:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '`neuralnet`包为我们提供了一个方便的方法，通过`compute()`函数使用我们的模型进行预测。本质上，它不仅为我们提供了观察数据框的预测输出，还显示了模型架构中所有神经元的输出值。为了评估模型的性能，我们对神经网络在测试集上的输出感兴趣：'
- en: '[PRE9]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can access the predicted outputs of the neural network using the `net.result`
    attribute of the `test_predictions` object as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过`test_predictions`对象的`net.result`属性访问神经网络的预测输出，如下所示：
- en: '[PRE10]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As this is a regression problem, we would like to be able to use the MSE in
    order to evaluate the performance of our model on both target outputs. In order
    to do that, we need to transform our predicted outputs back onto their original
    scale for a fair assessment to be made. The scaling constants we used on our data
    are stored in the `ranges` attribute of the `eneff_train_out_pp` object:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个回归问题，我们希望能够使用均方误差（MSE）来评估我们的模型在目标输出上的性能。为了做到这一点，我们需要将我们的预测输出转换回原始尺度，以便进行公平的评估。我们用于数据缩放的常数存储在`eneff_train_out_pp`对象的`ranges`属性中：
- en: '[PRE11]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The first row contains the minimum values of the original data, and the second
    row contains the maximum values. We''ll now write a function that will take in
    a scaled vector and another vector that contains the original minimum and maximum
    values, and will return the original unscaled vector:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行包含原始数据的最小值，第二行包含最大值。现在我们将编写一个函数，该函数将接受一个缩放向量以及包含原始最小值和最大值的另一个向量，并返回原始未缩放向量：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we''ll use this to obtain the unscaled predicted outputs for our test
    set:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用这个方法来获取测试集的未缩放预测输出：
- en: '[PRE13]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We can also define a simple function to compute the MSE and use it to check
    the performance on our two tasks:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以定义一个简单的函数来计算MSE，并使用它来检查我们在两个任务上的性能：
- en: '[PRE14]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'These values are very low, indicating that we have very good prediction accuracy.
    We can also investigate correlation, which is scale independent, and we could
    have used it on the unscaled outputs as well:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值非常低，表明我们的预测准确度非常高。我们还可以研究相关性，它是尺度无关的，我们也可以在未缩放输出上使用它：
- en: '[PRE15]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These values are extremely high, indicating that we have near-perfect performance,
    something very rare to see with real-world data. If the accuracy were not this
    high, we would experiment by making the architecture more complicated. We could,
    for example, build a model with an additional layer by setting `hidden=c(10,5)`
    so that we would have an additional layer of five neurons before the output layer.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这些值非常高，表明我们几乎达到了完美的性能，这在现实世界的数据中是非常罕见的。如果准确率不是这么高，我们可能会通过使架构更复杂来进行实验。例如，我们可以通过设置`hidden=c(10,5)`来构建一个具有额外层的模型，这样我们就在输出层之前有一个额外的五神经元层。
- en: Predicting glass type revisited
  id: totrans-189
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重新审视预测玻璃类型
- en: In [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Linear Regression*, we analyzed the glass identification
    dataset, whose task is to identify the type of glass comprising a glass fragment
    found at a crime scene. The output of this dataset is a factor with several class
    levels corresponding to different types of glass. Our previous approach was to
    build a one-versus-all model using multinomial logistic regression. The results
    were not very promising, and one of the main points of concern was a poor model
    fit on the training data.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7 "第3章。线性回归")中，我们分析了玻璃识别数据集，其任务是识别在犯罪现场发现的玻璃碎片所属的玻璃类型。该数据集的输出是一个具有多个类别级别的因子，对应不同的玻璃类型。我们之前的方法是使用多项逻辑回归构建一个一对一模型。结果并不十分令人鼓舞，主要问题之一是训练数据上的模型拟合度较差。
- en: 'In this section, we will revisit this dataset and see whether a neural network
    model can do better. At the same time, we will demonstrate how neural networks
    can handle classification problems as well:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新审视这个数据集，看看神经网络模型是否能做得更好。同时，我们将展示神经网络如何处理分类问题：
- en: '[PRE16]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Our output is a multiclass factor and so we will want to dummy-encode this into
    binary columns. With the `neuralnet` package, we would normally need to do this
    manually as a preprocessing step before we can build our model.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的输出是一个多类因素，因此我们希望将其虚拟编码为二进制列。在`neuralnet`包中，我们通常需要作为预处理步骤手动执行此操作，然后才能构建我们的模型。
- en: 'In this section, we will look at a second package that contains functions for
    building neural networks, `nnet`. This is actually the same package that we used
    for multinomial logistic regression. One of the benefits of this package is that
    for multiclass classification, the `nnet()` function that trains the neural network
    will automatically detect outputs that are factors and perform the dummy encoding
    for us. With that in mind, we will prepare a training and test set:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨一个包含构建神经网络函数的第二个包，即`nnet`。实际上，这正是我们用于多项式逻辑回归的同一个包。这个包的一个好处是，对于多类分类，训练神经网络的`nnet()`函数将自动检测输出因素并为我们执行虚拟编码。考虑到这一点，我们将准备一个训练集和测试集：
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, just as with our previous dataset, we will normalize our input data:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，就像我们之前的数据集一样，我们将对输入数据进行归一化处理：
- en: '[PRE18]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We are now ready to train our model. Whereas the `neuralnet` package is able
    to model multiple hidden layers, the `nnet` package is designed to model neural
    networks with a single hidden layer. As a result, we still specify a formula as
    before, but this time, instead of a `hidden` parameter that can be either a scalar
    or a vector of integers, we specify a `size` parameter that is an integer representing
    the number of nodes in the single hidden layer of our model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好训练我们的模型。虽然`neuralnet`包能够模拟多个隐藏层，但`nnet`包旨在模拟具有单个隐藏层的神经网络。因此，我们仍然像以前一样指定一个公式，但这次，我们不是指定一个可以是标量或整数向量的`hidden`参数，而是指定一个`size`参数，它是一个整数，表示模型单个隐藏层中的节点数。
- en: 'Also, the default neural network model in the `nnet` package is for classification,
    as the output layer uses a logistic activation function. It is really important
    when working with different packages for training the same type of model, such
    as multilayer perceptrons, to check the default values for the various model parameters,
    as these will be different from package to package. One other difference between
    the two packages that we will mention here is that `nnet` currently does not offer
    any plotting capabilities. Without further ado, we will now train our model:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，`nnet`包中的默认神经网络模型用于分类，因为输出层使用逻辑激活函数。当使用不同包训练相同类型的模型时，例如多层感知器，检查各种模型参数的默认值非常重要，因为这些值会因包而异。我们在这里要提到的两个包之间的另一个区别是，`nnet`目前不提供任何绘图功能。无需进一步说明，我们现在将训练我们的模型：
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'From the output, we can see that the model has not converged, stopping after
    the default value of 100 iterations. To converge, we can either rerun this code
    a number of times or we can increase the number of allowed iterations to 1,000
    using the `maxit` parameter:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中我们可以看到，模型没有收敛，在默认的100次迭代后停止。为了收敛，我们可以多次重新运行此代码，或者我们可以使用`maxit`参数将允许的迭代次数增加到1,000：
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Let''s first investigate the accuracy of our model on the training data in
    order to assess the quality of fit. To compute predictions, we use the `predict()`
    function and specify the type parameter to be `class`. This lets the `predict()`
    function know that we want the class with highest probability to be selected.
    If we want to see the probabilities of each class, we can specify the value `response`
    for the `type` parameter. Finally, remember that we must pass in a data frame
    without the outputs to the `predict()` function, and thus the need to subset the
    training data frame:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先调查我们的模型在训练数据上的准确性，以评估拟合质量。为了计算预测值，我们使用`predict()`函数并指定类型参数为`class`。这会让`predict()`函数知道我们想要选择概率最高的类别。如果我们想看到每个类别的概率，我们可以为`type`参数指定值`response`。最后，请记住，我们必须将不带输出的数据框传递给`predict()`函数，因此需要对训练数据框进行子集化：
- en: '[PRE21]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our first attempt shows us that we are getting the same quality of fit as with
    our multinomial logistic regression model. To improve upon this, we''ll increase
    the complexity of the model by adding more neurons in our hidden layer. We will
    also increase our `maxit` parameter to `10,000` as the model is more complex and
    might need more iterations to converge:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们第一次尝试表明，我们得到的拟合质量与我们的多项式逻辑回归模型相同。为了改进这一点，我们将通过在隐藏层中添加更多神经元来增加模型的复杂性。我们还将把`maxit`参数增加到`10,000`，因为模型更复杂，可能需要更多的迭代才能收敛：
- en: '[PRE22]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As we can see, we have now achieved 100 percent training accuracy. Now that
    we have a decent model fit, we can investigate our performance on the test set:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们现在已经达到了100%的训练准确率。现在我们有一个相当好的模型拟合，我们可以调查我们在测试集上的性能：
- en: '[PRE23]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Even though our model fits the training data perfectly, we see that the accuracy
    on the test set is only 60 percent. Even factoring in that the dataset is very
    small, this discrepancy is a classic signal that our model is overfitting on the
    training data. When we looked at linear and logistic regression, we saw that there
    are shrinkage methods, such as the lasso, which are designed to combat overfitting
    by restricting the size of the coefficients in the model.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的模型完美地拟合了训练数据，但我们看到测试集上的准确率仅为60%。即使考虑到数据集非常小，这种差异也是一个经典的信号，表明我们的模型在训练数据上过度拟合。当我们查看线性回归和逻辑回归时，我们看到了如lasso这样的收缩方法，这些方法旨在通过限制模型中系数的大小来对抗过度拟合。
- en: 'An analogous technique known as **weight decay** exists for neural networks.
    With this approach, the product of a decay constant and the sum of the squares
    of all the network weights is added to the cost function. This limits any weights
    from taking overly large values and thus performs regularization on the network.
    Whereas there is currently no option for regularization with `neuralnet()`, `nnet()`
    uses the `decay` parameter:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 对于神经网络，存在一个称为**权重衰减**的类似技术。使用这种方法，将衰减常数与所有网络权重平方和的乘积添加到成本函数中。这限制了任何权重取过大的值，从而对网络进行正则化。尽管目前`neuralnet()`没有正则化选项，但`nnet()`使用`decay`参数：
- en: '[PRE24]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: With this model, the fit on our training data is still very high, and substantially
    higher than we achieved with multinomial logistic regression. On the test set,
    the performance is still worse than on the training set, but much better than
    we had before.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个模型，我们的训练数据拟合仍然非常高，并且比我们使用多项式逻辑回归所达到的要高得多。在测试集上，性能仍然比训练集差，但比我们之前的好得多。
- en: We won't spend any more time on the glass identification data. Instead, we will
    reflect on a few lessons learned before moving on. The first of these is that
    achieving good performance with a neural network, and sometimes even just reaching
    convergence, might be tricky. Training the model involves a random initialization
    of network weights and the final result is often quite sensitive to these starting
    conditions. We can convince ourselves of this fact by training the different model
    configurations we have seen so far a number of times and noticing that certain
    configurations on some runs might not converge, and the performance on our training
    and test set does tend to differ from one run to the next.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会在玻璃识别数据上花费更多时间。相反，我们将在继续之前反思一些学到的经验教训。其中之一是，使用神经网络获得良好的性能，有时甚至只是达到收敛，可能很棘手。训练模型涉及网络权重的随机初始化，最终结果往往对这些初始条件非常敏感。我们可以通过多次训练我们迄今为止看到的不同模型配置，并注意到某些运行中的某些配置可能无法收敛，以及我们的训练集和测试集的性能确实会随每次运行而有所不同，来证实这一事实。
- en: Another insight is that training a neural network involves tuning a diverse
    range of parameters, from the number and arrangement of hidden neurons to the
    value of the `decay` parameter. Others that we did not experiment with include
    the choice of nonlinear activation function to use with the hidden layer neurons,
    the criteria for convergence, and the particular cost function we use to fit our
    model. For example, instead of using least squares, we could use a criterion known
    as **entropy**.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个洞见是训练神经网络涉及调整各种参数，从隐藏神经元的数量和排列到`衰减`参数的值。我们没有实验过的其他参数包括用于隐藏层神经元的非线性激活函数的选择、收敛的准则以及我们用来拟合模型的特定成本函数。例如，我们不是使用最小二乘法，而可以使用一个称为**熵**的准则。
- en: 'Before settling on a final choice of model, therefore, it pays to try out as
    many different combinations of these as possible. A good place to experiment with
    different parameter combinations is the `train()` function of the `caret` package.
    It provides a unified interface for both neural network packages we have seen
    and, in conjunction with `expand.grid()`, allows the simultaneous training and
    evaluation of several different neural network configurations. We''ll provide
    just a vignette here, and the interested reader can use this to continue their
    investigation further:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在确定最终模型选择之前，尝试尽可能多的不同组合是值得的。在`caret`包的`train()`函数中实验不同的参数组合是一个好地方。它为我们所看到的神经网络包提供了一个统一的接口，并且与`expand.grid()`结合使用，允许同时训练和评估几个不同的神经网络配置。我们在这里只提供一个示例，感兴趣的读者可以使用这个示例继续他们的研究：
- en: '[PRE25]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Predicting handwritten digits
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测手写数字
- en: Our final application for neural networks will be the handwritten digit prediction
    task. In this task, the goal is to build a model that will be presented with an
    image of a numerical digit (0–9) and the model must predict which digit is being
    shown. We will use the *MNIST* database of handwritten digits from [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们神经网络应用的最终任务是手写数字预测。在这个任务中，目标是构建一个模型，该模型将展示一个数字（0-9）的图像，并且模型必须预测显示的是哪个数字。我们将使用来自[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)的*MNIST*手写数字数据库。
- en: 'From this page, we have downloaded and unzipped the two training files, `train-images-idx3-ubyte.gz`
    and `train-images-idx3-ubyte.gz`. The former contains the data from the images
    and the latter contains the corresponding digit labels. The advantage of using
    this website is that the data has already been preprocessed by centering each
    digit in the image and scaling the digits to a uniform size. To load the data,
    we''ve used information from the website about the IDX format to write two functions:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个页面，我们已经下载并解压了两个训练文件，`train-images-idx3-ubyte.gz`和`train-images-idx3-ubyte.gz`。前者包含图像数据，后者包含相应的数字标签。使用这个网站的优势在于数据已经被预处理，每个数字都在图像中居中，并且将数字缩放到统一的大小。为了加载数据，我们使用了网站关于IDX格式的信息来编写两个函数：
- en: '[PRE26]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can then load our two data files by issuing the following two commands:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过以下两个命令加载我们的两个数据文件：
- en: '[PRE27]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Each image is represented by a 28-pixel by 28-pixel matrix of grayscale values
    in the range 0 to 255, where 0 is white and 255 is black. Thus, our observations
    each have 282 = 784 feature values. Each image is stored as a vector by rasterizing
    the matrix from right to left and top to bottom. There are 60,000 images in the
    training data, and our `mnist_train` object stores these as a matrix of 60,000
    rows by 78 columns so that each row corresponds to a single image. To get an idea
    of what our data looks like, we can visualize the first seven images:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像由一个28像素乘以28像素的灰度值矩阵表示，灰度值范围在0到255之间，其中0是白色，255是黑色。因此，我们的每个观测值都有28^2=784个特征值。每个图像通过从右到左和从上到下光栅化矩阵存储为一个向量。训练数据中有60,000个图像，我们的`mnist_train`对象将这些存储为一个60,000行乘以78列的矩阵，这样每一行对应一个单独的图像。为了了解我们的数据看起来像什么，我们可以可视化前七个图像：
- en: '![Predicting handwritten digits](img/00105.jpeg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![预测手写数字](img/00105.jpeg)'
- en: To analyze this dataset, we will introduce our third and final R package for
    training neural network models, `RSNNS`. This package is actually an R wrapper
    around the **Stuttgart Neural Network Simulator** (**SNNS**), a popular software
    package containing standard implementations of neural networks in C created at
    the University of Stuttgart.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析这个数据集，我们将介绍我们的第三个也是最后一个用于训练神经网络模型的R包，`RSNNS`。实际上，这个包是围绕**斯图加特神经网络模拟器**（**SNNS**）的R包装器，这是一个在斯图加特大学创建的包含标准神经网络C实现的流行软件包。
- en: 'The package authors have added a convenient interface for the many functions
    in the original software. One of the benefits of using this package is that it
    provides several of its own functions for data processing, such as splitting the
    data into a training and test set. Another is that it implements many different
    types of neural networks, not just MLPs. We will begin by normalizing our data
    to the unit interval by dividing by `255` and then indicating that our output
    is a factor with each level corresponding to a digit:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 包的作者为原始软件中的许多函数添加了一个便捷的接口。使用此包的好处之一是它提供了自己的几个数据处理函数，例如将数据分割成训练集和测试集。另一个好处是它实现了许多不同类型的神经网络，而不仅仅是MLP。我们将首先通过除以`255`将数据规范化到单位区间，然后指出我们的输出是一个因子，每个级别对应一个数字：
- en: '[PRE28]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Although the MNIST website already contains separate files with test data,
    we have chosen to split the training data file as the models already take quite
    a while to run. The reader is encouraged to repeat the analysis that follows with
    the supplied test files as well. To prepare the data for splitting, we will randomly
    shuffle our images in the training data:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然MNIST网站已经包含了包含测试数据的单独文件，但我们选择将训练数据文件分割，因为模型已经运行了相当长的时间。鼓励读者使用提供的测试文件重复以下分析。为了准备分割数据，我们将随机打乱训练数据中的图像：
- en: '[PRE29]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Next, we must dummy-encode our output factor as this is not done automatically
    for us. The `decodeClassLabels()` function from the `RSNNS` package is a convenient
    way to do this. Additionally, we will split our shuffled data into an 80–20 training
    and test set split using `splitForTrainingAndTest()`. This will store the features
    and labels for the training and test sets separately, which will be useful for
    us shortly.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须对输出因子进行虚拟编码，因为这并不是自动为我们完成的。`RSNNS`包中的`decodeClassLabels()`函数是完成这一任务的便捷方式。此外，我们将使用`splitForTrainingAndTest()`函数将我们的打乱数据分成80-20的训练和测试集分割。这将分别存储训练集和测试集的特征和标签，这将在不久的将来对我们很有用。
- en: 'Finally, we can also normalize our data using the `normTrainingAndTestSet()`
    function. To specify unit interval normalization, we must set the `type` parameter
    to `0_1`:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还可以使用`normTrainingAndTestSet()`函数来规范化我们的数据。为了指定单位区间规范化，我们必须将`type`参数设置为`0_1`：
- en: '[PRE30]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: For comparison, we will train two MLP networks using the `mlp()` function. By
    default, this is configured for classification and uses the logistic function
    as the activation function for hidden layer neurons. The first model will have
    a single hidden layer with 100 neurons; the second model will use 300.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们将使用`mlp()`函数训练两个MLP网络。默认情况下，这是配置为分类，并使用逻辑函数作为隐藏层神经元的激活函数。第一个模型将有一个包含100个神经元的单个隐藏层；第二个模型将使用300个。
- en: The first argument to the `mlp()` function is the matrix of input features and
    the second is the vector of labels. The `size` parameter plays the same role as
    the `hidden` parameter in the `neuralnet` package. That is to say, we can specify
    a single integer for a single hidden layer, or a vector of integers specifying
    the number of hidden neurons per layer when we want more than one hidden layer.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`mlp()`函数的第一个参数是输入特征矩阵，第二个参数是标签向量。`size`参数在`neuralnet`包中与`hidden`参数扮演相同的角色。也就是说，当我们想要一个以上的隐藏层时，我们可以指定一个整数来表示单个隐藏层，或者指定一个整数向量来表示每层的隐藏神经元数量。'
- en: 'Next, we can use the `inputsTest` and `targetsTest` parameters to specify the
    features and labels of our test set beforehand, so that we can be ready to observe
    the performance on our test set in one call. The models we will train will take
    several hours to run. If we want to know how long each model took to run, we can
    save the current time using `proc.time()` before training a model and comparing
    it against the time when the model completes. Putting all this together, here
    is how we trained our two MLP models:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`inputsTest`和`targetsTest`参数事先指定测试集的特征和标签，这样我们就可以在一次调用中准备好观察测试集的性能。我们将训练的模型将需要数小时才能运行。如果我们想知道每个模型运行了多长时间，我们可以在训练模型之前使用`proc.time()`保存当前时间，并与模型完成时的时间进行比较。将所有这些放在一起，以下是我们的两个MLP模型是如何训练的：
- en: '[PRE31]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As we can see, the models take quite a long time to run (the values are in
    seconds). For reference, these were trained on a 2.5 GHz Intel Core i7 Apple MacBook
    Pro with 16 GB of memory. The model predictions on our test set are saved in the
    `fittedTestValues` attribute (and for our training set, they are stored in the
    `fitted.values` attribute). We will focus on test set accuracy. First, we must
    decode the dummy-encoded network outputs by selecting the binary column with the
    maximum value. We must also do this for the target outputs. Note that the first
    column corresponds to the digit `0`:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型运行时间相当长（数值以秒为单位）。为了参考，这些模型是在2.5 GHz英特尔酷睿i7苹果MacBook Pro上，16 GB内存上训练的。我们的测试集上的模型预测被保存在`fittedTestValues`属性中（而对于我们的训练集，它们存储在`fitted.values`属性中）。我们将关注测试集的准确率。首先，我们必须通过选择具有最大值的二进制列来解码虚拟编码的网络输出。我们也必须对目标输出执行此操作。请注意，第一列对应数字`0`：
- en: '[PRE32]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now we can check the accuracy of our two models, as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以检查我们两个模型的准确率，如下所示：
- en: '[PRE33]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The accuracy is very high for both models, with the second model slightly outperforming
    the first. We can use the `confusionMatrix()` function to see the errors made
    in detail:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型的准确率都非常高，第二个模型略优于第一个。我们可以使用`confusionMatrix()`函数来详细查看所犯的错误：
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As expected, we see quite a bit of symmetry in this matrix because certain pairs
    of digits are often harder to distinguish than others. For example, the most common
    pair of digits that the model confuses is the pair (3,5). The test data available
    on the website contains some examples of digits that are harder to distinguish
    from others.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，我们在这个矩阵中看到了相当多的对称性，因为某些数字对通常比其他数字对更难区分。例如，模型最常混淆的数字对是(3,5)。网站上可用的测试数据包含一些难以与其他数字区分的数字示例。
- en: By default, the `mlp()` function allows for a maximum of 100 iterations, via
    its `maxint` parameter. Often, we don't know the number of iterations we should
    run for a particular model; a good way to determine this is to plot the training
    and testing error rates versus iteration number. With the RSNNS package, we can
    do this with the `plotIterativeError()` function.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`mlp()`函数通过其`maxint`参数允许最大100次迭代。通常，我们不知道应该为特定模型运行多少次迭代；确定这一点的一个好方法是绘制训练和测试错误率与迭代次数的关系图。使用RSNNS包，我们可以使用`plotIterativeError()`函数来完成这项工作。
- en: 'The following graphs show that for our two models, both errors plateau after
    30 iterations:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示，对于我们的两个模型，两个错误在30次迭代后都达到了平台期：
- en: '![Predicting handwritten digits](img/00106.jpeg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![预测手写数字](img/00106.jpeg)'
- en: Receiver operating characteristic curves
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 受试者工作特征曲线
- en: In [Chapter 3](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7
    "Chapter 3. Linear Regression"), *Logistic Regression*, we studied the precision-recall
    graph as an example of an important graph showing the trade-off between two important
    performance metrics of a binary classifier--precision and recall. In this chapter,
    we will present another related and commonly used graph to show binary classification
    performance, the **receiver operating characteristic** (**ROC**) curve.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](part0026_split_000.html#OPEK2-c6198d576bbb4f42b630392bd61137d7 "第3章。线性回归")中，我们研究了逻辑回归作为展示二元分类器两个重要性能指标（精确率和召回率）之间权衡的重要图表的例子。在本章中，我们将介绍另一个相关且常用的图表来展示二元分类性能，即**受试者工作特征**（**ROC**）曲线。
- en: This curve is a plot of the true positive rate on the *y* axis and the false
    positive rate on the *x* axis. The true positive rate, as we know, is just the
    recall or, equivalently, the sensitivity of a binary classifier. The false positive
    rate is just 1 minus the specificity. A random binary classifier will have a true
    positive rate equal to the false positive rate and thus, on the ROC curve, the
    line *y = x* is the line showing the performance of a random classifier. Any curve
    lying above this line will perform better than a random classifier.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 此曲线是在y轴上的真正例率和x轴上的假正例率之间的一个图。正如我们所知，真正例率只是召回率，或者说是一个二元分类器的灵敏度。假正例率是1减去特异性。一个随机的二元分类器将具有与假正例率相同的真正例率，因此，在ROC曲线上，*y
    = x*线表示随机分类器的性能。任何位于此线之上的曲线都将比随机分类器表现更好。
- en: A perfect classifier will exhibit a curve from the origin to the point (0,1),
    which corresponds to a 100 percent true positive rate and a 0 percent false positive
    rate. We often talk about the **ROC Area Under the Curve** (**ROC AUC**) as a
    performance metric. The area under the random classifier is just 0.5 as we are
    computing the area under the line *y = x* on a unit square. By convention, the
    area under a perfect classifier is 1 as the curve passes through the point (0,1).
    In practice, we obtain values between these two. For our MNIST digit classifier,
    we have a multiclass problem, but we can use the `plotROC()` function of the RSNNS
    package to study the performance of our classifier on individual digits.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完美的分类器将显示出从原点到点（0,1）的曲线，这对应于100%的真正率和0%的假正率。我们经常将**ROC曲线下的面积**（**ROC AUC**）作为一个性能指标。随机分类器下的面积仅为0.5，因为我们正在计算单位正方形上直线*y
    = x*下的面积。按照惯例，完美分类器下的面积为1，因为曲线通过点（0,1）。在实践中，我们获得介于这两个值之间的值。对于我们的MNIST数字分类器，我们有一个多类问题，但我们可以使用RSNNS包的`plotROC()`函数来研究我们的分类器在单个数字上的性能。
- en: 'The following plot shows the ROC curve for digit 1, which is almost perfect:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了数字1的ROC曲线，几乎完美：
- en: '![Receiver operating characteristic curves](img/00107.jpeg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![接收者操作特征曲线](img/00107.jpeg)'
- en: Radial basis function networks
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 径向基函数网络
- en: A radial basis function network-based upon the concept of function approximation
    - is a kind of artificial neural network that uses *radial basis functions* to
    define a node's output (given a set of inputs). The output of the network consists
    of a *linear combination* of radial basis functions of the inputs and neuron parameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 基于函数逼近概念的径向基函数网络是一种使用*径向基函数*来定义节点输出（给定一组输入）的人工神经网络。网络的输出由输入和神经元参数的径向基函数的线性组合组成。
- en: '**Radial basis function** (**RBF**) networks (also referred to as RBFNN for
    Radial Basis Function Neural Networks) will have three separate layers: an **input**
    layer, a **hidden** layer, and a linear **output** layer. The input layer will
    be a set of several nodes that transfer transition the input values to the second
    (or hidden) layer where activation patterns are applied. These patterns will be
    selected radial basis functions that best fit the application or objective. This
    transformation occurs in a non-linear fashion. The third layer (or output layer)
    provides the response of the network to the activation or RFB functions applied
    to the inputs. In an RFB network, the transformation from the hidden layer to
    the output layer is nonlinear.'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**径向基函数**（**RBF**）网络（也称为RBFNN，即径向基函数神经网络）将具有三个独立的层：一个**输入**层、一个**隐藏**层和一个线性**输出**层。输入层将是一组节点，它们将输入值传递到第二层（或隐藏层），在那里应用激活模式。这些模式将是最佳拟合应用或目标的径向基函数。这种转换以非线性方式进行。第三层（或输出层）提供网络对输入激活或RFB函数的响应。在RFB网络中，从隐藏层到输出层的转换是非线性的。'
- en: A radial basis function network is a neural network usually approached by viewing
    the design as a curve-fitting (guesstimate) problem in a high dimensional space.
    Learning is equivalent to finding a multidimensional function that provides a
    best fit to the training data, with the criterion for best fit being measured
    in some statistical sense.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 径向基函数网络是一种神经网络，通常将其设计视为高维空间中的曲线拟合（猜测）问题。学习等同于找到一个多维度函数，该函数为训练数据提供最佳拟合，最佳拟合的准则以某种统计意义来衡量。
- en: Generally, RBF networks seem to have the advantages of a more easily understood
    design, generalization ability, and a record of good tolerance to "noise" within
    the data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，RBF网络似乎具有易于理解的设计、泛化能力和良好的对数据“噪声”容忍度的记录。
- en: The properties of RBF networks make it a very good choice for designing control
    systems that are required to be very flexible in that they must continually evaluate
    the various *paths to completion* and determine the most efficient. The study
    most famous in using RBF networks is in solving the traveling salesman problem
    (finding the shortest closed path between a group of cities).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: RBF网络（径向基函数网络）的特性使其成为设计需要非常灵活的控制系统的理想选择，因为这些系统必须不断评估各种*完成路径*并确定最有效的路径。在RBF网络应用中最著名的研究是解决旅行商问题（在一系列城市之间找到最短闭合路径）。
- en: Summary
  id: totrans-259
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw neural networks as a nonlinear method capable of solving
    both regression and classification problems. Motivated by the biological analogy
    to human neurons, we first introduced the simplest neural network, the perceptron.
    This is able to solve binary classification problems only when the two classes
    are linearly separable, something that we very rarely rely upon in practice.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将神经网络视为一种非线性方法，能够解决回归和分类问题。受人类神经元生物类比启发，我们首先介绍了最简单的神经网络——感知器。只有当两个类别线性可分时，感知器才能解决二元分类问题，这在实际应用中是非常少见的。
- en: By changing the function that transforms the linear weighted combination of
    inputs, namely the activation function, we discovered how to create different
    types of individual neurons. A linear activation function creates a neuron that
    performs linear regression, whereas the logistic activation function creates a
    neuron that performs logistic regression. By organizing and connecting neurons
    into layers, we can create multilayer neural networks that are powerful models
    for solving nonlinear problems.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变转换输入线性加权组合的函数，即激活函数，我们发现如何创建不同类型的单个神经元。线性激活函数创建一个执行线性回归的神经元，而逻辑激活函数创建一个执行逻辑回归的神经元。通过组织和连接神经元到层中，我们可以创建多层神经网络，这些网络是解决非线性问题的强大模型。
- en: The idea behind having hidden layers of neurons is that each hidden layer learns
    a new set of features from its inputs. As the most common type of multilayer neural
    network, we introduced the multilayer perceptron and saw that it can naturally
    learn multiple outputs with the same network. In addition, we experimented on
    real-world datasets for both regression and classification tasks, including a
    multiclass classification problem that we saw is also handled naturally. R has
    a number of packages for implementing neural networks, including `neuralnet`,
    `nnet`, and `RSNNS`, and we experimented with each of these in turn. Each has
    its respective advantages and disadvantages and there isn't a clear winner for
    every circumstance.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层神经元背后的思想是，每个隐藏层都会从其输入中学习一组新的特征。作为最常见类型的多层神经网络，我们介绍了多层感知器，并看到它可以自然地使用相同的网络学习多个输出。此外，我们还对回归和分类任务的真实世界数据集进行了实验，包括一个多类分类问题，我们看到了它也是自然处理的。R语言有多个用于实现神经网络的包，包括`neuralnet`、`nnet`和`RSNNS`，我们逐一尝试了这些包。每个包都有其各自的优缺点，并没有一个在所有情况下都是明确的赢家。
- en: An important benefit of working with neural networks is that they can be very
    powerful in solving highly complex nonlinear problems of regression and classification
    alike without making any significant assumptions about the relationships between
    the input features. On the other hand, neural networks can often be quite tricky
    to train. Scaling input features is important. It is also important to be aware
    of the various parameters affecting the convergence of the model, such as the
    learning rate and the error gradient tolerance. Another crucial decision to make
    is the number and distribution of hidden layer neurons. As the complexity of the
    network, the number of input features, or the size of the training data increases,
    the training time often becomes quite long compared to other supervised learning
    methods.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络一起工作的一个重要好处是，它们在解决回归和分类的复杂非线性问题时非常强大，而不需要对输入特征之间的关系做出任何重大假设。另一方面，神经网络通常很难训练。缩放输入特征很重要。同时，了解影响模型收敛的各种参数也很重要，例如学习率和误差梯度容差。另一个需要做出的关键决定是隐藏层神经元的数量和分布。随着网络复杂度、输入特征数量或训练数据大小的增加，与其它监督学习方法相比，训练时间通常会变得相当长。
- en: We also saw in our regression example that because of the flexibility and power
    of neural networks, they can be prone to overfitting the data, thus overestimating
    the model's accuracy. Regularization approaches, such as weight decay, exist to
    mitigate this problem to a certain extent. Finally, one clear disadvantage that
    deserves mention is that the neural weights have no direct interpretation, unlike
    regression coefficients, and even though the neural network topology may learn
    features, these are difficult to explain or interpret.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的回归示例中，我们也看到，由于神经网络的灵活性和强大功能，它们可能会过度拟合数据，从而高估模型的准确性。为了在一定程度上缓解这个问题，存在正则化方法，例如权重衰减。最后，一个值得注意的明显缺点是，神经权重没有直接的解释，与回归系数不同。即使神经网络拓扑可能学习到特征，这些特征也难以解释或理解。
- en: Our next chapter continues our foray into the world of supervised learning and
    presents support vector machines, our third nonlinear modeling tool, which is
    primarily used for dealing with classification problems.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一章将继续探索监督学习的世界，并介绍支持向量机，这是我们第三个非线性建模工具，主要用于处理分类问题。
