- en: Chapter 9. Big Data Machine Learning – The Final Frontier
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 9 章. 大数据机器学习 – 最终边疆
- en: In recent years, we have seen an exponential growth in data generated by humans
    and machines. Varied sources, including home sensors, healthcare-related monitoring
    devices, news feeds, conversations on social media, images, and worldwide commerce
    transactions—an endless list—contribute to the vast volumes of data generated
    every day.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们见证了人类和机器生成数据的指数级增长。包括家庭传感器、与医疗相关的监测设备、新闻源、社交媒体上的对话、图像和全球商业交易在内的各种来源——这是一个无休止的列表——每天都会产生大量数据。
- en: 'Facebook had 1.28 billion daily active users in March 2017 sharing close to
    four million pieces of unstructured information as text, images, URLs, news, and
    videos (Source: Facebook). 1.3 billion Twitter users share approximately 500 million
    tweets a day (Source: Twitter). **Internet of Things** (**IoT**) sensors in lights,
    thermostats, sensor in cars, watches, smart devices, and so on, will grow from
    50 billion to 200 billion by 2020 (Source: IDC estimates). YouTube users upload
    300 hours of new video content every five minutes. Netflix has 30 million viewers
    who stream 77,000 hours of video daily. Amazon has sold approximately 480 million
    products and has approximately 244 million customers. In the financial sector,
    the volume of transactional data generated by even a single large institution
    is enormous—approximately 25 million households in the US have Bank of America,
    a major financial institution, as their primary bank, and together produce petabytes
    of data annually. Overall, it is estimated that the global Big Data industry will
    be worth 43 billion US dollars in 2017 (Source: [www.statista.com](http://www.statista.com)).'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 2017 年 3 月，Facebook 有 12.8 亿每日活跃用户分享了近四百万条非结构化信息，包括文本、图像、URL、新闻和视频（来源：Facebook）。1.3
    亿 Twitter 用户每天分享约 5 亿条推文（来源：Twitter）。**物联网**（**IoT**）中的传感器，如灯光、恒温器、汽车中的传感器、手表、智能设备等，到
    2020 年将从 500 亿增长到 2000 亿（来源：IDC 估计）。YouTube 用户每五分钟上传 300 小时的新视频内容。Netflix 有 3000
    万观众每天流式传输 77,000 小时的视频。亚马逊销售了约 4.8 亿件产品，拥有约 2.44 亿客户。在金融领域，即使是单一大型机构产生的交易数据量也非常巨大——美国约有
    2500 万户家庭将美国银行（一家主要金融机构）作为其主要银行，每年共同产生数百万兆字节的数据。总体而言，预计到 2017 年，全球大数据产业的价值将达到
    430 亿美元（来源：[www.statista.com](http://www.statista.com))。
- en: Each of the aforementioned companies and many more like them face the real problem
    of storing all this data (structured and unstructured), processing the data, and
    learning hidden patterns from the data to increase their revenue and to improve
    customer satisfaction. We will explore how current methods, tools and technology
    can help us learn from data in Big Data-scale environments and how as practitioners
    in the field we must recognize challenges unique to this problem space.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 上述公司以及许多类似的公司都面临着存储所有这些数据（结构化和非结构化）、处理数据以及从数据中学习隐藏模式以增加收入和提高客户满意度的真实问题。我们将探讨当前的方法、工具和技术如何帮助我们在大数据规模环境中从数据中学习，以及作为该领域从业者，我们必须认识到这一特定问题空间所特有的挑战。
- en: 'This chapter has the following structure:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结构如下：
- en: What are the characteristics of Big Data?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据的特征是什么？
- en: Big Data Machine Learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据机器学习
- en: 'General Big Data Framework:'
  id: totrans-7
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通用大数据框架：
- en: Big data cluster deployments frameworks
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大数据集群部署框架
- en: HortonWorks Data Platform (HDP)
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: HortonWorks 数据平台 (HDP)
- en: Cloudera CDH
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloudera CDH
- en: Amazon Elastic MapReduce (EMR)
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Elastic MapReduce (EMR)
- en: Microsoft HDInsight
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft HDInsight
- en: 'Data acquisition:'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采集：
- en: Publish-subscribe framework
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布-订阅框架
- en: Source-sink framework
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源-汇框架
- en: SQL framework
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SQL 框架
- en: Message queueing framework
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息队列框架
- en: Custom framework
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义框架
- en: 'Data storage:'
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据存储：
- en: Hadoop Distributed File System (HDFS)
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 分布式文件系统 (HDFS)
- en: NoSQL
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: NoSQL
- en: 'Data processing and preparation:'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据处理和准备：
- en: Hive and Hive Query Language (HQL)
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hive 和 Hive 查询语言 (HQL)
- en: Spark SQL
  id: totrans-24
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Amazon Redshift
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Amazon Redshift
- en: Real-time stream processing
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时流处理
- en: Machine Learning
  id: totrans-27
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习
- en: Visualization and analysis
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化和分析
- en: Batch Big Data Machine Learning
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批量大数据机器学习
- en: 'H2O:'
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O：
- en: H2O architecture
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O 架构
- en: Machine learning in H2O
  id: totrans-32
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: H2O 中的机器学习
- en: Tools and usage
  id: totrans-33
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和用法
- en: Case study
  id: totrans-34
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 案例研究
- en: Business problems
  id: totrans-35
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 商业问题
- en: Machine Learning mapping
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: Data collection
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集
- en: Data sampling and transformation
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据采样和转换
- en: Experiments, results, and analysis
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验、结果和分析
- en: 'Spark MLlib:'
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Spark MLlib:'
- en: Spark architecture
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 架构
- en: Machine Learning in MLlib
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: MLlib 中的机器学习
- en: Tools and usage
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和用法
- en: Experiments, results, and analysis
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验、结果和分析
- en: Real-time Big Data Machine Learning
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实时大数据机器学习
- en: 'Scalable Advanced Massive Online Analysis (SAMOA):'
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可扩展的高级大规模在线分析（SAMOA）：
- en: SAMOA architecture
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: SAMOA架构
- en: Machine Learning algorithms
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: Tools and usage
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工具和用法
- en: Experiments, results, and analysis
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实验、结果和分析
- en: The future of Machine Learning
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的未来
- en: What are the characteristics of Big Data?
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据的特征有哪些？
- en: 'There are many characteristics of Big Data that are different than normal data.
    Here we highlight them as four *V*s that characterize Big Data. Each of these
    makes it necessary to use specialized tools, frameworks, and algorithms for data
    acquisition, storage, processing, and analytics:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据有许多与普通数据不同的特征。在这里，我们将它们突出为四个“V”，以表征大数据。每个“V”都使得使用专门的工具、框架和算法进行数据采集、存储、处理和分析成为必要。
- en: '**Volume**: One of the characteristic of Big Data is the size of the content,
    structured or unstructured, which doesn''t fit the storage capacity or processing
    power available on a single machine and therefore needs multiple machines.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体积**：大数据的一个特征是内容的大小，无论是结构化还是非结构化，都不适合单台机器的存储容量或处理能力，因此需要多台机器。'
- en: '**Velocity**: Another characteristic of Big Data is the rate at which the content
    is generated, which contributes to volume but needs to be handled in a time sensitive
    manner. Social media content and IoT sensor information are the best examples
    of high velocity Big Data.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**速度**：大数据的另一个特征是内容生成的速率，这有助于增加数据量，但需要以时间敏感的方式处理。社交媒体内容和物联网传感器信息是高速大数据的最佳例子。'
- en: '**Variety**: This generally refers to multiple formats in which data exists,
    that is, structured, semi-structured, and unstructured and furthermore, each of
    them has different forms. Social media content with images, video, audio, text,
    and structured information about activities, background, networks, and so on,
    is the best example of where data from various sources must be analyzed.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多样性**：这通常指的是数据存在的多种格式，即结构化、半结构化和非结构化，而且每种格式都有不同的形式。包含图像、视频、音频、文本以及关于活动、背景、网络等结构化信息的社交媒体内容是必须分析来自各种来源的数据的最佳例子。'
- en: '**Veracity**: This refers to a wide variety of factors such as noise, uncertainty,
    biases, and abnormality in the data that must be addressed, especially given the
    volume, velocity, and variety of data. One of the key steps, as we will discuss
    in the context of Big Data Machine Learning, is processing and cleaning such "unclean"
    data.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真实性**：这涉及到数据中的各种因素，如噪声、不确定性、偏差和异常，必须加以解决，尤其是在数据量、速度和多样性的情况下。我们将讨论的一个关键步骤是处理和清理这些“不干净”的数据，正如我们将在大数据机器学习背景下讨论的那样。'
- en: Many have added other characteristics such as value, validity, and volatility
    to the preceding list, but we believe they are largely derived from the previous
    four.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人已经将价值、有效性和波动性等特征添加到前面的列表中，但我们认为它们在很大程度上是从前四个特征派生出来的。
- en: Big Data Machine Learning
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 大数据机器学习
- en: In this section, we will discuss the general flow and components that are required
    for Big Data Machine Learning. Although many of the components, such as data acquisition
    or storage, are not directly related to Machine Learning methodologies, they inevitably
    have an impact on the frameworks and processes. Giving a complete catalog of the
    available components and tools is beyond the scope of this book, but we will discuss
    the general responsibilities of the tasks involved and give some information on
    the techniques and tools available to accomplish them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论大数据机器学习所需的一般流程和组件。尽管许多组件，如数据采集或存储，与机器学习方法没有直接关系，但它们不可避免地对框架和流程产生影响。提供所有可用组件和工具的完整目录超出了本书的范围，但我们将讨论涉及任务的一般职责，并介绍一些可用于完成这些任务的技术和工具。
- en: General Big Data framework
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通用大数据框架
- en: 'The general Big Data framework is illustrated in the following figure:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示为通用大数据框架：
- en: '![General Big Data framework](img/B05137_09_001.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![通用大数据框架](img/B05137_09_001.jpg)'
- en: 'Figure 1: Big data framework'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：大数据框架
- en: The choice of how the Big Data framework is set up and deployed in the cluster
    is one of the decisions that affects the choice of tools, techniques, and cost.
    The data acquisition or collection component is the first step and it consists
    of several techniques, both synchronous and asynchronous, to absorb data into
    the system. Various techniques ranging from publish-subscribe, source-sink, relational
    database queries, and custom data connectors are available in the components.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中如何设置和部署大数据框架的选择是影响工具、技术和成本决策的因素之一。数据采集或收集组件是第一步，它包括多种同步和异步技术，用于将数据吸收到系统中。组件中提供了从发布-订阅、源-汇、关系型数据库查询和自定义数据连接器等各种技术。
- en: Data storage choices ranging from distributed filesystems such as HDFS to non-relational
    databases (NoSQL) are available based on various other functional requirements.
    NoSQL databases are described in the section on *Data Storage*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 根据各种其他功能需求，数据存储选择包括从分布式文件系统如HDFS到非关系型数据库（NoSQL）。NoSQL数据库在*数据存储*部分进行了描述。
- en: 'Data preparation, or transforming the large volume of stored data so that it
    is consumable by the Machine Learning analytics, is an important processing step.
    This has some dependencies on the frameworks, techniques, and tools used in storage.
    It also has some dependency on the next step: the choice of Machine Learning analytics/frameworks
    that will be used. There are a wide range of choices for processing frameworks
    that will be discussed in the following sub-section.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备，即转换大量存储的数据，使其能够被机器学习分析所消费，是一个重要的处理步骤。这依赖于存储中使用的框架、技术和工具。它还依赖于下一步：选择将要使用的机器学习分析/框架。以下子节将讨论广泛的选择处理框架。
- en: Recall that, in batch learning, the model is trained simultaneously on a number
    of examples that have been previously collected. In contrast to batch learning,
    in real-time learning model training is continuous, each new instance that arrives
    becoming part of a dynamic training set. See [Chapter 5](ch05.html "Chapter 5. Real-Time
    Stream Machine Learning"), *Real-Time Stream Machine Learning* for details. Once
    the data is collected, stored, and transformed based on the domain requirements,
    different Machine Learning methodologies can be employed, including batch learning,
    real-time learning, and batch-real-time mixed learning. Whether one selects supervised
    learning, unsupervised learning, or a combination of the two also depends on the
    data, the availability of labels, and label quality. These will be discussed in
    detail later in this chapter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在批量学习中，模型是在之前收集的多个示例上同时训练的。与批量学习相反，实时学习模型训练是连续的，每个到达的新实例都成为动态训练集的一部分。有关详细信息，请参阅[第5章](ch05.html
    "第5章。实时流机器学习")，*实时流机器学习*。一旦数据根据领域要求收集、存储和转换，就可以采用不同的机器学习方法，包括批量学习、实时学习和批量-实时混合学习。选择监督学习、无监督学习或两者的组合也取决于数据、标签的可用性和标签质量。这些将在本章后面详细讨论。
- en: The results of analytics during the development stage as well as the production
    or runtime stage also need to be stored and visualized for humans and automated
    tasks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 开发阶段以及生产或运行时阶段的分析结果也需要存储和可视化，以便于人类和自动化任务。
- en: Big Data cluster deployment frameworks
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 大数据集群部署框架
- en: There are many frameworks that are built on the core Hadoop (*References* [3])
    open source platform. Each of them provides a number of tools for the Big Data
    components described previously.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 基于核心Hadoop (*参考资料* [3]) 开源平台构建了许多框架。每个框架都为之前描述的大数据组件提供了一系列工具。
- en: Hortonworks Data Platform
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hortonworks 数据平台
- en: '**Hortonworks Data Platform** (**HDP**) provides an open source distribution
    comprising various components in its stack, from data acquisition to visualization.
    Apache Ambari is often the user interface used for managing services and provisioning
    and monitoring clusters. The following screenshot depicts Ambari used for configuring
    various services and the health-check dashboard:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hortonworks 数据平台** (**HDP**) 提供了一个开源分布，包括其堆栈中的各种组件，从数据采集到可视化。Apache Ambari通常是用于管理服务和提供集群配置和监控的用户界面。以下截图展示了用于配置各种服务和健康检查仪表板的Ambari：'
- en: '![Hortonworks Data Platform](img/B05137_09_002.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![Hortonworks 数据平台](img/B05137_09_002.jpg)'
- en: 'Figure 2: Ambari dashboard user interface'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：Ambari仪表板用户界面
- en: Cloudera CDH
  id: totrans-76
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Cloudera CDH
- en: 'Like HDP, Cloudera CDH (*References* [4]) provides similar services and Cloudera
    Services Manager can be used in a similar way to Ambari for cluster management
    and health checks, as shown in the following screenshot:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 与HDP类似，Cloudera CDH (*参考文献[4]*) 提供了类似的服务，Cloudera服务管理器可以像Ambari一样用于集群管理和健康检查，如下面的截图所示：
- en: '![Cloudera CDH](img/B05137_09_003.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![Cloudera CDH](img/B05137_09_003.jpg)'
- en: 'Figure 3: Cloudera Service Manager user interface'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：Cloudera服务管理器用户界面
- en: Amazon Elastic MapReduce
  id: totrans-80
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Amazon Elastic MapReduce
- en: 'Amazon Elastic MapReduce (EMR) (*References* [5]) is another Big Data cluster,
    platform similar to HDP and Cloudera, which supports a wide variety of frameworks.
    EMR has two modes—**cluster mode** and **step execution mode**. In cluster mode,
    you choose the Big Data stack vendor EMR or MapR and in step execution mode, you
    give jobs ranging from JARs to SQL queries for execution. In the following screenshot,
    we see the interface for configuring a new cluster as well as defining a new job
    flow:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Elastic MapReduce (EMR) (*参考文献[5]*) 是另一个类似于HDP和Cloudera的大数据集群平台，它支持广泛的框架。EMR有两种模式——**集群模式**和**步骤执行模式**。在集群模式下，您可以选择EMR或MapR的大数据堆栈供应商，而在步骤执行模式下，您可以为执行提供从JAR文件到SQL查询的各种作业。以下截图显示了配置新集群以及定义新作业流的界面：
- en: '![Amazon Elastic MapReduce](img/B05137_09_004.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![Amazon Elastic MapReduce](img/B05137_09_004.jpg)'
- en: 'Figure 4: Amazon Elastic MapReduce cluster management user interface'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4：Amazon Elastic MapReduce集群管理用户界面
- en: Microsoft Azure HDInsight
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Microsoft Azure HDInsight
- en: 'Microsoft Azure HDInsight (*References* [6]) is another platform that allows
    cluster management with most of the services that are required, including storage,
    processing, and Machine Learning. The Azure portal, as shown in the following
    screenshot, is used to create, manage, and help in learning the statuses of the
    various components of the cluster:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Microsoft Azure HDInsight (*参考文献[6]*) 是另一个平台，它允许使用包括存储、处理和机器学习在内的大多数服务进行集群管理。如下面的截图所示，Azure门户用于创建、管理和帮助学习集群各个组件的状态：
- en: '![Microsoft Azure HDInsight](img/B05137_09_005.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![Microsoft Azure HDInsight](img/B05137_09_005.jpg)'
- en: 'Figure 5: Microsoft Azure HDInsight cluster management user interface'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图5：Microsoft Azure HDInsight集群管理用户界面
- en: Data acquisition
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据采集
- en: In the Big Data framework, the acquisition component plays an important role
    in collecting the data from disparate source systems and storing it in Big Data
    storage. Based on types of source and volume, velocity, functional, and performance-based
    requirements, there are a wide variety of acquisition frameworks and tools. We
    will describe a few of the most well-known frameworks and tools used to give readers
    some insight.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据框架中，采集组件在从不同的源系统收集数据并将其存储在大数据存储中方面发挥着重要作用。根据源类型、数量、速度、功能以及性能要求，存在各种采集框架和工具。我们将描述一些最知名框架和工具，以给读者提供一些洞察。
- en: Publish-subscribe frameworks
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 发布-订阅框架
- en: In publish-subscribe based frameworks, the publishing source pushes the data
    in different formats to the broker, which has different subscribers waiting to
    consume them. The publisher and subscriber are unaware of each other, with the
    broker mediating in between.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于发布-订阅的框架中，发布源将数据以不同格式推送到代理，该代理有不同的订阅者等待消费。发布者和订阅者彼此之间不知情，由代理在中间调解。
- en: '**Apache Kafka** (*References* [9]) and **Amazon Kinesis** are two well-known
    implementations that are based on this model. Apache Kafka defines the concepts
    of publishers, consumers, and topics—on which things get published and consumed—and
    a broker to manage the topics. Amazon Kinesis is built on similar concepts with
    producers and consumers connected through Kinesis streams, which are similar to
    topics in Kafka.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '**Apache Kafka** (*参考文献[9]*) 和 **Amazon Kinesis** 是基于此模型的两个知名实现。Apache Kafka定义了发布者、消费者和主题的概念——事物在此发布和消费，以及一个用于管理主题的代理。Amazon
    Kinesis基于类似的概念，通过Kinesis流连接生产者和消费者，这些流类似于Kafka中的主题。'
- en: Source-sink frameworks
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 源-汇框架
- en: In source-sink models, sources push the data into the framework and the framework
    pushes the system to the sinks. Apache Flume (*References* [7]) is a well-known
    implementation of this kind of framework with a variety of sources, channels to
    buffer the data, and a number of sinks to store the data in the Big Data world.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在源-汇模型中，源将数据推入框架，框架将系统推送到汇。Apache Flume (*参考文献[7]*) 是此类框架的一个知名实现，具有各种源、用于缓冲数据的通道以及在大数据世界中存储数据的多个汇。
- en: SQL frameworks
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SQL框架
- en: Since many traditional data stores are in the form of SQL-based RDBMS, SQL-based
    frameworks provide a generic way to import the data from RDBMS and store it in
    Big Data, mainly in the HDFS format. Apache Sqoop (*References* [10]) is a well-known
    implementation that can import data from any JDBC-based RDBMS and store it in
    HDFS-based systems.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多传统数据存储以基于SQL的关系型数据库管理系统（RDBMS）的形式存在，基于SQL的框架提供了一种通用的方式来从RDBMS导入数据并将其存储在大数据中，主要是HDFS格式。Apache
    Sqoop（*参考文献* [10]）是一个知名的实现，可以从任何基于JDBC的RDBMS导入数据并将其存储在基于HDFS的系统。
- en: Message queueing frameworks
  id: totrans-97
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 消息队列框架
- en: Message queueing frameworks are push-pull based frameworks similar to publisher-subscriber
    systems. Message queues separate the producers and consumers and can store the
    data in the queue, in an asynchronous communication pattern. Many protocols have
    been developed on this such as Advanced Message Queueing Protocol (AMQP) and ZeroMQ
    Message Transfer Protocol (ZMTP). RabbitMQ, ZeroMQ, Amazon SQS, and so on, are
    some well-known implementations of this framework.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 消息队列框架是基于推送-拉取的框架，类似于发布-订阅系统。消息队列将生产者和消费者分开，并可以在队列中存储数据，采用异步通信模式。已经开发了许多协议，例如高级消息队列协议（AMQP）和ZeroMQ消息传输协议（ZMTP）。RabbitMQ、ZeroMQ、Amazon
    SQS等是一些此框架的知名实现。
- en: Custom frameworks
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自定义框架
- en: Specialized connectors for different sources such as IoT, HTTP, WebSockets,
    and so on, have resulted in many specific connectors such as Amazon IoT Hub, REST-connectors,
    WebSocket, and so on.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 针对不同来源（如IoT、HTTP、WebSockets等）的专用连接器导致了许多特定连接器的出现，例如Amazon IoT Hub、REST连接器、WebSocket等。
- en: Data storage
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据存储
- en: The data storage component plays a key part in connecting the acquisition and
    the rest of the components together. Performance, impact on data processing, cost,
    high-availability, ease of management, and so on, should be taken into consideration
    while deciding on data storage. For pure real-time or near real-time systems there
    are in-memory based frameworks for storage, but for batch-based systems there
    are mainly distributed File Systems such as HDFS or NoSQL.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储组件在连接获取和其他组件方面发挥着关键作用。在决定数据存储时，应考虑性能、对数据处理的影响、成本、高可用性、易于管理等因素。对于纯实时或近实时系统，有基于内存的存储框架，但对于基于批次的系统，主要有分布式文件系统，如HDFS或NoSQL。
- en: HDFS
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: HDFS
- en: HDFS can run on a large cluster of nodes and provide all the important features
    such as high-throughput, replications, fail-over, and so on.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS可以在大型节点集群上运行，并提供所有重要功能，如高吞吐量、复制、故障转移等。
- en: '![HDFS](img/B05137_09_006.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![HDFS](img/B05137_09_006.jpg)'
- en: 'The basic architecture of HDFS has the following components:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS的基本架构包含以下组件：
- en: '**NameNode**: The HDFS client always sends the request to the NameNode, which
    keeps the metadata of the file while the real data is distributed in blocks on
    the DataNodes. NameNodes are only responsible for handling opening and closing
    a file while the remaining interactions of reading, writing, and appending happen
    between clients and the data nodes. The NameNode stores the metadata in two files:
    `fsimage` and `edit` files. The `fsimage` contains the filesystem metadata as
    a snapshot, while edit files contain the incremental changes to the metadata.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**NameNode**：HDFS客户端始终将请求发送到NameNode，它保存文件的元数据，而实际数据以块的形式分布在DataNode上。NameNode只负责处理文件的打开和关闭，而读取、写入和追加的其余交互发生在客户端和数据节点之间。NameNode将元数据存储在两个文件中：`fsimage`和`edit`文件。`fsimage`包含文件系统元数据作为快照，而edit文件包含对元数据的增量更改。'
- en: '**Secondary NameNode**: Secondary NameNode provides redundancy to the metadata
    in the NameNode by keeping a copy of the `fsimage` and `edit` files at every predefined
    checkpoint.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Secondary NameNode**：Secondary NameNode通过在每个预定义检查点保留`fsimage`和`edit`文件的副本，为NameNode中的元数据提供冗余。'
- en: '**DataNode**: DataNodes manage the actual blocks of data and facilitate read-write
    operation on these datablocks. DataNodes keep communicating with the NameNodes
    using heartbeat signals indicating they are alive. The data blocks stored in DataNodes
    are also replicated for redundancy. Replication of the data blocks in the DataNodes
    is governed by the rack-aware placement policy.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DataNode**：DataNode管理实际的数据块并促进对这些数据块的读写操作。DataNode通过心跳信号与NameNode保持通信，以表明它们处于活动状态。存储在DataNode中的数据块也进行了冗余复制。DataNode中的数据块复制受制于机架感知放置策略。'
- en: NoSQL
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NoSQL
- en: Non-relational databases, also referred to as NoSQL databases, are gaining enormous
    popularity in the Big Data world. High throughput, better horizontal scaling,
    improved performance on retrieval, and storage at the cost of weaker consistency
    models are notable characteristics of most NoSQL databases. We will discuss some
    important forms of NoSQL database in this section along with implementations.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 非关系型数据库，也称为NoSQL数据库，在大数据世界中越来越受欢迎。高吞吐量、更好的水平扩展、检索性能的提高以及以牺牲较弱的一致性模型为代价的存储是大多数NoSQL数据库的显著特征。在本节中，我们将讨论一些重要的NoSQL数据库形式及其实现。
- en: Key-value databases
  id: totrans-112
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 键值数据库
- en: Key-value databases are the most prominent NoSQL databases used mostly for semi-structured
    or unstructured data. As the name suggests, the structure of storage is quite
    basic, with unique keys associating the data values that can be of any type including
    string, integer, double precision, and so on—even BLOBS. Hashing the keys for
    quick lookup and retrieval of the values together with partitioning the data across
    multiple nodes gives high throughput and scalability. The query capabilities are
    very limited. Amazon DynamoDB, Oracle NoSQL, MemcacheDB, and so on, are examples
    of key-value databases.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 键值数据库是最突出的NoSQL数据库，主要用于半结构化或非结构化数据。正如其名所示，存储结构相当基本，具有独特的键将数据值（可以是字符串、整数、双精度等类型，甚至BLOBS）与之关联。对键进行哈希处理以快速查找和检索值，以及将数据跨多个节点分区，提供了高吞吐量和可伸缩性。查询能力非常有限。Amazon
    DynamoDB、Oracle NoSQL、MemcacheDB等是一些键值数据库的例子。
- en: Document databases
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文档数据库
- en: Document databases store semi-structured data in the form of XML, JSON, or YAML
    documents, to name some of the most popular formats. The documents have unique
    keys to which they are mapped. Though it is possible to store documents in key-value
    stores, the query capabilities offered by document stores are greater as the primitives
    making up the structure of the document—which may include names or attributes—can
    also be used for retrieval. When the data is ever-changing and has variable numbers
    or lengths of fields, document databases are often a good choice. Document databases
    do not offer join capabilities and hence all information needs to be captured
    in the document values. MongoDB, ElasticSearch, Apache Solr, and so on, are some
    well-known implementations of document databases.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 文档数据库以XML、JSON或YAML文档的形式存储半结构化数据，以下是一些最流行的格式。文档具有独特的键，它们被映射到这些键上。尽管在键值存储中存储文档是可能的，但文档存储提供的查询能力更强，因为构成文档结构的原语（可能包括名称或属性）也可以用于检索。当数据不断变化且字段数量或长度可变时，文档数据库通常是一个不错的选择。文档数据库不提供连接能力，因此所有信息都需要在文档值中捕获。MongoDB、ElasticSearch、Apache
    Solr等是一些著名的文档数据库实现。
- en: Columnar databases
  id: totrans-116
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列式数据库
- en: The use of columns as the basic unit of storage with name, value, and often
    timestamp, differentiates columnar databases from traditional relational databases.
    Columns are further combined to form column families. A row is indexed by the
    row key and has multiple column families associated with the row. Certain rows
    can use only column families that are populated, giving it a good storage representation
    in sparse data. Columnar databases do not have fixed schema-like relational databases;
    new columns and families can be added at any time, giving them a significant advantage.
    **HBase**, **Cassandra**, and **Parquet** are some well-known implementations
    of columnar databases.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列作为存储的基本单元，具有名称、值和通常的时间戳，这区分了列式数据库与传统的关系数据库。列进一步组合形成列族。行通过行键索引，并关联多个列族。某些行可能只能使用已填充的列族，这使其在稀疏数据中具有很好的存储表示。列式数据库没有像关系数据库那样的固定模式；新列和列族可以随时添加，这给了它们显著的优势。**HBase**、**Cassandra**和**Parquet**是一些著名的列式数据库实现。
- en: Graph databases
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图数据库
- en: In many applications, the data has an inherent graph structure with nodes and
    links. Storing such data in graph databases makes it more efficient for storage,
    retrieval, and queries. The nodes have a set of attributes and generally represent
    entities, while links represent relationships between the nodes that can be directed
    or undirected. **Neo4J**, **OrientDB**, and **ArangoDB** are some well-known implementations
    of graph databases.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，数据具有固有的图结构，包括节点和链接。在图数据库中存储此类数据使其在存储、检索和查询方面更加高效。节点有一组属性，通常代表实体，而链接表示节点之间的关系，可以是定向的或非定向的。**Neo4J**、**OrientDB**和**ArangoDB**是一些著名的图数据库实现。
- en: Data processing and preparation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据处理和准备
- en: 'The data preparation step involves various preprocessing steps before the data
    is ready to be consumed by analytics and machine learning algorithms. Some of
    the key tasks involved are:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备步骤涉及在数据准备好由分析和机器学习算法消费之前的各种预处理步骤。涉及的一些关键任务包括：
- en: '**Data cleansing**: Involves everything from correcting errors, type matching,
    normalization of elements, and so on, on the raw data.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据清洗**：涉及对原始数据进行错误纠正、类型匹配、元素归一化等所有内容。'
- en: '**Data scraping and curating**: Converting data elements and normalizing the
    data from one structure to another.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据抓取和整理**：将数据元素从一种结构转换为另一种结构并进行归一化。'
- en: '**Data transformation**: Many analytical algorithms need features that are
    aggregates built on raw or historical data. Transforming and computing those extra
    features are done in this step.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：许多分析算法需要基于原始或历史数据的聚合特征。在这个步骤中，会进行这些额外特征的转换和计算。'
- en: Hive and HQL
  id: totrans-125
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Hive 和 HQL
- en: Apache Hive (*References* [11]) is a powerful tool for performing various data
    preparation activities in HDFS systems. Hive organizes the underlying HDFS data
    a of structure that is similar to relational databases. HQL is like SQL and helps
    in performing various aggregates, transformations, cleanup, and normalization,
    and the data is then serialized back to HDFS. The logical tables in Hive are partitioned
    across and sub-divided into buckets for speed-up. Complex joins and aggregate
    queries in Hive are automatically converted into MapReduce jobs for throughput
    and speed-up.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hive (*参考文献[11]*) 是在 HDFS 系统中执行各种数据准备活动的强大工具。Hive 将底层 HDFS 数据组织成类似于关系数据库的结构。HQL
    类似于 SQL，有助于执行各种聚合、转换、清理和归一化，然后数据被序列化回 HDFS。Hive 中的逻辑表被分区并在子分区中划分以提高速度。Hive 中的复杂连接和聚合查询会自动转换为
    MapReduce 作业以实现吞吐量和速度提升。
- en: Spark SQL
  id: totrans-127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Spark SQL
- en: Spark SQL, which is a major component of Apache Spark (*References* [1] and
    [2]), provides SQL-like functionality—similar to what HQL provides—for performing
    changes to the Big Data. Spark SQL can work with underlying data storage systems
    such as Hive or NoSQL databases such as Parquet. We will touch upon some aspects
    of Spark SQL in the section on Spark later.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL，Apache Spark 的主要组件 (*参考文献[1]和[2]*)，提供了类似 SQL 的功能——类似于 HQL 提供的功能——用于对大数据进行更改。Spark
    SQL 可以与底层数据存储系统（如 Hive 或 NoSQL 数据库如 Parquet）一起工作。我们将在 Spark 部分的章节中涉及 Spark SQL
    的某些方面。
- en: Amazon Redshift
  id: totrans-129
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Amazon Redshift
- en: Amazon Redshift provides several warehousing capabilities especially on Amazon
    EMR setups. It can process petabytes of data using its **massively parallel processing**
    (**MPP**) data warehouse architecture.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Redshift 在 Amazon EMR 设置上提供了一些仓库功能。它可以使用其 **大规模并行处理** (*MPP*) 数据仓库架构处理PB级的数据。
- en: Real-time stream processing
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实时流处理
- en: In many Big Data deployments, processing and performing the transformations
    specified previously must be done on the stream of data in real time rather than
    from stored batch data. There are various **Stream Processing Engines** (**SPE**)
    such as Apache Storm (*References* [12]) and Apache Samza, and in-memory processing
    engines such as Spark-Streaming that are used for stream processing.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多大数据部署中，必须对之前指定的转换在实时数据流上而不是从存储的批量数据中实时进行处理。有各种 **流处理引擎** (*SPE*)，如 Apache
    Storm (*参考文献[12]*) 和 Apache Samza，以及内存处理引擎如 Spark-Streaming，它们用于流处理。
- en: Machine Learning
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习
- en: 'Machine learning helps to perform descriptive, predictive, and prescriptive
    analysis on Big Data. There are two broad extremes that will be covered in this
    chapter:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习有助于对大数据进行描述性、预测性和规范性分析。本章将涵盖两个广泛的极端：
- en: Machine learning can be done on batch historical data and then the learning/models
    can be applied to new batch/real-time data
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习可以在批量历史数据上执行，然后可以将学习/模型应用于新的批量/实时数据
- en: Machine learning can be done on real-time data and applied simultaneously to
    the real-time data
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习可以在实时数据上执行，并同时应用于实时数据
- en: Both topics are covered at length in the remainder of this chapter.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的剩余部分将详细讨论这两个主题。
- en: Visualization and analysis
  id: totrans-138
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化和分析
- en: With batch learning done at modeling time and real-time learning done at runtime,
    predictions—the output of applying the models to new data—must be stored in some
    data structure and then analyzed by the users. Visualization tools and other reporting
    tools are frequently used to extract and present information to the users. Based
    on the domain and the requirements of the users, the analysis and visualization
    can be static, dynamic, or interactive.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模时间完成批量学习，在运行时完成实时学习，预测——将模型应用于新数据的输出——必须存储在某种数据结构中，然后由用户进行分析。可视化工具和其他报告工具经常被用来提取和向用户展示信息。根据领域和用户的需求，分析和可视化可以是静态的、动态的或交互式的。
- en: Lightning is a framework for performing interactive visualizations on the Web
    with different binding APIs using REST for Python, R, Scala, and JavaScript languages.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Lightning是一个框架，使用不同的绑定API（通过REST为Python、R、Scala和JavaScript语言）在Web上执行交互式可视化。
- en: Pygal and Seaborn are Python-based libraries that help in plotting all possible
    charts and graphs in Python for analysis, reporting, and visualizations.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Pygal和Seaborn是基于Python的库，它们帮助在Python中绘制所有可能的图表和图形，用于分析、报告和可视化。
- en: Batch Big Data Machine Learning
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 批量大数据机器学习
- en: 'Batch Big Data Machine Learning involves two basic steps, as discussed in [Chapter
    2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised Learning"),
    *Practical Approach to Real-World Supervised Learning*, [Chapter 3](ch03.html
    "Chapter 3. Unsupervised Machine Learning Techniques"), *Unsupervised Machine
    Learning Techniques*, and [Chapter 4](ch04.html "Chapter 4. Semi-Supervised and
    Active Learning"), *Semi-Supervised and Active Learning*: learning or training
    data from historical datasets and applying the learned models to unseen future
    data. The following figure demonstrates the two environments along with the component
    tasks and some technologies/frameworks that accomplish them:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 批量大数据机器学习涉及两个基本步骤，如[第2章](ch02.html "第2章. 实际应用中的监督学习")《实际应用中的监督学习》、[第3章](ch03.html
    "第3章. 无监督机器学习技术")《无监督机器学习技术》和[第4章](ch04.html "第4章. 半监督和主动学习")《半监督和主动学习》中所述，即从历史数据集中学习或训练数据，并将学习到的模型应用于未见过的未来数据。以下图展示了这两个环境以及完成这些任务的组件任务和一些技术/框架：
- en: '![Batch Big Data Machine Learning](img/B05137_09_007.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![批量大数据机器学习](img/B05137_09_007.jpg)'
- en: 'Figure 6: Model time and run time components for Big Data and providers'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：大数据和提供者的建模时间和运行时组件
- en: We will discuss two of the most well-known frameworks for doing Machine Learning
    in the context of batch data and will use the case study to highlight either the
    code or tools to perform modeling.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论在批量数据背景下进行机器学习的两个最著名的框架，并使用案例研究来突出执行建模的代码或工具。
- en: H2O as Big Data Machine Learning platform
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: H2O作为大数据机器学习平台
- en: H2O (*References* [13]) is a leading open source platform for Machine Learning
    at Big Data scale, with a focus on bringing AI to the enterprise. The company
    was founded in 2011 and counts several leading lights in statistical learning
    theory and optimization among its scientific advisors. It supports programming
    environments in multiple languages. While the H2O software is freely available,
    customer service and custom extensions to the product can be purchased.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: H2O（*参考文献[13]*）是一个领先的开源大数据机器学习平台，专注于将人工智能引入企业。该公司成立于2011年，拥有几位在统计学习理论和优化领域的杰出科学家作为其科学顾问。它支持多种编程环境。虽然H2O软件是免费提供的，但客户服务和产品的定制扩展可以购买。
- en: H2O architecture
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H2O架构
- en: The following figure gives a high-level architecture of H2O with important components.
    H2O can access data from various data stores such as HDFS, SQL, NoSQL, and Amazon
    S3, to name a few. The most popular deployment of H2O is to use one of the deployment
    stacks discussed earlier with Spark or to run it in a H2O cluster itself.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了H2O的高级架构及其重要组件。H2O可以访问来自各种数据存储的数据，例如HDFS、SQL、NoSQL和Amazon S3等。H2O最流行的部署方式是使用之前讨论过的部署堆栈之一与Spark一起使用，或者在其自己的H2O集群中运行。
- en: The core of H2O is an optimized way of handling Big Data in memory, so that
    iterative algorithms that go through the same data can be handled efficiently
    and achieve good performance. Important Machine Learning algorithms in supervised
    and unsupervised learning are implemented specially to handle horizontal scalability
    across multiple nodes and JVMs. H2O provides not only its own user interface,
    known as flow, to manage and run modeling tasks, but also has different language
    bindings and connector APIs to Java, R, Python, and Scala.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: H2O的核心是在内存中处理大数据的优化方式，以便可以有效地处理通过相同数据的迭代算法，并实现良好的性能。在监督学习和无监督学习中，重要的机器学习算法被特别实现以处理跨多个节点和JVM的水平可伸缩性。H2O不仅提供了自己的用户界面，称为flow，用于管理和运行建模任务，而且还具有不同的语言绑定和连接器API，用于Java、R、Python和Scala。
- en: '![H2O architecture](img/B05137_09_008.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![H2O架构](img/B05137_09_008.jpg)'
- en: 'Figure 7: H2O high level architecture'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：H2O高级架构
- en: 'Most Machine Learning algorithms, optimization algorithms, and utilities use
    the concept of fork-join or MapReduce. As shown in *Figure 8*, the entire dataset
    is considered as a **Data Frame** in H2O, and comprises vectors, which are features
    or columns in the dataset. The rows or instances are made up of one element from
    each Vector arranged side-by-side. The rows are grouped together to form a processing
    unit known as a **Chunk**. Several chunks are combined in one JVM. Any algorithmic
    or optimization work begins by sending the information from the topmost JVM to
    fork on to the next JVM, then on to the next, and so on, similar to the map operation
    in MapReduce. Each JVM works on the rows in the chunks to establish the task and
    finally the results flow back in the reduce operation:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习算法、优化算法和实用工具都使用了分叉-合并或MapReduce的概念。如图8所示，整个数据集在H2O中被视为一个**数据框**，并包含向量，这些向量是数据集中的特征或列。行或实例由每个向量中的一个元素并排排列组成。行被分组在一起形成一个称为**块**的处理单元。多个块在一个JVM中组合。任何算法或优化工作都是从最顶层的JVM发送信息到下一个JVM进行分叉，然后继续到下一个，以此类推，类似于MapReduce中的map操作。每个JVM在块中的行上执行任务，并最终在reduce操作中将结果流回：
- en: '![H2O architecture](img/B05137_09_009.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![H2O架构](img/B05137_09_009.jpg)'
- en: 'Figure 8: H2O distributed data processing using chunking'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：使用分块进行H2O分布式数据处理
- en: Machine learning in H2O
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: H2O中的机器学习
- en: 'The following figure shows all the Machine Learning algorithms supported in
    H2O v3 for supervised and unsupervised learning:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了H2O v3支持的所有监督学习和无监督学习的机器学习算法：
- en: '![Machine learning in H2O](img/B05137_09_010.jpg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![H2O中的机器学习](img/B05137_09_010.jpg)'
- en: 'Figure 9: H2O v3 Machine learning algorithms'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：H2O v3机器学习算法
- en: Tools and usage
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具和用法
- en: H2O Flow is an interactive web application that helps data scientists to perform
    various tasks from importing data to running complex models using point and click
    and wizard-based concepts.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: H2O Flow是一个交互式Web应用程序，帮助数据科学家执行从导入数据到使用点击和向导概念运行复杂模型的各种任务。
- en: 'H2O is run in local mode as:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: H2O以本地模式运行如下：
- en: '[PRE0]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The default way to start Flow is to point your browser and go to the following
    URL: `http://192.168.1.7:54321/`. The right-side of Flow captures every user action
    performed under the tab **OUTLINE**. The actions taken can be edited and saved
    as named flows for reuse and collaboration, as shown in *Figure 10*:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 启动Flow的默认方式是将浏览器指向以下URL：`http://192.168.1.7:54321/`。Flow的右侧捕获在**概要**标签下执行的每个用户操作。这些操作可以编辑并保存为命名的流程以供重用和协作，如图10所示：
- en: '![Tools and usage](img/B05137_09_011.jpg)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![工具和用法](img/B05137_09_011.jpg)'
- en: 'Figure 10: H2O Flow in the browser'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：浏览器中的H2O流
- en: '*Figure 11* shows the interface for importing files from the local filesystem
    or HDFS and displays detailed summary statistics as well as next actions that
    can be performed on the dataset. Once the data is imported, it gets a data frame
    reference in the H2O framework with the extension of `.hex`. The summary statistics
    are useful in understanding the characteristics of data such as **missing**, **mean**,
    **max**, **min**, and so on. It also has an easy way to transform the features
    from one type to another, for example, numeric features with a few unique values
    to categorical/nominal types known as `enum` in H2O.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '*图11*显示了从本地文件系统或HDFS导入文件的界面，并显示数据集的详细摘要统计以及可以执行的操作。一旦数据被导入，它就在H2O框架中获得一个以`.hex`为扩展名的数据框引用。摘要统计有助于理解数据的特征，如**缺失**、**平均值**、**最大值**、**最小值**等。它还有一个简单的方法将特征从一种类型转换为另一种类型，例如，具有少量唯一值的数值特征转换为H2O中称为`enum`的分类/名义类型。'
- en: 'The actions that can be performed on the datasets are:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在数据集上执行的操作包括：
- en: Visualize the data.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据。
- en: Split the data into different sets such as training, validation, and testing.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分割成不同的集合，如训练、验证和测试。
- en: Build supervised and unsupervised models.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建监督和无监督模型。
- en: Use the models to predict.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用模型进行预测。
- en: Download and export the files in various formats.![Tools and usage](img/B05137_09_012.jpg)
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载并导出各种格式的文件。![工具和用法](img/B05137_09_012.jpg)
- en: 'Figure 11: Importing data as frames, summarizations, and actions that can be
    performed'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图11：以框架、摘要和可执行操作导入数据
- en: 'Building supervised or unsupervised models in H2O is done through an interactive
    screen. Every modeling algorithm has its parameters classified into three sections:
    basic, advanced, and expert. Any parameter that supports hyper-parameter searches
    for tuning the model has a checkbox grid next to it, and more than one parameter
    value can be used.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在H2O中构建监督或无监督模型是通过交互式屏幕完成的。每个建模算法都有其参数，分为三个部分：基本、高级和专家。任何支持超参数搜索以调整模型的参数旁边都有一个复选框网格，并且可以使用多个参数值。
- en: Some basic parameters such as **training_frame**, **validation_frame**, and
    **response_column**, are common to every supervised algorithm; others are specific
    to model types, such as the choice of solver for GLM, the activation function
    for deep learning, and so on. All such common parameters are available in the
    basic section. Advanced parameters are settings that afford greater flexibility
    and control to the modeler if the default behavior must be overridden. Several
    of these parameters are also common across some algorithms—two examples are the
    choice of method for assigning the fold index (if cross-validation was selected
    in the basic section), and selecting the column containing weights (if each example
    is weighted separately), and so on.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基本参数，如**training_frame**、**validation_frame**和**response_column**，是每个监督算法共有的；其他参数特定于模型类型，例如GLM的求解器选择、深度学习的激活函数等。所有这些通用参数都在基本部分中。高级参数是允许模型器在必须覆盖默认行为时获得更多灵活性和控制的设置。其中一些参数在算法之间也是通用的——两个例子是分配折指数的方法选择（如果在基本部分中选择了交叉验证），以及选择包含权重的列（如果每个示例单独加权），等等。
- en: 'Expert parameters define more complex elements such as how to handle the missing
    values, model-specific parameters that need more than a basic understanding of
    the algorithms, and other esoteric variables. In *Figure 12*, GLM, a supervised
    learning algorithm, is being configured with 10-fold cross-validation, binomial
    (two-class) classification, efficient LBFGS optimization algorithm, and stratified
    sampling for cross-validation split:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 专家参数定义了更复杂的元素，例如如何处理缺失值、需要更多算法理解的模型特定参数，以及其他神秘变量。在*图12*中，GLM，一个监督学习算法，正在使用10折交叉验证、二项式（双类）分类、高效的LBFGS优化算法和分层采样进行交叉验证分割进行配置：
- en: '![Tools and usage](img/B05137_09_013.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![工具和用法](img/B05137_09_013.jpg)'
- en: 'Figure 12: Modeling algorithm parameters and validations'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：建模算法参数和验证
- en: The model results screen contains a detailed analysis of the results using important
    evaluation charts, depending on the validation method that was used. At the top
    of the screen are possible actions that can be taken, such as to run the model
    on unseen data for prediction, download the model as POJO format, export the results,
    and so on.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 模型结果屏幕包含了对结果的详细分析，使用重要的评估图表，具体取决于所使用的验证方法。屏幕顶部是可能采取的操作，例如在未见数据上运行模型进行预测、下载模型为POJO格式、导出结果等。
- en: Some of the charts are algorithm-specific, like the scoring history that shows
    how the training loss or the objective function changes over the iterations in
    GLM—this gives the user insight into the speed of convergence as well as into
    the tuning of the iterations parameter. We see the ROC curves and the Area Under
    Curve metric on the validation data in addition to the gains and lift charts,
    which give the cumulative capture rate and cumulative lift over the validation
    sample respectively.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一些图表是算法特定的，例如评分历史记录显示了在GLM中迭代过程中训练损失或目标函数如何变化——这使用户能够了解收敛速度以及迭代参数的调整。我们在验证数据中看到了ROC曲线和曲线下面积指标，以及增益和提升图表，分别给出了验证样本的累积捕获率和累积提升。
- en: '*Figure 13* shows **SCORING HISTORY**, **ROC CURVE**, and **GAINS/LIFT** charts
    for GLM on 10-fold cross-validation on the `CoverType` dataset:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13* 展示了 GLM 在 `CoverType` 数据集上 10 折交叉验证的 **评分历史**、**ROC 曲线**和 **增益/提升**
    图表：'
- en: '![Tools and usage](img/B05137_09_014.jpg)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![工具和用法](img/B05137_09_014.jpg)'
- en: 'Figure 13: Modeling and validation ROC curves, objective functions, and lift/gain
    charts'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13：建模和验证 ROC 曲线、目标函数和提升/增益图表
- en: The output of validation gives detailed evaluation measures such as accuracy,
    AUC, err, errors, f1 measure, MCC (Mathews Correlation Coefficient), precision,
    and recall for each validation fold in the case of cross-validation as well as
    the mean and standard deviation computed across all.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 验证输出的输出提供了详细的评估指标，如准确率、AUC、误差、错误、f1 指标、MCC（马修斯相关系数）、精确率和召回率，对于交叉验证中的每个验证折以及所有计算出的平均值和标准差。
- en: '![Tools and usage](img/B05137_09_015.jpg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![工具和用法](img/B05137_09_015.jpg)'
- en: 'Figure 14: Validation results and summary'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14：验证结果和总结
- en: The prediction action runs the model using unseen held-out data to estimate
    the out-of-sample performance. Important measures such as errors, accuracy, area
    under curve, ROC plots, and so on, are given as the output of predictions that
    can be saved or exported.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 预测操作使用未见过的保留数据运行模型来估计样本外性能。重要的指标，如错误、准确率、曲线下面积、ROC 图等，作为预测的输出给出，可以保存或导出。
- en: '![Tools and usage](img/B05137_09_016.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![工具和用法](img/B05137_09_016.jpg)'
- en: 'Figure 15: Running test data, predictions, and ROC curves'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15：运行测试数据、预测和 ROC 曲线
- en: Case study
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究
- en: In this case study, we use the `CoverType` dataset to demonstrate classification
    and clustering algorithms from H2O, Apache Spark MLlib, and SAMOA Machine Learning
    libraries in Java.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们使用 `CoverType` 数据集来展示 H2O、Apache Spark MLlib 和 SAMOA 机器学习库在 Java 中的分类和聚类算法。
- en: Business problem
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业问题
- en: The `CoverType` dataset available from the UCI machine learning repository ([https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype))
    contains unscaled cartographic data for 581,012 cells of forest land 30 x 30 m2
    in dimension, accompanied by actual forest cover type labels. In the experiments
    conducted here, we use the normalized version of the data. Including one-hot encoding
    of two categorical types, there are a total of 54 attributes in each row.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 可从 UCI 机器学习仓库获取的 `CoverType` 数据集（[https://archive.ics.uci.edu/ml/datasets/Covertype](https://archive.ics.uci.edu/ml/datasets/Covertype)）包含
    581,012 个 30 x 30 m2 尺寸的森林土地的未缩放地图数据，并附带实际的森林覆盖类型标签。在此处进行的实验中，我们使用数据的归一化版本。包括两种分类类型的
    one-hot 编码，每行总共有 54 个属性。
- en: Machine Learning mapping
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: First, we treat the problem as one of classification using the labels included
    in the dataset and perform several supervised learning experiments. With the models
    generated, we make predictions about the forest cover type of an unseen held out
    test dataset. For the clustering experiments that follow, we ignore the data labels,
    determine the number of clusters to use, and then report the corresponding cost
    using various algorithms implemented in H2O and Spark MLLib.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将问题视为一个分类问题，使用数据集中包含的标签执行多个监督学习实验。使用生成的模型，我们对未见过的保留测试数据集的森林覆盖类型进行预测。对于后续的聚类实验，我们忽略数据标签，确定要使用的聚类数量，然后报告使用
    H2O 和 Spark MLLib 中实现的多种算法对应的成本。
- en: Data collection
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: This dataset was collected using cartographic measurements only and no remote
    sensing. It was derived from data originally collected by the **US Forest Service**
    (**USFS**) and the **US Geological Survey** (**USGS**).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集仅使用地图测量收集，没有使用遥感。它来源于最初由 **美国森林服务局**（**USFS**）和**美国地质调查局**（**USGS**）收集的数据。
- en: Data sampling and transformation
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采样和转换
- en: Train and test data—The dataset was split into two sets in the ratio 20% for
    testing and 80% for training.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和测试数据—数据集被分成两个集合，测试占 20%，训练占 80%。
- en: The categorical Soil Type designation was represented by 40 binary variable
    attributes. A value of 1 indicates the presence of a soil type in the observation;
    a 0 indicates its absence.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 土壤类型分类的表示由 40 个二元变量属性组成。值为 1 表示观测中存在土壤类型；值为 0 表示不存在。
- en: The wilderness area designation is likewise a categorical attribute with four
    binary columns, with 1 indicating presence and 0 absence.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 野生动植物区域分类同样是一个分类属性，有四个二元列，其中 1 表示存在，0 表示不存在。
- en: All continuous value attributes have been normalized prior to use.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 所有连续值属性在使用前都已归一化。
- en: Experiments, results, and analysis
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验、结果和分析
- en: In the first set of experiments in this case study, we used the H2O framework.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究的第一个实验集中，我们使用了H2O框架。
- en: Feature relevance and analysis
  id: totrans-207
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征相关性和分析
- en: Though H2O doesn't have explicit feature selection algorithms, many learners
    such as GLM, random forest, GBT, and so on, give feature importance metrics based
    on training/validation of the models. In our analysis, we have used GLM for feature
    selection, as shown in *Figure 16*. It is interesting that the feature **Elevation**
    emerges as the most discriminating feature along with some categorical features
    that are converted into numeric/binary such as **Soil_Type2**, **Soil_Type4**,
    and so on. Many of the soil type categorical features have no relevance and can
    be dropped from the modeling perspective.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管H2O没有显式的特征选择算法，但许多学习器如GLM、随机森林、GBT等，基于模型的训练/验证提供特征重要性指标。在我们的分析中，我们使用了GLM进行特征选择，如图16所示。有趣的是，特征**高程**与一些转换为数值/二进制分类特征（如**Soil_Type2**、**Soil_Type4**等）一起成为最具区分性的特征。许多土壤类型分类特征没有相关性，可以从建模角度删除。
- en: 'Learning algorithms included in this set of experiments were: **Generalized
    Linear Models** (**GLM**), **Gradient Boosting Machine** (**GBM**), **Random Forest**
    (**RF**), **Naïve Bayes** (**NB**), and **Deep Learning** (**DL**). The deep learning
    model supported by H2O is the **multi-layered perceptron** (**MLP**).'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 本组实验中包含的学习算法有：**广义线性模型** (**GLM**)、**梯度提升机** (**GBM**)、**随机森林** (**RF**)、**朴素贝叶斯**
    (**NB**) 和 **深度学习** (**DL**)。H2O支持的深度学习模型是**多层感知器** (**MLP**)。
- en: '![Feature relevance and analysis](img/B05137_09_017.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![特征相关性和分析](img/B05137_09_017.jpg)'
- en: 'Figure 16: Feature selection using GLM'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图16：使用GLM进行特征选择
- en: Evaluation on test data
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 测试数据的评估
- en: 'The results using all the features are shown in the table:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用所有特征的结果显示在表中：
- en: '| Algorithm | Parameters | AUC | Max Accuracy | Max F1 | Max Precision | Max
    Recall | Max Specificity |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 参数 | AUC | 最大准确率 | 最大F1 | 最大精确率 | 最大召回率 | 最大特异性 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **GLM** | Default | 0.84 | 0.79 | 0.84 | 0.98 | 1.0(1) | 0.99 |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **GLM** | 默认 | 0.84 | 0.79 | 0.84 | 0.98 | 1.0(1) | 0.99 |'
- en: '| **GBM** | Default | 0.86 | 0.82 | 0.86 | 1.0(1) | 1.0(1) | 1.0(1) |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **GBM** | 默认 | 0.86 | 0.82 | 0.86 | 1.0(1) | 1.0(1) | 1.0(1) |'
- en: '| **Random Forest** (**RF**) | Default | 0.88(1) | 0.83(1) | 0.87(1) | 0.97
    | 1.0(1) | 0.99 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **随机森林** (**RF**) | 默认 | 0.88(1) | 0.83(1) | 0.87(1) | 0.97 | 1.0(1) | 0.99
    |'
- en: '| **Naïve Bayes** (**NB**) | Laplace=50 | 0.66 | 0.72 | 0.81 | 0.68 | 1.0(1)
    | 0.33 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **朴素贝叶斯** (**NB**) | Laplace=50 | 0.66 | 0.72 | 0.81 | 0.68 | 1.0(1) | 0.33
    |'
- en: '| **Deep Learning** (**DL**) | Rect,300, 300,Dropout | 0. | 0.78 | 0.83 | 0.88
    | 1.0(1) | 0.99 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **深度学习** (**DL**) | Rect,300, 300,Dropout | 0. | 0.78 | 0.83 | 0.88 | 1.0(1)
    | 0.99 |'
- en: '| **Deep Learning** (**DL**) | 300, 300,MaxDropout | 0.82 | 0.8 | 0.84 | 1.0(1)
    | 1.0(1) | 1.0(1) |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **深度学习** (**DL**) | 300, 300, MaxDropout | 0.82 | 0.8 | 0.84 | 1.0(1) | 1.0(1)
    | 1.0(1) |'
- en: 'The results after removing features not scoring well in feature relevance were:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 移除在特征相关性评分中表现不佳的特征后的结果如下：
- en: '| Algorithm | Parameters | AUC | Max Accuracy | Max F1 | MaxPrecision | Max
    Recall | Max Specificity |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 参数 | AUC | 最大准确率 | 最大F1 | 最大精确率 | 最大召回率 | 最大特异性 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **GLM** | Default | 0.84 | 0.80 | 0.85 | 1.0 | 1.0 | 1.0 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| **GLM** | 默认 | 0.84 | 0.80 | 0.85 | 1.0 | 1.0 | 1.0 |'
- en: '| **GBM** | Default | 0.85 | 0.82 | 0.86 | 1.0 | 1.0 | 1.0 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| **GBM** | 默认 | 0.85 | 0.82 | 0.86 | 1.0 | 1.0 | 1.0 |'
- en: '| **Random Forest** (**RF**) | Default | 0.88 | 0.83 | 0.87 | 1.0 | 1.0 | 1.0
    |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| **随机森林** (**RF**) | 默认 | 0.88 | 0.83 | 0.87 | 1.0 | 1.0 | 1.0 |'
- en: '| **Naïve Bayes** (**NB**) | Laplace=50 | 0.76 | 0.74 | 0.81 | 0.89 | 1.0 |
    0.95 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| **朴素贝叶斯** (**NB**) | Laplace=50 | 0.76 | 0.74 | 0.81 | 0.89 | 1.0 | 0.95
    |'
- en: '| **Deep Learning** (**DL**) | 300,300, RectDropout | 0.81 | 0.79 | 0.84 |
    1.0 | 1.0 | 1.0 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| **深度学习** (**DL**) | 300,300, RectDropout | 0.81 | 0.79 | 0.84 | 1.0 | 1.0
    | 1.0 |'
- en: '| **Deep Learning** (**DL**) | 300, 300, MaxDropout | 0.85 | 0.80 | 0.84 |
    0.89 | 0.90 | 1.0 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| **深度学习** (**DL**) | 300, 300, MaxDropout | 0.85 | 0.80 | 0.84 | 0.89 | 0.90
    | 1.0 |'
- en: 'Table 1: Model evaluation results with all features included'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：包含所有特征的模型评估结果
- en: Analysis of results
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 结果分析
- en: The main observations from an analysis of the results obtained are quite instructive
    and are presented here.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果分析中获得的主要观察结果非常有启发性，在此处展示。
- en: The feature relevance analysis shows how the **Elevation** feature is a highly
    discriminating feature, whereas many categorical attributes converted to binary
    features, such as **SoilType_10**, and so on, have near-zero to no relevance.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征相关性分析显示了**海拔**特征是一个高度区分性的特征，而许多转换为二进制特征的分类属性，如**SoilType_10**等，其相关性几乎为零或没有。
- en: The results for experiments with all features included, shown in *Table 1*,
    clearly indicate that the non-linear ensemble technique Random Forest is the best
    algorithm as shown by the majority of the evaluation metrics including accuracy,
    F1, AUC, and recall.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 包含所有特征的实验结果，如*表1*所示，清楚地表明，非线性集成技术随机森林（Random Forest）是最佳算法，这从包括准确率、F1、AUC和召回率在内的多数评估指标中可以看出。
- en: '*Table 1* also highlights the fact that whereas the faster, linear Naive Bayes
    algorithm may not be best-suited, GLM, which also falls in the category of linear
    algorithms, demonstrates much better performance—this points to some inter-dependence
    among features!'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*表1*也突出了这样一个事实：虽然较快的线性朴素贝叶斯（Naive Bayes）算法可能不是最佳选择，但属于线性算法类别的GLM表现出更好的性能——这表明特征之间存在一些相互依赖性！'
- en: As we saw in [Chapter 7](ch07.html "Chapter 7. Deep Learning"), *Deep Learning,*
    algorithms used in deep learning typically need a lot of tuning; however, even
    with a few small tuning changes, the results from DL are comparable to Random
    Forest, especially with MaxDropout.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如我们在[第7章](ch07.html "第7章。深度学习")中看到的，*深度学习*算法通常需要大量的调整；然而，即使只有少数小的调整，深度学习（DL）的结果与随机森林（Random
    Forest）相比也是可以相提并论的，尤其是在使用MaxDropout的情况下。
- en: '*Table 2* shows the results of all the algorithms after removing low-relevance
    features from the training set. It can be seen that Naive Bayes—which has the
    most impact due to multiplication of probabilities based on the assumption of
    independence between features—gets the most benefit and highest uplift in performance.
    Most of the other algorithms such as Random Forest have inbuilt feature selection
    as we discussed in [Chapter 2](ch02.html "Chapter 2. Practical Approach to Real-World
    Supervised Learning"), *Practical Approach to Real-World Supervised Learning,*
    and as a result removing the unimportant features has little or no effect on their
    performance.'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*表2*显示了从训练集中移除低相关性特征后所有算法的结果。可以看出，由于基于特征之间独立性的假设进行概率乘法，朴素贝叶斯（Naive Bayes）的影响最大，因此它获得了最大的利益和性能提升。大多数其他算法，如随机森林（Random
    Forest），如我们在[第2章](ch02.html "第2章。面向现实世界监督学习的实用方法")中讨论的，*面向现实世界监督学习的实用方法*，内置了特征选择，因此移除不重要的特征对它们的性能影响很小或没有影响。'
- en: Spark MLlib as Big Data Machine Learning platform
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark MLlib作为大数据机器学习平台
- en: Apache Spark, started in 2009 at AMPLab at UC Berkley, was donated to Apache
    Software Foundation in 2013 under Apache License 2.0\. The core idea of Spark
    was to build a cluster computing framework that would overcome the issues of Hadoop,
    especially for iterative and in-memory computations.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark始于2009年，在加州大学伯克利分校的AMPLab，于2013年在Apache License 2.0下捐赠给了Apache软件基金会。Spark的核心思想是构建一个集群计算框架，以克服Hadoop的问题，特别是对于迭代和内存计算。
- en: Spark architecture
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark架构
- en: The Spark stack as shown in *Figure 17* can use any kind of data stores such
    as HDFS, SQL, NoSQL, or local filesystems. It can be deployed on Hadoop, Mesos,
    or even standalone.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图17*所示的Spark堆栈可以使用任何类型的数据存储，如HDFS、SQL、NoSQL或本地文件系统。它可以在Hadoop、Mesos或独立部署。
- en: The most important component of Spark is the Spark Core, which provides a framework
    to handle and manipulate the data in a high-throughput, fault-tolerant, and scalable
    manner.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: Spark最重要的组件是Spark Core，它提供了一个框架，以高吞吐量、容错和可扩展的方式处理和操作数据。
- en: Built on top of Spark core are various libraries each meant for various functionalities
    needed in processing data and doing analytics in the Big Data world. Spark SQL
    gives us a language for performing data manipulation in Big Data stores using
    a querying language very much like SQL, the *lingua franca* of databases. Spark
    GraphX provides APIs to perform graph-related manipulations and graph-based algorithms
    on Big Data. Spark Streaming provides APIs to handle real-time operations needed
    in stream processing ranging from data manipulations to queries on the streams.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在Spark核心之上的各种库，每个库都针对大数据世界中处理数据和进行数据分析所需的各种功能。Spark SQL为我们提供了一种使用类似于SQL的查询语言在大数据存储中进行数据操作的语言，SQL是数据库的*通用语言*。Spark
    GraphX提供了执行图相关操作和基于图的算法的API。Spark Streaming提供了处理流处理中所需实时操作的API，从数据操作到对流的查询。
- en: Spark-MLlib is the Machine Learning library that has an extensive set of Machine
    Learning algorithms to perform supervised and unsupervised tasks from feature
    selection to modeling. Spark has various language bindings such as Java, R, Scala,
    and Python. MLlib has a clear advantage running on top of the Spark engine, especially
    because of caching data in memory across multiple nodes and running MapReduce
    jobs, thus improving performance as compared to Mahout and other large-scale Machine
    Learning engines by a significant factor. MLlib also has other advantages such
    as fault tolerance and scalability without explicitly managing it in the Machine
    Learning algorithms.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: Spark-MLlib 是一个机器学习库，拥有广泛的机器学习算法，可以执行从特征选择到建模的监督和未监督任务。Spark 有各种语言绑定，如 Java、R、Scala
    和 Python。MLlib 在 Spark 引擎上运行具有明显的优势，尤其是在跨多个节点缓存数据以及在内存中运行 MapReduce 作业方面，从而与 Mahout
    和其他大规模机器学习引擎相比，性能得到了显著提升。MLlib 还具有其他优势，如容错性和可伸缩性，无需在机器学习算法中显式管理。
- en: '![Spark architecture](img/B05137_09_018.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 架构](img/B05137_09_018.jpg)'
- en: 'Figure 17: Apache Spark high level architecture'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17：Apache Spark 高级架构
- en: 'The Spark core has the following components:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 核心组件如下：
- en: '**Resilient Distributed Datasets** (**RDD**): RDDs are the basic immutable
    collection of objects that Spark Core knows how to partition and distribute across
    the cluster for performing tasks. RDDs are composed of "partitions", dependent
    on parent RDDs and metadata about data placement.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**弹性分布式数据集**（**RDD**）：RDD 是 Spark Core 知道如何分区和分布到集群以执行任务的不可变对象的基本集合。RDD 由“分区”组成，这些分区依赖于父
    RDD 和关于数据放置的元数据。'
- en: 'Two distinct operations are performed on RDDs:'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 RDD 上执行两种不同的操作：
- en: '**Transformations**: Operations that are lazily evaluated and transform one
    RDD into another. Lazy evaluation defers evaluation as long as possible, which
    makes some resource optimizations possible.'
  id: totrans-251
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换**：这些操作是延迟评估的，可以将一个 RDD 转换为另一个 RDD。延迟评估尽可能推迟评估，这使得一些资源优化成为可能。'
- en: '**Action**: The actual operation that triggers transformations and returns
    output values'
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：触发转换并返回输出值的实际操作'
- en: '**Lineage graph**: The pipeline or flow of data describing the computation
    for a particular task, including different RDDs created in transformations and
    actions is known as the lineage graph of the task. The lineage graph plays a key
    role in fault tolerance.![Spark architecture](img/B05137_09_019.jpg)'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**血缘图**：描述特定任务的计算流程或数据流，包括在转换和操作中创建的不同 RDD，称为任务的血缘图。血缘图在容错中扮演着关键角色。![Spark
    架构](img/B05137_09_019.jpg)'
- en: 'Figure 18: Apache Spark lineage graph'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 18：Apache Spark 血缘图
- en: Spark is agnostic to the cluster management and can work with several implementations—including
    YARN and Mesos—for managing the nodes, distributing the work, and communications.
    The distribution of tasks in Transformations and Actions across the cluster is
    done by the scheduler, starting from the driver node where the Spark context is
    created, to the many worker nodes as shown in *Figure 19*. When running with YARN,
    Spark gives the user the choice of the number of executors, heap, and core allocation
    per JVM at the node level.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 对集群管理是中立的，可以与 YARN 和 Mesos 等多个实现一起工作，以管理节点、分配工作和通信。转换和操作在集群中的任务分配是由调度器完成的，从创建
    Spark 上下文的驱动节点开始，到如图 19 所示的许多工作节点。当与 YARN 一起运行时，Spark 允许用户在节点级别选择执行器的数量、堆大小和每个
    JVM 的核心分配。
- en: '![Spark architecture](img/B05137_09_020.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![Spark 架构](img/B05137_09_020.jpg)'
- en: 'Figure 19: Apace Spark cluster deployment and task distribution'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19：Apache Spark 集群部署和任务分配
- en: Machine Learning in MLlib
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLlib 中的机器学习
- en: 'Spark MLlib has a comprehensive Machine Learning toolkit, offering more algorithms
    than H2O at the time of writing, as shown in *Figure 20*:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib 拥有一个全面的机器学习工具包，提供的算法比写作时的 H2O 更多，如图 20 所示：
- en: '![Machine Learning in MLlib](img/B05137_09_021.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![MLlib 中的机器学习](img/B05137_09_021.jpg)'
- en: 'Figure 20: Apache Spark MLlib machine learning algorithms'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图 20：Apache Spark MLlib 机器学习算法
- en: Many extensions have been written for Spark, including Spark MLlib, and the
    user community continues to contribute more packages. You can download third-party
    packages or register your own at [https://spark-packages.org/](https://spark-packages.org/).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Spark 编写了许多扩展，包括 Spark MLlib，用户社区还在继续贡献更多包。您可以在 [https://spark-packages.org/](https://spark-packages.org/)
    下载第三方包或注册您自己的包。
- en: Tools and usage
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工具和用法
- en: Spark MLlib provides APIs for other languages in addition to Java, including
    Scala, Python, and R. When a `SparkContext` is created, it launches a monitoring
    and instrumentation web console at port `4040`, which lets us see key information
    about the runtime, including scheduled tasks and their progress, RDD sizes and
    memory use, and so on. There are also external profiling tools available for use.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib除了Java之外，还为Scala、Python和R等语言提供了API。当创建`SparkContext`时，它会在端口`4040`启动一个监控和仪表化Web控制台，使我们能够查看有关运行时的关键信息，包括计划的任务及其进度、RDD大小和内存使用情况等。还有可用的外部分析工具。
- en: Experiments, results, and analysis
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实验、结果和分析
- en: The business problem we tackled here is the same as the one described earlier
    for experiments using H2O. We employed five learning algorithms using MLlib, in
    all. The first was k-Means with all features using a *k* value determined from
    computing the cost—specifically, the **Sum of Squared Errors** (**SSE**)—over
    a large number of values of *k* and selecting the "elbow" of the curve. Determining
    the optimal value of *k* is typically not an easy task; often, evaluation measures
    such as silhouette are compared in order to pick the best *k*. Even though we
    know the number of classes in the dataset is *7*, it is instructive to see where
    experiments like this lead if we pretend we did not have labeled data. The optimal
    *k* found using the elbow method was 27\. In the real world, business decisions
    may often guide the selection of *k*.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里解决的业务问题与之前使用H2O进行实验所描述的问题相同。我们总共使用了MLlib中的五种学习算法。第一个是使用从计算大量*k*值得到的成本（特别是**平方和误差**（**SSE**））确定的*k*值进行k-Means聚类，并选择曲线的“肘部”。确定最优的*k*值通常不是一个容易的任务；通常，为了选择最佳的*k*，会比较如轮廓等评估指标。尽管我们知道数据集中类的数量是*7*，但如果假设我们没有标记数据，看到此类实验的走向也是有益的。使用肘部方法找到的最优*k*值为27。在现实世界中，业务决策可能经常指导*k*的选择。
- en: In the following listings, we show how to use different models from the MLlib
    suite to do cluster analysis and classification. The code is based on examples
    available in the MLlib API Guide ([https://spark.apache.org/docs/latest/mllib-guide.html](https://spark.apache.org/docs/latest/mllib-guide.html)).
    We use the normalized UCI `CoverType` dataset in CSV format. Note that it is more
    natural to use `spark.sql.Dataset` with the newer `spark.ml` package, whereas
    the `spark.mllib` package works more closely with `JavaRDD`. This provides an
    abstraction over RDDs and allows for optimization of the transformations beneath
    the covers. In the case of most unsupervised learning algorithms, this means the
    data must be transformed such that the dataset to be used for training and testing
    should have a column called features by default that contains all the features
    of an observation as a vector. A `VectorAssembler` object can be used for this
    transformation. A glimpse into the use of ML pipelines, which is a way to chain
    tasks together, is given in the source code for training a Random Forest classifier.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中，我们展示了如何使用MLlib套件中的不同模型来进行聚类分析和分类。代码基于MLlib API指南中提供的示例（[https://spark.apache.org/docs/latest/mllib-guide.html](https://spark.apache.org/docs/latest/mllib-guide.html)）。我们使用CSV格式的标准化UCI
    `CoverType`数据集。请注意，使用较新的`spark.ml`包中的`spark.sql.Dataset`更为自然，而`spark.mllib`包则更紧密地与`JavaRDD`协同工作。这为RDD提供了抽象，并允许对底层的转换进行优化。对于大多数无监督学习算法来说，这意味着数据必须进行转换，以便用于训练和测试的数据集默认包含一个名为features的列，该列包含观察到的所有特征作为向量。可以使用`VectorAssembler`对象进行这种转换。源代码中给出了对ML管道的使用示例，这是一种将任务链式连接起来的方法，用于训练随机森林分类器。
- en: k-Means
  id: totrans-268
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: k-Means
- en: 'The following code fragment for the k-Means experiment uses the algorithm from
    the `org.apache.spark.ml.clustering` package. The code includes minimal boilerplate
    for setting up the `SparkSession`, which is the handle to the Spark runtime. Note
    that eight cores have been specified in local mode in the setup:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 以下k-Means实验的代码片段使用了来自`org.apache.spark.ml.clustering`包的算法。代码包括设置`SparkSession`（Spark运行时的句柄）的最小模板代码。请注意，在设置中指定了本地模式下的八个核心：
- en: '[PRE1]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The optimal value for the number of clusters was arrived at by evaluating and
    plotting the sum of squared errors for several different values and choosing the
    one at the elbow of the curve. The value used here is *27*.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通过评估和绘制不同值的平方和误差，并选择曲线的肘部值，得到了最佳聚类数量值。这里使用的值是*27*。
- en: k-Means with PCA
  id: totrans-272
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 带PCA的k-Means
- en: 'In the second experiment, we used k-Means again, but first we reduced the number
    of dimensions in the data through PCA. Again, we used a rule of thumb here, which
    is to select a value for the PCA parameter for the number of dimensions such that
    at least 85% of the variance in the original dataset is preserved after the reduction
    in dimensionality. This produced 16 features in the transformed dataset from an
    initial 54, and this dataset was used in this and subsequent experiments. The
    following code shows the relevant code for PCA analysis:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个实验中，我们再次使用了k-Means，但首先通过PCA减少了数据中的维度。在这里，我们使用了一个经验法则，即选择PCA参数的维度值，以确保在降维后至少保留了原始数据集85%的方差。这从最初的54个特征产生了转换数据集中的16个特征，并且这个数据集被用于本实验和后续实验。以下代码显示了PCA分析的相关代码：
- en: '[PRE2]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Bisecting k-Means (with PCA)
  id: totrans-275
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Bisecting k-Means (with PCA)
- en: 'The third experiment used MLlib''s Bisecting k-Means algorithm. This algorithm
    is similar to a top-down hierarchical clustering technique where all instances
    are in the same cluster at the outset, followed by successive splits:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个实验使用了MLlib的Bisecting k-Means算法。这个算法类似于一种自上而下的层次聚类技术，其中所有实例最初都在同一个簇中，然后进行连续的分割：
- en: '[PRE3]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Gaussian Mixture Model
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 高斯混合模型
- en: 'In the next experiment, we used MLlib''s **Gaussian Mixture Model** (**GMM**),
    another clustering model. The assumption inherent to this model is that the data
    distribution in each cluster is Gaussian in nature, with unknown parameters. The
    same number of clusters is specified here, and default values have been used for
    the maximum number of iterations and tolerance, which dictate when the algorithm
    is considered to have converged:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个实验中，我们使用了MLlib的**高斯混合模型**（**GMM**），另一种聚类模型。该模型固有的假设是每个簇中的数据分布本质上是高斯分布，具有未知参数。这里指定了相同数量的簇，并且已使用默认值作为最大迭代次数和容忍度，这些值决定了算法何时被认为已收敛：
- en: '[PRE4]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Random Forest
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 随机森林
- en: 'Finally, we ran Random Forest, which is the only available ensemble learner
    that can handle multi-class classification. In the following code, we see that
    this algorithm needs some preparatory tasks to be performed prior to training.
    Pre-processing stages are composed into a pipeline of Transformers and Estimators.
    The pipeline is then used to fit the data. You can learn more about Pipelines
    on the Apache Spark website ([https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们运行了随机森林，这是唯一可用的能够处理多类分类的集成学习器。在以下代码中，我们可以看到这个算法在训练之前需要执行一些预备任务。预处理阶段被组合成一个由Transformers和Estimators组成的管道。然后使用该管道来拟合数据。您可以在Apache
    Spark网站上了解更多关于管道的信息（[https://spark.apache.org/docs/latest/ml-pipeline.html](https://spark.apache.org/docs/latest/ml-pipeline.html)）：
- en: '[PRE5]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The sum of squared errors for the experiments using k-Means and Bisecting k-Means
    are given in the following table:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-Means和Bisecting k-Means的实验的均方误差在以下表中给出：
- en: '| Algorithm | k | Features | SSE |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | k | 特征 | SSE |'
- en: '| --- | --- | --- | --- |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| k-Means | 27 | 54 | 214,702 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| k-Means | 27 | 54 | 214,702 |'
- en: '| k-Means(PCA) | 27 | 16 | 241,155 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| k-Means(PCA) | 27 | 16 | 241,155 |'
- en: '| Bisecting k-Means(PCA) | 27 | 16 | 305,644 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| Bisecting k-Means(PCA) | 27 | 16 | 305,644 |'
- en: 'Table 3: Results with k-Means'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 表3：k-Means的结果
- en: The GMM model was used to illustrate the use of the API; it outputs the parameters
    of the Gaussian mixture for every cluster as well as the cluster weight. Output
    for all the clusters can be seen at the website for this book.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: GMM模型被用来展示API的使用；它为每个簇输出高斯混合的参数以及簇权重。所有簇的输出可以在本书的网站上查看。
- en: 'For the case of Random Forest these are the results for runs with different
    numbers of trees. All 54 features were used here:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 对于随机森林的情况，这些是不同树的数量运行的结果。这里使用了所有54个特征：
- en: '| Number of trees | Accuracy | F1 measure | Weighted precision | Weighted recall
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 树的数量 | 准确率 | F1度量 | 加权精度 | 加权召回率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 15 | 0.6806 | 0.6489 | 0.6213 | 0.6806 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.6806 | 0.6489 | 0.6213 | 0.6806 |'
- en: '| 20 | 0.6776 | 0.6470 | 0.6191 | 0.6776 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.6776 | 0.6470 | 0.6191 | 0.6776 |'
- en: '| 25 | 0.5968 | 0.5325 | 0.5717 | 0.5968 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 0.5968 | 0.5325 | 0.5717 | 0.5968 |'
- en: '| 30 | 0.6547 | 0.6207 | 0.5972 | 0.6547 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 30 | 0.6547 | 0.6207 | 0.5972 | 0.6547 |'
- en: '| 40 | 0.6594 | 0.6272 | 0.6006 | 0.6594 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 40 | 0.6594 | 0.6272 | 0.6006 | 0.6594 |'
- en: 'Table 4: Results for Random Forest'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 表4：随机森林的结果
- en: Analysis of results
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结果分析
- en: As can be seen from *Table 3*, there is a small increase in cost when fewer
    dimensions are used after PCA with the same number of clusters. Varying *k* with
    PCA might suggest a better *k* for the PCA case. Notice also that in this experiment,
    for the same *k*, Bisecting K-Means with PCA-derived features has the highest
    cost of all. The stopping number of clusters used for Bisecting k-Means has simply
    been picked to be the one determined for basic k-Means, but this need not be so.
    A similar search for *k* that yields the best cost may be done independently for
    Bisecting k-Means.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *表3* 所示，在PCA后使用较少维度时，成本略有增加，而簇的数量保持不变。随着PCA中 *k* 的变化，可能表明PCA情况下的更好 *k*。注意，在这个实验中，对于相同的
    *k*，使用PCA导出的特征的Bisecting K-Means具有最高的成本。用于Bisecting k-Means的停止簇数简单地被选为基本k-Means确定的那个，但不必如此。对于Bisecting
    k-Means，可以独立进行寻找最佳成本 *k* 的类似搜索。
- en: In the case of Random Forest, we see the best performance when using *15* trees.
    All trees have a depth of three. This hyper-parameter can be varied to tune the
    model as well. Even though Random Forest is not susceptible to over-fitting due
    to accounting for variance across trees in the training stages, increasing the
    value for the number of trees beyond an optimum number can degrade performance.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林的情况下，当使用 *15* 棵树时，我们看到最佳性能。所有树都有三个深度。这个超参数可以变化以调整模型。尽管随机森林由于在训练阶段考虑了树之间的方差，因此不易过拟合，但将树的数量增加到最佳数量以上可能会降低性能。
- en: Real-time Big Data Machine Learning
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实时大数据机器学习
- en: In this section, we will discuss the real-time version of Big Data Machine Learning
    where data arrives in large volumes and is changing at a rapid rate at the same
    time. Under these conditions, Machine Learning analytics cannot be applied *per*
    the traditional practice of "batch learning and deploy" (*References* [14]).
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论大数据机器学习的实时版本，其中数据以大量形式到达，并且同时以快速的速度发生变化。在这些条件下，机器学习分析不能按照传统的“批量学习与部署”实践进行
    *per* (*参考文献* [14])。
- en: '![Real-time Big Data Machine Learning](img/B05137_09_023.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![实时大数据机器学习](img/B05137_09_023.jpg)'
- en: 'Figure 21: Use case for real-time Big Data Machine Learning'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图21：实时大数据机器学习的用例
- en: 'Let us consider a case where labeled data is available for a short duration,
    and we perform the appropriate modeling techniques on the data and then apply
    the most suitable evaluation methods on the resulting models. Next, we select
    the best model and use it for predictions on unseen data at runtime. We then observe,
    with some dismay, that model performance drops significantly over time. Repeating
    the exercise with new data shows a similar degradation in performance! What are
    we to do now? This quandary, combined with large volumes of data motivates the
    need for a different approach: real-time Big Data Machine Learning.'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个案例，其中在短时间内有标记数据可用，我们对数据进行适当的建模技术，然后对生成的模型应用最合适的评估方法。接下来，我们选择最佳模型，并在运行时使用它对未见数据进行预测。然后，我们观察到，模型性能随着时间的推移显著下降。用新数据进行重复的练习显示出类似性能的退化！我们现在该怎么办？这种困境，加上大量数据，促使我们需要不同的方法：实时大数据机器学习。
- en: Like the batch learning framework, the real-time framework in big data may have
    similar components up until the data preparation stage. When the computations
    involved in data preparation must take place on streams or combined stream and
    batch data, we require specialized computation engines such as **Spark Streaming**.
    Like stream computations, Machine Learning must work across the cluster and perform
    different Machine Learning tasks on the stream. This adds an additional layer
    of complexity to the implementations of single machine multi-threaded streaming
    algorithms.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 与批量学习框架类似，大数据中的实时框架在数据准备阶段之前可能具有类似的组件。当数据准备中涉及的计算必须在流或流与批量数据组合上进行时，我们需要专门的计算引擎，例如
    **Spark Streaming**。像流计算一样，机器学习必须在集群上工作，并在流上执行不同的机器学习任务。这给单机多线程流算法的实现增加了额外的复杂性。
- en: '![Real-time Big Data Machine Learning](img/B05137_09_024.jpg)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![实时大数据机器学习](img/B05137_09_024.jpg)'
- en: 'Figure 22: Real-time big data components and providers'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图22：实时大数据组件和提供商
- en: SAMOA as a real-time Big Data Machine Learning framework
  id: totrans-312
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SAMOA作为实时大数据机器学习框架
- en: For a single machine, in [Chapter 5](ch05.html "Chapter 5. Real-Time Stream
    Machine Learning"), *Real-Time Stream Machine Learning*, we discussed the MOA
    framework at length. SAMOA is the distributed framework for performing Machine
    Learning on streams.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](ch05.html "第5章. 实时流机器学习")“实时流机器学习”中，我们详细讨论了MOA框架。SAMOA是用于在流上执行机器学习的分布式框架。
- en: At the time of writing, SAMOA is an incubator-level open source project with
    Apache 2.0 license and good integration with different stream processing engines
    such as **Apache Storm**, **Samza**, and **S4**.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，SAMOA是一个孵化级开源项目，拥有Apache 2.0许可证，并与不同的流处理引擎（如**Apache Storm**、**Samza**和**S4**）有良好的集成。
- en: SAMOA architecture
  id: totrans-315
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: SAMOA架构
- en: The SAMOA framework offers several key streaming services to an extendable set
    of stream processing engines, with existing implementations for the most popular
    engines of today.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: SAMOA框架为可扩展的流处理引擎集提供几个关键流服务，现有实现适用于今天最受欢迎的引擎。
- en: '![SAMOA architecture](img/B05137_09_025.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![SAMOA架构](img/B05137_09_025.jpg)'
- en: 'Figure 23: SAMOA high level architecture'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图23：SAMOA高级架构
- en: '`TopologyBuilder` is the interface that acts as a factory to create different
    components and connect them together in SAMOA. The core of SAMOA is in building
    processing elements for data streams. The basic unit for processing consists of
    `ProcessingItem` and the `Processor` interface, as shown in *Figure 24*. `ProcessingItem`
    is an encapsulated hidden element, while Processor is the core implementation
    where the logic for handling streams is coded.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`TopologyBuilder`是一个充当工厂的接口，用于在SAMOA中创建不同的组件并将它们连接在一起。SAMOA的核心在于构建数据流的处理元素。处理的基本单元由`ProcessingItem`和`Processor`接口组成，如图24所示。`ProcessingItem`是一个封装的隐藏元素，而`Processor`是核心实现，其中编码了处理流的逻辑。'
- en: '![SAMOA architecture](img/B05137_09_026.jpg)'
  id: totrans-320
  prefs: []
  type: TYPE_IMG
  zh: '![SAMOA架构](img/B05137_09_026.jpg)'
- en: 'Figure 24: SAMOA processing data streams'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 图24：SAMOA处理数据流
- en: '**Stream** is another interface that connects various **Processors** together
    as the source and destination created by `TopologyBuilder`. A Stream can have
    one source and multiple destinations. Stream supports three forms of communication
    between source and destinations:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**流**是另一个接口，它将各种**处理器**连接起来，作为由`TopologyBuilder`创建的源和目的地。一个流可以有一个源和多个目的地。流支持源和目的地之间的三种通信形式：'
- en: '**All**: In this communication, all messages from source are sent to all the
    destinations'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**所有**: 在本通信中，所有来自源的消息都被发送到所有目的地'
- en: '**Key**: In this communication, messages with the same keys are sent to the
    same processors'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**键**: 在本通信中，具有相同键的消息被发送到相同的处理器'
- en: '**Shuffle**: In this communication, messages are randomly sent to the processors'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**洗牌**: 在本通信中，消息被随机发送到处理器'
- en: All the messages or events in SAMOA are implementations of the interface `ContentEvent`,
    encapsulating mostly the data in the streams as a value and having some form of
    key for uniqueness.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: SAMOA中的所有消息或事件都是`ContentEvent`接口的实现，主要封装流中的数据作为值，并具有某种形式的键以实现唯一性。
- en: Each stream processing engine has an implementation for all the key interfaces
    as a plugin and integrates with SAMOA. The Apache Storm implementations StormTopology,
    StormStream, and StormProcessingItem, and so on are shown in the API in *Figure
    25*.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 每个流处理引擎都有一个作为插件的实现，用于所有关键接口，并与SAMOA集成。API中展示了Apache Storm的实现，如StormTopology、StormStream和StormProcessingItem等，如图25所示。
- en: Task is another unit of work in SAMOA, having the responsibility of execution.
    All the classification or clustering evaluation and validation techniques such
    as prequential, holdout, and so on, are implemented as Tasks.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: Task是SAMOA中的另一个工作单元，负责执行。所有分类或聚类评估和验证技术，如预quential、holdout等，都作为任务实现。
- en: Learner is the interface for implementing all Supervised and Unsupervised Learning
    capability in SAMOA. Learners can be local or distributed and have different extensions
    such as `ClassificationLearner` and `RegressionLearner`.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: Learner是用于在SAMOA中实现所有监督学习和无监督学习能力的接口。学习者可以是本地的或分布式的，并具有不同的扩展，如`ClassificationLearner`和`RegressionLearner`。
- en: Machine Learning algorithms
  id: totrans-330
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 机器学习算法
- en: '![Machine Learning algorithms](img/B05137_09_027.jpg)![Machine Learning algorithms](img/B05137_09_028.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习算法](img/B05137_09_027.jpg)![机器学习算法](img/B05137_09_028.jpg)'
- en: 'Figure 25: SAMOA machine learning algorithms'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：SAMOA机器学习算法
- en: '*Figure 25* shows the core components of the SAMOA topology and their implementation
    for various engines.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '*图25*展示了SAMOA拓扑的核心组件及其对不同引擎的实现。'
- en: Tools and usage
  id: totrans-334
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 工具和用法
- en: 'We continue with the same business problem as before. The command line to launch
    the training job for the `covtype` dataset is:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用之前相同的商业问题。启动`covtype`数据集训练作业的命令行是：
- en: '[PRE6]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Tools and usage](img/B05137_09_029.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![工具和用法](img/B05137_09_029.jpg)'
- en: 'Figure 25: Bagging model performance'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 图25：袋装模型性能
- en: 'When running with Storm, this is the command line:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 当与Storm一起运行时，这是命令行：
- en: '[PRE7]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Experiments, results, and analysis
  id: totrans-341
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 实验、结果和分析
- en: The results of experiments using SAMOA as a stream-based learning platform for
    Big Data are given in *Table 5*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SAMOA作为大数据基于流的平台进行实验的结果在*表5*中给出。
- en: '| Algorithm | Best Accuracy | Final Accuracy | Final Kappa Statistic | Final
    Kappa Temporal Statistic |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 最佳准确率 | 最终准确率 | 最终Kappa统计量 | 最终Kappa时间统计量 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Bagging | 79.16 | 64.09 | 37.52 | -69.51 |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '| Bagging | 79.16 | 64.09 | 37.52 | -69.51 |'
- en: '| Boosting | 78.05 | 47.82 | 0 | -1215.1 |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '| Boosting | 78.05 | 47.82 | 0 | -1215.1 |'
- en: '| VerticalHoeffdingTree | 83.23 | 67.51 | 44.35 | -719.51 |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| VerticalHoeffdingTree | 83.23 | 67.51 | 44.35 | -719.51 |'
- en: '| AdaptiveBagging | 81.03 | 64.64 | 38.99 | -67.37 |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| AdaptiveBagging | 81.03 | 64.64 | 38.99 | -67.37 |'
- en: 'Table 5: Experimental results with Big Data real-time learning using SAMOA'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 表5：使用SAMOA进行大数据实时学习实验结果
- en: Analysis of results
  id: totrans-350
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结果分析
- en: 'From an analysis of the results, the following observations can be made:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果分析中，可以得出以下观察：
- en: '*Table 5* shows that the popular non-linear decision tree-based VHDT on SAMOA
    is the best performing algorithm according to almost all the metrics.'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*表5*显示，在几乎所有的指标中，基于流行的非线性决策树VHDT的SAMOA是最具表现力的算法。'
- en: The adaptive bagging algorithm performs better than bagging because it employs
    Hoeffding Adaptive Trees in the implementation, which are more robust than basic
    online stream bagging.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应袋装算法比袋装算法表现更好，因为它在实现中采用了Hoeffding自适应树，这些树比基本的在线流袋装更稳健。
- en: The online boosting algorithm with its dependency on the weak learners and no
    adaptability ranked the lowest as expected.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如预期的那样，在线提升算法由于其依赖弱学习者和缺乏适应性而排名最低。
- en: The bagging plot in *Figure 25* shows a nice trend of stability achieved as
    the number of examples increased, validating the general consensus that if the
    patterns are stationary, more examples lead to robust models.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*图25*中的袋装图显示了随着示例数量的增加所实现的稳定趋势，这验证了普遍共识，即如果模式是平稳的，更多的示例会导致稳健的模型。'
- en: The future of Machine Learning
  id: totrans-356
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习的未来
- en: The impact of Machine Learning on businesses, social interactions, and indeed,
    our day-to-day lives today is undeniable, though not always immediately obvious.
    In the near future, it will be ubiquitous and inescapable. According to a report
    by McKinsey Global Institute published in December 2016 (*References* [15]), there
    is a vast unexploited potential for data and analytics in major industry sectors,
    especially healthcare and the public sector. Machine Learning is one of the key
    technologies poised to help exploit that potential. More compute power is at our
    disposal than ever before. More data is available than ever before, and we have
    cheaper and greater storage capacity than ever before.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习对商业、社会互动，以及我们日常生活的冲击是无可否认的，尽管这种影响并不总是立即显而易见。在不久的将来，它将无处不在，无法避免。根据麦肯锡全球研究院2016年12月发布的一份报告（*参考文献*
    [15]），在主要行业部门，尤其是在医疗保健和公共部门，数据和分析存在着巨大的未开发潜力。机器学习是帮助利用这种潜力的关键技术之一。我们现在可用的计算能力比以往任何时候都要多。可用的数据也比以往任何时候都要多，而且我们拥有比以往任何时候都要便宜和更大的存储容量。
- en: Already, the unmet demand for data scientists has spurred changes to college
    curricula worldwide, and has caused an increase of 16% a year in wages for data
    scientists in the US, in the period 2012-2014\. The solution to a wide swathe
    of problems is within reach with Machine Learning, including resource allocation,
    forecasting, predictive analytics, predictive maintenance, and price and product
    optimization.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 已经，对数据科学家的未满足需求已经促使全球大学课程发生了变化，并在2012-2014年期间导致美国数据科学家的工资每年增长16%。机器学习可以解决广泛的问题，包括资源分配、预测、预测分析、预测维护以及价格和产品优化。
- en: 'The same McKinsey report emphasizes the increasing role of Machine Learning,
    including deep learning in a variety of use cases across industries such as agriculture,
    pharma, manufacturing, energy, media, and finance. These scenarios run the gamut:
    predict personalized health outcomes, identify fraud transactions, optimize pricing
    and scheduling, personalize crops to individual conditions, identify and navigate
    roads, diagnose disease, and personalize advertising. Deep learning has great
    potential in automating an increasing number of occupations. Just improving natural
    language understanding would potentially cause a USD 3 trillion impact on global
    wages, affecting jobs like customer service and support worldwide.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 同一份麦肯锡报告强调了机器学习在包括农业、制药、制造、能源、媒体和金融等行业各种用例中的日益增长的作用，包括深度学习。这些场景涵盖了从预测个性化健康结果、识别欺诈交易、优化定价和调度、根据个体条件个性化作物、识别和导航道路、诊断疾病到个性化广告的各个方面。深度学习在自动化越来越多的职业方面具有巨大潜力。仅仅改善自然语言理解就可能导致全球工资产生3万亿美元的潜在影响，影响全球的客户服务和支持类工作。
- en: Giant strides in image and voice recognition and language processing have made
    applications such as personal digital assistants commonplace, thanks to remarkable
    advances in deep learning techniques. The symbolism of AlphaGO's success in defeating
    Lee Sedol, alluded to in the opening chapter of this book, is enormous, as it
    is a vivid example of how progress in artificial intelligence is besting our own
    predictions of milestones in AI advancement. Yet this is the tip of the proverbial
    iceberg. Recent work in areas such as transfer learning offers the promise of
    more broadly intelligent systems that will be able to solve a wider range of problems
    rather than narrowly specializing in just one. General Artificial Intelligence,
    where AI can develop objective reasoning, proposes a methodology to solve a problem,
    and learn from its mistakes, is some distance away at this point, but check back
    in a few years and that distance may well have shrunk beyond our current expectations!
    Increasingly, the confluence of transformative advances in technologies incrementally
    enabling each other spells a future of dizzying possibilities that we can already
    glimpse around us. The role of Machine Learning, it would appear, is to continue
    to shape that future in profound and extraordinary ways. Of that, there is little
    doubt.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图像和语音识别以及语言处理方面的巨大进步，得益于深度学习技术的显著进步，使得个人数字助理等应用变得司空见惯。本书开篇章节提到的AlphaGO战胜李世石的成功象征意义巨大，它是人工智能进步超越我们预测里程碑的一个生动例证。然而，这只是冰山一角。最近在迁移学习等领域的研究为更广泛智能系统的承诺提供了希望，这些系统能够解决更广泛的问题，而不仅仅是专门解决一个问题。通用人工智能，即AI能够发展客观推理、提出解决问题的方法并从其错误中学习，目前还相距甚远，但几年后，这个距离可能会缩小到我们目前的预期之外！日益增长的、相互促进的技术变革正在预示着一个令人眼花缭乱的未来，我们已经在周围看到了这种可能性。看起来，机器学习的角色将继续以前所未有的方式塑造那个未来。对此，我们几乎毫无疑问。
- en: Summary
  id: totrans-361
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The final chapter of this book deals with Machine Learning adapted to what is
    arguably one of the most significant paradigm shifts in information management
    and analytics to have emerged in the last few decades—Big Data. Much as many other
    areas of computer science and engineering have seen, AI—and Machine Learning in
    particular—has benefited from innovative solutions and dedicated communities adapting
    to face the many challenges posed by Big Data.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 本书最后一章讨论了适应信息管理和分析领域在过去几十年中出现的最显著范式转变之一——大数据的机器学习。正如计算机科学和工程的其他许多领域所看到的那样，人工智能——特别是机器学习——从创新解决方案和适应大数据带来的众多挑战的专门社区中受益。
- en: One way to characterize Big Data is by volume, velocity, variety, and veracity.
    This demands a new set of tools and frameworks to conduct the tasks of effective
    analytics at large.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 一种描述大数据的方式是通过其体积、速度、多样性和真实性。这要求有一套新的工具和框架来执行大规模有效分析的任务。
- en: Choosing a Big Data framework involves selecting distributed storage systems,
    data preparation techniques, batch or real-time Machine Learning, as well as visualization
    and reporting tools.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 选择大数据框架涉及选择分布式存储系统、数据准备技术、批量或实时机器学习，以及可视化和报告工具。
- en: Several open source deployment frameworks are available including Hortonworks
    Data Platform, Cloudera CDH, Amazon Elastic MapReduce, and Microsoft Azure HDInsight.
    Each provides a platform with components supporting data acquisition, data preparation,
    Machine Learning, evaluation, and visualization of results.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 可用的开源部署框架包括Hortonworks数据平台、Cloudera CDH、Amazon Elastic MapReduce和Microsoft Azure
    HDInsight。每个都提供了一个平台，其中包含支持数据采集、数据准备、机器学习、评估和结果可视化的组件。
- en: Among the data acquisition components, publish-subscribe is a model offered
    by Apache Kafka (*References* [8]) and Amazon Kinesis, which involves a broker
    mediating between subscribers and publishers. Alternatives include source-sink,
    SQL, message queueing, and other custom frameworks.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据采集组件中，发布-订阅是Apache Kafka (*参考文献[8]*) 和Amazon Kinesis提供的一种模型，它涉及一个在订阅者和发布者之间进行调解的代理。其他选择包括源-汇，SQL，消息队列以及其他自定义框架。
- en: With regard to data storage, several factors contribute to the proper choice
    for whatever your needs may be. HDFS offers a distributed File System with robust
    fault tolerance and high throughput. NoSQL databases also offer high throughput,
    but generally with weak guarantees on consistency. They include key-value, document,
    columnar, and graph databases.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据存储，有几个因素有助于为满足您的需求做出适当的选择。HDFS提供了一个具有强大容错性和高吞吐量的分布式文件系统。NoSQL数据库也提供高吞吐量，但通常在一致性方面提供较弱保证。它们包括键值、文档、列和图数据库。
- en: Data processing and preparation come next in the flow, which includes data cleaning,
    scraping, and transformation. Hive and HQL provide these functions in HDFS systems.
    SparkSQL and Amazon Redshift offer similar capabilities. Real-time stream processing
    is available from products such as Storm and Samza.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理和准备是流程中的下一步，包括数据清洗、抓取和转换。Hive和HQL在HDFS系统中提供这些功能。SparkSQL和Amazon Redshift提供类似的功能。实时流处理可通过Storm和Samza等产品获得。
- en: The learning stage in Big Data analytics can include batch or real-time data.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据分析的学习阶段，可以包括批量或实时数据。
- en: A variety of rich visualization and analysis frameworks exist that are accessible
    from multiple programming environments.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着多种丰富的可视化和分析框架，可以从多个编程环境中访问。
- en: Two major Machine Learning frameworks on Big Data are H2O and Apache Spark MLlib.
    Both can access data from various sources such as HDFS, SQL, NoSQL, S3, and others.
    H2O supports a number of Machine Learning algorithms that can be run in a cluster.
    For Machine Learning with real-time data, SAMOA is a big data framework with a
    comprehensive set of stream-processing capabilities.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据领域，有两个主要的机器学习框架是H2O和Apache Spark MLlib。两者都可以从各种来源访问数据，如HDFS、SQL、NoSQL、S3等。H2O支持多种机器学习算法，可以在集群中运行。对于实时数据的机器学习，SAMOA是一个具有全面流处理能力的的大数据框架。
- en: The role of Machine Learning in the future is going to be a dominant one, with
    a wide-ranging impact on healthcare, finance, energy, and indeed on most industries.
    The expansion in the scope of automation will have inevitable societal effects.
    Increases in compute power, data, and storage per dollar are opening up great
    new vistas to Machine Learning applications that have the potential to increase
    productivity, engender innovation, and dramatically improve living standards the
    world over.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在未来将扮演主导角色，对医疗保健、金融、能源以及实际上大多数行业都将产生广泛的影响。自动化范围的扩展将不可避免地对社会产生影响。计算能力、数据和存储成本的增加为机器学习应用开辟了新的广阔视野，这些应用有可能提高生产力、激发创新并极大地改善全球的生活水平。
- en: References
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, *Ion
    Stoica:Spark: Cluster Computing with Working Sets*. HotCloud 2010'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Matei Zaharia, Mosharaf Chowdhury, Michael J. Franklin, Scott Shenker, *Ion
    Stoica:Spark:使用工作集的集群计算*. HotCloud 2010
- en: 'Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
    Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin,
    Ali Ghodsi, Joseph Gonzalez, Scott Shenker, *Ion Stoica:Apache Spark: a unified
    engine for Big Data processing*. Commun. ACM 59(11): 56-65 (2016)'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Matei Zaharia, Reynold S. Xin, Patrick Wendell, Tathagata Das, Michael Armbrust,
    Ankur Dave, Xiangrui Meng, Josh Rosen, Shivaram Venkataraman, Michael J. Franklin,
    Ali Ghodsi, Joseph Gonzalez, Scott Shenker, *Ion Stoica:Apache Spark:一个用于大数据处理的统一引擎*.
    Commun. ACM 59(11): 56-65 (2016)'
- en: 'Apache Hadoop: [https://hadoop.apache.org/](https://hadoop.apache.org/).'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Hadoop: [https://hadoop.apache.org/](https://hadoop.apache.org/).'
- en: 'Cloudera: [http://www.cloudera.com/](http://www.cloudera.com/).'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Cloudera: [http://www.cloudera.com/](http://www.cloudera.com/).'
- en: 'Hortonworks: [http://hortonworks.com/](http://hortonworks.com/).'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Hortonworks: [http://hortonworks.com/](http://hortonworks.com/).'
- en: 'Amazon EC2: [http://aws.amazon.com/ec2/](http://aws.amazon.com/ec2/).'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Amazon EC2: [http://aws.amazon.com/ec2/](http://aws.amazon.com/ec2/).'
- en: 'Microsoft Azure: [http://azure.microsoft.com/](http://azure.microsoft.com/).'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Microsoft Azure: [http://azure.microsoft.com/](http://azure.microsoft.com/).'
- en: 'Apache Flume: [https://flume.apache.org/](https://flume.apache.org/).'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Flume: [https://flume.apache.org/](https://flume.apache.org/).'
- en: 'Apache Kafka: [http://kafka.apache.org/](http://kafka.apache.org/).'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Kafka: [http://kafka.apache.org/](http://kafka.apache.org/).'
- en: 'Apache Sqoop: [http://sqoop.apache.org/](http://sqoop.apache.org/).'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Sqoop: [http://sqoop.apache.org/](http://sqoop.apache.org/).'
- en: 'Apache Hive: [http://hive.apache.org/](http://hive.apache.org/).'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Hive: [http://hive.apache.org/](http://hive.apache.org/).'
- en: 'Apache Storm: [https://storm.apache.org/](https://storm.apache.org/).'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Apache Storm: [https://storm.apache.org/](https://storm.apache.org/).'
- en: 'H2O: [http://h2o.ai/](http://h2o.ai/).'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'H2O: [http://h2o.ai/](http://h2o.ai/).'
- en: 'Shahrivari S, Jalili S. *Beyond batch processing: towards real-time and streaming
    Big Data*. Computers. 2014;3(4):117–29.'
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Shahrivari S, Jalili S. *超越批量处理：迈向实时和流式大数据*. 计算机. 2014;3(4):117–29.
- en: '*MGI, The Age of Analytics*–—Executive Summary [http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx](http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx).'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*MGI, 分析时代*——执行摘要 [http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx](http://www.mckinsey.com/~/media/McKinsey/Business%20Functions/McKinsey%20Analytics/Our%20Insights/The%20age%20of%20analytics%20Competing%20in%20a%20data%20driven%20world/MGI-The-Age-of-Analytics-Full-report.ashx).'
