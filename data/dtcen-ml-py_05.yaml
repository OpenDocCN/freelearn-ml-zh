- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Techniques for Data Cleaning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据清洗技术
- en: In this chapter, we will cover six key dimensions of data quality and their
    corresponding techniques to improve data quality, commonly known as techniques
    for cleaning data in machine learning. Simply put, data cleaning is the process
    of implementing techniques to improve data quality by fixing errors in data or
    removing erroneous data. As covered in *Chapters 1* and *2*, reducing errors in
    data is a highly efficient and effective way to improve model quality over using
    model-centric techniques such as adding more data and/or implementing complex
    algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍数据质量的六个关键维度及其提高数据质量的相关技术，这些技术通常被称为机器学习中的数据清洗技术。简单来说，数据清洗是通过实施技术来修复数据中的错误或删除错误数据，以提高数据质量的过程。正如第1章和第2章所涵盖的，减少数据中的错误是提高模型质量的一种非常高效和有效的方法，比使用以模型为中心的技术，如添加更多数据或实施复杂算法，更为有效。
- en: At a high level, data cleaning techniques involve fixing or removing incorrect,
    incomplete, invalid, biased, inconsistent, stale, or corrupted data. As data is
    captured at multiple sources, due to different annotators following their judgment
    or due to poor system designs, combining these sources can often result in data
    being mislabeled, inconsistent, duplicated, or incomplete. As discovered in earlier
    chapters, incorrect data can make algorithms and outcomes unreliable. So, to achieve
    reliability in machine learning systems, it is important to help data scientists
    and data engineers question the data and systematically improve data quality using
    data cleaning techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，数据清洗技术包括修复或删除不正确、不完整、无效、有偏见、不一致、过时或损坏的数据。由于数据是从多个来源捕获的，由于不同的注释者根据他们的判断或由于系统设计不良，将这些来源结合起来通常会导致数据被错误标记、不一致、重复或不完整。正如前几章所发现的，错误的数据会使算法和结果不可靠。因此，为了在机器学习系统中实现可靠性，重要的是帮助数据科学家和数据工程师质疑数据，并使用数据清洗技术系统地提高数据质量。
- en: 'The topics that will be covered in this chapter are as follows:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The six key dimensions of data quality
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据质量的六个关键维度
- en: Measuring data quality
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量数据质量
- en: Data cleaning techniques required to improve data quality across the six dimensions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高数据质量所需的六个维度的数据清洗技术
- en: The six key dimensions of data quality
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据质量的六个关键维度
- en: 'There are six key dimensions we can use to check the overall health of data.
    Ensuring good health across the data can ensure we can build reliable systems
    and make better decisions. For example, if 20% of survey data is duplicated, and
    the majority of the duplicates are filled by male candidates, we can imagine that
    the actions taken by decision-makers will favor the male candidates if data duplication
    is undetected. Hence, it’s important to understand the overall health of the data
    to make reliable and unbiased decisions. To measure data quality or look at the
    overall health of the data, we can break down data quality into the following
    dimensions:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用六个关键维度来检查数据的整体健康状况。确保数据整体健康可以确保我们能够构建可靠的系统并做出更好的决策。例如，如果20%的调查数据是重复的，并且大多数重复数据是由男性候选人填写的，我们可以想象，如果数据重复未被检测到，决策者的行动将有利于男性候选人。因此，了解数据的整体健康状况对于做出可靠且无偏见的决策非常重要。为了衡量数据质量或查看数据的整体健康状况，我们可以将数据质量分解为以下维度：
- en: '**Consistency**: This refers to whether the same data is maintained across
    the rows for a given column or feature. An example of this could be whether the
    gender label for males is consistent or not. The label can take values of “1,”
    “Male,” “M”, or “male,” but if the data has multiple values to represent males,
    then an algorithm will treat each label individually, which can cause randomness
    and errors. Hence, the goal of ensuring consistency is to ensure labels are defined
    consistently across the whole dataset.'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一致性**：这指的是对于给定的列或特征，同一数据是否在行之间保持一致。一个例子可能是男性性别标签是否一致。标签可以取“1”、“男性”、“M”或“male”等值，但如果数据有多个值来表示男性，那么算法将单独处理每个标签，这可能导致随机性和错误。因此，确保一致性的目标是确保标签在整个数据集中定义一致。'
- en: '**Uniqueness**: This refers to whether each record can be uniquely identified.
    If duplicate values enter the system, the model will become biased due to those
    records. For example, if one region has a high loan approval rate and due to system
    failure, records from this region are duplicated, the algorithm will be biased
    toward approving more loans from that region.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**唯一性**：这指的是每条记录是否可以唯一识别。如果系统中有重复值，模型将因为那些记录而变得有偏见。例如，如果一个地区有高贷款批准率，由于系统故障，该地区的记录被重复，算法将偏向于批准更多该地区的贷款。'
- en: '**Completeness**: This refers to whether data is complete across the rows for
    a given column or feature, and whether data is missing due to system errors or
    not captured, especially when that information will be used in the machine learning
    system.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：这指的是给定列或特征的数据在行之间是否完整，以及数据是否由于系统错误或未捕获而缺失，尤其是在该信息将被用于机器学习系统时。'
- en: '`semi_urban` might be invalid if one or a couple of annotators believed some
    suburbs are neither urban nor rural, and they violated the rules of data and entered
    `semi_urban`. This can introduce noise in the data, so it’s important to ensure
    data conforms to the business rules.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个或几个标注者认为某些郊区既不是城市也不是乡村，并违反了数据规则输入了`semi_urban`，那么`semi_urban`可能无效。这可能会在数据中引入噪声，因此确保数据符合业务规则非常重要。
- en: '**Accuracy**: This refers to whether data was entered correctly in the first
    place and can be verified from an internal or external source. An example of this
    in a healthcare setting could be that if the date and time of admission are entered
    in a different time zone, then the analysis and insights would be inaccurate.
    If the time of admission is a predictor of quality of care, then misaligned time
    zones might lead to wrong conclusions.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确性**：这指的是数据最初是否被正确输入，并且可以从内部或外部来源进行验证。在医疗保健环境中，一个例子可能是如果入院日期和时间输入了不同的时区，那么分析和洞察将是不准确的。如果入院时间是护理质量的预测因子，那么时区的不匹配可能会导致错误的结论。'
- en: '**Freshness**: This refers to whether the data available is recent and up to
    date to meet data requirements. Some applications require data to be real-time
    – that is, updated every second – whereas other applications require data to be
    available once a month or every few months. What was true yesterday may change
    due to changes in factors such as regulation, weather conditions, trends, competition,
    business changes, and more.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新鲜度**：这指的是可用的数据是否是最近的并且是最新的，以满足数据需求。某些应用程序需要数据是实时的——也就是说，每秒更新一次——而其他应用程序需要数据每月或每几个月可用一次。昨天的事实可能因为法规、天气条件、趋势、竞争、业务变化等因素的变化而改变。'
- en: Next, we will install various Python packages, and then dive into fixing and
    measuring data quality. In each section, we dive deeper into different data cleaning
    techniques and how to improve the quality of data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将安装各种 Python 包，然后深入到数据修复和质量测量的工作。在每个部分，我们将深入探讨不同的数据清洗技术以及如何提高数据质量。
- en: Installing the required packages
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装所需的包
- en: 'For this chapter, we will need the following Python packages or libraries:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们需要以下 Python 包或库：
- en: '`pandas` version 1.5.3'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas` 版本 1.5.3'
- en: '`numpy` version 1.22.4'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` 版本 1.22.4'
- en: '`scikit-learn` version 1.2.1'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`scikit-learn` 版本 1.2.1'
- en: '`jupyter` version 1.0.0'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jupyter` 版本 1.0.0'
- en: '`alibi` version 0.9.0'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alibi` 版本 0.9.0'
- en: '`alibi-detect` version 0.10.4'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alibi-detect` 版本 0.10.4'
- en: '`seaborn` version 0.12.2'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seaborn` 版本 0.12.2'
- en: '`matplotlib` version 3.6.3'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` 版本 3.6.3'
- en: '`missingno` version 0.5.1'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`missingno` 版本 0.5.1'
- en: '`feature-engine` version 1.5.2'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature-engine` 版本 1.5.2'
- en: Next, we will provide a brief introduction to the dataset and start exploring
    the data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将简要介绍数据集并开始探索数据。
- en: Introducing the dataset
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍数据集
- en: First, let’s introduce our problem statement. For loan providers, it is important
    to ensure that people who get a loan can make payment and don’t default. However,
    it is equally important that people are not denied a loan due to a model trained
    on poor-quality data. This is where the data-centric approach helps make the world
    a better place – it provides a framework for data scientists and data engineers
    to question the quality of data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们介绍我们的问题陈述。对于贷款提供者来说，确保获得贷款的人能够还款并且不会违约是很重要的。然而，同样重要的是，人们不应因为基于低质量数据训练的模型而被拒绝贷款。这就是以数据为中心的方法帮助使世界变得更美好的地方——它为数据科学家和数据工程师提供了一个质疑数据质量的框架。
- en: For this chapter, we will use the loan prediction dataset from Analytics Vidhya.
    You can download the dataset from [https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement).
    There are two files – one for training and one for testing. The test file doesn’t
    contain any labels. For this chapter, we will utilize the training file, which
    has been downloaded and saved as `train_loan_prediction.csv`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用 Analytics Vidhya 的贷款预测数据集。您可以从 [https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement](https://datahack.analyticsvidhya.com/contest/practice-problem-loan-prediction-iii/#ProblemStatement)
    下载数据集。有两个文件——一个用于训练，一个用于测试。测试文件不包含任何标签。对于本章，我们将利用训练文件，该文件已下载并保存为 `train_loan_prediction.csv`。
- en: 'First, we will look at the dataset and check the first five rows. To do this,
    we must import the following necessary packages:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看数据集并检查前五行。为此，我们必须导入以下必要的包：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will read the data:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将读取数据：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We will read the data using pandas’ `read_csv` method. Then, we will visualize
    the first five rows of the dataset using `.head()`. We can apply the `.T` method
    at the end of the `.head()` method if we have a large feature set. This will represent
    columns as rows and rows as columns, where column names will not exceed 5 as we
    want to visualize the first five rows.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 pandas 的 `read_csv` 方法读取数据。然后，我们将使用 `.head()` 方法可视化数据集的前五行。如果我们有一个大的特征集，我们可以在
    `.head()` 方法的末尾应用 `.T` 方法。这将表示列作为行，行作为列，其中列名不会超过 5 个，因为我们想可视化前五行。
- en: 'We get the following output:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '![Figure 5.1 – The first five rows of our df dataset](img/B19297_05_1.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 我们 df 数据集的前五行](img/B19297_05_1.jpg)'
- en: Figure 5.1 – The first five rows of our df dataset
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 我们 df 数据集的前五行
- en: As we can see, there are some inconsistencies across column names. All the columns
    follow the camel case convention, except `Loan_ID`, and long column names are
    separated by `_` except for `LoanAmount`, `CoapplicantIncome`, and `ApplicantIncome`.
    This indicates that the column names have inconsistent naming conventions. Within
    the data, we can also see that some columns have data in camel case but `Loan_ID`
    has all its values in uppercase. Within the `Education` column, the `Not Graduate`
    value is separated by a space. In machine learning, it’s important to ensure data
    is consistent; otherwise, models may produce inconsistent results. For instance,
    what happens if the `Gender` column has two distinct values for male customers
    – `Male` and `male`? If we don’t treat this, then our machine learning model will
    consider `male` data points separate from `Male`, and the model will not get an
    accurate signal.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，列名之间存在一些不一致。所有列都遵循驼峰命名法，除了 `Loan_ID`，而长列名由 `_` 分隔，除了 `LoanAmount`、`CoapplicantIncome`
    和 `ApplicantIncome`。这表明列名存在不一致的命名约定。在数据中，我们还可以看到一些列具有驼峰命名法的数据，但 `Loan_ID` 的所有值都是大写。在
    `Education` 列中，`Not Graduate` 值由空格分隔。在机器学习中，确保数据的一致性非常重要；否则，模型可能会产生不一致的结果。例如，如果
    `Gender` 列有两个不同的男性客户值——`Male` 和 `male`，会发生什么？如果我们不处理这个问题，那么我们的机器学习模型将把 `male`
    数据点视为与 `Male` 分离的，模型将不会得到准确的信号。
- en: Next, we will extract the list of column names from the data, make them all
    lowercase, and ensure words will be separated by a unique character, `_`. We will
    also go through the data values of categorical columns and make them all lowercase
    before replacing all the special characters with `_` and making our data consistent.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从数据中提取列名列表，将它们全部转换为小写，并确保单词之间由一个独特的字符 `_` 分隔。我们还将遍历分类列的数据值，在将所有特殊字符替换为
    `_` 并使我们的数据一致之前，将它们全部转换为小写。
- en: Ensuring the data is consistent
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保数据的一致性
- en: 'To ensure the data is consistent, we must check the names of the columns in
    the DataFrame:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据的一致性，我们必须检查 DataFrame 中列的名称：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we must get all the column names that don’t contain underscores:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须获取所有不包含下划线的列名：
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Since some columns have two uppercase letters in their names, we must add the
    underscore before the start of the second uppercase letter. Next, we create a
    Boolean mapping against the index of each letter of the column, where the location
    of capital letters will be mapped as `True` so that we can locate the index of
    the second capital letter and prefix it with an underscore:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于一些列的名称中有两个大写字母，我们必须在第二个大写字母之前添加下划线。接下来，我们创建一个布尔映射，针对每个列字母的索引，将大写字母的位置映射为`True`，这样我们就可以定位第二个大写字母的位置，并在其前面加上下划线：
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Then, we iterate over the mappings and print the column names that will require
    an underscore, and also print the location of the second capital letter:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们遍历映射并打印需要下划线的列名，以及打印第二个大写字母的位置：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Using this information, we build some logic for the `ApplicantIncome` column:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些信息，我们为`ApplicantIncome`列构建了一些逻辑：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we combine the previous steps and iterate over columns that require an
    underscore and build a mapping. Then, we print the names of the columns that require
    underscores. Finally, we create a mapping of old column names and new column names:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将前面的步骤结合起来，遍历需要下划线的列，构建映射。然后，我们打印需要下划线的列名。最后，我们创建旧列名和新列名的映射：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Finally, we apply the column mappings to update the column names and print
    the new column names:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将列映射应用于更新列名并打印新的列名：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Although the preceding code could have simply been updated by manually choosing
    the columns, by doing this programmatically, we can ensure that data is following
    programmatic rules. To improve consistency, we make all column names lowercase.
    First, we create some simple one-line logic to convert column names into lowercase:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的代码可以通过手动选择列来简单地更新，但通过这种方式编程化，我们可以确保数据遵循编程规则。为了提高一致性，我们将所有列名转换为小写。首先，我们创建一些简单的单行逻辑来将列名转换为小写：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then, we update the column names by passing the preceding logic:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过传递前面的逻辑来更新列名：
- en: '[PRE10]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'With that, we have ensured that the column names are consistent. But more importantly,
    we must ensure that categorical values are consistent so that we can future-proof
    machine learning systems from inconsistent data. First, we extract the `id` and
    `target` columns and then identify the categorical columns. These columns contain
    non-numerical data:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们已经确保了列名的一致性。但更重要的是，我们必须确保分类值的一致性，以便我们可以使机器学习系统免受不一致数据的影响。首先，我们提取`id`和`target`列，然后识别分类列。这些列包含非数值数据：
- en: '[PRE11]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We iterate over each column and check the unique values to ensure the values
    are distinct and not misspelled. We also check that the same value is not represented
    differently, such as it being in a different case or being abbreviated:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遍历每个列，检查唯一值以确保值是独特的且没有拼写错误。我们还检查相同的值是否以不同的方式表示，例如以不同的形式或缩写：
- en: '[PRE12]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Looking at the data, it seems that values are distinct and not abbreviated
    or misspelled. But machine learning systems can be made future-proof. For instance,
    if we make all values lowercase, then in the future, if the same value comes with
    a different case, before entering the system, it will be made lowercase. We can
    also see that some strings, such as `Not Graduate`, take up space. Just like how
    we ensured consistency for column names, we must replace all white spaces with
    underscores. First, we create a new DataFrame named `df_consistent`; then, we
    make all categorical values lowercase and replace all spaces with underscores:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据，似乎值是独特的，没有缩写或拼写错误。但机器学习系统可以被设计成面向未来。例如，如果我们将所有值都转换为小写，那么在未来，如果相同的值以不同的形式出现，在进入系统之前，它将被转换为小写。我们还可以看到，一些字符串，如`Not
    Graduate`，占用空间。就像我们确保列名的一致性一样，我们必须将所有空白替换为下划线。首先，我们创建一个新的DataFrame名为`df_consistent`；然后，我们将所有分类值转换为小写，并将所有空格替换为下划线：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: With that, we have ensured that data is consistent and all values are converted
    into lowercase.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，我们确保了数据的一致性，并且所有值都被转换为小写。
- en: 'As we can see, the `dependents` column contains numerical information. However,
    due to the presence of the `3+` value, the column values are encoded as strings.
    We must remove the special character and then encode this back to a numerical
    value since this column is ordinal:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`dependents`列包含数值信息。然而，由于存在`3+`这样的值，列值被编码为字符串。我们必须移除特殊字符，然后将此值编码回数值，因为该列是序数的：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we look at the `married` and `self_employed` columns since these are
    binary and must be encoded to `1` and `0`. The `gender` column has two values
    and can be binary encoded as well – for example, we can encode `male` as `1` and
    `female` as `0`. The `education` column also has two values, and we can encode
    `graduate` as `1` and `not_graduate` as `0`:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们查看`已婚`和`自雇`列，因为这些是二进制列，必须编码为`1`和`0`。`性别`列有两个值，也可以进行二进制编码——例如，我们可以将`男性`编码为`1`，将`女性`编码为`0`。`教育`列也有两个值，我们可以将`毕业生`编码为`1`，将`非毕业生`编码为`0`：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now that the data is consistent and has been encoded correctly, we must create
    a function for preprocessing data so that we can consistently process any future
    variations to categorical labels. Then, we apply the function to the DataFrame
    and print the values to ensure the function was applied correctly:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经一致并且正确编码，我们必须创建一个预处理数据的函数，以便我们可以一致地处理任何未来的分类标签变化。然后，我们将该函数应用于DataFrame，并打印值以确保函数被正确应用：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We have now ensured that the data is consistent so that if categorical values
    have spaces instead of `_` or are entered with a different case in the future,
    we can use the functionality we created here to clean the data and make it consistent
    before it enters our model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确保了数据的一致性，因此如果分类值在未来用空格代替`_`或以不同的案例输入，我们可以使用我们在这里创建的功能来清理数据并使其一致，在它进入我们的模型之前：
- en: Next, we will explore data uniqueness to ensure no duplicate records have been
    provided to create bias in the data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探索数据唯一性，以确保没有提供重复记录以在数据中造成偏差。
- en: Checking that the data is unique
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查数据是否唯一
- en: Now that we have ensured the data is consistent, we must also ensure it's unique,
    before it enters the machine learning system.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确保了数据的一致性，我们必须在它进入机器学习系统之前确保它的唯一性。
- en: In this section, we will investigate the data and check whether the values in
    the `loan_id` column are unique, as well as whether a combination of certain columns
    can ensure data is unique.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将调查数据并检查`loan_id`列中的值是否唯一，以及某些列的组合是否可以确保数据的唯一性。
- en: 'In pandas, we can utilize the `.nunique()` method to check the number of unique
    records for the column and compare it with the number of rows. First, we will
    check that `loan_id` is unique and that no duplicate applications have been entered:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在pandas中，我们可以利用`.nunique()`方法来检查列的唯一记录数，并将其与行数进行比较。首先，我们将检查`loan_id`是否唯一，以及是否没有输入重复的申请：
- en: '[PRE17]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'With this, we have ensured that loan IDs are unique. However, we can go one
    step further to ensure that incorrect data is not added to another loan application.
    We believe it’s quite unlikely that a loan application will require more than
    one combination of income and loan amount. We must check that we can use a combination
    of column values to ensure uniqueness across those columns:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经确保了贷款ID的唯一性。然而，我们可以更进一步，确保不会将错误数据添加到另一个贷款申请中。我们相信贷款申请不太可能需要超过一种收入和贷款金额的组合。我们必须检查我们是否可以使用列值的组合来确保这些列之间的唯一性：
- en: '[PRE18]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'As we can see, in the first row, there are two applications with the same income
    variables and loan amount. Let’s filter the dataset to find these believed-to-be
    duplicate records by using values from the first row:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在第一行中，有两个具有相同收入变量和贷款金额的申请。让我们通过使用第一行的值来过滤数据集，以找到这些被认为是重复的记录：
- en: '[PRE19]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Looking at this subset, it is quite obvious that the data contains duplicates
    or two different applications were made – one by the husband and another one by
    the wife. This data is not providing any more information, other than that one
    application has been made by a male candidate and another has been made by a female
    candidate. We could drop one of the data points, but there is a big imbalance
    in the ratio of male to female applications. Also, if the second one was a genuine
    application, then we should keep the data point:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 观察这个子集，很明显数据包含重复项或有两个不同的申请——一个是丈夫做的，另一个是妻子做的。这些数据除了表明一个男性候选人提交了一个申请，另一个女性候选人提交了一个申请之外，没有提供更多信息。我们可以删除其中一个数据点，但男性和女性申请的比例不平衡。此外，如果第二个申请是真实的，那么我们应该保留这个数据点：
- en: '[PRE20]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Based on this, we have understood what makes a data point unique – that is,
    a combination of `gender`, `applicant_income`, `coapplicant_income`, and `loan_amount`.
    It’s our goal, as data scientists and data engineers, to ensure that once uniqueness
    rules have been defined, data coming into the machine learning system conforms
    to those uniqueness checks.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 基于此，我们已经理解了什么使数据点独特——那就是`性别`、`申请人收入`、`共同申请人收入`和`贷款金额`的组合。作为数据科学家和数据工程师，我们的目标是确保一旦定义了唯一性规则，进入机器学习系统的数据都符合这些唯一性检查。
- en: In the next section, we will discuss data completeness or issues with incomplete
    data, and how to handle incomplete data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论数据完整性或数据不完整的问题，以及如何处理不完整的数据。
- en: Ensuring that the data is complete and not missing
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保数据完整且无缺失
- en: 'Now that we have achieved data consistency and uniqueness, it’s time to identify
    and address other quality issues. One such issue is missing information in the
    data or incomplete data. Missing data is a common problem with real datasets.
    As a dataset’s size increases, the chance of data points going missing in the
    data increases. Missing records can occur in several ways, some of which include:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经实现了数据的一致性和唯一性，是时候识别和解决其他质量问题了。其中一个问题是数据中的缺失信息或不完整数据。缺失数据是真实数据集中常见的问题。随着数据集大小的增加，数据点在数据中缺失的可能性也会增加。缺失记录可以以多种方式发生，其中包括：
- en: '**Merging of source datasets**: For example, when we try to match records against
    date of birth or a postcode to enrich data, and either of these is missing in
    one dataset or is inaccurate, such occurrences will take NA values.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**源数据集的合并**：例如，当我们尝试根据出生日期或邮编匹配记录以丰富数据时，如果这些信息在一个数据集中缺失或不准确，那么这种情况将产生NA值。'
- en: '**Random events**: This is quite common in surveys, where the person may not
    be aware of whether the information required is compulsory or they may not know
    the answer.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机事件**：这在调查中很常见，其中受访者可能不知道所需信息是否为必填项，或者他们可能不知道答案。'
- en: '**Failures of measurement**: For example, some traits, such as blood pressure,
    are known to have a very substantial component of random error when measured in
    the conventional way (that is, with a blood pressure cuff). If two people measure
    a subject’s blood pressure at almost the same time, or if one person measures
    a subject’s blood pressure twice in rapid succession, the measured values can
    easily differ by 10 mm/Hg (https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/measurement/).
    If a person is aware of these errors, they may decide to omit this information;
    for some patients, this data will take NA values. In finance, an important measurement
    ratio to determine the credit worthiness of someone or a firm is the debt-to-income
    ratio. There are scenarios when income is not declared, and in those circumstances,
    dividing debt by 0 or missing data would result in missing information for the
    ratio.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测量失败**：例如，一些特征，如血压，在传统方式（即使用血压计）测量时，已知具有很大的随机误差成分。如果两个人几乎同时测量一个受试者的血压，或者一个人在短时间内两次快速测量一个受试者的血压，测量的值可以很容易地相差10毫米汞柱（https://dept.stat.lsa.umich.edu/~kshedden/introds/topics/measurement/）。如果一个人意识到这些误差，他们可能会决定省略这些信息；对于一些患者，这些数据将产生NA值。在金融领域，一个重要的测量比率是确定个人或公司的信用价值，即债务收入比。在某些情况下，收入未申报，在这种情况下，将债务除以0或缺失数据会导致比率缺失信息。'
- en: '**Poor process design around collecting data**: For example, in health surveys,
    people are often asked about their BMI, and not everyone knows their BMI or understands
    the measurement. It would be simpler and easier if we were to ask for someone’s
    height and weight as they are more likely to know this. Another problem arises
    when someone is asked about their weight measurement, where some people might
    be likely to omit or lie about this information. If BMI cannot be understood or
    measured at the time of collecting data, the data will take NA values.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据收集过程中的设计不当**：例如，在健康调查中，人们经常被问及他们的BMI，并不是每个人都了解自己的BMI或理解测量方法。如果我们要求提供身高和体重会简单得多，因为人们更有可能知道这些信息。当有人被问及他们的体重测量时，有些人可能会省略或谎报这些信息。如果在收集数据时无法理解或测量BMI，数据将产生NA值。'
- en: 'When training datasets contain missing values, machine learning models can
    produce inaccurate predictions or fail to train properly due to the lack of complete
    information. In this section, we will discuss the following techniques for handling
    missing data:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练数据集包含缺失值时，机器学习模型可能会产生不准确的预测或由于信息不完整而无法正确训练。在本节中，我们将讨论以下处理缺失数据的技术：
- en: Deleting data
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除数据
- en: Encoding missingness
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值编码
- en: Imputation methods
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充方法
- en: One way to get rid of missing data is by deleting the missing records. This
    is also known as the **complete case analysis** (**CCA**) method. This is fine
    if less than 5% of rows contain missing values, but deleting more records could
    reduce the power of the model since the sample size will become smaller. There
    might also be a systematic bias in the data since this technique assumes that
    the data is missing completely and random, but it violates other assumptions such
    as when data is **missing at random** (**MAR**) or **missing not at random** (**MNAR**).
    Hence, blindly removing the data could make the model more biased. For instance,
    if a minority population has not declared their income in the past or has not
    held credit in the past, they may not have a credit score. If we remove this data
    blindly without understanding the reason for missingness, the algorithm could
    be more biased toward giving loans to majority groups that have credit information,
    and minority groups will be denied the opportunity, despite some members having
    solid income and creditworthiness.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 一种处理缺失数据的方法是通过删除缺失的记录。这也被称为**完整案例分析**（**CCA**）方法。如果少于5%的行包含缺失值，这通常是可行的，但删除更多记录可能会降低模型的效力，因为样本量会变小。由于这种技术假设数据是完整随机缺失的，因此可能存在系统偏差，但违反了其他假设，例如当数据是**随机缺失**（**MAR**）或**非随机缺失**（**MNAR**）时。因此，盲目删除数据可能会使模型更加有偏差。例如，如果一个少数群体在过去没有申报收入或没有持有信用，他们可能没有信用评分。如果我们不了解缺失的原因而盲目删除这些数据，算法可能会更有利于向有信用信息的多数群体提供贷款，而少数群体将失去机会，尽管其中一些成员有稳定的收入和信用。
- en: 'Let’s explore the dataset using the CCA technique, remove all rows where information
    is missing, and figure out what volume of data is lost:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用CCA技术来探索数据集，删除所有缺失信息的行，并找出丢失了多少数据量：
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Since 21% is almost one-fourth of the dataset, this is not a feasible method.
    Hence, in this section, we will explore how to identify missing data, uncover
    patterns or reasons for data being missing, and discover techniques for handling
    missing data so that the dataset can be used for machine learning.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于21%几乎占数据集的四分之一，这不是一个可行的方法。因此，在本节中，我们将探讨如何识别缺失数据，揭示数据缺失的模式或原因，并发现处理缺失数据的技术，以便数据集可以用于机器学习。
- en: 'First, we will extract categorical features, binary features, and numerical
    features. To do this, we must separate the identifier and the target label:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将提取分类特征、二元特征和数值特征。为此，我们必须分离标识符和目标标签：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'To check if data is missing in the dataset, pandas provides a convenience method
    called `.info()`. This method shows us how many rows are complete among the total
    records. The method also displays the data type of each column:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查数据集中是否存在缺失数据，pandas提供了一个方便的方法叫做`.info()`。此方法显示总记录中完整行数有多少。该方法还显示每列的数据类型：
- en: '[PRE23]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The pandas library has another convenience method called `.isnull()` to check
    which row is missing data for a column and which row is complete. By combining
    `.sum()` with `.isnull()`, we can get the total number of missing records for
    each column:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: pandas库还有一个方便的方法叫做`.isnull()`，用来检查某一列的哪些行缺失数据，哪些行是完整的。通过将`.sum()`与`.isnull()`结合使用，我们可以得到每个列的缺失记录总数：
- en: '[PRE24]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As we can see, the `credit_history` , `self_employed`, and `loan_amount` columns
    have the most missing data. Raw values can sometimes be difficult to comprehend
    and it’s more useful to know the percentage of data that’s missing from each column.
    In the next step, we will create a function that will take the DataFrame and print
    out the missing percentage of data against each column. Then, we sort the data
    in descending order of missingness:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，`credit_history`、`self_employed`和`loan_amount`列有最多的缺失数据。原始值有时难以理解，知道每个列缺失数据的百分比更有用。在下一步中，我们将创建一个函数，该函数将接受DataFrame并打印出每个列的缺失数据百分比。然后，我们将按缺失度降序排序数据：
- en: '[PRE25]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, we can extract the magnitude of missing data. However, before we dive into
    handling missing data, it is important to understand the patterns for missing
    data. By understanding these relationships, we will be able to take appropriate
    steps. This is because imputing missing data can alter the distribution of the
    data, which may further affect variable interaction.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以提取缺失数据的量级。然而，在我们深入处理缺失数据之前，了解缺失数据的模式非常重要。通过理解这些关系，我们将能够采取适当的步骤。这是因为填补缺失数据可能会改变数据的分布，这可能会进一步影响变量交互。
- en: We will utilize the `missingno` library and other visualizations to understand
    where data is missing, and in the absence of subject matter experts, we will make
    some assumptions on reasons for missing data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用`missingno`库和其他可视化工具来了解数据缺失的位置，在没有主题专家的情况下，我们将对缺失数据的原因做出一些假设。
- en: 'To see where values are missing and where there are gaps in the data, we will
    utilize a `matrix` plot. A `matrix` plot can be quite useful when the dataset
    has depth or when the data contains time-related information. The presence of
    data is represented in gray, while absent data is displayed in white:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看值缺失和数据中存在差距的位置，我们将利用矩阵图。当数据集具有深度或数据包含与时间相关的信息时，矩阵图非常有用。数据的存在用灰色表示，而缺失数据用白色显示：
- en: '[PRE26]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here’s the output:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是输出：
- en: '![Figure 5.2 – Matrix plot](img/B19297_05_2.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 矩阵图](img/B19297_05_2.jpg)'
- en: Figure 5.2 – Matrix plot
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 矩阵图
- en: Looking closer, we can see that the `credit_history` column has a lot of missing
    points, and the occurrence of missingness is spread throughout the data and not
    at a given point in time.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察，我们可以看到`credit_history`列有很多缺失点，缺失的发生在整个数据中分布，而不是在某个特定的时间点。
- en: 'As we touched upon earlier, understanding the reasons behind the missingness
    of data can help us choose the right technique to handle missing data. At a high
    level, we can call these mechanisms of missing data and classify them into three
    categories:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，了解数据缺失的原因可以帮助我们选择正确的技术来处理缺失数据。从高层次上讲，我们可以将这些缺失数据的机制称为，并将它们分为三类：
- en: '**Missing completely at** **random** (**MCAR**)'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全随机缺失**（MCAR）'
- en: '**Missing not at** **random** (**MNAR**)'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非随机缺失**（MNAR）'
- en: '**Missing at** **random** (**MAR**)'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机缺失**（MAR）'
- en: Data is MCAR when the likelihood of missing data is the same for all the observations,
    and there is no relationship between the data being missing and any other features
    in the dataset. For example, a mail questionnaire might get lost in the post or
    a person may have forgotten to answer a question if they were in a hurry. In such
    cases, data being missing has nothing to do with the type of question, age group,
    or gender (relationship with other variables), and we can classify such features
    or data points as MCAR. Removing these data points or changing the value to 0
    for such instances will not bias the prediction.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 数据在缺失数据对所有观测值的可能性相同，并且缺失数据与数据集中任何其他特征之间没有关系时，被认为是完全随机缺失（MCAR）。例如，一封邮件问卷可能在邮寄过程中丢失，或者如果一个人匆忙，他们可能忘记回答一个问题。在这种情况下，数据缺失与问题的类型、年龄组或性别（与其他变量的关系）无关，我们可以将这些特征或数据点归类为MCAR。删除这些数据点或将这些实例的值更改为0不会对预测造成偏差。
- en: On the other hand, data is MAR when the likelihood of a data point being missing
    depends on other existing data points. For instance, if men don’t disclose their
    weight 5% of the time on average, whereas women don’t disclose their weight 15%
    of the time, we can assume that missingness in data is caused by the presence
    of gender bias. This will lead to a higher percentage of data being missing for
    women than for men. For this mechanism, we can impute data using statistical techniques
    or machine learning to predict the missing value by utilizing other features in
    the dataset.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，当数据点的缺失可能性取决于其他现有数据点时，数据被认为是随机缺失（MAR）。例如，如果男性平均有5%的时间不披露他们的体重，而女性有15%的时间不披露他们的体重，我们可以假设数据缺失是由性别偏见引起的。这将导致女性比男性有更高比例的数据缺失。对于这种机制，我们可以使用统计技术或机器学习来预测缺失值，利用数据集中的其他特征。
- en: The third mechanism, MNAR, can often be confused with MCAR but is slightly different.
    In this scenario, a clear assumption can be made as to why data is not missing
    at random. For instance, if we are trying to understand what factors lead to depression
    (outcome), depressed people could be more likely to not answer questions or less
    likely to be contacted. Since missingness is related to the outcome, these missing
    records can be flagged as “missing” and for numerical features, we can use a combination
    of machine learning to impute missing data from other features and flag data points,
    where data is missing, by creating another variable.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个机制，MNAR，通常容易与MCAR混淆，但略有不同。在这种情况下，可以明确地假设为什么数据不是随机缺失的。例如，如果我们试图了解导致抑郁（结果）的因素，那么抑郁的人可能更不愿意回答问题或更不可能被联系。由于缺失与结果相关，这些缺失记录可以标记为“缺失”，对于数值特征，我们可以使用机器学习结合其他特征来估计缺失数据，并通过创建另一个变量来标记数据缺失的点。
- en: Now that we understand the different types of missing data, we will utilize
    the `heatmap` function from `missingno`, which will create a correlation heatmap.
    The visualization shows a nullity correlation between columns of the dataset.
    It shows how strongly the presence or absence of one feature affects the other.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了不同类型的缺失数据，我们将利用 `missingno` 中的 `heatmap` 函数，它将创建一个关联热图。可视化显示了数据集列之间的空值相关系数。它显示了某个特征的存在或缺失如何强烈地影响其他特征。
- en: 'Nullity correlation ranges from -1 to 1:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 空值相关系数范围从 -1 到 1：
- en: -1 means if one column (attribute) is present, the other is almost certainly
    absent
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: -1 表示如果一个列（属性）存在，另一个几乎肯定不存在
- en: 0 means there is no dependence between the columns (attributes)
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 0 表示列（属性）之间没有依赖关系
- en: 1 means that if one column (attribute) is present, the other is also certainly
    present
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 表示如果一个列（属性）存在，另一个也肯定存在
- en: Unlike a standard correlation heatmap, the following visualization is about
    the relationship between missing data features since few of them have missing
    data. Those columns that are always full or always empty have no meaningful correlation
    and are removed from the visualization.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 与标准的关联热图不同，以下可视化是关于缺失数据特征之间的关系，因为其中许多特征几乎没有缺失数据。那些始终完整或始终为空的列没有有意义的关联，并且被从可视化中移除。
- en: 'This heatmap helps identify data completeness correlations between attribute
    pairs, but it has limited explanatory ability for broader relationships:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个热图有助于识别属性对之间的数据完整性相关系数，但它对更广泛关系的解释能力有限：
- en: '[PRE27]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This results in the following heatmap:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下热图：
- en: '![Figure 5.3 – Heatmap plot](img/B19297_05_3.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 热图](img/B19297_05_3.jpg)'
- en: Figure 5.3 – Heatmap plot
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 热图
- en: From this plot, we can interpret relationships of missingness across a few variables.
    There is a correlation of 0.4 between `dependents` and `married`, which makes
    sense as the majority of the time, someone gets married first before having dependents.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图中，我们可以解释几个变量之间的缺失关系。`dependents` 和 `married` 之间存在 0.4 的相关性，这是有道理的，因为大多数情况下，人们先结婚再成为有依赖的人。
- en: 'Next, we will extract columns that contain missing data and use these for the
    next visualization. The `dendrogram` method uses hierarchical clustering and groups
    attributes together where missingness is associated with the missingness of another
    variable or completeness is associated with the completeness of another variable:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提取包含缺失数据的列，并使用这些列进行下一个可视化。`dendrogram` 方法使用层次聚类，将属性分组在一起，其中缺失与另一个变量的缺失相关联，或完整性与其他变量的完整性相关联：
- en: '[PRE28]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.4 – Dendrogram plot](img/B19297_05_4.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 系谱图](img/B19297_05_4.jpg)'
- en: Figure 5.4 – Dendrogram plot
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 系谱图
- en: We interpret the dendrogram based on a top-down approach – that is, we focus
    on the height at which any two columns are connected with matters of nullity.
    The bigger the height, the smaller the relation, and vice versa. For example,
    the missingness or presence of data in `credit_history` has no relationship with
    the missingness or completeness of any other variable.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们根据自上而下的方法来解释系谱图 – 即，我们关注任何两个列在空值问题上的连接高度。高度越大，关系越小，反之亦然。例如，`credit_history`
    中的数据缺失与任何其他变量的缺失或完整性没有关系。
- en: With that, we have understood patterns of missing data and if there are relationships
    between missing data columns. Next, we will explore the relationship between missing
    data and the outcome. Before we decide to remove missing data or impute it, we
    should also look at whether the missingness of a variable is associated with the
    outcome – that is, is there a chance that data may be MNAR?
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们已经了解了缺失数据的模式，以及缺失数据列之间是否存在关系。接下来，我们将探索缺失数据与结果之间的关系。在我们决定删除缺失数据或进行插补之前，我们还应该看看变量的缺失是否与结果相关联——也就是说，数据可能存在MNAR的机会吗？
- en: 'First, we will visualize this relationship in missing categorical data:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将可视化缺失分类数据中的这种关系：
- en: '[PRE29]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'This will create some plots and show how categorical features are associated
    with the target variable:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建一些图表，展示分类特征与目标变量之间的关系：
- en: '![](img/B19297_05_5.jpg)![](img/B19297_05_6.jpg)![](img/B19297_05_7.jpg)![](img/B19297_05_8.jpg)![](img/B19297_05_9.jpg)![](img/B19297_05_10.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图表1](img/B19297_05_5.jpg)![图表2](img/B19297_05_6.jpg)![图表3](img/B19297_05_7.jpg)![图表4](img/B19297_05_8.jpg)![图表5](img/B19297_05_9.jpg)![图表6](img/B19297_05_10.jpg)'
- en: Figure 5.5 – The output plots displaying the association of categorical features
    with the target variable
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 - 显示分类特征与目标变量关联的输出图表
- en: At a high level, we can assume that for variables such as `married`, `dependents`,
    `loan_amount_term`, `gender`, and `credit_history`, the missingness of data is
    associated with the loan-approved status. Hence, we can say that the data for
    these variables is MNAR. For these three variables, we can encode missing data
    with the word “missing” as this signal will help predict the outcome. The missingness
    or completeness of `credit_history` is slightly associated with the `self_employed`
    status, as indicated in the heatmap plot, which shows that the data might be missing
    at random. Similarly, the missingness of the `married` status is associated with
    the missingness of `dependents` and `loan_amount`.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，我们可以假设对于`married`、`dependents`、`loan_amount_term`、`gender`和`credit_history`等变量，数据的缺失与贷款批准状态相关联。因此，我们可以认为这些变量的数据是MNAR（Missing
    Not At Random，非随机缺失）。对于这三个变量，我们可以用“missing”这个词来编码缺失数据，因为这个信号将有助于预测结果。`credit_history`的缺失或完整性略与`self_employed`状态相关，如热图所示，这表明数据可能随机缺失。同样，`married`状态的缺失与`dependents`和`loan_amount`的缺失相关。
- en: For all binary variables where data is missing, we can assume that data is not
    MCAR and rather assume that data is MNAR as there was some relationship of missingness
    of information with the outcome, or MAR, since missingness is associated with
    the presence or absence of other variables, as shown in the dendrogram.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有数据缺失的二进制变量，我们可以假设数据不是MCAR（Missing Completely At Random，完全随机缺失），而是假设数据是MNAR，因为缺失信息与结果之间存在某种关系，或者MAR（Missing
    At Random，随机缺失），因为缺失与其他变量的存在或不存在相关，如树状图所示。
- en: One way to encode missing values is to encode these with the most frequent values
    or get rid of missing values or/and create an additional column that indicates
    missingness with 1 or 0\. However, for MAR scenarios, this is not the best technique.
    As mentioned earlier, the goal of a data-centric approach is to improve data quality
    and reduce bias. Hence, instead of using frequency imputation methods or just
    deleting records, we should consider asking annotators to provide information
    where data is missing or make system fixes to recover from missing information.
    If that is not possible, we should consider using machine learning techniques
    or probabilistic techniques to determine possible values over simple imputation
    methods of mode, mean, and median. However, when missingness exceeds certain thresholds,
    even advanced techniques are not reliable and it’s better to drop the feature.
    For the remaining variables, we will use a machine learning technique to determine
    the missing values since we cannot get annotators to help us provide complete
    information.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 编码缺失值的一种方法是将这些值编码为最频繁的值，或者去除缺失值，或者创建一个额外的列，用1或0表示缺失。然而，对于MAR场景，这并不是最佳技术。如前所述，数据中心方法的目标是提高数据质量和减少偏差。因此，我们不应使用频率插补方法或仅删除记录，而应考虑要求注释者提供缺失数据的信息，或者进行系统修复以恢复缺失信息。如果这不可能，我们应该考虑使用机器学习技术或概率技术来确定可能的值，而不是简单的众数、均值和中位数插补方法。然而，当缺失超过一定阈值时，即使是高级技术也不可靠，最好是删除该特征。对于剩余的变量，我们将使用机器学习技术来确定缺失值，因为我们无法获得注释者的帮助来提供完整信息。
- en: 'Now that we have identified how the missingness of categorical values is associated
    with the outcome, next, we will study the relationship between missing numerical
    data and the outcome:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经确定了分类值缺失与结果之间的关联，接下来，我们将研究缺失数值数据与结果之间的关系：
- en: '[PRE30]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This will display the following plot:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示以下图表：
- en: '![Figure 5.6 – Loan amount missing association with target](img/B19297_05_11.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图5.6 – 贷款金额缺失与目标的相关性](img/B19297_05_11.jpg)'
- en: Figure 5.6 – Loan amount missing association with target
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 贷款金额缺失与目标的相关性
- en: For `loan_amount`, it can be assumed that data is MNAR as well as MAR since
    the missingness or completion of data in the `married` and `dependents` variables
    is slightly associated with the missingness and completeness of `loan_amount`,
    as observed in the heatmap. Hence, we choose to impute missing values using machine
    learning and create an additional column to indicate missingness, which will provide
    a better signal to our model.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`loan_amount`，可以假设数据是MNAR以及MAR，因为`married`和`dependents`变量中的数据缺失或完成与`loan_amount`的数据缺失和完整性略有关联，如热图中所示。因此，我们选择使用机器学习来插补缺失值，并创建一个额外的列来指示缺失，这将为我们模型提供更好的信号。
- en: Next, we will dive into various methods of imputing data and compare these,
    as well as talking about the shortcomings of each approach. We will also discuss
    the implications of machine learning on imputing missing data in data-centric
    machine learning.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入研究各种数据插补方法，并进行比较，同时讨论每种方法的不足。我们还将讨论机器学习在以数据为中心的机器学习中插补缺失数据的影响。
- en: Following a model-centric approach, the standard rule of thumb for imputing
    numerical variables is that when 5% of the data is missing, impute it with the
    mean, median, or mode. This approach assumes that data is missing completely at
    random. If this assumption is not true, these simple imputation methods may obscure
    distributions and relationships within the data.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 采用以模型为中心的方法，插补数值变量的标准规则是，当5%的数据缺失时，使用均值、中位数或众数进行插补。这种方法假设数据是随机缺失的。如果这个假设不成立，这些简单的插补方法可能会掩盖数据中的分布和关系。
- en: 'First, we will explore the distribution of `loan_amount` without imputation
    and when imputed with the median. The distribution changes when we impute 6% of
    the values with the median:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将探索未插补和用中位数插补的`loan_amount`的分布。当我们用中位数插补6%的值时，分布会发生变化：
- en: '[PRE31]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The following plot is displayed as the output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表是输出显示：
- en: '![Figure 5.7 – Simple density plot imputation with the median](img/B19297_05_12.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 使用中位数的简单密度图插补](img/B19297_05_12.jpg)'
- en: Figure 5.7 – Simple density plot imputation with the median
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 使用中位数的简单密度图插补
- en: 'Next, we compare the standard deviation of the loan amount before and after
    imputation:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较插补前后贷款金额的标准差：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The preceding code shows how a simple imputation method can obscure the distribution
    of data. To counter these effects and preserve the distribution, we will use the
    random sample imputation method.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码展示了简单的插补方法如何掩盖数据的分布。为了抵消这些影响并保持分布，我们将使用随机样本插补方法。
- en: First, we extract all the rows where `loan_amount` is missing. Then, we compute
    the variables that are correlated with `loan_amount` and use those values to set
    a seed. This is because, if we use the same seed for all values, then the same
    random number will be generated and the method will behave similarly to arbitrary
    value imputation, which will be as ineffective as the simple imputation methods
    of mean and median.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们提取所有`loan_amount`缺失的行。然后，我们计算与`loan_amount`相关的变量，并使用这些值来设置种子。这是因为，如果我们对所有值使用相同的种子，那么将生成相同的随机数，该方法的行为将与任意值插补类似，这将与均值和中位数等简单插补方法一样无效。
- en: The downside to random sample distribution is that covariance will be affected
    and we need a method that preserves the covariance as well.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 随机样本分布的缺点是协方差将受到影响，我们需要一种同时保持协方差的方法。
- en: 'First, we check which feature is highly correlated with `loan_amount`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们检查哪个特征与`loan_amount`高度相关：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here, we can see that `loan_amount` is highly correlated with `applicant_income`,
    so for this example, we use this variable to set the seed. First, we extract the
    indexes where `loan_amount` is missing. Then, we use the `applicant_income` value
    at the missing location and use this value to set the seed. Next, we use this
    seed to generate a random value from `loan_amount` to impute the missing row.
    We use this approach to impute all the missing data for `loan_amount`:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到`loan_amount`与`applicant_income`高度相关，因此在这个例子中，我们使用这个变量来设置种子。首先，我们提取`loan_amount`缺失的索引。然后，我们使用缺失位置的`applicant_income`值，并使用这个值来设置种子。接下来，我们使用这个种子从`loan_amount`生成一个随机值来插补缺失的行。我们使用这种方法来插补`loan_amount`的所有缺失数据：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we compare the distribution of `loan_amount` with the random sample imputed
    `loan_amount` and median imputed `loan_amount`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较`loan_amount`的分布与随机样本插补的`loan_amount`和中位数插补的`loan_amount`：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This will output the following plot:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下图表：
- en: '![Figure 5.8 – Density plot showing random and median imputations](img/B19297_05_13.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – 显示随机和中位数插补的密度图](img/B19297_05_13.jpg)'
- en: Figure 5.8 – Density plot showing random and median imputations
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – 显示随机和中位数插补的密度图
- en: 'Now, we compare the standard deviation of the pre-imputed loan with the random
    sample imputed method and median imputed method:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们比较预先插补的贷款的标准差与随机样本插补方法和中位数插补方法：
- en: '[PRE36]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The random sample imputation method is much closer in distribution and standard
    deviation to the pre-imputed `loan_amount` method than the median imputed `loan_amount`
    method. Next, we check whether the random sample imputation method preserves the
    correlation with other variables compared to other methods:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 随机样本插补方法在分布和标准差上与预先插补的`loan_amount`方法比中位数插补的`loan_amount`方法更接近。接下来，我们检查随机样本插补方法是否与其他方法相比保留了与其他变量的相关性：
- en: '[PRE37]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The resulting DataFrame is as follows:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的DataFrame如下：
- en: '![Figure 5.9 – Correlation DataFrame](img/B19297_05_14.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – 相关性DataFrame](img/B19297_05_14.jpg)'
- en: Figure 5.9 – Correlation DataFrame
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – 相关性DataFrame
- en: 'From this, it’s evident that the random imputation method can preserve the
    distribution but may obscure the inter-relationships with other variables. We
    need a method that can preserve the distribution and also maintain the inter-relationships
    with other variables. We will use machine learning to help us achieve this. Before
    we move on to machine learning, we will first discuss the impact of simple imputation
    on categorical/binary variables. We impute the `credit_history` binary column
    with the most frequent value and compare the distribution before and after imputation:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点来看，很明显，随机插补方法可以保留分布，但可能会掩盖与其他变量的相互关系。我们需要一种可以保留分布并保持与其他变量相互关系的方法。我们将使用机器学习来帮助我们实现这一点。在我们转向机器学习之前，我们首先将讨论简单插补对分类/二进制变量的影响。我们使用最频繁的值插补`credit_history`二进制列，并比较插补前后的分布：
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: By imputing `credit_history` with the most frequent values, we have biased the
    data toward the `credit_history` status. As we discovered previously, the missingness
    of `credit_history` is not associated with any other variables, but it might be
    associated with the outcome.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用最频繁的值插补`credit_history`，我们使数据偏向于`credit_history`状态。正如我们之前发现的，`credit_history`的缺失与任何其他变量都不相关，但它可能与结果相关。
- en: The preceding examples show that if we utilize simple imputation methods then
    we may bias the data, and the distribution will be altered as well, whereas if
    we utilize random methods, the distribution will be preserved but the data relationships
    may change and data variance may increase. Hence, when data is MAR or MNAR, to
    achieve a balance between data bias and data variance, we can use a machine learning
    model.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的例子表明，如果我们使用简单的插补方法，那么我们可能会对数据进行偏差，分布也会相应改变，而如果我们使用随机方法，分布将得以保留，但数据关系可能会改变，数据方差可能会增加。因此，当数据是MAR（完全随机应答）或MNAR（非随机应答）时，为了在数据偏差和数据方差之间取得平衡，我们可以使用机器学习模型。
- en: To utilize machine learning for numerical imputation, we will leverage the nearest
    neighbors imputation method available in the `scikit-learn` library – `KNNImputer`.
    One issue with the imputer is that we can only pass a DataFrame to it, and not
    pass a list of columns. Hence, we will use the `SklearnTransformerWrapper` module,
    which is available as part of the `feature-engine` library, to pass a list of
    columns. Since KNN is a distance-based algorithm, to ensure that the model converges
    and one variable doesn’t overpower the other variable, we must scale the data
    before using this algorithm.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 为了利用机器学习进行数值插补，我们将利用`scikit-learn`库中可用的最近邻插补方法`KNNImputer`。这个插补器的一个问题是，我们只能传递一个DataFrame给它，而不能传递列的列表。因此，我们将使用`SklearnTransformerWrapper`模块，它是`feature-engine`库的一部分，来传递列的列表。由于KNN是一个基于距离的算法，为了确保模型收敛并且一个变量不会压倒另一个变量，我们必须在使用此算法之前对数据进行缩放。
- en: Another technique to impute data is referred to as **Multiple Imputation by
    Chained Equations** (**MICE**). MICE works by imputing all the data with the mean,
    median, or mode. Then, regarding the variable that will be imputed, the initial
    imputed values are converted back into missing values. Then, using other variables
    as predictors, a machine learning model is used to predict missing values. After
    this, the next variable is imputed in a similar manner, where initially imputed
    values are converted back into missing values, and other variables, including
    the recently imputed variable, are used as predictors to impute the missing values.
    Once all the variables with missing values have been modeled and values have been
    imputed with predictions, the first round of imputation is completed. This procedure
    is repeated *n* number of times (ideally 10), and from round two, round one predictions
    are used to predict records that were initially missing.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种用于插补数据的技术被称为**链式方程多重插补**（**MICE**）。MICE通过使用均值、中位数或众数来插补所有数据。然后，对于将要插补的变量，初始插补的值被转换回缺失值。接着，使用其他变量作为预测变量，利用机器学习模型预测缺失值。之后，以类似的方式插补下一个变量，其中初始插补的值被转换回缺失值，并使用包括最近插补的变量在内的其他变量作为预测变量来插补缺失值。一旦所有带有缺失值的变量都被建模，并且使用预测值插补了值，第一次插补轮次就完成了。这个过程重复*n*次（理想情况下为10次），从第二轮开始，使用第一轮的预测来预测最初缺失的记录。
- en: The reason for using several rounds is that, initially, we are modeling the
    missing data using other variables that also have NA values, and the initial strategy
    of imputation uses suboptimal methods such as the mean, median, or mode, which
    may bias the predictions. As we continue to regress over multiple rounds, predictions
    will stabilize and become less biased.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多轮的原因是，最初我们使用其他也包含NA值的变量来模拟缺失数据，而初始的插补策略使用的是次优方法，如均值、中位数或众数，这些方法可能会对预测产生偏差。随着我们继续进行多轮回归，预测将趋于稳定并减少偏差。
- en: One issue with MICE is that we have to choose which machine learning model to
    use for the task. We will implement MICE with the random forest algorithm, which
    in R language is referred to as `[missForest]`.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: MICE的一个问题是，我们必须选择用于任务的机器学习模型。我们将使用随机森林算法实现MICE，在R语言中这被称为`[missForest]`。
- en: In our implementation of MICE, we will refer to it as `missForest` since it
    will be a replica of how it is implemented in R (https://rpubs.com/lmorgan95/MissForest#:~:text=MissForest%20is%20a%20random%20forest,then%20predicts%20the%20missing%20part).
    To counter the effects of choosing an algorithm, we encourage practitioners to
    leverage automated machine learning, where for each imputation and iteration,
    a new algorithm will be chosen. The one disadvantage of this approach is that
    it’s computationally intensive and time-intensive when utilized for big datasets.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现的MICE中，我们将称之为`missForest`，因为它将复制R语言中实现的方式（[R语言中的MissForest](https://rpubs.com/lmorgan95/MissForest#:~:text=MissForest%20is%20a%20random%20forest,then%20predicts%20the%20missing%20part)）。为了对抗选择算法的影响，我们鼓励实践者利用自动化机器学习，对于每一次插补和迭代，都会选择一个新的算法。这种方法的一个缺点是，当用于大数据集时，它计算量大且耗时。
- en: 'First, we import the necessary packages:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的包：
- en: '[PRE39]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Next, we extract the numerical columns by filtering any columns that may have
    more than 15 categories while filtering the `id` column and outcome, as well as
    filtering newly created variables using imputation methods:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过过滤掉可能包含超过15个类别的任何列，同时过滤`id`列和结果列，以及使用插补方法过滤新创建的变量来提取数值列：
- en: '[PRE40]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Next, we create the DataFrame with numerical variables and visualize it:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建包含数值变量的DataFrame并可视化它：
- en: '[PRE41]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we build a function that takes the scaler (standard scaler or any other
    scaler) and DataFrame and returns scaled data and the processed scaler. We must
    scale the dataset before applying the KNN imputer since a distance-based method
    requires data to be on the same scale. Once we have scaled the data, we apply
    the KNN imputer to impute the data, and then unscale the data using the processed
    scaler returned by the function. Once we’ve done this, we can compare the machine
    learning imputed data with the median and random imputation methods:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们构建一个函数，该函数接受缩放器（标准缩放器或任何其他缩放器）和DataFrame，并返回缩放后的数据和经过处理的缩放器。在应用KNN估计器之前，我们必须缩放数据集，因为基于距离的方法需要数据处于相同的尺度。一旦我们缩放了数据，我们就应用KNN估计器来估计数据，然后使用函数返回的经过处理的缩放器来反缩放数据。完成这些后，我们可以比较机器学习估计的数据与中位数和随机估计方法：
- en: '[PRE42]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Next, we define the scaler and call the `scale_data` function:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义缩放器并调用`scale_data`函数：
- en: '[PRE43]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Then, we apply the KNN imputer with a parameter of 10 neighbors to impute the
    data. We utilize the `weights='distance'` parameter so that more weightage will
    be given to the votes of the nearer neighbors compared to the ones that are further
    away when predicting the outcome.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用10个邻居的参数应用KNN估计器来估计数据。我们利用`weights='distance'`参数，以便在预测结果时，更重视靠近邻居的投票，而不是远离邻居的投票。
- en: 'First, we initialize the imputer:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化估计器：
- en: '[PRE44]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Then, we apply the imputation:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们应用估计：
- en: '[PRE45]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we unscale the data by calling the `inverse_transform` method from the
    scaler object and overwrite the `df_imputed` DataFrame with unscaled values:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过调用缩放器对象的`inverse_transform`方法来反缩放数据，并用未缩放值覆盖`df_imputed` DataFrame：
- en: '[PRE46]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Next, we compare the distribution of the pre-imputed `loan_amount` and compare
    it with the machine learning imputed method. Then, we check the correlation of
    the machine learning imputed method to the applicant’s income and compare it with
    other imputed methods:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较预先估计的`loan_amount`的分布，并将其与机器学习估计的方法进行比较。然后，我们检查机器学习估计方法与申请者收入的关联性，并将其与其他估计方法进行比较：
- en: '[PRE47]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The resulting plot is as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如下：
- en: '![Figure 5.10 – Loan amount KNN imputation](img/B19297_05_15.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10 – 贷款金额KNN估计](img/B19297_05_15.jpg)'
- en: Figure 5.10 – Loan amount KNN imputation
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – 贷款金额KNN估计
- en: 'Next, we compare the standard deviation of the pre-imputed loan amount with
    all the imputation methods:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较预先估计的贷款金额的标准差与所有估计方法：
- en: '[PRE48]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Then, we will check if the correlation is maintained when `loan_amount` is
    imputed using machine learning:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将检查当使用机器学习来估计`loan_amount`时，相关性是否保持不变：
- en: '[PRE49]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![Figure 5.11 – Correlation after loan_amount is imputed](img/B19297_05_16.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图5.11 – 贷款金额估计后的相关性](img/B19297_05_16.jpg)'
- en: Figure 5.11 – Correlation after loan_amount is imputed
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 – 贷款金额估计后的相关性
- en: The machine learning-imputed method has almost the same distribution as the
    original. However, the correlation is a bit higher with `applicant_income` compared
    to the pre-imputed `loan_amount`. We have now seen how to use out-of-the-box techniques
    to impute missing data. One advantage of this method is that it’s simple to implement.
    However, the disadvantage is that we cannot choose another algorithm.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习估计的方法几乎与原始数据具有相同的分布。然而，与预先估计的`loan_amount`相比，与`applicant_income`的相关性略高。我们现在已经看到了如何使用现成的技术来估计缺失数据。这种方法的一个优点是易于实现。然而，缺点是我们不能选择另一个算法。
- en: Hence, in the next step, we go one step further and build a MICE implementation
    with random forest. First, we convert categorical data into numerical data using
    one-hot encoding. Then, we impute missing categorical data with the MICE implementation
    with `RandomForestClassifier`.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在下一步中，我们更进一步，使用随机森林构建一个MICE实现。首先，我们使用独热编码将分类数据转换为数值数据。然后，我们使用`RandomForestClassifier`的MICE实现来估计缺失的分类数据。
- en: Once the categorical data has been imputed, we use categorical and numerical
    data to impute numerical missing values by utilizing MICE with `RandomForestRegressor`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦分类数据被估计，我们使用分类和数值数据，通过利用MICE与`RandomForestRegressor`来估计数值缺失值。
- en: 'To build the MICE implementation, we use `IterativeImputer`, which is available
    in scikit-learn, to help with 10 rounds of MICE. To leverage `IterativeImputer`,
    we must import `enable_iterative_imputer` from scikit-learn’s experimental packages,
    as per the docs: [https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html).'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建MICE实现，我们使用scikit-learn中的`IterativeImputer`，它可以帮助进行10轮MICE。为了利用`IterativeImputer`，我们必须从scikit-learn的实验包中导入`enable_iterative_imputer`，如文档所述：[https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.IterativeImputer.html)。
- en: 'First, we import the necessary packages:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入必要的包：
- en: '[PRE50]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Next, we extract the categorical columns that are string-encoded so that we
    can one-hot encode these:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取字符串编码的分类列，以便我们可以对这些列进行独热编码：
- en: '[PRE51]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Then, we one-hot encode the categorical columns:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对分类列进行独热编码：
- en: '[PRE52]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'After that, we visualize the first five results of the one-hot encoded data:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们可视化独热编码数据的前五个结果：
- en: '[PRE53]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Next, we extract the categorical variables that are binary encoded, including
    the data that has been one-hot encoded:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们提取二进制编码的分类变量，包括已经独热编码的数据：
- en: '[PRE54]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Then, we build the MICE implementation with random forest to impute categorical
    data:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用随机森林构建MICE实现以填充分类数据：
- en: '[PRE55]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we extract the features from the imputation by converting the NumPy array
    into a DataFrame called `df_cat_imputed`:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将填充的数值通过将NumPy数组转换为名为`df_cat_imputed`的DataFrame来提取特征：
- en: '[PRE56]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Let’s ensure we don’t have any new unexpected values being created by the classifier.
    To check this, we iterate over all the columns and print the unique values for
    each column:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们确保分类器没有创建任何新的意外值。为此，我们遍历所有列并打印每列的唯一值：
- en: '[PRE57]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Now, we combine the categorical imputed data with numerical data. Then, we
    use all the data to impute numerical data:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将分类填充数据与数值数据合并。然后，我们使用所有数据来填充数值数据：
- en: '[PRE58]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Next, we implement MICE imputation with random forest to impute numerical data:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用随机森林实现MICE填充以填充数值数据：
- en: '[PRE59]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Now, we extract the features from the imputation by converting the NumPy array
    into a DataFrame:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将填充的数值通过将NumPy数组转换为DataFrame来提取特征：
- en: '[PRE60]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Then, we check whether all the columns have been imputed and there are no missing
    values:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们检查是否所有列都已填充并且没有缺失值：
- en: '[PRE61]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, we compare the distribution of the pre-imputed `loan_amount` and compare
    it with the MICE imputed method. We then check the correlation of the MICE imputed
    method with the applicant’s income and compare it with other imputed methods:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较预填充的`loan_amount`的分布，并将其与MICE填充方法进行比较。然后，我们检查MICE填充方法与申请者收入的关联性，并将其与其他填充方法进行比较：
- en: '[PRE62]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'The output is as follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.12 – loan_amount_miss_forest_imputed](img/B19297_05_17.jpg)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![图5.12 – loan_amount_miss_forest_imputed](img/B19297_05_17.jpg)'
- en: Figure 5.12 – loan_amount_miss_forest_imputed
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 – loan_amount_miss_forest_imputed
- en: 'Next, we compare the standard deviation of the pre-imputed loan amount with
    all the imputation methods, including the MICE imputation method:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们比较预填充贷款金额的标准差与所有填充方法，包括MICE填充方法：
- en: '[PRE63]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Then, we check if the correlation is maintained when `loan_amount` is imputed
    using the MICE imputation method compared to other methods:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们检查当使用MICE填充方法填充`loan_amount`时，与其他方法相比，相关性是否保持不变：
- en: '[PRE64]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'The output DataFrame is as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的DataFrame如下：
- en: "![Figure 5.13 – Correlation after imputing loan_amou\uFEFFnt using the MICE\
    \ imputation method](img/B19297_05_18.jpg)"
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: "![图5.13 – 使用MICE填充方法填充loan_amou\uFEFFnt后的相关性](img/B19297_05_18.jpg)"
- en: Figure 5.13 – Correlation after imputing loan_amount using the MICE imputation
    method
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 – 使用MICE填充方法填充loan_amount后的相关性
- en: The standard deviation is slightly below the random imputation but higher than
    the median imputed method. As we can see, the correlation with `applicant_income`
    hasn’t improved compared to the random imputation method or median imputation
    method. Hence, to test whether MICE with random forest is a better implementation
    for this use case, we can compare the evaluation metric of the machine learning
    model when MICE is utilized versus when median imputation is utilized.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 标准差略低于随机填充，但高于中位数填充方法。正如我们所见，与随机填充方法或中位数填充方法相比，与`applicant_income`的相关性没有改善。因此，为了测试MICE与随机森林是否是此用例的更好实现，我们可以比较当使用MICE时和当使用中位数填充时机器学习模型的评估指标。
- en: But before we do that, we want machine learning practitioners to explore **automated
    machine learning** (**AutoML**) with the MICE imputation framework. Machine learning
    can be a tedious process that consists of trial and error, hence why AutoML frameworks
    are getting quite popular when it comes to reducing human time. These frameworks
    automate feature engineering, cross-validation, model selection, and model tuning.
    One issue with the current implementation of MICE is that we have to choose which
    machine learning model to use for the task. What if we wanted to trial multiple
    algorithms to see which provided the best prediction for the imputation task,
    and while doing that wanted to ensure the prediction was generalizable and the
    model was not overfitted or underfitted? We can imagine the complexity. To counter
    this, we’ll combine AutoML with MICE.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们这样做之前，我们希望机器学习从业者使用MICE插补框架探索**自动化机器学习**（**AutoML**）。机器学习可能是一个繁琐的过程，它包括试错，这就是为什么AutoML框架在减少人工时间方面越来越受欢迎。这些框架自动化特征工程、交叉验证、模型选择和模型调优。MICE当前实现的一个问题是，我们必须选择用于任务的机器学习模型。如果我们想尝试多个算法，看看哪个提供了最佳的插补任务预测，并且在尝试过程中确保预测具有可推广性，并且模型没有过拟合或欠拟合，会怎样呢？我们可以想象其复杂性。为了解决这个问题，我们将结合AutoML和MICE。
- en: One advantage of this approach is that at each iteration, a new model will be
    picked by AutoML, thus freeing the machine learning practitioner from tedious
    tasks. However, the disadvantage of this approach is that when the data increases
    in size, a lot more resources will be needed, which may not be viable. Another
    disadvantage with some open source AutoML frameworks is that on some operating
    systems, full functionality is error-prone. For instance, on Mac computers, both
    TPOT and AutoSklearn frameworks give errors when parallel processing is used.
    Hence, we will let you explore your own flavor of AutoML with MICE.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是，在每次迭代中，AutoML将选择一个新的模型，从而让机器学习从业者从繁琐的任务中解放出来。然而，这种方法的缺点是，当数据量增加时，需要更多的资源，这可能不可行。另外，一些开源的AutoML框架的另一个缺点是，在某些操作系统上，完整功能可能会出现错误。例如，在Mac电脑上，TPOT和AutoSklearn框架在并行处理时都会出现错误。因此，我们将让您探索使用MICE的AutoML的个性化版本。
- en: Next, we will implement a scikit-learn pipeline that will include the MICE implementation
    with random forest. Then, we train the decision tree model with cross-validation
    and evaluate the model using accuracy and ROC. Once we’ve done this, we create
    another pipeline, which will use simple imputation methods, and compare the evaluation
    results. Finally, we explore techniques for further improving the data to enhance
    model performance.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个包含MICE实现和随机森林的scikit-learn流水线。然后，我们使用交叉验证训练决策树模型，并使用准确率和ROC评估模型。完成这些后，我们创建另一个流水线，它将使用简单的插补方法，并比较评估结果。最后，我们探索进一步改进数据以提高模型性能的技术。
- en: We’ll be converting these steps into a scikit-learn pipeline since by using
    a pipeline, we can define the sequence of steps and also save these steps as a
    pickle object. By utilizing this practice, we maintain machine learning system
    best practices and can ensure reliability and reproducibility without replicating
    the code in the inference environment.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这些步骤转换成scikit-learn流水线，因为通过使用流水线，我们可以定义步骤的顺序，并将这些步骤保存为pickle对象。通过利用这种做法，我们保持机器学习系统的最佳实践，并可以确保可靠性及可重复性，而无需在推理环境中重复编写代码。
- en: 'First, let’s drop all the newly created columns in the `df_consistent` DataFrame
    that end with `_imputed`:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们删除`df_consistent` DataFrame中所有以`_imputed`结尾的新创建的列：
- en: '[PRE65]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, we import all the necessary packages and modules to help split the data
    into train and test sets, evaluate the performance of the model, and create a
    machine learning pipeline:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将导入所有必要的包和模块，以帮助将数据分为训练集和测试集，评估模型的性能，并创建机器学习流水线：
- en: '[PRE66]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, we extract the features for the model and split the data into train and
    test sets, where 10% of the data is reserved for testing:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们提取模型所需的特征，并将数据分为训练集和测试集，其中10%的数据保留用于测试：
- en: '[PRE67]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Next, we extract the categorical data and numerical data into separate lists
    so that we can use these to set the pipeline for each type of data:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将提取分类数据和数值数据到单独的列表中，以便我们可以使用这些数据为每种类型的数据设置流水线：
- en: '[PRE68]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Now, we create a function that will return the pipeline for categorical data.
    First, the pipeline one-hot encodes the list of columns in the `ohe_cols` variable,
    which includes `property_area`. The pipeline then imputes the columns with missing
    data using the MICE implementation with random forest. The function will return
    the transformer so that when we pass the categorical data, while the transformer
    one-hot encodes the data and then imputes the missing data. The transformer will
    be run against the training data first so that it learns about the data and saves
    all the metadata for running the same steps with new data. The transformer can
    then be used to transform the test data:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们创建一个函数，该函数将返回用于分类数据的管道。首先，该管道将`ohe_cols`变量中的列列表进行独热编码，其中包括`property_area`。然后，该管道使用随机森林的MICE实现来填充缺失数据。该函数将返回转换器，以便当我们传递分类数据时，转换器将独热编码数据并填充缺失数据。转换器将首先在训练数据上运行，以便了解数据并保存所有元数据，以便用新数据运行相同的步骤。然后，转换器可以用来转换测试数据：
- en: '[PRE69]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we create a function that returns the pipeline transformer to impute
    numerical missing data with the MICE implementation. Similarly to the categorical
    transformer, the numerical transformer will be trained against the training data
    and then applied to the test data to impute missing values in the train and test
    data:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个函数，该函数返回用于用MICE实现填充数值缺失数据的管道转换器。与分类转换器类似，数值转换器将针对训练数据进行训练，然后应用于测试数据以填充训练和测试数据中的缺失值：
- en: '[PRE70]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Then, we initialize the categorical and numerical transformers, and then transform
    training and test data. The transformed categorical data is combined with numerical
    data before the numerical data is transformed. The output of this is imputed train
    and test DataFrames:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化分类和数值转换器，然后转换训练和测试数据。在转换数值数据之前，将转换后的分类数据与数值数据合并。这个输出的结果是填充后的训练和测试数据框：
- en: '[PRE71]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Before passing the complete datasets to the machine learning model, we check
    if both the train and test labels have similar loan approval rates:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在将完整数据集传递给机器学习模型之前，我们检查训练和测试标签是否具有相似的贷款批准率：
- en: '[PRE72]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Because the classes are slightly imbalanced, we can use the `class_weight='balanced'`
    option since this option uses the values of `y` to automatically adjust weights
    inversely proportional to class frequencies in the input data when training the
    algorithm. The objective of the problem is to identify better than a human being
    who is likely to get a loan. Since the majority class is trained on people who
    received a loan, the model will be biased toward giving someone a loan. By using
    `class_weight='balanced'`, the algorithm will put more emphasis on class label
    0 since it’s a minority class.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 由于类别略微不平衡，我们可以使用`class_weight='balanced'`选项，因为这个选项使用`y`的值在训练算法时自动调整与输入数据中类别频率成反比的权重。问题的目标是识别出优于可能获得贷款的人类。由于大多数类别是在收到贷款的人身上训练的，因此模型将偏向于给某人发放贷款。通过使用`class_weight='balanced'`，算法将更多地强调类别标签0，因为它是一个少数类别。
- en: 'We define grid search for the decision tree classifier to perform cross-validation,
    to ensure the model is generalizable:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了决策树分类器的网格搜索以执行交叉验证，以确保模型具有泛化能力：
- en: '[PRE73]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Next, we create a custom function that will take in training data, testing
    data, the classifier, and grid search parameters. The function performs 10K cross-validation
    to find the best hyperparameters and trains the model on the best parameters.
    The function then returns the model, predictions, training and test accuracies,
    and ROC-AUC score:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个自定义函数，该函数将接受训练数据、测试数据、分类器和网格搜索参数。该函数执行10K交叉验证以找到最佳超参数，并在最佳参数上训练模型。然后，该函数返回模型、预测、训练和测试准确率以及ROC-AUC分数：
- en: '[PRE74]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Next, we run the custom classifier and calculate model performance:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们运行自定义分类器并计算模型性能：
- en: '[PRE75]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The test accuracy is just under 80%. Let’s see where the model is performing
    poorly by observing the confusion matrix:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率略低于80%。让我们通过观察混淆矩阵来查看模型在哪些方面表现不佳：
- en: '[PRE76]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'This outputs the following confusion matrix:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下混淆矩阵：
- en: '![Figure 5.14 – Confusion matrix](img/B19297_05_19.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![图5.14 – 混淆矩阵](img/B19297_05_19.jpg)'
- en: Figure 5.14 – Confusion matrix
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 – 混淆矩阵
- en: We have now applied machine learning pipelines with MICE imputation to create
    a machine learning model. To demonstrate that MICE imputation is a better technique
    than the simple imputation technique, we will recreate the machine learning pipelines
    with simple imputation methods and evaluate model performance.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已应用了带有 MICE 插补的机器学习管道来创建机器学习模型。为了证明 MICE 插补技术比简单插补技术更好，我们将使用简单插补方法重新创建机器学习管道并评估模型性能。
- en: 'Once we have created the pipeline steps, we transform the train and test data
    before passing it to the decision tree classifier and custom classifier function
    to measure model performance:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了管道步骤，我们将在将其传递给决策树分类器和自定义分类器函数以衡量模型性能之前，对训练数据和测试数据进行转换：
- en: '[PRE77]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Next, we run the custom classifier and extract model performance:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们运行自定义分类器并提取模型性能：
- en: '[PRE78]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'The test accuracy has dropped under 67%, which is a 12% reduction, and the
    ROC-AUC has dropped by 6%. Next, we review the confusion matrix:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 测试准确率下降到低于 67%，下降了 12%，ROC-AUC 下降了 6%。接下来，我们回顾混淆矩阵：
- en: '[PRE79]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Here’s the output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出：
- en: '![Figure 5.15 – Confusion matrix](img/B19297_05_20.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 混淆矩阵](img/B19297_05_20.jpg)'
- en: Figure 5.15 – Confusion matrix
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 混淆矩阵
- en: The accuracy of the true positive class has dropped from 88% to 67%, whereas
    the accuracy of the negative class has increased from 58% to 63%. By using basic
    imputation techniques, we can conclude that the model is more likely to be biased
    and the model performance may be less accurate.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性类的准确率从 88% 下降到 67%，而阴性类的准确率从 58% 上升到 63%。通过使用基本的插补技术，我们可以得出结论，该模型更有可能存在偏差，模型性能可能不够准确。
- en: In data-centric machine learning, the goal is to improve on the data and tune
    it, rather than improving on the algorithm and tuning the model. But how can we
    identify whether a dataset contains poorly labeled data, missing features, or
    another data-related issue? We will cover how to identify if data is poorly labeled
    and apply techniques to improve on mislabeled data in [*Chapter 6*](B19297_06.xhtml#_idTextAnchor089),
    *Techniques for Programmatic Labeling in* *Machine Learning*.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在以数据为中心的机器学习中，目标是改进数据并调整它，而不是改进算法和调整模型。但如何确定一个数据集是否包含标签错误的数据、缺失特征或其他数据相关的问题？我们将在[*第
    6 章*](B19297_06.xhtml#_idTextAnchor089)中介绍如何识别数据标签错误并应用技术来改进错误标记的数据，*机器学习中的程序化标签技术*。
- en: To find out if more features are needed or more data is needed, we utilize a
    technique called error analysis. In machine learning, error analysis is utilized
    to identify and diagnose erroneous predictions by focusing on the pockets of data
    where the model performed well and poorly. Although the overall performance of
    the model might be 79%, this performance may not be uniform across all pockets
    of the data, and these highs and lows could be due to inputs present in some pockets
    and absent in other pockets of data.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出是否需要更多特征或更多数据，我们利用一种称为错误分析的技术。在机器学习中，错误分析用于通过关注模型表现良好和表现不佳的数据区域来识别和诊断错误的预测。尽管模型的总体性能可能为
    79%，但这种性能可能不会在整个数据区域中均匀分布，这些高低起伏可能是由某些区域存在的输入和在其他区域缺失的输入造成的。
- en: To identify data issues, we will start training the model with 10% of the data,
    and with each iteration add 10%. Then, we plot the training ROC and test the ROC
    concerning the increase in the size of the data. If the plot seems to converge
    and indicate an increase in the size of the data, this will lead to an improvement
    in the test ROC, at which point we will generate synthetic data to increase the
    data’s size. This technique will be covered in [*Chapter 7*](B19297_07.xhtml#_idTextAnchor111)*,
    Using Synthetic Data in Data-Centric* *Machine Learning*.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别数据问题，我们将使用 10% 的数据进行模型训练，并在每次迭代中增加 10%。然后，我们绘制训练 ROC 曲线和测试 ROC 曲线，以测试数据规模增加的情况。如果图表似乎收敛并表明数据规模增加，这将导致测试
    ROC 的提高，此时我们将生成合成数据以增加数据规模。这项技术将在[*第 7 章*](B19297_07.xhtml#_idTextAnchor111)中介绍，*以数据为中心的机器学习中的合成数据使用*。
- en: If the plot doesn’t seem to converge and indicates an increase in data, it will
    have a minimal impact on improving test ROC. In this case, we can observe which
    data points the model performed poorly on, and may utilize feature engineering
    to generate new columns. Although feature engineering can be an iterative approach,
    for the scope of this chapter, we cover adding a feature or two.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 如果图表似乎没有收敛，并表明数据增加，它将对提高测试 ROC 产生最小的影响。在这种情况下，我们可以观察到模型表现不佳的数据点，并可能利用特征工程生成新的列。尽管特征工程可能是一种迭代方法，但就本章的范围而言，我们只涵盖添加一个或两个特征。
- en: 'To run error analysis, first, we create data cutoff points from 0.1 to 1.0,
    where 0.1 means 10% of the training data and 1.0 means 100% of the training data:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行错误分析，首先，我们创建从 0.1 到 1.0 的数据截止点，其中 0.1 表示 10% 的训练数据，1.0 表示 100% 的训练数据：
- en: '[PRE80]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Next, we create an empty list called `scores` and run data preprocessing, model
    training, and evaluation with each cutoff of data. If the cutoff is < 1.0, we
    subset the training data; otherwise, we pass all the data for training. At the
    end of each iteration, we save the cutoff, train, and test evaluation metrics
    in `scores` by appending the metrics to the `scores` list:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个名为 `scores` 的空列表，并对每个数据截止点进行数据预处理、模型训练和评估。如果截止点小于 1.0，我们将子集训练数据；否则，我们将所有数据传递用于训练。在每个迭代结束时，我们将截止点、训练和测试评估指标保存到
    `scores` 中，通过将指标追加到 `scores` 列表：
- en: '[PRE81]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Next, we create a DataFrame from the `scores` list and pass the relevant column
    names:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从 `scores` 列表创建一个 DataFrame，并传递相关的列名：
- en: '[PRE82]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Then, we plot the train and test ROC against each cutoff:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们绘制训练和测试 ROC 与每个截止点的对比图：
- en: '[PRE83]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'This will output the following plot:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下图表：
- en: '![Figure 5.15 – Error analysis train and test ROC](img/B19297_05_21.jpg)'
  id: totrans-322
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.15 – 错误分析训练和测试 ROC](img/B19297_05_21.jpg)'
- en: Figure 5.15 – Error analysis train and test ROC
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 – 错误分析训练和测试 ROC
- en: 'Next, plot the train and test accuracy against each cutoff:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，绘制训练和测试准确率与每个截止点的对比图：
- en: '[PRE84]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'This will output the following plot:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下图表：
- en: '![Figure 5.17 – Error analysis train and test accuracy](img/B19297_05_22.jpg)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.17 – 错误分析训练和测试准确率](img/B19297_05_22.jpg)'
- en: Figure 5.17 – Error analysis train and test accuracy
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 – 错误分析训练和测试准确率
- en: Both the test ROC and test accuracy seem to show signs of convergence with the
    train ROC and train accuracy, which indicates that model performance may be boosted
    if more data points were made available. This is why we will generate synthetic
    data (data that mimics the real data) in the next chapter and retrain the model
    with added data to get better model performance.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 ROC 和测试准确率似乎显示出与训练 ROC 和训练准确率收敛的迹象，这表明如果提供更多的数据点，模型性能可能会得到提升。这就是为什么我们将在下一章生成模拟数据（模仿真实数据的数据）并使用添加的数据重新训练模型以获得更好的模型性能。
- en: As we learned in the previous chapters, one of the principles of data-centric
    machine learning is keeping humans in the loop. Let’s imagine we spoke to the
    domain experts and they mentioned that one of the key determinants for someone
    getting a loan is the income-to-debt ratio – that is, the total income divided
    by the loan amount. This determines if someone will be able to pay back the loan
    or not. An application with a lower income-to-loan ratio is more likely to be
    rejected. In the dataset, there are two income variables – applicant income and
    co-applicant income. Also, the loan amount is represented in thousand figures
    – that is, a loan amount in the data loan amount of 66 represents 66,000\. To
    create this ratio, we will multiply the loan amount by 1,000 and then combine
    the income of the applicant and co-applicant. Once we’ve done this, we will divide
    the combined income by the loan amount to get the income-to-loan ratio. The domain
    experts also mentioned that **equated monthly installments** (**EMIs**) can also
    determine a candidate’s capability to pay the loan. The lower the EMI, the more
    likely a loan will be accepted, whereas the higher the EMI, the more likely a
    loan will be rejected. To calculate this without the interest rate, we can use
    the loan term and loan amount to get an approximate EMI amount for each month.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中学到的，数据为中心的机器学习的原则之一是让人类参与其中。让我们想象我们与领域专家进行了交谈，他们提到，一个人能否获得贷款的关键决定因素之一是收入与债务比率——即总收入除以贷款金额。这决定了一个人是否能够偿还贷款。收入与贷款比率较低的申请更有可能被拒绝。在数据集中，有两个收入变量——申请人收入和共同申请人收入。此外，贷款金额以千位表示——即数据中的贷款金额66代表66,000。为了创建这个比率，我们将贷款金额乘以1,000，然后结合申请人和共同申请人的收入。完成这些后，我们将合并的收入除以贷款金额以获得收入与贷款比率。领域专家还提到，**等额本息还款**（**EMIs**）也可以决定候选人的还款能力。EMI越低，贷款被接受的可能性越大，而EMI越高，贷款被拒绝的可能性越大。为了在没有利率的情况下计算这个值，我们可以使用贷款期限和贷款金额来得到每月的近似EMI金额。
- en: For the income-to-loan ratio, we will create a custom transformer for multiplying
    the loan amount by 1,000 so that we can use it in the pipeline.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 对于收入与贷款比率，我们将创建一个自定义转换器，将贷款金额乘以1,000，以便我们可以在管道中使用它。
- en: This transformer is a Python class that we can use to overload the fit and transform
    functions required by the pipeline. This class will inherit from the `BaseEstimator`
    and `TransformerMixin` classes, both of which can be found in the `sklearn.base`
    module. The class will be used to implement the fit and transform methods. These
    methods should contain `X` and `y` parameters, and the transform method should
    return a pandas DataFrame to ensure compatibility with the scikit-learn pipeline.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换器是一个Python类，我们可以用它来覆盖管道所需的fit和transform函数。这个类将继承自`BaseEstimator`和`TransformerMixin`类，这两个类都可以在`sklearn.base`模块中找到。这个类将用于实现fit和transform方法。这些方法应该包含`X`和`y`参数，transform方法应该返回一个pandas
    DataFrame以确保与scikit-learn管道的兼容性。
- en: To create a full income column, we leverage the `feature_engine` library since
    it is already compatible with the scikit-learn pipeline and has methods to apply
    mathematical operations relative to other variables. First, we sum the income
    variables. The output of that transformation will be divided by the `loan_amount`
    variable to create the income-to-loan ratio.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建完整的收入列，我们利用`feature_engine`库，因为它已经与scikit-learn管道兼容，并且有应用于其他变量的数学运算方法。首先，我们求和收入变量。这个转换的输出将除以`loan_amount`变量以创建收入与贷款比率。
- en: To create the EMI, we leverage the `feature_engine` library and divide `loan_amount`
    with `loan_amount_term`. Once we have created these features, we remove the two
    income variables since we already created a combination of the two. For this step,
    we use the `DropFeatures` class from the `feature_engine` library. All these feature
    engineering steps will be combined in a new pipeline called `feature-transformer`
    and will be applied post-data imputation.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建EMI，我们利用`feature_engine`库，将`loan_amount`除以`loan_amount_term`。一旦我们创建了这些特征，我们就移除了两个收入变量，因为我们已经创建了这两个变量的组合。对于这一步，我们使用`feature_engine`库中的`DropFeatures`类。所有这些特征工程步骤将组合在一个新的名为`feature-transformer`的管道中，并在数据插补后应用。
- en: We believe that by adding these extra features, the model performance of the
    decision tree algorithm will improve. Let’s run the algorithm post-feature engineering
    and evaluate the results.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 我们相信，通过添加这些额外特征，决策树算法的模型性能将得到提高。让我们在特征工程后运行算法并评估结果。
- en: 'First, we create custom variables that will take in a list of variables for
    the feature engineering steps:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建用于特征工程步骤的自定义变量，它将接受一个变量列表：
- en: '[PRE85]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Next, we import relevant packages from `feature_engine` to perform the feature
    engineering steps and import the `BaseEstimator` and `TransformerMixin` classes:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从 `feature_engine` 导入相关包以执行特征工程步骤，并导入 `BaseEstimator` 和 `TransformerMixin`
    类：
- en: '[PRE86]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Then, we create a custom transformer that will take in variable names and a
    value that will be multiplied by each variable. By default, each variable will
    be multiplied by 1:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个自定义转换器，它将接受变量名和一个将被每个变量乘以的值。默认情况下，每个变量将被乘以 1：
- en: '[PRE87]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Next, we call the `missForest` categorical and numerical transformers we created
    previously. Once we’ve done this, we create a feature transformer pipeline that
    multiplies `loan_amount` by 1,000 by leveraging the custom transformer we created
    previously. The new pipeline then adds income variables to create one income variable,
    the income-to-loan ratio, and the EMI features. Finally, the pipeline drops the
    two income variables since the new income variable will be created. By using the
    transformer pipelines, the train and test data will be transformed, and new features
    will be created. The output of this step will be fully transformed into train
    and test data so that it can be passed to the custom classifier:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调用之前创建的 `missForest` 类别和数值转换器。完成此操作后，我们创建一个特征转换器管道，利用之前创建的自定义转换器将 `loan_amount`
    乘以 1,000。新的管道然后将收入变量添加到一个收入变量中，即收入与贷款比率，以及 EMI 特征。最后，管道删除了两个收入变量，因为将创建新的收入变量。通过使用转换器管道，训练和测试数据将被转换，并创建新特征。这一步骤的输出将完全转换为训练和测试数据，以便可以传递给自定义分类器：
- en: '[PRE88]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Next, we create the categorical transformers for imputation:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建用于插补的类别转换器：
- en: '[PRE89]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Then, we add the numerical imputation and complete the imputation steps:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们添加数值插补并完成插补步骤：
- en: '[PRE90]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Next, we transform the imputed data using the feature imputation pipeline we
    created previously:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前创建的特征插补管道对缺失数据进行转换：
- en: '[PRE91]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'At this point, we call the custom classifier function to evaluate model performance
    with added feature engineering steps:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们调用自定义分类器函数，通过添加特征工程步骤来评估模型性能：
- en: '[PRE92]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'Next, we call the confusion matrix:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调用混淆矩阵：
- en: '[PRE93]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The resulting confusion matrix is as follows:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的混淆矩阵如下：
- en: '![Figure 5.18 – Confusion matrix when using custom feature engineering](img/B19297_05_23.jpg)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.18 – 使用自定义特征工程时的混淆矩阵](img/B19297_05_23.jpg)'
- en: Figure 5.18 – Confusion matrix when using custom feature engineering
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 – 使用自定义特征工程时的混淆矩阵
- en: Our test accuracy has increased from 79% to 82% and the ROC has been boosted
    from 78.5% to 81.8%. The preceding confusion matrix shows that the accuracy of
    the positive class has been boosted from 88% to 91%, while the accuracy of the
    negative class has been boosted from 58% to 63%.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的测试准确率从 79% 提高到 82%，ROC 从 78.5% 提高到 81.8%。前面的混淆矩阵显示，正类准确率从 88% 提高到 91%，而负类准确率从
    58% 提高到 63%。
- en: With this, we have demonstrated that by using a data-centric approach, we can
    iterate over the data rather than iterate over multiple algorithms, and manage
    to improve model performance. We will explore how to add some synthetic data and
    boost model performance even further in the next chapter.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种方式，我们已经证明了通过使用以数据为中心的方法，我们可以遍历数据而不是遍历多个算法，并设法提高模型性能。我们将在下一章探讨如何添加一些合成数据，并进一步提高模型性能。
- en: Ensuring that the data is valid
  id: totrans-359
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保数据有效
- en: So far, we have ensured that our data is consistent, unique, and complete. But
    do we know if the data we have is valid? Do the data labels conform to the rules?
    For example, what if the property area in the dataset didn’t conform to the rules
    and `semi_urban` is invalid? What if one or a couple of annotators believed some
    suburbs are neither urban nor rural, and they violated the rules and entered `semi_urban`?
    To measure validity, we may need to look at business rules and check the percentage
    of data that conforms to these business rules. Let’s assume that `semi_urban`
    is an invalid value. In Python, we could check the percentage of invalid labels
    and then reach out to annotators to correct the data. We could also achieve this
    by using the data that was used to generate the label. If we had the `suburb_name`
    to `property_area` data mapping, and `suburb_name` was available in the dataset,
    then we could leverage the mapping and catch invalid values, as well as encoding
    labels programmatically. Building business rules in the system so that upcoming
    data is automatically encoded is referred to as programmatic labeling. We will
    dive into programmatic labeling in the upcoming chapters, where we explore techniques
    to make data consistent and valid at the time of data capture, so that when the
    data comes in, it’s already pristine and some of the data cleaning process will
    be redundant.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经确保我们的数据是一致的、唯一的和完整的。但我们是否知道我们拥有的数据是有效的？数据标签是否符合规则？例如，如果数据集中的属性区域不符合规则，`semi_urban`是无效的怎么办？如果一位或几位标注者认为一些郊区既不是城市也不是乡村，并且违反了规则，输入了`semi_urban`怎么办？为了衡量有效性，我们可能需要查看业务规则并检查符合这些业务规则的数据百分比。让我们假设`semi_urban`是一个无效值。在Python中，我们可以检查无效标签的百分比，然后联系标注者纠正数据。我们也可以通过使用用于生成标签的数据来实现这一点。如果我们有`suburb_name`到`property_area`的数据映射，并且`suburb_name`在数据集中可用，那么我们可以利用这个映射来捕获无效值，以及通过编程方式编码标签。在系统中构建业务规则，以便自动编码即将到来的数据，这被称为编程标签。我们将在接下来的章节中深入探讨编程标签，我们将探讨在数据捕获时使数据一致和有效的技术，这样当数据到来时，它已经是纯净的，并且一些数据清理过程将是多余的。
- en: 'First, we make a fake dataset that consists of 10 rows and write a business
    rule. It will contain an `id` column with values from 1 to 10, a `population`
    column with 10 random values between 1,000 and 100,000, and a `property_area`
    column with four values set to `urban`, five values set to `semi_urban`, and one
    value set to `rural`:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建一个包含10行数据的假数据集，并编写一个业务规则。它将包含一个`id`列，其值为1到10，一个`population`列，包含1,000到100,000之间的10个随机值，以及一个`property_area`列，其中四个值设置为`urban`，五个值设置为`semi_urban`，一个值设置为`rural`：
- en: '[PRE94]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Next, we print the first five rows:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们打印前五行：
- en: '[PRE95]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: Imagine that the business rule says a suburb or `property_area` is classified
    as urban when the population is more than 20,000; otherwise, it’s classified as
    rural. In this case, the validation rule should check that `property_area` only
    contains `urban` or `rural` values.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 假设业务规则说，当人口超过20,000时，郊区或`property_area`被归类为城市；否则，被归类为乡村。在这种情况下，验证规则应该检查`property_area`是否只包含`urban`或`rural`值。
- en: 'A simple way to check this in Python is to use the `value_counts()` method
    alongside the `normalize=True` parameter. The output of this will show that 50%
    of the data is invalid:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中检查这一点的简单方法是在`value_counts()`方法旁边使用`normalize=True`参数。这个输出的结果将显示50%的数据是无效的：
- en: '[PRE96]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Next, we can run the check against each row and flag when the value is in the
    expected list of values, as well as when the value is not in the expected set
    of values:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以对每一行运行检查，并在值位于预期值列表中时标记，以及当值不在预期值集中时标记：
- en: '[PRE97]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Now, we sum the rows where the data validation rule was breached and divide
    the number of invalid rows by the total rows to provide a metric – that is, the
    percentage of invalid labels:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将违反数据验证规则的数据行求和，并将无效行数除以总行数，以提供一个指标——即无效标签的百分比：
- en: '[PRE98]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Invalid data must be communicated to the source data providers and must be cleaned;
    otherwise, this data can creep in and the machine learning model will learn about
    these invalid labels. The model's learning capability significantly improves when
    trained with valid data, which provides stronger label signals, as opposed to
    invalid data that weakens these signals due to reduced exposure to valid label
    points.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 无效数据必须通知数据源提供者，并且必须进行清理；否则，这些数据可能会悄悄进入，机器学习模型将学习这些无效标签。当使用有效数据进行训练时，模型的学习能力显著提高，因为它提供了更强的标签信号，与无效数据相比，无效数据由于对有效标签点的接触减少而削弱了这些信号。
- en: Ensuring that the data is accurate
  id: totrans-373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保数据准确
- en: Even though the data is valid, it may not be accurate. Data accuracy measures
    the percentage of data that matches real-world data or verifiable sources. Considering
    the preceding example of the property area, to measure data accuracy, we may have
    to look up a reliable published dataset and check the population of the area and
    the type of the area. Let’s assume that the population matches the verifiable
    data source, but the area type source is unavailable. Using the rule of what defines
    a rural area and what defines an urban area, we can measure data accuracy.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 即使数据是有效的，它可能也不准确。数据准确性衡量的是与真实世界数据或可验证来源匹配的数据百分比。考虑到前面的`property_area`示例，为了衡量数据准确性，我们可能需要查找一个可靠的已发布数据集，并检查该地区的人口和地区类型。假设人口与可验证的数据源匹配，但地区类型来源不可用。使用定义农村地区和城市地区的规则，我们可以衡量数据准确性。
- en: 'Using this business rule, we will create a new label called `true_property_area`
    that takes `rural` as a value when the population is 20,000 or below; otherwise,
    takes `urban` as a value:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个业务规则，我们将创建一个新的标签`true_property_area`，当人口在20,000人或以下时，其值为`rural`；否则，其值为`urban`：
- en: '[PRE99]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'Next, we print the rows of the dataset to see if there are any mismatches between
    `property_area` and `true_property_area`:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将打印数据集的行以查看`property_area`和`true_property_area`之间是否存在任何不匹配：
- en: '[PRE100]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'Then, we sum the rows where `property_area` values match with the true values
    and divide this by the total number of rows to calculate data accuracy:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将匹配`property_area`值与真实值的行求和，然后除以总行数以计算数据准确性：
- en: '[PRE101]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Instead of creating a function to calculate the accuracy, we can leverage `accuracy_score`
    from scikit-learn:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不创建一个函数来计算准确性，而是利用scikit-learn中的`accuracy_score`：
- en: '[PRE102]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: As we can see, both methods return the same score. If inaccurate data enters
    the system, the model may learn inaccurately about semi-urban and rural areas,
    and at the time of inference, produce undesirable outcomes.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，两种方法都返回了相同的分数。如果错误的数据进入系统，模型可能会对半城市和农村地区学习不准确，并在推理时产生不理想的结果。
- en: Ensuring that the data is fresh
  id: totrans-384
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 确保数据新鲜
- en: Data freshness is another important aspect of measuring data quality that has
    an impact on the quality and robustness of machine learning applications. Let’s
    imagine that we have a machine learning application that’s been trained on 2019
    and 2020 customer behavior and utilized to predict hotel room bookings up to April
    2021\. Maybe January and February numbers were quite accurate, but when March
    and April hit, accuracy dropped. This might have been due to COVID-19, something
    that was unseen by the data, and its effects were not captured. In machine learning,
    this is called data drift. This is happening here; the data distribution in March
    and April was quite different from the data distribution in 2019 and 2020\. By
    ensuring that the data is fresh and up to date, we can train the model more regularly
    or as soon as data drift is detected.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 数据新鲜度是衡量数据质量的一个重要方面，它对机器学习应用的质量和鲁棒性有影响。让我们想象一下，我们有一个在2019年和2020年客户行为上训练的机器学习应用，并用于预测到2021年4月的酒店房间预订。也许一月份和二月份的数字相当准确，但当三月份和四月份到来时，准确性下降。这可能是由于COVID-19，这是数据未看到的情况，其影响没有被捕捉到。在机器学习中，这被称为数据漂移。这种情况正在发生；三月份和四月份的数据分布与2019年和2020年的数据分布有很大不同。通过确保数据新鲜且更新，我们可以更频繁地训练模型，或者在检测到数据漂移时立即训练。
- en: To measure data drift, we will use the `alibi` Python package. However, there
    are more extensive Python packages that can help with this job. We recommend Evidently
    AI ([https://www.evidentlyai.com/](https://www.evidentlyai.com/)), a data and
    machine learning model monitoring toolkit, or WhyLogs ([https://whylabs.ai/whylogs](https://whylabs.ai/whylogs)),
    an open source initiative by WhyLabs, to monitor model degradation and data drift.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测量数据漂移，我们将使用`alibi` Python包。然而，还有更多广泛的Python包可以帮助完成这项工作。我们推荐Evidently AI ([https://www.evidentlyai.com/](https://www.evidentlyai.com/))，这是一个数据与机器学习模型监控工具包，或者WhyLogs
    ([https://whylabs.ai/whylogs](https://whylabs.ai/whylogs))，这是WhyLabs的一个开源倡议，用于监控模型降级和数据漂移。
- en: Let’s imagine that, on average, model accuracy starts degrading when the model
    is trained on data that is more than 5 days old and the model does poorly and
    starts costing the business when data is more than 10 days old. We want to be
    able to alert and capture when this happens. To demonstrate this scenario, we
    will create a sample dataset with a date column and define error and warning thresholds
    – that is, we print a warning if the data is 5 days old and block the application
    if the data is more than 10 days old. In practice, it is recommended to train
    the model on the most recent data available. Following a data-centric approach,
    we must encourage practitioners to have thresholds and **service-level agreements**
    (**SLAs**) defined with the providers of data so that they have mechanisms in
    place to request up-to-date data, penalize when SLAs are breached, and maybe reward
    when SLAs are met (encourage the importance of maintaining good-quality data).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们假设，当模型在超过5天的旧数据上训练时，模型准确性开始下降，而当数据超过10天时，模型表现不佳并开始给企业带来成本。我们希望能够在这种情况发生时发出警报并捕获它。为了演示这个场景，我们将创建一个包含日期列的样本数据集，并定义错误和警告阈值——也就是说，如果数据是5天前的，我们打印警告；如果数据超过10天，我们阻止应用程序。在实践中，建议使用最新的可用数据进行模型训练。遵循以数据为中心的方法，我们必须鼓励从业者与数据提供者定义阈值和**服务级别协议**（**SLAs**），以便他们有机制来请求最新的数据，当SLA被违反时进行处罚，当SLA得到满足时进行奖励（鼓励保持高质量数据的重要性）。
- en: Now we will generate 100 sample data points and demonstrate how to identify
    if data is stale using a date variable.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将生成100个样本数据点，并演示如何使用日期变量来识别数据是否过时。
- en: We utilize the `alibi` package to detect drift in the `loan_prediction` dataset.
    We will demonstrate the pitfalls of not detecting and acting on data drift by
    comparing accuracy before and after drift.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`alibi`包来检测`loan_prediction`数据集中的漂移。我们将通过比较漂移前后的准确率来展示不检测和采取数据漂移措施的危害。
- en: 'First, we import the `datetime` and `warning` packages:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入`datetime`和`warning`包：
- en: '[PRE103]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: 'Next, we generate a base date – let’s say the date when we run the code – and
    from the base date, generate 100 more dates in the past by subtracting one day
    at a time:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们生成一个基准日期——比如说我们运行代码的日期——然后从基准日期开始，通过每天减去一天来生成100个过去的日期：
- en: '[PRE104]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'Then, we print the first 10 dates in the order these dates were generated,
    with the most recent being the first date:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们按生成日期的顺序打印前10个日期，最近的日期是第一个日期：
- en: '[PRE105]'
  id: totrans-395
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: 'Next, we create a DataFrame with 100 rows by creating four columns. It will
    contain an `id` column with values from 1 to 100, a `date_loaded` column that
    contains the 100 dates we created previously, a `population` column with 100 random
    values between 1,000 and 100,000, and a `property_area` column with 40 values
    set to `urban`, 50 values set to `semi_urban,` and 10 values set to `rural`:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含100行的DataFrame，通过创建四个列来实现。它将包含一个`id`列，其值为1到100，一个`date_loaded`列，包含我们之前创建的100个日期，一个`population`列，包含100个介于1,000到100,000之间的随机值，以及一个`property_area`列，其中40个值设置为`urban`，50个值设置为`semi_urban`，10个值设置为`rural`：
- en: '[PRE106]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: 'Now, we visualize the first five data points:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可视化前五个数据点：
- en: '[PRE107]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: 'Next, we create a one-line piece of code to demonstrate a way to subtract any
    date from today’s date and extract the number of days between the dates:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们编写一行代码来演示从今天日期减去任何日期并提取两个日期之间天数的方法：
- en: '[PRE108]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Then, we create a function that will accept a DataFrame and the date column
    of the DataFrame, and by default will issue a warning if data is more than 5 days
    old and block the application when data is more than 10 days old:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个函数，该函数将接受一个DataFrame及其日期列，默认情况下，如果数据超过5天，将发出警告；如果数据超过10天，将阻止应用程序：
- en: '[PRE109]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'Next, we run the function with the sample DataFrame we created previously.
    The function will state that the data is fresh and just 0 days old:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用之前创建的样本DataFrame运行该函数。该函数将声明数据是新鲜的，只有0天：
- en: '[PRE110]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: To demonstrate the function’s ability to issue a warning or error out when data
    is stale, we subset the data by removing 6 recent days and 12 recent days. We
    create two DataFrames – one that has 6 recent days removed and another that has
    12 recent days removed. Then, we run the `check_data_recency_day` function over
    these DataFrames. We see that when we run the function with 6-day-old data, the
    function will issue the warning, but when we run the function with 12-day-old
    data, the function will issue a `Value` error.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示函数在数据过时时的警告或错误输出能力，我们通过删除6天和12天内的数据来对数据进行子集划分。我们创建了两个DataFrame——一个删除了6天内的数据，另一个删除了12天内的数据。然后，我们在这些DataFrame上运行`check_data_recency_day`函数。我们看到，当我们用6天前的数据运行函数时，函数将发出警告，但当我们用12天前的数据运行函数时，函数将发出一个`Value`错误。
- en: 'Let’s create the two DataFrames:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建两个DataFrame：
- en: '[PRE111]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'Next, we run the function against the data that is 6 days old:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们对6天前的数据进行函数运行：
- en: '[PRE112]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: You can also run the function against the data that is 12 days old; it will
    generate similar output.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以对12天前的数据进行函数运行；它将生成类似的输出。
- en: With that, we have demonstrated how to measure data staleness, catch warnings,
    and block the application when data is extremely stale. Next, we will demonstrate
    the impact of data staleness on a real-life dataset.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些，我们已经展示了如何测量数据的新鲜度，捕捉警告，并在数据极度过时时阻止应用程序。接下来，我们将展示数据新鲜度对实际数据集的影响。
- en: In real life, we wouldn’t expect a company not to change its products or for
    consumer behavior not to change as newer products become available on the market.
    Companies have to constantly study changes in consumer behavior; otherwise, their
    performance degrades. Machine learning systems face the same issues as market
    forces change, data changes, and data distributions change. This has an impact
    on a model’s performance if new data is quite different from the data it was trained
    on.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，我们不会期望一家公司不改变其产品，或者消费者行为不会随着市场上新产品的出现而改变。公司必须不断研究消费者行为的变化；否则，他们的业绩会下降。机器学习系统面临着市场力量变化、数据变化和数据分布变化相同的问题。如果新数据与训练数据大相径庭，这将对模型性能产生影响。
- en: This is referred to as drift in machine learning and if it goes undetected and
    untreated, it can cause models to degrade.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 这在机器学习中被称为漂移，如果未检测到且未得到处理，它会导致模型退化。
- en: Let’s explore how to detect drift.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索如何检测漂移。
- en: 'First, we import `TabularDrift` from the `alibi-detect` package:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们从`alibi-detect`包中导入`TabularDrift`：
- en: '[PRE113]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'Next, we show the `TabularDrift` reference data, which is the data the machine
    learning system was trained on – in our case, the loan prediction transformed
    data before we passed it to the decision tree classifier. We also pass a value
    of `0.05` for the p-value test. If this value is breached by the test data distribution,
    the package will inform us that the test data has drifted from the training data:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展示`TabularDrift`参考数据，这是机器学习系统训练的数据——在我们的案例中，是在我们将数据传递给决策树分类器之前转换的贷款预测数据。我们还为p值测试传递了一个值为`0.05`的值。如果测试数据分布违反了此值，该包将通知我们测试数据已从训练数据中漂移：
- en: '[PRE114]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'Now, we run the `predict` method to check if the test data has drifted. The
    `alibi` package utilizes the `Kolmogorov-Smirnov` test to determine if the two
    distributions differ. If the p-value exceeds 0.05, then the null hypothesis is
    rejected and it can be inferred that the `test` data distribution differs from
    the `train` data distribution. The output for this step will be `No`:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们运行`predict`方法来检查测试数据是否发生了漂移。`alibi`包使用`Kolmogorov-Smirnov`测试来确定两个分布是否不同。如果p值超过0.05，则拒绝零假设，可以推断出`test`数据分布与`train`数据分布不同。这一步骤的输出将是`No`：
- en: '[PRE115]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'Now, let’s imagine that house prices started booming while incomes did not
    increase at the same rate. To simulate this scenario, we will increase the loan
    amount requested by 1.5 times the original test set, but increase the total income
    by 1.2 times the test set. Then, we update the new feature values that relied
    on the `loan_amount` and `income` variables:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们假设房价开始飙升，而收入并没有以相同的速度增长。为了模拟这种情况，我们将贷款金额增加到原始测试集的1.5倍，但将总收入增加到测试集的1.2倍。然后，我们更新依赖于`loan_amount`和`income`变量的新特征值：
- en: '[PRE116]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: 'Next, we rerun TabularDrift’s `predict` method to check whether drift was detected.
    The output of this step is `Yes`:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们再次运行TabularDrift的`predict`方法来检查是否检测到漂移。这一步骤的输出是`Yes`：
- en: '[PRE117]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: 'Then, we rerun the prediction on this drift-induced test data and check whether
    accuracy and ROC are affected:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们对由漂移引起的测试数据进行重新预测，并检查准确率和ROC是否受到影响：
- en: '[PRE118]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: As we can see, the distribution that was seen by the model when it was trained
    is different from the real data, and the impact is that model performance has
    degraded significantly. The ROC has dropped from 0.82 to 0.74 and accuracy has
    dropped from 82% to 70%. Hence, it’s important to ensure that data is fresh and
    that as soon as drift is detected, the model is retrained with new data to ensure
    model performance does not deteriorate.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，模型在训练过程中所看到的分布与真实数据不同，其影响是模型性能显著下降。ROC值从0.82降至0.74，准确率从82%降至70%。因此，确保数据新鲜非常重要，一旦检测到数据漂移，就需要用新数据重新训练模型，以确保模型性能不会下降。
- en: Summary
  id: totrans-429
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we gained a good understanding of the six key dimensions of
    data quality and why it’s important to improve data quality for superior model
    performance. We further dived into the data-centric approach of improving model
    performance by iterating over the data, rather than iterating over various algorithms
    (model-centric approach), by improving the overall health of the data.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入了解了数据质量的六个关键维度以及为什么提高数据质量对于提高模型性能至关重要。我们进一步探讨了通过迭代数据来提高模型性能的数据中心方法，而不是迭代各种算法（模型中心方法），通过提高数据的整体健康状况来实现。
- en: Next, we learned how to ensure data is consistent, unique, accurate, valid,
    fresh, and complete. We dived into various techniques of imputing missing values
    and when to apply which approach. We concluded that imputing missing values with
    machine learning can be better than using simple imputation methods, especially
    when data is MAR or MNAR. We also showed how to conduct error analysis and how
    to use the results to further improve model performance by either performing feature
    engineering, which involves building new features, or increasing the data size
    by creating synthetic data, which we will cover in the next chapter.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们学习了如何确保数据的一致性、唯一性、准确性、有效性、新鲜性和完整性。我们深入探讨了各种填充缺失值的技巧以及何时应用哪种方法。我们得出结论，使用机器学习填充缺失值可能比使用简单的填充方法更好，尤其是在数据是MAR或MNAR的情况下。我们还展示了如何进行错误分析，以及如何利用这些结果通过执行特征工程（涉及构建新特征）或通过创建合成数据来增加数据量，从而进一步提高模型性能，这些内容将在下一章中介绍。
- en: We also discussed why is it important to ensure data is fresh and not drifted
    from the original training set, and concluded that drifted data can hamper model
    performance.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还讨论了为什么确保数据新鲜且未从原始训练集中漂移很重要，并得出结论，漂移数据可能会损害模型性能。
- en: Now that we have understood the importance of ensuring good-quality data across
    the six key dimensions of data quality, in the next chapter, we will dive into
    using synthetic data to further improve model performance, especially over edge
    cases. We will also dive into data augmentation, a technique that’s used to create
    synthetic data for images so that algorithms can learn from more and better, especially
    when these new examples can come in various forms.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了确保数据质量在数据质量的六个关键维度中的重要性，在下一章中，我们将深入探讨使用合成数据来进一步提高模型性能，特别是在边缘情况中。我们还将深入探讨数据增强技术，这是一种用于为图像创建合成数据的技术，以便算法可以从更多更好的数据中学习，尤其是在这些新示例可以以各种形式出现时。
