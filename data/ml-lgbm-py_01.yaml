- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Introducing Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍机器学习
- en: Our journey starts with an introduction to machine learning and the fundamental
    concepts we’ll use throughout this book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的旅程从机器学习的介绍和本书中我们将使用的根本概念开始。
- en: 'We’ll start by providing an overview of machine learning from a software engineering
    perspective. Then, we’ll introduce the core concepts that are used in the field
    of machine learning and data science: models, datasets, learning paradigms, and
    other details. This introduction will include a practical example that clearly
    illustrates the machine learning terms discussed.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从软件工程的角度提供一个机器学习的概述。然后，我们将介绍机器学习和数据科学领域使用的核心概念：模型、数据集、学习范式以及其他细节。这个介绍将包括一个实际例子，清楚地说明了讨论的机器学习术语。
- en: We will also introduce decision trees, a crucially important machine learning
    algorithm that is our first step to understanding LightGBM.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将介绍决策树，这是一个至关重要的机器学习算法，是我们理解LightGBM的第一步。
- en: After completing this chapter, you will have established a solid foundation
    in machine learning and the practical application of machine learning techniques.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章后，您将在机器学习和机器学习技术的实际应用方面打下坚实的基础。
- en: 'The following main topics will be covered in this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: What is machine learning?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Introducing models, datasets, and supervised learning
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍模型、数据集和监督学习
- en: Decision tree learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树学习
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter includes examples of simple machine learning algorithms and introduces
    working with scikit-learn. You must install a Python environment with scikit-learn,
    NumPy, pandas, and Jupyter Notebook. The code for this chapter is available at
    [https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-1](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-1).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包括简单的机器学习算法示例，并介绍了使用scikit-learn。您必须安装一个带有scikit-learn、NumPy、pandas和Jupyter
    Notebook的Python环境。本章的代码可在[https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-1](https://github.com/PacktPublishing/Practical-Machine-Learning-with-LightGBM-and-Python/tree/main/chapter-1)找到。
- en: What is machine learning?
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: Machine learning is a part of the broader artificial intelligence field that
    involves methods and techniques that allow computers to “learn” specific tasks
    without explicit programming.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是更广泛的人工智能领域的一部分，涉及允许计算机“学习”特定任务而无需明确编程的方法和技术。
- en: Machine learning is just another way to write programs, albeit automatically,
    from data. Abstractly, a program is a set of *instructions* that transforms *inputs*
    into specific *outputs*. A programmer’s job is to understand all the relevant
    inputs to a computer program and develop a set of instructions to produce the
    correct outputs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习只是另一种从数据中自动编写程序的方式。抽象地说，一个程序是一系列将*输入*转换为特定*输出*的*指令*。程序员的任务是理解计算机程序的所有相关输入，并开发一套指令以产生正确的输出。
- en: However, what if the inputs are beyond the programmer’s understanding?
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果输入超出了程序员的认知范围怎么办呢？
- en: For example, let’s consider creating a program to forecast the total sales of
    a large retail store. The inputs to the program would be various factors that
    could affect sales. We could imagine factors such as historical sales figures,
    upcoming public holidays, stock availability, any special deals the store might
    be running, and even factors such as the weather forecast or proximity to other
    stores.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们考虑创建一个程序来预测大型零售店的总体销售额。程序的输入将是可能影响销售的各种因素。我们可以想象的因素包括历史销售数据、即将到来的公共假日、库存可用性、商店可能进行的任何特别优惠，甚至包括天气预报或与其他商店的邻近程度等因素。
- en: In our store example, the traditional approach would be to break down the inputs
    into manageable, understandable (by a programmer) pieces, perhaps consult an expert
    in store sales forecasting, and then devise handcrafted rules and instructions
    to attempt to forecast future sales.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的商店例子中，传统的方法是将输入分解成可管理的、可理解的（由程序员理解）部分，也许可以咨询一位商店销售预测方面的专家，然后制定手工定制的规则和指令来尝试预测未来的销售。
- en: While this approach is certainly possible, it is also brittle (in the sense
    that the program might have to undergo extensive changes regarding the input factors)
    and wholly based on the programmer’s (or domain expert’s) understanding of the
    problem. With potentially thousands of factors and billions of examples, this
    problem becomes untenable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这种方法当然可行，但它也很脆弱（从程序可能需要经历关于输入因素的广泛变化的角度来看）并且完全基于程序员（或领域专家）对问题的理解。面对可能成千上万的因素和数十亿个示例，这个问题变得难以承受。
- en: Machine learning offers us an alternative to this approach. Instead of creating
    rules and instructions, we repeatedly show the computer examples of the tasks
    we need to accomplish and then get it to figure out how to solve them automatically.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习为我们提供了这种方法的替代方案。不是创建规则和指令，我们反复向计算机展示我们需要完成的任务的示例，然后让它自己找出如何自动解决这些问题。
- en: However, where we previously had a set of instructions, we now have a **trained
    model** instead of a programmed one.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们之前有一组指令，现在我们有一个**训练好的模型**而不是编程的模型。
- en: 'The key realization here, especially if you are coming from a software background,
    is that our machine learning program still functions like a regular program: it
    accepts input, has a way to process it, and produces output. Like all other software
    programs, machine learning software must be tested for correctness, integrated
    into other systems, deployed, monitored, and optimized. Collectively, this forms
    the field of *machine learning engineering*. We’ll cover all these aspects and
    more in later chapters.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的一个关键认识，尤其是如果你来自软件背景，是我们的机器学习程序仍然像一个常规程序一样运行：它接受输入，有处理它的方法，并产生输出。像所有其他软件程序一样，机器学习软件必须经过正确性测试，集成到其他系统中，部署、监控和优化。所有这些共同构成了*机器学习工程*这一领域。我们将在后面的章节中涵盖所有这些方面以及更多内容。
- en: Machine learning paradigms
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习范式
- en: 'Broadly speaking, machine learning has three main paradigms: supervised, unsupervised,
    and reinforcement learning.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 广义而言，机器学习有三个主要范式：监督学习、无监督学习和强化学习。
- en: 'With **supervised learning**, the model is trained on labeled data: each instance
    in the dataset has its associated correct output, or label, for the input example.
    The model is expected to learn to predict the label for unseen input examples.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在**监督学习**中，模型在标记数据上训练：数据集中的每个实例都有其关联的正确输出，或标签，对于输入示例。模型预计会学习预测未见输入示例的标签。
- en: With **unsupervised learning**, the examples in the dataset are unlabeled; in
    this case, the model is expected to discover patterns and relationships in the
    data. Examples of unsupervised approaches are clustering algorithms, anomaly detection,
    and dimensionality reduction algorithms.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在**无监督学习**中，数据集中的示例是无标签的；在这种情况下，模型预计会在数据中发现模式和关系。无监督方法的例子包括聚类算法、异常检测和降维算法。
- en: Finally, **reinforcement learning** entails a model, usually called an agent,
    interacting with a particular environment and learning by receiving penalties
    or rewards for specific actions. The goal is for the agent to perform actions
    that maximize its reward. Reinforcement learning is widely used in robotics, control
    systems, or training computers to play games.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，**强化学习**涉及一个模型，通常称为代理，与特定环境交互，并通过接收特定行为的惩罚或奖励来学习。目标是让代理执行最大化其奖励的行为。强化学习在机器人学、控制系统或训练计算机玩游戏方面得到了广泛应用。
- en: LightGBM and most other algorithms discussed later in this book are examples
    of supervised learning techniques and are the focus of this book.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: LightGBM和本书后面讨论的大多数其他算法是监督学习技术的例子，也是本书的重点。
- en: The following section dives deeper into the machine learning terminology we’ll
    use throughout this book and the details of the machine learning process.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 以下章节将深入探讨本书中我们将使用的机器学习术语以及机器学习过程的细节。
- en: Introducing models, datasets, and supervised learning
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍模型、数据集和监督学习
- en: In the previous section, we introduced a model as a construct to replace a set
    of instructions that typically comprise a program to perform a specific task.
    This section covers models and other core machine learning concepts in more detail.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了一个模型作为替代一组指令的构建，这组指令通常构成一个程序以执行特定任务。本节更详细地介绍了模型和其他核心机器学习概念。
- en: Models
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型
- en: More formally, a model is a mathematical or algorithmic representation of a
    specific process that performs a particular task. A machine learning model learns
    a particular task by being trained on a **dataset** using a **training algorithm**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，模型是对执行特定任务的特定过程的数学或算法表示。机器学习模型通过使用**训练算法**在**数据集**上训练来学习特定任务。
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: An alternative term for training is **fit**. Historically, fit stems from the
    statistical field. A model is said to “fit the data” when trained. We’ll use both
    terms interchangeably throughout this book.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的另一个术语是**拟合**。从历史上看，拟合起源于统计学领域。当模型被训练时，我们说模型“拟合数据”。在这本书中，我们将这两个术语交替使用。
- en: Many distinct types of models exist, all of which use different mathematical,
    statistical, or algorithmic techniques to model the training data. Examples of
    machine learning algorithms include linear regression, logistic regression, decision
    trees, support vector machines, and neural networks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 存在许多不同类型的模型，它们都使用不同的数学、统计或算法技术来模拟训练数据。机器学习算法的例子包括线性回归、逻辑回归、决策树、支持向量机和神经网络。
- en: 'A distinction is made between the model type and a trained instance of that
    model: the majority of machine learning models can be trained to perform various
    tasks. For example, decision trees (a model type) can be trained to forecast sales,
    recognize heart disease, and predict football match results. However, each of
    these tasks requires a different *instance* of a decision tree that has been trained
    on a distinct dataset.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型类型和该模型的训练实例之间做出了区分：大多数机器学习模型都可以训练以执行各种任务。例如，决策树（一种模型类型）可以训练来预测销售、识别心脏病和预测足球比赛结果。然而，每个这些任务都需要一个不同的*实例*的决策树，该决策树是在不同的数据集上训练的。
- en: What a specific model does depends on the model’s **parameters**. Parameters
    are also sometimes called **weights**, which are technically particular types
    of model parameters.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特定模型做什么取决于模型的**参数**。参数有时也被称为**权重**，在技术上，它们是模型参数的特定类型。
- en: A **training algorithm** is an algorithm for finding the most appropriate model
    parameters for a specific task.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练算法**是用于找到特定任务最合适的模型参数的算法。'
- en: We determine the quality of fit, or how well the model performs, using an **objective
    function**. This is a mathematical function that measures the difference between
    the predicted output and the actual output for a given input. The objective function
    quantifies the performance of a model. We may seek to minimize or maximize the
    objective function depending on the problem we are solving. The objective is often
    measured as an error we aim to minimize during training.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用**目标函数**来确定拟合质量，即模型的表现如何。这是一个数学函数，它衡量给定输入的预测输出和实际输出之间的差异。目标函数量化了模型的表现。根据我们正在解决的问题，我们可能寻求最小化或最大化目标函数。目标通常在训练过程中作为我们试图最小化的错误来衡量。
- en: 'We can summarize the model training process as follows: a training algorithm
    uses data from a dataset to optimize a model’s parameters for a particular task,
    as measured through an objective function.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型训练过程总结如下：训练算法使用数据集的数据来优化模型参数以完成特定任务，这是通过目标函数来衡量的。
- en: Hyperparameters
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超参数
- en: 'While a model is composed of parameters, the training algorithm has parameters
    of its own called **hyperparameters**. A hyperparameter is a controllable value
    that influences the training process or algorithm. For example, consider finding
    the minimum of a parabola function: we could start by guessing a value and then
    take small steps in the direction that minimizes the function output. The step
    size would have to be chosen well: if our steps are too small, it will take a
    prohibitively long time to find the minimum. If the step size is too large, we
    may overshoot and miss the minimum and then continue oscillating (jumping back
    and forth) around the minimum:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个模型由参数组成时，训练算法有其自己的参数，称为**超参数**。超参数是一个可控的值，它会影响训练过程或算法。例如，考虑找到一个抛物线函数的最小值：我们可以先猜测一个值，然后朝着最小化函数输出的方向迈出小步。步长必须选择得当：如果我们的步子太小，找到最小值将需要过长的时间。如果步长太大，我们可能会超过最小值并错过它，然后继续在最小值周围振荡（来回跳跃）：
- en: '![Figure 1.1 – Effect of using a step size that is too large (left) and too
    small (right)](img/B16690_01_01.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 使用过大的步长（左）和过小的步长（右）的影响](img/B16690_01_01.jpg)'
- en: Figure 1.1 – Effect of using a step size that is too large (left) and too small
    (right)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 使用过大的步长（左侧）和过小的步长（右侧）的影响
- en: In this example, the step size would be a hyperparameter of our minimization
    algorithm. The effect of the step size is illustrated in *Figure 1**.1*.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，步长将是我们的最小化算法的超参数。步长的影响在*图1**.1*中得到了说明。
- en: Datasets
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: As explained previously, the machine learning model is trained using a dataset.
    Data is at the heart of the machine learning process, and data preparation is
    often the part of the process that takes up the most time.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，机器学习模型是使用数据集进行训练的。数据是机器学习过程的核心，数据准备通常是占用最多时间的流程部分。
- en: Throughout this book, we’ll work with *tabular* datasets. Tabular datasets are
    very common in the real world and consist of rows and columns. Rows are often
    called samples, examples, or observations, and columns are usually called features,
    variables, or attributes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的整个过程中，我们将与 *表格型* 数据集一起工作。表格型数据集在现实世界中非常常见，由行和列组成。行通常被称为样本、示例或观察，而列通常被称为特征、变量或属性。
- en: Importantly, there is no restriction on the data type in a column. Features
    may be strings, numbers, Booleans, geospatial coordinates, or encoded formats
    such as audio, images, or video.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，列中的数据类型没有限制。特征可以是字符串、数字、布尔值、地理空间坐标，或编码格式，如音频、图像或视频。
- en: Datasets are also rarely perfectly defined. Data may be incomplete, noisy, incorrect,
    inconsistent, and contain various formats.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集也 rarely 完美定义。数据可能不完整、有噪声、不正确、不一致，并包含各种格式。
- en: Therefore, *data preparation and cleaning* are essential parts of the machine
    learning process.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，*数据准备和清洗*是机器学习过程中的关键部分。
- en: 'Data preparation concerns processing the data to make it suitable for machine
    learning and typically consists of the following steps:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备涉及处理数据使其适合机器学习，通常包括以下步骤：
- en: '**Gathering and validation**: Some datasets are initially too small or represent
    the problem poorly (the data is not representative of the actual data population
    it’s been sampled from). In these cases, the practitioner must collect more data,
    and validation must be done to ensure the data represents the problem.'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**收集和验证**：一些数据集最初可能太小或表示问题不佳（数据不是从其抽取样本的实际数据总体有代表性）。在这些情况下，从业者必须收集更多数据，并进行验证以确保数据代表问题。'
- en: '**Checking for systemic errors and bias**: It is vital to check for and correct
    any systemic errors in the collection and validation process that may lead to
    bias in the dataset. In our sales example, a systemic collection error may be
    that data was only gathered from urban stores and excluded rural ones. A model
    trained on only urban store data will be biased in forecasting store sales, and
    we may expect poor performance when the model is used to predict sales for rural
    stores.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查系统错误和偏差**：检查并纠正收集和验证过程中可能导致的任何系统错误，这些错误可能导致数据集偏差至关重要。在我们的销售示例中，系统收集错误可能仅从城市商店收集数据，而排除农村商店。仅使用城市商店数据训练的模型在预测商店销售时将存在偏差，并且当模型用于预测农村商店的销售时，我们可能会期望性能不佳。'
- en: '**Cleaning the data**: Any format or value range inconsistencies must be addressed.
    Any missing values also need to be handled in a way that does not introduce bias.'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据清洗**：任何格式或值范围的不一致性都必须得到解决。任何缺失值也需要以不引入偏差的方式进行处理。'
- en: '**Feature engineering**: Certain features may need to be transformed to ensure
    the machine learning model can learn from them, such as numerically encoding a
    sentence of words. Additionally, new features may need to be prepared from existing
    features to help the model detect patterns.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征工程**：某些特征可能需要转换以确保机器学习模型能够从中学习，例如将一个单词句子进行数值编码。此外，可能需要从现有特征中准备新的特征，以帮助模型检测模式。'
- en: '**Normalizing and standardizing**: The relative ranges of features must be
    normalized and standardized. Normalizing and standardizing ensure that no one
    feature has an outsized effect on the overall prediction.'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**归一化和标准化**：特征的相关范围必须进行归一化和标准化。归一化和标准化确保没有任何一个特征对整体预测有不成比例的影响。'
- en: '**Balancing the dataset**: In cases where the dataset is imbalanced – that
    is, it contains many more examples of one class or prediction than another – the
    dataset needs to be balanced. Balancing is typically done by oversampling the
    minority examples to balance the dataset.'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**平衡数据集**：在数据集不平衡的情况下——也就是说，它包含一个类或预测的示例比另一个多得多——数据集需要被平衡。平衡通常是通过过度采样少数示例来实现的，以平衡数据集。'
- en: In [*Chapter 6*](B16690_06.xhtml#_idTextAnchor094)*, Solving Real-World Data
    Science Problems with LightGBM*, we’ll go through the entire data preparation
    process to show how the preceding steps are applied practically.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第6章*](B16690_06.xhtml#_idTextAnchor094)*，使用LightGBM解决现实世界数据科学问题*中，我们将通过整个数据准备过程来展示前面的步骤是如何在实际中应用的。
- en: Note
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A good adage to remember is “garbage in, garbage out”. A model learns from any
    data given to it, including any flaws or biases contained in the data. When we
    train the model on garbage data, it results in a garbage model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得记住的谚语是“垃圾进，垃圾出”。模型从它所给出的任何数据中学习，包括数据中包含的任何缺陷或偏差。当我们用垃圾数据训练模型时，结果就是一个垃圾模型。
- en: 'One final concept to understand regarding datasets is the training, validation,
    and test datasets. We split our datasets into these three subsets after the data
    preparation step is done:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据集，还有一个需要理解的概念是训练、验证和测试数据集。我们在数据准备步骤完成后将数据集分为这三个子集：
- en: The **training set** is the most significant subset and typically consists of
    60% to 80% of the data. This data is used to train the model.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**是最重要的子集，通常由60%到80%的数据组成。这些数据用于训练模型。'
- en: The **validation set** is separate from the training data and is used throughout
    the training process to evaluate the model. Having independent validation data
    ensures that the model is evaluated on data it has not seen before, also known
    as its generalization ability. Hyperparameter tuning, a process covered in detail
    in [*Chapter 5*](B16690_05.xhtml#_idTextAnchor083)*, LightGBM Parameter Optimization
    with Optuna*, also uses the validation set.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**与训练数据分开，并在整个训练过程中用于评估模型。拥有独立的验证数据确保模型是在它之前未见过的数据上评估的，也称为其泛化能力。超参数调整，在[*第5章*](B16690_05.xhtml#_idTextAnchor083)*，Optuna进行LightGBM参数优化*中详细介绍的流程，也使用验证集。'
- en: Finally, the **test set** is an optional hold-out set, similar to the validation
    set. It is used at the end of the process to evaluate the model’s performance
    on data that was not part of the training or tuning process.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，**测试集**是一个可选的保留集，类似于验证集。它用于过程的最后，以评估模型在训练或调整过程中未参与的数据上的性能。
- en: Another use of the validation set is to monitor whether the model is overfitting
    the data. Let’s discuss overfitting in more detail.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 验证集的另一个用途是监控模型是否过度拟合数据。让我们更详细地讨论一下过度拟合。
- en: Overfitting and generalization
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过度拟合与泛化
- en: 'To understand overfitting, we must first define what we mean by model **generalization**.
    As stated previously, generalization is the model’s ability to accurately predict
    data it has not seen before. Compared to training accuracy, generalization accuracy
    is more significant as an estimate of model performance as this indicates how
    our model will perform in production. Generalization comes in two forms, **interpolation**
    **and extrapolation**:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解过度拟合，我们首先必须定义我们所说的模型**泛化**是什么意思。如前所述，泛化是模型准确预测它之前未见过的数据的能力。与训练准确率相比，泛化准确率作为模型性能估计更为重要，因为这表明我们的模型在生产中的表现。泛化有两种形式，**插值**和**外推**：
- en: Interpolation refers to the model’s ability to predict a value between two known
    data points – stated another way, to generalize within the training data range.
    For example, let’s say we train our model with monthly data from January to July.
    When interpolating, we would ask the model to make a prediction on a particular
    day in April, a date within our training range.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 插值指的是模型预测两个已知数据点之间值的能力——换句话说，就是在训练数据范围内进行泛化。例如，假设我们用1月到7月的月度数据来训练我们的模型。在插值时，我们会要求模型对4月某一天进行预测，这是一个在我们训练范围内的日期。
- en: Extrapolation, as you might infer, is the model’s ability to predict values
    outside of the range defined by our training data. A typical example of extrapolation
    is forecasting – that is, predicting the future. In our previous example, if we
    ask the model to make a prediction in December, we expect it to extrapolate from
    the training data.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外推，正如你可能推断的那样，是模型预测训练数据定义范围之外值的能力。外推的一个典型例子是预测——即预测未来。在我们的上一个例子中，如果我们要求模型在十二月进行预测，我们期望它从训练数据中外推。
- en: Of the two types of generalization, extrapolation is much more challenging and
    may require a specific type of model to achieve. However, in both cases, a model
    can overfit the data, losing its ability to interpolate or extrapolate accurately.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在两种泛化类型中，外推更具挑战性，可能需要特定类型的模型来实现。然而，在两种情况下，模型都可能过度拟合数据，失去准确插值或外推的能力。
- en: '**Overfitting** is a phenomenon where the model fits the training data too
    closely and loses its ability to generalize to unseen data. Instead of learning
    the underlying pattern in the data, the model has memorized the training data.
    More technically, the model fits the *noise* contained in the training data. The
    term noise stems from the concept of data containing *signal* and *noise*. Signal
    refers to the underlying pattern or information captured in the data we are trying
    to predict. In contrast, noise refers to random or irrelevant variations of data
    points that mask the signal.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**过拟合**是一种现象，其中模型对训练数据拟合得太紧密，失去了泛化到未见数据的能力。模型不是学习数据中的潜在模式，而是记住了训练数据。更技术地说，模型拟合了训练数据中包含的**噪声**。这个术语“噪声”来源于数据包含**信号**和**噪声**的概念。信号指的是我们试图预测的数据中捕获的潜在模式或信息。相比之下，噪声指的是数据点的随机或不相关的变化，这些变化掩盖了信号。'
- en: 'For example, consider a dataset where we try to predict the rainfall for specific
    locations. The signal in the data would be the general trend of rainfall: rainfall
    increases in the winter or summer, or vice versa for other locations. The noise
    would be the slight variations in rainfall measurement for each month and location
    in our dataset.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑一个数据集，我们试图预测特定位置的降雨量。数据中的信号将是降雨的一般趋势：冬季或夏季降雨增加，或相反的其他位置。噪声将是我们在数据集中每个月和每个位置的降雨量测量的微小变化。
- en: 'The following graph illustrates the phenomenon of overfitting:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表说明了过拟合的现象：
- en: '![Figure 1.2 – Graph showing overfitting. The model has overfitted and predicted
    the training data perfectly but has lost the ability to generalize to the actual
    signal](img/B16690_01_02.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 展示过拟合的图表。模型过度拟合并完美预测了训练数据，但失去了泛化到实际信号的能力](img/B16690_01_02.jpg)'
- en: Figure 1.2 – Graph showing overfitting. The model has overfitted and predicted
    the training data perfectly but has lost the ability to generalize to the actual
    signal
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 展示过拟合的图表。模型过度拟合并完美预测了训练数据，但失去了泛化到实际信号的能力
- en: 'The preceding figure shows the difference between signal and noise: each data
    point was sampled from the actual signal. The data follows the general pattern
    of the signal, with slight, random variations. We can see how the model has overfitted
    the data: the model has fit the training data perfectly but at the cost of generalization.
    We can also see that if we use the model to *interpolate* by predicting a value
    for 4, we get a result much higher than the actual signal (6.72 versus 6.2). Also
    shown is the model’s failure to *extrapolate*: the prediction for 12 is much lower
    than a forecast of the signal (7.98 versus 8.6).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图表显示了信号和噪声之间的差异：每个数据点都是从实际信号中采样的。数据遵循信号的总体模式，有轻微的、随机的变化。我们可以看到模型是如何过度拟合数据的：模型完美地拟合了训练数据，但以泛化为代价。我们还可以看到，如果我们使用模型通过预测4的值来进行插值，我们得到的结果比实际信号（6.72比6.2）高得多。此外，还显示了模型外推失败的情况：对12的预测远低于信号的预测（7.98比8.6）。
- en: In reality, all real-world datasets contain noise. As data scientists, we aim
    to prepare the data to remove as much noise as possible, making the signal easier
    to detect. Data cleaning, normalization, feature selection, feature engineering,
    and regularization are techniques for removing noise from the data.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，所有现实世界的数据集都包含噪声。作为数据科学家，我们的目标是准备数据，尽可能多地去除噪声，使信号更容易检测。数据清洗、归一化、特征选择、特征工程和正则化是去除数据中噪声的技术。
- en: 'Since all real-world data contains noise, overfitting is impossible to eliminate.
    The following conditions may lead to overfitting:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有真实世界的数据都包含噪声，过拟合是无法完全消除的。以下条件可能导致过拟合：
- en: '**An overly complex model**: A model that is too complex for the amount of
    data we have utilizes additional complexity to memorize the noise in the data,
    leading to overfitting'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过于复杂的模型**：对于我们所拥有的数据量来说过于复杂的模型，会利用额外的复杂性来记住数据中的噪声，从而导致过拟合。'
- en: '**Insufficient data**: If we don’t have enough training data for the model
    we use, it’s similar to an overly complex model, which overfits the data'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据不足**：如果我们没有足够的训练数据用于模型，这类似于一个过于复杂的模型，它会过度拟合数据。'
- en: '**Too many features**: A dataset with too many features likely contains irrelevant
    (noisy) features that reduce the model’s generalization'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征过多**：具有过多特征的集合很可能包含无关的（噪声）特征，这会降低模型的泛化能力。'
- en: '**Overtraining**: Training the model for too long allows it to memorize the
    noise in the dataset'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过度训练**：对模型进行过长时间的训练，使其能够记住数据集中的噪声。'
- en: 'As the validation set is a part of the training data that remains unseen by
    the model, we use the validation set to monitor for overfitting. We can recognize
    the point of overfitting by looking at the training and generalization errors
    over time. At the point of overfitting, the validation error increases. In contrast,
    the training error continues to improve: the model is fitting noise in the training
    data and losing its ability to generalize.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于验证集是模型尚未见过的训练数据的一部分，我们使用验证集来监控过拟合。我们可以通过观察训练和泛化误差随时间的变化来识别过拟合的点。在过拟合的点，验证误差增加。相比之下，训练误差持续改善：模型正在拟合训练数据中的噪声，并失去了泛化的能力。
- en: 'Techniques that prevent overfitting usually aim to address the conditions that
    lead to overfitting we discussed previously. Here are some strategies to avoid
    overfitting:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 防止过拟合的技术通常旨在解决我们之前讨论的导致过拟合的条件。以下是一些避免过拟合的策略：
- en: '**Early stopping**: We can stop training when we see the validation error beginning
    to increase.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**提前停止**：当我们看到验证误差开始增加时，我们可以停止训练。'
- en: '**Simplifying the model**: A less complex model with fewer parameters would
    be incapable of learning the noise in the training data, thereby generalizing
    better.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化模型**：具有较少参数的简单模型将无法学习训练数据中的噪声，从而更好地泛化。'
- en: '**Get more data**: Either collecting more data or augmenting data is an effective
    method for preventing overfitting by giving the model a better chance to learn
    the signal in the data instead of the noise in a smaller dataset.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**获取更多数据**：收集更多数据或增强数据是防止过拟合的有效方法，因为它给模型提供了更好的机会来学习数据中的信号，而不是在较小数据集中的噪声。'
- en: '**Feature selection and dimensionality reduction**: As some features might
    be irrelevant to the problem being solved, we can discard features we think are
    redundant or use techniques such as Principal Component Analysis to reduce the
    dimensionality (features).'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择和降维**：由于某些特征可能对要解决的问题不相关，我们可以丢弃我们认为冗余的特征，或者使用主成分分析等技术来降低维度（特征）。'
- en: '**Adding regularization**: Smaller parameter values typically lead to better
    generalization, depending on the model (a neural network is an example of such
    a model). Regularization adds a penalty term to the objective function to discourage
    large parameter values. By driving the parameters to smaller (or zero) values,
    they contribute less to the prediction, effectively simplifying the model.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**添加正则化**：较小的参数值通常会导致更好的泛化，这取决于模型（神经网络就是一个例子）。正则化向目标函数添加一个惩罚项，以阻止参数值过大。通过将参数值驱动到更小（或零）的值，它们对预测的贡献更小，从而有效地简化了模型。'
- en: '**Ensemble methods**: Combining the prediction from multiple, *weaker* models
    can lead to better generalization while also improving performance.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：结合多个**较弱**模型的预测可以导致更好的泛化，同时提高性能。'
- en: It’s important to note that *overfitting and the techniques to prevent overfitting
    are specific to our model*. Our goal should always be to minimize overfitting
    to ensure generalization to unseen data. Some strategies, such as regularization,
    might not work for specific models, while others might be more effective. There
    are also more bespoke strategies for models, an example of which we’ll see when
    we discuss overfitting in decision trees.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，*过拟合以及防止过拟合的技术是针对我们模型的特定问题*。我们的目标始终应该是最小化过拟合，以确保对未见数据的泛化。一些策略，如正则化，可能对某些模型不起作用，而其他策略可能更有效。还有一些针对特定模型的定制策略，我们将在讨论决策树中的过拟合时看到一个例子。
- en: Supervised learning
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习
- en: The store sales example is an instance of **supervised learning** – we have
    a dataset consisting of features and are training the model to predict a target.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 店铺销售额的例子是**监督学习**的一个实例——我们有一个由特征组成的数据库，并且正在训练模型来预测一个目标。
- en: 'Supervised learning problems can be divided into two main types of problem
    categories: **classification problems** and **regression problems**.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习问题可以分为两大类问题：**分类问题**和**回归问题**。
- en: Classification and regression
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类与回归
- en: 'With a *classification problem*, the label that needs to be predicted by the
    model is categorical or defines a class. Some examples of classes are `spam` or
    `not spam`, `cat` or `dog`, and `diabetic` or `not diabetic`. These are examples
    of binary classifications: there are only two classes.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在**分类问题**中，模型需要预测的标签是分类的或定义了一个类别。一些类别的例子包括`垃圾邮件`或`非垃圾邮件`、`猫`或`狗`、以及`糖尿病患者`或`非糖尿病患者`。这些都是二元分类的例子：只有两个类别。
- en: Multi-class classification is also possible; for example, email may be classified
    as `Important`, `Promotional`, `Clutter`, or `Spam`; images of clouds could be
    classified as `Cirro`, `Cumulo`, `Strato`, or `Nimbo`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 多类分类也是可能的；例如，电子邮件可以被分类为`重要`、`促销`、`杂乱`或`垃圾邮件`；云朵的图片可以被分类为`卷云`、`积云`、`层云`或`雨层云`。
- en: With *regression problems*, the goal is to predict a continuous, numerical value.
    Examples include predicting revenue, sales, temperature, house prices, and crowd
    size.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在**回归问题**中，目标是预测一个连续的、数值的值。例子包括预测收入、销售额、温度、房价和人群数量。
- en: A big part of the *art* of machine learning is correctly defining or transcribing
    a problem as a classification or regression problem (or perhaps unsupervised or
    reinforcement). Later chapters will cover multiple end-to-end case studies of
    both types of problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习艺术中很大一部分是正确地将问题定义为分类或回归问题（或者可能是无监督或强化学习）。后面的章节将涵盖这两种类型问题的多个端到端案例研究。
- en: Model performance metrics
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型性能指标
- en: Let’s briefly discuss how we measure our model’s performance. Model performance
    refers to the ability of a machine learning model to make accurate predictions
    or generate meaningful outputs based on the given inputs. An evaluation metric
    quantifies how well a model generalizes to new, unseen data. High model performance
    indicates that the model has learned the underlying patterns in the data effectively
    and can make accurate predictions on data it has not seen before. We can measure
    the model’s performance relative to the known targets when working with supervised
    learning problems (either classification or regression problems).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要讨论一下我们如何衡量我们模型的表现。模型性能指的是机器学习模型根据给定的输入做出准确预测或生成有意义输出的能力。一个评估指标量化了模型对新、未见数据的泛化程度。高模型性能表明模型有效地学习了数据中的潜在模式，并且可以在它未见过的数据上做出准确的预测。当与监督学习问题（无论是分类还是回归问题）一起工作时，我们可以根据已知的目标来衡量模型的表现。
- en: Importantly, how we measure the model’s performance on classification tasks
    and regression tasks differs. scikit-learn has many built-in metrics functions
    ready for use with either a cla[ssification or regression problem (https://scikit-learn.org/s](https://scikit-learn.org/stable/modules/model_evaluation.xhtml)table/modules/model_evaluation.xhtml).
    Let’s review the most common of these.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们衡量模型在分类任务和回归任务上的表现方式不同。scikit-learn有许多内置的指标函数，可以用于分类或回归问题（[scikit-learn.org/stable/modules/model_evaluation.xhtml](https://scikit-learn.org/stable/modules/model_evaluation.xhtml)）。让我们回顾这些中最常见的。
- en: 'Classification metrics can be defined in terms of positive and negative predictions
    made by the model. The following definitions can be used to calculate classification
    metrics:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 可以用模型做出的正面和负面预测来定义分类指标。以下定义可以用来计算分类指标：
- en: '**True positive** (**TP**): A positive instance is correctly classified as
    positive'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正正例** (**TP**): 一个正例被正确地分类为正例'
- en: '**True negative** (**TN**): A negative instance is correctly classified as
    negative'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**真正负例** (**TN**): 一个负例被正确地分类为负例'
- en: '**False positive** (**FP**): A negative instance is incorrectly classified
    as positive'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阳性** (**FP**): 一个负例被错误地分类为正例'
- en: '**False negative** (**FN**): A positive instance is incorrectly classified
    as negative'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**假阴性** (**FN**): 一个正例被错误地分类为负例'
- en: 'Given these definitions, the most common *classification* metrics are as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些定义，最常见的 *分类* 指标如下：
- en: '**Accuracy**: Accuracy is the most straightforward classification metric. Accuracy
    is the number of correct predictions divided by the total number of predictions.
    However, accuracy is susceptible to an imbalance in the data. For example, suppose
    we have an email dataset with 8 examples of spam and 2 examples of non-spam, and
    our model predicts only spam. In that case, the model has an accuracy of 80%,
    even though it never correctly classified non-spam emails. Mathematically, we
    can define accuracy as follows:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**: 准确率是最直接的分类指标。准确率是正确预测的数量除以总预测数量。然而，准确率容易受到数据不平衡的影响。例如，假设我们有一个包含8个垃圾邮件示例和2个非垃圾邮件示例的电子邮件数据集，并且我们的模型只预测垃圾邮件。在这种情况下，模型的准确率为80%，尽管它从未正确分类非垃圾邮件。从数学上讲，我们可以如下定义准确率：'
- en: Accuracy =  TP + TN ______________  TP + FP + TN + FN
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率 =  TP + TN ______________  TP + FP + TN + FN
- en: '**Precision**: The precision score is one way of getting a more nuanced understanding
    of the classification performance. Precision is the ratio between the true positive
    prediction (correctly predicted) and all positive predictions (true positive and
    false positive). In other words, the precision score indicates how precise the
    model is in predicting positives. In our spam emails example, a model predicting
    only spam is not very precise (as it classifies all non-spam emails as spam) and
    has a lower precision score. The following formula can be used to calculate precision:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确率**: 精确率是获取对分类性能更深入理解的一种方式。精确率是真正正例预测（正确预测）与所有正例预测（真正正例和假阳性）的比例。换句话说，精确率指标表明模型在预测正例时的精确度。在我们的垃圾邮件示例中，仅预测垃圾邮件的模型精确度不高（因为它将所有非垃圾邮件分类为垃圾邮件），并且具有较低的精确率。以下公式可以用来计算精确率：'
- en: Precision =  TP _ TP + FP
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 精确率 =  TP _ TP + FP
- en: '**Recall**: The recall score is the counterpoint to the precision score. The
    recall score measures how effectively the model finds (or recalls) all true positive
    cases. The recall is calculated as the ratio between true positive predictions
    and all positive instances (true positive and false negative). In our spam example,
    a model predicting only spam has perfect recall (it can find all the spam). We
    can calculate recall like so:'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**召回率**: 召回率是精确率的对立面。召回率衡量模型有效地找到（或召回）所有真正正例的能力。召回率是真正正例预测与所有正例（真正正例和假阴性）的比例。在我们的垃圾邮件示例中，仅预测垃圾邮件的模型具有完美的召回率（它可以找到所有垃圾邮件）。我们可以这样计算召回率：'
- en: Recall =  TP _ TP + FN
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 召回率 =  TP _ TP + FN
- en: '**F1 score**: Finally, we have the F1 score. The F1 score is calculated as
    the harmonic mean between precision and recall. The F1 score balances precision
    and recall, giving us a singular value that summarizes the classifier’s performance.
    The following formula can be used to calculate the F1 score:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1分数**: 最后，我们有F1分数。F1分数是精确率和召回率的调和平均数。F1分数平衡了精确率和召回率，给出了一个总结分类器性能的单个值。以下公式可以用来计算F1分数：'
- en: F 1 =  2 × Precision × Recall  _______________  Precision + Recall  =  2 × TP _____________  2
    × TP + FP + FN
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: F 1 =  2 × 精确率 × 召回率 _______________  精确率 + 召回率  =  2 × TP _____________  2
    × TP + FP + FN
- en: The preceding classification metrics are the most common, but there are many
    more. Even though the F1 score is commonly used in classification problems (as
    it summarizes precision and recall), choosing the best metric is specific to the
    problem you are solving. Often, it might be the case that a specific metric is
    required, but other times, you must choose based on experience and your understanding
    of the data. We will look at examples of different metrics later in this book.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述分类指标是最常见的，但还有很多。尽管F1分数在分类问题中常用（因为它总结了精确率和召回率），但选择最佳指标取决于你解决的问题。通常，可能需要特定的指标，但有时必须根据经验和你对数据的理解来选择。我们将在本书的后面部分查看不同指标的一些示例。
- en: 'The following are common *regression* metrics:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些常见的 *回归* 指标：
- en: '**Mean squared error** (**MSE**): The MSE is calculated as the average of the
    squared differences between predicted and actual values. The MSE is commonly used
    because of one crucial mathematical property: the MSE is *differentiable* and
    is therefore appropriate for use with gradient-based learning methods. However,
    since the difference is squared, the MSE penalizes large errors more heavily than
    small errors, which may or may not be appropriate to the problem being solved.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**均方误差**（**MSE**）：MSE 是预测值和实际值之间平方差异的平均值。MSE 因其一个关键数学特性而常用：MSE 是 *可微的*，因此适用于与基于梯度的学习方法一起使用。然而，由于差异被平方，MSE
    对大误差的惩罚比对小误差更重，这可能或可能不适合要解决的问题。'
- en: '**Mean absolute error** (**MAE**): Instead of squaring the differences, the
    MAE is calculated as the average of the absolute differences between predicted
    and actual values. By avoiding the square of errors, the MAE is more robust against
    the magnitude of errors and less sensitive to outliers than the MSE. However,
    the MAE is not differentiable and, therefore, can’t be used with gradient-based
    learning methods.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均绝对误差**（**MAE**）：与平方差异不同，MAE 是预测值和实际值之间绝对差异的平均值。通过避免误差的平方，MAE 对误差的大小更稳健，对异常值比均方误差（MSE）更不敏感。然而，MAE
    不可微，因此不能与基于梯度的学习方法一起使用。'
- en: As with the classification metrics, choosing the most appropriate regression
    metric is specific to the problem you are trying to solve.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与分类指标一样，选择最合适的回归指标取决于你试图解决的问题。
- en: Metrics versus objectives
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 指标与目标
- en: We defined training a model as finding the most appropriate parameters to minimize
    an *objective function*. It’s important to note that the objective function and
    metrics used for a specific problem may differ. A good example is decision trees,
    where a measure of impurity (entropy) is used as the objective function when building
    a tree. However, we still calculate the metrics explained previously to determine
    the tree’s performance on the data.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将训练模型定义为找到最合适的参数以最小化一个 *目标函数*。需要注意的是，特定问题的目标函数和指标可能不同。一个很好的例子是决策树，在构建树时使用不纯度（熵）作为目标函数。然而，我们仍然计算之前解释的指标来确定树在数据上的性能。
- en: With our understanding of basic metrics in place, we can conclude our introduction
    to machine learning concepts. Now, let’s review the terms and concepts we’ve discussed
    using an example.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对基本指标有了理解之后，我们可以结束对机器学习概念的介绍。现在，让我们通过一个例子来回顾我们讨论过的术语和概念。
- en: A modeling example
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个建模例子
- en: 'Consider the following data of sales by month, in thousands:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下按月销售的以下数据（单位：千）：
- en: '| Jan | Feb | Mar | Apr | May | Jun |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| Jan | Feb | Mar | Apr | May | Jun |'
- en: '| 4,140 | 4,850 | 7,340 | 6,890 | 8,270 | 10,060 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 4,140 | 4,850 | 7,340 | 6,890 | 8,270 | 10,060 |'
- en: '| Jul | Aug | Sept | Oct | Nov | Dec |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| Jul | Aug | Sept | Oct | Nov | Dec |'
- en: '| 8,110 | 11,670 | 10,450 | 11,540 | 13,400 | 14,420 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 8,110 | 11,670 | 10,450 | 11,540 | 13,400 | 14,420 |'
- en: Table 1.1 – Sample sales data, by month, in thousands
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.1 – 按月样本销售数据，单位：千
- en: 'This problem is straightforward: there is only one feature, the month, and
    the target is the number of sales. Therefore, this is an example of a supervised
    regression problem.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题很简单：只有一个特征，即月份，目标是销售数量。因此，这是一个监督回归问题的例子。
- en: Note
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'You might have noticed that this is an example of a time series problem: time
    is the primary variable. Time series can also be predicted using more advanced
    time series-specific algorithms such as ANOVA, but we’ll use a simple algorithm
    for illustration purposes in this section.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到这是一个时间序列问题的例子：时间是主要变量。时间序列也可以使用更高级的时间序列特定算法（如方差分析）进行预测，但在这个部分我们将使用一个简单的算法进行说明。
- en: 'We can plot our data as a graph of sales per month to understand it better:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将我们的数据绘制成每月销售的图表，以更好地理解它：
- en: '![Figure 1.3 – Graph showing store sales by month](img/B16690_01_03.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – 显示按月商店销售的图表](img/B16690_01_03.jpg)'
- en: Figure 1.3 – Graph showing store sales by month
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – 显示按月商店销售的图表
- en: 'Here, we’re using a straight-line model, also known as simple linear regression,
    to model our sales data. The definition of a straight line is given by the following
    formula:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用直线模型，也称为简单线性回归，来模拟我们的销售数据。直线的定义如下公式：
- en: y = mx + c
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: y = mx + c
- en: Here, m is the line’s slope and c is the Y-intercept. In machine learning, the
    straight line is the model, and m and c are the model parameters.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，m 是直线的斜率，c 是 Y 轴截距。在机器学习中，直线是模型，而 m 和 c 是模型参数。
- en: 'To find the best parameters, we must measure how well our model fits the data
    for a particular set of parameters – that is, the error in our outputs. We will
    use the MAE as our metric:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到最佳参数，我们必须衡量我们的模型对于特定参数集的数据拟合程度如何 – 也就是说，我们输出的错误。我们将使用MAE作为我们的指标：
- en: MAE =  ∑ i=1 n  | ˆ y  − y| _ n
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: MAE =  ∑ i=1 n  | ˆ y  − y| _ n
- en: Here,  ˆ y  is the predicted output, y is the actual output, and n is the number
    of predictions. We calculate the MAE by making a prediction for each of our inputs
    and then calculating the MAE based on the formula.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ˆy是预测输出，y是实际输出，n是预测次数。我们通过为每个输入进行预测，然后根据公式计算MAE来计算MAE。
- en: Fitting the model
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 拟合模型
- en: 'Now, let’s fit our linear model to our data. Our process for fitting the line
    is iterative, and we start this process by guessing values for m and c and then
    iterating from there. For example, let’s consider m = 0.1, c = 4:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的线性模型拟合到我们的数据上。我们的拟合线的过程是迭代的，我们从这个过程开始，通过猜测m和c的值，然后从那里迭代。例如，让我们考虑m
    = 0.1，c = 4：
- en: '![Figure 1.4 – Graph showing the prediction of a linear model with m = 0.1
    and c = 4](img/B16690_01_04.jpg)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 显示m = 0.1和c = 4的线性模型预测的图表](img/B16690_01_04.jpg)'
- en: Figure 1.4 – Graph showing the prediction of a linear model with m = 0.1 and
    c = 4
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 显示m = 0.1和c = 4的线性模型预测的图表
- en: With these parameters, we achieve an error of `4,610`.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些参数，我们达到了`4,610`的错误率。
- en: Our guess is far too low, but that’s okay; we can now update the parameters
    to attempt to improve the error. In reality, updating the model parameters is
    done algorithmically using a training algorithm such as gradient descent. We’ll
    discuss gradient descent in [*Chapter 2*](B16690_02.xhtml#_idTextAnchor036)*,
    Ensemble Learning – Bagging* *and Boosting*.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的猜测值太低了，但这没关系；我们现在可以更新参数，尝试改进错误率。实际上，更新模型参数是通过使用梯度下降等训练算法算法化完成的。我们将在[*第2章*](B16690_02.xhtml#_idTextAnchor036)*，集成学习
    – Bagging* *和Boosting*中讨论梯度下降。
- en: In this example, we’ll use our understanding of straight lines and intuition
    to update the parameters for each iteration manually. Our line is too shallow,
    and the intercept is too low; therefore, we must increase both values. We can
    control the updates we make each iteration by choosing a *step size*. We must
    update the m and c values with each iteration by adding the step size. The results,
    for a step size of 0.1, is shown in *Table 1.2*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用我们对直线的理解以及直觉来手动更新每个迭代的参数。我们的线太浅，截距太低；因此，我们必须增加这两个值。我们可以通过选择*步长*来控制我们每次迭代所做的更新。我们必须通过添加步长来更新m和c值。对于步长为0.1的结果，请参阅*表1.2*。
- en: '| Guess# | m | c | MAE |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 猜测# | m | c | MAE |'
- en: '| 1 | 0.1 | 4 | 4.61 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1 | 4 | 4.61 |'
- en: '| 2 | 0.2 | 4.1 | 3.89 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.2 | 4.1 | 3.89 |'
- en: '| 3 | 0.3 | 4.2 | 3.17 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.3 | 4.2 | 3.17 |'
- en: '| 4 | 0.3 | 4.3 | 2.5 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0.3 | 4.3 | 2.5 |'
- en: '| 5 | 0.4 | 4.4 | 1.83 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.4 | 4.4 | 1.83 |'
- en: Table 1.2 – Step wise guessing of the slope (m) and y-intercept (c) for a straight
    line to fit our data. The quality of fit is measured using the MAE
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2 – 逐步猜测直线的斜率（m）和y截距（c）以拟合我们的数据。拟合质量是通过MAE来衡量的
- en: In our example, the *step size* is a *hyperparameter* of our training process.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，*步长*是我们训练过程中的一个*超参数*。
- en: We end up with an error of *1.83*, which means, on average, our predictions
    are wrong by less than *2,000*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终得到的错误率为*1.83*，这意味着平均来说，我们的预测错误不超过*2,000*。
- en: Now, let’s see how we can solve this problem using scikit-learn.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用scikit-learn解决这个问题。
- en: Linear regression with scikit-learn
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行线性回归
- en: Instead of manually modeling, we can use scikit-learn to build a linear regression
    model. As this is our first example, we’ll walk through the code line by line
    and explain what’s happening.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以不手动建模，而是使用scikit-learn构建线性回归模型。由于这是我们第一个例子，我们将逐行解释代码，说明正在发生什么。
- en: 'To start with, we must import the Python tools we are going to use:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须导入我们将要使用的Python工具：
- en: '[PRE0]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'There are three sets of imports: we import `numpy` and `pandas` first. Importing
    NumPy and pandas is a widely used way to start all your data science notebooks.
    Also, note the short names `np` and `pd`, which are the standard conventions when
    working with `numpy` and `pandas`.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 有三组导入：我们首先导入`numpy`和`pandas`。导入NumPy和pandas是开始所有数据科学笔记本的常用方法。此外，请注意短名称`np`和`pd`，这是在处理`numpy`和`pandas`时的标准约定。
- en: 'Next, we import a few standard plotting libraries we will use to plot some
    graphs: `pyplot` from `matplotlib` and `seaborn`. Matplotlib is a widely used
    plotting library that we access via the pyplot python interface. **Seaborn** is
    another visualization tool built on top of Matplotlib, which makes it easier to
    draw professional-looking graphs.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们导入一些标准的绘图库，我们将使用这些库来绘制一些图表：来自`matplotlib`的`pyplot`和`seaborn`。Matplotlib是一个广泛使用的绘图库，我们通过pyplot
    Python接口访问它。**Seaborn**是建立在Matplotlib之上的另一个可视化工具，它使得绘制专业外观的图表变得更加容易。
- en: Finally, we get to our scikit-learn imports. In Python code, the scikit-learn
    library is called `sklearn`. From its `linear_model` package, we import `LinearRegression`.
    scikit-learn implements a wide variety of predefined metrics, and here, we will
    be using `mean_absolute_error`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们到达了scikit-learn导入的部分。在Python代码中，scikit-learn库被称为`sklearn`。从其`linear_model`包中，我们导入`LinearRegression`。scikit-learn实现了许多预定义的度量，在这里，我们将使用`mean_absolute_error`。
- en: 'Now, we are ready to set up our data:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备设置我们的数据：
- en: '[PRE1]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, we define a new `numpy` array for the months and the corresponding sales,
    and to make them easier to work with, we gather both arrays into a new `pandas`
    DataFrame.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个新的`numpy`数组，用于月份和相应的销售，为了使它们更容易处理，我们将这两个数组收集到一个新的`pandas` DataFrame中。
- en: 'With the data in place, we get to the interesting part of the code: modeling
    using scikit-learn. The code is straightforward:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 数据就绪后，我们到达了代码的有趣部分：使用scikit-learn进行建模。代码很简单：
- en: '[PRE2]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: First, we create our model by constructing an instance of `LinearRegression`.
    We then fit our model using `model.fit` and passing in the month and sales data
    from our DataFrame. These two lines are all that’s required to fit a model, and
    as we’ll see in later chapters, even complicated models use the same recipe to
    instantiate and train a model.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过构造`LinearRegression`的实例来创建我们的模型。然后，我们使用`model.fit`和从我们的DataFrame中传递的月份和销售数据来拟合我们的模型。这两行代码就足以拟合一个模型，正如我们将在后面的章节中看到的，即使是复杂的模型也使用相同的配方来实例化和训练模型。
- en: 'We can now calculate our *MAE* by creating predictions for our data and passing
    the predictions and actual targets to the metric function:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过为我们的数据创建预测并将预测和实际目标传递给度量函数来计算我们的*MAE*：
- en: '[PRE3]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We get an error of *0.74*, which is slightly lower than our guesswork. We can
    also examine the model’s coefficient and intercept (*m* and *c* from earlier):'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到一个*0.74*的错误，这比我们的猜测略低。我们还可以检查模型系数和截距（*m*和*c*，来自之前的内容）：
- en: '[PRE4]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: scikit-learn has fitted a model with a coefficient of *0.85* and an intercept
    of *3.68*. We were in the right neighborhood with our guesses, but it might have
    taken us some time to get to the optimal values.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn已经使用系数为*0.85*和截距为*3.68*的模型进行了拟合。我们的猜测在正确的范围内，但可能需要一些时间才能得到最优值。
- en: That concludes our introduction to scikit-learn and the basics of modeling and
    machine learning. In our toy example, we did not split our data into separate
    datasets, optimize our model’s hyperparameters, or apply any techniques to ensure
    our model does not overfit. In the next section, we’ll look at classification
    and regression examples, where we’ll apply these and other best practices.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对scikit-learn、建模和机器学习基础介绍的介绍。在我们的玩具示例中，我们没有将数据分成单独的数据集，优化模型超参数，也没有应用任何确保模型不过拟合的技术。在下一节中，我们将查看分类和回归示例，我们将应用这些和其他最佳实践。
- en: Decision tree learning
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树学习
- en: This section introduces decision tree learning, a machine learning algorithm
    essential to understanding LightGBM. We’ll work through an example of how to build
    decision trees using scikit-learn. This section will also provide some mathematical
    definitions for building decision trees; understanding these definitions is not
    critical, but it will help us understand our discussion of the decision tree hyperparameters.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了决策树学习，这是理解LightGBM所必需的机器学习算法。我们将通过使用scikit-learn构建决策树的示例来进行分析。本节还将提供一些构建决策树的数学定义；理解这些定义不是必需的，但它将帮助我们理解我们对决策树超参数的讨论。
- en: Decision trees are tree-based learners that function by asking successive questions
    about the data to determine the result. A path is followed down the tree, making
    decisions about the input using one or more features. The path terminates at a
    leaf node, which represents the predicted class or value. Decision trees can be
    used for classification or regression.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是基于树的学习者，通过连续对数据进行提问以确定结果。沿着树路径向下，使用一个或多个特征对输入做出决策。路径在叶节点终止，它代表预测的类别或值。决策树可用于分类或回归。
- en: 'The following is an illustration of a decision tree fit on the Iris dataset:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 下图是Iris数据集上拟合的决策树示意图：
- en: '![Figure 1.5 – A decision tree modeling the Iris dataset](img/B16690_01_05.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 使用Iris数据集建模的决策树](img/B16690_01_05.jpg)'
- en: Figure 1.5 – A decision tree modeling the Iris dataset
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 使用Iris数据集建模的决策树
- en: 'The Iris dataset is a classification dataset where Iris flower sepal and petal
    dimensions are used to predict the type of Iris flower. Each non-leaf node uses
    one or more features to narrow down the samples in the dataset: the root node
    starts with all 150 samples and then splits them based on petal width, <= 0.8\.
    We continue down the tree, with each node splitting the samples further until
    we reach a leaf node that contains the predicted class (versicolor, virginica,
    or setosa).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: Iris数据集是一个分类数据集，其中使用Iris花的萼片和花瓣尺寸来预测Iris花的类型。每个非叶节点使用一个或多个特征来缩小数据集中的样本：根节点开始于所有150个样本，然后根据花瓣宽度进行分割，<=
    0.8。我们继续沿着树向下，每个节点进一步分割样本，直到我们达到包含预测类（versicolor、virginica或setosa）的叶节点。
- en: 'Compared to other models, decision trees have many advantages:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他模型相比，决策树有许多优点：
- en: '**Features may be numeric or categorical**: Samples can be split using either
    numerical features (by splitting a range) or categorical ones without us having
    to encode either.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征可以是数值或分类的**：可以使用数值特征（通过分割范围）或分类特征来分割样本，而无需我们对其进行编码。'
- en: '**Reduced need for data preparation**: Decision splits are not sensitive to
    data ranges or size. Many other models (for example, neural networks) require
    data to be normalized to unit ranges.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**减少数据准备需求**：决策分割对数据范围或大小不敏感。许多其他模型（例如，神经网络）需要将数据进行归一化到单位范围内。'
- en: '**Interpretability**: As shown previously, it’s straightforward to interpret
    the predictions made by a tree. Interpretability is valuable in contexts where
    a prediction must be explained to decision-makers.'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可解释性**：如前所述，解释树做出的预测是直接的。在需要向决策者解释预测的情况下，可解释性非常有价值。'
- en: 'These are just some of the advantages of using tree-based models. However,
    we also need to be aware of some of the disadvantages associated with decision
    trees:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这些只是使用基于树的模型的一些优点。然而，我们还需要意识到与决策树相关的一些缺点：
- en: '**Overfitting**: Decision trees are very prone to overfitting. Setting the
    correct hyperparameters is essential when fitting decision trees. Overfitting
    in decision trees will be discussed in detail later.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：决策树非常容易过拟合。在拟合决策树时设置正确的超参数是至关重要的。决策树中的过拟合将在后面详细讨论。'
- en: '**Poor extrapolation**: Decision trees are poor at extrapolation since their
    predictions are not continuous and are effectively bounded by the training data.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**较差的外推能力**：由于决策树的预测不是连续的，并且实际上由训练数据所限制，因此决策树在外推能力方面较差。'
- en: '**Unbalanced data**: When fitting a tree on unbalanced data, the high-frequency
    classes dominate the predictions. Data needs to be prepared to remove imbalances.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**不平衡数据**：当在不平衡数据上拟合树模型时，高频类别会主导预测。需要准备数据以消除不平衡。'
- en: A more detailed discussion of the advantages and disadvantages of decision trees
    is available at [https://scikit-learn.org/stable/modules/tree.xhtml](https://scikit-learn.org/stable/modules/tree.xhtml).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 关于决策树的优缺点有更详细的讨论，请参阅[https://scikit-learn.org/stable/modules/tree.xhtml](https://scikit-learn.org/stable/modules/tree.xhtml)。
- en: Entropy and information gain
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 熵和信息增益
- en: First, we need a rudimentary understanding of entropy and information gain before
    we look at an algorithm for building (or fitting) a decision tree.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在查看构建（或拟合）决策树的算法之前，我们需要对熵和信息增益有一个基本的理解。
- en: 'Entropy can be considered a way to measure the disorder or randomness of a
    system. Entropy measures how surprising the result of a specific input or event
    might be. Consider a well-shuffled deck of cards: drawing from the top of the
    deck could give us any of the cards in the deck (a surprising result each time);
    therefore, we can say that a shuffled deck of cards has **high entropy**. Drawing
    cards from the top of an ordered deck is unsurprising; we know which cards come
    next. Therefore, an ordered deck of cards has **low entropy**. Another way to
    interpret entropy is the impurity of the dataset: a low-entropy dataset (neatly
    ordered) has less impurity than a high-entropy dataset.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 熵可以被视为衡量系统无序或随机性的方法。熵衡量特定输入或事件的结果可能有多令人惊讶。考虑一副洗好的牌：从牌堆顶部抽取可能会给我们任何一张牌（每次都是令人惊讶的结果）；因此，我们可以说洗好的牌堆具有**高熵**。从有序牌堆的顶部抽取牌不会令人惊讶；我们知道下一张牌是什么。因此，有序牌堆的熵较低。另一种解释熵的方法是数据集的纯度：低熵数据集（整齐有序）比高熵数据集的纯度低。
- en: 'Information gain, in turn, is the amount of information gained when modifying
    or observing the underlying data. Information gain involves reducing entropy from
    before the observation. In our deck of cards example, we might take a shuffled
    deck of cards and split it into four smaller decks by suit (spades, hearts, diamonds,
    and clubs). If we draw from the smaller decks, the outcome is less of a surprise:
    we know that the next card is from the same suit. By splitting the deck by suit,
    we have reduced the entropy of the smaller decks. Splitting the deck of cards
    on a feature (the suit) is very similar to how the splits in a decision tree work;
    each division seeks to maximize the information gain – that is, they minimize
    the entropy after the split.'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 信息增益，反过来，是修改或观察底层数据时所获得的信息量。信息增益涉及在观察之前减少熵。在我们的牌堆示例中，我们可能将洗好的牌堆分成四个较小的牌堆，按花色（黑桃、红心、方块和梅花）。如果我们从小牌堆中抽取，结果就不会那么令人惊讶：我们知道下一张牌来自同一花色。通过按花色分割牌堆，我们已经减少了小牌堆的熵。在特征（花色）上分割牌堆与决策树中的分割非常相似；每次分割都试图最大化信息增益——也就是说，它们在分割后最小化熵。
- en: 'In decision trees, there are two common ways of measuring information gain
    or the loss of impurity:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树中，有两种常见的测量信息增益或纯度损失的方法：
- en: The Gini index
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 吉尼指数
- en: Log loss or entropy
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数损失或熵
- en: A detailed explanation of each is available at [https://scikit-learn.org/stable/modules/tree.xhtml#classification-criteria](https://scikit-learn.org/stable/modules/tree.xhtml#classification-criteria).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 详细解释可在[https://scikit-learn.org/stable/modules/tree.xhtml#classification-criteria](https://scikit-learn.org/stable/modules/tree.xhtml#classification-criteria)找到。
- en: Building a decision tree using C4.5
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用C4.5构建决策树
- en: 'C4.5 is an algorithm for building a decision tree from a dataset [1]. The algorithm
    is recursive and starts with the following base cases:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: C4.5是从数据集构建决策树的算法[1]。该算法是递归的，并从以下基本案例开始：
- en: If all the samples in a sub-dataset are of the same class, create a leaf node
    in the tree that chooses that class.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果子数据集中的所有样本都属于同一类，则在树中创建一个选择该类的叶节点。
- en: If no information can be gained by splitting using any of the features (the
    dataset can’t be divided any further), create a leaf node that predicts the most
    frequent class contained in the sub-dataset.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果使用任何特征分割无法获得信息（数据集不能再进一步分割），则创建一个叶节点，预测子数据集中包含的最频繁的类别。
- en: If a minimum threshold of samples is reached in a sub-dataset, create a leaf
    node that predicts the most frequent class contained in the sub-dataset.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果子数据集中达到最小样本阈值，则创建一个叶节点，预测子数据集中包含的最频繁的类别。
- en: 'Then, we can apply the algorithm:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以应用该算法：
- en: Check for any of the three base cases and stop splitting if any applies to the
    dataset.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查任何三种基本情况，如果任何一种适用于数据集，则停止分割。
- en: For each feature or attribute of the dataset, calculate the information gained
    by splitting the dataset on that feature.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据集的每个特征或属性，计算在该特征上分割数据集所获得的信息量。
- en: Create a decision node by splitting the dataset on the feature with the highest
    information gain.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在具有最高信息增益的特征上分割数据集来创建决策节点。
- en: Split the dataset into two sub-datasets based on the decision node and recursively
    reply to the algorithm on each sub-dataset.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据决策节点将数据集分割成两个子数据集，并递归地对每个子数据集应用算法。
- en: Once the tree has been built, pruning is applied. During pruning, decision nodes
    with a relatively lower information gain than other tree nodes are removed. Removing
    nodes avoids overfitting the training data and improves the tree’s generalization
    ability.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦树构建完成，就会应用剪枝。在剪枝过程中，信息增益相对较低的决策节点会被移除。移除节点可以避免过度拟合训练数据并提高树的泛化能力。
- en: Classification and Regression Tree
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分类和回归树
- en: You may have noticed that in the preceding explanations, we only used classes
    to split datasets using decision nodes; this is not by chance, as the canonical
    C4.5 algorithm only supports classification trees. **Classification and Regression
    Tree** (**CART**) extends C4.5 to support numerical target variables – that is,
    regression problems [2]. With CART, decision nodes can also split continuous numerical
    input variables to support regression, typically using a threshold (for example,
    x <= 0.3). When reaching a leaf node, the mean or median of the remaining numerical
    range is generally taken as the predicted value.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在前面的解释中，我们只使用了类别来使用决策节点分割数据集；这并非偶然，因为经典的 C4.5 算法仅支持分类树。**分类和回归树**（**CART**）扩展了
    C4.5 以支持数值目标变量——即回归问题 [2]。使用 CART，决策节点也可以分割连续的数值输入变量以支持回归，通常使用阈值（例如，x <= 0.3）。当达到叶节点时，剩余数值范围的均值或中位数通常被用作预测值。
- en: 'When building classification trees, only impurity is used to determine splits.
    However, with regression trees, impurity is combined with other criteria to calculate
    optimal splits:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分类树时，仅使用不纯度来确定分割。然而，对于回归树，不纯度会与其他标准结合来计算最佳分割：
- en: The MSE (or MAE)
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均方误差（MSE）或平均绝对误差（MAE）
- en: Half Poisson Deviance
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半泊松偏差
- en: A detailed mathematical explanation of each is available at [https://scikit-learn.org/stable/modules/tree.xhtml#regression-criteria](https://scikit-learn.org/stable/modules/tree.xhtml#regression-criteria).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 每个细节的数学解释都可以在 [https://scikit-learn.org/stable/modules/tree.xhtml#regression-criteria](https://scikit-learn.org/stable/modules/tree.xhtml#regression-criteria)
    找到。
- en: scikit-learn uses an optimized version of CART to build decision trees.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 使用 CART 的优化版本来构建决策树。
- en: Overfitting in decision trees
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树中的过度拟合
- en: 'One of the most significant disadvantages of decision trees is that they are
    prone to overfitting. Without proper hyperparameter choices, C4.5 and other training
    algorithms create overly complex and deep trees that fit the training data almost
    exactly. Managing overfitting is a crucial part of building decision trees. Here
    are some strategies to avoid overfitting:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树最显著的缺点之一是它们容易过度拟合。如果没有适当的超参数选择，C4.5 和其他训练算法会创建过于复杂和深的树，几乎完全符合训练数据。管理过度拟合是构建决策树的关键部分。以下是一些避免过度拟合的策略：
- en: '**Pruning**: As mentioned previously, we can remove branches that do not contribute
    much information gain; this reduces the tree’s complexity and improves generalization.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**剪枝**：如前所述，我们可以移除贡献信息增益不多的分支；这减少了树的复杂性并提高了泛化能力。'
- en: '**Maximum depth**: Limiting the depth of the tree also avoids overly complex
    trees and avoids overfitting.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大深度**：限制树的深度也可以避免过度复杂的树并避免过度拟合。'
- en: '**Maximum number of leaf nodes**: Similar to restricting depth, limiting the
    number of leaf nodes avoids overly specific branches and improves generalization.'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大叶节点数**：与限制深度类似，限制叶节点数可以避免过度具体的分支并提高泛化能力。'
- en: '**Minimum samples per leaf**: Setting a minimum limit on the number of samples
    a leaf may contain (stopping splitting when the sub-dataset is of the minimum
    size) also avoids overly specific leaf nodes.'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**每个叶节点的最小样本数**：设置每个叶节点可能包含的样本数的最小限制（当子数据集达到最小大小时停止分割）也可以避免过度具体的叶节点。'
- en: '**Ensemble methods**: Ensemble learning is a technique that combines multiple
    models to improve the prediction over an individual model. Averaging the prediction
    of multiple models can also reduce overfitting.'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成方法**：集成学习是一种结合多个模型以改善单个模型预测的技术。多个模型的预测平均值也可以减少过度拟合。'
- en: These strategies can be applied by setting the appropriate hyperparameters.
    Now that we understand how to build decision trees and strategies for overfitting,
    let’s look at building decision trees in scikit-learn.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这些策略可以通过设置适当的超参数来应用。现在我们了解了如何构建决策树以及避免过度拟合的策略，让我们看看如何在 scikit-learn 中构建决策树。
- en: Building decision trees with scikit-learn
  id: totrans-231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 构建决策树
- en: It is time to examine how we may use decision trees by training classification
    and regression trees using scikit-learn.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候检验我们如何通过使用scikit-learn训练分类和回归树来应用决策树了。
- en: For these examples, we’ll use the toy datasets included in scikit-learn. These
    datasets are small compared to real-world data but are easy to work with, allowing
    us to focus on the decision trees.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些示例，我们将使用scikit-learn中包含的玩具数据集。与真实世界数据相比，这些数据集较小，但易于处理，使我们能够专注于决策树。
- en: Classifying breast cancer
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乳腺癌分类
- en: We’ll use the Breast Cancer dataset (https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#breast-cancer-dataset)
    for our classification example. This dataset consists of features that have been
    calculated from the images of fine needle aspirated breast masses, and the task
    is to predict whether the mass is malignant or benign.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用乳腺癌数据集（https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#breast-cancer-dataset）作为我们的分类示例。这个数据集由从细针穿刺乳腺肿块图像计算得出的特征组成，任务是预测肿块是恶性还是良性。
- en: 'Using scikit-learn, we can solve this classification problem with five lines
    of code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用scikit-learn，我们可以用五行代码解决这个分类问题：
- en: '[PRE5]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'First, we load the dataset using `load_breast_cancer`. Then, we split our dataset
    into training and test sets using `train_test_split`; by default, 25% of the data
    is used for the test set. Like before, we instantiate our `DecisionTreeClassifier`
    model and train it on the training set using `model.fit`. The two hyperparameters
    we pass through when instantiating the model are notable: `max_depth` and `min_samples_split`.
    Both parameters control overfitting and will be discussed in more detail in the
    next section. We also specify `random_state` for both the train-test split and
    the model. By fixing the random state, we ensure the outcome is repeatable (otherwise,
    a new random state is created by scikit-learn for every execution).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`load_breast_cancer`函数加载数据集。然后，我们使用`train_test_split`将数据集分为训练集和测试集；默认情况下，25%的数据用于测试集。像之前一样，我们实例化`DecisionTreeClassifier`模型，并使用`model.fit`在训练集上训练它。在实例化模型时传递的两个超参数值得注意：`max_depth`和`min_samples_split`。这两个参数都控制过拟合，将在下一节中更详细地讨论。我们还指定了训练-测试分割和模型的`random_state`。通过固定随机状态，我们确保结果可重复（否则，scikit-learn将为每次执行创建一个新的随机状态）。
- en: 'Finally, we measure the performance using `f1_score`. Our model achieves an
    F1 score of 0.94 and an accuracy of 93.7%. F1 scores are out of 1.0, so we may
    conclude that the model does very well. If we break down our predictions, the
    model missed the prediction on only 9 of the 143 samples in the test set: 7 false
    positives and 2 false negatives.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用`f1_score`来衡量性能。我们的模型实现了0.94的F1分数和93.7%的准确率。F1分数是1.0的分数，因此我们可以得出结论，该模型表现非常好。如果我们分解我们的预测，模型在测试集的143个样本中只错了一个预测：7个假阳性和2个假阴性。
- en: Predicting diabetes progression
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测糖尿病进展
- en: To illustrate solving a regression problem with decision trees, we’ll use the
    Diabetes dataset ([https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset)).
    This dataset has 10 features (age, sex, body mass index, and others), and the
    model is tasked with predicting a quantitative measure of disease progression
    after 1 year.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明使用决策树解决回归问题，我们将使用糖尿病数据集（[https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.xhtml#diabetes-dataset)）。这个数据集有10个特征（年龄、性别、体重指数等），模型的任务是预测一年后疾病进展的定量指标。
- en: 'We can use the following code to build and evaluate a regression model:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码构建和评估回归模型：
- en: '[PRE6]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our model achieves an MAE of 45.28\. The code is almost identical to our classification
    example: instead of a classifier, we use `DecisionTreeRegressor` as our model
    and calculate `mean_absolute_error` instead of the F1 score. The consistency in
    the API for solving various problems with different types of models in scikit-learn
    is by design and illustrates a fundamental truth in machine learning work: even
    though data, models, and metrics change, *the overall process for building machine
    learning models remains the same*. In the coming chapters, we’ll expand on this
    general methodology and leverage the process’ consistency when building machine
    learning pipelines.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型实现了45.28的MAE。代码几乎与我们的分类示例相同：我们使用`DecisionTreeRegressor`作为模型，而不是分类器，并计算`mean_absolute_error`而不是F1分数。scikit-learn中用于解决不同类型模型的各种问题的API的一致性是设计上的，它展示了机器学习工作中的一条基本真理：尽管数据、模型和度量会变化，*构建机器学习模型的整体过程仍然保持不变*。在接下来的章节中，我们将扩展这一通用方法，并在构建机器学习管道时利用过程的这种一致性。
- en: Decision tree hyperparameters
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树超参数
- en: 'We used some decision tree hyperparameters in the preceding classification
    and regression examples to control overfitting. This section will look at the
    most critical decision tree hyperparameters provided by scikit-learn:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在先前的分类和回归示例中使用了一些决策树超参数来控制过拟合。本节将探讨scikit-learn提供的最关键的决策树超参数：
- en: '`max_depth`: The maximum depth the tree is allowed to reach. Deeper trees allow
    more splits, resulting in more complex trees and overfitting.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 树允许达到的最大深度。更深的树允许更多的分割，从而导致更复杂的树和过拟合。'
- en: '`min_samples_split`: The minimum number of samples required to split a node.
    Nodes containing only a few samples overfit the data, whereas having a larger
    minimum improves generalization.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`: 分割节点所需的最小样本数。仅包含少量样本的节点会导致数据过拟合，而增加最小样本数可以提高泛化能力。'
- en: '`min_samples_leaf`: The minimum number of samples allowed in leaf nodes. Like
    the minimum samples in a split, increasing the value leads to less complex trees,
    reducing overfitting.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf`: 允许在叶子节点中的最小样本数。类似于分割中的最小样本数，增加该值会导致更简单的树，减少过拟合。'
- en: '`max_leaf_nodes`: The maximum number of lead nodes to allow. Fewer leaf nodes
    reduce the tree size and, therefore, the complexity, which may improve generalization.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes`: 允许的最大叶子节点数。叶子节点越少，树的大小和复杂性就越小，这可能会提高泛化能力。'
- en: '`max_features`: The maximum features to consider when determining a split.
    Discarding some features reduces noise in the data, which improves overfitting.
    Features are chosen at random.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`: 确定分割时考虑的最大特征数。丢弃一些特征可以减少数据中的噪声，从而提高过拟合。特征是随机选择的。'
- en: '`criterion`: The impurity measure to use when determining a split, either `gini`
    or `entropy/log_loss`.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`criterion`: 确定分割时使用的杂质度量，可以是`gini`或`entropy/log_loss`。'
- en: As you may have noticed, most decision tree hyperparameters involve controlling
    overfitting by controlling the complexity of the tree. These parameters provide
    multiple ways of doing so, and finding the best combination of parameters and
    their values is non-trivial. Finding the best hyperparameters is called **hyperparameter
    tuning** and will be covered extensively later in this book.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经注意到的，大多数决策树超参数都涉及通过控制树的复杂性来控制过拟合。这些参数提供了多种方法来实现这一点，找到最佳参数及其值的组合并非易事。找到最佳超参数被称为**超参数调整**，本书后面将详细讨论。
- en: 'A complete list of the hyperparameters can be found at the following places:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的超参数列表可以在以下位置找到：
- en: https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml#sklearn-tree-decisiontreeclassifier
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml#sklearn-tree-decisiontreeclassifier](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml#sklearn.tree.DecisionTreeClassifier)'
- en: '[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn.tree.DecisionTreeRegressor)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn.tree.DecisionTreeRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn.tree.DecisionTreeRegressor)'
- en: Now, let’s summarize the key takeaways from this chapter.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们总结本章的关键要点。
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced machine learning as a method of creating software
    by learning to perform a task from a corpus of data instead of relying on programming
    the instructions by hand. We introduced the core concepts of machine learning
    with a focus on supervised learning and illustrated their applications through
    examples with scikit-learn.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了机器学习作为一种通过学习从数据集中执行任务来创建软件的方法，而不是依靠手动编程指令。我们通过scikit-learn的示例，重点介绍了机器学习的核心概念，并展示了它们的应用。
- en: We also introduced decision trees as a machine learning algorithm and discussed
    their strengths and weaknesses, as well as how to control overfitting using hyperparameters.
    We concluded this chapter with examples of how to solve classification and regression
    problems using decision trees in scikit-learn.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还介绍了决策树作为机器学习算法，并讨论了它们的优缺点，以及如何通过超参数控制过拟合。我们通过在scikit-learn中使用决策树解决分类和回归问题的示例来结束本章。
- en: This chapter has given us a foundational understanding of machine learning,
    enabling us to dive deeper into the data science process and the LightGBM library.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 本章为我们提供了机器学习的基础理解，使我们能够更深入地了解数据科学过程和LightGBM库。
- en: The next chapter will focus on ensemble learning in decision trees, a technique
    where the predictions of multiple decision trees are combined to improve the overall
    performance. Boosting, particularly gradient boosting, will be covered in detail.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将专注于决策树中的集成学习，这是一种将多个决策树的预测结果结合起来以提高整体性能的技术。特别是梯度提升将详细介绍。
- en: References
  id: totrans-263
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '| *[**1]* | *J. R. Quinlan, C4.5: Programs for machine learning,* *Elsevier,
    2014.* |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| *[**1]* | *J. R. Quinlan, 《C4.5：机器学习程序》，Elsevier出版社，2014年。* |'
- en: '| *[**2]* | *R. J. Lewis, An introduction to classification and regression
    tree (CART) analysis, in Annual meeting of the Society For Academic Emergency
    Medicine in San Francisco,* *California, 2000.* |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| *[**2]* | *R. J. Lewis, 《分类与回归树分析（CART）简介》，发表于旧金山学术急诊医学年会，加利福尼亚，2000年。* |'
