- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Compressing Data via Dimensionality Reduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过维度减少压缩数据
- en: In *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*, you
    learned about the different approaches for reducing the dimensionality of a dataset
    using different feature selection techniques. An alternative approach to feature
    selection for dimensionality reduction is **feature extraction**. In this chapter,
    you will learn about three fundamental techniques that will help you to summarize
    the information content of a dataset by transforming it onto a new feature subspace
    of lower dimensionality than the original one. Data compression is an important
    topic in machine learning, and it helps us to store and analyze the increasing
    amounts of data that are produced and collected in the modern age of technology.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*，*构建良好的训练数据集——数据预处理*中，您了解了使用不同特征选择技术来减少数据集维度的不同方法。另一种用于维度减少的特征选择方法是**特征提取**。在本章中，您将学习三种基本技术，它们将帮助您通过将数据转换到一个比原始数据低维的新特征子空间，从而总结数据集的信息内容。数据压缩是机器学习中的一个重要话题，它帮助我们存储和分析现代技术时代产生和收集的大量数据。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: '**Principal component analysis** (**PCA**) for unsupervised data compression'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）用于无监督数据压缩'
- en: '**Linear discriminant analysis** (**LDA**) as a supervised dimensionality reduction
    technique for maximizing class separability'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性判别分析**（**LDA**）作为一种监督式维度减少技术，用于最大化类分离度'
- en: Nonlinear dimensionality reduction via **kernel principal component analysis**
    (**KPCA**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过**核主成分分析**（**KPCA**）的非线性维度减少
- en: Unsupervised dimensionality reduction via principal component analysis
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过主成分分析进行无监督的维度减少
- en: Similar to feature selection, we can use different feature extraction techniques
    to reduce the number of features in a dataset. The difference between feature
    selection and feature extraction is that while we maintain the original features
    when we use feature selection algorithms, such as **sequential backward selection**,
    we use feature extraction to transform or project the data onto a new feature
    space.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于特征选择，我们可以使用不同的特征提取技术来减少数据集中的特征数量。特征选择和特征提取的区别在于，使用特征选择算法（如**顺序后向选择**）时，我们保留原始特征，而使用特征提取时，我们将数据转换或投影到一个新的特征空间。
- en: In the context of dimensionality reduction, feature extraction can be understood
    as an approach to data compression with the goal of maintaining most of the relevant
    information. In practice, feature extraction is not only used to improve storage
    space or the computational efficiency of the learning algorithm, but can also
    improve the predictive performance by reducing the **curse of dimensionality**—especially
    if we are working with non-regularized models.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在维度减少的背景下，特征提取可以理解为一种数据压缩方法，目的是保持大部分相关信息。实际上，特征提取不仅用于提高存储空间或学习算法的计算效率，还可以通过减少**维度诅咒**来提高预测性能——特别是当我们使用非正则化模型时。
- en: The main steps behind principal component analysis
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析的主要步骤
- en: In this section, we will discuss PCA, an unsupervised linear transformation
    technique that is widely used across different fields, most prominently for feature
    extraction and dimensionality reduction. Other popular applications of PCA include
    exploratory data analyses and the denoising of signals in stock market trading,
    and the analysis of genome data and gene expression levels in the field of bioinformatics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论主成分分析（PCA），这是一种无监督的线性变换技术，广泛应用于不同领域，最显著的应用包括特征提取和维度减少。PCA的其他常见应用包括探索性数据分析、股市交易中的信号去噪、以及生物信息学领域中基因组数据和基因表达水平的分析。
- en: 'PCA helps us to identify patterns in data based on the correlation between
    features. In a nutshell, PCA aims to find the directions of maximum variance in
    high-dimensional data and projects the data onto a new subspace with equal or
    fewer dimensions than the original one. The orthogonal axes (principal components)
    of the new subspace can be interpreted as the directions of maximum variance given
    the constraint that the new feature axes are orthogonal to each other, as illustrated
    in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: PCA帮助我们根据特征之间的相关性识别数据中的模式。简而言之，PCA旨在找到高维数据中最大方差的方向，并将数据投影到一个新的子空间，该子空间的维度与原始子空间相等或更少。新子空间的正交轴（主成分）可以解释为在新特征轴互相正交的约束下，最大方差的方向，如下图所示：
- en: '![](img/B13208_05_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_01.png)'
- en: In the preceding figure, ![](img/B13208_05_001.png) and ![](img/B13208_05_002.png)
    are the original feature axes, and **PC1** and **PC2** are the principal components.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，![](img/B13208_05_001.png)和![](img/B13208_05_002.png)是原始特征轴，**PC1**和**PC2**是主成分。
- en: 'If we use PCA for dimensionality reduction, we construct a ![](img/B13208_05_003.png)-dimensional
    transformation matrix, *W*, that allows us to map a vector, *x*, the features
    of a training example, onto a new *k*-dimensional feature subspace that has fewer
    dimensions than the original *d*-dimensional feature space. For instance, the
    process is as follows. Suppose we have a feature vector, *x*:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用PCA进行降维，我们构造一个![](img/B13208_05_003.png)-维的变换矩阵，*W*，它使我们能够将一个向量，*x*，即训练样本的特征，映射到一个新的*k*-维特征子空间，该子空间的维度比原始的*d*-维特征空间要低。例如，过程如下。假设我们有一个特征向量，*x*：
- en: '![](img/B13208_05_005.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_005.png)'
- en: 'which is then transformed by a transformation matrix, ![](img/B13208_05_006.png):'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过变换矩阵进行转换，![](img/B13208_05_006.png)：
- en: '![](img/B13208_05_007.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_007.png)'
- en: 'resulting in the output vector:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 从而得到输出向量：
- en: '![](img/B13208_05_008.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_008.png)'
- en: As a result of transforming the original *d*-dimensional data onto this new
    *k*-dimensional subspace (typically *k* << *d*), the first principal component
    will have the largest possible variance. All consequent principal components will
    have the largest variance given the constraint that these components are uncorrelated
    (orthogonal) to the other principal components—even if the input features are
    correlated, the resulting principal components will be mutually orthogonal (uncorrelated).
    Note that the PCA directions are highly sensitive to data scaling, and we need
    to standardize the features *prior* to PCA if the features were measured on different
    scales and we want to assign equal importance to all features.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 由于将原始的*d*-维数据转换到这个新的*k*-维子空间（通常*k* << *d*）的结果，第一个主成分将具有最大的方差。所有后续的主成分将具有最大方差，前提是这些成分与其他主成分不相关（正交）——即使输入特征是相关的，结果主成分也会是互相正交的（不相关）。请注意，PCA的方向对数据缩放非常敏感，如果特征是以不同的尺度度量的，而我们希望为所有特征分配相等的重要性，那么我们需要在执行PCA之前对特征进行标准化。
- en: 'Before looking at the PCA algorithm for dimensionality reduction in more detail,
    let''s summarize the approach in a few simple steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在更详细地了解PCA降维算法之前，让我们用几个简单的步骤总结一下该方法：
- en: Standardize the *d*-dimensional dataset.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化*d*-维数据集。
- en: Construct the covariance matrix.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建协方差矩阵。
- en: Decompose the covariance matrix into its eigenvectors and eigenvalues.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将协方差矩阵分解为其特征向量和特征值。
- en: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按特征值的降序排列，以对相应的特征向量进行排序。
- en: Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where
    *k* is the dimensionality of the new feature subspace (![](img/B13208_05_009.png)).
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与*k*个最大特征值相对应的*k*个特征向量，其中*k*是新特征子空间的维度（![](img/B13208_05_009.png)）。
- en: Construct a projection matrix, *W*, from the "top" *k* eigenvectors.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从“前面”的*k*个特征向量构建投影矩阵*W*。
- en: Transform the *d*-dimensional input dataset, *X*, using the projection matrix,
    *W*, to obtain the new *k*-dimensional feature subspace.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用投影矩阵*W*对*d*-维输入数据集*X*进行变换，以获得新的*k*-维特征子空间。
- en: In the following sections, we will perform a PCA step by step, using Python
    as a learning exercise. Then, we will see how to perform a PCA more conveniently
    using scikit-learn.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将逐步执行PCA，使用Python作为学习练习。然后，我们将看到如何使用scikit-learn更方便地执行PCA。
- en: Extracting the principal components step by step
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤提取主成分
- en: 'In this subsection, we will tackle the first four steps of a PCA:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将处理PCA的前四个步骤：
- en: Standardizing the data.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化数据。
- en: Constructing the covariance matrix.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建协方差矩阵。
- en: Obtaining the eigenvalues and eigenvectors of the covariance matrix.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取协方差矩阵的特征值和特征向量。
- en: Sorting the eigenvalues by decreasing order to rank the eigenvectors.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照降序排列特征值，以对特征向量进行排序。
- en: 'First, we will start by loading the Wine dataset that we were working with
    in *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从 *第4章* 中使用过的Wine数据集开始加载：*构建良好的训练数据集——数据预处理*：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '**Obtaining the Wine dataset**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**获取Wine数据集**'
- en: 'You can find a copy of the Wine dataset (and all other datasets used in this
    book) in the code bundle of this book, which you can use if you are working offline
    or the UCI server at [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)
    is temporarily unavailable. For instance, to load the Wine dataset from a local
    directory, you can replace the following line:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的代码包中找到一份Wine数据集的副本（以及本书中使用的所有其他数据集），如果你在离线工作或者UCI服务器（[https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data)）暂时无法访问时，可以使用这份副本。例如，要从本地目录加载Wine数据集，你可以替换以下代码行：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'with the following one:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 以及以下条件：
- en: '[PRE2]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will process the Wine data into separate training and test datasets—using
    70 percent and 30 percent of the data, respectively—and standardize it to unit
    variance:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将Wine数据分为训练集和测试集，分别使用数据的70%和30%，并将其标准化为单位方差：
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'After completing the mandatory preprocessing by executing the preceding code,
    let''s advance to the second step: constructing the covariance matrix. The symmetric
    ![](img/B13208_05_010.png)-dimensional covariance matrix, where *d* is the number
    of dimensions in the dataset, stores the pairwise covariances between the different
    features. For example, the covariance between two features, ![](img/B13208_05_011.png)
    and ![](img/B13208_05_012.png), on the population level can be calculated via
    the following equation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面的必要预处理代码后，接下来我们进入第二步：构建协方差矩阵。对称的 ![](img/B13208_05_010.png) 维协方差矩阵，其中 *d*
    是数据集中的维度数，存储不同特征之间的成对协方差。例如，两个特征 ![](img/B13208_05_011.png) 和 ![](img/B13208_05_012.png)
    在总体水平上的协方差可以通过以下公式计算：
- en: '![](img/B13208_05_013.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_013.png)'
- en: 'Here, ![](img/B13208_05_014.png) and ![](img/B13208_05_015.png) are the sample
    means of features *j* and *k*, respectively. Note that the sample means are zero
    if we standardized the dataset. A positive covariance between two features indicates
    that the features increase or decrease together, whereas a negative covariance
    indicates that the features vary in opposite directions. For example, the covariance
    matrix of three features can then be written as follows (note that ![](img/B13208_05_016.png)
    stands for the Greek uppercase letter sigma, which is not to be confused with
    the summation symbol):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_05_014.png) 和 ![](img/B13208_05_015.png) 分别是特征 *j* 和 *k* 的样本均值。请注意，如果我们对数据集进行了标准化，样本均值将为零。两个特征之间的正协方差表示这两个特征一起增加或减少，而负协方差则表示它们以相反的方向变化。例如，三个特征的协方差矩阵可以写成如下形式（注意，![](img/B13208_05_016.png)
    代表希腊大写字母 sigma，与求和符号区分开）：
- en: '![](img/B13208_05_017.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_017.png)'
- en: The eigenvectors of the covariance matrix represent the principal components
    (the directions of maximum variance), whereas the corresponding eigenvalues will
    define their magnitude. In the case of the Wine dataset, we would obtain 13 eigenvectors
    and eigenvalues from the ![](img/B13208_05_018.png)-dimensional covariance matrix.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵的特征向量代表主成分（最大方差的方向），而相应的特征值则定义了它们的大小。在Wine数据集的情况下，我们将从该 ![](img/B13208_05_018.png)
    维的协方差矩阵中获得13个特征向量和特征值。
- en: 'Now, for our third step, let''s obtain the eigenpairs of the covariance matrix.
    As you will remember from our introductory linear algebra classes, an eigenvector,
    *v*, satisfies the following condition:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，进入第三步，让我们获得协方差矩阵的特征对。如你从我们的线性代数入门课程中记得，特征向量 *v* 满足以下条件：
- en: '![](img/B13208_05_019.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_019.png)'
- en: 'Here, ![](img/B13208_04_020.png) is a scalar: the eigenvalue. Since the manual
    computation of eigenvectors and eigenvalues is a somewhat tedious and elaborate
    task, we will use the `linalg.eig` function from NumPy to obtain the eigenpairs
    of the Wine covariance matrix:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_04_020.png)是一个标量：特征值。由于手动计算特征向量和特征值是一个相当繁琐和复杂的任务，我们将使用NumPy的`linalg.eig`函数来获取Wine协方差矩阵的特征对：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Using the `numpy.cov` function, we computed the covariance matrix of the standardized
    training dataset. Using the `linalg.eig` function, we performed the eigendecomposition,
    which yielded a vector (`eigen_vals`) consisting of 13 eigenvalues and the corresponding
    eigenvectors stored as columns in a ![](img/B13208_05_021.png)-dimensional matrix
    (`eigen_vecs`).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`numpy.cov`函数，我们计算了标准化训练数据集的协方差矩阵。通过使用`linalg.eig`函数，我们进行了特征分解，得到一个向量（`eigen_vals`），其中包含13个特征值，以及存储为列的对应特征向量，保存在一个![](img/B13208_05_021.png)维度的矩阵（`eigen_vecs`）中。
- en: '**Eigendecomposition in NumPy**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy中的特征分解**'
- en: The `numpy.linalg.eig` function was designed to operate on both symmetric and
    non-symmetric square matrices. However, you may find that it returns complex eigenvalues
    in certain cases.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`numpy.linalg.eig`函数被设计为可以处理对称矩阵和非对称矩阵。然而，你可能会发现它在某些情况下返回复特征值。'
- en: A related function, `numpy.linalg.eigh`, has been implemented to decompose Hermetian
    matrices, which is a numerically more stable approach to working with symmetric
    matrices such as the covariance matrix; `numpy.linalg.eigh` always returns real
    eigenvalues.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 一个相关的函数`numpy.linalg.eigh`已经实现，它用于分解厄米矩阵，这是处理对称矩阵（如协方差矩阵）的数值上更稳定的方法；`numpy.linalg.eigh`总是返回实特征值。
- en: Total and explained variance
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总方差和解释方差
- en: 'Since we want to reduce the dimensionality of our dataset by compressing it
    onto a new feature subspace, we only select the subset of the eigenvectors (principal
    components) that contains most of the information (variance). The eigenvalues
    define the magnitude of the eigenvectors, so we have to sort the eigenvalues by
    decreasing magnitude; we are interested in the top *k* eigenvectors based on the
    values of their corresponding eigenvalues. But before we collect those *k* most
    informative eigenvectors, let''s plot the **variance explained ratios** of the
    eigenvalues. The variance explained ratio of an eigenvalue, ![](img/B13208_05_023.png),
    is simply the fraction of an eigenvalue, ![](img/B13208_05_023.png), and the total
    sum of the eigenvalues:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们希望通过将数据集压缩到一个新的特征子空间来减少数据的维度，因此我们只选择包含大部分信息（方差）的特征向量（主成分）的子集。特征值定义了特征向量的大小，因此我们需要按特征值的大小降序排序；我们关注的是基于其对应特征值的前*k*个特征向量。但在收集这*k*个最具信息量的特征向量之前，我们先绘制**方差解释比例**图。特征值的方差解释比例，![](img/B13208_05_023.png)，只是一个特征值，![](img/B13208_05_023.png)，与所有特征值总和的比值：
- en: '![](img/B13208_05_024.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_024.png)'
- en: 'Using the NumPy `cumsum` function, we can then calculate the cumulative sum
    of explained variances, which we will then plot via Matplotlib''s `step` function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy的`cumsum`函数，我们可以计算解释方差的累积和，然后通过Matplotlib的`step`函数绘制：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The resulting plot indicates that the first principal component alone accounts
    for approximately 40 percent of the variance.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示，单独的第一个主成分约占总方差的40%。
- en: 'Also, we can see that the first two principal components combined explain almost
    60 percent of the variance in the dataset:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 同时，我们还可以看到，前两个主成分组合解释了数据集近60%的方差：
- en: '![](img/B13208_05_02.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_02.png)'
- en: Although the explained variance plot reminds us of the feature importance values
    that we computed in *Chapter 4*, *Building Good Training Datasets – Data Preprocessing*,
    via random forests, we should remind ourselves that PCA is an unsupervised method,
    which means that information about the class labels is ignored. Whereas a random
    forest uses the class membership information to compute the node impurities, variance
    measures the spread of values along a feature axis.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管解释方差图让我们想起了我们在*第4章*，*构建良好的训练数据集—数据预处理*中通过随机森林计算的特征重要性值，但我们应该提醒自己，PCA是一种无监督方法，这意味着它忽略了类别标签的信息。而随机森林则使用类别成员信息来计算节点的杂质，方差则衡量了特征轴上值的分布。
- en: Feature transformation
  id: totrans-68
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征转换
- en: 'Now that we have successfully decomposed the covariance matrix into eigenpairs,
    let''s proceed with the last three steps to transform the Wine dataset onto the
    new principal component axes. The remaining steps we are going to tackle in this
    section are the following:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经成功地将协方差矩阵分解为特征对，让我们继续进行最后三步，将Wine数据集变换到新的主成分轴上。接下来我们将处理的步骤如下：
- en: Select *k* eigenvectors, which correspond to the *k* largest eigenvalues, where
    *k* is the dimensionality of the new feature subspace (![](img/B13208_05_025.png)).
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择 *k* 个特征向量，这些特征向量对应于 *k* 个最大特征值，其中 *k* 是新特征子空间的维度（![](img/B13208_05_025.png)）。
- en: Construct a projection matrix, *W*, from the "top" *k* eigenvectors.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从“前 *k* ”个特征向量构造投影矩阵 *W*。
- en: Transform the *d*-dimensional input dataset, *X*, using the projection matrix,
    *W*, to obtain the new *k*-dimensional feature subspace.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用投影矩阵 *W* 对 *d* 维输入数据集 *X* 进行变换，以获得新的 *k* 维特征子空间。
- en: Or, in less technical terms, we will sort the eigenpairs by descending order
    of the eigenvalues, construct a projection matrix from the selected eigenvectors,
    and use the projection matrix to transform the data onto the lower-dimensional
    subspace.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，用更通俗的说法，我们将按特征值的降序排序特征对，从选定的特征向量构建投影矩阵，并使用投影矩阵将数据变换到低维子空间。
- en: 'We start by sorting the eigenpairs by decreasing order of the eigenvalues:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先按特征值降序排列特征对：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we collect the two eigenvectors that correspond to the two largest eigenvalues,
    to capture about 60 percent of the variance in this dataset. Note that two eigenvectors
    have been chosen for the purpose of illustration, since we are going to plot the
    data via a two-dimensional scatter plot later in this subsection. In practice,
    the number of principal components has to be determined by a tradeoff between
    computational efficiency and the performance of the classifier:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们收集对应于两个最大特征值的两个特征向量，以捕捉数据集中约60%的方差。注意，选择了两个特征向量用于说明，因为稍后我们将在本小节中通过二维散点图绘制数据。实际上，主成分的数量需要通过计算效率与分类器性能之间的权衡来确定：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: By executing the preceding code, we have created a ![](img/B13208_05_026.png)-dimensional
    projection matrix, *W*, from the top two eigenvectors.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前面的代码后，我们已从前两个特征向量创建了一个 ![](img/B13208_05_026.png) 维的投影矩阵 *W*。
- en: '**Mirrored projections**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**镜像投影**'
- en: 'Depending on which versions of NumPy and LAPACK you are using, you may obtain
    the matrix, *W*, with its signs flipped. Please note that this is not an issue;
    if *v* is an eigenvector of a matrix, ![](img/B13208_05_027.png), we have:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你使用的NumPy和LAPACK版本，可能会得到带符号反转的矩阵 *W*。请注意，这不是问题；如果 *v* 是矩阵的特征向量 ![](img/B13208_05_027.png)，那么我们有：
- en: '![](img/B13208_05_028.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_028.png)'
- en: 'Here, *v* is the eigenvector, and *–v* is also an eigenvector, which we can
    show as follows. Using basic algebra, we can multiply both sides of the equation
    by a scalar, ![](img/B13208_05_029.png):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*v* 是特征向量，而 *–v* 也是特征向量，我们可以如下证明这一点。使用基本的代数，我们可以将方程两边同时乘以一个标量，![](img/B13208_05_029.png)：
- en: '![](img/B13208_05_030.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_030.png)'
- en: 'Since matrix multiplication is associative for scalar multiplication, we can
    then rearrange this to the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 由于矩阵乘法对于标量乘法是结合律的，我们可以将其重新排列为以下形式：
- en: '![](img/B13208_05_031.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_031.png)'
- en: Now, we can see that ![](img/B13208_05_032.png) is an eigenvector with the same
    eigenvalue, ![](img/B13208_05_033.png), for both ![](img/B13208_05_034.png) and
    ![](img/B13208_05_035.png). Hence, both *v* and *–v* are eigenvectors.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到 ![](img/B13208_05_032.png) 是一个特征向量，具有相同的特征值 ![](img/B13208_05_033.png)，适用于
    ![](img/B13208_05_034.png) 和 ![](img/B13208_05_035.png)。因此，*v* 和 *–v* 都是特征向量。
- en: 'Using the projection matrix, we can now transform an example, *x* (represented
    as a 13-dimensional row vector), onto the PCA subspace (the principal components
    one and two) obtaining ![](img/B13208_05_037.png), now a two-dimensional example
    vector consisting of two new features:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用投影矩阵，我们现在可以将一个示例 *x*（表示为13维行向量）变换到PCA子空间（主成分一和二），得到 ![](img/B13208_05_037.png)，现在这是一个二维示例向量，由两个新特征组成：
- en: '![](img/B13208_05_038.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_038.png)'
- en: '[PRE8]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Similarly, we can transform the entire ![](img/B13208_05_039.png)-dimensional
    training dataset onto the two principal components by calculating the matrix dot
    product:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以通过计算矩阵点积，将整个 ![](img/B13208_05_039.png) 维训练数据集变换到两个主成分上：
- en: '![](img/B13208_05_040.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_040.png)'
- en: '[PRE9]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Lastly, let''s visualize the transformed Wine training dataset, now stored
    as an ![](img/B13208_05_041.png)-dimensional matrix, in a two-dimensional scatterplot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将经过转换的 Wine 训练数据集（现在作为 ![](img/B13208_05_041.png) 维矩阵存储）在二维散点图中进行可视化：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As we can see in the resulting plot, the data is more spread along the *x*-axis—the
    first principal component—than the second principal component (*y*-axis), which
    is consistent with the explained variance ratio plot that we created in the previous
    subsection. However, we can tell that a linear classifier will likely be able
    to separate the classes well:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在结果图中所见，数据沿着 *x* 轴——第一主成分——比第二主成分（*y* 轴）分布得更广，这与我们在前一小节中创建的解释方差比率图一致。然而，我们可以看出，线性分类器很可能能够很好地分离这些类别：
- en: '![](img/B13208_05_03.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_03.png)'
- en: Although we encoded the class label information for the purpose of illustration
    in the preceding scatter plot, we have to keep in mind that PCA is an unsupervised
    technique that doesn't use any class label information.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在前述散点图中对类别标签信息进行了编码以便于说明，但我们必须牢记，PCA 是一种无监督技术，不使用任何类别标签信息。
- en: Principal component analysis in scikit-learn
  id: totrans-98
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn 中的主成分分析（PCA）
- en: Although the verbose approach in the previous subsection helped us to follow
    the inner workings of PCA, we will now discuss how to use the `PCA` class implemented
    in scikit-learn.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前一小节中的详细方法帮助我们理解了 PCA 的内部工作原理，但现在我们将讨论如何使用 scikit-learn 中实现的 `PCA` 类。
- en: 'The `PCA` class is another one of scikit-learn''s transformer classes, where
    we first fit the model using the training data before we transform both the training
    data and the test dataset using the same model parameters. Now, let''s use the
    `PCA` class from scikit-learn on the Wine training dataset, classify the transformed
    examples via logistic regression, and visualize the decision regions via the `plot_decision_regions`
    function that we defined in *Chapter 2*, *Training Simple Machine Learning Algorithms
    for Classification*:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`PCA` 类是 scikit-learn 中的另一个变换器类，我们首先使用训练数据拟合模型，然后使用相同的模型参数对训练数据和测试数据集进行变换。现在，让我们在
    Wine 训练数据集上使用 scikit-learn 的 `PCA` 类，通过逻辑回归对转换后的样本进行分类，并通过我们在*第2章*中定义的 `plot_decision_regions`
    函数可视化决策区域，*训练简单的机器学习分类算法*：'
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For your convenience, you can place the `plot_decision_regions` code shown above
    into a separate code file in your current working directory, for example, `plot_decision_regions_script.py`,
    and import it into your current Python session.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，您可以将上述的 `plot_decision_regions` 代码放入当前工作目录中的单独代码文件中，例如 `plot_decision_regions_script.py`，并将其导入到当前的
    Python 会话中。
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'By executing the preceding code, we should now see the decision regions for
    the training data reduced to two principal component axes:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行前述代码，我们现在应该能看到训练数据在两个主成分轴上的决策区域：
- en: '![](img/B13208_05_04.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_04.png)'
- en: When we compare PCA projections via scikit-learn with our own PCA implementation,
    it can happen that the resulting plots are mirror images of each other. Note that
    this is not due to an error in either of those two implementations; the reason
    for this difference is that, depending on the eigensolver, eigenvectors can have
    either negative or positive signs.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将通过 scikit-learn 进行的 PCA 投影与我们自己实现的 PCA 进行比较时，结果图可能是彼此的镜像。请注意，这并不是由于这两种实现中的任何错误；这种差异的原因是，取决于特征求解器，特征向量可能具有负号或正号。
- en: 'Not that it matters, but we could simply revert the mirror image by multiplying
    the data by –1 if we wanted to; note that eigenvectors are typically scaled to
    unit length 1\. For the sake of completeness, let''s plot the decision regions
    of the logistic regression on the transformed test dataset to see if it can separate
    the classes well:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不影响结果，但如果我们希望，可以通过将数据乘以 –1 来简单地反转镜像；请注意，特征向量通常会被缩放到单位长度 1。为了完整性，让我们绘制经过转换的测试数据集上逻辑回归的决策区域，看看它是否能够很好地分离类别：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After we plot the decision regions for the test dataset by executing the preceding
    code, we can see that logistic regression performs quite well on this small two-dimensional
    feature subspace and only misclassifies a few examples in the test dataset:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行前述代码绘制测试数据集的决策区域后，我们可以看到，逻辑回归在这个小的二维特征子空间中表现得相当好，仅仅将测试数据集中的少数几个样本分类错误：
- en: '![](img/B13208_05_05.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_05.png)'
- en: 'If we are interested in the explained variance ratios of the different principal
    components, we can simply initialize the `PCA` class with the `n_components` parameter
    set to `None`, so all principal components are kept and the explained variance
    ratio can then be accessed via the `explained_variance_ratio_` attribute:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对不同主成分的解释方差比感兴趣，可以简单地将 `PCA` 类初始化，并将 `n_components` 参数设置为 `None`，这样所有主成分都会被保留，然后可以通过
    `explained_variance_ratio_` 属性访问解释方差比：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note that we set `n_components=None` when we initialized the `PCA` class so
    that it will return all principal components in a sorted order, instead of performing
    a dimensionality reduction.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们初始化 `PCA` 类时，我们将 `n_components=None`，这样它将按排序顺序返回所有主成分，而不是进行降维。
- en: Supervised data compression via linear discriminant analysis
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过线性判别分析进行的监督式数据压缩
- en: LDA can be used as a technique for feature extraction to increase the computational
    efficiency and reduce the degree of overfitting due to the curse of dimensionality
    in non-regularized models. The general concept behind LDA is very similar to PCA,
    but whereas PCA attempts to find the orthogonal component axes of maximum variance
    in a dataset, the goal in LDA is to find the feature subspace that optimizes class
    separability. In the following sections, we will discuss the similarities between
    LDA and PCA in more detail and walk through the LDA approach step by step.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 可以作为一种特征提取技术，用于提高计算效率并减少由于高维数据导致的过拟合程度，尤其是在非正则化模型中。LDA 的基本概念与 PCA 非常相似，但
    PCA 旨在寻找数据集中最大方差的正交分量轴，而 LDA 的目标是找到优化类间可分性的特征子空间。在接下来的部分中，我们将更详细地讨论 LDA 和 PCA
    之间的相似性，并逐步讲解 LDA 方法。
- en: Principal component analysis versus linear discriminant analysis
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 主成分分析与线性判别分析
- en: 'Both PCA and LDA are linear transformation techniques that can be used to reduce
    the number of dimensions in a dataset; the former is an unsupervised algorithm,
    whereas the latter is supervised. Thus, we might think that LDA is a superior
    feature extraction technique for classification tasks compared to PCA. However,
    A.M. Martinez reported that preprocessing via PCA tends to result in better classification
    results in an image recognition task in certain cases, for instance, if each class
    consists of only a small number of examples (*PCA Versus LDA*, *A. M. Martinez*
    and *A. C. Kak*, *IEEE Transactions on Pattern Analysis and Machine Intelligence*,
    23(2): 228-233, *2001*).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 'PCA 和 LDA 都是可以用于减少数据集维度的线性变换技术；前者是无监督算法，而后者是监督算法。因此，我们可能认为 LDA 在分类任务中是优于 PCA
    的特征提取技术。然而，A.M. Martinez 报告称，在某些情况下，通过 PCA 进行预处理会导致更好的分类结果，尤其是在图像识别任务中，例如当每个类仅包含少量示例时（*PCA
    与 LDA*，*A. M. Martinez* 和 *A. C. Kak*，*IEEE Transactions on Pattern Analysis and
    Machine Intelligence*，23(2): 228-233，*2001*）。'
- en: '**Fisher LDA**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**Fisher LDA**'
- en: 'LDA is sometimes also called Fisher''s LDA. Ronald A. Fisher initially formulated
    *Fisher''s Linear Discriminant* for two-class classification problems in 1936
    (*The Use of Multiple Measurements in Taxonomic Problems*, *R. A. Fisher*, *Annals
    of Eugenics*, 7(2): 179-188, *1936*). Fisher''s linear discriminant was later
    generalized for multi-class problems by C. Radhakrishna Rao under the assumption
    of equal class covariances and normally distributed classes in 1948, which we
    now call LDA (*The Utilization of Multiple Measurements in Problems of Biological
    Classification*, *C. R. Rao*, *Journal of the Royal Statistical Society*. Series
    B (Methodological), 10(2): 159-203, *1948*).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 'LDA 有时也被称为 Fisher 的 LDA。Ronald A. Fisher 于 1936 年首次为二类分类问题提出了 *Fisher 线性判别*（*The
    Use of Multiple Measurements in Taxonomic Problems*，*R. A. Fisher*，*Annals of
    Eugenics*，7(2): 179-188，*1936*）。Fisher 的线性判别后来由 C. Radhakrishna Rao 在 1948 年针对多类问题进行了推广，假设类内协方差相等且各类呈正态分布，这一方法我们现在称为
    LDA（*The Utilization of Multiple Measurements in Problems of Biological Classification*，*C.
    R. Rao*，*Journal of the Royal Statistical Society*，Series B（Methodological），10(2):
    159-203，*1948*）。'
- en: 'The following figure summarizes the concept of LDA for a two-class problem.
    Examples from class 1 are shown as circles, and examples from class 2 are shown
    as crosses:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 下图总结了用于二类问题的 LDA 概念。类 1 的示例用圆圈表示，类 2 的示例用叉号表示：
- en: '![](img/B13208_05_06.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_06.png)'
- en: A linear discriminant, as shown on the *x*-axis (LD 1), would separate the two
    normal distributed classes well. Although the exemplary linear discriminant shown
    on the *y*-axis (LD 2) captures a lot of the variance in the dataset, it would
    fail as a good linear discriminant since it does not capture any of the class-discriminatory
    information.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如*x*轴所示的线性判别（LD 1），能够很好地分开两个正态分布的类别。尽管如*y*轴所示的示例线性判别（LD 2）捕获了数据集中的大量方差，但由于它没有捕捉到任何类别区分信息，因此不能作为有效的线性判别。
- en: One assumption in LDA is that the data is normally distributed. Also, we assume
    that the classes have identical covariance matrices and that the training examples
    are statistically independent of each other. However, even if one, or more, of
    those assumptions is (slightly) violated, LDA for dimensionality reduction can
    still work reasonably well (*Pattern Classification 2nd Edition*, *R. O. Duda*,
    *P. E. Hart*, and *D. G. Stork*, *New York*, *2001*).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的一个假设是数据呈正态分布。此外，我们假设类别具有相同的协方差矩阵，并且训练样本彼此统计独立。然而，即使这些假设中的一个或多个被（稍微）违反，LDA作为降维方法仍然可以合理有效地工作（*Pattern
    Classification 第二版*，*R. O. Duda*，*P. E. Hart*，和*D. G. Stork*，*纽约*，*2001*）。
- en: The inner workings of linear discriminant analysis
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性判别分析的内部工作原理
- en: 'Before we dive into the code implementation, let''s briefly summarize the main
    steps that are required to perform LDA:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入代码实现之前，让我们简要总结一下执行LDA所需的主要步骤：
- en: Standardize the *d*-dimensional dataset (*d* is the number of features).
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标准化*d*维数据集（*d*是特征的数量）。
- en: For each class, compute the *d*-dimensional mean vector.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个类别，计算*d*维均值向量。
- en: Construct the between-class scatter matrix, ![](img/B13208_05_042.png), and
    the within-class scatter matrix, ![](img/B13208_05_043.png).
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建类间散布矩阵![](img/B13208_05_042.png)和类内散布矩阵![](img/B13208_05_043.png)。
- en: Compute the eigenvectors and corresponding eigenvalues of the matrix, ![](img/B13208_05_044.png).
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算矩阵![](img/B13208_05_044.png)的特征向量及其对应的特征值。
- en: Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按降序排列特征值，以对应的特征向量进行排名。
- en: Choose the *k* eigenvectors that correspond to the *k* largest eigenvalues to
    construct a ![](img/B13208_05_045.png)-dimensional transformation matrix, *W*;
    the eigenvectors are the columns of this matrix.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与前*k*个最大特征值对应的*k*个特征向量，以构建一个![](img/B13208_05_045.png)维的转换矩阵*W*；特征向量是该矩阵的列。
- en: Project the examples onto the new feature subspace using the transformation
    matrix, *W*.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用转换矩阵*W*将样本投影到新的特征子空间。
- en: As we can see, LDA is quite similar to PCA in the sense that we are decomposing
    matrices into eigenvalues and eigenvectors, which will form the new lower-dimensional
    feature space. However, as mentioned before, LDA takes class label information
    into account, which is represented in the form of the mean vectors computed in
    step 2\. In the following sections, we will discuss these seven steps in more
    detail, accompanied by illustrative code implementations.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，LDA与PCA非常相似，因为我们将矩阵分解为特征值和特征向量，这些将形成新的低维特征空间。然而，正如前面提到的，LDA考虑了类别标签信息，这些信息以第2步中计算的均值向量的形式表示。在接下来的章节中，我们将更详细地讨论这七个步骤，并提供示例代码实现。
- en: Computing the scatter matrices
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算散布矩阵
- en: 'Since we already standardized the features of the Wine dataset in the PCA section
    at the beginning of this chapter, we can skip the first step and proceed with
    the calculation of the mean vectors, which we will use to construct the within-class
    scatter matrix and between-class scatter matrix, respectively. Each mean vector,
    ![](img/B13208_05_046.png), stores the mean feature value, ![](img/B13208_05_047.png),
    with respect to the examples of class *i*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在本章开头的PCA部分已经对Wine数据集的特征进行了标准化，因此我们可以跳过第一步，直接计算均值向量，然后使用这些均值向量分别构建类内散布矩阵和类间散布矩阵。每个均值向量![](img/B13208_05_046.png)存储了类别*i*样本的均值特征值![](img/B13208_05_047.png)：
- en: '![](img/B13208_05_048.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_048.png)'
- en: 'This results in three mean vectors:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生三个均值向量：
- en: '![](img/B13208_05_049.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_049.png)'
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Using the mean vectors, we can now compute the within-class scatter matrix,
    ![](img/B13208_05_050.png):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均值向量，我们现在可以计算类内散布矩阵![](img/B13208_05_050.png)：
- en: '![](img/B13208_05_051.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_051.png)'
- en: 'This is calculated by summing up the individual scatter matrices, ![](img/B13208_05_052.png),
    of each individual class *i*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过对每个类*i*的个别散布矩阵![](img/B13208_05_052.png)求和来计算的：
- en: '![](img/B13208_05_053.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_053.png)'
- en: '[PRE16]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The assumption that we are making when we are computing the scatter matrices
    is that the class labels in the training dataset are uniformly distributed. However,
    if we print the number of class labels, we see that this assumption is violated:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在计算散布矩阵时所做的假设是训练数据集中的类别标签是均匀分布的。然而，如果我们打印类别标签的数量，会发现这一假设被违背了：
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Thus, we want to scale the individual scatter matrices, ![](img/B13208_05_054.png),
    before we sum them up as scatter matrix ![](img/B13208_05_055.png). When we divide
    the scatter matrices by the number of class-examples, ![](img/B13208_05_056.png),
    we can see that computing the scatter matrix is in fact the same as computing
    the covariance matrix, ![](img/B13208_05_057.png)—the covariance matrix is a normalized
    version of the scatter matrix:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们希望在将个别散布矩阵![](img/B13208_05_054.png)相加形成散布矩阵![](img/B13208_05_055.png)之前，先对其进行缩放。当我们通过类别样本数量![](img/B13208_05_056.png)来除以散布矩阵时，我们可以看到，计算散布矩阵实际上与计算协方差矩阵是一样的![](img/B13208_05_057.png)—协方差矩阵是散布矩阵的标准化版本：
- en: '![](img/B13208_05_058.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_058.png)'
- en: 'The code for computing the scaled within-class scatter matrix is as follows:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 计算缩放后的类内散布矩阵的代码如下：
- en: '[PRE18]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After we compute the scaled within-class scatter matrix (or covariance matrix),
    we can move on to the next step and compute the between-class scatter matrix ![](img/B13208_05_059.png):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算完缩放后的类内散布矩阵（或协方差矩阵）后，我们可以进入下一步，计算类间散布矩阵![](img/B13208_05_059.png)：
- en: '![](img/B13208_05_060.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_060.png)'
- en: 'Here, *m* is the overall mean that is computed, including examples from all
    *c* classes:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*m*是计算得到的总体均值，包含来自所有*c*类别的样本：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Selecting linear discriminants for the new feature subspace
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为新的特征子空间选择线性判别
- en: 'The remaining steps of the LDA are similar to the steps of the PCA. However,
    instead of performing the eigendecomposition on the covariance matrix, we solve
    the generalized eigenvalue problem of the matrix, ![](img/B13208_05_061.png):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: LDA的其余步骤与PCA的步骤类似。然而，代替对协方差矩阵进行特征分解，我们需要解矩阵![](img/B13208_05_061.png)的广义特征值问题：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'After we compute the eigenpairs, we can sort the eigenvalues in descending
    order:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算特征对后，我们可以按降序排列特征值：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: In LDA, the number of linear discriminants is at most *c*−1, where *c* is the
    number of class labels, since the in-between scatter matrix, ![](img/B13208_05_062.png),
    is the sum of *c* matrices with rank one or less. We can indeed see that we only
    have two nonzero eigenvalues (the eigenvalues 3-13 are not exactly zero, but this
    is due to the floating-point arithmetic in NumPy).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在LDA中，线性判别数最多为*c*−1，其中*c*是类别标签的数量，因为类间散布矩阵![](img/B13208_05_062.png)是*c*个秩为1或更小的矩阵的总和。我们确实可以看到，只有两个非零特征值（特征值3-13并不完全为零，但这是由于NumPy中的浮点运算造成的）。
- en: '**Collinearity**'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**共线性**'
- en: Note that in the rare case of perfect collinearity (all aligned example points
    fall on a straight line), the covariance matrix would have rank one, which would
    result in only one eigenvector with a nonzero eigenvalue.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在完美共线性（所有对齐的样本点都位于一条直线上）的稀有情况下，协方差矩阵的秩为1，这将导致只有一个特征向量具有非零特征值。
- en: 'To measure how much of the class-discriminatory information is captured by
    the linear discriminants (eigenvectors), let''s plot the linear discriminants
    by decreasing eigenvalues, similar to the explained variance plot that we created
    in the PCA section. For simplicity, we will call the content of class-discriminatory
    information **discriminability**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量线性判别特征向量（特征向量）捕捉到的类别区分信息的多少，让我们绘制按特征值降序排列的线性判别特征向量图，类似于我们在PCA部分创建的解释方差图。为了简化，我们将类别区分信息的内容称为**判别度**：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As we can see in the resulting figure, the first two linear discriminants alone
    capture 100 percent of the useful information in the Wine training dataset:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在结果图中看到的，前两个线性判别特征向量就能捕捉到Wine训练数据集中的100%有用信息：
- en: '![](img/B13208_05_07.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_07.png)'
- en: 'Let''s now stack the two most discriminative eigenvector columns to create
    the transformation matrix, *W*:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将两个最具判别性的特征向量列堆叠起来，形成变换矩阵*W*：
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Projecting examples onto the new feature space
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将样本投影到新的特征空间
- en: 'Using the transformation matrix, *W*, that we created in the previous subsection,
    we can now transform the training dataset by multiplying the matrices:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在前一小节中创建的变换矩阵 *W*，我们现在可以通过矩阵相乘来转换训练数据集：
- en: '![](img/B13208_05_063.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_063.png)'
- en: '[PRE24]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'As we can see in the resulting plot, the three Wine classes are now perfectly
    linearly separable in the new feature subspace:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果图中我们可以看到，三类Wine数据现在在新的特征子空间中变得完全线性可分：
- en: '![](img/B13208_05_08.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_08.png)'
- en: LDA via scikit-learn
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过scikit-learn实现LDA
- en: 'That step-by-step implementation was a good exercise to understand the inner
    workings of an LDA and understand the differences between LDA and PCA. Now, let''s
    look at the `LDA` class implemented in scikit-learn:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步步的实现是理解LDA内部工作原理的好练习，并且有助于理解LDA与PCA之间的区别。现在，让我们来看看在scikit-learn中实现的`LDA`类：
- en: '[PRE25]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, let''s see how the logistic regression classifier handles the lower-dimensional
    training dataset after the LDA transformation:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看逻辑回归分类器在经过LDA变换后的低维训练数据集上的表现：
- en: '[PRE26]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Looking at the resulting plot, we can see that the logistic regression model
    misclassifies one of the examples from class 2:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果图中我们可以看到，逻辑回归模型错误地分类了来自类别2的一个样本：
- en: '![](img/B13208_05_09.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_09.png)'
- en: 'By lowering the regularization strength, we could probably shift the decision
    boundaries so that the logistic regression model classifies all examples in the
    training dataset correctly. However, and more importantly, let''s take a look
    at the results on the test dataset:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 通过降低正则化强度，我们可能会将决策边界调整到一个位置，使得逻辑回归模型能够正确分类训练数据集中的所有样本。然而，更重要的是，让我们来看看测试数据集上的结果：
- en: '[PRE27]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As we can see in the following plot, the logistic regression classifier is
    able to get a perfect accuracy score for classifying the examples in the test
    dataset by only using a two-dimensional feature subspace, instead of the original
    13 Wine features:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，逻辑回归分类器仅使用二维特征子空间，而不是原始的13个Wine特征，便能够完美地分类测试数据集中的样本，获得了完美的准确率：
- en: '![](img/B13208_05_10.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_10.png)'
- en: Using kernel principal component analysis for nonlinear mappings
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核主成分分析进行非线性映射
- en: 'Many machine learning algorithms make assumptions about the linear separability
    of the input data. You have learned that the perceptron even requires perfectly
    linearly separable training data to converge. Other algorithms that we have covered
    so far assume that the lack of perfect linear separability is due to noise: Adaline,
    logistic regression, and the (standard) SVM to just name a few.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习算法都假设输入数据是线性可分的。你已经学习过感知机甚至需要完全线性可分的训练数据才能收敛。我们迄今为止介绍的其他算法假设缺乏完美的线性可分性是由噪声造成的：例如Adaline、逻辑回归和（标准）SVM等。
- en: However, if we are dealing with nonlinear problems, which we may encounter rather
    frequently in real-world applications, linear transformation techniques for dimensionality
    reduction, such as PCA and LDA, may not be the best choice.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们处理的是非线性问题——这类问题在实际应用中可能非常常见——那么像PCA和LDA这样的线性降维技术可能不是最佳选择。
- en: In this section, we will take a look at a kernelized version of PCA, or KPCA,
    which relates to the concepts of kernel SVM that you will remember from *Chapter
    3*, *A Tour of Machine Learning Classifiers Using scikit-learn*. Using KPCA, we
    will learn how to transform data that is not linearly separable onto a new, lower-dimensional
    subspace that is suitable for linear classifiers.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将介绍一个核化版本的主成分分析（PCA），或者称为KPCA，它与*第3章*中你将会记得的核支持向量机（SVM）概念相关。通过使用KPCA，我们将学习如何将非线性可分的数据转换到一个新的低维子空间，从而使得线性分类器适用。
- en: '![](img/B13208_05_11.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_11.png)'
- en: Kernel functions and the kernel trick
  id: totrans-191
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 核函数和核技巧
- en: 'As you will remember from our discussion about kernel SVMs in *Chapter 3*,
    *A Tour of Machine Learning Classifiers Using scikit-learn*, we can tackle nonlinear
    problems by projecting them onto a new feature space of higher dimensionality
    where the classes become linearly separable. To transform the examples ![](img/B13208_05_064.png)
    onto this higher *k*-dimensional subspace, we defined a nonlinear mapping function,
    ![](img/B13208_05_065.png):'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你会记得我们在*第3章*中讨论的关于核支持向量机（SVM）的内容，我们可以通过将非线性问题投影到一个新的高维特征空间来解决此类问题，在这个空间中，类别变得线性可分。为了将样本
    ![](img/B13208_05_064.png) 转换到这个更高的 *k* 维子空间，我们定义了一个非线性映射函数，![](img/B13208_05_065.png)：
- en: '![](img/B13208_05_066.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_066.png)'
- en: We can think of ![](img/B13208_05_067.png) as a function that creates nonlinear
    combinations of the original features to map the original *d*-dimensional dataset
    onto a larger, *k*-dimensional feature space.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将 ![](img/B13208_05_067.png) 看作一个函数，用来创建原始特征的非线性组合，将原始的 *d* 维数据集映射到更大的 *k*
    维特征空间。
- en: 'For example, if we had a feature vector ![](img/B13208_05_068.png) (*x* is
    a column vector consisting of *d* features) with two dimensions (*d* = 2), a potential
    mapping onto a 3D-space could be:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个特征向量 ![](img/B13208_05_068.png)（*x* 是一个由 *d* 个特征组成的列向量），其维度为 2（*d*
    = 2），那么映射到 3D 空间的可能形式为：
- en: '![](img/B13208_05_069.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_069.png)'
- en: In other words, we perform a nonlinear mapping via KPCA that transforms the
    data onto a higher-dimensional space. We then use standard PCA in this higher-dimensional
    space to project the data back onto a lower-dimensional space where the examples
    can be separated by a linear classifier (under the condition that the examples
    can be separated by density in the input space). However, one downside of this
    approach is that it is computationally very expensive, and this is where we use
    the **kernel trick**. Using the kernel trick, we can compute the similarity between
    two high-dimension feature vectors in the original feature space.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们通过KPCA执行非线性映射，将数据转换到更高维空间。然后，我们在这个高维空间中使用标准PCA，将数据投影回低维空间，在该空间中，样本可以通过线性分类器分离（前提是样本在输入空间中可以通过密度分离）。然而，这种方法的一个缺点是计算开销非常大，这就是我们使用**核技巧**的地方。通过使用核技巧，我们可以在原始特征空间中计算两个高维特征向量之间的相似度。
- en: 'Before we proceed with more details about the kernel trick to tackle this computationally
    expensive problem, let''s think back to the standard PCA approach that we implemented
    at the beginning of this chapter. We computed the covariance between two features,
    *k* and *j*, as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续深入讨论用于解决计算密集型问题的核技巧之前，先回顾一下本章开始时实现的标准PCA方法。我们计算了两个特征，*k* 和 *j*，之间的协方差，如下所示：
- en: '![](img/B13208_05_070.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_070.png)'
- en: 'Since the standardizing of features centers them at mean zero, for instance,
    ![](img/B13208_05_071.png) and ![](img/B13208_05_072.png), we can simplify this
    equation as follows:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征的标准化使它们在均值为零的地方居中，例如，![](img/B13208_05_071.png) 和 ![](img/B13208_05_072.png)，我们可以将此方程简化如下：
- en: '![](img/B13208_05_073.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_073.png)'
- en: 'Note that the preceding equation refers to the covariance between two features;
    now, let''s write the general equation to calculate the covariance matrix, ![](img/B13208_05_074.png):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，前述方程指的是两个特征之间的协方差；现在，我们来写出计算协方差矩阵的通用方程，![](img/B13208_05_074.png)：
- en: '![](img/B13208_05_075.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_075.png)'
- en: 'Bernhard Scholkopf generalized this approach (*Kernel principal component analysis*,
    *B. Scholkopf*, *A. Smola*, and *K.R. Muller*, pages 583-588, *1997*) so that
    we can replace the dot products between examples in the original feature space
    with the nonlinear feature combinations via ![](img/B13208_05_076.png):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Bernhard Scholkopf 将这一方法进行了推广（*核主成分分析*，*B. Scholkopf*，*A. Smola*，和 *K.R. Muller*，第583-588页，*1997*），使得我们可以通过
    ![](img/B13208_05_076.png) 用非线性特征组合替代原始特征空间中样本之间的点积：
- en: '![](img/B13208_05_077.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_077.png)'
- en: 'To obtain the eigenvectors—the principal components—from this covariance matrix,
    we have to solve the following equation:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从这个协方差矩阵中获得特征向量——主成分，我们必须解以下方程：
- en: '![](img/B13208_05_078.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_078.png)'
- en: Here, ![](img/B13208_05_079.png) and *v* are the eigenvalues and eigenvectors
    of the covariance matrix, ![](img/B13208_05_080.png), and *a* can be obtained
    by extracting the eigenvectors of the kernel (similarity) matrix, *K*, as you
    will see in the following paragraphs.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_05_079.png) 和 *v* 是协方差矩阵的特征值和特征向量，![](img/B13208_05_080.png)，*a*
    可以通过提取核（相似性）矩阵 *K* 的特征向量得到，正如你将在接下来的段落中看到的。
- en: '**Deriving the kernel matrix**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**推导核矩阵**'
- en: 'The derivation of the kernel matrix can be shown as follows. First, let''s
    write the covariance matrix as in matrix notation, where ![](img/B13208_05_081.png)
    is an ![](img/B13208_05_082.png)-dimensional matrix:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 核矩阵的推导可以如下所示。首先，我们将协方差矩阵写成矩阵表示形式，其中 ![](img/B13208_05_081.png) 是一个 ![](img/B13208_05_082.png)
    维的矩阵：
- en: '![](img/B13208_05_083.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_083.png)'
- en: 'Now, we can write the eigenvector equation as follows:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将特征向量方程写成如下形式：
- en: '![](img/B13208_05_084.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_084.png)'
- en: 'Since ![](img/B13208_05_085.png), we get:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 ![](img/B13208_05_085.png)，我们得到：
- en: '![](img/B13208_05_086.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_086.png)'
- en: 'Multiplying it by ![](img/B13208_05_087.png) on both sides yields the following
    result:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 将其左右两边同时乘以![](img/B13208_05_087.png)得到以下结果：
- en: '![](img/B13208_05_088.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_088.png)'
- en: 'Here, *K* is the similarity (kernel) matrix:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*K*是相似度（核）矩阵：
- en: '![](img/B13208_05_089.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_089.png)'
- en: 'As you might recall from the *Solving nonlinear problems using a kernel SVM*
    section in *Chapter 3*, *A Tour of Machine Learning Classifiers Using scikit-learn*,
    we use the kernel trick to avoid calculating the pairwise dot products of the
    examples, *x*, under ![](img/B13208_05_090.png) explicitly by using a kernel function,
    ![](img/B13208_05_091.png), so that we don''t need to calculate the eigenvectors
    explicitly:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的那样，在*第3章*《使用scikit-learn进行机器学习分类器巡礼*》的*使用核SVM解决非线性问题*一节中，我们通过使用核函数![](img/B13208_05_091.png)来避免显式计算示例*x*之间的成对点积，*x*在![](img/B13208_05_090.png)下的核技巧，免去了显式计算特征向量：
- en: '![](img/B13208_05_092.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_092.png)'
- en: In other words, what we obtain after KPCA are the examples already projected
    onto the respective components, rather than constructing a transformation matrix
    as in the standard PCA approach. Basically, the kernel function (or simply kernel)
    can be understood as a function that calculates a dot product between two vectors—a
    measure of similarity.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，经过KPCA处理后，我们得到的是已经投影到相应成分上的示例，而不是像标准PCA方法那样构造一个变换矩阵。基本上，核函数（或简称核）可以理解为一个计算两个向量点积的函数——即相似度的度量。
- en: 'The most commonly used kernels are as follows:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的核函数如下：
- en: The polynomial kernel:![](img/B13208_05_093.png)
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式核函数：![](img/B13208_05_093.png)
- en: Here, ![](img/B13208_05_094.png) is the threshold and *p* is the power that
    has to be specified by the user.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_05_094.png)是阈值，*p*是需要用户指定的幂值。
- en: The hyperbolic tangent (sigmoid) kernel:![](img/B13208_05_095.png)
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 双曲正切（Sigmoid）核函数：![](img/B13208_05_095.png)
- en: The **radial basis function** (**RBF**) or Gaussian kernel, which we will use
    in the following examples in the next subsection:![](img/B13208_05_096.png)
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**径向基函数**（**RBF**）或高斯核，我们将在接下来的小节中使用该核函数的示例：![](img/B13208_05_096.png)'
- en: It is often written in the following form, introducing the variable ![](img/B13208_05_097.png).
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 它通常写成以下形式，引入变量![](img/B13208_05_097.png)。
- en: '![](img/B13208_05_098.png)'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B13208_05_098.png)'
- en: 'To summarize what we have learned so far, we can define the following three
    steps to implement an RBF KPCA:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结我们到目前为止的学习内容，我们可以定义以下三个步骤来实现RBF KPCA：
- en: We compute the kernel (similarity) matrix, *K*, where we need to calculate the
    following:![](img/B13208_05_099.png)
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们计算核（相似度）矩阵，*K*，需要计算以下内容：![](img/B13208_05_099.png)
- en: 'We do this for each pair of examples:'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们对每一对示例都这样做：
- en: '![](img/B13208_05_100.png)'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B13208_05_100.png)'
- en: For example, if our dataset contains 100 training examples, the symmetric kernel
    matrix of the pairwise similarities would be ![](img/B13208_05_101.png)-dimensional.
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，如果我们的数据集包含100个训练示例，那么成对相似度的对称核矩阵将是![](img/B13208_05_101.png)维的。
- en: We center the kernel matrix, *K*, using the following equation:![](img/B13208_05_102.png)
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用以下公式对核矩阵进行中心化，*K*：![](img/B13208_05_102.png)
- en: Here, ![](img/B13208_05_103.png) is an ![](img/B13208_05_104.png)-dimensional
    matrix (the same dimensions as the kernel matrix) where all values are equal to
    ![](img/B13208_05_105.png).
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_05_103.png)是一个![](img/B13208_05_104.png)维的矩阵（与核矩阵的维度相同），其中所有值都等于![](img/B13208_05_105.png)。
- en: We collect the top *k* eigenvectors of the centered kernel matrix based on their
    corresponding eigenvalues, which are ranked by decreasing magnitude. In contrast
    to standard PCA, the eigenvectors are not the principal component axes, but the
    examples already projected onto these axes.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们根据对应的特征值收集中心化核矩阵的前*k*个特征向量，这些特征值按降序排列。与标准PCA不同，特征向量不是主成分轴，而是已经投影到这些轴上的示例。
- en: At this point, you may be wondering why we need to center the kernel matrix
    in the second step. We previously assumed that we are working with standardized
    data, where all features have mean zero when we formulate the covariance matrix
    and replace the dot-products with the nonlinear feature combinations via ![](img/B13208_05_106.png).
    Thus, the centering of the kernel matrix in the second step becomes necessary,
    since we do not compute the new feature space explicitly so that we cannot guarantee
    that the new feature space is also centered at zero.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，您可能会想知道为什么在第二步中我们需要对核矩阵进行中心化。我们之前假设我们正在处理标准化数据，在我们构造协方差矩阵并通过![](img/B13208_05_106.png)用非线性特征组合替代点积时，所有特征的均值为零。因此，在第二步中对核矩阵进行中心化是必要的，因为我们并未显式计算新的特征空间，因此无法保证新的特征空间也以零为中心。
- en: In the next section, we will put those three steps into action by implementing
    a KPCA in Python.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将通过在Python中实现KPCA来将这三步付诸实践。
- en: Implementing a kernel principal component analysis in Python
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中实现核主成分分析
- en: 'In the previous subsection, we discussed the core concepts behind KPCA. Now,
    we are going to implement an RBF KPCA in Python following the three steps that
    summarized the KPCA approach. Using some SciPy and NumPy helper functions, we
    will see that implementing a KPCA is actually really simple:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一小节中，我们讨论了KPCA背后的核心概念。现在，我们将按照总结KPCA方法的三个步骤，在Python中实现一个RBF KPCA。通过使用一些SciPy和NumPy辅助函数，我们会看到实现一个KPCA实际上非常简单：
- en: '[PRE28]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: One downside of using an RBF KPCA for dimensionality reduction is that we have
    to specify the ![](img/B13208_05_107.png) parameter a priori. Finding an appropriate
    value for ![](img/B13208_05_108.png) requires experimentation and is best done
    using algorithms for parameter tuning, for example, performing a grid search,
    which we will discuss in more detail in *Chapter 6*, *Learning Best Practices
    for Model Evaluation and Hyperparameter Tuning*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RBF KPCA进行降维的一个缺点是我们必须事先指定![](img/B13208_05_107.png)参数。找到适当的![](img/B13208_05_108.png)值需要实验，最好使用参数调优算法来完成，例如执行网格搜索，我们将在*第6章*，*模型评估与超参数调优的最佳实践*中更详细地讨论这个问题。
- en: Example 1 – separating half-moon shapes
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例1 – 分离半月形状
- en: 'Now, let us apply our `rbf_kernel_pca` on some nonlinear example datasets.
    We will start by creating a two-dimensional dataset of 100 example points representing
    two half-moon shapes:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在一些非线性示例数据集上应用我们的`rbf_kernel_pca`。我们将首先创建一个包含100个示例点的二维数据集，表示两个半月形状：
- en: '[PRE29]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For the purposes of illustration, the half-moon of triangle symbols will represent
    one class, and the half-moon depicted by the circle symbols will represent the
    examples from another class:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 为了便于说明，三角符号表示一个类的半月形状，而圆形符号表示另一个类的示例：
- en: '![](img/B13208_05_12.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_12.png)'
- en: 'Clearly, these two half-moon shapes are not linearly separable, and our goal
    is to *unfold* the half-moons via KPCA so that the dataset can serve as a suitable
    input for a linear classifier. But first, let''s see how the dataset looks if
    we project it onto the principal components via standard PCA:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，这两个半月形状是不可线性分隔的，我们的目标是通过KPCA *展开*半月形状，使得数据集可以作为线性分类器的合适输入。但首先，让我们看看如果我们通过标准PCA将数据投影到主成分上，数据集会是什么样子：
- en: '[PRE30]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Clearly, we can see in the resulting figure that a linear classifier would
    be unable to perform well on the dataset transformed via standard PCA:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，在得到的图形中，我们可以看到线性分类器无法在通过标准PCA转换后的数据集上表现良好：
- en: '![](img/B13208_05_13.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_13.png)'
- en: Note that when we plotted the first principal component only (right subplot),
    we shifted the triangular examples slightly upward and the circular examples slightly
    downward to better visualize the class overlap. As the left subplot shows, the
    original half-moon shapes are only slightly sheared and flipped across the vertical
    center—this transformation would not help a linear classifier in discriminating
    between circles and triangles. Similarly, the circles and triangles corresponding
    to the two half-moon shapes are not linearly separable if we project the dataset
    onto a one-dimensional feature axis, as shown in the right subplot.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当我们仅绘制第一个主成分时（右侧子图），我们稍微将三角形示例向上移动，将圆形示例向下移动，以更好地可视化类别重叠。正如左侧子图所示，原始的半月形状仅略微发生了剪切，并且沿垂直中心翻转——这种转换对于线性分类器在区分圆形和三角形之间并无帮助。类似地，如果我们将数据集投影到一维特征轴上，正如右侧子图所示，代表两种半月形状的圆形和三角形也不能线性分开。
- en: '**PCA versus LDA**'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '**PCA 与 LDA**'
- en: Please remember that PCA is an unsupervised method and does not use class label
    information in order to maximize the variance in contrast to LDA. Here, the triangle
    and circle symbols were just added for visualization purposes to indicate the
    degree of separation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，PCA 是一种无监督方法，不使用类别标签信息来最大化方差，而与 LDA 相反。这里，三角形和圆形符号仅用于可视化目的，以表示分离程度。
- en: 'Now, let''s try out our kernel PCA function, `rbf_kernel_pca`, which we implemented
    in the previous subsection:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们尝试一下我们在上一小节中实现的内核 PCA 函数`rbf_kernel_pca`：
- en: '[PRE31]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can now see that the two classes (circles and triangles) are linearly well
    separated so that we have a suitable training dataset for linear classifiers:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以看到，这两个类别（圆形和三角形）已经线性分开，因此我们得到了一个适合线性分类器的训练数据集：
- en: '![](img/B13208_05_14.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_14.png)'
- en: Unfortunately, there is no universal value for the tuning parameter, ![](img/B13208_05_109.png),
    that works well for different datasets. Finding a ![](img/B13208_05_110.png) value
    that is appropriate for a given problem requires experimentation. In *Chapter
    6*, *Learning Best Practices for Model Evaluation and Hyperparameter Tuning*,
    we will discuss techniques that can help us to automate the task of optimizing
    such tuning parameters. Here, we will use values for ![](img/B13208_05_111.png)
    that have been found to produce good results.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于不同的数据集，调节参数！[](img/B13208_05_109.png)并没有一个通用的最佳值。找到一个适合特定问题的！[](img/B13208_05_110.png)值需要进行实验。在*第六章*，*模型评估与超参数调优的最佳实践*中，我们将讨论一些有助于自动化优化这些调节参数的技术。这里，我们将使用已知能产生良好结果的！[](img/B13208_05_111.png)值。
- en: Example 2 – separating concentric circles
  id: totrans-261
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例 2 – 分离同心圆
- en: 'In the previous subsection, we saw how to separate half-moon shapes via KPCA.
    Since we put so much effort into understanding the concepts of KPCA, let''s take
    a look at another interesting example of a nonlinear problem, concentric circles:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一小节中，我们已经看到如何通过 KPCA 分离半月形状。既然我们在理解 KPCA 的概念上付出了很多努力，让我们来看看另一个有趣的非线性问题示例，同心圆：
- en: '[PRE32]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Again, we assume a two-class problem where the triangle shapes represent one
    class, and the circle shapes represent another class:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们假设这是一个二分类问题，其中三角形代表一个类别，圆形代表另一个类别：
- en: '![](img/B13208_05_15.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_15.png)'
- en: 'Let''s start with the standard PCA approach to compare it to the results of
    the RBF kernel PCA:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从标准 PCA 方法开始，将其与 RBF 内核 PCA 的结果进行比较：
- en: '[PRE33]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Again, we can see that standard PCA is not able to produce results suitable
    for training a linear classifier:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以看到标准 PCA 无法产生适合训练线性分类器的结果：
- en: '![](img/B13208_05_16.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_16.png)'
- en: 'Given an appropriate value for ![](img/B13208_05_112.png), let''s see if we
    are luckier using the RBF KPCA implementation:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个合适的！[](img/B13208_05_112.png)值，让我们看看使用 RBF KPCA 实现是否更有运气：
- en: '[PRE34]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Again, the RBF KPCA projected the data onto a new subspace where the two classes
    become linearly separable:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，RBF KPCA 将数据投影到一个新的子空间中，在该空间中，两个类别变得线性可分：
- en: '![](img/B13208_05_17.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_17.png)'
- en: Projecting new data points
  id: totrans-274
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 投影新的数据点
- en: In the two previous example applications of KPCA, the half-moon shapes and the
    concentric circles, we projected a single dataset onto a new feature. In real
    applications, however, we may have more than one dataset that we want to transform,
    for example, training and test data, and typically also new examples we will collect
    after the model building and evaluation. In this section, you will learn how to
    project data points that were not part of the training dataset.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前两个应用KPCA的示例中，半月形和同心圆形，我们将单一数据集投影到一个新的特征上。然而，在实际应用中，我们可能会有多个数据集需要转换，例如训练数据和测试数据，通常还包括我们在模型构建和评估后收集的新示例。在本节中，你将学习如何投影那些不属于训练数据集的数据点。
- en: As you will remember from the standard PCA approach at the beginning of this
    chapter, we project data by calculating the dot product between a transformation
    matrix and the input examples; the columns of the projection matrix are the top
    *k* eigenvectors (*v*) that we obtained from the covariance matrix.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你从本章开始的标准PCA方法中所记得的那样，我们通过计算变换矩阵与输入示例之间的点积来进行数据投影；投影矩阵的列是我们从协方差矩阵中获得的前*k*个特征向量（*v*）。
- en: 'Now, the question is how we can transfer this concept to KPCA. If we think
    back to the idea behind KPCA, we will remember that we obtained an eigenvector
    (*a*) of the centered kernel matrix (not the covariance matrix), which means that
    those are the examples that are already projected onto the principal component
    axis, *v*. Thus, if we want to project a new example, ![](img/B13208_05_113.png),
    onto this principal component axis, we will need to compute the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，问题是我们如何将这一概念转移到KPCA中。如果回想一下KPCA背后的理念，我们会记得我们得到的是中心化核矩阵的特征向量（*a*），而不是协方差矩阵的特征向量，这意味着这些已经被投影到主成分轴上的示例是*
    v *。因此，如果我们想要将新的示例，![](img/B13208_05_113.png)，投影到这个主成分轴上，我们需要计算以下内容：
- en: '![](img/B13208_05_114.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_114.png)'
- en: Fortunately, we can use the kernel trick so that we don't have to calculate
    the projection, ![](img/B13208_05_115.png), explicitly. However, it is worth noting
    that KPCA, in contrast to standard PCA, is a memory-based method, which means
    that we have to *reuse the original training dataset each time to project new
    examples*.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以使用核技巧，这样就不需要显式地计算投影，![](img/B13208_05_115.png)。然而，值得注意的是，KPCA与标准PCA不同，它是一种基于记忆的方法，这意味着我们必须*每次都重用原始训练数据集来投影新的示例*。
- en: 'We have to calculate the pairwise RBF kernel (similarity) between each *i*th
    example in the training dataset and the new example, ![](img/B13208_05_116.png):'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须计算训练数据集中每个第*i*个示例与新示例之间的成对RBF核（相似性），![](img/B13208_05_116.png)：
- en: '![](img/B13208_05_117.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_117.png)'
- en: 'Here, the eigenvectors, *a*, and eigenvalues, ![](img/B13208_05_118.png), of
    the kernel matrix, *K*, satisfy the following condition in the equation:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，核矩阵的特征向量，*a*，和特征值，![](img/B13208_05_118.png)，满足方程中的以下条件：
- en: '![](img/B13208_05_119.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_119.png)'
- en: 'After calculating the similarity between the new examples and the examples
    in the training dataset, we have to normalize the eigenvector, *a*, by its eigenvalue.
    Thus, let''s modify the `rbf_kernel_pca` function that we implemented earlier
    so that it also returns the eigenvalues of the kernel matrix:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算新示例与训练数据集中示例之间的相似性之后，我们需要通过其特征值来归一化特征向量，*a*。因此，让我们修改之前实现的`rbf_kernel_pca`函数，使其还返回核矩阵的特征值：
- en: '[PRE35]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, let''s create a new half-moon dataset and project it onto a one-dimensional
    subspace using the updated RBF KPCA implementation:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个新的半月形数据集，并使用更新后的RBF KPCA实现将其投影到一维子空间中：
- en: '[PRE36]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To make sure that we implemented the code for projecting new examples, let''s
    assume that the 26th point from the half-moon dataset is a new data point, ![](img/B13208_05_120.png),
    and our task is to project it onto this new subspace:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们已经实现了投影新示例的代码，假设半月形数据集中的第26个点是一个新的数据点，![](img/B13208_05_120.png)，我们的任务是将其投影到这个新的子空间中：
- en: '[PRE37]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'By executing the following code, we are able to reproduce the original projection.
    Using the `project_x` function, we will be able to project any new data example
    as well. The code is as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行以下代码，我们可以重现原始的投影。使用`project_x`函数，我们也能够投影任何新的数据示例。代码如下：
- en: '[PRE38]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Lastly, let''s visualize the projection on the first principal component:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们可视化在第一个主成分上的投影：
- en: '[PRE39]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'As we can now also see in the following scatterplot, we mapped the example,
    ![](img/B13208_05_121.png), onto the first principal component correctly:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在下面的散点图中看到的那样，我们将示例数据， ![](img/B13208_05_121.png)，正确地映射到了第一个主成分上：
- en: '![](img/B13208_05_18.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_18.png)'
- en: Kernel principal component analysis in scikit-learn
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: scikit-learn中的核主成分分析
- en: 'For our convenience, scikit-learn implements a KPCA class in the `sklearn.decomposition`
    submodule. The usage is similar to the standard PCA class, and we can specify
    the kernel via the `kernel` parameter:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便，我们可以使用scikit-learn在`sklearn.decomposition`子模块中实现的KPCA类。它的使用方法类似于标准的PCA类，我们可以通过`kernel`参数指定核函数：
- en: '[PRE40]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'To check that we get results that are consistent with our own KPCA implementation,
    let''s plot the transformed half-moon shape data onto the first two principal
    components:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的结果是否与自己的KPCA实现一致，让我们将变换后的半月形数据绘制到前两个主成分上：
- en: '[PRE41]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'As we can see, the results of scikit-learn''s `KernelPCA` are consistent with
    our own implementation:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，scikit-learn的`KernelPCA`结果与我们自己的实现一致：
- en: '![](img/B13208_05_19.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_05_19.png)'
- en: '**Manifold learning**'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '**流形学习**'
- en: The scikit-learn library also implements advanced techniques for nonlinear dimensionality
    reduction that are beyond the scope of this book. The interested reader can find
    a nice overview of the current implementations in scikit-learn, complemented by
    illustrative examples, at [http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn库还实现了超出本书范围的非线性降维技术。感兴趣的读者可以在[http://scikit-learn.org/stable/modules/manifold.html](http://scikit-learn.org/stable/modules/manifold.html)上找到一个有关当前scikit-learn实现的良好概述，并附有示例。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned about three different, fundamental dimensionality
    reduction techniques for feature extraction: standard PCA, LDA, and KPCA. Using
    PCA, we projected data onto a lower-dimensional subspace to maximize the variance
    along the orthogonal feature axes, while ignoring the class labels. LDA, in contrast
    to PCA, is a technique for supervised dimensionality reduction, which means that
    it considers class information in the training dataset to attempt to maximize
    the class-separability in a linear feature space.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，你学习了三种不同的、基础的特征提取降维技术：标准PCA、LDA和KPCA。通过PCA，我们将数据投影到低维子空间中，以最大化沿正交特征轴的方差，同时忽略类标签。与PCA不同，LDA是一种监督学习的降维技术，它考虑了训练数据集中的类别信息，旨在尽可能最大化类在特征空间中的可分性。
- en: Lastly, you learned about a nonlinear feature extractor, KPCA. Using the kernel
    trick and a temporary projection into a higher-dimensional feature space, you
    were ultimately able to compress datasets consisting of nonlinear features onto
    a lower-dimensional subspace where the classes became linearly separable.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你了解了一个非线性特征提取器——KPCA。通过核技巧和临时投影到更高维的特征空间，你最终能够将包含非线性特征的数据集压缩到一个低维子空间，在这个子空间中，类变得线性可分。
- en: Equipped with these essential preprocessing techniques, you are now well prepared
    to learn about the best practices for efficiently incorporating different preprocessing
    techniques and evaluating the performance of different models in the next chapter.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借这些基本的预处理技术，你现在已经为接下来章节中学习如何高效地整合不同的预处理技术，并评估不同模型的性能做好了充分准备。
