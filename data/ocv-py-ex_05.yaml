- en: Chapter 5. Extracting Features from an Image
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第五章：从图像中提取特征
- en: In this chapter, we are going to learn how to detect salient points, also known
    as keypoints, in an image. We will discuss why these keypoints are important and
    how we can use them to understand the image content. We will talk about different
    techniques that can be used to detect these keypoints, and understand how we can
    extract features from a given image.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在图像中检测显著点，也称为关键点。我们将讨论这些关键点为什么很重要，以及我们如何利用它们来理解图像内容。我们将讨论可以用来检测这些关键点的不同技术，以及我们如何从给定的图像中提取特征。
- en: 'By the end of this chapter, you will know:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将知道：
- en: What are keypoints and why do we care about them
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是关键点，为什么我们关心它们
- en: How to detect keypoints
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何检测关键点
- en: How to use keypoints for image content analysis
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用关键点进行图像内容分析
- en: The different techniques to detect keypoints
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测关键点的不同技术
- en: How to build a feature extractor
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何构建特征提取器
- en: Why do we care about keypoints?
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么我们关心关键点？
- en: Image content analysis refers to the process of understanding the content of
    an image so that we can take some action based on that. Let's take a step back
    and talk about how humans do it. Our brain is an extremely powerful machine that
    can do complicated things very quickly. When we look at something, our brain automatically
    creates a footprint based on the "interesting" aspects of that image. We will
    discuss what interesting means as we move along this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图像内容分析是指理解图像内容的过程，以便我们可以根据该内容采取某些行动。让我们回顾一下人类是如何做到这一点的。我们的大脑是一个极其强大的机器，可以非常快速地完成复杂的事情。当我们看某样东西时，大脑会自动根据该图像的“有趣”方面创建一个足迹。随着本章的进行，我们将讨论“有趣”的含义。
- en: 'For now, an interesting aspect is something that''s distinct in that region.
    If we call a point interesting, then there shouldn''t be another point in its
    neighborhood that satisfies the constraints. Let''s consider the following image:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，一个有趣的特点是那个区域中独特的东西。如果我们称一个点是有趣的，那么在其邻域内不应该有另一个满足约束条件的点。让我们考虑以下图像：
- en: '![Why do we care about keypoints?](img/B04554_05_01.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![为什么我们关心关键点？](img/B04554_05_01.jpg)'
- en: 'Now close your eyes and try to visualize this image. Do you see something specific?
    Can you recollect what''s in the left half of the image? Not really! The reason
    for this is that the image doesn''t have any interesting information. When our
    brain looks at something like this, there''s nothing to make note of. So it tends
    to wander around! Let''s take a look at the following image:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在闭上眼睛，尝试想象这幅图像。你看到了什么具体的东西吗？你能回忆起图像的左半部分吗？实际上不行！这是因为图像没有任何有趣的信息。当我们的大脑看到这样的东西时，没有什么值得注意的。所以它往往会四处游荡！让我们看看以下图像：
- en: '![Why do we care about keypoints?](img/B04554_05_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![为什么我们关心关键点？](img/B04554_05_02.jpg)'
- en: 'Now close your eyes and try to visualize this image. You will see that the
    recollection is vivid and you remember a lot of details about this image. The
    reason for this is that there are a lot of interesting regions in the image. The
    human eye is more sensitive to high frequency content as compared to low frequency
    content. This is the reason we tend to recollect the second image better than
    the first one. To further demonstrate this, let''s look at the following image:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在闭上眼睛，尝试想象这幅图像。你会发现回忆是生动的，你记得关于这幅图像的很多细节。这是因为图像中有许多有趣区域。与低频内容相比，人眼对高频内容更为敏感。这就是我们倾向于比第一幅图像更好地回忆第二幅图像的原因。为了进一步演示这一点，让我们看看以下图像：
- en: '![Why do we care about keypoints?](img/B04554_05_03.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![为什么我们关心关键点？](img/B04554_05_03.jpg)'
- en: If you notice, your eye immediately went to the TV remote, even though it's
    not at the center of the image. We automatically tend to gravitate towards the
    interesting regions in the image because that is where all the information is.
    This is what our brain needs to store in order to recollect it later.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，你的眼睛立即就转向了电视遥控器，即使它不在图像的中心。我们自动倾向于趋向图像中的有趣区域，因为那里有所有信息。这就是我们的大脑需要存储以便以后回忆起来的内容。
- en: When we build object recognition systems, we need to detect these "interesting"
    regions to create a signature for the image. These interesting regions are characterized
    by keypoints. This is why keypoint detection is critical in many modern computer
    vision systems.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建物体识别系统时，我们需要检测这些“有趣”的区域来为图像创建一个签名。这些有趣区域的特点是关键点。这就是为什么关键点检测在许多现代计算机视觉系统中至关重要。
- en: What are keypoints?
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是关键点？
- en: 'Now that we know that keypoints refer to the interesting regions in the image,
    let''s dig a little deeper. What are keypoints made of? Where are these points?
    When we say "interesting", it means that something is happening in that region.
    If the region is just uniform, then it''s not very interesting. For example, corners
    are interesting because there is sharp change in intensity in two different directions.
    Each corner is a unique point where two edges meet. If you look at the preceding
    images, you will see that the interesting regions are not completely made up of
    "interesting" content. If you look closely, we can still see plain regions within
    busy regions. For example, consider the following image:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道关键点指的是图像中的有趣区域，那么让我们深入探讨一下。关键点由什么组成？这些点在哪里？当我们说“有趣”时，意味着该区域正在发生某些事情。如果该区域只是均匀的，那么它并不很有趣。例如，角落是有趣的，因为两个不同方向上的强度发生了急剧变化。每个角落都是两条边相交的独特点。如果你看前面的图像，你会看到有趣区域并不完全由“有趣”的内容组成。如果你仔细观察，我们仍然可以在繁忙的区域中看到普通区域。例如，考虑以下图像：
- en: '![What are keypoints?](img/B04554_05_04.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![什么是关键点？](img/B04554_05_04.jpg)'
- en: If you look at the preceding object, the interior parts of the interesting regions
    are "uninteresting".
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看看前面的物体，有趣区域的内部部分是“无趣”的。
- en: '![What are keypoints?](img/B04554_05_05.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![什么是关键点？](img/B04554_05_05.jpg)'
- en: 'So, if we were to characterize this object, we would need to make sure that
    we picked the interesting points. Now, how do we define "interesting points"?
    Can we just say that anything that''s not uninteresting can be an interesting
    point? Let''s consider the following example:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们想要描述这个物体，我们需要确保我们选择了有趣点。现在，我们如何定义“有趣点”？我们能否说任何不是无趣的东西都可以是有趣点？让我们考虑以下例子：
- en: '![What are keypoints?](img/B04554_05_06.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![什么是关键点？](img/B04554_05_06.jpg)'
- en: Now, we can see that there is a lot of high frequency content in this image
    along the edge. But we cannot call the whole edge "interesting". It is important
    to understand that "interesting" doesn't necessarily refer to color or intensity
    values. It can be anything, as long as it is distinct. We need to isolate the
    points that are unique in their neighborhood. The points along the edge are not
    unique with respect to their neighbors. So, now that we know what we are looking
    for, how do we pick an interesting point?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到在这张图像的边缘有很多高频内容。但我们不能把整个边缘称为“有趣”。重要的是要理解，“有趣”不一定是指颜色或强度值。它可以是指任何东西，只要它是独特的。我们需要隔离其邻域中独特的点。沿着边缘的点相对于其邻居来说并不独特。所以，现在我们知道我们要找什么了，我们如何选择一个有趣点？
- en: What about the corner of the table? That's pretty interesting, right? It's unique
    with respect to its neighbors and we don't have anything like that in its vicinity.
    Now this point can be chosen as one of our keypoints. We take a bunch of these
    keypoints to characterize a particular image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 那么桌角的角落呢？那很吸引人，对吧？它相对于其邻居是独特的，并且在其附近我们没有类似的东西。现在这个点可以被选为我们中的一个关键点。我们选取这些关键点来描述特定的图像。
- en: When we do image analysis, we need to convert it into a numerical form before
    we deduce something. These keypoints are represented using a numerical form and
    a combination of these keypoints is then used to create the image signature. We
    want this image signature to represent a given image in the best possible way.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们进行图像分析时，在推导出任何东西之前，我们需要将其转换为数值形式。这些关键点使用数值形式和这些关键点的组合来创建图像签名。我们希望这个图像签名以最佳方式代表给定的图像。
- en: Detecting the corners
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测角落
- en: Since we know that the corners are "interesting", let's see how we can detect
    them. In computer vision, there is a popular corner detection technique called
    **Harris Corner Detector**. We basically construct a 2x2 matrix based on partial
    derivatives of the grayscale image, and then analyze the eigenvalues. This is
    actually an oversimplification of the actual algorithm, but it covers the gist.
    So, if you want to understand the underlying mathematical details, you can look
    into the original paper by Harris and Stephens at [http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf).
    A corner point is a point where both the eigenvalues would have large values.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道角点是“有趣的”，让我们看看我们如何检测它们。在计算机视觉中，有一个流行的角点检测技术叫做**Harris角点检测器**。我们基本上基于灰度图像的偏导数构建一个2x2矩阵，然后分析特征值。这实际上是对实际算法的过度简化，但它涵盖了要点。所以，如果你想了解背后的数学细节，你可以查阅Harris和Stephens在[http://www.bmva.org/bmvc/1988/avc-88-023.pdf](http://www.bmva.org/bmvc/1988/avc-88-023.pdf)上发表的原始论文。一个角点是两个特征值都应有较大值的点。
- en: 'Let''s consider the following image:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下图像：
- en: '![Detecting the corners](img/B04554_05_07.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![检测角点](img/B04554_05_07.jpg)'
- en: 'If you run the Harris corner detector on this image, you will see something
    like this:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在这个图像上运行Harris角点检测器，你会看到如下情况：
- en: '![Detecting the corners](img/B04554_05_08.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![检测角点](img/B04554_05_08.jpg)'
- en: 'As you can see, all the black dots correspond to the corners in the image.
    If you notice, the corners at the bottom of the box are not detected. The reason
    for this is that the corners are not sharp enough. You can adjust the thresholds
    in the corner detector to identify these corners. The code to do this is as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，所有的黑色点都对应于图像中的角点。如果你注意到，盒底部的角点没有被检测到。原因是这些角点不够尖锐。你可以在角点检测器中调整阈值来识别这些角点。执行此操作的代码如下：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Good Features To Track
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 好的特征追踪
- en: Harris corner detector performs well in many cases, but it misses out on a few
    things. Around six years after the original paper by Harris and Stephens, Shi-Tomasi
    came up with a better corner detector. You can read the original paper at [http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf).
    They used a different scoring function to improve the overall quality. Using this
    method, we can find the 'N' strongest corners in the given image. This is very
    useful when we don't want to use every single corner to extract information from
    the image.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Harris角点检测器在许多情况下表现良好，但它遗漏了一些东西。在Harris和Stephens的原始论文发表后的六年左右，Shi-Tomasi提出了一种更好的角点检测器。你可以在[http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf](http://www.ai.mit.edu/courses/6.891/handouts/shi94good.pdf)上阅读原始论文。他们使用不同的评分函数来提高整体质量。使用这种方法，我们可以在给定的图像中找到“N”个最强的角点。当我们不想使用图像中的每一个角点来提取信息时，这非常有用。
- en: 'If you apply the Shi-Tomasi corner detector to the image shown earlier, you
    will see something like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将Shi-Tomasi角点检测器应用于前面显示的图像，你会看到如下情况：
- en: '![Good Features To Track](img/B04554_05_09.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![好特征追踪](img/B04554_05_09.jpg)'
- en: 'Following is the code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Scale Invariant Feature Transform (SIFT)
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 尺度不变特征变换（SIFT）
- en: Even though corner features are "interesting", they are not good enough to characterize
    the truly interesting parts. When we talk about image content analysis, we want
    the image signature to be invariant to things such as scale, rotation, illumination,
    and so on. Humans are very good at these things. Even if I show you an image of
    an apple upside down that's dimmed, you will still recognize it. If I show you
    a really enlarged version of that image, you will still recognize it. We want
    our image recognition systems to be able to do the same.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管角点特征是“有趣的”，但它们不足以表征真正有趣的部分。当我们谈论图像内容分析时，我们希望图像签名对诸如尺度、旋转、光照等因素保持不变。人类在这些方面非常擅长。即使我给你看一个颠倒且昏暗的苹果图像，你仍然能认出它。如果我给你看这个图像的放大版本，你仍然能认出它。我们希望我们的图像识别系统能够做到同样的事情。
- en: Let's consider the corner features. If you enlarge an image, a corner might
    stop being a corner as shown below.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑角点特征。如果你放大一个图像，一个角点可能就不再是角点了，如下所示。
- en: '![Scale Invariant Feature Transform (SIFT)](img/B04554_05_10.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![尺度不变特征变换（SIFT）](img/B04554_05_10.jpg)'
- en: In the second case, the detector will not pick up this corner. And, since it
    was picked up in the original image, the second image will not be matched with
    the first one. It's basically the same image, but the corner features based method
    will totally miss it. This means that corner detector is not exactly scale invariant.
    This is why we need a better method to characterize an image.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，检测器将不会检测到这个角落。由于它在原始图像中被检测到，因此第二个图像将不会与第一个图像匹配。这基本上是同一张图像，但基于角落特征的算法将完全错过它。这意味着角落检测器并不完全具有尺度不变性。这就是为什么我们需要一个更好的方法来表征图像。
- en: SIFT is one of the most popular algorithms in all of computer vision. You can
    read David Lowe's original paper at [http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf).
    We can use this algorithm to extract keypoints and build the corresponding feature
    descriptors. There is a lot of good documentation available online, so we will
    keep our discussion brief. To identify a potential keypoint, SIFT builds a pyramid
    by downsampling an image and taking the difference of Gaussian. This means that
    we run a Gaussian filter at each level and take the difference to build the successive
    levels in the pyramid. In order to see if the current point is a keypoint, it
    looks at the neighbors as well as the pixels at the same location in neighboring
    levels of the pyramid. If it's a maxima, then the current point is picked up as
    a keypoint. This ensures that we keep the keypoints scale invariant.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: SIFT是计算机视觉中最受欢迎的算法之一。您可以在[http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf](http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf)阅读David
    Lowe的原始论文。我们可以使用这个算法来提取关键点和构建相应的特征描述符。网上有大量的良好文档，因此我们将简要讨论。为了识别一个潜在的关键点，SIFT通过下采样图像并计算高斯差分来构建一个金字塔。这意味着我们在金字塔的每个级别上运行高斯滤波器，并取差分来构建金字塔的连续级别。为了确定当前点是否为关键点，它不仅查看邻居，还查看金字塔相邻级别中相同位置的像素。如果是最大值，则当前点被选中作为关键点。这确保了我们的关键点具有尺度不变性。
- en: Now that we know how it achieves scale invariance, let's see how it achieves
    rotation invariance. Once we identify the keypoints, each keypoint is assigned
    an orientation. We take the neighborhood around each keypoint and compute the
    gradient magnitude and direction. This gives us a sense of the direction of that
    keypoint. If we have this information, we will be able to match this keypoint
    to the same point in another image even if it's rotated. Since we know the orientation,
    we will be able to normalize those keypoints before making the comparisons.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了它是如何实现尺度不变性的，让我们看看它是如何实现旋转不变性的。一旦我们确定了关键点，每个关键点都会被分配一个方向。我们取每个关键点周围的邻域，并计算梯度幅度和方向。这给我们一个关于该关键点方向的感觉。如果我们有这些信息，我们甚至可以在旋转的情况下将这个关键点与另一张图像中的相同点匹配。由于我们知道方向，我们将在比较之前对这些关键点进行归一化。
- en: Once we have all this information, how do we quantify it? We need to convert
    it to a set of numbers so that we can do some kind of matching on it. To achieve
    this, we just take the 16x16 neighborhood around each keypoint, and divide it
    into 16 blocks of size 4x4\. For each block, we compute the orientation histogram
    with 8 bins. So, we have a vector of length 8 associated with each block, which
    means that the neighborhood is represented by a vector of size 128 (8x16). This
    is the final keypoint descriptor that will be used. If we extract `N` keypoints
    from an image, then we will have `N` descriptors of length 128 each. This array
    of `N` descriptors characterizes the given image.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了所有这些信息，我们如何量化它？我们需要将其转换为一系列数字，以便我们可以对其进行某种匹配。为了实现这一点，我们只需取每个关键点周围的16x16邻域，并将其分成16个4x4大小的块。对于每个块，我们使用8个桶计算方向直方图。因此，我们与每个块相关联的向量长度为8，这意味着邻域由一个大小为128（8x16）的向量表示。这是最终将使用的关键点描述符。如果我们从一个图像中提取`N`个关键点，那么我们将有`N`个长度为128的描述符。这个`N`个描述符的数组表征了给定的图像。
- en: 'Consider the following image:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图像：
- en: '![Scale Invariant Feature Transform (SIFT)](img/B04554_05_11.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![尺度不变特征变换（SIFT）](img/B04554_05_11.jpg)'
- en: 'If you extract the keypoint locations using SIFT, you will see something like
    the following, where the size of the circle indicates the strength of the keypoints,
    and the line inside the circle indicates the orientation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用SIFT提取关键点位置，您将看到如下所示的内容，其中圆圈的大小表示关键点的强度，圆圈内的线条表示方向：
- en: '![Scale Invariant Feature Transform (SIFT)](img/B04554_05_12.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![尺度不变特征变换（SIFT）](img/B04554_05_12.jpg)'
- en: 'Before we look at the code, it is important to know that SIFT is patented and
    it''s not freely available for commercial use. Following is the code to do it:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看代码之前，重要的是要知道SIFT是受专利保护的，并且它不能免费用于商业用途。以下是如何实现的代码：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also compute the descriptors. OpenCV lets us do it separately or we
    can combine the detection and computation parts in the same step by using the
    following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以计算描述符。OpenCV让我们可以单独计算，或者我们可以通过使用以下方法将检测和计算部分合并到同一步骤中：
- en: '[PRE3]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Speeded Up Robust Features (SURF)
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速鲁棒特征（SURF）
- en: Even though SIFT is nice and useful, it's computationally intensive. This means
    that it's slow and we will have a hard time implementing a real-time system if
    it uses SIFT. We need a system that's fast and has all the advantages of SIFT.
    If you remember, SIFT uses the difference of Gaussian to build the pyramid and
    this process is slow. So, to overcome this, SURF uses a simple box filter to approximate
    the Gaussian. The good thing is that this is really easy to compute and it's reasonably
    fast. There's a lot of documentation available online on SURF at [http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf](http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf).
    So, you can go through it to see how they construct a descriptor. You can refer
    to the original paper at [http://www.vision.ee.ethz.ch/~surf/eccv06.pdf](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf).
    It is important to know that SURF is also patented and it is not freely available
    for commercial use.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SIFT很好用且很有用，但它计算量很大。这意味着它很慢，如果我们使用SIFT来实现实时系统，我们将遇到困难。我们需要一个既快又具有SIFT所有优点的系统。如果你还记得，SIFT使用高斯差分来构建金字塔，这个过程很慢。因此，为了克服这一点，SURF使用简单的盒式滤波器来近似高斯。好事是这很容易计算，而且速度相当快。关于SURF，网上有很多文档，可以在[http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf](http://opencv-python-tutroals.readthedocs.org/en/latest/py_tutorials/py_feature2d/py_surf_intro/py_surf_intro.html?highlight=surf)找到。因此，你可以阅读它，看看他们是如何构建描述符的。你可以参考原始论文[http://www.vision.ee.ethz.ch/~surf/eccv06.pdf](http://www.vision.ee.ethz.ch/~surf/eccv06.pdf)。重要的是要知道SURF也是受专利保护的，并且它不能免费用于商业用途。
- en: 'If you run the SURF keypoint detector on the earlier image, you will see something
    like the following one:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行SURF关键点检测器在之前的图像上，你会看到如下所示的一个：
- en: '![Speeded Up Robust Features (SURF)](img/B04554_05_13.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![加速鲁棒特征（SURF）](img/B04554_05_13.jpg)'
- en: 'Here is the code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Features from Accelerated Segment Test (FAST)
  id: totrans-64
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加速分割测试（FAST）特征
- en: Even though SURF is faster than SIFT, it's just not fast enough for a real-time
    system, especially when there are resource constraints. When you are building
    a real-time application on a mobile device, you won't have the luxury of using
    SURF to do computations in real time. We need something that's really fast and
    computationally inexpensive. Hence, Rosten and Drummond came up with FAST. As
    the name indicates, it's really fast!
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管SURF比SIFT快，但它对于实时系统来说还不够快，尤其是在资源受限的情况下。当你在一个移动设备上构建实时应用程序时，你不会有使用SURF进行实时计算的自由。我们需要的是真正快且计算成本低的系统。因此，Rosten和Drummond提出了FAST。正如其名所示，它真的很快！
- en: 'Instead of going through all the expensive calculations, they came up with
    a high-speed test to quickly determine if the current point is a potential keypoint.
    We need to note that FAST is just for keypoint detection. Once keypoints are detected,
    we need to use SIFT or SURF to compute the descriptors. Consider the following
    image:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 他们没有通过所有昂贵的计算，而是提出了一种高速测试方法，以快速确定当前点是否是一个潜在的关键点。我们需要注意的是，FAST仅用于关键点检测。一旦检测到关键点，我们需要使用SIFT或SURF来计算描述符。考虑以下图像：
- en: '![Features from Accelerated Segment Test (FAST)](img/B04554_05_14.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![加速分割测试（FAST）特征](img/B04554_05_14.jpg)'
- en: 'If we run the FAST keypoint detector on this image, you will see something
    like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在该图像上运行FAST关键点检测器，你会看到如下内容：
- en: '![Features from Accelerated Segment Test (FAST)](img/B04554_05_15.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![加速分割测试（FAST）特征](img/B04554_05_15.jpg)'
- en: 'If we clean it up and suppress the unimportant keypoints, it will look like
    this:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们清理并抑制不重要的关键点，它看起来会是这样：
- en: '![Features from Accelerated Segment Test (FAST)](img/B04554_05_16.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![加速分割测试（FAST）特征](img/B04554_05_16.jpg)'
- en: 'Following is the code for this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是这个的代码：
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Binary Robust Independent Elementary Features (BRIEF)
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 二进制鲁棒独立基本特征（BRIEF）
- en: Even though we have FAST to quickly detect the keypoints, we still have to use
    SIFT or SURF to compute the descriptors. We need a way to quickly compute the
    descriptors as well. This is where BRIEF comes into the picture. BRIEF is a method
    for extracting feature descriptors. It cannot detect the keypoints by itself,
    so we need to use it in conjunction with a keypoint detector. The good thing about
    BRIEF is that it's compact and fast.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们有FAST来快速检测关键点，但我们仍然需要使用SIFT或SURF来计算描述符。我们需要一种快速计算描述符的方法。这就是BRIEF发挥作用的地方。BRIEF是一种提取特征描述符的方法。它本身不能检测关键点，因此我们需要与关键点检测器一起使用它。BRIEF的好处是它紧凑且快速。
- en: 'Consider the following image:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图像：
- en: '![Binary Robust Independent Elementary Features (BRIEF)](img/B04554_05_17.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![二值鲁棒独立基本特征 (BRIEF)](img/B04554_05_17.jpg)'
- en: 'BRIEF takes the list of input keypoints and outputs an updated list. So if
    you run BRIEF on this image, you will see something like this:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: BRIEF算法接收输入的关键点列表并输出一个更新后的列表。因此，如果你在这个图像上运行BRIEF算法，你会看到类似以下的内容：
- en: '![Binary Robust Independent Elementary Features (BRIEF)](img/B04554_05_18.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![二值鲁棒独立基本特征 (BRIEF)](img/B04554_05_18.jpg)'
- en: 'Following is the code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE6]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Oriented FAST and Rotated BRIEF (ORB)
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 定向快速旋转BRIEF (ORB)
- en: So, now we have arrived at the best combination out of all the combinations
    that we have discussed so far. This algorithm came out of the OpenCV Labs. It's
    fast, robust, and open-source! Both SIFT and SURF algorithms are patented and
    you can't use them for commercial purposes. This is why ORB is good in many ways.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经到达了迄今为止讨论的所有组合中最好的组合。这个算法来自OpenCV实验室。它快速、鲁棒且开源！SIFT和SURF算法都是受专利保护的，你不能用于商业目的。这就是为什么ORB在很多方面都是好的。
- en: 'If you run the ORB keypoint extractor on one of the images shown earlier, you
    will see something like the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你运行前面显示的图像之一的ORB关键点提取器，你会看到类似以下的内容：
- en: '![Oriented FAST and Rotated BRIEF (ORB)](img/B04554_05_19.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![定向快速旋转BRIEF (ORB)](img/B04554_05_19.jpg)'
- en: 'Here is the code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Summary
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the importance of keypoints and why we need
    them. We discussed various algorithms to detect keypoints and compute feature
    descriptors. We will be using these algorithms in all the subsequent chapters
    in various different contexts. The concept of keypoints is central to computer
    vision, and plays an important role in many modern systems.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了关键点的重要性以及为什么我们需要它们。我们讨论了各种检测关键点和计算特征描述符的算法。我们将在后续的所有章节中，以不同的背景使用这些算法。关键点的概念在计算机视觉中处于核心地位，并在许多现代系统中发挥着重要作用。
- en: In the next chapter, we are going to discuss how to stitch multiple images of
    the same scene together to create a panoramic image.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论如何将同一场景的多个图像拼接在一起以创建全景图像。
