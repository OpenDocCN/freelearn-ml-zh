- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Reinforcement Learning for Time-Series
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习在时间序列中的应用
- en: Reinforcement learning is a widely successful paradigm for control problems
    and function optimization that doesn't require labeled data. It's a powerful framework
    for experience-driven autonomous learning, where an agent interacts directly with
    the environment by taking actions and improves its efficiency by trial and error.
    Reinforcement learning has been especially popular since the breakthrough of the
    London-based Google-owned company DeepMind in complex games.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种广泛成功的控制问题和函数优化范式，不需要标注数据。它是一个强大的框架，支持经验驱动的自主学习，其中智能体通过采取行动与环境直接互动，并通过试错来提高效率。自从总部位于伦敦、由谷歌拥有的
    DeepMind 在复杂游戏中取得突破以来，强化学习尤其受到关注。
- en: In this chapter, we'll discuss a classification of **reinforcement learning**
    (**RL**) approaches in time-series specifically economics, and we'll deal with
    the accuracy and applicability of RL-based time-series models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章节中，我们将讨论强化学习（**RL**）在时间序列中的一种分类，特别是经济学中的应用，并将探讨基于 RL 的时间序列模型的准确性和适用性。
- en: We'll start with core concepts and algorithms in RL relevant to time-series
    and we'll talk about open issues and challenges in current deep RL models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从与时间序列相关的强化学习核心概念和算法开始，随后讨论当前深度强化学习模型中的开放问题和挑战。
- en: 'I am going to cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我将涵盖以下主题：
- en: Introduction to Reinforcement Learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Reinforcement Learning for Time-Series
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习在时间序列中的应用
- en: Bandit algorithms
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 盗贼算法
- en: Deep Q-Learning
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度 Q 学习
- en: Python Practice
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 实践
- en: Let's start with an introduction to reinforcement learning and the main concepts.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从强化学习的简介和主要概念开始。
- en: Introduction to reinforcement learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习简介
- en: Reinforcement learning is one of the main paradigms in machine learning alongside
    supervised and unsupervised methods. A major distinction is that supervised or
    unsupervised methods are passive, responding to changes, whereas RL is actively
    changing the environment and seeking out new data. In fact, from a machine learning
    perspective, reinforcement learning algorithms can be viewed as alternating between
    finding good data and doing supervised learning on that data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是机器学习的主要范式之一，与监督学习和无监督学习方法并列。一个主要的区别在于，监督学习或无监督学习是被动的，应对变化，而强化学习是主动的，通过改变环境来寻找新数据。实际上，从机器学习的角度来看，强化学习算法可以被视为在寻找好数据和在这些数据上进行监督学习之间交替进行。
- en: Computer programs based on reinforcement learning have been breaking through
    barriers. In a watershed moment for artificial intelligence, in March 2016, DeepMind's
    AlphaGo defeated the professional Go board game player Lee Sedol. Previously,
    the game of Go was considered to be a hallmark of human creativity and intelligence,
    too complex to be learned by a machine.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基于强化学习的计算机程序一直在突破各类障碍。在人工智能的一个重要时刻，2016年3月，DeepMind的AlphaGo击败了职业围棋选手李世石。此前，围棋被认为是人类创造力和智能的象征，复杂到机器无法学习。
- en: It has been argued that it is edging us closer toward **Artificial General Intelligence**
    (**AGI**). For example, in their paper "*Reward is enough*" (2021), David Silver,
    Satinder Singh, Doina Precup, and Richard S. Sutton argue that reward-orientated
    learning is enough to acquire knowledge, learn, perceive, socialize, understand
    and produce language, generalize, and imitate. More emphatically, they state that
    reinforcement learning agents could constitute a solution to AGI.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 有人认为，它正将我们带向**人工通用智能**（**AGI**）的目标。例如，在他们的论文《奖励就足够》（2021年）中，David Silver、Satinder
    Singh、Doina Precup 和 Richard S. Sutton 认为，基于奖励的学习足以获得知识、学习、感知、社交、理解和生成语言、概括以及模仿。他们更加强调地指出，强化学习智能体可能是解决
    AGI 的关键。
- en: '**Artificial General Intelligence** (**AGI**) is the hypothetical ability of
    an intelligent agent to understand or learn any intellectual task that would require
    intelligence. What is **intelligence** though? Often this is defined as anything
    humans can do or would consider hard. According to Turing Award winning computer
    scientist John McCarthy ("*What Is AI?*" 1998), "*intelligence is the computational
    part of the ability to achieve goals in the world.*"'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工通用智能**（**AGI**）是指智能体能够理解或学习任何需要智能的智力任务的假设能力。那么，**智能**到底是什么呢？通常，它被定义为人类能够做到或认为困难的任何事物。根据图灵奖得主、计算机科学家
    John McCarthy（“*什么是人工智能？*”1998年）的定义，“*智能是实现世界目标能力的计算部分。*”'
- en: 'In reinforcement learning, an agent interacts with the environment through
    actions and gets feedback in the shape of rewards. Contrary to the situation in
    supervised learning, no labeled data is available, but rather the environment
    is explored and exploited on the basis of the expectation of cumulative rewards.
    This feedback cycle of action and reward is illustrated in this diagram:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，代理通过行动与环境进行互动，并通过奖励的形式获得反馈。与监督学习中的情况相反，强化学习中没有标注数据可用，而是基于累积奖励的期望探索和利用环境。这个行动与奖励的反馈循环在下图中有所说明：
- en: '![](img/B17577_11_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_11_01.png)'
- en: 'Figure 11.1: Feedback loop in reinforcement learning'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.1：强化学习中的反馈循环
- en: Reinforcement learning is concerned with the objective of reward maximization.
    By interacting with the environment, the agent gets feedback and learns to take
    better actions. By optimizing the cumulative reward, the agent develops goal-directed
    behavior.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习关注的是奖励最大化的目标。通过与环境的互动，代理获得反馈并学习采取更好的行动。通过优化累积奖励，代理发展出目标导向的行为。
- en: '**Reinforcement learning** (**RL**) is an approach where an agent interacts
    directly with the environment by taking actions. The agent learns through trial
    and error to maximize the reward.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是一种方法，代理通过采取行动与环境直接互动。代理通过试错学习来最大化奖励。'
- en: If you've read *Chapter 8*, *Online Learning for Time-Series*, you might be
    confused about the difference between reinforcement learning and online learning,
    and it might be worthwhile to consider the two approaches in comparison. Some
    of the most prominent algorithms for reinforcement learning, Q-learning and Temporal
    Difference (TD) learning, to just name a couple of examples, are online algorithms,
    the way they update the value function.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你读过*第8章*，*时间序列的在线学习*，你可能会对强化学习和在线学习的区别感到困惑，值得将这两种方法进行比较。一些最著名的强化学习算法，如Q学习和时序差分（TD）学习，仅举几个例子，实际上是在线算法，它们通过更新价值函数的方式来进行学习。
- en: However, reinforcement learning doesn't focus on predictions, but on the interaction
    with the environment. In **online learning**, information is processed continuously,
    and the problem is clearly defined in terms of what's correct and what's incorrect.
    In reinforcement learning, the goal is the optimization of a delayed reward over
    a number of steps interacting with the environment. This is the main difference
    between the two approaches, although there are many particular details proponents
    of each technique would claim as theirs. Some of these we'll discuss later in
    this chapter, such as exploration versus exploitation and experience replay.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，强化学习不专注于预测，而是专注于与环境的互动。在**在线学习**中，信息是持续处理的，问题明确地定义为什么是正确的，什么是错误的。而在强化学习中，目标是通过与环境互动优化延迟的奖励。这是两种方法之间的主要区别，尽管每种技术的支持者可能会主张自己有很多细节内容。我们将在本章稍后讨论其中的一些内容，例如探索与利用以及经验回放。
- en: 'A reinforcement problem is defined by three main components: the environment
    ε, the agent *A*, and the cumulative objective. The agent is a decision-making
    entity that can observe the current state of the environment and takes an action.
    By performing an action ![](img/B17577_11_001.png), the agent transitions from
    state to state, ![](img/B17577_11_002.png). Executing an action in a specific
    state provides the agent with a reward, which is a numerical score. The reward
    is an instantaneous measurement of progress towards a goal.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 一个强化学习问题由三个主要组成部分定义：环境ε、代理*A*和累积目标。代理是一个决策实体，可以观察环境的当前状态并采取行动。通过执行一个行动 ![](img/B17577_11_001.png)，代理从一个状态转移到另一个状态，![](img/B17577_11_002.png)。在特定状态下执行一个行动会为代理提供奖励，这是一个数值评分。奖励是衡量朝目标前进的即时指标。
- en: The environment is in a certain state that depends on some combination of the
    current state and the action taken, although some of the changes could be random.
    It's the agent's objective to maximize a cumulative reward function. This cumulative
    reward objective can be the sum of rewards over a number of steps, a discounted
    sum, or the average reward over time.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 环境处于某种状态，这个状态依赖于当前状态和采取的行动的某种组合，尽管其中一些变化可能是随机的。代理的目标是最大化累积奖励函数。这个累积奖励目标可以是多个步骤中的奖励总和、折扣总和，或者是随时间变化的平均奖励。
- en: More formally, an agent in the context of RL is a system (or program) that receives
    an observation *O*[t] of the environment at time *t* and outputs an action ![](img/B17577_11_003.png)
    given its history of experiences ![](img/B17577_11_004.png).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地说，在强化学习的背景下，智能体是一个系统（或程序），它在时间 *t* 接收来自环境的观察 *O*[t]，并根据其经验历史输出一个动作 ![](img/B17577_11_003.png)。
- en: 'Meanwhile, an environment is another system. It receives an action *A*[t] at
    time *t* and changes its state in accordance with the history of actions and past
    states and a random process ![](img/B17577_11_005.png). The state is accessible
    to the agent to a certain degree, and to simplify we can state: ![](img/B17577_11_006.png).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 与此同时，环境是另一个系统。它在时间 *t* 接收一个动作 *A*[t]，并根据动作历史、过去的状态以及随机过程 ![](img/B17577_11_005.png)
    来改变其状态。环境状态在某种程度上对智能体是可访问的，为了简化，我们可以表述为： ![](img/B17577_11_006.png)。
- en: Finally, a reward is a scalar observation that is emitted at every time step
    *t* by the environment that provides momentaneous feedback to the agent on how
    well it is doing.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，奖励是一个标量观察值，在每个时间步 *t* 由环境发出，提供给智能体有关它执行得如何的即时反馈。
- en: 'At the core of the reinforcement learning agent is a model that estimates the
    value of an environmental state or suggests the next action in the environment.
    These are the two main categories of reinforcement learning: in **value-based**
    learning, a model approximates the outcomes of actions or the value of environmental
    states with a value function (a model) and the action selection reduces to take
    the action with the best expected outcome. In **policy-based** learning, we focus
    on the more direct goal of choosing the action by predicting an action from the
    environmental state.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习智能体的核心是一个模型，它估计环境状态的价值或建议在环境中采取的下一步行动。这是强化学习的两大主要类别：在**基于价值**的学习中，模型通过价值函数（模型）来逼近动作的结果或环境状态的价值，动作选择则简化为采取具有最佳期望结果的动作。在**基于策略**的学习中，我们专注于通过从环境状态预测动作这一更直接的目标来选择动作。
- en: 'There''s another twist to reinforcement learning: the **exploration versus
    exploitation dilemma**. You can decide to keep doing what you know works best
    (exploitation) or try out new avenues (exploration). Trying out new things will
    probably lead to worse results in the short run but might teach you important
    lessons that you can draw from in the future.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习还有另一个难题：**探索与利用的困境**。你可以决定继续做你知道最有效的事情（利用），或者尝试新的途径（探索）。尝试新事物可能会在短期内导致更差的结果，但可能会教会你一些重要的经验，以后可以加以借鉴。
- en: 'A simple approach to balance the two against each other is **epsilon-greedy**.
    This is a simple method to balance exploration and exploitation by choosing between
    exploration and exploitation randomly: either we follow our model''s advice, or
    we don''t. Epsilon is the parameter for the probability that we do an action that''s
    not recognized as the best by the model; the higher epsilon, the more random the
    model''s actions.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡两者的一种简单方法是**ε-贪心算法**。这是一种通过随机选择探索与利用之间的平衡的简单方法：要么我们遵循模型的建议，要么我们不遵循。Epsilon
    是我们做出一个不被模型认为是最佳的动作的概率参数；epsilon 值越高，模型的动作越随机。
- en: '**Deep reinforcement learning** (**DRL**) techniques are a subset of reinforcement
    learning methods, where the model is a deep neural network (or, in a looser sense,
    a multilayer-perceptron).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '**深度强化学习**（**DRL**）技术是强化学习方法的一个子集，其中模型是深度神经网络（或在更宽松的意义上是多层感知器）。'
- en: In the next section, we'll look into how RL can be applied to time-series!
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何将强化学习应用于时间序列！
- en: Reinforcement Learning for Time-Series
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列的强化学习
- en: Reinforcement Learning (RL) can and has been applied to time-series, however,
    the problem has to be framed in a certain way. For reinforcement learning, we
    need to have significant feedback between predictions and ongoing (actions) of
    the system.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）可以并且已经应用于时间序列，但问题必须以特定的方式框定。对于强化学习，我们需要在预测和系统的持续行为（动作）之间有显著的反馈。
- en: In order to apply RL to time-series forecasting or predictions, the prediction
    has to condition an action, therefore the state evolution depends on the current
    state and the agent's action (and randomness). Hypothetically, rewards could be
    a performance metric about the accuracy of predictions. However, the consequences
    of good or bad predictions do not affect the original environment. Essentially
    this corresponds to a supervised learning problem.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将强化学习应用于时间序列预测或预测，预测必须基于某种动作，因此状态的演化依赖于当前状态和智能体的动作（以及随机性）。假设，奖励可以是关于预测准确性的性能指标。然而，良好或不良预测的后果不会影响原始环境。实质上，这相当于一个监督学习问题。
- en: More meaningfully, if we want to frame our situation as an RL problem, the state
    of the systems should be affected by the agents' decisions. For instance, in the
    case of interacting with the stock market, we would buy or sell based on predictions
    of the movements and include something that we influence such as our portfolio
    and funds in the state, or (only really if we are a market maker) the influence
    we have over the stock movements.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 更有意义的是，如果我们想将我们的情况框架化为一个强化学习（RL）问题，那么系统的状态应该受到智能体决策的影响。例如，在与股市互动的情况下，我们会根据对市场波动的预测来买入或卖出，并将我们影响的因素（如我们的投资组合和资金）纳入状态，或者（只有当我们是市场制造者时）考虑我们对股票波动的影响。
- en: 'In summary, RL is very apt for dealing with processes that change over time,
    although RL deals with those that can be controlled or influenced. A core application
    for time-series is in industrial processes and control – this was in fact already
    pointed out by Box and Jenkins in their classic book "*Time-Series Analysis: Forecasting
    and Control*").'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，强化学习非常适合处理随时间变化的过程，尽管强化学习处理的是那些可以控制或影响的过程。时间序列的一个核心应用是在工业过程和控制中——事实上，这一点已经在Box和Jenkins的经典著作《*时间序列分析：预测与控制*》中提到过。
- en: There are lots of applications that we could think of for reinforcement learning.
    Trading on the stock market is a major driver of business growth, and the presence
    of uncertainty and risk recommends it as a reinforcement learning use case. In
    pricing, for example in insurance or retail, reinforcement learning can help explore
    the space of value proposition for customers that would yield high sales, while
    optimizing the margin. Finally auction mechanisms, for example online bidding
    for advertisements, are another domain. In auctions, reinforcement agents have
    to develop responses in the presence of other players.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以想到很多强化学习的应用。股市交易是商业增长的主要驱动力，而其中的不确定性和风险使其成为强化学习的一个应用场景。在定价领域，例如保险或零售，强化学习可以帮助探索客户价值主张的空间，从而实现高销售额，并优化利润率。最后，拍卖机制，例如在线广告竞价，是另一个领域。在拍卖中，强化学习智能体必须在其他参与者的存在下开发响应策略。
- en: Let's go more into detail about a few algorithms – first, bandits.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细探讨一下几个算法——首先是赌博机问题。
- en: Bandit algorithms
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赌博机算法
- en: A **Multi-Armed Bandit** (**MAB**) is a classic reinforcement learning problem,
    in which a player is faced with a slot machine (bandit) that has *k* levers (arms),
    each with a different reward distribution. The agent's goal is to maximize its
    cumulative reward on a trial-by-trial basis. Since MABs are a simple but powerful
    framework for algorithms that make decisions over time under uncertainty, a large
    number of research articles have been dedicated to them.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**多臂赌博机**（**MAB**）是一个经典的强化学习问题，其中玩家面临一个老虎机（赌博机），它有*k*个拉杆（臂），每个拉杆都有不同的奖励分布。智能体的目标是在每次试验中最大化其累计奖励。由于多臂赌博机是一个简单但强大的框架，用于在不确定性下做出决策的算法，因此大量的研究文章致力于此问题。'
- en: Bandit learning refers to algorithms that aim to optimize a single unknown stationary
    objective function. An agent chooses an action from a set of actions ![](img/B17577_11_007.png).
    The environment reveals reward ![](img/B17577_11_008.png) of the chosen action
    at time *t*. As information is accumulated over multiple rounds, the agent can
    build a good representation of the value (or reward) distribution for each arm,
    ![](img/B17577_11_009.png).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 赌博机学习指的是旨在优化一个未知的静态目标函数的算法。智能体从一组动作中选择一个动作！[](img/B17577_11_007.png)。环境在时间*t*时揭示所选动作的奖励！[](img/B17577_11_008.png)。随着信息在多轮中积累，智能体可以建立一个良好的价值（或奖励）分布表示！[](img/B17577_11_009.png)。
- en: 'Therefore, a good policy might converge so that the choice of arm becomes optimal.
    According to one policy, **UCB1** (published by Peter Auer, Nicolò Cesa-Bianchi,
    and Paul Fischer, "*Finite-Time Analysis of the Multi-Armed Bandit Problem*",
    2002), given the expected values for each action, the action is chosen that maximizes
    this criterion:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个好的策略可能会收敛，使得选择的臂变得最优。根据一种策略，**UCB1**（由Peter Auer、Nicolò Cesa-Bianchi和Paul
    Fischer在2002年发布的《*有限时间分析多臂强盗问题*》中提出），在给定每个行动的预期值的情况下，选择能够最大化该标准的行动：
- en: '![](img/B17577_11_010.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_11_010.png)'
- en: The second term refers to the upper confidence bound of the reward values based
    on the information we have accumulated. Here, *t* refers to the number of iterations
    so far, the time step, and ![](img/B17577_11_011.png) to the number of times action
    *a* has been executed so far. This means that the nominator in the equation increases
    logarithmically with time and the denominator increases each time we receive reward
    information from the action.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 第二项指的是基于我们积累的信息，奖励值的上置信界。这里，*t*表示到目前为止的迭代次数，即时间步长，而![](img/B17577_11_011.png)表示到目前为止执行行动*a*的次数。这意味着方程中的分子随着时间以对数方式增加，而分母在每次我们从行动中获取奖励信息时都会增加。
- en: When the available rewards are binary (win or lose, yes or no, charge or no
    charge) then this can be described by a Beta distribution. The Beta distribution
    takes two parameters, ![](img/B17577_05_038.png) and ![](img/B17577_11_013.png),
    for wins and losses, respectively. The mean value is ![](img/B17577_11_014.png).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当可用的奖励是二元的（赢或输，是或否，收费或不收费）时，这可以用贝塔分布来描述。贝塔分布有两个参数，分别是![](img/B17577_05_038.png)和![](img/B17577_11_013.png)，用于表示赢和输。均值是![](img/B17577_11_014.png)。
- en: In **Thompson sampling**, we sample from the Beta distribution of each action
    (arm) and choose the action with the highest estimated return. The Beta distribution
    narrows with the number of tries, therefore actions that have been tried infrequently
    have wide distributions. Therefore, Beta sampling models the estimated mean reward
    and the level of confidence in the estimate. In **Dirichlet sampling**, instead
    of sampling from a Beta distribution, we are sampling from a Dirichlet distribution
    (also called multivariate Beta distribution).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在**汤普森采样**中，我们从每个行动（臂）的贝塔分布中采样，并选择具有最高预期回报的行动。随着尝试次数的增加，贝塔分布会变窄，因此那些尝试较少的行动具有较宽的分布。因此，贝塔采样模型估计了均值奖励和估计的置信度。在**狄利克雷采样**中，我们不从贝塔分布中采样，而是从狄利克雷分布中采样（也称为多元贝塔分布）。
- en: '**Contextual bandits** incorporate information about the environment for updating
    the reward expectation. If you think about ads, this contextual information could
    be if the ad is about traveling. The advantage of contextual bandits is agents
    can encode much richer information about the environment.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文强盗**将环境信息融入其中，用于更新奖励预期。如果你考虑广告，这种上下文信息可能是广告是否与旅行相关。上下文强盗的优点在于，代理可以对环境编码更丰富的信息。'
- en: 'In contextual bandits, an agent chooses an arm, the reward ![](img/B17577_11_015.png)
    is revealed, and the agent''s expectation of the reward is updated, but with context
    features: ![](img/B17577_11_016.png), where *x* is a set of features encoding
    the environment. In many implementations, the context is often restricted to discrete
    values, however, at least in theory, they could be either categorical or numerical.
    The value function could be any machine learning algorithm such as a neural network
    (NeuralBandit) or a random forest (BanditForest).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在上下文强盗中，代理选择一个臂，奖励![](img/B17577_11_015.png)被揭示，代理的奖励预期被更新，但带有上下文特征：![](img/B17577_11_016.png)，其中*x*是编码环境的一组特征。在许多实现中，上下文通常限制为离散值，但至少在理论上，它们可以是分类的或数值的。价值函数可以是任何机器学习算法，如神经网络（NeuralBandit）或随机森林（BanditForest）。
- en: Bandits find applications, among other fields, in information retrieval models
    such as recommender and ranking systems, which are employed in search engines
    or on consumer websites. The **probability ranking principle** (PRP; from S.E.
    Robertson's article "*The probability ranking principle in IR*", 1977) forms the
    theoretical basis for probabilistic models, which have been dominating IR. The
    PRP states that articles should be ranked in decreasing order of relevance probability.
    This is what we'll go through in an exercise in the practice section.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 强盗算法在多个领域中都有应用，包括信息检索模型，如推荐系统和排名系统，这些系统被用于搜索引擎或消费者网站中。**概率排名原则**（PRP；来自S.E.
    Robertson的文章“*信息检索中的概率排名原则*”，1977年）为概率模型提供了理论基础，而这些模型已经主导了信息检索领域。PRP指出，文章应该按照相关性概率的递减顺序进行排序。这也是我们将在练习部分中讲解的内容。
- en: Let's delve into Q-learning and deep Q-learning now.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们深入探讨Q学习和深度Q学习。
- en: Deep Q-Learning
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度Q学习
- en: Q-learning, introduced by Chris Watkins in 1989, is an algorithm to learn the
    value of an action in a particular state. Q-learning revolves around representing
    the expected rewards for an action taken in a given state.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习，由Chris Watkins于1989年提出，是一种学习在特定状态下动作价值的算法。Q学习围绕表示在给定状态下执行某个动作的期望回报展开。
- en: 'The expected reward of the state-action combination ![](img/B17577_11_017.png)
    is approximated by the Q function:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 状态-动作组合![](img/B17577_11_017.png)的期望回报由Q函数近似：
- en: '![](img/B17577_11_018.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_11_018.png)'
- en: '*Q* is initialized to a fixed value, usually at random. At each time step *t*,
    the agent selects an action ![](img/B17577_11_019.png) and sees a new state of
    the environment ![](img/B17577_11_020.png) as a consequence and receives a reward.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*被初始化为一个固定值，通常是随机的。在每个时间步* t *，智能体选择一个动作![](img/B17577_11_019.png)，并看到环境的新状态![](img/B17577_11_020.png)，作为结果并获得回报。'
- en: 'The value function *Q* can then be updated according to the Bellman equation
    as the weighted average of the old value and the new information:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 价值函数*Q*可以根据贝尔曼方程更新，作为旧价值和新信息的加权平均值：
- en: '![](img/B17577_11_021.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_11_021.png)'
- en: The weight is by ![](img/B17577_05_038.png), the learning rate – the higher
    the learning rate, the more adaptive the Q-function. The discount factor ![](img/B17577_11_023.png)
    is weighting the rewards by their immediacy – the higher ![](img/B17577_11_024.png),
    the more impatient (myopic) the agent becomes.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 权重由![](img/B17577_05_038.png)表示，学习率——学习率越高，Q函数越适应。折扣因子![](img/B17577_11_023.png)通过其即时性对回报进行加权——折扣因子越高，智能体越不耐烦（近视）。
- en: '![](img/B17577_11_025.png) represents the current reward. ![](img/B17577_11_026.png)
    is the reward obtained by ![](img/B17577_11_027.png) weighted by learning rate
    ![](img/B17577_11_028.png), and ![](img/B17577_11_029.png) is the weighted maximum
    reward that can be obtained from state ![](img/B17577_11_030.png).'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/B17577_11_025.png)表示当前回报。![](img/B17577_11_026.png)是通过学习率![](img/B17577_11_028.png)加权后的![](img/B17577_11_027.png)获得的回报，而![](img/B17577_11_029.png)是从状态![](img/B17577_11_030.png)中获得的加权最大回报。'
- en: 'This last part can be recursively broken down into simpler sub-problems like
    this:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一部分可以递归地分解成更简单的子问题，如下所示：
- en: '![](img/B17577_11_031.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_11_031.png)'
- en: In the simplest case, *Q* can be a lookup table, called a Q-table.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，*Q*可以是一个查找表，称为Q表。
- en: In 2014, Google DeepMind patented an algorithm called **deep Q-learning**. This
    algorithm was introduced in the Nature paper "*Human-level control through deep
    reinforcement learning*" with an application in Atari 2600 games.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 2014年，Google DeepMind申请了一种名为**深度Q学习**的算法的专利。该算法首次在《自然》杂志上的文章“*通过深度强化学习实现人类水平控制*”中提出，并应用于Atari
    2600游戏。
- en: In Deep Q-learning, a neural network is used for the Q-function as a nonlinear
    function approximator. They used a convolutional neural network to learn expected
    rewards from pixel values. They introduced a technique called **experience replay**
    to update Q over a randomly drawn sample of prior actions. This is done to reduce
    the learning instability of the Q updates.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度Q学习中，神经网络作为非线性函数逼近器被用于Q函数。他们使用卷积神经网络从像素值中学习期望回报。他们引入了一种名为**经验回放**的技术，通过随机抽取先前动作的样本来更新Q。这是为了减少Q更新的学习不稳定性。
- en: 'Q-learning can be shown in pseudocode roughly like this:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习的大致伪代码如下所示：
- en: '[PRE0]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This implements an epsilon-greedy policy by which a random (exploratory) choice
    is made according to the probability `epsilon`. A few more variables are assumed
    given. The handle for the environment, `env`, allows us to execute an action.
    We have a learning function, which applies gradient descent on the Q-function
    to learn better values according to the Bellman equation. The parameter `L` is
    the number of previous values that are used for learning.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 该库实现了一个epsilon-贪心策略，其中根据概率`epsilon`做出一个随机（探索性）选择。还假设了其他一些变量。环境句柄`env`允许我们执行一个动作。我们有一个学习函数，通过对Q函数应用梯度下降法，根据贝尔曼方程学习更好的值。参数`L`是用于学习的先前值的数量。
- en: The memory replay part is obviously simplified. In actuality, we would have
    a maximum size of the memory, and, once the memory capacity is reached, we would
    replace old associations of states, actions, and rewards with new ones.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 内存重放部分显然被简化了。实际上，我们会有一个内存的最大大小，一旦内存达到最大容量，我们就会用新的状态、动作和奖励替换旧的关联。
- en: We'll put some of this into practice now.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将实际操作一下。
- en: Python Practice
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python实践
- en: Let's get into modeling. We'll start by giving some recommendations for users
    using MABs.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始建模。我们将从给用户提供一些基于MAB的推荐开始。
- en: Recommendations
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推荐
- en: For this example, we'll take joke preferences by users, and we'll use them to
    simulate feedback on recommended jokes on our website. We'll use this feedback
    to tune our recommendations. We want to select the 10 best jokes to present to
    people visiting our site. The recommendations are going to be produced by 10 MABs
    that each have as many arms as there are jokes.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将收集用户的笑话偏好，并使用这些数据模拟网站上推荐笑话的反馈。我们将使用这些反馈来调整我们的推荐。我们的目标是选择10个最好的笑话，展示给访问我们网站的人。这些推荐将由10个MAB生成，每个MAB有与笑话数量相同的臂。
- en: This is adapted from an example from the `mab-ranking` library on GitHub by
    Kenza-AI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是从GitHub上Kenza-AI的`mab-ranking`库中的一个示例改编的。
- en: 'It''s a handy library that comes with implementations of different bandits.
    I''ve simplified the installation of this library in my fork of the library, so
    we''ll be using my fork here:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个方便的库，提供了不同强盗算法的实现。我在该库的分支中简化了库的安装，因此我们将在这里使用我的分支：
- en: '[PRE1]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After this is finished, we can get right to it!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以直接开始！
- en: 'We''ll download the `jester` dataset with joke preferences from S3\. Here''s
    the location:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从S3下载`jester`数据集，其中包含笑话偏好。下载地址如下：
- en: '[PRE2]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We''ll download them using pandas:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用pandas下载这些数据：
- en: '[PRE3]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We''ll make some cosmetic adjustments. The rows refer to users, the columns
    to jokes. We can make this clearer:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将做一些外观上的调整。行表示用户，列表示笑话。我们可以让这一点更加清晰：
- en: '[PRE4]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The encoding of choices is a bit weird, so we''ll fix this as well:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 选择的编码有点奇怪，所以我们也将修复这一点：
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'So either people chose the joke or they didn''t. We''ll get rid of people who
    didn''t choose any joke at all:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，要么人们选择了笑话，要么他们没有选择任何笑话。我们将去除那些没有选择任何笑话的人：
- en: '[PRE6]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Our dataset looks like this now:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集现在看起来是这样的：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_oTdq4A/Screenshot
    2021-09-04 at 16.41.24.png](img/B17577_11_02.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_oTdq4A/Screenshot
    2021-09-04 at 16.41.24.png](img/B17577_11_02.png)'
- en: 'Figure 11.2: Jester dataset'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2：Jester数据集
- en: 'We''ll set up our bandits as follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按以下方式设置我们的强盗：
- en: '[PRE7]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We choose independent bandits with Thompson sampling from the Beta distribution.
    We recommend the best 10 jokes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从Beta分布中使用汤普森采样选择独立的强盗。我们推荐最好的10个笑话。
- en: 'We can then start our simulation. Our hypothetical website has lots of visits,
    and we''ll get feedback on the 10 jokes that we''ll display as chosen by our independent
    bandits:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以开始模拟。我们假设的网站有很多访问者，我们将根据独立强盗选择的笑话展示，并获得反馈：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We are simulating 7,000 iterations (visits). At each visit, we'll change our
    choices according to the updated reward expectations.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在模拟7,000次迭代（访问）。在每次访问中，我们将根据更新后的奖励预期改变我们的选择。
- en: 'We can plot the hit rate, the jokes that users are selecting, as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按以下方式绘制命中率，以及用户选择的笑话：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'I''ve introduced a rolling average (over 200 iterations) to get a smoother
    graph:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我引入了一个滚动平均（基于200次迭代），以获得更平滑的图表：
- en: '![](img/B17577_11_03.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17577_11_03.png)'
- en: 'Figure 11.3: Hit rate over time (Dirichlet sampling)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3：随着时间推移的命中率（Dirichlet采样）
- en: The mab-ranking library supports contextual information, so we can try out giving
    additional information. Let's imagine this information as different user groups
    (cohorts). We could think of users who use different search or filter functionality
    on our imaginary website, say "newest jokes" or "most popular." Alternatively,
    they could be from different regions. Or it could be a timestamp category that
    corresponds to the time of the day of visits of users to our website.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: mab-ranking 库支持上下文信息，因此我们可以尝试提供额外信息。想象这些信息为不同的用户组（队列）。我们可以考虑使用不同搜索或过滤功能的用户，例如我们假设的网站上的
    "最新笑话" 或 "最流行"。或者它们可能来自不同的地区。或者它可以是对应用户访问我们网站的时间的时间戳类别。
- en: 'Let''s supply the categorical user group information, the context. We''ll cluster
    users by their preferences, and we''ll use the clusters as context:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们提供分类用户组信息，即上下文。我们将根据其偏好对用户进行聚类，并将使用这些群集作为上下文：
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This creates 5 user groups.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这创造了 5 个用户组。
- en: 'We''ll have to reset our bandits:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重置我们的强盗：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then, we can redo our simulation. Only now, we''ll supply the user context:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以重新进行我们的模拟。现在，我们将提供用户上下文：
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can visualize the hit rate over time again:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以再次可视化随时间变化的命中率：
- en: '![recommendations_dirichlet_context.png](img/B17577_11_04.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](/img/B17577_11_04.png)'
- en: 'Figure 11.4: Hit rate over time (Dirichlet sampling with context)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4：随时间变化的命中率（具有上下文的狄利克雷抽样）
- en: We can see that the hit rate is a bit higher than before.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到命中率比之前略高。
- en: This model ignores the order of the recommended jokes on our hypothetical website.
    There are other bandit implementations that model the ranks.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 此模型忽略了在我们假设的网站上推荐笑话的顺序。还有其他模型化排名的强盗实现。
- en: I'll leave it to the reader to play around with this more. A fun exercise is
    to create a probabilistic model of reward expectations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我将留给读者更多地进行探索。一个有趣的练习是创建奖励期望的概率模型。
- en: In the next section, we'll be playing around with a deep Q-learning trading
    bot. This is a more intricate model and will require more attention. We'll apply
    this to cryptocurrency trading.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将尝试深度 Q 学习交易机器人。这是一个更复杂的模型，需要更多的关注。我们将把它应用到加密货币交易中。
- en: Trading with DQN
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 DQN 进行交易
- en: This is based on a tutorial of the TensorTrade library, which we'll use in this
    example. TensorTrade is a framework for building, training, evaluating, and deploying
    robust trading algorithms using reinforcement learning.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这是基于 TensorTrade 库的教程，我们将在此示例中使用它。TensorTrade 是一个通过强化学习构建、训练、评估和部署稳健交易算法的框架。
- en: 'TensorTrade relies on existing tools such as OpenAI Gym, Keras, and TensorFlow
    to enable fast experimentation with algorithmic trading strategies. We''ll install
    it with pip as usual. We''ll make sure we install the latest version from GitHub:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: TensorTrade 依赖于诸如 OpenAI Gym、Keras 和 TensorFlow 等现有工具，以便快速实验算法交易策略。我们将像往常一样通过
    pip 安装它。我们会确保从 GitHub 安装最新版本：
- en: '[PRE13]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We could also install the `ta` library, which can provide additional signals
    useful for trading, but we'll leave this out here.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以安装 `ta` 库，它可以提供对交易有用的额外信号，但在这里我们将其省略。
- en: 'Let''s get a few imports out of the way:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先完成几个导入：
- en: '[PRE14]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These imports concern utilities for the (simulated) exchange, the portfolio,
    and the environment. Further, there are utilities for data loading and feeding
    it into the simulation, constants for currency conversion, and finally, there's
    a deep Q-agent, which consists of a Deep Q-Network (DQN).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这些导入涉及（模拟）交易、投资组合和环境的实用程序。此外，还有用于数据加载和将其提供给模拟的实用程序，用于货币转换的常量，最后还有一个深度 Q 代理，它包含一个深度
    Q 网络（DQN）。
- en: Please note that the matplotlib magic command `(%matplotlib inline`) is needed
    for the Plotly charts to show up as expected.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，matplotlib 的魔术命令 `(%matplotlib inline)` 是为了让 Plotly 图表按预期显示而需要的。
- en: 'As a first step, let''s load a dataset of historical cryptocurrency prices:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一步，让我们加载一个历史加密货币价格数据集：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This dataset consists of hourly Bitcoin prices in US dollars. It looks like
    this:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含以美元计价的比特币每小时价格。它看起来像这样：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_uXbUvY/Screenshot
    2021-09-05 at 10.57.42.png](img/B17577_11_05.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_uXbUvY/Screenshot
    2021-09-05 at 10.57.42.png)'
- en: 'Figure 11.5: Crypto dataset'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5：加密货币数据集
- en: We'll add a relative strength indicator signal, a technical indicator for the
    financial markets. It measures the strength or weakness of a market by comparing
    the closing prices of a recent trading period. We'll also add a **moving average
    convergence/divergence** (**MACD**) indicator, which is designed to reveal changes
    in the strength, direction, momentum, and duration of a trend.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将添加一个相对强弱指标信号，这是一个用于金融市场的技术指标。它通过比较最近交易周期的收盘价来衡量市场的强弱。我们还将添加一个**移动平均收敛/发散**（**MACD**）指标，它旨在揭示趋势的强度、方向、动量和持续时间的变化。
- en: 'These two are defined as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个定义如下：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Alternatively, here we could be using trading signals from the `ta` library.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用 `ta` 库中的交易信号。
- en: 'We''ll now set up the feed that goes into our decision making:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将设置进入决策过程的数据流：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We are selecting the closing price as a feature.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了收盘价作为特征。
- en: 'Now, we''ll add our indicators as additional features:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将添加我们的指标作为额外的特征：
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Aside from RSI and MACD, we are also adding a trend indicator (LR).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 RSI 和 MACD，我们还添加了一个趋势指标（LR）。
- en: 'We can have a look at the first five lines from the data feed:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看数据流中的前五行：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is what our trading signal features look like:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们交易信号特征的展示：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_y5gWwn/Screenshot
    2021-09-05 at 11.11.53.png](img/B17577_11_06.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_y5gWwn/Screenshot
    2021-09-05 at 11.11.53.png](img/B17577_11_06.png)'
- en: 'Figure 11.6: Data feed for trading'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6：交易数据流
- en: 'Let''s set up the broker:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置经纪人：
- en: '[PRE20]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The exchange is the interface that will let us execute orders. An exchange needs
    a name, an execution service, and streams of price data. Currently, TensorTrade
    supports a simulated execution service using simulated or stochastic data.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 交易所是让我们执行订单的接口。交易所需要一个名称、一个执行服务和价格数据流。目前，TensorTrade 支持使用模拟或随机数据的模拟执行服务。
- en: 'Now we need a portfolio:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要一个投资组合：
- en: '[PRE21]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: A portfolio can be any combination of exchanges and instruments that the exchange
    supports.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 投资组合可以是交易所支持的任何组合的交易所和工具。
- en: 'TensorTrade includes lots of monitoring tools, called renderers, which can
    be attached to the environment. They can draw a chart (`PlotlyTradingChart`) or
    log to a file (`FileLogger`), for example. Here''s our setup:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: TensorTrade 包含许多监控工具，称为渲染器，可以附加到环境中。例如，它们可以绘制图表（`PlotlyTradingChart`）或记录到文件（`FileLogger`）。这是我们的设置：
- en: '[PRE22]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, here''s the trading environment, which is an instance of the OpenAI
    Gym (the OpenAI Gym provides a wide variety of simulated environments):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这就是交易环境，它是 OpenAI Gym 的一个实例（OpenAI Gym 提供了各种各样的模拟环境）：
- en: '[PRE23]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: You might be familiar with Gym environments if you've done reinforcement learning
    before.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前做过强化学习，你可能会熟悉 Gym 环境。
- en: 'Let''s check the Gym feed:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看 Gym 数据：
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Here''s what comes through:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这是输出内容：
- en: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_GBkklu/Screenshot
    2021-09-05 at 11.40.11.png](img/B17577_11_07.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![/var/folders/80/g9sqgdws2rn0yc3rd5y3nd340000gp/T/TemporaryItems/NSIRD_screencaptureui_GBkklu/Screenshot
    2021-09-05 at 11.40.11.png](img/B17577_11_07.png)'
- en: 'Figure 11.7: Environment data feed for trading bot'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7：交易机器人环境数据流
- en: This is what the trading bot will be able to rely on for making decisions on
    executing trades.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这是交易机器人将依赖于执行交易决策的依据。
- en: 'Now we can set up and train our DQN trading agent:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以设置并训练我们的 DQN 交易代理：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: It might be a good point here to explain the difference between an epoch and
    an episode. Readers will probably be familiar with an epoch, which is a single
    pass over all training examples, whereas an episode is specific to the context
    of reinforcement learning. An episode is a sequence of states, actions, and rewards,
    which ends with a terminal state.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这里也许是个好机会来解释一下 epoch 和 episode 之间的区别。读者可能会熟悉 epoch，它是对所有训练样本的单次遍历，而 episode
    是强化学习中的专有概念。episode 是由一系列状态、动作和奖励组成的序列，以终止状态结束。
- en: 'We get lots of plotting output from our renderer. Here''s the first output
    I got (yours might differ a bit):'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从渲染器中获得了大量的绘图输出。这是我获得的第一个输出（你的可能会有所不同）：
- en: '![../trading_renderer.png](img/B17577_11_08.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![../trading_renderer.png](img/B17577_11_08.png)'
- en: 'Figure 11.8: PlotlyPlotRenderer – Episode 2/2 Step 51/200'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8：PlotlyPlotRenderer – Episode 2/2 Step 51/200
- en: This plot gives an overview of the market operations of our trading bot. The
    first subplot shows the up and down movements of the prices. Then the second subplot
    charts volumes of stock in the portfolio, and in the bottom-most subplot, you
    can see the portfolio net worth.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图展示了我们交易机器人市场操作的概览。第一个子图显示了价格的涨跌。接下来的第二个子图展示了投资组合中的股票交易量，而最底部的子图则显示了投资组合的净资产。
- en: 'If you want to see the net worth over time (not only the first snapshot as
    above), you can plot this as well:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想查看随时间变化的净资产（不仅仅是上面提到的第一张快照），你也可以绘制这个图：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Here''s the portfolio net worth over time:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这是随时间变化的投资组合净资产：
- en: '![networth.png](img/B17577_11_09.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![networth.png](img/B17577_11_09.png)'
- en: 'Figure 11.9: Portfolio worth over time'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.9：投资组合随时间变化的价值
- en: It looks like our trading bot could need some more training before getting let
    loose in the wild. I made a loss, so I am happy there wasn't real money on the
    line.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的交易机器人可能需要更多的训练才能在真实市场中投入使用。我亏损了，所以很庆幸没有投入真实资金。
- en: This is all folks. Let's summarize.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。让我们总结一下。
- en: Summary
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: While online learning, which we talked about in *Chapter 8*, *Online Learning
    for Time-Series* is tackling traditional supervised learning, reinforcement learning
    tries to deal with the environment. In this chapter, I've introduced reinforcement
    learning concepts relevant to time-series, and we've discussed many algorithms,
    such as deep Q-learning and **MABs**.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习，我们在*第8章*中讨论过，*时间序列的在线学习*处理的是传统的监督学习，而强化学习则试图应对环境。在本章中，我介绍了与时间序列相关的强化学习概念，并讨论了许多算法，如深度Q学习和**MABs**。
- en: Reinforcement learning algorithms are very useful in certain contexts like recommendations,
    trading, or – more generally – control scenarios. In the practice section, we
    implemented a recommender using MABs and a trading bot with a DQN.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习算法在某些场景下非常有用，比如推荐系统、交易，或者更一般地说，控制场景。在实践部分，我们实现了一个使用MAB的推荐系统和一个带有DQN的交易机器人。
- en: In the next chapter, we'll look at case studies with time-series. Among other
    things, we'll look at multivariate forecasts of energy demand.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将查看带有时间序列的案例研究。除此之外，我们还会看一下多元预测的能源需求。
