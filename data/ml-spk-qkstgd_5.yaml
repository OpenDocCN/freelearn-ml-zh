- en: Unsupervised Learning Using Apache Spark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Apache Spark进行无监督学习
- en: 'In this chapter, we will train and evaluate unsupervised machine learning models
    applied to a variety of real-world use cases, again using Python, Apache Spark,
    and its machine learning library, `MLlib`. Specifically, we will develop and interpret
    the following types of unsupervised machine learning models and techniques:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将训练和评估应用于各种实际用例的无监督机器学习模型，再次使用Python、Apache Spark及其机器学习库`MLlib`。具体来说，我们将开发并解释以下类型无监督机器学习模型和技术：
- en: Hierarchical clustering
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类
- en: K-means clustering
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means聚类
- en: Principal component analysis
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析
- en: Clustering
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: 'As described in [Chapter 3](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml), *Artificial
    Intelligence and Machine Learning*, in unsupervised learning, the goal is to uncover
    hidden relationships, trends, and patterns given only the input data, *x[i]*,
    with no output, *y[i]*. In other words, our input dataset will be of the following
    form:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第3章](a2def916-85c5-4a3a-b239-d2b2de09e199.xhtml)“人工智能与机器学习”中所述，在无监督学习中，目标是仅根据输入数据，即*x[i]*，没有输出*y[i]*，揭示隐藏的关系、趋势和模式。换句话说，我们的输入数据集将具有以下形式：
- en: '![](img/3f2330fc-359c-4772-824c-3ba8d26bb77d.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3f2330fc-359c-4772-824c-3ba8d26bb77d.png)'
- en: Clustering is a well-known example of a class of unsupervised learning algorithms
    where the goal is to segment data points into groups, where all of the data points
    in a specific group share similar features or attributes in common. By the nature
    of clustering, however, it is recommended that clustering models are trained on
    large datasets to avoid over fitting. The two most commonly used clustering algorithms
    are **hierarchical clustering** and **k-means clustering**, which are differentiated
    from each other by the processes by which they construct clusters. We shall study
    both of these algorithms in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是已知无监督学习算法类别的一个例子，其目标是将数据点分割成组，其中特定组中的所有数据点共享相似的特征或属性。然而，由于聚类的性质，建议在大型数据集上训练聚类模型以避免过拟合。最常用的两种聚类算法是**层次聚类**和**k-means聚类**，它们通过构建簇的过程彼此区分。我们将在本章中研究这两种算法。
- en: Euclidean distance
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欧几里得距离
- en: 'By definition, in order to cluster data points into groups, we require an understanding
    of the *distance* between two given data points. A common measure of distance
    is the **Euclidean distance**, which is simply the straight-line distance between
    two given points in *k*-dimensional space, where *k* is the number of independent
    variables or features. Formally, the Euclidean distance between two points, *p*
    and *q*, given *k* independent variables or dimensions is defined as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，为了将数据点聚类成组，我们需要了解两个给定数据点之间的*距离*。距离的一个常见度量是**欧几里得距离**，它简单地表示在*k*-维空间中两个给定点之间的直线距离，其中*k*是独立变量或特征的个数。形式上，两个点*p*和*q*之间的欧几里得距离，给定*k*个独立变量或维度，定义为以下：
- en: '![](img/1555c487-d45f-4a1d-ac70-8be68fef47a6.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1555c487-d45f-4a1d-ac70-8be68fef47a6.png)'
- en: 'Other common measures of distance include the **Manhattan distance**, which
    is the sum of the absolute values instead of squares ( ![](img/c9a63cea-d2fb-4e72-b7fc-9c7471329d7a.png))
    and the **maximum coordinate distance**, where measurements are only considered
    for those data points that deviate the most. For the remainder of this chapter,
    we will measure the Euclidean distance. Now that we have an understanding of distance,
    we can define the following measures between two clusters, as illustrated in *Figure
    5.1*:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常见的距离度量包括**曼哈顿距离**，它是绝对值的和而不是平方（![图片](img/c9a63cea-d2fb-4e72-b7fc-9c7471329d7a.png)）和**最大坐标距离**，其中只考虑那些偏离最大的数据点。在本章的剩余部分，我们将测量欧几里得距离。现在，我们已经了解了距离，我们可以定义两个簇之间的以下度量，如图*图5.1*所示：
- en: The *minimum distance* between clusters is the distance between the two points
    that are the closest to each other.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇之间的*最小距离*是彼此最近的两个点之间的距离。
- en: The *maximum distance* between clusters is the distance between the two points
    that are furthest away from each other.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇之间的*最大距离*是彼此距离最远的两个点之间的距离。
- en: 'The *centroid distance* between clusters is the distance between the centroids
    of each cluster, where the centroid is defined as the average of all data points
    in a given cluster:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 簇之间的*质心距离*是每个簇质心之间的距离，其中质心定义为给定簇中所有数据点的平均值：
- en: '![](img/f379f7d2-1234-405f-8932-d9bcbccef7bd.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f379f7d2-1234-405f-8932-d9bcbccef7bd.png)'
- en: 'Figure 5.1: Cluster distance measures'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：簇距离度量
- en: Hierarchical clustering
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: In hierarchical clustering, each data point starts off in its own self-defined
    cluster—for example, if you have 10 data points in your dataset, then there will
    initially be 10 clusters. The two *nearest* clusters, as defined by the Euclidean
    centroid distance, for example, are then combined. This process is then repeated
    for all distinct clusters until eventually all data points belong in the same
    cluster.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在层次聚类中，每个数据点最初都位于其自己定义的簇中——例如，如果您的数据集中有10个数据点，那么最初将有10个簇。然后，根据欧几里得重心距离定义的*最近*的两个簇将被合并。然后，对所有不同的簇重复此过程，直到最终所有数据点都属于同一个簇。
- en: 'This process can be visualized using a **dendrogram**, as illustrated in *Figure
    5.2*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用树状图来可视化此过程，如图*图5.2*所示：
- en: '![](img/66d40dfe-b419-4837-8425-c7650889885f.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/66d40dfe-b419-4837-8425-c7650889885f.png)'
- en: 'Figure 5.2: Hierarchical clustering dendrogram'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2：层次聚类树状图
- en: A dendrogram helps us to decide when to stop the hierarchical clustering process.
    It is generated by plotting the original data points on the *x *axis and the distance
    between clusters on the *y *axis. As new parent clusters are created, by combining
    the nearest clusters together, a horizontal line is plotted between those child
    clusters. Eventually, the dendrogram ends when all data points belong in the same
    cluster. The aim of the dendrogram is to tell us when to stop the hierarchical
    clustering process. We can deduce this by drawing a dashed horizontal line across
    the dendrogram, placed at a position that maximizes the vertical distance between
    this dashed horizontal line and the next horizontal line (up or down). The final
    number of clusters at which to stop the hierarchical clustering process is then
    the number of vertical lines the dashed horizontal line intersects. In *Figure
    5.2*, we would end up with two clusters containing the data points {5, 2, 7} and
    {8, 4, 10, 6, 1, 3, 9} respectively. However, make sure that the final number
    of clusters makes sense in the context of your use case.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图帮助我们决定何时停止层次聚类过程。它是通过在*x*轴上绘制原始数据点以及在*y*轴上绘制簇之间的距离来生成的。随着新的父簇通过合并最近的簇而创建，在这些子簇之间绘制一条水平线。最终，当所有数据点都属于同一个簇时，树状图结束。树状图的目标是告诉我们何时停止层次聚类过程。我们可以通过在树状图上画一条虚线，放置在最大化虚线与下一水平线（向上或向下）之间垂直距离的位置来推断这一点。然后，停止层次聚类过程的最终簇数就是虚线与垂直线相交的数量。在*图5.2*中，我们将得到包含数据点{5,
    2, 7}和{8, 4, 10, 6, 1, 3, 9}的两个簇。然而，请确保最终簇数在您的用例上下文中是有意义的。
- en: K-means clustering
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-均值聚类
- en: 'In k-means clustering, a different process is followed in order to segment
    data points into clusters. First, the final number of clusters, *k*, must be defined
    upfront based on the context of your use case. Once defined, each data point is
    randomly assigned to one of these *k* clusters, after which the following process
    is employed:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-均值聚类中，遵循不同的过程将数据点分割成簇。首先，必须根据用例上下文预先定义最终簇数*k*。一旦定义，每个数据点将被随机分配到这些*k*个簇中的一个，之后采用以下过程：
- en: The centroid of each cluster is computed
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算每个簇的重心
- en: Data points are then reassigned to those clusters that have the closest centroid
    to them
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将数据点重新分配到与它们最近的簇中
- en: The centroids of all clusters are then recomputed
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后重新计算所有簇的重心
- en: Data points are then reassigned once more
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后将数据点再次重新分配
- en: This process is repeated until no data points can be reassigned—that is, until
    there are no further improvements to be had and all data points belong to a cluster
    that has the closest centroid to them. Therefore, since the centroid of a cluster
    is defined as the mean average of all data points in a given cluster, k-means
    clustering effectively partitions the data points into *k* clusters with each
    data point assigned to a cluster with a mean average that is closest to it.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程重复进行，直到无法再重新分配数据点——也就是说，直到没有进一步的改进空间，并且所有数据点都属于一个与它们最近的簇。因此，由于簇的重心定义为给定簇中所有数据点的平均平均值，k-均值聚类实际上将数据点划分为*k*个簇，每个数据点分配到与其平均平均值最接近的簇中。
- en: Note that in both clustering processes (hierarchical and k-means), a measure
    of distance needs to be computed. However, distance scales differently based on
    the type and units of the independent variables involved—for example, height and
    weight. Therefore, it is important to normalize your data first (sometimes called
    feature scaling) before training a clustering model so that it works properly.
    To learn more about normalization, please visit [https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在两种聚类过程（层次聚类和k-means）中，都需要计算距离度量。然而，距离度量根据涉及的独立变量的类型和单位而有所不同——例如，身高和体重。因此，在训练聚类模型之前，首先对数据进行归一化（有时称为特征缩放）是很重要的，以确保其正常工作。要了解更多关于归一化的信息，请访问[https://en.wikipedia.org/wiki/Feature_scaling](https://en.wikipedia.org/wiki/Feature_scaling)。
- en: Case study – detecting brain tumors
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究——检测脑肿瘤
- en: 'Let''s apply k-means clustering to a very important real-world use case: detecting
    brain tumors from **magnetic resonance imaging** (**MRI**) scans. MRI scans are
    used across the world to generate detailed images of the human body, and can be
    used for a wide range of medical applications, from detecting cancerous cells
    to measuring blood flow. In this case study, we will use grayscale MRI scans of
    a healthy human brain as the input for a k-means clustering model. We will then
    apply the trained k-means clustering model to an MRI scan of another human brain
    to see if we can detect suspicious growths and tumors.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将k-means聚类应用于一个非常重要的实际应用案例：从**磁共振成像**（**MRI**）扫描中检测脑肿瘤。MRI扫描在全球范围内用于生成人体详细图像，可用于广泛的医疗应用，从检测癌细胞到测量血流。在本案例研究中，我们将使用健康人脑的灰度MRI扫描作为k-means聚类模型的输入。然后，我们将应用训练好的k-means聚类模型到另一人脑的MRI扫描中，以查看我们是否可以检测到可疑的生长物和肿瘤。
- en: Note that the images we will use in this case study are relatively simple, in
    that any suspicious growths that are present will be visible to the naked eye.
    The fundamental purpose of this case study is to show how Python may be used to
    manipulate images, and how `MLlib` may be used to natively train k-means clustering
    models via its k-means estimator.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在本案例研究中我们将使用的图像相对简单，因为任何存在的可疑生长物都可通过肉眼看到。本案例研究的基本目的是展示如何使用Python来操作图像，以及如何通过其k-means估计器原生地使用`MLlib`训练k-means聚类模型。
- en: Feature vectors from images
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像特征向量
- en: 'The first challenge for us is to convert images into numerical feature vectors
    in order to train our k-means clustering model. In our case, we will be using
    grayscale MRI scans. A grayscale image in general can be thought of as a matrix
    of pixel-intensity values between 0 (black) and 1 (white), as illustrated in *Figure
    5.3*:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们面临的第一挑战是将图像转换为数值特征向量，以便训练我们的k-means聚类模型。在我们的案例中，我们将使用灰度MRI扫描。一般来说，灰度图像可以被视为像素强度值（介于0（黑色）和1（白色）之间）的矩阵，如图*图5.3*所示：
- en: '![](img/f66f2887-1962-4aad-8f70-9dd144bde421.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f66f2887-1962-4aad-8f70-9dd144bde421.png)'
- en: 'Figure 5.3: Grayscale image mapped to a matrix of pixel-intensity values'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3：灰度图像映射到像素强度值矩阵
- en: The dimensions of the resulting matrix is equal to the height (*m*) and width
    (*n*) of the original image in pixels. The input into our k-means clustering model
    will therefore be (*m* x *n*) observations across one independent variable—the
    pixel-intensity value. This can subsequently be represented as a single vector
    containing (*m* x *n*) numerical elements—that is, (0.0, 0.0, 0.0, 0.2, 0.3, 0.4,
    0.3, 0.4, 0.5 …).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵的维度等于原始图像的像素高度（*m*）和宽度（*n*）。因此，进入我们k-means聚类模型的将是关于一个独立变量——像素强度值的（*m* x
    *n*）个观察值。这可以随后表示为一个包含（*m* x *n*）个数值元素的单一向量——即（0.0，0.0，0.0，0.2，0.3，0.4，0.3，0.4，0.5……）。
- en: Image segmentation
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图像分割
- en: Now that we have derived feature vectors from our grayscale MRI image, our k-means
    clustering model will assign each pixel-intensity value to one of the *k* clusters
    when we train it on our MRI scan of a healthy human brain. In the context of the
    real world, these *k* clusters represent different substances in the brain, such
    as grey matter, white matter, fatty tissue, and cerebral fluids, which our model
    will partition based on color, a process called image segmentation. Once we have
    trained our k-means clustering model on a healthy human brain and identified *k*
    distinct clusters, we can then apply those defined clusters to MRI brain scans
    of other patients in an attempt to identify the presence and volume of suspicious
    growths.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从我们的灰度MRI图像中导出了特征向量，当我们在健康人脑的MRI扫描上训练我们的k-means聚类模型时，它将把每个像素强度值分配给*k*个聚类中的一个。在现实世界的背景下，这些*k*个聚类代表了大脑中的不同物质，如灰质、白质、脂肪组织和脑脊液，我们的模型将根据颜色将它们分割，这个过程称为图像分割。一旦我们在健康人脑上训练了我们的k-means聚类模型并识别了*k*个不同的聚类，我们就可以将这些定义好的聚类应用于其他患者的MRI脑扫描，以尝试识别可疑生长物的存在和体积。
- en: K-means cost function
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means成本函数
- en: One of the challenges when using the k-means clustering algorithm is how to
    choose a suitable value for *k* upfront, especially if it is not obvious from
    the wider context of the use case in question. One method to help us is to plot
    a range of possible values of *k* on the *x *axis against the output of the k-means
    cost function on the *y *axis. The k-means cost function computes the total sum
    of the squared distance of every point to its corresponding cluster centroid for
    that value of *k*. The goal is to choose a suitable value of *k* that minimizes
    the cost function, but that is not so large that it increases the computational
    complexity of generating the clusters with only a small return in the reduction
    in cost. We will demonstrate how to generate this plot, and hence choose a suitable
    value of *k*, when we develop our Spark application for image segmentation in
    the next subsection.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k-means聚类算法时面临的挑战之一是如何提前选择一个合适的*k*值，尤其是如果它不明显地来自所讨论用例的更广泛背景。帮助我们的一种方法是，在*x*轴上绘制一系列可能的*k*值，与*y*轴上k-means成本函数的输出相对应。k-means成本函数计算每个点到其对应聚类质心的平方距离的总和。目标是选择一个合适的*k*值，以最小化成本函数，但不要太大，以免增加生成聚类时的计算复杂性，而成本降低的回报却很小。当我们在下一小节开发用于图像分割的Spark应用程序时，我们将展示如何生成此图，从而选择一个合适的*k*值。
- en: K-means clustering in Apache Spark
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的K-means聚类
- en: The MRI brain scans that we will use for our k-means clustering model have been
    downloaded from **The Cancer Imaging Archive** (**TCIA**), a service that anonymizes
    and hosts a large archive of medical images of cancer for public download, and
    that may be found at [http://www.cancerimagingarchive.net/](http://www.cancerimagingarchive.net/).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于我们的k-means聚类模型的MRI脑部扫描已从**癌症影像档案库**（**TCIA**）下载，这是一个匿名化和托管大量癌症医学图像档案以供公共下载的服务，您可以在[http://www.cancerimagingarchive.net/](http://www.cancerimagingarchive.net/)找到。
- en: 'The MRI scan of our healthy human brain may be found in the GitHub repository
    accompanying this book, and is called `mri-images-data`/`mri-healthy-brain.png`.
    The MRI scan of the test human brain is called `mri-images-data`/`mri-test-brain.png`.
    We will use both in the following Spark application when training our k-means
    clustering model and applying it to image segmentation. Let''s begin:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们健康人脑的MRI扫描可以在伴随本书的GitHub仓库中找到，称为`mri-images-data`/`mri-healthy-brain.png`。测试人脑的MRI扫描称为`mri-images-data`/`mri-test-brain.png`。在以下Spark应用程序中，当我们在健康人脑的MRI扫描上训练k-means聚类模型并将其应用于图像分割时，我们将使用这两个。让我们开始：
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp05-01-kmeans-clustering.ipynb`.
    It can be found in the GitHub repository accompanying this book.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 以下小节将描述对应于本用例的Jupyter笔记本中相关的每个单元格，该笔记本称为`chp05-01-kmeans-clustering.ipynb`。它可以在伴随本书的GitHub仓库中找到。
- en: 'Let''s open the grayscale MRI scan of the healthy human brain and take a look
    at it! We can achieve this using the `scikit-learn` machine learning library for
    Python as follows:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们打开健康人脑的灰度MRI扫描并查看它！我们可以使用Python的`scikit-learn`机器学习库来实现这一点：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The rendered image is illustrated in *Figure 5.4*:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 渲染的图像如图5.4所示：
- en: '![](img/641d2583-8db8-43a3-a049-0d373649e56e.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/641d2583-8db8-43a3-a049-0d373649e56e.png)'
- en: 'Figure 5.4: Original MRI scan rendered using scikit-learn and matplotlib'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：使用scikit-learn和matplotlib渲染的原始MRI扫描
- en: 'We now need to turn this image into a matrix of decimal point pixel-intensity
    values between 0 and 1\. Conveniently, this function is provided out of the box
    by `scikit-learn` using the `img_as_float` method, as shown in the following code.
    The dimensions of the resulting matrix are 256 x 256, implying an original image
    of 256 x 256 pixels:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在需要将这个图像转换为一个介于0和1之间的十进制点像素强度值的矩阵。方便的是，这个函数由`scikit-learn`提供的`img_as_float`方法直接提供，如下面的代码所示。结果矩阵的维度是256
    x 256，这意味着原始图像是256 x 256像素：
- en: '[PRE1]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we need to flatten this matrix into a single vector of 256 x 256 elements,
    where each element represents a pixel-intensity value. This can be thought of
    as another matrix of dimensions 1 x (256 x 256) = 1 x 65536\. We can achieve this
    using the `numpy` Python library. First, we convert our original 256 x 256 matrix
    into a 2-dimensional `numpy` array. We then use `numpy`''s `ravel()` method to
    flatten this 2-dimensional array into a 1-dimensional array. Finally, we represent
    this 1-dimensional array as a specialized array, or matrix, of dimensions 1 x
    65536 using the `np.matrix` command, as follows:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要将这个矩阵展平成一个256 x 256元素的单一向量，其中每个元素代表一个像素强度值。这可以被视为一个维度为1 x (256 x 256)
    = 1 x 65536的另一个矩阵。我们可以使用`numpy` Python库来实现这一点。首先，我们将原始的256 x 256矩阵转换为二维`numpy`数组。然后，我们使用`numpy`的`ravel()`方法将这个二维数组展平成一维数组。最后，我们使用`np.matrix`命令将这个一维数组表示为一个维度为1
    x 65536的特殊数组或矩阵，如下所示：
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we have our single vector, represented as a matrix of 1 x 65536 in
    dimension, we need to convert it into a Spark dataframe. To achieve this, we firstly
    transpose the matrix using numpy''s `reshape()` method so that it is 65536 x 1\.
    We then use the `createDataFrame()` method, exposed by Spark''s SQLContext, to
    create a Spark dataframe containing 65536 observations/rows and 1 column, representing
    65536 pixel-intensity values, as shown in the following code:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经得到了单个向量，表示为1 x 65536维度的矩阵，我们需要将其转换为Spark数据框。为了实现这一点，我们首先使用numpy的`reshape()`方法转置矩阵，使其变为65536
    x 1。然后，我们使用Spark的SQLContext公开的`createDataFrame()`方法创建一个包含65536个观测值/行和1列的Spark数据框，代表65536个像素强度值，如下面的代码所示：
- en: '[PRE3]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We are now ready to generate `MLlib` feature vectors using `VectorAssembler`,
    a method that we have seen before. The `feature_columns` for `VectorAssembler`
    will simply be the sole pixel-intensity column from our Spark dataframe. The output
    of applying `VectorAssembler` to our Spark dataframe via the `transform()` method
    will be a new Spark dataframe called `mri_healthy_brain_features_df`, containing
    our 65536 `MLlib` feature vectors, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用`VectorAssembler`生成`MLlib`特征向量，这是一个我们之前见过的方法。`VectorAssembler`的`feature_columns`将简单地是我们Spark数据框中唯一的像素强度列。通过`transform()`方法将`VectorAssembler`应用于我们的Spark数据框的输出将是一个新的Spark数据框，称为`mri_healthy_brain_features_df`，包含我们的65536个`MLlib`特征向量，如下所示：
- en: '[PRE4]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can now compute and plot the output of the k-means cost function for a range
    of *k* in order to determine the best value of *k* for this use case. We achieve
    this by using `MLlib`''s `KMeans()` estimator in the Spark dataframe containing
    our feature vectors, iterating over values of `k` in the `range(2, 20)`. We can
    then plot this using the `matplotlib` Python library, as shown in the following
    code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以计算并绘制k-means成本函数的输出，以确定此用例的最佳*k*值。我们通过在Spark数据框中使用`MLlib`的`KMeans()`估计器，遍历`range(2,
    20)`中的*k*值来实现这一点。然后，我们可以使用`matplotlib` Python库来绘制这个图表，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Based on the resulting plot, as illustrated in *Figure 5.5*, a value of *k* of
    either 5 or 6 would seem to be ideal. At these values, the k-means cost is minimized with
    little return gained thereafter, as shown in the following graph:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结果图，如图*图5.5*所示，*k*的值为5或6似乎是最理想的。在这些值下，k-means成本最小化，之后获得的回报很少，如下面的图表所示：
- en: '![](img/4ac60c7e-4c77-4411-b9b6-b4910e063ea3.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4ac60c7e-4c77-4411-b9b6-b4910e063ea3.png)'
- en: 'Figure 5.5: K-means cost function'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：K-means成本函数
- en: 'We are now ready to train our k-means clustering model! Again, we will use
    `MLlib`''s `KMeans()` estimator, but this time using a defined value for *k* (5,
    in our case, as we decided in step 6). We will then apply it, via the `fit()`
    method, to the Spark dataframe containing our feature vectors and study the centroid
    values for each of our 5 resulting clusters, as follows:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在准备训练我们的k-means聚类模型！再次，我们将使用`MLlib`的`KMeans()`估计器，但这次我们将使用定义的*k*值（在我们的例子中是5，因为我们已在第6步中决定）。然后，我们将通过`fit()`方法将其应用于包含我们的特征向量的Spark数据框，并研究我们5个结果簇的质心值，如下所示：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Next, we will apply our trained k-means model to the Spark dataframe containing
    our feature vectors so that we may assign each of the 65536 pixel-intensity values
    to one of the five clusters. The result will be a new Spark dataframe containing
    our feature vectors mapped to a prediction, where in this case the prediction
    is simply a value between 0 and 4, representing one of the five clusters. Then,
    we convert this new dataframe into a 256 x 256 matrix so that we can visualize the
    segmented image, as follows:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的训练好的k-means模型应用于包含我们的特征向量的Spark数据框，以便将每个65536个像素强度值分配到五个簇中的一个。结果将是一个新的Spark数据框，其中包含我们的特征向量映射到预测，在这种情况下，预测是一个介于0到4之间的值，代表五个簇中的一个。然后，我们将这个新的数据框转换为256
    x 256矩阵，以便我们可以可视化分割图像，如下所示：
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting segmented image, rendered using `matplotlib`, is illustrated
    in *Figure 5.6*:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`matplotlib`渲染的分割图像结果如图5.6所示：
- en: '![](img/6cf03ae5-c26a-4983-84cf-70d8e7ce9315.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6cf03ae5-c26a-4983-84cf-70d8e7ce9315.png)'
- en: 'Figure 5.6: Segmented MRI scan'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：分割MRI扫描
- en: 'Now that we have our five defined clusters, we can apply our trained k-means
    model to a *new* image in order to segment it, also based on the same five clusters.
    First, we load the new grayscale MRI brain scan belonging to the test patient
    using the `scikit-learn` library, as we did before using the following code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经定义了五个簇，我们可以将我们的训练好的k-means模型应用于一张*新*图像以进行分割，也是基于相同的五个簇。首先，我们使用`scikit-learn`库加载属于测试患者的新灰度MRI脑扫描，就像我们之前使用以下代码做的那样：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Once we have loaded the new MRI brain scan image, we need to follow the same
    process to convert it into a Spark dataframe containing feature vectors representing
    the pixel-intensity values of the new test image. We then apply the trained k-means
    model, via the `transform()` method, to this test Spark dataframe in order to
    assign its pixels to one of the five clusters. Finally, we convert the Spark dataframe
    containing the test image predictions in to a matrix so that we can visualize
    the segmented test image, as follows:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦我们加载了新的MRI脑扫描图像，我们需要遵循相同的过程将其转换为包含代表新测试图像像素强度值的特征向量的Spark数据框。然后，我们将训练好的k-means模型通过`transform()`方法应用于这个测试Spark数据框，以便将它的像素分配到五个簇中的一个。最后，我们将包含测试图像预测的Spark数据框转换为矩阵，以便我们可以可视化分割后的测试图像，如下所示：
- en: '[PRE9]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The resulting segmented image belonging to the test patient, again rendered
    using `matplotlib`, is illustrated in *Figure 5.7*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`matplotlib`再次渲染的属于测试患者的分割图像如图5.7所示：
- en: '![](img/8354900b-5a2b-47cd-a4b9-9518b991bd69.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8354900b-5a2b-47cd-a4b9-9518b991bd69.png)'
- en: 'Figure 5.7: Segmented MRI scan belonging to the test patient'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7：测试患者的分割MRI扫描
- en: 'If we compare the two segmented images side by side (as illustrated in *Figure
    5.8*), we will see that, as a result of our k-means clustering model, five different
    colors have been rendered representing the five different clusters. In turn, these
    five different clusters represent different substances in the human brain, partitioned
    by color. We will also see that, in the test MRI brain scan, one of the colors
    takes up a substantially larger area compared to the healthy MRI brain scan, pointing
    to a suspicious growth that may potentially be a tumor requiring further analysis,
    as shown in the following image:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将两个分割图像并排比较（如图5.8所示），我们将看到，由于我们的k-means聚类模型，已经渲染了五种不同的颜色，代表五个不同的簇。反过来，这五个不同的簇代表大脑中的不同物质，通过颜色进行分区。我们还将看到，在测试MRI脑扫描中，其中一种颜色相对于健康MRI脑扫描占据了一个显著更大的区域，这表明可能是一个需要进一步分析的肿瘤，如图中所示：
- en: '![](img/f612788f-acd9-49f0-969c-dcc79a3844d8.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f612788f-acd9-49f0-969c-dcc79a3844d8.png)'
- en: 'Figure 5.8: Comparison of segmented MRI scans'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8：分割MRI扫描的比较
- en: Principal component analysis
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 主成分分析
- en: There are numerous real-world use cases where the number of features available
    that may potentially be used to train a model is very large. A common example
    is economic data, and using its constituent stock price data, employment data,
    banking data, industrial data, and housing data together to predict the **gross
    domestic product** (**GDP**). Such types of data are said to have high dimensionality.
    Though they offer numerous features that can be used to model a given use case,
    high-dimensional datasets increase the computational complexity of machine learning
    algorithms, and more importantly may also result in over fitting. Over fitting
    is one of the results of the **curse of dimensionality**, which formally describes
    the problem of analyzing data in high-dimensional spaces (which means that the
    data may contain many attributes, typically hundreds or even thousands of dimensions/features),
    but where that analysis no longer holds true in a lower-dimensional space.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多现实世界的用例中，可用于训练模型的特征数量可能非常大。一个常见的例子是经济数据，使用其构成成分的股票价格数据、就业数据、银行数据、工业数据和住房数据一起预测**国内生产总值**（GDP）。这类数据被称为具有高维性。虽然它们提供了可用于建模的许多特征，但高维数据集增加了机器学习算法的计算复杂性，更重要的是，还可能导致过拟合。过拟合是**维度诅咒**的结果之一，它正式描述了在高度空间（意味着数据可能包含许多属性，通常是数百甚至数千个维度/特征）中分析数据的问题，但在低维空间中，这种分析不再成立。
- en: Informally, it describes the value of additional dimensions at the cost of model
    performance. **Principal component analysis** (**PCA**)is an *unsupervised* technique
    used to preprocess and reduce the dimensionality of high-dimensional datasets
    while preserving the original structure and relationships inherent to the original
    dataset so that machine learning models can still learn from them and be used
    to make accurate predictions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 非正式地说，它描述了以模型性能为代价增加额外维度的价值。**主成分分析**（PCA）是一种**无监督**技术，用于预处理和降低高维数据集的维度，同时保留原始数据集固有的原始结构和关系，以便机器学习模型仍然可以从它们中学习并用于做出准确的预测。
- en: Case study – movie recommendation system
  id: totrans-86
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 案例研究 – 电影推荐系统
- en: To better understand PCA, let's study a movie recommendation use case. Our aim
    is to build a system that can make personalized movie recommendations to users
    based on historic user-community movie ratings (note that user viewing history
    data could also be used for such a system, but this is beyond the scope of this
    example).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解主成分分析（PCA），让我们研究一个电影推荐用例。我们的目标是构建一个系统，该系统能够根据历史用户社区电影评分（请注意，用户观看历史数据也可以用于此类系统，但这超出了本例的范围）为用户提供个性化的电影推荐。
- en: The historic user-community movie ratings data that we will use for our case
    study has been downloaded from GroupLens, a research laboratory based at the University
    of Minnesota that collects movie ratings and makes them available for public download
    at [https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/).
    For the purposes of this case study, we have transformed the individual *movies*
    and *ratings* datasets into a single pivot table where the 300 rows represent
    300 different users, and the 3,000 columns represent 3,000 different movies. This
    transformed, pipe-delimited dataset can be found in the GitHub repository accompanying
    this book, and is called `movie-ratings-data/user-movie-ratings.csv`.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于案例研究的用户社区电影评分历史数据已从明尼苏达大学的GroupLens研究实验室下载，该实验室收集电影评分并将其公开发布在[https://grouplens.org/datasets/movielens/](https://grouplens.org/datasets/movielens/)。为了本案例研究的目的，我们将单个*电影*和*评分*数据集转换为一个单一的交叉表，其中300行代表300个不同的用户，而3000列代表3000部不同的电影。这个转换后的、管道分隔的数据集可以在本书附带的GitHub仓库中找到，并称为`movie-ratings-data/user-movie-ratings.csv`。
- en: 'A sample of the historic user-community movie ratings dataset that we will
    study looks as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要研究的用户社区电影评分历史数据样本如下：
- en: '|  | **Movie #1****Toy Story** | **Movie #2****Monsters Inc.** | **Movie #3****Saw**
    | **Movie #4****Ring** | **Movie #5****Hitch** |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '|  | **电影 #1**Toy Story | **电影 #2**Monsters Inc. | **电影 #3**Saw | **电影 #4**Ring
    | **电影 #5**Hitch |'
- en: '| **User #1** | 4 | 5 | 1 | NULL | 4 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| **用户 #1** | 4 | 5 | 1 | NULL | 4 |'
- en: '| **User #2** | 5 | NULL | 1 | 1 | NULL |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **用户 #2** | 5 | NULL | 1 | 1 | NULL |'
- en: '| **User #3** | 5 | 4 | 3 | NULL | 3 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **用户 #3** | 5 | 4 | 3 | NULL | 3 |'
- en: '| **User #4** | 5 | 4 | 1 | 1 | NULL |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **用户 #4** | 5 | 4 | 1 | 1 | NULL |'
- en: '| **User #5** | 5 | 5 | NULL | NULL | 3 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| **用户 #5** | 5 | 5 | NULL | NULL | 3 |'
- en: In this case, each movie is a different feature (or dimension), and each different
    user is a different instance (or observation). This sample table, therefore, represents
    a dataset containing 5 features. However, our actual dataset contains 3,000 different
    movies, and therefore 3,000 features/dimensions. Furthermore, in a real-life representation,
    not all users would have rated all the movies, and so there will be a significant
    number of missing values. Such a dataset, and the matrix used to represent it,
    is described as *sparse*. These issues would pose a problem for machine learning
    algorithms, both in terms of computational complexity and the likelihood of over
    fitting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，每部电影是一个不同的特征（或维度），每个不同的用户是一个不同的实例（或观察）。因此，这个样本表代表了一个包含5个特征的数据库。然而，我们的实际数据集包含3,000部电影的不同，因此有3,000个特征/维度。此外，在现实生活中的表示中，并非所有用户都会对所有电影进行评分，因此将会有大量的缺失值。这样的数据集，以及用来表示它的矩阵，被称为**稀疏的**。这些问题会给机器学习算法带来问题，无论是在计算复杂性还是在过拟合的可能性方面。
- en: 'To solve this problem, take a closer look at the previous sample table. It
    seems that users that rated Movie #1 highly (Toy Story) generally also rated Movie
    #2 highly (Monsters Inc.) as well. We could say, for example, that User #1 is
    *representative* of all fans of computer-animated children''s films, and so we
    could recommend to User #2 the other movies that User #1 has historically rated
    highly (this type of recommendation system where we use data from other users
    is called **collaborative filtering**). At a high level, this is what PCA does—it
    identifies *typical representations*, called **principal components**, within
    a high-dimensional dataset so that the dimensions of the original dataset can
    be reduced while preserving its underlying structure and still be representative
    in *lower* dimensions! These reduced datasets can then be fed into machine learning
    models to make predictions as normal, without the fear of any adverse effects
    from reducing the raw size of the original dataset. Our formal definition of PCA
    can therefore now be extended so that we can define PCA as the identification
    of a linear subspace of lower dimensionality where the largest variance in the
    original dataset is maintained.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，仔细观察之前的样本表。似乎评分很高的用户（对电影#1评分很高的是《玩具总动员》）通常也对电影#2（《怪物公司》）给出了很高的评分。例如，我们可以说，用户#1是所有电脑动画儿童电影爱好者的**代表**，因此我们可以向用户#2推荐用户#1历史上评分很高的其他电影（这种使用其他用户数据的推荐系统称为**协同过滤**）。从高层次来看，这就是PCA所做的——它在高维数据集中识别**典型表示**，称为**主成分**，以便在保留其潜在结构和在**低维**中仍然具有代表性的同时，减少原始数据集的维度！然后，这些减少的数据集可以被输入到机器学习模型中进行预测，就像正常一样，而不必担心减少原始数据集的原始大小所带来的任何不利影响。因此，我们可以将PCA的正式定义现在扩展，以便我们可以将PCA定义为识别一个低维线性子空间，其中原始数据集的最大方差得到保持。
- en: 'Returning to our historic user-community movie ratings dataset, instead of
    eliminating Movie #2 entirely, we could seek to create a new feature that combines
    Movie #1 and Movie #2 in some manner. Extending this concept, we can create new
    features where each new feature is based on all the old features, and thereafter
    order these new features by how well they help us in predicting user movie ratings.
    Once ordered, we can drop the least important ones, thereby resulting in a reduction
    in dimensionality. So how does PCA achieve this? It does so by performing the
    following steps:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们历史用户社区电影评分数据集，我们不是完全消除电影#2，而是试图创建一个新特征，该特征以某种方式结合了电影#1和电影#2。扩展这个概念，我们可以创建新的特征，其中每个新特征都是基于所有旧特征，然后根据这些新特征在预测用户电影评分方面的帮助程度对这些新特征进行排序。一旦排序，我们可以删除最不重要的那些，从而实现降维。那么PCA是如何实现这一点的呢？它是通过以下步骤实现的：
- en: First, we standardize the original high-dimensional dataset.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们对原始高维数据集进行标准化。
- en: Next, we take the standardized data and compute a covariance matrix that provides
    a means to measure how all our features relate to each other.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们取标准化的数据并计算一个协方差矩阵，该矩阵提供了一种衡量所有特征之间相互关系的方法。
- en: After computing the covariance matrix, we then find its *eigenvectors* and corresponding
    *eigenvalues*. Eigenvectors represent the principal components and provide a means
    to understand the direction of the data. Corresponding eigenvalues represent how
    much variance there is in the data in that direction.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在计算协方差矩阵之后，我们然后找到其**特征向量**和相应的**特征值**。特征向量代表主成分，提供了一种理解数据方向的方法。相应的特征值代表在该方向上数据中有多少方差。
- en: The eigenvectors are then sorted in descending order based on their corresponding
    eigenvalues, after which the top *k* eigenvectors are selected representing the
    most important representations found in the data.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将特征向量根据其对应的特征值降序排列，之后选择前*k*个特征向量，代表数据中找到的最重要表示。
- en: A new matrix is then constructed with these *k* eigenvectors, thereby reducing
    the original *n*-dimensional dataset into reduced *k* dimensions.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用这些*k*个特征向量构建一个新的矩阵，从而将原始的*n*维数据集减少到减少的*k*维。
- en: Covariance matrix
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 协方差矩阵
- en: 'In mathematics, **variance** refers to a measure of how spread out a dataset
    is, and is calculated by the sum of the squared distances of each data point, *x[i]*,
    from the mean *x-bar*, divided by the total number of data points, *N*. This is
    represented by the following formula:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学中，**方差**是指数据集分散程度的度量，它是每个数据点，*x[i]*，与均值*x-bar*的平方距离之和除以数据点的总数，*N*。这可以用以下公式表示：
- en: '![](img/787eabe6-db53-4369-9e63-43761b34b1d8.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/787eabe6-db53-4369-9e63-43761b34b1d8.png)'
- en: '**Covariance** refers to a measure of how strong the correlation between two
    or more random variables is (in our case, our independent variables), and is calculated
    for variables *x* and *y* over *i* dimensions, as follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '**协方差**是指两个或多个随机变量（在我们的情况下，是独立变量）之间相关性强弱的度量，它是通过*i*维度的变量*x*和*y*计算的，如下所示：'
- en: '![](img/21109685-3428-485a-8bfc-2c75ae5a0a1d.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/21109685-3428-485a-8bfc-2c75ae5a0a1d.png)'
- en: If the covariance is positive, this implies that the independent variables are
    positively correlated. If the covariance is negative, this implies that the independent
    variables are negatively correlated. Finally, a covariance of zero implies that
    there is no correlation between the independent variables. You may note that we
    described correlation in [Chapter 4](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml), *Supervised
    Learning Using Apache Spark*, when discussing multivariate linear regression.
    At that time, we computed the one-way covariance mapping between the dependent
    variable to all its independent variables. Now we are computing the covariance
    between all variables.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果协方差是正的，这表明独立变量之间是正相关。如果协方差是负的，这表明独立变量之间是负相关。最后，协方差为零意味着独立变量之间没有相关性。您可能会注意到，我们在[第4章](ea16659b-adfc-4cab-8e71-99c155b07a46.xhtml)，*使用Apache
    Spark进行监督学习*中讨论多元线性回归时描述了相关性。当时，我们计算了因变量与其所有独立变量之间的单向协方差映射。现在我们正在计算所有变量之间的协方差。
- en: A **covariance matrix** is a symmetric square matrix where the general element
    (*i*, *j*) is the covariance, *cov(i, j)*, between independent variables *i* and
    *j* (which is the same as the symmetric covariance between *j* and *i*). Note
    that the diagonal in a covariance matrix actually represents just the *variance*
    between those elements, by definition.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**协方差矩阵**是一个对称的方阵，其中一般元素(*i*, *j*)是独立变量*i*和*j*之间的协方差，cov(i, j)（这与*j*和*i*之间的对称协方差相同）。请注意，协方差矩阵中的对角线实际上代表的是那些元素之间的*方差*，根据定义。'
- en: 'The covariance matrix is shown in the following table:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 协方差矩阵如下表所示：
- en: '|  | **x** | **y** | **z** |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '|  | **x** | **y** | **z** |'
- en: '| **x** | var(x) | cov(x, y) | cov(x, z) |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **x** | var(x) | cov(x, y) | cov(x, z) |'
- en: '| **y** | cov(y, x) | var(y) | cov(y, z) |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| **y** | cov(y, x) | var(y) | cov(y, z) |'
- en: '| **z** | cov(z, x) | cov(z, y) | var(z) |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| **z** | cov(z, x) | cov(z, y) | var(z) |'
- en: Identity matrix
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 单位矩阵
- en: 'An identity matrix is a square matrix in which all the elements along the main
    diagonal are 1 and the remaining elements are 0\. Identity matrices are important
    for when we need to find all of the eigenvectors for a matrix. For example, a
    3 x 3 identity matrix looks as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 单位矩阵是一个主对角线上的所有元素都是1，其余元素都是0的方阵。单位矩阵在我们需要找到矩阵的所有特征向量时非常重要。例如，一个3x3的单位矩阵如下所示：
- en: '![](img/15e61c67-15f7-474d-b0f3-f1b47599e4bf.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15e61c67-15f7-474d-b0f3-f1b47599e4bf.png)'
- en: Eigenvectors and eigenvalues
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征向量和特征值
- en: In linear algebra, eigenvectors are a special set of vectors whose *direction*
    remains unchanged when a linear transformation is applied to it, and only changes
    by a *scalar* factor. In the context of dimensionality reduction, eigenvectors
    represent the principal components and provide a means to understand the direction
    of the data.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性代数中，特征向量是一组特殊的向量，当对其进行线性变换时，其*方向*保持不变，仅通过一个*标量*因子改变。在降维的背景下，特征向量代表主成分，并提供了一种理解数据方向的方法。
- en: 'Consider a matrix, *A*, of dimensions (*m* x *n*). We can multiply *A* by a
    vector, *x* (of dimensions *n* x 1 by definition), which results in a new vector, *b*
    (of dimensions *m* x 1), as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个维度为(*m* x *n*)的矩阵*A*。我们可以将*A*乘以一个向量*x*（根据定义，其维度为*n* x 1），这将产生一个新的向量*b*（维度为*m*
    x 1），如下所示：
- en: '![](img/1a360801-1a58-40ec-acce-35f7e62c63c7.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1a360801-1a58-40ec-acce-35f7e62c63c7.png)'
- en: In other words, ![](img/ec5d7f90-b023-49b0-bf87-81f10dce29d7.png).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，![图片](img/ec5d7f90-b023-49b0-bf87-81f10dce29d7.png)。
- en: 'However, in some cases, the resulting vector, *b*, is actually a scaled version
    of the original vector, *x*. We call this scalar factor *λ*, in which case the
    formula above can be rewritten as follows:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在某些情况下，得到的向量，*b*，实际上是原始向量，*x*的缩放版本。我们称这个标量因子为*λ*，在这种情况下，上述公式可以重写如下：
- en: '![](img/d1492866-60c5-45cf-bd49-c9612a240212.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1492866-60c5-45cf-bd49-c9612a240212.png)'
- en: We say that *λ* is an *eigenvalue* of matrix *A*, and *x* is an *eigenvector*
    associated with *λ*. In the context of dimensionality reduction, eigenvalues represent
    how much variance there is in the data in that direction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们说*λ*是矩阵*A*的*特征值*，*x*是与*λ*相关的*特征向量*。在降维的上下文中，特征值表示数据在该方向上的方差有多大。
- en: 'In order to find all the eigenvectors for a matrix, we need to solve the following
    equation for each eigenvalue, where *I* is an identity matrix with the same dimensions
    as matrix *A*:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找到一个矩阵的所有特征向量，我们需要为每个特征值解以下方程，其中*I*是与矩阵*A*相同维度的单位矩阵：
- en: '![](img/c433abbc-2457-45b1-8721-857bfe6dd7da.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c433abbc-2457-45b1-8721-857bfe6dd7da.png)'
- en: The process by which to solve this equation is beyond the scope of this book.
    However, to learn more about eigenvectors and eigenvalues, please visit [https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个方程的过程超出了本书的范围。然而，要了解更多关于特征向量和特征值的信息，请访问[https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors](https://en.wikipedia.org/wiki/Eigenvalues_and_eigenvectors)。
- en: Once all of the eigenvectors for the covariance matrix are found, these are
    then sorted in descending order by their corresponding eigenvalues. Since eigenvalues
    represent the amount of variance in the data for that direction, the first eigenvector
    in the ordered list represents the principal component that captures the most
    variance in the original variables from the original dataset, and so on. For example,
    as illustrated in *Figure 5.9*, if we were to plot a dataset with two dimensions
    or features, the first eigenvector (which will be the first principal component
    in order of importance) would represent the direction of most variation between
    the two features.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦找到了协方差矩阵的所有特征向量，这些向量将根据它们对应的特征值按降序排序。由于特征值表示数据在该方向上的方差量，排序列表中的第一个特征向量代表了从原始数据集中捕获原始变量最大方差的第一个主成分，依此类推。例如，如图*图5.9*所示，如果我们绘制一个具有两个维度或特征的数据集，第一个特征向量（将按重要性顺序成为第一个主成分）将代表两个特征之间最大变化的方向。
- en: 'The second eigenvector (the second principal component in order of importance)
    would represent the direction of second-most variation between the two features:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个特征向量（按重要性顺序的第二主成分）将代表两个特征之间第二大的变化方向：
- en: '![](img/f81a5743-0cc2-4b65-8cc9-ff48c27cf675.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f81a5743-0cc2-4b65-8cc9-ff48c27cf675.png)'
- en: 'Figure 5.9: Principal components across two dimensions'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9：两个维度上的主成分
- en: 'To help choose the number of principal components, *k*, to select from the
    top of the ordered list of eigenvectors, we can plot the number of principal components
    on the *x *axis against the cumulative explained variance on the *y *axis, as
    illustrated in *Figure 5.10*, where the explained variance is the ratio between
    the variance of that principal component and the total variance (that is, the
    sum of all eigenvalues):'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助选择主成分的数量，*k*，从特征向量排序列表的顶部选择，我们可以在*x*轴上绘制主成分的数量，与*y*轴上的累积解释方差进行对比，如图*图5.10*所示，其中解释方差是那个主成分的方差与总方差（即所有特征值的和）的比率：
- en: '![](img/8097f834-824c-4dfb-9fc0-663de2082ad7.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8097f834-824c-4dfb-9fc0-663de2082ad7.png)'
- en: 'Figure 5.10: Cumulative explained variance'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10：累积解释方差
- en: Using *Figure 5.10* as an example, we would select around the first 300 principal
    components, as these describe the most variation within the data out of the 3,000
    in total. Finally, we construct a new matrix by projecting the original dataset
    into *k*-dimensional space represented by the eigenvectors selected, thereby reducing
    the dimensionality of the original dataset from 3,000 dimensions to 300 dimensions.
    This preprocessed and reduced dataset can then be used to train machine learning
    models as normal.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以*图5.10*为例，我们会选择大约前300个主成分，因为这些描述了数据中的最大变化，总共有3,000个。最后，我们通过将原始数据集投影到由选定的特征向量表示的*k*-维空间中，从而构建一个新的矩阵，从而将原始数据集的维度从3,000维降低到300维。这个预处理和降维后的数据集可以用来训练机器学习模型，就像平常一样。
- en: PCA in Apache Spark
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark中的PCA
- en: 'Let''s now return to our transformed pipe-delimited user-community movie ratings
    dataset, `movie-ratings-data/user-movie-ratings.csv`, which contains ratings by
    300 users covering 3,000 movies. We will develop an application in Apache Spark
    that seeks to reduce the dimensionality of this dataset while preserving its structure
    using PCA. To do this, we will go through the following steps:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到我们的转换后的管道分隔的用户社区电影评分数据集，`movie-ratings-data/user-movie-ratings.csv`，它包含300个用户对3,000部电影的评价。我们将在Apache
    Spark中开发一个应用程序，旨在使用PCA（主成分分析）来降低该数据集的维度，同时保留其结构。为此，我们将执行以下步骤：
- en: The following subsections describe each of the pertinent cells in the corresponding
    Jupyter notebook for this use case, called `chp05-02-principal-component-analysis.ipynb`.
    This can be found in the GitHub repository accompanying this book.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的子节描述了对应于这个用例的Jupyter笔记本中相关的每个单元格，该笔记本称为`chp05-02-principal-component-analysis.ipynb`。这个笔记本可以在伴随这本书的GitHub仓库中找到。
- en: 'First, let''s load the transformed, pipe-delimited user-community movie ratings
    dataset into a Spark dataframe using the following code. The resulting Spark dataframe
    will have 300 rows (representing the 300 different users) and 3,001 columns (representing
    the 3,000 different movies plus the user ID column):'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们使用以下代码将转换后的管道分隔的用户社区电影评分数据集加载到Spark数据框中。生成的Spark数据框将包含300行（代表300个不同的用户）和3,001列（代表3,000部电影加上用户ID列）：
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can now generate `MLlib` feature vectors containing 3,000 elements (representing
    the 3,000 features) using `MLlib`''s `VectorAssembler`, as we have seen before.
    We can achieve this using the following code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以使用`MLlib`的`VectorAssembler`生成包含3,000个元素（代表3,000个特征）的`MLlib`特征向量，就像我们之前看到的那样。我们可以使用以下代码实现这一点：
- en: '[PRE11]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Before we can reduce the dimensionality of the dataset using PCA, we first
    need to standardize the features that we described previously. This can be achieved
    using `MLlib`''s `StandardScaler` estimator and fitting it to the Spark dataframe
    containing our feature vectors, as follows:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们能够使用PCA降低数据集的维度之前，我们首先需要标准化我们之前描述的特征。这可以通过使用`MLlib`的`StandardScaler`估计器并拟合包含我们的特征向量的Spark数据框来实现，如下所示：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we convert our scaled features into a `MLlib` `RowMatrix`instance. A
    `RowMatrix` is a distributed matrix with no index, where each row is a vector.
    We achieve this by converting our scaled features data frame into an RDD and mapping
    each row of the RDD to the corresponding scaled feature vector. We then pass this
    RDD to `MLlib`''s `RowMatrix()` (as shown in the following code), resulting in
    a matrix of standardized feature vectors of dimensions 300 x 3,000:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的缩放特征转换为`MLlib` `RowMatrix`实例。`RowMatrix`是一个没有索引的分布式矩阵，其中每一行都是一个向量。我们通过将我们的缩放特征数据帧转换为RDD，并将RDD的每一行映射到相应的缩放特征向量来实现这一点。然后，我们将这个RDD传递给`MLlib`的`RowMatrix()`（如下面的代码所示），从而得到一个300
    x 3,000维度的标准化特征向量矩阵：
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now that we have our standardized data in matrix form, we can easily compute
    the top *k* principal components by invoking the `computePrincipalComponents()`
    method exposed by `MLlib`''s `RowMatrix`. We can compute the top 300 principal
    components as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经将标准化数据以矩阵形式表示，我们可以通过调用`MLlib`的`RowMatrix`公开的`computePrincipalComponents()`方法轻松地计算前*k*个主成分。我们可以如下计算前300个主成分：
- en: '[PRE14]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now that we have identified the top 300 principal components, we can project
    the standardized user-community movie ratings data from 3,000 dimensions to a
    linear subspace of only 300 dimensions while preserving the largest variances
    from the original dataset. This is achieved by using matrix multiplication and
    multiplying the matrix containing the standardized data by the matrix containing
    the top 300 principal components, as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 既然我们已经确定了前300个主成分，我们就可以将标准化后的用户社区电影评分数据从3000维投影到仅300维的线性子空间，同时保留原始数据集的最大方差。这是通过使用矩阵乘法，并将包含标准化数据的矩阵与包含前300个主成分的矩阵相乘来实现的，如下所示：
- en: '[PRE15]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The resulting matrix now has dimensions of 300 x 300, confirming the reduction
    in dimensionality from the original 3,000 to only 300! We can now use this projected
    matrix and its PCA feature vectors as the input into subsequent machine learning
    models as normal.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果矩阵现在具有300 x 300的维度，证实了从原始的3000维到仅300维的降维！现在我们可以像平常一样使用这个投影矩阵及其PCA特征向量作为后续机器学习模型的输入。
- en: 'Alternatively, we can use `MLlib`''s `PCA()` estimator directly on the dataframe
    containing our standardized feature vectors to generate a new dataframe with a
    new column containing the PCA feature vectors, as follows:'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，我们可以直接在包含我们的标准化特征向量的数据框上使用`MLlib`的`PCA()`估计器，以生成一个新的数据框，其中包含一个新的列，包含PCA特征向量，如下所示：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Again, this new dataframe and its PCA feature vectors can then be used to train
    subsequent machine learning models as normal.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这个新的数据框及其PCA特征向量可以像平常一样用于训练后续的机器学习模型。
- en: 'Finally, we can extract the explained variance for each principal component
    from our PCA model by accessing its `explainedVariance` attribute as follows:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以通过访问其`explainedVariance`属性来从我们的PCA模型中提取每个主成分的解释方差，如下所示：
- en: '[PRE17]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The resulting vector (of 300 elements) shows that, in our example, the first
    eigenvector (and therefore the first principal component) in the ordered list
    of principal components explains 8.2% of the variance, the second explains 4%,
    and so on.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 结果向量（300个元素）显示，在我们的例子中，主成分有序列表中的第一个特征向量（因此是第一个主成分）解释了8.2%的方差，第二个解释了4%，依此类推。
- en: In this case study, we have demonstrated how we can reduce the dimensionality
    of the user-community movie ratings dataset from 3,000 dimensions to only 300
    dimensions while preserving its structure using PCA. The resulting reduced dataset
    can then be used to train machine learning models as normal, such as a hierarchical
    clustering model for collaborative filtering.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个案例研究中，我们展示了如何使用PCA将用户社区电影评分数据集的维度从3000维降低到仅300维，同时保留其结构。然后，可以像平常一样使用这个降低维度的数据集来训练机器学习模型，例如用于协同过滤的层次聚类模型。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have trained and evaluated various unsupervised machine
    learning models and techniques in Apache Spark using a variety of real-world use
    cases, including partitioning the various substances found in the human brain
    using image segmentation and helping to develop a movie recommendation system
    by reducing the dimensionality of a high-dimensional user-community movie ratings
    dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Apache Spark和多种现实世界的用例训练和评估了各种无监督机器学习模型和技术，包括使用图像分割将人类大脑中发现的多种物质进行分区，以及通过降低高维用户社区电影评分数据集的维度来帮助开发电影推荐系统。
- en: In the next chapter, we will develop, test, and evaluate some common algorithms
    that are used in **natural language processing** (**NLP**) in an attempt to train
    machines to automatically analyze and understand human text and speech!
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开发、测试和评估一些在**自然语言处理**（**NLP**）中常用的算法，试图训练机器自动分析和理解人类文本和语音！
