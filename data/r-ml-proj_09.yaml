- en: Winning the Casino Slot Machines with Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用强化学习赢得赌场老虎机
- en: 'If you have been following **machine learning** (**ML**) news, I am sure you
    will have encountered this kind of headline: *computers performing better than
    world champions in various games*. If you haven''t, the following are sample news
    snippets from my quick Google search that are worth spending time reading to understand
    the situation:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在关注**机器学习**（**ML**）新闻，我敢肯定你一定遇到过这样的标题：*计算机在多种游戏中表现优于世界冠军*。如果你还没有，以下是我快速谷歌搜索的一些新闻片段，值得花时间阅读以了解情况：
- en: 'Check this out: [https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/](https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/):'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '看看这个：[https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/](https://www.theverge.com/2017/10/18/16495548/deepmind-ai-go-alphago-zero-self-taught/):'
- en: '![](img/c22df343-7a83-4023-914e-93d75586eafa.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c22df343-7a83-4023-914e-93d75586eafa.png)'
- en: 'See this: [https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/](https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/):'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '看这个：[https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/](https://www.makeuseof.com/tag/ais-winning-5-times-computers-beat-humans/):'
- en: '![](img/b0739d12-24c3-46f6-9062-02488a93f2be.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b0739d12-24c3-46f6-9062-02488a93f2be.png)'
- en: '**Reinforcement learning** (**RL**) is a subarea of **artificial intelligence**
    (**AI**) that powers computer systems who are able to demonstrate better performance
    in games such as Atari Breakout and Go than human players.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）是**人工智能**（**AI**）的一个子领域，它使计算机系统在诸如Atari Breakout和围棋等游戏中表现出比人类玩家更好的性能。'
- en: 'In this chapter, we will look at the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨以下主题：
- en: The concept of RL
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的概念
- en: The multi-arm bandit problem
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多臂老虎机问题
- en: Methods for solving the multi-arm bandit problem
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决多臂老虎机问题的方法
- en: Real-world applications of RL
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的实际应用
- en: Implementing a project using RL techniques to maximize our chances of winning
    at a multi-arm bandit machine
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用强化学习技术实施一个项目，以最大化我们在多臂老虎机机器上获胜的机会
- en: Understanding RL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解强化学习
- en: RL is a very important area but is sometimes overlooked by practitioners for
    solving complex, real-world problems. It is unfortunate that even most ML textbooks
    focus only on supervised and unsupervised learning while totally ignorning RL.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一个非常重要的领域，但有时从业者会忽视它来解决复杂、现实世界的问题。遗憾的是，甚至大多数机器学习教科书只关注监督学习和无监督学习，而完全忽略了强化学习。
- en: RL as an area has picked up momentum in recent years; however, its origins date
    back to 1980\. It was invented by Rich Sutton and Andrew Barto, Rich's PhD thesis
    advisor. It was thought of as archaic, even back in the 1980s. Rich, however,
    believed in RL and its promise, maintaining that it would eventually be recognized.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个领域，强化学习近年来势头强劲；然而，其起源可以追溯到1980年。它是由Rich Sutton和Andrew Barto发明的，Rich的博士论文导师。即使在1980年代，它也被认为过时。然而，Rich相信强化学习和它的承诺，坚持认为它最终会被认可。
- en: A quick Google search with the term RL shows that RL methods are often used
    in games, such as checkers and chess. Gaming problems are problems that require
    taking actions over time to find a long-term optimal solution to a dynamic problem.
    They are dynamic in the sense that the conditions are constantly changing, sometimes
    in response to other agents, which can be adversarial.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RL术语进行快速谷歌搜索显示，RL方法通常用于游戏，如跳棋和国际象棋。游戏问题是需要采取行动以找到动态问题的长期最优解的问题。它们是动态的，因为条件不断变化，有时是对其他代理的响应，这可能是对抗性的。
- en: Although the success of RL is proven in the area of games, it is also an emerging
    area that is increasingly applied in other fields, such as finance, economics,
    and other inter-disciplinary areas. There are a number of methods in the RL area
    that have grown independently within the AI and operations research communities.
    Therefore, it is key area for a ML practitioners to learn about.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强化学习在游戏领域的成功得到了证明，但它也是一个新兴领域，正越来越多地应用于其他领域，如金融、经济学和其他跨学科领域。在强化学习领域有许多方法，它们在人工智能和运筹学社区中独立发展。因此，这是一个机器学习从业者需要了解的关键领域。
- en: 'In simple terms, RL is an area that mainly focuses on creating models that
    learn from mistakes. Imagine that a person is put in a new environment. At first,
    they will make mistakes, but they will learn from them, so that when the same
    situation should arise in future, they will not make the same mistake again. RL
    uses the same technique to train the model as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，强化学习是一个主要关注创建从错误中学习的模型的领域。想象一下，一个人被置于一个新环境中。起初，他们会犯错误，但他们会从中学习，这样当相同的情况在未来再次出现时，他们就不会再犯同样的错误。强化学习使用以下技术来训练模型：
- en: Environment ----------> Try and fail -----------> Learn from failures ---------->
    Reach goal
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 环境 ----------> 尝试并失败 -----------> 从失败中学习 ----------> 达成目标
- en: Historically, you couldn't use ML to get an algorithm learn how to become better
    than a human at performing a certain task. All that could be done was model the
    machine's behavior after a human's actions and, maybe, the computer would run
    through them faster. RL, however, makes it possible to create models that become
    better at performing certain tasks than humans.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史上看，你不能使用机器学习来让一个算法学会如何在某项任务上比人类做得更好。所能做的只是模仿人类的行为，也许计算机可以更快地运行它们。然而，强化学习（RL）使得创建比人类在执行某些任务上更好的模型成为可能。
- en: Isaac Abhadu, CEO and co-founder at SYBBIO, had this wonderful explanation on
    Quora detailing the working of RL compared to supervised learning. He stated that
    an RL framework, in a nutshell, is very similar to that of supervised learning.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: SYBBIO的CEO和联合创始人Isaac Abhadu在Quora上给出了一个精彩的解释，详细说明了强化学习与监督学习的区别。他指出，简而言之，强化学习框架与监督学习非常相似。
- en: Suppose we're trying to get an algorithm to excel at the game of Pong. We have
    input frames that we will run through a model to get it to produce some random
    output actions, just as we would in a supervised learning setting. The difference,
    however, is that in the case of RL, we ourselves do not know what the target labels
    are, and so we don't tell the machine what's better to do in every specific situation.
    Instead, we apply something called a **policy gradients** method.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们试图让一个算法在乒乓球游戏中表现出色。我们将输入帧通过模型运行，使其产生一些随机输出动作，就像在监督学习设置中做的那样。然而，在强化学习的情况下，我们自己并不知道目标标签是什么，因此我们不会告诉机器在每种特定情况下应该做什么。相反，我们应用一种称为**策略梯度**的方法。
- en: So, we start with a random network and feed to it an input frame so it produces
    a random output action to react to that frame. This action is then sent back to
    the game engine, which makes it produce another frame. This loop continues over
    and over. The only feedback it will give is the game's scoreboard. Whenever our
    agent does something right – that is, it produces some successful sequence – it
    will get a point, generally termed as a **reward**. Whenever it produces a failing
    sequence, it will get a point removed—this is a **penalty**.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们从一个随机网络开始，向它输入一个输入帧，使其产生一个随机输出动作来响应该帧。这个动作随后被发送回游戏引擎，使其产生另一个帧。这个循环不断重复。它唯一会提供的反馈是游戏的计分板。每当我们的代理做对了——也就是说，它产生了一些成功的序列——它将获得一分，通常被称为**奖励**。每当它产生一个失败的序列，它将失去一分——这被称为**惩罚**。
- en: The ultimate goal the agent is pursuing is to keep updating its policy to get
    as much rewards as possible. So, over time, it will figure out how to beat a human
    at the game.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 代理追求的最终目标是不断更新其策略，以获得尽可能多的奖励。因此，随着时间的推移，它将找出如何在这款游戏中战胜人类。
- en: RL is not quick. The agent is going to lose a lot at first. But we will keep
    feeding it frames so it keeps producing random output actions, and it will stumble
    upon actions that are successful. It will keep accumulating knowledge about what
    moves are successful and, after a while, will become invincible.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习不是一件快速的事情。代理最初会输很多。但我们会继续给它输入帧，使其不断产生随机输出动作，并最终发现成功的动作。它将不断积累关于哪些动作是成功的知识，经过一段时间后，将变得不可战胜。
- en: Comparison of RL with other ML algorithms
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习与其他机器学习算法的比较
- en: 'RL involves an **environment**, which is the problem set to be solved, and
    an **agent**, which is simply the AI algorithm. The agent will perform a certain
    action and the result of the action will be a change in the **state** of the agent.
    The change leads to the agent getting a reward, which is a positive reward, or
    a penalty, which is a negative reward for having performed an incorrect action.
    By repeating the action and reward process, the agent learns the environment.
    It understands the various states and the various actions that are desirable and
    undesirable. This process of performing actions and learning from the rewards
    is RL. The following diagram is an illustration showing the relationship between
    the agent and the environment in RL:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习涉及一个**环境**，即要解决的问题集，以及一个**智能体**，它简单地说就是人工智能算法。智能体将执行某个动作，该动作的结果将导致智能体状态的改变。这种改变导致智能体获得奖励，这是一种积极的奖励，或者是一种惩罚，这是一种由于执行了错误动作而产生的负面奖励。通过重复动作和奖励过程，智能体学习环境。它理解各种状态和期望的以及不期望的各种动作。执行动作并从奖励中学习的过程就是强化学习。以下图示展示了强化学习中智能体和环境之间的关系：
- en: '![](img/e0363e4a-eefb-47da-aeff-5e949994c980.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e0363e4a-eefb-47da-aeff-5e949994c980.png)'
- en: Relationship between the agent and environment in RL
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中智能体和环境之间的关系
- en: RL, **deep learning** (**DL**), and ML all support automation in one way or
    another. All of them involve some kind of learning from the given data. However,
    what separates RL from the others is that RL learns the right actions by trail
    and error, whereas the others are focused on learning by finding patterns in the
    existing data. Another key difference is that for DL and ML algorithms to learn
    better, we will need to give them large labeled datasets, whereas this is not
    the case with RL.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习（RL）、**深度学习（DL**）和机器学习（ML）都以某种方式支持自动化。它们都涉及从给定数据中学习。然而，RL与其他技术的区别在于，RL通过试错来学习正确的动作，而其他技术则专注于通过在现有数据中寻找模式来学习。另一个关键区别是，为了使深度学习和机器学习算法更好地学习，我们需要向它们提供大量的标记数据集，而强化学习则不需要这样做。
- en: Let's understand RL better by taking the analogy of training pets at home. Imagine
    we are teaching our pet dog, Santy, some new tricks. Santy, unfortunately, does
    not understand English; therefore, we need to find an alternative way to train
    him. We emulate a situation, and Santy tries to respond in many different ways.
    We reward Santy with a bone treat for any desirable responses. What this inculcates
    in the pet dog is that the next time he encounters a similar situation, he will
    perform the desired behavior as he knows that there is a reward. So, this is learning
    from positive responses; if he is treated with negative responses, such as frowning,
    he will be discouraged from undesirable behavior.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将训练家中的宠物作为类比，让我们更好地理解强化学习。想象一下，我们正在教我们的宠物狗，桑迪，一些新的技巧。不幸的是，桑迪不懂英语；因此，我们需要找到一种替代方法来训练他。我们模拟一个情境，桑迪尝试以许多不同的方式做出回应。对于任何期望的回应，我们都会用骨头奖励桑迪。这使宠物狗明白，下次他遇到类似的情况时，他会执行期望的行为，因为他知道有奖励。所以，这是从积极回应中学习；如果他受到负面回应，比如皱眉，他将会被劝阻进行不期望的行为。
- en: Terminology of RL
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习术语
- en: 'Let''s understand the RL key terms—agent, environment, state, policy, reward,
    and penalty—with our pet dog training analogy:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过宠物狗训练的类比来理解强化学习的关键术语——智能体、环境、状态、策略、奖励和惩罚：
- en: Our pet dog, Santy, is the agent that is exposed to the environment.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的宠物狗桑迪是暴露在环境中的智能体。
- en: The environment is a house or play area, depending on what we want to teach
    to Santy.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 环境是一个房子或游乐区，这取决于我们想要教给桑迪什么。
- en: Each situation encountered is called the state. For example, Santy crawling
    under the bed or running can be interpreted as states.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个遇到的情况都称为状态。例如，桑迪爬到床下或奔跑可以被解释为状态。
- en: Santy, the agent, reacts by performing actions to change from one state to another.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能体桑迪通过执行动作来改变从一个状态到另一个状态。
- en: After changes in states, we give the agent either a reward or a penalty, depending
    on the action that is performed.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在状态改变后，我们根据执行的动作给予智能体奖励或惩罚。
- en: The policy refers to the strategy of choosing an action for finding better outcomes.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 策略指的是选择动作以找到更好结果的战略。
- en: 'Now that we understand each of the RL terms, let''s define the terms more formally
    and visualize the agent''s behavior in the diagram that follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经理解了每个强化学习（RL）术语，让我们更正式地定义这些术语，并在下面的图中可视化智能体的行为：
- en: '**States**: The complete description of the world is known as the states. We
    do not abstract any information that is present in the world. States can be a
    position, a constant, or a dynamic. States are generally recorded in arrays, matrices,
    or higher order tensors.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**状态**：世界的完整描述被称为状态。我们不会抽象掉世界上存在的任何信息。状态可以是位置、常数或动态的。状态通常记录在数组、矩阵或更高阶的张量中。'
- en: '**Actions**: The environment generally defines the possible actions; that is,
    different environments lead to different actions, based on the agent. The valid
    actions for an agent are recorded in a space called an action space. The possible
    valid actions in an environment are finite in number.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：环境通常定义可能的动作；也就是说，不同的环境根据智能体导致不同的动作。智能体的有效动作记录在一个称为动作空间的空间中。环境中可能的有效动作数量是有限的。'
- en: '**Environment**: This is the space where the agent lives and with which the
    agent interacts. For different types of environments, we use different rewards
    and policies.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境**：这是智能体生活和与之交互的空间。对于不同类型的环境，我们使用不同的奖励和政策。'
- en: '**Reward and return**: The reward function is the one that must be kept track
    of at all times in RL. It plays a vital role in tuning, optimizing the algorithm,
    and stopping the training of the algorithm. The reward is computed based on the
    current state of the world, the action just taken, and the next state of the world.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奖励和回报**：奖励函数是强化学习中必须始终跟踪的函数。它在调整、优化算法和停止算法训练中起着至关重要的作用。奖励是根据当前世界状态、刚刚采取的动作和下一个世界状态计算的。'
- en: '**Policies**: A policy in RL is a rule that''s used by an agent for choosing
    the next action; the policy is also known as the agent''s brain.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**策略**：在强化学习中，策略是智能体用于选择下一个动作的规则；策略也被称为智能体的大脑。'
- en: 'Take a look at the following flowchart to understand the process better:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的流程图，以更好地理解这个过程：
- en: '![](img/4bc9d744-0c4c-4e35-a4c6-d917cc652880.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4bc9d744-0c4c-4e35-a4c6-d917cc652880.png)'
- en: Agent behavior in RL
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的智能体行为
- en: 'At each step, *t*, the agent performs the following tasks:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个步骤，*t*，智能体执行以下任务：
- en: Executes action *a[t]*
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行动作 *a[t]*
- en: Receives observation *s[t]*
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收观察结果 *s[t]*
- en: Receives scalar reward *r[t]*
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接收标量奖励 *r[t]*
- en: 'The environment implements the following tasks:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 环境执行以下任务：
- en: Changes upon action *a[t]*
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 行动 *a[t]* 的变化
- en: Emits observation *s[t+1]*
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发出观察结果 *s[t+1]*
- en: Emits scalar reward *r[t+1]*
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发出标量奖励 *r[t+1]*
- en: Time step *t* is incremented after each iteration.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 时间步长 *t* 在每次迭代后增加。
- en: The multi-arm bandit problem
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机问题
- en: Let me start with an analogy to understand this topic better. Do you like pizza?
    I like it a lot! My favorite restaurant in Bangalore serves delicious pizzas.
    I go to this place almost every time I feel like eating a pizza, and I am almost
    sure that I will get the best pizza. However, going to the same restaurant every
    time worries me that I am missing out on pizzas that are even better and served
    elsewhere in the town!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我用一个类比来更好地理解这个话题。你喜欢披萨吗？我非常喜欢！我在班加罗尔的最喜欢的餐厅提供美味的披萨。每次我想吃披萨时，我都会去这个地方，而且我几乎可以肯定我会得到最好的披萨。然而，每次都去同一家餐厅让我担心我错过了镇上其他地方更好吃的披萨！
- en: One alternative available is to try out restaurants one by one and sample the
    pizzas there, but this means that there is a very high probability that I will
    end up eating pizzas that aren't very nice. However, this is the one way for me
    to find a restaurant that serves better pizzas than the one I am currently aware
    of. I am aware you must be wondering why am I talking about pizzas when I am supposed
    to be talking about RL. Let me get to the point.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一个可用的替代方案是逐一尝试餐厅并品尝那里的披萨，但这意味着我最终可能会吃到不太好的披萨的概率非常高。然而，这是我发现比我所知道的餐厅提供更好披萨的唯一方法。我知道你一定在想，为什么我要谈论披萨，而我应该谈论强化学习。让我切入正题。
- en: 'The dilemma with this task arises from incomplete information. In other words,
    to solve this task, it is essential to gather enough information to formulate
    the best overall strategy and then explore new actions. This will eventually lead
    to a minimization of overall bad experiences. This situation can otherwise be
    termed as the **exploration** versus **exploitation** dilemma:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个任务中的困境源于信息不完整。换句话说，要解决这个问题，必须收集足够的信息来制定最佳的整体策略，然后探索新的行动。这最终将导致总体不良体验的最小化。这种情况也可以被称为**探索**与**利用**的困境：
- en: '![](img/edcd5dd5-b248-4187-a01c-ebeb54d077f3.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/edcd5dd5-b248-4187-a01c-ebeb54d077f3.png)'
- en: Exploration versus exploitation dilemma
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用的困境
- en: The preceding diagram aptly summarizes my best-pizza problem.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表恰当地总结了最好的披萨问题。
- en: The **multi-arm bandit problem** (**MABP**) is a simplified form of the pizza
    analogy. It is used to represent similar kinds of problems, and finding a good
    strategy to solve them is already helping a lot of industries.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**多臂老虎机问题**（**MABP**）是披萨类比的一个简化形式。它用来表示类似的问题，并且找到解决这些问题的良好策略已经在很大程度上帮助了许多行业。'
- en: 'A **bandit** is defined as someone who steals your money! A one-armed bandit
    is a simple slot machine. We find this sort of machine in a casino: you insert
    a coin into the slot machine, pull a lever, and pray to the luck god to get an
    immediate reward. But the million-dollar question is why is a slot machine called
    a bandit? It turns out that all casinos configure the slot machines in such a
    way that all gamblers end up losing money!'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**老虎机**被定义为偷你钱的人！单臂老虎机是一种简单的老虎机。我们在赌场中找到这种机器：你将硬币投入老虎机，拉动杠杆，并向幸运之神祈祷以获得即时奖励。但百万美元的问题是为什么老虎机被称为老虎机？结果证明，所有赌场都这样配置老虎机，使得所有赌徒最终都会输钱！'
- en: 'A multi-arm bandit is a hypothetical but complicated slot machine where we
    have more than one slot machine lined up in a row. A gambler can pull several
    levers, with each lever giving a different return. The following diagram depicts
    the probability distribution for the corresponding reward that is different to
    each layer and unknown to the gambler:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机是一个假设但复杂的老虎机，其中我们有一排排列的多台老虎机。赌徒可以拉动几个杠杆，每个杠杆给出不同的回报。以下图表描述了对应奖励的概率分布，这些奖励对于每一层都是不同的，并且对赌徒来说是未知的：
- en: '![](img/b18bda36-4411-4f1b-94bd-ac6ea8271f7b.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b18bda36-4411-4f1b-94bd-ac6ea8271f7b.png)'
- en: Multi-arm bandit
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机
- en: 'Given these slot machines and after a set of initial trials, the task is to
    identify what lever to pull to get the maximum reward. In other words, pulling
    any one of the arms gives us a stochastic reward of either R=+1 for success, or
    R=0 for failure; this is called an **immediate reward**. A multi-arm bandit that
    issues a reward of 1 or 0 is called a **Bernoulli**. The objective is to pull
    the arms one-by-one in a sequence while gathering information to maximize the
    total payout over the long run. Formally, a Bernoulli MABP can be described as
    a tuple of (A,R), where the following applies:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这些老虎机和一系列初始试验后，任务是确定拉动哪个杠杆以获得最大奖励。换句话说，拉动任何一个臂都会给我们一个随机奖励，要么 R=+1 表示成功，要么
    R=0 表示失败；这被称为**即时奖励**。发出 1 或 0 奖励的多臂老虎机被称为**伯努利**。目标是按顺序拉动臂，同时收集信息以最大化长期总回报。形式上，伯努利
    MABP 可以描述为 (A,R) 的元组，其中以下适用：
- en: We have KK machines with reward probabilities, {θ1,…,θK}.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有 K 台机器，具有奖励概率 {θ1,…,θK}。
- en: At each time step, *t*, we take an action, *a*, on one slot machine and receive
    a reward, *r*.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个时间步 *t*，我们选择一台老虎机的动作 *a* 并获得奖励 *r*。
- en: '*A* is a set of actions, each referring to the interaction with one slot machine.
    The value of action *a* is the expected reward, [![](img/508d27c3-9820-4d02-8bda-d5b6ac02d4ef.png)].
    If action *a* at time step *t* is on the *i*-th machine, then [![](img/7b8aad37-eaa2-408a-81b6-4c361ebbf469.png)]. 
    Q(a) is generally referred to as the action-value function.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*A* 是一组动作，每个动作都指代与一台老虎机的交互。动作 *a* 的值是期望奖励，[![](img/508d27c3-9820-4d02-8bda-d5b6ac02d4ef.png)]。如果时间步
    *t* 中的动作 *a* 在第 *i* 台机器上，那么 [![](img/7b8aad37-eaa2-408a-81b6-4c361ebbf469.png)]。Q(a)
    通常被称为动作值函数。'
- en: '*R* is a reward function. In the case of the Bernoulli bandit, we observe a
    reward, *r*, in a stochastic fashion. At time step *t*, [![](img/7a7913f0-0999-4f43-a251-161a5ddd8fc6.png)]
    may return reward 1 with a probability of [![](img/84f9c0d3-29d8-44d3-9fcf-e0813cabb2e5.png),] or
    0 otherwise.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*R* 是一个奖励函数。在伯努利老虎机的情况下，我们以随机的方式观察到奖励 *r*。在时间步 *t*，[![](img/7a7913f0-0999-4f43-a251-161a5ddd8fc6.png)]
    可能以概率 [![](img/84f9c0d3-29d8-44d3-9fcf-e0813cabb2e5.png)，] 返回奖励 1，否则为 0。'
- en: We can solve the MABP with multiple strategies. We will review some of the strategies
    shortly in this section. To decide on the best strategy and to compare the different
    strategies, we need a quantitative method. One method is to directly compute the
    cumulative rewards after a certain predefined number of trials. Comparing the
    cumulative rewards from each of the strategies gives us an opportunity to identify
    the best strategies for the problem.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用多种策略解决 MABP。我们将在本节中简要回顾一些策略。为了确定最佳策略并比较不同的策略，我们需要一个定量方法。一种方法是在一定预定义的试验次数后直接计算累积奖励。比较每种策略的累积奖励为我们提供了识别问题最佳策略的机会。
- en: At times, we may already know the best action for the given bandit problem.
    In those cases, it may be interesting to look at the concept of regret.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们可能已经知道给定老虎机问题的最佳动作。在这些情况下，研究后悔的概念可能很有趣。
- en: 'Let''s imagine that we know of the details of the best arm to pull for the
    given bandit problem. Assume that by repeatedly pulling this best arm, we get
    a maximum expected reward, which is shown as a horizontal line in the following
    diagram:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们已知给定老虎机问题的最佳拉动臂的详细信息。假设通过反复拉动这个最佳臂，我们可以获得最大的期望奖励，这在下图的水平线上表示：
- en: '![](img/b1997e98-45a3-4ac4-8558-5b81dbbab53f.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b1997e98-45a3-4ac4-8558-5b81dbbab53f.png)'
- en: Maximum reward obtained by pulling the best arm for a MABP
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过拉动最佳臂在 MABP 中获得的最大奖励
- en: As per the problem statement, we need to make repeated trials by pulling different
    arms of the multi-arm bandit until we are approximately sure of the arm to pull
    for the maximum average return at time *t*. There are a number of rounds involved
    while we explore and decide upon the best arm. The number of rounds, otherwise
    called **trials**, also incurs some loss, and this is called **regret**. In other
    words, we want to maximize the reward even during the learning phase. Regret can
    be summarized as a quantification of exactly how much we regret not picking the
    optimal arm.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题描述，我们需要通过拉动多臂老虎机的不同臂进行重复试验，直到我们大致确定在时间 *t* 时拉动哪个臂可以获得最大的平均回报。在探索和决定最佳臂的过程中涉及到许多轮次。这些轮次，通常称为
    **试验**，也会产生一些损失，这被称为 **后悔**。换句话说，我们希望在学习的阶段也最大化奖励。后悔可以概括为衡量我们没有选择最优臂的后悔程度。
- en: 'The following diagram is an illustration showing the regret due to trials done
    to find the best arm:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了由于尝试找到最佳臂而产生的后悔：
- en: '![](img/7c7058b4-f051-4e70-84da-58a1531f8c4e.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c7058b4-f051-4e70-84da-58a1531f8c4e.png)'
- en: Concept of regret in MAB
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: MAB 中的后悔概念
- en: Strategies for solving MABP
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解决 MABP 的策略
- en: 'Based on how the exploration is done, the strategies to solve the MABP can
    be classified into the following types:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 根据探索的方式，解决 MABP 的策略可以分为以下几种类型：
- en: No exploration
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无探索
- en: Exploration at random
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机探索
- en: Exploration smartly with preference to uncertainty
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能探索，优先考虑不确定性
- en: Let's delve into the details of some of the algorithms that fall under each
    of the strategy types.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解属于每种策略类型的某些算法的细节。
- en: Let's consider one very naive approach that involves playing just one slot machine
    for a long time. Here, we do no exploration at all and just randomly pick one
    arm to repeatedly pull to maximize the long-term rewards. You must be wondering
    how this works! Let's explore.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个非常简单的方法，即长时间只玩一台老虎机。在这里，我们不做任何探索，只是随机选择一个臂并反复拉动以最大化长期奖励。你一定想知道这是怎么工作的！让我们来探究一下。
- en: In probability theory, the law of large numbers is a theorem that describes
    the result of performing the same experiment a large number of times. According
    to this law, the average of the results obtained from a large number of trials
    should be close to the expected value, and will tend to become closer as more
    trials are performed.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在概率论中，大数定律是一个描述进行相同实验大量次的结果的定理。根据这个定律，从大量试验中获得的结果的平均值应该接近期望值，并且随着试验次数的增加而越来越接近。
- en: We can just play with one machine for a large number of rounds so as to eventually
    estimate the true reward probability according to the law of large numbers.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以只玩一台机器进行大量轮次，以便最终根据大数定律估计真实的奖励概率。
- en: However, there are some problems with this strategy. First and foremost, we
    do not know the value of a large number of rounds. Second, it is super resource
    intensive to play the same slot repeatedly for large number of times. And, most
    importantly, there is no guarantee that we will obtain the best long-term reward
    with this strategy.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种策略存在一些问题。首先，我们不知道大量轮次的价值。其次，重复玩相同的插槽需要超级资源密集。最重要的是，我们无法保证使用这种策略将获得最佳长期奖励。
- en: The epsilon-greedy algorithm
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ε-贪婪算法
- en: The greedy algorithm in RL is a complete exploitation algorithm, which does
    not care for exploration. Greedy algorithms always select the action with the
    highest estimated action value. The action value is estimated according to past
    experience by averaging the rewards associated with the target action that have
    been observed so far.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的贪婪算法是一个完全的利用算法，它不考虑探索。贪婪算法总是选择具有最高估计动作值的动作。动作值是根据过去经验通过平均迄今为止观察到的与目标动作相关的奖励来估计的。
- en: However, use of a greedy algorithm can be a smart approach if we are able to
    successfully estimate the action value to the expected action value; if we know
    the true distribution, we can just select the best actions. An epsilon-greedy
    algorithm is a simple combination of the greedy and random approaches.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们能够成功地将动作值估计为期望的动作值；如果我们知道真实的分布，我们就可以直接选择最佳的动作。ε-贪婪算法是贪婪和随机方法的简单组合。
- en: Epsilon helps to do this estimate. It adds exploration as part of the greedy
    algorithm. In order to counter the logic of always selecting the best action,
    as per the estimated action value, occasionally, the epsilon probability selects
    a random action for the sake of exploration; the rest of the time, it behaves
    as the original greedy algorithm and select the best known action.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ε有助于进行这种估计。它将探索作为贪婪算法的一部分。为了对抗总是根据估计的动作值选择最佳动作的逻辑，偶尔，ε概率会为了探索而选择一个随机动作；其余时间，它表现得像原始的贪婪算法，选择已知的最佳动作。
- en: The epsilon in this algorithm is an adjustable parameter that determines the
    probability of taking a random, rather than principled, action. It is also possible
    to adjust the epsilon value during training. Generally, at the start of the training
    process, the epsilon value is often initialized to a large probability. As the
    environment is unknown, the large epsilon value encourages exploration. The value
    is then gradually reduced to a small constant (often set to 0.1). This will increase
    the rate of exploitation selection.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在此算法中，ε是一个可调整的参数，它决定了采取随机行动而不是基于原则行动的概率。在训练过程中，也可以调整ε的值。通常，在训练过程的开始，ε的值通常初始化为一个较大的概率。由于环境未知，较大的ε值鼓励探索。然后，该值逐渐减少到一个小的常数（通常设置为0.1）。这将增加利用选择的速率。
- en: Due to the simplicity of the algorithm, the approach has become the de facto
    technique for most recent RL algorithms.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法的简单性，这种方法已成为最近大多数强化学习算法的事实上的技术。
- en: Despite the common usage that the algorithm enjoys, this method is far from
    optimal, since it takes into account only whether actions are most rewarding or
    not.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管该算法被广泛使用，但这种方法远非最优，因为它只考虑了动作是否最有利。
- en: Boltzmann or softmax exploration
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 鲍尔兹曼或softmax探索
- en: Boltzmann exploration is also called **softmax exploration**. As opposed to
    either taking the optimal action all the time or taking a random action all the
    time, this exploration favors both through weighted probabilities. This is done
    through a softmax over the network's estimates of values for each action. In this
    case, although not guaranteed, the action that the agent estimates to be optimal
    is most likely to be chosen.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 鲍尔兹曼探索也称为**softmax探索**。与始终采取最佳动作或始终采取随机动作相反，这种探索通过加权概率同时偏好这两种方法。这是通过对网络对每个动作的值估计进行softmax操作来实现的。在这种情况下，尽管不能保证，但代理估计为最佳的动作最有可能被选择。
- en: Boltzmann exploration has the biggest advantage over epsilon greedy. This method
    has information about the likely values of the other actions. In other words,
    let's imagine that there are five actions available to an agent. Generally, in
    the epsilon-greedy method, four actions are estimated as non-optimal and they
    are all considered equally. However, in Boltzmann exploration, the four sub-optimal
    choices are weighed by their relative value. This enables the agent to ignore
    actions that are estimated to be largely sub-optimal and give more attention to
    potentially promising, but not necessarily ideal, actions.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 拉普拉斯探索相较于ε贪婪算法具有最大的优势。这种方法了解其他动作可能值的概率。换句话说，让我们想象一个智能体有五个可用的动作。通常，在ε贪婪方法中，四个动作被估计为非最优，并且它们都被同等考虑。然而，在拉普拉斯探索中，这四个次优选择根据它们的相对价值进行权衡。这使得智能体能够忽略估计为很大程度上次优的动作，并更多地关注可能具有潜力的、但不一定是理想的动作。
- en: The temperature parameter (*τ*) controls the spread of the softmax distribution,
    so that all actions are considered equally at the start of training, and actions
    are sparsely distributed by the end of training. The parameter is annealed over
    time.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 温度参数(*τ*)控制softmax分布的扩散，使得在训练开始时所有动作都被同等考虑，而在训练结束时动作分布变得稀疏。该参数随时间衰减。
- en: Decayed epsilon greedy
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 衰减的ε贪婪
- en: The value of epsilon is key in determining how well the epsilon-greedy algorithm
    works for a given problem. Instead of setting this value at the start and then
    decreasing it, we can make epsilon dependent on time. For example, epsilon can
    be kept equal to 1 / log(t + 0.00001). As time passes, the epsilon value will
    keep reducing. This method works as over the time that epsilon is reduced, we
    become more confident of the optimal action and less exploring is required.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ε的值对于确定ε贪婪算法在给定问题上的表现至关重要。我们不必在开始时设置此值然后逐渐减小，而是可以使ε依赖于时间。例如，ε可以保持为1 / log(t
    + 0.00001)。随着时间的推移，ε的值将不断减少。这种方法有效，因为随着时间的推移ε的减少，我们对最优动作的信心增强，探索的需求减少。
- en: The problem with the random selection of actions is that after sufficient time
    steps, even if we know that some arm is bad, this algorithm will keep choosing
    that with probability *epsilon/n*. Essentially, we are exploring a bad action,
    which does not sound very efficient. The approach to get around this could be
    to favor exploration of arms with strong potential in order to get an optimal
    value.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随机选择动作的问题在于，经过足够的时间步数后，即使我们知道某些臂是坏的，此算法仍会以概率*epsilon/n*继续选择它。本质上，我们正在探索一个坏动作，这听起来并不很高效。绕过这一问题的方法可能是优先探索具有强大潜力的臂，以获得最优值。
- en: The upper confidence bound algorithm
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 上置信界算法
- en: The **upper confidence bound** (**UCB**) algorithm is the most popular and widely
    used solution for MABPs. This algorithm is based on the principle of optimism
    in the face of uncertainty. This essentially means, the less uncertain we are
    about an arm, the more important it becomes to explore that arm.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**上置信界**(**UCB**)算法是MABP最流行和最广泛使用的解决方案。该算法基于面对不确定性的乐观原则。这本质上意味着，我们对臂的不确定性越小，探索该臂的重要性就越大。'
- en: 'Assume that we have two arms that can be tried out. If we have tried out the
    first arm 100 times but the second arm only once, then we are probably reasonably
    confident about the payoff of the first arm. However, we are very uncertain about
    the payoff of the second arm. This gives rise to the family of UCB algorithms.
    This can be further explained through the following diagram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个可以尝试的臂。如果我们已经尝试了第一个臂100次，但第二个臂只尝试了一次，那么我们对第一个臂的回报可能相当有信心。然而，我们对第二个臂的回报非常不确定。这导致了UCB算法系列的产生。这可以通过以下图表进一步解释：
- en: '![](img/08208bea-abf0-4d4d-b37e-886510e89950.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08208bea-abf0-4d4d-b37e-886510e89950.png)'
- en: Illustration to explain upper confident bound algorithm
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 解释上置信界算法的插图
- en: In the preceding diagram, each bar represents a different arm or an action.
    The red dot is the true expected reward and the center of the bar represents the
    observed average reward. The width of the bar represents the confidence interval.
    We are already aware that, by the law of large numbers, the more samples we have,
    the closer the observed average gets to the true average, and the more the bar
    shrinks.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，每个条形代表不同的臂或动作。红色点代表真实的期望奖励，条形的中心代表观察到的平均奖励。条形的宽度代表置信区间。我们已经知道，根据大数定律，我们拥有的样本越多，观察到的平均数就越接近真实平均数，条形就越缩小。
- en: The idea behind UCB algorithms is to always pick the arm or action with the
    highest upper bound, which is the sum of the observed average and the one-sided
    width of the confidence interval. This balances the exploration of arms that have
    not been tried many times with the exploitation of arms that have.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: UCB算法背后的思想是始终选择具有最高上界的臂或动作，即观察到的平均数和置信区间单侧宽度的总和。这平衡了对尚未尝试很多次的臂的探索和对已经尝试过的臂的利用。
- en: Thompson sampling
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Thompson采样
- en: Thompson sampling is one of the oldest heuristics for MABPs. It is a randomized
    algorithm based on Bayesian ideas, and has recently generated significant interest
    after several studies demonstrated it to have better empirical performance compared
    to other methods.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Thompson采样是MABP（多臂老虎机）中最古老的启发式算法之一。它是一种基于贝叶斯思想的随机算法，在几项研究表明它比其他方法具有更好的经验性能后，最近引起了极大的兴趣。
- en: There is a beautiful explanation I found on [https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms](https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms).
    I do not think I can do  better job at explaining Thompson sampling than this.
    You can refer to this for further reference.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms](https://stats.stackexchange.com/questions/187059/could-anyone-explain-thompson-sampling-in-simplest-terms)上找到了一个美丽的解释。我认为我无法比这个更好地解释Thompson采样。您可以参考这个链接以获取更多信息。
- en: Multi-arm bandit – real-world use cases
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多臂老虎机——现实世界用例
- en: 'We encounter so many situations in the real world that are similar to that
    of the MABP we reviewed in this chapter. We could apply RL strategies to all these
    situations. The following are some of the real-world use cases similar to that
    of the MABP:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界中，我们遇到了许多与本章中我们回顾的MABP类似的情况。我们可以将这些RL策略应用于所有这些情况。以下是一些与MABP类似的现实世界用例：
- en: Finding the best medicine/s among many alternatives
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在众多替代方案中找到最佳药物/药品
- en: Identifying the best product to launch among possible products
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在可能的产品中确定推出最佳产品
- en: Deciding the amount of traffic (users) that we need to allocate for each website
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决定为每个网站分配的流量（用户）数量
- en: Identifying the best marketing strategy for launching a product
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定推出产品的最佳营销策略
- en: Identifying the best stocks portfolio to maximize profit
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定最佳股票组合以最大化利润
- en: Finding out the best stock to invest in
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找出最佳投资股票
- en: Figuring out the shortest path in a given map
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定的地图上找出最短路径
- en: Click-through rate prediction for ads and articles
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告和文章的点击率预测
- en: Predicting the most beneficial content to be cached at a router based upon the
    content of articles
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据文章内容预测在路由器上缓存的最有益内容
- en: Allocation of funding for different departments of an organization
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为组织不同部门分配资金
- en: Picking best-performing athletes out of a group of students given limited time
    and an arbitrary selection threshold
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在有限的时间和任意选择阈值下，从一群学生中挑选出表现最佳的运动员
- en: So far, we have covered almost all of the basic details that we need to know
    to progress to the practical implementation of RL to the MABP. Let's kick-start
    coding solutions to the MABP in our next section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了几乎所有我们需要知道的基本细节，以便将RL（强化学习）应用到MABP（多臂老虎机）的实际实现中。让我们在下一节中开始编写解决MABP的代码解决方案。
- en: Solving the MABP with UCB and Thompson sampling algorithms
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用UCB和Thompson采样算法解决MABP问题
- en: 'In this project, we will use upper confidence limits and Thompson sampling
    algorithms to solve the MABP. We will compare their performance and strategy in
    three different situations—standard rewards, standard but more volatile rewards,
    and somewhat chaotic rewards. Let''s prepare the simulation data, and once the
    data is prepared, we will view the simulated data using the following code:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用上置信限和Thompson采样算法来解决MABP问题。我们将比较它们在三种不同情况下的性能和策略——标准奖励、标准但更波动的奖励以及有些混乱的奖励。让我们准备模拟数据，一旦数据准备就绪，我们将使用以下代码查看模拟数据：
- en: '[PRE0]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will give the following output:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/af7c8009-6639-486b-aa13-f7b31d4a198e.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/af7c8009-6639-486b-aa13-f7b31d4a198e.png)'
- en: 'Now, create a melted dataset with an arm and reward combination, and then convert
    the arm column to the nominal type using the following code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，创建一个包含臂和奖励组合的熔化数据集，然后使用以下代码将臂列转换为名义类型：
- en: '[PRE1]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give us the following output:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/6773aaa3-4263-436a-abda-d6523137ce4c.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6773aaa3-4263-436a-abda-d6523137ce4c.png)'
- en: 'Now, plot the distributions of rewards from bandits using the following code:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用以下代码绘制带奖机的奖励分布：
- en: '[PRE2]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give us the following output:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/98af8da3-8b26-4f28-8670-55ecd4975f42.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/98af8da3-8b26-4f28-8670-55ecd4975f42.png)'
- en: 'Now let''s implement the UCB algorithm on the hypothesized arm with a normal
    distribution using the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码在假设的手臂上实现UCB算法，该手臂具有正态分布：
- en: '[PRE3]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You will get the following as the resultant output:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下结果：
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we will implement the Thompson sampling algorithm using a **normal-gamma**
    prior and normal likelihood to estimate posterior distributions using the following
    code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用**正态-伽马**先验和正态似然函数实现Thompson抽样算法，以以下代码估计后验分布：
- en: '[PRE5]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You will get the following as the resultant output:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下结果：
- en: '[PRE6]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From the results, we can infer that the UCB algorithm quickly identified that
    the 10^(th) arm yields the most reward. We also observe that Thompson sampling
    tried the worst arms a lot more times before finding the best one.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以推断出UCB算法迅速识别出第10个手臂产生最多的奖励。我们还观察到，Thompson抽样在找到最佳选择之前尝试了更多次最差的带奖机。
- en: 'Now, let''s simulate the data of bandits with normally distributed rewards
    with large variance and plot the distributions of rewards by using the following
    code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用以下代码模拟具有大方差正态分布奖励的带奖机数据，并绘制奖励分布：
- en: '[PRE7]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You will get the following graph as the resultant output:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下结果图：
- en: '![](img/f3110523-de32-48fd-8998-a20d758ca85e.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f3110523-de32-48fd-8998-a20d758ca85e.png)'
- en: 'Apply UCB on rewards with higher variance using the following code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码对具有更高方差的奖励应用UCB：
- en: '[PRE8]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You will get the following output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '[PRE9]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, apply Thompson sampling on rewards with higher variance by using the
    following code:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下代码对具有更高方差的奖励应用Thompson抽样：
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You will get the following output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '[PRE11]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: From the results, we can infer that when the fluctuation of rewards is greater,
    the UCB algorithm is more susceptible to being stuck at a suboptimal choice and
    never finds the optimal bandit. Thompson sampling is generally more robust and
    is able to find the optimal bandit in all kinds of situations.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 从结果中，我们可以推断出，当奖励的波动性更大时，UCB算法更容易陷入次优选择，并且永远找不到最优的带奖机。Thompson抽样通常更稳健，能够在各种情况下找到最优的带奖机。
- en: 'Now let''s simulate the more chaotic distribution bandit data and plot the
    distribution of rewards from bandits by using the following code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过以下代码模拟更混乱的分布型带奖机数据，并绘制带奖机的奖励分布：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'You will get the following graph as the resultant output:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下结果图：
- en: '![](img/e2dc6b43-3e5f-4697-a714-ac73f64ed9f6.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2dc6b43-3e5f-4697-a714-ac73f64ed9f6.png)'
- en: 'Apply UCB on rewards with different distributions by using the following code:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码通过UCB对具有不同分布的奖励应用：
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You will get the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '[PRE14]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, apply Thompson sampling on rewards with different distributions by using
    the following code:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用以下代码对具有不同分布的奖励应用Thompson抽样：
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You will get the following as the resultant output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下结果：
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: From the preceding results, we see that the performance of the two algorithms
    is similar. A major reason for the Thompson sampling algorithm trying all bandits
    several times before choosing the one it considers best is because we chose a
    prior distribution with a relatively high mean in this project. With the prior
    having a larger mean, the algorithm favors **exploration over exploitation** at
    the beginning. Only when the algorithm becomes very confident that it has found
    the best choice does it value exploitation over exploration. If we decrease the
    mean of the prior, exploitation would have a higher value and the algorithm would
    stop exploring faster. By altering the prior distribution used, you can adjust
    the relative importance of exploration over exploitation to suit the specific
    problem at hand. This is more evidence highlighting the flexibility of the Thompson
    sampling algorithm.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的结果中，我们看到两种算法的性能相似。Thompson抽样算法在选择它认为最好的带奖机之前尝试所有带奖机多次的主要原因是我们在这个项目中选择了一个具有相对较高均值的先验分布。由于先验具有更大的均值，算法在开始时更倾向于**探索而非利用**。只有当算法非常确信它已经找到了最佳选择时，它才会将利用的价值置于探索之上。如果我们降低先验的均值，利用的价值就会更高，算法就会更快地停止探索。通过改变使用的先验分布，您可以调整探索与利用的相对重要性，以适应具体问题。这是更多证据，突出了Thompson抽样算法的灵活性。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about RL. We started the chapter by defining RL
    and its difference when compared with other ML techniques. We then reviewed the
    details of the MABP and looked at the various strategies that can be used to solve
    this problem. Use cases that are similar to the MABP were discussed. Finally,
    a project was implemented with UCB and Thompson sampling algorithms to solve the
    MABP using three different simulated datasets.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了强化学习（RL）。我们首先定义了RL及其与其他机器学习（ML）技术的区别。然后，我们回顾了多臂老虎机问题（MABP）的细节，并探讨了可以用来解决此问题的各种策略。讨论了与MABP类似的用例。最后，通过使用UCB和Thompson抽样算法，在三个不同的模拟数据集上实现了项目来解决MABP。
- en: We have almost reached the end of this book. The appendix of this book, *The
    Road Ahead*, as the name reflects, is a guidance chapter suggesting details on
    what's next from here to become a better R data scientist. I am super excited
    that I am at the last leg of this R projects journey. Are you with me on this
    as well?
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎到达了这本书的结尾。本书的附录*未来之路*，正如其名所示，是一个指导章节，建议从现在开始如何成为更好的R数据科学家。我非常兴奋，我已经到达了R项目之旅的最后阶段。你们也和我一起吗？
