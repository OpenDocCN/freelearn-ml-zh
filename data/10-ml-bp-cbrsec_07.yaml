- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Detecting Machine-Generated Text
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测机器生成文本
- en: In the previous chapter, we discussed deepfakes, which are synthetic media that
    can depict a person in a video and show the person to be saying or doing things
    that they did not say or do. Using powerful deep learning methods, it has been
    possible to create realistic deepfakes that cannot be distinguished from real
    media. Similar to such deepfakes, machine learning models have also succeeded
    in creating fake text – text that is generated by a model but appears to be written
    by a human. While the technology has been used to power chatbots and develop question-answering
    systems, it has also found its use in several nefarious applications.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了深度伪造，这是一种可以描绘视频中人物并显示该人物说或做他们实际上没有说或做的事情的合成媒体。使用强大的深度学习方法，已经能够创建出逼真的深度伪造，它们与真实媒体无法区分。与这样的深度伪造类似，机器学习模型也成功地创建了虚假文本——由模型生成但看起来像是人类所写的文本。虽然这项技术已被用于驱动聊天机器人和开发问答系统，但它也被用于一些恶意应用中。
- en: Generative text models can be used to enhance bots and fake profiles on social
    networking sites. Given a prompt text, the model can be used to write messages,
    posts, and articles, thus adding credibility to the bot. A bot can now pretend
    to be a real person, and a victim might be fooled because of the realistic-appearing
    chat messages. These models allow customization by style, tone, sentiment, domain,
    and even political leaning. It is easily possible to provide a prompt and generate
    a news-style article; such articles can be used to spread misinformation. Models
    can be automated and deployed at scale on the internet, which means that there
    can be millions of fake profiles pretending to be real people, and millions of
    Twitter accounts generating and posting misleading articles. Detecting automated
    text is an important problem on the internet today.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 生成文本模型可以用来增强社交网站上的机器人账户和虚假个人资料。给定一个提示文本，该模型可以用来撰写消息、帖子以及文章，从而为机器人账户增加可信度。机器人现在可以假装成真人，受害者可能会因为看似真实的聊天信息而被欺骗。这些模型可以通过风格、语气、情感、领域甚至政治倾向进行定制。很容易提供提示并生成一篇新闻风格的文章；这样的文章可以用来散布虚假信息。模型可以自动化并在互联网上大规模部署，这意味着可能有数百万个虚假个人资料假装成真人，以及数百万个Twitter账户生成和发布误导性文章。检测自动生成的文本是互联网上今天的一个重要问题。
- en: This chapter will explore the fundamentals of generative models, how they can
    be used to create text, and techniques to detect them.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将探讨生成模型的基本原理，以及如何使用它们来创建文本，以及检测这些文本的技术。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Text generation models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本生成模型
- en: Naïve detection
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 天真检测
- en: Transformer methods for detecting automated text
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于检测自动生成文本的Transformer方法
- en: By the end of this chapter, you will have a firm understanding of text generation
    models and approaches to detecting bot-generated text.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将对文本生成模型和检测机器人生成文本的方法有一个牢固的理解。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206](
    https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在GitHub上找到本章的代码文件，链接为[https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%206)。
- en: Text generation models
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本生成模型
- en: In the previous chapter, we saw how machine learning models can be trained to
    generate images of people. The images generated were so realistic that it was
    impossible in most cases to tell them apart from real images with the naked eye.
    Along similar lines, machine learning models have made great progress in the area
    of text generation as well. It is now possible to generate high-quality text in
    an automated fashion using deep learning models. Just like images, this text is
    so well written that it is not possible to distinguish it from human-generated
    text.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何训练机器学习模型来生成人物图像。生成的图像如此逼真，以至于在大多数情况下，用肉眼几乎无法将其与真实图像区分开来。沿着类似的路线，机器学习模型在文本生成领域也取得了巨大进步。现在，使用深度学习模型可以自动生成高质量的文本。就像图像一样，这些文本写得如此之好，以至于无法区分它们是由人类还是机器生成的。
- en: Fundamentally, a language model is a machine learning system that is able to
    look at a part of a sentence and predict what comes next. The words predicted
    are appended to the existing sentence, and this newly formed sentence is used
    to predict what will come next. The process continues recursively until a specific
    token denoting the end of the text is generated. Note that when we say that the
    next word is predicted, in reality, the model generates a probability distribution
    over possible output words. Language models can also operate at the character
    level.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，语言模型是一种机器学习系统，它能够观察句子的一部分并预测接下来会发生什么。预测的单词会被添加到现有的句子中，然后这个新形成的句子被用来预测接下来会发生什么。这个过程会递归地进行，直到生成一个表示文本结束的特定标记。请注意，当我们说预测下一个单词时，实际上模型是在生成一个可能的输出单词的概率分布。语言模型也可以在字符级别上操作。
- en: Most text generation models take in a prompt text as input. Trained on massive
    datasets (such as all Wikipedia articles or entire books), the models have learned
    to produce text based on these prompts. Training on different kinds of text (stories,
    biographies, technical articles, and news articles) enables models to generate
    those specific kinds of text.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数文本生成模型都会接收一个提示文本作为输入。这些模型在庞大的数据集（如所有维基百科文章或整本书）上进行了训练，从而学会了根据这些提示生成文本。在不同的文本类型（故事、传记、技术文章和新闻文章）上进行训练，使得模型能够生成那些特定的文本类型。
- en: 'To see the power of AI-based text generation with your own eyes, explore the
    open source text generator called **Grover**. This is a tool that was produced
    by researchers at the University of Washington and allows you to produce a real-looking
    news article based on any given prompt. The website provides an interface as shown
    in the following figure, where you can provide a prompt title and an author name:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为了亲眼见证基于AI的文本生成的力量，探索名为**Grover**的开源文本生成器。这是一个由华盛顿大学的研究人员开发出的工具，允许你根据任何给定的提示生成一篇看起来真实的新闻文章。网站提供了一个如图所示的界面，你可以在这里提供提示标题和作者姓名：
- en: '![Figure 6.1 – The Grover interface to generate text](img/B19327_06_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – Grover生成文本的界面](img/B19327_06_01.jpg)'
- en: Figure 6.1 – The Grover interface to generate text
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – Grover生成文本的界面
- en: 'Once you set these parameters, Grover will generate an article that appears
    real. Here is the article that was generated with our given prompt:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了这些参数，Grover将会生成一篇看起来很真实的文章。以下是使用我们提供的提示生成的文章：
- en: '| **Link found between autism** **and vaccines**May 29, 2019 - Sofia OjedaWritten
    by Sofia Ojeda, CNNA paper published in the journal Genetics and Human Behavior
    claims to find a “significant link” between autism and a type of booster vaccine
    given in childhood. The paper focuses on the mercury-based preservative thimerosal
    and considers the possibility that the injectable vaccine contributes to autism-related
    symptoms. These include poor social skills, language delays and seizures, according
    to the paper.Thimerosal, which is commonly used as a preservative in vaccines,
    was phased out of children’s vaccines in 2010\. In 2010, the Food and Drug Administration
    declared that thimerosal was safe. “We don’t know of any adverse effects associated
    with the endocrine-disrupting, mercury-containing preservative in vaccines, and
    believe that this outcome is beneficial to public health,” said Dr. Lisa M. Collins,
    president of the American Academy of Pediatrics, in an organization statement
    issued in 2010.The new study, led by researchers at Tel Aviv University, is based
    on case studies of two women with autistic children and one non-autistic mother.
    The researchers found that autism symptoms tended to worsen after only one dose
    of the thimerosal-containing hepatitis B booster vaccine. “The use of unadjuvanted
    Hepatitis B booster vaccines, against which the only adverse effects are fatigue,
    headache, cough, and fever, remains a substantial and unrefined source of vaccine-related
    injury and disability worldwide,” the study authors wrote.Using a mathematical
    model, the study authors described multiple risks associated with the use of unadjuvanted
    injectable vaccines. Among them, vaccines increased the risk of seizures, such
    as those associated with autism; autoimmune disorders and cancer.“There are other
    types of vaccines that pose similar risk to autism, including rotavirus vaccines
    and the polio vaccine,” said lead author Professor Uri Ayalon, head of the pediatrics
    department at Tel Aviv University.The authors also found that the autism risk
    increased by twofold in children aged 3 to 10 who received the hepatitis B booster
    vaccine against Hepatitis B, regardless of the following vaccination schedules:
    aged 9 months or older with only one dose; or under 4 months old and only one
    dose. “Since these vaccines are widely used, regardless of age, it may be important
    to limit its utilization,” the researchers wrote.Lead author Dr. Yonatan Schulmann
    said there were no apparent risks associated with a standard influenza vaccination.
    “The flu vaccine probably represents an acceptable source of vaccine-related injury
    and disability,” he said. “This is not true for most vaccines. The flu vaccine
    is relatively inexpensive (free of charges) and has no significant health effects,”
    he said.The timing of vaccination is also important, said Schulmann. “Autism spectrum
    disorders are most often diagnosed in early adolescence, the upper age range at
    which it is most likely that vaccination data is available,” he said. Furthermore,
    the authors said they found no clear differences between children who received
    hepatitis B vaccine against Hepatitis B and other children. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| **自闭症与疫苗之间发现关联** 2019年5月29日 - 索菲亚·奥耶达 撰写于CNN 一篇发表在《遗传学与人类行为》杂志上的论文声称在自闭症和儿童时期接种的一种加强型疫苗之间发现了一个“显著的关联”。该论文重点关注基于汞的防腐剂硫柳汞，并考虑了注射疫苗可能导致自闭症相关症状的可能性。根据论文，这些症状包括社交技能差、语言延迟和癫痫发作。硫柳汞通常用作疫苗的防腐剂，于2010年从儿童疫苗中逐步淘汰。2010年，食品药品监督管理局宣布硫柳汞是安全的。“我们不知道与疫苗中内分泌干扰、含汞防腐剂相关的任何不良影响，并相信这一结果有利于公共卫生，”美国儿科学会主席丽莎·M·柯林斯博士在2010年发布的一项组织声明中表示。由特拉维夫大学的研究人员领导的新研究基于两名患有自闭症儿童的女性和一名非自闭症母亲的病例研究。研究人员发现，在接种含有硫柳汞的乙肝加强型疫苗后，自闭症症状往往会恶化。“未添加佐剂的乙肝加强型疫苗，其唯一的不良反应是疲劳、头痛、咳嗽和发烧，仍然是全球疫苗相关伤害和残疾的一个重大且未经过滤的来源，”研究作者写道。研究人员使用数学模型描述了使用未添加佐剂的注射疫苗的多种风险。其中，疫苗增加了与自闭症相关的癫痫发作等风险；自身免疫性疾病和癌症。“还有其他类型的疫苗对自闭症具有类似的风险，包括轮状病毒疫苗和脊髓灰质炎疫苗，”特拉维夫大学儿科系主任、首席作者尤里·阿亚隆教授说。作者还发现，在3至10岁的儿童中，无论接种乙肝疫苗后的接种计划如何：9个月或以上年龄且仅接种一剂；或4个月以下且仅接种一剂，接种乙肝加强型疫苗后自闭症风险增加了一倍。研究人员写道：“由于这些疫苗被广泛使用，无论年龄如何，限制其使用可能很重要。”首席作者约纳坦·舒尔曼博士表示，标准流感疫苗接种没有明显的风险。“流感疫苗可能是疫苗相关伤害和残疾的可接受来源，”他说。“这并不适用于大多数疫苗。流感疫苗相对便宜（免费）且没有显著的健康影响，”他说。舒尔曼还表示，疫苗接种的时间也很重要。“自闭症谱系障碍通常在青春期早期被诊断，这是疫苗接种数据最有可能可用的年龄上限，”他说。此外，作者表示，他们发现接受乙肝疫苗的儿童与其他儿童之间没有明显的差异。
    |'
- en: Note how the article has the stylistic features that you would typically expect
    from journalistic writings. The sentence construction is grammatically correct
    and the whole text reads as a coherent article. There are quotes from researchers
    and professors who are subject matter experts, complete with statistics and experimental
    results cited. Overall, the article could pass off as something written by a human.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意文章的风格特点，这通常是新闻写作所期望的。句子结构语法正确，整个文本读起来像一篇连贯的文章。其中引用了研究人员和教授的话，包括引用的统计数据和实验结果。总的来说，这篇文章可以伪装成由人类撰写的。
- en: Understanding GPT
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解GPT
- en: GPT stands for **Generative Pretrained Transformer**, and GPT models have dazzled
    the NLP world because they can generate coherent essays that are beyond those
    produced by traditional language models such as those based on **Recurrent Neural
    Networks** (**RNNs**). GPT models are also based on the transformer architecture
    (recall the BERT architecture that we used for malware detection was also based
    on the transformer).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: GPT代表**生成预训练变换器**，GPT模型在NLP世界中令人眼花缭乱，因为它们可以生成连贯的文章，这些文章超越了传统语言模型（如基于**循环神经网络**（**RNNs**）的语言模型）所产生的内容。GPT模型也基于变换器架构（回想一下我们用于恶意软件检测的BERT架构也是基于变换器的）。
- en: Recall the concepts of attention that we introduced in [*Chapter 3*](B19327_03.xhtml#_idTextAnchor015),
    *Malware Detection Using Transformers and BERT*. We introduced two kinds of blocks
    – the encoder and decoder – both of which were built using transformers that leveraged
    the attention mechanism. The transformer encoder had a self-attention layer followed
    by a fully connected feed-forward neural network. The decoder layer was similar
    except that it had an additional masked self-attention layer that ensured that
    the transformer did not attend to the future tokens (which would defeat the purpose
    of the language model). For example, if the decoder decodes the fourth word, it
    will attend to all words up to the third predicted word and all the words in the
    input.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在[*第三章*](B19327_03.xhtml#_idTextAnchor015)“使用Transformers和Bert进行恶意软件检测”中介绍的注意力概念。我们介绍了两种类型的块——编码器和解码器——它们都使用利用注意力机制的transformers构建。Transformer编码器包含一个自注意力层，后面跟着一个全连接的前馈神经网络。解码器层类似，但额外包含一个掩码自注意力层，确保transformer不会关注未来的标记（这将违背语言模型的目的）。例如，如果解码器解码第四个单词，它将关注所有预测到第三个单词之前的单词以及输入中的所有单词。
- en: In general, GPT models use only the decoder blocks, which are stacked one after
    the other. When a token is fed into the model, it is converted into an embedding
    representation using a matrix lookup. Additionally, a positional encoding is added
    to it to indicate the sequence of words/tokens. The two matrices (embedding and
    positional encoding) are parts of the pretrained models we use. When the first
    token is passed to the model, it gets converted into a vector using the embedding
    lookup and positional encoding matrices. It passes through the first decoder block,
    which performs self-attention, passes the output to the neural network layer,
    and forwards the output to the next decoder block.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，GPT模型只使用解码器块，它们一个接一个地堆叠。当一个标记输入到模型中时，它使用矩阵查找转换为嵌入表示。此外，还添加了位置编码来指示单词/标记的序列。这两个矩阵（嵌入和位置编码）是我们使用的预训练模型的一部分。当第一个标记传递给模型时，它使用嵌入查找和位置编码矩阵转换为向量。它通过第一个解码器块，该块执行自注意力，将输出传递到神经网络层，然后将输出转发到下一个解码器块。
- en: After processing by the final decoder, the output vector is multiplied with
    the embedding matrix to obtain a probability distribution over the output token
    to be produced. This probability distribution can be used to select the next word.
    The most straightforward strategy is to choose the word with the highest probability
    – however, we run the risk of being stuck in a loop. For instance, if the tokens
    produced so far are “*The man and*” and we always select the word with the highest
    probability, we might end up producing “*The man and the man and the man and the*
    *man…..*” indefinitely.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 经过最终解码器的处理，输出向量与嵌入矩阵相乘，以获得输出标记的概率分布。这个概率分布可以用来选择下一个单词。最直接的战略是选择概率最高的单词——然而，我们面临陷入循环的风险。例如，如果到目前为止产生的标记是“*The
    man and*”，并且我们总是选择概率最高的单词，我们可能会无限期地产生“*The man and the man and the man and the*
    *man…..*”。
- en: To avoid this, we apply a top-*K* sampling. We select the top *K* words (based
    on the probability) and sample a word from them, where words with a higher score
    have a higher chance of being selected. Since this process is non-deterministic,
    the model does not end up in the loop of choosing the same set of words again
    and again. The process continues until a certain number of tokens has been produced,
    or the end-of-string token is found.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免这种情况，我们采用顶*K*抽样。我们选择基于概率的顶*K*个单词，并从中抽取一个单词，得分较高的单词有更高的被选中机会。由于这个过程是非确定性的，模型不会陷入反复选择相同单词集的循环。这个过程会持续进行，直到产生一定数量的标记，或者找到字符串的结束标记。
- en: Generation by GPT models can be either conditional or unconditional. To see
    generation in action, we can use the Write with Transformer ([https://transformer.huggingface.co/doc/gpt2-large](https://transformer.huggingface.co/doc/gpt2-large))
    web app developed by Hugging Face, which uses GPT-2\. The website allows you to
    simulate both conditional and unconditional generation.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型的生成可以是条件性的或非条件性的。为了看到生成的实际效果，我们可以使用由Hugging Face开发的Write with Transformer
    ([https://transformer.huggingface.co/doc/gpt2-large](https://transformer.huggingface.co/doc/gpt2-large))
    网络应用，它使用GPT-2。该网站允许你模拟条件性和非条件性生成。
- en: 'In conditional generation, we provide the model with a set of words as a prompt,
    which is used to seed the generation. This initial set of words provides the context
    used to drive the rest of the text, as shown:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在条件生成中，我们向模型提供一组单词作为提示，该提示用于启动生成。这个初始单词集提供了用于驱动其余文本的上下文，如下所示：
- en: '![Figure 6.2 – Generating text with a prompt](img/B19327_06_02.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图6.2 – 使用提示生成文本](img/B19327_06_02.jpg)'
- en: Figure 6.2 – Generating text with a prompt
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 使用提示生成文本
- en: 'On the other hand, in unconditional generation, we just provide the `<s>` token,
    which is used to indicate the start of a string, and allow the model to freely
    produce what it wants. If you press the *Tab* key on Write With Transformer, you
    should see such unconditional samples generated:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，在非条件生成中，我们只提供`<s>`标记，该标记用于指示字符串的开始，并允许模型自由生成它想要的任何内容。如果你在Write With Transformer上按下*Tab*键，你应该会看到这样的非条件样本生成：
- en: '![Figure 6.3 – Generating text without prompts](img/B19327_06_03.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图6.3 – 无提示生成文本](img/B19327_06_03.jpg)'
- en: Figure 6.3 – Generating text without prompts
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – 无提示生成文本
- en: There have been multiple versions of GPT models released by OpenAI, the latest
    one that has made the news being ChatGPT, based on GPT 3.5\. In an upcoming section,
    we will use ChatGPT to create our own dataset of fake news.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: OpenAI已经发布了多个版本的GPT模型，最新一个引起关注的是基于GPT 3.5的ChatGPT。在接下来的章节中，我们将使用ChatGPT创建我们自己的虚假新闻数据集。
- en: Naïve detection
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简单检测
- en: In this section, we will focus on naïve methods for detecting bot-generated
    text. We will first create our own dataset, extract features, and then apply machine
    learning models to determine whether a particular text is machine-generated or
    not.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注检测由机器人生成的文本的简单方法。我们首先将创建自己的数据集，提取特征，然后应用机器学习模型来确定特定文本是否由机器生成。
- en: Creating the dataset
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建数据集
- en: The task we will focus on is detecting bot-generated fake news. However, the
    concepts and techniques we will learn are fairly generic and can be applied to
    parallel tasks such as detecting bot-generated tweets, reviews, posts, and so
    on. As such a dataset is not readily available to the public, we will create our
    own.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注的任务是检测由机器人生成的虚假新闻。然而，我们将学习到的概念和技术相当通用，可以应用于并行任务，如检测由机器人生成的推文、评论、帖子等。由于此类数据集尚未向公众开放，我们将创建自己的数据集。
- en: How are we creating our dataset? We will use the News Aggregator dataset ([https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator))
    from the UCI Dataset Repository. The dataset contains a set of news articles (that
    is, links to the articles on the web). We will scrape these articles, and these
    are our human-generated articles. Then, we will use the article title as a prompt
    to seed generation by GPT-2, and generate an article that will be on the same
    theme and topic, but generated by GPT-2! This makes up our positive class.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是如何创建我们的数据集的？我们将使用来自UCI数据集仓库的新闻聚合器数据集 ([https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator))。该数据集包含一组新闻文章（即网络上的文章链接）。我们将抓取这些文章，这些就是我们的由人类生成的文章。然后，我们将使用文章标题作为提示来启动GPT-2的生成，并生成一篇与同一主题和话题相关但由GPT-2生成的文章！这构成了我们的正类。
- en: Scraping real articles
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 抓取真实文章
- en: The News Aggregator dataset from UCI contains information on over 420k news
    articles. It was developed for research purposes by scientists at the Roma Tre
    University in Italy. News articles span multiple categories such as business,
    health, entertainment, and science and technology. For each article, we have the
    title and the URL of the article online. You will need to download the dataset
    from the UCI Machine Learning Repository website ([https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: UCI新闻聚合器数据集包含超过42万篇新闻文章的信息。它是由意大利罗马三大学的科学家为研究目的开发的。新闻文章涵盖多个类别，如商业、健康、娱乐和科学技术。对于每篇文章，我们都有文章的标题和在线URL。您需要从UCI机器学习仓库网站下载该数据集（[https://archive.ics.uci.edu/ml/datasets/News+Aggregator](https://archive.ics.uci.edu/ml/datasets/News+Aggregator)）。
- en: 'Take a look at the data using the `head()` functionality (note that you will
    have to change the path according to how you store the file locally):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`head()`功能查看数据（请注意，您将需要根据您在本地存储文件的方式更改路径）：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This will show you the first five rows of the DataFrame. As you can see in the
    following screenshot, we have an ID to refer to each row and the title and URL
    of the news article. We also have the hostname (the website where the article
    appeared) and the timestamp, which denotes the time when the news was published.
    The **STORY** field contains an ID that is used to indicate a cluster containing
    similar news stories.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这将显示DataFrame的前五行。如图下截图所示，我们有一个ID来引用每一行，以及新闻文章的标题和URL。我们还有主机名（文章出现的地方的网站）和发布时间戳，它表示新闻发布的时间。**STORY**字段包含一个ID，用于指示包含类似新闻故事的集群。
- en: '![Figure 6.4 – UCI News Aggregator data](img/B19327_06_04.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图6.4 – UCI新闻聚合器数据](img/B19327_06_04.jpg)'
- en: Figure 6.4 – UCI News Aggregator data
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – UCI新闻聚合器数据
- en: 'Let us take a look at the distribution of the articles across categories:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看文章在类别上的分布：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will produce the following result:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '![Figure 6.5 – News article distribution by category](img/B19327_06_05.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图6.5 – 按类别划分的新闻文章分布](img/B19327_06_05.jpg)'
- en: Figure 6.5 – News article distribution by category
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 按类别划分的新闻文章分布
- en: From the documentation, we see that the categories **e**, **b**, **t**, and
    **m** represent entertainment, business, technology, and health, respectively.
    Entertainment has the highest number of articles, followed by business and technology
    (which are similar), and health has the least.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从文档中，我们看到类别**e**、**b**、**t**和**m**分别代表娱乐、商业、技术和健康。娱乐文章数量最多，其次是商业和技术（两者相似），健康最少。
- en: 'Similarly, we can also inspect the top domains where the articles come from:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们还可以检查文章来源的前几个域名：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You will get the following output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您将得到以下输出：
- en: '![Figure 6.6 – Distribution of news articles across sources](img/B19327_06_06.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图6.6 – 新闻文章在不同来源的分布](img/B19327_06_06.jpg)'
- en: Figure 6.6 – Distribution of news articles across sources
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 新闻文章在不同来源的分布
- en: In order to scrape the article from the website, we would need to simulate a
    browser session using a browser tool such as Selenium, find the article text by
    parsing the HTML source, and then extract it. Fortunately, there is a library
    in Python that does all of this for us. The `Newspaper` Python package ([https://github.com/codelucas/newspaper/](https://github.com/codelucas/newspaper/))
    provides an interface for downloading and parsing news articles. It can extract
    text, keywords, author names, summaries, and images from the HTML source of an
    article. It has support for multiple languages including English, Spanish, Russian,
    and German. You can also use a general-purpose web scraping library such as `BeautifulSoup`,
    but the `Newspaper` library is designed specifically to capture news articles
    and hence provides a lot of functions that we would have had to write custom if
    using `BeautifulSoup`.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从网站上抓取文章，我们需要使用像Selenium这样的浏览器工具模拟浏览器会话，通过解析HTML源代码找到文章文本，然后提取它。幸运的是，Python中有一个库为我们做了所有这些。`Newspaper`
    Python包（[https://github.com/codelucas/newspaper/](https://github.com/codelucas/newspaper/））提供了一个下载和解析新闻文章的接口。它可以从文章的HTML源中提取文本、关键词、作者姓名、摘要和图片。它支持包括英语、西班牙语、俄语和德语在内的多种语言。您也可以使用像`BeautifulSoup`这样的通用网络抓取库，但`Newspaper`库是专门设计来抓取新闻文章的，因此提供了许多我们如果使用`BeautifulSoup`就必须自定义的功能。
- en: To create our dataset of real articles, we will iterate through the News Aggregator
    DataFrame and use the `Newspaper` library to extract the text for each article.
    Note that the dataset has upward of 420k articles – for the purposes of demonstration,
    we will sample 1,000 articles randomly from the dataset. For each article, we
    will use the `Newspaper` library to scrape the text. We will create a directory
    to hold these articles.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建我们的真实文章数据集，我们将遍历新闻聚合器数据框，并使用`Newspaper`库提取每篇文章的文本。请注意，数据集包含超过42万篇文章——为了演示目的，我们将从数据集中随机抽取1,000篇文章。对于每篇文章，我们将使用`Newspaper`库抓取文本。我们将创建一个目录来存放这些文章。
- en: 'First, let us create the directory structure:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们创建目录结构：
- en: '[PRE3]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let us sample articles from the 400k articles we have. In order to avoid
    bias and overfitting, we should not focus on a particular category. Rather, our
    goal should be to sample uniformly at random so we have a well-distributed dataset
    across all four categories. This general principle also applies to other areas
    where you are designing machine learning models; the more diverse your dataset
    is, the better the generalization. We will sample 250 articles from each of our
    4 categories:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从我们拥有的40万篇文章中抽取文章。为了避免偏差和过拟合，我们不应该专注于某个特定类别。相反，我们的目标应该是随机均匀抽样，以便我们在所有四个类别中都有一个分布良好的数据集。这一般原则也适用于其他领域，当你设计机器学习模型时；你的数据集越多样化，泛化能力就越好。我们将从我们的4个类别中每个类别抽取250篇文章：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If you check the distribution now, you will see that it is equal across all
    categories:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你现在检查分布情况，你会看到它在所有类别中都是相等的：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can see the distribution clearly in the following plot:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在下面的图表中清楚地看到分布情况：
- en: '![Figure 6.7 – Distribution of sampled articles](img/B19327_06_07.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图6.7 – 抽样文章的分布](img/B19327_06_07.jpg)'
- en: Figure 6.7 – Distribution of sampled articles
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 抽样文章的分布
- en: 'We will now iterate through this DataFrame and scrape each article. We will
    scrape the article, read the text, and save it into a file in the real directory
    we created earlier. Note that this is essentially a web scraper – as different
    websites have different HTML structures, the newspaper library may hit some errors.
    Certain websites may also block scrapers. For such articles, we will print out
    a message with the article URL. In practice, when such a situation is encountered,
    data scientists will fill the gap manually if the number of missing articles is
    small enough:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将遍历这个数据框，并抓取每篇文章。我们将抓取文章、读取文本，并将其保存到我们之前创建的真实目录中的文件中。请注意，这本质上是一个网络爬虫——由于不同网站有不同的HTML结构，新闻库可能会遇到一些错误。某些网站也可能阻止爬虫。对于这样的文章，我们将打印出带有文章URL的消息。在实际操作中，当遇到这种情况时，如果缺失的文章数量足够少，数据科学家将手动填补空白：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now we have our real articles downloaded locally. It’s time to get into the
    good stuff – creating our set of fake articles!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将真实文章下载到本地。是时候进入精彩的部分——创建我们的假文章集！
- en: Using GPT to create a dataset
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用GPT创建数据集
- en: In this section, we will use GPT-3 to create our own dataset of machine-generated
    text. OpenAI, a San Francisco-based artificial intelligence research lab, developed
    GPT-3, a pretrained universal language model that utilizes deep learning transformers
    to create text that is remarkably human-like. Released in 2020, GPT-3 has made
    headlines in various industries, as its potential use cases are virtually limitless.
    With the help of the GPT-3 API family and ChatGPT, individuals have used it to
    write fiction and poetry, code websites, respond to customer feedback, improve
    grammar, translate languages, generate dialog, optimize tax deductions, and automate
    A/B testing, among other things. The model’s high-quality results have impressed
    many.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用GPT-3创建我们自己的机器生成文本数据集。位于旧金山的OpenAI人工智能研究实验室开发了GPT-3，这是一个预训练的通用语言模型，它利用深度学习转换器创建出非常类似人类文本。2020年发布，GPT-3在各个行业中引起了轰动，因为其潜在的应用场景几乎是无限的。借助GPT-3
    API家族和ChatGPT，个人已经用它来撰写小说和诗歌、编写网站、回应客户反馈、改进语法、翻译语言、生成对话、优化税收减免、自动化A/B测试等。该模型的高质量结果给许多人留下了深刻印象。
- en: 'We can use the `transformers` library from HuggingFace to download and run
    inference on ChatGPT models. To do this, we can first load the model as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用HuggingFace的`transformers`库下载并运行ChatGPT模型的推理。为此，我们首先可以按照以下方式加载模型：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This will download the model for you locally. Note that this involves downloading
    a sizeable model from the online repository, and hence will take quite some time.
    The time taken to execute will depend on your system usage, resources, and network
    speed at the time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为您本地下载模型。请注意，这涉及到从在线存储库下载一个相当大的模型，因此将花费相当长的时间。执行所需的时间将取决于您当时的系统使用情况、资源和网络速度。
- en: 'We can generate a sample text using this new model. For example, if we want
    to generate a poem about flowers, we can do the following:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个新模型生成一个示例文本。例如，如果我们想生成一首关于花朵的诗，我们可以这样做：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'And this gave me the following poem (note that the results may differ for you):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这给了我以下这首诗（请注意，你的结果可能会有所不同）：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have already downloaded and initialized the model we want. Now, we can iterate
    through our list of article titles and generate articles one by one by passing
    the title as a seed prefix. Just like the scraped articles, each article must
    be saved into a text file so that we can later access it for training:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经下载并初始化了我们想要的模型。现在，我们可以遍历我们的文章标题列表，通过传递标题作为种子前缀来逐个生成文章。就像抓取的文章一样，每篇文章都必须保存到文本文件中，这样我们就可以稍后用于训练：
- en: '[PRE10]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'All that is left now is to read all of the data we have into a common array
    or list, which can then be used in all of our experiments. We will read each file
    in the real directory and add it to an array. At the same time, we will keep appending
    `0` (indicating a real article) to another array that holds labels. We will repeat
    the same process with the fake articles and append `1` as the label:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在剩下的就是将我们所有的数据读入一个公共数组或列表中，这样我们就可以在所有的实验中使用它了。我们将读取真实目录中的每个文件，并将其添加到数组中。同时，我们将向另一个包含标签的数组追加`0`（表示真实文章）的标签。我们将对假文章执行相同的过程，并追加`1`作为标签：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Now, we have our text in the `X` list and associated labels in the `Y` list.
    Our dataset is ready!
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的文本在`X`列表中，相关的标签在`Y`列表中。我们的数据集已经准备好了！
- en: Feature exploration
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征探索
- en: Now that we have our dataset, we want to build a machine learning model to detect
    bot-generated news articles. Recall that machine learning algorithms are mathematical
    models and, therefore, operate on numbers; they cannot operate directly on text!
    Let us now extract some features from the text.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了我们的数据集，我们想要构建一个机器学习模型来检测由机器人生成的新闻文章。回想一下，机器学习算法是数学模型，因此它们在数字上操作；它们不能直接在文本上操作！现在让我们从文本中提取一些特征。
- en: This section will focus on hand-crafting features – the process where subject
    matter experts theorize potential differences between the two classes and build
    features that will effectively capture the differences. There is no unified technique
    for doing this; data scientists experiment with several features based on domain
    knowledge to identify the best ones.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将专注于手工制作特征——这是一个领域专家理论化两个类别之间潜在差异并构建能够有效捕捉这些差异的特征的过程。没有统一的技术来做这件事；数据科学家会基于领域知识尝试多个特征，以确定最佳特征。
- en: Here, we are concerned with text data – so let us engineer a few features from
    that domain. Prior work in NLP and linguistics has analyzed human writing and
    identified certain characteristics. We will engineer three features based on prior
    research.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们关注的是文本数据——因此，让我们从这个领域构建一些特征。先前在自然语言处理和语言学领域的工作已经分析了人类写作并识别了某些特征。我们将基于先前研究构建三个特征。
- en: Function words
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 函数词
- en: These are supporting words in the text that do not contribute to meaning but
    add continuity and flow to the sentence. They are generally determiners (*the*,
    *an*, *many*, *a little*, and *none*), conjunctions (*and* and *but*), prepositions
    (*around*, *within*, and *on*), pronouns (*he*, *her*, and *their*), auxiliary
    verbs (*be*, *have*, and *do*), modal auxiliary (*can*, *should*, *could*, and
    *would*), qualifiers (*really* and *quite*), or question words (*how* and *why*).
    Linguistic studies have shown that every human uses these unpredictably, so there
    might be randomness in the usage pattern. As our feature, we will count the number
    of function words that we see in the sentence, and then normalize it by the length
    of the sentence in words.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在文本中起辅助作用的词语，它们并不贡献于意义，但增加了句子的连贯性和流畅性。它们通常是限定词（如**the**、**an**、**many**、**a
    little**和**none**）、连词（如**and**和**but**）、介词（如**around**、**within**和**on**）、代词（如**he**、**her**和**their**）、助动词（如**be**、**have**和**do**）、情态助动词（如**can**、**should**、**could**和**would**）、修饰词（如**really**和**quite**）或疑问词（如**how**和**why**）。语言学研究表明，每个人都会不可预测地使用这些词语，因此使用模式中可能存在随机性。作为我们的特征，我们将计算句子中看到的函数词数量，然后通过句子的单词长度进行归一化。
- en: 'We will use a file that contains a list of the top function words and read
    the list of all function words. Then, we will count the function words in each
    text and normalize this count by the length. We will wrap this up in a function
    that can be used to featurize multiple instances of text. Note that as the list
    of function words would be the same for all texts, we do not need to repeat it
    in each function call – we will keep that part outside the function:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个包含顶级功能词列表的文件，并读取所有功能词的列表。然后，我们将计算每个文本中的功能词数量，并通过长度进行归一化。我们将把这个功能封装在一个可以用于特征化多个文本实例的函数中。请注意，由于所有文本的功能词列表都是相同的，我们不需要在每个函数调用中重复它——我们将这部分放在函数外部：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Punctuation
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标点符号
- en: 'Punctuation symbols (commas, periods, question marks, exclamations, and semi-colons)
    set the tone of the text and inform how it should be read. Prior research has
    shown that the count of punctuation symbols may be an important feature in detecting
    bot-generated text. We will first compile a list of punctuation symbols (readily
    available in the Python `string` package). Similar to the function words, we will
    count the occurrences of punctuation and normalize them by length. Note that this
    time, however, we need to normalize by the length in terms of the number of characters
    as opposed to words:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 标点符号（逗号、句号、问号、感叹号和分号）设定了文本的基调，并告知应该如何阅读。先前的研究表明，标点符号的数量可能在检测机器人生成的文本中是一个重要的特征。我们将首先编译一个标点符号列表（在Python的`string`包中可以轻松获得）。类似于功能词，我们将计算标点的出现次数，并通过长度进行归一化。请注意，然而，这次我们需要根据字符数而不是单词数进行归一化：
- en: '[PRE13]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Readability
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可读性
- en: 'Research in early childhood education has studied text in detail and derived
    several metrics that indicate how readable a particular blob of text is. These
    metrics analyze the vocabulary and complexity of the text and determine the ease
    with which a reader can read and understand the text. There are several measures
    of readability defined in prior literature ([https://en.wikipedia.org/wiki/Readability](https://en.wikipedia.org/wiki/Readability)),
    but we will be using the most popular one called the **Automated Readability Index**
    (**ARI**) ([https://readabilityformulas.com/automated-readability-index.php](https://readabilityformulas.com/automated-readability-index.php)).
    It depends on two factors – word difficulty (the number of letters per word) and
    sentence difficulty (the number of words per sentence), and is calculated as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 早期儿童教育研究详细研究了文本，并推导出几个指标，这些指标可以表明特定文本块的可读性。这些指标分析了文本的词汇和复杂性，并确定读者阅读和理解文本的难易程度。在先前的文献中定义了几个可读性度量方法
    ([https://en.wikipedia.org/wiki/Readability](https://en.wikipedia.org/wiki/Readability))，但我们将使用最流行的一个，称为**自动可读性指数**（**ARI**）([https://readabilityformulas.com/automated-readability-index.php](https://readabilityformulas.com/automated-readability-index.php))。它取决于两个因素——单词难度（每个单词的字母数）和句子难度（每个句子的单词数），计算方法如下：
- en: 'ARI = 4.71 ( # characters _  # words ) + 0.5 ( # words _ # sentences ) − 21.43'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 'ARI = 4.71 ( # characters _  # words ) + 0.5 ( # words _ # sentences ) − 21.43'
- en: 'In theory, the ARI represents the approximate age needed to understand the
    text. We will now develop a function that calculates the ARI for our input text,
    and wrap it into a function like we did for the previous features:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，ARI表示理解文本所需的近似年龄。现在，我们将开发一个函数来计算输入文本的ARI，并将其封装成与之前特征相同的函数：
- en: '[PRE14]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This completes our discussion of naive feature extraction. In the next section,
    we will use these features to train and evaluate machine learning models.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对朴素特征提取的讨论。在下一节中，我们将使用这些特征来训练和评估机器学习模型。
- en: Using machine learning models for detecting text
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用机器学习模型检测文本
- en: 'We have now hand-crafted three different features: punctuation counts, function
    word counts, and the readability index. We also defined functions for each. Now,
    we are ready to apply these to our dataset and build models. Recall that the `X`
    array contains all of our text. We want to represent each text sample using a
    three-element vector (as we have three features):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经手工制作了三个不同的特征：标点符号计数、功能词计数和可读性指数。我们还为每个特征定义了函数。现在，我们准备将这些特征应用到我们的数据集上并构建模型。回想一下，`X`数组包含我们所有的文本。我们希望使用一个包含三个元素的向量来表示每个文本样本（因为我们有三个特征）：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, each text sample is represented by a three-element vector in `X_Features`.
    The first, second, and third elements represent the normalized function word count,
    punctuation count, and ARI, respectively. Note that this order is arbitrary –
    you may choose your own order as it does not affect the final model.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，每个文本样本在`X_Features`中表示为一个三个元素的向量。第一个、第二个和第三个元素分别代表归一化的函数词计数、标点符号计数和ARI。请注意，这个顺序是任意的——你可以选择自己的顺序，因为它不会影响最终模型。
- en: 'Our features are ready, so now we will do the usual. We begin by splitting
    our data into training and test sets. We then fit a model on the training data
    and evaluate its performance on the test data. In previous chapters, we used the
    confusion matrix function to plot the confusion matrix and visually observe the
    true positives, false positives, true negatives, and false negatives. We will
    now build another function on top of it that will take in these values and calculate
    metrics of interest. We will calculate the true positives, false positives, true
    negatives, and false negatives, and then calculate the accuracy, precision, recall,
    and F1 score. We will return all of these as a dictionary:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的特征已经准备好了，所以现在我们将进行常规操作。我们首先将数据分为训练集和测试集。然后我们在训练数据上拟合一个模型，并在测试数据上评估其性能。在之前的章节中，我们使用了混淆矩阵函数来绘制混淆矩阵，并直观地观察真实阳性、假阳性、真实阴性和假阴性。现在，我们将在其基础上构建另一个函数，该函数将接受这些值并计算感兴趣的指标。我们将计算真实阳性、假阳性、真实阴性和假阴性，然后计算准确率、精确率、召回率和F1分数。我们将所有这些作为字典返回：
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let us split the data into training and testing:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将数据分为训练集和测试集：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now, we will fit a model on the training data, and evaluate its performance
    on the test data. Here, we will use random forests, logistic regression, SVM,
    and a deep neural network.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在训练数据上拟合一个模型，并在测试数据上评估其性能。在这里，我们将使用随机森林、逻辑回归、SVM和深度神经网络。
- en: 'The logistic regression classifier is a statistical model that expresses the
    probability of an input belonging to a particular class as a linear combination
    of features. Specifically, the model produces a linear combination of inputs (just
    like linear regression) and applies a sigmoid to this combination to obtain an
    output probability:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归分类器是一个统计模型，它将输入属于特定类别的概率表示为特征的线性组合。具体来说，模型产生输入的线性组合（就像线性回归一样），然后对这个组合应用sigmoid函数以获得输出概率：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Random forests are ensemble classifiers consisting of multiple decision trees.
    Each tree is a hierarchical structure with nodes as conditions and leaves as class
    labels. A classification label is derived by following the path of the tree through
    the root. The random forest contains multiple such trees, each trained on a random
    sample of data and features:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林是由多个决策树组成的集成分类器。每一棵树是一个具有节点作为条件和叶子作为类别标签的分层结构。通过跟随树的路径从根节点开始，可以推导出一个分类标签。随机森林包含多个这样的树，每棵树都在数据特征的一个随机样本上训练：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'A **multilayer perceptron** (**MLP**) is a fully connected deep neural network,
    with multiple hidden layers. The input data undergoes transformations through
    these layers, and the final layer is a sigmoid or softmax function, which generates
    the probability of the data belonging to a particular class:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）是一个全连接的深度神经网络，具有多个隐藏层。输入数据通过这些层进行转换，最后一层是sigmoid或softmax函数，它生成数据属于特定类别的概率：'
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The SVM constructs a decision boundary between two classes such that the best
    classification accuracy is obtained. In case the boundary is not linear, the SVM
    transforms the features into a higher dimensional space and obtains a non-linear
    boundary:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: SVM在两个类别之间构建一个决策边界，以获得最佳的分类准确率。如果边界不是线性的，SVM将特征转换到更高维的空间，并获取一个非线性边界：
- en: '[PRE21]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Running this code should print out the evaluation dictionaries for each model,
    which tells you the accuracy, recall, and precision. You can also plot the confusion
    matrix (as we did in previous chapters) to visually see the false positives and
    negatives, and get an overall sense of how good the model is.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码应打印出每个模型的评估字典，这告诉你准确率、召回率和精确率。你还可以绘制混淆矩阵（就像我们在之前的章节中所做的那样），以直观地看到假阳性和假阴性，并总体了解模型的好坏。
- en: Playing around with the model
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在模型上玩弄
- en: 'We have explored here only three features – however, the possibilities for
    hand-crafted features are endless. I encourage you to experiment by adding more
    features to the mix. Examples of some features are as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里只探讨了三个特征——然而，手工制作特征的可能性是无限的。我鼓励你通过添加更多特征来实验。以下是一些特征的例子：
- en: Length of the text
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本长度
- en: Number of proper nouns
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 名词数量
- en: Number of numeric characters
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数字字符数量
- en: Average sentence length
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均句子长度
- en: Number of times the letter *q* was used
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字母 *q* 出现的次数
- en: This is certainly not an exhaustive list, and you should experiment by adding
    other features to see whether the model's performance improves.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这当然不是一份详尽的列表，你应该通过添加其他特征来实验，看看是否可以提高模型的表现。
- en: Automatic feature extraction
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自动特征提取
- en: In the previous section, we discussed how features can be engineered from text.
    However, hand-crafting features might not always be the best idea. This is because
    it requires expert knowledge. In this case, data scientists or machine learning
    engineers alone will not be able to design these features – they will need experts
    from linguistics and language studies to identify the nuances of language and
    suggest appropriate features such as the readability index. Additionally, the
    process is time-consuming; each feature has to be identified, implemented, and
    tested one after the other.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了如何从文本中构建特征。然而，手工制作特征可能并不总是最好的选择。这是因为它需要专业知识。在这种情况下，数据科学家或机器学习工程师单独将无法设计这些特征——他们需要来自语言学和语言研究的专家来识别语言的细微差别，并提出适当的特点，如可读性指数。此外，这个过程很耗时；每个特征都必须依次识别、实现和测试。
- en: We will now explore some methods for automatic feature extraction from text.
    This means that we do not manually design features such as the punctuation count,
    readability index, and so on. We will use existing models and techniques, which
    can take in the input text and generate a feature vector for us.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨一些从文本中自动提取特征的方法。这意味着我们不会手动设计诸如标点符号计数、可读性指数等特征。我们将使用现有的模型和技术，它们可以接受输入文本并为我们生成特征向量。
- en: TF-IDF
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TF-IDF
- en: '**Term Frequency – Inverse Document Frequency** (**TF-IDF**) is a commonly
    used technique in natural language processing to convert text into numeric features.
    Every word in the text is assigned a score that indicates how important the word
    is in that text. This is done by multiplying two metrics:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**词频-逆文档频率**（**TF-IDF**）是自然语言处理中常用的一种技术，用于将文本转换为数值特征。文本中的每个单词都被分配一个分数，表示该单词在该文本中的重要性。这是通过乘以两个指标来完成的：'
- en: '**Term Frequency**: How frequently does the word appear in the text sample?
    This can be normalized by the length of the text in words, as texts that differ
    in length by a large number can cause skews. The term frequency measures how common
    a word is in this particular text.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**词频**：单词在文本样本中出现的频率如何？这可以通过文本的单词长度进行归一化，因为长度差异较大的文本可能会导致偏差。词频衡量一个单词在这个特定文本中的普遍程度。'
- en: '**Inverse Document Frequency**: How frequently does the word appear in the
    rest of the corpus? First, the number of text samples containing this word is
    obtained. The total number of samples is divided by this number. Simply put, IDF
    is the inverse of the fraction of text samples containing the word. IDF measures
    how common the word is in the rest of the corpus.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逆文档频率**：这个词在其余语料库中出现的频率如何？首先，获得包含这个单词的文本样本数量。将总样本数除以这个数字。简单来说，IDF 是包含该单词的文本样本分数的倒数。IDF
    衡量该单词在其余语料库中的普遍程度。'
- en: For every word in each text, the TF-IDF score is a statistical measure of the
    importance of the word to the sentence. A word that is common in a text but rare
    in the rest of the corpus is surely important and a distinguishing characteristic
    of the text, and will have a high TF-IDF score. Alternately, a word that is very
    common in the corpus (that is, present in nearly all text samples) will not be
    a distinguishing one – it will have a low TF-IDF score.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每篇文本中的每个单词，TF-IDF 分数是衡量该单词对句子重要性的统计指标。一个在文本中常见但在其余语料库中罕见的单词肯定很重要，并且是文本的一个区分特征，将会有一个高的
    TF-IDF 分数。相反，一个在语料库中非常常见的单词（即在几乎所有文本样本中都存在）将不会是一个区分特征——它将有一个低的 TF-IDF 分数。
- en: In order to convert the text into a vector, we first calculate the TF-IDF score
    of each word in each text. Then, we replace the word with a sequence of TF-IDF
    scores corresponding to the words. The `scikit-learn` library provides us with
    an implementation of TF-IDF vectorization out of the box.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将文本转换为向量，我们首先计算每个文本中每个单词的TF-IDF分数。然后，我们将单词替换为与单词对应的TF-IDF分数序列。`scikit-learn`库为我们提供了一个TF-IDF向量化的实现。
- en: 'Note a fine nuance here: the goal of our experiment is to build a model for
    bot detection that can be used to classify new text as being generated by bots
    or not. Thus, when we are training, we have no idea about the test data that will
    come in the future. To ensure that we simulate this, we will do the TF-IDF score
    calculation over only the training data. When we vectorize the test data, we will
    simply use the calculated scores as a lookup:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这里的细微差别：我们实验的目标是构建一个用于检测机器人的模型，该模型可以用来分类新的文本是否由机器人生成。因此，当我们进行训练时，我们对未来将要到来的测试数据一无所知。为了确保我们能够模拟这种情况，我们将只对训练数据进行TF-IDF分数计算。当我们对测试数据进行向量化时，我们将简单地使用计算出的分数作为查找：
- en: '[PRE22]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You can manually inspect a few samples from the generated list. What do they
    look like?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以手动检查生成列表中的几个样本。它们看起来像什么？
- en: 'Now that we have the feature vectors, we can use them to train the classification
    models. The overall procedure remains the same: initialize a model, fit a model
    on the training data, and evaluate it on the testing data. The MLP example is
    shown here; however, you could replace this with any of the models we discussed:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了特征向量，我们可以使用它们来训练分类模型。整体流程保持不变：初始化一个模型，在训练数据上拟合模型，并在测试数据上评估它。这里展示了MLP示例；然而，你可以用我们讨论过的任何模型来替换它：
- en: '[PRE23]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: How does the performance of this model compare to the performance of the same
    model with handcrafted features? How about the performance of the other models?
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型的性能与具有手工特征的同模型性能相比如何？其他模型的性能又如何？
- en: Word embeddings
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: The TF-IDF approach is considered to be what we call a *bag of words* approach
    in machine learning terms. Each word is scored based on its presence, irrespective
    of the order in which it appears. Word embeddings are numeric representations
    of words assigned such that words that are similar in meaning have similar embeddings
    – the numeric representations are close to each other in the feature space. The
    most fundamental technique used to to generate word embeddings is called **Word2Vec**.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF方法在机器学习术语中被认为是所谓的“词袋”方法。每个词的得分基于其出现，而不考虑其出现的顺序。词嵌入是分配给单词的数值表示，使得在意义上相似的单词具有相似的嵌入——这些数值表示在特征空间中彼此接近。用于生成词嵌入的最基本技术被称为**Word2Vec**。
- en: Word2Vec embeddings are produced by a shallow neural network. Recall that the
    last layer of a classification model is a sigmoid or softmax layer for producing
    an output probability distribution. This softmax layer operates on the features
    it receives from the pre-final layer – these features can be treated as high-dimensional
    representations of the input. If we chop off the last layer, the neural network
    without the classification layer can be used to extract these embeddings.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec嵌入是由一个浅层神经网络产生的。回想一下，分类模型的最后一层是一个sigmoid或softmax层，用于产生输出概率分布。这个softmax层作用于它从预最终层接收到的特征——这些特征可以被视为输入的高维表示。如果我们移除最后一层，没有分类层的神经网络可以用来提取这些嵌入。
- en: 'Word2Vec can work in one of two ways:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec可以以两种方式工作：
- en: '`X` = *I went to walk the* and `Y` = *dog* would be one training example.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X` = *I went to walk the* 和 `Y` = *dog* 将是一个训练示例。'
- en: '**Skip-Gram**: This is the more widely used technique. Instead of predicting
    the target word, we train a model to predict the surrounding words. For example,
    if the text corpus contains the sentence *I went to walk the dog*, then our input
    would be *walk* and the output would be a prediction (or probabilistic prediction)
    of the surrounding two or more words. Because of this design, the model learns
    to generate similar embeddings for similar words. After the model is trained,
    we can pass the word of interest as an input, and use the features of the final
    layer as our embedding.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Skip-Gram**：这是更广泛使用的技术。我们不是预测目标词，而是训练一个模型来预测周围的词。例如，如果文本语料库包含句子 *I went to
    walk the dog*，那么我们的输入将是 *walk*，输出将是周围两个或更多词的预测（或概率预测）。由于这种设计，模型学会了为相似的词生成相似的嵌入。模型训练完成后，我们可以将感兴趣的词作为输入传递，并使用最终层的特征作为我们的嵌入。'
- en: Note that while this is still a classification task, it is not supervised learning.
    Rather, it is a self-supervised approach. We have no ground truth, but by framing
    the problem uniquely, we generate our own ground truth.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然这仍然是一个分类任务，但它不是监督学习。相反，它是一种自监督方法。我们没有地面实况，但通过独特地构建问题，我们生成了自己的地面实况。
- en: We will now build our word embedding model using the `gensim` Python library.
    We will fit the model on our training data, and then vectorize each sentence using
    the embeddings. After we have the vectors, we can fit and evaluate the models.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`gensim` Python库构建我们的词嵌入模型。我们将模型拟合到我们的训练数据上，然后使用嵌入对每个句子进行向量化。在我们得到向量后，我们可以拟合和评估模型。
- en: 'First, we fit the model on training data. Because of the way Word2Vec operates,
    we need to combine our texts into a list of sentences and then tokenize it into
    words:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在训练数据上拟合模型。由于Word2Vec的操作方式，我们需要将我们的文本组合成一个句子列表，然后将其分词成单词：
- en: '[PRE24]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we can fit the embedding model. By passing in the `vector_size` parameter,
    we control the size of the generated embedding. The larger the size, the more
    the expressive the power of the embeddings:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以拟合嵌入模型。通过传递`vector_size`参数，我们控制生成的嵌入的大小。大小越大，嵌入的表达能力越强：
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We now have the embedding model and can start using it to tokenize the text.
    Here, we have two strategies. One strategy is that we can calculate the embedding
    for all the words in the text and simply average them to find the mean embedding
    for the text. Here’s how we would do this:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们有了嵌入模型，可以开始使用它来对文本进行分词。这里，我们有两种策略。一种策略是，我们可以计算文本中所有单词的嵌入，然后简单地平均它们以找到文本的平均嵌入。以下是我们的操作方法：
- en: '[PRE26]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The `X_train_vector_mean` array now holds an embedding representation for each
    text in our corpus. The same process can be repeated to generate the feature set
    with test data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_train_vector_mean`数组现在为我们的语料库中的每个文本持有嵌入表示。可以使用相同的过程重复生成测试数据的特征集。'
- en: The second strategy is, instead of averaging the vectors, we append them one
    after the other. This retains more expressive power as it takes into account the
    order of words in the sentence. However, each text will have a different length
    and we require a fixed-size vector. Therefore, we take only a fixed number of
    words from the text and concatenate their embeddings.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种策略是，而不是平均向量，我们将它们一个接一个地附加。这保留了更多的表达能力，因为它考虑了句子中单词的顺序。然而，每个文本的长度都不同，我们需要一个固定大小的向量。因此，我们只从文本中取固定数量的单词，并将它们的嵌入连接起来。
- en: 'Here, we set the maximum number of words to be `40`. If a text has more than
    40 words, we will consider only the first 40\. If it has less than 40 words, we
    will consider all of the words and pad the remaining elements of the vector with
    zeros:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将最大单词数设置为`40`。如果一个文本超过40个单词，我们将只考虑前40个单词。如果少于40个单词，我们将考虑所有单词，并用零填充向量的剩余元素：
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The same code snippet can be repeated with the test data as well. Remember that
    the approach you use (averaging or appending) has to be consistent across training
    and testing data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的代码片段也可以用于测试数据。记住，你使用的（平均或附加）方法必须在训练和测试数据中保持一致。
- en: 'Now that the features are ready, we train and evaluate the model as usual.
    Here’s how you would do it with an MLP; this is easily extensible to other models
    we have seen:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在特征已经准备好了，我们像往常一样训练和评估模型。以下是如何使用MLP（多层感知器）来做这件事；这很容易扩展到我们看到的其他模型：
- en: '[PRE28]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Note that, here, the dimensions of the hidden layers we passed to the model
    are different from before. In the very first example with hand-crafted features,
    our feature vector was only three-dimensional. However, in this instance, every
    text instance will be represented by 40 words, and each word represented by a
    30-dimensional embedding, meaning that the feature vector has 1,200 elements.
    The higher number of neurons in the hidden layer helps handle the high-dimensional
    feature space.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，我们传递给模型的隐藏层维度与之前不同。在第一个使用手工特征的手工例子中，我们的特征向量只有三维。然而，在这个例子中，每个文本实例将由40个单词表示，每个单词由一个30维的嵌入表示，这意味着特征向量有1,200个元素。隐藏层中神经元数量的增加有助于处理高维特征空间。
- en: 'As an exercise, you are encouraged to experiment with three changes and check
    whether there is an improvement in the model performance:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项练习，我们鼓励你尝试三种变化，并检查模型性能是否有改进：
- en: The size of the word embeddings, which has been set to `30` for now. What happens
    to the model performance as you increase or decrease this number?
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前设置为`30`的词嵌入大小。当您增加或减少这个数字时，模型性能会发生什么变化？
- en: The number of words has been chosen as 0\. What happens if this is reduced or
    increased?
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词数量已被选择为0。如果这个数字减少或增加会发生什么？
- en: Using the MLP, how does the model performance change as you vary the number
    of layers?
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MLP，当您改变层数时，模型性能如何变化？
- en: Combine word embeddings and TF-IDF. Instead of a simple average, calculate a
    weighted average where the embedding for each word is weighted by the TF-IDF score.
    This will ensure that more important words influence the average more. How does
    this affect model performance?
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结合词嵌入和TF-IDF。而不是简单的平均，计算一个加权平均，其中每个词的嵌入由TF-IDF分数加权。这将确保更重要的词对平均的影响更大。这如何影响模型性能？
- en: Transformer methods for detecting automated text
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于检测自动化文本的Transformer方法
- en: In the previous sections, we have used traditional hand-crafted features, automated
    bag of words features, as well as embedding representations for text classification.
    We saw the power of BERT as a language model in the previous chapter. While describing
    BERT, we referenced that the embeddings generated by BERT can be used for downstream
    classification tasks. In this section, we will extract BERT embeddings for our
    classification task.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用了传统的手工特征、自动化的词袋特征以及文本分类的嵌入表示。我们在上一章中看到了BERT作为语言模型的力量。在描述BERT时，我们提到BERT生成的嵌入可以被用于下游分类任务。在本节中，我们将为我们的分类任务提取BERT嵌入。
- en: The embeddings generated by BERT are different from those generated by the Word2Vec
    model. Recall that in BERT, we use the masked language model and a transformer-based
    architecture based on attention. This means that the embedding of a word depends
    on the context in which it occurs; based on the surrounding words, BERT knows
    which other words to pay attention to and generate the embedding.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: BERT生成的嵌入与Word2Vec模型生成的嵌入不同。回想一下，在BERT中，我们使用掩码语言模型和基于注意力的Transformer架构。这意味着一个词的嵌入取决于它出现的上下文；基于周围的词，BERT知道应该关注哪些其他词并生成嵌入。
- en: In traditional word embeddings, a word will have the same embedding, irrespective
    of the context. The word *match* will have the same embedding in the sentence
    *They were a perfect match!* and *I lit a match last night*. BERT, on the other
    hand, conditions the embeddings based on context. The word *match* would have
    different embeddings in these two sentences.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的词嵌入中，一个词将具有相同的嵌入，无论其上下文如何。在句子"They were a perfect match!"和"I lit a match
    last night"中，单词*match*将具有相同的嵌入。另一方面，BERT根据上下文条件嵌入。在这两个句子中，单词*match*将具有不同的嵌入。
- en: 'Recall that we have already used BERT once, for malware detection. There are
    two major differences in how we use it now versus when we implemented it for malware
    detection:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们之前已经使用BERT进行了一次恶意软件检测。现在我们使用它与我们之前为恶意软件检测实现它的方式有两个主要区别：
- en: Previously, we used BERT in the fine-tuning mode. This means that we used the
    entire transformer architecture initialized with pretrained weights, added a neural
    network on top of it, and trained the whole model end to end. The pretrained model
    enabled learning sequence features, and the fine-tuning helped adapt it to the
    specific task. However, now we will use BERT only as a feature extractor. We will
    load a pretrained model, run the sentence through it, and use the pre-final layer
    to construct our features.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之前，我们使用BERT在微调模式下。这意味着我们使用了整个初始化为预训练权重的transformer架构，在其之上添加了一个神经网络，并从头到尾训练整个模型。预训练模型使学习序列特征成为可能，微调有助于将其适应特定任务。然而，现在我们只将BERT用作特征提取器。我们将加载一个预训练模型，将其运行通过句子，并使用预最终层来构建我们的特征。
- en: In the previous chapter, we used TensorFlow for implementing BERT. Now, we will
    use PyTorch, a deep learning framework developed by researchers from Facebook.
    This provides a much more intuitive, straightforward, and understandable interface
    to design and run deep neural networks. It also has a `transformers` library,
    which provides easy implementations of all pretrained models.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了TensorFlow来实现BERT。现在，我们将使用由Facebook研究人员开发的深度学习框架PyTorch。这提供了一个更加直观、直接和易于理解的用户界面来设计和运行深度神经网络。它还包含一个`transformers`库，该库提供了所有预训练模型的简单实现。
- en: 'First, we will initialize the BERT model and set it to evaluation mode. In
    the evaluation mode, there is no learning, just inferencing. Therefore, we need
    only the forward pass and no backpropagation:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将初始化BERT模型并将其设置为评估模式。在评估模式下，没有学习，只有推理。因此，我们只需要前向传递而不需要反向传播：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We will now prepare our data in the format needed by BERT. This includes adding
    the two special tokens to indicate the start and separation. Then, we will run
    the model in inference mode to obtain the embeddings (hidden states). Recall that
    when we used Word2Vec embeddings, we averaged the embeddings for each word. In
    the case of BERT embeddings, we have multiple choices:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将准备BERT所需的数据格式。这包括添加两个特殊标记来指示开始和分隔。然后，我们将以推理模式运行模型以获得嵌入（隐藏状态）。回想一下，当我们使用Word2Vec嵌入时，我们对每个单词的嵌入进行了平均。在BERT嵌入的情况下，我们有多个选择：
- en: 'Use just the last hidden state as the embedding:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用最后一个隐藏状态作为嵌入：
- en: '[PRE30]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[PRE31]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[PRE32]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[PRE34]'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[PRE37]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[PRE38]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[PRE40]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Use the sum of all hidden states as the embedding:'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用所有隐藏状态的和作为嵌入：
- en: '[PRE43]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '[PRE45]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '[PRE46]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '[PRE48]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '[PRE49]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '[PRE52]'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '[PRE53]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '[PRE55]'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[PRE56]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Use the sum of the last four layers as an embedding:'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最后四层的和作为嵌入：
- en: '[PRE57]'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-213
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '[PRE60]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '[PRE61]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '[PRE62]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '[PRE63]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '[PRE64]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '[PRE65]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '[PRE66]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[PRE67]'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '[PRE68]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[PRE69]'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[PRE70]'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Concatenate the last four layers and use that as the embedding:'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将最后四层连接起来并用作嵌入：
- en: '[PRE71]'
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[PRE72]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[PRE73]'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '[PRE74]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: '[PRE75]'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: '[PRE76]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '[PRE77]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[PRE78]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: '[PRE79]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: '[PRE80]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: '[PRE81]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '[PRE82]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: '[PRE83]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '[PRE84]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Once we have the BERT features, we train and evaluate the model using our usual
    methodology. We will show an example of an MLP here, but the same process can
    be repeated for all the classifiers:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了BERT特征，我们就会使用我们通常的方法来训练和评估模型。这里我们将展示一个MLP的例子，但同样的过程可以重复用于所有分类器：
- en: '[PRE85]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: This completes our analysis of how transformers can be used to detect machine-generated
    text.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对如何使用变压器检测机器生成文本的分析。
- en: Compare and contrast
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较和对比
- en: 'By now, we have explored several techniques for detecting bot-generated news.
    Here’s a list of all of them:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经探索了几种检测机器人生成新闻的技术。以下是所有这些技术的列表：
- en: Hand-crafted features such as function words, punctuation words, and automated
    readability index
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手工制作的特征，如功能词、标点符号词和自动可读性指数
- en: TF-IDF scores for words
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单词的TF-IDF分数
- en: 'Word2Vec embeddings:'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec嵌入：
- en: Averaged across the text for all words
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有单词的文本中平均
- en: Concatenated for each word across the text
  id: totrans-250
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在文本中每个单词上连接
- en: 'BERT embeddings:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: BERT嵌入：
- en: Using only the last hidden state
  id: totrans-252
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅使用最后一个隐藏状态
- en: Using the sum of all hidden states
  id: totrans-253
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用所有隐藏状态的和
- en: Using the sum of the last four hidden states
  id: totrans-254
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最后四个隐藏状态的和
- en: Using the concatenation of the last four hidden states
  id: totrans-255
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最后四个隐藏状态的连接
- en: 'We can see that we have eight feature sets at our disposal. Additionally, we
    experimented with four different models: random forests, logistic regression,
    SVM, and deep neural network (MLP). This means that we have a total of 32 configurations
    (feature set `x` model) that we can use for building a classifier to detect bot-generated
    fake news.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们有八个特征集可供使用。此外，我们还尝试了四种不同的模型：随机森林、逻辑回归、SVM和深度神经网络（MLP）。这意味着我们总共有32种配置（特征集`x`模型）可以用来构建一个分类器以检测机器人生成的虚假新闻。
- en: I leave it up to you to construct this 8x4 matrix and determine which is the
    best approach among all of them!
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我把它留给你来构建这个8x4矩阵，并确定在所有这些方法中哪种是最好的！
- en: Summary
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we described approaches and techniques for detecting bot-generated
    fake news. With the rising prowess of artificial intelligence and the widespread
    availability of language models, attackers are using automated text generation
    to run bots on social media. These sock-puppet accounts can generate real-looking
    responses, posts, and, as we saw, even news-style articles. Data scientists in
    the security space, particularly those working in the social media domain, will
    often be up against attackers who leverage AI to spew out text and carpet-bomb
    a platform.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们描述了检测机器人生成虚假新闻的方法和技术。随着人工智能能力的提升和语言模型的广泛应用，攻击者正在使用自动文本生成在社交媒体上运行机器人。这些木偶账户可以生成看起来真实的回复、帖子，正如我们所看到的，甚至新闻风格的文章。在安全领域的数据科学家，尤其是在社交媒体领域工作的数据科学家，经常会遇到利用AI喷出文本并地毯式轰炸平台的攻击者。
- en: This chapter aims to equip practitioners against such adversaries. We began
    by understanding how text generation exactly works and created our own dataset
    for machine learning experiments. We then used a variety of features (hand-crafted,
    TF-IDF, and word embeddings) to detect the bot-generated text. Finally, we used
    contextual embeddings to build improved mechanisms.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 本章旨在为从业者提供对抗此类对手的准备。我们首先理解了文本生成是如何精确工作的，并为机器学习实验创建了我们的数据集。然后，我们使用了各种特征（手工制作、TF-IDF和词嵌入）来检测由机器人生成的文本。最后，我们使用了上下文嵌入来构建改进的机制。
- en: In the next chapter, we will study the problem of authorship attribution and
    obfuscation and the social and technical issues surrounding it.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将研究作者归属和混淆的问题，以及围绕它的社会和技术问题。
