- en: Introduction to Feature Engineering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程简介
- en: 'In recent years, engineers and executives have been attempting to implement
    **machine learning** (**ML**) and **artificial intelligence** (**AI**) to solve
    problems that, for the most part, have been solved using fairly manual methodologies.
    A great example would have to be advancements in **natural language processing**
    (**NLP**) and more specifically in natural language generation and understanding.
    Even more specifically, we point to AI systems that are able to read in raw text
    from a user (perhaps a disgruntled user of the latest smartphone) and can articulately
    and accurately respond with the prose of a human and the speed of a machine. In
    this chapter, we will be introducing topics of feature engineering, such as:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，工程师和执行人员一直在尝试实施**机器学习**（**ML**）和**人工智能**（**AI**）来解决大部分使用相当手动方法解决的问题。一个很好的例子就是**自然语言处理**（**NLP**）的进步，更具体地说是在自然语言生成和理解方面。更具体地说，我们指的是能够从用户那里读取原始文本的人工智能系统（可能是最新智能手机的不满用户），并且能够以人类的散文和机器的速度准确、清晰地回应。在本章中，我们将介绍特征工程等主题，例如：
- en: Motivating examples of why feature engineering matters
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么特征工程很重要的激励性例子
- en: Basic understanding of machine learning, including performance, evaluation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对机器学习的基本理解，包括性能、评估
- en: A detailed list of the chapters included in this book
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本书包含的章节的详细列表
- en: Motivating example – AI-powered communications
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激励性例子 - AI驱动的通信
- en: Meet Arty, our AI chat system that is able to handle and respond to incoming
    customer support requests, just as any of our human employees would. Arty is endowed
    with the knowledge of our company and is ready to go at a moment’s notice.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 来认识一下Arty，我们的AI聊天系统，它能够处理和回应客户支持请求，就像我们任何人类员工一样。Arty拥有我们公司的知识，随时准备出发。
- en: 'Here is how a sample dialogue between a human and an AI customer support system
    would transpire:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个人类和AI客户支持系统之间样本对话的进行方式：
- en: '| **Human** | **AI** |'
  id: totrans-8
  prefs: []
  type: TYPE_TB
  zh: '| **人类** | **AI** |'
- en: '| Hello, my phone is broken. | Sorry to hear that, how is it broken? |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 我的手机坏了。 | 很抱歉听到这个，它是怎么坏的？ |'
- en: '| It’s frozen and I can’t reset it. | What kind of phone is it? |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 它冻住了，我无法重置它。 | 这是什么手机？ |'
- en: '| The new iDroid 28 | Ahh, I see. Hold the power and volume down button for
    20 seconds and it should reset. |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 新的iDroid 28 | 哦，我明白了。按住电源和音量下键20秒，它应该会重置。 |'
- en: '| It worked, thanks! | No problem, have a great day. |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 它工作了，谢谢！ | 没问题，祝你有个美好的一天。 |'
- en: 'The reason that these types of systems are exciting and are disrupting major
    markets is the simplicity of such a complicated system. Let us break it down.
    On the surface, you might think, *what an easy problem!* The person has a simple
    problem with a simple solution. A request comes in and a response comes out. *Hello,
    my phone froze, what should I do?* Easy, just reset it. And sure, on the surface,
    that is what is happening here:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这些类型系统令人兴奋并颠覆主要市场的原因是这样一个复杂系统的简单性。让我们来分解一下。表面上，你可能认为，*这是一个多么简单的问题!* 这个人有一个简单的问题，有一个简单的解决方案。一个请求进来，一个响应出来。*你好，我的手机冻住了，我该怎么办？*
    简单，只需重置它。当然，表面上，这里发生的就是这样：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The tough part comes in when you look at it from the AI’s perspective. It hasn’t
    had the entire human experience that we have had. It hasn’t had the privilege
    to read *The Illiad* or even *Clifford the Big Red Dog* and learn to internalize
    their messages. The point is, the AI hasn’t had a lot of experience in reading
    things. This AI has probably been given a few hundred thousand (maybe even millions)
    of previous chat dialogues of people in the past and was told to figure it out.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 当从人工智能的角度来看时，问题就来了。它没有经历过我们经历的人类全部体验。它没有阅读过*《伊利亚特》*或甚至*《大红狗克利福德》*并学习内化它们的信息的特权。关键是，人工智能在阅读事物方面没有太多的经验。这个人工智能可能被给了几十万（甚至可能是几百万）以前人们的聊天对话，并被告诉去弄清楚。
- en: 'The following is a sample of data given to our AI system based on previous
    chat logs:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们AI系统基于以前聊天日志给出的样本数据：
- en: '| **Request** | **Response** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **请求** | **响应** |'
- en: '| Helllo | Hi, what seems to be the problem? |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 嗨 | 你好，有什么问题吗？ |'
- en: '| My phone doesn’t work!!!! | Oh noo!!!! What’s going on with it? |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 我的手机不工作了！！！！ | 哦不！！！它怎么了？ |'
- en: '| >Hold on, I have to go walk my dog. BRB. | OK. I will wait. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| >等等，我得去遛狗。待会儿回来。 | 好的。我会等你的。 |'
- en: '| Hey. | Hello. My name is Mark, how can I help? |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 嗨。 | 你好。我叫马克，我能帮什么忙？ |'
- en: The data is organized into two columns where the `Request` column represents
    what the end user types into a chat support dialogue. The next column, `Response`,
    represents the customer support agent’s response to the incoming message.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 数据被组织成两列，其中`Request`列表示最终用户在聊天支持对话中输入的内容。下一列，`Response`，表示客户支持代理对传入消息的回复。
- en: While reading over the thousands of typos, angry messages, and disconnected
    chats, the AI starts to think that it has this customer support thing down. Once
    this happens, the humans set the AI loose on new chats coming in. The humans,
    not realizing their mistake, start to notice that the AI hasn’t fully gotten the
    hang of this yet. The AI can’t seem to recognize even simple messages and keeps
    returning nonsensical responses. It’s easy to think that the AI just needs more
    time or more data, but these solutions are just band-aids to the bigger problem,
    and often do not even solve the issue in the first place.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读数千个错别字、愤怒的消息和脱节的聊天之后，AI开始认为自己已经掌握了这项客户支持工作。一旦发生这种情况，人类就会让AI处理新收到的聊天。人类没有意识到自己的错误，开始注意到AI还没有完全掌握这项工作。AI似乎无法识别甚至简单的消息，并不断返回无意义的回复。人们很容易认为AI只需要更多的时间或更多的数据，但这些解决方案只是对更大问题的临时补救，而且往往甚至不能解决根本问题。
- en: 'The underlying problem is likely that the data given to the AI in the form
    of raw text wasn’t good enough and the AI wasn’t able to pick up on the nuances
    of the English language. For example, some of the problems would likely include:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 根本问题可能是，提供给AI的原始文本数据不够好，AI无法捕捉到英语语言的细微差别。例如，可能的问题包括：
- en: Typos artificially expand the AI’s vocabulary without cause. *Helllo* and *hello*
    are two different words that are not related to each other.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错别字人为地扩大了AI的词汇量，却没有原因。*Helllo*和*hello*是两个不同的词，彼此之间没有关联。
- en: Synonyms mean nothing to the AI. Words such as *hello* and *hey* have no similarity
    and therefore make the problem artificially harder.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同义词对AI来说毫无意义。例如，*hello*和*hey*这两个词没有任何相似之处，因此人为地增加了问题的难度。
- en: Why feature engineering matters
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么特征工程很重要
- en: Data scientists and machine learning engineers frequently gather data in order
    to solve a problem. Because the problem they are attempting to solve is often
    highly relevant and exists and occurs naturally in this messy world, the data
    that is meant to represent the problem can also end up being quite messy and unfiltered,
    and often incomplete.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家和机器学习工程师经常收集数据以解决问题。因为他们试图解决的问题通常与实际情况高度相关，并且在这个混乱的世界中自然存在，所以旨在代表问题的数据也可能非常混乱、未经筛选，并且常常不完整。
- en: This is why in the past several years, positions with titles such as *Data Engineer*
    have been popping up. These engineers have the unique job of engineering pipelines
    and architectures designed to handle and transform raw data into something usable
    by the rest of the company, particularly the data scientists and machine learning
    engineers. This job is not only as important as the machine learning experts’
    job of creating machine learning pipelines, it is often overlooked and undervalued.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么在过去的几年里，诸如*数据工程师*等头衔的职位不断涌现。这些工程师的独特任务是构建管道和架构，用于处理和转换原始数据，使其能够被公司其他部门使用，尤其是数据科学家和机器学习工程师。这项工作不仅与机器学习专家创建机器学习管道的工作一样重要，而且常常被忽视和低估。
- en: A survey conducted by data scientists in the field revealed that over 80% of
    their time was spent capturing, cleaning, and organizing data. The remaining less
    than 20% of their time was spent creating these machine learning pipelines that
    end up dominating the conversation. Moreover, these data scientists are spending
    most of their time preparing the data; more than 75% of them also reported that
    preparing data was the least enjoyable part of their process.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 由数据科学家进行的调查显示，他们超过80%的时间用于捕获、清理和组织数据。剩余的不到20%的时间用于创建这些最终主导对话的机器学习管道。此外，这些数据科学家的大部分时间都花在准备数据上；超过75%的人还报告说，准备数据是他们流程中最不愉快的一部分。
- en: 'Here are the findings of the survey mentioned earlier:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是之前提到的调查结果：
- en: 'Following is the graph of the what Data Scientist spend the most time doing:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了数据科学家花费最多时间做的事情：
- en: '![](img/842bd915-a8ce-4600-a5cf-58f6e238686d.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/842bd915-a8ce-4600-a5cf-58f6e238686d.png)'
- en: 'As seen from the preceding graph, we breakup the Data Scientists''s task in
    the following percentage :'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 从前面的图表中可以看出，我们将数据科学家的任务分解为以下百分比：
- en: '**Building training sets**: 3%'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建训练集**：3%'
- en: '**Cleaning and organizing data**: 60%'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洗和整理数据**：60%'
- en: '**Collecting data for sets**: 19%'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集数据集**：19%'
- en: '**Mining data for patterns**: 9%'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挖掘数据模式**：9%'
- en: '**Refining algorithms**: 5%'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法**：5%'
- en: 'A similar pie diagram for what is the least enjoyable part of data science:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据科学中最不愉快部分相似的饼图：
- en: '![](img/3266eab3-da62-484c-975a-8eacc7f272c3.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3266eab3-da62-484c-975a-8eacc7f272c3.png)'
- en: 'From the graph a similar poll for the least enjoyable part of data science
    revealed:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从图表中可以看出，对于数据科学中最不愉快部分的类似调查结果如下：
- en: '**Building training sets**: 10 %'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构建训练集**：10 %'
- en: '**Cleaning and organizing data**: 57%'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清洗和整理数据**：57%'
- en: '**Collecting data sets**: 21%'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**收集数据集**：21%'
- en: '**Mining for data patterns**: 3%'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**挖掘数据模式**：3%'
- en: '**Refining algorithms**: 4%'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化算法**：4%'
- en: '**Others**: 5%'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他**：5%'
- en: The uppermost chart represents the percentage of time that data scientists spend
    on different parts of the process. Over 80% of a data scientists' time is spent
    preparing data for further use. The lower chart represents the percentage of those
    surveyed reporting their least enjoyable part of the process of data science.
    Over 75% of them report that preparing data is their least enjoyable part.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 最上面的图表表示数据科学家在不同部分上花费的时间百分比。超过80%的数据科学家时间用于准备数据以供进一步使用。下面的图表表示那些被调查的人报告的数据科学过程中最不愉快部分的比例。其中超过75%的人报告说准备数据是他们最不愉快的一部分。
- en: 'Source of the data: [https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/](https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 数据来源：[https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/](https://whatsthebigdata.com/2016/05/01/data-scientists-spend-most-of-their-time-cleaning-data/).
- en: A stellar data scientist knows that preparing data is not only so important
    that it takes up most of their time, they also know that it is an arduous process
    and can be unenjoyable. Far too often, we take for granted clean data given to
    us by machine learning competitions and academic sources. More than 90% of data,
    the data that is interesting, and the most useful, exists in this raw format,
    like in the AI chat system described earlier.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一位杰出的数据科学家知道，准备数据不仅非常重要，占据了他们大部分的时间，而且他们也知道这是一个艰巨的过程，可能并不愉快。我们往往过于理所当然地接受机器学习竞赛和学术来源提供的干净数据。超过90%的数据，有趣的数据，最有用的数据，都存在于这种原始格式中，就像之前描述的AI聊天系统中的数据一样。
- en: '**Preparing data** can be a vague phrase. Preparing takes into account capturing
    data, storing data, cleaning data, and so on. As seen in the charts shown earlier,
    a smaller, but still majority chunk of a data scientist''s time is spent on cleaning
    and organizing data. It is in this process that our Data Engineers are the most
    useful to us. Cleaning refers to the process of transforming data into a format
    that can be easily interpreted by our cloud systems and databases. Organizing
    generally refers to a more radical transformation. Organizing tends to involve
    changing the entire format of the dataset into a much neater format, such as transforming
    raw chat logs into a tabular row/column structure.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**准备数据**可能是一个模糊的短语。准备包括捕获数据、存储数据、清洗数据等。如前述图表所示，数据科学家的大部分时间，尽管比例较小，但仍然是一大部分，用于清洗和整理数据。在这个过程中，我们的数据工程师对我们最有用。清洗是指将数据转换成可以被我们的云系统和数据库轻松解释的格式的过程。整理通常指的是更彻底的转换。整理往往涉及将整个数据集的格式改变成一个更整洁的格式，例如将原始聊天记录转换成表格的行/列结构。'
- en: 'Here is an illustration of **Cleaning** and **Organizing**:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是**清洗**和**整理**的示例：
- en: '![](img/7541d1c9-4f5c-41c7-8fe7-8f0528f84ec7.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7541d1c9-4f5c-41c7-8fe7-8f0528f84ec7.png)'
- en: The top transformation represents cleaning up a sample of server logs that include
    both the data and a text explanation of what is occurring on the servers. Notice
    that while cleaning, the **&amp;** character, which is a Unicode character, was
    transformed into a more readable ampersand (**&**). The cleaning phase left the
    document pretty much in the same exact format as before. The bottom organizing
    transformation was a much more radical one. It turned the raw document into a
    row/column structure, in which each row represents a single action taken by the
    server and the columns represent attributes of the server action. In this case,
    the two attributes are **D****ate** and **Text**.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部的转换表示清理包含数据和服务器上发生情况的文本解释的服务器日志样本。请注意，在清理过程中，Unicode字符`&amp;`被转换成了更易读的符号（`&`）。清理阶段使文档几乎保持了之前的完全相同的格式。底部的组织转换则是一个更为激进的转换。它将原始文档转换成了行/列结构，其中每一行代表服务器执行的单个操作，而列代表服务器操作的特征。在这种情况下，两个特征是**日期**和**文本**。
- en: Both cleaning and organizing fall under a larger category of data science, which
    just so happens to be the topic of this book, feature engineering.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 清洁和组织都属于数据科学的一个更广泛的类别，恰好这也是本书的主题，即特征工程。
- en: What is feature engineering?
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是特征工程？
- en: Finally, the title of the book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，本书的标题。
- en: Yes, folks, feature engineering will be the topic of this book. We will be focusing
    on the process of cleaning and organizing data for the purposes of machine learning
    pipelines. We will also go beyond these concepts and look at more complex transformations
    of data in the forms of mathematical formulas and neural understanding, but we
    are getting ahead of ourselves. Let’s start a high level.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，朋友们，特征工程将是这本书的主题。我们将专注于为机器学习管道清理和组织数据的过程。我们还将超越这些概念，探讨数据更复杂的转换形式，如数学公式和神经理解，但我们现在有些过于超前了。让我们从高层次开始。
- en: '**Feature engineering** is the process of transforming data into features that
    better represent the underlying problem, resulting in improved machine learning
    performance.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征工程**是将数据转换为更好地代表潜在问题的特征的过程，从而提高机器学习性能。'
- en: 'To break this definition down a bit further, let''s look at precisely what
    feature engineering entails:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步解释这个定义，让我们看看特征工程究竟包含哪些内容：
- en: '**Process of transforming data**: Note that we are not specifying raw data,
    unfiltered data, and so on. Feature engineering can be applied to data at any
    stage. Oftentimes, we will be applying feature engineering techniques to data
    that is already *processed* in the eyes of the data distributor. It is also important
    to mention that the data that we will be working with will usually be in a tabular
    format. The data will be organized into rows (observations) and columns (attributes).
    There will be times when we will start with data at its most raw form, such as
    in the examples of the server logs mentioned previously, but for the most part,
    we will deal with data already somewhat cleaned and organized.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换数据的过程**：请注意，我们并没有指定原始数据、未过滤数据等。特征工程可以应用于数据的任何阶段。通常情况下，我们将对数据分销者眼中的已处理数据进行特征工程。同样重要的是要提到，我们将要处理的数据通常将以表格格式存在。数据将被组织成行（观测值）和列（属性）。有时，我们将从数据的最原始形式开始，例如在之前提到的服务器日志示例中，但大部分情况下，我们将处理已经有一定程度清洁和组织的数据。'
- en: '**Features**: The word features will obviously be used a lot in this book.
    At its most basic level, a feature is an attribute of data that is meaningful
    to the machine learning process. Many times we will be diagnosing tabular data
    and identifying which columns are features and which are merely attributes.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征**：在这本书中，显然会大量使用“特征”这个词。在最基本的意义上，特征是数据的一个属性，对机器学习过程有意义。很多时候，我们将诊断表格数据，并确定哪些列是特征，哪些仅仅是属性。'
- en: '**Better represent the underlying problem**: The data that we will be working
    with will always serve to represent a specific problem in a specific domain. It
    is important to ensure that while we are performing these techniques, we do not
    lose sight of the bigger picture. We want to transform data so that it better
    represents the bigger problem at hand.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更好地代表潜在问题**：我们将要处理的数据始终用于在特定领域代表特定问题。在执行这些技术的同时，我们确保不失去对整体情况的关注是很重要的。我们希望转换数据，使其更好地代表手头的大问题。'
- en: '**Resulting in improved machine learning performance**: Feature engineering
    exists as a single part of the process of data science. As we saw, it is an important
    and oftentimes undervalued part. The eventual goal of feature engineering is to
    obtain data that our learning algorithms will be able to extract patterns from
    and use in order to obtain better results. We will talk in depth about machine
    learning metrics and results later on in this book, but for now, know that we
    perform feature engineering not only to obtain cleaner data, but to eventually
    use that data in our machine learning pipelines.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**导致机器学习性能提升**：特征工程是数据科学过程中的一个单独部分。正如我们所见，它是一个重要且经常被低估的部分。特征工程的最终目标是获取我们的学习算法能够从中提取模式并用于获得更好结果的数据。我们将在本书的后面深入讨论机器学习指标和结果，但到目前为止，要知道我们进行特征工程不仅是为了获得更干净的数据，而且是为了最终在我们的机器学习管道中使用这些数据。'
- en: We know what you’re thinking, *why should I spend my time reading about a process
    that people say they do not enjoy doing?* We believe that many people do not enjoy
    the process of feature engineering because they often do not have the benefits
    of understanding the results of the work that they do.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道你在想什么，*为什么我应该花时间阅读关于一个人们说他们不喜欢做的事情的过程？* 我们认为许多人不喜欢特征工程的过程，因为他们往往没有理解他们所做工作的结果的益处。
- en: Most companies employ both data engineers and machine learning engineers. The
    data engineers are primarily concerned with the preparation and transformation
    of the data, while the machine learning engineers usually have a working knowledge
    of learning algorithms and how to mine patterns from already cleaned data.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数公司都雇佣了数据工程师和机器学习工程师。数据工程师主要关注数据的准备和转换，而机器学习工程师通常对学习算法以及如何从已经清洗过的数据中挖掘模式有实际的知识。
- en: Their jobs are often separate but intertwined and iterative. The data engineers
    will present a dataset for the machine learning engineers, which they will claim
    they cannot get good results from, and ask the Data Engineers to try to transform
    the data further, and so on, and so forth. This process can not only be monotonous
    and repetitive, it can also hurt the bigger picture.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的工作往往是分开的，但又是相互交织和迭代的。数据工程师将向机器学习工程师展示一个数据集，他们声称无法从中获得好的结果，并要求数据工程师进一步转换数据，等等。这个过程不仅可能单调且重复，还可能损害整体大局。
- en: Without having knowledge of both feature and machine learning engineering, the
    entire process might not be as effective as it could be. That’s where this book
    comes in. We will be talking about feature engineering and how it relates directly
    to machine learning. It will be a results-driven approach where we will deem techniques
    as helpful if, and only if, they can lead to a boost in performance. It is worth
    now diving a bit into the basics of data, the structure of data, and machine learning,
    to ensure standardization of terminology.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果没有关于特征和机器学习工程的知识，整个过程可能不会像它本可以的那样有效。这就是这本书的作用所在。我们将讨论特征工程以及它与机器学习的直接关系。这将是一个以结果为导向的方法，我们将认为技术是有帮助的，如果并且只有如果它们能够提高性能。现在深入探讨数据、数据结构和机器学习的基础是值得的，以确保术语的标准化。
- en: Understanding the basics of data and machine learning
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据和机器学习的基础
- en: When we talk about data, we are generally dealing with tabular data, that is,
    data that is organized into rows and columns. Think of this as being able to be
    opened in a spreadsheet technology such as Microsoft Excel. Each row of data,
    otherwise known as an **observation**, represents a single instance/example of
    a problem. If our data belongs to the domain of day-trading in the stock market,
    an observation might represent an hour’s worth of changes in the overall market
    and price.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论数据时，我们通常处理的是表格数据，即组织成行和列的数据。想象一下，这些数据可以在像Microsoft Excel这样的电子表格技术中打开。每一行数据，也称为**观察结果**，代表了一个问题的单个实例/例子。如果我们的数据属于股票市场日内交易领域，一个观察结果可能代表整个市场及价格的一个小时的变化。
- en: For example, when dealing with the domain of network security, an observation
    could represent a possible attack or a packet of data sent over a wireless system.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在处理网络安全领域时，一个观察结果可能代表一次可能的攻击或通过无线系统发送的数据包。
- en: 'The following shows sample tabular data in the domain of cyber security and
    more specifically, network intrusion:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下展示了网络安全领域以及更具体地说，网络入侵领域的样本表格数据：
- en: '| **DateTime** | **Protocol** | **Urgent** | **Malicious** |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '| **日期时间** | **协议** | **紧急** | **恶意** |'
- en: '| June 2nd, 2018 | TCP | FALSE | TRUE |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月2日 | TCP | 否 | 是 |'
- en: '| June 2nd, 2018 | HTTP | TRUE | TRUE |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月2日 | HTTP | 是 | 是 |'
- en: '| June 2nd, 2018 | HTTP | TRUE | FALSE |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月2日 | HTTP | 是 | 否 |'
- en: '| June 3rd, 2018 | HTTP | FALSE | TRUE |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月3日 | HTTP | 否 | 是 |'
- en: 'We see that each row or observation consists of a network connection and we
    have four attributes of the observation: `DateTime`, `Protocol`, `Urgent`, and
    `Malicious`. While we will not dive into these specific attributes, we will simply
    notice the structure of the data given to us in a tabular format.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，每一行或观察结果都包含一个网络连接，并且我们有四个观察属性：`DateTime`（日期时间）、`Protocol`（协议）、`Urgent`（紧急）和`Malicious`（恶意）。虽然我们不会深入探讨这些特定属性，但我们会注意到以表格格式给出的数据结构。
- en: Because we will, for the most part, consider our data to be tabular, we can
    also look at specific instances where the matrix of data has only one column/attribute.
    For example, if we are building a piece of software that is able to take in a
    single image of a room and output whether or not there is a human in that room.
    The data for the input might be represented as a matrix of a single column where
    the single column is simply a URL to a photo of a room and nothing else.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们大部分时间都会将数据视为表格形式，我们还可以查看数据矩阵只有一列/属性的具体实例。例如，如果我们正在构建一个能够接收单个房间图像并输出该房间是否有人存在的软件，输入数据可能表示为一个单列矩阵，其中单列只是一个指向房间照片的URL，没有其他内容。
- en: 'For example, considering the following table of table that has only a single
    column titled, `Photo URL`. The values of the table are URLs (these are fake and
    do not lead anywhere and are purely for example) of photos that are relevant to
    the data scientist:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下只有一个名为`Photo URL`的列的表格。表格的值是照片的URL（这些都是虚构的，不会导向任何地方，仅用于示例）：
- en: '| Photo URL |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 照片URL |'
- en: '| [http://photo-storage.io/room/1](http://photo-storage.io/room/1) |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| [http://photo-storage.io/room/1](http://photo-storage.io/room/1) |'
- en: '| [http://photo-storage.io/room/2](http://photo-storage.io/room/2) |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| [http://photo-storage.io/room/2](http://photo-storage.io/room/2) |'
- en: '| [http://photo-storage.io/room/3](http://photo-storage.io/room/3) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| [http://photo-storage.io/room/3](http://photo-storage.io/room/3) |'
- en: '| [http://photo-storage.io/room/4](http://photo-storage.io/room/4) |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| [http://photo-storage.io/room/4](http://photo-storage.io/room/4) |'
- en: The data that is inputted into the system might only be a single column, such
    as in this case. In our ability to create a system that can analyze images, the
    input might simply be a URL to the image in question. It would be up to us as
    data scientists to engineer features from the URL.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 输入到系统中的数据可能只有一列，例如在这个案例中。在我们创建能够分析图像的系统时，输入可能只是一个指向图像的URL。作为数据科学家，我们将负责从URL中提取特征。
- en: As data scientists, we must be ready to ingest and handle data that might be
    large, small, wide, narrow (in terms of attributes), sparse in completion (there
    might be missing values), and be ready to utilize this data for the purposes of
    machine learning. Now’s a good time to talk more about that. Machine learning
    algorithms belong to a class of algorithms that are defined by their ability to
    extract and exploit patterns in data to accomplish a task based on historical
    training data. Vague, right? machine learning can handle many types of tasks,
    and therefore we will leave the definition of machine learning as is and dive
    a bit deeper.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 作为数据科学家，我们必须准备好处理可能很大、很小、宽泛、狭窄（就属性而言）、完成度低（可能存在缺失值）的数据，并准备好利用这些数据进行机器学习。现在是讨论这个话题的好时机。机器学习算法属于一类算法，它们通过从数据中提取和利用模式来完成基于历史训练数据的任务。这听起来很模糊，对吧？机器学习可以处理许多类型的任务，因此我们将机器学习的定义保持不变，并深入探讨一下。
- en: We generally separate machine learning into two main types, supervised and unsupervised
    learning. Each type of machine learning algorithm can benefit from feature engineering,
    and therefore it is important that we understand each type.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将机器学习分为两种主要类型：监督学习和无监督学习。每种机器学习算法都可以从特征工程中受益，因此了解每种类型都很重要。
- en: Supervised learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: 'Oftentimes, we hear about feature engineering in the specific context of supervised
    learning, otherwise known as predictive analytics. Supervised learning algorithms
    specifically deal with the task of predicting a value, usually one of the attributes
    of the data, using the other attributes of the data. Take, for example, the dataset
    representing the network intrusion:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 很多次，我们在监督学习的特定背景下听到特征工程，也称为预测分析。监督学习算法专门处理使用数据的其他属性来预测一个值（通常是数据的一个属性）的任务。以表示网络入侵的数据集为例：
- en: '| **DateTime** | **Protocol** | **Urgent** | **Malicious** |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| **DateTime** | **Protocol** | **Urgent** | **Malicious** |'
- en: '| June 2nd, 2018 | TCP | FALSE | TRUE |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月2日 | TCP | FALSE | TRUE |'
- en: '| June 2nd, 2018 | HTTP | TRUE | TRUE |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月2日 | HTTP | TRUE | TRUE |'
- en: '| June 2nd, 2018 | HTTP | TRUE | FALSE |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月2日 | HTTP | TRUE | FALSE |'
- en: '| June 3rd, 2018 | HTTP | FALSE | TRUE |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2018年6月3日 | HTTP | FALSE | TRUE |'
- en: This is the same dataset as before, but let's dissect it further in the context
    of predictive analytics.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是之前相同的同一个数据集，但让我们在预测分析的背景下进一步剖析它。
- en: 'Notice that we have four attributes of this dataset: `DateTime`, `Protocol`,
    `Urgent`, and `Malicious`. Suppose now that the malicious attribute contains values
    that represent whether or not the observation was a malicious intrusion attempt.
    So in our very small dataset of four network connections, the first, second, and
    fourth connection were malicious attempts to intrude a network.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个数据集有四个属性：`DateTime`、`Protocol`、`Urgent`和`Malicious`。假设现在恶意属性包含表示观察是否为恶意入侵尝试的值。所以，在我们的非常小的四个网络连接数据集中，第一个、第二个和第四个连接是试图入侵网络的恶意尝试。
- en: 'Suppose further that given this dataset, our task is to be able to take in
    three of the attributes (`datetime`, `protocol`, and `urgent`) and be able to
    accurately predict the value of malicious. In laymen’s terms, we want a system
    that can map the values of `datetime`, `protocol`, and `urgent` to the values
    in malicious. This is exactly how a supervised learning problem is set up:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 假设进一步，给定这个数据集，我们的任务是能够接受三个属性（`datetime`、`protocol`和`urgent`）并能够准确预测恶意属性的值。用通俗的话说，我们希望有一个系统可以将`datetime`、`protocol`和`urgent`的值映射到恶意属性的值。这正是监督学习问题设置的方式：
- en: '[PRE1]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: When we are working with supervised learning, we generally call the attribute
    (usually only one of them, but that is not necessary) of the dataset that we are
    attempting to predict the response of. The remaining attributes of the dataset
    are then called the **features**.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们与监督学习一起工作时，我们通常称数据集中我们试图预测响应的属性（通常只有一个，但这不是必需的）为属性。数据集的其余属性被称为**特征**。
- en: Supervised learning can also be considered the class of algorithms attempting
    to exploit the structure in data. By this, we mean that the machine learning algorithms
    try to extract patterns in usually very nice and neat data. As discussed earlier,
    we should not always expect data to come in tidy; this is where feature engineering
    comes in.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习也可以被认为是试图利用数据结构的一类算法。通过这种方式，我们指的是机器学习算法试图从通常非常整洁和有序的数据中提取模式。如前所述，我们不应总是期望数据整齐有序；这正是特征工程发挥作用的地方。
- en: But if we are not predicting something, what good is machine learning you may
    ask? I’m glad you did. Before machine learning can exploit the structure of data,
    sometimes we have to alter or even create structure. That’s where unsupervised
    learning becomes a valuable tool.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果你不预测任何东西，机器学习有什么好处呢？我很高兴你问了。在机器学习能够利用数据结构之前，有时我们必须改变甚至创造结构。这就是无监督学习成为一项宝贵工具的地方。
- en: Unsupervised learning
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: Supervised learning is all about making predictions. We utilize features of
    the data and use them to make informative predictions about the response of the
    data. If we aren’t making predictions by exploring structure, we are attempting
    to extract structure from our data. We generally do so by applying mathematical
    transformations to numerical matrix representations of data or iterative procedures
    to obtain new sets of features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习全部关于做出预测。我们利用数据的特点，并使用它们来对数据的响应做出有信息的预测。如果我们不是通过探索结构来做出预测，我们就是在尝试从我们的数据中提取结构。我们通常通过应用数学变换到数据的数值矩阵表示或迭代过程来获得新的特征集。
- en: This concept can be a bit more difficult to grasp than supervised learning,
    and so I will present a motivating example to help elucidate how this all works.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念可能比监督学习更难理解，所以我会提供一个激励性的例子来帮助阐明这一切是如何工作的。
- en: Unsupervised learning example – marketing segments
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习示例 - 营销细分市场
- en: 'Suppose we are given a large (one million rows) dataset where each row/observation
    is a single person with basic demographic information (age, gender, and so on)
    as well as the number of items purchased, which represents how many items this
    person has bought from a particular store:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们得到了一个大型（一百万行）数据集，其中每一行/观测值代表一个单独的人，包含基本的人口统计信息（年龄、性别等）以及购买商品的数量，这代表这个人从特定商店购买的商品数量：
- en: '| **Age** | **Gender** | **Number of items purchased** |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **性别** | **购买商品数量** |'
- en: '| 25 | F | 1 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 25 | F | 1 |'
- en: '| 28 | F | 23 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 28 | F | 23 |'
- en: '| 61 | F | 3 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 61 | F | 3 |'
- en: '| 54 | M | 17 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 54 | M | 17 |'
- en: '| 51 | M | 8 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 51 | M | 8 |'
- en: '| 47 | F | 3 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 47 | F | 3 |'
- en: '| 27 | M | 22 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 27 | M | 22 |'
- en: '| 31 | F | 14 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 31 | F | 14 |'
- en: This is a sample of our marketing dataset where each row represents a single
    customer with three basic attributes about each person. Our goal will be to segment
    this dataset into types or **clusters** of people so that the company performing
    the analysis can understand the customer profiles much better.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个我们的营销数据集的样本，其中每一行代表一个单独的客户，包含关于每个人的三个基本属性。我们的目标将是将这个数据集细分为不同类型或**聚类**的人群，以便进行数据分析的公司能更好地理解客户画像。
- en: Now, of course, We’ve only shown 8 out of one million rows, which can be daunting.
    Of course, we can perform basic descriptive statistics on this dataset and get
    averages, standard deviations, and so on of our numerical columns; however, what
    if we wished to segment these one million people into different **types** so that
    the marketing department can have a much better sense of the types of people who
    shop and create more appropriate advertisements for each segment?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当然，我们只展示了百万行中的一行，这可能会让人感到 daunting。当然，我们可以对这个数据集进行基本的描述性统计，并得到数值列的平均值、标准差等；然而，如果我们希望将这百万个人分成不同的**类型**，以便营销部门能更好地了解购物人群的类型，并为每个细分市场制作更合适的广告呢？
- en: Each type of customer would exhibit particular qualities that make that segment
    unique. For example, they may find that 20% of their customers fall into a category
    they like to call young and wealthy that are generally younger and purchase several
    items.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 每种类型的客户都会表现出独特的品质，使该细分市场与众不同。例如，他们可能会发现，20%的客户属于他们喜欢称之为年轻且富有的类别，这些人通常较年轻，购买了几件商品。
- en: 'This type of analysis and the creation of these types can fall under a specific
    type of unsupervised learning called **clustering**. We will discuss this machine
    learning algorithm in further detail later on in this book, but for now, clustering
    will create a new feature that separates out the people into distinct types or
    clusters:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分析和创建这些类型可以归入一种特定的无监督学习类型，称为**聚类**。我们将在本书的后面部分进一步讨论这种机器学习算法，但就目前而言，聚类将创建一个新的特征，将人们分离成不同的类型或聚类：
- en: '| **Age** | **Gender** | **Number of items purchased** | **Cluster** |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **年龄** | **性别** | **购买商品数量** | **聚类** |'
- en: '| 25 | F | 1 | 6 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 25 | F | 1 | 6 |'
- en: '| 28 | F | 23 | 1 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 28 | F | 23 | 1 |'
- en: '| 61 | F | 3 | 3 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 61 | F | 3 | 3 |'
- en: '| 54 | M | 17 | 2 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 54 | M | 17 | 2 |'
- en: '| 51 | M | 8 | 3 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 51 | M | 8 | 3 |'
- en: '| 47 | F | 3 | 8 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 47 | F | 3 | 8 |'
- en: '| 27 | M | 22 | 5 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 27 | M | 22 | 5 |'
- en: '| 31 | F | 14 | 1 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 31 | F | 14 | 1 |'
- en: This shows our customer dataset after a clustering algorithm has been applied.
    Note the new column at the end called `cluster` that represents the types of people
    that the algorithm has identified. The idea is that the people who belong to similar clusters
    *behave* similarly in regards to the data (have similar ages, genders, purchase
    behaviors). Perhaps cluster six might be renamed as *young buyers*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了在应用聚类算法后的客户数据集。注意末尾的新列，称为`cluster`，它代表算法已识别的人的类型。想法是，属于相似聚类的**人们**在数据方面（年龄、性别、购买行为）会有相似的行为。也许可以将第六个聚类重命名为*年轻买家*。
- en: This example of clustering shows us why sometimes we aren’t concerned with predicting
    anything, but instead wish to understand our data on a deeper level by adding
    new and interesting features, or even removing irrelevant features.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这个聚类的例子告诉我们，有时我们并不关心预测任何事情，而是希望通过添加新的有趣特征或甚至删除无关特征，在更深的层面上理解我们的数据。
- en: Note that we are referring to every column as a feature because there is no
    response in unsupervised learning since there is no prediction occurring.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们之所以将每一列都称为特征，是因为在无监督学习中没有响应，因为没有预测发生。
- en: It’s all starting to make sense now, isn’t it? These features that we talk about
    repeatedly are what this book is primarily concerned with. Feature engineering
    involves the understanding and transforming of features in relation to both unsupervised
    and supervised learning.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这一切开始变得有道理了，不是吗？我们反复讨论的这些特征正是本书主要关注的内容。特征工程涉及对特征的理解和转换，这既与无监督学习相关，也与监督学习相关。
- en: Evaluation of machine learning algorithms and feature engineering procedures
  id: totrans-135
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习算法和特征工程流程的评估
- en: It is important to note that in literature, oftentimes there is a stark contrast
    between the terms *features* and *attributes*. The term **attribute** is generally
    given to columns in tabular data, while the term **feature** is generally given
    only to attributes that contribute to the success of machine learning algorithms.
    That is to say, some attributes can be unhelpful or even hurtful to our machine
    learning systems. For example, when predicting how long a used car will last before
    requiring servicing, the color of the car will probably not very indicative of
    this value.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在文献中，*特征*和*属性*这两个术语之间往往存在鲜明的对比。术语**属性**通常用于表格数据中的列，而术语**特征**通常仅用于对机器学习算法的成功有贡献的属性。也就是说，一些属性可能对我们的机器学习系统无益，甚至有害。例如，当预测一辆二手车在需要维修之前能使用多长时间时，车的颜色可能对这个值不太具有指示性。
- en: In this book, we will generally refer to all columns as features until they
    are proven to be unhelpful or hurtful. When this happens, we will usually cast
    those attributes aside in the code. It is extremely important, then, to consider
    the basis for this decision. How does one evaluate a machine learning system and
    then use this evaluation to perform feature engineering?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们将通常将所有列都称为特征，直到它们被证明是无用的或有害的。当这种情况发生时，我们通常会在代码中将这些属性弃用。因此，考虑这个决策的基础非常重要。一个人如何评估机器学习系统，然后使用这种评估来执行特征工程？
- en: Example of feature engineering procedures – can anyone really predict the weather?
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征工程流程的示例——真的有人能预测天气吗？
- en: Consider a machine learning pipeline that was built to predict the weather.
    For the sake of simplicity in our introduction chapter, assume that our algorithm
    takes in atmospheric data directly from sensors and is set up to predict between
    one of two values, *sun* or *rain*. This pipeline is then, clearly, a classification
    pipeline that can only spit out one of two answers. We will run this algorithm
    at the beginning of every day. If the algorithm outputs *sun* and the day is mostly
    sunny, the algorithm was correct, likewise, if the algorithm predicts *rain* and
    the day is mostly rainy, the algorithm was correct. In any other instance, the
    algorithm would be considered incorrect. If we run the algorithm every day for
    a month, we would obtain nearly 30 values of the predicted weather and the actual,
    observed weather. We can calculate an accuracy of the algorithm. Perhaps the algorithm
    predicted correctly for 20 out of the 30 days, leading us to label the algorithm
    with a two out of three or about 67% accuracy. Using this standardized value or
    accuracy, we could tweak our algorithm and see if the accuracy goes up or down.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个旨在预测天气的机器学习流程。为了在引言章节中简化，假设我们的算法直接从传感器接收大气数据，并设置为预测两个值之一，*太阳*或*雨*。这个流程显然是一个分类流程，只能输出两个答案中的一个。我们将在每天开始时运行这个算法。如果算法输出*太阳*而当天大部分时间都是晴天，那么算法是正确的；同样，如果算法预测*雨*而当天大部分时间都是雨天，那么算法也是正确的。在任何其他情况下，算法将被认为是错误的。如果我们每天运行这个算法一个月，我们将获得近30个预测天气和实际观察天气的值。我们可以计算算法的准确率。也许算法在30天中有20天预测正确，这使我们将其标记为三分之二或大约67%的准确率。使用这个标准化的值或准确率，我们可以调整我们的算法，看看准确率是上升还是下降。
- en: Of course, this is an oversimplification, but the idea is that for any machine
    learning pipeline, it is essentially useless if we cannot evaluate its performance
    using a set of standard metrics and therefore, feature engineering as applied
    to the bettering of machine learning, is impossible without said evaluation procedure.
    Throughout this book, we will revisit this idea of evaluation; however, let’s
    talk briefly about how, in general, we will approach this idea.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个过于简化的说法，但想法是，对于任何机器学习流程，如果我们不能使用一组标准指标来评估其性能，那么它基本上是无用的。因此，应用于改进机器学习的特征工程，没有这种评估程序是不可能的。在本书中，我们将重新审视这种评估的想法；然而，让我们简要地谈谈我们通常将如何处理这个想法。
- en: When presented with a topic in feature engineering, it will usually involve
    transforming our dataset (as per the definition of feature engineering). In order
    to definitely say whether or not a particular feature engineering procedure has
    helped our machine learning algorithm, we will follow the steps detailed in the
    following section.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当面对特征工程的主题时，通常涉及转换我们的数据集（根据特征工程的定义）。为了确定特定的特征工程过程是否有助于我们的机器学习算法，我们将遵循以下章节中详细说明的步骤。
- en: Steps to evaluate a feature engineering procedure
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估特征工程过程的步骤
- en: 'Here are the steps to evaluate a feature engineering procedure:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 评估特征工程过程的步骤：
- en: Obtain a baseline performance of the machine learning model before applying
    any feature engineering procedures
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在应用任何特征工程过程之前，获取机器学习模型的基线性能
- en: Apply feature engineering and combinations of feature engineering procedures
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用特征工程和特征工程过程的组合
- en: For each application of feature engineering, obtain a performance measure and
    compare it to our baseline performance
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每次特征工程的应用，获取一个性能指标并将其与我们的基线性能进行比较
- en: If the delta (change in) performance precedes a threshold (usually defined by
    the human), we deem that procedure helpful and apply it to our machine learning
    pipeline
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果性能变化（delta）先于阈值（通常由人类定义），我们认为该过程是有益的，并将其应用于我们的机器学习流程
- en: This change in performance will usually be measured as a percentage (if the
    baseline went from 40% accuracy to 76% accuracy, that is a 90% improvement)
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这种性能的变化通常以百分比来衡量（如果基线从40%的准确率提高到76%的准确率，那么这是一个90%的提升）
- en: In terms of performance, this idea varies between machine learning algorithms.
    Most good primers on machine learning will tell you that there are dozens of accepted
    metrics when practicing data science.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，这个想法在机器学习算法之间有所不同。大多数优秀的机器学习入门书籍都会告诉你，在数据科学实践中有数十种接受的指标。
- en: In our case, because the focus of this book is not necessarily on machine learning
    and rather on the understanding and transformation of features, we will use baseline
    machine learning algorithms and associated baseline metrics in order to evaluate
    the feature engineering procedures.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，因为这本书的重点不一定在机器学习，而是对特征的理解和转换，我们将使用基线机器学习算法和相关基线指标来评估特征工程过程。
- en: Evaluating supervised learning algorithms
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估监督学习算法
- en: When performing predictive modeling, otherwise known as **supervised learning**,
    performance is directly tied to the model’s ability to exploit structure in the
    data and use that structure to make appropriate predictions. In general, we can
    further break down supervised learning into two more specific types, **classification**
    (predicting qualitative responses) and **regression** (predicting quantitative
    responses).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行预测建模，也称为**监督学习**时，性能直接与模型利用数据结构的能力以及使用该结构进行适当预测的能力相关。一般来说，我们可以将监督学习进一步细分为两种更具体的类型，**分类**（预测定性响应）和**回归**（预测定量响应）。
- en: 'When we are evaluating classification problems, we will directly calculate
    the accuracy of a logistic regression model using a five-fold cross-validation:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估分类问题时，我们将直接使用五折交叉验证计算逻辑回归模型的准确率：
- en: '[PRE2]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Similarly, when evaluating a regression problem, we will use the **mean squared
    error** (**MSE**) of a linear regression using a five-fold cross-validation:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在评估回归问题时，我们将使用五折交叉验证的线性回归的**均方误差**（MSE）：
- en: '[PRE3]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We will use these two linear models instead of newer, more advanced models for
    their speed and their low variance. This way, we can be surer that any increase
    in performance is directly related to the feature engineering procedure and not
    to the model’s ability to pick up on obscure and hidden patterns.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这两个线性模型而不是更新、更先进的模型，因为它们的速度更快，方差更低。这样，我们可以更有信心地认为任何性能的提升都是直接与特征工程过程相关，而不是与模型捕捉到隐秘和隐藏模式的能力相关。
- en: Evaluating unsupervised learning algorithms
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估无监督学习算法
- en: This is a bit trickier. Because unsupervised learning is not concerned with
    predictions, we cannot directly evaluate performance based on how well the model
    can predict a value. That being said, if we are performing a cluster analysis,
    such as in the previous marketing segmentation example, then we will usually utilize
    the **silhouette coefficient** (a measure of separation and cohesion of clusters
    between -1 and 1) and some human-driven analysis to decide if a feature engineering
    procedure has improved model performance or if we are merely wasting our time.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这有点棘手。因为无监督学习不涉及预测，我们不能直接根据模型预测值的准确性来评估性能。话虽如此，如果我们正在进行聚类分析，例如在先前的市场细分示例中，那么我们通常会利用**轮廓系数**（一个介于-1和1之间的聚类分离和凝聚度的度量）和一些人为驱动的分析来判断特征工程程序是否提高了模型性能，或者我们只是在浪费时间。
- en: 'Here is an example of using Python and scikit-learn to import and calculate
    the silhouette coefficient for some fake data:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用Python和scikit-learn导入和计算一些虚假数据的轮廓系数的例子：
- en: '[PRE4]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We will spend much more time on unsupervised learning later on in this book
    as it becomes more relevant. Most of our examples will revolve around predictive
    analytics/supervised learning.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的后续部分，我们将花更多的时间在无监督学习上，因为它变得更加相关。我们的大部分例子将围绕预测分析/监督学习展开。
- en: It is important to remember that the reason we are standardizing algorithms
    and metrics is so that we may showcase the power of feature engineering and so
    that you may repeat our procedures with success. Practically, it is conceivable
    that you are optimizing for something other than accuracy (such as a true positive
    rate, for example) and wish to use decision trees instead of logistic regression.
    This is not only fine but encouraged. You should always remember though to follow
    the steps to evaluating a feature engineering procedure and compare baseline and
    post-engineering performance.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要记住，我们标准化算法和指标的原因是为了展示特征工程的力量，以及让你能够成功重复我们的程序。实际上，你可能会优化除了准确率之外的其他东西（例如，例如真正的阳性率），并希望使用决策树而不是逻辑回归。这不仅是可以接受的，而且是鼓励的。然而，你应该始终记住，要遵循评估特征工程程序的标准步骤，并比较基线和工程后的性能。
- en: It is possible that you are not reading this book for the purposes of improving
    machine learning performance. Feature engineering is useful in other domains such
    as hypothesis testing and general statistics. In a few examples in this book,
    we will be taking a look at feature engineering and data transformations as applied
    to a statistical significance of various statistical tests. We will be exploring
    metrics such as R^(2 )and p-values in order to make judgements about how our procedures
    are helping.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能你不是为了提高机器学习性能而阅读这本书。特征工程在假设检验和一般统计学等其他领域也是有用的。在本书的一些例子中，我们将探讨特征工程和数据转换在应用于各种统计测试的统计显著性方面的应用。我们将探索诸如R^(2)和p值等指标，以便对我们的程序如何帮助做出判断。
- en: 'In general, we will quantify the benefits of feature engineering in the context
    of three categories:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们将从三个类别中量化特征工程的好处：
- en: '**Supervised learning**: Otherwise known as **predictive analytics**'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**：也称为**预测分析**'
- en: 'Regression analysis—predicting a *quantitative* variable:'
  id: totrans-167
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归分析——预测一个**定量**变量：
- en: Will utilize MSE as our primary metric of measurement
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将利用均方误差（MSE）作为我们的主要度量指标
- en: Classification analysis—predicting a *qualitative* variable
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类分析——预测一个**定性**变量
- en: Will utilize accuracy as our primary metric of measurement
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将利用准确率作为我们的主要度量指标
- en: '**Unsupervised learning**: Clustering—the assigning of meta-attributes as denoted
    by the behavior of data:'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无监督学习**：聚类——根据数据的特征分配元属性：'
- en: Will utilize the silhouette coefficient as our primary metric of measurement
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将利用轮廓系数作为我们的主要度量指标
- en: '**Statistical testing**: Using correlation coefficients, t-tests, chi-squared
    tests, and others to evaluate and quantify the usefulness of our raw and transformed
    data'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**统计测试**：使用相关系数、t检验、卡方检验等来评估和量化我们原始和转换数据的效用'
- en: In the following few sections, we will look at what will be covered throughout
    this book.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将查看本书将涵盖的内容。
- en: Feature understanding – what’s in my dataset?
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征理解——我的数据集中有什么？
- en: In our first subtopic, we will start to build our fundamentals in dealing with
    data. By understanding the data in front of us, we can start to have a better
    idea of where to go next. We will begin to explore the different types of data
    out there as well as how to recognize the type of data inside datasets. We will
    look at datasets from several domains and identify how they are different from
    each other and how they are similar to each other. Once we are able to comfortably
    examine data and identify the characteristics of different attributes, we can
    start to understand the types of transformations that are allowed and that promise
    to improve our machine learning algorithms.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个子主题中，我们将开始构建处理数据的基本知识。通过理解我们面前的数据，我们可以开始更好地了解下一步该去哪里。我们将开始探索不同类型的数据以及如何识别数据集中数据的类型。我们将查看来自几个领域的数据集，并确定它们彼此之间的不同之处以及它们之间的相似之处。一旦我们能够舒适地检查数据并识别不同属性的特征，我们就可以开始理解允许的类型以及承诺改进我们的机器学习算法的转换。
- en: 'Among the different methods of understanding, we will be looking at:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同的理解方法中，我们将探讨：
- en: Structured versus unstructured data
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化数据与非结构化数据
- en: The four levels of data
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据的四个层次
- en: Identifying missing data values
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别缺失数据值
- en: Exploratory data analysis
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Descriptive statistics
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述性统计
- en: Data visualizations
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可视化
- en: We will begin at a basic level by identifying the structure of, and then the
    types of data in front of us. Once we are able to understand what the data is,
    we can start to fix problems with the data. As an example, we must know how much
    of our data is missing and what to do when we have missing data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从基本水平开始，通过识别我们面前数据的结构以及数据类型。一旦我们能够理解数据是什么，我们就可以开始解决数据的问题。例如，我们必须知道我们有多少数据是缺失的，以及当我们有缺失数据时应该做什么。
- en: Make no mistake, data visualizations, descriptive statistics, and exploratory
    data analysis are all a part of feature engineering. We will be exploring each
    of these procedures from the perspective of the machine learning engineer. Each
    of these procedures has the ability to enhance our machine learning pipelines
    and we will test and alter hypotheses about our data using them.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 不要误解，数据可视化、描述性统计和探索性数据分析都是特征工程的一部分。我们将从机器学习工程师的角度探索这些程序中的每一个。每个程序都有能力增强我们的机器学习管道，我们将使用它们来测试和改变关于我们数据的假设。
- en: Feature improvement – cleaning datasets
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征改进——清理数据集
- en: In this topic, we take the results of our understanding of the data and use
    them in order to clean the dataset. Much of this book will flow in such a way,
    using results from previous sections to be able to work on current sections. In
    feature improvement, our understanding will allow us to begin our first manipulations
    of datasets. We will be using mathematical transformations to enhance the given
    data, but not remove or insert any new attributes (this is for the next chapters).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们利用我们对数据的理解，并使用这些理解来清理数据集。本书的大部分内容将以这种方式流动，使用前几节的结果来处理当前章节。在特征改进中，我们的理解将使我们开始对数据集进行第一次操作。我们将使用数学变换来增强给定的数据，但不会删除或插入任何新的属性（这将在下一章中讨论）。
- en: 'We will explore several topics in this section, including:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨几个主题，包括：
- en: Structuring unstructured data
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结构化非结构化数据
- en: Data imputing—inserting data where there was not a data before (missing data)
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据插补——在之前没有数据的地方插入数据（缺失数据）
- en: 'Normalization of data:'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据归一化：
- en: Standardization (known as z-score normalization)
  id: totrans-192
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化（称为z分数归一化）
- en: Min-max scaling
  id: totrans-193
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Min-max缩放
- en: L1 and L2 normalization (projecting into different spaces, fun stuff)
  id: totrans-194
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1和L2归一化（投影到不同的空间，很有趣）
- en: By this point in the book, we will be able to identify whether our data has
    a *structure* or not. That is, whether our data is in a nice, tabular format.
    If it is not, this chapter will give us the tools to transform that data into
    a more tabular format. This is imperative when attempting to create machine learning
    pipelines.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 到这本书的这一阶段，我们将能够确定我们的数据是否有结构。也就是说，我们的数据是否以整洁的表格格式存在。如果不是，这一章将为我们提供将数据转换为更表格化格式的工具。在尝试创建机器学习管道时，这是必不可少的。
- en: Data imputing is a particularly interesting topic. The ability to fill in data
    where data was missing previously is trickier than it sounds. We will be proposing
    all kinds of solutions from the very, very easy, merely removing the column altogether,
    boom no more missing data, to the interestingly complex, using machine learning
    on the rest of the features to fill in missing spots. Once we have filled in a
    bulk of our missing data, we can then measure how that affected our machine learning
    algorithms.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 数据插补是一个特别有趣的话题。在数据之前缺失的地方填补数据的能力比听起来要复杂。我们将提出各种解决方案，从非常简单的，仅仅删除整个列，Boom，不再有缺失数据，到复杂有趣的，使用机器学习在其余特征上填补缺失的部分。一旦我们填补了大量缺失数据，我们就可以衡量这如何影响我们的机器学习算法。
- en: Normalization uses (generally simple) mathematical tools used to change the
    scaling of our data. Again, this ranges from the easy, turning miles into feet
    or pounds into kilograms, to the more difficult, such as projecting our data onto
    the unit sphere (more on that to come).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 标准化使用（通常简单的）数学工具，用于改变我们数据的缩放。同样，这从简单的，如将英里转换为英尺或磅转换为千克，到更复杂的，如将我们的数据投影到单位球面上（关于这一点将在后面详细介绍）。
- en: This chapter and remaining chapters will be much more heavily focused on our
    quantitative feature engineering procedure evaluation flow. Nearly every single
    time we look at a new dataset or feature engineering procedure, we will put it
    to the test. We will be grading the performance of various feature engineering
    methods on the merits of machine learning performance, speed, and other metrics.
    This text should only be used as a reference and not as a guide to select with
    feature engineering the procedures you are allowed to **ignore** based on difficulty
    and change in performance. Every new data task comes with its own caveats and
    may require different procedures than the previous data task.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本章以及后续章节将更加侧重于我们的量化特征工程流程评估。几乎每次我们查看新的数据集或特征工程流程时，我们都会对其进行测试。我们将根据机器学习性能、速度和其他指标来评估各种特征工程方法的表现。本文本仅应作为参考，而不应作为根据难度和性能变化选择可**忽略**的特征工程流程的指南。每个新的数据任务都伴随着其自身的注意事项，可能需要与之前的数据任务不同的流程。
- en: Feature selection – say no to bad attributes
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择 – 对不良属性说“不”
- en: By this chapter, we will have a level of comfort when dealing with new datasets.
    We will have under our belt the abilities to understand and clean the data in
    front of us. Once we are able to work with the data given to us, we can start
    to make big decisions such as, *at what point is a feature actually an attribute*.
    Recall that by this distinction, feature versus attribute, the question really
    is, *which columns are not helping my ML pipeline and therefore are hurting my
    pipeline and should be removed?* This chapter focuses on techniques used to make
    the decision of which attributes to get rid of in our dataset. We will explore
    several statistical and iterative processes that will aid us in this decision.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章为止，我们将对处理新数据集感到更加自在。我们将具备理解和清理面前数据的技能。一旦我们能够处理给定的数据，我们就可以开始做出重大决策，例如，**何时一个特征实际上是一个属性**。回想一下，通过这种区分，特征与属性，真正的问题其实是，*哪些列没有帮助我的机器学习流程，因此损害了我的流程，应该被移除？*本章重点介绍用于决定在数据集中移除哪些属性的技巧。我们将探讨几种统计和迭代过程，这些过程将帮助我们做出这个决定。
- en: 'Among these processes are:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这些过程包括：
- en: Correlation coefficients
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关系数
- en: Identifying and removing multicollinearity
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别和消除多重共线性
- en: Chi-squared tests
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卡方检验
- en: Anova tests
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安诺瓦检验
- en: Interpretation of p-values
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: p值的解释
- en: Iterative feature selection
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迭代特征选择
- en: Using machine learning to measure entropy and information gain
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用机器学习来衡量熵和信息增益
- en: All of these procedures will attempt to suggest the removal of features and
    will give different reasons for doing so. Ultimately, it will be up to us, the
    data scientists, to make the final call over which features will be allowed to
    remain and contribute to our machine learning algorithms.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些流程都将尝试建议删除特征，并给出不同的理由。最终，将由我们，即数据科学家，来做出最终决定，决定哪些特征可以保留并贡献于我们的机器学习算法。
- en: Feature construction – can we build it?
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征构建 – 我们能构建它吗？
- en: While in previous chapters we focused heavily on removing features that were
    not helping us with our machine learning pipelines, this chapter will look at
    techniques in creating brand new features and placing them correctly within our
    dataset. These new features will ideally hold new information and generate new
    patterns that ML pipelines will be able to exploit and use to increase performance.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们主要关注移除对我们机器学习流程没有帮助的特征，而本章将探讨创建全新的特征并在我们的数据集中正确放置这些特征的技术。这些新特征理想情况下将包含新的信息，并生成新的模式，机器学习流程可以利用这些模式并用于提高性能。
- en: 'These created features can come from many places. Oftentimes, we will create
    new features out of existing features given to us. We can create new features
    by applying transformations to existing features and placing the resulting vectors
    alongside their previous counterparts. We will also look at adding new features
    from separate party systems. As an example, if we are working with data attempting
    to cluster people based on shopping behaviors, then we might benefit from adding
    in census data that is separate from the corporation and their purchasing data.
    However, this will present a few problems:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这些创建的特征可以来自许多地方。通常，我们会从给定的现有特征中创建新的特征。我们可以通过对现有特征应用变换并放置结果向量与它们的先前对应向量旁边来创建新的特征。我们还将探讨从不同的系统添加新特征。例如，如果我们正在处理尝试根据购物行为对人群进行聚类的数据，那么添加来自公司及其购买数据之外的人口普查数据可能会对我们有所帮助。然而，这将带来一些问题：
- en: If the census is aware of 1,700 Jon does and the corporation only knows 13,
    how do we know which of the 1,700 people match up to the 13? This is called *entity
    matching*
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果人口普查知道1,700个约翰，而公司只知道13个，我们如何知道这1,700个人中的哪一个与这13个人匹配？这被称为*实体匹配*
- en: The census data would be quite large and entity matching would take a very long
    time
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口普查数据将会相当大，实体匹配将需要非常长的时间
- en: These problems and more make for a fairly difficult procedure but oftentimes
    create a very dense and data-rich environment.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题和更多的问题使得整个过程相当困难，但往往创造出非常密集且数据丰富的环境。
- en: 'In this chapter, we will take some time to talk about the manual creation of
    features through highly unstructured data. Two big examples are text and images.
    These pieces of data by themselves are incomprehensible to machine learning and
    artificial intelligence pipelines, so it is up to us to manually create features
    that represent the images/pieces of text. As a simple example, imagine that we
    are making the basics of a self-driving car and to start, we want to make a model
    that can take in an image of what the car is seeing in front of it and decide
    whether or not it should stop. The raw image is not good enough because a machine
    learning algorithm would have no idea what to do with it. We have to manually
    construct features out of it. Given this raw image, we can split it up in a few
    ways:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将花一些时间来讨论通过高度无结构化的数据手动创建特征。两个大的例子是文本和图像。这些数据本身对机器学习和人工智能流程来说是不可理解的，因此我们需要手动创建代表图像/文本片段的特征。作为一个简单的例子，想象一下我们正在制作自动驾驶汽车的基础，首先，我们想要创建一个模型，它可以接收汽车前方看到的图像，并决定是否应该停车。原始图像不够好，因为机器学习算法不知道如何处理它。我们必须从其中手动构建特征。给定这个原始图像，我们可以以几种方式将其分割：
- en: 'We could consider the color intensity of each pixel and consider each pixel
    an attribute:'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以考虑每个像素的颜色强度，并将每个像素视为一个属性：
- en: For example, if the camera of the car produces images of 2,048 x 1,536 pixels,
    we would have 3,145,728 columns
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，如果汽车的摄像头产生2,048 x 1,536像素的图像，我们将有3,145,728列
- en: 'We could consider each row of pixels as an attribute and the average color
    of each row being the value:'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将像素的每一行视为一个属性，每行的平均颜色作为其值：
- en: In this case, there would only be 1,536 rows
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在这种情况下，将只有1,536行
- en: 'We could project this image into space where features represent objects within
    the image. This is the hardest of the three and would look something like this:'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将这个图像投影到空间中，其中特征代表图像中的对象。这是三者中最难的，看起来可能像这样：
- en: '| **Stop sign** | **Cat** | **Sky** | **Road** | **Patches of grass** | **Submarine**
    |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **停止标志** | **猫** | **天空** | **道路** | **草地斑块** | **潜艇** |'
- en: '| 1 | 0 | 1 | 1 | 4 | 0 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 1 | 4 | 0 |'
- en: Where each feature is an object that may or may not be within the image and
    the value represents the number of times that object appears in the image. If
    a model were given this information, it would be a fairly good idea to stop!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 其中每个特征都是一个可能或可能不在图像中的对象，其值表示该对象在图像中出现的次数。如果模型被提供了这些信息，停止就相当好了！
- en: Feature transformation – enter math-man
  id: totrans-225
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征变换 – 进入数学达人
- en: This chapter is where things get mathematical and interesting. We'll have talked
    about understating features and cleaning them. We'll also have looked at how to
    remove and add new features. In our feature construction chapter, we had to manually
    create these new features. We, the human, had to use our brains and come up with
    those three ways of decomposing that image of a stop sign. Sure, we can create
    code that makes the features automatically, but we ultimately chose what features
    we wanted to use.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是事情变得数学化和有趣的地方。我们已经讨论了理解特征和清理它们。我们还探讨了如何删除和添加新特征。在我们的特征构建章节中，我们必须手动创建这些新特征。作为人类，我们必须用我们的大脑想出分解停车标志图像的那三种方法。当然，我们可以编写自动创建特征的代码，但最终我们选择了我们想要使用的特征。
- en: This chapter will start to look at the automatic creation of these features
    as it applies to mathematical dimensionality. If we regard our data as vectors
    in an n-space (n being the number of columns), we will ask ourselves, *can we
    create a new dataset in a k-space (where k < n) that fully or nearly represents
    the original data, but might give us speed boosts or performance enhancements
    in machine learning?* The goal here is to create a dataset of smaller dimensionality
    that performs better than our original dataset at a larger dimensionality.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将开始探讨这些特征在数学维度上的自动创建。如果我们把我们的数据视为n维空间（n代表列数）中的向量，我们会问自己，*我们能否在k维空间（k < n）中创建一个新的数据集，该数据集可以完全或近似地表示原始数据，但可能会在机器学习中提供速度提升或性能提升？*
    这里的目标是创建一个维度更小的数据集，其性能优于原始数据集在更大维度上的性能。
- en: The first question here is, *weren't we creating data in smaller dimensionality
    before when we were feature selecting? If we start with 17 features and remove
    five, we've reduced the dimensionality to 12, right?* Yes, of course! However,
    we aren't talking simply about removing columns here, we are talking about using
    complex mathematical transformations (usually taken from our studies in linear
    algebra) and applying them to our datasets.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的第一个问题是，*我们之前在特征选择时不是已经在创建低维度的数据了吗？如果我们从17个特征开始，去掉五个，我们不是已经将维度减少到12了吗？* 当然是！然而，我们在这里讨论的不仅仅是去掉列，我们是在讨论使用复杂的数学变换（通常来自我们的线性代数研究）并将它们应用于我们的数据集。
- en: One notable example we will spend some time on is called**Principal Components
    Analysis** (**PCA**)*. *It is a transformation that breaks down our data into
    three different datasets, and we can use these results to create brand new datasets
    that can outperform our original!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将花一些时间讨论的一个显著例子被称为**主成分分析**（**PCA**）。*这是一种将我们的数据分解成三个不同数据集的变换，我们可以使用这些结果来创建全新的数据集，这些数据集可以超越我们的原始数据！
- en: 'Here is a visual example is taken from a Princeton University research experiment
    that used PCA to exploit patterns in gene expressions. This is a great application
    of dimensionality reduction as there are so many genes and combinations of genes,
    it would take even the most sophisticated algorithms in the world plenty of time
    to process them:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个来自普林斯顿大学研究实验的视觉示例，该实验使用了PCA来利用基因表达的模式。这是维度降低的一个很好的应用，因为有如此多的基因和基因组合，即使是世界上最复杂的算法也需要很长时间来处理：
- en: '![](img/3172690f-5e88-42ff-af38-dabdf8adfe90.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3172690f-5e88-42ff-af38-dabdf8adfe90.png)'
- en: In the preceding screenshot, **A** represents the original dataset, where **U**,
    **W**, and **V^T** represent the results of a singular value decomposition. The
    results are then put together to make a brand new dataset that can replace **A**
    to a certain extent.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的屏幕截图中，**A**代表原始数据集，其中**U**、**W**和**V^T**代表奇异值分解的结果。然后，将这些结果组合起来，创建一个新的数据集，可以在一定程度上取代**A**。
- en: Feature learning – using AI to better our AI
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征学习 – 使用AI来提升我们的AI
- en: The cherry on top, a cherry powered by the most sophisticated algorithms used
    today in the automatic construction of features for the betterment of machine
    learning and AI pipelines.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 顶部的樱桃，一个由今天在机器学习和AI管道自动构建特征的最先进算法驱动的樱桃。
- en: The previous chapter dealt with automatic feature creation using mathematical
    formulas, but once again, in the end, it is us, the humans, that choose the formulas
    and reap the benefits of them. This chapter will outline algorithms that are not
    in and of themselves a mathematical formula, but an architecture attempting to
    understand and model data in such a way that it will exploit patterns in data
    in order to create new data. This may sound vague at the moment, but we hope to
    get you excited about it!
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 上一章讨论了使用数学公式自动创建特征，但再次强调，最终是我们人类选择公式并从中获益。本章将概述一些算法，这些算法本身并不是数学公式，而是一种试图以某种方式理解和建模数据以利用数据中的模式来创建新数据的架构。这听起来可能有些模糊，但我们希望让你对它感到兴奋！
- en: 'We will focus mainly on neural algorithms that are specially designed to use
    a neural network design (nodes and weights). These algorithms will then impose
    features onto the data in such a way that can sometimes be unintelligible to humans,
    but extremely useful for machines. Some of the topics we''ll look at are:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将主要关注专门设计用于使用神经网络设计（节点和权重）的神经网络算法。这些算法将把特征强加到数据上，有时对人类来说可能难以理解，但对机器来说却非常有用。我们将探讨的一些主题包括：
- en: Restricted Boltzmann machines
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 限制性玻尔兹曼机
- en: Word2Vec/GLoVe for word embedding
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Word2Vec/GLoVe用于词嵌入
- en: 'Word2Vec and GLoVe are two ways of adding large dimensionality data to seemingly
    word tokens in the text. For example, if we look at a visual representation of
    the results of a Word2Vec algorithm, we might see the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Word2Vec和GLoVe是将高维数据添加到文本中看似单词标记的两种方法。例如，如果我们查看Word2Vec算法结果的视觉表示，我们可能会看到以下内容：
- en: '![](img/bdac3750-c12c-4cfe-8929-73dc0cb6835a.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bdac3750-c12c-4cfe-8929-73dc0cb6835a.png)'
- en: By representing words as vectors in Euclidean space, we can achieve mathematical-esque
    results. In the previous example, by adding these automatically generated features
    we can *add* and *subtract* words by adding and subtracting their vector representations
    as given to us by Word2Vec. We can then generate interesting conclusions, such
    as **king+man-woman=queen**. Cool!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将单词表示为欧几里得空间中的向量，我们可以实现类似数学的结果。在先前的例子中，通过添加这些自动生成的特征，我们可以通过添加和减去Word2Vec给出的向量表示来*添加*和*减去*单词。然后我们可以得出有趣的结论，例如**king+man-woman=queen**。酷！
- en: Summary
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Feature engineering is a massive task to be undertaken by data scientists and
    machine learning engineers. It is a task that is imperative to having successful
    and production-ready machine learning pipelines. In the coming seven chapters,
    we are going to explore six major aspects of feature engineering:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程是数据科学家和机器学习工程师要承担的巨大任务。这是成功和可投入生产的机器学习管道所必需的任务。在接下来的七个章节中，我们将探讨特征工程的六个主要方面：
- en: 'Feature understanding: learning how to identify data based on its qualities
    and quantitative state'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征理解：学习如何根据其质量和定量状态来识别数据
- en: 'Feature improvement: cleaning and imputing missing data values in order to
    maximize the dataset''s value'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征改进：清理和填充缺失数据值，以最大化数据集的价值
- en: Feature selection -statistically selecting and subsetting feature sets in order
    to reduce the noise in our data
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择 - 统计选择和子集特征集，以减少我们数据中的噪声
- en: Feature construction - building new features with the intention of exploiting
    feature interactions
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征构造 - 构建新的特征，目的是利用特征交互
- en: Feature transformation - extracting latent (hidden) structure within datasets
    in order to mathematically transform our datasets into something new (and usually
    better)
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征转换 - 从数据集中提取潜在（隐藏）结构，以便将我们的数据集在数学上转换成新的（通常更好的）东西
- en: Feature learning - harnessing the power of deep learning to view data in a whole
    new light that will open up new problems to be solved.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征学习 - 利用深度学习的力量以全新的视角看待数据，从而开辟新的问题来解决。
- en: In this book, we will be exploring feature engineering as it relates to our
    machine learning endeavors. By breaking down this large topic into our subtopics
    and diving deep into each one in separate chapters, we will be able to get a much
    broader and more useful understanding of how these procedures work and how to
    apply each one in Python.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们将探讨与我们的机器学习努力相关的特征工程。通过将这个大主题分解为我们的子主题，并在单独的章节中深入探讨每一个子主题，我们将能够获得更广泛、更有用的理解，了解这些过程是如何工作的，以及如何在Python中应用每一个过程。
- en: In our next chapter, we will dive straight into our first subsection, *Feature
    understanding*. We will finally be getting our hands on some real data, so let's
    begin!
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们下一章中，我们将直接进入我们的第一个小节，*特征理解*。我们终于要接触一些真实数据了，所以让我们开始吧！
