- en: Building a Chatbot
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建一个聊天机器人
- en: Imagine for a moment that you're sitting alone in a quiet, spacious room. To
    your right is a small table with a stack of white printer paper and a single black
    pen. In front of you is what seems to be a large, red cube with a tiny opening—slightly
    smaller than the size of a mail slot. An inscription just above the slot invites
    you to write down a question and pass it through the slot. As it happens, you
    speak Mandarin; so, you write down your question in Mandarin on one of the sheets
    and insert it into the opening. A few moments pass, and then slowly, an answer
    emerges. It's also written in Chinese and is the just the sort of answer you might
    have expected. So, what did you ask? *Are you a person or a computer?* And the
    response? *Why yes, yes I am*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，假设你正独自坐在一个安静宽敞的房间里。右侧有一张小桌子，上面放着一叠白色打印纸和一支黑色的钢笔。你面前似乎有一个大红色的立方体，顶部有一个小孔——大小略小于邮筒的孔。洞口上方的铭文邀请你写下一个问题并把它穿过孔口。碰巧你会说普通话，于是你在纸张上写下你的问题，并将其放入孔中。几秒钟后，慢慢地，一个回答出现了。它同样是中文写的，正是你可能期待的那种回答。那么，你问了什么？*你是人还是计算机？*
    回答是什么？*是的，是的，我是。*
- en: This thought experiment is based on philosopher John Searle's Chinese Room Argument.
    The premise of the experiment is that if there were a person in the room who spoke
    no Chinese, but had a set of rules that allowed them to perfectly map English
    characters to Chinese characters, they could appear to the questioner to understand
    Chinese without actually having any understanding of it. Searle's argument was
    that algorithmic procedures that produce an intelligible output can't be said
    to have an *understanding* of that output. They lack a *mind*. His thought experiment
    was an attempt to combat the ideas of *strong AI*, or the notion that the human
    brain is essentially just a *wet machine*. Searle didn't believe that AI could
    be said to have consciousness, no matter how sophisticated its behavior appeared
    to an outside observer.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这个思想实验基于哲学家约翰·赛尔的“中文房间”论证。实验的前提是，如果房间里有一个不会说中文的人，但他有一套规则，能够将英文字符完美地映射到中文字符上，那么他可能看起来像是理解中文，尽管他实际上并不懂中文。赛尔的论点是，产生可理解输出的算法程序不能说自己“理解”这一输出。它们缺乏*意识*。他的思想实验旨在反对*强人工智能*的观点，或认为人类大脑本质上只是一种*湿机器*的观点。赛尔不认为无论AI的行为看起来多么复杂，都能被称为拥有意识。
- en: Searle published this experiment in 1980\. 31 years later, Siri would be released
    on the iPhone 4S. For anyone who has used Siri, it's clear that we have a long
    way to go before we might be confronted with uncertainty of whether the agent
    we are speaking to has a mind (though we might doubt it in people we know to be
    human). Despite the clunkiness these agents, or chatbots, have demonstrated in
    the past, the field is rapidly advancing.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 赛尔于1980年发布了这个实验。31年后，Siri将在iPhone 4S上发布。对于使用过Siri的人来说，显然我们还有很长的路要走，才可能面临是否我们正在与一个具有意识的代理交流的不确定性（尽管我们可能会对那些已知为人类的人的意识产生怀疑）。尽管这些代理，或聊天机器人，过去表现得笨拙，但该领域正在迅速发展。
- en: In this chapter, we're going to learn how to construct a chatbot from scratch.
    Along the way, we'll learn more about the history of the field and its future
    prospects.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何从零开始构建一个聊天机器人。在这个过程中，我们还会了解该领域的历史及其未来前景。
- en: 'We''ll cover the following topics in this chapter:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: The Turing Test
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图灵测试
- en: The history of chatbots
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人的历史
- en: The design of chatbots
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聊天机器人设计
- en: Building a chatbot
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建聊天机器人
- en: The Turing Test
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图灵测试
- en: 30 years before Searle's Chinese Room, Alan Turing posed the question, *can
    machines think?* in one of his more famous papers. Being the practical genius
    he was, he decided not to tackle that question head on, but to instead pose it
    in the framework of the *problem of other minds*. This problem asks, *how do we
    truly know that other people have minds like our own?* Since we can only observe
    their behavior—and not the inner workings of their mind—we must take it on faith
    that they are like us. Turing proposed that if a machine could behave as if it
    were intelligent, then we should view it as such. This, in a nutshell, is the
    *Turing Test*. Passing the test means convincing humans that a machine is a fellow
    human.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在塞尔的中文房间理论提出的30年前，艾伦·图灵在他的一篇较为著名的论文中提出了问题，*机器能思考吗？* 作为一位实践天才，他决定不是直接面对这个问题，而是将其置于*他人心智问题*的框架内。这个问题是：*我们如何才能真正知道其他人有像我们一样的心智？*
    由于我们只能观察他们的行为，而无法看到他们内心的工作机制，我们必须信任他们像我们一样。图灵提出，如果一台机器能够表现得像智能一样，那么我们应该把它看作是智能的。这，简而言之，就是*图灵测试*。通过测试意味着让人类相信这台机器是一个人类。
- en: You might be wondering at this point, *has a program ever successfully passed
    the Turing Test?* And the answer is that there really is no official Turing Test.
    The closest thing to an official test is the Loebner Prize. This is a contest
    held yearly that awards modest prizes to those chatbots that appear the most convincingly
    human to a panel of judges. If this sounds rigorous and academic, it has been
    described as anything but. Loebner himself is said to be quite the character,
    frequently professing his love of wine, marijuana, and prostitutes. His sponsorship
    for the contest is said to have been a product of his total distain for labor
    of any kind.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可能在想，*有程序成功通过过图灵测试吗？* 答案是，实际上并没有官方的图灵测试。最接近官方测试的是洛布纳奖。这是一个每年举办的比赛，奖励那些在评委眼中最能表现得像人的聊天机器人。如果这听起来很严谨和学术，实际上并非如此。传闻洛布纳本人是个相当有个性的人，经常公开表达自己对葡萄酒、大麻和妓女的喜爱。据说，他资助这项比赛完全是因为他对任何形式的劳动都抱有极大的厌恶。
- en: Hopefully, by this point, you'll have realized that any claims about *passing
    the Turing Test *are more spectacle than actual science. Despite this, Turing's
    original concept is a useful construct.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 希望到这个时候，你已经意识到，关于*通过图灵测试*的任何说法，更多的是一种表演，而非真正的科学。尽管如此，图灵的原始概念依然是一个有用的构建。
- en: The history of chatbots
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聊天机器人的历史
- en: 'I still remember the day my father sat me down in front of his IBM XT with
    its black-and-green flickering monitor and asked me to talk to *it*. The conversation
    went something like the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我依然记得那天，我父亲让我坐在他的IBM XT前，那台黑绿相间、屏幕闪烁的显示器前，叫我与*它*对话。我们的对话大概是这样的：
- en: '>Hello. How are you feeling today?'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '>你好，今天感觉怎么样？'
- en: '>Hi. Who is this?'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '>嗨，这是谁？'
- en: '>Hi there... How are you today?'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '>嗨，今天怎么样？'
- en: '>I''m fine. You didn''t tell me who you are.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '>我很好。你没告诉我你是谁。'
- en: '>We should be discussing you, not me.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '>我们应该讨论的是你，而不是我。'
- en: '>We should? Why?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '>我们应该？为什么？'
- en: '>Let''s change focus a bit... Tell me about your family.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '>我们换个话题吧... 说说你的家人。'
- en: '>You are impossible to have a conversation with.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '>你简直无法进行对话。'
- en: '>Can you elaborate on that?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '>你能详细说明一下吗？'
- en: '>You are killing me.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '>你快把我搞死了。'
- en: '>Perhaps you would like me to be killing you.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '>也许你希望我在杀了你？'
- en: '>OK, you are freaking me out.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '>好吧，你让我有点害怕。'
- en: '>Perhaps you''re really talking about yourself?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '>也许你真的是在说你自己？'
- en: '>What?? That makes no sense.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '>什么？？这毫无意义。'
- en: '>How would an answer to that help you?'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '>这个回答能对你有帮助吗？'
- en: '>END'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '>结束'
- en: '>Very interesting.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '>非常有趣。'
- en: '>QUIT'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '>退出'
- en: '>Thank you for talking with me.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '>谢谢你和我交谈。'
- en: '>#$###$#@@....'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '>#$###$#@@....'
- en: The *it* I was talking to was a program called **ELIZA**. Written in the mid-1960s
    at MIT's Artificial Intelligence Lab by Joseph Weizenbaum, ELIZA was created to
    mimic the responses of a Rogerian psychotherapist. Though nearly comical when
    examined in any depth, the program was capable of convincing some users that they
    were chatting to an actual human—a remarkable feat considering it was a scant
    200 lines of code that used randomization and regular expressions to parrot back
    responses. Even today, this simple program remains a staple of popular culture.
    If you ask Siri who ELIZA is, she'll tell you she's a friend and a brilliant psychiatrist.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我所说的那个 *it* 是一个名为 **ELIZA** 的程序。ELIZA 是由约瑟夫·韦伊泽恩鲍姆（Joseph Weizenbaum）在 1960
    年代中期编写于麻省理工学院人工智能实验室的，旨在模仿罗杰斯式心理治疗师的回应。尽管在深入研究时几乎显得滑稽，但这个程序能够让一些用户相信他们正在与真正的人类交谈，这是一个了不起的成就，考虑到它仅仅是使用随机化和正则表达式来模仿回复的
    200 行代码。即使在今天，这个简单的程序仍然是流行文化的重要组成部分。如果你问 Siri 谁是 ELIZA，她会告诉你她是你的朋友和一位杰出的心理医生。
- en: If ELIZA was an early example of chatbots, what have we seen since that time?
    In recent years, there has been an explosion of new chatbots. The most notable
    of these is Cleverbot.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 ELIZA 是聊天机器人的早期示例，那么自那时以来我们看到了什么？近年来，新型聊天机器人如雨后春笋般涌现，其中最引人注目的是 Cleverbot。
- en: Cleverbot was released to the world using the web in 1997\. In the years since,
    the bot has racked up hundreds of millions of conversions, and, unlike early chatbots,
    Cleverbot, as its name suggests, appears to become more intelligent with each
    conversion. Though the exact details of the workings of the algorithm are difficult
    to find, it's said to work by recording all conversations in a database and finding
    the most appropriate response by identifying the most similar questions and responses
    in the database.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Cleverbot 在 1997 年通过网络发布到世界上。多年来，该机器人已累积了数亿次对话，与早期的聊天机器人不同，正如其名称所示，Cleverbot
    似乎随着每次对话变得更加智能。尽管其算法的确切细节难以找到，据说它通过记录所有对话并在数据库中查找最相似的问题和回答来工作，以找到最合适的回应。
- en: 'I made up a nonsensical question, shown as follows, and you can see that it
    found something similar to the object of my question in terms of a string match:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我编造了一个无意义的问题，如下所示，你可以看到它在字符串匹配方面找到了与我的问题对象相似的内容：
- en: '![](img/c726d9f1-a00b-4a60-a52c-0909e423500b.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c726d9f1-a00b-4a60-a52c-0909e423500b.png)'
- en: 'I persisted:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '我坚持说：  '
- en: '![](img/1ee74215-5384-4f54-83c4-1c1df46de9ca.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1ee74215-5384-4f54-83c4-1c1df46de9ca.png)'
- en: And, again, I got something... similar?
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，我又得到了类似的东西...
- en: You'll also notice that topics can persist across the conversation. In response,
    I was asked to go into more detail and justify my answer. This is one of the things
    that appears to make Cleverbot, well, clever.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，话题可以在对话中持续存在。作为回应，我被要求详细阐述并证明我的答案。这似乎是使 Cleverbot 变得聪明的其中一点。
- en: While chatbots that learn from humans can be quite amusing, they can also have
    a darker side.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管能从人类那里学到东西的聊天机器人可能相当有趣，但它们也可能有更黑暗的一面。
- en: Several years ago, Microsoft released a chatbot named Tay on to Twitter. People
    were invited to ask Tay questions, and Tay would respond in accordance with her
    *personality*. Microsoft had apparently programmed the bot to appear to be a 19-year-old
    American girl. She was intended to be your virtual *bestie*; the only problem
    was that she started tweeting out extremely racist remarks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，微软在 Twitter 上发布了一个名为 Tay 的聊天机器人。人们被邀请向 Tay 提问，而 Tay 则会根据其 *个性* 回应。微软显然将该机器人编程为看起来像一个
    19 岁的美国女孩。她旨在成为你的虚拟 *闺蜜*；唯一的问题是，她开始发布极端种族主义言论。
- en: As a result of these unbelievably inflammatory tweets, Microsoft was forced
    to pull Tay off Twitter and issue an apology.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些令人难以置信的煽动性推文，微软被迫将 Tay 从 Twitter 下线，并发布了道歉声明。
- en: '"As many of you know by now, on Wednesday we launched a chatbot called Tay.
    We are deeply sorry for the unintended offensive and hurtful tweets from Tay,
    which do not represent who we are or what we stand for, nor how we designed Tay.
    Tay is now offline and we''ll look to bring Tay back only when we are confident
    we can better anticipate malicious intent that conflicts with our principles and
    values."'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '"正如你们许多人现在所知道的，我们在周三推出了一个名为 Tay 的聊天机器人。我们对 Tay 不经意的冒犯性和伤人的推文深表歉意，这些推文不代表我们是谁或我们的立场，也不代表我们设计
    Tay 的方式。Tay 现在已下线，只有当我们有信心能更好地预测与我们原则和价值观相冲突的恶意意图时，我们才会考虑重新启动 Tay。"'
- en: -March 25, 2016 Official Microsoft Blog
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: -2016 年 3 月 25 日 官方微软博客
- en: Clearly, brands that want to release chatbots into the wild in the future should
    take a lesson from this debacle and plan for users to attempt to manipulate them
    to display the worst of human behavior.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，未来那些希望将聊天机器人投入市场的品牌应该从这次的失败中吸取教训，并计划好让用户尝试操控它们，展示人类最糟糕的行为。
- en: There's no doubt that brands are embracing chatbots. Everyone from Facebook
    to Taco Bell is getting in on the game.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 毋庸置疑，品牌们正在拥抱聊天机器人。从 Facebook 到 Taco Bell，每个品牌都在加入这场游戏。
- en: 'Witness the TacoBot:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 见证 TacoBot：
- en: '![](img/99d3a529-9e70-49f4-9478-89c70df123e8.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/99d3a529-9e70-49f4-9478-89c70df123e8.png)'
- en: Yes, it's a real thing. And, despite the stumbles, like Tay, there's a good
    chance the future of UI looks a lot like TacoBot. One last example might even
    help explain why.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，它真的是个现实存在的东西。尽管像 Tay 这样的失败让人跌倒，但未来的用户界面很可能会像 TacoBot 那样。最后的一个例子甚至可能帮助解释其中的原因。
- en: 'Quartz recently launched an app that turns news into a conversation. Rather
    than lay out the day''s stories as a flat list, you are engaged in a chat as if
    you were getting news from a friend:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Quartz 最近推出了一款将新闻转化为对话的应用。与其将当天的新闻按平铺方式展示，它让你参与一场对话，就像是从朋友那里获取新闻一样：
- en: '![](img/230650be-ef8d-4ccb-b59f-00e15d4e6267.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/230650be-ef8d-4ccb-b59f-00e15d4e6267.png)'
- en: 'David Gasca, a PM at Twitter, describes his experience using the app in a post
    on Medium. He describes how the conversational nature invoked feelings normally
    only triggered in human relationships:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Twitter 的项目经理 David Gasca 在 Medium 上发布了一篇文章，描述了他使用该应用的体验。他讲述了这种对话式的设计如何唤起通常只在人与人关系中才会触发的情感：
- en: '"Unlike a simple display ad, in a conversational relationship with my app I
    feel like I owe something to it: I want to click. At the most subconscious level
    I feel the need to reciprocate and not let the app down: "The app has given me
    this content. It''s been very nice so far and I enjoyed the GIFs. I should probably
    click since it''s asking nicely."'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: “与简单的展示广告不同，在与我的应用建立对话关系时，我感觉自己欠它什么：我想要点击。在最潜意识的层面，我感到需要回报，不想让应用失望：‘应用给了我这个内容。到目前为止非常好，我很喜欢这些
    GIF。我应该点击一下，因为它很有礼貌地请求了。’”
- en: 'If that experience is universal—and I expect it is—this could be the next big
    thing in advertising, and I have no doubt that advertising profits will drive
    UI design:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这种体验是普遍的——我相信是——这可能会成为广告的下一个大趋势，我毫不怀疑广告利润将推动用户界面设计的发展：
- en: '"The more the bot acts like a human, the more it will be treated like a human."'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: “机器人越像人类，就越会被当作人类对待。”
- en: -Mat Webb, Technologist and Co-Author of Mind Hacks
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: -Mat Webb，技术专家，Mind Hacks 的合著者
- en: At this point, you're probably dying to know how these things work, so let's
    get on with it!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到这时，你可能迫不及待地想知道这些东西是如何工作的，那我们就继续吧！
- en: The design of chatbots
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聊天机器人的设计
- en: 'The original ELIZA application was 200-odd lines of code. The Python NLTK implementation
    is similarly short. An excerpt is provided from NLTK''s website ([http://www.nltk.org/_modules/nltk/chat/eliza.html](http://www.nltk.org/_modules/nltk/chat/eliza.html)):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的 ELIZA 应用程序大约是 200 行代码。Python 的 NLTK 实现也同样简短。以下是 NLTK 网站上的一段摘录（[http://www.nltk.org/_modules/nltk/chat/eliza.html](http://www.nltk.org/_modules/nltk/chat/eliza.html)）：
- en: '![](img/cd034e74-d600-432f-8611-380af2fee11e.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cd034e74-d600-432f-8611-380af2fee11e.png)'
- en: As you can see from the code, input text was parsed and then matched against
    a series of regular expressions. Once the input was matched, a randomized response
    (that sometimes echoed back a portion of the input) was returned. So, something
    such as, *I need a taco* would trigger a response of, *Would it really help you
    to get a taco?* Obviously, the answer is yes, and, fortunately, we have advanced
    to the point that technology can provide you one (bless you, TacoBot), but this
    was early days still. Shockingly, some people actually believed ELIZA was a real
    human.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 从代码中可以看到，输入文本首先被解析，然后与一系列正则表达式进行匹配。一旦输入匹配成功，系统会返回一个随机的回应（有时会回响部分输入内容）。所以，像 *我需要一个塔可*
    这样的输入会触发一个回应：*你真的需要一个塔可吗？* 显然，答案是“是的”，而且幸运的是，我们已经发展到技术可以提供它（感谢你，TacoBot），但那时仍是初期阶段。令人震惊的是，有些人真的相信
    ELIZA 是一个真实的人类。
- en: But what about more advanced bots? How are they built?
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 那么更先进的机器人呢？它们是如何构建的？
- en: Surprisingly, most chatbots you're likely to encounter aren't even using **machine
    learning** (**ML**); they're what's known as **retrieval-based** models. This
    means responses are predefined according to the question and the context. The
    most common architecture for these bots is something called **Artificial Intelligence
    Markup Language** (**AIML**). AIML is an XML-based schema for representing how
    the bot should interact given the user's input. It's really just a more advanced
    version of how ELIZA works.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，大多数你可能遇到的聊天机器人甚至没有使用**机器学习**（**ML**）；它们被称为**基于检索**的模型。这意味着回答是根据问题和上下文预先定义的。这些机器人的最常见架构是被称为**人工智能标记语言**（**AIML**）的东西。AIML是一种基于XML的模式，用于表示机器人在接收到用户输入时应如何互动。它其实就是ELIZA工作方式的更高级版本。
- en: 'Let''s take a look at how responses are generated using AIML. First, all input
    are preprocessed to normalize them. This means when you input *Waaazzup???* it''s
    mapped to *WHAT IS UP*. This preprocessing step funnels down the myriad ways of
    saying the same thing into one input that can run against a single rule. Punctuation
    and other extraneous input are removed as well at this point. Once that''s complete,
    the input is matched against the appropriate rule. The following is a sample template:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用AIML生成回应。首先，所有输入都会被预处理以进行标准化。这意味着当你输入*Waaazzup???*时，它会被映射为*WHAT IS
    UP*。这个预处理步骤将表达相同意思的多种说法归纳为一个输入，这个输入可以通过一个规则进行匹配。在这一过程中，标点符号和其他无关的输入也会被去除。一旦完成这些处理，输入就会与相应的规则进行匹配。以下是一个样例模板：
- en: '[PRE0]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'That is the basic setup, but you can also layer in wildcards, randomization,
    and prioritization schemes. For example, the following pattern uses wildcard matching:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是基本设置，但你也可以在其中加入通配符、随机化和优先级方案。例如，以下模式使用了通配符匹配：
- en: '[PRE1]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Here, the `*` wildcard matches one or more words before `FOR ME` and then repeats
    those back in the output template. If the user were to type in `Dance for me!`,
    the response would be `I'm a bot. I don't dance. Ever`.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`*`通配符匹配`FOR ME`之前的一个或多个单词，然后在输出模板中重复这些单词。如果用户输入的是`为我跳舞！`，则回复将是`我是一个机器人。我不跳舞。永远不`。
- en: As you can see, these rules don't make for anything that approximates any type
    of real intelligence, but there are a few tricks that strengthen the illusion.
    One of the better ones is the ability to generate responses conditioned on a topic.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，这些规则并不能产生任何接近真实智能的东西，但有一些技巧可以增强这种错觉。其中一个更好的技巧是能够生成基于话题的回应。
- en: 'For example, here''s a rule that invokes a topic:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，这里有一个引发话题的规则：
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the topic is set, the rules specific to that context can be matched:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦话题确定，特定于该上下文的规则可以进行匹配：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Let''s see what this interaction might look like:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这种互动可能是什么样子的：
- en: '>I like turtles!'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '>我喜欢乌龟！'
- en: '>I feel like this whole turtle thing could be a problem. What do you like about
    them?'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '>我觉得这整个乌龟的事儿可能会是个问题。你喜欢它们什么呢？'
- en: '>I like how they hide in their shell.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '>我喜欢它们如何躲进壳里。'
- en: '>I wish, like a turtle, I could hide from this conversation.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '>我希望，像乌龟一样，我能躲避这场对话。'
- en: You can see that the continuity across the conversation adds a measure of realism.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，贯穿整个对话的连贯性增加了一定的现实感。
- en: You're probably thinking that this can't be state of the art in this age of
    deep learning, and you're right. While most bots are rule-based, the next generation
    of chatbots are emerging, and they're based on neural networks.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能在想，在这个深度学习的时代，这种技术不可能是最先进的，没错。虽然大多数聊天机器人是基于规则的，但下一代聊天机器人正在崭露头角，它们基于神经网络。
- en: In 2015, Oriol Vinyas and Quoc Le of Google published a paper, [http://arxiv.org/pdf/1506.05869v1.pdf](https://arxiv.org/pdf/1506.05869v1.pdf),
    that described the construction of a neural network based on sequence-to-sequence
    models. This type of model maps an input sequence, such as *ABC*, to and output
    sequence, such as *XYZ*. These inputs and outputs might be translations from one
    language to another, for example. In the case of their work here, the training
    data was not language translation, but rather tech support transcripts and movie
    dialogues. While the results from both models are interesting, it was the interactions
    based on the movie model that stole the headlines.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在2015年，谷歌的Oriol Vinyas和Quoc Le发表了一篇论文，[http://arxiv.org/pdf/1506.05869v1.pdf](https://arxiv.org/pdf/1506.05869v1.pdf)，描述了基于序列到序列模型构建神经网络的过程。这种类型的模型将输入序列（如*ABC*）映射到输出序列（如*XYZ*）。这些输入和输出可能是不同语言之间的翻译。例如，在他们的研究中，训练数据并不是语言翻译，而是技术支持记录和电影对话。尽管这两个模型的结果都很有趣，但基于电影模型的互动却成为了头条新闻。
- en: 'The following are sample interactions taken from the paper:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是论文中的一些示例互动：
- en: '![](img/0dce3fa1-0caf-44b3-bfec-863d7afe8cb6.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0dce3fa1-0caf-44b3-bfec-863d7afe8cb6.png)'
- en: 'None of this was explicitly encoded by humans or present in the training set
    as asked, and, yet, looking at this, it''s frighteningly like speaking with a
    human. But let''s see more:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 这些内容没有被人类明确编码，也不在训练集里，如问题所要求的。然而，看着这些，感觉像是在和一个人对话，真让人不寒而栗。接下来我们来看看更多内容：
- en: '![](img/591bc3ed-fb58-47f6-ac3e-77da623bd8ad.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/591bc3ed-fb58-47f6-ac3e-77da623bd8ad.png)'
- en: 'Notice that the model is responding with what appears to be knowledge of gender
    (**he**, **she**), **place** (England), and career (**player**). Even questions
    of meaning, ethics, and morality are fair game:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型正在响应看起来像是性别（**他**，**她**）、**地点**（英格兰）和职业（**运动员**）的知识。即使是关于意义、伦理和道德的问题也是可以探讨的：
- en: '![](img/7d841798-609f-4249-8237-a9b8092442e4.png)![](img/7dc81f71-3e26-4ae0-8cd0-2f5652b4ad62.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7d841798-609f-4249-8237-a9b8092442e4.png)![](img/7dc81f71-3e26-4ae0-8cd0-2f5652b4ad62.png)'
- en: If that transcript doesn't give you a slight chill, there's a chance you might
    already be some sort of AI.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个对话记录没有让你感到一丝寒意，那你很可能已经是某种人工智能了。
- en: I wholeheartedly recommend reading the entire paper. It isn't overly technical,
    and it will definitely give you a glimpse of where the technology is headed.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈推荐通读整篇论文。它并不太技术性，但肯定会让你看到这项技术的未来发展方向。
- en: We've talked a lot about the history, types, and design of chatbots, but let's
    now move on to building our own. We'll take two approaches to this. This first
    will use a technique we saw in previously, cosine similarity, and the second will
    leverage sequence-to-sequence learning.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了很多关于聊天机器人的历史、类型和设计，但现在我们来开始构建我们自己的聊天机器人。我们将采用两种方法。第一种将使用我们之前看到的余弦相似度技术，第二种将利用序列到序列学习。
- en: Building a chatbot
  id: totrans-96
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建聊天机器人
- en: Now, having seen what's possible in terms of chatbots, you most likely want
    to build the best, most state-of-the-art, Google-level bot out there, right? Well,
    just put that out of your mind for now because we're going start by doing the
    exact opposite. We're going to build the most amazingly awful bot ever!
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，既然已经看到聊天机器人的潜力，你可能想要构建最好的、最先进的、类似Google级别的机器人，对吧？好吧，先把这个想法放在一边，因为我们现在将从做完全相反的事情开始。我们将构建一个最糟糕、最糟糕的机器人！
- en: This may sound disappointing, but if your goal is just to build something very
    cool and engaging (that doesn't take hours and hours to construct), this is a
    great place to start.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来可能让人失望，但如果你的目标只是构建一些非常酷且吸引人的东西（而且不需要花费数小时来构建），这是一个很好的起点。
- en: We're going to leverage the training data derived from a set of real conversations
    with Cleverbot. The data was collected from [http://notsocleverbot.jimrule.com](http://notsocleverbot.jimrule.com).
    This site is perfect, as it has people submit the most absurd conversations they
    had with Cleverbot.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用从Cleverbot的真实对话中获取的训练数据。这些数据是从[http://notsocleverbot.jimrule.com](http://notsocleverbot.jimrule.com)收集的。这个网站非常适合，因为它收录了人们与Cleverbot进行的最荒谬的对话。
- en: 'Let''s take a look at a sample conversation between Cleverbot and a user from
    the site:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下Cleverbot与用户之间的示例对话：
- en: '![](img/6d9fb676-13dc-4bb1-b64c-9a172256926b.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6d9fb676-13dc-4bb1-b64c-9a172256926b.png)'
- en: While you are free to use the techniques for web scraping that we used in earlier
    chapters to collect the data, you can find a `.csv` of the data in the GitHub
    repo for this chapter.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可以自由使用我们在前几章中介绍的网页抓取技术来收集数据，但你也可以在本章的GitHub仓库中找到一个`.csv`格式的数据。
- en: 'We''ll start again in our Jupyter Notebook. We''ll load, parse, and examine
    the data. We''ll first import pandas and the Python regular expressions library,
    `re`. We''re also going to set the option in pandas to widen our column width
    so we can see the data better:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次从Jupyter Notebook开始。我们将加载、解析并检查数据。首先，我们将导入pandas库和Python的正则表达式库`re`。我们还将设置pandas的选项，扩大列宽，以便更好地查看数据：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now we''ll load in our data:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将加载我们的数据：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code results in the following output:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会产生以下输出：
- en: '![](img/a48abdb7-bcff-42f5-9d55-c7da9f2aed78.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a48abdb7-bcff-42f5-9d55-c7da9f2aed78.png)'
- en: 'Since we''re only interested in the first column, the conversation data, we''ll
    parse that out:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们只对第一列——对话数据感兴趣，因此我们将只解析这一列：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code results in the following output:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码会产生以下输出：
- en: '![](img/63ee8de3-1c46-4190-afb9-bad34daa3a40.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](img/63ee8de3-1c46-4190-afb9-bad34daa3a40.png)'
- en: 'You should be able to make out that we have interactions between **User** and
    **Cleverbot**, and that either can initiate the conversation. To get the data
    in the format we need, we''ll have to parse it into question-and-response pairs.
    We aren''t necessarily concerned with who says what, but with matching up each
    response to each question. You''ll see why in a bit. Let''s now do a bit of regular
    expression magic on the text:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能看出我们有**用户**和**Cleverbot**之间的互动，且任一方都可以发起对话。为了获得我们所需的格式，我们必须将数据解析为问答对。我们不一定关注谁说了什么，而是关注如何将每个回答与每个问题匹配。稍后你会明白为什么。现在，让我们对文本进行一些正则表达式的魔法处理：
- en: '[PRE7]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code results in the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/b8069791-f4f5-4908-851f-585e4ef3b2d7.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b8069791-f4f5-4908-851f-585e4ef3b2d7.png)'
- en: OK, lots of code there. What just happened? We first created a list to hold
    our question-and-response tuples. We then passed our conversations through a function
    to split them into those pairs using regular expressions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，这里有很多代码。刚才发生了什么？我们首先创建了一个列表来存储问题和回答的元组。然后我们通过一个函数将我们的对话拆分成这些对，使用了正则表达式。
- en: Finally, we set it all into a pandas DataFrame with columns labelled `q` and
    `a`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将所有这些放入一个 pandas DataFrame 中，列标为 `q` 和 `a`。
- en: 'We''re now going to apply a bit of algorithm magic to match up the closest
    question to the one a user inputs:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将应用一些算法魔法，来匹配与用户输入问题最相似的问题：
- en: '[PRE8]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the preceding code, we imported our tf-idf vectorization library and the
    cosine similarity library. We then used our training data to create a tf-idf matrix.
    We can now use this to transform our own new questions and measure the similarity
    to existing questions in our training set. Let''s do that now:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们导入了 tf-idf 向量化库和余弦相似度库。然后我们使用训练数据创建了一个 tf-idf 矩阵。现在我们可以利用这个矩阵来转换我们自己的新问题，并衡量它们与训练集中现有问题的相似度。现在就让我们做这个：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding code results in the following output:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/66fa20ef-b27f-44e2-8af5-bae861983084.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](img/66fa20ef-b27f-44e2-8af5-bae861983084.png)'
- en: 'What are we looking at here? This is the cosine similarity between the question
    I asked and the top-five closest questions. On the left is the index, and on the
    right is the cosine similarity. Let''s take a look at those:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到了什么？这是我提出的问题与最相似的五个问题之间的余弦相似度。在左侧是索引，右侧是余弦相似度。让我们来看一下这些：
- en: '[PRE10]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This results in the following output:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/e0f4a09b-d60c-4db2-a958-9e0b0ee6411a.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0f4a09b-d60c-4db2-a958-9e0b0ee6411a.png)'
- en: As you can see, nothing is exactly the same, but there are definitely some similarities.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，没有完全相同的，但确实有一些相似之处。
- en: 'Let''s now take a look at the response:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看这个回答：
- en: '[PRE11]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code results in the following output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/6f020de1-b522-4912-ba1e-52f45a718d23.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6f020de1-b522-4912-ba1e-52f45a718d23.png)'
- en: OK, so our bot seems to have an attitude already. Let's push further.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们的机器人似乎已经有了个性。让我们更进一步。
- en: 'We''ll create a handy function so that we can test a number of statements easily:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建一个方便的函数，这样我们就能轻松地测试多个陈述：
- en: '[PRE12]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This results in the following output:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/c9a792e9-1f44-42d2-946b-9b3736c2d8b0.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9a792e9-1f44-42d2-946b-9b3736c2d8b0.png)'
- en: 'We have clearly created a monster, so we''ll continue:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们显然已经创造了一个怪物，所以我们会继续：
- en: '[PRE13]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This results in the following output:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '![](img/9377dd8e-c551-4e69-8e32-709f149a06c1.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9377dd8e-c551-4e69-8e32-709f149a06c1.png)'
- en: 'I''m enjoying this. Let''s keep rolling with it:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我很享受这个过程。让我们继续：
- en: '[PRE14]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](img/beab46df-ae86-4fba-9292-b72e5ce81c0d.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](img/beab46df-ae86-4fba-9292-b72e5ce81c0d.png)'
- en: '[PRE15]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](img/15082c21-bd1f-4d20-ad70-d46420b6a5db.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15082c21-bd1f-4d20-ad70-d46420b6a5db.png)'
- en: '[PRE16]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](img/265ec47e-43b4-4108-8508-f84206c682d4.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](img/265ec47e-43b4-4108-8508-f84206c682d4.png)'
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '![](img/4b18b671-7d88-47a2-aee5-9ee79c643ede.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4b18b671-7d88-47a2-aee5-9ee79c643ede.png)'
- en: '[PRE18]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](img/25bd083c-6922-4dd8-a00b-390058199df7.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25bd083c-6922-4dd8-a00b-390058199df7.png)'
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](img/f9571b10-2ff4-4cba-bb2b-d493ba9a961e.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f9571b10-2ff4-4cba-bb2b-d493ba9a961e.png)'
- en: Remarkably, this may be one of the best conversations I've had in a while, bot
    or not.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 令人惊讶的是，这可能是我一段时间以来经历过的最棒的对话之一，无论是机器人还是其他。
- en: Now while this was a fun little project, let's now move on to a more advanced
    modeling technique using sequence-to-sequence modeling.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个有趣的小项目，但现在让我们进入一个更高级的建模技术：序列到序列建模。
- en: Sequence-to-sequence modeling for chatbots
  id: totrans-158
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聊天机器人序列到序列建模
- en: For this next task, we'll leverage a couple libraries discussed in [Chapter
    8](5df6fae8-a5c0-4fab-8508-baef0085b4f5.xhtml), *Classifying Images with Convolutional
    Neural Networks*, TensorFlow and Keras. Both can be `pip` installed if you haven't
    done that already.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于接下来的任务，我们将利用在[第8章](5df6fae8-a5c0-4fab-8508-baef0085b4f5.xhtml)中讨论的几个库，*使用卷积神经网络对图像进行分类*，TensorFlow
    和 Keras。如果你还没有安装它们，可以通过 `pip` 安装。
- en: 'We''re also going to use the type of advanced modeling discussed earlier in
    the chapter; it''s a type of deep learning called **sequence-to-sequence modeling**.
    This is frequently used in machine translation and question-answering applications,
    as it allows us to map an input sequence of any length to an output sequence of
    any length:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将使用本章前面讨论的那种高级建模方法；它是一种深度学习方法，叫做**序列到序列建模**。这种方法常用于机器翻译和问答应用，因为它可以将任何长度的输入序列映射到任何长度的输出序列：
- en: '![](img/2b04592c-6902-4403-870e-4266daf360c7.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2b04592c-6902-4403-870e-4266daf360c7.png)'
- en: 'Source: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 来源：https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html
- en: Francois Chollet has an excellent introduction to this type of model on the
    blog for Keras: [https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html).
    It's worth a read.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: François Chollet 在 Keras 博客中有一个很好的关于这种模型的介绍：[https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html](https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html)。值得一读。
- en: 'We''re going to make heavy use of his example code to build out our model.
    While his example uses machine translation, English to French, we''re going to
    repurpose it for question-answering using our Cleverbot dataset:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将大量使用他的示例代码来构建我们的模型。尽管他的示例使用的是机器翻译（英语到法语），但我们将重新利用它来进行问答，并使用我们的 Cleverbot
    数据集：
- en: 'Set the imports:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置导入项：
- en: '[PRE20]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Set up the training parameters:'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置训练参数：
- en: '[PRE21]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: We'll use these to start. We can examine the success of our model and then adjust
    as necessary.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这些开始。我们可以检查模型的成功，并根据需要进行调整。
- en: 'The first step in data processing will be to take our data, get it in the proper
    format, and then vectorize it. We''ll go step by step:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理的第一步是将数据获取到正确的格式，然后进行向量化。我们将一步步进行：
- en: '[PRE22]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This creates lists for our questions and answers (the targets) as well as sets
    for the individual characters in our questions and answers. This model will actually
    work by generating one character at a time:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们的提问和回答（目标）创建列表，并为我们的问题和答案中的单个字符创建集合。该模型实际上是通过一次生成一个字符来工作的：
- en: 'Let''s limit our question-and-answer pairs to 50 characters or fewer. This
    will help speed up our training:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将问题和回答对的字符数限制为50个或更少。这将有助于加速训练：
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let''s set up our input and target text lists:'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们设置输入文本和目标文本列表：
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The preceding code gets our data in the proper format. Note that we add a tab
    (`\t`) and a newline (`\n`) character to the target texts. This will serve as
    our start and stop tokens for the decoder.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码将数据格式化为正确的格式。请注意，我们向目标文本中添加了制表符（`\t`）和换行符（`\n`）。这些将作为解码器的开始和停止标记。
- en: 'Let''s take a look at the input texts and the target texts:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看输入文本和目标文本：
- en: '[PRE25]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The preceding code generates the following output:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/3318e65a-0fc7-4a07-8620-8d81902c6079.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/3318e65a-0fc7-4a07-8620-8d81902c6079.png)'
- en: '[PRE26]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding code generates the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/0e4cad67-bd2e-41bf-a7d0-19bb870fe629.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0e4cad67-bd2e-41bf-a7d0-19bb870fe629.png)'
- en: 'Let''s take a look at those input and target-character sets now:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看这些输入和目标字符集：
- en: '[PRE27]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The preceding code generates the following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/32be9bf4-34e1-4e41-bc29-af5e999cf409.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](img/32be9bf4-34e1-4e41-bc29-af5e999cf409.png)'
- en: '[PRE28]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The preceding code generates the following output:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/a1f54278-2513-40d3-ae69-42a7a06014d5.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a1f54278-2513-40d3-ae69-42a7a06014d5.png)'
- en: 'Next, we''ll do some additional preparation for the data that will feed into
    the model. Although data can be fed in any length and returned in any length,
    we need to add padding up to the max length of the data for the model to work:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将对传入模型的数据做一些额外的准备。尽管数据可以是任意长度并且返回的长度也可以是任意的，但我们需要将数据填充到最大长度，以便模型可以正常工作：
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The preceding code generates the following output:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了以下输出：
- en: '![](img/34094ae5-2198-4796-afd4-066b044361a9.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](img/34094ae5-2198-4796-afd4-066b044361a9.png)'
- en: 'Next, we''ll vectorize our data using one-hot encoding:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用独热编码对数据进行向量化：
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Let''s take a look at one of these vectors:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下其中一个向量：
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding code generates the following output:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '![](img/bc180728-de1a-4d09-bc5d-68c4f6a3e34b.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bc180728-de1a-4d09-bc5d-68c4f6a3e34b.png)'
- en: From the preceding figure, you'll notice that we have a one-hot encoded vector
    of our character data, which will be used in our model.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从上图中，你会注意到我们有一个对字符数据进行独热编码的向量，这将在我们的模型中使用。
- en: 'We now set up our sequence-to-sequence model-encoder and -decoder LSTMs:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们设置好我们的序列到序列模型的编码器和解码器LSTM：
- en: '[PRE32]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then we move on to the model itself:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们继续讲解模型本身：
- en: '[PRE33]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: In the preceding code, we defined our model using our encoder and decoder input
    and our decoder output. We then compile it, fit it, and save it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们使用编码器和解码器的输入以及解码器的输出来定义模型。然后我们编译它，训练它，并保存它。
- en: We set up the model to use 1,000 samples. Here, we also split the data 80/20
    to training and validation, respectively. We also set our epochs at 100, so this
    will essentially run for 100 cycles. On a standard MacBook Pro, this may take
    around an hour to complete.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将模型设置为使用1,000个样本。在这里，我们还将数据按80/20的比例分为训练集和验证集。我们还将训练周期设为100，因此它将运行100个周期。在一台标准的MacBook
    Pro上，这大约需要一个小时才能完成。
- en: 'Once that cell is run, the following output will be generated:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦该单元运行，以下输出将被生成：
- en: '![](img/ad208163-5986-4a4d-b6d4-95b800737727.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ad208163-5986-4a4d-b6d4-95b800737727.png)'
- en: 'The next step is our inference step. We''ll use the states generated from this
    model to feed into our next model to generate our output:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是我们的推理步骤。我们将使用这个模型生成的状态作为输入，传递给下一个模型以生成我们的输出：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The preceding code generates the following output:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码生成了以下输出：
- en: '![](img/57a91f41-a7da-468e-a92a-7589eb73593c.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![](img/57a91f41-a7da-468e-a92a-7589eb73593c.png)'
- en: As you can see, the results of our model are quite repetitive. But then we only
    used 1,000 samples and the responses were generated one character at a time, so
    this is actually fairly impressive.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们模型的结果相当重复。但是我们仅使用了1,000个样本，并且响应是一个字符一个字符生成的，所以这实际上已经相当令人印象深刻。
- en: If you want better results, rerun the model using more sample data and more
    epochs.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想获得更好的结果，可以使用更多样本数据和更多训练周期重新运行模型。
- en: 'Here, I have provide some of the more humorous output I''ve noted from much
    longer training periods:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我提供了一些我从更长时间的训练中记录下来的较为幽默的输出：
- en: '![](img/e99022f7-f9da-4cf0-8a12-fb58d03c72ff.png)![](img/6936b02e-4137-4c66-ad25-1e9291112554.png)![](img/7ed8edd3-c574-482d-9d93-d184a8e3f59e.png)![](img/4096f414-dfd7-46c7-a6e0-efb90b3712bd.png)![](img/78e8de23-1a8e-488e-b326-a2e01a4bbf44.png)![](img/c60b8d01-7f73-4862-b039-3ae1cbcea5c3.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e99022f7-f9da-4cf0-8a12-fb58d03c72ff.png)![](img/6936b02e-4137-4c66-ad25-1e9291112554.png)![](img/7ed8edd3-c574-482d-9d93-d184a8e3f59e.png)![](img/4096f414-dfd7-46c7-a6e0-efb90b3712bd.png)![](img/78e8de23-1a8e-488e-b326-a2e01a4bbf44.png)![](img/c60b8d01-7f73-4862-b039-3ae1cbcea5c3.png)'
- en: Summary
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we took a full tour of the chatbot landscape. It's clear that
    we're on the cusp of an explosion of these types of applications. The *Conversational
    UI* revolution is just about to begin. Hopefully, this chapter has inspired you
    to create your own bot, but if not, we hope you have a much richer understanding
    of how these applications work and how they'll shape our future.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们对聊天机器人领域进行了全面的探索。很明显，我们正处于这类应用程序爆炸性增长的前夕。*对话式用户界面*的革命即将开始。希望本章能激励你创建自己的聊天机器人，如果没有，也希望你对这些应用的工作原理及其如何塑造我们的未来有了更丰富的理解。
- en: 'I''ll let the app say the final words:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我会让应用程序说出最后的结论：
- en: '[PRE35]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
