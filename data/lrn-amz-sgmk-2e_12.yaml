- en: 'Chapter 9: Scaling Your Training Jobs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第九章：扩展你的训练任务
- en: In the four previous chapters, you learned how to train models with built-in
    algorithms, frameworks, or your own code.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在前四章中，你学习了如何使用内置算法、框架或你自己的代码来训练模型。
- en: 'In this chapter, you''ll learn how to scale training jobs, allowing them to
    train on larger datasets while keeping training time and cost under control. We''ll
    start by discussing when and how to take scaling decisions, thanks to monitoring
    information and simple guidelines. You''ll also see how to collect profiling information
    with **Amazon** **SageMaker Debugger**, in order to understand how efficient your
    training jobs are. Then, we''ll look at several key techniques for scaling: **pipe
    mode**, **distributed training**, **data parallelism**, and **model parallelism**.
    After that, we''ll launch a large training job on the large **ImageNet** dataset
    and see how to scale it. Finally, we''ll discuss storage alternatives to **S3**
    for large-scale training, namely **Amazon** **EFS** and **Amazon** **FSx for Lustre**.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何扩展训练任务，使其能够在更大的数据集上训练，同时控制训练时间和成本。我们将首先讨论如何根据监控信息和简单的指南来做出扩展决策。你还将看到如何使用**Amazon**
    **SageMaker Debugger**收集分析信息，以了解训练任务的效率。接着，我们将探讨几种扩展的关键技术：**管道模式**、**分布式训练**、**数据并行性**和**模型并行性**。之后，我们将在大型**ImageNet**数据集上启动一个大型训练任务，并观察如何进行扩展。最后，我们将讨论用于大规模训练的存储替代方案，即**Amazon**
    **EFS**和**Amazon** **FSx for Lustre**。
- en: 'We''ll cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Understanding when and how to scale
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解何时以及如何进行扩展
- en: Monitoring and profiling training jobs with Amazon SageMaker Debugger
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon SageMaker Debugger监控和分析训练任务
- en: Streaming datasets with pipe mode
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用管道模式流式传输数据集
- en: Distributing training jobs
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分发训练任务
- en: Scaling an image classification model on ImageNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在ImageNet上扩展图像分类模型
- en: Training with data and model parallelism
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据并行性和模型并行性进行训练
- en: Using other storage services
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用其他存储服务
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser at [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create it. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个AWS账户才能运行本章中包含的示例。如果你还没有账户，请访问[https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)创建一个。你还应熟悉AWS免费套餐([https://aws.amazon.com/free/](https://aws.amazon.com/free/))，它允许你在一定的使用限制内免费使用许多AWS服务。
- en: You will need to install and configure the AWS **Command Line Interface** (**CLI**)
    for your account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置AWS **命令行界面** (**CLI**) ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/))。
- en: You will need a working `pandas`, `numpy`, and more).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要安装并配置`pandas`、`numpy`等工具。
- en: Code examples included in this book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可以在GitHub上找到，链接为[https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装Git客户端来访问这些示例([https://git-scm.com/](https://git-scm.com/))。
- en: Understanding when and how to scale
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解何时以及如何进行扩展
- en: Before we dive into scaling techniques, let's first discuss the monitoring information
    that we should consider when deciding whether we need to scale, and how we should
    do it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解扩展技术之前，让我们首先讨论在决定是否需要扩展以及如何扩展时应考虑的监控信息。
- en: Understanding what scaling means
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解扩展意味着什么
- en: The training log tells us how long the job lasted. In itself, this isn't really
    useful. How long is *too long*? This feels very subjective, doesn't it? Furthermore,
    even when training on the same dataset and infrastructure, changing a single hyperparameter
    can significantly impact training time. Batch size is one example of this, and
    there are many more.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志告诉我们任务持续了多长时间。但单凭这一点，其实并不太有用。多长时间是*太长*了呢？这个问题似乎非常主观，不是吗？此外，即使在相同的数据集和基础设施上，改变一个超参数也可能显著影响训练时间。批次大小就是一个例子，且这种情况还有很多。
- en: 'When we''re concerned about training time, I think we''re really trying to
    answer three questions:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们关注训练时间时，我认为我们实际上是在尝试回答三个问题：
- en: Is the training time compatible with our business requirements?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练时间是否与我们的业务需求兼容？
- en: Are we making good use of the infrastructure we're paying for? Did we underprovision
    or overprovision?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们是否在充分利用我们所支付的基础设施？我们是否进行了资源不足或过度配置？
- en: Could we train faster without spending more money?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能在不花更多钱的情况下训练得更快吗？
- en: Adapting training time to business requirements
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使训练时间适应业务需求
- en: Ask yourself this question—what would be the direct impact on your business
    if your training job ran twice as fast? In many cases, the honest answer should
    be *none*. There is no clear business metric that would be improved.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 问问自己这个问题——如果您的训练任务运行速度加倍，直接对您的业务产生什么影响？在很多情况下，诚实的答案应该是*没有*。没有任何明确的业务指标会因此得到改善。
- en: Sure, some companies run training jobs that last days, even weeks—think autonomous
    driving or life sciences. For them, any significant reduction in training time
    means that they get results much faster, analyze them, and launch the next iteration.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，一些公司运行的训练任务可能需要几天甚至几周——比如自动驾驶或生命科学领域。对他们来说，训练时间的任何显著减少意味着他们能够更快获得结果，进行分析，并启动下一个迭代。
- en: Some other companies want the freshest models possible, and they retrain every
    hour. Of course, training time needs to be kept under control to make the deadline.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 其他一些公司希望拥有最新的模型，并且每小时都会重新训练。当然，训练时间需要控制在一定范围内，以确保能按时完成。
- en: In both types of companies, scaling is vital. For everyone else, things are
    not so clear. If your company trains a production model every week or every month,
    does it really matter whether training reaches the same level of accuracy 30 minutes
    sooner? Probably not.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种类型的公司中，扩展能力至关重要。对于其他公司而言，情况就不那么明确。如果您的公司每周或每月训练一次生产模型，训练时间提前 30 分钟能否带来明显的效果？可能不会。
- en: Some people would certainly object that they need to train a lot of models all
    of the time. I'm afraid this is a fallacy. As SageMaker lets you create on-demand
    infrastructure whenever you need it, training activities will not be capacity-bound.
    This is the case when you work with physical infrastructure, but not with cloud
    infrastructure. Even if you need to train 1,000 **XGBoost** jobs every day, does
    it really matter whether each individual job takes 5 minutes instead of 6? Probably
    not.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 有些人肯定会反对，说他们需要一直训练很多模型。我恐怕这是一种误解。由于 SageMaker 允许您根据需要随时创建按需基础设施，因此训练活动不会受到容量的限制。这种情况适用于物理基础设施，但不适用于云基础设施。即使您每天需要训练
    1,000 个 **XGBoost** 任务，是否真的在乎每个任务需要 5 分钟而不是 6 分钟？可能不。
- en: Some would retort that "the faster you train, the less it costs." Again, this
    is a fallacy. The cost of a SageMaker training job is the training time in seconds
    multiplied by the cost of the instance type and by the number of instances. If
    you pick a larger instance type, training time will most probably decrease. Will
    it decrease enough to offset the increased instance cost? Maybe, maybe not. Some
    training workloads will make good use of the extra infrastructure, and some won't.
    The only way to know is to run tests and make data-driven decisions.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 有人可能会反驳，“训练越快，成本越低。”再次提醒，这是一种误解。SageMaker 训练任务的成本是训练时间（以秒为单位）乘以实例类型的成本，再乘以实例数量。如果您选择更大的实例类型，训练时间很可能会缩短。它是否会缩短足够多，以抵消增加的实例成本？也许会，也许不会。某些训练工作负载能充分利用额外的基础设施，而某些则不能。唯一的办法就是进行测试并做出数据驱动的决策。
- en: Right-sizing training infrastructure
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确配置训练基础设施
- en: SageMaker supports a long list of instance types, which looks like a very nice
    candy store ([https://aws.amazon.com/sagemaker/pricing/instance-types](https://aws.amazon.com/sagemaker/pricing/instance-types)).
    All you have to do is call an API to fire up an 8 GPU EC2 instance – more powerful
    than any server your company would have allowed you to buy. Caveat emptor – don't
    forget the "pricing" part of the URL!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 支持一长串实例类型，看起来像一个非常诱人的糖果店 ([https://aws.amazon.com/sagemaker/pricing/instance-types](https://aws.amazon.com/sagemaker/pricing/instance-types))。您所需要做的，就是调用一个
    API 启动一个 8 GPU 的 EC2 实例——它比您公司允许您购买的任何服务器都要强大。提醒注意——不要忘记 URL 中的“定价”部分！
- en: Note
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If the words "EC2 instance" don't mean much to you, I would definitely recommend
    reading a bit about **Amazon** **EC2** at [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“EC2 实例”这几个词对您来说没有太大意义，我强烈建议您阅读一下关于 **Amazon** **EC2** 的资料，[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/concepts.html)。
- en: Granted, cloud infrastructure doesn't require you to pay a lot of money upfront
    to buy and host servers. Still, the AWS bill will come at the end of the month.
    Hence, even using cost optimization techniques such as **Managed Spot Training**
    (which we'll discuss in the next chapter), it's critical that you right-size your
    training infrastructure.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，云基础设施不要求你提前支付大量资金购买和托管服务器。然而，AWS 的账单还是会在月底到来。因此，即使使用了诸如 **Managed Spot Training**（我们将在下一章讨论）等成本优化技术，正确配置你的训练基础设施仍然至关重要。
- en: 'My advice is always the same:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我的建议始终是一样的：
- en: Identify business requirements that depend on training time.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定依赖于训练时间的业务需求。
- en: Start with the smallest reasonable amount of infrastructure.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从最小合理的基础设施开始。
- en: Measure technical metrics and cost.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量技术指标和成本。
- en: 'If business requirements are met, did you overprovision? There are two possible
    answers:'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果业务需求已满足，你是否过度配置了？有两种可能的答案：
- en: 'a) **Yes**: Scale down and repeat.'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: a) **是**：缩小规模并重复。
- en: 'b) **No**: You''re done.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) **否**：你完成了。
- en: If business requirements are not met, identify bottlenecks.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果业务需求未能满足，识别瓶颈。
- en: Run some tests on scaling up (larger instance type) and scaling out (more instances).
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行一些扩展测试（更大的实例类型）和横向扩展（更多实例）。
- en: Measure technical metrics and costs.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量技术指标和成本。
- en: Implement the best solution for your business context.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施最适合你业务环境的解决方案。
- en: Repeat.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复。
- en: Of course, this process is as good as the people who take part in it. Be critical!
    "Too slow" is not a data point—it's an opinion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个过程的效果取决于参与者的水平。要保持批判性思维！“太慢”不是一个数据点——它只是一个意见。
- en: Deciding when to scale
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定何时扩展
- en: 'When it comes to monitoring information, you can rely on three sources: the
    training log, **Amazon** **CloudWatch** metrics, and the profiling capability
    in **Amazon** **SageMaker Debugger**.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控信息方面，你可以依赖三个来源：训练日志、**Amazon** **CloudWatch** 指标，以及 **Amazon** **SageMaker
    Debugger** 中的性能分析功能。
- en: Note
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: If "CloudWatch" doesn't mean much to you, I would definitely recommend reading
    a bit about it at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如果“CloudWatch”对你来说意义不大，我强烈推荐你阅读一下 [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/)。
- en: The training log shows you the total training time and the number of samples
    per second. As discussed in the previous section, total training time is not a
    very useful metric. Unless you have very strict deadlines, it's best to ignore
    it. The number of samples per second is more interesting. You can use it to compare
    your training job to benchmarks available in research papers or blog posts. If
    someone has managed to train the same model twice as fast on the same GPU, you
    should be able to do the same. When you get close to that number, you'll also
    know that there's not a lot of room for improvement and that other scaling techniques
    should be considered.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 训练日志展示了总训练时间和每秒样本数。正如前一部分讨论的，总训练时间不是一个很有用的指标。除非你有非常严格的截止日期，否则最好忽略它。每秒样本数更为有趣。你可以用它来将你的训练任务与研究论文或博客文章中提供的基准进行比较。如果某人在相同的
    GPU 上将同一个模型训练速度提高了一倍，你也应该能做到。当你接近那个数字时，你也能知道改进的空间不大，需要考虑其他扩展技术。
- en: CloudWatch gives you coarse-grained infrastructure metrics with a 1-minute resolution.
    For simple training jobs, these metrics are all you need to check if your training
    makes efficient use of the underlying infrastructure and identify potential bottlenecks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: CloudWatch 提供了 1 分钟分辨率的粗粒度基础设施指标。对于简单的训练任务，这些指标足以检查你的训练是否高效利用了底层基础设施，并帮助你识别潜在的瓶颈。
- en: For more complex jobs (distributed training, custom code, and so on), SageMaker
    Debugger gives you fine-grained, near real-time infrastructure and Python metrics,
    with a resolution as low as 100 milliseconds. This information will let you drill
    down and identify complex performance and scaling problems.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的任务（分布式训练、自定义代码等），SageMaker Debugger 提供了细粒度、近实时的基础设施和 Python 指标，分辨率低至 100
    毫秒。这些信息能帮助你深入分析并识别复杂的性能和扩展问题。
- en: Deciding how to scale
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定如何扩展
- en: As mentioned earlier, you can either scale up (move to a bigger instance) or
    scale out (use several instances for distributed training). Let's look at the
    pros and cons.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，你可以选择扩展（迁移到更大的实例）或横向扩展（使用多个实例进行分布式训练）。我们来看看各自的优缺点。
- en: Scaling up
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展
- en: Scaling up is simple. You just need to change the instance type. Monitoring
    stays the same, and there's only one training log to read. Last but not least,
    training on a single instance is predictable and very often delivers the best
    accuracy, as there's only one set of model parameters to learn and update.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 向上扩展很简单。你只需要更改实例类型。监控保持不变，而且只需要读取一个训练日志。最后但同样重要的是，单实例训练具有可预测性，并且通常能提供最佳的准确性，因为只有一组模型参数需要学习和更新。
- en: On the downside, your algorithm may not be compute-intensive and parallel enough
    to benefit from the extra computing power. Extra vCPUs and GPUs are only useful
    if they're put to work. Your network and storage layers must also be fast enough
    to keep them busy at all times, which may require using alternatives to S3, generating
    some extra engineering work. Even if you don't hit any of these problems, there
    comes a point where there simply isn't a bigger instance you can use!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你的算法计算密集度不高或并行性不足，那么额外的计算能力可能不会带来好处。额外的 vCPU 和 GPU 只有在被充分利用时才有用。你的网络和存储层也必须足够快速，以确保它们始终保持繁忙，这可能需要使用
    S3 以外的替代方案，从而增加一些额外的工程工作。即使你没有遇到这些问题，也有一个时刻，你会发现再没有更大的实例可以使用了！
- en: Scaling up with multi-GPU instances
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过多 GPU 实例进行扩展
- en: As tempting as multi-GPU instances are, they create specific challenges. An
    `ml.g4dn.16xlarge` and `ml.p3dn.24xlarge` support 100-Gbit networking and ultra-fast
    SSD NVMe local storage. Still, that level of performance comes at a price, and
    you need to make sure it's really worth it.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管多 GPU 实例非常诱人，但它们也带来了特定的挑战。`ml.g4dn.16xlarge` 和 `ml.p3dn.24xlarge` 支持 100 Gbit
    的网络和超快速的 SSD NVMe 本地存储。但这种性能水平是有代价的，你需要确保它真的是值得的。
- en: You should keep in mind that bigger isn't always better. Inter-GPU communication,
    no matter how fast, introduces some overhead that could kill the performance of
    smaller training jobs. Here too, you should experiment and find the sweetest spot.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该记住，更大并不总是更好。无论 GPU 之间的通信多么快速，它都会引入一些开销，这可能会影响较小训练任务的性能。这里，你也需要进行实验，找到最合适的平衡点。
- en: In my experience, getting great performance with multi-GPU instances takes some
    work. Unless the model is too large to fit on a single GPU or the algorithm doesn't
    support distributed training, I'd recommend trying first to scale out on single-GPU
    instances.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我的经验，想要在多 GPU 实例上获得出色的性能需要一些努力。除非模型太大无法放入单个 GPU，或者算法不支持分布式训练，否则我建议首先尝试在单 GPU
    实例上进行扩展。
- en: Scaling out
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 扩展
- en: Scaling out lets you distribute large datasets to a cluster of training instances.
    Even if your training job doesn't scale linearly, you'll get a noticeable speedup
    compared to single-instance training. You can use plenty of smaller instances
    that only process a subset of your dataset, which helps to keep costs under control.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展使你能够将大数据集分发到训练实例的集群中。即使你的训练任务无法线性扩展，与你的单实例训练相比，你仍然会看到明显的加速效果。你可以使用大量较小的实例，这些实例只处理数据集的一个子集，有助于控制成本。
- en: On the downside, datasets need to be prepared in a format that can be efficiently
    distributed across training clusters. As distributed training is pretty chatty,
    network I/O can also become a bottleneck. Still, the main problem is usually accuracy,
    which is often lower than for single-instance training, as each instance works
    with its own set of model parameters. This can be alleviated by asking training
    instances to synchronize their work periodically, but this is a costly operation
    that impacts training time.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据集需要以能够高效分发到训练集群的格式进行准备。由于分布式训练需要频繁的通信，网络 I/O 也可能成为瓶颈。不过，通常最大的问题是准确性，通常低于单实例训练的准确性，因为每个实例都使用自己的一组模型参数。这可以通过定期同步训练实例的工作来缓解，但这是一项成本高昂的操作，会影响训练时间。
- en: If you think that scaling is harder than it seems, you're right. Let's try to
    put all of these notions into practice with a first simple example.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你觉得扩展比看起来的要困难，你是对的。让我们尝试用第一个简单的例子将这些概念付诸实践。
- en: Scaling a BlazingText training job
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 扩展 BlazingText 训练任务
- en: 'In [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training
    Natural Language Processing Models*, we used **BlazingText** and the Amazon reviews
    dataset to train a sentiment analysis model. At the time, we only trained it on
    100,000 reviews. This time, we''ll train it on the full dataset: 1.8 million reviews
    (151 million words).'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [*第六章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)，*训练自然语言处理模型* 中，我们使用
    **BlazingText** 和 Amazon 评论数据集训练了情感分析模型。当时，我们只训练了 100,000 条评论。这一次，我们将使用完整的数据集进行训练：180
    万条评论（1.51 亿个单词）。
- en: Reusing our SageMaker Processing notebook, we process the full dataset on an
    `ml.c5.9xlarge` instance, store results in S3, and feed them to our training job.
    The size of the training set has grown to a respectable 720 MB.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 重新利用我们的 SageMaker Processing 笔记本，我们在 `ml.c5.9xlarge` 实例上处理完整的数据集，将结果存储在 S3 中，并将其馈送到训练作业中。训练集的大小已增长到
    720 MB。
- en: 'To give BlazingText extra work, we apply the following hyperparameters to increase
    the complexity of the word vectors the job will learn:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给 BlazingText 增加额外工作量，我们应用以下超参数来提高任务将学习的词向量的复杂度：
- en: '[PRE0]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We train on a single `ml.c5.2xlarge` instance. It has 8 vCPU and 16 GB of RAM
    and uses `gp2` class, which is SSD-based).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在单个 `ml.c5.2xlarge` 实例上进行训练。它具有 8 个 vCPU 和 16 GB 的 RAM，并使用 `gp2` 类型（基于 SSD）。
- en: 'The job runs for 2,109 seconds (a little more than 35 minutes), peaking at
    4.84 million words per second. Let''s take a look at the CloudWatch metrics:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 该任务运行了 2,109 秒（略超过 35 分钟），最大速度为每秒 484 万个词。我们来看一下 CloudWatch 指标：
- en: Starting from the **Experiments and trials** panel in **SageMaker Studio**,
    we locate the training job and right-click on **Open in trial details**.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 **SageMaker Studio** 中的 **实验与试验** 面板开始，我们定位到训练作业，并右键点击 **在试验详情中打开**。
- en: Then, we select the **AWS settings** tab. Scrolling down, we see a link named
    **View instance metrics**. Clicking on it takes us directly to the CloudWatch
    metrics for our training job.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们选择 **AWS 设置** 标签。向下滚动，我们看到一个名为 **查看实例指标** 的链接。点击它会直接带我们到训练作业的 CloudWatch
    指标页面。
- en: 'Let''s select `CPUUtilization` and `MemoryUtilization` in **All metrics** and
    visualize them as shown in the next screenshot:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在 **所有指标** 中选择 `CPUUtilization` 和 `MemoryUtilization`，并按下图所示进行可视化：
- en: '![Figure 9.1 – Viewing CloudWatch metrics'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.1 – 查看 CloudWatch 指标'
- en: '](img/B17705_09_1.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_1.jpg)'
- en: Figure 9.1 – Viewing CloudWatch metrics
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – 查看 CloudWatch 指标
- en: On the right-hand Y-axis, memory utilization is stable at 20%, so we definitely
    don't need more RAM.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在右侧的 Y 轴上，内存利用率稳定在 20%，所以我们肯定不需要更多的 RAM。
- en: Still on the right-hand Y-axis, disk utilization is about 3% during the training,
    going up to 12% when the model is saved. We allocated way too much storage to
    this instance. By default, SageMaker instances get 30 GB of Amazon EBS storage,
    so how much money did we waste here? The EBS cost for SageMaker in `eu-west-1`
    is $0.154 per GB-month, so 30 GB for 2,117 seconds costs 0.154*30*(2109/(24*30*3600))
    = $0.00376\. That's a silly low amount, but if you train thousands of jobs per
    month, it will add up. Even if this saves us $10 a year, we should save that!
    This can easily be done by setting the `volume_size` parameter in all estimators.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然在右侧的 Y 轴上，磁盘利用率在训练过程中约为 3%，当模型被保存时，磁盘利用率会上升到 12%。我们为这个实例分配了太多的存储。默认情况下，SageMaker
    实例会获得 30 GB 的 Amazon EBS 存储，那么我们浪费了多少钱呢？在 `eu-west-1` 区域，SageMaker 的 EBS 成本为每
    GB 每月 0.154 美元，因此 30 GB 在 2,117 秒内的费用为 0.154*30*(2109/(24*30*3600)) = 0.00376
    美元。虽然这个金额非常低，但如果每月训练数千个任务，这些费用会累积起来。即使每年节省 10 美元，我们也应该节省！这可以通过在所有估算器中设置 `volume_size`
    参数来轻松实现。
- en: On the left-hand Y-axis, we see that the CPU utilization plateaus around 790%,
    very close to the maximum value of 800% (8 vCPUs at 100% usage). This job is obviously
    compute-bound.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在左侧的 Y 轴上，我们看到 CPU 利用率稳定在 790% 左右，非常接近最大值 800%（8 个 vCPU 在 100% 使用率下）。这个任务显然是计算密集型的。
- en: 'So, what are our options? If BlazingText supported distributed training in
    supervised mode (it doesn''t), we could have considered scaling out with smaller
    `ml.c5.xlarge` instances (4 vCPUs and 8 GB of RAM). That''s more than enough RAM,
    and adding capacity in small chunks is good practice. This is what right-sizing
    is all about: not too much, not too little—it should be just right.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们有哪些选择呢？如果 BlazingText 支持监督模式下的分布式训练（但实际上不支持），我们可以考虑使用更小的 `ml.c5.xlarge`
    实例（4 个 vCPU 和 8 GB 的 RAM）进行扩展。那样的 RAM 已经足够了，按小步增长容量是一种良好的实践。这就是右尺寸的核心：不多不少，正好合适。
- en: Anyway, our only choice here is to scale up. Looking at the list of available
    instances, we could try `ml.c5.4xlarge`. As BlazingText supports single-GPU acceleration,
    `ml.p3.2xlarge` (1 NVIDIA V100 GPU) is also an option.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 无论如何，我们此时唯一的选择是扩展实例。查看可用实例列表，我们可以尝试 `ml.c5.4xlarge`。由于 BlazingText 支持单个 GPU
    加速，`ml.p3.2xlarge`（1 个 NVIDIA V100 GPU）也是一个选项。
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, the cost-effective `ml.g4dn.xlarge` is unfortunately
    not supported by BlazingText.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 写作时，成本效益较高的 `ml.g4dn.xlarge` 不幸未被 BlazingText 支持。
- en: Let's try both and compare training times and costs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试两者并比较训练时间和成本。
- en: '![](img/011.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/011.jpg)'
- en: The `ml.c5.4xlarge` instance provides a nice speedup for a moderate price increase.
    Interestingly, the job is still compute-bound, so I decided to try the even larger
    `ml.c5.9xlarge` instance (36 vCPUs) for good measure, but the speedup was large
    enough to offset the increased cost.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`ml.c5.4xlarge` 实例在适度价格增加的情况下提供了不错的加速效果。有趣的是，工作负载仍然是计算密集型的，因此我决定尝试更大的 `ml.c5.9xlarge`
    实例（36 个 vCPU），以进一步测试，但加速效果足以抵消成本增加。'
- en: The GPU instance is almost 3x faster, as BlazingText has been optimized to utilize
    thousands of cores. It's also about 3x more expensive, which could be acceptable
    if minimizing training time was very important.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 实例速度几乎是原来的 3 倍，因为 BlazingText 已经过优化，能够利用数千个核心。它的费用大约是原来的 3 倍，如果最小化训练时间非常重要，这可能是可以接受的。
- en: This simple example shows you that right-sizing your training infrastructure
    is not black magic. By following simple rules, looking at a few metrics, and using
    common sense, you can find the right instance size for your project.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的示例告诉我们，调整训练基础设施并非神秘难懂。通过遵循简单的规则，查看一些指标并运用常识，你可以为你的项目找到合适的实例大小。
- en: Now, let's introduce the monitoring and profiling capability in Amazon SageMaker
    Debugger, which will give us even more information on the performance of our training
    jobs.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们介绍 Amazon SageMaker Debugger 中的监控和分析功能，它将为我们提供更多关于训练任务性能的信息。
- en: Monitoring and profiling training jobs with Amazon SageMaker Debugger
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker Debugger 监控和分析训练任务
- en: SageMaker Debugger includes a monitoring and profiling capability that lets
    us collect infrastructure and code performance information at much lower time
    resolution than CloudWatch (as often as every 100 milliseconds). It also allows
    us to configure and trigger built-in or custom rules that watch for unwanted conditions
    in our training jobs.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Debugger 包含监控和分析功能，使我们能够以比 CloudWatch 更低的时间分辨率收集基础设施和代码性能信息（通常每 100
    毫秒一次）。它还允许我们配置和触发内置或自定义规则，监控训练任务中的不良条件。
- en: 'Profiling is very easy to use, and in fact, it''s on by default! You may have
    noticed a line such as this one in your training log:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 分析功能非常容易使用，事实上，它默认是开启的！你可能在训练日志中注意到类似下面这样的行：
- en: '[PRE1]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This tells us that SageMaker is automatically running a profiling job, in parallel
    with our training job. The role of the profiling job is to collect data points
    that we can then display in SageMaker Studio, in order to visualize metrics and
    understand potential performance issues.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，SageMaker 正在自动运行一个分析任务，并与我们的训练任务并行进行。分析任务的作用是收集数据点，我们可以在 SageMaker Studio
    中显示这些数据点，以便可视化指标并了解潜在的性能问题。
- en: Viewing monitoring and profiling information in SageMaker Studio
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker Studio 中查看监控和分析信息
- en: 'Let''s go back to the `ml.p3.2xlarge` instance. We right-click on it and select
    **Open Debugger for insights** this time. This opens a new tab, visible in the
    next screenshot:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到 `ml.p3.2xlarge` 实例。我们右键点击它，然后选择 **打开调试器以获取洞察**。这会打开一个新标签，见下图所示：
- en: '![Figure 9.2 – Viewing monitoring and profiling information'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.2 – 查看监控和分析信息'
- en: '](img/B17705_09_2.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_2.jpg)'
- en: Figure 9.2 – Viewing monitoring and profiling information
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 查看监控和分析信息
- en: 'At the top, we can see that monitoring is indeed on by default and that profiling
    isn''t. Expanding the **Resource utilization summary** item in the **Overview**
    tab, we see a summary of infrastructure metrics, as shown in the next screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在顶部，我们可以看到监控默认是开启的，而分析功能没有开启。在 **概览** 标签页中展开 **资源使用情况总结** 项目，我们可以看到基础设施指标的总结，如下一个截图所示：
- en: '![Figure 9.3 – Viewing utilization summary'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.3 – 查看使用情况总结'
- en: '](img/B17705_09_3.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_3.jpg)'
- en: Figure 9.3 – Viewing utilization summary
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3 – 查看使用情况总结
- en: Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: P50, p95, and p99 are percentiles. If you're not familiar with this concept,
    you can find more information at [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: P50、p95 和 p99 是百分位数。如果你不熟悉这个概念，可以访问 [https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles](https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html#Percentiles)
    了解更多信息。
- en: 'Moving on to the `algo-1`. For example, you can see its GPU utilization in
    the next screenshot:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 继续讨论 `algo-1`。例如，您可以在下一个截图中看到其 GPU 使用情况：
- en: '![Figure 9.4 – Viewing GPU utilization over time'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.4 – 查看 GPU 使用情况随时间变化'
- en: '](img/B17705_09_4.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_4.jpg)'
- en: Figure 9.4 – Viewing GPU utilization over time
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4 – 查看 GPU 使用情况随时间变化
- en: 'We also get a very nice view of system utilization over time, with one line
    per vCPU and GPU, as shown in the next screenshot:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到一个非常好的系统使用情况视图，按 vCPU 和 GPU 每个一条线，如下图所示：
- en: '![Figure 9.5 – Viewing system utilization over time'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.5 – 查看系统使用情况随时间变化'
- en: '](img/B17705_09_5.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_5.jpg)'
- en: Figure 9.5 – Viewing system utilization over time
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 – 查看系统使用情况随时间变化
- en: All this information is updated in near-real-time while your training job is
    running. Just launch a training job, open this view, and, after a few minutes,
    the graphs will show up and get updated.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些信息会在训练任务运行时近乎实时更新。只需启动一个训练任务，打开此视图，几分钟后，图表就会显示并更新。
- en: Now, let's see how we can enable detailed profiling information in our training
    jobs.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在训练任务中启用详细的分析信息。
- en: Enabling profiling in SageMaker Debugger
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SageMaker Debugger 中启用分析
- en: Profiling collects framework metrics (**TensorFlow**, **PyTorch**, **Apache**
    **MXNet**, and XGBoost), data loader metrics, and Python metrics. For the latter,
    we can use **CProfile** or **Pyinstrument**.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 分析会收集框架指标（**TensorFlow**、**PyTorch**、**Apache** **MXNet** 和 XGBoost）、数据加载器指标和
    Python 指标。对于后者，我们可以使用 **CProfile** 或 **Pyinstrument**。
- en: Profiling can be configured in the estimator (which is the option we'll use).
    You can also enable it manually in SageMaker Studio on a running job (see the
    slider in *Figure 9.2*).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 分析可以在估算器中配置（这是我们将使用的选项）。你也可以在 SageMaker Studio 中的运行任务上手动启用它（见 *图 9.2* 中的滑块）。
- en: 'Let''s reuse our TensorFlow/Keras example from [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Computer Vision Models*, and collect all profiling information every
    100 milliseconds:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重用我们在 [*第 6 章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108) 中的 TensorFlow/Keras
    示例，*训练计算机视觉模型*，并每 100 毫秒收集一次所有分析信息：
- en: 'First, we create a `FrameworkProfile` object containing default settings for
    the profiling, data loading, and Python configurations. For each one of these,
    we could specify precise time ranges or step ranges for data collection:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个 `FrameworkProfile` 对象，包含分析、数据加载和 Python 配置的默认设置。对于每一项配置，我们都可以指定精确的时间范围或步骤范围来进行数据收集：
- en: '[PRE2]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then, we create a `ProfilerConfig` object that sets framework parameters and
    the time interval for data collection:'
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个 `ProfilerConfig` 对象，设置框架参数和数据收集的时间间隔：
- en: '[PRE3]'
  id: totrans-128
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, we pass this configuration to our estimator, and train as usual:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将这个配置传递给估算器，然后像往常一样进行训练：
- en: '[PRE4]'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As the training job runs, profiling data is automatically collected and saved
    in a default location in S3 (you can define a custom path with the `s3_output_path`
    parameter in `ProfilingConfig`). We could also use the `smdebug` **SDK** ([https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger))
    to load and inspect profiling data.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当训练任务运行时，分析数据会自动收集并保存在 S3 的默认位置（你可以通过 `ProfilingConfig` 中的 `s3_output_path`
    参数定义自定义路径）。我们还可以使用 `smdebug` **SDK**（[https://github.com/awslabs/sagemaker-debugger](https://github.com/awslabs/sagemaker-debugger)）来加载并检查分析数据。
- en: Shortly after the training job completes, we see summary information in the
    **Overview** tab, as shown in the next screenshot:![Figure 9.6 – Viewing profiling
    information
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练任务完成后不久，我们会在 **概览** 标签中看到汇总信息，如下图所示：![图 9.6 – 查看分析信息
- en: '](img/B17705_09_6.jpg)'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_09_6.jpg)'
- en: Figure 9.6 – Viewing profiling information
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.6 – 查看分析信息
- en: 'We can also download a detailed report in HTML format (see the button in *Figure
    9.2*). For example, it tells us which are the most expensive GPU operators. Unsurprisingly,
    we see our `fmnist_model` function and the TensorFlow operator for 2D convolution,
    as visible in the next screenshot:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以下载详细的 HTML 格式报告（见 *图 9.2* 中的按钮）。例如，它会告诉我们哪些是最昂贵的 GPU 操作。毫无意外地，我们看到我们的 `fmnist_model`
    函数和用于二维卷积的 TensorFlow 操作，如下图所示：
- en: '![Figure 9.7 – Viewing the profiling report'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.7 – 查看分析报告'
- en: '](img/B17705_09_7.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_7.jpg)'
- en: Figure 9.7 – Viewing the profiling report
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 查看分析报告
- en: The report also contains information on built-in rules that have been triggered
    during training, warning us about conditions such as low GPU usage, CPU bottlenecks,
    and more. These rules have default settings that can be customized if needed.
    We'll cover rules in more details in the next chapter when we'll discuss how to
    use SageMaker Debugger to debug training jobs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 报告还包含训练过程中触发的内置规则信息，提醒我们如 GPU 使用率低、CPU 瓶颈等情况。这些规则有默认设置，如果需要，可以进行自定义。我们将在下一章详细讨论规则，当时我们将讨论如何使用
    SageMaker Debugger 调试训练任务。
- en: For now, let's look at some common scaling issues for training jobs, and how
    we could address them. In the process, we'll mention several SageMaker features
    that will be covered in the rest of this chapter.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看看训练任务中的一些常见扩展问题，以及我们如何解决它们。在此过程中，我们将提到本章后面将介绍的几个 SageMaker 特性。
- en: Solving training challenges
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决训练中的挑战
- en: 'We will dive into the challenges, and their solutions, as follows:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入探讨挑战及其解决方案，如下所示：
- en: '*I need lots of storage on training instances.*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '*我在训练实例上需要大量存储。*'
- en: As discussed in the previous example, most SageMaker training instances use
    EBS volumes, and you can set their size in the estimator. The maximum size of
    an EBS volume is 16 TB, so you should have more than enough. If your algorithm
    needs lots of temporary storage for intermediate results, this is the way to go.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的例子所述，大多数 SageMaker 训练实例使用 EBS 卷，你可以在估算器中设置它们的大小。EBS 卷的最大大小为 16 TB，所以应该绰绰有余。如果你的算法需要大量临时存储来存放中间结果，这就是解决方案。
- en: '*My dataset is very large, and it takes a long time to copy it to training
    instances.*'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的数据集非常大，复制到训练实例上需要很长时间。*'
- en: Define "long"! If you're looking for a quick fix, you can use instance types
    with high network performance. For example, `ml.g4dn` and `ml.p3dn` instances
    support the **Elastic Fabric Adapter** (https://aws.amazon.com/hpc/efa), and can
    go all the way to 100 Gbit/s.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一下“长时间”！如果你在寻找快速解决方案，你可以使用具有高网络性能的实例类型。例如，`ml.g4dn` 和 `ml.p3dn` 实例支持 **弹性结构适配器**（[https://aws.amazon.com/hpc/efa](https://aws.amazon.com/hpc/efa)），可以达到最高
    100 Gbit/s 的速度。
- en: If that's not enough, and if you're training on a single instance, you should
    use pipe mode, which streams data from S3 instead of copying it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这还不够，而且如果你正在对单个实例进行训练，你应该使用管道模式，它从 S3 流式传输数据，而不是复制数据。
- en: If training is distributed, you can switch the `FullyReplicated` to `ShardedbyS3Key`,
    which will only distribute a fraction of the dataset to each instance. This can
    be combined with pipe mode for extra performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如果训练是分布式的，你可以将 `FullyReplicated` 更改为 `ShardedbyS3Key`，这将仅将数据集的一部分分发到每个实例。这可以与管道模式结合使用，以提高性能。
- en: '*My dataset is very large, and it doesn''t fit in RAM.*'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*我的数据集非常大，无法完全装入内存。*'
- en: If you want to stick to a single instance, a quick way to solve the problem
    is to scale up. The `ml.r5d.24xlarge` and `ml.p3dn.24xlarge` instances have 768
    GB of RAM! If distributed training is an option, then you should configure it
    and apply data parallelism.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想坚持使用单个实例，解决这个问题的快速方法是进行扩展。`ml.r5d.24xlarge` 和 `ml.p3dn.24xlarge` 实例具有 768
    GB 的内存！如果分布式训练是一个选项，那么你应该配置它并应用数据并行处理。
- en: '*CPU utilization is low.*'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '*CPU 使用率很低。*'
- en: Assuming you haven't overprovisioned, the most likely cause is I/O latency (network
    or storage). The CPU is stalled because it's waiting for data to be fetched from
    wherever it's stored.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你没有过度配置，最可能的原因是 I/O 延迟（网络或存储）。CPU 被阻塞，因为它在等待从存储位置获取数据。
- en: The first thing you should review is the data format. As discussed in previous
    chapters, there's no escaping **RecordIO** or **TFRecord** files. If you're using
    other formats (CSV, individual images, and so on), you should start there before
    tweaking the infrastructure.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先你应该检查数据格式。如前几章所述，**RecordIO** 或 **TFRecord** 文件是不可避免的。如果你使用的是其他格式（如 CSV、单独的图片等），你应该在调整基础设施之前从这些地方入手。
- en: 'If data is copied from S3 to an EBS volume, you can try using an instance with
    more EBS bandwidth. Numbers are available at the following location:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是从 S3 复制到 EBS 卷的，你可以尝试使用具有更多 EBS 带宽的实例。有关详细信息，请访问以下位置：
- en: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html)'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ebs-optimized.html)'
- en: You can also switch to an instance type with local NVMe storage (g4dn and p3dn).
    If the problem persists, you should review the code that reads data and passes
    it to the training algorithm. It probably needs more parallelism.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以切换到具有本地 NVMe 存储的实例类型（g4dn 和 p3dn）。如果问题仍然存在，你应该检查读取数据并将其传递给训练算法的代码。它可能需要更多的并行处理。
- en: If data is streamed from S3 with pipe mode, it's unlikely that you've hit the
    maximum transfer speed of 25 GB/s, but it's worth checking the instance metric
    in CloudWatch. If you're sure that nothing else could be the cause, you should
    move to other file storage services, such as **Amazon** **EFS** and **Amazon**
    **FSx for Lustre**.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是通过管道模式从 S3 流式传输的，那么你不太可能已经达到了 25 GB/s 的最大传输速度，但值得检查 CloudWatch 中的实例指标。如果你确定没有其他原因导致这种情况，你应该转向其他文件存储服务，如
    **Amazon** **EFS** 和 **Amazon** **FSx for Lustre**。
- en: '*GPU memory utilization is low.*'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPU 内存利用率低。*'
- en: 'The GPU doesn''t receive enough data from the CPU. You need to increase batch
    size until memory utilization is close to 100%. If you increase it too much, you''ll
    get an angry `out of memory` error message, such as this one:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: GPU 没有从 CPU 获取足够的数据。你需要增加批次大小，直到内存利用率接近 100%。如果增加过多，你将会收到一个令人烦恼的 `out of memory`
    错误信息，如下所示：
- en: '[PRE5]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: When working with a multi-GPU instance in a data-parallel configuration, you
    should multiply the batch size passed to the estimator by the number of GPUs present
    in an instance.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用数据并行配置的多 GPU 实例时，你应该将传递给估算器的批次大小乘以实例中 GPU 的数量。
- en: When increasing batch size, you have to factor in the number of training samples
    available. For example, the **Pascal** VOC dataset that we used for Semantic Segmentation
    in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training Computer
    Vision Models*, only has 1,464 samples, so it would probably not make sense to
    increase batch size above 64 or 128.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 增加批次大小时，你需要考虑可用的训练样本数量。例如，我们在 [*第 5 章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)《训练计算机视觉模型》中使用的
    **Pascal** VOC 数据集只有 1,464 个样本，因此可能没有必要将批次大小增加到 64 或 128 以上。
- en: Finally, batch size has an important effect on job convergence. Very large batches
    may slow it down, so you may want to increase the learning rate accordingly.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，批次大小对作业收敛性有重要影响。非常大的批次可能会使收敛变慢，因此你可能需要相应地增加学习率。
- en: Sometimes, you'll simply have to accept that GPU memory utilization is low!
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，你只能接受 GPU 内存利用率低的事实！
- en: '*GPU utilization is low.*'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPU 利用率低。*'
- en: Maybe your model is simply not large enough to keep the GPU really busy. You
    should try scaling down on a smaller GPU.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你的模型根本没有足够大，无法充分利用 GPU。你应该尝试在更小的 GPU 上进行缩放。
- en: If you're working with a large model, the GPU is probably stalled because the
    CPU can't feed it fast enough. If you're in control of the data loading code,
    you should try to add more parallelism, such as additional threads for data loading
    and preprocessing. If you're not, you should try a larger instance type with more
    vCPUs. Hopefully, they can be put to good use by the data-loading code.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在处理一个大型模型，GPU 可能会因为 CPU 无法提供足够的数据而停滞。如果你能够控制数据加载代码，应该尝试增加更多的并行性，例如为数据加载和预处理增加更多的线程。如果无法控制代码，你应该尝试使用更多
    vCPU 的更大实例类型。希望数据加载代码能够充分利用它们。
- en: If there's enough parallelism in the data loading code, then slow I/O is likely
    to be responsible. You should look for a faster alternative (NVMe, EFS, or FSx
    for Lustre).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据加载代码中有足够的并行性，那么慢 I/O 很可能是罪魁祸首。你应该寻找更快的替代方案（如 NVMe、EFS 或 FSx for Lustre）。
- en: '*GPU utilization is high*.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*GPU 利用率高。*'
- en: That's a good place to be! You're efficiently using the infrastructure that
    you're paying for. As discussed in the previous example, you can try scaling up
    (more vCPUs or more GPUs), or scaling out (more instances). Combining both can
    work for highly parallel workloads such as deep learning.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个好情况！你在高效地使用你为其支付的基础设施。正如前面示例中讨论的，你可以尝试扩展（更多 vCPU 或更多 GPU），或者横向扩展（更多实例）。将两者结合使用，适用于诸如深度学习之类的高度并行工作负载。
- en: Now we know a little more about scaling jobs, let's learn about more SageMaker
    features, starting with pipe mode.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对作业扩展有了更多的了解，接下来让我们学习更多 SageMaker 的功能，从管道模式开始。
- en: Streaming datasets with pipe mode
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用管道模式流式传输数据集
- en: 'The default setting of estimators is to copy the dataset to training instances,
    which is known as **file mode**. Instead, **pipe mode** streams it directly from
    S3\. The name of the feature comes from its use of **Unix** **named pipes** (also
    known as **FIFOs**): at the beginning of each epoch, one pipe is created per input
    channel.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 估算器的默认设置是将数据集复制到训练实例中，这被称为 **文件模式**。而 **管道模式** 则直接从 S3 流式传输数据。该功能的名称来自于它使用 **Unix**
    **命名管道**（也称为 **FIFOs**）：在每个 epoch 开始时，每个输入通道会创建一个管道。
- en: Pipe mode removes the need to copy any data to training instances. Obviously,
    training jobs start quicker. They generally run faster too, as pipe mode is highly
    optimized. Another benefit is that you won't have to provision any storage for
    the dataset on training instances.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 管道模式消除了将数据复制到训练实例的需求。显然，训练任务开始得更快。它们通常也运行得更快，因为管道模式经过高度优化。另一个好处是，您不必为训练实例上的数据集预置存储。
- en: 'Cutting on training time and storage means that you''ll save money. The larger
    the dataset, the more you''ll save. You can find benchmarks at the following link:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 减少训练时间和存储意味着您可以节省成本。数据集越大，节省的成本就越多。您可以在以下链接找到基准测试：
- en: '[https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/accelerate-model-training-using-faster-pipe-mode-on-amazon-sagemaker/)'
- en: In practice, you can start experimenting with pipe mode for datasets in the
    hundreds of megabytes and beyond. In fact, this feature enables you to work with
    infinitely large datasets. As storage and RAM requirements are no longer coupled
    to the size of the dataset, there's no practical limit on the amount of data that
    your algorithm can crunch. Training on petabyte-scale datasets becomes possible.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，您可以开始使用管道模式处理数百兆字节及以上的数据集。事实上，这项功能使您能够处理无限大的数据集。由于存储和内存需求不再与数据集大小挂钩，因此您的算法可以处理的数据量没有实际限制。可以在
    PB 级别的数据集上进行训练。
- en: Using pipe mode with built-in algorithms
  id: totrans-178
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用管道模式与内置算法
- en: 'The prime candidates for pipe mode are built-in algorithms, as most of them
    support it natively:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 管道模式的主要候选者是内置算法，因为大多数内置算法本身就支持此模式：
- en: '**Linear Learner**, **k-Means**, **k-Nearest Neighbors**, **Principal Component
    Analysis**, **Random Cut Forest**, and **Neural Topic Modeling**: RecordIO-wrapped
    protobuf or CSV data'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性学习器**、**k-均值**、**k-最近邻**、**主成分分析**、**随机切割森林**和**神经主题建模**：RecordIO 封装的 protobuf
    或 CSV 数据'
- en: '**Factorization Machines**, **Latent Dirichlet Allocation**: RecordIO-wrapped
    protobuf data'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**因式分解机**、**潜在狄利克雷分配**：RecordIO 封装的 protobuf 数据'
- en: '**BlazingText** (supervised mode): Augmented manifest'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BlazingText**（监督模式）：增强的清单'
- en: '**Image Classification** or **Object Detection**: RecordIO-wrapped protobuf
    data or augmented manifest'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像分类** 或 **目标检测**：使用 RecordIO 封装的 protobuf 数据或增强的清单'
- en: '**Semantic segmentation**: Augmented manifest.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**语义分割**：增强的清单。'
- en: You should already be familiar with `im2rec` tool has an option to generate
    multiple list files (`--chunks`). If you have existing list files, you can of
    course split them yourself.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该已经熟悉 `im2rec` 工具，该工具具有生成多个列表文件（`--chunks`）的选项。如果您已有现成的列表文件，当然也可以自己分割它们。
- en: 'We looked at the **augmented manifest** format when we discussed datasets annotated
    by **SageMaker** **Ground Truth** in [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091),
    *Training Computer Vision Models*. For computer vision algorithms, this **JSON
    Lines** file contains the location of images in S3 and their labeling information.
    You can learn more at the following link:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论由 **SageMaker** **Ground Truth** 标注的数据集时，我们曾经查看过 **增强清单** 格式，详见 [*第 5 章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)，《训练计算机视觉模型》。对于计算机视觉算法，**JSON
    Lines** 文件包含 S3 中图像的位置及其标注信息。您可以在以下链接了解更多内容：
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html](https://docs.aws.amazon.com/sagemaker/latest/dg/augmented-manifest.html)'
- en: Using pipe mode with other algorithms and frameworks
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用管道模式与其他算法和框架
- en: 'TensorFlow supports pipe mode thanks to the `PipeModeDataset` class implemented
    by AWS. Here are some useful resources:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 支持管道模式，感谢 AWS 实现的 `PipeModeDataset` 类。以下是一些有用的资源：
- en: '[https://github.com/aws/sagemaker-tensorflow-extensions](https://github.com/aws/sagemaker-tensorflow-extensions)'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/aws/sagemaker-tensorflow-extensions](https://github.com/aws/sagemaker-tensorflow-extensions)'
- en: '[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_pipe_mode](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_pipe_mode)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_pipe_mode](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk/tensorflow_script_mode_pipe_mode)'
- en: '[https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233](mailto:https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233](mailto:https://medium.com/@julsimon/making-amazon-sagemaker-and-tensorflow-work-for-you-893365184233)'
- en: 'For other frameworks and for your own custom code, it''s still possible to
    implement pipe mode inside the training container. A Python example is available
    at the following link:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他框架以及你自己的自定义代码，仍然可以在训练容器中实现管道模式。一个 Python 示例可以在以下链接找到：
- en: '[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/pipe_bring_your_own](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/pipe_bring_your_own)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/pipe_bring_your_own](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/advanced_functionality/pipe_bring_your_own)'
- en: Simplifying data loading with MLIO
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 MLIO 简化数据加载
- en: '**MLIO** (https://github.com/awslabs/ml-io) is an AWS open source project that
    lets you load data stored in memory, on local storage, or in S3 with pipe mode.
    The data can then be converted into different popular formats.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '**MLIO** (https://github.com/awslabs/ml-io) 是一个 AWS 开源项目，允许你使用管道模式加载存储在内存、本地存储或
    S3 中的数据。然后，可以将数据转换为不同的流行格式。'
- en: 'Here are the high-level features:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一些高级特性：
- en: '**Input formats**: **CSV**, **Parquet**, RecordIO-protobuf, **JPEG**, **PNG**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入格式**：**CSV**、**Parquet**、RecordIO-protobuf、**JPEG**、**PNG**'
- en: '**Conversion formats**: NumPy arrays, SciPy matrices, **Pandas** **DataFrames**,
    TensorFlow tensors, PyTorch tensors, Apache MXNet arrays, and **Apache** **Arrow**'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**转换格式**：NumPy 数组、SciPy 矩阵、**Pandas** **DataFrame**、TensorFlow 张量、PyTorch 张量、Apache
    MXNet 数组以及 **Apache** **Arrow**'
- en: API available in Python and **C++**
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API 可用于 Python 和 **C++**
- en: Now, let's run some examples with pipe mode.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用管道模式运行一些示例。
- en: Training factorization machines with pipe mode
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用管道模式训练分解机器
- en: 'We''re going to revisit the example we used in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*, where we trained a recommendation model on
    the **MovieLens** dataset. At the time, we used a small version of the dataset,
    limited to 100,000 reviews. This time, we''ll go for the largest version:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将重新访问在 [*第 4 章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069) 中使用的示例，*训练机器学习模型*，当时我们在
    **MovieLens** 数据集上训练了一个推荐模型。当时，我们使用了一个小版本的数据集，限制为 100,000 条评论。这一次，我们将使用最大版本：
- en: 'We download and extract the dataset:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载并解压数据集：
- en: '[PRE6]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This dataset includes 25,000,095 reviews, from 162,541 users, on 62,423 movies.
    Unlike the 100k version, movies are not numbered sequentially. The last movie
    ID is 209,171, which needlessly increases the number of features. The alternative
    would be to renumber movies, but let''s not do that here:'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该数据集包括来自 162,541 个用户的 25,000,095 条评论，涉及 62,423 部电影。与 100k 版本不同，电影编号不是顺序的。最后一部电影的
    ID 为 209,171，这无谓地增加了特征的数量。另一种选择是重新编号电影，但我们这里不这样做：
- en: '[PRE7]'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Just like in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)*,
    Training Machine Learning Models* we load the dataset into a sparse matrix (`lil_matrix`
    from SciPy), split it for training and testing, and convert both datasets into
    RecordIO-wrapped protobuf. Given the size of the dataset, this could take 45 minutes
    on a small Studio instance. Then, we upload the datasets to S3.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 就像在 [*第 4 章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)*，训练机器学习模型* 中一样，我们将数据集加载到一个稀疏矩阵（来自
    SciPy 的 `lil_matrix`），并将其拆分为训练集和测试集，然后将这两个数据集转换为 RecordIO 封装的 protobuf。考虑到数据集的大小，这个过程在一个小型
    Studio 实例上可能需要 45 分钟。然后，我们将数据集上传到 S3。
- en: 'Next, we configure the two input channels, and we set their input mode to pipe
    mode instead of file mode:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们配置两个输入通道，并将它们的输入模式设置为管道模式，而不是文件模式：
- en: '[PRE8]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We then configure the estimator, and train as usual on an `ml.c5.xlarge` instance
    (4 vCPUs, 8 GB RAM, $0.23 per hour in `eu-west-1`).
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们配置估算器，并像往常一样在 `ml.c5.xlarge` 实例（4 个虚拟 CPU、8 GB 内存，$0.23 每小时，位于 `eu-west-1`
    区域）上进行训练。
- en: 'Looking at the training log, we see the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 查看训练日志，我们看到如下信息：
- en: '[PRE9]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As expected, no time was spent copying the dataset. The same step in file mode
    takes 66 seconds. Even with a modest 1.5 GB dataset, pipe mode already makes sense.
    As datasets get bigger, this advantage will only increase!
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，复制数据集并未花费任何时间。文件模式下的相同步骤需要 66 秒。即使数据集只有 1.5 GB，管道模式依然非常有效。随着数据集的增大，这一优势将只会增加！
- en: Now, let's move on to distributed training.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续进行分布式训练。
- en: Distributing training jobs
  id: totrans-216
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分布式训练任务
- en: 'Distributed training lets you scale training jobs by running them on a cluster
    of CPU or GPU instances. It can be used to solve two different problems: very
    large datasets, and very large models.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练让你通过在CPU或GPU实例的集群上运行训练作业来扩展训练规模。它可以用来解决两个不同的问题：非常大的数据集和非常大的模型。
- en: Understanding data parallelism and model parallelism
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据并行和模型并行
- en: Some datasets are too large to be trained in a reasonable amount of time on
    a single CPU or GPU. Using a technique called *data parallelism*, we can distribute
    data across the training cluster. The full model is still loaded on each CPU/GPU,
    which only receive an equal share of the dataset, not the full dataset. In theory,
    this should speed up training linearly according to the number of CPU/GPUs involved,
    and as you can guess, the reality is often different.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一些数据集太大，单个CPU或GPU在合理时间内无法完成训练。通过使用一种叫做*数据并行*的技术，我们可以将数据分配到训练集群中。完整的模型仍然加载到每个CPU/GPU上，但每个CPU/GPU只接收数据集的一部分，而不是整个数据集。理论上，这应该根据参与的CPU/GPU数量线性加速训练，但如你所料，现实往往不同。
- en: Believe it or not, some state-of-the-art-deep learning models are too large
    to fit on a single GPU. Using a technique called *model parallelism*, we can split
    it, and distribute the layers across a cluster of GPUs. Hence, training batches
    will flow across several GPUs to be processed by all layers.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，一些最先进的深度学习模型太大，无法装载到单个GPU上。通过使用一种叫做*模型并行*的技术，我们可以将其拆分，并将各层分布到GPU集群中。因此，训练批次将跨多个GPU流动，由所有层共同处理。
- en: Now, let's see where we can use distributed training in SageMaker.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在哪里可以在SageMaker中使用分布式训练。
- en: Distributing training for built-in algorithms
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为内置算法分发训练
- en: Data parallelism is available for almost all built-in algorithms (semantic segmentation
    and LDA are notable exceptions). As they are implemented with Apache MXNet, they
    automatically use its native distributed training mechanism.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行几乎适用于所有内置算法（语义分割和LDA是显著的例外）。由于它们是用Apache MXNet实现的，因此会自动使用其原生的分布式训练机制。
- en: Distributing training for built-in frameworks
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为内置框架分发训练
- en: TensorFlow, PyTorch, Apache MXNet, and **Hugging Face** have native data parallelism
    mechanisms, and they're supported on SageMaker. **Horovod** ([https://github.com/horovod/horovod](https://github.com/horovod/horovod))
    is available too.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow、PyTorch、Apache MXNet和**Hugging Face**都有原生的数据并行机制，并且都支持SageMaker。**Horovod**
    ([https://github.com/horovod/horovod](https://github.com/horovod/horovod)) 也可用。
- en: For TensorFlow, PyTorch, and Hugging Face, you can also use the newer **SageMaker
    Distributed Data Parallel Library** and **SageMaker Model Parallel Library**.
    Both will be covered later in this chapter.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 对于TensorFlow、PyTorch和Hugging Face，你还可以使用更新的**SageMaker分布式数据并行库**和**SageMaker模型并行库**。这两者将在本章后面讨论。
- en: 'Distributed training often requires framework-specific changes to your training
    code. You can find more information in the framework documentation (for example
    [https://www.tensorflow.org/guide/distributed_training](https://www.tensorflow.org/guide/distributed_training)),
    and in sample notebooks hosted at [https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples):'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式训练通常需要在训练代码中进行框架特定的修改。你可以在框架文档中找到更多信息（例如 [https://www.tensorflow.org/guide/distributed_training](https://www.tensorflow.org/guide/distributed_training)），以及在[https://github.com/awslabs/amazon-sagemaker-examples](https://github.com/awslabs/amazon-sagemaker-examples)上托管的示例笔记本：
- en: '`sagemaker-python-sdk/tensorflow_script_mode_horovod`'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sagemaker-python-sdk/tensorflow_script_mode_horovod`'
- en: b) `advanced_functionality/distributed_tensorflow_mask_rcnn`
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: b) `advanced_functionality/distributed_tensorflow_mask_rcnn`
- en: '`sagemaker-python-sdk/keras_script_mode_pipe_mode_horovod`'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sagemaker-python-sdk/keras_script_mode_pipe_mode_horovod`'
- en: '`sagemaker-python-sdk/pytorch_horovod_mnist`'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sagemaker-python-sdk/pytorch_horovod_mnist`'
- en: Each framework has its peculiarities, yet everything we discussed in the previous
    sections stands true. If you want to make the most of your infrastructure, you
    need to pay attention to batch size, synchronization, and so on. Experiment, monitor,
    analyze, and iterate!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 每个框架都有其独特之处，但我们在前面章节中讨论的内容依然适用。如果你想最大限度地利用你的基础设施，需要关注批处理大小、同步等方面。进行实验、监控、分析并迭代！
- en: Distributing training for custom containers
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 为自定义容器分发训练
- en: If you're training with your own custom container, you have to implement your
    own distributed training mechanism. Let's face it, this is going to be a lot of
    work. SageMaker only helps to provide the name of cluster instances and the name
    of the container network interface. They are available inside the container in
    the `/opt/ml/input/config/resourceconfig.json` file.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是自己的定制容器进行训练，你必须实现自己的分布式训练机制。说实话，这会是非常繁琐的工作。SageMaker仅帮助提供集群实例的名称和容器网络接口的名称。这些信息可以在容器内的`/opt/ml/input/config/resourceconfig.json`文件中找到。
- en: 'You can find more information at the following link:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接找到更多信息：
- en: '[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html)'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html](https://docs.aws.amazon.com/sagemaker/latest/dg/your-algorithms-training-algo-running-container.html)'
- en: It's time for a distributed training example!
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候进行分布式训练示例了！
- en: Scaling an image classification model on ImageNet
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在ImageNet上扩展图像分类模型
- en: In [*Chapter 5*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091), *Training
    Computer Vision Models*, we trained the image classification algorithm on a small
    dataset with dog and cat images (25,000 training images). This time, let's go
    for something a little bigger.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第5章*](B17705_05_Final_JM_ePub.xhtml#_idTextAnchor091)，《训练计算机视觉模型》中，我们使用包含狗和猫图像的小型数据集（25,000张训练图像）来训练图像分类算法。这一次，我们来挑战一个稍大的数据集。
- en: We're going to train a ResNet-50 network from scratch on the **ImageNet** dataset
    – the reference dataset for many computer vision applications ([http://www.image-net.org](http://www.image-net.org)).
    The 2012 version contains 1,281,167 training images (140 GB) and 50,000 validation
    images (6.4 GB) from 1,000 classes.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从头开始在**ImageNet**数据集上训练一个ResNet-50网络——这是许多计算机视觉应用的参考数据集（[http://www.image-net.org](http://www.image-net.org)）。2012版包含1,281,167张训练图像（140
    GB）和50,000张验证图像（6.4 GB），涵盖1,000个类别。
- en: If you want to experiment at a smaller scale, you can work with 5-10% of the
    dataset. Final accuracy won't be as good, but it doesn't matter for our purposes.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在较小的规模上进行实验，可以使用数据集的5-10%。最终的准确度可能不会那么好，但对我们的目的来说无关紧要。
- en: Preparing the ImageNet dataset
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备ImageNet数据集
- en: This requires a lot of storage – the dataset is 150 GB, so please make sure
    you have at least 500 GB available to store it in ZIP and processed formats. You're
    also going to need a lot of bandwidth and a lot of patience to download it. I
    used an EC2 instance running `us-east-1` region, and my download took *five days*.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要大量的存储空间——数据集大小为150 GB，因此请确保你有至少500 GB的可用空间，以存储ZIP格式和处理后的格式。你还需要大量的带宽和耐心来下载它。我使用的是运行在`us-east-1`区域的EC2实例，我的下载花费了*五天*。
- en: Visit the ImageNet website, register to download the dataset, and accept the
    conditions. You'll get a username and an access key allowing you to download the
    dataset.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问ImageNet网站，注册以下载数据集并接受条款。你将获得一个用户名和访问密钥，允许你下载数据集。
- en: 'One of the TensorFlow repositories includes a great script that will download
    the dataset and extract it. Using `nohup` is essential so that the process continues
    running even if your session is terminated:'
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 其中一个TensorFlow的库包含一个很好的脚本，可以下载并解压数据集。使用`nohup`非常关键，这样即使会话终止，过程仍然会继续运行：
- en: '[PRE10]'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Once this is over (again, downloading will take days), the `imagenet/train`
    directory contains the training dataset (one folder per class). The `imagenet/validation`
    directory contains 50,000 images in the same folder. We can use a simple script
    to organize it with one folder per class:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦完成（再次强调，下载需要几天时间），`imagenet/train`目录包含训练数据集（每个类别一个文件夹）。`imagenet/validation`目录包含50,000张图片，存储在同一个文件夹中。我们可以使用一个简单的脚本按类别组织这些文件夹：
- en: '[PRE11]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We''re going to build RecordIO files with the `im2rec` tool present in the
    Apache MXNet repository. Let''s install dependencies, and fetch `im2rec`:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用Apache MXNet库中的`im2rec`工具来构建RecordIO文件。让我们先安装依赖项并获取`im2rec`：
- en: '[PRE12]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In the `imagenet` directory, we run `im2rec` twice – once to build the list
    files, and once to build the RecordIO files. We create RecordIO files that are
    approximately 1 GB each (we''ll see why that matters in a second). We also resize
    the smaller dimension of images to `224` so that the algorithm won''t have to
    do it:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`imagenet`目录中，我们运行`im2rec`两次——第一次构建列表文件，第二次构建RecordIO文件。我们创建了大约1 GB大小的RecordIO文件（稍后我们会解释为什么这很重要）。我们还将图像的较小尺寸调整为`224`，这样算法就不需要再做此操作：
- en: '[PRE13]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we sync the dataset to S3:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将数据集同步到S3：
- en: '[PRE14]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The dataset is now ready for training.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集现在已经准备好进行训练。
- en: Defining our training job
  id: totrans-256
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义我们的训练作业
- en: 'Now that the dataset is ready, we need to think about the configuration of
    our training job. Specifically, we need to come up with the following:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经准备好，我们需要思考一下训练作业的配置。具体来说，我们需要考虑以下内容：
- en: An input configuration, defining the location and the properties of the dataset
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入配置，定义数据集的位置和属性
- en: Infrastructure requirements to run the training job
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行训练作业的基础设施要求
- en: Hyperparameters to configure the algorithm
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置算法的超参数
- en: Let's look at each one of these items in detail.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些项目。
- en: Defining the input configuration
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义输入配置
- en: Given the size of the dataset, pipe mode sounds like a great idea. Out of curiosity,
    I tried training in file mode. Even with a 100 Gbit/s network interface, it took
    almost 25 minutes to copy the dataset from S3 to local storage. Pipe mode it is!
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到数据集的大小，管道模式似乎是一个好主意。出于好奇，我尝试过使用文件模式进行训练。即使拥有100 Gbit/s的网络接口，将数据集从S3复制到本地存储也花费了近25分钟。所以，还是使用管道模式吧！
- en: 'You may wonder why we took care of splitting the dataset into multiple files.
    Here''s why:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想，为什么我们要把数据集拆分成多个文件。原因如下：
- en: In general, multiple files create opportunities for more parallelism, making
    it easier to write fast data loading and processing code.
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一般来说，多个文件创造了更多的并行机会，使得编写快速数据加载和处理代码变得更加容易。
- en: We can shuffle the files at the beginning of each epoch, removing any potential
    bias caused by the order of samples.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在每个训练周期开始时对文件进行洗牌，消除由于样本顺序导致的任何潜在偏差。
- en: It makes it very easy to work with a fraction of the dataset.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它使得处理数据集的一部分变得非常容易。
- en: Now that we've defined the input configuration, what about infrastructure requirements?
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经定义了输入配置，那基础设施要求呢？
- en: Defining infrastructure requirements
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定义基础设施要求
- en: ImageNet is a large and complex dataset that requires a lot of training to reach
    good accuracy.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ImageNet是一个庞大而复杂的数据集，需要大量训练才能达到良好的准确度。
- en: A quick test shows that a single `ml.p3.2xlarge` instance with the batch size
    set to 128 will crunch through the dataset at about 335 images per second. As
    we have about 1,281,167 images, we can expect one epoch to last about 3,824 seconds
    (about 1 hour and 4 minutes).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 经过快速测试，单个`ml.p3.2xlarge`实例在批量大小设置为128时，可以以每秒约335张图像的速度处理数据集。由于我们有大约1,281,167张图像，可以预计一个周期大约需要3,824秒（约1小时4分钟）。
- en: Assuming that we need to train for 150 epochs to get decent accuracy, we're
    looking at a job that should last (3,824/3,600)*150 = 158 hours (about 6.5 days).
    This is probably not acceptable from a business perspective. For the record, at
    $3.825 per instance per hour in `us-east-1`, that job would cost about $573.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们需要训练150个周期以获得合理的准确率，我们预计作业将持续（3,824/3,600）*150 = 158小时（约6.5天）。从商业角度来看，这可能不可接受。为了记录，在`us-east-1`区域，每小时$3.825的实例费用下，这个作业大约需要花费$573。
- en: Let's try to speed up our job with `ml.p3dn.24xlarge` instances. Each one hosts
    eight NVIDIA V100s with 32 GB of GPU memory (twice the amount available on other
    `p3` instances). They also have 96 **Intel** **Skylake** cores, 768 GB of RAM,
    and 1.8 TB of local NVMe storage. Although we're not going to use it here, the
    latter is a fantastic storage option for long-running, large-scale jobs. Last
    but not least, this instance type has 100 Gbit/s networking, a great feature for
    streaming data from S3 and for inter-instance communication.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们来尝试使用`ml.p3dn.24xlarge`实例来加速我们的作业。每个实例配备八个NVIDIA V100，每个有32 GB的GPU内存（是其他`p3`实例的两倍）。它们还配备96个**Intel**
    **Skylake**核心，768 GB的RAM，以及1.8 TB的本地NVMe存储。虽然我们在这里不会使用它，但后者对于长期运行的大规模作业来说是一个极好的存储选项。最后但同样重要的是，这种实例类型具有100
    Gbit/s的网络连接，适合从S3进行数据流传输和实例间通信。
- en: Note
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At $35.894 per hour per instance in `us-east-1`, you may not want to try this
    at home or even at work without getting permission. Your service quotas probably
    don't let you run that much infrastructure anyway, and you would have to get in
    touch with AWS Support first.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 每小时$35.894的费用，你可能不想在家或工作时尝试，除非获得许可。你的服务配额可能根本不允许你运行如此多的基础设施，而且你必须首先联系AWS支持。
- en: In the next chapter, we're going to talk about *managed spot training* – a great
    way to slash training costs. We'll revisit the ImageNet example once we've covered
    this topic, so you definitely should refrain from training right now!
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论*托管抢占训练*—一种大幅降低训练成本的好方法。我们将在介绍完这一主题后再次回到 ImageNet 示例，因此你现在一定不要进行训练！
- en: Training on ImageNet
  id: totrans-277
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 ImageNet 上训练
- en: 'Let''s configure the training job:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们配置训练任务：
- en: 'We configure pipe mode on both input channels. The files of the training channel
    are shuffled for extra randomness:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在两个输入通道上配置了管道模式。训练通道的文件会被打乱，以增加额外的随机性：
- en: '[PRE15]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'To begin with, we configure the `Estimator` module with a single `ml.p3dn.24xlarge`
    instance:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们配置 `Estimator` 模块，使用一个单独的 `ml.p3dn.24xlarge` 实例：
- en: '[PRE16]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We set hyperparameters, starting with a reasonable batch size of 1,024, and
    we launch training:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置了超参数，首先使用合理的批量大小 1,024，然后启动训练：
- en: '[PRE17]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Updating batch size
  id: totrans-285
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新批量大小
- en: Time per epochs is 727 seconds. For 150 epochs, this translates into 30.3 hours
    of training (1.25 days), and a cost of $1,087\. The good news is that we're going
    5x faster. The bad news is that cost has gone up 2x. Let's start scaling this.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 epoch 的时间是 727 秒。对于 150 个 epoch，这相当于 30.3 小时的训练（1.25 天），成本为 $1,087。好消息是我们加速了
    5 倍。坏消息是成本增加了 2 倍。让我们开始扩展。
- en: 'Looking at total GPU utilization in CloudWatch, we see that it doesn''t exceed
    300%. That is, 37.5% on each GPU. This probably means that our batch size is too
    low to keep the GPUs fully busy. Let''s bump it to (1,024/0.375)=2730, rounded
    up to 2,736 to be divisible by 8:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 查看 CloudWatch 中的总 GPU 利用率，我们看到它没有超过 300%。也就是说，每个 GPU 的利用率为 37.5%。这可能意味着我们的批量大小太小，不能让
    GPU 完全忙碌。让我们将批量大小提升到 (1,024/0.375)=2730，四舍五入到 2,736，以便能被 8 整除：
- en: Note
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Depending on algorithm versions, `out of memory` errors.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 根据算法版本，可能会出现 `out of memory` 错误。
- en: '[PRE18]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Training again, an epoch now lasts 758 seconds. It looks like maxing out GPU
    memory usage didn't make a big difference this time. Maybe it's offset by the
    cost of synchronizing gradients? Anyway, keeping GPU cores as busy as possible
    is good practice.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 再次训练，现在每个 epoch 持续 758 秒。看起来这次将 GPU 内存使用率最大化并没有带来太大的差异。也许它被同步梯度的成本抵消了？无论如何，尽可能保持
    GPU 核心的高负载是一个好习惯。
- en: Adding more instances
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增加更多实例
- en: 'Now, let''s add a second instance to scale out the training job:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们添加第二个实例以扩展训练任务：
- en: '[PRE19]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Time for epoch is now 378 seconds! For 150 epochs, this translates to 15.75
    hours of training, and a cost of $1,221\. Compared to our initial job, this is
    2x faster and 3x cheaper!
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个 epoch 的时间是 378 秒！对于 150 个 epoch，这相当于 15.75 小时的训练，成本为 $1,221。与我们最初的任务相比，这速度提高了
    2 倍，成本降低了 3 倍！
- en: 'How about four instances? Let''s see if we can we keep scaling:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 那四个实例怎么样？看看我们能否继续扩展：
- en: '[PRE20]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Time for epoch is now 198 seconds! For 150 epochs, this translates to 8.25 hours
    of training, and a cost of $1,279\. We sped up 2x again, with a marginal cost
    increase.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个 epoch 的时间是 198 秒！对于 150 个 epoch，这相当于 8.25 小时的训练，成本为 $1,279。我们再次加速了 2 倍，且成本略有增加。
- en: 'Now, shall we train eight instances? Of course! Who wouldn''t want to train
    on 64 GPUs, 327K CUDA cores, and 2 TB (!) of GPU RAM:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们是否要训练八个实例？当然！谁不想在 64 个 GPU、327K CUDA 核心和 2 TB（！）GPU 内存上进行训练呢？
- en: '[PRE21]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Time for epoch is now 99 seconds. For 150 epochs, this translates into 4.12
    hours of training, and a cost of $1,277\. We sped up 2x *again*, at no cost increase.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 现在每个 epoch 的时间是 99 秒。对于 150 个 epoch，这相当于 4.12 小时的训练，成本为 $1,277。我们再次加速了 2 倍，而且没有增加任何额外成本。
- en: Summing things up
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结一下
- en: For 2x the initial cost, we've accelerated our training job 38x, thanks to pipe
    mode, distributed training, and state-of-the-art GPU instances.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 以最初成本的 2 倍，我们通过管道模式、分布式训练和最先进的 GPU 实例将训练任务加速了 38 倍。
- en: '![Fig 9.8 Outcome of the training jobs'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.8 训练任务的结果'
- en: '](img/02.jpg)'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/02.jpg)'
- en: Fig 9.8 Outcome of the training jobs
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 训练任务的结果
- en: Not bad at all! Saving days on your training jobs helps you iterate faster,
    get to a high-quality model quicker, and get to production sooner. I'm pretty
    sure this would easily offset the extra cost. Still, in the next chapter, we'll
    see how we can slash training costs massively with managed spot training.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 不错！节省训练时间有助于加快迭代速度，更快地得到高质量模型，并更早地进入生产阶段。我很确定这很容易抵消额外的成本。不过，在下一章中，我们将看到如何通过托管的抢占式训练大幅削减训练成本。
- en: Now that we're familiar with distributed training, let's take a look at two
    new SageMaker libraries for data parallelism and model parallelism.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经熟悉了分布式训练，接下来让我们看看两个新的 SageMaker 库，用于数据并行和模型并行。
- en: Training with the SageMaker data and model parallel libraries
  id: totrans-309
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker 数据和模型并行库进行训练
- en: These two libraries were introduced in late 2020, and significantly improve
    the performance of large-scale training jobs.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个库是在 2020 年底推出的，显著提升了大规模训练任务的性能。
- en: 'The **SageMaker** **Distributed Data Parallel** (**DDP**) library implements
    a very efficient distribution of computation on GPU clusters. It optimizes network
    communication by eliminating inter-GPU communication, maximizing the amount of
    time and resources they spend on training. You can learn more at the following
    link:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker** **分布式数据并行（DDP）**库实现了GPU集群上计算的高效分布。它通过消除GPU间通信来优化网络通信，最大化它们在训练中所花费的时间和资源。你可以通过以下链接了解更多内容：'
- en: '[https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/](https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/](https://aws.amazon.com/blogs/aws/managed-data-parallelism-in-amazon-sagemaker-simplifies-training-on-large-datasets/)'
- en: DDP is available for TensorFlow, PyTorch, and Hugging Face. The first two require
    minor modifications to the training code, but the last one doesn't. As DDP only
    makes sense for large, long-running training jobs, available instance sizes are
    `ml.p3.16xlarge`, `ml.p3dn24dnxlarge`, and `ml.p4d.24xlarge`.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: DDP支持TensorFlow、PyTorch和Hugging Face。前两个需要对训练代码进行小的修改，但最后一个则不需要。由于DDP仅在大规模、长时间运行的训练任务中才有意义，因此可用的实例类型包括`ml.p3.16xlarge`、`ml.p3dn24dnxlarge`和`ml.p4d.24xlarge`。
- en: 'The **SageMaker** **Distributed Model Parallel** (**DMP**) library solves a
    different problem. Some large deep learning models are simply too bulky to fit
    inside the memory of a single GPU. Others barely fit, forcing you to work with
    very small batch sizes, and slowing down your training jobs. DMP solves this problem
    by automatically partitioning models across a cluster of GPUs and orchestrating
    the flow of data through these different partitions. You can learn more at the
    following link:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**SageMaker** **分布式模型并行（DMP）**库解决了另一个问题。某些大型深度学习模型过于庞大，无法适应单个GPU的内存。另一些模型勉强能够适应，但会迫使你使用非常小的批量大小，从而减慢训练速度。DMP通过自动将模型分割到GPU集群中，并协调数据在这些分区之间流动，解决了这个问题。你可以通过以下链接了解更多内容：'
- en: '[https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/](https://aws.amazon.com/blogs/aws/amazon-sagemaker-simplifies-training-deep-learning-models-with-billions-of-parameters/)'
- en: DMP is available for TensorFlow, PyTorch, and Hugging Face. Again, the first
    two require small modifications to the training code, and the last one doesn't,
    as the Hugging Face `Trainer` API fully supports DMP.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: DMP支持TensorFlow、PyTorch和Hugging Face。再次说明，前两个需要对训练代码进行小的修改，而Hugging Face则不需要，因为其`Trainer`
    API完全支持DMP。
- en: Let's give both a try by revisiting our TensorFlow and Hugging Face examples
    from [*Chapter 7*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130), *Extending
    Machine Learning Services Using Built-In Frameworks*.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过回顾我们在[*第7章*](B17705_07_Final_JM_ePub.xhtml#_idTextAnchor130)中的TensorFlow和Hugging
    Face示例来尝试这两种方法，*使用内置框架扩展机器学习服务*。
- en: Training on TensorFlow with SageMaker DDP
  id: totrans-318
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker DDP进行TensorFlow训练
- en: 'Our initial code used the high-level Keras API: `compile()`, `fit()`, and so
    on. In order to implement DDP, we need to rewrite this code to use `tf.GradientTape()`,
    and to implement a custom training loop. It''s not as difficult as it sounds,
    so let''s get to work:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最初的代码使用了高级的Keras API：`compile()`、`fit()`等。为了实现DDP，我们需要重写这些代码，使用`tf.GradientTape()`并实现一个自定义训练循环。其实并不像听起来那么难，接下来让我们开始吧：
- en: 'First, we need to import and initialize DDP:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要导入并初始化DDP：
- en: '[PRE22]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Then, we retrieve the list of GPUs present on an instance, and we assign them
    a local DDP rank, which is just an integer identifier. We also allow memory growth,
    a TensorFlow feature required by DDP:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们获取实例上存在的GPU列表，并为它们分配一个本地的DDP排名，这是一个简单的整数标识符。我们还允许内存增长，这是DDP所需的TensorFlow功能：
- en: '[PRE23]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'As recommended by the documentation, we increase the batch size and the learning
    rate according to the number of GPUs present in the training cluster. This is
    very important for job accuracy:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据文档的建议，我们根据训练集群中GPU的数量增加批量大小和学习率。这对任务的准确性至关重要：
- en: '[PRE24]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We then create a loss function and an optimizer. Labels have been one-hot encoded
    during preprocessing, so we use `CategoricalCrossentropy`, not `SparseCategoricalCrossentropy`.
    We also initialize model and optimizer variables on all GPUs:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接着，我们创建一个损失函数和优化器。标签在预处理过程中已经进行了独热编码，所以我们使用 `CategoricalCrossentropy`，而不是 `SparseCategoricalCrossentropy`。我们还在所有
    GPU 上初始化模型和优化器变量：
- en: '[PRE25]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we need to write a `training_step()` function, and decorate it with `@tf.function`
    so that DDP recognizes it. As its name implies, this function is responsible for
    running a training step on each GPU in the training cluster: predict a batch,
    compute loss, compute gradients, and apply them. It''s based on the `tf.GradientTape()`
    API, which we simply wrap with `sdp.DistributedGradientTape()`. At the end of
    each training step, we use `sdp.oob_allreduce()` to compute the average loss,
    using values coming from all GPUs:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们需要编写一个 `training_step()` 函数，并用 `@tf.function` 装饰它，以便 DDP 能识别它。顾名思义，这个函数负责在训练集群中的每个
    GPU 上运行训练步骤：预测一个批次，计算损失，计算梯度并应用它们。它基于 `tf.GradientTape()` API，我们只是将其包装成 `sdp.DistributedGradientTape()`。在每个训练步骤的结束时，我们使用
    `sdp.oob_allreduce()` 来计算平均损失，使用来自所有 GPU 的值：
- en: '[PRE26]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we write the training loop. There''s nothing particular about it. To
    avoid log pollution, we only print out messages from the master GPU (rank 0):'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们编写训练循环。没有特别之处。为了避免日志污染，我们仅打印主 GPU（rank 0）的消息：
- en: '[PRE27]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Finally, we save the model on GPU #0 only:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '最后，我们只在 GPU #0 上保存模型：'
- en: '[PRE28]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Moving to our notebook, we configure this job with two `ml.p3.16xlarge` instances,
    and we enable data parallelism with an additional parameter in the estimator:'
  id: totrans-334
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在我们的笔记本中，我们使用两个 `ml.p3.16xlarge` 实例来配置此任务，并通过在估算器中添加额外参数启用数据并行：
- en: '[PRE29]'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We train as usual, and we see steps going by in the training log:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们照常训练，可以在训练日志中看到步骤在进行：
- en: '[PRE30]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: As you can see, it's not really difficult to scale training jobs with SageMaker
    DDP, especially if your training code already uses low-level APIs. We used TensorFlow
    here, and the process for PyTorch is very similar.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用 SageMaker DDP 扩展训练任务其实并不难，尤其是当你的训练代码已经使用了低级 API 时。我们这里使用了 TensorFlow，PyTorch
    的过程非常相似。
- en: Now, let's see how we can train large Hugging Face models with both libraries.
    Indeed, state-of-the-art NLP models are getting larger and more complex all the
    time, and they're good candidates for data parallelism and model parallelism.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用这两个库训练大型 Hugging Face 模型。确实，最先进的 NLP 模型在不断变大和变得更加复杂，它们是数据并行和模型并行的理想候选者。
- en: Training on Hugging Face with SageMaker DDP
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Hugging Face 上使用 SageMaker DDP 进行训练
- en: 'As the Hugging Face `Trainer` API fully supports DDP, we don''t need to change
    anything in our training script. Woohoo. All it takes is an extra parameter in
    the estimator. Set the instance type and instance count, and you''re good to go:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Hugging Face 的 `Trainer` API 完全支持 DDP，我们不需要修改训练脚本。太棒了！只需在估算器中添加一个额外的参数，设置实例类型和实例数量，就可以开始了：
- en: '[PRE31]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Training on Hugging Face with SageMaker DMP
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 Hugging Face 上使用 SageMaker DMP 进行训练
- en: 'Adding DMP is not difficult either. Our Hugging Face example uses a **DistilBERT**
    model that is about 250 MB. That''s small enough to fit on a single GPU, but let''s
    try to train with DMP anyway:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 添加 DMP 也不难。我们的 Hugging Face 示例使用了一个 **DistilBERT** 模型，大小大约为 250 MB。这个大小足以在单个
    GPU 上运行，但我们还是来尝试用 DMP 进行训练：
- en: 'First, we need to configure `processes_per_host` to a value lower or equal
    to the number of GPUs on a training instance. Here, I''ll use an `ml.p3dn.24xlarge`
    instance with 8 NVIDIA V100 GPUs:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要将 `processes_per_host` 配置为小于或等于训练实例上 GPU 数量的值。在这里，我将使用一个带有 8 个 NVIDIA
    V100 GPU 的 `ml.p3dn.24xlarge` 实例：
- en: '[PRE32]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Then, we configure DMP options. Here, I set the most important ones – the number
    of model partitions that we want (`partitions`), and how many times they should
    be replicated for increased parallelism (`microbatches`). In other words, our
    model will be split in four, each split will be duplicated, and these eight splits
    will each run on a different GPU. You can find more information on all parameters
    at the following link:'
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们配置 DMP 选项。这里，我设置了最重要的参数——我们希望的模型分区数（`partitions`），以及为了增加并行性而应复制多少次它们（`microbatches`）。换句话说，我们的模型将被分成四个部分，每个部分会被复制，并且这八个部分将分别运行在不同的
    GPU 上。你可以在以下链接找到更多关于所有参数的信息：
- en: '[https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html)'
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html](https://sagemaker.readthedocs.io/en/stable/api/training/smd_model_parallel_general.html)'
- en: '[PRE33]'
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Finally, we configure our estimator and train as usual:'
  id: totrans-350
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们配置估算器并按常规进行训练：
- en: '[PRE34]'
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'You can find additional examples here:'
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 您可以在这里找到更多示例：
- en: TensorFlow and PyTorch
  id: totrans-353
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 和 PyTorch
- en: '[https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training](https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training)'
  id: totrans-354
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training](https://github.com/aws/amazon-sagemaker-examples/tree/master/training/distributed_training)'
- en: 'Hugging Face: [https://github.com/huggingface/notebooks/tree/master/sagemaker](https://github.com/huggingface/notebooks/tree/master/sagemaker)'
  id: totrans-355
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hugging Face：[https://github.com/huggingface/notebooks/tree/master/sagemaker](https://github.com/huggingface/notebooks/tree/master/sagemaker)
- en: To close this chapter, let's now look at storage options you should consider
    for very large-scale, high-performance training jobs.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，我们将查看您应考虑的针对大规模、高性能训练任务的存储选项。
- en: Using other storage services
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用其他存储服务
- en: 'So far, we''ve used S3 to store training data. At a large scale, throughput
    and latency can become a bottleneck, making it necessary to consider other storage
    services:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经使用 S3 存储训练数据。在大规模情况下，吞吐量和延迟可能成为瓶颈，因此有必要考虑其他存储服务：
- en: '**Amazon** **Elastic File System** (**EFS**): [https://aws.amazon.com/efs](https://aws.amazon.com/efs)'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon 弹性文件系统**（**EFS**）：[https://aws.amazon.com/efs](https://aws.amazon.com/efs)'
- en: '**Amazon** **FSx for Lustre**: [https://aws.amazon.com/fsx/lustre](https://aws.amazon.com/fsx/lustre).'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Amazon FSx for Lustre**：[https://aws.amazon.com/fsx/lustre](https://aws.amazon.com/fsx/lustre)。'
- en: Note
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'This section requires a little bit of AWS knowledge on VPCs, subnets, and security
    groups. If you''re not familiar at all with these, I''d recommend reading the
    following:'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本节内容需要一些关于 VPC、子网和安全组的 AWS 知识。如果您对此不熟悉，建议您阅读以下内容：
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html)'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Subnets.html)'
- en: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html](https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html)'
- en: Working with SageMaker and Amazon EFS
  id: totrans-365
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 SageMaker 和 Amazon EFS
- en: EFS is a managed storage service compatible with **NFS** v4\. It lets you create
    volumes that can be attached to EC2 instances and SageMaker instances. This is
    a convenient way to share data, and you can use it to scale I/O for large training
    jobs.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: EFS 是一个托管存储服务，兼容 **NFS** v4。它允许您创建可以附加到 EC2 实例和 SageMaker 实例的卷。这是一种方便的数据共享方式，您可以使用它来扩展大规模训练任务的
    I/O。
- en: By default, files are stored in the **Standard** class. You can enable a life
    cycle policy that automatically moves files that haven't been accessed for a certain
    time to the **Infrequent Access**, which is slower but more cost-effective.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，文件存储在**标准**类中。您可以启用生命周期策略，自动将一段时间内未访问的文件移至**低频访问**，这种方式较慢，但更加具有成本效益。
- en: 'You can pick one of two throughput modes:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择两种吞吐量模式中的一种：
- en: '**Bursting throughput**: Burst credits are accumulated over time, and burst
    capacity depends on the size of the filesystem: 100 MB/s, plus an extra 100 MB/s
    for each TB of storage.'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**突发吞吐量**：突发积分会随着时间积累，突发容量取决于文件系统的大小：100 MB/s，并且每个 TB 的存储会额外增加 100 MB/s。'
- en: '**Provisioned throughput**: You set the expected throughput, from 1 to 1,024
    MB/s.'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预配置吞吐量**：您可以设置期望的吞吐量，范围从 1 到 1,024 MB/s。'
- en: 'You can also pick one of two performance modes:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择两种性能模式中的一种：
- en: '**General purpose**: This is fine for most applications.'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用用途**：对于大多数应用程序来说，这种模式足够了。'
- en: '**Max I/O**: This is the one to use if tens or hundreds of instances are accessing
    the volume. Throughput will be maximized at the expense of latency.'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大 I/O**：当数十个或数百个实例访问卷时，使用这个设置。吞吐量将最大化，但会牺牲延迟。'
- en: Let's create an 8 GB EFS volume. Then, we'll mount it on an EC2 instance to
    copy the **Pascal VOC** dataset that we previously prepared, and we'll train an
    object detection job. To keep costs reasonable, we won't scale the job, but the
    overall process would be exactly the same at any scale.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个 8 GB 的 EFS 卷。然后，我们将在 EC2 实例上挂载它，复制之前准备好的**Pascal VOC**数据集，并训练一个目标检测任务。为了保持合理的成本，我们不会扩展该任务，但无论规模大小，整个过程都是相同的。
- en: Provisioning an EFS volume
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 配置 EFS 卷
- en: 'The EFS console makes it extremely simple to create a volume. You can find
    detailed instructions at [https://docs.aws.amazon.com/efs/latest/ug/getting-started.html](https://docs.aws.amazon.com/efs/latest/ug/getting-started.html):'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: EFS 控制台使创建卷变得极其简单。你可以在 [https://docs.aws.amazon.com/efs/latest/ug/getting-started.html](https://docs.aws.amazon.com/efs/latest/ug/getting-started.html)
    找到详细的操作说明：
- en: We set the volume name to `sagemaker-demo`.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将卷名称设置为 `sagemaker-demo`。
- en: We select our default VPC, and use **Regional** availability.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们选择我们的默认 VPC，并使用 **区域** 可用性。
- en: 'We create the volume. Once it''s ready, you should see something similar to
    the following screenshot:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建了卷。一旦卷准备好，你应该会看到类似下面的截图：
- en: '![Figure 9.9– Creating an EFS volume'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 9.9– 创建 EFS 卷](img/B17705_09_9.jpg)'
- en: '](img/B17705_09_8.jpg)'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_8.jpg)'
- en: Figure 9.9– Creating an EFS volume
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9– 创建 EFS 卷
- en: The EFS volume is ready to receive data. We're now going to create a new EC2
    instance, mount the EFS volume, and copy the dataset.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: EFS 卷已准备好接收数据。我们现在将创建一个新的 EC2 实例，挂载 EFS 卷，并复制数据集。
- en: Creating an EC2 instance
  id: totrans-384
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 EC2 实例
- en: 'As EFS volumes live inside a VPC, they can only be accessed by instances located
    in the same VPC. These instances must also have a *security group* that allows
    inbound NFS traffic:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 EFS 卷位于 VPC 内部，它们只能由位于同一 VPC 中的实例访问。这些实例必须还拥有一个 *安全组*，该安全组允许进入的 NFS 流量：
- en: In the VPC console ([https://console.aws.amazon.com/vpc/#vpcs:sort=VpcId](https://console.aws.amazon.com/vpc/#vpcs:sort=VpcId)),
    we write down the ID of our default VPC. For me, it's `vpc-def884bb`.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 VPC 控制台中（[https://console.aws.amazon.com/vpc/#vpcs:sort=VpcId](https://console.aws.amazon.com/vpc/#vpcs:sort=VpcId)），我们记录下默认
    VPC 的 ID。对我而言，它是 `vpc-def884bb`。
- en: Still in the VPC console, we move to the **Subnets** section ([https://console.aws.amazon.com/vpc/#subnets:sort=SubnetId](https://console.aws.amazon.com/vpc/#subnets:sort=SubnetId)).
    We write down the subnet IDs and the availability zone for all subnets hosted
    in the default VPC.
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仍然在 VPC 控制台中，我们转到 **子网** 部分（[https://console.aws.amazon.com/vpc/#subnets:sort=SubnetId](https://console.aws.amazon.com/vpc/#subnets:sort=SubnetId)）。我们记录下默认
    VPC 中所有子网的子网 ID 和可用区。
- en: 'For me, they look like what''s shown in the next screenshot:'
  id: totrans-388
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 对我来说，它们看起来像下图所示：
- en: '![Figure 9.10 – Viewing subnets for the default VPC'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 9.10 – 查看默认 VPC 的子网](img/B17705_09_10.jpg)'
- en: '](img/B17705_09_9.jpg)'
  id: totrans-390
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_09_9.jpg)'
- en: Figure 9.10 – Viewing subnets for the default VPC
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 9.10 – 查看默认 VPC 的子网
- en: Moving to the EC2 console, we create an EC2 instance. We select the Amazon Linux
    2 image and a `t2.micro` instance size.
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 进入 EC2 控制台，我们创建一个 EC2 实例。我们选择 Amazon Linux 2 镜像和 `t2.micro` 实例类型。
- en: Next, we set `eu-west-1a` **Availability Zone**. We also assign it the security
    group we just created, **IAM role** to a role with appropriate S3 permissions,
    and **File Systems** to the EFS filesystem that we just created. We also make
    sure to tick the box that automatically creates and attaches the required security
    groups.
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们设置 `eu-west-1a` **可用区**。我们还将其分配给我们刚刚创建的安全组，**IAM 角色**分配给具有适当 S3 权限的角色，以及
    **文件系统**分配给我们刚刚创建的 EFS 文件系统。我们还确保勾选了自动创建并附加所需安全组的选项。
- en: In the next screens, we leave storage and tags as they are, and we attach a
    security group that allows incoming `ssh`. Finally, we launch instance creation.
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的界面中，我们保持存储和标签不变，并附加一个允许 `ssh` 连接的安全组。最后，我们启动实例创建。
- en: Accessing an EFS volume
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问 EFS 卷
- en: 'Once the instance is ready, we can `ssh` to it:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦实例准备好，我们可以通过 `ssh` 连接到它：
- en: 'We see that the EFS volume has been automatically mounted:'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们看到 EFS 卷已自动挂载：
- en: '[PRE35]'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: We move to that location, and sync our PascalVOC dataset from S3\. As the filesystem
    is mounted as `root`, we need to use `sudo`.
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们移动到该位置，并从 S3 同步我们的 PascalVOC 数据集。由于文件系统以 `root` 身份挂载，我们需要使用 `sudo`。
- en: '[PRE36]'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Job done. We can log out and shut down or terminate the instance, as we won't
    need it anymore.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 工作完成。我们可以退出并关闭或终止实例，因为我们不再需要它。
- en: Now, let's train with this dataset.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个数据集进行训练。
- en: Training an object detection model with EFS
  id: totrans-403
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 EFS 训练物体检测模型
- en: 'The training process is identical, except for the location of the input data:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 训练过程是相同的，唯一不同的是输入数据的位置：
- en: 'Instead of using the `TrainingInput` object to define input channels, we use
    the `FileSystemInput` object, passing the identifier of our EFS volume and the
    absolute data path inside the volume:'
  id: totrans-405
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们不再使用 `TrainingInput` 对象来定义输入通道，而是使用 `FileSystemInput` 对象，传入我们 EFS 卷的标识符和卷内的绝对数据路径：
- en: '[PRE37]'
  id: totrans-406
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: We configure the `Estimator` module, passing the list of subnets for the VPC
    hosting the EFS volume. SageMaker will launch training instances there so that
    they may mount the EFS volume. We also need to pass a security group allowing
    NFS traffic. We can reuse the one that was automatically created for our EC2 instance
    (not the one allowing ssh access) – it's visible in the **Security** tab in the
    instance details, as shown in the next screenshot:![Figure 9.11 – Viewing security
    groups
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置`Estimator`模块，传递托管EFS卷的VPC子网列表。SageMaker将在那里启动训练实例，以便它们可以挂载EFS卷。我们还需要传递一个安全组，允许NFS流量。我们可以重用为EC2实例自动创建的那个安全组（不是允许ssh访问的那个）–
    它在实例详情的**Security**标签中可见，如下图所示：![Figure 9.11 – 查看安全组
- en: '](img/B17705_09_10.jpg)'
  id: totrans-408
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_09_10.jpg)'
- en: '[PRE38]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: For testing purposes, we only train for one epoch. Business as usual, although,
    this time, data is loaded from our EFS volume.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了测试目的，我们只训练一个周期。像往常一样，虽然这次数据是从我们的EFS卷加载的。
- en: Once training is complete, you may delete the EFS volume in the EFS console
    to avoid unnecessary costs.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，你可以在EFS控制台中删除EFS卷，以避免不必要的费用。
- en: Now, let's see how we can use another storage service – Amazon FSx for Lustre.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何使用另一个存储服务——Amazon FSx for Lustre。
- en: Working with SageMaker and Amazon FSx for Lustre
  id: totrans-413
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用SageMaker和Amazon FSx for Lustre
- en: Very large-scale workloads require high throughput and low latency storage –
    two qualities that Amazon FSx for Lustre possesses. As the name implies, this
    service is based on the Lustre filesystem ([http://lustre.org](http://lustre.org)),
    a popular open source choice for **HPC** applications.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 非常大规模的工作负载需要高吞吐量和低延迟存储——这正是Amazon FSx for Lustre所具备的两个特点。顾名思义，这项服务基于Lustre文件系统（[http://lustre.org](http://lustre.org)），这是一个流行的开源选择，适用于**HPC**应用。
- en: 'The smallest filesystem you can create is 1.2 TB (like I said, "very large-scale").
    We can pick one of two deployment options for FSx filesystems:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以创建的最小文件系统是1.2 TB（就像我说的，“非常大规模”）。我们可以为FSx文件系统选择两种部署选项之一：
- en: '**Persistent**: This should be used for long-term storage that requires high
    availability.'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持久化**：应用于需要高可用性的长期存储。'
- en: '**Scratch**: Data is not replicated, and it won''t persist if a file server
    fails. In exchange, we get high burst throughput, making this is a good choice
    for spiky, short-term jobs.'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**临时存储**：数据不会复制，如果文件服务器发生故障，数据将不会持久化。作为交换，我们获得了高爆发吞吐量，使其成为突发性短期作业的良好选择。'
- en: Optionally, a filesystem can be backed by an S3 bucket. Objects are automatically
    copied from S3 to FSx when they're first accessed.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，文件系统可以由S3桶支持。对象首次访问时，会自动从S3复制到FSx。
- en: 'Just like for EFS, a filesystem lives inside a VPC, and we''ll need a security
    group allowing inbound Lustre traffic (ports 988 and 1,021-2,023). You can create
    this in the EC2 console, and it should be similar to the following screenshot:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 就像EFS一样，文件系统存在于VPC中，我们需要一个安全组，允许传入Lustre流量（端口988和1,021-2,023）。你可以在EC2控制台中创建它，它应该类似于以下截图：
- en: '![Figure 9.12 – Creating a security group for FSx for Lustre'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.12 – 为FSx for Lustre创建安全组'
- en: '](img/B17705_09_11.jpg)'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_11.jpg)'
- en: Figure 9.12 – Creating a security group for FSx for Lustre
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 – 为FSx for Lustre创建安全组
- en: 'Let''s create the filesystem:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建文件系统：
- en: In the FSx console, we create a filesystem named `sagemaker-demo`, and we select
    the **Scratch** deployment type.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在FSx控制台中，我们创建了一个名为`sagemaker-demo`的文件系统，并选择了**临时存储**部署类型。
- en: We set storage capacity to 1.2 TB.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们设置存储容量为1.2 TB。
- en: In the `eu-west-1a` subnet of the default VPC, and we assign it to the security
    group we just created.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在默认VPC的`eu-west-1a`子网中，我们将其分配给我们刚刚创建的安全组。
- en: In the `s3://sagemaker-eu-west-1-123456789012`) and the prefix (`pascalvoc`).
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`s3://sagemaker-eu-west-1-123456789012`）和前缀（`pascalvoc`）中。
- en: On the next screen, we review our choices, as shown in the following screenshot,
    and we create the filesystem.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一个屏幕上，我们检查我们的选择，如下图所示，然后创建文件系统。
- en: 'After a few minutes, the filesystem is in service, as shown in the following
    screenshot:'
  id: totrans-429
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 几分钟后，文件系统已投入使用，如下图所示：
- en: '![Figure 9.13 – Creating an FSx volume'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 9.13 – 创建FSx卷'
- en: '](img/B17705_09_12.jpg)'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_09_12.jpg)'
- en: Figure 9.13 – Creating an FSx volume
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.13 – 创建FSx卷
- en: As the filesystem is backed by an S3 bucket, we don't need to populate it. We
    can proceed directly to training.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 由于文件系统由S3桶支持，我们无需填充它。我们可以直接进行训练。
- en: Training an object detection model with FSx for Lustre
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用FSx for Lustre训练物体检测模型
- en: 'Now, we will train the model using FSx as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用FSx训练模型，具体如下：
- en: 'Similar to what we just did with EFS, we define input channels with `FileSystemInput`.
    One difference is that the directory path must start with the name of the filesystem
    mount point. You can find it as **Mount name** in the FSx console:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似于我们刚刚在EFS中所做的，我们使用`FileSystemInput`定义输入通道。一个区别是目录路径必须以文件系统挂载点的名称开头。您可以在FSx控制台中找到它作为**挂载名称**：
- en: '[PRE39]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: All other steps are identical. Don't forget to update the name of the security
    group passed to the `Estimator` module.
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有其他步骤都是相同的。不要忘记更新传递给`Estimator`模块的安全组名称。
- en: When we're done training, we delete the FSx filesystem in the console.
  id: totrans-439
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们完成训练后，在控制台中删除FSx文件系统。
- en: 'This concludes our exploration of storage options for SageMaker. Summing things
    up, here are my recommendations:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对SageMaker存储选项的探索。总结一下，这里是我的建议：
- en: First, you should use RecordIO or TFRecord data as much as possible. They're
    convenient to move around, faster to train on, and they work with both file mode
    and pipe mode.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，您应尽可能使用RecordIO或TFRecord数据。它们便于移动，训练速度更快，并且可以同时使用文件模式和管道模式。
- en: For development and small-scale production, file mode is completely fine. Your
    primary focus should always be your machine learning problem, not useless optimization.
    Even at a small scale, EFS can be an interesting option for collaboration, as
    it makes it easy to share datasets and notebooks.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于开发和小规模生产，文件模式完全没问题。您的主要关注点应始终是您的机器学习问题，而不是无用的优化。即使在小规模下，EFS也可以作为协作的有趣选择，因为它便于共享数据集和笔记本。
- en: If you train with built-in algorithms, pipe mode is a no-brainer, and you should
    use it at every opportunity. If you train with frameworks or your own code, implementing
    pipe mode will take some work, and is probably not worth the engineering effort
    unless you're working at a significant scale (hundreds of gigabytes or more).
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您使用内置算法进行训练，管道模式是一个明智的选择，并且应该在每一个机会中使用它。如果您使用框架或自己的代码进行训练，实施管道模式将需要一些工作，可能不值得工程投入，除非您在进行大规模工作（数百GB或更多）。
- en: If you have large, distributed workloads with tens of instances or more, EFS
    in Performance Mode is worth trying. Don't go near the mind-blowing FSx for Lustre
    unless you have insane workloads.
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您有大规模、分布式的工作负载，涉及数十个实例或更多，性能模式下的EFS值得一试。不要接近令人惊叹的FSx for Lustre，除非您有疯狂的工作负载。
- en: Summary
  id: totrans-445
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, you learned how and when to scale training jobs. You saw that
    it definitely takes some careful analysis and experimentation to find the best
    setup: scaling up versus scaling out, CPU versus GPU versus multi-GPU, and so
    on. This should help you to make the right decisions for your own workloads and
    avoid costly mistakes.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了何时以及如何扩展训练作业。您看到，要找到最佳设置，肯定需要进行一些仔细的分析和实验：扩展与扩展，CPU与GPU与多GPU等。这应该帮助您为自己的工作负载做出正确的决策，并避免昂贵的错误。
- en: You also learned how to achieve significant speedup with techniques such as
    distributed training, data parallelism, model parallelism, RecordIO, and pipe
    mode. Finally, you learned how to set Amazon EFS and Amazon FSx for Lustre for
    large-scale training jobs.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 您还学习了如何通过分布式训练、数据并行、模型并行、RecordIO和管道模式等技术实现显著加速。最后，您学习了如何为大规模训练作业设置Amazon EFS和Amazon
    FSx for Lustre。
- en: In the next chapter, we'll cover advanced features for hyperparameter optimization,
    cost optimization, model debugging, and more.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍用于超参数优化、成本优化、模型调试等高级功能。
