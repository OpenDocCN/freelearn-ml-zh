- en: Chapter 8. Recommendations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章 推荐系统
- en: Recommendations have become one of the staples of online services and commerce.
    This type of automated system can provide each user with a personalized list of
    suggestions (be it a list of products to purchase, features to use, or new connections).
    In this chapter, we will see the basic ways in which automated recommendation
    generation systems work. The field of recommendation based on consumer inputs
    is often called collaborative filtering, as the users collaborate through the
    system to find the best items for each other.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统已经成为在线服务和电商的基础之一。这种自动化系统可以为每个用户提供个性化的建议列表（无论是购买的产品、使用的功能还是新的社交连接）。在本章中，我们将看到自动化推荐生成系统的基本工作原理。基于消费者输入的推荐领域通常被称为协同过滤，因为用户通过系统进行协作，帮助彼此找到最佳产品。
- en: 'In the first part of this chapter, we will see how we can use past product
    ratings from consumers to predict new ratings. We start with a few ideas that
    are helpful and then combine all of them. When combining, we use regression to
    learn the best way in they can be combined. This will also allow us to explore
    a generic concept in machine learning: ensemble learning.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们将看到如何利用消费者过去的产品评分来预测新的评分。我们从一些有用的想法开始，然后将它们结合在一起。在结合时，我们使用回归分析来学习它们可以如何最优地组合。这也将让我们探讨机器学习中的一个通用概念：集成学习。
- en: 'In the second part of this chapter, we will take a look at a different way
    of learning recommendations: basket analysis. Unlike the case in which we have
    numeric ratings, in the basket analysis setting, all we have is information about
    the shopping baskets, that is, what items were bought together. The goal is to
    learn about recommendations. You have probably already seen features of the form
    "people who bought X also bought Y" in online shopping. We will develop a similar
    feature of our own.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第二部分，我们将探讨一种不同的推荐学习方法：购物篮分析。与我们拥有数字评分的情况不同，在购物篮分析中，我们仅拥有关于购物篮的信息，也就是说，哪些商品是一起购买的。目标是学习如何进行推荐。你可能已经在在线购物中见过类似“购买X的人也购买了Y”这样的功能。我们将开发出一个类似的功能。
- en: Rating predictions and recommendations
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评分预测和推荐
- en: If you have used any online shopping system in the last 10 years, you have probably
    seen these recommendations. Some are like Amazon's "costumers who bought X also
    bought Y". These will be dealt with later in the chapter in the *Basket analysis*
    section. Other recommendations are based on predicting the rating of a product,
    such as a movie.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在过去10年里使用过任何在线购物系统，你可能已经见过这些推荐。有些类似于亚马逊的“购买X的客户也购买了Y”。这些将在本章的*购物篮分析*部分中进一步探讨。其他推荐则基于预测产品的评分，比如电影的评分。
- en: The problem of learning recommendations based on past product ratings was made
    famous by the Netflix Prize, a million-dollar machine learning public challenge
    by Netflix. Netflix (well-known in the USA and UK and in a process of international
    expansion) is a movie rental company. Traditionally, you would receive DVDs in
    the mail; more recently, Netflix has focused on the online streaming of movies
    and TV shows. From the start, one of the distinguishing features of the service
    was that it gives users the option to rate the films they have seen. Netflix then
    uses these ratings to recommend other films to its customers. In this machine
    learning problem, you not only have the information about which films the user
    saw but also about how the user rated them.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基于过去产品评分学习推荐的问题由Netflix大奖而闻名，Netflix大奖是Netflix举办的百万美元机器学习公开挑战赛。Netflix（在美国和英国非常知名，并正在进行国际扩展）是一家电影租赁公司。传统上，你会收到邮寄来的DVD；最近，Netflix专注于在线电影和电视节目的流媒体播放。从一开始，Netflix的一个独特之处在于它允许用户对看过的电影进行评分。Netflix随后使用这些评分向用户推荐其他电影。在这个机器学习问题中，你不仅知道用户看了哪些电影，还知道用户如何评分。
- en: In 2006, Netflix made a large number of customer ratings of films in its database
    available for a public challenge. The goal was to improve on their in-house algorithm
    for rating prediction. Whoever would be able to beat it by 10 percent or more
    would win 1 million dollars. In 2009, an international team named BellKor's Pragmatic
    Chaos was able to beat this mark and take the prize. They did so just 20 minutes
    before another team, The Ensemble, and passed the 10 percent mark as well—an exciting
    photo finish for a competition that lasted several years.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 2006年，Netflix将其数据库中大量客户对电影的评分数据公开，举行了一场公开挑战。目标是改进Netflix内部的评分预测算法。任何能够将其提升10％或更多的人将赢得100万美元的奖金。2009年，一个名为BellKor's
    Pragmatic Chaos的国际团队成功突破了这个标准，并获得了奖金。他们是在另一个团队The Ensemble的前20分钟成功做到的，并且同样突破了10％的改进——这场持续了数年的竞赛最终以一个激动人心的结局落下帷幕。
- en: Tip
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Machine learning in the real world**'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**现实世界中的机器学习**'
- en: 'Much has been written about the Netflix Prize, and you may learn a lot by reading
    up on it. The techniques that won were a mixture of advanced machine learning
    and a lot of work put into preprocessing the data. For example, some users like
    to rate everything very highly, while others are always more negative; if you
    do not account for this in preprocessing, your model will suffer. Other normalizations
    were also necessary for a good result: how old is the film and how many ratings
    did it receive. Good algorithms are a good thing, but you always need to "get
    your hands dirty" and tune your methods to the properties of the data you have
    in front of you. Preprocessing and normalizing the data is often the most time-consuming
    part of the machine learning process. However, this is also the place where one
    can have the biggest impact on the final performance of the system.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Netflix奖已经有很多相关的讨论，你可以通过阅读相关资料了解更多。获奖的技术是先进的机器学习方法与大量数据预处理工作相结合的结果。例如，有些用户倾向于给所有电影打很高的分，而有些用户总是给出较低的评价；如果你在预处理阶段不考虑这一点，你的模型就会受到影响。为了获得好的结果，还需要进行其他的归一化处理：例如，电影的上映年份以及它收到的评价数量。好的算法很重要，但你始终需要“亲自动手”，根据你手头数据的特性调整你的方法。数据的预处理和归一化往往是机器学习过程中最耗时的部分。然而，这也是对系统最终表现产生最大影响的地方。
- en: The first thing to note about the Netflix Prize is how hard it was. Roughly
    speaking, the internal system that Netflix used was about 10 percent better than
    no recommendations (that is, assigning each movie just the average value for all
    users). The goal was to obtain just another 10 percent improvement on this. In
    total, the winning system was roughly just 20 percent better than no personalization.
    Yet, it took a tremendous amount of time and effort to achieve this goal. And
    even though 20 percent does not seem like much, the result is a system that is
    useful in practice.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 关于Netflix奖，首先要注意的是它的难度。大致来说，Netflix使用的内部系统比没有推荐的系统（即每个电影仅给所有用户的平均值）好10%左右。目标只是要在此基础上再提升10%。最终，获奖系统比没有个性化的系统好大约20%。然而，要实现这个目标，付出了巨大的时间和努力。尽管20%的提升看起来并不多，但最终的结果是一个在实际中非常有用的系统。
- en: Unfortunately, for legal reasons, this dataset is no longer available. Although
    the data was anonymous, there were concerns that it might be possible to discover
    who the clients were and reveal private details of movie rentals. However, we
    can use an academic dataset with similar characteristics. This data comes from
    GroupLens, a research laboratory at the University of Minnesota.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，出于法律原因，这个数据集目前已经无法获取。尽管数据是匿名的，但人们担心可能会揭示出客户的身份以及电影租赁的私人信息。不过，我们可以使用一个具有类似特征的学术数据集。这些数据来自GroupLens，这是明尼苏达大学的一个研究实验室。
- en: How can we solve a Netflix style ratings prediction question? We will see two
    different approaches, neighborhood approaches and regression approaches. We will
    also see how to combine these to obtain a single prediction.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解决类似Netflix的评分预测问题呢？我们将看到两种不同的方法，邻域方法和回归方法。我们还会看到如何将这两种方法结合起来，得出一个统一的预测结果。
- en: Splitting into training and testing
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 划分训练集和测试集
- en: 'At a high level, splitting the dataset into training and testing data in order
    to obtain a principled estimate of the system''s performance is performed as in
    previous chapters: we take a certain fraction of our data points (we will use
    10 percent) and reserve them for testing; the rest will be used for training.
    However, because the data is structured differently in this context, the code
    is different. The first step is to load the data from disk, for which we use the
    following function:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从高层次上看，将数据集划分为训练数据和测试数据，以便获得系统性能的原则性估计，方法与之前的章节相同：我们将取一定比例的数据点（我们将使用 10%）并将其保留用于测试；其余数据用于训练。然而，由于在此上下文中数据的结构不同，因此代码也有所不同。第一步是从磁盘加载数据，我们使用以下函数：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note that zero entries in this matrix represent missing ratings.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个矩阵中的零条目表示缺失的评分。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We now use the standard random module to choose indices to test:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用标准的随机模块选择要测试的索引：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, we build the `train` matrix, which is like `reviews`, but with the testing
    entries set to zero:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们构建 `train` 矩阵，它类似于 `reviews`，但将测试条目的值设为零：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Finally, the `test` matrix contains just the testing values:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`test` 矩阵只包含测试值：
- en: '[PRE4]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: From now on, we will work on taking the training data, and try to predict all
    the missing entries in the dataset. That is, we will write code that assigns each
    (user, movie) pair a recommendation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 从现在起，我们将开始处理训练数据，并尝试预测数据集中所有缺失的条目。也就是说，我们将编写代码，为每个（用户，电影）对分配一个推荐。
- en: Normalizing the training data
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对训练数据进行标准化
- en: 'As we discussed, it is best to normalize the data to remove obvious movie or
    user-specific effects. We will just use one very simple type of normalization,
    which we used before: conversion to z-scores.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的那样，最好的做法是对数据进行标准化，以去除明显的电影或用户特定的效应。我们将使用一种非常简单的标准化方法，之前也使用过：转换为 z 分数。
- en: Unfortunately, we cannot simply use scikit-learn's normalization objects as
    we have to deal with the missing values in our data (that is, not all movies were
    rated by all users). Thus, we want to normalize by the mean and standard deviation
    of the values that are, in fact, present.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们不能简单地使用 scikit-learn 的标准化对象，因为我们必须处理数据中的缺失值（即，并非所有电影都被所有用户评分）。因此，我们希望通过实际存在的值的均值和标准差进行标准化。
- en: 'We will write our own class, which ignores missing values. This class will
    follow the scikit-learn preprocessing API:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将编写自己的类，它忽略缺失值。这个类将遵循 scikit-learn 的预处理 API：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We want to choose the axis of normalization. By default, we normalize along
    the first axis, but sometimes it will be useful to normalize along the second
    one. This follows the convention of many other NumPy-related functions:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望选择标准化的轴。默认情况下，我们沿第一个轴进行标准化，但有时沿第二个轴进行标准化也会很有用。这遵循了许多其他与 NumPy 相关的函数的约定：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The most important method is the fit method. In our implementation, we compute
    the mean and standard deviation of the values that are not zero. Recall that zeros
    indicate "missing values":'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的方法是 fit 方法。在我们的实现中，我们计算的是非零值的均值和标准差。请记住，零值表示“缺失值”：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'If the axis is 1, we operate on the transposed array as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果轴为 1，我们将在转置数组上进行操作，如下所示：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We add 0.1 to the direct estimate of the standard deviation to avoid underestimating
    the value of the standard deviation when there are only a few samples, all of
    which may be exactly the same. The exact value used does not matter much for the
    final result, but we need to avoid division by zero.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 0.1 加到标准差的直接估计中，以避免在样本很少且所有样本可能完全相同的情况下低估标准差的值。所用的确切值对最终结果影响不大，但我们需要避免除以零的情况。
- en: 'The `transform` method needs to take care of maintaining the binary structure
    as follows:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '`transform` 方法需要维护二进制结构，如下所示：'
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Notice how we took care of transposing the input matrix when the axis is 1
    and then transformed it back so that the return value has the same shape as the
    input. The `inverse_transform` method performs the inverse operation to transform
    as shown here:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当轴为 1 时，我们如何处理输入矩阵的转置，然后再将其转换回来，以便返回值与输入矩阵的形状相同。`inverse_transform` 方法执行逆操作以进行如这里所示的转换：
- en: '[PRE10]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, we add the `fit_transform` method which, as the name indicates, combines
    both the `fit` and `transform` operations:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们添加了 `fit_transform` 方法，顾名思义，它结合了 `fit` 和 `transform` 两个操作：
- en: '[PRE11]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The methods that we defined (`fit`, `transform`, `transform_inverse`, and `fit_transform`)
    were the same as the objects defined in the `sklearn.preprocessing` module. In
    the following sections, we will first normalize the inputs, generate normalized
    predictions, and finally apply the inverse transformation to obtain the final
    predictions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义的方法（`fit`、`transform`、`transform_inverse` 和 `fit_transform`）与 `sklearn.preprocessing`
    模块中定义的对象相同。在接下来的章节中，我们将首先规范化输入，生成规范化的预测值，最后应用逆变换以获得最终预测结果。
- en: A neighborhood approach to recommendations
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种基于邻域的推荐方法
- en: 'The neighborhood concept can be implemented in two ways: user neighbors or
    movie neighbors. User neighborhoods are based on a very simple concept: to know
    how a user will rate a movie, find the users most similar to them, and look at
    their ratings. We will only consider user neighbors for the moment. At the end
    of this section, we will discuss how the code can be adapted to compute movie
    neighbors.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 邻域概念可以通过两种方式实现：用户邻域或电影邻域。用户邻域基于一个非常简单的概念：要知道一个用户如何评分电影，找到与他们最相似的用户，并查看他们的评分。我们暂时只考虑用户邻域。在本节结束时，我们将讨论如何调整代码来计算电影邻域。
- en: One of the interesting techniques that we will now explore is to just see which
    movies each user has rated, even without taking a look at what rating was given.
    Even with a binary matrix where we have an entry equal to 1 when a user rates
    a movie, and 0 when they did not, we can make useful predictions. In hindsight,
    this makes perfect sense; we do not completely randomly choose movies to watch,
    but instead pick those where we already have an expectation of liking them. We
    also do not make random choices of which movies to rate, but perhaps only rate
    those we feel most strongly about (naturally, there are exceptions, but on average
    this is probably true).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在要探讨的一种有趣技巧是，仅仅查看每个用户评分过的电影，即使不查看他们给出的评分。即使我们有一个二元矩阵，其中用户评分的电影用1表示，未评分的电影用0表示，我们仍然可以做出有用的预测。事后看来，这完全是合理的；我们并不是随机选择电影观看，而是选择那些我们已经有一定喜好预期的电影。我们也不是随机选择要评分的电影，而是可能只会评分那些我们感受最强烈的电影（当然，也有例外，但平均来看，这大概是对的）。
- en: We can visualize the values of the matrix as an image, where each rating is
    depicted as a little square. Black represents the absence of a rating and the
    gray levels represent the rating value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将矩阵的值可视化为一张图像，每个评分表示为一个小方块。黑色表示没有评分，灰度级表示评分值。
- en: 'The code to visualize the data is very simple (you can adapt it to show a larger
    fraction of the matrix than is possible to show in this book), as shown in the
    following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化数据的代码非常简单（你可以调整它来显示矩阵的更多部分，而不仅仅是本书中能显示的部分），如下所示：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The following screenshot is the output of this code:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图是该代码的输出：
- en: '![A neighborhood approach to recommendations](img/2772OS_08_03.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![一种基于邻域的推荐方法](img/2772OS_08_03.jpg)'
- en: We can see that the matrix is sparse—most of the squares are black. We can also
    see that some users rate a lot more movies than others and that some movies are
    the target of many more ratings than others.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到矩阵是稀疏的——大部分格子是黑色的。我们也可以看到，有些用户评分的电影比其他用户多，而有些电影比其他电影有更多的评分。
- en: 'We are now going to use this binary matrix to make predictions of movie ratings.
    The general algorithm will be (in pseudo code) as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用这个二元矩阵来预测电影评分。一般的算法将是（伪代码）如下：
- en: For each user, rank every other user in terms of closeness. For this step, we
    will use the binary matrix and use correlation as the measure of closeness (interpreting
    the binary matrix as zeros and ones allows us to perform this computation).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个用户，按相似度对其他所有用户进行排序。此步骤中，我们将使用二元矩阵，并使用相关性作为相似度的度量（将二元矩阵视为零和一使得我们可以进行这种计算）。
- en: 'When we need to estimate a rating for a (user, movie) pair, we look at all
    the users who have rated that movie and split them into two: the most similar
    half and the most dissimilar half. We then use the average of the most similar
    half as the prediction.'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当我们需要估计一个（用户，电影）对的评分时，我们会查看所有评分过该电影的用户，并将他们分为两组：最相似的那一半和最不相似的那一半。然后我们使用最相似那一半的平均值作为预测。
- en: 'We can use the `scipy.spatial.distance.pdist` function to obtain the distance
    between all the users as a matrix. This function returns the correlation distance,
    which transforms the correlation value by inverting it so that larger numbers
    mean less similar. Mathematically, the correlation distance is ![A neighborhood
    approach to recommendations](img/2772OS_08_06.jpg), where ![A neighborhood approach
    to recommendations](img/2772OS_11_15.jpg) is the correlation value. The code is
    as follows:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`scipy.spatial.distance.pdist`函数来获取所有用户之间的距离矩阵。这个函数返回相关距离，它通过反转相关值来转化，使得更大的数值表示不那么相似。从数学上讲，相关距离是![A
    neighborhood approach to recommendations](img/2772OS_08_06.jpg)，其中![A neighborhood
    approach to recommendations](img/2772OS_11_15.jpg)是相关值。代码如下：
- en: '[PRE13]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We can use this matrix to select the nearest neighbors of each user. These are
    the users that most resemble it.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个矩阵来选择每个用户的最近邻居。这些用户是最相似的用户。
- en: '[PRE14]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we iterate over all users to estimate predictions for all inputs:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们遍历所有用户以估算所有输入的预测值：
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The tricky part in the preceding snippet is indexing by the right values to
    select the neighbors who have rated the movie. Then, we choose the half that is
    closest to the user (in the `rev[:n]` line) and average those. Because some films
    have many reviews and others very few, it is hard to find a single number of users
    for all cases. Choosing half of the available data is a more generic approach.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段中的关键部分是通过正确的值进行索引，以选择已经对电影评分的邻居。然后，我们选择距离用户最近的那一半（在`rev[:n]`这一行中）并对它们求平均。由于一些电影有很多评论，而其他电影的评论非常少，因此很难为所有情况找到一个统一的用户数量。选择可用数据的那一半是更通用的方法。
- en: 'To obtain the final result, we need to un-normalize the predictions as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 要得到最终结果，我们需要将预测值反归一化，具体如下：
- en: '[PRE16]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can use the same metrics we learned about in the previous chapter:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用上一章学到的相同度量方法：
- en: '[PRE17]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The preceding code computes the result for user neighbors, but we can use it
    to compute the movie neighbors by simply transposing the input matrix. In fact,
    the code computes neighbors of whatever are the rows of its input matrix.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码计算了用户邻居的结果，但我们可以通过简单地转置输入矩阵来计算电影邻居。事实上，代码计算的是输入矩阵的行的邻居。
- en: 'So we can rerun the following code, by just inserting the following line at
    the top:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们可以重新运行以下代码，只需在顶部插入以下行：
- en: '[PRE18]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Thus we can see that the results are not that different.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 因此我们可以看到，结果并没有那么不同。
- en: In this book's code repository, the neighborhood code has been wrapped into
    a simple function, which makes it easier to reuse.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的代码库中，邻居代码已被封装成一个简单的函数，便于重用。
- en: A regression approach to recommendations
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一种回归方法推荐系统
- en: An alternative to neighborhoods is to formulate recommendations as a regression
    problem and apply the methods that we learned in the previous chapter.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 邻居的替代方案是将推荐问题表述为回归问题，并应用我们在上一章学到的方法。
- en: 'We also consider why this problem is not a good fit for a classification formulation.
    We could certainly attempt to learn a five-class model, using one class for each
    possible movie rating. There are two problems with this approach:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还考虑为什么这个问题不适合分类模型。我们当然可以尝试学习一个五类模型，每个类别对应一种可能的电影评分。这个方法有两个问题：
- en: The different possible errors are not at all the same. For example, mistaking
    a 5-star movie for a 4-star one is not as serious a mistake as mistaking a 5-star
    movie for a 1-star one.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的错误类型是完全不同的。例如，把一部5星电影误认为4星电影并不是那么严重的错误，而把5星电影误认为1星电影就严重得多。
- en: Intermediate values make sense. Even if our inputs are only integer values,
    it is perfectly meaningful to say that the prediction is 4.3\. We can see that
    this is a different prediction than 3.5, even if they both round to 4.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 中间值是有意义的。即使我们的输入只有整数值，说预测值是4.3也是完全有意义的。我们可以看到，这与3.5的预测是不同的，尽管它们都四舍五入到4。
- en: These two factors together mean that classification is not a good fit to the
    problem. The regression framework is a better fit.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个因素共同表明，分类方法不适合这个问题。回归框架更加适合。
- en: 'For a basic approach, we again have two choices: we can build movie-specific
    or user-specific models. In our case, we are going to first build user-specific
    models. This means that, for each user, we take the movies that the user has rated
    as our target variable. The inputs are the ratings of other users. We hypothesize
    that this will give a high value to users who are similar to our user (or a negative
    value to users who like the same movies that our user dislikes).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基本方法，我们有两种选择：我们可以构建电影特定的模型或用户特定的模型。在我们的例子中，我们将首先构建用户特定的模型。这意味着，对于每个用户，我们将用户已评分的电影作为目标变量。输入是其他用户的评分。我们假设，这样做将为与我们用户相似的用户赋予较高的值（或者为喜欢我们用户不喜欢的电影的用户赋予负值）。
- en: 'Setting up the `train` and `test` matrices is as before (including running
    the normalization steps). Therefore, we jump directly to the learning step. First,
    we instantiate a regressor as follows:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 设置`train`和`test`矩阵与之前一样（包括执行标准化步骤）。因此，我们直接进入学习步骤。首先，我们按如下方式实例化一个回归器：
- en: '[PRE19]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We build a data matrix, which will contain a rating for every (user, movie)
    pair. We initialize it as a copy of the training data:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建一个数据矩阵，矩阵中包含每一对（用户，电影）的评分。我们将其初始化为训练数据的副本：
- en: '[PRE20]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, we iterate over all the users, and each time learn a regression model
    based only on the data that that user has given us:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们遍历所有用户，每次仅基于该用户提供的数据学习回归模型：
- en: '[PRE21]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Evaluating the method can be done exactly as before:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 评估该方法可以像以前一样进行：
- en: '[PRE22]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As before, we can adapt this code to perform movie regression by using the transposed
    matrix.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们可以调整这段代码，使用转置矩阵执行电影回归。
- en: Combining multiple methods
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 结合多种方法
- en: We now combine the aforementioned methods in a single prediction. This seems
    intuitively a good idea, but how can we do this in practice? Perhaps, the first
    thought that comes to mind is that we can average the predictions. This might
    give decent results, but there is no reason to think that all estimated predictions
    should be treated the same. It might be that one is better than others.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将上述方法结合在一起，进行单一预测。直观上看，这个想法是好的，但我们如何在实践中做到这一点呢？也许，第一个想到的办法是我们可以对预测结果进行平均。这可能会给出不错的结果，但并没有理由认为所有的预测结果应该被同等对待。可能某些预测比其他预测更好。
- en: We can try a weighted average, multiplying each prediction by a given weight
    before summing it all up. How do we find the best weights, though? We learn them
    from the data, of course!
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以尝试加权平均，在求和之前将每个预测值乘以一个给定的权重。然而，我们如何找到最佳的权重呢？当然，我们是通过从数据中学习它们！
- en: Tip
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: '**Ensemble learning**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成学习**'
- en: 'We are using a general technique in machine learning, which is not only applicable
    in regression: **ensemble learning**. We learn an ensemble (that is, a set) of
    predictors. Then, we to combine them to obtain a single output. What is interesting
    is that we can see each prediction as being a new feature, and we are now just
    combining features based on training data, which is what we have been doing all
    along. Note that we are doing so for regression here, but the same reasoning is
    applicable to classification: you learn several classifiers, then a master classifier,
    which takes the output of all of them and gives a final prediction. Different
    forms of ensemble learning differ on how you combine the base predictors.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用一种机器学习中的通用技术，这种技术不仅适用于回归问题：**集成学习**。我们学习一个预测器的集成（即一组预测器）。然后，我们将它们组合起来得到一个单一的输出。有趣的是，我们可以将每一个预测视为一个新的特征，我们现在只是根据训练数据组合特征，这正是我们一直在做的事情。请注意，这里我们是在做回归，但相同的思路也适用于分类问题：你学习多个分类器，然后使用一个主分类器，该分类器接受所有分类器的输出并给出最终预测。不同形式的集成学习在于如何组合基础预测器。
- en: 'In order to combine the methods, we will use a technique called **stacked learning**.
    The idea is you learn a set of predictors, then you use the output of these predictors
    as features for another predictor. You can even have several layers, where each
    layer learns by using the output of the previous layer as features for its prediction.
    Have a look at the following diagram:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结合这些方法，我们将使用一种叫做**堆叠学习**的技术。其思路是，你先学习一组预测器，然后将这些预测器的输出作为另一个预测器的特征。你甚至可以有多层，每一层都通过使用上一层的输出作为其预测的特征来进行学习。看看下面的图示：
- en: '![Combining multiple methods](img/2772OS_08_01.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![结合多种方法](img/2772OS_08_01.jpg)'
- en: In order to fit this combination model, we will split the training data into
    two. Alternatively, we could have used cross-validation (the original stacked
    learning model worked like this). However, in this case, we have enough data to
    obtain good estimates by leaving some aside.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了拟合这个组合模型，我们将训练数据分成两部分。或者，我们可以使用交叉验证（原始的堆叠学习模型就是这样工作的）。然而，在这种情况下，我们有足够的数据，通过留出一部分数据来获得良好的估计。
- en: 'As when fitting hyperparameters, though, we need two layers of training/testing
    splits: a first, higher-level split, and then, inside the training split, a second
    split to be able to fit the stacked learner, as show in the following:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在调整超参数时一样，我们需要两个层次的训练/测试划分：一个更高层次的划分，然后在训练划分内部，进行第二次划分，以便拟合堆叠学习器，如下所示：
- en: '[PRE23]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now, we apply the whole process to the testing split and evaluate:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将整个过程应用于测试集并进行评估：
- en: '[PRE24]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Evaluation is as before:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 评估与之前相同：
- en: '[PRE25]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The result of stacked learning is better than what any single method had achieved.
    It is quite typical that combining methods is a simple way to obtain a small performance
    boost, but that the results are not earth shattering.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠学习的结果比任何单一方法的结果都要好。将方法结合起来通常是一种简单的方式，可以获得小幅的性能提升，但结果并不会引起轰动。
- en: By having a flexible way to combine multiple methods, we can simply try any
    idea we wish by adding it into the mix of learners and letting the system fold
    it into the prediction. We can, for example, replace the neighborhood criterion
    in the nearest neighbor code.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过灵活地组合多种方法，我们可以简单地通过将任何想法添加到学习器的混合中，并让系统将其折入预测中来尝试任何我们希望的想法。例如，我们可以替换最近邻代码中的邻域标准。
- en: However, we do have to be careful to not overfit our dataset. In fact, if we
    randomly try too many things, some of them will work well on this dataset, but
    will not generalize. Even though we are splitting our data, we are not rigorously
    cross-validating our design decisions. In order to have a good estimate, and if
    data is plentiful, you should leave a portion of the data untouched until you
    have a final model that is about to go into production. Then, testing your model
    on this held out data gives you an unbiased prediction of how well you should
    expect it to work in the real world.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们必须小心不要让数据集过拟合。事实上，如果我们随意尝试太多东西，其中一些可能在这个数据集上效果很好，但无法推广到其他数据。尽管我们正在划分数据集，但我们并没有严格地进行交叉验证我们的设计决策。为了获得良好的估计，并且如果数据量充足，你应该将一部分数据留着，直到你有一个即将投入生产的最终模型。然后，在这个保留的数据上测试你的模型，可以给你一个无偏的预测，了解它在现实世界中的表现如何。
- en: Basket analysis
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 购物篮分析
- en: The methods we have discussed so far work well when you have numeric ratings
    of how much a user liked a product. This type of information is not always available,
    as it requires active behavior on the part of consumers.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论的方法在你拥有用户对产品喜好程度的数字评分时效果很好。但这种信息并不总是可用，因为它需要消费者的主动行为。
- en: 'Basket analysis is an alternative mode of learning recommendations. In this
    mode, our data consists only of what items were bought together; it does not contain
    any information on whether individual items were enjoyed or not. Even if users
    sometimes buy items they regret, on average, knowing their purchases gives you
    enough information to build good recommendations. It is often easier to get this
    data rather than rating data, as many users will not provide ratings, while the
    basket data is generated as a side effect of shopping. The following screenshot
    shows you a snippet of Amazon.com''s web page for Tolstoy''s classic book *War
    and Peace*, which demonstrates a common way to use these results:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 购物篮分析是一种推荐学习的替代模式。在这种模式下，我们的数据仅包括哪些商品是一起购买的；它不包含任何关于单个商品是否被喜好的信息。即使用户有时购买了他们后悔的商品，平均而言，知道他们的购买记录也足以帮助你构建良好的推荐系统。获取这类数据通常比评分数据更容易，因为许多用户不会提供评分，而购物篮数据则是购物的副作用。以下截图展示了亚马逊网站上托尔斯泰经典小说《战争与和平》的页面片段，演示了如何使用这些结果的常见方式：
- en: '![Basket analysis](img/2772OS_08_02.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![购物篮分析](img/2772OS_08_02.jpg)'
- en: This mode of learning is not only applicable to actual shopping baskets, naturally.
    It is applicable in any setting where you have groups of objects together and
    need to recommend another. For example, recommending additional recipients to
    a user writing an e-mail is done by Gmail and could be implemented using similar
    techniques (we do not know what Gmail uses internally; perhaps, they combine multiple
    techniques, as we did earlier). Or, we could use these methods to develop an app
    to recommend web pages to visit based on your browsing history. Even if we are
    handling purchases, it may make sense to group all purchases by a customer into
    a single basket, independently of whether the items were bought together or on
    separate transactions. This depends on the business context, but keep in mind
    that the techniques are flexible and can be useful in many settings.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这种学习模式不仅仅适用于实际的购物篮，自然也可以应用于任何有一组对象并且需要推荐其他对象的场景。例如，Gmail会向正在写电子邮件的用户推荐额外的收件人，类似的技术可以应用于这种推荐（我们并不知道Gmail内部使用了什么技术；也许，他们像我们之前所做的那样，结合了多种技术）。或者，我们也可以使用这些方法开发一个应用程序，根据用户的浏览历史推荐访问的网页。即使我们处理的是购物，也有可能将顾客的所有购买合并成一个购物篮，而不考虑这些商品是否是一起购买的或是在不同交易中购买的。这取决于商业环境，但请记住，这些技术是灵活的，可以在许多不同的场合中发挥作用。
- en: Note
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Beer and diapers. One of the stories that is often mentioned in the context
    of basket analysis is the *diapers and beer* story. It says that when supermarkets
    first started to look at their data, they found that diapers were often bought
    together with beer. Supposedly, it was the father who would go out to the supermarket
    to buy diapers and then would pick up some beer as well. There has been much discussion
    of whether this is true or just an urban myth. In this case, it seems that it
    is true. In the early 1990s, Osco Drug did discover that in the early evening,
    beer and diapers were bought together, and it did surprise the managers who had,
    until then, never considered these two products to be similar. What is not true
    is that this led the store to move the beer display closer to the diaper section.
    Also, we have no idea whether it was really that fathers were buying beer and
    diapers together more than mothers (or grandparents).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 啤酒和尿布。关于购物篮分析，常常提到的一个故事是*尿布和啤酒*的故事。这个故事说的是，当超市开始分析他们的数据时，他们发现尿布常常和啤酒一起购买。可以假设是父亲去超市买尿布时顺便也买了啤酒。关于这个故事是否真实，还是仅仅一个都市传说，存在很多讨论。在这种情况下，似乎是事实。1990年代初，Osco
    Drug确实发现，在傍晚时分，啤酒和尿布经常一起购买，这让当时的经理们感到非常惊讶，因为他们之前从未认为这两种产品有任何相似性。事实并非如此的是，这并没有导致商店将啤酒陈列架移到尿布区域附近。而且，我们也不知道是否真的是父亲比母亲（或祖父母）更多地购买啤酒和尿布。
- en: Obtaining useful predictions
  id: totrans-114
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取有用的预测
- en: It is not just "customers who bought X also bought Y" even though that is how
    many online retailers phrase it (see the Amazon.com screenshot given earlier);
    a real system cannot work like this. Why not? Because such a system would get
    fooled by very frequently bought items and would simply recommend that which is
    popular without any personalization.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅仅是“购买了X的顾客也购买了Y”，尽管许多在线零售商是这样表述的（参见之前给出的亚马逊截图）；一个真正的系统不能这样运作。为什么不行？因为这样的系统会被非常频繁购买的商品所欺骗，推荐的只是那些流行的商品，而没有任何个性化的推荐。
- en: For example, at a supermarket, many customers buy bread every time they shop
    or close to it (for the sake of argument, let us say that 50 percent of visits
    include bread). So, if you focus on any particular item, say dishwasher soap and
    look at what is frequently bought with dishwasher soap, you might find that bread
    is frequently bought with soap. In fact, just by random chance, 50 percent of
    the times someone buys dishwasher soap, they buy bread. However, bread is frequently
    bought with anything else just because everybody buys bread very often.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在超市中，许多顾客每次购物时都会购买面包，或者购买的时间非常接近（为了说明问题，我们假设有50%的购物包含面包）。因此，如果你关注任何特定商品，比如洗碗液，并查看与洗碗液一起常常购买的商品，你可能会发现面包常常与洗碗液一起购买。事实上，仅仅通过随机机会，每次有人购买洗碗液时，50%的几率他们也会购买面包。然而，面包与其他商品一起购买是因为每个人购买面包的频率都非常高。
- en: What we are really looking for is "customers who bought X, are statistically
    more likely to buy Y than the average customer who has not bought X". If you buy
    dishwasher soap, you are likely to buy bread, but not more so than the baseline.
    Similarly, a bookstore that simply recommended bestsellers no matter which books
    you had already bought would not be doing a good job of personalizing recommendations.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 我们真正寻找的是“购买了X的顾客，比那些没有购买X的普通顾客更有可能购买Y”。如果你购买了洗碗机洗涤剂，你可能会购买面包，但不会比基准更频繁。类似地，一家书店如果只是推荐畅销书，而不考虑你已经购买的书籍，那就不算是很好的个性化推荐。
- en: Analyzing supermarket shopping baskets
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分析超市购物篮
- en: As an example, we will look at a dataset consisting of anonymous transactions
    at a supermarket in Belgium. This dataset was made available by Tom Brijs at Hasselt
    University. Due to privacy concerns, the data has been anonymized, so we only
    have a number for each product and a basket is a set of numbers. The data file
    is available from several online sources (including this book's companion website).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 作为例子，我们将看一个包含比利时超市匿名交易的数据集。这个数据集由哈瑟尔特大学的Tom Brijs提供。由于隐私问题，数据已经被匿名化，因此我们只有每个产品的编号，购物篮是由一组编号组成。该数据文件可以从多个在线来源获得（包括本书的伴随网站）。
- en: 'We begin by loading the dataset and looking at some statistics (this is always
    a good idea):'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载数据集并查看一些统计信息（这总是个好主意）：
- en: '[PRE26]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can see the resulting counts summarized in the following table:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下表格中看到总结的结果：
- en: '| # of times bought | # of products |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 购买次数 | 产品数量 |'
- en: '| --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Just once | 2,224 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 仅购买一次 | 2,224 |'
- en: '| 2 or 3 | 2,438 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 2或3 | 2,438 |'
- en: '| 4 to 7 | 2,508 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 4到7 | 2,508 |'
- en: '| 8 to 15 | 2,251 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 8到15 | 2,251 |'
- en: '| 16 to 31 | 2,182 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 16到31 | 2,182 |'
- en: '| 32 to 63 | 1,940 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 32到63 | 1,940 |'
- en: '| 64 to 127 | 1,523 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 64到127 | 1,523 |'
- en: '| 128 to 511 | 1,225 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 128到511 | 1,225 |'
- en: '| 512 or more | 179 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| 512次或更多 | 179 |'
- en: There are many products that have only been bought a few times. For example,
    33 percent of products were bought four or fewer times. However, this represents
    only 1 percent of purchases. This phenomenon that many products are only purchased
    a small number of times is sometimes labeled *the long tail* and has only become
    more prominent as the Internet made it cheaper to stock and sell niche items.
    In order to be able to provide recommendations for these products, we would need
    a lot more data.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多产品只被购买了少数几次。例如，33%的产品购买次数为四次或更少。然而，这仅占所有购买的1%。这种许多产品仅被购买少数次的现象有时被称为*长尾现象*，随着互联网使得库存和销售小众商品变得更加便宜，这一现象变得更加显著。为了能够为这些产品提供推荐，我们需要更多的数据。
- en: There are a few open source implementations of basket analysis algorithms out
    there, but none that are well integrated with scikit-learn or any of the other
    packages we have been using. Therefore, we are going to implement one classic
    algorithm ourselves. This algorithm is called the Apriori algorithm, and it is
    a bit old (it was published in 1994 by Rakesh Agrawal and Ramakrishnan Srikant),
    but it still works (algorithms, of course, never stop working, they just get superceded
    by better ideas).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些开源的购物篮分析算法实现，但没有一个与scikit-learn或我们一直在使用的其他软件包很好地集成。因此，我们将自己实现一个经典的算法。这个算法叫做Apriori算法，虽然它有点老（由Rakesh
    Agrawal和Ramakrishnan Srikant于1994年发布），但它仍然有效（算法当然永远有效，只是会被更好的想法所取代）。
- en: Formally, the Apriori algorithm takes a collection of sets (that is, your shopping
    baskets) and returns sets that are very frequent as subsets (that is, items that
    together are part of many shopping baskets).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，Apriori算法接收一个集合（即你的购物篮），并返回作为子集非常频繁的集合（即共同出现在许多购物篮中的商品）。
- en: 'The algorithm works using a bottom-up approach: starting with the smallest
    candidates (those composed of one single element), it builds up, adding one element
    at a time. Formally, the algorithm takes a set of baskets and the minimum input
    that should be considered (a parameter we will call minsupport). The first step
    is to consider all baskets with just one element with minimal support. Then, these
    are combined in all possible ways to build up two-element baskets. These are filtered
    in order to keep only those that have minimal support. Then, all possible three-element
    baskets are considered and those with minimal support are kept, and so on. The
    trick of Apriori is that when building a larger basket, *it only needs to consider
    those that are built up of smaller sets*.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法采用自下而上的方法：从最小的候选项集（由一个单独的元素组成）开始，逐步构建，每次添加一个元素。正式来说，算法接受一个购物篮集和应考虑的最小输入（我们称之为
    minsupport）。第一步是考虑所有仅包含一个元素且具有最小支持度的购物篮。然后，这些项集以所有可能的方式组合，构建出二元素购物篮。接着，筛选出仅保留那些具有最小支持度的项集。然后，考虑所有可能的三元素购物篮，并保留那些具有最小支持度的项集，如此继续。Apriori
    的技巧在于，当构建一个更大的购物篮时，*它只需要考虑由更小的集合构成的购物篮*。
- en: 'The following diagram presents a schematic view of the algorithm:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图示展示了该算法的示意图：
- en: '![Analyzing supermarket shopping baskets](img/2772OS_08_04.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![分析超市购物篮](img/2772OS_08_04.jpg)'
- en: 'We shall now implement this algorithm in code. We need to define the minimum
    support we are looking for:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在代码中实现这个算法。我们需要定义我们寻找的最小支持度：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Support is the number of times a set of products was purchased together. The
    goal of Apriori is to find itemsets with high support. Logically, any itemset
    with more than minimal support can only be composed of items which themselves
    have at least minimal support:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 支持度是指一组产品一起被购买的次数。Apriori 的目标是找到具有高支持度的项集。从逻辑上讲，任何支持度高于最小支持度的项集只能由那些本身至少具有最小支持度的项组成：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Our initial itemsets are singletons (sets with a single element). In particular,
    all singletons that have at least minimal support are frequent itemsets:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的初始项集是单一项集（包含单个元素的集合）。特别地，所有至少具有最小支持度的单一项集都是频繁项集：
- en: '[PRE29]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now, our loop is given as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的循环如下所示：
- en: '[PRE30]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This works correctly, but it is slow. A better implementation has more infrastructure
    to avoid having to loop over all the datasets to get the count (`support_c`).
    In particular, we keep track of which shopping baskets have which frequent itemsets.
    This accelerates the loop but makes the code harder to follow. Therefore we will
    not show it here. As usual, you can find both the implementations on this book's
    companion website. The code there is also wrapped into a function that can be
    applied to other datasets.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法是正确的，但比较慢。一个更好的实现有更多的基础设施来避免遍历所有数据集来获取计数（`support_c`）。特别地，我们跟踪哪些购物篮包含哪些频繁项集。这样可以加速循环，但也使代码更难理解。因此，我们在这里不展示它。像往常一样，您可以在本书的配套网站上找到这两种实现。网站上的代码也被封装成一个函数，可以应用于其他数据集。
- en: The Apriori algorithm returns frequent itemsets, that is, baskets that are present
    above a certain threshold (given by the `minsupport` variable in the code).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Apriori 算法返回频繁项集，即那些出现频率超过某一阈值（由代码中的`minsupport`变量给定）的购物篮。
- en: Association rule mining
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关联规则挖掘
- en: Frequent itemsets are not very useful by themselves. The next step is to build
    **association rules**. Because of this final goal, the whole field of basket analysis
    is sometimes called association rule mining.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 频繁项集本身并不是很有用。下一步是构建**关联规则**。由于这个最终目标，整个购物篮分析领域有时被称为关联规则挖掘。
- en: 'An association rule is a statement of the type "If X, then Y", for example,
    "if a customer bought War and Peace, then they will buy Anna Karenina". Note that
    the rule is not deterministic (not all customers who buy X will buy Y), but it
    is rather cumbersome to always spell it out: "if a customer bought X, he is more
    likely than baseline to buy Y"; thus, we say "if X, then Y", but we mean it in
    a probabilistic sense.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则是一种“如果 X，则 Y”的语句，例如，“如果顾客购买了《战争与和平》，那么他们将购买《安娜·卡列尼娜》”。请注意，这条规则并不是确定性的（并不是所有购买
    X 的顾客都会购买 Y），但每次都要这样表达是很繁琐的：“如果顾客购买了 X，他购买 Y 的可能性比基线高”；因此，我们说“如果 X，则 Y”，但我们是从概率的角度来理解这句话。
- en: 'Interestingly, both the antecedent and the conclusion may contain multiple
    objects: costumers who bought X, Y, and Z also bought A, B, and C. Multiple antecedents
    may allow you to make more specific predictions than are possible from a single
    item.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，前件和结论可能都包含多个对象：购买了X、Y和Z的顾客也购买了A、B和C。多个前件可能使你能够做出比单个项目更具体的预测。
- en: 'You can get from a frequent set to a rule by just trying all the possible combinations
    of X implies Y. It is easy to generate many of these rules. However, you only
    want to have valuable rules. Therefore, we need to measure the value of a rule.
    A commonly used measure is called the **lift**. The lift is the ratio between
    the probability obtained by applying the rule and the baseline, as in the following
    formula:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过尝试所有可能的X蕴含Y的组合，从频繁项集生成规则。生成这些规则很容易。然而，你只希望得到有价值的规则。因此，我们需要衡量规则的价值。一个常用的度量叫做**提升度**。提升度是应用规则所得到的概率与基准概率之间的比值，公式如下：
- en: '![Association rule mining](img/2772OS_08_05.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![关联规则挖掘](img/2772OS_08_05.jpg)'
- en: In the preceding formula, P(Y) is the fraction of all the transactions that
    include Y, while P(Y|X) is the fraction of transactions that include Y, given
    that they also include X. Using the lift helps avoid the problem of recommending
    bestsellers; for a bestseller, both P(Y) and P(Y|X) will be large. Therefore,
    the lift will be close to one and the rule will be deemed irrelevant. In practice,
    we wish to have values of lift of at least 10, perhaps even 100.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述公式中，P(Y)表示包含Y的所有交易所占的比例，而P(Y|X)表示在交易包含X的前提下，包含Y的交易所占的比例。使用提升度可以避免推荐畅销书的问题；对于畅销书，P(Y)和P(Y|X)都将较大。因此，提升度会接近1，规则将被视为无关紧要。实际上，我们希望提升度的值至少为10，甚至可能达到100。
- en: 'Refer to the following code:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下代码：
- en: '[PRE31]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Some of the results are shown in the following table. The counts are the number
    of transactions which include the **consequent alone** (that is, the base rate
    at which that product is bought), **all the items in the antecedent**, and **all
    the items in the antecedent and the consequent**.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格展示了部分结果。计数是指包含**仅后件**（即该商品被购买的基本比率）、**前件中的所有项**以及**前件和后件中的所有项**的交易数量。
- en: '| Antecedent | Consequent | Consequent count | Antecedent count | Antecedent
    & consequent count | Lift |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 前件 | 后件 | 后件计数 | 前件计数 | 前件和后件计数 | 提升度 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 1,378, 1,379, 1,380 | 1,269 | 279 (0.3 percent) | 80 | 57 | 225 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 1,378, 1,379, 1,380 | 1,269 | 279（0.3%） | 80 | 57 | 225 |'
- en: '| 48, 41, 976 | 117 | 1026 (1.1 percent) | 122 | 51 | 35 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 48, 41, 976 | 117 | 1026（1.1%） | 122 | 51 | 35 |'
- en: '| 48, 41, 1,6011 | 16,010 | 1316 (1.5 percent ) | 165 | 159 | 64 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 48, 41, 1,6011 | 16,010 | 1316（1.5%） | 165 | 159 | 64 |'
- en: We can see, for example, that there were 80 transactions in which 1,378, 1,379,
    and 1,380 were bought together. Of these, 57 also included 1,269, so the estimated
    conditional probability is 57/80 ≈ 71 percent. Compared to the fact that only
    0.3 percent of all transactions included 1,269, this gives us a lift of 255.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以看到，有80笔交易中购买了1,378、1,379和1,380三个商品。在这些交易中，57笔也包含了1,269，因此估算的条件概率为57/80
    ≈ 71%。与所有交易中只有0.3%包含1,269这一事实相比，这给我们带来了255的提升度。
- en: The need to have a decent number of transactions in these counts in order to
    be able to make relatively solid inferences is why we must first select frequent
    itemsets. If we were to generate rules from an infrequent itemset, the counts
    would be very small; due to this, the relative values would be meaningless (or
    subject to very large error bars).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在这些计数中拥有足够数量的交易，以便能够做出相对可靠的推断，这就是为什么我们必须首先选择频繁项集的原因。如果我们从不频繁项集生成规则，计数会非常小；因此，相关值将毫无意义（或者会受到非常大的误差范围的影响）。
- en: 'Note that there are many more association rules discovered from this dataset:
    the algorithm discovers 1,030 rules (requiring support for the baskets of at least
    80 and a minimum lift of 5). This is still a small dataset when compared to what
    is now possible with the web. With datasets containing millions of transactions,
    you can expect to generate many thousands of rules, even millions.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，从这个数据集中发现了更多的关联规则：该算法发现了1,030条规则（要求支持至少80个购物篮，并且最小提升度为5）。与如今互联网所能处理的数据集相比，这仍然是一个较小的数据集。对于包含数百万笔交易的数据集，你可以预计会生成成千上万的规则，甚至是数百万条规则。
- en: However, for each customer or each product, only a few rules will be relevant
    at any given time. So each costumer only receives a small number of recommendations.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于每个客户或每个产品，只有少数几个规则在任何给定时刻是相关的。因此，每个客户只会收到少量推荐。
- en: More advanced basket analysis
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更高级的购物篮分析
- en: There are now other algorithms for basket analysis that run faster than Apriori.
    The code we saw earlier was simple and it was good enough for us, as we only had
    circa 100 thousand transactions. If we had many millions, it might be worthwhile
    to use a faster algorithm. Note, though, that learning association rules can often
    be done offline, where efficiency is not as great a concern.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一些比Apriori更快的购物篮分析算法。我们之前看到的代码比较简单，足够满足我们的需求，因为我们只有大约10万个交易记录。如果我们有数百万条交易记录，可能值得使用更快的算法。不过需要注意的是，学习关联规则通常可以离线进行，在这种情况下效率不是那么大的问题。
- en: There are also methods to work with temporal information, leading to rules that
    take into account the order in which you have made your purchases. Consider, as
    an example, that someone buying supplies for a large party may come back for trash
    bags. Thus, it may make sense to propose trash bags on the first visit. However,
    it would not make sense to propose party supplies to everyone who buys a trash
    bag.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一些方法可以处理时间序列信息，从而得出考虑到购买顺序的规则。例如，假设某人购买了大量派对用品后，可能会回来购买垃圾袋。因此，在第一次访问时推荐垃圾袋可能是有意义的。然而，向所有购买垃圾袋的人推荐派对用品就不太合适了。
- en: Summary
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started by using regression for rating predictions. We saw
    a couple of different ways in which to do so, and then combined them all in a
    single prediction by learning a set of weights. This technique, ensemble learning,
    in particular stacked learning, is a general technique that can be used in many
    situations, not just for regression. It allows you to combine different ideas
    even if their internal mechanics are completely different; you can combine their
    final outputs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们从使用回归进行评分预测开始。我们看到了一些不同的方式来实现这一目标，然后通过学习一组权重将它们结合成一个单一的预测。这种技术，特别是堆叠学习，作为一种集成学习方法，是一种可以在许多情况下使用的通用技术，不仅仅适用于回归。它允许你结合不同的思路，即使它们的内部机制完全不同；你也可以将它们的最终输出结合起来。
- en: 'In the second half of the chapter, we switched gears and looked at another
    mode of producing recommendations: shopping basket analysis or association rule
    mining. In this mode, we try to discover (probabilistic) association rules of
    the "customers who bought X are likely to be interested in Y" form. This takes
    advantage of the data that is generated from sales alone without requiring users
    to numerically rate items. This is not available in scikit-learn at this moment,
    so we wrote our own code.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后半部分，我们转变了方向，研究了另一种产生推荐的模式：购物篮分析或关联规则挖掘。在这种模式下，我们试图发现“购买X的客户很可能对Y感兴趣”的（概率性）关联规则。这利用了仅从销售数据生成的数据，而无需用户对商品进行数值评分。目前，scikit-learn中没有这个功能，所以我们编写了自己的代码。
- en: Association rule mining needs to be careful to not simply recommend bestsellers
    to every user (otherwise, what is the point of personalization?). In order to
    do this, we learned about measuring the value of rules in relation to the baseline,
    using a measure called the lift of a rule.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 关联规则挖掘需要小心，不能只是向每个用户推荐畅销书（否则，个性化推荐还有什么意义？）。为了做到这一点，我们学习了如何通过称为“规则提升度”的度量，衡量规则相对于基线的价值。
- en: 'At this point in the book, we have seen the major modes of machine learning:
    classification. In the next two chapters, we will look at techniques used for
    two specific kinds of data, music and images. Our first goal will be to build
    a music genre classifier.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了机器学习的主要模式：分类。在接下来的两章中，我们将探索用于两种特定数据类型的技术，即音乐和图像。我们的第一个目标是构建一个音乐类型分类器。
