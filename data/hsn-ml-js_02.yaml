- en: Data Exploration
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据探索
- en: The single most important thing for a beginner to know about **machine learning**
    (**ML**) is that *machine learning is not magic*. Taking a large dataset and naively
    applying a neural network to it will not automatically give you earth-shaking
    insights. ML is built on top of sound and familiar mathematical principles, such
    as probability, statistics, linear algebra, and vector calculus—voodoo not included
    (though some readers may liken vector calculus to voodoo)!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 对于初学者来说，关于**机器学习**（**ML**）最重要的认识是，*机器学习不是魔法*。将大量数据集拿过来，天真地应用神经网络，并不会自动给你带来震撼的见解。机器学习建立在坚实且熟悉的数学原理之上，如概率、统计学、线性代数和向量微积分——不包括巫术（尽管一些读者可能会把向量微积分比作巫术）！
- en: 'We will be covering the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: An overview
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概述
- en: Variable identification
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量识别
- en: Cleaning of data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清洗
- en: Transformation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换
- en: Types of analysis
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析类型
- en: Missing values treatment
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值处理
- en: Outlier treatment
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值处理
- en: An overview
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: One misconception I would like to dispel early on is that implementing the ML
    algorithm itself is the bulk of the work you'll need to do to accomplish some
    task. If you're new to this, you may be under the impression that 95% of your
    time should be spent on implementing a neural network, and that the neural network
    is solely responsible for the results you get. Build a neural network, put data
    in, magically get results out. What could be easier?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望尽早澄清的一个误解是，实现机器学习算法本身是完成某些任务所需工作的主要部分。如果你是新手，你可能会有这样的印象，即你应该花费95%的时间来实现神经网络，并且神经网络完全负责你得到的结果。构建一个神经网络，放入数据，神奇地得到结果。还有什么比这更容易的吗？
- en: 'The reality of ML is that the algorithm you use is only as good as the data
    you put into it. Furthermore, the results you get are only as good as your ability
    to process and interpret them. The age-old computer science acronym **GIGO** fits
    well here: *Garbage In*, *Garbage Out*.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的现实是，你使用的算法只和你放入的数据一样好。此外，你得到的结果只和你处理和解释它们的能力一样好。古老的计算机科学缩写词**GIGO**（垃圾输入，垃圾输出）在这里非常适用：*垃圾输入*，*垃圾输出*。
- en: When implementing ML techniques, you must also pay close attention to their
    preprocessing and postprocessing of data. Data preprocessing is required for many
    reasons, and is the focus of this chapter. Postprocessing relates to your interpretation
    of the algorithm's output, whether your confidence in the algorithm's result is
    high enough to take action on it, and your ability to apply the results to your
    business problem. Since postprocessing of results strongly depends on the algorithm
    in question, we'll address postprocessing considerations as they come up in our
    specific examples throughout this book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现机器学习技术时，你还必须密切关注它们的预处理和后处理。数据预处理需要很多原因，这也是本章的重点。后处理与你对算法输出的解释有关，无论是你对算法结果的信心是否足够高，以至于可以采取行动，以及你将结果应用于业务问题的能力。由于结果的后处理强烈依赖于所讨论的算法，因此我们将根据本书中的具体示例来讨论后处理考虑事项。
- en: Preprocessing of data, like postprocessing of data, often depends on the algorithm
    used, as different algorithms have different requirements. One straightforward
    example is image processing with **Convolutional Neural Networks** (**CNNs**),
    covered in a later chapter. All images processed by a single CNN are expected
    to have the same dimensions, or at least the same number of pixels and the same
    number of color channels (RGB versus RGBA versus grayscale, and so on). The CNN
    was configured to expect a specific number of inputs, and so every image you give
    to it must be preprocessed to make sure it complies with the neural network's
    expectations. You may need to resize, scale, crop, or pad input images before
    feeding them to the network. You may need to convert color images to grayscale.
    You may need to detect and remove images that have been corrupted from your dataset.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据预处理，就像数据后处理一样，通常取决于所使用的算法，因为不同的算法有不同的要求。一个直接的例子是图像处理，使用**卷积神经网络**（**CNNs**），这在后面的章节中会有介绍。所有由单个CNN处理的图像都应具有相同的尺寸，或者至少具有相同数量的像素和相同数量的颜色通道（RGB与RGBA与灰度等）。CNN被配置为期望特定的输入数量，因此你给它的每一张图像都必须进行预处理，以确保它符合神经网络的要求。在将图像输入网络之前，你可能需要调整大小、缩放、裁剪或填充输入图像。你可能需要将彩色图像转换为灰度图像。你可能需要检测并从你的数据集中移除损坏的图像。
- en: 'Some algorithms simply won''t work if you attempt to give them the wrong input.
    If a CNN expects 10,000 grayscale pixel intensity inputs (namely an image that''s
    100 x 100 pixels), there''s no way you can give it an image that''s sized 150
    x 200\. This is a best-case scenario for us: the algorithm fails loudly, and we
    are able to change our approach before attempting to use our network.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法如果输入错误的数据，根本就无法工作。如果一个卷积神经网络（CNN）期望接收10,000个灰度像素强度输入（即一个100 x 100像素的图像），那么你不可能给它一个150
    x 200像素大小的图像。这是我们最好的情况：算法会大声失败，我们能够在尝试使用我们的网络之前改变我们的方法。
- en: 'Other algorithms, however, will fail silently if you give them bad input. The
    algorithm will appear to be working, and even give you results that look reasonable
    but are actually wholly inaccurate. This is our worst-case scenario: we think
    the algorithm is working as expected, but in reality we''re in a GIGO situation.
    Just think about how long it will take you to discover that the algorithm is actually
    giving you nonsensical results. How many bad business decisions have you made
    based on incorrect analysis or poor data? These are the types of situations we
    must avoid, and it all starts at the beginning: making sure the data we use is
    appropriate for the application.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，其他算法如果输入错误的数据，可能会无声无息地失败。算法看起来似乎在工作，甚至给出看似合理的但实际完全错误的结果。这是我们最坏的情况：我们以为算法按预期工作，但实际上我们陷入了垃圾进垃圾出（GIGO）的情况。想想看，你需要花多长时间才能发现算法实际上给你的是无意义的输出。你基于错误的分析或糟糕的数据做出了多少不良的商业决策？我们必须避免这些情况，而这一切都始于开始：确保我们使用的数据适合应用。
- en: Most ML algorithms make assumptions about the data they process. Some algorithms
    expect data to be of a given size and shape (as in neural networks), some algorithms
    expect data to be bucketed, some algorithms expect data to be normalized over
    a range (between 0 and 1 or between -1 and +1), some algorithms are resilient
    to missing values and others aren't. It is ultimately your responsibility to understand
    what assumptions the algorithm makes about your data, and also to align the data
    with the expectations of the algorithm.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数机器学习（ML）算法对其处理的数据做出假设。一些算法期望数据具有特定的尺寸和形状（如神经网络），一些算法期望数据被分类，一些算法期望数据在某个范围内归一化（在0到1或-1到+1之间），一些算法对缺失值有弹性，而另一些则没有。最终，你的责任是理解算法对你数据的假设，并将数据与算法的期望相匹配。
- en: 'For the most part, the aforementioned relates to the format, shape, and size
    of data. There is another consideration: the quality of the data. A data point
    may be perfectly formatted and aligned with the expectations of an algorithm,
    but still be *wrong.* Perhaps someone wrote down the wrong value for a measurement,
    maybe there was an instrumentation failure, or maybe some environmental effect
    has contaminated or tainted your data. In these cases the format, shape, and size
    may be correct, but the data itself may harm your model and prevent it from converging
    on a stable or accurate result. In many of these cases, the data point in question
    is an **outlier**, or a data point that doesn''t seem to fit within the set.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分上述内容都与数据的格式、形状和大小有关。还有另一个考虑因素：数据的质量。一个数据点可能格式正确，并且与算法的期望相匹配，但仍然可能是错误的。也许有人记录了一个错误的测量值，也许有仪器故障，或者可能某些环境效应已经污染或损害了你的数据。在这些情况下，格式、形状和大小可能正确，但数据本身可能会损害你的模型，并阻止它收敛到一个稳定或准确的结果。在这些情况中，有问题的数据点可能是一个**异常值**，或者是一个似乎不适用于集合的数据点。
- en: Outliers exist in real life, and are often valid data. It's not always apparent
    by looking at the data by itself whether an outlier is valid or not, and we must
    also consider the context and algorithm when determining how to handle the data.
    For instance, let's say you're running a meta-analysis that relates patients'
    height to their heart performance and you've got 100 medical records available
    to analyze. One of the patients is listed with a height of 7'3" (221 cm). Is this
    a typo? Did the person who recorded the data actually mean 6'3" (190 cm)? What
    are the odds that, of only 100 random individuals, one of them is actually that
    tall? Should you still use this data point in your analysis, even though it will
    skew your otherwise very clean-looking results? What if the sample size were 1
    million records instead of only 100? In that case, it's much more likely that
    you did actually select a very tall person. What if the sample size were only
    100, but they were all NBA players?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值在现实生活中存在，通常是有效数据。仅凭观察数据本身，我们往往无法确定异常值是否有效，我们还需要考虑上下文和算法来确定如何处理数据。例如，假设您正在进行一项元分析，将患者的身高与他们的心脏功能联系起来，并且您有100份医疗记录可供分析。其中一位患者的身高被记录为7'3"（221厘米）。这是否是一个打字错误？记录数据的人实际上是否意味着6'3"（190厘米）？在只有100个随机个体的情况下，其中一个人实际上那么高的可能性有多大？即使这会扭曲您原本看起来非常干净的结果，您是否仍然应该使用这个数据点进行分析？如果样本量是100万条记录而不是只有100条呢？在这种情况下，您实际上确实选择了一个非常高的人的可能性就更大了。如果样本量只有100，但他们都是NBA球员呢？
- en: As you can see, dealing with outliers is not straightforward. You should always
    be hesitant to discard data, especially if in doubt. By discarding data, you run
    the risk of creating a self-fulfilling prophecy by which you've consciously or
    subconsciously selected only the data that will support your hypothesis, even
    if your hypothesis is wrong. On the other hand, using legitimately bad data can
    ruin your results and prevent progress.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，处理异常值并不简单。您应该始终谨慎对待删除数据，尤其是在有疑问的情况下。通过删除数据，您可能会无意中创造出一个自我实现的预言，即您有意识地或无意识地只选择了支持您假设的数据，即使您的假设是错误的。另一方面，使用不合法的坏数据可能会毁掉您的结果并阻碍进步。
- en: In this chapter, we will discuss a number of different considerations you must
    make when preprocessing data, including data transformations, handling missing
    data, selecting the correct parameters, handling outliers, and other forms of
    analysis that will be helpful in the data preprocessing stage.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在数据预处理阶段必须考虑的多个不同因素，包括数据转换、处理缺失数据、选择正确的参数、处理异常值以及其他有助于数据预处理阶段的分析形式。
- en: Feature identification
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征识别
- en: 'Imagine that you are responsible for placing targeted product advertisements
    on an e-commerce store that you help run. The goal is to analyze a visitor''s
    past shopping trends and select products to display that will increase the shopper''s
    likelihood to make a purchase. Given then the gift of foresight, you''ve been
    collecting 50 different metrics on all of your shoppers for months: you''ve been
    recording past purchases, the product categories of those purchases, the price
    tag on each purchase, the time on site each user spent before making a purchase,
    and so on.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，您负责在一个您帮助运营的电子商务网站上放置目标产品广告。目标是分析访客过去的购物趋势，并选择展示的产品以提高购物者购买的可能性。鉴于您拥有先见之明，您已经收集了数月来所有购物者的50个不同指标：您记录了过去的购买，这些购买的产品类别，每次购买的标价，用户在购买前在网站上的停留时间等等。
- en: Believing that ML is a silver bullet, believing that more data is better, and
    believing that more training of your model is better, you load all 50 dimensions
    of data into an algorithm and train it for days on end. When testing your algorithm
    you find that its accuracy is very high when evaluating data points that you've
    trained the algorithm on, but also find that the algorithm fails spectacularly
    when evaluating against your validation set. Additionally, the model has taken
    a very long time to train. What went wrong here?
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 认为机器学习是一剂万能药，认为数据越多越好，以及认为对模型进行更多训练越好，您将所有50个维度的数据加载到算法中，并连续训练了数天。当测试您的算法时，您发现它在评估您训练的数据点时准确性非常高，但同时也发现当评估验证集时，算法表现糟糕。此外，模型训练时间非常长。这里出了什么问题？
- en: 'First, you''ve made the assumption that all of your 50 dimensions of data are
    relevant to the task at hand. It turns out that not all data is relevant. ML is
    great at finding patterns within data, but not all data actually contains patterns.
    Some data is random, and other data is not random but is also uninteresting. One
    example of uninteresting data that fits a pattern might be the time of day that
    the shopper is browsing your site on: users can only shop while they''re awake,
    so most of your users shop between 7 a.m. and midnight. This data obviously follows
    a pattern, but may not actually affect the user''s purchase intent. Of course,
    there may indeed be an interesting pattern: perhaps night owls tend to make late-night
    impulse purchases—but maybe not.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你假设你所有的50个数据维度都与当前任务相关。结果证明并非所有数据都相关。机器学习擅长在数据中找到模式，但并非所有数据实际上都包含模式。一些数据是随机的，而其他数据虽然不是随机的，但也不有趣。一个符合模式但无趣的数据例子可能是购物者在你的网站上浏览的时间：用户只能在清醒时购物，所以大多数用户在早上7点到午夜之间购物。这些数据显然遵循一个模式，但可能实际上并不影响用户的购买意图。当然，确实可能存在一个有趣的模式：也许夜猫子倾向于在深夜进行冲动购物——但也许不是。
- en: 'Second, using all 50 dimensions and training your model for a long period of
    time may cause overfitting of your model: instead of being able to generalize
    behavioral patterns and making shopping predictions, your overfitted model is
    now very good at identifying that a certain behavior represents Steve Johnson
    (one specific shopper), rather than generalizing Steve''s behavior into a widely
    applicable trend. This overfit was caused by two factors: the long training time
    and the existence of irrelevant data in the training set. If one of the dimensions
    you''ve recorded is largely random and you spend a lot of time training a model
    on that data, the model may end up using that random data as an identifier for
    a user rather than filtering it out as a non-trend. The model may learn that, when
    the user''s time on site is exactly 182 seconds, they will purchase a product
    worth $120, simply because you''ve trained the model on that data point many thousands
    of times in the training process.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，使用所有50个维度并长时间训练你的模型可能会导致模型过拟合：你的过拟合模型现在非常擅长识别某种行为代表史蒂夫·约翰逊（一个特定的购物者），而不是将史蒂夫的行为归纳为一个广泛适用的趋势。这种过拟合是由两个因素造成的：长时间的训练时间和训练集中存在无关数据。如果你记录的一个维度大部分是随机的，并且你花了大量时间在这个数据上训练模型，那么模型最终可能会将那些随机数据作为用户的标识符，而不是将其过滤掉作为非趋势。模型可能会学习到，当用户在网站上的时间是正好182秒时，他们会购买价值120美元的产品，仅仅是因为你在训练过程中多次在训练数据点上训练了模型。
- en: 'Let''s consider a different example: face identification. You''ve got thousands
    of photos of peoples'' faces and want to be able to analyze a photo and determine
    who the subject is. You train a CNN on your data, and find that the accuracy of
    your algorithm is quite low, only being able to correctly identify the subject
    60% of the time. The problem here may be that your CNN, working with raw pixel
    data, has not been able to automatically identify the features of a face that
    actually matter. For instance, Sarah Jane always takes her selfies in her kitchen,
    and her favorite spatula is always on display in the background. Any other user
    who also happens to have a spatula in the picture may be falsely identified as
    Sarah Jane, even if their faces are quite different. The data has overtrained
    the neural network to recognize spatulas as Sarah Jane, rather than actually looking
    at the user''s face.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个不同的例子：人脸识别。你拥有成千上万张人们的面孔照片，并希望能够分析一张照片并确定主题人物是谁。你在自己的数据上训练了一个卷积神经网络（CNN），发现你的算法准确率相当低，只有60%的时间能够正确识别主题人物。这里的问题可能在于你的CNN在处理原始像素数据时，未能自动识别真正重要的面部特征。例如，莎拉·简总是在她厨房里拍自拍，她最喜欢的勺子总是放在背景中展示。任何其他恰好也在照片中有勺子的人可能会被错误地识别为莎拉·简，即使他们的面孔相当不同。数据已经过度训练了神经网络，使其将勺子识别为莎拉·简，而不是真正查看用户的脸部。
- en: 'In both of these examples, the problem starts with insufficient preprocessing
    of data. In the e-commerce store example, you have not correctly identified the
    features of a shopper that actually matter, and so have trained your model with
    a lot of irrelevant data. The same problem exists in the face detection example:
    not every pixel in the photograph represents a person or their features, and in
    seeing a reliable pattern of spatulas the algorithm has learned that Sarah Jane
    is a spatula.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两个例子中，问题始于数据预处理不足。在电子商务商店的例子中，你没有正确识别出真正重要的购物者特征，因此用大量无关数据训练了你的模型。在人脸检测的例子中，也存在相同的问题：照片中的每个像素并不代表一个人或其特征，算法在看到可靠的勺子模式后，学会了Sarah
    Jane是勺子。
- en: To solve both of these problems, you will need to make better selections of
    the features that you give to your ML model. In the e-commerce example, it may
    turn out that only 10 of your 50 recorded dimensions are relevant, and to fix
    the problem you must identify what those 10 dimensions are and only use those
    when training your model. In the face detection example, perhaps the neural network
    should not receive raw pixel intensity data but instead facial dimensions such
    as *nose bridge length*, *mouth width*, *distance between pupils*, *distance between
    pupil and eyebrow*, *distance between earlobes*, *distance from chin to hairline*,
    and so on. Both of these examples demonstrate the need to select the most relevant
    and appropriate features of your data. Making the appropriate selection of features
    will serve to improve both the speed and accuracy of your model.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这两个问题，你需要更好地选择提供给你的机器学习模型的特征。在电子商务的例子中，可能只有你记录的50个维度中的10个是相关的，为了解决这个问题，你必须确定这10个维度是什么，并且只在训练模型时使用这些维度。在人脸检测的例子中，也许神经网络不应该接收原始像素强度数据，而应该接收面部维度，例如**鼻梁长度**、**嘴巴宽度**、**瞳孔间距**、**瞳孔与眉毛之间的距离**、**耳垂间距**、**下巴到发际线的距离**等等。这两个例子都说明了选择数据中最相关和适当特征的需要。适当选择特征将有助于提高模型的速度和准确性。
- en: The curse of dimensionality
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 维度诅咒
- en: In ML applications, we often have high-dimensional data. If we're recording
    50 different metrics for each of our shoppers, we're working in a space with 50
    dimensions. If we're analyzing grayscale images sized 100 x 100, we're working
    in a space with 10,000 dimensions. If the images are RGB-colored, the dimensionality
    increases to 30,000 dimensions (one dimension for each color channel in each pixel
    in the image)!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习应用中，我们经常处理高维数据。如果我们为每位购物者记录50个不同的指标，我们就在一个50维的空间中工作。如果我们分析100 x 100像素的灰度图像，我们就在一个10,000维的空间中工作。如果图像是RGB彩色，维度将增加到30,000维（图像中每个像素的每个颜色通道都是一个维度）！
- en: This problem is called the **curse of dimensionality**. On one hand, ML excels
    at analyzing data with many dimensions. Humans are not good at finding patterns
    that may be spread out across so many dimensions, especially if those dimensions
    are interrelated in counter-intuitive ways. On the other hand, as we add more
    dimensions we also increase the processing power we need to analyze the data,
    and we also increase the amount of training data required to make meaningful models.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题被称为**维度诅咒**。一方面，机器学习擅长分析具有许多维度的数据。人类不擅长在如此多的维度中找到可能分布的模式，尤其是如果这些维度以反直觉的方式相互关联。另一方面，随着我们添加更多维度，我们也需要更多的处理能力来分析数据，并且我们也需要更多的训练数据来构建有意义的模型。
- en: One area that clearly demonstrates the curse of dimensionality is **natural
    language processing** (**NLP**). Imagine you are using a Bayesian classifier to
    perform sentiment analysis of tweets relating to brands or other topics. As you
    will learn in a later chapter, part of data preprocessing for NLP is tokenization
    of input strings into **n-grams**, or groups of words. Those n-grams are the features
    that are given to the Bayesian classifier algorithm.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明显体现维度诅咒的领域是**自然语言处理**（**NLP**）。想象一下，你正在使用贝叶斯分类器对与品牌或其他主题相关的推文进行情感分析。正如你将在后面的章节中学到的，NLP数据预处理的一部分是将输入字符串分解成**n-gram**，即单词组。这些n-gram是提供给贝叶斯分类器算法的特征。
- en: 'Consider a few input strings: `I love cheese`, `I like cheese`, `I hate cheese`,
    `I don''t love cheese`, `I don''t really like cheese`. These examples are straightforward
    to us, since we''ve been using natural language our entire lives. How would an
    algorithm view these examples, though? If we are doing a 1-gram or **unigram**
    analysis—meaning that we split the input string into individual words—we see `love`
    in the first example, `like` in the second, `hate` in the third, `love` in the
    fourth, and `like` in the fifth. Our unigram analysis may be accurate for the
    first three examples, but it fails on the fourth and fifth because it does not
    learn that `don''t love` and `don''t really like` are coherent statements; the
    algorithm is only looking at the effects of individual words. This algorithm runs
    very quickly and requires little storage space, because in the preceding example
    there are only seven unique words used in the four phrases above (`I`, `love`,
    `cheese`, `like`, `hate`, `don''t`, and `really`).'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑几个输入字符串：`I love cheese`、`I like cheese`、`I hate cheese`、`I don't love cheese`、`I
    don't really like cheese`。对我们来说，这些例子很简单，因为我们整个一生都在使用自然语言。然而，一个算法会如何看待这些例子呢？如果我们进行1-gram或**unigram**分析——这意味着我们将输入字符串分割成单个单词——我们在第一个例子中看到`love`，在第二个例子中看到`like`，在第三个例子中看到`hate`，在第四个例子中看到`love`，在第五个例子中看到`like`。我们的unigram分析可能对前三个例子是准确的，但对于第四和第五个例子失败了，因为它没有学习到`don't
    love`和`don't really like`是连贯的陈述；算法只关注单个单词的影响。这个算法运行非常快，需要的存储空间也很小，因为在先前的例子中，上述四个短语中只使用了七个独特的单词（`I`、`love`、`cheese`、`like`、`hate`、`don't`和`really`）。
- en: You may then modify the tokenization preprocessing to use `bigrams`, or 2-grams—or
    groups of two words at a time. This increases the dimensionality of our data,
    requiring more storage space and processing time, but also yields better results.
    The algorithm now sees dimensions like `I love` and `love cheese`, and can now
    also recognize that `don't love` is different from `I love`. Using the bigram
    approach the algorithm may correctly identify the sentiment of the first four
    examples but still fail for the fifth, which is parsed as `I don't`, `don't really`,
    `really like`, and `like cheese`. The classification algorithm will see `really
    like` and `like cheese`, and incorrectly relate that to the positive sentiment
    in the second example. Still, the bigram approach is working for 80% of our examples.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以修改分词预处理以使用`bigrams`，即2-gram，或者每次两个词的组合。这增加了我们数据的维度，需要更多的存储空间和处理时间，但也能得到更好的结果。算法现在可以看到像`I
    love`和`love cheese`这样的维度，现在也能识别出`don't love`与`I love`是不同的。使用bigram方法，算法可能正确地识别前四个示例的情感，但对于第五个示例，它被解析为`I
    don't`、`don't really`、`really like`和`like cheese`。分类算法将看到`really like`和`like cheese`，并错误地将它与第二个示例中的积极情感联系起来。尽管如此，bigram方法在我们的示例中有80%是有效的。
- en: You might now be tempted to upgrade the tokenization once more to capture trigrams,
    or groups of three words at a time. Instead of getting an increase in accuracy,
    the algorithm takes a nosedive and is unable to correctly identify anything. We
    now have too many dimensions in our data. The algorithm learns what `I love cheese` means,
    but no other training example includes the phrase `I love cheese` so that knowledge
    can't be applied in any way. The fifth example parses into the trigrams `I don't
    really`, `don't really like`, and `really like cheese`—none of which have ever
    been encountered before! This algorithm ends up giving you a 50% sentiment for
    every example, because there simply isn't enough data in the training set to capture
    all of the relevant combinations of trigrams.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可能想再次升级分词以捕获trigrams，即每次三个词的组合。然而，算法并没有提高准确性，而是急剧下降，无法正确识别任何内容。现在我们的数据维度太多了。算法学习了`I
    love cheese`的含义，但没有任何其他训练示例包含这个短语，因此这种知识无法以任何方式应用。第五个示例被解析为trigrams `I don't really`、`don't
    really like`和`really like cheese`——这些之前都从未遇到过！这个算法最终给每个示例都给出了50%的情感评分，因为训练集中没有足够的数据来捕捉所有相关的trigrams组合。
- en: 'This is the curse of dimensionality at play: the **trigram** approach may indeed
    give you better accuracy than the bigram approach, but only if you have a huge
    training set that provides data on all the different possible combinations of
    three words at a time. You also now need a tremendous amount of storage space
    because there are a much larger number of combinations of three words than there
    are of two words. Choosing the preprocessing approach will therefore depend on
    the context of the problem, the computing resources available, and also the training
    data available to you. If you have a lot of training data and tons of resources,
    the trigram approach may be more accurate, but in more realistic conditions, the
    bigram approach may be better overall, even if it does misclassify some tweets.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这是维度灾难在发挥作用：**三元组**方法确实可能比二元组方法提供更好的准确性，但前提是你有一个巨大的训练集，它提供了关于每次三个不同单词的所有可能组合的数据。你现在还需要大量的存储空间，因为三个单词的组合比两个单词的组合要多得多。因此，选择预处理方法将取决于问题的上下文、可用的计算资源以及你拥有的训练数据。如果你有大量的训练数据和大量的资源，三元组方法可能更准确，但在更现实的情况下，二元组方法可能总体上更好，即使它确实会错误分类一些推文。
- en: The preceding discussion relates to the concepts of **feature selection**, **feature
    extraction**, and **dimensionality**. In general, our goal is to *select* only
    relevant features (ignore shopper trends that aren't interesting to us), *extract*
    or *derive* features that better represent our data (by using facial measurements
    rather than photograph pixels), and ultimately *reduce dimensionality* such that
    we use the fewest, most relevant dimensions we can.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的讨论涉及到**特征选择**、**特征提取**和**维度**的概念。一般来说，我们的目标是只**选择**相关的特征（忽略对我们不感兴趣的客户趋势），**提取**或**推导**出更好地代表我们数据的特征（通过使用面部测量而不是照片像素），并最终**降低维度**，这样我们就可以使用尽可能少且最相关的维度。
- en: Feature selection and feature extraction
  id: totrans-39
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择和特征提取
- en: Both feature selection and feature extraction are techniques used to reduce
    dimensionality, though they are slightly different concepts. Feature selection
    is the practice of using only variables or features that are relevant to the problem
    at hand. In general, feature selection looks at individual features (such as `time
    on site`) and makes a determination of the relevance of that single feature. Feature
    extraction is similar, however feature extraction often looks at multiple correlated
    features and combines them into a single feature (like looking at hundreds of
    individual pixels and converting them into a **distance between pupils **measurement).
    In both cases, we are reducing the dimensionality of the problem, but the difference
    between the two is whether we are simply filtering out irrelevant dimensions (feature
    selection) or combining existing features in order to derive a new representative
    feature (feature extraction).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择和特征提取都是用于降维的技术，尽管它们是略有不同的概念。特征选择是指只使用与当前问题相关的变量或特征。一般来说，特征选择会查看单个特征（例如“网站停留时间”）并判断该单个特征的相关性。特征提取与此类似，然而特征提取通常查看多个相关特征并将它们组合成一个单一的特征（例如查看数百个单个像素并将它们转换为**瞳孔间距**测量）。在这两种情况下，我们都在降低问题的维度，但两者的区别在于我们是在简单地过滤掉不相关的维度（特征选择）还是通过组合现有特征来推导出一个新的代表性特征（特征提取）。
- en: 'The goal of feature selection is to select the subset of features or dimensions
    of your data that optimizes the accuracy of your model. Let''s take a look at
    the naive approach to solving this problem: an exhaustive, brute force search
    of all possible subsets of dimensions. This approach is not viable in real-world
    applications, but it serves to frame the problem for us. If we take the e-commerce
    store example, our goal is to find some subset of dimensions or features that
    gives us the best results from our model. We know we have 50 features to choose
    from, but we don''t know how many are in the optimum set of features. Solving
    this problem by brute force, we would first pick only one feature at a time, and
    train and evaluate our model for each feature.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的目标是选择数据中特征或维度的子集，以优化模型的准确率。让我们看看解决这个问题的直观方法：对所有可能的维度子集进行穷举、暴力搜索。这种方法在现实世界的应用中不可行，但它有助于为我们界定问题。如果我们以电子商务商店为例，我们的目标是找到一些维度或特征的子集，从我们的模型中获得最佳结果。我们知道我们有50个特征可供选择，但我们不知道最佳特征集中有多少个。通过暴力解决此问题，我们首先一次只选择一个特征，并为每个特征训练和评估我们的模型。
- en: 'For instance, we would use only `time on site` as a data point, train the model
    on that data point, evaluate the model, and record the accuracy of the model.
    Then we move on to `total past purchase amount`, train the model, evaluate the
    model, and record results. We do this 48 more times for the remaining features
    and record the performance of each. Then we have to consider combinations of two
    features at a time, for instance by training and evaluating the model on `time
    on site` and `total past purchase amount`, and then training and evaluating on
    `time on site` and `last purchase date`, and so on. There are 1,225 unique pairs
    of features out of our set of 50, and we must repeat the procedure for each pair.
    Then we must consider groups of three features at a time, of which there are 19,600
    combinations. Then we must consider groups of four features, of which there are
    230,300 unique combinations. There are 2,118,760 combinations of five features,
    and nearly 16 million combinations of six features available to us, and so on.
    Obviously this exhaustive search for the optimal set of features to use cannot
    be done in a reasonable amount of time: we''d have to train our model billions
    of times just to find out what the best subset of features to use is! We must
    find a better approach.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们只会使用“网站停留时间”作为一个数据点，在该数据点上训练模型，评估模型，并记录模型的准确率。然后我们转向“过去总购买金额”，在该数据点上训练模型，评估模型，并记录结果。我们对剩余的每个特征重复此过程48次，并记录每个特征的性能。然后我们必须考虑每次两个特征的组合，例如通过在“网站停留时间”和“过去总购买金额”上训练和评估模型，然后训练和评估在“网站停留时间”和“最后购买日期”上，等等。在我们的50个特征集中有1,225个独特的特征对，我们必须对每一对重复此过程。然后我们必须考虑每次三个特征的组合，其中共有19,600种组合。然后我们必须考虑每次四个特征的组合，其中共有230,300个独特的组合。有2,118,760个五个特征的组合，以及近1600万个六个特征的组合可供我们选择，等等。显然，这种对最优特征集的全面搜索无法在合理的时间内完成：我们可能需要训练我们的模型数十亿次，才能找出最佳的子集特征！我们必须找到更好的方法。
- en: 'In general, feature selection techniques are split into three categories: filter
    methods, wrapper methods, and embedded methods. Each category has a number of
    techniques, and the technique you select will depend on the data, the context,
    and the algorithm of your specific situation.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，特征选择技术分为三类：过滤方法、包装方法和嵌入式方法。每个类别都有多种技术，你选择的技术将取决于你的数据、上下文以及特定情况下的算法。
- en: Filter methods are the easiest to implement and typically have the best performance.
    Filter methods for feature selection analyze a single feature at a time and attempt
    to determine that feature's relevance to the data. Filter methods typically do
    not have any relation to the ML algorithm you use afterwards, and are more typically
    statistical methods that analyze the feature itself.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤方法是最容易实现的，并且通常具有最佳性能。特征选择的过滤方法一次分析一个特征，并试图确定该特征与数据的相关性。过滤方法通常与之后使用的机器学习算法无关，而是更典型的分析特征本身的统计方法。
- en: For instance, you may use the Pearson correlation coefficient to determine if
    a feature has a linear relationship with the output variable, and remove features
    with a correlation very close to zero. This family of approaches will be very
    fast in terms of computational time, but has the disadvantage of not being able
    to identify features that are cross-correlated with one another, and, depending
    on the filter algorithm you use, may not be able to identify nonlinear or complex
    relationships.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你可以使用皮尔逊相关系数来确定一个特征是否与输出变量有线性关系，并移除与零非常接近的相关性特征。这种方法族在计算时间上会非常快，但缺点是无法识别相互交叉相关的特征，并且根据你使用的过滤器算法，可能无法识别非线性或复杂关系。
- en: Wrapper methods are similar to the brute force approach described earlier, however
    with the goal of avoiding a full exhaustive search of every combination of features
    as we did previously. For instance, you may use a genetic algorithm to select
    subsets of features, train and evaluate the model, and then use the evaluation
    of the model as evolutionary pressure to find the next subset of features to test.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 包装方法与之前描述的暴力方法类似，但目标是避免像之前那样对每个特征组合进行全面穷举搜索。例如，你可以使用遗传算法来选择特征子集，训练和评估模型，然后使用模型的评估作为进化压力来找到下一个要测试的特征子集。
- en: The genetic algorithm approach may not find the perfect subset of features,
    but will likely discover a very good subset of features to use. Depending on the
    actual machine learning model you use and the size of the dataset, this approach
    may still take a long time, but it will not take an intractably long amount of
    time like the exhaustive search would. The advantage of wrapper methods is that
    they interact with the actual model you're training and therefore serve to directly
    optimize your model, rather than simply attempting to independently statistically
    filter out individual features. The major disadvantage of these methods is the
    computational time it takes to achieve the desired results.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 遗传算法方法可能找不到完美的特征子集，但很可能会发现一个非常好的特征子集来使用。根据你实际使用的机器学习模型和数据集的大小，这种方法可能仍然需要很长时间，但不会像穷举搜索那样需要无法处理的大量时间。包装方法的优势在于它们与正在训练的实际模型交互，因此直接优化你的模型，而不是简单地尝试独立地统计过滤出单个特征。这些方法的重大缺点是实现所需结果所需的计算时间。
- en: There is also a family of methods called **embedded methods**, however this
    family of techniques relies on algorithms that have their own feature selection
    algorithm built in and are therefore quite specialized; we will not discuss them
    here.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一种称为**嵌入式方法**的方法族，然而这个技术族依赖于具有内置特征选择算法的算法，因此非常专业化；我们在这里不会讨论它们。
- en: 'Feature extraction techniques focus on combining existing features into new,
    derived features that better represent your data while also eliminating extra
    or redundant dimensionality. Imagine that your e-commerce shopper data includes
    both `time on site` and `total pixel scrolling distance while browsing` as dimensions.
    Also imagine that both of these dimensions do strongly correlate to the amount
    of money a shopper spends on the site. Naturally, these two features are related
    to each other: the more time a user spends on the site, the more likely they are
    to have scrolled a farther distance. Using only feature selection techniques,
    such as the Pearson correlation analysis, you would find that you should keep
    both `time on site` and `total distance scrolled` as features. The feature selection
    technique, which analyzes these features independently, has determined that both
    are relevant to your problem, but has not understood that the two features are
    actually highly related to each other and therefore redundant.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取技术专注于将现有特征组合成新的、派生特征，这些特征更好地代表你的数据，同时消除额外的或冗余的维度。想象一下，你的电子商务购物者数据包括`网站停留时间`和`浏览时的总像素滚动距离`作为维度。也想象一下，这两个维度都与购物者在网站上花费的金额有很强的相关性。自然地，这两个特征是相互关联的：用户在网站上花费的时间越多，他们滚动距离越远的可能性就越大。仅使用特征选择技术，如皮尔逊相关分析，你会发现在特征中应该保留`网站停留时间`和`总滚动距离`。这种独立分析这些特征的特征选择技术已经确定这两个都与你的问题相关，但没有理解到这两个特征实际上高度相关，因此是冗余的。
- en: A more sophisticated feature extraction technique, such as **Principal Component
    Analysis** (**PCA**), would be able to identify that time on site and scroll distance
    can actually be combined into a single, new feature (let's call it `site engagement`)
    that encapsulates the data represented by what used to be two separate features.
    In this case we have *extracted* a new feature from the time on site and scrolling
    distance measurements, and we are using that single feature instead of the two
    original features separately. This differs from feature selection; in feature
    selection we are simply choosing which of the original features to use when training
    our model, however in feature extraction we are creating brand new features from
    related combinations of original features. Both feature selection and feature
    extraction therefore reduce the dimensionality of our data, but do so in different
    ways.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更复杂的特征提取技术，例如**主成分分析**（**PCA**），能够识别出网站停留时间和滚动距离实际上可以合并成一个单一的新特征（让我们称它为“网站参与度”），它封装了以前由两个单独特征表示的数据。在这种情况下，我们从网站停留时间和滚动距离测量中提取了一个新特征，并使用这个单一特征而不是两个原始特征分别。这与特征选择不同；在特征选择中，我们只是在训练模型时选择使用原始特征中的哪一个，然而在特征提取中，我们是从原始特征的关联组合中创建全新的特征。因此，特征选择和特征提取都减少了我们数据的维度，但以不同的方式做到这一点。
- en: Pearson correlation example
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 皮尔逊相关系数示例
- en: 'Let''s return to our example of shoppers on the e-commerce store and consider
    how we might use the Pearson correlation coefficient to select data features.
    Consider the following example data, which records purchase amounts for shoppers
    given their time spent on site and the amount of money they had spent on purchases
    previously:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到电子商务商店购物者的例子，并考虑我们如何使用皮尔逊相关系数来选择数据特征。考虑以下示例数据，它记录了购物者在网站上的停留时间和他们之前在购买上花费的金额所对应的购买金额：
- en: '| **Purchase Amount** | **Time on Site (seconds)** | **Past Purchase Amount**
    |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **购买金额** | **网站停留时间（秒）** | **过去购买金额** |'
- en: '| $10.00 | 53 | $7.00 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 10.00 | 53 | 7.00 |'
- en: '| $14.00 | 220 | $12.00 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 14.00 | 220 | 12.00 |'
- en: '| $18.00 | 252 | $22.00 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 18.00 | 252 | 22.00 |'
- en: '| $20.00 | 571 | $17.00 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 20.00 | 571 | 17.00 |'
- en: '| $22.00 | 397 | $21.00 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 22.00 | 397 | 21.00 |'
- en: '| $34.00 | 220 | $23.00 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 34.00 | 220 | 23.00 |'
- en: '| $38.00 | 776 | $29.00 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 38.00 | 776 | 29.00 |'
- en: '| $50.00 | 462 | $74.00 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 50.00 | 462 | 74.00 |'
- en: '| $52.00 | 354 | $63.00 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 52.00 | 354 | 63.00 |'
- en: '| $56.00 | 23 | $61.00 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 56.00 | 23 | 61.00 |'
- en: Of course, in a real application of this problem you may have thousands or hundreds
    of thousands of rows, and dozens of columns, each representing a different dimension
    of data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在实际应用这个问题时，你可能会有数千或数百万行，以及数十列，每列代表数据的不同维度。
- en: Let's now select features for this data manually. The `purchase amount` column
    is our output data, or the data that we want our algorithm to predict given other
    features. In this exercise, we can choose to train the model using both time on
    site and previous purchase amount, time on site alone, or previous purchase amount
    alone.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将手动选择这些数据的特点。`购买金额`列是我们的输出数据，或我们希望算法根据其他特征预测的数据。在这个练习中，我们可以选择使用网站停留时间和之前的购买金额、仅使用网站停留时间，或者仅使用之前的购买金额来训练模型。
- en: When using a filter method for feature selection we consider one feature at
    a time, so we must look at time on site's relation to purchase amount independently
    of past purchase amount's relation to purchase amount. One manual approach to
    this problem would be to chart each of our two candidate features against the
    `Purchase Amount` column, and calculate a correlation coefficient to determine
    how strongly each feature is related to the purchase amount data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用过滤器方法进行特征选择时，我们一次考虑一个特征，因此我们必须独立于过去购买金额与购买金额的关系，来查看网站停留时间与购买金额的关系。解决这个问题的一个手动方法是将我们的两个候选特征分别与“购买金额”列进行图表化，并计算相关系数以确定每个特征与购买金额数据的相关程度。
- en: 'First, we''ll chart time on site versus purchase amount, and use our spreadsheet
    tool to calculate the Pearson correlation coefficient:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将图表化网站停留时间与购买金额，并使用我们的电子表格工具计算皮尔逊相关系数：
- en: '![](img/afddac71-1293-4b6b-a716-4c16ace48d71.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/afddac71-1293-4b6b-a716-4c16ace48d71.png)'
- en: Even a simple visual inspection of the data hints to the fact that there is
    only a small relationship—if any at all—between time on site and purchase amount.
    Calculating the Pearson correlation coefficient yields a correlation of about
    +0.1, a very weak, essentially insignificant correlation between the two sets
    of data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是简单的数据视觉检查也暗示着网站停留时间和购买金额之间只有很小的关系——如果有的话。计算皮尔逊相关系数得到大约 +0.1 的相关性，这是非常弱、几乎不相关的两个数据集之间的相关性。
- en: 'However, if we chart the past purchase amount versus current purchase amount,
    we see a very different relationship:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们绘制过去购买金额与当前购买金额的图表，我们会看到一个非常不同的关系：
- en: '![](img/42d01751-2f64-434d-b706-3ef5aff2c71f.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/42d01751-2f64-434d-b706-3ef5aff2c71f.png)'
- en: In this case, our visual inspection tells us that there is a linear but somewhat
    noisy relationship between the past purchase amount and the current purchase amount.
    Calculating the correlation coefficient gives us a correlation value of +0.9,
    quite a strong linear relationship!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们的视觉检查告诉我们，过去购买金额和当前购买金额之间存在线性但有些嘈杂的关系。计算相关系数给我们一个相关性值为 +0.9，这是一个相当强的线性关系！
- en: This type of analysis tells us that we can ignore the time on site data when
    training our model, as there seems to be little to no statistical significance
    in that information. By ignoring time on site data, we can reduce the number of
    dimensions we need to train our model on by one, allowing our model to better
    generalize data and also improve performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这种分析告诉我们，在训练我们的模型时可以忽略网站停留时间数据，因为似乎在该信息中几乎没有或没有统计意义。通过忽略网站停留时间数据，我们可以减少训练模型所需的维度数量，从而让我们的模型更好地泛化数据并提高性能。
- en: If we had 48 other numerical dimensions to consider, we could simply calculate
    the correlation coefficient for each of them and discard each dimension whose
    correlation falls beneath some threshold. Not every feature can be analyzed using
    correlation coefficients, however, so you can only apply the Pearson algorithm
    to those features where such a statistical analysis makes sense; it would not
    make sense to use Pearson correlation to analyze a feature that lists *recently
    browsed product category*, for instance. You can, and should, use other types
    of feature selection filters for different dimensions representing different types
    of data. Over time, you will develop a toolkit of analysis techniques that can
    apply to different types of data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们还有48个其他数值维度需要考虑，我们可以简单地计算每个维度的相关系数，并丢弃那些相关性低于某个阈值的维度。然而，并非每个特征都可以使用相关系数进行分析，因此您只能将皮尔逊算法应用于那些进行此类统计分析有意义的特征；例如，使用皮尔逊相关系数分析列出*最近浏览的产品类别*的特征就没有意义。您可以使用，并且应该使用其他类型的特征选择过滤器来处理代表不同类型数据的维度。随着时间的推移，您将开发出一套适用于不同类型数据的分析技术工具箱。
- en: Unfortunately, a thorough explanation of all the possible feature extraction
    and feature selection algorithms and tools is not possible here; you will have
    to research various techniques and determine which ones fit the shape and style
    of your features and data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，在这里不可能对所有可能的特征提取和特征选择算法及工具进行详尽的解释；您将不得不研究各种技术并确定哪些适合您特征和数据的形式和风格。
- en: Some algorithms to consider for filter techniques are the Pearson and Spearman
    correlation coefficients, the chi-squared test, and information gain algorithms
    such as the Kullback–Leibler divergence.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于过滤技术，可以考虑的算法包括皮尔逊和斯皮尔曼相关系数、卡方检验和信息增益算法，如库尔巴克-莱布勒散度。
- en: Approaches to consider for wrapper techniques are optimization techniques such
    as genetic algorithms, tree-search algorithms such as best-first search, stochastic
    techniques such as random hill-climb algorithms, and heuristic techniques such
    as recursive feature elimination and simulated annealing. All of these techniques
    aim to select the best set of features that optimize the output of your model,
    so any optimization technique can be a candidate, however, genetic algorithms
    are quite effective and popular.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包装技术，可以考虑的方法包括遗传算法、最佳优先搜索等树搜索算法、随机技术如随机爬山算法以及启发式技术如递归特征消除和模拟退火。所有这些技术旨在选择最佳的特征集以优化您模型的输出，因此任何优化技术都可以作为候选，然而，遗传算法非常有效且受欢迎。
- en: Feature extraction has many algorithms to consider, and generally focuses on
    cross-correlation of features in order to determine new features that minimize
    some error function; that is, how can two or more features be combined such that
    a minimum amount of data is lost. Relevant algorithms include PCA, partial least
    squares, and autoencoding. In NLP, latent semantic analysis is popular. Image
    processing has many specialized feature extraction algorithms, such as edge detection,
    corner detection, and thresholding, and further specializations based on problem
    domain such as face identification or motion detection.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取有许多算法需要考虑，通常关注于特征之间的互相关，以确定最小化某些误差函数的新特征；也就是说，如何将两个或多个特征结合起来，使得损失的数据量最小。相关的算法包括主成分分析（PCA）、偏最小二乘法和自动编码。在自然语言处理（NLP）中，潜在语义分析很受欢迎。图像处理有许多专门的特征提取算法，例如边缘检测、角点检测和阈值处理，以及基于问题域的进一步专业化，如人脸识别或运动检测。
- en: Cleaning and preparing data
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 清洗和准备数据
- en: Feature selection is not the only consideration required when preprocessing
    your data. There are many other things that you may need to do to prepare your
    data for the algorithm that will ultimately analyze the data. Perhaps there are
    measurement errors that create significant outliers. There can also be instrumentation
    noise in the data that needs to be smoothed out. Your data may have missing values
    for some features. These are all issues that can either be ignored or addressed,
    depending, as always, on the context, the data, and the algorithm involved.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在预处理数据时，特征选择并不是唯一需要考虑的因素。还有许多其他事情你可能需要做，以准备数据供最终分析数据的算法使用。可能存在测量误差，导致显著的异常值。数据中也可能存在需要平滑的仪器噪声。数据中可能存在某些特征的缺失值。这些都是可以根据上下文、数据和涉及的算法选择忽略或解决的问题。
- en: Additionally, the algorithm you use may require the data to be normalized to
    some range of values. Or perhaps your data is in a different format that the algorithm
    cannot use, as is often the case with neural networks which expect you to provide
    a vector of values, but you have JSON objects that come from a database. Sometimes
    you need to analyze only a specific subset of data from a larger source. If you're
    working with images you may need to resize, scale, pad, crop, or reduce the image
    to grayscale.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你使用的算法可能要求数据被归一化到某个值域。或者，也许你的数据格式与算法不兼容，例如神经网络通常期望你提供一个值向量，但你从数据库中得到的却是JSON对象。有时你可能只需要分析来自更大数据源的具体子集。如果你处理图像，你可能需要调整大小、缩放、填充、裁剪或降低图像到灰度。
- en: These tasks all fall into the realm of data preprocessing. Let's take a look
    at some specific scenarios and discuss possible approaches for each.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务都属于数据预处理的范畴。让我们看看一些具体的场景，并讨论每个场景的可能方法。
- en: Handling missing data
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失数据
- en: In many cases, several data points may have values missing from certain features.
    If you're looking at Yes/No responses to survey questions, several participants
    may have accidentally or purposefully skipped a given question. If you're looking
    at time series data, your measurement tool may have had an error for a given period
    or measurement. If you're looking at e-commerce shopping habits, some features
    may not be relevant to a user, for instance `last login date` for users that shop
    as an anonymous guest. The individual situation and scenario, as well as your
    algorithm's tolerance for missing data, determines the approach you must take
    to remediate missing data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，几个数据点可能某些特征值缺失。如果你查看调查问题的Yes/No回答，几个参与者可能意外或故意跳过了一个给定问题。如果你查看时间序列数据，你的测量工具可能在某个时间段或测量中出现了错误。如果你查看电子商务购物习惯，某些特征可能对用户不相关，例如对作为匿名客人的用户来说的`最后登录日期`。具体情况和场景，以及你的算法对缺失数据的容忍度，决定了你必须采取的方法来修复缺失数据。
- en: Missing categorical data
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失的类别数据
- en: In the case of categorical data, such as Yes/No survey questions that may not
    have been responded to, or an image that has not yet been labeled with its category,
    often the best approach is to create a new category called *undefined*, *N/A*,
    *unknown*, or *similar*. Alternatively, you may be able to select a reasonable
    default category to use for these missing values, perhaps choosing the most frequent
    category from the set, or choosing a category that represents the data point's
    logical parent. If you're analyzing photographs uploaded by users and are missing
    the category tag for a given photograph, you may instead use the *user's* stated
    category in place of the photo's individual category. That is, if a user is tagged
    as a fashion photographer, you may use the *fashion* category for the photo, even
    though the user has also uploaded a number of *travel* photographs. This approach
    will add noise in the form of miscategorized data points to the system, but may
    in fact have a positive overall effect of forcing the algorithm to generalize
    its model; the model may eventually learn that fashion and travel photography
    are similar.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类数据的情况下，例如可能未回答的Yes/No调查问题，或尚未对其类别进行标记的图像，通常最好的方法是创建一个新的类别，称为*未定义*、*N/A*、*未知*或*类似*。或者，你可能能够选择一个合理的默认类别来用于这些缺失值，例如选择集合中最频繁的类别，或者选择代表数据点逻辑父类的类别。如果你正在分析用户上传的图片，并且缺少特定照片的类别标签，你可以使用*用户*声明的类别代替照片的个别类别。也就是说，如果一个用户被标记为时尚摄影师，你可以为该照片使用*时尚*类别，即使该用户也上传了一些*旅行*照片。这种方法将以误分类数据点的形式向系统中添加噪声，但实际上可能对算法泛化模型有积极的影响；模型最终可能学会时尚摄影和旅行摄影是相似的。
- en: Using an *undefined* or *N/A* category is also a preferred approach, as the
    fact that a data point has no category may be significant in and of itself—*No
    category* can itself be a valid category. The size of the dataset, the algorithm
    used, and the relative size of the *N/A* category within the dataset will affect
    whether this is a reasonable approach to take. In a classification scenario, for
    instance, two effects are possible. If the uncategorized items *do* form a pattern
    (for instance, *fashion* photos are uncategorized more often than other photos),
    you may find that your classifier incorrectly learns that fashion photos should
    be categorized as N/A! In this scenario, it may be better to ignore uncategorized
    data points entirely.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*未定义*或*N/A*类别也是一个首选的方法，因为数据点没有类别的事实本身可能就很重要——*无类别*本身可以是一个有效的类别。数据集的大小、所使用的算法以及数据集中*N/A*类别的相对大小将影响这是否是一个合理的处理方法。例如，在分类场景中，可能出现两种效果。如果未分类的项目*确实*形成了一个模式（例如，*时尚*照片比其他照片更常被未分类），你可能会发现你的分类器错误地学习到时尚照片应该被分类为N/A！在这种情况下，最好完全忽略未分类的数据点。
- en: However, if the uncategorized photos are comprised of photos from various categories
    equally, your classifier may end up identifying difficult-to-classify photos as
    N/A, which could actually be a desired effect. In this scenario, you can consider
    N/A as a class of its own, being comprised of difficult, broken, or unresolvable
    photos.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果未分类的照片由来自各种类别的照片均匀组成，你的分类器最终可能会将难以分类的照片识别为N/A，这实际上可能是一个期望的效果。在这种情况下，你可以考虑N/A作为一个包含难以分类、损坏或无法解决的图片的类别。
- en: Missing numerical data
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 缺失的数值数据
- en: 'Missing values for numerical data is trickier to handle than categorical data,
    as there is often no reasonable default for missing numerical values. Depending
    on the dataset, you may be able to use zeros as replacements, however in some
    cases using the mean or median value of that feature is more appropriate. In other
    scenarios, and depending on the algorithm used, it may be useful to fill in missing
    values with a very large value: if that data point needs to have an error calculation
    performed on it, using a large value will mark the data point with a large error,
    and discourage the algorithm from considering that point.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数值数据的缺失值比处理分类数据更复杂，因为通常没有合理的默认值来替换缺失的数值。根据数据集的不同，你可能可以使用零作为替代值，然而在某些情况下，使用该特征的均值或中位数可能更合适。在其他情况下，根据所使用的算法，用一个非常大的值填充缺失值可能是有用的：如果需要对数据点进行错误计算，使用大值将标记数据点为具有大错误，从而阻止算法考虑该点。
- en: In other cases, you can use a linear interpolation to fill in missing data points.
    This makes sense in some time series applications. If your algorithm expects 31
    data points representing the growth of some metric, and you're missing one value
    for day 12, you can use the average of day 11's and day 13's values to serve as
    an estimate for day 12's value.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他情况下，你可以使用线性插值来填充缺失的数据点。这在某些时间序列应用中是有意义的。如果你的算法期望有31个数据点表示某些指标的增长，而你缺少第12天的一个值，你可以使用第11天和第13天的平均值作为第12天值的估计。
- en: 'Often the correct approach is to ignore and filter out data points with missing
    values, however, you must consider the effects of such an action. If the data
    points with missing values strongly represent a specific category of data, you
    may end up creating a strong selection bias as a side effect, as your analysis
    would have ignored a significant group. You must balance this type of side effect
    with the possible side effects caused by the other approaches: will zeroing out
    missing values significantly skew your distribution? Will using the mean or median
    as replacements taint the rest of the analysis? These questions can only be answered
    on a case-by-case basis.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 通常正确的做法是忽略并过滤掉缺失值的数据点，然而，你必须考虑这种行为的效应。如果具有缺失值的数据点强烈代表特定类别数据，你可能会在副作用中创建一个强烈的选择偏差，因为你的分析会忽略一个重要的群体。你必须平衡这种类型的副作用与其他方法可能引起的副作用：将缺失值置零会显著扭曲你的分布吗？使用平均值或中位数作为替代品会污染分析的其他部分吗？这些问题只能根据具体情况回答。
- en: Handling noise
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理噪声
- en: Noise in data can come from many sources, but is not often a significant issue
    as most machine learning techniques are resilient to noisy datasets. Noise can
    come from environmental factors (for instance, the air conditioner compressor
    turning on randomly and causing signal noise in a nearby sensor), it can come
    from transcription errors (somebody recorded the wrong data point, selected the
    wrong option in a survey, or an OCR algorithm read a `3` as an `8`), or it can
    be inherent to the data itself (such as fluctuations in temperature recordings,
    which will follow a seasonal pattern but have a noisy daily pattern).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中的噪声可能来自许多来源，但通常不是一个重大问题，因为大多数机器学习技术对噪声数据集具有弹性。噪声可能来自环境因素（例如，空调压缩机随机启动并导致附近传感器的信号噪声），也可能来自转录错误（有人记录了错误的数据点，在调查中选择了错误的选项，或者OCR算法将`3`读作`8`），或者它可能是数据本身固有的（例如，温度记录的波动将遵循季节性模式，但具有嘈杂的日间模式）。
- en: Noise in categorical data can also be caused by category labels that aren't
    normalized, such as images that are tagged `fashion` or `fashions` when the category
    is supposed to be `Fashion`. In those scenarios, the best approach is to simply
    normalize the category label, perhaps by forcing all category labels to be made
    singular and fully lowercase—this will combine the `Fashion`, `fashion`, and `fashions`
    categories into one single `fashion` category.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 类别数据中的噪声也可能是由未归一化的类别标签引起的，例如，当类别应该是`Fashion`时，图像被标记为`fashion`或`fashions`。在这些情况下，最佳做法是简单地归一化类别标签，可能通过强制所有类别标签变为单数并完全小写——这将把`Fashion`、`fashion`和`fashions`类别合并为一个单一的`fashion`类别。
- en: Noise in time series data can be smoothed by taking a moving average of multiple
    values; however, first you should evaluate if smoothing the data is important
    to your algorithm and results in the first place. Often, the algorithm will still
    perform well enough for practical applications if there is a small amount of noise,
    and especially if the noise is random rather than systemic.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据中的噪声可以通过取多个值的移动平均来平滑；然而，首先你应该评估平滑数据对你的算法和结果是否重要。通常，如果噪声量很小，算法仍然足以满足实际应用，特别是如果噪声是随机的而不是系统性的。
- en: 'Consider the following example of daily measurements of some sensor:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下某个传感器每日测量的示例：
- en: '| **Day** | **Value** |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **Day** | **Value** |'
- en: '| 1 | 0.1381426172 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1381426172 |'
- en: '| 2 | 0.5678176776 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.5678176776 |'
- en: '| 3 | 0.3564009968 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.3564009968 |'
- en: '| 4 | 1.239499423 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.239499423 |'
- en: '| 5 | 1.267606181 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1.267606181 |'
- en: '| 6 | 1.440843361 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1.440843361 |'
- en: '| 7 | 0.3322843208 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.3322843208 |'
- en: '| 8 | 0.4329166745 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.4329166745 |'
- en: '| 9 | 0.5499234277 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.5499234277 |'
- en: '| 10 | -0.4016070826 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 10 | -0.4016070826 |'
- en: '| 11 | 0.06216906816 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.06216906816 |'
- en: '| 12 | -0.9689103112 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 12 | -0.9689103112 |'
- en: '| 13 | -1.170421963 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 13 | -1.170421963 |'
- en: '| 14 | -0.784125647 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 14 | -0.784125647 |'
- en: '| 15 | -1.224217169 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 15 | -1.224217169 |'
- en: '| 16 | -0.4689120937 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 16 | -0.4689120937 |'
- en: '| 17 | -0.7458561671 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 17 | -0.7458561671 |'
- en: '| 18 | -0.6746415566 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 18 | -0.6746415566 |'
- en: '| 19 | -0.0429460593 |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| 19 | -0.0429460593 |'
- en: '| 20 | 0.06757010626 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.06757010626 |'
- en: '| 21 | 0.480806698 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 0.480806698 |'
- en: '| 22 | 0.2019759014 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 0.2019759014 |'
- en: '| 23 | 0.7857692899 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 0.7857692899 |'
- en: '| 24 | 0.725414402 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 0.725414402 |'
- en: '| 25 | 1.188534085 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 1.188534085 |'
- en: '| 26 | 0.458488458 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 0.458488458 |'
- en: '| 27 | 0.3017212831 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 0.3017212831 |'
- en: '| 28 | 0.5249332545 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 28 | 0.5249332545 |'
- en: '| 29 | 0.3333153146 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 29 | 0.3333153146 |'
- en: '| 30 | -0.3517342423 |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 30 | -0.3517342423 |'
- en: '| 31 | -0.721682062 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 31 | -0.721682062 |'
- en: 'Graphing this data shows a noisy but periodic pattern:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制这些数据会显示出一种嘈杂但周期性的模式：
- en: '![](img/77ebb25f-47d8-4c98-84c4-c899370b9c64.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](img/77ebb25f-47d8-4c98-84c4-c899370b9c64.png)'
- en: This may be acceptable in many scenarios, but other applications may require
    smoother data.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这在许多情况下可能是可接受的，但其他应用可能需要更平滑的数据。
- en: Also, note that several of the data points exceed +1 and -1, which may be of
    significance especially if your algorithm is expecting data between the -1 and
    +1 range.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，一些数据点超过了+1和-1，这可能在您的算法期望数据在-1和+1范围内时特别有意义。
- en: We can apply a `5-Day Moving Average` to the data to generate a smoother curve.
    To perform a `5-Day Moving Average`, start with day `3`, sum the values for days
    `1` to `5`, and divide by 5\. The result becomes the moving average for day `3`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以对数据进行`5-Day Moving Average`处理以生成更平滑的曲线。要执行`5-Day Moving Average`，从第`3`天开始，将第`1`天到第`5`天的值相加，然后除以5。结果成为第`3`天的移动平均值。
- en: Note that in this approach, we lose days `1` and `2`, and also days `30` and
    `31`, because we cannot look two days before day `1` nor can we look two days
    after day `31`. However, if you require values for those days, you may use the
    raw values for days `1`, `2`, `30`, and `31`, or you may use `3-Day Moving Averages`
    for days `2` and `30` in addition to single values for days `1` and `31`. If you
    have more historical data, you can use data from the previous month calculating
    the `5-Day Moving Average` for days `1` and `2` (calculate day `1` by using the
    previous month's last two days). The approach to how you handle this moving average
    will depend on the data available to you and the importance of having 5-day averages
    for each data point versus combining 5-day averages with 3-day and 1-day averages
    at the boundaries.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在此方法中，我们失去了第`1`天和第`2`天，以及第`30`天和第`31`天，因为我们不能查看第`1`天之前的两天，也不能查看第`31`天之后的两天。然而，如果您需要这些天的值，您可以使用第`1`天、第`2`天、第`30`天和第`31`天的原始值，或者您可以使用第`2`天和第`30`天的`3-Day
    Moving Averages`以及第`1`天和第`31`天的单个值。如果您有更多历史数据，您可以使用上个月的数据，计算第`1`天和第`2`天的`5-Day
    Moving Average`（通过使用上个月最后两天来计算第`1`天）。如何处理这个移动平均的方法将取决于您可用的数据以及拥有每个数据点的5天平均值的重要性，以及将5天平均值与边界处的3天和1天平均值相结合的重要性。
- en: 'If we calculate the `5-Day Moving Average` for our month, the data becomes
    the following:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们计算我们这个月的`5-Day Moving Average`，数据将如下所示：
- en: '| **Day** | **Value** | **5-Day Moving Average** |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| **Day** | **Value** | **5-Day Moving Average** |'
- en: '| 1 | 0.1381426172 |  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1381426172 |  |'
- en: '| 2 | 0.5678176776 |  |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.5678176776 |  |'
- en: '| 3 | 0.3564009968 | 0.7138933792 |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.3564009968 | 0.7138933792 |'
- en: '| 4 | 1.239499423 | 0.974433528 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.239499423 | 0.974433528 |'
- en: '| 5 | 1.267606181 | 0.9273268566 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1.267606181 | 0.9273268566 |'
- en: '| 6 | 1.440843361 | 0.9426299922 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1.440843361 | 0.9426299922 |'
- en: '| 7 | 0.3322843208 | 0.8047147931 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.3322843208 | 0.8047147931 |'
- en: '| 8 | 0.4329166745 | 0.4708721403 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.4329166745 | 0.4708721403 |'
- en: '| 9 | 0.5499234277 | 0.1951372817 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.5499234277 | 0.1951372817 |'
- en: '| 10 | -0.4016070826 | -0.06510164468 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 10 | -0.4016070826 | -0.06510164468 |'
- en: '| 11 | 0.06216906816 | -0.3857693722 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.06216906816 | -0.3857693722 |'
- en: '| 12 | -0.9689103112 | -0.6525791871 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| 12 | -0.9689103112 | -0.6525791871 |'
- en: '| 13 | -1.170421963 | -0.8171012043 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 13 | -1.170421963 | -0.8171012043 |'
- en: '| 14 | -0.784125647 | -0.9233174367 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 14 | -0.784125647 | -0.9233174367 |'
- en: '| 15 | -1.224217169 | -0.8787066079 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 15 | -1.224217169 | -0.8787066079 |'
- en: '| 16 | -0.4689120937 | -0.7795505266 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 16 | -0.4689120937 | -0.7795505266 |'
- en: '| 17 | -0.7458561671 | -0.631314609 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 17 | -0.7458561671 | -0.631314609 |'
- en: '| 18 | -0.6746415566 | -0.3729571541 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 18 | -0.6746415566 | -0.3729571541 |'
- en: '| 19 | -0.0429460593 | -0.1830133958 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 19 | -0.0429460593 | -0.1830133958 |'
- en: '| 20 | 0.06757010626 | 0.006553017948 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.06757010626 | 0.006553017948 |'
- en: '| 21 | 0.480806698 | 0.2986351872 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 0.480806698 | 0.2986351872 |'
- en: '| 22 | 0.2019759014 | 0.4523072795 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 0.2019759014 | 0.4523072795 |'
- en: '| 23 | 0.7857692899 | 0.6765000752 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 0.7857692899 | 0.6765000752 |'
- en: '| 24 | 0.725414402 | 0.6720364272 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 0.725414402 | 0.6720364272 |'
- en: '| 25 | 1.188534085 | 0.6919855036 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 1.188534085 | 0.6919855036 |'
- en: '| 26 | 0.458488458 | 0.6398182965 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 0.458488458 | 0.6398182965 |'
- en: '| 27 | 0.3017212831 | 0.561398479 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 0.3017212831 | 0.561398479 |'
- en: '| 28 | 0.5249332545 | 0.2533448136 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 28 | 0.5249332545 | 0.2533448136 |'
- en: '| 29 | 0.3333153146 | 0.0173107096 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 29 | 0.3333153146 | 0.0173107096 |'
- en: '| 30 | -0.3517342423 |  |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 30 | -0.3517342423 |  |'
- en: '| 31 | -0.721682062 |  |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 31 | -0.721682062 |  |'
- en: In some cases, the moving average differs from the day's data point by a significant
    margin. On day `3`, for instance, the moving average is double that of the day's
    measurement.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，移动平均线与每日数据点的差异很大。例如，在第三天，移动平均线是当日测量值的两倍。
- en: 'This approach would not be appropriate in instances where you need to consider
    a given day''s measurement in isolation, however, when we graph the moving average
    against the daily data points, we can see the value of this approach:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在需要单独考虑给定一天测量值的情况下，这种方法并不合适；然而，当我们把移动平均线与每日数据点绘图时，我们可以看到这种方法的价值：
- en: '![](img/f5147b1d-5741-4d10-b9d4-b6c6f2f88682.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f5147b1d-5741-4d10-b9d4-b6c6f2f88682.png)'
- en: We can see that the moving average is much smoother than the daily measurements,
    and that the moving average better represents the periodic, sinusoidal nature
    of our data. An added bonus for us is that the moving average data no longer contains
    points that lie outside our [-1, +1] range; because the noise in this data was
    random, the random fluctuations have largely canceled each other out and brought
    our data back into range.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，移动平均线比每日测量值要平滑得多，并且移动平均线更好地代表了我们的数据的周期性和正弦性质。对我们来说，一个额外的优点是移动平均线数据不再包含位于我们的[-1,
    +1]范围之外的点；因为此数据中的噪声是随机的，随机波动在很大程度上相互抵消，使我们的数据回归到范围内。
- en: Increasing the window of the moving average will result in broader and broader
    averages, reducing resolution; if we were to take a *31-Day Moving Average*, we
    would simply have the average measurement for the entire month. If your application
    simply needs to smooth out data rather than reduce data to lower resolutions,
    you should start by applying the smallest moving average window that serves to
    clean the data enough, for instance, a 3-point moving average.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 增加移动平均线的窗口将导致越来越宽的平均值，降低分辨率；如果我们采用*31天移动平均线*，我们就会得到整个月的平均测量值。如果你的应用只需要平滑数据而不是降低数据分辨率，你应该从应用最小的移动平均线窗口开始，足以清理数据，例如，一个3点移动平均线。
- en: If you're dealing with measurements that are not time series, then a moving
    average approach may not be appropriate. For instance, if you're measuring the
    value of a sensor at arbitrary and random times where the time of measurement
    is not recorded, a moving average would not be appropriate because the dimension
    to average over is unknown (that is, we do not know the time period that the average
    moves over).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处理的是非时间序列的测量值，那么移动平均线方法可能不适用。例如，如果你在任意和随机的时间测量传感器的值，而测量时间没有记录，那么移动平均线就不适用，因为平均要跨越的维度是未知的（也就是说，我们不知道平均移动的时间段）。
- en: If you still need to eliminate noise from your data, you can try *binning* the
    measurements by creating a histogram of the data. This approach changes the nature
    of the data itself and does not apply to every situation, however, it can serve
    to obfuscate individual measurement fluctuations while still representing the
    relative frequency of different measurements.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然需要从你的数据中消除噪声，你可以尝试通过创建数据的直方图来对测量值进行*分箱*。这种方法改变了数据本身的性质，并且不适用于所有情况，然而，它可以用来模糊单个测量值的波动，同时仍然表示不同测量值的相对频率。
- en: Handling outliers
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异常值
- en: Your data will often have outlying values, or data points that are far away
    from the expected value for your dataset. Sometimes, outliers are caused by noise
    or errors (somebody recording a height of 7'3" rather than 6'3"), but other times,
    outliers are legitimate data points (one celebrity with a Twitter reach of 10
    million followers joining your service where most of the users have 10,000 to
    100,000 followers). In either case, you'll first want to identify outliers so
    that you can determine what to do with them.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据通常会包含异常值，或者远离数据集预期值的测量点。有时，异常值是由噪声或错误引起的（某人记录了7'3"的高度而不是6'3"），但有时，异常值是合法的数据点（一位拥有1000万Twitter粉丝的明星加入你的服务，而大多数用户只有1万到10万粉丝）。在两种情况下，你首先想要识别异常值，以便确定如何处理它们。
- en: 'One approach to identifying outliers is to calculate the mean and standard
    deviation of your dataset, and determine how many standard deviations away from
    the mean each data point is. The standard deviation of a dataset represents the
    overall variance or dispersion of the data. Consider the following data which
    represents the number of Twitter followers of accounts that you''re analyzing:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 识别异常值的一种方法是通过计算数据集的平均值和标准差，并确定每个数据点与平均值的偏差。数据集的标准差代表数据的整体方差或分散度。考虑以下数据，它代表了你正在分析的用户账户的Twitter关注者数量：
- en: '| **Followers** |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **关注者** |'
- en: '| 1075 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 1075 |'
- en: '| 1879 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 1879 |'
- en: '| 3794 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 3794 |'
- en: '| 4111 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 4111 |'
- en: '| 4243 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 4243 |'
- en: '| 4885 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 4885 |'
- en: '| 7617 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 7617 |'
- en: '| 8555 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 8555 |'
- en: '| 8755 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 8755 |'
- en: '| 19422 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 19422 |'
- en: '| 31914 |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 31914 |'
- en: '| 36732 |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 36732 |'
- en: '| 39570 |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 39570 |'
- en: '| 1230324 |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 1230324 |'
- en: 'As you can see, the last value is much larger than the other values in the
    set. However, this discrepancy may not be so obvious if you''re analyzing millions
    of records with dozens of features each. To automate our outlier identification
    we should first calculate the mean average of all our users, which in this case
    is an average of **100,205** followers. Then, we should calculate the standard
    deviation of the dataset, which for this data is **325,523** followers. Finally,
    we can inspect each data point by determining how many standard deviations away
    from the mean that data point is: find the absolute difference between the data
    point and the mean, and then divide by the standard deviation:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，最后一个值比集合中的其他值大得多。然而，如果你正在分析数百万条记录，每条记录有数十个特征，这种差异可能并不那么明显。为了自动化我们的异常值识别，我们首先应该计算所有用户的平均平均值，在这个例子中是**100,205**个关注者的平均值。然后，我们应该计算数据集的标准差，对于这个数据来说，是**325,523**个关注者的标准差。最后，我们可以通过确定每个数据点与平均值的偏差来检查每个数据点：找到数据点与平均值之间的绝对差值，然后除以标准差：
- en: '| **Followers** | **Deviation** |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **关注者** | **偏差** |'
- en: '| 1075 | 0.3045078726 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 1075 | 0.3045078726 |'
- en: '| 1879 | 0.3020381533 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 1879 | 0.3020381533 |'
- en: '| 3794 | 0.2961556752 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| 3794 | 0.2961556752 |'
- en: '| 4111 | 0.2951819177 |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 4111 | 0.2951819177 |'
- en: '| 4243 | 0.2947764414 |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 4243 | 0.2947764414 |'
- en: '| 4885 | 0.2928043522 |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| 4885 | 0.2928043522 |'
- en: '| 7617 | 0.2844122215 |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| 7617 | 0.2844122215 |'
- en: '| 8555 | 0.2815308824 |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| 8555 | 0.2815308824 |'
- en: '| 8755 | 0.2809165243 |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '| 8755 | 0.2809165243 |'
- en: '| 19422 | 0.248149739 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 19422 | 0.248149739 |'
- en: '| 31914 | 0.2097769366 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 31914 | 0.2097769366 |'
- en: '| 36732 | 0.1949770517 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 36732 | 0.1949770517 |'
- en: '| 39570 | 0.1862593113 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 39570 | 0.1862593113 |'
- en: '| 1230324 | 3.471487079 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 1230324 | 3.471487079 |'
- en: 'The approach has yielded good results: all data points except one are found
    within one standard deviation of the mean, and our outlier is far away from the
    average with a distance of nearly 3.5 deviations. In general, you can consider
    data points more than two or three standard deviations away from the mean to be
    outliers.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法产生了良好的结果：除了一个数据点外，所有数据点都在平均值的一个标准差内，我们的异常值与平均值的距离近3.5个标准差。一般来说，你可以将距离平均值两个或三个标准差以上的数据点视为异常值。
- en: 'If your dataset represents a normal distribution, then you can use the **68-95-99.7** rule:
    68% of data points are expected to be within one standard deviation, 95% are expected
    to be within two deviations, and 99.7% of data points are expected to be within
    three standard deviations. In a normal distribution, therefore, only 0.3% of data
    is expected to be farther than three standard deviations from the mean.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据集代表正态分布，那么你可以使用**68-95-99.7**规则：68%的数据点预计将在一个标准差内，95%预计将在两个标准差内，99.7%的数据点预计将在三个标准差内。因此，在正态分布中，只有0.3%的数据预计将比平均值远三个标准差。
- en: Note that the preceding data presented is not a normal distribution, and much
    of your data will not follow normal distributions either, but the concept of standard
    deviation may still apply (the ratios of data points expected per standard deviation
    will differ based on the distribution).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，前面提供的数据不是正态分布，而且你的大部分数据也不会遵循正态分布，但标准差的概念仍然适用（每个标准差预期的数据点比率将根据分布而有所不同）。
- en: Now that an outlier is identified, a determination must be made as to how to
    handle the outlying data point. In some cases, it's better to keep the outliers
    in your dataset and continue processing as usual; outliers that are based in real
    data are often important data points that can't be ignored, because they represent
    uncommon but possible values for your data.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经识别出异常值，必须确定如何处理这个异常数据点。在某些情况下，最好保留数据集中的异常值并继续正常处理；基于实际数据的异常值通常是重要的数据点，不能被忽略，因为它们代表了数据中不常见但可能出现的值。
- en: For instance, if you're monitoring a server's CPU load average and find an average
    value of 2.0 with a standard deviation of 1.0, you would not want to ignore data
    points with load averages of 10.0—those data points still represent load averages
    that your CPU actually experienced, and for many types of analysis it would be
    self-defeating to ignore that data, even though those points are far away from
    the mean. Those points should be considered and accounted for in your analysis.
    However, in our Twitter followers example we may want to ignore the outlier, especially
    if our analysis is to determine behavioral patterns of Twitter users' audiences—our
    outlier most likely exhibits a completely separate class of behavioral patterns
    that may simply confuse our analysis.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在监控服务器的CPU负载平均值，并发现平均值为2.0，标准差为1.0，你不会想忽略负载平均值为10.0的数据点——这些数据点仍然代表了CPU实际经历的平均负载，对于许多类型的分析，忽略这些数据点可能是自相矛盾的，尽管这些点远离平均值。这些点应该被考虑并在分析中予以考虑。然而，在我们的Twitter粉丝示例中，我们可能希望忽略异常值，特别是如果我们分析的目标是确定Twitter用户受众的行为模式——我们的异常值很可能表现出完全不同的行为模式，这可能会简单地混淆我们的分析。
- en: 'There is another approach to handling outliers that works well when considering
    data that''s expected to be linear, polynomial, exponential, or periodic—the types
    of datasets where a regression can be performed. Consider data that is expected
    to be linear, like the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑预期为线性、多项式、指数或周期性数据时，还有另一种处理异常值的方法，这些数据类型的数据集可以进行回归分析。考虑以下预期为线性的数据：
- en: '| **Observation** | **Value** |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **观察** | **值** |'
- en: '| 1 | 1 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 |'
- en: '| 2 | 2 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 |'
- en: '| 3 | 3 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 3 |'
- en: '| 4 | 4 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 4 |'
- en: '| 5 | 5 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 5 |'
- en: '| 6 | 6 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 6 |'
- en: '| 7 | 22 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 22 |'
- en: '| 8 | 8 |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 8 |'
- en: '| 9 | 9 |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 9 |'
- en: '| 10 | 10 |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 10 |'
- en: 'When performing a linear regression on this data, we can see that the outlying
    data point skews the regression upwards:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在对此数据进行线性回归时，我们可以看到异常数据点使回归向上倾斜：
- en: '![](img/a21613d5-db9b-4a1f-9de0-31722a5c158d.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a21613d5-db9b-4a1f-9de0-31722a5c158d.png)'
- en: For this small set of data points the error in the regression may not be significant,
    but if you're using the regression to extrapolate future values, for instance,
    for observation number 30, the predicted value will be far from the actual value
    as the small error introduced by the outlier compounds the further you extrapolate
    values. In this case, we would want to remove the outlier before performing the
    regression so that the regression's extrapolation is more accurate.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样一组小的数据点，回归中的误差可能并不显著，但如果你使用回归来外推未来的值，例如，对于第30次观察，预测值将远远偏离实际值，因为异常值引入的小误差在外推值的过程中会累积。在这种情况下，我们希望在执行回归之前移除异常值，以便回归的外推更加准确。
- en: 'In order to identify the outlier, we can perform a linear regression as we
    have before, and then calculate the squared error from the trendline for each
    point. If the data point exceeds an error of, for instance, 25%, we can consider
    that point an outlier and remove it before performing the regression a second
    time. Once we''ve removed the outlier and re-performed the regression, the trendline
    fits the data much better:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了识别异常值，我们可以像之前一样执行线性回归，然后计算每个点的趋势线的平方误差。如果数据点超过例如25%的误差，我们可以认为该点为异常值，并在第二次执行回归之前将其移除。一旦我们移除了异常值并重新执行了回归，趋势线将更好地拟合数据：
- en: '![](img/273c129c-5c13-4fec-b3ed-61054c42fb68.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/273c129c-5c13-4fec-b3ed-61054c42fb68.png)'
- en: Transforming and normalizing data
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换和归一化数据
- en: 'The most common preprocessing task is to transform and/or normalize data into
    a representation that can be used by your algorithm. For instance, you may receive
    JSON objects from an API endpoint that you need to transform into vectors used
    by your algorithm. Consider the following JSON data:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的预处理任务是转换和/或归一化数据，使其能够被你的算法使用。例如，你可能从API端点接收JSON对象，需要将其转换为算法使用的向量。考虑以下JSON数据：
- en: '[PRE0]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Your neural network that processes the data expects input data in vector form,
    like so:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您处理数据的神经网络期望以向量形式输入数据，如下所示：
- en: '[PRE1]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'In JavaScript, the easiest way to transform our JSON data in this situation
    is to use the built-in `Array.map` function. The following code will generate
    an array of vectors (an array of arrays). This form of transformation will be
    very common throughout this book:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JavaScript 中，在这种情况下转换我们的 JSON 数据最简单的方法是使用内置的 `Array.map` 函数。以下代码将生成一个向量数组（数组中的数组）。这种转换形式将在本书中非常常见：
- en: '[PRE2]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Note that we are using the shortest form of ES6 arrow functions, which doesn''t
    require parentheses around the parameters nor an explicit return statement, since
    we return our array of features directly. An equivalent ES5 example would look
    like the following:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们正在使用 ES6 箭头函数的最简形式，它不需要在参数周围使用括号，也不需要显式的返回语句，因为我们直接返回特征数组。一个等效的 ES5 示例将如下所示：
- en: '[PRE3]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Also note that the `is_verified` field was converted to an integer using the
    ternary operator, `user.is_verified ? 1 : 0`. Neural networks can only work with
    numeric values, and so we must represent the Boolean value as an integer.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '还请注意，`is_verified` 字段已使用三元运算符转换为整数，即 `user.is_verified ? 1 : 0`。神经网络只能处理数值，因此我们必须将布尔值表示为整数。'
- en: We will discuss techniques for using natural language with neural networks in
    a later chapter.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后面的章节中讨论使用自然语言与神经网络结合的技术。
- en: 'Another common data transformation is to normalize data values into a given
    range, for instance between -1 and +1\. Many algorithms depend on data values
    falling within this range, however, most real-world data does not. Let''s revisit
    our noisy daily sensor data from earlier in the chapter, and let''s assume that
    we have access to this data in a simple JavaScript array called **measurements** (detail-oriented
    readers will notice I changed the value of day `15` as compared with the earlier
    example):'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的数据转换是将数据值归一化到给定范围，例如在 -1 和 +1 之间。许多算法依赖于数据值落在这个范围内，然而，大多数现实世界的数据并不如此。让我们回顾一下本章前面提到的嘈杂的每日传感器数据，并假设我们能够通过一个简单的名为
    **measurements** 的 JavaScript 数组访问这些数据（注重细节的读者会注意到，与前面的示例相比，我已更改了第 15 天的值）：
- en: '| **Day** | **Value** |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| **Day** | **Value** |'
- en: '| 1 | 0.1381426172 |'
  id: totrans-245
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1381426172 |'
- en: '| 2 | 0.5678176776 |'
  id: totrans-246
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.5678176776 |'
- en: '| 3 | 0.3564009968 |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.3564009968 |'
- en: '| 4 | 1.239499423 |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.239499423 |'
- en: '| 5 | 1.267606181 |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1.267606181 |'
- en: '| 6 | 1.440843361 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1.440843361 |'
- en: '| 7 | 0.3322843208 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.3322843208 |'
- en: '| 8 | 0.4329166745 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.4329166745 |'
- en: '| 9 | 0.5499234277 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.5499234277 |'
- en: '| 10 | -0.4016070826 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 10 | -0.4016070826 |'
- en: '| 11 | 0.06216906816 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.06216906816 |'
- en: '| 12 | -0.9689103112 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| 12 | -0.9689103112 |'
- en: '| 13 | -1.170421963 |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '| 13 | -1.170421963 |'
- en: '| 14 | -0.784125647 |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| 14 | -0.784125647 |'
- en: '| 15 | -1.524217169 |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| 15 | -1.524217169 |'
- en: '| 16 | -0.4689120937 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 16 | -0.4689120937 |'
- en: '| 17 | -0.7458561671 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 17 | -0.7458561671 |'
- en: '| 18 | -0.6746415566 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 18 | -0.6746415566 |'
- en: '| 19 | -0.0429460593 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 19 | -0.0429460593 |'
- en: '| 20 | 0.06757010626 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.06757010626 |'
- en: '| 21 | 0.480806698 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 0.480806698 |'
- en: '| 22 | 0.2019759014 |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 0.2019759014 |'
- en: '| 23 | 0.7857692899 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 0.7857692899 |'
- en: '| 24 | 0.725414402 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 0.725414402 |'
- en: '| 25 | 1.188534085 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 1.188534085 |'
- en: '| 26 | 0.458488458 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 0.458488458 |'
- en: '| 27 | 0.3017212831 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 0.3017212831 |'
- en: '| 28 | 0.5249332545 |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| 28 | 0.5249332545 |'
- en: '| 29 | 0.3333153146 |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 29 | 0.3333153146 |'
- en: '| 30 | -0.3517342423 |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| 30 | -0.3517342423 |'
- en: '| 31 | -0.721682062 |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| 31 | -0.721682062 |'
- en: If we wish to normalize this data to the range [-1, +1], we must first discover
    the largest *absolute value* of all numbers in the set, which in this case is
    day 15's value of `-1.52`. If we were to simply use JavaScript's `Math.max` on
    this data, we would find the maximum value on the number line, which is day 6's
    value of `1.44`—however, day `15` is more negative than day `6` is positive.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们希望将此数据归一化到 [-1, +1] 的范围，我们必须首先找到集合中所有数字的最大 *绝对值*，在这个例子中是第 15 天的值 `-1.52`。如果我们简单地使用
    JavaScript 的 `Math.max` 来处理这些数据，我们会找到数轴上的最大值，即第 6 天的值 `1.44`——然而，第 15 天的负值比第 6
    天的正值更负。
- en: 'Finding the maximum absolute value in a JavaScript array can be accomplished
    with the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在 JavaScript 数组中找到最大绝对值可以通过以下方式实现：
- en: '[PRE4]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The value of `absolute_max` will be +1.524217169—the number became positive
    when we called `Math.abs` using `measurements.map`. It is important that the absolute
    maximum value remains positive, because in the next step we will divide by the
    maximum and want to preserve the signs of all data points.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`absolute_max` 的值将是 +1.524217169——当我们使用 `measurements.map` 调用 `Math.abs` 时，这个数字变成了正数。保持绝对最大值正数非常重要，因为在下一步中我们将除以最大值，并希望保留所有数据点的符号。'
- en: 'Given the absolute maximum value, we can normalize our data points like so:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 给定绝对最大值，我们可以这样规范化我们的数据点：
- en: '[PRE5]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'By dividing each number by the maximum value in the set, we ensure that all
    values lie in the range [-1, +1]. The maximum value will be (in this case) -1,
    and all other numbers in the set will be closer to 0 than the maximum will. After
    normalizing, our data now looks like this:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将每个数字除以集合中的最大值，我们确保所有值都位于范围[-1, +1]内。最大值将是（在这种情况下）-1，集合中的其他所有数字将比最大值更接近0。规范化后，我们的数据现在看起来像这样：
- en: '| **Day** | **Value** | **Normalized** |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| **Day** | **Value** | **Normalized** |'
- en: '| 1 | 0.1381426172 | 0.09063184696 |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1381426172 | 0.09063184696 |'
- en: '| 2 | 0.5678176776 | 0.3725306927 |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.5678176776 | 0.3725306927 |'
- en: '| 3 | 0.3564009968 | 0.2338256018 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.3564009968 | 0.2338256018 |'
- en: '| 4 | 1.239499423 | 0.8132039508 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.239499423 | 0.8132039508 |'
- en: '| 5 | 1.267606181 | 0.8316440777 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 1.267606181 | 0.8316440777 |'
- en: '| 6 | 1.440843361 | 0.9453005718 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 1.440843361 | 0.9453005718 |'
- en: '| 7 | 0.3322843208 | 0.218003266 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0.3322843208 | 0.218003266 |'
- en: '| 8 | 0.4329166745 | 0.284025586 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0.4329166745 | 0.284025586 |'
- en: '| 9 | 0.5499234277 | 0.3607907319 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0.5499234277 | 0.3607907319 |'
- en: '| 10 | -0.4016070826 | -0.2634841615 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 10 | -0.4016070826 | -0.2634841615 |'
- en: '| 11 | 0.06216906816 | 0.04078753963 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0.06216906816 | 0.04078753963 |'
- en: '| 12 | -0.9689103112 | -0.6356773373 |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 12 | -0.9689103112 | -0.6356773373 |'
- en: '| 13 | -1.170421963 | -0.7678839913 |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 13 | -1.170421963 | -0.7678839913 |'
- en: '| 14 | -0.784125647 | -0.5144448332 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 14 | -0.784125647 | -0.5144448332 |'
- en: '| 15 | -1.524217169 | -1 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 15 | -1.524217169 | -1 |'
- en: '| 16 | -0.4689120937 | -0.3076412623 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 16 | -0.4689120937 | -0.3076412623 |'
- en: '| 17 | -0.7458561671 | -0.4893372037 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 17 | -0.7458561671 | -0.4893372037 |'
- en: '| 18 | -0.6746415566 | -0.4426151145 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 18 | -0.6746415566 | -0.4426151145 |'
- en: '| 19 | -0.0429460593 | -0.02817581391 |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 19 | -0.0429460593 | -0.02817581391 |'
- en: '| 20 | 0.06757010626 | 0.04433102293 |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.06757010626 | 0.04433102293 |'
- en: '| 21 | 0.480806698 | 0.3154450087 |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 21 | 0.480806698 | 0.3154450087 |'
- en: '| 22 | 0.2019759014 | 0.1325112363 |'
  id: totrans-305
  prefs: []
  type: TYPE_TB
  zh: '| 22 | 0.2019759014 | 0.1325112363 |'
- en: '| 23 | 0.7857692899 | 0.5155231854 |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| 23 | 0.7857692899 | 0.5155231854 |'
- en: '| 24 | 0.725414402 | 0.4759258831 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 24 | 0.725414402 | 0.4759258831 |'
- en: '| 25 | 1.188534085 | 0.7797668924 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 25 | 1.188534085 | 0.7797668924 |'
- en: '| 26 | 0.458488458 | 0.3008025808 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 26 | 0.458488458 | 0.3008025808 |'
- en: '| 27 | 0.3017212831 | 0.1979516366 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 27 | 0.3017212831 | 0.1979516366 |'
- en: '| 28 | 0.5249332545 | 0.3443953167 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 28 | 0.5249332545 | 0.3443953167 |'
- en: '| 29 | 0.3333153146 | 0.2186796747 |'
  id: totrans-312
  prefs: []
  type: TYPE_TB
  zh: '| 29 | 0.3333153146 | 0.2186796747 |'
- en: '| 30 | -0.3517342423 | -0.2307638633 |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '| 30 | -0.3517342423 | -0.2307638633 |'
- en: '| 31 | -0.721682062 | -0.4734771901 |'
  id: totrans-314
  prefs: []
  type: TYPE_TB
  zh: '| 31 | -0.721682062 | -0.4734771901 |'
- en: 'There are no data points outside of the [-1, +1] range, and you can also see
    that day `15`, with the maximum absolute value of the data, has been normalized
    as `-1`. Graphing the data shows the relationship between the original and normalized
    values:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据点位于[-1, +1]范围之外，你还可以看到，数据绝对值最大的第15天已经被规范化为`-1`。绘制数据显示了原始值和规范化值之间的关系：
- en: '![](img/4abf6168-227e-42e3-a946-48011ad77e60.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4abf6168-227e-42e3-a946-48011ad77e60.png)'
- en: The shape of the data has been preserved, and the chart has simply been scaled
    by a constant factor. This data is now ready to use in algorithms that require
    normalized ranges, such as PCA, for instance.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的形状已经保留，图表只是通过一个常数因子进行了缩放。现在这些数据可以用于需要规范化范围的算法，例如PCA。
- en: Your data is likely much more complex than these preceding examples. Perhaps
    your JSON data is composed of complex objects with nested entities and arrays.
    You may need to run an analysis on only those items which have specific sub-elements,
    or you may need to generate dynamic subsets of data based on some user-provided
    query or filter.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你的数据可能比这些先前的例子复杂得多。也许你的JSON数据由复杂的对象组成，其中嵌套了实体和数组。你可能需要对具有特定子元素的项目进行分析，或者你可能需要根据用户提供的查询或过滤器生成数据的动态子集。
- en: 'For complex situations and datasets, you may want some help from a third-party
    library such as `DataCollection.js`, which is a library that adds SQL and NoSQL
    style query functionality to JavaScript arrays. Imagine that our preceding JSON
    data of **users** also contained an object called **locale** which gives the user''s
    country and language:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 对于复杂的情况和数据集，你可能需要第三方库的帮助，例如`DataCollection.js`，这是一个向JavaScript数组添加SQL和NoSQL风格查询功能的库。想象一下，我们之前的**用户**JSON数据还包含一个名为**locale**的对象，它提供了用户的国籍和语言：
- en: '[PRE6]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To find only users whose language is `en_US`, you could perform the following
    query using `DataCollection.js`:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到语言为`en_US`的用户，你可以使用`DataCollection.js`执行以下查询：
- en: '[PRE7]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Of course, you can accomplish the aforementioned in pure JavaScript easily:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以轻松地在纯JavaScript中完成上述操作：
- en: '[PRE8]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: However, the pure JavaScript version needs some tedious modifications to be
    resilient against undefined or null `locale` objects, and of course more complicated
    filters become ever more tedious to write in pure JavaScript. Most of the time,
    we will use pure JavaScript for the examples in this book, however, our examples
    will be contrived and much cleaner than real-world use cases; use a tool such
    as `DataCollection.js`, if you feel you need it.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，纯JavaScript版本需要对未定义或null `locale`对象进行一些繁琐的修改，以使其具有弹性，当然，在纯JavaScript中编写更复杂的过滤器变得越来越繁琐。大多数时候，我们将使用纯JavaScript来展示本书中的示例，然而，我们的示例将是人为设计的，并且比现实世界的用例要干净得多；如果你觉得需要，可以使用像`DataCollection.js`这样的工具。
- en: Summary
  id: totrans-326
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed data preprocessing, or the art of delivering
    the most useful possible data to our machine learning algorithms. We discussed
    the importance of appropriate feature selection and the relevance of feature selection,
    both to overfitting and to the curse of dimensionality. We looked at correlation
    coefficients as a technique to help us determine the appropriate features to select,
    and also discussed more sophisticated wrapper methods for feature selection, such
    as using a genetic algorithm to determine the optimal set of features to choose.
    We then discussed the more advanced topic of feature extraction, which is a category
    of algorithms that can be used to combine multiple features into new individual
    features, further reducing the dimensionality of the data.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了数据预处理，即向我们的机器学习算法提供尽可能有用的数据的艺术。我们讨论了适当特征选择的重要性以及特征选择的相关性，无论是对于过拟合还是对于维度灾难。我们探讨了相关系数作为帮助我们确定要选择适当特征的技术，并讨论了更复杂的包装方法用于特征选择，例如使用遗传算法来确定要选择的最佳特征集。然后我们讨论了更高级的主题——特征提取，这是一类可以将多个特征组合成新的单个特征，从而进一步降低数据维度的算法。
- en: We then looked at some common scenarios you might face when dealing with real-world
    datasets, such as missing values, outliers, and measurement noise. We discussed
    various techniques you can use to correct for those issues. We also discussed
    common data transformations and normalizations you may need to perform, such as
    normalizing values to a range or vectorizing objects.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着探讨了在处理现实世界数据集时可能会遇到的一些常见场景，例如缺失值、异常值和测量噪声。我们讨论了你可以使用的各种技术来纠正这些问题。我们还讨论了你可能需要执行的一些常见数据转换和归一化，例如将值归一化到某个范围或对对象进行矢量化。
- en: In the next chapter, we will look at machine learning in broad strokes and begin
    to introduce specific algorithms and their applications.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从宏观角度探讨机器学习，并开始介绍具体的算法及其应用。
