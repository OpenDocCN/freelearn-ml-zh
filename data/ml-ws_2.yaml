- en: 2\. Unsupervised Learning – Real-Life Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2\. 无监督学习 – 现实生活中的应用
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: This chapter explains the concept of clustering in machine learning. It explains
    three of the most common clustering algorithms, with a hands-on approximation
    to solve a real-life data problem. By the end of this chapter, you should have
    a firm understanding of how to create clusters out of a dataset using the k-means,
    mean-shift, and DBSCAN algorithms, as well as the ability to measure the accuracy
    of those clusters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍机器学习中的聚类概念。它解释了三种最常见的聚类算法，并通过实际数据问题的近似解决方案进行了实际操作。通过本章的学习，您应该能够深入了解如何使用k-means、均值漂移和DBSCAN算法从数据集中创建簇，以及如何衡量这些簇的准确性。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: In the previous chapter, we learned how to represent data in a tabular format,
    created features and target matrices, pre-processed data, and learned how to choose
    the algorithm that best suits the problem at hand. We also learned how the scikit-learn
    API works and why it is easy to use, as well as the difference between supervised
    and unsupervised learning.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们学习了如何以表格格式表示数据，创建特征和目标矩阵，预处理数据，并学习了如何选择最适合问题的算法。我们还学习了scikit-learn API的工作原理以及为什么易于使用，以及监督学习和无监督学习之间的区别。
- en: 'This chapter focuses on the most important task in the field of unsupervised
    learning: clustering. Consider a situation in which you are a store owner wanting
    to make a targeted social media campaign to promote selected products to certain
    customers. Using clustering algorithms, you would be able to create subgroups
    of your customers, allowing you to profile those subgroups and target them accordingly.
    The main objective of this chapter is to solve a case study, where you will implement
    three different unsupervised learning solutions. These different applications
    serve to demonstrate the uniformity of the scikit-learn API, as well as to explain
    the steps taken to solve machine learning problems. By the end of this chapter,
    you will be able to understand the use of unsupervised learning to comprehend
    data in order to make informed decisions.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章侧重于无监督学习领域中最重要的任务：聚类。考虑一种情况，您是一家店铺的所有者，希望通过定向社交媒体活动来促销特定产品给某些客户。使用聚类算法，您将能够创建客户的子群体，从而可以根据客户的特点来定位和目标客户。本章的主要目标是解决一个案例研究，您将在其中实施三种不同的无监督学习解决方案。这些不同的应用程序旨在演示scikit-learn
    API的一致性，以及解决机器学习问题所需的步骤。通过本章，您将能够了解使用无监督学习来理解数据以便做出明智决策。
- en: Clustering
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: '**Clustering** is a type of unsupervised learning technique where the objective
    is to arrive at conclusions based on the patterns found within unlabeled input
    data. This technique is mainly used to segregate large data into subgroups in
    order to make informed decisions.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一种无监督学习技术，其目标是基于未标记的输入数据中发现的模式得出结论。这种技术主要用于将大数据分成子群体，以便做出明智的决策。'
- en: For instance, from a large list of restaurants in a city, it would be useful
    to segregate the data into subgroups (clusters) based on the type of food, quantity
    of clients, and style of experience, in order to be able to offer each cluster
    a service that's been configured to its specific needs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，从一个城市的大型餐厅列表中，根据食物类型、客户数量和体验风格，将数据分成子群体（簇）将非常有用，以便为每个簇提供针对其特定需求配置的服务。
- en: Clustering algorithms divide the data points into *n* number of clusters so
    that the data points in the same cluster have similar features, whereas they differ
    significantly from the data points in other clusters.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法将数据点划分为*n*个簇，使得同一簇内的数据点具有相似的特征，而与其他簇的数据点显著不同。
- en: Clustering Types
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类类型
- en: Clustering algorithms can classify data points using a methodology that is either
    **hard** or **soft**. The former designates data points completely to a cluster,
    whereas the latter method calculates the probability of each data point belonging
    to each cluster. For example, for a dataset containing customer's past orders
    that are divided into eight subgroups (clusters), hard clustering occurs when
    each customer is placed inside one of the eight clusters. On the other hand, soft
    clustering assigns each customer a probability of belonging to each of the eight
    clusters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法可以使用**硬**或**软**的方法对数据点进行分类。前者将数据点完全指定给某一个簇，而后者则计算每个数据点属于每个簇的概率。例如，对于一个包含顾客过去订单的数据集，这些订单被分成八个子组（簇），硬聚类发生在每个顾客被放入八个簇中的其中一个。而软聚类则为每个顾客分配一个属于八个簇中每个簇的概率。
- en: 'Considering that clusters are created based on the similarity between data
    points, clustering algorithms can be further divided into several groups, depending
    on the set of rules used to measure similarity. Four of the most commonly known
    sets of rules are explained as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到簇是基于数据点之间的相似性创建的，聚类算法可以根据用于衡量相似性的规则集进一步划分为几类。以下是四种最常见的规则集的解释：
- en: '**Connectivity-based models**: This model''s approach to similarity is based
    on proximity in a data space. The creation of clusters can be done by assigning
    all data points to a single cluster and then partitioning the data into smaller
    clusters as the distance between data points increases. Likewise, the algorithm
    can also start by assigning each data point an individual cluster, and then aggregating
    data points that are close by. An example of a connectivity-based model is hierarchical
    clustering.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于连通性的模型**：该模型的相似性方法基于数据空间中的接近度。簇的创建可以通过将所有数据点分配到一个簇中，然后随着数据点之间的距离增加，将数据划分为更小的簇来实现。同样，算法也可以从为每个数据点分配一个独立的簇开始，然后将接近的数据点合并在一起。基于连通性的模型的一个例子是层次聚类。'
- en: '**Density-based models**: As the name suggests, these models define clusters
    by their density in the data space. This means that areas with a high density
    of data points will become clusters, which are typically separated from one another
    by low-density areas. An example of this is the DBSCAN algorithm, which will be
    covered later in this chapter.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的模型**：顾名思义，这些模型通过数据空间中的密度来定义簇。这意味着数据点密度较高的区域将形成簇，簇通常会被低密度区域分开。一个例子是DBSCAN算法，本章稍后将详细介绍。'
- en: '**Distribution-based models**: Models that fall into this category are based
    on the probability that all the data points from a cluster follow the same distribution,
    such as a Gaussian distribution. An example of such a model is the Gaussian Mixture
    algorithm, which assumes that all data points come from a mixture of a finite
    number of Gaussian distributions.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于分布的模型**：属于此类别的模型基于所有来自同一簇的数据点遵循相同分布的概率，例如高斯分布。此类模型的一个例子是高斯混合算法，它假设所有数据点来自有限数量的高斯分布的混合体。'
- en: '**Centroid-based models**: These models are based on algorithms that define
    a centroid for each cluster, which is updated constantly by an iterative process.
    The data points are assigned to the cluster where their proximity to the centroid
    is minimized. An example of such a model is the k-means algorithm, which will
    be discussed later in this chapter.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于质心的模型**：这些模型基于定义每个簇的质心的算法，该质心通过迭代过程不断更新。数据点会被分配到与其质心的距离最小的簇。此类模型的一个例子是k均值算法，本章稍后会讨论。'
- en: In conclusion, data points are assigned to clusters based on their similarity
    to each other and their difference from data points in other clusters. This classification
    into clusters can be either absolute or variably distributed by determining the
    probability of each data point belonging to each cluster.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，数据点根据它们之间的相似性以及与其他簇中数据点的差异来分配到不同的簇中。这样的簇划分可以是绝对的，也可以通过确定每个数据点属于每个簇的概率来进行变动。
- en: Moreover, there is no fixed set of rules to determine similarity between data
    points, which is why different clustering algorithms use different rules. Some
    of the most commonly known sets of rules are connectivity-based, density-based,
    distribution-based, and centroid-based.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，并没有一套固定的规则来确定数据点之间的相似性，这也是不同聚类算法使用不同规则的原因。一些最常见的规则集包括基于连接性、基于密度、基于分布和基于质心的规则。
- en: Applications of Clustering
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类的应用
- en: 'As with all machine learning algorithms, clustering has many applications in
    different fields, some of which are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有机器学习算法一样，聚类在不同领域有着广泛的应用，以下是其中的一些应用：
- en: '**Search engine results**: Clustering can be used to generate search engine
    results containing keywords that are approximate to the keywords searched by the
    user and ordered as per the search result with greater similarity. Consider Google
    as an example; it uses clustering not only for retrieving results but also for
    suggesting new possible searches.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**搜索引擎结果**：聚类可以用于生成包含与用户搜索关键词相近的关键词的搜索引擎结果，并按与搜索结果的相似度进行排序。以谷歌为例，它不仅使用聚类来检索结果，还用聚类来建议新的可能搜索内容。'
- en: '**Recommendation programs**: It can also be used in recommendation programs
    that cluster together, for instance, people that fall into a similar profile,
    and then make recommendations based on the products that each member of the cluster
    has bought. Consider Amazon, for example, which recommends more items based on
    your purchase history and the purchases of similar users.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推荐程序**：聚类也可以用于推荐程序，例如将具有相似特征的用户聚集在一起，然后根据每个成员的购买历史进行推荐。以亚马逊为例，它根据你的购买历史以及相似用户的购买记录推荐更多商品。'
- en: '**Image recognition**: This is where clusters are used to group images that
    are considered to be similar. For instance, Facebook uses clustering to help suggest
    who is present in a picture.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像识别**：聚类可用于将相似的图像分组。例如，Facebook使用聚类来帮助识别图片中的人物。'
- en: '**Market segmentation**: Clustering can also be used for market segmentation
    to divide a list of prospects or clients into subgroups in order to provide a
    customized experience or product. For example, Adobe uses clustering analysis
    to segment customers in order to target them differently by recognizing those
    who are more willing to spend money.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场细分**：聚类还可以用于市场细分，将潜在客户或客户列表分成子群体，以提供定制化的体验或产品。例如，Adobe利用聚类分析对客户进行细分，从而根据客户的消费意愿进行不同的定位。'
- en: The preceding examples demonstrate that clustering algorithms can be used to
    solve different data problems in different industries, with the primary purpose
    of understanding large amounts of historical data that, in some cases, can be
    used to classify new instances.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 上述示例表明，聚类算法可以用来解决不同行业中的不同数据问题，其主要目的是理解大量的历史数据，这些数据在某些情况下可以用来分类新的实例。
- en: Exploring a Dataset – Wholesale Customers Dataset
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索数据集 – 批发客户数据集
- en: As part of the process of learning the behavior and applications of clustering
    algorithms, the following sections of this chapter will focus on solving a real-life
    data problem using the Wholesale Customers dataset, which is available at the
    UC Irvine Machine Learning Repository.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 作为学习聚类算法的行为和应用的一部分，本章的以下部分将重点解决一个实际的数据问题，使用的是在UC Irvine机器学习仓库中提供的批发客户数据集。
- en: Note
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Datasets in repositories may contain raw, partially pre-processed, or pre-processed
    data. To use any of these datasets, ensure that you read the specifications of
    the data that's available to understand the process that needs to be followed
    to model the data effectively, or whether it is the right dataset for your purpose.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中的数据集可能包含原始数据、部分预处理数据或已处理数据。在使用这些数据集时，请确保阅读可用数据的规格说明，以了解有效建模数据的过程，或者判断该数据集是否适合你的目的。
- en: 'For instance, the current dataset is an extract from a larger dataset, as per
    the following citation:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，当前数据集是一个来自更大数据集的提取，引用如下：
- en: 'The dataset originates from a larger database referred on: Abreu, N. (2011).
    Analise do perfil do cliente Recheio e desenvolvimento de um sistema promocional.
    Mestrado em Marketing, ISCTE-IUL, Lisbon.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集来源于一个更大的数据库，相关文献为：Abreu, N. (2011). 《客户Recheio的分析与促销系统的开发》。市场营销硕士，ISCTE-IUL，里斯本。
- en: 'In the following section, we will analyze the contents of the dataset, which
    will then be used in *Activity 2.01*, *Using Data Visualization to Aid the Pre-processing
    Process*. To download a dataset from the UC Irvine Machine Learning Repository,
    perform the following steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分，我们将分析数据集的内容，然后将其用于*活动 2.01*，*使用数据可视化辅助预处理过程*。要从 UC Irvine 机器学习库下载数据集，请执行以下步骤：
- en: 'Access the following link: [http://archive.ics.uci.edu/ml/datasets/Wholesale+customers](http://archive.ics.uci.edu/ml/datasets/Wholesale+customers).'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 访问以下链接：[http://archive.ics.uci.edu/ml/datasets/Wholesale+customers](http://archive.ics.uci.edu/ml/datasets/Wholesale+customers)。
- en: Below the dataset's title, find the download section and click on `Data Folder`.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在数据集标题下方，找到下载部分并点击`Data Folder`。
- en: Click on the `Wholesale Customers data.csv` file to trigger the download and
    save the file in the same path as that of your current Jupyter Notebook.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击`Wholesale Customers data.csv`文件以触发下载并将文件保存在当前 Jupyter Notebook 的相同路径下。
- en: Note
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can also access it by going to this book''s GitHub repository: [https://packt.live/3c3hfKp](https://packt.live/3c3hfKp
    )'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以通过访问这本书的 GitHub 存储库来访问它：[https://packt.live/3c3hfKp](https://packt.live/3c3hfKp
    )
- en: Understanding the Dataset
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'Each step will be explained generically and will then be followed by an explanation
    of its application in the current case study (the Wholesale Customers dataset):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 每一步都将以通用方式进行解释，然后会对其在当前案例研究（批发客户数据集）中的应用进行解释：
- en: First of all, it is crucial to understand the way in which data is presented
    by the person who's responsible for gathering and maintaining it.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，理解负责收集和维护数据的人员呈现数据的方式至关重要。
- en: Considering that the dataset of the case study was obtained from an online repository,
    the format in which it is presented must be understood. The Wholesale Customers
    dataset consists of a snippet of historical data of clients from a wholesale distributor.
    It contains a total of 440 instances (each row) and eight features (each column).
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到案例研究的数据集是从在线资源库获取的，必须理解其呈现的格式。批发客户数据集包含批发分销商客户的历史数据片段。它包含总共 440 个实例（每一行）和八个特征（每一列）。
- en: Next, it is important to determine the purpose of the study, which is dependent
    on the data that's available. Even though this might seem like a redundant statement,
    many data problems become problematic because the researcher does not have a clear
    view of the purpose of the study, and hence the pre-processing methodology, the
    model, and the performance metrics are chosen incorrectly.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，确定研究的目的非常重要，这取决于可用的数据。尽管这可能看起来像是一个冗余的陈述，但许多数据问题变得棘手，因为研究人员对研究目的没有清晰的了解，从而选择了错误的预处理方法、模型和性能指标。
- en: The purpose of using clustering algorithms on the Wholesale Customers dataset
    is to understand the behavior of each customer. This will allow you to group customers
    with similar behaviors into one cluster. The behavior of a customer will be defined
    by how much they spent on each category of product, as well as the channel and
    the region where they bought products.
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在批发客户数据集上使用聚类算法的目的是理解每个客户的行为。这将使您能够将具有相似行为的客户分组到一个簇中。客户的行为将由他们在每个产品类别上的支出量、以及购买产品的渠道和地区来定义。
- en: 'Subsequently explore all the features that are available. This is mainly done
    for two reasons: first, to rule out features that are considered to be of low
    relevance based on the purpose of the study or that are considered to be redundant,
    and second, to understand the way the values are presented to determine some of
    the pre-processing techniques that may be needed.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随后探索所有可用的特征。这主要是出于两个原因：首先，根据研究目的排除被认为具有低相关性的特征或被认为是多余的特征，其次，理解呈现值的方式以确定可能需要的一些预处理技术。
- en: 'The current case study has eight features, each one of which is considered
    to be relevant to the purpose of the study. Each feature is explained in the following table:'
  id: totrans-45
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 当前案例研究具有八个特征，每个特征都被认为与研究目的相关。下表对每个特征进行了解释：
- en: '![Figure 2.1: A table explaining the features in the case study](img/B15781_02_01.jpg)'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 2.1：案例研究中特征解释的表格](img/B15781_02_01.jpg)'
- en: 'Figure 2.1: A table explaining the features in the case study'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：案例研究中特征解释的表格
- en: In the preceding table, no features are to be dismissed, and nominal (categorical)
    features have already been handled by the author of the dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表格中，没有特征需要被忽略，且名义（分类）特征已经由数据集的作者处理。
- en: As a summary, the first thing to do when choosing a dataset or being handed
    one is to understand the characteristics that are visible at first glance, which
    involves recognizing the information available, then determining the purpose of
    the project, and finally revising the features to select those that will be part
    of the study. After this, the data can be visualized so that it can be understood
    before it's pre-processed.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，选择数据集或收到数据集时，首先需要了解初步可见的特征，这包括识别可用信息，然后确定项目的目的，最后修改特征，选择那些将用于研究的特征。之后，数据可以进行可视化，以便在预处理之前理解数据。
- en: Data Visualization
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据可视化
- en: 'Once data has been revised to ensure that it can be used for the desired purpose,
    it is time to load the dataset and use data visualization to further understand
    it. Data visualization is not a requirement for developing a machine learning
    project, especially when dealing with datasets with hundreds or thousands of features.
    However, it has become an integral part of machine learning, mainly for visualizing
    the following:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据被修改以确保能够用于预期目的，就可以加载数据集并使用数据可视化进一步理解数据。数据可视化并不是开发机器学习项目的必需步骤，尤其是在处理具有数百或数千个特征的数据集时。然而，它已成为机器学习的重要组成部分，主要用于可视化以下内容：
- en: Specific features that are causing trouble (for example, those that contain
    many missing or outlier values) and how to deal with them.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定的导致问题的特征（例如，包含大量缺失值或异常值的特征）以及如何处理这些问题。
- en: The results from the model, such as the clusters that have been created or the
    number of predicted instances for each labeled category.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自模型的结果，例如已创建的聚类或每个标签类别的预测实例数量。
- en: The performance of the model, in order to see the behavior along different iterations.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的性能，以便查看不同迭代中的行为。
- en: Data visualization's popularity in the aforementioned tasks can be explained
    by the fact that the human brain processes information easily when it is presented
    as charts or graphs, which allows us to have a general understanding of the data.
    It also helps us to identify areas that require attention, such as outliers.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 数据可视化在上述任务中的流行，可以通过人脑在图表或图形的呈现下容易处理信息来解释，这使得我们能够对数据有一个总体的理解。它还帮助我们识别需要注意的领域，比如异常值。
- en: Loading the Dataset Using pandas
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用pandas加载数据集
- en: One way of storing a dataset to easily manage it is by using pandas DataFrames.
    These work as two-dimensional size-mutable matrices with labeled axes. They facilitate
    the use of different pandas functions to modify the dataset for pre-processing
    purposes.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一种便于管理数据集的存储方式是使用pandas DataFrame。这些工作方式类似于具有标签轴的二维大小可变矩阵。它们便于使用不同的pandas函数来修改数据集以进行预处理。
- en: 'Most datasets found in online repositories or gathered by companies for data
    analysis are in **Comma-Separated Values** (**CSV**) files. CSV files are text
    files that display the data in the form of a table. Columns are separated by commas
    (,) and rows are on separate lines:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数在线存储库中找到的数据集，或公司为数据分析收集的数据集，都存储在**逗号分隔值**（**CSV**）文件中。CSV文件是以表格形式显示数据的文本文件。列由逗号（,）分隔，行位于不同的行上：
- en: '![Figure 2.2: A screenshot of a CSV file'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2：CSV文件的屏幕截图]'
- en: '](img/B15781_02_02.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_02_02.jpg)'
- en: 'Figure 2.2: A screenshot of a CSV file'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2：CSV文件的屏幕截图
- en: Loading a dataset stored in a CSV file and placing it into a DataFrame is extremely
    easy with the pandas `read_csv()` function. It receives the path to your file
    as an argument.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用pandas的`read_csv()`函数加载存储在CSV文件中的数据集并将其放入DataFrame中是非常简单的。它接收文件路径作为参数。
- en: Note
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When datasets are stored in different forms of files, such as in Excel or SQL
    databases, use the pandas `read_xlsx()` or `read_sql()` function, respectively.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集存储在不同格式的文件中时，如Excel或SQL数据库，分别使用pandas的`read_xlsx()`或`read_sql()`函数。
- en: 'The following code shows how to load a dataset using `pandas`:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用`pandas`加载数据集：
- en: '[PRE0]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: First of all, pandas is imported. Next, the path to the file is defined in order
    to input it into the `read_csv()` function. Finally, the type of the `data` variable
    is printed to verify that a Pandas DataFrame has been created.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入pandas库。接下来，定义文件的路径，以便将其输入到`read_csv()`函数中。最后，打印`data`变量的类型，以验证已创建一个Pandas
    DataFrame。
- en: 'The output is as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE1]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As shown in the preceding snippet, the variable named `data` is of a pandas DataFrame.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如前面的代码片段所示，名为 `data` 的变量是 pandas DataFrame 类型。
- en: Visualization Tools
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可视化工具
- en: There are different open source visualization libraries available, from which
    seaborn and matplotlib stand out. In the previous chapter, seaborn was used to
    load and display data; however, from this section onward, matplotlib will be used
    as our visualization library of choice. This is mainly because seaborn is built
    on top of matplotlib with the sole purpose of introducing a couple of plot types
    and to improve the format of the displays. Therefore, once you've learned about
    matplotlib, you will also be able to import seaborn to improve the visual quality
    of your plots.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有不同的开源可视化库，其中 seaborn 和 matplotlib 很突出。在上一章中，使用 seaborn 加载并显示数据；然而，从本节开始，我们将选择
    matplotlib 作为我们的可视化库。这主要是因为 seaborn 是基于 matplotlib 构建的，目的是引入几种绘图类型并改善显示格式。因此，一旦你学习了
    matplotlib，你也可以导入 seaborn 来提高绘图的视觉质量。
- en: Note
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information about the seaborn library, visit the following link: [https://seaborn.pydata.org/](https://seaborn.pydata.org/).'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 有关 seaborn 库的更多信息，请访问以下链接：[https://seaborn.pydata.org/](https://seaborn.pydata.org/)。
- en: In general terms, matplotlib is an easy-to-use Python library that prints 2D
    quality figures. For simple plotting, the `pyplot` model of the library will suffice.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，matplotlib 是一个易于使用的 Python 库，用于绘制 2D 高质量图形。对于简单的绘图，库的 `pyplot` 模块就足够了。
- en: 'Some of the most commonly used plot types are explained in the following table:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格解释了一些最常用的绘图类型：
- en: '![Figure 2.3: A table listing the commonly used plot types (*)'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.3：列出常用绘图类型的表格（*）'
- en: '](img/B15781_02_03.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_02_03.jpg)'
- en: 'Figure 2.3: A table listing the commonly used plot types (*)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：列出常用绘图类型的表格（*）
- en: The functions in the third column can be used after importing matplotlib and
    its `pyplot` model.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第三列中的函数在导入 matplotlib 及其 `pyplot` 模块后可以使用。
- en: Note
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Access matplotlib's documentation regarding the type of plot that you wish to
    use at [https://matplotlib.org/](https://matplotlib.org/) so that you can play
    around with the different arguments and functions that you can use to edit the
    result of your plot.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 matplotlib 的文档，了解您希望使用的绘图类型：[https://matplotlib.org/](https://matplotlib.org/)，以便您可以尝试不同的参数和函数来编辑绘图结果。
- en: 'Exercise 2.01: Plotting a Histogram of One Feature from the Circles Dataset'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.01：绘制 Circles 数据集中的一个特征的直方图
- en: 'In this exercise, we will be plotting a histogram of one feature from the circles
    dataset. Perform the following steps to complete this exercise:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将绘制 circles 数据集中的一个特征的直方图。执行以下步骤以完成此练习：
- en: Note
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Use the same Jupyter Notebook for all the exercises within this chapter. The `circles.csv`
    file is available at [https://packt.live/2xRg3ea](https://packt.live/2xRg3ea).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对本章中的所有练习使用相同的 Jupyter Notebook。`circles.csv` 文件可以在 [https://packt.live/2xRg3ea](https://packt.live/2xRg3ea)
    下载。
- en: For all the exercises and activities within this chapter, you will need to have
    Python 3.7, matplotlib, NumPy, Jupyter, and pandas installed on your system.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的所有练习和活动，您需要在系统中安装 Python 3.7、matplotlib、NumPy、Jupyter 和 pandas。
- en: Open a Jupyter Notebook to implement this exercise.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个 Jupyter Notebook 来实现此练习。
- en: 'First, import all of the libraries that you are going to be using by typing
    the following code:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，通过输入以下代码来导入你将要使用的所有库：
- en: '[PRE2]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `pandas` library is used to save the dataset into a DataFrame, `matplotlib`
    is used for visualization, and NumPy is used in later exercises of this chapter,
    but since the same Notebook will be used, it has been imported here.
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`pandas` 库用于将数据集保存为 DataFrame，`matplotlib` 用于可视化，NumPy 在本章后面的练习中会使用，但由于使用的是相同的
    Notebook，这里已经导入了。'
- en: 'Load the circles dataset by using Pandas'' `read_csv` function. Type in the
    following code:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Pandas 的 `read_csv` 函数加载 circles 数据集。输入以下代码：
- en: '[PRE3]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A variable named `data` is created to store the circles dataset. Finally, a
    scatter plot is drawn to display the data points in a data space, where the first
    element is the first column of the dataset and the second element is the second
    column of the dataset, creating a two-dimensional plot:'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 创建一个名为 `data` 的变量来存储 circles 数据集。最后，绘制一个散点图来显示数据空间中的数据点，其中第一个元素是数据集的第一列，第二个元素是数据集的第二列，从而创建一个二维图：
- en: Note
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The Matplotlib's `show()` function is used to trigger the display of the plot,
    considering that the preceding lines only create it. When programming in Jupyter
    Notebooks, using the `show()` function is not required, but it is good practice
    to use it since, in other programming environments, it is required to use the
    function to be able to display the plots. This will also allow flexibility in
    the code. Also, in Jupyter Notebooks, this function results in a much cleaner
    output.
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Matplotlib的`show()`函数用于触发图表的显示，考虑到前面的代码行只是创建了图表。在Jupyter Notebooks中，使用`show()`函数并非必需，但它是一个好习惯，因为在其他编程环境中，必须使用此函数才能显示图表。这也能提高代码的灵活性。另外，在Jupyter
    Notebooks中，使用此函数会使输出更干净。
- en: '![Figure 2.4: A scatter plot of the circles dataset'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.4: 圆形数据集的散点图'
- en: '](img/B15781_02_04.jpg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_02_04.jpg)'
- en: 'Figure 2.4: A scatter plot of the circles dataset'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '图 2.4: 圆形数据集的散点图'
- en: The final output is a dataset with two features and 1,500 instances. Here, the
    dot represents a data point (an observation), where the location is marked by
    the values of each of the features of the dataset.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 最终输出是一个包含两个特征和1,500个实例的数据集。在这里，点代表一个数据点（一个观察值），其位置由数据集中每个特征的值标记。
- en: 'Create a histogram out of one of the two features. Use slicing to select the
    feature that you wish to plot:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从两个特征中的一个创建直方图。使用切片选择你想要绘制的特征：
- en: '[PRE4]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The plot will look similar to the one shown in the following graph:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该图将类似于以下图表所示：
- en: '![Figure 2.5: A screenshot showing the histogram obtained using data from the
    first feature'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.5: 显示使用第一个特征数据获得的直方图的截图'
- en: '](img/B15781_02_05.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_02_05.jpg)'
- en: 'Figure 2.5: A screenshot showing the histogram obtained using data from the
    first feature'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '图 2.5: 显示使用第一个特征数据获得的直方图的截图'
- en: Note
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2xRg3ea](https://packt.live/2xRg3ea).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2xRg3ea](https://packt.live/2xRg3ea)。
- en: You can also run this example online at [https://packt.live/2N0L0Rj](https://packt.live/2N0L0Rj).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，链接：[https://packt.live/2N0L0Rj](https://packt.live/2N0L0Rj)。你必须执行整个Notebook，才能获得期望的结果。
- en: You have successfully created a scatter plot and a histogram using matplotlib.
    Similarly, different plot types can be created using matplotlib.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经成功地使用matplotlib创建了散点图和直方图。同样，使用matplotlib也可以创建不同类型的图表。
- en: In conclusion, visualization tools help you better understand the data that's
    available in a dataset, the results from a model, and the performance of the model.
    This happens because the human brain is receptive to visual forms, instead of
    large files of data.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，数据可视化工具帮助你更好地理解数据集中可用的数据、模型的结果以及模型的性能。这是因为人类大脑更容易接受视觉形式，而不是大量的数据文件。
- en: Matplotlib has become one of the most commonly used libraries to perform data
    visualization. Among the different plot types that the library supports, there
    are histograms, bar charts, and scatter plots.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib已成为最常用的数据可视化库之一。在该库支持的不同类型的图表中，包括直方图、条形图和散点图。
- en: 'Activity 2.01: Using Data Visualization to Aid the Pre-processing Process'
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '活动 2.01: 使用数据可视化来辅助预处理过程'
- en: 'The marketing team of your company wants to know about the different profiles
    of the clients so that it can focus its marketing effort on the individual needs
    of each profile. To do so, it has provided your team with a list of 440 pieces
    of previous sales data. Your first task is to pre-process the data. You will present
    your findings using data visualization techniques in order to help your colleagues
    understand the decisions you took in that process. You should load a CSV dataset
    using pandas and use data visualization tools to help with the pre-processing
    process. The following steps will guide you on how to do this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 你公司市场团队希望了解客户的不同档案，以便能够将营销精力集中在每个档案的个人需求上。为此，团队提供了440条先前的销售数据给你们的团队。你的第一个任务是对数据进行预处理。你将使用数据可视化技术展示你的发现，以帮助同事理解你在此过程中做出的决策。你应该使用pandas加载CSV数据集，并使用数据可视化工具帮助预处理过程。以下步骤将指导你如何完成这一过程：
- en: Import all the required elements to load the dataset and pre-process it.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所有必要的元素以加载数据集并进行预处理。
- en: Load the previously downloaded dataset by using Pandas' `read_csv()` function,
    given that the dataset is stored in a CSV file. Store the dataset in a pandas
    DataFrame named `data`.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用Pandas的`read_csv()`函数加载先前下载的数据集，假设数据集存储在CSV文件中。将数据集存储在一个名为`data`的pandas DataFrame中。
- en: Check for missing values in your DataFrame. If present, handle the missing values
    and support your decision with data visualization.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查你的DataFrame中是否存在缺失值。如果存在，处理缺失值，并通过数据可视化支持你的决策。
- en: Note
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Use `data.isnull().sum()` to check for missing values in the entire dataset
    at once, as we learned in the previous chapter.
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用`data.isnull().sum()`来一次性检查整个数据集中的缺失值，就像我们在上一章中学到的那样。
- en: Check for outliers in your DataFrame. If present, handle the outliers and support
    your decision with data visualization.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查你的DataFrame中是否存在离群值。如果存在，处理离群值，并通过数据可视化支持你的决策。
- en: Note
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Mark all the values that are three standard deviations away from the mean as
    outliers.
  id: totrans-122
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将所有偏离均值三个标准差的值标记为离群值。
- en: Rescale the data using the formula for normalization or standardization.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用归一化或标准化公式重新缩放数据。
- en: Note
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Standardization tends to work better for clustering purposes. Note that you
    can find the solution to this activity on page 216.
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标准化通常在聚类目的上效果更好。注意，你可以在第216页找到这个活动的解决方案。
- en: 'Expected output: Upon checking the DataFrame, you should find no missing values
    in the dataset and six features with outliers.'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 预期输出：检查DataFrame时，你应该发现数据集中没有缺失值，并且有六个特征包含离群值。
- en: k-means Algorithm
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means算法
- en: The k-means algorithm is used to model data without a labeled class. It involves
    dividing the data into *K* number of subgroups. The classification of data points
    into each group is done based on similarity, as explained previously (refer to
    the *Clustering Types* section), which, for this algorithm, is measured by the
    distance from the center (centroid) of the cluster. The final output of the algorithm
    is each data point linked to the cluster it belongs to and the centroid of that
    cluster, which can be used to label new data in the same clusters.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法用于建模没有标签类别的数据。它涉及将数据分成*K*个子组。数据点分类到每个组是基于相似性，如前所述（参见*聚类类型*部分），对于该算法，相似性通过数据点到簇的中心（质心）的距离来衡量。算法的最终输出是每个数据点与其所属簇的质心相关联，这可以用于为同一簇中的新数据进行标记。
- en: The centroid of each cluster represents a collection of features that can be
    used to define the nature of the data points that belong there.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 每个簇的质心代表一组特征，可以用来定义属于该簇的数据点的性质。
- en: Understanding the Algorithm
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解算法
- en: 'The k-means algorithm works through an iterative process that involves the
    following steps:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法通过一个迭代过程工作，涉及以下步骤：
- en: Based on the number of clusters defined by the user, the centroids are generated
    either by setting initial estimates or by randomly choosing them from the data
    points. This step is known as *initialization*.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据用户定义的簇数量，质心可以通过设置初始估计值或从数据点中随机选择来生成。这个步骤称为*初始化*。
- en: 'All the data points are assigned to the nearest cluster in the data space by
    measuring their respective distances from the centroid, known as the assignment
    step. The objective is to minimize the squared Euclidean distance, which can be
    defined by the following formula:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有数据点通过测量它们到质心的距离分配给数据空间中最近的簇，这被称为分配步骤。目标是最小化平方欧氏距离，公式如下：
- en: '[PRE5]'
  id: totrans-134
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, `c` represents a centroid, `x` refers to a data point, and `dist()` is
    the Euclidean distance.
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，`c`表示质心，`x`表示数据点，`dist()`是欧氏距离。
- en: Centroids are calculated again by computing the mean of all the data points
    belonging to a cluster. This step is known as the *update step*.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 质心通过计算属于同一簇的所有数据点的均值重新计算。这个步骤称为*更新步骤*。
- en: '*Steps 2* and *3* are repeated in an iterative process until a criterion is
    met. This criterion can be as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2*和*步骤3*在迭代过程中重复执行，直到满足某个标准。这个标准可以是：'
- en: The number of iterations defined.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义的迭代次数。
- en: The data points do not change from cluster to cluster.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点不会在簇之间变化。
- en: The Euclidean distance is minimized.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化欧氏距离。
- en: The algorithm is set to always arrive at a result, even though this result may
    converge to a local or a global optimum.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 算法设置为始终得出一个结果，尽管这个结果可能会收敛到局部或全局最优解。
- en: The k-means algorithm receives several parameters as inputs to run the model.
    The most important ones to consider are the initialization method (`init`) and
    the number of clusters (`K`).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法接收多个参数作为输入以运行模型。最重要的参数是初始化方法（`init`）和聚类数目（`K`）。
- en: Note
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: 'To check out the other parameters of the k-means algorithm in the scikit-learn
    library, visit the following link: [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html).'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 若要查看 scikit-learn 库中 k-means 算法的其他参数，请访问以下链接：[http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)。
- en: Initialization Methods
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 初始化方法
- en: 'An important input of the algorithm is the initialization method to be used
    to generate the initial centroids. The initialization methods allowed by the scikit-learn
    library are explained as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的一个重要输入是用于生成初始质心的初始化方法。scikit-learn 库允许的初始化方法如下所述：
- en: '`k-means++`: This is the default option. Centroids are chosen randomly from
    the set of data points, considering that centroids must be far away from one another.
    To achieve this, the method assigns a higher probability of being a centroid to
    those data points that are farther away from other centroids.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k-means++`：这是默认选项。质心是从数据点集中随机选择的，考虑到质心必须彼此远离。为此，该方法为那些距离其他质心较远的数据点分配更高的作为质心的概率。'
- en: '`random`: This method chooses K observations randomly from the data points
    as the initial centroids.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random`：此方法从数据点中随机选择 K 个观察值作为初始质心。'
- en: Choosing the Number of Clusters
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择聚类数目
- en: As we discussed previously, the number of clusters that the data is to be divided
    into is set by the user; hence, it is important to choose the number of clusters appropriately.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，数据被划分成的聚类数目是由用户设置的，因此选择合适的聚类数目非常重要。
- en: One of the metrics that's used to measure the performance of the k-means algorithm
    is the mean distance of the data points from the centroid of the cluster that
    they belong to. However, this measure can be counterproductive as the higher the
    number of clusters, the smaller the distance between the data points and its centroid,
    which may result in the number of clusters (*K*) matching the number of data points,
    thereby harming the purpose of clustering algorithms.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量 k-means 算法性能的一个指标是数据点与其所属聚类质心之间的平均距离。然而，这个指标可能会适得其反，因为聚类数目越多，数据点与其质心之间的距离越小，这可能导致聚类数（*K*）与数据点的数量相同，从而破坏聚类算法的目的。
- en: 'To avoid this, you can plot the average distance between the data points and
    the cluster centroid against the number of clusters. The appropriate number of
    clusters corresponds to the breaking point of the plot, where the rate of decrease
    drastically changes. In the following diagram, the dotted circle represents the
    ideal number of clusters:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为避免这种情况，可以绘制数据点与聚类质心之间的平均距离与聚类数的关系图。适当的聚类数对应于图中的断点，即减少速率发生剧烈变化的地方。在下图中，虚线圆圈表示理想的聚类数：
- en: '![Figure 2.6: A graph demonstrating how to estimate the breaking point'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.6：展示如何估计断点的图表'
- en: '](img/B15781_02_06.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_02_06.jpg)'
- en: 'Figure 2.6: A graph demonstrating how to estimate the breaking point'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6：展示如何估计断点的图表
- en: 'Exercise 2.02: Importing and Training the k-means Algorithm over a Dataset'
  id: totrans-156
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.02：导入并在数据集上训练 k-means 算法
- en: 'The following exercise will be performed using the same dataset from the previous
    exercise. Considering this, use the same Jupyter Notebook that you used to develop
    the previous exercise. Perform the following steps to complete this exercise:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的练习将使用与上一练习相同的数据集。考虑到这一点，请使用你在上一练习中使用的同一个 Jupyter Notebook。执行以下步骤以完成此练习：
- en: Open the Jupyter Notebook that you used for the previous exercise. Here, you
    should have imported all the required libraries and stored the dataset in a variable
    named `data`.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你用于上一练习的 Jupyter Notebook。在这里，你应该已经导入了所有必需的库，并将数据集存储在名为 `data` 的变量中。
- en: 'Import the k-means algorithm from scikit-learn as follows:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，从 scikit-learn 导入 k-means 算法：
- en: '[PRE6]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To choose the value for *K* (that is, the ideal number of clusters), calculate
    the average distance of data points from their cluster centroid in relation to
    the number of clusters. Use 20 as the maximum number of clusters for this exercise.
    The following is a snippet of the code for this:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要选择*K*的值（即理想的簇数），请计算数据点到其簇质心的平均距离，并与簇的数量进行对比。此练习中使用20作为最大簇数。以下是此代码片段：
- en: '[PRE7]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Note
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The `random_state` argument is used to ensure reproducibility of results by
    making sure that the random initialization of the algorithm remains constant.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`random_state`参数用于确保结果的可重复性，确保算法的随机初始化保持一致。'
- en: First, create the variables that will store the values as an array and name
    it `ideal_k`. Next, perform a `for` loop that starts at one cluster and goes as
    high as desired (considering that the maximum number of clusters must not exceed
    the total number of instances).
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，创建一个变量用来存储值，并命名为`ideal_k`。接下来，执行一个`for`循环，从1个簇开始，直到达到所需的数量（考虑到簇的最大数量不得超过实例的总数）。
- en: For the previous example, there was a limitation of a maximum of 20 clusters
    to be created. As a consequence of this limitation, the `for` loop goes from 1
    to 20 clusters.
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的示例中，有一个限制，最多只能创建20个簇。由于这个限制，`for`循环从1到20个簇。
- en: Note
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that `range()` is an upper bound exclusive function, meaning that the
    range will go as far as one value below the upper bound. When the upper bound
    is 21, the range will go as far as 20.
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 记住，`range()`是一个上界不包含的函数，这意味着范围会到达上界以下的一个值。当上界为21时，范围会到达20。
- en: Inside the `for` loop, instantiate the algorithm with the number of clusters
    to be created, and then fit the data to the model. Next, append the pairs of data
    (number of clusters, average distance to the centroid) to the list named `ideal_k`.
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在`for`循环中，实例化算法并设定要创建的簇的数量，然后将数据拟合到模型中。接下来，将数据对（簇的数量，平均距离到质心）追加到名为`ideal_k`的列表中。
- en: The average distance to the centroid does not need to be calculated as the model
    outputs it under the `inertia_` attribute, which can be called out as `[model_name].inertia_`.
  id: totrans-170
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 到质心的平均距离无需计算，因为模型会在`inertia_`属性下输出它，可以通过`[model_name].inertia_`调用。
- en: 'Convert the `ideal_k` list into a NumPy array so that it can be plotted. Use
    the following code snippet:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`ideal_k`列表转换为NumPy数组，以便绘制图形。使用以下代码片段：
- en: '[PRE8]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Plot the relations that you calculated in the preceding steps to find the ideal
    *K* to input to the final model:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制你在前面步骤中计算的关系，找到理想的*K*值，以便输入到最终模型中：
- en: '[PRE9]'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.7: A screenshot showing the output of the plot function used'
  id: totrans-176
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图2.7：显示绘图函数输出的截图'
- en: '](img/B15781_02_07.jpg)'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_02_07.jpg)'
- en: 'Figure 2.7: A screenshot showing the output of the plot function used'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图2.7：显示绘图函数输出的截图
- en: In the preceding plot, the *x-axis* represents the number of clusters, while
    the *y-axis* refers to the calculated average distance of each point in a cluster
    from their centroid.
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在前面的图表中，*x轴*表示簇的数量，而*y轴*表示每个簇中数据点到其质心的平均距离。
- en: The breaking point of the plot is around `5`.
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绘图的拐点大约在`5`。
- en: 'Train the model with `K=5`. Use the following code:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`K=5`训练模型。使用以下代码：
- en: '[PRE10]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The first line instantiates the model with `5` as the number of clusters. Then,
    the data is fit to the model. Finally, the model is used to assign a cluster to
    each data point.
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 第一行使用`5`作为簇的数量实例化模型。然后，数据被拟合到模型中。最后，模型用来为每个数据点分配一个簇。
- en: 'Plot the results from the clustering of data points into clusters:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制数据点聚类结果的图表：
- en: '[PRE11]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output is as follows:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.8: A screenshot showing the output of the plot function used'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图2.8：显示绘图函数输出的截图'
- en: '](img/B15781_02_08.jpg)'
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_02_08.jpg)'
- en: 'Figure 2.8: A screenshot showing the output of the plot function used'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8：显示绘图函数输出的截图
- en: Since the dataset only contains two features, each feature is passed as input
    to the scatter plot function, meaning that each feature is represented by an axis.
    Additionally, the labels that were obtained from the clustering process are used
    as the colors to display the data points. Thus, each data point is located in
    the data space based on the values of both features, and the colors represent
    the clusters that were formed.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据集仅包含两个特征，因此每个特征作为输入传递给散点图函数，这意味着每个特征由一个坐标轴表示。此外，从聚类过程中获得的标签用作显示数据点的颜色。因此，每个数据点根据两个特征的值定位于数据空间中，颜色代表形成的聚类。
- en: Note
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: For datasets with over two features, the visual representation of clusters is
    not as explicit as that shown in the preceding screenshot. This is mainly because
    the location of each data point (observation) in the data space is based on the
    collection of all of its features, and visually, it is only possible to display
    up to three features.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有两个以上特征的数据集，聚类的可视化表示不像前述截图中那样直观。这主要是因为每个数据点（观察值）在数据空间中的位置是基于所有特征的集合，而在视觉上只能显示最多三个特征。
- en: You have successfully imported and trained the k-means algorithm.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经成功导入并训练了 k-means 算法。
- en: Note
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this exercise, please refer to [https://packt.live/30GXWE1](https://packt.live/30GXWE1).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本练习的源代码，请参考[https://packt.live/30GXWE1](https://packt.live/30GXWE1)。
- en: You can also run this example online at [https://packt.live/2B6N1c3](https://packt.live/2B6N1c3).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在[https://packt.live/2B6N1c3](https://packt.live/2B6N1c3)上在线运行此示例。您必须执行整个
    Notebook 才能获得所需的结果。
- en: In conclusion, the k-means algorithm seeks to divide the data into *K* number
    of clusters, *K* being a parameter set by the user. Data points are grouped together
    based on their proximity to the centroid of a cluster, which is calculated by
    an iterative process.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，k-means 算法旨在将数据分为*K*个聚类，*K*是由用户设置的参数。数据点根据其与聚类中心的接近程度被分组，聚类中心通过迭代过程计算得出。
- en: The initial centroids are set according to the initialization method that's
    been defined. Then, all the data points are assigned to the clusters with the
    centroid closer to their location in the data space, using the Euclidean distance
    as a measure. Once the data points have been divided into clusters, the centroid
    of each cluster is recalculated as the mean of all data points. This process is
    repeated several times until a stopping criterion is met.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 初始中心点根据已定义的初始化方法设置。然后，所有数据点都会被分配到离它们在数据空间中的位置更近的聚类中心，使用欧氏距离作为度量。一旦数据点被分配到聚类中，每个聚类的中心点会重新计算为所有数据点的均值。这个过程会重复多次，直到满足停止标准。
- en: 'Activity 2.02: Applying the k-means Algorithm to a Dataset'
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.02：将 k-means 算法应用于数据集
- en: Ensure that you have completed *Activity 2.01*, *Using Data Visualization to
    Aid the Pre-processing Process*, before you proceed with this activity.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行此活动之前，请确保您已完成*活动 2.01*，*使用数据可视化辅助预处理过程*。
- en: 'Continuing with the analysis of your company''s past orders, you are now in
    charge of applying the k-means algorithm to the dataset. Using the previously
    loaded Wholesale Customers dataset, apply the k-means algorithm to the data and
    classify the data into clusters. Perform the following steps to complete this
    activity:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续分析您公司过往订单的过程中，您现在负责将 k-means 算法应用于数据集。使用之前加载的批发客户数据集，对数据应用 k-means 算法并将数据分类为聚类。执行以下步骤完成此活动：
- en: Open the Jupyter Notebook that you used for the previous activity. There, you
    should have imported all the required libraries and performed the necessary steps
    to pre-process the dataset.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开您在上一个活动中使用的 Jupyter Notebook。在那里，您应该已导入所有必需的库并执行了必要的步骤以预处理数据集。
- en: Calculate the average distance of the data points from their cluster centroid
    in relation to the number of clusters. Based on this distance, select the appropriate
    number of clusters to train the model.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数据点到其聚类中心的平均距离，并根据聚类数量来选择合适的聚类数以训练模型。
- en: Train the model and assign a cluster to each data point in your dataset. Plot
    the results.
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并为数据集中的每个数据点分配一个聚类。绘制结果。
- en: Note
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can use the `subplots()` function from Matplotlib to plot two scatter graphs
    at a time. To learn more about this function, visit Matplotlib''s documentation
    at the following link: [https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html).'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以使用 Matplotlib 的`subplots()`函数同时绘制两个散点图。要了解更多关于此函数的信息，请访问 Matplotlib 的文档，网址如下：[https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html)。
- en: You can find the solution to this activity on page 220.
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在第220页找到此活动的解决方案。
- en: The visualization of clusters will differ based on the number of clusters (k)
    and the features to be plotted.
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类的可视化将根据聚类的数量（k）和需要绘制的特征而有所不同。
- en: Mean-Shift Algorithm
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 均值漂移算法
- en: The **mean-shift algorithm** works by assigning each data point a cluster based
    on the density of the data points in the data space, also known as the mode in
    a distribution function. Contrary to the k-means algorithm, the mean-shift algorithm
    does not require you to specify the number of clusters as a parameter.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**均值漂移算法**通过根据数据空间中数据点的密度为每个数据点分配一个簇，密度也称为分布函数中的模式。与 k-means 算法不同，均值漂移算法不需要你指定簇的数量作为参数。'
- en: The algorithm works by modeling the data points as a distribution function,
    where high-density areas (high concentration of data points) represent high peaks.
    Then, the general idea is to shift each data point until it reaches its nearest
    peak, which becomes a cluster.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过将数据点建模为分布函数来工作，其中高密度区域（数据点密集的区域）代表高峰值。然后，基本思路是将每个数据点移动到最近的峰值，从而形成一个簇。
- en: Understanding the Algorithm
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解算法
- en: 'The first step of the mean-shift algorithm is to represent the data points
    as a density distribution. To do so, the algorithm builds upon the idea of **Kernel
    Density Estimation** (**KDE**), which is a method that''s used to estimate the
    distribution of a set of data:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移算法的第一步是将数据点表示为一个密度分布。为此，算法基于**核密度估计**（**KDE**）的方法进行构建，KDE 是一种用于估算数据集分布的方法：
- en: '![Figure 2.9: An image depicting the idea behind Kernel Density Estimation'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.9：描述核密度估计（KDE）背后思想的图像'
- en: '](img/B15781_02_09.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_02_09.jpg)'
- en: 'Figure 2.9: An image depicting the idea behind Kernel Density Estimation'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9：描述核密度估计（KDE）背后思想的图像
- en: 'In the preceding diagram, the dots at the bottom of the shape represent the
    data points that the user inputs, while the cone-shaped lines represent the estimated
    distribution of the data points. The peaks (high-density areas) will be the clusters.
    The process of assigning data points to each cluster is as follows:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，形状底部的点代表用户输入的数据点，而锥形线条代表数据点的估算分布。峰值（高密度区域）将成为簇。为每个簇分配数据点的过程如下：
- en: A window of a specified size (bandwidth) is drawn around each data point.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个数据点周围绘制一个指定大小（带宽）的窗口。
- en: The mean of the data inside the window is computed.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算窗口内数据点的均值。
- en: The center of the window is shifted to the mean.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 窗口的中心移动到均值位置。
- en: '*Steps 2* and *3* are repeated until the data point reaches a peak, which will
    determine the cluster that it belongs to.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*步骤2* 和 *步骤3* 会重复进行，直到数据点达到一个峰值，确定它所属的簇。'
- en: The bandwidth value should be coherent with the distribution of the data points
    in the dataset. For example, for a dataset normalized between 0 and 1, the bandwidth
    value should be within that range, while for a dataset with all values between
    1,000 and 2,000, it would make more sense to have a bandwidth between 100 and
    500.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 带宽值应与数据集中的数据点分布保持一致。例如，对于一个规范化在0到1之间的数据集，带宽值应该在该范围内，而对于一个所有值都在1000到2000之间的数据集，带宽值最好设置在100到500之间。
- en: 'In the following diagram, the estimated distribution is represented by the
    lines, while the data points are the dots. In each of the boxes, the data points
    shift to the nearest peak. All the data points in a certain peak belong to that
    cluster:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在下图中，估算的分布通过线条表示，而数据点则是点。在每个框中，数据点都会移动到最近的峰值。所有属于某个峰值的数据点都属于同一个簇：
- en: '![Figure 2.10: A sequence of images illustrating the working of the mean-shift
    algorithm'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.10：展示均值漂移算法工作原理的一系列图像'
- en: '](img/B15781_02_10.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_02_10.jpg)'
- en: 'Figure 2.10: A sequence of images illustrating the working of the mean-shift
    algorithm'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10：展示均值漂移算法工作原理的一系列图像
- en: The number of shifts that a data point has to make to reach a peak depends on
    its bandwidth (the size of the window) and its distance from the peak.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点到达峰值所需的移动次数取决于其带宽（窗口大小）以及与峰值的距离。
- en: Note
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To explore all the parameters of the mean-shift algorithm in scikit-learn, visit
    [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html).
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 要探索 scikit-learn 中均值漂移算法的所有参数，请访问 [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.MeanShift.html)。
- en: 'Exercise 2.03: Importing and Training the Mean-Shift Algorithm over a Dataset'
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.03：在数据集上导入并训练均值漂移算法
- en: 'The following exercise will be performed using the same dataset that we loaded
    in *Exercise 2.01*, *Plotting a Histogram of One Feature from the Circles Dataset*.
    Considering this, use the same Jupyter Notebook that you used to develop the previous
    exercises. Perform the following steps to complete this exercise:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 以下练习将使用我们在*练习 2.01，绘制 Circles 数据集一个特征的直方图*中加载的相同数据集。鉴于此，请使用你用来开发前面练习的相同 Jupyter
    Notebook。执行以下步骤来完成此练习：
- en: Open the Jupyter Notebook that you used for the previous exercise.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你在上一个练习中使用的 Jupyter Notebook。
- en: 'Import the k-means algorithm class from scikit-learn as follows:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按如下方式从 scikit-learn 导入 k-means 算法类：
- en: '[PRE12]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Train the model with a bandwidth of `0.5`:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用带宽为 `0.5` 训练模型：
- en: '[PRE13]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: First, the model is instantiated with a bandwidth of `0.5`. Next, the model
    is fit to the data. Finally, the model is used to assign a cluster to each data
    point.
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，模型使用 `0.5` 的带宽进行实例化。接下来，将模型拟合到数据。最后，使用该模型为每个数据点分配一个聚类。
- en: Considering that the dataset contains values ranging from −1 to 1, the bandwidth
    value should not be above 1\. The value of `0.5` was chosen after trying out other
    values, such as 0.1 and 0.9.
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到数据集包含的值范围从 −1 到 1，带宽值不应超过 1。`0.5` 这个值是在尝试其他值（如 0.1 和 0.9）后选择的。
- en: Note
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Take into account the fact that the bandwidth is a parameter of the algorithm
    and that, as a parameter, it can be fine-tuned to arrive at the best performance.
    This fine-tuning process will be covered in *Chapter 3, Supervised Learning –
    Key Steps*.
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请考虑带宽是算法的一个参数，并且作为参数，它可以进行微调，以实现最佳性能。这个微调过程将在*第 3 章，有监督学习——关键步骤*中讲解。
- en: 'Plot the results from clustering the data points into clusters:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制将数据点聚类的结果：
- en: '[PRE14]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.11: The plot obtained using the preceding code'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.11：使用前面代码获得的图'
- en: '](img/B15781_02_11.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_02_11.jpg)'
- en: 'Figure 2.11: The plot obtained using the preceding code'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11：使用前面代码获得的图
- en: Again, as the dataset only contains two features, both are passed as inputs
    to the scatter function, which become the values of the axes. Also, the labels
    that were obtained from the clustering process are used as the colors to display
    the data points.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 再次提醒，由于数据集仅包含两个特征，这两个特征都作为输入传递给散点图函数，并成为坐标轴的值。此外，从聚类过程中获得的标签被用作显示数据点的颜色。
- en: The total number of clusters that have been created is four.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的聚类总数是四个。
- en: Note
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this exercise, please refer to [https://packt.live/37vBOOk](https://packt.live/37vBOOk).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此练习的源代码，请参考 [https://packt.live/37vBOOk](https://packt.live/37vBOOk)。
- en: You can also run this example online at [https://packt.live/3e6uqM2](https://packt.live/3e6uqM2).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/3e6uqM2](https://packt.live/3e6uqM2) 在线运行这个例子。你必须执行整个
    Notebook 才能得到期望的结果。
- en: You have successfully imported and trained the mean-shift algorithm.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经成功地导入并训练了均值漂移算法。
- en: In conclusion, the mean-shift algorithm starts by drawing the distribution function
    that represents the set of data points. This process consists of creating peaks
    in high-density areas, while leaving the areas with a low density flat.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，均值漂移算法首先绘制出表示数据点集的分布函数。这个过程包括在高密度区域创建峰值，而在低密度区域保持平坦。
- en: Following this, the algorithm proceeds to classify the data points into clusters
    by shifting each point slowly and iteratively until it reaches a peak, which becomes
    its cluster.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，算法继续通过缓慢且迭代地移动每个数据点，直到它达到一个峰值，进而将其归为一个聚类。
- en: 'Activity 2.03: Applying the Mean-Shift Algorithm to a Dataset'
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 2.03：将均值漂移算法应用于数据集
- en: 'In this activity, you will apply the mean-shift algorithm to the dataset to
    see which algorithm fits the data better. Therefore, using the previously loaded
    Wholesale Consumers dataset, apply the mean-shift algorithm to the data and classify
    the data into clusters. Perform the following steps to complete this activity:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，你将应用均值迁移算法来处理数据集，以查看哪种算法更适合数据。因此，使用之前加载的批发消费者数据集，将均值迁移算法应用于数据并将数据分类为聚类。按照以下步骤完成此活动：
- en: Open the Jupyter Notebook that you used for the previous activity.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你在之前活动中使用的 Jupyter Notebook。
- en: Note
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Considering that you are using the same Jupyter Notebook, be careful not to
    overwrite any previous variables.
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 考虑到你使用的是相同的 Jupyter Notebook，请小心不要覆盖任何先前的变量。
- en: Train the model and assign a cluster to each data point in your dataset. Plot
    the results.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型，并为数据集中的每个数据点分配一个聚类。绘制结果。
- en: The visualization of clusters will differ based on the bandwidth and the features
    that have been chosen to be plotted.
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类的可视化将根据带宽和选择绘制的特征而有所不同。
- en: Note
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 223.
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第 223 页找到。
- en: DBSCAN Algorithm
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN 算法
- en: The **density-based spatial clustering of applications with noise** (**DBSCAN**)
    algorithm groups together points that are close to each other (with many neighbors)
    and marks those points that are further away with no close neighbors as outliers.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于密度的空间聚类与噪声**（**DBSCAN**）算法将彼此接近的点（具有许多邻居的点）分为一组，并将那些距离较远且没有接近邻居的点标记为离群点。'
- en: According to this, and as its name states, the algorithm classifies data points
    based on the density of all data points in the data space.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一点，正如其名称所示，算法根据数据空间中所有数据点的密度来对数据点进行分类。
- en: Understanding the Algorithm
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解算法
- en: 'The DBSCAN algorithm requires two main parameters: epsilon and the minimum
    number of observations.'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN 算法需要两个主要参数：epsilon 和最小观察数。
- en: '`eps`, is the maximum distance that defines the radius within which the algorithm
    searches for neighbors. The `min_samples`). However, the latter is optional in
    scikit-learn as the default value is set to `5`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '`eps` 是定义算法在其内搜索邻居的半径的最大距离。`min_samples` 参数是可选的，因为在 scikit-learn 中它的默认值为 `5`：'
- en: '![Figure 2.12: An illustration of how the DBSCAN algorithm classifies data
    into clusters'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.12：DBSCAN 算法如何将数据分类为聚类的示意图'
- en: '](img/B15781_02_12.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15781_02_12.jpg)'
- en: 'Figure 2.12: An illustration of how the DBSCAN algorithm classifies data into
    clusters'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12：DBSCAN 算法如何将数据分类为聚类的示意图
- en: In the preceding diagram, the dots to the left are assigned to cluster `A`,
    while the dots to the upper right are assigned to cluster `B`. Moreover, the dots
    at the bottom right (`C`) are considered to be outliers, as well as any other
    data point in the data space, as they do not meet the required parameters to belong
    to a high-density area (that is, the minimum number of samples is not met, which,
    in this example, was set to `5`).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在上图中，左侧的点被分配到聚类 `A`，而右上方的点被分配到聚类 `B`。此外，右下方的点（`C`）被认为是离群点，以及数据空间中任何其他不符合属于高密度区域所需参数的点（即未满足最小样本数要求，在这个例子中设定为
    `5`）。
- en: Note
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Similar to the bandwidth parameter, the epsilon value should be coherent with
    the distribution of the data points in the dataset, considering that it represents
    a radius around each data point.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于带宽参数，epsilon 值应与数据集中数据点的分布一致，因为它表示每个数据点周围的半径。
- en: 'According to this, each data point can be classified as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这一点，每个数据点可以按以下方式分类：
- en: '`eps` radius.'
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eps` 半径。'
- en: '**A border point**: A point that is within the *eps* radius of a core point,
    but does not have the required number of data points within its own radius.'
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界点**：一个位于核心点的 *eps* 半径范围内，但在其自身的半径内没有满足要求数量的数据点。'
- en: '**A noise point**: All points that do not meet the preceding descriptions.'
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**噪声点**：所有不符合前述描述的点。'
- en: Note
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To explore all the parameters of the DBSCAN algorithm in scikit-learn, visit
    [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要探索 scikit-learn 中 DBSCAN 算法的所有参数，请访问 [http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)。
- en: 'Exercise 2.04: Importing and Training the DBSCAN Algorithm over a Dataset'
  id: totrans-282
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.04：在数据集上导入并训练 DBSCAN 算法
- en: 'This exercise discusses how to import and train the DBSCAN algorithm over a
    dataset. We will be using the circles dataset from the previous exercises. Perform
    the following steps to complete this exercise:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习讲解如何在数据集上导入并训练DBSCAN算法。我们将使用前面练习中的圆形数据集。请执行以下步骤以完成此练习：
- en: Open the Jupyter Notebook that you used for the previous exercise.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你用于前一个练习的Jupyter Notebook。
- en: 'Import the DBSCAN algorithm class from scikit-learn as follows:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如下所示，从scikit-learn导入DBSCAN算法类：
- en: '[PRE15]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Train the model with epsilon equal to `0.1`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`0.1`的epsilon训练模型：
- en: '[PRE16]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: First, the model is instantiated with `eps` of `0.1`. Then, we use the `fit_predict()`
    function to fit the model to the data and assign a cluster to each data point.
    This bundled function, which includes both the `fit` and `predict` methods, is
    used because the DBSCAN algorithm in scikit-learn does not contain a `predict()`
    method alone.
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，模型通过`eps`设置为`0.1`进行实例化。然后，我们使用`fit_predict()`函数将模型拟合到数据并为每个数据点分配一个聚类。这个包含`fit`和`predict`方法的封装函数被使用，因为scikit-learn中的DBSCAN算法并没有单独的`predict()`方法。
- en: Again, the value of `0.1` was chosen after trying out all other possible values.
  id: totrans-290
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，`0.1`这个值是在尝试了所有其他可能的值后选择的。
- en: 'Plot the results from the clustering process:'
  id: totrans-291
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制聚类过程的结果：
- en: '[PRE17]'
  id: totrans-292
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The output is as follows:'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.13: The plot obtained with the preceding code'
  id: totrans-294
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图2.13：通过前面的代码获得的图表'
- en: '](img/B15781_02_13.jpg)'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15781_02_13.jpg)'
- en: 'Figure 2.13: The plot obtained with the preceding code'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13：通过前面的代码获得的图表
- en: As before, both features are passed as inputs to the scatter function. Also,
    the labels that were obtained from the clustering process are used as the colors
    to display the data points.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，两个特征被作为输入传递给scatter函数。此外，聚类过程中获得的标签被用作显示数据点颜色的依据。
- en: The total number of clusters that have been created is two.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 创建的聚类总数为两个。
- en: As you can see, the total number of clusters created by each algorithm is different.
    This is because, as mentioned previously, each of these algorithms defines similarity
    differently and, as a consequence, each interprets the data differently.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，每个算法创建的聚类总数不同。这是因为，如前所述，每个算法对相似性的定义不同，因此每个算法对数据的解释也不同。
- en: Due to this, it is crucial to test different algorithms over the data to compare
    the results and define which one generalizes better to the data. The following
    topic will explore some methods that we can use to evaluate performance to help
    choose an algorithm.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，测试不同的算法来比较结果，并定义哪个算法对数据的泛化能力更强是至关重要的。接下来的内容将探讨一些我们可以用来评估性能的方法，以帮助选择算法。
- en: Note
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this exercise, please refer to [https://packt.live/2Bcanxa](https://packt.live/2Bcanxa).
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问这个练习的源代码，请参考[https://packt.live/2Bcanxa](https://packt.live/2Bcanxa)。
- en: You can also run this example online at [https://packt.live/2UKHFdp](https://packt.live/2UKHFdp).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2UKHFdp](https://packt.live/2UKHFdp)上在线运行这个示例。你必须执行整个Notebook才能得到预期的结果。
- en: You have successfully imported and trained the DBSCAN algorithm.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经成功导入并训练了DBSCAN算法。
- en: In conclusion, the DBSCAN algorithm bases its clustering classification on the
    density of data points in the data space. This means that clusters are formed
    by data points with many neighbors. This is done by considering that core points
    are those that contain a minimum number of neighbors within a set radius, border
    points are those that are located inside the radius of a core point but do not
    have the minimum number of neighbors within their own radius, and noise points
    are those that do not meet any of the specifications.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，DBSCAN算法通过数据空间中数据点的密度来进行聚类分类。这意味着聚类是由邻居较多的数据点组成的。通过以下方式实现：核心点是指在设定半径范围内包含最小数量邻居的数据点，边界点是指位于核心点半径范围内，但在自己的半径范围内没有最小数量邻居的数据点，噪声点则是指不符合任何规格的数据点。
- en: 'Activity 2.04: Applying the DBSCAN Algorithm to the Dataset'
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动2.04：将DBSCAN算法应用于数据集
- en: 'You will apply the DBSCAN algorithm to the dataset as well. This is basically
    because it is good practice to test out different algorithms when solving a data
    problem in order to choose the one that best fits the data, considering that there
    is no one model that performs well for all data problems. Using the previously
    loaded Wholesale Consumers dataset, apply the DBSCAN algorithm to the data and
    classify the data into clusters. Perform the following steps:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 你还将对数据集应用DBSCAN算法。这基本上是因为在解决数据问题时，测试不同算法是一种良好的实践，以便选择最适合数据的算法，因为没有一种模型能在所有数据问题上表现良好。使用之前加载的批发消费者数据集，对数据应用DBSCAN算法并将数据分类到不同的聚类中。执行以下步骤：
- en: Open the Jupyter Notebook that you used for the previous activity.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你用于之前活动的Jupyter Notebook。
- en: Train the model and assign a cluster to each data point in your dataset. Plot the results.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型并为数据集中的每个数据点分配一个聚类。绘制结果。
- en: Note
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 225.
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 本活动的解答可以在第225页找到。
- en: The visualization of clusters will differ based on the epsilon and the features
    chosen to be plotted.
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 聚类的可视化将根据选择的epsilon值和绘制的特征而有所不同。
- en: Evaluating the Performance of Clusters
  id: totrans-313
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估聚类的表现
- en: After applying a clustering algorithm, it is necessary to evaluate how well
    the algorithm has performed. This is especially important when it is difficult
    to visually evaluate the clusters; for example, when there are several features.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用聚类算法之后，必须评估算法的表现如何。特别是在难以直观评估聚类时，这一点尤为重要；例如，当数据有多个特征时。
- en: Usually, with supervised algorithms, it is easy to evaluate their performance
    by simply comparing the prediction of each instance with its true value (class).
    On the other hand, when dealing with unsupervised models (such as clustering algorithms),
    it is necessary to pursue other strategies.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于有监督的算法，通过将每个实例的预测值与其真实值（类别）进行比较，就很容易评估其性能。另一方面，处理无监督模型（如聚类算法）时，需要采用其他策略。
- en: In the specific case of clustering algorithms, it is possible to evaluate performance
    by measuring the similarity of the data points that belong to the same cluster.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类算法的具体情况下，可以通过衡量属于同一聚类的数据点的相似度来评估性能。
- en: Available Metrics in Scikit-Learn
  id: totrans-317
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Scikit-Learn中的可用度量
- en: Scikit-learn allows its users to use three different scores for evaluating the
    performance of unsupervised clustering algorithms. The main idea behind these
    scores is to measure how well-defined the cluster's edges are, instead of measuring
    the dispersion within a cluster. Hence, it is worth mentioning that the scores
    do not take into account the size of each cluster.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn允许用户使用三种不同的得分来评估无监督聚类算法的表现。这些得分背后的主要思想是衡量聚类边界的清晰度，而不是衡量聚类内的分散度。因此，值得提到的是，这些得分并不考虑每个聚类的大小。
- en: 'The two most commonly used scores for measuring unsupervised clustering tasks
    are explained as follows:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是用于衡量无监督聚类任务的两种最常用评分方法的解释：
- en: 'The **Silhouette Coefficient Score** calculates the mean distance between each
    point and all the other points of a cluster (*a*), as well as the mean distance
    between each point and all the other points of its nearest clusters (*b*). It
    relates both of them according to the following equation:'
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**轮廓系数（Silhouette Coefficient Score）**计算每个点与同一聚类中所有其他点的平均距离（*a*），以及每个点与其最近聚类中所有其他点的平均距离（*b*）。它根据以下公式将二者联系起来：'
- en: '[PRE18]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The result of the score is a value between -1 and 1\. The lower the value, the
    worse the performance of the algorithm. Values around 0 will imply overlapping
    of clusters. It is also important to clarify that this score does not work very
    well when using density-based algorithms such as DBSCAN.
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 得分结果的值介于-1和1之间。值越低，算法的表现越差。接近0的值意味着聚类之间可能存在重叠。还需要澄清的是，这个得分在使用基于密度的算法（如DBSCAN）时效果不太好。
- en: The **Calinski–Harabasz Index** was created to measure the relationship between
    the variance of each cluster and the variance of all clusters. More specifically,
    the variance of each cluster is the mean square error of each point with respect
    to the centroid of that cluster. On the other hand, the variance of all clusters
    refers to the overall inter-cluster variance.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Calinski-Harabasz指数**是为了衡量每个聚类的方差与所有聚类的方差之间的关系而创建的。更具体地说，每个聚类的方差是每个点相对于该聚类中心的均方误差。另一方面，所有聚类的方差是指聚类之间的总体方差。'
- en: The higher the value of the Calinski–Harabasz Index, the better the definition
    and separation of the clusters. There is no acceptable cut-off value, so the performance
    of the algorithms using this index is evaluated through comparison, where the
    algorithm with the highest value is the one that performs best. As with the Silhouette
    Coefficient, this score does not perform well on density-based algorithms such
    as DBSCAN.
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Calinski–Harabasz 指数值越高，聚类的定义和分离度越好。没有可接受的截止值，因此，使用此指标的算法性能通过比较来评估，具有最高值的算法表现最好。与轮廓系数一样，该得分在基于密度的算法（如
    DBSCAN）上表现不佳。
- en: Unfortunately, the scikit-learn library does not contain other methods for effectively
    measuring the performance of density-based clustering algorithms, and although
    the methods mentioned here may work in some cases to measure the performance of
    these algorithms, when they do not, there is no other way to measure this other
    than via manual evaluation.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，scikit-learn 库没有其他有效测量基于密度的聚类算法性能的方法，尽管这里提到的方法在某些情况下可以用来衡量这些算法的性能，但当它们不起作用时，除了通过人工评估外，没有其他办法来衡量此类算法的性能。
- en: However, it is worth mentioning that there are additional performance measures
    in scikit-learn for cases where a ground truth label is known, known as supervised
    clustering; for instance, when performing clustering over a set of observations
    of journalism students who have already signed up for a major or a specialization
    area. If we were to use their demographic information as well as some student
    records to categorize them into clusters that represent their choice of major,
    it would be possible to compare the predicted classification with the actual classification.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，值得一提的是，scikit-learn 中还有其他性能度量方法，适用于已知真实标签的情况，称为监督聚类；例如，当对已经选择专业或领域的新闻学学生进行聚类时，如果我们使用他们的个人信息以及一些学生记录来将他们分类为表示专业选择的聚类，就可以将预测分类与实际分类进行比较。
- en: 'Some of these measures are as follows:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些度量标准如下：
- en: '`metrics` module, and it receives the list of ground truth clusters and the
    list of predicted clusters as inputs, as follows:'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`metrics` 模块，它接收真实聚类列表和预测聚类列表作为输入，格式如下：'
- en: '[PRE19]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '**Completeness score**: Opposite to the homogeneity score, a clustering task
    satisfies completeness if all data points that belong to a given class label belong
    to the same cluster. Again, the output measure is a number between 0 and 1, with
    1 being the output for perfect completeness. This score is also part of scikit-learn''s
    metrics modules, and it also receives the ground truth labels and the predicted
    ones as inputs, as follows:'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性得分**：与同质性得分相对，聚类任务满足完整性条件时，如果所有属于给定类别标签的数据点都属于同一聚类，则认为满足完整性要求。同样，输出的度量值在
    0 到 1 之间，1 表示完美的完整性。该得分也是 scikit-learn 库中的度量标准之一，它接收真实标签和预测标签作为输入，格式如下：'
- en: '[PRE20]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'To explore other measures that evaluate the performance of supervised clustering
    tasks, visit the following URL, under the clustering section: https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics.'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要探索评估监督聚类任务性能的其他度量标准，请访问以下网址，在聚类部分查找：https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics。
- en: 'Exercise 2.05: Evaluating the Silhouette Coefficient Score and Calinski–Harabasz
    Index'
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 2.05：评估轮廓系数得分和 Calinski–Harabasz 指数
- en: 'In this exercise, we will learn how to calculate the two scores we discussed
    in the previous section that are available in scikit-learn. Perform the following
    steps to complete this exercise:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将学习如何计算前一节中讨论的、在 scikit-learn 中可用的两个得分。请按照以下步骤完成此练习：
- en: 'Import the Silhouette Coefficient score and the Calinski-Harabasz Index from
    the scikit-learn library:'
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 库中导入轮廓系数得分和 Calinski-Harabasz 指数：
- en: '[PRE21]'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Calculate the Silhouette Coefficient score for each of the algorithms we modeled
    in all of the previous exercises. Use the Euclidean distance as the metric for
    measuring the distance between points.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算我们在所有之前练习中建模的每个算法的轮廓系数得分。使用欧几里得距离作为度量标准，来衡量点与点之间的距离。
- en: 'The input parameters of the `silhouette_score()` function are the data, the
    predicted values of the model (the clusters assigned to each data point), and
    the distance measure:'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`silhouette_score()` 函数的输入参数包括数据、模型的预测值（分配给每个数据点的聚类）以及距离度量方法：'
- en: '[PRE22]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The first three lines call the `silhouette_score()` function over each of the
    models (the k-mean, the mean-shift, and the DBSCAN algorithms) by inputting the
    data, the predictions, and the distance measure. The last line of code prints
    out the score for each of the models.
  id: totrans-341
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 前三行代码通过输入数据、预测结果和距离度量，调用`silhouette_score()`函数计算每个模型（k-means、mean-shift和DBSCAN算法）的评分。最后一行代码打印出每个模型的评分。
- en: The scores come to be around `0.359`, `0.3705`, and `0.1139` for the k-means,
    mean-shift, and DBSCAN algorithms, respectively.
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: k-means、mean-shift和DBSCAN算法的评分分别为`0.359`、`0.3705`和`0.1139`。
- en: You can observe that both k-means and mean-shift algorithms have similar scores,
    while the DBSCAN score is closer to zero. This can indicate that the performance
    of the first two algorithms is much better, and hence, the DBSCAN algorithm should
    not be considered to solve the data problem.
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以观察到，k-means和mean-shift算法的评分相似，而DBSCAN的评分则接近零。这表明前两种算法的性能要好得多，因此，DBSCAN算法不应被考虑用来解决数据问题。
- en: Nevertheless, it is important to remember that this type of score does not perform
    well when evaluating the DBSCAN algorithm. This is basically because as one cluster
    is surrounding the other one, the score can interpret that as an overlap when,
    in reality, the clusters are very well-defined, as is the case of the current
    dataset.
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然而，重要的是要记住，这种评分在评估DBSCAN算法时表现不佳。这主要是因为当一个簇围绕另一个簇时，评分可能会将其解释为重叠，而实际上这些簇是非常清晰定义的，就像当前数据集的情况一样。
- en: 'Calculate the Calinski-Harabasz index for each of the algorithms we modeled
    in the previous exercises in this chapter. The input parameters of the `calinski_harabasz_score()`
    function are the data and the predicted values of the model (the clusters assigned
    to each data point):'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算我们在本章前面练习中所建模的每种算法的Calinski-Harabasz指数。`calinski_harabasz_score()`函数的输入参数是数据和模型的预测值（分配给每个数据点的簇）：
- en: '[PRE23]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Again, the first three lines apply the `calinski_harabasz_score()` function
    over the three models by passing the data and the prediction as inputs. The last
    line prints out the results.
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 同样，前三行代码使用`calinski_harabasz_score()`函数对三个模型进行计算，输入数据和预测结果，最后一行打印出结果。
- en: The values come to approximately `1379.7`, `1305.14`, and `0.0017` for the k-means,
    mean-shift, and DBSCAN algorithms, respectively. Once again, the results are similar
    to the ones we obtained using the Silhouette Coefficient score, where both the
    k-means and mean-shift algorithms performed similarly well, while the DBSCAN algorithm
    did not.
  id: totrans-348
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: k-means、mean-shift和DBSCAN算法的值分别约为`1379.7`、`1305.14`和`0.0017`。再次强调，这些结果与我们使用轮廓系数评分时得到的结果相似，其中k-means和mean-shift算法表现良好，而DBSCAN算法则不行。
- en: Moreover, it is worth mentioning that the scale of each method (the Silhouette
    Coefficient score and the Calinski-Harabasz index) differs significantly, so they
    are not easily comparable.
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此外，值得一提的是，每种方法的尺度（轮廓系数评分和Calinski-Harabasz指数）差异显著，因此它们不容易进行比较。
- en: Note
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/3e3YIif](https://packt.live/3e3YIif).
  id: totrans-351
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/3e3YIif](https://packt.live/3e3YIif)。
- en: You can also run this example online at [https://packt.live/2MXOQdZ](https://packt.live/2MXOQdZ).
    You must execute the entire Notebook in order to get the desired result.
  id: totrans-352
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在网上运行这个示例，链接是[https://packt.live/2MXOQdZ](https://packt.live/2MXOQdZ)。你必须执行整个Notebook才能得到期望的结果。
- en: You have successfully measured the performance of three different clustering algorithms.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经成功地测量了三种不同聚类算法的性能。
- en: In conclusion, the scores presented in this topic are a way of evaluating the
    performance of clustering algorithms. However, it is important to consider that
    the results from these scores are not definitive as their performance varies from
    algorithm to algorithm.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，本主题中呈现的评分是一种评估聚类算法性能的方式。然而，重要的是要考虑，这些评分的结果并非决定性的，因为它们的性能因算法而异。
- en: 'Activity 2.05: Measuring and Comparing the Performance of the Algorithms'
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动2.05：测量和比较算法的性能
- en: 'You might find yourself in a situation in which you are not sure about the
    performance of the algorithms as it cannot be evaluated graphically. Therefore,
    you will have to measure the performance of the algorithms using numerical metrics
    that can be used to make comparisons. For the previously trained models, calculate
    the Silhouette Coefficient score and the Calinski-Harabasz index to measure the
    performance of the algorithms. The following steps provide hints regarding how
    you can do this:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到一种情况，即无法图形化评估算法的性能，因此你无法确定算法的表现。在这种情况下，你需要使用数值指标来衡量算法的性能，并进行比较。对于之前训练的模型，计算轮廓系数分数和卡林斯基-哈拉巴兹指数来衡量算法的表现。以下步骤提供了有关如何进行此操作的提示：
- en: Open the Jupyter Notebook that you used for the previous activity.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开你用于上一活动的Jupyter Notebook。
- en: Calculate both the Silhouette Coefficient score and the Calinski-Harabasz index
    for all of the models that you trained previously.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算你之前训练的所有模型的轮廓系数分数和卡林斯基-哈拉巴兹指数。
- en: 'The results may differ based on the choices you made during the development
    of the previous activities and how you initialized certain parameters in each
    algorithm. Nevertheless, the following results can be expected for a k-means algorithm
    set to divide the dataset into six clusters, a mean-shift algorithm with a bandwidth
    equal to 0.4, and a DBSCAN algorithm with an epsilon score of 0.8:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 结果可能会因你在之前活动开发过程中做出的选择以及如何初始化每个算法中的某些参数而有所不同。然而，以下结果可以预期：将数据集分为六个簇的k-means算法，带有带宽为0.4的均值漂移算法，以及epsilon值为0.8的DBSCAN算法：
- en: '[PRE24]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 226.
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个活动的解答可以在第226页找到。
- en: Summary
  id: totrans-363
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Data problems where the input data is unrelated to the labeled output are handled
    using unsupervised learning models. The main objective of such data problems is
    to understand the data by finding patterns that, in some cases, can be generalized
    to new instances.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 输入数据与标签输出无关的数据问题通过使用无监督学习模型来处理。此类数据问题的主要目标是通过寻找模式来理解数据，这些模式在某些情况下可以推广到新的实例。
- en: In this context, this chapter covered clustering algorithms, which work by aggregating
    similar data points into clusters, while separating data points that differ significantly.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在此背景下，本章介绍了聚类算法，这些算法通过将相似的数据点聚集成簇，同时将差异显著的数据点分开。
- en: Three different clustering algorithms were applied to the dataset and their
    performance was compared so that we can choose the one that best fits the data.
    Two different metrics for performance evaluation, the Silhouette Coefficient metric
    and the Calinski-Harabasz index, were also discussed in light of the inability
    to represent all of the features in a plot, and thereby graphically evaluate performance
    of the algorithms. However, it is important to understand that the result from
    the metric's evaluation is not absolute as some metrics perform better (by default)
    for some algorithms than for others.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据集应用了三种不同的聚类算法，并比较了它们的表现，以便我们可以选择最适合数据的算法。我们还讨论了两种性能评估指标——轮廓系数（Silhouette
    Coefficient）和卡林斯基-哈拉巴兹指数（Calinski-Harabasz index），考虑到无法在图表中表示所有特征，因此无法通过图形化方式评估算法的表现。然而，重要的是要理解，指标评估的结果并非绝对的，因为某些指标（默认情况下）对某些算法比对其他算法表现得更好。
- en: In the next chapter, we will understand the steps involved in solving a data
    problem using supervised machine learning algorithms and learn how to perform
    error analysis.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将了解使用监督学习算法解决数据问题的步骤，并学习如何进行误差分析。
