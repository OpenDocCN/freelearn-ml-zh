- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Interpretation Challenges
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可解释性挑战
- en: In this chapter, we will discuss the traditional methods used for machine learning
    interpretation for both regression and classification. This includes model performance
    evaluation methods such as RMSE, R-squared, AUC, ROC curves, and the many metrics
    derived from confusion matrices. We will then examine the limitations of these
    performance metrics and explain what exactly makes “white-box” models intrinsically
    interpretable and why we cannot always use white-box models. To answer these questions,
    we’ll consider the trade-off between prediction performance and model interpretability.
    Finally, we will discover some new “glass-box” models such as **Explainable Boosting
    Machines** (**EBMs**) and GAMI-Net that attempt to not compromise on this trade-off
    between predictive performance and interpretability.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论用于机器学习解释的传统方法，包括回归和分类。这包括模型性能评估方法，如RMSE、R-squared、AUC、ROC曲线以及从混淆矩阵派生出的许多指标。然后，我们将检查这些性能指标的局限性，并解释“白盒”模型本质上可解释的原因以及为什么我们并不总是可以使用白盒模型。为了回答这些问题，我们将考虑预测性能和模型可解释性之间的权衡。最后，我们将发现一些新的“玻璃盒”模型，如**可解释提升机**（**EBMs**）和GAMI-Net，它们试图不在这两种性能之间做出妥协。
- en: 'The following are the main topics that will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要主题：
- en: Reviewing traditional model interpretation methods
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾传统的模型解释方法
- en: Understanding the limitations of traditional model interpretation methods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解传统模型解释方法的局限性
- en: Studying intrinsically interpretable (white-box) models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究本质上可解释的（白盒）模型
- en: Recognizing the trade-off between performance and interpretability
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到性能和可解释性之间的权衡
- en: Discovering newer interpretable (glass-box) models
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现新的可解释（玻璃盒）模型
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: From *Chapter 2*, *Key Concepts of Interpretability*, onward, we are using a
    custom `mldatasets` library to load our datasets. Instructions on how to install
    this library can be found in the *Preface*. In addition to `mldatasets`, this
    chapter’s examples also use the `pandas`, `numpy`, `sklearn`, `rulefit`, `interpret`,
    `statsmodels`, `matplotlib`, and `gaminet` libraries.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 从**第二章**，**可解释性关键概念**开始，我们使用自定义的`mldatasets`库来加载我们的数据集。有关如何安装此库的说明可以在**前言**中找到。除了`mldatasets`，本章的示例还使用了`pandas`、`numpy`、`sklearn`、`rulefit`、`interpret`、`statsmodels`、`matplotlib`和`gaminet`库。
- en: 'The code for this chapter is located here: [packt.link/swCyB](http://packt.link/swCyB).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[packt.link/swCyB](http://packt.link/swCyB)。
- en: The mission
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: Picture yourself, a data science consultant, in a conference room in Fort Worth,
    Texas, during early January 2019\. In this conference room, executives for one
    of the world’s largest airlines, **American Airlines** (**AA**), are briefing
    you on their **On-Time Performance** (**OTP**). OTP is a widely accepted **Key
    Performance Indicator** (**KPI**) for flight punctuality. It is measured as the
    percentage of flights that arrived within 15 minutes of the scheduled arrival.
    It turns out that AA has achieved an OTP of just over 80% for 3 years in a row,
    which is acceptable, and a significant improvement, but they are still ninth in
    the world and fifth in North America. To brag about it next year in their advertising,
    they aspire to achieve, at least, number one in North America for 2019, besting
    their biggest rivals.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一位数据科学顾问，在2019年1月初的德克萨斯州沃斯堡的一个会议室里。在这个会议室里，世界上最大航空公司之一的**美国航空公司**（**AA**）的行政人员正在向您介绍他们的**准时性能**（**OTP**）。OTP是衡量航班准时的一个广泛接受的**关键绩效指标**（**KPI**）。它被定义为在预定到达时间前后15分钟内到达的航班百分比。结果发现，AA连续3年实现了略高于80%的OTP，这是可以接受的，并且是一个显著的改进，但他们仍然在全球排名第九，在北美排名第五。为了在明年的广告中炫耀，他们渴望至少在2019年成为北美第一，超越他们最大的竞争对手。
- en: On the financial front, it is estimated that delays cost the airline close to
    $2 billion, so reducing this by 25–35% to be on parity with their competitors
    could produce sizable savings. And it is estimated that it costs passengers just
    as much due to tens of millions of lost hours. A reduction in delays would result
    in happier customers, which could lead to an increase in ticket sales.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在财务方面，预计延误将使航空公司损失近20亿美元，因此减少25-35%以与竞争对手持平可以产生可观的节省。而且，由于数千万小时的损失，乘客的损失也大致相同。减少延误将导致更满意的客户，这可能导致机票销售额的增加。
- en: 'Your task is to create models that can accurately predict delays for domestic
    flights only. What they hope to gain from the models is the following:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您的任务是创建可以准确预测国内航班延误的模型。他们希望从这些模型中获得以下信息：
- en: To understand what factors impacted domestic arrival delays the most in 2018
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解哪些因素在2018年对国内到达延误影响最大
- en: To anticipate a delay caused by the airline in midair with enough accuracy to
    mitigate some of these factors in 2019
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了在2019年足够准确地预测空中由航空公司造成的延误，以减轻这些因素中的一些
- en: But not all delays are made equal. The **International Air Transport Association**
    (**IATA**) has over 80 delay codes ranging from 14 (*oversales booking errors*)
    to 75 (*de-icing of aircraft, removal of ice/snow, frost prevention*). Some are
    preventable, and others unavoidable.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 但并非所有延误都是平等的。**国际航空运输协会**（**IATA**）有超过80个延误代码，从14（*超额预订错误*）到75（*飞机除冰、清除冰雪、防霜*）。有些是可以预防的，而有些则是不可避免的。
- en: The airline executives told you that the airline is not, for now, interested
    in predicting delays caused by events out of their control, such as extreme weather,
    security events, and air traffic control issues. They are also not interested
    in delays caused by late arrivals from previous flights using the same aircraft
    because this was not the root cause. Nevertheless, they would like to know the
    effect of a busy hub on avoidable delays even if this has to do with congestion
    because, after all, perhaps there’s something they can do with flight scheduling
    or flight speed, or even gate selection. And while they understand that international
    flights occasionally impact domestic flights, they hope to tackle the sizeable
    local market first.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 航空公司高管们告诉您，目前航空公司对预测由他们无法控制的事件（如极端天气、安全事件和空中交通管制问题）造成的延误不感兴趣。他们也不对由于使用同一架飞机的先前航班延误造成的延误感兴趣，因为这不是根本原因。尽管如此，他们仍然希望了解繁忙枢纽对可避免延误的影响，即使这与拥堵有关，因为毕竟，他们可能可以通过航班调度或航班速度，甚至登机口选择来做些什么。而且，虽然他们理解国际航班偶尔会影响国内航班，但他们希望首先解决庞大的本地市场。
- en: Executives have provided you with a dataset from the United States Department
    of Transportation *Bureau of Transportation Statistics* with all 2018 AA domestic
    flights.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 高管们向您提供了美国交通部*运输统计局*的所有2018年AA国内航班的数据库。
- en: The approach
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'Upon careful consideration, you have decided to approach this both as a regression
    problem and a classification problem. Therefore, you will produce models that
    predict minutes delayed as well as models that classify whether flights were delayed
    by more than 15 minutes. For interpretation, using both will enable you to use
    a wider variety of methods and expand your interpretation accordingly. So we will
    approach this example by taking the following steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 经过仔细考虑，您决定将此问题同时视为回归问题和分类问题。因此，您将创建预测延误分钟的模型以及分类航班是否延误超过15分钟的模型。为了解释，使用这两种方法将使您能够使用更广泛的方法，并相应地扩展解释。因此，我们将通过以下步骤来处理此示例：
- en: Predicting minutes delayed with various regression methods
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用各种回归方法预测延误的分钟数
- en: Classifying flights as delayed or not delayed with various classification methods
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用各种分类方法将航班分类为延误或未延误
- en: These steps in the *Reviewing traditional model interpretation methods* section
    are followed by conclusions spread out in the rest of the sections of this chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在“*回顾传统模型解释方法*”部分中，这些步骤后面是本章其余部分分散的结论。
- en: The preparations
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/03/FlightDelays.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/03/FlightDelays.ipynb).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接中找到此示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/03/FlightDelays.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/blob/main/03/FlightDelays.ipynb)。
- en: Loading the libraries
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，您需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`mldatasets`来加载数据集
- en: '`pandas` and `numpy` to manipulate it'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pandas`和`numpy`来操作它'
- en: '`sklearn` (scikit-learn), `rulefit`, `statsmodels`, `interpret`, `tf`, and
    `gaminet` to fit models and calculate performance metrics'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn`（scikit-learn）、`rulefit`、`statsmodels`、`interpret`、`tf`和`gaminet`来拟合模型和计算性能指标'
- en: '`matplotlib` to create visualizations'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib`来创建可视化'
- en: 'Load these libraries as seen in the following snippet:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下代码片段加载这些库：
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Understanding and preparing the data
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We then load the data as shown:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后按如下方式加载数据：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'There should be nearly 900,000 records and 23 columns. We can take a peek at
    what was loaded like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 应该有近90万条记录和23列。我们可以这样查看加载的内容：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The following is the output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为输出结果：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Everything seems to be in order because all columns are there and there are
    no `null` values.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一切似乎都井然有序，因为所有列都在那里，并且没有`null`值。
- en: The data dictionary
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据字典
- en: Let’s examine the data dictionary.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查数据字典。
- en: 'General features are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一般特征如下：
- en: '`FL_NUM`: Flight number.'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`FL_NUM`: 航班号。'
- en: '`ORIGIN`: Starting airport code (IATA).'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ORIGIN`: 起始机场代码（IATA）。'
- en: '`DEST`: Destination airport code (IATA).'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEST`: 目的地机场代码（IATA）。'
- en: 'Departure features are as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 离开特征如下：
- en: '`PLANNED_DEP_DATETIME`: The planned date and time of the flight.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PLANNED_DEP_DATETIME`: 航班的计划日期和时间。'
- en: '`CRS_DEP_TIME`: The planned departure time.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CRS_DEP_TIME`: 计划离开时间。'
- en: '`DEP_TIME`: The actual departure time.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEP_TIME`: 实际离开时间。'
- en: '`DEP_AFPH`: The number of actual flights per hour occurring during the interval
    in between the planned and actual departure from the origin airport (factoring
    in 30 minutes of padding). The feature tells you how busy the origin airport was
    during takeoff.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEP_AFPH`: 在计划出发和实际出发之间的间隔内实际每小时航班数（考虑了30分钟的填充时间）。该特征告诉你出发机场在起飞时的繁忙程度。'
- en: '`DEP_RFPH`: The departure relative flights per hour is the ratio of actual
    flights per hour over the median number of flights per hour that occur at the
    origin airport at that time of day, day of the week, and month of the year. The
    feature tells you how *relatively* busy the origin airport was during takeoff.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEP_RFPH`: 离开相对航班每小时数是实际每小时航班数与当天、星期和月份在出发机场发生的平均每小时航班数的比率。该特征告诉你出发机场在起飞时的相对繁忙程度。'
- en: '`TAXI_OUT`: The time duration elapsed between the departure from the origin
    airport gate and wheels off.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TAXI_OUT`: 从出发机场登机口出发到飞机轮子离地的持续时间。'
- en: '`WHEELS_OFF`: The point in time that the aircraft’s wheels leave the ground.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WHEELS_OFF`: 飞机轮子离地的时间点。'
- en: 'In-flight features are as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 飞行中的特征如下：
- en: '`CRS_ELAPSED_TIME`: The planned amount of time needed for the flight trip.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CRS_ELAPSED_TIME`: 飞行行程计划所需的时间。'
- en: '`PCT_ELAPSED_TIME`: The ratio of actual flight time over planned flight time
    to gauge the plane’s relative speed.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PCT_ELAPSED_TIME`: 实际飞行时间与计划飞行时间的比率，以衡量飞机的相对速度。'
- en: '`DISTANCE`: The distance between two airports.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DISTANCE`: 两个机场之间的距离。'
- en: 'Arrival features are as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 到达特征如下：
- en: '`CRS_ARR_TIME`: The planned arrival time.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CRS_ARR_TIME`: 计划到达时间。'
- en: '`ARR_AFPH`: The number of actual flights per hour occurring during the interval
    between the planned and actual arrival time at the destination airport (factoring
    in 30 minutes of padding). The feature tells you how busy the destination airport
    was during landing.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARR_AFPH`: 在计划到达和实际到达时间之间的间隔内实际每小时航班数（考虑了30分钟的填充时间）。该特征告诉你目的地机场在着陆时的繁忙程度。'
- en: '`ARR_RFPH`: The arrival relative flights per hour is the ratio of actual flights
    per hour over the median number of flights per hour that occur at the destination
    airport at that time of day, day of the week, and month of the year. The feature
    tells you how *relatively* busy the destination airport was during landing.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARR_RFPH`: 到达相对航班每小时数是实际每小时航班数与当天、星期和月份在该目的地机场发生的平均每小时航班数的比率。该特征告诉你目的地机场在着陆时的相对繁忙程度。'
- en: 'Delay features are as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 延误特征如下：
- en: '`DEP_DELAY`: The total delay on departure in minutes.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DEP_DELAY`: 离开延误的总分钟数。'
- en: '`ARR_DELAY`: The total delay on arrival in minutes can be subdivided into any
    or all of the following:'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ARR_DELAY`: 到达延误的总分钟数可以细分为以下任何一个或所有：'
- en: '`CARRIER_DELAY`: The delay in minutes caused by circumstances within the airline’s
    control (for example, maintenance or crew problems, aircraft cleaning, baggage
    loading, fueling, and so on).'
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`CARRIER_DELAY`: 由航空公司控制因素（例如，维护或机组人员问题、飞机清洁、行李装载、加油等）造成的延误分钟数。'
- en: '`WEATHER_DELAY`: The delay in minutes caused by significant meteorological
    conditions (actual or forecasted).'
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`WEATHER_DELAY`: 由重大气象条件（实际或预报）造成的延误分钟数。'
- en: '`NAS_DELAY`: The delay in minutes mandated by a national aviation system such
    as non-extreme weather conditions, airport operations, heavy traffic volume, and
    air traffic control.'
  id: totrans-71
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`NAS_DELAY`：由国家航空系统（如非极端天气条件、机场运营、交通量过大和空中交通管制）规定的延误分钟数。'
- en: '`SECURITY_DELAY`: The delay in minutes caused by the evacuation of a terminal
    or concourse, re-boarding of an aircraft because of a security breach, faulty
    screening equipment, or long lines above 29 minutes in screening areas.'
  id: totrans-72
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`SECURITY_DELAY`：由于疏散机场或候机楼、因安全漏洞重新登机、检查设备故障或安检区域超过29分钟的长时间排队等原因造成的延误分钟数。'
- en: '`LATE_AIRCRAFT_DELAY`: The delay in minutes caused by a previous flight with
    the same aircraft that arrived late.'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`LATE_AIRCRAFT_DELAY`：由于同一架飞机的先前航班延误而造成的延误分钟数。'
- en: Data preparation
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据准备
- en: 'For starters, `PLANNED_DEP_DATETIME` must be a datetime data type:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`PLANNED_DEP_DATETIME`必须是日期时间数据类型：
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The exact day and time of a flight don’t matter, but maybe the month and day
    of the week do because of weather and seasonal patterns that can only be appreciated
    at this level of granularity. Also, the executives mentioned weekends and winters
    being especially bad for delays. Therefore, we will create features for the month
    and day of the week:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 飞行的确切日期和时间并不重要，但也许月份和星期几很重要，因为天气和季节性模式只能在这个粒度级别上得到欣赏。此外，提到的管理人员表示周末和冬季的延误尤其严重。因此，我们将为月份和星期几创建特征：
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We don’t need the `PLANNED_DEP_DATETIME` column so let’s drop it like this:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要`PLANNED_DEP_DATETIME`列，所以让我们像这样删除它：
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'It is essential to record whether the arrival or destination airport is a hub.
    AA, in 2019, had 10 hubs: Charlotte, Chicago–O’Hare, Dallas/Fort Worth, Los Angeles,
    Miami, New York–JFK, New York–LaGuardia, Philadelphia, Phoenix–Sky Harbor, and
    Washington–National. Therefore, we can encode which `ORIGIN` and `DEST` airports
    are AA hubs using their IATA codes, and get rid of columns with codes since they
    are too specific (`FL_NUM`, `ORIGIN`, and `DEST`):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 记录到达或目的地机场是否为枢纽机场是至关重要的。美国航空公司（AA）在2019年有10个枢纽机场：夏洛特、芝加哥-奥黑尔、达拉斯/沃斯堡、洛杉矶、迈阿密、纽约-肯尼迪、纽约-拉瓜迪亚、费城、凤凰城-天港和华盛顿-国家机场。因此，我们可以使用它们的IATA代码来编码哪些`ORIGIN`和`DEST`机场是AA的枢纽机场，并删除包含代码的列，因为它们过于具体（`FL_NUM`、`ORIGIN`和`DEST`）：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After all these operations, we have a fair number of useful features, but we
    are yet to determine the target feature. There are two columns that could serve
    this purpose. We have `ARR_DELAY`, which is the total number of minutes delayed
    regardless of the reason, and then there’s `CARRIER_DELAY`, which is just the
    total number of those minutes that can be attributed to the airline. For instance,
    look at the following sample of flights delayed over 15 minutes (which is considered
    late according to the airline’s definition):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行所有这些操作之后，我们拥有相当数量的有用特征，但我们尚未确定目标特征。有两列可以用于此目的。我们有`ARR_DELAY`，这是无论原因如何延迟的总分钟数，然后是`CARRIER_DELAY`，这只是可以归因于航空公司的那些分钟数的总和。例如，看看以下样本，这些航班延误超过15分钟（根据航空公司的定义，这被认为是延误）：
- en: '[PRE8]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code outputs *Figure 3.1*:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了**图3.1**：
- en: '![Table  Description automatically generated](img/B18406_03_01.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_01.png)'
- en: 'Figure 3.1: Sample observations with arrival delays over 15 minutes'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**图3.1**：样本观察值，到达延误超过15分钟'
- en: 'Of all the delays in *Figure 3.1*, one of them (#26) wasn’t at all the responsibility
    of the airline because only 0 minutes could be attributed to the airline. Four
    of them were partially the responsibility of the airline (#8, #16, #33, and #40),
    two of which were over 15 minutes late due to the airline (#8 and #40). The rest
    of them were entirely the airline’s fault. We can tell that although the total
    delay is useful information, the airline executives were only interested in delays
    caused by the airline so `ARR_DELAY` can be discarded. Furthermore, there’s another
    more important reason it should be discarded, and it’s that if the task at hand
    is to predict a delay, we cannot use pretty much the very same delay (minus the
    portions not due to the airline) to predict it. For this very same reason, it
    is best to remove `ARR_DELAY`:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.1*的所有延误中，其中之一（#26）根本不是航空公司的责任，因为只有0分钟可以归因于航空公司。其中四个部分是航空公司的责任（#8、#16、#33和#40），其中两个由于航空公司而晚于15分钟（#8和#40）。其余的完全是航空公司的责任。我们可以看出，尽管总延误是有用的信息，但航空公司的高管们只对由航空公司造成的延误感兴趣，因此`ARR_DELAY`可以被丢弃。此外，还有一个更重要的原因应该丢弃它，那就是如果当前的任务是预测延误，我们不能使用几乎完全相同的延误（减去不是由于航空公司的部分）来预测它。出于这个原因，最好移除`ARR_DELAY`：
- en: '[PRE9]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, we can put the target feature alone as `y` and all the rest as `X`.
    After this, we split `y` and `X` into train and test datasets. Please note that
    the target feature (`y`) stays the same for regression, so we split it into `y_train_reg`
    and `y_test_reg`. However, for classification, we must make binary versions of
    these labels denoting whether it’s more than 15 minutes late or not, called `y_train_class`
    and `y_test_class`. Please note that we are setting a fixed `random_state` for
    reproducibility:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将目标特征单独作为`y`，其余所有特征作为`X`。之后，我们将`y`和`X`分为训练集和测试集。请注意，对于回归，目标特征（`y`）保持不变，因此我们将其分为`y_train_reg`和`y_test_reg`。然而，对于分类，我们必须将这些标签的二进制版本表示为是否晚于15分钟以上，称为`y_train_class`和`y_test_class`。请注意，我们正在设置一个固定的`random_state`以确保可重复性：
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'To examine how linearly correlated the features are to the target `CARRIER_DELAY`,
    we can compute Pearson’s correlation coefficient, turn coefficients to absolute
    values (because we aren’t interested in whether they are positively or negatively
    correlated), and sort them in descending order:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查特征与目标`CARRIER_DELAY`的线性相关性，我们可以计算皮尔逊相关系数，将系数转换为绝对值（因为我们不感兴趣它们是正相关还是负相关），并按降序排序：
- en: '[PRE11]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As you can tell from the output, only one feature (`DEP_DELAY`) is highly correlated.
    The others aren’t:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从输出中可以看出，只有一个特征（`DEP_DELAY`）高度相关。其他特征则不然：
- en: '[PRE12]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: However, this is only *linearly* correlated and on a one-by-one basis. It doesn’t
    mean that they don’t have a non-linear relationship, or that several features
    interacting together wouldn’t impact the target. In the next section, we will
    discuss this further.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仅仅是*线性*相关的，并且是逐个比较的。这并不意味着它们没有非线性关系，或者几个特征相互作用不会影响目标。在下一节中，我们将进一步讨论这个问题。
- en: Reviewing traditional model interpretation methods
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 审查传统的模型解释方法
- en: To explore as many model classes and interpretation methods as possible, we
    will fit the data into regression and classification models.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 为了尽可能探索多种模型类别和解释方法，我们将数据拟合到回归和分类模型中。
- en: Predicting minutes delayed with various regression methods
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用各种回归方法预测延误的分钟数
- en: 'To compare and contrast regression methods, we will first create a dictionary
    named `reg_models`. Each model is its own dictionary and the function that creates
    it is the `model` attribute. This structure will be used later to neatly store
    the fitted model and its metrics. Model classes in this dictionary have been chosen
    to represent several model families and to illustrate important concepts that
    we will discuss later:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较和对比回归方法，我们首先创建一个名为`reg_models`的字典。每个模型都是其自己的字典，创建它的函数是`model`属性。这种结构将在以后用来整洁地存储拟合的模型及其指标。这个字典中的模型类别已被选择来代表几个模型家族，并展示我们将要讨论的重要概念：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Before we start fitting the data to these models, we will briefly explain them
    one by one:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们将数据拟合到这些模型之前，我们将逐一简要解释它们：
- en: '`linear`: **Linear regression** was the first model class we discussed. For
    better or for worse, it makes several assumptions about the data. Chief among
    them is the assumption that the prediction must be a linear combination of *X*
    features. This, naturally, limits the capacity to discover non-linear relationships
    and interactions among the features.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linear`: **线性回归**是我们讨论的第一个模型类别。不管好坏，它对数据做出了一些假设。其中最重要的是假设预测必须是*X*特征的线性组合。这自然限制了发现特征之间非线性关系和交互的能力。'
- en: '`linear_poly`: **Polynomial regression** extends linear regression by adding
    polynomial features. In this case, as indicated by `degree=2`, the polynomial
    degree is two, so it’s quadratic. This means, in addition to having all features
    in their monomial form (for example, `DEP_FPH`), it also has them in a quadratic
    form (for example, `DEP_FPH²`), plus the many interaction terms for all of the
    21 features. In other words, for `DEP_FPH`, there would be interaction terms such
    as `DEP_FPH` ![](img/B18406_03_001.png) `DISTANCE`, `DEP_FPH` ![](img/B18406_03_001.png)
    `DELAY`, and so on for the rest of the features.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linear_poly`: **多项式回归**通过添加多项式特征扩展了线性回归。在这种情况下，如`degree=2`所示，多项式度数为二，因此它是二次的。这意味着除了所有特征都以单变量形式（例如，`DEP_FPH`）存在之外，它们还以二次形式存在（例如，`DEP_FPH²`），以及所有21个特征的许多交互项。换句话说，对于`DEP_FPH`，会有如`DEP_FPH`
    ![](img/B18406_03_001.png) `DISTANCE`、`DEP_FPH` ![](img/B18406_03_001.png) `DELAY`等交互项，以及其他所有特征的类似项。'
- en: '`linear_interact`: This is just like the **polynomial regression** model but
    without the quadratic terms – in other words, only the interactions, as `interaction_only=True`
    would suggest. It’s useful because there is no reason to believe any of our features
    have a relationship that is better fitted with quadratic terms. Still, perhaps
    it’s the interaction with other features that makes an impact.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`linear_interact`: 这就像**多项式回归**模型，但没有二次项——换句话说，只有交互项，正如`interaction_only=True`所暗示的那样。它是有用的，因为我们没有理由相信我们的任何特征与二次项有更好的拟合关系。然而，也许正是与其他特征的交互产生了影响。'
- en: '`ridge`: **Ridge regression** is a variation of linear regression. However,
    even though the method behind linear regression, called **ordinary least squares**
    (**OLS**), does a pretty good job of reducing the error and fitting the model
    to the features, it does it without considering **overfitting**. The problem here
    is that OLS treats all features equally, so the model becomes more complex as
    each variable is added. As the word *overfitting* suggests, the resulting model
    fits the training data too well, resulting in the lowest bias but the highest
    variance. There’s a sweet spot in this **trade-off between bias and variance**,
    and one way of getting to this spot is by reducing the complexity added by the
    introduction of too many features. Linear regression is not equipped to do so
    on its own.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ridge`: **岭回归**是线性回归的一种变体。然而，尽管线性回归背后的方法，称为**普通最小二乘法**（**OLS**），在减少误差和将模型拟合到特征方面做得相当不错，但它并没有考虑**过拟合**。问题在于OLS平等地对待所有特征，因此随着每个变量的增加，模型变得更加复杂。正如单词*过拟合*所暗示的，结果模型对训练数据拟合得太好，导致最低的偏差但最高的方差。在这个**偏差和方差之间的权衡**中有一个甜蜜点，而到达这个点的一种方法是通过减少引入过多特征所增加的复杂性。线性回归本身并没有装备去做到这一点。'
- en: This is where ridge regression comes along, with our friend **regularization**.
    It does this by shrinking coefficients that don’t contribute to the outcome with
    a penalty term called the **L2 norm**. It penalizes complexity, thus constraining
    the algorithm from overfitting. In this example, we use a cross-validated version
    of `ridge` (`RidgeCV`) that tests several regularization strengths (`alphas`).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 正是在这里，岭回归伴随着我们的朋友**正则化**出现。它是通过引入一个称为**L2范数**的惩罚项来缩小对结果没有贡献的系数来做到这一点的。它惩罚复杂性，从而约束算法不过拟合。在这个例子中，我们使用了一个交叉验证版本的`ridge`（`RidgeCV`），它测试了几个正则化强度（`alphas`）。
- en: '`decision_tree`: A **decision tree** is precisely as the name suggests. Imagine
    a tree-like structure where at every point that branches subdivide to form more
    branches, there is a “test” performed on a feature, partitioning the datasets
    into each branch. When branches stop subdividing, they become leaves, and at every
    leaf, there’s *a decision*, be it to assign a *class* for classification or a
    fixed value for regression. We are limiting this tree to `max_depth=7` to prevent
    overfitting because the larger the tree, the better it will fit our training data,
    and the less likely the tree will generalize to non-training data.'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decision_tree`: **决策树**正如其名所示。想象一个树状结构，在每一个分支点，数据集被细分以形成更多的分支，都会对某个特征进行“测试”，将数据集划分到每个分支。当分支停止细分时，它们变成叶子节点，在每一个叶子节点，都会做出一个“决策”，无论是为分类分配一个**类别**还是为回归提供一个固定值。我们将此树限制在
    `max_depth=7` 以防止过拟合，因为树越大，它将更好地拟合我们的训练数据，并且越不可能将树泛化到非训练数据。'
- en: '`rule_fit`: **RuleFit** is a regularized linear regression expanded to include
    feature interactions in the form of rules. The rules are formed by traversing
    a decision tree, except it discards the leaves and keeps the feature interactions
    found traversing the branches toward these leaves. It uses **LASSO Regression**,
    which, like ridge, uses regularization, but instead of using the **L2 norm**,
    it uses the **L1 norm**. The result is that useless features end up with a coefficient
    of zero and do not just converge to zero, as they do with L2, which makes it easy
    for the algorithm to filter them out. We are limiting the rules to 150 (`max_rules=150`)
    and the attribute `rfmode=''regress''` tells RuleFit that this is a regression
    problem, since it can also be used for classification. Unlike all other models
    used here, this isn’t a scikit-learn one but was created by Christoph Molnar adapting
    a paper called *Predictive learning via rule ensembles*.'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rule_fit`: **RuleFit** 是一种正则化线性回归，扩展到包括以规则形式出现的特征交互。这些规则是通过遍历决策树形成的，除了它丢弃了叶子节点并保留了在向这些叶子节点分支过程中发现的特征交互。它使用
    **LASSO 回归**，与岭回归类似，使用正则化，但它使用的是 **L1 范数**而不是 **L2 范数**。结果是，无用的特征最终会得到零系数，并且不会像
    L2 那样简单地收敛到零，这使得算法可以轻松地将它们过滤掉。我们将规则限制在 150 条（`max_rules=150`），属性 `rfmode=''regress''`
    告诉 RuleFit 这是一个回归问题，因为它也可以用于分类。与这里使用的所有其他模型不同，这不是一个 scikit-learn 模型，而是由 Christoph
    Molnar 创建的，他改编了一篇名为 *Predictive learning via rule ensembles* 的论文。'
- en: '`knn`: **k-Nearest Neighbors** (**k-NN**) is a simple method based on the *locality*
    assumption, which is that data points that are close to each other are similar.
    In other words, they must have similar predicted values, and, in practice, this
    isn’t a bad guess, so it takes data points nearest to the point you want to predict
    and derives a prediction based on that. In this case, `n_neighbors=7` so k = 7\.
    It’s an **instance-based machine learning model**, also known as a **lazy learner**
    because it simply stores the training data. During inference, it employs training
    data to calculate the similarity with points and generate a prediction based on
    that. This is opposed to what model-based machine learning techniques, or **eager
    learners**, do, which is to use training data to learn formulas, parameters, coefficients,
    or bias/weights, which they then leverages to make a prediction during inference.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`knn`: **k-Nearest Neighbors** (**k-NN**) 是一种基于**局部性**假设的简单方法，该假设认为彼此靠近的数据点相似。换句话说，它们必须具有相似的预测值，而在实践中，这并不是一个糟糕的猜测，因此它选取离你想要预测的点最近的数据点，并根据这些点进行预测。在这种情况下，`n_neighbors=7`，所以
    k = 7。它是一个**基于实例的机器学习模型**，也称为**懒惰学习器**，因为它只是存储训练数据。在推理过程中，它使用训练数据来计算与点的相似性，并根据这些相似性生成预测。这与基于模型的机器学习技术，或**急切学习器**所做的方法相反，后者使用训练数据来学习公式、参数、系数或偏差/权重，然后利用这些信息在推理过程中进行预测。'
- en: '`random_forest`: Imagine not one but hundreds of decision trees trained on
    random combinations of the features and random samples of data. **Random forest**
    takes an average of these randomly generated decision trees to create the best
    tree. This concept of training less effective models in parallel and combining
    them using an averaging process is called **bagging**. It is an **ensemble** method
    because it combines more than one model (usually called **weak learners**) into
    a **strong learner**. In addition to *bagging*, there are two other ensemble techniques,
    called **boosting** and **stacking**. For bagging deeper, trees are better because
    they reduce variance, so this is why we are using `max_depth=7`.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`随机森林`：想象一下，不是一棵，而是成百上千棵决策树，这些决策树是在特征随机组合和数据随机样本上训练的。**随机森林**通过平均这些随机生成的决策树来创建最佳树。这种在并行训练较少有效模型并使用平均过程将它们组合起来的概念被称为**袋装法**。它是一种**集成**方法，因为它将多个模型（通常称为**弱学习器**）组合成一个**强学习器**。除了**袋装法**之外，还有两种其他集成技术，称为**提升法**和**堆叠法**。对于更深的袋装，树更好，因为它们减少了方差，这就是为什么我们使用`max_depth=7`的原因。'
- en: '`mlp`: **A multi-layer perceptron** is a “vanilla” feedforward (sequential)
    neural network, so it uses non-linear activation functions `(MLPRegressor` uses
    *ReLU* by default), stochastic gradient descent, and backpropagation. In this
    case, we are using 21 neurons in the first and only hidden layer, hence `hidden_layer_sizes=(21,)`,
    running training for 500 epochs (`max_iter=500`), and terminating training when
    the validation score is not improving (`early_stopping=True`).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp`：**多层感知器**是一个“普通”的前馈（顺序）神经网络，因此它使用非线性激活函数（`MLPRegressor`默认使用*ReLU*），随机梯度下降和反向传播。在这种情况下，我们在第一个也是唯一的隐藏层中使用21个神经元，因此`hidden_layer_sizes=(21,)`，运行500个训练周期（`max_iter=500`），并在验证分数不再提高时终止训练（`early_stopping=True`）。'
- en: If you are unfamiliar with some of these models, don’t fret! We will cover them
    in more detail later in this chapter and the book. Also, please note that some
    of these models have a random process somewhere. To ensure reproducibility, we
    have set `random_state`. It is best to always set this; otherwise, it will randomly
    set it every single time, which will make your results hard to reproduce.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对这些模型中的某些不熟悉，不要担心！我们将在本章和书中更详细地介绍它们。此外，请注意，这些模型中的某些模型在某个地方有一个随机过程。为了确保可重复性，我们已经设置了`random_state`。最好总是设置它；否则，它将每次随机设置，这将使你的结果难以重复。
- en: 'Now, let’s iterate over our dictionary of models (`reg_models`), fit them to
    the training data, and predict and compute two metrics based on the quality of
    these predictions. We’ll then save the fitted model, test predictions, and metrics
    in the dictionary for later use. Note that `rulefit` only accepts `numpy` arrays,
    so we can’t `fit` it in the same way. Also, note that `rulefit` and `mlp` take
    longer than the rest to train, so this can take a few minutes to run:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们遍历我们的模型字典（`reg_models`），将它们拟合到训练数据上，并根据这些预测的质量计算两个指标。然后我们将保存拟合的模型、测试预测和指标到字典中，以供以后使用。请注意，`rulefit`只接受`numpy`数组，所以我们不能以同样的方式`fit`它。另外，请注意，`rulefit`和`mlp`的训练时间比其他模型长，所以这可能需要几分钟才能运行：
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can now convert the dictionary to a `DataFrame` and display the metrics
    in a sorted and color-coded fashion:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将字典转换为`DataFrame`，并以排序和彩色编码的方式显示指标：
- en: '[PRE15]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code outputs *Figure 3.2*. Please note that color-coding doesn’t
    work in all Jupyter Notebook implementations:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了*图3.2*。请注意，彩色编码并不适用于所有Jupyter Notebook实现：
- en: '![Table  Description automatically generated](img/B18406_03_02.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_02.png)'
- en: 'Figure 3.2: Regression metrics for our models'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2：我们模型的回归指标
- en: 'To interpret the metrics in *Figure 3.2*, we ought to first understand what
    they mean, both in general and in the context of this regression exercise:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释*图3.2*中的指标，我们首先应该了解它们在一般和回归练习的上下文中的含义：
- en: '**RMSE**: **Root Mean Square Error** is defined as the standard deviation of
    the residuals. It’s the square root of the squared residuals divided by the number
    of observations – in this case, flights. It tells you, on average, how far apart
    the predictions are from the actuals, and as you can probably tell from the color-coding,
    less is better because you want your predictions to be as close as possible to
    the actuals in the *test* (**hold-out**) dataset. We have also included this metric
    for the **train** dataset to see how well it’s generalizing. You expect the test
    error to be higher than the training error, but not by much. If it is, like it
    is for `random_forest`, you need to tune some of the parameters to reduce overfitting.
    In this case, reducing the trees’ maximum depth, increasing the number of trees
    (also called **estimators**), and reducing the maximum number of features to use
    should do the trick. On the other hand, with `knn`, you can adjust the number
    of neighbors, but it is expected, because of its **lazy learner** nature, to overperform
    on the training data.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSE**：**均方根误差**定义为残差的均方差。它是平方残差除以观测数（在这种情况下，航班）的平方根。它告诉你，平均而言，预测值与实际值之间的差距有多大，正如你可能从颜色编码中看出，差距越小越好，因为你想让你的预测值尽可能接近实际值在
    *测试*（**保留**）数据集中的实际值。我们还包含了该指标用于 **训练** 数据集，以查看其泛化能力如何。你预计测试误差将高于训练误差，但不会高很多。如果是这样，就像
    `random_forest` 的情况一样，你需要调整一些参数以减少过拟合。在这种情况下，减少树的最大深度，增加树的数量（也称为 **估计器**），以及减少可使用特征的最大数量应该会有效。另一方面，对于
    `knn`，你可以调整邻居的数量，但由于其 **懒惰学习器** 的特性，预计在训练数据上表现良好。'
- en: In any case, these numbers are pretty good because even our worst performing
    model is below a test RMSE of 10 minutes, and about half of them have a test RMSE
    of less than 7.5, quite possibly predicting a delay effectively, on average, since
    the threshold for a delay is 15 minutes.
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 无论如何，这些数字相当不错，因为即使是我们表现最差的模型，其测试 RMSE 也低于 10 分钟，大约一半的模型测试 RMSE 低于 7.5，很可能在平均意义上有效地预测了延误，因为延误的阈值是
    15 分钟。
- en: Note that `linear_poly` is the second and `linear_interact` is the fourth most
    performant model, significantly ahead of `linear`, suggesting that non-linearity
    and interactivity are important factors to produce better predictive performance.
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，`linear_poly` 是第二高效模型，而 `linear_interact` 是第四高效模型，显著优于 `linear`，这表明非线性交互是产生更好预测性能的重要因素。
- en: '**R**²: **R-squared** is also known as the **coefficient of determination**.
    It’s defined as the proportion of the variance in the *y* (outcome) target that
    can be explained by the *X* (predictors) features in the model. It answers the
    question of what proportion of the model variability is explainable? And as you
    can probably tell from the color-coding, more is better. And our models appear
    to include significant *X* features, as evidenced by our *Pearson’s correlation
    coefficients*. So if this *R*² value was low, perhaps adding additional features
    would help, such as flight logs, terminal conditions, and even those things airline
    executives said they weren’t interested in exploring right now, such as *knock-off*
    effects and international flights. These could fill in the gaps in the unexplained
    variance.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R**²：**R平方**也称为**确定系数**。它定义为模型中 *y*（结果）目标中可以由 *X*（预测器）特征解释的方差比例。它回答了模型变异性中有多少比例是可解释的问题？正如你可能从颜色编码中看出，越多越好。我们的模型似乎包括重要的
    *X* 特征，正如我们的 *皮尔逊相关系数* 所证明的那样。所以如果这个 *R*² 值很低，也许添加额外的特征会有所帮助，例如航班日志、终端条件，甚至那些航空公司高管表示他们现在不感兴趣探索的事情，比如
    *仿制品* 影响，以及国际航班。这些可以填补未解释方差中的空白。'
- en: Let’s see if we can get good metrics with classification.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是否能通过分类获得良好的指标。
- en: Classifying flights as delayed or not delayed with various classification methods
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用各种分类方法将航班分类为延误或未延误
- en: 'Just as we did with regression, to compare and contrast classification methods,
    we will first create a dictionary for them named `class_models`. Each model is
    its own dictionary and the function that creates it is the `model` attribute.
    This structure will be used later to store the fitted model and its metrics. Model
    classes in this dictionary have been chosen to represent several model families
    and to illustrate important concepts that we will discuss later. Some of these
    will look familiar because they are the same methods used in regression but applied
    to classification:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在回归中做的那样，为了比较和对比分类方法，我们首先为它们创建一个名为 `class_models` 的字典。每个模型都是一个自己的字典，创建它的函数是
    `model` 属性。这种结构将用于稍后存储拟合的模型及其指标。这个字典中的模型类被选择来代表几个模型家族，并展示我们将要讨论的重要概念。其中一些可能看起来很熟悉，因为它们是回归中使用的相同方法，但应用于分类：
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Before we start fitting the data to these models, we will briefly explain them
    one by one:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始将这些模型拟合到数据之前，我们将逐一简要解释它们：
- en: '`logistic`: **logistic regression** was introduced in *Chapter 2*, *Key Concepts
    of Interpretability*. It has many of the same pros and cons as **linear regression**.
    For instance, feature interactions must be added manually. Like other classification
    models, it returns a probability between 0 and 1, which, when closer to 1, denotes
    a probable match to a **positive class** while, when closer to 0, it denotes an
    improbable match to the **positive class**, and therefore a probable match to
    the **negative class**. Naturally, 0.5 is the threshold used to decide between
    classes, but it doesn’t have to be. As we will examine later in the book, there
    are interpretation and performance reasons to adjust the threshold. Note that
    this is a binary classification problem, so we are only choosing between delayed
    (positive) and not delayed (negative), but this method could be extended to multi-class
    classification. It would then be called **multinomial classification**.'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`logistic`：**逻辑回归**在**第 2 章**，**可解释性的关键概念**中介绍。它具有与**线性回归**许多相同的优缺点。例如，必须手动添加特征交互。与其他分类模型一样，它返回一个介于
    0 和 1 之间的概率，当接近 1 时，表示与**正类**的匹配可能性较大，而当接近 0 时，表示与**正类**的匹配可能性较小，因此更可能是**负类**。自然地，0.5
    是用于决定类别的阈值，但不必如此。正如我们将在本书稍后考察的那样，调整阈值有解释和性能的原因。请注意，这是一个二分类问题，所以我们只是在延迟（正类）和未延迟（负类）之间进行选择，但这种方法可以扩展到多分类。那时它将被称为**多项式分类**。'
- en: '`ridge`: **Ridge classification** leverages the same regularization technique
    used in **ridge regression** but applied to classification. It does this by converting
    the target values to -1 (for a negative class) and keeping 1 for a positive class
    and then performing ridge regression. At its heart, its regression in disguise
    will predict values between -1 and 1, and then convert them back to a 0–1 scale.
    Like with `RidgeCV` for regression, `RidgeClassifierCV` uses leave-one-out cross-validation,
    which means it first splits the data into different equal-size sets – in this
    case, we are using five sets (`cv=5`) – and then removes features one at a time
    to see how well the model performs without them, on average in all the five sets.
    Those features that don’t make much of a difference are penalized by testing several
    regularization strengths (`alphas`) to find the optimal strength. As with all
    *regularization* techniques, the point is to discourage learning from unnecessary
    complexity, minimizing the impact of less salient features.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ridge`：**岭分类**利用与**岭回归**中使用的相同正则化技术，但应用于分类。它是通过将目标值转换为 -1（对于负类）并保留 1（对于正类）来实现的，然后执行岭回归。在其核心，这种伪装的回归将预测介于
    -1 和 1 之间的值，然后将它们转换回 0–1 的尺度。与回归中的 `RidgeCV` 类似，`RidgeClassifierCV` 使用留一法交叉验证，这意味着它首先将数据分割成不同大小相等的集合——在这种情况下，我们使用五个集合（`cv=5`）——然后逐个移除特征，以查看模型在没有这些特征的情况下表现如何，平均在所有五个集合上。那些没有太大差别的特征会通过测试几个正则化强度（`alphas`）来受到惩罚，以找到最佳强度。与所有**正则化**技术一样，目的是阻止从不必要的复杂性中学习，最小化不太显著的特征的影响。'
- en: '`decision_tree`: A standard **decision tree**, such as this one, is also known
    as a **CART** (**classification and regression tree**) because it can be used
    for regression or classification tasks. It has the same algorithm for both tasks
    but functions slightly differently, like the algorithm used to decide where to
    “split” a branch. In this case, we are only allowing our trees to have a depth
    of 7.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decision_tree`：这种标准的**决策树**，如这个例子，也被称为**CART**（**分类和回归树**），因为它可以用于回归或分类任务。它对这两个任务都有相同的算法，但功能略有不同，比如用于决定在哪里“分裂”分支的算法。在这种情况下，我们只允许我们的树具有7的深度。'
- en: '`knn`: **k-NN** can also be applied to classification tasks, except instead
    of averaging what the nearest neighbors’ target features (or labels) are, it chooses
    the most frequent one (also known as the **mode**). We are also using a k-value
    of 7 for classification (`n_neighbors`).'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`knn`：**k-NN**也可以应用于分类任务，除了不平均最近邻的目标特征（或标签）之外，它选择最频繁的一个（也称为**众数**）。我们还在分类中使用7作为k值（`n_neighbors`）。'
- en: '`naive_bayes`: **Gaussian Naïve Bayes** is part of the family of *Naïve Bayes*
    classifiers, which are called naïve because they make the assumption that the
    features are independent of each other, which is usually not the case. This dramatically
    impedes its capacity to predict unless the assumption is correct. It’s called
    *Bayes* because it’s based on **Bayes’ theorem of conditional probabilities**,
    which is that the conditional probability of a class is the class probability
    times the feature probability given the class. *Gaussian Naïve Bayes* makes an
    additional assumption, which is that continuous values have a normal distribution,
    also known as a **Gaussian distribution**.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`naive_bayes`：**高斯朴素贝叶斯**是**朴素贝叶斯**分类器家族的一部分，之所以称为朴素，是因为它们假设特征之间相互独立，这通常不是情况。除非这个假设是正确的，否则这会极大地限制其预测能力。它被称为**贝叶斯**，因为它基于**贝叶斯条件概率定理**，即类的条件概率是类的概率乘以给定类的特征概率。**高斯朴素贝叶斯**还做出了一个额外的假设，即连续值具有正态分布，也称为**高斯分布**。'
- en: '`gradient_boosting`: Like **random forest**, **gradient-boosted trees** are
    also an ensemble method, but that leverages **boosting** instead of **bagging**.
    **Boosting** doesn’t work in parallel but in sequence, iteratively training weak
    learners and incorporating their strengths into a stronger learner, while adapting
    another weak learner to tackle their weaknesses. Although ensembles and boosting,
    in particular, can be done with a model class, this method uses decision trees.
    We have limited the number of trees to 210 (`n_estimators=210`).'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`gradient_boosting`：与**随机森林**类似，**梯度提升树**也是一种集成方法，但它使用**提升**而不是**袋装**。**提升**不是并行工作，而是按顺序，迭代地训练弱学习器，并将它们的优势纳入更强的学习器中，同时调整另一个弱学习器来克服它们的弱点。尽管集成和提升，尤其是提升，可以用模型类来完成，但这种方法使用的是决策树。我们将树的数量限制为210（`n_estimators=210`）。'
- en: '`random_forest`: The same **random forest** as with regression except it generates
    classification decision trees and not regression trees.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_forest`：与回归相同的**随机森林**，除了它生成分类决策树而不是回归树。'
- en: '`mlp`: The same **multi-layer perceptron** as with regression, but the output
    layer, by default, uses a **logistic** function in the output layer to yield probabilities,
    which it then converts to 1 or 0, based on the 0.5 threshold. Another difference
    is that we are using seven neurons in the first and only hidden layer (`hidden_layer_sizes=(7,)`)
    because binary classification tends to require fewer of them to achieve an optimal
    result.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mlp`：与回归相同的**多层感知器**，但默认情况下，输出层使用**对数**函数来产生概率，然后根据0.5阈值将其转换为1或0。另一个区别是我们使用七个神经元在第一个也是唯一的隐藏层中（`hidden_layer_sizes=(7,)`），因为二分类通常需要较少的神经元来实现最佳结果。'
- en: 'Please note that some of these models use balanced weights for the classes
    (`class_weight=''balanced''`), which is very important because this happens to
    be an **imbalanced classification** task. By that, we mean that negative classes
    vastly outnumber positive classes. We can find out what this looks like for our
    training data:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这些模型中的一些使用平衡权重进行类别分配（`class_weight='balanced'`），这一点非常重要，因为这是一个**不平衡分类**任务。换句话说，负类远远多于正类。我们可以查看我们的训练数据是什么样的：
- en: '[PRE17]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As you can see, the output in our training data’s positive classes represents
    only 6% of the total. Models that account for this will achieve *more balanced*
    results. There are different ways of accounting for *class imbalance*, which we
    will discuss in further detail in *Chapter 11*, *Bias Mitigation and Causal Inference
    Methods*, but `class_weight='balanced'` applies a weight inversely proportional
    to class frequencies, giving the outnumbered *positive* class a leg up.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，训练数据中正类的输出只占总数的6%。考虑到这一点，模型将实现**更平衡**的结果。有几种方法可以处理**类别不平衡**，我们将在第11章**偏差缓解和因果推断方法**中详细讨论，但`class_weight='balanced'`通过类频率的反比应用权重，给数量较少的**正类**一个优势。
- en: Training and evaluating the classification models
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练和评估分类模型
- en: '[PRE18]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now convert the dictionary to a `DataFrame` and display the metrics
    in a sorted and color-coded fashion:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以将字典转换为`DataFrame`，并以排序和彩色编码的方式显示指标：
- en: '[PRE20]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code outputs *Figure 3.3*:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了**图3.3**：
- en: '![Table  Description automatically generated](img/B18406_03_03.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_03.png)'
- en: 'Figure 3.3: Classification metrics for our models'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3：我们模型的分类指标
- en: 'To interpret the metrics in *Figure 3.3*, we ought to first understand what
    they mean, both in general and in the context of this classification exercise:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释**图3.3**中的指标，我们首先应该了解它们在一般意义上的含义，以及在这个分类练习中的具体含义：
- en: '**Accuracy**: Accuracy is the simplest way to measure the effectiveness of
    a classification task, and it’s the percentage of correct predictions over all
    predictions. In other words, in a binary classification task, you can calculate
    this by adding the number of **True Positives** (**TPs**)and **True Negatives**
    (**TNs**) and dividing them by a tally of all predictions made. As with regression
    metrics, you can measure accuracy for both train and test to gauge overfitting.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：准确率是衡量分类任务有效性的最简单方法，它是所有预测中正确预测的百分比。换句话说，在二元分类任务中，你可以通过将**真正例**（TPs）和**真负例**（TNs）的数量相加，然后除以所有预测的总数来计算这个值。与回归指标一样，你可以测量训练和测试的准确率来评估过拟合。'
- en: '**Recall**: Even though accuracy sounds like a great metric, recall is much
    better in this case and the reason is you could have an accuracy of 94%, which
    sounds pretty good, but it turns out you are always predicting no delay! In other
    words, even if you get high accuracy, it is meaningless unless you are predicting
    accurately for the least represented class, delays. We can find this number with
    recall (also known as **sensitivity** or **true positive rate**), which is ![](img/B18406_03_003.png)
    , and it can be interpreted as how much of the relevant results were returned
    – in other words, in this case, what percentage of the actual delays were predicted.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**回忆率**：尽管准确率听起来像是一个很好的指标，但在这种情况下，召回率要好得多，原因是你可能会有94%的准确率，这听起来相当不错，但结果却是你总是预测没有延迟！换句话说，即使你得到了很高的准确率，除非你对最不常见的类别，即延迟，进行准确预测，否则它就没有意义。我们可以通过召回率（也称为**灵敏度**或**真正例率**）来找到这个数字，它表示为
    ![](img/B18406_03_003.png) ，它可以解释为有多少相关结果被返回——换句话说，在这种情况下，实际延迟的百分比是多少。'
- en: Another good measure involving true positives is **precision**, which is how
    much our predicted samples are relevant, which is ![](img/B18406_03_004.png).
    In this case, that would be what percentage of predicted delays were actual delays.
    For imbalanced classes, it is recommended to use both, but depending on your preference
    for *FN* over *FP*, you will prefer recall over precision or vice versa.
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 另一个涉及真正例的好指标是**精确率**，它表示我们的预测样本的相关性，表示为 ![](img/B18406_03_004.png) 。在这种情况下，这将是预测的延迟中有多少是实际延迟。对于不平衡的类别，建议同时使用两者，但根据你对**假正例**（FP）的偏好，你可能更喜欢召回率而不是精确率，反之亦然。
- en: '**ROC-AUC**: **ROC** is an acronym for **Receiver Operating Characteristic**
    and was designed to separate signal from noise. What it does is plot the proportion
    of **true positive rate** (**recall**) on the *x* axis and the false positive
    rate on the *y* axis. **AUC** stands for **area under the curve**, which is a
    number between 0 and 1 that assesses the prediction ability of the classifier
    1 being perfect, 0.5 being as good as a random coin toss, and anything lower meaning
    that if we inverted the results of our prediction, we would have a better prediction.
    To illustrate this, let’s generate a ROC curve for our worst-performing model,
    Naïve Bayes, according to the AUC metric:'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ROC-AUC**: **ROC**代表**接收者操作特征**，它被设计用来区分信号和噪声。它所做的是在*x*轴上绘制**真正例率**（**召回率**）的比例，在*y*轴上绘制假正例率。**AUC**代表**曲线下面积**，这是一个介于0和1之间的数字，用于评估分类器的预测能力：1表示完美，0.5表示与随机抛硬币一样好，任何低于这个值的都意味着如果我们反转预测结果，我们会得到更好的预测。为了说明这一点，让我们根据AUC指标为我们的表现最差的模型，朴素贝叶斯，生成一个ROC曲线：'
- en: '[PRE21]'
  id: totrans-155
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The preceding code outputs *Figure 3.4*. Note that the diagonal line signifies
    half the area. In other words, the point where it has a coin-toss-like prediction
    quality:'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 上述代码输出了*图3.4*。注意，对角线表示一半的面积。换句话说，它具有类似抛硬币预测质量的点：
- en: '![A picture containing polygon  Description automatically generated](img/B18406_03_04.png)'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![包含多边形的图片 自动生成描述](img/B18406_03_04.png)'
- en: 'Figure 3.4: ROC curve for Naïve Bayes'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图3.4：朴素贝叶斯的ROC曲线
- en: '**F1**: The **F1-score** is also called the harmonic average of precision and
    recall because it’s calculated like this: ![](img/B18406_03_005.png). Since it
    includes both precision and recall metrics, which pertain to the proportion of
    true positives, it’s a good metric choice to use when your dataset is imbalanced,
    and you don’t prefer either precision or recall.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1**: **F1分数**也称为精确率和召回率的调和平均，因为它是这样计算的：![](img/B18406_03_005.png)。由于它包括了精确率和召回率指标，这些指标与真正例的比例有关，因此当你的数据集不平衡，你既不偏好精确率也不偏好召回率时，它是一个很好的指标选择。'
- en: '**MCC**: The **Matthews correlation coefficient** is a metric drawn from biostatistics.
    It’s gaining popularity in the broader data science community because it has the
    ability to produce high scores considering *TP*, *FN*, *TN*, and *FP* fairly,
    because it takes into account the proportions of classes. This makes it optimal
    for imbalanced classification tasks. Unlike all other metrics used so far, it
    doesn’t range from 0 to 1 but from -1, complete disagreement, to 1, a total agreement
    between predictions and actuals. The mid-point, 0, is equivalent to a random prediction:'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MCC**: **马修斯相关系数**是从生物统计学中提取的一个指标。它在更广泛的数据科学社区中越来越受欢迎，因为它能够公平地考虑*TP*、*FN*、*TN*和*FP*，因为它考虑了类别的比例。这使得它非常适合不平衡的分类任务。与迄今为止使用的所有其他指标不同，它的范围不是从0到1，而是从-1（完全不一致）到1（预测与实际完全一致）。中间点，0，相当于随机预测：'
- en: '![](img/B18406_03_006.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_006.png)'
- en: Our classification metrics are mostly very good, exceeding 96% accuracy and
    75% recall. However, even recall isn’t everything. For instance, `RandomForest`,
    due to its class balancing with weights, got the highest recall but did poorly
    in F1 and MCC, which suggests that precision is not very good.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的分类指标大多非常好，准确率超过96%，召回率超过75%。然而，召回率并不是一切。例如，`RandomForest`由于使用了带权重的类别平衡，召回率最高，但在F1和MCC上表现不佳，这表明精确度不是很好。
- en: Ridge classification also had the same setting and had such a poor F1 score
    that the precision must have been dismal. This doesn’t mean this weighting technique
    is inherently wrong, but it often requires more control. This book will cover
    techniques to achieve the right balance between fairness and accuracy, accuracy
    and reliability, reliability and validity, and so on. This is a balancing act
    that requires many metrics and visualizations. A key takeaway from this exercise
    should be that a **single metric will not tell you the whole story**, and interpretation
    is about **telling the most relevant and sufficiently complete story**.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 岭分类也有相同的设置，并且F1分数非常低，以至于精确度肯定很糟糕。这并不意味着这种加权技术本质上就是错误的，但它通常需要更多的控制。这本书将涵盖实现公平性和准确性、准确性可靠性、可靠性有效性之间正确平衡的技术。这是一个需要许多指标和可视化的平衡行为。从这个练习中，我们应该得到的一个关键教训是**单一指标并不能告诉你全部的故事**，而解释就是**讲述最相关且足够完整的故事**。
- en: Understanding limitations of traditional model interpretation methods
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解传统模型解释方法的局限性
- en: 'In a nutshell, traditional interpretation methods *only cover high-level questions
    about your models* such as the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，传统的解释方法**仅涵盖关于您模型的高级问题**，例如以下内容：
- en: In aggregate, do they perform well?
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总体而言，它们的性能如何？
- en: '*What* changes in hyperparameters may impact predictive performance?'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪些超参数的变化可能会影响预测性能？
- en: '*What* latent patterns can you find between the features and their predictive
    performance?'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以在特征和它们的预测性能之间找到哪些**潜在模式**？
- en: These questions are very limiting if you are trying to understand not only whether
    your model works but *why* and *how*?
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您试图了解的不仅是模型是否工作，而是它**为什么**和**如何**工作，这些问题就非常有限了？
- en: This gap in understanding can lead to unexpected issues with your model that
    won’t necessarily be immediately apparent. Let’s consider that models, once deployed,
    are not static but dynamic. They face different challenges than they did in the
    “lab” when you were training them. They may face not only performance issues but
    issues with bias, such as imbalance with underrepresented classes, or security
    vulnerabilities with adversarial attacks. Realizing that the features have changed
    in the real-world environment, we might have to add new features instead of merely
    retraining with the same feature set. And if there are some troubling assumptions
    made by your model, you might have to re-examine the whole pipeline. But how do
    you recognize that these problems exist in the first place? That’s when you will
    need a whole new set of interpretation tools that can help you dig deeper and
    answer more specific questions about your model. These tools provide interpretations
    that can truly account for **Fairness, Accountability, and Transparency** (**FAT**),
    which we discussed in *Chapter 1*, *Interpretation, Interpretability, and Explainability;
    and Why Does It All Matter?*
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这种理解上的差距可能导致模型出现意想不到的问题，这些问题可能不会立即显现。让我们考虑一下，一旦部署，模型不是静态的，而是动态的。它们面临与您在训练它们时的“实验室”中不同的挑战。它们可能不仅面临性能问题，还可能面临偏差问题，例如代表性不足的类别的失衡，或者对抗攻击的安全漏洞。意识到特征在现实世界环境中的变化，我们可能需要添加新特征，而不仅仅是使用相同的特征集重新训练。如果您的模型做出了某些令人不安的假设，您可能需要重新审查整个流程。但您如何识别这些问题最初存在呢？这就是您需要一套全新的解释工具的时候了，这些工具可以帮助您深入了解并回答关于您模型更具体的问题。这些工具提供的解释可以真正考虑到**公平性、责任和透明度**（**FAT**），这是我们讨论过的*第一章*，*解释、可解释性和可解释性；以及为什么这一切都很重要？*
- en: Studying intrinsically interpretable (white-box) models
  id: totrans-171
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 研究本质上可解释（白盒）模型
- en: So far, in this chapter, we have already fitted our training data to model classes
    representing each of these “white-box” model families. The purpose of this section
    is to show you exactly why they are *intrinsically interpretable*. We’ll do so
    by employing the models that were previously fitted.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经将我们的训练数据拟合到代表每个这些“白盒”模型家族的模型类别。本节的目的就是向您展示它们为什么是**本质上可解释的**。我们将通过使用之前拟合的模型来实现这一点。
- en: Generalized Linear Models (GLMs)
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型（GLMs）
- en: 'GLMs are a large family of model classes that have a model for every statistical
    distribution. Just like **linear regression** assumes your target feature and
    residuals have a normal distribution, **logistic regression** assumes the Bernoulli
    distribution. There are GLMs for every distribution, such as **Poisson regression**
    for Poisson distribution and **multinomial response** for multinomial distribution.
    You choose which GLM to use based on the distribution of your target variable
    and whether your data meets the other assumptions of the GLM (they vary). In addition
    to an underlying distribution, what ties GLMs together into a single family is
    the fact that they all have a linear predictor. In other words, the ![](img/B18406_03_007.png)
    target variable (or predictor) can be expressed mathematically as a weighted sum
    of *X* features, where weights are called *b* coefficients. This is the simple
    formula, the linear predictor function, that all GLMs share:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: GLMs（广义线性模型）是一个包含各种模型类的大家族，每个统计分布都有一个对应的模型。就像**线性回归**假设目标特征和残差具有正态分布一样，**逻辑回归**假设伯努利分布。对于每个分布都有相应的GLM，例如**泊松回归**用于泊松分布和**多项式响应**用于多项式分布。您可以根据目标变量的分布以及数据是否满足GLM的其他假设（这些假设各不相同）来选择使用哪种GLM。除了基础分布之外，将GLMs联系在一起成为一个单一家族的事实是它们都有一个线性预测器。换句话说，目标变量（或预测器）可以用数学方式表示为*X*特征的加权总和，其中权重被称为*b*系数。这是所有GLM共有的简单公式，即线性预测函数：
- en: '![](img/B18406_03_008.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_008.png)'
- en: However, although they share this same formula, they each have a different link
    function, which provides a link between the linear predictor function and the
    mean of the statistical distribution of the GLM. This can add some non-linearity
    to the resulting model formula while retaining the linear combination between
    the *b* coefficients and the *X* input data, which can be a source of confusion.
    Still, it’s linear because of the linear combination.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管它们有相同的公式，但每个都有不同的链接函数，它提供了线性预测函数和GLM的统计分布的均值之间的联系。这可以在保留*b*系数和*X*输入数据之间的线性组合的同时，给结果模型公式添加一些非线性，这可能会引起混淆。尽管如此，由于线性组合，它仍然是线性的。
- en: There are also many variations for specific GLMs. For instance, **Polynomial
    regression** is *linear regression* with polynomials of its features, and **ridge
    regression** is *linear regression* with L2 regularization. We won’t cover all
    GLMs in this section because they aren’t needed for the example in this chapter,
    but all have plausible use cases.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特定的GLM也有许多变体。例如，**多项式回归**是其特征的多项式形式的*线性回归*，而**岭回归**是带有L2正则化的*线性回归*。在本节的例子中，我们不会涵盖所有GLM，因为它们对于本章的例子不是必需的，但它们都有合理的应用场景。
- en: Incidentally, there’s also a similar concept called **Generalized Additive Models**
    (**GAMs**), which are GLMs that don’t require linear combinations of features
    and coefficients and instead retain the addition part, but of arbitrary functions
    applied to the features. GAMs are also interpretable, but they are not as common,
    and are usually tailored to specific use cases *ad hoc*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 偶然的是，还有一个类似的概念叫做**广义加性模型**（**GAMs**），这些是GLM，它们不需要特征和系数的线性组合，而是保留了加法部分，但应用于特征的任意函数。GAMs也是可解释的，但它们并不常见，通常是为特定用例专门定制的*ad
    hoc*。
- en: Linear regression
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性回归
- en: 'In *Chapter 1*, *Interpretation, Interpretability, and Explainability, and
    Why Does It All Matter?*, we covered the formula of simple linear regression,
    which only has a single *X* feature. Multiple linear regression extends this to
    have any number of features, so instead of being:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*解释、可解释性和可解释性，以及为什么这一切都很重要？*中，我们介绍了简单线性回归的公式，它只有一个*X*特征。多元线性回归扩展到任何数量的特征，所以不再是：
- en: '![](img/B18406_03_009.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_009.png)'
- en: 'it can be:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 它可以是：
- en: '![](img/B18406_03_010.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_010.png)'
- en: 'with *n* features, and where ![](img/B18406_03_011.png) is the intercept, and
    thanks to linear algebra this can be a simple matrix multiplication:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 有*n*个特征，其中![](img/B18406_03_011.png)是截距，多亏了线性代数，这可以是一个简单的矩阵乘法：
- en: '![](img/B18406_03_008.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_008.png)'
- en: 'The method used to arrive at the optimal *b* coefficients, **OLS**, is well-studied
    and understood. Also, in addition to the coefficients, you can extract confidence
    intervals for each. The model’s correctness depends on whether the input data
    meets the assumptions: **linearity**, normality, independence, a lack of multicollinearity,
    and homoscedasticity. We’ve discussed linearity, so far, quite a bit so we will
    briefly explain the rest:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 用于得到最优*b*系数的方法，**OLS**，已经被深入研究并理解。此外，除了系数之外，你还可以为每个系数提取置信区间。模型的正确性取决于输入数据是否满足假设：**线性**、**正态性**、**独立性**、**多重共线性**和**同方差性**。我们已经讨论了线性，所以到目前为止，我们将简要解释其余部分：
- en: '**Normality** is the property that each feature is normally distributed. This
    can be tested with a **Q-Q plot**, histogram, or **Kolmogorov-Smirnov** test,
    and non-normality can be corrected with non-linear transformations. If a feature
    isn’t normally distributed, it will make its coefficient confidence intervals
    invalid.'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正态性**是每个特征都是正态分布的性质。这可以通过**Q-Q图**、直方图或**Kolmogorov-Smirnov**测试来测试，非正态性可以通过非线性变换来纠正。如果一个特征不是正态分布的，它将使其系数置信区间无效。'
- en: '**Independence** is when your *observations* (the rows in your dataset) are
    independent of each other, like different and unrelated events. If your *observations*
    aren’t independent, it could affect your interpretation of the results. In this
    chapter’s example, if you had multiple rows about the same flight, that could
    violate this assumption and make results hard to understand. This can be tested
    by looking for duplicate flight numbers.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**独立性**是指你的*观测值*（数据集中的行）彼此独立，就像不同且无关的事件一样。如果你的*观测值*不独立，可能会影响你对结果的理解。在本章的例子中，如果你有关于同一航班的多个行，可能会违反这个假设，使结果难以理解。这可以通过查找重复的航班号来测试。'
- en: Multicollinearity occurs when the features are highly correlated with each other.
    **Lack of multicollinearity** is desirable because otherwise, you’d have inaccurate
    coefficients. This can be tested with a **correlation matrix**, **tolerance measure**,
    or **Variance Inflation Factor** (**VIF**), and it can be fixed by removing one
    of each highly correlated feature.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当特征之间高度相关时，会发生多重共线性。**缺乏多重共线性**是可取的，因为否则你会得到不准确的系数。这可以通过**相关矩阵**、**容忍度度量**或**方差膨胀因子**（**VIF**）来测试，并且可以通过删除每个高度相关特征中的一个来纠正。
- en: '**Homoscedasticity** was briefly discussed in *Chapter 1*, *Interpretation,
    Interpretability, and Explainability; and Why Does It All Matter?* and it’s when
    the residuals (the errors) are more or less equal across the regression line.
    This can be tested with the **Goldfeld–Quandt test**, and heteroscedasticity (the
    lack of homoscedasticity) can be corrected with non-linear transformations. This
    assumption is often violated in practice.'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**同方差性**在*第一章*，*解释、可解释性和可解释性；以及这一切为什么都重要？*中简要讨论过，这是指残差（误差）在回归线上大致相等。这可以通过**Goldfeld-Quandt检验**来测试，而异方差性（同方差性的缺乏）可以通过非线性变换来纠正。在实践中，这个假设通常被违反。'
- en: 'Even though we haven’t done it for this chapter’s example, if you are going
    to rely on linear regression heavily, it’s always good to test these assumptions
    before you even begin to fit your data to a linear regression model. This book
    won’t detail how this is done because it’s more about model-agnostic and deep-learning
    interpretation methods than delving into how to meet the assumptions of a specific
    class of models, such as **normality** and **homoscedasticity**. However, we covered
    the characteristics that trump interpretation the most in *Chapter 2,* *Key Concepts
    of Interpretability*, and we will continue to look for these characteristics:
    **non-linearity**, **non-monotonicity**, and **interactivity**. We will do this
    mainly because the linearity and correlation of and between features are still
    relevant, regardless of the modeling class used to make predictions. And these
    are characteristics that can be easily tested in the methods used for linear regression.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在这个章节的示例中没有这样做，但如果你将大量依赖线性回归，那么在开始将数据拟合到线性回归模型之前测试这些假设总是好的。本书不会详细介绍如何这样做，因为它更多地关注模型无关和深度学习解释方法，而不是深入探讨如何满足特定模型类（如**正态性**和**同方差性**）的假设。然而，我们在*第二章*，*可解释性的关键概念*中涵盖了最影响解释的特征，我们将继续寻找这些特征：**非线性**、**非单调性**和**交互性**。我们将这样做主要是因为无论使用哪种建模类进行预测，特征之间的线性和相关性仍然是相关的。而且，这些特征可以通过线性回归中使用的方法轻松测试。
- en: Interpretation
  id: totrans-192
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释
- en: 'So how do we interpret a linear regression model? Easy! Just get the coefficients
    and the intercept. Our scikit-learn models have these attributes embedded in the
    fitted model:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何解释线性回归模型呢？很简单！只需获取系数和截距。我们的scikit-learn模型具有这些属性，它们嵌入在拟合的模型中：
- en: '[PRE22]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code outputs the following:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了以下内容：
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'So now you know the formula, which looks something like this:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了公式，看起来可能像这样：
- en: '![](img/B18406_03_013.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_013.png)'
- en: 'This formula should provide some intuition on how the model can be interpreted
    globally. Interpreting each coefficient in the model can be done for multiple
    linear regression, just as we did with the simple linear regression example in
    *Chapter 1*, *Interpretation, Interpretability, and Explainability; and Why Does
    It All Matter?*. The coefficients act as weights, but they also tell a story that
    varies depending on the kind of feature. To make interpretation more manageable,
    let’s put our coefficients in a `DataFrame` alongside the names of each feature:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式应该可以提供一些关于模型如何全局解释的直觉。在多元线性回归中，解释模型中的每个系数可以像我们在*第一章*，*解释、可解释性和可解释性；以及这一切为什么都重要？*中的简单线性回归示例中所做的那样进行。系数充当权重，但它们也根据特征类型讲述不同的故事。为了使解释更易于管理，让我们将我们的系数放在一个`DataFrame`中，并附带每个特征的名称：
- en: '[PRE24]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The preceding code produces the DataFrame in *Figure 3.5*:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图3.5*中的DataFrame：
- en: '![Table  Description automatically generated](img/B18406_03_05.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_05.png)'
- en: 'Figure 3.5: Coefficients of linear regression features'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5：线性回归特征系数
- en: 'Here’s how to interpret a feature using the coefficients in *Figure 3.5*:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 这是如何使用*图3.5*中的系数来解释一个特征的示例：
- en: '**Continuous**: Like `ARR_RFPH`, you know that for every one-unit increase
    (relative flights per hour), it increases the predicted delay by 0.373844 minutes,
    if all other features stay the same.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续型**: 就像 `ARR_RFPH` 一样，你知道对于每增加一个单位（相对每小时航班数），如果其他所有特征保持不变，它将使预测延误增加0.373844分钟。'
- en: '**Binary**: Like `ORIGIN_HUB`, you know the difference between the origin airport
    being a hub or not is expressed by the coefficient -1.029088\. In other words,
    since it’s a negative number, the origin airport is a hub. It reduces the delay
    by just over 1 minute if all other features stay the same.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**二元型**: 就像 `ORIGIN_HUB` 一样，你知道出发机场是否是枢纽通过系数 -1.029088 来表达。换句话说，由于它是一个负数，出发机场是枢纽。如果其他所有特征保持不变，它将减少超过1分钟的延误。'
- en: '**Categorical**: We don’t have categorical features, but we have ordinal features
    that could have been, and **actually should have been**, categorical features.
    For instance, `DEP_MONTH` and `DEP_DOW` are integers from 1–12 and 0–6, respectively.
    If they are treated as ordinals, we are assuming because of the linear nature
    of linear regression that an increase or decrease in months has an impact on the
    outcome. It’s the same with the day of the week. But the impact is tiny. Had we
    treated them as dummy or one-hot encoded features, we could measure whether Fridays
    are more prone to carrier delays than Saturdays and Wednesdays, or Julys more
    than Octobers and Junes. This couldn’t possibly be modeled with them in order,
    because they have no relation to this order (yep – it’s non-linear!). So, say
    we had a feature called `DEP_FRIDAY` and another called `DEP_JULY`. They are treated
    like binary features and can tell you precisely what effect a departure being
    on a Friday or in July has on the model. Some features were kept as ordinal or
    continuous on purpose, despite being good candidates for being categorical, to
    demonstrate how not making the right adjustments to your features can impact the
    **expressive power** of model interpretation. It would have been good to tell
    airline executives more about how the day and time of a departure impacted delays.
    Also, in some cases – not in this one – an oversight like this can grossly affect
    a linear regression model’s performance.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**类别型**: 我们没有类别特征，但我们有可能是类别特征的有序特征。实际上，它们应该是类别特征。例如，`DEP_MONTH` 和 `DEP_DOW`
    分别是 1–12 和 0–6 的整数。如果将它们视为有序特征，我们假设由于线性回归的线性性质，月份的增加或减少会对结果产生影响。对一周中的某一天来说也是如此。但影响非常小。如果我们将它们视为虚拟变量或独热编码的特征，我们就可以测量周五是否比周六和周三更容易出现承运人延误，或者七月是否比十月和六月更容易出现延误。这些特征按顺序排列是无法进行建模的，因为它们与这种顺序没有关系（是的——它是非线性的！）。所以，假设我们有一个名为
    `DEP_FRIDAY` 的特征和另一个名为 `DEP_JULY` 的特征。它们被视为二元特征，可以精确地告诉你周五或七月出发对模型的影响。一些特征被有意保留为有序或连续的，尽管它们是类别特征的合适候选，以展示如果不正确调整特征，可能会影响模型解释的**表达能力**。本可以更好地告诉航空公司关于出发日和时刻如何影响延误的信息。此外，在某些情况下——不是在这个例子中——这种疏忽可能会极大地影响线性回归模型的表现。'
- en: The intercept (-37.86) is not a feature, but it does have a meaning, which is,
    if all features were at 0, what would the prediction be? In practice, this doesn’t
    happen unless your features happen to all have a plausible reason to be 0\. Just
    as in *Chapter 1*, *Interpretation, Interpretability, and Explainability; and
    Why Does It All Matter?* you wouldn’t have expected anyone to have a height of
    0, in this example, you wouldn’t expect a flight to have a distance of 0\. However,
    if you standardized the features so that they had a mean of 0, then you would
    change the interpretation of the intercept to be the prediction you expect if
    all features are their mean value.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 截距（-37.86）不是一个特征，但它确实有含义，即如果所有特征都是0，预测会是什么？在实践中，除非你的特征都有合理的理由是0，否则这种情况不会发生。就像在
    *第一章*，*解释、可解释性和可解释性；以及为什么这一切都很重要？* 中你不会期望任何人的身高是0一样，在这个例子中，你不会期望航班距离是0。然而，如果你将特征标准化，使它们的平均值是0，那么你会改变截距的解释，使其成为所有特征都是其平均值时的预测值。
- en: Feature importance
  id: totrans-209
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'The coefficients can also be leveraged to calculate feature importance. Unfortunately,
    scikit-learn’s linear regressor is ill-equipped to do this because it doesn’t
    output the standard error of the coefficients. According to their importance,
    all it takes to rank features is to divide the ![](img/B18406_03_014.png)s by
    their corresponding standard errors. This result is something called the **t-statistic**:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 系数也可以用来计算特征重要性。不幸的是，scikit-learn的线性回归器不适合做这件事，因为它不输出系数的标准误差。根据它们的重要性，只需要将![](img/B18406_03_014.png)除以它们对应的标准误差来对特征进行排序。这个结果被称为**t统计量**：
- en: '![](img/B18406_03_015.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_015.png)'
- en: 'And then you take an absolute value of this and sort them from high to low.
    It’s easy enough to calculate, but you need the standard error. You could reverse-engineer
    the linear algebra involved to retrieve it using the intercept, and the coefficients
    returned by scikit-learn. However, it’s probably a lot easier to fit the linear
    regression model again, but this time using the `statsmodels` library, which has
    a summary with all the statistics included! By the way, `statsmodels` names its
    linear regressor `OLS`, which makes sense because `OLS` is the name of the mathematical
    method that fits the data:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你取这个值的绝对值，并按从高到低的顺序排序。这很容易计算，但你需要标准误差。你可以通过逆向工程涉及的线性代数来使用截距和scikit-learn返回的系数来检索它。然而，可能更容易再次拟合线性回归模型，但这次使用包含所有统计信息的`statsmodels`库！顺便说一句，`statsmodels`将其线性回归器命名为`OLS`，这是有道理的，因为`OLS`是拟合数据的数学方法名称：
- en: '[PRE25]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: There’s quite a bit to unpack in the regression summary. This book won’t address
    everything except that the t-statistic can tell you how important features are
    in relation to each other. There’s another more pertinent statistical interpretation,
    which is that if you were to hypothesize that the *b* coefficient is 0 – in other
    words, that the feature has no impact on the model – the distance of the t-statistic
    from 0 helps reject that null hypothesis. This is what the **p-value** to the
    right of the t-statistic does. It’s no coincidence that the closest *t* to 0 (for
    `ARR_AFPH`) has the only p-value above 0.05\. This puts this feature at a level
    of insignificance since everything below 0.05 is statistically significant according
    to this method of hypothesis testing.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在回归总结中有很多内容需要解析。本书不会涵盖所有内容，除了t统计量可以告诉你特征之间的重要性。还有一个更相关的统计解释，即如果你假设*b*系数为0——换句话说，即特征对模型没有影响——t统计量与0的距离有助于拒绝这个零假设。这就是t统计量右侧的**p值**所做的事情。最接近0的*t*（对于`ARR_AFPH`）只有一个p值大于0.05并不是巧合。这表明这个特征在统计上不显著，因为根据这种方法进行假设检验，所有低于0.05的都是统计显著的。
- en: 'So to rank our features, let’s extract the DataFrame from the `statsmodels`
    summary. Then, we drop the `const` (the intercept) because this is not a feature.
    Then, we make a new column with the absolute value of the t-statistic and sort
    it accordingly. To demonstrate how the absolute value of the t-statistic and p-value
    are inversely related, we are also color-coding these columns:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，为了对特征进行排序，让我们从`statsmodels`总结中提取DataFrame。然后，我们删除`const`（截距），因为这不是特征。然后，我们创建一个新列，包含t统计量的绝对值，并相应地排序。为了展示t统计量的绝对值和p值是反向相关的，我们还对这些列进行了着色：
- en: '[PRE26]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding code outputs *Figure 3.6*:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码输出了*图3.6*：
- en: '![Table, Excel  Description automatically generated](img/B18406_03_06.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![表格，Excel 描述自动生成](img/B18406_03_06.png)'
- en: 'Figure 3.6: Linear regression summary table sorted by the absolute value of
    the t-statistic'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6：按t统计量的绝对值排序的线性回归总结表
- en: 'Something particularly interesting about the feature importance in *Figure
    3.6* is that different kinds of delays occupy five out of the top six positions.
    Of course, this could be because linear regression is confounding the different
    non-linear effects these have, or perhaps there’s something here we should look
    further into – especially since the `statsmodels` summary in the “**Warnings**”
    section cautions:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.6*中关于特征重要性的一个特别有趣的现象是，不同类型的延迟占据了前六位中的五位。当然，这可能是因为线性回归混淆了这些不同的非线性效应，或者可能这里有一些我们应该进一步研究的东西——特别是由于“**警告**”部分中的`statsmodels`总结警告说：
- en: '[PRE27]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: This is odd. Hold that thought. We will examine this further later.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这很奇怪。记住这个想法。我们稍后会进一步探讨。
- en: Ridge regression
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 岭回归
- en: Ridge regression is part of a sub-family of **penalized** or **regularized**
    regression along with the likes of LASSO and ElasticNet because, as explained
    earlier in this chapter, it penalizes using the *L2 norm*. This sub-family is
    also called **sparse linear models** because, thanks to the regularization, it
    cuts out some of the noise by making irrelevant features less relevant. **Sparsity**
    in this context means less is more because reduced complexity will lead to lower
    variance and improved generalization.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归是**惩罚**或**正则化**回归的一个子族，与LASSO和ElasticNet等类似，因为，正如本章前面所解释的，它惩罚使用**L2范数**。这个子族也被称为**稀疏线性模型**，因为，多亏了正则化，它通过使不相关的特征变得不那么相关来消除一些噪声。在这个上下文中，“稀疏”意味着少即是多，因为降低复杂性将导致方差降低并提高泛化能力。
- en: To illustrate this concept, look at the feature importance table (*Figure 3.6*)
    we output for linear regression. Something that should be immediately apparent
    is how the `t_abs` column starts with every row a different color, and then a
    whole bunch of them are the same shade of yellow. Because of the variation in
    confidence intervals, the absolute t-value is not something you can take proportionally
    and say that your top feature is hundreds of times more relevant than every one
    of your bottom 10 features. However, it should indicate that there are significantly
    more important features than others to the point of irrelevance, and possibly
    confoundment, hence creating noise. There’s ample research on how there’s a tendency
    for a small subset of features to have the most substantial effects on the outcome
    of the model. This is called the **bet on sparsity principle**. Whether it’s true
    or not for your data, it’s always good to test the theory by applying regularization,
    especially in cases where data is very wide (many features) or exhibits multicollinearity.
    These regularized regression techniques can be incorporated into feature selection
    processes or to inform your understanding of what features are essential.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，看看我们为线性回归输出的特征重要性表(*图3.6*)。应该立即明显的是，`t_abs`列的每一行都以不同的颜色开始，然后一大堆都是相同的黄色。由于置信区间的变化，绝对t值不是你可以按比例取的，说你最顶部的特征比你的底部10个特征中的每一个都要相关数百倍。然而，它应该表明，有一些特征比其他特征显著更重要，以至于到了无关紧要的程度，甚至可能造成混淆，从而产生噪声。关于一小部分特征对模型结果有最大影响的倾向，有大量研究。这被称为**稀疏性原则的赌注**。无论对于你的数据是否真实，通过应用正则化来测试这个理论总是好的，尤其是在数据非常宽（许多特征）或表现出多重共线性时。这些正则化回归技术可以纳入特征选择过程，或用来了解哪些特征是必不可少的。
- en: There is a technique to adapt ridge regression to classification problems. It
    was briefly discussed before. It converts the labels to a -1 to 1 scale for training
    to predict values between -1 and 1, and then turns them back to a 0–1 scale. However,
    it uses regularized linear regression to fit the data and can be interpreted in
    the same way.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种技术可以将岭回归应用于分类问题。这之前已经简要讨论过。它将标签转换为-1到1的尺度进行训练，以预测-1和1之间的值，然后将其转换回0-1尺度。然而，它使用正则化线性回归来拟合数据，可以以相同的方式进行解释。
- en: Interpretation
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释
- en: 'Ridge regression can be interpreted in the same way as linear regression, both
    globally and locally, because once the model has been fitted, there’s no difference.
    The formula is the same:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 岭回归可以像线性回归一样进行解释，无论是全局还是局部，因为一旦模型被拟合，就没有区别。公式是相同的：
- en: '![](img/B18406_03_016.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_016.png)'
- en: Except ![](img/B18406_03_017.png) coefficients are different because they were
    penalized with a ![](img/B18406_03_018.png) parameter, which controls how much
    shrinkage to apply.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 除了![](img/B18406_03_017.png)系数不同，因为它们被一个![](img/B18406_03_018.png)参数惩罚，该参数控制应用收缩的程度。
- en: 'We can quickly compare coefficients by extracting the ridge coefficients from
    their fitted model and placing them side by side in a `DataFrame` with the coefficients
    of the linear regression:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过从拟合模型中提取岭回归系数并将它们并排放置在一个`DataFrame`中，与线性回归的系数并排放置来快速比较系数：
- en: '[PRE28]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As you can tell in the *Figure 3.7* output of the preceding code, the coefficients
    are always slightly different, but sometimes they are lower and sometimes higher:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从前面代码的*图3.7*输出中可以看出，系数总是略有不同，但有时它们较低，有时较高：
- en: '![Table  Description automatically generated](img/B18406_03_07.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_07.png)'
- en: 'Figure 3.7: Linear regression coefficients compared to ridge regression coefficients'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：线性回归系数与岭回归系数的比较
- en: 'We didn’t save the ![](img/B18406_03_018.png) parameter (which scikit-learn
    calls *alpha*) that the ridge regression cross-validation deemed optimal. However,
    we can run a little experiment of our own to figure out which parameter was the
    best. We do this by iterating through 100 possible alphas values between 100 (1)
    and 1013 (100,000,000,000,000), fitting the data to the ridge model, and then
    appending the coefficients to an array. We exclude the eight coefficient in the
    array because it’s so much larger than the rest, and it will make it harder to
    visualize the effects of shrinkage:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有保存岭回归交叉验证认为最优的![](img/B18406_03_018.png)参数（scikit-learn称为*alpha*）。然而，我们可以进行一个小实验来找出哪个参数是最好的。我们通过在100（1）和10^13（100,000,000,000,000）之间迭代100个可能的alpha值，将数据拟合到岭回归模型，然后将系数追加到一个数组中。我们排除了数组中的第八个系数，因为它比其他系数大得多，这将使可视化收缩效应更困难：
- en: '[PRE29]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now that we have an array of coefficients, we can plot the progression of coefficients:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了系数数组，我们可以绘制系数的进展图：
- en: '[PRE30]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The preceding code generates *Figure 3.8*:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图3.8*：
- en: '![Chart, line chart  Description automatically generated](img/B18406_03_08.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_03_08.png)'
- en: 'Figure 3.8: Value of alpha hyperparameters versus the value of ridge regression
    coefficients'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：alpha超参数值与岭回归系数值的关系
- en: Something to note in *Figure 3.8* is that the higher the alpha, the higher the
    regularization. This is why when alpha is 1012, all coefficients have converged
    to 0, and as the alpha becomes smaller, they get to a point where they have all
    diverged and more or less stabilized. In this case, this point is reached at about
    102\. Another way of seeing it is when all coefficients are around 0, it means
    that the regularization is so strong that all features are irrelevant. When they
    have sufficiently diverged and stabilized, the regularization makes them all relevant,
    which defeats the purpose.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.8*中需要注意的一点是，alpha值越高，正则化程度越高。这就是为什么当alpha为10^12时，所有系数都收敛到0，而当alpha变小时，它们达到一个点，所有系数都发散并大致稳定。在这种情况下，这个点大约在10^2时达到。另一种看待它的方法是，当所有系数都接近0时，这意味着正则化非常强，以至于所有特征都无关紧要。当它们足够发散并稳定后，正则化使它们都变得相关，这违背了目的。
- en: 'Now on that note, if we go back to our code, we will find that this is what
    we chose for alphas in our `RidgeCV`: `alphas=[1e-3, 1e-2, 1e-1, 1]`. As you can
    tell from the preceding plot, by the time the alphas have reached `1` and below,
    the coefficients have already stabilized even though they are still fluctuating
    slightly. This can explain why our ridge was not better performing than linear
    regression. Usually, you would expect a regularized model to perform better than
    one that isn’t – unless your hyperparameters are not right.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到我们的代码，我们会发现这是我们为`RidgeCV`中的alpha选择的：`alphas=[1e-3, 1e-2, 1e-1, 1]`。正如你可以从前面的图中看到的，当alpha达到1及以下时，系数已经稳定，尽管它们仍在轻微波动。这可以解释为什么我们的岭回归没有比线性回归表现更好。通常，你会期望正则化模型的表现比未正则化的模型好——除非你的超参数设置不正确。
- en: Interpretation and hyperparameters
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 解释和超参数
- en: 'Well-tuned regularization can help cut out the noise and thus increase interpretability,
    but the alphas chosen for `RidgeCV` were selected on purpose to be able to convey
    this point: *regularization can only work if you chose hyperparameters correctly*,
    or, when regularization hyperparameter tuning is automatic, the method must be
    optimal for your dataset.'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 调整得当的正则化可以帮助去除噪声，从而提高可解释性，但为`RidgeCV`选择的alpha是故意选定的，以便传达这个观点：*正则化只有在正确选择超参数的情况下才能起作用*，或者，当正则化超参数调整是自动进行时，该方法必须针对你的数据集是最优的。
- en: Feature importance
  id: totrans-247
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征重要性
- en: This is precisely the same as with linear regression, but again we need the
    standard error of the coefficients, which is something that cannot be extracted
    from the scikit-learn model. You can use the `statsmodels fit_regularized` method
    to this effect.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这与线性回归完全相同，但我们需要系数的标准误差，这是无法从scikit-learn模型中提取出来的。你可以使用`statsmodels fit_regularized`方法来实现这一点。
- en: Polynomial regression
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多项式回归
- en: Polynomial regression is a special case of linear or logistic regression where
    the features have been expanded to have higher degree terms. We have only performed
    polynomial linear regression in this chapter’s exercise, so we will only discuss
    this variation. However, it is applied similarly.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归是线性或逻辑回归的一种特殊情况，其中特征被扩展为具有更高次项。我们只在本章的练习中进行了多项式线性回归，因此我们只会讨论这种变化。然而，它的应用方式是相似的。
- en: 'A two-feature multiple linear regression would look like this:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 一个双特征多重线性回归看起来是这样的：
- en: '![](img/B18406_03_020.png)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_020.png)'
- en: 'However, in polynomial regression, every feature is expanded to have higher
    degree terms and interactions between all the features. So, if this two-feature
    example was expanded to a second-degree polynomial, the linear regression formula
    would look like this:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在多项式回归中，每个特征都被扩展为具有更高次项，并且所有特征之间都有相互作用。因此，如果这个双特征示例扩展为二次多项式，线性回归公式将看起来像这样：
- en: '![](img/B18406_03_021.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_021.png)'
- en: 'It’s still linear regression in every way except it has extra features, higher-degree
    terms, and interactions. While you can limit polynomial expansion to only one
    or a few features, we used `PolynomialFeatures`, which does this to all features.
    Therefore, 21 features were likely multiplied many times over. We can extract
    the coefficients from our fitted model and, using the `shape` property of the
    `numpy` array, return how many coefficients were generated. This amount corresponds
    to the number of features generated:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 它在各个方面仍然是线性回归，除了它有额外的特征、更高次项和相互作用。虽然你可以将多项式扩展限制在只有一个或几个特征上，但我们使用了`PolynomialFeatures`，它对所有特征都这样做。因此，21个特征可能被多次相乘。我们可以从我们的拟合模型中提取系数，并使用`numpy`数组的`shape`属性来返回生成的系数数量。这个数量对应于生成的特征数量：
- en: '[PRE31]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'It outputs `253`. We can do the same with the version of polynomial regression,
    which was with interaction terms only:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 它输出`253`。我们可以用多项式回归的版本做同样的事情，这个版本只有交互项：
- en: '[PRE32]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The above code outputs `232`. The reality is that most terms in a polynomial
    generated like this are interactions between all the features.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出`232`。实际上，这种生成的多项式中的大多数项都是所有特征之间的相互作用。
- en: Interpretation and feature importance
  id: totrans-260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释和特征重要性
- en: Polynomial regression can be interpreted, both globally and locally, in precisely
    the same way as linear regression. In this case, it’s not practical to understand
    a formula with 253 linearly combined terms, so it loses what we defined in *Chapter
    2*, *Key Concepts of Interpretability*, as **global holistic interpretation**.
    However, it still can be interpreted in all other scopes and retains many of the
    properties of linear regression. For instance, since the model is additive, it
    is easy to separate the effects of the features. You can also use the same many
    peer-reviewed tried and tested statistical methods that are used for linear regression.
    For instance, you can use the t-statistic, p-value, confidence bounds, R-squared,
    as well as the many tests used to assess goodness of fit, residual analysis, linear
    correlation, and analysis of variance. This wealth of statistically proven methods
    to test and interpret models isn’t something most model classes can count on.
    Unfortunately, many of them are model-specific to linear regression and its special
    cases.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式回归可以像线性回归一样，在全局和局部进行精确的解释。在这种情况下，理解一个由253个线性组合项构成的公式并不实际，因此它失去了我们在*第二章*中定义的，即*可解释性的关键概念*中的*全局整体解释*。然而，它仍然可以在所有其他范围内进行解释，并保留线性回归的许多属性。例如，由于模型是加性的，因此很容易分离特征的影响。你还可以使用与线性回归相同的许多同行评审的经过验证的统计方法。例如，你可以使用t统计量、p值、置信区间、R平方，以及用于评估拟合优度、残差分析、线性相关性和方差分析的许多测试。这些丰富的经过统计验证的方法来测试和解释模型并不是大多数模型类别都能依赖的。不幸的是，其中许多都是针对线性回归及其特殊情况的特定模型。
- en: Also, we won’t do it here because there are so many terms. Still, you could
    undoubtedly rank features for polynomial regression in the same way we have for
    linear regression using the `statsmodels` library. The challenge is figuring out
    the order of the features generated by `PolynomialFeatures` to name them accordingly
    in the feature name column. Once this is done, you can tell if some second-degree
    terms or interactions are important. This could tell you if these features have
    a non-linear nature or highly depend on other features.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在这里不会这样做，因为有很多项。尽管如此，你无疑可以用与线性回归相同的方式使用 `statsmodels` 库对多项式回归进行特征排序。挑战在于确定由
    `PolynomialFeatures` 生成的特征的顺序，以便在特征名称列中相应地命名。一旦完成这项工作，你就可以判断某些二次项或交互项是否重要。这可能会告诉你这些特征是否具有非线性性质或高度依赖于其他特征。
- en: Logistic regression
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'We discussed logistic regression as well as its interpretation and feature
    importance in *Chapter 2*, *Key Concepts of Interpretability*. We will only expand
    on that a bit here in the context of this chapter’s classification exercise and
    to underpin why exactly it is interpretable. The fitted logistic regression model
    has coefficients and intercepts just as the linear regression model does:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 *第二章*，*可解释性的关键概念* 中讨论了逻辑回归及其解释和特征重要性。我们将在本章的分类练习的背景下对此进行一些扩展，并阐述为什么逻辑回归是可解释的。拟合的逻辑回归模型具有系数和截距，就像线性回归模型一样：
- en: '[PRE33]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code outputs this:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出如下：
- en: '[PRE34]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'However, the way these coefficients appear in the formula for a specific prediction
    ![](img/B18406_03_022.png)is entirely different:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些系数在特定预测公式中的出现方式与 ![](img/B18406_03_022.png) 完全不同：
- en: '![](img/B18406_03_023.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_023.png)'
- en: In other words, the probability that ![](img/B18406_03_024.png) (is a positive
    case) is expressed by a **logistic function** that involves exponentials of the
    linear combination of ![](img/B18406_03_014.png) coefficients and the *x* features.
    The presence of the exponentials explains why the coefficients extracted from
    the model are log odds because to isolate the coefficients, you should apply a
    logarithm to both sides of the equation.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，![](img/B18406_03_024.png)（是阳性案例）的概率由一个涉及 ![](img/B18406_03_014.png) 系数和
    *x* 特征的线性组合的指数的 **逻辑函数** 表示。指数的存在解释了为什么从模型中提取的系数是对数似然，因为为了隔离系数，你应该在方程式的两边应用对数。
- en: Interpretation
  id: totrans-271
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释
- en: To interpret each coefficient, you do it in precisely the same way as with linear
    regression, except with each unit increase in the features, you increase the odds
    of getting the positive case by a factor expressed by the exponential of the coefficient
    – all things being equal (remember the **ceteris paribus** assumption discussed
    in *Chapter 2*, *Key Concepts of Interpretability*). An exponential (![](img/B18406_03_026.png))
    has to be applied to each coefficient because they express an increase in log
    odds and not odds. Besides incorporating the log odds into the interpretation,
    the same that was said about continuous, binary, and categorical in linear regression
    interpretation applies to logistic regression.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 解释每个系数的方法与线性回归完全相同，只是当特征增加一个单位时，通过系数的指数因子增加获得阳性案例的概率（所有其他条件相同，记住在 *第二章* 中讨论的
    **ceteris paribus** 假设）。必须对每个系数应用指数 (![](img/B18406_03_026.png))，因为它们表示对数似然的增加，而不是概率。除了将对数似然纳入解释之外，关于线性回归中连续、二元和分类的解释同样适用于逻辑回归。
- en: Feature importance
  id: totrans-273
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'Frustrating as it is, there isn’t consensus yet from the statistical community
    on how to best get feature importance for logistic regression. There’s a standardize-all-features-first
    method, a pseudo R² method, a *one feature at a time* ROC AUC method, a partial
    chi-squared statistic method, and then the simplest one, which is multiplying
    the standard deviations of each feature times the coefficients. We won’t cover
    all these methods, but it has to be noted that computing feature importance consistently
    and reliably is a problem for most model classes, even white-box ones. We will
    dig deeper into this in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*.
    For logistic regression, perhaps the most popular method is achieved by standardizing
    all the features before training – that is, making sure they are centered at zero
    and divided by their standard deviation. But we didn’t do this because although
    it has other benefits, it makes the interpretation of coefficients more difficult,
    so here we are using the rather crude method leveraged in *Chapter 2*, *Key Concepts
    of Interpretability*, which is to multiply the standard deviations of each feature
    times the coefficients:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，统计界在如何最好地获取逻辑回归的特征重要性方面还没有达成共识。有标准化所有特征的方法，伪 R² 方法，一次一个特征的 ROC AUC 方法，部分卡方统计方法，然后最简单的一个，即乘以每个特征的方差和系数。我们不会涵盖所有这些方法，但必须指出，对于大多数模型类别，包括白盒模型，计算特征重要性的一致性和可靠性是一个问题。我们将在
    *第 4 章*，*全局模型无关解释方法* 中更深入地探讨这个问题。对于逻辑回归，可能最流行的方法是在训练之前标准化所有特征——也就是说，确保它们以零为中心，并除以它们的方差。但我们没有这样做，因为尽管它有其他好处，但它使得系数的解释更加困难，因此在这里我们使用的是
    *第 2 章*，*可解释性关键概念* 中提到的相当粗糙的方法，即乘以每个特征的方差和系数：
- en: '[PRE35]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The preceding code yields the following output:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码生成了以下输出：
- en: '[PRE36]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: It can still approximate the importance of features quite well. And just like
    with linear regression, you can tell that delay features are ranking quite high.
    All five of them are among the top eight features. Indeed, it’s something we should
    look into. We will discuss more on that as we discuss some other white-box methods.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 它仍然可以相当好地近似特征的重要性。就像线性回归一样，你可以看出延迟特征排名相当高。这五个特征都在前八个特征之中。事实上，这是我们应当关注的。当我们讨论其他白盒方法时，我们将对此进行更多讨论。
- en: Decision trees
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: Decision trees have been used for the longest time, even before they were turned
    into algorithms. They hardly require any mathematical abilities to understand
    them, and this low barrier to comprehensibility makes them extremely interpretable
    in their simplest representations. However, in practice, there are many types
    of decision tree learning, and most of them are not very interpretable because
    they use **ensemble methods** (boosting, bagging, and stacking), or even leverage
    PCA or some other embedder. Even non-ensembled decision trees can get extremely
    complicated as they become deeper. Regardless of the complexity of a decision
    tree, they can always be mined for important insights about your data and expected
    predictions, and they can be fitted to both regression and classification tasks.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树被使用了很长时间，甚至在它们被转化为算法之前。它们几乎不需要任何数学能力就能理解，这种低门槛的可理解性使得它们在最简单的表示中具有极高的可解释性。然而，在实践中，有许多类型的决策树学习方法，其中大多数都不是非常可解释的，因为它们使用了
    **集成方法**（提升、袋装和堆叠），或者甚至利用 PCA 或其他嵌入器。即使是非集成决策树，随着它们的深度增加，也可能变得极其复杂。无论决策树的复杂性如何，它们总能挖掘出关于你的数据和预期预测的重要见解，并且它们可以拟合回归和分类任务。
- en: CART decision trees
  id: totrans-281
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: CART 决策树
- en: The **Classification and Regression Trees** (**CART**) algorithm is the “vanilla”
    no-frills decision tree of choice in most use cases. And as noted, most decision
    trees aren’t white-box models, but this one is because it is expressed as a mathematical
    formula, visualized, and printed as a set of rules that subdivides the tree into
    branches and eventually the leaves.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类和回归树**（**CART**）算法是在大多数用例中选择的“纯朴”无附加功能的决策树。正如所注，大多数决策树不是白盒模型，但这个是，因为它被表达为一个数学公式，可视化，并以一组规则的形式打印出来，这些规则将树细分为分支，最终变为叶子。'
- en: 'The mathematical formula:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 数学公式：
- en: '![](img/B18406_03_027.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_027.png)'
- en: And what this means is that if according to the identity function *I*, *x* is
    in the subset *R*[m], then it returns a 1, otherwise a 0\. This binary term is
    multiplicated by the averages of all elements in the subset *R*[m] denoted as
    ![](img/B18406_03_028.png). So if x[i] is in the subset belonging to the leaf
    node *R*[k] then the prediction ![](img/B18406_03_029.png). In other words, the
    prediction is the average of all elements in the subset*R*[k]. This is what happens
    to regression tasks, and in binary classification, there is simply no ![](img/B18406_03_028.png)
    to multiply the *I* identify function.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，如果根据恒等函数 *I*，*x* 在子集 *R*[m] 中，则返回1，否则返回0。这个二元项与子集 *R*[m] 中所有元素的均值相乘，表示为
    ![](img/B18406_03_028.png)。所以如果 x[i] 在属于叶节点 *R*[k] 的子集中，那么预测 ![](img/B18406_03_029.png)。换句话说，预测是子集
    *R*[k] 中所有元素的均值。这就是回归任务发生的情况，在二分类中，简单地没有 ![](img/B18406_03_028.png) 来乘以 *I* 识别函数。
- en: At the heart of every decision tree algorithm, there’s a method to generate
    the *R*[m] subsets. For CART, this is achieved using something called the **Gini
    index**, recursively splitting on where the two branches are as different as possible.
    This concept will be explained in greater detail in *Chapter 4*, *Global Model-Agnostic
    Interpretation Methods*.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 每个决策树算法的核心都有一个生成 *R*[m] 子集的方法。对于CART，这是通过使用所谓的**基尼指数**来实现的，通过递归地在两个分支尽可能不同的情况下进行分割。这个概念将在*第4章*，*全局模型无关解释方法*中做更详细的解释。
- en: Interpretation
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释
- en: 'A decision tree can be globally and locally interpreted visually. Here, we
    have established a maximum depth of 2 (`max_depth=2`) because we could generate
    all 7 layers, but the text would be too small to appreciate. One of the limitations
    of this method is that it can get complicated to visualize with depths above 3
    or 4\. However, you can always programmatically traverse the branches of the tree
    and visualize only some branches at a time:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以全局和局部地通过视觉进行解释。在这里，我们设定了最大深度为2（`max_depth=2`），因为我们本可以生成所有7层，但文本太小，无法欣赏。这种方法的一个局限性是，当深度超过3或4时，可视化会变得复杂。然而，你总是可以通过编程遍历树的分支，并一次可视化一些分支：
- en: '[PRE37]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The preceding code prints out the tree in *Figure 3.9*. From the tree, you
    can tell that the very first branch splits the decision tree based on the value
    of `DEP_DELAY` being equal to or smaller than 20.5\. It tells you the Gini index
    that informed that decision and the number of `samples` (just another way of saying
    observations, data points, or rows) present. You can traverse these branches till
    they reach a leaf. There is one leaf node in this tree, and it is on the far left.
    This is a classification tree, so you can tell by the value =[629167, 0] that
    all 629,167 samples left in this node have been classified as a 0 (not delayed):'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码打印出了*图3.9*中的树。从树中，你可以看出第一个分支是根据 `DEP_DELAY` 的值等于或小于20.5来分割决策树的。它告诉你做出这个决策的基尼指数和存在的
    `samples`（这只是说观察、数据点或行的一种方式）的数量。你可以遍历这些分支，直到它们达到叶节点。这个树中有一个叶节点，它在最左边。这是一个分类树，所以你可以通过值=[629167,
    0]来判断这个节点中剩余的所有629,167个样本都被分类为0（未延迟）：
- en: '![](img/B18406_03_09.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_09.png)'
- en: 'Figure 3.9: Our models’ plotted decision tree'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：我们的模型绘制的决策树
- en: 'Another way the tree can be better visualized but with fewer details, such
    as the Gini index and sample size, is by printing out the decisions made in every
    branch and the class in every node:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 树的另一种可视化方式是打印出每个分支所做的决策和每个节点中的类别，但细节较少，例如基尼指数和样本大小：
- en: '[PRE38]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'And the preceding code outputs the following:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码输出了以下内容：
- en: '![A close-up of a document  Description automatically generated with medium
    confidence](img/B18406_03_10.png)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![文档的特写  描述自动生成，中等置信度](img/B18406_03_10.png)'
- en: 'Figure 3.10: Our decision tree’s structure'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10：我们的决策树结构
- en: There’s a lot more that can be done with a decision tree, and scikit-learn provides
    an API to explore the tree.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树还有很多可以做的事情，scikit-learn提供了一个API来探索树。
- en: Feature importance
  id: totrans-299
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征重要性
- en: 'Calculating feature importance in a CART decision tree is reasonably straightforward.
    As you can appreciate from the visualizations, some features appear more often
    in the decisions, but their appearances are weighted by how much they contributed
    to the overall reduction in the Gini index compared to the previous node. All
    the sum of the relative decrease in the Gini index throughout the tree is tallied,
    and the contribution of each feature is a percentage of this reduction:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在CART决策树中计算特征重要性相对直接。正如你可以从可视化中欣赏到的，一些特征在决策中出现的频率更高，但它们的出现是按照它们对Gini指数整体降低的贡献相对于前一个节点来加权的。整个树中所有相对降低的Gini指数总和被计算出来，每个特征的贡献是这个降低的百分比：
- en: '[PRE39]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The `dt_imp_df` DataFrame output by the preceding code can be appreciated in
    *Figure 3.11*:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码输出的`dt_imp_df` DataFrame可以在*图3.11*中欣赏到：
- en: '![Table  Description automatically generated](img/B18406_03_11.png)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_11.png)'
- en: 'Figure 3.11: Our decision tree’s feature importance'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：我们的决策树特征重要性
- en: This last feature importance table, *Figure 3.11*, increases suspicions about
    the delay features. They occupy, yet again, five of the top six positions. Is
    it possible that all five of them have such an outsized effect on the model?
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最后的功能重要性表，*图3.11*，增加了对延迟特征的怀疑。它们再次占据了前六个位置中的五个。这五者是否可能都对模型产生了如此巨大的影响？
- en: '**Interpretation and domain expertise**'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释和领域专业知识**'
- en: The target `CARRIER_DELAY` is also called a dependent variable because it’s
    dependent on all the other features, the independent variables. Even though a
    statistical relationship doesn’t imply causation, we want to inform our feature
    selection based on our understanding of what independent variables could plausibly
    affect a dependent one.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 目标`CARRIER_DELAY`也被称为因变量，因为它依赖于所有其他特征，即自变量。尽管统计关系并不一定意味着因果关系，但我们希望根据我们对哪些自变量可能影响因变量的理解来指导我们的特征选择。
- en: It makes sense that a departure delay (`DEPARTURE_DELAY`) affects the arrival
    delay (which we removed), and therefore, `CARRIER_DELAY`. Similarly, `LATE_AIRCRAFT_DELAY`
    makes sense as a predictor because it is known before the flight takes off if
    a previous aircraft was several minutes late, causing this flight to be at risk
    of arriving late, but not as a cause of the current flight (ruling this option
    out). However, even though the Bureau of Transportation Statistics website defines
    delays in such a way that they appear to be discrete categories, some may be determined
    well after a flight has departed. For instance, in predicting a delay mid-flight,
    could we use `WEATHER_DELAY` if the bad weather hasn’t yet happened? And could
    we use `SECURITY_DELAY` if the security breach hasn’t yet occurred? The answers
    to these questions are that we probably shouldn’t because the rationale for including
    them is they could serve to rule out `CARRIER_DELAY`, but this only works if they
    are discrete categories that pre-date the dependent variable! If they don’t they
    would be producing what is known as data leakage. Before coming to further conclusions,
    what you would need to do is talk to the airline executives to determine the timeline
    on which each delay category gets consistently set and (hypothetically) is accessible
    from the cockpit or the airline’s command center. Even if you are forced to remove
    them from the models, maybe other data can fill the void in a meaningful way,
    such as the first 30 minutes of flight logs and/or historical weather patterns.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 离港延误（`DEPARTURE_DELAY`）会影响到达延误（我们已移除），因此也会影响`CARRIER_DELAY`，这是有道理的。同样，`LATE_AIRCRAFT_DELAY`作为一个预测因子也是有道理的，因为它在飞机起飞前就已经知道如果之前的飞机晚了几分钟，那么这次航班就有可能晚到，但这并不是当前航班的原因（排除这个选项）。然而，尽管运输统计局网站将延误定义为似乎是有序类别的，但有些可能是在航班起飞后才确定的。例如，在预测中途延误时，如果恶劣天气还没有发生，我们能否使用`WEATHER_DELAY`？如果安全漏洞还没有发生，我们能否使用`SECURITY_DELAY`？对这些问题的回答是，我们可能不应该这样做，因为包括它们的理由是它们可以用来排除`CARRIER_DELAY`，但这只适用于它们是先于因变量存在的有序类别！如果它们不是，它们就会产生所谓的数据泄露。在得出进一步结论之前，你需要做的是与航空公司高管交谈，以确定每个延误类别被一致设置的时间表，以及（假设性地）从驾驶舱或航空公司的指挥中心可以访问的时间表。即使你被迫从模型中删除它们，也许其他数据可以在有意义的方式上填补空白，例如飞行记录的前30分钟和/或历史天气模式。
- en: Interpretation is not always directly inferred from the data and the machine
    learning models, but by working closely with domain experts. But sometimes domain
    experts can mislead you too. In fact, another insight is with all the time-based
    metrics and categorical features we engineered at the beginning of the chapter
    (`DEP_DOW`, `DEST_HUB`, `ORIGIN_HUB`, and so on). It turns out they have consistently
    had little to no effect on the models. Despite the airline executives hinting
    at the importance of days of the week, hubs, and congestion, we should have explored
    the data further, looking for correlations before engineering the data. But even
    if we do engineer some useless features, it also helps to use a white-box model
    to assess their impact, as we have. In data science, practitioners often will
    learn the same way that the most performant machine learning models do – by trial
    and error!
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 解释并不总是直接从数据和机器学习模型中推断出来，而是通过与领域专家紧密合作。但有时领域专家也可能误导你。事实上，另一个见解是，在本章开头我们构建的所有基于时间的指标和分类特征（如`DEP_DOW`、`DEST_HUB`、`ORIGIN_HUB`等）。结果证明，它们对模型的影响微乎其微。尽管航空公司高管暗示了星期几、枢纽和拥堵的重要性，但我们本应该进一步探索数据，在构建数据之前寻找相关性。但即使我们构建了一些无用的特征，使用白盒模型来评估它们的影响也是有帮助的，就像我们做的那样。在数据科学中，从业者通常会以最有效的机器学习模型的方式学习——通过试错！
- en: RuleFit
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: RuleFit
- en: '**RuleFit** is a model-class family that is a hybrid between a LASSO linear
    regression to get regularized coefficients for every feature and decision rules,
    which also uses LASSO to regularize. These **decision** **rules** are extracted
    by traversing a decision tree, finding interaction effects between features, and
    assigning coefficients to them based on their impact on the model. The implementation
    used in this chapter uses gradient-boosted decision trees to perform this task.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '**RuleFit**是一个模型类家族，它是LASSO线性回归和决策规则之间的混合体，为每个特征获取正则化系数，并且也使用LASSO进行正则化。这些**决策****规则**通过遍历决策树，找到特征之间的交互效应，并根据它们对模型的影响分配系数来提取。本章使用的实现使用梯度提升决策树来完成这项任务。'
- en: We haven’t covered decision rules explicitly in this chapter, but they are yet
    another family of **intrinsically interpretable models**. They weren’t included
    because, at the time of writing, the only Python library that supports decision
    rules, called **Bayesian Rule List** (**BRL**) by Skater, is still at an experimental
    stage. In any case, the concept behind decision rules is very similar. They extract
    the feature interactions from a decision tree but don’t discard the leaf node,
    and instead of assigning coefficients, they use the predictions in the leaf node
    to construct the rules. The last rule is a catch-all, like an *ELSE* statement.
    Unlike RuleFit, it can only be understood sequentially because it’s so similar
    to any *IF-THEN-ELSE* statement, but that’s its main advantage.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中并未明确介绍决策规则，但它们是另一类**本质上可解释的模型**。它们未被包含在内，是因为在撰写本书时，唯一支持决策规则的Python库，由Skater开发的名为**贝叶斯规则列表**（**BRL**），仍处于实验阶段。无论如何，决策规则背后的概念非常相似。它们从决策树中提取特征交互，但不会丢弃叶节点，而不是分配系数，而是使用叶节点中的预测来构建规则。最后的规则是一个通配符，就像一个*ELSE*语句。与RuleFit不同，它只能按顺序理解，因为它与任何*IF-THEN-ELSE*语句如此相似，但这正是它的主要优势。
- en: Interpretation and feature importance
  id: totrans-313
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释和特征重要性
- en: 'You can put everything you need to know about RuleFit into a single DataFrame
    (`rulefit_df`). Then you remove the rules that have a coefficient of `0`. It has
    these because in LASSO, unlike ridge, coefficient estimates converge to zero.
    You can sort the DataFrame by importance in a descending manner to see what features
    or feature interactions (in the form of rules) are most important:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将关于RuleFit所需了解的所有内容放入一个单独的DataFrame（`rulefit_df`）中。然后你移除那些系数为`0`的规则。这是因为与岭回归不同，在LASSO中，系数估计会收敛到零。你可以按重要性降序对DataFrame进行排序，以查看哪些特征或特征交互（以规则的形式）最为重要：
- en: '[PRE40]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The rules in the `rulefit_df` DataFrame can be seen in *Figure 3.12*:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '`rulefit_df` DataFrame中的规则可以在*图3.12*中看到：'
- en: '![Table  Description automatically generated](img/B18406_03_12.png)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![表格描述自动生成](img/B18406_03_12.png)'
- en: 'Figure 3.12: RuleFit’s rules'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：RuleFit的规则
- en: There’s a `type` for every RuleFit feature in *Figure 3.12*. Those that are
    `linear` are interpreted as you would any linear regression coefficient. Those
    that are `type=rule` are also to be treated like binary features in a linear regression
    model. For instance, if the rule `LATE_AIRCRAFT_DELAY <= 333.5 & DEP_DELAY > 477.5`
    is true, then the coefficient `172.103034` is applied to the prediction. The rules
    capture the interaction effects, so you don’t have to add interaction terms to
    the model manually or use some non-linear method to find them. Furthermore, it
    does this in an easy-to-understand manner. You can use RuleFit to guide your understanding
    of feature interactions even if you choose to productionize other models.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图3.12*中，每个RuleFit特征都有一个`type`。那些是`linear`的，就像解释任何线性回归系数一样。那些是`type=rule`的，也像线性回归模型中的二元特征一样处理。例如，如果规则`LATE_AIRCRAFT_DELAY
    <= 333.5 & DEP_DELAY > 477.5`为真，则将系数`172.103034`应用于预测。规则捕捉交互效应，因此你不必手动添加交互项到模型或使用某些非线性方法来找到它们。此外，它以易于理解的方式进行。即使你选择生产化其他模型，你也可以使用RuleFit来指导你对特征交互的理解。
- en: Nearest neighbors
  id: totrans-320
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最近邻
- en: Nearest neighbors is a family of models that even includes unsupervised methods.
    All of them use the closeness between data points to inform their predictions.
    Of all these methods, only the supervised k-NN and its cousin Radius Nearest Neighbors
    are somewhat interpretable.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 最近邻是一系列模型，甚至包括无监督方法。所有这些方法都使用数据点之间的接近性来提供预测信息。在这些方法中，只有监督k-NN及其近亲半径最近邻是可解释的。
- en: k-Nearest Neighbors
  id: totrans-322
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-最近邻
- en: The idea behind **k-NN** is straightforward. It takes the *k* closest points
    to a data point in the training data and uses their labels (`y_train`) to inform
    the predictions. If it’s a classification task, it’s the **mode** of all the labels,
    and if it’s a regression task, it’s the **mean**. It’s a **lazy learner** because
    the “fitted model” is not much more than the training data and the parameters,
    such as *k* and the list of classes (if it’s a classification). It doesn’t do
    much till inference. That’s when it leverages the training data, tapping into
    it directly rather than extracting parameters, weights/biases, or coefficients
    learned by the model as **eager learners** do.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-NN**背后的思想很简单。它选取训练数据中与数据点最近的*k*个点，并使用它们的标签（`y_train`）来提供预测信息。如果是分类任务，则是所有标签的**众数**，如果是回归任务，则是**均值**。它是一个**懒惰学习器**，因为“拟合模型”并不比训练数据和参数（如*k*和类别列表，如果是分类）多多少。它在推理之前不做太多。那时，它利用训练数据，直接从中提取，而不是像**急切学习器**那样提取模型学到的参数、权重/偏差或系数。'
- en: Interpretation
  id: totrans-324
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释
- en: '**k-NN** only has local interpretability because since there’s no fitted model,
    you don’t have global modular or global holistic interpretability. For classification
    tasks, you could attempt to get a sense of this using the decision boundaries
    and regions we studied in *Chapter 2*, *Key Concepts of Interpretability*. Still,
    it’s always based on local instances.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '**k-NN**只有局部可解释性，因为没有拟合模型，所以没有全局模块化或全局整体可解释性。对于分类任务，你可以尝试使用我们在*第二章*、*可解释性关键概念*中研究的决策边界和区域来获得这种感觉。然而，这始终基于局部实例。'
- en: 'To interpret a local point from our test dataset, we query the `pandas` DataFrame
    using its index. We will be using flight #721043:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释测试数据集中的局部点，我们使用其索引查询`pandas` DataFrame。我们将使用航班#721043：
- en: '[PRE41]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The preceding code outputs the following `pandas` series:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码输出了以下`pandas`序列：
- en: '[PRE42]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'In the `y_test_class` labels for flight #721043, we can tell that it was delayed
    because this code outputs 1:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在`y_test_class`标签中，对于航班#721043，我们可以看出它延误了，因为这段代码输出了1：
- en: '[PRE43]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'However, our k-NN model predicted that it was not because this code outputs
    0:'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的k-NN模型预测它不是延误，因为这段代码输出了0：
- en: '[PRE44]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Please note that the predictions are output as a `numpy` array, so we can’t
    access the prediction for flight #721043 using its `pandas` index (721043). We
    have to use the sequential location of this index in the test dataset using `get_loc`
    to retrieve it.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，预测以`numpy`数组的形式输出，因此我们无法使用航班#721043的`pandas`索引（721043）来访问预测。我们必须使用测试数据集中此索引的顺序位置，通过`get_loc`来检索它。
- en: 'To find out why this was the case, we can use `kneighbors` on our model to
    find the seven nearest neighbors of this point. To this end, we have to `reshape`
    our data because `kneighbors` will only accept it in the same shape found in the
    training set, which is (n, 21) where n is the number of observations (rows). In
    this case, `n=1` because we only want the nearest neighbors for a single data
    point. And as you can tell from what was output by `X_test.loc[721043,:]`, the
    `pandas` series has a shape of (21,1), so we have to reverse this shape:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出为什么会出现这种情况，我们可以使用模型中的`kneighbors`来找到这个点的七个最近邻。为此，我们必须`reshape`我们的数据，因为`kneighbors`只接受与训练集中相同的形状，即（n，21），其中n是观察数（行数）。在这种情况下，`n=1`，因为我们只想为单个数据点找到最近邻。而且正如你可以从`X_test.loc[721043,:]`输出的内容中看出，`pandas`序列的形状为（21，1），因此我们必须反转这个形状：
- en: '[PRE45]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`kneighbors` outputs two arrays:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '`kneighbors`输出两个数组：'
- en: '[PRE46]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'The first is the distance of each of the seven closest training points to our
    test data point. And the second is the location of these data points in the training
    data:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个是七个最近训练点到我们的测试数据点的距离。第二个是这些数据点在训练数据中的位置：
- en: '[PRE47]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The preceding code outputs the following `pandas` series:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了以下`pandas`序列：
- en: '[PRE48]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We can tell that the prediction reflects the **mode** because the most common
    class in the seven nearest points was 0 (not delayed). You can increase or decrease
    the *k* to see if this holds. Incidentally, when using binary classification,
    it’s recommended to choose an odd-numbered *k* so that there are no ties. Another
    important aspect is the distance metric that was used to select the closest data
    points. You can easily find out which one it is using:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看出预测反映了**众数**，因为在七个最近邻点中最常见的类别是0（未延迟）。你可以增加或减少*k*来查看这是否成立。顺便说一下，当使用二分类时，建议选择奇数*k*，这样就没有平局。另一个重要方面是用于选择最近数据点的距离度量。你可以很容易地找出它是哪一个：
- en: '[PRE49]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The output is Euclidean, which makes sense for this example. After all, Euclidean
    is optimal for a **real-valued vector space** because most features are continuous.
    You could also test alternative distance metrics such as `minkowski`, `seuclidean`,
    or `mahalanobis`. When most of your features are binary and categorical, you have
    an **integer-valued** **vector space**. So your distances ought to be calculated
    with algorithms suited for this space such as `hamming` or `canberra`.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是欧几里得距离，这对于这个例子是有意义的。毕竟，欧几里得距离对于**实值向量空间**是最优的，因为大多数特征是连续的。你也可以测试其他距离度量，如`minkowski`、`seuclidean`或`mahalanobis`。当你的大多数特征是二元和分类时，你有一个**整数值**的**向量空间**。因此，你的距离应该使用适合此空间的算法来计算，如`hamming`或`canberra`。
- en: Feature importance
  id: totrans-346
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 特征重要性
- en: Feature importance is, after all, a global model interpretation method and k-NN
    has a hyper-local nature, so there’s no way of deriving feature importance from
    a k-NN model.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 特征重要性毕竟是一种全局模型解释方法，而k-NN具有超局部性质，因此无法从k-NN模型中推导出特征重要性。
- en: Naïve Bayes
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Like GLMs, Naïve Bayes is a family of model classes with a model tailored to
    different statistical distributions. However, unlike GLMs’ assumption that the
    target *y* feature has the chosen distribution, all Naïve Bayes models assume
    that your *X* features have this distribution. More importantly, they were based
    on Bayes’ theorem of conditional probability, so they output a probability and
    are, therefore, exclusively classifiers. But they treat the probability of each
    feature impacting the model independently, which is a strong assumption. This
    is why they are called naïve. There’s one for Bernoulli called Bernoulli Naïve
    Bayes, one for multinomial called **Multinomial Naïve Bayes**, and, of course,
    one for Gaussian, which is the most common.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 与GLMs一样，朴素贝叶斯是一系列针对不同统计分布定制的模型类。然而，与GLMs假设目标*y*特征具有所选分布不同，所有朴素贝叶斯模型都假设你的*x*特征具有这种分布。更重要的是，它们基于贝叶斯定理的条件概率，因此输出一个概率，因此是专门的分类器。但它们独立地处理每个特征对模型的影响的概率，这是一个强烈的假设。这就是为什么它们被称为朴素。有一个伯努利朴素贝叶斯，一个多项式朴素贝叶斯，称为**多项式朴素贝叶斯**，当然还有一个高斯朴素贝叶斯，这是最常见的一种。
- en: Gaussian Naïve Bayes
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 高斯朴素贝叶斯
- en: 'Bayes’ theorem is defined by this formula: ![](img/B18406_03_031.png)'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理由以下公式定义：![](img/B18406_03_031.png)
- en: 'In other words, to find the probability of *A* happening given that *B* is
    true, you take the conditional probability of *B* given *A* is true times the
    probability of *A* occurring, divided by the probability of *B*. In the context
    of a machine learning classifier, this formula can be rewritten as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，要找到在*B*为真的条件下*A*发生的概率，你需要取在*A*为真的条件下*B*的条件概率乘以*A*发生的概率，然后除以*B*的概率。在机器学习分类器的上下文中，这个公式可以重写如下：
- en: '![](img/B18406_03_032.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_032.png)'
- en: 'This is because what we want is the probability of *y* given *X* is true. But
    our *X* has more than one feature, so this can be expanded like this:'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们想要的是在*X*为真的条件下*Y*的概率。但是我们的*X*有多个特征，所以这可以展开如下：
- en: '![](img/B18406_03_033.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_033.png)'
- en: 'To compute ![](img/B18406_03_007.png) predictions, we have to consider that
    we have to calculate and compare probabilities for each *C*[k] class (the probability
    of a delay versus the probability of no delay) and choose the class with the highest
    probability:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算![图片](img/B18406_03_007.png)预测，我们必须考虑我们必须计算和比较每个*C*[k]类（延迟的概率与无延迟的概率）的概率，并选择概率最高的类别：
- en: '![](img/B18406_03_035.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_035.png)'
- en: 'Calculating the probability of each class ![](img/B18406_03_036.png) (also
    known as the class prior) is relatively trivial. In fact, the fitted model has
    stored this in an attribute called `class_prior_`:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 计算每个类别的概率![图片](img/B18406_03_036.png)（也称为类先验）相对简单。事实上，拟合的模型已经将此存储在一个名为`class_prior_`的属性中：
- en: '[PRE50]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This outputs the following:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出以下内容：
- en: '[PRE51]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Naturally, since delays caused by the carrier only occur 6% of the time, there
    is a marginal probability of this occurring.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 自然地，由于由运营商引起的延迟只发生在6%的时间内，因此这种情况发生的边缘概率很小。
- en: 'Then the formula has a product ![](img/B18406_03_037.png) of conditional probabilities
    that each feature belongs to a class ![](img/B18406_03_038.png). Since this is
    binary there’s no need to calculate the probabilities of multiple classes because
    they are inversely proportional. Therefore, we can drop *C*[k] and replace it
    with a 1 like this:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，公式有一个条件概率的乘积![图片](img/B18406_03_037.png)，每个特征属于一个类![图片](img/B18406_03_038.png)。由于这是二元的，因此不需要计算多个类的概率，因为它们是成反比的。因此，我们可以省略*C*[k]并替换为1，如下所示：
- en: '![](img/B18406_03_039.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_039.png)'
- en: 'This is because what we are trying to predict is the probability of a delay.
    Also, ![](img/B18406_03_040.png) is its own formula, which differs according to
    the assumed distribution of the model – in this case, Gaussian:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为我们试图预测的是延迟的概率。此外，![图片](img/B18406_03_040.png)是其自己的公式，它根据模型的假设分布而有所不同——在这种情况下，高斯：
- en: '![](img/B18406_03_041.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_041.png)'
- en: This formula is called the probability density of the Gaussian distribution.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式被称为高斯分布的概率密度。
- en: Interpretation and feature importance
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 解释和特征重要性
- en: 'So what are these **sigmas** (![](img/B18406_03_042.png)) and **thetas** (![](img/B18406_03_043.png))
    in the formula? They are, respectively, the variance and mean of the *x*[i] feature
    when ![](img/B18406_03_044.png). The concept behind this is that features have
    a different variance and mean in one class versus another, which can inform the
    classification. This is a binary classification task, but you could calculate
    ![](img/B18406_03_045.png)and ![](img/B18406_03_043.png) for both classes. Fortunately,
    the fitted model has this stored:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，公式中的这些**sigma**(![图片](img/B18406_03_042.png))和**theta**(![图片](img/B18406_03_043.png))是什么？它们分别是当![图片](img/B18406_03_044.png)时*x*[i]特征的方差和均值。这个概念背后的想法是，特征在一个类别与另一个类别中具有不同的方差和均值，这可以提供分类信息。这是一个二元分类任务，但你也可以为两个类别计算![图片](img/B18406_03_045.png)和![图片](img/B18406_03_043.png)。幸运的是，拟合的模型已经存储了这些：
- en: '[PRE52]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'There are two arrays output, the first one corresponding to the negative class
    and the second to the positive. The arrays contain the sigma (variance) for each
    of the 21 features given the class:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个数组输出，第一个对应于负类，第二个对应于正类。数组包含给定类别的每个21个特征的sigma（方差）：
- en: '[PRE53]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'You can also extract the thetas (means) from the model:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以从模型中提取theta（均值）：
- en: '[PRE54]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The preceding code also outputs two arrays, one for each class:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码也输出了两个数组，每个类别一个：
- en: '[PRE55]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: These two arrays are all you need to debug and interpret Naïve Bayes results
    because you can use them to compute the conditional probability that the *x*[i]
    feature is given a positive class ![](img/B18406_03_047.png). You could use this
    probability to rank the features by importance on a global level, or interpret
    a specific prediction on a local level.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数组是你调试和解释朴素贝叶斯结果所需的所有内容，因为你可以使用它们来计算给定一个正类别的 *x*[i] 特征的条件概率 ![img/B18406_03_047.png](img/B18406_03_047.png)。你可以使用这个概率来按全局重要性对特征进行排序，或者在局部级别上解释一个特定的预测。
- en: '*Naïve Bayes* is a fast algorithm with some good use cases, such as spam filtering
    and recommendation systems, but the independence assumption hinders its performance
    in most situations. Speaking of performance, let’s discuss this topic in the context
    of interpretability.'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '*朴素贝叶斯*是一个快速算法，有一些很好的用例，例如垃圾邮件过滤和推荐系统，但独立性假设阻碍了它在大多数情况下的性能。说到性能，让我们在可解释性的背景下讨论这个话题。'
- en: Recognizing the trade-off between performance and interpretability
  id: totrans-379
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 认识到性能和可解释性之间的权衡
- en: 'We have briefly touched on this topic before, but high performance often requires
    complexity, and complexity inhibits interpretability. As studied in *Chapter 2*,
    *Key Concepts of Interpretability*, this complexity comes from primarily three
    sources: non-linearity, non-monotonicity, and interactivity. If the model adds
    any complexity, it is **compounded by the number and nature of features** in your
    dataset, which by itself is a source of complexity.'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前已经简要地涉及过这个话题，但高性能往往需要复杂性，而复杂性会阻碍可解释性。正如在*第二章*，*可解释性的关键概念*中研究的那样，这种复杂性主要来自三个来源：非线性、非单调性和交互性。如果模型增加了任何复杂性，它将由你数据集中**特征的数量和性质**所**复合**，这本身就是一个复杂性的来源。
- en: Special model properties
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特殊模型特性
- en: These special properties can help make a model more interpretable.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特殊属性可以帮助使模型更具可解释性。
- en: 'The key property: explainability'
  id: totrans-383
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 关键特性：可解释性
- en: In *Chapter 1*, *Interpretation, Interpretability, and Explainability; and Why
    Does It All Matter?*, we discussed why being able to look under the hood of the
    model and intuitively understand how all its moving parts derive its predictions
    in a consistent manner is, mostly, what separates *explainability* from *interpretability*.
    This property is also called **transparency** or **translucency**. A model can
    be interpretable without this, but in the same way as interpreting a person’s
    decisions because we can’t understand what is going on “under the hood.” This
    is often called **post-hoc interpretability** and this is the kind of interpretability
    this book primarily focuses on, with a few exceptions. That being said, we ought
    to recognize that if a model is understood by leveraging its mathematical formula
    (grounded in statistical and probability theory), as we’ve done with linear regression
    and Naïve Bayes, or by visualizing a human-interpretable structure, as with decision
    trees, or a set of rules as with RuleFit, it is much more interpretable than machine
    learning model classes where none of this is practically possible.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第一章*，*解释、可解释性和可解释性；以及为什么这一切都很重要？*中，我们讨论了为什么能够查看模型的内部并直观地理解所有移动部件如何以一致的方式推导出其预测，这主要是将*可解释性*与*可解释性*区分开来的原因。这个特性也被称为**透明度**或**半透明度**。一个模型可以没有这个特性仍然具有可解释性，但就像我们因为无法理解“内部”发生的事情而解释一个人的决定一样。这通常被称为**事后可解释性**，这正是本书主要关注的一种可解释性，尽管有一些例外。话虽如此，我们应该认识到，如果一个模型可以通过利用其数学公式（基于统计和概率理论）来理解，就像我们在线性回归和朴素贝叶斯中所做的那样，或者通过可视化一个可由人类解释的结构，就像决策树或一组规则（如RuleFit）那样，那么它比那些在实际上不可能做到这一点的机器学习模型类别要容易解释得多。
- en: White-box models will always have the upper hand in this regard, and as listed
    in *Chapter 1*, *Interpretation, Interpretability, and Explainability; and Why
    Does It All Matter?*, there are many use cases in which a white-box model is a
    must-have. But even if you don’t productionize white-box models, they can always
    serve a purpose in assisting with interpretation, if data dimensionality allows.
    Transparency is a key property because it wouldn’t matter if it didn’t comply
    with the other properties as long as it had explainability; it would still be
    more interpretable than those without it.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 白盒模型在这方面始终具有优势，如*第一章*中所述，*解释、可解释性和可解释性；以及为什么这一切都很重要？*，有许多用例中白盒模型是必不可少的。但即使您不将白盒模型投入生产，只要数据维度允许，它们也可以在辅助解释方面发挥作用。透明性是一个关键属性，因为即使它不符合其他属性，只要它具有可解释性，它仍然比没有它的模型更具可解释性。
- en: 'The remedial property: regularization'
  id: totrans-386
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 补救性质：正则化
- en: In this chapter, we’ve learned that *regularization* limits the complexity added
    by the introduction of too many features, and this can make the model more interpretable,
    not to mention more performant. Some models incorporate regularization into the
    training algorithm, such as RuleFit and gradient-boosted trees; others have the
    ability to integrate it, such as multi-layer perceptron, or linear regression,
    and some cannot include it, such as k-NN. Regularization comes in many forms.
    Decision trees have a method called pruning, which can help reduce complexity
    by removing non-significant branches. Neural networks have a technique called
    dropout, which randomly drops neural network nodes from layers during training.
    Regularization is a remedial property because it can help even the least interpretable
    models lessen complexity and thus improve interpretability.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经了解到*正则化*限制了过多特征引入所增加的复杂性，这可以使模型更具可解释性，更不用说性能更佳。一些模型将正则化纳入训练算法中，例如RuleFit和梯度提升树；其他模型具有集成正则化的能力，例如多层感知器或线性回归，而有些模型则不能包含它，例如k-NN。正则化有多种形式。决策树有一种称为剪枝的方法，可以通过删除非显著分支来帮助降低复杂性。神经网络有一种称为dropout的技术，在训练过程中会随机从层中丢弃神经网络节点。正则化是一种补救性质，因为它可以帮助即使是可解释性最差的模型减少复杂性，从而提高可解释性。
- en: Assessing performance
  id: totrans-388
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估性能
- en: By now, in this chapter, you have already assessed performance on all of the
    white-box models reviewed in the last section as well as a few black-box models.
    Maybe you’ve already noticed that black-box models have topped most metrics, and
    for most use cases, this is generally the case.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，您已经评估了上一节中审查的所有白盒模型以及一些黑盒模型的性能。也许您已经注意到，黑盒模型在大多数指标上都位居前列，对于大多数用例来说，这通常是情况。
- en: Figuring out which model classes are more interpretable is not an exact science,
    but the following table (*Figure 3.17*) is sorted by those models with the most
    desirable properties – that is, they don’t introduce non-linearity, non-monotonicity,
    and interactivity. Of course, explainability on its own is a property that is
    a game-changer, regardless, and regularization can help. There are also cases
    in which it’s hard to assess properties. For instance, polynomial (linear) regression
    implements a linear model, but it fits non-linear relationships, which is why
    it is color-coded differently. As you will learn in *Chapter 12*, *Monotonic Constraints
    and Model Tuning for Interpretability*, some libraries support adding monotonic
    constraints to gradient-boosted trees and neural networks, which means it’s possible
    to make these monotonic. However, the black-box methods we used in this chapter
    do not support monotonic constraints.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 确定哪些模型类别更具可解释性并不是一门精确的科学，但以下表格（*图3.17*）是根据具有最理想属性的模型排序的——也就是说，它们不引入非线性、非单调性和交互性。当然，可解释性本身就是一个颠覆性的属性，无论如何，正则化都能帮助。也有一些情况下很难评估属性。例如，多项式（线性）回归实现了一个线性模型，但它拟合了非线性关系，这就是为什么它被用不同的颜色编码。正如您将在*第12章*中学习到的，*单调约束和模型调优以提高可解释性*，一些库支持向梯度提升树和神经网络添加单调约束，这意味着可以使这些模型单调。然而，本章中使用的黑盒方法不支持单调约束。
- en: 'The task columns tell you whether they can be used for regression or classification.
    And the **Performance Rank** columns show you how well these models ranked in
    RMSE (for regression) and ROC AUC (for classification), where lower ranks are
    better. Please note that even though we have used only one metric to assess performance
    for this chart for simplicity’s sake, the discussion about performance should
    be more nuanced than that. Another thing to note is that ridge regression did
    poorly, but this is because we used the wrong hyperparameters, as explained in
    the previous section:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 任务列告诉你它们是否可用于回归或分类。而**性能排名**列显示了这些模型在RMSE（回归）和ROC AUC（分类）中的排名情况，排名越低越好。请注意，尽管为了简化起见，我们只使用了一个指标来评估此图表的性能，但关于性能的讨论应该比这更细致。另一件需要注意的事情是，岭回归表现不佳，但这是因为我们使用了错误的超参数，如前节所述：
- en: '![A picture containing chart  Description automatically generated](img/B18406_03_13.png)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![包含图表的图片，自动生成描述](img/B18406_03_13.png)'
- en: 'Figure 3.13: A table assessing the interpretability and performance of several
    white-hat and black-box models we have explored in this chapter'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13：评估我们在本章中探索的几个白盒和黑盒模型的可解释性和性能的表格
- en: Because it’s compliant with all five properties, it’s easy to tell why **linear
    regression is the gold standard for interpretability**. Also, while recognizing
    that this is anecdotal evidence, it should be immediately apparent that most of
    the best ranks are with black-box models. This is no accident! The math behind
    neural networks and gradient-boosted trees is brutally efficient in achieving
    the best metrics. Still, as the red dots suggest, they have all the properties
    that make a model less interpretable, making their biggest strength (complexity)
    a potential weakness.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它符合所有五个属性，所以很容易理解为什么**线性回归是可解释性的黄金标准**。此外，虽然认识到这只是一个轶事证据，但应该立即明显的是，大多数最佳排名都是黑盒模型。这不是偶然！神经网络和梯度提升树背后的数学在实现最佳指标方面非常高效。然而，正如红色圆点所暗示的，它们都具有使模型不太可解释的特性，使它们的最大优势（复杂性）成为潜在的弱点。
- en: 'This is precisely why black-box models are our primary interest in this book,
    although many of the methods you will learn to apply to white-box models. In *Part
    2*, which comprises *Chapters 4* to *9*, we will learn model-agnostic and deep-learning-specific
    methods that assist with interpretation. And in *Part 3*, which includes *Chapters
    10* to *14*, we will learn how to tune models and datasets to increase interpretability:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 这正是为什么在本书中，尽管我们将学习许多应用于白盒模型的方法，但黑盒模型是我们的主要兴趣所在。在**第二部分**，包括**第4章**到**第9章**，我们将学习模型无关和深度学习特定方法，这些方法有助于解释。而在**第三部分**，包括**第10章**到**第14章**，我们将学习如何调整模型和数据集以增加可解释性：
- en: '![Chart, funnel chart  Description automatically generated](img/B18406_03_14.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图表，漏斗图，自动生成描述](img/B18406_03_14.png)'
- en: 'Figure 3.14: A table comparing white-box, black-box, and glass-box models,
    or at least what is known so far about them'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：比较白盒、黑盒和玻璃盒模型，或者至少是关于它们的已知信息表格
- en: Discovering newer interpretable (glass-box) models
  id: totrans-398
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现新的可解释（玻璃盒）模型
- en: In the last decade, there have been significant efforts in both industry and
    in academia to create new models that can have enough complexity to find the sweet
    spot between underfitting and overfitting, known as the **bias-variance trade-off**,
    but retain an adequate level of explainability.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去十年中，无论是工业界还是学术界，都做出了重大努力，创造新的模型，这些模型具有足够的复杂性，可以在欠拟合和过拟合之间找到最佳平衡点，即所谓的**偏差-方差权衡**，同时保持足够的可解释性水平。
- en: Many models fit this description, but most of them are meant for specific use
    cases, haven’t been properly tested yet, or have released a library or open-sourced
    code. However, two general-purpose ones are already gaining traction, which we
    will look at now.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 许多模型符合这种描述，但其中大多数是为特定用例设计的，尚未经过适当测试，或者已经发布了一个库或开源代码。然而，已经有两种通用模型开始受到关注，我们现在将探讨它们。
- en: Explainable Boosting Machine (EBM)
  id: totrans-401
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 可解释提升机（EBM）
- en: '**EBM** is part of Microsoft’s InterpretML framework, which includes many of
    the model-agnostic methods we will use later in the book.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**EBM**是微软的InterpretML框架的一部分，该框架包括我们在本书后面将使用的许多模型无关方法。'
- en: 'EBM leverages the **GAMs** we mentioned earlier, which are like linear models
    but look like this:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: EBM利用了我们之前提到的**GAMs**，它们类似于线性模型，但看起来是这样的：
- en: '![](img/B18406_03_048.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_048.png)'
- en: 'Individual functions *f*[1] through *f*[p] are fitted to each feature using
    spline functions. Then a link function *g* adapts the GAM to perform different
    tasks such as classification or regression, or adjust predictions to different
    statistical distributions. GAMs are white-box models, so what makes EBM a glass-box
    model? It incorporates bagging and gradient boosting, which tend to make models
    more performant. The boosting is done one feature at a time using a low learning
    rate so as not to confound them. It also finds practical interaction terms automatically,
    which improves performance while maintaining interpretability:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 使用样条函数将单个函数 *f*[1] 到 *f*[p] 分别拟合到每个特征。然后，一个链接函数 *g* 适应GAM执行不同的任务，如分类或回归，或将预测调整到不同的统计分布。GAM是白盒模型，那么是什么让EBM成为玻璃盒模型呢？它结合了袋装和梯度提升，这往往会使模型性能更佳。提升是逐个特征进行的，使用低学习率以避免混淆。它还可以自动找到实用的交互项，这提高了性能同时保持了可解释性：
- en: '![](img/B18406_03_049.png)'
  id: totrans-406
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_03_049.png)'
- en: Once fitted, this formula is made up of complicated non-linear formulas, so
    a global holistic interpretation isn’t likely feasible. However, since the effects
    of each feature or pairwise interaction terms are additive, they are easily separable,
    and global modular interpretation is entirely possible. Local interpretation is
    equally easy, given that a mathematical formula can assist in debugging any prediction.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦拟合，这个公式就由复杂的非线性公式组成，因此全球整体解释不太可能可行。然而，由于每个特征或成对交互项的效果是可加的，它们很容易分离，全球模块化解释是完全可能的。局部解释同样简单，因为数学公式可以帮助调试任何预测。
- en: 'One drawback is that EBM can be much slower than gradient-boosted trees and
    neural networks because of the *one feature at a time* approach, a low learning
    rate not impacting the feature order, and spline fitting methods. However, it
    is parallelizable, so in environments with ample resources and multiple cores
    or machines, it will be much quicker. To avoid waiting for results for an hour
    or two, it is best to create abbreviated versions of `X_train` and `X_test` –
    that is, with fewer columns representing only the eight features white-box models
    found to be most important: `DEP_DELAY`, `LATE_AIRCRAFT_DELAY`, `PCT_ELAPSED_TIME`,
    `WEATHER_DELAY, NAS_DELAY`, `SECURITY_DELAY`, `DISTANCE`, `CRS_ELAPSED_TIME`,
    and `TAXI_OUT`. These are placed in a `feature_samp` array, and then the `X_train`
    and `X_test` DataFrames are subset to only include this feature. We are setting
    the `sample2_size` to 10%, but if you feel you have enough resources to handle
    it, adjust accordingly:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 一个缺点是，由于 *逐个特征* 的方法、低学习率不影响特征顺序以及样条拟合方法，EBM可能比梯度提升树和神经网络慢得多。然而，它是可并行的，所以在拥有充足资源和多个核心或机器的环境中，它会快得多。为了避免等待结果花费一两个小时，最好是创建
    `X_train` 和 `X_test` 的简略版本——也就是说，只有较少的列代表白盒模型发现的最重要八个特征：`DEP_DELAY`（出发延误）、`LATE_AIRCRAFT_DELAY`（晚点飞机延误）、`PCT_ELAPSED_TIME`（已过时间百分比）、`WEATHER_DELAY`（天气延误）、`NAS_DELAY`（机场延误）、`SECURITY_DELAY`（安全延误）、`DISTANCE`（距离）、`CRS_ELAPSED_TIME`（航班已过时间）和
    `TAXI_OUT`（滑出）。这些被放置在 `feature_samp` 数组中，然后 `X_train` 和 `X_test` DataFrame被子集化，只包括这些特征。我们将
    `sample2_size` 设置为10%，但如果你觉得你有足够的资源来处理它，相应地调整：
- en: '[PRE56]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'To train your EBM, all you have to do is instantiate an `ExplainableBoostingClassifier()`
    and then fit your model to your training data. Note that we are using `sample_idx`
    to sample a portion of the data so that it takes less time:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 要训练你的EBM，你只需要实例化一个 `ExplainableBoostingClassifier()`，然后将你的模型拟合到你的训练数据。注意，我们正在使用
    `sample_idx` 来采样数据的一部分，这样它花费的时间会更少：
- en: '[PRE57]'
  id: totrans-411
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Global interpretation
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全球解释
- en: 'Global interpretation is dead simple. It comes with an `explain_global` dashboard
    you can explore. It loads with the feature importance plot first, and you can
    select individual features to graph what was learned from each one:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 全球解释非常简单。它包含一个你可以探索的 `explain_global` 仪表板。它首先加载特征重要性图，你可以选择单个特征来绘制从每个特征中学到的内容：
- en: '[PRE58]'
  id: totrans-414
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The preceding code generates a dashboard that looks like *Figure 3.15*:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 上一段代码生成一个看起来像 *图3.15* 的仪表板：
- en: '![Chart, bar chart  Description automatically generated](img/B18406_03_15.png)'
  id: totrans-416
  prefs: []
  type: TYPE_IMG
  zh: '![图表，条形图  自动生成的描述](img/B18406_03_15.png)'
- en: 'Figure 3.15: EBM’s global interpretation dashboard'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15：EBM的全球解释仪表板
- en: Local interpretation
  id: totrans-418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部解释
- en: 'Local interpretation uses a dashboard like global does, except you choose specific
    predictions to interpret with `explain_local`. In this case, we are selecting
    #76, which, as you can tell, was incorrectly predicted. But the LIME-like plot
    we will study in *Chapter 5*, *Local Model-Agnostic Interpretation Methods*, helps
    us make sense of it:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 局部解释使用与全局相同的仪表板，除了你选择使用`explain_local`解释的具体预测。在这种情况下，我们选择了#76，正如你可以看到的，这是一个错误的预测。但我们在第5章“局部模型无关解释方法”中将要研究的类似LIME的图可以帮助我们理解它：
- en: '[PRE59]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Similar to the global dashboard, the preceding code generates another one,
    depicted in *Figure 3.16*:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 与全局仪表板类似，前面的代码生成了另一个，如图3.16所示：
- en: '![Chart, bar chart  Description automatically generated](img/B18406_03_16.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![图表，柱状图  自动生成的描述](img/B18406_03_16.png)'
- en: 'Figure 3.16: EBM’s local interpretation dashboard'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16：EBM的局部解释仪表板
- en: Performance
  id: totrans-424
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能
- en: EBM performance, at least measured with the ROC AUC, is not far from what was
    achieved by the top two classification models, and we can only expect it to get
    better with 10 times more training and testing data!
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: EBM性能，至少用ROC AUC来衡量，并不远低于前两个分类模型所取得的成果，我们只能期待在10倍更多训练和测试数据的情况下它会变得更好！
- en: '[PRE60]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: You can appreciate the performance dashboard produced by the preceding code
    in *Figure 3.17*.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图3.17中欣赏到前面代码生成的性能仪表板。
- en: '![Chart, line chart  Description automatically generated](img/B18406_03_17.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成的描述](img/B18406_03_17.png)'
- en: 'Figure 3.17: One of EBM’s performance dashboards'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17：EBM的一个性能仪表板
- en: The performance dashboard can also compare several models at a time since its
    explainers are model-agnostic. And there’s even a fourth dashboard that can be
    used for data exploration. Next, we will cover another GAM-based model.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其解释器是模型无关的，性能仪表板可以同时比较多个模型。还有一个第四个仪表板，可以用于数据探索。接下来，我们将介绍另一个基于GAM的模型。
- en: GAMI-Net
  id: totrans-431
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: GAMI-Net
- en: There’s also a newer GAM-based method with similar properties to EBM but trained
    with neural networks. At the time of writing, this method has yet to get commercial
    traction but yields good interpretability and performance.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 也有一种新的基于GAM的方法，与EBM具有相似的性质，但使用神经网络进行训练。在撰写本文时，这种方法尚未获得商业上的吸引力，但提供了良好的可解释性和性能。
- en: As we have previously discussed, interpretability is decreased by each additional
    feature, especially those that don’t significantly impact model performance. In
    addition to too many features, it’s also trumped by the added complexity of non-linearities,
    non-monotonicity, and interactions. GAMI-Net tackles all these problems by fitting
    non-linear subnetworks for each feature in the main effects network first. Then,
    fitting a pairwise interaction network with subnetworks for each combination of
    features. The user provides a maximum number of interactions to keep, which are
    then fitted to the residuals of the main effects network. See *Figure 3.18* for
    a diagram.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，可解释性会因每个额外特征而降低，尤其是那些对模型性能没有显著影响的特征。除了过多的特征外，它还受到非线性、非单调性和交互增加的复杂性的影响。GAMI-Net通过首先为主效应网络中的每个特征拟合非线性子网络来解决所有这些问题。然后，为每个特征的组合拟合一个成对交互网络。用户提供要保留的最大交互数，然后将其拟合到主效应网络的残差。请参见图3.18以获取图解。
- en: '![](img/B18406_03_18.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_03_18.png)'
- en: 'Figure 3.18: Diagram of the GAMI-Net model'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18：GAMI-Net模型图
- en: 'GAMI-Net has three interpretability constraints built in:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: GAMI-Net内置了三个可解释性约束：
- en: '**Sparsity**: Only the top features and interactions are kept.'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稀疏性**：只保留顶级特征和交互。'
- en: '**Heredity**: A pairwise interaction can be included if at least one of its
    parent features is included.'
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗传性**：如果至少包含其父特征中的一个，则可以包含成对交互。'
- en: '**Marginal clarity**: Non-orthogonality in interactions is penalized to approximate
    better marginal clarity.'
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边际清晰度**：交互中的非正交性会受到惩罚，以更好地近似边际清晰度。'
- en: The GAMI-Net implementation can also enforce monotonic constraints, which we
    will cover in more detail in *Chapter 12*, *Monotonic Constraints and Model Tuning
    for Interpretability*.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: GAMI-Net的实现还可以强制执行单调约束，我们将在第12章“单调约束和模型调优以提高可解释性”中更详细地介绍。
- en: 'Before we start, we must create a dictionary called `meta_info` with details
    about each feature and target, such as the type (continuous, categorical, and
    target) and the scaler used to scale each feature — since the library expects
    each feature to be scaled independently. All the features in the abbreviated dataset
    are continuous so we can leverage dictionary comprehension to this easily:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们必须创建一个名为 `meta_info` 的字典，其中包含每个特征和目标的相关细节，例如类型（连续、分类和目标）以及用于缩放每个特征的缩放器——因为库期望每个特征都独立缩放。由于简化的数据集中的所有特征都是连续的，我们可以利用字典推导来轻松实现这一点：
- en: '[PRE61]'
  id: totrans-442
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Next, we will create a copy of `X_train_abbrev` and `X_train_abbrev` and then
    scale them and store the scalers in the dictionary. Then, we will append information
    about the target variable to the dictionary. And lastly, we will convert all the
    data to `numpy` format:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建 `X_train_abbrev` 和 `X_train_abbrev` 的副本，然后对它们进行缩放并将缩放器存储在字典中。然后，我们将有关目标变量的信息添加到字典中。最后，我们将所有数据转换为
    `numpy` 格式：
- en: '[PRE62]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Now that we have a `meta_info` dictionary and the dataset is ready, we can
    initialize and fit GAMI-Net to the training data. In addition to `meta_info`,
    it has a lot of parameters: `interact_num` defines how many top interactions it
    should consider, and `task_type` defines whether it’s a classification or regression
    task. Note that GAMI-Net trains three neural networks, so there are three epoch
    parameters to fill in (`main_effect_epochs`, `interaction_epochs`, and `tuning_epochs`).
    The learning rate (`lr_bp`) and early stopping thresholds (`early_stop_thres`)
    are entered as a list for each of the epoch parameters. You will also find lists
    for the architecture of the networks, where each item corresponds to a number
    of nodes per layer (`interact_arch` and `subnet_arch`). Furthermore, there are
    additional parameters for batch size, activation function, whether to enforce
    heredity constraint, a loss threshold for early stopping, and what percentage
    of the training data to use for validation (`val_ratio`). Finally, there are two
    optional parameters for monotonic constraints (`mono_increasing_list`, `mono_decreasing_list`):'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了 `meta_info` 字典，数据集也已准备就绪，我们可以初始化并拟合 GAMI-Net 到训练数据。除了 `meta_info`，它还有很多参数：`interact_num`
    定义了它应该考虑多少个顶级交互，而 `task_type` 定义了它是一个分类任务还是回归任务。请注意，GAMI-Net 训练三个神经网络，因此有三个 epoch
    参数需要填写（`main_effect_epochs`、`interaction_epochs` 和 `tuning_epochs`）。学习率 (`lr_bp`)
    和早期停止阈值 (`early_stop_thres`) 作为每个 epoch 参数的列表输入。你还会找到网络架构的列表，其中每个条目对应于每层的节点数（`interact_arch`
    和 `subnet_arch`）。此外，还有批大小、激活函数、是否强制执行遗传约束、早期停止的损失阈值以及用于验证的训练数据百分比（`val_ratio`）等附加参数。最后，还有两个用于单调约束的可选参数（`mono_increasing_list`、`mono_decreasing_list`）：
- en: '[PRE63]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'We can plot the training loss for each epoch across all three trainings with
    `plot_trajectory`. Then, with `plot_regularization`, we can plot the outcome for
    the regularization of both the main effects and interaction networks. Both plotting
    functions can save the image in a folder but will do so in a folder called `results`
    by default, unless you change the path with the `folder` parameter:'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `plot_trajectory` 绘制所有三个训练的每个 epoch 的训练损失。然后，使用 `plot_regularization`，我们可以绘制主效应网络和交互网络正则化的结果。这两个绘图函数都可以将图像保存到文件夹中，但默认情况下会保存到名为
    `results` 的文件夹中，除非你使用 `folder` 参数更改路径：
- en: '[PRE64]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '![Graphical user interface  Description automatically generated](img/B18406_03_19.png)Figure
    3.19: The trajectory and regularization plots for the GAMI-net training process'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '![图形用户界面 自动生成的描述](img/B18406_03_19.png)图3.19：GAMI-net 训练过程的轨迹和正则化图'
- en: '*Figure 3.19* tells the story of how the three stages sequentially reduce loss
    while regularizing to only keep the fewest features and interactions as possible.'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.19* 讲述了三个阶段如何依次减少损失，同时在正则化过程中尽可能保留最少的特征和交互。'
- en: Global interpretation
  id: totrans-451
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 全局解释
- en: 'Global explanations can be extracted in a dictionary with the `global_explain`
    function and then turned into a feature importance plot with `feature_importance_visualize`,
    like in the following snippet:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用 `global_explain` 函数从字典中提取全局解释，然后使用 `feature_importance_visualize` 将其转换为特征重要性图，如下面的代码片段所示：
- en: '[PRE65]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'The preceding snippet outputs the following plot:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码片段输出以下图表：
- en: '![Histogram  Description automatically generated with medium confidence](img/B18406_03_20.png)'
  id: totrans-455
  prefs: []
  type: TYPE_IMG
  zh: '![直方图 使用中等置信度自动生成的描述](img/B18406_03_20.png)'
- en: 'Figure 3.20: A global explanation plot'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20：全局解释图
- en: As you can tell by *Figure 3.20*, the most important feature is, by far, `DEP_DELAY`
    and one interaction is among the top six features in the plot. We can also use
    the `global_visualize_density` plot to output partial dependence plots, which
    we will cover in *Chapter 4*, *Global Model-Agnostic Interpretation Methods*.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 3.20* 所示，最重要的特征无疑是 `DEP_DELAY`，其中一个交互作用在图表中排名前六。我们还可以使用 `global_visualize_density`
    图表来输出部分依赖图，这将在 *第 4 章*，*全局模型无关解释方法* 中介绍。
- en: Local interpretation
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 局部解释
- en: 'Let’s examine an explanation for a single prediction using `local_explain`,
    followed by `local_visualize`. We are selecting test case #73:'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们使用 `local_explain` 来解释单个预测，然后是 `local_visualize`。我们选择了测试案例 #73：'
- en: '[PRE66]'
  id: totrans-460
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'The preceding code generates the plot in *Figure 3.21*:'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码生成了 *图 3.21* 的图表：
- en: '![Graphical user interface, application, Teams  Description automatically generated](img/B18406_03_21.png)'
  id: totrans-462
  prefs: []
  type: TYPE_IMG
  zh: '![图形用户界面，应用程序，团队 描述自动生成](img/B18406_03_21.png)'
- en: 'Figure 3.21: A local explanation plot for test case #73'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '图 3.21：测试案例 #73 的局部解释图'
- en: '*Figure 3.21* tells the story of how each feature weighs in the outcome. Note
    that `DEP_DELAY` is over 50 but that there’s an intercept that almost cancels
    it out. The intercept is a counterbalance – after all, the dataset is unbalanced
    toward it being less likely to be a `CARRIER_DELAY`. But all the subsequent features
    after the intercept are not enough to push the outcome positively.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 3.21* 讲述了每个特征在结果中的权重。注意 `DEP_DELAY` 超过 50，但有一个截距几乎抵消了它。截距是一种平衡因素——毕竟，数据集在
    `CARRIER_DELAY` 不太可能的情况下是不平衡的。但截距之后的所有后续特征都不足以推动结果向积极方向变化。'
- en: Performance
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 性能
- en: 'To determine the predictive performance of the GAMI-Net model, all we need
    to do is get the scores (`y_test_prob`) and predictions (`y_test_pred`) for the
    test dataset and then use scikit-learn’s `metrics` functions to compute them:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定 GAMI-Net 模型的预测性能，我们只需要获取测试数据集的分数 (`y_test_prob`) 和预测 (`y_test_pred`)，然后使用
    scikit-learn 的 `metrics` 函数来计算它们：
- en: '[PRE67]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The preceding code yields the following metrics:'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码产生了以下指标：
- en: '[PRE68]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The performance was not bad considering it was trained on 10% of the training
    data and evaluated on only 10% of the test data – especially the recall score,
    which was among the top three places.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到它在 10% 的训练数据上训练，并在仅 10% 的测试数据上评估，性能还不错——特别是召回率，它排名前三。
- en: Mission accomplished
  id: totrans-471
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: The mission was to train models that could predict preventable delays with enough
    accuracy to be useful, and then, to understand the factors that impacted these
    delays, according to these models, to improve OTP. The resulting regression models
    all predicted delays, on average, well below the 15-minute threshold according
    to the RMSE. And most of the classification models achieved an F1 score well above
    50% – one of them reached 98.8%! We also managed to find factors that impacted
    delays for all white-box models, some of which performed reasonably well. So,
    it seems like it was a resounding success!
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是训练模型，能够以足够的准确性预测可预防的延误，然后，根据这些模型，了解影响这些延误的因素，以改善 OTP。结果回归模型平均预测的延误都低于 15
    分钟的阈值，根据 RMSE 来看。而且，大多数分类模型达到了 50% 以上的 F1 分数——其中一个达到了 98.8%！我们还设法找到了影响所有白盒模型延误的因素，其中一些表现相当不错。所以，这似乎是一次巨大的成功！
- en: Don’t celebrate just yet! Despite the high metrics, this mission was a failure.
    Through interpretation methods, we realized that the models were accurate mostly
    for the wrong reasons. This realization helps underpin the mission-critical lesson
    that a model can easily be right for the wrong reasons, so *the question “why?”
    is not a question to be asked only when it performs poorly but always*. And using
    interpretation methods is how we ask that question.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 还不要庆祝！尽管指标很高，但这次任务失败了。通过解释方法，我们发现模型之所以准确，主要是因为错误的原因。这一认识有助于巩固一个关键的教训，即模型可以轻易地因为错误的原因而正确，所以
    *“为什么？”这个问题不仅仅是在模型表现不佳时才需要问，而是一直都应该问*。而使用解释方法正是我们提出这个问题的途径。
- en: But if the mission failed, why is this section called *Mission accomplished?*
    Good question!
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果任务失败了，为什么这个部分还被称为 *Mission accomplished?* 好问题！
- en: 'It turns out there was a secret mission. Hint: it’s the title of this chapter.
    The point of it was to learn about common interpretation challenges through the
    failure of the overt mission. In case you missed them, here are the interpretation
    challenges we stumbled upon:'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，有一个秘密任务。提示：这是本章的标题。它的目的是通过公开任务的失败来了解常见的解释挑战。以防你错过了，以下是我们在探索中遇到的解释挑战：
- en: Traditional model interpretation methods only cover surface-level questions
    about your models. Note that we had to resort to model-specific global interpretation
    methods to discover that the models were right for the wrong reasons.
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传统的模型解释方法仅涵盖关于你模型的表面级问题。请注意，我们不得不求助于特定模型的全球解释方法来发现模型之所以正确，是因为错误的原因。
- en: Assumptions can derail any machine learning project since this is information
    that you suppose without evidence. Note that it is crucial to work closely with
    domain experts to inform decisions throughout the machine learning workflow, but
    sometimes they can also mislead you. Ensure you check for inconsistencies between
    the data and what you assume to be the truth about that data. Finding and correcting
    these problems is at the heart of what interpretability is about.
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设可能会使任何机器学习项目脱轨，因为这是你没有证据就假设的信息。请注意，与领域专家密切合作，在整个机器学习工作流程中做出决策至关重要，但有时他们也可能误导你。确保你检查数据与你对数据的假设之间的不一致性。发现和纠正这些问题是可解释性的核心。
- en: Many model classes, even white-box models, have issues with computing feature
    importance consistently and reliably.
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多模型类别，即使是白盒模型，在一致和可靠地计算特征重要性方面都有问题。
- en: Incorrect model tuning can lead to a model that performs well enough but is
    less interpretable. Note that a regularized model overfits less but is also more
    interpretable. We will cover methods to address this challenge in *Chapter 12*,
    *Monotonic Constraints and Model Tuning for Interpretability*. Feature selection
    and engineering can also have the same effect, which you can read about in *Chapter
    10*, *Feature Selection and Engineering for Interpretability*.
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型调优不当可能导致模型性能足够好，但可解释性较差。请注意，正则化模型过拟合较少，但可解释性也更强。我们将在第12章中介绍解决这一挑战的方法，即*单调约束和模型调优以提高可解释性*。特征选择和工程也可能产生相同的效果，你可以在第10章中阅读有关内容，即*特征选择和工程以提高可解释性*。
- en: There’s a trade-off between predictive performance and interpretability. And
    this trade-off extends to execution speed. For these reasons, this book primarily
    focuses on black-box models, which have the predictive performance we want and
    a reasonable execution speed but could use some help on the interpretability side.
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测性能与可解释性之间存在权衡。这种权衡还扩展到执行速度。因此，本书主要关注黑盒模型，它们具有我们想要的预测性能和合理的执行速度，但在可解释性方面可能需要一些帮助。
- en: If you learned about these challenges, then congratulations! Mission accomplished!
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你了解了这些挑战，那么恭喜你！任务完成！
- en: Summary
  id: totrans-482
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, we covered some traditional methods for interpretability
    and what their limitations are. We learned about **intrinsically interpretable
    models** and how to both use them and interpret them, for both regression and
    classification. We also studied the **performance versus interpretability trade-off**
    and some models that attempt not to compromise in this trade-off. We also discovered
    many practical interpretation challenges involving the roles of feature selection
    and engineering, hyperparameters, domain experts, and execution speed.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本章之后，我们了解了某些传统的可解释性方法及其局限性。我们学习了**内在可解释模型**以及如何使用和解释这些模型，无论是回归还是分类。我们还研究了**性能与可解释性之间的权衡**，以及一些试图不在这场权衡中妥协的模型。我们还发现了许多涉及特征选择和工程、超参数、领域专家和执行速度的实际解释挑战。
- en: In the next chapter, we will learn more about different interpretation methods
    to measure the effect of a feature on a model.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将学习更多关于不同解释方法来衡量特征对模型影响的内容。
- en: Dataset sources
  id: totrans-485
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集来源
- en: United States Department of Transportation Bureau of Transportation Statistics.
    (2018). Airline On-Time Performance Data. Originally retrieved from [https://www.transtats.bts.gov](https://www.transtats.bts.gov).
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 美国交通部运输统计局。（2018）。航空公司准时性能数据。最初从[https://www.transtats.bts.gov](https://www.transtats.bts.gov)检索。
- en: Further reading
  id: totrans-487
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: Friedman, J. and Popescu, B, 2008, *Predictive Learning via Rule Ensembles*.
    The Annals of Applied Statistics, 2(3), 916-954\. [http://doi.org/10.1214/07-AOAS148](http://doi.org/10.1214/07-AOAS148)
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Friedman, J. 和 Popescu, B, 2008, *通过规则集预测学习*. 应用统计年鉴，2(3)，916-954\. [http://doi.org/10.1214/07-AOAS148](http://doi.org/10.1214/07-AOAS148)
- en: 'Hastie, T., Tibshirani, R., and Wainwright, M., 2015, *Statistical Learning
    with Sparsity: The Lasso and Generalizations*. Chapman & Hall/Crc Monographs on
    Statistics & Applied Probability, Taylor & Francis'
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hastie, T., Tibshirani, R., 和 Wainwright, M., 2015, *稀疏性统计学习：Lasso 及其推广*. 奇普曼
    & 哈尔/统计学与应用概率系列专著，泰勒 & 弗朗西斯
- en: 'Thomas, D.R., Hughes, E., and Zumbo, B.D., 1998, *On Variable Importance in
    Linear Regression.* Social Indicators Research 45, 253–275: [https://doi.org/10.1023/A:1006954016433](https://doi.org/10.1023/A:1006954016433)'
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thomas, D.R., Hughes, E., 和 Zumbo, B.D., 1998, *关于线性回归中的变量重要性.* 社会指标研究 45，253–275:
    [https://doi.org/10.1023/A:1006954016433](https://doi.org/10.1023/A:1006954016433)'
- en: 'Nori, H., Jenkins, S., Koch, P., and Caruana, R., 2019, *InterpretML: A Unified
    Framework for Machine Learning Interpretability*. arXiv preprint: [https://arxiv.org/pdf/1909.09223.pdf](https://arxiv.org/pdf/1909.09223.pdf)'
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nori, H., Jenkins, S., Koch, P., 和 Caruana, R., 2019, *InterpretML：机器学习可解释性的统一框架*.
    arXiv 预印本：[https://arxiv.org/pdf/1909.09223.pdf](https://arxiv.org/pdf/1909.09223.pdf)
- en: 'Hastie, T., and Tibshirani, R., 1987, *Generalized Additive Models: Some Applications.*
    Journal of the American Statistical Association, 82(398):371–386: [http://doi.org/10.2307%2F2289439](http://doi.org/10.2307%2F2289439)'
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Hastie, T. 和 Tibshirani, R., 1987, *广义加性模型：一些应用.* 美国统计学会会刊，82(398)：371–386:
    [http://doi.org/10.2307%2F2289439](http://doi.org/10.2307%2F2289439)'
- en: Learn more on Discord
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的 Discord 社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_3.xhtml)'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_3.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code107161072033138125.png)'
