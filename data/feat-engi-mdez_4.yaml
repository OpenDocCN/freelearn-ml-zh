- en: Feature Construction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征构建
- en: In the previous chapter, we worked with the `Pima Indian Diabetes Prediction`
    dataset to get a better understanding of which given features in our dataset are
    most valuable. Working with the features that were available to us, we identified
    missing values within our columns and employed techniques of dropping missing
    values, imputing, and normalizing/standardizing our data to improve the accuracy
    of our machine learning model.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用 `Pima Indian Diabetes Prediction` 数据集来更好地了解我们数据集中哪些给定的特征最有价值。使用我们可用的特征，我们识别了列中的缺失值，并采用了删除缺失值、填充以及归一化/标准化数据的技术，以提高我们机器学习模型的准确性。
- en: It is important to note that, up to this point, we have only worked with features
    that are quantitative. We will now shift into dealing with categorical data, in
    addition to the quantitative data that has missing values. Our main focus will
    be to work with our given features to construct entirely new features for our
    models to learn from.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，到目前为止，我们只处理了定量特征。现在，我们将转向处理除了具有缺失值的定量数据之外，还要处理分类数据。我们的主要焦点将是使用给定的特征来构建模型可以从中学习的新特征。
- en: There are various methods we can utilize to construct our features, with the
    most basic starting with the pandas library in Python to scale an existing feature
    by a multiples. We will be diving into some more mathematically intensive methods,
    and will employ various packages available to us through the scikit-learn library;
    we will also create our own custom classes. We will go over these classes in detail
    as we get into the code.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用各种方法来构建我们的特征，最基本的方法是从 Python 中的 pandas 库开始，通过乘数来缩放现有特征。我们将深入研究一些更数学密集的方法，并使用通过
    scikit-learn 库提供的各种包；我们还将创建自己的自定义类。随着我们进入代码，我们将详细介绍这些类。
- en: 'We will be covering the following topics in our discussions:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在我们的讨论中涵盖以下主题：
- en: Examining our dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查我们的数据集
- en: Imputing categorical features
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填充分类特征
- en: Encoding categorical variables
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码分类变量
- en: Extending numerical features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展数值特征
- en: Text-specific feature construction
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本特定特征构建
- en: Examining our dataset
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检查我们的数据集
- en: For demonstrative purposes, in this chapter, we will utilize a dataset that
    we have created, so that we can showcase a variety of data levels and types. Let's
    set up our DataFrame and dive into our data.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，在本章中，我们将使用我们创建的数据集，这样我们就可以展示各种数据级别和类型。让我们设置我们的 DataFrame 并深入了解我们的数据。
- en: 'We will use pandas to create the DataFrame we will work with, as this is the
    primary data structure in pandas. The advantage of a pandas DataFrame is that
    there are several attributes and methods available for us to perform on our data.
    This allows us to logically manipulate the data to develop a thorough understanding
    of what we are working with, and how best to structure our machine learning models:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 pandas 创建我们将要工作的 DataFrame，因为这是 pandas 中的主要数据结构。pandas DataFrame 的优势在于，我们有几个属性和方法可用于对数据进行操作。这使我们能够逻辑地操作数据，以全面了解我们正在处理的内容，以及如何最好地构建我们的机器学习模型：
- en: 'First, let''s import `pandas`:'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入 `pandas`：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, we can set up our `DataFrame X`. To do this, we will utilize the `DataFrame`
    method in pandas, which creates a tabular data structure (table with rows and
    columns). This method can take in a few types of data (NumPy arrays or dictionaries,
    to name a couple). Here, we will be passing-in a dictionary with keys as column
    headers and values as lists, with each list representing a column:'
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以设置我们的 `DataFrame X`。为此，我们将利用 pandas 中的 `DataFrame` 方法，该方法创建一个表格数据结构（具有行和列的表格）。此方法可以接受几种类型的数据（例如
    NumPy 数组或字典）。在这里，我们将传递一个字典，其键为列标题，值为列表，每个列表代表一列：
- en: '[PRE1]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This will give us a DataFrame with four columns and six rows. Let''s print
    our DataFrame `X` and take a look at the data:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这将给我们一个具有四列和六行的 DataFrame。让我们打印我们的 DataFrame `X` 并查看数据：
- en: '[PRE2]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We get the output as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下输出：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | **布尔值** | **城市** | **有序列** | **定量列** |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 是 | 东京 | 有点像 | 1.0 |'
- en: '| **1** | no | None | like | 11.0 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 否 | 无 | 喜欢的 | 11.0 |'
- en: '| **2** | None | london | somewhat like | -0.5 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 无 | 伦敦 | 有点像 | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 否 | 西雅图 | 喜欢的 | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 否 | 旧金山 | 有点像 | NaN |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 是 | 东京 | 不喜欢的 | 20.0 |'
- en: 'Let''s take a look at our columns and identify our data levels and types:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下我们的列，并确定我们的数据级别和类型：
- en: '`boolean`: This column is represented by binary categorical data (yes/no),
    and is at the nominal level'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`布尔`：这个列由二元分类数据（是/否）表示，处于名称级别'
- en: '`city`: This column is represented by categorical data, also at the nominal
    level'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`城市`：这个列由分类数据表示，也处于名称级别'
- en: '`ordinal_column`: As you may have guessed by the column name, this column is
    represented by ordinal data, at the ordinal level'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`序号列`：正如你可能从列名猜到的，这个列由序数数据表示，处于序数级别'
- en: '`quantitative_column`: This column is represented by integers at the ratio
    level'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`定量列`：这个列由整数在比例级别表示'
- en: Imputing categorical features
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 填充分类特征
- en: 'Now that we have an understanding of the data we are working with, let''s take
    a look at our missing values:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了我们正在处理的数据，让我们看看我们的缺失值：
- en: To do this, we can use the `isnull` method available to us in pandas for DataFrames.
    This method returns a `boolean` same-sized object indicating if the values are
    null.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以使用pandas为我们提供的`isnull`方法。此方法返回一个与值大小相同的`boolean`对象，指示值是否为空。
- en: 'We will then `sum` these to see which columns have missing data:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将`sum`这些值以查看哪些列有缺失数据：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we can see that three of our columns are missing values. Our course of
    action will be to impute these missing values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到我们有三列数据缺失。我们的行动方案将是填充这些缺失值。
- en: If you recall, we implemented scikit-learn's `Imputer` class in a previous chapter
    to fill in numerical data. `Imputer` does have a categorical option, `most_frequent`,
    however it only works on categorical data that has been encoded as integers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得，我们在上一章中实现了scikit-learn的`Imputer`类来填充数值数据。`Imputer`确实有一个分类选项，`most_frequent`，然而它只适用于已经被编码为整数的分类数据。
- en: We may not always want to transform our categorical data this way, as it can
    change how we interpret the categorical information, so we will build our own
    transformer. By transformer, we mean a method by which a column will impute missing
    values.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能并不总是想以这种方式转换我们的分类数据，因为它可能会改变我们解释分类信息的方式，因此我们将构建自己的转换器。在这里，我们所说的转换器是指一种方法，通过这种方法，列将填充缺失值。
- en: In fact, we will build several custom transformers in this chapter, as they
    are quite useful for making transformations to our data, and give us options that
    are not readily available in pandas or scikit-learn.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，在本章中，我们将构建几个自定义转换器，因为它们对于对我们的数据进行转换非常有用，并为我们提供了在pandas或scikit-learn中不可轻易获得的选择。
- en: Let's start with our categorical column, `city`. Just as we have the strategy
    of imputing the mean value to fill missing rows for numerical data, we have a
    similar method for categorical data. To impute values for categorical data, fill
    missing rows with the most common category.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从我们的分类列`城市`开始。正如我们用均值填充缺失行来填充数值数据，我们也有一个类似的方法用于分类数据。为了填充分类数据的值，用最常见的类别填充缺失行。
- en: 'To do so, we will need to find out what the most common category is in our
    `city` column:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 要这样做，我们需要找出`城市`列中最常见的类别：
- en: Note that we need to specify the column we are working with to employ a method
    called `value_counts`. This will return an object that will be in descending order
    so that the first element is the most frequently-occurring element.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要指定我们正在处理的列来应用一个名为`value_counts`的方法。这将返回一个按降序排列的对象，因此第一个元素是最频繁出现的元素。
- en: 'We will grab only the first element in the object:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将只获取对象中的第一个元素：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can see that `tokyo` appears to be the most common city. Now that we know
    which value to use to impute our missing rows, let''s fill these slots. There
    is a `fillna` function that allows us to specify exactly how we want to fill missing
    values:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到`东京`似乎是最常见的城市。现在我们知道要使用哪个值来填充我们的缺失行，让我们填充这些空位。有一个`fillna`函数允许我们指定我们想要如何填充缺失值：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `city` column now looks like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`城市`列现在看起来是这样的：'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Great, now our `city` column no longer has missing values. However, our other
    categorical column, `boolean`, still does. Rather than going through the same
    method, let's build a custom imputer that will be able to handle imputing all
    categorical data.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了，现在我们的`城市`列不再有缺失值。然而，我们的其他分类列`布尔`仍然有。与其重复同样的方法，让我们构建一个能够处理所有分类数据填充的自定义填充器。
- en: Custom imputers
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义填充器
- en: 'Before we jump into the code, let''s have a quick refresher of pipelines:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入代码之前，让我们快速回顾一下管道：
- en: Pipelines allow us to sequentially apply a list of transforms and a final estimator
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道允许我们按顺序应用一系列转换和一个最终估计器
- en: Intermediate steps of the pipeline must be **transforms**, meaning they must
    implement `fit` and `transform` methods
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道的中间步骤必须是 **转换器**，这意味着它们必须实现 `fit` 和 `transform` 方法
- en: The final estimator only needs to implement `fit`
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终估计器只需要实现 `fit`
- en: The purpose of the pipeline is to assemble several steps that can be cross-validated
    together while setting different parameters. Once we have built our custom transformers
    for each column that needs imputing, we will pass them all through a pipeline
    so that our data can be transformed in one go. Let's build our custom category
    imputer to start.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 管道的目的是组装几个可以一起交叉验证的步骤，同时设置不同的参数。一旦我们为需要填充的每一列构建了自定义的转换器，我们将它们全部通过管道传递，以便我们的数据可以一次性进行转换。让我们从构建自定义类别填充器开始。
- en: Custom category imputer
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义类别填充器
- en: First, we will utilize the scikit-learn `TransformerMixin` base class to create
    our own custom categorical imputer. This transformer (and all other custom transformers
    in this chapter) will work as an element in a pipeline with a fit and `transform`
    method.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将利用 scikit-learn 的 `TransformerMixin` 基类来创建我们自己的自定义类别填充器。这个转换器（以及本章中的所有其他自定义转换器）将作为一个具有
    `fit` 和 `transform` 方法的管道元素工作。
- en: 'The following code block will become very familiar throughout this chapter,
    so we will go over each line in detail:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块将在本章中变得非常熟悉，因此我们将逐行详细讲解：
- en: '[PRE7]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'There is a lot happening in this code block, so let''s break it down by line:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个代码块中发生了很多事情，所以让我们逐行分解：
- en: 'First, we have a new `import` statement:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们有一个新的 `import` 语句：
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will inherit the `TransformerMixin` class from scikit-learn, which includes
    a `.fit_transform` method that calls upon the `.fit` and `.transform` methods
    we will create. This allows us to maintain a similar structure in our transformer
    to that of scikit-learn. Let''s initialize our custom class:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从 scikit-learn 继承 `TransformerMixin` 类，它包括一个 `.fit_transform` 方法，该方法调用我们将创建的
    `.fit` 和 `.transform` 方法。这允许我们在转换器中保持与 scikit-learn 相似的结构。让我们初始化我们的自定义类：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We have now instantiated our custom class and have our `__init__` method that
    initializes our attributes. In our case, we only need to initialize one instance
    attribute, `self.cols` (which will be the columns that we specify as a parameter).
    Now, we can build our `fit` and `transform` methods:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经实例化了我们的自定义类，并有了我们的 `__init__` 方法，该方法初始化我们的属性。在我们的情况下，我们只需要初始化一个实例属性 `self.cols`（它将是我们指定的参数中的列）。现在，我们可以构建我们的
    `fit` 和 `transform` 方法：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Here, we have our `transform` method. It takes in a DataFrame, and the first
    step is to copy and rename the DataFrame to `X`. Then, we will iterate over the
    columns we have specified in our `cols` parameter to fill in the missing slots.
    The `fillna` portion may feel familiar, as it is the function we employed in our
    first example. We are using the same function and setting it up so that our custom
    categorical imputer can work across several columns at once. After the missing
    values have been filled, we return our filled DataFrame. Next comes our `fit`
    method:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这里，我们有我们的 `transform` 方法。它接受一个 DataFrame，第一步是复制并重命名 DataFrame 为 `X`。然后，我们将遍历我们在
    `cols` 参数中指定的列来填充缺失的槽位。`fillna` 部分可能感觉熟悉，因为我们已经在第一个例子中使用了这个函数。我们正在使用相同的函数，并设置它，以便我们的自定义类别填充器可以一次跨多个列工作。在填充了缺失值之后，我们返回填充后的
    DataFrame。接下来是我们的 `fit` 方法：
- en: '[PRE11]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We have set up our `fit` method to simply `return self`, as is the standard
    of `.fit` methods in scikit-learn.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经设置了我们的 `fit` 方法简单地 `return self`，这是 scikit-learn 中 `.fit` 方法的标准。
- en: 'Now we have a custom method that allows us to impute our categorical data!
    Let''s see it in action with our two categorical columns, `city` and `boolean`:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们有一个自定义方法，允许我们填充我们的类别数据！让我们通过我们的两个类别列 `city` 和 `boolean` 来看看它的实际效果：
- en: '[PRE12]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We have initialized our custom categorical imputer, and we now need to `fit_transform`
    this imputer to our dataset:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们已经初始化了我们的自定义类别填充器，现在我们需要将这个填充器 `fit_transform` 到我们的数据集中：
- en: '[PRE13]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our dataset now looks like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集现在看起来像这样：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | somewhat like | 1.0 |'
- en: '| **1** | no | tokyo | like | 11.0 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | tokyo | like | 11.0 |'
- en: '| **2** | no | london | somewhat like | -0.5 |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **2** | no | london | somewhat like | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | like | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | somewhat like | NaN |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | dislike | 20.0 |'
- en: Great! Our `city` and `boolean` columns are no longer missing values. However,
    our quantitative column still has null values. Since the default imputer cannot
    select columns, let's make another custom one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们的`city`和`boolean`列不再有缺失值。然而，我们的定量列仍然有null值。由于默认的填充器不能选择列，让我们再做一个自定义的。
- en: Custom quantitative imputer
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自定义定量填充器
- en: 'We will use the same structure as our custom category imputer. The main difference
    here is that we will utilize scikit-learn''s `Imputer` class to actually make
    the transformation on our columns:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与我们的自定义分类填充器相同的结构。这里的主要区别是我们将利用scikit-learn的`Imputer`类来实际上在我们的列上执行转换：
- en: '[PRE14]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: For our `CustomQuantitativeImputer`, we have added a `strategy` parameter that
    will allow us to specify exactly how we want to impute missing values for our
    quantitative data. Here, we have selected the `mean` to replace missing values
    and still employ the `transform` and `fit` methods.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的`CustomQuantitativeImputer`，我们增加了一个`strategy`参数，这将允许我们指定我们想要如何为我们的定量数据填充缺失值。在这里，我们选择了`mean`来替换缺失值，并且仍然使用`transform`和`fit`方法。
- en: 'Once again, in order to impute our data, we will call the `fit_transform` method,
    this time specifying both the column and the `strategy` to use to impute:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了填充我们的数据，我们将调用`fit_transform`方法，这次指定了列和用于填充的`strategy`：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Alternatively, rather than calling and `fit_transforming` our `CustomCategoryImputer`
    and our `CustomQuantitativeImputer` separately, we can also set them up in a pipeline
    so that we can transform our dataset in one go. Let''s see how:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，而不是分别调用和`fit_transform`我们的`CustomCategoryImputer`和`CustomQuantitativeImputer`，我们也可以将它们设置在一个pipeline中，这样我们就可以一次性转换我们的dataset。让我们看看如何：
- en: 'Start with our `import` statement:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从我们的`import`语句开始：
- en: '[PRE16]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, we can pass through our custom imputers:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以传递我们的自定义填充器：
- en: '[PRE17]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let''s see what our dataset looks like after our pipeline transformations:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的dataset在pipeline转换后看起来像什么：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | somewhat like | 1.0 |'
- en: '| **1** | no | tokyo | like | 11.0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | tokyo | like | 11.0 |'
- en: '| **2** | no | london | somewhat like | -0.5 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| **2** | no | london | somewhat like | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | like | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | 8.3 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | somewhat like | 8.3 |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | dislike | 20.0 |'
- en: Now we have a dataset with no missing values to work with!
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个没有缺失值的dataset可以工作了！
- en: Encoding categorical variables
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 编码分类变量
- en: To recap, thus far we have successfully imputed our dataset—both our categorical
    and quantitative columns. At this point, you may be wondering, *how do we utilize
    the categorical data with a machine learning algorithm?*
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，到目前为止，我们已经成功填充了我们的dataset——包括我们的分类和定量列。在这个时候，你可能想知道，*我们如何利用分类数据与机器学习算法结合使用？*
- en: Simply put, we need to transform this categorical data into numerical data.
    So far, we have ensured that the most common category was used to fill the missing
    values. Now that this is done, we need to take it a step further.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，我们需要将这个分类数据转换为数值数据。到目前为止，我们已经确保使用最常见的类别来填充缺失值。现在这件事已经完成，我们需要更进一步。
- en: Any machine learning algorithm, whether it is a linear-regression or a KNN-utilizing
    Euclidean distance, requires numerical input features to learn from. There are
    several methods we can rely on to transform our categorical data into numerical
    data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习算法，无论是线性回归还是使用欧几里得距离的KNN，都需要数值输入特征来学习。我们可以依赖几种方法将我们的分类数据转换为数值数据。
- en: Encoding at the nominal level
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 名义级别的编码
- en: 'Let''s begin with data at the nominal level. The main method we have is to
    transform our categorical data into dummy variables. We have two options to do
    this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从名义级别的数据开始。我们主要的方法是将我们的分类数据转换为虚拟变量。我们有两种方法来做这件事：
- en: Utilize pandas to automatically find the categorical variables and dummy code
    them
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用pandas自动找到分类变量并将它们转换为虚拟变量
- en: Create our own custom transformer using dummy variables to work in a pipeline
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用虚拟变量创建我们自己的自定义转换器以在pipeline中工作
- en: Before we delve into these options, let's go over exactly what dummy variables
    are.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨这些选项之前，让我们先了解一下虚拟变量究竟是什么。
- en: Dummy variables take the value zero or one to indicate the absence or presence
    of a category. They are proxy variables, or numerical stand-ins, for qualitative
    data.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟变量取值为零或一，以表示类别的缺失或存在。它们是代理变量，或数值替代变量，用于定性数据。
- en: Consider a simple regression analysis for wage determination. Say we are given
    gender, which is qualitative, and years of education, which is quantitative. In
    order to see if gender has an effect on wages, we would dummy code when the person
    is a female to female = 1, and female = 0 when the person is male.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个简单的回归分析来确定工资。比如说，我们被给出了性别，这是一个定性变量，以及教育年限，这是一个定量变量。为了看看性别是否对工资有影响，我们会在女性时将虚拟编码为女性
    = 1，在男性时将女性编码为0。
- en: When working with dummy variables, it is important to be aware of and avoid
    the dummy variable trap. The dummy variable trap is when you have independent
    variables that are multicollinear, or highly correlated. Simply put, these variables
    can be predicted from each other. So, in our gender example, the dummy variable
    trap would be if we include both female as (0|1) and male as (0|1), essentially
    creating a duplicate category. It can be inferred that a 0 female value indicates
    a male.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用虚拟变量时，重要的是要意识到并避免虚拟变量陷阱。虚拟变量陷阱是指你拥有独立的变量是多线性的，或者高度相关的。简单来说，这些变量可以从彼此预测。所以，在我们的性别例子中，虚拟变量陷阱就是如果我们同时包含女性作为（0|1）和男性作为（0|1），实际上创建了一个重复的分类。可以推断出0个女性值表示男性。
- en: To avoid the dummy variable trap, simply leave out the constant term or one
    of the dummy categories. The left out dummy can become the base category to which
    the rest are compared to.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免虚拟变量陷阱，只需省略常数项或其中一个虚拟类别。省略的虚拟变量可以成为与其他变量比较的基础类别。
- en: 'Let''s come back to our dataset and employ some methods to encode our categorical
    data into dummy variables. pandas has a handy `get_dummies` method that actually
    finds all of the categorical variables and dummy codes them for us:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的数据集，并采用一些方法将我们的分类数据编码为虚拟变量。pandas有一个方便的`get_dummies`方法，实际上它会找到所有的分类变量，并为我们进行虚拟编码：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We have to be sure to specify which columns we want to apply this to because
    it will also dummy code the ordinal columns, and this wouldn't make much sense.
    We will take a more in-depth look into why dummy coding ordinal data doesn't makes
    sense shortly.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须确保指定我们想要应用到的列，因为它也会对序数列进行虚拟编码，这不会很有意义。我们将在稍后更深入地探讨为什么对序数数据进行虚拟编码没有意义。
- en: 'Our data, with our dummy coded columns, now looks like this:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据，加上我们的虚拟编码列，现在看起来是这样的：
- en: '|  | **ordinal_column** | **quantitative_column** | **city__london** | **city_san
    francisco** | **city_seattle** | **city_tokyo** | **boolean_no** | **boolean_yes**
    |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  | **ordinal_column** | **quantitative_column** | **city__london** | **city_san
    francisco** | **city_seattle** | **city_tokyo** | **boolean_no** | **boolean_yes**
    |'
- en: '| **0** | somewhat like | 1.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| **0** | somewhat like | 1.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
- en: '| **1** | like | 11.0 | 0 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| **1** | like | 11.0 | 0 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **2** | somewhat like | -0.5 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| **2** | somewhat like | -0.5 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| **3** | like | 10.0 | 0 | 0 | 1 | 0 | 1 | 0 |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| **3** | like | 10.0 | 0 | 0 | 1 | 0 | 1 | 0 |'
- en: '| **4** | somewhat like | NaN | 0 | 1 | 0 | 0 | 1 | 0 |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| **4** | somewhat like | NaN | 0 | 1 | 0 | 0 | 1 | 0 |'
- en: '| **5** | dislike | 20.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| **5** | dislike | 20.0 | 0 | 0 | 0 | 1 | 0 | 1 |'
- en: Our other option for dummy coding our data is to create our own custom dummifier.
    Creating this allows us to set up a pipeline to transform our whole dataset in
    one go.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对数据进行虚拟编码的另一种选择是创建自己的自定义虚拟化器。创建这个虚拟化器允许我们设置一个管道，一次将整个数据集转换。
- en: 'Once again, we will use the same structure as our previous two custom imputers.
    Here, our `transform` method will use the handy pandas `get_dummies` method to
    create dummy variables for specified columns. The only parameter we have in this
    custom dummifier is `cols`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们将使用与之前两个自定义填充器相同的结构。在这里，我们的`transform`方法将使用方便的pandas `get_dummies`方法为指定的列创建虚拟变量。在这个自定义虚拟化器中，我们唯一的参数是`cols`：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Our custom dummifier mimics scikit-learn's `OneHotEncoding`, but with the added
    advantage of working on our entire DataFrame.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的定制虚拟化器模仿scikit-learn的`OneHotEncoding`，但具有在完整DataFrame上工作的附加优势。
- en: Encoding at the ordinal level
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对序数级别的编码
- en: Now, let's take a look at our ordinal columns. There is still useful information
    here, however, we need to transform the strings into numerical data. At the ordinal
    level, since there is meaning in the data having a specific order, it does not
    make sense to use dummy variables. To maintain the order, we will use a label
    encoder.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们的有序列。这里仍然有一些有用的信息，然而，我们需要将字符串转换为数值数据。在有序级别，由于数据具有特定的顺序，因此使用虚拟变量是没有意义的。为了保持顺序，我们将使用标签编码器。
- en: By a label encoder, we mean that each label in our ordinal data will have a
    numerical value associated to it. In our example, this means that the ordinal
    column values (`dislike`, `somewhat like`, and `like`) will be represented as
    `0`, `1`, and `2`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 通过标签编码器，我们指的是在我们的有序数据中，每个标签都将与一个数值相关联。在我们的例子中，这意味着有序列值（`dislike`、`somewhat like`和`like`）将被表示为`0`、`1`和`2`。
- en: 'In the simplest form, the code is as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 以最简单的方式，代码如下所示：
- en: '[PRE20]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Here, we have set up a list for ordering our labels. This is key, as we will
    be utilizing the index of our list to transform the labels to numerical data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们设置了一个列表来排序我们的标签。这是关键，因为我们将利用列表的索引来将标签转换为数值数据。
- en: 'Here, we will implement a function called `map` on our column, that allows
    us to specify the function we want to implement on the column. We specify this
    function using a construct called `lambda`, which essentially allows us to create
    an anonymous function, or one that is not bound to a name:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将在我们的列上实现一个名为`map`的函数，它允许我们指定我们想要在列上实现的函数。我们使用一个称为`lambda`的结构来指定这个函数，它本质上允许我们创建一个匿名函数，或者一个没有绑定到名称的函数：
- en: '[PRE21]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This specific code is creating a function that will apply the index of our
    list called `ordering` to each element. Now, we map this to our ordinal column:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这段特定的代码创建了一个函数，该函数将我们的列表`ordering`的索引应用于每个元素。现在，我们将此映射到我们的有序列：
- en: '[PRE22]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Our ordinal column is now represented as labeled data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的有序列现在被表示为标记数据。
- en: Note that scikit-learn has a `LabelEncoder`, but we are not using this method
    because it does not include the ability to order categories (`0` for dislike,
    `1` for somewhat like, `2` for like) as we have done previously. Rather, the default
    is a sorting method, which is not what we want to use here.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，scikit-learn 有一个`LabelEncoder`，但我们没有使用这种方法，因为它不包括排序类别的能力（`0`表示不喜欢，`1`表示有点喜欢，`2`表示喜欢），正如我们之前所做的那样。相反，默认是排序方法，这不是我们在这里想要使用的。
- en: 'Once again, let us make a custom label encoder that will fit into our pipeline:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们创建一个自定义标签编码器，使其适合我们的管道：
- en: '[PRE23]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We have maintained the structure of the other custom transformers in this chapter.
    Here, we have utilized the `map` and `lambda` functions detailed previously to
    transform the specified columns. Note the key parameter, `ordering`, which will
    determine which numerical values the labels will be encoding into.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中维护了其他自定义转换器的结构。在这里，我们使用了前面详细说明的`map`和`lambda`函数来转换指定的列。注意关键参数`ordering`，它将确定标签将编码成哪些数值。
- en: 'Let''s call our custom encoder:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们称我们的自定义编码器为：
- en: '[PRE24]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Our dataset after these transformations looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些转换后，我们的数据集看起来如下所示：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  | **布尔值** | **城市** | **有序列** | **数量列** |'
- en: '| **0** | yes | tokyo | 1 | 1.0 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | 1 | 1.0 |'
- en: '| **1** | no | None | 2 | 11.0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | None | 2 | 11.0 |'
- en: '| **2** | None | london | 1 | -0.5 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| **2** | None | london | 1 | -0.5 |'
- en: '| **3** | no | seattle | 2 | 10.0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | 2 | 10.0 |'
- en: '| **4** | no | san francisco | 1 | NaN |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | 1 | NaN |'
- en: '| **5** | yes | tokyo | 0 | 20.0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | 0 | 20.0 |'
- en: Our ordinal column is now labeled.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的有序列现在被标记。
- en: 'Up to this point, we have transformed the following columns accordingly:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经相应地转换了以下列：
- en: '`boolean`, `city`: dummy encoding'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`布尔值`、`城市`: 虚拟编码'
- en: '`ordinal_column`: label encoding'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`有序列`: 标签编码'
- en: Bucketing continuous features into categories
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将连续特征分桶到类别中
- en: Sometimes, when you have continuous numerical data, it may make sense to transform
    a continuous variable into a categorical variable. For example, say you have ages,
    but it would be more useful to work with age ranges.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，当你有连续数值数据时，将连续变量转换为分类变量可能是有意义的。例如，假设你有年龄，但使用年龄范围可能更有用。
- en: pandas has a useful function called `cut` that will bin your data for you. By
    binning, we mean it will create the ranges for your data.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: pandas 有一个有用的函数`cut`，可以为你对数据进行分箱。通过分箱，我们指的是它将为你的数据创建范围。
- en: 'Let''s see how this function could work on our `quantitative_column`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个函数如何在我们的`quantitative_column`上工作：
- en: '[PRE25]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output of the `cut` function for our quantitative column looks like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`cut`函数对我们定量列的输出看起来是这样的：'
- en: '[PRE26]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: When we specify `bins` to be an integer (`bins = 3`), it defines the number
    of equal–width bins in the range of `X`. However, in this case, the range of `X`
    is extended by .1% on each side to include the min or max values of `X`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们指定`bins`为整数（`bins = 3`）时，它定义了`X`范围内的等宽`bins`的数量。然而，在这种情况下，`X`的范围在每边扩展了0.1%，以包括`X`的最小值或最大值。
- en: 'We can also set labels to `False`, which will return only integer indicators
    of the `bins`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以将`labels`设置为`False`，这将只返回`bins`的整数指标：
- en: '[PRE27]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is what the integer indicators look like for our `quantitative_column`:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们`quantitative_column`的整数指标看起来是这样的：
- en: '[PRE28]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Seeing our options with the `cut` function, we can also build our own `CustomCutter`
    for our pipeline. Once again, we will mimic the structure of our transformers.
    Our `transform` method will use the `cut` function, and so we will need to set
    `bins` and `labels` as parameters:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cut`函数查看我们的选项，我们还可以为我们的管道构建自己的`CustomCutter`。再次，我们将模仿我们的转换器的结构。我们的`transform`方法将使用`cut`函数，因此我们需要将`bins`和`labels`作为参数设置：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Note that we have set the default labels parameter to `False`. Initialize our
    `CustomCutter`, specifying the column to transform and the number of bins to use:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们已经将默认的`labels`参数设置为`False`。初始化我们的`CustomCutter`，指定要转换的列和要使用的`bins`数量：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'With our `CustomCutter` transforming our `quantitative_column`, our data now
    looks like this:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的`CustomCutter`转换`quantitative_column`，我们的数据现在看起来是这样的：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | somewhat like | 1.0 |'
- en: '| **1** | no | None | like | 11.0 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | None | like | 11.0 |'
- en: '| **2** | None | london | somewhat like | -0.5 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| **2** | None | london | somewhat like | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | like | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | somewhat like | NaN |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | dislike | 20.0 |'
- en: Note that our `quantitative_column` is now ordinal, and so there is no need
    to dummify the data.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的`quantitative_column`现在是序数，因此不需要对数据进行空编码。
- en: Creating our pipeline
  id: totrans-186
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建我们的管道
- en: 'To review, we have transformed the columns in our dataset in the following
    ways thus far:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾，到目前为止，我们已经以以下方式转换了数据集中的列：
- en: '`boolean, city`: dummy encoding'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`boolean, city`: 空编码'
- en: '`ordinal_column`: label encoding'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ordinal_column`: 标签编码'
- en: '`quantitative_column`: ordinal level data'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`quantitative_column`: 序数级别数据'
- en: Since we now have transformations for all of our columns, let's put everything
    together in a pipeline.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在已经为所有列设置了转换，让我们将所有内容组合到一个管道中。
- en: 'Start with importing our `Pipeline` class from scikit-learn:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 从导入我们的`Pipeline`类开始，来自scikit-learn：
- en: '[PRE31]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will bring together each of the custom transformers that we have created.
    Here is the order we will follow in our pipeline:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将汇集我们创建的每个自定义转换器。以下是我们在管道中遵循的顺序：
- en: First, we will utilize the `imputer` to fill in missing values
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将使用`imputer`来填充缺失值
- en: Next, we will dummify our categorical columns
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将对分类列进行空编码
- en: Then, we will encode the `ordinal_column`
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将对`ordinal_column`进行编码
- en: Finally, we will bucket the `quantitative_column`
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将对`quantitative_column`进行分桶
- en: 'Let''s set up our pipeline as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下方式设置我们的管道：
- en: '[PRE32]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'In order to see the full transformation of our data using our pipeline, let''s
    take a look at our data with zero transformations:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了查看我们使用管道对数据进行完整转换的样子，让我们看看零转换的数据：
- en: '[PRE33]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This is what our data looked like in the beginning before any transformations
    were made:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们数据在开始时，在执行任何转换之前的样子：
- en: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  | **boolean** | **city** | **ordinal_column** | **quantitative_column**
    |'
- en: '| **0** | yes | tokyo | somewhat like | 1.0 |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| **0** | yes | tokyo | somewhat like | 1.0 |'
- en: '| **1** | no | None | like | 11.0 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| **1** | no | None | like | 11.0 |'
- en: '| **2** | None | london | somewhat like | -0.5 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| **2** | None | london | somewhat like | -0.5 |'
- en: '| **3** | no | seattle | like | 10.0 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| **3** | no | seattle | like | 10.0 |'
- en: '| **4** | no | san francisco | somewhat like | NaN |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| **4** | no | san francisco | somewhat like | NaN |'
- en: '| **5** | yes | tokyo | dislike | 20.0 |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **5** | yes | tokyo | dislike | 20.0 |'
- en: 'We can now `fit` our pipeline:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以`fit`我们的管道：
- en: '[PRE34]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We have created our pipeline object, let''s transform our DataFrame:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了管道对象，让我们转换我们的DataFrame：
- en: '[PRE35]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Here is what our final dataset looks like after undergoing all of the appropriate
    transformations by column:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有适当的列变换之后，我们的最终数据集看起来是这样的：
- en: '|  | **ordinal_column** | **quantitative_column** | **boolean_no** | **boolean_yes**
    | **city_london** | **city_san francisco** | **city_seattle** | **city_tokyo**
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|  | **ordinal_column** | **quantitative_column** | **boolean_no** | **boolean_yes**
    | **city_london** | **city_san francisco** | **city_seattle** | **city_tokyo**
    |'
- en: '| **0** | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 1 |'
- en: '| **1** | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 2 | 1 | 1 | 0 | 0 | 0 | 0 | 1 |'
- en: '| **2** | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 1 | 0 | 1 | 0 | 1 | 0 | 0 | 0 |'
- en: '| **3** | 2 | 1 | 1 | 0 | 0 | 0 | 1 | 0 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 2 | 1 | 1 | 0 | 0 | 0 | 1 | 0 |'
- en: '| **4** | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 1 | 1 | 1 | 0 | 0 | 1 | 0 | 0 |'
- en: '| **5** | 0 | 2 | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| **5** | 0 | 2 | 0 | 1 | 0 | 0 | 0 | 1 |'
- en: Extending numerical features
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 扩展数值特征
- en: Numerical features can undergo various methods to create extended features from
    them. Previously, we saw how we can transform continuous numerical data into ordinal
    data. Now, we will dive into extending our numerical features further.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 数值特征可以通过各种方法进行扩展，从而创建出更丰富的特征。之前，我们看到了如何将连续数值数据转换为有序数据。现在，我们将进一步扩展我们的数值特征。
- en: Before we go any deeper into these methods, we will introduce a new dataset
    to work with.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨这些方法之前，我们将引入一个新的数据集进行操作。
- en: Activity recognition from the Single Chest-Mounted Accelerometer dataset
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从单个胸挂加速度计数据集进行活动识别
- en: This dataset collects data from a wearable accelerometer, mounted on the chest,
    collected from fifteen participants performing seven activities. The sampling
    frequency of the accelerometer is 52 Hz and the accelerometer data is uncalibrated.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集收集了十五名参与者进行七种活动的可穿戴加速度计上的数据。加速度计的采样频率为52 Hz，加速度计数据未经校准。
- en: 'The dataset is separated by participant and contains the following:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集按参与者分隔，包含以下内容：
- en: Sequential number
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序号
- en: x acceleration
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: x加速度
- en: y acceleration
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y加速度
- en: z acceleration
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: z加速度
- en: Label
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签
- en: 'Labels are codified by numbers and represent an activity, as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 标签用数字编码，代表一个活动，如下所示：
- en: Working at a computer
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在电脑上工作
- en: Standing up, walking, and going up/down stairs
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 站立、行走和上/下楼梯
- en: Standing
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 站立
- en: Walking
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行走
- en: Going up/down stairs
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上/下楼梯
- en: Walking and talking with someone
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与人边走边谈
- en: Talking while standing
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 站立时说话
- en: 'Further information on this dataset is available on the UCI *Machine Learning
    Repository* at:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于这个数据集的信息可以在UCI *机器学习仓库*上找到：
- en: '[https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer](https://archive.ics.uci.edu/ml/datasets/Activity+Recognition+from+Single+Chest-Mounted+Accelerometer)'
- en: 'Let''s take a look at our data. First, we need to load in our CSV file and
    set our column headers:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的数据。首先，我们需要加载我们的CSV文件并设置列标题：
- en: '[PRE36]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Now, let''s examine the first few rows with the `.head` method, which will
    default to the first five rows, unless we specify how many rows to show:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`.head`方法检查前几行，默认为前五行，除非我们指定要显示的行数：
- en: '[PRE37]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This shows us:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明：
- en: '|  | **index** | **x** | **y** | **z** | **activity** |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  | **index** | **x** | **y** | **z** | **activity** |'
- en: '| **0** | 0.0 | 1502 | 2215 | 2153 | 1 |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 0.0 | 1502 | 2215 | 2153 | 1 |'
- en: '| **1** | 1.0 | 1667 | 2072 | 2047 | 1 |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1.0 | 1667 | 2072 | 2047 | 1 |'
- en: '| **2** | 2.0 | 1611 | 1957 | 1906 | 1 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 2.0 | 1611 | 1957 | 1906 | 1 |'
- en: '| **3** | 3.0 | 1601 | 1939 | 1831 | 1 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 3.0 | 1601 | 1939 | 1831 | 1 |'
- en: '| **4** | 4.0 | 1643 | 1965 | 1879 | 1 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 4.0 | 1643 | 1965 | 1879 | 1 |'
- en: 'This dataset is meant to train models to recognize a user''s current physical
    activity given an accelerometer''s `x`, `y`, and `z` position on a device such
    as a smartphone. According to the website, the options for the `activity` column
    are:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集旨在训练模型，根据智能手机等设备上的加速度计的`x`、`y`和`z`位置来识别用户的当前身体活动。根据网站信息，`activity`列的选项如下：
- en: '**1**: Working at a computer'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1**: 在电脑上工作'
- en: '**2**: Standing Up and Going updown stairs'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2**: 站立并上/下楼梯'
- en: '**3**: Standing'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3**: 站立'
- en: '**4**: Walking'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4**: 走路'
- en: '**5**: Going UpDown Stairs'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5**: 上/下楼梯'
- en: '**6**: Walking and Talking with Someone'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6**: 与人边走边谈'
- en: '**7**: Talking while Standing'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**7**: 站立时说话'
- en: 'The `activity` column will be the target variable we will be trying to predict,
    using the other columns. Let''s determine the null accuracy to beat in our machine
    learning model. To do this, we will invoke the `value_counts` method with the
    `normalize` option set to `True` to give us the most commonly occurring activity
    as a percentage:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '`activity`列将是我们将尝试预测的目标变量，使用其他列。让我们确定我们的机器学习模型中要击败的零准确率。为此，我们将调用`value_counts`方法，并将`normalize`选项设置为`True`，以给出最常见的活动作为百分比：'
- en: '[PRE38]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The null accuracy to beat is 51.53%, meaning that if we guessed seven (talking
    while standing), then we would be right over half of the time. Now, let's do some
    machine learning! Let's step through, line by line, setting up our model.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 要击败的零准确率是51.53%，这意味着如果我们猜测七个（站立时说话），那么我们正确的时间会超过一半。现在，让我们来进行一些机器学习！让我们逐行进行，设置我们的模型。
- en: 'First, we have our `import` statements:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们有我们的`import`语句：
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'You may be familiar with these import statements from last chapter. Once again,
    we will be utilizing scikit-learn''s **K-Nearest Neighbors** (**KNN**) classification
    model. We will also use the grid search module that automatically finds the best
    combination of parameters for the KNN model that best fits our data with respect
    to cross-validated accuracy. Next, we create a feature matrix (`X`) and a response
    variable (`y`) for our predictive model:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能对上一章中的这些导入语句很熟悉。再一次，我们将利用scikit-learn的**K-Nearest Neighbors**（**KNN**）分类模型。我们还将使用网格搜索模块，该模块自动找到最适合我们数据的KNN模型的最佳参数组合。接下来，我们为我们的预测模型创建一个特征矩阵（`X`）和一个响应变量（`y`）：
- en: '[PRE40]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Once our `X` and `y` are set up, we can introduce the variables and instances
    we need to successfully run a grid search:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的`X`和`y`设置好，我们就可以引入我们成功运行网格搜索所需的变量和实例：
- en: '[PRE41]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Next, we will instantiate a KNN model and a grid search module and fit it to
    our feature matrix and response variable:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实例化一个KNN模型和一个网格搜索模块，并将其拟合到我们的特征矩阵和响应变量：
- en: '[PRE42]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can `print` the best accuracy and parameters that were used to learn
    from:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以`print`出最佳的准确率和用于学习的参数：
- en: '[PRE43]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Using five neighbors as its parameter, our KNN model was able to achieve a 72.07%
    accuracy, much better than our null accuracy of around 51.53%! Perhaps we can
    utilize another method to get our accuracy up even more.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 使用五个邻居作为其参数，我们的KNN模型能够达到72.07%的准确率，比我们大约51.53%的零准确率要好得多！也许我们可以利用另一种方法将我们的准确率进一步提高。
- en: Polynomial features
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多项式特征
- en: A key method of working with numerical data and creating more features is through
    scikit-learn's `PolynomialFeatures` class. In its simplest form, this constructor will
    create new columns that are products of existing columns to capture feature interactions.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数值数据并创建更多特征的关键方法是通过scikit-learn的`PolynomialFeatures`类。在其最简单的形式中，这个构造函数将创建新的列，这些列是现有列的乘积，以捕捉特征交互。
- en: 'More specifically, this class will generate a new feature matrix with all of
    the polynomial combinations of the features with a degree less than or equal to
    the specified degree. Meaning that, if your input sample is two-dimensional, like
    so: [a, b], then the degree-2 polynomial features are as follows: [1, a, b, a^2,
    ab, b^2].'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，这个类将生成一个新的特征矩阵，包含所有小于或等于指定度的特征的多项式组合。这意味着，如果你的输入样本是二维的，如下所示：[a, b]，那么二次多项式特征如下：[1,
    a, b, a^2, ab, b^2]。
- en: Parameters
  id: totrans-280
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参数
- en: 'When instantiating polynomial features, there are three parameters to keep
    in mind:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在实例化多项式特征时，有三个参数需要考虑：
- en: degree
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: degree
- en: '`interaction_only`'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`interaction_only`'
- en: '`include_bias`'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`include_bias`'
- en: Degree corresponds to the degree of the polynomial features, with the default
    set to two.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 度数对应于多项式特征的度数，默认设置为二。
- en: '`interaction_only` is a boolean that, when true, only interaction features
    are produced, meaning features that are products of degree distinct features.
    The default for `interaction_only` is false.'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`interaction_only`是一个布尔值，当为true时，只产生交互特征，这意味着是不同度数特征的乘积。`interaction_only`的默认值是false。'
- en: '`include_bias` is also a boolean that, when true (default), includes a `bias`
    column, the feature in which all polynomial powers are zero, adding a column of
    all ones.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '`include_bias`也是一个布尔值，当为true（默认）时，包括一个`bias`列，即所有多项式幂为零的特征，添加一列全为1。'
- en: 'Let''s set up a polynomial feature instance by first importing the class and
    instantiating with our parameters. At first, let''s take a look at what features
    we get when setting `interaction_only` to `False`:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先导入类并使用我们的参数实例化，来设置一个多项式特征实例。首先，让我们看看当将 `interaction_only` 设置为 `False` 时我们得到哪些特征：
- en: '[PRE44]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, we can `fit_transform` these polynomial features to our dataset and look
    at the `shape` of our extended dataset:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将这些多项式特征 `fit_transform` 到我们的数据集中，并查看扩展数据集的 `shape`：
- en: '[PRE45]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Our dataset has now expanded to `162501` rows and `9` columns.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集现在扩展到了 `162501` 行和 `9` 列。
- en: 'Let''s place our data into a DataFrame, setting the column headers to the `feature_names`,
    and taking a look at the first few rows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把数据放入一个 DataFrame 中，设置列标题为 `feature_names`，并查看前几行：
- en: '[PRE46]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This shows us:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示给我们：
- en: '|  | **x0** | **x1** | **x2** | **x0^2** | **x0 x1** | **x0 x2** | **x1^2**
    | **x1 x2** | **x2^2** |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '|  | **x0** | **x1** | **x2** | **x0^2** | **x0 x1** | **x0 x2** | **x1^2**
    | **x1 x2** | **x2^2** |'
- en: '| **0** | 1502.0 | 2215.0 | 2153.0 | 2256004.0 | 3326930.0 | 3233806.0 | 4906225.0
    | 4768895.0 | 4635409.0 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1502.0 | 2215.0 | 2153.0 | 2256004.0 | 3326930.0 | 3233806.0 | 4906225.0
    | 4768895.0 | 4635409.0 |'
- en: '| **1** | 1667.0 | 2072.0 | 2047.0 | 2778889.0 | 3454024.0 | 3412349.0 | 4293184.0
    | 4241384.0 | 4190209.0 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1667.0 | 2072.0 | 2047.0 | 2778889.0 | 3454024.0 | 3412349.0 | 4293184.0
    | 4241384.0 | 4190209.0 |'
- en: '| **2** | 1611.0 | 1957.0 | 1906.0 | 2595321.0 | 3152727.0 | 3070566.0 | 3829849.0
    | 3730042.0 | 3632836.0 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 1611.0 | 1957.0 | 1906.0 | 2595321.0 | 3152727.0 | 3070566.0 | 3829849.0
    | 3730042.0 | 3632836.0 |'
- en: '| **3** | 1601.0 | 1939.0 | 1831.0 | 2563201.0 | 3104339.0 | 2931431.0 | 3759721.0
    | 3550309.0 | 3352561.0 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1601.0 | 1939.0 | 1831.0 | 2563201.0 | 3104339.0 | 2931431.0 | 3759721.0
    | 3550309.0 | 3352561.0 |'
- en: '| **4** | 1643.0 | 1965.0 | 1879.0 | 2699449.0 | 3228495.0 | 3087197.0 | 3861225.0
    | 3692235.0 | 3530641.0 |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 1643.0 | 1965.0 | 1879.0 | 2699449.0 | 3228495.0 | 3087197.0 | 3861225.0
    | 3692235.0 | 3530641.0 |'
- en: Exploratory data analysis
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Now we can conduct some exploratory data analysis. Since the purpose of polynomial
    features is to get a better sense of feature interaction in the original data,
    the best way to visualize this is through a correlation `heatmap`.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以进行一些探索性数据分析。由于多项式特征的目的是在原始数据中获得更好的特征交互感，最佳的可视化方式是通过相关性 `heatmap`。
- en: 'We need to import a data visualization tool that will allow us to create a
    `heatmap`:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要导入一个数据可视化工具，以便我们可以创建 `heatmap`：
- en: '[PRE47]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Matplotlib and Seaborn are popular data visualization tools. We can now visualize
    our correlation `heatmap` as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Matplotlib 和 Seaborn 是流行的数据可视化工具。我们现在可以如下可视化我们的相关性 `heatmap`：
- en: '[PRE48]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '`.corr` is a function we can call on our DataFrame that gives us a correlation
    matrix of our features. Let''s take a look at our feature interactions:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '`.corr` 是一个我们可以调用我们的 DataFrame 的函数，它给我们一个特征的相关矩阵。让我们看一下我们的特征交互：'
- en: '![](img/d80a9fbb-3ed3-461d-add6-513e6fb79d8f.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d80a9fbb-3ed3-461d-add6-513e6fb79d8f.png)'
- en: The colors on the `heatmap` are based on pure values; the darker the color,
    the greater the correlation of the features.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 热图上的颜色基于纯数值；颜色越深，特征的相关性越大。
- en: So far, we have looked at our polynomial features with our `interaction_only`
    parameter set to `False`. Let's set this to `True` and see what our features look
    like without repeat variables.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经查看了我们设置 `interaction_only` 参数为 `False` 时的多项式特征。让我们将其设置为 `True` 并看看没有重复变量时我们的特征看起来如何。
- en: 'We will set up this polynomial feature instance the same as we did previously.
    Note the only difference is that `interaction_only` is now `True`:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照之前的方式设置这个多项式特征实例。注意唯一的区别是 `interaction_only` 现在是 `True`：
- en: '[PRE49]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'We now have `162501` rows by `6` columns. Let''s take a look:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有 `162501` 行和 `6` 列。让我们看一下：
- en: '[PRE50]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The DataFrame now looks as follows:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 现在看起来如下：
- en: '|  | **x0** | **x1** | **x2** | **x0 x1** | **x0 x2** | **x1 x2** |'
  id: totrans-317
  prefs: []
  type: TYPE_TB
  zh: '|  | **x0** | **x1** | **x2** | **x0 x1** | **x0 x2** | **x1 x2** |'
- en: '| **0** | 1502.0 | 2215.0 | 2153.0 | 3326930.0 | 3233806.0 | 4768895.0 |'
  id: totrans-318
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1502.0 | 2215.0 | 2153.0 | 3326930.0 | 3233806.0 | 4768895.0 |'
- en: '| **1** | 1667.0 | 2072.0 | 2047.0 | 3454024.0 | 3412349.0 | 4241384.0 |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 1667.0 | 2072.0 | 2047.0 | 3454024.0 | 3412349.0 | 4241384.0 |'
- en: '| **2** | 1611.0 | 1957.0 | 1906.0 | 3152727.0 | 3070566.0 | 3730042.0 |'
  id: totrans-320
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 1611.0 | 1957.0 | 1906.0 | 3152727.0 | 3070566.0 | 3730042.0 |'
- en: '| **3** | 1601.0 | 1939.0 | 1831.0 | 3104339.0 | 2931431.0 | 3550309.0 |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 1601.0 | 1939.0 | 1831.0 | 3104339.0 | 2931431.0 | 3550309.0 |'
- en: '| **4** | 1643.0 | 1965.0 | 1879.0 | 3228495.0 | 3087197.0 | 3692235.0 |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 1643.0 | 1965.0 | 1879.0 | 3228495.0 | 3087197.0 | 3692235.0 |'
- en: 'Since `interaction_only` has been set to `True` this time, `x0^2`, `x1^2`,
    and `x2^2` have disappeared since they were repeat variables. Let''s see what
    our correlation matrix looks like now:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这次`interaction_only`被设置为`True`，因此`x0^2`、`x1^2`和`x2^2`消失了，因为它们是重复变量。现在让我们看看我们的相关矩阵现在是什么样子：
- en: '[PRE51]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We get the following result:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下结果：
- en: '![](img/17488850-dc5c-4e56-8aed-7882370d9704.png)'
  id: totrans-326
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/17488850-dc5c-4e56-8aed-7882370d9704.png)'
- en: 'We are able to see how the features interact with each other. We can also perform
    a grid search of our KNN model with the new polynomial features, which can also
    be grid searched in a pipeline:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够看到特征是如何相互作用的。我们还可以使用新的多项式特征对KNN模型进行网格搜索，这些特征也可以在管道中进行网格搜索：
- en: 'Let''s set up our pipeline parameters first:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们先设置管道参数：
- en: '[PRE52]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Now, instantiate our `Pipeline`:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，实例化我们的`Pipeline`：
- en: '[PRE53]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'From here, we can set up our grid search and print the best score and parameters
    to learn from:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，我们可以设置我们的网格搜索并打印出最佳得分和参数以供学习：
- en: '[PRE54]'
  id: totrans-333
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Our accuracy is now 72.12%, which is an improvement from our accuracy without
    expanding our features using polynomial features!
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的准确率是72.12%，这比我们使用多项式特征扩展特征时的准确率有所提高！
- en: Text-specific feature construction
  id: totrans-335
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本特定特征构建
- en: Until this point, we have been working with categorical and numerical data.
    While our categorical data has come in the form of a string, the text has been
    part of a single category. We will now dive deeper into longer—form text data.
    This form of text data is much more complex than single—category text, because
    we now have a series of categories, or tokens.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理分类数据和数值数据。虽然我们的分类数据以字符串的形式出现，但文本一直是单一类别的一部分。现在我们将更深入地研究更长的文本数据。这种文本数据的形式比单一类别的文本数据要复杂得多，因为我们现在有一系列类别，或称为标记。
- en: Before we get any further into working with text data, let's make sure we have
    a good understanding of what we mean when we refer to text data. Consider a service
    like Yelp, where users write up reviews of restaurants and businesses to share
    their thoughts on their experience. These reviews, all written in text format,
    contain a wealth of information that would be useful for machine learning purposes,
    for example, in predicting the best restaurant to visit.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进一步深入处理文本数据之前，让我们确保我们清楚当我们提到文本数据时我们指的是什么。考虑一个像Yelp这样的服务，用户会撰写关于餐厅和企业的评论来分享他们的体验。这些评论，都是以文本格式编写的，包含大量对机器学习有用的信息，例如，在预测最佳餐厅访问方面。
- en: In general, a large part of how we communicate in today's world is through written
    text, whether in messaging services, social media, or email. As a result, so much
    can be garnered from this information through modeling. For example, we can conduct
    a sentiment analysis from Twitter data.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今世界，我们的大部分沟通都是通过书面文字进行的，无论是在消息服务、社交媒体还是电子邮件中。因此，通过建模可以从这些信息中获得很多。例如，我们可以从Twitter数据中执行情感分析。
- en: This type of work can be referred to as **natural language processing** (**NLP**).
    This is a field primarily concerned with interactions between computers and humans,
    specifically where computers can be programmed to process natural language.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 这种类型的工作可以被称为**自然语言处理**（**NLP**）。这是一个主要关注计算机与人类之间交互的领域，特别是计算机可以被编程来处理自然语言。
- en: Now, as we've mentioned before, it's important to note that all machine learning
    models require numerical inputs, so we have to be creative and think strategically
    when we work with text and convert such data into numerical features. There are
    several options for doing so, so let's get started.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，正如我们之前提到的，需要注意的是，所有机器学习模型都需要数值输入，因此当我们处理文本并将此类数据转换为数值特征时，我们必须富有创意并战略性地思考。有几种方法可以实现这一点，让我们开始吧。
- en: Bag of words representation
  id: totrans-341
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词袋表示
- en: The scikit-learn has a handy module called `feature_extraction` that allows
    us to, as the name suggests, extract features for data such as text in a format
    supported by machine learning algorithms. This module has methods for us to utilize
    when working with text.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn有一个方便的模块叫做`feature_extraction`，它允许我们，正如其名所示，从机器学习算法支持的格式中提取文本等数据的特征。这个模块为我们提供了在处理文本时可以利用的方法。
- en: Going forward, we may refer to our text data as a corpus, specifically meaning
    an aggregate of text content or documents.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，我们可能会将我们的文本数据称为语料库，具体来说，是指文本内容或文档的集合。
- en: 'The most common method to transform a corpus into a numerical representation,
    a process known as vectorization, is through a method called **bag-of-words**.
    The basic idea behind the bag of words approach is that documents are described
    by word occurrences while completely ignoring the positioning of words in the
    document. In its simplest form, text is represented as a **bag**,without regard
    for grammar or word order, and is maintained as a set, with importance given to
    multiplicity. A bag of words representation is achieved in the following three
    steps:'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 将语料库转换为数值表示的一种最常见方法，称为向量化，是通过一种称为**词袋模型**的方法实现的。词袋模型背后的基本思想是，文档由单词出现来描述，而完全忽略单词在文档中的位置。在其最简单形式中，文本被表示为一个**袋**，不考虑语法或单词顺序，并作为一个集合维护，给予多重性以重要性。词袋模型表示通过以下三个步骤实现：
- en: Tokenizing
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分词
- en: Counting
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数
- en: Normalizing
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化
- en: Let's start with tokenizing. This process uses white spaces and punctuation
    to separate words from each other, turning them into tokens. Each possible token
    is given an integer ID.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分词开始。这个过程使用空白和标点符号将单词分开，将它们转换为标记。每个可能的标记都被赋予一个整数ID。
- en: Next comes counting. This step simply counts the occurrences of tokens within
    a document.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是计数。这一步只是简单地计算文档中标记的出现次数。
- en: Last comes normalizing, meaning that tokens are weighted with diminishing importance
    when they occur in the majority of documents.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 最后是正则化，这意味着当标记在大多数文档中出现时，它们的权重会随着重要性的降低而降低。
- en: Let's consider a couple more methods for vectorizing.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑更多用于向量化的方法。
- en: CountVectorizer
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CountVectorizer
- en: '`CountVectorizer` is the most commonly used method to convert text data into
    their vector representations. It is similar to dummy variables, in the sense that
    `CountVectorizer` converts text columns into matrices where columns are tokens
    and cell values are counts of occurrences of each token in each document. The
    resulting matrix is referred to as a **document-term matrix** because each row
    will represent a **document** (in this case, a tweet) and each column represents
    a **term** (a word).'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`是将文本数据转换为它们的向量表示的最常用方法。在某种程度上，它与虚拟变量相似，因为`CountVectorizer`将文本列转换为矩阵，其中列是标记，单元格值是每个文档中每个标记的出现次数。得到的矩阵被称为**文档-词矩阵**，因为每一行将代表一个**文档**（在这种情况下，是一条推文），每一列代表一个**术语**（一个单词）。'
- en: Let's take a look at a new dataset, and see how `CountVectorizer` works. The
    Twitter Sentiment Analysis dataset contains 1,578,627 classified tweets, and each
    row is marked as one for positive sentiment and zero for negative sentiment.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看一个新的数据集，看看`CountVectorizer`是如何工作的。Twitter情感分析数据集包含1,578,627条分类推文，每行标记为正情感为1，负情感为0。
- en: Further information on this dataset can be found at [http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/.](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/)
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 关于此数据集的更多信息可以在[http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/](http://thinknook.com/twitter-sentiment-analysis-training-corpus-dataset-2012-09-22/)找到。
- en: 'Let''s load in our data using pandas'' `read_csv` method. Note that we are
    specifying an `encoding` as an optional parameter to ensure that we handle all
    special characters in the tweets properly:'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用pandas的`read_csv`方法加载数据。请注意，我们指定了一个可选的`encoding`参数，以确保我们正确处理推文中的所有特殊字符：
- en: '[PRE55]'
  id: totrans-357
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: This allows us to load in our data in a specific format and map text characters
    appropriately.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们能够以特定格式加载数据，并适当地映射文本字符。
- en: 'Take a look at the first few rows of data:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 看看数据的前几行：
- en: '[PRE56]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We get the following data:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到以下数据：
- en: '|  | **ItemID** | **Sentiment** | **SentimentText** |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '|  | **项目ID** | **情感** | **情感文本** |'
- en: '| **0** | 1 | 0 | is so sad for my APL frie... |'
  id: totrans-363
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 1 | 0 | 我为我的APL朋友感到难过... |'
- en: '| **1** | 2 | 0 | I missed the New Moon trail... |'
  id: totrans-364
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 2 | 0 | 我错过了新月天体... |'
- en: '| **2** | 3 | 1 | omg its already 7:30 :O |'
  id: totrans-365
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 3 | 1 | omg 它已经7:30 :O |'
- en: '| **3** | 4 | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 4 | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
- en: '| **4** | 5 | 0 | i think mi bf is cheating on me!!! ... |'
  id: totrans-367
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 5 | 0 | 我觉得我的bf在欺骗我!!! ... |'
- en: 'We are only concerned with the `Sentiment` and `SentimentText` columns, so
    we will delete the `ItemID` column for now:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只关心`情感`和`情感文本`列，所以现在我们将删除`项目ID`列：
- en: '[PRE57]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Our data looks as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据看起来如下：
- en: '|  | **Sentiment** | **SentimentText** |'
  id: totrans-371
  prefs: []
  type: TYPE_TB
  zh: '|  | **情感** | **情感文本** |'
- en: '| **0** | 0 | is so sad for my APL frie... |'
  id: totrans-372
  prefs: []
  type: TYPE_TB
  zh: '| **0** | 0 | 我为我的APL朋友感到难过... |'
- en: '| **1** | 0 | I missed the New Moon trail... |'
  id: totrans-373
  prefs: []
  type: TYPE_TB
  zh: '| **1** | 0 | 我错过了新月天体... |'
- en: '| **2** | 1 | omg its already 7:30 :O |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '| **2** | 1 | omg its already 7:30 :O |'
- en: '| **3** | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
  id: totrans-375
  prefs: []
  type: TYPE_TB
  zh: '| **3** | 0 | .. Omgaga. Im sooo im gunna CRy. I''... |'
- en: '| **4** | 0 | i think mi bf is cheating on me!!! ... |'
  id: totrans-376
  prefs: []
  type: TYPE_TB
  zh: '| **4** | 0 | i think mi bf is cheating on me!!! ... |'
- en: 'Now, we can import `CountVectorizer` and get a better understanding of the
    text we are working with:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以导入`CountVectorizer`，更好地理解我们正在处理文本：
- en: '[PRE58]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s set up our `X` and `y`:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设置我们的`X`和`y`：
- en: '[PRE59]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'The `CountVectorizer` class works very similarly to the custom transformers
    we have been working with so far, and has a `fit_transform` function to manipulate
    the data:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`类与迄今为止我们一直在使用的自定义转换器非常相似，并且有一个`fit_transform`函数来处理数据：'
- en: '[PRE60]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: After our `CountVectorizer` has transformed our data, we have 99,989 rows and
    105,849 columns.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的`CountVectorizer`转换我们的数据后，我们有99,989行和105,849列。
- en: '`CountVectorizer` has many different parameters that can change the number
    of features that are constructed. Let''s go over a few of these parameters to
    get a better sense of how these features are created.'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer`有许多不同的参数可以改变构建的特征数量。让我们简要回顾一下这些参数，以更好地了解这些特征是如何创建的。'
- en: CountVectorizer parameters
  id: totrans-385
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CountVectorizer参数
- en: 'A few parameters that we will go over include:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的一些参数包括：
- en: '`stop_words`'
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`stop_words`'
- en: '`min_df`'
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_df`'
- en: '`max_df`'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_df`'
- en: '`ngram_range`'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ngram_range`'
- en: '`analyzer`'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`analyzer`'
- en: '`stop_words` is a frequently used parameter in `CountVectorizer`. You can pass
    in the string `english` to this parameter, and a built-in stop word list for English
    is used. You can also specify a list of words yourself. These words will then
    be removed from the tokens and will not appear as features in your data.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: '`stop_words`是`CountVectorizer`中常用的一个参数。你可以向该参数传递字符串`english`，并使用内置的英语停用词列表。你也可以指定一个单词列表。这些单词将被从标记中删除，并且不会出现在你的数据中的特征中。'
- en: 'Here is an example:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子：
- en: '[PRE61]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: You can see that the feature columns have gone down from 105,849 when stop words
    were not used, to 105,545 when English stop words have been set. The idea behind
    using stop words is to remove noise within your features and take out words that
    occur so often that there won't be much meaning to garner from them in your models.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，当没有使用停用词时，特征列从105,849减少到105,545，当设置了英语停用词时。使用停用词的目的是从特征中去除噪声，并移除那些在模型中不会带来太多意义的常用词。
- en: Another parameter is called `min_df`. This parameter is used to skim the number
    of features, by ignoring terms that have a document frequency lower than the given
    threshold or cut-off.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个参数称为`min_df`。该参数用于通过忽略低于给定阈值或截止值的文档频率较低的术语来筛选特征数量。
- en: 'Here is an implementation of our `CountVectorizer` with `min_df`:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是带有`min_df`的我们的`CountVectorizer`实现：
- en: '[PRE62]'
  id: totrans-398
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: This is a method that is utilized to significantly reduce the number of features
    created.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个用于显著减少创建的特征数量的方法。
- en: 'There is also a parameter called `max_df`:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 同样还有一个参数称为`max_df`：
- en: '[PRE63]'
  id: totrans-401
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: This is similar to trying to understand what stop words exist in the document.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于试图了解文档中存在哪些停用词。
- en: 'Next, let''s look at the `ngram_range` parameter. This parameter takes in a
    tuple where the lower and upper boundary of the range of n-values indicates the
    number of different n-grams to be extracted. N-grams represent phrases, so a value
    of one would represent one token, however a value of two would represent two tokens
    together. As you can imagine, this will expand our feature set quite significantly:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看`ngram_range`参数。该参数接受一个元组，其中n值的范围的下限和上限表示要提取的不同n-gram的数量。N-gram代表短语，所以一个值代表一个标记，然而两个值则代表两个标记一起。正如你可以想象的那样，这将显著扩大我们的特征集：
- en: '[PRE64]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: See, we now have 3,219,557 features. Since sets of words (phrases) can sometimes
    have more meaning, using n-gram ranges can be useful for modeling.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 看看，我们现在有3,219,557个特征。由于单词集（短语）有时可以传达更多的意义，使用n-gram范围对于建模是有用的。
- en: 'You can also set an analyzer as a parameter in `CountVectorizer`. The analyzer
    determines whether the feature should be made of word or character n-grams. Word
    is the default:'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以在`CountVectorizer`中将分析器作为一个参数设置。分析器确定特征应该由单词或字符n-gram组成。默认情况下是单词：
- en: '[PRE65]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Given that word is the default, our feature column number doesn't change much
    from the original.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 由于默认情况下是单词，我们的特征列数量与原始数据变化不大。
- en: We can even create our own custom analyzer. Conceptually, words are built from
    root words, or stems, and we can construct a custom analyzer that accounts for
    this.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 我们甚至可以创建自己的自定义分析器。从概念上讲，单词是由词根或词干构建的，我们可以构建一个考虑这一点的自定义分析器。
- en: Stemming is a common natural language processing method that allows us to stem
    our vocabulary, or make it smaller by converting words to their roots. There is
    a natural language toolkit, known as NLTK, that has several packages that allow
    us to perform operations on text data. One such package is a `stemmer`.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取是一种常见的自然语言处理方法，它允许我们将词汇表简化，或者通过将单词转换为它们的词根来缩小它。有一个名为 NLTK 的自然语言工具包，它有几个包允许我们对文本数据进行操作。其中一个包就是
    `stemmer`。
- en: 'Let''s see how it works:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看它是如何工作的：
- en: 'First, import our `stemmer` and then initialize it:'
  id: totrans-412
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，导入我们的 `stemmer` 并初始化它：
- en: '[PRE66]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, let''s see how some words are stemmed:'
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看一些词是如何进行词根提取的：
- en: '[PRE67]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'So, the word `interesting` can be reduced to the root stem. We can now use
    this to create a function that will allow us to tokenize words into their stems:'
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，单词 `interesting` 可以被缩减到其词根。现在我们可以使用这个来创建一个函数，允许我们将单词标记为其词根：
- en: '[PRE68]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Let''s see what our function outputs:'
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看我们的函数输出的是什么：
- en: '[PRE69]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We can now place this tokenizer function into our analyzer parameter:'
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以将这个标记函数放入我们的分析器参数中：
- en: '[PRE70]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: This yields us fewer features, which intuitively makes sense since our vocabulary
    has reduced with stemming.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了更少的功能，这在直观上是有意义的，因为我们的词汇量随着词干提取而减少。
- en: '`CountVectorizer` is a very useful tool to help us expand our features and
    convert text to numerical features. There is another common vectorizer that we
    will look into.'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '`CountVectorizer` 是一个非常有用的工具，可以帮助我们扩展特征并将文本转换为数值特征。还有一个常见的向量化器我们将要探讨。'
- en: The Tf-idf vectorizer
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Tf-idf 向量化器
- en: A `Tf-idfVectorizer` can be broken down into two components. First, the *tf*
    part, which represents **term frequency**, and the *idf* part, meaning **inverse
    document frequency**. It is a term—weighting method that has applications in information—retrieval
    and clustering.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: '`Tf-idfVectorizer` 可以分解为两个组件。首先，是 *tf* 部分，它代表 **词频**，而 *idf* 部分则意味着 **逆文档频率**。这是一种在信息检索和聚类中应用的词—权重方法。'
- en: 'A weight is given to evaluate how important a word is to a document in a corpus.
    Let''s look into each part a little more:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 一个权重被赋予以评估一个词在语料库中的文档中的重要性。让我们更深入地看看每个部分：
- en: '**tf: term frequency**: Measures how frequently a term occurs in a document.
    Since documents can be different in length, it is possible that a term would appear
    many more times in longer documents than shorter ones. Thus, the term frequency
    is often divided by the document length, or the total number of terms in the document,
    as a way of normalization.'
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**tf：词频**：衡量一个词在文档中出现的频率。由于文档的长度可能不同，一个词在较长的文档中可能出现的次数比在较短的文档中多得多。因此，词频通常被除以文档长度，或者文档中的总词数，作为归一化的方式。'
- en: '**idf: i****nverse document frequency**: Measures how important a term is.
    While computing term frequency, all terms are considered equally important. However,
    certain terms, such as *is*, *of*, and *that*, may appear a lot of times but have
    little importance. So, we need to weight the frequent terms less, while we scale
    up the rare ones.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**idf：逆文档频率**：衡量一个词的重要性。在计算词频时，所有词都被视为同等重要。然而，某些词，如 *is*、*of* 和 *that*，可能出现很多次，但重要性很小。因此，我们需要减少频繁词的权重，同时增加罕见词的权重。'
- en: To re-emphasize, a `TfidfVectorizer` is the same as `CountVectorizer`, in that
    it constructs features from tokens, but it takes a step further and normalizes
    counts to frequency of occurrences across a corpus. Let's see an example of this
    in action.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 为了再次强调，`TfidfVectorizer` 与 `CountVectorizer` 相同，即它从标记中构建特征，但它更进一步，将计数归一化到语料库中出现的频率。让我们看看这个动作的一个例子。
- en: 'First, our import:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们的导入：
- en: '[PRE71]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'To bring up some code from before, a plain vanilla `CountVectorizer` will output
    a document-term matrix:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 为了引用之前的代码，一个普通的 `CountVectorizer` 将输出一个文档-词矩阵：
- en: '[PRE72]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Our  `TfidfVectorizer` can be set up as follows:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `TfidfVectorizer` 可以设置如下：
- en: '[PRE73]'
  id: totrans-435
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: We can see that both vectorizers output the same number of rows and columns,
    but produce different values in each cell. This is because `TfidfVectorizer` and
    `CountVectorizer` are both used to transform text data into quantitative data,
    but the way in which they fill in cell values differ.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，这两个向量化器输出相同数量的行和列，但在每个单元格中产生不同的值。这是因为 `TfidfVectorizer` 和 `CountVectorizer`
    都用于将文本数据转换为定量数据，但它们填充单元格值的方式不同。
- en: Using text in machine learning pipelines
  id: totrans-437
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在机器学习管道中使用文本
- en: 'Of course, the ultimate goal of our vectorizers is to use them to make text
    data ingestible for our machine learning pipelines. Because `CountVectorizer`
    and `TfidfVectorizer` act like any other transformer we have been working with
    in this book, we will have to utilize a scikit-learn pipeline to ensure accuracy
    and honesty in our machine learning pipeline. In our example, we are going to
    be working with a large number of columns (in the hundreds of thousands), so I
    will use a classifier that is known to be more efficient in this case, a Naive
    Bayes model:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的向量器的最终目标是将它们用于使文本数据可被我们的机器学习管道摄取。因为`CountVectorizer`和`TfidfVectorizer`就像我们在本书中使用的任何其他转换器一样，我们将不得不利用scikit-learn管道来确保我们的机器学习管道的准确性和诚实性。在我们的例子中，我们将处理大量的列（数以万计），因此我将使用在这种情况下已知更有效的分类器，即朴素贝叶斯模型：
- en: '[PRE74]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Before we start building our pipelines, let''s get our null accuracy of the
    response column, which is either zero (negative) or one (positive):'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建管道之前，让我们获取响应列的空准确率，该列要么为零（消极），要么为一（积极）：
- en: '[PRE75]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'Making the accuracy beat 56.5%. Now, let''s create a pipeline with two steps:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 使准确率超过56.5%。现在，让我们创建一个包含两个步骤的管道：
- en: '`CountVectorizer` to featurize the tweets'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`CountVectorizer`对推文进行特征提取
- en: '`MultiNomialNB` Naive Bayes model to classify between positive and negative
    sentiment'
  id: totrans-444
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MultiNomialNB`朴素贝叶斯模型用于区分积极和消极情绪'
- en: 'First let''s start with setting up our pipeline parameters as follows, and
    then instantiate our grid search as follows:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们设置我们的管道参数如下，然后按照以下方式实例化我们的网格搜索：
- en: '[PRE76]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: And we got 75.6%, which is great! Now, let's kick things into high-gear and
    incorporate the `TfidfVectorizer`. Instead of rebuilding the pipeline using tf-idf
    instead of `CountVectorizer`, let's try using something a bit different. The scikit-learn
    has a `FeatureUnion` module that facilitates horizontal stacking of features (side-by-side).
    This allows us to use multiple types of text featurizers in the same pipeline.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了75.6%，这很棒！现在，让我们加快速度，并引入`TfidfVectorizer`。而不是使用tf-idf重建管道而不是`CountVectorizer`，让我们尝试使用一些不同的东西。scikit-learn有一个`FeatureUnion`模块，它促进了特征的横向堆叠（并排）。这允许我们在同一个管道中使用多种类型的文本特征提取器。
- en: 'For example, we can build a `featurizer` that runs both a `TfidfVectorizer`
    and a `CountVectorizer` on our tweets and concatenates them horizontally (keeping
    the same number of rows but increasing the number of columns):'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以在我们的推文中运行一个`featurizer`，它同时运行一个`TfidfVectorizer`和一个`CountVectorizer`，并将它们水平连接（保持行数不变但增加列数）：
- en: '[PRE77]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Once we build the `featurizer`, we can use it to see how it affects the shape
    of our data:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们构建了`featurizer`，我们就可以用它来查看它如何影响我们数据的形状：
- en: '[PRE78]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We can see that unioning the two featurizers results in a dataset with the
    same number of rows, but doubles the number of either the `CountVectorizer` or
    the `TfidfVectorizer`. This is because the resulting dataset is literally both
    datasets side-by-side. This way, our machine learning models may learn from both
    sets of data simultaneously. Let''s change the `params` of our `featurizer` object
    slightly and see what difference it makes:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，将两个特征提取器合并会导致具有相同行数的数据集，但将`CountVectorizer`或`TfidfVectorizer`的数量翻倍。这是因为结果数据集实际上是两个数据集并排放置。这样，我们的机器学习模型可以同时从这两组数据中学习。让我们稍微改变一下`featurizer`对象的`params`，看看它会产生什么差异：
- en: '[PRE79]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Let''s build a much more comprehensive pipeline that incorporates the feature
    union of both of our vectorizers:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个更全面的管道，它结合了两个向量器的特征合并：
- en: '[PRE80]'
  id: totrans-455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Nice, even better than just `CountVectorizer` alone! It is also interesting
    to note that the best `ngram_range` for the `CountVectorizer` was `(1, 2)`, while
    it was `(1, 1)` for the `TfidfVectorizer`, implying that word occurrences alone
    were not as important as two-word phrase occurrences.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 很好，甚至比单独使用`CountVectorizer`还要好！还有一点值得注意的是，`CountVectorizer`的最佳`ngram_range`是`(1,
    2)`，而`TfidfVectorizer`是`(1, 1)`，这意味着单独的单词出现并不像两个单词短语的出现那样重要。
- en: 'By this point, it should be obvious that we could have made our pipeline much
    more complicated by:'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，应该很明显，我们可以通过以下方式使我们的管道变得更加复杂：
- en: Grid searching across dozens of parameters for each vectorizer
  id: totrans-458
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个向量器进行数十个参数的网格搜索
- en: Adding in more steps to our pipeline such as polynomial feature construction
  id: totrans-459
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的管道中添加更多步骤，例如多项式特征构造
- en: But this would have been very cumbersome for this text and would take hours
    to run on most commercial laptops. Feel free to expand on this pipeline and beat
    our score!
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 但这对本文来说会很繁琐，而且在大多数商业笔记本电脑上运行可能需要数小时。请随意扩展这个管道，并超越我们的分数！
- en: Phew, that was a lot. Text can be difficult to work with. Between sarcasm, misspellings,
    and vocabulary size, data scientists and machine learning engineers have their
    hands full. This introduction to working with text will allow you, the reader,
    to experiment with your own large text datasets and obtain your own results!
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 呼，这可真不少。文本处理可能会很困难。在讽刺、拼写错误和词汇量方面，数据科学家和机器学习工程师的工作量很大。本指南将使您，作为读者，能够对自己的大型文本数据集进行实验，并获得自己的结果！
- en: Summary
  id: totrans-462
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Thus far, we have gone over several methods of imputing missing values in our
    categorical and numerical data, encoding our categorical variables, and creating
    custom transformers to fit into a pipeline. We also dove into several feature
    construction methods for both numerical data and text-based data.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了在分类和数值数据中填充缺失值的方法，对分类变量进行编码，以及创建自定义转换器以适应管道。我们还深入探讨了针对数值数据和基于文本数据的特征构造方法。
- en: In the next chapter, we will take a look at the features we have constructed,
    and consider appropriate methods of selecting the right features to use for our
    machine learning models.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将查看我们构建的特征，并考虑选择适当的方法来选择用于我们的机器学习模型的正确特征。
