- en: Generative Adversarial Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: In this chapter, we are going to provide a brief introduction to a family of
    generative models based on some game theory concepts. Their main peculiarity is
    an adversarial training procedure that is aimed at learning to distinguish between
    true and fake samples, driving, at the same time, another component that generates
    samples more and more similar to the training examples.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将简要介绍基于一些博弈论概念的生成模型家族。它们的主要特点是针对学习区分真实样本和伪造样本的对抗性训练过程，同时，推动另一个生成越来越接近训练样本的样本的组件。
- en: 'In particular, we will be discussing:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，我们将讨论：
- en: Adversarial training and standard** Generative Adversarial Networks** (**GANs**)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对抗性训练和标准**生成对抗网络（GANs**）
- en: '**Deep Convolutional GANs** (**DCGAN**)'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**深度卷积生成对抗网络（DCGAN**）'
- en: '**Wasserstein GANs** (**WGAN**)'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Wasserstein GANs（WGAN**）'
- en: Adversarial training
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性训练
- en: The brilliant idea of adversarial training, proposed by Goodfellow and others
    (in *Generative Adversarial Networks*,*Goodfellow I*. *J*., *Pouget-Abadie J*.,
    *Mirza M*., *Xu B*., *Warde-Farley D*., *Ozair S*., *Courville A*., *Bengio Y*.,
    *arXiv:1406.2661* [*stat.ML*]), ushered in a new generation of generative models
    that immediately outperformed the majority of existing algorithms. All of the
    derived models are based on the same fundamental concept of adversarial training,
    which is an approach partially inspired by game theory.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Goodfellow 和其他人（在 *Generative Adversarial Networks*，*Goodfellow I*.*J*., *Pouget-Abadie
    J*., *Mirza M*., *Xu B*., *Warde-Farley D*., *Ozair S*., *Courville A*., *Bengio
    Y*., *arXiv:1406.2661* [*stat.ML*]）提出的对抗性训练的杰出想法，引领了一代新的生成模型，这些模型立即优于大多数现有算法。所有衍生模型都基于相同的对抗性训练基本概念，这是一种部分受博弈论启发的途径。
- en: 'Let''s suppose that we have a data generating process, *p[data](x)*, that represents
    an actual data distribution and a finite number of samples that we suppose are
    drawn from *p[data]*:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个数据生成过程，*p[data](x)*，它代表一个实际的数据分布和有限数量的样本，我们假设这些样本是从 *p[data]* 中抽取的：
- en: '![](img/caf2bfbb-3f87-4f34-ad30-fa5c2fa2e697.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](img/caf2bfbb-3f87-4f34-ad30-fa5c2fa2e697.png)'
- en: 'Our goal is to train a model called a generator, whose distribution must be
    as close as possible to *p[data]*. This is the trickiest part of the algorithm,
    because instead of standard methods (for example, variational autoencoders), adversarial
    training is based on a minimax game between two players (we can simply say that,
    given an objective, the goal of both players is to minimize the maximum possible
    loss; but in this case, each of them works on different parameters). One player
    is the generator, we can define as a parameterized function of a noise sample:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是训练一个称为生成器的模型，其分布必须尽可能接近 *p[data]*。这是算法中最棘手的部分，因为与标准方法（例如，变分自编码器）不同，对抗性训练基于两个玩家之间的最小-最大博弈（我们可以简单地说，给定一个目标，两个玩家的目标是使最大可能的损失最小化；但在这个案例中，他们各自工作在不同的参数上）。一个玩家是生成器，我们可以将其定义为一个噪声样本的参数化函数：
- en: '![](img/20c40179-342d-4870-83c7-10c7d02f44cb.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![](img/20c40179-342d-4870-83c7-10c7d02f44cb.png)'
- en: 'The generator is fed with a noise vector (in this case, we have employed a
    uniform distribution, but there are no particular restrictions; therefore, we
    are simply going to say that *z* is drawn from a noise distribution *p[noise]*),
    and outputs a value that has the same dimensionality of the samples drawn from
    *p[data]*. Without any further control, the generator distribution will be completely
    different from the data generating process, but this is the moment for the other
    player to enter the scene. The second model is called the *discriminator* (or
    Critic), and it has the responsibility of evaluating the samples drawn from *p[data]*
    and the ones produced by the generator:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器被一个噪声向量（在这种情况下，我们使用了均匀分布，但没有任何特别的限制；因此，我们只是简单地说 *z* 是从噪声分布 *p[noise]* 中抽取的），并输出一个与从
    *p[data]* 中抽取的样本具有相同维度的值。在没有进一步控制的情况下，生成器的分布将完全不同于数据生成过程，但这是另一个玩家进入场景的时刻。第二个模型被称为
    *判别器*（或评论家），它负责评估从 *p[data]* 中抽取的样本和由生成器产生的样本：
- en: '![](img/975272aa-e4ee-40e0-8509-6ab22347e7d6.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/975272aa-e4ee-40e0-8509-6ab22347e7d6.png)'
- en: 'The role of this model is to output a probability that must reflect the fact
    that the sample is drawn from *p[data]*, instead of being generated by *G(z; θ[g])*.
    What happens is very simple: the first player (the generator) outputs a sample, *x*.
    If *x* actually belongs to *p[data]*, the discriminator will output a value close
    to 1, while if it''s very different from the other true samples, *D(x; θ[d])*
    will output a very low probability. The real structure of the game is based on
    the idea of training the generator to deceive the discriminator, by producing
    samples that can potentially be drawn from *p[data]*. This result can be achieved
    by trying to maximize the log-probability, *log(D(x; θ[d]**))*, when *x* is a
    true sample (drawn from *p[data]*), while minimizing the log-probability, *log(1
    - D(G(z; θ[g]); θ[d]))*, with *z* sampled from a noise distribution.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的作用是输出一个概率，这个概率必须反映样本是从 *p[data]* 中抽取的，而不是由 *G(z; θ[g]*) 生成的。发生的情况非常简单：第一个玩家（生成器）输出一个样本，*x*。如果
    *x* 实际上属于 *p[data]*，判别器将输出一个接近 1 的值，而如果它与其他真实样本非常不同，*D(x; θ[d]*) 将输出一个非常低的概率。游戏的真正结构基于训练生成器欺骗判别器的想法，通过产生可以潜在地从
    *p[data]* 中抽取的样本。通过在 *x* 是从 *p[data]* 中抽取的真实样本时尝试最大化对数概率，*log(D(x; θ[d]**))，同时最小化对数概率，*log(1
    - D(G(z; θ[g]); θ[d]))*，其中 *z* 从噪声分布中抽取。
- en: The first operation forces the discriminator to become more and more aware of
    the true samples (this condition is necessary to avoid being deceived too easily).
    The second objective is a little bit more complex, because the discriminator has
    to evaluate a sample that can be acceptable or not. Let's suppose that the generator
    is not smart enough, and outputs a sample that cannot belong to *p[data]*. As
    the discriminator is learning how *p[data]* is structured, it will very soon distinguish
    the wrong sample, outputting a low probability. Hence, by minimizing *log(1 -
    D(G(z; θ[g]); θ[d]))*, we are forcing the discriminator to become more and more
    critical when the samples are quite different from the ones drawn from *p[d]*[*ata*],
    and the becomes generator more and more able to produce acceptable samples. On
    the other hand, if the generator outputs a sample that belongs to the data generating
    process, the discriminator will output a high probability, and the minimization
    falls back in the previous case.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个操作迫使判别器越来越意识到真实样本（这个条件是必要的，以避免被轻易欺骗）。第二个目标稍微复杂一些，因为判别器必须评估一个可能被接受或不被接受的样本。假设生成器不够聪明，输出的样本不属于
    *p[data]*。随着判别器学习 *p[data]* 的结构，它很快就会区分出错误的样本，输出一个低概率。因此，通过最小化 *log(1 - D(G(z;
    θ[g]); θ[d]))*，我们迫使判别器在样本与从 *p[d]*[*ata*] 中抽取的样本相当不同时越来越挑剔，从而使生成器越来越能够产生可接受的样本。另一方面，如果生成器输出的样本属于数据生成过程，判别器将输出一个高概率，最小化将回到之前的情况。
- en: 'The authors expressed this minimax game using a shared value function, *V(G,
    D)*, that must be minimized by the generator and maximized by the discriminator:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作者使用一个共享的价值函数，*V(G, D)*，来表示这个最小-最大游戏，该函数必须由生成器最小化，由判别器最大化：
- en: '![](img/9c2dfbf2-2dc2-4aa2-9776-d8789f81fba1.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9c2dfbf2-2dc2-4aa2-9776-d8789f81fba1.png)'
- en: This formula represents the dynamics of a non-cooperative game between two players
    (for further information, refer to *Tadelis S*., *Game Theory, Princeton University
    Press*) that theoretically admits a special configuration, called a *Nash equilibrium*,
    that can be described by saying that if the two players know each other's strategy,
    they have no reason to change their own strategy if the other player doesn't.
    In this case, both the discriminator and generator will pursue their strategies
    until no change is needed, reaching a final, stable configuration, which is potentially
    a Nash equilibrium (even if there are many factors that can prevent reaching this
    goal). A common problem is the premature convergence of the discriminator, which
    forces the gradients to vanish because the loss function becomes flat in a region
    close to 0\. As this is a game, a fundamental condition is the possibility to
    provide information to allow the player to make corrections. If the discriminator
    learns how to separate true samples from fake ones too quickly, the generator
    convergence slows down, and the player can remain trapped in a sub-optimal configuration.
    In general, when the distributions are rather complex, the discriminator is slower
    than the generator; but, in some cases, it is necessary to update the generator
    more times after each single discriminator update. Unfortunately, there are no
    rules of thumb; but, for example, when working with images, it's possible to observe
    the samples generated after a sufficiently large number of iterations. If the
    discriminator loss has become very small and the samples appear corrupted or incoherent,
    it means that the generator did not have enough time to learn the distribution,
    and it's necessary to slow down the discriminator.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式代表了两个玩家之间非合作博弈的动力学（欲了解更多信息，请参阅*Tadelis S*., *博弈论，普林斯顿大学出版社*）。从理论上讲，这种博弈可以接受一种特殊配置，称为**纳什均衡**，可以这样描述：如果两个玩家知道彼此的策略，那么如果对方玩家不改变自己的策略，他们就没有理由改变自己的策略。在这种情况下，判别器和生成器都会追求自己的策略，直到不再需要改变，达到一个最终、稳定的配置，这可能是纳什均衡（即使有许多因素可能阻止达到这个目标）。一个常见的问题是判别器过早收敛，这导致梯度消失，因为损失函数在接近0的区域变得平坦。由于这是一个博弈，一个基本条件是提供信息以允许玩家进行纠正。如果判别器学习如何快速区分真实样本和伪造样本，那么生成器的收敛速度会减慢，玩家可能会被困在次优配置中。一般来说，当分布相当复杂时，判别器比生成器慢；但在某些情况下，在每次单独更新判别器之后，可能需要更新生成器更多次。不幸的是，没有经验法则；例如，当处理图像时，可以观察在足够多的迭代后生成的样本。如果判别器损失变得非常小，并且样本看起来被破坏或不连贯，这意味着生成器没有足够的时间学习分布，需要减慢判别器的速度。
- en: 'The authors in the aforementioned paper showed that, given a generator characterized
    by a distribution *p[g](x)*, the optimal discriminator is:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 上文提到的论文的作者表明，给定一个由分布 *p[g](x)* 特征化的生成器，最优判别器是：
- en: '![](img/6905cecd-0b6b-4604-8b67-992f19fee8df.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6905cecd-0b6b-4604-8b67-992f19fee8df.png)'
- en: 'At this point, considering the previous value function *V(G*, *D)* and using
    the optimal discriminator, we can rewrite it in a single objective (as a function
    of *G*) that must be minimized by the generator:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，考虑到前面的价值函数 *V(G*, *D)* 和使用最优判别器，我们可以将其重写为一个单一目标（作为 *G* 的函数），生成器必须最小化这个目标：
- en: '![](img/85879f7c-9cf7-408c-9672-ddabf7e0989d.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/85879f7c-9cf7-408c-9672-ddabf7e0989d.png)'
- en: 'To better understand how a **GAN** works, we need to expand the previous expression:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解**生成对抗网络（GAN**）的工作原理，我们需要扩展前面的表达式：
- en: '![](img/0287e84f-5d7a-4c11-8cc3-54b8c26eb2f5.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0287e84f-5d7a-4c11-8cc3-54b8c26eb2f5.png)'
- en: 'Applying some simple manipulations, we get the following:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一些简单的操作，我们得到以下结果：
- en: '![](img/3058f142-4c04-4230-94dd-422eaaec8d37.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3058f142-4c04-4230-94dd-422eaaec8d37.png)'
- en: 'The last term represents the Jensen-Shannon divergence between *p[data]* and
    *p[g]*. This measure is similar to the Kullback-Leibler divergence, but it''s
    symmetric and bounded between *0* and *log(2)*. When the two distributions are
    identical, *D[JS] = 0*, but if their supports (the value sets where *p(x) > 0*)
    are disjoint, *D[JS]** = log(2) (*while *D[KL] = ∞)*. Therefore, the value function
    can be expressed as:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个项代表了 *p[data]* 和 *p[g]* 之间的Jensen-Shannon散度。这个度量与Kullback-Leibler散度相似，但它是对称的，并且介于
    *0* 和 *log(2)* 之间。当两个分布相同，*D[JS] = 0*；但如果它们的支撑（*p(x) > 0* 的值集）不相交，*D[JS]** = log(2)（而
    *D[KL] = ∞*）。因此，价值函数可以表示为：
- en: '![](img/1856a1c4-b41c-4ca3-8d60-a82a38f6a07e.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1856a1c4-b41c-4ca3-8d60-a82a38f6a07e.png)'
- en: Now, it should be clearer that a GAN tries to minimize the Jensen-Shannon divergence
    between the data generating process and the generator distribution. In general,
    this procedure is quite effective; however, when the supports are disjointed,
    a GAN has no pieces of information about the true distance. This consideration
    (analyzed with more mathematical rigor in *Improved Techniques for Training GANs*, *Salimans
    T*., *Goodfellow I*., *Zaremba W*., *Cheung V*., *Radford A*.,and *Chen X*., *arXiv:1606.03498
    [cs.LG]*) explains why training a GAN can become quite difficult, and, consequently,
    why the Nash equilibrium cannot be found in many cases. For these reasons, we
    are going to analyze an alternative approach in the next section.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，应该更清楚的是，GAN 尝试最小化数据生成过程和生成器分布之间的 Jensen-Shannon 散度。一般来说，这个程序相当有效；然而，当支持集不连续时，GAN
    没有关于真实距离的信息。这种考虑（在 *Improved Techniques for Training GANs*，*Salimans T*., *Goodfellow
    I*., *Zaremba W*., *Cheung V*., *Radford A*.,and *Chen X*., *arXiv:1606.03498
    [cs.LG]*) 中以更严格的数学方式分析）解释了为什么训练 GAN 可以变得相当困难，并且因此，为什么在许多情况下无法找到纳什均衡。出于这些原因，我们将在下一节分析一种替代方法。
- en: 'The complete GAN algorithm (as proposed by the authors) is:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的 GAN 算法（如作者所提出）如下：
- en: Set the number of epochs, *N[epochs]*
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置迭代次数，*N[epochs]*
- en: Set the number of discriminator iterations, *N[iter]* (in most cases, *N[iter]
    = 1*)
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置判别器迭代次数，*N[iter]*（在大多数情况下，*N[iter] = 1*）
- en: Set the batch size, *k*
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置批量大小，*k*
- en: Define a noise generating process, *M* (for example, *U(-1, 1)*)
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个噪声生成过程，*M*（例如，*U(-1, 1)*）
- en: 'For *e=1* to *N[epochs]*:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *e=1* 到 *N[epochs]*：
- en: Sample *k* values from *X*
  id: totrans-36
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *X* 中采样 *k* 个值
- en: Sample *k* values from *N*
  id: totrans-37
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *N* 中采样 *k* 个值
- en: 'For *i=1* to *N[iter]*:'
  id: totrans-38
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 *i=1* 到 *N[iter]*：
- en: Compute the gradients, *∇[d] V(G, D)* (only with respect to the discriminator
    variables). The expected value is approximated with a sample mean.
  id: totrans-39
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度，*∇[d] V(G, D)*（仅针对判别器变量）。期望值通过样本均值近似。
- en: Update the discriminator parameters by *Stochastic Gradient Ascent* (as we are
    working with logarithms, it's possible to minimize the negative loss).
  id: totrans-40
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机梯度上升更新判别器参数（由于我们处理对数，可以最小化负损失）。
- en: Sample *k* values from *N*
  id: totrans-41
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 *N* 中采样 *k* 个值
- en: Compute the gradients, *∇[g] V[noise](G, D)* (only with respect to the generator
    variables)
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度，*∇[g] V[noise](G, D)*（仅针对生成器变量）
- en: Update the generator parameters by Stochastic Gradient Descent
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机梯度下降更新生成器参数
- en: As these models need to sample noisy vectors in order to guarantee the reproducibility,
    I suggest setting the random seed in both NumPy (`np.random.seed(...)`) and TensorFlow
    (`tf.set_random_seed(...)`). The default value chosen for all of these experiments
    is 1,000.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些模型需要采样噪声向量以保证可重复性，我建议在 NumPy (`np.random.seed(...)`) 和 TensorFlow (`tf.set_random_seed(...)`)
    中设置随机种子。所有这些实验的默认值选择为 1,000。
- en: Example of DCGAN with TensorFlow
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow 中的 DCGAN 示例
- en: 'In this example, we want to build a DCGAN (proposed in *Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks*, *Radford A*.,
    *Metz L*., *Chintala S*., , *arXiv:1511.06434 [cs.LG]*) with the Fashion-MNIST
    dataset (obtained through the `keras` helper function). As the training speed
    is not very high, we limit the number of samples to 5,000, but I suggest repeating
    the experiment with larger values. The first step is loading and normalizing (between
    -1 and 1) the dataset:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们希望使用 Fashion-MNIST 数据集（通过 `keras` 辅助函数获得）构建一个 DCGAN（由 *Unsupervised Representation
    Learning with Deep Convolutional Generative Adversarial Networks* 提出，*Radford
    A*., *Metz L*., *Chintala S*., , *arXiv:1511.06434 [cs.LG]*)。由于训练速度不是很高，我们将样本数量限制为
    5,000，但我建议使用更大的值重复实验。第一步是加载并归一化（介于 -1 和 1 之间）数据集：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'According to the original paper, the generator is based on four transpose convolutions
    with kernel sizes equal to *(4, 4)* and strides equal to *(2, 2)*. The input is
    a single multi-channel pixel (*1 × 1 × code_length*) that is expanded by subsequent
    convolutions. The number of filters is 1024, 512, 256, 128, and 1 (we are working
    with grayscale images). The authors suggest employing a symmetric-valued dataset
    (that''s why we have normalized between -1 and 1), batch normalization after each
    layer, and leaky ReLU activation (with a default negative slope set to 0.2):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 根据原始论文，生成器基于四个大小为 *(4, 4)* 且步长为 *(2, 2)* 的转置卷积。输入是一个单通道像素 (*1 × 1 × code_length*)，随后通过后续卷积进行扩展。滤波器的数量为
    1024、512、256、128 和 1（我们处理的是灰度图像）。作者建议使用对称值数据集（这就是为什么我们在 -1 和 1 之间进行归一化的原因），在每个层后进行批量归一化，并使用漏斗
    ReLU 激活（默认负斜率设置为 0.2）：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The strides are set to work with 64 × 64 images (unfortunately, the Fashion-MNIST
    dataset has 28 × 28 samples, which cannot be generated with power-of-two modules);
    therefore, we are going to resize the samples while training. As we need to compute
    the gradients of the discriminator and generator separately, it's necessary to
    set the variable scope (using the context manager `tf.variable_scope()`) to immediately
    extract only the variables whose names have the scope as a prefix (for example, `generator/Conv_1_1/...`).
    The `is_training` parameter is necessary to disable the batch normalization during
    the generation phase.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 步长设置为与 64 × 64 图像一起工作（遗憾的是，Fashion-MNIST 数据集有 28 × 28 样本，无法使用二进制幂模块生成）；因此，在训练过程中我们将调整样本大小。由于我们需要分别计算判别器和生成器的梯度，因此有必要设置变量作用域（使用上下文管理器
    `tf.variable_scope()`）以立即提取只有名称具有作用域作为前缀的变量（例如，`generator/Conv_1_1/...`）。`is_training`
    参数在生成阶段是必要的，以禁用批归一化。
- en: 'The discriminator is almost the same as a generator (the only main differences
    are the inverse convolution sequence and the absence of batch normalization after
    the first layer):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 判别器几乎与生成器相同（唯一的主要区别是逆卷积序列和第一层之后没有批归一化）：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this case, we have an extra parameter (`reuse_variables`) that is necessary
    when building the loss functions. In fact, we need to declare two discriminators
    (fed with real samples and with the generator output), but they are not made up
    of separate layers; hence, the second one must reuse the variables defined by
    the first one. We can now create a graph and define all of the placeholders and
    operations:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个额外的参数（`reuse_variables`），在构建损失函数时是必要的。实际上，我们需要声明两个判别器（分别用真实样本和生成器输出进行喂养），但它们不是由单独的层组成；因此，第二个必须重用第一个定义的变量。现在我们可以创建一个图并定义所有占位符和操作：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The first step is defining the placeholders:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是定义占位符：
- en: '`input_x` contains the true samples drawn from *X*'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_x` 包含从 *X* 中抽取的真实样本'
- en: '`input_z` contains the noise samples'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_z` 包含噪声样本'
- en: '`is_training` is a Boolean indicating whether or not the batch normalization
    must be active'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`is_training` 是一个布尔值，指示是否必须激活批归一化'
- en: Then, we define the generator instance after reshaping the noise sample as a
    *(1 × 1 × code_length)* matrix (this is necessary to work efficiently with transpose
    convolutions). As this is a fundamental hyperparameter, I suggest testing different
    values and comparing the final performances.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在将噪声样本重塑为 *(1 × 1 × code_length)* 矩阵后定义生成器实例（这是为了有效地使用转置卷积）。由于这是一个基本超参数，我建议测试不同的值并比较最终的性能。
- en: As explained previously, the input images are resized before defining the two
    discriminators (the second one reuses the variables previously defined). The  `discr_1_l `instance
    is fed with the true samples, while `discr_2_l` works with the generator output.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在定义两个判别器之前（第二个重用先前定义的变量）输入图像被调整大小。`discr_1_l` 实例用真实样本进行喂养，而 `discr_2_l`
    使用生成器输出工作。
- en: 'The next step is defining the loss functions. As we are working with logarithms,
    there can be stability problems when the values become close to 0\. For this reason,
    it''s preferable to employ the TensorFlow built-in function `tf.nn.sigmoid_cross_entropy_with_logits()`,
    which guarantees numerical stability in every case. This function takes a *logit*
    as input and applies the sigmoid transformation internally. In general, the output
    is:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是定义损失函数。由于我们使用对数，当值接近 0 时可能会出现稳定性问题。因此，最好使用 TensorFlow 内置函数 `tf.nn.sigmoid_cross_entropy_with_logits()`，该函数在所有情况下都保证数值稳定性。此函数接受一个
    *logit* 作为输入，并在内部应用 sigmoid 变换。通常，输出如下：
- en: '![](img/113fa46f-60de-4854-8b88-6afa017fec4a.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/113fa46f-60de-4854-8b88-6afa017fec4a.png)'
- en: Therefore, setting the label equal to 1 forces the second term to be null, and
    vice versa. At this point, we need to create two lists containing the variables
    belonging to each scope (this can be easily achieved by using the `tf.trainable_variables()` function,
    which outputs a list of all variables). The last step consists of defining the
    optimizers. As suggested in the official TensorFlow documentation, when working
    with batch normalizations, it's necessary to wrap the training operations in a
    context manager that checks whether all dependencies (in this case, batch average
    and variance) have been computed. We have employed the Adam optimizer with *η*
    = 0.0002, and a gradient momentum forgetting factor (*μ1*) equal to 0.5 (this
    is a choice motivated by the potential instability that a high momentum can yield).
    As it's possible to see, in both cases, the minimization is limited to a specific
    subset of the variables (providing a list through the `var_list` parameter).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，将标签设置为1将迫使第二项为零，反之亦然。此时，我们需要创建两个列表，包含每个作用域的变量（这可以通过使用`tf.trainable_variables()`函数轻松实现，该函数输出所有变量的列表）。最后一步是定义优化器。如官方TensorFlow文档中建议的，当与批量归一化一起工作时，有必要将训练操作包装在一个上下文管理器中，该管理器检查是否已计算所有依赖项（在这种情况下，批量平均值和方差）。我们使用了Adam优化器，*η*
    = 0.0002，以及一个梯度动量遗忘因子(*μ1*)等于0.5（这是一个受高动量可能导致的潜在不稳定性所驱动的选择）。正如所见，在两种情况下，最小化仅限于变量的特定子集（通过`var_list`参数提供列表）。
- en: 'At this point, we can create a `Session` (we are going to use an `InteractiveSession`),
    initialize all variables, and start the training procedure (with 200 epochs and
    a batch size equal to 128):'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们可以创建一个`Session`（我们将使用`InteractiveSession`），初始化所有变量，并开始训练过程（200个周期和批大小为128）：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The training step (with a single discriminator iteration) is split into two
    phases:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 训练步骤（包含单个判别器迭代）分为两个阶段：
- en: Discriminator training with a batch of true images and noise samples
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用真实图像和噪声样本批次的判别器训练
- en: Generator training with a batch of noise samples
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用噪声样本批次的生成器训练
- en: 'Once the training process has finished, we can generate some images (50) by
    executing the generator with a matrix of noise samples:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练过程完成，我们可以通过执行包含噪声样本矩阵的生成器来生成一些图像（50个）：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The result is shown in the following screenshot:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下截图所示：
- en: '![](img/1f960533-fb13-4f56-8901-1f74b65f1635.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f960533-fb13-4f56-8901-1f74b65f1635.png)'
- en: Samples generated by a DCGAN trained with the Fashion-MNIST dataset
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Fashion-MNIST数据集训练的DCGAN生成的样本
- en: As an exercise, I invite the reader to employ more complex convolutional architectures
    and an RGB dataset such as CIFAR-10 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，我邀请读者使用更复杂的卷积架构和RGB数据集，如CIFAR-10 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))。
- en: The training phase of this example and the following one, even if limited to
    5,000 samples, can be quite slow (around 12-15 hours) particularly when no GPU
    is available. The reader can simplify the examples by reducing the complexity
    of the networks (paying attention to the shapes) and reducing the number of samples.
    To avoid mismatches, I suggest adding the `print(gen.shape)` command after the
    generator instance. The expected shape should be `(?, 64, 64, 1)`. Alternatively,
    it's possible to employ smaller target dimensions (like 32 × 32), setting one
    of the strides (possibly the last one) equal to `(1, 1)`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这个示例和下一个示例的训练阶段仅限于5,000个样本，也可能相当慢（大约12-15小时），尤其是在没有GPU的情况下。读者可以通过减少网络的复杂性（注意形状）和减少样本数量来简化示例。为了避免不匹配，我建议在生成器实例之后添加`print(gen.shape)`命令。预期的形状应该是`(?,
    64, 64, 1)`。或者，可以采用较小的目标维度（如32×32），将其中一个步长（可能是最后一个）设置为`(1, 1)`。
- en: Wasserstein GAN (WGAN)
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 水晶石生成对抗网络（WGAN）
- en: As explained in the previous section, one of the most difficult problems with
    standard GANs is caused by the loss function based on the Jensen-Shannon divergence,
    whose value becomes constant when two distributions have disjointed supports.
    This situation is quite common with high-dimensional, semantically structured
    datasets. For example, images are constrained to having particular features in
    order to represent a specific subject (this is a consequence of the manifold assumption
    discussed in [Chapter 2](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml), *Introduction
    to Semi-Supervised Learning*). The initial generator distribution is very unlikely
    to overlap a true dataset, and in many cases, they are also very far from each
    other. This condition increases the risk of learning a wrong representation (a
    problem known as mode collapse), even when the discriminator is able to distinguish
    between true and generated samples (such a condition arises when the discriminator
    learns too quickly, with respect to the generator). Moreover, the Nash equilibrium
    becomes harder to achieve, and the GAN can easily remain blocked in a sub-optimal
    configuration.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所述，标准生成对抗网络（GAN）中最困难的问题之一是由基于Jensen-Shannon散度的损失函数引起的，当两个分布具有不连续的支持时，其值会变得恒定。这种情况在高维、语义结构化数据集中相当常见。例如，图像被限制具有特定的特征以表示特定的主题（这是在[第2章](f6860c85-a228-4e63-96ac-22a0caf1a767.xhtml)，《半监督学习导论》中讨论的流形假设的结果）。初始生成器分布与真实数据集重叠的可能性非常低，在许多情况下，它们彼此之间也非常遥远。这种条件增加了学习错误表示（一个被称为模式崩溃的问题）的风险，即使判别器能够区分真实样本和生成样本（这种情况发生在判别器相对于生成器学习得太快时）。此外，纳什均衡的实现变得更加困难，GAN很容易陷入次优配置。
- en: 'In order to mitigate this problem, Arjovsky, Chintala, and Bottou (in *Wasserstein
    GAN*, *Arjovsky M*., *Chintala S*., *Bottou L.*, *arXiv:1701.07875 [stat.ML]*)
    proposed employing a different divergence, called the *Wasserstein distance* (or
    Earth Mover''s distance), which is formally defined as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这个问题，Arjovsky、Chintala和Bottou（在*Wasserstein GAN*，Arjovsky M.，Chintala S.，Bottou
    L.，arXiv:1701.07875 [stat.ML]）提出了采用不同的散度，称为*Wasserstein距离*（或地球迁移距离），其正式定义如下：
- en: '![](img/db345cf3-03b3-4cd5-9293-99159663a83a.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db345cf3-03b3-4cd5-9293-99159663a83a.png)'
- en: 'The term *∏(p[data], p[g])* represents the set of all possible joint probability
    distributions between *p[data]*, *p**[g]*. Hence, the Wasserstein distance is
    the infimum (considering all joint distributions) of the set of expected values
    of *||x - y||,* where *x* and *y* are sampled from the joint distribution μ. The
    main property of *D[W]* is that, even when two distributions have disjointed support,
    its value is proportional to the actual distributional distance. The formal proof
    is not very complex, but it''s easier to understand the concept intuitively. In
    fact, given two distributions with disjointed support, the infimum operator forces
    taking the shortest distance between each possible couple of samples. Clearly,
    this measure is more robust than the Jensen-Shannon divergence, but there''s a
    practical drawback: it''s extremely difficult to compute. As we cannot work with
    all possible joint distributions (nor with an approximation), a further step is
    necessary to employ this loss function. In the aforementioned paper, the authors
    proved that it''s possible to apply a transformation, thanks to the Kantorovich-Rubinstein
    theorem (the topic is quite complex, but the reader can find further information
    in *On the Kantorovich–Rubinstein Theorem*, *Edwards D*. *A*., *Expositiones Mathematicae*,
    2011):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 术语*∏(p[data], p[g])*表示*p[data]*和*p[g]*之间所有可能的联合概率分布的集合。因此，Wasserstein距离是所有期望值集合的下确界（考虑所有联合分布），其中*x*和*y*是从联合分布μ中抽取的。*D[W]*的主要特性是，即使两个分布具有不连续的支持，其值也正比于实际的分布距离。形式证明并不复杂，但直观上更容易理解。事实上，给定两个具有不连续支持分布，下确界运算强制取每个可能样本对之间的最短距离。显然，这种度量比Jensen-Shannon散度更稳健，但有一个实际的缺点：它极其难以计算。由于我们无法处理所有可能的联合分布（也不能使用近似），需要进一步步骤来应用这个损失函数。在上述论文中，作者证明了可以通过Kantorovich-Rubinstein定理（这个主题相当复杂，但读者可以在*On
    the Kantorovich–Rubinstein Theorem*，Edwards D. *A*., *Expositiones Mathematicae*,
    2011中找到更多信息）应用一个变换：
- en: '![](img/d9eee8eb-17a7-4e32-93b1-43888c8c6ee6.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d9eee8eb-17a7-4e32-93b1-43888c8c6ee6.png)'
- en: 'The first element to consider is the nature of *f(•)*. The theorem imposes
    considering only L-Lipschitz functions, which means that *f(•)* (assuming a real-valued
    function of a single variable) must obey:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先考虑的是*f(•)*的性质。定理要求只考虑L-Lipschitz函数，这意味着*f(•)*（假设是一个单变量的实值函数）必须遵守：
- en: '![](img/c7dc640f-292c-4ed7-a144-4fa6ff1f6ff2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c7dc640f-292c-4ed7-a144-4fa6ff1f6ff2.png)'
- en: 'At this point, the Wasserstein distance is proportional to the supremum (with
    respect to all L-Lipschitz functions) of the difference between two expected values,
    which are extremely easy to compute. In a WGAN, the *f(•)* function is represented
    by a neural network; therefore, we have no warranties about the Lipschitz condition.
    To solve this problem, the author suggested a very simple procedure: clipping
    the discriminator (which is normally called Critic, and whose responsibility is
    to represent the parameterized function *f(•)*) variables after applying the corrections.
    If the input is bounded, all of the transformations will yield a bounded output;
    however, the clipping factor must be small enough (0.01, or even smaller) to avoid
    the additive effect of multiple operations leading to an inversion of the Lipschitz
    condition. This is not an efficient solution (because it slows down the training
    process when it''s not necessary), but it allows for exploiting the Kantorovich-Rubinstein
    theorem, even when there are no formal constraints imposed on the function family.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 到这一点，Wasserstein距离与两个期望值之差的上确界（相对于所有L-Lipschitz函数）成正比，这两个期望值非常容易计算。在WGAN中，*f(•)*函数由一个神经网络表示；因此，我们没有关于Lipschitz条件的保证。为了解决这个问题，作者提出了一种非常简单的程序：在应用校正后裁剪判别器（通常称为Critic，其责任是表示参数化函数*f(•)*）变量。如果输入是有界的，所有的变换都将产生有界输出；然而，裁剪因子必须足够小（0.01，甚至更小），以避免多次操作产生的加性效应导致Lipschitz条件的反转。这不是一个有效的解决方案（因为它在不必要的时候会减慢训练过程），但它允许在没有任何形式约束函数族的情况下利用Kantorovich-Rubinstein定理。
- en: 'Using a parameterized function (such as a Deep Convolutional Network), the
    Wasserstein distance becomes as follows (omitting the term *L*, which is constant):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用参数化函数（例如深度卷积网络），Wasserstein距离如下（省略常数项 *L*）：
- en: '![](img/a63cc711-96c9-4bd8-ad48-731742488753.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a63cc711-96c9-4bd8-ad48-731742488753.png)'
- en: In the previous expression, we explicitly extracted the generator output, and
    in the last step, separated the term that will be optimized separately. The reader
    has probably noticed that the computation is simpler than a standard GAN because,
    in this case, we have to average over only the *f(•)* values of a batch (there's
    no more need for a logarithm). However, as the Critic variables are clipped, the
    number of required iterations is normally larger, and in order to compensate the
    difference between the training speeds of the Critic and generator, it's often
    necessary to set *N[critic] > 1* (the authors suggest a value equal to 5, but
    this is a hyperparameter that must be tuned in every specific context).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的表达式中，我们明确提取了生成器的输出，并在最后一步，将将单独优化的项分离出来。读者可能已经注意到，计算比标准的GAN要简单，因为在这种情况下，我们只需要对一批的*f(•)*值进行平均（不再需要对数）。然而，由于Critic变量被裁剪，所需的迭代次数通常更多，为了补偿Critic和生成器训练速度之间的差异，通常需要设置*N[critic]
    > 1*（作者建议值为5，但这是一个必须在每个特定环境中调整的超参数）。
- en: 'The complete WGAN algorithm is:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的WGAN算法是：
- en: Set the number of epochs, *N[epochs.]*
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置epoch的数量，*N[epochs]*。
- en: Set the number of Critic iterations, *N[critic]* (in most cases, *N[iter]* =
    5).
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置Critic迭代的次数，*N[critic]*（在大多数情况下，*N[iter]* = 5）。
- en: Set the batch size, *k.*
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置批量大小，*k*。
- en: Set a clipping constant, c (for example, c = 0.01).
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置裁剪常数，c（例如，c = 0.01）。
- en: Define a noise generating process, M (for example, *U(-1, 1)*).
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个噪声生成过程，M（例如，*U(-1, 1)*）。
- en: 'For *e=1* to *N[epochs]*:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*e=1*到*N[epochs]*：
- en: Sample *k* values from *X*.
  id: totrans-95
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*X*中采样*k*个值。
- en: Sample *k* values from *N*.
  id: totrans-96
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*N*中采样*k*个值。
- en: 'For *i=1* to *N[critic]*:'
  id: totrans-97
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于*i=1*到*N[critic]*：
- en: Compute the gradients, *∇[c] D[W](p[data]||p[g])* (only with respect to the
    Critic variables). The expected values are approximated by sample means.
  id: totrans-98
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度，*∇[c] D[W](p[data]||p[g])*（仅针对Critic变量）。期望值通过样本均值来近似。
- en: Update the Critic parameters by Stochastic Gradient Ascent.
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机梯度上升更新Critic参数。
- en: Clip the Critic parameters in the range [*-c, c*].
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将Critic参数裁剪到范围[*-c, c*]内。
- en: Sample *k* values from *N*.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从*N*中采样*k*个值。
- en: Compute the gradients, *∇[g] W[noise]* (only with respect to the generator variables).
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算梯度，*∇[g] W[noise]*（仅针对生成器变量）。
- en: Update the generator parameters by Stochastic Gradient Descent.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过随机梯度下降更新生成器参数。
- en: Example of WGAN with TensorFlow
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: TensorFlow中的WGAN示例
- en: 'This example can be considered a variant of the previous one because it uses
    the same dataset, generator, and discriminator. The only main difference is that
    in this case, the discriminator (together with its variable scope) has been renamed `critic()`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例可以被视为上一个示例的变体，因为它使用了相同的dataset、generator和discriminator。唯一的主要区别是，在这种情况下，discriminator（连同其变量作用域）已被重命名为`critic()`：
- en: '[PRE6]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'At this point, we can step directly to the creation of the `Graph` containing
    all of the placeholders, operations, and loss functions:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们可以直接进入创建包含所有占位符、操作和损失函数的`Graph`：
- en: '[PRE7]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As it's possible to see, there are no differences in the placeholder section,
    in the definition of the generator, and in the image resizing to the target dimensions
    of 64 × 64\. In the next block, we define the two Critic instances (which are
    perfectly analogous to the ones declared in the previous example).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，占位符部分、生成器的定义以及将图像调整到64 × 64目标尺寸的操作之间没有差异。在下一个块中，我们定义了两个Critic实例（它们与上一个示例中声明的实例完全类似）。
- en: The two loss functions are simpler than a standard GAN, as they work directly
    with the Critic outputs, computing the sample mean over a batch. In the original
    paper, the authors suggest using RMSProp as the standard optimizer, in order to
    avoid the instabilities that a momentum-based algorithm can produce. However,
    Adam, with lower forgetting factors (*μ[1] = 0.5* and *μ[2] = 0.9*) and a learning
    rate *η = 0.00005*, is faster than RMSProp, and doesn't lead to instabilities.
    I suggest testing both options, trying to maximize the training speed while preventing
    the mode collapse. Contrary to the previous example, in this case we need to clip
    all of the Critic variables after each training step. To avoid that, the internal
    concurrency can alter the order of some operations; it's necessary to employ a
    nested dependency control context manager. In this way, the actual `training_step_c` (responsible
    for clipping and reassigning the values to each variable) will be executed only
    after the `optimizer_c` step has completed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 两个损失函数比标准的GAN更简单，因为它们直接与Critic输出一起工作，计算批次的样本均值。在原始论文中，作者建议使用RMSProp作为标准优化器，以避免基于动量的算法可能产生的稳定性问题。然而，Adam（具有更低的遗忘因子*μ[1]
    = 0.5*和*μ[2] = 0.9*以及学习率*η = 0.00005*）比RMSProp更快，并且不会导致不稳定性。我建议测试这两种选项，尝试最大化训练速度同时防止模式坍塌。与上一个示例相反，在这种情况下，我们需要在每个训练步骤之后剪辑所有的Critic变量。为了避免这种情况，内部并发可能会改变某些操作的顺序；需要使用嵌套依赖控制上下文管理器。这样，实际的`training_step_c`（负责剪辑并将值重新分配给每个变量）将仅在`optimizer_c`步骤完成后执行。
- en: 'Now, we can create the `InteractiveSession`, initialize the variables, and
    start the training process, which is very similar to the previous one:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建`InteractiveSession`，初始化变量，并开始训练过程，这与上一个示例非常相似：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The main difference is that, in this case, the Critic is trained `n_critic`
    times before each generator training step. The result of the generation of 50
    random samples is shown in the following screenshot:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 主要区别在于，在这种情况下，在每次生成器训练步骤之前，Critic会被训练`n_critic`次。以下截图显示了生成50个随机样本的结果：
- en: '![](img/e952ca1d-0bd6-4cdd-800d-3474c876432b.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e952ca1d-0bd6-4cdd-800d-3474c876432b.png)'
- en: Samples generated by a WGAN trained with the Fashion MNIST dataset
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Fashion MNIST数据集训练的WGAN生成的样本
- en: As it's possible to see, the quality is slightly higher, and the samples are
    smoother. I invite the reader to also test this model with an RGB dataset, because
    the final quality is normally excellent.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，质量略有提高，样本也更加平滑。我邀请读者也用RGB数据集测试这个模型，因为最终质量通常非常出色。
- en: When working with these models, the training time can be very long. To avoid
    waiting to see the initial results (and to perform the required tuning), I suggest
    using Jupyter. In this way, it's possible to stop the learning process, check
    the generator ability, and restart it without any problem. Of course, the graph
    must remain the same, and the variable initialization must be performed only at
    the beginning.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用这些模型时，训练时间可能会非常长。为了避免等待看到初始结果（以及进行必要的调整），我建议使用Jupyter。这样，就可以停止学习过程，检查生成器能力，并且可以无问题地重新启动。当然，图必须保持不变，并且变量初始化必须在开始时进行。
- en: Summary
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we discussed the main principles of adversarial training,
    and explained the roles of two players: the generator and discriminator. We described
    how to model and train them using a minimax approach whose double goal is to force
    the generator to learn the true data distribution *p[data]*, and get the discriminator
    to distinguish perfectly between true samples (belonging to *p[data]*) and unacceptable
    ones. In the same section, we analyzed the inner dynamics of a Generative Adversarial
    Network and some common problems that can slow down the training process and lead
    to a sub-optimal final configuration.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了对抗训练的主要原则，并解释了两个玩家的角色：生成器和判别器。我们描述了如何使用最小-最大方法来建模和训练它们，该方法的两个目标是迫使生成器学习真实数据分布
    *p[data]*，并使判别器能够完美地区分真实样本（属于 *p[data]*) 和不可接受的样本。在同一部分中，我们分析了生成对抗网络的内部动态和一些可能导致训练过程缓慢并导致最终配置次优的常见问题。
- en: One of the most difficult problems experienced with standard GANs arises when
    the data generating process and the generator distribution have disjointed support.
    In this case, the Jensen-Shannon divergence becomes constant and doesn't provide
    precise information about the distance. An excellent alternative is provided by
    the Wasserstein measure, which is employed in a more efficient model, called WGAN.
    This method can efficiently manage disjointed distributions, but it's necessary
    to enforce the L-Lipschitz condition on the Critic. The standard approach is based
    on clipping the parameters after each gradient ascent update. This simple technique
    guarantees the L-Lipschitz condition, but it's necessary to use very small clipping
    factors, and this can lead to a slower conversion. For this reason, it's normally
    necessary to repeat the training of the Critic a fixed number of times (such as
    five) before each single generator training step.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准 GAN 中遇到的最困难的问题之一出现在数据生成过程和生成器分布具有不连续支持时。在这种情况下，Jensen-Shannon 散度变为常数，不提供关于距离的精确信息。Wasserstein
    测度提供了一个极好的替代方案，它在称为 WGAN 的更有效模型中被采用。这种方法可以有效地管理不连续分布，但需要在 Critic 上强制执行 L-Lipschitz
    条件。标准方法基于在每次梯度上升更新后剪辑参数。这种简单技术保证了 L-Lipschitz 条件，但需要使用非常小的剪辑因子，这可能导致转换速度变慢。因此，通常在每次单独的生成器训练步骤之前，需要重复训练
    Critic 一个固定的次数（例如五次）。
- en: In the next chapter, we are going to introduce another probabilistic generative
    neural model, based on a particular kind of neural network, called the Restricted
    Boltzmann Machine.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍另一种基于特定类型神经网络的概率生成神经网络模型，这种神经网络被称为受限玻尔兹曼机。
