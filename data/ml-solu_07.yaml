- en: Chapter 7. Text Summarization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 文本摘要
- en: In this chapter, we will be building the summarization application. We will
    specifically focus on the textual dataset. Our primary goal is to perform the
    summarization task on medical notes. Basically, the idea is to come up with a
    good solution to summarize medical transcription documents.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建摘要应用。我们将特别关注文本数据集。我们的主要目标是针对病历进行摘要任务。基本上，我们的想法是提出一个很好的解决方案来总结医学转录文档。
- en: This kind of summarization application helps doctors a great manner. You ask
    how? Let's take an example. Suppose a patient has 10 years of history with a certain
    disease, and after 10 years, he consults a new doctor for better results. On the
    first day, the patient needs to hand over their last 10 years of medical prescriptions
    to this new doctor. After that, the doctor will need to study all these documents.
    The doctor also relies on the conversation he had with the patient. By using medical
    notes and conversations with the patient, the doctor can find out the patient's
    health status. This is quite a lengthy method.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 这种摘要应用以极大的方式帮助医生。你问如何？让我们举一个例子。假设一个患者有10年的某种疾病病史，10年后，他为了更好的结果而咨询一位新医生。在第一天，患者需要将他们过去10年的医疗处方交给这位新医生。之后，医生将需要研究所有这些文件。医生还依赖于他与患者之间的对话。通过使用病历和与患者的对话，医生可以了解患者的健康状况。这是一个相当费时的方法。
- en: 'However, what if we could generate a summary of the patient''s medical notes
    and provide these summarized documents to the doctor? It seems like a promising
    solution because this way, we can save the doctor''s time and efforts. Doctors
    can understand their patients'' issues in an efficient and accurate way. Patients
    can start getting treatment from their first meeting with the doctor. This is
    a win-win situation for both parties, and this kind of solution is what we are
    trying to build here. So, in this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们能够为患者的病历生成摘要并提供这些摘要文档给医生，会怎样呢？这似乎是一个很有前景的解决方案，因为这样我们可以节省医生的时间和精力。医生可以以高效和准确的方式了解他们的患者问题。患者可以从与医生的第一次会面开始接受治疗。这对双方都是双赢的局面，这正是我们在这里试图构建的解决方案。因此，在本章中，我们将涵盖以下主题：
- en: Understanding the basics of summarization
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解摘要的基本原理
- en: Introducing the problem statement
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: Understanding datasets
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集
- en: 'Building the baseline approach:'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建基线方法：
- en: Implementing the baseline approach
  id: totrans-8
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现基线方法
- en: Problems with the baseline approach
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基线方法的缺陷
- en: Optimizing the baseline approach
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化基线方法
- en: 'Building the revised approach:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建修订的方法：
- en: Implementing the revised approach
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现修订的方法
- en: Problems with the revised approach
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修订方法的缺陷
- en: Understanding how to improve the revised approach
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解如何改进修订的方法
- en: 'The best approach:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法：
- en: Implementing the best approach
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现最佳方法
- en: 'The best approach: building a summarization application for Amazon reviews'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳方法：为亚马逊评论构建摘要应用
- en: Summary
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要
- en: Understanding the basics of summarization
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解摘要的基本原理
- en: In this section, we will be focusing on the basic concepts of summarization.
    In today's fast-growing information age, text summarization has become an important
    tool. It will be difficult for humans to generate a summary for large text documents.
    There are lots of documents available on the web today. So, we need a solution
    that can automatically generate a summary for documents efficiently, accurately,
    and intelligently. This task is referred to as automatic text summarization.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将关注摘要的基本概念。在当今信息快速发展的时代，文本摘要已成为一个重要的工具。人类为大型文本文档生成摘要将变得困难。如今，网络上有很多文档。因此，我们需要一个能够高效、准确、智能地自动生成文档摘要的解决方案。这项任务被称为自动文本摘要。
- en: 'Automatic text summarization is all about finding relevant information from
    the large text document in a small amount of time. Basically, there are two types
    of summarization:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 自动文本摘要主要是关于在很短的时间内从大量文本文档中找到相关信息。基本上，有两种类型的摘要：
- en: Extractive summarization
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取式摘要
- en: Abstractive summarization
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 抽象式摘要
- en: Let's look at the types of summarization one by one.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐一看看摘要的类型。
- en: Extractive summarization
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 提取式摘要
- en: In the extractive summarization method, we will be generating a summary of the
    document by selecting words, phrases, or sentences from the original document.
    We will be using concepts such as **Term-Frequency, Inverse-Document Frequency**
    (**TF-IDF**), Count vectorizers, Cosine similarity, and the ranking algorithm
    to generate this type of summary.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在提取式摘要方法中，我们将通过从原始文档中选择单词、短语或句子来生成文档的摘要。我们将使用诸如**词频-逆文档频率（TF-IDF**）、计数向量器、余弦相似度和排名算法等概念来生成此类摘要。
- en: We have covered concepts such as TF-IDF, Count vectorizers, and Cosine similarity
    in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems for E-Commerce"),
    *Recommendation Systems for E-Commerce*, section *Understanding TF-IDF*. We will
    look at the ranking mechanism when we implement the code for it in this chapter.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第4章](ch04.xhtml "第4章。电子商务推荐系统") *电子商务推荐系统* 的 *理解TF-IDF* 部分介绍了TF-IDF、计数向量器和余弦相似度等概念。当我们在本章中实现代码时，我们将探讨排名机制。
- en: Abstractive summarization
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 抽象摘要
- en: In the abstractive summarization method, we will try and make the machine learn
    internal language representation so that it can generate more human-like summaries
    by paraphrasing.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在抽象摘要方法中，我们将尝试让机器学习内部语言表示，以便通过释义生成更类似人类的摘要。
- en: In order to implement this type of summarization, we will be using deep learning
    algorithms such as a sequence-to-sequence model with an attention mechanism. You
    will learn about the algorithm and concepts later on in this chapter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现此类摘要，我们将使用深度学习算法，如具有注意力机制的序列到序列模型。你将在本章后面部分了解该算法和概念。
- en: Introducing the problem statement
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍问题陈述
- en: At the beginning of the chapter, we already looked at an overview of the problem
    statement. Here, we will be delving into further details. We want to build an
    automatic text summarization application. We will be providing a medical transcription
    document as the input. Our goal is to generate the summary of this document. Note
    that here, we are going to provide a single document as the input, and as an output,
    we will be generating the summary of that single document. We want to generate
    an informative summary for the document. An informative summary is a type of summary
    where the summarization document is a substitute of the original document as far
    as the converging of information is concerned. This is because we are dealing
    with the medical domain.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们已经概述了问题陈述。在这里，我们将进一步深入细节。我们想要构建一个自动文本摘要应用程序。我们将提供一个医疗转录文档作为输入。我们的目标是生成该文档的摘要。请注意，在这里，我们将提供一个单独的文档作为输入，并且作为输出，我们将生成该单个文档的摘要。我们希望生成一个信息性摘要。信息性摘要是一种摘要，其中摘要文档在信息聚合方面是原始文档的替代品。这是因为我们正在处理医疗领域。
- en: Initially, we use extractive summarization methods in our approaches. We will
    be generating the extractive summary for a medical document. Later on in this
    chapter, we will be also developing a solution that can generate an abstractive
    summarization of Amazon reviews.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我们在我们的方法中使用提取式摘要方法。我们将为医疗文档生成提取式摘要。在本章的后面部分，我们还将开发一个可以生成亚马逊评论抽象摘要的解决方案。
- en: Now, it is time to explore the dataset and look at the challenges we have faced
    in accessing the dataset.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候探索数据集并查看我们在访问数据集时面临的挑战了。
- en: Understanding datasets
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解数据集
- en: This section is divided into two parts. In the first part, we need to discuss
    the challenges we have faced in order to generate the dataset. In the later section,
    we will be discussing the attributes of the dataset.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 本节分为两部分。在第一部分，我们需要讨论我们在生成数据集时面临的挑战。在后面的部分，我们将讨论数据集的属性。
- en: Challenges in obtaining the dataset
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据集的挑战
- en: 'As we all know, the health domain is a highly regulated domain when it comes
    to obtaining the dataset. These are some of the challenges I want to highlight:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所知，在获取数据集方面，健康领域是一个高度监管的领域。以下是我想要强调的一些挑战：
- en: For summarization, ideally, we need to have a corpus that contains original
    text as well as a summary of that text. This is called parallel corpus. Unfortunately,
    there is no good, free parallel corpus available for medical document summarization.
    We need to obtain this kind of parallel dataset for the English language.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于摘要，理想情况下，我们需要一个包含原始文本及其摘要的语料库，这被称为并行语料库。不幸的是，目前没有好的、免费的并行语料库可用于医学文档摘要。我们需要获取这种类型的英语并行数据集。
- en: There are some free datasets available, such as the MIMIC II and MIMIC III dataset,
    but they won't contain summaries of the medical transcription. We can access just
    the medical transcription from this dataset. Gaining access to this dataset is
    a lengthy and time-consuming process.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有一些免费的数据集可用，例如MIMIC II和MIMIC III数据集，但它们不会包含医学转录的摘要。我们只能从这个数据集中获取医学转录。获取这个数据集是一个漫长且耗时的过程。
- en: In order to solve the preceding challenges, professionals, researchers, academics,
    and big tech companies need to come forward and make good quality, freely available
    datasets for the medical domain. Now let's look at how to get the medical transcription
    dataset.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决上述挑战，专业人士、研究人员、学者和大型科技公司需要站出来，为医学领域提供高质量、免费的数据集。现在让我们看看如何获取医学转录数据集。
- en: Understanding the medical transcription dataset
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解医学转录数据集
- en: 'You might wonder if we do not have a parallel dataset with us, then how will
    we build the summarization application? There is a workaround here. I have a sample
    medical transcription from the MIMIC – II dataset. We will be using them and generating
    an extractive summary of the documents. Apart from that, we will be referring
    to [www.mtsamples.com](http://www.mtsamples.com) in order to get an idea about
    the different kind of medical transcriptions we could possibly have. With the
    help of a minimum number of documents, we are going to build the summarization
    application. You can see what these medical transcriptions will look like in the
    following figure:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道如果没有并行数据集，我们如何构建摘要应用？这里有一个解决方案。我有一个来自MIMIC – II数据集的样本医学转录。我们将使用这些数据并生成文档的提取摘要。除此之外，我们还将参考[www.mtsamples.com](http://www.mtsamples.com)以了解我们可能拥有的不同类型的医学转录。通过使用最少数量的文档，我们将构建摘要应用。你可以在下面的图中看到这些医学转录的样子：
- en: '![Understanding the medical transcription dataset](img/B08394_07_01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![理解医学转录数据集](img/B08394_07_01.jpg)'
- en: 'Figure 7.1: Sample medical transcription'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：样本医学转录
- en: 'Generally, in medical transcriptions, there are a couple of sections, and they
    are as follows:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，在医学转录中，有几个部分，具体如下：
- en: '**Chief complaint**: This section describes the main problem or disease that
    the patient is facing'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主诉**: 本节描述了患者面临的主要问题或疾病'
- en: '**Hist****ory of patient''s illness**: This section has a detailed description
    of the patient''s medical status and their history of a similar disease or other
    kinds of diseases'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**患者病史**: 本节详细描述了患者的医疗状况及其类似疾病或其他类型疾病的病史'
- en: '**Past medical history**: This section describes the name of the diseases that
    the patient had in past'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**既往病史**: 本节描述了患者过去所患疾病的名称'
- en: '**Past surgical history**: If the patient had any surgeries in the past, then
    the name of those surgeries is mentioned here'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**既往手术史**: 如果患者过去有过手术，那么这些手术的名称将在此处提及'
- en: '**Family history**: If any family member has the same type of disease or a
    history of certain kinds of diseases in the family, then those are mentioned in
    this section'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**家族史**: 如果家庭成员有相同类型的疾病或家族中存在某些疾病的病史，那么这些信息将在此节中提及'
- en: '**Medications**: This section describes the medicine names'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**药物**: 本节描述了药物名称'
- en: '**Physical examination**: This section has all the descriptions related to
    physical examinations'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**体格检查**: 本节包含所有与体格检查相关的描述'
- en: '**Assessment**: This section contains the details about the potential disease
    the patient may have after taking all preceding parameters into consideration.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估**: 本节包含在考虑所有先前参数后，患者可能患有的潜在疾病的详细信息'
- en: '**Recommendations**: This section describes the recommended solution for the
    patient''s complaints'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**建议**: 本节描述了针对患者投诉的推荐解决方案'
- en: '**Keywords**: This section has the keywords that can describe the entire document
    properly so the dataset can be used for the topic modeling task as well'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键词**：本节包含可以正确描述整个文档的关键词，因此数据集也可以用于主题建模任务'
- en: This kind of transcription is random in certain sections. Some transcriptions
    contain all the preceding sections, and some do not. So, the number of sections
    for this kind of document may vary a lot.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种转录在某个部分是随机的。一些转录包含所有前面的部分，而一些则不包含。因此，这种文档的章节数量可能会有很大的变化。
- en: Now let's look at details related to the Amazon review dataset.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看与亚马逊评论数据集相关的细节。
- en: Understanding Amazon's review dataset
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解亚马逊评论数据集
- en: 'Later on in this chapter, we will be using the Amazon review dataset in order
    to generate the abstractive summary. So, it is better if you understand basic
    data attributes for this dataset. First of all, you can download that dataset
    by using this link: [https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data](https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data).
    The name of the file you need to download is `Reviews.csv`.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的后面部分，我们将使用亚马逊评论数据集来生成摘要。因此，了解这个数据集的基本数据属性会更好。首先，您可以通过使用此链接下载该数据集：[https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data](https://www.kaggle.com/currie32/summarizing-text-with-amazon-reviews/data)。您需要下载的文件名为`Reviews.csv`。
- en: 'You can look at the content of this dataset by referring to the following screenshot:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过参考以下截图来查看这个数据集的内容：
- en: '![Understanding Amazon''s review dataset](img/B08394_07_02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![理解亚马逊评论数据集](img/B08394_07_02.jpg)'
- en: 'Figure 7.2: Data records from Amazon''s review dataset'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：来自亚马逊评论数据集的数据记录
- en: 'Let''s understand each of the data attributes of this dataset:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们了解这个数据集的每个数据属性：
- en: '`ID`: This attribute indicates the serial number for data records.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ID`：这个属性表示数据记录的序列号。'
- en: '`ProductId`: This attribute indicates the unique ID for the particular product.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ProductId`：这个属性表示特定产品的唯一ID。'
- en: '`UserId`: This attribute indicates the unique user ID of the user who has shared
    their review for a particular product.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UserId`：这个属性表示分享特定产品评论的用户的唯一用户ID。'
- en: '`ProfileName`: This data attribute is the user''s profile name. Using this
    profile name, the user will have submitted their review.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ProfileName`：这个数据属性是用户的个人资料名称。使用这个个人资料名称，用户将提交他们的评论。'
- en: '`HelpfulnessNumerator`: This attribute indicates how many other users found
    this review useful in a positive way.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HelpfulnessNumerator`：这个属性表示有多少其他用户认为这条评论在积极方面是有用的。'
- en: '`HelpfulnessDenominator`: This attribute indicates the total number of users
    who voted as to whether this review was useful or not useful.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`HelpfulnessDenominator`：这个属性表示对这条评论是否有用进行投票的总用户数。'
- en: '`Score`: This is the score for a particular product. Zero means the user didn''t
    like it, and five means the user liked it a lot.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Score`：这是特定产品的评分。零表示用户不喜欢它，而五表示用户非常喜欢它。'
- en: '`Time`: This attribute indicates the timestamp at which the review has been
    submitted.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Time`：这个属性表示评论提交的时间戳。'
- en: '`Summary`: This attribute is quite useful as it indicates the summary for the
    entire review.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Summary`：这个属性非常有用，因为它表示整个评论的摘要。'
- en: '`Text`: This attribute is the long text review for any given product.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Text`：这个属性是任何给定产品的长文本评论。'
- en: Now we have looked at both the datasets. Let's move on to the next section.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看过这两个数据集了。让我们继续到下一节。
- en: Building the baseline approach
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建基线方法
- en: 'In this section, we will be implementing the baseline approach for the summarization
    application. We will be using medical transcriptions to generate the summary.
    Here we will be using a small trial MIMIC-II dataset which contains a few sample
    medical documents and [www.mtsamples.com](http://www.mtsamples.com) for getting
    medical transcriptions. You can find the code by using this GitHub link: [https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach).'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将实现文本摘要应用的基线方法。我们将使用医疗转录来生成摘要。在这里，我们将使用一个小型的MIMIC-II数据集进行试验，该数据集包含一些样本医疗文档，以及[www.mtsamples.com](http://www.mtsamples.com)来获取医疗转录。您可以通过使用此GitHub链接找到代码：[https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Base_line_approach)。
- en: Let's start building the baseline approach.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始构建基线方法。
- en: Implementing the baseline approach
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现基线方法
- en: 'Here, we will be performing the following steps in order to build the baseline
    approach:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在此，我们将按照以下步骤构建基线方法：
- en: Install python dependencies
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Python 依赖项
- en: Write code and generate summary
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写代码并生成摘要
- en: Installing python dependencies
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 安装 Python 依赖项
- en: 'We will be using two python dependencies, which are really easy to use, in
    order to develop the summarization application. One is `PyTeaser`, and the second
    one is `Sumy`. You need to execute the following commands in order to install
    these two dependencies:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开发摘要应用程序，我们将使用两个非常容易使用的 Python 依赖项。一个是 `PyTeaser`，另一个是 `Sumy`。您需要执行以下命令来安装这两个依赖项：
- en: '[PRE0]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the `PyTeaser` library works only with `python 2.7`. Sumy can work
    with `python 2.7` and `python 3.3+`.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`PyTeaser` 库仅适用于 `python 2.7`。Sumy 可以与 `python 2.7` 和 `python 3.3+` 一起使用。
- en: Now let's write the code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们编写代码。
- en: Writing the code and generating the summary
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编写代码和生成摘要
- en: 'Both the `PyTeaser` and Sumy libraries have great features. They take any weburl
    as the input and generate the summary for the given weburl. You can refer to the
    code snippet given in the following screenshot:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`PyTeaser` 和 Sumy 库都拥有出色的功能。它们可以将任何网页URL作为输入，并为给定的网页URL生成摘要。您可以参考以下截图中的代码片段：'
- en: '![Writing the code and generating the summary](img/B08394_07_03.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![编写代码和生成摘要](img/B08394_07_03.jpg)'
- en: 'Figure 7.3: Code snippet for generating summarization using PyTeaser'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3：使用 PyTeaser 生成摘要的代码片段
- en: 'As you can see, we are passing the weburl of the sample medical transcription
    from [www.mtsamples.com](http://www.mtsamples.com). The `PyTeaser` library will
    generate the top five sentences of the document as the summary. To view the output,
    you can take a look at the the following screenshot:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，我们正在传递来自 [www.mtsamples.com](http://www.mtsamples.com) 的样本医学转录的 weburl。`PyTeaser`
    库将生成文档的前五句话作为摘要。要查看输出，您可以查看以下截图：
- en: '![Writing the code and generating the summary](img/B08394_07_04.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![编写代码和生成摘要](img/B08394_07_04.jpg)'
- en: 'Figure 7.4: Summary for the medical transcription using PyTeaser'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4：使用 PyTeaser 对医学转录的摘要
- en: 'Now let''s try out the `Sumy` library. You can refer to the code given in the
    following screenshot:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们尝试使用 `Sumy` 库。您可以参考以下截图中的代码：
- en: '![Writing the code and generating the summary](img/B08394_07_05.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![编写代码和生成摘要](img/B08394_07_05.jpg)'
- en: 'Figure 7.5: Code snippet for generating summarization using Sumy'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.5：使用 Sumy 生成摘要的代码片段
- en: 'In the `Sumy` library, we need to pass the weburl as the input, but there is
    one difference. As you can see in the preceding code, we have provided `SENTENCES_COUNT
    = 10`, which means our summary or output has 10 sentences. We can control the
    number of statements by using the `SENTENCES_COUNT` parameter. You can refer to
    the output given in the following figure:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `Sumy` 库中，我们需要将 weburl 作为输入，但有一个区别。正如您在前面的代码中所看到的，我们提供了 `SENTENCES_COUNT =
    10`，这意味着我们的摘要或输出有 10 个句子。我们可以通过使用 `SENTENCES_COUNT` 参数来控制语句的数量。您可以参考以下图中的输出：
- en: '![Writing the code and generating the summary](img/B08394_07_06.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![编写代码和生成摘要](img/B08394_07_06.jpg)'
- en: 'Figure 7.6: Summary for medical transcription using Sumy'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6：使用 Sumy 对医学转录的摘要
- en: If you view and compare the output of the `Sumy` and `PyTeaser` libraries, then
    you could say that the `Sumy` library is performing really well compared to the
    `PyTeaser` library. As you can see, both these libraries obtain a basic summary
    of the given document. These libraries are using the ranking algorithm and the
    frequency of the words in order to obtain the summaries. We don't have control
    over their internal mechanisms. You might be wondering whether we can make our
    own summarization so that we can optimize the code as and when needed. The answer
    is yes; we can develop our code for this task. Before that, let's discuss the
    shortcomings of this approach, and then we will build our own code with the revised
    approach.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看并比较 `Sumy` 和 `PyTeaser` 库的输出，那么您可以说与 `PyTeaser` 库相比，`Sumy` 库的表现非常好。如您所见，这两个库都获得了给定文档的基本摘要。这些库正在使用排名算法和单词频率来获取摘要。我们无法控制它们的内部机制。您可能会想知道我们是否可以自己进行摘要，以便我们可以根据需要优化代码。答案是肯定的；我们可以为这个任务开发我们的代码。在那之前，让我们讨论这个方法的缺点，然后我们将使用改进的方法构建自己的代码。
- en: Problems with the baseline approach
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基线方法的问题
- en: 'Here, we will be discussing the shortcomings of the baseline approach so that
    we can take care of these disadvantages in the next iteration:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将讨论基线方法的缺点，以便我们可以在下一次迭代中注意这些缺点：
- en: As mentioned earlier, we do not have full ownership over the code of these libraries.
    So, we cannot change or add functionalities easily.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如前所述，我们并不完全拥有这些库的代码。因此，我们无法轻松地更改或添加功能。
- en: We have obtained a basic kind of summary, so we need to improve the result of
    the summary.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经获得了一种基本的摘要类型，因此我们需要提高摘要的结果。
- en: Because of the lack of a parallel corpus, we cannot build a solution that can
    generate an abstractive summary for medical documents.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于缺乏平行语料库，我们无法构建一个可以为医学文档生成摘要的解决方案。
- en: These are three main shortcomings of the baseline approach, and we need to solve
    them. In this chapter, we will be focusing on first and second shortcomings. For
    the third shortcoming, we cannot do much about it. So, we have to live with that
    shortcoming.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是基线方法的三个主要缺点，我们需要解决它们。在本章中，我们将重点关注前两个缺点。对于第三个缺点，我们无法做太多。所以，我们必须忍受这个缺点。
- en: Let's discuss how we will be optimizing this approach.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论我们将如何优化这种方法。
- en: Optimizing the baseline approach
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优化基线方法
- en: 'In this section, we will be discussing how to optimize the baseline approach.
    We will be implementing a simple summarization algorithm. The idea behind this
    algorithm is simple: This approach is also generating an extractive summary for
    the medical document. We need to perform the following steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何优化基线方法。我们将实现一个简单的摘要算法。这个算法背后的思想很简单：这种方法也为医学文档生成提取摘要。我们需要执行以下步骤：
- en: First, we need to determine the frequencies of the words in the given document.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要确定给定文档中单词的频率。
- en: The, we split the document into a series of sentences.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将文档拆分为一系列句子。
- en: In order to generate the summary, we select the sentences that have more frequent
    words.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了生成摘要，我们选择具有更多频繁单词的句子。
- en: Finally, we reorder summarize sentences so that the generated output is aligned
    with the original document.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们重新排序摘要句子，以确保生成的输出与原始文档对齐。
- en: The preceding algorithm can solve our two shortcomings, although we may need
    help with the third one because right now, there is no availability of the dataset
    that can be used in the summarization task, especially in the medical domain.
    For this chapter, we have to live with this shortcoming (unfortunately, we don't
    have any other option), but don't worry. This doesn't mean we will not learn how
    to generate the abstractive summary. In order to learn how to generate abstractive
    summaries, we will be using the Amazon review dataset later on this chapter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的算法可以解决我们的两个缺点，尽管我们可能需要帮助来解决第三个缺点，因为目前没有可用于摘要任务的语料库，尤其是在医学领域。对于这一章，我们必须忍受这个缺点（不幸的是，我们没有其他选择），但不用担心。这并不意味着我们不会学习如何生成摘要。为了学习如何生成摘要，我们将在本章后面使用亚马逊评论数据集。
- en: Now let's implement the steps of the algorithms that we described in this section.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现本节中描述的算法步骤。
- en: Building the revised approach
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建修订的方法
- en: 'Now we will be coding the algorithm that we discussed in the previous section.
    After implementing it, we will check how well or badly our algorithm is performing.
    This algorithm is easy to implement, so let''s begin with the code. You can find
    the code at this GitHub link: [https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将编写我们在上一节中讨论的算法。实现后，我们将检查我们的算法表现得好还是不好。这个算法很容易实现，所以让我们从代码开始。您可以在以下GitHub链接中找到代码：[https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Revised_approach)。
- en: Implementing the revised approach
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施修订的方法
- en: 'In this section, we will be implementing the summarization algorithm step by
    step. These are the functions that we will be building here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步实现摘要算法。以下是我们将在这里构建的函数：
- en: The get_summarized function
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`get_summarized` 函数'
- en: The reorder_sentences function
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`reorder_sentences` 函数'
- en: The summarize function
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摘要函数
- en: Let's begin with the first one.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个开始。
- en: The get_summarized function
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '`get_summarized` 函数'
- en: 'Basically, this function performs the summarization task. First, it will take
    the content of the document as input in the form of string. After that, this function
    generates the frequency of the words, so we need to tokenize the sentences into
    words. After that, we will be generating the top 100 most frequent words from
    the given document. For small of dataset, the top 100 most frequent words can
    describe the vocabulary of the given dataset really well so we are not considering
    more words. If you have large dataset, then you can consider the top 1,000 or
    top 10,000 most frequent words based on the size of the dataset. You can refer
    to the code given in the following figure:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这个函数执行摘要任务。首先，它将以字符串形式将文档内容作为输入。然后，这个函数生成单词的频率，因此我们需要将句子标记化成单词。之后，我们将从给定文档中生成前100个最频繁的单词。对于小数据集，前100个最频繁的单词可以很好地描述给定数据集的词汇，所以我们不考虑更多的单词。如果您有大数据集，则可以根据数据集的大小考虑前1,000个或前10,000个最频繁的单词。您可以参考以下图中的代码：
- en: '![The get_summarized function](img/B08394_07_07.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![get_summarized函数](img/B08394_07_07.jpg)'
- en: 'Figure 7.7: Code snippet for generating the most frequent words from the given
    input document'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7：从给定输入文档生成最频繁单词的代码片段
- en: 'Now let''s code the second step. We need to split the documents into sentences.
    We will convert the sentences into lowercase. We will use the NLTK sentence splitter
    here. You can refer to the code given in the following figure:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来编写第二步的代码。我们需要将文档拆分成句子。我们将句子转换为小写。在这里，我们将使用NLTK句子分割器。您可以参考以下图中的代码：
- en: '![The get_summarized function](img/B08394_07_08.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![get_summarized函数](img/B08394_07_08.jpg)'
- en: 'Figure 7.8: Code snippet for generating sentences from the input document'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8：从输入文档生成句子的代码片段
- en: 'In the third step, we will iterate over the list of the most frequent words
    and find out the sentences that include a higher amount of frequent words. You
    can refer to the code shown in the following figure:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三步中，我们将遍历最频繁单词的列表，找出包含更多频繁单词的句子。您可以参考以下图中的代码：
- en: '![The get_summarized function](img/B08394_07_09.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![get_summarized函数](img/B08394_07_09.jpg)'
- en: 'Figure 7.9: Code snippet for generating the sentence that has a higher amount
    of frequent words'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9：生成包含更多频繁单词的句子的代码片段
- en: Now it's time to rearrange the sentences so that the sentence order aligns with
    the original input document.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候重新排列句子，以便句子顺序与原始输入文档一致。
- en: The reorder_sentences function
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: reorder_sentences函数
- en: 'This function basically reorders the summarized sentence so that all the sentences
    align with the order of the sentences of the original document. We will take summarized
    sentences and sentences from the original document into consideration and perform
    the sorting operation. You can refer to the code given in the following figure:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数基本上重新排列了总结的句子，使得所有句子都与原始文档的句子顺序一致。我们将考虑总结句子和原始文档中的句子，并执行排序操作。您可以参考以下图中的代码：
- en: '![The reorder_sentences function](img/B08394_07_10.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![reorder_sentences函数](img/B08394_07_10.jpg)'
- en: 'Figure 7.10: Code snippet for reordering the summarized sentences'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10：重新排列总结句子的代码片段
- en: Now let's move on to the final step.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续到最后一步。
- en: The summarize function
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: summarize函数
- en: 'This function basically generates the summary. This is the method that we can
    call from any other file. Here, we need to pass the input data and the number
    of sentences we need in the summarized content. You can refer to the code that
    is displayed in the following figure:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数基本上生成摘要。这是我们可以在任何其他文件中调用的方法。在这里，我们需要传递输入数据和所需总结内容中的句子数。您可以参考以下图中的代码：
- en: '![The summarize function](img/B08394_07_11.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![summarize函数](img/B08394_07_11.jpg)'
- en: 'Figure 7.11: Code snippet for defining the function that can be called outside
    of the class'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11：定义可以在类外调用的函数的代码片段
- en: Generating the summary
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成摘要
- en: 'Now let''s look at a demonstration of this this code and generate the summary
    for the document. We will pass the textual content from [www.mtsamples.com](http://www.mtsamples.com)
    and then try to generate a summary of the content. You can refer to the code snippet
    given in the following figure:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下这个代码的演示，并为文档生成摘要。我们将传递来自[www.mtsamples.com](http://www.mtsamples.com)的文本内容，然后尝试生成内容摘要。您可以参考以下图中的代码片段：
- en: '![Generating the summary](img/B08394_07_12.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![生成摘要](img/B08394_07_12.jpg)'
- en: 'Figure 7.12: Code snippet to call the summarized function'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12：调用摘要函数的代码片段
- en: 'The output of the preceding code is given in the following figure:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出如下所示：
- en: '![Generating the summary](img/B08394_07_13.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![生成摘要](img/B08394_07_13.jpg)'
- en: 'Figure 7.13: Output for the revised approach'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13：修订方法的输出
- en: As you can see, the output is more relevant than the baseline approach. We know
    the approach for the kind of steps we have been performing so far. This approach
    gives us clarity about how we can generate the extractive summary for the medical
    transcription. The good part of this approach is that we do not need any parallel
    summarization corpus.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，输出比基线方法更相关。我们知道我们迄今为止所执行步骤的方法。这种方法让我们清楚地了解如何为医学转录生成提取摘要。这种方法的好处是，我们不需要任何并行摘要语料库。
- en: Now let's discuss the shortcomings of the revised approach.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来讨论修订方法的缺点。
- en: Problems with the revised approach
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修订方法的问题
- en: 'In this section, we will be discussing the shortcomings of the revised approach,
    as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论修订方法的缺点，如下所述：
- en: The revised approach does not have the ranking mechanism to rank the sentences
    based on their importance.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修订方法没有排名机制来根据其重要性对句子进行排名。
- en: We have considered word frequencies so far; we have not considered their importance
    with respect to the other words. Suppose word *a* appears a thousand times in
    a document. That doesn't mean it carries more importance.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了词频；我们还没有考虑它们相对于其他词的重要性。假设词*a*在一篇文档中出现了1000次。这并不意味着它具有更大的重要性。
- en: Now let's see how we can overcome these shortcomings.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看我们如何克服这些缺点。
- en: Understanding how to improve the revised approach
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解如何改进修订方法
- en: In this section, we will be discussing the steps that we should take in order
    to improve the revised approach. To obtain the best result for extractive summarization,
    we need to use TF-IDF and the sentence ranking mechanism to generate the summary.
    We have covered TF-IDF in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for E-Commerce*, in the *Generating
    features using TF-IDF* section. We will be building the ranking mechanism by using
    cosine similarity and LSA (Latent Semantic Analysis). We have already looked at
    cosine similarity in [Chapter 4](ch04.xhtml "Chapter 4. Recommendation Systems
    for E-Commerce"), *Recommendation Systems for E-Commerce*. Let's explore the LSA
    algorithm.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论我们应该采取的步骤来改进修订方法。为了获得提取摘要的最佳结果，我们需要使用TF-IDF和句子排名机制来生成摘要。我们在[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")的*“使用TF-IDF生成特征”*部分已经介绍了TF-IDF。我们将通过使用余弦相似度和LSA（潜在语义分析）来构建排名机制。我们已经在[第4章](ch04.xhtml
    "第4章。电子商务推荐系统")中了解了余弦相似度。让我们来探索LSA算法。
- en: The LSA algorithm
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: LSA算法
- en: 'The LSA algorithm is similar to the cosine similarity. We will generate the
    matrix by using the words present in the paragraphs of the document. The row of
    the matrix will represent the unique words present in each paragraph, and columns
    represent each paragraph. You can view the matrix representation for the LSA algorithm
    in the following figure:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: LSA算法类似于余弦相似度。我们将使用文档段落中的单词生成矩阵。矩阵的行将代表每个段落中存在的唯一单词，列代表每个段落。您可以在以下图中查看LSA算法的矩阵表示：
- en: '![The LSA algorithm](img/B08394_07_14.jpg)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![LSA算法](img/B08394_07_14.jpg)'
- en: 'Figure 7.14: Matrix representation for the LSA algorithm'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14：LSA算法的矩阵表示
- en: The basic assumption for the LSA algorithm is that words that are close in their
    meaning will occur in a similar piece of text. As you can see from the preceding
    example, if we say that the word pair (cat, is) occurs more frequently, it means
    that it carries higher semantic meaning than the (cat, mouse) word pair. This
    is the meaning of the assumption behind the algorithm. We generate the matrix
    that is given in the previous figure and then try to reduce the number of rows
    of the matrix by using the **single value decomposition** (**SVD**) method. SVD
    is basically a factorization of the matrix.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: LSA算法的基本假设是，意义相近的单词将出现在相似文本中。正如您从前面的例子中可以看到的，如果我们说单词对（猫，是）出现的频率更高，这意味着它比（猫，老鼠）单词对具有更高的语义意义。这是算法背后的假设的含义。我们生成前面图中给出的矩阵，然后尝试使用**奇异值分解**（**SVD**）方法减少矩阵的行数。SVD基本上是矩阵的分解。
- en: Note
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'You can read more on SVD by using this link: [https://en.wikipedia.org/wiki/Singular-value_decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下链接了解更多关于奇异值分解（SVD）的信息：[https://en.wikipedia.org/wiki/Singular-value_decomposition](https://en.wikipedia.org/wiki/Singular-value_decomposition)。
- en: Here, we are reducing the number of rows (which means the number of words) while
    preserving the similarity structure among columns (which means paragraphs). In
    order to generate the similarity score between word pairs, we are using cosine
    similarity. This is more than enough to keep in mind in order to build the summarization
    application.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们正在减少行数（即单词的数量），同时保留列（即段落）之间的相似性结构。为了生成单词对之间的相似度分数，我们使用余弦相似度。这对于构建摘要应用来说已经足够了。
- en: Now let's discuss the approach we are taking in order to build the best possible
    solution for generating an extractive summary for medical documents.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论我们采取的方法，以构建生成医学文档提取摘要的最佳解决方案。
- en: The idea behind the best approach
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最佳方法背后的理念
- en: 'We will perform the following steps in order to build the best approach:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行以下步骤以构建最佳方法：
- en: First of all, we will take the content of the document in the form of a string.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将以字符串的形式获取文档的内容。
- en: We will parse the sentence, and after that, we will remove the stop words and
    special characters. We will be converting the abbreviations into their full forms.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将解析句子，然后移除停用词和特殊字符。我们将缩写词转换为它们的完整形式。
- en: 'After that, we will generate the lemma of the words and their **Part-of-Speech**
    (**POS**) tags. Lemma is nothing but the root form of words and POS tags indicate
    whether the word is used as a verb, noun, adjective, or adverb. There are many
    POS tags available. You can find a list of POS tags at this site: [https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html).'
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 之后，我们将生成单词的词元及其**词性**（**POS**）标签。词元不过是单词的词根形式，而词性标签表示单词是作为动词、名词、形容词还是副词使用。有许多词性标签可用。您可以在以下网站上找到词性标签列表：[https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)。
- en: We will generate the matrix of the TF-IDF vectors for the words.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将为单词生成TF-IDF向量的矩阵。
- en: We will generate the SVD matrix using the `SciPy` library for the given TF-IDF
    matrix.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用`SciPy`库为给定的TF-IDF矩阵生成SVD矩阵。
- en: Finally, using cosine-similarity, we can rank the sentences and generate the
    summary.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，使用余弦相似度，我们可以对句子进行排序并生成摘要。
- en: Now let's look at the implementation of these steps.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这些步骤的实现。
- en: The best approach
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最佳方法
- en: 'In this section, we will look at the implementation of the best approach. We
    will also discuss the structure of the code. So, without wasting time, let''s
    begin with the implementation. You can find the code by using this GitHub link:
    [https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨最佳方法的实现。我们还将讨论代码的结构。因此，让我们不浪费时间，直接开始实施。您可以通过以下GitHub链接找到代码：[https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach](https://github.com/jalajthanaki/medical_notes_extractive_summarization/tree/master/Best_approach)。
- en: Implementing the best approach
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实施最佳方法
- en: 'The steps you need to take in order to implement the code are provided in the
    following list:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 实现代码所需采取的步骤在以下列表中提供：
- en: Understanding the structure of the project
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解项目结构
- en: Understanding helper functions
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 理解辅助函数
- en: Generating the summary
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成摘要
- en: Let's start with the first step.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始。
- en: Understanding the structure of the project
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解项目结构
- en: 'The structure of the project is quite important here. There will be four different
    files in which we will be writing code. You can see the structure of the project
    in the following figure:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 项目的结构在这里非常重要。我们将有四个不同的文件，我们将在这四个文件中编写代码。您可以在以下图中看到项目的结构：
- en: '![Understanding the structure of the project](img/B08394_07_15.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![理解项目结构](img/B08394_07_15.jpg)'
- en: 'Figure 7.15: Structure of the project''s code files'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15：项目代码文件的结构
- en: 'There are four code files. I will explain their usage one by one:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 有四个代码文件。我将逐一解释它们的用法：
- en: '`Contractions.py`: This file contains an extensive list of all of the abbreviations,
    especially grammatical abbreviations. You can take a look at the list abbreviations
    in the following figure:![Understanding the structure of the project](img/B08394_07_16.jpg)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Contractions.py`：此文件包含所有缩写的详尽列表，特别是语法缩写。您可以查看以下图中的缩写列表：![Understanding the
    structure of the project](img/B08394_07_16.jpg)'
- en: 'Figure 7.16: List of abbreviations and their full forms'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.16：缩写及其全称列表
- en: '`Normalization.py`: This file contains various helper functions for the preprocessing
    step'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Normalization.py`：此文件包含预处理步骤的各种辅助函数'
- en: '`Utils.py`: This file contains the helper function that is used to calculate
    TF-IDF and obtain the SVD matrix for the given TF-IDF matrix'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Utils.py`：此文件包含用于计算TF-IDF和获取给定TF-IDF矩阵的SVD矩阵的辅助函数'
- en: '`Document_summarization.py`: This file uses the already defined helper function
    and generates a summary for the document'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Document_summarization.py`：此文件使用已定义的辅助函数并为文档生成摘要'
- en: Now let's see what kind of helper functions we have defined in each file.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看每个文件中定义了哪些辅助函数。
- en: Understanding helper functions
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解辅助函数
- en: We will discuss the helper function file-wise so you will get an idea as to
    which helper function is part of which file.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按文件方式讨论辅助函数，这样您就会了解哪个辅助函数属于哪个文件。
- en: Normalization.py
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Normalization.py
- en: 'This file contains many helper functions. I will explain each helper function
    based on the sequence of its usage:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件包含许多辅助函数。我将根据其使用顺序解释每个辅助函数：
- en: '`parse_document`: This function takes the content of the document as the input
    and tokenizes it sentence-wise. This means we are splitting the string sentence
    by sentence. We will consider only the Unicode string here. You can refer to the
    code snippet given in the following figure:![Normalization.py](img/B08394_07_17.jpg)'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parse_document`：此函数将文档内容作为输入，并按句子进行分词。这意味着我们按句子分割字符串。在这里，我们将仅考虑Unicode字符串。您可以参考以下图中给出的代码片段：![Normalization.py](img/B08394_07_17.jpg)'
- en: 'Figure 7.17: Code snippet for parsing documents'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.17：解析文档的代码片段
- en: '`remove_special_characters`: This function will remove the special characters
    from the strings. You can refer to the code snippet given in the following figure
    for a better idea:![Normalization.py](img/B08394_07_18.jpg)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_special_characters`：此函数将从字符串中删除特殊字符。您可以参考以下图中给出的代码片段以获得更好的理解：![Normalization.py](img/B08394_07_18.jpg)'
- en: 'Figure 7.18: Code snippet for removing special characters from the string'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.18：从字符串中删除特殊字符的代码片段
- en: '`remove_stopwords`: This function will remove the stop words from the sentences.
    You can refer to the code snippet given in the following figure:![Normalization.py](img/B08394_07_19.jpg)'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`remove_stopwords`：此函数将从句子中删除停用词。您可以参考以下图中给出的代码片段：![Normalization.py](img/B08394_07_19.jpg)'
- en: 'Figure 7.19: Code snippet for removing stop words'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.19：删除停用词的代码片段
- en: '`unescape_html`: This function removes HTML tags from the sentences. You can
    refer to the code snippet given in the following figure:![Normalization.py](img/B08394_07_20.jpg)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`unescape_html`：此函数将从句子中删除HTML标签。您可以参考以下图中给出的代码片段：![Normalization.py](img/B08394_07_20.jpg)'
- en: 'Figure 7.20: Code snippet for removing HTML tags'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.20：删除HTML标签的代码片段
- en: '`pos_tag_text`: This function tokenizes the sentences into words, after which
    it will provide POS tags to these words. You can refer to the code snippet given
    in the following figure:![Normalization.py](img/B08394_07_21.jpg)'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos_tag_text`：此函数将句子分词成单词，之后将为这些单词提供词性标注。您可以参考以下图中给出的代码片段：![Normalization.py](img/B08394_07_21.jpg)'
- en: 'Figure 7.21: Code snippet for generating POS tags'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.21：生成词性标注的代码片段
- en: '`lemmatize_text`: This function will tokenize the sentence into words and then
    generate the lemma of the words. You can refer to the code given in the following
    figure:![Normalization.py](img/B08394_07_22.jpg)'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lemmatize_text`：此函数将句子分词成单词，然后生成单词的词元。您可以参考以下图中给出的代码：![Normalization.py](img/B08394_07_22.jpg)'
- en: 'Figure 7.22: Code snippet for generating the lemma of the words'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.22：生成单词词元的代码片段
- en: '`expand_contractions`: This function looks at the abbreviations. If there is
    any abbreviation that is present in our list in the given sentence, then we will
    replace that abbreviation with its full form. You can refer to the code displayed
    in the following figure:![Normalization.py](img/B08394_07_23.jpg)'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`expand_contractions`：这个函数检查缩写。如果给定句子中存在我们列表中的任何缩写，那么我们将用其全称替换该缩写。你可以参考下面图中显示的代码：![Normalization.py](img/B08394_07_23.jpg)'
- en: 'Figure 7.23: Code snippet for replacing abbreviations with full forms'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.23：替换缩写为全称的代码片段
- en: '`normalize_corpus`: This function calls all the preceding helper functions
    and generates the preprocessed sentences. You can refer to the code given in the
    following figure:![Normalization.py](img/B08394_07_24.jpg)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`normalize_corpus`：这个函数调用所有前面的辅助函数并生成预处理句子。你可以参考下面图中给出的代码：![Normalization.py](img/B08394_07_24.jpg)'
- en: 'Figure 7.24: Code snippet for generating preprocessed sentences'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.24：生成预处理句子的代码片段
- en: Now let's see what functions we have defined in the `utils.py` file.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看在`utils.py`文件中定义了哪些函数。
- en: Utils.py
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Utils.py
- en: In this file, there are only two helper functions. They are described here.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个文件中，只有两个辅助函数。它们在这里进行了描述。
- en: '`build_feature_matrixs`: This function generates the TF-IDF vectors using the
    scikit-learn `Tfidfvectorizer` API. We are providing the preprocessed text as
    the input, and as the output, we have the matrix. This matrix contains the vectorized
    value of the given words. You can refer to the code snippet for this, which is
    provided in the following figure:![Utils.py](img/B08394_07_25.jpg)'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`build_feature_matrixs`：这个函数使用scikit-learn的`Tfidfvectorizer` API生成TF-IDF向量。我们提供预处理文本作为输入，输出是一个矩阵。这个矩阵包含了给定单词的向量值。你可以参考下面的代码片段，它提供在以下图中：![Utils.py](img/B08394_07_25.jpg)'
- en: 'Figure 7.25: Code snippet for generating TF-IDF vectors'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.25：生成TF-IDF向量的代码片段
- en: '`low_rank_svd`: This particular function uses the API from python''s `SciPy`
    library. It performs the SVD on the TF-IDF matrix, and after that, we obtain the
    cosine similarity score. Based on the score, we will rank the sentences. Here,
    we just define the function that can generate the SVD for the TF-IDF matrix. You
    can refer to the code snippet given in the following figure:![Utils.py](img/B08394_07_26.jpg)'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`low_rank_svd`：这个特定的函数使用Python的`SciPy`库的API。它对TF-IDF矩阵执行奇异值分解（SVD），然后我们获得余弦相似度分数。基于这个分数，我们将对句子进行排名。在这里，我们只定义了一个可以生成TF-IDF矩阵SVD的函数。你可以参考下面图中给出的代码片段：![Utils.py](img/B08394_07_26.jpg)'
- en: 'Figure 7.26: Code snippet for generating SVD'
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.26：生成SVD的代码片段
- en: Now let's use all these helper functions in order to generate the summary.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用所有这些辅助函数来生成摘要。
- en: Generating the summary
  id: totrans-227
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成摘要
- en: 'In this section, we will look at the code that is given in the `document_summarization.py`
    file. There are two methods that are responsible for generating the summary for
    the given document. They are as follows:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看`document_summarization.py`文件中给出的代码。有两个方法负责生成给定文档的摘要。它们如下：
- en: '`textrank_text_summarizer`: This method takes the preprocessed document as
    the input, and by using the `build_feature_matrix` helper function, we will generate
    the TF-IDF matrix. After that, we will generate the similarity score. Based on
    the similarity score, we will sort the sentences and provide them a rank. As an
    output, we will display these sorted sentences. Here, sentence sequence is aligned
    with the original document, so we don''t need to worry about that. You can take
    a look at the code snippet given in the following figure:![Generating the summary](img/B08394_07_27.jpg)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`textrank_text_summarizer`：这个方法将预处理文档作为输入，通过使用`build_feature_matrix`辅助函数，我们将生成TF-IDF矩阵。之后，我们将生成相似度分数。基于相似度分数，我们将对句子进行排序并赋予它们排名。作为输出，我们将显示这些排序后的句子。在这里，句子序列与原始文档对齐，所以我们不需要担心这一点。你可以看看下面图中给出的代码片段：![生成摘要](img/B08394_07_27.jpg)'
- en: 'Figure 7.27: Code snippet in order to generate the summary using the textrank_text_summarizer
    method'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.27：使用`textrank_text_summarizer`方法生成摘要的代码片段
- en: '`lsa_text_summarizer:` This function takes the preprocessed text as the input
    and generates the TF-IDF matrix. After that, the `low_rank_svd` method is applied
    on the matrix, and we get our factorized matrix. We will generate the similarity
    score using these factorized matrices. After sorting sentences based on this similarity
    score, we can generate the summary. You can refer to the code snippet displayed
    in the following figure:![Generating the summary](img/B08394_07_28.jpg)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lsa_text_summarizer:` 此函数将预处理文本作为输入并生成TF-IDF矩阵。然后，在矩阵上应用`low_rank_svd`方法，我们得到我们的因子矩阵。我们将使用这些因子矩阵生成相似度分数。根据这个相似度分数对句子进行排序后，我们可以生成摘要。您可以参考以下图中显示的代码片段：![生成摘要](img/B08394_07_28.jpg)'
- en: 'Figure 7.28: Code snippet for generating the summary using lsa_text_summarizer'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.28：使用lsa_text_summarizer生成摘要的代码片段
- en: 'We will call these functions and generate the output. The code snippet for
    that is given in the following figure:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将调用这些函数并生成输出。相应的代码片段如下所示：
- en: '![Generating the summary](img/B08394_07_29.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![生成摘要](img/B08394_07_29.jpg)'
- en: 'Figure 7.29: Code snippet for generating the output summary'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.29：生成输出摘要的代码片段
- en: 'You can take a look at the output shown in the following figure:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以查看以下图中显示的输出：
- en: '![Generating the summary](img/B08394_07_30.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![生成摘要](img/B08394_07_30.jpg)'
- en: 'Figure 7.30: Output summary for document_1'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.30：文档_1的输出摘要
- en: 'The output for another document is given in the following figure:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个文档的输出如下所示：
- en: '![Generating the summary](img/B08394_07_31.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![生成摘要](img/B08394_07_31.jpg)'
- en: 'Figure 7.31: Output summary for document_2'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.31：文档_2的输出摘要
- en: As you can see, compared to the revised approach, we will get a much more relevant
    extractive type of summary for the given document. Now let's build the abstractive
    summarization application using Amazon's product review dataset.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，与修订方法相比，我们将为给定文档获得一个更相关的提取型摘要。现在让我们使用亚马逊的产品评论数据集构建抽象摘要应用。
- en: Building the summarization application using Amazon reviews
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用亚马逊评论构建摘要应用
- en: 'We are building this application so that you can learn how to use parallel
    corpus in order to generate the abstractive summary for the textual dataset. We
    have already explained basic stuff related to the dataset earlier in the chapter.
    Here, we will cover how to build an abstractive summarization application using
    the Deep Learning (DL) algorithm. You can refer to the code using this GitHub
    link: [https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb](https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建此应用是为了让您了解如何使用并行语料库来为文本数据集生成抽象摘要。我们已经在章节中解释了与数据集相关的基本内容。在这里，我们将介绍如何使用深度学习（DL）算法构建抽象摘要应用。您可以参考以下GitHub链接中的代码：[https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb](https://github.com/jalajthanaki/Amazon_review_summarization/blob/master/summarize_reviews.ipynb)。
- en: 'You can also download the pre-trained model using this link: [https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg](https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg).'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用此链接下载预训练模型：[https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg](https://drive.google.com/open?id=1inExMtqR6Krddv7nHR4ldWTYY7_hMALg)。
- en: 'For this application, we will perform the following steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此应用，我们将执行以下步骤：
- en: Loading the dataset
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载数据集
- en: Exploring the dataset
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索数据集
- en: Preparing the dataset
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据集
- en: Building the DL model
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建DL模型
- en: Training the DL model
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度学习（DL）模型
- en: Testing the DL model
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试DL模型
- en: Loading the dataset
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'In this section, we will see the code for how we can load the dataset. Our
    dataset is in the CSV file format. We will be using pandas to read our dataset.
    You can refer to the code snippet given in the following figure:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何加载数据集的代码。我们的数据集是CSV文件格式。我们将使用pandas来读取我们的数据集。您可以参考以下图中给出的代码片段：
- en: '![Loading the dataset](img/B08394_07_32.jpg)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![加载数据集](img/B08394_07_32.jpg)'
- en: 'Figure 7.32: Code snippet for loading the dataset'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.32：加载数据集的代码片段
- en: Exploring the dataset
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 探索数据集
- en: 'In this section, we will be doing some basic analysis of the dataset. We will
    check whether any null entries are present. If there are, then we will remove
    them. You can refer to the code snippet given in the following figure:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将对数据集进行一些基本分析。我们将检查是否存在任何空值条目。如果有，我们将删除它们。您可以参考以下图中给出的代码片段：
- en: '![Exploring the dataset](img/B08394_07_33.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![探索数据集](img/B08394_07_33.jpg)'
- en: 'Figure 7.33: Code snippet for removing null data entries'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.33：移除空数据条目的代码片段
- en: Now let's prepare the dataset that can be used to train the model.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们准备可以用于训练模型的数据库。
- en: Preparing the dataset
  id: totrans-262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'These are the steps that we will perform in order to prepare the dataset:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将执行以准备数据集的步骤：
- en: We will replace the abbreviations that appeared in the text with their full
    forms
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用全称替换文本中出现的缩写
- en: We will remove special characters, URLs, and HTML tags from the review data
    column
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从审查数据列中移除特殊字符、URL和HTML标签
- en: We will remove stop words from the reviews
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将从评论中移除停用词
- en: 'We have performed all the preceding steps and generated the junk-free review.
    You can refer to the code snippet given in the following figure:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了所有前面的步骤并生成了无垃圾评论。你可以参考以下图中的代码片段：
- en: '![Preparing the dataset](img/B08394_07_34.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![准备数据集](img/B08394_07_34.jpg)'
- en: 'Figure 7.34: Code snippet for performing preprocessing of the reviews'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.34：执行评论预处理的代码片段
- en: 'Here, there are 132,884 unique words. You can find the size of the vocabulary
    when you run the code. These unique words are the vocabulary for this application,
    and we need to convert these words into a vector format. The vector format of
    the words is called word embeddings. You can use Word2vec, Numberbatch, or GloVe
    in order to generate word embeddings. Here, we will be using Numberbatch''s embedding
    pre-trained model in order to generate word embedding for this application. The
    Numberbatch''s pretrained model is more optimize and faster than GloVe so we are
    using Numberbatch''s model. You can refer to the code snippet given in the following
    figure:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这里共有132,884个独特的单词。当你运行代码时，你可以找到词汇表的大小。这些独特的单词是本应用的词汇表，我们需要将这些单词转换为向量格式。单词的向量格式称为词嵌入。你可以使用Word2vec、Numberbatch或GloVe来生成词嵌入。在这里，我们将使用Numberbatch的预训练嵌入模型来生成本应用的词嵌入。Numberbatch的预训练模型比GloVe更优化且更快，所以我们使用Numberbatch的模型。你可以参考以下图中的代码片段：
- en: '![Preparing the dataset](img/B08394_07_35.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![准备数据集](img/B08394_07_35.jpg)'
- en: 'Figure 7.35: Code snippet for generating word embedding using Numberbatch''s
    pre-trained model'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.35：使用Numberbatch预训练模型生成词嵌入的代码片段
- en: If you want to learn more about word2vec, then you can refer to my previous
    book, *Python Natural Language Processing*, particularly [Chapter 6](ch06.xhtml
    "Chapter 6. Job Recommendation Engine"), *Advance Feature Engineering and NLP
    Algorithms*. The link is [https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于word2vec的信息，你可以参考我的上一本书，*Python自然语言处理*，特别是[第6章](ch06.xhtml "第6章。职位推荐引擎")，*高级特征工程和NLP算法*。链接是[https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6](https://www.packtpub.com/mapt/book/big_data_and_business_intelligence/9781787121423/6)。
- en: Building the DL model
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建深度学习模型
- en: In this section, we will be building the DL algorithm. We are using the seq2seq
    neural network. Basically, the seq2seq model is used to process the sequential
    data. Language or sentences are the sequence of words. In this algorithm, there
    is an encoder that accepts the word embedding and learns the language representation.
    The output of this layer is fed to the decoding layer. Here, we will also use
    the attention mechanism. The attention mechanism will focus on the most import
    part of the sentences. It will store the semantic representation of the sentences.
    For the attention mechanism, we will use the LSTM cell with the recurrent neural
    network architecture, which learns the complex semantic representation of the
    language and stores it in the LSTM network. When we generate the final output,
    we will be using the weight of the decoder cells as well as the weight of LSTM
    cells and will generate the final word embedding. Based on the word embedding,
    we will generate the summary.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建深度学习算法。我们使用seq2seq神经网络。基本上，seq2seq模型用于处理序列数据。语言或句子是单词的序列。在这个算法中，有一个编码器接受词嵌入并学习语言表示。这个层的输出被输入到解码层。在这里，我们也将使用注意力机制。注意力机制将关注句子的最重要部分。它将存储句子的语义表示。对于注意力机制，我们将使用具有循环神经网络架构的LSTM单元，它学习语言的复杂语义表示并将其存储在LSTM网络中。当我们生成最终输出时，我们将使用解码器单元的权重以及LSTM单元的权重，并生成最终的词嵌入。基于词嵌入，我们将生成摘要。
- en: 'In order to achieve this, we need to build seq2seq using a **Recurrent Neural
    Network** (**RNN**) with the attention mechanism. You can refer to the code given
    in the following figure:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们需要使用具有注意力机制的**循环神经网络**（**RNN**）来构建seq2seq。您可以在以下图中查看给出的代码：
- en: '![Building the DL model](img/B08394_07_36.jpg)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![构建深度学习模型](img/B08394_07_36.jpg)'
- en: 'Figure 7.36: Code snippet for building the RNN encoding layer'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.36：构建RNN编码层的代码片段
- en: 'You can refer to the code snippet given in the following figure:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以参考以下图中给出的代码片段：
- en: '![Building the DL model](img/B08394_07_37.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![构建深度学习模型](img/B08394_07_37.jpg)'
- en: 'Figure 7.37: Code snippet for building the RNN decoding layer'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.37：构建RNN解码层的代码片段
- en: 'The code snippet for building the seq2seq model is given in the following figure:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 构建seq2seq模型的代码片段如下所示：
- en: '![Building the DL model](img/B08394_07_38.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![构建深度学习模型](img/B08394_07_38.jpg)'
- en: 'Figure 7.38: Code snippet for building seq2seq'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.38：构建seq2seq的代码片段
- en: Now let's train the model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始训练模型。
- en: Training the DL model
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练深度学习模型
- en: 'Basically, we have built the neural network, and now it''s time to start the
    training. In this section, we will define the values for all hyperparameters,
    such as the learning rate, the batch size, and so on. You can refer to the code
    given in the following figure:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们已经构建了神经网络，现在是时候开始训练了。在本节中，我们将定义所有超参数的值，例如学习率、批量大小等。您可以在以下图中查看给出的代码：
- en: '![Training the DL model](img/B08394_07_39.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![训练深度学习模型](img/B08394_07_39.jpg)'
- en: 'Figure 7.39: Code snippet for training the model'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.39：训练模型的代码片段
- en: 'During the training, we will be tracking the loss function and using the gradient
    descent algorithm, and we will try to minimize the value of our loss function.
    You can refer to the code snippet given in the following figure:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们将追踪损失函数并使用梯度下降算法，并尝试最小化我们的损失函数值。您可以在以下图中查看给出的代码片段：
- en: '![Training the DL model](img/B08394_07_40.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![训练深度学习模型](img/B08394_07_40.jpg)'
- en: 'Figure 7.40: Code snippet for tracing the loss function'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.40：追踪损失函数的代码片段
- en: Here, we have the trained the model on CPU for 6 to 8 hours, and we have the
    loss value 1.413\. You can train the model for more amount time as well. Now let's
    test the trained model.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已经将模型在CPU上训练了6到8小时，并且我们得到了损失值1.413。您也可以将模型训练更长的时间。现在让我们测试一下训练好的模型。
- en: Testing the DL model
  id: totrans-294
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 测试深度学习模型
- en: 'In this section, we load the trained model and generate the summary for a randomly
    selected review. You can refer to the code snippet given in the following figure:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们加载了训练好的模型，并为随机选择的评论生成了摘要。您可以在以下图中查看给出的代码片段：
- en: '![Testing the DL model](img/B08394_07_41.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![测试深度学习模型](img/B08394_07_41.jpg)'
- en: 'Figure 7.41: Code snippet for generating the summary for the given review'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.41：生成给定评论摘要的代码片段
- en: 'The output for the preceding code is shown in the following figure:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 前述代码的输出如下所示：
- en: '![Testing the DL model](img/B08394_07_42.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![测试深度学习模型](img/B08394_07_42.jpg)'
- en: 'Figure 7.42: Summary for the given review'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.42：给定评论的总结
- en: This approach is great if we want to generate a one-line summary for the given
    textual data. In future, if we will have the parallel medical transcription dataset
    for the summarization task, then this approach will work well.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要为给定的文本数据生成一行总结，这种方法非常好。在未来，如果我们有用于总结任务的并行医学转录数据集，那么这种方法将工作得很好。
- en: Summary
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we built the summarization application for medical transcriptions.
    In the beginning, we listed the challenges in order to generate a good parallel
    corpus for the summarization task in the medical domain. After that, for our baseline
    approach, we used the already available Python libraries, such as `PyTeaser` and
    `Sumy`. In the revised approach, we used word frequencies to generate the summary
    of the medical document. In the best possible approach, we combined the word frequency-based
    approach and the ranking mechanism in order to generate a summary for medical
    notes.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们为医学转录构建了摘要应用。一开始，我们列出了为了在医学领域生成好的平行语料库以用于摘要任务所面临的挑战。之后，为了我们的基线方法，我们使用了现成的Python库，如`PyTeaser`和`Sumy`。在改进的方法中，我们使用了词频来生成医学文档的摘要。在最佳的方法中，我们结合了基于词频的方法和排名机制，以生成医学笔记的摘要。
- en: In the end, we developed a solution, where we used Amazon's review dataset,
    which is the parallel corpus for the summarization task, and we built the deep
    learning-based model for summarization. I would recommend that researchers, community
    members, and everyone else come forward to build high-quality datasets that can
    be used for building some great data science applications for the health and medical
    domains.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们开发了一个解决方案，其中我们使用了亚马逊的评论数据集，这是摘要任务的平行语料库，并构建了基于深度学习的摘要模型。我建议研究人员、社区成员以及其他人积极向前，构建高质量的可以用于构建健康和医疗领域的一些优秀数据科学应用的数据集。
- en: In the next chapter, we will be building chatbots. Chatbots, or virtual assistants,
    have become a hot topic in the data science domain over the last couple of years.
    So, in the next chapter, we will take into consideration a movie dialog dataset
    and the Facebook `bAbI` dataset. With the help of these datasets and by using
    deep learning algorithms, we will build chatbots. So, if you want to learn how
    to build one for yourself, then keep reading!
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将构建聊天机器人。聊天机器人，或虚拟助手，在过去的几年中已经成为数据科学领域的一个热门话题。因此，在下一章中，我们将考虑一个电影对话数据集和Facebook的`bAbI`数据集。借助这些数据集，并通过使用深度学习算法，我们将构建聊天机器人。所以，如果你想学习如何为自己构建一个，那么请继续阅读！
