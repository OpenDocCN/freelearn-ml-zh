- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Predicting Online Ad Click-Through with Tree-Based Algorithms
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基于树的算法预测在线广告点击率
- en: 'In the previous chapter, we built a movie recommender. In this chapter and
    the next, we will be solving one of the most data-driven problems in digital advertising:
    ad click-through prediction—given a user and the page they are visiting, this
    predicts how likely it is that they will click on a given ad. We will focus on
    learning tree-based algorithms (including decision trees, random forest models,
    and boosted trees) and utilize them to tackle this billion-dollar problem.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了一个电影推荐系统。在本章和下一章中，我们将解决数字广告中最具数据驱动问题之一：广告点击率预测——给定用户及其访问的页面，预测他们点击某个广告的可能性。我们将重点学习基于树的算法（包括决策树、随机森林模型和提升树），并利用它们来解决这个价值数十亿美元的问题。
- en: We will be exploring decision trees from the root to the leaves, as well as
    the aggregated version, a forest of trees. This won’t be a theory-only chapter,
    as there are a lot of hand calculations and implementations of tree models from
    scratch included. We will be using scikit-learn and XGBoost, a popular Python
    package for tree-based algorithms.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探索从根到叶子的决策树，以及集成版本，即一片树的森林。这不仅仅是一个理论章节，还包含了许多手工计算和从零开始实现树模型的部分。我们将使用 scikit-learn
    和 XGBoost，这是一个流行的 Python 包，用于基于树的算法。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涉及以下主题：
- en: A brief overview of ad click-through prediction
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告点击率预测的简要概述
- en: Exploring a decision tree from the root to the leaves
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从根到叶子探索决策树
- en: Implementing a decision tree from scratch
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始实现决策树
- en: Implementing a decision tree with scikit-learn
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 实现决策树
- en: Predicting ad click-through with a decision tree
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树预测广告点击率
- en: Ensembling decision trees – random forests
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成决策树——随机森林
- en: Ensembling decision trees – gradient-boosted trees
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成决策树——梯度提升树
- en: A brief overview of ad click-through prediction
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广告点击率预测的简要概述
- en: Online display advertising is a multibillion-dollar industry. Online display
    ads come in different formats, including banner ads composed of text, images,
    and flash, and rich media such as audio and video. Advertisers, or their agencies,
    place ads on a variety of websites, and even mobile apps, across the internet
    in order to reach potential customers and deliver an advertising message.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在线展示广告是一个数十亿美元的行业。在线展示广告有不同的格式，包括由文本、图像和 Flash 组成的横幅广告，以及富媒体广告，如音频和视频。广告商或其代理商将广告投放到各种网站，甚至移动应用程序上，以接触潜在客户并传递广告信息。
- en: 'Online display advertising has served as one of the greatest examples of machine
    learning utilization. Obviously, advertisers and consumers are keenly interested
    in well-targeted ads. In the last 20 years, the industry has relied heavily on
    the ability of machine learning models to predict the effectiveness of ad targeting:
    how likely it is that an audience of a certain age group will be interested in
    this product, that customers with a certain household income will purchase this
    product after seeing the ad, that frequent sports site visitors will spend more
    time reading this ad, and so on. The most common measurement of effectiveness
    is the **Click-Through Rate** (**CTR**), which is the ratio of clicks on a specific
    ad to its total number of views. In general cases without clickbait or spammy
    content, a higher CTR indicates that an ad is targeted well and that an online
    advertising campaign is successful.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在线展示广告已成为机器学习应用的最佳示例之一。显然，广告商和消费者都对精准定位的广告充满兴趣。在过去20年里，行业在很大程度上依赖机器学习模型预测广告定位的有效性：例如，某个年龄段的观众对某产品感兴趣的可能性，某些家庭收入群体的顾客看到广告后会购买某产品的可能性，频繁访问体育网站的用户会花更多时间阅读某个广告的可能性，等等。最常见的效果衡量标准是**点击率**（**CTR**），即某个广告被点击的次数与其总展示次数的比率。通常，在没有点击诱饵或垃圾内容的情况下，较高的CTR表示广告定位准确，在线广告活动成功。
- en: 'Click-through prediction entails both the promises and challenges of machine
    learning. It mainly involves the binary classification of whether a given ad on
    a given page (or app) will be clicked on by a given user, with predictive features
    from the following three aspects:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 点击率预测包含了机器学习的潜力和挑战。它主要涉及二分类问题，即预测给定页面（或应用）上的某个广告是否会被用户点击，预测特征来自以下三个方面：
- en: Ad content and information (category, position, text, format, and so on)
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告内容和信息（类别、位置、文本、格式等）
- en: Page content and publisher information (category, context, domain, and so on)
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面内容和发布者信息（类别、上下文、领域等）
- en: User information (age, gender, location, income, interests, search history,
    browsing history, device, and so on)
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户信息（年龄、性别、位置、收入、兴趣、搜索历史、浏览历史、设备等）
- en: 'Suppose we, as an agency, are operating ads on behalf of several advertisers,
    and our job is to place the right ads for the right audience. Let’s say that we
    have an existing dataset in hand (the following small chunk is an example; the
    number of predictive features can easily go into the thousands in reality) taken
    from millions of records of campaigns run a month ago, and we need to develop
    a classification model to learn and predict future ad placement outcomes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们作为一个广告代理机构，代表多个广告主进行广告投放，我们的工作是为正确的受众投放合适的广告。假设我们手头有一个现有数据集（以下小段示例；实际上，预测特征的数量可能会达到数千个），该数据集来自一个月前运行的数百万条广告记录，我们需要开发一个分类模型来学习并预测未来的广告投放结果：
- en: '![A picture containing text, number, screenshot, font  Description automatically
    generated](img/B21047_03_01.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, number, screenshot, font  Description automatically
    generated](img/B21047_03_01.png)'
- en: 'Figure 3.1: Ad samples for training and prediction'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1：用于训练和预测的广告样本
- en: As you can see in *Figure 3.1*, the features are mostly categorical. In fact,
    data can be either numerical or categorical. Let’s explore this in more detail
    in the next section.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在*图3.1*中看到的，特征大多是类别型的。事实上，数据可以是数值型或类别型的。让我们在下一节中更详细地探讨这一点。
- en: Getting started with two types of data – numerical and categorical
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从两种数据类型开始——数值型和类别型
- en: At first glance, the features in the preceding dataset are **categorical** –
    for example, male or female, one of four age groups, one of the predefined site
    categories, and whether the user is interested in sports. Such data is different
    from the **numerical** feature data we have worked with until now.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 乍一看，前述数据集中的特征是**类别型**的——例如，男性或女性、四个年龄组之一、预定义的站点类别之一，以及用户是否对体育感兴趣。这类数据与我们之前处理的**数值型**特征数据有所不同。
- en: '**Categorical features**, also known as **qualitative features**, represent
    distinct characteristics or groups with a countable number of options. Categorical
    features may or may not have a logical order. For example, household income from
    low to medium to high is an **ordinal** feature, while the category of an ad is
    not ordinal.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**类别型特征**，也称为**定性特征**，表示具有可计数选项的不同特征或类别。类别型特征可能有，也可能没有逻辑顺序。例如，家庭收入从低到中到高是一个**有序**特征，而广告的类别则不是有序的。'
- en: Numerical (also called **quantitative**) features, on the other hand, have mathematical
    meaning as a measurement and, of course, are ordered. For instance, counts of
    items (e.g., number of children in a family, number of bedrooms in a house, and
    number of days until an event ) are discrete numerical features; the height of
    individuals, temperature, and the weight of objects are continuous numerical.
    The cardiotocography dataset ([https://archive.ics.uci.edu/ml/datasets/Cardiotocography](https://archive.ics.uci.edu/ml/datasets/Cardiotocography))
    contains both discrete (such as the number of accelerations per second or the
    number of fetal movements per second) and continuous (such as the mean value of
    long-term variability) numerical features.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数值型（也叫**定量**）特征则具有数学意义，作为一种测量方式，当然也是有序的。例如，物品的计数（例如，家庭中孩子的数量、房屋中的卧室数量以及距离某个事件的天数）是离散的数值型特征；个体的身高、温度和物体的重量是连续的数值型特征。心电图数据集（[https://archive.ics.uci.edu/ml/datasets/Cardiotocography](https://archive.ics.uci.edu/ml/datasets/Cardiotocography)）包含了离散型（例如每秒加速度的次数或每秒胎动的次数）和连续型（例如长期变异性的均值）数值特征。
- en: Categorical features can also take on numerical values. For example, 1 to 12
    can represent months of the year, and 1 and 0 can indicate adult and minor. Still,
    these values do not have mathematical implications.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 类别型特征也可以取数字值。例如，1到12可以表示一年中的月份，1和0可以表示成年人和未成年人。但这些值并不具有数学含义。
- en: The Naïve Bayes classifier you learned about previously works for both numerical
    and categorical features as the likelihoods, *P*(*x* |*y*) or *P*(*feature* |*class*),
    are calculated in the same way.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 你之前学习的朴素贝叶斯分类器适用于数值型和类别型特征，因为似然值，*P*(*x* |*y*) 或 *P*(*feature* |*class*)，的计算方式是相同的。
- en: Now, say we are thinking of predicting click-through using Naïve Bayes and trying
    to explain the model to our advertising clients. However, our clients may find
    it difficult to understand the prior and the likelihood of individual attributes
    and their multiplication. Is there a classifier that is easy to interpret and
    explain to clients, and that is able to directly handle categorical data? Decision
    trees are the answer!
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在考虑使用朴素贝叶斯预测点击率，并且尝试向我们的广告客户解释模型。然而，我们的客户可能会发现很难理解每个属性的先验和可能性及其乘积。那么，有没有一种分类器，既易于解释又能直接处理分类数据呢？决策树就是答案！
- en: Exploring a decision tree from the root to the leaves
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从根节点到叶节点探索决策树
- en: A decision tree is a tree-like graph, that is, a sequential diagram illustrating
    all of the possible decision alternatives and their corresponding outcomes. Starting
    from the **root** of a tree, every internal **node** represents the basis on which
    a decision is made. Each branch of a node represents how a choice may lead to
    the next node. And, finally, each **terminal node**, the **leaf**, represents
    the outcome produced.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种树形图，即一个顺序图，展示了所有可能的决策选择及其相应的结果。从决策树的**根节点**开始，每个内部**节点**表示做出决策的依据。每个节点的分支表示一个选择如何导致下一个节点。最后，每个**终端节点**，即**叶子**，代表产生的结果。
- en: 'For example, we have just made a couple of decisions that brought us to the
    point of using a decision tree to solve our advertising problem:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们刚刚做出了几个决策，导致我们使用决策树来解决广告问题：
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_03_02.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![包含文本、截图、图表、字体的图片，描述自动生成](img/B21047_03_02.png)'
- en: 'Figure 3.2: Using a decision tree to find the right algorithm'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：使用决策树找到合适的算法
- en: The first condition, or the root, is whether the feature type is numerical or
    categorical. Let’s assume our ad clickstream data contains mostly categorical
    features, so it goes to the right branch. In the next node, our work needs to
    be interpretable by non-technical clients, so, it goes to the right branch and
    reaches the leaf for choosing the decision tree classifier.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个条件，或称根节点，是特征类型是数值型还是类别型。假设我们的广告点击流数据主要包含类别特征，那么它会走右侧分支。在下一个节点，我们的工作需要能够被非技术客户理解，因此它会走右侧分支并到达选择决策树分类器的叶节点。
- en: You can also look at the paths and see what kinds of problems they can fit in.
    A decision tree classifier operates in the form of a decision tree. It maps observations
    to class assignments (symbolized as leaf nodes) through a series of tests (represented
    as internal nodes) based on feature values and corresponding conditions (represented
    as branches). In each node, a question regarding the values and characteristics
    of a feature is asked; depending on the answer to the question, the observations
    are split into subsets. Sequential tests are conducted until a conclusion about
    the observations’ target label is reached. The paths from the root to the end
    leaves represent the decision-making process and the classification rules.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以查看路径，看看它们适合哪些问题。决策树分类器以决策树的形式工作。它通过一系列基于特征值和相应条件的测试（表现为内部节点）将观察结果映射到类分配（表示为叶节点）。在每个节点，会提出一个关于特征值和特征特性的问询；根据问题的答案，观察结果会被分割成子集。进行顺序测试，直到得出关于观察结果目标标签的结论。从根节点到叶节点的路径代表了决策过程和分类规则。
- en: 'In a more simplified scenario, as shown in *Figure 3.3*, where we want to predict
    **Click** or **No click** on a self-driven car ad, we can manually construct a
    decision tree classifier that works for an available dataset. For example, if
    a user is interested in technology and has a car, they will tend to click on the
    ad; a person outside of this subset is unlikely to click on the ad, hypothetically.
    We then use the trained tree to predict two new inputs, whose results are **Click**
    and **No click**, respectively:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个更简化的场景中，如*图 3.3*所示，我们希望预测自驾车广告上的**点击**或**不点击**，我们可以手动构建一个适用于现有数据集的决策树分类器。例如，如果一个用户对科技感兴趣并且拥有汽车，他们更有可能点击广告；而在这个子集之外的人假设不太可能点击广告。然后，我们使用训练过的树来预测两个新的输入，其结果分别是**点击**和**不点击**：
- en: '![](img/B21047_03_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_03.png)'
- en: 'Figure 3.3: Predicting Click/No Click with a trained decision tree'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：使用训练过的决策树预测点击/不点击
- en: 'After a decision tree has been constructed, classifying a new sample is straightforward,
    as you just saw: starting from the root, apply the test condition and follow the
    branch accordingly until a leaf node is reached, and the class label associated
    will be assigned to the new sample.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树构建完成后，分类新样本是非常简单的，如你刚刚看到的那样：从根节点开始，应用测试条件并按相应分支进行，直到达到叶节点，并将与该叶节点关联的类标签分配给新样本。
- en: So, how can we build an appropriate decision tree?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何构建一个合适的决策树呢？
- en: Constructing a decision tree
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建决策树
- en: A decision tree is constructed by partitioning the training samples into successive
    subsets. The partitioning process is repeated in a recursive fashion on each subset.
    For each partitioning at a node, a condition test is conducted based on the value
    of a feature of the subset. When the subset shares the same class label, or when
    no further splitting can improve the class purity of this subset, recursive partitioning
    on this node is finished.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是通过将训练样本划分为连续的子集来构建的。这个划分过程会在每个子集上递归地重复进行。在每个节点的划分中，会根据子集中特征的值进行条件测试。当子集内的所有样本属于同一类，或再进行分裂不能改善该子集的类纯度时，该节点的递归划分结束。
- en: '**Important note**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**重要说明**'
- en: '**Class purity** refers to the homogeneity of the target variable (class labels)
    within a subset of data. A subset is considered to have high class purity if the
    majority of its instances belong to the same class. In other words, a subset with
    high class purity contains mostly instances of the same class label, while a subset
    with low class purity contains instances from multiple classes.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**类纯度**是指在数据子集中目标变量（类标签）的同质性。如果大多数实例属于同一类别，则认为子集具有较高的类纯度。换句话说，具有高类纯度的子集大多数包含相同类标签的实例，而具有低类纯度的子集则包含来自多个类的实例。'
- en: 'Theoretically, to partition a feature (numerical or categorical) with *n* different
    values, there are *n* different methods of binary splitting (**Yes** or **No**
    to the condition test, as illustrated in *Figure 3.4*), not to mention other ways
    of splitting (for example, three- and four-way splitting in *Figure 3.4*):'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，要对具有*n*种不同值的特征（无论是数值型还是类别型）进行划分，有*n*种不同的二叉分裂方法（如图*3.4*所示，通过**是**或**否**来进行条件测试），更不用说其他的划分方式（例如图*3.4*中的三路和四路分裂）：
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_03_04.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_03_04.png)'
- en: 'Figure 3.4: Examples of binary splitting and multiway splitting'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：二叉分裂和多路分裂的示例
- en: Without considering the order of features that partitioning is taking place
    on, there are already *n*^m possible trees for an *m*-dimensional dataset.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不考虑划分特征的顺序，对于一个*m*维数据集，已经有*n*^m种可能的树形结构。
- en: 'Many algorithms have been developed to efficiently construct an accurate decision
    tree. Popular ones include the following:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 已经开发了许多算法来高效地构建准确的决策树。常见的包括以下几种：
- en: '**Iterative Dichotomiser 3** (**ID3**): This algorithm uses a greedy search
    in a top-down manner by selecting the best attribute to split the dataset on with
    each iteration without backtracking.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代二分法 3**（**ID3**）：该算法通过贪心搜索的方式，以自上而下的顺序选择最佳属性进行数据集的划分，每次迭代都不进行回溯。'
- en: '**C4.5**: This is an improved version of ID3 that introduces backtracking.
    It traverses the constructed tree and replaces the branches with leaf nodes if
    purity is improved this way.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C4.5**：这是ID3的改进版本，引入了回溯机制。它会遍历已构建的树，并在纯度得到改善时，用叶节点替换分支节点。'
- en: '**Classification and Regression Tree** (**CART**): This constructs the tree
    using binary splitting, which we will discuss in more detail shortly. CART’s flexibility,
    efficiency, interpretability, and robustness make it a popular choice for various
    classification and regression tasks.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类与回归树**（**CART**）：该算法使用二叉分裂来构建决策树，我们稍后会更详细地讨论。CART的灵活性、效率、可解释性和鲁棒性使其成为各种分类和回归任务的热门选择。'
- en: '**Chi-squared Automatic Interaction Detector** (**CHAID**): This algorithm
    is often used in direct marketing. It involves complicated statistical concepts,
    but basically, it determines the optimal way of merging predictive variables in
    order to best explain the outcome.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卡方自动交互检测法**（**CHAID**）：该算法常用于直接营销。它涉及复杂的统计概念，但基本原理是确定合并预测变量的最佳方式，从而最有效地解释结果。'
- en: The basic idea of these algorithms is to grow the tree greedily by making a
    series of local optimizations when choosing the most significant feature to use
    to partition the data. The dataset is then split based on the optimal value of
    that feature. We will discuss the measurement of a significant feature and the
    optimal splitting value of a feature in the next section.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的基本思想是通过在选择最显著特征进行数据划分时进行一系列局部优化来贪婪地生长树。数据集根据该特征的最优值进行划分。我们将在下一节讨论显著特征的度量和特征的最优分割值。
- en: 'First, we will study the CART algorithm in more detail, and we will implement
    it as the most notable decision tree algorithm after that. It constructs the tree
    using binary splitting and grows each node into left and right children. In each
    partition, it greedily searches for the most significant combination of a feature
    and its value; all different possible combinations are tried and tested using
    a measurement function. With the selected feature and value as a splitting point,
    the algorithm then divides the dataset as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将更详细地研究CART算法，然后我们将实现它作为最著名的决策树算法。它使用二元分割构建树，并将每个节点生长为左右子节点。在每次分割中，它贪婪地搜索最显著的特征及其值的组合；所有可能的组合都会被尝试并通过度量函数进行测试。选定的特征和值作为分割点，算法随后将数据集划分如下：
- en: Samples with the feature of this value (for a categorical feature) or a greater
    value (for a numerical feature) become the right child
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有此值特征（对于分类特征）或更大值（对于数值特征）的样本成为右子节点
- en: The remaining samples become the left child
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的样本成为左子节点
- en: 'This partitioning process repeats and recursively divides up the input samples
    into two subgroups. The splitting process stops at a subgroup where either of
    the following two criteria is met:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这个分割过程会重复进行，并递归地将输入样本划分为两个子组。当以下任一标准满足时，分割过程停止：
- en: '**The minimum number of samples for a new node**: When the number of samples
    is not greater than the minimum number of samples required for a further split,
    the partitioning stops in order to prevent the tree from excessively tailoring
    to the training set and, as a result, overfitting.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**新节点的最小样本数**：当样本数不大于进一步分割所需的最小样本数时，分割停止，以防止树过度调整到训练集，从而导致过拟合。'
- en: '**The maximum depth of the tree**: A node stops growing when its depth, which
    is defined as the number of partitions taking place from the top down, starting
    from the root node and ending in a terminal node, meets the maximum tree depth.
    Deeper trees are more specific to the training set and can lead to overfitting.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**树的最大深度**：当节点的深度达到最大树深度时，节点停止生长。深度定义为从根节点到终端节点之间发生的分割次数。更深的树对训练集更加具体，可能导致过拟合。'
- en: A node with no branches becomes a leaf, and the dominant class of samples at
    this node is the prediction. Once all the splitting processes have finished, the
    tree is constructed and is portrayed with the assigned labels at the terminal
    nodes and the splitting points (feature and value) at all the internal nodes above.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 没有分支的节点成为叶子节点，该节点的样本的主导类别就是预测值。一旦所有分割过程完成，树就构建好了，并且在终端节点处标注了分配的标签，所有内部节点上方的分割点（特征和值）也已显示。
- en: We will implement the CART decision tree algorithm from scratch after studying
    the metrics of selecting the optimal splitting feature and value, as promised.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的那样，在研究了选择最优分割特征和数值的度量标准后，我们将从头开始实现CART决策树算法。
- en: The metrics for measuring a split
  id: totrans-64
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 衡量分割的度量标准
- en: When selecting the best combination of a feature and a value as the splitting
    point, two criteria, such as **Gini Impurity** and **Information Gain**, can be
    used to measure the quality of separation.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择最佳的特征和值组合作为分割点时，可以使用两个标准，例如**基尼不纯度**和**信息增益**，来衡量分割的质量。
- en: Gini Impurity
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基尼不纯度
- en: 'Gini Impurity, as its name implies, measures the impurity rate of the class
    distribution of data points, or the class mixture rate. For a dataset with *K*
    classes, suppose that data from class *k(1 ≤ k ≤ K)* takes up a fraction *f*[k]*(0
    ≤ f*[k] *≤ 1)* of the entire dataset; then the *Gini Impurity* of this dataset
    is written as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度，顾名思义，衡量数据点的类别分布的不纯度率，或者类别混合率。对于一个具有*K*类的数据集，假设来自类别*k(1 ≤ k ≤ K)*的数据占整个数据集的比例*f*[k]*(0
    ≤ f*[k]* ≤ 1)，则该数据集的*基尼不纯度*可以表示为：
- en: '![](img/B21047_03_001.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_001.png)'
- en: A lower Gini Impurity indicates a purer dataset. For example, when the dataset
    contains only one class, say, the fraction of this class is `1` and that of the
    others is `0`, its Gini Impurity becomes 1 – (1² + 0²) = 0\. In another example,
    a dataset records a large number of coin flips, and heads and tails each take
    up half of the samples. The Gini Impurity is 1 – (0.5² + 0.5²) = 0.5.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的基尼不纯度意味着数据集更加纯净。例如，当数据集只包含一个类别时，假设该类别的比例为`1`，其他类别的比例为`0`，其基尼不纯度为 1 – (1²
    + 0²) = 0。再举个例子，假设数据集记录了大量掷硬币的结果，正面和反面各占一半样本。此时基尼不纯度为 1 – (0.5² + 0.5²) = 0.5。
- en: 'In binary cases, Gini Impurity, under different values of the positive class
    fraction, can be visualized with the following code blocks:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在二元分类情况下，基尼不纯度在不同的正类比例下可以通过以下代码块进行可视化：
- en: '[PRE0]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The fraction of the positive class varies from `0` to `1`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 正类的比例从`0`变化到`1`：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The Gini Impurity is calculated accordingly, followed by the plot of **Gini
    Impurity** versus **positive fraction**:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度相应地被计算出来，接着是**基尼不纯度**与**正类比例**的图示：
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Here, `1-pos_fraction` is the negative fraction:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，`1-pos_fraction`是负类比例：
- en: '[PRE3]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Refer to *Figure 3.5* for the end result:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 请参见*图 3.5*以获取最终结果：
- en: '![A picture containing line, plot, diagram, screenshot  Description automatically
    generated](img/B21047_03_05.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing line, plot, diagram, screenshot  Description automatically
    generated](img/B21047_03_05.png)'
- en: 'Figure 3.5: Gini Impurity versus positive fraction'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：基尼不纯度与正类比例的关系
- en: As you can see, in binary cases, if the positive fraction is 50%, the impurity
    will be the highest at `0.5`; if the positive fraction is 100% or 0%, it will
    reach `0` impurity.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在二元分类情况下，如果正类比例为 50%，则不纯度将达到最高值`0.5`；如果正类比例为 100%或 0%，则不纯度为`0`。
- en: 'Given the labels of a dataset, we can implement the Gini Impurity calculation
    function as follows:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集的标签，我们可以实现基尼不纯度计算函数，如下所示：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Test it out with some examples:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试一些示例：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In order to evaluate the quality of a split, we simply add up the Gini Impurity
    of all resulting subgroups, combining the proportions of each subgroup as corresponding
    weight factors. And again, the smaller the weighted sum of the Gini Impurity,
    the better the split.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估划分的质量，我们只需将所有结果子组的基尼不纯度加总，并结合每个子组的比例作为相应的权重因子。同样，基尼不纯度的加权和越小，表示划分越好。
- en: 'Take a look at the following self-driving car ad example. Here, we split the
    data based on a user’s gender and interest in technology, respectively:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 看看下面的自动驾驶汽车广告示例。在这里，我们分别根据用户的性别和对技术的兴趣来划分数据：
- en: '![A picture containing text, screenshot, number, font  Description automatically
    generated](img/B21047_03_06.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, number, font  Description automatically
    generated](img/B21047_03_06.png)'
- en: 'Figure 3.6: Splitting the data based on gender or interest in tech'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：根据性别或对技术的兴趣划分数据
- en: 'The weighted Gini Impurity of the first split can be calculated as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第一轮划分的加权基尼不纯度可以按以下方式计算：
- en: '![](img/B21047_03_002.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_002.png)'
- en: 'The second split is as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次划分如下：
- en: '![](img/B21047_03_003.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_003.png)'
- en: Therefore, splitting data based on the user’s interest in technology is a better
    strategy than gender.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据用户对技术的兴趣来划分数据比根据性别划分更为有效。
- en: Information Gain
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息增益
- en: Another metric, **Information Gain**, measures the improvement of purity after
    splitting or, in other words, the reduction of uncertainty due to a split. Higher
    Information Gain implies better splitting. We obtain the Information Gain of a
    split by comparing the **entropy** before and after the split.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个指标，**信息增益**，衡量的是划分后纯度的改善，或者说是通过划分减少的不确定性。信息增益越高，表示划分效果越好。我们通过比较划分前后的**熵**来获得信息增益。
- en: 'Entropy is a probabilistic measure of uncertainty. Given a *K*-class dataset,
    and *f*[k] *(0 ≤* *f*[k] *≤ 1)* denoted as the fraction of data from class *k
    (1 ≤ k ≤ K)*, the *entropy* of the dataset is defined as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 熵是一个表示不确定性的概率度量。给定一个*K*类数据集，且*f*[k] *(0 ≤* *f*[k] *≤ 1)* 表示来自类别 *k (1 ≤ k ≤
    K)* 的数据比例，则该数据集的*熵*定义如下：
- en: '![](img/B21047_03_004.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_004.png)'
- en: 'Lower entropy implies a purer dataset with less ambiguity. In a perfect case,
    where the dataset contains only one class, the entropy is:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 较低的熵意味着数据集更加纯净，模糊性更小。在完美的情况下，若数据集仅包含一个类别，则熵为：
- en: '![](img/B21047_03_005.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_005.png)'
- en: 'In the coin flip example, the entropy becomes:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在掷硬币的例子中，熵变为：
- en: '![](img/B21047_03_006.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_006.png)'
- en: 'Similarly, we can visualize how entropy changes with different values of the
    positive class fraction in binary cases using the following lines of code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以通过以下代码，直观地展示在二分类情况下，正类比例变化时熵的变化：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will give us the following output:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![A picture containing screenshot, plot, line, diagram  Description automatically
    generated](img/B21047_03_07.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, plot, line, diagram  Description automatically
    generated](img/B21047_03_07.png)'
- en: 'Figure 3.7: Entropy versus positive fraction'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.7：熵与正类比例的关系
- en: As you can see, in binary cases, if the positive fraction is 50%, the entropy
    will be the highest at `1`; if the positive fraction is 100% or 0%, it will reach
    `0` entropy.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在二分类情况下，如果正类比例为50%，熵会在`1`时达到最大；如果正类比例为100%或0%，熵将降到`0`。
- en: 'Given the labels of a dataset, the `entropy` calculation function can be implemented
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 给定数据集的标签，可以按如下方式实现`entropy`计算函数：
- en: '[PRE7]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Test it out with some examples:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 用一些例子来测试一下：
- en: '[PRE8]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now that you have fully understood entropy, we can look into how Information
    Gain measures how much uncertainty was reduced after splitting, which is defined
    as the difference in entropy before a split (parent) and after a split (children):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经完全理解了熵的概念，我们可以探讨一下信息增益如何衡量分割后不确定性的减少，定义为分割前（父节点）与分割后（子节点）熵的差异：
- en: '*Information Gain* = *Entropy*(*before*) - *Entropy*(*after*) = *Entropy*(*parent*)
    - *Entropy*(*children*)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*信息增益* = *熵*（*前*） - *熵*（*后*） = *熵*（*父节点*） - *熵*（*子节点*）'
- en: Entropy after a split is calculated as the weighted sum of the entropy of each
    child, which is similar to the weighted Gini Impurity.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 分割后的熵是各子节点熵的加权和，这与加权基尼不纯度类似。
- en: During the process of constructing a node in a tree, our goal is to search for
    the splitting point where the maximum Information Gain is obtained. As the entropy
    of the parent node is unchanged, we just need to measure the entropy of the resulting
    children due to a split. The best split is the one with the lowest entropy of
    its resulting children.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建树节点的过程中，我们的目标是寻找能够获得最大信息增益的分割点。由于父节点的熵不变，我们只需要衡量分割后子节点的熵。最佳分割是子节点熵最低的那个分割。
- en: To understand this better, let’s look at the self-driving car ad example again.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这一点，让我们再次查看自动驾驶汽车广告的例子。
- en: 'For the first option, the entropy after the split can be calculated as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个选项，分割后的熵可以按如下方式计算：
- en: '![](img/B21047_03_007.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_007.png)'
- en: 'The second way of splitting is as follows:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种分割方式如下：
- en: '![](img/B21047_03_008.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_008.png)'
- en: 'For exploration purposes, we can also calculate the Information Gain with:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索，我们还可以计算信息增益，方法如下：
- en: '![](img/B21047_03_009.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_009.png)'
- en: '#1 *Information Gain*=0.971-0.951=0.020'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '#1 *信息增益* = 0.971 - 0.951 = 0.020'
- en: '#2 *Information Gain*=0.971-0.551=0.420'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '#2 *信息增益* = 0.971 - 0.551 = 0.420'
- en: According to the **information Gain = entropy-based evaluation**, the second
    split is preferable, which is the conclusion of the Gini Impurity criterion.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 根据**信息增益 = 基于熵的评估**，第二次分割是更优的，这也是基尼不纯度准则得出的结论。
- en: 'In general, the choice between the two metrics, Gini Impurity and Information
    Gain, has little effect on the performance of the trained decision tree. They
    both measure the weighted impurity of the children after a split. We can combine
    them into one function to calculate the weighted impurity:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，选择这两个指标中的一个——基尼不纯度和信息增益——对训练出的决策树性能影响较小。它们都衡量了分割后子节点的加权不纯度。我们可以将它们合并为一个函数来计算加权不纯度：
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Test it with the example we just hand-calculated, as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 用我们刚刚手动计算的例子来测试，如下所示：
- en: '[PRE10]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now that you have a solid understanding of partitioning evaluation metrics,
    let’s implement the CART tree algorithm from scratch in the next section.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经牢牢掌握了分割评估指标，我们将在下一部分中从零开始实现CART树算法。
- en: Implementing a decision tree from scratch
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从零开始实现决策树
- en: 'We develop the CART tree algorithm by hand on a toy dataset as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们手动在一个玩具数据集上开发CART树算法，步骤如下：
- en: '![](img/B21047_03_08.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_08.png)'
- en: 'Figure 3.8: An example of ad data'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8：一个广告数据的示例
- en: 'To begin with, we decide on the first splitting point, the root, by trying
    out all possible values for each of the two features. We utilize the `weighted_impurity`
    function we just defined to calculate the weighted Gini Impurity for each possible
    combination, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们通过尝试每个特征的所有可能值来决定第一个划分点，即根节点。我们利用刚刚定义的`weighted_impurity`函数来计算每个可能组合的加权基尼不纯度，如下所示：
- en: 'If we partition according to whether the user interest is tech, we have the
    1^(st), 5^(th), and 6^(th) samples for one group and the remaining samples for
    another group. Then the classes for the first group are `[1, 1, 0]`, and the classes
    for the second group are `[0, 0, 0, 1]`:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据用户是否对科技感兴趣来划分数据，我们就有第1、第5和第6个样本作为一组，剩余的样本作为另一组。第一组的类别为`[1, 1, 0]`，第二组的类别为`[0,
    0, 0, 1]`：
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'If we partition according to whether the user’s interest is fashion, we have
    the 2^(nd) and 3^(rd) samples for one group and the remaining samples for another
    group. Then the classes for the first group are `[0, 0]`, and the classes for
    the second group are `[1, 0, 1, 0, 1]`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们根据用户是否对时尚感兴趣来划分数据，我们就有第2和第3个样本作为一组，剩余的样本作为另一组。第一组的类别为`[0, 0]`，第二组的类别为`[1,
    0, 1, 0, 1]`：
- en: '[PRE12]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Similarly, we have the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，我们有以下情况：
- en: '[PRE13]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The root goes to the user interest feature with the fashion value, as this
    combination achieves the lowest weighted impurity or the highest Information Gain.
    We can now build the first level of the tree, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点选择用户兴趣特征中的“时尚”值，因为这个组合实现了最低的加权不纯度或最高的信息增益。我们现在可以构建树的第一层，如下所示：
- en: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_03_09.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, font, number  Description automatically
    generated](img/B21047_03_09.png)'
- en: 'Figure 3.9: Partitioning the data according to “Is interested in fashion?”'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.9：根据“是否对时尚感兴趣？”划分数据
- en: If we are satisfied with a one-level-deep tree, we can stop here by assigning
    the right branch label **0** and the left branch label **1** as the majority class.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对一层深的树感到满意，我们可以在这里停止，将右分支标签**0**和左分支标签**1**作为多数类标签分配。
- en: 'Alternatively, we can go further down the road, constructing the second level
    from the left branch (the right branch cannot be split further):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以继续深入，从左分支构建第二层（右分支无法再分割）：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With the second splitting point specified by `(occupation, professional)` with
    the lowest Gini Impurity, our tree becomes this:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用由`(occupation, professional)`指定的第二个划分点，具有最低的基尼不纯度，我们的树变成了这样：
- en: '![A picture containing text, screenshot, diagram, number  Description automatically
    generated](img/B21047_03_10.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, diagram, number  Description automatically
    generated](img/B21047_03_10.png)'
- en: 'Figure 3.10: Further partitioning of the data according to “Is occupation professional?”'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：根据“职业是否为专业？”进一步划分数据
- en: We can repeat the splitting process as long as the tree does not exceed the
    maximum depth and the node contains enough samples.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 只要树的深度不超过最大深度且节点包含足够的样本，我们就可以继续划分过程。
- en: Now that the process of the tree construction has been made clear, it is time
    for coding.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在树构建的过程已经清楚了，是时候开始编码了。
- en: 'We start with defining a utility function to split a node into left and right
    children based on a feature and a value:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先定义一个工具函数，根据特征和值将节点划分为左右子节点：
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We check whether the feature is numerical or categorical and split the data
    accordingly.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查特征是数值型还是类别型，并相应地划分数据。
- en: 'With the splitting measurement and generation functions available, we now define
    the greedy search function, which tries out all possible splits and returns the
    best one given a selection criterion, along with the resulting children:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 有了划分测量和生成函数，我们现在定义贪心搜索函数，它尝试所有可能的划分，并根据选择标准返回最佳划分及其结果的子节点：
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The selection and splitting process occurs in a recursive manner on each of
    the subsequent children. When a stopping criterion is met, the process stops at
    a node, and the major label is assigned to this leaf node:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 选择和划分过程会在每个后续子节点上以递归方式进行。当满足停止标准时，过程会在某个节点停止，并将主要标签分配给该叶节点：
- en: '[PRE17]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'And, finally, the recursive function links all of them together:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，递归函数将它们全部连接起来：
- en: It assigns a leaf node if one of two child nodes is empty
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果两个子节点之一为空，它就会分配一个叶节点
- en: It assigns a leaf node if the current branch depth exceeds the maximum depth
    allowed
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果当前分支的深度超过允许的最大深度，它就会分配一个叶节点
- en: It assigns a leaf node if the node does not contain sufficient samples required
    for a further split
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果节点不包含进行进一步分裂所需的足够样本，它将分配一个叶节点
- en: Otherwise, it proceeds with a further split with the optimal splitting point
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，程序将继续按照最优分裂点进行进一步分裂
- en: 'This can be done with the following function:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下函数实现：
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The function first extracts the left and right children from the node dictionary.
    It then checks whether either the left or right child is empty. If so, it assigns
    a leaf node to the corresponding child. Next, it checks whether the current depth
    exceeds the maximum depth allowed for the tree. If so, it assigns leaf nodes to
    both children. If the left child has enough samples to split (greater than `min_size`),
    it computes the best split using the `get_best_split` function. If the resulting
    split produces empty children, it assigns a leaf node to the corresponding child;
    otherwise, it recursively calls the `split` function on the left child. Similar
    steps are repeated for the right child.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数首先从节点字典中提取左右子节点。然后检查左子节点或右子节点是否为空。如果为空，它会给相应的子节点分配一个叶节点。接着，它检查当前深度是否超过树允许的最大深度。如果超过，则为左右子节点都分配叶节点。如果左子节点有足够的样本进行分裂（大于
    `min_size`），它将使用 `get_best_split` 函数计算最佳分裂。如果结果分裂产生空子节点，它将为相应子节点分配叶节点；否则，它会递归地对左子节点调用
    `split` 函数。右子节点执行类似的步骤。
- en: 'Finally, the entry point of the tree’s construction is as follows:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，树构建的入口点如下：
- en: '[PRE19]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Now, let’s test it with the preceding hand-calculated example:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用前面的手工计算例子来测试它：
- en: '[PRE20]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'To verify that the resulting tree from the model is identical to what we constructed
    by hand, we write a function displaying the tree:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证模型生成的树与我们手工构建的树是相同的，我们编写一个函数来显示这棵树：
- en: '[PRE21]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can test it with a numerical example, as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下数值例子进行测试：
- en: '[PRE22]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The resulting trees from our decision tree model are the same as those we hand-crafted.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的决策树模型生成的树与我们手工制作的树相同。
- en: Now that you have a more solid understanding of decision trees after implementing
    one from scratch, we can move on with implementing a decision tree with scikit-learn.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在通过从头实现一个决策树后，你对决策树有了更深入的理解，我们可以继续使用 scikit-learn 实现决策树。
- en: Implementing a decision tree with scikit-learn
  id: totrans-179
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 实现决策树
- en: 'Here, we’ll use scikit-learn’s decision tree module ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)),
    which is already well developed and optimized:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 scikit-learn 的决策树模块（[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)），该模块已经得到充分开发和优化：
- en: '[PRE23]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To visualize the tree we just built, we utilize the built-in `export_graphviz`
    function, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化我们刚刚构建的树，我们利用内置的 `export_graphviz` 函数，如下所示：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Running this will generate a file called `tree.dot`, which can be converted
    into a PNG image file using **Graphviz** (the introduction and installation instructions
    can be found at [http://www.graphviz.org](http://www.graphviz.org)) by running
    the following command in the terminal:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令将生成一个名为 `tree.dot` 的文件，可以通过 **Graphviz** 转换为 PNG 图像文件（介绍和安装说明可在 [http://www.graphviz.org](http://www.graphviz.org)
    找到），方法是在终端中运行以下命令：
- en: '[PRE25]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Refer to *Figure 3.11* for the result:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考*图 3.11*查看结果：
- en: '![A picture containing text, handwriting, font, screenshot  Description automatically
    generated](img/B21047_03_11.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, handwriting, font, screenshot  Description automatically
    generated](img/B21047_03_11.png)'
- en: 'Figure 3.11: Tree visualization'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11：树形结构可视化
- en: The generated tree is essentially the same as the one we had before.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的树与我们之前构建的树基本相同。
- en: I know you can’t wait to employ a decision tree to predict ad click-through.
    Let’s move on to the next section.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你迫不及待想使用决策树来预测广告点击率。让我们继续下一部分。
- en: Predicting ad click-through with a decision tree
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树预测广告点击率
- en: After several examples, it is now time to predict ad click-through using the
    decision tree algorithm you have just thoroughly learned about and practiced with.
    We will use the dataset from a Kaggle machine learning competition, *Click-Through
    Rate Prediction* ([https://www.kaggle.com/c/avazu-ctr-prediction](https://www.kaggle.com/c/avazu-ctr-prediction)).
    The dataset can be downloaded from [https://www.kaggle.com/c/avazu-ctr-prediction/data](https://www.kaggle.com/c/avazu-ctr-prediction/data).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几个示例后，现在是时候使用你刚刚学过并练习过的决策树算法来预测广告点击率了。我们将使用Kaggle机器学习竞赛的数据集，*点击率预测*（[https://www.kaggle.com/c/avazu-ctr-prediction](https://www.kaggle.com/c/avazu-ctr-prediction)）。该数据集可以从[https://www.kaggle.com/c/avazu-ctr-prediction/data](https://www.kaggle.com/c/avazu-ctr-prediction/data)下载。
- en: Only the `train.gz` file contains labeled samples, so we only need to download
    this and unzip it (it will take a while). In this chapter, we will focus on only
    the first 300,000 samples from the `train.csv` file unzipped from `train.gz`.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 只有`train.gz`文件包含标注样本，因此我们只需要下载此文件并解压（这可能需要一些时间）。在本章中，我们将仅关注从`train.gz`解压出的`train.csv`文件中的前300,000个样本。
- en: 'The fields in the raw file are as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 原始文件中的字段如下：
- en: '![A screenshot of a computer  Description automatically generated with low
    confidence](img/B21047_03_12.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![计算机截图  描述自动生成，信心较低](img/B21047_03_12.png)'
- en: 'Figure 3.12: Description and example values of the dataset'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.12：数据集的描述和示例值
- en: 'We take a glance at the head of the file by running the following command:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行以下命令来快速查看文件的开头：
- en: '[PRE26]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Rather than a simple `head train`, the output is cleaner as all the columns
    are aligned:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 与简单的`head train`不同，输出更加整洁，因为所有列都对齐了：
- en: '![A picture containing text, screenshot, black and white, menu  Description
    automatically generated](img/B21047_03_13.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本的图片，截图，黑白，菜单  描述自动生成](img/B21047_03_13.png)'
- en: 'Figure 3.13: The first few rows of the data'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13：数据的前几行
- en: Don’t be scared by the anonymized and hashed values. They are categorical features,
    and each of their possible values corresponds to a real and meaningful value,
    but it is presented this way due to the privacy policy. Possibly, `C1` means user
    gender, and `1005` and `1002` represent male and female, respectively.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被匿名化和哈希化的值吓到。它们是分类特征，每个可能的值都对应着一个真实且有意义的值，但由于隐私政策，它们以这种方式呈现。可能`C1`代表用户性别，`1005`和`1002`分别表示男性和女性。
- en: 'Now, let’s start by reading the dataset using `pandas`. That’s right, `pandas`
    is extremely good at handling data in a tabular format:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过使用`pandas`来读取数据集。没错，`pandas`在处理表格数据时非常高效：
- en: '[PRE27]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The first 300,000 lines of the file are loaded and stored in a DataFrame. Take
    a quick look at the first five rows of the DataFrame:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的前300,000行已加载并存储在一个DataFrame中。快速查看DataFrame的前五行：
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The target variable is the `click` column:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 目标变量是`click`列：
- en: '[PRE29]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For the remaining columns, there are several columns that should be removed
    from the features (`id`, `hour`, `device_id`, and `device_ip`) as they do not
    contain much useful information:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其余的列，有几个列应从特征中移除（`id`、`hour`、`device_id`和`device_ip`），因为它们不包含太多有用的信息：
- en: '[PRE30]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Each sample has `19` predictive attributes.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 每个样本有`19`个预测属性。
- en: 'Next, we need to split the data into training and testing sets. Normally, we
    do this by randomly picking samples. However, in our case, the samples are in
    chronological order, as indicated in the `hour` field. Obviously, we cannot use
    future samples to predict past ones. Hence, we take the first 90% as training
    samples and the rest as testing samples:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将数据划分为训练集和测试集。通常，我们通过随机选择样本来进行划分。然而，在我们的案例中，样本是按时间顺序排列的，正如`hour`字段所示。显然，我们不能使用未来的样本来预测过去的样本。因此，我们将前90%作为训练样本，其余的作为测试样本：
- en: '[PRE31]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As mentioned earlier, decision tree models can take in categorical features.
    However, because the tree-based algorithms in scikit-learn (the current version
    is 1.4.1 as of early 2024) only allow numeric input, we need to transform the
    categorical features into numerical ones. But note that, in general, we do not
    need to do this; for example, the decision tree classifier we developed from scratch
    earlier can directly take in categorical features.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，决策树模型可以接受分类特征。然而，由于scikit-learn中的树基算法（截至2024年初的当前版本为1.4.1）仅允许数值输入，我们需要将分类特征转换为数值特征。但请注意，通常我们不需要这么做；例如，我们之前从零开始开发的决策树分类器就可以直接接受分类特征。
- en: We will now transform string-based categorical features into one-hot encoded
    vectors using the `OneHotEncoder` module from `scikit-learn`. One-hot encoding
    was briefly mentioned in *Chapter 1*, *Getting Started with Machine Learning and
    Python*. To recap, it basically converts a categorical feature with *k* possible
    values into *k* binary features. For example, the site category feature with three
    possible values, `news`, `education`, and `sports`, will be encoded into three
    binary features, such as `is_news`, `is_education`, and `is_sports`, whose values
    are either `1` or `0`.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用`scikit-learn`中的`OneHotEncoder`模块将基于字符串的分类特征转换为one-hot编码向量。one-hot编码在*第1章*、*机器学习与Python入门*中简要提到过。简而言之，它将具有*k*个可能值的分类特征转换为*k*个二进制特征。例如，具有三个可能值`news`、`education`和`sports`的网站类别特征，将被编码为三个二进制特征，如`is_news`、`is_education`和`is_sports`，其值为`1`或`0`。
- en: 'We initialize a `OneHotEncoder` object as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们初始化一个`OneHotEncoder`对象如下：
- en: '[PRE32]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We fit it on the training set as follows:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练集上拟合模型如下：
- en: '[PRE33]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Each converted sample is a sparse vector.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 每个转换后的样本都是一个稀疏向量。
- en: 'We transform the testing set using the trained one-hot encoder as follows:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用训练好的one-hot编码器对测试集进行转换，如下所示：
- en: '[PRE34]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Remember, we specified the `handle_unknown='ignore'` parameter in the one-hot
    encoder earlier. This is to prevent errors due to any unseen categorical values.
    To use the previous site category example, if there is a sample with the value
    `movie`, all of the three converted binary features (`is_news`, `is_education`,
    and `is_sports`) become `0`. If we do not specify `ignore`, an error will be raised.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们在之前的one-hot编码器中指定了`handle_unknown='ignore'`参数。这是为了防止因遇到未见过的分类值而导致错误。以之前的网站类别例子为例，如果有一个样本值为`movie`，那么所有三个转换后的二进制特征（`is_news`、`is_education`和`is_sports`）的值都会变成`0`。如果我们没有指定`ignore`，将会抛出错误。
- en: 'The way we have conducted cross-validation so far is to explicitly split data
    into folds and repetitively write a `for` loop to consecutively examine each hyperparameter.
    To make this less redundant, we’ll introduce a more elegant approach utilizing
    the `GridSearchCV` module from scikit-learn. `GridSearchCV` handles the entire
    process implicitly, including data splitting, fold generation, cross-training
    and validation, and finally, an exhaustive search over the best set of parameters.
    What is left for us is just to specify the hyperparameter(s) to tune and the values
    to explore for each individual hyperparameter. For demonstration purposes, we
    will only tweak the `max_depth` hyperparameter (other hyperparameters, such as
    `min_samples_split` and `class_weight`, are also highly recommended):'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们进行交叉验证的方式是显式地将数据划分为折叠，并重复写一个`for`循环来逐个检查每个超参数。为了减少这种冗余，我们将引入一种更优雅的方法，使用来自scikit-learn的`GridSearchCV`模块。`GridSearchCV`隐式处理整个过程，包括数据拆分、折叠生成、交叉训练与验证，最后进行最佳参数的全面搜索。剩下的工作就是指定要调整的超参数以及每个超参数要探索的值。为了演示目的，我们将只调整`max_depth`超参数（其他超参数，如`min_samples_split`和`class_weight`，也强烈推荐调整）：
- en: '[PRE35]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We pick three options for the maximal depth – `3`, `10`, and unbounded. We
    initialize a decision tree model with Gini Impurity as the metric and `30` as
    the minimum number of samples required to split further:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为最大深度选择了三个选项——`3`、`10`和无限制。我们初始化一个使用基尼不纯度作为度量，并且将`30`作为进一步分裂所需的最小样本数的决策树模型：
- en: '[PRE36]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The classification metric should be the AUC of the ROC curve, as it is an imbalanced
    binary case (only 51,211 out of 300,000 training samples are clicks, which is
    a 17% positive CTR; I encourage you to figure out the class distribution yourself).
    As for grid search, we use three-fold (as the training set is relatively small)
    cross-validation and select the best-performing hyperparameter measured by the
    AUC:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 分类度量应该是ROC曲线的AUC，因为这是一个不平衡的二分类问题（在300,000个训练样本中，只有51,211个是点击，正样本CTR为17%；我鼓励你自己去计算类分布）。至于网格搜索，我们使用三折交叉验证（因为训练集相对较小），并选择通过AUC衡量的最佳超参数：
- en: '[PRE37]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Note, `n_jobs=-1` means that we use all of the available CPU processors:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`n_jobs=-1`意味着我们使用所有可用的CPU处理器：
- en: '[PRE38]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We use the model with the optimal parameter to predict any future test cases
    as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用具有最佳参数的模型对任何未来的测试案例进行预测，如下所示：
- en: '[PRE39]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The AUC we can achieve with the optimal decision tree model is 0.72\. This
    does not seem to be very high, but click-through involves many intricate human
    factors, which is why predicting it is not an easy task. Although we can further
    optimize the hyperparameters, an AUC of 0.72 is actually pretty good. As a comparison,
    randomly selecting 17% of the samples to be clicked on will generate an AUC of
    `0.499`:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在最优决策树模型上能达到的AUC值是0.72。这看起来并不是很高，但点击率涉及许多复杂的人为因素，这就是为什么预测点击率并不是一项容易的任务。尽管我们可以进一步优化超参数，0.72的AUC实际上已经相当不错了。作为对比，随机选择17%的样本进行点击会生成AUC值`0.499`：
- en: '[PRE40]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Our decision tree model significantly outperforms the random predictor. Looking
    back, we can see that a decision tree is a sequence of greedy searches for the
    best splitting point at each step, based on the training dataset. However, this
    tends to cause overfitting as it is likely that the optimal points only work well
    for the training samples. Fortunately, ensembling is the technique to correct
    this, and random forest is an ensemble tree model that usually outperforms a simple
    decision tree.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的决策树模型显著优于随机预测器。回头看，我们可以看到，决策树是一个基于训练数据集在每一步进行贪心搜索，寻找最佳分裂点的过程。然而，这往往会导致过拟合，因为最优分裂点很可能仅对训练样本有效。幸运的是，集成方法可以纠正这一点，随机森林就是一种通常优于简单决策树的集成树模型。
- en: '**Best practice**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'Here are two best practices for getting data ready for tree-based algorithms:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是为树算法准备数据的两个最佳实践：
- en: '**Encode categorical features**: As mentioned earlier, we need to encode categorical
    features before feeding them into the models. One-hot encoding and label encoding
    are popular choices.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**编码类别特征**：如前所述，我们需要在将类别特征输入模型之前进行编码。独热编码和标签编码是常用的选择。'
- en: '**Scale numerical features**: We need to pay attention to the scales of numerical
    features to prevent features with larger scales from dominating the splitting
    decisions in the tree. Normalization or standardization are commonly used for
    this purpose.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缩放数值特征**：我们需要注意数值特征的尺度，以防止尺度较大的特征在树的分裂决策中占主导地位。归一化或标准化通常用于此目的。'
- en: Ensembling decision trees – random forests
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成决策树——随机森林
- en: The **ensemble** technique of **bagging** (which stands for **bootstrap aggregating**),
    which I briefly mentioned in *Chapter 1*, *Getting Started with Machine Learning
    and Python*, can effectively overcome overfitting. To recap, different sets of
    training samples are randomly drawn with replacements from the original training
    data; each resulting set is used to fit an individual classification model. The
    results of these separately trained models are then combined together through
    a **majority vote** to make the final decision.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '**集成**技术中的**袋装法**（即**自助聚合**），我在*第1章*《*机器学习与Python入门*》中简要提到过，能够有效克服过拟合问题。回顾一下，不同的训练样本集从原始训练数据中随机抽取并有放回地选取；每个得到的样本集用于拟合一个单独的分类模型。然后，这些分别训练的模型的结果通过**多数投票**结合在一起，做出最终决策。'
- en: Tree bagging, as described in the preceding paragraph, reduces the high variance
    that a decision tree model suffers from and, hence, in general, performs better
    than a single tree. However, in some cases, where one or more features are strong
    indicators, individual trees are constructed largely based on these features and,
    as a result, become highly correlated. Aggregating multiple correlated trees will
    not make much difference. To force each tree to become uncorrelated, random forest
    only considers a random subset of the features when searching for the best splitting
    point at each node. Individual trees are now trained based on different sequential
    sets of features, which guarantees more diversity and better performance. Random
    forest is a variant of the tree bagging model with additional **feature-based
    bagging**.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文所述，树袋装法可以减少决策树模型的高方差，因此通常比单一的决策树表现更好。然而，在某些情况下，如果一个或多个特征是强指示器，个体树会在很大程度上根据这些特征构建，因此可能高度相关。聚合多个相关树不会有太大区别。为了让每棵树都不相关，随机森林在搜索每个节点的最佳分裂点时只考虑特征的随机子集。个体树现在基于不同的特征顺序集进行训练，这保证了更多的多样性和更好的表现。随机森林是树袋装模型的一种变体，具有额外的**基于特征的袋装**。
- en: 'To employ random forest in our click-through prediction project, we can use
    the package from scikit-learn. Similarly to the way we implemented the decision
    tree in the preceding section, we only tweak the `max_depth` parameter:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们的点击率预测项目中使用随机森林，我们可以使用来自 scikit-learn 的包。与我们在前一节中实现决策树的方式类似，我们只需调整 `max_depth`
    参数：
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Besides `max_depth`, `min_samples_split`, and `class_weight`, which are important
    hyperparameters related to a single decision tree, hyperparameters that are related
    to a random forest (a set of trees) such as `n_estimators` are also highly recommended.
    We fine-tune `max_depth` as follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `max_depth`、`min_samples_split` 和 `class_weight`，这些是与单个决策树相关的重要超参数之外，还强烈推荐与随机森林（一组树）相关的超参数，例如
    `n_estimators`。我们如下微调 `max_depth`：
- en: '[PRE42]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We use the model with the optimal parameter `None` for `max_depth` (the nodes
    are expanded until another stopping criterion is met) to predict any future unseen
    cases:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用具有最优参数 `None` 的模型来预测任何未来未见案例（节点扩展直至满足另一个停止标准）：
- en: '[PRE43]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: It turns out that the random forest model gives a substantial lift to the performance.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，随机森林模型大大提升了性能。
- en: 'Let’s summarize several critical hyperparameters to tune:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结几个关键的超参数调整：
- en: '`max_depth`: This is the deepest individual tree. It tends to overfit if it
    is too deep or underfit if it is too shallow.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`：这是单个树的最大深度。如果太深可能会过拟合，如果太浅可能会欠拟合。'
- en: '`min_samples_split`: This hyperparameter represents the minimum number of samples
    required for further splitting at a node. Too small a value tends to cause overfitting,
    while too large a value is likely to introduce underfitting. `10`, `30`, and `50`
    might be good options to start with.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_split`：此超参数表示在节点进一步分割所需的最小样本数。过小的值往往会导致过拟合，而过大的值可能会引入欠拟合。`10`、`30`
    和 `50` 可能是良好的起始选项。'
- en: 'The preceding two hyperparameters are generally related to individual decision
    trees. The following two parameters are more related to a random forest or collection
    of trees:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 上述两个超参数通常与单个决策树相关。以下两个参数更与随机森林或树集合相关：
- en: '`max_features`: This parameter represents the number of features to consider
    for each best splitting point search. Typically, for an *m*-dimensional dataset,
    ![](img/B21047_03_010.png) (rounded) is a recommended value for `max_features`.
    This can be specified as `max_features="sqrt"` in scikit-learn. Other options
    include `log2`, 20%, and 50% of the original features.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_features`：此参数表示每次最佳分割点搜索考虑的特征数。通常情况下，对于一个 *m* 维数据集，推荐使用 ![](img/B21047_03_010.png)（四舍五入）。在
    scikit-learn 中可以指定为 `max_features="sqrt"`。其他选项包括 `log2`，20%，和原始特征的 50%。'
- en: '`n_estimators`: This parameter represents the number of trees considered for
    majority voting. Generally speaking, the more trees, the better the performance
    but the longer the computation time. It is usually set as `100`, `200`, `500`,
    and so on.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_estimators`：此参数表示用于多数投票考虑的树的数量。一般来说，树越多，性能越好，但计算时间越长。通常设置为 `100`、`200`、`500`
    等。'
- en: Next, we’ll discuss gradient-boosted trees.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论梯度提升树。
- en: Ensembling decision trees – gradient-boosted trees
  id: totrans-258
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成决策树 - 梯度提升树
- en: '**Boosting**, which is another ensemble technique, takes an iterative approach
    instead of combining multiple learners in parallel. In boosted trees, individual
    trees are no longer trained separately. Specifically, in **Gradient-Boosted Trees**
    (**GBT**) (also called **Gradient-Boosting Machines**), individual trees are trained
    in succession where a tree aims to correct the errors made by the previous tree.
    The following two diagrams illustrate the difference between random forest and
    GBT.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**提升**，这是另一种集成技术，采用迭代方法而不是并行组合多个学习器。在提升树中，不再单独训练个体树。具体来说，在**梯度提升树**（GBT）（也称为**梯度提升机**）中，个体树按顺序训练，其中每棵树旨在纠正前一棵树的错误。以下两个图表说明了随机森林和GBT之间的差异。'
- en: 'The random forest model builds each tree independently using a different subset
    of the dataset, and then combines the results at the end by majority votes or
    averaging:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型独立构建每棵树，使用数据集的不同子集，然后通过多数投票或平均结果结合：
- en: '![A diagram of a tree  Description automatically generated with low confidence](img/B21047_03_14.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![一棵树的图表 描述自动生成，置信度低](img/B21047_03_14.png)'
- en: 'Figure 3.14: The random forest workflow'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.14：随机森林工作流程
- en: 'The GBT model builds one tree at a time and combines the results along the
    way:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: GBT模型一次构建一棵树，并在此过程中不断组合结果：
- en: '![](img/B21047_03_15.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_03_15.png)'
- en: 'Figure 3.15: The GBT workflow'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.15：GBT工作流程
- en: 'GBT works by iteratively improving the ensemble’s predictions through the addition
    of sequentially trained decision trees, with each tree focusing on the residuals
    of the previous ones. Here’s how it works:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: GBT通过不断改进集成模型的预测来工作，通过依次添加经过训练的决策树，每棵树都集中在前一棵树的残差上。它是这样工作的：
- en: '**Initialization**: The process starts with an initial simple model, often
    a single decision tree, which serves as the starting point for the ensemble.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**初始化**：该过程从一个初始的简单模型开始，通常是一个单一的决策树，它作为集成方法的起始点。'
- en: '**Sequential training**: Subsequent decision trees are trained sequentially,
    with each tree attempting to correct the errors of the previous ones. Each new
    tree is trained on the residuals (the differences between the actual and predicted
    values) of the ensemble’s predictions from the previous trees.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**顺序训练**：后续的决策树按顺序训练，每棵树都试图纠正前一棵树的错误。每棵新树都是在集成模型前一棵树的预测残差（实际值与预测值之间的差异）上进行训练的。'
- en: '**Additive modeling**: Each new decision tree is added to the ensemble in a
    way that minimizes the overall error. The trees are typically shallow, with a
    limited number of nodes, to avoid overfitting and improve generalization.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**加法建模**：每棵新决策树都以最小化总体误差的方式被添加到集成模型中。这些树通常是浅层的，具有有限的节点数，以避免过拟合并提高泛化能力。'
- en: '**Learning rate**: GBT introduces a learning rate parameter, which controls
    the contribution of each tree to the ensemble. A lower learning rate leads to
    slower learning but can enhance the overall performance and stability of the ensemble.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习率**：GBT引入了一个学习率参数，它控制每棵树对集成模型的贡献。较低的学习率会导致学习进程较慢，但能够提升集成模型的整体表现和稳定性。'
- en: '**Ensemble prediction**: The final prediction is made by combining the predictions
    of all the trees in the ensemble.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成预测**：最终预测是通过将所有树的预测结果结合起来得到的。'
- en: 'We will use the `XGBoost` package ([https://xgboost.readthedocs.io/en/latest/](https://xgboost.readthedocs.io/en/latest/))
    to implement GBT. We first install the `XGBoost Python API` via the following
    command with `conda`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`XGBoost`包（[https://xgboost.readthedocs.io/en/latest/](https://xgboost.readthedocs.io/en/latest/)）来实现GBT。我们首先通过以下命令使用`conda`安装`XGBoost
    Python API`：
- en: '[PRE44]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can also use `pip`, as follows:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用`pip`，如下所示：
- en: '[PRE45]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'If you run into a problem, please install or upgrade `CMake` (a cross-platform
    build system generator), as follows:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你遇到问题，请安装或升级`CMake`（一个跨平台的构建系统生成器），如下所示：
- en: '[PRE46]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s now take a look at the following steps. You will see how we predict clicks
    using GBT:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看接下来的步骤。你将看到我们如何使用GBT预测点击：
- en: 'We import XGBoost and initialize a GBT model:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入XGBoost并初始化GBT模型：
- en: '[PRE47]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We set the learning rate to `0.1`, which determines how fast or slow we want
    to proceed with learning in each step (in each tree, in GBT). We will discuss
    the learning rate in more detail in *Chapter 4*, *Predicting Online Ad Click-Through
    with Logistic Regression*. `max_depth` for individual trees is set to 10\. Additionally,
    1,000 trees will be trained in sequence in our GBT model.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习率设置为`0.1`，它决定了我们在每一步（在每棵树上，GBT中的每一步）学习的速度。我们将在*第4章*，*使用逻辑回归预测在线广告点击率*中详细讨论学习率。`max_depth`的值为10，表示每棵树的最大深度。此外，我们将在GBT模型中按顺序训练1,000棵树。
- en: 'Next, we train the GBT model on the training set we prepared previously:'
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将在之前准备的训练集上训练GBT模型：
- en: '[PRE48]'
  id: totrans-283
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We use the trained model to make predictions on the testing set and calculate
    the ROC AUC accordingly:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用训练好的模型对测试集进行预测，并相应地计算ROC AUC：
- en: '[PRE49]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: We are able to achieve `0.77` AUC using the XGBoost GBT model.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用XGBoost GBT模型达到了`0.77`的AUC。
- en: In this section, you learned about another type of tree ensembling, GBT, and
    applied it to our ad click-through prediction.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你了解了另一种树集成方法——GBT，并将其应用于我们的广告点击预测。
- en: '**Best practice**'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**'
- en: 'So, you’ve learned about several tree-based algorithms in the chapter – awesome!
    But picking the right one can be tricky. Here is a practical guide:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，你在本章中学到了几种基于树的算法——太棒了！但是，选择合适的算法可能会有点棘手。这里有一个实用的指南：
- en: '**Decision tree (CART)**: This is the most simple and interpretable algorithm.
    We usually use it for smaller datasets.'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树（CART）**：这是最简单且最易解释的算法。我们通常将其用于较小的数据集。'
- en: '**Random forest**: This is more robust to overfitting, and can handle larger
    or complex datasets well.'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**：它对过拟合更具鲁棒性，能够处理较大或复杂的数据集。'
- en: '**GBT**: This is considered the most powerful algorithm for complex problems,
    and the most popular tree-based algorithm in the industry. At the same time, however,
    it can be prone to overfitting. Hence, using hyperparameter tuning and regularization
    techniques to avoid overfitting is recommended.'
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GBT**：这是被认为是最强大的算法，适用于复杂问题，也是行业中最流行的基于树的算法。然而，它也容易发生过拟合。因此，建议使用超参数调优和正则化技术来避免过拟合。'
- en: Summary
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started with an introduction to a typical machine learning
    problem, online ad click-through prediction, and its inherent challenges, including
    categorical features. We then looked at tree-based algorithms that can take in
    both numerical and categorical features.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了一个典型的机器学习问题——在线广告点击率预测及其固有的挑战，包括分类特征。然后，我们研究了可以处理数值特征和分类特征的基于树的算法。
- en: 'Next, we had an in-depth discussion about the decision tree algorithm: its
    mechanics, its different types, how to construct a tree, and two metrics (Gini
    Impurity and entropy) that measure the effectiveness of a split at a node. After
    constructing a tree by hand, we implemented the algorithm from scratch.'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们深入讨论了决策树算法：它的原理、不同类型、如何构建树，以及衡量节点分裂有效性的两个指标（基尼不纯度和熵）。在手动构建树之后，我们从零开始实现了该算法。
- en: You also learned how to use the decision tree package from scikit-learn and
    applied it to predict the CTR. We continued to improve performance by adopting
    the feature-based random forest bagging algorithm. Finally, the chapter ended
    with several ways in which to tune a random forest model, along with two different
    ways of ensembling decision trees, random forest and GBT modeling. Bagging and
    boosting are two approaches to model ensembling that can improve learning performance.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 你还学习了如何使用scikit-learn中的决策树包，并将其应用于预测CTR。我们通过采用基于特征的随机森林袋装算法，继续提高性能。最后，本章以几种调优随机森林模型的方法结束，并介绍了两种不同的集成决策树方法：随机森林和GBT建模。袋装（Bagging）和提升（Boosting）是两种可以提高学习性能的模型集成方法。
- en: 'More practice is always good for honing your skills. I recommend that you complete
    the following exercises before moving on to the next chapter, where we will solve
    ad click-through prediction using another algorithm: **logistic regression**.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的练习总是有助于磨练你的技能。我建议在进入下一章之前，完成以下练习，我们将使用另一种算法：**逻辑回归**来解决广告点击率预测问题。
- en: Exercises
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: In the decision tree click-through prediction project, can you also tweak other
    hyperparameters, such as `min_samples_split` and `class_weight`? What is the highest
    AUC you are able to achieve?
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在决策树点击率预测项目中，你是否可以调整其他超参数，如`min_samples_split`和`class_weight`？你能够达到的最高AUC是多少？
- en: In the random forest-based click-through prediction project, can you also tweak
    other hyperparameters, such as `min_samples_split`, `max_features`, and `n_estimators`,
    in scikit-learn? What is the highest AUC you are able to achieve?
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于随机森林的点击率预测项目中，你是否可以调整其他超参数，如`min_samples_split`、`max_features`和`n_estimators`，在scikit-learn中？你能够达到的最高AUC是多少？
- en: In the GBT-based click-through prediction project, what hyperparameters can
    you tweak? What is the highest AUC you are able to achieve? You can read [https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)
    to figure it out.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在基于GBT的点击率预测项目中，你可以调整哪些超参数？你能够达到的最高AUC是多少？你可以阅读[https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn)来找出答案。
- en: Join our book’s Discord space
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-305
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1878468721786989681.png)'
