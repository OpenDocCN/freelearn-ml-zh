- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: AI/ML Tooling and the Google Cloud AI/ML Landscape
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AI/ML 工具和 Google Cloud AI/ML 生态系统
- en: In this chapter, we take a look at the various tools in Google Cloud that can
    be used to implement AI/ML workloads. We start off with a quick overview of some
    of the fundamental Google Cloud services that function as the building blocks
    for almost all workloads on Google Cloud. We then progress toward more advanced
    services that are used specifically for data science and AI/ML workloads. This
    is the final chapter in the *Basics* part of this book, and like the previous
    two chapters, it provides foundational information that we build upon throughout
    the book. If you already have knowledge of Google Cloud’s services, this chapter
    may serve as a refresher for that knowledge. If you are new to Google Cloud, this
    chapter is an essential part of your learning process, because it introduces concepts
    that are assumed to be known in the rest of the book.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨 Google Cloud 中可用于实现 AI/ML 工作负载的各种工具。我们首先快速概述一些作为 Google Cloud 上几乎所有工作负载基石的基本
    Google Cloud 服务。然后，我们进一步探讨专门用于数据科学和 AI/ML 工作负载的更高级服务。这是本书“基础知识”部分的最后一章，就像前两章一样，它提供了我们在整本书中构建的基础信息。如果您已经了解
    Google Cloud 的服务，本章可能有助于巩固这些知识。如果您是 Google Cloud 的新手，本章是您学习过程中的一个重要部分，因为它介绍了本书其他部分假定已知的概念。
- en: To describe how each tool is used in the context of data science projects, we
    will refer to the steps of the ML model lifecycle, as laid out in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035).
    *Figure 3**.1* shows a simplified diagram of the ML model lifecycle. In reality,
    combinations of these steps could be repeated in cycles throughout the model lifecycle,
    but we will omit those details for simplicity at this point. Our simplified workflow
    example assumes that the outputs from each step are satisfactory, and we can move
    on to the next step in the process. It also includes hyperparameter optimization
    in the **Train** **Model** step.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了描述每个工具在数据科学项目中的应用，我们将参考第 2 章中概述的 ML 模型生命周期步骤。[图 3.1]*显示了一个简化了的 ML 模型生命周期图。实际上，这些步骤的组合可以在模型生命周期中循环重复，但为了简化，我们在此省略这些细节。我们的简化工作流程示例假设每个步骤的输出都是令人满意的，我们可以继续到下一个步骤。它还包括在**训练模型**步骤中的超参数优化。
- en: '![Figure 3.1: Simplified ML model lifecycle](img/B18143_03_1.jpg)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.1：简化 ML 模型生命周期](img/B18143_03_1.jpg)'
- en: 'Figure 3.1: Simplified ML model lifecycle'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1：简化 ML 模型生命周期
- en: 'This chapter will cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Why Google Cloud?
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么选择 Google Cloud？
- en: Prerequisites for using Google Cloud tools
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Google Cloud 工具的先决条件
- en: Google Cloud services overview
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud 服务概述
- en: Google Cloud tools for data processing
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud 数据处理工具
- en: Google Cloud VertexAI
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud VertexAI
- en: Standard industry tools on Google Cloud
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google Cloud 上的标准行业工具
- en: Choosing the right tool for the job
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择合适的工具
- en: Let’s begin with a discussion of why we would want to use Google Cloud for data
    science and AI/ML use cases in the first place.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论为什么我们最初想要使用 Google Cloud 进行数据科学和 AI/ML 应用场景开始。
- en: Why Google Cloud?
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为什么选择 Google Cloud？
- en: Google has been a well-known leader in the AI/ML space for a very long time.
    They have contributed a lot to the AI/ML industry, through countless research
    papers, publications, and donations of AI/ML libraries to the open source community,
    such as TensorFlow, one of the most widely used ML libraries of all time. Their
    search and advertising algorithms have been leading their respective industries
    for years, and their peer organizations, such as DeepMind, dedicate their entire
    existence to pure AI/ML research.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Google 在 AI/ML 领域已经是一个长期知名领导者。他们通过无数的研究论文、出版物以及向开源社区捐赠 AI/ML 库（如 TensorFlow，有史以来最广泛使用的
    ML 库之一）等方式，为 AI/ML 行业做出了大量贡献。他们的搜索和广告算法多年来一直引领着各自的行业，他们的同行组织，如 DeepMind，将他们的全部存在都致力于纯
    AI/ML 研究。
- en: Google has also been spearheading initiatives such as **Ethical AI**, championing
    the concepts of fairness and explainability to ensure that AI is held accountable
    and used only for purposes that are beneficial to humans. AI/ML is not something
    that Google is trying to use, but rather it is a core tenet of Google’s business.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Google 还一直在引领诸如**道德 AI**等倡议，倡导公平性和可解释性的概念，以确保 AI 负有责任，并且仅用于对人类有益的目的。AI/ML 不是
    Google 试图利用的东西，而是 Google 商业的核心原则。
- en: A significant testament to Google Cloud’s leadership in this space was when
    Gartner officially recognized them as a leader in the 2022 Gartner® Magic Quadrant™
    for Cloud AI Developer Services. Google Cloud provides a wide array of services
    and tools for implementing AI/ML use cases and embraces open source and third-party
    solutions in order to provide the broadest selection possible to their customers.
    By using Google Cloud for AI/ML workloads, you can benefit from the decades of
    AI/ML research performed, and expertise gained, by Google in this space.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 在这个领域的领导地位的一个显著证明是，Gartner 正式将其认定为 2022 年 Gartner® 魔力四边形™ 云 AI
    开发服务领域的领导者。Google Cloud 提供了一系列服务和工具，用于实施 AI/ML 用例，并拥抱开源和第三方解决方案，以便为顾客提供尽可能广泛的选择。通过使用
    Google Cloud 进行 AI/ML 工作负载，您可以受益于 Google 在这个领域数十年的 AI/ML 研究和积累的专业知识。
- en: Prerequisites for using Google Cloud tools and services
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Google Cloud 工具和服务的先决条件
- en: This section is going to be pretty simple because Google Cloud makes it very
    easy to get started in trying out its services. If you have a Gmail account, then
    you pretty much already have everything you need to get started on Google Cloud.
    As a generous bonus, Google Cloud gives USD $300 in credits to new customers,
    plus additional free credits to new customers who verify their business email
    addresses. You can use those free credits to explore and evaluate Google Cloud’s
    various services. Also, many of Google Cloud’s services provide a **Free Tier**,
    which allows you to use those services free of charge up to their specified free
    usage limit, the details of which you can find in the Google Cloud documentation
    ([https://cloud.google.com/free/docs/free-cloud-features](https://cloud.google.com/free/docs/free-cloud-features)).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节将会相当简单，因为 Google Cloud 让尝试其服务变得非常容易。如果您有 Gmail 账户，那么您基本上已经拥有了在 Google Cloud
    上开始所需的一切。作为一项慷慨的额外奖励，Google Cloud 向新客户提供 300 美元的信用额度，以及验证其商业电子邮件地址的新客户将获得额外的免费信用额度。您可以使用这些免费信用额度来探索和评估
    Google Cloud 的各种服务。此外，Google Cloud 的许多服务提供 **免费层**，允许您免费使用这些服务，直到达到指定的免费使用限制，具体详情您可以在
    Google Cloud 文档中找到（[https://cloud.google.com/free/docs/free-cloud-features](https://cloud.google.com/free/docs/free-cloud-features)）。
- en: If you need to create a new Google Cloud account, you can sign up at [https://console.cloud.google.com/freetrial](https://console.cloud.google.com/freetrial),
    and when you need to go beyond free usage, you can upgrade to a paid Cloud Billing
    account.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要创建新的 Google Cloud 账户，您可以在 [https://console.cloud.google.com/freetrial](https://console.cloud.google.com/freetrial)
    注册，当您需要超出免费使用范围时，您可以升级到付费的 Cloud Billing 账户。
- en: After you’ve created and logged into your account, you can start using the Google
    Cloud services that we’ll be using in this book, and many more. When you first
    try to use a Google Cloud service, you may need to enable the API for that service.
    This is a simple, one-click action that you only need to perform once in each
    Google Cloud project. *Figure 3**.2* shows an example of the page that’s displayed
    when you first try to use the Google Filestore service (we will describe Filestore
    in more detail later in this chapter). You can simply click the **Enable** button
    to enable the API.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在您创建并登录账户后，您就可以开始使用本书中将要使用的 Google Cloud 服务以及更多服务。当您第一次尝试使用 Google Cloud 服务时，您可能需要启用该服务的
    API。这是一个简单的单次点击操作，您只需在每个 Google Cloud 项目中执行一次。*图 3.2* 展示了您第一次尝试使用 Google Filestore
    服务时显示的页面（我们将在本章后面更详细地介绍 Filestore）。您只需点击 **启用** 按钮即可启用 API。
- en: Definition
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: A Google Cloud project organizes all your Google Cloud resources. It consists
    of a set of users; a set of APIs; and billing, authentication, and monitoring
    settings for those APIs. All Google Cloud resources, along with user permissions
    for accessing them, reside in a project.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 项目组织了您所有的 Google Cloud 资源。它包括一组用户；一组 API；以及这些 API 的计费、身份验证和监控设置。所有
    Google Cloud 资源，以及访问它们的用户权限，都位于一个项目中。
- en: '![Figure 3.2: Enabling a Google Cloud API on first use](img/B18143_03_2.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2：首次使用时启用 Google Cloud API](img/B18143_03_2.jpg)'
- en: 'Figure 3.2: Enabling a Google Cloud API on first use'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2：首次使用时启用 Google Cloud API
- en: Security, privacy, and compliance
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 安全、隐私和合规性
- en: The first thing we need to think about when we’ve created our Google Cloud account
    is security. This also extends to privacy and compliance, and these are hot topics
    in the data analytics and AI/ML industries today because your customers want to
    know that their data is being handled securely. Fortunately, these topics are
    major priorities for Google Cloud, and as a result, Google Cloud provides a plethora
    of default controls and dedicated services to facilitate and uphold these priorities.
    We will introduce some of the important concepts and related services briefly
    here, and we will dive deeper into these topics in later chapters of this book.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们创建了我们的谷歌云账户后，首先需要考虑的是安全性。这同样也涉及到隐私和合规性，这些是目前数据分析和人工智能/机器学习行业的热门话题，因为您的客户希望知道他们的数据正在被安全地处理。幸运的是，这些话题是谷歌云的主要优先事项，因此，谷歌云提供了一系列默认控制和专用服务来促进和维持这些优先事项。我们将在本文简要介绍一些重要概念和相关服务，并在本书的后续章节中深入探讨这些话题。
- en: Who has access to what?
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 谁有权访问什么？
- en: The first topic to discuss in the context of security, privacy, and compliance
    is identity and access management; that is, identifying and controlling who has
    access to which resources in your Google Cloud environments. Google Cloud provides
    the **Identity and Access Management** (**IAM**) service for this purpose. This
    service enables you to define identities such as users and groups of users, and
    permissions with regard to accessing Google Cloud resources. For every attempted
    action on Google Cloud, whether it’s to read an object from storage or to run
    a piece of code as a cloud function, the permissions associated with that action,
    the resource upon which the action would act, and the invoking identity would
    be evaluated by Google Cloud IAM, and the action would only be permitted if the
    correct combination of permissions has been applied to all relevant identities
    and resources. For additional convenience, you can integrate Google Cloud IAM
    with external **Identity Providers** (**IdPs**) and directories, such as Active
    Directory.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在安全、隐私和合规性的背景下，首先需要讨论的主题是身份和访问管理；也就是说，识别和控制谁有权访问您的谷歌云环境中的哪些资源。谷歌云为此提供了**身份和访问管理**（**IAM**）服务。此服务使您能够定义用户和用户组等身份，以及关于访问谷歌云资源的权限。对于在谷歌云上进行的每一次尝试操作，无论是从存储中读取对象还是作为云函数运行一段代码，谷歌云IAM都会评估与此操作相关的权限、操作将作用其上的资源以及调用身份，并且只有当所有相关身份和资源都应用了正确的权限组合时，操作才会被允许。为了提供额外的便利，您可以将谷歌云IAM与外部**身份提供者**（**IdPs**）和目录，如Active
    Directory集成。
- en: Data security
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据安全
- en: Google Cloud encrypts all data at rest by default. You can control the keys
    that are used to encrypt your data by using **Customer-Managed Encryption Keys**
    (**CMEKs**), or you can let Google Cloud manage all that functionality for you.
    Regarding data in transit, Google has built global networks with stringent security
    controls, and TLS encryption is used to protect data that is being transported
    throughout these global networks. Google Cloud also enables you to encrypt your
    data even when the data is actively in use, through Confidential Computing, which
    uses a hardware-based **Trusted Execution Environment** (**TEE**). TEEs are secure
    and isolated environments that prevent unauthorized access or modification of
    applications and data while they are in use.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌云默认对所有静态数据进行加密。您可以通过使用**客户管理加密密钥**（**CMEKs**）来控制用于加密数据的密钥，或者您可以让谷歌云为您管理所有这些功能。至于传输中的数据，谷歌已经建立了具有严格安全控制的全球网络，并使用TLS加密来保护在这些全球网络中传输的数据。谷歌云还通过使用基于硬件的**可信执行环境**（**TEE**）的保密计算，使您能够在数据积极使用时对其进行加密。TEE是安全且隔离的环境，在数据使用期间防止未经授权的访问或修改应用程序和数据。
- en: Infrastructure security
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基础设施安全
- en: In addition to Google Cloud’s state-of-the-art infrastructure security controls,
    Google Cloud provides tools to help prevent and detect potential security threats
    and vulnerabilities. For example, you can use Cloud Firewall and Cloud Armor to
    prevent **Distributed Denial-of-Service** (**DDoS**) and common OWASP threats.
    You can use Chronicle, Security Command Center, and Mandiant, for **Security Incident
    and Event Monitoring** (**SIEM**), **Security Orchestration Automation and Response**
    (**SOAR**), intrusion detection, and threat intelligence. In addition to all of
    these Google Cloud services, you can use third-party observability and reporting
    services such as Splunk on Google Cloud.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 Google Cloud 先进的云基础设施安全控制之外，Google Cloud 还提供工具来帮助防止和检测潜在的安全威胁和漏洞。例如，您可以使用
    Cloud Firewall 和 Cloud Armor 来防止 **分布式拒绝服务**（**DDoS**）和常见的 OWASP 威胁。您可以使用 Chronicle、Security
    Command Center 和 Mandiant 进行 **安全事件和事故监控**（**SIEM**）、**安全编排自动化和响应**（**SOAR**）、入侵检测和威胁情报。除了所有这些
    Google Cloud 服务之外，您还可以使用 Google Cloud 上的第三方可观察性和报告服务，如 Splunk。
- en: Compliance
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 合规性
- en: Google Cloud provides audit data that tracks the actions being performed by
    identities on resources in your environments, which is important for compliance
    reasons. Google Cloud participates in formal compliance programs such as FedRamp,
    SOC2, and SOC3, and supports compliance standards such as **Payment Card Industry
    Data Security Standard** (**PCI DSS**), and multiple ISO/IEC international standards.
    You can see additional details regarding Google Cloud’s compliance program participation
    at [https://cloud.google.com/security/compliance](https://cloud.google.com/security/compliance).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 提供审计数据，跟踪在您的环境中对资源执行的操作，这对于合规性原因非常重要。Google Cloud 参与正式的合规性计划，如
    FedRamp、SOC2 和 SOC3，并支持如 **支付卡行业数据安全标准**（**PCI DSS**）和多个 ISO/IEC 国际标准等合规性标准。您可以在
    [https://cloud.google.com/security/compliance](https://cloud.google.com/security/compliance)
    查看有关 Google Cloud 合规性计划参与的更多详细信息。
- en: Interacting with Google Cloud services
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 与 Google Cloud 服务交互
- en: There are numerous ways in which you can interact with Google Cloud services.
    At a high level, you can either use the **Graphical User Interface** (**GUI**),
    the **Command-Line Interface** (**CLI**), or the API. We explore each of these
    options in more detail in this section.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过多种方式与 Google Cloud 服务进行交互。从高层次来看，您可以使用 **图形用户界面**（**GUI**）、**命令行界面**（**CLI**）或
    API。我们将在本节中更详细地探讨这些选项。
- en: Console
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 控制台
- en: One of the most straightforward ways to interact with Google Cloud services
    is via the Google Cloud console, which provides a GUI. You can access the console
    at [https://console.cloud.google.com/](https://console.cloud.google.com/).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Google Cloud 服务交互的最直接方式之一是通过 Google Cloud 控制台，它提供了一个 GUI。您可以通过 [https://console.cloud.google.com/](https://console.cloud.google.com/)
    访问控制台。
- en: The console enables you to perform actions in Google Cloud by clicking around
    in a web-based interface in your browser. For example, you create a **virtual
    machine** (**VM**) by clicking **Compute Engine** in the products menu, and then
    going to the VM instances page and clicking **Create an instance** to specify
    the desired properties of your VM, as shown in *Figure 3**.3*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 控制台允许您通过在浏览器中的基于网页的界面中点击来执行 Google Cloud 的操作。例如，您可以通过在产品菜单中点击 **Compute Engine**
    来创建 **虚拟机**（**VM**），然后转到虚拟机实例页面并点击 **创建实例**来指定 VM 的所需属性，如图 *图 3**.3* 所示。
- en: '![Figure 3.3: Creating a VM in the Google Cloud console](img/B18143_03_3.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.3：在 Google Cloud 控制台中创建虚拟机](img/B18143_03_3.jpg)'
- en: 'Figure 3.3: Creating a VM in the Google Cloud console'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.3：在 Google Cloud 控制台中创建虚拟机
- en: The gcloud CLI
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gcloud CLI
- en: If you prefer to use a CLI, Google Cloud has built a tool named **gcloud**,
    which enables you to interact with Google Cloud services by executing text-based
    commands. This is particularly useful if you wish to automate sequences of Google
    Cloud service API actions by composing scripts that run multiple commands in order.
    For example, you could create a Bash script that contains multiple commands, and
    you could execute that script either manually or on a periodic schedule if it
    contains actions that frequently need to be repeated. This approach would be suitable
    for ad hoc automation that can be implemented with little effort. There are other
    ways to automate more complex sequences of actions on Google Cloud, which we will
    explore in later chapters.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢使用 CLI，Google Cloud 已经构建了一个名为 **gcloud** 的工具，它允许你通过执行基于文本的命令与 Google Cloud
    服务交互。这在你希望通过编写包含多个命令的脚本来自动化一系列 Google Cloud 服务 API 操作时特别有用。例如，你可以创建一个包含多个命令的 Bash
    脚本，并且你可以手动执行该脚本，或者如果它包含需要频繁重复执行的操作，你可以按周期性计划执行该脚本。这种方法适用于只需少量努力即可实现的临时自动化。在后面的章节中，我们将探讨在
    Google Cloud 上自动化更复杂操作序列的其他方法。
- en: 'The following is an example of a `gcloud` command. This command will enable
    the API for `SERVICE_NAME`, which is a placeholder for the name of the Google
    Cloud service with which we want to interact:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 `gcloud` 命令的示例。此命令将启用 `SERVICE_NAME` 的 API，其中 `SERVICE_NAME` 是我们想要交互的
    Google Cloud 服务的名称占位符：
- en: '[PRE0]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For example, to enable the Filestore API, rather than clicking the `gcloud`
    CLI command:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，要启用 Filestore API，而不是点击 `gcloud` CLI 命令：
- en: '[PRE1]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In this case, `file` is the command-line name for the Google Cloud Filestore
    service (the full service name is `file.googleapis.com`).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，`file` 是 Google Cloud Filestore 服务的命令行名称（完整服务名称为 `file.googleapis.com`）。
- en: To use the gcloud CLI, you can install it on any machine on which you wish to
    run it, as it supports many different operating systems, such as Linux, macOS,
    and Windows, or you can use Google Cloud Shell, described next.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 gcloud CLI，你可以在任何你希望运行它的机器上安装它，因为它支持许多不同的操作系统，如 Linux、macOS 和 Windows，或者你可以使用下一节中描述的
    Google Cloud Shell。
- en: Google Cloud Shell
  id: totrans-52
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Google Cloud Shell
- en: Google Cloud Shell is a very convenient way to use the gcloud CLI and interact
    with Google Cloud APIs. It’s a tool that provides a Linux-based environment in
    which you can issue commands to Google Cloud service APIs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Shell 是使用 gcloud CLI 和与 Google Cloud APIs 交互的一种非常方便的方式。这是一个提供基于
    Linux 环境的工具，你可以在此环境中向 Google Cloud 服务 API 发送命令。
- en: 'You can open the Cloud Shell by clicking on the ![](img/icon.png) symbol in
    the top-right corner of the Google Cloud console screen, as shown in *Figure 3**.4*:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过点击 Google Cloud 控制台屏幕右上角的 ![](img/icon.png) 图标来打开 Cloud Shell，如图 3.4 所示：
- en: '![Figure 3.4: Activating Google Cloud Shell](img/B18143_03_4.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.4：激活 Google Cloud Shell](img/B18143_03_4.jpg)'
- en: 'Figure 3.4: Activating Google Cloud Shell'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.4：激活 Google Cloud Shell
- en: 'The terminal will then appear at the bottom of the screen, as shown in *Figure
    3**.5*:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，终端将出现在屏幕底部，如图 3.5 所示：
- en: '![Figure 3.5: Google Cloud Shell](img/B18143_03_5.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.5：Google Cloud Shell](img/B18143_03_5.jpg)'
- en: 'Figure 3.5: Google Cloud Shell'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.5：Google Cloud Shell
- en: When you first try to use Cloud Shell, you need to authorize it to interact
    with Google Cloud service APIs, as depicted in *Figure 3**.6*.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 当你第一次尝试使用 Cloud Shell 时，你需要授权它与 Google Cloud 服务 API 交互，如图 3.6 所示。
- en: '![Figure 3.6: Authorizing Google Cloud Shell](img/B18143_03_6.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.6：授权 Google Cloud Shell](img/B18143_03_6.jpg)'
- en: 'Figure 3.6: Authorizing Google Cloud Shell'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.6：授权 Google Cloud Shell
- en: API access
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: API 访问
- en: 'The most low-level method for interacting with Google Cloud services is by
    programmatically invoking their APIs directly. This method differs from the GUI
    and CLI access because it is not intended for direct human interaction, but rather
    it is suitable for more advanced use cases, such as interacting with Google Cloud
    services via your application software. As an example, let’s consider an application
    that saves users’ photos in the cloud. When new users sign up, we may wish to
    create a new Google Cloud Storage bucket to store their photos, among other signup-related
    activities (we will describe the Google Cloud Storage service later in this chapter).
    We could create the following REST API request for that purpose:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Google Cloud 服务交互的最底层方法是程序化直接调用它们的 API。这种方法与 GUI 和 CLI 访问不同，因为它不是旨在直接与人类交互，而是适合更高级的使用案例，例如通过您的应用程序软件与
    Google Cloud 服务交互。作为一个例子，让我们考虑一个将用户照片保存在云中的应用程序。当新用户注册时，我们可能希望创建一个新的 Google Cloud
    Storage 存储桶来存储他们的照片，以及其他与注册相关的活动（我们将在本章后面描述 Google Cloud Storage 服务）。为了这个目的，我们可以创建以下
    REST API 请求：
- en: '[PRE2]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s break this down:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解一下：
- en: '`JSON_FILE_NAME` is the name of the required JSON file that specifies the bucket
    details'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`JSON_FILE_NAME` 是指定存储桶详细信息的所需 JSON 文件的名称'
- en: '`OAUTH2_TOKEN` is an access token that is required to invoke the API'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OAUTH2_TOKEN` 是调用 API 所需的访问令牌'
- en: '`PROJECT_IDENTIFIER` is the ID or number of the project with which our bucket
    will be associated, for example, `my-project`'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PROJECT_IDENTIFIER` 是与我们的存储桶相关联的项目 ID 或编号，例如，`my-project`'
- en: 'The required JSON file is structured as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的 JSON 文件结构如下：
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here’s the breakdown of this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的分解如下：
- en: '`BUCKET_NAME` is the name we want to give our bucket.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BUCKET_NAME` 是我们想要给我们的存储桶的名称。'
- en: '`BUCKET_LOCATION` is the location where you want to store your bucket object
    data. For more information regarding Google Cloud locations, refer to the Google
    Cloud documentation here: [https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BUCKET_LOCATION` 是您想要存储您的存储桶对象数据的位置。有关 Google Cloud 位置的更多信息，请参阅以下 Google Cloud
    文档：[https://cloud.google.com/compute/docs/regions-zones](https://cloud.google.com/compute/docs/regions-zones)。'
- en: '`STORAGE_CLASS` is the default storage class of your bucket. For more information
    regarding Google Cloud Storage classes, refer to the Google Cloud documentation
    here: [https://cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STORAGE_CLASS` 是您存储桶的默认存储类别。有关 Google Cloud 存储类别的更多信息，请参阅以下 Google Cloud 文档：[https://cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes)。'
- en: 'In practice, it’s most common to use Google Cloud’s client **Software Development
    Kits** (**SDK**s) to create such API calls programmatically. For example, in the
    following Python code, we import the Google Cloud Storage client library, and
    we then define a function to create a new bucket, specifying the bucket name,
    the location, and the storage class:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，最常见的是使用 Google Cloud 的客户端 **软件开发工具包**（**SDK**s）来程序化地创建这样的 API 调用。例如，在下面的
    Python 代码中，我们导入 Google Cloud Storage 客户端库，然后定义一个函数来创建一个新的存储桶，指定存储桶名称、位置和存储类别：
- en: '[PRE4]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now that we’ve covered some of the basics of how to interact with Google Cloud
    services, let’s discuss the types of Google Cloud services we will use in this
    book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了一些如何与 Google Cloud 服务交互的基础知识，让我们讨论一下本书中将使用的 Google Cloud 服务类型。
- en: Google Cloud services overview
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud 服务概述
- en: Having covered the basics of how to set up a Google Cloud account and how to
    enable and interact with the various services, we will now introduce the services
    that we are going to use in this book to create AI/ML workloads. We will first
    cover the fundamental cloud services upon which almost all workloads are built,
    and then we will cover the more advanced services related to data science and
    AI/ML.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍了如何设置 Google Cloud 账户以及如何启用和交互各种服务的基础知识之后，我们现在将介绍本书中将要使用的服务，以创建 AI/ML 工作负载。我们将首先介绍几乎所有工作负载都基于的基本云服务，然后我们将介绍与数据科学和
    AI/ML 相关的更高级服务。
- en: Google Cloud computing services
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud 计算服务
- en: Considering that the word *computing* is included directly in the term **cloud
    computing**, and the fact that computing services form the basis of all other
    cloud services, we will start this section with a brief overview of Google Cloud’s
    computing services.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到单词 *computing* 直接包含在术语 **云计算** 中，以及计算服务构成了所有其他云服务的基础，我们将从这个部分开始，简要概述 Google
    Cloud 的计算服务。
- en: Google Compute Engine (GCE)
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google计算引擎（GCE）
- en: A few years ago, the term cloud computing was pretty much synonymous with the
    term **virtualization**. Traditionally, companies had physical servers on their
    own premises, and this was then contrasted with creating virtual servers in the
    cloud, either public or private. Hence, perhaps the easiest concept to understand
    in cloud computing is virtualization, where we simply create a virtual server
    instead of a physical server by introducing an abstraction layer called a hypervisor
    between the hardware and our server’s operating system, as depicted in *Figure
    3**.7*. For most companies, if they’re already running physical servers, then
    their first foray into the world of the cloud is usually implemented by using
    VMs because this is the simplest step to transition from the physical paradigm
    to the cloud paradigm. **Google Compute Engine** (**GCE**) is Google Cloud’s service
    for running VMs in the cloud. It provides some useful features, such as auto-scaling
    based on demand, which is one of the well-established benefits of cloud computing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 几年前，云计算这个术语几乎与虚拟化这个术语同义。传统上，公司在自己的场所拥有物理服务器，然后与之形成对比的是在云中创建虚拟服务器，无论是公共的还是私有的。因此，在云计算中，最容易理解的概念可能是虚拟化，我们通过在硬件和我们的服务器操作系统之间引入一个称为虚拟机的抽象层，简单地创建一个虚拟服务器而不是物理服务器，如图*3**.7*所示。对于大多数公司来说，如果他们已经在运行物理服务器，那么他们进入云计算世界的第一步通常是通过使用虚拟机来实现的，因为这是从物理范式过渡到云范式最简单的一步。**Google计算引擎**（**GCE**）是Google云在云中运行虚拟机的服务。它提供了一些有用的功能，例如基于需求的自动扩展，这是云计算已确立的众多好处之一。
- en: '![Figure 3.7: Example VM implementation](img/B18143_03_7.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图3.7：示例虚拟机实现](img/B18143_03_7.jpg)'
- en: 'Figure 3.7: Example VM implementation'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7：示例虚拟机实现
- en: Google Kubernetes Engine (GKE)
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Kubernetes引擎（GKE）
- en: A newer type of virtualization was created in the 2000s by using Linux **cgroups**
    and **Namespaces** to isolate computing resources such as CPUs, RAM, and storage
    resources, for specific processes within a running operating system. With containerization,
    the abstraction layer moves higher in the stack, whereby it exists between the
    operating system and our applications, as depicted in *Figure 3**.8*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在2000年代，通过使用Linux **cgroups** 和 **Namespaces** 来隔离运行操作系统中特定进程的计算资源，如CPU、RAM和存储资源，创建了一种新型的虚拟化技术。随着容器化的出现，抽象层在堆栈中的位置上升，存在于操作系统和我们的应用程序之间，如图*3**.8*所示。
- en: '![Figure 3.8: Example container implementation](img/B18143_03_8.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图3.8：示例容器实现](img/B18143_03_8.jpg)'
- en: 'Figure 3.8: Example container implementation'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8：示例容器实现
- en: This gives us some interesting benefits beyond those afforded by hypervisor-based
    virtualization. For example, containers are generally much smaller and more *lightweight*
    than VMs, meaning they contain much fewer software components. While a VM has
    to boot up an entire operating system and lots of software applications before
    it becomes usable, which can take a few minutes, a container usually only contains
    your application code and any required dependencies and can therefore be loaded
    in seconds. This makes a big difference when it comes to auto-scaling and auto-healing
    cloud-based software workloads. Starting up new VMs in relation to a sudden increase
    in traffic may not happen quickly enough and you may lose some requests while
    the VMs boot up and load your application. The same applies to restarting a VM
    due to some kind of problem. In both of those cases, a container would usually
    start much more quickly. Containing fewer components also means that containers
    can be deployed much more quickly, and this makes them a perfect environment for
    microservices with DevOps CI/CD pipelines. There are also many other benefits
    of containers, such as portability and easy manageability.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们带来了比基于虚拟机的虚拟化所提供的更多有趣的好处。例如，容器通常比虚拟机小得多，更**轻量级**，这意味着它们包含的软件组件要少得多。虚拟机必须在启动整个操作系统和大量软件应用程序之后才能变得可用，这可能需要几分钟的时间，而容器通常只包含你的应用程序代码和任何所需的依赖项，因此可以在几秒钟内加载。这在自动扩展和自动修复基于云的软件工作负载时产生了重大差异。与突然增加的流量相关的新虚拟机的启动可能不够快，你可能会在虚拟机启动并加载你的应用程序时丢失一些请求。同样，由于某种问题重新启动虚拟机也是如此。在这两种情况下，容器通常会启动得更快。包含更少的组件也意味着容器可以部署得更快，这使得它们成为具有DevOps
    CI/CD管道的微服务的完美环境。容器还有许多其他好处，例如可移植性和易于管理。
- en: However, one of the challenges introduced by containerization also stems from
    their lighter footprint. Because they are generally smaller than VMs, it’s common
    to have more of them in a single application deployment. Managing lots of tiny
    containers can be challenging, especially in terms of application lifecycle management
    and orchestration; that is, determining how and where to run your workloads, and
    assigning adequate computing resources to them. This is where Kubernetes comes
    into the picture. The following are the official definitions for Kubernetes, in
    general, and **Google Kubernetes Engine** (**GKE**), in particular.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，容器化带来的挑战之一也源于它们的轻量级特性。因为它们通常比虚拟机小，所以在单个应用程序部署中拥有更多的容器是很常见的。管理大量的微型容器可能具有挑战性，尤其是在应用程序生命周期管理和编排方面；也就是说，确定如何以及在哪里运行您的负载，并分配足够的计算资源给它们。这正是Kubernetes发挥作用的地方。以下是对Kubernetes以及特别针对**Google
    Kubernetes Engine**（**GKE**）的官方定义。
- en: Kubernetes, also known as **K8s**, is an open source system for automating the
    deployment, scaling, and management of containerized applications. It groups containers
    that make up an application into logical units for easy management and discovery.
    GKE provides a managed environment for deploying, managing, and scaling your containerized
    applications using Google infrastructure.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes，也称为**K8s**，是一个开源系统，用于自动化容器化应用程序的部署、扩展和管理。它将构成应用程序的容器组合成逻辑单元，以便于管理和发现。GKE提供了一个使用Google基础设施部署、管理和扩展容器化应用程序的托管环境。
- en: '*Figure 3**.9* shows an example of how Kubernetes organizes and orchestrates
    applications. It deploys your applications as Pods, which are groups of containers
    with similar functionality, and it deploys agents on your hardware servers or
    the host operating systems that keep track of resource utilization and communicate
    that information back to the Kubernetes master, which uses that information to
    manage Pod deployments.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*图3.9*展示了Kubernetes如何组织和编排应用程序的示例。它将您的应用程序部署为Pod，Pod是由具有相似功能的容器组成的组，并在您的硬件服务器或宿主操作系统上部署代理，以跟踪资源利用率并将该信息反馈给Kubernetes主节点，Kubernetes主节点使用这些信息来管理Pod部署。'
- en: '![Figure 3.9: Example GKE implementation  (source: https://kubernetes.io/docs/concepts/architecture/)](img/B18143_03_9.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图3.9：示例GKE实现（来源：https://kubernetes.io/docs/concepts/architecture/）](img/B18143_03_9.jpg)'
- en: 'Figure 3.9: Example GKE implementation (source: https://kubernetes.io/docs/concepts/architecture/)'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9：示例GKE实现（来源：https://kubernetes.io/docs/concepts/architecture/)
- en: Google Cloud serverless computing
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud无服务器计算
- en: The term **serverless** in the context of cloud computing refers to the concept
    of running your code on a cloud provider’s infrastructure without needing to manage
    any of the servers that will be used to run your code. In reality, there are still
    servers that are being used, behind the scenes, but the cloud provider creates
    and manages them on your behalf so that you don’t need to perform those actions.
    Google Cloud has two primary services that relate to serverless computing, which
    are named **Cloud Functions**, and **Cloud Run**. Another Google Cloud service,
    named **App Engine**, is also often bundled under the serverless umbrella, and
    we will describe that service, and how it differs from Cloud Functions and Cloud
    Run, later in this section.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在云计算的上下文中，**无服务器**一词指的是在云提供商的基础设施上运行您的代码，而不需要管理任何用于运行您的代码的服务器。实际上，仍然有服务器在幕后被使用，但云提供商代表您创建和管理这些服务器，这样您就不需要执行这些操作。Google
    Cloud有两个与无服务器计算相关的核心服务，分别命名为**云函数**和**云运行**。另一个名为**App Engine**的Google Cloud服务也经常被归类在无服务器范畴之下，我们将在本节稍后描述该服务，以及它与云函数和云运行的区别。
- en: Note
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Many other Google Cloud services also run in a serverless fashion, whereby the
    actions they perform on your behalf run on servers that are managed in the background,
    without any need for you to manage those servers. However, Cloud Functions and
    Cloud Run are the two Google Cloud services that relate to *serverless computing*,
    which specifically refers to running your code without the need to explicitly
    manage servers.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 许多其他Google Cloud服务也以无服务器的方式运行，即它们代表您执行的操作在后台管理的服务器上运行，您无需管理这些服务器。然而，云函数和云运行是两个与**无服务器计算**相关的Google
    Cloud服务，这特别指的是运行您的代码而不需要明确管理服务器。
- en: Cloud Functions
  id: totrans-101
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 云函数
- en: With Cloud Functions, you simply write small pieces of code — for example, a
    single function — and Google Cloud will run that code for you in response to events
    that you specify as triggers to run that code. You don’t need to manage any containers,
    servers, or infrastructure on which your code executes, and there are many types
    of triggers that you can configure. For example, whenever a file is uploaded to
    your Google Cloud Storage bucket, that could trigger your piece of code to execute.
    Your code could then process that file in some way, feed it into another Google
    Cloud service to be processed, or simply send a notification to inform somebody
    that the file has been uploaded.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用云函数，您只需编写小段代码——例如，一个单独的函数——谷歌云将根据您指定的触发器运行该代码。您不需要管理任何容器、服务器或您的代码执行的任何基础设施，并且您可以配置许多类型的触发器。例如，每当文件上传到您的谷歌云存储桶时，这可能会触发您的代码片段执行。然后，您的代码可以以某种方式处理该文件，将其输入到另一个谷歌云服务进行处理，或者简单地发送通知告知某人文件已上传。
- en: This concept is referred to as **Functions as a Service** (**FaaS**) because
    it is usually used to execute a single function for each event trigger. This approach
    is suitable for when you want to simply write and run small code snippets that
    respond to events that occur in your environment. You can also use Cloud Functions
    to connect with other Google Cloud or third-party cloud services to streamline
    challenging orchestration problems.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念被称为**函数即服务**（**FaaS**），因为它通常用于为每个事件触发执行单个函数。这种方法适用于您只想简单地编写和运行响应您环境中发生的事件的小代码片段时。您还可以使用云函数连接到其他谷歌云或第三方云服务，以简化复杂的编排问题。
- en: In addition to sparing you the trouble of managing servers, another advantage
    of using Cloud Functions is that you don’t have to pay for servers when no events
    are happening in your environment.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 除了让您免于管理服务器的麻烦之外，使用云函数的另一个优点是，当您的环境中没有事件发生时，您不需要为服务器付费。
- en: Cloud Run
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Cloud Run
- en: Cloud Run is a different type of serverless computing service that is more suitable
    for long-running application processes. While cloud functions are intended to
    run small pieces of code in response to specific events that occur, Cloud Run
    can run more complex applications. This also means that it provides more flexibility
    and control with regard to how your code executes. For example, it runs your code
    in containers, and you have more control over what executes in those containers.
    If your application requires custom software package dependencies, for example,
    you can provision those dependencies to be available in your containers.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Run是一种更适合长时间运行的应用程序进程的无服务器计算服务。虽然云函数旨在响应特定事件运行小段代码，但Cloud Run可以运行更复杂的应用程序。这也意味着它提供了更多关于代码执行方面的灵活性和控制。例如，它将在容器中运行您的代码，并且您对这些容器中执行的内容有更多的控制。如果您的应用程序需要自定义软件包依赖项，例如，您可以将这些依赖项配置为在容器中可用。
- en: Cloud Run abstracts away all infrastructure management by automatically scaling
    up and down from zero almost instantaneously, depending on traffic, and it only
    charges you for the exact resources you use.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Cloud Run通过自动从零几乎瞬间扩展和缩减，根据流量进行管理，并且只对您使用的确切资源收费，从而抽象化了所有基础设施管理。
- en: App Engine
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: App Engine
- en: While App Engine can also be considered as a serverless service, because it
    manages the underlying infrastructure for you, its use cases differ from those
    of Cloud Functions and Cloud Run. App Engine comes in two levels of service, referred
    to as **Standard** and **Flexible**. In the standard environment, your application
    runs on a lightweight server inside a sandbox. This sandbox restricts what your
    application can do. For example, the sandbox only allows your app to use a limited
    set of software binary libraries, and your app cannot write to a permanent disk.
    The standard environment also limits the CPU and memory options available to your
    application. Because of these restrictions, most App Engine standard applications
    tend to be stateless web applications that respond to HTTP requests quickly. In
    contrast, the flexible environment runs your application in Docker containers
    on Google Compute Engine VMs, which have fewer restrictions.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 App Engine 也可以被视为一种无服务器服务，因为它为您管理底层基础设施，但其用例与 Cloud Functions 和 Cloud Run
    不同。App Engine 有两个服务级别，称为 **标准** 和 **灵活**。在标准环境中，您的应用程序在一个沙盒内的轻量级服务器上运行。这个沙盒限制了您的应用程序可以执行的操作。例如，沙盒只允许您的应用程序使用有限的一组软件二进制库，并且您的应用程序不能写入永久磁盘。标准环境还限制了应用程序可用的
    CPU 和内存选项。由于这些限制，大多数 App Engine 标准应用程序通常是无状态的 Web 应用程序，能够快速响应 HTTP 请求。相比之下，灵活环境在
    Google Compute Engine 虚拟机上运行您的应用程序，这些虚拟机具有较少的限制。
- en: Also, note that the standard environment can scale from zero instances up to
    thousands very quickly, but the flexible environment must have at least one instance
    running and can take longer to scale up in response to sudden traffic increases.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，标准环境可以从零实例快速扩展到数千个实例，但灵活环境必须至少运行一个实例，并且可能需要更长的时间来响应突增的流量。
- en: App Engine is generally suited to large web applications. Its flexible environment
    can be more customizable than Cloud Run. However, if you want to deploy a long-running
    web application without managing the underlying infrastructure, I recommend first
    evaluating whether Cloud Run could meet your application’s needs and comparing
    the costs of running your app on Cloud Run versus App Engine.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: App Engine 通常适用于大型网络应用程序。其灵活的环境可以比 Cloud Run 更具可定制性。然而，如果您想部署一个无需管理底层基础设施的长期运行的网络应用程序，我建议首先评估
    Cloud Run 是否能满足您的应用程序需求，并比较在 Cloud Run 上运行应用程序与在 App Engine 上运行的成本。
- en: Google Cloud Batch
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud 批处理
- en: Some workflows are intended to run for a long time without the need for human
    interaction. Examples of such workloads include media transcoding, computational
    fluid dynamics, Monte Carlo simulations, genomics processing, and drug discovery,
    among others. These kinds of workloads usually require large amounts of computing
    power and can be optimized by running tasks in parallel. Creating and running
    these jobs by yourself can incur overheads such as managing servers, queueing
    mechanisms, parallelization, and failure logic. Fortunately, the Google Cloud
    Batch service has been built to manage all these kinds of activities for you.
    As a fully managed job scheduler, it automatically scales the infrastructure required
    to run your batch jobs up or down and handles parallelization and retry logic
    that you can configure in case any errors occur during execution.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 一些工作流程旨在长时间运行，无需人工交互。这类工作负载的例子包括媒体转码、计算流体动力学、蒙特卡洛模拟、基因组处理和药物发现等。这类工作负载通常需要大量的计算能力，可以通过并行运行任务来优化。自行创建和运行这些作业可能会产生一些开销，例如管理服务器、排队机制、并行化和故障逻辑。幸运的是，Google
    Cloud 批处理服务已经构建好以管理您所需的所有这些活动。作为一个完全管理的作业调度器，它自动调整运行批处理作业所需的基础设施规模，并处理您在执行过程中配置的并行化和重试逻辑，以防出现任何错误。
- en: Now that we’ve covered the primary computing services on Google Cloud, let’s
    review some of the services you can use to integrate between different Google
    Cloud services.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了 Google Cloud 上的主要计算服务，让我们回顾一下您可以使用它们在 Google Cloud 之间进行集成的服务。
- en: Google Cloud integration services
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Google Cloud 集成服务
- en: In addition to Google Cloud infrastructure services such as compute and storage,
    we often need to implement integrations between the services in order to create
    complex workloads. Google Cloud has created tools specifically for this purpose,
    and we will briefly discuss some of the relevant tools in this section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算和存储等 Google Cloud 基础设施服务之外，我们通常还需要实现服务之间的集成，以便创建复杂的工作负载。Google Cloud 为此目的创建了专门的工具，我们将在本节中简要讨论一些相关的工具。
- en: Pub/Sub
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Pub/Sub
- en: Google Cloud Pub/Sub is a messaging service that can be used to pass data between
    components of your system architecture, whether those components are other Google
    Cloud services, third-party services, or components you’ve built yourself. It’s
    an extremely versatile service that can be used for a wide array of system integration
    use cases, such as decoupling microservices, or streaming data into a data lake.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Pub/Sub 是一种消息服务，可用于在系统架构的组件之间传递数据，无论这些组件是其他 Google Cloud 服务、第三方服务还是您自己构建的组件。这是一个极其灵活的服务，可用于广泛的系统集成用例，例如解耦微服务或将数据流式传输到数据湖。
- en: Pub/Sub relates to the system architecture concept of publishing and subscribing,
    whereby one system can publish a message or a piece of data to a shared space,
    or **Topic**, and other systems can then receive that piece of data by subscribing
    to that topic. The messages can be delivered via either a **Push** or **Pull**
    mechanism. In the case of a push approach, the Pub/Sub service initiates the communication
    with the subscriber systems and sends the message to those systems. In the case
    of a pull model, the subscriber systems initiate the communication to the Pub/Sub
    service and then request or pull the information from the Pub/Sub service.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: Pub/Sub 与发布和订阅的系统架构概念相关，其中一个系统可以向共享空间或**主题**发布一条消息或数据，然后其他系统可以通过订阅该主题来接收该数据。消息可以通过**推送**或**拉取**机制进行传递。在推送方法的情况下，Pub/Sub
    服务与订阅者系统建立通信，并将消息发送到这些系统。在拉取模型的情况下，订阅者系统与 Pub/Sub 服务建立通信，然后从 Pub/Sub 服务请求或拉取信息。
- en: Pub/Sub also caters to nuanced messaging needs such as publishing messages in
    order (if required) and retrying failed message transmissions. Google Cloud also
    provides an offering called Pub/Sub Lite, which is a lower-cost option with fewer
    features than the regular Pub/Sub product.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Pub/Sub 还满足了一些细微的消息需求，例如按顺序发布消息（如果需要）和重试失败的消息传输。Google Cloud 还提供了一种名为 Pub/Sub
    Lite 的产品，它是一种比常规 Pub/Sub 产品功能更少、成本更低的选项。
- en: Google Cloud Tasks
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Tasks
- en: Similar to Pub/Sub, Google Cloud Tasks is a service that can be used to implement
    message passing and asynchronous system integration. With Pub/Sub, the publishers
    and subscribers are completely decoupled, and they have no control over each other’s
    implementations. On the other hand, with Cloud Tasks, the publisher (or **task
    producer**) fully controls the overall execution of the workload. It can be specifically
    used for cases where a task producer needs to control the execution timing of
    a specific webhook or remote procedure call. Cloud Tasks is included in this section
    for completeness because it’s an alternative to Pub/Sub for some use cases, but
    we will not use Cloud Tasks in this book.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Pub/Sub 类似，Google Cloud Tasks 是一种可用于实现消息传递和异步系统集成的服务。在 Pub/Sub 中，发布者和订阅者是完全解耦的，它们对彼此的实现没有控制权。另一方面，在
    Cloud Tasks 中，发布者（或**任务生产者**）完全控制工作负载的整体执行。它可以专门用于任务生产者需要控制特定 webhook 或远程过程调用执行时间的情况。Cloud
    Tasks 包含在本节中是为了完整性，因为它是 Pub/Sub 在某些用例中的替代方案，但我们在本书中不会使用 Cloud Tasks。
- en: Eventarc
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Eventarc
- en: Eventarc is a Google Cloud service that enables you to build *event-driven*
    workloads. This is a common pattern for companies that want their workloads to
    execute in response to events that happen in their environment. We touched on
    this topic briefly when we introduced Cloud Functions. Cloud Functions can be
    triggered directly by certain event sources, but Eventarc provides much more flexibility
    and control for implementing complex event-driven architectures, in conjunction
    with Cloud Functions and other Google Cloud services, as well as some third-party
    applications.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: Eventarc 是一种 Google Cloud 服务，使您能够构建**事件驱动**的工作负载。这是希望其工作负载在环境中的事件发生时执行的公司常用的模式。当我们介绍
    Cloud Functions 时，我们简要地提到了这个话题。Cloud Functions 可以由某些事件源直接触发，但 Eventarc 提供了更多的灵活性和控制，以实现复杂的事件驱动架构，与
    Cloud Functions 和其他 Google Cloud 服务以及一些第三方应用程序一起使用。
- en: Eventarc uses Pub/Sub to route messages from **Event Providers** to **Event
    Destinations**. As the names suggest, event providers send events to Eventarc,
    and Eventarc sends events to event destinations. It provides a way to standardize
    your event processing architectures, rather than building random, ad hoc event-driven
    implementations between your various system components.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Eventarc 使用 Pub/Sub 将来自 **事件提供者** 的消息路由到 **事件目的地**。正如其名称所暗示的，事件提供者将事件发送到 Eventarc，Eventarc
    将事件发送到事件目的地。它提供了一种标准化您的事件处理架构的方法，而不是在您的各种系统组件之间构建随机、临时的基于事件的实现。
- en: Workflows
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 工作流程
- en: While Eventarc provides a mechanism for standardizing your event-driven workloads,
    Google Cloud Workflows, as the name suggests, is a service that has been specifically
    built to orchestrate complex workflows, in which the coordination of activities
    among various systems needs to be implemented in a specific order. With this in
    mind, Google Cloud Workflows and Eventarc make a great pair when used together
    to implement complex, event-driven workloads. Workflows can either be triggered
    by events, or you can create batch workflows that can be triggered in different
    ways.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Eventarc 提供了一种标准化事件驱动工作负载的机制，但 Google Cloud Workflows，正如其名称所暗示的，是一种专门构建来编排复杂工作流程的服务，在这些工作流程中，需要按照特定顺序实现不同系统之间活动的协调。考虑到这一点，当一起使用时，Google
    Cloud Workflows 和 Eventarc 是实现复杂、事件驱动工作负载的绝佳组合。工作流程可以由事件触发，或者您可以创建可以以不同方式触发的批量工作流程。
- en: Workflows can orchestrate activities between various microservices and custom
    or third-party APIs. The Workflows service maintains the state of each step in
    your workload during execution, meaning that it tracks the inputs and outputs
    of each step, and it knows which steps have already been completed, which steps
    are currently executing, and which steps remain to be invoked in the workflow.
    It allows you to visualize all of your workflow’s steps and their dependencies,
    and if any step in the process fails, you can use the Workflows service to figure
    out which one and determine what to do next. *Figure 3**.10* shows an example
    of a workflow process for an online retail system, in which a customer purchases
    an item. Various Google Cloud computing products are used to run each of the software
    services in the workflow, and the coordination of each of the steps in the process
    is managed by the Workflows service. While this is an example of a simple order
    processing workflow, note that most large retail companies work with supply chains
    consisting of extremely complex webs of interconnected systems and partners.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程可以在各种微服务和自定义或第三方 API 之间协调活动。工作流程服务在执行过程中维护您工作负载中每个步骤的状态，这意味着它跟踪每个步骤的输入和输出，并且知道哪些步骤已经完成，哪些步骤正在执行，以及哪些步骤需要在工作流程中调用。它允许您可视化工作流程的所有步骤及其依赖关系，如果在过程中任何步骤失败，您可以使用工作流程服务来确定是哪个步骤，并确定下一步要做什么。*图
    3.10* 展示了在线零售系统的工作流程示例，其中客户购买了一件商品。工作流程中的每个软件服务都使用各种 Google Cloud 计算产品运行，每个步骤的协调都由工作流程服务管理。虽然这是一个简单的订单处理工作流程示例，但请注意，大多数大型零售公司都与由极其复杂的互联系统和合作伙伴组成的供应链合作。
- en: '![Figure 3.10: Example workflow for online retail system](img/B18143_03_10.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.10：在线零售系统的工作流程示例](img/B18143_03_10.jpg)'
- en: 'Figure 3.10: Example workflow for online retail system'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10：在线零售系统的工作流程示例
- en: The Workflows service is best suited to orchestrating activities between services.
    If you want to implement an orchestration workflow for data engineering, then
    Google Cloud Composer may be more suitable. We discuss Google Cloud Composer later
    in this chapter.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程服务最适合在服务之间编排活动。如果您想实现数据工程的工作流程编排，那么 Google Cloud Composer 可能更合适。我们将在本章后面讨论
    Google Cloud Composer。
- en: Scheduler
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 调度器
- en: Google Cloud Scheduler is a relatively simple but very useful service that can
    be used to execute workloads according to a schedule. For example, if you want
    a process to run at the same time every day, every hour, or every month, you could
    use Cloud Scheduler to define and kick off those executions. Any of you who are
    familiar with Unix-based operating systems may see a similarity with the cron
    service.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Scheduler 是一种相对简单但非常有用的服务，可以用于根据计划执行工作负载。例如，如果您希望一个进程每天、每小时或每月在同一时间运行，您可以使用
    Cloud Scheduler 定义并启动这些执行。任何熟悉基于 Unix 的操作系统的您可能会看到与 cron 服务的相似之处。
- en: Google Cloud Scheduler can be used in conjunction with many of the integration
    services we’ve described in this section. For example, you could schedule a message
    to be sent to a Pub/Sub topic every 15 minutes, and that could then be sent to
    Eventarc and used to invoke a cloud function.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Scheduler 可以与我们在本节中描述的许多集成服务一起使用。例如，你可以安排每15分钟向 Pub/Sub 主题发送一条消息，然后可以将其发送到
    Eventarc 并用于调用云函数。
- en: Networking and connectivity
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络和连接
- en: Very few workloads exist without the need to set up some kind of network connectivity.
    For example, even if you have only a single server, you generally need to connect
    to it in some way in order to perform any actions on it. As you scale beyond a
    single server, those servers usually need to communicate with each other. In this
    section, we discuss the fundamental networking and connectivity concepts upon
    which we will build our workloads in later chapters of this book.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎没有工作负载不需要设置某种形式的网络连接。例如，即使你只有一个服务器，你也通常需要以某种方式连接到它，才能对其执行任何操作。当你扩展到单个服务器之外时，那些服务器通常需要相互通信。在本节中，我们将讨论构建本书后续章节中工作负载的基础网络和连接概念。
- en: Virtual Private Cloud (VPC)
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟专用云（VPC）
- en: The first concept we introduce in this section is the **Virtual Private Cloud**
    (**VPC**) concept. A VPC is a virtual network that can span all Google Cloud regions.
    The reason it is called a virtual private cloud is that it defines the boundaries
    of your networking infrastructure, and therefore where you run your workloads
    within Google Cloud. You can, however, peer or share connectivity with other VPCs
    in order to communicate across VPC boundaries.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中我们首先介绍的概念是**虚拟专用云**（**VPC**）概念。VPC 是一个可以跨越所有谷歌云区域的虚拟网络。之所以称为虚拟专用云，是因为它定义了你的网络基础设施的边界，因此你在谷歌云中运行工作负载的位置。然而，你可以与其他
    VPC 进行对等或共享连接，以便跨 VPC 边界进行通信。
- en: Hybrid networking
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合网络
- en: If you’re working for a company that has its own on-premises servers and networks,
    and you want to connect them to the cloud, this is referred to as **Hybrid Connectivity**.
    It’s a common need for many companies, and therefore, Google Cloud has created
    specific solutions to facilitate this kind of connectivity, which consist of the
    following offerings.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你为一家拥有自己的本地服务器和网络的公同工作，并且你想将它们连接到云端，这被称为**混合连接性**。这对于许多公司来说是一个常见需求，因此，谷歌云已经创建了特定的解决方案来促进这种连接，这些解决方案包括以下服务。
- en: Dedicated Interconnect
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 专用互连
- en: Dedicated Interconnect provides direct physical connections between your on-premises
    network and Google’s network. It offers a guaranteed uptime of 99.99% and can
    connect either one or two links that each can support up to 100 **gigabits per
    second** (**Gbps**) in bandwidth. It requires hardware connectivity to be set
    up at specific Dedicated Interconnect locations, and therefore it can require
    non-trivial effort to set it up. This option is for companies who need high-bandwidth
    networking between their premises and Google Cloud for long-lived connectivity.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 专用互连为你的本地网络和谷歌的网络之间提供直接的物理连接。它提供99.99%的保证正常运行时间，并且可以连接一个或两个链路，每个链路都可以支持高达100
    **千兆比特每秒**（**Gbps**）的带宽。它需要在特定的专用互连位置设置硬件连接，因此可能需要相当大的努力来设置。此选项适用于需要在其场所和谷歌云之间进行长期连接的高带宽网络的公司。
- en: Partner Interconnect
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 合作伙伴互连
- en: If you don’t have your own infrastructure built at one of the Dedicated Interconnect
    locations, there are Google Cloud partners that offer connectivity with up to
    99.99% availability through Partner Interconnect. This option also requires some
    effort in working with the partners to set it up, but it doesn’t require the same
    amount of investment as Dedicated Interconnect. A trade-off is that the partners
    generally share the connections among many customers, so the bandwidth is less
    than that of Dedicated Interconnect.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有在专用互连位置之一建立自己的基础设施，那么有一些谷歌云合作伙伴提供高达99.99%可用性的连接服务，即合作伙伴互连。此选项也需要与合作伙伴合作以设置它，但它不需要与专用互连相同的投资。一种权衡是，合作伙伴通常将连接共享给许多客户，因此带宽低于专用互连。
- en: Private Google Access (PGA) for on-premises
  id: totrans-145
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本地部署的私有谷歌访问（PGA）
- en: This is a basic connectivity option that provides direct access to Google services
    such as Cloud Storage and BigQuery from your on-premises locations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的连接选项，它从你的本地位置直接提供对谷歌服务（如云存储和 BigQuery）的访问。
- en: Virtual Private Network (VPN)
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 虚拟专用网络（VPN）
- en: Perhaps the easiest way to connect your on-premises resources to your Google
    Cloud VPC is via a **Virtual Private Network** (**VPN**), which uses **IP security**
    (**IPsec**) mechanisms to offer a low-cost option that delivers 1.5 – 3.0 Gbps
    of throughput over an encrypted public internet connection. Unlike the Interconnect
    services mentioned previously, this option does not require any special, hardware-related
    network connectivity in any specific location.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的本地资源连接到Google Cloud VPC可能最简单的方式是通过**虚拟专用网络**（**VPN**），它使用**IP安全**（**IPsec**）机制提供了一种低成本选项，通过加密的公共互联网连接提供1.5
    – 3.0 Gbps的吞吐量。与之前提到的Interconnect服务不同，此选项不需要在任何特定位置进行任何特殊、与硬件相关的网络连接。
- en: Now that we’ve covered the fundamental Google Cloud services that underpin the
    workloads that we’ll build in this book, it’s time to dive into the services that
    we will directly use to create our data processing workloads to prepare for our
    AI/ML use cases.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了支撑我们在本书中构建的工作负载的基本Google Cloud服务，现在是时候深入探讨我们将直接用于创建我们的数据处理工作负载以准备我们的AI/ML用例的服务了。
- en: Google Cloud tools for data storage and processing
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud数据存储和处理工具
- en: Since gathering data is the first major step in an AI/ML project (after establishing
    the business objectives of the project), we begin our exploration of Google Cloud’s
    AI/ML-related services by first reviewing the tools for storing and processing
    data. *Figure 3**.2* shows the steps in the life cycle that relate to ingesting,
    storing, and processing data. It should be noted that the **Train Model**, **Evaluate
    Model**, and **Monitor Model** steps would also usually create outputs that need
    to be stored somewhere.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 由于收集数据是AI/ML项目中的第一个主要步骤（在确定项目的业务目标之后），我们首先通过回顾存储和处理数据的工具来探索Google Cloud的AI/ML相关服务。*图3.2*显示了与摄取、存储和处理数据相关的生命周期步骤。需要注意的是，**训练模型**、**评估模型**和**监控模型**步骤通常也会创建需要存储的输出。
- en: '![Figure 3.11: Ingesting, storing, exploring, and processing data](img/B18143_03_11.jpg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![图3.11：数据摄取、存储、探索和处理](img/B18143_03_11.jpg)'
- en: 'Figure 3.11: Ingesting, storing, exploring, and processing data'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11：数据摄取、存储、探索和处理
- en: As can be seen in *Figure 3**.11*, and as we’ve discussed previously, working
    with data is a very prominent part of any AI/ML project.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.11*所示，并且正如我们之前讨论的，与数据打交道是任何AI/ML项目的一个非常突出的部分。
- en: Data ingestion
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: Before we can do anything with data in Google Cloud, we need to get access to
    the data, and we often want to ingest that data into some kind of storage service
    on Google Cloud. In this section, we’ll discuss some of the tools that exist on
    Google Cloud for ingesting data, and in the next section, we will cover the Google
    Cloud storage systems into which the ingestion services ingest data. We will not
    focus on Google Cloud’s database services in this chapter, nor the related **Database
    Migration Service** (**DMS**), because for machine learning purposes, we would
    usually extract data from databases and place it into one of the storage systems
    described in this section. An exception to this may be Google Cloud Bigtable,
    but we will discuss that service separately in a later chapter.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够在Google Cloud中对数据进行任何操作之前，我们需要获取数据，并且我们通常希望将数据摄取到Google Cloud上的某种存储服务中。在本节中，我们将讨论Google
    Cloud上用于摄取数据的某些工具，在下一节中，我们将介绍摄取服务所摄取数据的Google Cloud存储系统。在本章中，我们不会关注Google Cloud的数据库服务，也不会涉及相关的**数据库迁移服务**（**DMS**），因为在机器学习的目的下，我们通常会从数据库中提取数据并将其放置在本节描述的某个存储系统中。对此的一个例外可能是Google
    Cloud Bigtable，但我们将单独在稍后的章节中讨论该服务。
- en: gsutil
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: gsutil
- en: Perhaps the simplest way to transfer data to **Google Cloud Storage** (**GCS**),
    or between GCS buckets, is via the gsutil command-line tool, which can be used
    to transfer up to 1 TB of data with a simple command.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据传输到**Google Cloud Storage**（**GCS**）或GCS存储桶之间可能最简单的方式是通过gsutil命令行工具，它可以用来通过简单的命令传输高达1
    TB的数据。
- en: The Data transfer service
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据传输服务
- en: If you want to transfer more than 1 TB of data, you can use the data transfer
    service, which transfers data quickly and securely from on-premises systems or
    other public cloud providers. For large data migration projects, in which you
    may wish to run multiple data transfer jobs, it lets you centralize your job management
    to monitor the status of each job. You can transfer petabytes of data consisting
    of billions of files, at up to tens of Gbps of bandwidth, and the data transfer
    service will optimize your network bandwidth to accelerate transfers. You can
    ingest your data into GCS and then have other Google Cloud services access it
    from there.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您要传输超过 1 TB 的数据，可以使用数据传输服务，该服务可以从本地系统或其他公共云提供商快速且安全地传输数据。对于可能需要运行多个数据传输作业的大型数据迁移项目，它允许您集中管理作业以监控每个作业的状态。您可以通过高达数十
    Gbps 的带宽传输包含数十亿文件的 PB 级数据，数据传输服务将优化您的网络带宽以加速传输。您可以将数据导入 GCS，然后让其他 Google Cloud
    服务从那里访问它。
- en: The BigQuery Data Transfer Service
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BigQuery 数据传输服务
- en: The BigQuery Data Transfer Service automates data movement specifically into
    BigQuery, on a scheduled, managed basis. You can access the BigQuery Data Transfer
    Service using the Google Cloud console, the **bq** command-line tool, or the BigQuery
    Data Transfer Service API. It supports lots of data sources, such as GCS, Google
    Ads, YouTube, Amazon S3, Amazon Redshift, Teradata, and many more.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: BigQuery 数据传输服务自动化了数据在 BigQuery 中的特定移动，以计划和管理为基础。您可以通过 Google Cloud 控制台、**bq**
    命令行工具或 BigQuery 数据传输服务 API 访问 BigQuery 数据传输服务。它支持许多数据源，例如 GCS、Google Ads、YouTube、Amazon
    S3、Amazon Redshift、Teradata 以及更多。
- en: Data storage
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据存储
- en: There are many different ways to store data in Google Cloud, and the types of
    tools and services you select for data storage will depend on your use case and
    what you’re trying to achieve. In this section, we’ll take a look at the different
    products and services provided by Google Cloud in the data storage space, and
    the kinds of workloads to which they best relate.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 中存储数据有许多不同的方式，您选择的数据存储工具和服务类型将取决于您的用例和您试图实现的目标。在本节中，我们将探讨 Google
    Cloud 在数据存储领域提供的不同产品和服务的类型，以及它们最适合的工作负载。
- en: Concepts – data warehouses, data lakes, and lake houses
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概念 - 数据仓库、数据湖和湖屋
- en: Before diving into each of the major Google Cloud data storage services, it’s
    important to discuss the concepts of data warehouses, data lakes, and lake houses,
    which are all terms that have become quite popular in the industry in recent years.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解每个主要的 Google Cloud 数据存储服务之前，讨论数据仓库、数据湖和湖屋的概念非常重要，这些术语在近年来在业界变得相当流行。
- en: A **data warehouse** usually contains structured data in a format that is optimized
    for analytics purposes, such as columnar data formats like **Parquet** or **Optimized
    Row Columnar** (**ORC**). This is because data analytics queries often operate
    on database columns rather than rows. For example, we might run a query to find
    out the average age of customers who buy our products, and this query would therefore
    focus on the *age* column in our customer database table. Columnar data formats
    store all of the elements of each column near each other on the physical storage
    disks so queries that operate on database columns run more efficiently.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据仓库**通常包含针对分析目的优化的结构化数据格式，例如 **Parquet** 或 **优化行列存储**（**ORC**）。这是因为数据分析查询通常在数据库列上而不是行上操作。例如，我们可能运行一个查询来找出购买我们产品的客户的平均年龄，因此这个查询将专注于我们客户数据库表中的
    **年龄** 列。列式数据格式将每个列的所有元素在物理存储磁盘上彼此靠近存储，因此操作数据库列的查询运行得更高效。'
- en: A **data lake**, as the name suggests, serves as a reservoir in which you can
    store huge amounts of data in a variety of formats, both structured and unstructured.
    Because there are no specific requirements regarding query optimization, data
    lakes can usually store much more data than data warehouses. Data lakes are a
    key ingredient in breaking down the problematic data silos that we described in
    [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), and they can serve as the foundation
    of your data management strategy.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，**数据湖**是一个可以以各种格式存储大量数据的存储库，包括结构化和非结构化数据。由于没有关于查询优化的具体要求，数据湖通常可以存储比数据仓库多得多的数据。数据湖是打破我们在
    [*第 2 章*](B18143_02.xhtml#_idTextAnchor035) 中描述的问题数据孤岛的关键组成部分，并且可以作为您数据管理策略的基础。
- en: The term, **data lake house**, refers to more recently emerging patterns, in
    which companies utilize a combination of data warehouses and data lakes in order
    to get the best of both worlds and support a broader set of use cases, such as
    real-time analytics, batch data processing, machine learning, and visualization,
    all from the same source.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**数据湖屋**指的是最近出现的模式，其中公司利用数据仓库和数据湖的组合，以获得两者的最佳效果，并支持更广泛的用例，如实时分析、批量数据处理、机器学习和可视化，所有这些都可以从同一来源进行。
- en: Google Cloud Storage (GCS)
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Storage (GCS)
- en: In addition to Google Compute Engine, **Google Cloud Storage** (**GCS**) is
    one of the most fundamental services in Google Cloud. It supports what’s referred
    to as **object** storage, and it is perhaps the most versatile of all the storage
    services in Google Cloud, because you can store pretty much any type of data in
    GCS, and it can be directly accessed from most Google Cloud services that process
    data. It is especially suitable for large amounts of data, and it can be used
    as the basis for building an enterprise data lake.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Google Compute Engine之外，**Google Cloud Storage**（**GCS**）是Google Cloud中最基础的服务之一。它支持所谓的**对象**存储，可能是Google
    Cloud中所有存储服务中最灵活的，因为您几乎可以在GCS中存储任何类型的数据，并且可以直接从处理数据的几乎所有Google Cloud服务中访问它。它特别适合大量数据，并且可以用作构建企业数据湖的基础。
- en: GCS provides different storage classes to optimize your usage based on cost
    and frequency of access. For objects that you don’t access frequently, you can
    put them in a lower-cost storage class, and you can even configure GCS to automatically
    move your objects between storage classes based on criteria such as the age of
    each object. For more information on each of the different storage classes, and
    which one works best for different use cases, reference the table at [https://cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: GCS提供不同的存储类别，根据成本和访问频率优化您的使用。对于您不经常访问的对象，您可以将其放入成本更低的存储类别，甚至可以配置GCS根据每个对象的年龄等标准自动在存储类别之间移动您的对象。有关每个不同存储类别的更多信息以及哪个最适合不同的用例，请参考[https://cloud.google.com/storage/docs/storage-classes](https://cloud.google.com/storage/docs/storage-classes)中的表格。
- en: Filestore
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Filestore
- en: The Google Cloud Filestore service is a high-performance, fully managed file
    storage service, used for workloads in which a structured file system is needed.
    This is the concept of **Network Attached Storage**, in which your VMs and containers
    can *mount* a shared filesystem, and can access and operate on the files in the
    shared directory structure. It uses the **Network File System version 3** (**NFSv3**)
    protocol and supports any NFSv3-compatible clients.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Filestore服务是一种高性能、完全管理的文件存储服务，用于需要结构化文件系统的作业。这是**网络附加存储**的概念，其中您的虚拟机和容器可以*挂载*共享文件系统，并可以访问和操作共享目录结构中的文件。它使用**网络文件系统版本3**（**NFSv3**）协议，并支持任何NFSv3兼容客户端。
- en: 'Filestore is available in three different formats:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Filestore提供三种不同的格式：
- en: Filestore Basic, which is best for file sharing, software development, and web
    hosting
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Filestore Basic，最适合文件共享、软件开发和网站托管
- en: Filestore Enterprise, which is best for critical applications such as SAP workloads
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Filestore Enterprise，最适合关键应用，如SAP工作负载
- en: Filestore High Scale, which is best for high-performance computing, including
    genome sequencing, financial services trading analysis, and other high-performance
    workloads
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Filestore High Scale，最适合高性能计算，包括基因组测序、金融服务交易分析和其他高性能工作负载
- en: Access to the shared file system depends on the permissions you’ve configured,
    and the networking connectivity that you have set up by using the products that
    we discussed in the *Networking and connectivity* section of this chapter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 对共享文件系统的访问取决于您配置的权限以及您使用本章“网络和连接”部分中讨论的产品设置的联网连接性。
- en: Persistent Disk
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 持久磁盘
- en: So far, we’ve covered object storage and file storage. Another type of storage
    is referred to as block storage. This type of storage may be most familiar because
    it’s the traditional type of storage used by disks that are directly attached
    to computers; that is, **Direct Attached Storage** (**DAS**). For example, the
    disk drive in your laptop uses this type of storage. In companies’ own on-premises
    data centers, many servers may be connected to shared block storage devices referred
    to as a **Storage Area Network** (**SAN**), using the types of shared RAID array
    configurations that we briefly discussed in [*Chapter 1*](B18143_01.xhtml#_idTextAnchor015).
    In either case, these block storage devices appear to our server operating systems
    as if they are directly attached disks, and they are used as such by the applications
    running on our servers or in our containers.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了对象存储和文件存储。另一种存储类型被称为块存储。这种存储类型可能最为熟悉，因为它是由直接连接到计算机的磁盘使用的传统存储类型；也就是说，**直接附加存储**（**DAS**）。例如，您的笔记本电脑中的硬盘驱动器使用这种类型的存储。在公司自己的本地数据中心中，许多服务器可能连接到称为**存储区域网络**（**SAN**）的共享块存储设备，使用我们在[*第一章*](B18143_01.xhtml#_idTextAnchor015)中简要讨论的共享RAID数组配置类型。在任一情况下，这些块存储设备都似乎在我们的服务器操作系统中被视为直接附加的磁盘，并且它们被我们的服务器或容器中的应用程序用作此类磁盘。
- en: SANs can require a lot of effort to set up and maintain, but when using Google
    Cloud Persistent Disk, you can simply define what kind of disk storage you want,
    and the required capacity, and all of the underlying infrastructure is managed
    for you by Google.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: SANs可能需要大量的设置和维护工作，但使用Google Cloud Persistent Disk时，您只需简单地定义您想要的磁盘存储类型和所需容量，所有底层基础设施都由Google为您管理。
- en: At a high level, Persistent Disk provides two different storage types, which
    are **Hard Disk Drives** (**HDDs**) and **Solid State Drives** (**SSDs**). HDDs
    offer low-cost storage when bulk throughput is of primary importance. SSDs offer
    high performance and speed for both random-access workloads and bulk throughput.
    Both types can be sized up to 64 TB.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，Persistent Disk提供两种不同的存储类型，即**硬盘驱动器**（**HDDs**）和**固态驱动器**（**SSDs**）。当大量吞吐量是首要考虑时，HDDs提供低成本存储。SSDs提供高性能和速度，适用于随机访问工作负载和大量吞吐量。这两种类型都可以扩展到64
    TB。
- en: BigQuery
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BigQuery
- en: Google Cloud BigQuery is a serverless data warehouse, meaning that you can use
    it without needing to configure or manage any servers. As a data warehouse, it
    straddles both storage and processing. It can store your data in a format that’s
    optimized for data analytics workloads, and it provides tools that allow you to
    run SQL queries on that data. You can also use it to run queries on other storage
    systems such as GCS, Cloud SQL, Cloud Spanner, Cloud Bigtable, and even storage
    systems on AWS or Azure. Additionally, it provides built-in machine learning that
    enables you to get ML inferences from your data via SQL queries, without needing
    to use other services. On the other hand, if you explicitly want to use other
    services such as Vertex AI, which we will describe later in this chapter, it integrates
    easily with many other Google Cloud services.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud BigQuery是一个无服务器数据仓库，这意味着您可以在不配置或管理任何服务器的情况下使用它。作为一个数据仓库，它跨越存储和处理。它可以以优化数据分析工作负载的格式存储您的数据，并提供允许您在该数据上运行SQL查询的工具。您还可以使用它来对其他存储系统（如GCS、Cloud
    SQL、Cloud Spanner、Cloud Bigtable）以及AWS或Azure上的存储系统运行查询。此外，它还提供了内置的机器学习功能，允许您通过SQL查询从数据中获得机器学习推断，而无需使用其他服务。另一方面，如果您明确想要使用其他服务，例如我们将在本章后面描述的Vertex
    AI，它可以轻松地与其他许多Google Cloud服务集成。
- en: It supports geospatial analysis, so you can augment your analytics workflows
    with location data, and it supports real-time analytics on streaming data when
    you integrate streaming solutions such as Dataflow with BigQuery BI Engine, which
    is an in-memory analysis service that provides a sub-second query response time.
    BI Engine also natively integrates with Looker Studio and works with many business
    intelligence tools. BigQuery is an extremely popular service on Google Cloud,
    and you will learn how to use many of its features in this book.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 它支持地理空间分析，因此您可以通过添加位置数据来增强您的分析工作流程，并且当您将流式处理解决方案（如Dataflow）与BigQuery BI Engine集成时，它还支持对流数据的实时分析。BI
    Engine还与Looker Studio原生集成，并支持许多商业智能工具。BigQuery是Google Cloud上极受欢迎的服务，您将在本书中学习如何使用其许多功能。
- en: After you’ve ingested and stored data in Google Cloud, you will often want to
    organize and manage it so that it can easily be discovered and utilized effectively.
    In the next section, we will discuss Google Cloud’s data management tools.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在您将数据存储在谷歌云之后，您通常会希望对其进行组织和管理工作，以便它可以轻松被发现并有效利用。在下一节中，我们将讨论谷歌云的数据管理工具。
- en: Data management
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据管理
- en: The services we describe in this section enable you to organize your data and
    make it easier to manage how your data can be discovered and accessed by users
    in your organization, thus breaking down or preventing data silos. These tools
    act as a supporting layer between the data storage services we discussed in the
    previous section and the data processing services we will discuss in subsequent
    sections in this chapter.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中描述的服务使您能够组织您的数据，并使您的数据更容易被组织内的用户发现和访问，从而打破或预防数据孤岛。这些工具作为我们之前章节中讨论的数据存储服务和支持层之间的一个支持层，以及我们将在本章后续部分讨论的数据处理服务。
- en: BigLake
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BigLake
- en: BigLake is a storage engine that unifies data warehouses and data lakes by enabling
    BigQuery and open source frameworks such as Spark to access data with fine-grained
    access control. For example, you could store data in GCS and make it available
    as a BigLake table, and then you could access that data from BigQuery or Spark.
    Fine-grained access control means that you can control access to the data at the
    table, row, and column level. As an example, you could ensure that your data scientists
    can see all columns except the credit card information column, or you could ensure
    that the sales department for a particular geographical location can only see
    the rows that pertain to that location, and cannot see data in any rows that relate
    to other geographical locations.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: BigLake 是一种存储引擎，通过允许 BigQuery 和如 Spark 等开源框架以细粒度访问控制访问数据，统一了数据仓库和数据湖。例如，您可以将数据存储在
    GCS 中，并将其作为 BigLake 表提供，然后您可以从 BigQuery 或 Spark 访问这些数据。细粒度访问控制意味着您可以在表、行和列级别控制对数据的访问。例如，您可以确保您的数据科学家可以看到除信用卡信息列之外的所有列，或者您可以确保特定地理位置的销售部门只能看到与该地理位置相关的行，而无法看到与任何其他地理位置相关的数据。
- en: BigLake allows you to perform analytics on distributed data regardless of where
    and how it’s stored, using your preferred analytics tools – open source or cloud
    native – over a single copy of the data. This is important because it means you
    don’t need to move the data around between your data lakes and data warehouses,
    which has traditionally been laborious and expensive to do. BigLake also supports
    open source engines such as Apache Spark, Presto, and Trino, and open formats
    such as Parquet, Avro, ORC, CSV, and JSON, serving multiple compute engines through
    Apache Arrow. You can centrally manage data security policies in one place and
    have them consistently enforced across multiple query engines, and across multiple
    clouds when using BigQuery Omni. It can also integrate with Google Cloud Dataplex,
    which we will describe next, to enhance this functionality and provide unified
    data governance and management at scale.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: BigLake 允许您在数据存储的任何位置和方式上执行分析，无论使用的是您偏好的分析工具——开源或云原生——都可以在单一份数据副本上操作。这一点很重要，因为它意味着您不需要在您的数据湖和数据仓库之间移动数据，这通常既费时又昂贵。BigLake
    还支持如 Apache Spark、Presto 和 Trino 等开源引擎，以及 Parquet、Avro、ORC、CSV 和 JSON 等开放格式，通过
    Apache Arrow 为多个计算引擎提供服务。您可以在一个地方集中管理数据安全策略，并确保它们在多个查询引擎以及使用 BigQuery Omni 时在多个云中一致执行。它还可以与我们将要描述的谷歌云
    Dataplex 集成，以增强这一功能并提供大规模的统一数据治理和管理。
- en: Dataplex
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dataplex
- en: Google refers to Dataplex as an “*intelligent data fabric that enables organizations
    to centrally discover, manage, monitor, and govern their data across data lakes,
    data warehouses, and data marts, with consistent controls*.” This relates to the
    concept of breaking down data silos. In [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    we talked about data silos being a common challenge that companies run into when
    they wish to perform data science tasks and the complexities of managing who can
    access the data securely when you have many datasets owned by various organizations
    throughout your company. Dataplex helps to overcome these challenges by enabling
    data discovery and providing a single pane of glass for data management across
    data silos, and centralized security and governance. This means that you can define
    security and governance policies in Dataplex, and have them applied to data that
    is stored and accessed by other systems, in a consistent manner. It integrates
    with other Google Cloud data management services such as BigQuery, Cloud Storage,
    and Vertex AI.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: Google 将 Dataplex 称为“*一种智能数据布料，它使组织能够集中发现、管理、监控和治理其跨数据湖、数据仓库和数据集市的数据，并具有一致的控制*。”这关系到打破数据孤岛的概念。在
    [*第二章*](B18143_02.xhtml#_idTextAnchor035) 中，我们讨论了数据孤岛是公司在执行数据科学任务时遇到的常见挑战，以及当你拥有多个由公司内不同组织拥有的数据集时，管理谁可以安全访问数据的复杂性。Dataplex
    通过启用数据发现，提供一个跨数据孤岛的数据管理单一视角，以及集中式安全和治理来帮助克服这些挑战。这意味着你可以在 Dataplex 中定义安全和治理策略，并以一致的方式应用于由其他系统存储和访问的数据。它集成了其他
    Google Cloud 数据管理服务，如 BigQuery、Cloud Storage 和 Vertex AI。
- en: With Dataplex, the idea is to create a *data mesh*, in which there are logical
    connections between your various data stores and data processing systems, rather
    than disjointed data silos. It also uses Google’s AI/ML capabilities to provide
    additional features such as automated data life cycle management, data quality
    enforcement, and lineage tracking (you may remember that, in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    we also talked about lineage tracking being a difficult, common challenge that
    companies face when implementing data science workloads).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Dataplex 的理念是创建一个 *数据网格*，在这个网格中，你的各种数据存储和数据处理系统之间存在逻辑连接，而不是孤立的数据孤岛。它还利用 Google
    的 AI/ML 能力提供额外的功能，例如自动数据生命周期管理、数据质量执行和血缘跟踪（你可能还记得，在 [*第二章*](B18143_02.xhtml#_idTextAnchor035)
    中，我们也讨论了血缘跟踪是公司在实施数据科学工作负载时面临的困难且常见的挑战）。
- en: Google Cloud originally had a standalone service named Data Catalog, which could
    be used to store metadata about your various datasets, and therefore provide discoverability
    by allowing you to view and search through the metadata to understand what datasets
    are available. This service is now provided within Dataplex, and it can even automate
    data discovery, classification, and metadata enrichment. It can then logically
    organize data that exists across multiple storage services into business-specific
    domains using the concepts of Dataplex lakes and data zones.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud 最初有一个名为 Data Catalog 的独立服务，可以用来存储关于你各种数据集的元数据，因此通过允许你查看和搜索元数据来提供可发现性，以了解哪些数据集可用。这项服务现在包含在
    Dataplex 中，甚至可以自动化数据发现、分类和元数据丰富。然后，它可以使用 Dataplex 湖和数据区域的概念，将存在于多个存储服务中的数据进行逻辑组织，进入业务特定的领域。
- en: Dataplex also provides some data processing functionality via its *serverless
    data exploration workbench*, which provides one-click access to Spark SQL scripts
    and Jupyter notebooks, allowing you to interactively query your datasets. The
    workbench also allows teams to publish, share, and search for datasets, therefore
    enabling discoverability and collaboration across teams.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Dataplex 还通过其 *无服务器数据探索工作台* 提供一些数据处理功能，该工作台提供一键访问 Spark SQL 脚本和 Jupyter 笔记本，允许你交互式地查询你的数据集。工作台还允许团队发布、共享和搜索数据集，从而实现跨团队的发现性和协作。
- en: Speaking of data processing, our next section will cover some of the primary
    tools and services available for processing data on Google Cloud.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 谈到数据处理，我们下一节将介绍一些在 Google Cloud 上处理数据的初级工具和服务。
- en: Data processing
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据处理
- en: When you have used some of the services we discussed in the previous sections
    to store, organize, and manage data, you may then wish to process that data in
    some way. Fortunately, there are a number of tools and services in Google Cloud
    that can be used for this purpose, and we will explore them here.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用我们在上一节中讨论的一些服务来存储、组织和管理工作负载时，你可能还希望以某种方式处理这些数据。幸运的是，Google Cloud 中有许多工具和服务可以用于此目的，我们将在下面探讨它们。
- en: Dataproc
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dataproc
- en: Dataproc is a fully managed and highly scalable service for running Apache Hadoop,
    Apache Spark, Apache Flink, Presto, and 30+ open source tools and frameworks.
    It is therefore very popular for data processing workloads on Google Cloud; especially
    when open source tools are preferred. You can either manage the servers that process
    your data yourself, or Dataproc also provides a serverless option, in which Google
    will manage all of the servers for you. People sometimes want to manage the servers
    themselves if they want to use customized configurations or customized tools.
    Dataproc also integrates with other Google tools such as BigQuery, and Vertex
    AI to cater to flexible data management needs and data science projects, and you
    can enforce fine-grained row and column-level access controls with Dataproc, BigLake,
    and Dataplex. You can also manage and enforce user authorization and authentication
    using existing Kerberos and Apache Ranger policies, and it provides the built-in
    Dataproc Metastore, which eliminates the need to run your own Hive Metastore or
    catalog service.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Dataproc 是一个完全托管且高度可扩展的服务，用于运行 Apache Hadoop、Apache Spark、Apache Flink、Presto
    以及 30 多个开源工具和框架。因此，它在 Google Cloud 上非常受欢迎，尤其是在偏好开源工具的数据处理工作负载中；你可以自己管理处理你数据的服务器，或者
    Dataproc 也提供无服务器选项，在这种情况下，Google 将为你管理所有服务器。人们有时会想自己管理服务器，如果他们想使用自定义配置或自定义工具的话。Dataproc
    还集成了其他 Google 工具，如 BigQuery 和 Vertex AI，以满足灵活的数据管理需求和数据科学项目，并且你可以使用 Dataproc、BigLake
    和 Dataplex 强制执行细粒度的行和列级访问控制。你也可以使用现有的 Kerberos 和 Apache Ranger 策略来管理和强制执行用户授权和身份验证，并且它提供了内置的
    Dataproc Metastore，从而消除了运行自己的 Hive Metastore 或目录服务的需要。
- en: Managing your own on-premises Hadoop or Spark clusters can require a lot of
    work. One of the great things about Dataproc is that you can easily spin up clusters
    on demand in order to run a data processing workload, and then automatically shut
    them down when you’re not using them, and clusters can automatically scale up
    and down to meet your needs, which helps to save costs. You can also use Google
    Compute Engine Spot instances to further save costs for workloads that can tolerate
    being interrupted. You can run your workloads in VMs or containers, and it also
    supports GPUs if you need to use those in your data processing workloads.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 管理你自己的本地 Hadoop 或 Spark 集群可能需要大量工作。Dataproc 的一个优点是你可以轻松地按需启动集群以运行数据处理工作负载，然后在你不再使用它们时自动关闭它们，并且集群可以自动扩展和缩减以满足你的需求，这有助于节省成本。你也可以使用
    Google Compute Engine Spot 实例进一步节省可以容忍中断的工作负载的成本。你可以在虚拟机或容器中运行你的工作负载，并且它还支持 GPU，如果你需要在数据处理工作负载中使用它们的话。
- en: Dataprep
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Dataprep
- en: Dataprep by Trifacta is a tool for visually exploring, cleaning, and preparing
    structured and unstructured data for analysis, reporting, and machine learning.
    It’s serverless, so there is no infrastructure to deploy or manage. It’s a very
    useful tool for the data exploration phase of a data science project, enabling
    you to explore and understand data with visual data distributions, and automatically
    detecting schemas, data types, possible joins, and anomalies such as missing values,
    outliers, and duplicates. You can then define a sequence of transformations to
    clean up and prepare your data for training ML models. You can do all of this
    visually, without needing to write any code, and it even suggests what kinds of
    transformations you may wish to implement, such as aggregation, pivot, unpivot,
    joins, union, extraction, calculation, comparison, condition, merge, regular expressions,
    and more. You can also apply data quality rules to ensure that your data meets
    your quality requirements, and it allows teams to collaborate on datasets by sharing
    or copying them as needed.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: Trifacta的Dataprep是一个用于可视化探索、清理和准备结构化和非结构化数据以供分析、报告和机器学习的工具。它是无服务器的，因此无需部署或管理任何基础设施。它是数据科学项目数据探索阶段的一个非常有用的工具，使您能够通过可视化的数据分布来探索和理解数据，并自动检测模式、数据类型、可能的连接以及缺失值、异常值和重复值等异常。然后，您可以定义一系列转换来清理和准备您的数据，以便训练机器学习模型。您可以通过可视化操作完成所有这些操作，无需编写任何代码，并且它甚至建议您可能希望实施的各种转换类型，例如聚合、转置、非转置、连接、并集、提取、计算、比较、条件、合并、正则表达式等等。您还可以应用数据质量规则以确保您的数据满足质量要求，并允许团队通过共享或按需复制数据集来协作。
- en: Dataflow
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据流
- en: In order to discuss Google Cloud Dataflow, we will first take a minute to introduce
    Apache Beam. In previous sections and chapters of this book, we’ve referred to
    *batch* data processing, in which large amounts of data are processed by long-running
    jobs, and *streaming* data processing, in which small pieces of data are processed
    very quickly, usually in real time or near real time. There are generally slightly
    different tools for each of those processing types. For example, you might use
    Hadoop for batch processing and you might use Apache Flink for stream processing.
    Apache Beam provides a unified model that can be used for both batch and streaming
    workloads. In the words of the Apache Beam project management committee, this
    allows you to “*write once, run anywhere*.” This can be very useful as it enables
    your data engineers to simplify how they code their data processing workloads
    by using this unified model instead of using completely different tools and code
    for their batch and streaming use cases.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论Google Cloud Dataflow，我们首先花一分钟介绍Apache Beam。在本书的前几节和章节中，我们提到了*批处理*数据，其中大量数据通过长时间运行的工作进行处理，以及*流处理*数据，其中小数据块被非常快速地处理，通常是在实时或接近实时。通常，每种处理类型都有不同的工具。例如，您可能使用Hadoop进行批处理，而您可能使用Apache
    Flink进行流处理。Apache Beam提供了一个统一的模型，可以用于批处理和流式工作负载。用Apache Beam项目管理委员会的话说，这允许您“*一次编写，到处运行*。”这非常有用，因为它使您的数据工程师能够通过使用这个统一的模型来简化他们编码数据处理工作负载的方式，而不是为他们的批处理和流式用例使用完全不同的工具和代码。
- en: Google Cloud Dataflow is a fully managed, serverless service for executing Apache
    Beam pipelines. Once you have defined your data processing steps as an Apache
    Beam pipeline, you can then use your preferred data processing engines, such as
    Spark, Flink, or Google Cloud Dataflow, to execute the pipeline steps. As a fully
    managed service, Dataflow can automatically provision and scale the resources
    required to run your data processing steps, and it integrates with other tools
    and services such as BigQuery, enabling you to use SQL to access your data, and
    Vertex AI notebooks for ML model training use cases.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Dataflow是一个完全托管、无服务器的服务，用于执行Apache Beam管道。一旦您将数据处理步骤定义为Apache Beam管道，然后您可以使用您首选的数据处理引擎，例如Spark、Flink或Google
    Cloud Dataflow来执行管道步骤。作为一个完全托管的服务，Dataflow可以自动配置和扩展运行数据处理步骤所需的资源，并且它与BigQuery等工具和服务集成，使您能够使用SQL访问您的数据，以及Vertex
    AI笔记本用于机器学习模型训练用例。
- en: Looker
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Looker
- en: Looker is Google Cloud’s business intelligence platform with embedded analytics.
    One of its main advantages is LookML, which is a powerful SQL-based modeling language.
    You can use LookML to centrally define and manage business rules and definitions
    as a version-controlled data model, and LookML can then create efficient SQL queries
    on your behalf. As a business intelligence tool, Looker then provides a user interface
    in which you can visualize your data in graphs, charts, and dashboards.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: Looker是谷歌云的企业智能平台，具有嵌入式分析功能。其主要优势之一是LookML，这是一种基于SQL的强大建模语言。您可以使用LookML作为版本控制的数据模型集中定义和管理业务规则和定义，然后LookML可以代表您创建高效的SQL查询。作为一个企业智能工具，Looker提供了一个用户界面，您可以在其中以图表、图表和仪表板的形式可视化您的数据。
- en: It comes in a few different service tiers, providing different levels of business
    intelligence functionality. Google originally had a business intelligence tool
    named Data Studio, and Looker was created by a separate company, but Google acquired
    that company and has integrated Looker into its Cloud Service portfolio. They
    also integrated their original Data Studio product into Looker, creating Looker
    Studio, and added an enterprise version of that tool, named Looker Studio Pro,
    which provides additional functionality as well as customer support.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 它提供几种不同的服务级别，提供不同层次的企业智能功能。谷歌最初有一个名为数据工作室的企业智能工具，而Looker是由另一家公司创建的，但谷歌收购了那家公司，并将Looker整合到其云服务组合中。他们还将原始的数据工作室产品整合到Looker中，创建了Looker
    Studio，并增加了一个企业版工具，名为Looker Studio Pro，它提供了额外的功能以及客户支持。
- en: Data Fusion
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据融合
- en: In order to discuss Data Fusion, let’s first talk about the concepts of **ETL**
    and **ELT**, which stand for **Extract, Transform, Load**, and **Extract, Load,
    Transform**, respectively. The ETL concept has been around since the 1970s, and
    it’s a common pattern that’s used in data science and data engineering when you
    need to perform transformations on your data. The pattern used in this case is
    to extract your data from its source storage location, transform it in some way,
    and then load it into your desired destination storage location. Examples of transformations
    could be to change the format of the data from CSV to JSON, or to remove all rows
    that contain missing information. A complex data engineering project may require
    the creation of ETL pipelines that define multiple transformation steps. ELT,
    on the other hand, has gained popularity in recent years, especially in relation
    to cloud-based data processing workloads. The idea with ELT is that you can extract
    your data from source locations and load it into a data lake or data warehouse,
    and then perform different kinds of transformations based on your project needs.
    This is also often referred to as *data integration*. Using this approach can
    enable analysts to use SQL to get insights and value from the data without requiring
    data engineers to create complex ETL pipelines.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 为了讨论数据融合，让我们首先谈谈**ETL**和**ELT**的概念，分别代表**提取、转换、加载**和**提取、加载、转换**。ETL概念自20世纪70年代以来一直存在，它是数据科学和数据工程中在需要对数据进行转换时使用的一种常见模式。在这种情况下使用的模式是从数据源存储位置提取数据，以某种方式对其进行转换，然后将其加载到所需的存储位置。转换的例子可能包括将数据格式从CSV转换为JSON，或者删除包含缺失信息的所有行。复杂的数据工程项目可能需要创建定义多个转换步骤的ETL管道。另一方面，ELT近年来在云基础数据处理工作负载中获得了流行，特别是在与云基础数据处理工作负载相关的情况下。ELT的想法是，您可以从源位置提取数据并将其加载到数据湖或数据仓库中，然后根据项目需求执行不同的转换。这通常也被称为*数据集成*。使用这种方法可以使分析师能够使用SQL从数据中获得洞察和价值，而无需数据工程师创建复杂的ETL管道。
- en: With this in mind, Data Fusion is Google Cloud’s serverless product that allows
    you to run your ETL/ELT workloads without having to manage any servers or infrastructure.
    It provides a visual interface that enables you to define your data transformation
    steps without having to write code, which makes it easy for less technical analysts
    to process the data, and it even tracks that ever-important data lineage that
    we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035), as your data
    progresses through each transformation step.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这一点，数据融合是谷歌云的无服务器产品，允许您运行ETL/ELT工作负载，而无需管理任何服务器或基础设施。它提供了一个可视化界面，使您能够在不编写代码的情况下定义数据转换步骤，这使得非技术分析师处理数据变得容易，并且它甚至跟踪我们讨论过的非常重要的数据血缘，即[*第2章*](B18143_02.xhtml#_idTextAnchor035)，随着您的数据通过每个转换步骤。
- en: Data Fusion integrates with other Google Cloud services such as BigQuery, Dataflow,
    Dataproc, Datastore, Cloud Storage, and Pub/Sub, making it easy to perform data
    processing workflows across those services.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 数据融合与Google Cloud的其他服务集成，例如BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage和Pub/Sub，这使得在这些服务之间执行数据处理工作流程变得容易。
- en: Google Cloud Composer
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Google Cloud Composer
- en: Composer is a Google Cloud orchestration service built on the open source Apache
    Airflow project, and it is particularly useful for data integration or data processing
    workloads. Like Data Fusion, it integrates with other Google Cloud services such
    as BigQuery, Dataflow, Dataproc, Datastore, Cloud Storage, and Pub/Sub, making
    it easy to orchestrate data processing workflows across those services.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Composer是基于开源Apache Airflow项目构建的Google Cloud编排服务，它特别适用于数据集成或数据处理工作负载。与数据融合类似，它集成了其他Google
    Cloud服务，如BigQuery、Dataflow、Dataproc、Datastore、Cloud Storage和Pub/Sub，这使得在这些服务之间编排数据处理工作流程变得容易。
- en: It is highly scalable, and it can also be used to implement workloads across
    multiple cloud providers and on-premises locations. It orchestrates workflows
    by using **Directed Acyclic Graphs** (**DAGs**), which represent the tasks your
    workflow needs to execute, as well as all of the relationships and dependencies
    between them.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 它具有高度的可扩展性，并且可以用于在多个云提供商和本地位置实施工作负载。它通过使用**有向无环图**（**DAGs**）来编排工作流程，这些图表示工作流程需要执行的任务，以及它们之间所有的关系和依赖。
- en: Similar to Google Cloud Workflows, Composer monitors the execution of the tasks
    in your workload and tracks whether each step is completed correctly or whether
    any problems have occurred. *Figure 3**.12* shows an example of a workflow being
    orchestrated by Google Cloud Composer, in which data regarding customer orders
    is ingested periodically from Cloud Bigtable into Cloud Dataproc and enriched
    with data regarding broader retail trends from third-party providers, which is
    provided via Google Cloud Storage. The outputs are then stored in BigQuery for
    analysis. This data could then be used to build business intelligence dashboards
    in Looker or to train machine learning models in Vertex AI, for example.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 与Google Cloud Workflows类似，Composer监控工作负载中任务的执行情况，并跟踪每个步骤是否正确完成或是否发生了任何问题。*图3.12*展示了Google
    Cloud Composer编排的一个工作流程示例，其中客户订单数据定期从Cloud Bigtable导入到Cloud Dataproc，并使用第三方提供商提供的关于更广泛零售趋势的数据进行丰富，这些数据通过Google
    Cloud Storage提供。然后，输出存储在BigQuery中以供分析。这些数据可以用来在Looker中构建业务智能仪表板，或者例如在Vertex AI中训练机器学习模型。
- en: '![Figure 3.12: Data processing workflow](img/B18143_03_12.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图3.12：数据处理工作流程](img/B18143_03_12.jpg)'
- en: 'Figure 3.12: Data processing workflow'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12：数据处理工作流程
- en: As depicted in *Figure 3**.12*, when we’ve processed and stored our data, we
    may want to use it to train a machine learning model. Before we get into training
    our own models, let’s explore some of the AI/ML capabilities that we can use on
    Google Cloud that provide models trained by Google.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图3.12*所示，当我们处理和存储我们的数据后，我们可能想使用它来训练一个机器学习模型。在我们开始训练自己的模型之前，让我们探索一些我们可以在Google
    Cloud上使用的AI/ML能力，这些能力提供了由Google训练的模型。
- en: Google Cloud AI tools and AutoML
  id: totrans-223
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud AI工具和AutoML
- en: In this section, we will cover Google Cloud’s AI tools that can be used to implement
    AI use cases without the need to understand the underlying machine learning concepts.
    With these services, you can simply send a request to an API, and get a response
    from ML models that are created and maintained by Google. There’s no need to manually
    preprocess data, train or hyper-tune machine learning models, or manage any infrastructure.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍Google Cloud的AI工具，这些工具可以用于实现AI用例，而无需了解底层机器学习概念。使用这些服务，您只需向API发送请求，就可以从由Google创建和维护的ML模型中获得响应。无需手动预处理数据，训练或超调机器学习模型，或管理任何基础设施。
- en: 'We will group these services into the following categories: **Natural Language
    Processing** (**NLP**), **Computer Vision**, and **Discovery**.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些服务分为以下几类：**自然语言处理**（**NLP**）、**计算机视觉**和**发现**。
- en: Note
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing this, in January 2023, Google has also announced a preview
    of a service named **Timeseries Insights API**, which can be used to perform analysis
    and gather insights in real time from your time series datasets, for use cases
    such as time series forecasting, or detecting anomalies and trends in your data
    while they are happening. This is an interesting new service because running forecasting
    and anomaly detection workloads over billions of time series data points is computationally
    intensive, and most existing systems implement these workloads as batch jobs,
    which limits the type of analysis you can perform online, such as deciding whether
    to alert based on a sudden increase or decrease in data values.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，即 2023 年 1 月，Google 还宣布了一个名为 **Timeseries Insights API** 的服务的预览版，该服务可以用于实时分析您的时序数据集并获取洞察，适用于时序预测或检测数据在发生时的异常和趋势。这是一个有趣的新服务，因为对数十亿个时序数据点进行预测和异常检测工作负载是计算密集型的，而大多数现有系统将这些工作负载作为批量作业实现，这限制了您在线可以执行的分析类型，例如根据数据值的突然增加或减少来决定是否发出警报。
- en: NLP
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: NLP
- en: Natural language processing, in the context of AI, refers to the use of computers
    to understand and process natural human language. It can be further broken down
    into **Natural Language Understanding** (**NLU**) and **Natural Language Generation**
    (**NLG**). NLU is concerned with understanding the content and meaning of words
    and sentences, as they are understood by humans. NLG takes this one step further
    and attempts to create or generate words and sentences in a way that can be understood
    by humans. In this section, we’ll discuss some of Google Cloud’s NLP-related services.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在人工智能的背景下，自然语言处理（NLP）是指使用计算机来理解和处理自然的人类语言。它可以进一步细分为 **自然语言理解**（**NLU**）和 **自然语言生成**（**NLG**）。NLU
    关注于理解人类如何理解单词和句子的内容和意义。NLG 则更进一步，试图以人类可以理解的方式创建或生成单词和句子。在本节中，我们将讨论一些 Google Cloud
    的与 NLP 相关的服务。
- en: The Natural Language API
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自然语言 API
- en: You can use the Google Cloud Natural Language API to understand the contents
    of textual inputs. This can be used for purposes such as sentiment analysis, entity
    analysis, content classification, and syntax analysis. With sentiment analysis,
    it can tell you what kinds of emotions are suggested by the content. For example,
    you could feed all of your product reviews into this API and get an understanding
    of whether people are responding positively to your product, or whether they may
    be frustrated by something about the product. Entity analysis can identify what
    kinds of content exist in the text, such as people’s names, place names, locations,
    addresses, and phone numbers. The content classification feature is useful if
    you have large amounts of textual data and you want to organize and categorize
    it based on the contents.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Google Cloud Natural Language API 来理解文本输入的内容。这可以用于情感分析、实体分析、内容分类和语法分析等目的。通过情感分析，它可以告诉您内容中暗示了哪些类型的情感。例如，您可以将所有产品评论输入到这个
    API 中，并了解人们是否对您的产品做出积极反应，或者他们可能因为产品的某些方面而感到沮丧。实体分析可以识别文本中存在的内容类型，例如人名、地名、地点、地址和电话号码。如果您有大量文本数据，并且希望根据内容对其进行组织和分类，内容分类功能将非常有用。
- en: Text-to-Speech
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本转语音
- en: The name of this service is somewhat self-explanatory. The service will take
    a textual input that you provide, and it will convert it to an audible spoken
    output. This can be very useful for accessibility use cases, whereby if somebody
    is visually impaired and cannot read text content, it could be automatically spoken
    to them. It provides the option to use many different voices to personalize the
    user experience, and you can even create custom voices using your own recordings.
    As of January 2023, it supported more than 40 languages and variants. It also
    supports **Speech Synthesis Markup Language** (**SSML**), which enables you to
    have more control over how words and phrases are pronounced.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这个服务的名称在一定程度上是自我解释的。该服务将接受您提供的文本输入，并将其转换为可听说的语音输出。这对于可访问性用例非常有用，如果某人视力受损且无法阅读文本内容，它可以将文本自动读给他们听。它提供了使用许多不同声音来个性化用户体验的选项，您甚至可以使用自己的录音创建自定义声音。截至
    2023 年 1 月，它支持超过 40 种语言和变体。它还支持 **语音合成标记语言**（**SSML**），这使您能够更控制地处理单词和短语的发音。
- en: Speech-to-Text
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 语音转文本
- en: This service does pretty much the opposite of the previous service. In this
    case, you can provide audio inputs, and the service will transcribe any spoken
    language into a textual output. This is useful for dictation purposes, accessibility
    use cases such as closed captioning, and other use cases such as quality control.
    For example, you could provide recordings of your customer service calls and it
    would convert them to text. Then you could feed that text into the Natural Language
    API to understand whether your customers are frustrated or happy with the service
    they are receiving. As of January 2023, the service supported an impressive 125
    languages and variants.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 此项服务基本上与之前的服务相反。在这种情况下，您可以提供音频输入，该服务将任何口语语言转录成文本输出。这对于语音输入、辅助功能用例如字幕，以及其他用例如质量控制等非常有用。例如，您可以提供客户服务通话的录音，并将其转换为文本。然后，您可以将该文本输入到自然语言API中，以了解您的客户对所收到的服务是否感到沮丧或满意。截至2023年1月，该服务支持令人印象深刻的125种语言和变体。
- en: Translation AI
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 翻译人工智能
- en: Another service whose name is self-explanatory, this service can be used to
    translate from one language to another. It can help you to internationalize your
    products, and engage your customers with localization of content. It can detect
    more than 100 languages, and you can customize the translations with industry-
    or domain-specific terms. It provides **Translation Hub**, which allows you to
    manage translation workloads at scale, as well as Media Translation API, which
    can deliver real-time audio translation directly to your applications.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 另一项名为“自解释”的服务，这项服务可用于将一种语言翻译成另一种语言。它可以帮助您国际化您的产品，并通过本地化内容与客户互动。它可以检测超过100种语言，您可以使用行业或领域特定的术语自定义翻译。它提供**翻译中心**，允许您大规模管理翻译工作负载，以及媒体翻译API，可以将实时音频翻译直接发送到您的应用程序。
- en: Contact Center AI (CCAI)
  id: totrans-238
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 联系中心人工智能（CCAI）
- en: '**Contact Center AI** (**CCAI**) provides human-like AI-powered contact center
    experiences. It consists of a number of different components, such as Dialogflow,
    which can be used to create chatbots that can have intelligent, human-like conversations
    with customers.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**联系中心人工智能**（**CCAI**）提供类似人类的AI驱动的联系中心体验。它由多个不同的组件组成，例如Dialogflow，可用于创建能够与客户进行智能、类似人类的对话的聊天机器人。'
- en: Have you ever been on hold with a company’s customer service line for an hour
    before anybody helps you with your concerns? This, of course, happens when customer
    service centers are overloaded with calls. Using chatbots can offload a significant
    amount of cases with simple questions, freeing up humans to focus on more complex
    customer interactions.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 您是否曾经遇到过在公司的客户服务热线上等待一个小时才有人帮助您解决您的问题的情况？当然，这种情况发生在客户服务中心电话过多时。使用聊天机器人可以处理大量简单问题的案例，从而让人类能够专注于更复杂的客户互动。
- en: When a human does need to get involved, CCAI has another feature, named Agent
    Assist, that provides support to human agents while they handle customer interactions.
    It can recommend ready-to-send responses to customers, provide answers to customer
    questions from a centralized knowledge base, and transcribe calls in real time.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 当人类确实需要介入时，CCAI还有一个名为“代理助手”的功能，在人类代理处理客户互动时提供支持。它可以推荐准备发送给客户的响应，从集中式知识库中提供客户问题的答案，并实时转录通话。
- en: CCAI also includes CCAI Insights, which uses NLP to identify customer sentiment
    and reasons for calls, which helps contact center managers learn about customer
    interactions to improve call outcomes.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: CCAI还包括CCAI洞察，它使用NLP来识别客户情绪和通话原因，这有助于联系中心经理了解客户互动以改善通话结果。
- en: There’s also the option to use CCAI Platform, which provides a **Contact Center
    as a Service** (**CCaaS**) solution.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以选择使用CCAI平台，该平台提供**联系中心即服务**（**CCaaS**）解决方案。
- en: Document AI
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文档人工智能
- en: Document AI goes beyond understanding the content of textual inputs, and also
    incorporates structure. It provides pre-trained models for data extraction, or
    you can use Document AI Workbench to create custom models, and you can use Document
    AI Warehouse to search and store documents. For example, if you collect information
    from people via forms, Document AI could be used to extract the data from those
    forms and store it in a database, or send it to another data processing system
    to process the data in some way or feed it to another ML model to perform some
    kind of other task. It could also be used to categorize and organize documents
    based on their contents. Some companies process millions of forms and contracts
    per year, and before these kinds of AI systems existed, all of those documents
    had to be processed by humans, which led to extremely laborious and error-prone
    work. With Document AI, you can automate that work, and you can also enrich the
    data using Google **Enterprise Knowledge Graph** (**EKG**), or you can enhance
    the functionality of Document AI with human inputs by using its **Human-in-the-Loop**
    (**HITL**) AI functionality. With HITL AI, experts can verify the outputs from
    Document AI, and provide corrections if needed. You can either use your own workforce
    of experts for this purpose or, if you don’t have such experts in your employment,
    you can use Google’s HITL workforce.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 文档 AI 不仅理解文本输入的内容，还结合了结构。它提供用于数据提取的预训练模型，或者您可以使用文档 AI 工作台创建自定义模型，您还可以使用文档 AI
    仓库搜索和存储文档。例如，如果您通过表格收集信息，文档 AI 可以用于从这些表格中提取数据并将其存储在数据库中，或者将其发送到另一个数据处理系统以某种方式处理数据，或者将其提供给另一个
    ML 模型以执行其他任务。它还可以根据其内容对文档进行分类和组织。一些公司每年处理数百万份表格和合同，在这些类型的 AI 系统存在之前，所有这些文档都必须由人工处理，这导致了极其繁重且易出错的劳动。有了文档
    AI，您可以自动化这项工作，您还可以使用 Google **企业知识图谱** (**EKG**) 来丰富数据，或者您可以通过使用其 **人机交互** (**HITL**)
    AI 功能来增强文档 AI 的功能。使用 HITL AI，专家可以验证文档 AI 的输出，并在需要时提供更正。您可以使用自己的专家团队来完成这项工作，或者如果您没有雇佣这样的专家，您可以使用
    Google 的 HITL 工作团队。
- en: Document AI’s **Optical Character Recognition** (**OCR**) functionality flows
    over into the computer vision realm, which we will discuss next.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 文档 AI 的 **光学字符识别** (**OCR**) 功能扩展到了计算机视觉领域，我们将在下一节中讨论。
- en: Computer vision
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: 'You can use Google Cloud Vision AI to create your own computer vision applications
    or get insights from images and videos with pre-trained APIs, AutoML, or custom
    models. It enables you to spin up video and image analytics applications in minutes,
    for use cases such as detecting objects, reading handwriting, or creating image
    metadata. It consists of three main components: Vertex AI Vision, Vision API,
    and custom ML models. Vertex AI Vision includes **Streams** to ingest real-time
    video data, **Applications** to let you create an application by combining various
    components, and **Vision Warehouse** to store model output and streaming data.
    The Vision API offers pre-trained ML models that you can access through REST and
    RPC APIs, allowing you to assign labels to images and classify them into millions
    of predefined categories. If you need to develop more specialized models, you
    can use AutoML or build your own custom models.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Google Cloud Vision AI 来创建自己的计算机视觉应用程序，或者通过预训练的 API、AutoML 或自定义模型从图像和视频中获取洞察。它使您能够在几分钟内启动视频和图像分析应用程序，用于诸如检测对象、读取手写内容或创建图像元数据等用例。它由三个主要组件组成：Vertex
    AI Vision、Vision API 和自定义 ML 模型。Vertex AI Vision 包括 **Streams** 用于摄取实时视频数据，**Applications**
    允许您通过组合各种组件来创建应用程序，以及 **Vision Warehouse** 用于存储模型输出和流数据。Vision API 提供预训练的 ML 模型，您可以通过
    REST 和 RPC API 访问它们，允许您为图像分配标签并将它们分类到数百万个预定义的类别中。如果您需要开发更专业的模型，您可以使用 AutoML 或构建自己的自定义模型。
- en: Vision AI can be used to implement interesting use cases such as image search,
    in which you could use Vision API and AutoML Vision to make images searchable
    based on topics and scenes detected in the images, or product search, in which
    you could enable customers to find products of interest within images and visually
    search product catalogs using the Vision API.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉 AI 可以用于实现有趣的应用场景，例如图像搜索，您可以使用 Vision API 和 AutoML Vision 根据图像中检测到的主题和场景使图像可搜索，或者产品搜索，您可以使用
    Vision API 允许客户在图像中找到感兴趣的产品，并使用视觉搜索产品目录。
- en: 'Also in the computer vision space is Google Cloud Video AI, which can analyze
    video content for use cases such as content discovery. Video AI can recognize
    over 20,000 objects, places, and actions in video, it can extract metadata at
    the video, shot, or frame level, and you can even create your own custom entity
    labels with AutoML Video Intelligence. It consists of two main components: the
    Video Intelligence API, which provides pre-trained ML models, and Vertex AI for
    AutoML video, which provides a graphical interface to train your own custom models
    to classify and track objects in videos, without the need for ML experience. Vertex
    AI for AutoML video can be used for projects requiring custom labels that aren’t
    covered by the pre-trained Video Intelligence API.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉领域，还有Google Cloud Video AI，它可以分析视频内容，用于内容发现等用例。视频AI可以识别视频中的20,000多个物体、地点和动作，它可以在视频、镜头或帧级别提取元数据，您甚至可以使用AutoML
    Video Intelligence创建自己的自定义实体标签。它由两个主要组件组成：Video Intelligence API，它提供预训练的ML模型，以及Vertex
    AI for AutoML视频，它提供了一个图形界面来训练您自己的自定义模型，以在视频中分类和跟踪对象，无需ML经验。Vertex AI for AutoML视频可用于需要自定义标签的项目，这些标签超出了预训练的Video
    Intelligence API的范围。
- en: Discovery AI
  id: totrans-251
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Discovery AI
- en: Google Cloud Discovery AI includes services such as search and recommendation
    engines. These kinds of features are essential for today’s online businesses,
    in which it’s important to ensure that your customers find what they want on your
    website as quickly and easily as possible. If this doesn’t happen, they’ll go
    to your competitor’s website. Imagine if your company could integrate Google-quality
    search into its website. This includes functionality such as image-based product
    searches, which we discussed in the previous section, making it easier for customers
    to search for products with an image, by using object recognition to provide real-time
    results of similar items from your product catalog.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: Google Cloud Discovery AI 包括搜索和推荐引擎等服务。这类功能对于当今的在线业务至关重要，确保您的客户能够尽可能快、尽可能容易地在您的网站上找到他们想要的东西非常重要。如果这种情况没有发生，他们就会去竞争对手的网站。想象一下，如果您的公司能够将其网站集成Google质量的搜索功能，这将包括图像产品搜索等功能，正如我们在上一节中讨论的那样，这将使客户能够通过使用物体识别来提供实时结果，从而更容易地通过图像搜索产品。
- en: In addition to your customers finding products by directly searching for them,
    companies see a significant amount of business being driven by recommendation
    engines, which can make personalized recommendations for products based on customers’
    previous purchasing behavior. I’ve experienced this many times myself as a consumer,
    whereby I’m purchasing something on a website, I see a recommendation for something
    else that I may be interested in purchasing, and I think “*Actually, yes I do
    also want one of those*,” and I go ahead and add it to my cart before checking
    out. These kinds of personalized experiences help to maintain customer loyalty,
    which is also extremely important in today’s online business world.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 除了客户通过直接搜索产品来找到产品外，公司还发现，推荐引擎可以基于客户的先前购买行为为产品做出个性化推荐，这推动了大量的业务。我自己作为一个消费者也多次经历过这种情况，我在网站上购买某样东西时，会看到一些我可能感兴趣的推荐，我想“*实际上，我也想买一个*”，然后我就在结账前将它添加到购物车中。这类个性化体验有助于保持客户忠诚度，这在当今的在线商业世界中也非常重要。
- en: Bringing these two concepts together – that is, search and personalization –
    makes a lot of sense, because not only will the search results intelligently match
    what the customer searched for, but the ranking of the results can be catered
    to what the specific customer is more likely to find relevant to their preferences.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 将这两个概念结合起来——即搜索和个人化——非常有意义，因为不仅搜索结果会智能地匹配客户搜索的内容，而且结果排名可以根据特定客户更可能认为与其偏好相关的结果进行定制。
- en: A third component of Discovery AI is Browse AI, which extends the search functionality
    to work on category pages in addition to text queries. Without this, retailers
    would mostly sort products on their category and navigation pages by historic
    bestsellers. This ordering doesn’t adapt well to new product additions, changes
    in product availability, and sales. With Browse AI, retailers can sort products
    on these pages in an order that is personalized to each user, and based on predicted,
    rather than historic, bestsellers. This ordering can rapidly adapt to new products,
    product stockouts, and price changes, without needing to wait for backward-looking
    bestseller lists to catch up.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: Discovery AI 的第三个组件是浏览 AI，它将搜索功能扩展到除了文本查询外，还能在分类页面上工作。如果没有这个功能，零售商在分类和导航页面上主要会根据历史畅销品来排序产品。这种排序方式并不适应新产品添加、产品可用性变化和销售情况。有了浏览
    AI，零售商可以按个人用户定制的产品顺序对这些页面上的产品进行排序，而不是基于历史畅销品。这种排序可以快速适应新产品、产品缺货和价格变化，无需等待回顾性的畅销品列表来更新。
- en: Now that we’ve taken a look at the high-level AI services on Google Cloud, which
    you can use to get inferences from ML models that are trained and maintained by
    Google by simply calling an API, let’s discuss how you can start to train your
    own models on Google Cloud, starting with AutoML.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Google Cloud 上的高级 AI 服务，您可以通过调用 API 来获取由 Google 训练和维护的 ML 模型的推断，接下来让我们讨论如何在
    Google Cloud 上开始训练自己的模型，从 AutoML 开始。
- en: AutoML
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AutoML
- en: Some of the services we discussed in the previous sections use completely pre-trained
    models, and others allow you to bring your own data to either train or *up-train*
    a model based on your data. Pre-trained models are trained on datasets provided
    by Google or other sources, and the term, up-train, refers to augmenting a pre-trained
    model with additional data. If you want to create more customized use cases than
    those supported by the high-level API services, you may want to train your own
    models. Vertex AI, which we will describe later in this chapter, provides a plethora
    of tools for implementing every step in the model development process. However,
    before we get to the level of customizing every step in the process, one way in
    which you can easily start getting inferences from ML models that are trained
    on your data in Google Cloud is to use AutoML, which enables developers with limited
    ML expertise to train models specific to their business needs in as little as
    a few minutes or hours. The actual amount of time depends on the algorithms being
    used, how much data is used for training, and some other factors, but in any case,
    AutoML saves you a lot of work and time, considering that data scientists can
    spend weeks on these tasks when not using AutoML. Looking at our ML model development
    lifecycle diagram in *Figure 3**.13*, AutoML automatically performs all of the
    steps in the process for us, as indicated by everything encapsulated within the
    blue box.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几节讨论的一些服务使用的是完全预训练的模型，而另一些服务则允许您将自己的数据带来训练或*升级训练*基于您数据的模型。预训练模型是在由 Google
    或其他来源提供的数据集上训练的，而“升级训练”一词指的是用额外数据增强预训练模型。如果您想创建比高级 API 服务支持的更多定制用例，您可能需要训练自己的模型。我们将在本章后面描述的
    Vertex AI 提供了大量的工具来实现模型开发过程中的每一步。然而，在我们达到定制过程中每一步的程度之前，您可以通过使用 AutoML 来轻松地从在 Google
    Cloud 上训练的 ML 模型中获取推断，AutoML 使具有有限 ML 专长的开发者能够在几分钟或几小时内训练针对其业务需求的特定模型。实际所需时间取决于所使用的算法、用于训练的数据量以及一些其他因素，但无论如何，AutoML
    都能为您节省大量工作和时间，考虑到在不使用 AutoML 的情况下，数据科学家可能需要花费数周时间来完成这些任务。查看我们的 ML 模型开发生命周期图*图
    3.13*，AutoML 会自动执行过程中的所有步骤，如蓝色框内所示的一切。
- en: '![Figure 3.13: ML model lifecycle managed by AutoML](img/B18143_03_13.jpg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.13：由 AutoML 管理的 ML 模型生命周期](img/B18143_03_13.jpg)'
- en: 'Figure 3.13: ML model lifecycle managed by AutoML'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13：由 AutoML 管理的 ML 模型生命周期
- en: How does AutoML work? If we think back to all of the steps in a typical data
    science project that we discussed in [*Chapter 2*](B18143_02.xhtml#_idTextAnchor035),
    you may remember that each step – especially in the early stages of a project
    – required a lot of trial and error. For example, you might try a number of different
    data transformation techniques, and then try out a few different algorithms during
    training, as well as different combinations of hyperparameter values for those
    algorithms. Each of those steps could take many days or weeks of work before you
    find a good candidate model (i.e., a model that we believe may satisfy our business
    objectives and metrics). AutoML automatically runs many trial jobs very quickly
    – generally much more quickly than a human could achieve – and evaluates the outcomes
    against desired metric thresholds. Jobs whose outcomes do not meet the desired
    criteria are not selected as candidates, and AutoML continues to try other options
    until suitable candidates are found, or some other threshold is met, such as all
    options being exhausted.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: AutoML是如何工作的？如果我们回顾一下我们在[*第二章*](B18143_02.xhtml#_idTextAnchor035)中讨论的典型数据科学项目的所有步骤，你可能还记得每个步骤——尤其是在项目的早期阶段——都需要大量的试错。例如，你可能尝试了多种不同的数据转换技术，然后在训练期间尝试了几种不同的算法，以及这些算法的超参数值的多种组合。在找到好的候选模型（即我们认为可能满足我们的业务目标和指标）之前，这些步骤可能需要花费许多天或几周的时间。AutoML自动快速运行许多试错作业——通常比人类能够做到的更快——并评估结果是否符合期望的指标阈值。不符合期望标准的结果不会被选为候选模型，AutoML将继续尝试其他选项，直到找到合适的候选模型，或者达到某些其他阈值，例如所有选项都已耗尽。
- en: How can you start using AutoML? Some of the AI services we discussed in the
    previous sections in this chapter already use AutoML in order to train the models
    that you access via those APIs. Examples include AutoML image, in which you can
    get insights from object detection and image classification, AutoML Video for
    streaming video analysis, AutoML Text to understand the structure, meaning, and
    sentiment of text, AutoML Translation to translate between languages, and AutoML
    Forecasting to provide forecasts based on time series data.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 如何开始使用AutoML？在本章前面部分讨论的一些AI服务已经使用AutoML来训练通过这些API访问的模型。例如，AutoML图像，你可以从目标检测和图像分类中获得见解；AutoML视频用于流视频分析；AutoML文本用于理解文本的结构、意义和情感；AutoML翻译用于在不同语言之间进行翻译；以及AutoML预测，它基于时间序列数据提供预测。
- en: Another AutoML use case that we haven’t explored yet is AutoML for tabular data
    (structured data that’s stored in tables). This is a very common format for storing
    business data, as it provides a way to organize information that is easy for humans
    to read and understand. AutoML Tabular supports multiple ML use cases with tabular
    data, such as binary classification, multi-class classification, regression, and
    forecasting.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们尚未探索的AutoML用例是针对表格数据的AutoML（存储在表中的结构化数据）。这是存储业务数据的一种非常常见的格式，因为它提供了一种组织信息的方式，便于人类阅读和理解。AutoML
    Tabular支持多种使用表格数据的ML用例，例如二分类、多分类、回归和预测。
- en: We can use AutoML to automate a lot of the trial and error steps and develop
    candidate models, and then we can customize further from that point if we wish.
    For example, we could use Vertex AI Tabular Workflows, which creates a *glassbox*-managed
    AutoML pipeline that lets us see and interpret each step in the model building
    and deployment process. We can then tweak any steps in the process as we see fit,
    and automate any updates via MLOps pipelines. We will be performing exactly these
    kinds of activities in later chapters of this book.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用AutoML来自动化许多试错步骤并开发候选模型，然后如果我们愿意，我们可以从那个点开始进一步定制。例如，我们可以使用Vertex AI Tabular
    Workflows，它创建一个由*玻璃箱*管理的AutoML管道，让我们能够看到并解释模型构建和部署过程中的每一步。然后我们可以根据需要调整过程中的任何步骤，并通过MLOps管道自动化任何更新。我们将在本书的后续章节中精确执行这些活动。
- en: Next, we’re going to dive deeper into ML model customization, and we will explore
    Vertex AI in more detail, as it can be used to customize every step in a data
    science project.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更深入地探讨ML模型定制，并将更详细地探讨Vertex AI，因为它可以用于定制数据科学项目中的每个步骤。
- en: Google Cloud Vertex AI
  id: totrans-266
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud Vertex AI
- en: 'This is where we start getting to expert-level AI/ML. Think back on all of
    the data science concepts we’ve covered so far in this book; all of the different
    types of ML approaches, algorithms, use cases, and tasks. Vertex AI is where you
    can accomplish all of them, and many more that we will yet explore in this book.
    You can use Vertex AI as your central command center for performing everything
    AI/ML-related. Our ML model lifecycle diagram in *Figure 3**.14* illustrates this
    point graphically. Traditionally, we would expect an AI/ML platform to mainly
    take care of training, evaluating, deploying, and monitoring models. This is what’s
    represented by the blue box on the right-hand side of *Figure 3**.14*, and all
    of these activities are of course supported by Vertex AI. However, with additional
    features such as notebooks and MLOps pipelines, Vertex goes beyond just those
    traditional ML activities, to also enable us to perform all of the tasks in our
    lifecycle, including data exploration and processing, as represented by the light
    blue dashed box on the left-hand side of the following diagram:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们开始进入专家级AI/ML的地方。回想一下我们在本书中迄今为止所涵盖的所有数据科学概念；所有不同类型的ML方法、算法、用例和任务。Vertex AI是你可以完成所有这些的地方，以及我们将在本书中进一步探索的许多其他内容。你可以将Vertex
    AI作为你执行所有AI/ML相关活动的中央指挥中心。我们的ML模型生命周期图*图3.14*以图形方式说明了这一点。传统上，我们会期望一个AI/ML平台主要关注训练、评估、部署和监控模型。这由*图3.14*右侧的蓝色框表示，所有这些活动当然都由Vertex
    AI支持。然而，通过额外的功能，如笔记本和MLOps管道，Vertex不仅超越了传统的ML活动，还使我们能够执行生命周期中的所有任务，包括数据探索和处理，正如以下图中左侧的浅蓝色虚线框所示：
- en: '![Figure 3.14: ML model lifecycle with Vertex AI](img/B18143_03_14.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![图3.14：使用Vertex AI的ML模型生命周期](img/B18143_03_14.jpg)'
- en: 'Figure 3.14: ML model lifecycle with Vertex AI'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14：使用Vertex AI的ML模型生命周期
- en: Let’s take a look at Vertex AI’s features in a bit more detail. Starting with
    the basics, we can use Vertex AI’s Deep Learning VM Images to instantiate a VM
    image containing the most popular AI frameworks on a Compute Engine instance,
    or we can use Vertex AI Deep Learning Containers to quickly build and deploy models
    in a portable and consistent containerized environment.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地了解一下Vertex AI的功能。从基础开始，我们可以使用Vertex AI的深度学习VM镜像在Compute Engine实例上实例化包含最流行AI框架的VM镜像，或者我们可以使用Vertex
    AI深度学习容器在可移植和一致的容器化环境中快速构建和部署模型。
- en: The VMs and container images provide building blocks on which we can develop
    customized ML workloads, but we can also use Vertex AI in other ways to perform
    and manage all of the steps in our model development lifecycle. We can use Jupyter
    notebooks to explore data and experiment with each of the steps in the process,
    while Vertex AI Data Labeling enables us to get highly accurate labels from human
    labelers for creating better supervised ML models, and Vertex AI Feature Store
    provides a fully managed feature repository for serving, sharing, and reusing
    ML features. Vertex AI Training provides pre-built algorithms and allows users
    to bring their custom code to train models in a fully managed training service
    for greater flexibility and customization, or for users running training on-premises
    or in another cloud environment. Vertex AI Vizier automates all of our hyperparameter
    tuning jobs for us, finding an optimal set of hyperparameter values for us, and
    saving us from having to do a lot of painstaking work manually! Optimized hyperparameter
    values lead to more accurate and more efficient models.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 虚拟机和容器镜像为我们提供了构建块，基于这些构建块我们可以开发定制的ML工作负载，但我们也可以以其他方式使用Vertex AI来执行和管理模型开发生命周期中的所有步骤。我们可以使用Jupyter笔记本来探索数据并实验过程中每个步骤，而Vertex
    AI Data Labeling则使我们能够从人工标注者那里获得高度准确的标签，以创建更好的监督式ML模型，Vertex AI Feature Store则提供了一个完全管理的特征存储库，用于服务、共享和重用ML特征。Vertex
    AI Training提供了预构建的算法，并允许用户将自定义代码带入完全管理的训练服务中，以实现更大的灵活性和定制化，或者对于在本地或另一个云环境中运行训练的用户。Vertex
    AI Vizier为我们自动化所有超参数调优任务，为我们找到最优的超参数值集，从而节省我们大量繁琐的手动工作！优化的超参数值可以导致更准确和更高效的模型。
- en: When we need to deploy our models, we can use Vertex AI Predictions, which will
    host our models on infrastructure that’s managed by Google, which auto-scales
    to meet our models’ traffic needs. It can host our models for batch or online
    use cases, and it offers a unified framework to deploy custom models built on
    any framework, such as TensorFlow, PyTorch, scikit-learn, or XGB, as well as BigQuery
    ML and AutoML models, and on a broad range of machine types and GPUs. After deployment,
    we can use Vertex AI Model Monitoring to provide automated alerts for data drift,
    concept drift, or other model performance incidents that may require supervision.
    We can then automate the entire process as MLOps pipelines by using Vertex Pipelines,
    which allows us to trigger retraining of our models when needed, and to manage
    updates to our models in a version-controlled way. We can also use Vertex AI TensorBoard
    to visualize ML experiment outcomes and compare models and model versions against
    each other to easily identify the best-performing models. This visualization and
    tracking tool for ML experimentation includes model graphs that display images,
    text, and audio data.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们需要部署我们的模型时，我们可以使用 Vertex AI Predictions，它将在由 Google 管理的基础设施上托管我们的模型，该基础设施会自动扩展以满足我们模型的流量需求。它可以托管我们的模型用于批量或在线用例，并提供一个统一的框架来部署基于任何框架（如
    TensorFlow、PyTorch、scikit-learn 或 XGB）的定制模型，以及 BigQuery ML 和 AutoML 模型，并在广泛的机器类型和
    GPU 上运行。部署后，我们可以使用 Vertex AI 模型监控来提供针对数据漂移、概念漂移或其他可能需要监督的模型性能事件的自动警报。然后，我们可以通过使用
    Vertex Pipelines 自动化整个过程，作为 MLOps 管道，它允许我们在需要时触发模型的重新训练，并以版本控制的方式管理我们模型的更新。我们还可以使用
    Vertex AI TensorBoard 来可视化 ML 实验结果，并比较模型和模型版本，以轻松识别性能最佳的模型。这个用于 ML 实验的视觉化和跟踪工具包括显示图像、文本和音频数据的模型图。
- en: What’s really great is that we can perform and manage all of these activities
    from Vertex AI Workbench, which is a Jupyter-based fully managed, scalable, enterprise-ready
    compute infrastructure with security controls and user management capabilities.
    All the while, the lineage details of our models as they progress through every
    step in the process can be tracked by Vertex Experiments and the Vertex ML Metadata
    service, which provides artifact, lineage, and execution tracking for ML workflows,
    with an easy-to-use Python SDK.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 真正令人兴奋的是，我们可以从 Vertex AI Workbench 中执行和管理所有这些活动，这是一个基于 Jupyter 的完全托管、可扩展、企业级计算基础设施，具有安全控制和用户管理功能。在此过程中，我们的模型在每一步进展中的谱系细节都可以通过
    Vertex Experiments 和 Vertex ML 元数据服务进行跟踪，这两个服务为 ML 工作流程提供易于使用的 Python SDK，以实现工件、谱系和执行跟踪。
- en: Vertex AI provides even more functionality beyond all of the features we mentioned
    previously, such as Vertex AI Matching Engine, which is a massively scalable,
    low latency, and cost-efficient vector similarity matching service. Vertex AI
    Neural Architecture Search enables us to build new model architectures targeting
    application-specific needs and optimize our existing model architectures for latency,
    memory, and power, in an automated service, and we can use Vertex Explainable
    AI to understand and build trust in our model predictions with actionable explanations
    integrated into Vertex AI Prediction, AutoML Tables, and Vertex AI Workbench.
    Explainable AI provides detailed model evaluation metrics and feature attributions,
    indicating how important each input feature is to our predictions.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 提供了比我们之前提到的所有功能更多的功能，例如 Vertex AI 匹配引擎，这是一个大规模可扩展、低延迟且成本效益高的向量相似度匹配服务。Vertex
    AI 神经架构搜索使我们能够针对特定应用需求构建新的模型架构，并在自动化的服务中优化我们现有的模型架构以降低延迟、内存和功耗，我们还可以使用 Vertex
    可解释 AI 来理解并建立对模型预测的信任，通过 Vertex AI 预测、AutoML 表格和 Vertex AI Workbench 中的可操作解释来实现。可解释
    AI 提供详细的模型评估指标和特征归因，表明每个输入特征对我们预测的重要性。
- en: Standard industry tools on Google Cloud
  id: totrans-275
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Google Cloud 上的标准行业工具
- en: In addition to Google Cloud’s own data science tools that we’ve been describing
    so far in this chapter, you can also use other data science tools such as open
    source frameworks or other popular industry solutions. There are lots of great
    libraries out there that make it easy to perform the various tasks in the model
    development life cycle. When it comes to data exploration and processing, for
    example, the beloved pandas library is a staple of any ML and data analysis course.
    You can use it for handling missing data, slicing, subsetting, reshaping, merging,
    and joining datasets. Matplotlib is right up there with pandas for data exploration
    as it allows you to visualize your data via customizable and interactive plots
    and charts that can be exported into various file formats. NumPy allows you to
    easily manipulate and play around with the kinds of *n*-dimensional arrays and
    vectors we find in so many ML implementations. Learning NumPy also sets you up
    to start using frameworks such as scikit-Learn, TensorFlow, and PyTorch.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在本章中我们已经描述的Google Cloud自家的数据科学工具之外，您还可以使用其他数据科学工具，例如开源框架或其他流行的行业解决方案。有许多优秀的库可以轻松完成模型开发生命周期中的各种任务。例如，在数据探索和处理方面，备受喜爱的pandas库是任何机器学习和数据分析课程的必备工具。您可以使用它来处理缺失数据、切片、子集、重塑、合并和连接数据集。Matplotlib在数据探索方面与pandas不相上下，因为它允许您通过可定制的交互式图表和图形来可视化数据，这些图表和图形可以导出为各种文件格式。NumPy允许您轻松地操作和玩转我们在许多机器学习实现中发现的*n*-维数组和向量。学习NumPy也为您使用scikit-Learn、TensorFlow和PyTorch等框架奠定了基础。
- en: Speaking of scikit-learn, it is as much of a staple in any machine learning
    course as pandas. If you take an ML course, you will almost certainly use scikit-learn
    at some point in your learning process, and for good reason; it’s a framework
    that’s easy to use and understand and contains lots of built-in algorithms and
    datasets that you can use to implement ML workloads. And, it’s more than just
    an easy framework for learning purposes; many companies also use scikit-learn
    for their production ML workloads.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 说到scikit-learn，它在任何机器学习课程中的地位与pandas不相上下。如果您参加机器学习课程，您在学习过程中几乎肯定会用到scikit-learn，这是有充分理由的；它是一个易于使用和理解的框架，包含大量内置算法和数据集，您可以使用它们来实施机器学习工作负载。而且，它不仅仅是一个用于学习的简单框架；许多公司也使用scikit-learn来处理他们的生产机器学习工作负载。
- en: While we’re still on the topic of general-purpose ML frameworks, the next framework
    we’ll discuss is the wildly popular TensorFlow, which was originally created by
    Google before they open sourced it, and is therefore very well supported on Google
    Cloud. TensorFlow can be used for everything from processing data to NLP and computer
    vision. You can use it to train, deploy, and serve models, and with **TensorFlow
    Extended** (**TFX**), you can implement end-to-end MLOps pipelines to automate
    all of those steps. We’ll certainly be exploring TensorFlow in more detail in
    this book, as well as Keras, which is an API that provides access to TensorFlow
    via a Python interface that’s popular for its ease of use, and advertises itself
    as “*an API designed for human beings,* *not machines*.”
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们还在讨论通用机器学习框架的话题，但接下来我们将讨论的是广受欢迎的TensorFlow，它最初是由Google创建的，在开源之前，因此在Google
    Cloud上得到了非常好的支持。TensorFlow可以用于从数据处理到自然语言处理和计算机视觉的各个方面。您可以使用它来训练、部署和提供模型，并且通过**TensorFlow
    Extended**（**TFX**），您可以实现端到端的MLOps管道来自动化所有这些步骤。我们当然会在本书中更详细地探讨TensorFlow，以及Keras，这是一个通过Python接口提供对TensorFlow访问的API，因其易用性而受到欢迎，并自称是“*为人类设计的API，*
    *而不是为机器*。”
- en: We’ll round out our discussion of general-purpose ML frameworks with PyTorch,
    which was originally developed by Facebook (Meta AI) and is now open sourced under
    the Linux Foundation. PyTorch has been rapidly gaining popularity in recent years,
    especially among Python developers, and it has become a very widely used framework
    in addition to TensorFlow. In this book, we’re not going to get into the argument
    of which framework is better. There are staunch supporters on each side of this
    debate, and if you Google “TensorFlow versus PyTorch,” you’ll find no shortage
    of websites and forums highlighting how one is better than the other for particular
    types of use cases. We will also be using PyTorch in later chapters of this book.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用PyTorch来结束对通用机器学习框架的讨论，PyTorch最初由Facebook（Meta AI）开发，现在在Linux基金会下开源。PyTorch近年来迅速获得人气，尤其是在Python开发者中，它已经成为除了TensorFlow之外一个非常广泛使用的框架。在这本书中，我们不会涉及哪个框架更好的争论。每个阵营都有坚定的支持者，如果你在Google上搜索“TensorFlow与PyTorch”，你会找到许多网站和论坛强调其中一个在特定类型的用例中比另一个更好。我们还将在这本书的后续章节中使用PyTorch。
- en: Switching gears from general-purpose ML frameworks to more specialized frameworks,
    you might want to use something such as OpenCV for computer vision workloads,
    or SpaCy for NLP. OpenCV has a broad selection of algorithms for applying ML and
    deep learning to images and video content, to perform many of the tasks we discussed
    earlier in this chapter, such as object recognition, and tracking objects and
    actions through video frames. SpaCy has lots of pre-trained word embeddings and
    pipelines supporting multiple languages, and it supports custom models written
    in TensorFlow and PyTorch and lots of Python packages for NLP.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 从通用机器学习框架转向更专业的框架，你可能希望使用像OpenCV这样的工具来处理计算机视觉工作负载，或者使用SpaCy来处理自然语言处理。OpenCV提供了广泛的选择，可以将机器学习和深度学习应用于图像和视频内容，以执行我们在本章前面讨论的许多任务，例如物体识别，以及通过视频帧跟踪物体和动作。SpaCy拥有大量的预训练词嵌入和多种语言的流水线，它还支持使用TensorFlow和PyTorch编写的自定义模型，以及大量的Python
    NLP包。
- en: The good news is that all of the tools and frameworks we’ve discussed in this
    section can easily be used on Google Cloud. In addition to open source tools,
    there are many popular third-party data science solutions that can be used on
    Google Cloud, providing flexibility for people to use whatever tools they prefer
    for the objectives they want to achieve. We’ve already talked about using Spark
    on Google Cloud through services such as Dataproc and Vertex AI, and you can use
    third-party Spark offerings such as Databricks via the Google Cloud Marketplace.
    The marketplace allows you to find thousands of solutions that run on Google Cloud,
    including deep learning solutions from companies such as Hugging Face.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，我们在这个部分讨论的所有工具和框架都可以轻松地在Google Cloud上使用。除了开源工具之外，还有许多流行的第三方数据科学解决方案可以在Google
    Cloud上使用，为人们提供灵活性，让他们可以使用他们偏好的工具来实现他们想要达到的目标。我们已经讨论了通过Dataproc和Vertex AI等服务在Google
    Cloud上使用Spark，你也可以通过Google Cloud Marketplace使用第三方Spark产品，如Databricks。市场允许你找到在Google
    Cloud上运行的数千种解决方案，包括来自Hugging Face等公司的深度学习解决方案。
- en: With all of these tools and all of the services provided by Google Cloud, you
    might wonder how to choose the right tool for your data science workloads. Let’s
    take a look at that discussion in more detail in the next section.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在拥有所有这些工具以及Google Cloud提供的服务的情况下，你可能想知道如何为你的数据科学工作负载选择合适的工具。让我们在下一节更详细地探讨这个问题。
- en: Choosing the right tool for the job
  id: totrans-283
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择合适工具的工作
- en: Your choice of data processing tools will depend heavily on what kind of data
    processing tasks you need to accomplish. If you have a bunch of raw data that
    you need to transform in bulk, as in an ETL/ELT task, Data Fusion would be a good
    place to start your assessment, whereas if you want to perform relatively simple
    transformations using SQL syntax, then start with BigQuery, and if you want to
    visualize and transform data via an easy-to-use GUI, then go with Dataprep. If
    you prefer to stick to using open source tools, then you might want to use something
    like pandas or Spark. We discussed pandas being a good starting point for people
    who are beginning to learn about data exploration and preprocessing, and how it’s
    also more than an educational tool. pandas is really great for initial data exploration
    and data processing at a moderate scale. However, for large-scale data processing
    projects, Spark’s highly parallelized functionality will be a lot more efficient,
    and if you want to use it without managing the infrastructure yourself, then you
    can run it on Dataproc. Dataflow is the recommended choice for streaming data,
    and it has the additional benefit of also working well for batch data processing
    with its unified programming model.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 你选择的数据处理工具将主要取决于你需要完成哪种数据处理任务。如果你有一大堆需要批量转换的原始数据，就像ETL/ELT任务一样，那么数据融合将是开始评估的好地方；而如果你想要使用SQL语法执行相对简单的转换，那么从BigQuery开始；如果你想要通过易于使用的GUI可视化并转换数据，那么选择Dataprep。如果你更喜欢使用开源工具，那么你可能想使用像pandas或Spark这样的工具。我们讨论了pandas对于刚开始学习数据探索和预处理的人来说是一个很好的起点，以及它不仅仅是一个教育工具。pandas对于初始数据探索和中等规模的数据处理来说非常好。然而，对于大规模数据处理项目，Spark高度并行化的功能将更加高效，如果你不想自己管理基础设施，那么你可以在Dataproc上运行它。Dataflow是推荐用于流数据的选项，并且它还有一个额外的好处，就是它的统一编程模型也适用于批量数据处理。
- en: When it comes to choosing a tool for AI/ML workloads, the decision may be a
    bit more straightforward. If you or your company has a pre-existing affinity for
    a specific third party such as Hugging Face, then you may be guided in that direction.
    The general best practice is that you should start with the highest level of abstraction
    available to you because then you won’t have to spend a lot of time and effort
    building and maintaining something that is already available as a service for
    you to use. If that does not meet your needs for any reason, such as specific
    business requirements that require some level of customization, then move to a
    solution that provides more control over how the workload is implemented. For
    example, if you’re building an application that needs to include some kind of
    NLP functionality, start by evaluating the Google Cloud NLP API. If, for any reason,
    you cannot achieve your desired objective with that solution, move on to evaluating
    the use of AutoML to automate training a model on your specific data. If your
    objective is still not met by the models created during that process, then it’s
    time to increase your level of customization, and potentially use Vertex AI to
    build a completely customized model.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到为AI/ML工作负载选择工具时，决策可能更为直接。如果你或你的公司对某个特定第三方（如Hugging Face）有先前的偏好，那么你可能会被引导到那个方向。一般最佳实践是，你应该从你可用的高级抽象级别开始，因为这样你就不必花费大量时间和精力去构建和维护那些已经作为服务可供你使用的东西。如果这不能满足你的任何需求，比如需要一定程度的定制化的特定业务需求，那么就转向一个提供更多控制工作负载实施方式的解决方案。例如，如果你正在构建一个需要包含某种NLP功能的应用程序，那么首先评估Google
    Cloud NLP API。如果出于任何原因，你无法通过该解决方案实现你的预期目标，那么就转向评估使用AutoML来自动化在特定数据上训练模型。如果你的目标仍然没有通过该过程中创建的模型得到满足，那么是时候提高你的定制化水平，并可能使用Vertex
    AI来构建一个完全定制的模型。
- en: Another very important factor in your decision process is your budget. Services
    that do more work for you – such as the higher-level AI services – may cost more
    than the lower-level services, but it’s very important to factor in how much you
    are paying your employees to manage infrastructure and perform the tasks that
    could be performed on your behalf by the higher-level services.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的决策过程中，另一个非常重要的因素是你的预算。为你做更多工作的服务（如高级AI服务）可能比低级服务成本更高，但非常重要的一点是要考虑你支付给员工多少来管理基础设施和执行那些可以由高级服务代表你执行的任务。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we covered all of the foundational and primary services that
    you can use for implementing AI/ML workloads on Google Cloud. We started with
    the basic services such as Google Compute Engine, and Google Cloud networking
    services, upon which all other services are built. We then explored how you could
    use those services to set up connectivity with systems outside of Google Cloud.
    Next, we discussed the services that you can use to import or transfer data to
    Google Cloud and the various storage systems that you can use for storing that
    data in the cloud. Having covered the primary storage systems, we moved on to
    discuss Google Cloud’s data management and data processing services. The final
    step in our journey was to understand all of the different AI/ML services that
    exist in Google Cloud.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们涵盖了所有可用于在谷歌云上实施 AI/ML 工作负载的基础和主要服务。我们首先介绍了基本服务，如谷歌计算引擎和谷歌云网络服务，所有其他服务都是基于这些服务构建的。然后，我们探讨了如何使用这些服务与谷歌云之外的系统建立连接。接下来，我们讨论了可用于将数据导入或传输到谷歌云的服务以及可用于在云中存储数据的各种存储系统。在介绍了主要的存储系统之后，我们转而讨论谷歌云的数据管理和数据处理服务。我们旅程的最后一站是了解谷歌云中存在的所有不同的
    AI/ML 服务。
- en: At this point, you have already learned a lot, and with this knowledge, you
    could now have an intelligent discussion about AI/ML and Google Cloud. This is
    a significant achievement because people spend a very long time trying to learn
    about AI/ML, and a very long time trying to learn about Google Cloud. Right now,
    you know more than a lot of people out there, and you should feel proud and give
    yourself a pat on the back for coming this far.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学到了很多，凭借这些知识，你现在可以就 AI/ML 和谷歌云进行有见地的讨论。这是一个重大的成就，因为人们花费了很长时间来学习 AI/ML，也花费了很长时间来学习谷歌云。现在，你比很多人知道的都多，你应该感到自豪，并为走这么远而给自己鼓掌。
- en: This concludes the *Basics* part of our book, and all of the knowledge you have
    gained in the previous chapters will form the basis of what you will learn in
    the rest of this book. In the next chapter and beyond, we will start performing
    hands-on activities, diving deeper into the services and concepts we have covered
    thus far, and actually start to build data science workloads on Google Cloud!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们书籍的“基础知识”部分的结束，你在前几章中获得的所有知识将成为你在这本书剩余部分所学内容的基础。在下一章以及之后，我们将开始进行动手活动，更深入地了解我们迄今为止所涵盖的服务和概念，并开始在谷歌云上构建数据科学工作负载！
