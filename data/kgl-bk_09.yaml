- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Modeling for Tabular Competitions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为表格竞赛建模
- en: Until 2017, there was no need to distinguish too much between competition types
    and, since the vast majority of competitions were based on tabular data, you could
    not even find mention of “tabular competitions” on Kaggle forums. Suddenly, something
    changed. After a relative shortage of competitions (see [https://www.kaggle.com/general/49904](https://www.kaggle.com/general/49904)),
    deep learning competitions took the upper hand and tabular competitions became
    rarer, disappointing many. They became so rare that Kaggle recently had to launch
    a series of tabular competitions based on synthetic data. What happened?
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 直到2017年，没有必要在竞赛类型之间区分太多，由于绝大多数竞赛都是基于表格数据，你甚至找不到Kaggle论坛上关于“表格竞赛”的提及。突然，一切都变了。在相对缺乏竞赛（见[https://www.kaggle.com/general/49904](https://www.kaggle.com/general/49904)）之后，深度学习竞赛占据了上风，表格竞赛变得更为罕见，让许多人感到失望。它们变得如此罕见，以至于Kaggle最近不得不基于合成数据推出一系列表格竞赛。发生了什么？
- en: By 2017-2018, data science had grown to full maturity and many companies had
    initiated their data journeys. Data science was still a hot topic, but no longer
    such an uncommon one. Solutions to problems similar to those that had populated
    Kaggle for years at the time had become standard practice in many companies. Under
    these circumstances, sponsors were less motivated to launch external tabular competitions,
    since they were already dealing with the same problems internally. By contrast,
    deep learning is still a much-undiscovered domain and will continue to be for
    a long time, so it makes sense to start competitions to challenge the state of
    the art and see if something new emerges.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到2017-2018年，数据科学已经发展到成熟阶段，许多公司已经开始了他们的数据之旅。数据科学仍然是一个热门话题，但不再是那么罕见。与当时Kaggle上充斥的问题类似的问题解决方案已经成为了许多公司的标准实践。在这种情况下，赞助商不太可能启动外部表格竞赛，因为他们已经在内部处理相同的问题。相比之下，深度学习仍然是一个未被充分探索的领域，并且将在很长时间内继续如此，因此开始竞赛来挑战现状并看看是否会出现新的东西是有意义的。
- en: In this chapter, we will discuss tabular competitions. We will touch on some
    famous historical ones and also focus on the more recent reality of the Tabular
    Playground Series, because tabular problems are standard practice for the majority
    of data scientists around and there really is a lot to learn from Kaggle. We will
    start by discussing **exploratory data analysis** (**EDA**) and **feature engineering**,
    two common activities in these competitions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论表格竞赛。我们将涉及一些著名的历史性竞赛，并专注于更近期的表格游乐场系列，因为表格问题是大多数数据科学家标准实践的一部分，而且从Kaggle中确实有很多东西可以学习。我们将从讨论**探索性数据分析**（EDA）和**特征工程**开始，这两者是这些竞赛中的常见活动。
- en: After presenting key strategies for feature engineering, we will expand to many
    related topics, such as categorical encoding, feature selection, target transformations,
    and pseudo-labeling. We will end by touching on deep learning methodologies for
    tabular data, presenting a few specialized deep neural networks such as TabNet
    and illustrating a denoising autoencoder. We will explain why autoencoders have
    become so relevant for recent Kaggle competitions while still being marginal in
    real-world applications.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍特征工程的关键策略之后，我们将扩展到许多相关主题，例如分类编码、特征选择、目标变换和伪标签。我们将以讨论表格数据的深度学习方法结束，介绍一些专门的深度神经网络，如TabNet，并展示一个降噪自编码器。我们将解释为什么自编码器在最近的Kaggle竞赛中变得如此相关，同时在现实世界的应用中仍然处于边缘地位。
- en: 'We will cover:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将涵盖：
- en: The Tabular Playground Series
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格游乐场系列
- en: Setting a random state for reproducibility
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置随机状态以实现可重复性
- en: The importance of EDA
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: EDA的重要性
- en: Reducing the size of your data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减小你的数据大小
- en: Applying feature engineering
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用特征工程
- en: Pseudo-labeling
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 伪标签
- en: Denoising with autoencoders
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用自编码器进行降噪
- en: Neural networks for tabular competitions
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表格竞赛中的神经网络
- en: The chapter won’t cover every topic related to tabular competitions, but you
    can easily find this in many other books since they are at the core of data science.
    What this chapter will do is present a range of special techniques and approaches
    that characterize tabular competitions on Kaggle and that you won’t easily find
    elsewhere, except on Kaggle forums.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章不会涵盖与表格竞赛相关的所有主题，但你可以在许多其他书籍中轻松找到这些主题，因为它们是数据科学的核心。本章将做的是展示一系列特殊技术和方法，这些技术和方法表征了Kaggle上的表格竞赛，而且你不太可能在其他地方轻易找到，除非是在Kaggle论坛上。
- en: The Tabular Playground Series
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 表格游乐场系列
- en: Due to the large demand for tabular problems, Kaggle staff started an experiment
    in 2021, launching a monthly contest called the Tabular Playground Series. The
    contests were based on synthetic datasets that replicated public data or data
    from previous competitions. The synthetic data was created thanks to a deep learning
    generative network called **CTGAN**.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对表格问题的巨大需求，Kaggle 员工在 2021 年开始了一项实验，推出了一项名为“表格游乐场系列”的月度比赛。这些比赛基于复制公共数据或先前比赛数据的合成数据集。这些合成数据是通过名为
    **CTGAN** 的深度学习生成网络创建的。
- en: You can find the CTGAN code at [https://github.com/sdv-dev/CTGAN](https://github.com/sdv-dev/CTGAN).
    There’s also a relevant paper explaining how it works by modeling the probability
    distribution of rows in tabular data and then generating realistic synthetic data
    (see [https://arxiv.org/pdf/1907.00503v2.pdf](https://arxiv.org/pdf/1907.00503v2.pdf)).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/sdv-dev/CTGAN](https://github.com/sdv-dev/CTGAN)找到 CTGAN
    代码。还有一篇相关的论文解释了它是如何通过模拟表格数据中行的概率分布来工作的，然后生成逼真的合成数据（见[https://arxiv.org/pdf/1907.00503v2.pdf](https://arxiv.org/pdf/1907.00503v2.pdf))。
- en: '*Synthetic Data Vault* ([https://sdv.dev/](https://sdv.dev/)), an MIT initiative,
    created the technology behind CTGAN and quite a number of tools around it. The
    result is a set of open-source software systems built to help enterprises generate
    synthetic data that mimics real data; it can help data scientists to create anonymous
    datasets based on real ones, as well as augment existing ones for modeling purposes.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*合成数据宝库* ([https://sdv.dev/](https://sdv.dev/))，一个麻省理工学院的倡议，创造了 CTGAN 背后的技术以及围绕它的许多工具。结果是建立了一套开源软件系统，旨在帮助企业生成模仿真实数据的合成数据；它可以帮助数据科学家根据真实数据创建匿名数据集，以及为建模目的增强现有数据集。'
- en: 'Kaggle launched 13 fairly successful competitions in 2021, which have attracted
    many Kagglers despite not offering points, medals, or prizes (only some merchandise).
    Here is the 2021 list; you can use it to locate specific problems by type or metric
    and look for related resources such as focused discussions or Notebooks:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 在 2021 年推出了 13 个相当成功的比赛，尽管没有提供积分、奖牌或奖品（只有一些商品），但仍然吸引了众多 Kagglers。以下是
    2021 年的列表；您可以使用它通过类型或指标定位特定问题，并查找相关的资源，如专题讨论或笔记本：
- en: '| **Month** | **Problem** | **Variables** | **Metric** | **Missing data** |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| **月份** | **问题** | **变量** | **指标** | **缺失数据** |'
- en: '| **January 2021** | Regression on an unspecified problem | Numeric | RMSE
    | No |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| **一月 2021** | 对未指定问题的回归 | 数值 | RMSE | 否 |'
- en: '| **February 2021** | Regression predicting the value of an insurance claim
    | Numeric and categorical | RMSE | No |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **二月 2021** | 预测保险索赔价值的回归 | 数值和分类 | RMSE | 否 |'
- en: '| **March 2021** | Binary classification predicting an insurance claim | Numeric
    and categorical | AUC | No |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| **三月 2021** | 预测保险索赔的二分类 | 数值和分类 | AUC | 否 |'
- en: '| **April 2021** | Binary classification on a replica very similar to the original
    Titanic dataset | Numeric and categorical | Accuracy | Yes |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| **四月 2021** | 与原始泰坦尼克数据集非常相似的复制品的二分类 | 数值和分类 | 准确率 | 是 |'
- en: '| **May 2021** | Multiclass classification predicting the category on an e-commerce
    product given various attributes about the listing | Categorical | Multiclass
    LogLoss | No |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| **五月 2021** | 预测电子商务产品类别的多分类，基于列表的各种属性 | 分类 | 多分类 LogLoss | 否 |'
- en: '| **June 2021** | Multiclass classification predicting the category on an e-commerce
    product given various attributes about the listing | Numeric and categorical |
    Multiclass LogLoss | No |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| **六月 2021** | 预测电子商务产品类别的多分类，基于列表的各种属性 | 数值和分类 | 多分类 LogLoss | 否 |'
- en: '| **July 2021** | Multiple regression predicting air pollution in a city via
    various input sensor values (for example, a time series) | Numeric, time | RMSLE
    | Yes |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| **七月 2021** | 通过各种输入传感器值（例如，时间序列）预测城市空气污染的多元回归 | 数值，时间 | RMSLE | 是 |'
- en: '| **August 2021** | Regression calculating the loss associated with a loan
    default | Numeric | RMSE | No |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| **八月 2021** | 计算与贷款违约相关的损失的回归 | 数值 | RMSE | 否 |'
- en: '| **30 Days of ML** | Regression on the value of an insurance claim | Numeric
    and categorical | RMSE | No |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| **30 天的机器学习** | 保险索赔价值的回归 | 数值和分类 | RMSE | 否 |'
- en: '| **September 2021** | Binary classification predicting whether a claim will
    be made on an insurance policy | Numeric | AUC | Yes |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| **九月 2021** | 预测是否会在保险单上提出索赔的二分类 | 数值 | AUC | 是 |'
- en: '| **October 2021** | Binary classification predicting the biological response
    of molecules given various chemical properties | Numeric and categorical | AUC
    | No |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| **2021年10月** | 二元分类预测给定各种化学性质的分子的生物反应 | 数值和分类 | AUC | 否 |'
- en: '| **November 2021** | Binary classification identifying spam emails via various
    features extracted from the email | Numeric | AUC | No |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| **2021年11月** | 通过从电子邮件中提取的各种特征识别垃圾邮件的二进制分类 | 数值 | AUC | 否 |'
- en: '| **December 2021** | Multiclass classification based on the original *Forest
    Cover Type Prediction* competition | Numeric and categorical | Multiclass classification
    accuracy | No |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| **2021年12月** | 基于原始*森林覆盖类型预测*竞赛的多类分类 | 数值和分类 | 多类分类准确率 | 否 |'
- en: 'Table 7.1: Tabular Playground Series competitions in 2021'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1：2021年Tabular Playground Series竞赛
- en: 'The Tabular Playground competitions continued in 2022, with even more sophisticated
    and challenging problems:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Tabular Playground竞赛在2022年继续进行，面临更加复杂和具有挑战性的问题：
- en: '| **January 2022** | Forecasting the sales of Kaggle merchandise from two fictitious
    independent store chains | Dates and categorical | Symmetric mean absolute percentage
    error (SMAPE) | No |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| **2022年1月** | 预测Kaggle商品的销售，基于两个虚构的独立商店链 | 日期和分类 | 对称平均绝对百分比误差（SMAPE） | 否
    |'
- en: '| **February 2022** | Classifying 10 different bacteria species using data
    from a genomic analysis technique that contains some data compression and data
    loss | Numeric | Categorization accuracy | No |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| **2022年2月** | 使用包含一些数据压缩和数据丢失的基因组分析技术数据对10种不同的细菌物种进行分类 | 数值 | 分类准确率 | 否 |'
- en: 'Table 7.2: Tabular Playground Series competitions in 2022'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2：2022年Tabular Playground Series竞赛
- en: Much of this chapter has been written by observing the code and discussion that
    emerged in these competitions, instead of analyzing more glorious competitions
    from the past. As we mentioned, we believe that tabular competitions are indeed
    gone for good given the changed professional landscape, and that you will find
    it more useful to read suggestions and hints relating to the present than the
    past.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的大部分内容是通过观察在这些竞赛中出现的代码和讨论来撰写的，而不是分析过去的更辉煌的竞赛。正如我们提到的，我们认为由于专业环境的改变，表格竞赛确实已经永久消失了，您会发现阅读与现在相关的建议和提示比过去更有用。
- en: 'As in other fully fledged competitions with Kaggle points and medals, in tabular
    competitions we recommend you follow a simple, yet very effective, pipeline that
    we have discussed elsewhere in the book:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 正如在其他带有Kaggle积分和奖牌的完整竞赛中一样，在表格竞赛中，我们建议您遵循一个简单但非常有效的流程，我们在本书的其他地方讨论过：
- en: Explorative data analysis (EDA)
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索性数据分析（EDA）
- en: Data preparation
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Modeling (using a cross-validation strategy for model validation)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模（使用交叉验证策略进行模型验证）
- en: Post-processing
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理
- en: Submission
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提交
- en: As a rule, you also have to take care to maintain reproducibility and to save
    all the models (from every fold), the list of the parameters used, all the fold
    predictions, all the out-of-fold predictions, and all predictions from models
    trained on all the data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您还必须注意保持可重复性和保存所有模型（来自每个折叠），使用的参数列表，所有折叠预测，所有出折叠预测，以及训练在所有数据上的模型的所有预测。
- en: 'You should save all this information in a way that makes it easy to recover
    and reconstruct, for instance using appropriate labeling, keeping track of MD5
    hashing values (you can refer to this Stack Overflow answer for details: [https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python](https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python)),
    and tracking the CV scores and leaderboard results from each experiment. Most
    Kagglers do this with simple tools such as `.txt` files or Excel spreadsheets,
    but there exist ways that are more sophisticated, such as using:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该以易于恢复和重建的方式保存所有这些信息，例如使用适当的标签，跟踪MD5散列值（您可以参考此Stack Overflow答案以获取详细信息：[https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python](https://stackoverflow.com/questions/16874598/how-do-i-calculate-the-md5-checksum-of-a-file-in-python)），以及跟踪每个实验的CV分数和排行榜结果。大多数Kagglers使用简单的工具，如`.txt`文件或Excel电子表格来完成这项工作，但存在更复杂的方法，例如使用：
- en: '**DVC** ([https://dvc.org/](https://dvc.org/))'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**DVC** ([https://dvc.org/](https://dvc.org/))'
- en: '**Weights and Biases** ([https://wandb.ai/site](https://wandb.ai/site))'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Weights and Biases** ([https://wandb.ai/site](https://wandb.ai/site))'
- en: '**MLflow** ([https://mlflow.org/](https://mlflow.org/))'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MLflow** ([https://mlflow.org/](https://mlflow.org/))'
- en: '**Neptune** ([https://neptune.ai/experiment-tracking](https://neptune.ai/experiment-tracking))'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Neptune**([https://neptune.ai/experiment-tracking](https://neptune.ai/experiment-tracking))'
- en: In the end, what matters are the results, not the tool you use, so try your
    best to keep order in your experiments and models, even in the heat of a competition.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重要的是结果，而不是你使用的工具，所以尽量在实验和模型中保持秩序，即使在竞赛的激烈竞争中也是如此。
- en: Before we proceed, consider also thinking about the technology that Kaggle used
    to generate the data for these competitions; if you can properly understand how
    the data has been generated, you get an important advantage. In addition, understanding
    how synthetic data works can really have an impact on the way you do data science
    in the real world, because it gives you a way to easily obtain more varied data
    for training.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，也要考虑一下Kaggle用于生成这些竞赛数据的科技；如果你能正确理解数据是如何生成的，这将给你带来重要的优势。此外，理解合成数据的工作原理确实可以影响你在现实世界中做数据科学的方式，因为它为你提供了一种轻松获取更多样化数据用于训练的方法。
- en: 'For instance, let’s take the *Google Brain – Ventilator Pressure Prediction*
    competition ([https://www.kaggle.com/c/ventilator-pressure-prediction](https://www.kaggle.com/c/ventilator-pressure-prediction)).
    In this competition, you had to develop machine learning for mechanical ventilation
    control. Although you could obtain good results by modeling the data provided
    with deep learning, given the synthetic origin of the data, you could also reverse
    engineer its generative process and obtain a top leaderboard result, as *Jun Koda*
    ([https://www.kaggle.com/junkoda](https://www.kaggle.com/junkoda)) did and explains
    in his post: [https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/285278](https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/285278).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们以*Google Brain – 呼吸机压力预测*竞赛([https://www.kaggle.com/c/ventilator-pressure-prediction](https://www.kaggle.com/c/ventilator-pressure-prediction))为例。在这个竞赛中，你必须开发用于机械通气控制的机器学习。虽然你可以通过使用深度学习对提供的数据进行建模来获得良好的结果，但由于数据的合成来源，你也可以逆向工程其生成过程，并获得排行榜上的顶尖结果，正如*Jun
    Koda*([https://www.kaggle.com/junkoda](https://www.kaggle.com/junkoda))所做并在他的帖子中解释的那样：[https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/285278](https://www.kaggle.com/c/ventilator-pressure-prediction/discussion/285278)。
- en: Generating artificial data by yourself and understanding synthetic data has
    never been so easy, as you can verify from this Notebook ([https://www.kaggle.com/lucamassaron/how-to-use-ctgan-to-generate-more-data](https://www.kaggle.com/lucamassaron/how-to-use-ctgan-to-generate-more-data)),
    derived from a Notebook originally coded and tested by *Dariush Bahrami* ([https://www.kaggle.com/dariushbahrami](https://www.kaggle.com/dariushbahrami)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 通过自己生成人工数据和理解合成数据从未如此简单，您可以从这个笔记本([https://www.kaggle.com/lucamassaron/how-to-use-ctgan-to-generate-more-data](https://www.kaggle.com/lucamassaron/how-to-use-ctgan-to-generate-more-data))中验证，这个笔记本最初是由*Dariush
    Bahrami*([https://www.kaggle.com/dariushbahrami](https://www.kaggle.com/dariushbahrami))编写和测试的。
- en: Setting a random state for reproducibility
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置随机状态以实现可重现性
- en: Before we start discussing the steps and models you may use in a tabular competition,
    it will be useful to return to the theme of **reproducibility** we mentioned above.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始讨论在表格竞赛中可能使用的步骤和模型之前，回到我们上面提到的**可重现性**主题将是有用的。
- en: 'In most of the commands in the code you see on Kaggle Notebooks, you will find
    a parameter declaring a number, a **seed**, as the random state. This setting
    is important for the reproducibility of your results. Since many algorithms are
    not deterministic but are based on randomness, by setting a seed you influence
    the behavior of the random generator, making it *predictable* in its randomness:
    the same random seed corresponds to the same sequence of random numbers. In other
    words, it allows you to obtain the same results after every run of the same code.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在你看到的Kaggle笔记本上的大多数命令中，你都会找到一个参数声明一个数字，一个**种子**，作为随机状态。这个设置对你的结果的可重现性很重要。由于许多算法不是确定性的，而是基于随机性，通过设置种子，你影响随机生成器的行为，使其随机性变得*可预测*：相同的随机种子对应相同的随机数序列。换句话说，它允许你在每次运行相同的代码后获得相同的结果。
- en: That is why you find a random seed setting parameter in all machine learning
    algorithms in Scikit-learn as well as in all Scikit-learn-compatible models (for
    instance, XGBoost, LightGBM, and CatBoost, to name the most popular ones).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正因如此，你在Scikit-learn中的所有机器学习算法以及所有与Scikit-learn兼容的模型（例如，XGBoost、LightGBM和CatBoost，仅举一些最受欢迎的）中都会找到一个随机种子设置参数。
- en: Reproducibility of results is important in real-world projects as well as in
    Kaggle competitions. In the real world, having a reproducible model allows for
    better tracking of model development and consistency. In Kaggle competitions,
    reproducibility helps in testing hypotheses better because you are controlling
    any source of variation in your models. For instance, if you created a new feature,
    putting it into a reproducible pipeline will help you understand if the feature
    is advantageous or not. You will be sure that any improvement or deterioration
    in the model can be attributed only to the feature, and not to the effects of
    some random process that has changed since the last time you ran the model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界项目和Kaggle比赛中，结果的可重复性都同样重要。在现实世界中，拥有一个可重复的模型可以更好地跟踪模型开发和一致性。在Kaggle比赛中，可重复性有助于更好地测试假设，因为你正在控制模型中的任何变化来源。例如，如果你创建了一个新特征，将其放入可重复的管道中，将有助于你了解该特征是否有优势。你可以确信模型中的任何改进或恶化只能归因于该特征，而不是由于自上次运行模型以来某些随机过程的变化。
- en: 'Again, reproducibility can be used to your advantage when dealing with public
    Notebooks. Most often, these Notebooks will have a fixed seed that could be 0,
    1, or 42\. The value 42 is quite popular because it is a reference to Douglas
    Adam’s *The Hitchhiker’s Guide to the Galaxy*, in which it is the “Answer to the
    Ultimate Question of Life, the Universe, and Everything,” calculated by an enormous
    supercomputer named Deep Thought over a period of 7.5 million years. Now, if everyone
    in a competition is using the same random seed, it could have a double effect:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，在处理公共笔记本时，可重复性可以为你带来优势。这些笔记本通常会有一个固定的种子值，可能是0、1或42。数字42之所以流行，是因为它是对道格拉斯·亚当斯的《银河系漫游指南》的引用，在那里它是“生命、宇宙和一切的终极问题的答案”，由名为“深思想”的超级计算机在7500万年内计算得出。现在，如果比赛中的每个人都使用相同的随机种子，可能会产生双重效果：
- en: The random seed might be working too well with the public leaderboard, which
    means overfitting
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机种子可能与公共排行榜配合得过于完美，这意味着过度拟合
- en: A lot of Kagglers will produce similar results that will influence their standings
    in the private leaderboard in the same way
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多Kagglers会产生类似的结果，这将以相同的方式影响他们在私人排行榜上的排名
- en: By changing the random seed, you are avoiding overfitting and also breaking
    rank; in other words, you are getting different results from everyone else, which
    could put you at an advantage in the end. In addition, if you end up winning a
    Kaggle competition, you need to demonstrate how your models produced the winning
    submission, so it is paramount that everything is completely reproducible if you
    want to obtain your prize quickly.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 通过改变随机种子，你正在避免过度拟合并打破排名；换句话说，你得到了与其他人不同的结果，这最终可能让你处于优势地位。此外，如果你最终赢得了Kaggle比赛，你需要展示你的模型是如何产生获奖提交的，因此，如果你想要快速获得奖金，确保一切完全可重复至关重要。
- en: 'TensorFlow and PyTorch models don’t explicitly use a random seed parameter,
    so it is more challenging to ensure their complete reproducibility. The following
    code snippet, when run, sets the same random seed for TensorFlow and PyTorch models:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和PyTorch模型没有明确使用随机种子参数，因此确保它们的完全可重复性更具挑战性。以下代码片段在运行时为TensorFlow和PyTorch模型设置相同的随机种子：
- en: '[PRE0]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As for Scikit-learn, it is instead advisable to set the random seed directly
    – when it is allowed by the class or the function – using the `random_state` parameter.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 对于Scikit-learn来说，相反建议直接设置随机种子——当类或函数允许时——使用`random_state`参数。
- en: The importance of EDA
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: EDA的重要性
- en: The term **EDA** comes from the work of *John W. Tukey*, one of the most prominent
    exponents of modern statistical methodology. In his 1977 book *Exploratory Data
    Analysis* (hence the acronym EDA), Tukey thinks of EDA as a way to explore data,
    uncover evidence, and develop hypotheses that can later be confirmed by statistical
    tests.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**EDA**（探索性数据分析）这个术语来源于现代统计方法学的杰出代表约翰·W·图基的工作。在他的1977年著作《探索性数据分析》（EDA的缩写），图基认为EDA是一种探索数据、揭示证据并发展可以由统计测试后来证实假设的方法。'
- en: His idea was that how we define statistical hypotheses could be based more on
    observation and reasoning than just sequential tests based on mathematical computations.
    This idea translates well to the world of machine learning because, as we will
    discuss in the next section, data can be improved and pre-digested so that learning
    algorithms can work better and more efficiently.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 他的想法是，我们如何定义统计假设可能更多地基于观察和推理，而不是仅仅基于数学计算的顺序测试。这个想法很好地转化为机器学习领域，因为，正如我们将在下一节讨论的，数据可以被改进和预处理，以便学习算法可以更好地、更有效地工作。
- en: 'In an EDA for a Kaggle competition, you will be looking for:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle竞赛的EDA中，你将寻找：
- en: Missing values and, most importantly, missing value patterns correlated with
    the target.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺失值，更重要的是，与目标相关的缺失值模式。
- en: Skewed numeric variables and their possible transformations.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏斜的数值变量及其可能的转换。
- en: Rare categories in categorical variables that can be grouped together.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以一起分组的分类变量中的罕见类别。
- en: Potential outliers, both univariate and multivariate.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能的异常值，包括单变量和多变量。
- en: Highly correlated (or even duplicated) features. For categorical variables,
    focus on categories that overlap.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度相关（甚至重复）的特征。对于分类变量，关注重叠的类别。
- en: The most predictive features for the problem.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于该问题最有预测性的特征。
- en: You achieve this by several descriptive analyses, graphs, and charts, first
    examining each distinct feature (**univariate analysis**, in statistical terms),
    then matching a couple of variables (**bivariate** analysis, such as in a scatterplot),
    and finally considering more features together at once (a **multivariate** approach).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过几种描述性分析、图表和图表来实现这一点，首先检查每个独特的特征（**单变量分析**，在统计学中），然后匹配几个变量（**双变量分析**，例如在散点图中），最后同时考虑更多特征（**多变量方法**）。
- en: 'If you are feeling lazy or unsure about how and where to start, relying on
    automated strategies initially can help you. For instance, you may find that **AutoViz**
    ([https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz)),
    a popular rapid EDA freeware tool, can save you a lot of time. You can install
    it on your Notebook by running the following command:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感到懒惰或者不确定如何以及从哪里开始，最初依赖自动化策略可能会有所帮助。例如，你可能发现**AutoViz** ([https://github.com/AutoViML/AutoViz](https://github.com/AutoViML/AutoViz))，这是一个流行的快速EDA免费软件工具，可以为你节省大量时间。你可以在笔记本上通过运行以下命令来安装它：
- en: '[PRE1]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can obtain a clearer understanding of what AutoViz can do for you by reading
    this Medium article by *Dan Roth* at [https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad](https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad)
    or browsing a few interesting public Notebooks such as [https://www.kaggle.com/gvyshnya/automating-eda-and-feature-importance-detection](https://www.kaggle.com/gvyshnya/automating-eda-and-feature-importance-detection)
    by *Georgii Vyshnia* ([https://www.kaggle.com/gvyshnya](https://www.kaggle.com/gvyshnya)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过阅读Dan Roth在Medium上发表的这篇文章，你可以更清楚地了解AutoViz能为你做什么，文章链接为[https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad](https://towardsdatascience.com/autoviz-a-new-tool-for-automated-visualization-ec9c1744a6ad)，或者浏览一些有趣的公共笔记本，例如Georgii
    Vyshnia的[https://www.kaggle.com/gvyshnya/automating-eda-and-feature-importance-detection](https://www.kaggle.com/gvyshnya/automating-eda-and-feature-importance-detection)，[https://www.kaggle.com/gvyshnya](https://www.kaggle.com/gvyshnya)。
- en: In the latter link, you will also find references to another tool, **Sweetviz**
    ([https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz)).
    Sweetviz has an overview article and tutorial based on the Titanic dataset, at
    [https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34](https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的链接中，你还可以找到对另一个工具**Sweetviz** ([https://github.com/fbdesignpro/sweetviz](https://github.com/fbdesignpro/sweetviz))的引用。Sweetviz有一篇基于泰坦尼克号数据集的概述文章和教程，可在[https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34](https://towardsdatascience.com/powerful-eda-exploratory-data-analysis-in-just-two-lines-of-code-using-sweetviz-6c943d32f34)找到。
- en: 'Another popular tool that you may find useful using is **Pandas Profiling**
    ([https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling)),
    which is more reliant on classical statistical descriptive statistics and visualization,
    as explained by this article: [https://medium.com/analytics-vidhya/pandas-profiling-5ecd0b977ecd](https://medium.com/analytics-vidhya/pandas-profiling-5ecd0b977ecd).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会发现另一个有用的流行工具是**Pandas Profiling** ([https://github.com/pandas-profiling/pandas-profiling](https://github.com/pandas-profiling/pandas-profiling))，它更依赖于经典的统计描述性统计和可视化，如这篇文章所述：[https://medium.com/analytics-vidhya/pandas-profiling-5ecd0b977ecd](https://medium.com/analytics-vidhya/pandas-profiling-5ecd0b977ecd)。
- en: Waiting for other Kagglers to publish interesting EDA Notebooks could also be
    a solution, so always keep an eye on the Notebooks sections; sometimes, precious
    hints may appear. This should kick-start your modeling phase and help you understand
    the basic dos and don’ts of the competition. However, remember that EDA stops
    being a commodity and becomes an asset for the competition when it is *highly
    specific to the problem at hand*; this is something that you will never find from
    automated solutions and seldom in public Notebooks. You have to do your EDA by
    yourself and gather key, winning insights.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 等待其他Kagglers（数据科学家社区）发布有趣的EDA笔记本也可能是一个解决方案，所以请始终关注笔记本部分；有时，宝贵的提示可能会出现。这应该会启动你的建模阶段，并帮助你了解竞赛的基本规则和禁忌。然而，记住，当EDA对特定问题高度具体时，它就不再是商品，而成为竞赛的资产；这是你在自动化解决方案中永远找不到的，在公开笔记本中也很少见。你必须自己进行EDA并收集关键、获胜的见解。
- en: All things considered, our suggestion is to look into the automated tools a
    bit because they are really easy to learn and run. You will save a lot of time
    that you can instead spend looking at charts and reasoning about possible insights,
    and that will certainly help your competition performance. However, after doing
    that, you need to pick up Matplotlib and Seaborn and try something by yourself
    on not-so-standard plots that depend on the type of data provided and the problem.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些因素，我们的建议是稍微了解一下自动化工具，因为它们真的很容易学习和运行。你可以节省大量的时间，这些时间你可以用来查看图表和推理可能的见解，这无疑会帮助你在竞赛中的表现。然而，在这样做之后，你需要学习Matplotlib和Seaborn，并尝试在不太标准的图表上做一些自己的尝试，这些图表依赖于提供的数据类型和问题。
- en: For example, if you are given a series of measurements performed over time,
    plotting the continuous function based on time is as useful as plotting the single
    recorded points in time, for instance showing different lags between one observation
    and another, a fact that may point to revealing insights for better predictions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你被给了一系列随时间进行的测量数据，基于时间的连续函数的绘图与绘制单个记录的时间点一样有用，例如显示一个观测值与另一个观测值之间的不同滞后，这可能表明揭示了更好的预测的见解。
- en: Dimensionality reduction with t-SNE and UMAP
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用t-SNE和UMAP进行降维
- en: There are many possible plots you can create when doing EDA and it is not our
    intention to list them all here, but there are a couple of dimensionality reduction
    plots that are worth spending a few words on because they can provide as much
    information as very specific and data-tailored charts. These are **t-SNE** ([https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))
    and **UMAP** ([https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行EDA（电子设计自动化）时，你可以创建许多可能的图表，我们并没有意图在这里列出所有，但有一些降维图表值得花点篇幅讨论，因为它们可以提供与非常具体和定制化图表一样多的信息。这些是**t-SNE**
    ([https://lvdmaaten.github.io/tsne/](https://lvdmaaten.github.io/tsne/))和**UMAP**
    ([https://github.com/lmcinnes/umap](https://github.com/lmcinnes/umap))。
- en: t-SNE and UMAP are two techniques, often used by data scientists, that allow
    you to project multivariate data into lower dimensions. They are often used to
    represent complex sets of data in two dimensions. 2-D UMAP and t-SNE plots can
    reveal the presence of outliers and relevant clusters for your data problem.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE和UMAP是两种数据科学家经常使用的技术，它们允许你将多元数据投影到低维空间。它们通常用于在二维空间中表示复杂的数据集。2-D UMAP和t-SNE图表可以揭示数据问题中异常值和相关的聚类。
- en: In fact, if you can plot the scatter graph of the resulting 2-D projection and
    color it by target value, the plot may give you hints about possible strategies
    for dealing with subgroups.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，如果你能绘制出结果2-D投影的散点图，并按目标值着色，这个图表可能会给你一些关于处理子组的可能策略的提示。
- en: Although it is related to an image competition, a good example of how UMAP and
    t-SNE can help you understand your data better is *Chris Deotte*’s analysis for
    the *SIIM-ISIC Melanoma Classification* competition (see [https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/168028](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/168028)).
    In this example, Chris has related training and test data on the same low-dimensionality
    projections, highlighting portions where only test examples were present.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它与图像竞赛有关，但UMAP和t-SNE如何帮助你更好地理解数据的良好例子是*Chris Deotte*为*SIIM-ISIC黑色素瘤分类*竞赛的分析（见[https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/168028](https://www.kaggle.com/c/siim-isic-melanoma-classification/discussion/168028)）。在这个例子中，Chris将训练数据和测试数据关联到了同一低维投影上，突出了只有测试示例存在的部分。
- en: Though UMAP and t-SNE offer invaluable help in discovering patterns in data
    that are hard to find, you still can use them as features in your modeling efforts.
    An interesting example of this usage was demonstrated in the *Otto Group Product
    Classification Challenge*, where *Mike Kim* used t-SNE projections as training
    features for the competition ([https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14295](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14295)).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管UMAP和t-SNE在发现难以找到的数据模式方面提供了无价的帮助，但你仍然可以将它们用作建模努力中的特征。在*Otto Group产品分类挑战*中，*Mike
    Kim*使用t-SNE投影作为竞赛的训练特征，展示了这种使用的有趣例子（见[https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14295](https://www.kaggle.com/c/otto-group-product-classification-challenge/discussion/14295)）。
- en: As stated by the article *How to t-SNE Effectively* ([https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/)),
    you have to use these techniques properly, because it is easy to spot clusters
    and patterns where there are none. The same warning is valid for UMAP, because
    it can also produce plots that can be misread. Guides such as [https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/)
    offer sound advice on the performance of both UMAP and t-SNE on real-world data,
    providing suggestions and caveats.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如文章*如何有效地使用t-SNE*([https://distill.pub/2016/misread-tsne/](https://distill.pub/2016/misread-tsne/))所述，你必须正确使用这些技术，因为很容易在没有任何聚类和模式的地方发现它们。同样的警告也适用于UMAP，因为它也可以生成可能被误读的图表。例如[https://pair-code.github.io/understanding-umap/](https://pair-code.github.io/understanding-umap/)这样的指南为UMAP和t-SNE在真实世界数据上的性能提供了合理的建议和注意事项。
- en: Despite these dangers, in our experience, these approaches are certainly more
    revealing than the classical methods based on variance restructuring by linear
    combination such as PCA or SVD. Compared to these approaches, UMAP and t-SNE manage
    to reduce the dimensionality extremely, allowing visual charting of the results
    while maintaining the topography of the data. As a side effect, they are much
    slower to fit. However, NVIDIA has released its **RAPIDS** suite ([https://developer.nvidia.com/rapids](https://developer.nvidia.com/rapids))
    based on CUDA, which, using a GPU-powered Notebook or script, returns the results
    of both UMAP and t-SNE in a very reasonable timeframe, allowing their effective
    use as an EDA tool.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在这些危险，但根据我们的经验，这些方法肯定比基于PCA或SVD等线性组合方差重构的经典方法更有揭示性。与这些方法相比，UMAP和t-SNE能够极大地降低维度，同时保持数据的拓扑结构，允许可视化结果。然而，作为副作用，它们的拟合速度要慢得多。不过，NVIDIA已经发布了基于CUDA的**RAPIDS**套件([https://developer.nvidia.com/rapids](https://developer.nvidia.com/rapids))，使用GPU驱动的笔记本或脚本，可以在非常合理的时间内返回UMAP和t-SNE的结果，从而有效地将其用作EDA工具。
- en: 'You can find a useful example of applying both UMAP and t-SNE with a RAPIDS
    implementation and a GPU for data exploration purposes for the *30 Days of ML*
    competition at the following link: [https://www.kaggle.com/lucamassaron/interesting-eda-tsne-umap/](https://www.kaggle.com/lucamassaron/interesting-eda-tsne-umap/).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下链接中找到一个有用的例子，展示了如何使用RAPIDS实现和GPU进行数据探索，以参加*30 Days of ML*竞赛：[https://www.kaggle.com/lucamassaron/interesting-eda-tsne-umap/](https://www.kaggle.com/lucamassaron/interesting-eda-tsne-umap/)。
- en: 'In the figure below, which is the output of the example Notebook above, you
    can see how multiple clusters populate the dataset, but none of them could be
    deemed to reveal a particular relationship with the target:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，这是上述示例笔记本的输出，你可以看到多个聚类如何填充数据集，但没有任何一个可以被认定为与目标有特定的关系：
- en: '![__results___9_0.png](img/B17574_07_01.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![__results___9_0.png](img/B17574_07_01.png)'
- en: 'Figure 7.1: Multiple clusters appearing in a t-SNE plot'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1：t-SNE图中出现的多个聚类
- en: 'In another Notebook ([https://www.kaggle.com/lucamassaron/really-not-missing-at-random](https://www.kaggle.com/lucamassaron/really-not-missing-at-random)),
    the same techniques are applied to the binary indicators for missing samples instead,
    revealing evocative figures that hint at specific and separate areas dominated
    by a certain type of response. Indeed, in that example, missing samples did not
    occur at random and they were quite predictive:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个笔记本([https://www.kaggle.com/lucamassaron/really-not-missing-at-random](https://www.kaggle.com/lucamassaron/really-not-missing-at-random))中，相同的技巧被应用于缺失样本的二进制指标，揭示出一些引人入胜的图表，这些图表暗示了由某种特定类型响应主导的特定和独立区域。实际上，在那个例子中，缺失样本并不是随机出现的，它们具有相当强的预测性：
- en: '![__results___10_0.png](img/B17574_07_02.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![__results___10_0.png](img/B17574_07_02.png)'
- en: 'Figure 7.2: This t-SNE plot easily reveals areas where the positive target
    is predominant'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2：这个t-SNE图很容易揭示出正目标占主导地位的区域
- en: Reducing the size of your data
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 减少数据大小
- en: If you are working directly on Kaggle Notebooks, you will find their limitations
    quite annoying and dealing with them a timesink. One of these limitations is the
    out-of-memory errors that will stop the execution and force you to restart the
    script from the beginning. This is quite common in many competitions. However,
    unlike deep learning competitions based on text or images where you can retrieve
    the data from disk in small batches and have them processed, most of the algorithms
    that work with tabular data require handling all the data in memory.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你直接在Kaggle笔记本上工作，你会发现它们的限制相当令人烦恼，处理它们有时会变成一个耗时的工作。这些限制之一是内存不足错误，这会停止执行并迫使你从脚本开始重新启动。这在许多比赛中相当常见。然而，与基于文本或图像的深度学习比赛不同，在这些比赛中你可以分批从磁盘检索数据并处理它们，而大多数处理表格数据的算法都需要在内存中处理所有数据。
- en: 'The most common situation is when you have uploaded the data from a CSV file
    using Pandas’ `read_csv`, but the DataFrame is too large to be handled for feature
    engineering and machine learning in a Kaggle Notebook. The solution is to compress
    the size of the Pandas DataFrame you are using without losing any information
    (**lossless compression**). This can easily be achieved using the following script
    derived from the work by *Guillaume Martin* (you can find the original Notebook
    here: [https://www.kaggle.com/gemartin/load-data-reduce-memory-usage](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage)).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的情况是，当你使用Pandas的 `read_csv` 从CSV文件上传数据时，但DataFrame太大，无法在Kaggle笔记本中进行特征工程和机器学习。解决方案是在不丢失任何信息的情况下压缩你使用的Pandas
    DataFrame的大小（**无损压缩**）。这可以通过以下脚本轻松实现，该脚本源自 *Guillaume Martin* 的工作（你可以在以下位置找到原始笔记本：[https://www.kaggle.com/gemartin/load-data-reduce-memory-usage](https://www.kaggle.com/gemartin/load-data-reduce-memory-usage))。
- en: '[PRE2]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '*Guillaume Martin* was not the first to propose an idea like this on Kaggle.
    The very first Kaggler with this idea of compressing a Pandas DataFrame was *Arjan
    Groen*, who wrote a reducing function during the Zillow competition ([https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65)).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '*Guillaume Martin* 并不是第一个在Kaggle上提出这种想法的人。第一个有这种压缩Pandas DataFrame想法的Kaggler是
    *Arjan Groen*，他在Zillow比赛中编写了一个减少函数([https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65](https://www.kaggle.com/arjanso/reducing-dataframe-memory-size-by-65))。'
- en: This script leverages the fact that all the numeric features in a dataset reside
    in a specific range of values. Since we have different types of integer and floating-point
    numeric variables in Python, based on the number of bytes they occupy in memory,
    the script compares the range of values found in each feature to the maximum and
    minimum value that each numeric type can accept. This is done in order to set
    the feature to the numeric type that works with its range of values and that requires
    the lowest memory.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本利用了这样一个事实：数据集中的所有数值特征都位于一个特定的值域内。由于Python中有不同类型的整数和浮点数值变量，根据它们在内存中占用的字节数，脚本会将每个特征中找到的值域与每种数值类型可以接受的最大值和最小值进行比较。这样做是为了将特征设置为与其值域相匹配且需要最低内存的数值类型。
- en: The approach works like a breeze on Kaggle Notebooks, but with some caveats.
    Once you have set the best-fitting numeric type for each feature by compression,
    you cannot apply any feature engineering that may result in values exceeding the
    capacity of the set numeric types, because such an operation will produce erroneous
    results. Our suggestion is to apply it after feature engineering or before major
    transformations that do not rescale your existing data. Combining it with the
    garbage collection library `gc` and the `gc.collect()` method will improve the
    memory situation of your Kaggle Notebook.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在Kaggle笔记本上工作得像微风一样，但也有一些注意事项。一旦你通过压缩为每个特征设置了最佳匹配的数值类型，你就不能应用任何可能导致数值超过设定数值类型容量的特征工程，因为这样的操作会产生错误的结果。我们的建议是在特征工程之后或在进行不会重新缩放现有数据的主要转换之前应用它。结合垃圾收集库`gc`和`gc.collect()`方法将改善你的Kaggle笔记本的内存状况。
- en: Another way to reduce the size of your data (among other things) is to use feature
    engineering (in particular, feature selection and data compression).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 减小你的数据大小（以及其他事情）的另一种方式是使用特征工程（特别是特征选择和数据压缩）。
- en: Applying feature engineering
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用特征工程
- en: In real-world projects, what can make the difference between a successful machine
    learning model and a mediocre one is often the data, not the model. When we talk
    about data, the differentiator between bad, good, and excellent data is not just
    the lack of missing values and the reliability of the values (its “quality”),
    or the number of available examples (its “quantity”). In our experience, the real
    differentiator is the informational value of the content itself, which is represented
    by the type of features.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的项目中，能够区分成功机器学习模型和一般模型的往往是数据，而不是模型。当我们谈论数据时，区分糟糕、良好和优秀数据的不同之处不仅仅是缺失值的缺乏和值的可靠性（其“质量”），或者是可用示例的数量（其“数量”）。根据我们的经验，真正的区分因素是内容本身的信息价值，这由特征的类型来表示。
- en: The features are the real clay to mold in a data science project, because they
    contain the information that models use to separate the classes or estimate the
    values. Every model has an expressiveness and an ability to transform features
    into predictions, but if you are lacking on the side of features, no model can
    bootstrap you and offer better predictions. *Models only make apparent the value
    in data. They are not magic in themselves*.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 特征是数据科学项目中真正可以塑造的粘土，因为它们包含了模型用来分离类别或估计值的所需信息。每个模型都有表达性和将特征转换为预测的能力，但如果你在特征方面有所欠缺，没有任何模型能帮助你启动并给出更好的预测。*模型只是使数据中的价值显现出来。它们本身并不具有魔法*。
- en: On Kaggle, apart from the rare competitions where you can look for further data
    to add, all participants have the same data available from the beginning. At that
    point, how you handle the data makes most of the difference. Overlooking the fact
    that you can improve the data you have is a common mistake made by many Kagglers.
    **Feature engineering**, a set of techniques for transforming data into more useful
    information for your models, is invariably the key to performing better in competitions.
    Even the more powerful models you can apply need you to process the data and render
    it into a more understandable form.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kaggle上，除了那些罕见的比赛中你可以寻找更多数据来添加之外，所有参与者从开始就拥有相同的数据。在那个阶段，你如何处理数据就构成了大部分差异。忽视你可以改进现有数据的这一事实是许多Kagglers常犯的一个错误。**特征工程**，一套将数据转换为对模型更有用信息的技术的集合，是提高比赛表现的关键。即使你应用的模型更强大，也需要你处理数据并将其呈现为更易于理解的形式。
- en: 'Feature engineering is also the way you embed any **prior knowledge** (usually
    specialist expertise on the problem) into the data: by summing, subtracting, or
    dividing the existing features, you obtain indicators or estimates that you know
    can better explain the problem you are dealing with. There are also other purposes
    of feature engineering, which are less valuable in a Kaggle competition but could
    prove important in a real-world project. The first is to reduce the size of the
    training data (this could also be useful in a Kaggle competition when working
    with Notebooks, which have limits in memory). The second is to make interpretation
    of the resulting model easier by using features understandable to humans.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程也是将任何**先验知识**（通常是关于问题的专业知识）嵌入数据的方式：通过求和、减法或除法现有特征，您可以得到可以更好地解释您正在处理的问题的指标或估计。特征工程还有其他目的，在Kaggle竞赛中可能不那么有价值，但在现实世界的项目中可能很重要。第一个目的是减少训练数据的大小（在处理Notebooks时，这可能在Kaggle竞赛中也有用，因为Notebooks有内存限制）。第二个目的是通过使用人类可理解的特征来使结果的模型更容易解释。
- en: Each domain may have encoded specific variable transformations that are not
    necessarily self-evident, but well known to experts of the fields. Just think
    of finance, where you have to separate signals from noise for different sets of
    features representing market and company data, by applying specific transformations
    like Kalman filters or wavelet transformations. Given the large number of possible
    fields and the complexity of many feature engineering procedures, in this section,
    we won’t enter into specific domains of expertise and their particular ways of
    dealing with features.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 每个领域可能都有编码特定的变量变换，这些变换可能不是显而易见的，但对该领域专家来说是众所周知的。想想金融领域，在那里您必须通过应用特定的变换（如卡尔曼滤波或小波变换）来区分不同特征集的信号和噪声，这些特征集代表市场和公司数据。鉴于可能存在的领域数量和许多特征工程过程的复杂性，在本节中，我们不会深入探讨特定领域的专业知识及其处理特征的特殊方式。
- en: Instead, we will present you with the most common and most general techniques
    that you can apply in any tabular competition.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将向您展示最常见和最通用的技术，您可以在任何表格竞赛中应用这些技术。
- en: Easily derived features
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 容易导出的特征
- en: 'Deriving features with transformations is the simplest approach, but often
    the most effective. For instance, computing feature ratios (dividing one feature
    by another) can prove quite effective because many algorithms cannot mimic divisions
    (for example, gradient boosting) or can have a hard time trying to (for example,
    deep neural networks). Here are the most common transformations to try out:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变换来提取特征是最简单的方法，但通常也是最有效的。例如，计算特征比率（将一个特征除以另一个特征）可能非常有效，因为许多算法无法模拟除法（例如，梯度提升）或者很难尝试模拟（例如，深度神经网络）。以下是一些常见的变换尝试：
- en: '**Time feature processing**: Splitting a date into its elements (year, month,
    day); transforming it into week of the year and weekday; computing differences
    between dates; computing differences with key events (for instance, holidays).'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**时间特征处理**：将日期分解为其元素（年、月、日）；将其转换为年份中的周和星期几；计算日期之间的差异；计算与关键事件之间的差异（例如，假日）。'
- en: 'For dates, another common transformation is extracting time elements from a
    date or a time. Cyclic continuous transformations (based on sine and cosine transformations)
    are also useful for representing the continuity of time and creating periodic
    features:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日期，另一种常见的变换是从日期或时间中提取时间元素。基于正弦和余弦变换的循环连续变换也很有用，可以表示时间的连续性并创建周期性特征：
- en: '[PRE3]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Numeric feature transformations**: Scaling; normalization; logarithmic or
    exponential transformations; separating the integer and decimal parts; summing,
    subtracting, multiplying, or dividing two numeric features. Scaling obtained by
    standardization (the z-score method used in statistics) or by normalization (also
    called min-max scaling) of numeric features can make sense if you are using algorithms
    sensitive to the scale of features, such as any neural network.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值特征变换**：缩放；归一化；对数或指数变换；分离整数和十进制部分；对两个数值特征求和、减法、乘法或除法。通过标准化（统计学中使用的z分数方法）或归一化（也称为最小-最大缩放）获得的缩放对于使用对特征规模敏感的算法（如任何神经网络）是有意义的。'
- en: '**Binning of numeric features**: This is used to transform continuous variables
    into discrete ones by distributing their values into a number of bins. Binning
    helps remove noise and errors in data and it allows easy modeling of non-linear
    relationships between the binned features and the target variable when paired
    with **one-hot encoding** (see the Scikit-learn implementation, for instance:
    [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)).'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值特征的分箱**：这是通过将值分布到一定数量的箱中来将连续变量转换为离散变量。分箱有助于去除数据中的噪声和错误，并且当与**独热编码**（例如，查看Scikit-learn实现）结合使用时，它允许对分箱特征和目标变量之间的非线性关系进行建模。'
- en: '**Categorical feature encoding**: One-hot encoding; a categorical data processing
    that merges two or three categorical features together; or the more sophisticated
    target encoding (more on this in the following sections).'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类特征编码**：独热编码；将两个或三个分类特征合并在一起的数据处理；或者更复杂的目标编码（更多内容将在以下章节中介绍）。'
- en: '**Splitting and aggregating categorical features based on the levels**: For
    instance, in the *Titanic* competition ([https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic))
    you can split names and surnames, as well their initials, to create new features.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于级别的分类特征拆分和聚合**：例如，在*泰坦尼克号*竞赛([https://www.kaggle.com/c/titanic](https://www.kaggle.com/c/titanic))中，你可以拆分名字和姓氏，以及它们的缩写，以创建新的特征。'
- en: '**Polynomial features** are created by raising features to an exponent. See,
    for instance, this Scikit-learn function: [https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多项式特征**是通过将特征提升到指数来创建的。例如，查看这个Scikit-learn函数：[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).'
- en: 'While they are not proper feature engineering but more data cleaning techniques,
    missing data and outlier treatments involve making changes to the data that nevertheless
    transform your features, and they can help signals from the data emerge:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们不是正确的特征工程，而是更多数据清洗技术，但缺失数据和异常值处理涉及对数据进行更改，这些更改仍然会转换你的特征，并且它们可以帮助数据中的信号出现：
- en: '**Missing values treatment**: Make binary features that point out missing values,
    because sometimes missingness is not random and a missing value could have some
    important reason behind it. Usually, missingness points out something about the
    way data is recorded, acting like a proxy variable for something else. It is just
    like in census surveys: if someone doesn’t tell you their income, it means they
    are extremely poor or are extremely rich. If required by your learning algorithm,
    replace the missing values with the mean, median, or mode (it is seldom necessary
    to use methods that are more sophisticated).'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**缺失值处理**：创建指示缺失值的二元特征，因为有时缺失并不是随机的，缺失值背后可能有重要的原因。通常，缺失值表明了数据记录的方式，充当其他变量的代理变量。就像人口普查调查一样：如果有人不告诉你他们的收入，这意味着他们非常贫穷或非常富有。如果学习算法需要，可以用平均值、中位数或众数（很少需要使用更复杂的方法）替换缺失值。'
- en: 'You can refer to this complete guide written by *Parul Pandey* ([https://www.kaggle.com/parulpandey](https://www.kaggle.com/parulpandey))
    as a reference: [https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python](https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以参考由*Parul Pandey*编写的完整指南作为参考：[https://www.kaggle.com/parulpandey](https://www.kaggle.com/parulpandey)
    [https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python](https://www.kaggle.com/parulpandey/a-guide-to-handling-missing-values-in-python).
- en: 'Just keep in mind that some models can handle missing values by themselves
    and do so fairly better than many standard approaches, because the missing-values
    handling is part of their optimization procedure. The models that can handle missing
    values by themselves are all gradient boosting models:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 只需记住，一些模型可以自己处理缺失值，并且做得比许多标准方法都要好，因为缺失值处理是它们优化过程的一部分。可以自己处理缺失值的模型都是梯度提升模型：
- en: 'XGBoost: [https://xgboost.readthedocs.io/en/latest/faq.html](https://xgboost.readthedocs.io/en/latest/faq.html)'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'XGBoost: [https://xgboost.readthedocs.io/en/latest/faq.html](https://xgboost.readthedocs.io/en/latest/faq.html)'
- en: 'LightGBM: [https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html)'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'LightGBM: [https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html](https://lightgbm.readthedocs.io/en/latest/Advanced-Topics.html)'
- en: 'CatBoost: [https://catboost.ai/docs/concepts/algorithm-missing-values-processing.html](https://catboost.ai/docs/concepts/algorithm-missing-values-processing.html)'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'CatBoost: [https://catboost.ai/docs/concepts/algorithm-missing-values-processing.html](https://catboost.ai/docs/concepts/algorithm-missing-values-processing.html)'
- en: '**Outlier capping or removal**: Exclude, cap to a maximum or minimum value,
    or modify outlier values in your data. To do so, you can use sophisticated multivariate
    models, such as those present in Scikit-learn ([https://scikit-learn.org/stable/modules/outlier_detection.html](https://scikit-learn.org/stable/modules/outlier_detection.html)).'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值上限或移除**：排除、将值上限或下限设置为最大或最小值，或修改数据中的异常值。为此，你可以使用复杂的多变量模型，例如Scikit-learn中提供的那些（[https://scikit-learn.org/stable/modules/outlier_detection.html](https://scikit-learn.org/stable/modules/outlier_detection.html)）。'
- en: Otherwise, you can simply locate the outlying samples in a univariate fashion,
    basing your judgment on how many standard deviations they are from the mean, or
    their distance from the boundaries of the **interquartile range** (**IQR**). In
    this case, you might simply exclude any points that are above the value of `1.5
    * IQR + Q3` (upper outliers) and any points that are below `Q1 - 1.5 * IQR` (lower
    outliers). Once you have found the outliers, you can also proceed by pointing
    them out with a binary variable.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，你可以简单地以单变量方式定位异常样本，根据它们与平均值的多少个标准差或它们与**四分位数范围**（**IQR**）边界的距离来做出判断。在这种情况下，你可能简单地排除任何高于`1.5
    * IQR + Q3`（上异常值）或低于`Q1 - 1.5 * IQR`（下异常值）的点。一旦你找到了异常值，你也可以通过使用二元变量来指出它们。
- en: All these data transformations can add predictive performance to your models,
    but they are seldom decisive in a competition. Though it is necessary, you cannot
    simply rely on basic feature engineering. In the following sections, we’ll suggest
    more complex procedures for extracting value from your data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些数据转换都可以提高你的模型的预测性能，但在比赛中它们很少是决定性的。尽管这是必要的，但你不能仅仅依赖于基本的特征工程。在接下来的章节中，我们将建议更复杂的程序来从你的数据中提取价值。
- en: Meta-features based on rows and columns
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于行和列的元特征
- en: 'In order to perform competitively, you need trickier feature engineering. A
    good place to start is looking at features based on each **row**, considered separately:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在竞争中表现出色，你需要更复杂的特征工程。一个好的开始是查看基于每一**行**的特征，单独考虑：
- en: Compute the mean, median, sum, standard deviation, minimum, or maximum of the
    numeric values (or of a subset of them)
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算数值值（或其子集）的均值、中位数、总和、标准差、最小值或最大值
- en: Count the missing values
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算缺失值
- en: Compute the frequencies of common values found in the rows (for instance, considering
    the binary features and counting the positive values)
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算行中找到的常见值的频率（例如，考虑二元特征并计算正值）
- en: Assign each row to a cluster derived from a cluster analysis such as *k*-means
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每一行分配到由聚类分析（如**k**-均值）得出的簇中
- en: These **meta-features** (called thus because they are features that are representative
    of a set of single features) help to distinguish the different kinds of samples
    found in your data by pointing out specific groups of samples to your algorithm.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这些**元特征**（之所以称为这样，是因为它们是代表一组单个特征的特性）通过指出算法中的特定样本组来帮助区分你数据中找到的不同类型的样本。
- en: Meta-features can also be built based on **columns**. Aggregation and summarization
    operations on single features instead have the objective of providing further
    information about the value of numeric and categorical features; *is this characteristic
    common or rare?* This is information that the model cannot grasp because it cannot
    count categorical instances in a feature.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 元特征也可以基于**列**构建。对单个特征的聚合和汇总操作的目标是提供有关数值和分类特征值的信息；*这个特征是常见还是罕见？* 这是模型无法掌握的信息，因为它无法在特征中计数分类实例。
- en: 'As meta-features, you can use any kind of column statistic (such as mode, mean,
    median, sum, standard deviation, min, max, and also skewness and kurtosis for
    numerical features). For column-wise meta-features, you can proceed in a few different
    ways:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 作为元特征，你可以使用任何类型的列统计量（例如众数、均值、中位数、总和、标准差、最小值、最大值，以及对于数值特征的偏度和峰度）。对于列向的元特征，你可以采取几种不同的方法：
- en: '**Frequency encoding**: Simply count the frequency of the values in a categorical
    feature and then create a new feature where you replace those values with their
    frequency. You can also apply frequency encoding to numeric features when there
    are frequently recurring values.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**频率编码**：简单统计分类特征中值的频率，然后在新的特征中用这些值的频率来替换它们。当数值特征中存在频繁出现的值时，也可以对数值特征应用频率编码。'
- en: '**Frequencies and column statistics computed with respect to a relevant group**:
    In this case, you can create new features from the values of both numeric and
    categorical features because you are considering distinct groups in the data.
    A group could be a cluster you compute by cluster analysis, or a group you can
    define using a feature (for instance, age may produce age groups, locality may
    provide areas, and so on). The meta-features describing each group are then applied
    to each sample based on its group. For instance, using a Pandas `groupby` function,
    you can create your meta-features, which are then merged with the original data
    based on the grouping variable. The trickiest part of this feature engineering
    technique is finding meaningful groups in data to compute the features on.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**相对于相关组的频率和列统计信息计算**：在这种情况下，您可以从数值和分类特征的值中创建新特征，因为您正在考虑数据中的不同组。一个组可以是您通过聚类分析计算出的聚类，或者您可以使用特征定义的组（例如，年龄可以产生年龄组，地区可以提供区域，等等）。然后，根据每个样本所属的组应用描述每个组的元特征。例如，使用Pandas的`groupby`函数，您可以创建元特征，然后根据分组变量将它们与原始数据合并。这个特征工程技术的难点在于在数据中找到有意义的组来计算特征。'
- en: Further column frequencies and statistics can be derived by combining more groups
    together.
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过组合更多的组，可以进一步推导出列频率和统计信息。
- en: The list is certainly not exhaustive, but it should give you an idea of how
    to look for new features at the feature level and at the row level using frequencies
    and statistics.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表当然不是详尽的，但它应该能给您一个在特征级别和行级别使用频率和统计信息寻找新特征的思路。
- en: 'Let’s see a simple example based on the *Amazon Employee Access Challenge*
    data. First, we will apply a frequency encoding on the `ROLE_TITLE` feature:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个基于*Amazon Employee Access Challenge*数据的简单例子来看看。首先，我们将对`ROLE_TITLE`特征应用频率编码：
- en: '[PRE4]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The result will show that the feature classes have been replaced by their observed
    frequency.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将显示特征类别已被它们的观察频率所取代。
- en: We now proceed to encode the `ROLE_TITLE` feature based on the groupings of
    the `ROLE_DEPTNAME`, because we expect that different titles may be more common
    in certain departments and rarer in others.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续根据`ROLE_DEPTNAME`的分组对`ROLE_TITLE`特征进行编码，因为我们预计不同的头衔在某些部门可能更常见，而在其他部门则更罕见。
- en: 'The result is a new feature composed of both, which we use to count the frequency
    of its values:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将是一个由两者组成的新特征，我们用它来计算其值的频率：
- en: '[PRE5]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'You can find all the working code and the results in this Kaggle Notebook:
    [https://www.kaggle.com/lucamassaron/meta-features-and-target-encoding/](https://www.kaggle.com/lucamassaron/meta-features-and-target-encoding/).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下Kaggle笔记本中找到所有的工作代码和结果：[https://www.kaggle.com/lucamassaron/meta-features-and-target-encoding/](https://www.kaggle.com/lucamassaron/meta-features-and-target-encoding/).
- en: Target encoding
  id: totrans-158
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标编码
- en: 'Categorical features are usually not a challenge to deal with, thanks to simple
    functions offered by Scikit-learn such as:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Scikit-learn提供的简单函数，如：
- en: '`LabelEncoder`'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LabelEncoder`'
- en: '`OneHotEncoder`'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OneHotEncoder`'
- en: '`OrdinalEncoder`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`OrdinalEncoder`'
- en: These functions can transform categories into numeric features and then into
    binary features that are easily dealt with by machine learning algorithms. However,
    when the number of categories to deal with is too large, the dataset resulting
    from a one-hot encoding strategy becomes **sparse** (most values in it will be
    zero values) and cumbersome to handle for the memory and processor of your computer
    or Notebook. In these situations, we talk about a **high-cardinality feature**,
    which requires special handling.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这些函数可以将类别转换为数值特征，然后再转换为机器学习算法容易处理的二进制特征。然而，当需要处理的类别数量太多时，由单热编码策略产生的数据集会变得**稀疏**（其中大部分值将是零值），对于计算机或笔记本的内存和处理器来说处理起来很麻烦。在这些情况下，我们谈论的是**高基数特征**，这需要特殊处理。
- en: 'Since early Kaggle competitions, high-cardinality variables have in fact been
    processed using an encoding function that is computed according to Micci-Barreca,
    D. *A preprocessing scheme for high-cardinality categorical attributes in classification
    and prediction problems*. ACM SIGKDD Explorations Newsletter 3.1 (2001): 27-32.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '自从早期的 Kaggle 竞赛以来，高基数变量实际上已经使用一个根据 Micci-Barreca, D. 的编码函数进行处理，该函数是根据 *在分类和预测问题中处理高基数分类属性的前处理方案*。ACM
    SIGKDD Explorations Newsletter 3.1 (2001): 27-32。'
- en: The idea behind this approach is to transform the many categories of a categorical
    feature into their corresponding expected target value. In the case of a regression,
    this is the average expected value for that category; for a binary classification,
    it is the conditional probability given that category; for a multiclass classification,
    you have instead the conditional probability for each possible outcome.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的背后思想是将分类特征的多个类别转换为它们对应的预期目标值。在回归的情况下，这是该类别的平均预期值；对于二分类，它是给定该类别的条件概率；对于多分类，你有每个可能结果的条件概率。
- en: For instance, in the *Titanic* GettingStarted competition ([https://www.kaggle.com/competitions/titanic](https://www.kaggle.com/competitions/titanic)),
    where you have to figure out the survival probability of each passenger, target
    encoding a categorical feature, such as the gender feature, would mean replacing
    the gender value with its average probability of survival.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 *泰坦尼克号* 入门竞赛 ([https://www.kaggle.com/competitions/titanic](https://www.kaggle.com/competitions/titanic))
    中，你必须确定每位乘客的生存概率，对分类特征进行目标编码，例如性别特征，意味着用其平均生存概率替换性别值。
- en: In this way, the categorical feature is transformed into a numeric one without
    having to convert the data into a larger and sparser dataset. In short, this is
    **target encoding** and it is indeed very effective in many situations because
    it resembles a stacked prediction based on the high-cardinality feature. Like
    stacked predictions, however, where you are essentially using a prediction from
    another model as a feature, target encoding brings about the risk of overfitting.
    In fact, when some categories are too rare, using target encoding is almost equivalent
    to providing the target label. There are ways to avoid this.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，分类特征被转换为一个数值型特征，而无需将数据转换为更大和更稀疏的数据集。简而言之，这就是 **目标编码**，它在许多情况下确实非常有效，因为它类似于基于高基数特征的堆叠预测。然而，与堆叠预测一样，你实际上是在使用另一个模型的预测作为特征，目标编码会带来过拟合的风险。事实上，当某些类别非常罕见时，使用目标编码几乎等同于提供目标标签。有方法可以避免这种情况。
- en: 'Before seeing the implementation you can directly import into your code, let’s
    see an actual code example of target encoding. This code was used for one of the
    top-scoring submissions of the *PetFinder.my Adoption Prediction* competition:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到可以直接导入到你的代码中的实现之前，让我们看看一个实际的目标编码代码示例。此代码用于 *PetFinder.my 预测竞赛* 中的一个高分提交：
- en: '[PRE6]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The input parameters of the function are:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的输入参数为：
- en: '`categories`: The column names of the features you want to target-encode. You
    can leave `''auto''` on and the class will pick the object strings.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`categories`: 你想要进行目标编码的特征的列名。你可以保持 `''auto''` 选项开启，类将自动选择对象字符串。'
- en: '`k` (int): Minimum number of samples to take a category average into account.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`k` (int): 考虑类别平均值的样本的最小数量。'
- en: '`f` (int): Smoothing effect to balance the category average versus the prior
    probability, or the mean value relative to all the training examples.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`f` (int): 平滑效果，用于平衡类别平均数与先验概率，或相对于所有训练样本的平均值。'
- en: '`noise_level`: The amount of noise you want to add to the target encoding in
    order to avoid overfitting. Start with very small numbers.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`noise_level`: 你想要添加到目标编码中的噪声量，以避免过拟合。开始时使用非常小的数字。'
- en: '`random_state`: The reproducibility seed in order to replicate the same target
    encoding when `noise_level > 0`.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state`: 当 `noise_level > 0` 时，用于复制相同目标编码的可重复性种子。'
- en: Notice the presence of the `k` and the `f` parameters. In fact, for a level
    *i* of a categorical feature, we are looking for an approximate value that can
    help us better predict the target using a single encoded variable. Replacing the
    level with the observed conditional probability could be the solution, but doesn’t
    work well for levels with few observations. The solution is to blend the observed
    posterior probability on that level (the probability of the target given a certain
    value of the encoded feature) with the a priori probability (the probability of
    the target observed on the entire sample) using a lambda factor. This is called
    the **empirical Bayesian approach**.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 `k` 和 `f` 参数的存在。实际上，对于分类特征的 *i* 级别，我们正在寻找一个近似值，这个值可以帮助我们使用单个编码变量更好地预测目标。用观察到的条件概率替换级别可能是解决方案，但对于观察样本较少的级别则效果不佳。解决方案是将该级别的观察后验概率（给定编码特征的特定值的目标概率）与先验概率（在整个样本中观察到的目标概率）使用一个lambda因子进行混合。这被称为
    **经验贝叶斯方法**。
- en: In practical terms, we are using a function to determine if, for a given level
    of a categorical variable, we are going to use the conditional target value, the
    average target value, or a blend of the two. This is dictated by the lambda factor,
    which, for a fixed `k` parameter (usually it has a unit value, implying a minimum
    cell frequency of two samples) has different output values depending on the `f`
    value that we choose.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，我们使用一个函数来确定，对于分类变量的给定级别，我们将使用条件目标值、平均目标值，还是两者的混合。这由lambda因子决定，对于固定的 `k`
    参数（通常具有单位值，意味着最小单元格频率为两个样本），其输出值取决于我们选择的 `f` 值。
- en: '![__results___2_0.png](img/B17574_07_03.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![__results___2_0.png](img/B17574_07_03.png)'
- en: 'Figure 7.3: Plot of lambda values (on the y-axis) depending on f values and
    sample size of the categorical value (on the x-axis)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3：lambda值（y轴）随f值和分类值的样本大小（x轴）的变化图
- en: As shown by the chart, where the *x*-axis represents the number of cases for
    a given categorical level and the *y*-axis the weight of the conditional target
    value, smaller `f` values tend to switch abruptly from using the average target
    to using the conditional value. Higher values of `f` tend to blend the conditional
    value with the average unless we are dealing with a categorical level with a large
    sample size.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 如图表所示，其中 *x* 轴表示给定分类级别的案例数量，*y* 轴表示条件目标值的权重，较小的 `f` 值倾向于突然从使用平均目标值切换到使用条件值。较高的
    `f` 值倾向于将条件值与平均值混合，除非我们处理的是具有大量样本的分类级别。
- en: Therefore, for a fixed `k`, higher values of `f` dictate less trust in the observed
    empirical frequency and more reliance on the empirical probability for all cells.
    The right value for `f` is usually a matter of testing (supported by cross-validation),
    since you can consider the `f` parameter a hyperparameter in itself.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于固定的 `k`，`f` 的更高值意味着对观察到的经验频率的信任度降低，而对所有单元格的经验概率的依赖性增加。`f` 的正确值通常是一个需要测试的问题（由交叉验证支持），因为你可以将
    `f` 参数视为一个超参数本身。
- en: 'After all these explanations, the class is actually quite straightforward to
    use. Instantiate it with the name of the features you want to target-encode and
    the parameters you want to try and fit it on some training data. Then, you can
    transform any other piece of data, target-encoding only the fitted features:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些解释之后，这个类实际上非常容易使用。用你想要目标编码的特征名称和想要尝试的参数实例化它，并在一些训练数据上拟合它。然后，你可以转换任何其他数据，仅对拟合的特征进行目标编码：
- en: '[PRE7]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The example works on the same *Amazon Employee Access Challenge* data we used
    before and it target-encodes only the `ROLE_TITLE` feature.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例使用的是之前我们使用的相同的 *Amazon Employee Access Challenge* 数据，并且它仅对 `ROLE_TITLE` 特征进行目标编码。
- en: Instead of writing your own code, you can also use the package from [https://github.com/scikit-learn-contrib/category_encoders](https://github.com/scikit-learn-contrib/category_encoders)
    and its Target Encoder ([http://contrib.scikit-learn.org/category_encoders/targetencoder.html](http://contrib.scikit-learn.org/category_encoders/targetencoder.html)).
    It is an out-of-the-box solution that works exactly like the code in this section.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了编写自己的代码，你还可以使用来自 [https://github.com/scikit-learn-contrib/category_encoders](https://github.com/scikit-learn-contrib/category_encoders)
    的包及其目标编码器 ([http://contrib.scikit-learn.org/category_encoders/targetencoder.html](http://contrib.scikit-learn.org/category_encoders/targetencoder.html))。这是一个即用型解决方案，其工作方式与这一节中的代码完全相同。
- en: Using feature importance to evaluate your work
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用特征重要性评估你的工作
- en: 'Applying too much feature engineering can have side effects. If you create
    too many correlated features or features that are not important for the problem,
    models could take too long to complete their training and you may get worse results.
    This may seem like a paradox, but it is explained by the fact that every variable
    carries some noise (a random component due to measurement or recording errors)
    that may be picked by mistake by the model: the more variables you use, the higher
    the chance your model may pick up noise instead of signals. Therefore, you should
    try to keep only the relevant features in the dataset you use for training; consider
    feature selection as a part of your feature engineering process (the pruning phase).'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 过度应用特征工程可能会产生副作用。如果你创建了太多相关特征或者对于问题来说不重要的特征，模型可能需要太长时间来完成训练，你可能会得到更差的结果。这看起来可能像是一个悖论，但这是由以下事实解释的：每个变量都携带一些噪声（由于测量或记录错误而产生的随机成分），模型可能会错误地选择这些噪声而不是信号：你使用的变量越多，你的模型选择噪声而不是信号的几率就越高。因此，你应该尽量只保留你在训练数据集中使用的相关特征；将特征选择视为你特征工程过程（修剪阶段）的一部分。
- en: Figuring out the features you need to keep is a hard problem because, as the
    number of available features grows, the number of possible combinations grows
    too. There are various ways to select features, but first it is important to think
    about the stage in your data preparation pipeline where the selection has to happen.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 确定需要保留的特征是一个难题，因为随着可用特征数量的增加，可能的组合数量也增加。有各种方法可以用来选择特征，但首先重要的是要考虑你的数据准备流程中特征选择必须发生的阶段。
- en: Based on our experiences, we suggest you consider placing feature selection
    at the *end* of your data preparation pipeline. Since features share a part of
    their variance with other features, you cannot evaluate their effectiveness by
    testing them one at a time; you have to consider them all at once in order to
    correctly figure out which you should use.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的经验，我们建议你考虑将特征选择放在数据准备流程的**末尾**。由于特征与其他特征共享部分方差，你不能通过逐个测试它们来评估它们的有效性；你必须一次性考虑所有特征，才能正确地确定你应该使用哪些特征。
- en: 'In addition, you should then test the effectiveness of your selected features
    using cross-validation. Therefore, after you have all the features prepared and
    you have a consistent pipeline and a working model (it doesn’t need to be a fully
    optimized model, but it should work properly and return acceptable results for
    the competition), you are ready to test what features should be retained and what
    could be discarded. At this point, there are various ways to operate feature selection:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你还应该使用交叉验证来测试所选特征的有效性。因此，在你准备好所有特征并有一个一致的工作流程和有效的模型（它不需要是一个完全优化的模型，但它应该能够正常工作并返回可接受的竞赛结果）之后，你就可以测试应该保留哪些特征以及可以丢弃哪些特征。在这个阶段，有各种操作特征选择的方法：
- en: Classical approaches used in statistics resort to forward addition or backward
    elimination by testing each feature entering or leaving the set of predictors.
    Such an approach can be quite time-consuming, though, because it relies on some
    measure of internal importance of variables or on their effect on the performance
    of the model with respect to a specific metric, which you have to recalculate
    for every feature at every step of the process.
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计学中使用的经典方法依赖于通过测试每个特征进入或离开预测集集来执行正向添加或反向消除。然而，这种方法可能相当耗时，因为它依赖于变量的一些内部重要性度量或它们对模型性能（相对于特定指标）的影响，你必须在每个步骤中为每个特征重新计算这些度量。
- en: For regression models, using lasso selection can provide a hint about all the
    important yet correlated features (the procedure may, in fact, retain even highly
    correlated features), by using the **stability selection** procedure. In stability
    selection, you test multiple times (using a bagging procedure) what features should
    be retained – considering only the features whose coefficients are not zero at
    each test – and then you apply a voting system to keep the ones that are most
    frequently assigned non-zero coefficients.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于回归模型，使用lasso选择可以通过**稳定性选择**程序提供关于所有重要且相关的特征（实际上，该程序可能还会保留高度相关的特征）的提示。在稳定性选择中，你多次测试（使用袋装过程）应该保留哪些特征——只考虑在每个测试中系数不为零的特征——然后你应用一个投票系统来保留那些最频繁被分配非零系数的特征。
- en: 'You can get more details about the procedure at this repository: [https://github.com/scikit-learn-contrib/stability-selection](https://github.com/scikit-learn-contrib/stability-selection).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在这个仓库中了解更多关于该过程的细节：[https://github.com/scikit-learn-contrib/stability-selection](https://github.com/scikit-learn-contrib/stability-selection)。
- en: For tree-based models, such as random forests or gradient boosting, a decrease
    in impurity or a gain in the target metric based on splits are common ways to
    rank features. A threshold can cut away the least important ones.
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于树的模型，如随机森林或梯度提升，基于分裂的纯度降低或目标指标的增益是常见的特征排序方法。一个阈值可以去除最不重要的特征。
- en: Always for tree-based models, but easily generalizable to other models, test-based
    randomization of features (or simple comparisons with random features) helps to
    distinguish features that do help the model to predict correctly from features
    that are just noise or redundant.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于基于树的模型始终如此，但很容易推广到其他模型，基于测试的随机化特征（或与随机特征的简单比较）有助于区分那些有助于模型正确预测的特征和那些只是噪音或冗余的特征。
- en: 'An example of how randomizing features helps in selecting important features
    is proposed in this example by *Chris Deotte* in the *Ventilator Pressure Prediction*
    competition: [https://www.kaggle.com/cdeotte/lstm-feature-importance](https://www.kaggle.com/cdeotte/lstm-feature-importance).
    This Notebook tests the role of features in an LSTM-based neural network. First,
    the model is built and the baseline performance is recorded. Then, one by one,
    features are shuffled and the model is required to predict again. If the resulting
    prediction worsens, it suggests that you shuffled an important feature that shouldn’t
    be touched. Instead, if the prediction performance stays the same or even improves,
    the shuffled feature is not influential or even detrimental to the model.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*Chris Deotte* 在 *Ventilator Pressure Prediction* 竞赛中提出了一个如何通过随机化特征来选择重要特征的例子：[https://www.kaggle.com/cdeotte/lstm-feature-importance](https://www.kaggle.com/cdeotte/lstm-feature-importance)。这个笔记本测试了特征在基于LSTM的神经网络中的作用。首先，构建模型并记录基线性能。然后，逐个对特征进行随机排序，并要求模型再次进行预测。如果预测结果变差，则表明你随机排序了一个不应该被更改的重要特征。相反，如果预测性能保持不变甚至提高，则随机排序的特征对模型没有影响，甚至可能是有害的。
- en: There is also No Free Lunch in importance evaluation. Shuffling doesn’t require
    any re-training, which is a great advantage when training a fresh model costs
    time. However, it can fail in certain situations. Shuffling can sometimes create
    unrealistic input combinations that make no sense to evaluate. In other cases,
    it can be fooled by the presence of highly correlated features (incorrectly determining
    that one is important and the other is not). In this case, proceeding by removing
    the feature (instead of shuffling it), retraining the model, and then evaluating
    its performance against the baseline is the best solution.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在重要性评估中，也没有免费的午餐。随机排序不需要任何重新训练，这在训练新模型需要时间时是一个巨大的优势。然而，在某些情况下可能会失败。随机排序有时会创建不切实际的不合理输入组合，这些组合在评估时没有意义。在其他情况下，它可能会被高度相关的特征的存在所欺骗（错误地确定一个很重要而另一个不重要）。在这种情况下，通过删除特征（而不是随机排序它），重新训练模型，然后将其性能与基线进行比较是最佳解决方案。
- en: In another approach based on shuffled features, **Boruta** ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py))
    uses random features to test the validity of the model in an iterative fashion.
    An alternative version of the Boruta selection procedure, **BorutaShap** ([https://github.com/Ekeany/Boruta-Shap](https://github.com/Ekeany/Boruta-Shap)),
    leverages SHAP values in order to combine feature selection and for explainability
    reasons. The resulting selection is usually more reliable than simple rounds of
    removal or randomization of features, because features are tested multiple times
    against random features until they can statistically prove their importance. Boruta
    or BorutaShap may take up to 100 iterations and it can only be performed using
    tree-based machine learning algorithms.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于随机特征的另一种方法中，**Boruta** ([https://github.com/scikit-learn-contrib/boruta_py](https://github.com/scikit-learn-contrib/boruta_py))
    以迭代方式使用随机特征来测试模型的有效性。Boruta选择过程的替代版本，**BorutaShap** ([https://github.com/Ekeany/Boruta-Shap](https://github.com/Ekeany/Boruta-Shap))，利用SHAP值来结合特征选择和解释性原因。这种选择通常比简单的特征去除或随机化更可靠，因为特征需要多次与随机特征进行测试，直到它们可以从统计上证明其重要性。Boruta或BorutaShap可能需要多达100次迭代，并且只能使用基于树的机器学习算法进行操作。
- en: If you are selecting features for a linear model, Boruta may actually overshoot.
    This is because it will consider the features important both for their main effects
    and their interactions together with other features (but in a linear model, you
    care only about the main effects and a selected subset of interactions). You can
    still effectively use Boruta when selecting for a linear model by using a gradient
    boosting whose max depth is set to one tree, so you are considering only the main
    effects of the features and not their interactions.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在为线性模型选择特征，Boruta可能会过度选择。这是因为它会考虑特征的主要效应以及与其他特征一起的交互作用（但在线性模型中，您只关心主要效应和所选子集的交互作用）。您仍然可以通过使用梯度提升并设置最大深度为单棵树来有效地使用Boruta进行线性模型选择，这样您就只考虑特征的主要效应，而不考虑它们的交互作用。
- en: 'You can have a look at how simple and quick it is to set up a BorutaShap feature
    selection by following this tutorial Notebook presented during the *30 Days of
    ML* competition: [https://www.kaggle.com/lucamassaron/tutorial-feature-selection-with-boruta-shap](https://www.kaggle.com/lucamassaron/tutorial-feature-selection-with-boruta-shap).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过查看在*30天机器学习竞赛*期间展示的这篇教程笔记本来了解如何简单快速地设置BorutaShap特征选择：[https://www.kaggle.com/lucamassaron/tutorial-feature-selection-with-boruta-shap](https://www.kaggle.com/lucamassaron/tutorial-feature-selection-with-boruta-shap)。
- en: '![](img/Bojan_Tunguz.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Bojan_Tunguz.png)'
- en: Bojan Tunguz
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Bojan Tunguz
- en: '[https://www.kaggle.com/tunguz](https://www.kaggle.com/tunguz)'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/tunguz](https://www.kaggle.com/tunguz)'
- en: Bojan Tunguz is one Kaggler who definitely understands the importance of feature
    engineering (and is also a great fan of XGBoost ![](img/Smiley_face.png)). We
    were keen to speak to him about his experiences as a Machine Learning Modeler
    at NVIDIA and, impressively, a Kaggle Quadruple Grandmaster.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: Bojan Tunguz是Kaggle上一位确实理解特征工程重要性的Kaggler（同时也是XGBoost的忠实粉丝！![表情符号](img/Smiley_face.png)）。我们非常渴望与他交谈，了解他在NVIDIA作为机器学习模型师的经历，以及他作为Kaggle四冠大师的令人印象深刻的表现。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 您最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，您在Kaggle上的专长是什么？
- en: '*I love any non-code competition. This has changed a lot over the years. I
    used to be really into the image competitions, but the sophistication of the engineering
    stack required to be competitive in these has increased tremendously over the
    years. For a while I was really into the NLP competitions, but those have always
    been rare on Kaggle. One constant over the years, though, has been my interest
    in tabular data problems. Those used to be the quintessential Kaggle competition
    problems but have unfortunately become extinct. I am still very interested in
    that area of ML and have moved into doing some basic research in this domain.
    Compared to the other areas of ML/DL, there has been very little progress on improving
    ML for tabular data, and I believe there is a lot of opportunity here.*'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢任何非代码竞赛。这多年来已经发生了很大的变化。我曾经非常热衷于图像竞赛，但随着这些竞赛中竞争所需的工程堆栈的复杂性逐年增加，我已经不再那么热衷了。有一段时间，我非常热衷于自然语言处理竞赛，但这些在Kaggle上一直很少见。然而，多年来有一个不变的是我对表格数据问题的兴趣。这些曾经是Kaggle竞赛的典型问题，但不幸的是已经消失了。我仍然非常关注这个ML领域，并已经转向在这个领域做一些基本的研究。与其他ML/DL领域相比，在表格数据上改进ML的进展非常有限，我相信这里有很大的机会。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 您是如何应对Kaggle竞赛的？这种方法与您日常工作的方法有何不同？
- en: '*I have always taken the game aspect of Kaggle seriously. What that means for
    me is I usually start new Kaggle competitions very playfully – submitting simple
    solutions, whimsical solutions, modified solutions from other players, blends,
    etc. These help me get a feel for the problem, what sorts of things work, how
    far can I get with a few simple tricks, etc. Some of this is also applicable to
    my day-to-day modeling, but there one important aspect is missing – and that’s
    the support and feedback from the community and the leaderboard. When you are
    working on your own or with a small team, you never know if what you are building
    is the best that can be done, or if a better solution is possible.*'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '*我一直非常认真地对待Kaggle的游戏性。对我来说，这意味着我通常以非常轻松的态度开始新的Kaggle比赛——提交简单的解决方案、异想天开的解决方案、其他玩家修改的解决方案、混合方案等。这些帮助我了解问题，了解哪些方法有效，我可以用几个简单的技巧走多远，等等。其中一些也适用于我的日常建模，但有一个重要的方面是缺失的——那就是来自社区和排行榜的支持和反馈。当你独自工作或与一个小团队一起工作时，你永远不知道你所构建的是否是能做得最好的，或者是否有更好的解决方案。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉我们一个你参加的特别具有挑战性的比赛，以及你使用了哪些见解来应对这个任务。
- en: '*The most challenging and the most important competition of my Kaggle career
    was the* Home Credit Default Risk *competition. It is the second biggest Kaggle
    competition of all time, and it happened during a particularly challenging time
    in my life.*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '*在我Kaggling生涯中最具挑战性和最重要的比赛是*Home Credit Default Risk*比赛。这是有史以来第二大Kaggle比赛，而且发生在我人生中一个特别具有挑战性的时期。*'
- en: '*Credit underwriting is a very challenging data science problem and requires
    a lot of intelligent feature engineering and a reliable validation scheme. My
    own personal insight was to use simple linear modeling for feature selection,
    and it helped our overall model. Our team won that competition, and to this day
    I consider this victory the highlight of my Kaggle career.*'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '*信用承保是一个非常具有挑战性的数据科学问题，需要大量的智能特征工程和一个可靠的验证方案。我个人的见解是使用简单的线性建模进行特征选择，这有助于我们的整体模型。我们的团队赢得了那个比赛，时至今日，我仍然认为这是我的Kaggle生涯中最耀眼的时刻。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助了你在职业生涯中？如果是的话，是如何帮助的？
- en: '*Kaggle has been the single biggest booster of my ML career. Out of four ML
    jobs that I have held, three have been a direct consequence of my Kaggle success.
    It is impossible to overstate how important a Kaggle credential can be in one’s
    career.*'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle是我机器学习职业生涯的最大推动力。在我所持有的四个机器学习职位中，有三个是直接由我的Kaggle成功引起的。Kaggle证书在一个人职业生涯中的重要性是无法被过分强调的。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的经验中，不经验的Kaggler们通常忽略了什么？你现在知道的事情，你希望在你刚开始的时候就知道？
- en: '*There are two aspects of all ML problems, and Kaggle competitions in particular,
    that I have either underappreciated or not bothered enough with for way too long:
    feature engineering and a robust validation strategy. I love ML libraries and
    algorithms and have a tendency to start building the ML algorithm as soon as I
    can. But the single biggest impact on your model’s performance will come from
    very good features. Unfortunately, feature engineering is more of an art than
    a science and is usually very model- and dataset-dependent. Most of the more interesting
    feature engineering tricks and practices are rarely, if ever, taught in standard
    ML courses or resources. Many of them cannot be taught and are dependent on some
    special problem-specific insights. But the mindset of looking into feature engineering
    as default is something that can be cultivated. It will usually take many years
    of practice to get good at it.*'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '*所有机器学习问题，尤其是Kaggle比赛，有两个方面我过去很长时间都没有给予足够的重视：特征工程和稳健的验证策略。我喜欢机器学习库和算法，并且倾向于尽可能早地开始构建机器学习算法。但对你模型性能影响最大的将是非常好的特征。不幸的是，特征工程更多的是一种艺术而不是科学，并且通常与模型和数据集高度相关。大多数更有趣的特征工程技巧和实践很少，如果不是从未，在标准的机器学习课程或资源中教授。其中许多不能教授，并且依赖于一些特殊的问题特定见解。但将特征工程视为默认的做法是一种可以培养的心态。通常需要多年的实践才能精通它。*'
- en: Are there any tools or libraries that you would recommend using for Kaggling?
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你会推荐使用哪些工具或库来进行Kaggling？
- en: '*XGBoost is all you need!*'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: '*XGBoost就是你所需要的全部！*'
- en: Pseudo-labeling
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伪标签
- en: In competitions where the number of examples used for training can make a difference,
    **pseudo-labeling** can boost your scores by providing further examples taken
    from the test set. The idea is to add examples from the test set whose predictions
    you are confident about to your training set.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在那些训练示例数量可能影响结果的竞赛中，**伪标签化**可以通过提供从测试集中提取的更多示例来提高您的分数。其思路是将您对预测有信心的测试集示例添加到训练集中。
- en: 'First introduced in the *Santander Customer Transaction Prediction* competition
    by team Wizardry (read here: [https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003)),
    pseudo-labeling simply helps models to refine their coefficients thanks to more
    data available, but it won’t always work. First of all, it is not necessary in
    some competitions. That is, adding pseudo-labels won’t change the result; it may
    even worsen it if there is some added noise in the pseudo-labeled data.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法首次由团队Wizardry在 *Santander Customer Transaction Prediction* 竞赛中提出（阅读此处：[https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003](https://www.kaggle.com/c/santander-customer-transaction-prediction/discussion/89003)），伪标签化通过提供更多数据帮助模型优化其系数，但这并不总是有效。首先，在某些竞赛中并不必要。也就是说，添加伪标签不会改变结果；如果伪标签数据中存在一些额外的噪声，甚至可能会使结果变得更糟。
- en: 'Unfortunately, you cannot know for sure beforehand whether or not pseudo-labeling
    will work in a competition (you have to test it empirically), though plotting
    learning curves may provide you with a hint as to whether having more data could
    be useful (see this example provided by Scikit-learn: [https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)).'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，您无法事先确定伪标签化是否会在竞赛中有效（您必须通过实验来测试），尽管绘制学习曲线可能为您提供有关更多数据是否有用的线索（请参阅Scikit-learn提供的此示例：[https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html))。
- en: 'Second, it is not easy to decide which parts of the test set predictions to
    add or how to tune the entire procedure for the best results. Generally, this
    is the procedure:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，决定添加测试集预测的哪些部分或如何调整整个流程以获得最佳结果并不容易。通常，流程如下：
- en: Train your model
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练您的模型
- en: Predict on the test set
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上进行预测
- en: Establish a confidence measure
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 建立置信度度量
- en: Select the test set elements to add
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择要添加的测试集元素
- en: Build a new model with the combined data
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用合并后的数据构建新的模型
- en: Predict using this model and submit
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用此模型进行预测并提交
- en: 'A good example of the complete procedure for obtaining pseudo-labeling is offered
    by Chris Deotte in the *Instant Gratification* competition: [https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969).
    You don’t need to know more than a few tricks in order to apply it.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Chris Deotte在 *Instant Gratification* 竞赛中提供了一个获取伪标签的完整流程的示例：[https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969](https://www.kaggle.com/cdeotte/pseudo-labeling-qda-0-969)。您不需要知道太多技巧就可以应用它。
- en: 'There are a few caveats you should consider when trying to apply pseudo-labeling:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试应用伪标签化时，您应该考虑以下注意事项：
- en: You should have a very good model that produces good predictions for them to
    be usable in training. Otherwise, you will just add more noise.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您应该有一个非常好的模型，能够为这些预测产生良好的结果，这样它们才能在训练中使用。否则，您只会添加更多的噪声。
- en: Since it is impossible to have entirely perfect predictions in the test set,
    you need to distinguish the good ones from the ones you shouldn’t use. If you
    are predicting using CV folds, check the standard deviation of your predictions
    (this works both with regression and classification problems) and pick only the
    test examples where the standard deviation is the lowest. If you are predicting
    probabilities, use only high-end or low-end predicted probabilities (the cases
    where the model is actually more confident).
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于在测试集中完全完美的预测是不可能的，您需要区分好的预测和不应该使用的预测。如果您正在使用交叉验证（CV）折进行预测，检查您预测的标准差（这适用于回归和分类问题），并仅选择标准差最低的测试示例。如果您正在预测概率，请仅使用高端或低端预测概率（即模型实际上更有信心的情况）。
- en: In the second stage, when you concatenate the training examples with the test
    ones, do not put in more than 50% test examples. Ideally, a share of 70% original
    training examples and 30% pseudo-labeled examples is the best. If you put in too
    many pseudo-labeled examples, your new model will risk learning little from the
    original data and more from the easier test examples, resulting in a distilled
    model that does not perform better than the original. In fact, as you are training,
    your model is also learning how to deal with noise in labels, but pseudo-labeled
    examples do not have this noise.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二阶段，当你将训练示例与测试示例连接起来时，不要放入超过50%的测试示例。理想情况下，70%的原始训练示例和30%的伪标签示例是最好的。如果你放入太多的伪标签示例，你的新模型可能会从原始数据中学到很少，而从更容易的测试示例中学到更多，从而导致一个性能不如原始模型的蒸馏模型。实际上，当你训练时，你的模型也在学习如何处理标签中的噪声，但伪标签示例没有这种噪声。
- en: Don’t forget that you cannot completely trust your pseudo-labels, so keep in
    mind that you are also partially spoiling your data by using test predictions
    as training examples. The trick works when you get more benefits from doing so
    than negative effects.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 不要忘记，你无法完全信任你的伪标签，所以请记住，使用测试预测作为训练示例的同时，你也在部分地破坏你的数据。当这样做带来的好处大于负面影响时，这个技巧才有效。
- en: If you depend on validation for early stopping, fixing hyperparameters, or simply
    evaluating your model, do not use pseudo-labels in the validation. They could
    be highly misleading. Always use the original training cases for the same reasons
    we quoted above.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你依赖于验证来提前停止、固定超参数或简单地评估你的模型，不要在验证中使用伪标签。它们可能会非常误导。始终使用原始训练案例，原因如上所述。
- en: If possible, use a different kind of model when training to estimate the pseudo-labels
    and when training your final model using both the original labels and the pseudo-labels.
    This will ensure you are not simply enforcing the same information your previous
    model used, but you are also extracting new information from the pseudo-labels.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能的话，在训练时使用不同类型的模型来估计伪标签，并使用原始标签和伪标签来训练你的最终模型。这将确保你不仅强化了先前模型使用的信息，而且还从伪标签中提取了新的信息。
- en: Clearly, pseudo-labeling is more of an art than a science. It can make the difference
    in certain competitions but needs to be executed very well to generate results.
    Consider it a resource, and always try one submission based on pseudo-labels.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，伪标签更像是一门艺术而非科学。它可能在某些比赛中起到决定性作用，但需要非常熟练地执行才能产生结果。将其视为一种资源，并始终尝试基于伪标签提交一次。
- en: Denoising with autoencoders
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用自编码器进行去噪
- en: '**Autoencoders**, initially better known for non-linear data compression (a
    kind of non-linear PCA) and image denoising, started being recognized as an interesting
    tool for tabular competitions after *Michael Jahrer* ([https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer))
    successfully used them to win the *Porto Seguro’s Safe Driver Prediction* competition
    ([https://www.kaggle.com/c/porto-seguro-safe-driver-prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)).
    *Porto Seguro* was a popular, insurance-based risk analysis competition (more
    than 5,000 participants) characterized by particularly noisy features.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: '**自编码器**，最初因其非线性数据压缩（一种非线性PCA）和图像去噪而闻名，在Michael Jahrer（[https://www.kaggle.com/mjahrer](https://www.kaggle.com/mjahrer)）成功使用它们赢得*Porto
    Seguro的驾驶员安全预测*比赛（[https://www.kaggle.com/c/porto-seguro-safe-driver-prediction](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction)）后，开始被认可为表格竞赛中的一个有趣工具。*Porto
    Seguro*是一个基于保险的风险分析竞赛（超过5,000名参与者），以其特别嘈杂的特征为特点。'
- en: Michael Jahrer describes how he found a better representation of the numeric
    data for subsequent neural net supervised learning by using **denoising autoencoders**
    (**DAEs**). A DAE can produce a new dataset with a huge number of features based
    on the activations of the hidden layers at the center of the network, as well
    as the activations of the middle layers encoding the information.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: Michael Jahrer描述了他如何通过使用**去噪自编码器**（**DAEs**）找到更好的数值数据表示，以便进行后续的神经网络监督学习。一个DAE可以根据网络中心隐藏层的激活以及编码信息的中间层的激活，生成一个具有大量特征的新数据集。
- en: In his famous post ([https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629)),
    Michael Jahrer describes how a DAE can not only remove noise but also automatically
    create new features, so the representation of the features is learned in a similar
    way to what happens in image competitions. In the post, he mentions the secret
    sauce for the DAE recipe, which is not simply the layers, but the **noise** you
    put into the data in order to augment it. He also made clear that the technique
    requires stacking together training and test data, implying that the technique
    would not have applications beyond winning a Kaggle competition. In fact, after
    this winning exploit, the technique disappeared from the forums and most competitions
    until its recent re-emergence during the Tabular Playground Series.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在他著名的帖子([https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629))中，Michael
    Jahrer描述了DAE不仅可以去除噪声，还可以自动创建新特征，因此特征的表示是以与图像竞赛中发生的方式相似的方式学习的。在帖子中，他提到了DAE食谱的秘密成分，不仅仅是层，而是为了增强数据而放入数据中的**噪声**。他还明确指出，这项技术需要将训练数据和测试数据堆叠在一起，这意味着这项技术不会应用于Kaggle竞赛之外的应用。事实上，在这项获奖尝试之后，这项技术从论坛和大多数竞赛中消失，直到最近在Tabular
    Playground Series期间再次出现。
- en: DAEs are technically composed of an **encoding** part and a **decoding** part.
    The encoding part takes the training data as input and is followed by a few dense
    layers. Ideally, you have a hidden middle layer, whose activations just encode
    all the training information. If the number of nodes in this middle layer is smaller
    than the original input shape, you have a **compression** and hopefully, in statistical
    terms, you are representing some latent dimensionality that is behind the generative
    process of the input data; otherwise, you are simply eliminating redundancies
    and separating noise from signal (which is not a bad result).
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术上讲，DAE由一个**编码**部分和一个**解码**部分组成。编码部分以训练数据作为输入，后面跟着几个密集层。理想情况下，你有一个隐藏的中间层，其激活仅编码所有训练信息。如果这个中间层的节点数小于原始输入形状，你有一个**压缩**，并且从统计学的角度来看，你代表了一些潜在维度，这是输入数据的生成过程背后的；否则，你只是消除冗余，将噪声与信号分离（这并不是一个坏的结果）。
- en: In the second part of the layer, the decoder part, you are enlarging the layers
    again until they regain the shape of the original input. The output is compared
    with the input to compute an error loss to backpropagate to the network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在层的第二部分，解码部分，你再次扩大层，直到它们恢复原始输入的形状。输出与输入进行比较，以计算误差损失，并将其反向传播到网络中。
- en: 'From these solutions, you can deduce that there are two types of DAEs:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些解决方案中，你可以推断出有两种类型的DAE：
- en: In **bottleneck DAEs**, mimicking the approach used in image processing, you
    take as new features the activations from the middle layer, the one separating
    the encoding part from the decoding part. These architectures have an hourglass
    shape, first reducing the number of neurons layer by layer until the middle bottleneck
    layer, then enlarging it back in the second part. The number of hidden layers
    is always odd.
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**瓶颈DAE**中，模仿图像处理中使用的方案，你将中间层的激活作为新特征，这个中间层将编码部分与解码部分分开。这些架构具有沙漏形状，首先逐层减少神经元数量，直到中间瓶颈层，然后在第二部分再次扩大。隐藏层的数量总是奇数。
- en: '![](img/B17574_07_04.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_07_04.png)'
- en: 'Figure 7.4: In a bottleneck DAE, you take only the bottleneck layer weights
    as features'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4：在瓶颈DAE中，你只取瓶颈层的权重作为特征
- en: In **deep stack DAEs**, you take all the activations from the hidden layers,
    without distinguishing between the encoding, decoding, or middle layer. In these
    architectures, layers are the same size. The number of hidden layers can be even
    or odd.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**深度堆叠DAE**中，你将隐藏层的所有激活作为特征，不区分编码、解码或中间层。在这些架构中，层的尺寸相同。隐藏层的数量可以是偶数或奇数。
- en: '![](img/B17574_07_05.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B17574_07_05.png)'
- en: 'Figure 7.5: In a deep stack DAE, you take all the stacked hidden layer weights
    as features'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5：在深度堆叠DAE中，你将所有堆叠的隐藏层权重作为特征
- en: 'As we mentioned, an important aspect often discussed is adding some **random
    noise** to your DAE. In order to help train any kind of DAE, you need to inject
    noise that helps to augment the training data and avoid the overparameterized
    neural network just memorizing inputs (in other words, overfitting). In the *Porto
    Seguro* competition, Michael Jahrer added noise by using a technique called **swap
    noise**, which he described as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，经常讨论的一个重要方面是向你的 DAE 添加一些**随机噪声**。为了帮助训练任何类型的 DAE，你需要注入有助于增强训练数据并避免过参数化的神经网络仅仅记住输入（换句话说，过拟合）的噪声。在
    *Porto Seguro* 竞赛中，Michael Jahrer 通过使用一种称为**交换噪声**的技术添加噪声，他描述如下：
- en: Here I sample from the feature itself with a certain probability “inputSwapNoise”
    in the table above. 0.15 means 15% of features replaced by values from another
    row.
  id: totrans-252
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 在上表中的“inputSwapNoise”概率下，我以一定的概率从特征本身进行采样。0.15 表示 15% 的特征被来自另一行的值替换。
- en: 'What is described is basically an augmentation technique called **mixup** (which
    is also used in image augmentation: [https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412)).
    In mixup for tabular data, you decide a probability for mixing up. Based on that
    probability, you change some of the original values in a sample, replacing them
    with values from a more or less similar sample from the same training data.'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 所描述的是一种基本的增强技术，称为**mixup**（也用于图像增强：[https://arxiv.org/abs/1710.09412](https://arxiv.org/abs/1710.09412))。在表格数据的
    mixup 中，你决定一个混合概率。基于这个概率，你改变样本中的一些原始值，用来自相同训练数据中更或更相似的样本的值来替换它们。
- en: 'In his walkthrough ([https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta](https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta)),
    *Danzel* describes three approaches to this: column-wise, row-wise, and random:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在他的演示中（[https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta](https://www.kaggle.com/springmanndaniel/1st-place-turn-your-data-into-daeta))，*Danzel*
    描述了三种方法：列向、行向和随机：
- en: In **column-wise** noise swapping, you swap values in a certain number of columns.
    The proportion of columns whose values are to be swapped is decided based on your
    mixup probability.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**列向**噪声交换中，你交换一定数量列中的值。要交换的值的列的比例是基于你的混合概率决定的。
- en: In **row-wise** noise swapping, you always swap a certain number of the values
    in each row. Essentially, every row contains the same proportion of swapped values,
    based on the mixup probability, but the features swapped change from row to row.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**行向**噪声交换中，你始终交换每行中一定数量的值。本质上，每行包含相同比例的交换值，基于混合概率，但交换的特征会从行到行变化。
- en: In **random** noise swapping, you fix a number of values to be swapped, based
    on the mixup probability, and you randomly pick them up from the entire dataset
    (this is somewhat similar to row-wise swapping in effect).
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**随机**噪声交换中，你根据混合概率固定要交换的值的数量，并从整个数据集中随机选择它们（这在效果上与行向交换相似）。
- en: 'Like pseudo-labeling, DAE is also more of an art than a science, which is another
    way to say that it is all trial and error. It won’t always work and the details
    that make it work on one problem probably won’t help for another. In order to
    obtain a good DAE for your competition, you need to keep an eye on a series of
    aspects that need to be tested and tuned:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 与伪标签一样，DAE 也更像是艺术而不是科学，这另一种说法是它完全是试错。它并不总是有效，使它在某个问题上有效的一组细节可能对另一个问题没有帮助。为了在比赛中获得一个好的
    DAE，你需要关注一系列需要测试和调整的方面：
- en: Architecture of the DAE (deep stack tends to work better, but you need to determine
    the number of units per layer and the number of layers)
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DAE（深度堆叠架构）的架构（深度堆叠往往效果更好，但你需要确定每层的单元数和层数）
- en: Learning rate and batch size
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率和批量大小
- en: Loss (also distinguishing between the loss of numeric and categorical features
    helps)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 损失（区分数值特征和分类特征的损失也有帮助）
- en: Stopping point (the lowest loss is not always the best; use validation and early
    stopping if possible)
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止点（最低损失并不总是最好的；如果可能，请使用验证和早期停止）
- en: 'Depending on the problem, you should expect to face some difficulties in setting
    up the right architecture and adjusting it to work properly. Your efforts, however,
    could be rewarded by a top result on the final private leaderboard. In fact, in
    recent tabular competitions, DAE techniques appeared as part of the recipe of
    many winning submissions:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 根据问题，你应该预计在设置正确的架构和调整其正常工作时会遇到一些困难。然而，你的努力可能会在最终的私有排行榜上获得优异成绩。事实上，在最近的表格竞赛中，DAE
    技术作为许多获胜提交的一部分出现：
- en: Danzel ([https://www.kaggle.com/springmanndaniel](https://www.kaggle.com/springmanndaniel))
    reported in [https://www.kaggle.com/c/tabular-playground-series-jan-2021/discussion/216037](https://www.kaggle.com/c/tabular-playground-series-jan-2021/discussion/216037)
    having used the hidden weights of three 1,500-neuron layers, expanding the original
    data from 14 columns to 4,500\. This new, processed dataset was used as input
    in other neural networks and gradient boosting models.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Danzel ([https://www.kaggle.com/springmanndaniel](https://www.kaggle.com/springmanndaniel))
    在 [https://www.kaggle.com/c/tabular-playground-series-jan-2021/discussion/216037](https://www.kaggle.com/c/tabular-playground-series-jan-2021/discussion/216037)
    中报告说使用了三个1,500个神经元的隐藏权重，将原始数据从14列扩展到4,500列。这个新的、处理过的数据集被用作其他神经网络和梯度提升模型的输入。
- en: '*Ren Zhang* ([https://www.kaggle.com/ryanzhang](https://www.kaggle.com/ryanzhang))
    discussed his solution ([https://www.kaggle.com/c/tabular-playground-series-feb-2021/discussion/222745](https://www.kaggle.com/c/tabular-playground-series-feb-2021/discussion/222745))
    and shared his code ([https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder](https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder)),
    revealing that he used stacked transformer encoders rather than your typical linear
    and ReLU activated hidden layers (and that such an approach can mean it takes
    up to 20 hours to train a proper DAE). In his approach, he also suggested adding
    some random noise to the data (by using a noise mask) to be reconstructed and
    to compute the loss based not only on the error from reconstructing the original
    data, but also from the noise mask. Using this combined loss helps the network
    to converge better. Studying the code provided in the GitHub link and the graph
    in the Kaggle discussion post will help you to understand better and easily replicate
    this innovative approach.'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Ren Zhang* ([https://www.kaggle.com/ryanzhang](https://www.kaggle.com/ryanzhang))
    讨论了他的解决方案 ([https://www.kaggle.com/c/tabular-playground-series-feb-2021/discussion/222745](https://www.kaggle.com/c/tabular-playground-series-feb-2021/discussion/222745))
    并分享了他的代码 ([https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder](https://github.com/ryancheunggit/Denoise-Transformer-AutoEncoder))，揭示了他使用了堆叠的transformer编码器而不是你典型的线性激活和ReLU激活的隐藏层（并且这种做法可能意味着训练一个合适的DAE需要长达20小时）。在他的方法中，他还建议向数据中添加一些随机噪声（通过使用噪声掩码）以进行重建，并基于重建原始数据的误差以及噪声掩码来计算损失。使用这种组合损失有助于网络更好地收敛。研究GitHub链接中提供的代码和Kaggle讨论帖中的图表将有助于你更好地理解并轻松复制这种创新方法。'
- en: '*JianTT* ([https://www.kaggle.com/jiangtt](https://www.kaggle.com/jiangtt))
    noticed how some techniques key to DAEs, in particular creating new observations
    by adding noise, can be useful for training better algorithms without the need
    of creating a complete DAE: [https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/235739](https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/235739).'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*JianTT* ([https://www.kaggle.com/jiangtt](https://www.kaggle.com/jiangtt))
    注意到一些对DAEs至关重要的技术，特别是通过添加噪声来创建新的观测值，可以在无需创建完整DAE的情况下训练更好的算法：[https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/235739](https://www.kaggle.com/c/tabular-playground-series-apr-2021/discussion/235739)。'
- en: 'If you don’t want to spend too much time building your own DAE, but you would
    like to explore whether something like it could work for the competition you are
    taking on, you can test out a couple of pre-prepared solutions. First, you can
    refer to a Notebook for a PyTorch network from *Hung Khoi* ([https://www.kaggle.com/hungkhoi/train-denoise-transformer-autoencoder](https://www.kaggle.com/hungkhoi/train-denoise-transformer-autoencoder))
    and re-adapt it to your needs, or you can use the Kaggler library from *Jeong-Yoon
    Lee* ([https://www.kaggle.com/jeongyoonlee](https://www.kaggle.com/jeongyoonlee)).
    In his Notebook, Jeong-Yoon Lee presents how it works on one of the Tabular Playground
    competitions: [https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler](https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler).'
  id: totrans-267
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你不想花太多时间构建自己的DAE，但又想探索是否类似的方法可以适用于你正在参加的比赛，你可以尝试一些预先准备好的解决方案。首先，你可以参考*Hung
    Khoi* ([https://www.kaggle.com/hungkhoi/train-denoise-transformer-autoencoder](https://www.kaggle.com/hungkhoi/train-denoise-transformer-autoencoder))的PyTorch网络笔记本，并将其重新适配到你的需求中，或者你可以使用*Jeong-Yoon
    Lee* ([https://www.kaggle.com/jeongyoonlee](https://www.kaggle.com/jeongyoonlee))的Kaggler库。在他的笔记本中，Jeong-Yoon
    Lee展示了它在一个Tabular Playground比赛中是如何工作的：[https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler](https://www.kaggle.com/jeongyoonlee/dae-with-2-lines-of-code-with-kaggler)。
- en: Neural networks for tabular competitions
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于表格竞赛的神经网络
- en: Having discussed neural networks with DAEs, we have to complete this chapter
    by discussing how neural networks can help you in a tabular competition more generally.
    Gradient boosting solutions still clearly dominate tabular competitions (as well
    as real-world projects); however, sometimes neural networks can catch signals
    that gradient boosting models cannot get, and can be excellent single models or
    models that shine in an ensemble.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论了与DAE的神经网络之后，我们必须通过讨论神经网络如何在表格竞赛中更一般地帮助你来完成这一章。梯度提升解决方案在表格竞赛（以及现实世界项目）中仍然明显占据主导地位；然而，有时神经网络可以捕捉到梯度提升模型无法获取的信号，并且可以作为优秀的单一模型或是在集成中表现突出的模型。
- en: 'As many Grandmasters of the present and the past often quote, mixing together
    diverse models (such as a neural network and a gradient boosting model) always
    produces better results than single models taken separately in a tabular data
    problem. *Owen Zhang*, previously number one on Kaggle, discusses at length in
    the following interview how neural networks and GBMs can be blended nicely for
    better results in a competition: [https://www.youtube.com/watch?v=LgLcfZjNF44](https://www.youtube.com/watch?v=LgLcfZjNF44).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 正如许多现在和过去的围棋大师经常引用的那样，将不同的模型（如神经网络和梯度提升模型）混合在一起，在表格数据问题中总是比单独使用单个模型产生更好的结果。前Kaggle排名第一的*Owen
    Zhang*在以下采访中详细讨论了如何在竞赛中将神经网络和GBM混合在一起以获得更好的结果：[https://www.youtube.com/watch?v=LgLcfZjNF44](https://www.youtube.com/watch?v=LgLcfZjNF44)。
- en: Building a neural network quickly for a tabular competition is no longer a daunting
    challenge. Libraries such as TensorFlow/Keras and PyTorch make things easy, and
    having some pre-made networks such as TabNet already packaged for you into libraries
    makes them even easier.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 为表格竞赛快速构建神经网络不再是令人畏惧的挑战。TensorFlow/Keras和PyTorch等库使得事情变得简单，而且有一些预先制作的网络，如TabNet，已经打包到库中，使得它们更加容易使用。
- en: To quickly get started with building your own network, you can use various resources.
    We warmly suggest referring to the book we published, *Machine Learning Using
    TensorFlow Cookbook* ([https://www.packtpub.com/product/machine-learning-using-tensorflow-cookbook/9781800208865](https://www.packtpub.com/product/machine-learning-using-tensorflow-cookbook/9781800208865)),
    since there is an extensive chapter devoted to building DNNs with TensorFlow for
    tabular problems (*Chapter 7*, *Predicting with Tabular Data*). In the book, you
    can also find many other suggestions and recipes for using TensorFlow for Kaggle.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要快速开始构建自己的网络，你可以使用各种资源。我们强烈建议参考我们出版的书籍《Machine Learning Using TensorFlow Cookbook》（[https://www.packtpub.com/product/machine-learning-using-tensorflow-cookbook/9781800208865](https://www.packtpub.com/product/machine-learning-using-tensorflow-cookbook/9781800208865)），因为其中有一章专门介绍如何使用TensorFlow为表格问题构建DNN（第7章，*使用表格数据预测*）。在书中，你还可以找到许多其他关于如何使用TensorFlow进行Kaggle的建议和食谱。
- en: 'Otherwise, you can refer to a few online resources introducing you to the topic,
    as presented during the *30 Days of ML* competition:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，你可以参考一些在线资源，这些资源在*30 Days of ML*竞赛期间介绍过这个主题：
- en: 'Watch this video that explains how to use TensorFlow for tabular data: [https://www.youtube.com/watch?v=nQgUt_uADSE](https://www.youtube.com/watch?v=nQgUt_uADSE)'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观看这个解释如何使用TensorFlow处理表格数据的视频：[https://www.youtube.com/watch?v=nQgUt_uADSE](https://www.youtube.com/watch?v=nQgUt_uADSE)
- en: 'Use the code from the tutorial on GitHub: [https://github.com/lmassaron/deep_learning_for_tabular_data](https://github.com/lmassaron/deep_learning_for_tabular_data)'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GitHub上的教程代码：[https://github.com/lmassaron/deep_learning_for_tabular_data](https://github.com/lmassaron/deep_learning_for_tabular_data)
- en: 'Most importantly, find the tutorial Notebook applied to the competition here:
    [https://www.kaggle.com/lucamassaron/tutorial-tensorflow-2-x-for-tabular-data](https://www.kaggle.com/lucamassaron/tutorial-tensorflow-2-x-for-tabular-data)'
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最重要的是，在这里找到适用于竞赛的教程Notebook：[https://www.kaggle.com/lucamassaron/tutorial-tensorflow-2-x-for-tabular-data](https://www.kaggle.com/lucamassaron/tutorial-tensorflow-2-x-for-tabular-data)
- en: 'The key things to take into account when building these solutions are:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建这些解决方案时，需要考虑的关键因素包括：
- en: Use activations such as GeLU, SeLU, or Mish instead of ReLU; they are quoted
    in quite a few papers as being more suitable for modeling tabular data and our
    own experience confirms that they tend to perform better.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GeLU、SeLU或Mish等激活函数代替ReLU；它们在许多论文中被引用为更适合建模表格数据，而且我们自己的经验证实它们往往表现更好。
- en: Experiment with batch size.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试不同的批处理大小。
- en: Use augmentation with mixup (discussed in the section on autoencoders).
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用与mixup（在自动编码器部分讨论过）相结合的增强。
- en: Use quantile transformation on numeric features and force, as a result, uniform
    or Gaussian distributions.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数值特征进行分位数变换，并强制其结果为均匀分布或高斯分布。
- en: Leverage embedding layers, but also remember that embeddings do not model everything.
    In fact, they miss interactions between the embedded feature and all the others
    (so you have to force these interactions into the network with direct feature
    engineering).
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用嵌入层，但也记住嵌入并不涵盖一切。实际上，它们忽略了嵌入特征与其他所有特征之间的交互（因此你必须通过直接特征工程将这些交互强制进入网络）。
- en: In particular, remember that embedding layers are reusable. In fact, they consist
    only of a matrix multiplication that reduces the input (a sparse one-hot encoding
    of the high cardinality variable) to a dense one of lower dimensionality. By recording
    and storing away the embedding of a trained neural network, you can transform
    the same feature and use the resulting embeddings in many other different algorithms,
    from gradient boosting to linear models.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，记住嵌入层是可以重用的。实际上，它们只包含一个矩阵乘法，将输入（高基数变量的稀疏独热编码）减少到低维度的密集编码。通过记录和存储训练好的神经网络的嵌入，你可以转换相同的特征，并将结果嵌入用于许多不同的算法，从梯度提升到线性模型。
- en: 'Refer to the diagram in *Figure 7.6* for a clearer understanding of the process
    involving a categorical variable with 24 levels. In the chart, we demonstrate
    how a value from a categorical feature is transformed from a textual or an integer
    value into a vector of values that a neural network can handle:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图7.6中的图表，以更清楚地了解涉及24个级别的分类变量的过程。在图表中，我们展示了如何将分类特征的值从文本值或整数值转换为神经网络可以处理的值向量：
- en: '![](img/B17574_07_06.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B17574_07_06.png)'
- en: 'Figure 7.6: How an embedding layer works'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6：嵌入层的工作原理
- en: Everything starts with knowing how many distinct values the feature has. This
    constitutes the dictionary size and it is an important piece of information. In
    this example, we considered a feature presenting 24 distinct values. This information
    allows us to create a one-hot-encoded vector of size 24 representing each of the
    possible feature values. The resulting vector is then multiplied by a matrix whose
    row size corresponds to the size of the one-hot-encoded vector and column size
    to the size of the output dimensions. In this way, with a vector-matrix multiplication,
    the input of the categorical variable will be transformed into a multidimensional
    numeric one. The effectiveness of the multiplication is ensured by the backpropagation
    algorithm of the neural network, which will update each value in the matrix so
    the most predictive result is obtained from the multiplication.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都从知道特征有多少个不同的值开始。这构成了字典大小，这是一条重要的信息。在这个例子中，我们考虑了一个具有24个不同值的特征。这些信息使我们能够创建一个大小为24的独热编码向量，代表每个可能的特征值。然后，这个向量被乘以一个矩阵，其行大小对应于独热编码向量的大小，列大小对应于输出维度的大小。通过这种方式，通过向量-矩阵乘法，分类变量的输入将被转换为一个多维数值。乘法的有效性由神经网络的反向传播算法保证，该算法将更新矩阵中的每个值，以从乘法中获得最具预测性的结果。
- en: 'If you don’t want to build your own deep neural network in TensorFlow or PyTorch,
    you can rely on a few out-of-the-box architectural solutions. All these solutions
    come out of the box because they are packaged or because other Kagglers have written
    them based on the original papers. Based on their success in tabular competitions,
    here are the main ones you can try when taking on a tabular competition yourself:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不想在TensorFlow或PyTorch中构建自己的深度神经网络，你可以依赖一些现成的架构解决方案。所有这些解决方案都是现成的，因为它们被打包，或者是因为其他Kagglers基于原始论文编写了它们。基于它们在表格竞赛中的成功，以下是一些当你自己参与表格竞赛时可以尝试的主要方案：
- en: '**TabNet** is a network devised by Google researchers (Arık, S. O. and Pfister.
    T. *Tabnet: Attentive interpretable tabular learning.* arXiv 2020\. [https://www.aaai.org/AAAI21Papers/AAAI-1063.ArikS.pdf](https://www.aaai.org/AAAI21Papers/AAAI-1063.ArikS.pdf))
    that promises to help you select and process the relevant features and to deal
    with both categorical and numeric features in a smart way. It doesn’t have many
    hyperparameters to tune, though the results may deeply differ between an untuned
    network and a tuned one (hence the necessity of spending some time to make it
    work at its best). Here you have a few implementations, such as the excellent
    `pytorch-tabnet` package ([https://github.com/dreamquark-ai/tabnet](https://github.com/dreamquark-ai/tabnet))
    or the implementations coded by *Yirun Zhang* ([https://www.kaggle.com/gogo827jz](https://www.kaggle.com/gogo827jz)),
    found at [https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-training](https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-training)
    and [https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference](https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference).
    Both were devised for the *Mechanism of Action (MoA) Prediction* competition.'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TabNet** 是由谷歌研究人员设计的网络（Arık, S. O. 和 Pfister. T. *Tabnet: Attentive interpretable
    tabular learning.* arXiv 2020\. [https://www.aaai.org/AAAI21Papers/AAAI-1063.ArikS.pdf](https://www.aaai.org/AAAI21Papers/AAAI-1063.ArikS.pdf))，它承诺可以帮助你选择和处理相关特征，并以智能的方式处理分类和数值特征。尽管调整超参数的选项不多，但未调整的网络和调整后的网络之间的结果可能会有很大差异（因此需要花时间让它发挥最佳效果）。这里有一些实现，例如出色的`pytorch-tabnet`包
    ([https://github.com/dreamquark-ai/tabnet](https://github.com/dreamquark-ai/tabnet))
    或由*Yirun Zhang* 编码的实现，可以在[https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-training](https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-training)
    和 [https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference](https://www.kaggle.com/ludovick/introduction-to-tabnet-kfold-10-inference)
    找到。这两个都是为*机制作用（MoA）预测*竞赛设计的。'
- en: '**Neural Oblivious Decision Ensembles** (**NODE**) is an architecture that
    tries to mimic in a neural network how a decision tree works (Popov, S., Morozov,
    S., and Babenko, A. *Neural oblivious decision ensembles for deep learning on
    tabular data*. arXiv preprint arXiv:1909.06312, 2019\. [https://arxiv.org/abs/1909.06312](https://arxiv.org/abs/1909.06312)).
    You can use the implementation offered by Yirun Zhang for TensorFlow at [https://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras](https://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras)
    or for PyTorch at [https://www.kaggle.com/gogo827jz/moa-public-pytorch-node](https://www.kaggle.com/gogo827jz/moa-public-pytorch-node).'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**神经无意识决策集成** (**NODE**) 是一种试图在神经网络中模仿决策树工作方式的架构（Popov, S., Morozov, S., 和
    Babenko, A. *Neural oblivious decision ensembles for deep learning on tabular
    data*. arXiv预印本 arXiv:1909.06312, 2019\. [https://arxiv.org/abs/1909.06312](https://arxiv.org/abs/1909.06312))。你可以使用Yirun
    Zhang为TensorFlow提供的实现，链接为[https://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras](https://www.kaggle.com/gogo827jz/moa-neural-oblivious-decision-ensembles-tf-keras)，或者为PyTorch提供的实现，链接为[https://www.kaggle.com/gogo827jz/moa-public-pytorch-node](https://www.kaggle.com/gogo827jz/moa-public-pytorch-node)。'
- en: You can use a wide range of models, such as Wide & Deep, DeepFM, xDeepFM, AutoInt,
    and many others based on factorization machines and mostly devised for click-through
    rate estimation. You don’t have to build all these neural architectures by yourself;
    you can rely on packages such as DeepCTR ([https://github.com/shenweichen/DeepCTR](https://github.com/shenweichen/DeepCTR))
    or DeepTables ([https://github.com/DataCanvasIO/deeptables](https://github.com/DataCanvasIO/deeptables))
    as suggested by *Changhao Lee* ([https://www.kaggle.com/leechh](https://www.kaggle.com/leechh))
    and *Jian Yang* ([https://www.kaggle.com/jackguagua](https://www.kaggle.com/jackguagua)),
    second and first place respectively in the *Categorical Feature Encoding Challenge
    II* competition.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用多种模型，例如Wide & Deep、DeepFM、xDeepFM、AutoInt以及许多基于因子分解机且主要用于点击率估计的模型。你不必自己构建所有这些神经网络架构；你可以依赖如DeepCTR
    ([https://github.com/shenweichen/DeepCTR](https://github.com/shenweichen/DeepCTR))
    或DeepTables ([https://github.com/DataCanvasIO/deeptables](https://github.com/DataCanvasIO/deeptables))
    这样的包，正如*Changhao Lee* ([https://www.kaggle.com/leechh](https://www.kaggle.com/leechh))
    和*Jian Yang* ([https://www.kaggle.com/jackguagua](https://www.kaggle.com/jackguagua))
    所建议的，他们分别在*Categorical Feature Encoding Challenge II*竞赛中获得了第二和第一名。
- en: 'In conclusion, you can build your own neural network for tabular data by mixing
    together embedding layers for categorical features and dense layers for numeric
    ones. However, if it doesn’t pay off, you can always rely on quite a wide range
    of good solutions provided by well-written packages. Always be on the lookout
    for a new package appearing: it may help you to perform better both in Kaggle
    competitions and real-world projects. Also, as a piece of advice based on our
    experience, don’t expect a neural network to be the best model in a tabular competition;
    this seldom happens. Instead, blend solutions from classical tabular data models,
    such as gradient boosting models and neural networks, because they tend to pick
    up different signals from the data that you can integrate together in an ensemble.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，你可以通过混合用于分类特征的嵌入层和用于数值特征的密集层来为表格数据构建自己的神经网络。然而，如果这样做没有效果，你总是可以依赖由编写良好的包提供的相当广泛的良好解决方案。始终关注新包的出现：它可能有助于你在Kaggle竞赛和现实世界项目中表现更好。此外，根据我们的经验，不要期望神经网络在表格竞赛中是最好的模型；这种情况很少发生。相反，混合来自经典表格数据模型（如梯度提升模型和神经网络）的解决方案，因为它们倾向于从数据中提取不同的信号，你可以将这些信号集成在一起进行集成。
- en: '![](img/Jean-Francois_Puget.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Jean-Francois_Puget.png)'
- en: Jean-François Puget
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Jean-François Puget
- en: '[https://www.kaggle.com/cpmpml](https://www.kaggle.com/cpmpml)'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.kaggle.com/cpmpml](https://www.kaggle.com/cpmpml)'
- en: We spoke to Jean-François Puget, aka CPMP, about the importance of reproducibility,
    how to work with data, his best competition, and more. As a Kaggle Grandmaster
    in Competitions and Discussions, and a Distinguished Engineer at RAPIDS, NVIDIA,
    he had many good insights to share with us. The editor particularly likes what
    he has to say about the scientific method.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们与Jean-François Puget，又名CPMP，讨论了可重复性的重要性、如何处理数据、他最好的竞赛以及更多内容。作为Kaggle竞赛和讨论的大师级人物，以及NVIDIA
    RAPIDS的杰出工程师，他与我们分享了众多宝贵的见解。编辑特别喜欢他关于科学方法的看法。
- en: What’s your favorite kind of competition and why? In terms of techniques and
    solving approaches, what is your specialty on Kaggle?
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你最喜欢的竞赛类型是什么？为什么？在技术和解决方法方面，你在Kaggle上的专长是什么？
- en: '*I like competitions with a scientific background, or a background I can relate
    to. I dislike anonymous data and synthetic data, unless the data is generated
    via a very precise physics simulation. More generally, I like Kaggle competitions
    on domains I don’t know much about, as this is where I will learn the most. It
    is not the most effective way to get ranking points, but it is the one I entertain
    most.*'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*我喜欢具有科学背景或我能与之相关联的背景的竞赛。我不喜欢匿名数据和合成数据，除非数据是通过非常精确的物理模拟生成的。更普遍地说，我喜欢我对不太了解的领域的Kaggle竞赛，因为这是我将学到最多东西的地方。这并不是获得排名积分最有效的方法，但这是我最感兴趣的方法。*'
- en: How do you approach a Kaggle competition? How different is this approach to
    what you do in your day-to-day work?
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 你是如何处理Kaggle竞赛的？这种方法与你在日常工作中所做的方法有何不同？
- en: '*I start by looking at data and understanding it as well as possible. I try
    to find patterns in it, especially predictive patterns. What I often do is plot
    samples using two features or derived features on the x and y axis, and a third
    feature for color coding samples. One of the three features can be the target.
    I use lots of visualization, as I believe that human vision is the best data analysis
    tool there is.*'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '*我先从查看数据并尽可能理解它开始。我试图在其中找到模式，特别是预测性模式。我经常做的事情是使用两个特征或派生特征在x轴和y轴上绘制样本，并为样本着色编码使用第三个特征。这三个特征中的一个可以是目标。我使用了大量的可视化，因为我相信人类的视觉是最佳的数据分析工具。*'
- en: '*The second thing I spend time on is how to assess model or pipeline performance.
    Indeed, it is extremely important to be able to evaluate the performance of a
    model as accurately as possible. There is no surprise here; evaluation is often
    a variant of k-fold cross-validation. But the fold definition can be tailored
    to the competition type (time-based folds for forecasting competitions, group
    k-fold when samples are linked together for some reason, e.g., actions with the
    same user ID, etc.).*'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '*我花时间做的第二件事是如何评估模型或管道的性能。确实，能够尽可能准确地评估模型性能非常重要。这并不令人惊讶；评估通常是k折交叉验证的一种变体。但是，折的定义可以根据竞赛类型进行调整（基于时间的折用于预测竞赛，当样本由于某种原因相互关联时，例如，具有相同用户ID的操作，可以使用分组k折）。*'
- en: '*I then create an end-to-end baseline that goes from data to submission, and
    try it. If this is a code competition, then testing that you have gotten your
    pipeline right is key.*'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '*我随后创建了一个从数据到提交的端到端基线，并尝试了一下。如果这是一个代码竞赛，那么测试你是否正确设置了你的流水线是关键。*'
- en: '*Then I try more complex models (if using deep learning models), or more features
    (if using XGBoost or other models from RAPIDS or sklearn). I submit these to see
    if there is a correlation between my local evaluation score and the public test
    score. If the correlation is good, then I submit less and less.*'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '*然后我尝试更复杂的模型（如果使用深度学习模型），或者更多的特征（如果使用XGBoost或其他来自RAPIDS或sklearn的模型）。我将这些提交以查看我的本地评估分数和公共测试分数之间是否存在相关性。如果相关性良好，那么我就越来越少地提交。*'
- en: '*After a few weeks, I spend time doing hyperparameter tuning. But I do it only
    once, or maybe twice with a last tuning near the end of the competition. Indeed,
    hyperparameter tuning is one of the best ways to overfit, and I fear overfitting
    a lot.*'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '*几周后，我花时间进行超参数调整。但我只做一次，或者最多两次，在竞赛接近尾声时进行最后一次调整。实际上，超参数调整是过度拟合的最佳方式之一，我非常担心过度拟合。*'
- en: Tell us about a particularly challenging competition you entered, and what insights
    you used to tackle the task.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 请告诉我们你参加的一个特别具有挑战性的竞赛，以及你使用了哪些见解来应对这项任务。
- en: '*One of the competitions I am the most proud of is the* TalkingData AdTracking
    Fraud Detection Challenge *competition, where we had a very large volume of click
    history and we had to predict which clicks led to some app downloads. There were
    very few features and a large number of rows (like half a billion). At the time
    I only had a 64 GB machine, and I had to implement a very efficient way to create
    new features and evaluate them. I had a few insights in this competition. First,
    that the click that led to an app download was the last click on the app download
    page for a user. Therefore, the “time to next click from the same user on the
    same app” was the most important feature. A derived insight was this: there were
    quite a number of clicks from the same user and app with the same timestamp. I
    hypothesized that the one with a download, if any, was the last one. A third insight
    was to use a matrix factorization approach to approximate feature value co-occurrences.
    I implemented a libFM model in Keras at the time, and adding the latent vectors
    as features helped. The only other team doing this was the top team. With this,
    I got a solo 6th place among teams of GMs. I was not a Kaggle GM yet.*'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*我最自豪的竞赛之一是* TalkingData AdTracking Fraud Detection Challenge *竞赛，我们拥有大量的点击历史记录，我们必须预测哪些点击导致了某些应用程序的下载。特征非常少，行数非常多（如五亿行）。当时我只有一台64GB的机器，我必须实现一种非常高效的方法来创建新特征并评估它们。在这个竞赛中，我有一些见解。首先，导致应用程序下载的点击是用户在应用程序下载页面上的最后一个点击。因此，“同一用户在相同应用程序上的下一次点击时间”是最重要的特征。一个派生的见解是：有相当多的来自同一用户和应用程序的点击具有相同的戳记。我假设如果有下载，那么它将是最后一个。第三个见解是使用矩阵分解方法来近似特征值共现。当时我在Keras中实现了一个libFM模型，添加潜在向量作为特征有帮助。唯一其他这样做的是排名第一的团队。有了这个，我在GM团队中获得了第六名。我那时还不是Kaggle
    GM。*'
- en: Has Kaggle helped you in your career? If so, how?
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle是否帮助你在职业生涯中取得进展？如果是的话，是如何帮助的？
- en: '*Kaggle helped me twice. At IBM, Kaggle was a great source of knowledge on
    SOTA machine learning practices. I used that knowledge to inform and guide the
    development of IBM machine learning tooling (IBM Watson Studio and IBM Watson
    Machine Learning).*'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: '*Kaggle帮助了我两次。在IBM，Kaggle是SOTA机器学习实践的宝贵知识来源。我利用这些知识来指导和开发IBM机器学习工具（IBM Watson
    Studio和IBM Watson Machine Learning）。*'
- en: '*For instance, I managed to have IBM support Python packages in 2016 at a time
    when IBM was a Java/Scala powerhouse. Without me, IBM would have bet on Spark
    and Scala for machine learning, and would have missed the Python wave entirely.
    I also pushed for XGBoost very early, when IBM wanted to only support Spark ML
    or TensorFlow.*'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '*例如，我设法在2016年让IBM支持Python包，当时IBM是Java/Scala的强大力量。如果没有我，IBM可能会押注Spark和Scala进行机器学习，并完全错过Python浪潮。我还非常早期地推动了XGBoost，当时IBM只想支持Spark
    ML或TensorFlow。*'
- en: '*The second time Kaggle helped me was for getting my current job. NVIDIA was
    looking for Kaggle competition GMs with good social presence to help promote the
    NVIDIA stack, including the RAPIDS GPU accelerated ML package.*'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '*第二次Kaggle帮助我找到了我的当前工作。NVIDIA正在寻找具有良好社交影响力的Kaggle竞赛GM，以帮助推广NVIDIA堆栈，包括RAPIDS
    GPU加速机器学习包。*'
- en: In your experience, what do inexperienced Kagglers often overlook? What do you
    know now that you wish you’d known when you first started?
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的经验中，不经验证的Kagglers通常会忽略什么？现在您知道什么，而您希望在最初开始时就知道？
- en: '*The one thing that differentiates Kagglers from other data scientists is the
    evaluation of model performance. Kagglers need to master this, because if they
    don’t, then they choose submissions that look great on the public leaderboard
    but perform poorly on the private leaderboard. Once a Kaggler knows how to build
    models that perform well on the private leaderboard, then they know how to build
    models that perform well on new data, i.e., models that do not overfit.*'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '*区分Kagglers和其他数据科学家的一点是模型性能的评估。Kagglers需要掌握这一点，因为如果不掌握，他们可能会选择在公共排行榜上看起来很好但在私有排行榜上表现不佳的提交。一旦Kaggler知道如何构建在私有排行榜上表现良好的模型，那么他们也知道如何构建在新数据上表现良好的模型，即不会过拟合的模型。*'
- en: '*The other thing that inexperienced Kagglers do is to ask if method/model X
    can work in a given competition. My answer to this is always, “Try it and see
    if it works or not.” People often miss that machine learning is an experimental
    science. In order to build good models, one must follow the scientific method:*'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*不经验证的Kagglers通常会询问方法/模型X是否可以在特定的竞赛中工作。我的回答总是，“试一试看看是否可行。”人们常常忽略机器学习是一个实验科学。为了构建好的模型，必须遵循科学方法：*'
- en: '*Make a hypothesis (e.g., adding this feature, or adding this NN layer, will
    improve pipeline performance)*'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提出一个假设（例如，添加这个特征，或者添加这个神经网络层，将提高流水线性能）*'
- en: '*Run an experiment to test the hypothesis (train the modified pipeline)*'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*运行一个实验来测试假设（训练修改后的流水线）*'
- en: '*Analyze experiment results (is CV score better than before? Where is it better?
    Where is it worse?)*'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分析实验结果（交叉验证分数是否比之前更好？哪里更好？哪里更差？）*'
- en: '*Each experiment should be done so that it can confirm or reject a hypothesis.
    For this, an experiment should change only one thing at a time. Often, inexperienced
    people change many things, then cannot conclude what worked or not.*'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '*每个实验都应该进行，以便可以确认或拒绝一个假设。为此，实验应该一次只改变一件事。通常，不经验的人会改变很多事，然后无法得出什么有效或无效的结论。*'
- en: Are there any particular tools or libraries that you would recommend using for
    data analysis and machine learning?
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 您会推荐使用哪些特定的工具或库来进行数据分析与机器学习？
- en: '*I use Matplotlib plots mostly for data exploration. I do data wrangling in
    Pandas if the dataset is small, or in cuDF (from RAPIDS) if the dataset is large.
    For machine learning, I use cuML from RAPIDS, XGBoost with GPU acceleration, and
    PyTorch. If possible, I will use pretrained models, for instance NLP models from
    Hugging Face, or image classification models from the timm package.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '*我主要使用Matplotlib进行数据探索。如果数据集较小，我在Pandas中进行数据处理；如果数据集较大，我在cuDF（来自RAPIDS）中进行。对于机器学习，我使用RAPIDS的cuML，带有GPU加速的XGBoost，以及PyTorch。如果可能，我会使用预训练模型，例如来自Hugging
    Face的自然语言处理模型，或者来自timm包的图像分类模型。*'
- en: What’s the most important thing someone should keep in mind or do when they’re
    entering a competition?
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们参加竞赛时，应该牢记或做什么最重要？
- en: '*Make sure you can spend enough time on it.*'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '*确保你有足够的时间投入其中。*'
- en: Summary
  id: totrans-322
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed tabular competitions on Kaggle. Since most
    of the knowledge applicable in a tabular competition overlaps with standard data
    science knowledge and practices, we have focused our attention on techniques more
    specific to Kaggle.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了Kaggle上的表格竞赛。由于在表格竞赛中适用的多数知识都与标准数据科学知识和实践重叠，因此我们专注于Kaggle更具体的技巧。
- en: Starting from the recently introduced Tabular Playground Series, we touched
    on topics relating to reproducibility, EDA, feature engineering, feature selection,
    target encoding, pseudo-labeling, and neural networks applied to tabular datasets.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 从最近推出的表格游乐场系列开始，我们触及了与可重复性、EDA、特征工程、特征选择、目标编码、伪标签以及应用于表格数据集的神经网络相关的话题。
- en: EDA is a crucial phase if you want to get insights on how to win a competition.
    It is also quite unstructured and heavily dependent on the kind of data you have.
    Aside from giving you general advice on EDA, we brought your attention to techniques
    such as t-SNE and UMAP that can summarize your entire dataset at a glance. The
    next phase, feature engineering, is also strongly dependent on the kind of data
    you are working on. We therefore provided a series of possible feature engineering
    ideas that you can try applying to your specific case. As for feature selection,
    after a brief overview, we drew your attention to techniques based on feature
    importance and randomization, which can be applied to almost any machine learning
    algorithm.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: EDA（探索性数据分析）是如果您想了解如何赢得比赛的关键阶段。它也非常不结构化，并且高度依赖于您拥有的数据类型。除了给您提供EDA的一般建议外，我们还引起了您对t-SNE和UMAP等技术的注意，这些技术可以一眼总结您的整个数据集。下一个阶段，特征工程，也强烈依赖于您正在处理的数据类型。因此，我们提供了一系列可能的特征工程想法，您可以尝试将其应用于您的特定案例。至于特征选择，在简要概述后，我们引起了您对基于特征重要性和随机化的技术的注意，这些技术几乎可以应用于任何机器学习算法。
- en: 'After explaining target encoding, which we wanted to point out cannot be dealt
    with in an automated way, we moved on to special techniques that you probably
    won’t apply in your real-world projects but that can work very well in Kaggle
    competitions: pseudo-labeling and denoising autoencoders for tabular competitions.
    Finally, after discussing how categorical features can also be dealt with using
    embedding layers in neural networks, we gave you a quick overview of the pre-made
    neural architectures that could work for tabular data.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释了目标编码后，我们指出这不能以自动化的方式处理，然后我们转向了一些你可能不会在实际项目中应用但可以在Kaggle竞赛中非常有效的特殊技术：用于表格竞赛的伪标签和去噪自编码器。最后，在讨论了如何使用神经网络中的嵌入层处理分类特征之后，我们为您快速概述了适用于表格数据的预制神经网络架构。
- en: In the next chapter, we will complete our overview of all the techniques that
    you need to take on tabular competitions by discussing how best to perform hyperparameter
    optimization.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将通过讨论如何最佳地进行超参数优化，来完成对您在表格竞赛中需要采用的所有技术的概述。
- en: Join our book’s Discord space
  id: totrans-328
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的Discord空间
- en: 'Join the book’s Discord workspace for a monthly *Ask me Anything* session with
    the authors:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 加入本书的Discord工作空间，参加每月一次的作者“问我任何问题”活动：
- en: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/KaggleDiscord](https://packt.link/KaggleDiscord)'
- en: '![](img/QR_Code40480600921811704671.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code40480600921811704671.png)'
