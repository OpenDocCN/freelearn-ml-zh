- en: Chapter 7. Overview of Machine Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。机器学习技术概述
- en: There are different machine learning techniques and this chapter gives an overview
    of the most relevant ones. Some of them have already been introduced in the previous
    chapters and some are new.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同的机器学习技术，本章将概述最相关的技术。其中一些已经在前面的章节中介绍过，一些是新的。
- en: 'In this chapter, you will learn the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习以下主题：
- en: 'The most relevant branches of techniques: supervised and unsupervised learning'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最相关的技术分支：监督学习和无监督学习
- en: Making predictions with supervised learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用监督学习进行预测
- en: Identifying hidden patterns and structures with unsupervised learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用无监督学习识别隐藏的模式和结构
- en: Pros and cons of these techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些技术的优缺点
- en: Overview
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: 'There are different categories of machine learning techniques and in this chapter
    we will see the two most relevant branches—supervised and unsupervised learning,
    as shown in the following figure:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习技术有不同的类别，在本章中我们将看到两个最相关的分支——监督学习和无监督学习，如下所示：
- en: '![Overview](img/7740OS_07_01.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![概述](img/7740OS_07_01.jpg)'
- en: 'The supervised and unsupervised learning techniques deal with objects described
    by features. An example of supervised learning techniques is decision tree learning,
    and an example of unsupervised technique is k-means. In both cases, the algorithms
    learn from a set of objects and the difference is their target: supervised techniques
    predict attributes whose nature is already known and unsupervised techniques identify
    new patterns.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习技术处理由特征描述的对象。监督学习技术的例子是决策树学习，无监督技术的例子是k-means。在这两种情况下，算法都是从一组对象中学习的，区别在于它们的目标：监督技术预测已知性质的属性，而无监督技术识别新的模式。
- en: 'The supervised learning techniques predict an attribute of the objects. The
    algorithms learn from a training set of objects whose attribute is known and they
    predict the attribute of other objects. There are two categories of supervised
    learning techniques: classification and regression. We talk about classification
    if the predicted attribute is categoric and about regression if the attribute
    is numeric.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习技术预测对象的属性。算法从已知属性的训练集对象中学习，并预测其他对象的属性。监督学习技术分为两类：分类和回归。如果预测的属性是分类的，我们谈论分类；如果属性是数值的，我们谈论回归。
- en: 'The unsupervised learning techniques identify patterns and structures of a
    set of objects. The two main branches of unsupervised learning are clustering
    and dimensionality reduction. The clustering techniques identify homogeneous groups
    of objects on the basis of their attributes and an example is k-means. Dimensionality
    reduction techniques identify a small set of significant features describing the
    objects and an example is the principal component analysis. The difference between
    clustering and dimensionality reduction depends on the identified attribute that
    is categoric or numeric respectively, as shown in the following figure:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习技术识别一组对象的模式和结构。无监督学习的两个主要分支是聚类和降维。聚类技术根据对象的属性识别同质群体，例如k-means。降维技术识别一组描述对象的显著特征，例如主成分分析。聚类和降维之间的区别在于所识别的属性是分类的或数值的，如下所示：
- en: '![Overview](img/7740OS_07_02.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![概述](img/7740OS_07_02.jpg)'
- en: This chapter will show you some popular techniques for each branch. In order
    to illustrate the techniques, we will reuse the flag dataset of [Chapters 4](ch04.html
    "Chapter 4. Step 1 – Data Exploration and Feature Engineering"), *Step 1 – Data
    Exploration and Feature Engineering*; [Chapter 5](ch05.html "Chapter 5. Step 2
    – Applying Machine Learning Techniques"), *Step 2 – Applying Machine Learning
    Techniques*; and [Chapter 6](ch06.html "Chapter 6. Step 3 – Validating the Results"),
    *Step 3 – Validating the Results* that can be found in the supporting code bundle
    with this book.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将展示每个分支的一些流行技术。为了说明技术，我们将重复使用[第4章](ch04.html "第4章。步骤1 – 数据探索和特征工程")、*步骤1 –
    数据探索和特征工程*；[第5章](ch05.html "第5章。步骤2 – 应用机器学习技术")、*步骤2 – 应用机器学习技术*；以及[第6章](ch06.html
    "第6章。步骤3 – 验证结果")、*步骤3 – 验证结果*中的标志数据集，这些数据集可以在本书的支持代码包中找到。
- en: Supervised learning
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 监督学习
- en: This chapter will show you some examples of popular supervised learning algorithms.
    These techniques are very useful for facing business problems because they make
    predictions about future attributes and outcomes. In addition, it is possible
    to measure the accuracy of each technique and/or parameter in order to choose
    the most suitable one and set it up in the best way.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将向您展示一些流行的监督学习算法的示例。这些技术在面对商业问题时非常有用，因为它们可以预测未来的属性和结果。此外，可以测量每种技术及其/或参数的准确性，以便选择最合适的技术并以最佳方式设置它。
- en: 'As anticipated, there are two categories of techniques: classification and
    regression. However, most of the techniques can be used in both the contexts.
    Each of the following subsections introduces a different algorithm.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，有两种技术类别：分类和回归。然而，大多数技术都可以在这两种情况下使用。以下每个小节介绍一个不同的算法。
- en: The k-nearest neighbors algorithm
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: K最近邻算法
- en: KNN is a supervised learning algorithm that performs classification or regression.
    Given a new object, the algorithm predicts its attribute starting from its *k*
    neighbors that are its most similar objects. KNN is a lazy learning algorithm
    in the sense that it directly queries the training data to make a prediction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: K最近邻算法是一种监督学习算法，用于分类或回归。给定一个新对象，算法从其最相似的**k**个邻居对象预测其属性。K最近邻算法是一种懒惰学习算法，因为它直接查询训练数据来做出预测。
- en: In the case of a categoric attribute, the algorithm estimates it as the most
    common among the similar objects. In the case of a numeric attribute, it computes
    the median or average between them. In order to state which are the *k* most *similar*
    objects, KNN uses a similarity function that evaluates how similar two objects
    are. In order to measure similarity, the starting point is often a distance matrix
    expressing the dissimilarity. Then, the algorithm computes the similarity between
    the new object and each other and picks the *k* most similar objects.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类属性的情况下，算法将其估计为相似对象中最常见的。在数值属性的情况下，它计算它们之间的中位数或平均值。为了说明哪些是**k**个最**相似**的对象，KNN使用一个相似性函数来评估两个对象有多相似。为了测量相似性，起点通常是一个表示差异的距离矩阵。然后，算法计算新对象与每个其他对象的相似性，并选择**k**个最相似的对象。
- en: In our example, we will use the flag dataset and the features are the number
    of stripes and the number of colors in the flags. The attribute that we want to
    predict starting from its flag attributes is the language of a new country.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将使用国旗数据集，特征是国旗上的条纹数量和颜色数量。我们想要从其国旗属性预测的属性是新国家的语言。
- en: 'The training set is composed of some countries in such a way that there are
    no two countries with the same flag features. First, let''s visualize the data.
    We can show the countries in a chart whose dimensions are the two features and
    whose color is the language, as follows:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练集由一些国家组成，这些国家没有两个国家的国旗特征相同。首先，让我们可视化这些数据。我们可以显示国家在图表中，其维度是两个特征，颜色是语言，如下所示：
- en: '![The k-nearest neighbors algorithm](img/7740OS_07_03.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![K最近邻算法](img/7740OS_07_03.jpg)'
- en: 'We have two new countries with:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个新国家：
- en: 7 stripes and 4 colors
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 7条条纹和4种颜色
- en: 3 stripes and 7 colors
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3条条纹和7种颜色
- en: 'We want to determine the language of two new countries using a 4-nearest-neighbor
    algorithm. We can add the two countries to the chart and determine the 4 closest
    points for each of them, as shown here:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想使用4-最近邻算法确定两个新国家的语言。我们可以将这两个国家添加到图表中，并确定每个国家的4个最近点，如下所示：
- en: '![The k-nearest neighbors algorithm](img/7740OS_07_04.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![K最近邻算法](img/7740OS_07_04.jpg)'
- en: 'With regard to the country on the right-hand side of the chart, all its 4 closest
    neighbors belong to **Others**, so we estimate that its language is **Others**.
    The other country has a mixed neighborhood: 1 English, 1 Other Indo-European,
    and 2 Spanish countries. The most common language is Spanish, so we estimate that
    it is a Spanish-speaking country.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 关于图表右侧的国家，其最近的4个邻居都属于**其他**类别，因此我们估计该国的语言为**其他**。另一个国家的邻域是混合的：1个英语国家，1个其他印欧语系国家，以及2个西班牙国家。最常见的语言是西班牙语，所以我们估计它是一个讲西班牙语的国家。
- en: The KNN is a simple and scalable algorithm that achieves good results in many
    contexts. However, in the presence of many features, the similarity function takes
    account of all of them, including the less relevant, making it difficult to use
    the distance. In that context, the KNN is not able to identify the meaningful
    nearest neighbors and this issue is called the curse of dimensionality. A solution
    is to reduce the dimensionality by selecting the most relevant features or using
    a dimensionality reduction technique (this is the topic of the next section).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: KNN是一种简单且可扩展的算法，在许多情况下都能取得良好的结果。然而，在存在许多特征的情况下，相似度函数考虑了所有这些特征，包括不那么相关的特征，这使得使用距离变得困难。在这种情况下，KNN无法识别有意义的最近邻，这个问题被称为维度诅咒。一种解决方案是通过选择最相关的特征或使用降维技术来降低维度（这是下一节的主题）。
- en: Decision tree learning
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树学习
- en: Decision tree learning is a supervised learning algorithm that builds a classification
    or regression tree. Each leaf of the tree represents the attribute estimation
    and each node splits the data accordingly with a condition of the features.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习是一种监督学习算法，它构建一个分类或回归树。树的每个叶子节点代表属性估计，每个节点根据特征的某个条件对数据进行分割。
- en: The decision tree learning is an eager method in the sense that it uses a training
    set to build a model that doesn't require you to query the data. All the other
    supervised learning techniques are eager as well.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习是一种贪婪方法，因为它使用训练集来构建一个不需要你查询数据的模型。所有其他监督学习技术也都是贪婪的。
- en: The target of the algorithm is to define the most relevant feature and split
    the set in two groups accordingly with it. Then, for each group, the algorithm
    identifies its most relevant feature and divides the objects of the groups into
    two parts. This procedure goes on until we identify the leaves as small groups
    of objects. For each leaf, the algorithm estimates the feature as a mode, if it
    is categoric, or average, if it is numeric. After building the tree, if we have
    too many leaves, we can define a level in which we stop splitting the tree. In
    this way, each leaf will contain a reasonably big group. This procedure of stopping
    splitting is called pruning. In this way, we find a less complex and more accurate
    prediction.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的目标是定义最相关的特征，并根据它将集合分成两组。然后，对于每个组，算法识别其最相关的特征，并将组中的对象分成两部分。这个过程一直进行，直到我们识别出叶子节点作为对象的小组。对于每个叶子节点，如果它是分类的，算法估计特征为众数；如果是数值的，则估计为平均值。在构建树之后，如果我们有太多的叶子节点，我们可以定义一个停止分割树的级别。这样，每个叶子节点将包含一个合理大的组。这种停止分割的过程称为剪枝。通过这种方式，我们找到了一个更简单且更准确的预测。
- en: 'In our example, we want to determine the language of a new country starting
    from different flag attributes, such as colors and patterns. The algorithm builds
    the tree learning from a training set. Let''s visualize it:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们想要根据不同的旗帜属性（如颜色和图案）确定一个新国家的语言。算法从训练集构建树学习。让我们可视化它：
- en: '![Decision tree learning](img/7740OS_07_05.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![决策树学习](img/7740OS_07_05.jpg)'
- en: In any node, if the answer is **true**, we go to the left, and if the answer
    is **false**, we go to the right. First, the model identifies the most relevant
    attribute that is **saltires**. If a flag contains a saltire, we go to the left
    and we determine that the related country is English. Otherwise, we go to the
    right and we check if the flag contains the blue color. Then, we go on checking
    the conditions until we reach a leaf.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何节点，如果答案是**true**，我们向左走，如果答案是**false**，我们向右走。首先，模型识别出最相关的属性是**十字形**。如果一个旗帜包含十字形，我们向左走，并确定相关国家是英国。否则，我们向右走，检查旗帜是否包含蓝色。然后，我们继续检查条件，直到达到叶子节点。
- en: Let's suppose that we built the tree without taking account of the Spanish flag.
    How do we estimate the language of Spain? Starting from the top, we check the
    conditions on each node we encounter.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们没有考虑西班牙国旗来构建树。我们如何估计西班牙的语言？从顶部开始，我们检查遇到的每个节点的条件。
- en: 'These are the steps:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是步骤：
- en: The flag doesn't contain a saltire, so we go to the left.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旗帜上不包含十字形，所以我们向左走。
- en: The flag contains the blue color, so we go to the right.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旗帜包含蓝色，所以我们向右走。
- en: The flag doesn't contain a cross, so `crosses = no` is `true` and we go to the
    left.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旗帜上不包含十字架，所以`crosses = no`为`true`，我们向左走。
- en: The flag doesn't contain an animated image, so we go to the right.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 旗帜上不包含动画图像，所以我们向右走。
- en: The flag has two main colors, so `number of colors not equal to 4 or 5` is `true`
    and we go to the left.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 国旗有两种主要颜色，所以`number of colors not equal to 4 or 5`是`true`，我们向左移动。
- en: The flag doesn't contain any bars, so we go to the left.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 国旗没有任何条形，所以我们向左移动。
- en: 'The flag doesn''t have any vertical stripes, so `nStrp0 = no` is `true` and
    we go to the left, as shown here:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 国旗没有垂直条纹，所以`nStrp0 = no`是`true`，我们向左移动，如图所示：
- en: '![Decision tree learning](img/7740OS_07_06.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![决策树学习](img/7740OS_07_06.jpg)'
- en: In the end, the estimated language is `Spanish`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，估计的语言是`西班牙语`。
- en: The decision tree learning can deal with numeric and/or categoric features and
    attributes, so it can be applied in different contexts with just a little data
    preparation. In addition, it is applicable when there are many features, different
    from other algorithms. A disadvantage is that the algorithm can overfit in the
    sense that the model is too close to the data and is more complicated than the
    reality, although pruning can help with this.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树学习可以处理数值和/或分类特征和属性，因此它可以在只需要少量数据准备的不同环境中应用。此外，它适用于有大量特征的情况，这与其他算法不同。一个缺点是算法可能会过拟合，即模型过于接近数据并且比现实更复杂，尽管剪枝可以帮助解决这个问题。
- en: Linear regression
  id: totrans-50
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: Linear regression is a statistical model identifying a relationship between
    numeric variables. Given a set of objects described by the *y* attribute and the
    `x1, …,` and `xn` features, the model defines a relationship between the features
    and the attribute. The relationship is described by the linear function *y = a0
    + a1 * x1 + … + an * xn*, and `a0, …,` and `an` are parameters defined by the
    method in such a way that the relationship is as close as possible to the data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种统计模型，用于识别数值变量之间的关系。给定一组由*y*属性和`x1, …,`和`xn`特征描述的对象，该模型定义了特征与属性之间的关系。这种关系由线性函数*y
    = a0 + a1 * x1 + … + an * xn*描述，而`a0, …,`和`an`是由方法定义的参数，使得关系尽可能接近数据。
- en: In the case of machine learning, linear regression can be used to predict a
    numeric attribute. The algorithm learns from the training dataset to determine
    the parameters. Then, given a new object, the model inserts its features into
    the linear function to estimate the attribute.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的情况下，线性回归可以用来预测数值属性。算法从训练数据集中学习以确定参数。然后，给定一个新的对象，模型将它的特征插入到线性函数中以估计属性。
- en: 'In our example, we want to estimate the population of a country starting from
    its area. First, let''s visualize the data about the area (in thousand km2) and
    the population (in millions), as shown in the following figure:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们想要从国家的面积估计其人口。首先，让我们可视化面积（以千平方公里为单位）和人口（以百万为单位）的数据，如图下所示：
- en: '![Linear regression](img/7740OS_07_07.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/7740OS_07_07.jpg)'
- en: 'Most of the countries have an area below 3000 thousand km2 and a population
    below 200 million and just a few countries have a much higher area and/or population.
    For this reason, most of the points are concentrated in the bottom-left area of
    the chart. In order to spread the points, we can transform the features using
    the logarithmic area and population, as shown in the following figure:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数国家的面积在3000千平方公里以下，人口在2亿以下，只有少数国家的面积和/或人口要高得多。因此，大多数点都集中在图表的左下角。为了分散点，我们可以使用对数面积和人口来转换特征，如图下所示：
- en: '![Linear regression](img/7740OS_07_08.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/7740OS_07_08.jpg)'
- en: 'The target of linear regression is to identify a linear relationship that is
    as close to the data as possible. In our case, we have two dimensions, so we can
    visualize the relationship using a line. Given the area, the linear regression
    estimates that the population is on the line. Let''s see it in the chart with
    the logarithmic features:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是识别一个尽可能接近数据的线性关系。在我们的例子中，我们有两个维度，因此我们可以用一条线来可视化这种关系。给定区域，线性回归估计人口位于这条线上。让我们在以下图表中查看具有对数特征的示例：
- en: '![Linear regression](img/7740OS_07_09.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/7740OS_07_09.jpg)'
- en: Given a new country about which we know the area, we can estimate its population
    using the regression line. In the chart, there is a new country of which we know
    the area. The linear regression estimates that its point is on the red line.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个关于我们已知其面积的新国家，我们可以使用回归线来估计其人口。在图表中，有一个我们已知其面积的新国家。线性回归估计该点位于红色线上。
- en: Linear regression is a very simple and basic technique. The disadvantage is
    that it requires numeric features and attributes, so there are many contexts in
    which it is not applicable. However, it is possible to convert the categoric features
    into a numeric format using dummy variables or other techniques.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是一种非常简单和基本的技术。缺点是它需要数值特征和属性，因此在许多情况下不适用。然而，可以使用虚拟变量或其他技术将分类特征转换为数值格式。
- en: Another disadvantage is that the model makes strong assumptions on how the features
    and the attributes are related. The function estimating the output is linear,
    so in some contexts it might be far from the real relationship. In addition, if
    in reality the features interact with each other, the model is not able to keep
    track of the interaction. It's possible to solve this problem using a transformation
    that makes the relationship linear. It is also possible to define new features
    expressing non-linear interactions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个缺点是模型对特征和属性之间关系的假设很强。估计输出的函数是线性的，所以在某些情况下，它可能与真实关系相差甚远。此外，如果现实中特征之间相互影响，模型无法跟踪这种影响。可以使用使关系线性的转换来解决此问题。也可以定义新的特征来表示非线性交互。
- en: The linear regression is very basic and it is the starting point of some other
    techniques. For instance, the logistic regression predicts an attribute whose
    value is in the 0 to 1 range.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归非常基础，它是某些其他技术的起点。例如，逻辑回归预测一个值在0到1范围内的属性。
- en: Perceptron
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: '**Artificial Neural Networks** (**ANN**) are the supervised learning techniques
    whose logic is similar to biological neural systems. A simple ANN technique is
    the single-layer perceptron and it is a classification technique estimating a
    binary attribute whose value can be 0 or 1\. The perceptron works like a neuron
    in the sense that it sums the impact of all the inputs and outputs to 1 if the
    sum is above a defined threshold. The model is based on the following parameters:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）是逻辑类似于生物神经系统的监督学习技术。简单的人工神经网络技术是单层感知器，它是一种分类技术，估计一个二进制属性，其值可以是0或1。感知器的工作方式类似于神经元，即它将所有输入的影响相加，如果总和高于定义的阈值，则输出为1。该模型基于以下参数：'
- en: A weight for each feature, defining its impact
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征的权重，定义其影响
- en: A threshold above which the estimated output is 1
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计输出为1的阈值
- en: Starting from the features, the model estimates the attribute through these
    steps
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从特征开始，模型通过以下步骤估计属性
- en: 'Compute the output through a linear regression: multiply each feature by its
    weight and sum all of them'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过线性回归计算输出：将每个特征乘以其权重，并将它们相加
- en: Estimate the attribute to 1 if the output is above the threshold and to 0 otherwise
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果输出高于阈值，则估计属性为1，否则为0
- en: 'The models are as shown in the following figure:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 模型如图所示：
- en: '![Perceptron](img/7740OS_07_10.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/7740OS_07_10.jpg)'
- en: In the beginning, the algorithm builds the perceptron with a defined set of
    coefficients and with a threshold. Then, the algorithm iteratively improves the
    coefficients using the training set. At each step, the algorithm estimates the
    attribute of each object. Then, the algorithm computes the difference between
    the real and the estimated attribute and uses the difference to modify the coefficients.
    In many situations, the algorithm does not reach a stable set of coefficients
    that are not modified, so we need to define at which point we stop. In the end,
    we have a perception defined by a set of coefficients and we can use it to estimate
    the attribute of new objects.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时，算法使用定义好的系数集和阈值构建感知器。然后，算法使用训练集迭代地改进系数。在每一步中，算法估计每个对象的属性。然后，算法计算真实属性和估计属性之间的差异，并使用该差异来修改系数。在许多情况下，算法无法达到一个稳定的系数集，这些系数不再被修改，因此我们需要定义何时停止。最后，我们有一个由系数集定义的感知器，我们可以用它来估计新对象的属性。
- en: 'The perceptron is a simple example of a neural network and it allows us to
    easily understand the impact of the variables. However, the perceptron depends
    on a linear regression, so it is limited in the same way: the feature impact is
    linear and the features can''t interact with each other.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是神经网络的一个简单例子，它使我们能够轻松理解变量的影响。然而，感知器依赖于线性回归，因此它在同一程度上有限：特征影响是线性的，特征不能相互影响。
- en: Ensembles
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成
- en: 'Each algorithm has some weaknesses leading to incorrect results. What if we
    were able to solve the same problem using different algorithms and to pick the
    best result? If just a few algorithms commit the same mistake, we can just ignore
    them. It is not possible to determine which result is correct and which is not,
    but there is another option. By performing supervised learning on a new object,
    we can apply different algorithms and pick the most common or average result among
    them. In this way, if most of the algorithms identify the correct estimation,
    we will take it into account. The ensemble methods are based on this principle:
    they combine different classification or regression algorithms to increase their
    accuracy.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 每个算法都有一些弱点，导致结果不正确。如果我们能够使用不同的算法解决相同的问题并选择最佳结果会怎样？如果只有少数算法犯了同样的错误，我们可以忽略它们。我们无法确定哪个结果是正确的，哪个是错误的，但还有一个选择。通过在新对象上执行监督学习，我们可以应用不同的算法，并从中选择最常见或平均的结果。这样，如果大多数算法识别出正确的估计，我们将考虑它。集成方法基于这个原则：它们结合不同的分类或回归算法以提高准确性。
- en: 'An ensemble method requires variability between the results coming from different
    algorithms and/or training datasets. Some options are:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法需要不同算法和/或训练数据集产生的结果之间的可变性。一些选项包括：
- en: '**Changing the algorithm configuration**: The algorithm is the same and its
    parameters vary within a range.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改变算法配置**：算法是相同的，其参数在一个范围内变化。'
- en: '**Changing the algorithm**: We predict the attribute using different techniques.
    In addition, for each technique, we can use different configurations.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**改变算法**：我们使用不同的技术来预测属性。此外，对于每种技术，我们可以使用不同的配置。'
- en: '**Using different data subsets**: The algorithm is the same and every time
    it learns from a different random subset of the training data.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用不同的数据子集**：算法是相同的，每次它都从训练数据的不同随机子集中学习。'
- en: '**Using different data samples (bagging)**: The algorithm is the same and it
    learns from a bootstrap sample, that is, a set of objects picked randomly from
    the training dataset. The same object can be picked more than once.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用不同的数据样本（袋装）**：算法是相同的，它从自助样本中学习，即从训练数据集中随机选择的一组对象。同一个对象可以被选择多次。'
- en: The final result combines the output of all the algorithms. In the case of classification,
    we use the mode, and in the case of regression, we use the average or median.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 最终结果结合了所有算法的输出。在分类的情况下，我们使用众数，在回归的情况下，我们使用平均值或中位数。
- en: We can build an ensemble algorithm using any combination of supervised learning
    techniques, so there are several options. An example is a random forest that combines
    decision tree learning algorithms using bagging (the technique explained in the
    last bullet point in the previous list).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用任何监督学习技术的组合来构建集成算法，因此有几种选择。一个例子是随机森林，它通过袋装（在上一个列表中的最后一个要点中解释的技术）结合了决策树学习算法。
- en: The ensemble methods often perform much better than the single algorithms. In
    the case of classification, the ensemble removes the biases affecting just a small
    part of the algorithms. However, the logic of different algorithms is often related
    and the same bias might be common. In this case, the ensemble keeps the bias.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法通常比单个算法表现更好。在分类的情况下，集成方法消除了仅影响算法一小部分的偏差。然而，不同算法的逻辑通常是相关的，相同的偏差可能很常见。在这种情况下，集成方法保留了偏差。
- en: The ensemble methods don't always work in the case of regression problems since
    the biases affect the final result. For instance, if there is just an algorithm
    computing a very biased result, the average will be highly affected by that. In
    this context, the median works better since it is much more stable and it is not
    affected by outliers.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法并不总是适用于回归问题，因为偏差会影响最终结果。例如，如果只有一个算法计算出一个非常偏差的结果，平均结果会受到很大影响。在这种情况下，中位数表现更好，因为它更加稳定，并且不受异常值的影响。
- en: Unsupervised learning
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: This chapter shows some unsupervised learning techniques. When facing a business
    problem, these techniques allow us to identify hidden structures and patterns
    and perform exploratory data analysis. In addition, unsupervised learning can
    simplify the problem, allowing us to build more accurate and less elaborated solutions.
    These techniques can also be used in the solution of the problem itself.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 本章展示了某些无监督学习技术。当面对商业问题时，这些技术使我们能够识别隐藏的结构和模式，并执行探索性数据分析。此外，无监督学习可以简化问题，使我们能够构建更准确且更简化的解决方案。这些技术也可以用于解决本身的问题。
- en: The two branches of techniques are clustering and dimensionality reduction and
    most of them are not applicable in both the contexts. This chapter shows some
    popular techniques.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 技术的两个分支是聚类和降维，其中大多数技术不适用于两种上下文。本章展示了某些流行技术。
- en: k-means
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means
- en: k-means is a centroid-based clustering technique. Given a set of objects, the
    algorithm identifies *k* homogeneous clusters. k-means is centroid-based in the
    sense that each cluster is defined by its centroid representing its average object.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: k-means是一种基于质心的聚类技术。给定一组对象，算法识别*k*个同质簇。k-means是基于质心的，因为每个簇由其质心表示，代表其平均对象。
- en: The target of the algorithm is to identify *k* centroids. Then, the k-means
    associates each object to the closest centroid, defining *k* clusters. The algorithm
    starts with a random set of centroids and it iteratively changes them, improving
    the clustering.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的目的是识别*k*个质心。然后，k-means将每个对象关联到最近的质心，定义*k*个簇。算法从一个随机的质心集合开始，并迭代地改变它们，以改进聚类。
- en: 'In our example, the data is about the country flags and the two features are
    the number of stripes and the number of colors. We select a subset of the countries
    in such a way that there are no two flags with the same value of the attributes.
    Our target is to identify two homogeneous groups of countries. The first step
    of the k-means is identifying two random centroids. Let''s visualize the data
    and the centroids in a chart:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，数据是关于国家旗帜的，两个特征是条纹数量和颜色数量。我们选择国家子集的方式是确保没有任何两面旗帜具有相同的属性值。我们的目标是识别两个同质的国家群体。k-means算法的第一步是确定两个随机质心。让我们在图表中可视化数据和质心：
- en: '![k-means](img/7740OS_07_11.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![k-means](img/7740OS_07_11.jpg)'
- en: 'The **o** represents the country flags and the **x** represents the centroids.
    Before running k-means, we need to define a distance that is a way of determining
    dissimilarity between objects. For instance, in the preceding chart, we can use
    the Euclidean distance that expresses the length of the line connecting two points.
    The algorithm is iterative and each step consists of the following steps:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**o**代表国家旗帜，**x**代表质心。在运行k-means之前，我们需要定义一个距离，这是一种确定对象之间差异性的方法。例如，在上面的图表中，我们可以使用欧几里得距离，它表示连接两个点的线段的长度。该算法是迭代的，每一步包括以下步骤：'
- en: For each point, determine the centroid whose distance is the minimum. Then,
    assign the point to the cluster related to the closest centroid.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个点，确定距离最小的质心。然后，将该点分配到与最近质心相关的簇。
- en: Recompute the centroid of each cluster in such a way that it is the average
    between its objects.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以一种方式重新计算每个簇的质心，使其成为其对象的平均值。
- en: 'In the end, we have two clusters with the related centroids representing average
    objects. Let''s visualize them, as shown here:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有两个簇，相关的质心代表平均对象。让我们可视化它们，如图所示：
- en: '![k-means](img/7740OS_07_12.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![k-means](img/7740OS_07_12.jpg)'
- en: The colors represent the clusters and the black **x** represents the final centroids.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色代表簇，黑色**x**代表最终的质心。
- en: k-means is one of the most popular clustering techniques because it is easy
    to understand and it doesn't require a lot of computational power. However, the
    algorithm has some limitations. It contains a random component, so if we run it
    twice on the same set of data it will probably identify different clusters. Another
    disadvantage is that it is not able to identify the clusters in some specific
    contexts, for instance, when the clusters have different sizes or elaborated shapes.
    k-means is a very simple and basic algorithm and it is the starting point to some
    more elaborate techniques.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: k-means是最受欢迎的聚类技术之一，因为它易于理解，并且不需要太多的计算能力。然而，该算法有一些局限性。它包含一个随机成分，因此如果我们对同一组数据运行两次，它可能会识别出不同的聚类。另一个缺点是它无法在特定环境中识别聚类，例如，当聚类具有不同的大小或复杂形状时。k-means是一个非常简单和基本的算法，它是某些更复杂技术的起点。
- en: Hierarchical clustering
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Hierarchical clustering is a branch of clustering techniques. Starting from
    a set of objects, the target builds a hierarchy of clusters. In agglomerative
    hierarchical clustering, each object belongs to a different cluster in the beginning.
    Then, the algorithm merges the clusters until there is one cluster containing
    all the objects. After having identified the hierarchy, we can define the clusters
    and stop their merging at any point.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是聚类技术的一个分支。从一个对象集合开始，目标构建一个聚类层次。在聚合层次聚类中，每个对象最初属于不同的聚类。然后，算法将聚类合并，直到有一个包含所有对象的聚类。在确定了层次之后，我们可以在任何点上定义聚类并停止它们的合并。
- en: 'During each agglomeration step, the algorithm merges the two most similar clusters
    and there are some parameters defining the similarity. First, we need to define
    a way to measure how similar two objects are. There are several options, depending
    on the situation. Then, we need to define the similarity between clusters; the
    methods are called **linkage**. In order to measure the similarity, we start defining
    a distance function that is the opposite. To determine the distance between cluster1
    and cluster2, we measure the distance between every possible object of cluster1
    and every object of cluster2\. Some options to measure the distance between the
    two clusters are:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在每次聚合步骤中，算法将两个最相似的聚类合并，并且有一些参数定义了相似性。首先，我们需要定义一种方法来衡量两个对象之间的相似程度。根据情况，有多种选择。然后，我们需要定义聚类之间的相似性；这些方法被称为**链接**。为了衡量相似性，我们首先定义一个距离函数，它是相反的。为了确定聚类1和聚类2之间的距离，我们测量聚类1中每个可能对象与聚类2中每个对象之间的距离。测量两个聚类之间距离的选项包括：
- en: '**Single linkage**: This is the minimum distance'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接**：这是最小距离'
- en: '**Complete** **linkage**: This is the maximum distance'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全** **链接**：这是最大距离'
- en: '**Average** **linkage**: This is the average distance'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均** **链接**：这是平均距离'
- en: Depending on the linkage, the results of the algorithms will be different.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 根据链接方式的不同，算法的结果也会不同。
- en: 'The example uses the same data as k-means. The country flags are represented
    by the number of stripes and colors and we want to identify homogeneous groups.
    The distance that we use is the Euclidean (just the distance between two points)
    and the linkage is complete. First, let''s identify the clusters from their hierarchy,
    as shown in the following figure:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 该示例使用与k-means相同的数据。国家旗帜由条纹和颜色数量表示，我们希望识别同质群体。我们使用的距离是欧几里得距离（仅仅是两点之间的距离）和链接方式为完全链接。首先，让我们从它们的层次结构中识别聚类，如图所示：
- en: '![Hierarchical clustering](img/7740OS_07_13.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/7740OS_07_13.jpg)'
- en: The chart is called a **dendrogram** and at the bottom of the chart every object
    belongs to a different cluster. Then, going up, we merge the clusters until all
    the objects belong to the same cluster. The height is the distance at which the
    algorithm merges the clusters. For instance, at a height of 3, all the clusters
    whose distance is below 3 are already merged.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 该图表称为**树状图**，图表底部每个对象属于不同的聚类。然后，向上合并聚类，直到所有对象属于同一个聚类。高度是算法合并聚类时的距离。例如，在高度3处，所有距离低于3的聚类已经合并。
- en: 'The red line is at a height of 6 and it defines when we stop merging and below
    it the objects are divided in 4 clusters. Now we can visualize the clusters in
    a chart as follows:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 红线位于高度6处，它定义了何时停止合并，其下方的对象被分为4个聚类。现在我们可以按照以下方式在图表中可视化聚类：
- en: '![Hierarchical clustering](img/7740OS_07_14.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![层次聚类](img/7740OS_07_14.jpg)'
- en: The colors of the points represent the clusters. The algorithm has correctly
    identified the group on the right and has split the group on the left in three
    parts in a good way.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 点的颜色代表簇。算法正确地识别了右侧的组，并且以良好的方式将左侧的组分为三部分。
- en: There are different options for the hierarchic cluster and some of them produce
    very good results in some contexts. Different from the k-means, the algorithm
    is deterministic, so it always leads to the same result.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类有多种选项，其中一些在某些情境下会产生非常好的结果。与k-means不同，该算法是确定性的，因此它总是导致相同的结果。
- en: A big disadvantage of hierarchic cluster is the computational time (`O(n3)`)
    that makes it impossible to apply it on large datasets. Another lack is the manual
    component to choose the algorithm configuration and the dendrogram cut. In order
    to identify a good solution, we usually need to run the algorithm with different
    configurations and to visualize the dendrogram to define its cut.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的缺点之一是计算时间（`O(n^3)`），这使得它无法应用于大型数据集。另一个缺点是需要手动选择算法配置和树状图切割。为了确定一个好的解决方案，我们通常需要用不同的配置运行算法，并可视化树状图以定义其切割。
- en: PCA
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA
- en: '**Principal Components Analysis** (**PCA**) is a statistical procedure transforming
    the features. The PCA logic is based on the concepts of linear correlation and
    variance. In a machine learning context, the PCA is a dimensionality reduction
    technique.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）是一种将特征进行转换的统计过程。PCA的原理基于线性相关性和方差的概念。在机器学习环境中，PCA是一种降维技术。'
- en: Starting with the features describing a set of objects, the target defines other
    variables that are linearly uncorrelated with each other. The output is a new
    set of variables defined as linear combinations of the initial features. In addition,
    the new variables are ranked on the basis of their relevance. The number of the
    new variables is less than or equal to the initial number of features and it is
    possible to select the most relevant features. Then, we are able to define a smaller
    set of features, reducing the problem dimension.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从描述一组对象的特征开始，目标定义了其他彼此线性不相关的变量。输出是一个新的变量集，这些变量定义为初始特征的线性组合。此外，新变量根据其相关性进行排序。新变量的数量小于或等于初始特征的数量，并且可以选择最相关的特征。然后，我们能够定义一组更小的特征，从而降低问题维度。
- en: The algorithm starts defining the feature combination with the highest variance.
    Then, at each step, it iteratively defines another feature combination maximizing
    the variance, under the condition that the new combination is not linearly correlated
    with the others.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 算法从具有最高方差的特征组合开始定义，然后在每一步迭代地定义另一个特征组合，以最大化方差，条件是新组合与其他组合不线性相关。
- en: In the example of [Chapter 4](ch04.html "Chapter 4. Step 1 – Data Exploration
    and Feature Engineering"), *Step 1 – Data Exploration and Feature Engineering*,
    [Chapter 5](ch05.html "Chapter 5. Step 2 – Applying Machine Learning Techniques"),
    *Step 2 – Applying Machine Learning Techniques*, and [Chapter 6](ch06.html "Chapter 6. Step
    3 – Validating the Results"), *Step 3 – Validating the Results*, we have defined
    37 attributes describing each country flag. Applying the PCA, we can define 37
    new attributes defined as linear combination of the variables. The attributes
    are ranked by relevance, so we can select the top six and in this way have a small
    table describing the flag. In this way, we are able to build a supervised learning
    model estimating the language on the basis of six relevant features.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.html "第4章。步骤1 – 数据探索和特征工程")的例子中，*步骤1 – 数据探索和特征工程*，[第5章](ch05.html
    "第5章。步骤2 – 应用机器学习技术")，*步骤2 – 应用机器学习技术*，以及[第6章](ch06.html "第6章。步骤3 – 验证结果")，*步骤3
    – 验证结果*中，我们定义了37个属性来描述每个国家国旗。应用PCA后，我们可以定义37个新的属性，这些属性是变量的线性组合。属性按相关性排序，因此我们可以选择前六个，从而得到一个描述国旗的小表格。这样，我们能够构建一个基于六个相关特征的监督学习模型来估计语言。
- en: In the presence of a lot of features, the PCA allows us to define a smaller
    set of relevant variables. However, this technique is not applicable in all the
    contexts. A lack is that the result depends on how the features are scaled, so
    it is necessary to standardize the variables first.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在存在大量特征的情况下，PCA允许我们定义一组更小的相关变量。然而，这项技术并不适用于所有情境。一个缺点是结果取决于特征的缩放方式，因此有必要首先标准化变量。
- en: Dealing with a supervised learning problem, we can use the PCA to reduce its
    dimensionality. However, the PCA only takes into account the features, ignoring
    how they are related with the attribute to predict, so it might select feature
    combinations that are not very relevant to the problem.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 处理监督学习问题时，我们可以使用PCA来降低其维度。然而，PCA只考虑特征，而忽略了它们与预测属性之间的关系，因此它可能会选择与问题不太相关的特征组合。
- en: Summary
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we learned about the main branches of machine learning techniques:
    supervised and unsupervised learning. We saw how to estimate a numeric or categoric
    attribute using supervised learning techniques such as KNN, decision tree, linear
    regression, and neural networks. We saw that it is possible to increase performance
    using ensembles that are techniques combining different supervised learning algorithms.
    We learned how to identify homogeneous groups using clustering techniques such
    as k-means and hierarchic clustering. We have also understood the importance of
    dimensionality reduction techniques such as the PCA to transform the features
    defining a smaller set of variables.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了机器学习技术的主要分支：监督学习和无监督学习。我们了解了如何使用监督学习技术，如KNN、决策树、线性回归和神经网络来估计数值或分类属性。我们还看到，通过结合不同的监督学习算法的技术，即集成，可以提高性能。我们学习了如何使用k-means和层次聚类等聚类技术来识别同质群体。我们还理解了降维技术，如PCA，对于将定义较小变量集的特征进行转换的重要性。
- en: The next chapter shows an example of a business problem that can be faced using
    machine learning techniques. We will also see examples of both supervised and
    unsupervised learning techniques.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将展示一个可以使用机器学习技术解决的商业问题的例子。我们还将看到监督学习和无监督学习技术的示例。
