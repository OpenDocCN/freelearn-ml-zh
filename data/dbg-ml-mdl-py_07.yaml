- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Decreasing Bias and Achieving Fairness
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 降低偏见和实现公平性
- en: Fairness is an important topic when it comes to using machine learning across
    different industries, as we discussed in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119),
    *Debugging toward Responsible AI*. In this chapter, we will provide you with some
    widely used notions and definitions of fairness in machine learning settings,
    as well as how to use fairness and explainability Python libraries that are designed
    to not only help you in assessing fairness in your models but also improve them
    in this regard.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用机器学习跨越不同行业时，公平性是一个重要的话题，正如我们在[*第3章*](B16369_03.xhtml#_idTextAnchor119)，“向负责任的人工智能调试”中讨论的那样。在本章中，我们将向您提供一些在机器学习环境中广泛使用的公平性概念和定义，以及如何使用旨在不仅帮助您评估模型中的公平性，而且在此方面改进它们的公平性和可解释性Python库。
- en: This chapter includes many figures and code examples to help you better understand
    these concepts and start benefiting from them in your projects. Note that one
    chapter is far from enough to make you an expert on the topic of fairness, but
    this chapter will provide you with the necessary knowledge and tools to start
    practicing this subject in your projects. You can learn more about this topic
    using more advanced resources dedicated to machine learning fairness.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含许多图表和代码示例，以帮助您更好地理解这些概念，并在项目中开始从中受益。请注意，一个章节远远不足以使您成为公平性主题的专家，但本章将为您提供开始在实际项目中实践这一主题所需的知识和工具。您可以通过使用更多致力于机器学习公平性的高级资源来了解更多关于这个主题的信息。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Fairness in machine learning modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习建模中的公平性
- en: Sources of bias
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏见的来源
- en: Using explainability techniques
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用可解释性技术
- en: Fairness assessment and improvement in Python
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python中的公平性评估和改进
- en: By the end of this chapter, you will have learned about some technical details
    and Python tools that you can use to assess fairness and reduce biases in your
    models. You will also learn how to benefit from the machine learning explainability
    techniques you learned about in [*Chapter 6*](B16369_06.xhtml#_idTextAnchor201),
    *Interpretability and Explainability in Machine* *Learning Modeling*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将了解一些技术细节和Python工具，您可以使用它们来评估模型的公平性并减少偏见。您还将了解如何从您在[*第6章*](B16369_06.xhtml#_idTextAnchor201)，“机器学习建模中的可解释性和可解释性”中学习的机器学习可解释性技术中受益。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following requirements should be considered for this chapter as they will
    help you better understand the concepts, use them in your projects, and practice
    with the provided code:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 以下要求应在本章中考虑，因为它们将帮助您更好地理解概念，在项目中使用它们，并使用提供的代码进行实践：
- en: 'Python library requirements:'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python库要求：
- en: '`sklearn` >= 1.2.2'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sklearn` >= 1.2.2'
- en: '`numpy` >= 1.22.4'
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` >= 1.22.4'
- en: '`pytest` >= 7.2.2'
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pytest` >= 7.2.2'
- en: '`shap` >= 0.41.0'
  id: totrans-16
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shap` >= 0.41.0'
- en: '`aif360` >= 0.5.0'
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`aif360` >= 0.5.0'
- en: '`fairlearn` >= 0.8.0'
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fairlearn` >= 0.8.0'
- en: Basic knowledge of the machine learning explainability concepts discussed in
    the previous chapter
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对上一章中讨论的机器学习可解释性概念的基本了解
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本章的代码文件，网址为[https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07](https://github.com/PacktPublishing/Debugging-Machine-Learning-Models-with-Python/tree/main/Chapter07)。
- en: Fairness in machine learning modeling
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习建模中的公平性
- en: 'To assess fairness, we need to have specific considerations in mind and then
    use proper metrics to quantify fairness in our models. *Table 7.1* provides you
    with some of the considerations, definitions, and approaches to either evaluate
    or achieve fairness in machine learning modeling. We will go through the mathematical
    definitions of **demographic parity**, **equality of odds** or **equalized odds**,
    and **equality of opportunity** here as different group fairness definitions.
    Group fairness definitions ensure the fairness of groups of people with common
    attributes and characteristics instead of individuals:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要评估公平性，我们需要在心中考虑特定的因素，然后使用适当的指标来量化模型中的公平性。*表7.1*为您提供了评估或实现机器学习建模中公平性的考虑因素、定义和方法的示例。我们将讨论**人口统计学平等**、**机会均等**或**均衡机会**和**机会平等**的数学定义，作为不同的群体公平性定义。群体公平性定义确保具有共同属性和特征的人群群体的公平性，而不是个人：
- en: '| **Topics in Machine** **Learning Fairness** | **Description** |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| **机器学习公平主题** | **描述** |'
- en: '| Demographic parity | Ensures predictions are not dependent on a given sensitive
    attribute, such as ethnicity, sex, or race |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 人口统计学平等 | 确保预测不依赖于给定的敏感属性，例如种族、性别或种族 |'
- en: '| Equality of odds | Ensures the independence of predictions to a given sensitive
    attribute, such as ethnicity, sex, or race given a true output |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 概率平等 | 确保预测对给定敏感属性的独立性，例如在给定真实输出时对种族、性别或种族的独立性 |'
- en: '| Equality of opportunity | Ensures the equality of opportunities provided
    for individuals or groups of people |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 机会平等 | 确保为个人或人群提供的平等机会 |'
- en: '| Individual fairness | Ensures fairness for individuals rather than groups
    of people with common attributes |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| 个人公平 | 确保对个人而不是具有共同属性的群体公平 |'
- en: '| Consistency | Provides consistency in decision-making not only between similar
    data points or users but also across time |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 一致性 | 不仅在相似的数据点或用户之间，而且在时间上提供决策的一致性 |'
- en: '| Fairness through unawareness | Achieves fairness if you’re unaware of sensitive
    attributes in decision making |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 通过无意识实现公平 | 如果你在决策过程中不知道敏感属性，则可以实现公平 |'
- en: '| Fairness through transparency | Improves fairness through transparency and
    trust building through explainability |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 通过透明度实现公平 | 通过透明度和通过可解释性建立信任来提高公平性 |'
- en: Table 7.1 – Some important topics and considerations in fairness in machine
    learning and artificial intelligence
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 – 机器学习和人工智能中关于公平性的重要主题和考虑因素
- en: 'Demographic parity is a group fairness definition that ensures that a model’s
    predictions are not dependent on a given sensitive attribute, such as ethnicity
    or sex. Mathematically, we can define it as the equality of probability of predicting
    a class, such as C i, for different groups of a given attribute, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人口统计学平等是一个群体公平定义，确保模型预测不依赖于给定的敏感属性，例如种族或性别。从数学上讲，我们可以将其定义为预测类别的概率相等，例如C i，对于给定属性的各个群体，如下所示：
- en: P(C = C i|G = g 1) = P(C = C i|G = g 2)
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: P(C = C i|G = g 1) = P(C = C i|G = g 2)
- en: 'To better understand the meaning of demographic parity, we can consider the
    following examples, which meet fairness according to demographic parity:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解人口统计学平等的含义，我们可以考虑以下例子，这些例子符合人口统计学平等的公平性：
- en: The same percentage of bail denial in each race group in COMPAS. We covered
    COMPAS in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119), *Debugging toward*
    *Responsible AI*.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在COMPAS中每个种族群体中拒绝保释的百分比相同。我们已在[*第3章*](B16369_03.xhtml#_idTextAnchor119)中介绍了COMPAS，*向负责任的AI调试*。
- en: The same acceptance rate for loan applications between men and women.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 男女贷款申请的接受率相同。
- en: The same likelihood of hospitalization between poor and rich neighborhoods.
    We covered more about this problem in [*Chapter 3*](B16369_03.xhtml#_idTextAnchor119),
    *Debugging toward* *Responsible AI*.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贫富社区之间住院的可能性相同。我们已在[*第3章*](B16369_03.xhtml#_idTextAnchor119)中详细介绍了这个问题，*向负责任的AI调试*。
- en: '**Disparate impact ratio** (**DIR**) is a metric that quantifies the deviation
    from equality based on demographic parity:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**差异影响比率**（**DIR**）是一个衡量基于人口统计学平等差异的指标：'
- en: DIR =  P(C = 1|G = g 1)  _____________  P(C = 1|G = g 2)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DIR =  P(C = 1|G = g 1)  _____________  P(C = 1|G = g 2)
- en: The DIR value’s range is [0, ∞), where a value of 1 satisfies demographic parity
    while deviation toward higher or lower values translates to deviation from fairness
    based on this definition. DIR values of greater and less than 1 are referred to
    as negative and positive bias, respectively, considering the group we use in the
    numerator.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: DIR值范围是[0, ∞)，其中1的值满足人口统计学平等，而向更高或更低值的偏差则表示根据此定义的公平性偏差。大于和小于1的DIR值分别被称为正偏差和负偏差，考虑到我们在分子中使用的群体。
- en: 'Despite the importance of demographic parity in fairness, it has its limitations.
    For example, in the case of DIR in the data itself (that is, the difference in
    class prevalence between different groups), a perfect model will not meet demographic
    parity criteria. Also, it doesn’t reflect the quality of predictions for each
    group. Other definitions help us improve our fairness assessment. Equality of
    odds or equalized odds is one such definition. Equalized odds is satisfied when
    a given prediction is independent of the group of a given sensitive attribute
    and the real output:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管人口统计学上的平等在公平性中很重要，但它有其局限性。例如，在数据本身中的DIR（即不同群体之间类别普遍性的差异）的情况下，一个完美的模型将不会满足人口统计学上的平等标准。此外，它也不反映每个群体的预测质量。其他定义有助于我们改进公平性评估。机会平等或均衡机会是一个这样的定义。当给定预测与给定敏感属性所属的群体以及真实输出无关时，均衡机会得到满足：
- en: P( ˆ y |y, G = g 1) = P( ˆ y |y, G = g 2) = P( ˆ y |y)
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: P( ˆ y |y, G = g 1) = P( ˆ y |y, G = g 2) = P( ˆ y |y)
- en: 'The definition of equality of opportunity is very similar to equalized odds,
    which assesses the independence of a prediction concerning groups for a given
    real output. But equality of opportunity focuses on a particular label of true
    values. Usually, the positive class is considered the target class and is representative
    of providing an opportunity for individuals, such as admission to school or having
    a high salary. Here is a formula for equality of opportunity:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 机会平等的定义与均衡机会非常相似，它评估预测与给定真实输出相关的群体的独立性。但机会平等专注于特定的真实值标签。通常，正类被认为是目标类，代表为个人提供机会，例如入学或高薪。以下是机会平等的公式：
- en: P( ˆ y |y = 1, G = g 1) = P( ˆ y |y = 1, G = g 2) = P( ˆ y |y = 1)
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: P( ˆ y |y = 1, G = g 1) = P( ˆ y |y = 1, G = g 2) = P( ˆ y |y = 1)
- en: According to these notions of fairness, each could give you a different result.
    You need to consider the differences between different notions so that you don’t
    generalize fairness based on one definition or another.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些公平性的概念，每个概念都可能给出不同的结果。你需要考虑不同概念之间的差异，以免基于一个或另一个定义泛化公平性。
- en: Proxies for sensitive variables
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 敏感变量的代理
- en: 'One of the challenges in assessing fairness in machine learning models is the
    existence of proxies for sensitive attributes such as sex and race. These proxies
    could be among the major contributors in generating model outputs and could result
    in bias in our models to specific groups. However, we cannot simply remove them
    as this could have a significant effect on performance. *Table 7.2* provides some
    examples of these proxies for different sensitive attributes:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估机器学习模型中的公平性时，一个挑战是存在敏感属性（如性别和种族）的代理。这些代理可能是生成模型输出的主要贡献者，并可能导致我们的模型对特定群体产生偏差。然而，我们不能简单地移除它们，因为这可能会对性能产生重大影响。*表7.2*提供了这些代理的示例，针对不同的敏感属性：
- en: '| **Sensitive Variable** | **Example Proxies** |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| **敏感变量** | **示例代理** |'
- en: '| Sex | Level of education, salary and income (in some countries), occupation,
    history of a felony charge, keywords in user-generated content (for example, in
    a resume or social media), being a university faculty |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 性别 | 教育水平、工资和收入（在某些国家），职业，犯罪指控历史，用户生成内容中的关键词（例如，在简历或社交媒体中），作为大学教职员工 |'
- en: '| Race | History of a felony charge, keywords in user-generated content (for
    example, in a resume or social media), ZIP or postal code |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 种族 | 犯罪指控历史、用户生成内容中的关键词（例如，在简历或社交媒体中）、ZIP或邮政编码 |'
- en: '| Disabilities | Speed of walking, eye movement, body posture |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 残疾 | 行走速度、眼动、身体姿势 |'
- en: '| Marital status | Level of education, salary and income (in some countries),
    and house size and number of bedrooms |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 婚姻状况 | 教育水平、工资和收入（在某些国家），以及房屋大小和卧室数量 |'
- en: '| Age | Posture and keywords in user-generated content (for example, in a resume
    or social media) |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 年龄 | 姿势和用户生成内容中的关键词（例如，在简历或社交媒体中） |'
- en: Table 7.2 – Examples of proxies for some of the important sensitive variables,
    in the context of fairness (Caton and Haas, 2020)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 – 在公平性的背景下，一些重要敏感变量的代理示例（Caton和Haas，2020）
- en: Now that you’ve learned about the importance of fairness and some important
    definitions under this topic, let’s review some of the possible sources of bias
    that play against your goal of achieving fairness in your models.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了公平性的重要性以及这个主题下的一些重要定义，让我们回顾一下可能产生偏差的来源，这些偏差可能会阻碍你在模型中实现公平性的目标。
- en: Sources of bias
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差的来源
- en: There are different sources of bias in a machine learning life cycle. Bias could
    exist in the collected data, introduced in the data subsampling, cleaning and
    filtering, or model training and selection. Here, we will review examples of such
    sources to help you better understand how to avoid or detect such biases throughout
    the life cycle of a machine learning project.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习生命周期中存在不同的偏差来源。偏差可能存在于收集的数据中，在数据子采样、清理和过滤中引入，或者在模型训练和选择中。在这里，我们将回顾这些来源的例子，以帮助您更好地理解如何在机器学习项目的整个生命周期中避免或检测此类偏差。
- en: Biases introduced in data generation and collection
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据生成和收集中引入的偏差
- en: 'The data that we feed into our models could be biased by default, even before
    the modeling starts. The first source of such biases we want to review here is
    the issue of dataset size. Consider a dataset as a sample of a bigger population
    – for example, a survey of 100 students or the loan application information of
    200 customers of a bank. The small size of these datasets could increase the chance
    of bias. Let’s simulate this with a simple random data generation. We will write
    a function that generates two vectors of random binary values using `np.random.randint()`
    and then calculates *DIR* between the two groups of 0 and 1:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们输入到模型中的数据可能默认就有偏差，甚至在建模开始之前。我们在这里想要回顾的第一个这种偏差的来源是数据集大小的问题。将数据集视为更大人群的一个样本——例如，100名学生的调查或200名银行客户的贷款申请信息。这些数据集的小规模可能会增加偏差的机会。让我们用一个简单的随机数据生成来模拟这一点。我们将编写一个函数，使用`np.random.randint()`生成两个随机二进制值的向量，然后计算两个0和1组之间的*DIR*：
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let’s use this function to calculate DIR for 1,000 different groups of
    different sizes, including `50`, `100`, `1000`, `10000`, and `1000000` data points:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个函数来计算1,000个不同规模的不同组别的DIR，包括`50`、`100`、`1000`、`10000`和`1000000`个数据点：
- en: '[PRE1]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The following boxplots show the distributions of *DIR* across different sample
    sizes. You can see that lower sample sizes have wider distributions covering very
    low or high *DIR* values, distant from the ideal case of 1:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 以下箱线图显示了不同样本规模下*DIR*的分布。你可以看到，较小的样本规模具有更宽的分布，覆盖了非常低或高的*DIR*值，远离理想的1值：
- en: '![Figure 7.1 – Distributions of DIR across different sampling sizes](img/B16369_07_01..jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图7.1 – 不同采样规模下的DIR分布](img/B16369_07_01..jpg)'
- en: Figure 7.1 – Distributions of DIR across different sampling sizes
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 – 不同采样规模下的DIR分布
- en: 'We can also calculate the percentage of sampled groups of different sizes that
    don’t pass a specific threshold, such as >=0.8 and <=1.2\. *Figure 7**.2* shows
    that higher dataset sizes result in a lower chance of having datasets that have
    positive or negative bias given a sensitive attribute:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算不同规模样本组的百分比，这些组没有通过特定的阈值，例如>=0.8和<=1.2。*图7.2*显示，较高的数据集规模导致具有正或负偏差的数据集的机会降低：
- en: '![Figure 7.2 – Percentage of sets of samples that don’t pass DIR thresholds](img/B16369_07_02..jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 – 未通过DIR阈值的样本集百分比](img/B16369_07_02..jpg)'
- en: Figure 7.2 – Percentage of sets of samples that don’t pass DIR thresholds
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 – 未通过DIR阈值的样本集百分比
- en: 'The source of existing bias in datasets might not just be an artifact of a
    small sample size. For example, if you were to train a model to predict if an
    individual will end up in STEM, which is an acronym for fields of science, technology,
    engineering, and math, then you must consider the reality of the existence of
    it being imbalanced toward men over women in the corresponding data in fields,
    such as engineering, even up until recently (*Figure 7**.3*):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中现有偏差的来源可能不仅仅是小样本大小的产物。例如，如果你要训练一个模型来预测个人是否会进入STEM领域，STEM是科学、技术、工程和数学领域的缩写，那么你必须考虑这样一个现实：在工程等领域的相应数据中，男性相对于女性的存在是不平衡的，甚至直到最近（*图7.3*）：
- en: '![Figure 7.3 – Percentage of women in STEM jobs between 1970 and 2019](img/B16369_07_03..jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图7.3 – 1970年至2019年间STEM职业中女性的百分比](img/B16369_07_03..jpg)'
- en: Figure 7.3 – Percentage of women in STEM jobs between 1970 and 2019
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 – 1970年至2019年间STEM职业中女性的百分比
- en: Having less than 20% of engineers being women over the years, because of their
    lower interest, bias in hiring processes, or stereotypes in society, has resulted
    in bias in the data on workers in this field. If this is not rectified with fairness
    in your data processing and modeling tasks, it could result in predicting a higher
    chance for men getting into STEM compared to women, despite their talents, knowledge,
    and experience.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，工程师中女性比例不到20%，这可能是由于她们对这一领域的兴趣较低、招聘过程中的偏见，或是社会中的刻板印象，这导致了该领域工作者数据中的偏见。如果数据处理的建模任务中不公平，可能会导致预测男性进入STEM领域的可能性高于女性，尽管她们拥有才能、知识和经验。
- en: There is another category of intrinsic bias in the data, although it needs to
    be considered when developing machine learning models. For example, less than
    1% of breast cancer cases occur in men ([www.breastcancer.org](https://www.breastcancer.org)).
    This prevalence difference between men and women is not caused by any sort of
    bias in data generation or collection or biases that have existed in societies.
    It is the natural difference between the prevalence of breast cancer occurrence
    between men and women. But if you were responsible for developing a machine learning
    model to diagnose breast cancer, there could be a high chance of false negatives
    (that is, not diagnosing breast cancer) in men. If your model doesn’t consider
    the high prevalence of women over men, it will not be a fair model in breast cancer
    diagnosis for men. This was a high-level example to clarify this kind of bias.
    There are many other considerations in building a machine learning tool for cancer
    diagnosis.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中还存在另一类内在偏见，尽管在开发机器学习模型时需要考虑它。例如，不到1%的乳腺癌病例发生在男性身上([www.breastcancer.org](https://www.breastcancer.org))。男性和女性之间这种患病率的差异并非由数据生成或收集中的任何偏见，或社会存在的偏见所导致。这是男性和女性乳腺癌发生率的自然差异。但如果负责开发用于诊断乳腺癌的机器学习模型，男性可能会出现较高的假阴性率（即未诊断出乳腺癌）。如果你的模型没有考虑到女性比男性高得多的患病率，那么它将不是一个对男性公平的乳腺癌诊断模型。这是一个高级示例，用于阐明这类偏见。在构建用于癌症诊断的机器学习工具时，还有许多其他需要考虑的因素。
- en: Bias in model training and testing
  id: totrans-74
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型训练和测试中的偏见
- en: If a dataset has a high imbalance toward men or women, different ethnicities,
    or any sort of bias considering different sensitive attributes, our models could
    have biases due to the way the corresponding machine learning algorithms use the
    features in predicting the outcome of data points. For example, our models could
    be highly reliant on sensitive attributes or their proxies (*Table 7.2*). This
    is an important consideration in model selection. In the model selection process,
    we need to select a model among the trained models, with different methods or
    hyperparameters of the same method, to be pushed for further testing or production.
    If we base our decision-making solely on performance, then we might select a model
    that is not fair. We need to consider both fairness and performance in our model
    selection process if we have sensitive attributes and those models will directly
    or indirectly affect individuals of different groups.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集在男性或女性、不同种族或其他敏感属性方面存在高度不平衡，我们的模型可能会因为相应的机器学习算法在预测数据点结果时使用特征的方式而产生偏见。例如，我们的模型可能会高度依赖敏感属性或它们的代理（*表7.2*）。这是模型选择过程中的一个重要考虑因素。在模型选择过程中，我们需要从训练的模型中选择一个，使用不同的方法或同一方法的超参数，以进行进一步的测试或生产。如果我们仅基于性能做出决定，那么我们可能会选择一个不公平的模型。如果我们有敏感属性，并且这些模型将直接或间接影响不同群体的人，那么在模型选择过程中，我们需要同时考虑公平性和性能。
- en: Bias in production
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生产中的偏见
- en: Bias and unfairness in production could happen because of differences in the
    distribution of data between training, testing, and production. For example, women
    and men could have some differences in the production stage that don’t exist in
    your training and test data. This situation could result in biases in production
    that might not have been detectable in previous stages of the life cycle. We will
    talk about such kinds of differences in more detail in [*Chapter 11*](B16369_11.xhtml#_idTextAnchor300),
    *Avoiding and Detecting Data and* *Concept Drifts*.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练、测试和生产阶段数据分布的不同，生产过程中可能会出现偏见和不公平现象。例如，在生产和测试数据中不存在的性别差异可能会在生产的某个阶段出现。这种情况可能导致在生命周期早期阶段无法检测到的生产偏见。我们将在[*第11章*](B16369_11.xhtml#_idTextAnchor300)中更详细地讨论这类差异，*避免和检测数据与概念漂移*。
- en: The next step in this chapter is to start practicing with techniques and Python
    libraries that help you in detecting and eliminating model biases. First, will
    practice using the explainability techniques that were introduced in [*Chapter
    6*](B16369_06.xhtml#_idTextAnchor201), *Interpretability and Explainability in
    Machine* *Learning Modeling*.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的下一步是开始练习使用帮助你在检测和消除模型偏差方面的技术和Python库。首先，我们将练习使用在[*第6章*](B16369_06.xhtml#_idTextAnchor201)中介绍的*机器学习建模中的可解释性和可解释性技术*。
- en: Using explainability techniques
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用可解释性技术
- en: 'We can use explainability techniques to identify potential biases in our models
    and then plan to improve them toward fairness. Here, we want to practice this
    concept with SHAP and identify fairness issues between male and female groups
    in the adult income dataset we practiced with in the previous chapter. Using the
    same SHAP explainer object we built for the XGBoost model we trained on adult
    income data in the previous chapter, in the following bar plots, we can see that
    there is a low, but non-negligible, dependency on *sex* regarding the whole dataset
    or only the incorrectly predicted data points:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用可解释性技术来识别我们模型中的潜在偏差，然后计划改进它们以实现公平性。在这里，我们想通过SHAP练习这个概念，并识别我们在上一章练习的成人收入数据集中男性和女性群体之间的公平性问题。使用我们在上一章为成人收入数据训练的XGBoost模型构建的相同的SHAP解释器对象，在下面的条形图中，我们可以看到，关于整个数据集或仅错误预测的数据点，对*性别*的依赖性很低，但并非微不足道：
- en: '![Figure 7.4 – SHAP summary plot for the whole adult income dataset and incorrectly
    predicted data points](img/B16369_07_04.jpg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 整个成人收入数据集和错误预测数据点的SHAP摘要图](img/B16369_07_04.jpg)'
- en: Figure 7.4 – SHAP summary plot for the whole adult income dataset and incorrectly
    predicted data points
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 整个成人收入数据集和错误预测数据点的SHAP摘要图
- en: 'Now, we can extract the fraction of misclassified data points in each sex group,
    as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以提取每个性别群体中误分类数据点的比例，如下所示：
- en: '[PRE2]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will produce the following result:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下结果：
- en: '![Figure 7.5 – Number of males and females among correct and incorrect predictions](img/B16369_07_05.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – 正确和错误预测中男性和女性的数量](img/B16369_07_05.jpg)'
- en: Figure 7.5 – Number of males and females among correct and incorrect predictions
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – 正确和错误预测中男性和女性的数量
- en: Here, we have 6.83% and 20.08% misclassification percentages for female and
    male groups, respectively. The ROC-AUC of the predictions of the model for only
    male and female groups in the test set are 0.90 and 0.94, respectively.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们分别有女性和男性群体的6.83%和20.08%的误分类百分比。测试集中仅针对男性和女性群体的模型预测的ROC-AUC分别为0.90和0.94。
- en: 'You might consider identifying the correlation between features as an approach
    to identifying proxies and potential ways of removing biases in your models. The
    following code and heatmap (*Figure 7**.6*) show a correlation between the features
    of this dataset:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以考虑将识别特征之间的相关性作为一种识别代理和潜在去除模型中偏差的方法。以下代码和热图（*图7.6*）显示了该数据集特征之间的相关性：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be as follows:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将如下所示：
- en: '![Figure 7.6 – Correlation DataFrame between the features of the adult income
    dataset](img/B16369_07_06.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图7.6 – 成人收入数据集特征之间的相关性DataFrame](img/B16369_07_06.jpg)'
- en: Figure 7.6 – Correlation DataFrame between the features of the adult income
    dataset
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 – 成人收入数据集特征之间的相关性DataFrame
- en: 'However, there are disadvantages to using such correlation analysis as the
    way of approaching the problem of proxy identification or even for filtering features
    toward improving performance. Here are two of these disadvantages:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，使用这种相关性分析作为处理代理识别问题或甚至用于过滤特征以提高性能的方法存在缺点。以下是其中两个缺点：
- en: You need to consider proper correlation measures for each pair of features.
    For example, *Pearson* correlation cannot be used for all feature pairs as the
    distribution of data for each pair has to satisfy the assumptions for this method.
    Both variables need to follow normal distributions and data should not have any
    outliers as two of the assumptions for proper use of *Pearson* correlation. This
    means that to have a proper use of the feature correlation analysis approach,
    you need to use proper correlation measures to compare the features. Non-parametric
    statistical measures such as *Spearman* rank correlation could be more suitable
    as there are fewer assumptions behind its use across different variable pairs.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要考虑每对特征适当的关联度量。例如，*皮尔逊*相关不能用于所有特征对，因为每对数据的分布必须满足该方法的使用假设。两个变量都需要遵循正态分布，数据不应有任何异常值，这是*皮尔逊*相关适当使用时的两个假设之一。这意味着为了正确使用特征相关性分析方法，你需要使用适当的关联度量来比较特征。非参数统计度量，如*斯皮尔曼*等级相关，可能更适合，因为在使用不同变量对时，其背后的假设较少。
- en: Not all numerical values have the same meaning. Some of the features are categorical
    and, through different methods, are transformed into numerical features. Sex is
    one of those features. Values of 0 and 1 can be used to show female and male groups
    but they don’t have any numerical meaning that you can find in numerical features
    such as age or salary.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 并非所有数值都有相同的意义。一些特征是分类的，并且通过不同的方法被转换成数值特征。性别就是这些特征之一。0和1的值可以用来表示女性和男性群体，但它们在数值特征（如年龄或薪水）中没有任何数值意义。
- en: 'Explainability techniques such as SHAP tell you about dependencies to sensitive
    attributes and their contributions to the outcome of data points. However, by
    default, they don’t offer a way to improve the models in terms of fairness. In
    this example, we can try to split the data into male and female groups for training
    and testing. The following code shows this approach for the female group. Similarly,
    you can repeat this for the male group by separating the train and test input
    and output data with the `Sex` feature of `1`. The models that were built separately
    for male and female groups resulted in 0.90 and 0.93 ROC-AUCs, respectively, which
    is almost the same as the performance without the separation of the groups:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性技术如SHAP会告诉你关于敏感属性及其对数据点结果贡献的依赖关系。然而，默认情况下，它们并不提供改进模型公平性的方法。在这个例子中，我们可以尝试将数据分割成男性和女性群体进行训练和测试。以下代码展示了针对女性群体的这种方法。同样，你可以通过使用“性别”特征的“1”来分离训练和测试输入输出数据，为男性群体重复此操作。为男性和女性群体分别构建的模型分别得到了0.90和0.93的ROC-AUC值，这几乎与分组分离的性能相同：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We didn’t remove the `Sex` feature from the models. This feature cannot contribute
    to the model’s performance as there is no difference between the values of this
    feature across the data points of each model. This is also shown by zero Shapely
    values in the bar plots:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有从模型中移除“性别”特征。这个特征不能对模型的性能做出贡献，因为每个模型的数据点中这个特征的值没有差异。这也在条形图中通过零Shapely值得到了体现：
- en: '![Figure 7.7 – SHAP summary plot for models trained and tested on female and
    male groups separately](img/B16369_07_07.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图7.7 – 分别在女性和男性群体上训练和测试的模型的SHAP摘要图](img/B16369_07_07.jpg)'
- en: Figure 7.7 – SHAP summary plot for models trained and tested on female and male
    groups separately
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 – 分别在女性和男性群体上训练和测试的模型的SHAP摘要图
- en: This approach of separating groups according to a sensitive attribute, although
    sometimes seen as taken, is not an ideal way of dealing with the issue of fairness.
    It might not be an effective approach as the model could be highly reliant on
    other sensitive features. Also, we cannot split the data into small chunks according
    to all combinations of all sensitive attributes in our dataset. There are fairness
    tools that could help you not only assess fairness and detect biases but select
    a model that better satisfies fairness notions.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 根据敏感属性分离群体的这种做法，尽管有时被视为一种选择，但并不是处理公平性问题的一个理想方式。它可能不是一个有效的办法，因为模型可能高度依赖于其他敏感特征。此外，我们无法根据数据集中所有敏感属性的组合将数据分割成小块。有一些公平性工具可以帮助你不仅评估公平性和检测偏差，还可以选择一个更好地满足公平性概念的模型。
- en: In addition to libraries for explainability, there are Python libraries that
    are designed specifically for fairness detection and improvement in machine learning
    modeling, which we will cover next.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于可解释性的库之外，还有一些专门为机器学习建模中的公平性检测和改进设计的Python库，我们将在下一部分介绍。
- en: Fairness assessment and improvement in Python
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python中的公平性评估和改进
- en: 'There are few widely used Python libraries to assess fairness in your models
    (*Table 7.3*). You can use these libraries to identify if the model satisfies
    fairness definitions according to the different sensitive attributes in a dataset
    you want to or have used for modeling:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估模型公平性的Python库中，广泛使用的并不多（*表7.3*）。您可以使用这些库来识别模型是否满足根据数据集中不同的敏感属性所定义的公平性：
- en: '| **Library** | **Library Name for Importing** **and Installation** | **URL**
    |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| **库** | **导入和安装库名称** | **URL** |'
- en: '| IBM AI Fairness 360 | `aif360` | [https://pypi.org/project/aif360/](https://pypi.org/project/aif360/)
    |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| IBM AI Fairness 360 | `aif360` | [https://pypi.org/project/aif360/](https://pypi.org/project/aif360/)
    |'
- en: '| Fairlearn | `fairlearn` | [https://pypi.org/project/fairlearn/](https://pypi.org/project/fairlearn/)
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| Fairlearn | `fairlearn` | [https://pypi.org/project/fairlearn/](https://pypi.org/project/fairlearn/)
    |'
- en: '| Black Box Auditing | `BlackBoxAuditing` | [https://pypi.org/project/BlackBoxAuditing/](https://pypi.org/project/BlackBoxAuditing/)
    |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒审计 | `BlackBoxAuditing` | [https://pypi.org/project/BlackBoxAuditing/](https://pypi.org/project/BlackBoxAuditing/)
    |'
- en: '| Aequitas | `aequitas` | [https://pypi.org/project/aequitas/](https://pypi.org/project/aequitas/)
    |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| Aequitas | `aequitas` | [https://pypi.org/project/aequitas/](https://pypi.org/project/aequitas/)
    |'
- en: '| Responsible AI Toolbox | `responsibleai` | [https://pypi.org/project/responsibleai/](https://pypi.org/project/responsibleai/)
    |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 负责任AI工具箱 | `responsibleai` | [https://pypi.org/project/responsibleai/](https://pypi.org/project/responsibleai/)
    |'
- en: '| Responsibly | `responsibly` | [https://pypi.org/project/responsibly/](https://pypi.org/project/responsibly/)
    |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 负责任地 | `responsibly` | [https://pypi.org/project/responsibly/](https://pypi.org/project/responsibly/)
    |'
- en: '| Amazon Sagemaker Clarify | `smclarify` | [https://pypi.org/project/smclarify/](https://pypi.org/project/smclarify/)
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| Amazon Sagemaker Clarify | `smclarify` | [https://pypi.org/project/smclarify/](https://pypi.org/project/smclarify/)
    |'
- en: '| Fairness-aware machine learning | `fairness` | [https://pypi.org/project/fairness/](https://pypi.org/project/fairness/)
    |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 公平感知机器学习 | `fairness` | [https://pypi.org/project/fairness/](https://pypi.org/project/fairness/)
    |'
- en: '| Bias correction | `biascorrection` | [https://pypi.org/project/biascorrection/](https://pypi.org/project/biascorrection/)
    |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 偏差校正 | `biascorrection` | [https://pypi.org/project/biascorrection/](https://pypi.org/project/biascorrection/)
    |'
- en: Table 7.3 – Python libraries or repositories with available functionalities
    for machine learning fairness
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3 – 具有机器学习公平性可用功能的Python库或存储库
- en: 'First, let’s load the adult income dataset, after importing the required libraries,
    and prepare the training and test sets, as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们加载成人收入数据集，在导入所需的库之后，并准备训练和测试集，如下所示：
- en: '[PRE5]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now, we can train and test an XGBoost model:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以训练和测试一个XGBoost模型：
- en: '[PRE6]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Here, we want to use `aif360` to calculate the *DIR* of real and predicted
    outcomes in the training and test data according to the `Sex` attribute:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们希望使用`aif360`根据`Sex`属性计算训练和测试数据中真实和预测结果的方向性指标（DIR）：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The following group bar plot shows that the predictions make the *DIR* even
    worse in both the training and test sets:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的分组条形图显示，预测在训练和测试集中使DIR变得更糟：
- en: '![Figure 7.8 – Comparison of DIR in the original data and predicted outputs](img/B16369_07_08.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图7.8 – 原始数据和预测输出的DIR比较](img/B16369_07_08.jpg)'
- en: Figure 7.8 – Comparison of DIR in the original data and predicted outputs
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 – 原始数据和预测输出的DIR比较
- en: 'We can use `aif360` to improve our models toward fairness. Reject option classification
    is a postprocessing technique that gives favorable outcomes to unprivileged groups
    and unfavorable outcomes to privileged groups in a confidence band around the
    decision boundary with the highest uncertainty ([https://aif360.readthedocs.io/](https://aif360.readthedocs.io/),
    Kamira et al., 2012). First, let’s import all the necessary libraries and functionalities
    we need for doing so in Python:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`aif360`来提高我们的模型向公平性发展。拒绝选项分类是一种后处理技术，在决策边界最高不确定性的置信区间内，对无特权群体给予有利结果，对特权群体给予不利结果（[https://aif360.readthedocs.io/](https://aif360.readthedocs.io/),
    Kamira等，2012）。首先，让我们导入在Python中执行此操作所需的全部必要库和功能：
- en: '[PRE8]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, we can use `RejectOptionClassifierCV()`to train and validate a random
    forest classifier on the adult dataset available in `aif360`. We switched from
    XGBoost to random forest solely for the sake of practicing with different models.
    We need to fit a `PostProcessingMeta()` object with an initial random forest model
    and `RejectOptionClassifierCV()`. `''sex''` is considered the sensitive feature
    in the process:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用 `RejectOptionClassifierCV()` 在 `aif360` 中可用的成人数据集上训练和验证一个随机森林分类器。我们之所以从
    XGBoost 切换到随机森林，只是为了练习不同的模型。我们需要将一个初始的随机森林模型和 `RejectOptionClassifierCV()` 与 `PostProcessingMeta()`
    对象拟合。在过程中，`'sex'` 被认为是敏感特征：
- en: '[PRE9]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can then plot the balanced accuracy and *DIR* across different attempts
    in the grid search to show the best-chosen parameters, which is the starred point
    in the scatter plot in *Figure 7**.9*. The points in cyan show you the Pareto
    front for the tradeoff between balanced accuracy and *DIR*:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以绘制平衡准确率和 *DIR* 在网格搜索中不同尝试的图表，以显示最佳选择的参数，这是 *图 7**.9* 中散点图中的星号点。蓝色点向您展示了平衡准确率和
    *DIR* 之间的权衡的帕累托前沿：
- en: '![Figure 7.9 – Balanced accuracy versus DIR in a grid search](img/B16369_07_09.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 网格搜索中平衡准确率与 DIR 的比较](img/B16369_07_09.jpg)'
- en: Figure 7.9 – Balanced accuracy versus DIR in a grid search
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 网格搜索中平衡准确率与 DIR 的比较
- en: As you can see, there is a compromise between performance and fairness in this
    case. But in this case, a less than 4% decrease in performance results in improving
    *DIR* from lower than 0.4 to 0.8.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在这种情况下，性能和公平性之间存在权衡。但在这个例子中，性能降低不到 4% 就能将 *DIR* 从低于 0.4 提高到 0.8。
- en: As you saw in this example, we can use `aif360` to assess fairness and improve
    our model’s fairness with little loss in performance. You can use other libraries
    in Python similarly. And each one has its functionality for the two objectives
    of fairness assessment and improvement in machine learning modeling.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在示例中看到的那样，我们可以使用 `aif360` 来评估公平性，并在性能损失很小的情况下提高我们模型的公平性。您可以使用 Python 中的其他库以类似的方式做到这一点。每个库都有其功能，用于机器学习建模中的公平性评估和改进的两个目标。
- en: What we provided in this chapter was only the tip of the iceberg of fairness
    in machine learning. But at this point, you are ready to try different libraries
    and techniques and learn about them with the help of the practices we went through.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中提供的内容只是机器学习公平性冰山的一角。但到目前为止，您已经准备好尝试不同的库和技术，并通过我们经历过的实践来了解它们。
- en: Summary
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you learned more about the concept of fairness in the machine
    learning era, as well as the metrics, definitions, and challenges for assessing
    fairness. We talked about example proxies for sensitive attributes such as *sex*
    and *race*. We also talked about possible sources of bias, such as in data collection
    or model training. You also learned how you can use Python libraries for model
    explainability and fairness to assess fairness or improve it in your models, as
    well as avoid biases that not only would be unethical but could have legal and
    financial consequences for your organization.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您学习了更多关于机器学习时代公平性的概念，以及评估公平性的指标、定义和挑战。我们讨论了诸如 *sex* 和 *race* 这样的敏感属性的示例代理。我们还讨论了可能的偏差来源，例如数据收集或模型训练。您还学习了如何使用
    Python 库来评估模型的可解释性和公平性，以评估或改进您的模型，以及避免不仅不道德，而且可能对您的组织产生法律和财务后果的偏差。
- en: In the next chapter, you will learn about test-driven development and concepts
    such as unit and differential testing. We will also talk about machine learning
    experiment tracking and how it helps us avoid issues in our models in the model
    training, testing, and selection processes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习测试驱动开发以及单元测试和差分测试等概念。我们还将讨论机器学习实验跟踪以及它在模型训练、测试和选择过程中如何帮助我们避免模型问题。
- en: Questions
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Does fairness depend only on observable features?
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 公平性是否只取决于可观察的特征？
- en: What are examples of proxy features for `'sex'`?
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`''sex''` 的代理特征有哪些例子？'
- en: If one model is fair according to demographic parity, would it be fair according
    to other notions of fairness such as equalized odds?
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果一个模型根据人口统计学平等是公平的，那么根据其他公平性概念，如均衡机会，它是否也是公平的？
- en: What is the difference between demographic parity and equalized odds as two
    fairness metrics?
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 作为两种公平性指标，人口统计学平等和均衡机会有何区别？
- en: If you have a `'sex'` feature in your model and your model would have a low
    dependency on that, does it mean that your model is fair across different sex
    groups?
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果您的模型中有一个 `'sex'` 特征，并且您的模型对它的依赖性较低，这意味着您的模型在不同性别群体中是公平的吗？
- en: How could you use explainability techniques to assess fairness in your models?
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何使用可解释性技术来评估你模型中的公平性？
- en: References
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Barocas, Solon, Moritz Hardt, and Arvind Narayanan. *Fairness in machine learning*.
    Nips tutorial 1 (2017): 2017.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Barocas, Solon, Moritz Hardt 和 Arvind Narayanan. *机器学习中的公平性*. Nips教程 1 (2017):
    2017.'
- en: 'Mehrabi, Ninareh, et al. *A survey on bias and fairness in machine learning*.
    ACM Computing Surveys (CSUR) 54.6 (2021): 1-35.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Mehrabi, Ninareh, 等人. *机器学习中的偏差与公平性调查*. ACM 计算机调查 (CSUR) 54.6 (2021): 1-35.'
- en: 'Caton, Simon, and Christian Haas. *Fairness in machine learning: A survey*.
    arXiv preprint arXiv:2010.04053 (2020).'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Caton, Simon 和 Christian Haas. *机器学习中的公平性：调查*. arXiv 预印本 arXiv:2010.04053 (2020).
- en: 'Pessach, Dana, and Erez Shmueli. *A review on fairness in machine learning*.
    ACM Computing Surveys (CSUR) 55.3 (2022): 1-44.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Pessach, Dana 和 Erez Shmueli. *机器学习中的公平性综述*. ACM 计算机调查 (CSUR) 55.3 (2022):
    1-44.'
- en: Lechner, Tosca, et al. *Impossibility results for fair representations*. arXiv
    preprint arXiv:2107.03483 (2021).
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Lechner, Tosca, 等人. *公平表示的不可能性结果*. arXiv 预印本 arXiv:2107.03483 (2021).
- en: 'McCalman, Lachlan, et al. *Assessing AI fairness in finance*. Computer 55.1
    (2022): 94-97.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'McCalman, Lachlan, 等人. *评估金融中的AI公平性*. 计算机 55.1 (2022): 94-97.'
- en: F. Kamiran, A. Karim, and X. Zhang, *Decision Theory for Discrimination-Aware
    Classification*. IEEE International Conference on Data Mining, 2012.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F. Kamiran, A. Karim 和 X. Zhang，*歧视感知分类的决策理论*. 国际数据挖掘会议，2012.
- en: Part 3:Low-Bug Machine Learning Development and Deployment
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：低错误机器学习开发和部署
- en: With this part of the book, we will provide the essential practices to ensure
    the robustness and reliability of machine learning models, especially in production.
    We will start with the adoption of Test-Driven Development, illustrating its crucial
    role in mitigating risks during model development. Subsequently, we will delve
    into the testing techniques and the significance of model monitoring, ensuring
    that our models remain dependable when deployed. We will then explain techniques
    and challenges in achieving reproducibility in machine learning through code,
    data, and model versioning. We will conclude this part by addressing the challenges
    of data and concept drifts to have reliable models in production.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，我们将提供确保机器学习模型稳健性和可靠性的基本实践，尤其是在生产环境中。我们将从采用测试驱动开发开始，说明它在模型开发过程中降低风险的关键作用。随后，我们将深入研究测试技术和模型监控的重要性，确保我们的模型在部署后仍保持可靠性。然后，我们将解释通过代码、数据和模型版本化实现机器学习可重复性的技术和挑战。最后，我们将讨论数据漂移和概念漂移的挑战，以确保在生产中有可靠的模型。
- en: 'This part has the following chapters:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包含以下章节：
- en: '[*Chapter 8*](B16369_08.xhtml#_idTextAnchor243), *Controlling Risks Using Test-Driven
    Development*'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B16369_08.xhtml#_idTextAnchor243), *使用测试驱动开发控制风险*'
- en: '[*Chapter 9*](B16369_09.xhtml#_idTextAnchor261), *Testing and Debugging for
    Production*'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B16369_09.xhtml#_idTextAnchor261), *生产环境下的测试和调试*'
- en: '[*Chapter 10*](B16369_10.xhtml#_idTextAnchor286), *Versioning and Reproducible
    Machine Learning Modeling*'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第10章*](B16369_10.xhtml#_idTextAnchor286), *版本控制和可重复的机器学习建模*'
- en: '[*Chapter 11*](B16369_11.xhtml#_idTextAnchor300), *Avoiding and Detecting Data
    and Concept Drifts*'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第11章*](B16369_11.xhtml#_idTextAnchor300), *避免和检测数据及概念漂移*'
