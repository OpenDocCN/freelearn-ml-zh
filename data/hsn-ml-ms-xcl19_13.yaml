- en: Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络
- en: Artificial neural networks try to mimic the way the human brain works. They
    are used to solve a number of difficult problems, such as understanding written
    or spoken language, identifying objects in an image, or driving a car.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络试图模仿人脑的工作方式。它们被用于解决一系列难题，例如理解书面或口头语言、识别图像中的物体，或驾驶汽车。
- en: You will learn the basics of how an artificial neural network works, look at
    the steps and mathematical calculations needed to train it, and have a general
    view of complex neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 您将学习人工神经网络的基本工作原理，查看训练它所需的步骤和数学计算，并对复杂神经网络有一个总体了解。
- en: 'The following topics will be covered in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节将涵盖以下主题：
- en: Introducing the perceptron – the simplest type of neural network
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍感知器——最简单的神经网络类型
- en: Building a deep network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建深度网络
- en: Understanding the backpropagation algorithm
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解反向传播算法
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: To complete this chapter, you will need to download the `transfusion.xlsx` file
    from the GitHub repository at [https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09)[.](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成本章，您需要从GitHub仓库[https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09)下载`transfusion.xlsx`文件[.](https://github.com/PacktPublishing/Hands-On-Machine-Learning-with-Microsoft-Excel-2019/tree/master/Chapter09)
- en: Introducing the perceptron – the simplest type of neural network
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍感知器——最简单的神经网络类型
- en: Neural networks are inspired by the human brain' more specifically, by the neuron
    cells that compose it. Actually, since there have been major advances in neuroscience
    since the first artificial neuron was designed, it would be better to say that
    they are inspired by what was known about the brain some years ago.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络受到人脑的启发，更具体地说，是构成它的神经元细胞。实际上，自从第一个人工神经元被设计以来，神经科学已经取得了重大进展，因此最好说它们是受到几年前对大脑所知内容的启发。
- en: 'The perceptron was the first attempt to build an artificial neural network
    (Frank Rosenblatt, 1959). It was actually a model of a single neuron, with multiple
    inputs and one output. The value at the output is calculated as the weighted sum
    of the inputs and these weights are adjusted iteratively. This simple implementation
    has many disadvantages and limitations, so it was later replaced by the multilayer
    perceptron. The most basic model of this artificial neural network has the structure
    shown in the following diagram:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 感知器是构建人工神经网络的第一次尝试（弗兰克·罗森布拉特，1959年）。它实际上是一个单神经元的模型，具有多个输入和一个输出。输出值是输入的加权总和，这些权重会迭代调整。这种简单的实现有许多缺点和限制，因此后来被多层感知器所取代。这种人工神经网络的最基本模型在以下图中显示：
- en: '![](img/6bdc8e1b-7880-4957-97dc-3eaa548f0d74.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6bdc8e1b-7880-4957-97dc-3eaa548f0d74.png)'
- en: 'The input and output layers are taken from the perceptron, but a hidden layer
    of nodes is now added. Each node in this layer acts in practice as a neuron. To
    understand how the inputs and outputs of each neuron work and how information
    is sent through the network, we need to know the details of how each neuron is
    built. A schematic view of an artificial neuron could be represented as follows:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层和输出层来自感知器，但现在增加了一个隐藏层节点。实际上，这一层的每个节点都充当一个神经元。为了理解每个神经元的输入和输出是如何工作的，以及信息是如何通过网络传递的，我们需要知道每个神经元构建的细节。人工神经元的示意图可以表示如下：
- en: '![](img/e343766d-3cc8-4b5f-b2bb-9f6ff1a0e948.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e343766d-3cc8-4b5f-b2bb-9f6ff1a0e948.png)'
- en: The combination function calculates the resulting input as the sum of the inputs
    weighted by w[i]. The activation function calculates the output using this input.
    The output range is usually limited to [0;1], using different functions. It is
    often the case that the neuron transmits the signal only if the input value is
    above a certain threshold.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 组合函数计算加权输入的总和作为结果输入，其中w[i]是权重。激活函数使用这个输入来计算输出。输出范围通常限制在[0;1]之间，使用不同的函数。通常情况下，只有当输入值高于某个特定阈值时，神经元才会传递信号。
- en: How does an artificial neural network learn? A training dataset is used, for
    which the outputs are known. The input values are fed into the network, the predicted
    output is compared to the real output, and the w[i] weights are adjusted iteratively
    at each step. This means that a neural network is a supervised learning model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络是如何学习的？使用一个训练数据集，其中输出是已知的。将输入值输入网络，将预测输出与实际输出进行比较，并在每一步迭代中调整 w[i] 权重。这意味着神经网络是一个监督学习模型。
- en: The more complex the problem, the larger the number of training samples needed
    to adjust the weights. We will also see that the number of hidden layers and neurons
    are also adjusted depending on the problem. Fine tuning these parameters is a
    complex problem, almost a field of study in itself.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 问题越复杂，所需的训练样本数量就越大，以调整权重。我们还将看到，隐藏层和神经元的数量也会根据问题进行调整。调整这些参数是一个复杂的问题，几乎可以成为一个研究领域。
- en: Artificial neural networks are useful since they can model any mathematical
    function. So, even if the relationship between the input values is unknown, we
    can use a network to reproduce it and make predictions.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络很有用，因为它们可以模拟任何数学函数。因此，即使输入值之间的关系未知，我们也可以使用网络来重现它并做出预测。
- en: Since the training process can be complicated and the number of parameters that
    are adjusted at training time is large, it is often difficult to understand why
    an artificial neural network correctly predicts a given value. The explainability
    of the artificial intelligence models, based on neural networks, is also an extensively
    studied problem.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练过程可能很复杂，并且在训练时调整的参数数量很大，因此通常很难理解为什么人工神经网络能够正确预测某个值。基于神经网络的智能模型的可解释性也是一个广泛研究的问题。
- en: 'Some applications of neural networks are as follows:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的某些应用如下：
- en: Image analysis—faces, objects, colors, expressions, and gestures
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分析——面部、物体、颜色、表情和手势
- en: Sound analysis—voices, speech to text, and sentiment
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 声音分析——声音、语音转文本和情感
- en: Text classification—email spam, fraud in document content, and sentiment
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类——电子邮件垃圾邮件、文档内容中的欺诈和情感
- en: Hardware failures—predictive and/or diagnostic
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 硬件故障——预测性和/或诊断性
- en: Health risks and/or diagnostics
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 健康风险和/或诊断
- en: Customer or employee churn
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 客户或员工流失
- en: Let's see how training works in practice, following an example.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来看看实际中的训练是如何进行的。
- en: Training a neural network
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练神经网络
- en: 'We will use a public dataset from the Blood Transfusion Service Center in Hsin-Chu
    City, Taiwan (*Knowledge discovery on RFM model using Bernoulli sequence*, by
    Yeh, I-Cheng, Yang, King-Jang, and Ting, Tao-Ming, Expert Systems with Applications,
    2008). The set contains information about blood donors, summarized in five variables:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用来自台湾新竹市血液输血服务中心的公共数据集（由 Yeh, I-Cheng, Yang, King-Jang 和 Ting, Tao-Ming
    编写的《使用伯努利序列在RFM模型上的知识发现》，发表于2008年的《Expert Systems with Applications》）。该数据集包含有关献血者的信息，总结为五个变量：
- en: R (Recency – months since last donation)
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R（最近性 - 自上次捐赠以来的月数）
- en: F (Frequency – total number of donations)
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: F（频率 - 总捐赠次数）
- en: M (Monetary – total blood donated in c.c.)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: M（货币 - 总捐赠血液量，单位为cc）
- en: T (Time – months since first donation)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: T（时间 - 自首次捐赠以来的月数）
- en: A binary variable representing whether they donated blood in March 2007 (one
    stands for donating blood; zero stands for not donating blood)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个二元变量，表示他们是否在2007年3月捐赠了血液（1表示捐赠血液；0表示未捐赠血液）
- en: 'We would like to prove how well an artificial neural network can learn from
    the first four of the preceding features, and predict the target, variable five.
    Follow these steps to reproduce and learn about the calculations already shown
    in the `transfusion.xlsx` file:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想证明人工神经网络如何从前面的四个特征中学习，并预测目标变量五。按照以下步骤重现并了解 `transfusion.xlsx` 文件中已显示的计算：
- en: Load the `transfusion.xlsx` file into Excel.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `transfusion.xlsx` 文件加载到 Excel 中。
- en: 'In the worksheet named `transfusion`*,* you will find the input data. It should
    look something like the following screenshot:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在名为 `transfusion`* 的表格中，您将找到输入数据。它应该看起来像以下截图：
- en: '![](img/7967366a-93c2-410f-ad15-6d600228ff6d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7967366a-93c2-410f-ad15-6d600228ff6d.png)'
- en: Since the data is not presented in any particular order, we can use the first
    500 entries to train the neural network. Open a new worksheet and rename it `training1`
    (remember that we are repeating the steps to create the worksheets already present
    in the file, so that you can compare your results).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于数据没有按照任何特定的顺序呈现，我们可以使用前500个条目来训练神经网络。打开一个新的工作表并将其重命名为 `training1`（记住，我们正在重复创建文件中已经存在的工作表，以便你可以比较你的结果）。
- en: 'Create a set of variables like the one you see in the following screenshot.
    If you use the same cells, it will be easier to follow the next steps:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一组变量，就像你在下面的屏幕截图中所看到的那样。如果你使用相同的单元格，将更容易跟随下一步：
- en: '![](img/69450203-d090-42ce-92f0-2302834382e2.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/69450203-d090-42ce-92f0-2302834382e2.png)'
- en: 'If we build an artificial neural network with four inputs (the four features
    in the input data) and one hidden layer containing two neurons, we need eight
    weight parameters: `w[11]`, `w[12]`, `w[13]`, and `w[14]` for hidden neuron one,
    and `w[2][1]`, `w[21]`, `w[23]` and `w[24]` for hidden neuron two. The remaining
    parameters will be explained later.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们构建一个具有四个输入（输入数据中的四个特征）和一个包含两个神经元的隐藏层的艺术神经网络，我们需要八个权重参数：`w[11]`、`w[12]`、`w[13]`
    和 `w[14]` 用于隐藏神经元一，以及 `w[2][1]`、`w[21]`、`w[23]` 和 `w[24]` 用于隐藏神经元二。其余参数将在稍后解释。
- en: From the worksheet named `transfusion`*,* copy the first 500 data rows (excluding
    the header).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从名为 `transfusion` 的电子表格中复制前500行数据（不包括标题）。
- en: In the `training1` worksheet click on cell *B22.*
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 `training1` 工作表中点击B22单元格。
- en: Paste the copied cells.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 粘贴复制的单元格。
- en: You now have a table containing the input values, called x[1], x[2], x[3], [and]
    x[4], plus the output binary value, y. Column *#* in the table just shows the
    row number.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你现在有一个包含输入值、称为x[1]、x[2]、x[3]、[和] x[4]，以及输出二进制值y的表格。表格中的 *#* 列仅显示行号。
- en: 'The combination function of the hidden neuron *j* is the weighted sum of the
    inputs, as shown in the following formula:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏神经元 *j* 的组合函数是输入的加权和，如下公式所示：
- en: '![](img/61e5a896-c544-4479-92ce-70b8a57ad7b9.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61e5a896-c544-4479-92ce-70b8a57ad7b9.png)'
- en: 'In our example, *N=4*, which gives us the next two expressions:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，*N=4*，这给我们带来了以下两个表达式：
- en: '![](img/d7b2f35d-7a34-4e15-84a2-f6aaf583b23f.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7b2f35d-7a34-4e15-84a2-f6aaf583b23f.png)'
- en: '![](img/2e2aefae-4e9f-4556-acbb-d2ec47513588.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e2aefae-4e9f-4556-acbb-d2ec47513588.png)'
- en: 'Taking into account these expressions, write the following formula in cell
    *G22*:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 考虑到这些表达式，在G22单元格中写下以下公式：
- en: '*=$E$3*B22+$E$4*C22+$E$5*D22+$E$6*E22*'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '*=$E$3*B22+$E$4*C22+$E$5*D22+$E$6*E22*'
- en: 'In cell *H22*, write the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在H22单元格中写下以下内容：
- en: '*=$E$7*B22+$E$8*C22+$E$9*D22+$E$10*E22*'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '*=$E$7*B22+$E$8*C22+$E$9*D22+$E$10*E22*'
- en: 'Copy these expressions down to the rest of the cells in columns G and H. The
    simplest and most commonly used activation function is the following sigmoid function:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些表达式复制到G列和H列的其余单元格中。最简单且最常用的激活函数是以下Sigmoid函数：
- en: '![](img/0f5b8a1e-0a46-446d-a652-b3fb87820ca5.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f5b8a1e-0a46-446d-a652-b3fb87820ca5.png)'
- en: 'In our example, x is the combination function calculated for each hidden neuron
    and each entry used for training:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，x是针对每个隐藏神经元和每个用于训练的条目计算的组合函数：
- en: '![](img/61804567-fd35-481b-8e62-d2f33bb64a3d.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/61804567-fd35-481b-8e62-d2f33bb64a3d.png)'
- en: '![](img/8d789415-0d7b-46c7-bc99-fbb0f31af1d0.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8d789415-0d7b-46c7-bc99-fbb0f31af1d0.png)'
- en: Define cell I22 as *=1/(1+EXP(-G22))* and cell J22 as *=1/(1+EXP(-H22)).*
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义I22单元格为 *=1/(1+EXP(-G22))*，并将J22单元格定义为 *=1/(1+EXP(-H22)).*
- en: 'Copy these formulas down to the rest of the rows in columns I and J. The last
    calculation is the neural network output, which is a weighted sum of the outputs
    from the hidden neurons, plus a constant value that acts as a threshold; if the
    total input is less that this value, the output is zero and the network does not
    activate. This can be expressed by the following formula:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这些公式复制到I列和J列的其余行中。最后的计算是神经网络输出，它是隐藏神经元输出的加权和，加上一个作为阈值的常数；如果总输入小于此值，输出为零且网络不激活。这可以用以下公式表示：
- en: '![](img/342955f2-5ac0-4bcc-ac54-baca8e6f59a2.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](img/342955f2-5ac0-4bcc-ac54-baca8e6f59a2.png)'
- en: 'You can then write the following in cell *K22*:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，你可以在K22单元格中写下以下内容：
- en: '*=$E$11+$E$12*I22+$E$13*J22*'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '*=$E$11+$E$12*I22+$E$13*J22*'
- en: 'Copy the formula down to the rest of the cells in column K. Since *E11*, *E12*,
    and *E13* are the cells we have saved for theta 1, theta 2 and theta 3 respectively.
    We use all the defined weights and parameters in our calculations, but we don''t
    have values for them. Training a neural network implies finding the values for
    these parameters that make the output as close as the target value as possible,
    for example, for each combination of *x[1]*, *x**[2]*, *x[3]*,and *x[4]*, the
    difference between the value of *Output* and the value of *y* should be the minimum
    possible. We need to calculate three values then: the output error (Output-y),
    the squared error (Error²), and the sum of the squared errors, which is the value
    to minimize.'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将公式复制到 K 列的其余单元格。由于 *E11*、*E12* 和 *E13* 分别是我们为 theta 1、theta 2 和 theta 3 保存的单元格。我们在计算中使用了所有定义的权重和参数，但它们没有值。训练神经网络意味着找到这些参数的值，使得输出尽可能接近目标值，例如，对于
    *x[1]*、*x**[2]*、*x[3]* 和 *x[4]* 的每一种组合，*Output* 的值与 *y* 的值之间的差异应该是可能的最小值。我们需要计算三个值：输出误差（Output-y）、平方误差（Error²）和平方误差的总和，这是需要最小化的值。
- en: The function we are minimizing, the sum of the squared errors, is only one possible
    **loss function***.* There are other functions that are used to compare the output
    of the neural network with the training value. Studying when to apply each function
    is shown in more advanced machine learning books.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们要最小化的函数，即平方误差的总和，只是可能损失函数之一。还有其他函数用于比较神经网络输出与训练值。在更高级的机器学习书籍中可以看到何时应用每个函数。
- en: Define cell L22 as *=K22-F22.*
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单元格 L22 定义为 *=K22-F22.*
- en: Copy the formula down to the rest of the rows in column L.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将公式复制到 L 列的其余行。
- en: Define cell M22 as *=L22^2*.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单元格 M22 定义为 *=L22^2*。
- en: Copy the formula down to the rest of the rows in column M.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将公式复制到 M 列的其余行。
- en: Define cell E15 as *=SUM(M22:M521).* This is the sum of the squared errors.
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单元格 E15 定义为 *=SUM(M22:M521)*。这是平方误差的总和。
- en: 'We can now use Excel''s Solver to set values to w[11], w[12], w[13],w[14],
    w[2][1], w[21], w[23], w[24], θ[o], θ[1], and θ[2], while minimizing the sum of
    the squared errors:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 Excel 的“求解器”设置 w[11]、w[12]、w[13]、w[14]、w[2][1]、w[21]、w[23]、w[24]、θ[o]、θ[1]
    和 θ[2] 的值，同时最小化平方误差的总和：
- en: Navigate to Data.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导航到“数据”。
- en: Click on Solver.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“求解”。
- en: 'Fill in the details as shown in the following screenshot:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写以下截图所示的详细信息：
- en: '![](img/6c5c74e7-bfe4-4416-9cc2-d95d14f046be.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6c5c74e7-bfe4-4416-9cc2-d95d14f046be.png)'
- en: The objective is E15, where we store the sum of squared errors, and the variable
    cells E3 to E13.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是 E15，其中我们存储平方误差的总和，以及变量单元格 E3 到 E13。
- en: Click on Solve.
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“求解”。
- en: 'The optimal result is shown in the following table:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下表格显示了最佳结果：
- en: '| **Parameters** | **Values** |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **值** |'
- en: '| w11 | -3.915205816 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| w11 | -3.915205816 |'
- en: '| w12 | 0.055009315 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| w12 | 0.055009315 |'
- en: '| w13 | 0.016855755 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| w13 | 0.016855755 |'
- en: '| w14 | -0.301397506 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| w14 | -0.301397506 |'
- en: '| w21 | -0.016701972 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| w21 | -0.016701972 |'
- en: '| w22 | 0.451221978 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| w22 | 0.451221978 |'
- en: '| w23 | -0.001645853 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| w23 | -0.001645853 |'
- en: '| w24 | -0.011395209 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| w24 | -0.011395209 |'
- en: '| theta0 | -0.349977457 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| theta0 | -0.349977457 |'
- en: '| theta1 | 0.247932886 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| theta1 | 0.247932886 |'
- en: '| theta2 | 1.256803829 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| theta2 | 1.256803829 |'
- en: '| **Square error** | **77.02669809** |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| **平方误差** | **77.02669809** |'
- en: The results may vary depending on the type of regression used in Solver and
    on the initial values. The gradient descent algorithm search (explained in the
    *Understanding the backpropagation algorithm* section) might be trapped in a local
    minimum that has a larger value than the global minimum.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能因求解器中使用的回归类型和初始值而异。梯度下降算法搜索（在 *理解反向传播算法* 部分中解释）可能会陷入一个局部最小值，其值大于全局最小值。
- en: Define cell N22 as *=round(K22)* to convert the output of the neural network
    to binary values.
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将单元格 N22 定义为 *=round(K22)* 以将神经网络的输出转换为二进制值。
- en: 'Comparing the predicted and linear values, you can build the confusion matrix:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较预测值和线性值，你可以构建混淆矩阵：
- en: '|  |  | Real |  |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 实际值 |  |'
- en: '|  |  | 1 | 0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  |  | 1 | 0 |'
- en: '| Predicted | 1 | 32 | 22 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 预测值 | 1 | 32 | 22 |'
- en: '|  | 0 | 86 | 360 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 0 | 86 | 360 |'
- en: Use the confusion matrix to measure the accuracy of the neural network training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 使用混淆矩阵来衡量神经网络训练的准确性。
- en: Testing the neural network
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试神经网络
- en: Once you are satisfied with the training, you can use the values obtained for
    the parameters to **predict** the y value for the rest of the data (which was
    never used in the training, and can then be used to test the network output).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对训练结果满意，你可以使用获得的参数值来**预测**剩余数据的 y 值（这些数据在训练中从未使用过，然后可以用来测试网络输出）。
- en: 'Follow these steps to predict the target variable using the test dataset:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤使用测试数据集预测目标变量：
- en: Make a copy of the worksheet named `training1`. Name the new worksheet `test1`.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制名为 `training1` 的工作表。将新工作表命名为 `test1`。
- en: Delete the range of cells *B22:F521.*
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除 B22:F521 的单元格范围。
- en: Copy the last 248 rows in the worksheet named `transfusion` to the new worksheet,
    starting on cell B22.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将名为 `transfusion` 的工作表中的最后 248 行复制到新的工作表中，从 B22 单元格开始。
- en: All calculations should work and you should be able to see the results of using
    the test data as input.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有计算都应该正常工作，你应该能够看到使用测试数据作为输入的结果。
- en: We have now developed a simple exercise that shows how an artificial neural
    network learns from input data. The calculations we made are the base of the **backpropagation**
    algorithm, which is explained in detail in the last section of this chapter.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在开发了一个简单的练习，展示了人工神经网络如何从输入数据中学习。我们进行的计算是**反向传播**算法的基础，该算法在本章的最后部分进行了详细解释。
- en: Building a deep network
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 构建深度网络
- en: 'Our example of artificial neural network is very simple and only contains one
    hidden layer. Can we add more layers? Of course we can! The next step in complexity
    could be something similar to the following diagram:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单人工神经网络示例非常简单，只包含一个隐藏层。我们能否添加更多层？当然可以！复杂性的下一步可能类似于以下图示：
- en: '![](img/5c050b30-e1ed-4643-aa73-191c25058bc4.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5c050b30-e1ed-4643-aa73-191c25058bc4.png)'
- en: We added a new hidden layer with two neurons, but we could add more layers and
    more neurons per layer. The architecture of a network depends on the specific
    use we give it. Multilayer artificial neural networks are often known as **deep
    neural networks***.*
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们添加了一个包含两个神经元的新的隐藏层，但我们也可以添加更多层，每层更多的神经元。网络的架构取决于我们赋予它的特定用途。多层人工神经网络通常被称为**深度神经网络**。
- en: The output of a deep network is calculated in analogy with the single layer
    one, considering all inputs to each neuron, the activation function, and the addition
    of all the inputs to the output neuron. Looking at the preceding diagram, it is
    clear that each layer in the network is affected by the previous one. It is usually
    the case that, in order to solve complex problems, each layer learns a specific
    set of characteristics. For example, when identifying an image, the first layer
    could train on colors, the second on shapes, the third on objects, and so on,
    increasing in complexity as we advance toward the output.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 深度网络的输出计算类似于单层网络，考虑了每个神经元的所有输入、激活函数以及所有输入到输出神经元的加和。从先前的图中可以看出，网络中的每一层都受到前一层的影响。通常情况下，为了解决复杂问题，每一层都会学习一组特定的特征。例如，在识别图像时，第一层可能训练颜色，第二层训练形状，第三层训练物体，等等，随着我们向输出方向前进，复杂性逐渐增加。
- en: As we add more neurons to the network, there are more parameters we need to
    adjust. The way this is done in practice will become clear in the following section,
    where the backpropagation algorithm is described.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们在网络中添加更多的神经元，我们需要调整的参数也更多。在实践中如何做到这一点将在以下部分中变得清晰，其中将描述反向传播算法。
- en: Understanding the backpropagation algorithm
  id: totrans-116
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解反向传播算法
- en: 'There are two phases in the training process of a deep neural network: forward
    and back propagation. We have seen the forward phase in detail:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 深度神经网络训练过程中有两个阶段：前向传播和反向传播。我们已经详细了解了前向传播阶段：
- en: 'Calculate the weighted sum of the inputs:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输入的加权总和：
- en: '![](img/f2c41a91-087f-4c24-9267-6ffa5e890830.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f2c41a91-087f-4c24-9267-6ffa5e890830.png)'
- en: 'Apply the activation function to the result:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将激活函数应用于结果：
- en: '![](img/16c6e136-e030-4cd2-a071-df9461aca7dc.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/16c6e136-e030-4cd2-a071-df9461aca7dc.png)'
- en: Find different activation functions in the suggested reading at the end of the
    chapter. The sigmoid function is the most common and is easier to use, but not
    the only one.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在章节末尾的建议阅读材料中找到不同的激活函数。Sigmoid 函数是最常见的，且易于使用，但并非唯一的选择。
- en: 'Calculate the output by adding all the results from the last layer (N neurons):'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将最后一层的所有结果（N 个神经元）相加来计算输出：
- en: '![](img/d5f5294b-af9c-4713-85b2-41757d612f22.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d5f5294b-af9c-4713-85b2-41757d612f22.png)'
- en: 'After the forward phase, we calculate the error as the difference between the
    output and the known target value: *Error = (Output-y)².*'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播阶段之后，我们计算误差为输出和已知目标值之间的差异：*误差 = (输出-y)²*。
- en: All weights are assigned random values at the beginning of the forward phase.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 所有权重在正向传播阶段开始时都分配了随机值。
- en: 'The output, and therefore the error, are functions of the weights *w[i]* and
    *θ[i]*. This means that we could go backward from the error and see how a small
    variation in each weight affects the result. This is expressed in mathematical
    terms as the derivative or gradient:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 输出以及因此产生的误差是权重 *w[i]* 和 *θ[i]* 的函数。这意味着我们可以从误差出发，查看每个权重的微小变化如何影响结果。这在数学上表示为导数或梯度：
- en: '![](img/08aca1b3-5c40-4fa9-b81c-6537eb05be9c.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/08aca1b3-5c40-4fa9-b81c-6537eb05be9c.png)'
- en: 'This equation measures the change in the error every time we change w[1] by
    a small amount. We actually apply an activation function inside each neuron, so
    the change in error turns into the following equation (known as the chain rule):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程衡量了每次我们通过微小地改变 w[1] 而产生的误差变化。实际上，我们在每个神经元内部应用一个激活函数，因此误差的变化转化为以下方程（称为链式法则）：
- en: '![](img/c02d2573-640e-4923-b633-c129c4ca12aa.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c02d2573-640e-4923-b633-c129c4ca12aa.png)'
- en: 'We want to change all weights values in the direction that decreases the error.
    This is the reason why the optimization method is called **gradient descent**.
    If we imagine the error as a function of two weights (there are more than two,
    of course, but we human beings have a hard time thinking beyond three dimensions!),
    we can picture this optimization as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望改变所有权重值的方向，以减少误差。这就是为什么优化方法被称为**梯度下降**。如果我们想象误差是两个权重（当然，实际上不止两个，但我们人类很难想象超过三维的情况！）的函数，我们可以这样想象这个优化：
- en: '![](img/a5ef8f3f-cfa8-4b61-aaf1-05b5f915d86f.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a5ef8f3f-cfa8-4b61-aaf1-05b5f915d86f.png)'
- en: 'When are the weights adjusted? There are three methods:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 权重何时进行调整？有三种方法：
- en: '**Online**: With each new training sample, all weights are recalculated. This
    is very time-consuming and could lead to problems if the dataset has too many
    outliers.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线**: 每次新的训练样本，所有权重都会重新计算。这非常耗时，如果数据集有太多异常值，可能会导致问题。'
- en: '**Batch**: The weights are calculated for the whole training dataset, calculating
    the accumulated error and using it to correct them.'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**批量**: 对整个训练数据集计算权重，计算累积误差并用来纠正它们。'
- en: '**Stochastic**: The batch mode is used taking small samples of the training
    data. This speeds up the whole process and makes the method more robust against
    local optimal values.'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机**: 使用批量模式，取训练数据的小样本。这加快了整个过程，并使方法对局部最优值更加稳健。'
- en: We are now familiar with how artificial neural networks are built and how their
    output is calculated. It is generally impractical to perform these calculations
    as the size of the network grows, as often happens for all practical and useful
    implementations.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在熟悉了人工神经网络是如何构建的以及它们的输出是如何计算的。随着网络规模的增大，通常不切实际进行这些计算，这在所有实际和有用的实现中经常发生。
- en: Summary
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We have studied the basic principles of how artificial neural networks are built
    and how they learn from the input data. Even if the actual method, in practice,
    for using neural networks is different than what we have done in our example,
    our approach is useful in order to understand the details and to go beyond the
    idea that neural networks are mysterious black boxes that magically solve problems.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经研究了人工神经网络构建的基本原理以及它们如何从输入数据中学习。即使实际使用神经网络的方法与我们的示例不同，我们的方法对于理解细节和超越神经网络是神秘的黑盒子、神奇地解决问题的想法是有用的。
- en: In the next chapter, we will see how we can use pre-built machine learning models
    available in Azure, connecting them to Excel to solve the problems we have presented
    up to now.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何使用Azure中可用的预构建机器学习模型，将它们连接到Excel来解决我们迄今为止提出的问题。
- en: Questions
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题
- en: Using the results of the perceptron test, build the confusion matrix and evaluate
    the quality of the prediction.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用感知器测试的结果，构建混淆矩阵并评估预测的质量。
- en: 'There is one important step that is missing in the binary classification problem
    that we solved with our artificial neural network, which might improve the result
    if we implement it. What did we miss? Hint: build an histogram of the binary variable
    that indicates whether there was a blood donation in March 2007.'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们用人工神经网络解决的二分类问题中，遗漏了一个重要的步骤，如果我们实现它可能会改善结果。我们遗漏了什么？提示：构建一个表示2007年3月是否有献血行为的二进制变量的直方图。
- en: Further reading
  id: totrans-144
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*Brief Introduction to Neural Networks* by David Kriesel*,* available online
    at [http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《神经网络简明介绍》* by David Kriesel*，可在[http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-1col-dkrieselcom.pdf)在线获取'
- en: '*Neural Networks and Deep Learning* by Michael A. Nielsen*,* available online
    at [http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《神经网络与深度学习》* by Michael A. Nielsen*，可在[http://neuralnetworksanddeeplearning.com/](http://neuralnetworksanddeeplearning.com/)在线获取'
- en: '*Deep Learning: Using Algorithms to Make Machines Think*, [https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/](https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*《深度学习：利用算法让机器思考》*，[https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/](https://opensourceforu.com/2017/12/deep-learning-using-algorithms-to-make-machines-think/)'
