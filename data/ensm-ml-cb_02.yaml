- en: Getting Started with Ensemble Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开始学习集成机器学习
- en: 'In this chapter, we''ll cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍以下食谱：
- en: Max-voting
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大投票
- en: Averaging
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均值
- en: Weighted averaging
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权平均
- en: Introduction to ensemble machine learning
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成机器学习简介
- en: Simply speaking, ensemble machine learning refers to a technique that integrates
    output from multiple learners and is applied to a dataset to make a prediction.
    These multiple learners are usually referred to as base learners. When multiple
    base models are used to extract predictions that are combined into one single
    prediction, that prediction is likely to provide better accuracy than individual
    base learners.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，集成机器学习是一种将多个学习者的输出集成并应用于数据集进行预测的技术。这些多个学习者通常被称为基学习器。当使用多个基模型提取预测并将它们组合成一个单一的预测时，这个预测很可能比单个基学习器提供更高的准确性。
- en: Ensemble models are known for providing an advantage over single models in terms
    of performance. They can be applied to both regression and classification problems.
    You can either decide to build ensemble models with algorithms from the same family
    or opt to pick them from different families. If multiple models are built on the
    same dataset using neural networks only, then that ensemble would be called a
    **homogeneous ensemble model**. If multiple models are built using different algorithms,
    such as **support vector machines** (**SVMs**), neural networks, and random forests,
    then the ensemble model would be called a **heterogeneous ensemble model**.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型因其性能优于单个模型而闻名。它们可以应用于回归和分类问题。你可以选择使用同一家族的算法构建集成模型，或者选择从不同家族中选择。如果仅使用神经网络在同一个数据集上构建多个模型，那么这个集成模型被称为**同质集成模型**。如果使用不同的算法，如**支持向量机**（**SVMs**）、神经网络和随机森林构建多个模型，那么这个集成模型被称为**异质集成模型**。
- en: 'The construction of an ensemble model requires two steps:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 构建集成模型需要两个步骤：
- en: Base learners are learners that are designed and fit on training data
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基学习器是设计并拟合在训练数据上的学习者
- en: The base learners are combined to form a single prediction model by using specific
    ensembling techniques such as max-voting, averaging, and weighted averaging
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用特定的集成技术，如最大投票、平均和加权平均，将基学习器组合成一个单一的预测模型。
- en: 'The following diagram shows the structure of the ensemble model:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了集成模型的结构：
- en: '![](img/f805ffc2-b5f2-44a0-a330-a1e09b822eac.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f805ffc2-b5f2-44a0-a330-a1e09b822eac.png)'
- en: However, to get an ensemble model that performs well, the base learners themselves
    should be as accurate as possible. A common way to measure the performance of
    a model is to evaluate its generalization error. A generalization error is a term
    to measure how accurately a model is able to make a prediction, based on a new
    dataset that the model hasn't seen.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了得到性能良好的集成模型，基学习器本身应该尽可能准确。衡量模型性能的常见方法是对其泛化误差进行评估。泛化误差是一个术语，用来衡量模型基于一个模型尚未见过的新的数据集进行预测的准确性。
- en: To perform well, the ensemble models require a sufficient amount of data. Ensemble
    techniques prove to be more useful when you have large and non-linear datasets.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表现良好，集成模型需要足够的数据。当拥有大型和非线性数据集时，集成技术证明更有用。
- en: An ensemble model may overfit if too many models are included, although this
    isn't very common.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果包含太多的模型，集成模型可能会过拟合，尽管这种情况并不常见。
- en: Irrespective of how well you fine-tune your models, there's always the risk
    of high bias or high variance. Even the best model can fail if the bias and variance
    aren't taken into account while training the model. Both bias and variance represent
    a kind of error in the predictions. In fact, the total error is comprised of bias-related
    error, variance-related error, and unavoidable noise-related error (or irreducible
    error). The noise-related error is mainly due to noise in the training data and
    can't be removed. However, the errors due to bias and variance can be reduced.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你如何微调你的模型，都存在高偏差或高方差的风险。即使是最优秀的模型，如果在训练模型时没有考虑到偏差和方差，也可能失败。偏差和方差都代表了预测中的一种错误。实际上，总误差由偏差相关误差、方差相关误差和不可避免的噪声相关误差（或不可减少误差）组成。噪声相关误差主要由于训练数据中的噪声引起，无法消除。然而，偏差和方差引起的误差可以减少。
- en: 'The total error can be expressed as follows:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 总误差可以表示如下：
- en: '[PRE0]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A measure such as **mean square error** (**MSE**) captures all of these errors
    for a continuous target variable and can be represented as follows:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一个如**均方误差**（**MSE**）这样的度量可以捕捉连续目标变量的所有误差，可以表示如下：
- en: '![](img/89f95d7f-2f1e-4f9e-b2c4-3eda6906ffb2.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![img/89f95d7f-2f1e-4f9e-b2c4-3eda6906ffb2.png](img/89f95d7f-2f1e-4f9e-b2c4-3eda6906ffb2.png)'
- en: 'In this formula, *E* stands for the expected mean, *Y* represents the actual
    target values and ![](img/9e4b0739-22af-4cc1-816a-a36dd771abf2.png) is the predicted
    values for the target variable. It can be broken down into its components such
    as bias, variance and noise as shown in the following formula:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*E*代表期望均值，*Y*代表实际的目标值，而![img/9e4b0739-22af-4cc1-816a-a36dd771abf2.png](img/9e4b0739-22af-4cc1-816a-a36dd771abf2.png)是目标变量的预测值。它可以分解为其组成部分，如偏差、方差和噪声，如下公式所示：
- en: '![](img/21b41a6e-ac17-4354-ae2b-daea48ef913e.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![img/21b41a6e-ac17-4354-ae2b-daea48ef913e.png](img/21b41a6e-ac17-4354-ae2b-daea48ef913e.png)'
- en: While bias refers to how close is the ground truth to the expected value of
    our estimate, the variance, on the other hand, measures the deviation from the
    expected estimator value. Estimators with small MSE is what is desirable. In order
    to minimize the MSE error, we would like to be centered (0-bias) at ground truth
    and have a low deviation (low variance) from the ground truth (correct) value.
    In other words, we'd like to be confident (low variance, low uncertainty, more
    peaked distribution) about the value of our estimate. High bias degrades the performance
    of the algorithm on the training dataset and leads to underfitting. High variance,
    on the other hand, is characterized by low training errors and high validation
    errors. Having high variance reduces the performance of the learners on unseen
    data, leading to overfitting.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当偏差指的是真实值与我们的估计期望值有多接近时，方差另一方面衡量的是与期望估计值偏差的程度。具有小MSE的估计器是可取的。为了最小化MSE误差，我们希望我们的估计值在真实值上居中（0偏差）并且与真实值（正确）的偏差低（低方差）。换句话说，我们希望对估计值的价值有信心（低方差，低不确定性，分布更尖锐）。高偏差会降低算法在训练数据集上的性能并导致欠拟合。另一方面，高方差的特点是训练误差低而验证误差高。高方差会降低学习者在未见数据上的性能，导致过拟合。
- en: Ensemble models can reduce bias and/or variance in the models.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 集成模型可以减少模型中的偏差和/或方差。
- en: Max-voting
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大投票法
- en: Max-voting, which is generally used for classification problems, is one of the
    simplest ways of combining predictions from multiple machine learning algorithms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最大投票法，通常用于分类问题，是结合多个机器学习算法预测的简单方法之一。
- en: In max-voting, each base model makes a prediction and votes for each sample.
    Only the sample class with the highest votes is included in the final predictive
    class.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在最大投票法中，每个基础模型做出预测并对每个样本进行投票。只有获得最高票数的样本类别被包含在最终的预测类别中。
- en: For example, let's say we have an online survey, in which consumers answer a
    question in a five-level Likert scale. We can assume that a few consumers will
    provide a rating of five, while others will provide a rating of four, and so on.
    If a majority, say more than 50% of the consumers, provide a rating of four, then
    the final rating is taken as four. In this example, taking the final rating as
    four is similar to taking a mode for all of the ratings.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个在线调查，消费者在五级李克特量表上回答一个问题。我们可以假设一些消费者会给出五分的评价，而其他人会给出四分的评价，依此类推。如果大多数消费者，比如说超过50%的消费者给出了四分的评价，那么最终的评分就是四分。在这个例子中，将最终评分定为四分类似于对所有评分取众数。
- en: Getting ready
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'In the following steps we will download the following packages:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下步骤中，我们将下载以下包：
- en: 'To start with, import the `os` and `pandas` packages and set your working directory
    according to your requirements:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入`os`和`pandas`包，并根据您的需求设置工作目录：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Download the `Cryotherapy.csv` dataset from GitHub and copy it to your working
    directory. Read the dataset:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 从GitHub下载`Cryotherapy.csv`数据集并将其复制到您的工作目录。读取数据集：
- en: '[PRE2]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Take a look at the data with the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码查看数据：
- en: '[PRE3]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We can see that the data has been read properly and has the `Result_of_Treatment` class
    variable. We then move on to creating models with `Result_of_Treatment` as the
    response variable.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据已被正确读取，并且有`Result_of_Treatment`类变量。然后我们继续创建以`Result_of_Treatment`作为响应变量的模型。
- en: How to do it...
  id: totrans-38
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: 'You can create a voting ensemble model for a classification problem using the
    `VotingClassifier` class from Python''s `scikit-learn` library. The following
    steps showcase an example of how to combine the predictions of the decision tree,
    SVMs, and logistic regression models for a classification problem:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用Python的`scikit-learn`库中的`VotingClassifier`类为分类问题创建一个投票集成模型。以下步骤展示了如何将决策树、SVM和逻辑回归模型的预测组合起来解决一个分类问题：
- en: 'Import the required libraries for building the decision tree, SVM, and logistic
    regression models. We also import `VotingClassifier` for max-voting:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入构建决策树、SVM和逻辑回归模型所需的库。我们还导入`VotingClassifier`进行最大投票：
- en: '[PRE4]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then move on to building our feature set and creating our train and test
    datasets:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们接下来构建我们的特征集并创建我们的训练和测试数据集：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We build our models with the decision tree, SVM, and logistic regression algorithms:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用决策树、SVM和逻辑回归算法构建我们的模型：
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We build individual models with each of the classifiers we''ve chosen:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用我们选择的每个分类器构建单独的模型：
- en: '[PRE7]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can then see the accuracy score of each of the individual base learners:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以查看每个单个基学习器的准确度得分：
- en: '![](img/8525dd2f-69c0-494d-b766-f40177dc16d2.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8525dd2f-69c0-494d-b766-f40177dc16d2.png)'
- en: 'We proceed to ensemble our models and use `VotingClassifier` to score the accuracy
    of the ensemble model:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们继续集成我们的模型，并使用`VotingClassifier`来评估集成模型的准确度：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can see the accuracy score of the ensemble model using `Hard Voting`:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Hard Voting`查看集成模型的准确度得分：
- en: '![](img/7cfc6f63-ea79-42d0-93f7-ae8601618f96.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7cfc6f63-ea79-42d0-93f7-ae8601618f96.png)'
- en: How it works...
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: '`VotingClassifier` implements two types of voting—**hard** and **soft** voting.
    In hard voting, the final class label is predicted as the class label that has
    been predicted most frequently by the classification models. In other words, the
    predictions from all classifiers are aggregated to predict the class that gets
    the most votes. In simple terms, it takes the mode of the predicted class labels.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`VotingClassifier`实现了两种投票类型——**硬投票**和**软投票**。在硬投票中，最终的类别标签被预测为被分类模型预测最频繁的类别标签。换句话说，所有分类器的预测被汇总以预测获得最多投票的类别。简单来说，它取预测类别标签的众数。'
- en: 'In hard voting for the class labels, ![](img/cdcf3a61-7859-4677-ae40-0ad28f93b20b.png) is
    the prediction based on the majority voting of each classifier ![](img/14f699f6-50a7-48d2-9954-00f6f3b93265.png),
    where *i=1.....n* observations, we have the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在对类别标签进行硬投票时，![](img/cdcf3a61-7859-4677-ae40-0ad28f93b20b.png)是基于每个分类器![](img/14f699f6-50a7-48d2-9954-00f6f3b93265.png)的多数投票预测，其中*i=1.....n*个观察值，我们得到以下结果：
- en: '![](img/06173c0a-41c4-4350-b2ec-d5bcfa423a2d.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/06173c0a-41c4-4350-b2ec-d5bcfa423a2d.png)'
- en: 'As shown in the previous section, we have three models, one from the decision tree,
    one from the SVMs, and one from logistic regression. Let''s say that the models
    classify a training observation as class 1, class 0, and class 1 respectively.
    Then with majority voting, we have the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如前节所示，我们有三个模型，一个来自决策树，一个来自SVM，一个来自逻辑回归。假设这些模型将训练观察值分别分类为类别1、类别0和类别1。然后通过多数投票，我们得到以下结果：
- en: '![](img/9957aac7-8217-431a-8822-5a4e87bea84c.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9957aac7-8217-431a-8822-5a4e87bea84c.png)'
- en: In this case, we would classify the observation as class 1.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将观察值分类为类别1。
- en: In the preceding section, in* Step 1*, we imported the required libraries to
    build our models. In *Step 2*, we created our feature set. We also split our data
    to create the training and testing samples. In *Step 3*, we trained three models
    with the decision tree, SVMs, and logistic regression respectively. In *Step 4*,
    we looked at the accuracy score of each of the base learners, while in *Step 5*,
    we ensembled the models using `VotingClassifier()` and looked at the accuracy
    score of the ensemble model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一节中，在*步骤1*中，我们导入了构建模型所需的库。在*步骤2*中，我们创建了特征集。我们还分割数据以创建训练和测试样本。在*步骤3*中，我们分别使用决策树、SVM和逻辑回归训练了三个模型。在*步骤4*中，我们查看每个基学习器的准确度得分，而在*步骤5*中，我们使用`VotingClassifier()`集成模型并查看集成模型的准确度得分。
- en: There's more...
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 还有更多...
- en: Many classifiers can estimate class probabilities. In this case, the class labels
    are predicted by averaging the class probabilities. This is called **soft voting** and is
    recommended for an ensemble of well-tuned classifiers.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分类器可以估计类别概率。在这种情况下，类别标签是通过平均类别概率来预测的。这被称为**软投票**，并且对于一组调优良好的分类器来说是被推荐的。
- en: In the `scikit-learn` library, many classification algorithms have the `predict_proba()`
    method to predict the class probabilities. To perform the ensemble with soft voting,
    simply replace `voting='hard'` with `voting='soft'` in `VotingClassifier()`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在`scikit-learn`库中，许多分类算法都有`predict_proba()`方法来预测类概率。要执行软投票集成，只需在`VotingClassifier()`中将`voting='hard'`替换为`voting='soft'`。
- en: 'The following code creates an ensemble using soft voting:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码创建了一个使用软投票的集成：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We get to see the accuracy from individual learners and the ensemble learner
    using soft voting:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到单个学习器和使用软投票的集成学习器的准确率：
- en: '![](img/e5c04d3c-702b-4e19-9e34-631718d46d9e.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e5c04d3c-702b-4e19-9e34-631718d46d9e.png)'
- en: The `SVC` class can't estimate class probabilities by default, so we've set
    its probability hyper-parameter to `True` in the preceding code. With `probability=True`,
    `SVC` will be able to estimate class probabilities.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`SVC`类默认不能估计类概率，因此我们在前面的代码中将它的概率超参数设置为`True`。当`probability=True`时，`SVC`将能够估计类概率。'
- en: Averaging
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 平均
- en: Averaging is usually used for regression problems or can be used while estimating
    the probabilities in classification tasks. Predictions are extracted from multiple
    models and an average of the predictions are used to make the final prediction.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 平均通常用于回归问题，或者在分类任务中估计概率时使用。预测是从多个模型中提取的，并使用预测的平均值来做出最终预测。
- en: Getting ready
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Let us get ready to build multiple learners and see how to implement averaging:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们准备好构建多个学习器，并看看如何实现平均：
- en: 'Download the `whitewines.csv` dataset from GitHub and copy it to your working
    directory, and let''s read the dataset:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 从GitHub下载`whitewines.csv`数据集并将其复制到工作目录，然后读取数据集：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let''s take a look at the data with the following code:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下代码看看数据：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following screenshot, we can see that the data has been read properly:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下屏幕截图中，我们可以看到数据已被正确读取：
- en: '![](img/f3e9bfa0-d5c9-4605-b4bd-133db9ca6cc4.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f3e9bfa0-d5c9-4605-b4bd-133db9ca6cc4.png)'
- en: How to do it...
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'We have a dataset that is based on the properties of wines. Using this dataset,
    we''ll build multiple regression models with the quality as our response variable.
    With multiple learners, we extract multiple predictions. The averaging technique
    would take the average of all of the predicted values for each training sample:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个基于葡萄酒特性的数据集。使用这个数据集，我们将构建多个回归模型，其中质量作为响应变量。使用多个学习器，我们可以提取多个预测。平均技术将取每个训练样本所有预测值的平均值：
- en: 'Import the required libraries:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Create the response and feature sets:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建响应和特征集：
- en: '[PRE13]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Split the data into training and testing sets:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集：
- en: '[PRE14]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Build the base regression learners using linear regression, `SVR`, and a decision
    tree:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用线性回归、`SVR`和决策树构建基础回归学习器：
- en: '[PRE15]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Use the base learners to make a prediction based on the test data:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用基础学习器根据测试数据做出预测：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Add the predictions and divide by the number of base learners:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加预测值并除以基础学习器的数量：
- en: '[PRE17]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: How it works...
  id: totrans-94
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step 1*, we imported the required packages. In *Step 2*, we separated the
    feature set and the response variable from our dataset. We split our dataset into
    training and testing samples in *Step 3*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤1*中，我们导入了所需的包。在*步骤2*中，我们将特征集和响应变量从数据集中分离出来。在*步骤3*中，我们将数据集分为训练样本和测试样本。
- en: Note that our response variable is continuous in nature. For this reason, we
    built our regression base learners in *Step 4* using linear regression, `SVR`,
    and a decision tree. In *Step 5*, we passed our test dataset to the `predict()`
    function to predict our response variable. And finally, in *Step 6*, we added
    all of the predictions together and divided them by the number of base learners,
    which is three in our example.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们的响应变量在本质上连续。因此，我们在*步骤4*中使用了线性回归、`SVR`和决策树来构建回归基础学习器。在*步骤5*中，我们将测试数据集传递给`predict()`函数来预测响应变量。最后，在*步骤6*中，我们将所有预测值相加，并除以基础学习器的数量，在我们的例子中是三个。
- en: Weighted averaging
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加权平均
- en: Like averaging, weighted averaging is also used for regression tasks. Alternatively,
    it can be used while estimating probabilities in classification problems. Base
    learners are assigned different weights, which represent the importance of each
    model in the prediction.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与平均一样，加权平均也用于回归任务。或者，它可以在分类问题中估计概率时使用。基础学习器被分配不同的权重，这些权重代表每个模型在预测中的重要性。
- en: A weight-averaged model should always be at least as good as your best model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 加权平均模型应该至少与你的最佳模型一样好。
- en: Getting ready
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'Download the `wisc_bc_data.csv` dataset from GitHub and copy it to your working
    directory. Let''s read the dataset:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从 GitHub 下载`wisc_bc_data.csv`数据集并将其复制到你的工作目录。让我们读取数据集：
- en: '[PRE18]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Take a look at the data with the following code:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码查看数据：
- en: '[PRE19]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can see that the data has been read properly:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到数据已经被正确读取：
- en: '![](img/2cc3ae34-687d-4175-a392-89311304bed7.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2cc3ae34-687d-4175-a392-89311304bed7.png)'
- en: How to do it...
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何做到这一点...
- en: Here, we have a dataset based on the properties of cancerous tumors. Using this
    dataset, we'll build multiple classification models with `diagnosis` as our response
    variable. The diagnosis variable has the values, `B` and `M`, which indicate whether
    the tumor is benign or malignant. With multiple learners, we extract multiple
    predictions. The weighted averaging technique takes the average of all of the
    predicted values for each training sample.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们有一个基于癌症肿瘤特性的数据集。使用这个数据集，我们将构建多个分类模型，其中`diagnosis`作为我们的响应变量。诊断变量具有`B`和`M`值，表示肿瘤是良性还是恶性。使用多个学习器，我们提取多个预测。加权平均技术取每个训练样本的所有预测值的平均值。
- en: 'In this example, we consider the predicted probabilities as the output and
    use the `predict_proba()` function of the scikit-learn algorithms to predict the
    class probabilities:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将预测概率视为输出，并使用 scikit-learn 算法的`predict_proba()`函数来预测类概率：
- en: 'Import the required libraries:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Create the response and feature sets:'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建响应和特征集：
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We retrieved the feature columns using the `iloc()` function of the `pandas`
    DataFrame, which is purely integer-location based indexing for selection by position. The
    `iloc()` function takes row and column selection as its parameter, in the form: `data.iloc(<row
    selection>, <column selection>)`. The row and column selection can either be an
    integer list or a slice of rows and columns. For example, it might look as follows:
    `df_cancerdata.iloc(2:100, 2:30)`.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`pandas` DataFrame 的`iloc()`函数检索特征列，该函数是基于整数位置的索引选择，用于按位置选择。`iloc()`函数接受行和列选择作为其参数，形式为：`data.iloc(<row
    selection>, <column selection>)`。行和列选择可以是整数列表或行和列的切片。例如，它可能看起来如下：`df_cancerdata.iloc(2:100,
    2:30)`。
- en: 'We''ll then split our data into training and testing sets:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将数据分为训练集和测试集：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Build the base classifier models:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建基础分类器模型：
- en: '[PRE23]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Fit the models on the test data:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试数据上拟合模型：
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Use the `predict_proba()` function to predict the class probabilities:'
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`predict_proba()`函数预测类概率：
- en: '[PRE25]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Assign different weights to each of the models to get our final predictions:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个模型分配不同的权重以获得最终的预测：
- en: '[PRE26]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: How it works...
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In *Step **1*, we imported the libraries that are required to build our models.
    In *Step 2*, we created the response and feature sets. We retrieved our feature
    set using the `iloc()` function of the `pandas` DataFrame. In *Step 3*, we split
    our dataset into training and testing sets. In *Step 4*, we built our base classifiers.
    Kindly note that we passed `probability=True` to our `SVC` function to allow `SVC()`
    to return class probabilities. In the `SVC` class, the default is `probability=False`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 1*中，我们导入了构建我们模型所需的库。在*步骤 2*中，我们创建了响应和特征集。我们使用`pandas` DataFrame 的`iloc()`函数检索我们的特征集。在*步骤
    3*中，我们将数据集分为训练集和测试集。在*步骤 4*中，我们构建了我们的基础分类器。请注意，我们向`SVC`函数传递了`probability=True`，以允许`SVC()`返回类概率。在`SVC`类中，默认值为`probability=False`。
- en: In *Step 5*, we fitted our model to the training data. We used the `predict_proba()`
    function in *Step 6* to predict the class probabilities for our test observations.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在*步骤 5*中，我们将模型拟合到训练数据。在*步骤 6*中，我们使用`predict_proba()`函数预测测试观察值的类概率。
- en: Finally, in *Step 7*, we assigned different weights to each of our models to
    estimate the weighted average predictions. The question that comes up is how to
    choose the weights. One way is to sample the weights uniformly and to make sure
    they normalize to one and validate on the test set and repeat keeping track of
    weights that provide the highest accuracy. This is an example of a random search.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在*步骤 7*中，我们为我们的每个模型分配了不同的权重来估计加权平均预测。随之而来的问题是如何选择权重。一种方法是对权重进行均匀采样，并确保它们归一化到
    1，然后在测试集上进行验证并重复，同时跟踪提供最高准确率的权重。这是一个随机搜索的例子。
- en: See also
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参见
- en: 'The following are the scikit reference links:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些 scikit 参考链接：
- en: Scikit guide to ensemble methods [(https://bit.ly/2oVNogs)](https://bit.ly/2oVNogs)
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit 模型集成方法指南 [(https://bit.ly/2oVNogs)](https://bit.ly/2oVNogs)
- en: Scikit guide to `VotingClassifier` [(https://bit.ly/2oW0avo)](https://bit.ly/2oW0avo)
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit 模型指南：`VotingClassifier` [(https://bit.ly/2oW0avo)](https://bit.ly/2oW0avo)
