- en: Risk versus Reward – Reinforcement Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 风险与回报 – 强化学习
- en: 'In this chapter, we will go a little deeper and learn about one of the hot
    topics in machine learning: reinforcement learning. We will cover several exciting
    examples to show how you can use this in your application. We''ll go over a few
    algorithms, and then after our first, more formal example, we will take you to
    a final exciting example that you are sure to enjoy!'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将更深入地探讨机器学习中的一个热门话题：强化学习。我们将涵盖几个令人兴奋的示例，以展示你如何在应用程序中使用它。我们将介绍几个算法，然后在我们的第一个更正式的示例之后，我们将带你到一个最终令人兴奋的示例，你一定会喜欢的！
- en: 'The following topics will covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Overviewing reinforcement learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习的概述
- en: Types of learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习类型
- en: Q-learning
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q学习
- en: SARSA
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SARSA
- en: Running our application
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行我们的应用程序
- en: Tower of Hanoi
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 汉诺塔
- en: Overviewing reinforcement learning
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 强化学习的概述
- en: 'As mentioned in [Chapter 1](7a1f2cca-1be5-426a-8e8a-6a4a3828cd76.xhtml), *Machine
    Learning Basics*, reinforcement learning is a case where the machine is trained
    for a specific outcome with the sole purpose of maximizing efficiency and/or performance.
    The algorithm is rewarded for making correct decisions and penalized for making
    incorrect ones, as shown in the following diagram:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第1章](7a1f2cca-1be5-426a-8e8a-6a4a3828cd76.xhtml)“机器学习基础”中提到的，强化学习是一个机器被训练以实现特定结果的情况，其唯一目的是最大化效率和/或性能。算法因做出正确决策而获得奖励，因做出错误决策而受到惩罚，如下面的图表所示：
- en: '![](img/33096271-1bd8-4af9-b961-c9d556df32c4.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/33096271-1bd8-4af9-b961-c9d556df32c4.png)'
- en: Continual training is used to constantly improve performance. The focus here
    is on performance, meaning somehow finding a balance between unseen data and what
    the algorithms have already learned. The algorithm applies an action to its environment,
    receives a reward or a penalty based on what it has done, repeats the process,
    and so on.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 持续训练用于不断改进性能。这里的重点是性能，意味着在未知数据和算法已学到的内容之间找到某种平衡。算法对其环境采取行动，根据其行为获得奖励或惩罚，然后重复此过程。
- en: We're going to dive right into the application in this chapter, and we're going
    to use the incredible Accord.NET open source machine learning framework to highlight
    how we can use reinforcement learning to help an autonomous object get from its
    starting location, depicted by a black object, to a desired end point, depicted
    by a red object.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将直接深入到应用程序的应用，并使用令人难以置信的 Accord.NET 开源机器学习框架来突出展示我们如何使用强化学习帮助一个自主物体从其起始位置（由黑色物体表示）到达一个期望的终点（由红色物体表示）。
- en: 'The concept is similar, although on a much lower scale of complexity, to what
    autonomous vehicles do to get you from point A to point B. Our example will allow
    you to use maps of various complexity, meaning various obstacles may appear in
    between your autonomous object and the desired location. Let''s look at our application:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概念与自主车辆从A点到B点的行为类似，尽管复杂度要低得多。我们的示例将允许你使用不同复杂度的地图，这意味着在自主物体和期望位置之间可能会有各种障碍。让我们看看我们的应用程序：
- en: '![](img/0e7b2b7c-9d43-4f03-9d8e-4e1d9576a2b5.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0e7b2b7c-9d43-4f03-9d8e-4e1d9576a2b5.png)'
- en: 'Here, you can see that we have a very basic map loaded, one with no obstacles
    but only exterior confining walls. The black block (start) is our autonomous object
    and the red block (stop) is our destination. Our goal in this application is to
    navigate the walls to get to our desired location. If our next move puts us onto
    a white block, our algorithm will be rewarded. If our next move puts us into a
    wall, it will be penalized. From this, our autonomous object should be able to
    get to its destination. The question is: how fast can it learn? In this example,
    there are absolutely no obstacles in its path, so there should be no issues solving
    the problem in the shortest number of moves possible.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你可以看到我们加载了一个非常基础的地图，一个没有障碍物，只有外部限制墙的地图。黑色方块（起点）是我们的自主物体，红色方块（停止）是我们的目的地。在这个应用程序中，我们的目标是导航墙壁以到达我们期望的位置。如果我们下一步移动到白色方块上，我们的算法将获得奖励。如果我们下一步移动到墙壁上，它将受到惩罚。从这个角度来看，我们的自主物体应该能够到达其目的地。问题是：它能多快学会？在这个例子中，它的路径上绝对没有任何障碍，所以应该没有问题在尽可能少的移动次数内解决问题。
- en: 'The following is another example of a somewhat more complicated map for our
    environment:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是我们环境中的另一个相对复杂的地图示例：
- en: '![](img/4c05f744-440e-460c-a01b-07de77b354c9.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4c05f744-440e-460c-a01b-07de77b354c9.png)'
- en: Types of learning
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习类型
- en: 'On the right-hand side of our application are our settings, as seen in the
    following screenshot. The first thing that we see is the learning algorithm. In
    this application, we will be dealing with two distinct learning algorithms, **Q-learning**
    and **State-Action-Reward-State-Action** (**SARSA**). Let''s briefly discuss both
    of these algorithms:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们应用程序的右侧是我们的设置，如下面的截图所示。我们看到的第一件事是学习算法。在这个应用程序中，我们将处理两种不同的学习算法，**Q-learning**和**状态-动作-奖励-状态-动作**（**SARSA**）。让我们简要讨论这两种算法：
- en: '![](img/4c0a67d9-da76-4634-b528-5f839ad71d04.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4c0a67d9-da76-4634-b528-5f839ad71d04.png)'
- en: Q-learning
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Q-learning
- en: Q-learning can identify an optimal action (that which has the highest value
    in each state) while in a given state without having a completely defined model
    of the environment. It is also great at handling problems with stochastic transitions
    and rewards without requiring tweaking or adaptations.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Q-learning可以在给定状态下识别出最佳动作（在每个状态下具有最高价值的动作），而无需对环境有一个完全定义的模型。它也非常擅长处理具有随机转换和奖励的问题，而无需调整或适应。
- en: 'Here is the mathematical intuition for Q-learning:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是Q-learning的数学直觉：
- en: '![](img/57e66621-c1c9-4b6b-9fd7-463cb2612e4b.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/57e66621-c1c9-4b6b-9fd7-463cb2612e4b.png)'
- en: Perhaps it's easier to comprehend if we provide a very high-level abstract example.
    The agent starts at state 1\. It then performs action 1 and gets reward 1\. Next,
    it looks around and sees what the maximum possible reward for an action in state
    2 is; it uses that to update the value of action 1\. And so on!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们提供一个非常高级的抽象示例，可能更容易理解。代理从状态1开始。然后执行动作1并获得奖励1。接下来，它四处张望，看看在状态2中动作的最大可能奖励是多少；它使用这个来更新动作1的价值。以此类推！
- en: SARSA
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SARSA
- en: '**SARSA** (you can already guess where this one is, going by the name) works
    like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**SARSA**（根据名字，你可以猜到这一点）的工作方式如下：'
- en: The agent starts at state 1
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 代理从状态1开始
- en: It then performs action 1 and gets reward 1
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后执行动作1并获得奖励1
- en: Next, it moves on to state 2, performs action 2, and gets reward 2
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，它移动到状态2，执行动作2，并获得奖励2
- en: Then, the agent goes back and updates the value of action 1
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，代理返回并更新动作1的价值
- en: As you can see, the difference in the two algorithms is in the way the future
    reward is found. Q-learning uses the highest action possible from state 2, while
    SARSA uses the value of the action that is actually taken.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，两种算法的区别在于寻找未来奖励的方式。Q-learning使用从状态2可能采取的最高动作，而SARSA使用实际采取的动作的价值。
- en: 'Here is the mathematical intuition for SARSA:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是SARSA的数学直觉：
- en: '![](img/a07b00f1-71ee-4b11-aee4-7d5eb98c7e82.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a07b00f1-71ee-4b11-aee4-7d5eb98c7e82.png)'
- en: Running our application
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行我们的应用程序
- en: For now, let's start using our application with our default parameters. Simply
    click on the Start button and the learning will commence. Once this is complete,
    you will be able to click on the Show Solution button, and the learned path will
    be animated from start to finish.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用默认参数开始使用我们的应用程序。只需点击“开始”按钮，学习过程就会开始。一旦完成，你将能够点击“显示答案”按钮，学习路径将从开始到结束进行动画展示。
- en: 'Clicking on Start will begin the learning stage and continue until the black
    object reaches its goal:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“开始”将开始学习阶段，并持续到黑色物体达到目标：
- en: '![](img/24bfed96-785a-4460-b149-73ccf8778c20.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/24bfed96-785a-4460-b149-73ccf8778c20.png)'
- en: 'Here you will see that as the learning progresses, we are sending the output
    to `ReflectInsight` to help us see and learn what the algorithm is doing internally.
    You see that for each iteration, different object positions are being evaluated,
    and so are their actions and rewards:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你会看到随着学习的进行，我们将输出发送到`ReflectInsight`，以帮助我们了解和学习算法内部正在做什么。你会看到对于每一次迭代，都会评估不同的物体位置，以及它们的行为和奖励：
- en: '![](img/e2221e39-43ce-497d-9929-65eb7dcfc453.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2221e39-43ce-497d-9929-65eb7dcfc453.png)'
- en: 'Once the learning is complete, we can click on the Show Solution button to
    replay the final solution. When complete, the black object will sit atop the red
    object:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习完成，我们可以点击“显示答案”按钮来重新播放最终解决方案。当完成后，黑色物体将位于红色物体之上：
- en: '![](img/6a2ea377-0fdb-44ab-8753-94ba7fd68298.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6a2ea377-0fdb-44ab-8753-94ba7fd68298.png)'
- en: 'Now let''s look at the code from our application. There are two methods of
    learning that we highlighted previously. Here''s how Q-learning looks:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们应用程序中的代码。我们之前强调了两种学习方式。以下是Q-learning的看起来：
- en: '[PRE0]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'How does SARSA learning differ? Let''s take a look at the `while` loop of SARSA
    learning and understand:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: SARSA学习有何不同？让我们看看SARSA学习的`while`循环，并理解：
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Our last step is to see how we can animate the solution. This will be needed
    for us to see that our algorithm achieved its goal. Here is the code:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最后一步是看看我们如何可以动画化解决方案。这将帮助我们确认我们的算法达到了目标。以下是代码：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: And here is our `while` loop where all the magic happens!
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的`while`循环，所有的魔法都在这里发生！
- en: '[PRE3]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let's break this down into more digestible sections. The first thing that we
    do is establish our tabu policy. If you are not familiar with tabu searching,
    note that it is designed to enhance the performance of a local search by relaxing
    its rule. At each step, sometimes worsening a move is acceptable if there are
    no alternatives (moves with reward).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个问题分解成更易于消化的部分。我们首先建立tabu策略。如果你不熟悉tabu搜索，请注意，它旨在通过放宽其规则来提高局部搜索的性能。在每一步，如果没有其他选择（具有奖励的动作），有时允许动作变差是可以接受的。
- en: Additionally, prohibitions (tabu) are put in place to ensure that the algorithm
    does not return to the previously visited solution.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了确保算法不会回到之前访问过的解决方案，我们设置了禁止（tabu）规则。
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we have to position our agent and prepare the map.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们必须定位我们的代理并准备地图。
- en: '![](img/c364af85-1701-4fa3-a222-40c17923fc6d.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c364af85-1701-4fa3-a222-40c17923fc6d.png)'
- en: 'Here is our main execution loop, which will show the animated solution:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们的主要执行循环，它将展示动画解决方案：
- en: '[PRE5]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tower of Hanoi
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 汉诺塔
- en: Since we've discussed Q-learning, I want to spend the rest of this chapter highlighting
    some fantastic work done by Kenan Deen. His Tower of Hanoi solution is a great
    example of how you can use reinforcement learning to solve real-world problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经讨论了Q学习，我想在本章的剩余部分突出展示Kenan Deen的一些出色工作。他的汉诺塔解决方案是使用强化学习解决现实世界问题的绝佳例子。
- en: This form of reinforcement learning is more formally known as a **Markov Decision
    Process** (**MDP**). An MDP is a discrete-time stochastic control process, which
    means that at each time step, the process is in state *x*. The decision maker
    may choose any available action for that state, and the process will respond at
    the next time step by randomly moving into a new state and giving the decision
    maker a reward. The probability that the process moves into its new state is determined
    by the chosen action. So, the next state depends on the current state and the
    decision maker's action. Given the state and the action, the next move is completely
    independent of all previous states and actions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这种形式的强化学习更正式地被称为**马尔可夫决策过程**（**MDP**）。MDP是一个离散时间随机控制过程，这意味着在每个时间步，过程处于状态 *x*。决策者可以选择该状态下的任何可用动作，过程将在下一个时间步通过随机移动到新状态并向决策者提供奖励来响应。过程移动到新状态的概率由所选动作决定。因此，下一个状态取决于当前状态和决策者的动作。给定状态和动作，下一步完全独立于所有先前状态和动作。
- en: The Tower of Hanoi consists of three rods and several sequentially sized disks
    in the leftmost rod. The objective is to move all the disks from the leftmost
    rod to the rightmost one **with the fewest possible number of moves**.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 汉诺塔由三根杆和几个按顺序排列的盘子组成，最左边的杆上。目标是使用尽可能少的移动次数将所有盘子从最左边的杆移动到最右边的杆。
- en: 'Two important rules you have to follow are that you can move only one disk
    at a time, and you can''t put a bigger disk on top of a smaller one; that is,
    in any rod, the order of disks must always be from the largest disk at the bottom
    to the smallest disk at the top, depicted as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须遵循的两个重要规则是，你一次只能移动一个盘子，你不能把一个更大的盘子放在一个较小的盘子上面；也就是说，在任何杆上，盘子的顺序必须始终是从底部最大的盘子到顶部最小的盘子，如下所示：
- en: '![](img/4c35f9c3-1da9-4304-bec3-adb147155875.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4c35f9c3-1da9-4304-bec3-adb147155875.png)'
- en: 'Let''s say we are using three disks, as pictured just now. In this scenario,
    there are 3³ possible states, shown as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们正在使用三个盘子，如上图所示。在这种情况下，有3³种可能的状态，如下所示：
- en: '![](img/4c79d9df-58e4-4e23-b5f8-3f99cce61828.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4c79d9df-58e4-4e23-b5f8-3f99cce61828.png)'
- en: The total number of all possible states in a Tower of Hanoi puzzle is 3 raised
    to the number of disks.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 汉诺塔谜题中所有可能状态的总数是盘子的数量3的幂。
- en: '*||S|| = 3^n*'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '*||S|| = 3^n*'
- en: Where ||*S*|| is the number of elements in the set states, and *n* is the number
    of disks.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ||*S*|| 是状态集中的元素数量，*n* 是盘子的数量。
- en: So, in our example, we have *3 x 3 x 3 = 27* unique possible states of the distribution
    of disks over the three rods, including empty rods; but two empty rods can be
    in a state at max.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的例子中，我们有 *3 x 3 x 3 = 27* 种独特的磁盘分布状态，包括空杆；但最多只能有两个空杆处于某种状态。
- en: 'With the total number of states being defined, here are all the possible actions
    our algorithm has available to move from one state to another:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了总状态数之后，这里列出了我们的算法从一种状态移动到另一种状态所具有的所有可能动作：
- en: '![](img/dd82cb57-d7fd-4e8f-8f1f-2214abf7135f.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/dd82cb57-d7fd-4e8f-8f1f-2214abf7135f.png)'
- en: The least possible number of moves for this puzzle is:*LeastPossibleMoves =
    2^n - 1*
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此谜题可能的最少移动次数为：*LeastPossibleMoves = 2^n - 1*
- en: Where *n* is the number of disks.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *n* 是盘子的数量。
- en: 'The Q-learning algorithm can be formally defined as follows:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Q学习算法可以正式定义为以下内容：
- en: '![](img/96bd1e3a-69a6-466b-b761-7b13295edd00.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/96bd1e3a-69a6-466b-b761-7b13295edd00.png)'
- en: 'In this Q-learning algorithm, we have the following variables being used:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个Q学习算法中，我们使用了以下变量：
- en: '**Q matrix**: A 2D array that, at first, is populated with a fixed value for
    all elements (usually 0). It is used to hold the calculated policy over all states;
    that is, for every state, it holds the rewards for the respective possible actions.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Q矩阵**：一个二维数组，最初为所有元素填充一个固定值（通常是0）。它用于存储所有状态的计算策略；也就是说，对于每个状态，它存储相应可能动作的奖励。'
- en: '**R matrix:** A 2D array that holds the initial rewards and allows the program
    to determine the list of possible actions for a specific state.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**R矩阵**：一个二维数组，包含初始奖励并允许程序确定特定状态的可能的动作列表。'
- en: '**Discount factor:** Determines the policy of the agent in how it deals with
    rewards. A discount factor closer to 0 will make the agent greedy by only considering
    current rewards, while a discount factor approaching 1 will make it more strategic
    and farsighted for better rewards in the long run.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**折扣因子**：决定了智能体如何处理奖励的策略。接近0的折扣因子将使智能体变得贪婪，只考虑当前奖励，而接近1的折扣因子将使其更具战略性和远见，以获得长期更好的奖励。'
- en: 'We should briefly highlight some of the methods of our Q-learning class:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该简要概述我们Q学习类的一些方法：
- en: '`Init`: Called for generation of all possible states as well as for the start
    of the learning process.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Init`：用于生成所有可能的状态以及学习过程的开始。'
- en: '`Learn`: Has sequential steps for the learning process.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`学习`：具有学习过程的连续步骤。'
- en: '`InitRMatrix`: This initializes the reward matrix with one of these values:'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InitRMatrix`：使用这些值之一初始化奖励矩阵：'
- en: '`0`: We do **not** have information about the reward when taking this action
    in this state'
  id: totrans-85
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`0`：在当前状态下采取此动作时，我们没有关于奖励的信息'
- en: '`X`: There is no way to take this action in this state'
  id: totrans-86
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X`：在当前状态下无法采取此动作'
- en: '`100`: This is our big reward in the final state, where we want to go'
  id: totrans-87
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`100`：这是我们最终状态的大奖励，我们希望达到这个状态'
- en: '`TrainQMatrix`: Contains the actual iterative value update rule of the Q matrix.
    When completed, we expect to have a trained agent.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`TrainQMatrix`：包含Q矩阵的实际迭代值更新规则。完成后，我们期望有一个训练好的智能体。'
- en: '`NormalizeQMatrix`: This normalizes the values of the Q matrix making them
    percentages.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`NormalizeQMatrix`：将Q矩阵的值标准化，使其成为百分比。'
- en: '`Test`: Provides textual input from the user and displays the optimal shortest
    path to solve the puzzle.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`测试`: 从用户处提供文本输入并显示解决谜题的最佳最短路径。'
- en: 'Let''s look deeper into our `TrainQMatrix` code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地了解我们的`TrainQMatrix`代码：
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Running the application with three disks:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用三个盘子运行应用程序：
- en: '![](img/f64ce769-eca7-4410-a4e6-43b5ea7fdf3d.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f64ce769-eca7-4410-a4e6-43b5ea7fdf3d.png)'
- en: 'Running the application with four disks:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用四个盘子运行应用程序：
- en: '![](img/b105397f-e116-46e2-88ba-cd5b1b64c195.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b105397f-e116-46e2-88ba-cd5b1b64c195.png)'
- en: 'And here''s running with seven disks. The optimal number of moves is 127, so
    you can see how fast the solution can multiply the possible combinations:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是使用七个盘子运行的情况。最佳移动次数为127，您可以看到解决方案如何快速地乘以可能的组合：
- en: '![](img/f284c705-27e8-4e11-b992-c8c3ca150248.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f284c705-27e8-4e11-b992-c8c3ca150248.png)'
- en: Summary
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about reinforcement learning, various types of learning
    algorithms that go with it, and how you can apply it to real-world learning problems.
    In the next chapter, we're going to jump into fuzzy logic and see not only what
    it means, but also how we can apply it to everyday problems.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了强化学习，与之相关的各种学习算法，以及如何将其应用于现实世界的学习问题。在下一章中，我们将跳入模糊逻辑，不仅了解其含义，还将了解如何将其应用于日常问题。
- en: References
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: Wikipedia, Creative Commons ShareAlike License
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维基百科，创意共享署名许可
- en: Watkins, C.J.C.H. (1989), *Learning from Delayed Rewards* (Ph.D. thesis), Cambridge
    University
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Watkins, C.J.C.H. (1989), *延迟奖励学习*（博士论文），剑桥大学
- en: '*Online Q-Learning using Connectionist Systems*, Rummery & Niranjan (1994)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*使用连接主义系统的在线 Q-Learning*，Rummery & Niranjan (1994)'
- en: 'Wiering, Marco; Schmidhuber, Jürgen (1998-10-01), *Fast Online Q(λ)*. *Machine
    Learning*. **33** (1): 105-115'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Wiering, Marco; Schmidhuber, Jürgen (1998-10-01), *快速在线 Q(λ)*. *机器学习*. **33**
    (1): 105-115'
- en: 'Copyright (c) 2009-2017, Accord.NET Authors at: `authors@accord-framework.net`'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版权所有 (c) 2009-2017，Accord.NET 作者，联系邮箱：`authors@accord-framework.net`
- en: Kenan Deen, [https://kenandeen.wordpress.com/](https://kenandeen.wordpress.com/)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kenan Deen, [https://kenandeen.wordpress.com/](https://kenandeen.wordpress.com/)
