- en: Chapter 9. Large-scale Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章 大规模机器学习
- en: In this chapter, we will explore a few methodologies for handling large volumes
    of data to train machine learning models. In the latter section of this chapter,
    we will also demonstrate how to use cloud-based services for machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些处理大量数据以训练机器学习模型的方法。在本章的后半部分，我们还将演示如何使用基于云的服务进行机器学习。
- en: Using MapReduce
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MapReduce
- en: A data-processing methodology that is often encountered in the context of data
    parallelism is **MapReduce**. This technique is inspired by the **map** and **reduce**
    functions from functional programming. Although these functions serve as a basis
    to understand the algorithm, actual implementations of MapReduce focus more on
    scaling and distributing the processing of data. There are currently several active
    implementations of MapReduce, such as Apache Hadoop and Google Bigtable.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据并行性的背景下，经常遇到的一种数据处理方法是**MapReduce**。这种技术灵感来源于函数式编程中的**map**和**reduce**函数。尽管这些函数作为理解算法的基础，但MapReduce的实际实现更侧重于扩展和分布数据处理。目前有几种活跃的MapReduce实现，例如Apache
    Hadoop和Google Bigtable。
- en: A MapReduce engine or program is composed of a function that performs some processing
    over a given record in a potentially large dataset (for more information, refer
    to "Map-Reduce for Machine Learning on Multicore"). This function represents the
    `Map()` step of the algorithm. This function is applied to all the records in
    the dataset and the results are then combined. The latter step of extracting the
    results is termed as the `Reduce()` step of the algorithm. In order to scale this
    process over huge datasets, the input data provided to the `Map()` step is first
    partitioned and then processed on different computing nodes. These nodes may or
    may not be on separate machines, but the processing performed by a given node
    is independent from that of the other nodes in the system.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce引擎或程序由一个函数组成，该函数在可能非常大的数据集的给定记录上执行一些处理（更多信息，请参阅“多核机器学习中的Map-Reduce”）。这个函数代表了算法的`Map()`步骤。这个函数应用于数据集中的所有记录，然后结果被合并。提取结果的后续步骤被称为算法的`Reduce()`步骤。为了在大数据集上扩展此过程，首先将提供给`Map()`步骤的输入数据分区，然后在不同的计算节点上处理。这些节点可能位于不同的机器上，也可能不在，但给定节点执行的处理与其他节点在系统中的处理是独立的。
- en: Some systems follow a different design in which the code or query is sent to
    nodes that contain the data, instead of the other way around. This step of partitioning
    the input data and then forwarding the query or data to different nodes is called
    the `Partition()` step of the algorithm. To summarize, this method of handling
    a large dataset is quite different from traditional methods of iterating over
    the entire data as fast as possible.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一些系统采用不同的设计，其中代码或查询被发送到包含数据的节点，而不是相反。这个步骤，即对输入数据进行分区，然后将查询或数据转发到不同的节点，被称为算法的`Partition()`步骤。总结来说，这种处理大数据集的方法与传统方法大相径庭，传统方法尽可能快地迭代整个数据。
- en: MapReduce scales better than other methods because the partitions of the input
    data can be processed independently on physically different machines and then
    combined later. This gain in scalability is not only because the input is divided
    among several nodes, but because of an intrinsic reduction in complexity. An NP-hard
    problem cannot be solved for a large problem space, but can be solved if the problem
    space is smaller.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce比其他方法具有更好的可扩展性，因为输入数据的分区可以在物理上不同的机器上独立处理，然后稍后合并。这种可扩展性的提升不仅是因为输入被分配到多个节点，还因为复杂性的内在降低。对于大的问题空间，一个NP难问题无法解决，但如果问题空间较小，则可以解决。
- en: For problems with an algorithmic complexity of ![Using MapReduce](img/4351OS_09_01.jpg)
    or ![Using MapReduce](img/4351OS_09_02.jpg), partitioning the problem space will
    actually increase the time needed to solve the given problem. However, if the
    algorithmic complexity is ![Using MapReduce](img/4351OS_09_03.jpg), where ![Using
    MapReduce](img/4351OS_09_04.jpg), partitioning the problem space will reduce the
    time needed to solve the problem. In case of NP-hard problems, ![Using MapReduce](img/4351OS_09_05.jpg).
    Thus, MapReduce decreases the time needed to solve NP-hard problems by partitioning
    the problem space (for more information, refer to *Evaluating MapReduce for Multi-core
    and Multiprocessor Systems*).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有![使用MapReduce](img/4351OS_09_01.jpg)或![使用MapReduce](img/4351OS_09_02.jpg)算法复杂度的问题，实际上将问题空间分区会增加解决给定问题所需的时间。然而，如果算法复杂度是![使用MapReduce](img/4351OS_09_03.jpg)，其中![使用MapReduce](img/4351OS_09_04.jpg)，那么将问题空间分区将减少解决问题所需的时间。在NP-hard问题的情况下，![使用MapReduce](img/4351OS_09_05.jpg)。因此，通过分区问题空间，MapReduce减少了解决NP-hard问题所需的时间（更多信息，请参阅*评估MapReduce在多核和多处理器系统中的应用*）。
- en: 'The MapReduce algorithm can be illustrated using the following diagram:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: MapReduce算法可以用以下图示说明：
- en: '![Using MapReduce](img/4351OS_09_06.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![使用MapReduce](img/4351OS_09_06.jpg)'
- en: In the previous diagram, the input data is first partitioned, and each partition
    is independently processed in the `Map()` step. Finally, the results are combined
    in the `Reduce()` step.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，输入数据首先被分区，然后在`Map()`步骤中独立处理每个分区。最后，在`Reduce()`步骤中将结果合并。
- en: 'We can concisely define the MapReduce algorithm in Clojure pseudo-code, as
    shown in the following code:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用Clojure伪代码简洁地定义MapReduce算法，如下面的代码所示：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The `map-reduce` function defined in the previous code distributes the application
    of the function `f` among several processors (or threads) using the standard `pmap`
    (abbreviation for parallel map) function. The input data, represented by the collection
    `coll`, is first partitioned using the `partition-all` function, and the function
    `f` is then applied to each partition in parallel using the `pmap` function. The
    results of this `Map()` step are then combined using a composition of the standard
    `reduce` and `concat` functions. Note that this is possible in Clojure due to
    the fact the each partition of data is a sequence, and the `pmap` function will
    thus return a sequence of partitions that can be joined or concatenated into a
    single sequence to produce the result of the computation.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中定义的`map-reduce`函数使用标准的`pmap`（并行映射的缩写）函数将函数`f`的应用分配给多个处理器（或线程）。输入数据，由集合`coll`表示，首先使用`partition-all`函数进行分区，然后使用`pmap`函数并行地将函数`f`应用于每个分区。然后使用标准`reduce`和`concat`函数的组合将这个`Map()`步骤的结果合并。请注意，这是由于每个数据分区都是一个序列，因此`pmap`函数将返回一个分区的序列，可以连接或连接成一个单一的序列以产生计算的结果。
- en: Of course, this is only a theoretical explanation of the core of the MapReduce
    algorithm. Actual implementations tend to focus more on distributing the processing
    among several machines, rather than among several processors or threads as shown
    in the `map-reduce` function defined in the previous code.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这只是一个关于MapReduce算法核心的理论解释。实际的实现往往更关注于在多台机器之间分配处理，而不是在之前代码中定义的`map-reduce`函数中的多个处理器或线程之间。
- en: Querying and storing datasets
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查询和存储数据集
- en: When dealing with large datasets, it's useful to be able to query the data based
    on some arbitrary conditions. Also, it's more reliable to store the data in a
    database rather than in a flat file or as an in-memory resource. The Incanter
    library provides us with several useful functions to perform these operations,
    as we will demonstrate in the code example that will follow.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理大量数据集时，能够根据一些任意条件查询数据是非常有用的。此外，将数据存储在数据库中而不是平面文件或内存资源中更为可靠。Incanter库为我们提供了几个有用的函数来执行这些操作，我们将在接下来的代码示例中展示。
- en: Note
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The Incanter library and the MongoDB driver used in the upcoming example can
    be added to a Leiningen project by adding the following dependency to the `project.clj`
    file:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的示例中使用的Incanter库和MongoDB驱动程序可以通过将以下依赖项添加到`project.clj`文件中来添加到Leiningen项目中：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将到来的示例，命名空间声明应类似于以下声明：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Also, this example requires MongoDB to be installed and running.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，此示例需要MongoDB已安装并运行。
- en: 'For this example, we will use the Iris dataset, which can be fetched using
    the `get-dataset` function from the `incanter.datasets` namespace. The code is
    as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们将使用 Iris 数据集，可以使用 `incanter.datasets` 命名空间中的 `get-dataset` 函数获取。代码如下：
- en: '[PRE3]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'As shown in the previous code, we simply bind the Iris dataset to a variable
    `iris`. We can perform various operations on this dataset using the `with-data`
    function. To view the data, we can use the `view` function along with the `with-data`
    function to provide a tabular representation of the dataset, as shown in the following
    code:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前代码所示，我们只需将 Iris 数据集绑定到一个变量 `iris`。我们可以使用 `with-data` 函数对这个数据集执行各种操作。要查看数据，我们可以使用
    `view` 函数结合 `with-data` 函数来提供数据集的表格表示，如下代码所示：
- en: '[PRE4]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The `$data` variable is a special binding that can be used to represent the
    entire dataset within the scope of the `with-data` function. In the previous code,
    we add an extra column to represent the row number of a record to the data using
    a composition of the `conj-cols`, `nrows`, and `range` functions. The data is
    then displayed in a spreadsheet-like table using the `view` function. The previous
    code produces the following table that represents the dataset:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`$data` 变量是一个特殊绑定，可以在 `with-data` 函数的作用域内表示整个数据集。在前面的代码中，我们通过 `conj-cols`、`nrows`
    和 `range` 函数的组合向数据中添加一个额外的列来表示记录的行号。然后使用 `view` 函数以类似电子表格的表格形式显示数据。前面的代码生成了以下表格，表示数据集：'
- en: '![Querying and storing datasets](img/4351OS_09_07.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![查询和存储数据集](img/4351OS_09_07.jpg)'
- en: 'We can also select columns we are interested in from the original dataset using
    the `$` function within the scope of the `with-data` function, as shown in the
    following code:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用 `with-data` 函数作用域内的 `$` 函数从原始数据集中选择我们感兴趣的列，如下代码所示：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The `$` function selects the `:Species` and `:Sepal.Length` columns from the
    `iris` dataset in the code example shown previously. We can also filter the data
    based on a condition using the `$where` function, as shown in the following code:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面所示的代码示例中，`$` 函数从 `iris` 数据集中选择了 `:Species` 和 `:Sepal.Length` 列。我们还可以使用 `$where`
    函数根据条件过滤数据，如下代码所示：
- en: '[PRE6]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The previous example queries the `iris` dataset for records with the `:Sepal.Length`
    column equal to `7.7` using the `$where` function. We can also specify the lower
    or upper bound of the value to compare a column to using the `:$gt` and `:$lt`
    symbols in a map passed to `$where` function, as shown in the following code:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例使用 `$where` 函数查询 `iris` 数据集中 `:Sepal.Length` 列等于 `7.7` 的记录。我们还可以在传递给 `$where`
    函数的映射中使用 `:$gt` 和 `:$lt` 符号来指定值的上下限，如下代码所示：
- en: '[PRE7]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The previous example checks for records that have a `:Sepal.Length` attribute
    with a value greater than `7`. To check whether a column''s value lies within
    a given range, we can specify both the `:$gt` and `:$lt` keys in the map passed
    to the `$where` function, as shown in the following code:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的示例检查具有大于 `7` 的 `:Sepal.Length` 属性值的记录。要检查列的值是否在给定的范围内，我们可以在传递给 `$where` 函数的映射中指定
    `:$gt` 和 `:$lt` 键，如下代码所示：
- en: '[PRE8]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The previous example checks for records that have a `:Sepal.Length` attribute
    within the range of `7.0` and `7.5`. We can also specify a discrete set of values
    using the `$:in` key, such as in the expression `{:$in #{7.2 7.3 7.5}}`. The Incanter
    library also provides several other functions such as `$join` and `$group-by`
    that can be used to express more complex queries.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '前面的示例检查具有 `7.0` 到 `7.5` 范围内的 `:Sepal.Length` 属性值的记录。我们还可以使用 `$:in` 键指定一组离散值，例如在表达式
    `{:$in #{7.2 7.3 7.5}}` 中。Incanter 库还提供了其他几个函数，如 `$join` 和 `$group-by`，可以用来表达更复杂的查询。'
- en: 'The Incanter library provides functions to operate with MongoDB to persist
    and fetch datasets. MongoDB is a nonrelational document database that allows for
    storage of JSON documents with dynamic schemas. To connect to a MongoDB instance,
    we use the `mongo!` function, as shown in the following code:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: Incanter 库提供了操作 MongoDB 的函数，以持久化和检索数据集。MongoDB 是一个非关系型文档数据库，允许存储具有动态模式的 JSON
    文档。要连接到 MongoDB 实例，我们使用 `mongo!` 函数，如下代码所示：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In the previous code, the database name `sampledb` is specified as a keyword
    argument with the key `:db` to the `mongo!` function. We can also specify the
    hostname and port of the instance to connect to using the `:host` and `:post`
    keyword arguments, respectively.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，数据库名称 `sampledb` 作为关键字参数指定给 `mongo!` 函数的 `:db` 键。我们还可以分别使用 `:host` 和
    `:port` 关键字参数指定要连接的实例的主机名和端口号。
- en: 'We can store datasets in the connected MongoDB instance using the `insert-dataset`
    function from the `incanter.mongodb` namespace. Unfortunately, MongoDB does not
    support the use of the dot character (.) as column names, and so we must change
    the names of the columns in the `iris` dataset in order to successfully store
    it using the `insert-dataset` function. Replacing the column names can be done
    using the `col-names` function, as shown in the following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`incanter.mongodb`命名空间中的`insert-dataset`函数将数据集存储在连接的MongoDB实例中。不幸的是，MongoDB不支持使用点字符（.）作为列名，因此我们必须更改`iris`数据集中的列名，才能成功使用`insert-dataset`函数存储它。可以使用`col-names`函数替换列名，如下面的代码所示：
- en: '[PRE10]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The previous code stores the `iris` dataset in the MongoDB instance after replacing
    the dot characters in the column names.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码在替换列名中的点字符后，将`iris`数据集存储在MongoDB实例中。
- en: Note
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the dataset will be stored in a collection named `iris` in the `sampledb`
    database. Also, MongoDB will assign a hash-based ID to each record in the dataset
    that was stored in the database. This column can be referred to using the `:_id`
    keyword.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，数据集将被存储在`sampledb`数据库中名为`iris`的集合中。此外，MongoDB将为存储在数据库中的数据集中的每条记录分配一个基于哈希的ID。此列可以使用`:_id`关键字引用。
- en: To fetch the dataset back from the database, we use the `fetch-dataset` function,
    as shown in the following code. The value returned by this function can be directly
    used by the `with-data` function to query and view the dataset fetched.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要从数据库中检索数据集，我们使用`fetch-dataset`函数，如下面的代码所示。此函数返回的值可以直接由`with-data`函数使用，以查询和查看检索到的数据集。
- en: '[PRE11]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can also inspect the database after storing our dataset, using the `mongo`
    client, as shown in the following code. As we mentioned our database name is `sampledb`,
    we must select this database using the `use` command, as shown in the following
    terminal output:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在存储我们的数据集之后，使用`mongo`客户端检查数据库，如下面的代码所示。正如我们提到的，我们的数据库名为`sampledb`，我们必须使用`use`命令选择此数据库，如下面的终端输出所示：
- en: '[PRE12]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can view all collections in the database using the `show collections` command.
    The queries can be executed using the `find()` function on the appropriate property
    in the variable `db` instance, as shown in the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`show collections`命令查看数据库中的所有集合。查询可以通过在变量`db`实例的适当属性上执行`find()`函数来执行，如下面的代码所示：
- en: '[PRE13]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To conclude, the Incanter library provides us with a sufficient set of tools
    for querying and storing datasets. Also, MongoDB can be easily used to store datasets
    via the Incanter library.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Incanter库为我们提供了足够的工具来查询和存储数据集。此外，MongoDB可以通过Incanter库轻松地用于存储数据集。
- en: Machine learning in the cloud
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云端机器学习
- en: In the modern day of web-based and cloud services, it is also possible to persist
    both datasets and machine learning models to online cloud storage. This is a great
    solution when dealing with enormous amounts of data, since cloud solutions take
    care of both the storage and processing of huge amounts of data.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于网络和云服务的现代，也可以将数据集和机器学习模型持久化到在线云存储中。当处理大量数据时，这是一个很好的解决方案，因为云解决方案负责处理大量数据的存储和处理。
- en: '**BigML** ([http://bigml.com/](http://bigml.com/)) is a cloud provider for
    machine learning resources. BigML internally uses **Classification and Regression
    Trees** (**CARTs**), which are a specialization of decision trees (for more information,
    refer to *Top-down induction of decision trees classifiers-a survey*), as a machine
    learning model.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**BigML** ([http://bigml.com/](http://bigml.com/)) 是一个机器学习资源的云提供商。BigML内部使用**分类和回归树**（**CARTs**），这是决策树的一种特殊化（更多信息，请参阅*Top-down
    induction of decision trees classifiers-a survey*），作为机器学习模型。'
- en: BigML provides developers with a simple REST API that can be used to work with
    the service from any language or platform that can send HTTP requests. The service
    supports several file formats such as **CSV** (**comma-separated values**), Excel
    spreadsheet, and the Weka library's ARFF format, and also supports a variety of
    data compression formats such as TAR and GZIP. This service also takes a white-box
    approach, in the sense that models can be downloaded for local use, apart from
    the use of the models for predictions through the online web interface.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: BigML为开发者提供了一个简单的REST API，可以用于从任何可以发送HTTP请求的语言或平台中与该服务交互。该服务支持多种文件格式，如**CSV**（逗号分隔值）、Excel电子表格和Weka库的ARFF格式，还支持多种数据压缩格式，如TAR和GZIP。此服务还采用白盒方法，即在模型可以下载用于本地使用的同时，还可以通过在线Web界面使用模型进行预测。
- en: There are bindings for BigML in several languages, and we will demonstrate a
    Clojure client library for BigML in this section. Like other cloud services, users
    and developers of BigML must first register for an account. They can then use
    this account and a provided API key to access BigML from a client library. A new
    BigML account provides a few example datasets to experiment with, including the
    Iris dataset that we've frequently encountered in this book.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: BigML在几种语言中都有绑定，我们将在本节中演示BigML的Clojure客户端库。与其他云服务一样，BigML的用户和开发者必须首先注册账户。然后，他们可以使用这个账户和提供的API密钥通过客户端库访问BigML。一个新的BigML账户提供了一些示例数据集以供实验，包括我们在本书中经常遇到的Iris数据集。
- en: The dashboard of a BigML account provides a simple web-based user interface
    for all the resources available to the account.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: BigML账户的仪表板提供了一个简单的基于Web的用户界面，用于账户可用的所有资源。
- en: '![Machine learning in the cloud](img/4351OS_09_08.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![云中的机器学习](img/4351OS_09_08.jpg)'
- en: BigML resources include sources, datasets, models, predictions, and evaluations.
    We will discuss each of these resources in the upcoming code example.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: BigML资源包括来源、数据集、模型、预测和评估。我们将在接下来的代码示例中讨论这些资源中的每一个。
- en: Note
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'The BigML Clojure library can be added to a Leiningen project by adding the
    following dependency to the `project.clj` file:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过在`project.clj`文件中添加以下依赖项将BigML Clojure库添加到Leiningen项目中：
- en: '[PRE14]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'For the upcoming example, the namespace declaration should look similar to
    the following declaration:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于即将到来的示例，命名空间声明应类似于以下声明：
- en: '[PRE15]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Firstly, we will have to provide authentication details for the BigML service.
    This is done using the `make-connection` function from the `bigml.api` namespace.
    We must provide a username, an API key, and a flag indicating whether we are using
    development or production datasets to the `make-connection` function, as shown
    in the following code. Note that this username and API key will be shown on your
    BigML account page.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须为BigML服务提供认证详情。这是通过使用`bigml.api`命名空间中的`make-connection`函数来完成的。我们必须向`make-connection`函数提供用户名、API密钥和一个标志，指示我们是否使用开发或生产数据集，如下面的代码所示。请注意，此用户名和API密钥将显示在您的BigML账户页面上。
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To use the connection `default-connection` defined in the previous code, we
    must use the `with-connection` function. We can avoid repeating the use of the
    `with-connection` function with the `default-connection` variable by use of a
    simple macro, as shown in the following code:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用之前代码中定义的`default-connection`连接，我们必须使用`with-connection`函数。我们可以通过使用一个简单的宏来避免重复使用带有`default-connection`变量的`with-connection`函数，如下面的代码所示：
- en: '[PRE17]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In effect, using `with-default-connection` is as good as using the `with-connection`
    function with the `default-connection` binding, thus helping us avoid repeating
    code.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，使用`with-default-connection`与使用带有`default-connection`绑定的`with-connection`函数一样好，从而帮助我们避免代码重复。
- en: 'BigML has the notion of sources to represent resources that can be converted
    to training data. BigML supports local files, remote files, and inline code resources
    as sources, and also supports multiple data types. To create a resource, we can
    use the `create` function from the `bigml.source` namespace, as shown in the following
    code:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: BigML有一个表示可以转换为训练数据的资源的概念。BigML支持本地文件、远程文件和内联代码资源作为来源，并支持多种数据类型。要创建资源，我们可以使用`bigml.source`命名空间中的`create`函数，如下面的代码所示：
- en: '[PRE18]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In the previous code, we define a source using some inline data. The data is
    actually a set of features of various car models, such as their year of manufacture
    and total weight. The last feature is the mileage or MPG of the car model. By
    convention, BigML sources treat the last column as the output or objective variable
    of the machine learning model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在前代码中，我们使用一些内联数据定义了一个源。实际上，这些数据是一组各种汽车模型的特征，例如它们的制造年份和总重量。最后一个特征是汽车模型的里程或MPG。按照惯例，BigML源将最后一列视为机器学习模型的输出或目标变量。
- en: We must now convert the source to a BigML dataset, which is a structured and
    indexed representation of the raw data from a source. Each feature in the data
    is assigned a unique integer ID in a dataset. This dataset can then be used to
    train a machine learning CART model, which is simply termed as a model in BigML
    jargon. We can create a dataset and a model using the `dataset/create` and `model/create`
    functions, respectively, as shown in the following code. Also, we will have to
    use the `api/get-final` function to finalize a resource that has been sent to
    the BigML cloud service for processing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须将源转换为BigML数据集，这是一个源原始数据的结构化和索引表示。数据集中的每个特征都被分配一个唯一的整数ID。然后，可以使用此数据集来训练一个机器学习CART模型，在BigML术语中简单地称为模型。我们可以使用`dataset/create`和`model/create`函数分别创建数据集和模型，如下代码所示。此外，我们还将不得不使用`api/get-final`函数来最终化已发送到BigML云服务进行处理的资源。
- en: '[PRE19]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'BigML also provides an interactive visualization of a trained CART model. For
    our training data, the following visualization is produced:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: BigML还提供了一个训练好的CART模型的交互式可视化。对于我们的训练数据，以下可视化被生成：
- en: '![Machine learning in the cloud](img/4351OS_09_09.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![云中的机器学习](img/4351OS_09_09.jpg)'
- en: 'We can now use the trained model to predict the value of the output variable.
    Each prediction is stored in the BigML cloud service, and is shown in the **Predictions**
    tab of the dashboard. This is done using the `create` function from the `bigml.prediction`
    namespace, as shown in the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用训练好的模型来预测输出变量的值。每次预测都存储在BigML云服务中，并在仪表板的**预测**选项卡中显示。这是通过`bigml.prediction`命名空间中的`create`函数完成的，如下代码所示：
- en: '[PRE20]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In the previous code, we attempt to predict the MPG (miles per gallon, a measure
    of mileage) of a car model by providing values for the year of manufacture and
    the weight of the car to the `prediction/create` function. The value returned
    by this function is a map, which contains a key `:prediction` among other things,
    that represents the predicted value of the output variable. The value of this
    key is another map that contains column IDs as keys and their predicted values
    as values in the map, as shown in the following code:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前代码中，我们尝试通过向`prediction/create`函数提供制造年份和汽车重量来预测汽车模型的MPG（每加仑英里数，是里程的衡量标准）。该函数返回的值是一个映射，其中包含一个名为`:prediction`的键，它代表输出变量的预测值。此键的值是另一个映射，其中包含列ID作为键和它们的预测值作为映射中的值，如下代码所示：
- en: '[PRE21]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The MPG column, which has the ID `000004`, is predicted to have a value of
    `33` from the trained model, as shown in the previous code. The `prediction/create`
    function creates an online, or remote, prediction, and sends data to the BigML
    service whenever it is called. Alternatively, we can download a function from
    the BigML service that we can use to perform predictions locally using the `prediction/predictor`
    function, as shown in the following code:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前代码所示，MPG列（ID为`000004`）被预测为具有值为33，这是由训练模型预测的。`prediction/create`函数创建了一个在线或远程预测，并在每次调用时将数据发送到BigML服务。或者，我们可以从BigML服务下载一个函数，我们可以使用该函数通过`prediction/predictor`函数在本地执行预测，如下代码所示：
- en: '[PRE22]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can now use the downloaded function, `default-local-predictor`, to perform
    local predictions, as shown in the following REPL output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用下载的函数`default-local-predictor`来执行本地预测，如下面的REPL输出所示：
- en: '[PRE23]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As shown in the previous code, the local prediction function predicts the MPG
    of a car manufactured in `1983` as `22.4`. We can also pass the `:details` keyword
    argument to the `default-local-predictor` function to provide more information
    about the prediction.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如前代码所示，局部预测函数预测1983年制造的汽车的MPG值为22.4。我们还可以传递`:details`关键字参数给`default-local-predictor`函数，以提供更多关于预测的信息。
- en: BigML also allows us to evaluate trained CART models. We will now train a model
    using the Iris dataset and then cross-validate it. The `evaluation/create` function
    from the BigML library will create an evaluation using a trained model and some
    cross-validation data. This function returns a map that contains all cross-validation
    information about the model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: BigML还允许我们评估训练好的CART模型。我们现在将使用Iris数据集训练一个模型，然后进行交叉验证。BigML库中的`evaluation/create`函数将使用训练模型和一些交叉验证数据创建一个评估。此函数返回一个包含模型所有交叉验证信息的映射。
- en: In the previous code snippets, we used the `api/get-final` function in almost
    all stages of training a model. In the following code example, we will attempt
    to avoid repeated use of this function by using a macro. We first define a function
    to apply the `api/get-final` and `with-default-connection` functions to an arbitrary
    function that takes any number of arguments.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们在训练模型的几乎所有阶段都使用了`api/get-final`函数。在以下代码示例中，我们将尝试通过使用宏来避免重复使用此函数。我们首先定义一个函数，将`api/get-final`和`with-default-connection`函数应用于接受任意数量参数的任意函数。
- en: '[PRE24]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Using the `final-with-default-connection` function defined in the previous
    code, we can define a macro that will map it to a list of values, as shown in
    the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用之前代码中定义的`final-with-default-connection`函数，我们可以定义一个宏，它将映射到值列表，如以下代码所示：
- en: '[PRE25]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `get-final->` macro defined in the previous code basically uses the `->>`
    threading macro to pass the value in the `head` argument through the functions
    in the `body` argument. Also, the previous macro interleaves application of the
    `final-with-default-connection` function to finalize the values returned by functions
    in the `body` argument. We can now use the `get-final->` macro to create a source,
    dataset, and model in a single expression, and then evaluate the model using the
    `evaluation/create` function, as shown in the following code:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码中定义的`get-final->`宏基本上使用`->>`线程宏将`head`参数中的值通过`body`参数中的函数传递。此外，之前的宏将`final-with-default-connection`函数的应用与`body`参数中函数返回的值交错。现在我们可以使用`get-final->`宏在单个表达式中创建源、数据集和模型，然后使用`evaluation/create`函数评估模型，如以下代码所示：
- en: '[PRE26]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In the previous code snippet, we use a remote file that contains the Iris sample
    data as a source, and pass it to the `source/create`, `dataset/create`, and `model/create`
    functions in sequence using the `get-final->` macro we previously defined.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们使用包含Iris样本数据的远程文件作为源，并使用我们之前定义的`get-final->`宏按顺序将其传递给`source/create`、`dataset/create`和`model/create`函数。
- en: 'The formulated model is then evaluated using a composition of the `api/get-final`
    and `evaluation/create` functions, and the result is stored in the variable `iris-evaluation`.
    Note that we use the training data itself to cross-validate the model, which doesn''t
    really achieve anything useful. In practice, however, we should use unseen data
    to evaluate a trained machine learning model. Obviously, as we use the training
    data to cross-validate the model, the accuracy of the model is found to be a 100
    percent or 1, as shown in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`api/get-final`和`evaluation/create`函数的组合对构建的模型进行评估，并将结果存储在变量`iris-evaluation`中。请注意，我们使用训练数据本身进行交叉验证模型，这实际上并没有真正达到任何有用的效果。然而，在实践中，我们应该使用未见过的数据来评估训练好的机器学习模型。显然，当我们使用训练数据交叉验证模型时，模型的准确率被发现是100%或1，如以下代码所示：
- en: '[PRE27]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The BigML dashboard will also provide a visualization (as shown in the following
    diagram) of the model formulated from the data in the previous example. This illustration
    depicts the CART decision tree that was formulated from the Iris sample dataset.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: BigML仪表板还将提供从之前示例中的数据构建的模型的可视化（如图所示）。此图展示了从Iris样本数据集构建的CART决策树。
- en: '![Machine learning in the cloud](img/4351OS_09_10.jpg)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![云中的机器学习](img/4351OS_09_10.jpg)'
- en: To conclude, the BigML cloud service provides us with several flexible options
    to estimate CARTs from large datasets in a scalable and platform-independent manner.
    BigML is just one of the many machine learning services available online, and
    the reader is encouraged to explore other cloud service providers of machine learning.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，BigML云服务为我们提供了多种灵活的选项，以可扩展和平台无关的方式从大型数据集中估计CART。BigML只是众多在线机器学习服务中的一种，鼓励读者探索其他机器学习云服务提供商。
- en: Summary
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we explored a few useful techniques to deal with huge amounts
    of sample data. We also described how we can use machine learning models through
    online services such as BigML, as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了处理大量样本数据的一些有用技术。我们还描述了如何通过BigML等在线服务使用机器学习模型，如下所示：
- en: We described MapReduce and how it is used to process large volumes of data using
    parallel and distributed computations
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们描述了MapReduce及其如何通过并行和分布式计算处理大量数据。
- en: We explored how we can query and persist datasets using the Incanter library
    and MongoDB
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们探讨了如何使用Incanter库和MongoDB查询和持久化数据集。
- en: We briefly studied the BigML cloud service provider and how we can use this
    service to formulate and evaluate CARTs from sample data
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们简要研究了BigML云服务提供商以及我们如何利用这项服务从样本数据中制定和评估CARTs。
- en: In conclusion, we described several techniques and tools that can be used to
    implement machine learning systems in this book. Clojure helps us build these
    systems in a simple and scalable manner by leveraging the power of the JVM and
    equally powerful libraries. We also studied how we can evaluate and improve machine
    learning systems. Programmers and architects can use these tools and techniques
    to model and learn from their users' data, as well as build machine learning systems
    that provide users with a better experience.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，我们描述了本书中可以用来实现机器学习系统的几种技术和工具。Clojure通过利用JVM和同等强大的库的力量，以简单和可扩展的方式帮助我们构建这些系统。我们还研究了如何评估和改进机器学习系统。程序员和架构师可以使用这些工具和技术来对用户数据进行建模和学习，以及构建为用户提供更好体验的机器学习系统。
- en: You can explore the academia and research in machine learning through the various
    citations and references that have been used in this book. New academic papers
    and articles on machine learning provide even more insight into the cutting-edge
    of machine learning, and you are encouraged to find and explore them.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过本书中使用的各种引用和参考文献来探索机器学习的学术和研究。新的学术论文和关于机器学习的文章提供了对机器学习前沿的更多见解，并鼓励您去寻找和探索它们。
