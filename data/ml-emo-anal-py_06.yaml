- en: '6'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '6'
- en: Naïve Bayes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: In [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons and
    Vector-Space Models*, we investigated the use of simple lexicon-based classifiers,
    using both a hand-coded sentiment lexicon and extracting a lexicon from a corpus
    of marked-up texts. The results from this investigation were that such models
    can produce reasonable scores, with a variety of tweaks (using a stemmer or changing
    the way that weights are calculated, such as by using TF-IDF scores) that produce
    improvements in some cases but not in others. We will now turn to a range of machine
    learning algorithms to see whether they will lead to better results.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第五章*](B18714_05.xhtml#_idTextAnchor116)《情感词典和向量空间模型》中，我们研究了使用简单的基于词典的分类器，既使用了手工编写的情感词典，也从标注文本语料库中提取词典。这项调查的结果表明，这样的模型可以产生合理的分数，通过一系列调整（使用词干提取器或改变权重计算方式，例如使用TF-IDF分数）在某些情况下可以改善性能，但在其他情况下则不行。现在，我们将转向一系列机器学习算法，看看它们是否能带来更好的结果。
- en: 'For most of the algorithms that we will be looking at, we will use the Python
    scikit-learn (`sklearn`) implementations. A wide range of implementations for
    all these algorithms are available. The `sklearn` versions have two substantial
    advantages: they are freely available with a fairly consistent interface to the
    training and testing data and they can be easily installed and run on a standard
    computer. They also have a significant disadvantage, in that some of them are
    slower than versions that have been designed to run on computers with fast GPUs
    or other highly parallel processors. Fortunately, most of them run reasonably
    quickly on a standard machine, and even the slowest with our largest dataset can
    train a model in about half an hour. So, for the tasks we are investigating here,
    where we are interested in comparing the performance of the various algorithms
    on identical datasets, the advantages outweigh the fact that on very large datasets,
    some of them will take an infeasibly long time.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们将要探讨的大多数算法，我们将使用Python的scikit-learn（`sklearn`）实现。所有这些算法都有广泛的实现。`sklearn`版本有两个显著优势：它们可以免费获得，并且具有相当一致的接口来处理训练和测试数据，并且可以轻松安装在标准计算机上运行。它们也有一个显著的缺点，即其中一些比专为在配备快速GPU或其他高度并行处理器的计算机上运行的版本要慢。幸运的是，大多数算法在标准机器上运行速度相当快，即使是运行速度最慢的，在我们的最大数据集上也能在大约半小时内训练出一个模型。因此，对于我们在这里探讨的任务，即比较各种算法在相同数据集上的性能，优势超过了在非常大的数据集上，其中一些算法将花费不可行的时间的事实。
- en: In this chapter, we will look at the Naïve Bayes algorithm. We will look at
    the effects of the various preprocessing steps we used in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector Space Models* but we will not look at all the tweaks
    and parameter settings that are provided with this package. The various `sklearn`
    packages provide a range of options that can affect either the accuracy or the
    speed of the given algorithm, but we will not generally try out all the options
    – it is very easy to get distracted into playing with the parameters in the hope
    of gaining a few percentage points, but for our goal of looking at ways of carrying
    out sentiment mining, it is more useful to consider how changes to the data can
    affect performance. Once you have chosen your algorithm, then it may be worth
    investigating the effects of changing the parameters, but this book aims to see
    what the algorithms do with tweets that have been annotated with emotion labels,
    not to look at all the minutiae of the algorithms themselves.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨朴素贝叶斯算法。我们将研究在[*第五章*](B18714_05.xhtml#_idTextAnchor116)《情感词典和向量空间模型》中使用的各种预处理步骤的影响，但不会探讨这个包提供的所有调整和参数设置。`sklearn`的各种包提供了一系列选项，这些选项可以影响给定算法的准确度或速度，但通常我们不会尝试所有选项——很容易被参数调整所吸引，希望获得几个百分点的提升，但对于我们的目标，即探讨情感挖掘的方法，考虑数据变化对性能的影响更有用。一旦你选择了算法，那么研究参数变化的影响可能是有价值的，但本书的目的是查看算法对带有情感标签的推文的处理效果，而不是研究算法本身的细节。
- en: We will start this chapter by looking at how to prepare our datasets to match
    the `sklearn` representation. We will then give a brief introduction to the Naïve
    Bayes approach to machine learning, and then
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将本章的起点放在如何准备我们的数据集以匹配`sklearn`表示上。然后，我们将简要介绍朴素贝叶斯机器学习方法，然后
- en: apply the Naïve Bayes implementation from `sklearn.naive_bayes.MultinomialNB`
    to our datasets and consider why the algorithm behaves as it does and what we
    can do to improve its performance on our data. By the end of this chapter, you’ll
    have a clear understanding of the theory behind Naive Bayes and the effectiveness
    of this as a way of assigning emotions to tweets.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 将`sklearn.naive_bayes.MultinomialNB`中的朴素贝叶斯实现应用于我们的数据集，并考虑为什么算法会表现出这样的行为，以及我们可以做些什么来提高它在我们的数据上的性能。在本章结束时，你将清楚地理解朴素贝叶斯背后的理论，以及将其作为为推文分配情感的方式的有效性。
- en: 'In this chapter, we’ll cover the following topics:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Preparing the data for `sklearn`
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为`sklearn`准备数据
- en: Naïve Bayes as a machine learning algorithm
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯作为机器学习算法
- en: Naively applying Bayes' theorem as a classifier
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 惊叹号地应用贝叶斯定理作为分类器
- en: Multi-label and multi-class datasets
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多标签和多类别数据集
- en: Preparing the data for sklearn
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为`sklearn`准备数据
- en: The `sklearn` packages expect training data consisting of a set of data points,
    where each data point is a real-valued vector, and a set of numerical labels representing
    the class to which each data point has been assigned. Our data consists of sets
    of tweets, where each tweet is represented by, among other things, a set of words
    and a set of values such as `[0, 0, 1, 1, 0, 0]`, where each element of the set
    corresponds to a single dimension. So, if the set of emotions in some training
    set were `['anger', 'fear', 'joy', 'love', 'sadness', 'surprise']`, then the `[0,
    0, 1, 1, 0, 0]set` would indicate that the given tweet had been labeled as expressing
    joy and love. We will use the CARER dataset to illustrate how to convert our datasets
    into, as near as possible, the format required by the `sklearn` packages.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn`包期望训练数据由一组数据点组成，其中每个数据点是一个实值向量，以及一组表示每个数据点所属类别的数值标签。我们的数据由推文集合组成，其中每个推文由一组单词和其他值表示，例如`[0,
    0, 1, 1, 0, 0]`，其中集合中的每个元素对应一个单一维度。因此，如果某个训练集中的情感集合是`[''anger'', ''fear'', ''joy'',
    ''love'', ''sadness'', ''surprise'']`，那么`[0, 0, 1, 1, 0, 0]`集合将表示给定的推文被标记为表达喜悦和爱情。我们将使用CARER数据集来说明如何将我们的数据集转换为尽可能符合`sklearn`包要求的格式。'
- en: 'Initially, we will represent a dataset as a DATASET, as defined in [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment* *Lexicons and* *Vector Space
    Models.* To convert a dataset into a form that is suitable for `sklearn`, we have
    to convert the one-hot encoding of the assignment of labels to a tweet into a
    single numerical label and the tokens that represent a tweet into a sparse matrix.
    The first of these is straightforward: we just enumerate the list of values until
    we hit a non-zero case, at which point the index of that case is the required
    value. If there is more than one non-zero column, this encoding will just record
    the first that is found – this will distort the data, but it is inevitable if
    we use a one-hot encoding for data with multiple labels. The only complication
    arises when it is possible for a tweet to have no label assigned to it because
    in that case, we will get to the end of the list without returning a value. If
    `allowZeros` is set to `True`, then we will return a column beyond the actual
    range of possible cases – that is, we will encode the absence of a value as a
    new explicit value:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，我们将数据集表示为在[*第5章*](B18714_05.xhtml#_idTextAnchor116)中定义的DATASET，*情感* *词典和*
    *向量空间模型*。要将数据集转换为适合`sklearn`的形式，我们必须将标签分配给推文的独热编码转换为单个数值标签，并将代表推文的标记转换为稀疏矩阵。前者是直接的：我们只需枚举值列表，直到遇到非零情况，此时该情况的索引就是所需值。如果有多个非零列，这种编码将只记录找到的第一个——这将扭曲数据，但如果我们使用具有多个标签的数据的独热编码，这是不可避免的。唯一复杂的情况是，如果推文可能没有分配标签，因为在这种情况下，我们将到达列表的末尾而没有返回值。如果`allowZeros`设置为`True`，那么我们将返回实际可能情况范围之外的列——也就是说，我们将编码值的缺失作为一个新的显式值：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can use this to help construct the sparse matrix representation of the training
    set, as discussed in the *Vector spaces* section in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector Space Models.* To make a sparse matrix, you must
    collect parallel lists of rows, columns, and data for all cases where the data
    is non-zero. So, what we have to do is go through the tweets one by one (tweet
    number = row number), and then go through the tokens in the tweet; we must look
    the token up in the index (token index = column number), work out what value we
    want to use for that token (either 1 or its `idf`), and add those to `rows`, `columns`,
    and `data`. Once we have these three lists, we can just invoke the constructor
    for sparse matrices. There are several forms of sparse matrices: `csc_matrix`
    makes a representation that is suitable when each row contains only a few entries.
    We must exclude words that occur no more than `wthreshold` times because including
    very rare words makes the matrix less sparse, and hence slows things down, without
    improving the performance of the algorithms:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用它来帮助构建训练集的稀疏矩阵表示，如在第5章的*向量空间*部分所述，[*第5章*](B18714_05.xhtml#_idTextAnchor116)，*情感词典和向量空间模型*。要创建稀疏矩阵，你必须收集所有数据非零情况下的行、列和数据并行列表。因此，我们必须逐个检查推文（推文编号=行编号），然后检查推文中的标记；我们必须在索引中查找标记（标记索引=列编号），确定我们想要为该标记使用的值（要么是1，要么是其`idf`），然后将这些添加到`rows`、`columns`和`data`中。一旦我们有了这三个列表，我们就可以直接调用稀疏矩阵的构造函数。稀疏矩阵有几种形式：`csc_matrix`创建一个当每行只包含少量条目时适合的表示。我们必须排除出现次数不超过`wthreshold`次的单词，因为包含非常罕见的单词会使矩阵变得不那么稀疏，从而减慢速度，而不会提高算法的性能：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Once we can convert the representation of the labels assigned to a tweet into
    a one-hot format and we can convert a set of tweets with Gold Standard labels
    into a sparse matrix, we have all we need for making a classifier. Exactly what
    we do with these structures will depend on the type of classifier and the one-hot
    values for the data points to single class identifiers. All our `sklearn` classifiers
    will be subclasses of a generic class called `SKLEARNCLASSIFIER`: the definition
    of `SKLEARNCLASSIFIER` does not include a constructor. We will only ever make
    instances of subclasses of this class, so it is in some ways like an abstract
    class – it provides some methods that will be shared by several subclasses, such
    as for making Naïve Bayes classifiers, support vector machine classifiers, or
    deep neural network'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们将分配给推文的标签表示转换为独热格式，并且我们可以将一组带有黄金标准标签的推文转换为稀疏矩阵，我们就拥有了构建分类器所需的一切。我们如何使用这些结构将完全取决于分类器的类型和数据点到单类标识符的独热值。我们所有的`sklearn`分类器都将是一个名为`SKLEARNCLASSIFIER`的通用类的子类：`SKLEARNCLASSIFIER`的定义不包括构造函数。我们只会创建这个类的子类的实例，所以在某种程度上它就像一个抽象类——它提供了一些将被多个子类共享的方法，例如用于创建朴素贝叶斯分类器、支持向量机分类器或深度神经网络分类器。
- en: 'classifiers, but we will never actually make a `SKLEARNCLASSIFIER` class. The
    first thing we will need in `SKLEARNCLASSIFIER` is something for reading the training
    data and converting it into a sparse matrix. `readTrainingData` does this by using
    `makeDATASET` from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment
    Lexicons and Vector-Space Models*, and then converting the training data into
    a sparse matrix and the labels associated with the training data into one-hot
    format:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器，但我们永远不会实际创建一个`SKLEARNCLASSIFIER`类。在`SKLEARNCLASSIFIER`中，我们首先需要的是读取训练数据并将其转换为稀疏矩阵的功能。`readTrainingData`通过使用[*第5章*](B18714_05.xhtml#_idTextAnchor116)，*情感词典和向量空间模型*中的`makeDATASET`来实现这一点，然后将训练数据转换为稀疏矩阵，并将与训练数据相关的标签转换为独热格式：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will need a function to apply a classifier to a tweet. The default value
    for this, defined as a method of `SKLEARNCLASSIFIER`, wraps up the `predict` method
    for the underlying `sklearn` class and returns the result in one of several formats,
    depending on what is wanted:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来将分类器应用于推文。这个默认值被定义为`SKLEARNCLASSIFIER`的方法，它封装了底层`sklearn`类的`predict`方法，并返回结果，具体格式取决于需要什么：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: All our classifiers that make use of `sklearn` will be subclasses of this generic
    type. They will all make use of `readTrainingData` – that is, the machinery for
    converting sets of tweets into sparse matrices – and they will all require a version
    of `applyToTweet`. `SKLEARNCLASSIFIER` provides the default versions of these,
    though some of the classifiers may override them. The first classifier that we
    will develop using `SKLEARNCLASSIFIER` as a base class will involve using Bayes’
    theorem as a way of assigning probabilities to events. First, we will look at
    the theory behind Bayes’ theorem and its use for classification before turning
    to the details of how this may be implemented.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所有使用`sklearn`的分类器都将是这个通用类型的子类。它们都将使用`readTrainingData`——即，将一组推文转换为稀疏矩阵的机制——并且它们都将需要`applyToTweet`的某个版本。`SKLEARNCLASSIFIER`提供了这些的默认版本，尽管一些分类器可能会覆盖它们。我们将使用`SKLEARNCLASSIFIER`作为基类开发的第一个分类器将涉及使用贝叶斯定理来分配事件的概率。首先，我们将研究贝叶斯定理背后的理论及其在分类中的应用，然后再转向如何实现这些细节的具体方法。
- en: Naïve Bayes as a machine learning algorithm
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级贝叶斯作为机器学习算法
- en: The key idea behind the Naïve Bayes algorithm is that you can estimate the likelihood
    of some outcome given a set of observations by using **conditional probabilities**
    and linking the individual observations to the outcome. Defining what conditional
    probability is turns out to be surprisingly slippery because the notion of probability
    itself is very slippery. Probabilities are often defined as something similar
    to proportions, but this view becomes difficult to maintain when you are looking
    at unique or unbounded sets, which is usually the case when you want to make use
    of them.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Naïve Bayes算法背后的关键思想是，你可以通过使用**条件概率**并将单个观察结果与结果联系起来，来估计给定一组观察结果的一些结果的似然性。定义什么是条件概率出人意料地困难，因为概率本身的概念非常模糊。概率通常被定义为与比例类似的东西，但当你观察独特或无界的集合时，这种观点就变得难以维持，这通常是你想要使用它们的时候。
- en: Suppose, for instance, that I am trying to work out how likely it is that France
    will win the FIFA 2022 World Cup (this is being written 2 days before the final,
    between France and Argentina, is to be played). In some sense, it is reasonable
    to ask about this probability – if the bookmakers are offering 3 to 1 against
    France and the probability that they will win is 0.75, then I should place a bet
    on that outcome. But this probability *cannot* be defined as *#(times that France
    win the 2022 World Cup)/#(times that France play in the 2022 World Cup final)*.
    Right now, both those numbers are 0, so the probability appears to be 0/0, which
    is undefined. By the time you are reading this, the first of them will be either
    0 or 1 and the second will be 1, so the probability that France has won the World
    Cup will be either 0 or 1\. Bookmakers and gamblers will make estimates of this
    likelihood, but they cannot do so by actually counting the proportion of times
    the outcome of this yet-to-be-played match comes out in France’s favor.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设，例如，我正在试图计算法国赢得2022年FIFA世界杯的可能性（本文是在决赛前两天写的，法国和阿根廷之间的决赛即将进行）。在某种意义上，询问这个概率是合理的——如果博彩公司提供3赔1的赔率，他们赢得的概率是0.75，那么我应该在这个结果上下注。但是，这个概率*不能*定义为*#(法国赢得2022年世界杯的次数)/#(法国参加2022年世界杯决赛的次数)*。现在，这两个数字都是0，所以概率看起来是0/0，这是未定义的。等你读到这篇文章的时候，第一个数字将是0或1，第二个数字将是1，所以法国赢得世界杯的概率将是0或1。博彩公司和赌徒会估计这个可能性，但他们不能通过实际计算尚未进行的比赛结果以法国为胜的比例来做到这一点。
- en: So, we cannot define the likelihood of an outcome for a future one-off event
    in terms of the proportion of times that the event has the given outcome since
    we have not yet observed the outcome, and we cannot sensibly define the likelihood
    of an outcome for a past one-off event this way either, since it is bound to be
    either 0 or 1 once the event has occurred.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不能通过观察事件发生给定结果的比例来定义未来一次性事件的概率，因为我们还没有观察到结果，同样地，我们也不能以这种方式合理地定义过去一次性事件的概率，因为一旦事件发生，它必然是0或1。
- en: But we also cannot define the likelihood of an outcome, for instance, of a series
    of apparently similar events as a proportion. The fact that I have seen it become
    lighter in the morning every day of my life – that is, 25,488 out of 25,488 times
    – does not mean that the likelihood of it getting lighter tomorrow morning is
    1\. Tomorrow morning might be different. The sun may have turned into a black
    hole and have stopped emitting radiation. There may have been an enormous volcanic
    eruption and the sky might be completely blotted out. *Tomorrow may not be the
    same* *as today*.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们也不能将一个结果的可能性，例如一系列看似相似事件的似然性定义为比例。我一生中每天早上都看到它变亮的事实——即，25,488次中有25,488次——并不意味着明天早上它会变亮的可能性是1。明天早上可能会有所不同。太阳可能变成了黑洞并停止了辐射。可能发生了一次巨大的火山爆发，天空可能被完全遮蔽。*明天可能不会和今天一样*。
- en: 'And we also can’t define the likelihood that a member of an unbounded set satisfies
    some property in terms of the proportion of times that members of a finite subset
    of that property satisfy it. Consider the likelihood that a randomly chosen integer
    is prime. If we plot the number of occurrences of a prime number in the first
    few integers, we get a plot similar to the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们也不能用有限子集成员满足该属性的频率来定义一个无界集成员满足该属性的似然性。考虑一个随机选择的整数是质数的可能性。如果我们绘制前几个整数中质数出现的次数，我们得到的图类似于以下：
- en: '![Figure 6.1 – The number of primes in the first N integers](img/B18714_06_01.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1 – 前N个整数中的质数数量](img/B18714_06_01.jpg)'
- en: Figure 6.1 – The number of primes in the first N integers
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 前N个整数中的质数数量
- en: It looks as though the number of primes in the first 10,000 integers goes up
    linearly, with about 10% of numbers being prime. If we look at the first 100,000,000,
    then about 6% are prime. What is the true probability? It cannot be defined as
    the ratio of the number of primes to the number of integers because these two
    are both infinite and ∞/∞ is undefined. It looks as though the proportion declines
    as we look at more cases, so it probably tends to 0, but it isn’t 0\. It turns
    out to be very hard to either define or estimate probabilities involving unbounded
    sets.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，前10,000个整数中的质数数量呈线性增长，大约有10%的数字是质数。如果我们查看前10亿个整数，那么大约有6%是质数。真正的概率是什么？不能将其定义为质数数量与整数数量的比率，因为这两个都是无限的，∞/∞是未定义的。看起来，当我们查看更多案例时，比例会下降，所以它可能趋于0，但它不是0。结果发现，定义或估计涉及无界集的概率非常困难。
- en: We can *estimate* the probability of the first two kinds of events. We can look
    at all football matches between teams that we believe to be similar to the current
    French and Argentinian teams and use the number of times that the team that is
    like the current French one beat the one that is like the current Argentinian
    one. I can look back at all the days of my life and say that if tomorrow is just
    like all the others *in all relevant respects*, then my estimate of the likelihood
    that it will get lighter in the morning is 1\. But these are just estimates and
    they depend on the next event being the same as the previous ones in all relevant
    respects.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以*估计*前两种事件的可能性。我们可以查看所有我们认为与当前法国和阿根廷队相似的球队之间的足球比赛，并使用类似当前法国队的球队击败类似当前阿根廷队的球队次数。我可以回顾我生命中的所有日子，并说如果明天在所有相关方面都像所有其他日子一样，那么我对它明天早上会变轻的可能性估计是1。但这些只是估计，并且它们取决于下一个事件在所有相关方面都与前一个事件相同。
- en: This has been a thorny issue in probability theory and statistics since the
    19th century. Due to this, Thomas Bayes, among other people, defined probability
    as being, essentially, the odds that someone might reasonably assign for an outcome
    (Bayes, T, 1958). Counting previous experience might well be an important part
    of the information that such a person might use in coming up with their reasonable
    assignment, but since it is not possible to know that the next event will be similar
    to past ones in all relevant aspects, it cannot be used as the definition.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这一直是自19世纪以来概率论和统计学中的一个棘手问题。由于这个原因，托马斯·贝叶斯（Thomas Bayes）等人将概率定义为，本质上，某人可能合理地分配给某个结果的几率（Bayes,
    T, 1958）。计算以往的经验可能是一个重要部分，这样的人可能会在提出他们的合理分配时使用这些信息，但由于无法知道下一个事件在所有相关方面都会与过去的事件相似，因此不能将其用作定义。
- en: So, we cannot say what the probability of a given outcome is. What we can do,
    however, is define how such a probability should behave if we had it. If your
    reasonable estimate does not obey these constraints, then you should revise it!
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们无法说出给定结果的概率是什么。然而，我们可以定义如果我们有这样一个概率，它应该如何表现。如果你的合理估计不遵守这些约束，那么你应该修改它！
- en: 'What should a probability distribution be like? Assuming that we have a finite
    set, *{O1, ... On}*, of distinct possible outcomes, any probability distribution
    should satisfy the following constraints:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 概率分布应该是什么样的？假设我们有一个有限集，*{O1, ... On}*，的可能结果，任何概率分布都应该满足以下约束：
- en: '*p(Oi) >= 0* for all outcomes *Oi*'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于所有结果*Oi*，*p(Oi) >= 0*
- en: '*p(O1) + ... +p(On) = 1*'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(O1) + ... +p(On) = 1*'
- en: '*p(Oi or Oj) = p(Oi)+p(Oj)* for *i ≠ j*'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(Oi or Oj) = p(Oi)+p(Oj)* 对于*i ≠ j*'
- en: The first two constraints taken together mean that *p(Oi) <= 1* for all *Oi*,
    and the second and third mean that *p(not(Oi)) = 1-p(Oi)* (since *not(Oi)* is
    *O1* or *O2* or ... or *Oi-1* or *Oi+1* or .. or *On*).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个约束共同意味着对于所有*Oi*，*p(Oi) <= 1*，第二个和第三个意味着*p(not(Oi)) = 1-p(Oi)*（因为*not(Oi)*是*O1*或*O2*或...或*Oi-1*或*Oi+1*或..或*On*）。
- en: These constraints say nothing about the likelihood of *Oi* and *Oj* both occurring.
    Given the initial conditions, this is not possible since *O1*, ... On were specified
    as distinct possible outcomes. The most that we can say about multiple outcomes
    is that if we have two *completely distinct and unconnected* sets of events, each
    with a set of possible outcomes, *O1*, ... *On* and *Q1*, ..., *Qm*, then the
    probability of *Oi* and *Qj* occurring must be *p(Oi) X p(Qj)*. In the same way
    that we could not tell whether the event we are concerned with is like all the
    others in the set in all relevant ways, we cannot tell whether two sets of events
    are indeed unconnected, so, again, this is a constraint on how a probability measure
    should behave rather than a definition.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些约束对*Oi*和*Oj*同时发生的可能性没有任何说明。考虑到初始条件，这是不可能的，因为*O1*，... *On*被指定为不同的可能结果。关于多个结果，我们最多只能说，如果我们有两个*完全不同且不相连*的事件集，每个集都有可能的结果，*O1*，...
    *On*和*Q1*，... *Qm*，那么*Oi*和*Qj*发生的概率必须是*p(Oi) X p(Qj)*。就像我们无法确定我们关心的事件是否在所有相关方面都像集合中的其他事件一样，我们也无法确定两组事件是否真的不相连，因此，这又是一个关于概率度量应该如何表现而不是定义的约束。
- en: 'Given all this, we can define the conditional probability of some event, *A*,
    given that we know that some other event, *B*, has occurred (or indeed that we
    know that *B* will occur):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些，我们可以定义在已知某个事件*B*已经发生（或者实际上我们知道*B*将会发生）的情况下，某个事件*A*的条件概率：
- en: '*p(A | B) = p(A &* *B)/p(B)*'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(A | B) = p(A &* *B)/p(B)*'
- en: How likely is *A* given that we know *B*? Well, it’s how likely that they occur
    together divided by how likely *B* is by itself (so, if they occur together 5%
    of the time and B occurs 95% of the time, then seeing B will not make us much
    more likely to expect *A*, since *A* only occurs 1 in 19 times that *B* does;
    however, if they occur together 5% of the time but *B* itself only occurs 6%,
    then seeing B will be a strong clue that *A* will happen since *A* occurs 5 in
    6 times that *B* does).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 已知我们了解*B*，*A*的可能性有多大？嗯，这就是它们一起发生的可能性除以*B*本身的可能性（所以，如果它们一起发生5%的时间，而*B*本身发生95%的时间，那么看到*B*不会让我们更有可能期望*A*，因为*A*只在*B*发生的19次中出现1次；然而，如果它们一起发生5%的时间，但*B*本身只发生6%，那么看到*B*将是一个强有力的线索，表明*A*将会发生，因为*A*在*B*发生的6次中有5次发生）。
- en: 'This definition leads very straightforwardly to **Bayes’ theorem**:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个定义直接导致**贝叶斯定理**：
- en: '*p(A | B) = p(A & B)/p(B)* definition'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(A | B) = p(A & B)/p(B)* 定义'
- en: '*p(B | A) = p(B & A)/p(A)* definition'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(B | A) = p(B & A)/p(A)* 定义'
- en: '*p(A & B)= p(B &A)* constraint on *A* and *B*'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(A & B)= p(B &A)* 对*A*和*B*的约束'
- en: '*p(B & A) = p(B | A)×p(A)* rearrange (*2*)'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(B & A) = p(B | A)×p(A)* 重新排列(*2*)'
- en: '*p(A | B) = p(B | A)×p(A)/p(B)* substitute (*4*) into (*1*)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p(A | B) = p(B | A)×p(A)/p(B)* 将(*4*)代入(*1*)'
- en: If we have a set of events, *B1*, ... *Bn*, then we can use Bayes’ theorem to
    say that *p(A | B1 & ... Bn) = p(B1 & ...Bn | A)×p(A)/p(B1 & ...& Bn)*. And *if
    the B**i* *are completely unconnected*, we can say that *p(A | B1 & ... Bn) =
    p(B1 | A) ×p(Bn |* *A)×p(A)/(p(B1) ×p(Bn))*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个事件集，*B1*, ... *Bn*，那么我们可以使用贝叶斯定理来说明*p(A | B1 & ... Bn) = p(B1 & ...Bn
    | A)×p(A)/p(B1 & ...& Bn)*。如果*B**i* *是完全不相连的*，我们可以说*p(A | B1 & ... Bn) = p(B1
    | A) ×p(Bn |* *A)×p(A)/(p(B1) ×p(Bn))*。
- en: This can be very convenient. Suppose that *A* is “this tweet is labeled as angry”
    and *B1*, ..., *Bn* are “this tweet contains the word *furious*,” “this tweet
    contains the word *cross*,” ..., “this tweet contains the word *irritated*.” We
    may have never seen a tweet that contains these three words before, so we cannot
    estimate the likelihood of *A* by counting. However, we will have seen tweets
    that contain these words individually, and we can count how many tweets that have
    been labeled as **angry** contain *furious* (or *cross* or *irritated*), how many
    in total have been labeled as **angry**, ignoring what words they contain, and
    how many contain **furious** (or *cross* or *irritated*), ignoring how they are
    labeled. So, we can make sensible estimates of these, and we can then use Bayes’
    theorem to estimate *p(A | B1 & ..* *Bn)*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以非常方便。假设 *A* 是“这条推文被标记为愤怒”，而 *B1*、...、*Bn* 是“这条推文包含单词 *furious*”、“这条推文包含单词
    *cross*”、“...”、“这条推文包含单词 *irritated*”。我们可能从未见过包含这三个单词的推文，因此我们无法通过计数来估计 *A* 的似然。然而，我们将看到包含这些单词的推文，我们可以计算被标记为
    **angry** 的推文中包含 *furious*（或 *cross* 或 *irritated*）的数量，以及总共被标记为 **angry** 的数量，忽略它们包含的单词，以及包含
    **furious**（或 *cross* 或 *irritated*）的数量，忽略它们的标签。因此，我们可以对这些做出合理的估计，然后我们可以使用贝叶斯定理来估计
    *p(A | B1 & ..* *Bn)*。
- en: 'This way of applying Bayes’ theorem assumes that the events, *B1*, ... *Bn*,
    are completely unconnected. This is rarely true: a tweet that contains the word
    *cross* is much more likely to also contain *irritated* than one that doesn’t.
    So, while we can indeed *naively* misuse Bayes’ theorem in this way to get usable
    estimates of some outcome given a set of observations, we should never lose sight
    of the fact that these estimates are intrinsically unreliable. In the next section,
    we’ll look at how to implement this kind of naïve application of Bayes’ theorem
    as a classifier and investigate how well it works with our various datasets. The
    key to the success of this approach is that while the estimates of the likelihood
    of some outcome are not reliable, the ranking of different outcomes is often sensible
    – if the estimates of the probability that some tweet is **angry** or **sad**
    are 0.6 and 0.3, respectively, then it is indeed more likely to be **angry** than
    **sad**, even if the actual numbers cannot be relied on.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种应用贝叶斯定理的方法假设事件 *B1*、...、*Bn* 完全不相关。这很少是真实的：包含单词 *cross* 的推文更有可能也包含 *irritated*，而不是不包含。因此，虽然我们可以确实地以这种方式
    *天真地* 错误地使用贝叶斯定理来获得给定一组观察结果的一些结果的可用估计，但我们永远不应该忽视这些估计本质上是不可靠的这一事实。在下一节中，我们将探讨如何实现这种天真地应用贝叶斯定理作为分类器的方法，并研究它在我们的各种数据集上的表现如何。这种方法成功的关键在于，尽管某些结果的似然估计并不可靠，但不同结果的排名通常是有意义的——如果某些推文是
    **angry** 或 **sad** 的概率估计分别是 0.6 和 0.3，那么它确实更有可能是 **angry** 而不是 **sad**，即使实际的数字不能被信赖。
- en: Naively applying Bayes’ theorem as a classifier
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Naively applying Bayes’ theorem as a classifier
- en: '`sklearn.naive_bayes.MultinomialNB` does these sums for us (they are not very
    difficult sums, but it is handy to have a package that does them very fast). Given
    this, the class of `NBCLASSIFIER` is very simple to define:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`sklearn.naive_bayes.MultinomialNB` 为我们计算这些总和（这些总和并不困难，但有一个快速计算它们的包是非常方便的）。鉴于这一点，`NBCLASSIFIER`
    类的定义非常简单：'
- en: '[PRE4]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'That’s all we need to make a Naïve Bayes classifier: make `SKLEARNCLASSIFIER`
    using `sklearn.naive_bayes.MultinomialNB.`'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是制作朴素贝叶斯分类器所需的所有内容：使用 `sklearn.naive_bayes.MultinomialNB` 创建 `SKLEARNCLASSIFIER`。
- en: 'How well does this work? We will try this out on our datasets, using stemming
    for the non-English datasets but not for the English ones (we will do this from
    here on since those seemed to be generally the right choices in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)
    *, Sentiment Lexicons and Vector* *Space Models*):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这效果如何？我们将尝试在我们的数据集上使用这种方法，对于非英语数据集使用词干提取，但英语数据集则不使用（从现在开始我们将这样做，因为这似乎是 [*第5章*](B18714_05.xhtml#_idTextAnchor116)
    中提到的正确选择，*情感词典和向量空间模型*）：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM4-EN | 0.873 | 0.873 | 0.873 | 0.873 | 0.775 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.873 | 0.873 | 0.873 | 0.873 | 0.775 |'
- en: '| SEM11-EN | 0.625 | 0.262 | 0.369 | 0.373 | 0.227 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.625 | 0.262 | 0.369 | 0.373 | 0.227 |'
- en: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
- en: '| CARER-EN | 0.874 | 0.874 | 0.874 | 0.874 | 0.776 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.874 | 0.874 | 0.874 | 0.874 | 0.776 |'
- en: '| IMDB-EN | 0.849 | 0.849 | 0.849 | 0.849 | 0.738 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.849 | 0.849 | 0.849 | 0.849 | 0.738 |'
- en: '| SEM4-AR | 0.694 | 0.694 | 0.694 | 0.694 | 0.531 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.694 | 0.694 | 0.694 | 0.694 | 0.531 |'
- en: '| SEM11-AR | 0.628 | 0.274 | 0.381 | 0.393 | 0.236 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.628 | 0.274 | 0.381 | 0.393 | 0.236 |'
- en: '| KWT.M-AR | 0.667 | 0.655 | 0.661 | 0.664 | 0.494 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.667 | 0.655 | 0.661 | 0.664 | 0.494 |'
- en: '| SEM4-ES | 0.525 | 0.535 | 0.530 | 0.462 | 0.360 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | 0.525 | 0.535 | 0.530 | 0.462 | 0.360 |'
- en: '| SEM11-ES | 0.508 | 0.296 | 0.374 | 0.380 | 0.230 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.508 | 0.296 | 0.374 | 0.380 | 0.230 |'
- en: Figure 6.2 – Naïve Bayes, one emotion per tweet
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 – 朴素贝叶斯，每条推文一个情绪
- en: The first thing to note is that both making and applying a Naïve Bayes classifier
    is very quick – 10K tweets can be classified per second, and even training on
    a dataset containing 400K tweets takes just under 10 seconds. But, as before,
    what matters is whether the classifier is any good at the task we want it to carry
    out. The preceding table shows that for most of the English datasets, the scores
    are better than the scores in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116),
    *Sentiment Lexicons and Vector Space Models* with the improvement for the CARER
    dataset being particularly marked and the score for SEM11-EN being substantially
    worse than in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons
    and Vector* *Space Models*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，构建和应用朴素贝叶斯分类器都非常快——每秒可以分类10K条推文，甚至在包含40K条推文的训练集上训练也只需不到10秒。但是，正如之前所说，重要的是分类器是否擅长我们希望它执行的任务。前表显示，对于大多数英语数据集，分数优于[第5章](B18714_05.xhtml#_idTextAnchor116)中*情感词典和向量空间模型*的分数，特别是CARER数据集的改进尤为明显，而SEM11-EN的分数在[第5章](B18714_05.xhtml#_idTextAnchor116)*情感词典和向量空间模型*中明显较低。
- en: 'Recall the main differences between CARER and the others: CARER is much bigger
    than the others, and, in contrast to SEM11, every tweet has exactly one label
    associated with it. To see whether the issue is the size of the training set,
    we will plot the accuracy for this dataset against an increasing training size:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾CARER与其他数据集的主要区别：CARER比其他数据集大得多，并且与SEM11相反，每条推文都与一个标签精确关联。为了查看问题是否与训练集的大小有关，我们将绘制该数据集的准确率与不断增加的训练大小之间的关系：
- en: '![](img/B18714_06_03.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18714_06_03.jpg)'
- en: Figure 6.3 – Jaccard score against training size, Naïve Bayes, with the CARER
    dataset
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – Jaccard分数与训练大小对比，朴素贝叶斯，使用CARER数据集
- en: 'The Jaccard score increases steadily from quite a low base, and while it is
    beginning to flatten out as we get to around 400K training tweets, it is clear
    that Naïve Bayes does require quite a lot of data. This is likely to be at least
    part of the reason why it is less effective for the other datasets: they simply
    do not contain enough data.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard分数从相当低的基数稳步上升，尽管当我们达到大约40K条训练推文时它开始趋于平坦，但很明显朴素贝叶斯确实需要相当多的数据。这可能是它对其他数据集效果较差的部分原因：它们简单地没有足够的数据。
- en: 'It is worth looking in some detail at the inner workings of this algorithm.
    Just like the lexicon-based classifiers, Naïve Bayes constructs a lexicon where
    each word has associated scores for the various emotions:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 值得详细研究一下这个算法的内部工作原理。就像基于词典的分类器一样，朴素贝叶斯构建了一个词典，其中每个词都与各种情绪相关联的分数：
- en: '|  | **anger** | **fear** | **joy** | **love** | **sadness** | **surprise**
    |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  | **愤怒** | **恐惧** | **喜悦** | **爱情** | **悲伤** | **惊讶** |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| a | 0.0187 | 0.0194 | 0.0203 | 0.0201 | 0.0190 | 0.0172 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| a | 0.0187 | 0.0194 | 0.0203 | 0.0201 | 0.0190 | 0.0172 |'
- en: '| and | 0.0291 | 0.0284 | 0.0311 | 0.0286 | 0.0308 | 0.0247 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| and | 0.0291 | 0.0284 | 0.0311 | 0.0286 | 0.0308 | 0.0247 |'
- en: '| the | 0.0241 | 0.0238 | 0.0284 | 0.0275 | 0.0245 | 0.0230 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| the | 0.0241 | 0.0238 | 0.0284 | 0.0275 | 0.0245 | 0.0230 |'
- en: '| angry | 0.0020 | 0.0003 | 0.0001 | 0.0001 | 0.0003 | 0.0001 |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| angry | 0.0020 | 0.0003 | 0.0001 | 0.0001 | 0.0003 | 0.0001 |'
- en: '| happy | 0.0005 | 0.0003 | 0.0014 | 0.0004 | 0.0005 | 0.0004 |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| happy | 0.0005 | 0.0003 | 0.0014 | 0.0004 | 0.0005 | 0.0004 |'
- en: '| hate | 0.0007 | 0.0005 | 0.0002 | 0.0003 | 0.0007 | 0.0002 |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| hate | 0.0007 | 0.0005 | 0.0002 | 0.0003 | 0.0007 | 0.0002 |'
- en: '| irritated | 0.0013 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| irritated | 0.0013 | 0.0000 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |'
- en: '| joy | 0.0001 | 0.0001 | 0.0002 | 0.0002 | 0.0001 | 0.0001 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| joy | 0.0001 | 0.0001 | 0.0002 | 0.0002 | 0.0001 | 0.0001 |'
- en: '| love | 0.0009 | 0.0009 | 0.0019 | 0.0030 | 0.0011 | 0.0011 |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| love | 0.0009 | 0.0009 | 0.0019 | 0.0030 | 0.0011 | 0.0011 |'
- en: '| sad | 0.0005 | 0.0003 | 0.0002 | 0.0002 | 0.0010 | 0.0003 |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| sad | 0.0005 | 0.0003 | 0.0002 | 0.0002 | 0.0010 | 0.0003 |'
- en: '| scared | 0.0001 | 0.0019 | 0.0001 | 0.0001 | 0.0002 | 0.0001 |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| scared | 0.0001 | 0.0019 | 0.0001 | 0.0001 | 0.0002 | 0.0001 |'
- en: '| terrified | 0.0000 | 0.0014 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| terrified | 0.0000 | 0.0014 | 0.0000 | 0.0000 | 0.0000 | 0.0000 |'
- en: Figure 6.4 – Scores for individual words, Naïve Bayes, with the CARER dataset
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 单个单词、朴素贝叶斯与CARER数据集的得分
- en: 'As with the lexicon-based models, the scores for *a*, *and*, and *the* are
    quite high, reflecting the fact that these words occur in most tweets, and hence
    the conditional probability that they will occur in tweets that express the various
    emotions is also quite high. These words will get largely canceled out when we
    divide the contributions that they make by their overall frequencies. The others
    all have very small scores, but by and large, they do match the expected emotions
    – *angry* and *irritated* are most strongly linked to **anger**, *joy* is (just
    about) most strongly linked to **joy**, and so on. The differences in the levels
    of association to different emotions are much less marked than was the case for
    the simple lexicon-based algorithms, so the improved performance must be caused
    by the improvement in the way that Bayes’ theorem combines scores. It is clear
    that these words are not independently distributed: the proportion of tweets in
    the CARER dataset that contain *angry* and *irritated* and both *angry* and *irritated*
    are 0.008, 0.003, and 0.0001, respectively. If we take these as estimates of the
    respective probabilities, we will find that p(*angry* + *irritated*)/p(*angry*)
    X p(*irritated*) = 3.6, where it should be 1 if these words were distributed independently.
    This is hardly surprising – you are much more likely to use two words that express
    the same emotion in a single tweet than you are to use ones that express different
    emotions or that have nothing to do with each other. Nonetheless, Bayes’ theorem
    is robust enough to give us useful results even when the conditions for applying
    it soundly do not apply, so long as we have enough data.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于词典的模型一样，*a*、*and*和*the*的得分相当高，反映了这些词在大多数推文中出现的事实，因此它们在表达各种情感的推文中出现的条件概率也相当高。当我们通过它们的总体频率来划分它们所做出的贡献时，这些词将被大量抵消。其他所有词的得分都非常小，但总体上，它们与预期的情感相匹配
    – *angry*和*irritated*与**愤怒**联系最为紧密，*joy*（几乎）与**快乐**联系最为紧密，依此类推。与不同情感关联程度的差异远不如简单的基于词典的算法明显，因此改进的性能必须归因于贝叶斯定理结合得分的方式的改进。很明显，这些词不是独立分布的：在CARER数据集中包含*angry*和*irritated*以及两者都包含的推文比例分别为0.008、0.003和0.0001。如果我们把这些作为相应概率的估计，我们会发现
    p(*angry* + *irritated*)/p(*angry*) X p(*irritated*) = 3.6，如果这些词是独立分布的，那么这个值应该是1。这并不令人惊讶
    – 你在单个推文中使用表达相同情感的词比使用表达不同情感或彼此无关的词的可能性要大得多。尽管如此，贝叶斯定理足够稳健，即使在应用它的条件不完全适用的情况下，只要我们有足够的数据，它也能给出有用的结果。
- en: Multi-label datasets
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多标签数据集
- en: 'The key difference between SEM11 and the other datasets is that tweets in the
    SEM11 sets can be assigned any number of emotions – they are multi-label datasets,
    as defined in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116), *Sentiment Lexicons
    and Vector-Space Models*. The actual distributions are as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: SEM11与其他数据集的关键区别在于SEM11集中的推文可以被分配任意数量的情感 – 它们是多标签数据集，如[*第5章*](B18714_05.xhtml#_idTextAnchor116)中定义的，*情感词典和向量空间模型*。实际的分布如下：
- en: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8**
    | **9** | **10** |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | **0** | **1** | **2** | **3** | **4** | **5** | **6** | **7** | **8**
    | **9** | **10** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| SEM11-EN | 205 | 997 | 2827 | 2151 | 662 | 100 | 11 | 0 | 0 | 0 | 0 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 205 | 997 | 2827 | 2151 | 662 | 100 | 11 | 0 | 0 | 0 | 0 | 0 |'
- en: '| SEM11-AR | 17 | 544 | 1005 | 769 | 210 | 33 | 3 | 0 | 0 | 0 | 0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 17 | 544 | 1005 | 769 | 210 | 33 | 3 | 0 | 0 | 0 | 0 | 0 |'
- en: '| SEM11-ES | 179 | 1499 | 1605 | 479 | 52 | 1 | 1 | 0 | 0 | 0 | 0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 179 | 1499 | 1605 | 479 | 52 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: Figure 6.5 – Number of tweets with 0, 1, 2, ... emotion labels for each SEM11
    dataset
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – 每个SEM11数据集中带有0，1，2，...情感标签的推文数量
- en: In each case, most tweets have two or more labels. This makes it all but impossible
    for any algorithm that assigns exactly one label to each tweet to score highly
    – there has to be a false positive for every tweet that has zero labels, and there
    have to be K-1 false negatives for every tweet that has K labels (since, at most,
    one of these, K, has been picked, and hence K-1 was not). Suppose we have *N*
    tweets, where *Z* has no labels, *O* has exactly one label, and *M* has more than
    one label. So, even if we assume that our classifier gets one of the labels right
    whenever a tweet has at least one label, the best Jaccard score that can be obtained
    is *(O+M)/(O+2*M+Z)* – there will be *O+M* true positives (all the cases that
    ought to be assigned one label, plus all the cases that ought to have more than
    one, by the assumption), at least *Z* false positives (one for each tweet that
    should have no labels), and at least *M* false negatives.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，大多数推文都有两个或更多的标签。这使得任何为每条推文分配一个标签的算法很难获得高分——对于每个没有标签的推文，必须有假阳性，对于每个有K个标签的推文，必须有K-1个假阴性（因为，最多只有一个，K，被选中，因此K-1没有被选中）。假设我们有*N*条推文，其中*Z*没有标签，*O*恰好有一个标签，而*M*有多个标签。所以，即使我们假设我们的分类器在推文至少有一个标签时总是正确地得到一个标签，所能获得的最佳Jaccard分数是*(O+M)/(O+2*M+Z)*——将有*O+M*个真阳性（所有应该分配一个标签的情况，加上根据假设应该有多个标签的情况），至少*Z*个假阳性（每个应该没有标签的推文一个），至少*M*个假阴性。
- en: Thus, the best Jaccard score that can be obtained by an algorithm that assigns
    exactly one label per tweet for the SEM11-EN dataset is 0.41 (if every label that
    was assigned to any of the tweets that have one or more labels in their Gold Standards
    set was correct, then we would have 6,748 true positives, 205 false positives,
    and 9,570 false negatives). If that is the maximum possible Jaccard score for
    an algorithm, then the scores of around 0.2 that we obtained previously are not
    too bad.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于SEM11-EN数据集，通过算法为每条推文分配一个标签所能获得的最佳Jaccard分数是0.41（如果分配给任何具有一个或多个标签的推文的标签都是正确的，那么我们将有6,748个真阳性，205个假阳性，和9,570个假阴性）。如果这是算法可能达到的最大Jaccard分数，那么我们之前获得的约0.2的分数并不算太差。
- en: But they are not as good as the scores we got for these datasets in [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*.
    We need to somehow make Naïve Bayes return multiple labels.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 但它们并不如我们在[*第五章*](B18714_05.xhtml#_idTextAnchor116)“情感词典和向量空间模型”中为这些数据集获得的分数好。我们需要
    somehow 让朴素贝叶斯返回多个标签。
- en: 'This turns out to be fairly straightforward. We can use Bayes’ theorem to provide
    an estimate of the probability of each possible outcome. `sklearn.naive_bayes.MultinomialNB`
    usually picks the outcome with the highest probability, but it has a method, `predict_log_proba`,
    that returns the log of the probabilities for each possible outcome (it is often
    convenient to use the log of the probabilities since working with logs allows
    us to replace multiplications with additions, which are significantly faster).
    We can use this to pick, for instance, every outcome whose probability exceeds
    some threshold, or to pick the best two rather than just the best one. We will
    look at these two options in turn. For the first, we will use the same constructor
    as for `NBCLASSIFIER`, and we will just change `applyToTweet` so that it uses
    `predict_log_proba` rather than `predict`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上相当简单。我们可以使用贝叶斯定理来估计每个可能结果的概率。`sklearn.naive_bayes.MultinomialNB`通常选择概率最高的结果，但它有一个方法，`predict_log_proba`，它返回每个可能结果的概率的对数（由于使用对数可以替换乘法为加法，这通常很方便，因为加法运算比乘法运算快得多）。我们可以使用这个来选择，例如，每个概率超过某个阈值的输出，或者选择最好的两个而不是仅仅最好的一个。我们将依次查看这两个选项。对于第一个，我们将使用与`NBCLASSIFIER`相同的构造函数，我们只需将`applyToTweet`更改为使用`predict_log_proba`而不是`predict`：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The following table is just a copy of the earlier table for Naïve Bayes that
    deals with the multi-label cases for ease of comparison:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下表只是为了方便比较而复制了之前用于处理朴素贝叶斯多标签情况的表格：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确度** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM11-EN | 0.625 | 0.262 | 0.369 | 0.373 | 0.227 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.625 | 0.262 | 0.369 | 0.373 | 0.227 |'
- en: '| SEM11-AR | 0.628 | 0.274 | 0.381 | 0.393 | 0.236 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.628 | 0.274 | 0.381 | 0.393 | 0.236 |'
- en: '| KWT.M-AR | 0.667 | 0.655 | 0.661 | 0.664 | 0.494 |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.667 | 0.655 | 0.661 | 0.664 | 0.494 |'
- en: '| SEM11-ES | 0.508 | 0.296 | 0.374 | 0.380 | 0.230 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.508 | 0.296 | 0.374 | 0.380 | 0.230 |'
- en: Figure 6.6 – Naïve Bayes, one emotion per tweet, multi-class cases
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – 朴素贝叶斯，每条推文一个情感，多类情况
- en: 'The following table shows what happens when we allow the classifier to assign
    more than one emotion to a tweet:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了当我们允许分类器为推文分配多个情感时会发生什么：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确率** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM11-EN | 0.515 | 0.356 | 0.421 | 0.424 | 0.267 |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.515 | 0.356 | 0.421 | 0.424 | 0.267 |'
- en: '| SEM11-AR | 0.494 | 0.381 | 0.430 | 0.444 | 0.274 |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.494 | 0.381 | 0.430 | 0.444 | 0.274 |'
- en: '| KWT.M-AR | 0.645 | 0.704 | 0.673 | 0.677 | 0.507 |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.645 | 0.704 | 0.673 | 0.677 | 0.507 |'
- en: '| SEM11-ES | 0.419 | 0.394 | 0.406 | 0.415 | 0.255 |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.419 | 0.394 | 0.406 | 0.415 | 0.255 |'
- en: Figure 6.7 – Naïve Bayes, multiple outcomes with an optimal threshold, SEM11
    datasets
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – 朴素贝叶斯，具有最佳阈值的多个结果，SEM11数据集
- en: In each case, we have improved the recall considerably (because we are now allowing
    more than one label per tweet to be picked), at the cost of worsening precision.
    The Jaccard scores have increased slightly, but not to the point where they are
    better than the scores obtained in [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector* *Space Models*.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在每种情况下，我们显著提高了召回率（因为我们现在允许每条推文选择多个标签），但代价是精确率下降。Jaccard分数略有上升，但并未达到比第[*第五章*](B18714_05.xhtml#_idTextAnchor116)中获得的分数更好的程度，即*情感词典和向量空间模型*。
- en: We can also simply demand to have two labels per tweet. Again, this will improve
    the recall since we have two labels for all the cases that should have two labels,
    two for all the cases that should have three, and two for all the cases that should
    have four – that is, we will potentially decrease the number of false
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以简单地要求每条推文有两个标签。同样，这将提高召回率，因为我们为所有应该有两个标签的情况都提供了两个标签，为所有应该有三个标签的情况提供了两个，为所有应该有四个标签的情况提供了两个——也就是说，我们可能会减少错误标签的数量
- en: 'negatives. We will also inevitably increase the number of false positives since
    we will have two where we should have either none or one. This is an extremely
    simplistic algorithm since it pays no attention to when we should allow two labels
    – we just assume that this is the right thing to do in every case:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 负面标签的数量也会不可避免地增加，因为我们会在本应没有或只有一个标签的地方有两个标签。这是一个极其简单的算法，因为它没有注意到何时应该允许两个标签——我们只是假设在每种情况下这都是正确的做法：
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This produces a further slight improvement for the SEM11 cases, but still not
    enough to improve over the [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment
    Lexicons and Vector Space Models* results, and is disastrous for KWT.M-AR, where
    there are a small number of cases with multiple assignments and a large number
    with no assignments at all – forcing the classifier to choose two assignments
    when there should be none will have a major effect on the precision!
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这进一步略微提高了SEM11案例的精确率，但仍然不足以超过第[*第五章*](B18714_05.xhtml#_idTextAnchor116)中*情感词典和向量空间模型*的结果，对KWT.M-AR来说更是灾难性的，其中有一些案例有多个分配，而大多数案例则完全没有分配——迫使分类器在应该没有标签的情况下选择两个标签将对精确率产生重大影响！
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确率** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM11-EN | 0.477 | 0.404 | 0.437 | 0.429 | 0.280 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.477 | 0.404 | 0.437 | 0.429 | 0.280 |'
- en: '| SEM11-AR | 0.474 | 0.413 | 0.441 | 0.440 | 0.283 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.474 | 0.413 | 0.441 | 0.440 | 0.283 |'
- en: '| KWT.M-AR | 0.461 | 0.906 | 0.611 | 0.612 | 0.440 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | 0.461 | 0.906 | 0.611 | 0.612 | 0.440 |'
- en: '| SEM11-ES | 0.370 | 0.431 | 0.398 | 0.395 | 0.249 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.370 | 0.431 | 0.398 | 0.395 | 0.249 |'
- en: Figure 6.8 – Naïve Bayes, best two outcomes, multi-label datasets
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – 朴素贝叶斯，最佳两个结果，多标签数据集
- en: So, we have two very simple ways of turning Naïve Bayes into a classifier with
    multiple (or zero) outcomes. In both cases, the improvement over the standard
    version is minor but useful. And in both cases, it requires us to know something
    about the training set – the first requires us to choose a threshold to compare
    the individual scores with, and the second requires us to know the distribution
    of the number of outcomes per tweet. This means that, in both cases, we have to
    use the training data for two things – to find the conditional probabilities,
    as in the standard case, and then to pick the best possible threshold or to look
    at the distribution of the number of outcomes; for this, we have to split the
    training data into two parts, a training section to find the basic probabilities
    and then a **development** section to find the extra information. This is common
    in situations where you have to tune a basic model. No rule says that you *must*
    keep the training and development sections distinct like you must keep the training
    and test sets distinct, but pragmatically, it turns out that doing so usually
    produces better results than using the training set as the development set.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们有两种非常简单的方法将朴素贝叶斯转换为具有多个（或零个）结果的分类器。在两种情况下，与标准版本相比的改进都是微小的但很有用。在两种情况下，都需要我们了解一些关于训练集的信息——第一种需要我们选择一个阈值来比较单个分数，第二种需要我们知道每条推文的输出数量分布。这意味着在这两种情况下，我们都要使用训练数据做两件事——像标准情况一样找到条件概率，然后选择最佳可能的阈值或查看输出数量的分布；为此，我们必须将训练数据分成两部分，一个用于找到基本概率的训练部分，然后是一个**开发**部分用于找到额外信息。这在需要调整基本模型的情况下很常见。没有规则说我们必须像必须保持训练集和测试集分离一样保持训练集和开发集分离，但实践中，这样做通常会产生比使用训练集作为开发集更好的结果。
- en: The scores for the multi-label datasets are still, however, worse than in [*Chapter
    5*](B18714_05.xhtml#_idTextAnchor116)*, Sentiment Lexicons and Vector Space Models*.
    We can try combinations of these two strategies, such as by demanding the best
    two outcomes so long as they both satisfy some
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多标签数据集的分数仍然比在[*第5章*](B18714_05.xhtml#_idTextAnchor116)“情感词典和向量空间模型”中的分数要差。我们可以尝试这两种策略的组合，例如，只要两者都满足某些条件，就可以要求最好的两个结果。
- en: threshold, but no amount of fiddling around is going to transform Naïve Bayes
    into a good classifier for multi-label problems. We will return to this issue
    in [*Chapter* *10*](B18714_10.xhtml#_idTextAnchor193), *Multiclassifiers*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 阈值，但无论如何调整都不会将朴素贝叶斯转变为多标签问题的良好分类器。我们将在[*第10章*](B18714_10.xhtml#_idTextAnchor193)“多分类器”中回到这个问题。
- en: 'We also need to try to work out why Naïve Bayes produces a considerable improvement
    over the lexicon-based approaches for the SEM4, CARER, and IMDB datasets but a
    worse performance for WASSA. We have already seen that the performance of Naïve
    Bayes improves substantially for CARER as we increase the training data. The dataset
    sizes for these three datasets are SEM4-EN 6812, WASSA-EN 3564, and CARER-EN 411809\.
    What happens if we restrict the training data for all three cases to be the same
    as for WASSA? The following table is a copy of the relevant part of the original
    table, using the full dataset in each case:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要尝试找出为什么朴素贝叶斯在SEM4、CARER和IMDB数据集上相对于基于词典的方法有相当大的改进，但在WASSA上表现较差。我们已经看到，随着训练数据的增加，朴素贝叶斯在CARER上的性能显著提高。这三个数据集的大小分别是SEM4-EN
    6812，WASSA-EN 3564，和CARER-EN 411809。如果我们将这三个案例的训练数据限制为与WASSA相同，会发生什么？以下表格是原始表格的相关部分的副本，每个案例都使用完整数据集：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确率** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM4-EN | 0.873 | 0.873 | 0.873 | 0.873 | 0.775 |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.873 | 0.873 | 0.873 | 0.873 | 0.775 |'
- en: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
- en: '| CARER-EN | 0.874 | 0.874 | 0.874 | 0.874 | 0.776 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.874 | 0.874 | 0.874 | 0.874 | 0.776 |'
- en: '| IMDB-EN | 0.849 | 0.849 | 0.849 | 0.849 | 0.738 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.849 | 0.849 | 0.849 | 0.849 | 0.738 |'
- en: Figure 6.9 – Naïve Bayes, English single-class datasets – full training sets
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 – 基于朴素贝叶斯，英文单类数据集 – 完整训练集
- en: 'When we reduce the amount of data available to be the same as for WASSA, the
    results get worse, as expected:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将可用的数据量减少到与WASSA相同，结果如预期的那样变得更差：
- en: '|  | **Precision** | **Recall** | **Micro F1** | **Macro F1** | **Jaccard**
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | **精确率** | **召回率** | **微观F1** | **宏观F1** | **Jaccard** |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| SEM4-EN | 0.837 | 0.837 | 0.837 | 0.837 | 0.719 |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.837 | 0.837 | 0.837 | 0.837 | 0.719 |'
- en: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.830 | 0.830 | 0.830 | 0.830 | 0.709 |'
- en: '| CARER-EN | 0.732 | 0.732 | 0.732 | 0.732 | 0.577 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.732 | 0.732 | 0.732 | 0.732 | 0.577 |'
- en: '| IMDB-EN | 0.825 | 0.825 | 0.825 | 0.825 | 0.703 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.825 | 0.825 | 0.825 | 0.825 | 0.703 |'
- en: Figure 6.10 – Naïve Bayes, English single-class datasets – restricted training
    sets
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 朴素贝叶斯，英语单类别数据集 – 限制训练集
- en: 'The improvements that we’ve made over the results from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector Space Models* for the SEM4-EN, CARER-EN, and IMDB-EN
    datasets are now less marked, particularly for CARER-EN: the loss of information
    when we restrict the size of the dataset is significant.'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对SEM4-EN、CARER-EN和IMDB-EN数据集的改进相对于第5章中[*“情感词典和向量空间模型”*](B18714_05.xhtml#_idTextAnchor116)的结果现在不那么明显，尤其是对于CARER-EN：当我们限制数据集大小时，信息丢失是显著的。
- en: Is there anything else that might explain the differences? Having more classes
    will make the problem more difficult. If you have, for instance, 10 classes, then
    making a random choice will get it right 10%
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 是否还有其他可能解释这些差异的因素？拥有更多的类别会使问题更加困难。例如，如果你有10个类别，那么随机选择正确的概率将是10%
- en: 'of the time whereas with 5 classes, a random choice will get it right 20% of
    the time. However, both SEM4-EN and WASSA-EN have the same set of labels, namely
    **anger**, **fear**, **joy**, and **sadness**, with CARER-EN having these four
    plus **love** and **surprise**, so if this were the key factor, we would expect
    the versions of SEM4-EN and WASSA to produce similar results and CARER to be a
    bit worse, which is not what we find. It is also likely that having a set where
    the distribution between classes is very uneven may make a difference. However,
    the distributions of the various emotions between SEM4-EN and WASSA-EN are fairly
    similar:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在5个类别的情况下，随机选择正确率仅为20%。然而，SEM4-EN和WASSA-EN具有相同的标签集，即**愤怒**、**恐惧**、**快乐**和**悲伤**，而CARER-EN除了这四个标签外还有**爱**和**惊讶**，所以如果这是关键因素，我们预计SEM4-EN和WASSA的版本会产生相似的结果，而CARER会略差一些，但这并不是我们观察到的结果。此外，可能存在一个类别分布非常不均的集合，这可能会产生影响。然而，SEM4-EN和WASSA-EN之间各种情绪的分布相当相似：
- en: 'SEM4-EN: anger: 834, fear: 466, joy: 821, sadness: 1443'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 'SEM4-EN: 愤怒：834，恐惧：466，快乐：821，悲伤：1443'
- en: 'WASSA-EN: anger: 857, fear: 1098, joy: 823, sadness: 786'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 'WASSA-EN: 愤怒：857，恐惧：1098，快乐：823，悲伤：786'
- en: SEM4-EN has more tweets that express sadness and WASSA-EN has more that express
    fear, but the differences are not of a kind that would lead you to expect a difference
    in the performance of a classifier. The two also have almost identical vocabulary
    sizes (75723 versus 75795) and almost identical average numbers of tokens per
    tweet (both 21.2). Sometimes, it just seems that one classifier is well suited
    to one task, and a different classifier is better suited to another.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: SEM4-EN有更多表达悲伤的推文，而WASSA-EN有更多表达恐惧的推文，但差异并不足以导致你期望分类器的性能有差异。这两个数据集的词汇量几乎相同（75723与75795），平均每条推文的标记数也几乎相同（均为21.2）。有时，似乎一个分类器非常适合一个任务，而另一个分类器更适合另一个任务。
- en: Summary
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we saw that Naïve Bayes can work extremely well as a classifier
    for finding emotions in tweets. It works particularly well with large training
    sets (and takes very little time to train since it simply counts occurrences of
    words and the emotions associated with the tweets they appear in). It can be adapted
    fairly straightforwardly to work with datasets where a single tweet may have any
    number of labels (including zero) but is outperformed on the test sets with this
    property by the lexicon-based approaches from [*Chapter 5*](B18714_05.xhtml#_idTextAnchor116)*,
    Sentiment Lexicons and Vector Space Models*. *Figure 6**.11* shows the best classifiers
    so far for the various datasets:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了朴素贝叶斯可以作为寻找推文中情感的分类器工作得非常好。它在大训练集上尤其有效（由于它只是计算单词及其出现在其中的推文所关联的情感的出现次数，因此训练时间非常短）。它可以相当直接地适应与可能具有任何数量标签（包括零）的单条推文一起工作的数据集，但在具有此属性的测试集上，它被第5章中[*“情感词典和向量空间模型”*](B18714_05.xhtml#_idTextAnchor116)的基于词典的方法所超越。*图6.11*显示了迄今为止各种数据集的最佳分类器：
- en: '|  | **LEX** | **CP** | **NB (single)** | **NB (multi)** |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | **LEX** | **CP** | **NB (single)** | **NB (multi)** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| SEM4-EN | 0.503 | 0.593 | 0.775 | *******0.778*** |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-EN | 0.503 | 0.593 | 0.775 | *******0.778*** |'
- en: '| SEM11-EN | 0.347 | *******0.353*** | 0.227 | 0.267 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-EN | 0.347 | *******0.353*** | 0.227 | 0.267 |'
- en: '| WASSA-EN | 0.445 | 0.505 | *******0.709*** | 0.707 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| WASSA-EN | 0.445 | 0.505 | *******0.709*** | 0.707 |'
- en: '| CARER-EN | 0.350 | 0.395 | *******0.776*** | 0.774 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| CARER-EN | 0.350 | 0.395 | *******0.776*** | 0.774 |'
- en: '| IMDB-EN | 0.722 | 0.722 | 0.738 | *******0.740*** |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| IMDB-EN | 0.722 | 0.722 | 0.738 | *******0.740*** |'
- en: '| SEM4-AR | 0.506 | 0.513 | 0.531 | *******0.532*** |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-AR | 0.506 | 0.513 | 0.531 | *******0.532*** |'
- en: '| SEM11-AR | 0.378 | *******0.382*** | 0.236 | 0.274 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-AR | 0.378 | *******0.382*** | 0.236 | 0.274 |'
- en: '| KWT.M-AR | *******0.687*** | 0.666 | 0.494 | 0.507 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| KWT.M-AR | *******0.687*** | 0.666 | 0.494 | 0.507 |'
- en: '| SEM4-ES | *******0.425*** | 0.177 | 0.360 | 0.331 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| SEM4-ES | *******0.425*** | 0.177 | 0.360 | 0.331 |'
- en: '| SEM11-ES | 0.269 | *******0.278*** | 0.230 | 0.255 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| SEM11-ES | 0.269 | *******0.278*** | 0.230 | 0.255 |'
- en: Figure 6.11 – Best classifiers so far
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – 目前最佳分类器
- en: In general, Naïve Bayes is the best classifier for datasets where each tweet
    has only one label, with marginal differences in these datasets between the version
    of Naïve Bayes that assumes there is only one label per tweet and the version
    that allows for multiple labels. For the multi-label datasets, the version that
    allows for multiple labels always outperforms the one that doesn’t, but in all
    these cases, one of the lexicon-based classifiers from *wwwwwwwwwww, Sentiment
    Lexicons and Vector Space Models* is best. For now, the biggest lesson from this
    chapter is that when trying to solve a classification problem, you should try
    various approaches and take the one that works best. We will see what happens
    when we look at more sophisticated machine learning algorithms in the following
    chapters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，朴素贝叶斯是针对每个推文只有一个标签的数据集的最佳分类器，在这些数据集中，朴素贝叶斯假设每个推文只有一个标签的版本与允许多个标签的版本之间的边际差异不大。对于多标签数据集，允许多个标签的版本总是优于不允许的版本，但在所有这些情况下，来自
    *wwwwwwwwwww, Sentiment Lexicons and Vector Space Models* 的基于词典的分类器表现最佳。到目前为止，本章最大的教训是，在尝试解决分类问题时，你应该尝试各种方法，并选择效果最好的方法。在接下来的章节中，我们将看到当我们查看更复杂的机器学习算法时会发生什么。
- en: References
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'To learn more about the topics that were covered in this chapter, take a look
    at the following resources:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于本章所涉及的主题的信息，请查看以下资源：
- en: Bayes, T. (1958). *An essay towards solving a problem in the doctrine of chances*.
    Biometrika, 45(3–4), 296–315\. [https://doi.org/10.1093/biomet/45.3-4.296](https://doi.org/10.1093/biomet/45.3-4.296).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bayes, T. (1958). *An essay towards solving a problem in the doctrine of chances*.
    Biometrika, 45(3–4), 296–315\. [https://doi.org/10.1093/biomet/45.3-4.296](https://doi.org/10.1093/biomet/45.3-4.296).
