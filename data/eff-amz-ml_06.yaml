- en: Predictions and Performances
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测和性能
- en: It is time to make some predictions! In [Chapter 4](08d9b49a-a25c-4706-8846-36be9538b087.xhtml),
    *Loading and Preparing the Dataset,* we did split the `Titanic` dataset into two
    subsets, the training and held-out subsets, respectively consisting of 70% and
    30% of the original dataset randomly shuffled. We have used variations of the training
    subset extensively in [chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml) *Model
    Creation,* to train and select the best classification model. But so far, we have
    not used the held-out subset at all. In this chapter, we apply our models to this
    held-out subset to make predictions on unseen data and make a final assessment
    of the performance and robustness of our models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候做出一些预测了！在[第4章](08d9b49a-a25c-4706-8846-36be9538b087.xhtml)，“加载数据集和准备数据”，我们将`Titanic`数据集分为两个子集，分别是训练集和保留集，分别占原始数据集的70%和30%，并且随机打乱。我们在[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)“模型创建”中广泛使用了训练子集，以训练和选择最佳的分类模型。但到目前为止，我们还没有使用保留集。在本章中，我们将我们的模型应用于这个保留集，对未见数据做出预测，并对我们模型的性能和鲁棒性进行最终评估。
- en: 'Amazon ML offers two types of predictions: batch and streaming. Batch prediction
    requires a datasource. The samples you want to predict are given to the model
    all at once in batch mode. Streaming, also known as real-time or online predictions,
    requires the creation of an API endpoint and consists of submitting sequences
    of samples, one by one, via HTTP requests. Real-time predictions do not involve
    the creation of a datasource.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML提供两种类型的预测：批量预测和流式预测。批量预测需要一个数据源。你想要预测的样本将以批量模式一次性提供给模型。流式预测，也称为实时或在线预测，需要创建一个API端点，并包括通过HTTP请求逐个提交样本序列。实时预测不涉及创建数据源。
- en: We will start with batch predictions on the Titanic held-out set. We will confirm
    that our different models perform similarly on the held-out dataset as they did
    on the validation subsets, assuming that all the subsets have a similar variable
    distribution. In [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model
    Creation*, we concluded that out of our three datasources — suggested recipe with
    quantile binning (QB), recipe without QB, and the extended dataset — the one with extra
    variables (`deck`, `title`, `log_fare`, and so on) resulted in the best score
    on the validation subset. We will verify that this is also the case on the held-out
    subset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从对`Titanic`保留集的批量预测开始。我们将确认我们的不同模型在保留数据集上的表现与在验证子集上的表现相似，假设所有子集具有相似的变量分布。在[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)“模型创建”中，我们得出结论，在我们的三个数据源——建议的量分箱（QB）食谱、无QB的食谱和扩展数据集——中，包含额外变量（如`deck`、`title`、`log_fare`等）的那个数据源在验证子集上得到了最佳的分数。我们将验证这一点也适用于保留集。
- en: This chapter is organized in two parts. In the first part, we look at batch
    predictions on the `Titanic` dataset. In the second part, we look at real-time,
    streaming predictions, with a new text-based quantile binning from the UCI repository.
    The `Spam` dataset is large enough to simulate streaming data. We will create
    an Amazon ML endpoint and use the Python SDK to send and retrieve classification
    predictions.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为两部分。在第一部分，我们查看`Titanic`数据集的批量预测。在第二部分，我们查看基于UCI存储库的新文本量分箱的实时、流式预测。`Spam`数据集足够大，可以模拟流数据。我们将创建一个Amazon
    ML端点，并使用Python SDK发送和检索分类预测。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Making batch predictions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作批量预测
- en: Making real-time predictions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 制作实时预测
- en: In real-world classification problems or regression problems, the previously
    unseen data you want to make predictions on will not include the target values.
    In our case, the held-out datasets do contain the solution, and this allows us
    to assess the model performance on previously unseen data. But with real-world
    problems, you do not have that luxury and you will have to trust your model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的分类问题或回归问题中，你想要对其做出预测的先前未见数据将不包括目标值。在我们的案例中，保留数据集确实包含解决方案，这使我们能够评估模型在先前未见数据上的性能。但在现实世界的问题中，你不会有这样的奢侈，你将不得不信任你的模型。
- en: Making batch predictions
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 制作批量预测
- en: 'Making batch predictions on Amazon ML is straightforward and follows this process:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon ML上制作批量预测的过程简单明了，遵循以下步骤：
- en: From the dashboard, create a new Batch prediction.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从仪表板创建一个新的批量预测。
- en: Select the model.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择模型。
- en: Select the datasource on which to apply the model.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择应用模型的数据源。
- en: Set the prediction output folder and grant permissions.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置预测输出文件夹并授予权限。
- en: Review and launch.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查并启动。
- en: We call the `prediction` dataset or datasource, the data on which we want to
    make predictions. In this chapter, we are in a testing context and the `prediction`
    dataset is the `held-out` dataset we extracted from the whole original dataset.
    In a real-world context, the prediction dataset refers to entirely new data and
    does not include the target variable.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称预测数据集或数据源为，我们想要进行预测的数据。在本章中，我们处于测试环境中，预测数据集是我们从整个原始数据集中提取的`保留`数据集。在现实世界的情况下，预测数据集指的是全新的数据，并且不包含目标变量。
- en: The prediction can only work if the distribution of the prediction dataset is
    similar to the distribution of the training dataset on which the model has been
    trained. The prediction datasource and the training datasource must also share
    the same schema, with one difference the prediction dataset does not need to include
    the target variable. Amazon ML will verify that the schema defined for your training
    data is relevant to your prediction data and will issue a warning if the datasets
    are not similar.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 预测只有在预测数据集的分布与模型训练所用的训练数据集的分布相似时才能工作。预测数据源和训练数据源还必须共享相同的模式，唯一的区别是预测数据集不需要包含目标变量。Amazon
    ML 将验证为您的训练数据定义的模式是否与您的预测数据相关，如果数据集不相似，将发出警告。
- en: For the sake of convenience, we have recreated the datasets, datasources, and
    models for this chapter. All datasets and scripts are available in the GitHub
    repository at [https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6).
    Since we reshuffled the original Titanic data, the evaluation scores will be different
    from the ones obtained previously for the same dataset.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，我们已重新创建了本章的数据集、数据源和模型。所有数据集和脚本均可在 GitHub 仓库中找到，网址为 [https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6)。由于我们对原始的泰坦尼克号数据进行了重新排序，因此评估分数将与之前相同数据集获得的分数不同。
- en: Creating the batch prediction job
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建批量预测作业
- en: 'To create a batch prediction, go to the Amazon ML dashboard and click on Create
    new batch prediction:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建批量预测，请转到 Amazon ML 仪表板并点击创建新的批量预测：
- en: '![](img/B05028_06_01.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_01.png)'
- en: 'Then select the model. We choose the original model related to the `Titanic`
    dataset, the one using the Amazon ML suggested recipe with quantile binning:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 然后选择模型。我们选择与`泰坦尼克号`数据集相关的原始模型，使用 Amazon ML 建议的食谱进行分位数分箱：
- en: Quantile binning of all numeric variables
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有数值变量的分位数分箱
- en: L2 mild regularization
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 轻度正则化
- en: '![](img/B05028_06_02.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_02.png)'
- en: 'After the model selection comes the datasource selection. If you have not yet
    created a datasource for the held-out set, you can do so now. First, upload your
    prediction dataset to S3 and specify the S3 path for the source of the data:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型选择之后，进行数据源选择。如果您尚未为保留集创建数据源，现在可以创建。首先，将您的预测数据集上传到 S3，并指定数据的 S3 路径：
- en: '![](img/B05028_06_03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_03.png)'
- en: 'When you click on Verify, Amazon ML will check that the prediction dataset
    follows the same schema as the training dataset on which the model was trained:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当您点击验证时，Amazon ML 将检查预测数据集是否遵循与模型训练所用的训练数据集相同的模式：
- en: '![](img/B05028_06_04.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_04.png)'
- en: Interpreting prediction outputs
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解释预测输出
- en: 'The output of the Amazon ML prediction job will consists of two files: the
    manifest file and the actual prediction results given in a compressed CSV file.
    Amazon ML will create the files on S3 in an S3 location, `s3://bucket/folder`,
    which you must specify. We use the same path as our data path: `s3://aml.packt/data/`.
    Amazon ML will create a `/batch_prediction` folder, where it will write the manifest
    file as well as an extra subfolder `/results`, where the CSV file with the actual
    predictions will be written. To recap, in our context, the manifest file will
    be in the  `s3://aml.packt/data/batch_prediction` folder, and the compressed CSV
    results file will be in the `s3://aml.packt/data/batch_prediction/results/` folder.The
    name given to the batch prediction will dictate the naming of the manifest and
    results files:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习预测作业的输出将包括两个文件：清单文件和以压缩CSV文件格式提供的实际预测结果。亚马逊机器学习将在S3上的指定位置`s3://bucket/folder`创建这些文件，您必须指定该路径。我们使用与数据路径相同的路径：`s3://aml.packt/data/`。亚马逊机器学习将在`/batch_prediction`文件夹中创建文件，其中它将写入清单文件以及一个额外的子文件夹`/results`，实际预测的CSV文件将写入该子文件夹。总结一下，在我们的场景中，清单文件将位于`
    s3://aml.packt/data/batch_prediction`文件夹中，压缩的CSV结果文件将位于`s3://aml.packt/data/batch_prediction/results/`文件夹中。分配给批量预测的名称将决定清单和结果文件的命名：
- en: '![](img/B05028_06_05.png)**Prediction pricing**: If you just created the datasource
    for the batch prediction, Amazon ML does not yet have access to the data statistics
    that it needs to calculate the prediction costs. In that case, it will simply
    inform you of the price, of $0.10 per 1,000 predictions. If the prediction datasource
    has already been validated and Amazon ML knows the number of records, the estimated
    price will be the number of rows times the price per prediction rounded up to
    the nearest cent. You are not billed for the invalid samples Amazon ML fails to
    predict. More information is available at [http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html](http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html).'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![预测定价](img/B05028_06_05.png)**预测定价**：如果您刚刚创建了批量预测的数据源，亚马逊机器学习尚未获得计算预测成本所需的数据统计信息。在这种情况下，它将简单地通知您价格，每1,000次预测0.10美元。如果预测数据源已经过验证，并且亚马逊机器学习知道记录数，则估计价格将是行数乘以每次预测的价格，四舍五入到最接近的美分。亚马逊机器学习无法预测的无效样本不会产生费用。更多详细信息请参阅[http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html](http://docs.aws.amazon.com/machine-learning/latest/dg/pricing.html)。'
- en: Review and click on the Create batch prediction button. The batch prediction
    job will take a few minutes to complete. When finished, it will have created the
    manifest and results files in S3 and will show up as completed in the Batch Prediction
    section of the Amazon ML dashboard.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 查看并点击创建批量预测按钮。批量预测作业将需要几分钟才能完成。完成后，它将在S3中创建清单和结果文件，并在亚马逊机器学习仪表板的批量预测部分显示为已完成。
- en: Reading the manifest file
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取清单文件
- en: 'The manifest file contains JSON-formatted data that maps the input file to
    the prediction results file, as follows:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 清单文件包含JSON格式的数据，将输入文件映射到预测结果文件，如下所示：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In our context, the manifest file contains the following line:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，清单文件包含以下行：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '**Multiple input files**: If your input data is split into several files, all
    stored in the same S3 location `s3://examplebucket/input/`, all the input files
    will be considered by the batch prediction job. The manifest file will then contain
    the mapping from the different input files to the associated results files. For
    instance, if you have three input files named `data1.csv`, `data2.csv`, and `data3.csv`,
    and they are all stored in the S3 location `s3://examplebucket/input/`, you will see
    a mapping string that looks like as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**多个输入文件**：如果您的输入数据被分割成几个文件，并且所有文件都存储在同一个S3位置`s3://examplebucket/input/`，所有输入文件都将被批量预测作业考虑。然后清单文件将包含不同输入文件到相关结果文件的映射。例如，如果您有三个名为`data1.csv`、`data2.csv`和`data3.csv`的输入文件，并且它们都存储在S3位置`s3://examplebucket/input/`，您将看到一个如下所示的映射字符串：'
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '**Maximum size for predictions**: Amazon ML allows up to 1TB of data for prediction
    files. If the data on which you want to make predictions is larger, it is possible
    to split your data into several files, upload them to a specific S3 location,
    and Amazon ML will handle the different files and generate as many prediction
    result files as there are input files by running several batches in parallel.
    The manifest file will contain all the different input/output pairs, `{input_file.csv
    : prediction_results.csv.gz}` for your different batch prediction files.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测的最大大小**：Amazon ML允许预测文件的最大数据量为1TB。如果你想要进行预测的数据量更大，可以将数据分割成几个文件，上传到特定的S3位置，Amazon
    ML将处理不同的文件，并通过并行运行多个批次生成与输入文件数量相等的预测结果文件。清单文件将包含所有不同的输入/输出对，`{input_file.csv :
    prediction_results.csv.gz}`，针对你的不同批量预测文件。'
- en: Reading the results file
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 读取结果文件
- en: The output results file is compressed with gzip, originates from the UNIX world,
    and offers better compression than the more common zip compression. A simple click
    should be sufficient to open and decompress the gzipped results file into a readable
    CSV file. Alternatively, a call to the gunzip command from the command line should
    work. Take a look at [http://www.gzip.org/](http://www.gzip.org/) for installation
    on different systems.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 输出结果文件使用gzip压缩，源自UNIX世界，提供的压缩效果优于更常见的zip压缩。简单点击即可打开和解压缩gzip压缩的结果文件，将其转换为可读的CSV文件。或者，可以从命令行调用gunzip命令。查看[http://www.gzip.org/](http://www.gzip.org/)获取不同系统的安装信息。
- en: 'For binary classification, the decompressed results file contains two or three
    columns, depending on whether the initial input file contained the target or not.
    In our case of binary classification, the result file has the following columns:
    `trueLabel`, `bestAnswer`, and `score`, where `trueLabel` is the initial `survived`
    column. If your initial batch prediction dataset did not include the target values,
    the results file will only have the bestAnswer and score columns:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于二元分类，解压缩的结果文件包含两列或三列，具体取决于初始输入文件是否包含目标值。在我们的二元分类案例中，结果文件包含以下列：`trueLabel`、`bestAnswer`和`score`，其中`trueLabel`是初始的`survived`列。如果你的初始批量预测数据集没有包含目标值，结果文件将只包含`bestAnswer`和`score`列：
- en: '`trueLabel` is the original target value contained in the input file'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trueLabel`是输入文件中包含的原始目标值'
- en: '`bestAnswer` is the classification result: 0 or 1'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bestAnswer`是分类结果：0或1'
- en: '`Score` is the probability for that classification written in scientific notation'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Score`是以科学记数法表示的那个分类的概率'
- en: The classification cutoff threshold for the score probability is 0.5 by default,
    or set to the threshold value you chose while evaluating the model.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 分数的概率分类截止阈值默认为0.5，或者在评估模型时设置的阈值值。
- en: For multiclass classification with *N* potential target classes, the results
    file will have *N+1* or *N+2* columns. The `trueLabel`, `bestAnswer`, and `N`
    columns each with the probability scores for each one of the N classes. The chosen
    class will be the one that bears the highest probability score.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*N*个潜在目标类别的多类分类，结果文件将包含*N+1*或*N+2*列。`trueLabel`、`bestAnswer`和*N*列分别包含N个类别中每个类别的概率分数。所选的类别将是具有最高概率分数的类别。
- en: For a regression model, the results file will only contain one/two score columns
    with the predicted value, and possibly the `trueLabel` column.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对于回归模型，结果文件将只包含一个/两个分数列，包含预测值，可能还有`trueLabel`列。
- en: Assessing our predictions
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估我们的预测
- en: Since we know the real class of our held-out samples, we can calculate the **ROC-AUC**
    score and other metrics to see how close our prediction and validation scores
    are. Assuming that our data subsets have very similar distributions, both scores
    should end up very close. The difference only comes from randomness in the samples
    for the validation and held-out sets.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们知道保留样本的真实类别，我们可以计算**ROC-AUC**分数和其他指标，以查看我们的预测和验证分数有多接近。假设我们的数据子集具有非常相似的分布，这两个分数最终应该非常接近。差异仅来自验证和保留集样本中的随机性。
- en: 'The following Python script uses the `scikit-learn` library ([http://scikit-learn.org/](http://scikit-learn.org/))
    as well as the pandas library. It takes a few lines of Python to calculate the
    AUC score of the model on that prediction dataset. First, download the gzipped
    file from S3 and then, in a Python Notebook or console, run the following:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python脚本使用了`scikit-learn`库([http://scikit-learn.org/](http://scikit-learn.org/))以及pandas库。只需几行Python代码即可计算模型在该预测数据集上的AUC得分。首先，从S3下载压缩文件，然后在Python笔记本或控制台中运行以下代码：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '**Python environment**: All the Python code in this book is for Python 3.5
    or above. For more information on the Anaconda library, take a look at [https://www.continuum.io/downloads](https://www.continuum.io/downloads).
    Anaconda is an amazingly powerful open source data science platform in Python.
    It contains the most important libraries (`numpy`, `pandas`, `scikit-learn`, `matplotlib`,
    and many others) as well as the Jupyter Notebooks environment. We use the IPython
    console for its simplicity of use and many magic commands ([http://ipython.readthedocs.io/en/stable/interactive/magics.html](http://ipython.readthedocs.io/en/stable/interactive/magics.html)).'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**Python环境**：本书中的所有Python代码都是针对Python 3.5或更高版本。有关Anaconda库的更多信息，请参阅[https://www.continuum.io/downloads](https://www.continuum.io/downloads)。Anaconda是一个惊人的强大开源数据科学平台，在Python中。它包含最重要的库（`numpy`、`pandas`、`scikit-learn`、`matplotlib`等）以及Jupyter笔记本环境。我们使用IPython控制台，因为它使用简单，并且有许多魔法命令([http://ipython.readthedocs.io/en/stable/interactive/magics.html](http://ipython.readthedocs.io/en/stable/interactive/magics.html))。'
- en: Running the previous Python script on the predictions results, we obtain an
    AUC of 0.84 on our held-out dataset, which is very close to the AUC (0.85) we
    obtained on the validation set in [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml),
    *Model Creation*. We can conclude that our model is pretty stable and robust when
    facing new, previously unforeseen data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测结果上运行之前的Python脚本，我们在保留集数据集上获得AUC为0.84，这非常接近我们在[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)“模型创建”中验证集上获得的AUC（0.85）。我们可以得出结论，我们的模型在面对新的、之前未预见的数据时相当稳定和健壮。
- en: 'The following plot shows both the ROC curves for the validation (dotted line)
    and held-out (solid-line) sets for the chosen model. The validation set is slightly
    better for higher values of the threshold. This difference is a reflection of
    the different data distributions in the two datasets:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了所选模型的验证集（虚线）和保留集（实线）的ROC曲线。对于较高的阈值，验证集略好。这种差异反映了两个数据集中数据分布的不同：
- en: '![](img/B05028_06_06.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_06.png)'
- en: Evaluating the held-out dataset
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估保留集
- en: In [Chapter 5](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml), *Model Creation*,
    we evaluated the performance of our different models on a slice of the training
    datasource. We obtained for each model an AUC score, and selected the AUC with
    the best AUC score. We relied on Amazon ML to create the validation set, by splitting
    the training dataset into two, with 70% for training and 30% of the data for validation.
    We could have done that split ourselves, created the validation datasource, and
    specified which datasource to use for the evaluation of the model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](b71d5007-58a0-4b79-9446-4a36e7476037.xhtml)“模型创建”中，我们评估了我们的不同模型在训练数据源的一个切片上的性能。我们对每个模型都获得了AUC得分，并选择了具有最佳AUC得分的AUC。我们依赖Amazon
    ML创建验证集，通过将训练数据集分成两部分，其中70%用于训练，30%的数据用于验证。我们本可以自己进行分割，创建验证数据源，并指定用于模型评估的数据源。
- en: 'In fact, nothing prevents us from running a model evaluation on the held-out
    dataset. If you go to the model summary page, you will notice a Perform another
    Evaluation button in the Evaluation section:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们没有任何阻止我们在保留集数据集上运行模型评估。如果您转到模型摘要页面，您会在评估部分注意到一个“执行另一个评估”按钮：
- en: '![](img/B05028_06_15.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_15.png)'
- en: 'Click on it. You are asked to select the datasource for the evaluation. Select
    the held-out dataset; Amazon ML will verify that the data follows the same schema
    and is similar to the training data. You end up with two evaluations on the model:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 点击它。您将被要求选择用于评估的数据源。选择保留集；Amazon ML将验证数据是否遵循相同的模式，并且与训练数据相似。您最终会在模型上获得两个评估：
- en: '![](img/B05028_06_16.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_16.png)'
- en: 'And as expected, the evaluation AUC for the held-out dataset is equal to the
    AUC we obtained by downloading the results and calculating the AUC in Python:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，保留集的评估AUC与我们通过下载结果并在Python中计算AUC获得的AUC相等：
- en: '![](img/B05028_06_17.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_17.png)'
- en: Finding out who will survive
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现谁将生存
- en: The true value of predictions, however, is not about validating the robustness
    of our model; it's about making predictions on our prediction dataset, in our
    context, getting survival predictions on this `new` list of passengers contained
    in the held-out dataset.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，预测的真正价值并不在于验证我们模型的鲁棒性；它在于在我们的预测数据集、我们的环境中进行预测，对保留数据集中的这个“新”乘客列表进行生存预测。
- en: 'The rows in the results file follow the exact same order as the rows in the
    prediction file. We can put side by side the first rows of the held-out file and the
    first rows of the results file, and see that the `survived` and the `trueLabel`
    columns are identical:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 结果文件中的行与预测文件中的行顺序完全相同。我们可以将保留文件的前几行和结果文件的前几行并排放置，并看到“survived”和“trueLabel”列是相同的：
- en: '![](img/B05028_06_13.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_06_13.png)'
- en: Multiplying trials
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 乘法试验
- en: 'The evaluation scores on the various models and dataset version are, to a certain
    extent, dependent on the samples contained in the evaluation sets. If we run the
    following experiment several times on the three datasets, we see certain variations
    in the scores:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种模型和数据集版本上的评估分数在一定程度上取决于评估集中的样本。如果我们在这三个数据集上多次运行以下实验，我们会看到分数的某些变化：
- en: Shuffle and split the dataset into three -- training, validation, and held-out
    and create the respective datasources
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 打乱并分割数据集为三个部分——训练、验证和保留，并创建相应的数据源
- en: Train a model on the training dataset, keeping the default Amazon ML settings
    (mild L2 regularization)
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练数据集上训练模型，保持默认的亚马逊机器学习设置（轻微的L2正则化）
- en: Evaluate the model on the evaluation and held-out datasets
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在评估和保留数据集上评估模型
- en: 'The following plot shows the respective performances of the three models for
    several trials. The average AUC is written on the graph. We see that on average,
    the extended dataset performs better (*AUC = 0.87*) than the original dataset
    with the default recipe (*AUC = 0.84*) and the original dataset without quantile
    binning (*AUC = 0.83*). We also notice that in some trials, the extended dataset
    performs worse than the original one. For trial 3, the default recipe is even
    slightly less performant than the no quantile binning one:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了三个模型在多次试验中的相应性能。图表上写有平均AUC。我们看到平均而言，扩展数据集的性能优于默认配方下的原始数据集（AUC = 0.84）和没有分位数分箱的原始数据集（AUC
    = 0.83）。我们还注意到，在某些试验中，扩展数据集的性能不如原始数据集。对于试验3，默认配方甚至略逊于没有分位数分箱的配方：
- en: '![](img/B05028_06_14.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_06_14.png)'
- en: This shows the importance of inner data distribution variability. When trying
    out several variants of your datasets with different features and processing,
    it's important to base your conclusions on several runs of your models. A single evaluation
    may lead to missing the best model.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了内部数据分布变异性的重要性。当尝试使用不同特征和处理的多个数据集变体时，基于您模型的多次运行得出结论是很重要的。单次评估可能会导致错过最佳模型。
- en: Making real-time predictions
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行实时预测
- en: With batch predictions, you submit all the samples you want the model to predict
    at once to Amazon ML by creating a datasource. With real-time predictions, also
    called streaming or online predictions, the idea is to send one sample at a time
    to an API endpoint, a URL, via HTTP queries, and receive back predictions and
    information for each one of the samples.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 使用批量预测时，您可以通过创建数据源一次将所有希望模型预测的样本提交给亚马逊机器学习。使用实时预测（也称为流式或在线预测），想法是每次发送一个样本到一个API端点、一个URL，通过HTTP查询，并为每个样本接收预测和信息。
- en: Setting up real-time predictions on a model consists of knowing the prediction
    API endpoint URL and writing a script that can read your data, send each new sample to
    that API URL, and retrieve the predicted class or value. We will present a Python-based
    example in the following section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型上设置实时预测包括了解预测API端点URL和编写一个脚本，该脚本可以读取您的数据，将每个新样本发送到该API URL，并检索预测的类别或值。我们将在下一节中提供一个基于Python的示例。
- en: Amazon ML also offers a way to make predictions on data you create on the fly
    on the prediction page. We can input the profile of a would-be passenger on the
    `Titanic` and see whether that profile would have survived or not. It is a great
    way to explore the influence of the dataset variables on the outcome.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊机器学习（Amazon ML）还提供了一种方法，可以在预测页面上对您实时创建的数据进行预测。我们可以输入一个潜在的乘客在“泰坦尼克号”上的档案，并查看该档案是否能够幸存。这是一种探索数据集变量对结果影响的绝佳方式。
- en: Before setting up API for streaming, let’s see what we can gather from submitting
    several single passenger profiles. We can even try to answer the question – *W**ould
    you have survived on the Titanic?*
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置流式API之前，让我们看看通过提交几个单个乘客档案我们能收集到什么信息。我们甚至可以尝试回答这个问题——*你会在大西洋号上幸存下来吗？*
- en: Manually exploring variable influence
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手动探索变量影响
- en: Go to the model summary page and click on the Try real-time predictions link
    on the left side of the page. The following page shows a form where you can fill
    out values for the variables in our dataset except for the target.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前往模型摘要页面，然后点击页面左侧的“尝试实时预测”链接。接下来的页面显示一个表单，您可以在其中填写我们数据集中变量的值，除了目标变量。
- en: 'Let''s see if Alex Mr. Perrier, a first-class passenger who embarked at Southhampton
    with his family (3 sibsp and 2 parch) and who paid a fare of 100 pounds, would
    have survived. Well, in that case, the model gives a very low probability of survival
    (0.001), meaning that the model predicts with confidence that this passenger would
    not have survived. His 12 year old daughter would have had better chances of surviving
    (*probability 0.56*), though the model is less sure of it. However, if that same
    girl was traveling alone (*sibsp = parch = 0*), her chance of survival would surge
    to 0.98 under the condition that she traveled in 1st class. In 3rd class, she
    would have been less fortunate (*probability: 0.28*):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看亚历克斯·佩里埃先生，一位头等舱乘客，他在南安普顿和他的家人（3个兄弟姐妹和2个孩子）登船，并支付了100英镑的船费，他是否会幸存。好吧，在这种情况下，模型给出了非常低的生存概率（0.001），这意味着模型有信心预测这位乘客不会幸存。他的12岁女儿有更大的生存机会（*概率为0.56*），尽管模型对此不太确定。然而，如果那个女孩独自旅行（*兄弟姐妹=孩子=0*），在头等舱的条件下，她的生存机会会激增到0.98。在三等舱，她将不太幸运（*概率：0.28*）：
- en: '![](img/B05028_06_18.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_18.png)'
- en: So, by changing one variable at a time in the data, we can have a better understanding
    of the impact of each variable on the outcome.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，通过逐个改变数据中的变量，我们可以更好地理解每个变量对结果的影响。
- en: Setting up real-time predictions
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置实时预测
- en: 'To demonstrate real-time predictions, we will use the `Spam` dataset from the
    UCI repository. This dataset is composed of 5,574 SMS messages annotated spam
    or ham (non-spam). There are no missing values and only two variables: the nature
    of the SMS (ham or spam) and the text message of the SMS, nothing else.  The `Spam`
    dataset is available at [https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection) in
    its raw form, and in the book''s GitHub repository at [https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6).
    We have simply transformed the target from categorical: `spam` and `ham` values
    to binary: 1 (for spam) and 0 (for ham) so that Amazon ML understands the prediction
    to be of the binary-classification type.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示实时预测，我们将使用来自UCI仓库的`Spam`数据集。这个数据集由5,574条标注为垃圾邮件或非垃圾邮件（非垃圾邮件）的短信组成。没有缺失值，只有两个变量：短信的性质（垃圾邮件或非垃圾邮件）和短信文本，没有其他内容。《Spam》数据集以原始形式可在[https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection](https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection)找到，并在本书的GitHub仓库[https://github.com/alexperrier/packt-aml/tree/master/ch6](https://github.com/alexperrier/packt-aml/tree/master/ch6)中提供。我们简单地将目标从分类值：`垃圾邮件`和`非垃圾邮件`转换为二进制值：1（代表垃圾邮件）和0（代表非垃圾邮件），这样Amazon
    ML就能理解预测为二分类类型。
- en: AWS SDK
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS SDK
- en: AWS offers several APIs and **Software Development Kits (SDKs)** to its many
    services. You can programmatically manage your files on S3, set up EC2 instances,
    and create datasources, models, and evaluations on Amazon ML without using the
    web-based user interface. The AWS APIs are low-level endpoints. In general, it
    is simpler and more efficient to use the SDKs, which are wrappers around the APIs
    and are available for several languages (Python, Ruby, Java, C++, and so on).
    In this book, we will use the Python SDK based on the `Boto3` library. We explore
    in detail the use of the Python SDK in [Chapter 7](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml),
    *Command Line and SDK.* For now, we will only use the `predict()` method necessary
    for real-time predictions. But first, we need to enable access to AWS by setting
    up AWS credentials on our local machine.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: AWS 为其众多服务提供了几个 API 和 **软件开发工具包 (SDKs)**。您可以使用编程方式管理 S3 上的文件，设置 EC2 实例，并在 Amazon
    ML 上创建数据源、模型和评估，而无需使用基于网页的用户界面。AWS API 是低级端点。通常，使用 SDKs 更简单、更高效，SDKs 是 API 的包装器，并支持多种语言（Python、Ruby、Java、C++
    等）。在这本书中，我们将使用基于 `Boto3` 库的 Python SDK。我们将在第 7 章 [Command Line and SDK](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml)
    中详细探讨 Python SDK 的使用。现在，我们只将使用 `predict()` 方法，这是实时预测所必需的。但首先，我们需要通过在我们的本地机器上设置
    AWS 凭据来启用对 AWS 的访问。
- en: Setting up AWS credentials
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 AWS 凭据
- en: 'In order to access AWS programmatically, we first need to access AWS via the
    command line. This requires the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以编程方式访问 AWS，我们首先需要通过命令行访问 AWS。这需要以下内容：
- en: Creating access keys on AWS IAM for your user
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 AWS IAM 为您的用户创建访问密钥
- en: Installing the `AWS-CLI` command-line interface on local
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地安装 `AWS-CLI` 命令行界面
- en: Configuring `AWS-CLI` with the AWS access keys
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 `AWS-CLI` 使用 AWS 访问密钥
- en: AWS access keys
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AWS 访问密钥
- en: 'If you recall from [Chapter 3](5938ed0c-9243-49cc-bf67-314ffb5f9386.xhtml),
    *Overview of an Amazon Machine Learning Workflow*, we had created access keys
    for our `AML@Packt` user. Access keys are user-based and composed of two parts:
    the **Access Key ID**, which is always available in the user security credentials
    tab in IAM, and the **Secret Access Key**, which is only shown at creation time.
    When creating these access keys, you are given the possibility of downloading
    them. If you did not do so at that time, you can recreate access keys for your
    user now. Go to the IAM console at  [https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam),
    click on your user profile, select the Security Credentials tab, and click on
    the Create Access Key button. This time make sure you download the keys on your
    local machine or copy them somewhere. Note that there’s a limit of two sets of
    access keys per user. You will have to delete existing keys before creating new
    ones if you already have two keys associated to your user:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得第 3 章 [Overview of an Amazon Machine Learning Workflow](5938ed0c-9243-49cc-bf67-314ffb5f9386.xhtml)，我们为
    `AML@Packt` 用户创建了访问密钥。访问密钥是基于用户的，由两部分组成：**访问密钥 ID**，它始终在 IAM 的用户安全凭证选项卡中可用，以及**秘密访问密钥**，仅在创建时显示。在创建这些访问密钥时，您有下载它们的机会。如果您当时没有这样做，现在可以为您的用户重新创建访问密钥。转到
    IAM 控制台 [https://console.aws.amazon.com/iam](https://console.aws.amazon.com/iam)，点击您的用户配置文件，选择安全凭证选项卡，然后点击创建访问密钥按钮。这次请确保在您的本地机器上下载密钥或将它们复制到其他地方。请注意，每个用户最多只能有两套访问密钥。如果您已经有两个与您的用户关联的密钥，您必须删除现有的密钥才能创建新的密钥：
- en: '![](img/B05028_03_03.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_03_03.png)'
- en: Setting up AWS CLI
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置 AWS CLI
- en: So far, we have only worked with the AWS web interface, clicking from page to
    page on the AWS website. Another way to interact with AWS services is via the
    command line in a terminal window, using the `aws cli` library. CLI stands for
    Command Line Interface.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只使用过 AWS 网页界面，在 AWS 网站上逐页点击。另一种与 AWS 服务交互的方式是通过终端窗口中的命令行，使用 `aws cli`
    库。CLI 代表命令行界面。
- en: 'To install the `aws cli` library, open a terminal window. For a Python-based
    environment (Python 2 version 2.6.5+ or Python 3 version 3.3+), installing `aws
    cli` consists of running the following command in a terminal:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装 `aws cli` 库，打开一个终端窗口。对于基于 Python 的环境（Python 2 版本 2.6.5+ 或 Python 3 版本 3.3+），安装
    `aws cli` 包括在终端中运行以下命令：
- en: '[PRE4]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Full instructions for installation in other environments are available at [http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html).
     Once AWS-CLI is installed, run the following command to configure it:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 其他环境下的安装完整说明可在[http://docs.aws.amazon.com/cli/latest/userguide/installing.html](http://docs.aws.amazon.com/cli/latest/userguide/installing.html)找到。一旦AWS-CLI安装完成，运行以下命令来配置它：
- en: '[PRE5]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You will be asked for your access keys, the default region, and format. See
    [http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration)
    for more in-depth explanations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被要求输入您的访问密钥、默认区域和格式。有关更深入的说明，请参阅[http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration](http://docs.aws.amazon.com/cli/latest/userguide/cli-chap-getting-started.html#cli-quick-configuration)。
- en: 'In short, AWS-CLI commands follow this syntax:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，AWS-CLI命令遵循以下语法：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Test your setup by running the following:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行以下命令来测试您的设置：
- en: '[PRE7]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'You should see a list of your buckets, folders, and files in your s3 account.
    This is what my output looks like when I list the file and folders in the `aml.packt`
    bucket:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该能看到您s3账户中的所有存储桶、文件夹和文件列表。以下是我列出`aml.packt`存储桶中的文件和文件夹时的输出：
- en: '![](img/B05028_06_07.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05028_06_07.png)'
- en: We will explore in detail how to run your Amazon ML projects using CLI in [Chapter
    7](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml), C*ommand Line and SDK*.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第7章[efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml]，C*ommand Line and SDK*中详细探讨如何使用CLI运行您的Amazon
    ML项目。
- en: Python SDK
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Python SDK
- en: 'We will not use the AWS-CLI any further in this chapter, but instead switch
    to the Python SDK. We needed to setup the credentials for the AWS CLI in order
    for our SDKs scripts to be able to access our AWS account. To use the Python SDK,
    we need to install the `Boto3` library, which comes bundled in the Anaconda distribution.
    If you use Anaconda as your Python environment, you should already have the `boto3`
    package installed. If not, you can install it using `pip` with the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将不再使用AWS-CLI，而是切换到Python SDK。我们需要为AWS CLI设置凭证，以便我们的SDK脚本能够访问我们的AWS账户。要使用Python
    SDK，我们需要安装`Boto3`库，该库包含在Anaconda发行版中。如果您使用Anaconda作为Python环境，您应该已经安装了`boto3`包。如果没有，您可以使用以下命令使用`pip`安装它：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '**Boto3** will use the credentials we configured for the AWS CLI. There’s no
    need for a specific setup. Boto3 is available for most AWS services. The full
    documentation is available at [https://boto3.readthedocs.io/](https://boto3.readthedocs.io/).
    Our minimal use of `Boto3` only requires to specify the service we want, Machine
    Learning, and then use the `predict()` method to send the proper data to the model.
    In return, we obtain the predictions we wanted. The following Python code initiates
    a client to access the machine learning service.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**Boto3**将使用我们为AWS CLI配置的凭证。无需进行特定设置。Boto3适用于大多数AWS服务。完整文档可在[https://boto3.readthedocs.io/](https://boto3.readthedocs.io/)找到。我们最小化使用`Boto3`，只需指定我们想要使用的服务，即机器学习，然后使用`predict()`方法将适当的数据发送到模型。作为回报，我们获得所需的预测。以下Python代码初始化一个客户端以访问机器学习服务。'
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `predict()` method requires the following parameters:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`方法需要以下参数：'
- en: '`MLModelId`: The ID of the model your want to use to predict'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MLModelId`: 您想要用于预测的模型的ID'
- en: '`PredictEndpoint`*:* The URL of the Amazon ML endpoint for your model'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PredictEndpoint`*:* 您模型对应的Amazon ML端点URL'
- en: '`Record`*:* A JSON-formatted version of the sample'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Record`*:* 样本的JSON格式版本'
- en: The `MLModelId` and `PredictEndpoint` URL can be obtained from the model summary
    page. The `Record` is a JSON-formatted string. We will simulate a streaming application
    by opening a held-out set of samples, looping through each sample and sending
    it via the `predict()` method.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '`MLModelId`和`PredictEndpoint` URL可以从模型摘要页面获取。`Record`是一个JSON格式的字符串。我们将通过打开保留的样本集，循环遍历每个样本，并通过`predict()`方法发送它来模拟一个流式应用程序。'
- en: 'We have split the initial dataset into a training set of 4,400 samples and
    a held-out set of 1,174 samples. These subsets are available at the GitHub repository. We
    create a datasource for the training subset, and create a model and its evaluation
    with default settings (mild L2 regularization). We keep the inferred schema (binary
    and text), the suggested recipe (no transformation besides tokenization of the
    text variable), and use the default model parameters (10 passes and mild L2 regularization).
    The training dataset is further split by Amazon ML into a smaller training dataset
    and a validation dataset, respectively 70% and 30% of the initial *4,400* samples.
    The AUC score obtained on the validation set is very high at *0.98*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将初始数据集分为一个包含4,400个样本的训练集和一个包含1,174个样本的保留集。这些子集可在GitHub仓库中找到。我们为训练子集创建一个数据源，并使用默认设置（轻微的L2正则化）创建一个模型及其评估。我们保留推断的架构（二进制和文本），建议的配方（除了对文本变量的标记化之外没有转换），并使用默认模型参数（10次遍历和轻微的L2正则化）。训练数据集进一步由Amazon
    ML分割为较小的训练数据集和验证数据集，分别为初始*4,400*样本的70%和30%。在验证集上获得的AUC分数非常高，为*0.98*：
- en: '![](img/B05028_06_08.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_08.png)'
- en: 'To get the `ModelID` and the `endpoint` URL, go to your summary page for the
    model. Copy the `ModelID` from the top of the page. Then scroll down to get to
    the prediction section and click on the Create endpoint button:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取`ModelID`和`endpoint` URL，请访问模型的摘要页面。从页面顶部复制`ModelID`。然后向下滚动到预测部分，点击创建`endpoint`按钮：
- en: '![](img/B05028_06_09.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_09.png)'
- en: At that point, you will be given an estimate of the real-time prediction pricing
    for your model and asked to confirm the creation of the endpoint.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，您将获得模型实时预测定价的估计，并要求确认创建`endpoint`。
- en: The size of your model is 502.1 KB. You will incur the reserved capacity charge
    of $0.001 for every hour your endpoint is active. The prediction charge for real-time
    predictions is $0.0001 per prediction, rounded up to the nearest penny**.**
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 您的模型大小为502.1 KB。每当您的`endpoint`处于活动状态时，您将产生0.001美元的预留容量费用。实时预测的预测费用为每次预测0.0001美元，向上取整到最接近的美分**。**
- en: 'After a few minutes, the endpoint will be created and you will have the endpoint
    URL:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，`endpoint`将被创建，您将获得`endpoint` URL：
- en: '![](img/B05028_06_10.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_10.png)'
- en: Now that we know the endpoint URL, we can write a simple Python code that sends
    an SMS message, a simple text, to our prediction model and see whether this message is
    predicted to be spam or ham. We send the text `Hello world, my name is Alex` to
    be classified as ham, while the text `Call now to get dating contacts for free,
    no cash no credit card` should probably be detected as spam due to the presence
    of the words *free*, *cash*, *dating*, and so forth.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了`endpoint` URL，我们可以编写一个简单的Python代码，将短信消息、简单的文本发送到我们的预测模型，看看这条消息是否被预测为垃圾邮件或正常邮件。我们将文本`Hello
    world, my name is Alex`发送以分类为正常邮件，而文本`Call now to get dating contacts for free,
    no cash no credit card`可能因为包含`free`、`cash`、`dating`等词语而被检测为垃圾邮件。
- en: 'The initialization/declaration part of the code is as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 代码的初始化/声明部分如下：
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We now use the `predict()` function of the machine learning service SDK:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用机器学习服务SDK的`predict()`函数：
- en: '[PRE11]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, pretty print the response:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，格式化打印响应：
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This returns the following JSON-formatted string:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这返回以下JSON格式的字符串：
- en: '[PRE13]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The JSON response is composed of two parts: the first part is related to the
    request itself, `ResponseMetadata`, and the second is related to the `Prediction`.
    The `HTTPStatusCode` in the `*ResponseMetadata*` part tells us that our query was
    successful `("HTTPStatusCode": 200)`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'JSON响应由两部分组成：第一部分与请求本身相关，`ResponseMetadata`，第二部分与`Prediction`相关。`ResponseMetadata`部分的`HTTPStatusCode`告诉我们我们的查询是成功的`("HTTPStatusCode":
    200)`。'
- en: The interpretation of the Prediction part is straightforward. The SMS was predicted
    to be spam with a very low probability of 0.12%, hence it was classified as ham,
    which is what we expected for the text `Hello world, my name is Alex`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 预测部分的解释很简单。短信被预测为垃圾邮件，概率极低，为0.12%，因此被分类为正常邮件，这正是我们对文本`Hello world, my name is
    Alex`的预期。
- en: 'We expect the text `Call now to get dating contacts for free, no cash no credit
    card` to be classified as spam, the words `free`, `call`, and `dating` usually
    being strong indicators of spam messages. We get the following in return:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们预计文本`Call now to get dating contacts for free, no cash no credit card`将被分类为垃圾邮件，因为`free`、`call`和`dating`等词语通常是垃圾邮件的强烈指示。我们得到的以下结果：
- en: '[PRE14]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The text is classified as spam, which is what we expected. As far as we can
    tell from these two simple examples, our model seems to be working fine. Once
    we can obtain prediction via API calls on a sample-by-sample basis, it becomes
    feasible to hook an incoming stream of data into the endpoint and obtain real-time
    predictions.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 文本被分类为垃圾邮件，这正是我们预期的。从这两个简单的例子来看，我们的模型似乎运行良好。一旦我们可以通过API调用逐个样本地获取预测，就可以将数据流连接到端点并获取实时预测。
- en: To simulate that pipeline, we can use Python to read a whole file of new samples,
    send each one to the model, and capture the results. Let’s do that with the held-out
    set of Spam samples.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模拟这个流程，我们可以使用Python读取整个新样本文件，将每个样本发送到模型，并捕获结果。让我们用保留的Spam样本集来做这件事。
- en: 'The following Python code reads the file, loads it into a panda dataframe,
    and loops over each row of the dataframe. We use `iterrows()` to loop over each
    row of the dataframe. This method is slower than `itertuples()`, but has better
    code readability. The following code is not optimized:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python代码读取文件，将其加载到pandas数据框中，并遍历数据框中的每一行。我们使用`iterrows()`遍历数据框中的每一行。这种方法比`itertuples()`慢，但代码可读性更好。以下代码没有经过优化：
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The responses from Amazon ML are blazingly fast. Thousands of samples are processed
    in a few seconds. This is an extract of the results we get:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon ML的响应速度极快。数千个样本在几秒钟内被处理。以下是我们的结果摘要：
- en: '![](img/B05028_06_11.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_11.png)'
- en: 'Here, each line is formatted as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每一行格式如下：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Notice that, out of the three SMS detected as spam, only two were actually spam
    SMS. The text "*Money i have won wining number 946 wot do i do next*" was probably
    detected as spam due to the presence of the words "Money" or "wining" but was
    in fact a ham message.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在检测到的三个垃圾短信中，只有两个实际上是垃圾短信。文本“*Money i have won wining number 946 wot do i
    do next*”可能因为包含“Money”或“wining”等词语而被检测为垃圾短信，但实际上是一条正常信息。
- en: 'Overall, across the whole predictions, the probabilities are very close to
    either 0 or 1, indicating that the model is very decisive in its classification.
    No hesitation. The ROC curve for the held-out dataset shows that high level of
    accuracy:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，在整个预测过程中，概率非常接近0或1，这表明模型在分类上非常果断，没有犹豫。保留数据集的ROC曲线显示了高水平的确切性：
- en: '![](img/B05028_06_12.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05028_06_12.png)'
- en: Summary
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we explored the final step in the Amazon ML workflow, the predictions.
    Amazon ML offers several ways to apply your models to new datasets in order to
    make predictions. Batch mode involves submitting all the new data at once to the
    model and returning the actual predictions in a csv file on S3\. Real-time predictions,
    on the other hand, are based on sending samples one by one to an API and getting
    prediction results in return. We looked at how to create an API on the Amazon
    ML platform. We also started using the command line and the Python SDK to interact
    with the Amazon ML service -- something we will explore in more depth in [Chapter
    7](efe6f699-a4eb-4c88-8e81-1408d6c3c5c4.xhtml), *Command Line and SDK*.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了Amazon ML工作流程的最后一步，即预测。Amazon ML提供了几种方法将您的模型应用于新的数据集以进行预测。批处理模式涉及一次性将所有新数据提交给模型，并在S3上返回实际的预测csv文件。另一方面，实时预测是基于逐个发送样本到API并获取预测结果。我们探讨了如何在Amazon
    ML平台上创建API。我们还开始使用命令行和Python SDK与Amazon ML服务交互——我们将在第7章“命令行和SDK”中更深入地探讨这一点。
- en: As explained in the previous chapters, the Amazon ML service is built around
    the Stochastic Gradient Descent (SGD) algorithm. This algorithm has been around
    for many years and is used in many different domains and applications, from signal
    processing and adaptive filtering to predictive analysis or deep learning.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，Amazon ML服务是围绕随机梯度下降（SGD）算法构建的。这个算法已经存在很多年了，被用于许多不同的领域和应用，从信号处理和自适应滤波到预测分析或深度学习。
- en: In the next chapter, we will present the algorithm and some if its versions,
    and bring to light its behavior when dealing with different types of data and
    prediction problems. We will explain why quantile binning of numeric values, which
    is often frowned upon, is such a performance and stability booster in our case.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍算法及其一些版本，并揭示它在处理不同类型的数据和预测问题时的行为。我们将解释为什么在我们的情况下，通常被轻视的数值分位数装箱是性能和稳定性的提升者。
