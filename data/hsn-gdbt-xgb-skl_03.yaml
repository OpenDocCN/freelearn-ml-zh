- en: '*Chapter 2*: Decision Trees in Depth'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第2章*：决策树深入解析'
- en: In this chapter, you will gain proficiency with **decision trees**, the primary
    machine learning algorithm from which XGBoost models are built. You will also
    gain first-hand experience in the science and art of **hyperparameter fine-tuning**.
    Since decision trees are the foundation of XGBoost models, the skills that you
    learn in this chapter are essential to building robust XGBoost models going forward.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将熟练掌握**决策树**，这也是XGBoost模型的核心机器学习算法。你还将亲自体验**超参数调优**的科学和艺术。由于决策树是XGBoost模型的基础，因此你在本章学到的技能对构建强健的XGBoost模型至关重要。
- en: In this chapter, you will build and evaluate **decision tree classifiers** and
    **decision tree regressors**, visualize and analyze decision trees in terms of
    variance and bias, and fine-tune decision tree hyperparameters. In addition, you
    will apply decision trees to a case study that predicts heart disease in patients.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将构建和评估**决策树分类器**和**决策树回归器**，可视化并分析决策树的方差和偏差，并调优决策树的超参数。此外，你还将应用决策树进行一个案例研究，该研究预测患者的心脏病。
- en: 'This chapter covers the following main topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章包含以下主要内容：
- en: Introducing decision trees with XGBoost
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引入XGBoost中的决策树
- en: Exploring decision trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索决策树
- en: Contrasting variance and bias
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对比方差和偏差
- en: Tuning decision tree hyperparameters
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调优决策树超参数
- en: Predicting heart disease – a case study
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测心脏病——案例研究
- en: Introducing decision trees with XGBoost
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入XGBoost中的决策树
- en: XGBoost is an **ensemble method**, meaning that it is composed of different
    machine learning models that combine to work together. The individual models that
    make up the ensemble in XGBoost are called **base learners**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是一种**集成方法**，意味着它由多个机器学习模型组成，这些模型共同协作。构成XGBoost集成的单个模型被称为**基学习器**。
- en: Decision trees, the most commonly used XGBoost base learners, are unique in
    the machine learning landscape. Instead of multiplying column values by numeric
    weights, as in linear regression and logistic regression ([*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022),
    *Machine Learning Landscape*), decision trees split the data by asking questions
    about the columns. In fact, building decision trees is like playing a game of
    20 Questions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是最常用的XGBoost基学习器，在机器学习领域具有独特性。与线性回归和逻辑回归（见[*第1章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)，*机器学习领域*）通过数值权重乘以列值不同，决策树通过提问列的内容来分割数据。实际上，构建决策树就像是在玩“20个问题”游戏。
- en: For instance, a decision tree may have a temperature column, and that column
    could branch into two groups, one with temperatures above 70 degrees, and one
    with temperatures below 70 degrees. The next split could be based on the seasons,
    following one branch if it's summer and another branch otherwise. Now the data
    has been split into four separate groups. This process of splitting data into
    new groups via branching continues until the algorithm reaches a desired level
    of accuracy.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，决策树可能包含一个温度列，该列可以分为两组，一组是温度高于70度，另一组是温度低于70度。接下来的分裂可能基于季节，如果是夏季，则走一条分支，否则走另一条分支。此时，数据已被分为四个独立的组。通过分支将数据分割成新组的过程会持续进行，直到算法达到预期的准确度。
- en: A decision tree can create thousands of branches until it uniquely maps each
    sample to the correct target in the training set. This means that the training
    set can have 100% accuracy. Such a model, however, will not generalize well to
    new data.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树可以创建成千上万的分支，直到它将每个样本唯一地映射到训练集中的正确目标。这意味着训练集可以达到100%的准确度。然而，这样的模型对于新数据的泛化能力较差。
- en: Decision trees are prone to overfitting the data. In other words, decision trees
    can map too closely to the training data, a problem explored later in this chapter
    in terms of variance and bias. Hyperparameter fine-tuning is one solution to prevent
    overfitting. Another solution is to aggregate the predictions of many trees, a
    strategy that **Random Forests** and XGBoost employ.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树容易对数据过拟合。换句话说，决策树可能会过于精确地拟合训练数据，这个问题将在本章后面通过方差和偏差进行探讨。超参数调优是防止过拟合的一种解决方案。另一种解决方案是聚合多棵树的预测，这是**随机森林**和XGBoost采用的策略。
- en: While Random Forests and XGBoost will be the focus of subsequent chapters, we
    now take a deep look inside decision trees.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随机森林和XGBoost将是后续章节的重点，但我们现在深入探讨决策树。
- en: Exploring decision trees
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索决策树
- en: Decision Trees work by splitting the data into *branches*. The branches are
    followed down to *leaves* where predictions are made. Understanding how branches
    and leaves are created is much easier with a practical example. Before going into
    further detail, let's build our first decision tree model.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树通过将数据分割成*分支*来工作。沿着分支向下到达*叶子*，在这里做出预测。理解分支和叶子是如何创建的，通过实际示例要容易得多。在深入了解之前，让我们构建第一个决策树模型。
- en: First decision tree model
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一个决策树模型
- en: 'We start by building a decision tree to predict whether someone makes over
    50K US dollars using the Census dataset from [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022),
    *Machine Learning Landscape*:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过构建一个决策树来预测某人是否年收入超过50K美元，使用的是来自[*第1章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)的《*机器学习领域*》中的人口普查数据集：
- en: 'First, open a new Jupyter Notebook and start with the following imports:'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，打开一个新的Jupyter Notebook，并从以下导入开始：
- en: '[PRE0]'
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, open the file `''census_cleaned.csv''` that has been uploaded for you
    at [https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter02).
    If you downloaded all files for this book from the Packt GitHub page, as recommended
    in the *preface*, you can navigate to [*Chapter 2*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)*,
    Decision Trees in Depth*, after launching Anaconda in the same way that you navigate
    to other chapters. Otherwise, go our GitHub page and clone the files now:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，打开文件`'census_cleaned.csv'`，它已经上传到[https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter02](https://github.com/PacktPublishing/Hands-On-Gradient-Boosting-with-XGBoost-and-Scikit-learn/tree/master/Chapter02)供你使用。如果你按照*前言*的建议从Packt
    GitHub页面下载了本书的所有文件，那么在启动Anaconda后，你可以像浏览其他章节一样浏览到[*第2章*](B15551_02_Final_NM_ePUB.xhtml#_idTextAnchor047)《*决策树深入解析*》。否则，现在就去我们的GitHub页面克隆文件：
- en: '[PRE1]'
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After uploading the data into a DataFrame, declare your predictor and target
    columns, `X` and `y`, as follows:'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据上传到DataFrame后，声明预测变量和目标列`X`和`y`，如下所示：
- en: '[PRE2]'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, import `train_test_split` to split the data into training and tests set
    with `random_state=2` to ensure consistent results:'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，导入`train_test_split`，以`random_state=2`的方式将数据分割为训练集和测试集，以确保结果一致：
- en: '[PRE3]'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As with other machine learning classifiers, when using decision trees, we initialize
    the model, fit it on the training set, and test it using `accuracy_score`.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与其他机器学习分类器一样，在使用决策树时，我们初始化模型，使用训练集进行训练，并使用`accuracy_score`进行测试。
- en: The `accuracy_score` determines the number of correct predictions divided by
    the total number of predictions. If 19 of 20 predictions are correct, the `accuracy_score`
    is 95%.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '`accuracy_score`确定的是正确预测的数量与总预测数量之比。如果20次预测中有19次正确，那么`accuracy_score`为95%。'
- en: 'First, import the `DecisionTreeClassifier` and `accuracy_score`:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入`DecisionTreeClassifier`和`accuracy_score`：
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, we build a decision tree classifier with the standard steps:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们按照标准步骤构建一个决策树分类器：
- en: 'Initialize a machine learning model with `random_state=2` to ensure consistent
    results:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个机器学习模型，设置`random_state=2`以确保结果一致：
- en: '[PRE5]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Fit the model on the training set:'
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集上拟合模型：
- en: '[PRE6]'
  id: totrans-36
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Make predictions for the test set:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对测试集进行预测：
- en: '[PRE7]'
  id: totrans-38
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Compare predictions with the test set:'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测结果与测试集进行比较：
- en: '[PRE8]'
  id: totrans-40
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `accuracy_score` is as follows:'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`accuracy_score`如下：'
- en: '[PRE9]'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: An accuracy of 81% is comparable to the accuracy of Logistic Regression from
    the same dataset in [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)*,
    Machine Learning Landscape*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 81%的准确率与同一数据集中的逻辑回归模型在[*第1章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)中的准确率相当，*机器学习领域*。
- en: Now that you have seen how to build a decision tree, let's take a look inside.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到如何构建决策树，接下来让我们深入看看决策树内部。
- en: Inside a decision tree
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树内部
- en: Decision Trees come with nice visuals that reveal their inner workings.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树具有很好的可视化效果，可以揭示其内部工作原理。
- en: 'Here is a decision tree from the Census dataset with only two splits:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个来自人口普查数据集的决策树，只有两个分裂：
- en: '![Figure 2.1 – Census dataset decision tree](img/B15551_02_01.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图2.1 – 人口普查数据集决策树](img/B15551_02_01.jpg)'
- en: Figure 2.1 – Census dataset decision tree
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 – 人口普查数据集决策树
- en: The top of the tree is the root, the **True**/**False** arrows are branches,
    and the data points are nodes. At the end of the tree, the nodes are classified
    as leaves. Let's study the preceding diagram in depth.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 树的顶部是根节点，**True**/**False**箭头是分支，数据点是节点。在树的末端，节点被归类为叶子节点。让我们深入研究前面的图示。
- en: Root
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根节点
- en: The root of the tree is at the top. The first line reads **marital-status_Married-civ-spouse
    <=5**. **marital-status** is a binary column, so all values are **0** (negative)
    or **1** (positive). The first split is based on whether someone is married or
    not. The left side of the tree is the **True** branch, meaning the user is unmarried,
    and the right side is the **False** branch, meaning the user is married.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 树的根节点位于顶部。第一行显示 **marital-status_Married-civ-spouse <=5**。**marital-status**
    是一个二值列，因此所有值要么是 **0**（负类），要么是 **1**（正类）。第一次分割是基于一个人是否已婚。树的左侧是**True**分支，表示用户未婚，右侧是**False**分支，表示用户已婚。
- en: Gini criterion
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Gini 系数
- en: The second line of the root reads `Gini` index of 0 means 0 errors. A gini index
    of 1 means all errors. A gini index of 0.5, which shows an equal distribution
    of elements, means the predictions are no better than random guessing. The closer
    to 0, the lower the error. At the root, a gini of 0.364 means the training set
    is imbalanced with 36.4 percent of class 1.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点的第二行显示：`Gini` 系数为 0 意味着没有错误。Gini 系数为 1 意味着所有的预测都是错误的。Gini 系数为 0.5 表示元素均匀分布，这意味着预测效果不比随机猜测好。越接近
    0，错误率越低。在根节点处，Gini 系数为 0.364，意味着训练集不平衡，类别 1 占比 36.4%。
- en: 'The equation for the gini index is as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Gini 系数的公式如下：
- en: '![Figure 2.2 – gini index equation](img/Formula_02_001.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.2 – Gini 系数公式](img/Formula_02_001.jpg)'
- en: Figure 2.2 – gini index equation
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 – Gini 系数公式
- en: '![](img/Formula_02_002.png)is the probability that the split results in the
    correct value, and c is the total number of classes: 2 in the preceding example.
    Another way of looking at this is that ![](img/Formula_02_003.png) is the fraction
    of items in the set with the correct output label.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '![](img/Formula_02_002.png) 是分割结果为正确值的概率，c 是类别的总数：在前面的例子中是 2。另一种解释是，![](img/Formula_02_003.png)
    是集合中具有正确输出标签的项的比例。'
- en: Samples, values, class
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 样本、值、类别
- en: The root of the tree states that there are 24,420 samples. This is the total
    number of samples in the training set. The following line reads **[18575 , 5845]**.
    The ordering is 0 then 1, so 18,575 samples have a value of 0 (they make less
    than 50K) and 5,845 have a value of 1 (they make more than 50K).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 树的根节点显示总共有 24,420 个样本。这是训练集中的样本总数。接下来的行显示 **[18575 , 5845]**。排序为 0 和 1，因此 18,575
    个样本的值为 0（收入少于 50K），而 5,845 个样本的值为 1（收入超过 50K）。
- en: True/false nodes
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: True/False 节点
- en: Following the first branch, you see **True** on the left side, and **False**
    on the right. The pattern of True – left and False – right continues throughout
    the tree.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 跟随第一条分支，你会看到左侧是**True**，右侧是**False**。True 在左，False 在右的模式贯穿整个树。
- en: In the left node in the second row, the split **capital_gain <= 7073.5** is
    applied to subsequent nodes. The remaining information comes from the split above
    the previous branch. Of the 13,160 unmarried people, 12,311 have an income of
    less than 50K, while 849 have an income of more than 50K. The gini index, **0.121**,
    is a very good score.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二行的左侧节点中，分割条件 **capital_gain <= 7073.5** 被应用到后续节点。其余的信息来自上一个分支的分割。对于 13,160
    个未婚的人，12,311 人的收入低于 50K，而 849 人的收入高于 50K。此时的 Gini 系数 **0.121**，是一个非常好的得分。
- en: Stumps
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 树桩
- en: It's possible to have a tree with only one split. Such a tree is called a **stump**.
    Although stumps are not powerful predictors in themselves, stumps can become powerful
    when used as boosters, as covered in [*Chapter 4*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)*,
    From Gradient Boosting to XGBoost*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 一棵树有可能只有一个分割，这样的树叫做**树桩**。虽然树桩本身不是强大的预测器，但当用作提升器时，树桩可以变得非常强大，这在[*第 4 章*](B15551_04_Final_NM_ePUB.xhtml#_idTextAnchor093)《从梯度提升到
    XGBoost》中有详细讨论。
- en: Leaves
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 叶节点
- en: The nodes at the end of the tree are leaves. The leaves contain all final predictions.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 树的末端节点是叶节点，叶节点包含所有最终的预测结果。
- en: The far-left leaf has a gini index of **0.093**, correctly predicting 12,304
    of 12,938 cases, which is 95%. We are 95% confident that unmarried users with
    capital gains of less than 7,073.50 do not make more than 50K.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最左侧的叶节点的 Gini 系数为 **0.093**，正确预测了 12,304 个中的 12,938 个案例，准确率为 95%。我们有 95% 的信心认为，资本收益低于
    7,073.50 的未婚用户年收入不超过 50K。
- en: Other leaves may be interpreted similarly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其他叶节点可以类似地解释。
- en: Now let's see where these predictions go wrong.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看这些预测在哪些地方出错。
- en: Contrasting variance and bias
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对比方差和偏差
- en: Imagine that you have the data points displayed in the following graph. Your
    task is to fit a line or curve that will allow you to make predictions for new
    points.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有如下图所示的数据点。你的任务是拟合一条直线或曲线，以便可以对新的数据点进行预测。
- en: 'Here is a graph of random points:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个随机点的图：
- en: '![Figure 2.3 – Graph of random points](img/B15551_02_03.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.3 – 随机点图](img/B15551_02_03.jpg)'
- en: Figure 2.3 – Graph of random points
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 – 随机点图
- en: 'One idea is to use Linear Regression, which minimizes the square of the distance
    between each point and the line, as shown in the following graph:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一种方法是使用线性回归，通过最小化每个点与直线之间的距离平方，来拟合数据，如下图所示：
- en: '![Figure 2.4 – Minimizing distance using Linear Regression](img/B15551_02_04.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.4 – 使用线性回归最小化距离](img/B15551_02_04.jpg)'
- en: Figure 2.4 – Minimizing distance using Linear Regression
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 – 使用线性回归最小化距离
- en: A straight line generally has high **bias**. In machine learning bias is a mathematical
    term that comes from estimating the error when applying the model to a real-life
    problem. The bias of the straight line is high because the predictions are restricted
    to the line and fail to account for changes in the data.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 直线通常具有较高的**偏差**。在机器学习中，偏差是一个数学术语，表示在将模型应用于现实问题时对误差的估计。直线的偏差较高，因为预测结果仅限于直线，未能考虑数据的变化。
- en: In many cases, a straight line is not complex enough to make accurate predictions.
    When this happens, we say that the machine learning model has underfit the data
    with high bias.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，直线的复杂度不足以进行准确的预测。当发生这种情况时，我们说机器学习模型存在高偏差的欠拟合。
- en: 'A second option is to fit the points with an eight-degree polynomial. Since
    there are only nine points, an eight-degree polynomial will fit the data perfectly,
    as you can see in the following graph:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项是使用八次多项式拟合这些点。由于只有九个点，八次多项式将完美拟合这些数据，正如你在以下图形中看到的那样：
- en: '![Figure 2.5 – Eight-degree poynomial](img/B15551_02_05.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.5 – 八次多项式](img/B15551_02_05.jpg)'
- en: Figure 2.5 – Eight-degree poynomial
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.5 – 八次多项式
- en: This model has high **variance**. In machine learning, variance is a mathematical
    term indicating how much a model will change given a different set of training
    data. Formally, variance is the measure of the squared deviation between a random
    variable and its mean. Given nine different data points in the training set, the
    eighth-degree polynomial will be completely different, resulting in high variance.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型具有较高的**方差**。在机器学习中，方差是一个数学术语，表示给定不同的训练数据集时，模型变化的程度。严格来说，方差是随机变量与其均值之间的平方偏差的度量。由于训练集中有九个不同的数据点，八次多项式将会完全不同，从而导致高方差。
- en: Models with high variance often overfit the data. These models do not generalize
    well to new data points because they have fit the training data too closely.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 高方差的模型通常会过拟合数据。这些模型无法很好地推广到新的数据点，因为它们过于贴合训练数据。
- en: In the world of big data, overfitting is a big problem. More data results in
    larger training sets, and machine learning models like decision trees fit the
    training data too well.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在大数据的世界里，过拟合是一个大问题。更多的数据导致更大的训练集，而像决策树这样的机器学习模型会过度拟合训练数据。
- en: 'As a final option, consider a third-degree polynomial that fits the data points
    as shown in the following graph:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最终选项，考虑使用三次多项式拟合数据点，如下图所示：
- en: '![Figure 2.6 – Third-degree polynomial](img/B15551_02_06.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.6 – 三次多项式](img/B15551_02_06.jpg)'
- en: Figure 2.6 – Third-degree polynomial
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.6 – 三次多项式
- en: This third-degree polynomial provides a nice balance between variance and bias,
    following the curve generally, yet adapting to the variation. Low variance means
    that a different training set will not result in a curve that differs by a significant
    amount. Low bias indicates that the error when applying this model to a real-world
    situation will not be too high. In machine learning, the combination of low variance
    and low bias is ideal.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这个三次多项式在方差和偏差之间提供了良好的平衡，一般跟随曲线，但又能适应变化。低方差意味着不同的训练集不会导致曲线发生显著变化。低偏差表示在将该模型应用于实际问题时，误差不会太高。在机器学习中，低方差和低偏差的结合是理想的。
- en: One of the best machine learning strategies to strike a nice balance between
    variance and bias is to fine-tune hyperparameters.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 达到方差和偏差之间良好平衡的最佳机器学习策略之一是微调超参数。
- en: Tuning decision tree hyperparameters
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整决策树的超参数
- en: Hyperparameters are not the same as parameters.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数与参数不同。
- en: In machine learning, parameters are adjusted when the model is being tuned.
    The weights in linear and Logistic Regression, for example, are parameters adjusted
    during the build phase to minimize errors. Hyperparameters, by contrast, are chosen
    in advance of the build phase. If no hyperparameters are selected, default values
    are used.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，参数是在模型调整过程中进行调整的。例如，线性回归和逻辑回归中的权重就是在构建阶段调整的参数，目的是最小化误差。相比之下，超参数是在构建阶段之前选择的。如果没有选择超参数，则使用默认值。
- en: Decision Tree regressor
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树回归器
- en: The best way to learn about hyperparameters is through experimentation. Although
    there are theories behind the range of hyperparameters chosen, results trump theory.
    Different datasets see improvements with different hyperparameter values.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 学习超参数的最佳方式是通过实验。虽然选择超参数范围背后有理论依据，但结果胜过理论。不同的数据集在不同的超参数值下会有不同的改进效果。
- en: 'Before selecting hyperparameters, let''s start by finding a baseline score
    using a `DecisionTreeRegressor` and `cross_val_score` with the following steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择超参数之前，让我们先使用`DecisionTreeRegressor`和`cross_val_score`找到一个基准分数，步骤如下：
- en: 'Download the `''bike_rentals_cleaned''` dataset and split it into `X_bikes`
    (predictor columns) and `y_bikes` (training columns):'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载`'bike_rentals_cleaned'`数据集并将其拆分为`X_bikes`（预测变量）和`y_bikes`（训练数据）：
- en: '[PRE10]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Import the `DecisionTreeRegressor` and `cross_val_score`:'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`DecisionTreeRegressor`和`cross_val_score`：
- en: '[PRE11]'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Initialize `DecisionTreeRegressor` and fit the model in `cross_val_score`:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化`DecisionTreeRegressor`并在`cross_val_score`中拟合模型：
- en: '[PRE12]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Compute the `1233.36`. This is worse than the `972.06` obtained from Linear
    Regression in [*Chapter 1*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)*,
    Machine Learning Landscape*, and from the `887.31` obtained by XGBoost.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算`1233.36`。这比在[*第一章*](B15551_01_Final_NM_ePUB.xhtml#_idTextAnchor022)*机器学习概况*中通过线性回归得到的`972.06`以及通过XGBoost得到的`887.31`还要差。
- en: Is the model overfitting the data because the variance is too high?
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 模型是否因为方差太高而导致过拟合？
- en: 'This question may be answered by seeing how well the decision tree makes predictions
    on the training set alone. The following code checks the error of the training
    set, before it makes predictions on the test set:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题可以通过查看决策树在训练集上的预测效果来回答。以下代码检查训练集的误差，然后再对测试集进行预测：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The result is as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: A RMSE of `0.0` means that the model has perfectly fit every data point! This
    perfect score combined with a cross-validation error of `1233.36` is proof that
    the decision tree is overfitting the data with high variance. The training set
    fit perfectly, but the test set missed badly.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一个`0.0`的RMSE意味着模型已经完美拟合了每一个数据点！这个完美的分数，再加上`1233.36`的交叉验证误差，证明决策树在数据上出现了过拟合，且方差很高。训练集完美拟合，但测试集表现很差。
- en: Hyperparameters may rectify the situation.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数可能会解决这个问题。
- en: Hyperparameters in general
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一般而言的超参数
- en: Hyperparameter details for all scikit-learn models may be viewed on scikit-learn's
    official documentation pages.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所有scikit-learn模型的超参数详细信息可以在scikit-learn的官方文档页面查看。
- en: Here is an excerpt from the DecisionTreeRegressor website ([https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自DecisionTreeRegressor网站的摘录（[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.html)）。
- en: Note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*sklearn* is short for *scikit-learn*.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*sklearn*是*scikit-learn*的缩写。'
- en: '![Figure 2.7\. Excerpt of DecisionTreeRegressor official documentation page](img/B15551_02_07.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.7\. DecisionTreeRegressor官方文档页面摘录](img/B15551_02_07.jpg)'
- en: Figure 2.7\. Excerpt of DecisionTreeRegressor official documentation page
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.7\. DecisionTreeRegressor官方文档页面摘录
- en: The official documentation explains the meaning behind the hyperparameters.
    Note that `Parameters` here is short for *hyperparameters*. When working on your
    own, checking the official documentation is your most reliable resource.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 官方文档解释了超参数背后的含义。请注意，这里的`Parameters`是*超参数*的缩写。在自己工作时，查看官方文档是最可靠的资源。
- en: Let's go over the hyperparameters one at a time.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐个讨论超参数。
- en: max_depth
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: max_depth
- en: '`max_depth` defines the depth of the tree, determined by the number of times
    splits are made. By default, there is no limit to `max_depth`, so there may be
    hundreds or thousands of splits that result in overfitting. By limiting `max_depth` to
    smaller numbers, variance is reduced, and the model generalizes better to new
    data.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth` 定义了树的深度，取决于分割的次数。默认情况下，`max_depth` 没有限制，这可能会导致成百上千次的分割，进而引起过拟合。通过限制
    `max_depth` 为较小的值，可以减少方差，使模型在新数据上具有更好的泛化能力。'
- en: How can you choose the best number for `max_depth`?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如何选择 `max_depth` 的最佳值？
- en: You can always try `max_depth=1`, then `max_depth=2`, then `max_depth=3`, and
    so on, but this process would be exhausting. Instead, you may use a wonderful
    tool called `GridSearchCV`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您总是可以尝试 `max_depth=1`，然后是 `max_depth=2`，然后是 `max_depth=3`，依此类推，但这个过程会很累人。相反，您可以使用一个叫做
    `GridSearchCV` 的神奇工具。
- en: GridSearchCV
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GridSearchCV
- en: '`GridSearchCV` searches a grid of hyperparameters using cross-validation to
    deliver the best results.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV` 使用交叉验证搜索超参数网格，以提供最佳结果。'
- en: '`GridSearchCV` functions as any machine learning algorithm, meaning that it''s
    fit on a training set, and scored on a test set. The primary difference is that
    `GridSearchCV` checks all hyperparameters before finalizing a model.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV` 像任何机器学习算法一样工作，这意味着它在训练集上进行拟合，并在测试集上进行评分。主要的不同点是 `GridSearchCV`
    在最终确定模型之前会检查所有超参数。'
- en: The key with `GridSearchCV` is to establish a dictionary of hyperparameter values.
    There is no correct set of values to try. One strategy is to select a smallest
    and largest value with evenly spaced numbers in between. Since we are trying to
    reduce overfitting, the general idea is to try more values on the lower side for
    `max_depth`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV` 的关键是建立一个超参数值的字典。没有一个正确的值集可以尝试。一种策略是选择一个最小值和一个最大值，并在两者之间均匀分布数字。由于我们试图减少过拟合，通常的做法是为
    `max_depth` 在较低端尝试更多的值。'
- en: 'Import `GridSearchCV` and define a list of hyperparameters for `max_depth`
    as follows:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 导入 `GridSearchCV` 并按如下方式定义 `max_depth` 的超参数列表：
- en: '[PRE15]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The `params` dictionary contains one key, `'max_depth'`, written as a string,
    and one value, a list of numbers that we have chosen. Note that `None` is the
    default, meaning that there is no limit to `max_depth`.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '`params` 字典包含一个键 `''max_depth''`，它是一个字符串，以及一个我们选择的数字列表。请注意，`None` 是默认值，这意味着
    `max_depth` 没有限制。'
- en: Tip
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Generally speaking, decreasing max hyperparameters and increasing min hyperparameters
    will reduce variation and prevent overfitting.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，减少最大超参数并增加最小超参数将减少变化并防止过拟合。
- en: 'Next, initialize a `DecisionTreeRegressor`, and place it inside of `GridSearchCV`
    along with `params` and the scoring metric:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，初始化一个 `DecisionTreeRegressor`，并将其放入 `GridSearchCV` 中，与 `params` 和评分标准一起使用：
- en: '[PRE16]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that `GridSearchCV` has been fit on the data, you can view the best hyperparameters
    as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`GridSearchCV` 已经在数据上进行了拟合，您可以按如下方式查看最佳超参数：
- en: '[PRE17]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The result is as follows:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE18]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As you can see, a `max_depth` value of `6` resulted in the best cross-validation
    score in the training set.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`max_depth` 值为 `6` 在训练集上获得了最佳交叉验证分数。
- en: 'The training score may be displayed using the `best_score` attribute:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 训练分数可以通过 `best_score` 属性显示：
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The score is as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE20]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The test score may be displayed as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 测试分数可以如下显示：
- en: '[PRE21]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The score is as follows:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE22]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Variance has been substantially reduced.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 方差已经大大减少。
- en: min_samples_leaf
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: min_samples_leaf
- en: '`min_samples_leaf` provides a restriction by increasing the number of samples
    that a leaf may have. As with `max_depth`, `min_samples_leaf` is designed to reduce
    overfitting.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_samples_leaf` 通过增加叶子节点可能包含的样本数量来提供限制。与 `max_depth` 类似，`min_samples_leaf`
    旨在减少过拟合。'
- en: When there are no restrictions, `min_samples_leaf=1` is the default, meaning
    that leaves may consist of unique samples (prone to overfitting). Increasing `min_samples_leaf` reduces
    variance. If `min_samples_leaf=8`, all leaves must contain eight or more samples.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当没有限制时，`min_samples_leaf=1` 是默认值，这意味着叶子节点可能包含唯一的样本（容易导致过拟合）。增加 `min_samples_leaf`
    可以减少方差。如果 `min_samples_leaf=8`，那么所有叶子节点必须包含至少八个样本。
- en: 'Testing a range of values for `min_samples_leaf` requires going through the
    same process as before. Instead of copying and pasting, we write a function that
    displays the best parameters, training score, and test score using `GridSearchCV`
    with `DecisionTreeRegressor(random_state=2)` assigned to `reg` as a default parameter:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 测试 `min_samples_leaf` 的一系列值需要经过与之前相同的过程。我们写一个函数，使用 `GridSearchCV` 和 `DecisionTreeRegressor(random_state=2)`
    作为默认参数 `reg` 来显示最佳参数、训练分数和测试分数，而不是复制和粘贴：
- en: '[PRE23]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Tip
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When writing your own functions, it's advantageous to include default keyword
    arguments. A default keyword argument is a named parameter with a default value
    that may be changed for later use and testing. Default keyword arguments greatly
    enhance the capabilities of Python.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写自己的函数时，包含默认关键字参数是有利的。默认关键字参数是一个带有默认值的命名参数，可以在后续使用和测试时修改。默认关键字参数大大增强了 Python
    的功能。
- en: 'When choosing the range of hyperparameters, it''s helpful to know the size
    of the training set on which the model is built. Pandas comes with a nice method, `.shape`,
    that returns the rows and columns of the data:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择超参数的范围时，了解构建模型的训练集大小是非常有帮助的。Pandas 提供了一个很好的方法，`.shape`，它返回数据的行和列：
- en: '[PRE24]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The rows and columns of data are as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的行和列如下：
- en: '[PRE25]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Since the training set has `548` rows, this helps determine reasonable values
    for `min_samples_leaf`. Let''s try `[1, 2, 4, 6, 8, 10, 20, 30]` as the input
    of our `grid_search`:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 由于训练集有 `548` 行数据，这有助于确定 `min_samples_leaf` 的合理值。我们试试 `[1, 2, 4, 6, 8, 10, 20,
    30]` 作为 `grid_search` 的输入：
- en: '[PRE26]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The score is as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE27]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Since the test score is better than the training score, variance has been reduced.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 由于测试分数高于训练分数，因此方差已减少。
- en: 'What happens when we put `min_samples_leaf` and `max_depth` together? Let''s
    see:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 `min_samples_leaf` 和 `max_depth` 放在一起时，会发生什么呢？我们来看一下：
- en: '[PRE28]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The score is as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE29]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The result may be a surprise. Even though the training score has improved, the
    test score has not. `min_samples_leaf` has decreased from `8` to `2`, while `max_depth` has
    remained the same.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能会让人惊讶。尽管训练分数提高了，但测试分数却没有变化。`min_samples_leaf` 从 `8` 降低到 `2`，而 `max_depth`
    保持不变。
- en: Tip
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: This is a valuable lesson in hyperparameter tuning: Hyperparameters should not
    be chosen in isolation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有关超参数调优的重要经验：超参数不应单独选择。
- en: 'As for reducing variance in the preceding example, limiting `min_samples_leaf`
    to values greater than three may help:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 至于减少前面示例中的方差，将 `min_samples_leaf` 限制为大于三的值可能会有所帮助：
- en: '[PRE30]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The score is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 分数如下：
- en: '[PRE31]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, the test score has improved.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，测试分数已提高。
- en: We will now explore the remaining decision tree hyperparameters without individual
    testing.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探索其余的决策树超参数，而无需单独测试。
- en: max_leaf_nodes
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: max_leaf_nodes
- en: '`max_leaf_nodes` is similar to `min_samples_leaf`. Instead of specifying the
    number of samples per leaf, it specifies the total number of leaves. So, `max_leaf_nodes=10`
    means that the model cannot have more than 10 leaves. It could have fewer.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_leaf_nodes` 类似于 `min_samples_leaf`。不同之处在于，它指定了叶子的总数，而不是每个叶子的样本数。因此，`max_leaf_nodes=10`
    意味着模型不能有超过 10 个叶子，但可以少于 10 个。'
- en: max_features
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: max_features
- en: '`max_features` is an effective hyperparameter for reducing variance. Instead
    of considering every possible feature for a split, it chooses from a select number
    of features each round.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features` 是减少方差的有效超参数。它并不是考虑所有可能的特征进行分裂，而是每次选择一部分特征进行分裂。'
- en: 'It''s standard to see `max_features` with the following options:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_features` 常见的选项如下：'
- en: '`''auto''` is the default, which provides no limitations.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''auto''` 是默认选项，没有任何限制。'
- en: '`''sqrt''` is the square root of the total number of features.'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''sqrt''` 是特征总数的平方根。'
- en: '`''log2''` is the log of the total number of features in base 2\. 32 columns
    resolves to 5 since 2 ^5 = 32.'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''log2''` 是特征总数的以 2 为底的对数。32 列特征对应 5，因为 2^5 = 32。'
- en: min_samples_split
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: min_samples_split
- en: Another splitting technique is `min_samples_split`. As the name indicates, `min_samples_split` provides
    a limit to the number of samples required before a split can be made. The default
    is `2`, since two samples may be split into one sample each, ending as single
    leaves. If the limit is increased to `5`, no further splits are permitted for
    nodes with five samples or fewer.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种分割技术是 `min_samples_split`。顾名思义，`min_samples_split` 为进行分裂之前要求的样本数设定了限制。默认值为
    `2`，因为两个样本可以被分成各自的单个叶子。如果将限制增加到 `5`，那么对于包含五个样本或更少的节点，不再允许进一步的分裂。
- en: splitter
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: splitter
- en: There are two options for `splitter`, `'random'` and `'best'`. Splitter tells
    the model how to select the feature to split each branch. The `'best'` option,
    the default, selects the feature that results in the greatest gain of information.
    The `'random'` option, by contrast, selects the split randomly.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`splitter`有两个选项，`''random''`和`''best''`。Splitter告诉模型如何选择分裂每个分支的特征。默认的`''best''`选项选择能带来最大信息增益的特征。与此相对，`''random''`选项会随机选择分裂。'
- en: Changing `splitter` to `'random'` is a great way to prevent overfitting and
    diversify trees.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 将`splitter`更改为`'random'`是防止过拟合并使树结构多样化的好方法。
- en: criterion
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: criterion
- en: The `criterion` for splitting decision tree regressors and classifiers are different.
    The `criterion` provides the method the machine learning model uses to determine
    how splits should be made. It's the scoring method for splits. For each possible
    split, the `criterion` calculates a number for a possible split and compares it
    to other options. The split with the best score wins.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 用于决策树回归器和分类器分裂的`criterion`是不同的。`criterion`提供了机器学习模型用于决定如何进行分裂的方法。这是分裂的评分方法。对于每个可能的分裂，`criterion`计算一个分裂的数值，并将其与其他选项进行比较。分裂得分最高的选项会被选中。
- en: The options for decision tree regressors are `mse` (mean squared error), `friedman_mse`,
    (which includes Friedman's adjustment), and `mae` (mean absolute error). The default
    is `mse`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树回归器，选项有`mse`（均方误差）、`friedman_mse`（包括Friedman调整）和`mae`（平均绝对误差）。默认值是`mse`。
- en: For classifiers, `gini`, which was described earlier, and `entropy` usually
    give similar results.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类器，之前提到的`gini`和`entropy`通常会给出相似的结果。
- en: min_impurity_decrease
  id: totrans-196
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: min_impurity_decrease
- en: Previously known as `min_impurity_split`, `min_impurity_decrease` results in
    a split when the impurity is greater than or equal to this value.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 以前被称为`min_impurity_split`，`min_impurity_decrease`在不纯度大于或等于该值时会进行分裂。
- en: '*Impurity* is a measure of how pure the predictions are for every node. A tree
    with 100% accuracy would have an impurity of 0.0\. A tree with 80% accuracy would
    have an impurity of 0.20.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*不纯度*是衡量每个节点预测纯度的标准。一个具有100%准确率的树的不纯度为0.0。一个具有80%准确率的树的不纯度为0.20。'
- en: Impurity is an important idea in Decision Trees. Throughout the tree-building
    process, impurity should continually decrease. Splits that result in the greatest
    decrease of impurity are chosen for each node.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 不纯度是决策树中的一个重要概念。在整个树构建过程中，不纯度应不断减少。每个节点选择的分裂是那些能最大程度降低不纯度的分裂。
- en: The default value is `0.0`. This number can be increased so that trees stop
    building when a certain threshold is reached.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值为`0.0`。这个数值可以增大，以便树在达到某个阈值时停止构建。
- en: min_weight_fraction_leaf
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: min_weight_fraction_leaf
- en: '`min_weight_fraction_leaf` is the minimum weighted fraction of the total weights
    required to be a leaf. According to the documentation, *Samples have equal weight
    when sample_weight is not provided*.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '`min_weight_fraction_leaf`是成为叶节点所需的最小权重占总权重的比例。根据文档，*当未提供sample_weight时，样本具有相等的权重*。'
- en: For practical purposes, `min_weight_fraction_leaf` is another hyperparameter
    that reduces variance and prevents overfitting. The default is 0.0\. Assuming
    equal weights, a restriction of 1%, 0.01, would require at least 5 of the 500
    samples to be a leaf.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 从实际应用角度来看，`min_weight_fraction_leaf`是另一个可以减少方差并防止过拟合的超参数。默认值是0.0。假设权重相等，1%的限制，即0.01，将要求至少有500个样本中的5个作为叶节点。
- en: ccp_alpha
  id: totrans-204
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: ccp_alpha
- en: 'The `ccp_alpha` hyperparameter will not be discussed here, as it is designed
    for pruning after the tree has been built. For a full discussion, check out minimal
    cost complexity pruning: [https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`ccp_alpha`超参数在此不进行讨论，因为它是用于树构建后修剪的。欲了解更多内容，请参阅最小成本复杂度修剪：[https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning](https://scikit-learn.org/stable/modules/tree.html#minimal-cost-complexity-pruning)。'
- en: Putting it all together
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'When fine-tuning hyperparameters, several factors come into play:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在微调超参数时，涉及几个因素：
- en: The amount of time allotted
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分配的时间量
- en: The number of hyperparameters
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数的数量
- en: The number of decimal places of accuracy desired
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所需的精度小数位数
- en: The time spent, number of hyperparameters fine-tuned, and accuracy desired depend
    on you, the dataset, and the project at hand. Since hyperparameters are interrelated,
    it's not required to modify them all. Fine-tuning a smaller range may lead to
    better results.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 所花费的时间、调优的超参数数量以及期望的准确度取决于你、数据集和手头的项目。由于超参数相互关联，并不要求修改所有超参数。调整较小范围的超参数可能会带来更好的结果。
- en: Now that you understand the fundamentals of decision trees and decision tree
    hyperparameters, it's time to apply what you have learned.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了决策树和决策树超参数的基本原理，是时候应用你所学到的知识了。
- en: Tip
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: There are too many decision tree hyperparameters to consistently use them all.
    In my experience, `max_depth`, `max_features`, `min_samples_leaf`, `max_leaf_nodes`,
    `min_impurity_decrease`, and `min_samples_split` are often sufficient.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的超参数太多，无法始终如一地使用所有超参数。根据我的经验，`max_depth`、`max_features`、`min_samples_leaf`、`max_leaf_nodes`、`min_impurity_decrease`
    和 `min_samples_split` 通常已经足够。
- en: Predicting heart disease – a case study
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测心脏病——案例研究
- en: You have been asked by a hospital to use machine learning to predict heart disease.
    Your job is to develop a model and highlight two to three important features that
    doctors and nurses can focus on to improve patient health.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 医院要求你使用机器学习来预测心脏病。你的任务是开发一个模型，并突出显示医生和护士可以专注的两个到三个重要特征，以改善患者健康。
- en: You decide to use a decision tree classifier with fine-tuned hyperparameters.
    After the model has been built, you will interpret results using `feature_importances_`,
    an attribute that determines the most important features in predicting heart disease.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定使用经过调优的超参数的决策树分类器。在构建模型后，你将使用`feature_importances_`属性来解释结果，该属性确定了预测心脏病时最重要的特征。
- en: Heart Disease dataset
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 心脏病数据集
- en: The Heart Disease dataset has been uploaded to GitHub as `heart_disease.csv`.
    This is a slight modification to the original Heart Disease dataset ([https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)) provided
    by the UCI Machine Learning Repository ([https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)) with
    null values cleaned up for your convenience.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 心脏病数据集已上传至GitHub，文件名为`heart_disease.csv`。这是对原始心脏病数据集（[https://archive.ics.uci.edu/ml/datasets/Heart+Disease](https://archive.ics.uci.edu/ml/datasets/Heart+Disease)）的轻微修改，由UCI机器学习库（[https://archive.ics.uci.edu/ml/index.php](https://archive.ics.uci.edu/ml/index.php)）提供，并已清理了空值，方便使用。
- en: 'Upload the file and display the first five rows as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 上传文件并显示前五行，如下所示：
- en: '[PRE32]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding code produces the following table:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会生成如下表格：
- en: '![Figure 2.8 – heart_disease.csv output](img/B15551_02_08.jpg)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图2.8 – heart_disease.csv 输出](img/B15551_02_08.jpg)'
- en: Figure 2.8 – heart_disease.csv output
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 – heart_disease.csv 输出
- en: The target column, conveniently labeled '`target`' is binary, with `1` indicating
    that the patient has heart disease and `0` indicating that they do not.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 目标列，便捷地标记为`target`，是二值型的，其中`1`表示患者患有心脏病，`0`表示患者没有心脏病。
- en: 'Here are the meanings of the predictor columns, taken from the data source
    linked previously:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是从之前链接的数据源中提取的预测变量列的含义：
- en: '`age`: Age in years'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`age`：年龄（单位：年）'
- en: '`sex`: Sex (`1` = male; `0` = female)'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sex`：性别（`1` = 男性；`0` = 女性）'
- en: '`cp`: Chest pain type (`1` = typical angina, `2` = atypical angina, `3` = non-anginal
    pain, `4` = asymptomatic)'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cp`：胸痛类型（`1` = 典型心绞痛，`2` = 非典型心绞痛，`3` = 非心绞痛性疼痛，`4` = 无症状）'
- en: '`trestbps`: Resting blood pressure (in mm Hg on admission to the hospital)'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`trestbps`：静息血压（单位：mm Hg，入院时测量）'
- en: '`chol`: Serum cholesterol in mg/dl 6 fbs: (fasting blood sugar > 120 mg/dl)
    (`1` = true; `0` = false)'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`chol`：血清胆固醇（单位：mg/dl） `fbs`：（空腹血糖 > 120 mg/dl）（`1` = 是；`0` = 否）'
- en: '`fbs`: Fasting blood sugar > 120 mg/dl (`1` = true; `0` = false)'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fbs`：空腹血糖 > 120 mg/dl（`1` = 是；`0` = 否）'
- en: '`restecg`: Resting electrocardiographic results (`0` = normal, `1` = having
    ST-T wave abnormality (T wave inversions and/or ST elevation or depression of
    > 0.05 mV), `2` = showing probable or definite left ventricular hypertrophy by
    Estes'' criteria)'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`restecg`：静息心电图结果（`0` = 正常，`1` = 存在ST-T波异常（T波倒置和/或ST段抬高或低于0.05 mV），`2` = 根据Estes标准显示可能或确诊的左心室肥厚）'
- en: '`thalach`: Maximum heart rate achieved'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thalach`：最大心率'
- en: '`exang`: Exercise induced angina (`1` = yes; `0` = no)'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`exang`：运动诱发的心绞痛（`1` = 是；`0` = 否）'
- en: '`oldpeak`: ST depression induced by exercise relative to rest'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`oldpeak`：运动诱发的ST段压低（与静息状态相比）'
- en: '`slope`: The slope of the peak exercise ST segment (`1` = upsloping, `2` =
    flat, `3` = downsloping)'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`slope`：最大运动ST段的坡度（`1` = 上升，`2` = 平坦，`3` = 下降）'
- en: '`ca`: Number of major vessels (0-3) colored by fluoroscopy'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ca`: 主要血管数量（0-3），由透视法显示'
- en: '`thal`: `3` = normal; `6` = fixed defect; `7` = reversible defect'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`thal`: `3` = 正常；`6` = 固定缺陷；`7` = 可逆缺陷'
- en: 'Split the data into training and test sets in preparation for machine learning:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集，为机器学习做准备：
- en: '[PRE33]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: You are now ready to make predictions.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经准备好进行预测了。
- en: Decision Tree classifier
  id: totrans-243
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: Before implementing hyperparameters, it's helpful to have a baseline model for
    comparison.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现超参数之前，拥有一个基准模型进行比较是很有帮助的。
- en: 'Use `cross_val_score` with a `DecisionTreeClassifier` as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`cross_val_score`和`DecisionTreeClassifier`如下：
- en: '[PRE34]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The result is as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE35]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The initial accuracy is 76%. Let's see what gains can be made with hyperparameter
    fine-tuning.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 初始准确率为76%。我们来看看通过超参数微调可以获得多少提升。
- en: RandomizedSearch CLF function
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机搜索分类器函数
- en: When fine-tuning many hyperparameters, `GridSearchCV` can take too much time.
    The scikit-learn library provides `RandomizedSearchCV` as a wonderful alternative.
    `RandomizedSearchCV` works in the same way as `GridSearchCV`, but instead of trying
    all hyperparameters, it tries a random number of combinations. It's not meant
    to be exhaustive. It's meant to find the best combinations in limited time.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 当调整多个超参数时，`GridSearchCV`可能需要过长时间。scikit-learn库提供了`RandomizedSearchCV`作为一个很好的替代方案。`RandomizedSearchCV`与`GridSearchCV`的工作方式相同，但它并不是尝试所有的超参数，而是尝试一个随机数量的组合。它并不是要穷举所有组合，而是要在有限的时间内找到最佳的组合。
- en: 'Here''s a function that uses `RandomizedSearchCV` to return the best model
    along with the scores. The inputs are `params` (a dictionary of hyperparameters
    to test), `runs` (number of hyperparameter combinations to check), and `DecisionTreeClassifier`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个使用`RandomizedSearchCV`的函数，它返回最佳模型及其得分。输入是`params`（待测试的超参数字典）、`runs`（要检查的超参数组合数）和`DecisionTreeClassifier`：
- en: '[PRE36]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Now, let's pick a range of hyperparameters.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们选择一个超参数范围。
- en: Choosing hyperparameters
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择超参数
- en: 'There is no single correct approach for choosing hyperparameters. Experimentation
    is the name of the game. Here is an initial list, placed inside the `randomized_search_clf` function.
    These numbers have been chosen with the aim of reducing variance and trying an
    expansive range:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 选择超参数没有单一的正确方法。实验才是关键。这里是一个初步的列表，放入`randomized_search_clf`函数中。这些数值的选择旨在减少方差，并尝试一个广泛的范围：
- en: '[PRE37]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This is a definite improvement, and the model generalizes well on the test set.
    Let's see if we can do better by narrowing the range.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个显著的改进，且该模型在测试集上具有良好的泛化能力。我们来看看通过缩小范围能否做到更好。
- en: Narrowing the range
  id: totrans-259
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 缩小范围
- en: Narrowing the range is one strategy to improve hyperparameters.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 缩小范围是改善超参数的一种策略。
- en: As an example, using a baseline of `max_depth=8` chosen from the best model,
    we may narrow the range to from `7` to `9`.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，使用从最佳模型中选择的`max_depth=8`作为基准，我们可以将范围缩小到`7`到`9`。
- en: Another strategy is to stop checking hyperparameters whose defaults are working
    fine. `entropy`, for instance, is not recommended over `'gini'` as the differences
    are very slight. `min_impurity_split` and `min_impurity_decrease` may also be
    left at their defaults.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种策略是停止检查那些默认值已经表现良好的超参数。例如，`entropy`相较于`'gini'`的差异非常小，因此不推荐使用`entropy`。`min_impurity_split`和`min_impurity_decrease`也可以保留默认值。
- en: 'Here is a new hyperparameter range with an increase of `100` runs:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个新的超参数范围，增加了`100`次运行：
- en: '[PRE38]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: This model is more accurate in the training and test score.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型在训练和测试得分上更为准确。
- en: 'For a proper baseline of comparison, however, it''s essential to put the new
    model into `cross_val_clf`. This may be achieved by copying and pasting the preceding
    model:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，为了进行适当的比较基准，必须将新模型放入`cross_val_clf`中。可以通过复制并粘贴前面的模型来实现：
- en: '[PRE39]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The result is as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '[PRE40]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: This is six percentage points higher than the default model. When it comes to
    predicting heart disease, more accuracy can save lives.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这比默认模型高出六个百分点。在预测心脏病时，更高的准确性能够挽救生命。
- en: feature_importances_
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: feature_importances_
- en: The final piece of the puzzle is to communicate the most important features
    of the machine learning model. Decision trees come with a nice attribute, `feature_importances_`,
    that does exactly this.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 拼图的最后一块是传达机器学习模型中最重要的特征。决策树有一个非常有用的属性，`feature_importances_`，它正是用于显示这些重要特征的。
- en: First, we need to finalize the best model. Our function returned the best model,
    but it has not been saved.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要最终确定最佳模型。我们的函数已经返回了最佳模型，但它尚未被保存。
- en: When testing, it's important not to mix and match training and test sets. After
    a final model has been selected, however, fitting the model on the entire dataset
    can be beneficial. Why? Because the goal is to test the model on data that has
    never been seen and fitting the model on the entire dataset may lead to additional
    gains in accuracy.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试时，重要的是不要混用训练集和测试集。然而，在选择最终模型后，将模型拟合到整个数据集上可能是有益的。为什么呢？因为目标是测试模型在从未见过的数据上的表现，且将模型拟合到整个数据集可能会提高准确性。
- en: 'Let''s define the model using the best hyperparameters and fit it on the entire
    dataset:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用最佳超参数定义模型，并将其拟合到整个数据集上：
- en: '[PRE41]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'In order to determine the most important features, we can run the `feature_importances_` attribute
    on `best_clf`:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定最重要的特征，我们可以在`best_clf`上运行`feature_importances_`属性：
- en: '[PRE42]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'It''s not easy to interpret these results. The following code zips the columns
    along with the most important features into a dictionary before displaying them
    in reverse order for a clean output that is easy to interpret:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 解释这些结果并不容易。以下代码会将列与最重要的特征一起压缩成字典，然后按相反的顺序显示，以便清晰地输出并容易理解：
- en: '[PRE43]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The three most important features are as follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 三个最重要的特征如下：
- en: '`''cp''`: Chest pain type (`1` = typical angina, `2` = atypical angina, `3`
    = non-anginal pain, `4` = asymptomatic)'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''cp''`：胸痛类型（`1` = 典型心绞痛，`2` = 非典型心绞痛，`3` = 非心绞痛性疼痛，`4` = 无症状）'
- en: '`''thalach''`: Maximum heart rate achieved'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''thalach''`：最大心率'
- en: '`''ca''`: Number of major vessels (0-3) colored by fluoroscopy'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`''ca''`：通过透视染色的主要血管数量（0-3）'
- en: These numbers may be interpreted as their explanation of variance, so `'cp'`
    accounts for 48% of the variance, which is more than `'thal'` and `'ca'` combined.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数字可以解释为它们对方差的贡献，因此`'cp'`解释了48%的方差，超过了`'thal'`和`'ca'`的总和。
- en: You can tell the doctors and nurses that your model predicts if the patient
    has a heart disease with 82% accuracy using chest pain, maximum heart rate, and
    fluoroscopy as the three most important characteristics.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以告诉医生和护士，使用胸痛、最大心率和透视作为三大最重要特征，你的模型能够以82%的准确率预测患者是否患有心脏病。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you have taken a big leap toward mastering XGBoost by examining
    decision trees, the primary XGBoost base learners. You built decision tree regressors
    and classifiers by fine-tuning hyperparameters with `GridSearchCV` and `RandomizedSearchCV`.
    You visualized decision trees and analyzed their errors and accuracy in terms
    of variance and bias. Furthermore, you learned about an indispensable tool, `feature_importances_`,
    which is used to communicate the most important features of your model that is
    also an attribute of XGBoost.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，通过研究决策树这一XGBoost的基础学习器，你已经在掌握XGBoost的道路上迈出了重要一步。你通过`GridSearchCV`和`RandomizedSearchCV`微调超参数，构建了决策树回归器和分类器。你还可视化了决策树，并从方差和偏差的角度分析了它们的误差和准确性。此外，你还学会了一个不可或缺的工具——`feature_importances_`，它是XGBoost的一个属性，用于传达模型中最重要的特征。
- en: In the next chapter, you will learn how to build Random Forests, our first ensemble
    method and a rival of XGBoost. The applications of Random Forests are important
    for comprehending the difference between bagging and boosting, generating machine
    learning models comparable to XGBoost, and learning about the limitations of Random
    Forests that facilitated the development of XGBoost in the first place.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何构建随机森林，这是我们第一个集成方法，也是XGBoost的竞争对手。随机森林的应用对于理解bagging和boosting的区别、生成与XGBoost相当的机器学习模型以及了解随机森林的局限性至关重要，正是这些局限性促成了XGBoost的发展。
