- en: Important Elements in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习中的重要元素
- en: In this chapter, we're going to discuss some important elements and approaches
    which span through all machine learning topics and also create a philosophical
    foundation for many common techniques. First of all, it's useful to understand
    the mathematical foundation of data formats and prediction functions. In most
    algorithms, these concepts are treated in different ways, but the goal is always
    the same. More recent techniques, such as deep learning, extensively use energy/loss
    functions, just like the one described in this chapter, and even if there are
    slight differences, a good machine learning result is normally associated with
    the choice of the best loss function and the usage of the right algorithm to minimize
    it.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一些跨越所有机器学习主题的重要元素和方法，并为许多常见技术提供一个哲学基础。首先，了解数据格式和预测函数的数学基础是有用的。在大多数算法中，这些概念以不同的方式处理，但目标始终相同。更近期的技术，如深度学习，广泛使用能量/损失函数，就像本章中描述的那样，即使有细微的差别，好的机器学习结果通常与最佳损失函数的选择和使用正确的算法来最小化它有关。
- en: Data formats
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据格式
- en: 'In a supervised learning problem, there will always be a dataset, defined as
    a finite set of real vectors with *m* features each:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习问题中，将始终存在一个数据集，定义为具有每个 *m* 个特征的有限个实向量集合：
- en: '![](img/cf7ee9ce-5462-4e32-948e-4b45f311dd60.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/cf7ee9ce-5462-4e32-948e-4b45f311dd60.png)'
- en: 'Considering that our approach is always probabilistic, we need to consider
    each *X* as drawn from a statistical multivariate distribution *D*. For our purposes,
    it''s also useful to add a very important condition upon the whole dataset *X*:
    we expect all samples to be **independent and** **identically distributed **(**i.i.d**).
    This means all variables belong to the same distribution *D,* and considering
    an arbitrary subset of *m* values, it happens that:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们的方法始终是概率性的，我们需要将每个 *X* 视为从统计多元分布 *D* 中抽取的。对于我们的目的，在整体数据集 *X* 上添加一个非常重要的条件也是有用的：我们期望所有样本都是
    **独立且** **同分布的**（**i.i.d**）。这意味着所有变量都属于同一个分布 *D*，并且考虑一个任意的 *m* 个值的子集，它发生的情况是：
- en: '![](img/c9c6791e-c1bf-4105-8c44-fa90d01adc90.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c9c6791e-c1bf-4105-8c44-fa90d01adc90.png)'
- en: 'The corresponding output values can be both numerical-continuous or categorical.
    In the first case, the process is called **regression**, while in the second,
    it is called **classification**. Examples of numerical outputs are:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对应的输出值可以是数值-连续或分类。在前一种情况下，该过程称为 **回归**，而在第二种情况下，它称为 **分类**。数值输出的例子包括：
- en: '![](img/de59db9e-0a89-4320-87b9-18318f790f48.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/de59db9e-0a89-4320-87b9-18318f790f48.png)'
- en: 'Categorical examples are:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 分类示例包括：
- en: '![](img/294da3eb-db76-4f54-bc9a-eab0f4668bb0.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/294da3eb-db76-4f54-bc9a-eab0f4668bb0.png)'
- en: 'We define generic **regressor**, a vector-valued function which associates
    an input value to a continuous output and generic **classifier**, a vector-values
    function whose predicted output is categorical (discrete). If they also depend
    on an internal parameter vector which determines the actual instance of a generic
    predictor, the approach is called **parametric learnin****g**:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了通用的 **回归器**，一个将输入值关联到连续输出值的向量值函数，以及通用的 **分类器**，一个预测输出为分类（离散）的向量值函数。如果它们还依赖于一个内部参数向量，该向量决定了通用预测器的实际实例，则该方法称为
    **参数学习方法**：
- en: '![](img/61b89813-adc8-4be6-889d-fbc1cbb395fa.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61b89813-adc8-4be6-889d-fbc1cbb395fa.png)'
- en: 'On the other hand, **non-parametric learning** doesn''t make initial assumptions
    about the family of predictors (for example, defining a generic parameterized
    version of *r(...)* and *c(...))*. A very common non-parametric family is called
    **instance-based learning** and makes real-time predictions (without pre-computing
    parameter values) based on hypothesis determined only by the training samples
    (instance set). A simple and widespread approach adopts the concept of neighborhoods
    (with a fixed radius). In a classification problem, a new sample is automatically
    surrounded by classified training elements and the output class is determined
    considering the preponderant one in the neighborhood. In this book, we''re going
    to talk about another very important algorithm family belonging to this class:
    **kernel-based support vector machines**. More examples can be found in Russel
    S., Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson*.*'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**非参数学习**不对预测器的家族做出初始假设（例如，定义*r(...)*和*c(...)*的通用参数化版本）。一个非常常见的非参数家族被称为**基于实例的学习**，它基于仅由训练样本（实例集）确定的假设进行实时预测（无需预先计算参数值）。一种简单且广泛采用的方法是采用邻域的概念（具有固定的半径）。在分类问题中，一个新样本会自动被分类的训练元素所包围，输出类别是根据邻域中的主导元素确定的。在这本书中，我们将讨论属于此类的一个非常重要的算法家族：**基于核的支持向量机**。更多示例可以在Russel
    S.，Norvig P.的《人工智能：一种现代方法》，Pearson*中找到。
- en: The internal dynamics and the interpretation of all elements are peculiar to
    each single algorithm, and for this reason, we prefer not to talk now about thresholds
    or probabilities and try to work with an abstract definition. A generic parametric
    training process must find the best parameter vector which minimizes the regression/classification
    error given a specific training dataset and it should also generate a predictor that can
    correctly generalize when unknown samples are provided.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 内部动态和所有元素的解释都是每个单独算法特有的，因此我们更愿意现在不讨论阈值或概率，而是尝试使用一个抽象的定义。一个通用的参数化训练过程必须找到最佳参数向量，该向量在给定的特定训练数据集上最小化回归/分类误差，并且它还应该生成一个预测器，当提供未知样本时可以正确泛化。
- en: 'Another interpretation can be expressed in terms of additive noise:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种解释可以用加性噪声来表示：
- en: '![](img/9c69d743-ce0e-44c0-9e00-c00f90ddbcb0.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9c69d743-ce0e-44c0-9e00-c00f90ddbcb0.png)'
- en: For our purposes, we can expect zero-mean and low-variance Gaussian noise added
    to a perfect prediction. A training task must increase the signal-noise ratio
    by optimizing the parameters. Of course, whenever such a term doesn't have zero
    mean (independently from the other *X* values), probably it means that there's
    a hidden trend that must be taken into account (maybe a feature that has been
    prematurely discarded). On the other hand, high noise variance means that *X*
    is dirty and its measures are not reliable.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的目的，我们可以期望在完美预测中添加零均值和低方差的高斯噪声。训练任务必须通过优化参数来增加信噪比。当然，每当这样的术语不具有零均值（独立于其他*X*值）时，可能意味着存在一个必须考虑的隐藏趋势（可能是一个过早被丢弃的特征）。另一方面，高噪声方差意味着*X*是脏的，其测量不可靠。
- en: Until now we've assumed that both regression and classification operate on *m*-length
    vectors but produce a single value or single label (in other words, an input vector
    is always associated with only one output element). However, there are many strategies
    to handle multi-label classification and multi-output regression.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们假设回归和分类都操作于*m*-长度向量，但只产生一个值或一个标签（换句话说，一个输入向量总是与一个输出元素相关联）。然而，有许多策略来处理多标签分类和多输出回归。
- en: 'In unsupervised learning, we normally only have an input set *X* with *m*-length
    vectors, and we define clustering function (with *n* target clusters) with the
    following expression:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在无监督学习中，我们通常只有一个输入集*X*，其中包含*m*-长度向量，我们定义聚类函数（具有*n*个目标聚类）如下：
- en: '![](img/14adc763-f369-4892-a91a-53ebbb98cf2f.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/14adc763-f369-4892-a91a-53ebbb98cf2f.png)'
- en: 'In most scikit-learn models, there is an instance variable `coef_` which contains
    all trained parameters. For example, in a single parameter linear regression (we''re
    going to widely discuss it in the next chapters), the output will be:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数scikit-learn模型中，存在一个实例变量`coef_`，其中包含所有训练参数。例如，在单参数线性回归（我们将在下一章中广泛讨论它），输出将是：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Multiclass strategies
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类策略
- en: 'When the number of output classes is greater than one, there are two main possibilities
    to manage a classification problem:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当输出类别的数量大于一个时，管理分类问题有两种主要可能性：
- en: One-vs-all
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对多
- en: One-vs-one
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一
- en: In both cases, the choice is transparent and the output returned to the user
    will always be the final value or class. However, it's important to understand
    the different dynamics in order to optimize the model and to always pick the best
    alternative.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，选择都是透明的，返回给用户的输出将始终是最终值或类别。然而，为了优化模型并始终选择最佳替代方案，理解不同的动态是很重要的。
- en: One-vs-all
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对多
- en: This is probably the most common strategy and is widely adopted by scikit-learn
    for most of its algorithms. If there are *n* output classes, *n* classifiers will
    be trained in parallel considering there is always a separation between an actual
    class and the remaining ones. This approach is relatively lightweight (at most,
    *n-1* checks are needed to find the right class, so it has an *O(n)* complexity)
    and, for this reason, it's normally the default choice and there's no need for
    further actions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最常用的策略，并且scikit-learn的大多数算法都广泛采用。如果有*n*个输出类别，将并行训练*n*个分类器，考虑到实际类别和剩余类别之间总是存在分离。这种方法相对轻量（最多需要*n-1*次检查来找到正确的类别，因此它具有*O(n)*的复杂性），因此通常是默认选择，无需进一步操作。
- en: One-vs-one
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 一对一
- en: The alternative to one-vs-all is training a model for each pair of classes.
    The complexity is no longer linear (it's *O(n²)* indeed) and the right class is
    determined by a majority vote. In general, this choice is more expensive and should
    be adopted only when a full dataset comparison is not preferable.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与一对多相比，一种替代方案是为每一对类别训练一个模型。复杂性不再是线性的（实际上它是*O(n²)*）并且正确的类别是通过多数投票确定的。一般来说，这种选择成本更高，只有在完整数据集比较不合适时才应采用。
- en: If you want to learn more about multiclass strategies implemented by scikit-learn,
    visit
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于scikit-learn实现的多类策略，请访问
- en: '[http://scikit-learn.org/stable/modules/multiclass.html](http://scikit-learn.org/stable/modules/multiclass.html).'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://scikit-learn.org/stable/modules/multiclass.html](http://scikit-learn.org/stable/modules/multiclass.html)。'
- en: Learnability
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可学习性
- en: 'A parametric model can be split into two parts: a static structure and a dynamic
    set of parameters. The former is determined by choice of a specific algorithm
    and is normally immutable (except in the cases when the model provides some re-modeling
    functionalities), while the latter is the objective of our optimization. Considering
    *n* unbounded parameters, they generate an *n*-dimensional space (imposing bounds
    results in a sub-space without relevant changes in our discussion) where each
    point, together with the immutable part of the estimator function, represents
    a learning hypothesis *H* (associated with a specific set of parameters):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 一个参数模型可以被分为两部分：一个静态结构和一组动态参数。前者由特定算法的选择决定，通常是不可变的（除了模型提供某些重新建模功能的情况），而后者是我们优化的目标。考虑到*n*个无界参数，它们生成一个*n*-维空间（施加边界会产生一个子空间，在我们的讨论中不会引起相关变化）其中每个点，连同估计函数的不可变部分，代表一个学习假设*H*（与一组特定的参数相关联）：
- en: '![](img/12d82cf0-4495-47ea-9e82-7e28b2b3cae6.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/12d82cf0-4495-47ea-9e82-7e28b2b3cae6.png)'
- en: 'The goal of a parametric learning process is to find the best hypothesis whose
    corresponding prediction error is minimum and the residual generalization ability
    is enough to avoid overfitting. In the following figure, there''s an example of
    a dataset whose points must be classified as red (**Class A**) or blue (**Class
    B**). Three hypotheses are shown: the first one (the middle line starting from
    left) misclassifies one sample, while the lower and upper ones misclassify 13
    and 23 samples respectively:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 参数学习过程的目标是找到最佳假设，其对应的预测误差最小，并且剩余的泛化能力足够以避免过拟合。在下面的图中，有一个示例数据集，其点必须被分类为红色（**类别A**）或蓝色（**类别B**）。显示了三个假设：第一个（从左到右开始的中间线）错误地分类了一个样本，而下面和上面的分别错误地分类了13个和23个样本：
- en: '![](img/8f0af772-cd16-48a8-8d8b-927248b69f7b.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8f0af772-cd16-48a8-8d8b-927248b69f7b.png)'
- en: Of course, the first hypothesis is optimal and should be selected; however,
    it's important to understand an essential concept which can determine a potential
    overfitting. Think about an *n*-dimensional binary classification problem. We
    say that the dataset *X* is ***linearly **separable*** (without transformations)
    if there exists a hyperplane which divides the space into two subspaces containing
    only elements belonging to the same class. Removing the constraint of linearity,
    we have infinite alternatives using generic hypersurfaces. However, a parametric
    model adopts only a family of non-periodic and approximate functions whose ability
    to oscillate and fit the dataset is determined (sometimes in a very complex way)
    by the number of parameters.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，第一个假设是最优的，应该被选中；然而，理解一个基本概念非常重要，这个概念可以决定潜在的过拟合。思考一个*n*-维二进制分类问题。我们说数据集*X*是***线性可分***（没有变换）的，如果存在一个超平面可以将空间分为两个子空间，只包含属于同一类的元素。移除线性的约束，我们有无穷多的使用通用超面的替代方案。然而，参数模型只采用一族非周期性和近似函数，其振荡和拟合数据集的能力由参数的数量（有时以非常复杂的方式）决定。
- en: 'Consider the example shown in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下图中所示的示例：
- en: '![](img/d1c55062-664f-41a9-bef6-0810531d9121.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d1c55062-664f-41a9-bef6-0810531d9121.png)'
- en: 'The blue classifier is linear while the red one is cubic. At a glance, non-linear
    strategy seems to perform better, because it can capture more expressivity, thanks
    to its concavities. However, if new samples are added following the trend defined
    by the last four ones (from the right), they''ll be completely misclassified.
    In fact, while a linear function is globally better but cannot capture the initial
    oscillation between 0 and 4, a cubic approach can fit this data almost perfectly
    but, at the same time, loses its ability to keep a global linear trend. Therefore,
    there are two possibilities:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝色分类器是线性的，而红色的是立方的。乍一看，非线性策略似乎表现更好，因为它可以通过其凹面捕捉更多的表达能力。然而，如果按照最后四个样本（从右到左）定义的趋势添加新的样本，它们将被完全错误分类。事实上，虽然线性函数在全局上更好，但不能捕捉0和4之间的初始振荡，而立方方法可以几乎完美地拟合这些数据，但同时也失去了保持全局线性趋势的能力。因此，有两种可能性：
- en: If we expect future data to be exactly distributed as training samples, a more
    complex model can be a good choice, to capture small variations that a lower-level
    one will discard. In this case, a linear (or lower-level) model will drive to
    underfitting, because it won't be able to capture an appropriate level of expressivity.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们期望未来的数据与训练样本的分布完全相同，那么一个更复杂的模型可以是一个好的选择，以捕捉低级模型会丢弃的小变化。在这种情况下，一个线性（或低级）模型将导致欠拟合，因为它无法捕捉适当的表达能力水平。
- en: If we think that future data can be locally distributed differently but keeps
    a global trend, it's preferable to have a higher residual misclassification error
    as well as a more precise generalization ability. Using a bigger model focusing
    only on training data can drive to overfitting.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们认为未来的数据可以在局部以不同的方式分布，但保持全局趋势，那么更倾向于有更高的残余误分类错误以及更精确的泛化能力。仅关注训练数据的大模型可能导致过拟合。
- en: Underfitting and overfitting
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合
- en: 'The purpose of a machine learning model is to approximate an unknown function
    that associates input elements to output ones (for a classifier, we call them
    classes). However, a training set is normally a representation of a global distribution,
    but it cannot contain all possible elements; otherwise the problem could be solved
    with a one-to-one association. In the same way, we don''t know the analytic expression
    of a possible underlying function, therefore, when training, it''s necessary to
    think about fitting the model but keeping it free to generalize when an unknown
    input is presented. Unfortunately, this ideal condition is not always easy to
    find and it''s important to consider two different dangers:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型的目的是对一个未知函数进行近似，该函数将输入元素与输出元素（对于分类器，我们称之为类别）关联起来。然而，训练集通常是一个全局分布的表示，但它不能包含所有可能的元素；否则问题可以用一对一的关联来解决。同样，我们不知道可能的基本函数的解析表达式，因此，在训练时，必须考虑拟合模型，但保持它在面对未知输入时可以自由泛化的能力。不幸的是，这种理想条件并不总是容易找到，并且考虑两种不同的危险是很重要的：
- en: '**Underfitting**: It means that the model isn''t able to capture the dynamics
    shown by the same training set (probably because its capacity is too limited).'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欠拟合**：这意味着模型无法捕捉出由相同的训练集所展示的动态（可能是因为其容量过于有限）。'
- en: '**Overfitting**: the model has an excessive capacity and it''s not more able
    to generalize considering the original dynamics provided by the training set.
    It can associate almost perfectly all the known samples to the corresponding output
    values, but when an unknown input is presented, the corresponding prediction error
    can be very high.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**过拟合**：模型具有过度的容量，并且它不能根据训练集提供的原始动态进行泛化。它可以几乎完美地将所有已知样本与其相应的输出值关联起来，但当呈现未知输入时，相应的预测误差可能非常高。'
- en: 'In the following picture, there are examples of interpolation with low-capacity
    (underfitting), normal-capacity (normal fitting), and excessive capacity (overfitting):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图片中，有低容量（欠拟合）、正常容量（正常拟合）和过度容量（过拟合）插值的例子：
- en: '![](img/bce053b0-b0da-434d-ab6d-7b2c125c14f9.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bce053b0-b0da-434d-ab6d-7b2c125c14f9.png)'
- en: It's very important to avoid both underfitting and overfitting. Underfitting
    is easier to detect considering the prediction error, while overfitting may prove
    to be more difficult to discover as it could be initially considered the result
    of a perfect fitting.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 避免欠拟合和过拟合非常重要。考虑到预测误差，欠拟合更容易检测，而过拟合可能证明更难发现，因为它最初可能被认为是完美拟合的结果。
- en: Cross-validation and other techniques that we're going to discuss in the next
    chapters can easily show how our model works with test samples never seen during
    the training phase. That way, it would be possible to assess the generalization
    ability in a broader context (remember that we're not working with all possible
    values, but always with a subset that should reflect the original distribution).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中我们将讨论的交叉验证和其他技术可以很容易地显示我们的模型如何与训练阶段从未见过的测试样本一起工作。这样，我们就可以在一个更广泛的环境中评估泛化能力（记住，我们不是处理所有可能值，而是一直处理一个应该反映原始分布的子集）。
- en: However, a generic rule of thumb says that a residual error is always necessary
    to guarantee a good generalization ability, while a model that shows a validation
    accuracy of 99.999... percent on training samples is almost surely overfitted
    and will likely be unable to predict correctly when never-seen input samples are
    provided.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，一个通用的经验法则表明，一个残差误差总是必要的，以保证良好的泛化能力，而一个在训练样本上显示99.999...百分比的验证精度的模型几乎肯定过拟合了，并且很可能无法正确预测从未见过的输入样本。
- en: Error measures
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 误差度量
- en: 'In general, when working with a supervised scenario, we define a non-negative
    error measure *e[m]* which takes two arguments (expected and predicted output)
    and allows us to compute a total error value over the whole dataset (made up of
    *n* samples):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当在监督场景下工作时，我们定义一个非负误差度量 *e[m]*，它接受两个参数（预期输出和预测输出），并允许我们计算整个数据集（由 *n* 个样本组成）的总误差值：
- en: '![](img/b89bd043-d0fe-429f-a1df-45f1799c6c5e.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b89bd043-d0fe-429f-a1df-45f1799c6c5e.png)'
- en: 'This value is also implicitly dependent on the specific hypothesis *H* through
    the parameter set, therefore optimizing the error implies finding an optimal hypothesis
    (considering the hardness of many optimization problems, this is not the absolute
    best one, but an acceptable approximation). In many cases, it''s useful to consider
    the **mean square error** (**MSE**):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值也隐式地依赖于特定的假设 *H* 通过参数集，因此优化误差意味着找到一个最优假设（考虑到许多优化问题的难度，这并不是绝对最好的，但是一个可接受的近似）。在许多情况下，考虑**均方误差**（**MSE**）是有用的：
- en: '![](img/8f4e6658-1504-46cd-a092-24ca8e396991.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8f4e6658-1504-46cd-a092-24ca8e396991.png)'
- en: Its initial value represents a starting point over the surface of a n-variables
    function. A generic training algorithm has to find the global minimum or a point
    quite close to it (there's always a tolerance to avoid an excessive number of
    iterations and a consequent risk of overfitting). This measure is also called
    **loss function** because its value must be minimized through an optimization
    problem. When it's easy to determine an element which must be maximized, the corresponding
    loss function will be its reciprocal.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 其初始值代表了一个n变量函数表面的一个起点。一个通用的训练算法必须找到全局最小值或一个非常接近它的点（总是有一个容忍度以避免过多的迭代和随之而来的过拟合风险）。这个度量也被称为**损失函数**，因为它的值必须通过一个优化问题来最小化。当容易确定一个必须最大化的元素时，相应的损失函数将是它的倒数。
- en: 'Another useful loss function is called **zero-one-loss** and it''s particularly
    efficient for binary classifications (also for one-vs-rest multiclass strategy):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个有用的损失函数被称为**零一损失**，它特别适用于二元分类（也适用于一对一多类策略）：
- en: '![](img/5e6faa5c-914f-44c6-80e4-330e82586929.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5e6faa5c-914f-44c6-80e4-330e82586929.png)'
- en: This function is implicitly an indicator and can be easily adopted in loss functions
    based on the probability of misclassification.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数隐式地是一个指示器，可以很容易地应用于基于误分类概率的损失函数。
- en: 'A helpful interpretation of a generic (and continuous) loss function can be
    expressed in terms of potential energy:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的（连续的）损失函数的有帮助的解释可以用势能来表示：
- en: '![](img/fddbc218-c2e9-4728-9de2-0a94bd59eaee.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fddbc218-c2e9-4728-9de2-0a94bd59eaee.png)'
- en: 'The predictor is like a ball upon a rough surface: starting from a random point
    where energy (=error) is usually rather high, it must move until it reaches a
    stable equilibrium point where its energy (relative to the global minimum) is
    null. In the following figure, there''s a schematic representation of some different
    situations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 预测器就像粗糙表面上的一颗球：从一个能量（=错误）通常相当高的随机点开始，它必须移动直到它达到一个稳定的平衡点，在那里它的能量（相对于全局最小值）为零。在下面的图中，有一些不同情况的概念图示：
- en: '![](img/8e5b3a8a-8b1d-4fc3-8155-b827d210d6c8.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8e5b3a8a-8b1d-4fc3-8155-b827d210d6c8.png)'
- en: Just like in the physical situation, the starting point is stable without any
    external perturbation, so to start the process, it's needed to provide initial
    kinetic energy. However, if such an energy is strong enough, then after descending
    over the slope the ball cannot stop in the global minimum. The residual kinetic
    energy can be enough to overcome the ridge and reach the right valley. If there
    are not other energy sources, the ball gets trapped in the plain valley and cannot
    move anymore. There are many techniques that have been engineered to solve this
    problem and avoid local minima. However, every situation must always be carefully
    analyzed to understand what level of residual energy (or error) is acceptable,
    or whether it's better to adopt a different strategy. We're going to discuss some
    of them in the next chapters.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在物理情况中一样，起点在没有外部扰动的情况下是稳定的，因此为了启动这个过程，需要提供初始动能。然而，如果这种能量足够强大，那么在沿着斜坡下降后，球不能停在全局最小值处。剩余的动能可能足以克服脊并达到正确的山谷。如果没有其他能量来源，球就会被困在平原山谷中，无法再移动。已经开发了许多技术来解决此问题并避免局部最小值。然而，必须始终仔细分析每种情况，以了解可以接受的剩余能量（或错误）水平，或者是否最好采用不同的策略。我们将在下一章中讨论其中的一些。
- en: PAC learning
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PAC 学习
- en: In many cases machine learning seems to work seamlessly, but is there any way
    to determine formally the learnability of a concept? In 1984, the computer scientist
    L. Valiant proposed a mathematical approach to determine whether a problem is
    learnable by a computer. The name of this technique is **PAC**, or **probably
    approximately correct**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，机器学习似乎无缝工作，但有没有什么方法可以正式确定一个概念的可学习性？1984 年，计算机科学家 L. Valiant 提出了一种数学方法来确定一个问题是否可以通过计算机学习。这种技术的名称是
    **PAC**，或 **可能近似正确**。
- en: The original formulation (you can read it in Valiant L., *A Theory of the Learnable*,
    *Communications of the ACM*, Vol. 27, No. 11 , Nov. 1984) is based on a particular
    hypothesis, however, without a considerable loss of precision, we can think about
    a classification problem where an algorithm *A* has to learn a set of concepts*.*
    In particular, a concept is a subset of input patterns *X* which determine the
    same output element. Therefore, learning a concept (parametrically) means minimizing
    the corresponding loss function restricted to a specific class, while learning
    all possible concepts (belonging to the same universe), means finding the minimum
    of a global loss function.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 原始公式（您可以在 Valiant L. 的《可学习理论》*《ACM 通讯》*，第 27 卷，第 11 期，1984 年 11 月中找到）基于一个特定的假设，然而，在不损失大量精度的情况下，我们可以考虑一个分类问题，其中算法
    *A* 必须学习一组概念*.* 具体来说，一个概念是输入模式 *X* 的一个子集，它决定了相同的输出元素。因此，学习一个概念（参数化地）意味着最小化对应损失函数在特定类别的限制，而学习所有可能的概念（属于同一宇宙），意味着找到全局损失函数的最小值。
- en: However, given a problem, we have many possible (sometimes, theoretically infinite)
    hypotheses and a probabilistic trade-off is often necessary. For this reason,
    we accept good approximations with high probability based on a limited number
    of input elements and produced in polynomial time.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，给定一个问题，我们有许多可能的（有时，理论上无限）假设，并且通常需要进行概率权衡。因此，我们基于有限数量的输入元素和多项式时间内产生的结果，以高概率接受良好的近似。
- en: Therefore, an algorithm *A* can learn the class *C* of all concepts (making
    them PAC learnable) if it's able to find a hypothesis *H* with a procedure *O(n^k)*
    so that *A*, with a probability *p*, can classify all patterns correctly with
    a maximum allowed error *m[e]*. This must be valid for all statistical distributions
    on *X* and for a number of training samples which must be greater than or equal
    to a minimum value depending only on *p* and *m[e]*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，算法*A*能够学习所有概念（使它们成为PAC可学习的）的类别*C*，如果它能够找到一个假设*H*，通过一个*O(n^k)*过程，使得*A*，以概率*p*，可以在最大允许误差*m[e]*下正确分类所有模式。这必须对所有*X*上的统计分布和训练样本的数量有效，该数量必须大于或等于仅取决于*p*和*m[e]*的最小值。
- en: The constraint to computation complexity is not a secondary matter, in fact,
    we expect our algorithms to learn efficiently in a reasonable time also when the
    problem is quite complex. An exponential time could lead to computational explosions
    when the datasets are too large or the optimization starting point is very far
    from an acceptable minimum. Moreover, it's important to remember the so-called
    **curse of dimensionality**, which is an effect that often happens in some models
    where training or prediction time is proportional (not always linearly) to the
    dimensions, so when the number of features increases, the performance of the models
    (that can be reasonable when the input dimensionality is small) gets dramatically
    reduced. Moreover, in many cases, in order to capture the full expressivity, it's
    necessary to have a very large dataset and without enough training data, the approximation
    can become problematic (this is called **Hughes phenomenon**). For these reasons,
    looking for polynomial-time algorithms is more than a simple effort, because it
    can determine the success or the failure of a machine learning problem. For these
    reasons, in the next chapters, we're going to introduce some techniques that can
    be used to efficiently reduce the dimensionality of a dataset without a problematic
    loss of information.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 计算复杂度的限制不是一个次要问题，实际上，我们期望我们的算法在问题相当复杂的情况下也能在合理的时间内高效地学习。当数据集太大或优化起点离可接受的最低点非常远时，指数时间可能导致计算爆炸。此外，重要的是要记住所谓的**维度诅咒**，这是一种在某些模型中经常发生的效果，其中训练或预测时间与维度成比例（不一定是线性关系），因此当特征数量增加时，模型的性能（当输入维度较小时可能是合理的）会急剧下降。此外，在许多情况下，为了捕捉完整的表达能力，需要一个非常大的数据集，而没有足够的训练数据，近似可能会变得有问题（这被称为**休斯现象**）。因此，寻找多项式时间算法不仅仅是一个简单的努力，因为它可以决定机器学习问题的成功或失败。因此，在接下来的章节中，我们将介绍一些可以用来有效地降低数据集维度而不损失信息的技术的技术。
- en: Statistical learning approaches
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 统计学习方法
- en: 'Imagine that you need to design a spam-filtering algorithm starting from this
    initial (over-simplistic) classification based on two parameters:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你需要从以下初始（过于简化的）基于两个参数的分类开始设计一个垃圾邮件过滤算法：
- en: '| **Parameter** | **Spam emails (***X[1]***)** | **Regular emails (***X2***)**
    |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **参数** | **垃圾邮件（***X[1]***)** | **常规邮件（***X2***)** |'
- en: '| *p[1]* - Contains > 5 blacklisted words | 80 | 20 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| *p[1]* - 包含> 5个黑名单词汇 | 80 | 20 |'
- en: '| *p[2]*- Message length < 20 characters | 75 | 25 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| *p[2]*- 消息长度< 20个字符 | 75 | 25 |'
- en: 'We have collected 200 email messages (*X)* (for simplicity, we consider *p[1]*
    and *p*[*2 *]mutually exclusive) and we need to find a couple of probabilistic
    hypotheses (expressed in terms of *p[1]* and *p[2]*), to determine:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们收集了200封电子邮件（*X*）（为了简单起见，我们考虑*p[1]*和*p[2]*相互排斥）并且我们需要找到一对概率假设（用*p[1]*和*p[2]*来表示），以确定：
- en: '![](img/8351e161-8c7e-466e-88d1-9fc05a23d71c.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/8351e161-8c7e-466e-88d1-9fc05a23d71c.png)'
- en: We also assume the conditional independence of both terms (it means that *h[p1]*
    and *h[p2]* contribute conjunctly to spam in the same way as they were alone).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还假设这两个项的条件独立性（这意味着*h[p1]*和*h[p2]*以相同的方式共同贡献垃圾邮件，就像它们单独存在时一样）。
- en: 'For example, we could think about rules (hypotheses) like: "If there are more
    than five blacklisted words" or "If the message is less than 20 characters in
    length" then "the probability of spam is high" (for example, greater than 50 percent).
    However, without assigning probabilities, it''s difficult to generalize when the
    dataset changes (like in a real world antispam filter). We also want to determine
    a partitioning threshold (such as green, yellow, and red signals) to help the
    user in deciding what to keep and what to trash.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以考虑以下规则（假设）：“如果有超过五个黑名单词汇”或“如果消息长度小于20个字符”那么“垃圾邮件的概率很高”（例如，大于50%）。然而，如果没有分配概率，当数据集发生变化时（如现实世界中的反垃圾邮件过滤器），很难进行泛化。我们还想确定一个分区阈值（如绿色、黄色和红色信号），以帮助用户决定保留什么和删除什么。
- en: 'As the hypotheses are determined through the dataset *X*, we can also write
    (in a discrete form):'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于假设是通过数据集 *X* 确定的，我们也可以以离散形式写出：
- en: '![](img/493d60ac-f47e-4c5b-a678-14302e1c18b4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/493d60ac-f47e-4c5b-a678-14302e1c18b4.png)'
- en: 'In this example, it''s quite easy to determine the value of each term. However,
    in general, it''s necessary to introduce the Bayes formula (which will be discussed
    in [Chapter 6](e8152109-eb49-4f05-bd6b-16b532a68696.xhtml), *Naive Bayes*):'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，确定每个项的值相当容易。然而，在一般情况下，有必要引入贝叶斯公式（将在[第6章](e8152109-eb49-4f05-bd6b-16b532a68696.xhtml)，*朴素贝叶斯*）：
- en: '![](img/bb33ae98-e245-45bc-a594-0086a72f5484.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bb33ae98-e245-45bc-a594-0086a72f5484.png)'
- en: The proportionality is necessary to avoid the introduction of the marginal probability
    *P(X)*, which acts only as a normalization factor (remember that in a discrete
    random variable, the sum of all possible probability outcomes must be equal to
    1).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 比例性是必要的，以避免引入边缘概率 *P(X)*，它只起一个规范化因子的作用（记住，在离散随机变量中，所有可能的概率结果之和必须等于1）。
- en: 'In the previous equation, the first term is called **a posteriori** (which
    comes after) probability, because it''s determined by a marginal **Apriori** (which
    comes first) probability multiplied by a factor which is called **likelihood**.
    To understand the philosophy of such an approach, it''s useful to take a simple
    example: tossing a fair coin. Everybody knows that the marginal probability of
    each face is equal to 0.5, but who decided that? It''s a theoretical consequence
    of logic and probability axioms (a good physicist would say that it''s never 0.5
    because of several factors that we simply discard). After tossing the coin 100
    times, we observe the outcomes and, surprisingly, we discover that the ratio between
    heads and tails is slightly different (for example, 0.46). How can we correct
    our estimation? The term called **likelihood** measures how much our actual experiments
    confirm the Apriori hypothesis and determines another probability (**a posteriori**)
    which reflects the actual situation. The likelihood, therefore, helps us in correcting
    our estimation dynamically, overcoming the problem of a fixed probability.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一个方程中，第一项被称为**后验概率**（which comes after），因为它是由一个边缘**先验概率**（which comes first）乘以一个称为**似然**的因子所确定的。为了理解这种方法的哲学，举一个简单的例子很有用：抛一个公平的硬币。每个人都知道每个面的边缘概率都是相等的，等于0.5，但谁决定了这一点？这是逻辑和概率公理的理论结果（一个好的物理学家会说，由于我们简单地忽略了几个因素，它永远不会是0.5）。抛硬币100次后，我们观察结果，令人惊讶的是，我们发现正反比略有所不同（例如，0.46）。我们如何纠正我们的估计？称为**似然**的项衡量我们的实际实验在多大程度上证实了先验假设，并确定另一个概率（**后验概率**），它反映了实际情况。因此，似然帮助我们动态地纠正估计，克服固定概率的问题。
- en: 'In [Chapter 6](e8152109-eb49-4f05-bd6b-16b532a68696.xhtml), *Naive Bayes*, dedicated
    to naive Bayes algorithms, we''re going to discuss these topics deeply and implement
    a few examples with scikit-learn, however, it''s useful to introduce here two
    statistical learning approaches which are very diffused. Refer to *Russel S.,
    Norvig P., Artificial Intelligence: A Modern Approach, Pearson *for further information.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](e8152109-eb49-4f05-bd6b-16b532a68696.xhtml)，*朴素贝叶斯*，专门讨论朴素贝叶斯算法的章节中，我们将深入讨论这些主题，并使用scikit-learn实现一些示例，然而，在这里介绍两种非常普遍的统计学习方法是有用的。有关更多信息，请参阅*Russel
    S.，Norvig P.，人工智能：一种现代方法，Pearson*。
- en: MAP learning
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MAP学习
- en: 'When selecting the right hypothesis, a Bayesian approach is normally one of
    the best choices, because it takes into account all the factors and, as we''ll
    see, even if it''s based on conditional independence, such an approach works perfectly
    when some factors are partially dependent. However, its complexity (in terms of
    probabilities) can easily grow because all terms must always be taken into account.
    For example, a real coin is a very short cylinder, so, in tossing a coin, we should
    also consider the probability of even. Let''s say, it''s 0.001\. It means that
    we have three possible outcomes: *P(head)* = *P(tail)* = (1.0 - 0.001) / 2.0 and
    *P(even)* = 0.001\. The latter event is obviously unlikely, but in Bayesian learning
    it must be considered (even if it''ll be squeezed by the strength of the other
    terms).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择合适的假设时，贝叶斯方法通常是最好的选择之一，因为它考虑了所有因素，并且正如我们将要看到的，即使它基于条件独立性，当某些因素部分相关时，这种方法也能完美地工作。然而，其复杂性（从概率的角度来看）可以轻易增长，因为所有项都必须始终被考虑。例如，一枚真实的硬币是一个非常短的圆柱体，所以在抛硬币时，我们也应该考虑偶数的概率。比如说，它是0.001。这意味着我们有三种可能的结果：*P(head)*
    = *P(tail)* = (1.0 - 0.001) / 2.0 和 *P(even)* = 0.001。后者事件显然不太可能，但在贝叶斯学习中必须考虑（即使它会被其他项的强度所压缩）。
- en: 'An alternative is picking the most probable hypothesis in terms of **a posteriori**
    probability:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个选择是选择基于**后验概率**的最可能假设：
- en: '![](img/1c07ea8b-dbff-4736-9df4-5df7d7303421.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c07ea8b-dbff-4736-9df4-5df7d7303421.png)'
- en: 'This approach is called **MAP** (**maximum a posteriori**)and it can really
    simplify the scenario when some hypotheses are quite unlikely (for example, in
    tossing a coin, a MAP hypothesis will discard *P(even)*). However, it still does
    have an important drawback: it depends on Apriori probabilities (remember that
    maximizing the a posteriori implies considering also the Apriori). As Russel and
    Norvig (Russel S., Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson)
    pointed out, this is often a delicate part of an inferential process, because
    there''s always a theoretical background which can drive to a particular choice
    and exclude others. In order to rely only on data, it''s necessary to have a different
    approach.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法被称为**最大后验概率估计（MAP**）并且它确实可以简化某些假设非常不可能的情况（例如，在抛硬币时，MAP假设将丢弃*P(even)*）。然而，它仍然有一个重要的缺点：它依赖于先验概率（记住，最大化后验概率意味着也要考虑先验概率）。正如Russel和Norvig（Russel
    S.，Norvig P.，《人工智能：一种现代方法》，Pearson）所指出的，这通常是推理过程中的一个微妙部分，因为总有一个理论背景可以导致特定的选择并排除其他选择。为了仅依赖于数据，有必要采用不同的方法。
- en: Maximum-likelihood learning
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最大似然学习
- en: 'We have defined likelihood as a filtering term in the Bayes formula. In general,
    it has the form of:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将似然定义为贝叶斯公式中的过滤项。一般来说，它具有以下形式：
- en: '![](img/ed45edfa-51f8-45d3-86ce-f7587f981802.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ed45edfa-51f8-45d3-86ce-f7587f981802.png)'
- en: 'Here the first term expresses the actual likelihood of a hypothesis, given
    a dataset *X*. As you can imagine, in this formula there are no more Apriori probabilities,
    so, maximizing it doesn''t imply accepting a theoretical preferential hypothesis,
    nor considering unlikely ones. A very common approach, known as **expectation-maximization** and
    used in many algorithms (we''re going to see an example in logistic regression),
    is split into two main parts:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这里第一个项表示给定数据集*X*的假设的实际似然。正如你可以想象的，在这个公式中不再有先验概率，所以最大化它并不意味着接受一个理论上的偏好假设，也不考虑不可能的假设。一个非常常见的方法，称为**期望最大化（expectation-maximization**）并且被许多算法使用（我们将在逻辑回归中看到一个例子），分为两个主要部分：
- en: Determining a log-likelihood expression based on model parameters (they will
    be optimized accordingly)
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于模型参数确定对数似然表达式（它们将被相应优化）
- en: Maximizing it until residual error is small enough
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化它直到残差误差足够小
- en: 'A log-likelihood (normally called **L**) is a useful trick that can simplify
    gradient calculations. A generic likelihood expression is:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对数似然（通常称为**L**）是一个有用的技巧，可以简化梯度计算。一个通用的似然表达式是：
- en: '![](img/46a676cb-6f2a-46da-baf9-782ac755edfc.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/46a676cb-6f2a-46da-baf9-782ac755edfc.png)'
- en: 'As all parameters are inside *h[i]*, the gradient is a complex expression which
    isn''t very manageable. However our goal is maximizing the likelihood, but it''s
    easier minimizing its reciprocal:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 由于所有参数都在*h[i]*中，梯度是一个复杂的表达式，不太容易管理。然而，我们的目标是最大化似然，但最小化其倒数更容易：
- en: '![](img/88406a0c-88b2-4ae5-8254-65818be8795d.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/88406a0c-88b2-4ae5-8254-65818be8795d.png)'
- en: 'This can be turned into a very simple expression by applying natural logarithm
    (which is monotonic):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用自然对数（这是一个单调函数），这可以转化为一个非常简单的表达式：
- en: '![](img/51c3835c-1cff-46a8-907d-94789b26a129.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![](img/51c3835c-1cff-46a8-907d-94789b26a129.png)'
- en: 'The last term is a summation which can be easily derived and used in most of
    the optimization algorithms. At the end of this process, we can find a set of
    parameters which provides the maximum likelihood without any strong statement
    about prior distributions. This approach can seem very technical, but its logic
    is really simple and intuitive. To understand how it works, I propose a simple
    exercise, which is part of Gaussian mixture technique discussed also in Russel
    S., Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson*.*'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后这一项是一个求和，它可以在大多数优化算法中轻松推导和使用。在完成这个过程后，我们可以找到一组参数，它提供了最大似然，而不需要对先验分布做出任何强烈的声明。这种方法可能看起来非常技术性，但它的逻辑实际上非常简单直观。为了理解它是如何工作的，我提出一个简单的练习，这是高斯混合技术的一部分，也在Russel
    S.，Norvig P.的《人工智能：一种现代方法》，Pearson*.*中讨论过。
- en: 'Let''s consider 100 points drawn from a Gaussian distribution with zero mean
    and a standard deviation equal to 2.0 (quasi-white noise made of independent samples):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑从均值为零、标准差等于2.0的高斯分布中抽取的100个点（由独立样本组成的准白色噪声）：
- en: '[PRE1]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The plot is shown next:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了情节：
- en: '![](img/2fdd0020-4519-4ffd-a8a5-afd1efc8e214.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2fdd0020-4519-4ffd-a8a5-afd1efc8e214.png)'
- en: 'In this case, there''s no need for a deep exploration (we know how they are
    generated), however, after restricting the hypothesis space to the Gaussian family
    (the most suitable considering only the graph), we''d like to find the best value
    for mean and variance. First of all, we need to compute the log-likelihood (which
    is rather simple thanks to the exponential function):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，没有必要进行深入探索（我们知道它们是如何生成的），然而，在将假设空间限制为高斯族（仅考虑图的情况下最合适的）之后，我们希望找到均值和方差的最佳值。首先，我们需要计算对数似然（由于指数函数的存在，这相当简单）：
- en: '![](img/44af75ad-1e1c-4dd7-b53e-95dde3a3e07d.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](img/44af75ad-1e1c-4dd7-b53e-95dde3a3e07d.png)'
- en: 'A simple Python implementation is provided next (for ease of use, there''s
    only a single array which contains both mean (0) and variance (1)):'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来提供了一个简单的Python实现（为了方便使用，这里只有一个数组，它包含均值（0）和方差（1））：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Then we need to find its minimum (in terms of mean and variance) with any of
    the available methods (gradient descent or another numerical optimization algorithm).
    For example, using the `scipy` minimization function, we can easily get:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们需要使用任何可用的方法（梯度下降或其他数值优化算法）找到它的最小值（就均值和方差而言）。例如，使用`scipy`的最小化函数，我们可以轻松地得到：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A graph of the negative log-likelihood function is plotted next. The global
    minimum of this function corresponds to an optimal likelihood given a certain
    distribution. It doesn''t mean that the problem has been completely solved, because
    the first step of this algorithm is determining an expectation, which must be
    always realistic. The likelihood function, however, is quite sensitive to wrong
    distributions because it can easily get close to zero when the probabilities are
    low. For this reason, **maximum-likelihood** (**ML**) learning is often preferable
    to MAP learning, which needs Apriori distributions and can fail when they are
    not selected in the most appropriate way:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来绘制了负对数似然函数的图表。该函数的全局最小值对应于给定一定分布下的最优似然。这并不意味着问题已经完全解决，因为该算法的第一步是确定一个期望值，这个期望值必须始终是现实的。然而，似然函数对错误的分布非常敏感，因为它在概率低时很容易接近零。因此，**最大似然**（**ML**）学习通常比需要先验分布的MAP学习更可取，后者在未以最合适的方式选择时可能会失败：
- en: '![](img/0be8899e-8370-4402-be60-3f56a2da2cac.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0be8899e-8370-4402-be60-3f56a2da2cac.png)'
- en: 'This approach has been applied to a specific distribution family (which is
    indeed very easy to manage), but it also works perfectly when the model is more
    complex. Of course, it''s always necessary to have an initial awareness about
    how the likelihood should be determined because more than one feasible family
    can generate the same dataset. In all these cases, **Occam''s razor** is the best
    way to proceed: the simplest hypothesis should be considered first. If it doesn''t
    fit, an extra level of complexity can be added to our model. As we''ll see, in
    many situations, the easiest solution is the winning one, and increasing the number
    of parameters or using a more detailed model can only add noise and a higher possibility
    of overfitting.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法已经应用于特定的分布族（这确实很容易管理），但当模型更复杂时，它也能完美地工作。当然，始终有必要对如何确定似然性有一个初步的了解，因为多个可行的家族可以生成相同的数据集。在这些所有情况下，**奥卡姆剃刀**是最佳的前进方式：应该首先考虑最简单的假设。如果它不合适，可以给我们的模型添加额外的复杂度。正如我们将看到的，在许多情况下，最简单的解决方案就是获胜的方案，增加参数的数量或使用更详细模型只会增加噪声和过拟合的可能性。
- en: SciPy ([https://www.scipy.org](https://www.scipy.org)) is a set of high-end
    scientific and data-oriented libraries available for Python. It includes NumPy,
    Pandas, and many other useful frameworks. If you want to read more about Python
    scientific computing, refer to Johansson R., *Numerical Python*, Apress or Landau
    R. H., Pàez M. J., Bordeianu C. C., *Computational Physics. Problem Solving with
    Python,* Wiley-VCH.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy ([https://www.scipy.org](https://www.scipy.org)) 是一组针对 Python 的高端科学和数据导向的库。它包括
    NumPy、Pandas 以及许多其他有用的框架。如果你想了解更多关于 Python 科学计算的信息，请参考 Johansson R. 的 *Numerical
    Python*，Apress 出版，或者 Landau R. H.、Pàez M. J.、Bordeianu C. C. 的 *Computational
    Physics. Problem Solving with Python*，Wiley-VCH 出版。
- en: Elements of information theory
  id: totrans-122
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 信息论要素
- en: A machine learning problem can also be analyzed in terms of information transfer
    or exchange. Our dataset is composed of *n* features, which are considered independent
    (for simplicity, even if it's often a realistic assumption) drawn from *n* different
    statistical distributions. Therefore, there are *n* probability density functions
    *p[i](x)* which must be approximated through other *n* *q**[i](x)* functions.
    In any machine learning task, it's very important to understand how two corresponding
    distributions diverge and what is the amount of information we lose when approximating
    the original dataset.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习问题也可以从信息传递或交换的角度进行分析。我们的数据集由 *n* 个特征组成，这些特征被认为是独立的（为了简单起见，即使这通常是一个现实的假设），它们来自
    *n* 个不同的统计分布。因此，有 *n* 个概率密度函数 *p[i](x)*，必须通过其他 *n* 个 *q**[i](x)* 函数来近似。在任何机器学习任务中，理解两个相应的分布如何发散以及当我们近似原始数据集时损失多少信息是非常重要的。
- en: 'The most useful measure is called **entropy**:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最有用的度量称为**熵**：
- en: '![](img/a0de87df-2154-4f2e-b197-7202471d184a.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a0de87df-2154-4f2e-b197-7202471d184a.png)'
- en: 'This value is proportional to the uncertainty of *X* and it''s measured in
    **bits** (if the logarithm has another base, this unit can change too). For many
    purposes, a high entropy is preferable, because it means that a certain feature
    contains more information. For example, in tossing a coin (two possible outcomes), *H(X)*
    = 1 bit, but if the number of outcomes grows, even with the same probability,
    *H(X)* also does because of a higher number of different values and therefore
    increased variability. It''s possible to prove that for a Gaussian distribution
    (using natural logarithm):'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 这个值与 *X* 的不确定性成正比，并且以 **比特**（如果对数的底数不同，这个单位也可能改变）来衡量。对于许多目的来说，高熵是首选的，因为它意味着某个特征包含更多的信息。例如，在抛硬币（两种可能的结果）的情况下，*H(X)*
    = 1 比特，但如果结果的数量增加，即使概率相同，*H(X)* 也会因为更多不同的值和因此增加的变异性而增加。可以证明，对于高斯分布（使用自然对数）：
- en: '![](img/02490395-4402-4343-b04e-b487a35973c4.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](img/02490395-4402-4343-b04e-b487a35973c4.png)'
- en: 'So, the entropy is proportional to the variance, which is a measure of the
    amount of information carried by a single feature. In the next chapter, we''re
    going to discuss a method for feature selection based on variance threshold. Gaussian
    distributions are very common, so this example can be considered just as a general
    approach to feature filtering: low variance implies low information level and
    a model could often discard all those features.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，熵与方差成正比，方差是衡量单个特征所携带信息量的度量。在下一章中，我们将讨论一种基于方差阈值的特征选择方法。高斯分布非常常见，因此这个例子可以被视为特征过滤的一般方法：低方差意味着低信息水平，模型通常会丢弃所有这些特征。
- en: 'In the following figure, there''s a plot of *H(X)* for a Gaussian distribution
    expressed in **nats** (which is the corresponding unit measure when using natural
    logarithms):'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，展示了高斯分布的 *H(X)* 图形，该分布以 **nats**（当使用自然对数时对应的单位）表示：
- en: '![](img/da315d54-20ba-49d6-abc8-93340e600d26.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/da315d54-20ba-49d6-abc8-93340e600d26.png)'
- en: For example, if a dataset is made up of some features whose variance (here it's
    more convenient talking about standard deviation) is bounded between 8 and 10
    and a few with *STD < 1.5*, the latter could be discarded with a limited loss
    in terms of information. These concepts are very important in real-life problems
    when large datasets must be cleaned and processed in an efficient way.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个数据集由一些特征组成，这些特征的方差（在这里更方便谈论标准差）介于8和10之间，而少数具有 *STD < 1.5* 的特征，后者可以在有限的信息损失下被丢弃。这些概念在现实生活中的问题中非常重要，当必须以高效的方式清理和处理大量数据集时。
- en: 'If we have a target probability distribution *p(x)*, which is approximated
    by another distribution *q(x)*, a useful measure is **cross-entropy** between
    *p* and *q* (we are using the discrete definition as our problems must be solved
    using numerical computations):'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个目标概率分布 *p(x)*，它被另一个分布 *q(x)* 近似，一个有用的度量是 *p* 和 *q* 之间的 **交叉熵**（我们使用离散定义，因为我们的问题必须通过数值计算来解决）：
- en: '![](img/f2c2381a-0820-48fc-864c-bb723a5313e6.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f2c2381a-0820-48fc-864c-bb723a5313e6.png)'
- en: 'If the logarithm base is 2, it measures the number of bits requested to decode
    an event drawn from *P* when using a code optimized for *Q*. In many machine learning
    problems, we have a source distribution and we need to train an estimator to be
    able to identify correctly the class of a sample. If the error is null, *P = Q*
    and the cross-entropy is minimum (corresponding to the entropy *H(P)*). However,
    as a null error is almost impossible when working with *Q*, we need to *pay* a
    price of *H(P, Q)* bits, to determine the right class starting from a prediction.
    Our goal is often to minimize it, so to reduce this *price* under a threshold
    that cannot alter the predicted output if not paid. In other words, think about
    a binary output and a sigmoid function: we have a threshold of 0.5 (this is the
    maximum *price* we can pay) to identify the correct class using a step function
    (0.6 -> 1, 0.1 -> 0, 0.4999 -> 0, and so on). As we''re not able to pay this *price*,
    since our classifier doesn''t know the original distribution, it''s necessary
    to reduce the cross-entropy under a tolerable noise-robustness threshold (which
    is always the smallest achievable one).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果以2为底数，它衡量使用针对 *Q* 优化的代码解码从 *P* 中抽取的事件所需的位数。在许多机器学习问题中，我们有一个源分布，我们需要训练一个估计器来正确识别样本的类别。如果错误为零，*P
    = Q* 且交叉熵最小（对应于熵 *H(P)*）。然而，由于与 *Q* 一起工作时零错误几乎是不可能的，我们需要支付 *H(P, Q)* 位的代价，从预测开始确定正确的类别。我们的目标通常是使其最小化，以在不会改变预测输出的阈值以下减少这种“代价”。换句话说，考虑一个二元输出和sigmoid函数：我们有一个阈值为0.5（这是我们能够支付的最大的“代价”），使用阶跃函数（0.6
    -> 1，0.1 -> 0，0.4999 -> 0，等等）来识别正确的类别。由于我们的分类器不知道原始分布，我们不能支付这个“代价”，因此有必要将交叉熵降低到可接受的噪声鲁棒性阈值以下（这总是可以实现的最低值）。
- en: 'In order to understand how a machine learning approach is performing, it''s
    also useful to introduce a **conditional** entropy or the uncertainty of *X* given
    the knowledge of *Y*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解机器学习方法的表现，引入一个 **条件** 熵或 *X* 在知道 *Y* 的知识下的不确定性也是很有用的：
- en: '![](img/b6c53585-df26-477f-af85-9e87070e5658.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b6c53585-df26-477f-af85-9e87070e5658.png)'
- en: 'Through this concept, it''s possible to introduce the idea of mutual information,
    which is the amount of information shared by both variables and therefore, the
    reduction of uncertainty about *X* provided by the knowledge of *Y*:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这个概念，可以引入互信息的思想，即两个变量共享的信息量，因此，通过 *Y* 的知识提供的关于 *X* 的不确定性减少：
- en: '![](img/791c1b5b-71b8-4426-8e8f-a8889290990f.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/791c1b5b-71b8-4426-8e8f-a8889290990f.png)'
- en: Intuitively, when *X* and *Y* are independent, they don't share any information.
    However, in machine learning tasks, there's a very tight dependence between an
    original feature and its prediction, so we want to maximize the information shared
    by both distributions. If the conditional entropy is small enough (so *Y* is able
    to describe *X* quite well), the mutual information gets close to the marginal
    entropy *H(X)*, which measures the amount of information we want to learn.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，当*X*和*Y*是独立的，它们不共享任何信息。然而，在机器学习任务中，原始特征与其预测之间存在非常紧密的依赖关系，因此我们希望最大化两个分布共享的信息。如果条件熵足够小（因此*Y*能够很好地描述*X*），则互信息接近边缘熵*H(X*)，它衡量我们想要学习的信息量。
- en: 'An interesting learning approach based on the information theory, called **Minimum
    Description Length** (**MDL**), is discussed in Russel S., Norvig P., *Artificial
    Intelligence: A Modern Approach*, Pearson, where I suggest you look for any further
    information about these topics.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 一种基于信息论的有趣的学习方法，称为**最小描述长度**（**MDL**），在Russel S.，Norvig P.的《人工智能：一种现代方法》一书中进行了讨论，Pearson出版社，我建议您在此处查找有关这些主题的更多信息。
- en: References
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Russel S., Norvig P., *Artificial Intelligence: A Modern Approach*, Pearson'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Russel S.，Norvig P.，《人工智能：一种现代方法》，Pearson
- en: Valiant L., *A Theory of the Learnable, Communications of the ACM*, Vol. 27,
    No. 11 (Nov. 1984)
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Valiant L.，《可学习理论》，ACM通讯，第27卷，第11期（1984年11月）
- en: 'Hastie T., Tibshirani R., Friedman J., *The Elements of Statistical Learning:
    Data Mining, Inference and, Prediction*, Springer'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hastie T.，Tibshirani R.，Friedman J.，《统计学习的元素：数据挖掘、推理和预测》，Springer
- en: 'Aleksandrov A.D., Kolmogorov A.N, Lavrent''ev M.A., *Mathematics: Its contents,
    Methods, and Meaning*, Courier Corporation'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Aleksandrov A.D.，Kolmogorov A.N，Lavrent'ev M.A.，《数学：其内容、方法和意义》，Courier Corporation
- en: Summary
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have introduced some main concepts about machine learning.
    We started with some basic mathematical definitions, to have a clear view about
    data formats, standards, and kind of functions. This notation will be adopted
    in all the other chapters and it's also the most diffused in technical publications.
    We discussed how scikit-learn seamlessly works with multi-class problems, and
    when a strategy is preferable to another.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一些关于机器学习的主要概念。我们首先从一些基本的数学定义开始，以便对数据格式、标准和函数类型有一个清晰的认识。这种符号将在所有其他章节中采用，并且在技术出版物中也是最广泛使用的。我们讨论了scikit-learn如何无缝地处理多类问题，以及何时一种策略比另一种策略更可取。
- en: 'The next step was the introduction of some fundamental theoretical concepts
    about learnability. The main questions we tried to answer were: how can we decide
    if a problem can be learned by an algorithm and what is the maximum precision
    we can achieve. PAC learning is a generic but powerful definition that can be
    adopted when defining the boundaries of an algorithm. A PAC learnable problem,
    in fact, is not only manageable by a suitable algorithm but is also fast enough
    to be computed in polynomial time. Then we introduced some common statistical
    learning concepts, in particular, MAP and maximum likelihood learning approaches.
    The former tries to pick the hypothesis which maximizes the a posteriori probability,
    while the latter works the likelihood, looking for the hypothesis that best fits
    the data. This strategy is one of the most diffused in many machine learning problems
    because it''s not affected by Apriori probabilities and it''s very easy to implement
    in many different contexts. We also gave a physical interpretation of a loss function
    as an energy function. The goal of a training algorithm is to always try to find
    the global minimum point, which corresponds to the deepest valley in the error
    surface. At the end of this chapter, there was a brief introduction to information
    theory and how we can reinterpret our problems in terms of information gain and
    entropy. Every machine learning approach should work to minimize the amount of
    information needed to start from prediction and recover original (desired) outcomes.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是引入一些关于可学习性的基本理论概念。我们试图回答的主要问题是：我们如何决定一个问题是否可以通过算法来学习，以及我们能够达到的最大精度是什么。PAC学习是一个通用的但强大的定义，可以在定义算法边界时采用。实际上，一个PAC可学习的问题不仅可以通过合适的算法来管理，而且足够快，可以在多项式时间内计算。然后我们引入了一些常见的统计学习概念，特别是MAP和最大似然学习方法。前者试图选择最大化后验概率的假设，而后者则处理似然性，寻找与数据最吻合的假设。这种策略在许多机器学习问题中非常普遍，因为它不受先验概率的影响，并且在许多不同的环境中很容易实现。我们还给出了损失函数作为能量函数的物理解释。训练算法的目标是始终尝试找到全局最小点，这对应于误差表面的最深谷。在本章的结尾，简要介绍了信息论以及我们如何用信息增益和熵来重新解释我们的问题。每个机器学习方法都应该努力减少从预测开始并恢复原始（期望）结果所需的信息量。
- en: In the next chapter, we're going to discuss the fundamental concepts of feature
    engineering, which is the first step in almost every machine learning pipeline.
    We're going to show how to manage different kinds of data (numerical and categorical)
    and how it's possible to reduce the dimensionality without a dramatic loss of
    information.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论特征工程的基本概念，这是几乎所有机器学习流程的第一步。我们将展示如何管理不同类型的数据（数值和分类）以及如何在信息损失不大的情况下降低维度。
