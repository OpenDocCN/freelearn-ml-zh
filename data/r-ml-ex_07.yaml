- en: Chapter 6. Credit Risk Detection and Prediction – Predictive Analytics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章：信用风险检测与预测 - 预测分析
- en: In the previous chapter, we covered a lot of ground in the financial domain
    where we took up the challenge of detecting and predicting bank customers who
    could be potential credit risks. We now have a good idea about our main objective
    regarding credit risk analysis. Besides this, the substantial knowledge gained
    from descriptive analytics of the dataset and its features will be useful for
    predictive analytics, as we had mentioned earlier.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们在金融领域覆盖了很多内容，我们接受了检测和预测可能成为潜在信用风险的银行客户的挑战。现在，我们对关于信用风险分析的主要目标有了很好的了解。此外，从数据集及其特征的描述性分析中获得的大量知识将对预测分析有用，正如我们之前提到的。
- en: In this chapter, we will be journeying through the world of predictive analytics,
    which sits at the core of machine learning and data science. Predictive analytics
    encompasses several things which include classification algorithms, regression
    algorithms, domain knowledge, and business logic which are combined to build predictive
    models and derive useful insights from data. We had discussed various machine
    learning algorithms at the end of the previous chapter which would be applicable
    for solving our objective, and we will be exploring several of them in this chapter
    when we build predictive models using the given dataset and these algorithms.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将穿越预测分析的世界，这是机器学习和数据科学的核心。预测分析包括几个方面，如分类算法、回归算法、领域知识和业务逻辑，它们结合在一起构建预测模型并从数据中提取有用见解。我们在上一章的末尾讨论了各种机器学习算法，这些算法将适用于解决我们的目标，当我们使用给定的数据集和这些算法构建预测模型时，我们将在本章中探索其中的一些。
- en: An interesting take on predictive analytics is that it holds a lot of promise
    for organizations who want to strengthen their business and profits in the future.
    With the advent of big data, most organizations now have more data than they can
    analyze! While this is a big challenge, a tougher challenge is to select the right
    data points from this data and build predictive models which would be capable
    of predicting outcomes correctly in the future. However, there are several caveats
    in this approach because each model is basically mathematical functions based
    on formulae, assumptions, and probability. Also, in the real world, conditions
    and scenarios keep changing and evolving and thus one must remember that a predictive
    model built today may be completely redundant tomorrow.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对预测分析的一个有趣的观点是，它为希望在未来加强其业务和利润的组织提供了很多希望。随着大数据的出现，现在大多数组织拥有的数据比他们能分析的数据还要多！虽然这是一个大挑战，但更严峻的挑战是从这些数据中选择正确的数据点并构建能够正确预测未来结果的预测模型。然而，这种方法有几个注意事项，因为每个模型基本上是基于公式、假设和概率的数学函数。此外，在现实世界中，条件和场景不断变化和演变，因此我们必须记住，今天构建的预测模型明天可能完全过时。
- en: A lot of skeptics say that it is extremely difficult for computers to mimic
    humans to predict outcomes which even humans can't predict due of the ever changing
    nature of the environment with time, and hence all statistical methods are only
    valuable under ideal assumptions and conditions. While this is true to some extent,
    with the right data, a proper mindset, and by applying the right algorithms and
    techniques, we can build robust predictive models which can definitely try and
    tackle problems which would be otherwise impossible to tackle by conventional
    or brute-force methods.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 许多怀疑者认为，由于环境的不断变化，计算机模仿人类预测结果非常困难，甚至人类也无法预测，因此所有统计方法仅在理想假设和条件下才有价值。虽然这在某种程度上是正确的，但有了正确的数据、正确的思维方式和应用正确的算法和技术，我们可以构建稳健的预测模型，这些模型肯定可以尝试解决传统或蛮力方法无法解决的问题。
- en: Predictive modeling is a difficult task and while there might be a lot of challenges
    and results might be difficult to obtain always, one must take these challenges
    with a pinch of salt and remember the quotation from the famous statistician George
    E.P. Box, who claimed that *Essentially all models are wrong but some are useful!*,
    which is quite true based on what we discussed earlier. Always remember that a
    predictive model will never be 100% perfect but, if it is built with the right
    principles, it will be very useful!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 预测建模是一个困难的任务，尽管可能存在许多挑战，结果可能总是难以获得，但我们必须带着一点盐来接受这些挑战，并记住著名统计学家乔治·E·P·博克斯的名言：“本质上所有模型都是错误的，但其中一些是有用的！”这一点在我们之前讨论的内容中得到了充分的证实。始终记住，预测模型永远不会达到100%完美，但如果它是基于正确的原则构建的，它将非常有用！
- en: 'In this chapter, we will focus on the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注以下主题：
- en: Predictive analytics
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测分析
- en: How to predict credit risk
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何预测信用风险
- en: Important concepts in predictive modeling
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测建模中的重要概念
- en: Getting the data
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取数据
- en: Data preprocessing
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据预处理
- en: Feature selection
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择
- en: Modeling using logistic regression
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归建模
- en: Modeling using support vector machines
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持向量机建模
- en: Modeling using decision trees
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用决策树建模
- en: Modeling using random forests
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林建模
- en: Modeling using neural networks
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用神经网络建模
- en: Model comparison and selection
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型比较和选择
- en: Predictive analytics
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测分析
- en: We had already discussed a fair bit about predictive analytics in the previous
    chapter to give you a general overview of what it means. We will be discussing
    it in more detail in this section. Predictive analytics can be defined as a subset
    of the machine learning universe, which encompasses a wide variety of supervised
    learning algorithms based on data science, statistics, and mathematical formulae
    which enable us to build predictive models using these algorithms and data which
    has already been collected. These models enable us to make predictions of what
    might happen in the future based on past observations. Combining this with domain
    knowledge, expertise, and business logic enables analysts to make data driven
    decisions using these predictions, which is the ultimate outcome of predictive
    analytics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们已经就预测分析进行了相当多的讨论，以向您提供一个关于其含义的概述。在本节中，我们将更详细地讨论它。预测分析可以定义为机器学习领域的一个子集，它包括基于数据科学、统计学和数学公式的广泛监督学习算法，这些算法使我们能够使用这些算法和已经收集的数据来构建预测模型。这些模型使我们能够根据过去的观察预测未来可能发生的事情。结合领域知识、专业知识和商业逻辑，分析师可以使用这些预测进行数据驱动决策，这是预测分析最终的结果。
- en: The data we are talking about here is data which has already been observed in
    the past and has been collected over a period of time for analysis. This data
    is often known as historical data or training data which is fed to the model.
    However, most of the time in the predictive modeling methodology, we do not feed
    the raw data directly but use features extracted from the data after suitable
    transformations. The data features along with a supervised learning algorithm
    form a predictive model. The data which is obtained in the present can then be
    fed to this model to predict outcomes which are under observation and also to
    test the performance of the model with regards to various accuracy metrics. This
    data is known as testing data in the machine learning world.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里讨论的数据是过去已经观察到的数据，并且在一段时间内为了分析而收集的数据。这些数据通常被称为历史数据或训练数据，它们被输入到模型中。然而，在预测建模方法中，我们大多数时候并不直接输入原始数据，而是使用从数据中提取的特征，这些特征经过适当的转换。数据特征与监督学习算法结合形成一个预测模型。在当前获得的这些数据可以输入到这个模型中，以预测正在观察的结果，并测试模型在各个准确度指标方面的性能。在机器学习领域，这种数据被称为测试数据。
- en: 'The analytics pipeline that we will be following for carrying out predictive
    analytics in this chapter is a standard process, which is explained briefly in
    the following steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中执行预测分析将遵循的分析管道是一个标准流程，以下步骤简要解释：
- en: '**Getting the data**: Here we get the dataset on which we will be building
    the predictive model. We will perform some basic descriptive analysis of the dataset,
    which we have already covered in the previous chapter. Once we have the data we
    will move on to the next step.'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**获取数据**：在这里，我们获取构建预测模型所需的数据集。我们将对数据集进行一些基本的描述性分析，这已经在上一章中介绍过。一旦我们有了数据，我们就会进入下一步。'
- en: '**Data preprocessing**: In this step, we carry out data transformations, such
    as changing data types, feature scaling, and normalization, if necessary, to prepare
    the data for being trained by models. Usually this step is carried out after the
    dataset preparation step. However, in this case, the end results are the same,
    so we can perform these steps in any order.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据预处理**：在这个步骤中，我们执行数据转换，例如更改数据类型、特征缩放和归一化，如果需要，以准备数据供模型训练。通常这个步骤是在数据集准备步骤之后执行的。然而，在这种情况下，最终结果是一样的，因此我们可以按任何顺序执行这些步骤。'
- en: '**Dataset preparation**: In this step, we use some ratio like 70:30 or 60:40
    to separate the instances from the data into training and testing datasets. We
    usually use the training dataset to train a model and then check its performance
    and predicting capability with the testing dataset. Often data is divided in proportions
    of 60:20:20 where we also have a validation dataset besides the other two datasets.
    However, we will just keep it to two datasets in this chapter.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据集准备**：在这个步骤中，我们使用一些比例，如70:30或60:40，将数据实例从数据中分离成训练集和测试集。我们通常使用训练集来训练模型，然后使用测试集检查其性能和预测能力。通常数据按60:20:20的比例划分，我们除了其他两个数据集外，还有一个验证集。然而，在本章中，我们只保留两个数据集。'
- en: '**Feature selection**: This process is an iterative one which even occurs in
    a later stage if needed. The main objective in this step is to choose a set of
    attributes or features from the training dataset that enables the predictive model
    to give the best predictions possible, minimizing error rates and maximizing accuracy.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征选择**：这个过程是迭代的，如果需要，甚至在后续阶段也会发生。在这个步骤中的主要目标是选择一组属性或特征，从训练数据集中选择，使预测模型能够给出最佳预测，最小化错误率并最大化准确性。'
- en: '**Predictive modeling**: This is the main step where we select a machine learning
    algorithm best suited for solving the problem and build the predictive model using
    the algorithm by feeding it the features extracted from the data in the training
    dataset. The output of this stage is a predictive model which can be used for
    predictions on future data instances.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**预测建模**：这是主要步骤，我们选择最适合解决该问题的机器学习算法，并使用算法构建预测模型，通过向其提供从训练数据集中的数据中提取的特征来构建预测模型。这个阶段的输出是一个预测模型，可以用于对未来数据实例的预测。'
- en: '**Model evaluation**: In this phase, we use the testing dataset to get predictions
    from the predictive model and use a variety of techniques and metrics to measure
    the performance of the model.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型评估**：在这个阶段，我们使用测试数据集从预测模型中获得预测结果，并使用各种技术和指标来衡量模型的性能。'
- en: '**Model tuning**: We fine tune the various parameters of the model and perform
    feature selection again if necessary. We then rebuild the model and re-evaluate
    it until we are satisfied with the results.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型调优**：我们微调模型的各个参数，并在必要时再次进行特征选择。然后我们重新构建模型并重新评估它，直到我们对结果满意。'
- en: '**Model deployment**: Once the predictive model gives a satisfactory performance,
    we can deploy this model by using a web service in any application to provide
    predictions in real time or near real time. This step focuses more on software
    and application development around deploying the model, so we won''t be covering
    this step since it is out of scope. However, there are a lot of tutorials out
    there regarding building web services around predictive models to enable *Prediction
    as a service*.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模型部署**：一旦预测模型给出令人满意的表现，我们就可以通过在任何应用程序中使用Web服务来部署这个模型，以提供实时或近实时的预测。这个步骤更多地关注围绕部署模型进行的软件和应用开发，因此我们不会涉及这个步骤，因为它超出了范围。然而，关于围绕预测模型构建Web服务以实现“预测即服务”的教程有很多。'
- en: '**The last three steps are iterative and may be performed several times if
    needed**.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**最后三个步骤是迭代的，如果需要，可以执行多次**。'
- en: Even though the preceding process might look pretty intensive at first glance,
    it is really a very simple and straight-forward process, which once understood
    would be useful in building any type of predictive modeling. An important thing
    to remember is that predictive modeling is an iterative process where we might
    need to analyze the data and build the model several times by getting feedback
    from the model predictions and evaluating them. It is therefore extremely important
    that you do not get discouraged even if your model doesn't perform well on the
    first go because a model can never be perfect, as we mentioned before, and building
    a good predictive model is an art as well as science!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管乍一看这个过程可能看起来相当复杂，但实际上它是一个非常简单且直接的过程，一旦理解，就可以用于构建任何类型的预测模型。需要记住的一个重要事情是，预测建模是一个迭代的过程，我们可能需要通过从模型预测中获得反馈并评估它们来多次分析数据和构建模型。因此，即使你的模型在第一次尝试时表现不佳，你也绝不能气馁，因为正如我们之前提到的，模型永远不可能完美，构建一个好的预测模型既是艺术也是科学！
- en: In the next section, we will be focusing on how we would apply predictive analytics
    to solve our prediction problem and the kind of machine learning algorithms we
    will be exploring in this chapter.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将关注如何应用预测分析来解决我们的预测问题，以及在本章中我们将探索的机器学习算法类型。
- en: How to predict credit risk
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如何预测信用风险
- en: If you remember our main objective from the previous chapter, we were dealing
    with customer data from a German bank. We will quickly recap our main problem
    scenario to refresh your memory. These bank customers are potential candidates
    who ask for credit loans from the bank with the stipulation that they make monthly
    payments with some interest on the amount to repay the credit amount. In a perfect
    world there would be credit loans dished out freely and people would pay them
    back without issues. Unfortunately, we are not living in a utopian world, and
    so there will be customers who will default on their credit loans and be unable
    to repay the amount, causing huge losses to the bank. Therefore, credit risk analysis
    is one of the crucial areas which banks focus on where they analyze detailed information
    pertaining to customers and their credit history.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得上一章的主要目标，我们当时处理的是来自德国银行的客户数据。我们将快速回顾我们的主要问题场景以刷新你的记忆。这些银行客户是潜在的候选人，他们向银行申请信用贷款，条件是他们必须每月支付一定的利息来偿还贷款金额。在理想的世界里，信用贷款会被自由发放，人们会毫无问题地偿还它们。不幸的是，我们并不生活在一个乌托邦的世界，因此将会有一些客户会违约，无法偿还贷款金额，这会给银行造成巨大的损失。因此，信用风险分析是银行关注的几个关键领域之一，他们分析与客户及其信用历史相关的详细信息。
- en: Now coming back to the main question, for predicting credit risk, we need to
    analyze the dataset pertaining to customers, build a predictive model around it
    using machine learning algorithms, and predict whether a customer is likely to
    default on paying the credit loan and could be labeled as a potential credit risk.
    The process which we will follow for achieving this is what we discussed in the
    previous section. You already have an idea about the data and features associated
    with it from the previous chapter. We will explore several predictive models,
    understand the concepts behind how the models work, and then build these models
    for predicting credit risk. Once we start predicting the outcomes, we will compare
    the performance of these different models and then talk about the business impact
    and how to derive insights from the model prediction outcomes. Do note that the
    predictions are not the output in the predictive analytics life cycle but the
    valuable insights that we derive from these predictions is the end goal. Businesses
    such as financial institutions get value only from using domain knowledge to translate
    prediction outcomes and raw numbers from machine learning algorithms to data driven
    decisions, which, when executed at the right time, help grow the business.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在回到主要问题，对于预测信用风险，我们需要分析客户相关的数据集，使用机器学习算法围绕它构建预测模型，并预测客户是否可能拖欠信用贷款，并可能被标记为潜在的信用风险。我们将遵循的过程是我们在上一节中讨论的。你已经从上一章中了解了数据及其相关的特征。我们将探索几个预测模型，了解模型工作背后的概念，然后构建这些模型以预测信用风险。一旦我们开始预测结果，我们将比较这些不同模型的性能，然后讨论业务影响以及如何从模型预测结果中得出见解。请注意，预测不是预测分析生命周期中的输出，我们从这些预测中得出的宝贵见解才是最终目标。像金融机构这样的企业只能从使用领域知识将预测结果和机器学习算法的原始数字转换为数据驱动决策中获得价值，这些决策在正确的时间执行时有助于业务增长。
- en: 'For this scenario, if you remember the dataset well, the feature `credit.rating`
    is the response or class variable, which indicates the credit rating of the customers.
    We will be predicting this value for the other customers based on other features
    which are independent variables. For modeling, we will be using machine learning
    algorithms which belong to the supervised learning family of algorithms. These
    algorithms are used for predictions and can be divided into two broad categories:
    classification and regression. However, they have some differences which we will
    talk about now. In the case of regression, the values for the variables to be
    predicted are continuous values, like predicting prices of houses based on different
    features such as the number of rooms, the area of the house, and so on. Regression
    mostly deals with estimating and predicting a response value based on input features.
    In the case of classification, the values for the variables to be predicted have
    discrete and distinct labels, such as predicting the credit rating for customers
    for our bank, where the credit rating can either be good, which is denoted by
    `1` or bad, which is denoted by `0`. Classification mostly deals with categorizing
    and identifying group memberships for each data tuple in the dataset. Algorithms
    such as logistic regression are special cases of regression models which are used
    for classification, where the algorithm estimates the odds that a variable is
    in one of the class labels as a function of the other features. We will be building
    predictive models using the following machine learning algorithms in this chapter:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个场景，如果你对数据集记得很清楚，特征`credit.rating`是响应或类别变量，它表示客户的信用评级。我们将根据其他特征（独立变量）预测这个值，以预测其他客户的信用评级。在建模时，我们将使用属于监督学习算法家族的机器学习算法。这些算法用于预测，可以分为两大类：分类和回归。然而，它们有一些区别，我们现在将讨论。在回归的情况下，要预测的变量的值是连续值，例如根据不同的特征（如房间数量、房屋面积等）预测房屋价格。回归主要处理基于输入特征估计和预测响应值。在分类的情况下，要预测的变量的值具有离散且独特的标签，例如预测我们银行的客户信用评级，其中信用评级可以是好的，用`1`表示，或者坏，用`0`表示。分类主要处理对数据集中的每个数据元进行分类和识别组成员资格。逻辑回归等算法是回归模型的特殊情况，用于分类，其中算法将变量属于某个类别标签的概率估计为其他特征的函数。在本章中，我们将使用以下机器学习算法构建预测模型：
- en: Logistic regression
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: Support vector machines
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: Decision trees
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Random forests
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林
- en: Neural networks
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络
- en: We have chosen these algorithms to give a good flavor of the diverse set of
    supervised machine learning algorithms which are present, so that you gain knowledge
    not only about the concepts behind these models but also learn to implement building
    models using them, and compare model performances using various techniques. Before
    we begin our analysis, we will glance over some basic concepts in predictive modeling
    that are mentioned in this book and talk about some of them in detail so you get
    a good idea of what goes on behind the scenes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择这些算法是为了展示现有的各种监督机器学习算法的多样性，这样你不仅可以了解这些模型背后的概念，还可以学习如何使用它们构建模型，并使用各种技术比较模型性能。在我们开始分析之前，我们将简要回顾本书中提到的预测建模的一些基本概念，并详细讨论其中的一些，以便你对幕后发生的事情有一个良好的了解。
- en: Important concepts in predictive modeling
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预测建模中的重要概念
- en: We already looked at several concepts when we talked about the machine learning
    pipeline. In this section, we will look at typical terms which are used in predictive
    modeling, and also discuss about model building and evaluation concepts in detail.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论机器学习流程时，已经探讨了几个概念。在本节中，我们将探讨预测建模中常用的典型术语，并详细讨论模型构建和评估概念。
- en: Preparing the data
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'The data preparation step, as discussed earlier, involves preparing the datasets
    necessary for feature selection and building the predictive models using the data.
    We frequently use the following terms in this context:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备步骤，如前所述，涉及准备用于特征选择和构建预测模型所需的数据集。在这个背景下，我们经常使用以下术语：
- en: '**Datasets**: They are typically a collection of data points or observations.
    Most datasets usually correspond to some form of structured data which involves
    a two dimensional data structure, such as a data matrix or data table (in R this
    is usually represented using a data frame) containing various values. An example
    is our `german_credit_dataset.csv` file from [Chapter 5](part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8
    "Chapter 5. Credit Risk Detection and Prediction – Descriptive Analytics"), *Credit
    Risk Detection and Prediction – Descriptive Analytics*.'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：它们通常是数据点的集合或观察结果。大多数数据集通常对应于某种形式的结构化数据，涉及二维数据结构，如数据矩阵或数据表（在R中通常使用数据框表示）包含各种值。例如，我们来自[第5章](part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8
    "第5章。信用风险检测与预测 – 描述性分析")的`german_credit_dataset.csv`文件，*信用风险检测与预测 – 描述性分析*。'
- en: '**Data observations**: They are the rows in a dataset where each row consists
    of a set of observations against a set of attributes. These rows are also often
    called tuples. For our dataset, each row containing information about a customer
    is a good example.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据观察**：它们是数据集中的行，其中每行包含一组观察结果与一组属性。这些行也常被称为元组。对于我们数据集，包含有关客户信息的每行都是一个很好的例子。'
- en: '**Data features**: They are the columns in a dataset which describe each row
    in the dataset. These features are often called attributes or variables. Features
    such as `credit.rating`, `account.balance`, and so on form the features of our
    credit risk dataset.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据特征**：它们是数据集中的列，描述数据集中的每一行。这些特征通常被称为属性或变量。例如`credit.rating`、`account.balance`等特征构成了我们的信用风险数据集的特征。'
- en: '**Data transformation**: It refers to the act of transforming various data
    features as needed based on observations from descriptive analytics. Data type
    conversions, missing values imputation, and scaling and normalization are some
    of the most used techniques. Also, for categorical variables, if your algorithms
    are not able to detect the different levels in the variable, you need to convert
    it to several dummy variables; this process is known as one-hot encoding.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换**：它指的是根据描述性分析中的观察结果，根据需要转换各种数据特征。数据类型转换、缺失值插补、缩放和归一化是最常用的技术。此外，对于分类变量，如果你的算法无法检测变量的不同级别，你需要将其转换为几个虚拟变量；这个过程被称为独热编码。'
- en: '**Training data**: It refers to the data which is solely used to train the
    predictive models. The machine learning algorithm picks up the tuples from this
    dataset and tries to find out patterns and learn from the various observation
    instances.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：它指的是仅用于训练预测模型的数据。机器学习算法从这个数据集中提取元组，并试图从各种观察实例中找出模式和进行学习。'
- en: '**Testing data**: It refers to the data which is fed to the predictive model
    to get predictions and then we check the accuracy of the model using the class
    labels which are already present in the tuples for this dataset. We never train
    the model with the testing data because it would bias the model and give incorrect
    evaluations.'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据**：它指的是输入到预测模型中以获取预测结果的数据，然后我们使用该数据集中已经存在的类别标签来检查模型的准确性。我们从不使用测试数据来训练模型，因为这会偏置模型并给出不正确的评估。'
- en: Building predictive models
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建预测模型
- en: 'We build the actual predictive models using machine learning algorithms and
    data features which finally start giving out predictions as we feed it new data
    tuples. Some concepts associated with building predictive models are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用机器学习算法和数据特征来构建实际的预测模型，并最终在输入新的数据元组时开始给出预测。与构建预测模型相关的概念如下：
- en: '**Model training**: It is analogous to building the predictive model where
    we use a supervised machine learning algorithm and feed the training data features
    to it and build the predictive model.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型训练**：它与构建预测模型类似，其中我们使用监督机器学习算法，并将训练数据特征输入其中来构建预测模型。'
- en: '**Predictive model**: It is based on some machine learning algorithm, which
    is essentially a mathematical model at heart, with some assumptions, formulae,
    and parameter values.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测模型**：它基于某种机器学习算法，本质上是一个数学模型，具有一些假设、公式和参数值。'
- en: '**Model selection**: It is a process where the main objective is to select
    a predictive model from several iterations of predictive models. The criteria
    for selecting the best model can vary, depending on the metrics we want to choose,
    such as maximizing the accuracy, minimizing the error rate, or getting the maximum
    AUC, which is something we will discuss later. Cross-validation is a good way
    to run this iterative process.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型选择**：这是一个过程，其主要目标是从多个预测模型的迭代中选择一个预测模型。选择最佳模型的标准可能因我们想要选择的指标而异，例如最大化准确性、最小化错误率或获得最大的AUC（我们将在后面讨论）。交叉验证是运行此迭代过程的好方法。'
- en: '**Hyperparameter optimization**: It is basically trying to choose a set of
    the hyperparameters used by the algorithm in the model such that the performance
    of the model is optimal with regards to its prediction accuracy. This is usually
    done by a grid search algorithm.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**超参数优化**：这基本上是尝试选择算法在模型中使用的一组超参数，使得模型的预测准确性最优。这通常通过网格搜索算法来完成。'
- en: '**Cross validation**: It is a model validation technique which is used to estimate
    how a model would perform in a generic fashion. It is mainly used in iterative
    processes where the end goal is to optimize the model and make sure it is not
    over fit to the data so that the model can generalize well with new data and make
    good predictions. Usually, several rounds of cross validation are run iteratively.
    Each round of cross validation involves splitting the data into train and test
    sets; using the training data to train the model and then evaluating its performance
    with the test set. At the end of this, we get a model which is the best of the
    lot.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：这是一种模型验证技术，用于估计模型以通用方式的表现。它主要用于迭代过程，其中最终目标是优化模型并确保模型不会过度拟合数据，以便模型能够很好地泛化新数据并做出良好的预测。通常，会进行多轮交叉验证。每一轮交叉验证都涉及将数据分为训练集和测试集；使用训练数据来训练模型，然后使用测试集评估其性能。最终，我们得到一个模型，它是所有模型中最好的。'
- en: Evaluating predictive models
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估预测模型
- en: 'The most important part in predictive modeling is testing whether the models
    created are actually useful. This is done by evaluating the models on the testing
    data and using various metrics to measure the performance of the model. We will
    discuss some popular model evaluation techniques here. To explain the concepts
    clearly, we will consider an example with our data. Let us assume we have 100
    customers and 40 of them have a bad credit rating with class label 0 and the remaining
    60 have a good credit rating with class label 1 in the test data. Let us now assume
    that our model predicts 22 instances out of the 40 bad instances as bad and the
    remaining 18 as good. The model also predicts 40 instances out of the 60 good
    customers as good and the remaining 20 as bad. We will now see how we evaluate
    the model performance with different techniques:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 预测建模中最重要的部分是测试创建的模型是否真正有用。这是通过在测试数据上评估模型并使用各种指标来衡量模型性能来完成的。我们将在下面讨论一些流行的模型评估技术。为了清楚地解释这些概念，我们将考虑一个与我们的数据相关的例子。让我们假设我们有100名客户，其中40名客户的信用评级不良，类别标签为0，剩余的60名客户的信用评级良好，类别标签为1。现在假设我们的模型将40个不良实例中的22个预测为不良，其余的18个预测为良好。模型还将60名良好客户中的40个预测为良好，其余的20个预测为不良。现在我们将看到我们将如何使用不同的技术来评估模型性能：
- en: '**Prediction values**: They are usually discrete values which belong to a specific
    class or category and are often known as class labels. In our case, it is a binary
    classification problem where we deal with two classes where label 1 indicates
    customers with good credit rating and 0 indicates bad credit rating.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测值**：它们通常是离散值，属于特定的类别或类别，通常被称为类别标签。在我们的案例中，这是一个二元分类问题，我们处理两个类别，其中标签1表示信用评级良好的客户，0表示信用评级不良。'
- en: '**Confusion matrix**: It is a nice way to see how the model is predicting the
    different classes. It is a contingency table with usually two rows and two columns
    for a binary classification problem like ours. It reports the number of predicted
    instances in each class against the actual class values. For our preceding example,
    the confusion matrix would be a 2x2 matrix where two rows would indicate the predicted
    class labels and two columns would indicate the actual class labels. The total
    number of predictions with the bad (0) class label which are actually having the
    bad label is called **True Negative** (**TN**) and the remaining bad instances
    wrongly predicted as good are called **False Positive** (**FP**). Correspondingly,
    the total number of predictions with the good (1) class label that are actually
    labeled as good are called **True Positive** (**TP**) and the remaining good instances
    wrongly predicted as bad are called **False Negative** (**FN**).'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混淆矩阵**：这是一种很好的方式来查看模型是如何预测不同类别的。它是一个通常有两行两列的列联表，用于我们这样的二元分类问题。它报告了每个类别中预测实例的数量与实际类别值。对于我们的先前列举的例子，混淆矩阵将是一个2x2的矩阵，其中两行将表示预测的类别标签，两列将表示实际的类别标签。总共有坏（0）类别标签的预测实例数，实际上具有坏标签，被称为**真负**（**TN**），而剩余的错误预测为好的坏实例被称为**假正**（**FP**）。相应地，总共有好（1）类别标签的预测实例数，实际上被标记为好的，被称为**真正**（**TP**），而剩余的错误预测为坏的好的实例被称为**假负**（**FN**）。'
- en: 'We will depict this in the following figure and discuss some important metrics
    derived from the confusion matrix, also depicted in the same figure:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们将在以下图中展示这一点，并讨论从混淆矩阵中导出的一些重要指标，这些指标也将在同一图中展示：
- en: '![Evaluating predictive models](img/00163.jpeg)'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![评估预测模型](img/00163.jpeg)'
- en: 'In the preceding figure, the values which are highlighted in the 2x2 matrix
    are the ones which were correctly predicted by our model. The ones in white were
    incorrectly predicted by the model. We can therefore infer the following measures
    quite easily: TN is 22, **FP** is **18**, **TP** is **40**, and **FN** is **20**.
    Total **N** is **40** and total P is **60**, which add up to 100 customers in
    our example dataset.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在先前的图中，2x2矩阵中突出显示的值是我们模型正确预测的值。白色中的值是模型错误预测的。因此，我们可以很容易地推断以下指标：TN是22，**FP**是**18**，**TP**是**40**，**FN**是**20**。总**N**是**40**，总**P**是**60**，在我们的示例数据集中总和为100名客户。
- en: '**Specificity** is also known as **true negative rate**, and can be represented
    by the formula ![Evaluating predictive models](img/00164.jpeg), which gives us
    the proportion of total true negatives correctly predicted by the total number
    of instances which are actually negative. In our case, we have a specificity of
    **55%**.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '**特异性**也称为**真阴性率**，可以用公式![评估预测模型](img/00164.jpeg)表示，它给出了总真阴性数被正确预测的比例，这些真阴性数是所有实际为负的实例总数。在我们的案例中，特异性为**55%**。'
- en: '**Sensitivity**, also known as **true positive rate** and **recall**, has the
    formula ![Evaluating predictive models](img/00165.jpeg), which indicates the proportion
    of total true positives correctly predicted by the total number of instances which
    are actually positive. Our example has a sensitivity of **67%**.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**灵敏度**，也称为**真阳性率**和**召回率**，其公式为![评估预测模型](img/00165.jpeg)，表示在所有实际为正的实例中，正确预测的总真阳性数所占的比例。我们的例子中灵敏度为**67%**。'
- en: '**Precision**, also known as **positive predictive value**, has the formula
    ![Evaluating predictive models](img/00166.jpeg), which indicates the number of
    actual positive instances out of all the positive predictions. Our example has
    a precision of **69%**.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '**精确率**，也称为**阳性预测值**，其公式为![评估预测模型](img/00166.jpeg)，表示在所有正预测中实际正实例的数量。我们的例子中精确率为**69%**。'
- en: '**Negative** **predictive value** has the formula ![Evaluating predictive models](img/00167.jpeg),
    which indicates the number of actual negative instances out of all the negative
    predictions. Our example has an NPV of **52%**.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**负预测值**的公式为![评估预测模型](img/00167.jpeg)，表示在所有负预测中实际负实例的数量。我们的例子中NPV为**52%**。'
- en: '**False** **positive rate**, also known as **fall-out**, is basically the inverse
    of specificity; where the formula is ![Evaluating predictive models](img/00168.jpeg),
    which indicates the number of false positive predictions out of all the negative
    instances. Our example has an FPR of **45%**.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**假阳性率**，也称为**误报率**，基本上是特异性的倒数；其公式为![评估预测模型](img/00168.jpeg)，表示在所有负例中错误正预测的数量。我们的例子中FPR为**45%**。'
- en: '**False** **Negative Rate**, also known as **miss rate**, is basically the
    inverse of sensitivity; where the formula is ![Evaluating predictive models](img/00169.jpeg),
    which indicates the number of false negative predictions out of all the positive
    instances. Our example has an FNR of **33%**.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**假阴性率**，也称为**漏报率**，基本上是敏感度的倒数；其公式为![评估预测模型](img/00169.jpeg)，表示在所有正例中错误负预测的数量。我们的例子中FNR为**33%**。'
- en: '**Accuracy** is basically the metric which denotes how accurate the model is
    in making predictions, where the formula is ![Evaluating predictive models](img/00170.jpeg).
    Our prediction accuracy is **62%**.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**准确率**基本上是衡量模型在做出预测时准确性的指标，其公式为![评估预测模型](img/00170.jpeg)。我们的预测准确率为**62%**。'
- en: '**F1** score is another metric of measuring a model''s accuracy. It takes into
    account both the precision and recall values by computing the harmonic mean of
    the values, depicted by the formula ![Evaluating predictive models](img/00171.jpeg).
    Our model has an **f1** score of **68%**.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**F1**分数是衡量模型准确性的另一个指标。它通过计算值的调和平均值来考虑精确率和召回率，公式表示为![评估预测模型](img/00171.jpeg)。我们的模型f1分数为**68%**。'
- en: 'A **Receiver** **Operator Characteristic** (**ROC**) curve is basically a plot
    which is used to visualize the model performance as we vary its threshold. The
    ROC plot is defined by the FPR and TPR as the *x* and *y* axes respectively, and
    each prediction sample can be fit as a point in the ROC space. Perfect plot would
    involve a TPR of 1 and an FPR of 0 for all the data points. An average model or
    a baseline model would be a diagonal straight line from *(0, 0)* to *(1, 1)* indicating
    both values to be `0.5`. If our model has an ROC curve above the base diagonal
    line, it indicates that it is performing better than the baseline. The following
    figure explains how a typical ROC curve looks like in general:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**接收者操作特征**（**ROC**）曲线基本上是一个图表，用于可视化模型性能，当我们改变其阈值时。ROC图由FPR和TPR作为*x*轴和*y*轴分别定义，每个预测样本都可以拟合为ROC空间中的一个点。完美的图表将涉及所有数据点的TPR为1和FPR为0。一个平均模型或基线模型将是一条从*(0,
    0)*到*(1, 1)*的对角线，表示两个值都是`0.5`。如果我们的模型ROC曲线在基线对角线之上，则表明其性能优于基线。以下图解展示了典型的ROC曲线在一般情况下的样子：'
- en: '![Evaluating predictive models](img/00172.jpeg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![评估预测模型](img/00172.jpeg)'
- en: '**Area under curve** (**AUC**) is basically the area under the ROC curve obtained
    from the model evaluation. The AUC is a value which indicates the probability
    that the model will rank a randomly chosen positive instance higher than a randomly
    chosen negative one. Therefore, the higher the AUC, the better it is. Do check
    out the file `performance_plot_utils.R` (shared with the code bundle of the chapter),
    which has some utility functions to plot and depict these values that we will
    be using later when we evaluate our model.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '**曲线下面积**（**AUC**）基本上是从模型评估中获得的ROC曲线下的面积。AUC是一个值，表示模型将随机选择的正实例排名高于随机选择的负实例的概率。因此，AUC越高，越好。请查看文件`performance_plot_utils.R`（与章节代码包共享），其中包含一些实用函数，用于绘制和描述我们稍后评估模型时将使用的这些值。'
- en: This should give you enough background on important terms and concepts related
    to predictive modeling, and now we will start with our predictive analysis on
    the data!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该为你提供了关于预测建模中重要术语和概念足够的背景知识，现在我们将开始对数据进行预测分析！
- en: Getting the data
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'In [Chapter 5](part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8
    "Chapter 5. Credit Risk Detection and Prediction – Descriptive Analytics"), *Credit
    Risk Detection and Prediction – Descriptive Analytics*, we had analyzed the credit
    dataset from the German bank and performed several transformations already. We
    will be working on that transformed dataset in this chapter. We had saved the
    transformed dataset which you can check out by opening the `credit_dataset_final.csv`
    file. We will be doing all our analysis in R as usual. To load the data in memory,
    run the following code snippet:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第5章](part0038_split_000.html#147LC1-973e731d75c2419489ee73e3a0cf4be8 "第5章。信用风险检测与预测
    – 描述性分析")中，*信用风险检测与预测 – 描述性分析*，我们已经分析了德国银行的信用数据集并进行了几个转换。在本章中，我们将处理这个转换后的数据集。我们已经保存了转换后的数据集，你可以通过打开`credit_dataset_final.csv`文件来查看。我们将像往常一样在R中进行所有分析。要加载数据到内存中，请运行以下代码片段：
- en: '[PRE0]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This loads the dataset into a data frame which can now be readily accessed using
    the `credit.df` variable. Next, we will focus on data transformation and normalization.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这将数据集加载到一个数据框中，现在可以通过`credit.df`变量轻松访问。接下来，我们将关注数据转换和归一化。
- en: Data preprocessing
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'In the data preprocessing step, we will be focusing on two things mainly: data
    type transformations and data normalization. Finally we will split the data into
    training and testing datasets for predictive modeling. You can access the code
    for this section in the `data_preparation.R` file. We will be using some utility
    functions, which are mentioned in the following code snippet. Remember to load
    them up in memory by running them in the R console:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据预处理步骤中，我们将主要关注两个方面：数据类型转换和数据归一化。最后，我们将数据分割成训练集和测试集以进行预测建模。你可以通过打开`data_preparation.R`文件来访问这一部分的代码。我们将使用一些实用函数，这些函数在下面的代码片段中提到。请记住，通过在R控制台中运行它们来将它们加载到内存中：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding functions operate on the data frame to transform the data. For
    data type transformations, we mainly perform factoring of the categorical variables,
    where we transform the data type of the categorical features from numeric to factor.
    There are several numeric variables, which include `credit.amount`, `age`, and
    `credit.duration.months`, which all have various values and if you remember the
    distributions in the previous chapter, they were all skewed distributions. This
    has multiple adverse effects, such as induced collinearity, gradients being affected,
    and models taking longer times to converge. Hence, we will be using z-score normalization,
    where each value represented by, let''s say, ![Data preprocessing](img/00173.jpeg),
    for a feature named E, can be calculated using the formula ![Data preprocessing](img/00174.jpeg)
    where ![Data preprocessing](img/00175.jpeg) represents the overall mean and ![Data
    preprocessing](img/00176.jpeg) represents the standard deviation of the feature
    `E`. We use the following code snippet to perform these transformations on our
    data:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的函数在数据框上操作以转换数据。对于数据类型转换，我们主要对分类变量进行分解，即将分类特征的类型从数值转换为因子。有几个数值变量，包括`credit.amount`、`age`和`credit.duration.months`，它们都有各种值。如果你还记得上一章中的分布，它们都是偏斜分布。这有多重不利影响，如引起共线性、梯度受到影响以及模型收敛时间更长。因此，我们将使用z分数标准化，其中，例如，一个名为E的特征的值，可以用以下公式计算：![数据预处理](img/00173.jpeg)，其中![数据预处理](img/00174.jpeg)代表特征E的整体均值，![数据预处理](img/00175.jpeg)代表特征E的标准差。我们使用以下代码片段来对我们的数据进行这些转换：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Once the preprocessing is complete, we will split our data into training and
    test datasets in the ratio of 60:40, where 600 tuples will be in the training
    dataset and 400 tuples will be in the testing dataset. They will be selected in
    a random fashion as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理完成后，我们将数据集分成训练集和测试集，比例为60:40，其中训练集将包含600个元组，测试集将包含400个元组。它们将以以下随机方式选择：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have our datasets ready, we will explore feature importance and
    selection in the following section.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了数据集，我们将在下一节中探讨特征的重要性和选择。
- en: Feature selection
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征选择
- en: The process of feature selection involves ranking variables or features according
    to their importance by training a predictive model using them and then trying
    to find out which variables were the most relevant features for that model. While
    each model often has its own set of important features, for classification we
    will use a random forest model here to try and figure out which variables might
    be of importance in general for classification-based predictions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择的过程涉及通过使用它们训练预测模型并根据其重要性对变量或特征进行排名，然后试图找出哪些变量是该模型中最相关的特征。虽然每个模型通常都有自己的重要特征集，但在分类中，我们将在这里使用随机森林模型来尝试确定哪些变量可能在基于分类的预测中普遍重要。
- en: 'We perform feature selection for several reasons, which include:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行特征选择有几个原因，包括：
- en: Removing redundant or irrelevant features without too much information loss
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不损失太多信息的情况下移除冗余或不相关的特征
- en: Preventing overfitting of models by using too many features
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用过多的特征防止模型过拟合
- en: Reducing variance of the model which is contributed from excess features
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少由过多特征引起的模型方差
- en: Reducing training time and converging time of models
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少模型的训练时间和收敛时间
- en: Building simple and easy to interpret models
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建简单且易于理解的模型
- en: We will be using a recursive feature elimination algorithm for feature selection
    and an evaluation algorithm using a predictive model where we repeatedly construct
    several machine learning models with different features in different iterations.
    At each iteration, we keep eliminating irrelevant or redundant features and check
    the feature subset for which we get maximum accuracy and minimum error. Since
    this is an iterative process and follows the principle of the popular greedy hill
    climbing algorithm, an exhaustive search with a global optima outcome is generally
    not possible and depending on the starting point, we may end up at local optima
    with a subset of features which may be different from the subset of features we
    obtain in a different run. However, most of the features in the obtained subset
    will usually be constant if we run it several times using cross-validation. We
    will use the random forest algorithm, which we will explain in more detail later
    on. For now, just remember it is an ensemble learning algorithm that uses several
    decision trees at each stage in its training process. This tends to reduce variance
    and overfitting with a small increase towards bias of the model since we introduce
    some randomness into this process at each stage in the algorithm.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用递归特征消除算法进行特征选择，并使用预测模型进行评估算法。在这个过程中，我们将反复构建几个具有不同特征的机器学习模型。在每次迭代中，我们都会消除无关或冗余的特征，并检查获得最大准确率和最小误差的特征子集。由于这是一个迭代过程，遵循流行的贪婪爬山算法的原则，通常不可能进行穷举搜索并得到全局最优解，而且根据起始点不同，我们可能会陷入局部最优解，得到的特征子集可能与不同运行中获得的特征子集不同。然而，如果我们多次使用交叉验证运行它，通常大多数特征在获得的子集中将是恒定的。我们将使用随机森林算法，我们将在稍后详细解释。现在，只需记住它是一个集成学习算法，在其训练过程的每个阶段都使用多个决策树。这倾向于减少方差和过拟合，同时由于我们在算法的每个阶段引入了一些随机性，模型偏差略有增加。
- en: 'The code for this section is present in the `feature_selection.R` file. We
    will first load the necessary libraries. Install them in case you do not have
    them installed, as we did in the previous chapters:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 本节代码位于`feature_selection.R`文件中。我们首先加载必要的库。如果你还没有安装它们，按照我们在前几章的做法进行安装：
- en: '[PRE4]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By default, the preceding code uses cross-validation where the data is split
    into training and test sets. For each iteration, recursive feature elimination
    takes place and the model is trained and tested for accuracy and errors on the
    test set. The data partitions keep changing randomly for every iteration to prevent
    overfitting of the model and ultimately give a generalized estimate of how the
    model would perform in a generic fashion. If you observe, our function runs it
    for 20 iterations by default. Remember, in our case, we always train on the training
    data which is internally partitioned for cross-validation by the function. The
    variable `feature.vars` indicate all the independent feature variables that can
    be accessed in the training dataset using the `train.data[,-1]` subsetting, and
    to access the `class.var`,which indicates the class variable to be predicted,
    we subset using `train.data[,1]`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，前面的代码使用交叉验证，将数据分为训练集和测试集。对于每次迭代，都会进行递归特征消除，并在测试集上对模型进行训练和测试，以检查准确率和误差。数据分区在每次迭代中都会随机变化，以防止模型过拟合，并最终给出一个通用的估计，即模型在一般情况下的表现。如果你观察，我们的函数默认运行20次迭代。记住，在我们的情况下，我们总是在训练数据上训练，该数据由函数内部分区进行交叉验证。变量`feature.vars`表示在训练数据集中可以使用`train.data[,-1]`子集访问的所有独立特征变量，要访问表示要预测的类变量的`class.var`，我们使用`train.data[,1]`进行子集操作。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: We do not touch the test data at all because we will be using it only for predictions
    and model evaluations. Therefore, we would not want to influence the model by
    using that data since it would lead to incorrect evaluations.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们完全不接触测试数据，因为我们只打算用它来进行预测和模型评估。因此，我们不想通过使用这些数据来影响模型，因为这会导致评估结果不准确。
- en: 'We now run the algorithm using our defined function on the training data using
    the following code. It may take some time to run, so be patient if you see that
    R is taking some time to return the results:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用定义好的函数在训练数据上运行算法，以下代码展示了这一过程。运行可能需要一些时间，所以如果你看到R在返回结果时花费了一些时间，请耐心等待：
- en: '[PRE7]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'On viewing the results, we get the following output:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 查看结果后，我们得到以下输出：
- en: '![Feature selection](img/00177.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![特征选择](img/00177.jpeg)'
- en: From the output, you can see that it has found a total of 10 features that were
    the most important out of the 20 and it has returned the top five features by
    default. You can play around with this result variable even further and see all
    the variables with their importance by using the `varImp(rfe.results)` command
    in the R console. The values and importance values may differ for you because
    the training and test data partitions are done randomly, if you remember, so do
    not panic if you see different values from the screenshot. However, the top five
    features will usually remain consistent based on our observations. We will now
    start building predictive models using the different machine learning algorithms
    for the next stage of our analytics pipeline. However, do remember that since
    the training and test sets are randomly chosen, your sets might give slightly
    different results than what we depict here when we performed these experiments.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 从输出中，你可以看到它从20个特征中找到了最重要的10个特征，并且默认返回了前五个特征。你可以进一步玩转这个结果变量，并使用R控制台中的`varImp(rfe.results)`命令查看所有变量的重要性。由于训练和测试数据分区是随机进行的，如果你记得，所以如果你看到与截图不同的值，请不要慌张。然而，根据我们的观察，前五个特征通常会保持一致。现在，我们将开始构建预测模型，使用不同的机器学习算法进行我们分析管道的下一阶段。然而，请记住，由于训练和测试集是随机选择的，你的集合可能给出的结果与我们在这里进行实验时描述的不同。
- en: Modeling using logistic regression
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用逻辑回归建模
- en: Logistic regression is a type of regression model where the dependent or class
    variable is not continuous but categorical, just as in our case, credit rating
    is the dependent variable with two classes. In principle, logistic regression
    is usually perceived as a special case of the family of generalized linear models.
    This model functions by trying to find out the relationship between the class
    variable and the other independent feature variables by estimating probabilities.
    It uses the logistic or sigmoid function for estimating these probabilities. Logistic
    regression does not predict classes directly but the probability of the outcome.
    For our model, since we are dealing with a binary classification problem, we will
    be dealing with binomial logistic regression.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一种回归模型，其中因变量或类别变量不是连续的，而是分类的，就像在我们的案例中，信用评级是具有两个类别的因变量。原则上，逻辑回归通常被视为广义线性模型家族的一个特例。该模型通过估计概率来尝试找出类别变量与其他独立特征变量之间的关系。它使用逻辑或Sigmoid函数来估计这些概率。逻辑回归不直接预测类别，而是预测结果的概率。对于我们的模型，由于我们处理的是一个二元分类问题，我们将处理二项逻辑回归。
- en: 'First we will load the library dependencies as follows and separate the testing
    feature and class variables:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将如下加载库依赖项，并分别分离测试特征和类别变量：
- en: '[PRE8]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now we will train the initial model with all the independent variables as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用所有独立变量训练初始模型如下：
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can view the model details using the `summary(lr.model)` command, which
    shows you the various variables and their importance based on their significance
    values. We show a part of these details in the following snapshot:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`summary(lr.model)`命令查看模型细节，该命令显示基于其显著性值的各个变量及其重要性。我们在以下快照中展示了这些细节的一部分：
- en: '![Modeling using logistic regression](img/00178.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归建模](img/00178.jpeg)'
- en: You can see that the model automatically performs one-hot encoding of categorical
    variables, which is basically having a variable for each category in that variable.
    The variables with stars beside them have p-values `< 0.05` (which we discussed
    in the previous chapter) and are therefore significant.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到模型自动对分类变量执行了一元编码，这基本上是在该变量中为每个类别都有一个变量。旁边带有星号的变量具有p值`< 0.05`（我们在上一章中讨论过），因此是显著的。
- en: 'Next, we perform predictions on the test data and evaluate the results as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在测试数据上执行预测，并如下评估结果：
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: On running this, we get a confusion matrix with associated metrics, which we
    discussed earlier, which are shown in the following figure. It is quite interesting
    to see that we achieved an overall accuracy of **71.75%**, which is quite decent,
    considering this dataset has a majority of good credit rating customers. It is
    predicting bad credit ratings quite well, which is evident from the **specificity**
    of **48%**. **Sensitivity** is **83%**, which is quite good, **NPV** is **58%**,
    and **PPV** is **76%**.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，我们得到一个混淆矩阵以及相关的指标，我们之前已经讨论过，如下所示。看到我们实现了**71.75%**的整体准确率，这相当不错，考虑到这个数据集大多数客户都有良好的信用评级。它对不良信用评级的预测相当准确，这从**48%**的**特异性**中可以看出。**灵敏度**为**83%**，相当不错，**NPV**为**58%**，**PPV**为**76%**。
- en: '![Modeling using logistic regression](img/00179.jpeg)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归建模](img/00179.jpeg)'
- en: 'We will now try to build another model with some selected features and see
    how it performs. If you remember, we had some generic features that are important
    for classification, which we obtained in the earlier section on feature selection.
    We will still run feature selection specifically for logistic regression to see
    feature importance using the following code snippet:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将尝试使用一些选定的特征构建另一个模型，并看看它的表现如何。如果你还记得，我们在特征选择部分获得了一些对分类很重要的通用特征。我们仍然会为逻辑回归运行特征选择，以使用以下代码片段查看特征的重要性：
- en: '[PRE11]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: We get the following plot from which we select the top five variables to build
    the next model. As you can see, reading the plot is pretty simple. The greater
    the importance, the more important the variable is. Feel free to add more variables
    and build different models using them!
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从以下图中选择了前五个变量来构建下一个模型。正如你所见，阅读这个图相当简单。重要性越大，变量就越重要。你可以自由地添加更多变量，并使用它们构建不同的模型！
- en: '![Modeling using logistic regression](img/00180.jpeg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归建模](img/00180.jpeg)'
- en: 'Next, we build the model using a similar approach to before and test the model
    performance on the test data using the following code snippet:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用与之前类似的方法构建模型，并使用以下代码片段在测试数据上测试模型性能：
- en: '[PRE12]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We get the following confusion matrix. However, if you look at the model evaluation
    results, as shown in following output, you will see that now accuracy has slightly
    increased and is **72.25%**. **Sensitivity** has shot up to **94%**, which is
    excellent, but sadly this has happened at the cost of specificity, which has gone
    down to **27%**, and you can clearly see that more bad credit ratings are being
    predicted as good, which is 95 out of the total 130 bad credit rating customers
    in the test data! **NPV** has gone up to **69%** because fewer positive credit
    ratings are being misclassified as false negatives because of higher sensitivity.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下混淆矩阵。然而，如果你查看模型评估结果，如以下输出所示，你会发现现在准确率略有提高，达到了**72.25%**。**灵敏度**大幅上升到**94%**，这是非常好的，但遗憾的是，这是以特异性下降为代价的，特异性下降到**27**%，你可以清楚地看到，更多的不良信用评级被预测为好，这在测试数据中的130个不良信用评级客户中有95个！**NPV**上升到**69%**，因为由于灵敏度更高，较少的正信用评级被错误地分类为假阴性。
- en: '![Modeling using logistic regression](img/00181.jpeg)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归建模](img/00181.jpeg)'
- en: Now comes the question of which model we want to select for predictions. This
    does not solely depend on the accuracy but on the domain and business requirements
    of the problem. If we predict a customer with a **bad credit rating** (**0**)
    as **good** (**1**), it means we are going to approve the credit loan for the
    customer who will end up not paying it, which will cause losses to the bank. However,
    if we predict a customer with **good credit rating** (**1**) as **bad** (**0**),
    it means we will deny him the loan in which case the bank will neither profit
    nor will incur any losses. This is much better than incurring huge losses by wrongly
    predicting bad credit ratings as good.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是我们要选择哪个模型进行预测。这不仅仅取决于准确率，还取决于问题的领域和业务需求。如果我们预测一个客户的不良信用评级（**0**）为**好**（**1**），这意味着我们将批准该客户的信用贷款，而该客户最终可能不会偿还，这将给银行造成损失。然而，如果我们预测一个客户的良好信用评级（**1**）为**坏**（**0**），这意味着我们将拒绝他的贷款，在这种情况下，银行既不会盈利也不会遭受任何损失。这比错误地将不良信用评级预测为好要安全得多。
- en: 'Therefore, we choose our first model as the best one and now we will view some
    metric evaluation plots using the following code snippet:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们选择我们的第一个模型作为最佳模型，现在我们将使用以下代码片段查看一些指标评估图：
- en: '[PRE13]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We get the following plots from the preceding code:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从前面的代码中得到了以下图：
- en: '![Modeling using logistic regression](img/00182.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![使用逻辑回归建模](img/00182.jpeg)'
- en: You can see from the preceding plot that the **AUC** is **0.74**, which is pretty
    good for a start. We will now build the next predictive model using support vector
    machines using a similar process and see how it fares.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从前面的图中看到，**AUC**为**0.74**，这对于一个开始来说相当不错。我们现在将使用类似的过程构建下一个预测模型，使用支持向量机，并看看它的表现如何。
- en: Modeling using support vector machines
  id: totrans-140
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机建模
- en: Support vector machines belong to the family of supervised machine learning
    algorithms used for both classification and regression. Considering our binary
    classification problem, unlike logistic regression, the SVM algorithm will build
    a model around the training data in such a way that the training data points belonging
    to different classes are separated by a clear gap, which is optimized such that
    the distance of separation is the maximum. The samples on the margins are typically
    called the support vectors. The middle of the margin which separates the two classes
    is called the optimal separating hyperplane.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机属于用于分类和回归的监督机器学习算法家族。考虑到我们的二分类问题，与逻辑回归不同，支持向量机算法将围绕训练数据构建模型，使得属于不同类别的训练数据点通过一个清晰的间隙分开，这个间隙被优化到最大分离距离。位于边缘的样本通常被称为支持向量。分隔两个类别的边缘中间部分称为最优分离超平面。
- en: 'Data points on the wrong side of the margin are weighed down to reduce their
    influence and this is called the soft margin compared to the hard margins of separation
    we discussed earlier. SVM classifiers can be simple linear classifiers where the
    data points can be linearly separated. However, if we are dealing with data consisting
    of several features such that a linear separation is not possible directly, then
    we make use of several kernels to achieve the same and these form the non-linear
    SVM classifiers. You will be able to visualize how an SVM classifier actually
    looks much better with the following figure from the official documentation for
    the `svm` library in R:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 位于错误边缘的数据点被降低权重以减少其影响，这被称为软边缘，与我们之前讨论的硬边缘分离相比。支持向量机分类器可以是简单的线性分类器，其中数据点可以线性分离。然而，如果我们处理的是由多个特征组成的数据，而这些特征无法直接进行线性分离，那么我们就使用多个核来实现这一点，这些核形成了非线性支持向量机分类器。您将能够通过以下来自R中`svm`库官方文档的图来可视化支持向量机分类器实际的样子：
- en: '![Modeling using support vector machines](img/00183.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00183.jpeg)'
- en: 'Image source: [https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来源：[https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf](https://cran.r-project.org/web/packages/e1071/vignettes/svmdoc.pdf)
- en: From the figure, you can clearly see that we can place multiple hyperplanes
    separating the data points. However, the criterion for choosing the separating
    hyperplane is such that the distance of separation from the two classes is the
    maximum and the support vectors are the representative samples of the two classes
    as depicted on the margins. Revisiting the issue of non-linear classifiers, SVM
    has several kernels which can be used to achieve this besides the regular linear
    kernel used for linear classification. These include polynomial, **radial basis**
    **function** (**RBF**), and several others. The main principle behind these non-linear
    kernel functions is that, even if linear separation is not possible in the original
    feature space, they enable the separation to happen in a higher dimensional transformed
    feature space where we can use a hyperplane to separate the classes. An important
    thing to remember is the curse of dimensionality that applies here; since we may
    end up working with higher dimensional feature spaces, the model generalization
    error increases and the predictive power of the model decreases. It we have enough
    data, it still performs reasonably. We will be using the RBF kernel, also known
    as the radial basis function, in our model and for that two important parameters
    are cost and gamma.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，你可以清楚地看到我们可以放置多个超平面来分隔数据点。然而，选择分隔超平面的标准是两个类别的分隔距离最大，支持向量是两个类别的代表性样本，如图中边缘所示。回顾非线性分类器的问题，SVM除了用于线性分类的常规线性核之外，还有几个核可以用来实现这一点。这些包括多项式、**径向基**
    **函数**（**RBF**）以及几个其他核。这些非线性核函数背后的主要原理是，即使在线性特征空间中无法进行线性分隔，它们也能使分隔在更高维度的变换特征空间中发生，在那里我们可以使用超平面来分隔类别。需要记住的一个重要事情是维度的诅咒；由于我们可能最终要处理更高维度的特征空间，模型的泛化误差增加，模型的预测能力降低。如果我们有足够的数据，它仍然表现合理。在我们的模型中，我们将使用RBF核，也称为径向基函数，为此有两个重要的参数是成本和gamma。
- en: 'We will start by loading the necessary dependencies and preparing the testing
    data features:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先加载必要的依赖项并准备测试数据特征：
- en: '[PRE14]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Once this is done, we build the SVM model using the training data and the RBF
    kernel on all the training set features:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这些，我们将使用训练数据和RBF核在所有训练集特征上构建SVM模型：
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The properties of the model are generated as follows from the `summary` function:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的属性如下所示，来自`summary`函数：
- en: '![Modeling using support vector machines](img/00184.jpeg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00184.jpeg)'
- en: 'Now we use our testing data on this model to make predictions and evaluate
    the results as follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用测试数据对这个模型进行预测并评估结果如下：
- en: '[PRE16]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This gives us the following confusion matrix like we saw in logistic regression
    and the details are depicted for the model performance. We observe that the **accuracy**
    is **67.5%**, **sensitivity** is **100%**, and **specificity** is **0%**, which
    means that it is a very aggressive model which just predicts every customer rating
    as good. This model clearly suffers from the major class classification problem
    and we need to improve this.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了如下混淆矩阵，就像我们在逻辑回归中看到的那样，模型性能的细节如下。我们观察到**准确率**是**67.5**%，**灵敏度**是**100**%，**特异性**是**0**%，这意味着这是一个非常激进的模型，它只是预测每个客户评价为好。这个模型显然存在主要类别分类问题，我们需要改进这一点。
- en: '![Modeling using support vector machines](img/00185.jpeg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00185.jpeg)'
- en: 'To build a better model, we need some feature selection. We already have the
    top five best features which we had obtained in the *Feature selection* section.
    Nevertheless, we will still run a feature selection algorithm specifically for
    SVM to see feature importance, as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建更好的模型，我们需要进行一些特征选择。我们已经在*特征选择*部分获得了前五个最佳特征。尽管如此，我们仍然会运行一个专门为SVM设计的特征选择算法来查看特征重要性，如下所示：
- en: '[PRE17]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This gives us a plot and we see that the top five important variables are similar
    to our top five best features, except this algorithm ranks age as more important
    than `credit.amount`, so you can test this by building several models with different
    features and see which one gives the best results. For us, the features selected
    from random forests gave a better result. The variable importance plot is depicted
    as follows:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个图，我们看到前五个重要变量与我们的前五个最佳特征相似，但这个算法将年龄的重要性排名高于`credit.amount`，所以你可以通过构建具有不同特征的几个模型来测试这一点，看看哪一个给出最好的结果。对我们来说，从随机森林中选择的特征给出了更好的结果。变量重要性图如下所示：
- en: '![Modeling using support vector machines](img/00186.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00186.jpeg)'
- en: 'We now build a new SVM model based on the top five features that gave us the
    best results and evaluate its performance on the test data using the following
    code snippet:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们基于给我们最佳结果的五个特征构建一个新的SVM模型，并使用以下代码片段在测试数据上评估其性能：
- en: '[PRE18]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![Modeling using support vector machines](img/00187.jpeg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00187.jpeg)'
- en: 'We will definitely select this model and move on to model optimization by hyperparameter
    tuning using a grid search algorithm as follows to optimize the cost and gamma
    parameters:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们肯定会选择这个模型，并继续使用网格搜索算法进行模型优化，如下所示，以优化成本和gamma参数：
- en: '[PRE20]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '**Output**:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出结果：**'
- en: '![Modeling using support vector machines](img/00188.jpeg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00188.jpeg)'
- en: 'The grid search plot can be viewed as follows:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索图可以如下查看：
- en: '[PRE21]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Output:**'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出结果：**'
- en: '![Modeling using support vector machines](img/00189.jpeg)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00189.jpeg)'
- en: 'The darkest region shows the parameter values which gave the best performance.
    We now select the best model and evaluate it once again as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最暗的区域显示了给出最佳性能的参数值。我们现在选择最佳模型并再次评估，如下所示：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'On observing the confusion matrix results we obtained from the following output
    (we are henceforth depicting only the metrics which we are tracking), we see that
    the overall **accuracy** has increased to **71%**, **sensitivity** to **86%**,
    and **specificity** to **41%**, which is excellent compared to the previous model
    results:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 观察从以下输出中获得的混淆矩阵结果（我们此后只描述我们跟踪的指标），我们看到整体**准确率**增加到**71**%，**灵敏度**到**86**%，**特异性**到**41**%，与之前的模型结果相比，这是非常好的：
- en: '![Modeling using support vector machines](img/00190.jpeg)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00190.jpeg)'
- en: 'You see how powerful hyperparameter optimizations can be in predictive modeling!
    We also plot some evaluation curves as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到超参数优化在预测建模中的强大作用！我们还会绘制一些评估曲线，如下所示：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We can see how the predictions are plotted in the evaluation space, and we
    see that the AUC in this case is 0.69 from the following ROC plot:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到预测是如何在评估空间中绘制的，并且我们看到从以下ROC图中，AUC在这种情况下为0.69：
- en: '![Modeling using support vector machines](img/00191.jpeg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00191.jpeg)'
- en: 'Now, let''s say we want to optimize the model based on this ROC plot with the
    objective of maximizing the AUC. We will try that now, but first we need to encode
    the values of the categorical variables to include some letters because R causes
    some problems when representing column names of factor variables that have only
    numbers. So basically, if `credit.rating` has values `0`, `1` then it gets transformed
    to **X0** and **X1**; ultimately our categories are still distinct and nothing
    changes. We transform our data first with the following code snippet:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设我们想要根据这个ROC图优化模型，目标是最大化AUC。我们现在将尝试这样做，但首先我们需要将分类变量的值编码为一些字母，因为R在表示只有数字的因子变量的列名时会引起一些问题。所以基本上，如果`credit.rating`的值为`0`、`1`，则它被转换为**X0**和**X1**；最终我们的类别仍然是不同的，没有任何变化。我们首先使用以下代码片段转换我们的数据：
- en: '[PRE24]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now we build an AUC optimized model using grid search again, as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们再次使用网格搜索构建一个AUC优化的模型，如下所示：
- en: '[PRE25]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Our next step is to perform predictions on the test data and evaluate the confusion
    matrix:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步是对测试数据进行预测并评估混淆矩阵：
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This gives us the following results:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下结果：
- en: '![Modeling using support vector machines](img/00192.jpeg)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00192.jpeg)'
- en: 'We see now that **accuracy** has increased further to **72%** and **specificity**
    has decreased slightly to **40%**, but **sensitivity** has increased to **87%**,
    which is good. We plot the curves once again, as follows:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在看到**准确率**进一步提高到**72**%，而**特异性**略有下降到**40**%，但**灵敏度**增加到**87**%，这是好的。我们再次绘制曲线，如下所示：
- en: '[PRE27]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This gives us the following plots, the same as we did in our earlier iterations:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下图表，与我们的早期迭代中做的一样：
- en: '![Modeling using support vector machines](img/00193.jpeg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![使用支持向量机建模](img/00193.jpeg)'
- en: It is quite pleasing to see that the AUC has indeed increased from 0.69 earlier
    to 0.74 now, which means the AUC based optimization algorithm indeed worked, since
    it has given better performance than the previous model in all the aspects we
    have been tracking. Up next, we will look at how to build predictive models using
    decision trees.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 看到AUC确实从之前的0.69增加到现在的0.74，这非常令人满意，这意味着基于AUC的优化算法确实有效，因为它在所有我们跟踪的方面都给出了比之前模型更好的性能。接下来，我们将探讨如何使用决策树构建预测模型。
- en: Modeling using decision trees
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用决策树进行建模
- en: Decision trees are algorithms which again belong to the supervised machine learning
    algorithms family. They are also used for both classification and regression,
    often called **CART**, which stands for **classification and regression trees**.
    These are used a lot in decision support systems, business intelligence, and operations
    research.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是再次属于监督机器学习算法家族的算法。它们也用于分类和回归，通常称为**CART**，代表**分类和回归树**。这些在决策支持系统、商业智能和运筹学中应用广泛。
- en: Decision trees are mainly used for making decisions that would be most useful
    in reaching some objective and designing a strategy based on these decisions.
    At the core, a decision tree is just a flowchart with several nodes and conditional
    edges. Each non-leaf node represents a conditional test on one of the features
    and each edge represents an outcome of the test. Each leaf node represents a class
    label where predictions are made for the final outcome. Paths from the root to
    all the leaf nodes give us all the classification rules. Decision trees are easy
    to represent, construct, and understand. However, the drawback is that they are
    very prone to overfitting and often these models do not generalize well. We will
    follow a similar analytics pipeline as before, to build some models based on decision
    trees.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树主要用于做出最有用的决策，以实现某些目标并基于这些决策设计策略。在核心上，决策树只是一个包含几个节点和条件边的流程图。每个非叶节点代表对某个特征的测试条件，每条边代表测试的结果。每个叶节点代表一个类标签，其中对最终结果进行预测。从根节点到所有叶节点的路径给出了所有的分类规则。决策树易于表示、构建和理解。然而，缺点是它们非常容易过拟合，并且这些模型通常泛化能力不佳。我们将遵循之前类似的分析流程，基于决策树构建一些模型。
- en: 'We start with loading the necessary dependencies and test data features:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载必要的依赖项和测试数据特征：
- en: '[PRE28]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now we will build an initial model with all the features as follows:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用以下所有特征构建一个初始模型：
- en: '[PRE29]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We predict and evaluate the model on the test data with the following code:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下代码在测试数据上预测和评估模型：
- en: '[PRE30]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'From the following output, we see that the model **accuracy** is around **68%**,
    **sensitivity** is **92%**, which is excellent, but **specificity** is only **18%**,
    which we should try and improve:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，我们看到模型的**准确率**大约为**68**%，**灵敏度**为**92**%，这是非常好的，但**特异性**仅为**18**%，这是我们应努力改进的：
- en: '![Modeling using decision trees](img/00194.jpeg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树进行建模](img/00194.jpeg)'
- en: 'We will now try feature selection to improve the model. We use the following
    code to train the model and rank the features by their importance:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将尝试特征选择来改进模型。我们使用以下代码来训练模型并按其重要性对特征进行排序：
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'This gives us the following plot showing the importance of different features:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下图表，显示了不同特征的重要性：
- en: '![Modeling using decision trees](img/00195.jpeg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树进行建模](img/00195.jpeg)'
- en: 'If you observe closely, the decision tree does not use all the features in
    the model construction and the top five features are the same as those we obtained
    earlier when we talked about feature selection. We will now build a model using
    these features as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细观察，决策树在模型构建中并没有使用所有特征，并且前五个特征与我们之前在讨论特征选择时获得的是相同的。我们现在将使用以下特征构建一个模型：
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We now make predictions on the test data and evaluate it, as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在对测试数据进行预测并评估，如下所示：
- en: '[PRE33]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This gives us the following confusion matrix with other metrics:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下混淆矩阵和其他指标：
- en: '![Modeling using decision trees](img/00196.jpeg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树进行建模](img/00196.jpeg)'
- en: You can see now that the overall model **accuracy** has decreased a bit to **62%**.
    However, we have increased our bad credit rating prediction where we predict a
    100 bad credit rating customers out of 130, which is excellent! Consequently,
    **specificity** has jumped up to **77%** and **sensitivity** is down to **55%**,
    but we still classify a substantial number of good credit rating customers as
    good. Though this model is a bit aggressive, it is a reasonable model because
    though we deny credit loans to more customers who could default on the payment,
    we also make sure a reasonable number of good customers get their credit loans
    approved.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以看到，整体模型的**准确率**略有下降，达到了**62%**。然而，我们在不良信用评分预测方面取得了进步，我们预测在130个客户中有100个不良信用评分客户，这非常出色！因此，**特异性**上升到**77%**，而**灵敏度**下降到**55**%，但我们仍然将大量良好信用评分的客户分类为良好。尽管这个模型有些激进，但这是一个合理的模型，因为我们虽然拒绝了更多可能违约的客户的信用贷款，但我们同时也确保了合理数量的良好客户能够获得他们的信用贷款。
- en: The reason we obtained these results is because we have built the model with
    a parameter called prior, if you check the modeling section earlier. This prior
    basically empowers us to apply weightages to the different classes in the class
    variable. If you remember, we had **700** people with a **good credit rating**
    and **300** people with a **bad credit rating** in our dataset, which was highly
    skewed, so while training the model, we can use prior to specify the importance
    of each of the classes in this variable and thus adjust the importance of misclassification
    of each class. In our model, we give more importance to the bad credit rating
    customers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得这些结果的原因是因为我们使用了一个名为先验的参数来构建模型，如果你检查前面的建模部分。这个先验参数基本上使我们能够对类别变量中的不同类别应用权重。如果你记得，在我们的数据集中有**700**个信用评分良好的人和**300**个信用评分不良的人，这是一个高度倾斜的数据集，所以在训练模型时，我们可以使用先验来指定这个变量中每个类别的相对重要性，从而调整每个类别的误分类的重要性。在我们的模型中，我们给予不良信用评分客户更多的重视。
- en: 'You can reverse the priors and give more importance to the good rating customers
    by using the parameter as `prior = c(0.7, 0.3)`, which would give the following
    confusion matrix:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过使用参数`prior = c(0.7, 0.3)`来反转先验概率，并给予良好信用评分客户更多的重视，这将给出以下混淆矩阵：
- en: '![Modeling using decision trees](img/00197.jpeg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树建模](img/00197.jpeg)'
- en: You can clearly see now that, since we gave more importance to good credit ratings,
    the **sensitivity** has jumped up to **92%** and **specificity** has gone down
    to **18%**. You can see that this gives you a lot of flexibility over your modeling
    depending on what you want to achieve.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以清楚地看到，由于我们给予了良好信用评分更多的重视，**灵敏度**上升到**92%**，而**特异性**下降到**18%**。你可以看到，这为你提供了很大的灵活性，取决于你想要实现的目标。
- en: 'To view the model, we can use the following code snippet:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看模型，我们可以使用以下代码片段：
- en: '[PRE34]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Output**:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '![Modeling using decision trees](img/00198.jpeg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树建模](img/00198.jpeg)'
- en: 'To visualize the preceding tree, you can use the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化前面的树，你可以使用以下代码：
- en: '[PRE35]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This gives us the following tree, and we can see that, using the priors, the
    only feature that is being used now out of the five features is `account.balance`
    and it has ignored all the other features. You can try and optimize the model
    further by using hyperparameter tuning by exploring the `tune.rpart` function
    from the `e1071` package:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下树形图，我们可以看到，使用先验概率，现在在五个特征中只使用了`account.balance`这个特征，并且忽略了其他所有特征。你可以尝试通过使用`e1071`包中的`tune.rpart`函数进行超参数调整来进一步优化模型：
- en: '![Modeling using decision trees](img/00199.jpeg)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树建模](img/00199.jpeg)'
- en: 'We finish our analysis by plotting some metric evaluation curves as follows:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过绘制以下指标评估曲线来完成我们的分析：
- en: '[PRE36]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The **AUC** is around **0.66**, which is not the best but definitely better
    than the baseline denoted by the red line in the following plot:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**AUC**大约为**0.66**，这并不是最好的，但肯定比以下图中用红线表示的基线要好：'
- en: '![Modeling using decision trees](img/00200.jpeg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![使用决策树建模](img/00200.jpeg)'
- en: Based on our business requirements, this model is quite fair. We will discuss
    model comparison later on in this chapter. We will now use random forests to build
    our next set of predictive models.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们的业务需求，这个模型相当公平。我们将在本章后面讨论模型比较。现在我们将使用随机森林来构建我们的下一组预测模型。
- en: Modeling using random forests
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用随机森林建模
- en: Random forests, also known as random decision forests, are a machine learning
    algorithm that comes from the family of ensemble learning algorithms. It is used
    for both regression and classification tasks. Random forests are nothing but a
    collection or ensemble of decision trees, hence the name.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林，也称为随机决策森林，是一种来自集成学习算法家族的机器学习算法。它用于回归和分类任务。随机森林实际上就是决策树集合或集成，因此得名。
- en: 'The working of the algorithm can be described briefly as follows. At any point
    in time, each tree in the ensemble of decision trees is built from a bootstrap
    sample, which is basically sampling with replacement. This sampling is done on
    the training dataset. During the construction of the decision tree, the split
    which was earlier being chosen as the best split among all the features is not
    done anymore. Now the best split is always chosen from a random subset of the
    features each time. This introduction of randomness into the model increases the
    bias of the model slightly but decreases the variance of the model greatly which
    prevents the overfitting of models, which is a serious concern in the case of
    decision trees. Overall, this yields much better performing generalized models.
    We will now start our analytics pipeline by loading the necessary dependencies:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的工作原理可以简要描述如下。在任何时刻，决策树集成中的每一棵树都是从一个自助样本中构建的，这基本上是带有替换的采样。这种采样是在训练数据集上进行的。在构建决策树的过程中，之前在所有特征中选择为最佳分割的分割不再进行。现在，每次分割时总是从特征的一个随机子集中选择最佳分割。这种随机性引入到模型中略微增加了模型的偏差，但大大减少了模型的方差，这防止了模型的过拟合，这在决策树的情况下是一个严重的问题。总的来说，这产生了性能更好的泛化模型。现在我们将开始我们的分析流程，通过加载必要的依赖项：
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Next, we will build the initial training model with all the features as follows:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用所有特征构建初始训练模型，如下所示：
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'You can view the model details by using the following code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下代码查看模型详情：
- en: '[PRE39]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '**Output**:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '![Modeling using random forests](img/00201.jpeg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机森林建模](img/00201.jpeg)'
- en: This gives us information about the **out of bag error** (**OOBE**), which is
    around **23%**, and the confusion matrix which is calculated on the training data,
    and also how many variables it is using at each split.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们提供了关于**袋外误差**（**OOBE**）的信息，大约为**23**%，以及基于训练数据的混淆矩阵，以及它在每个分割中使用的变量数量。
- en: 'Next, we will perform predictions using this model on the test data and evaluate
    them:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用此模型在测试数据上进行预测，并评估它们：
- en: '[PRE40]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[PRE42]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We now make predictions with this model on the test data and evaluate its performance
    as follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在使用此模型在测试数据上进行预测，并如下评估其性能：
- en: '[PRE43]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This gives us the following confusion matrix as the output with the other essential
    performance metrics:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下混淆矩阵作为输出，以及其他关键性能指标：
- en: '![Modeling using random forests](img/00203.jpeg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机森林建模](img/00203.jpeg)'
- en: We get a slightly decreased **accuracy** of **71%**, which is obvious because
    we have eliminated many features, but now the **specificity** has increased to
    **42%**, which indicates it is able to classify more bad instances correctly as
    bad. **Sensitivity** has decreased slightly to **84%**. We will now use grid search
    to perform hyperparameter tuning on this model as follows, to see if we can improve
    the performance further. The parameters of interest here include `ntree`, indicating
    the number of trees, `nodesize`, indicating the minimum size of terminal nodes,
    and `mtry`, indicating the number of variables sampled randomly at each split.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了略微降低的**准确率**为**71**%，这是显而易见的，因为我们已经消除了许多特征，但现在**特异性**已增加到**42**%，这表明它能够更准确地分类更多的坏实例。**灵敏度**略有下降至**84**%。现在我们将使用网格搜索来对此模型进行超参数调整，如下所示，以查看我们是否可以进一步提高性能。这里感兴趣的参数包括`ntree`，表示树的数量，`nodesize`，表示终端节点的最小大小，以及`mtry`，表示每次分割时随机采样的变量数量。
- en: '[PRE44]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '**Output**:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：'
- en: '![Modeling using random forests](img/00204.jpeg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机森林建模](img/00204.jpeg)'
- en: 'We now get the best model from the preceding grid search, perform predictions
    on the test data, and evaluate its performance with the following code snippet:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在从先前的网格搜索中得到最佳模型，对测试数据进行预测，并使用以下代码片段评估其性能：
- en: '[PRE45]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We can make several observations from the following output. Performance has
    improved very negligibly as the overall **accuracy** remains the same at **71%**
    and **specificity** at **42%**. **Sensitivity** has increased slightly to **85%**
    from **84%**:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中，我们可以得出几个观察结果。性能提高非常微不足道，因为整体**准确率**保持在**71%**，**特异性**保持在**42%**，**灵敏度**略有提高，从**84%**增加到**85%**：
- en: '![Modeling using random forests](img/00205.jpeg)'
  id: totrans-258
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机森林建模](img/00205.jpeg)'
- en: 'We now plot some performance curves for this model, as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在为这个模型绘制一些性能曲线，如下所示：
- en: '[PRE46]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We observe that the total **AUC** is about **0.7** and is much better than
    the red baseline **AUC** of **0.5** in the following plot:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们观察到总**AUC**约为**0.7**，在以下图中比红色基线**AUC**的**0.5**要好得多：
- en: '![Modeling using random forests](img/00206.jpeg)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![使用随机森林建模](img/00206.jpeg)'
- en: The last algorithm we will explore is neural networks and we will build our
    models using them in the following section.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要探索的最后一种算法是神经网络，我们将在下一节中使用它们来构建模型。
- en: Modeling using neural networks
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用神经网络建模
- en: 'Neural networks, or to be more specific in this case, artificial neural networks,
    is a family of machine learning models whose concepts are based on the working
    of biological neural networks, just like our nervous system. Neural networks have
    been there for a long time, but recently there has been an upsurge of interest
    in building highly intelligent systems using deep learning and artificial intelligence.
    Deep learning makes use of deep neural networks, which are essentially neural
    networks with a huge number of hidden layers between the input and output layers.
    A typical neural network can be visualized with the following figure:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络，或者更具体地说，在这种情况下，人工神经网络，是一系列基于生物神经网络工作原理的机器学习模型，就像我们的神经系统一样。神经网络已经存在很长时间了，但最近，人们对于使用深度学习和人工智能构建高度智能系统的兴趣激增。深度学习利用深度神经网络，这些网络在输入层和输出层之间有大量的隐藏层。一个典型的神经网络可以用以下图示来表示：
- en: '![Modeling using neural networks](img/00207.jpeg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模](img/00207.jpeg)'
- en: From the figure, you can deduce that this neural network is an interconnected
    network of various nodes, also called neurons. Each node represents a neuron which
    is nothing but a mathematical function. It is impossible to go into every detail
    of how to represent a node mathematically but we will give the gist here. These
    mathematical functions receive one or more inputs with weights, which are represented
    in the preceding figure as edges, and then it performs some computation on these
    inputs to give an output. Various popular functions used in these nodes include
    step function and the sigmoid function, which you have already seen in use in
    the logistic regression algorithm. Once the inputs are weighed and transformed
    by the function, the activation of these functions is sent to further nodes until
    it reaches the output layer. A collection of nodes form a layer, just like in
    the earlier figure, we have three layers.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，你可以推断出这个神经网络是一个由各种节点相互连接的网络，也称为神经元。每个节点代表一个神经元，它实际上就是一个数学函数。我们不可能详细说明如何从数学上表示一个节点，但在这里我们会给出要点。这些数学函数接收一个或多个带有权重的输入，这些输入在先前的图中表示为边，然后对这些输入进行一些计算以给出输出。在这些节点中使用的各种流行函数包括阶跃函数和sigmoid函数，这些函数你已经在逻辑回归算法中看到过使用。一旦输入被函数加权并转换，这些函数的激活就会发送到后续的节点，直到达到输出层。节点集合形成一层，就像在先前的图中，我们有三个层。
- en: 'So, a neural network depends on several neurons or nodes and the pattern of
    interconnection between them, the learning process that is used to update the
    weights of the connections at each iteration (popularly called as epoch), and
    the activation functions of the nodes that convert the node''s inputs with weights
    to its output activation, which is passed layer by layer till we get the output
    prediction. We will start with loading the necessary dependencies as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，神经网络依赖于多个神经元或节点以及它们之间的互连模式，学习过程用于在每次迭代（通常称为一个epoch）中更新连接的权重，以及节点的激活函数，这些激活函数将带有权重的节点输入转换为输出激活，该激活通过层传递，直到我们得到输出预测。我们将从以下方式开始加载必要的依赖项：
- en: '[PRE47]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'We will now have to do some feature value encoding, similar to what we did
    when we did AUC optimization for SVM. To refresh your memory, you can run the
    following code snippet:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不得不进行一些特征值编码，类似于我们在为SVM进行AUC优化时所做的那样。为了刷新你的记忆，你可以运行以下代码片段：
- en: '[PRE48]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Once we have our data ready, we will build our initial neural network model
    using all the features as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好数据，我们将使用所有特征构建我们的初始神经网络模型，如下所示：
- en: '[PRE49]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '[PRE50]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'We now perform predictions on the test data and evaluate the model performance
    as follows:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在测试数据上执行预测并评估模型性能如下：
- en: '[PRE51]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: You can observe from the following output that our model has an **accuracy**
    of **72%**, which is quite good. It is predicting bad ratings as bad quite well,
    which is evident from the **specificity** which is **48%**, and as usual **sensitivity**
    is good at **84%**.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从以下输出中观察到，我们的模型具有**72%**的**准确率**，这相当不错。它很好地预测了负面评价为负面，这从**特异性**（**48%**）中可以看出，而且通常**灵敏度**在**84%**时表现良好。
- en: '![Modeling using neural networks](img/00208.jpeg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模](img/00208.jpeg)'
- en: 'We will now use the following code snippet to plot the features of importance
    for neural network based models:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用以下代码片段来绘制基于神经网络的模型的重要性特征：
- en: '[PRE52]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'This gives us the following plot ranking variables according to their importance:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下根据重要性排序的绘图排名变量：
- en: '![Modeling using neural networks](img/00209.jpeg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模](img/00209.jpeg)'
- en: 'We select some of the most important features from the preceding plot and build
    our next model as follows:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从先前的图中选择了最重要的特征，并按以下方式构建我们的下一个模型：
- en: '[PRE53]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We now perform predictions on the test data and evaluate the model performance:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在在测试数据上执行预测并评估模型性能：
- en: '[PRE54]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'This gives us the following confusion matrix with various metrics of our interest.
    We observe from the following output that the **accuracy** has increased slightly
    to **73%** and **sensitivity** has now increased to **87%** at the cost of **specificity**,
    which has dropped to **43%**:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这为我们提供了以下混淆矩阵，其中包含我们感兴趣的各个指标。从以下输出中我们可以观察到，**准确率**略有提高至**73%**，而**灵敏度**现在提高至**87**%，但以**特异性**为代价，它已降至**43%**：
- en: '![Modeling using neural networks](img/00210.jpeg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模](img/00210.jpeg)'
- en: 'You can check the hyperparameter tuning which it has done internally, as follows:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以检查它内部进行的超参数调整，如下所示：
- en: '[PRE55]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The following plot shows the accuracy of the various models with different
    numbers of nodes in the hidden layer and the weight decay:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了具有不同隐藏层节点数和权重衰减的各种模型的准确率：
- en: '![Modeling using neural networks](img/00211.jpeg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模](img/00211.jpeg)'
- en: 'Based on our requirement that the bank makes minimum losses, we select the
    best model as the initial neural network model that was built, since it has an
    accuracy similar to the new model and its specificity is much higher which is
    extremely important. We now plot some performance curves for the best model as
    follows:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 根据银行要求最小化损失的要求，我们选择最佳模型作为最初构建的初始神经网络模型，因为它具有与新模型相似的准确率，并且其**特异性**要高得多，这非常重要。我们现在绘制最佳模型的一些性能曲线如下：
- en: '[PRE56]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We observe from the following plot that the **AUC** is **0.74**, which is quite
    good and performs a lot better than the baseline denoted in red:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下图中我们可以观察到，**AUC**为**0.74**，这相当不错，并且比用红色表示的基线表现要好得多：
- en: '![Modeling using neural networks](img/00212.jpeg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![使用神经网络建模](img/00212.jpeg)'
- en: This concludes our predictive modeling session and we will wrap it up with model
    selection and comparisons.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着我们的预测建模会话的结束，我们将通过模型选择和比较来总结。
- en: Model comparison and selection
  id: totrans-298
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型比较和选择
- en: We have explored various machine learning techniques and built several models
    to predict the credit ratings of customers, so now comes the question of which
    model we should select and how the models compare against each other. Our test
    data has 130 instances of customers with a **bad credit rating** (**0**) and 270
    customers with a **good credit rating** (**1**).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经探索了各种机器学习技术，并构建了几个模型来预测客户的信用评级，因此现在的问题是我们应该选择哪个模型以及模型之间是如何相互比较的。我们的测试数据有130个客户的**不良信用评级**（**0**）和270个客户的**良好信用评级**（**1**）。
- en: 'If you remember, earlier we had talked about using domain knowledge and business
    requirements after doing modeling to interpret results and make decisions. Right
    now, our decision is to choose the best model to maximize profits and minimize
    losses for the German bank. Let us consider the following conditions:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得，我们之前讨论过在建模后使用领域知识和业务需求来解释结果并做出决策。目前，我们的决定是选择最佳模型以最大化德国银行的利润并最小化损失。让我们考虑以下条件：
- en: If we incorrectly predict a customer with bad credit rating as good, the bank
    will end up losing the whole credit amount lent to him since he will default on
    the payment and so loss is 100%, which can be denoted as -1 for our ease of calculation.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们错误地将信用评级差的客户预测为好，银行将损失其贷出的全部信用金额，因为他将违约支付，因此损失为100%，可以用-1表示，以方便计算。
- en: If we correctly predict a customer with bad credit rating as bad, we correctly
    deny him a credit loan and so there is neither any loss nor any profit.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们正确地将信用评级差的客户预测为坏，我们就正确地拒绝了他信用贷款，因此既没有损失也没有利润。
- en: If we correctly predict a customer with good credit rating as good, we correctly
    give him the credit loan. Assuming the bank has an interest rate on the sum of
    money lent, let us assume the profit is 30% from the interest money that is paid
    back monthly by the customer. Therefore, profit is denoted as 30% or +0.3 for
    our ease of calculation.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们正确地将信用评级好的客户预测为好，我们就正确地给他提供了信用贷款。假设银行对贷出的金额有利息，让我们假设利润是客户每月支付的利息的30%。因此，利润表示为30%或+0.3，以方便计算。
- en: If we incorrectly predict a customer with good credit rating as bad, we incorrectly
    deny him the credit loan but there is neither any profit nor any loss involved
    in this case.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们错误地将信用评级好的客户预测为坏，我们就错误地拒绝了他信用贷款，但在此情况下既没有利润也没有损失。
- en: 'Keeping these conditions in mind, we will make a comparison table for the various
    models, including some of the metrics we had calculated earlier for the best model
    for each machine learning algorithm. Remember that considering all the model performance
    metrics and business requirements, there is no one model that is the best among
    them all. Each model has its own set of good performance points, which is evident
    in the following analysis:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这些条件，我们将为各种模型制作一个比较表，包括我们之前为每个机器学习算法的最佳模型计算的一些指标。记住，考虑到所有模型性能指标和业务需求，没有一种模型在所有模型中都是最佳的。每个模型都有其自身的良好性能点，这在以下分析中是显而易见的：
- en: '![Model comparison and selection](img/00213.jpeg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![模型比较和选择](img/00213.jpeg)'
- en: The cells highlighted in the preceding table show the best performance for that
    particular metric. As we mentioned earlier, there is no best model and we have
    listed down the models that have performed best against each metric. Considering
    the total overall gain, decision tree seems to be the model of choice. However,
    this is assuming that the credit loan amount requested is constant per customer.
    Remember that if each customer requests loans of different amounts then this notion
    of total gain cannot be compared because then the profit from one loan might be
    different to another and the loss incurred might be different on different loans.
    This analysis is a bit complex and out of the scope of this chapter, but we will
    mention briefly how this can be computed. If you remember, there is a `credit.amount`
    feature, which specifies the credit amount requested by the customer. Since we
    already have the customer numbers in the training data, we can aggregate the rated
    customers with their requested amount and sum up the ones for which losses and
    profits are incurred, and then we will get the total gain of the bank for each
    method!
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 前表中突出显示的单元格显示了该特定指标的最佳性能。正如我们之前提到的，没有最佳模型，我们已经列出了针对每个指标表现最好的模型。考虑到总整体收益，决策树似乎是最优模型。然而，这是假设每个客户请求的信用贷款金额是恒定的。记住，如果每个客户请求的贷款金额不同，那么这种总收益的概念就无法比较，因为在这种情况下，一笔贷款的利润可能不同于另一笔，而损失也可能在不同贷款中不同。这种分析有些复杂，超出了本章的范围，但我们将简要说明如何计算。如果你还记得，有一个`credit.amount`特征，它指定了客户请求的信用金额。由于我们已经在训练数据中有了客户编号，我们可以将评级客户及其请求的金额进行汇总，并对那些产生损失和利润的客户进行汇总，然后我们将得到每种方法的银行总收益！
- en: Summary
  id: totrans-308
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We explored several important areas in the world of supervised learning in this
    chapter. If you have followed this chapter from the beginning of our journey and
    braved your way till the end, give yourself a pat on the back! You now know what
    constitutes predictive analytics and some of the important concepts associated
    with it. Also, we have seen how predictive modeling works and the full predictive
    analytics pipeline in actual practice. This will enable you to build your own
    predictive models in the future and start deriving valuable insights from model
    predictions. We also saw how to actually use models to make predictions and evaluate
    these predictions to test model performance so that we can optimize the models
    further and then select the best model based on metrics as well and business requirements.
    Before we conclude and you start your own journey into predictive analytics, I
    will like to mention that you should always remember Occam's razor, which states
    that *Among competing hypotheses, the one with the fewest assumptions should be
    selected,* which can be also interpreted as *Sometimes, the simplest solution
    is the best one*. Do not blindly jump into building predictive models with the
    latest packages and techniques, because first you need to understand the problem
    you are solving and then start from the simplest implementation, which will often
    lead to better results than most complex solutions.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了监督学习领域中的几个重要方面。如果你从我们旅程的开始就跟随了这一章，并且勇敢地走到了最后，给自己鼓掌吧！你现在已经知道了构成预测分析的内容以及与之相关的一些重要概念。此外，我们还看到了预测模型是如何在实际中工作的，以及完整的预测分析流程。这将使你能够在未来构建自己的预测模型，并从模型预测中开始获得有价值的见解。我们还看到了如何实际使用模型进行预测，以及如何评估这些预测以测试模型性能，以便我们可以进一步优化模型，并根据指标和业务需求选择最佳模型。在我们得出结论并开始你自己的预测分析之旅之前，我想提到的是，你应该始终记住奥卡姆剃刀原理，它指出*在相互竞争的假设中，应该选择假设最少的那一个*，这也可以被解释为*有时，最简单的解决方案是最好的一个*。不要盲目地使用最新的包和技术来构建预测模型，因为首先你需要理解你正在解决的问题，然后从最简单的实现开始，这通常会带来比大多数复杂解决方案更好的结果。
