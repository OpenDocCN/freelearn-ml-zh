- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: Modeling Sequential Data Using Recurrent Neural Networks
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用递归神经网络建模顺序数据
- en: In the previous chapter, we focused on **convolutional neural networks** (**CNNs**).
    We covered the building blocks of CNN architectures and how to implement deep
    CNNs in TensorFlow. Finally, you learned how to use CNNs for image classification.
    In this chapter, we will explore **recurrent neural networks** (**RNNs**) and
    see their application in modeling sequential data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们集中讨论了**卷积神经网络**（**CNNs**）。我们介绍了CNN架构的构建块以及如何在TensorFlow中实现深度CNN。最后，你学习了如何使用CNN进行图像分类。在本章中，我们将探讨**递归神经网络**（**RNNs**），并看到它们在建模顺序数据中的应用。
- en: 'We will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论以下主题：
- en: Introducing sequential data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍顺序数据
- en: RNNs for modeling sequences
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用RNN建模序列
- en: Long short-term memory (LSTM)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 长短期记忆（LSTM）
- en: Truncated backpropagation through time (TBPTT)
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断反向传播通过时间（TBPTT）
- en: Implementing a multilayer RNN for sequence modeling in TensorFlow
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现多层RNN进行序列建模
- en: 'Project one: RNN sentiment analysis of the IMDb movie review dataset'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目一：RNN情感分析——IMDb电影评论数据集
- en: 'Project two: RNN character-level language modeling with LSTM cells, using text
    data from Jules Verne''s *The Mysterious Island*'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目二：使用LSTM单元进行RNN字符级语言建模，使用来自儒勒·凡尔纳的《*神秘岛*》的文本数据
- en: Using gradient clipping to avoid exploding gradients
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用梯度裁剪来避免梯度爆炸
- en: Introducing the *Transformer* model and understanding the *self-attention mechanism*
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍*Transformer*模型并理解*自注意力机制*
- en: Introducing sequential data
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍顺序数据
- en: Let's begin our discussion of RNNs by looking at the nature of sequential data,
    which is more commonly known as sequence data or **sequences**. We will take a
    look at the unique properties of sequences that make them different to other kinds
    of data. We will then see how we can represent sequential data and explore the
    various categories of models for sequential data, which are based on the input
    and output of a model. This will help us to explore the relationship between RNNs
    and sequences in this chapter.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过先了解顺序数据的性质开始讨论RNN，这种数据通常被称为序列数据或**序列**。我们将查看序列的独特属性，这些属性使其与其他类型的数据不同。然后我们将看到如何表示顺序数据，并探讨针对顺序数据的各种模型类别，这些类别基于模型的输入和输出。这样有助于我们在本章中探讨RNN与序列的关系。
- en: Modeling sequential data – order matters
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模顺序数据——顺序很重要
- en: What makes sequences unique, compared to other types of data, is that elements
    in a sequence appear in a certain order and are not independent of each other.
    Typical machine learning algorithms for supervised learning assume that the input
    is **independent and identically distributed** (**IID**) data, which means that
    the training examples are *mutually independent* and have the same underlying
    distribution. In this regard, based on the mutual independence assumption, the
    order in which the training examples are given to the model is irrelevant. For
    example, if we have a sample consisting of *n* training examples, ![](img/B13208_16_001.png),
    the order in which we use the data for training our machine learning algorithm
    does not matter. An example of this scenario would be the Iris dataset that we
    previously worked with. In the Iris dataset, each flower has been measured independently,
    and the measurements of one flower do not influence the measurements of another
    flower.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的数据相比，顺序数据的独特之处在于，序列中的元素按特定顺序出现，并且它们彼此之间不是独立的。典型的监督学习机器学习算法假设输入是**独立同分布**（**IID**）的数据，这意味着训练样本是*相互独立*的，并且有相同的底层分布。在这种假设下，训练样本的顺序对模型并不重要。例如，如果我们有一个包含*n*个训练样本的样本集，![](img/B13208_16_001.png)，我们用这些数据训练机器学习算法时，数据的顺序并不重要。这种情形的例子可以是我们之前使用的鸢尾花数据集。在鸢尾花数据集中，每朵花的测量值是独立的，某朵花的测量结果不会影响其他花的测量结果。
- en: However, this assumption is not valid when we deal with sequences—by definition,
    order matters. Predicting the market value of a particular stock would be an example
    of this scenario. For instance, assume we have a sample of *n* training examples,
    where each training example represents the market value of a certain stock on
    a particular day. If our task is to predict the stock market value for the next
    three days, it would make sense to consider the previous stock prices in a date-sorted
    order to derive trends rather than utilize these training examples in a randomized
    order.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们处理序列时，这一假设是不成立的——根据定义，顺序非常重要。预测某只股票的市场价值就是这种情况的一个例子。例如，假设我们有一个包含*n*个训练样本的数据集，其中每个训练样本表示某只股票在某一天的市场价值。如果我们的任务是预测未来三天的股市价值，那么考虑按日期排序的历史股价来推导趋势，而不是随机顺序使用这些训练样本，会更有意义。
- en: '**Sequential data versus time-series data**'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**顺序数据与时间序列数据**'
- en: Time-series data is a special type of sequential data, where each example is
    associated with a dimension for time. In time-series data, samples are taken at
    successive time stamps, and therefore, the time dimension determines the order
    among the data points. For example, stock prices and voice or speech records are
    time-series data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 时间序列数据是一种特殊类型的顺序数据，其中每个示例都与时间维度相关联。在时间序列数据中，样本在连续的时间戳上采集，因此时间维度决定了数据点之间的顺序。例如，股价和语音或语音记录就是时间序列数据。
- en: On the other hand, not all sequential data has the time dimension, for example,
    text data or DNA sequences, where the examples are ordered but they do not qualify
    as time-series data. As you will see, in this chapter, we will cover some examples
    of natural language processing (NLP) and text modeling that are not time-series
    data, but note that RNNs can also be used for time-series data.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，并非所有的顺序数据都具有时间维度，例如文本数据或DNA序列，尽管这些示例是有序的，但它们并不符合时间序列数据的定义。正如您将在本章中看到的，我们将涵盖一些自然语言处理（NLP）和文本建模的示例，它们不是时间序列数据，但需要注意的是，RNN同样可以用于时间序列数据。
- en: Representing sequences
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列表示
- en: 'We''ve established that order among data points is important in sequential
    data, so we next need to find a way to leverage this ordering information in a
    machine learning model. Throughout this chapter, we will represent sequences as
    ![](img/B13208_16_002.png). The superscript indices indicate the order of the
    instances, and the length of the sequence is *T*. For a sensible example of sequences,
    consider time-series data, where each example point, ![](img/B13208_16_003.png),
    belongs to a particular time, *t*. The following figure shows an example of time-series
    data where both the input features (*x*''s) and the target labels (*y*''s) naturally
    follow the order according to their time axis; therefore, both the *x*''s and
    *y*''s are sequences:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确定，数据点之间的顺序在顺序数据中非常重要，因此我们接下来需要找到一种方法，将这个顺序信息融入到机器学习模型中。在本章中，我们将以 ![](img/B13208_16_002.png)
    来表示序列。上标索引表示实例的顺序，序列的长度为*T*。为了给出一个合理的序列示例，考虑时间序列数据，其中每个示例点，![](img/B13208_16_003.png)，对应于某一特定时间点，*t*。下图展示了时间序列数据的一个示例，其中输入特征（*x*）和目标标签（*y*）自然地按时间轴顺序排列；因此，*x*
    和 *y* 都是序列：
- en: '![](img/B13208_16_01.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_01.png)'
- en: As we have already mentioned, the standard neural network (NN) models that we
    have covered so far, such as multilayer perceptron (MLP) and CNNs for image data,
    assume that the training examples are independent of each other and thus do not
    incorporate *ordering information*. We can say that such models do not have a
    *memory* of previously seen training examples. For instance, the samples are passed
    through the feedforward and backpropagation steps, and the weights are updated
    independently of the order in which the training examples are processed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，我们目前讨论的标准神经网络（NN）模型，如多层感知器（MLP）和用于图像数据的卷积神经网络（CNN），假设训练样本彼此独立，因此没有考虑*顺序信息*。我们可以说，这些模型没有*记忆*以前见过的训练样本。例如，样本会经过前馈和反向传播步骤，权重的更新与训练样本处理的顺序无关。
- en: RNNs, by contrast, are designed for modeling sequences and are capable of remembering
    past information and processing new events accordingly, which is a clear advantage
    when working with sequence data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与此相对，循环神经网络（RNN）专门用于建模序列，能够记住过去的信息并根据新事件进行处理，这在处理序列数据时具有明显的优势。
- en: The different categories of sequence modeling
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 序列建模的不同类别
- en: 'Sequence modeling has many fascinating applications, such as language translation
    (for example, translating text from English to German), image captioning, and
    text generation. However, in order to choose an appropriate architecture and approach,
    we have to understand and be able to distinguish between these different sequence
    modeling tasks. The following figure, based on the explanations in the excellent
    article *The Unreasonable Effectiveness of Recurrent Neural Networks*, by *Andrej
    Karpathy* ([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)),
    summarizes the most common sequence modeling tasks, which depend on the relationship
    categories of input and output data:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 序列建模有许多引人入胜的应用，例如语言翻译（例如，将英语文本翻译成德语）、图像标注和文本生成。然而，为了选择适当的架构和方法，我们必须理解并能够区分这些不同的序列建模任务。下面的图基于*Andrej
    Karpathy*的优秀文章《递归神经网络的非理性有效性》([http://karpathy.github.io/2015/05/21/rnn-effectiveness/](http://karpathy.github.io/2015/05/21/rnn-effectiveness/))中的解释，总结了最常见的序列建模任务，这些任务取决于输入和输出数据的关系类别：
- en: '![](img/B13208_16_02.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_02.png)'
- en: 'Let''s discuss the different relationship categories between input and output
    data, which were depicted in the previous figure, in more detail. If neither the
    input nor output data represents sequences, then we are dealing with standard
    data, and we could simply use a multilayer perceptron (or another classification
    model previously covered in this book) to model such data. However, if either
    the input or output is a sequence, the modeling task likely falls into one of
    these categories:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论输入和输出数据之间的不同关系类别，这些类别在之前的图中有所展示。如果输入和输出数据都不是序列，那么我们处理的是标准数据，可能只需使用多层感知机（或本书之前介绍的其他分类模型）来建模此类数据。然而，如果输入或输出是序列，那么建模任务可能属于以下类别之一：
- en: '**Many-to-one**: The input data is a sequence, but the output is a fixed-size
    vector or scalar, not a sequence. For example, in sentiment analysis, the input
    is text-based (for example, a movie review) and the output is a class label (for
    example, a label denoting whether a reviewer liked the movie).'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对一**：输入数据是一个序列，但输出是一个固定大小的向量或标量，而不是一个序列。例如，在情感分析中，输入是基于文本的（例如，一篇电影评论），输出是一个类别标签（例如，表示评论者是否喜欢这部电影的标签）。'
- en: '**One-to-many**: The input data is in standard format and not a sequence, but
    the output is a sequence. An example of this category is image captioning—the
    input is an image and the output is an English phrase summarizing the content
    of that image.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**一对多**：输入数据是标准格式，而不是序列，但输出是一个序列。这个类别的一个例子是图像标注——输入是一张图像，输出是一个总结该图像内容的英文短语。'
- en: '**Many-to-many**: Both the input and output arrays are sequences. This category
    can be further divided based on whether the input and output are synchronized.
    An example of a synchronized many-to-many modeling task is video classification,
    where each frame in a video is labeled. An example of a *delayed* many-to-many
    modeling task would be translating one language into another. For instance, an
    entire English sentence must be read and processed by a machine before its translation
    into German is produced.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多对多**：输入和输出数组都是序列。这个类别可以根据输入和输出是否同步进一步细分。一个同步的多对多建模任务的例子是视频分类，其中视频中的每一帧都被标注。一个*延迟*的多对多建模任务的例子是将一种语言翻译成另一种语言。例如，必须先由机器读取并处理整个英语句子，然后才能生成其德语翻译。'
- en: Now, after summarizing the three broad categories of sequence modeling, we can
    move forward to discussing the structure of an RNN.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在总结了序列建模的三大类之后，我们可以继续讨论RNN的结构。
- en: RNNs for modeling sequences
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用于建模序列的RNN
- en: In this section, before we start implementing RNNs in TensorFlow, we will discuss
    the main concepts of RNNs. We will begin by looking at the typical structure of
    an RNN, which includes a recursive component to model sequence data. Then, we
    will examine how the neuron activations are computed in a typical RNN. This will
    create a context for us to discuss the common challenges in training RNNs, and
    we will then discuss solutions to these challenges, such as LSTM and gated recurrent
    units (GRUs).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，在开始在TensorFlow中实现RNN之前，我们将讨论RNN的主要概念。我们将首先查看RNN的典型结构，其中包含递归组件来建模序列数据。然后，我们将研究在典型RNN中神经元激活是如何计算的。这将为我们讨论训练RNN时遇到的常见挑战提供背景，并接着讨论解决这些挑战的方法，例如LSTM和门控递归单元（GRU）。
- en: Understanding the RNN looping mechanism
  id: totrans-36
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解 RNN 循环机制
- en: 'Let''s start with the architecture of an RNN. The following figure shows a
    standard feedforward NN and an RNN side by side for comparison:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从 RNN 的架构开始。下图显示了一个标准的前馈神经网络和一个 RNN，便于进行比较：
- en: '![](img/B13208_16_03.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_03.png)'
- en: Both of these networks have only one hidden layer. In this representation, the
    units are not displayed, but we assume that the input layer (*x*), hidden layer
    (*h*), and output layer (*o*) are vectors that contain many units.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个网络只有一个隐藏层。在这个表示中，单元未显示，但我们假设输入层（*x*）、隐藏层（*h*）和输出层（*o*）是包含多个单元的向量。
- en: '**Determining the type of output from an RNN**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**确定 RNN 的输出类型**'
- en: This generic RNN architecture could correspond to the two sequence modeling
    categories where the input is a sequence. Typically, a recurrent layer can return
    a sequence as output, ![](img/B13208_16_004.png), or simply return the last output
    (at *t* = *T*, that is, ![](img/B13208_16_005.png)). Thus, it could be either
    many-to-many, or it could be many-to-one if, for example, we only use the last
    element, ![](img/B13208_16_006.png), as the final output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这种通用 RNN 架构可能对应两种序列建模类别，其中输入是一个序列。通常，循环层可以返回一个序列作为输出，![](img/B13208_16_004.png)，或者仅返回最后一个输出（在
    *t* = *T* 时，即 ![](img/B13208_16_005.png)）。因此，它可以是多对多的，或者如果我们仅使用最后一个元素，![](img/B13208_16_006.png)，作为最终输出，则它也可以是多对一的。
- en: As you will see later, in the TensorFlow Keras API, the behavior of a recurrent
    layer with respect to returning a sequence as output or simply using the last
    output can be specified by setting the argument `return_sequences` to `True` or
    `False`, respectively.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你稍后将看到的，在 TensorFlow Keras API 中，可以通过将参数`return_sequences`设置为`True`或`False`来指定循环层的行为，分别表示返回一个序列作为输出或仅使用最后一个输出。
- en: In a standard feedforward network, information flows from the input to the hidden
    layer, and then from the hidden layer to the output layer. On the other hand,
    in an RNN, the hidden layer receives its input from both the input layer of the
    current time step and the hidden layer from the previous time step.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的前馈神经网络中，信息从输入层流向隐藏层，然后再从隐藏层流向输出层。另一方面，在 RNN 中，隐藏层的输入来自当前时间步的输入层和前一个时间步的隐藏层。
- en: The flow of information in adjacent time steps in the hidden layer allows the
    network to have a memory of past events. This flow of information is usually displayed
    as a loop, also known as a **recurrent edge** in graph notation, which is how
    this general RNN architecture got its name.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 邻近时间步中隐藏层的信息流动使得网络能够记住过去的事件。这种信息流通常显示为一个循环，也称为**循环边**，在图示符号中就是这种通用 RNN 架构的名字来源。
- en: 'Similar to multilayer perceptrons, RNNs can consist of multiple hidden layers.
    Note that it''s a common convention to refer to RNNs with one hidden layer as
    a *single-layer RNN*, which is not to be confused with single-layer NNs without
    a hidden layer, such as Adaline or logistic regression. The following figure illustrates
    an RNN with one hidden layer (top) and an RNN with two hidden layers (bottom):'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 与多层感知器类似，RNN 也可以包含多个隐藏层。注意，通常约定将只有一个隐藏层的 RNN 称为*单层 RNN*，这与没有隐藏层的单层神经网络（如 Adaline
    或逻辑回归）不同。下图展示了一个带有一个隐藏层的 RNN（上图）和一个带有两个隐藏层的 RNN（下图）：
- en: '![](img/B13208_16_04.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_04.png)'
- en: In order to examine the architecture of RNNs and the flow of information, a
    compact representation with a recurrent edge can be unfolded, which you can see
    in the preceding figure.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查 RNN 的架构和信息流，可以展开带有循环边的紧凑表示，你可以在前面的图中看到它。
- en: As we know, each hidden unit in a standard NN receives only one input—the net
    preactivation associated with the input layer. In contrast, each hidden unit in
    an RNN receives two *distinct* sets of input—the preactivation from the input
    layer and the activation of the same hidden layer from the previous time step,
    *t* – 1.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所知，标准 NN 中的每个隐藏单元只接收一个输入——与输入层相关的网络预激活。相比之下，RNN 中的每个隐藏单元接收两个*不同*的输入——来自输入层的预激活和来自前一个时间步（*t*
    - 1）的相同隐藏层的激活。
- en: At the first time step, *t* = 0, the hidden units are initialized to zeros or
    small random values. Then, at a time step where *t* > 0, the hidden units receive
    their input from the data point at the current time, ![](img/B13208_16_007.png),
    and the previous values of hidden units at *t* – 1, indicated as ![](img/B13208_16_008.png).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个时间步 *t* = 0 时，隐藏单元被初始化为零或小的随机值。然后，在 *t* > 0 的时间步中，隐藏单元从当前时间的数据点！[](img/B13208_16_007.png)和前一个时间步
    *t* – 1 的隐藏单元值接收输入，表示为！[](img/B13208_16_008.png)。
- en: 'Similarly, in the case of a multilayer RNN, we can summarize the information
    flow as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，在多层RNN的情况下，我们可以将信息流总结如下：
- en: '*layer* = 1: Here, the hidden layer is represented as ![](img/B13208_16_009.png)
    and it receives its input from the data point, ![](img/B13208_16_010.png), and
    the hidden values in the same layer, but at the previous time step, ![](img/B13208_16_011.png).'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层* = 1：这里，隐藏层表示为！[](img/B13208_16_009.png)，并接收来自数据点！[](img/B13208_16_010.png)的输入，以及来自同一层但在前一个时间步的隐藏值！[](img/B13208_16_011.png)。'
- en: '*layer* = 2: The second hidden layer, ![](img/B13208_16_012.png), receives
    its inputs from the outputs of the layer below at the current time step (![](img/B13208_16_013.png))
    and its own hidden values from the previous time step, ![](img/B13208_16_014.png).'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*层* = 2：第二个隐藏层，![](img/B13208_16_012.png)，接收来自下层的当前时间步的输出（![](img/B13208_16_013.png)）以及来自前一个时间步的自身隐藏值，![](img/B13208_16_014.png)。'
- en: Since, in this case, each recurrent layer must receive a sequence as input,
    all the recurrent layers except the last one must *return a sequence as output*
    (that is, `return_sequences=True`). The behavior of the last recurrent layer depends
    on the type of problem.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在这种情况下，每个递归层必须接收一个序列作为输入，所以除最后一个层外，所有递归层都必须*返回一个序列作为输出*（即，`return_sequences=True`）。最后一个递归层的行为取决于问题的类型。
- en: Computing activations in an RNN
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在RNN中计算激活值
- en: Now that you understand the structure and general flow of information in an
    RNN, let's get more specific and compute the actual activations of the hidden
    layers, as well as the output layer. For simplicity, we will consider just a single
    hidden layer; however, the same concept applies to multilayer RNNs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了RNN的结构和信息流的一般流程，让我们更加具体地计算隐藏层的实际激活值，以及输出层的激活值。为简化起见，我们只考虑一个隐藏层；然而，同样的概念也适用于多层RNN。
- en: 'Each directed edge (the connections between boxes) in the representation of
    an RNN that we just looked at is associated with a weight matrix. Those weights
    do not depend on time, *t*; therefore, they are shared across the time axis. The
    different weight matrices in a single-layer RNN are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到的RNN表示中的每条有向边（框之间的连接）都与一个权重矩阵相关联。这些权重与时间无关，*t*；因此，它们在时间轴上是共享的。单层RNN中的不同权重矩阵如下：
- en: '![](img/B13208_16_015.png): The weight matrix between the input, ![](img/B13208_16_016.png),
    and the hidden layer, *h*'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B13208_16_015.png)：输入与隐藏层之间的权重矩阵，*h*'
- en: '![](img/B13208_16_017.png): The weight matrix associated with the recurrent
    edge'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B13208_16_017.png)：与递归边相关的权重矩阵'
- en: '![](img/B13208_16_018.png): The weight matrix between the hidden layer and
    output layer'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](img/B13208_16_018.png)：隐藏层与输出层之间的权重矩阵'
- en: 'These weight matrices are depicted in the following figure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这些权重矩阵在下图中表示：
- en: '![](img/B13208_16_05.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_05.png)'
- en: In certain implementations, you may observe that the weight matrices, ![](img/B13208_16_019.png)
    and ![](img/B13208_16_020.png), are concatenated to a combined matrix, ![](img/B13208_16_021.png).
    Later in this section, we will make use of this notation as well.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些实现中，你可能会看到权重矩阵！[](img/B13208_16_019.png) 和 ![](img/B13208_16_020.png) 被合并为一个矩阵！[](img/B13208_16_021.png)。稍后在本节中，我们也将使用这种符号。
- en: 'Computing the activations is very similar to standard multilayer perceptrons
    and other types of feedforward NNs. For the hidden layer, the net input, ![](img/B13208_16_022.png)
    (preactivation), is computed through a linear combination, that is, we compute
    the sum of the multiplications of the weight matrices with the corresponding vectors
    and add the bias unit:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 计算激活值非常类似于标准的多层感知机和其他类型的前馈神经网络。对于隐藏层，净输入！[](img/B13208_16_022.png)（预激活值）是通过线性组合计算的，即我们计算权重矩阵与相应向量的乘积之和，并加上偏置单元：
- en: '![](img/B13208_16_023.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_023.png)'
- en: 'Then, the activations of the hidden units at the time step, *t*, are calculated
    as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，隐藏单元在时间步长 *t* 时的激活计算如下：
- en: '![](img/B13208_16_024.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_024.png)'
- en: Here, ![](img/B13208_16_025.png) is the bias vector for the hidden units and
    ![](img/B13208_16_026.png) is the activation function of the hidden layer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_16_025.png) 是隐藏单元的偏置向量，![](img/B13208_16_026.png) 是隐藏层的激活函数。
- en: 'In case you want to use the concatenated weight matrix, ![](img/B13208_16_027.png),
    the formula for computing hidden units will change, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想使用连接的权重矩阵，![](img/B13208_16_027.png)，则计算隐藏单元的公式将发生变化，如下所示：
- en: '![](img/B13208_16_028.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_028.png)'
- en: 'Once the activations of the hidden units at the current time step are computed,
    then the activations of the output units will be computed, as follows:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算出当前时间步的隐藏单元的激活值，就会计算输出单元的激活值，如下所示：
- en: '![](img/B13208_16_029.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_029.png)'
- en: 'To help clarify this further, the following figure shows the process of computing
    these activations with both formulations:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步澄清这一点，以下图展示了使用这两种公式计算激活值的过程：
- en: '![](img/B13208_16_06.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_06.png)'
- en: '**Training RNNs using backpropogation through time (BPTT)**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用通过时间反向传播（BPTT）训练RNN**'
- en: 'The learning algorithm for RNNs was introduced in 1990: *Backpropagation Through
    Time: What It Does and How to Do It* (*Paul Werbos*, *Proceedings of IEEE*, 78(10):
    1550-1560, *1990*).'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RNN的学习算法于1990年提出：*反向传播通过时间：它的作用及如何实现*（*Paul Werbos*，*IEEE会议录*，78(10)：1550-1560，*1990*）。
- en: 'The derivation of the gradients might be a bit complicated, but the basic idea
    is that the overall loss, *L*, is the sum of all the loss functions at times *t*
    = 1 to *t* = *T*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度的推导可能有点复杂，但基本思想是整体损失，*L*，是从时间 *t* = 1 到 *t* = *T* 所有损失函数的总和：
- en: '![](img/B13208_16_030.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_030.png)'
- en: 'Since the loss at time *t* is dependent on the hidden units at all previous
    time steps 1 : *t*, the gradient will be computed as follows:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '由于时间 *t* 的损失依赖于所有之前时间步1 : *t*的隐藏单元，因此梯度将按如下方式计算：'
- en: '![](img/B13208_16_031.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_031.png)'
- en: 'Here, ![](img/B13208_16_032.png) is computed as a multiplication of adjacent
    time steps:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B13208_16_032.png) 是通过相邻时间步的乘法计算得出的：
- en: '![](img/B13208_16_033.png)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_033.png)'
- en: Hidden-recurrence versus output-recurrence
  id: totrans-82
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 隐藏层递归与输出层递归
- en: 'So far, you have seen recurrent networks in which the hidden layer has the
    recurrent property. However, note that there is an alternative model in which
    the recurrent connection comes from the output layer. In this case, the net activations
    from the output layer at the previous time step, ![](img/B13208_16_034.png), can
    be added in one of two ways:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经看到了隐藏层具有递归特性的递归网络。然而，注意还有一种替代模型，其中递归连接来自输出层。在这种情况下，来自前一时间步的输出层的净激活值，![](img/B13208_16_034.png)，可以通过以下两种方式之一添加：
- en: To the hidden layer at the current time step, ![](img/B13208_16_035.png) (shown
    in the following figure as output-to-hidden recurrence)
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对当前时间步的隐藏层，![](img/B13208_16_035.png)（下图显示为输出到隐藏的递归）
- en: To the output layer at the current time step, ![](img/B13208_16_036.png) (shown
    in the following figure as output-to-output recurrence)
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对当前时间步的输出层，![](img/B13208_16_036.png)（下图显示为输出到输出的递归）
- en: '![](img/B13208_16_07.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_07.png)'
- en: As shown in the previous figure, the differences between these architectures
    can be clearly seen in the recurring connections. Following our notation, the
    weights associated with the recurrent connection will be denoted for the hidden-to-hidden
    recurrence by ![](img/B13208_16_037.png), for the output-to-hidden recurrence
    by ![](img/B13208_16_038.png), and for the output-to-output recurrence by ![](img/B13208_16_039.png).
    In some articles in literature, the weights associated with the recurrent connections
    are also denoted by ![](img/B13208_16_040.png).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，这些架构之间的差异可以清晰地通过递归连接看到。根据我们的符号，隐藏到隐藏的递归的权重将用 ![](img/B13208_16_037.png)
    表示，输出到隐藏的递归的权重用 ![](img/B13208_16_038.png) 表示，输出到输出的递归的权重用 ![](img/B13208_16_039.png)
    表示。在一些文献中，与递归连接相关的权重也用 ![](img/B13208_16_040.png) 表示。
- en: 'To see how this works in practice, let''s manually compute the forward pass
    for one of these recurrent types. Using the TensorFlow Keras API, a recurrent
    layer can be defined via `SimpleRNN`, which is similar to the output-to-output
    recurrence. In the following code, we will create a recurrent layer from `SimpleRNN`
    and perform a forward pass on an input sequence of length 3 to compute the output.
    We will also manually compute the forward pass and compare the results with those
    of `SimpleRNN`. First, let''s create the layer and assign the weights for our
    manual computations:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看看这种方法如何在实践中工作，让我们手动计算其中一种循环类型的前向传递。使用TensorFlow Keras API，可以通过`SimpleRNN`定义一个循环层，这类似于输出到输出的递归。在下面的代码中，我们将从`SimpleRNN`创建一个循环层，并对长度为3的输入序列进行前向传递，计算输出。我们还将手动计算前向传递，并将结果与`SimpleRNN`的结果进行比较。首先，让我们创建这个层并为我们的手动计算分配权重：
- en: '[PRE0]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The input shape for this layer is `(None, None, 5)`, where the first dimension
    is the batch dimension (using `None` for variable batch size), the second dimension
    corresponds to the sequence (using `None` for the variable sequence length), and
    the last dimension corresponds to the features. Notice that we set `return_sequences=True`,
    which, for an input sequence of length 3, will result in the output sequence ![](img/B13208_16_041.png).
    Otherwise, it would only return the final output, ![](img/B13208_16_042.png).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 该层的输入形状是`(None, None, 5)`，其中第一个维度是批次维度（使用`None`表示可变批量大小），第二个维度对应序列（使用`None`表示可变序列长度），最后一个维度对应特征。注意，我们设置了`return_sequences=True`，这对于长度为3的输入序列将导致输出序列为
    ![](img/B13208_16_041.png)。否则，它只会返回最终输出， ![](img/B13208_16_042.png)。
- en: 'Now, we will call the forward pass on the `rnn_layer` and manually compute
    the outputs at each time step and compare them:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将调用`rnn_layer`的前向传递，并手动计算每个时间步的输出并进行比较：
- en: '[PRE1]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In our manual forward computation, we used the hyperbolic tangent (tanh) activation
    function, since it is also used in `SimpleRNN` (the default activation). As you
    can see from the printed results, the outputs from the manual forward computations
    exactly match the output of the `SimpleRNN` layer at each time step. Hopefully,
    this hands-on task has enlightened you on the mysteries of recurrent networks.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的手动前向计算中，我们使用了双曲正切（tanh）激活函数，因为它也用于`SimpleRNN`（默认激活函数）。从打印的结果可以看出，手动前向计算的输出与每个时间步的`SimpleRNN`层输出完全一致。希望这个动手任务能够启发你了解循环神经网络的奥秘。
- en: The challenges of learning long-range interactions
  id: totrans-94
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习长期交互的挑战
- en: 'BPTT, which was briefly mentioned earlier, introduces some new challenges.
    Because of the multiplicative factor, ![](img/B13208_16_043.png), in computing
    the gradients of a loss function, the so-called **vanishing** and **exploding**
    gradient problems arise. These problems are explained by the examples in the following
    figure, which shows an RNN with only one hidden unit for simplicity:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: BPTT（之前简要提到的反向传播通过时间）引入了一些新的挑战。由于在计算损失函数的梯度时存在乘法因子 ![](img/B13208_16_043.png)，因此出现了所谓的**消失**和**爆炸**梯度问题。以下图中的示例解释了这些问题，图中展示了一个仅包含一个隐藏单元的RNN，简化了计算过程：
- en: '![](img/B13208_16_08.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_08.png)'
- en: Basically, ![](img/B13208_16_0431.png) has *t* – *k* multiplications; therefore,
    multiplying the weight, *w,* by itself *t* – *k* times results in a factor, ![](img/B13208_16_045.png).
    As a result, if ![](img/B13208_16_046.png), this factor becomes very small when
    *t* – *k* is large. On the other hand, if the weight of the recurrent edge is
    ![](img/B13208_16_047.png), then ![](img/B13208_16_048.png) becomes very large
    when *t* – *k* is large. Note that large *t* – *k* refers to long-range dependencies.
    We can see that a naive solution to avoid vanishing or exploding gradients can
    be reached by ensuring ![](img/B13208_16_049.png). If you are interested and would
    like to investigate this in more detail, read *On the difficulty of training recurrent
    neural networks*, by *R. Pascanu*, *T. Mikolov*, and *Y. Bengio*, *2012* ([https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf)).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，![](img/B13208_16_0431.png) 具有 *t* – *k* 次乘法运算；因此，将权重 *w* 自身乘以 *t* – *k*
    次，结果得到一个因子，![](img/B13208_16_045.png)。因此，如果 ![](img/B13208_16_046.png)，当 *t* –
    *k* 很大时，这个因子会变得非常小。另一方面，如果递归边的权重是 ![](img/B13208_16_047.png)，那么当 *t* – *k* 很大时，![](img/B13208_16_048.png)
    会变得非常大。需要注意的是，大的 *t* – *k* 指的是长程依赖。我们可以看出，通过确保 ![](img/B13208_16_049.png)，可以得到一个避免梯度消失或爆炸的简单解决方案。如果你有兴趣并希望更详细地了解，可以阅读
    *R. Pascanu*、*T. Mikolov* 和 *Y. Bengio* 2012年发表的 *On the difficulty of training
    recurrent neural networks*（[https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf)）。
- en: 'In practice, there are at least three solutions to this problem:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，解决此问题的方案至少有三种：
- en: Gradient clipping
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度裁剪
- en: TBPTT
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TBPTT
- en: LSTM
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LSTM
- en: Using gradient clipping, we specify a cut-off or threshold value for the gradients,
    and we assign this cut-off value to gradient values that exceed this value. In
    contrast, TBPTT simply limits the number of time steps that the signal can backpropagate
    after each forward pass. For example, even if the sequence has 100 elements or
    steps, we may only backpropagate the most recent 20 time steps.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 使用梯度裁剪时，我们为梯度指定一个截止值或阈值，并将此截止值分配给超出此值的梯度值。相比之下，TBPTT仅限于信号在每次前向传播后反向传播的时间步数。例如，即使序列有100个元素或步骤，我们也许只能反向传播最近的20个时间步。
- en: While both gradient clipping and TBPTT can solve the exploding gradient problem,
    the truncation limits the number of steps that the gradient can effectively flow
    back and properly update the weights. On the other hand, LSTM, designed in 1997
    by Sepp Hochreiter and Jürgen Schmidhuber, has been more successful in vanishing
    and exploding gradient problems while modeling long-range dependencies through
    the use of memory cells. Let's discuss LSTM in more detail.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然梯度裁剪和TBPTT都可以解决梯度爆炸问题，但截断限制了梯度有效反向传播并正确更新权重的步骤数。另一方面，LSTM由Sepp Hochreiter和Jürgen
    Schmidhuber于1997年设计，在解决梯度消失和爆炸问题时，通过使用记忆单元在建模长程依赖方面取得了更大的成功。让我们更详细地讨论LSTM。
- en: Long short-term memory cells
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 长短期记忆单元（LSTM）
- en: 'As stated previously, LSTMs were first introduced to overcome the vanishing
    gradient problem (*Long Short-Term Memory*, *S. Hochreiter* and *J. Schmidhuber*,
    *Neural Computation*, 9(8): 1735-1780, *1997*). The building block of an LSTM
    is a **memory cell**, which essentially represents or replaces the hidden layer
    of standard RNNs.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，LSTM最初是为了克服梯度消失问题而提出的（*长短期记忆*，*S. Hochreiter* 和 *J. Schmidhuber*，*Neural
    Computation*，9(8)：1735-1780，*1997*）。LSTM的构建模块是**记忆单元**，它本质上代表或取代了标准RNN的隐藏层。
- en: 'In each memory cell, there is a recurrent edge that has the desirable weight,
    *w* = 1, as we discussed, to overcome the vanishing and exploding gradient problems.
    The values associated with this recurrent edge are collectively called the **cell
    state**. The unfolded structure of a modern LSTM cell is shown in the following
    figure:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个记忆单元中，都有一条递归边，其权重为我们所讨论的理想值 *w* = 1，用以克服梯度消失和爆炸问题。与此递归边相关的值统称为 **单元状态**。现代LSTM单元的展开结构如下图所示：
- en: '![](img/B13208_16_09.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_09.png)'
- en: Notice that the cell state from the previous time step, ![](img/B13208_16_050.png),
    is modified to get the cell state at the current time step, ![](img/B13208_16_051.png),
    without being multiplied directly with any weight factor. The flow of information
    in this memory cell is controlled by several computation units (often called *gates*)
    that will be described here. In the previous figure, ![](img/B13208_16_052.png)
    refers to the **element-wise product** (element-wise multiplication) and ![](img/B13208_16_053.png)
    means **element-wise summation** (element-wise addition). Furthermore, ![](img/B13208_16_054.png)
    refers to the input data at time *t*, and ![](img/B13208_16_055.png) indicates
    the hidden units at time *t* – 1\. Four boxes are indicated with an activation
    function, either the sigmoid function (![](img/B13208_16_056.png)) or tanh, and
    a set of weights; these boxes apply a linear combination by performing matrix-vector
    multiplications on their inputs (which are ![](img/B13208_16_057.png) and ![](img/B13208_16_058.png)).
    These units of computation with sigmoid activation functions, whose output units
    are passed through ![](img/B13208_16_059.png), are called gates.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，从前一个时间步的单元状态！[](img/B13208_16_050.png)被修改以获得当前时间步的单元状态！[](img/B13208_16_051.png)，并没有直接与任何权重因子相乘。这个记忆单元的信息流由几个计算单元（通常称为*门*）控制，这些计算单元将在此处描述。在前图中，![](img/B13208_16_052.png)表示**元素级乘积**（逐元素乘法），而![](img/B13208_16_053.png)表示**元素级求和**（逐元素加法）。此外，![](img/B13208_16_054.png)表示时间*t*的输入数据，而![](img/B13208_16_055.png)表示时间*t*
    - 1的隐藏单元。四个框表示带有激活函数的单元，激活函数可能是sigmoid函数（![](img/B13208_16_056.png)）或tanh函数，并且有一组权重；这些框通过对输入（即![](img/B13208_16_057.png)和![](img/B13208_16_058.png)）执行矩阵向量乘法，应用线性组合。这些带有sigmoid激活函数的计算单元，其输出单位经过![](img/B13208_16_059.png)处理，称为“门”。
- en: 'In an LSTM cell, there are three different types of gates, which are known
    as the forget gate, the input gate, and the output gate:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSTM单元中，有三种不同类型的门，它们分别是遗忘门、输入门和输出门：
- en: The **forget gate** ( ![](img/B13208_16_060.png)) allows the memory cell to
    reset the cell state without growing indefinitely. In fact, the forget gate decides
    which information is allowed to go through and which information to suppress.
    Now, ![](img/B13208_16_0601.png) is computed as follows:![](img/B13208_16_062.png)
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**（![](img/B13208_16_060.png)）允许记忆单元重置单元状态，从而避免无限增长。实际上，遗忘门决定哪些信息允许通过，哪些信息需要抑制。现在，![](img/B13208_16_0601.png)的计算方式如下：![](img/B13208_16_062.png)'
- en: 'Note that the forget gate was not part of the original LSTM cell; it was added
    a few years later to improve the original model (*Learning to Forget: Continual
    Prediction with LSTM*, *F. Gers*, *J. Schmidhuber*, and *F. Cummins*, *Neural
    Computation 12*, 2451-2471, *2000*).'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，遗忘门并不是原始LSTM单元的一部分；它是在几年后被添加进来的，用以改进原始模型（*学习遗忘：LSTM的持续预测*，*F. Gers*，*J.
    Schmidhuber*，和*F. Cummins*，*神经计算 12*，2451-2471，*2000*）。
- en: The **input gate** (![](img/B13208_16_063.png)) and **candidate value** (![](img/B13208_16_064.png))
    are responsible for updating the cell state. They are computed as follows:![](img/B13208_16_065.png)![](img/B13208_16_066.png)
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**（![](img/B13208_16_063.png)）和**候选值**（![](img/B13208_16_064.png)）负责更新单元状态。它们的计算方式如下：![](img/B13208_16_065.png)![](img/B13208_16_066.png)'
- en: 'The cell state at time *t* is computed as follows:'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在时间*t*的单元状态计算如下：
- en: '![](img/B13208_16_067.png)'
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B13208_16_067.png)'
- en: The **output gate** (![](img/B13208_16_068.png)) decides how to update the values
    of hidden units:![](img/B13208_16_069.png)
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**（![](img/B13208_16_068.png)）决定如何更新隐藏单元的值：![](img/B13208_16_069.png)'
- en: 'Given this, the hidden units at the current time step are computed as follows:'
  id: totrans-116
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 给定这一点，当前时间步的隐藏单元计算如下：
- en: '![](img/B13208_16_070.png)'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/B13208_16_070.png)'
- en: The structure of an LSTM cell and its underlying computations might seem very
    complex and hard to implement. However, the good news is that TensorFlow has already
    implemented everything in optimized wrapper functions, which allows us to define
    our LSTM cells easily and efficiently. We will apply RNNs and LSTMs to real-world
    datasets later in this chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM单元的结构及其底层计算可能看起来非常复杂且难以实现。然而，好消息是，TensorFlow已经将所有内容实现为优化的包装函数，使我们能够轻松高效地定义LSTM单元。我们将在本章后面将RNN和LSTM应用于实际数据集。
- en: '**Other advanced RNN models**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**其他先进的RNN模型**'
- en: LSTMs provide a basic approach for modeling long-range dependencies in sequences.
    Yet, it is important to note that there are many variations of LSTMs described
    in literature (*An Empirical Exploration of Recurrent Network Architectures*,
    *Rafal Jozefowicz*, *Wojciech Zaremba*, and *Ilya Sutskever*, *Proceedings of
    ICML*, 2342-2350, *2015*). Also worth noting is a more recent approach, Gated
    Recurrent Unit (GRU), which was proposed in 2014\. GRUs have a simpler architecture
    than LSTMs; therefore, they are computationally more efficient, while their performance
    in some tasks, such as polyphonic music modeling, is comparable to LSTMs. If you
    are interested in learning more about these modern RNN architectures, refer to
    the paper, *Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
    Modeling*, by *Junyoung Chung* and others, *2014* ([https://arxiv.org/pdf/1412.3555v1.pdf](https://arxiv.org/pdf/1412.3555v1.pdf)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM提供了一种基础的方法，用于建模序列中的长距离依赖关系。然而，值得注意的是，文献中有许多不同类型的LSTM变种（*An Empirical Exploration
    of Recurrent Network Architectures*，*Rafal Jozefowicz*，*Wojciech Zaremba*，*Ilya
    Sutskever*，*Proceedings of ICML*，2342-2350，*2015*）。同样值得注意的是，2014年提出的一种更新方法——门控递归单元（GRU）。GRU的架构比LSTM更为简洁，因此在计算上更高效，同时在一些任务（如多音音乐建模）中的表现与LSTM相当。如果你对这些现代RNN架构感兴趣，可以参考*Junyoung
    Chung*等人的论文*Empirical Evaluation of Gated Recurrent Neural Networks on Sequence
    Modeling*，*2014*（[https://arxiv.org/pdf/1412.3555v1.pdf](https://arxiv.org/pdf/1412.3555v1.pdf)）。
- en: Implementing RNNs for sequence modeling in TensorFlow
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在TensorFlow中实现RNN用于序列建模
- en: 'Now that we have covered the underlying theory behind RNNs, we are ready to
    move on to the more practical portion of this chapter: implementing RNNs in TensorFlow.
    During the rest of this chapter, we will apply RNNs to two common problem tasks:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了RNN背后的基础理论，准备进入本章的更实际部分：在TensorFlow中实现RNN。在接下来的章节中，我们将把RNN应用于两个常见的任务：
- en: Sentiment analysis
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 情感分析
- en: Language modeling
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语言建模
- en: These two projects, which we will walk through together in the following pages,
    are both fascinating but also quite involved. Thus, instead of providing the code
    all at once, we will break the implementation up into several steps and discuss
    the code in detail. If you like to have a big picture overview and want to see
    all the code at once before diving into the discussion, take a look at the code
    implementation first, which you can view at [https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch16](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch16).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个项目将在接下来的页面中一起讨论，既有趣又涉及内容丰富。因此，我们不会一次性给出所有代码，而是将实现过程分成几个步骤，并详细讨论代码。如果你想先了解大致框架，并在深入讨论之前看到完整代码，可以先查看代码实现，网址为[https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch16](https://github.com/rasbt/python-machine-learning-book-3rd-edition/tree/master/ch16)。
- en: Project one – predicting the sentiment of IMDb movie reviews
  id: totrans-126
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目一——预测IMDb电影评论的情感
- en: You may recall from *Chapter 8*, *Applying Machine Learning to Sentiment Analysis*,
    that sentiment analysis is concerned with analyzing the expressed opinion of a
    sentence or a text document. In this section and the following subsections, we
    will implement a multilayer RNN for sentiment analysis using a many-to-one architecture.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得在*第8章*，*将机器学习应用于情感分析*中，情感分析是关于分析句子或文本文档中表达的意见。在本节及随后的子节中，我们将使用多层RNN实现情感分析，采用多对一架构。
- en: In the next section, we will implement a many-to-many RNN for an application
    of language modeling. While the chosen examples are purposefully simple to introduce
    the main concepts of RNNs, language modeling has a wide range of interesting applications,
    such as building chatbots—giving computers the ability to directly talk and interact
    with humans.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将实现一个多对多的RNN，应用于语言建模。虽然所选择的示例故意简单，以介绍RNN的主要概念，但语言建模有许多有趣的应用，比如构建聊天机器人——赋予计算机直接与人类交谈和互动的能力。
- en: Preparing the movie review data
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备电影评论数据
- en: 'In the preprocessing steps in *Chapter 8*, we created a clean dataset named
    `movie_data.csv`, which we will use again now. First, we will import the necessary
    modules and read the data into a pandas `DataFrame`, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*的预处理步骤中，我们创建了一个名为`movie_data.csv`的干净数据集，今天我们将再次使用它。首先，我们将导入必要的模块，并将数据读取到pandas
    `DataFrame`中，如下所示：
- en: '[PRE2]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Remember that this data frame, `df`, consists of two columns, namely `'review'`
    and `'sentiment'`, where `'review'` contains the text of movie reviews (the input
    features), and `'sentiment'` represents the target label we want to predict (`0`
    refers to negative sentiment and `1` refers to positive sentiment). The text component
    of these movie reviews is sequences of words, and the RNN model classifies each
    sequence as a positive (`1`) or negative (`0`) review.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这个数据框 `df` 包含两列，分别是 `'review'` 和 `'sentiment'`，其中 `'review'` 包含电影评论的文本（输入特征），而
    `'sentiment'` 代表我们想要预测的目标标签（`0` 代表负面情感，`1` 代表正面情感）。这些电影评论的文本由一系列单词组成，RNN 模型将每个序列分类为正面（`1`）或负面（`0`）评论。
- en: 'However, before we can feed the data into an RNN model, we need to apply several
    preprocessing steps:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们将数据输入到 RNN 模型之前，我们需要执行几个预处理步骤：
- en: Create a TensorFlow dataset object and split it into separate training, testing,
    and validation partitions.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个 TensorFlow 数据集对象，并将其划分为独立的训练、测试和验证分区。
- en: Identify the unique words in the training dataset.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别训练数据集中的唯一词。
- en: Map each unique word to a unique integer and encode the review text into encoded
    integers (an index of each unique word).
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个独特的词映射到一个唯一的整数，并将评论文本编码为编码后的整数（每个唯一词的索引）。
- en: Divide the dataset into mini-batches as input to the model.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集划分为小批次，作为模型的输入。
- en: 'Let''s proceed with the first step: creating a TensorFlow dataset from this
    data frame:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始：从这个数据框创建一个 TensorFlow 数据集：
- en: '[PRE3]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, we can split it into training, testing, and validation datasets. The entire
    dataset contains 50,000 examples. We will keep the first 25,000 examples for evaluation
    (hold-out testing dataset), and then 20,000 examples will be used for training
    and 5,000 for validation. The code is as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将其划分为训练集、测试集和验证集。整个数据集包含 50,000 个样本。我们将保留前 25,000 个样本用于评估（保留测试集），然后 20,000
    个样本用于训练，5,000 个样本用于验证。代码如下：
- en: '[PRE4]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: To prepare the data for input to a NN, we need to encode it into numeric values,
    as was mentioned in steps 2 and 3\. To do this, we will first find the unique
    words (tokens) in the training dataset. While finding unique tokens is a process
    for which we can use Python datasets, it can be more efficient to use the `Counter`
    class from the `collections` package, which is part of Python's standard library.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据准备好输入到神经网络（NN），我们需要将其编码为数值，如步骤 2 和 3 所提到的那样。为此，我们将首先在训练数据集中找到唯一的单词（标记）。虽然找到唯一的标记是一个可以使用
    Python 数据集的过程，但使用 Python 标准库中的 `collections` 包中的 `Counter` 类会更高效。
- en: In the following code, we will instantiate a new `Counter` object (`token_counts`)
    that will collect the unique word frequencies. Note that in this particular application
    (and in contrast to the bag-of-words model), we are only interested in the set
    of unique words and won't require the word counts, which are created as a side
    product. To split the text into words (or tokens), the `tensorflow_datasets` package
    provides a `Tokenizer` class.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们将实例化一个新的 `Counter` 对象（`token_counts`），该对象将收集唯一单词的频率。请注意，在这个特定的应用中（与词袋模型不同），我们只关心唯一词的集合，而不需要词频，它们是作为副产品创建的。为了将文本拆分为单词（或标记），`tensorflow_datasets`
    包提供了一个 `Tokenizer` 类。
- en: 'The code for collecting unique tokens is as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 收集唯一标记的代码如下：
- en: '[PRE5]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you want to learn more about `Counter`, refer to its documentation at [https://docs.python.org/3/library/collections.html#collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于 `Counter` 的信息，可以参考其文档：[https://docs.python.org/3/library/collections.html#collections.Counter](https://docs.python.org/3/library/collections.html#collections.Counter)。
- en: 'Next, we are going to map each unique word to a unique integer. This can be
    done manually using a Python dictionary, where the keys are the unique tokens
    (words) and the value associated with each key is a unique integer. However, the
    `tensorflow_datasets` package already provides a class, `TokenTextEncoder`, which
    we can use to create such a mapping and encode the entire dataset. First, we will
    create an `encoder` object from the `TokenTextEncoder` class by passing the unique
    tokens (`token_counts` contains the tokens and their counts, although here, their
    counts are not needed, so they will be ignored). Calling the `encoder.encode()`
    method will then convert its input text into a list of integer values:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将把每个独特的单词映射到一个唯一的整数。这可以通过手动使用 Python 字典来完成，其中键是独特的词汇（单词），每个键对应的值是唯一的整数。然而，`tensorflow_datasets`
    包已经提供了一个类，`TokenTextEncoder`，我们可以用它来创建这样的映射并对整个数据集进行编码。首先，我们将通过传递唯一的词汇（`token_counts`
    包含了词汇和它们的计数，尽管这里计数不需要，因此会被忽略）来从 `TokenTextEncoder` 类创建一个 `encoder` 对象。调用 `encoder.encode()`
    方法将把输入文本转换为一个整数值的列表：
- en: '[PRE6]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Note that there might be some tokens in the validation or testing data that
    are not present in the training data and are thus not included in the mapping.
    If we have *q* tokens (that is the size of `token_counts` passed to the `TokenTextEncoder`,
    which in this case is 87,007), then all tokens that haven't been seen before,
    and are thus not included in `token_counts`, will be assigned the integer *q*
    + 1 (which will be 87,008 in our case). In other words, the index *q* + 1 is reserved
    for unknown words. Another reserved value is the integer 0, which serves as a
    placeholder for adjusting the sequence length. Later, when we are building an
    RNN model in TensorFlow, we will consider these two placeholders, 0 and *q* +
    1, in more detail.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，验证数据或测试数据中可能存在一些在训练数据中没有出现的词汇，因此它们不包含在映射中。如果我们有 *q* 个词汇（即传递给 `TokenTextEncoder`
    的 `token_counts` 大小，在此案例中为 87,007），那么所有之前未见过的词汇，因而不在 `token_counts` 中的，将被分配整数
    *q* + 1（在我们这个例子中为 87,008）。换句话说，索引 *q* + 1 被保留给未知词汇。另一个保留值是整数 0，它作为占位符来调整序列长度。稍后，当我们在
    TensorFlow 中构建 RNN 模型时，我们将更加详细地考虑这两个占位符，0 和 *q* + 1。
- en: 'We can use the `map()` method of the dataset objects to transform each text
    in the dataset accordingly, just like we would apply any other transformation
    to a dataset. However, there is a small problem: here, the text data is enclosed
    in tensor objects, which we can access by calling the `numpy()` method on a tensor
    in the eager execution mode. But during transformations by the `map()` method,
    the eager execution will be disabled. To solve this problem, we can define two
    functions. The first function will treat the input tensors as if the eager execution
    mode is enabled:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用数据集对象的 `map()` 方法来相应地转换数据集中的每个文本，就像我们对数据集应用任何其他变换一样。然而，有一个小问题：在这里，文本数据被封装在张量对象中，我们可以通过在激活执行模式下调用张量的
    `numpy()` 方法来访问它们。但在通过 `map()` 方法进行转换时，激活执行将被禁用。为了解决这个问题，我们可以定义两个函数，第一个函数将处理输入的张量，就像启用了激活执行模式一样：
- en: '[PRE7]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the second function, we will wrap the first function using `tf.py_function`
    to convert it into a TensorFlow operator, which can then be used via its `map()`
    method. This process of encoding text into a list of integers can be carried out
    using the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个函数中，我们将使用 `tf.py_function` 包装第一个函数，将其转换为 TensorFlow 操作符，之后可以通过其 `map()`
    方法使用。将文本编码为整数列表的过程可以使用以下代码完成：
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: So far, we've converted sequences of words into sequences of integers. However,
    there is one issue that we still need to resolve—the sequences currently have
    different lengths (as shown in the result of executing the previous code for five
    randomly chosen examples). Although, in general, RNNs can handle sequences with
    different lengths, we still need to make sure that all the sequences in a mini-batch
    have the same length to store them efficiently in a tensor.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经将单词序列转换为整数序列。然而，我们仍然有一个问题需要解决——当前的序列长度不同（如执行前面代码后，随机选择的五个例子的结果所示）。尽管一般来说，RNN
    可以处理不同长度的序列，但我们仍需要确保每个小批次中的所有序列具有相同的长度，以便能有效地存储在张量中。
- en: 'To divide a dataset that has elements with different shapes into mini-batches,
    TensorFlow provides a different method, `padded_batch()` (instead of `batch()`),
    which will automatically pad the consecutive elements that are to be combined
    into a batch with placeholder values (0s) so that all sequences within a batch
    will have the same shape. To illustrate this with a practical example, let''s
    take a small subset of size 8 from the training dataset, `ds_train`, and apply
    the `padded_batch()` method to this subset with `batch_size=4`. We will also print
    the sizes of the individual elements before combining these into mini-batches,
    as well as the dimensions of the resulting mini-batches:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将具有不同形状元素的数据集划分为 mini-batch，TensorFlow 提供了一种不同的方法 `padded_batch()`（代替 `batch()`），它会自动将要合并为批次的连续元素用占位符值（0）进行填充，从而确保每个批次中的所有序列具有相同的形状。为了通过实际示例说明这一点，假设我们从训练数据集
    `ds_train` 中提取一个大小为 8 的小子集，并对该子集应用 `padded_batch()` 方法，`batch_size=4`。我们还将打印合并成
    mini-batch 前单个元素的大小，以及结果 mini-batch 的维度：
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As you can observe from the printed tensor shapes, the number of columns (that
    is, `.shape[1]`) in the first batch is 688, which resulted from combining the
    first four examples into a single batch and using the maximum size of these examples.
    That means that the other three examples in this batch are padded as much as necessary
    to match this size. Similarly, the second batch keeps the maximum size of its
    individual four examples, which is 453, and pads the other examples so that their
    length is smaller than the maximum length.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如从打印的张量形状中可以观察到，第一批次的列数（即 `.shape[1]`）为 688，这是通过将前四个示例合并成一个批次并使用这些示例的最大大小得到的。这意味着该批次中的其他三个示例已经被填充了必要的零值，以匹配此大小。类似地，第二批次保持其四个示例的最大大小，即
    453，并填充其他示例，使它们的长度小于最大长度。
- en: 'Let''s divide all three datasets into mini-batches with a batch size of 32:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有三个数据集分成 mini-batch，每个批次大小为 32：
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Now, the data is in a suitable format for an RNN model, which we are going to
    implement in the following subsections. In the next subsection, however, we will
    first discuss feature **embedding**, which is an optional but highly recommended
    preprocessing step that is used for reducing the dimensionality of the word vectors.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，数据已经被转换为适合 RNN 模型的格式，我们将在接下来的子章节中实现该模型。然而，在下一节中，我们将首先讨论特征**嵌入**，这是一种可选但强烈推荐的预处理步骤，用于减少词向量的维度。
- en: Embedding layers for sentence encoding
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用于句子编码的嵌入层
- en: During the data preparation in the previous step, we generated sequences of
    the same length. The elements of these sequences were integer numbers that corresponded
    to the *indices* of unique words. These word indices can be converted into input
    features in several different ways. One naive way is to apply one-hot encoding
    to convert the indices into vectors of zeros and ones. Then, each word will be
    mapped to a vector whose size is the number of unique words in the entire dataset.
    Given that the number of unique words (the size of the vocabulary) can be in the
    order of ![](img/B13208_16_071.png), which will also be the number of our input
    features, a model trained on such features may suffer from the **curse of dimensionality**.
    Furthermore, these features are very sparse, since all are zero except one.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一步的数据准备过程中，我们生成了相同长度的序列。这些序列的元素是整数，表示唯一单词的*索引*。这些单词索引可以通过几种不同的方式转换为输入特征。一种简单的方法是应用
    one-hot 编码，将索引转换为零和一的向量。然后，每个单词将被映射到一个向量，其大小是整个数据集中唯一单词的数量。考虑到唯一单词的数量（词汇表大小）可能达到
    ![](img/B13208_16_071.png)，这也将是我们的输入特征的数量，基于这些特征训练的模型可能会遭遇**维度灾难**。此外，这些特征非常稀疏，因为除了一个以外，所有元素都是零。
- en: A more elegant approach is to map each word to a vector of a fixed size with
    real-valued elements (not necessarily integers). In contrast to the one-hot encoded
    vectors, we can use finite-sized vectors to represent an infinite number of real
    numbers. (In theory, we can extract infinite real numbers from a given interval,
    for example [–1, 1].)
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更优雅的方法是将每个单词映射到一个具有固定大小的向量，向量元素为实数（不一定是整数）。与 one-hot 编码向量不同，我们可以使用有限大小的向量来表示无限多个实数。（理论上，我们可以从给定的区间中提取无限多个实数，例如
    [–1, 1]。）
- en: This is the idea behind embedding, which is a feature-learning technique that
    we can utilize here to automatically learn the salient features to represent the
    words in our dataset. Given the number of unique words, ![](img/B13208_16_072.png),
    we can select the size of the embedding vectors (a.k.a., embedding dimension)
    to be much smaller than the number of unique words (![](img/B13208_16_073.png))
    to represent the entire vocabulary as input features.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是嵌入（embedding）背后的思想，它是一种特征学习技术，我们可以在这里利用它自动学习显著特征来表示数据集中的单词。考虑到唯一单词的数量，![](img/B13208_16_072.png)，我们可以选择嵌入向量的大小（即嵌入维度）远小于唯一单词的数量（![](img/B13208_16_073.png)），以便将整个词汇表表示为输入特征。
- en: 'The advantages of embedding over one-hot encoding are as follows:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入相对于独热编码的优势如下：
- en: A reduction in the dimensionality of the feature space to decrease the effect
    of the curse of dimensionality
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少特征空间的维度来降低维度灾难的影响
- en: The extraction of salient features since the embedding layer in an NN can be
    optimized (or learned)
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取显著特征，因为神经网络中的嵌入层可以被优化（或学习）
- en: 'The following schematic representation shows how embedding works by mapping
    token indices to a trainable embedding matrix:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示意图展示了嵌入是如何通过将标记索引映射到一个可训练的嵌入矩阵来工作的：
- en: '![](img/B13208_16_10.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_10.png)'
- en: 'Given a set of tokens of size *n* + 2 (*n* is the size of the token set, plus
    index 0 is reserved for the padding placeholder, and *n* + 1 is for the words
    not present in the token set), an embedding matrix of size ![](img/B13208_16_074.png)
    will be created where each row of this matrix represents numeric features associated
    with a token. Therefore, when an integer index, *i*, is given as input to the
    embedding, it will look up the corresponding row of the matrix at index *i* and
    return the numeric features. The embedding matrix serves as the input layer to
    our NN models. In practice, creating an embedding layer can simply be done using
    `tf.keras.layers.Embedding`. Let''s see an example where we will create a model
    and add an embedding layer, as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个包含*n* + 2个标记的集合（*n*是标记集合的大小，索引0保留用于填充占位符，*n* + 1用于表示不在标记集合中的单词），将创建一个大小为![](img/B13208_16_074.png)的嵌入矩阵，其中矩阵的每一行表示与一个标记相关联的数值特征。因此，当输入一个整数索引*i*时，嵌入层将查找矩阵中索引*i*对应的行，并返回数值特征。嵌入矩阵作为我们的神经网络模型的输入层。在实践中，创建嵌入层可以通过`tf.keras.layers.Embedding`来简便地实现。接下来让我们看一个示例，我们将创建一个模型并添加嵌入层，如下所示：
- en: '[PRE11]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The input to this model (embedding layer) must have rank 2 with dimensionality
    ![](img/B13208_16_075.png), where ![](img/B13208_16_076.png) is the length of
    sequences (here, set to 20 via the `input_length` argument). For example, an input
    sequence in the mini-batch could be ![](img/B13208_16_077.png), where each element
    of this sequence is the index of the unique words. The output will have dimensionality
    ![](img/B13208_16_078.png), where ![](img/B13208_16_079.png) is the size of the
    embedding features (here, set to 6 via `output_dim`). The other argument provided
    to the embedding layer, `input_dim,` corresponds to the unique integer values
    that the model will receive as input (for instance, *n* + 2, set here to 100).
    Therefore, the embedding matrix in this case has the size ![](img/B13208_16_080.png).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型的输入（嵌入层）必须具有2的秩，维度为![](img/B13208_16_075.png)，其中![](img/B13208_16_076.png)是序列的长度（在这里通过`input_length`参数设置为20）。例如，迷你批次中的一个输入序列可能是![](img/B13208_16_077.png)，其中每个元素是唯一单词的索引。输出将具有维度![](img/B13208_16_078.png)，其中![](img/B13208_16_079.png)是嵌入特征的大小（在这里通过`output_dim`设置为6）。嵌入层的另一个参数`input_dim`对应于模型将接收的唯一整数值（例如，*n*
    + 2，这里设置为100）。因此，在这种情况下，嵌入矩阵的大小是![](img/B13208_16_080.png)。
- en: '**Dealing with variable sequence lengths**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理可变序列长度**'
- en: Note that the `input_length` argument is not required, and we can use `None`
    for cases where the lengths of input sequences vary. You can find more information
    about this function in the official documentation at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`input_length`参数不是必需的，我们可以在输入序列长度变化的情况下使用`None`。你可以在官方文档中找到更多关于此函数的信息，链接是：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding)。
- en: Building an RNN model
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建一个RNN模型
- en: 'Now we''re ready to build an RNN model. Using the Keras `Sequential` class,
    we can combine the embedding layer, the recurrent layers of the RNN, and the fully
    connected non-recurrent layers. For the recurrent layers, we can use any of the
    following implementations:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备好构建RNN模型了。使用Keras的 `Sequential` 类，我们可以将嵌入层、RNN的循环层和非循环的全连接层结合起来。对于循环层，我们可以使用以下任何一种实现：
- en: '`SimpleRNN`: a regular RNN layer, that is, a fully connected recurrent layer'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SimpleRNN`：一个常规的RNN层，即完全连接的循环层'
- en: '`LSTM`: a long short-term memory RNN, which is useful for capturing the long-term
    dependencies'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`LSTM`：长短期记忆RNN，用于捕捉长期依赖关系'
- en: '`GRU`: a recurrent layer with a gated recurrent unit, as proposed in *Learning
    Phrase Representations Using RNN Encoder–Decoder for Statistical Machine Translation*
    ([https://arxiv.org/abs/1406.1078v3](https://arxiv.org/abs/1406.1078v3)), as an
    alternative to LSTMs'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`GRU`：带有门控循环单元的循环层，正如在 *《使用RNN编码器-解码器进行统计机器翻译学习短语表示》*（[https://arxiv.org/abs/1406.1078v3](https://arxiv.org/abs/1406.1078v3)）中提出的，作为LSTM的替代方案'
- en: 'To see how a multilayer RNN model can be built using one of these recurrent
    layers, in the following example, we will create an RNN model, starting with an
    embedding layer with `input_dim=1000` and `output_dim=32`. Then, two recurrent
    layers of type `SimpleRNN` will be added. Finally, we will add a non-recurrent
    fully connected layer as the output layer, which will return a single output value
    as the prediction:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看如何使用这些循环层之一构建多层RNN模型，在以下示例中，我们将创建一个RNN模型，首先使用 `input_dim=1000` 和 `output_dim=32`
    的嵌入层。然后，添加两个类型为 `SimpleRNN` 的循环层。最后，我们将添加一个非循环的全连接层作为输出层，该层将返回一个单一的输出值作为预测：
- en: '[PRE12]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you can see, building an RNN model using these recurrent layers is pretty
    straightforward. In the next subsection, we will go back to our sentiment analysis
    task and build an RNN model to solve that.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用这些循环层构建RNN模型是相当简单的。在接下来的小节中，我们将回到情感分析任务，并构建一个RNN模型来解决该问题。
- en: Building an RNN model for the sentiment analysis task
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为情感分析任务构建RNN模型
- en: 'Since we have very long sequences, we are going to use an LSTM layer to account
    for long-term effects. In addition, we will put the LSTM layer inside a `Bidirectional`
    wrapper, which will make the recurrent layers pass through the input sequences
    from both directions, start to end, as well as the reverse direction:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有非常长的序列，我们将使用LSTM层来考虑长期效应。此外，我们还会将LSTM层放入 `Bidirectional` 包装器中，这样循环层将从两个方向处理输入序列，从前到后以及反向方向：
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: After training this model for 10 epochs, evaluation on the test data shows 85
    percent accuracy. (Note that this result is not the best when compared to the
    state-of-the-art methods used on the IMDb dataset. The goal was simply to show
    how RNN works.)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练此模型10个epoch后，对测试数据的评估显示准确率为85%。 （请注意，与在IMDb数据集上使用的最先进方法相比，这个结果并不是最好的。目标仅仅是展示RNN如何工作。）
- en: '**More on the bidirectional RNN**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于双向RNN的更多内容**'
- en: 'The `Bidirectional` wrapper makes two passes over each input sequence: a forward
    pass and a reverse or backward pass (note that this is not to be confused with
    the forward and backward passes in the context of backpropagation). The results
    of these forward and backward passes will be concatenated by default. But if you
    want to change this behavior, you can set the argument `merge_mode` to `''sum''`
    (for summation), `''mul''` (for multiplying the results of the two passes), `''ave''`
    (for taking the average of the two), `''concat''` (which is the default), or `None`,
    which returns the two tensors in a list. For more information about the `Bidirectional`
    wrapper, feel free to look at the official documentation at [https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Bidirectional](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Bidirectional).'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`Bidirectional` 包装器对每个输入序列进行两次处理：一次前向传递和一次反向或后向传递（注意，这与反向传播中的前向和后向传递不同）。这些前向和后向传递的结果默认会被连接在一起。但如果你想改变这个行为，可以将参数
    `merge_mode` 设置为 `''sum''`（求和）、`''mul''`（将两个传递结果相乘）、`''ave''`（取两个结果的平均值）、`''concat''`（默认值）或
    `None`，后者会将两个张量返回为列表。如需了解有关 `Bidirectional` 包装器的更多信息，请查看官方文档：[https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Bidirectional](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Bidirectional)。'
- en: We can also try other types of recurrent layers, such as `SimpleRNN`. However,
    as it turns out, a model built with regular recurrent layers won't be able to
    reach a good predictive performance (even on the training data). For example,
    if you try replacing the bidirectional LSTM layer in the previous code with a
    unidirectional `SimpleRNN` layer and train the model on full-length sequences,
    you may observe that the loss will not even decrease during training. The reason
    is that the sequences in this dataset are too long, so a model with a `SimpleRNN`
    layer cannot learn the long-term dependencies and may suffer from vanishing or
    exploding gradient problems.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试其他类型的递归层，例如 `SimpleRNN`。然而，事实证明，使用常规递归层构建的模型无法达到良好的预测性能（即使是在训练数据上）。例如，如果你尝试用单向的
    `SimpleRNN` 层替换前面代码中的双向 LSTM 层，并用全长序列训练模型，你可能会发现训练过程中损失甚至没有下降。原因是该数据集中的序列过长，因此具有
    `SimpleRNN` 层的模型无法学习长期依赖关系，可能会遭遇梯度消失或爆炸的问题。
- en: In order to obtain reasonable predictive performance on this dataset using a
    `SimpleRNN`, we can truncate the sequences. Also, utilizing our "domain knowledge,"
    we may hypothesize that the last paragraphs of a movie review may contain most
    of the information about its sentiment. Hence, we can focus only on the last portion
    of each review. To do this, we will define a helper function, `preprocess_datasets()`,
    to combine the preprocessing steps 2-4\. An optional argument to this function
    is `max_seq_length`, which determines how many tokens from each review should
    be used. For example, if we set `max_seq_length=100` and a review has more than
    100 tokens, only the last 100 tokens will be used. If `max_seq_length` is set
    to `None`, then full-length sequences will be used as before. Trying different
    values for `max_seq_length` will give us more insights on the capability of different
    RNN models to handle long sequences.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在此数据集上使用 `SimpleRNN` 获得合理的预测性能，我们可以截断序列。此外，利用我们的“领域知识”，我们可能会假设电影评论的最后几段包含大部分关于其情感的信息。因此，我们可以只关注每个评论的最后部分。为此，我们将定义一个辅助函数
    `preprocess_datasets()`，以结合预处理步骤 2-4。此函数的一个可选参数是 `max_seq_length`，它决定每个评论应使用多少个标记。例如，如果我们设置
    `max_seq_length=100` 并且某个评论的标记数超过 100，那么只会使用最后的 100 个标记。如果将 `max_seq_length` 设置为
    `None`，则会像之前一样使用全长序列。尝试不同的 `max_seq_length` 值将为我们提供更多关于不同 RNN 模型处理长序列能力的见解。
- en: 'The code for the `preprocess_datasets()` function is as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`preprocess_datasets()` 函数的代码如下：'
- en: '[PRE14]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will define another helper function, `build_rnn_model()`, for building
    models with different architectures more conveniently:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义另一个辅助函数 `build_rnn_model()`，用于更方便地构建具有不同架构的模型：
- en: '[PRE15]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, using these two fairly general, but convenient, helper functions, we can
    readily compare different RNN models with different input sequence lengths. As
    an example, in the following code, we will try a model with a single recurrent
    layer of type `SimpleRNN` while truncating the sequences to a maximum length of
    100 tokens:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，利用这两个相对通用且便捷的辅助函数，我们可以轻松比较不同的 RNN 模型和不同输入序列长度的表现。作为示例，在下面的代码中，我们将尝试使用一个具有单层
    `SimpleRNN` 的模型，并将序列截断为最多 100 个标记的长度：
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: For instance, truncating the sequences to 100 tokens and using a bidirectional
    `SimpleRNN` layer resulted in 80 percent classification accuracy. Although the
    prediction is slightly lower when compared to the previous bidirectional LSTM
    model (85.15 percent accuracy on the test dataset), the performance on these truncated
    sequences is much better than the performance we could achieve with a `SimpleRNN`
    on full-length movie reviews. As an optional exercise, you can verify this by
    using the two helper functions we have already defined. Try it with `max_seq_length=None`
    and set the `bidirectional` argument inside the `build_rnn_model()` helper function
    to `False`. (For your convenience, this code is available in the online materials
    of this book.)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，将序列截断为 100 个标记并使用双向 `SimpleRNN` 层，最终获得了 80% 的分类准确率。尽管与之前的双向 LSTM 模型（在测试数据集上的准确率为
    85.15%）相比，预测稍低，但在这些截断序列上的表现要远好于我们用 `SimpleRNN` 对全长电影评论进行建模时的表现。作为一个可选练习，你可以通过使用我们已经定义的两个辅助函数来验证这一点。尝试将
    `max_seq_length=None`，并在 `build_rnn_model()` 辅助函数中将 `bidirectional` 参数设置为 `False`。
    （为了方便你，这段代码可以在本书的在线材料中找到。）
- en: Project two – character-level language modeling in TensorFlow
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 项目二 – 在 TensorFlow 中进行字符级语言建模
- en: Language modeling is a fascinating application that enables machines to perform
    human language-related tasks, such as generating English sentences. One of the
    interesting studies in this area is *Generating Text with Recurrent Neural Networks*,
    *Ilya Sutskever*, *James Martens*, and *Geoffrey E. Hinton*, *Proceedings of the
    28th International Conference on Machine Learning (ICML-11)*, *2011*, [https://pdfs.semanticscholar.org/93c2/0e38c85b69fc2d2eb314b3c1217913f7db11.pdf](https://pdfs.semanticscholar.org/93c2/0e38c85b69fc2d2eb314b3c1217913f7db11.pdf)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 语言建模是一项迷人的应用，使机器能够执行与人类语言相关的任务，例如生成英语句子。该领域的一个有趣研究是*生成文本的递归神经网络*，*伊利亚·苏茨凯弗*，*詹姆斯·马滕斯*，*杰弗里·E·辛顿*，*第28届国际机器学习会议论文集（ICML-11）*，*2011年*，[https://pdfs.semanticscholar.org/93c2/0e38c85b69fc2d2eb314b3c1217913f7db11.pdf](https://pdfs.semanticscholar.org/93c2/0e38c85b69fc2d2eb314b3c1217913f7db11.pdf)。
- en: In the model that we will build now, the input is a text document, and our goal
    is to develop a model that can generate new text that is similar in style to the
    input document. Examples of such an input are a book or a computer program in
    a specific programming language.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们现在要构建的模型中，输入是一个文本文件，我们的目标是开发一个可以生成与输入文件风格相似的新文本的模型。这类输入的示例包括书籍或特定编程语言中的计算机程序。
- en: 'In character-level language modeling, the input is broken down into a sequence
    of characters that are fed into our network one character at a time. The network
    will process each new character in conjunction with the memory of the previously
    seen characters to predict the next one. The following figure shows an example
    of character-level language modeling (note that EOS stands for "end of sequence"):'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在字符级语言建模中，输入被分解为一系列字符，并且每次将一个字符输入到我们的网络中。网络将结合先前看到的字符的记忆处理每个新字符，以预测下一个字符。下图展示了一个字符级语言建模的示例（请注意，EOS代表“序列结束”）：
- en: '![](img/B13208_16_11.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_11.png)'
- en: 'We can break this implementation down into three separate steps: preparing
    the data, building the RNN model, and performing next-character prediction and
    sampling to generate new text.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此实现分为三个独立的步骤：准备数据、构建RNN模型，以及执行下一个字符预测和采样来生成新文本。
- en: Preprocessing the dataset
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集预处理
- en: In this section, we will prepare the data for character-level language modeling.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将准备字符级语言建模的数据。
- en: To obtain the input data, visit the Project Gutenberg website at [https://www.gutenberg.org/](https://www.gutenberg.org/),
    which provides thousands of free e-books. For our example, you can download the
    book *The Mysterious Island*, by Jules Verne (published in 1874) in plain text
    format from [http://www.gutenberg.org/files/1268/1268-0.txt](http://www.gutenberg.org/files/1268/1268-0.txt).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取输入数据，请访问[Project Gutenberg网站](https://www.gutenberg.org/)，该网站提供了成千上万的免费电子书。以我们的示例为例，您可以从[http://www.gutenberg.org/files/1268/1268-0.txt](http://www.gutenberg.org/files/1268/1268-0.txt)下载《神秘岛》一书（由儒勒·凡尔纳于1874年出版）的纯文本格式。
- en: 'Note that this link will take you directly to the download page. If you are
    using macOS or a Linux operating system, you can download the file with the following
    command in the terminal:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，此链接将直接带您到下载页面。如果您使用的是macOS或Linux操作系统，您可以在终端中使用以下命令下载文件：
- en: '[PRE17]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If this resource becomes unavailable in the future, a copy of this text is also
    included in this chapter's code directory in the book's code repository at [https://github.com/rasbt/python-machine-learning-book-3rd-edition/code/ch16](https://github.com/rasbt/python-machine-learning-book-3rd-edition/code/ch16).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果此资源将来无法使用，本章代码库中书籍的代码仓库也包含了该文本的副本，位于[https://github.com/rasbt/python-machine-learning-book-3rd-edition/code/ch16](https://github.com/rasbt/python-machine-learning-book-3rd-edition/code/ch16)。
- en: 'Once we have downloaded the dataset, we can read it into a Python session as
    plain text. Using the following code, we will read the text directly from the
    downloaded file and remove portions from the beginning and the end (these contain
    certain descriptions of the Gutenberg project). Then, we will create a Python
    variable, `char_set`, that represents the set of *unique* characters observed
    in this text:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们下载了数据集，就可以将其作为纯文本读取到Python会话中。使用以下代码，我们将直接从下载的文件中读取文本，并去掉开头和结尾的部分（这些部分包含一些关于古腾堡项目的描述）。然后，我们将创建一个Python变量`char_set`，表示在此文本中观察到的*唯一*字符集：
- en: '[PRE18]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After downloading and preprocessing the text, we have a sequence consisting
    of 1,112,350 characters in total and 80 unique characters. However, most NN libraries
    and RNN implementations cannot deal with input data in string format, which is
    why we have to convert the text into a numeric format. To do this, we will create
    a simple Python dictionary that maps each character to an integer, `char2int`.
    We will also need a reverse mapping to convert the results of our model back to
    text. Although the reverse can be done using a dictionary that associates integer
    keys with character values, using a NumPy array and indexing the array to map
    indices to those unique characters is more efficient. The following figure shows
    an example of converting characters into integers and the reverse for the words
    `"Hello"` and `"world"`:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 下载并预处理文本后，我们得到了一个包含1,112,350个字符、80个独特字符的序列。然而，大多数神经网络库和循环神经网络（RNN）实现无法处理字符串格式的输入数据，这就是为什么我们需要将文本转换为数字格式。为此，我们将创建一个简单的Python字典，将每个字符映射到一个整数，`char2int`。我们还需要一个反向映射，将模型的输出结果转换回文本。尽管可以通过使用字典将整数键与字符值关联来实现反向映射，但使用NumPy数组并通过索引数组将索引映射到这些独特的字符更高效。下图展示了将字符转换为整数及其反向操作的示例，以单词`"Hello"`和`"world"`为例：
- en: '![](img/B13208_16_12.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_12.png)'
- en: 'Building the dictionary to map characters to integers, and reverse mapping
    via indexing a NumPy array, as was shown in the previous figure, is as follows:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，构建字典以将字符映射到整数，并通过索引NumPy数组进行反向映射如下所示：
- en: '[PRE19]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The NumPy array `text_encoded` contains the encoded values for all the characters
    in the text. Now, we will create a TensorFlow dataset from this array:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy数组`text_encoded`包含文本中所有字符的编码值。现在，我们将从该数组创建一个TensorFlow数据集：
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: So far, we have created an iterable `Dataset` object for obtaining characters
    in the order they appear in the text. Now, let's step back and look at the big
    picture of what we are trying to do. For the text generation task, we can formulate
    the problem as a classification task.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经创建了一个可迭代的`Dataset`对象，用于按文本中字符出现的顺序获取字符。现在，让我们回顾一下我们正在尝试做的大局观。对于文本生成任务，我们可以将问题表述为一个分类任务。
- en: 'Suppose we have a set of sequences of text characters that are incomplete,
    as shown in the following figure:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组不完整的文本字符序列，如下图所示：
- en: '![](img/B13208_16_13.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_13.png)'
- en: In the previous figure, we can consider the sequences shown in the left-hand
    box to be the input. In order to generate new text, our goal is to design a model
    that can predict the next character of a given input sequence, where the input
    sequence represents an incomplete text. For example, after seeing *"Deep Learn"*,
    the model should predict *"i"* as the next character. Given that we have 80 unique
    characters, this problem becomes a multiclass classification task.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以将左侧框中显示的序列视为输入。为了生成新的文本，我们的目标是设计一个模型，该模型能够预测给定输入序列的下一个字符，其中输入序列表示一个不完整的文本。例如，在看到
    *"Deep Learn"* 后，模型应该预测 *"i"* 作为下一个字符。考虑到我们有80个独特的字符，这个问题变成了一个多类别分类任务。
- en: 'Starting with a sequence of length 1 (that is, one single letter), we can iteratively
    generate new text based on this multiclass classification approach, as illustrated
    in the following figure:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 从长度为1的序列开始（即一个单独的字母），我们可以基于这种多类别分类方法迭代地生成新文本，如下图所示：
- en: '![](img/B13208_16_14.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_14.png)'
- en: To implement the text generation task in TensorFlow, let's first clip the sequence
    length to 40\. This means that the input tensor, *x*, consists of 40 tokens. In
    practice, the sequence length impacts the quality of the generated text. Longer
    sequences can result in more meaningful sentences. For shorter sequences, however,
    the model might focus on capturing individual words correctly, while ignoring
    the context for the most part. Although longer sequences usually result in more
    meaningful sentences, as mentioned, for long sequences, the RNN model will have
    problems capturing long-term dependencies. Thus, in practice, finding a sweet
    spot and good value for the sequence length is a hyperparameter optimization problem,
    which we have to evaluate empirically. Here, we are going to choose 40, as it
    offers a good tradeoff.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在TensorFlow中实现文本生成任务，首先我们将序列长度限制为40。这意味着输入张量 *x* 包含40个标记。实际上，序列长度会影响生成文本的质量。较长的序列可能会生成更有意义的句子。然而，对于较短的序列，模型可能会专注于正确捕捉单个词汇，而大部分情况下忽略上下文。尽管较长的序列通常能生成更有意义的句子，但如前所述，对于长序列，RNN模型在捕捉长期依赖时可能会出现问题。因此，实际中，找到合适的序列长度是一项超参数优化问题，我们需要通过经验评估。在这里，我们选择40，因为它提供了一个较好的折衷方案。
- en: 'As you can see in the previous figure, the inputs, *x*, and targets, *y*, are
    offset by one character. Hence, we will split the text into chunks of size 41:
    the first 40 characters will form the input sequence, *x*, and the last 40 elements
    will form the target sequence, *y*.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，输入 *x* 和目标 *y* 相差一个字符。因此，我们将把文本拆分为41个字符的块：前40个字符将构成输入序列 *x*，最后40个元素将构成目标序列
    *y*。
- en: 'We have already stored the entire encoded text in its original order in a `Dataset`
    object, `ds_text_encoded`. Using the techniques concerning transforming datasets
    that we already covered in this chapter (in the section *Preparing the movie review
    data*), can you think of a way to obtain the input, *x*, and target, *y*, as it
    was shown in the previous figure? The answer is very simple: we will first use
    the `batch()` method to create text chunks consisting of 41 characters each. This
    means that we will set `batch_size=41`. We will further get rid of the last batch
    if it is shorter than 41 characters. As a result, the new chunked dataset, named
    `ds_chunks`, will always contain sequences of size 41\. The 41-character chunks
    will then be used to construct the sequence *x* (that is, the input), as well
    as the sequence *y* (that is, the target), both of which will have 40 elements.
    For instance, sequence *x* will consist of the elements with indices [0, 1, …,
    39]. Furthermore, since sequence *y* will be shifted by one position with respect
    to *x*, its corresponding indices will be [1, 2, …, 40]. Then, we will apply a
    transformation function using the `map()` method to separate the *x* and *y* sequences
    accordingly:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将整个编码后的文本按原始顺序存储在 `Dataset` 对象 `ds_text_encoded` 中。使用本章中已涉及的关于转换数据集的技术（在
    *准备电影评论数据* 部分），你能想到一种方法来获得输入 *x* 和目标 *y*，如前图所示吗？答案很简单：我们将首先使用 `batch()` 方法创建每个包含41个字符的文本块。这意味着我们将设置
    `batch_size=41`。如果最后一个批次少于41个字符，我们会将其去除。因此，新的分块数据集，命名为 `ds_chunks`，将始终包含41个字符大小的序列。然后，这些41个字符的块将用于构建序列
    *x*（即输入），以及序列 *y*（即目标），这两个序列都将包含40个元素。例如，序列 *x* 将由索引[0, 1, …, 39]的元素组成。此外，由于序列
    *y* 相对于 *x* 会向右移动一个位置，因此其对应的索引将是[1, 2, …, 40]。然后，我们将使用 `map()` 方法应用一个转换函数，相应地分离
    *x* 和 *y* 序列：
- en: '[PRE21]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s take a look at some example sequences from this transformed dataset:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下从这个转换后的数据集中提取的示例序列：
- en: '[PRE22]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, the last step in preparing the dataset is to divide this dataset into
    mini-batches. During the first preprocessing step to divide the dataset into batches,
    we created chunks of sentences. Each chunk represents one sentence, which corresponds
    to one training example. Now, we will shuffle the training examples and divide
    the inputs into mini-batches again; however, this time, each batch will contain
    multiple training examples:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，准备数据集的最后一步是将数据集划分为小批次。在第一次预处理步骤中，我们将数据集划分为批次时，创建了句子块。每个块代表一个句子，对应一个训练样本。现在，我们将重新洗牌训练样本，并再次将输入划分为小批次；不过这一次，每个批次将包含多个训练样本：
- en: '[PRE23]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Building a character-level RNN model
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 构建字符级RNN模型
- en: 'Now that the dataset is ready, building the model will be relatively straightforward.
    For code reusability, we will write a function, `build_model`, that defines an
    RNN model using the Keras `Sequential` class. Then, we can specify the training
    parameters and call that function to obtain an RNN model:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据集已经准备好，构建模型将相对简单。为了代码的可重用性，我们将编写一个名为`build_model`的函数，通过Keras的`Sequential`类定义一个RNN模型。然后，我们可以指定训练参数并调用该函数来获取RNN模型：
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Notice that the LSTM layer in this model has the output shape `(None, None,
    512)`, which means the output of LSTM is rank 3\. The first dimension stands for
    the number of batches, the second dimension for the output sequence length, and
    the last dimension corresponds to the number of hidden units. The reason for having
    rank-3 output from the LSTM layer is because we have specified `return_sequences=True`
    when defining our LSTM layer. A fully connected layer (`Dense`) receives the output
    from the LSTM cell and computes the logits for each element of the output sequences.
    As a result, the final output of the model will be a rank-3 tensor as well.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，该模型中的LSTM层具有输出形状`(None, None, 512)`，这意味着LSTM的输出是3维的。第一个维度表示批次数，第二个维度表示输出序列的长度，最后一个维度对应隐藏单元的数量。LSTM层输出为3维的原因是我们在定义LSTM层时指定了`return_sequences=True`。全连接层（`Dense`）接收来自LSTM单元的输出，并计算输出序列中每个元素的logits。因此，模型的最终输出也将是一个3维的张量。
- en: 'Furthermore, we specified `activation=None` for the final fully connected layer.
    The reason for this is that we will need to have the logits as outputs of the
    model so that we can sample from the model predictions in order to generate new
    text. We will get to this sampling part later. For now, let''s train the model:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们在最后的全连接层中指定了`activation=None`。这样做的原因是，我们需要将logits作为模型的输出，以便从模型预测中采样生成新文本。采样部分稍后会介绍。现在，让我们开始训练模型：
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Now, we can evaluate the model to generate new text, starting with a given short
    string. In the next section, we will define a function to evaluate the trained
    model.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以评估模型以生成新文本，从给定的短字符串开始。在下一节中，我们将定义一个函数来评估训练好的模型。
- en: Evaluation phase – generating new text passages
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估阶段 – 生成新文本段落
- en: The RNN model we trained in the previous section returns the logits of size
    80 for each unique character. These logits can be readily converted to probabilities,
    via the softmax function, that a particular character will be encountered as the
    next character. To predict the next character in the sequence, we can simply select
    the element with the maximum logit value, which is equivalent to selecting the
    character with the highest probability. However, instead of always selecting the
    character with the highest likelihood, we want to (randomly) *sample* from the
    outputs; otherwise, the model will always produce the same text. TensorFlow already
    provides a function, `tf.random.categorical()`, which we can use to draw random
    samples from a categorical distribution. To see how this works, let's generate
    some random samples from three categories [0, 1, 2], with input logits [1, 1,
    1].
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中训练的RNN模型返回每个独特字符的logits，大小为80。这些logits可以通过softmax函数轻松转换为概率，即一个特定字符作为下一个字符出现的概率。为了预测序列中的下一个字符，我们可以简单地选择具有最大logit值的元素，这等同于选择具有最高概率的字符。然而，我们并不总是选择具有最高概率的字符，而是希望从输出中（随机）*采样*；否则，模型将始终生成相同的文本。TensorFlow已经提供了一个函数`tf.random.categorical()`，我们可以使用它从分类分布中抽取随机样本。为了理解这一过程，让我们从三个类别[0,
    1, 2]中生成一些随机样本，输入logits为[1, 1, 1]。
- en: '[PRE26]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'As you can see, with the given logits, the categories have the same probabilities
    (that is, equiprobable categories). Therefore, if we use a large sample size (![](img/B13208_16_081.png)),
    we would expect the number of occurrences of each category to reach ![](img/B13208_16_082.png)
    of the sample size. If we change the logits to [1, 1, 3], then we would expect
    to observe more occurrences for category 2 (when a very large number of examples
    are drawn from this distribution):'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，给定的logits下，各类别的概率是相同的（即，各类别具有相同概率）。因此，如果我们使用一个大样本量（![](img/B13208_16_081.png)），我们预计每个类别的出现次数将达到样本量的
    ![](img/B13208_16_082.png)。如果我们将logits更改为[1, 1, 3]，那么我们预计类别2会出现更多的次数（当从该分布中抽取大量样本时）：
- en: '[PRE27]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Using `tf.random.categorical`, we can generate examples based on the logits
    computed by our model. We define a function, `sample()`, that receives a short
    starting string, `starting_str`, and generate a new string, `generated_str`, which
    is initially set to the input string. Then, a string of size `max_input_length`
    is taken from the end of `generated_str` and encoded to a sequence of integers,
    `encoded_input`. The `encoded_input` is passed to the RNN model to compute the
    logits. Note that the output from the RNN model is a sequence of logits with the
    same length as the input sequence, since we specified `return_sequences=True`
    for the last recurrent layer of our RNN model. Therefore, each element in the
    output of the RNN model represents the logits (here, a vector of size 80, which
    is the total number of characters) for the next character after observing the
    input sequence by the model.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `tf.random.categorical` 函数，我们可以基于模型计算的 logits 生成示例。我们定义一个名为 `sample()` 的函数，接收一个短起始字符串
    `starting_str`，并生成一个新字符串 `generated_str`，其初始设置为输入字符串。然后，从 `generated_str` 的末尾取大小为
    `max_input_length` 的字符串，并将其编码为整数序列 `encoded_input`。`encoded_input` 被传递给 RNN 模型以计算
    logits。需要注意的是，由于我们为 RNN 模型的最后一个循环层指定了 `return_sequences=True`，因此从 RNN 模型的输出是与输入序列相同长度的
    logits 序列。因此，RNN 模型输出的每个元素表示模型在观察输入序列后下一个字符的 logits（这里是一个大小为 80 的向量，即字符的总数）。
- en: Here, we only use the last element of the output `logits` (that is, ![](img/B13208_16_083.png)),
    which is passed to the `tf.random.categorical()` function to generate a new sample.
    This new sample is converted to a character, which is then appended to the end
    of the generated string, `generated_text`, increasing its length by 1\. Then,
    this process is repeated, taking the last `max_input_length` number of characters
    from the end of the `generated_str`, and using that to generate a new character
    until the length of the generated string reaches the desired value. The process
    of consuming the generated sequence as input for generating new elements is called
    *auto-regression*.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们仅使用输出 `logits` 的最后一个元素（即 ![](img/B13208_16_083.png)），该元素被传递给 `tf.random.categorical()`
    函数以生成一个新的样本。这个新样本被转换为一个字符，然后附加到生成的字符串 `generated_text` 的末尾，使其长度增加 1。然后，此过程重复进行，从
    `generated_str` 的末尾取最后 `max_input_length` 个字符，并使用它来生成一个新字符，直到生成字符串的长度达到所需值。将生成序列作为生成新元素的输入的过程称为
    *自回归*。
- en: '**Returning sequences as output**'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回序列作为输出**'
- en: You may wonder why we use `return_sequences=True` when we only use the last
    character to sample a new character and ignore the rest of the output. While this
    question makes perfect sense, you should not forget that we used the entire output
    sequence for training. The loss is computed based on each prediction in the output
    and not just the last one.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道为什么在仅使用最后一个字符来生成新字符并忽略输出的其余部分时我们使用 `return_sequences=True`。虽然这个问题非常合理，但不要忘记我们使用整个输出序列进行训练。损失是基于输出的每个预测而不仅仅是最后一个来计算的。
- en: 'The code for the `sample()` function is as follows:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '`sample()` 函数的代码如下所示：'
- en: '[PRE28]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Let''s now generate some new text:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们生成一些新文本：
- en: '[PRE29]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: As you can see, the model generates mostly correct words, and, in some cases,
    the sentences are partially meaningful. You can further tune the training parameters,
    such as the length of input sequences for training, the model architecture, and
    sampling parameters (such as `max_input_length`).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，模型生成的大部分单词是正确的，并且在某些情况下，句子部分有意义。您可以进一步调整训练参数，例如用于训练的输入序列的长度，模型架构以及抽样参数（例如
    `max_input_length`）。
- en: 'Furthermore, in order to control the predictability of the generated samples
    (that is, generating text following the learned patterns from the training text
    versus adding more randomness), the logits computed by the RNN model can be scaled
    before being passed to `tf.random.categorical()` for sampling. The scaling factor,
    ![](img/B13208_16_084.png), can be interpreted as the inverse of the temperature
    in physics. Higher temperatures result in more randomness versus more predictable
    behavior at lower temperatures. By scaling the logits with ![](img/B13208_16_085.png),
    the probabilities computed by the softmax function become more uniform, as shown
    in the following code:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，为了控制生成样本的可预测性（即生成遵循训练文本中学习到的模式的文本，还是增加更多的随机性），RNN 模型计算出的 logits 可以在传递给 `tf.random.categorical()`
    进行采样之前进行缩放。缩放因子，![](img/B13208_16_084.png)，可以解释为物理学中的温度的倒数。较高的温度会导致更多的随机性，而较低的温度则会产生更可预测的行为。通过使用
    ![](img/B13208_16_085.png) 缩放 logits，softmax 函数计算出的概率变得更加均匀，如以下代码所示：
- en: '[PRE30]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'As you can see, scaling the logits by ![](img/B13208_16_086.png) results in
    near-uniform probabilities [0.31, 0.31, 0.38]. Now, we can compare the generated
    text with ![](img/B13208_16_087.png) and ![](img/B13208_16_088.png), as shown
    in the following points:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过 ![](img/B13208_16_086.png) 缩放 logits 会产生接近均匀的概率 [0.31, 0.31, 0.38]。现在，我们可以将生成的文本与
    ![](img/B13208_16_087.png) 和 ![](img/B13208_16_088.png) 进行比较，如下所示：
- en: '![](img/B13208_16_089.png)'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_16_089.png)'
- en: '[PRE31]'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](img/B13208_16_090.png)'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_IMG
  zh: '![](img/B13208_16_090.png)'
- en: '[PRE32]'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The results show that scaling the logits with ![](img/B13208_16_091.png) (increasing
    the temperature) generates more random text. There is a tradeoff between the novelty
    of the generated text and its correctness.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，通过 ![](img/B13208_16_091.png) 缩放 logits（提高温度）会生成更多随机的文本。在生成文本的新颖性和正确性之间存在权衡。
- en: In this section, we worked with character-level text generation, which is a
    sequence-to-sequence (seq2seq) modeling task. While this example may not be very
    useful by itself, it is easy to think of several useful applications for these
    types of models; for example, a similar RNN model can be trained as a chatbot
    to assist users with simple queries.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们使用了字符级文本生成，这是一个序列到序列（seq2seq）建模任务。虽然这个例子本身可能并不特别有用，但很容易想到几种这种类型模型的实际应用；例如，可以训练一个类似的
    RNN 模型作为聊天机器人，以帮助用户解答简单问题。
- en: Understanding language with the Transformer model
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解使用 Transformer 模型的语言
- en: In this chapter, we solved two sequence modeling problems using RNN-based NNs.
    However, a new architecture has recently emerged that has been shown to outperform
    the RNN-based seq2seq models in several NLP tasks.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们使用基于 RNN 的神经网络解决了两个序列建模问题。然而，最近出现了一种新的架构，它在多个自然语言处理任务中已经被证明优于基于 RNN 的
    seq2seq 模型。
- en: It is called the**Transformer** architecture, capable of modeling global dependencies
    between input and output sequences, and was introduced in 2017 by Ashish Vaswani,
    et. al., in the NeurIPS paper *Attention Is All You Need* (available online at
    [http://papers.nips.cc/paper/7181-attention-is-all-you-need](http://papers.nips.cc/paper/7181-attention-is-all-you-need)).
    The Transformer architecture is based on a concept called **attention**, and more
    specifically, the **self-attention mechanism**. Let's consider the sentiment analysis
    task that we covered earlier in this chapter. In this case, using the attention
    mechanism would mean that our model would be able to learn to focus on the parts
    of an input sequence that are more relevant to the sentiment.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这种架构被称为 **Transformer**，它能够建模输入和输出序列之间的全局依赖关系，并且在2017年由 Ashish Vaswani 等人在 NeurIPS
    论文 *Attention Is All You Need* 中提出（该论文可以在线查阅：[http://papers.nips.cc/paper/7181-attention-is-all-you-need](http://papers.nips.cc/paper/7181-attention-is-all-you-need)）。Transformer
    架构基于一种名为 **注意力** 的概念，具体来说，是 **自注意力机制**。我们来回顾一下本章早些时候讨论的情感分析任务。在这种情况下，使用注意力机制意味着我们的模型能够学会集中注意输入序列中与情感相关性更强的部分。
- en: Understanding the self-attention mechanism
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解自注意力机制
- en: This section will explain the *self-attention mechanism* and how it helps a
    Transformer model to focus on important parts of a sequence for NLP. The first
    subsection will cover a very basic form of self-attention to illustrate the overall
    idea behind learning text representations. Then, we will add different weight
    parameters so that we arrive at the self-attention mechanism that is commonly
    used in Transformer models.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将解释*自注意力机制*以及它如何帮助 Transformer 模型在自然语言处理（NLP）中聚焦于序列中的重要部分。第一小节将介绍一种非常基础的自注意力形式，以阐明学习文本表示的整体思路。接下来，我们将加入不同的权重参数，从而得到
    Transformer 模型中常用的自注意力机制。
- en: A basic version of self-attention
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自注意力的基础版本
- en: To introduce the basic idea behind self-attention, let's assume we have an input
    sequence of length *T*, ![](img/B13208_16_092.png), as well as an output sequence,
    ![](img/B13208_16_093.png). Each element of these sequences, ![](img/B13208_16_094.png)
    and ![](img/B13208_16_095.png), are vectors of size *d* (that is, ![](img/B13208_16_096.png)).
    Then, for a seq2seq task, the goal of self-attention is to model the dependencies
    of each element in the output sequence to the input elements. In order to achieve
    this, attention mechanisms are composed of three stages. Firstly, we derive importance
    weights based on the similarity between the current element and all other elements
    in the sequence. Secondly, we normalize the weights, which usually involves the
    use of the already familiar softmax function. Thirdly, we use these weights in
    combination with the corresponding sequence elements in order to compute the attention
    value.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了介绍自注意力背后的基本概念，假设我们有一个长度为 *T* 的输入序列，![](img/B13208_16_092.png)，以及一个输出序列，![](img/B13208_16_093.png)。这些序列的每个元素，![](img/B13208_16_094.png)
    和 ![](img/B13208_16_095.png)，是大小为 *d* 的向量（即，![](img/B13208_16_096.png)）。然后，对于
    seq2seq 任务，自注意力的目标是对输出序列中每个元素与输入元素之间的依赖关系建模。为了实现这一点，注意力机制由三个阶段组成。首先，我们基于当前元素与序列中所有其他元素之间的相似性来推导重要性权重。其次，我们对这些权重进行归一化，这通常涉及使用已熟悉的
    softmax 函数。第三，我们将这些权重与相应的序列元素结合使用，以计算注意力值。
- en: 'More formally, the output of self-attention is the weighted sum of all input
    sequences. For instance, for the *i*th input element, the corresponding output
    value is computed as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 更正式地，自注意力的输出是所有输入序列的加权和。例如，对于*第 i* 个输入元素，计算其对应的输出值如下：
- en: '![](img/B13208_16_097.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_097.png)'
- en: 'Here, the weights, ![](img/B13208_16_098.png), are computed based on the similarity
    between the current input element, ![](img/B13208_16_099.png), and all other elements
    in the input sequence. More concretely, this similarity is computed as the dot
    product between the current input element, ![](img/B13208_16_0991.png), and another
    element in the input sequence, ![](img/B13208_16_101.png):'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，权重，![](img/B13208_16_098.png)，是基于当前输入元素，![](img/B13208_16_099.png)，与输入序列中所有其他元素之间的相似性来计算的。更具体地，这种相似性是通过当前输入元素，![](img/B13208_16_0991.png)，与输入序列中的另一个元素，![](img/B13208_16_101.png)，的点积来计算的：
- en: '![](img/B13208_16_102.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_102.png)'
- en: 'After computing these similarity-based weights for the *i*th input and all
    inputs in the sequence (![](img/B13208_16_103.png) to ![](img/B13208_16_104.png)),
    the "raw" weights (![](img/B13208_16_105.png) to ![](img/B13208_16_106.png)) are
    then normalized using the familiar softmax function, as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在为*第 i* 个输入以及序列中的所有输入计算这些基于相似性的权重之后（![](img/B13208_16_103.png) 到 ![](img/B13208_16_104.png)），这些“原始”权重（![](img/B13208_16_105.png)
    到 ![](img/B13208_16_106.png)）将使用熟悉的 softmax 函数进行归一化，如下所示：
- en: '![](img/B13208_16_107.png)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_107.png)'
- en: Notice that as a consequence of applying the softmax function, the weights will
    sum to 1 after this normalization, that is,
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于应用了 softmax 函数，权重在此归一化后将总和为 1，即，
- en: '![](img/B13208_16_108.png)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_108.png)'
- en: 'To recap, let''s summarize the three main steps behind the self-attention operation:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，让我们概括自注意力操作背后的三个主要步骤：
- en: For a given input element, ![](img/B13208_16_109.png), and each *j*th element
    in the range [0, *T*], compute the dot product, ![](img/B13208_16_110.png)
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于给定的输入元素，![](img/B13208_16_109.png)，以及区间 [0，*T*] 中的每个*第 j* 个元素，计算点积，![](img/B13208_16_110.png)
- en: Obtain the weight, ![](img/B13208_16_111.png), by normalizing the dot products
    using the softmax function
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 softmax 函数归一化点积来获得权重，![](img/B13208_16_111.png)
- en: 'Compute the output, ![](img/B13208_16_112.png), as the weighted sum over the
    entire input sequence: ![](img/B13208_16_113.png)'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出，![](img/B13208_16_112.png)，作为对整个输入序列的加权和：![](img/B13208_16_113.png)
- en: 'These steps are further illustrated in the following figure:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图进一步说明这些步骤：
- en: '![](img/B13208_16_15.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_15.png)'
- en: Parameterizing the self-attention mechanism with query, key, and value weights
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 用查询、键和值权重参数化自注意力机制
- en: 'Now that you have been introduced to the basic concept behind self-attention,
    this subsection summarizes the more advanced self-attention mechanism that is
    used in the Transformer model. Note that in the previous subsection, we didn''t
    involve any learnable parameters when computing the outputs. Hence, if we want
    to learn a language model and want to change the attention values to optimize
    an objective, such as minimizing the classification error, we will need to change
    the word embeddings (that is, input vectors) that underlie each input element,
    ![](img/B13208_16_114.png). In other words, using the previously introduced basic
    self-attention mechanism, the Transformer model is rather limited with regard
    to how it can update or change the attention values during model optimization
    for a given sequence. To make the self-attention mechanism more flexible and amenable
    to model optimization, we will introduce three additional weight matrices that
    can be fit as model parameters during model training. We denote these three weight
    matrices as ![](img/B13208_16_115.png), ![](img/B13208_16_116.png), and ![](img/B13208_16_117.png).
    They are used to project the inputs into *query*, *key*, and *value* sequence
    elements:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经了解了自注意力机制的基本概念，本小节总结了在 Transformer 模型中使用的更高级的自注意力机制。请注意，在上一小节中，我们在计算输出时没有涉及任何可学习的参数。因此，如果我们想要训练一个语言模型，并希望通过改变注意力值来优化目标，例如最小化分类错误，我们就需要更改每个输入元素的词嵌入（即输入向量）！[](img/B13208_16_114.png)。换句话说，使用前面介绍的基本自注意力机制，Transformer
    模型在优化给定序列时，更新或更改注意力值的能力相当有限。为了使自注意力机制更加灵活，并便于模型优化，我们将引入三个额外的权重矩阵，这些矩阵可以在模型训练期间作为模型参数进行拟合。我们将这三个权重矩阵表示为！[](img/B13208_16_115.png)，！[](img/B13208_16_116.png)，和！[](img/B13208_16_117.png)。它们用于将输入投影到*查询*、*键*和*值*序列元素：
- en: 'Query sequence: ![](img/B13208_16_118.png) for ![](img/B13208_16_119.png),'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询序列：![](img/B13208_16_118.png) 对应 ![](img/B13208_16_119.png)，
- en: 'Key sequence: ![](img/B13208_16_120.png) for ![](img/B13208_16_121.png),'
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键序列：![](img/B13208_16_120.png) 对应 ![](img/B13208_16_121.png)，
- en: 'Value sequence: ![](img/B13208_16_122.png) for ![](img/B13208_16_123.png)'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 值序列：![](img/B13208_16_122.png) 对应 ![](img/B13208_16_123.png)
- en: 'Here, both ![](img/B13208_16_124.png) and ![](img/B13208_16_125.png) are vectors
    of size ![](img/B13208_16_126.png). Therefore, the projection matrices ![](img/B13208_16_127.png)
    and ![](img/B13208_16_128.png) have the shape ![](img/B13208_16_129.png), while
    ![](img/B13208_16_130.png) has the shape ![](img/B13208_16_131.png). For simplicity,
    we can design these vectors to have the same shape, for example, using ![](img/B13208_16_132.png).
    Now, instead of computing the unnormalized weight as the pairwise dot product
    between the given input sequence element, ![](img/B13208_16_133.png), and the
    *j*th sequence element, ![](img/B13208_16_134.png), we can compute the dot product
    between the query and key:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，！[](img/B13208_16_124.png) 和！[](img/B13208_16_125.png)都是大小为！[](img/B13208_16_126.png)的向量。因此，投影矩阵！[](img/B13208_16_127.png)
    和！[](img/B13208_16_128.png)的形状为！[](img/B13208_16_129.png)，而！[](img/B13208_16_130.png)的形状为！[](img/B13208_16_131.png)。为了简化，我们可以设计这些向量具有相同的形状，例如使用！[](img/B13208_16_132.png)。现在，我们可以计算查询和键之间的点积，而不是计算给定输入序列元素！[](img/B13208_16_133.png)与*第j*个序列元素！[](img/B13208_16_134.png)之间的未归一化权重的成对点积：
- en: '![](img/B13208_16_135.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_135.png)'
- en: 'We can then further use *m*, or, more precisely, ![](img/B13208_16_136.png),
    to scale ![](img/B13208_16_137.png) before normalizing it via the softmax function,
    as follows:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以进一步使用 *m*，或更准确地说，！[](img/B13208_16_136.png)，来缩放 ![](img/B13208_16_137.png)，然后通过
    softmax 函数对其进行归一化，如下所示：
- en: '![](img/B13208_16_138.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_138.png)'
- en: Note that scaling ![](img/B13208_16_139.png) by ![](img/B13208_16_140.png) will
    ensure that the Euclidean length of the weight vectors will be approximately in
    the same range.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，通过 ![](img/B13208_16_140.png) 缩放 ![](img/B13208_16_139.png)，可以确保权重向量的欧几里得长度大致处于相同范围内。
- en: Multi-head attention and the Transformer block
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多头注意力和 Transformer 块
- en: 'Another trick that greatly improves the discriminatory power of the self-attention
    mechanism is **multi-head attention** (**MHA**), which combines multiple self-attention
    operations together. In this case, each self-attention mechanism is called a *head*,
    which can be computed in parallel. Using *r* parallel heads, each head results
    in a vector, *h*, of size *m*. These vectors are then concatenated to obtain a
    vector, *z*, with the shape ![](img/B13208_16_141.png). Finally, the concatenated
    vector is projected using the output matrix ![](img/B13208_16_142.png) to obtain
    the final output, as follows:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个大大提升自注意力机制判别能力的技巧是**多头注意力**（**MHA**），它将多个自注意力操作组合在一起。在这种情况下，每个自注意力机制被称为*头*，可以并行计算。使用*r*个并行头，每个头都会产生一个大小为*m*的向量*h*。这些向量随后被串联起来，得到一个形状为![](img/B13208_16_141.png)的向量*z*。最后，使用输出矩阵![](img/B13208_16_142.png)对串联向量进行投影，得到最终输出，如下所示：
- en: '![](img/B13208_16_143.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_143.png)'
- en: 'The architecture of a Transformer block is shown in the following figure:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer块的架构如下图所示：
- en: '![](img/B13208_16_16.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_16_16.png)'
- en: Notice that in the Transformer architecture shown in the previous figure, we
    added two additional components that we haven't discussed yet. One of these components
    is the *residual connection*, which adds the output from a layer (or even a group
    of layers) to its input, that is, *x* + *layer*(*x*). The block consisting of
    a layer (or multiple layers) with such a residual connection is called a *residual
    block*. The Transformer block shown in the previous figure has two residual blocks.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面图示的Transformer架构中，我们添加了两个尚未讨论的额外组件。其中一个组件是*残差连接*，它将层（或多个层）的输出添加到其输入中，也就是说，*x*
    + *layer*(*x*)。由一个层（或多个层）与这种残差连接组成的模块称为*残差块*。前面图示中的Transformer模块包含两个残差块。
- en: The other new component is *layer normalization*, which is denoted in the previous
    figure as "Layer norm." There is a family of normalization layers including batch
    normalization, which we will cover in *Chapter 17*, *Generative Adversarial Networks
    for Synthesizing New Data*. For now, you can think of layer normalization as a
    fancy or more advanced way of normalizing or scaling the NN inputs and activations
    in each layer.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个新的组件是*层归一化*，在前面图中表示为“Layer norm”。归一化层有一个家族，包括批量归一化，我们将在*第17章*《用于生成新数据的生成对抗网络》中介绍。现在，你可以将层归一化看作是规范化或缩放每层神经网络输入和激活的一种更花哨或更先进的方法。
- en: Returning to the illustration of the Transformer model in the previous figure,
    let's now discuss how this model works. First, the input sequence is passed to
    the MHA layers, which is based on the self-attention mechanism that we discussed
    earlier. In addition, the input sequences are added to the output of the MHA layers
    via the residual connections—this ensures that the earlier layers will receive
    sufficient gradient signals during training, which is a common trick that is used
    to improve training speed and convergence. If you are interested, you can read
    more about the concept behind residual connections in the research article *Deep
    Residual Learning for Image Recognition,* by *Kaiming He, Xiangyu Zhang, Shaoqing
    Ren, and Jian Sun*, which is freely available at [http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 回到前面图示的Transformer模型，现在我们来讨论这个模型是如何工作的。首先，输入序列传递到MHA层，该层基于我们之前讨论的自注意力机制。此外，输入序列通过残差连接添加到MHA层的输出中——这确保了在训练过程中早期的层将接收到足够的梯度信号，这是提高训练速度和收敛性的一种常见技巧。如果你感兴趣，你可以阅读*Deep
    Residual Learning for Image Recognition*（深度残差学习用于图像识别）一文，作者为*何凯明、张祥宇、任少卿和孙剑*，该文章可以在[http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html](http://openaccess.thecvf.com/content_cvpr_2016/html/He_Deep_Residual_Learning_CVPR_2016_paper.html)免费获取。
- en: After the input sequences are added to the output of the MHA layers, the outputs
    are normalized via layer normalization. These normalized signals then go through
    a series of MLP (that is, fully connected) layers, which also have a residual
    connection. Finally, the output from the residual block is normalized again and
    returned as the output sequence, which can be used for sequence classification
    or sequence generation.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 在将输入序列添加到MHA层的输出后，通过层归一化对输出进行归一化。然后，这些归一化信号通过一系列MLP（即全连接）层，这些层还具有残差连接。最后，残差块的输出再次进行归一化，并作为输出序列返回，可用于序列分类或序列生成。
- en: Instructions for implementing and training Transformer models were omitted to
    conserve space. However, the interested reader can find an excellent implementation
    and walk-through in the official TensorFlow documentation at
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 为了节省空间，省略了Transformer模型的实现和训练说明。然而，有兴趣的读者可以在官方TensorFlow文档中找到出色的实现和详细说明，链接如下：
- en: '[https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer).'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://www.tensorflow.org/tutorials/text/transformer](https://www.tensorflow.org/tutorials/text/transformer)。'
- en: Summary
  id: totrans-304
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, you first learned about the properties of sequences that make
    them different to other types of data, such as structured data or images. We then
    covered the foundations of RNNs for sequence modeling. You learned how a basic
    RNN model works and discussed its limitations with regard to capturing long-term
    dependencies in sequence data. Next, we covered LSTM cells, which consist of a
    gating mechanism to reduce the effect of exploding and vanishing gradient problems,
    which are common in basic RNN models.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您首先了解了使序列与其他类型的数据（如结构化数据或图像）不同的特性。然后，我们介绍了用于序列建模的RNN基础知识。您了解了基本RNN模型的工作原理，并讨论了其在捕获序列数据中的长期依赖方面的局限性。接下来，我们介绍了LSTM单元，它包括门控机制以减少基本RNN模型中常见的梯度爆炸和消失问题的影响。
- en: After discussing the main concepts behind RNNs, we implemented several RNN models
    with different recurrent layers using the Keras API. In particular, we implemented
    an RNN model for sentiment analysis, as well as an RNN model for generating text.
    Finally, we covered the Transformer model, which leverages the self-attention
    mechanism in order to focus on the relevant parts of a sequence.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 讨论了RNN背后的主要概念后，我们使用Keras API实现了几个具有不同递归层的RNN模型。特别是，我们实现了一个用于情感分析的RNN模型，以及一个用于生成文本的RNN模型。最后，我们介绍了Transformer模型，它利用自注意力机制来集中关注序列中的相关部分。
- en: In the next chapter, you will learn about generative models and, in particular,
    **generative adversarial networks** (**GANs**), which have shown remarkable results
    in the computer vision community for various vision tasks.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，您将学习生成模型，特别是在计算机视觉社区中展示了显著结果的**生成对抗网络**（**GANs**）的相关视觉任务。
