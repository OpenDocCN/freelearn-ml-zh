- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Recognizing Faces with Support Vector Machine
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量机识别人脸
- en: In the previous chapter, we discovered underlying topics using clustering and
    topic modeling techniques. This chapter continues our journey of supervised learning
    and classification, with a particular emphasis on **Support Vector Machine** (**SVM**)
    classifiers.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用聚类和主题建模技术发现了潜在的主题。本章将继续我们对监督学习和分类的探索，特别是强调**支持向量机**（**SVM**）分类器。
- en: SVM is one of the most popular algorithms when it comes to high-dimensional
    spaces. The goal of the algorithm is to find a decision boundary in order to separate
    data from different classes. We will discuss in detail how that works. Also, we
    will implement the algorithm with scikit-learn and apply it to solve various real-life
    problems, including our main project of face recognition. A dimensionality reduction
    technique called **principal component analysis**, which boosts the performance
    of the image classifier, will also be covered in this chapter, as will support
    vector regression.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维空间中，SVM是最受欢迎的算法之一。该算法的目标是找到一个决策边界，以便将不同类别的数据分开。我们将详细讨论它是如何工作的。同时，我们将使用scikit-learn实现该算法，并将其应用于解决各种现实生活中的问题，包括我们的主要项目——人脸识别。本章还将介绍一种称为**主成分分析**的降维技术，它可以提升图像分类器的性能，此外还会涉及支持向量回归。
- en: 'This chapter explores the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨以下主题：
- en: Finding the separating boundary with SVM
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVM寻找分隔边界
- en: Classifying face images with SVM
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SVM分类人脸图像
- en: Estimating with support vector regression
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用支持向量回归进行估计
- en: Finding the separating boundary with SVM
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SVM寻找分隔边界
- en: SVM is another great classifier, which is effective in cases with high-dimensional
    spaces or where the number of dimensions is greater than the number of samples.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是另一种优秀的分类器，尤其在高维空间或维度数量大于样本数量的情况下表现出色。
- en: In machine learning classification, SVM finds an optimal hyperplane that best
    segregates observations from different classes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习分类中，SVM找到一个最优的超平面，能够最好地将不同类别的观察数据分开。
- en: A **hyperplane** is a plane of *n - 1* dimensions that separates the *n*-dimensional
    feature space of the observations into two spaces. For example, the hyperplane
    in a two-dimensional feature space is a line, and in a three-dimensional feature
    space, the hyperplane is a surface. The optimal hyperplane is picked so that the
    distance from its nearest points in each space to itself is maximized, and these
    nearest points are the so-called **support vectors**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**超平面**是一个具有* n - 1 *维度的平面，它将观察的* n *维特征空间分割为两个空间。例如，在二维特征空间中的超平面是一个直线，在三维特征空间中，超平面是一个面。选择最佳超平面是为了最大化其在每个空间中到最近点的距离，这些最近点就是所谓的**支持向量**。'
- en: 'The following toy example demonstrates what support vectors and a separating
    hyperplane (along with the distance margin, which I will explain later) look like
    in a binary classification case:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 以下的玩具示例展示了在二分类问题中，支持向量和分隔超平面（以及稍后我会解释的距离边界）是什么样子的：
- en: '![A picture containing screenshot, diagram, line, plot  Description automatically
    generated](img/B21047_09_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, diagram, line, plot  Description automatically
    generated](img/B21047_09_01.png)'
- en: 'Figure 9.1: Example of support vectors and a hyperplane in binary classification'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1：二分类中支持向量和超平面的示例
- en: The ultimate goal of SVM is to find an optimal hyperplane, but the burning question
    is “How can we find this optimal hyperplane?” You will get the answer as we explore
    the following scenarios. It’s not as hard as you may think. The first thing we
    will look at is how to find a hyperplane.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）的最终目标是找到一个最优的超平面，但迫切的问题是“我们如何找到这个最优超平面？”在接下来的探索中，你会得到答案。这并不像你想象的那么困难。我们首先要看的，是如何找到一个超平面。
- en: Scenario 1 – identifying a separating hyperplane
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景 1 – 确定分隔超平面
- en: 'First, you need to understand what qualifies as a separating hyperplane. In
    the following example, hyperplane *C* is the only correct one, as it successfully
    segregates observations by their labels, while hyperplanes *A* and *B* fail:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要理解什么样的超平面才是分隔超平面。在以下示例中，超平面*C*是唯一正确的，它成功地按标签将观察结果分隔开，而超平面*A*和*B*都未能做到这一点：
- en: '![A picture containing diagram, line, screenshot  Description automatically
    generated](img/B21047_09_02.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing diagram, line, screenshot  Description automatically
    generated](img/B21047_09_02.png)'
- en: 'Figure 9.2: Example of qualified and unqualified hyperplanes'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2：合格与不合格超平面的示例
- en: This is an easy observation. Let’s express a separating hyperplane in a formal
    or mathematical way next.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个容易观察到的现象。接下来，让我们以正式或数学的方式表达一个分隔超平面。
- en: 'In a two-dimensional space, a line can be defined by a slope vector *w* (represented
    as a two-dimensional vector), and an intercept *b*. Similarly, in a space of *n*
    dimensions, a hyperplane can be defined by an *n*-dimensional vector *w* and an
    intercept *b*. Any data point *x* on the hyperplane satisfies *wx + b = 0*. A
    hyperplane is a separating hyperplane if the following conditions are satisfied:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，一条直线可以由一个斜率向量 *w*（表示为二维向量）和一个截距 *b* 来定义。类似地，在 *n* 维空间中，一个超平面可以由一个 *n*
    维向量 *w* 和截距 *b* 来定义。任何位于超平面上的数据点 *x* 都满足 *wx + b = 0*。如果满足以下条件，则该超平面是分隔超平面：
- en: For any data point *x* from one class, it satisfies *wx* + *b* > *0*
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于来自某一类别的任意数据点 *x*，它满足 *wx* + *b* > *0*
- en: For any data point *x* from another class, it satisfies *wx* + *b* < *0*
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于来自另一个类别的任意数据点 *x*，它满足 *wx* + *b* < *0*
- en: However, there can be countless possible solutions for *w* and *b*. You can
    move or rotate hyperplane *C* to a certain extent, and it will still remain a
    separating hyperplane. Next, you will learn how to identify the best hyperplane
    among various possible separating hyperplanes.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，*w* 和 *b* 可能有无数种解。你可以在一定范围内移动或旋转超平面 *C*，它仍然保持分隔超平面。接下来，你将学习如何从多个可能的分隔超平面中识别出最佳超平面。
- en: Scenario 2 – determining the optimal hyperplane
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景 2 - 确定最优超平面
- en: 'Look at the following example: hyperplane *C* is preferred, as it enables the
    maximum sum of the distance between the nearest data point on the positive side
    and itself, and the distance between the nearest data point on the negative side
    and itself:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 看下面的例子：超平面 *C* 更受青睐，因为它能够最大化正侧最近数据点与自身之间的距离和负侧最近数据点与自身之间的距离之和：
- en: '![A picture containing screenshot, line, diagram  Description automatically
    generated](img/B21047_09_03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![截图，线条，示意图，自动生成的描述可信度低](img/B21047_09_03.png)'
- en: 'Figure 9.3: An example of optimal and suboptimal hyperplanes'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：最优和次优超平面的示例
- en: The nearest point(s) on the positive side can constitute a hyperplane parallel
    to the decision hyperplane, which we call a **positive hyperplane**; conversely,
    the nearest point(s) on the negative side can constitute the **negative hyperplane**.
    The perpendicular distance between the positive and negative hyperplanes is called
    the **margin**, the value of which equates to the sum of the two aforementioned
    distances. A **decision** hyperplane is deemed **optimal** if the margin is maximized.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正侧最近的点可以构成一个与决策超平面平行的超平面，我们称之为**正超平面**；相反，负侧最近的点可以构成**负超平面**。正负超平面之间的垂直距离被称为**间隔**，其值等于前述两者的距离之和。如果间隔最大化，则该**决策**超平面被认为是**最优**的。
- en: 'The optimal (also called **maximum-margin**) hyperplane and the distance margins
    for a trained SVM model are illustrated in the following diagram. Again, samples
    on the margin (two from one class and one from another class, as shown) are the
    so-called **support vectors**:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后的 SVM 模型中的最优（也叫**最大间隔**）超平面和距离间隔如以下图所示。再次强调，位于间隔上的样本（每一类别各两个样本，另一个类别一个样本，如图所示）即为所谓的**支持向量**：
- en: '![A diagram of a positive hyperplane  Description automatically generated with
    low confidence](img/B21047_09_04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![正超平面示意图，自动生成的描述可信度低](img/B21047_09_04.png)'
- en: 'Figure 9.4: An example of an optimal hyperplane and distance margins'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.4：最优超平面和距离间隔的示例
- en: 'We can interpret it mathematically by first describing the positive and negative
    hyperplanes, as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过首先描述正负超平面来从数学上进行解释，具体如下：
- en: '![](img/B21047_09_001.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_001.png)'
- en: '![](img/B21047_09_002.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_002.png)'
- en: Here, ![](img/B21047_09_003.png) is a data point on the positive hyperplane,
    and ![](img/B21047_09_004.png) is a data point on the negative hyperplane.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B21047_09_003.png) 是正超平面上的一个数据点，![](img/B21047_09_004.png) 是负超平面上的一个数据点。
- en: 'The distance between a point ![](img/B21047_09_005.png) and the decision hyperplane
    can be calculated as follows:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 点与决策超平面之间的距离可以通过以下方式计算：
- en: '![](img/B21047_09_006.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_006.png)'
- en: 'Similarly, the distance between a point ![](img/B21047_09_007.png) and the
    decision hyperplane is as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，点与决策超平面之间的距离如下：
- en: '![](img/B21047_09_008.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_008.png)'
- en: 'So the margin becomes ![](img/B21047_09_009.png). As a result, we need to minimize
    ![](img/B21047_09_010.png) in order to maximize the margin. Importantly, to comply
    with the fact that the support vectors on the positive and negative hyperplanes
    are the nearest data points to the decision hyperplane, we add a condition that
    no data point falls between the positive and negative hyperplanes:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 所以边距变成了 ![](img/B21047_09_009.png)。因此，我们需要最小化 ![](img/B21047_09_010.png)，以最大化边距。重要的是，为了符合正负超平面上的支持向量是离决策超平面最近的数据点这一事实，我们添加了一个条件：没有数据点位于正负超平面之间：
- en: '![](img/B21047_09_011.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_011.png)'
- en: '![](img/B21047_09_012.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_012.png)'
- en: 'Here, ![](img/B21047_09_013.png) is an observation. This can be combined further
    into the following:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/B21047_09_013.png) 是一个观察值。这可以进一步组合成如下形式：
- en: '![](img/B21047_09_014.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_014.png)'
- en: 'To summarize, *w* and *b*, which determine the SVM decision hyperplane, are
    trained and solved by the following optimization problem:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，决定 SVM 决策超平面的 *w* 和 *b*，是通过以下优化问题进行训练和求解的：
- en: Minimizing ![](img/B21047_09_015.png)
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化 ![](img/B21047_09_015.png)
- en: Subject to ![](img/B21047_09_016.png), for a training set of ![](img/B21047_09_017.png),
    ![](img/B21047_09_018.png),… ![](img/B21047_09_019.png)…, and ![](img/B21047_09_020.png)
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 约束条件为 ![](img/B21047_09_016.png)，对于训练集中的 ![](img/B21047_09_017.png)，![](img/B21047_09_018.png)，……![](img/B21047_09_019.png)……，以及
    ![](img/B21047_09_020.png)
- en: To solve this optimization problem, we need to resort to quadratic programming
    techniques, which are beyond the scope of our learning journey. Therefore, we
    will not cover the computation methods in detail and, instead, will implement
    the classifier using the `SVC` and `LinearSVC` modules from scikit-learn, which
    are respectively based on `libsvm` ([https://www.csie.ntu.edu.tw/~cjlin/libsvm/](https://www.csie.ntu.edu.tw/~cjlin/libsvm/))
    and `liblinear` ([https://www.csie.ntu.edu.tw/~cjlin/liblinear/](https://www.csie.ntu.edu.tw/~cjlin/liblinear/)),
    two popular open-source SVM machine learning libraries. However, it is always
    valuable to understand the concepts of computing SVM.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个优化问题，我们需要借助二次规划技术，这超出了我们学习旅程的范围。因此，我们不会详细讲解计算方法，而是使用 scikit-learn 中的 `SVC`
    和 `LinearSVC` 模块来实现分类器，这两个模块分别基于 `libsvm` ([https://www.csie.ntu.edu.tw/~cjlin/libsvm/](https://www.csie.ntu.edu.tw/~cjlin/libsvm/))
    和 `liblinear` ([https://www.csie.ntu.edu.tw/~cjlin/liblinear/](https://www.csie.ntu.edu.tw/~cjlin/liblinear/))，这两个是流行的开源
    SVM 机器学习库。不过，理解 SVM 的计算概念始终是很有价值的。
- en: '*Pegasos: Primal estimated sub-gradient solver for SVM* (*Mathematical Programming*,
    March 2011, volume 127, issue 1, pp. 3–30) by Shai Shalev-Shwartz et al. and *A
    dual coordinate descent method for large-scale linear SVM* (*Proceedings of the
    25th international conference on machine learning*, pp 408–415) by Cho-Jui Hsieh
    et al. are great learning materials. They cover two modern approaches, sub-gradient
    descent and coordinate descent.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '*Pegasos: SVM 的原始估计子梯度求解器*（《数学编程》，2011年3月，第127卷，第1期，页码3–30）由 Shai Shalev-Shwartz
    等人编写，*大规模线性 SVM 的对偶坐标下降法*（《第25届国际机器学习大会论文集》，页码408–415）由 Cho-Jui Hsieh 等人编写，是非常好的学习材料。它们涵盖了两种现代方法：子梯度下降法和坐标下降法。'
- en: 'The learned model parameters *w* and *b* are then used to classify a new sample
    *x*’, based on the following conditions:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，学习到的模型参数 *w* 和 *b* 会被用来根据以下条件对新样本 *x*’ 进行分类：
- en: '![](img/B21047_09_021.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_021.png)'
- en: 'Moreover, ![](img/B21047_09_022.png) can be portrayed as the distance from
    the data point *x*’ to the decision hyperplane, and it can also be interpreted
    as the confidence of prediction: the higher the value, the further away the data
    point is from the decision boundary, hence the higher prediction certainty.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，![](img/B21047_09_022.png) 可以表示为数据点 *x*’ 到决策超平面的距离，也可以解释为预测的置信度：值越大，数据点离决策边界越远，因此预测的确定性越高。
- en: 'Although you might be eager to implement the SVM algorithm, let’s take a step
    back and look at a common scenario where data points are not linearly separable
    in a strict way. Try to find a separating hyperplane in the following example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然你可能迫不及待想实现 SVM 算法，但我们还是先退后一步，看看一个常见场景，在这个场景中数据点并不是严格线性可分的。试着在以下示例中找到一个分割超平面：
- en: '![A picture containing screenshot, line, diagram, design  Description automatically
    generated](img/B21047_09_05.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing screenshot, line, diagram, design  Description automatically
    generated](img/B21047_09_05.png)'
- en: 'Figure 9.5: An example of data points that are not strictly linearly separable'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：一个数据点不能严格线性分割的示例
- en: How can we deal with cases where it is impossible to strictly linearly segregate
    a set of observations containing outliers? Let’s see in the next section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如何处理那些无法严格线性分隔包含离群值的观察值集的情况呢？让我们在下一节中探讨。
- en: Scenario 3 – handling outliers
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景 3 – 处理离群值
- en: 'To handle scenarios where we cannot linearly segregate a set of observations
    containing outliers, we can actually allow the misclassification of outliers and
    try to minimize the error introduced. The misclassification error ![](img/B21047_09_023.png)
    (also called **hinge loss**) for a sample ![](img/B21047_09_024.png) can be expressed
    as follows:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理无法线性分隔包含离群值的观察值集的场景，我们实际上可以允许离群值的误分类，并尝试最小化由此引入的误差。样本 ![](img/B21047_09_024.png)
    的误分类误差 ![](img/B21047_09_023.png)（也称为 **铰链损失**）可以表示如下：
- en: '![](img/B21047_09_025.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_025.png)'
- en: 'Together with the ultimate term ![](img/B21047_09_026.png) that we want to
    reduce, the final objective value we want to minimize becomes the following:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 连同我们希望减少的最终项 ![](img/B21047_09_026.png)，我们希望最小化的最终目标值变为如下：
- en: '![](img/B21047_09_027.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_027.png)'
- en: 'As regards a training set of *m* samples ![](img/B21047_09_028.png), ![](img/B21047_09_029.png),…
    ![](img/B21047_09_030.png)…, and ![](img/B21047_09_031.png), where the hyperparameter
    *C* controls the trade-off between the two terms, the following apply:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含 *m* 个样本的训练集 ![](img/B21047_09_028.png)，![](img/B21047_09_029.png)，… ![](img/B21047_09_030.png)，…
    和 ![](img/B21047_09_031.png)，其中超参数 *C* 控制两项之间的权衡，以下公式适用：
- en: If a large value of *C* is chosen, the penalty for misclassification becomes
    relatively high. This means the rule of thumb of data segregation becomes stricter
    and the model might be prone to overfitting, since few mistakes are allowed during
    training. An SVM model with a large *C* has a low bias, but it might suffer from
    high variance.
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果选择了较大的 *C* 值，误分类的惩罚会变得相对较高。这意味着数据分隔的经验法则变得更加严格，模型可能容易过拟合，因为在训练过程中允许的错误很少。具有大
    *C* 值的 SVM 模型具有低偏差，但可能会遭遇高方差。
- en: Conversely, if the value of *C* is sufficiently small, the influence of misclassification
    becomes fairly low. This model allows more misclassified data points than a model
    with a large *C*. Thus, data separation becomes less strict. Such a model has
    low variance, but it might be compromised by high bias.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相反，如果 *C* 的值足够小，误分类的影响会变得相对较低。这个模型允许比大 *C* 值模型更多的误分类数据点。因此，数据分隔变得不那么严格。这样的模型具有低方差，但可能会受到高偏差的影响。
- en: 'A comparison between a large and small *C* is shown in the following diagram:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了大与小的 *C* 值之间的对比：
- en: '![A screenshot of a screen  Description automatically generated with low confidence](img/B21047_09_06.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![屏幕截图，描述自动生成，信心较低](img/B21047_09_06.png)'
- en: 'Figure 9.6: How the value of C affects the strictness of segregation and the
    margin'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6：*C* 值如何影响分隔的严格性和边际
- en: The parameter *C* determines the balance between bias and variance. It can be
    fine-tuned with cross-validation, which we will practice shortly.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 *C* 决定了偏差与方差之间的平衡。它可以通过交叉验证进行微调，接下来我们将进行实践。
- en: Implementing SVM
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现 SVM
- en: We have largely covered the fundamentals of the SVM classifier. Now, let’s apply
    it right away to an easy binary classification dataset. We will use the classic
    breast cancer Wisconsin dataset ([https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html))
    from scikit-learn.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经大致讲解了 SVM 分类器的基础知识。现在，让我们立即将其应用于一个简单的二分类数据集。我们将使用 scikit-learn 中经典的乳腺癌威斯康星数据集（[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html)）。
- en: 'Let’s take a look at the following steps:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下步骤：
- en: 'We first load the dataset and do some basic analysis, as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载数据集并进行一些基础分析，如下所示：
- en: '[PRE0]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you can see, the dataset has 569 samples with 30 features; its label is binary,
    and 63% of samples are positive (benign). Again, always check whether classes
    are imbalanced before trying to solve any classification problem. In this case,
    they are relatively balanced.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，数据集包含 569 个样本，具有 30 个特征；其标签是二元的，其中 63% 的样本为正样本（良性）。再次提醒，在尝试解决任何分类问题之前，请始终检查类别是否不平衡。在这种情况下，它们是相对平衡的。
- en: 'Next, we split the data into training and testing sets:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分为训练集和测试集：
- en: '[PRE1]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For reproducibility, don’t forget to specify a random seed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保证可复现性，别忘了指定随机种子。
- en: 'We can now apply the SVM classifier to the data. We first initialize an `SVC`
    model with the `kernel` parameter set to `linear` (linear kernel refers to the
    use of a linear decision boundary to separate classes in the input space. I will
    explain what kernel means in *Scenario 5*) and the penalty hyperparameter `C`
    set to the default value, `1.0`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们现在可以将 SVM 分类器应用于数据。我们首先初始化一个 `SVC` 模型，并将 `kernel` 参数设置为 `linear`（线性核指的是使用线性决策边界来分隔输入空间中的类。我将在
    *场景 5* 中解释什么是核）和惩罚超参数 `C` 设置为默认值 `1.0`：
- en: '[PRE2]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We then fit our model on the training set, as follows:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们在训练集上拟合我们的模型，如下所示：
- en: '[PRE3]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Then, we predict on the testing set with the trained model and obtain the prediction
    accuracy directly:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们使用训练好的模型对测试集进行预测，直接获得预测准确率：
- en: '[PRE4]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Our first SVM model works just great, achieving an accuracy of `95.8%`. How
    about dealing with more than two topics? How does SVM handle multiclass classification?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一个 SVM 模型表现得非常好，达到了 `95.8%` 的准确率。那么，如何处理多个主题呢？SVM 如何处理多类分类？
- en: Scenario 4 – dealing with more than two classes
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景 4 – 处理多于两类的情况
- en: SVM and many other classifiers can be applied to cases with more than two classes.
    There are two typical approaches we can take, **one-vs-rest** (also called **one-vs-all**)
    and **one-vs-one**.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 和许多其他分类器可以应用于多类情况。我们可以采取两种典型方法，**一对多**（也称为 **一对全**）和 **一对一**。
- en: One-vs-rest
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一对多
- en: 'In the one-vs-rest setting, for a *K*-class problem, we construct *K* different
    binary SVM classifiers. For the *k*^(th) classifier, it treats the *k*^(th) class
    as the positive case and the remaining *K-1* classes as the negative case as a
    whole; the hyperplane denoted as (*w*[k]*, b*[k]) is trained to separate these
    two cases. To predict the class of a new sample, *x*’, it compares the resulting
    predictions ![](img/B21047_09_032.png) from *K* individual classifiers from *1*
    to *k*. As we discussed in the previous section, the larger value of ![](img/B21047_09_033.png)
    means higher confidence that *x*’ belongs to the positive case. Therefore, it
    assigns *x*’ to the class *i* where ![](img/B21047_09_034.png) has the largest
    value among all prediction results:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对多设置中，对于 *K* 类问题，我们构建 *K* 个不同的二元 SVM 分类器。对于第 *k* 个分类器，它将第 *k* 类作为正类，剩余的 *K-1*
    类作为负类整体；表示为 (*w*[k]*, b*[k]) 的超平面被训练用来分隔这两类。为了预测一个新样本 *x*’ 的类别，它比较从 *1* 到 *K*
    的各个分类器得出的预测结果 ![](img/B21047_09_032.png)。正如我们在前一部分讨论的，值较大的 ![](img/B21047_09_033.png)
    表示 *x*’ 属于正类的置信度更高。因此，它将 *x*’ 分配给预测结果中值最大的类别 *i*：
- en: '![](img/B21047_09_035.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_035.png)'
- en: 'The following diagram shows how the one-vs-rest strategy works in a three-class
    case:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了一对多策略如何在三类情况下工作：
- en: '![A picture containing diagram, screenshot, line, plot  Description automatically
    generated](img/B21047_09_07.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing diagram, screenshot, line, plot  Description automatically
    generated](img/B21047_09_07.png)'
- en: 'Figure 9.7: An example of three-class classification using the one-vs-rest
    strategy'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7：使用一对多策略的三类分类示例
- en: 'For instance, if we have the following (*r*, *b*, and *g* denote the red cross,
    blue dot, and green square classes, respectively):'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有以下内容（*r*、*b* 和 *g* 分别表示红十字、蓝点和绿方块类）：
- en: '*w*[r]*x’*+*b*[r] = 0.78'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[r]*x’*+*b*[r] = 0.78'
- en: '*w*[b]*x’*+*b*[b] = 0.35'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[b]*x’*+*b*[b] = 0.35'
- en: '*w*[g]*x’*+*b*[g] = -0.64'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[g]*x’*+*b*[g] = -0.64'
- en: we can say *x*’ belongs to the red cross class, since *0.78 > 0.35 > -0.64*.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以说 *x*’ 属于红十字类，因为 *0.78 > 0.35 > -0.64*。
- en: 'If we have the following:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有以下内容：
- en: '*w*[r]*x’*+*b*[r] = -0.78'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[r]*x’*+*b*[r] = -0.78'
- en: '*w*[b]*x’*+*b*[b] = -0.35'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[b]*x’*+*b*[b] = -0.35'
- en: '*w*[g]*x’*+*b*[g] = -0.64'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*w*[g]*x’*+*b*[g] = -0.64'
- en: then we can determine that *x*’ belongs to the blue dot class regardless of
    the sign, since -*0.35 > -0.64 > -0.78*.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们可以确定 *x*’ 属于蓝点类，无论符号如何，因为 -*0.35 > -0.64 > -0.78*。
- en: One-vs-one
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一对一
- en: In the one-vs-one strategy, we conduct a pairwise comparison by building a set
    of SVM classifiers that can distinguish data points from each pair of classes.
    This will result in ![](img/B21047_09_036.png) different classifiers.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对一策略中，我们通过构建一组 SVM 分类器进行成对比较，以区分每对类之间的数据点。这将产生 ![](img/B21047_09_036.png)
    个不同的分类器。
- en: For a classifier associated with classes *i* and *j*, the hyperplane denoted
    as (*w*[ij],*b*[ij]) is trained only on the basis of observations from *i* (can
    be viewed as a positive case) and *j* (can be viewed as a negative case); it then
    assigns the class, either *i* or *j*, to a new sample, *x*’, based on the sign
    of *w*[ij]*x’*+*b*[ij]. Finally, the class with the highest number of assignments
    is considered the predicted result of *x*’. The winner is the class that gets
    the most votes.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于与类*i*和*j*相关联的分类器，超平面(*w*[ij],*b*[ij])仅在基于*i*（可以看作正例）和*j*（可以看作负例）的观测数据上进行训练；然后，它会根据*w*[ij]*x’*+*b*[ij]的符号，将类*i*或*j*分配给新样本*x*’。最后，拥有最多分配的类被视为*x*’的预测结果。获得最多投票的类为最终胜者。
- en: 'The following diagram shows how the one-vs-one strategy works in a three-class
    case:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了在三类问题中一对一策略的工作原理：
- en: '![A picture containing diagram, screenshot, line, text  Description automatically
    generated](img/B21047_09_08.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing diagram, screenshot, line, text  Description automatically
    generated](img/B21047_09_08.png)'
- en: 'Figure 9.8: An example of three-class classification using the one-vs-one strategy'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8：使用一对一策略的三类分类示例
- en: In general, an SVM classifier with a one-vs-rest setting and a classifier with
    a one-vs-one setting perform comparably in terms of accuracy. The choice between
    these two strategies is largely computational.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，使用一对多设置的SVM分类器与使用一对一设置的分类器在准确性上表现相似。这两种策略的选择主要取决于计算的需求。
- en: Although one-vs-one requires more classifiers, ![](img/B21047_09_037.png), than
    one-vs-rest (*K*), each pairwise classifier only needs to learn on a small subset
    of data, as opposed to the entire set in the one-vs-rest setting. As a result,
    training an SVM model in the one-vs-one setting is generally more memory-efficient
    and less computationally expensive; hence, it is preferable for practical use,
    as argued in Chih-Wei Hsu and Chih-Jen Lin’s *A comparison of methods for multiclass
    support vector machines* (*IEEE Transactions on Neural Networks*, March 2002,
    Volume 13, pp. 415–425).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管一对一方法需要更多的分类器，![](img/B21047_09_037.png)，与一对多方法(*K*)相比，每对分类器只需要在数据的小子集上进行学习，而不是在一对多设置下使用整个数据集。因此，在一对一设置下训练SVM模型通常更加节省内存和计算资源；因此，在实际应用中，正如Chih-Wei
    Hsu和Chih-Jen Lin在其论文《多类支持向量机方法比较》中所述，它更为优选（*IEEE Transactions on Neural Networks*,
    2002年3月，卷13，第415-425页）。
- en: Multiclass cases in scikit-learn
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: scikit-learn中的多类问题
- en: 'In scikit-learn, classifiers handle multiclass cases internally, and we do
    not need to explicitly write any additional code to enable this. You can see how
    simple it is in the wine classification example ([https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine))
    with three classes, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，分类器会自动处理多类问题，我们不需要显式编写额外的代码来启用此功能。你可以看到，在以下的葡萄酒分类示例中（[https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets.load_wine](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_wine.html#sklearn.datasets/load_wine)），如何简单地处理包含三类的数据：
- en: 'We first load the dataset and do some basic analysis, as follows:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先加载数据集并进行一些基本分析，如下所示：
- en: '[PRE5]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the dataset has 178 samples with 13 features; its label has
    three possible values taking up 33%, 40%, and 27%, respectively.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，该数据集包含178个样本和13个特征；其标签有三个可能的值，分别占33%、40%和27%。
- en: 'Next, we split the data into training and testing sets:'
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分为训练集和测试集：
- en: '[PRE6]'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can now apply the SVM classifier to the data. We first initialize an `SVC`
    model and fit it against the training set:'
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将SVM分类器应用于数据。我们首先初始化一个`SVC`模型并将其拟合到训练集上：
- en: '[PRE7]'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In an `SVC` model, multiclass support is implicitly handled according to the
    one-vs-one scheme.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在`SVC`模型中，多类支持是根据一对一方案隐式处理的。
- en: 'Next, we predict on the testing set with the trained model and obtain the prediction
    accuracy directly:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用训练好的模型对测试集进行预测，并直接获得预测准确率：
- en: '[PRE8]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our SVM model also works well in this multiclass case, achieving an accuracy
    of `97.8%`.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的SVM模型在这个多类问题中也表现良好，达到了`97.8%`的准确率。
- en: 'We also check how it performs for individual classes:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还检查了它在各个类上的表现：
- en: '[PRE9]'
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It looks excellent! Is the example too easy? Maybe. What do we do in tricky
    cases? Of course, we could tweak the values of the kernel and C hyperparameters.
    As discussed, factor C controls the strictness of separation, and it can be tuned
    to achieve the best trade-off between bias and variance. How about the kernel?
    What does it mean and what are the alternatives to a linear kernel?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来很棒！这个例子是不是太简单了？也许是。那在复杂情况下我们该怎么办呢？当然，我们可以调整核函数和C超参数的值。如前所述，C的作用是控制分隔的严格程度，可以通过调节它来实现偏差和方差之间的最佳平衡。那核函数呢？它意味着什么，线性核函数还有什么替代方案？
- en: In the next section, we will answer those two questions we just raised. You
    will see how the kernel trick makes SVM so powerful.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将解答这两个问题。你将看到核技巧如何让支持向量机（SVM）变得如此强大。
- en: Scenario 5 – solving linearly non-separable problems with kernels
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 场景 5 – 使用核函数解决线性不可分问题
- en: 'The hyperplanes we have found so far are linear, for instance, a line in a
    two-dimensional feature space, or a surface in a three-dimensional one. However,
    in the following example, we are not able to find a linear hyperplane that can
    separate two classes:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们找到的超平面都是线性的，例如，在二维特征空间中的一条直线，或者在三维空间中的一个平面。然而，在以下例子中，我们无法找到一个线性超平面来分隔两个类别：
- en: '![A picture containing line, screenshot, diagram  Description automatically
    generated](img/B21047_09_09.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含线条、截图、图表的图片  描述自动生成](img/B21047_09_09.png)'
- en: 'Figure 9.9: The linearly non-separable case'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9：线性不可分案例
- en: 'Intuitively, we observe that data points from one class are closer to the origin
    than those from another class. The distance to the origin provides distinguishable
    information. So we add a new feature, *z*=(*x*[1]²+*x*[2]²)², and transform the
    original two-dimensional space into a three-dimensional one. In the new space,
    as displayed in the following diagram, we can find a surface hyperplane separating
    the data (see the bottom left graph in *Figure 9.10*), or a line in the two-dimensional
    view (see the bottom right graph in *Figure 9.10*). With the additional feature,
    the dataset becomes linearly separable in the higher dimensional space, (*x*[1]*,x*[2]*,z*):'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们观察到来自一个类别的数据点比来自另一个类别的数据点离原点更近。到原点的距离提供了可区分的信息。所以我们添加了一个新特征，*z*=(*x*[1]²+*x*[2]²)²，并将原始的二维空间转换为三维空间。在新的空间中，如下图所示，我们可以找到一个超平面来分隔数据（见*图9.10*左下角的图），或者在二维视图中找到一条直线（见*图9.10*右下角的图）。通过添加这个新特征，数据集在更高维的空间中变得线性可分，(*x*[1]*,x*[2]*,z*)：
- en: '![A picture containing diagram, line, screenshot, plot  Description automatically
    generated](img/B21047_09_10.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含图表、线条、截图、绘图的图片  描述自动生成](img/B21047_09_10.png)'
- en: 'Figure 9.10: Making a non-separable case separable'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.10：将不可分案例变为可分
- en: Based upon similar logic, **SVMs with kernels** were invented to solve non-linear
    classification problems by converting the original feature space, ![](img/B21047_09_038.png),
    to a higher dimensional feature space with a transformation function, ![](img/B21047_09_039.png),
    such that the transformed dataset ![](img/B21047_09_040.png) is linearly separable.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 基于类似的逻辑，**带核函数的SVM**被发明出来，用于通过转换原始特征空间！[](img/B21047_09_038.png)，将其映射到一个更高维的特征空间，通过变换函数！[](img/B21047_09_039.png)，使得变换后的数据集！[](img/B21047_09_040.png)变得线性可分。
- en: A linear hyperplane ![](img/B21047_09_041.png) is then learned, using observations
    ![](img/B21047_09_042.png). For an unknown sample *x*’, it is first transformed
    into ![](img/B21047_09_043.png); the predicted class is determined by ![](img/B21047_09_044.png).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用观测值！[](img/B21047_09_042.png)，学习到了一个线性超平面！[](img/B21047_09_041.png)。对于一个未知样本*x*’，首先将其转换为！[](img/B21047_09_043.png)；然后通过！[](img/B21047_09_044.png)确定预测的类别。
- en: An SVM with kernels enables non-linear separation, but it does not explicitly
    map each original data point to the high-dimensional space and then perform expensive
    computation in the new space. Instead, it approaches this in a tricky way.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 带有核函数的SVM能够实现非线性分隔，但它并不会显式地将每个原始数据点映射到高维空间，然后在新空间中进行昂贵的计算。相反，它以一种巧妙的方式实现这一点。
- en: 'During the course of solving the SVM optimization problems, feature vectors
    *x*^((1)), *x*^((2)),…,*x*^((m)) are involved only in the form of a pairwise dot
    product *x*^((i)) *x*^((j)), although we will not expand this mathematically in
    this book. With kernels, the new feature vectors are ![](img/B21047_09_045.png),
    and their pairwise dot products can be expressed as ![](img/B21047_09_046.png).
    It would be computationally efficient to first implicitly conduct a pairwise operation
    on two low-dimensional vectors and later map the result to the high-dimensional
    space. In fact, a function *K* that satisfies this does exist:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在求解SVM优化问题的过程中，特征向量*x*^((1))、*x*^((2))、…、*x*^((m))仅以成对的点积形式出现，即*x*^((i)) *x*^((j))，尽管我们在本书中不会数学展开这个内容。使用核函数时，新的特征向量是！[](img/B21047_09_045.png)，它们的成对点积可以表示为！[](img/B21047_09_046.png)。从计算效率角度来看，先在两个低维向量上隐式进行成对操作，然后再将结果映射到高维空间，是一种有效的方法。事实上，存在一个满足此要求的函数*K*：
- en: '![](img/B21047_09_047.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_047.png)'
- en: The function *K* is the so-called **kernel function**. It is the mathematical
    formula that does the transformation.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 函数*K*就是所谓的**核函数**。它是进行转换的数学公式。
- en: There are different types of kernels, each suited for different kinds of data.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有不同类型的核，每种类型适用于不同的数据。
- en: With the kernel function, the transformation ![](img/B21047_09_048.png) becomes
    implicit, and the non-linear decision boundary can be efficiently learned by simply
    replacing the term ![](img/B21047_09_049.png) with ![](img/B21047_09_050.png).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 有了核函数，转换！[](img/B21047_09_048.png)变得隐式，非线性决策边界可以通过简单地将术语！[](img/B21047_09_049.png)替换为！[](img/B21047_09_050.png)来高效学习。
- en: The data is transformed into a higher-dimensional space. You don’t actually
    need to compute this space explicitly; the kernel function just works with the
    original data and performs the calculations necessary for the SVM.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 数据被转换为更高维的空间。你实际上不需要显式计算这个空间；核函数只需与原始数据一起工作，并进行支持向量机（SVM）所需的计算。
- en: 'The most popular kernel function is probably the **Radial Basis Function**
    (**RBF**) kernel (also called the **Gaussian** kernel), which is defined as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的核函数可能是**径向基函数**（**RBF**）核（也称为**高斯**核），定义如下：
- en: '![](img/B21047_09_051.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_051.png)'
- en: 'Here, ![](img/B21047_09_052.png) In the Gaussian function, the standard deviation
    ![](img/B21047_09_053.png) controls the amount of variation or dispersion allowed:
    the higher the ![](img/B21047_09_053.png) (or the lower the ![](img/B21047_09_055.png)),
    the larger the width of the bell, and the wider the range in which data points
    are allowed to spread out over. Therefore, ![](img/B21047_09_055.png) as the **kernel
    coefficient** determines how strictly or generally the kernel function fits the
    observations. A large ![](img/B21047_09_055.png) implies a small variance allowed
    and a relatively exact fit on the training samples, which might lead to overfitting.
    Conversely, a small ![](img/B21047_09_055.png) implies a high variance allowed
    and a loose fit on the training samples, which might cause underfitting.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在高斯函数中，标准差！[](img/B21047_09_053.png)控制着允许的变化量或分散量：标准差越大！[](img/B21047_09_053.png)（或标准差越小！[](img/B21047_09_055.png)），钟形曲线的宽度越大，数据点分布的范围也越广。因此，！[](img/B21047_09_055.png)作为**核系数**决定了核函数拟合观测值的严格程度或一般程度。较大的！[](img/B21047_09_055.png)意味着允许的小方差和相对准确的拟合训练样本，这可能导致过拟合。相反，较小的！[](img/B21047_09_055.png)意味着允许的方差较大，且训练样本的拟合较松散，这可能会导致欠拟合。
- en: 'To illustrate this trade-off, let’s apply the RBF kernel with different values
    to a toy dataset:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这种权衡，让我们对一个玩具数据集应用不同值的RBF核：
- en: '[PRE10]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Eight data points are from one class, and eight are from another. We take three
    values, `1`, `2`, and `4`, for kernel coefficient options as an example:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 八个数据点来自一类，另有八个来自另一类。我们以核系数的三个值`1`、`2`和`4`为例进行说明：
- en: '[PRE11]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Under each kernel coefficient, we fit an individual SVM classifier and visualize
    the trained decision boundary:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个核系数下，我们拟合一个单独的SVM分类器，并可视化训练后的决策边界：
- en: '[PRE12]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Refer to the following screenshot for the end results:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 请参考以下截图查看最终结果：
- en: '![](img/B21047_09_11.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_11.png)'
- en: 'Figure 9.11: The SVM classification decision boundary under different values
    of gamma'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11：在不同的gamma值下，SVM分类决策边界
- en: We can observe that a larger ![](img/B21047_09_055.png) results in narrow regions,
    which means a stricter fit on the dataset; a smaller ![](img/B21047_09_055.png)
    results in broad regions, which means a loose fit on the dataset. Of course, ![](img/B21047_09_055.png)
    can be fine-tuned through cross-validation to obtain the best performance.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以观察到，较大的 ![](img/B21047_09_055.png) 会导致较窄的区域，这意味着对数据集的拟合更严格；较小的 ![](img/B21047_09_055.png)
    会导致较宽的区域，这意味着对数据集的拟合较松散。当然，![](img/B21047_09_055.png) 可以通过交叉验证进行微调，以获得最佳性能。
- en: 'Some other common kernel functions include the **polynomial** kernel:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常见的核函数包括**多项式**核函数：
- en: '![](img/B21047_09_062.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_062.png)'
- en: 'and the **sigmoid** kernel:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 以及**Sigmoid**核函数：
- en: '![](img/B21047_09_063.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_063.png)'
- en: In the absence of prior knowledge of the distribution, the RBF kernel is usually
    preferable in practical usage, as there is an additional parameter to tweak in
    the polynomial kernel (polynomial degree *d*), and the empirical sigmoid kernel
    can perform approximately on par with the RBF, but only under certain parameters.
    Hence, we arrive at a debate between the linear (also considered no kernel) and
    the RBF kernel when given a dataset.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在缺乏分布先验知识的情况下，RBF 核函数通常更受欢迎，因为多项式核函数有一个额外的可调参数（多项式阶数 *d*），而经验性的 Sigmoid 核函数在某些参数下可以与
    RBF 核函数的表现相当。因此，我们会在给定数据集时讨论线性（也可以看作没有核函数）与 RBF 核函数之间的选择。
- en: Choosing between linear and RBF kernels
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在选择线性核函数和 RBF 核函数时
- en: Of course, linear separability is the rule of thumb when choosing the right
    kernel to start with due to its simplicity and efficiency. However, most of the
    time, this is very difficult to identify, unless you have sufficient prior knowledge
    of the dataset, or its features are of low dimensions (1 to 3).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，线性可分性是选择合适核函数的经验法则，因为它简单高效。然而，大多数情况下，除非你对数据集有足够的先验知识，或者其特征的维度较低（1 到 3），否则很难识别出线性可分性。
- en: Some general prior knowledge that is commonly known is that text data is often
    linearly separable, while data generated from the XOR function ([https://en.wikipedia.org/wiki/XOR_gate](https://en.wikipedia.org/wiki/XOR_gate))
    is not.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 一些常见的先验知识是，文本数据通常是线性可分的，而来自 XOR 函数的数据（[https://en.wikipedia.org/wiki/XOR_gate](https://en.wikipedia.org/wiki/XOR_gate)）则不是。
- en: Now, let’s look at the following three scenarios where the linear kernel is
    favored over RBF.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看一下在以下三种场景中，线性核函数相较于 RBF 更受青睐。
- en: '**Scenario 1**: Both the number of features and the number of instances are
    large (more than 104 or 105). Since the dimension of the feature space is high
    enough, additional features as a result of RBF transformation will not provide
    a performance improvement, but this will increase the computational expense. Some
    examples from the UCI machine learning repository (a collection of databases,
    and data generators widely used for empirical analysis of ML algorithms) are of
    this type:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景 1**：特征数量和样本数量都很大（超过 104 或 105）。由于特征空间的维度足够高，RBF 转换所增加的特征不会提供性能提升，反而会增加计算开销。UCI
    机器学习库中的一些例子属于这一类型（UCI 机器学习库是一个广泛用于机器学习算法经验分析的数据库和数据生成器集合）：'
- en: '**URL Reputation Dataset**: [https://archive.ics.uci.edu/ml/datasets/URL+Reputation](https://archive.ics.uci.edu/ml/datasets/URL+Reputation)
    (the number of instances: 2,396,130; the number of features: 3,231,961). This
    is designed for malicious URL detection based on their lexical and host information.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**URL 声誉数据集**：[https://archive.ics.uci.edu/ml/datasets/URL+Reputation](https://archive.ics.uci.edu/ml/datasets/URL+Reputation)（实例数量：2,396,130；特征数量：3,231,961）。该数据集用于基于词汇和主机信息检测恶意
    URL。'
- en: '**YouTube Multiview Video Games Dataset**: [https://archive.ics.uci.edu/ml/datasets/YouTube+Multiview+Video+Games+Dataset](https://archive.ics.uci.edu/ml/datasets/YouTube+Multiview+Video+Games+Dataset)
    (the number of instances: 120,000; the number of features: 1,000,000). This is
    designed for topic classification.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**YouTube 多视角视频游戏数据集**：[https://archive.ics.uci.edu/ml/datasets/YouTube+Multiview+Video+Games+Dataset](https://archive.ics.uci.edu/ml/datasets/YouTube+Multiview+Video+Games+Dataset)（实例数量：120,000；特征数量：1,000,000）。该数据集用于主题分类。'
- en: '**Scenario 2**: The number of features is noticeably large compared to the
    number of training samples. Apart from the reasons stated in *scenario 1*, the
    RBF kernel is significantly more prone to overfitting. Such a scenario occurs,
    for example, in the following examples:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景 2**：特征的数量与训练样本的数量相比明显较大。除了*场景 1*中提到的原因外，RBF 核函数更容易出现过拟合。这样的场景出现在以下几个例子中：'
- en: '**Dorothea Dataset**: [https://archive.ics.uci.edu/ml/datasets/Dorothea](https://archive.ics.uci.edu/ml/datasets/Dorothea)
    (the number of instances: 1,950; the number of features: 100,000). This is designed
    for drug discovery that classifies chemical compounds as active or inactive, according
    to their structural molecular features.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dorothea 数据集**：[https://archive.ics.uci.edu/ml/datasets/Dorothea](https://archive.ics.uci.edu/ml/datasets/Dorothea)（实例数量：1,950；特征数量：100,000）。该数据集旨在用于药物发现，通过化学化合物的结构分子特征，将其分类为活性或非活性。'
- en: '**Arcene Dataset**: [https://archive.ics.uci.edu/ml/datasets/Arcene](https://archive.ics.uci.edu/ml/datasets/Arcene)
    (the number of instances: 900; the number of features: 10,000). This represents
    a mass spectrometry dataset for cancer detection.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Arcene 数据集**：[https://archive.ics.uci.edu/ml/datasets/Arcene](https://archive.ics.uci.edu/ml/datasets/Arcene)（实例数量：900；特征数量：10,000）。这是一个用于癌症检测的质谱数据集。'
- en: '**Scenario 3**: The number of instances is significantly large compared to
    the number of features. For a dataset of low dimensions, the RBF kernel will,
    in general, boost the performance by mapping it to a higher-dimensional space.
    However, due to the training complexity, it usually becomes inefficient on a training
    set with more than 106 or 107 samples. Example datasets include the following:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**场景 3**：实例数量与特征数量相比显著较大。对于低维数据集，RBF 核通常通过将其映射到更高维空间来提升性能。然而，由于训练复杂度，通常在样本数超过
    106 或 107 时，RBF 核在训练集上的效率较低。示例数据集包括以下内容：'
- en: '*Heterogeneity Activity Recognition Dataset*: [https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition](https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition)
    (the number of instances: `43,930,257`; the number of features: `16`). This is
    designed for human activity recognition.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*异质性活动识别数据集*：[https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition](https://archive.ics.uci.edu/ml/datasets/Heterogeneity+Activity+Recognition)（实例数量：`43,930,257`；特征数量：`16`）。该数据集旨在进行人类活动识别。'
- en: '*HIGGS Dataset*: [https://archive.ics.uci.edu/ml/datasets/HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS)
    (the number of instances: 11,000,000; the number of features: 28). This is designed
    to distinguish between a signal process producing Higgs bosons or a background
    process.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*HIGGS 数据集*：[https://archive.ics.uci.edu/ml/datasets/HIGGS](https://archive.ics.uci.edu/ml/datasets/HIGGS)（实例数量：11,000,000；特征数量：28）。该数据集旨在区分产生希格斯玻色子的信号过程和背景过程。'
- en: Aside from these three scenarios, you can consider experimenting with RBF kernels.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 除了这三种场景，你还可以考虑尝试使用 RBF 核。
- en: 'The rules for choosing between linear and RBF kernels can be summarized as
    follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 选择线性核与 RBF 核的规则可以总结如下：
- en: '| **Scenario** | **Linear** | **RBF** |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| **场景** | **线性** | **RBF** |'
- en: '| Prior knowledge | If linearly separable | If nonlinearly separable |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 先验知识 | 如果线性可分 | 如果非线性可分 |'
- en: '| Visualizable data of 1 to 3 dimension(s) | If linearly separable | If nonlinearly
    separable |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 1 到 3 维可视化数据 | 如果线性可分 | 如果非线性可分 |'
- en: '| Both the number of features and number of instances are large. | First choice
    |  |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 特征数量和实例数量都很大。 | 首选 |  |'
- en: '| Features >> Instances | First choice |  |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 特征 >> 实例 | 首选 |  |'
- en: '| Instances >> Features | First choice |  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 实例 >> 特征 | 首选 |  |'
- en: '| Others |  | First choice |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 其他 |  | 首选 |'
- en: 'Table 9.1: Rules for choosing between linear and RBF kernels'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1：选择线性核与 RBF 核的规则
- en: Once again, **first choice** means we can **begin with** this option; it does
    not mean that this is the only option moving forward.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，**首选**意味着我们可以**从这个选项开始**；并不意味着这是唯一的选择。
- en: Next, let’s take a look at classifying face images.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看如何分类人脸图像。
- en: Classifying face images with SVM
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SVM 分类人脸图像
- en: Finally, it is time to build an SVM-based face image classifier using everything
    you just learned. We will do so in parts, exploring the image dataset first.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候利用你刚刚学到的所有知识，构建一个基于 SVM 的人脸图像分类器了。我们将分部分进行，首先探索图像数据集。
- en: Exploring the face image dataset
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索人脸图像数据集
- en: We will use the **Labeled Faces in the Wild** (**LFW**) people dataset ([https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html))
    from scikit-learn. It consists of more than 13,000 curated face images of more
    than 5,000 famous people. Each class has various numbers of image samples.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**LFW人脸数据集**（**LFW**）([https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html))，该数据集包含了超过5,000位名人的13,000多张精选人脸图像。每个类别有不同数量的图像样本。
- en: 'First, we load the face image data as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们加载人脸图像数据，如下所示：
- en: '[PRE13]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We only load classes with at least `80` samples so that we will have enough
    training data. Note that if you run into the problem of `ImportError: The Python
    Imaging Library (PIL) is required to load data from jpeg files`, please install
    the `pillow` package in the terminal, as follows:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '我们只加载至少有`80`个样本的类别，以确保有足够的训练数据。注意，如果你遇到`ImportError: The Python Imaging Library
    (PIL) is required to load data from jpeg files`的错误，请在终端中安装`pillow`包，方法如下：'
- en: '[PRE14]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If you encounter an `urlopen` error, you can download the four data files manually
    from the following URLs:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到`urlopen`错误，可以手动从以下链接下载四个数据文件：
- en: '`pairsDevTrain.txt`: [https://ndownloader.figshare.com/files/5976012](https://ndownloader.figshare.com/files/5976012)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pairsDevTrain.txt`：[https://ndownloader.figshare.com/files/5976012](https://ndownloader.figshare.com/files/5976012)'
- en: '`pairsDevTest.txt`: [https://ndownloader.figshare.com/files/5976009](https://ndownloader.figshare.com/files/5976009)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pairsDevTest.txt`：[https://ndownloader.figshare.com/files/5976009](https://ndownloader.figshare.com/files/5976009)'
- en: '`pairs.txt`: [https://ndownloader.figshare.com/files/5976006](https://ndownloader.figshare.com/files/5976006)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pairs.txt`：[https://ndownloader.figshare.com/files/5976006](https://ndownloader.figshare.com/files/5976006)'
- en: '`lfw-funneled.tgz`: [https://ndownloader.figshare.com/files/5976015](https://ndownloader.figshare.com/files/5976015)'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lfw-funneled.tgz`：[https://ndownloader.figshare.com/files/5976015](https://ndownloader.figshare.com/files/5976015)'
- en: 'You can then place them in a designated folder, for example, the current path,
    `./`. Accordingly, the code to load image data becomes the following:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以将它们放在一个指定的文件夹中，例如当前路径`./`。因此，加载图像数据的代码如下：
- en: '[PRE15]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Next, we take a look at the data we just loaded:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们查看刚刚加载的数据：
- en: '[PRE16]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This five-class dataset contains `1,140` samples and a sample is of `2,914`
    dimensions. As a good practice, we analyze the label distribution as follows:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 这个五类数据集包含`1,140`个样本，每个样本有`2,914`个维度。作为一种好的实践，我们分析标签分布如下：
- en: '[PRE17]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The dataset is rather imbalanced. Let’s keep this in mind when we build the
    model.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集相当不平衡。我们在构建模型时要考虑这一点。
- en: 'Now, let’s plot a few face images:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们绘制一些人脸图像：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You will see the following 12 images with their labels:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你将看到以下12张带有标签的图像：
- en: '![](img/B21047_09_12.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B21047_09_12.png)'
- en: 'Figure 9.12: Samples from the LFW people dataset'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12：来自LFW人脸数据集的样本
- en: Now that we have covered exploratory data analysis, we will move on to the model
    development phase in the next section.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了探索性数据分析，接下来我们将在下一部分进入模型开发阶段。
- en: Building an SVM-based image classifier
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建基于SVM的图像分类器
- en: 'First, we split the data into the training and testing set:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据拆分为训练集和测试集：
- en: '[PRE19]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In this project, the number of dimensions is greater than the number of samples.
    This is a classification case that SVM is effective at solving. In our solution,
    we will tune the hyperparameters, including the penalty `C`, the kernel (linear
    or RBF), and ![](img/B21047_09_055.png) (for the RBF kernel) through cross-validation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，维度数大于样本数。这是一个SVM能够有效解决的分类问题。在我们的解决方案中，我们将通过交叉验证调整超参数，包括惩罚项`C`、核函数（线性或RBF）以及![](img/B21047_09_055.png)（对于RBF核）。
- en: 'We then initialize a common SVM model:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们初始化一个常见的SVM模型：
- en: '[PRE20]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The dataset is imbalanced, so we set `class_weight='balanced`' to emphasize
    the underrepresented classes.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集不平衡，因此我们设置`class_weight='balanced'`以强调少数类。
- en: 'We utilize the GridSearchCV module from scikit-learn to search for the best
    combination of hyperparameters over several candidates. We will explore the following
    hyperparameter candidates:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用scikit-learn的GridSearchCV模块在多个候选参数中搜索最佳超参数组合。我们将探索以下超参数候选值：
- en: '[PRE21]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: If you are unsure about the suitable value of gamma to start with for RBF kernel,
    opting for 1 divided by the feature dimension is consistently a reliable choice.
    So in this example, `1/2914 = 0.0003`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定要为RBF核选择哪个合适的gamma值，选择`1`除以特征维度通常是一个可靠的选择。因此，在这个例子中，`1/2914 = 0.0003`。
- en: 'The `GridSearchCV` model we just initialized will conduct five-fold cross-validation
    (`cv=5`) and will run in parallel on all available cores (`n_jobs=-1`). We then
    perform hyperparameter tuning by simply applying the `fit` method:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚初始化的`GridSearchCV`模型将进行五折交叉验证（`cv=5`），并将在所有可用的核心上并行运行（`n_jobs=-1`）。然后我们通过简单地应用`fit`方法来进行超参数调优：
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We obtain the optimal set of hyperparameters using the following code:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过以下代码获取最佳超参数集：
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then, we obtain the best five-fold averaged performance under the optimal set
    of parameters by using the following code:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过以下代码获得在最佳参数集下的五折交叉验证平均性能：
- en: '[PRE24]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We then retrieve the SVM model with the optimal set of hyperparameters and
    apply it to the testing set:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们获取最佳超参数集的 SVM 模型，并将其应用到测试集上：
- en: '[PRE25]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We then calculate the accuracy and classification report:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们计算准确率和分类报告：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It should be noted that we tune the model based on the original training set,
    which is divided into folds for cross-training and validation internally, and
    that we apply the optimal model to the original testing set. We examine the classification
    performance in this manner to measure how well generalized the model is, in order
    to make correct predictions on a completely new dataset. An accuracy of `89.8%`
    is achieved with the best SVM model.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，我们基于原始训练集对模型进行调优，训练集会被划分为若干折进行交叉训练和验证，然后将最佳模型应用到原始测试集上。我们通过这种方式检验分类性能，以衡量模型的泛化能力，从而确保能够在全新的数据集上做出正确的预测。最佳
    SVM 模型达到了`89.8%`的准确率。
- en: There is another SVM classifier, `LinearSVC` ([https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)),
    from scikit-learn. How is it different from `SVC`? `LinearSVC` is similar to `SVC`
    with linear kernels, but it is implemented based on the `liblinear` library, which
    is better optimized than `libsvm` with the linear kernel, and its penalty function
    is more flexible.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，scikit-learn 中还有一个 SVM 分类器，`LinearSVC`（[https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html)）。它与`SVC`有何不同？`LinearSVC`与具有线性核的`SVC`相似，但它基于`liblinear`库实现，优化优于使用线性核的`libsvm`，并且其惩罚函数更加灵活。
- en: In general, training with the `LinearSVC` model is faster than `SVC`. This is
    because the `liblinear` library with high scalability is designed for large datasets,
    while the `libsvm` library, with more than quadratic computation complexity, is
    not able to scale well with more than 10⁵ training instances. But again, the `LinearSVC`
    model is limited to only linear kernels.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，使用`LinearSVC`模型训练比`SVC`模型训练更快。这是因为具有高可扩展性的`liblinear`库是为大规模数据集设计的，而`libsvm`库的计算复杂度超过二次，无法很好地处理超过
    10⁵ 个训练实例的数据。但同样，`LinearSVC`模型仅限于线性核。
- en: Boosting image classification performance with PCA
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PCA 提升图像分类性能
- en: We can also improve the image classifier by compressing the input features with
    **Principal Component Analysis** (**PCA**) (*A Tutorial on Principal Component
    Analysis* by Jonathon Shlens). This reduces the dimension of the original feature
    space and preserves the most important internal relationships among features.
    In simple terms, PCA projects the original data into a smaller space with the
    most important directions (coordinates). We hope that in cases where we have more
    features than training samples, considering fewer features as a result of dimensionality
    reduction using PCA can prevent overfitting.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过使用**主成分分析**（**PCA**）来压缩输入特征，从而改进图像分类器（参考 Jonathon Shlens 的《主成分分析教程》）。这会减少原始特征空间的维度，并保留特征之间最重要的内部关系。简单来说，PCA
    将原始数据投影到一个包含最重要方向（坐标）的较小空间中。我们希望，在特征数量超过训练样本数的情况下，通过使用 PCA 进行降维，减少特征的数量可以防止过拟合。
- en: 'Here’s how PCA works:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是 PCA 的工作原理：
- en: '**Data Standardization**: Before applying PCA, it is essential to standardize
    the data by subtracting the mean and dividing it by the standard deviation for
    each feature. This step ensures that all features are on the same scale and prevents
    any single feature from dominating the analysis.'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据标准化**：在应用 PCA 之前，必须通过减去均值并除以每个特征的标准差来对数据进行标准化。这一步骤确保所有特征处于相同的尺度上，并防止某个特征在分析中占主导地位。'
- en: '**Covariance Matrix Calculation**: PCA calculates the covariance matrix of
    the standardized data. The covariance matrix shows how each pair of features varies
    together. The diagonal elements of the covariance matrix represent the variance
    of individual features, while the off-diagonal elements represent the covariance
    between pairs of features.'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**协方差矩阵计算（Covariance Matrix Calculation）**：PCA 计算标准化数据的协方差矩阵。协方差矩阵显示每对特征之间的变化关系。协方差矩阵的对角元素表示单个特征的方差，而非对角元素表示特征对之间的协方差。'
- en: '**Eigendecomposition**: The next step is to perform eigendecomposition on the
    covariance matrix. Eigendecomposition breaks down the covariance matrix into its
    eigenvectors and eigenvalues. The eigenvectors represent the principal components,
    and the corresponding eigenvalues indicate the amount of variance explained by
    each principal component.'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征分解（Eigendecomposition）**：下一步是对协方差矩阵进行特征分解。特征分解将协方差矩阵分解成其特征向量和特征值。特征向量表示主成分，而相应的特征值则表示每个主成分所解释的方差量。'
- en: '**Selecting Principal Components**: The principal components are sorted based
    on their corresponding eigenvalues in descending order. The first principal component
    (PC1) explains the highest variance, followed by PC2, PC3, and so on. Typically,
    you select a subset of principal components that explain a significant portion
    (e.g., 95% or more) of the total variance.'
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**选择主成分（Selecting Principal Components）**：根据特征值的大小降序排列主成分。第一个主成分（PC1）解释了最高的方差，其次是
    PC2、PC3 等。通常，你会选择一部分主成分，它们解释了总方差中的重要部分（例如，95% 或更多）。'
- en: '**Projection**: Finally, the data is projected onto the selected principal
    components to create a lower-dimensional representation of the original data.
    This lower-dimensional representation captures most of the variance in the data
    while reducing the number of features.'
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**投影（Projection）**：最后，将数据投影到选定的主成分上，以创建原始数据的低维表示。这种低维表示保留了数据中的大部分方差信息，同时减少了特征的数量。'
- en: You can read more about PCA at [https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained](https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained)
    if you are interested. We will implement PCA with the `PCA` module ([https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))
    from scikit-learn. We will first apply PCA to reduce the dimensionality and train
    the classifier on the resulting data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣，可以阅读更多关于 PCA 的内容，链接为 [https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained](https://www.kaggle.com/nirajvermafcb/principal-component-analysis-explained)。我们将使用来自
    scikit-learn 的 `PCA` 模块（[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)）来实现
    PCA。我们首先应用 PCA 降维，然后在降维后的数据上训练分类器。
- en: In machine learning, we usually concatenate multiple consecutive steps and treat
    them as one “model.” We call this process **pipelining**. We utilize the `pipeline`
    API ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html))
    from scikit-learn to facilitate this.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们通常将多个连续的步骤串联在一起，视为一个“模型”。我们称这个过程为**管道化（pipelining）**。我们利用来自 scikit-learn
    的 `pipeline` API（[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)）来简化这一过程。
- en: 'Now, let’s initialize a PCA model, an SVC model, and a model pipelining these
    two:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们初始化一个 PCA 模型、一个 SVC 模型，并将这两个模型通过管道连接起来：
- en: '[PRE27]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The PCA component projects the original data into a 100-dimension space, followed
    by the SVC classifier with the RBF kernel. We then perform a grid search for the
    best model among a few options:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 组件将原始数据投影到 100 维空间中，接着使用带有 RBF 核的 SVC 分类器。然后，我们在一些选项中执行网格搜索，以寻找最佳模型：
- en: '[PRE28]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**Best practice**'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '**最佳实践**  '
- en: 'Choosing the initial values of hyperparameters like C and gamma in grid search
    for SVMs is crucial for efficiently finding optimal values. Here are some best
    practices:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在网格搜索 SVM 的超参数（如 C 和 gamma）的初始值时，选择合理的初值非常重要，这样可以高效地找到最佳值。以下是一些最佳实践：
- en: '**Start with a coarse grid**: Begin with a coarse grid that covers a wide range
    of values for C and gamma. This allows you to quickly explore the hyperparameter
    space and identify promising regions.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从粗略网格开始**：从一个覆盖 C 和 gamma 范围广泛的粗略网格开始。这可以让你快速探索超参数空间，找到有潜力的区域。'
- en: '**Consider specific knowledge**: Incorporate any prior knowledge or domain
    expertise about the problem into the selection of initial values. For example,
    if you know that the dataset is noisy or has outliers, you may want to prioritize
    larger values of C to allow for more flexibility in the decision boundary.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**考虑特定的知识**：在选择初始值时，结合有关问题的先验知识或领域专长。例如，如果你知道数据集噪声较大或包含异常值，可能需要优先考虑较大的C值，以便在决策边界上提供更多的灵活性。'
- en: '**Use cross-validation**: This helps to assess how well the initial values
    generalize to unseen data and guides the refinement of the grid search.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用交叉验证**：这有助于评估初始值对未见数据的泛化能力，并指导网格搜索的优化。'
- en: '**Iteratively refine the grid**: Based on the results of initial cross-validation,
    iteratively refine the grid around regions that show promising performance. Narrow
    down the range of values for C and gamma to focus the search on areas where the
    optimal values are likely to lie.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**迭代优化网格**：基于初步交叉验证的结果，围绕表现良好的区域迭代地优化网格。缩小C和gamma的取值范围，将搜索重点放在可能包含最优值的区域。'
- en: 'Finally, we print out the best set of hyperparameters and the classification
    performance with the best model:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印出最佳超参数集和最佳模型的分类性能：
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The model composed of a PCA and an SVM classifier achieves an accuracy of `92.3%`.
    PCA boosts the performance of the SVM-based image classifier.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 由PCA和SVM分类器组成的模型实现了`92.3%`的准确率。PCA提高了基于SVM的图像分类器的性能。
- en: Following the successful application of SVM in image classification, we will
    look at its application in regression.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在SVM成功应用于图像分类之后，我们将探讨其在回归中的应用。
- en: Estimating with support vector regression
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用支持向量回归进行估计
- en: As the name implies, SVR is part of the support vector family and a sibling
    of the **Support Vector Machine** (**SVM**) for classification (or we can just
    call it **SVC**).
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 顾名思义，SVR是支持向量家族的一部分，是**支持向量机**（**SVM**）的回归兄弟（或者我们也可以直接称其为**SVC**）。
- en: 'To recap, SVC seeks an optimal hyperplane that best segregates observations
    from different classes. In SVR, our goal is to find a decision hyperplane (defined
    by a slope vector *w* and intercept *b*) so that two hyperplanes ![](img/B21047_09_065.png)
    (negative hyperplane) and ![](img/B21047_09_066.png) (positive hyperplane) can
    cover the ![](img/B21047_09_067.png) bands of the optimal hyperplane. Simultaneously,
    the optimal hyperplane is as flat as possible, which means *w* is as small as
    possible, as shown in the following diagram:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，SVC寻求一个最佳的超平面，将不同类别的观测值尽可能分开。而在SVR中，我们的目标是找到一个决策超平面（由斜率向量*w*和截距*b*定义），使得两个超平面
    ![](img/B21047_09_065.png)（负超平面）和 ![](img/B21047_09_066.png)（正超平面）可以覆盖最优超平面的 ![](img/B21047_09_067.png)带域。与此同时，最优超平面尽可能平坦，这意味着*w*越小越好，如下图所示：
- en: '![A diagram of a positive hyperplane  Description automatically generated with
    low confidence](img/B21047_09_13.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![正超平面的示意图，描述自动生成，置信度较低](img/B21047_09_13.png)'
- en: 'Figure 9.13: Finding the decision hyperplane in SVR'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13：在SVR中找到决策超平面
- en: 'This translates into deriving the optimal *w* and *b* by solving the following
    optimization problem:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着通过求解以下优化问题来推导出最优的*w*和*b*：
- en: Minimizing ![](img/B21047_09_068.png)
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化 ![](img/B21047_09_068.png)
- en: Subject to ![](img/B21047_09_069.png) , given a training set of ![](img/B21047_09_070.png),
    ![](img/B21047_09_071.png), … ![](img/B21047_09_072.png)…, ![](img/B21047_09_073.png)
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 受限于 ![](img/B21047_09_069.png)，给定一个训练集 ![](img/B21047_09_070.png)、 ![](img/B21047_09_071.png)、……
    ![](img/B21047_09_072.png)……， ![](img/B21047_09_073.png)
- en: The theory behind SVR is very similar to SVM. In the next section, let’s see
    the implementation of SVR.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量回归（SVR）的理论与支持向量机（SVM）非常相似。在接下来的部分中，让我们来看看SVR的实现。
- en: Implementing SVR
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实现SVR
- en: Again, to solve the preceding optimization problem, we need to resort to quadratic
    programming techniques, which are beyond the scope of our learning journey. Therefore,
    we won’t cover the computation methods in detail and will implement the regression
    algorithm using the `SVR` package ([https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html))
    from scikit-learn.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，要解决前述的优化问题，我们需要求助于二次规划技术，这些技术超出了我们学习之旅的范围。因此，我们不会详细介绍计算方法，而是使用scikit-learn的`SVR`包([https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html))来实现回归算法。
- en: Important techniques used in SVM, such as penalty as a trade-off between bias
    and variance, and the kernel (RBF, for example) handling linear non-separation,
    are transferable to SVR. The `SVR` package from scikit-learn also supports these
    techniques.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）中使用的重要技术，如惩罚项作为偏差与方差之间的权衡，以及核函数（例如RBF）处理线性不可分的问题，都是可以迁移到SVR中的。scikit-learn中的`SVR`包也支持这些技术。
- en: 'Let’s solve the previous diabetes prediction problem with `SVR` this time,
    as we did in *Chapter 5*, *Predicting Stock Prices with Regression Algorithms*:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们这次用`SVR`解决之前的糖尿病预测问题，正如我们在*第5章*《使用回归算法预测股票价格》中所做的那样：
- en: 'Initially, we load the dataset and check the data size, as follows:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载数据集并检查数据大小，如下所示：
- en: '[PRE30]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we designate the last 30 samples as the testing set, while the remaining
    samples serve as the training set:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将最后30个样本指定为测试集，其余样本作为训练集：
- en: '[PRE31]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We can now apply the SVR regressor to the data. We first initialize an `SVC`
    model and fit it against the training set:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以将SVR回归模型应用于数据。首先，我们初始化一个`SVC`模型，并将其拟合到训练集上：
- en: '[PRE32]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Here, we start with a linear kernel.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们从线性核函数开始。
- en: 'We predict on the testing set with the trained model and obtain the prediction
    performance:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们用训练好的模型在测试集上进行预测，并获得预测性能：
- en: '[PRE33]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: With this simple model, we are able to achieve an *R*² of `0.59`.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个简单的模型，我们能够获得*R*²为`0.59`。
- en: 'Let’s further improve it with a grid search to find the best model from the
    following options:'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们通过网格搜索进一步改进，找出以下选项中的最佳模型：
- en: '[PRE34]'
  id: totrans-285
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After searching over 18 sets of hyperparameters, we find the best model with
    the following combination of hyperparameters:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对18组超参数进行搜索后，我们找到了以下超参数组合下的最佳模型：
- en: '[PRE35]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Finally, we use the best model to make predictions and evaluate its performance:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们使用最好的模型进行预测并评估其性能：
- en: '[PRE36]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We are able to boost the *R*² score to `0.68` after fine-tuning.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 经过微调后，我们将*R*²得分提升到`0.68`。
- en: Unlike SVM for classification, where the goal is to separate data into distinct
    classes, SVR focuses on finding a function that best fits the data, by minimizing
    the prediction error while allowing some tolerance.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 与用于分类的支持向量机（SVM）不同，SVM的目标是将数据分成不同的类别，而支持向量回归（SVR）则侧重于找到一个最适合数据的函数，通过最小化预测误差并允许一定的容忍度。
- en: Summary
  id: totrans-292
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we continued our journey of supervised learning with SVM. You
    learned about the mechanics of an SVM, kernel techniques, implementations of SVM,
    and other important concepts of machine learning classification, including multiclass
    classification strategies and grid search, as well as useful tips to use an SVM
    (for example, choosing between kernels and tuning parameters). Then, we finally
    put into practice what you learned in the form of real-world use cases, including
    face recognition. You also learned about SVM’s extension to regression, SVR.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续了使用支持向量机（SVM）进行监督学习的旅程。你学习了SVM的原理、核函数技巧、SVM的实现方法以及其他机器学习分类的重要概念，包括多类分类策略和网格搜索，同时也学到了使用SVM的一些实用技巧（例如选择核函数和调优参数）。最后，我们通过真实的应用案例来实践所学内容，包括人脸识别。你还学习了SVM在回归中的扩展，即支持向量回归（SVR）。
- en: In the next chapter, we will review what you have learned so far in this book
    and examine the best practices of real-world machine learning. The chapter aims
    to make your learning foolproof and get you ready for the entire machine learning
    workflow and productionization. This will be a wrap-up of the general machine
    learning techniques before we move on to more complex topics in the final three
    chapters.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将回顾你在本书中学到的内容，并研究现实世界机器学习的最佳实践。本章旨在让你的学习万无一失，为整个机器学习工作流程和生产化做好准备。这将是对一般机器学习技术的总结，然后我们将进入最后三章更复杂的主题。
- en: Exercises
  id: totrans-295
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you implement SVM using the `LinearSVC` module? What are the hyperparameters
    that you need to tweak, and what is the best performance of face recognition you
    can achieve?
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能使用`LinearSVC`模块实现SVM吗？你需要调整哪些超参数，能达到的人脸识别的最佳性能是多少？
- en: Can you classify more classes in the image recognition project? As an example,
    you can set `min_faces_per_person=50`. What is the best performance you can achieve
    using grid search and cross-validation?
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在图像识别项目中分类更多类别吗？举个例子，你可以设置`min_faces_per_person=50`。通过网格搜索和交叉验证，你能达到最佳性能吗？
- en: Explore stock price prediction using SVR. You can reuse the dataset and feature
    generation functions from *Chapter 5*, *Predicting Stock Prices with Regression
    Algorithms*.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 探索使用SVR进行股票价格预测。你可以重用*第五章*中的数据集和特征生成函数，*使用回归算法预测股票价格*。
- en: Join our book’s Discord space
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的Discord空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code187846872178698968.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code187846872178698968.png)'
