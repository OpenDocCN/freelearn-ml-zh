- en: Anomaly Detection – Finding Outliers in Data
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测 – 找出数据中的异常值
- en: Detecting anomalies in data is a recurring theme in machine learning. In [Chapter
    10](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=32&action=edit)[,](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=32&action=edit)*Imbalanced
    Learning – Not Even 1% Win the Lottery*, we learned how to spot these interesting
    minorities in our data. Back then, the data was labeled and the classification
    algorithms from the previous chapters were apt for the problem. Aside from **labeled
    anomaly detection** problems, there are cases where data is unlabeled.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 检测数据中的异常是机器学习中的一个重复性主题。在[第10章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=32&action=edit)[,](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=32&action=edit)*Imbalanced
    Learning – Not Even 1% Win the Lottery*，我们学习了如何在数据中发现这些有趣的少数群体。那时，数据是有标签的，并且之前章节中的分类算法适用于该问题。除了**有标签异常检测**问题外，还有一些情况下数据是无标签的。
- en: 'In this chapter, we are going to learn how to identify outliers in our data,
    even when no labels are provided. We will use three different algorithms and we
    will learn about the two branches of **unlabeled anomaly detection**. Here are
    the topics that will be covered in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习如何在没有标签的情况下识别数据中的异常值。我们将使用三种不同的算法，并学习**无标签异常检测**的两个分支。本章将涵盖以下主题：
- en: Unlabeled anomaly detection
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无标签异常检测
- en: Detecting anomalies using basic statistics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基本统计方法检测异常
- en: Detecting outliers using `EllipticEnvelope`
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`EllipticEnvelope`检测异常值
- en: Outlier and novelty detection using **Local Outlier Factor** (**LOF**)
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用**局部异常因子**（**LOF**）进行异常值和新颖性检测
- en: Detecting outliers using isolation forest
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用隔离森林检测异常值
- en: Unlabeled anomaly detection
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无标签异常检测
- en: In this chapter, we will start with some unlabeled data and we will need to
    spot the anomalous samples in it. We may be given inliers only, and we want to
    learn what normal data looks likefrom them. Then, after fitting a model on our
    inliers, we are given new data and need to spot any outliers that diverge from
    the data seen so far. These kinds of problems are referred to as **novelty detectio****n**.
    On the other hand, if we fit our model on a dataset that consists of a combination
    of inliers and outliers, then this problem is referred to as an **outlier detection**
    problem.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将从一些无标签数据开始，我们需要在其中找到异常样本。我们可能只会得到正常数据（inliers），并希望从中学习正常数据的特征。然后，在我们对正常数据拟合一个模型后，给定新的数据，我们需要找出与已知数据不符的异常值（outliers）。这类问题被称为**新颖性检测**。另一方面，如果我们在一个包含正常数据和异常值的数据集上拟合我们的模型，那么这个问题被称为**异常值检测**问题。
- en: Like any other unlabeled algorithm, the `fit` method ignores any labels given.
    This method's interface allows you to pass in both *x* and *y*, for the sake of
    consistency, but *y* is simply ignored. In cases of novelty detection, it is logical
    to firstuse the`fit`method on a dataset that includes no outliers, and then use
    the algorithm's`predict`method later on for data that includes both inliers and
    outliers. Conversely, for outlier detection problems, it is common to apply your
    `fit` method and predict all at once with the `fit_predict` method.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他无标签算法一样，`fit`方法会忽略任何给定的标签。该方法的接口允许你传入* x * 和 * y *，为了保持一致性，但 * y * 会被简单忽略。在新颖性检测的情况下，首先在没有异常值的数据集上使用`fit`方法，然后在包含正常数据和异常值的数据上使用算法的`predict`方法是合乎逻辑的。相反，对于异常值检测问题，通常会同时使用`fit`方法进行拟合，并通过`fit_predict`方法进行预测。
- en: Before using any of our algorithms, we need to create a sample dataset to be
    used throughout this chapter. Our data will include 1,000 samples, with 98% of
    them coming from certain distributions and the remaining 2% coming from different
    distributions. In the next section, we are going to see how to create this sample
    data in detail.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用任何算法之前，我们需要创建一个样本数据集，以便在本章中使用。我们的数据将包括1,000个样本，其中98%的样本来自特定分布，剩余的2%来自不同的分布。在下一节中，我们将详细介绍如何创建这个样本数据。
- en: Generating sample data
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 生成样本数据
- en: The `make_classification` function allows us to specify the number of samples,
    as well as the number of features. We can limit the number of informative features
    and make some features redundant—that is, dependent on the informative features.
    We can also make some features copies of any of the informative or redundant features.
    In our current use case, we will make sure that all our features are informative
    since we are going to limit ourselves to two features only. Since the `make_classification`
    function is meant to produce data for classification problems, it returns both
    *x* and *y*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '`make_classification`函数允许我们指定样本数量和特征数量。我们可以限制信息性特征的数量，并使一些特征冗余——即依赖于信息性特征。我们也可以将一些特征设置为任何信息性或冗余特征的副本。在我们当前的使用案例中，我们将确保所有特征都是信息性的，因为我们将仅限于使用两个特征。由于`make_classification`函数是用于生成分类问题的数据，它同时返回*x*和*y*。'
- en: 'We will ignore *y* when building our models and only use it for evaluation
    later on. We will make sure each class comes from two different distributions
    by setting `n_clusters_per_class` to `2`. We will keep the two features to the
    same scale by setting `scale` to a single value. We will also make sure the data
    is randomly shuffled (`shuffle=True`) and that no samples from one class are labeled
    as members of the other class (`flip_y=0`). Finally, we will set `random_state`
    to `0` to make sure we get the exact same random data when running the following
    code on our computer:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建模型时，我们将忽略*y*，并只在后续评估中使用它。我们将确保每个类别来自两个不同的分布，通过将`n_clusters_per_class`设置为`2`。我们将通过将`scale`设置为一个单一值，确保两个特征保持相同的尺度。我们还将确保数据是随机洗牌的（`shuffle=True`），并且没有任何一个类别的样本被标记为另一个类别的成员（`flip_y=0`）。最后，我们将`random_state`设置为`0`，确保在我们的计算机上运行以下代码时获得完全相同的随机数据：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that the sample data is ready, it is time to think of ways to detect the
    outliers in it.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在样本数据已经准备好，是时候考虑如何检测其中的离群点了。
- en: Detecting anomalies using basic statistics
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用基本统计学检测异常值
- en: 'Rather than jumping straight into the available algorithms in scikit-learn,
    let''s start by thinking about ways to detect the anomalous samples. Imagine measuring
    the traffic to your website every hour, which gives you the following numbers:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在直接进入scikit-learn中现有的算法之前，让我们先思考一些方法来检测异常样本。假设每小时测量你网站的流量，这样你会得到以下数字：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Looking at these numbers, `500` sounds quite high compared to the others. Formally
    speaking, if the hourly traffic data is assumed to be normally distributed, then
    `500` is further away from its mean or expected value. We can measure this by
    calculating the mean of these numbers and then checking the numbers that are more
    than 2 or 3 standard deviations away from the mean. Similarly, we can calculate
    a high quantile and check which numbers are above it. Here, we find the values
    above the 95^(th) percentile:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 看这些数字，`500`相比其他数值看起来相当高。正式来说，如果假设每小时的流量数据符合正态分布，那么`500`就更远离其均值或期望值。我们可以通过计算这些数字的均值，并检查那些距离均值超过2或3个标准差的数值来衡量这一点。类似地，我们也可以计算一个高分位数，并检查哪些数值超过了这个分位数。这里，我们找到了高于95^(th)百分位数的值：
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This code will give an array of the `False` values, except for the penultimate
    value, which is the one corresponding to `500`. Before printing out the results,
    let''s put the preceding code in the form of an estimator with its `fit` and `predict`
    methods. The `fit` method calculates the threshold and saves it, and the `predict`
    method compares the new data to the saved threshold. I also added a `fit_predict`
    method that carries out these two operations in sequence. Here is the code for
    the estimator:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将给出一个`False`值的数组，除了倒数第二个值，它对应于`500`。在打印结果之前，让我们将前面的代码转化为一个估算器，并包含它的`fit`和`predict`方法。`fit`方法计算阈值并保存，而`predict`方法将新数据与保存的阈值进行比较。我还添加了一个`fit_predict`方法，它按顺序执行这两个操作。以下是估算器的代码：
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can now use our newly created estimator. In the following code snippet,
    we use the 95^(th) percentile for our estimator. We then put the resulting predictions
    alongside the original data into a data frame. Finally, I added some styling logic
    to mark the rows with outliers in bold:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用我们新创建的估算器。在以下代码片段中，我们使用95^(th)百分位数作为我们的估算器。然后，我们将得到的预测结果与原始数据一起放入数据框中。最后，我添加了一些样式逻辑，将离群点所在的行标记为粗体：
- en: '[PRE4]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here is the resulting data frame:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这是得到的数据框：
- en: '![](img/252fef3a-a4df-40de-85fb-cf0a6cf8dedc.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/252fef3a-a4df-40de-85fb-cf0a6cf8dedc.png)'
- en: Can we apply the same logic to the dataset from the previous section? Well,
    yes, but we need to figure out how to apply it to multi-dimensional data first.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能将相同的逻辑应用于前一部分的 dataset 吗？当然可以，但我们首先需要弄清楚如何将其应用于多维数据。
- en: Using percentiles for multi-dimensional data
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用百分位数处理多维数据
- en: 'Unlike the `hourly_traffic` data, the data we generated using the `make_classification`
    function is multi-dimensional. We have more than one feature to check this time.
    Obviously, we can check each feature separately. Here is the code for checking
    the outliers with respect to the first feature:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 与`hourly_traffic`数据不同，我们使用`make_classification`函数生成的数据是多维的。这次我们有多个特征需要检查。显然，我们可以分别检查每个特征。以下是检查第一个特征的离群点的代码：
- en: '[PRE5]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can do the same for the other feature as well:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以对其他特征做同样的事情：
- en: '[PRE6]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, we have ended up with two predictions. We can combine them in a way that
    each sample is marked as an outlier if it is an outlier with respect to any of
    the two features. In the following code snippet, we will tweak the `PercentileDetection`**estimator
    to do that:**
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们得出了两个预测结果。我们可以以一种方式将它们结合起来，如果某个样本相对于任何一个特征是离群点，那么它就被标记为离群点。在下面的代码片段中，我们将调整`PercentileDetection`**估算器来实现这一点：**
- en: '**[PRE7]'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**[PRE7]'
- en: 'Now, we can use the tweaked estimator as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以按如下方式使用调整后的估算器：
- en: '[PRE8]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We can also use the labels we ignored earlier to calculate the precision and
    recall of our new estimator. Since we care about the minority class, whose label
    is `1`, we set `pos_label` to `1` in the following code snippet:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用之前忽略的标签来计算我们新估算器的精度和召回率。因为我们关心的是标签为`1`的少数类，所以在以下代码片段中，我们将`pos_label`设置为`1`：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This gives a precision of `4%` and a recall of `5%`. Did you expect better
    results? I did too. Maybe we need to plot our data to understand what might be
    the problem with our method. Here is the dataset, where each sample is marked
    according to its label:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了`4%`的精度和`5%`的召回率。你期望更好的结果吗？我也希望如此。也许我们需要绘制数据来理解我们的方法可能存在哪些问题。以下是数据集，其中每个样本根据其标签进行标记：
- en: '![](img/c802e490-abac-42b3-a719-e7a2fcbf9e36.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c802e490-abac-42b3-a719-e7a2fcbf9e36.png)'
- en: Our method checks each point and sees whether it is extreme on one of the two
    axes. Despite the fact that the outliers are further away from the inliers, there
    are still inliers that share the same horizontal or vertical position of each
    point of the outliers. In other words, if you project your points onto any of
    the two axes, you will not be able to separate the outliers from the inliers anymore.
    So, we need a way to consider the two axes at once. What if we find the mean point
    of the two axes—that is, the center of our data—and then draw a circle or an ellipse
    around it? Then, we can consider any point that falls outside this ellipse an
    outlier. Would this new strategy help? Luckily, that's what the `EllipticEnvelope`
    algorithm does.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法检查每个点，看看它是否在两个轴中的一个上极端。尽管离群点距离内点较远，但仍然有一些内点与离群点的每个点共享相同的水平或垂直位置。换句话说，如果你将点投影到任意一个轴上，你将无法再将离群点与内点区分开来。因此，我们需要一种方法来同时考虑这两个轴。如果我们找到这两个轴的平均点——即我们的数据的中心，然后围绕它绘制一个圆或椭圆？然后，我们可以将任何位于椭圆外的点视为离群点。这个新策略会有效吗？幸运的是，这正是`EllipticEnvelope`算法的作用。
- en: Detecting outliers using EllipticEnvelope
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用EllipticEnvelope检测离群点
- en: '"I''m intimidated by the fear of being average."'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: “我害怕变得平庸。”
- en: – Taylor Swift
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: – 泰勒·斯威夫特
- en: The `EllipticEnvelope` algorithm finds the center of the data samples and then
    draws an ellipsoid around that center. The radii of the ellipsoid in each axis
    are measured in the **Mahalanobis** distance. You can think of the Mahalanobis
    distance as a Euclideandistance whose units are the number of standard deviations
    in each direction. After the ellipsoid is drawn, the points that fall outside
    it can be considered outliers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '`EllipticEnvelope`算法通过找到数据样本的中心，然后在该中心周围绘制一个椭圆体。椭圆体在每个轴上的半径是通过**马哈拉诺比斯**距离来衡量的。你可以将马哈拉诺比斯距离视为一种欧氏距离，其单位是每个方向上标准差的数量。绘制椭圆体后，位于椭圆体外的点可以被视为离群点。'
- en: The **multivariate Gaussian distribution** is a key concept of the `EllipticEnvelope`
    algorithm. It's a generalization of the one-dimensional Gaussian distribution.
    If the Gaussian distribution is defined by single-valued mean and variance, then
    the multivariate Gaussian distribution is defined by matrices for means and covariances.
    The multivariate Gaussian distribution is then used to draw an ellipsoid that
    defines what is normal and what is an outlier.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**多元高斯分布**是`EllipticEnvelope`算法的一个关键概念。它是单维高斯分布的推广。如果高斯分布通过单一的均值和方差来定义，那么多元高斯分布则通过均值和协方差的矩阵来定义。然后，多元高斯分布用于绘制一个椭球体，定义什么是正常的，什么是异常值。'
- en: 'Here is how we use the `EllipticEnvelope` algorithm to detect the data outliers,
    using the algorithm''s default settings. Keep in mind that the `predict` methods
    for all the outlier detection algorithms in this chapter return `-1` for outliers
    and `1` for inliers:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们如何使用`EllipticEnvelope`算法来检测数据中的异常值，使用该算法的默认设置。请记住，本章所有异常值检测算法的`predict`方法会返回`-1`表示异常值，返回`1`表示内点：
- en: '[PRE10]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can calculate the precision and the recall scores for the predictions using
    the exact same code from the previous section:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用前一节中的完全相同代码来计算预测的精确度和召回率：
- en: '[PRE11]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This time, we get a precision of `9%` and a recall of `45%`. That's already
    better than the previous scores, but can we do better? Well, if you take another
    look at the data, you will notice that it is non-convex. We already know that
    the samples in each class come from more than one distribution, and so the shape
    of the points doesn't seem like it would perfectly fit into an ellipse. This means
    that we should instead use an algorithm that bases its decision on local distances
    and densities, rather than comparing everything to a fixed centroid. The **Local
    Outlier Factor** (**LOF**) gives us that feature. If the **k-means****clustering****algorithm**
    of the previous chapter falls into the same group as the elliptic envelope algorithm,
    then the LOF would be the counterpart of the **DBSCAN** algorithm.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这一次，我们得到了`9%`的精确度和`45%`的召回率。这已经比之前的分数更好了，但我们能做得更好吗？嗯，如果你再看一下数据，你会注意到它是非凸的。我们已经知道每个类别中的样本来自多个分布，因此这些点的形状似乎无法完美地拟合一个椭圆。这意味着我们应该使用一种基于局部距离和密度的算法，而不是将所有东西与一个固定的中心点进行比较。**局部异常因子**（**LOF**）为我们提供了这种特性。如果上一章的**k均值聚类**算法属于椭圆包络算法的同一类，那么
    LOF 就是 **DBSCAN** 算法的对应物。
- en: Outlier and novelty detection using LOF
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 LOF 进行异常值和新颖性检测
- en: '"Madness is rare in individuals – but in groups, parties, nations, and ages,
    it is the rule."'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: “疯狂在个体中是罕见的——但在群体、党派、国家和时代中，它是常态。”
- en: – Friedrich Nietzsche
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: – 弗里德里希·尼采
- en: 'LOF takes an opposite approach to Nietzsche''s—it compares the density of a
    sample to the local densities of its neighbors. A sample existing in a low-density
    area compared to its neighbors is considered an outlier. Like any other neighbor-based
    algorithms, we have parameters to specify the number of neighbors to consider
    (`n_neighbors`) and the distance metric to use to find the neighbors (`metric`
    and `p`). By default, the Euclidean distance is used—that is, `metric=''minkowski''`
    and `p=2`. You can refer to [Chapter 5](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit),
    *Image Processing with Nearest Neighbors*, for more information about the available
    distance metrics. Here is how we use`LocalOutlierFactor` for outlier detection,
    using 50 neighbors and its default distance metric:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: LOF 与尼采的方式正好相反——它将样本的密度与其邻居的局部密度进行比较。与邻居相比，处于低密度区域的样本被视为异常值。像其他基于邻居的算法一样，我们可以设置参数来指定要考虑的邻居数量（`n_neighbors`）以及用于查找邻居的距离度量（`metric`
    和 `p`）。默认情况下，使用的是欧几里得距离——即，`metric='minkowski'` 和 `p=2`。有关可用距离度量的更多信息，您可以参考[第
    5 章](https://cdp.packtpub.com/hands_on_machine_learning_with_scikit_learn/wp-admin/post.php?post=28&action=edit)，*最近邻图像处理*。下面是我们如何使用`LocalOutlierFactor`进行异常值检测，使用50个邻居及其默认的距离度量：
- en: '[PRE12]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The precision and recall scores have now further improved the event. We got
    a precision value of `26%` and a recall value of `65%`.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 精确度和召回率得分现在已经进一步改善了结果。我们得到了`26%`的精确度和`65%`的召回率。
- en: 'Just like the classifiers, which have the `predict` method as well as `predict_proba`,
    outlier detection algorithms not only give us binary predictions, but can also
    tell us how confident they are that a sample is an outlier. Once the LOF algorithm
    is fitted, it stores its outlier factor scores in `negative_outlier_factor_`.
    A sample is more likely to be an outlier if the score is closer to `-1`. So, we
    can use this score and set its bottom 1%, 2%, or 10% values as outliers, and consider
    the rest inliers. Here is a comparison for the different performance metrics at
    each of the aforementioned thresholds:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分类器拥有`predict`方法以及`predict_proba`方法一样，离群点检测算法不仅会给出二分类预测，还可以告诉我们它们对于某个样本是否为离群点的置信度。一旦LOF算法被拟合，它会将其离群点因子分数存储在`negative_outlier_factor_`中。如果分数接近`-1`，则该样本更有可能是离群点。因此，我们可以使用这个分数，将最低的1%、2%或10%作为离群点，其余部分视为正常点。以下是不同阈值下的性能指标比较：
- en: '[PRE13]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here are the different precision and recall scores:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是不同的精确度和召回率分数：
- en: '[PRE14]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As in the case with the classifiers' probabilities, there is a trade-off here
    between the precision and recall scores for the different thresholds. This is
    how you can fine-tune your predictions to suit your needs. You can also use `negative_outlier_factor_`
    to plot the **Receiver Operating Characteristic** (**ROC**) or **Precision-Recall**
    (**PR**) curves if the true labels are known.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就像分类器的概率一样，在不同阈值下，精确度和召回率之间存在权衡。这就是你如何微调预测结果以满足需求的方法。如果已知真实标签，你还可以使用`negative_outlier_factor_`绘制**接收器操作特性**（**ROC**）曲线或**精确度-召回率**（**PR**）曲线。
- en: Aside from its use for outlier detection, theLOF**algorithm can also be used
    for novelty detection.**
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于离群点检测，LOF算法还可以用于新颖性检测。
- en: '**## Novelty detection using LOF'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '**## 使用LOF进行新颖性检测'
- en: 'When used for outlier detection, the algorithm has to be fitted on the dataset
    with both its inliers and outliers. In the case of novelty detection, we are expected
    to fit the algorithm on the inliers only, and then predict on a contaminated dataset
    later on. Furthermore, to be used for novelty detection, you have `novelty=True`
    during the algorithm''s initialization. Here, we remove the outliers from our
    data and use the resulting subsample, `x_inliers`, with the `fit` function. Then,
    we predict for the original dataset as normal:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 当用于离群点检测时，算法必须在包含正常点和离群点的数据集上进行拟合。而在新颖性检测的情况下，我们需要只在正常点（inliers）上拟合该算法，然后在后续预测中使用被污染的数据集。此外，为了用于新颖性检测，在算法初始化时需要将`novelty=True`。在这里，我们从数据中去除离群点，并使用得到的子样本`x_inliers`与`fit`函数进行拟合。然后，我们按照正常流程对原始数据集进行预测：
- en: '[PRE15]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The resulting precision (`26.53%`) and recall (`65.00%`) values did not vary
    much compared to when we used the algorithm for outlier detection. In the end,
    the choice in terms of novelty detection versus the outlier detection approach
    is a tactical one. It depends on the available data when the model is built and
    whether it contains outliers.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的精确度（`26.53%`）和召回率（`65.00%`）与我们使用该算法进行离群点检测时差异不大。最终，关于新颖性检测和离群点检测方法的选择是一个策略性的问题。它取决于模型建立时可用的数据，以及这些数据是否包含离群点。
- en: You probably already know by now that I like using the ensemble methods, and
    so it is hard for me to end this chapter without presenting an ensemble algorithm
    for outlier detection. In the next section, we are going to look at the **isola****tion****forest**
    algorithm.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经知道，我喜欢使用集成方法，所以我很难在没有介绍一个集成算法来进行离群点检测的情况下结束这一章。在下一节中，我们将讨论**隔离森林**（**isolation
    forest**）算法。
- en: Detecting outliers using isolation forest
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用隔离森林检测离群点
- en: In previous approaches, we started by defining what normal is, and then considered
    anything that doesn't conform to this as outliers. The isolation forest algorithm
    follows a different approach. Since the outliers are few and different, they are
    easier to isolate from the rest. So, when building a forest of random trees, a
    sample that ends in leaf nodes early in a tree—that is, it did not need a lot
    of branching effort to be isolated—is more likely to be an outlier.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的方法中，我们首先定义什么是正常的，然后将任何不符合此标准的样本视为离群点。隔离森林算法采用了不同的方法。由于离群点数量较少且与其他样本差异较大，因此它们更容易从其余样本中隔离出来。因此，当构建随机树森林时，在树的叶节点较早结束的样本——也就是说，它不需要太多分支就能被隔离——更可能是离群点。
- en: 'As a tree-based ensemble, this algorithm shares many hyperparameters with its
    counterparts, such as the number of random trees to build (`n_estimators`), the
    ratio of samples to use when building each tree (`max_samples`), the ratio of
    features to consider when building each tree (`max_features`), and whether to
    sample with a replacement or not (`bootstrap`). You can also build the trees in
    parallel using all the available CPUs on your machine by setting `n_jobs` to `-1`.
    Here, we will build an isolation forest algorithm of 200 trees, then use it to
    predict the outliers in our dataset. Like all the other algorithms in this chapter,
    a prediction of `-1` means that the sample is seen as an outlier:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种基于树的集成算法，这个算法与其对手共享许多超参数，比如构建随机树的数量（`n_estimators`）、构建每棵树时使用的样本比例（`max_samples`）、构建每棵树时考虑的特征比例（`max_features`）以及是否进行有放回抽样（`bootstrap`）。你还可以通过将
    `n_jobs` 设置为 `-1`，利用机器上所有可用的 CPU 并行构建树。在这里，我们将构建一个包含 200 棵树的隔离森林算法，然后用它来预测数据集中的异常值。像本章中的所有其他算法一样，`-1`
    的预测结果表示该样本被视为异常值：
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The resulting precision (`6.5%`) and recall (`60.0%`) values are not as good
    as the previous approaches. Clearly, LOF is the most suitable algorithm for the
    data we have at hand here. We were able to compare the three algorithms since
    the original labels were available to us. In reality, labels are usually unavailable,
    and it is hard to decide which algorithm to use. The field of unlabeled anomaly
    detection evaluation is actively being researched, and I hope to see scikit-learn
    implement reliable evaluation metrics once they are available in the future.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 得到的精度（`6.5%`）和召回率（`60.0%`）值不如之前的方法。显然，LOF 是最适合我们手头数据的算法。由于原始标签可用，我们能够对比这三种算法。实际上，标签通常是不可用的，决定使用哪种算法也变得困难。无标签异常检测评估的领域正在积极研究中，我希望在未来能够看到
    scikit-learn 实现可靠的评估指标。
- en: In the case of supervised learning, you can use true labels to evaluate models
    using the PR curves. When it comes to unlabeled data, recent researchers are trying
    to tailor evaluation criteria, such as the **Excess-Mass** (**EM**) and **Mass-Volume**
    (**MV**) curves.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习的情况下，你可以使用真实标签通过 PR 曲线来评估模型。对于无标签数据，最近的研究者们正在尝试量身定制评估标准，比如**超质量**（**Excess-Mass**，**EM**）和**质量体积**（**Mass-Volume**，**MV**）曲线。
- en: Summary
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: So far in this book, we have used supervised learning algorithms to spot anomalous
    samples. This chapter offered additional solutions when no labels are provided.
    The solutions explained here stem from different fields of machine learning, such
    as statistical learning, nearest-neighbor, and tree-based ensembles. Each one
    of the three tools explained here can excel, but also have disadvantages. We also
    learned that evaluating machine learning algorithms when no labels are provided
    is tricky.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本书中我们使用了监督学习算法来识别异常样本。本章提供了当没有标签时的额外解决方案。这里解释的解决方案源自机器学习的不同领域，如统计学习、最近邻和基于树的集成方法。每种方法都可以表现出色，但也有缺点。我们还学到了，当没有标签时，评估机器学习算法是很棘手的。
- en: This chapter will deal with unlabeled data. In the previous chapter, we learned
    how to cluster data, and then we learned how to detect the outliers in it here.
    We still have one more unsupervised learning topic to discuss in this book, though.
    In the next chapter, we will cover an important topic relating to e-commerce—recommendation
    engines. Since it is the last chapter of this book, I'd alsolike to go through
    the possible approaches to machine learning model deployment. We will learn how
    to save and load our models and how to deploy them on **Application Programming
    Interfaces** (**APIs**).****
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将处理无标签数据。在上一章中，我们学习了如何聚类数据，接着在这一章我们学习了如何检测其中的异常值。然而，这本书里我们还有一个无监督学习的话题要讨论。下一章我们将讨论与电子商务相关的重要话题——推荐引擎。因为这是本书的最后一章，我还想讨论机器学习模型部署的可能方法。我们将学习如何保存和加载我们的模型，并如何将其部署到**应用程序接口**（**APIs**）上。
