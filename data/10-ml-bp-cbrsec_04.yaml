- en: '3'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '3'
- en: Malware Detection Using Transformers and BERT
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Transformer和Bert进行恶意软件检测
- en: Malware refers to malicious software applications that run on computers, smartphones,
    and other devices for nefarious purposes. They execute surreptitiously in the
    background, and often, users are not even aware that their device is infected
    with malware. They can be used to steal sensitive user information (such as passwords
    or banking information) and share it with an adversary, use your device resources
    for cryptocurrency mining or click fraud, or corrupt your data (such as deleting
    photos and emails) and ask for a ransom to recover it. In the 21st century, where
    smartphones are our lifeline, malware can have catastrophic effects. Learning
    how to identify, detect, and remove malware is an important and emerging problem
    in cybersecurity.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件是指运行在计算机、智能手机和其他设备上，用于邪恶目的的恶意软件应用。它们在后台秘密执行，通常用户甚至不知道他们的设备已被恶意软件感染。它们可以用来窃取敏感用户信息（如密码或银行信息）并与对手分享，使用您的设备资源进行加密货币挖掘或点击欺诈，或者破坏您的数据（如删除照片和电子邮件）并索要赎金以恢复它们。在21世纪，智能手机是我们的生命线，恶意软件可能产生灾难性的影响。学习如何识别、检测和删除恶意软件是网络安全中的一个重要且新兴的问题。
- en: Because of its ability to identify and learn patterns in behavior, machine learning
    techniques have been applied to detect malware. This chapter will begin with an
    overview of malware including its life cycle and operating characteristics. We
    will then cover an upcoming and state-of-the-art architecture, known as the **transformer**,
    which is typically used for **natural language processing** (**NLP**) applications.
    Finally, we will combine the two and show how we can build an extremely high-precision
    malware classifier using BERT, which is a model built on top of the transformer
    architecture.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于其识别和学习行为模式的能力，机器学习技术已被应用于检测恶意软件。本章将从恶意软件的概述开始，包括其生命周期和运行特性。然后，我们将介绍一种即将到来且处于最前沿的架构，称为**transformer**，它通常用于**自然语言处理**（NLP）应用。最后，我们将结合两者，展示如何使用BERT构建一个极高精度的恶意软件分类器，BERT是一个建立在Transformer架构之上的模型。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Basics of malware
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件的基础知识
- en: Transformers and attention
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Transformer和注意力机制
- en: Detecting malware with BERT
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Bert检测恶意软件
- en: By the end of this chapter, you will have a better understanding of how malware
    works. Most importantly, you will be able to apply transformers and BERT to a
    variety of security-related classification problems based on the concepts we will
    study here.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将更好地理解恶意软件的工作原理。最重要的是，您将能够根据我们在这里学习到的概念，将Transformer和Bert应用于各种与安全相关的分类问题。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%203](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%203).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在GitHub上找到本章的代码文件：[https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%203](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%203)。
- en: Basics of malware
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恶意软件的基础知识
- en: Before we learn about *detecting* malware, let us briefly understand what exactly
    malware is and how it works.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们学习如何**检测**恶意软件之前，让我们简要了解恶意软件究竟是什么以及它是如何工作的。
- en: What is malware?
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是恶意软件？
- en: Malware is simply any *mal*icious soft*ware*. It will install itself on your
    device (such as a computer, tablet, or smartphone) and operate in the background,
    often without your knowledge. It is designed to quietly change files on your device,
    and thus steal or corrupt sensitive information. Malware is generally camouflaged
    and pretends to be an otherwise innocent application. For example, a browser extension
    that offers free emojis can actually be malware that is secretly reading your
    passwords and siphoning them off to a third party.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件简单地指任何*恶意*软件。它将自行安装在您的设备上（如计算机、平板电脑或智能手机）并在后台运行，通常在您不知情的情况下。它被设计为在您的设备上悄悄更改文件，从而窃取或破坏敏感信息。恶意软件通常伪装成其他无恶意的应用程序。例如，一个提供免费表情符号的浏览器扩展实际上可能是一种恶意软件，它秘密读取您的密码并将它们偷走传递给第三方。
- en: 'Devices can be infected by malware in multiple ways. Here are some of the popular
    vectors attackers exploit to deliver malware to a user device:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 设备可以通过多种方式被恶意软件感染。以下是攻击者用来将恶意软件传递给用户设备的一些流行向量：
- en: Leveraging the premise of “free” software, such as a cracked version of expensive
    software such as Adobe Photoshop
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用“免费”软件的前提，例如破解版本的昂贵软件，如Adobe Photoshop
- en: USB devices with the malware installed plugged into the user’s computer
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了恶意软件的USB设备插入到用户的计算机中
- en: Phishing emails where attackers pretend to be the employer or IT support and
    ask to download and install a malicious software
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欺诈电子邮件，攻击者假装是雇主或IT支持，要求下载和安装恶意软件
- en: Websites that prompt users to install malicious extensions to continue
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提示用户安装恶意扩展以继续的网站
- en: 'An example ([https://www.myantispyware.com/2017/02/20/remove-to-continue-the-work-of-your-browser-you-should-install-the-extension-pop-ups/](https://www.myantispyware.com/2017/02/20/remove-to-continue-the-work-of-your-browser-you-should-install-the-extension-pop-ups/))
    of a website prompting users to install an extension is shown here:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了提示用户安装扩展的网站的一个例子（[https://www.myantispyware.com/2017/02/20/remove-to-continue-the-work-of-your-browser-you-should-install-the-extension-pop-ups/](https://www.myantispyware.com/2017/02/20/remove-to-continue-the-work-of-your-browser-you-should-install-the-extension-pop-ups/)）：
- en: '![Figure 3.1 – A suspicious website prompting malware downloads](img/B19327_03_01.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图3.1 – 一个提示下载恶意软件的可疑网站](img/B19327_03_01.jpg)'
- en: Figure 3.1 – A suspicious website prompting malware downloads
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 – 一个提示下载恶意软件的可疑网站
- en: Malware applications come in multiple forms and flavors, each with a different
    attack strategy. In the next subsection, we will study some popular variants of
    malware.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件应用有多种形式和口味，每种都有不同的攻击策略。在下一小节中，我们将研究一些流行的恶意软件变体。
- en: Types of malware
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恶意软件类型
- en: Now, let us briefly take a look at the various kinds of malware.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们简要地看看各种恶意软件。
- en: Virus
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 病毒
- en: A virus is a malicious software application that functions in a manner similar
    to its biological counterpart – an actual virus. A virus program is one that replicates
    by creating multiple copies of itself and hogs all system resources. Viruses hide
    in computer files, and once the file is run, they can begin replicating and spreading.
    Viruses can be boot infectors (which target the operating system directly and
    install them as part of the booting process) or file infectors (those which are
    hidden away in executable files, such as free versions of software downloaded
    from shady websites). Some applications also allow third-party extensions to interface
    with them. Examples include macros or extensions. Viruses can also run as part
    of these macros.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 病毒是一种恶意软件应用，其功能类似于其生物对应物——真正的病毒。病毒程序是一种通过创建自身多个副本并占用所有系统资源来复制的程序。病毒隐藏在计算机文件中，一旦文件被运行，它们就可以开始复制和传播。病毒可以是引导型感染（直接针对操作系统并作为引导过程的一部分安装）或文件型感染（那些隐藏在可执行文件中，如从可疑网站下载的软件免费版本）。一些应用还允许第三方扩展与之交互。例如，宏或扩展。病毒也可以作为这些宏的一部分运行。
- en: Worms
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 蠕虫
- en: Worms are similar to viruses in operation in terms of their modus operandi,
    which is to replicate and spread. However, worms are standalone applications;
    they are not embedded into files like viruses are. While viruses require users
    to execute the file (such as a `.exe` file or a macro), worms are more dangerous
    because they can execute by themselves. Once they infect a computer, they can
    automatically replicate across the entire network. Worms generally crash the device
    or overload the network by increasing resource usage.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 蠕虫在操作方式上与病毒相似，其操作模式是复制和传播。然而，蠕虫是独立的应用程序；它们不像病毒那样嵌入到文件中。虽然病毒需要用户执行文件（如`.exe`文件或宏），但蠕虫更危险，因为它们可以自行执行。一旦它们感染了计算机，它们就可以在整个网络上自动复制。蠕虫通常通过增加资源使用量来使设备崩溃或网络过载。
- en: Rootkits
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 根套件
- en: Rootkits are malware applications that work with the goal of getting the attacker
    complete administrative rights to your system (the term **root** refers to the
    administrator or master user in operating systems, and is a user account that
    can control permissions and other user accounts). A rootkit can allow an adversary
    to have full control of the user’s computer without the user knowing. This means
    that the attacker can read and write to all files, execute malicious applications,
    and lock legitimate users out of the system. The attacker can also execute illegal
    activities, such as launching a DDoS attack, and avoid being caught (as it is
    the user’s machine to which the crime will be traced back). While malicious for
    the most part, some rootkits are also used for good. For example, if a system
    that contains highly sensitive data (such as information pertaining to national
    security) is accessed by an adversary, a rootkit can be used as a **backdoor**
    (a secret or hidden entry point into a system, and one that can be used to bypass
    security mechanisms and gain unauthorized access to a system or data) to access
    it and wipe it off to prevent the information from falling into the wrong hands.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 根套件是一种恶意软件应用程序，其工作目标是让攻击者获得对您系统的完整管理权限（术语**root**指的是操作系统中的管理员或主用户，这是一个可以控制权限和其他用户账户的用户账户）。根套件可以使攻击者完全控制用户的计算机，而用户却毫不知情。这意味着攻击者可以读取和写入所有文件，执行恶意应用程序，并将合法用户锁定在系统之外。攻击者还可以执行非法活动，例如发起DDoS攻击，并避免被抓住（因为犯罪将被追溯到用户的机器）。虽然大多数情况下是恶意的，但一些根套件也被用于好的目的。例如，如果一个包含高度敏感数据（如涉及国家安全的情报）的系统被攻击者访问，可以使用根套件作为**后门**（一个秘密或隐藏的系统入口点，可以用来绕过安全机制并获取对系统或数据的未授权访问）来访问它并将其擦除，以防止信息落入错误的手中。
- en: Ransomware
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 勒索软件
- en: Ransomware are malware applications that block user access to data. Ransomware
    may encrypt the data so that users are powerless to do anything with their devices.
    Attackers ask for a *ransom* to be paid, and threaten that the data will be deleted
    forever or published on the Internet if they do not receive the ransom. There
    is no guarantee that the attacker will actually hold up their end of the bargain
    once the ransom is paid. Ransomware attacks have been on the rise, and the emergence
    of cryptocurrency (such as BTC) has made it possible for attackers to receive
    and spend money pseudo-anonymously.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 勒索软件是阻止用户访问数据的恶意软件应用程序。勒索软件可能会加密数据，使得用户无法对其设备进行任何操作。攻击者要求支付赎金，并威胁如果不支付赎金，数据将被永久删除或发布到互联网上。支付赎金后，攻击者实际上是否履行承诺没有保证。勒索软件攻击呈上升趋势，加密货币（如BTC）的出现使得攻击者可以匿名地接收和花费金钱。
- en: Keyloggers
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 键盘记录器
- en: A keylogger is an application that records the keyboard activities of a user.
    All information typed is logged as keystrokes and siphoned off to an attacker.
    The attacker can extract information such as usernames, passwords, credit card
    numbers, and secure PINs. As keyloggers do not cause any visible harm (such as
    deleting or locking files), they are hard to detect from a user’s perspective.
    They sit quietly in the background and ship your keystroke information to the
    attacker.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 键盘记录器是一种记录用户键盘活动的应用程序。所有输入的信息都被记录为按键并传输给攻击者。攻击者可以提取诸如用户名、密码、信用卡号码和安全的PIN码等信息。由于键盘记录器不会造成任何明显的损害（如删除或锁定文件），因此从用户的角度来看很难检测到。它们安静地存在于后台，并将您的按键信息发送给攻击者。
- en: Now that we have explored the different kinds of malware, let us turn to how
    malware detectors work.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经探讨了不同种类的恶意软件，让我们转向了解恶意软件检测器是如何工作的。
- en: Malware detection
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 恶意软件检测
- en: As the prevalence of malware grows, so does the need for detecting it. Routine
    system scans and analysis by malware detection algorithms can help users stay
    safe and keep their systems clean.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 随着恶意软件的普遍存在，检测恶意软件的需求也在增加。常规的系统扫描和恶意软件检测算法的分析可以帮助用户保持安全并保持系统清洁。
- en: Malware detection methods
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恶意软件检测方法
- en: 'Malware detection can be divided broadly into three main categories: signature-based,
    behavioral-based, and heuristic methods. In this section, we will look at what
    these methods are in short and also discuss techniques for analysis.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件检测可以大致分为三大类：基于签名、基于行为和启发式方法。在本节中，我们将简要介绍这些方法，并讨论分析技术。
- en: Signature-based methods
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于签名的检测方法
- en: These methods aim to detect malware by storing a database of known malware examples.
    All applications are checked against this database to identify whether they are
    malicious. The algorithm examines each application and calculates a signature
    using a hash function. In computer security, the hash of a file can be treated
    as its unique identity. It is nearly impossible to have two files with the same
    hash unless they are identical. Therefore, this method works really well in detecting
    known malware. While the simplicity of this technique is unmatched, it is easily
    thwarted; a change of even a single bit in the executable file will cause the
    hash to be completely different, and undetectable by its signature.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法旨在通过存储已知恶意软件示例的数据库来检测恶意软件。所有应用程序都会与这个数据库进行比对，以确定它们是否恶意。算法检查每个应用程序，并使用哈希函数计算一个签名。在计算机安全中，文件的哈希值可以被视为其唯一标识。除非两个文件完全相同，否则几乎不可能有两个相同的哈希值。因此，这种方法在检测已知恶意软件方面非常有效。尽管这种技术的简单性无与伦比，但它很容易被挫败；可执行文件中即使只改变一个比特，也会导致哈希值完全不同，从而无法通过其签名检测到。
- en: Behavioral-based methods
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基于行为的方法
- en: 'These methods aim to detect malware by looking for evidence of certain malicious
    activity. Signature-based methods detect malware based on what the application
    says, but behavioral methods detect it based on what the application does. It
    can collect a variety of features from the behavior of the application, such as:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法旨在通过寻找某些恶意活动的证据来检测恶意软件。基于签名的检测方法根据应用程序所说的内容来检测恶意软件，但基于行为的方法则是根据应用程序的行为来检测。它可以从应用程序的行为中收集各种特征，例如：
- en: How many GET requests did the app make?
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序发出了多少个GET请求？
- en: How many suspicious URLs did it connect to?
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它连接了多少个可疑的URL？
- en: Does the application have access to file storage?
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序是否有文件存储的访问权限？
- en: How many distinct IP addresses did the application contact in the past seven
    days?
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在过去七天中，应用程序联系了多少个不同的IP地址？
- en: Using these features, common-sense rules can be built to flag malicious behavior.
    Past examples of known malware are also studied in detail to identify strategies
    that can be checked for. Behavioral methods are more robust against evasion, as
    an adversary will have to explicitly change the behavior of an app to avoid detection.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些功能，可以构建常识性规则来标记恶意行为。已知恶意软件的过去例子也被详细研究，以识别可以检查的策略。基于行为的方法对规避攻击更具鲁棒性，因为攻击者必须明确改变应用程序的行为以避免检测。
- en: Heuristic methods
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 启发式方法
- en: These are the most powerful methods known to us. Rather than look for a specific
    behavior, they use data mining and machine learning models to learn what malicious
    applications look like. These methods leverage API calls, OpCode Sequences, call
    graphs, and other features from the application and train a classification model.
    Neural networks and Random Forests have been shown to achieve a high-accuracy
    and high-precision classifier for malware. Heuristic methods are even more robust
    than behavioral methods, as changing specific parameters may not necessarily fool
    the model.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们已知的最强大的方法。它们不是寻找特定的行为，而是使用数据挖掘和机器学习模型来学习恶意应用程序的外观。这些方法利用应用程序的API调用、OpCode序列、调用图和其他特征来训练分类模型。神经网络和随机森林已被证明能够实现高准确性和高精度的恶意软件分类器。启发式方法甚至比基于行为的方法更鲁棒，因为改变特定参数不一定能欺骗模型。
- en: Malware analysis
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恶意软件分析
- en: 'In the previous section, we discussed malware detection methods. Once a potential
    malware application has been flagged, it needs to be examined to identify its
    behavior, method of spreading, origin, and any potential impact. Researchers often
    dissect malware as it can provide insights into the skills and tactics available
    to an adversary. This process of examining a malware file in detail is known as
    malware analysis. There are two methods for malware analysis: static and dynamic.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了恶意软件检测方法。一旦标记出潜在的恶意软件应用程序，就需要对其进行检查，以确定其行为、传播方式、来源以及任何潜在的影响。研究人员经常剖析恶意软件，因为它可以提供有关攻击者可用技能和策略的见解。详细检查恶意软件文件的过程被称为恶意软件分析。恶意软件分析有两种方法：静态和动态。
- en: Static analysis
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 静态分析
- en: This method examines the malware file as a whole by collecting information about
    the application without actually running it. The hash of the application is checked
    against known malware samples. The executable file is decompiled and the code
    is analyzed in detail; this provides a deep insight into what the goal of the
    malware was and what the adversary was looking for. Common patterns in the code
    may also indicate the origin or developer of the malware. Any strategies found
    can now be used to develop stronger detection mechanisms.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过收集有关应用程序的信息而不实际运行它来检查恶意软件文件。检查应用程序的哈希值是否与已知的恶意软件样本匹配。可执行文件被反编译，代码被详细分析；这提供了对恶意软件目标以及攻击者所寻找内容的深入了解。代码中的常见模式也可能表明恶意软件的来源或开发者。现在可以找到的策略现在可以用来开发更强大的检测机制。
- en: Dynamic analysis
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动态分析
- en: Dynamic analysis involves studying malware by actually executing it. A protected
    sandbox environment is created, and the malware is allowed to execute in it. This
    allows researchers the opportunity to look at the malware in action. Some behavior
    may not be obvious in the code or may dynamically evolve at runtime. Such behavior
    can be observed when the malware is actually running. Moreover, allowing the application
    to run allows you to collect API call sequences and other behavioral features,
    which can be used for heuristic methods.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 动态分析涉及通过实际执行恶意软件来研究恶意软件。创建一个受保护的沙盒环境，并允许恶意软件在其中执行。这使研究人员有机会观察恶意软件的实际运行情况。某些行为可能在代码中不明显，或可能在运行时动态演变。当恶意软件实际运行时，可以观察到这种行为。此外，允许应用程序运行可以收集
    API 调用序列和其他行为特征，这些特征可用于启发式方法。
- en: It is important to note that handling malware can be a dangerous task. Inadvertently
    running it may cause the virus or Trojan to take control of your system. There
    are several commercial tools that facilitate malware analysis in a secure way.
    In later sections, we will be using files generated by one such tool.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '重要的是要注意，处理恶意软件可能是一项危险的任务。无意中运行它可能导致病毒或特洛伊木马控制您的系统。有几种商业工具可以以安全的方式促进恶意软件分析。在后面的章节中，我们将使用此类工具生成的文件。 '
- en: Transformers and attention
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 变换器和注意力
- en: Transformers are an architecture taking the machine learning world by storm,
    especially in the fields of natural language processing. An improvement over classical
    **recurrent neural networks** (**RNN**) for sequence modeling, transformers work
    on the principle of attention. In this section, we will discuss the attention
    mechanism, transformers, and the BERT architecture.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 变换器是一种席卷机器学习世界的架构，尤其是在自然语言处理领域。它是经典 **循环神经网络**（**RNN**）在序列建模方面的改进，基于注意力的原理工作。在本节中，我们将讨论注意力机制、变换器和
    BERT 架构。
- en: Understanding attention
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解注意力
- en: We will now take a look at *attention*, a recent deep learning paradigm that
    has made great advances in the world of natural language processing.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探讨 *注意力*，这是一种最近在自然语言处理领域取得重大进展的深度学习范式。
- en: Sequence-to-sequence models
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 序列到序列模型
- en: Most natural language tasks rely heavily on sequence-to-sequence models. While
    traditional methods are used for classifying a particular data point, sequence-to-sequence
    architectures map sequences in one domain to sequences in another. An excellent
    example of this is language translation. An automatic machine translator will
    take in sequences of tokens (sentences and words) from the source language and
    map them to other sentences in the target language.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数自然语言任务都严重依赖于序列到序列模型。虽然传统方法用于对特定数据点进行分类，但序列到序列架构将一个域中的序列映射到另一个域中的序列。语言翻译是这种应用的绝佳例子。自动机器翻译器将接收源语言的标记序列（句子和单词），并将它们映射到目标语言的句子。
- en: 'A sequence-to-sequence model generally has two components: the encoder and
    the decoder. The encoder takes in the source sequences as input and maps them
    to an intermediate vector known as a **context vector**, or embedding. The decoder
    takes in the embedding and maps it to sequences in the target domain. The entire
    model is trained end to end instead of encoders and decoders being trained separately
    as shown in *Figure 3**.2*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 序列到序列模型通常有两个组件：编码器和解码器。编码器接收源序列作为输入，并将它们映射到一个称为 **上下文向量**或嵌入的中间向量。解码器接收嵌入并将其映射到目标域的序列。整个模型是端到端训练的，而不是像
    *图 3.2* 所示的那样分别训练编码器和解码器：
- en: '![Figure 3.2 – Traditional sequence-to-sequence architecture](img/B19327_03_02.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图 3.2 – 传统序列到序列架构](img/B19327_03_02.jpg)'
- en: Figure 3.2 – Traditional sequence-to-sequence architecture
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 – 传统的序列到序列架构
- en: The encoder and decoder are typically RNNs, which maintain an internal state
    (hidden state) that has some memory of past inputs. In a traditional sequence-to-sequence
    model, the context vector would simply be a high-dimensional representation of
    the input sentence in a vector space. In *Figure 3**.2*, the words from the French
    sentence are passed one by one to the model (one every time step). The encoder
    contains an RNN that maintains some memory at every time step. After the final
    time step, the hidden state of the RNN becomes the context vector.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器通常是RNN，它们保持一个内部状态（隐藏状态），该状态对过去的输入有一些记忆。在传统的序列到序列模型中，上下文向量将简单地是输入句子在向量空间中的高维表示。在*图3*.*2*中，从法语句子中的单词逐个传递到模型中（每次一个时间步）。编码器包含一个RNN，它在每个时间步保持一些记忆。在最后一个时间步之后，RNN的隐藏状态成为上下文向量。
- en: 'This is similar to autoencoders discussed previously, except for one major
    difference: in sequence-to-sequence models, the input and output sequences can
    be of different lengths. This is often the case in language translation. For example,
    the French sentence “Ca va?” translates to “How are you?” in English. Sequence-to-sequence
    models are powerful because they learn the relationships and order between tokens
    and map them to tokens in the target language.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这与之前讨论的自动编码器类似，但有一个主要区别：在序列到序列模型中，输入和输出序列可以是不同长度的。这在语言翻译中通常是这种情况。例如，法语句子“Ca
    va?”翻译成英语是“你好吗？”。序列到序列模型之所以强大，是因为它们学习标记之间的关系和顺序，并将它们映射到目标语言中的标记。
- en: Attention
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力机制
- en: The key challenge in the standard encoder/decoder architecture is the bottleneck
    created by the context vector. Being a fixed-size vector, there is a limitation
    on how much information can be compressed into it. As a result, it cannot retain
    information from longer sequences and cannot capture information from multiple
    timesteps that the RNN encoder goes through. RNNs have a tendency to forget the
    information they learn. In longer sequences, they will remember the later parts
    of the sequence and start forgetting earlier ones.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的编码器/解码器架构中，关键挑战是由上下文向量造成的瓶颈。作为一个固定大小的向量，它对可以压缩进其中的信息量有限制。因此，它无法保留较长序列中的信息，也无法捕捉到RNN编码器经过的多个时间步长的信息。RNN有遗忘它们所学习信息的倾向。在较长的序列中，它们会记住序列的后期部分，并开始忘记早期部分。
- en: The attention mechanism aims to solve the problem of long-term dependencies
    and allow the decoder to access as much information as it needs in order to decode
    the sequence properly. Via attention, the decoder focuses only on the relevant
    parts of the input sequence in order to produce the output sequence. The model
    examines multiple time steps from the encoder and "pays attention" to only the
    ones that it deems to be important.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力机制旨在解决长期依赖问题，并允许解码器访问它解码序列所需的所有信息。通过注意力，解码器只关注输入序列的相关部分，以产生输出序列。模型检查编码器的多个时间步，并“关注”它认为重要的那些。
- en: Concretely speaking, while a traditional sequence-to-sequence model would just
    pass the last hidden state of the RNN to the decoder, an attention model will
    pass all of the hidden states. For example, in an English-French translation model,
    input sentences are English sentences. A hidden state will be created at every
    word position as the RNN encoder steps through the sequence, and all of these
    will be passed to the decoder.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，虽然传统的序列到序列模型只会将RNN的最后一个隐藏状态传递给解码器，但注意力模型会传递所有隐藏状态。例如，在一个英语-法语翻译模型中，输入句子是英语句子。每当RNN编码器遍历序列时，就会在每个单词位置创建一个隐藏状态，所有这些都会传递给解码器。
- en: The decoder now has access to the context vector at each time step in the input.
    While decoding, it will try to focus on the parts of the input that are meaningful
    to decoding this time step. It will examine the encoder’s hidden states (remember,
    all of the hidden states have been passed) and score each hidden state. The score
    represents the relevance of that hidden state to the current word being decoded;
    the higher the score, the greater the relevance. The score for each state is normalized
    using a `softmax` function overall scores. Finally, each hidden state is multiplied
    by the `softmax` transformed score. Hidden states with high scores (relevant to
    decoding at this time step) are amplified in value, whereas the ones with low
    scores are diminished. Using the values of these vectors, the decoded output word
    can be produced.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器现在可以在输入的每个时间步访问上下文向量。在解码过程中，它将尝试关注对解码这个时间步有意义的输入部分。它将检查编码器的隐藏状态（记住，所有隐藏状态都已传递）并对每个隐藏状态进行评分。评分表示该隐藏状态对当前解码单词的相关性；评分越高，相关性越大。每个状态的评分都使用`softmax`函数进行归一化处理。最后，每个隐藏状态都乘以`softmax`变换后的评分。评分高的隐藏状态（与当前时间步的解码相关）在数值上被放大，而评分低的隐藏状态则被减弱。利用这些向量的值，可以生成解码输出的单词。
- en: Attention in action
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意力在行动
- en: We discussed that the attention decoder is able to selectively pay attention
    to the relevant words in the source sequence. To demonstrate that the model does
    not mindlessly do a word-by-word translation, we show an example here from the
    paper that first presented the idea of attention ([https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了注意力解码器能够选择性地关注源序列中的相关单词。为了证明模型不会无意识地逐词翻译，我们在这里展示了一个例子，该例子首次提出了注意力的想法（[https://arxiv.org/abs/1409.0473](https://arxiv.org/abs/1409.0473))。
- en: Consider the problem of translating French sentences into English. This is the
    perfect domain in which to demonstrate attention. The French language has a peculiar
    ordering of the parts of speech (adverbs, adjectives, and nouns) that is different
    from English. If a model is doing a word-by-word translation without attention,
    the translated output would be grammatically incorrect.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将法语句子翻译成英语的问题。这是演示注意力的完美领域。法语语言中词素的顺序（副词、形容词和名词）与英语不同。如果一个模型在翻译时不考虑注意力，翻译后的输出将是语法错误的。
- en: 'Here is a confusion matrix that demonstrates the attention that the model paid
    to specific tokens in the input to generate specific tokens in the output. The
    brighter the color, the stronger the attention:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个混淆矩阵，展示了模型在生成输出中的特定标记时对输入中特定标记的关注。颜色越亮，关注越强：
- en: '![Figure 3.3 – Confusion matrix denoting attention](img/B19327_03_03.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图3.3 – 表示注意力的混淆矩阵](img/B19327_03_03.jpg)'
- en: Figure 3.3 – Confusion matrix denoting attention
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 – 表示注意力的混淆矩阵
- en: In French, the adjective is generally placed after the noun. So the “European
    Economic Zone” becomes the “Zone économique européenne.” Looking at the confusion
    matrix, note how the model has paid attention to the correct word pairs, irrespective
    of their order. If a model was simply mapping words, the sentence would have been
    translated from the French version to “Area Economic European.” The confusion
    matrix shows that irrespective of the order, the model knew what words to pay
    attention to while decoding certain time steps.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在法语中，形容词通常放在名词之后。因此，“欧洲经济区”变为“Zone économique européenne”。查看混淆矩阵，注意模型如何关注正确的单词对，而不考虑它们的顺序。如果模型只是映射单词，句子将被从法语版本翻译为“Area
    Economic European”。混淆矩阵显示，无论顺序如何，模型在解码某些时间步时都知道应该关注哪些单词。
- en: This is the fundamental concept of attention. The actual mechanisms (how the
    hidden states are scored, and how the feature vector is constructed) are out of
    scope here. However, those of you who are interested can refer to the foundational
    paper behind attention for a detailed description of the mechanism.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是注意力的基本概念。实际的机制（如何评分隐藏状态以及如何构建特征向量）在这里不涉及。然而，对那些感兴趣的人来说，可以参考注意力背后的基础论文，以获得对机制的详细描述。
- en: Understanding transformers
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解Transformer
- en: 'In the previous section, we discussed the attention mechanism and how it helps
    sequence-to-sequence applications such as neural machine translation. Now, we
    will look at the transformer: an architecture that leverages attention in multiple
    forms and stages to get the best out of it.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了注意力机制以及它是如何帮助序列到序列应用，例如神经机器翻译的。现在，我们将探讨变换器：一种利用多种形式和阶段的注意力以获得最佳效果的架构。
- en: 'The fundamental architecture of the transformer model is reproduced in *Figure
    3**.4* from a 2017 paper ([https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf))
    for convenience:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，变换器模型的基本架构在2017年的一篇论文中（*Figure 3**.4*）重现了（[https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)）：
- en: '![Figure 3.4 – Transformer architecture](img/B19327_03_04.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图3.4 – 变换器架构](img/B19327_03_04.jpg)'
- en: Figure 3.4 – Transformer architecture
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 – 变换器架构
- en: 'The model has two components: the encoder (depicted by the blocks on the left)
    and the decoder (depicted by the blocks on the right). The goal of the blocks
    on the left is to take in the input sequence and transform it into the context
    vectors that are fed to the decoder. The decoder blocks on the right receive the
    output of the encoder along with the output of the decoder at the previous time
    step to generate the output sequence. Let us now look at the encoder and decoder
    blocks in more detail.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型有两个组件：编码器（由左侧的块表示）和解码器（由右侧的块表示）。左侧块的目的是接收输入序列并将其转换为解码器输入的上下文向量。右侧的解码器块接收编码器的输出以及前一时间步的解码器输出以生成输出序列。现在让我们更详细地看看编码器和解码器块。
- en: The encoder
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 编码器
- en: 'The encoder consists of two modules: a multi-headed attention module and a
    fully connected feed-forward neural network. The multi-headed attention module
    will apply a technique called self-attention, which allows the model to associate
    each word in the input with other words. For example, consider the sentence, *I
    could not drink the soup because it was too hot*. Here, the word *it* in the latter
    half of the sentence refers to the word *soup* in the first half. Self-attention
    would be able to discover such relationships. As the name suggests, multi-headed
    attention includes multiple blocks (or heads) for attention. The expectation is
    that each head will learn a different relationship. The multi-headed attention
    module calculates the attention weights for the input sequence and generates a
    vector as output that indicates how each word in the sequence should *pay attention*
    to the others.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器由两个模块组成：一个多头注意力模块和一个全连接前馈神经网络。多头注意力模块将应用一种称为自注意力的技术，这使得模型能够将输入中的每个词与其他词关联起来。例如，考虑这个句子，“我无法喝汤，因为它太热了”。在这里，句子后半部分的词“它”指的是前半部分的词“汤”。自注意力能够发现这样的关系。正如其名所示，多头注意力包括多个块（或头）用于注意力。预期每个头将学习不同的关系。多头注意力模块计算输入序列的注意力权重，并生成一个向量作为输出，指示序列中的每个词应该如何*关注*其他词。
- en: The output of the attention module is added back to the input and then passed
    through a normalization layer. Normalization helps control the range of parameters
    and keeps the model stable. The normalized output is passed to the second module,
    which is a feed-forward neural network. This can be any neural network, but generally
    speaking, it consists of multiple fully connected layers with a ReLU activation.
    The addition and normalization process repeats once again, and the final encoder
    output is produced.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 注意力模块的输出被加回到输入中，然后通过一个归一化层。归一化有助于控制参数的范围并保持模型稳定。归一化后的输出传递到第二个模块，这是一个前馈神经网络。这可以是任何神经网络，但一般来说，它由多个全连接层和一个ReLU激活组成。加法和归一化过程再次重复，最终产生编码器的输出。
- en: The decoder
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解码器
- en: The role of the decoder is to take in the encoder’s output and produce an output
    sequence. Note that the general modules remain the same, but the structure differs
    slightly.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的作用是接收编码器的输出并生成一个输出序列。请注意，通用模块保持不变，但结构略有不同。
- en: The decoder has two attention modules. One of them takes in the embedding of
    the previously produced output and applies the attention mechanism to it. This
    module applies *masked attention*. During the training, we will have pairs of
    input and output sequences that the model will learn from. It is important that
    the decoder learns to produce the next output by looking at only past tokens.
    It should not pay attention to the future tokens (otherwise, the whole point of
    developing a predictive model is moot). The masked attention module zeroes out
    the attention weights for the future tokens.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器有两个注意力模块。其中一个模块接收先前产生的输出的嵌入，并对其应用注意力机制。此模块应用*掩码注意力*。在训练过程中，我们将有一对输入和输出序列，模型将从这些序列中学习。解码器学会通过仅查看过去标记来产生下一个输出是很重要的。它不应该关注未来标记（否则，开发预测模型的全局目标就毫无意义）。掩码注意力模块将未来标记的注意力权重置零。
- en: 'The second attention module takes in two inputs: the normalized output of the
    first module and the output from our encoder. The attention module has three inputs,
    known as the *query*, *key*, and *value* vectors. However, we will not go into
    specifics of these.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个注意力模块接收两个输入：第一个模块的标准化输出和编码器的输出。注意力模块有三个输入，称为*查询*、*键*和*值*向量。然而，我们不会深入探讨这些的具体细节。
- en: Finally, note that the decoder has an additional linear layer and a `softmax`
    layer at the end. The goal of the decoder is to produce sequences (mainly text)
    in the target language. Therefore, the embeddings that are generated must be somehow
    mapped to words. The `softmax` layer outputs a probability distribution over the
    vocabulary of tokens. The one with the maximum probability is chosen to be the
    output word.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请注意，解码器在末尾还有一个额外的线性层和一个`softmax`层。解码器的目标是产生目标语言的序列（主要是文本）。因此，生成的嵌入必须以某种方式映射到单词。`softmax`层输出一个关于标记词汇表的概率分布。具有最大概率的标记被选为输出单词。
- en: Understanding BERT
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解BERT
- en: So far, we have seen how the attention mechanism works and how the transformers
    leverage it for effective sequence-to-sequence modeling. As a final step, we will
    learn about BERT, a model that uses transformers and a novel set of training methodologies.
    The effectiveness of BERT and the utility of a pre-trained model in downstream
    tasks will be critical for our malware detection task.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了注意力机制的工作原理以及如何利用它进行有效的序列到序列建模。作为最后一步，我们将学习BERT，这是一个使用transformers和一套新颖的训练方法的模型。BERT的有效性和在下游任务中使用预训练模型的有用性对于我们进行恶意软件检测任务至关重要。
- en: '**BERT** stands for **Bidirectional Encoder Representations from Transformers**.
    Ever since its introduction in 2018, it has made great impacts on the natural
    language processing world. It is a significant discovery that allows researchers
    and scientists to harness the power of large-scale machine learning language models,
    without the need for massive data or extensive computing resources.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '**BERT**代表**来自Transformers的双向编码器表示**。自从2018年引入以来，它对自然语言处理领域产生了巨大影响。这是一个重要的发现，使得研究人员和科学家能够利用大规模机器学习语言模型的力量，而无需大量数据或广泛的计算资源。'
- en: BERT architecture
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: BERT架构
- en: Architecturally speaking, BERT leverages transformers to create a structure.
    The BERT base model has 12 transformers stacked on top of each other and 12 self-attention
    heads. The BERT large model has 24 transformer layers and 16 self-attention heads.
    Both these models are *tremendously large* (with 110M and 40M parameters respectively).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从架构角度来看，BERT利用transformers来创建结构。BERT基础模型由12个堆叠在一起的transformers组成，每个transformers有12个自注意力头。BERT大型模型有24个transformer层和16个自注意力头。这两个模型都*非常大*（分别有1100万和400万个参数）。
- en: Both of these models have been released by Google as open source and are freely
    available for anyone to use.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个模型都已由谷歌作为开源软件发布，任何人都可以免费使用。
- en: MLM as a training task
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MLM作为训练任务
- en: 'Traditional language models examine text sequences in only one direction: from
    left to right or right to left. This approach works just fine for generating sentences.
    The overarching goal in a language model is, given the words that have occurred
    so far, to predict the next word likely to appear.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 传统语言模型只从单一方向检查文本序列：从左到右或从右到左。这种方法对于生成句子来说效果很好。在语言模型中的总体目标是，给定到目前为止出现的单词，预测下一个可能出现的单词。
- en: However, BERT takes this a step further. Instead of looking at the sequence
    either left to right or right to left, it looks at the sequence both ways. It
    is trained on a task known as `[MASK]` token. Now, the goal is not to predict
    the next word in the sentence. The model will now learn to predict the masked
    words, given the surrounding words in both directions.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，BERT更进一步。它不仅从左到右或从右到左查看序列，而是从两个方向都查看序列。它在名为`token`的任务上进行训练。现在，目标不再是预测句子中的下一个单词。模型现在将学会根据两个方向的周围单词预测被掩码的单词。
- en: The word embeddings generated by traditional models represent words in a numeric
    space such that words similar in meaning are close to one another in the vector
    space. However, BERT will generate embeddings for a word depending on the context
    of the word. In such embeddings, the vector representation of a word changes with
    the context in which the word is being used.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 传统模型生成的词嵌入在数值空间中表示单词，使得意义相似的单词在向量空间中彼此靠近。然而，BERT将根据单词的上下文为单词生成嵌入。在这些嵌入中，单词的向量表示会随着单词使用的上下文而变化。
- en: In traditional word embeddings, a word will have the same embedding irrespective
    of the context. The word *match* will have the same embedding in the sentence
    “They were a perfect match” and “I lit a match last night.” We clearly see that
    although the word is the same, the context matters and changes the meaning. BERT
    recognizes this and conditions the embeddings based on context. The word match
    has different embeddings in these two sentences.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统的词嵌入中，一个单词的嵌入将不受上下文的影响。单词“match”在这句话“他们是一个完美的匹配”和“我昨晚点燃了一根火柴”中都有相同的嵌入。我们清楚地看到，尽管单词相同，但上下文很重要，并且会改变意义。BERT认识到这一点，并根据上下文条件化嵌入。在这两个句子中，match单词有不同的嵌入。
- en: Fine-tuning BERT
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调BERT
- en: As mentioned before, the power of BERT lies in fine-tuning. The original BERT
    model has been trained on the masked language model task using the BooksCorpus
    data (containing 800 million words) and the Wikipedia data (containing 2.5 billion
    words). The model learns from large-scale datasets, training that we cannot reproduce
    trivially. For context, the BERT large model required 4 days and 16 cloud TPUs
    for training.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，BERT的力量在于微调。原始BERT模型使用BooksCorpus数据（包含8亿个单词）和维基百科数据（包含25亿个单词）进行了掩码语言模型任务的训练。模型从大规模数据集中学习，这种训练我们无法轻易复制。为了说明，BERT大型模型需要4天和16个云TPU进行训练。
- en: The concept of transfer learning helps us leverage this already-trained model
    for our downstream tasks. The idea behind this is that we take a generic language
    model, and fine-tune it for our specific task. High-level concepts are already
    learned by the model; we simply need to teach it more about a specific task.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 转移学习的概念帮助我们利用这个已经训练好的模型来完成我们的下游任务。这个想法是我们取一个通用的语言模型，并针对我们的特定任务对其进行微调。高级概念已经被模型学习；我们只需要教它更多关于特定任务的知识。
- en: 'In order to do this, we use the pre-trained model and add a single layer (often
    a single-layered neural network) on top of it. The nature of the layer will depend
    on the task we are fine-tuning for. For any task, we simply plug in the task-specific
    inputs and outputs in the correct format into BERT and fine-tune all the parameters
    end to end. As we fine-tune, we will achieve two tasks:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们使用预训练模型，并在其上方添加一个单层（通常是单层神经网络）。该层的性质将取决于我们微调的任务。对于任何任务，我们只需将特定任务的输入和输出以正确的格式插入到BERT中，并端到端微调所有参数。随着微调的进行，我们将实现两个任务：
- en: The parameters of the transformer will be updated iteratively to refine the
    embeddings and generate task-specific contextual embeddings
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换器的参数将迭代更新以细化嵌入并生成特定于任务的上下文嵌入
- en: The newly added layer will be trained (that is, will learn appropriate parameters)
    to classify the new class of embeddings
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新增的层将被训练（即学习适当的参数）以分类新的嵌入类别
- en: Fine-tuning BERT is an inexpensive task both in terms of time and resources.
    Classification models can be built using an hour on a TPU, around 4 hours on a
    GPU, and 8-10 hours on a regular CPU. BERT has been used after fine-tuning several
    tasks such as question-answering, sentence completion, and text understanding.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 微调BERT在时间和资源方面都是一个低成本的任务。分类模型可以在TPU上使用一个小时，在GPU上大约4小时，在普通CPU上8-10小时。BERT在微调了多个任务（如问答、句子补全和文本理解）之后被使用。
- en: Detecting malware with BERT
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用BERT检测恶意软件
- en: So far, we have seen attention, transformers, and BERT. But all of it has been
    very specific to language-related tasks. How is all of what we have learned relevant
    to our task of malware detection, which has nothing to do with language? In this
    section, we will first discuss how we can leverage BERT for malware detection
    and then demonstrate an implementation of the same.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了注意力机制、Transformer和BERT。但所有这些都非常特定于与语言相关的任务。我们所学到的所有内容如何与我们的恶意软件检测任务相关，这个任务与语言无关呢？在本节中，我们将首先讨论如何利用BERT进行恶意软件检测，然后演示相应的实现。
- en: Malware as language
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 恶意软件作为语言
- en: 'We saw that BERT shows excellent performance on sentence-related tasks. A sentence
    is merely a sequence of words. Note that we as humans find meaning in a sequence
    because we understand language. Instead of words, the tokens could be anything:
    integers, symbols, or images. So BERT performs well on sequence tasks.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到BERT在句子相关任务上表现出色。句子仅仅是单词的序列。请注意，我们作为人类，我们能在序列中找到意义，因为我们理解语言。与单词不同，标记可以是任何东西：整数、符号或图像。因此，BERT在序列任务上表现良好。
- en: Now, imagine that instead of words, our tokens were calls made by an application.
    The life cycle of an application could be described as a series of API calls it
    makes. For instance, `<START>` `<REQUEST-URL>` `<DOWNLOAD-FILE>` `<EXECUTE-FILE>`
    `<OPEN-CONTACTS>` `<POST-URL>` `<END>` could represent the behavior of an application.
    Just like a sentence is a sequence of words, an application can be thought of
    as a sequence of API calls.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，想象一下，如果我们用应用程序的调用代替单词作为标记。应用程序的生命周期可以描述为一系列它所发出的API调用。例如，`<START>` `<REQUEST-URL>`
    `<DOWNLOAD-FILE>` `<EXECUTE-FILE>` `<OPEN-CONTACTS>` `<POST-URL>` `<END>` 可以代表一个应用程序的行为。就像句子是单词的序列一样，一个应用程序可以被看作是API调用的序列。
- en: The relevance of BERT
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: BERT的相关性
- en: Recall that BERT learned contextual embeddings for a word. If we use BERT with
    malware data (with API calls as words and their sequence as sentences), the model
    will be able to learn embedding representations for each API call and condition
    it on the context. This is useful for malware detection because a single API call
    cannot determine whether an application is malware or not, but the context in
    which it is called might. For example, by itself, the API call for making a request
    to a third-party URL may not be malicious, but coupled with accessing stored passwords
    and contacts, it may indicate malware at work. This is our motivation behind choosing
    BERT as a model for malware detection.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，BERT学习了一个单词的上下文嵌入。如果我们用恶意软件数据（以API调用作为单词，它们的序列作为句子）使用BERT，该模型将能够为每个API调用学习嵌入表示，并基于上下文进行条件化。这对于恶意软件检测是有用的，因为单个API调用不能确定一个应用程序是否是恶意软件，但它在被调用的上下文中可能可以。例如，单独的API调用请求第三方URL可能不是恶意的，但与访问存储的密码和联系人相结合，可能表明正在运行恶意软件。这是我们选择BERT作为恶意软件检测模型背后的动机。
- en: 'We will use BERT on the malware classification task just like a sentence classification
    task. Here we have two options:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在恶意软件分类任务上使用BERT，就像句子分类任务一样。这里我们有两种选择：
- en: Train a new BERT model from scratch
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练一个新的BERT模型
- en: Fine-tune the existing BERT model (pre-trained on language data) on malware
    classification
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在基于语言数据的预训练BERT模型上微调现有的BERT模型以进行恶意软件分类
- en: As the domains of the pre-trained model are different from malware (that is,
    tokens in malware API sequences will not appear in the Wikipedia or BooksCorpus
    datasets), the first option might seem better. However, recall that the datasets
    for pre-training were massive, and we do not have access to malware data at that
    scale.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 由于预训练模型的领域与恶意软件不同（即，恶意软件API序列中的标记不会出现在维基百科或BooksCorpus数据集中），第一个选项可能看起来更好。然而，请记住，预训练的数据集是庞大的，我们没有访问到如此规模的恶意软件数据。
- en: Prior research has shown that a BERT model, even when pre-trained in the English
    language, serves as an excellent candidate for malware detection. This is because
    the pre-training results in an optimal set of parameters that results in faster
    convergence of any downstream task such as malware detection. In the following
    sections, this is the approach we will take. We will first preprocess the data,
    read it into a DataFrame, and then fine-tune it on a pre-trained BERT model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的研究表明，即使是用英语预训练的BERT模型，也是恶意软件检测的一个优秀候选者。这是因为预训练产生了一组最优参数，这些参数使得任何下游任务（如恶意软件检测）的收敛速度更快。在接下来的章节中，我们将采取这种方法。我们首先预处理数据，将其读入DataFrame，然后在预训练的BERT模型上进行微调。
- en: Getting the data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 获取数据
- en: As described previously, malware can be analyzed using both static and dynamic
    methods. Several commercial tools exist for decompiling malware binaries, understanding
    the behavior, and examining their activities. One such tool for malware analysis
    is WildFire ([https://www.paloaltonetworks.com/products/secure-the-network/wildfire](https://www.paloaltonetworks.com/products/secure-the-network/wildfire)),
    developed by Palo Alto Networks. It is a cloud malware protection engine that
    utilizes advanced machine learning models to detect targeted malware attacks in
    real time.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，恶意软件可以通过静态和动态方法进行分析。存在一些商业工具用于反编译恶意软件二进制文件、理解其行为和检查其活动。其中一种用于恶意软件分析的工具有WildFire（[https://www.paloaltonetworks.com/products/secure-the-network/wildfire](https://www.paloaltonetworks.com/products/secure-the-network/wildfire)），由帕洛阿尔托网络公司开发。它是一个云恶意软件保护引擎，利用先进的机器学习模型实时检测定向恶意软件攻击。
- en: Creating your own datasets for malware detection is challenging. First, using
    tools such as WildFire to generate dynamic analysis files is an expensive task
    (commercial tools are generally patented and require a license) and also outside
    the scope of this book. Second, examples of malware, particularly those seen in
    the wild, are hard to come by. Finally, experimenting with malware executables
    may inadvertently infect your systems. We will, therefore, use a commercially
    available malware dataset.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为恶意软件检测创建自己的数据集具有挑战性。首先，使用WildFire等工具生成动态分析文件是一项昂贵的任务（商业工具通常是专利的，需要许可证），而且超出了本书的范围。其次，恶意软件的示例，尤其是那些在野外看到的，很难找到。最后，对恶意软件可执行文件进行实验可能会无意中感染您的系统。因此，我们将使用商业可用的恶意软件数据集。
- en: In 2018, Palo Alto Networks released a research paper ([https://arxiv.org/pdf/1812.07858.pdf](https://arxiv.org/pdf/1812.07858.pdf))
    that discussed common cybersecurity problems, in which malware detection was discussed
    in great detail. Along with the paper, they released a malware dataset that contained
    analysis files for over 180,000 different applications. The dataset is made of
    a sample of malware identified by Palo Alto Networks in a given period. For each
    malware, they provide identifiers and the domains accessed by the file, along
    with the sequence of API calls made by the application.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 2018年，帕洛阿尔托网络公司发布了一篇研究论文（[https://arxiv.org/pdf/1812.07858.pdf](https://arxiv.org/pdf/1812.07858.pdf)），讨论了常见的网络安全问题，其中详细讨论了恶意软件检测。与论文一起，他们发布了一个包含超过18万个不同应用程序分析文件的数据集。该数据集由帕洛阿尔托网络公司在特定时期内识别的恶意软件样本组成。对于每个恶意软件，他们提供了标识符和文件访问的域名，以及应用程序发出的API调用序列。
- en: The dataset is freely available for students and researchers and can be obtained
    by contacting Palo Alto Networks as described in the paper. However, the method
    we present is fairly generic and can be applied to any malware dataset that you
    can gain access to.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集对学生和研究人员免费开放，可以通过在论文中描述的方式联系帕洛阿尔托网络公司获取。然而，我们提出的方法相当通用，可以应用于您能够访问的任何恶意软件数据集。
- en: Preprocessing the data
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预处理数据
- en: The Palo Alto Networks dataset contains several features for every application,
    including static and dynamic analysis files. However, of particular interest to
    us is the API call sequence. This is because we want to exploit the power of transformers.
    A defining characteristic of transformers is that they excel at handling sequential
    data via attention mechanisms.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 帕洛阿尔托网络公司数据集包含每个应用程序的几个特征，包括静态和动态分析文件。然而，对我们来说特别感兴趣的是API调用序列。这是因为我们想利用变换器的力量。变换器的一个定义特征是它们擅长通过注意力机制处理序列数据。
- en: 'The dynamic analysis file will give us the sequence of API calls made by the
    application. Each API call consists of two parts: the `GET` request). The key
    refers to the parameters passed to the API call (such as the actual domain to
    which the application is connected). While examining the parameters will also
    reveal significant information about whether an app is malicious, here we simply
    focus on the action of the API call.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 动态分析文件将为我们提供应用程序发出的API调用序列。每个API调用由两部分组成：`GET`请求。键指的是传递给API调用的参数（例如应用程序连接的实际域名）。在检查参数的同时，也会揭示有关应用程序是否恶意的重要信息，但在这里我们只关注API调用的动作。
- en: 'Every application (whether malware or benign) has an associated XML file that
    contains the API call logs. Once they are extracted, we will have access to the
    set of actions taken by the application. An example snippet could look like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 每个应用程序（无论是恶意软件还是良性软件）都有一个相关的XML文件，其中包含API调用日志。一旦提取出来，我们将能够访问应用程序采取的动作集。一个示例片段可能如下所示：
- en: '![Figure 3.5 – API call sequence](img/B19327_03_05.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图3.5 – API调用序列](img/B19327_03_05.jpg)'
- en: Figure 3.5 – API call sequence
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 – API调用序列
- en: We first extract the sequence for all the applications. Now that we have the
    sequence, it must be converted into tokens suitable for consumption by a machine
    learning model. Once we have all the sequences, we can compute the universe of
    known API calls, and assign a unique integer to each API call. Every application
    can now be represented as a sequence of integers.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先提取所有应用程序的序列。现在我们有了序列，它必须被转换为适合由机器学习模型消费的标记。一旦我们有了所有序列，我们就可以计算已知的API调用集，并为每个API调用分配一个唯一的整数。现在，每个应用程序都可以表示为整数的序列。
- en: Note that the steps discussed so far are not specific to the Palo Alto Networks
    dataset, and this is the reason why we did not introduce any specific functions
    or code to do the preprocessing. You can apply this same technique to construct
    a feature vector from any malware dataset.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，到目前为止所讨论的步骤并不特定于帕洛阿尔托网络数据集，这也是我们没有引入任何特定函数或代码进行预处理的原因。您可以将此相同技术应用于从任何恶意软件数据集中构建特征向量。
- en: For convenience and simplicity, we will provide the preprocessed versions of
    the dataset. Note that the API call sequences are represented by integers, as
    we cannot share the exact API calls publicly. Interested readers can refer to
    the original paper and data if they are curious to learn more.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便和简单，我们将提供数据集的预处理版本。请注意，API调用序列由整数表示，因为我们不能公开分享确切的API调用。对了解更多感兴趣的读者可以参考原始论文和数据。
- en: Building a classifier
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建分类器
- en: We will leverage the BERT model to build our classifier. Much of the code here
    is borrowed from the official notebooks released by Google when they released
    the BERT model in 2019\. Some of the code may seem intimidating; however, do not
    worry. A lot of this is just boilerplate environment setup and function definitions
    that you absolutely do not need to understand in detail. We will go over the code
    and discuss what parts need to be changed when you implement this on your own
    or wish to use this setup for a different problem.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用BERT模型来构建我们的分类器。这里的大部分代码是从谷歌在2019年发布BERT模型时发布的官方笔记本中借用的。一些代码可能看起来令人畏惧；然而，请不要担心。其中很多只是样板环境设置和函数定义，您绝对不需要详细了解。我们将审查代码并讨论在您自己实现或希望为不同问题使用此设置时需要更改的部分。
- en: 'First, we will import the required libraries. If any of these are not already
    installed (and Python will throw an error saying so), then they can be installed
    using the `pip` utility:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所需的库。如果其中任何一个尚未安装（Python会抛出错误），则可以使用`pip`实用程序进行安装：
- en: '[PRE0]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will leverage a pre-trained BERT model for building our classifier. TensorFlow
    Hub contains all of these pre-trained models and they are available for use by
    the public. This function reads the model and its vocabulary and generates a tokenizer.
    The tokenizer is responsible for converting the words we see into tokens that
    can be understood by the machine learning model:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将利用预训练的BERT模型来构建我们的分类器。TensorFlow Hub包含了所有这些预训练模型，并且它们对公众开放。此函数读取模型及其词汇表并生成一个标记器。标记器负责将我们看到的单词转换为机器学习模型可以理解的标记：
- en: '[PRE1]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now, we actually create the classification model. This function will create
    a BERT module and define the input and output structures. We will then obtain
    the output layer and find the parameters there; these are the ones that would
    be used to run inference. We apply a dropout to this layer and calculate the logits
    (that is, the `softmax` output of the layer). During the training phase, the `softmax`
    output is used to compute the loss relative to the ground truth. In the inferencing
    phase, the output can be used to predict the token depending on which probability
    in the `softmax` output is the highest. Note that the ground truth is in the form
    of categorical tokens (in our case, API calls) and, therefore, needs to be converted
    into a numeric form using one-hot encoding:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们实际上创建分类模型。此函数将创建一个BERT模块并定义输入和输出结构。然后我们将获取输出层并找到那里的参数；这些参数将用于运行推理。我们对这一层应用dropout并计算logits（即层的`softmax`输出）。在训练阶段，`softmax`输出用于计算相对于真实值的损失。在推理阶段，输出可以用来根据`softmax`输出中哪个概率最高来预测标记。请注意，真实值以分类标记的形式存在（在我们的情况下，是API调用），因此需要使用one-hot编码将其转换为数值形式：
- en: '[PRE2]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The code so far has been mostly boilerplate functions for setting up the parameters.
    Now, we will create a function that actually defines our training and inference
    settings. Recall that the previous function defined the steps to create the model.
    This function will leverage the previous one for training and inference.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止的代码主要是设置参数的样板函数。现在，我们将创建一个实际定义我们的训练和推理设置的函数。回想一下，之前的函数定义了创建模型的步骤。这个函数将利用之前的函数进行训练和推理。
- en: First, we read the input features and labels that are crucial to training. These
    will be passed to the function as parameters. If the training phase is going on,
    the function will use the `create_model` function to calculate a loss that will
    be optimized for training. If not, it will simply score the data point on the
    model and return a predicted label and output probabilities.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们读取对训练至关重要的输入特征和标签。这些将被作为参数传递给函数。如果正在进行训练阶段，该函数将使用`create_model`函数计算一个损失，该损失将用于训练优化。如果不进行训练，它将简单地根据模型对数据点进行评分并返回一个预测标签和输出概率。
- en: 'This function also has a metric calculation function defined. This is crucial
    to analyzing and comparing the performance of our model. TensorFlow has built-in
    functions that calculate common metrics such as precision, recall, false positives,
    false negatives, F1 score, and so on. We leverage these built-in functions and
    return a dictionary, which contains various metrics:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数还定义了一个度量计算函数。这对于分析和比较我们模型的表现至关重要。TensorFlow内置了计算常见度量（如精确率、召回率、假阳性、假阴性、F1分数等）的函数。我们利用这些内置函数并返回一个字典，其中包含各种度量：
- en: '[PRE3]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that all of the functions to initialize the model, compute loss and training,
    and inferencing are good to go, we can use this setup by plugging in our malware
    data. Note that the procedure so far is generically applicable to any fine-tuning
    problem you are trying to solve using BERT; nothing has been hardcoded or is specific
    to malware data. If you want to use the fine-tuning approach to BERT on another
    task (sentiment analysis, hate speech detection, or misinformation detection),
    all of the steps we have completed so far remain valid.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在所有用于初始化模型、计算损失和训练以及推理的函数都准备好了，我们可以通过插入我们的恶意数据来使用这个设置。请注意，到目前为止的过程是通用的，适用于任何你试图使用BERT解决的微调问题；没有任何东西是硬编码的或特定于恶意数据的。如果你想使用BERT的微调方法来处理其他任务（情感分析、仇恨言论检测或虚假信息检测），我们之前完成的所有步骤仍然有效。
- en: We will now define some parameters used for training. The first set of parameters
    are standard machine learning ones. The batch size defines the number of examples
    that will be used to calculate loss at a time, and the learning rate defines the
    rate at which parameters will be updated in the gradient descent optimization
    algorithm. The number of epochs is set here to `3`, which is a small number. This
    is because we are not training a model from scratch; we are simply using an already
    trained model, and fine-tuning it to operate on our dataset.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将定义一些用于训练的参数。第一组参数是标准的机器学习参数。批大小定义了每次计算损失时将使用的示例数量，而学习率定义了参数在梯度下降优化算法中更新的速率。这里将epoch数设置为`3`，这是一个较小的数字。这是因为我们并不是从头开始训练模型；我们只是在使用一个已经训练好的模型，并对其进行微调以在我们的数据集上运行。
- en: The next set of parameters exists for optimization and ease in training. It
    defines after how many steps a new version of the model should be saved. Here,
    we have set it to `500`, meaning that after every 500 steps, a new model will
    be saved. This helps us if we run into an unexpected error or crash; the model
    simply reads the latest saved model and picks up training from that point.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个参数集是为了优化和简化训练而存在的。它定义了在多少步之后应该保存模型的新版本。在这里，我们将其设置为`500`，这意味着每500步之后，将保存一个新的模型。这有助于我们在遇到意外错误或崩溃时；模型只需读取最新的保存模型并从该点继续训练。
- en: 'Finally, the last set of parameters defines the positive ratio. This is the
    proportion of malware samples in the training data. Here we set it to 0.001, which
    amounts to 0.1%:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一组参数定义了正样本比例。这是训练数据中恶意样本的比例。在这里我们将其设置为0.001，相当于0.1%：
- en: '[PRE4]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now, we read the data frame that contains our data. Recall that the `VERDICT`
    column contained a 0/1 label indicating whether that particular data point was
    malware or not. We separate the malware and benign samples, sample the required
    fraction from the positive class, and then combine it with the negative class.
    This way, we have a dataset that contains only the required proportion of malware
    samples:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们读取包含我们数据的数据帧。回想一下，`VERDICT`列包含一个0/1标签，表示该特定数据点是否为恶意软件。我们将恶意软件和良性样本分开，从正类中采样所需的分数，然后与负类结合。这样，我们就有一个只包含所需比例的恶意软件样本的数据集：
- en: '[PRE5]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We now split our data into training and testing sets. Note that here we have
    a very small proportion of malware in our sampled data. If we split randomly,
    we might end up with all of the malware entirely in the training or testing set.
    To avoid this, we apply stratified sampling. With stratified sampling, the proportion
    of labels remains roughly the same in both the training and testing datasets:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将我们的数据分为训练集和测试集。请注意，在这里我们的样本数据中恶意软件的比例非常小。如果我们随机分割，我们可能会在训练集或测试集中完全包含所有恶意软件。为了避免这种情况，我们应用分层抽样。在分层抽样中，标签的比例在训练集和测试数据集中大致相同：
- en: '[PRE6]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Remember that the data we have is in the form of API call sequences. This has
    to be converted into a form suitable for being consumed by BERT. We do this in
    our next step and transform both the training and test data into the required
    format.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们拥有的数据是以API调用序列的形式。这必须转换为BERT可以消费的形式。我们在下一步中这样做，并将训练数据和测试数据转换为所需格式。
- en: 'First, we use the `InputExample` class to wrap our API sequence string and
    labels together:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`InputExample`类将我们的API序列字符串和标签一起包装：
- en: '[PRE7]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we transform the sequence into features using our tokenizer:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用我们的分词器将序列转换为特征：
- en: '[PRE8]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We now have our features and labels. We are ready to train the model! We will
    use the model function builder we defined earlier to create the model and pass
    it to the TensorFlow estimator, which will take care of the training for us. We
    specify the output directory in which to save the trained model as well as the
    parameters we defined earlier (summary and checkpoint steps) in a run configuration.
    This also gets passed to the estimator:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了特征和标签。我们准备好训练模型了！我们将使用我们之前定义的模型函数构建器来创建模型，并将其传递给TensorFlow估计器，它会为我们处理训练。我们在运行配置中指定了保存训练模型的输出目录以及我们之前定义的参数（摘要和检查点步骤），这也传递给了估计器：
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This will produce a lot of output, most of which you do not need to understand.
    The training time will vary depending on the processor you are using, the GPU
    (if any), and system usage. Without a GPU, the fine-tuning took approximately
    29,000 seconds, which amounts to roughly eight hours.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生大量的输出，其中大部分你不需要理解。训练时间将取决于你使用的处理器、GPU（如果有）以及系统使用情况。如果没有GPU，微调大约需要29,000秒，这相当于大约八小时。
- en: 'Finally, we want to use this fine-tuned model to make predictions for new data
    and evaluate its performance. We can use the same estimator in inference mode:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们希望使用这个微调模型对新数据进行预测并评估其性能。我们可以使用相同的估计器在推理模式下：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This should show you an output that prints out the metrics. Note that your
    numbers may differ slightly from the ones you see here:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示一个输出，打印出指标。请注意，你的数字可能与我这里看到的有轻微差异：
- en: '[PRE11]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Recall that earlier, we defined a model evaluation function. The variable metrics
    will contain the dictionary with the various evaluation metrics. If you print
    it out, you should be able to examine the accuracy, precision, recall, and F-1
    score.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，我们之前定义了一个模型评估函数。变量metrics将包含包含各种评估指标的字典。如果你打印出来，你应该能够检查准确率、精确率、召回率和F-1分数。
- en: 'This completes our experiment! We have successfully used a BERT model pre-trained
    on a language task and fine-tuned it to classify malicious applications based
    on the API call sequence. Feel free to experiment with the code. Here are some
    things to ponder:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们的实验！我们成功使用了一个在语言任务上预训练的BERT模型，并将其微调以根据API调用序列对恶意应用程序进行分类。请随意实验代码。以下是一些需要思考的事情：
- en: What happens if you use the BERT large model instead of the BERT base model?
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用BERT大型模型而不是BERT基础模型会发生什么？
- en: How does the performance vary with the positive rate fraction?
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能如何随着正样本率分数的变化而变化？
- en: What happens if you vary the architecture (add more layers)?
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你改变架构（添加更多层）会发生什么？
- en: With that, we have come to the end of this chapter.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样，我们来到了这一章的结尾。
- en: Summary
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter provided an introduction to malware and a hands-on blueprint for
    how it can be detected using transformers. First, we discussed the concepts of
    malware and the various forms they come in (rootkits, viruses, and worms). We
    then discussed the attention mechanism and transformer architecture, which are
    recent advances that have taken the machine learning world by storm. We also looked
    at BERT, a model that has beat several baselines in tasks such as sentence classification
    and question-answering. We leveraged BERT for malware detection by fine-tuning
    a pre-trained model on API call sequence data.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了恶意软件及其使用变压器进行检测的实战蓝图。首先，我们讨论了恶意软件的概念及其各种形式（如rootkits、病毒和蠕虫）。然后，我们讨论了注意力机制和变压器架构，这些是近年来在机器学习领域引起轰动的前沿进展。我们还研究了BERT模型，该模型在句子分类和问答等任务中击败了多个基线。我们通过在API调用序列数据上微调预训练模型，利用BERT进行恶意软件检测。
- en: Malware is a pressing problem that places users of phones and computers at great
    risk. Data scientists and machine learning practitioners who are interested in
    the security space need to have a strong understanding of how malware works and
    the architecture of models that can be used for detection. This chapter provided
    all of the knowledge needed and is a must to master for a SecML professional.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 恶意软件是一个紧迫的问题，它使手机和电脑用户面临巨大的风险。对安全领域感兴趣的数据科学家和机器学习从业者需要深入了解恶意软件的工作原理以及可用于检测的模型架构。本章提供了所需的所有知识，对于SecML专业人士来说，掌握这些知识是必不可少的。
- en: 'In the next chapter, we will switch gears and turn to a different problem:
    fake online reviews.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将转换方向，转向一个不同的问题：虚假在线评论。
