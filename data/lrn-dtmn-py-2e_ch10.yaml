- en: Clustering News Articles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 新闻文章聚类
- en: In most of the earlier chapters, we performed data mining knowing what we were
    looking for. Our use of *target classes* allowed us to learn how our features
    model those targets during the training phase, which lets the algorithm set internal
    parameters to maximize its learning. This type of learning, where we have targets
    to train against, is called **supervised learning**. In this chapter, we'll consider
    what we do without those targets. This is **unsupervised learning** and it's much
    more of an exploratory task. Rather than wanting to classify with our model, the
    goal in unsupervised learning is to explore the data to find insights.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数早期章节中，我们在知道我们要寻找什么的情况下进行数据挖掘。我们使用*目标类别*使我们能够在训练阶段学习我们的特征模型如何模拟这些目标，这使得算法可以设置内部参数以最大化其学习。这种有目标进行训练的学习类型被称为**监督学习**。在本章中，我们将考虑在没有这些目标的情况下我们做什么。这是**无监督学习**，它更多的是一种探索性任务。在无监督学习中，我们的目标不是用我们的模型进行分类，而是探索数据以发现洞察。
- en: In this chapter, we will look at clustering news articles to find trends and
    patterns in the data. We'll look at how we can extract data from different websites
    using a link aggregation website to show a variety of news stories.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何通过聚类新闻文章来发现数据中的趋势和模式。我们将研究如何使用链接聚合网站从不同的网站提取数据，以展示各种新闻故事。
- en: 'The key concepts covered in this chapter include:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的关键概念包括：
- en: Using the reddit API to collect interesting news stories
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Reddit API收集有趣的新闻故事
- en: Obtaining text from arbitrary websites
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从任意网站获取文本
- en: Cluster analysis for unsupervised data mining
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督数据挖掘的聚类分析
- en: Extracting topics from documents
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从文档中提取主题
- en: Online learning for updating a model without retraining it
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在不重新训练的情况下更新模型的在线学习
- en: Cluster ensembling to combine different models
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类集成以组合不同的模型
- en: Trending topic discovery
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 趋势主题发现
- en: In this chapter, we will build a system that takes a live feed of news articles
    and groups them together such that the groups have similar topics. You could run
    the system multiple times over several weeks (or longer) to see how trends change
    over that time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个系统，该系统可以接收新闻文章的实时流并将其分组，使得组内的文章具有相似的主题。你可以运行该系统几周（或更长）多次，以查看趋势如何随时间变化。
- en: Our system will start with the popular link aggregation website ([https://www.reddit.com](https://www.reddit.com/)),
    which stores lists of links to other websites, as well as a comments section for
    discussion. Links on reddit are broken into several categories of links, called
    **subreddits**. There are subreddits devoted to particular TV shows, funny images,
    and many other things. What we are interested in are the subreddits for news.
    We will use the */r/worldnews* subreddit in this chapter, but the code should
    work with any other text-based subreddit.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的系统将从流行的链接聚合网站([https://www.reddit.com](https://www.reddit.com/))开始，该网站存储指向其他网站的链接列表，以及一个用于讨论的评论部分。Reddit上的链接被分为几个链接类别，称为**subreddits**。有专门针对特定电视节目、搞笑图片和其他许多内容的subreddits。我们感兴趣的是新闻的subreddits。在本章中，我们将使用*/r/worldnews*
    subreddits，但代码应该适用于任何其他基于文本的subreddits。
- en: 'In this chapter, our goal is to download popular stories and then cluster them
    to see any major themes or concepts that occur. This will give us an insight into
    the popular focus, without having to manually analyze hundreds of individual stories.
    The general process is to:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目标是下载热门故事，然后对它们进行聚类，以查看任何主要主题或概念。这将使我们能够了解热门焦点，而无需手动分析数百个单独的故事。一般过程如下：
- en: Collect links to recent popular news stories from reddit.
  id: totrans-14
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Reddit收集最近的热门新闻链接。
- en: Download the web page from those links.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这些链接下载网页。
- en: Extract just the news story from the downloaded website.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 只从下载的网站中提取新闻故事。
- en: Perform cluster analysis to find clusters of stories.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行聚类分析以找到故事集群。
- en: Analyse those clusters to discover trends.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析这些集群以发现趋势。
- en: Using a web API to get data
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Web API获取数据
- en: We have used web-based APIs to extract data in several of our previous chapters.
    For instance, in [Chapter 7](lrn-dtmn-py-2e_ch07.html)*, Follow Recommendations
    Using Graph Mining*, we used Twitter's API to extract data. Collecting data is
    a critical part of the data mining pipeline, and web-based APIs are a fantastic
    way to collect data on a variety of topics.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在之前的一些章节中使用了基于Web的API来提取数据。例如，在[第7章](lrn-dtmn-py-2e_ch07.html)*使用图挖掘进行推荐跟踪*中，我们使用了Twitter的API来提取数据。收集数据是数据挖掘流程中的关键部分，基于Web的API是收集各种主题数据的一种极好的方式。
- en: 'There are three things you need to consider when using a web-based API for
    collecting data: authorization methods, rate limiting, and API endpoints.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用基于Web的API收集数据时，你需要考虑三个因素：授权方法、速率限制和API端点。
- en: '**Authorization methods** allow the data provider to know who is collecting
    the data, in order to ensure that they are being appropriately rate-limited and
    that data access can be tracked. For most websites, a personal account is often
    enough to start collecting data, but some websites will ask you to create a formal
    developer account to get this access.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '**授权方法**允许数据提供者知道谁在收集数据，以确保它们被适当地限制速率，并且数据访问可以被追踪。对于大多数网站，一个个人账户通常足以开始收集数据，但有些网站会要求你创建一个正式的开发者账户以获取这种访问权限。'
- en: '**Rate limiting** is applied to data collection, particularly free services.
    It is important to be aware of the rules when using APIs, as they can and do change
    from website to website. Twitter''s API limit is 180 requests per 15 minutes (depending
    on the particular API call). Reddit, as we will see later, allows 30 requests
    per minute. Other websites impose daily limits, while others limit on a per-second
    basis. Even within websites, there are drastic differences for different API calls.
    For example, Google Maps has smaller limits and different API limits per-resource,
    with different allowances for the number of requests per hour.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**速率限制**应用于数据收集，尤其是免费服务。在使用API时了解规则很重要，因为它们可以从网站到网站发生变化。Twitter的API限制为每15分钟180次请求（具体取决于特定的API调用）。Reddit，如我们稍后所见，允许每分钟30次请求。其他网站可能实施每日限制，而有些网站则按每秒限制。即使在网站内部，不同的API调用之间也存在巨大的差异。例如，Google
    Maps有更小的限制和不同的API限制，每个资源都有不同的每小时请求次数限制。'
- en: If you find you are creating an app or running an experiment that needs more
    requests and faster responses, most API providers have commercial plans that allow
    for more calls. Contact the provider for more details.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现你正在创建一个需要更多请求和更快响应的应用或进行实验，大多数API提供者都有商业计划，允许进行更多调用。联系提供者获取更多详情。
- en: '**API Endpoints** are the actual URLs that you use to extract information.
    These vary from website to website. Most often, web-based APIs will follow a RESTful
    interface (short for **Representational State Transfer**). RESTful interfaces
    often use the same actions that HTTP does: `GET`, `POST`, and `DELETE` are the
    most common. For instance, to retrieve information on a resource, we might use
    the following (example only) API endpoint:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**API端点**是实际用于提取信息的URL。这些在不同的网站之间各不相同。基于Web的API通常遵循RESTful接口（代表状态转移的缩写）。RESTful接口通常使用HTTP相同的操作：`GET`、`POST`和`DELETE`是最常见的。例如，为了检索资源的信息，我们可能会使用以下（仅作示例）API端点：'
- en: '[www.dataprovider.com/api/resource_type/resource_id/](http://www.dataprovider.com/api/resource_type/resource_id/)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '[www.dataprovider.com/api/resource_type/resource_id/](http://www.dataprovider.com/api/resource_type/resource_id/)'
- en: To get the information, we just send an HTTP `GET` request to this URL. This
    will return information on the resource with the given type and ID. Most APIs
    follow this structure, although there are some differences in the implementation.
    Most websites with APIs will have them appropriately documented, giving you details
    of all the APIs that you can retrieve.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取信息，我们只需向该URL发送一个HTTP `GET`请求。这将返回给定类型和ID的资源信息。大多数API都遵循这种结构，尽管在实现上可能存在一些差异。大多数拥有API的网站都会对其进行适当的文档说明，提供你可以检索的所有API的详细信息。
- en: 'First, we set up the parameters to connect to the service. To do this, you
    will need a developer key for reddit. In order to get this key, log into the site
    at [https://www.reddit.com/login](https://www.reddit.com/login) and go to [https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps).
    From here, click on are you a developer? create an app... and fill out the form,
    setting the type as script. You will get your client ID and a secret, which you
    can add to a new Jupyter Notebook:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们设置参数以连接到服务。为此，你需要Reddit的开发者密钥。为了获取这个密钥，登录到[https://www.reddit.com/login](https://www.reddit.com/login)网站，然后转到[https://www.reddit.com/prefs/apps](https://www.reddit.com/prefs/apps)。从这里，点击“你是一位开发者吗？创建一个应用...”，填写表格，将类型设置为脚本。你将获得客户端ID和密钥，可以将它们添加到一个新的Jupyter
    Notebook中：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Reddit also asks you (when you use their API) to set the user agent to a unique
    string that includes your username. Create a user agent string that uniquely identifies
    your application. I used the name of the book, chapter 10, and a version number
    of 0.1 to create my user agent, but it can be any string you like. Note that not
    doing this may result in your connection being heavily rate-limited:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit 还要求您（当您使用他们的 API 时）设置一个包含您用户名的唯一字符串作为用户代理。创建一个唯一标识您的应用程序的用户代理字符串。我使用了书名、第
    10 章，以及版本号 0.1 来创建我的用户代理，但可以是您喜欢的任何字符串。请注意，如果不这样做，可能会导致您的连接被严重限制速率：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In addition, you will need to log in to reddit using your username and password.
    If you don't have one already, sign up for a new one (it is free and you don't
    need to verify with personal information either).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还需要使用您的用户名和密码登录 Reddit。如果您还没有，可以注册一个新的账户（这是免费的，而且您也不需要用个人信息进行验证）。
- en: You will need your password to complete the next step, so be careful before
    sharing your code to others to remove it. If you don't put your password in, set
    it to none and you will be prompted to enter it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成下一步之前，您将需要密码，所以在将代码分享给他人之前请小心，以移除密码。如果您不输入密码，将其设置为 none，您将被提示输入它。
- en: 'Now let''s create the username and password:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建用户名和密码：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we are going to create a function to log with this information. The reddit
    login API will return a token that you can use for further connections, which
    will be the result of this function. The code obtains the necessary information
    to log in to reddit, set the user agent, and then obtain an access token that
    we can use with future requests:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建一个函数来记录这些信息。Reddit 登录 API 将返回一个令牌，您可以使用它进行后续连接，这将是这个函数的结果。代码获取登录 Reddit、设置用户代理和获取我们可以用于未来请求的访问令牌所必需的信息：
- en: '[PRE3]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We can call now our function to get an access token:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以调用我们的函数来获取访问令牌：
- en: '[PRE4]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This token object is just a dictionary, but it contains the `access_token`
    string that we will pass along with future requests. It also contains other information
    such as the scope of the token (which would be everything) and the time in which
    it expires, for example:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这个令牌对象只是一个字典，但它包含我们将与未来请求一起传递的 `access_token` 字符串。它还包含其他信息，例如令牌的作用域（将是一切）和它过期的时间，例如：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you are creating a production-level app, make sure you check the expiry of
    the token and to refresh it if it runs out. You'll also know this has happened
    if your access token stops working when trying to make an API call.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在创建一个生产级别的应用程序，请确保检查令牌的过期时间，并在其到期时刷新它。您也会知道这种情况发生了，如果您的访问令牌在尝试进行 API 调用时停止工作。
- en: Reddit as a data source
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Reddit 作为数据源
- en: Reddit is a link aggregation website used by millions worldwide, although the
    English versions are US-centric. Any user can contribute a link to a website they
    found interesting, along with a title for that link. Other users can then upvote
    it, indicating that they liked the link, or downvote it, indicating they didn't
    like the link. The highest voted links are moved to the top of the page, while
    the lower ones are not shown. Older links are removed from the front page over
    time, depending on how many upvotes it has. Users who have stories upvoted earn
    points called karma, providing an incentive to submit only good stories.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit 是一个全球数百万用户使用的链接聚合网站，尽管其英文版本以美国为中心。任何用户都可以提交他们发现的有趣网站的链接，并为该链接添加标题。其他用户可以对其进行点赞，表示喜欢该链接，或者踩它，表示不喜欢该链接。得票最高的链接会被移至页面顶部，而得票较低的则不会显示。随着时间的推移，根据获得的点赞数，较旧的链接会被从首页移除。获得点赞的故事的用户会获得称为
    karma 的积分，这为提交优质故事提供了激励。
- en: Reddit also allows non-link content, called self-posts. These contain a title
    and some text that the submitter enters. These are used for asking questions and
    starting discussions. For this chapter, we will be considering only link-based
    posts, and not comment-based posts.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Reddit 还允许非链接内容，称为自投稿。这些包含提交者输入的标题和一些文本。这些用于提问和开始讨论。在本章中，我们将只考虑基于链接的帖子，而不是基于评论的帖子。
- en: Posts are separated into different sections of the website called subreddits.
    A subreddit is a collection of posts that are related. When a user submits a link
    to reddit, they choose which subreddit it goes into. Subreddits have their own
    administrators, and have their own rules about what is valid content for that
    subreddit.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 帖子被分为网站的不同部分，称为子版块。子版块是一系列相关的帖子。当用户向 Reddit 提交链接时，他们会选择它所属的子版块。子版块有自己的管理员，并有自己的规则，关于哪些内容对该子版块是有效的。
- en: By default, posts are sorted by **Hot**, which is a function of the age of a
    post, the number of upvotes, the number of downvotes it has received and how liberal
    the content is. There is also **New**, which just gives you the most recently
    posted stories (and therefore contains lots of spam and bad posts), and **Top**,
    which is the highest voted stories for a given time period. In this chapter, we
    will be using Hot, which will give us recent, higher-quality stories (there really
    are a lot of poor-quality links in New).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，帖子按 **热门** 排序，这是一个基于帖子的年龄、收到的点赞数、踩数以及内容自由度的函数。还有 **新**，它只提供最近发布的帖子（因此包含大量垃圾邮件和差评帖子），以及
    **Top**，它是在特定时间段内获得最高票数的帖子。在本章中，我们将使用热门排序，这将给我们带来最近、质量较高的故事（在新中确实有很多低质量的链接）。
- en: 'Using the token we previously created, we can now obtain sets of links from
    a subreddit. To do that, we will use the /r/<subredditname> API endpoint that,
    by default, returns the Hot stories. We will use the /r/worldnews subreddit:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前创建的令牌，我们现在可以获取子版块的链接集合。为此，我们将使用 /r/<subredditname> API 端点，默认情况下，它返回热门故事。我们将使用
    /r/worldnews 子版块：
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The URL for the previous endpoint lets us create the full URL, which we can
    set using string formatting:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 之前端点的 URL 允许我们创建完整的 URL，我们可以使用字符串格式化来设置它：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we need to set the headers. This is needed for two reasons: to allow
    us to use the authorization token we received earlier and to set the user agent
    to stop our requests from being heavily restricted. The code is as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要设置头部信息。这有两个原因：一是允许我们使用之前收到的授权令牌，二是将用户代理设置为阻止我们的请求受到过度限制。代码如下：
- en: '[PRE8]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Then, as before, we use the requests library to make the call, ensuring that
    we set the headers:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，就像之前一样，我们使用 requests 库来发起调用，确保我们设置了头部信息：
- en: '[PRE9]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Calling `json()` on this will result in a Python dictionary containing the
    information returned by reddit. It will contain the top 25 results from the given
    subreddit. We can get the title by iterating over the stories in this response.
    The stories themselves are stored under the dictionary''s data key. The code is
    as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个对象上调用 `json()` 将返回一个包含 Reddit 返回信息的 Python 字典。它将包含给定子版块的前 25 个结果。我们可以通过遍历此响应中的故事来获取标题。这些故事存储在字典的
    data 键下。代码如下：
- en: '[PRE10]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Getting the data
  id: totrans-58
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: Our dataset is going to consist of posts from the Hot list of the /r/worldnews
    subreddit. We saw in the previous section how to connect to reddit and how to
    download links. To put it all together, we will create a function that will extract
    the titles, links, and score for each item in a given subreddit.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集将包括来自/r/worldnews 子版块的 Hot 列表中的帖子。在前一节中，我们看到了如何连接到 Reddit 以及如何下载链接。为了将这些步骤整合起来，我们将创建一个函数，用于提取给定子版块中每个帖子的标题、链接和评分。
- en: We will iterate through the subreddit, getting a maximum of 100 stories at a
    time. We can also do pagination to get more results. We can read a large number
    of pages before reddit will stop us, but we will limit it to 5 pages.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遍历子版块，每次获取最多 100 个故事。我们也可以进行分页以获取更多结果。在 Reddit 停止我们之前，我们可以读取大量的页面，但我们将限制为
    5 页。
- en: 'As our code will be making repeated calls to an API, it is important to remember
    to rate-limit our calls. To do so, we will need the sleep function:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的代码将重复调用 API，因此记住对我们的调用进行速率限制非常重要。为此，我们需要 sleep 函数：
- en: '[PRE11]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Our function will accept a subreddit name and an authorization token. We will
    also accept a number of pages to read, though we will set a default of 5:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将接受一个子版块名称和一个授权令牌。我们还将接受要读取的页数，尽管我们将默认设置为 5：
- en: '[PRE12]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We saw in [Chapter 7](lrn-dtmn-py-2e_ch07.html)*, Follow Recommendations Using
    Graph Mining*, how pagination works for the Twitter API. We get a cursor with
    our returned results, which we send with our request. Twitter will then use this
    cursor to get the next page of results. The reddit API does almost exactly the
    same thing, except it calls the parameter after. We don't need it for the first
    page, so we initially set it to None. We will set it to a meaningful value after
    our first page of results.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第7章](lrn-dtmn-py-2e_ch07.html)*，使用图挖掘遵循建议*中看到，Twitter API的翻页是如何工作的。我们通过返回的结果获得一个游标，并将其与我们的请求一起发送。Twitter将使用这个游标来获取下一页的结果。Reddit
    API几乎做了完全相同的事情，只是它调用参数的顺序不同。我们不需要它用于第一页，所以我们最初将其设置为None。在我们的第一页结果之后，我们将将其设置为有意义的值。
- en: 'Calling the stories function is a simple case of passing the authorization
    token and the subreddit name:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 调用故事功能是一个简单的案例，只需传递授权令牌和subreddit名称：
- en: '[PRE13]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The returned results should contain the title, URL, and 500 stories, which
    we will now use to extract the actual text from the resulting websites. Here is
    a sample of the titles that I received by running the script:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 返回的结果应包含标题、URL和500个故事，我们将使用这些结果提取实际文本。以下是我运行脚本后收到的标题样本：
- en: '*Russia considers banning sale of cigarettes to anyone born after 2015*'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '*俄罗斯考虑禁止向2015年后出生的人出售香烟*'
- en: '*Swiss Muslim girls must swim with boys*'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '*瑞士穆斯林女孩必须与男孩一起游泳*'
- en: '*Report: Russia spread fake news and disinformation in Sweden - Russia has
    coordinated a campaign over the past 2years to influence Sweden’s decision making
    by using disinformation, propaganda and false documents, according to a report
    by researchers at The Swedish Institute of International Affairs.*'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*报告：俄罗斯在瑞典散布虚假新闻和虚假信息 - 根据瑞典国际事务研究所研究人员的一份报告，俄罗斯在过去两年中协调开展了一场宣传活动，通过散布虚假信息、宣传和伪造文件来影响瑞典的决策*'
- en: '*100% of Dutch Trains Now Run on Wind Energy. The Netherlands met its renewable
    energy goals a year ahead of time.*'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '*荷兰100%的火车现在都使用风能。荷兰提前一年实现了可再生能源目标*'
- en: '*Legal challenge against UK’s sweeping surveillance laws quickly crowdfunded*'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '*对英国全面监控法律的挑战迅速获得众筹*'
- en: '*A 1,000-foot-thick ice block about the size of Delaware is snapping off of
    Antarctica*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '*大约有特拉华州大小的1000英尺厚的冰块正在从南极洲断裂*'
- en: '*The U.S. dropped an average of 72 bombs every day — the equivalent of three
    an hour — in 2016, according to an analysis of American strikes around the world.
    U.S. Bombed Iraq, Syria, Pakistan, Afghanistan, Libya, Yemen, Somalia in 2016*'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '*根据对全球美国打击的分析，2016年美国平均每天投下72枚炸弹——相当于每小时3枚——2016年美国轰炸了伊拉克、叙利亚、巴基斯坦、阿富汗、利比亚、也门、索马里*'
- en: '*The German government is investigating a recent surge in fake news following
    claims that Russia is attempting to meddle in the country’s parliamentary elections
    later this year.*'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*德国政府正在调查最近虚假新闻激增的情况，据称俄罗斯试图干预该国今年晚些时候的议会选举*'
- en: '*Pesticides kill over 10 million bees in a matter of days in Brazil countryside*'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*巴西乡村几天内就有超过1000万只蜜蜂因农药死亡*'
- en: '*The families of American victims of Islamic State terrorist attacks in Europe
    have sued Twitter, charging that the social media giant allowed the terror group
    to proliferate online*'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '*欧洲伊斯兰国恐怖袭击的美国受害者家属起诉Twitter，指控社交媒体巨头允许恐怖组织在网上传播*'
- en: '*Gas taxes drop globally despite climate change; oil &amp; gas industry gets
    $500 billion in subsidies; last new US gas tax was in 1993*'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*尽管气候变化，全球汽油税下降；石油和天然气行业获得5000亿美元补贴；美国上一次新的汽油税是在1993年*'
- en: '*Czech government tells citizens to arm themselves and shoot Muslim terrorists
    in case of ''Super Holocaust''*'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '*捷克政府告诉公民在“超级大屠杀”的情况下武装自己并射击穆斯林恐怖分子*'
- en: '*PLO threatens to revoke recognition of Israel if US embassy moves to Jerusalem*'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*巴勒斯坦解放组织威胁，如果美国大使馆迁至耶路撒冷，将撤销对以色列的承认*'
- en: '*Two-thirds of all new HIV cases in Europe are being recorded in just one country
    – Russia: More than a million Russians now live with the virus and that number
    is expected to nearly double in the next decade*'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '*欧洲所有新发现的HIV病例中有三分之二仅记录在一个国家——俄罗斯：现在有超过一百万俄罗斯人感染了病毒，预计在下一个十年内这个数字将几乎翻倍*'
- en: '*Czech government tells its citizens how to fight terrorists: Shoot them yourselves
    | The interior ministry is pushing a constitutional change that would let citizens
    use guns against terrorists*'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '*捷克政府告诉其公民如何对抗恐怖分子：自己开枪射击 | 内政部正在推动一项宪法变革，这将允许公民使用枪支对抗恐怖分子*'
- en: '*Morocco Prohibits Sale of Burqa*'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '*摩洛哥禁止出售布卡*'
- en: '*Mass killer Breivik makes Nazi salute at rights appeal case*'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '*大规模杀手布雷维克在权利申诉案中行纳粹敬礼*'
- en: '*Soros Groups Risk Purge After Trump’s Win Emboldens Hungary*'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*索罗斯集团在特朗普获胜后面临清洗风险，匈牙利变得更加大胆*'
- en: '*Nigeria purges 50,000 ‘ghost workers’ from State payroll in corruption sweep*'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '*尼日利亚在反腐行动中清除5万名“幽灵员工”*'
- en: '*Alcohol advertising is aggressive and linked to youth drinking, research finds
    | Society*'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '*研究发现，酒精广告具有侵略性，与青少年饮酒有关 | 社会*'
- en: '*UK Government quietly launched ‘assault on freedom’ while distracting people,
    say campaigners behind legal challenge - The Investigatory Powers Act became law
    at the end of last year, and gives spies the power to read through everyone’s
    entire internet history*'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*英国政府悄然发动“自由攻击”同时分散人们的注意力，法律挑战活动人士表示 - 《调查权力法案》去年年底成为法律，赋予间谍阅读每个人整个互联网历史的能力*'
- en: '*Russia’s Reserve Fund down 70 percent in 2016*'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '*俄罗斯的国家储备基金在2016年下跌70%*'
- en: '*Russian diplomat found dead in Athens*'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '*在雅典发现一名俄罗斯外交官死亡*'
- en: '*At least 21 people have been killed (most were civilians) and 45 wounded in
    twin bombings near the Afghan parliament in Kabul*'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '*在喀布尔阿富汗议会的附近发生双爆炸事件，造成至少21人死亡（其中大多数是平民）和45人受伤*'
- en: '*Pound’s Decline Deepens as Currency Reclaims Dubious Honor*'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '*英镑贬值加剧，货币重获可疑荣誉*'
- en: World news isn't usually the most optimistic of places, but it does give insight
    into what is going on around the world, and trends on this subreddit are usually
    indicative of trends in the world.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 世界新闻通常不是最乐观的地方，但它确实能让我们了解世界各地正在发生的事情，以及这个subreddit上的趋势通常能反映出全球趋势。
- en: Extracting text from arbitrary websites
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从任意网站上提取文本
- en: The links that we get from reddit go to arbitrary websites run by many different
    organizations. To make it harder, those pages were designed to be read by a human,
    not a computer program. This can cause a problem when trying to get the actual
    content/story of those results, as modern websites have a lot going on in the
    background. JavaScript libraries are called, style sheets are applied, advertisements
    are loaded using AJAX, extra content is added to sidebars, and various other things
    are done to make the modern web page a complex document. These features make the
    modern Web what it is, but make it difficult to automatically get good information
    from!
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从reddit获得的链接指向由许多不同组织运行的任意网站。为了使问题更加复杂，这些页面是为了让人阅读而不是让计算机程序阅读。当试图获取这些结果的实际内容/故事时，这可能会引起问题，因为现代网站在后台有很多活动。JavaScript库被调用，样式表被应用，广告通过AJAX加载，侧边栏中添加了额外的内容，以及进行各种其他操作，使现代网页成为一个复杂的文档。这些功能使现代Web成为它现在这样，但同时也使得从现代网页中自动获取好的信息变得困难！
- en: Finding the stories in arbitrary websites
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在任意网站上寻找故事
- en: 'To start with, we will download the full web page from each of these links
    and store them in our data folder, under a raw subfolder. We will process these
    to extract the useful information later on. This caching of results ensures that
    we don''t have to continuously download the websites while we are working. First,
    we set up the data folder path:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从每个链接下载完整的网页，并将它们存储在我们的数据文件夹中，在raw子文件夹下。我们将在稍后处理这些内容以提取有用的信息。这种结果的缓存确保了在我们工作时不需要持续下载网站。首先，我们设置数据文件夹路径：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: We are going to use MD5 hashing to create unique filenames for our articles,
    by hashing the URL, and we will import `hashlib` to do that. A `hash` function
    is a function that converts some input (in our case a string containing the title)
    into a string that is seemingly random. The same input will always return the
    same output, but slightly different inputs will return drastically different outputs.
    It is also impossible to go from a hash value to the original value, making it
    a one-way function.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MD5散列来为我们的文章创建唯一的文件名，通过散列URL来实现，我们将导入`hashlib`来完成这个任务。散列函数是一个将某些输入（在我们的例子中是一个包含标题的字符串）转换为看似随机的字符串的函数。相同的输入将始终返回相同的输出，但略微不同的输入将返回截然不同的输出。从散列值到原始值也是不可能的，这使得它成为一个单向函数。
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: For this chapter's experiments, we are going to simply skip any website downloads
    that fail. In order to make sure we don't lose too much information doing this,
    we maintain a simple counter of the number of errors that occur. We are going
    to suppress any error that occurs, which could result in a systematic problem
    prohibiting downloads. If this error counter is too high, we can look at what
    those errors were and try to fix them. For example, if the computer has no internet
    access, all 500 of the downloads will fail and you should probably fix that before
    continuing!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章的实验，我们将简单地跳过任何失败的网站下载。为了确保我们不会因为这样做而丢失太多信息，我们维护一个简单的错误计数器。我们将抑制发生的任何错误，这可能导致系统性的问题，阻止下载。如果这个错误计数器太高，我们可以查看这些错误是什么，并尝试修复它们。例如，如果计算机没有互联网访问，所有500次下载都会失败，你应该在继续之前修复这个问题！
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we iterate through each of our stories, download the website, and save
    the results to a file:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们遍历我们的每个故事，下载网站，并将结果保存到文件中：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If there is an error in obtaining the website, we simply skip this website and
    keep going. This code will work on a large number of websites and that is good
    enough for our application, as we are looking for general trends and not exactness.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在获取网站时出现错误，我们简单地跳过这个网站并继续。这段代码将适用于大量网站，这对我们的应用程序来说已经足够好了，因为我们寻找的是一般趋势，而不是精确性。
- en: Note that sometimes you do care about getting 100 percent of responses, and
    you should adjust your code to accommodate more errors. Be warned though that
    there is a significant increase in effort required to create code the works reliably
    on data from the internet. The code to get those final 5 to 10 percent of websites
    will be significantly more complex.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，有时你确实在乎获得100%的响应，你应该调整你的代码以适应更多的错误。但是要警告你，要创建在互联网数据上可靠工作的代码需要付出显著的努力。获取那些最终5%到10%的网站代码将会显著更复杂。
- en: In the preceding code, we simply catch any error that happens, record the error
    and move on.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们简单地捕获发生的任何错误，记录错误然后继续。
- en: If you find that too many errors occur, change the print(e) line to just type
    raise instead. This will cause the exception to be called, allowing you to debug
    the problem.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现错误太多，将print(e)行改为仅输入raise。这将导致异常被调用，允许你调试问题。
- en: After this has completed, we will have a bunch of websites in our `raw` subfolder.
    After taking a look at these pages (open the created files in a text editor),
    you can see that the content is there but there is HTML, JavaScript, CSS code,
    as well as other content. As we are only interested in the story itself, we now
    need a way to extract this information from these different websites.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此操作后，我们将在`raw`子文件夹中有一堆网站。在查看这些页面（在文本编辑器中打开创建的文件）后，你可以看到内容在那里，但还有HTML、JavaScript、CSS代码以及其他内容。因为我们只对故事本身感兴趣，所以我们现在需要一种方法来从这些不同的网站中提取这些信息。
- en: Extracting the content
  id: totrans-111
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提取内容
- en: After we get the raw data, we need to find the story in each. There are several
    complex algorithms for doing this, as well as some simple ones. We will stick
    with a simple method here, keeping in mind that often enough, the simple algorithm
    is good enough. This is part of data mining—knowing when to use simple algorithms
    to get a job done, versus using more complicated algorithms to obtain that extra
    bit of performance.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们获取原始数据后，我们需要在每个中找到故事。为此有几种复杂的算法，以及一些简单的算法。我们将坚持使用一种简单的方法，考虑到通常情况下，简单的算法就足够了。这是数据挖掘的一部分——知道何时使用简单的算法来完成工作，以及何时使用更复杂的算法来获得额外的性能。
- en: 'First, we get a list of each of the filenames in our `raw` subfolder:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们获取`raw`子文件夹中每个文件名的列表：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Next, we create an output folder for the text-only versions that we will extract:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为将要提取的纯文本版本创建一个输出文件夹：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, we develop the code that will extract the text from the files. We will
    use the lxml library to parse the HTML files, as it has a good HTML parser that
    deals with some badly formed expressions. The code is as follows:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们开发提取文件中文本的代码。我们将使用lxml库来解析HTML文件，因为它有一个处理一些格式不良表达式的良好HTML解析器。代码如下：
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The actual code for extracting text is based on three steps:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 提取文本的实际代码基于三个步骤：
- en: We iterate through each of the nodes in the HTML file and extract the text in
    it.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们遍历HTML文件中的每个节点，并从中提取文本。
- en: We skip any node that is JavaScript, styling, or a comment, as this is unlikely
    to contain information of interest to us.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们跳过任何JavaScript、样式或注释节点，因为这些不太可能包含对我们感兴趣的信息。
- en: We ensure that the content has at least 100 characters. This is a good baseline,
    but it could be improved upon for more accurate results.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们确保内容至少有100个字符。这是一个很好的基线，但可以通过更精确的结果来改进。
- en: 'As we said before, we aren''t interested in scripts, styles, or comments. So,
    we create a list to ignore nodes of those types. Any node that has a type in this
    list will not be considered as containing the story. The code is as follows:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所说的，我们对脚本、样式或注释不感兴趣。因此，我们创建了一个列表来忽略那些类型的节点。任何具有此列表中类型的节点都不会被视为包含故事。代码如下：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We will now create a function that parses an HTML file into an lxml `etree`,
    and then we will create another function that parses this tree looking for text.
    This first function is pretty straightforward; simply open the file and create
    a tree using the lxml library''s parsing function for HTML files. The code is
    as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将创建一个函数，该函数将解析一个HTML文件到lxml的`etree`中，然后我们将创建另一个函数来解析这个树，寻找其中的文本。这个第一个函数相当直接；只需打开文件并使用lxml库的HTML文件解析函数创建一个树。代码如下：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the last line of that function, we call the `getroot()` function to get the
    root node of the tree, rather than the full `etree`. This allows us to write our
    text extraction function to accept any node, and therefore write a recursive function.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在该函数的最后一条语句中，我们调用`getroot()`函数来获取树的根节点，而不是完整的`etree`。这使得我们可以编写我们的文本提取函数以接受任何节点，因此可以编写一个递归函数。
- en: This function will call itself on any child nodes to extract the text from them,
    and then return the concatenation of any child nodes text.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数将在任何子节点上调用自身以提取其文本，然后返回任何子节点文本的连接。
- en: If the node where this function is passed doesn't have any child nodes, we just
    return the text from it. If it doesn't have any text, we just return an empty
    string. Note that we also check here for our third condition—that the text is
    at least 100 characters long.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果传递给此函数的节点没有子节点，我们只需返回其文本。如果没有文本，我们只返回一个空字符串。注意，我们在这里也检查了第三个条件——文本至少有100个字符长。
- en: 'The code for checking that the text is at least 100 characters long is as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 检查文本至少有100个字符长的代码如下：
- en: '[PRE23]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: At this point, we know that the node has child nodes, so we recursively call
    this function on each of those child nodes and then join the results when they
    return.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们知道该节点有子节点，因此我们递归地调用此函数在每个子节点上，然后在它们返回时连接结果。
- en: The final condition inside the return line stops blank lines being returned
    (for example, when a node has no children and no text). We also use a generator,
    which makes the code more efficient by only grabbing text data when it is needed,
    namely the final return statement rather than creating a number of sub-lists.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 返回行中的最终条件阻止空白行被返回（例如，当一个节点没有子节点和文本时）。我们还使用了一个生成器，这使得代码更高效，因为它只在需要时获取文本数据，即最终的返回语句，而不是创建多个子列表。
- en: 'We can now run this code on all of the raw HTML pages by iterating through
    them, calling the text extraction function on each, and saving the results to
    the text-only subfolder:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过遍历所有原始HTML页面来运行此代码，对每个页面调用文本提取函数，并将结果保存到纯文本子文件夹中：
- en: '[PRE24]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: You can evaluate the results manually by opening each of the files in the text
    only subfolder and checking their content. If you find too many of the results
    have non-story content, try increasing the minimum-100-character limit. If you
    still can't get good results, or need better results for your application, try
    the methods listed in *Appendix A, Next Steps.*
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过打开纯文本子文件夹中的每个文件并检查其内容来手动评估结果。如果您发现结果中有太多非故事内容，尝试增加最小100字符限制。如果您仍然无法获得良好的结果，或者需要为您的应用程序获得更好的结果，请尝试在*附录A，下一步*中列出的方法。
- en: Grouping news articles
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组新闻文章
- en: The aim of this chapter is to discover trends in news articles by clustering,
    or grouping, them together. To do that, we will use the k-means algorithm, a classic
    machine learning algorithm originally developed in 1957.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是通过聚类或分组来发现新闻文章的趋势。为此，我们将使用k-means算法，这是一种经典的机器学习算法，最初于1957年开发。
- en: '**Clustering** is an unsupervised learning technique and we often use clustering
    algorithms for exploring data. Our dataset contains approximately 500 stories
    and it would be quite arduous to examine each of those stories individually. Using
    clustering allows us to group similar stories together, and we can explore the
    themes in each cluster independently.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '**聚类**是一种无监督学习技术，我们经常使用聚类算法来探索数据。我们的数据集包含大约500个故事，逐一检查这些故事将非常费力。使用聚类使我们能够将相似的故事分组在一起，并且我们可以独立地探索每个簇的主题。'
- en: We use clustering techniques when we don't have a clear set of target classes
    for our data. In that sense, clustering algorithms have little direction in their
    learning. They learn according to some function, regardless of the underlying
    meaning of the data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们没有明确的数据目标类别时，我们会使用聚类技术。从这个意义上说，聚类算法在他们的学习中几乎没有方向。它们根据某个函数学习，而不考虑数据的潜在含义。
- en: For this reason, it is critical to choose good features. In supervised learning,
    if you choose poor features, the learning algorithm can choose to not use those
    features. For instance, support vector machines will give little weight to features
    that aren't useful in classification. However, with clustering, all features are
    used in the final result—even if those features don't provide us with the answer
    we were looking for.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，选择好的特征至关重要。在监督学习中，如果你选择了不好的特征，学习算法可以选择不使用这些特征。例如，支持向量机会给在分类中无用的特征赋予很小的权重。然而，在聚类中，所有特征都会用于最终结果——即使这些特征没有为我们提供我们想要的答案。
- en: When performing cluster analysis on real-world data, it is always a good idea
    to have a sense of what sorts of features will work for your scenario. In this
    chapter, we will use the bag-of-words model. We are looking for topic-based groups,
    so we will use topic-based features to model the documents. We know those features
    work because of the work others have done in supervised versions of our problem.
    In contrast, if we were to perform an authorship-based clustering, we would use
    features such as those found in the [Chapter 9](lrn-dtmn-py-2e_ch09.html)*, Authorship
    Attribution*, experiment.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在对现实世界数据进行聚类分析时，始终有一个对场景中哪些特征将起作用的感觉是个好主意。在本章中，我们将使用词袋模型。我们寻找基于主题的组，因此我们将使用基于主题的特征来建模文档。我们知道这些特征有效，是因为其他人已经在我们的问题的监督版本中进行了相关工作。相比之下，如果我们进行基于作者身份的聚类，我们会使用诸如[第9章](lrn-dtmn-py-2e_ch09.html)中提到的“作者归属”实验中发现的那些特征。
- en: The k-means algorithm
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means算法
- en: The k-means clustering algorithm finds centroids that best represent the data
    using an iterative process. The algorithm starts with a predefined set of centroids,
    which are normally data points taken from the training data. The **k** in k-means
    is the number of centroids to look for and how many clusters the algorithm will
    find. For instance, setting k to 3 will find three clusters in the dataset.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类算法通过迭代过程找到最能代表数据的质心。算法从一个预定义的质心集合开始，这些质心通常是来自训练数据的数据点。k-means中的**k**表示要寻找的质心数量以及算法将找到多少个簇。例如，将k设置为3将在数据集中找到三个簇。
- en: 'There are two phases to the k-means: **assignment** and **updating**. They
    are explained as below:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: k-means有两个阶段：**分配**和**更新**。它们解释如下：
- en: In the **assignment** step, we set a label to every sample in the dataset linking
    it to the nearest centroid. For each sample nearest to centroid 1, we assign the
    label 1\. For each sample nearest to centroid 2, we assign a label 2 and so on
    for each of the k centroids. These labels form the clusters, so we say that each
    data point with the label 1 is in cluster 1 (at this time only, as assignments
    can change as the algorithm runs).
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**分配**步骤中，我们为数据集中的每个样本设置一个标签，将其与最近的质心联系起来。对于每个最接近质心1的样本，我们分配标签1。对于每个最接近质心2的样本，我们分配标签2，依此类推，直到k个质心。这些标签形成了簇，所以我们说带有标签1的每个数据点都在簇1中（在这个时候，因为分配可以在算法运行时改变）。
- en: In the **updating** step, we take each of the clusters and compute the centroid,
    which is the mean of all of the samples in that cluster.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**更新**步骤中，我们取每个簇并计算其质心，即该簇中所有样本的平均值。
- en: The algorithm then iterates between the assignment step and the updating step;
    each time the updating step occurs, each of the centroids moves a small amount.
    This causes the assignments to change slightly, causing the centroids to move
    a small amount in the next iteration. This repeats until some stopping criterion
    is reached.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 算法随后在分配步骤和更新步骤之间迭代；每次更新步骤发生时，每个质心都会移动一小段距离。这导致分配略有变化，导致质心在下一迭代中移动一小段距离。这个过程会重复，直到达到某个停止标准。
- en: It is common to stop after a certain number of iterations, or when the total
    movement of the centroids is very low. The algorithm can also complete in some
    scenarios, which means that the clusters are stable—the assignments do not change
    and neither do the centroids.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 常常在迭代一定次数后停止，或者当质心的总移动量非常低时停止。在某些情况下，算法也可以完成，这意味着聚类是稳定的——分配没有变化，质心也没有变化。
- en: In the following figure, k-means was performed over a dataset created randomly,
    but with three clusters in the data. The stars represent the starting location
    of the centroids, which were chosen randomly by picking a random sample from the
    dataset. Over 5 iterations of the k-means algorithm, the centroids move to the
    locations represented by the triangles.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的图中，k-means是在一个随机创建但包含三个聚类的数据集上进行的。星星代表质心的起始位置，这些位置是通过从数据集中随机选择一个样本来随机选择的。经过k-means算法的5次迭代，质心移动到了由三角形表示的位置。
- en: '![](img/B06162_10_01.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_01.png)'
- en: The k-means algorithm is fascinating for its mathematical properties and historical
    significance. It is an algorithm that (roughly) only has a single parameter, and
    is quite effective and frequently used, even more than 50 years after its discovery.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法因其数学特性和历史意义而令人着迷。这是一个（大致上）只有一个参数的算法，非常有效且经常使用，即使在其发现50多年后。
- en: 'There is a k-means algorithm in scikit-learn, which we import from the `cluster` module
    in scikit-learn:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中有一个k-means算法，我们是从scikit-learn的`cluster`模块中导入的：
- en: '[PRE25]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We also import the `CountVectorizer` class''s close cousin, `TfidfVectorizer`.
    This vectorizer applies a weighting to each term''s counts, depending on how many
    documents it appears in, using the equation: tf / log(df), where tf is a term''s
    frequency (how many times it appears in the current document) and df is the term''s
    document frequency (how many documents in our corpus it appears in). Terms that
    appear in many documents are weighted lower (by dividing the value by the log
    of the number of documents it appears in). For many text mining applications,
    using this type of weighting scheme can improve performance quite reliably. The
    code is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还引入了`CountVectorizer`类的近亲，`TfidfVectorizer`。这个向量器为每个词项的计数应用权重，这取决于它在多少个文档中出现的次数，使用以下公式：tf
    / log(df)，其中tf是词项的频率（在当前文档中出现的次数）和df是词项的文档频率（在我们的语料库中出现的文档数）。在许多文档中出现的词项被赋予较低的权重（通过除以它出现的文档数的对数）。对于许多文本挖掘应用，使用这种类型的加权方案可以相当可靠地提高性能。代码如下：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We then set up our pipeline for our analysis. This has two steps. The first
    is to apply our vectorizer, and the second is to apply our k-means algorithm.
    The code is as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后设置我们的分析流程。这有两个步骤。第一步是应用我们的向量器，第二步是应用我们的k-means算法。代码如下：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: The `max_df` parameter is set to a low value of 0.4, which says ignore any word
    that occurs in more than 40 percent of documents. This parameter is invaluable
    for removing function words that give little topic-based meaning on their own.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_df`参数被设置为0.4的低值，这意味着忽略任何在超过40%的文档中出现的单词。这个参数对于移除那些本身对主题意义贡献不大的功能词来说是无价的。'
- en: Removing any word that occurs in more than 40 percent of documents will remove
    function words, making this type of preprocessing quite useless for the work we
    saw in [Chapter 9](lrn-dtmn-py-2e_ch09.html)*, Authorship Attribution*.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 移除在超过40%的文档中出现的任何单词将移除功能词，这使得这种类型的预处理对我们在第9章中看到的工作来说相当无用，[第9章](lrn-dtmn-py-2e_ch09.html)*，作者归属*。
- en: '[PRE28]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then fit and predict this pipeline. We have followed this process a number
    of times in this book so far for classification tasks, but there is a difference
    here—we do not give the target classes for our dataset to the fit function. This
    is what makes this an unsupervised learning task! The code is as follows:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后拟合并预测这个流程。到目前为止，我们已经在本书中多次遵循这个过程进行分类任务，但这里有一个区别——我们不将数据集的目标类别提供给fit函数。这就是使其成为无监督学习任务的原因！代码如下：
- en: '[PRE29]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The labels variable now contains the cluster numbers for each sample. Samples
    with the same label are said to belong to the same cluster. It should be noted
    that the cluster labels themselves are meaningless: clusters 1 and 2 are no more
    similar than clusters 1 and 3.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '`labels`变量现在包含每个样本的簇编号。具有相同标签的样本被认为属于同一簇。需要注意的是，簇标签本身没有意义：簇1和2与簇1和3的相似度没有区别。'
- en: 'We can see how many samples were placed in each cluster using the `Counter`
    class:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`Counter`类查看每个簇中放置了多少个样本：
- en: '[PRE30]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Many of the results (keeping in mind that your dataset will be quite different
    to mine) consist of a large cluster with the majority of instances, several medium
    clusters, and some clusters with only one or two instances. This imbalance is
    quite normal in many clustering applications.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 许多结果（记住你的数据集将与我的大相径庭）由一个包含大多数实例的大簇、几个中等大小的簇以及一些只有一个或两个实例的簇组成。这种不平衡在许多聚类应用中相当正常。
- en: Evaluating the results
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估结果
- en: Clustering is mainly an exploratory analysis, and therefore it is difficult
    to evaluate a clustering algorithm's results effectively. A straightforward way
    is to evaluate the algorithm based on the criteria the algorithm tries to learn
    from.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析主要是一种探索性分析，因此很难有效地评估聚类算法的结果。一种直接的方法是根据算法试图学习的标准来评估算法。
- en: If you have a test set, you can evaluate clustering against it. For more details,
    visit [http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个测试集，你可以将其与聚类进行评估。更多详情，请访问[http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html](http://nlp.standford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html)
- en: 'In the case of the k-means algorithm, the criterion that it uses when developing
    the centroids is to minimize the distance from each sample to its nearest centroid.
    This is called the inertia of the algorithm and can be retrieved from any KMeans
    instance that has had fit called on it:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means算法的情况下，它在开发质心时使用的标准是最小化每个样本到其最近质心的距离。这被称为算法的惯性，可以从任何已经调用fit的KMeans实例中检索到：
- en: '[PRE31]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The result on my dataset was 343.94\. Unfortunately, this value is quite meaningless
    by itself, but we can use it to determine how many clusters we should use. In
    the preceding example, we set `n_clusters` to 10, but is this the best value?
    The following code runs the k-means algorithm 10 times with each value of `n_clusters`
    from 2 to 20, taking some time to complete the large number of runs.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的数据集上的结果是343.94。不幸的是，这个值本身相当没有意义，但我们可以用它来确定我们应该使用多少个簇。在先前的例子中，我们将`n_clusters`设置为10，但这真的是最佳值吗？以下代码运行k-means算法10次，每次`n_clusters`的值从2到20，需要一些时间来完成大量运行。
- en: For each run, it records the inertia of the result.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每次运行，它记录结果的惯性。
- en: You may notice the following code that we don't use a Pipeline; instead, we
    split out the steps. We only create the X matrix from our text documents once
    per value of `n_clusters` to (drastically) improve the speed of this code.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到以下代码，我们并没有使用Pipeline；相反，我们将步骤拆分出来。我们只从我们的文本文档中创建一次X矩阵，每次`n_clusters`的值变化时，以（大幅）提高此代码的速度。
- en: '[PRE32]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The`inertia_scores` variable now contains a list of inertia scores for each
    n_clusters value between 2 and 20\. We can plot this to get a sense of how this
    value interacts with `n_clusters`:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '`inertia_scores`变量现在包含从2到20个`n_clusters`值对应的惯性分数列表。我们可以绘制这些值以了解这个值如何与`n_clusters`相互作用：'
- en: '[PRE33]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](img/B06162_10_02.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_02.png)'
- en: Overall, the value of the inertia should decrease with reducing improvement
    as the number of clusters improves, which we can broadly see from these results.
    The increase between values of 6 to 7 is due only to the randomness in selecting
    the centroids, which directly affect how good the final results are. Despite this,
    there is a general trend (for my data; your results may vary) that about 6 clusters
    was the last time a major improvement in the inertia occurred.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，随着簇数的增加，惯性值应该随着改进的减少而降低，我们可以从这些结果中大致看出这一点。从6到7的值增加仅由于选择质心的随机性，这直接影响了最终结果的好坏。尽管如此，对于我的数据来说（你的结果可能会有所不同），大约6个簇是惯性发生重大改进的最后一次。
- en: After this point, only slight improvements are made to the inertia, although
    it is hard to be specific about vague criteria such as this. Looking for this
    type of pattern is called the elbow rule, in that we are looking for an elbow-esque
    bend in the graph. Some datasets have more pronounced elbows, but this feature
    isn't guaranteed to even appear (some graphs may be smooth!).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点之后，惯性只有轻微的改进，尽管很难具体说明这种模糊的标准。寻找这种类型模式被称为肘部规则，因为我们正在寻找图表中的肘部弯曲。一些数据集有更明显的肘部，但这个特征并不保证出现（某些图表可能是平滑的！）
- en: 'Based on this analysis, we set `n_clusters` to be 6 and then rerun the algorithm:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这种分析，我们将`n_clusters`设置为6，然后重新运行算法：
- en: '[PRE34]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Extracting topic information from clusters
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从聚类中提取主题信息
- en: Now we set our sights on the clusters in an attempt to discover the topics in
    each.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将目光投向聚类，试图发现每个聚类中的主题。
- en: 'We first extract the term list from our feature extraction step:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从特征提取步骤中提取术语列表：
- en: '[PRE35]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'We also set up another counter for counting the size of each of our classes:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还设置了一个计数器来统计每个类的大小：
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Iterating over each cluster, we print the size of the cluster as before.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 遍历每个聚类，我们像以前一样打印出聚类的大小。
- en: It is important to keep in mind the sizes of the clusters when evaluating the
    results—some of the clusters will only have one sample, and are therefore not
    indicative of a general trend.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估结果时，重要的是要记住聚类的大小——一些聚类可能只有一个样本，因此不能代表一般趋势。
- en: Next (and still in the loop), we iterate over the most important terms for this
    cluster. To do this, we take the five largest values from the centroid, which
    we get by finding the features that have the highest values in the centroid itself.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来（仍然在循环中），我们遍历这个聚类最重要的术语。为此，我们从质心中找到具有最高值的特征，然后取这五个最大值。
- en: '[PRE37]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The results can be quite indicative of current trends. In my results (obtained
    January 2017), the clusters correspond to health matters, Middle East tensions,
    Korean tensions, and Russian affairs. These were the main topics frequenting news
    around this time—although this has hardly changed for a number of years!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可以相当直观地反映当前趋势。在我的结果（2017年1月获得）中，聚类对应于健康问题、中东紧张局势、韩国紧张局势和俄罗斯事务。这些是当时新闻报道的主要话题——尽管这些年来几乎没有变化！
- en: 'You might notice some words that don''t provide much value come out on top,
    such as *you, her* and *mr.* These function words are great for authorship analysis
    - as we saw in [Chapter 9](lrn-dtmn-py-2e_ch09.html), *Authorship Attribution*,
    but are not generally very good for topic analysis. Passing the list of function
    words into the `stop_words` parameter of the **TfidfVectorizer** in our pipeline
    above will ignore those words. Here is the updated code for building such a pipeline:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到一些没有提供太多价值的词出现在顶部，例如 *你*、*她* 和 *mr.* 这些功能词对于作者身份分析很有用——正如我们在第9章中看到的，*作者归属*，但通常对于主题分析并不很好。将功能词列表传递到我们管道上**TfidfVectorizer**的`stop_words`参数将忽略这些词。以下是构建此类管道的更新代码：
- en: '[PRE38]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Using clustering algorithms as transformers
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将聚类算法作为转换器
- en: As a side note, one interesting property about the k-means algorithm (and any
    clustering algorithm) is that you can use it for feature reduction. There are
    many methods to reduce the number of features (or create new features to embed
    the dataset on), such as **Principle Component Analysis**, **Latent Semantic Indexing**,
    and many others. One issue with many of these algorithms is that they often need
    lots of computing power.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个旁注，关于k-means算法（以及任何聚类算法）的一个有趣特性是，你可以用它来进行特征降维。有许多方法可以减少特征数量（或创建新特征以嵌入数据集），例如**主成分分析**、**潜在语义索引**以及许多其他方法。这些算法中许多存在的问题是它们通常需要大量的计算能力。
- en: In the preceding example, the terms list had more than 14,000 entries in it—it
    is quite a large dataset. Our k-means algorithm transformed these into just six
    clusters. We can then create a dataset with a much lower number of features by
    taking the distance to each centroid as a feature.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，术语列表中有超过14,000个条目——这是一个相当大的数据集。我们的k-means算法将这些转换成仅仅六个聚类。然后我们可以通过将每个质心的距离作为特征来创建一个具有更低特征数量的数据集。
- en: 'To do this, we call the transform function on a KMeans instance. Our pipeline
    is fit for this purpose, as it has a k-means instance at the end:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们调用KMeans实例的transform函数。我们的管道适合这个目的，因为它在末尾有一个k-means实例：
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This calls the transform method on the final step of the pipeline, which is
    an instance of k-means. This results in a matrix that has six features and the
    number of samples is the same as the length of documents.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这在管道的最后一步调用转换方法，这是一个 k-means 的实例。这导致一个具有六个特征和样本数量与文档长度相同的矩阵。
- en: You can then perform your own second-level clustering on the result, or use
    it for classification if you have the target values. A possible workflow for this
    would be to perform some feature selection using the supervised data, use clustering
    to reduce the number of features to a more manageable number, and then use the
    results in a classification algorithm such as SVMs.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以对结果进行自己的二级聚类，或者如果你有目标值，可以使用它进行分类。这个工作流程的一个可能方案是使用监督数据执行一些特征选择，使用聚类将特征数量减少到更易于管理的数量，然后使用分类算法（如
    SVMs）的结果。
- en: Clustering ensembles
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类集成
- en: In [Chapter 3](lrn-dtmn-py-2e_ch03.html)*, Predicting Sports Winners with Decision
    Trees*, we looked at a classification ensemble using the random forest algorithm,
    which is an ensemble of many low-quality, tree-based classifiers. Ensembling can
    also be performed using clustering algorithms. One of the key reasons for doing
    this is to smooth the results from many runs of an algorithm. As we saw before,
    the results from running k-means are varied, depending on the selection of the
    initial centroids. Variation can be reduced by running the algorithm many times
    and then combining the results.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 3 章](lrn-dtmn-py-2e_ch03.html)*，使用决策树预测体育比赛胜者* 中，我们研究了使用随机森林算法的分类集成，随机森林算法是由许多低质量的基于树的分类器组成的集成。集成也可以使用聚类算法来执行。这样做的一个关键原因是为了平滑算法多次运行的结果。正如我们之前看到的，k-means
    运行的结果因初始质心的选择而异。通过多次运行算法并合并结果可以减少这种变化。
- en: Ensembling also reduces the effects of choosing parameters on the final result.
    Most clustering algorithms are quite sensitive to the parameter values chosen
    for the algorithm. Choosing slightly different parameters results in different
    clusters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 集成还可以减少选择参数对最终结果的影响。大多数聚类算法对算法选择的参数值非常敏感。选择略微不同的参数会导致不同的聚类。
- en: Evidence accumulation
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 证据累积
- en: As a basic ensemble, we can first cluster the data many times and record the
    labels from each run. We then record how many times each pair of samples was clustered
    together in a new matrix. This is the essence of the **Evidence Accumulation Clustering**
    (**EAC**) algorithm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种基本集成，我们首先多次对数据进行聚类并记录每次运行的标签。然后我们在一个新的矩阵中记录每对样本一起聚类的次数。这是 **证据累积聚类**（**EAC**）算法的本质。
- en: EAC has two major steps.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: EAC 有两个主要步骤。
- en: The first step is to cluster the data many times using a lower-level clustering
    algorithm, such as k-means and record the frequency that samples were in the same
    cluster, in each iteration. This is stored in a **co-association matrix**.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是多次使用低级聚类算法（如 k-means）对数据进行聚类，并记录每个迭代中样本位于同一聚类的频率。这被存储在一个 **共关联矩阵** 中。
- en: The second step is to perform a cluster analysis on the resulting co-association
    matrix, which is performed using another type of clustering algorithm called hierarchical
    clustering. This has an interesting graph-theory-based property, as it is mathematically
    the same as finding a tree that links all the nodes together and removing weak
    links.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二步是对生成的共关联矩阵进行聚类分析，这使用另一种称为层次聚类的聚类算法来完成。它具有一个有趣的基于图论的性质，因为它在数学上等同于找到一个连接所有节点的树并移除弱连接。
- en: 'We can create a co-association matrix from an array of labels by iterating
    over each of the labels and recording where two samples have the same label. We
    use SciPy''s `csr_matrix`, which is a type of sparse matrix:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过遍历每个标签并记录两个样本具有相同标签的位置，从标签数组中创建一个共关联矩阵。我们使用 SciPy 的 `csr_matrix`，这是一种稀疏矩阵：
- en: '[PRE40]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Our function definition takes a set of labels and then record the rows and
    columns of each match. We do these in a list. Sparse matrices are commonly just
    sets of lists recording the positions of nonzero values, and `csr_matrix` is an
    example of this type of sparse matrix. For each pair of samples with the same
    label, we record the position of both samples in our list:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能定义接受一组标签，然后记录每个匹配的行和列。我们在列表中这样做。稀疏矩阵通常只是记录非零值位置的列表集合，`csr_matrix` 就是这种稀疏矩阵的一个例子。对于具有相同标签的每对样本，我们在列表中记录这两个样本的位置：
- en: '[PRE41]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'To get the co-association matrix from the labels, we simply call this function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 要从标签中获得共关联矩阵，我们只需调用此函数：
- en: '[PRE42]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: From here, we can add multiple instances of these matrices together. This allows
    us to combine the results from multiple runs of k-means. Printing out `C` (just
    enter C into a new cell of your Jupyter Notebook and run it) will tell you how
    many cells have nonzero values in them. In my case, about half of the cells had
    values in them, as my clustering result had a large cluster (the more even the
    clusters, the lower the number of nonzero values).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里，我们可以将这些矩阵的多个实例相加。这使我们能够结合 k-means 多次运行的结果。打印出 `C`（只需在 Jupyter Notebook 的新单元格中输入
    C 并运行即可）将告诉你有多少单元格中有非零值。在我的情况下，大约一半的单元格中有值，因为我的聚类结果有一个大簇（簇越均匀，非零值的数量就越低）。
- en: The next step involves the hierarchical clustering of the co-association matrix.
    We will do this by finding minimum spanning trees on this matrix and removing
    edges with a weight lower than a given threshold.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步涉及共关联矩阵的层次聚类。我们将通过在这个矩阵上找到最小生成树并移除低于给定阈值的边来完成此操作。
- en: In graph theory, a spanning tree is a set of edges on a graph that connects
    all of the nodes together. The **Minimum Spanning Tree** (MST) is simply the spanning
    tree with the lowest total weight. For our application, the nodes in our graph
    are samples from our dataset, and the edge weights are the number of times those
    two samples were clustered together—that is, the value from our co-association
    matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在图论中，生成树是连接图中所有节点的边集。**最小生成树**（MST）只是具有最低总权重的生成树。在我们的应用中，我们的图中的节点来自我们的数据集的样本，边权重是这两个样本聚集在一起的次数——即我们的共关联矩阵中的值。
- en: In the following figure, a MST on a graph of six nodes is shown. Nodes on the
    graph can be connected to more than once in the MST, as long as all nodes are
    connected together.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，展示了六个节点的图上的最小生成树。在 MST 中，图上的节点可以多次连接，只要所有节点都连接在一起。
- en: '![](img/B06162_10_03.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B06162_10_03.png)'
- en: 'To compute the MST, we use SciPy''s `minimum_spanning_tree` function, which
    is found in the sparse package:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 MST，我们使用 SciPy 的 `minimum_spanning_tree` 函数，该函数位于稀疏包中：
- en: '[PRE43]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The `mst` function can be called directly on the sparse matrix returned by
    our co-association function:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接在我们的共关联函数返回的稀疏矩阵上调用 `mst` 函数：
- en: '[PRE44]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'However, in our co-association matrix C, higher values are indicative of samples
    that are clustered together more often—a similarity value. In contrast, `minimum_spanning_tree`
    sees the input as a distance, with higher scores penalized. For this reason, we
    compute the minimum spanning tree on the negation of the co-association matrix
    instead:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在我们的共关联矩阵 C 中，更高的值表示经常聚集在一起的样本——这是一个相似度值。相比之下，`minimum_spanning_tree` 将输入视为距离，对得分较高的进行惩罚。因此，我们在共关联矩阵的否定上计算最小生成树：
- en: '[PRE45]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The result from the preceding function is a matrix the same size as the co-association
    matrix (the number of rows and columns is the same as the number of samples in
    our dataset), with only the edges in the MST kept and all others removed.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个函数的结果是一个与共关联矩阵大小相同的矩阵（行数和列数与我们的数据集中的样本数量相同），只保留最小生成树中的边，并移除所有其他边。
- en: 'We then remove any node with a weight less than a predefined threshold. To
    do this, we iterate over the edges in the MST matrix, removing any that are less
    than a specific value. We can''t test this out with just a single iteration in
    a co-association matrix (the values will be either 1 or 0, so there isn''t much
    to work with). So, we will create extra labels first, create the co-association
    matrix, and then add the two matrices together. The code is as follows:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后移除任何权重小于预定义阈值的节点。为此，我们遍历 MST 矩阵中的边，移除任何小于特定值的边。我们无法仅通过在共关联矩阵中迭代一次来测试这一点（值将是
    1 或 0，因此没有太多可以工作的）。因此，我们首先创建额外的标签，创建共关联矩阵，然后将两个矩阵相加。代码如下：
- en: '[PRE46]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then compute the MST and remove any edge that didn''t occur in both of these
    labels:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随后计算最小生成树（MST），并移除在这两个标签中都没有出现的任何边：
- en: '[PRE47]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The threshold we wanted to cut off was any edge not in both clusterings—that
    is, with a value of 1\. However, as we negated the co-association matrix, we had
    to negate the threshold value too.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要截断的阈值是任何不在两个聚类中的边——即值为 1 的边。然而，由于我们否定了共关联矩阵，我们也必须否定阈值值。
- en: 'Lastly, we find all of the connected components, which is simply a way to find
    all of the samples that are still connected by edges after we removed the edges
    with low weights. The first returned value is the number of connected components
    (that is, the number of clusters) and the second is the labels for each sample.
    The code is as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们找到所有的连通分量，这仅仅是一种找到所有在移除低权重边后仍然通过边连接的样本的方法。第一个返回值是连通分量的数量（即簇的数量），第二个是每个样本的标签。代码如下：
- en: '[PRE48]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: In my dataset, I obtained eight clusters, with the clusters being approximately
    the same as before. This is hardly a surprise, given we only used two iterations
    of k-means; using more iterations of k-means (as we do in the next section) will
    result in more variance.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的数据集中，我获得了八个簇，簇与之前的大致相同。这几乎不足为奇，因为我们只使用了k-means的两次迭代；使用更多的k-means迭代（如我们在下一节中做的那样）将导致更大的方差。
- en: How it works
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 它是如何工作的
- en: 'In the k-means algorithm, each feature is used without any regard to its weight.
    In essence, all features are assumed to be on the same scale. We saw the problems
    with not scaling features in [Chapter 2](lrn-dtmn-py-2e_ch10.html)*, Classification
    with scikit-learn Estimators*. The result of this is that k-means is looking for
    circular clusters, visualized here:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means算法中，每个特征都是无差别地使用的。本质上，所有特征都被假定为处于相同的尺度。我们在[第2章](lrn-dtmn-py-2e_ch10.html)*，使用scikit-learn估计器的分类*中看到了不缩放特征的问题。结果是k-means正在寻找圆形簇，如下所示：
- en: '![](img/B06162_10_04.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_10_04.png)'
- en: 'Oval shaped clusters can also be discovered by k-means. The separation usually
    isn''t quite so smooth, but can be made easier with feature scaling. An example
    of this shaped cluster is as follows:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: k-means也可以发现椭圆形簇。分离通常并不那么平滑，但可以通过特征缩放来简化。以下是一个这种形状簇的例子：
- en: '![](img/B06162_10_05.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_10_05.png)'
- en: As we can see in the preceding screenshot, not all clusters have this shape.
    The blue cluster is circular and is of the type that k-means is very good at picking
    up. The red cluster is an ellipse. The k-means algorithm can pick up clusters
    of this shape with some feature scaling.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如前一个屏幕截图所示，并非所有簇都具有这种形状。蓝色簇是圆形的，是k-means非常擅长识别的类型。红色簇是椭圆形。通过一些特征缩放，k-means可以识别这种形状的簇。
- en: 'The bellow third cluster isn''t even convex—it is an odd shape that k-means
    will have trouble discovering, but would still be considered a *cluster*, at least
    by most humans looking at the picture:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的第三个簇甚至不是凸的——它是一个k-means可能会遇到困难的奇形怪状，但仍然会被认为是*簇*，至少对于大多数看图的人来说是这样的：
- en: '![](img/B06162_10_06.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B06162_10_06.png)'
- en: Cluster analysis is a hard task, with most of the difficulty simply in trying
    to define the problem. Many people intuitively understand what it means, but trying
    to define it in precise terms (necessary for machine learning) is very difficult.
    Even people often disagree on the term!
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是一项艰巨的任务，其中大部分困难仅仅在于试图定义问题。许多人直观地理解它的含义，但试图用精确的术语来定义它（这对于机器学习是必要的）是非常困难的。甚至人们经常对这个词有不同的看法！
- en: The EAC algorithm works by remapping the features onto a new space, in essence
    turning each run of the k-means algorithm into a transformer using the same principles
    we saw the previous section using k-means for feature reduction. In this case,
    though, we only use the actual label and not the distance to each centroid. This
    is the data that is recorded in the co-association matrix.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: EAC算法通过将特征重新映射到新的空间中工作，本质上是将k-means算法的每次运行转换为一个使用我们之前章节中使用的k-means进行特征降维的转换器。然而，在这种情况下，我们只使用实际的标签，而不是到每个质心的距离。这是记录在共关联矩阵中的数据。
- en: The result is that EAC now only cares about how close things are to each other,
    not how they are placed in the original feature space. There are still issues
    around unscaled features. Feature scaling is important and should be done anyway
    (we did it using tf**-**idf in this chapter, which results in feature values having
    the same scale).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是，EAC现在只关心事物之间的接近程度，而不是它们在原始特征空间中的位置。仍然存在未缩放特征的问题。特征缩放很重要，无论如何都应该进行（我们在本章中使用tf**-**idf进行了缩放，这导致特征值具有相同的尺度）。
- en: We saw a similar type of transformation in [Chapter 9](lrn-dtmn-py-2e_ch09.html)*,
    Authorship Attribution*, through the use of kernels in SVMs. These transformations
    are very powerful and should be kept in mind for complex datasets. The algorithms
    for remapping data onto a new feature space does not need to be complex though,
    as you'll see in the EAC algorithm.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第9章](lrn-dtmn-py-2e_ch09.html)*，作者归属*中看到了类似类型的转换，通过在SVM中使用核。这些转换非常强大，应该记住用于复杂的数据集。将数据重新映射到新特征空间上的算法不需要太复杂，正如你将在EAC算法中看到的那样。
- en: Implementation
  id: totrans-250
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: Putting all this all together, we can now create a clustering algorithm fitting
    the scikit-learn interface that performs all of the steps in EAC. First, we create
    the basic structure of the class using scikit-learn's *ClusterMixin*.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们现在可以创建一个符合scikit-learn接口的聚类算法，该算法执行EAC中的所有步骤。首先，我们使用scikit-learn的*ClusterMixin*创建类的基本结构。
- en: Our parameters are the number of k-means clusterings to perform in the first
    step (to create the co-association matrix), the threshold to cut off at, and the
    number of clusters to find in each k-means clustering. We set a range of n_clusters
    in order to get lots of variance in our k-means iterations. Generally, in ensemble
    terms, variance is a good thing; without it, the solution can be no better than
    the individual clusterings (that said, high variance is not an indicator that
    the ensemble will be better).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的参数是在第一步中要执行的k-means聚类次数（用于创建共关联矩阵）、截断的阈值以及每次k-means聚类中要找到的聚类数。我们设置一个n_clusters的范围，以便在我们的k-means迭代中获得大量的方差。通常，在集成术语中，方差是一个好东西；没有它，解决方案可能不如单个聚类（尽管如此，高方差并不是集成将更好的指标）。
- en: 'I''ll present the full class first, and then overview each of the functions:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先展示完整的类，然后概述每个函数：
- en: '[PRE49]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: The goal of the `fit` function is to perform the k-means clusters a number of
    times, combine the co-association matrices and then split it by finding the MST,
    as we saw earlier with the EAC example. We then perform our low-level clustering
    using k-means and sum the resulting co-association matrices from each iteration.
    We do this in a generator to save memory, creating only the co-association matrices
    when we need them. In each iteration of this generator, we create a new single
    k-means run with our dataset and then create the co-association matrix for it.
    We use `sum` to add these together.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '`fit`函数的目标是执行k-means聚类多次，合并共关联矩阵，然后通过找到最小生成树（MST）来分割，正如我们之前在EAC示例中看到的那样。然后我们使用k-means执行我们的低级聚类，并将每次迭代的共关联矩阵相加。我们这样做是为了节省内存，只在需要时创建共关联矩阵。在这个生成器的每个迭代中，我们使用我们的数据集创建一个新的单个k-means运行，然后为其创建共关联矩阵。我们使用`sum`将这些矩阵相加。'
- en: As before, we create the MST, remove any edges less than the given threshold
    (properly negating values as explained earlier), and find the connected components.
    As with any fit function in scikit-learn, we need to return self in order for
    the class to work in pipelines effectively.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们创建最小生成树（MST），移除任何小于给定阈值的边（如前所述，正确地取反值），然后找到连通分量。与scikit-learn中的任何拟合函数一样，我们需要返回self，以便类在管道中有效地工作。
- en: The `_single_clustering` function is designed to perform a single iteration
    of k-means on our data, and then return the predicted labels. To do this, we randomly
    choose a number of clusters to find using NumPy's `randint` function and our `n_clusters_range`
    parameter, which sets the range of possible values. We then cluster and predict
    the dataset using k-means. The return value here will be the labels coming from
    k-means.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '`_single_clustering`函数旨在在我们的数据上执行一次k-means迭代，然后返回预测标签。为此，我们使用NumPy的`randint`函数和我们的`n_clusters_range`参数（设置可能值的范围）随机选择要找到的聚类数。然后我们使用k-means聚类和预测数据集。这里的返回值将是来自k-means的标签。'
- en: Finally, the `fit_predict` function simply calls fit, and then returns the labels
    for the documents.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，`fit_predict`函数简单地调用fit，然后返回文档的标签。
- en: 'We can now run this on our previous code by setting up a pipeline as before
    and using EAC where we previously used a KMeans instance as our final stage of
    the pipeline. The code is as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过设置一个管道（与之前一样）并使用EAC（而不是之前作为管道最终阶段的KMeans实例）来运行此代码。代码如下：
- en: '[PRE50]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Online learning
  id: totrans-261
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在线学习
- en: In some cases, we don't have all of the data we need for training before we
    start our learning. Sometimes, we are waiting for new data to arrive, perhaps
    the data we have is too large to fit into memory, or we receive extra data after
    a prediction has been made. In cases like these, online learning is an option
    for training models over time.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，在我们开始学习之前，我们没有所有需要的训练数据。有时，我们正在等待新数据的到来，也许我们拥有的数据太大，无法放入内存，或者在一个预测之后收到了额外的数据。在这些情况下，在线学习是随着时间训练模型的一个选项。
- en: '**Online learning** is the incremental updating of a model as new data arrives.
    Algorithms that support online learning can be trained on one or a few samples
    at a time, and updated as new samples arrive. In contrast, algorithms that are
    not **online** require access to all of the data at once. The standard k-means
    algorithm is like this, as are most of the algorithms we have seen so far in this
    book.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '**在线学习**是新数据到达时模型的增量更新。支持在线学习的算法可以一次训练一个或几个样本，并在新样本到达时进行更新。相比之下，不支持在线的算法需要一次性访问所有数据。标准的k-means算法就是这样，本书中我们迄今为止看到的大多数算法也是如此。'
- en: Online versions of algorithms have a means to partially update their model with
    only a few samples. Neural networks are a standard example of an algorithm that
    works in an online fashion. As a new sample is given to the neural network, the
    weights in the network are updated according to a learning rate, which is often
    a very small value such as 0.01\. This means that any single instance only makes
    a small (but hopefully improving) change to the model.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的在线版本有一种方法，只需使用少量样本就可以部分更新其模型。神经网络是按在线方式工作的算法的标准示例。当一个新的样本被提供给神经网络时，网络中的权重会根据学习率进行更新，学习率通常是一个非常小的值，如0.01。这意味着任何单个实例只会对模型产生小的（但希望是改进的）变化。
- en: Neural networks can also be trained in batch mode, where a group of samples
    is given at once and the training is done in one step. Algorithms are generally
    faster in batch mode but use more memory.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络也可以以批处理模式进行训练，其中一次给出一个样本组，并在一步中完成训练。在批处理模式下，算法通常更快，但使用更多的内存。
- en: In this same vein, we can slightly update the k-means centroids after a single
    or small batch of samples. To do this, we apply a learning rate to the centroid
    movement in the updating step of the k-means algorithm. Assuming that samples
    are randomly chosen from the population, the centroids should tend to move towards
    the positions they would have in the standard, offline, and k-means algorithm.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个意义上，我们可以在单个或小批量样本之后稍微更新k-means中心。为此，我们在k-means算法的更新步骤中应用一个学习率。假设样本是从总体中随机选择的，中心应该倾向于移动到它们在标准、离线和k-means算法中的位置。
- en: Online learning is related to streaming-based learning; however, there are some
    important differences. Online learning is capable of reviewing older samples after
    they have been used in the model, while a streaming-based machine learning algorithm
    typically only gets one pass—that is, one opportunity to look at each sample.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在线学习与基于流的学习相关；然而，有一些重要的区别。在线学习能够在样本被用于模型之后回顾旧样本，而基于流的机器学习算法通常只能进行一次遍历——也就是说，只有一个机会查看每个样本。
- en: Implementation
  id: totrans-268
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现
- en: The scikit-learn package contains the **MiniBatchKMeans** algorithm, which allows
    online learning. This class implements a partial_fit function, which takes a set
    of samples and updates the model. In contrast, calling fit() will remove any previous
    training and refit the model only on the new data.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn包包含**MiniBatchKMeans**算法，它允许在线学习。这个类实现了一个partial_fit函数，它接受一组样本并更新模型。相比之下，调用fit()将删除任何以前的训练，并仅在新的数据上重新拟合模型。
- en: MiniBatchKMeans follows the same clustering format as other algorithms in scikit-learn,
    so creating and using it is much the same as other algorithms.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: MiniBatchKMeans遵循scikit-learn中其他算法相同的聚类格式，因此创建和使用它与其他算法非常相似。
- en: The algorithm works by taking a streaming average of all points that it has
    seen. To compute this, we only need to keep track of two values, which are the
    current sum of all seen points, and the number of points seen. We can then use
    this information, combined with a new set of points, to compute the new averages
    in the updating step.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过计算它所看到的所有点的流平均来工作。为此，我们只需要跟踪两个值，即所有已看到点的当前总和和已看到点的数量。然后我们可以使用这些信息，结合一组新的点，在更新步骤中计算新的平均值。
- en: 'Therefore, we can create a matrix X by extracting features from our dataset
    using `TfIDFVectorizer`, and then sample from this to incrementally update our
    model. The code is as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以通过使用`TfIDFVectorizer`从我们的数据集中提取特征来创建矩阵X，然后从这个矩阵中采样以增量更新我们的模型。代码如下：
- en: '[PRE51]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We then import MiniBatchKMeans and create an instance of it:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们导入MiniBatchKMeans并创建其实例：
- en: '[PRE52]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Next, we will randomly sample from our X matrix to simulate data coming in
    from an external source. Each time we get some data in, we update the model:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将从我们的X矩阵中随机采样以模拟来自外部源的数据。每次我们获取一些数据时，我们都会更新模型：
- en: '[PRE53]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We can then get the labels for the original dataset by asking the instance
    to predict:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过请求实例进行预测来获取原始数据集的标签：
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: At this stage, though, we can't do this in a pipeline as `TfidfVectorizer` is
    not an online algorithm. To get over this, we use a `HashingVectorizer`. The `HashingVectorizer`
    class is a clever use of hashing algorithms to drastically reduce the memory of
    computing the bag-of-words model. Instead of recording the feature names, such
    as words found in documents, we record only hashes of those names. This allows
    us to know our features before we even look at the dataset, as it is the set of
    all possible hashes. This is a very large number, usually of the order of 2^(18).
    Using sparse matrices, we can quite easily store and compute even a matrix of
    this size, as a very large proportion of the matrix will have the value 0.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在这个阶段，我们无法在流水线中这样做，因为`TfidfVectorizer`不是一个在线算法。为了解决这个问题，我们使用`HashingVectorizer`。`HashingVectorizer`类是巧妙地使用哈希算法来极大地减少计算词袋模型所需的内存。我们不是记录特征名称，如文档中找到的单词，而是只记录这些名称的哈希值。这使得我们甚至在查看数据集之前就能知道我们的特征，因为它是所有可能的哈希值的集合。这是一个非常大的数字，通常为2的18次方。使用稀疏矩阵，我们可以非常容易地存储和计算甚至这样大小的矩阵，因为矩阵中的很大一部分将具有值0。
- en: 'Currently, the `Pipeline` class doesn''t allow for its use in online learning.
    There are some nuances in different applications that mean there isn''t an obvious
    one-size-fits-all approach that could be implemented. Instead, we can create our
    own subclass of Pipeline, which allows us to use it for online learning. We first
    derive our class from Pipeline, as we only need to implement a single function:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，`Pipeline`类不允许其在在线学习中使用。不同应用中的一些细微差别意味着没有一种明显的一劳永逸的方法可以实施。相反，我们可以创建自己的`Pipeline`子类，这样我们就可以用它来进行在线学习。我们首先从`Pipeline`派生我们的类，因为我们只需要实现一个函数：
- en: '[PRE55]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The only function we need to implement is the `partial_fit` function, which
    is performed by first doing all transformation steps, and then calling partial
    fit on the final step (which should be the classifier or clustering algorithm).
    All other functions are the same as in the normal Pipeline, class, so we refer
    (through class inheritance) to those.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要实现的功能只有一个，即`partial_fit`函数，它首先执行所有转换步骤，然后在最后一步（应该是分类器或聚类算法）上调用部分拟合。所有其他功能与正常Pipeline类、类相同，因此我们通过类继承来引用那些。
- en: 'We can now create a pipeline to use our `MiniBatchKMeans` in online learning,
    alongside our `HashingVectorizer`. Other than using our new classes `PartialFitPipeline`
    and `HashingVectorizer`, this is the same process as used in the rest of this
    chapter, except we only fit on a few documents at a time. The code is as follows:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建一个流水线来使用我们的`MiniBatchKMeans`进行在线学习，同时使用我们的`HashingVectorizer`。除了使用我们新的类`PartialFitPipeline`和`HashingVectorizer`外，这个过程与本章其余部分使用的过程相同，只是我们一次只拟合少量文档。代码如下：
- en: '[PRE56]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: There are some downsides to this approach. For one, we can't easily find out
    which words are most important for each cluster. We can get around this by fitting
    another `CountVectorizer` and taking the hash of each word. We then look up values
    by hash rather than word. This is a bit cumbersome and defeats the memory gains
    from using HashingVectorizer. Further, we can't use the `max_df` parameter that
    we used earlier, as it requires us to know what the features mean and to count
    them over time.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法有一些缺点。首先，我们无法轻易找出每个簇中最重要的单词。我们可以通过拟合另一个`CountVectorizer`并取每个单词的哈希值来解决这个问题。然后我们通过哈希值而不是单词来查找值。这有点繁琐，并且抵消了使用HashingVectorizer带来的内存节省。此外，我们无法使用之前使用的`max_df`参数，因为它需要我们知道特征的含义并随时间计数。
- en: We also can't use tf-idf weighting when performing training online. It would
    be possible to approximate this and apply such weighting, but again this is a
    cumbersome approach. `HashingVectorizer` is still a very useful algorithm and
    a great use of hashing algorithms.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行在线训练时，我们也不能使用tf-idf加权。虽然可以近似这种加权并应用它，但这又是一个繁琐的方法。"HashingVectorizer"仍然是一个非常有用的算法，并且是散列算法的极好应用。
- en: Summary
  id: totrans-288
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at clustering, which is an unsupervised learning
    approach. We use unsupervised learning to explore data, rather than for classification
    and prediction purposes. In the experiment here, we didn't have topics for the
    news items we found on reddit, so we were unable to perform classification. We
    used k-means clustering to group together these news stories to find common topics
    and trends in the data.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了聚类，这是一种无监督学习方法。我们使用无监督学习来探索数据，而不是用于分类和预测目的。在本实验中，我们没有为在reddit上找到的新闻条目指定主题，因此无法进行分类。我们使用了k-means聚类来将这些新闻故事分组，以找到数据中的共同主题和趋势。
- en: In pulling data from reddit, we had to extract data from arbitrary websites.
    This was performed by looking for large text segments, rather than a full-blown
    machine learning approach. There are some interesting approaches to machine learning
    for this task that may improve upon these results. In the Appendix of this book,
    I've listed, for each chapter, avenues for going beyond the scope of the chapter
    and improving upon the results. This includes references to other sources of information
    and more difficult applications of the approaches in each chapter.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在从reddit获取数据时，我们必须从任意网站上提取数据。这是通过寻找大文本段来完成的，而不是采用完整的机器学习方法。对于这项任务，有一些有趣的机器学习方法可能会改进这些结果。在本书的附录中，我为每一章列出了超越章节范围并改进结果的方法。这包括对其他信息来源和每章中方法的更复杂应用的参考。
- en: We also looked at a straightforward ensemble algorithm, EAC. An ensemble is
    often a good way to deal with variance in the results, especially if you don't
    know how to choose good parameters (which is always difficult with clustering).
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还研究了简单的集成算法EAC。集成通常是一种处理结果方差的好方法，尤其是如果你不知道如何选择好的参数（这在聚类中总是困难的）。
- en: Finally, we introduced online learning. This is a gateway to larger learning
    exercises, including big data, which will be discussed in the final two chapters
    of this book. These final experiments are quite large and require management of
    data as well as learning a model from them.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们介绍了在线学习。这是通向更大学习练习的门户，包括大数据，这些内容将在本书的最后两章中讨论。这些最终实验相当庞大，需要管理数据以及从数据中学习模型。
- en: As an extension on the work in this chapter, try implementing EAC to be an online
    learning algorithm. This is not a trivial task and will involve some thought on
    what should happen when the algorithm is updated. Another extension is to collect
    more data from more data sources (such as other subreddits or directly from news
    websites or blogs) and look for general trends.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章工作的扩展，尝试实现EAC作为在线学习算法。这不是一个简单任务，需要考虑当算法更新时应该发生什么。另一个扩展是收集更多来自更多数据源（如其他subreddits或直接从新闻网站或博客）的数据，并寻找一般趋势。
- en: In the next chapter, we'll step away from unsupervised learning and go back
    to classification. We will look at deep learning, which is a classification method
    built on complex neural networks.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从无监督学习转向分类。我们将探讨深度学习，这是一种建立在复杂神经网络之上的分类方法。
