- en: Chapter 10
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬10ç« 
- en: Quantum Neural Networks
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œ
- en: '*The mind is not a vessel to be filled, but a fire to be kindled.*'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¿ƒçµä¸æ˜¯è¦è¢«å¡«æ»¡çš„å®¹å™¨ï¼Œè€Œæ˜¯ä¸€å›¢éœ€è¦è¢«ç‚¹ç‡ƒçš„ç«ç„°ã€‚*'
- en: â€” Plutarch
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: â€”â€”æ™®é²å¡”å…‹
- en: 'In the previous chapter, we explored our first family of quantum machine learning
    models: quantum support vector machines. Now it is time for us to take one step
    further and consider yet another family of models, that of **Quantum** **Neural
    Networks** (**QNNs**).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€ç« ä¸­ï¼Œæˆ‘ä»¬æ¢è®¨äº†æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé‡å­æœºå™¨å­¦ä¹ æ¨¡å‹å®¶æ—ï¼šé‡å­æ”¯æŒå‘é‡æœºã€‚ç°åœ¨æ˜¯æˆ‘ä»¬è¿›ä¸€æ­¥æ¢ç´¢å¦ä¸€ä¸ªæ¨¡å‹å®¶æ—çš„æ—¶å€™äº†ï¼Œé‚£å°±æ˜¯**é‡å­** **ç¥ç»ç½‘ç»œ**ï¼ˆ**QNNs**ï¼‰ã€‚
- en: 'In this chapter, you will learn how the notion of a quantum neural network
    can arise naturally from the ideas behind classical neural networks. Of course,
    you will also learn how quantum neural networks work and how they can be trained.
    Then, you will explore how quantum neural networks can actually be implemented,
    run, and trained using the two quantum frameworks that we have been working with
    so far: Qiskit and PennyLane.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œä½ å°†å­¦ä¹ é‡å­ç¥ç»ç½‘ç»œçš„æ¦‚å¿µå¦‚ä½•è‡ªç„¶åœ°ä»ç»å…¸ç¥ç»ç½‘ç»œèƒŒåçš„æ€æƒ³ä¸­äº§ç”Ÿã€‚å½“ç„¶ï¼Œä½ è¿˜å°†å­¦ä¹ é‡å­ç¥ç»ç½‘ç»œçš„å·¥ä½œåŸç†ä»¥åŠå®ƒä»¬çš„è®­ç»ƒæ–¹æ³•ã€‚ç„¶åï¼Œä½ å°†æ¢ç´¢å¦‚ä½•ä½¿ç”¨æˆ‘ä»¬è¿„ä»Šä¸ºæ­¢ä¸€ç›´åœ¨ä½¿ç”¨çš„ä¸¤ä¸ªé‡å­æ¡†æ¶â€”â€”Qiskitå’ŒPennyLaneâ€”â€”æ¥å®ç°ã€è¿è¡Œå’Œè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œã€‚
- en: 'These are the contents of this chapter:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« å†…å®¹å¦‚ä¸‹ï¼š
- en: Building and training quantum neural networks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æ„å»ºå’Œè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œ
- en: Quantum neural networks in PennyLane
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PennyLaneä¸­çš„é‡å­ç¥ç»ç½‘ç»œ
- en: 'Quantum neural networks in Qiskit: a commentary'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Qiskitä¸­çš„é‡å­ç¥ç»ç½‘ç»œï¼šè¯„è®º
- en: Quantum support vector machines and quantum neural networks are probably the
    two most popular families of QML models, so, by the end of this chapter, you will
    already have a solid foundation in quantum machine learning.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­æ”¯æŒå‘é‡æœºå’Œé‡å­ç¥ç»ç½‘ç»œå¯èƒ½æ˜¯QMLæ¨¡å‹ä¸­æœ€å—æ¬¢è¿çš„ä¸¤ä¸ªå®¶æ—ï¼Œæ‰€ä»¥ï¼Œåˆ°æœ¬ç« ç»“æŸæ—¶ï¼Œä½ å°†å·²ç»åœ¨é‡å­æœºå™¨å­¦ä¹ æ–¹é¢æ‰“ä¸‹åšå®çš„åŸºç¡€ã€‚
- en: To get started, letâ€™s understand how quantum neural networks work and how they
    can be effectively trained. Letâ€™s get to it!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¼€å§‹ï¼Œè®©æˆ‘ä»¬äº†è§£é‡å­ç¥ç»ç½‘ç»œæ˜¯å¦‚ä½•å·¥ä½œçš„ï¼Œä»¥åŠå®ƒä»¬å¦‚ä½•è¢«æœ‰æ•ˆåœ°è®­ç»ƒã€‚è®©æˆ‘ä»¬ç€æ‰‹å§ï¼
- en: 10.1 Building and training a quantum neural network
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.1 æ„å»ºå’Œè®­ç»ƒä¸€ä¸ªé‡å­ç¥ç»ç½‘ç»œ
- en: Just like quantum support vector machines, quantum neural networks are what
    we called â€CQ modelsâ€ back in *Chapter* [*8*](ch017.xhtml#x1-1390008), *What is
    Quantum Machine* *Learning?*, â€” models with purely classical inputs and outputs
    that use quantum computing at some stage. However, unlike QSVMs, quantum neural
    networks are not a â€particular caseâ€ of any classical model, although their behavior
    is inspired by that of classical neural networks. What is more, as we will soon
    see, quantum neural networks are â€purely quantumâ€ models, in the sense that their
    execution will only require classical computing for the preparation of circuits
    and the statistical analysis of measurements. Nevertheless, just like QSVMs, quantum
    neural networks will depend on classical parameters that will be optimized classically.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒé‡å­æ”¯æŒå‘é‡æœºä¸€æ ·ï¼Œé‡å­ç¥ç»ç½‘ç»œæ˜¯æˆ‘ä»¬ä¹‹å‰åœ¨*ç¬¬8ç« *[*8*](ch017.xhtml#x1-1390008)ï¼Œâ€œä»€ä¹ˆæ˜¯é‡å­æœºå™¨å­¦ä¹ ï¼Ÿâ€ä¸­æåˆ°çš„â€œCQæ¨¡å‹â€â€”â€”å³çº¯ç²¹ä½¿ç”¨ç»å…¸è¾“å…¥å’Œè¾“å‡ºï¼Œå¹¶åœ¨æŸä¸ªé˜¶æ®µä½¿ç”¨é‡å­è®¡ç®—çš„æ¨¡å‹ã€‚ç„¶è€Œï¼Œä¸QSVMsä¸åŒï¼Œé‡å­ç¥ç»ç½‘ç»œä¸æ˜¯ä»»ä½•ç»å…¸æ¨¡å‹çš„â€œç‰¹æ®Šæƒ…å†µâ€ï¼Œå°½ç®¡å®ƒä»¬çš„è¡Œä¸ºå—åˆ°ç»å…¸ç¥ç»ç½‘ç»œè¡Œä¸ºçš„å¯å‘ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬å¾ˆå¿«å°±ä¼šçœ‹åˆ°çš„ï¼Œé‡å­ç¥ç»ç½‘ç»œæ˜¯â€œçº¯ç²¹é‡å­â€æ¨¡å‹ï¼Œè¿™æ„å‘³ç€å®ƒä»¬çš„æ‰§è¡Œå°†ä»…éœ€è¦ç»å…¸è®¡ç®—æ¥å‡†å¤‡ç”µè·¯å’Œè¿›è¡Œæµ‹é‡ç»Ÿè®¡åˆ†æã€‚å°½ç®¡å¦‚æ­¤ï¼Œå°±åƒQSVMsä¸€æ ·ï¼Œé‡å­ç¥ç»ç½‘ç»œå°†ä¾èµ–äºç»å…¸å‚æ•°ï¼Œè¿™äº›å‚æ•°å°†é€šè¿‡ç»å…¸ä¼˜åŒ–è¿›è¡Œä¼˜åŒ–ã€‚
- en: To learn moreâ€¦
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: äº†è§£æ›´å¤š...
- en: As you surely know by now, (quantum) machine learning is a vast field in which
    terms hardly ever have a unique meaning. The term â€quantum neural networkâ€ can,
    in practice, be used to refer to any QML model that is inspired by the behavior
    of a classical neural network. Therefore, you should bear in mind that people
    may also use this name to refer to models different from the ones that we are
    considering to be quantum neural networks.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚ä½ æ­¤åˆ»æ‰€çŸ¥ï¼Œ(é‡å­)æœºå™¨å­¦ä¹ æ˜¯ä¸€ä¸ªæœ¯è¯­å«ä¹‰å‡ ä¹ä¸å”¯ä¸€çš„å¹¿é˜”é¢†åŸŸã€‚åœ¨å®è·µä¸­ï¼Œâ€œé‡å­ç¥ç»ç½‘ç»œâ€è¿™ä¸ªæœ¯è¯­å¯ä»¥ç”¨æ¥æŒ‡ä»£ä»»ä½•å—ç»å…¸ç¥ç»ç½‘ç»œè¡Œä¸ºå¯å‘çš„QMLæ¨¡å‹ã€‚å› æ­¤ï¼Œä½ åº”è¯¥è®°ä½ï¼Œäººä»¬ä¹Ÿå¯èƒ½ä½¿ç”¨è¿™ä¸ªåç§°æ¥æŒ‡ä»£ä¸æˆ‘ä»¬è€ƒè™‘çš„é‡å­ç¥ç»ç½‘ç»œä¸åŒçš„æ¨¡å‹ã€‚
- en: That should be enough of an introduction. Letâ€™s now get into the details. What
    actually are quantum neural networks and how do they relate to classical neural
    networks?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åº”è¯¥å·²ç»è¶³å¤Ÿä½œä¸ºä»‹ç»äº†ã€‚ç°åœ¨è®©æˆ‘ä»¬æ·±å…¥ç»†èŠ‚ã€‚é‡å­ç¥ç»ç½‘ç»œç©¶ç«Ÿæ˜¯ä»€ä¹ˆï¼Œå®ƒä»¬æ˜¯å¦‚ä½•ä¸ç»å…¸ç¥ç»ç½‘ç»œç›¸å…³çš„ï¼Ÿ
- en: 10.1.1 A journey from classical neural networks to quantum neural networks
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.1 ä»ç»å…¸ç¥ç»ç½‘ç»œåˆ°é‡å­ç¥ç»ç½‘ç»œçš„æ—…ç¨‹
- en: 'If we do a small exercise of abstraction, we can think of the action of a classical
    neural network as consisting of the following stages:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¿›è¡Œä¸€æ¬¡æŠ½è±¡çš„å°ç»ƒä¹ ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç»å…¸ç¥ç»ç½‘ç»œçš„åŠ¨ä½œçœ‹ä½œç”±ä»¥ä¸‹é˜¶æ®µç»„æˆï¼š
- en: '**Data preparation**: This simply amounts to taking some (classical) input
    data and maybe carrying out some (simple) transformations on it. These may include
    normalizing or scaling the input data.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å‡†å¤‡**ï¼šè¿™ä»…ä»…æ˜¯å°†ä¸€äº›ï¼ˆç»å…¸ï¼‰è¾“å…¥æ•°æ®å’Œå¯èƒ½å¯¹å…¶è¿›è¡Œçš„æŸäº›ï¼ˆç®€å•ï¼‰è½¬æ¢ã€‚è¿™äº›å¯èƒ½åŒ…æ‹¬å¯¹è¾“å…¥æ•°æ®è¿›è¡Œå½’ä¸€åŒ–æˆ–ç¼©æ”¾ã€‚'
- en: '**Data processing**: Feeding the data through a sequence of layers that â€transformâ€
    the data as it flows through them. The behavior of this processing depends on
    some optimizable parameters, which are adjusted in training.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å¤„ç†**ï¼šé€šè¿‡ä¸€ç³»åˆ—å±‚å°†æ•°æ®ä¼ é€’è¿‡å»ï¼Œè¿™äº›å±‚â€œè½¬æ¢â€æ•°æ®ï¼Œéšç€æ•°æ®æµè¿‡å®ƒä»¬ã€‚è¿™ç§å¤„ç†çš„è¡Œä¸ºå–å†³äºä¸€äº›å¯ä¼˜åŒ–çš„å‚æ•°ï¼Œè¿™äº›å‚æ•°åœ¨è®­ç»ƒä¸­ä¼šè¢«è°ƒæ•´ã€‚'
- en: '**Data output**: Returning the output through a final layer.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è¾“å‡º**ï¼šé€šè¿‡æœ€ç»ˆå±‚è¿”å›è¾“å‡ºã€‚'
- en: Letâ€™s see how we can take this scheme and use it to define an analogous quantum
    model.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹çœ‹æˆ‘ä»¬å¦‚ä½•å¯ä»¥å°†è¿™ä¸ªæ–¹æ¡ˆç”¨äºå®šä¹‰ä¸€ä¸ªç±»ä¼¼çš„é‡å­æ¨¡å‹ã€‚
- en: '**Data preparation**: Quantum neural networks are given classical inputs (in
    the form of an array of numbers), but quantum computers donâ€™t work on classical
    data â€” they work on quantum states! So how can we take these classical inputs
    and embed them into the space of quantum states?'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å‡†å¤‡**ï¼šé‡å­ç¥ç»ç½‘ç»œæ¥æ”¶ç»å…¸è¾“å…¥ï¼ˆä»¥æ•°å­—æ•°ç»„çš„å½¢å¼ï¼‰ï¼Œä½†é‡å­è®¡ç®—æœºå¹¶ä¸å¤„ç†ç»å…¸æ•°æ®â€”â€”å®ƒä»¬å¤„ç†é‡å­çŠ¶æ€ï¼é‚£ä¹ˆæˆ‘ä»¬å¦‚ä½•å°†è¿™äº›ç»å…¸è¾“å…¥åµŒå…¥åˆ°é‡å­çŠ¶æ€ç©ºé—´ä¸­å‘¢ï¼Ÿ'
- en: That is a problem that we have already dealt with in *Section* [*9.2*](ch018.xhtml#x1-1660009.2).
    In order to encode the classical input of a QNN into a quantum state, we just
    have to use any feature map of our choice. As you know, we may also need to normalize
    or scale the data, of course.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™æ˜¯æˆ‘ä»¬å·²ç»åœ¨ *ç¬¬9.2èŠ‚* ä¸­å¤„ç†è¿‡çš„é—®é¢˜ã€‚ä¸ºäº†å°†QNNçš„ç»å…¸è¾“å…¥ç¼–ç æˆé‡å­çŠ¶æ€ï¼Œæˆ‘ä»¬åªéœ€è¦ä½¿ç”¨æˆ‘ä»¬é€‰æ‹©çš„ä»»ä½•ç‰¹å¾æ˜ å°„ã€‚æ­£å¦‚ä½ æ‰€çŸ¥ï¼Œæˆ‘ä»¬å½“ç„¶å¯èƒ½è¿˜éœ€è¦å¯¹æ•°æ®è¿›è¡Œå½’ä¸€åŒ–æˆ–ç¼©æ”¾ã€‚
- en: 'And that is how we actually â€prepare the dataâ€ for a quantum neural network:
    feeding it into a feature map.'
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ­£æ˜¯è¿™æ ·ï¼Œæˆ‘ä»¬å®é™…ä¸Šä¸ºé‡å­ç¥ç»ç½‘ç»œâ€œå‡†å¤‡æ•°æ®â€ï¼šå°†å…¶è¾“å…¥åˆ°ç‰¹å¾æ˜ å°„ä¸­ã€‚
- en: '**Data processing**: At this point, we have successfully transformed our classical
    input into a â€quantum input,â€ in the form of a quantum state that encodes our
    classical data according to a certain feature map. Now, we need to figure out
    a way to process this input by drawing some inspiration from the processing in
    a classical neural network.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®å¤„ç†**ï¼šåœ¨è¿™ä¸ªé˜¶æ®µï¼Œæˆ‘ä»¬å·²ç»æˆåŠŸåœ°å°†æˆ‘ä»¬çš„ç»å…¸è¾“å…¥è½¬æ¢æˆäº†â€œé‡å­è¾“å…¥â€ï¼Œå³ä»¥é‡å­çŠ¶æ€çš„å½¢å¼ç¼–ç æˆ‘ä»¬çš„ç»å…¸æ•°æ®ï¼Œæ ¹æ®æŸä¸ªç‰¹å¾æ˜ å°„ã€‚ç°åœ¨ï¼Œæˆ‘ä»¬éœ€è¦æ‰¾å‡ºä¸€ç§æ–¹æ³•æ¥å¤„ç†è¿™ä¸ªè¾“å…¥ï¼Œå¯ä»¥ä»ç»å…¸ç¥ç»ç½‘ç»œçš„å¤„ç†ä¸­æ±²å–ä¸€äº›çµæ„Ÿã€‚'
- en: Trying to replicate the full, exact behavior of a classical neural network in
    a quantum neural network might prove not to be ideal given the state of current
    quantum hardware. Instead, we can look at the bigger picture.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨å½“å‰é‡å­ç¡¬ä»¶çš„çŠ¶æ€ä¸‹ï¼Œè¯•å›¾åœ¨é‡å­ç¥ç»ç½‘ç»œä¸­å¤åˆ¶ç»å…¸ç¥ç»ç½‘ç»œçš„å®Œæ•´ã€ç²¾ç¡®çš„è¡Œä¸ºå¯èƒ½å¹¶ä¸ç†æƒ³ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä»æ›´å¤§çš„å›¾æ™¯æ¥çœ‹ã€‚
- en: In essence, the processing stage of a classical neural network consists in the
    application of some transformations that depend, exclusively, on some optimizable
    parameters. And that is an idea that we can very easily export to a quantum computer.
    We can simply define the â€processingâ€ stage of a quantum neural network asâ€¦the
    application of a circuit that depends on some optimizable parameters! In addition
    to this, as we will see later in this section, this circuit can be structured
    in layers in a way that somewhat reassembles the spirit of a classical neural
    network. This circuit will be said to be a **variational form** â€” they are just
    like the ones we studied back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE:*
    *Variational Quantum Eigensolver*.
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æœ¬è´¨ä¸Šï¼Œç»å…¸ç¥ç»ç½‘ç»œçš„å¤„ç†é˜¶æ®µåŒ…æ‹¬åº”ç”¨ä¸€äº›ä»…ä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•°çš„è½¬æ¢ã€‚è¿™æ˜¯ä¸€ä¸ªæˆ‘ä»¬å¯ä»¥éå¸¸å®¹æ˜“åœ°ç§»æ¤åˆ°é‡å­è®¡ç®—æœºä¸Šçš„æƒ³æ³•ã€‚æˆ‘ä»¬å¯ä»¥ç®€å•åœ°å°†é‡å­ç¥ç»ç½‘ç»œçš„â€œå¤„ç†â€é˜¶æ®µå®šä¹‰ä¸ºâ€¦ä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•°çš„ç”µè·¯çš„åº”ç”¨ï¼æ­¤å¤–ï¼Œæ­£å¦‚æˆ‘ä»¬å°†åœ¨æœ¬èŠ‚åé¢çœ‹åˆ°çš„é‚£æ ·ï¼Œè¿™ä¸ªç”µè·¯å¯ä»¥è¢«åˆ†å±‚ç»“æ„åŒ–ï¼Œä»¥æŸç§æ–¹å¼é‡æ–°ç»„åˆç»å…¸ç¥ç»ç½‘ç»œçš„ç²¾é«“ã€‚è¿™ä¸ªç”µè·¯å°†è¢«ç§°ä¸º**å˜åˆ†å½¢å¼**â€”â€”å®ƒä»¬å°±åƒæˆ‘ä»¬åœ¨
    *ç¬¬7ç« * ä¸­ç ”ç©¶çš„é‚£äº›ä¸€æ ·ï¼Œ*VQEï¼šå˜åˆ†é‡å­æœ¬å¾å€¼æ±‚è§£å™¨*ã€‚
- en: '**Data output**: Once we have a processed state, we need to return a classical
    output. And this shall be the result of some measurement operation; this operation
    can be whichever one suits our problem best!'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æ•°æ®è¾“å‡º**ï¼šä¸€æ—¦æˆ‘ä»¬æœ‰ä¸€ä¸ªå¤„ç†è¿‡çš„çŠ¶æ€ï¼Œæˆ‘ä»¬éœ€è¦è¿”å›ä¸€ä¸ªç»å…¸è¾“å‡ºã€‚è¿™å°†æ˜¯æŸäº›æµ‹é‡æ“ä½œçš„ç»“æœï¼›è¿™ä¸ªæ“ä½œå¯ä»¥æ˜¯é€‚åˆæˆ‘ä»¬é—®é¢˜çš„æœ€ä½³é€‰æ‹©ï¼'
- en: For instance, if we wanted to build a binary classifier with a quantum neural
    network, a natural choice for this measurement operation could be, for example,
    taking the expectation value of the first qubit when measured on the computational
    basis. Remember that the expectation value of a qubit simply corresponds to the
    probability of obtaining ![1](img/file13.png "1") upon measuring the qubit on
    the computational basis.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œå¦‚æœæˆ‘ä»¬æƒ³ç”¨é‡å­ç¥ç»ç½‘ç»œæ„å»ºä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼Œè¿™ä¸ªæµ‹é‡æ“ä½œçš„ä¸€ä¸ªè‡ªç„¶é€‰æ‹©å¯èƒ½æ˜¯ï¼Œä¾‹å¦‚ï¼Œåœ¨è®¡ç®—åŸºä¸Šæµ‹é‡ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹æ—¶çš„æœŸæœ›å€¼ã€‚è®°ä½ï¼Œé‡å­æ¯”ç‰¹çš„æœŸæœ›å€¼ç®€å•åœ°å¯¹åº”äºåœ¨è®¡ç®—åŸºä¸Šæµ‹é‡é‡å­æ¯”ç‰¹è·å¾—![1](img/file13.png
    "1")çš„æ¦‚ç‡ã€‚
- en: And those are all the ingredients that make up a quantum neural network.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›å°±æ˜¯æ„æˆé‡å­ç¥ç»ç½‘ç»œçš„æ‰€æœ‰æˆåˆ†ã€‚
- en: 'As a matter of fact, feature maps and variational forms are both examples of
    **variational circuits**: quantum circuits that are controlled by some classical
    parameters. The only actual difference between a feature map and a variational
    form is their purpose: feature maps depend on the input data and are used to encode
    it, while variational forms depend on optimizable parameters and are used to transform
    a quantum input state.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œç‰¹å¾å›¾å’Œå˜åˆ†å½¢å¼éƒ½æ˜¯**å˜åˆ†ç”µè·¯**çš„ä¾‹å­ï¼šå—æŸäº›ç»å…¸å‚æ•°æ§åˆ¶çš„é‡å­ç”µè·¯ã€‚ç‰¹å¾å›¾å’Œå˜åˆ†å½¢å¼ä¹‹é—´çš„å”¯ä¸€å®é™…åŒºåˆ«æ˜¯å®ƒä»¬çš„ç›®çš„ï¼šç‰¹å¾å›¾ä¾èµ–äºè¾“å…¥æ•°æ®ï¼Œç”¨äºå¯¹å…¶è¿›è¡Œç¼–ç ï¼Œè€Œå˜åˆ†å½¢å¼ä¾èµ–äºå¯ä¼˜åŒ–å‚æ•°ï¼Œç”¨äºå°†é‡å­è¾“å…¥çŠ¶æ€è¿›è¡Œè½¬æ¢ã€‚
- en: This difference in purpose will materialize in the fact that we will often use
    different circuits for feature maps and variational forms. A good feature map
    need not be a good variational form, and vice versa.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§ç›®çš„ä¸Šçš„å·®å¼‚å°†ä½“ç°åœ¨æˆ‘ä»¬ç»å¸¸ä¼šä¸ºç‰¹å¾å›¾å’Œå˜åˆ†å½¢å¼ä½¿ç”¨ä¸åŒçš„ç”µè·¯ã€‚ä¸€ä¸ªå¥½çš„ç‰¹å¾å›¾ä¸ä¸€å®šæ˜¯ä¸€ä¸ªå¥½çš„å˜åˆ†å½¢å¼ï¼Œåä¹‹äº¦ç„¶ã€‚
- en: 'You should keep in mind that â€” like all things QML â€” the terms â€feature mapâ€
    and â€variational formâ€ are not entirely universal, and different authors may refer
    to them with different expressions. For example, variational forms are commonly
    referred to as **ansatzs**, as we did back in *Chapter* [*7*](ch015.xhtml#x1-1190007),
    *VQE: Variational* *Quantum Eigensolver*.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 'ä½ åº”è¯¥è®°ä½â€”â€”å°±åƒæ‰€æœ‰é‡å­æœºå™¨å­¦ä¹ ï¼ˆQMLï¼‰çš„ä¸œè¥¿ä¸€æ ·â€”â€”â€œç‰¹å¾å›¾â€å’Œâ€œå˜åˆ†å½¢å¼â€è¿™ä¸¤ä¸ªæœ¯è¯­å¹¶ä¸å®Œå…¨é€šç”¨ï¼Œä¸åŒçš„ä½œè€…å¯èƒ½ä¼šç”¨ä¸åŒçš„è¡¨è¾¾æ¥æŒ‡ä»£å®ƒä»¬ã€‚ä¾‹å¦‚ï¼Œå˜åˆ†å½¢å¼é€šå¸¸è¢«ç§°ä¸º**ansatzs**ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬7ç« *[*7*](ch015.xhtml#x1-1190007)
    *VQE: å˜åˆ†é‡å­æœ¬å¾å€¼æ±‚è§£å™¨*ä¸­åšçš„é‚£æ ·ã€‚'
- en: Important note
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æç¤º
- en: 'A quantum neural network takes a classical input ![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}") and maps it to a quantum state through a feature map
    ![F](img/file1320.png "F"). The resulting state then goes through a variational
    form ![V](img/file379.png "V"): a variational circuit dependent on some optimizable
    parameters ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}").
    The output of the quantum neural network is the result of a measurement operation
    on the final state. All this can be seen, schematically, in the following figure:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: é‡å­ç¥ç»ç½‘ç»œå°†ç»å…¸è¾“å…¥ ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}")
    é€šè¿‡ç‰¹å¾å›¾ ![F](img/file1320.png "F") æ˜ å°„åˆ°é‡å­çŠ¶æ€ã€‚ç„¶åï¼Œå¾—åˆ°çš„é‡å­çŠ¶æ€é€šè¿‡å˜åˆ†å½¢å¼ ![V](img/file379.png
    "V")ï¼šä¸€ä¸ªä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•° ![\overset{\rightarrow}{\theta}](img/file1321.png "\overset{\rightarrow}{\theta}")
    çš„å˜åˆ†ç”µè·¯ã€‚é‡å­ç¥ç»ç½‘ç»œçš„è¾“å‡ºæ˜¯å¯¹æœ€ç»ˆçŠ¶æ€çš„æµ‹é‡æ“ä½œçš„ç»“æœã€‚æ‰€æœ‰è¿™äº›éƒ½å¯ä»¥åœ¨ä»¥ä¸‹å›¾ä¸­ schematically çœ‹åˆ°ï¼š
- en: '![ nâƒ— |FV0âŸ©((âƒ—xğœƒ)) ](img/file1322.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![ nâƒ— |FV0âŸ©((âƒ—xğœƒ))] (img/file1322.jpg)'
- en: Thanks to our study of quantum support vector machines, we are already very
    familiar with feature maps, but we have yet to get acquainted with variational
    forms; that is what we will devote the next subsection to.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: æ„Ÿè°¢æˆ‘ä»¬å¯¹é‡å­æ”¯æŒå‘é‡æœºçš„ç ”ç©¶ï¼Œæˆ‘ä»¬å·²ç»éå¸¸ç†Ÿæ‚‰ç‰¹å¾å›¾ï¼Œä½†æˆ‘ä»¬è¿˜æ²¡æœ‰ç†Ÿæ‚‰å˜åˆ†å½¢å¼ï¼›è¿™æ­£æ˜¯æˆ‘ä»¬å°†è‡´åŠ›äºä¸‹ä¸€å°èŠ‚çš„å†…å®¹ã€‚
- en: 10.1.2 Variational forms
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.2 å˜åˆ†å½¢å¼
- en: In principle, a variational form could be any variational circuit of your choice,
    but, in general, variational forms for QNNs follow a â€layered structure,â€ trying
    to mimic the spirit of classical neural networks. We can now make this idea precise.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨åŸåˆ™ä¸Šï¼Œå˜åˆ†å½¢å¼å¯ä»¥æ˜¯ä»»ä½•ä½ é€‰æ‹©çš„å˜åˆ†ç”µè·¯ï¼Œä½†é€šå¸¸ï¼ŒQNNçš„å˜åˆ†å½¢å¼éµå¾ªâ€œåˆ†å±‚ç»“æ„â€ï¼Œè¯•å›¾æ¨¡ä»¿ç»å…¸ç¥ç»ç½‘ç»œçš„ç²¾é«“ã€‚æˆ‘ä»¬ç°åœ¨å¯ä»¥ç²¾ç¡®åœ°é˜è¿°è¿™ä¸ªæƒ³æ³•ã€‚
- en: If we wanted to define a variational form with ![k](img/file317.png "k") layers,
    we could consider ![k](img/file317.png "k") vectors of independent parameters
    ![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}").
    In order to define each layer ![j](img/file258.png "j"), we may take a variational
    circuit ![G_{j}](img/file1324.png "G_{j}") dependent on the parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}"). A common approach is to prepare variational
    forms by stacking these variational circuits consecutively and separating them
    by some circuits ![U_{}](img/file1326.png "U_{}")
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬è¦å®šä¹‰ä¸€ä¸ªå…·æœ‰![k](img/file317.png "k")å±‚çš„å˜åˆ†å½¢å¼ï¼Œæˆ‘ä»¬å¯ä»¥è€ƒè™‘![k](img/file317.png "k")ä¸ªç‹¬ç«‹å‚æ•°çš„å‘é‡![{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}](img/file1323.png
    "{\overset{\rightarrow}{\theta}}_{1},\ldots,{\overset{\rightarrow}{\theta}}_{k}")ã€‚ä¸ºäº†å®šä¹‰æ¯ä¸€å±‚![j](img/file258.png
    "j")ï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¸€ä¸ªä¾èµ–äºå‚æ•°![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}")çš„å˜åˆ†ç”µè·¯![G_{j}](img/file1324.png
    "G_{j}")ã€‚ä¸€ç§å¸¸è§çš„æ–¹æ³•æ˜¯é€šè¿‡è¿ç»­å †å è¿™äº›å˜åˆ†ç”µè·¯å¹¶ä½¿ç”¨ä¸€äº›ç”µè·¯![U_{}](img/file1326.png "U_{}")æ¥å‡†å¤‡å˜åˆ†å½¢å¼
- en: entË†t![,independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.](img/file1327.png
    ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: çº ç¼ ï¼Œç‹¬ç«‹äºä»»ä½•å‚æ•°ï¼Œæ—¨åœ¨åœ¨é‡å­æ¯”ç‰¹ä¹‹é—´åˆ›å»ºçº ç¼ ã€‚æ­£å¦‚![å›¾10.1](img/file1327.png ",independentofanyparameters,meanttocreateentanglementbetweenthequbits.Justasin{\textit{Figure~}\text{10.1}}.")æ‰€ç¤ºã€‚
- en: '![Figure 10.1: A variational form with k layers, each defined by a variational
    circuit G_{j} dependent on some parameters {\overset{\rightarrow}{\theta}}_{j}.
    The circuits U_{} entË†tareusedtocreateentanglement,andthestate \left| \psi_{}
    \right. enc\rangle denotes the output of the feature map ](img/file1331.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.1ï¼šä¸€ä¸ªå…·æœ‰kå±‚çš„å˜åˆ†å½¢å¼ï¼Œæ¯ä¸ªå±‚ç”±ä¸€ä¸ªä¾èµ–äºæŸäº›å‚æ•°{\overset{\rightarrow}{\theta}}_{j}çš„å˜åˆ†ç”µè·¯G_{j}å®šä¹‰ã€‚ç”µè·¯U_{}ç”¨äºåˆ›å»ºçº ç¼ ï¼ŒçŠ¶æ€\left|
    \psi_{} \right. enc\rangleè¡¨ç¤ºç‰¹å¾å›¾çš„è¾“å‡º](img/file1331.jpg)'
- en: '**Figure 10.1**: A variational form with ![k](img/file317.png "k") layers,
    each defined by a variational circuit ![G_{j}](img/file1324.png "G_{j}") dependent
    on some parameters ![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png "{\overset{\rightarrow}{\theta}}_{j}").
    The circuits ![U_{}](img/file1326.png "U_{}") entË†t![areusedtocreateentanglement,andthestate](img/file1328.png
    "areusedtocreateentanglement,andthestate") ![\left| \psi_{} \right.](img/file1329.png
    "\left| \psi_{} \right.") enc![\rangle](img/file1330.png "\rangle") denotes the
    output of the feature map'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾10.1**ï¼šä¸€ä¸ªå…·æœ‰![k](img/file317.png "k")å±‚çš„å˜åˆ†å½¢å¼ï¼Œæ¯ä¸ªå±‚ç”±ä¸€ä¸ªä¾èµ–äºæŸäº›å‚æ•°![{\overset{\rightarrow}{\theta}}_{j}](img/file1325.png
    "{\overset{\rightarrow}{\theta}}_{j}")çš„å˜åˆ†ç”µè·¯![G_{j}](img/file1324.png "G_{j}")å®šä¹‰ã€‚ç”µè·¯![U_{}](img/file1326.png
    "U_{}")ç”¨äºåˆ›å»ºçº ç¼ ï¼ŒçŠ¶æ€![\left| \psi_{} \right.](img/file1329.png "\left| \psi_{} \right.")![\rangle](img/file1330.png
    "\rangle")è¡¨ç¤ºç‰¹å¾å›¾çš„è¾“å‡º'
- en: 'We have now outlined one of the most common structures of variational forms,
    but variational forms are best illustrated by examples. There are lots of variational
    forms out there, and there is no way we could collect them all in this book â€”
    in truth, there would be no point either. For this reason, we will restrict ourselves
    to presenting just three variational forms, some of which we will use later in
    the book:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»æ¦‚è¿°äº†å˜åˆ†å½¢å¼ä¸­æœ€å¸¸è§çš„ç»“æ„ä¹‹ä¸€ï¼Œä½†å˜åˆ†å½¢å¼æœ€å¥½é€šè¿‡ä¾‹å­æ¥è¯´æ˜ã€‚å˜åˆ†å½¢å¼æœ‰å¾ˆå¤šï¼Œæˆ‘ä»¬ä¸å¯èƒ½åœ¨è¿™æœ¬ä¹¦ä¸­æ”¶é›†å®ƒä»¬æ‰€æœ‰â€”â€”å®é™…ä¸Šï¼Œè¿™æ ·åšä¹Ÿæ²¡æœ‰æ„ä¹‰ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†é™åˆ¶è‡ªå·±åªä»‹ç»ä¸‰ç§å˜åˆ†å½¢å¼ï¼Œå…¶ä¸­ä¸€äº›æˆ‘ä»¬å°†åœ¨æœ¬ä¹¦çš„åé¢éƒ¨åˆ†ä½¿ç”¨ï¼š
- en: '**Two-local**: The **two-local variational form** with ![k](img/file317.png
    "k") repetitions on ![n](img/file244.png "n") qubits relies on ![n \times (k +
    1)](img/file1332.png "n \times (k + 1)") optimizable parameters, which we will
    denote as ![\theta_{rj}](img/file1333.png "\theta_{rj}") with ![r = 0,\ldots,k](img/file1334.png
    "r = 0,\ldots,k") and ![j = 1,\ldots n](img/file1335.png "j = 1,\ldots n"). Its
    circuit is constructed as per the following procedure:'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŒå±€éƒ¨**ï¼šåœ¨![n](img/file244.png "n")ä¸ªé‡å­æ¯”ç‰¹ä¸Šé‡å¤![k](img/file317.png "k")æ¬¡çš„**åŒå±€éƒ¨å˜åˆ†å½¢å¼**ä¾èµ–äº![n
    \times (k + 1)](img/file1332.png "n \times (k + 1)")ä¸ªå¯ä¼˜åŒ–å‚æ•°ï¼Œæˆ‘ä»¬å°†ç”¨![\theta_{rj}](img/file1333.png
    "\theta_{rj}")è¡¨ç¤ºï¼Œå…¶ä¸­![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k")å’Œ![j =
    1,\ldots n](img/file1335.png "j = 1,\ldots n")ã€‚å…¶ç”µè·¯çš„æ„å»ºæŒ‰ç…§ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š'
- en: '**procedure** TwoLocal(![n,k,\theta](img/file1336.png "n,k,\theta"))'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**è¿‡ç¨‹** TwoLocal([![n,k,\theta](img/file1336.png "n,k,\theta"))](img/file1336.png
    "n,k,\theta")'
- en: '**for all** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **do**'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![r = 0,\ldots,k](img/file1334.png "r = 0,\ldots,k") **æ‰§è¡Œ**'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Add the* ![r](img/file1337.png
    "r")*-th* *layer.*Â Â Â Â  * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *æ·»åŠ * ![r](img/file1337.png
    "r")*-å±‚ã€‚Â Â Â Â  * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **æ‰§è¡Œ**'
- en: Apply a ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})") gate on
    qubit ![j](img/file258.png "j").
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­ä½ ![j](img/file258.png "j") ä¸Šåº”ç”¨ ![R_{Y}(\theta_{rj})](img/file1339.png "R_{Y}(\theta_{rj})")
    é—¨ã€‚
- en: '-'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: '![\vartriangleright](img/file655.png "\vartriangleright") * *Create entanglement
    between layers.*Â Â Â Â  * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![\vartriangleright](img/file655.png "\vartriangleright") * *åœ¨å±‚ä¹‹é—´åˆ›å»ºçº ç¼ ã€‚*Â Â Â Â 
    * ![\vartriangleleft](img/file1338.png "\vartriangleleft")'
- en: '**if** ![r < k](img/file1340.png "r < k") **then**'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¦‚æœ** ![r < k](img/file1340.png "r < k") **åˆ™**'
- en: '**for all** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **do**'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![t = 1,\ldots,n - 1](img/file1341.png "t = 1,\ldots,n - 1") **æ‰§è¡Œ**'
- en: Apply a CNOT gate with control on qubit ![t](img/file48.png "t") and target
    on qubit ![t + 1](img/file1342.png "t + 1").
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹æ§åˆ¶é‡å­ä½ ![t](img/file48.png "t") å’Œç›®æ ‡é‡å­ä½ ![t + 1](img/file1342.png "t + 1") åº”ç”¨CNOTé—¨ã€‚
- en: -![-](img/file1343.png "-")
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")
- en: -![-](img/file1343.png "-")**** ***In *Figure* [*10.2*](#Figure10.2) we have
    depicted the output of this procedure for ![n = 4](img/file837.png "n = 4") and
    ![k = 3](img/file1344.png "k = 3"). Sound familiar? The two-local variational
    form uses the same circuit as the angle encoding feature map for its layers, and
    then it relies on a cascade of controlled-NOT operations in order to create entanglement.
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![-](img/file1343.png "-")**** ***åœ¨*å›¾* [*10.2*](#Figure10.2) ä¸­ï¼Œæˆ‘ä»¬å±•ç¤ºäº†è¯¥è¿‡ç¨‹åœ¨ ![n
    = 4](img/file837.png "n = 4") å’Œ ![k = 3](img/file1344.png "k = 3") æ—¶çš„è¾“å‡ºã€‚å¬èµ·æ¥ç†Ÿæ‚‰å—ï¼Ÿä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼ä½¿ç”¨ä¸è§’åº¦ç¼–ç ç‰¹å¾å›¾ç›¸åŒçš„ç”µè·¯ä½œä¸ºå…¶å±‚ï¼Œç„¶åå®ƒä¾èµ–äºä¸€ç³»åˆ—å—æ§-NOTæ“ä½œæ¥åˆ›å»ºçº ç¼ ã€‚
- en: Notice, by the way, how the two-local variational form with ![k](img/file317.png
    "k") repetitions has ![k + 1](img/file1345.png "k + 1") layers, not ![k](img/file317.png
    "k"). This tiny detail can sometimes be misleading.
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæ³¨æ„ä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼é‡å¤ ![k](img/file317.png "k") æ¬¡æ—¶å…·æœ‰ ![k + 1](img/file1345.png
    "k + 1") å±‚ï¼Œè€Œä¸æ˜¯ ![k](img/file317.png "k") å±‚ã€‚è¿™ä¸ªå°å°çš„ç»†èŠ‚æœ‰æ—¶å¯èƒ½ä¼šè¯¯å¯¼ã€‚
- en: The two-local variational form is very versatile, and it can be used with any
    measurement operation.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼éå¸¸çµæ´»ï¼Œå®ƒå¯ä»¥ä¸ä»»ä½•æµ‹é‡æ“ä½œä¸€èµ·ä½¿ç”¨ã€‚
- en: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![Figure 10.2: Two-local variational form on four qubits and two repetitions](img/file1346.png)'
- en: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**Figure 10.2**: Two-local variational form on four qubits and two repetitions***'
- en: '***   **Tree tensor**: The **tree tensor** variational form with ![k + 1](img/file1345.png
    "k + 1") layers can be applied on ![n = 2^{k}](img/file1347.png "n = 2^{k}") qubits.
    Each layer has half the number of parameters as the previous one, so the variational
    form relies on ![2^{k} + 2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k
    - 1} + \cdots + 1") optimizable parameters of the form'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '***   **æ ‘å¼ é‡**ï¼šå…·æœ‰ ![k + 1](img/file1345.png "k + 1") å±‚çš„**æ ‘å¼ é‡**å˜åˆ†å½¢å¼å¯ä»¥åº”ç”¨äº ![n
    = 2^{k}](img/file1347.png "n = 2^{k}") é‡å­ä½ã€‚æ¯ä¸€å±‚çš„å‚æ•°æ•°é‡æ˜¯å‰ä¸€å±‚çš„ä¸€åŠï¼Œå› æ­¤å˜åˆ†å½¢å¼ä¾èµ–äº ![2^{k} +
    2^{k - 1} + \cdots + 1](img/file1348.png "2^{k} + 2^{k - 1} + \cdots + 1") ä¸ªå¯ä¼˜åŒ–å‚æ•°çš„å½¢å¼'
- en: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.](img/file1349.png
    "\theta_{rs},\qquad r = 0,\ldots,k,\qquad s = 0,\ldots,2^{k - r} - 1.") |'
- en: 'The procedure that defines is somewhat more opaque than that of the two-local
    variational form, and it reads as follows:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å®šä¹‰è¯¥è¿‡ç¨‹çš„æ­¥éª¤æ¯”ä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼ç¨å¾®éš¾ä»¥ç†è§£ï¼Œå…¶å†…å®¹å¦‚ä¸‹ï¼š
- en: '**procedure** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**è¿‡ç¨‹** TreeTensor(![k,\theta](img/file1350.png "k,\theta"))'
- en: On each qubit ![j](img/file258.png "j"), apply a rotation ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})").
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨æ¯ä¸ªé‡å­ä½ ![j](img/file258.png "j") ä¸Šåº”ç”¨æ—‹è½¬ ![R_{Y}(\theta_{0j})](img/file1351.png
    "R_{Y}(\theta_{0j})")ã€‚
- en: '**for all** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **do**'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![r = 1,\ldots,k](img/file1352.png "r = 1,\ldots,k") **æ‰§è¡Œ**'
- en: '**for all** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k
    - r} - 1") **do**'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![s = 0,\ldots,2^{k - r} - 1](img/file1353.png "s = 0,\ldots,2^{k -
    r} - 1") **æ‰§è¡Œ**'
- en: Apply a CNOT operation with target on qubit ![1 + s2^{r}](img/file1354.png "1
    + s2^{r}") and controlled by qubit ![1 + s2^{r} + 2^{r - 1}](img/file1355.png
    "1 + s2^{r} + 2^{r - 1}").
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹ç›®æ ‡åœ¨é‡å­ä½ ![1 + s2^{r}](img/file1354.png "1 + s2^{r}") ä¸Šã€ç”±é‡å­ä½ ![1 + s2^{r} + 2^{r
    - 1}](img/file1355.png "1 + s2^{r} + 2^{r - 1}") æ§åˆ¶çš„CNOTæ“ä½œè¿›è¡Œåº”ç”¨ã€‚
- en: Apply a rotation ![R_{Y}(\theta_{r,s})](img/file1356.png "R_{Y}(\theta_{r,s})")
    on qubit ![1 + s2^{r}](img/file1354.png "1 + s2^{r}").
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­æ¯”ç‰¹![1 + s2^{r}](img/file1354.png "1 + s2^{r}")ä¸Šåº”ç”¨ä¸€ä¸ª![R_{Y}(\theta_{r,s})](img/file1356.png
    "R_{Y}(\theta_{r,s})")æ—‹è½¬ã€‚
- en: -![-](img/file1343.png "-")
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![img/file1343.png "-"]
- en: '-'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: An image is worth a thousand words, so, please, refer to *Figure* [*10.3*](#Figure10.3)
    for a depiction of the output of this procedure for ![k = 3](img/file1344.png
    "k = 3").
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸€å›¾èƒœåƒè¨€ï¼Œå› æ­¤ï¼Œè¯·å‚è€ƒ*å›¾* [*10.3*](#Figure10.3)ä»¥äº†è§£æ­¤è¿‡ç¨‹åœ¨![k = 3](img/file1344.png "k = 3")æ—¶çš„è¾“å‡ºæè¿°ã€‚
- en: The tree tensor variational form fits best in quantum neural networks designed
    to work as binary classifiers. The most natural measurement operation that can
    be used in conjunction with it is the obtention of the expected value of the first
    qubit, as measured in the computational basis.
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æ ‘å¼ é‡å˜åˆ†å½¢å¼æœ€é€‚åˆç”¨äºä½œä¸ºäºŒåˆ†ç±»å™¨çš„é‡å­ç¥ç»ç½‘ç»œã€‚å¯ä»¥ä¸ä¹‹ç»“åˆä½¿ç”¨çš„æœ€è‡ªç„¶çš„æµ‹é‡æ“ä½œæ˜¯è®¡ç®—åŸºä¸‹ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹çš„æœŸæœ›å€¼ã€‚
- en: As a curiosity, the name of the tree tensor variational form comes from mathematical
    objects that are used for the simulation of physics systems and also in some machine
    learning models. See the survey paper by RomÃ¡n OrÃºs for model details [[71](ch030.xhtml#Xorus2014practical)].
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½œä¸ºä¸€ç§å¥½å¥‡ï¼Œæ ‘å¼ é‡å˜åˆ†å½¢å¼çš„åç§°æ¥æºäºç”¨äºç‰©ç†ç³»ç»Ÿæ¨¡æ‹Ÿçš„æ•°å­¦å¯¹è±¡ï¼Œä¹Ÿç”¨äºä¸€äº›æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­ã€‚æœ‰å…³æ¨¡å‹ç»†èŠ‚ï¼Œè¯·å‚é˜…RomÃ¡n OrÃºsçš„ç»¼è¿°è®ºæ–‡ [[71](ch030.xhtml#Xorus2014practical)]ã€‚
- en: '![Figure 10.3: Tree tensor variational form on 8 = 2^{3} qubits](img/file1358.png)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![å›¾10.3ï¼šåœ¨8 = 2^{3}é‡å­æ¯”ç‰¹ä¸Šçš„æ ‘å¼ é‡å˜åˆ†å½¢å¼](img/file1358.png)'
- en: '**Figure 10.3**: Tree tensor variational form on ![8 = 2^{3}](img/file1357.png
    "8 = 2^{3}") qubits'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å›¾10.3**ï¼šåœ¨![8 = 2^{3}](img/file1357.png "8 = 2^{3}")é‡å­æ¯”ç‰¹ä¸Šçš„æ ‘å¼ é‡å˜åˆ†å½¢å¼'
- en: '**Strongly entangling layers**: The strongly entangling layers variational
    form acts on ![n](img/file244.png "n") qubits and can have any number ![k](img/file317.png
    "k") of layers. Each layer ![l](img/file514.png "l") is given a **range** ![r_{l}](img/file1359.png
    "r_{l}"). In total, the variational form uses ![3nk](img/file1360.png "3nk") parameters
    of the form'
  id: totrans-79
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¼ºçº ç¼ å±‚**ï¼šå¼ºçº ç¼ å±‚å˜åˆ†å½¢å¼ä½œç”¨äº![n](img/file244.png "n")ä¸ªé‡å­æ¯”ç‰¹ï¼Œå¹¶ä¸”å¯ä»¥æœ‰ä»»æ„æ•°é‡çš„![k](img/file317.png
    "k")å±‚ã€‚æ¯ä¸€å±‚![l](img/file514.png "l")éƒ½æœ‰ä¸€ä¸ª**èŒƒå›´**![r_{l}](img/file1359.png "r_{l}")ã€‚æ€»å…±ï¼Œå˜åˆ†å½¢å¼ä½¿ç”¨äº†![3nk](img/file1360.png
    "3nk")ä¸ªå‚æ•°ï¼Œå½¢å¼å¦‚ä¸‹'
- en: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.](img/file1361.png
    "\theta_{ljx},\qquad l = 1,\ldots,k,\qquad j = 1,\ldots,n,\qquad x = 1,2,3.")
    |'
- en: 'The form is defined by the following algorithm:'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½¢å¼ç”±ä»¥ä¸‹ç®—æ³•å®šä¹‰ï¼š
- en: '**procedure** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**è¿‡ç¨‹** StronglyEntanglingLayers(![n,k,r,\theta](img/file1362.png "n,k,r,\theta"))'
- en: '**for all** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **do**'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![l = 1,\ldots,k](img/file1363.png "l = 1,\ldots,k") **æ‰§è¡Œ**'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **æ‰§è¡Œ**'
- en: Apply a rotation ![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")
    on qubit ![j](img/file258.png "j").
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­æ¯”ç‰¹![j](img/file258.png "j")ä¸Šåº”ç”¨ä¸€ä¸ª![R_{Z}(\theta_{lj1})](img/file1364.png "R_{Z}(\theta_{lj1})")æ—‹è½¬ã€‚
- en: Apply a rotation ![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")
    on qubit ![j](img/file258.png "j").
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­æ¯”ç‰¹![j](img/file258.png "j")ä¸Šåº”ç”¨ä¸€ä¸ª![R_{Y}(\theta_{lj2})](img/file1365.png "R_{Y}(\theta_{lj2})")æ—‹è½¬ã€‚
- en: Apply a rotation ![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")
    on qubit ![j](img/file258.png "j").
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­æ¯”ç‰¹![j](img/file258.png "j")ä¸Šåº”ç”¨ä¸€ä¸ª![R_{Z}(\theta_{lj3})](img/file1366.png "R_{Z}(\theta_{lj3})")æ—‹è½¬ã€‚
- en: '-'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: '**for all** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **do**'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å¯¹æ‰€æœ‰** ![j = 1,\ldots,n](img/file980.png "j = 1,\ldots,n") **æ‰§è¡Œ**'
- en: Apply a CNOT operation controlled by qubit ![j](img/file258.png "j") and with
    target on qubit ![\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1](img/file1367.png
    "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\ N\rbrack + 1").
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨é‡å­æ¯”ç‰¹![j](img/file258.png "j")çš„æ§åˆ¶ä¸‹åº”ç”¨ä¸€ä¸ªCNOTæ“ä½œï¼Œç›®æ ‡ä¸ºé‡å­æ¯”ç‰¹![\lbrack(j + r_{l} - 1)\
    \operatorname{mod}\ N\rbrack + 1](img/file1367.png "\lbrack(j + r_{l} - 1)\ \operatorname{mod}\
    N\rbrack + 1")ã€‚
- en: -![-](img/file1343.png "-")
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: -![img/file1343.png "-"]
- en: '-'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '-'
- en: You may find a representation of a sample of this form in *Figure* [*10.4*](#Figure10.4).
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åœ¨*å›¾* [*10.4*](#Figure10.4)ä¸­æ‰¾åˆ°ä¸€ä¸ªè¿™ç§å½¢å¼çš„æ ·æœ¬è¡¨ç¤ºã€‚
- en: '![Figure 10.4: Strongly entangling layers form on four qubits and two layers
    with respective ranges 1 and 2](img/file1368.png)'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![å›¾10.4ï¼šåœ¨å››ä¸ªé‡å­æ¯”ç‰¹ä¸Šå½¢æˆå¼ºçº ç¼ å±‚ï¼Œä»¥åŠä¸¤ä¸ªå…·æœ‰ç›¸åº”èŒƒå›´1å’Œ2çš„å±‚](img/file1368.png)'
- en: '**Figure 10.4**: Strongly entangling layers form on four qubits and two layers
    with respective ranges ![1](img/file13.png "1") and ![2](img/file302.png "2")**'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**å›¾10.4**ï¼šåœ¨å››ä¸ªé‡å­æ¯”ç‰¹ä¸Šå½¢æˆå¼ºçº ç¼ å±‚ï¼Œä»¥åŠä¸¤ä¸ªå…·æœ‰ç›¸åº”èŒƒå›´![1](img/file13.png "1")å’Œ![2](img/file302.png
    "2")çš„å±‚**'
- en: '**As a final remark, our choice to use mostly ![Y](img/file11.png "Y") rotations
    in the previous examples of variational forms is somewhat arbitrary. We couldâ€™ve
    also used ![X](img/file9.png "X") rotations, for example. The same goes for our
    choice to use controlled-![X](img/file9.png "X") operations in the entanglement
    circuits. We could have used a different controlled operation, for instance. In
    addition to this, in the two-local variational form, there are more options for
    the distribution of gates in the entanglement circuit beyond the one that we have
    considered. Our entanglement circuit is said to have a â€linearâ€ arrangement of
    gates, but other possibilities are shown in *Figure* [*10.5*](#Figure10.5).'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**ä½œä¸ºæœ€åçš„è¯„è®ºï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨ä¹‹å‰çš„å˜åˆ†å½¢å¼ç¤ºä¾‹ä¸­ä¸»è¦ä½¿ç”¨![Y](img/file11.png "Y")æ—‹è½¬æ˜¯æœ‰äº›ä»»æ„çš„ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨![X](img/file9.png
    "X")æ—‹è½¬ï¼Œä¾‹å¦‚ã€‚åŒæ ·ï¼Œæˆ‘ä»¬é€‰æ‹©åœ¨çº ç¼ ç”µè·¯ä¸­ä½¿ç”¨å—æ§-![X](img/file9.png "X")æ“ä½œä¹Ÿæ˜¯ä»»æ„çš„ã€‚æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨ä¸åŒçš„å—æ§æ“ä½œï¼Œä¾‹å¦‚ã€‚æ­¤å¤–ï¼Œåœ¨ä¸¤å±€éƒ¨å˜åˆ†å½¢å¼ä¸­ï¼Œåœ¨çº ç¼ ç”µè·¯ä¸­é—¨çš„åˆ†å¸ƒè¿˜æœ‰æ›´å¤šé€‰æ‹©ï¼Œè€Œä¸ä»…ä»…æ˜¯æˆ‘ä»¬æ‰€è€ƒè™‘çš„é‚£ç§ã€‚æˆ‘ä»¬çš„çº ç¼ ç”µè·¯è¢«è®¤ä¸ºå…·æœ‰â€œçº¿æ€§â€çš„é—¨æ’åˆ—ï¼Œä½†å…¶ä»–å¯èƒ½æ€§åœ¨*å›¾10.5*ä¸­æœ‰æ‰€å±•ç¤ºã€‚'
- en: '![(a) Linear](img/file1369.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![ï¼ˆaï¼‰çº¿æ€§](img/file1369.png)'
- en: '**(a)** Linear'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '**(a)** çº¿æ€§'
- en: '![(b) Circular](img/file1370.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![ï¼ˆbï¼‰åœ†å½¢](img/file1370.jpg)'
- en: '**(b)** Circular'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '**(b)** åœ†å½¢'
- en: '![(c) Full](img/file1371.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![ï¼ˆcï¼‰å®Œæ•´](img/file1371.jpg)'
- en: '**(c)** Full'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '**(c)** å®Œæ•´'
- en: '**Figure 10.5**: Different entanglement circuits'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾10.5**ï¼šä¸åŒçš„çº ç¼ ç”µè·¯'
- en: This is all we need to know, for now, about variational forms. Combined with
    our previous knowledge of feature maps, this ends our analysis of the elements
    of a quantum neural networkâ€¦almost. We still have to dive deeper into that seemingly
    innocent measurement operation at the end of every quantum neural network.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯æˆ‘ä»¬ç›®å‰éœ€è¦äº†è§£çš„æ‰€æœ‰å…³äºå˜åˆ†å½¢å¼çš„å†…å®¹ã€‚ç»“åˆæˆ‘ä»¬ä¹‹å‰å¯¹ç‰¹å¾å›¾çš„äº†è§£ï¼Œè¿™ç»“æŸäº†æˆ‘ä»¬å¯¹é‡å­ç¥ç»ç½‘ç»œå…ƒç´ çš„è§£æâ€¦â€¦å‡ ä¹ã€‚æˆ‘ä»¬ä»ç„¶éœ€è¦æ·±å…¥æ¢ç©¶æ¯ä¸ªé‡å­ç¥ç»ç½‘ç»œæœ«å°¾çœ‹ä¼¼æ— è¾œçš„æµ‹é‡æ“ä½œã€‚
- en: 10.1.3 A word about measurements
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.3 å…³äºæµ‹é‡çš„è¯´æ˜
- en: 'As we saw back in *Chapter* [*7*](ch015.xhtml#x1-1190007), *VQE: Variational
    Quantum Eigensolver*, any physical observable can be represented by a Hermitian
    operator in such a way that all the possible outcomes of the measurement of the
    observable can be matched to the different eigenvalues of the operator. If you
    havenâ€™t done so already, please, have a look at *Section* [*7.1.1*](ch015.xhtml#x1-1210007.1.1)
    if you are not familiar with this.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬åœ¨*ç¬¬7ç« *[*7*](ch015.xhtml#x1-1190007)ä¸­çœ‹åˆ°çš„ï¼Œâ€œVQEï¼šå˜åˆ†é‡å­æœ¬å¾å€¼æ±‚è§£å™¨â€ï¼Œä»»ä½•ç‰©ç†å¯è§‚æµ‹é‡éƒ½å¯ä»¥é€šè¿‡ä¸€ä¸ªå„ç±³ç®—ç¬¦æ¥è¡¨ç¤ºï¼Œä½¿å¾—æ‰€æœ‰å¯èƒ½çš„æµ‹é‡ç»“æœéƒ½å¯ä»¥ä¸ç®—ç¬¦çš„ä¸åŒæœ¬å¾å€¼ç›¸å¯¹åº”ã€‚å¦‚æœä½ è¿˜ä¸ç†Ÿæ‚‰è¿™ä¸€ç‚¹ï¼Œè¯·æŸ¥çœ‹*ç¬¬7.1.1èŠ‚*[*7.1.1*](ch015.xhtml#x1-1210007.1.1)ã€‚
- en: When we measure a single qubit in the computational basis, the coordinate matrix
    with respect to the computational basis of the associated Hermitian operator could
    well be either of
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬åœ¨è®¡ç®—åŸºä¸‹æµ‹é‡å•ä¸ªé‡å­æ¯”ç‰¹æ—¶ï¼Œä¸ç›¸å…³å„ç±³ç®—ç¬¦çš„è®¡ç®—åŸºåæ ‡çŸ©é˜µå¯èƒ½æ˜¯ä»¥ä¸‹ä¹‹ä¸€
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix}
    1 & 0 \\ 0 & {- 1} \\ \end{pmatrix}.](img/file1372.png "M = \begin{pmatrix} 1
    & 0 \\ 0 & 0 \\ \end{pmatrix},\qquad Z = \begin{pmatrix} 1 & 0 \\ 0 & {- 1} \\
    \end{pmatrix}.") |'
- en: Both of these operators represent the measurement of a qubit, but they differ
    in the eigenvalues that they associate to the distinct outputs. The first operator
    associates the eigenvalues ![1](img/file13.png "1") and ![0](img/file12.png "0")
    to the qubitâ€™s value being ![0](img/file12.png "0") and ![1](img/file13.png "1")
    respectively, while the second observable associates the eigenvalues ![1](img/file13.png
    "1") and ![- 1](img/file312.png "- 1") to these outcomes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸¤ä¸ªç®—ç¬¦éƒ½è¡¨ç¤ºå¯¹é‡å­æ¯”ç‰¹çš„æµ‹é‡ï¼Œä½†å®ƒä»¬åœ¨å…³è”ä¸åŒè¾“å‡ºæ—¶çš„æœ¬å¾å€¼ä¸Šæœ‰æ‰€ä¸åŒã€‚ç¬¬ä¸€ä¸ªç®—ç¬¦å°†æœ¬å¾å€¼![1](img/file13.png "1")å’Œ![0](img/file12.png
    "0")åˆ†åˆ«å…³è”åˆ°é‡å­æ¯”ç‰¹çš„å€¼ä¸º![0](img/file12.png "0")å’Œ![1](img/file13.png "1")ï¼Œè€Œç¬¬äºŒä¸ªå¯è§‚æµ‹é‡å°†æœ¬å¾å€¼![1](img/file13.png
    "1")å’Œ![- 1](img/file312.png "- 1")å…³è”åˆ°è¿™äº›ç»“æœã€‚
- en: Exercise 10.1
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 10.1
- en: The purpose of this exercise is for you to get more familiar with Dirac notation.
    Show that the two previous Hermitian operators may be written, respectively, as
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç»ƒä¹ çš„ç›®çš„æ˜¯è®©ä½ æ›´ç†Ÿæ‚‰ç‹„æ‹‰å…‹ç¬¦å·ã€‚è¯æ˜å‰ä¸¤ä¸ªå„ç±³ç®—ç¬¦å¯ä»¥åˆ†åˆ«å†™æˆ
- en: '| ![1\left&#124; 0 \right\rangle\left\langle 0 \right&#124; + 0\left&#124;
    1 \right\rangle\left\langle 1 \right&#124; = \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;,\quad\left&#124; 0 \right\rangle\left\langle 0 \right&#124; - \left&#124;
    1 \right\rangle\left\langle 1 \right&#124;.](img/file1373.png "1\left&#124; 0
    \right\rangle\left\langle 0 \right&#124; + 0\left&#124; 1 \right\rangle\left\langle
    1 \right&#124; = \left&#124; 1 \right\rangle\left\langle 1 \right&#124;,\quad\left&#124;
    0 \right\rangle\left\langle 0 \right&#124; - \left&#124; 1 \right\rangle\left\langle
    1 \right&#124;.") |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ![1\left| 0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle
    1 \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.](img/file1373.png "1\left|
    0 \right\rangle\left\langle 0 \right| + 0\left| 1 \right\rangle\left\langle 1
    \right| = \left| 1 \right\rangle\left\langle 1 \right|,\quad\left| 0 \right\rangle\left\langle
    0 \right| - \left| 1 \right\rangle\left\langle 1 \right|.") |'
- en: '*Hint*: Remember that the product of a ket (column vector) and a bra (row vector)
    is a matrix. We saw an example of this back in *Section* *[*7.2.1*](ch015.xhtml#x1-1240007.2.1).*'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*æç¤º*ï¼šè®°ä½ï¼ŒåŸºï¼ˆåˆ—å‘é‡ï¼‰å’Œæï¼ˆè¡Œå‘é‡ï¼‰çš„ä¹˜ç§¯æ˜¯ä¸€ä¸ªçŸ©é˜µã€‚æˆ‘ä»¬ä¹‹å‰åœ¨ *ç¬¬7.2.1èŠ‚* ä¸­çœ‹åˆ°äº†ä¸€ä¸ªä¾‹å­ã€‚'
- en: '*As we will see later on in the chapter, frameworks such as PennyLane allow
    you to work with measurement operations defined by any Hermitian operator. This
    can give you a lot of flexibility when defining the measurement operation of a
    neural network. For instance, in an ![n](img/file244.png "n")-qubit circuit, you
    will be able to instruct PennyLane to compute the expectation value of the observable
    ![M \otimes \cdots \otimes M](img/file1374.png "M \otimes \cdots \otimes M"),
    which has as its coordinate representation in the computational basis the matrix'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ­£å¦‚æˆ‘ä»¬å°†åœ¨æœ¬ç« åé¢çœ‹åˆ°çš„é‚£æ ·ï¼Œæ¡†æ¶å¦‚PennyLaneå…è®¸ä½ ä½¿ç”¨ç”±ä»»ä½•å„ç±³ç®—å­å®šä¹‰çš„æµ‹é‡æ“ä½œã€‚è¿™å¯ä»¥åœ¨å®šä¹‰ç¥ç»ç½‘ç»œçš„æµ‹é‡æ“ä½œæ—¶ç»™ä½ å¸¦æ¥å¾ˆå¤šçµæ´»æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨ä¸€ä¸ª
    ![n](img/file244.png "n")-é‡å­ä½ç”µè·¯ä¸­ï¼Œä½ å°†èƒ½å¤ŸæŒ‡ç¤ºPennyLaneè®¡ç®—å¯è§‚æµ‹é‡ ![M \otimes \cdots \otimes
    M](img/file1374.png "M \otimes \cdots \otimes M") çš„æœŸæœ›å€¼ï¼Œå…¶åœ¨è®¡ç®—åŸºçš„åæ ‡è¡¨ç¤ºæ˜¯çŸ©é˜µ'
- en: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| ![\begin{pmatrix} 0 & & & \\ & \ddots & & \\ & & 0 & \\ & & & 1 \\ \end{pmatrix}_{2^{n}
    \times 2^{n}}.](img/file1375.png "\begin{pmatrix} 0 & & & \\  & \ddots & & \\  &
    & 0 & \\  & & & 1 \\ \end{pmatrix}_{2^{n} \times 2^{n}}.") |'
- en: Alternatively, you may want to consider the observable ![Z \otimes \cdots \otimes
    Z](img/file1376.png "Z \otimes \cdots \otimes Z"). It is easy to see how this
    observable will return ![+ 1](img/file1377.png "+ 1") if an even number of qubits
    are measured as ![0](img/file12.png "0"), and ![- 1](img/file312.png "- 1") otherwise.
    Thatâ€™s the reason why ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes
    \cdots \otimes Z") is referred to as the **parity** observable.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œä½ å¯èƒ½æƒ³è€ƒè™‘å¯è§‚æµ‹é‡ ![Z \otimes \cdots \otimes Z](img/file1376.png "Z \otimes \cdots
    \otimes Z")ã€‚å¾ˆå®¹æ˜“çœ‹å‡ºï¼Œå¦‚æœæµ‹é‡åˆ°å¶æ•°ä¸ªé‡å­ä½ä¸º ![0](img/file12.png "0")ï¼Œåˆ™è¯¥å¯è§‚æµ‹é‡å°†è¿”å› ![+ 1](img/file1377.png
    "+ 1")ï¼Œå¦åˆ™è¿”å› ![- 1](img/file312.png "- 1")ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ ![Z \otimes \cdots \otimes Z](img/file1376.png
    "Z \otimes \cdots \otimes Z") è¢«ç§°ä¸º**å¶æ•°æ€§**å¯è§‚æµ‹é‡çš„åŸå› ã€‚
- en: Of course, you will also be able to take the measurement operation to be a good
    old expectation value on the first qubit. But, the point is, thereâ€™s also a plethora
    of options available to you, should you want to explore them!
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ è¿˜å¯ä»¥å°†æµ‹é‡æ“ä½œè§†ä¸ºç¬¬ä¸€ä¸ªé‡å­ä½çš„ç»å…¸æœŸæœ›å€¼ã€‚ä½†æ˜¯ï¼Œé‡ç‚¹æ˜¯ï¼Œå¦‚æœä½ æ„¿æ„æ¢ç´¢ï¼Œè¿˜æœ‰å¾ˆå¤šå…¶ä»–é€‰é¡¹å¯ä¾›é€‰æ‹©ï¼
- en: As we mentioned before, observables are the final building blocks of every quantum
    neural network architecture. Quantum neural networks accept an input, which usually
    consists of classical data being fed through a feature map. The resulting quantum
    state is then transformed by a variational form and, lastly, some (classical)
    numerical data is obtained through a measurement operation. In this way, we have
    a â€black boxâ€ transforming some numerical inputs into outputs, that is, a model
    that â€” just like any other classical ML model â€” can be trained.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œå¯è§‚æµ‹é‡æ˜¯æ¯ä¸ªé‡å­ç¥ç»ç½‘ç»œæ¶æ„çš„æœ€ç»ˆæ„å»ºå—ã€‚é‡å­ç¥ç»ç½‘ç»œæ¥å—ä¸€ä¸ªè¾“å…¥ï¼Œè¿™é€šå¸¸ç”±é€šè¿‡ç‰¹å¾å›¾è¾“å…¥çš„ç»å…¸æ•°æ®ç»„æˆã€‚ç„¶åï¼Œé€šè¿‡ä¸€ä¸ªå˜åˆ†å½¢å¼å°†å¾—åˆ°çš„é‡å­çŠ¶æ€è½¬æ¢ï¼Œæœ€åé€šè¿‡æµ‹é‡æ“ä½œè·å¾—ä¸€äº›ï¼ˆç»å…¸ï¼‰æ•°å€¼æ•°æ®ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬å°±å¾—åˆ°äº†ä¸€ä¸ªå°†ä¸€äº›æ•°å€¼è¾“å…¥è½¬æ¢ä¸ºè¾“å‡ºçš„â€œé»‘ç›’â€ï¼Œå³ä¸€ä¸ªæ¨¡å‹â€”â€”å°±åƒä»»ä½•å…¶ä»–ç»å…¸æœºå™¨å­¦ä¹ æ¨¡å‹ä¸€æ ·â€”â€”å¯ä»¥è¢«è®­ç»ƒã€‚
- en: 'We have now defined what quantum neural networks are and learned how to construct
    them, at least in theory. That means we have a model. But this is quantum machine
    learning, so a model is not enough: we need to train it. And in order to do so,
    we will need, among other things, an optimization algorithm.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å·²ç»å®šä¹‰äº†é‡å­ç¥ç»ç½‘ç»œæ˜¯ä»€ä¹ˆï¼Œå¹¶ä¸”å­¦ä¹ äº†å¦‚ä½•æ„å»ºå®ƒä»¬ï¼Œè‡³å°‘åœ¨ç†è®ºä¸Šæ˜¯å¦‚æ­¤ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬æœ‰ä¸€ä¸ªæ¨¡å‹ã€‚ä½†æ˜¯ï¼Œè¿™æ˜¯é‡å­æœºå™¨å­¦ä¹ ï¼Œæ‰€ä»¥ä¸€ä¸ªæ¨¡å‹æ˜¯ä¸å¤Ÿçš„ï¼šæˆ‘ä»¬éœ€è¦å¯¹å…¶è¿›è¡Œè®­ç»ƒã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å°†éœ€è¦ï¼ŒåŒ…æ‹¬ä½†ä¸é™äºï¼Œä¸€ä¸ªä¼˜åŒ–ç®—æ³•ã€‚
- en: 10.1.4 Gradient computation and the parameter shift rule
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.4 æ¢¯åº¦è®¡ç®—å’Œå‚æ•°å¹³ç§»è§„åˆ™
- en: Although it is not the only option, the optimization algorithms that we shall
    use for quantum neural networks will be gradient descent algorithms; in particular,
    we will use the Adam optimizer. But, as we saw in *Chapter* [*8*](ch017.xhtml#x1-1390008)*,*
    *What is Quantum Machine Learning?*, this algorithm needs to obtain the gradient
    of the expected value of a loss function in terms of the optimizable parameters.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: è™½ç„¶è¿™ä¸æ˜¯å”¯ä¸€çš„é€‰æ‹©ï¼Œä½†æˆ‘ä»¬å°†ä¸ºé‡å­ç¥ç»ç½‘ç»œä½¿ç”¨çš„ä¼˜åŒ–ç®—æ³•å°†æ˜¯æ¢¯åº¦ä¸‹é™ç®—æ³•ï¼›ç‰¹åˆ«æ˜¯ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨Adamä¼˜åŒ–å™¨ã€‚ä½†æ˜¯ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬8ç« *[*8*](ch017.xhtml#x1-1390008)*,*
    *ä»€ä¹ˆæ˜¯é‡å­æœºå™¨å­¦ä¹ ï¼Ÿ*ä¸­çœ‹åˆ°çš„ï¼Œè¿™ä¸ªç®—æ³•éœ€è¦è·å¾—æŸå¤±å‡½æ•°æœŸæœ›å€¼çš„æ¢¯åº¦ï¼Œç›¸å¯¹äºå¯ä¼˜åŒ–å‚æ•°è€Œè¨€ã€‚
- en: 'Since our model uses a quantum circuit, the computation of these gradients
    is not entirely trivial. We shall now go briefly over the three main kinds of
    differentiation methods in which these gradient computations may be carried out:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬çš„æ¨¡å‹ä½¿ç”¨é‡å­ç”µè·¯ï¼Œè¿™äº›æ¢¯åº¦çš„è®¡ç®—å¹¶ä¸å®Œå…¨ç®€å•ã€‚æˆ‘ä»¬ç°åœ¨ç®€è¦åœ°å›é¡¾ä¸€ä¸‹ä¸‰ç§ä¸»è¦çš„å¾®åˆ†æ–¹æ³•ï¼Œåœ¨è¿™äº›æ–¹æ³•ä¸­å¯ä»¥æ‰§è¡Œè¿™äº›æ¢¯åº¦è®¡ç®—ï¼š
- en: '**Numerical approximation**: Of course, we have a method that always works.
    It may not always be the most efficient one, but itâ€™s always there. In order to
    compute gradients, we may just estimate them numerically. In order to do this,
    of course, we will have to run our quantum neural network plenty of times.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ•°å€¼é€¼è¿‘**ï¼šå½“ç„¶ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ€»æ˜¯æœ‰æ•ˆçš„æ–¹æ³•ã€‚å®ƒå¯èƒ½ä¸æ˜¯æœ€æœ‰æ•ˆçš„æ–¹æ³•ï¼Œä½†æ€»æ˜¯å­˜åœ¨çš„ã€‚ä¸ºäº†è®¡ç®—æ¢¯åº¦ï¼Œæˆ‘ä»¬å¯èƒ½åªéœ€è¦è¿›è¡Œæ•°å€¼ä¼°è®¡ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬ä¸å¾—ä¸å¤šæ¬¡è¿è¡Œæˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œã€‚'
- en: Just to exemplify this a little bit, if we had a real-valued function taking
    ![n](img/file244.png "n") real inputs ![\left. f:R^{n}\rightarrow R \right.](img/file1378.png
    "\left. f:R^{n}\rightarrow R \right."), we could approximate its partial derivatives
    as
  id: totrans-124
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ä¸ºäº†ç¨å¾®ä¸¾ä¾‹è¯´æ˜è¿™ä¸€ç‚¹ï¼Œå¦‚æœæˆ‘ä»¬æœ‰ä¸€ä¸ªæ¥å—![n](img/file244.png "n")ä¸ªå®æ•°è¾“å…¥çš„å®å€¼å‡½æ•° ![\left. f:R^{n}\rightarrow
    R \right.](img/file1378.png "\left. f:R^{n}\rightarrow R \right.")ï¼Œæˆ‘ä»¬å¯ä»¥è¿‘ä¼¼å…¶åå¯¼æ•°å¦‚ä¸‹
- en: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![\frac{\partial f}{\partial x_{j}} = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n})
    - f(x_{1},\ldots,x_{n})}{h}](img/file1379.png "\frac{\partial f}{\partial x_{j}}
    = \frac{f(x_{1},\ldots,x_{j} + h,\ldots,x_{n}) - f(x_{1},\ldots,x_{n})}{h}") |'
- en: for a sufficiently small value of ![h](img/file519.png "h"). Thatâ€™s, of course,
    the most naive way to numerically approximate a derivative, but hopefully itâ€™s
    enough to give you an intuition of how this works.
  id: totrans-126
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äº![h](img/file519.png "h")è¶³å¤Ÿå°çš„å€¼ã€‚å½“ç„¶ï¼Œè¿™æ˜¯æ•°å€¼é€¼è¿‘å¯¼æ•°æœ€å¤©çœŸçš„ä¸€ç§æ–¹æ³•ï¼Œä½†å¸Œæœ›è¿™è¶³ä»¥è®©ä½ å¯¹å®ƒæ˜¯å¦‚ä½•å·¥ä½œçš„æœ‰ä¸€ä¸ªç›´è§‚çš„ç†è§£ã€‚
- en: '**Automatic differentiation**: Given the current state of real quantum hardware,
    odds are that most of the quantum neural networks that you will train will run
    on simulators. As non-ideal as this may be, it comes with some advantages. Most
    notably, on simulated quantum neural networks, a classical computer may compute
    exact gradients using techniques similar to those employed on classical neural
    networks. If you are interested, the book AurÃ©lien GÃ©ron [[104](ch030.xhtml#Xhandsonml),
    Chapter 10] and the one by Shai Shalev-Shwartz and Shai Ben-David [[105](ch030.xhtml#Xunderml),
    Â§20.6] discuss these techniques for classical neural networks.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è‡ªåŠ¨å¾®åˆ†**ï¼šé‰´äºå½“å‰çœŸå®é‡å­ç¡¬ä»¶çš„çŠ¶æ€ï¼Œä½ å°†è®­ç»ƒçš„å¤§å¤šæ•°é‡å­ç¥ç»ç½‘ç»œå¾ˆå¯èƒ½ä¼šåœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œã€‚å°½ç®¡è¿™å¯èƒ½å¹¶ä¸ç†æƒ³ï¼Œä½†å®ƒå¸¦æ¥äº†ä¸€äº›ä¼˜åŠ¿ã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œåœ¨æ¨¡æ‹Ÿé‡å­ç¥ç»ç½‘ç»œä¸Šï¼Œç»å…¸è®¡ç®—æœºå¯ä»¥ä½¿ç”¨ç±»ä¼¼äºåœ¨ç»å…¸ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨çš„æŠ€å·§æ¥è®¡ç®—ç²¾ç¡®çš„æ¢¯åº¦ã€‚å¦‚æœä½ æ„Ÿå…´è¶£ï¼ŒAurÃ©lien
    GÃ©ronçš„ä¹¦ç± [[104](ch030.xhtml#Xhandsonml)ï¼Œç¬¬10ç« ] å’ŒShai Shalev-Shwartzä»¥åŠShai Ben-Davidçš„ä¹¦ç±
    [[105](ch030.xhtml#Xunderml)ï¼Œç¬¬20.6èŠ‚] è®¨è®ºäº†è¿™äº›ç»å…¸ç¥ç»ç½‘ç»œçš„æŠ€å·§ã€‚'
- en: '**The parameter shift rule**: The standard automatic differentiation techniques
    can only be used on simulators. Fortunately, there is still another way to compute
    gradients when executing quantum neural networks on real hardware: using the **parameter
    shift rule**. As the name suggests, this technique enables us to compute gradients
    by using the same circuit in the quantum neural network, yet shifting the values
    of the optimizable parameters. The parameter shift rule canâ€™t always be applied,
    but it works on many common cases and can be used in conjunction with other techniques,
    such as numerical approximation.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‚æ•°å¹³ç§»è§„åˆ™**ï¼šæ ‡å‡†çš„è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯åªèƒ½åœ¨æ¨¡æ‹Ÿå™¨ä¸Šä½¿ç”¨ã€‚å¹¸è¿çš„æ˜¯ï¼Œå½“åœ¨çœŸå®ç¡¬ä»¶ä¸Šæ‰§è¡Œé‡å­ç¥ç»ç½‘ç»œæ—¶ï¼Œä»ç„¶æœ‰å¦ä¸€ç§æ–¹æ³•æ¥è®¡ç®—æ¢¯åº¦ï¼šä½¿ç”¨**å‚æ•°å¹³ç§»è§„åˆ™**ã€‚æ­£å¦‚å…¶åæ‰€ç¤ºï¼Œè¿™é¡¹æŠ€æœ¯ä½¿æˆ‘ä»¬èƒ½å¤Ÿé€šè¿‡åœ¨é‡å­ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨ç›¸åŒçš„ç”µè·¯ï¼ŒåŒæ—¶æ”¹å˜å¯ä¼˜åŒ–å‚æ•°çš„å€¼æ¥è®¡ç®—æ¢¯åº¦ã€‚å‚æ•°å¹³ç§»è§„åˆ™å¹¶ä¸æ€»æ˜¯é€‚ç”¨ï¼Œä½†å®ƒé€‚ç”¨äºè®¸å¤šå¸¸è§æƒ…å†µï¼Œå¹¶ä¸”å¯ä»¥ä¸å…¶ä»–æŠ€æœ¯ç»“åˆä½¿ç”¨ï¼Œä¾‹å¦‚æ•°å€¼è¿‘ä¼¼ã€‚'
- en: We wonâ€™t get into the details of how this method works, but you may have a look
    at a research paper by Maria Schuld and others [[109](ch030.xhtml#Xpshift-schuld)]
    for more information. For example, if you had a circuit consisting of a single
    rotation gate ![R_{X}(\theta)](img/file1380.png "R_{X}(\theta)") and the measurement
    of its expectation value ![E(\theta)](img/file1381.png "E(\theta)"), you would
    be able to compute its derivative with respect to ![\theta](img/file89.png "\theta")
    as
  id: totrans-129
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸ä¼šæ·±å…¥æ¢è®¨è¿™ç§æ–¹æ³•æ˜¯å¦‚ä½•å·¥ä½œçš„ç»†èŠ‚ï¼Œä½†ä½ å¯ä»¥æŸ¥çœ‹Maria Schuldå’Œå…¶ä»–äººå‘è¡¨çš„ç ”ç©¶è®ºæ–‡[[109](ch030.xhtml#Xpshift-schuld)]ä»¥è·å–æ›´å¤šä¿¡æ¯ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æœ‰ä¸€ä¸ªç”±å•ä¸ªæ—‹è½¬é—¨![R_{X}(\theta)](img/file1380.png
    "R_{X}(\theta)")å’Œå…¶æœŸæœ›å€¼![E(\theta)](img/file1381.png "E(\theta)")çš„æµ‹é‡ç»„æˆçš„ç”µè·¯ï¼Œä½ å°†èƒ½å¤Ÿè®¡ç®—å…¶å¯¹![\theta](img/file89.png
    "\theta")çš„å¯¼æ•°ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
  id: totrans-130
  prefs:
  - PREF_IND
  type: TYPE_TB
  zh: '| ![{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).](img/file1382.png
    "{\nabla}_{\theta}E(\theta) = \frac{1}{2}\left( {E\left( {\theta + \frac{\pi}{2}}
    \right) - E\left( {\theta - \frac{\pi}{2}} \right)} \right).") |'
- en: 'This is similar to what happens with some trigonometric functions: for instance,
    you can express the derivative of the sine function in terms of shifted values
    of the same sine function.'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¿™ä¸æŸäº›ä¸‰è§’å‡½æ•°å‘ç”Ÿçš„æƒ…å†µç±»ä¼¼ï¼šä¾‹å¦‚ï¼Œä½ å¯ä»¥ç”¨ç›¸åŒæ­£å¼¦å‡½æ•°çš„å¹³ç§»å€¼æ¥è¡¨è¾¾æ­£å¼¦å‡½æ•°çš„å¯¼æ•°ã€‚
- en: For our purposes, it will suffice to know that it exists and can be used. Of
    course, the parameter shift rule can also be used on simulators!
  id: totrans-132
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç›®çš„æ¥è¯´ï¼Œäº†è§£å®ƒå­˜åœ¨å¹¶ä¸”å¯ä»¥è¢«ä½¿ç”¨å°±è¶³å¤Ÿäº†ã€‚å½“ç„¶ï¼Œå‚æ•°å¹³ç§»è§„åˆ™ä¹Ÿå¯ä»¥ç”¨äºæ¨¡æ‹Ÿå™¨ï¼
- en: Important note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æç¤º
- en: When quantum neural networks are run on simulators, gradients can be computed
    using automatic differentiation techniques analogous to those of classical machine
    learning. When they are run on either real hardware or simulators, these gradients
    can also be computed â€” at least on many cases â€” using the parameter shift rule.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: å½“é‡å­ç¥ç»ç½‘ç»œåœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œæ—¶ï¼Œå¯ä»¥ä½¿ç”¨ç±»ä¼¼äºç»å…¸æœºå™¨å­¦ä¹ çš„è‡ªåŠ¨å¾®åˆ†æŠ€æœ¯æ¥è®¡ç®—æ¢¯åº¦ã€‚å½“å®ƒä»¬åœ¨çœŸå®ç¡¬ä»¶æˆ–æ¨¡æ‹Ÿå™¨ä¸Šè¿è¡Œæ—¶ï¼Œè¿™äº›æ¢¯åº¦ä¹Ÿå¯ä»¥è®¡ç®—â€”â€”è‡³å°‘åœ¨è®¸å¤šæƒ…å†µä¸‹å¯ä»¥â€”â€”ä½¿ç”¨å‚æ•°å¹³ç§»è§„åˆ™ã€‚
- en: Alternatively, numerical approximation is always an effective way to compute
    gradients.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œæ•°å€¼è¿‘ä¼¼å§‹ç»ˆæ˜¯è®¡ç®—æ¢¯åº¦çš„ä¸€ç§æœ‰æ•ˆæ–¹æ³•ã€‚
- en: As we have mentioned, all of these methods are already fully implemented in
    PennyLane, and we will try them all out in the following section.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬æåˆ°çš„ï¼Œæ‰€æœ‰è¿™äº›æ–¹æ³•å·²ç»åœ¨PennyLaneä¸­å®Œå…¨å®ç°ï¼Œæˆ‘ä»¬å°†åœ¨ä¸‹ä¸€èŠ‚å°è¯•æ‰€æœ‰è¿™äº›æ–¹æ³•ã€‚
- en: To learn moreâ€¦
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šâ€¦
- en: 'Everything looks good and promising, but quantum neural networks also pose
    some challenges when it comes to training them. Most notably, they are known to
    be vulnerable to **barren plateaus**: situations in which the training gradients
    vanish and, thus, the training can no longer progress (see the paper by McClean
    et. al for further explanation [[67](ch030.xhtml#Xmcclean2018barren)]). It is
    also known that the kind of measurement operation used and the depth of the QNN
    play a role in how likely these barren plateaus are to be found. This is studied,
    for instance, in a paper by Cerezo and collaborators [[24](ch030.xhtml#Xcerezo2021cost)].
    In any case, you should be vigilant when training your QNNs, and follow the literature
    for possible solutions should barren plateaus threaten the learning of your models.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€åˆ‡çœ‹èµ·æ¥éƒ½å¾ˆå¥½ï¼Œå¾ˆæœ‰å¸Œæœ›ï¼Œä½†é‡å­ç¥ç»ç½‘ç»œåœ¨è®­ç»ƒæ—¶ä¹Ÿå¸¦æ¥äº†ä¸€äº›æŒ‘æˆ˜ã€‚æœ€å€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œå®ƒä»¬å·²çŸ¥å®¹æ˜“å—åˆ°**è´«ç˜ å¹³åŸ**çš„å½±å“ï¼šè®­ç»ƒæ¢¯åº¦æ¶ˆå¤±çš„æƒ…å†µï¼Œå› æ­¤è®­ç»ƒæ— æ³•å†è¿›è¡Œï¼ˆå‚è§McCleanç­‰äººæ’°å†™çš„è®ºæ–‡ä»¥è·å–è¿›ä¸€æ­¥è§£é‡Š[[67](ch030.xhtml#Xmcclean2018barren)]ï¼‰ã€‚è¿˜çŸ¥é“æ‰€ä½¿ç”¨çš„æµ‹é‡æ“ä½œç±»å‹å’ŒQNNçš„æ·±åº¦ä¼šå½±å“è¿™äº›è´«ç˜ å¹³åŸå‡ºç°çš„å¯èƒ½æ€§ã€‚ä¾‹å¦‚ï¼Œåœ¨CerezoåŠå…¶åˆä½œè€…çš„è®ºæ–‡ä¸­å¯¹æ­¤è¿›è¡Œäº†ç ”ç©¶[[24](ch030.xhtml#Xcerezo2021cost)]ã€‚æ— è®ºå¦‚ä½•ï¼Œä½ åœ¨è®­ç»ƒä½ çš„QNNæ—¶åº”è¯¥ä¿æŒè­¦æƒ•ï¼Œå¦‚æœè´«ç˜ å¹³åŸå¨èƒåˆ°ä½ çš„æ¨¡å‹çš„å­¦ä¹ ï¼Œåº”éµå¾ªæ–‡çŒ®ä¸­çš„å¯èƒ½è§£å†³æ–¹æ¡ˆã€‚
- en: We now have all the ingredients necessary to construct and train quantum neural
    networks. But before we get to do that in practice, we will discuss a few techniques
    and tips that will help you get the most of our brand new quantum machine learning
    models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç°åœ¨å·²ç»æ‹¥æœ‰äº†æ„å»ºå’Œè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œæ‰€éœ€çš„æ‰€æœ‰æˆåˆ†ã€‚ä½†åœ¨æˆ‘ä»¬å®é™…æ“ä½œä¹‹å‰ï¼Œæˆ‘ä»¬å°†è®¨è®ºä¸€äº›æŠ€æœ¯å’ŒæŠ€å·§ï¼Œè¿™å°†å¸®åŠ©ä½ æœ€å¤§é™åº¦åœ°å‘æŒ¥æˆ‘ä»¬å…¨æ–°çš„é‡å­æœºå™¨å­¦ä¹ æ¨¡å‹çš„ä¼˜åŠ¿ã€‚
- en: 10.1.5 Practical usage of quantum neural networks
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1.5 é‡å­ç¥ç»ç½‘ç»œçš„å®é™…åº”ç”¨
- en: 'The following are a collection of ideas that you should keep in mind when designing
    QNN models and training them. You can think of it as a summary of the previous
    sections, with a few highlights from *Chapter* [*8*](ch017.xhtml#x1-1390008)*,
    What is Quantum* *Machine Learning?*:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯åœ¨è®¾è®¡QNNæ¨¡å‹å’Œè®­ç»ƒå®ƒä»¬æ—¶ä½ åº”è¯¥è®°ä½çš„ä¸€äº›æƒ³æ³•ã€‚ä½ å¯ä»¥å°†å…¶è§†ä¸ºå‰å‡ èŠ‚çš„æ€»ç»“ï¼Œå…¶ä¸­åŒ…å«æ¥è‡ª*ç¬¬8ç« *[*8*](ch017.xhtml#x1-1390008)*ï¼Œä»€ä¹ˆæ˜¯é‡å­*æœºå™¨å­¦ä¹ ï¼Ÿ*çš„ä¸€äº›äº®ç‚¹ï¼š
- en: '**Make wise choices**: When you set out to design a QNN, you have three important
    decisions to make: you have to pick a feature map, a variational form, and a measurement
    operation. Be intentional about these choices and consider the problem and the
    data that you are working with. Your decisions can influence how likely you are
    to find barren plateaus, for instance. A good recommendation is to check the literature
    for similar problems and to build up from there.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ˜æ™ºçš„é€‰æ‹©**ï¼šå½“ä½ å¼€å§‹è®¾è®¡ä¸€ä¸ªQNNæ—¶ï¼Œä½ å¿…é¡»åšå‡ºä¸‰ä¸ªé‡è¦çš„å†³å®šï¼šä½ å¿…é¡»é€‰æ‹©ä¸€ä¸ªç‰¹å¾å›¾ã€ä¸€ä¸ªå˜åˆ†å½¢å¼å’Œä¸€ä¸ªæµ‹é‡æ“ä½œã€‚å¯¹è¿™äº›é€‰æ‹©è¦æœ‰æ„ä¸ºä¹‹ï¼Œå¹¶è€ƒè™‘ä½ æ­£åœ¨å¤„ç†çš„é—®é¢˜å’Œæ•°æ®ã€‚ä½ çš„å†³å®šå¯èƒ½ä¼šå½±å“ä½ æ‰¾åˆ°è´«ç˜ å¹³åŸçš„å¯èƒ½æ€§ã€‚ä¸€ä¸ªå¥½çš„å»ºè®®æ˜¯æ£€æŸ¥æ–‡çŒ®ä¸­ç±»ä¼¼çš„é—®é¢˜ï¼Œå¹¶åœ¨æ­¤åŸºç¡€ä¸Šæ„å»ºã€‚'
- en: '**Size matters**: When you use a well-designed variational form, in general,
    the power of the resulting quantum neural network will be directly related to
    the number of optimizable parameters it has. Use too many parameters, and you
    may have a model that overfits. Use very few, and your model may end up underfitting.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å¤§å°å¾ˆé‡è¦**ï¼šå½“ä½ ä½¿ç”¨ä¸€ä¸ªè®¾è®¡è‰¯å¥½çš„å˜åˆ†å½¢å¼æ—¶ï¼Œé€šå¸¸ï¼Œæ‰€å¾—åˆ°çš„é‡å­ç¥ç»ç½‘ç»œçš„åŠ›é‡å°†ç›´æ¥ä¸å…¶æ‹¥æœ‰çš„å¯ä¼˜åŒ–å‚æ•°æ•°é‡ç›¸å…³ã€‚ä½¿ç”¨è¿‡å¤šçš„å‚æ•°ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°ä¸€ä¸ªè¿‡æ‹Ÿåˆçš„æ¨¡å‹ã€‚ä½¿ç”¨éå¸¸å°‘çš„å‚æ•°ï¼Œä½ çš„æ¨¡å‹æœ€ç»ˆå¯èƒ½æ¬ æ‹Ÿåˆã€‚'
- en: '**Optimize optimization**: For most problems, the Adam optimizer can be your
    go-to choice for training a quantum neural network. Remember that, as we discussed
    in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum* *Machine Learning?*,
    you will have to pick a learning rate and a batch size when using Adam.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼˜åŒ–ä¼˜åŒ–**ï¼šå¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼ŒAdamä¼˜åŒ–å™¨å¯ä»¥æ˜¯ä½ è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œçš„é»˜è®¤é€‰æ‹©ã€‚è®°ä½ï¼Œæ­£å¦‚æˆ‘ä»¬åœ¨*ç¬¬8ç« *[*8*](ch017.xhtml#x1-1390008)*ï¼Œä»€ä¹ˆæ˜¯é‡å­*æœºå™¨å­¦ä¹ ï¼Ÿ*ä¸­è®¨è®ºçš„é‚£æ ·ï¼Œå½“ä½ ä½¿ç”¨Adamæ—¶ï¼Œä½ å¿…é¡»é€‰æ‹©ä¸€ä¸ªå­¦ä¹ ç‡å’Œæ‰¹é‡å¤§å°ã€‚'
- en: A smaller learning rate will make the algorithm more accurate, but also slower.
    Analogously, a higher batch size should make the optimization more effective,
    to the detriment of execution time.
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: è¾ƒå°çš„å­¦ä¹ ç‡ä¼šä½¿ç®—æ³•æ›´å‡†ç¡®ï¼Œä½†ä¹Ÿä¼šæ›´æ…¢ã€‚ç±»ä¼¼åœ°ï¼Œè¾ƒå¤§çš„æ‰¹é‡å¤§å°åº”è¯¥ä¼šä½¿ä¼˜åŒ–æ›´æœ‰æ•ˆï¼Œä½†ä¼šæŸå®³æ‰§è¡Œæ—¶é—´ã€‚
- en: '**Feed your QNN properly**: The data that is fed to a quantum neural network
    should be normalized according to the requirements of the feature map in use.
    In addition, depending on the dimensions of the input data, you may want to rely
    on dimensionality reduction techniques.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æ­£ç¡®å–‚å…»ä½ çš„QNN**ï¼šæä¾›ç»™é‡å­ç¥ç»ç½‘ç»œçš„åº”è¯¥æ ¹æ®æ‰€ä½¿ç”¨çš„ç‰¹å¾å›¾çš„è¦æ±‚è¿›è¡Œå½’ä¸€åŒ–ã€‚æ­¤å¤–ï¼Œæ ¹æ®è¾“å…¥æ•°æ®çš„ç»´åº¦ï¼Œä½ å¯èƒ½æƒ³è¦ä¾èµ–é™ç»´æŠ€æœ¯ã€‚'
- en: Of course, the more data you have, the better. Nonetheless, one additional fact
    that you may want to take into account is that, under some conditions, quantum
    neural networks have been shown to need fewer data samples than classical neural
    networks in order to be successfully trained [[112](ch030.xhtml#Xqnn-lowdata)].
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: å½“ç„¶ï¼Œä½ æ‹¥æœ‰çš„æ•°æ®è¶Šå¤šï¼Œè¶Šå¥½ã€‚ä¸è¿‡ï¼Œä½ å¯èƒ½è¿˜æƒ³è€ƒè™‘çš„ä¸€ä¸ªé¢å¤–äº‹å®æ˜¯ï¼Œåœ¨æŸäº›æ¡ä»¶ä¸‹ï¼Œé‡å­ç¥ç»ç½‘ç»œå·²è¢«è¯æ˜åœ¨æˆåŠŸè®­ç»ƒæ—¶éœ€è¦æ¯”ç»å…¸ç¥ç»ç½‘ç»œæ›´å°‘çš„æ•°æ®æ ·æœ¬ [[112](ch030.xhtml#Xqnn-lowdata)]ã€‚
- en: To learn moreâ€¦
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤š...
- en: If you want to further boost the power of your quantum neural networks, you
    may want to consider the **data reuploading** technique [[110](ch030.xhtml#Xperez2020data)].
    In a vanilla QNN, you have a feature map ![F](img/file1320.png "F") dependent
    on some input data ![\overset{\rightarrow}{x}](img/file1206.png "\overset{\rightarrow}{x}"),
    which is then followed by a variational form ![V](img/file379.png "V") dependent
    on some optimizable parameters ![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}"). Data reuploading simply consists in repeating
    this scheme â€” any number of times you want â€” before performing the measurement
    operation of the QNN. The feature maps use the same input data in each repetition,
    but each instance of the variational form takes its own, independent, optimizable
    parameters.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¦è¿›ä¸€æ­¥å¢å¼ºä½ çš„é‡å­ç¥ç»ç½‘ç»œçš„èƒ½åŠ›ï¼Œä½ å¯èƒ½éœ€è¦è€ƒè™‘**æ•°æ®é‡ä¸Šä¼ **æŠ€æœ¯ [[110](ch030.xhtml#Xperez2020data)]ã€‚åœ¨ä¼ ç»Ÿçš„QNNä¸­ï¼Œä½ æœ‰ä¸€ä¸ªä¾èµ–äºæŸäº›è¾“å…¥æ•°æ®![\overset{\rightarrow}{x}](img/file1206.png
    "\overset{\rightarrow}{x}")çš„ç‰¹å¾å›¾![F](img/file1320.png "F")ï¼Œç„¶åæ˜¯ä¾èµ–äºæŸäº›å¯ä¼˜åŒ–å‚æ•°![\overset{\rightarrow}{\theta_{0}}](img/file1383.png
    "\overset{\rightarrow}{\theta_{0}}")çš„å˜åˆ†å½¢å¼![V](img/file379.png "V")ã€‚æ•°æ®é‡ä¸Šä¼ ç®€å•åœ°è¯´å°±æ˜¯é‡å¤è¿™ä¸ªæ–¹æ¡ˆâ€”â€”ä»»ä½•ä½ æƒ³è¦çš„æ¬¡æ•°â€”â€”åœ¨æ‰§è¡ŒQNNçš„æµ‹é‡æ“ä½œä¹‹å‰ã€‚ç‰¹å¾å›¾åœ¨æ¯ä¸ªé‡å¤ä¸­ä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ•°æ®ï¼Œä½†æ¯ä¸ªå˜åˆ†å½¢å¼çš„å®ä¾‹éƒ½é‡‡ç”¨å®ƒè‡ªå·±çš„ã€ç‹¬ç«‹çš„ã€å¯ä¼˜åŒ–çš„å‚æ•°ã€‚
- en: 'This is represented in the following diagram, which shows data reuploading
    with ![k](img/file317.png "k") repetitions:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åœ¨ä»¥ä¸‹å›¾è¡¨ä¸­è¡¨ç¤ºï¼Œå®ƒæ˜¾ç¤ºäº†å¸¦æœ‰![k](img/file317.png "k")æ¬¡é‡å¤çš„æ•°æ®é‡ä¸Šä¼ ï¼š
- en: '![|FVFV0âŸ©((((nâƒ—xâƒ—ğœƒâƒ—xâƒ—ğœƒ)1)k)) ... ](img/file1384.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![|FVFV0âŸ©((((nâƒ—xâƒ—ğœƒâƒ—xâƒ—ğœƒ)1)k)) ... ](img/file1384.jpg)'
- en: This has been shown, both in practice and in theory [[113](ch030.xhtml#Xdatare-schuld)],
    to offer some advantages over the simpler, standard approach at the cost of increasing
    the depth of the circuits that are used. In any case, it is good to have it in
    mind when implementing your own QNNs.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å·²ç»åœ¨å®è·µå’Œç†è®º [[113](ch030.xhtml#Xdatare-schuld)] ä¸­è¢«è¯æ˜ï¼Œä¸æ›´ç®€å•ã€æ ‡å‡†çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒæä¾›äº†ä¸€äº›ä¼˜åŠ¿ï¼Œä½†ä»£ä»·æ˜¯å¢åŠ äº†æ‰€ä½¿ç”¨ç”µè·¯çš„æ·±åº¦ã€‚æ— è®ºå¦‚ä½•ï¼Œåœ¨å®ç°ä½ è‡ªå·±çš„QNNæ—¶ï¼Œå°†å…¶ç‰¢è®°åœ¨å¿ƒæ˜¯å¥½çš„ã€‚
- en: This concludes our theoretical discussion of quantum neural networks. Now itâ€™s
    time for us to get our hands dirty with the actual implementation of all the fancy
    artifacts and techniques that we have discussed. In this regard, we will focus
    mostly on PennyLane. Letâ€™s begin!
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»“æŸäº†æˆ‘ä»¬å¯¹é‡å­ç¥ç»ç½‘ç»œçš„çº¯ç†è®ºè®¨è®ºã€‚ç°åœ¨æ˜¯æˆ‘ä»¬åŠ¨æ‰‹å®ç°æ‰€æœ‰æˆ‘ä»¬è®¨è®ºè¿‡çš„èŠ±å“¨çš„å…ƒç´ å’ŒæŠ€æœ¯çš„æ—¶å€™äº†ã€‚åœ¨è¿™æ–¹é¢ï¼Œæˆ‘ä»¬å°†ä¸»è¦å…³æ³¨PennyLaneã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼
- en: 10.2 Quantum neural networks in PennyLane
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.2 PennyLaneä¸­çš„é‡å­ç¥ç»ç½‘ç»œ
- en: We are now ready to implement and train our first quantum neural network with
    PennyLane. The PennyLane framework is great for many applications, but it shines
    the most when it comes to the implementation of quantum neural network models.
    This is all due to its flexibility and good integration with classical machine
    learning frameworks. We, in particular, are going to be using PennyLane in conjunction
    with TensorFlow to train a QNN-based binary classifier. All that effort that we
    invested in *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine
    Learning?*, is finally going to pay off!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œæˆ‘ä»¬å·²å‡†å¤‡å¥½ä½¿ç”¨PennyLaneå®ç°å’Œè®­ç»ƒæˆ‘ä»¬çš„ç¬¬ä¸€ä¸ªé‡å­ç¥ç»ç½‘ç»œã€‚PennyLaneæ¡†æ¶éå¸¸é€‚åˆè®¸å¤šåº”ç”¨ï¼Œä½†åœ¨å®ç°é‡å­ç¥ç»ç½‘ç»œæ¨¡å‹æ–¹é¢æœ€ä¸ºå‡ºè‰²ã€‚è¿™éƒ½å½’åŠŸäºå…¶çµæ´»æ€§å’Œä¸ç»å…¸æœºå™¨å­¦ä¹ æ¡†æ¶çš„è‰¯å¥½é›†æˆã€‚æˆ‘ä»¬å°†ç‰¹åˆ«ä½¿ç”¨PennyLaneä¸TensorFlowç»“åˆæ¥è®­ç»ƒä¸€ä¸ªåŸºäºQNNçš„äºŒåˆ†ç±»å™¨ã€‚æˆ‘ä»¬åœ¨*ç¬¬8ç« *[*8*](ch017.xhtml#x1-1390008)*ï¼Œä»€ä¹ˆæ˜¯é‡å­æœºå™¨å­¦ä¹ ï¼Ÿ*ä¸­æŠ•å…¥çš„æ‰€æœ‰åŠªåŠ›ï¼Œæœ€ç»ˆå°†å¾—åˆ°å›æŠ¥ï¼
- en: Important note
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: é‡è¦æç¤º
- en: Remember that we are using **version 2.9.1** of the TensorFlow package and **version
    0.26** of PennyLane.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è®°ä½ï¼Œæˆ‘ä»¬æ­£åœ¨ä½¿ç”¨TensorFlowåŒ…çš„**ç‰ˆæœ¬2.9.1**å’ŒPennyLaneçš„**ç‰ˆæœ¬0.26**ã€‚
- en: 'Letâ€™s begin by importing PennyLane, NumPy, and TensorFlow and setting some
    seeds for these packages, just to make sure that our results are reproducible.
    We can achieve this with the following piece of code:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»å¯¼å…¥PennyLaneã€NumPyå’ŒTensorFlowä»¥åŠä¸ºè¿™äº›åŒ…è®¾ç½®ä¸€äº›ç§å­å¼€å§‹ï¼Œä»¥ç¡®ä¿æˆ‘ä»¬çš„ç»“æœæ˜¯å¯é‡å¤çš„ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ç‰‡æ®µå®ç°è¿™ä¸€ç‚¹ï¼š
- en: '[PRE0]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Keep in mind that you may still get slightly different results from ours if
    you are using different package versions. However, the results you obtain will
    be fully reproducible in your own machine.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œå¦‚æœä½ ä½¿ç”¨ä¸åŒçš„åŒ…ç‰ˆæœ¬ï¼Œä½ å¯èƒ½ä¼šå¾—åˆ°ä¸æˆ‘ä»¬ç•¥å¾®ä¸åŒçš„ç»“æœã€‚ç„¶è€Œï¼Œä½ è·å¾—çš„ç»“æœå°†åœ¨ä½ è‡ªå·±çš„æœºå™¨ä¸Šå®Œå…¨å¯é‡å¤ã€‚
- en: 'Before we get to our problem, thereâ€™s one last detail that we need to sort
    out. PennyLane works with doubles while TensorFlow uses ordinary floats. This
    isnâ€™t always an issue, but itâ€™s a good idea to ask TensorFlow to work with doubles
    just as PennyLane does. We can accomplish this as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬é¢å¯¹æˆ‘ä»¬çš„é—®é¢˜ä¹‹å‰ï¼Œè¿˜æœ‰ä¸€ä¸ªæœ€åçš„ç»†èŠ‚éœ€è¦æˆ‘ä»¬è§£å†³ã€‚PennyLaneä½¿ç”¨åŒç²¾åº¦æµ®ç‚¹æ•°ï¼Œè€ŒTensorFlowä½¿ç”¨æ™®é€šçš„æµ®ç‚¹æ•°ã€‚è¿™å¹¶ä¸æ€»æ˜¯é—®é¢˜ï¼Œä½†æœ€å¥½è®©TensorFlowåƒPennyLaneä¸€æ ·ä½¿ç”¨åŒç²¾åº¦æµ®ç‚¹æ•°ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å®ç°è¿™ä¸€ç‚¹ï¼š
- en: '[PRE1]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With this out of the way, letâ€™s meet our problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªé—®é¢˜è§£å†³å®Œæ¯•ä¹‹åï¼Œè®©æˆ‘ä»¬æ¥é¢å¯¹æˆ‘ä»¬çš„é—®é¢˜ã€‚
- en: 10.2.1 Preparing data for a QNN
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.1 ä¸ºQNNå‡†å¤‡æ•°æ®
- en: As we have already mentioned, we are going to train a QNN model to implement
    a binary classifier. Our recurrent use of binary classifiers is no coincidence,
    for binary classifiers are perhaps the simplest machine learning models to train.
    Later in the book, however, we will explore more exciting use cases and architectures.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬å·²ç»æåˆ°çš„ï¼Œæˆ‘ä»¬å°†è®­ç»ƒä¸€ä¸ªQNNæ¨¡å‹æ¥å®ç°äºŒå…ƒåˆ†ç±»å™¨ã€‚æˆ‘ä»¬åå¤ä½¿ç”¨äºŒå…ƒåˆ†ç±»å™¨å¹¶ä¸æ˜¯å·§åˆï¼Œå› ä¸ºäºŒå…ƒåˆ†ç±»å™¨å¯èƒ½æ˜¯è®­ç»ƒèµ·æ¥æœ€ç®€å•çš„æœºå™¨å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œï¼Œåœ¨æœ¬ä¹¦çš„åé¢ï¼Œæˆ‘ä»¬å°†æ¢ç´¢æ›´å¤šä»¤äººå…´å¥‹çš„ä½¿ç”¨æ¡ˆä¾‹å’Œæ¶æ„ã€‚
- en: 'For our example problem, we are going to use one of the toy datasets provided
    by the scikit-learn package: the â€Breast cancer Wisconsin datasetâ€ [[32](ch030.xhtml#XDua:2019)].
    This dataset has a total of ![569](img/file1385.png "569") samples with ![30](img/file620.png
    "30") numerical variables each. These variables describe features that can be
    used to characterize whether a breast mass is benign or malignant. The label of
    each sample can be either ![0](img/file12.png "0") or ![1](img/file13.png "1"),
    corresponding to malignant or benign, respectively. You may find the documentation
    of this dataset online at [https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)
    (the original documentation of the dataset can also be found at [https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))).'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„ç¤ºä¾‹é—®é¢˜ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨scikit-learnåŒ…æä¾›çš„ç©å…·æ•°æ®é›†ä¹‹ä¸€ï¼šâ€œå¨æ–¯åº·æ˜Ÿä¹³è…ºç™Œæ•°æ®é›†â€ [[32](ch030.xhtml#XDua:2019)]ã€‚è¿™ä¸ªæ•°æ®é›†æ€»å…±æœ‰![569](img/file1385.png
    "569")ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ªæ ·æœ¬æœ‰![30](img/file620.png "30")ä¸ªæ•°å€¼å˜é‡ã€‚è¿™äº›å˜é‡æè¿°äº†å¯ä»¥ç”¨æ¥è¡¨å¾ä¹³è…ºè‚¿å—æ˜¯è‰¯æ€§è¿˜æ˜¯æ¶æ€§çš„ç‰¹å¾ã€‚æ¯ä¸ªæ ·æœ¬çš„æ ‡ç­¾å¯ä»¥æ˜¯![0](img/file12.png
    "0")æˆ–![1](img/file13.png "1")ï¼Œåˆ†åˆ«å¯¹åº”æ¶æ€§å’Œè‰¯æ€§ã€‚ä½ å¯ä»¥åœ¨ç½‘ä¸Šæ‰¾åˆ°è¿™ä¸ªæ•°æ®é›†çš„æ–‡æ¡£[https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset](https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset)ï¼ˆæ•°æ®é›†çš„åŸå§‹æ–‡æ¡£ä¹Ÿå¯ä»¥åœ¨[https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic)](https://archive.ics.uci.edu/ml/datasets/breast+cancer+wisconsin+(diagnostic))æ‰¾åˆ°ï¼‰ã€‚
- en: 'We can get this dataset by calling the `load_breast_cancer` function from `sklearn``.``datasets`,
    setting the optional argument `return_X_y` to true in order to retrieve the labels
    in addition to the samples. For that, we can use the following instructions:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡ä»`sklearn.datasets`è°ƒç”¨`load_breast_cancer`å‡½æ•°æ¥è·å–è¿™ä¸ªæ•°æ®é›†ï¼Œå°†å¯é€‰å‚æ•°`return_X_y`è®¾ç½®ä¸ºtrueï¼Œä»¥ä¾¿é™¤äº†æ ·æœ¬å¤–è¿˜å¯ä»¥æ£€ç´¢æ ‡ç­¾ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹è¯´æ˜ï¼š
- en: '[PRE2]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When we trained QSVMs, since we were not going to make any comparisons between
    models, a training and test dataset sufficed. In our case, however, we are going
    to train our models with early stopping on the validation loss. This means â€” in
    case you donâ€™t remember â€” that we will be keeping track of the validation loss
    and we will halt the training as soon as it doesnâ€™t improve â€” according to some
    criteria that we will define. What is more, we will keep the model configuration
    that best minimized the validation loss. Using the test dataset for this purpose
    wouldnâ€™t be good practice, for then the test dataset would have played a role
    in the training and it would not give a good estimate of the true error; thatâ€™s
    why we will need a separate validation dataset.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è®­ç»ƒQSVMæ—¶ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æ‰“ç®—åœ¨æ¨¡å‹ä¹‹é—´è¿›è¡Œæ¯”è¾ƒï¼Œæ‰€ä»¥ä¸€ä¸ªè®­ç»ƒé›†å’Œæµ‹è¯•é›†å°±è¶³å¤Ÿäº†ã€‚ç„¶è€Œï¼Œåœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨éªŒè¯æŸå¤±æå‰åœæ­¢æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚è¿™æ„å‘³ç€â€”â€”å¦‚æœä½ ä¸è®°å¾—çš„è¯â€”â€”æˆ‘ä»¬å°†è·Ÿè¸ªéªŒè¯æŸå¤±ï¼Œå¹¶ä¸”ä¸€æ—¦å®ƒä¸å†æ”¹å–„â€”â€”æ ¹æ®æˆ‘ä»¬å°†å®šä¹‰çš„ä¸€äº›æ ‡å‡†ï¼Œæˆ‘ä»¬å°†åœæ­¢è®­ç»ƒã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†ä¿ç•™æœ€ä½³æ¨¡å‹é…ç½®ï¼Œä»¥æœ€å°åŒ–éªŒè¯æŸå¤±ã€‚ä½¿ç”¨æµ‹è¯•é›†æ¥è¾¾åˆ°è¿™ä¸ªç›®çš„å¹¶ä¸æ˜¯ä¸€ä¸ªå¥½çš„å®è·µï¼Œå› ä¸ºè¿™æ ·æµ‹è¯•é›†å°±ä¼šåœ¨è®­ç»ƒä¸­å‘æŒ¥ä½œç”¨ï¼Œå®ƒä¸ä¼šç»™å‡ºçœŸå®é”™è¯¯çš„è‰¯å¥½ä¼°è®¡ï¼›è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ä¸€ä¸ªå•ç‹¬çš„éªŒè¯æ•°æ®é›†ã€‚
- en: 'We can split our dataset into a training, validation, and test dataset as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥å°†æˆ‘ä»¬çš„æ•°æ®é›†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'All the variables in the dataset are non-zero, but they are not normalized.
    In order to use them with any of our feature maps, we shall normalize the training
    data between ![0](img/file12.png "0") and ![1](img/file13.png "1") using `MaxAbsScaler`
    as follows:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: æ•°æ®é›†ä¸­çš„æ‰€æœ‰å˜é‡éƒ½ä¸æ˜¯é›¶ï¼Œä½†å®ƒä»¬æ²¡æœ‰å½’ä¸€åŒ–ã€‚ä¸ºäº†ä½¿ç”¨å®ƒä»¬ä¸æˆ‘ä»¬çš„ä»»ä½•ç‰¹å¾æ˜ å°„ï¼Œæˆ‘ä»¬åº”è¯¥ä½¿ç”¨`MaxAbsScaler`å°†è®­ç»ƒæ•°æ®å½’ä¸€åŒ–åˆ°![0](img/file12.png
    "0")å’Œ![1](img/file13.png "1")ä¹‹é—´ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'And we then normalize the test and validation datasets in the same proportions
    as the training dataset:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œæˆ‘ä»¬å°†æµ‹è¯•å’ŒéªŒè¯æ•°æ®é›†æŒ‰ç…§è®­ç»ƒæ•°æ®é›†çš„æ¯”ä¾‹è¿›è¡Œå½’ä¸€åŒ–ï¼š
- en: '[PRE5]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Just as we did when we trained a QSVM in the previous chapter!
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬åœ¨ä¸Šä¸€ç« è®­ç»ƒQSVMæ—¶æ‰€åšçš„é‚£æ ·ï¼
- en: 'So far, we have simply done some fairly standard data preprocessing, without
    having to think too much about the actual architecture of our future quantum neural
    network. But that changes now. We have a problem to address: our dataset has ![30](img/file620.png
    "30") variables, and that can be a pretty large number for current quantum hardware.
    Since we donâ€™t have access to quantum computers with ![30](img/file620.png "30")
    qubits, we may consider the following choices:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬åªæ˜¯è¿›è¡Œäº†ä¸€äº›ç›¸å½“æ ‡å‡†çš„é¢„å¤„ç†ï¼Œè€Œä¸å¿…è¿‡å¤šè€ƒè™‘æˆ‘ä»¬æœªæ¥é‡å­ç¥ç»ç½‘ç»œçš„å®é™…æ¶æ„ã€‚ä½†ç°åœ¨æƒ…å†µæ”¹å˜äº†ã€‚æˆ‘ä»¬æœ‰ä¸€ä¸ªé—®é¢˜è¦è§£å†³ï¼šæˆ‘ä»¬çš„æ•°æ®é›†æœ‰![30](img/file620.png
    "30")ä¸ªå˜é‡ï¼Œè¿™å¯¹äºå½“å‰çš„é‡å­ç¡¬ä»¶æ¥è¯´å¯èƒ½æ˜¯ä¸€ä¸ªç›¸å½“å¤§çš„æ•°å­—ã€‚ç”±äºæˆ‘ä»¬æ²¡æœ‰è®¿é—®åˆ°![30](img/file620.png "30")é‡å­æ¯”ç‰¹çš„é‡å­è®¡ç®—æœºï¼Œæˆ‘ä»¬å¯èƒ½è€ƒè™‘ä»¥ä¸‹é€‰æ‹©ï¼š
- en: Use the amplitude encoding feature map on ![5](img/file296.png "5") qubits,
    which can accommodate up to ![2^{5} = 32](img/file1386.png "2^{5} = 32") variables
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: åœ¨![5](img/file296.png "5")ä¸ªé‡å­æ¯”ç‰¹ä¸Šä½¿ç”¨å¹…åº¦ç¼–ç ç‰¹å¾æ˜ å°„ï¼Œå®ƒå¯ä»¥å®¹çº³å¤šè¾¾![2^{5} = 32](img/file1386.png
    "2^{5} = 32")ä¸ªå˜é‡
- en: Use any of the other feature maps that we have used, but in conjunction with
    a dimensionality reduction technique
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æˆ‘ä»¬ä¹‹å‰ä½¿ç”¨è¿‡çš„ä»»ä½•å…¶ä»–ç‰¹å¾æ˜ å°„ï¼Œä½†ä¸é™ç»´æŠ€æœ¯ç»“åˆä½¿ç”¨
- en: 'We will go for the latter choice. You can try the other possibility on your
    own: itâ€™s fairly straightforward if you use the `qml``.``AmplitudeEmbedding` template
    that we studied back in *Chapter* [*9*](ch018.xhtml#x1-1600009)*, Quantum Support
    Vector* *Machines*.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†é€‰æ‹©åè€…ã€‚ä½ å¯ä»¥è‡ªå·±å°è¯•å…¶ä»–å¯èƒ½æ€§ï¼šå¦‚æœä½ ä½¿ç”¨æˆ‘ä»¬åœ¨*ç¬¬* [*9*](ch018.xhtml#x1-1600009)*ç« ä¸­å­¦ä¹ çš„`qml`çš„`AmplitudeEmbedding`æ¨¡æ¿ï¼Œå®ƒç›¸å½“ç›´æ¥ã€‚
- en: Exercise 10.2
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 10.2
- en: As you follow along this section, try to implement a QNN using all the original
    variables with amplitude encoding on five qubits.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä½ è·Ÿéšæœ¬èŠ‚å†…å®¹çš„åŒæ—¶ï¼Œå°è¯•ä½¿ç”¨äº”ä¸ªé‡å­æ¯”ç‰¹çš„å¹…åº¦ç¼–ç å®ç°ä¸€ä¸ªQNNã€‚
- en: Keep in mind that, when feeding the data to the `qml``.` `AmplitudeEmbedding`
    object through the features argument, instead of using the `inputs` variable,
    you should use `[``a` `for` `a` `in` `inputs``]`. This is needed because of some
    internal type conversions that PennyLane needs to perform.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œå½“é€šè¿‡ç‰¹å¾å‚æ•°å°†æ•°æ®å–‚ç»™`qml`çš„`AmplitudeEmbedding`å¯¹è±¡æ—¶ï¼Œè€Œä¸æ˜¯ä½¿ç”¨`inputs`å˜é‡ï¼Œä½ åº”è¯¥ä½¿ç”¨`[``a` `for`
    `a` `in` `inputs``]`ã€‚è¿™æ˜¯å› ä¸ºPennyLaneéœ€è¦æ‰§è¡Œä¸€äº›å†…éƒ¨ç±»å‹è½¬æ¢ã€‚
- en: Training a quantum neural network on a simulator is a very computationally-intensive
    task. We donâ€™t want anyoneâ€™s computer to crash, so, just to make sure everyone
    can run this example smoothly, we will restrict ourselves to using ![4](img/file143.png
    "4")-qubit circuits. Thus, we will use a dimensionality reduction technique to
    shrink the number of variables to ![4](img/file143.png "4"), and then set up a
    QNN with a feature map that will take the resulting ![4](img/file143.png "4")
    input variables.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œæ˜¯ä¸€ä¸ªè®¡ç®—å¯†é›†å‹ä»»åŠ¡ã€‚æˆ‘ä»¬ä¸å¸Œæœ›ä»»ä½•äººçš„ç”µè„‘å´©æºƒï¼Œæ‰€ä»¥ï¼Œä¸ºäº†ç¡®ä¿æ¯ä¸ªäººéƒ½èƒ½é¡ºåˆ©è¿è¡Œè¿™ä¸ªç¤ºä¾‹ï¼Œæˆ‘ä»¬å°†é™åˆ¶è‡ªå·±ä½¿ç”¨![4](img/file143.png
    "4")-é‡å­æ¯”ç‰¹ç”µè·¯ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨é™ç»´æŠ€æœ¯å°†å˜é‡çš„æ•°é‡å‡å°‘åˆ°![4](img/file143.png "4")ï¼Œç„¶åè®¾ç½®ä¸€ä¸ªå…·æœ‰ç‰¹å¾æ˜ å°„çš„QNNï¼Œè¯¥æ˜ å°„å°†æ¥å—![4](img/file143.png
    "4")ä¸ªè¾“å…¥å˜é‡ã€‚
- en: 'As we did in the previous chapter, we will use principal component analysis
    in order to reduce the number of variables in our dataset to ![4](img/file143.png
    "4"):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£å¦‚æˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­æ‰€åšçš„é‚£æ ·ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä¸»æˆåˆ†åˆ†ææ¥å‡å°‘æ•°æ®é›†ä¸­å˜é‡çš„æ•°é‡åˆ°![4](img/file143.png "4")ï¼š
- en: '[PRE6]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now that we have our data fully ready, we need to choose how our quantum neural
    network is going to work. This is exactly the focus of the next subsection.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å®Œå…¨å‡†å¤‡å¥½äº†æ•°æ®ï¼Œæˆ‘ä»¬éœ€è¦é€‰æ‹©æˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œå°†å¦‚ä½•å·¥ä½œã€‚è¿™æ­£æ˜¯ä¸‹ä¸€å°èŠ‚çš„é‡ç‚¹ã€‚
- en: 10.2.2 Building the network
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.2 æ„å»ºç½‘ç»œ
- en: For our case, we will choose the ZZ feature map and the two-local variational
    form. Neither is built into PennyLane, so we have to provide our own implementation
    of these variational circuits. PennyLane includes, however, a version of the two-local
    form with circular entanglement (`qml``.``BasicEntanglerLayers`), in case you
    want to use it in your QNNs. To implement the circuits that we need, we can just
    use the pseudocode that we provided in *Section* *[*10.1.2*](#x1-18400010.1.2)
    and do something like the following:*
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæˆ‘ä»¬çš„æƒ…å†µï¼Œæˆ‘ä»¬å°†é€‰æ‹© ZZ ç‰¹å¾å›¾å’ŒåŒå±€éƒ¨å˜åˆ†å½¢å¼ã€‚è¿™ä¸¤ä¸ªéƒ½ä¸æ˜¯å†…ç½®åœ¨ PennyLane ä¸­çš„ï¼Œå› æ­¤æˆ‘ä»¬å¿…é¡»æä¾›æˆ‘ä»¬è‡ªå·±çš„å˜åˆ†ç”µè·¯å®ç°ã€‚ç„¶è€Œï¼ŒPennyLane
    åŒ…å«äº†ä¸€ä¸ªå…·æœ‰ç¯å½¢çº ç¼ çš„åŒå±€éƒ¨å½¢å¼ç‰ˆæœ¬ (`qml``.``BasicEntanglerLayers`)ï¼Œä»¥é˜²ä½ æƒ³è¦åœ¨ QNNs ä¸­ä½¿ç”¨å®ƒã€‚ä¸ºäº†å®ç°æˆ‘ä»¬éœ€è¦çš„ç”µè·¯ï¼Œæˆ‘ä»¬åªéœ€ä½¿ç”¨æˆ‘ä»¬åœ¨
    *ç¬¬ * *10.1.2* *èŠ‚ä¸­æä¾›çš„ä¼ªä»£ç ï¼Œå¹¶æ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š*
- en: '*[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '*[PRE7]'
- en: Remember that we already implemented the ZZ feature map in PennyLane in the
    previous chapter.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—æˆ‘ä»¬åœ¨ä¸Šä¸€ç« ä¸­å·²ç»åœ¨ PennyLane ä¸­å®ç°äº† ZZ ç‰¹å¾å›¾ã€‚
- en: In this chapter, we have talked about observables, and how these are represented
    by Hermitian operators in quantum mechanics. PennyLane allows us to work directly
    with these Hermitian representations.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬è®¨è®ºäº†å¯è§‚æµ‹é‡ï¼Œä»¥åŠè¿™äº›åœ¨é‡å­åŠ›å­¦ä¸­å¦‚ä½•ç”±å„ç±³ç®—å­è¡¨ç¤ºã€‚PennyLane å…è®¸æˆ‘ä»¬ç›´æ¥ä½¿ç”¨è¿™äº›å„ç±³è¡¨ç¤ºã€‚
- en: Remember how every circuit in PennyLane returns the result of some measurement
    operation? For instance, you may use `return` `qml``.``probs``(``wires` `=` `[0])`
    at the end of the definition of a circuit in order to get the probabilities of
    every possible measurement outcome on the computational basis. Well, it turns
    out that PennyLane offers a few more possibilities. For instance, given any Hermitian
    matrix ![A](img/file183.png "A") (encoded as a numpy array `A`), we may retrieve
    the expectation value of ![A](img/file183.png "A") on an array of wires `w` at
    the end of a circuit simply by calling `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)`. Of course, the dimensions of ![A](img/file183.png "A") must be compatible
    with the length of `w`. This is useful in our case, for in order to get the expectation
    value on the first qubit, we will just have to compute the expectation value of
    the Hermitian
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—åœ¨ PennyLane ä¸­æ¯ä¸ªç”µè·¯éƒ½è¿”å›æŸäº›æµ‹é‡æ“ä½œçš„ç»“æœå—ï¼Ÿä¾‹å¦‚ï¼Œä½ å¯ä»¥åœ¨ç”µè·¯å®šä¹‰çš„æœ«å°¾ä½¿ç”¨ `return` `qml``.``probs``(``wires`
    `=` `[0])` æ¥è·å–è®¡ç®—åŸºä¸Šæ¯ä¸ªå¯èƒ½æµ‹é‡ç»“æœçš„æ¦‚ç‡ã€‚å—¯ï¼Œç»“æœæ˜¯ PennyLane è¿˜æä¾›äº†ä¸€äº›å…¶ä»–å¯èƒ½æ€§ã€‚ä¾‹å¦‚ï¼Œç»™å®šä»»ä½•å„ç±³çŸ©é˜µ ![A](img/file183.png
    "A")ï¼ˆç¼–ç ä¸º numpy æ•°ç»„ `A`ï¼‰ï¼Œæˆ‘ä»¬åªéœ€åœ¨ç”µè·¯æœ«å°¾è°ƒç”¨ `return` `qml``.``expval``(``A``,` `wires`
    `=` `w``)` å°±å¯ä»¥æ£€ç´¢ `w` ä¸Š `A` çš„æœŸæœ›å€¼ã€‚å½“ç„¶ï¼Œ![A](img/file183.png "A") çš„ç»´åº¦å¿…é¡»ä¸ `w` çš„é•¿åº¦å…¼å®¹ã€‚è¿™åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹å¾ˆæœ‰ç”¨ï¼Œå› ä¸ºä¸ºäº†è·å–ç¬¬ä¸€ä¸ªé‡å­æ¯”ç‰¹çš„æœŸæœ›å€¼ï¼Œæˆ‘ä»¬åªéœ€è®¡ç®—å„ç±³çŸ©é˜µçš„æœŸæœ›å€¼
- en: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| ![M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.](img/file1387.png
    "M = \begin{pmatrix} 1 & 0 \\ 0 & 0 \\ \end{pmatrix}.") |'
- en: 'The matrix ![M](img/file704.png "M") can be constructed as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µ ![M](img/file704.png "M") å¯ä»¥æŒ‰ç…§ä»¥ä¸‹æ–¹å¼æ„å»ºï¼š
- en: '[PRE8]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In this construction, we have used the fact that ![M = \left| 0 \right\rangle\left\langle
    0 \right|](img/file1388.png "M = \left| 0 \right\rangle\left\langle 0 \right|"),
    as we discussed in an exercise earlier in this chapter. This will give us, as
    output, a value between ![0](img/file12.png "0") and ![1](img/file13.png "1"),
    which is perfect to construct a classifier: as usual, we will assign class ![1](img/file13.png
    "1") to every data instance with a value of ![0.5](img/file1166.png "0.5") or
    higher, and class ![0](img/file12.png "0") to all the rest.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªæ„å»ºä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¹‹å‰åœ¨æœ¬ç« ä¸­è®¨è®ºçš„äº‹å®ï¼Œå³ ![M = \left| 0 \right\rangle\left\langle 0 \right|](img/file1388.png
    "M = \left| 0 \right\rangle\left\langle 0 \right|")ï¼Œè¿™å°†ç»™æˆ‘ä»¬ä¸€ä¸ªä»‹äº ![0](img/file12.png
    "0") å’Œ ![1](img/file13.png "1") ä¹‹é—´çš„è¾“å‡ºå€¼ï¼Œè¿™å¯¹äºæ„å»ºåˆ†ç±»å™¨éå¸¸å®Œç¾ï¼šåƒå¾€å¸¸ä¸€æ ·ï¼Œæˆ‘ä»¬å°†å€¼å¤§äºç­‰äº ![0.5](img/file1166.png
    "0.5") çš„æ¯ä¸ªæ•°æ®å®ä¾‹åˆ†é…ä¸ºç±»åˆ« ![1](img/file13.png "1")ï¼Œè€Œå°†æ‰€æœ‰å…¶ä»–å€¼åˆ†é…ä¸ºç±»åˆ« ![0](img/file12.png
    "0")ã€‚
- en: 'Now we have all the pieces gathered in order to implement our quantum neural
    network. We are going to construct it as a quantum node with two arguments: `inputs`
    and `theta`. The first argument is mandatory: in order for PennyLane to be able
    to train a quantum neural network with TensorFlow, its first argument must accept
    an array with all the inputs to the network, and the name of this argument must
    be `inputs`. After this argument, we may add as many as we want. These can correspond
    to any parameters of the circuit, and, of course, they need to include the optimizable
    parameters in the variational form.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»æ”¶é›†äº†æ‰€æœ‰å¿…è¦çš„ç»„ä»¶æ¥å®ç°æˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å°†å°†å…¶æ„å»ºä¸ºä¸€ä¸ªå…·æœ‰ä¸¤ä¸ªå‚æ•°çš„é‡å­èŠ‚ç‚¹ï¼š`inputs` å’Œ `theta`ã€‚ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯å¼ºåˆ¶æ€§çš„ï¼šä¸ºäº†è®©
    PennyLane èƒ½å¤Ÿä½¿ç”¨ TensorFlow è®­ç»ƒé‡å­ç¥ç»ç½‘ç»œï¼Œå®ƒçš„ç¬¬ä¸€ä¸ªå‚æ•°å¿…é¡»æ¥å—ä¸€ä¸ªåŒ…å«ç½‘ç»œæ‰€æœ‰è¾“å…¥çš„æ•°ç»„ï¼Œå¹¶ä¸”è¿™ä¸ªå‚æ•°çš„åç§°å¿…é¡»æ˜¯ `inputs`ã€‚åœ¨è¿™ä¸ªå‚æ•°ä¹‹åï¼Œæˆ‘ä»¬å¯ä»¥æ·»åŠ å°½å¯èƒ½å¤šçš„å‚æ•°ã€‚è¿™äº›å¯ä»¥å¯¹åº”äºç”µè·¯çš„ä»»ä½•å‚æ•°ï¼Œå½“ç„¶ï¼Œå®ƒä»¬éœ€è¦åŒ…æ‹¬å˜åˆ†å½¢å¼ä¸­çš„å¯ä¼˜åŒ–å‚æ•°ã€‚
- en: 'Thus, we may implement our quantum neural network as follows:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥è¿™æ ·å®ç°æˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œï¼š
- en: '[PRE9]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: To keep things simple, we have chosen to use just one repetition of the variational
    form. If your dataset is more complex, you may want to increase this number in
    order to have more trainable parameters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¿æŒç®€å•ï¼Œæˆ‘ä»¬é€‰æ‹©åªä½¿ç”¨ä¸€æ¬¡å˜åˆ†å½¢å¼çš„é‡å ã€‚å¦‚æœä½ çš„æ•°æ®é›†æ›´å¤æ‚ï¼Œä½ å¯èƒ½éœ€è¦å¢åŠ è¿™ä¸ªæ•°å­—ï¼Œä»¥ä¾¿æœ‰æ›´å¤šçš„å¯è®­ç»ƒå‚æ•°ã€‚
- en: Notice, by the way, how we have added the argument `interface` `=` `"``tf``"`
    to the quantum node initializer. This is so that the quantum node will work with
    tensors (TensorFlowâ€™s data object) in lieu of with arrays, just to allow PennyLane
    to communicate smoothly with TensorFlow. Had we used the `@qml``.``qnode` decorator,
    we wouldâ€™ve had to include the argument in its call.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€ä¸‹ï¼Œæ³¨æ„æˆ‘ä»¬æ˜¯å¦‚ä½•åœ¨é‡å­èŠ‚ç‚¹åˆå§‹åŒ–å™¨ä¸­æ·»åŠ äº† `interface` `=` `"``tf``"` å‚æ•°çš„ã€‚è¿™æ ·åšæ˜¯ä¸ºäº†è®©é‡å­èŠ‚ç‚¹èƒ½å¤Ÿä½¿ç”¨å¼ é‡ï¼ˆTensorFlow
    çš„æ•°æ®å¯¹è±¡ï¼‰è€Œä¸æ˜¯æ•°ç»„æ¥å·¥ä½œï¼Œä»¥ä¾¿ PennyLane èƒ½å¤Ÿä¸ TensorFlow å¹³æ»‘é€šä¿¡ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨äº† `@qml``.``qnode` è£…é¥°å™¨ï¼Œæˆ‘ä»¬å°±éœ€è¦åœ¨å®ƒçš„è°ƒç”¨ä¸­åŒ…å«è¿™ä¸ªå‚æ•°ã€‚
- en: This defines the quantum node that implements our quantum neural network. Now
    we need to figure out a way to train it, and, for that purpose, we will rely on
    TensorFlow. Weâ€™ll do exactly that in the next subsection.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å®šä¹‰äº†å®ç°æˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œçš„é‡å­èŠ‚ç‚¹ã€‚ç°åœ¨æˆ‘ä»¬éœ€è¦æ‰¾å‡ºä¸€ç§æ–¹æ³•æ¥è®­ç»ƒå®ƒï¼Œä¸ºæ­¤æˆ‘ä»¬å°†ä¾èµ– TensorFlowã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€å°èŠ‚ä¸­è¿™æ ·åšã€‚
- en: 10.2.3 Using TensorFlow with PennyLane
  id: totrans-204
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.3 ä½¿ç”¨ TensorFlow ä¸ PennyLane
- en: In *Chapter* [*8*](ch017.xhtml#x1-1390008)*, What is Quantum Machine Learning?*,
    we already learned how TensorFlow can be used to train a classical neural network.
    Well, thanks to PennyLaneâ€™s great interoperability, we will now be able to train
    our quantum neural network with TensorFlow almost as if it were a classical one.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ *ç¬¬* [*8*](ch017.xhtml#x1-1390008)*ç« â€œä»€ä¹ˆæ˜¯é‡å­æœºå™¨å­¦ä¹ ï¼Ÿâ€ä¸­ï¼Œæˆ‘ä»¬å·²ç»å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨ TensorFlow è®­ç»ƒç»å…¸ç¥ç»ç½‘ç»œã€‚å¥½å§ï¼Œå¤šäºäº†
    PennyLane çš„å‡ºè‰²äº’æ“ä½œæ€§ï¼Œæˆ‘ä»¬ç°åœ¨å‡ ä¹å¯ä»¥åƒè®­ç»ƒç»å…¸ç¥ç»ç½‘ç»œä¸€æ ·è®­ç»ƒæˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œã€‚
- en: To learn moreâ€¦
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šâ€¦
- en: PennyLane can also be integrated with other classical machine learning frameworks
    such as PyTorch. In addition, it provides its own tools to train models based
    on the NumPy package, but these are more limited.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLane è¿˜å¯ä»¥ä¸å…¶ä»–ç»å…¸æœºå™¨å­¦ä¹ æ¡†æ¶é›†æˆï¼Œä¾‹å¦‚ PyTorchã€‚æ­¤å¤–ï¼Œå®ƒè¿˜æä¾›äº†åŸºäº NumPy åŒ…çš„æ¨¡å‹è®­ç»ƒå·¥å…·ï¼Œä½†è¿™äº›åŠŸèƒ½æ›´ä¸ºæœ‰é™ã€‚
- en: 'Remember how we could construct TensorFlow models using Keras layers and joining
    them in sequential models? Look at this:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: è®°å¾—æˆ‘ä»¬æ˜¯å¦‚ä½•ä½¿ç”¨ Keras å±‚æ„å»º TensorFlow æ¨¡å‹å¹¶å°†å®ƒä»¬ç»„åˆæˆé¡ºåºæ¨¡å‹å—ï¼Ÿçœ‹çœ‹è¿™ä¸ªä¾‹å­ï¼š
- en: '[PRE10]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: That is how you can create a Keras layer containing our quantum neural network
    â€” just as if it were any other layer in a classical model! In order to do this,
    weâ€™ve had to call `qml``.``qnn``.``KerasLayer`, and weâ€™ve had to pass a few things
    to it. First, of course, weâ€™ve sent the quantum node with the neural network.
    Then, a dictionary is indexed by the names of all the node arguments that take
    the optimizable parameters, and specifies, for each of these arguments, the number
    of parameters that they take. Since we only have one such argument, `theta`, and
    it should contain ![8](img/file506.png "8") optimizable parameters (that is, it
    will be an array of length ![8](img/file506.png "8")), we have sent in `{``"``theta``:`
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å¦‚ä½•åˆ›å»ºä¸€ä¸ªåŒ…å«æˆ‘ä»¬çš„é‡å­ç¥ç»ç½‘ç»œçš„ Keras å±‚â€”â€”å°±åƒå®ƒæ˜¯ä¸€ä¸ªç»å…¸æ¨¡å‹ä¸­çš„ä»»ä½•å…¶ä»–å±‚ä¸€æ ·ï¼ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä¸å¾—ä¸è°ƒç”¨ `qml``.``qnn``.``KerasLayer`ï¼Œå¹¶ä¸”æˆ‘ä»¬å¿…é¡»å‘å®ƒä¼ é€’ä¸€äº›ä¸œè¥¿ã€‚é¦–å…ˆï¼Œå½“ç„¶ï¼Œæˆ‘ä»¬å‘é€äº†åŒ…å«ç¥ç»ç½‘ç»œçš„é‡å­èŠ‚ç‚¹ã€‚ç„¶åï¼Œä¸€ä¸ªå­—å…¸é€šè¿‡æ‰€æœ‰æ¥å—å¯ä¼˜åŒ–å‚æ•°çš„èŠ‚ç‚¹å‚æ•°åç§°ç´¢å¼•ï¼Œå¹¶ä¸ºæ¯ä¸ªè¿™äº›å‚æ•°æŒ‡å®šäº†å®ƒä»¬æ¥å—çš„å‚æ•°æ•°é‡ã€‚ç”±äºæˆ‘ä»¬åªæœ‰ä¸€ä¸ªè¿™æ ·çš„å‚æ•°ï¼Œå³
    `theta`ï¼Œå¹¶ä¸”å®ƒåº”è¯¥åŒ…å« ![8](img/file506.png "8") ä¸ªå¯ä¼˜åŒ–å‚æ•°ï¼ˆå³ï¼Œå®ƒå°†æ˜¯ä¸€ä¸ªé•¿åº¦ä¸º ![8](img/file506.png
    "8") çš„æ•°ç»„ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å‘é€äº† `{``"``theta``:`
- en: '`8}`. Lastly, weâ€™ve had to specify the dimension of the output of the quantum
    node; since it only returns a numerical expectation value, this dimension is ![1](img/file13.png
    "1").'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '`8}`ã€‚æœ€åï¼Œæˆ‘ä»¬ä¸å¾—ä¸æŒ‡å®šé‡å­èŠ‚ç‚¹çš„è¾“å‡ºç»´åº¦ï¼›å› ä¸ºå®ƒåªè¿”å›ä¸€ä¸ªæ•°å€¼æœŸæœ›å€¼ï¼Œæ‰€ä»¥è¿™ä¸ªç»´åº¦æ˜¯![1](img/file13.png "1")ã€‚'
- en: 'Once we have a quantum layer, we can create a Keras model easily:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€æ—¦æˆ‘ä»¬æœ‰äº†é‡å­å±‚ï¼Œæˆ‘ä»¬å°±å¯ä»¥è½»æ¾åœ°åˆ›å»ºä¸€ä¸ªKerasæ¨¡å‹ï¼š
- en: '[PRE11]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The ability to integrate quantum nodes into neural networks with such a level
    of flexibility will enable us to easily construct more complex model architectures
    in the following chapter.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: èƒ½å¤Ÿä»¥è¿™ç§ç¨‹åº¦çš„çµæ´»æ€§å°†é‡å­èŠ‚ç‚¹é›†æˆåˆ°ç¥ç»ç½‘ç»œä¸­ï¼Œå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿè½»æ¾åœ°åœ¨ä¸‹ä¸€ç« æ„å»ºæ›´å¤æ‚çš„æ¨¡å‹æ¶æ„ã€‚
- en: 'Having our model ready, we now have to pick an optimizer and a loss function,
    and then we can compile the model just like any classical model. In our case,
    we will use the binary cross entropy loss (because we are training a binary classifier,
    after all) and we will rely on the Adam optimizer with a learning rate of ![0.005](img/file1389.png
    "0.005"). For the remaining parameters of the optimizer, we will trust the default
    values. Our code is, then, as follows:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬çš„æ¨¡å‹å‡†å¤‡å°±ç»ªåï¼Œæˆ‘ä»¬ç°åœ¨å¿…é¡»é€‰æ‹©ä¸€ä¸ªä¼˜åŒ–å™¨å’ŒæŸå¤±å‡½æ•°ï¼Œç„¶åæˆ‘ä»¬å¯ä»¥åƒä»»ä½•ç»å…¸æ¨¡å‹ä¸€æ ·ç¼–è¯‘æ¨¡å‹ã€‚åœ¨æˆ‘ä»¬çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼ˆå› ä¸ºæˆ‘ä»¬æ¯•ç«Ÿåœ¨è®­ç»ƒä¸€ä¸ªäºŒå…ƒåˆ†ç±»å™¨ï¼‰å¹¶ä¾èµ–äºå­¦ä¹ ç‡ä¸º![0.005](img/file1389.png
    "0.005")çš„Adamä¼˜åŒ–å™¨ã€‚å¯¹äºä¼˜åŒ–å™¨çš„å…¶ä½™å‚æ•°ï¼Œæˆ‘ä»¬å°†ä¿¡ä»»é»˜è®¤å€¼ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ä»£ç å¦‚ä¸‹ï¼š
- en: '[PRE12]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'In addition to this, we will use early stopping on the validation loss with
    a patience of two epochs by using the following instructions:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤åœ¨éªŒè¯æŸå¤±ä¸Šä½¿ç”¨æ—©åœæœºåˆ¶ï¼Œè€å¿ƒè®¾ç½®ä¸ºä¸¤ä¸ªepochï¼š
- en: '[PRE13]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: And we are now ready to send the final instruction to get our model trained.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»å‡†å¤‡å¥½å‘é€æœ€ç»ˆæŒ‡ä»¤æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: To learn moreâ€¦
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤š...
- en: You may remember that, at some point in this chapter, we discussed the different
    ways in which gradients involving quantum neural networks could be computed. And
    you might wonder why we havenâ€™t had to deal with that in order to get our model
    trained.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è®°å¾—ï¼Œåœ¨æœ¬ç« çš„æŸä¸ªæ—¶å€™ï¼Œæˆ‘ä»¬è®¨è®ºäº†æ¶‰åŠé‡å­ç¥ç»ç½‘ç»œçš„æ¢¯åº¦å¯ä»¥è®¡ç®—çš„ä¸åŒæ–¹å¼ã€‚ä½ å¯èƒ½ä¼šæƒ³çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬ä¸éœ€è¦å¤„ç†è¿™äº›æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ã€‚
- en: It turns out that PennyLane already picks the best differentiation method for
    us in order to compute gradients. Each quantum node can use certain differentiation
    methods â€” for instance, nodes with devices that act as interfaces to real hardware
    canâ€™t use automatic differentiation methods, but nodes with simulators can, and
    most do.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ç»“æœè¡¨æ˜ï¼ŒPennyLaneå·²ç»ä¸ºæˆ‘ä»¬é€‰æ‹©äº†æœ€ä½³å¾®åˆ†æ–¹æ³•æ¥è®¡ç®—æ¢¯åº¦ã€‚æ¯ä¸ªé‡å­èŠ‚ç‚¹å¯ä»¥ä½¿ç”¨æŸäº›å¾®åˆ†æ–¹æ³•â€”â€”ä¾‹å¦‚ï¼Œå……å½“çœŸå®ç¡¬ä»¶æ¥å£çš„è®¾å¤‡ä¸Šçš„èŠ‚ç‚¹ä¸èƒ½ä½¿ç”¨è‡ªåŠ¨å¾®åˆ†æ–¹æ³•ï¼Œä½†å…·æœ‰æ¨¡æ‹Ÿå™¨çš„èŠ‚ç‚¹å¯ä»¥ï¼Œè€Œä¸”å¤§å¤šæ•°éƒ½å¯ä»¥ã€‚
- en: Later in this section, we will discuss in detail all the differentiation methods
    that can be used in PennyLane.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬èŠ‚çš„åé¢ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®¨è®ºåœ¨PennyLaneä¸­å¯ä»¥ä½¿ç”¨çš„æ‰€æœ‰å¾®åˆ†æ–¹æ³•ã€‚
- en: 'To train our model, we just have to call the `fit` method. Since we will be
    using early stopping, we will be generous with the number of epochs and set it
    to ![50](img/file1390.png "50"). Also, we will fix a batch size of ![20](img/file588.png
    "20"). For that, we can use the following piece of code:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: è¦è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬åªéœ€è°ƒç”¨`fit`æ–¹æ³•ã€‚ç”±äºæˆ‘ä»¬å°†ä½¿ç”¨æ—©åœæœºåˆ¶ï¼Œæˆ‘ä»¬å°†æ…·æ…¨åœ°è®¾ç½®epochçš„æ•°é‡ï¼Œå°†å…¶è®¾ç½®ä¸º![50](img/file1390.png
    "50")ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†è®¾ç½®æ‰¹å¤§å°ä¸º![20](img/file588.png "20")ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š
- en: '[PRE14]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output that you will get upon running this instruction will be similar
    to the following:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰§è¡Œæ­¤æŒ‡ä»¤åï¼Œä½ å°†å¾—åˆ°ä»¥ä¸‹ç±»ä¼¼çš„ç»“æœï¼š
- en: '[PRE15]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: To learn moreâ€¦
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤š...
- en: If you followed all that weâ€™ve done so far without having asked TensorFlow to
    work with doubles, everything would work just fine â€” although you would get slightly
    different results. Nonetheless, if you try to fit a model using the Lightning
    simulator, you do need to ask TensorFlow to use doubles.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ åˆ°ç›®å‰ä¸ºæ­¢ä¸€ç›´æŒ‰ç…§æˆ‘ä»¬çš„æ­¥éª¤è¿›è¡Œï¼Œè€Œæ²¡æœ‰è¦æ±‚TensorFlowä½¿ç”¨åŒç²¾åº¦æµ®ç‚¹æ•°ï¼Œé‚£ä¹ˆä¸€åˆ‡éƒ½ä¼šæ­£å¸¸å·¥ä½œâ€”â€”å°½ç®¡ä½ å¯èƒ½ä¼šå¾—åˆ°ç•¥å¾®ä¸åŒçš„ç»“æœã€‚ç„¶è€Œï¼Œå¦‚æœä½ å°è¯•ä½¿ç”¨Lightningæ¨¡æ‹Ÿå™¨æ‹Ÿåˆæ¨¡å‹ï¼Œä½ ç¡®å®éœ€è¦è¦æ±‚TensorFlowä½¿ç”¨åŒç²¾åº¦æµ®ç‚¹æ•°ã€‚
- en: Note that we have manually shrunk the progress bar so that the output could
    fit within the width of the page. Also, keep in mind that the execution time may
    vary from device to device, but, in total, the training shouldnâ€™t take more than
    ![20](img/file588.png "20") minutes on an average
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œæˆ‘ä»¬å·²æ‰‹åŠ¨ç¼©å°è¿›åº¦æ¡ï¼Œä»¥ä¾¿è¾“å‡ºèƒ½å¤Ÿé€‚åº”é¡µé¢å®½åº¦ã€‚æ­¤å¤–ï¼Œè¯·è®°ä½ï¼Œæ‰§è¡Œæ—¶é—´å¯èƒ½ä¼šå› è®¾å¤‡è€Œå¼‚ï¼Œä½†æ€»ä½“è€Œè¨€ï¼Œè®­ç»ƒåœ¨å¹³å‡è®¾å¤‡ä¸Šä¸åº”è¶…è¿‡![20](img/file588.png
    "20")åˆ†é’Ÿã€‚
- en: computer.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: computer.
- en: Just by looking at the raw output, we can already see that the model is indeed
    learning, because there is a very significant drop in both the training and validation
    losses as the training progresses. It could be argued that there might be a tiny
    amount of overfitting, because the drop in the training loss is slightly greater
    than that in the validation loss. In any case, letâ€™s wait until we have a look
    at the accuracies before coming to any final conclusions.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: åªéœ€æŸ¥çœ‹åŸå§‹è¾“å‡ºï¼Œæˆ‘ä»¬å°±å¯ä»¥çœ‹å‡ºæ¨¡å‹ç¡®å®åœ¨å­¦ä¹ ï¼Œå› ä¸ºéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œè®­ç»ƒå’ŒéªŒè¯æŸå¤±éƒ½æœ‰éå¸¸æ˜¾è‘—çš„ä¸‹é™ã€‚å¯ä»¥äº‰è¾©è¯´å¯èƒ½å­˜åœ¨ä¸€ç‚¹è¿‡æ‹Ÿåˆï¼Œå› ä¸ºè®­ç»ƒæŸå¤±çš„ä¸‹é™ç•¥å¤§äºéªŒè¯æŸå¤±ã€‚æ— è®ºå¦‚ä½•ï¼Œè®©æˆ‘ä»¬åœ¨æŸ¥çœ‹å‡†ç¡®ç‡ä¹‹å‰ï¼Œä¸è¦å¾—å‡ºä»»ä½•æœ€ç»ˆç»“è®ºã€‚
- en: 'In this case, the training has only run for ![16](img/file619.png "16") epochs,
    so itâ€™s easy to get insights from the output returned by TensorFlow. Nonetheless,
    in the real world, training processes can go on for up to very large numbers of
    epochs, and, needless to say, in those situations the console output isnâ€™t particularly
    informative. In general, itâ€™s always a good practice to plot both the training
    and validation losses against the number of epochs, just to get a better insight
    into the performance of the training process. We can do this with the following
    instructions:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œè®­ç»ƒåªè¿›è¡Œäº†![16](img/file619.png "16")ä¸ªepochï¼Œå› æ­¤å¾ˆå®¹æ˜“ä»TensorFlowè¿”å›çš„è¾“å‡ºä¸­è·å¾—æ´å¯Ÿã€‚ç„¶è€Œï¼Œåœ¨ç°å®ä¸–ç•Œä¸­ï¼Œè®­ç»ƒè¿‡ç¨‹å¯èƒ½æŒç»­è¿›è¡Œåˆ°éå¸¸å¤§çš„epochæ•°é‡ï¼Œä¸ç”¨è¯´ï¼Œåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œæ§åˆ¶å°è¾“å‡ºå¹¶ä¸ç‰¹åˆ«å…·æœ‰ä¿¡æ¯é‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå§‹ç»ˆç»˜åˆ¶è®­ç»ƒå’ŒéªŒè¯æŸå¤±ä¸epochæ•°é‡çš„å¯¹æ¯”å›¾ï¼Œä»¥ä¾¿æ›´å¥½åœ°äº†è§£è®­ç»ƒè¿‡ç¨‹çš„æ€§èƒ½ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ä»¥ä¸‹æŒ‡ä»¤æ¥å®Œæˆï¼š
- en: '[PRE16]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Weâ€™ve decided to define a function just so that we can reuse it in future training
    processes. The resulting plot is shown in *Figure* [*10.6*](#Figure10.6).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å†³å®šå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥åœ¨æœªæ¥çš„è®­ç»ƒè¿‡ç¨‹ä¸­é‡ç”¨å®ƒã€‚ç”Ÿæˆçš„å›¾è¡¨æ˜¾ç¤ºåœ¨*å›¾* [*10.6*](#Figure10.6)ä¸­ã€‚
- en: '![Figure 10.6: Training and validation loss functions for every epoch](img/file1391.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾10.6ï¼šæ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±å‡½æ•°](img/file1391.png)'
- en: '**Figure 10.6**: Training and validation loss functions for every epoch'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**å›¾10.6**ï¼šæ¯ä¸ªepochçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±å‡½æ•°'
- en: 'And now itâ€™s time for our final test. Letâ€™s check the accuracy of our model
    on all our datasets to see if its performance is acceptable. This can be done
    with the following piece of code:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ˜¯æˆ‘ä»¬æœ€ç»ˆæµ‹è¯•çš„æ—¶å€™äº†ã€‚è®©æˆ‘ä»¬æ£€æŸ¥æˆ‘ä»¬çš„æ¨¡å‹åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å‡†ç¡®ç‡ï¼Œçœ‹çœ‹å…¶æ€§èƒ½æ˜¯å¦å¯æ¥å—ã€‚è¿™å¯ä»¥é€šè¿‡ä»¥ä¸‹ä»£ç ç‰‡æ®µæ¥å®Œæˆï¼š
- en: '[PRE17]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Upon running this, we get a training accuracy of ![71\%](img/file1392.png "71\%"),
    a validation accuracy of ![72\%](img/file1393.png "72\%"), and a test accuracy
    of ![72\%](img/file1393.png "72\%"). These results donâ€™t reflect any kind of overfitting.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤ä»£ç åï¼Œæˆ‘ä»¬å¾—åˆ°è®­ç»ƒå‡†ç¡®ç‡ä¸º![71%](img/file1392.png "71%")ï¼ŒéªŒè¯å‡†ç¡®ç‡ä¸º![72%](img/file1393.png
    "72%")ï¼Œæµ‹è¯•å‡†ç¡®ç‡ä¸º![72%](img/file1393.png "72%")ã€‚è¿™äº›ç»“æœå¹¶ä¸åæ˜ ä»»ä½•è¿‡æ‹Ÿåˆçš„æƒ…å†µã€‚
- en: Instead of implementing your own variational forms, you may prefer to use one
    of PennyLaneâ€™s built-in circuits. For instance, you could use the `StronglyEntanglingLayers`
    class. You should keep in mind, however, that the resulting variational form â€”
    as opposed to our own implementation of two-local â€” wonâ€™t take a one-dimensional
    array of inputs, but a three dimensional one! In particular, this form on ![n](img/file244.png
    "n") qubits with ![l](img/file514.png "l") layers will take as input a three-dimensional
    array of size ![n \times l \times 3](img/file1394.png "n \times l \times 3").
    Remember how, in this variational form, we need ![3](img/file472.png "3") arguments
    for the rotation gates, and there are ![n](img/file244.png "n") such gates in
    each of the ![l](img/file514.png "l") layers (you can take another look at *Figure*
    * [*10.4*](#Figure10.4)).*
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è€Œä¸æ˜¯å®ç°è‡ªå·±çš„å˜åˆ†å½¢å¼ï¼Œä½ å¯èƒ½æ›´æ„¿æ„ä½¿ç”¨PennyLaneå†…ç½®çš„ç”µè·¯ä¹‹ä¸€ã€‚ä¾‹å¦‚ï¼Œä½ å¯ä»¥ä½¿ç”¨`StronglyEntanglingLayers`ç±»ã€‚ç„¶è€Œï¼Œä½ åº”è¯¥è®°ä½ï¼Œç”±æ­¤äº§ç”Ÿçš„å˜åˆ†å½¢å¼â€”â€”ä¸æˆ‘ä»¬çš„ä¸¤å±€éƒ¨å®ç°ç›¸æ¯”â€”â€”ä¸ä¼šæ¥å—ä¸€ç»´è¾“å…¥æ•°ç»„ï¼Œè€Œæ˜¯ä¸€ä¸ªä¸‰ç»´æ•°ç»„ï¼ç‰¹åˆ«æ˜¯ï¼Œè¿™ä¸ªå½¢å¼åœ¨![n](img/file244.png
    "n")ä¸ªé‡å­æ¯”ç‰¹å’Œ![l](img/file514.png "l")å±‚çš„æƒ…å†µä¸‹å°†æ¥å—ä¸€ä¸ªå¤§å°ä¸º![n \times l \times 3](img/file1394.png
    "n \times l \times 3")çš„ä¸‰ç»´æ•°ç»„ä½œä¸ºè¾“å…¥ã€‚è®°å¾—åœ¨è¿™ä¸ªå˜åˆ†å½¢å¼ä¸­ï¼Œæˆ‘ä»¬éœ€è¦![3](img/file472.png "3")ä¸ªå‚æ•°æ¥æ—‹è½¬é—¨ï¼Œæ¯ä¸ª![l](img/file514.png
    "l")å±‚ä¸­æœ‰![n](img/file244.png "n")ä¸ªè¿™æ ·çš„é—¨ï¼ˆä½ å¯ä»¥å†æ¬¡æŸ¥çœ‹*å›¾* * [*10.4*](#Figure10.4))ï¼‰ã€‚
- en: '*If you are ever in doubt, you may call the `StronglyEntanglingLayers``.``shape`
    function specifying the number of layers and the number of qubits in the respective
    arguments `n_layers` and `n_wires`. This will return a three-tuple with the shape
    that the variational form expects.'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ æœ‰ä»»ä½•ç–‘é—®ï¼Œä½ å¯ä»¥è°ƒç”¨`StronglyEntanglingLayers.shape`å‡½æ•°ï¼ŒæŒ‡å®šå±‚æ•°å’Œé‡å­æ¯”ç‰¹æ•°ï¼Œåˆ†åˆ«åœ¨`n_layers`å’Œ`n_wires`å‚æ•°ä¸­ã€‚è¿™å°†è¿”å›ä¸€ä¸ªä¸‰ä¸ªå…ƒç´ çš„å…ƒç»„ï¼Œè¡¨ç¤ºå˜åˆ†å½¢å¼æœŸæœ›çš„å½¢çŠ¶ã€‚'
- en: 'For example, we could redefine our previous QNN to use this variational form
    as follows:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°å®šä¹‰æˆ‘ä»¬ä¹‹å‰çš„QNNï¼Œä½¿å…¶ä½¿ç”¨ä»¥ä¸‹å˜åˆ†å½¢å¼ï¼š
- en: '[PRE18]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: In this piece of code, we have stored in `nreps` the number of repetitions that
    we want in each instance of the variational form, in `weights_dim` the dimensions
    of the input that the variational form expects, and in `nweights` the number of
    inputs that each instance of the variational form will take. The rest is pretty
    self-explanatory. Inside the circuit, weâ€™ve had to reshape the `theta` array of
    parameters to make it fit into the shape that the variational form expects; in
    order to do this, weâ€™ve used the `tf``.``reshape` function, which can reshape
    TensorFlowâ€™s tensors while preserving all their metadata. The `weights_strong`
    dictionary that we defined at the end is the one that we would send to TensorFlow
    when constructing the Keras layer.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™æ®µä»£ç ä¸­ï¼Œæˆ‘ä»¬å°†æˆ‘ä»¬æƒ³è¦åœ¨æ¯ä¸ªå˜åˆ†å½¢å¼å®ä¾‹ä¸­é‡å¤çš„æ¬¡æ•°å­˜å‚¨åœ¨`nreps`ä¸­ï¼Œå°†å˜åˆ†å½¢å¼æœŸæœ›çš„è¾“å…¥ç»´åº¦å­˜å‚¨åœ¨`weights_dim`ä¸­ï¼Œå¹¶å°†æ¯ä¸ªå˜åˆ†å½¢å¼å®ä¾‹å°†æ¥å—çš„è¾“å…¥æ•°é‡å­˜å‚¨åœ¨`nweights`ä¸­ã€‚å…¶ä½™éƒ¨åˆ†ç›¸å½“ç›´è§‚ã€‚åœ¨ç”µè·¯å†…éƒ¨ï¼Œæˆ‘ä»¬å¿…é¡»å°†å‚æ•°çš„`theta`æ•°ç»„é‡å¡‘ä»¥é€‚åº”å˜åˆ†å½¢å¼æœŸæœ›çš„å½¢çŠ¶ï¼›ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†`tf.reshape`å‡½æ•°ï¼Œè¯¥å‡½æ•°å¯ä»¥åœ¨ä¿ç•™æ‰€æœ‰å…ƒæ•°æ®çš„åŒæ—¶é‡å¡‘TensorFlowçš„å¼ é‡ã€‚æˆ‘ä»¬å®šä¹‰åœ¨æœ€åçš„`weights_strong`å­—å…¸æ˜¯æˆ‘ä»¬æ„å»ºKeraså±‚æ—¶å°†å‘é€ç»™TensorFlowçš„å­—å…¸ã€‚
- en: Weâ€™ve already learned how you can train a quantum neural network using PennyLane
    and TensorFlow. We shall now discuss a few technical details in depth before bringing
    this section to an end.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨PennyLaneå’ŒTensorFlowè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œã€‚åœ¨ç»“æŸæœ¬èŠ‚ä¹‹å‰ï¼Œæˆ‘ä»¬å°†æ·±å…¥è®¨è®ºä¸€äº›æŠ€æœ¯ç»†èŠ‚ã€‚
- en: 10.2.4 Gradient computation in PennyLane
  id: totrans-247
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2.4 PennyLaneä¸­çš„æ¢¯åº¦è®¡ç®—
- en: As we have already mentioned, when you train a model with PennyLane, the framework
    itself figures out the best way in which to compute gradients. Different quantum
    nodes may be compatible with different methods of differentiation based on a variety
    of factors, most notably the kind of device they use.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æˆ‘ä»¬ä¹‹å‰æåˆ°çš„ï¼Œå½“ä½ ä½¿ç”¨PennyLaneè®­ç»ƒæ¨¡å‹æ—¶ï¼Œæ¡†æ¶æœ¬èº«ä¼šæ‰¾å‡ºè®¡ç®—æ¢¯åº¦çš„æœ€ä½³æ–¹å¼ã€‚ä¸åŒçš„é‡å­èŠ‚ç‚¹å¯èƒ½åŸºäºå„ç§å› ç´ ä¸ä¸åŒçš„å¾®åˆ†æ–¹æ³•å…¼å®¹ï¼Œæœ€æ˜¾è‘—çš„æ˜¯å®ƒä»¬ä½¿ç”¨çš„è®¾å¤‡ç±»å‹ã€‚
- en: To learn moreâ€¦
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤š...
- en: For an up-to-date reference of the differentiation methods that the `default``.`
    `qubit` simulator supports, you may check the online documentation at [https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æŸ¥çœ‹`default` `qubit`æ¨¡æ‹Ÿå™¨æ”¯æŒçš„å¾®åˆ†æ–¹æ³•çš„æœ€æ–°å‚è€ƒï¼Œä½ å¯ä»¥æŸ¥çœ‹åœ¨çº¿æ–‡æ¡£[https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations](https://docs.pennylane.ai/en/stable/introduction/interfaces.html#supported-configurations)ã€‚
- en: You will see that the compatibility of a quantum node with a differentiation
    method not only depends on the device itself but also on the return type of the
    node and the machine learning interface (in our case, the interface was TensorFlow).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¼šå‘ç°é‡å­èŠ‚ç‚¹ä¸å¾®åˆ†æ–¹æ³•çš„å…¼å®¹æ€§ä¸ä»…å–å†³äºè®¾å¤‡æœ¬èº«ï¼Œè¿˜å–å†³äºèŠ‚ç‚¹çš„è¿”å›ç±»å‹å’Œæœºå™¨å­¦ä¹ æ¥å£ï¼ˆåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œæ¥å£æ˜¯TensorFlowï¼‰ã€‚
- en: 'These are the differentiation methods that can be used in PennyLane:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›æ˜¯å¯ä»¥åœ¨PennyLaneä¸­ä½¿ç”¨çš„å¾®åˆ†æ–¹æ³•ï¼š
- en: '**Backpropagation**: Just the good old backpropagation method that is used
    in classical neural networks. Of course, this differentiation method only works
    on simulators that are compatible with automatic differentiation, because that
    is what is needed in order to analytically compute the gradients.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åå‘ä¼ æ’­**ï¼šè¿™åªæ˜¯ç»å…¸ç¥ç»ç½‘ç»œä¸­ä½¿ç”¨çš„è‰¯å¥½æ—§çš„åå‘ä¼ æ’­æ–¹æ³•ã€‚å½“ç„¶ï¼Œè¿™ç§å¾®åˆ†æ–¹æ³•ä»…åœ¨å…¼å®¹è‡ªåŠ¨å¾®åˆ†çš„æ¨¡æ‹Ÿå™¨ä¸Šå·¥ä½œï¼Œå› ä¸ºè¿™æ˜¯åˆ†æè®¡ç®—æ¢¯åº¦çš„å¿…è¦æ¡ä»¶ã€‚'
- en: The name of this method in PennyLane is `"``backprop``"`.
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨PennyLaneä¸­ï¼Œè¿™ç§æ–¹æ³•çš„åå­—æ˜¯`"backprop"`ã€‚
- en: '**Adjoint differentiation**: This is a more efficient version of backpropagation
    that relies on some of the nice computational â€odditiesâ€ of quantum computing,
    such as the fact that all the quantum circuits are implemented by unitary matrices,
    which are trivially invertible. Like backpropagation, this method only works on
    the simulators that are compatible with automatic differentiation, but it is more
    restrictive.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ä¼´éšå¾®åˆ†**ï¼šè¿™æ˜¯ä¾èµ–é‡å­è®¡ç®—çš„ä¸€äº›è®¡ç®—â€œæ€ªå¼‚â€ç‰¹æ€§çš„æ›´æœ‰æ•ˆç‰ˆæœ¬çš„åå‘ä¼ æ’­ï¼Œä¾‹å¦‚ï¼Œæ‰€æœ‰é‡å­ç”µè·¯éƒ½æ˜¯ç”±é…‰çŸ©é˜µå®ç°çš„ï¼Œè¿™äº›çŸ©é˜µå¯ä»¥ç®€å•åœ°æ±‚é€†ã€‚åƒåå‘ä¼ æ’­ä¸€æ ·ï¼Œè¿™ç§æ–¹æ³•ä»…åœ¨å…¼å®¹è‡ªåŠ¨å¾®åˆ†çš„æ¨¡æ‹Ÿå™¨ä¸Šå·¥ä½œï¼Œä½†å®ƒæ›´ä¸ºé™åˆ¶æ€§ã€‚'
- en: The name of this method in PennyLane is `"``adjoint``"`.
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨PennyLaneä¸­ï¼Œè¿™ç§æ–¹æ³•çš„åå­—æ˜¯`"adjoint"`ã€‚
- en: '**Finite differences**: Ever took a numerical analysis course at college? Then
    this will sound familiar. This method implements the old-school way of computing
    a numerical approximation of a gradient that we discussed in the previous section.
    It works on almost every quantum node.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**æœ‰é™å·®åˆ†æ³•**ï¼šä½ åœ¨å¤§å­¦é‡Œä¸Šè¿‡æ•°å€¼åˆ†æè¯¾ç¨‹å—ï¼Ÿé‚£ä¹ˆè¿™å¬èµ·æ¥ä¼šå¾ˆç†Ÿæ‚‰ã€‚è¿™ç§æ–¹æ³•å®ç°äº†æˆ‘ä»¬åœ¨ä¸Šä¸€èŠ‚ä¸­è®¨è®ºçš„æ—§å¼è®¡ç®—æ¢¯åº¦æ•°å€¼è¿‘ä¼¼çš„æ–¹æ³•ã€‚å®ƒå‡ ä¹é€‚ç”¨äºæ¯ä¸ªé‡å­èŠ‚ç‚¹ã€‚'
- en: The name of this method in PennyLane is `"``finite``-``diff``"`.
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨PennyLaneä¸­ï¼Œè¿™ç§æ–¹æ³•çš„åå­—æ˜¯ `"``finite``-``diff``"`.
- en: '**Parameter shift rule**: PennyLane fully implements the parameter-shift rule
    that we introduced previously. It works on most quantum nodes.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**å‚æ•°å¹³ç§»è§„åˆ™**ï¼šPennyLaneå®Œå…¨å®ç°äº†æˆ‘ä»¬ä¹‹å‰ä»‹ç»è¿‡çš„å‚æ•°å¹³ç§»è§„åˆ™ã€‚å®ƒé€‚ç”¨äºå¤§å¤šæ•°é‡å­èŠ‚ç‚¹ã€‚'
- en: The name of this method in PennyLane is `"``parameter``-``shift``"`.
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: åœ¨PennyLaneä¸­ï¼Œè¿™ç§æ–¹æ³•çš„åå­—æ˜¯ `"``parameter``-``shift``"`.
- en: '**Device gradient computation**: Some devices provide their own way of computing
    gradients. The name of the corresponding differentiation method is `"``device``"`.'
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**è®¾å¤‡æ¢¯åº¦è®¡ç®—**ï¼šä¸€äº›è®¾å¤‡æä¾›äº†è‡ªå·±çš„è®¡ç®—æ¢¯åº¦çš„æ–¹æ³•ã€‚ç›¸åº”çš„å¾®åˆ†æ–¹æ³•åä¸º `"``device``"`.'
- en: There are a couple of things that deserve clarification; the first of them is
    how a simulator could not be compatible with automatic differentiation. Oversimplifying
    a little bit, most simulators work by computing the evolution of the quantum state
    of a circuit and returning an output that is differentiable with respect to the
    parameters. The operations required to do all of this are themselves differentiable,
    and hence itâ€™s possible to use automatic differentiation on quantum nodes that
    use that simulator. But simulators may work differently. For instance, a simulator
    could return individual shots in a way that â€breaksâ€ the differentiability of
    the computation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ä»¶äº‹æƒ…éœ€è¦æ¾„æ¸…ï¼›å…¶ä¸­ä¹‹ä¸€æ˜¯æ¨¡æ‹Ÿå™¨å¦‚ä½•ä¸è‡ªåŠ¨å¾®åˆ†ä¸å…¼å®¹ã€‚ç¨å¾®ç®€åŒ–ä¸€ä¸‹ï¼Œå¤§å¤šæ•°æ¨¡æ‹Ÿå™¨é€šè¿‡è®¡ç®—ç”µè·¯çš„é‡å­æ€çš„æ¼”åŒ–å¹¶è¿”å›ä¸€ä¸ªç›¸å¯¹äºå‚æ•°å¯å¾®çš„è¾“å‡ºæ¥å·¥ä½œã€‚æ‰§è¡Œæ‰€æœ‰è¿™äº›æ“ä½œæ‰€éœ€çš„æ“ä½œæœ¬èº«æ˜¯å¯å¾®çš„ï¼Œå› æ­¤å¯ä»¥åœ¨ä½¿ç”¨è¯¥æ¨¡æ‹Ÿå™¨çš„é‡å­èŠ‚ç‚¹ä¸Šä½¿ç”¨è‡ªåŠ¨å¾®åˆ†ã€‚ä½†æ˜¯æ¨¡æ‹Ÿå™¨å¯èƒ½å·¥ä½œæ–¹å¼ä¸åŒã€‚ä¾‹å¦‚ï¼Œä¸€ä¸ªæ¨¡æ‹Ÿå™¨å¯èƒ½ä¼šä»¥â€œç ´åâ€è®¡ç®—å¯å¾®æ€§çš„æ–¹å¼è¿”å›å•ä¸ªå°„å‡»ã€‚
- en: Another thing that may have caught your attention is that the finite difference
    method can be used on â€mostâ€ quantum nodes, but not on all of them. Thatâ€™s because
    some quantum nodes may return outputs that donâ€™t make it possible for the finite
    differences method to work with them. For instance, if a node returns an array
    of samples, the differentiability is broken. If instead, it returned an expectation
    value â€” even if it were just an empirical approximation obtained from a collection
    of samples â€” then a gradient would exist and the finite differences method could
    be used to compute it.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä»¶å¯èƒ½å¼•èµ·ä½ æ³¨æ„çš„äº‹æƒ…æ˜¯ï¼Œæœ‰é™å·®åˆ†æ³•å¯ä»¥ç”¨äºâ€œå¤§å¤šæ•°â€é‡å­èŠ‚ç‚¹ï¼Œä½†ä¸æ˜¯æ‰€æœ‰ã€‚è¿™æ˜¯å› ä¸ºä¸€äº›é‡å­èŠ‚ç‚¹å¯èƒ½è¿”å›çš„è¾“å‡ºä½¿å¾—æœ‰é™å·®åˆ†æ³•æ— æ³•ä¸å®ƒä»¬ä¸€èµ·å·¥ä½œã€‚ä¾‹å¦‚ï¼Œå¦‚æœä¸€ä¸ªèŠ‚ç‚¹è¿”å›ä¸€ä¸ªæ ·æœ¬æ•°ç»„ï¼Œå¯å¾®æ€§å°±ä¼šä¸­æ–­ã€‚ç›¸åï¼Œå¦‚æœå®ƒè¿”å›ä¸€ä¸ªæœŸæœ›å€¼â€”â€”å³ä½¿å®ƒåªæ˜¯ä»æ ·æœ¬é›†åˆä¸­è·å¾—çš„ç»éªŒè¿‘ä¼¼â€”â€”é‚£ä¹ˆå°±ä¼šå­˜åœ¨æ¢¯åº¦ï¼Œå¹¶ä¸”å¯ä»¥ä½¿ç”¨æœ‰é™å·®åˆ†æ³•æ¥è®¡ç®—å®ƒã€‚
- en: Exercise 10.3
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 10.3
- en: List all the PennyLane differentiation methods that can be used on quantum hardware
    and all the differentiation methods that can be used on simulators.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ—å‡ºæ‰€æœ‰å¯ä»¥åœ¨é‡å­ç¡¬ä»¶ä¸Šä½¿ç”¨çš„PennyLaneå¾®åˆ†æ–¹æ³•ä»¥åŠæ‰€æœ‰å¯ä»¥åœ¨æ¨¡æ‹Ÿå™¨ä¸Šä½¿ç”¨çš„å¾®åˆ†æ–¹æ³•ã€‚
- en: The way in which you can ask PennyLane to use a specific differentiation method
    â€” letâ€™s say one named `"``method``"` â€” is by passing the optional argument `diff_method`
    `=` `"``method``"` to the quantum node decorator or initializer. That is, if you
    use the QNode decorator, you should write
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥é€šè¿‡å°†å¯é€‰å‚æ•° `diff_method` `=` `"``method``"` ä¼ é€’ç»™é‡å­èŠ‚ç‚¹è£…é¥°å™¨æˆ–åˆå§‹åŒ–å™¨æ¥è¯·æ±‚PennyLaneä½¿ç”¨ç‰¹å®šçš„å¾®åˆ†æ–¹æ³•â€”â€”æ¯”å¦‚è¯´ä¸€ä¸ªåä¸º
    `"``method``"` çš„æ–¹æ³•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œå¦‚æœä½ ä½¿ç”¨QNodeè£…é¥°å™¨ï¼Œä½ åº”è¯¥å†™
- en: '[PRE19]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Alternatively, if you decided to assemble a circuit `circuit` and a device
    `device` into a quantum node directly, you should call the following:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœä½ å†³å®šç›´æ¥å°†ç”µè·¯ `circuit` å’Œè®¾å¤‡ `device` ç»„è£…æˆä¸€ä¸ªé‡å­èŠ‚ç‚¹ï¼Œä½ åº”è¯¥è°ƒç”¨ä»¥ä¸‹æ“ä½œï¼š
- en: '[PRE20]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: By default, `diff_method` is set to `"``best``"`, which, as weâ€™ve said before,
    lets PennyLane choose on our behalf the best differentiation method.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œ`diff_method` è¢«è®¾ç½®ä¸º `"``best``"`ï¼Œæ­£å¦‚æˆ‘ä»¬ä¹‹å‰æ‰€è¯´çš„ï¼Œè¿™ä¼šè®©PennyLaneä»£è¡¨æˆ‘ä»¬é€‰æ‹©æœ€ä½³å¾®åˆ†æ–¹æ³•ã€‚
- en: In our particular case, PennyLane has been using the backpropagation differentiation
    method all this time â€” without us even noticing!
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æˆ‘ä»¬ç‰¹å®šçš„æ¡ˆä¾‹ä¸­ï¼ŒPennyLaneä¸€ç›´ä½¿ç”¨åå‘ä¼ æ’­å¾®åˆ†æ³•ï¼Œè€Œæˆ‘ä»¬ç”šè‡³æ²¡æœ‰æ³¨æ„åˆ°è¿™ä¸€ç‚¹ï¼
- en: To learn moreâ€¦
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤š...
- en: 'If you want to know which differentiation method PennyLane uses by default
    on a device `dev` and on a certain interface `inter` (in our case, `"``tensorflow``"`),
    you can just call the following function:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³çŸ¥é“PennyLaneåœ¨è®¾å¤‡`dev`å’Œç‰¹å®šæ¥å£`inter`ï¼ˆåœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œ`"tensorflow"`ï¼‰ä¸Šé»˜è®¤ä½¿ç”¨çš„å¾®åˆ†æ–¹æ³•ï¼Œä½ å¯ä»¥ç›´æ¥è°ƒç”¨ä»¥ä¸‹å‡½æ•°ï¼š
- en: '[PRE21]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Our quantum node is compatible with all the differentiation methods except with
    device differentiation, because `default``.``qubit` doesnâ€™t implement its own
    special way of computing gradients. Thus, just to better understand the differences
    in performance, we can try out all the differentiation methods and see how they
    behave.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬çš„é‡å­èŠ‚ç‚¹ä¸æ‰€æœ‰å¾®åˆ†æ–¹æ³•å…¼å®¹ï¼Œé™¤äº†è®¾å¤‡å¾®åˆ†ï¼Œå› ä¸º`default` `qubit`æ²¡æœ‰å®ç°è‡ªå·±çš„ç‰¹æ®Šè®¡ç®—æ¢¯åº¦æ–¹å¼ã€‚å› æ­¤ï¼Œä¸ºäº†æ›´å¥½åœ°ç†è§£æ€§èƒ½å·®å¼‚ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•æ‰€æœ‰å¾®åˆ†æ–¹æ³•å¹¶è§‚å¯Ÿå®ƒä»¬çš„è¿è¡Œæƒ…å†µã€‚
- en: To learn moreâ€¦
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: è¦äº†è§£æ›´å¤šä¿¡æ¯...
- en: You may remember that, when using the Lightning simulator, we do need to ask
    TensorFlow to use doubles all across the Keras model instead of floats â€” itâ€™s
    not an option, but a necessity. The same happens when we use differentiation methods
    other than backpropagation with `default``.` `qubit`.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è®°å¾—ï¼Œå½“ä½¿ç”¨Lightningæ¨¡æ‹Ÿå™¨æ—¶ï¼Œæˆ‘ä»¬ç¡®å®éœ€è¦è¦æ±‚TensorFlowåœ¨æ•´ä¸ªKerasæ¨¡å‹ä¸­ä½¿ç”¨åŒç²¾åº¦æµ®ç‚¹æ•°è€Œä¸æ˜¯å•ç²¾åº¦æµ®ç‚¹æ•°â€”â€”è¿™ä¸æ˜¯ä¸€ä¸ªé€‰é¡¹ï¼Œè€Œæ˜¯ä¸€ä¸ªå¿…éœ€å“ã€‚å½“æˆ‘ä»¬ä½¿ç”¨é™¤åå‘ä¼ æ’­ä¹‹å¤–çš„å¾®åˆ†æ–¹æ³•ä¸`default`
    `qubit`æ—¶ï¼Œæƒ…å†µä¹Ÿæ˜¯å¦‚æ­¤ã€‚
- en: 'Letâ€™s begin with adjoint differentiation. In order to retrain our model with
    this differentiation method, we will rerun all our previous code, but changing
    the quantum node definition to the following:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ä¼´éšå¾®åˆ†å¼€å§‹ã€‚ä¸ºäº†ä½¿ç”¨è¿™ç§å¾®åˆ†æ–¹æ³•é‡æ–°è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å°†é‡æ–°è¿è¡Œæ‰€æœ‰ä¹‹å‰çš„ä»£ç ï¼Œä½†å°†é‡å­èŠ‚ç‚¹å®šä¹‰æ›´æ”¹ä¸ºä»¥ä¸‹å†…å®¹ï¼š
- en: '[PRE22]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Reasonably enough, instead of rerunning all your code, you may want to add
    the execution of alternative differentiation methods as part of it â€” particularly
    if you are keeping your code in a notebook. If you want to do so while ensuring
    that the training is done in identical conditions (the same environment and seeds),
    these are the lines that you would have to run:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: è¶³å¤Ÿåˆç†çš„æ˜¯ï¼Œä½ å¯èƒ½ä¼šæƒ³åœ¨ä¸é‡æ–°è¿è¡Œæ‰€æœ‰ä»£ç çš„æƒ…å†µä¸‹ï¼Œå°†æ›¿ä»£å¾®åˆ†æ–¹æ³•çš„æ‰§è¡Œä½œä¸ºå…¶ä¸­çš„ä¸€éƒ¨åˆ†â€”â€”å°¤å…¶æ˜¯å¦‚æœä½ å°†ä»£ç ä¿å­˜åœ¨ç¬”è®°æœ¬ä¸­ã€‚å¦‚æœä½ æƒ³ç¡®ä¿åœ¨ç›¸åŒæ¡ä»¶ä¸‹ï¼ˆç›¸åŒçš„ç¯å¢ƒå’Œç§å­ï¼‰å®Œæˆè®­ç»ƒï¼Œä»¥ä¸‹æ˜¯ä½ å¿…é¡»è¿è¡Œçš„è¿™äº›è¡Œï¼š
- en: '[PRE23]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Upon running this, you will get the exact same training behavior that we got
    with backpropagation â€” the same evolution of the training and validation losses
    and, of course, the same accuracies. Where there is a noticeable change, however,
    is in training time. In our case, training with backpropagation took a rough average
    of ![21](img/file1395.png "21") seconds per epoch. Using adjoint differentiation,
    in contrast, the training took, on average, ![10](img/file161.png "10") seconds
    per epoch. Thatâ€™s a big gain!
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œæ­¤æ“ä½œåï¼Œä½ å°†è·å¾—ä¸åå‘ä¼ æ’­ç›¸åŒçš„ç²¾ç¡®è®­ç»ƒè¡Œä¸ºâ€”â€”ç›¸åŒçš„è®­ç»ƒå’ŒéªŒè¯æŸå¤±æ¼”å˜ï¼Œå½“ç„¶ï¼Œè¿˜æœ‰ç›¸åŒçš„å‡†ç¡®ç‡ã€‚ç„¶è€Œï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼Œè®­ç»ƒæ—¶é—´æœ‰æ‰€ä¸åŒã€‚åœ¨æˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­ï¼Œä½¿ç”¨åå‘ä¼ æ’­è¿›è¡Œè®­ç»ƒçš„å¹³å‡æ—¶é—´å¤§çº¦ä¸º![21](img/file1395.png
    "21")ç§’æ¯è½®ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œä½¿ç”¨ä¼´éšå¾®åˆ†ï¼Œå¹³å‡æ¯è½®è®­ç»ƒæ—¶é—´ä¸º![10](img/file161.png "10")ç§’ã€‚è¿™æ˜¯ä¸€ä¸ªå·¨å¤§çš„è¿›æ­¥ï¼
- en: Actually, if you wanted to further reduce the training time, you should try
    the Lightning simulator with the adjoint method. Depending on the hardware configuration
    of your computer, it can yield very significant boosts in performance.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œå¦‚æœä½ æƒ³è¿›ä¸€æ­¥å‡å°‘è®­ç»ƒæ—¶é—´ï¼Œä½ åº”è¯¥å°è¯•ä½¿ç”¨ä¼´éšæ–¹æ³•ä¸Lightningæ¨¡æ‹Ÿå™¨ã€‚æ ¹æ®ä½ ç”µè„‘çš„ç¡¬ä»¶é…ç½®ï¼Œå®ƒå¯ä»¥åœ¨æ€§èƒ½ä¸Šå¸¦æ¥éå¸¸æ˜¾è‘—çš„æå‡ã€‚
- en: 'Letâ€™s now train our model with the two remaining differentiation methods, which
    are the hardware-compatible ones: the parameter-shift rule and finite differences.
    In order to do that, we will just have to rerun our code changing the value of
    the differentiation method in the quantum node definition. In order to avoid redundancy,
    we wonâ€™t rewrite everything here â€” we trust these small changes to you!'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä½¿ç”¨å‰©ä¸‹çš„ä¸¤ç§å¾®åˆ†æ–¹æ³•æ¥è®­ç»ƒæˆ‘ä»¬çš„æ¨¡å‹ï¼Œè¿™ä¸¤ç§æ–¹æ³•æ˜¯ç¡¬ä»¶å…¼å®¹çš„ï¼šå‚æ•°ç§»ä½è§„åˆ™å’Œæœ‰é™å·®åˆ†ã€‚ä¸ºäº†åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬åªéœ€é‡æ–°è¿è¡Œæˆ‘ä»¬çš„ä»£ç ï¼Œæ›´æ”¹é‡å­èŠ‚ç‚¹å®šä¹‰ä¸­çš„å¾®åˆ†æ–¹æ³•å€¼ã€‚ä¸ºäº†é¿å…å†—ä½™ï¼Œæˆ‘ä»¬ä¸ä¼šåœ¨è¿™é‡Œé‡å†™æ‰€æœ‰å†…å®¹â€”â€”æˆ‘ä»¬ç›¸ä¿¡è¿™äº›å°çš„å˜åŒ–å¯ä»¥ç”±ä½ æ¥å®Œæˆï¼
- en: 'When retraining with these two models, these are the results we obtained:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨è¿™ä¸¤ç§æ¨¡å‹é‡æ–°è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬å¾—åˆ°äº†ä»¥ä¸‹ç»“æœï¼š
- en: Using the parameter shift rule yielded the very same results as the other differentiation
    methods. Regarding training time, each epoch took, on average, ![14](img/file1396.png
    "14") seconds to complete. Thatâ€™s better than the ![21](img/file1395.png "21")
    seconds that we got with backpropagation, but not as good as the ![10](img/file161.png
    "10") seconds that the adjoint method gave us.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‚æ•°å¹³ç§»è§„åˆ™å¾—åˆ°äº†ä¸å…¶ä»–å¾®åˆ†æ–¹æ³•ç›¸åŒçš„ç»“æœã€‚å…³äºè®­ç»ƒæ—¶é—´ï¼Œæ¯ä¸ªepochå¹³å‡éœ€è¦![14](img/file1396.png "14")ç§’æ¥å®Œæˆã€‚è¿™æ¯”æˆ‘ä»¬ç”¨åå‘ä¼ æ’­å¾—åˆ°çš„![21](img/file1395.png
    "21")ç§’è¦å¥½ï¼Œä½†ä¸å¦‚adjointæ–¹æ³•ç»™å‡ºçš„![10](img/file161.png "10")ç§’å¥½ã€‚
- en: When using finite differences differentiation, we got, once again, the same
    results that the other methods yielded. On average, each epoch took ![10](img/file161.png
    "10") seconds to complete, which matches the training time of adjoint differentiation.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å½“ä½¿ç”¨æœ‰é™å·®åˆ†å¾®åˆ†æ—¶ï¼Œæˆ‘ä»¬å†æ¬¡å¾—åˆ°äº†ä¸å…¶ä»–æ–¹æ³•ç›¸åŒçš„ç»“æœã€‚å¹³å‡è€Œè¨€ï¼Œæ¯ä¸ªepochéœ€è¦![10](img/file161.png "10")ç§’æ¥å®Œæˆï¼Œè¿™ä¸adjointå¾®åˆ†æ³•çš„è®­ç»ƒæ—¶é—´ç›¸åŒ¹é…ã€‚
- en: Keep in mind that this comparison holds for the particular model that we have
    considered. The results may vary as the complexity of the models increases and,
    in particular, hardware-compatible methods may perform more poorly on simulators
    when training complex QNN architectures.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œè¿™ä¸ªæ¯”è¾ƒä»…é€‚ç”¨äºæˆ‘ä»¬è€ƒè™‘çš„ç‰¹å®šæ¨¡å‹ã€‚éšç€æ¨¡å‹å¤æ‚æ€§çš„å¢åŠ ï¼Œç»“æœå¯èƒ½ä¼šæœ‰æ‰€ä¸åŒï¼Œç‰¹åˆ«æ˜¯ä¸ç¡¬ä»¶å…¼å®¹çš„æ–¹æ³•åœ¨è®­ç»ƒå¤æ‚çš„QNNæ¶æ„æ—¶å¯èƒ½åœ¨æ¨¡æ‹Ÿå™¨ä¸Šè¡¨ç°å¾—æ›´å·®ã€‚
- en: And thatâ€™s probably all you need to know about the differentiation methods that
    are available in PennyLane. Letâ€™s now have a look at what Qiskit has to offer
    in terms of quantum neural networks.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯æ‚¨éœ€è¦äº†è§£çš„å…³äºPennyLaneä¸­å¯ç”¨çš„å¾®åˆ†æ–¹æ³•çš„å…¨éƒ¨å†…å®¹ã€‚ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹Qiskitåœ¨é‡å­ç¥ç»ç½‘ç»œæ–¹é¢èƒ½æä¾›ä»€ä¹ˆã€‚
- en: '10.3 Quantum neural networks in Qiskit: a commentary'
  id: totrans-290
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10.3 Qiskitä¸­çš„é‡å­ç¥ç»ç½‘ç»œï¼šè¯„è®º
- en: In the previous section, we had a chance to explore in great depth the implementation
    and training of quantum neural networks in PennyLane. We wonâ€™t do an analogous
    discussion for Qiskit in such a level of detail, but we will at least give you
    a few ideas about how to get started should you ever need to use Qiskit in order
    to work with quantum neural networks.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¸Šä¸€èŠ‚ä¸­ï¼Œæˆ‘ä»¬æœ‰æœºä¼šæ·±å…¥æ¢è®¨PennyLaneä¸­é‡å­ç¥ç»ç½‘ç»œçš„å®ç°å’Œè®­ç»ƒã€‚æˆ‘ä»¬ä¸ä¼šä»¥å¦‚æ­¤è¯¦ç»†çš„ç¨‹åº¦å¯¹Qiskitè¿›è¡Œç±»ä¼¼çš„è®¨è®ºï¼Œä½†è‡³å°‘ä¼šç»™ä½ ä¸€äº›å…³äºå¦‚ä½•å¼€å§‹ä½¿ç”¨Qiskitæ¥å¤„ç†é‡å­ç¥ç»ç½‘ç»œçš„æƒ³æ³•ã€‚
- en: PennyLane provides a very homogeneous and flexible experience. No matter if
    youâ€™re training a simple binary classifier or a complex hybrid architecture like
    the ones we will study in the following chapter, itâ€™s all done in the same way.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLaneæä¾›äº†ä¸€ä¸ªéå¸¸ç»Ÿä¸€å’Œçµæ´»çš„ä½“éªŒã€‚æ— è®ºæ‚¨æ˜¯åœ¨è®­ç»ƒä¸€ä¸ªç®€å•çš„äºŒåˆ†ç±»å™¨ï¼Œè¿˜æ˜¯åƒæˆ‘ä»¬åœ¨ä¸‹ä¸€ç« å°†è¦ç ”ç©¶çš„é‚£æ ·å¤æ‚çš„æ··åˆæ¶æ„ï¼Œéƒ½æ˜¯ç”¨åŒæ ·çš„æ–¹å¼å®Œæˆçš„ã€‚
- en: Qiskit, by contrast, provides a more â€structuralâ€ approach. It gives you a suite
    of classes that can be used to train different kinds of neural networks and that
    allow you to define your networks in different ways. Itâ€™s difficult to judge whether
    this is a better or worse approach; in the end, itâ€™s just a matter of taste. On
    the one hand, training basic models in Qiskit might be simpler than training them
    in PennyLane because of the ease of use of some of these purpose-built classes.
    On the other hand, having different ways of accomplishing the same thing â€” one
    could argue â€” might generate some unnecessary complexity.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸æ¯”ä¹‹ä¸‹ï¼ŒQiskitæä¾›äº†ä¸€ç§æ›´â€œç»“æ„åŒ–â€çš„æ–¹æ³•ã€‚å®ƒæä¾›äº†ä¸€ç³»åˆ—å¯ä»¥ç”¨æ¥è®­ç»ƒä¸åŒç±»å‹ç¥ç»ç½‘ç»œçš„ç±»ï¼Œå¹¶å…è®¸æ‚¨ä»¥ä¸åŒçš„æ–¹å¼å®šä¹‰æ‚¨çš„ç½‘ç»œã€‚å¾ˆéš¾åˆ¤æ–­è¿™æ˜¯å¦æ˜¯ä¸€ç§æ›´å¥½æˆ–æ›´å·®çš„æ–¹æ³•ï¼›æœ€ç»ˆï¼Œè¿™åªæ˜¯ä¸ªäººå£å‘³çš„é—®é¢˜ã€‚ä¸€æ–¹é¢ï¼Œç”±äºä¸€äº›ä¸“ä¸ºç‰¹å®šç›®çš„è®¾è®¡çš„ç±»æ˜“äºä½¿ç”¨ï¼Œåœ¨Qiskitä¸­è®­ç»ƒåŸºæœ¬æ¨¡å‹å¯èƒ½æ¯”åœ¨PennyLaneä¸­è®­ç»ƒå®ƒä»¬è¦ç®€å•ã€‚å¦ä¸€æ–¹é¢ï¼Œæœ‰ä¸åŒæ–¹å¼å®ŒæˆåŒä¸€ä»¶äº‹â€”â€”æœ‰äººå¯èƒ½ä¼šè¯´â€”â€”å¯èƒ½ä¼šäº§ç”Ÿä¸€äº›ä¸å¿…è¦çš„å¤æ‚æ€§ã€‚
- en: 'The classes provided by Qiskit for the implementation of quantum neural networks
    can be imported from `qiskit_machine_learning``.``neural_networks` (please, refer
    to *Appendix* [*D*](ch027.xhtml#x1-240000D)*, Installing the Tools*, for installation
    instructions). These are some of them:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: Qiskitä¸ºé‡å­ç¥ç»ç½‘ç»œçš„å®ç°æä¾›çš„ç±»å¯ä»¥ä»`qiskit_machine_learning``.``neural_networks`å¯¼å…¥ï¼ˆè¯·å‚é˜…*é™„å½•*
    [*D*](ch027.xhtml#x1-240000D)*ï¼Œå®‰è£…å·¥å…·*ï¼Œä»¥è·å–å®‰è£…è¯´æ˜ï¼‰ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä¾‹å­ï¼š
- en: '**Two-layer QNN**: The `TwoLayerQNN` class can be used to implement a quantum
    neural network with a single feature map, a variational form, and an observable.
    It works for any vanilla quantum neural network.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**åŒå±‚QNN**ï¼š`TwoLayerQNN`ç±»å¯ä»¥ç”¨æ¥å®ç°ä¸€ä¸ªå…·æœ‰å•ä¸ªç‰¹å¾å›¾ã€å˜åˆ†å½¢å¼å’Œå¯è§‚å¯Ÿé‡çš„é‡å­ç¥ç»ç½‘ç»œã€‚å®ƒé€‚ç”¨äºä»»ä½•æ™®é€šçš„é‡å­ç¥ç»ç½‘ç»œã€‚'
- en: '**Circuit QNN**: The `CircuitQNN` class allows you to implement a quantum neural
    network from a parametrized circuit. The final state of the circuit will be measured
    on the computational basis, and each measurement result can be mapped to an integer
    label through an interpreter function. This can be useful, for instance, if you
    want to build a classifier.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**ç”µè·¯QNN**ï¼š`CircuitQNN`ç±»å…è®¸ä½ ä»ä¸€ä¸ªå‚æ•°åŒ–ç”µè·¯ä¸­å®ç°ä¸€ä¸ªé‡å­ç¥ç»ç½‘ç»œã€‚ç”µè·¯çš„æœ€ç»ˆçŠ¶æ€å°†åœ¨è®¡ç®—åŸºä¸Šè¢«æµ‹é‡ï¼Œå¹¶ä¸”æ¯ä¸ªæµ‹é‡ç»“æœå¯ä»¥é€šè¿‡ä¸€ä¸ªè§£é‡Šå‡½æ•°æ˜ å°„åˆ°ä¸€ä¸ªæ•´æ•°æ ‡ç­¾ã€‚è¿™å¾ˆæœ‰ç”¨ï¼Œä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³æ„å»ºä¸€ä¸ªåˆ†ç±»å™¨ã€‚'
- en: 'By the way, in Qiskit lingo, variational forms are called **ansatzs**. As you
    surely remember, this is also the name that was used in the context of the VQE
    algorithm that we studied in *Chapter* *[*7*](ch015.xhtml#x1-1190007)*, VQE: Variational
    Quantum* *Eigensolver*.*'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€å¥ï¼Œåœ¨Qiskitçš„æœ¯è¯­ä¸­ï¼Œå˜åˆ†å½¢å¼è¢«ç§°ä¸º**ansatzs**ã€‚æ­£å¦‚ä½ è‚¯å®šè®°å¾—çš„ï¼Œè¿™ä¸ªåå­—ä¹Ÿç”¨äºæˆ‘ä»¬åœ¨*ç¬¬7ç« *ä¸­ç ”ç©¶çš„VQEç®—æ³•çš„ä¸Šä¸‹æ–‡ä¸­ï¼ŒVQEï¼šå˜åˆ†é‡å­*æœ¬å¾å€¼æ±‚è§£å™¨*ã€‚
- en: '*If, when designing a neural network in Qiskit, you want to use the ZZ feature
    map or the two-local variational form, thereâ€™s no need for you to re-implement
    them; they are bundled with Qiskit. You can get them as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '*å¦‚æœä½ åœ¨Qiskitä¸­è®¾è®¡ç¥ç»ç½‘ç»œæ—¶æƒ³ä½¿ç”¨ZZç‰¹å¾å›¾æˆ–ä¸¤ä¸ªå±€éƒ¨çš„å˜åˆ†å½¢å¼ï¼Œä½ ä¸éœ€è¦é‡æ–°å®ç°å®ƒä»¬ï¼›å®ƒä»¬åŒ…å«åœ¨Qiskitä¸­ã€‚ä½ å¯ä»¥å¦‚ä¸‹è·å–å®ƒä»¬ï¼š'
- en: '[PRE24]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In the call to the ZZ feature map class, we have set the number of repetitions
    to ![1](img/file13.png "1") â€” any other number would yield a feature map with
    that number of repetitions of the ZZ feature map scheme. In the call to the two-local
    class, we have also specified â€” in addition to the repetitions â€” the rotation
    gates, the controlled gates, and the entanglement layout that we want to use.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è°ƒç”¨ZZç‰¹å¾å›¾ç±»æ—¶ï¼Œæˆ‘ä»¬è®¾ç½®äº†é‡å¤æ¬¡æ•°ä¸º![1](img/file13.png "1")â€”â€”ä»»ä½•å…¶ä»–æ•°å­—éƒ½ä¼šäº§ç”Ÿå…·æœ‰è¯¥æ•°é‡é‡å¤çš„ZZç‰¹å¾å›¾æ–¹æ¡ˆçš„ç‰¹å¾å›¾ã€‚åœ¨è°ƒç”¨ä¸¤ä¸ªå±€éƒ¨ç±»æ—¶ï¼Œæˆ‘ä»¬è¿˜æŒ‡å®šäº†â€”â€”é™¤äº†é‡å¤æ¬¡æ•°ä¹‹å¤–â€”â€”æˆ‘ä»¬æƒ³è¦ä½¿ç”¨çš„æ—‹è½¬é—¨ã€å—æ§é—¨å’Œçº ç¼ å¸ƒå±€ã€‚
- en: 'For the sake of example, we can define a `TwoLayer` quantum neural network
    on three qubits with the ZZ feature map and two-local variational form that we
    have just instantiated. We can do this as follows:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºäº†ä¸¾ä¾‹ï¼Œæˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªåœ¨ä¸‰ä¸ªé‡å­æ¯”ç‰¹ä¸Šä½¿ç”¨ZZç‰¹å¾å›¾å’Œä¸¤ä¸ªå±€éƒ¨å˜åˆ†å½¢å¼çš„`TwoLayer`é‡å­ç¥ç»ç½‘ç»œã€‚æˆ‘ä»¬å¯ä»¥è¿™æ ·åšï¼š
- en: '[PRE25]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Since we havenâ€™t specified an observable, the resulting QNN will return the
    expectation value of the ![Z \otimes Z \otimes Z](img/file1397.png "Z \otimes
    Z \otimes Z") observable measured after feeding the execution of the networkâ€™s
    circuit.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬æ²¡æœ‰æŒ‡å®šä¸€ä¸ªå¯è§‚å¯Ÿé‡ï¼Œæ‰€ä»¥ç»“æœQNNå°†è¿”å›åœ¨å°†ç½‘ç»œçš„ç”µè·¯æ‰§è¡Œåæµ‹é‡çš„![Z \otimes Z \otimes Z](img/file1397.png
    "Z \otimes Z \otimes Z")å¯è§‚å¯Ÿé‡çš„æœŸæœ›å€¼ã€‚
- en: 'We can simulate analytically the network that we have just created on some
    random inputs and optimizable parameters as follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨ä¸€äº›éšæœºè¾“å…¥å’Œå¯ä¼˜åŒ–å‚æ•°ä¸Šï¼Œå¦‚ä¸‹æ¨¡æ‹Ÿæˆ‘ä»¬åˆšåˆšåˆ›å»ºçš„ç½‘ç»œï¼š
- en: '[PRE26]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The first argument is an array with some (random) classical inputs while the
    second argument is an array with (random) values for the optimizable parameters.
    Notice how weâ€™ve used the `qnum_inputs` and `num_weights` properties of the quantum
    neural network.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯ä¸€ä¸ªåŒ…å«ä¸€äº›ï¼ˆéšæœºï¼‰ç»å…¸è¾“å…¥çš„æ•°ç»„ï¼Œè€Œç¬¬äºŒä¸ªå‚æ•°æ˜¯ä¸€ä¸ªåŒ…å«ï¼ˆéšæœºï¼‰å¯ä¼˜åŒ–å‚æ•°å€¼çš„æ•°ç»„ã€‚æ³¨æ„æˆ‘ä»¬æ˜¯å¦‚ä½•ä½¿ç”¨é‡å­ç¥ç»ç½‘ç»œçš„`qnum_inputs`å’Œ`num_weights`å±æ€§çš„ã€‚
- en: All the neural network classes that we have presented are subclasses of a `NeuralNetwork`
    class. For example, should you want to train a neural network as a classifier,
    you could rely on Qiskitâ€™s `NeuralNetworkClassifier` class. This class can be
    initialized with a `NeuralNetwork` object and specifying a loss function and an
    optimizer among other things.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æ‰€å±•ç¤ºçš„æ‰€æœ‰ç¥ç»ç½‘ç»œç±»éƒ½æ˜¯`NeuralNetwork`ç±»çš„å­ç±»ã€‚ä¾‹å¦‚ï¼Œå¦‚æœä½ æƒ³å°†ç¥ç»ç½‘ç»œè®­ç»ƒä¸ºä¸€ä¸ªåˆ†ç±»å™¨ï¼Œä½ å¯ä»¥ä¾èµ–Qiskitçš„`NeuralNetworkClassifier`ç±»ã€‚è¿™ä¸ªç±»å¯ä»¥é€šè¿‡ä¸€ä¸ª`NeuralNetwork`å¯¹è±¡ä»¥åŠæŒ‡å®šæŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨ç­‰æ¥åˆå§‹åŒ–ã€‚
- en: In addition to this, there is a subclass of `NeuralNetworkClassifier` that can
    be used to readily create a trainable neural network classifier directly, providing
    a feature map, a variational form, an optimizer, a loss, and so on.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€ä¸ª`NeuralNetworkClassifier`çš„å­ç±»å¯ä»¥è¢«ç”¨æ¥ç›´æ¥åˆ›å»ºä¸€ä¸ªå¯è®­ç»ƒçš„ç¥ç»ç½‘ç»œåˆ†ç±»å™¨ï¼Œæä¾›ç‰¹å¾å›¾ã€å˜åˆ†å½¢å¼ã€ä¼˜åŒ–å™¨ã€æŸå¤±å‡½æ•°ç­‰ã€‚
- en: This subclass is called `VQC` (short for Variational Quantum Classifier) and
    it can also be imported from the Qiskit module `qiskit_machine_learning``.``algorithms``.``classifiers`.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªå­ç±»è¢«ç§°ä¸º`VQC`ï¼ˆä»£è¡¨å˜åˆ†é‡å­åˆ†ç±»å™¨ï¼‰ï¼Œä¹Ÿå¯ä»¥ä»Qiskitæ¨¡å—`qiskit_machine_learning``.``algorithms``.``classifiers`ä¸­å¯¼å…¥ã€‚
- en: 'If you wanted to create a neural network classifier object from our previous
    `qnn` object using the default parameters provided by Qiskit, you could run the
    following instructions:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¦ä½¿ç”¨Qiskitæä¾›çš„é»˜è®¤å‚æ•°ä»æˆ‘ä»¬ä¹‹å‰çš„`qnn`å¯¹è±¡åˆ›å»ºä¸€ä¸ªç¥ç»ç½‘ç»œåˆ†ç±»å™¨å¯¹è±¡ï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š
- en: '[PRE27]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: By default, the classifier will use the squared error loss function and rely
    on the SLSQP optimizer [[62](ch030.xhtml#Xkraft1988software)].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: é»˜è®¤æƒ…å†µä¸‹ï¼Œåˆ†ç±»å™¨å°†ä½¿ç”¨å¹³æ–¹è¯¯å·®æŸå¤±å‡½æ•°å¹¶ä¾èµ–äºSLSQPä¼˜åŒ–å™¨[[62](ch030.xhtml#Xkraft1988software)]ã€‚
- en: 'Then, if you had some training data `data_train` with labels `labels_train`,
    you could train your newly-created classifier by calling the `fit` method as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åï¼Œå¦‚æœä½ æœ‰ä¸€äº›å¸¦æœ‰æ ‡ç­¾`labels_train`çš„è®­ç»ƒæ•°æ®`data_train`ï¼Œä½ å¯ä»¥é€šè¿‡è°ƒç”¨`fit`æ–¹æ³•æ¥è®­ç»ƒä½ æ–°åˆ›å»ºçš„åˆ†ç±»å™¨ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š
- en: '[PRE28]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'If you then wanted to compute the outcomes of the trained classifier on some
    data `data_test`, you could use the `predict` method like so:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ æƒ³è¦è®¡ç®—è®­ç»ƒå¥½çš„åˆ†ç±»å™¨åœ¨æŸä¸ªæ•°æ®`data_test`ä¸Šçš„ç»“æœï¼Œä½ å¯ä»¥ä½¿ç”¨`predict`æ–¹æ³•å¦‚ä¸‹ï¼š
- en: '[PRE29]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Alternatively, if you wanted to compute the accuracy score of the trained model
    on some test dataset (`data_test` and `labels_test`), you could run the following
    instruction:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è€…ï¼Œå¦‚æœä½ æƒ³è¦è®¡ç®—è®­ç»ƒæ¨¡å‹åœ¨æŸä¸ªæµ‹è¯•æ•°æ®é›†ï¼ˆ`data_test`å’Œ`labels_test`ï¼‰ä¸Šçš„å‡†ç¡®åº¦å¾—åˆ†ï¼Œä½ å¯ä»¥è¿è¡Œä»¥ä¸‹æŒ‡ä»¤ï¼š
- en: '[PRE30]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Nevertheless, you shouldnâ€™t care too much about the `NeuralNetworkClassifier`
    and `VQC` classes because, as it turns out, there is an alternative â€” and, in
    our opinion, better â€” way to train QNNs in Qiskit. We will discuss it in the following
    chapter, and it will involve an interface with an existing machine learning framework,
    PyTorch. What is more, being able to work with this interface will allow us to
    explore Qiskitâ€™s â€Torch Runtimeâ€: a Qiskit utility that will enable us to more
    efficiently train QNNs on IBMâ€™s real quantum hardware. This is the same technique
    that we used in *Chapter* *[*5*](ch013.xhtml#x1-940005)*, QAOA:* *Quantum Approximate
    Optimization Algorithm*, to run QAOA executions on quantum hardware. Exciting,
    isnâ€™t it? Bear with us until the end of the next chapter.*'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œä½ ä¸å¿…è¿‡äºå…³æ³¨`NeuralNetworkClassifier`å’Œ`VQC`ç±»ï¼Œå› ä¸ºæ­£å¦‚æˆ‘ä»¬æ‰€å‘ç°çš„é‚£æ ·ï¼Œæœ‰ä¸€ä¸ªæ›¿ä»£æ–¹æ¡ˆâ€”â€”å¹¶ä¸”ï¼Œåœ¨æˆ‘ä»¬çœ‹æ¥ï¼Œè¿™æ˜¯ä¸€ä¸ªæ›´å¥½çš„æ–¹æ³•æ¥åœ¨Qiskitä¸­è®­ç»ƒQNNsã€‚æˆ‘ä»¬å°†åœ¨ä¸‹ä¸€ç« ä¸­è®¨è®ºå®ƒï¼Œå®ƒå°†æ¶‰åŠä¸ç°æœ‰æœºå™¨å­¦ä¹ æ¡†æ¶PyTorchçš„æ¥å£ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œèƒ½å¤Ÿä½¿ç”¨è¿™ä¸ªæ¥å£å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ¢ç´¢Qiskitçš„â€œTorchè¿è¡Œæ—¶â€ï¼šè¿™æ˜¯ä¸€ä¸ªQiskitå®ç”¨å·¥å…·ï¼Œå°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´æœ‰æ•ˆåœ°åœ¨IBMçš„çœŸå®é‡å­ç¡¬ä»¶ä¸Šè®­ç»ƒQNNsã€‚è¿™æ­£æ˜¯æˆ‘ä»¬åœ¨*ç¬¬*
    *[*5*](ch013.xhtml#x1-940005)* *ç« *ï¼ŒQAOA:* *é‡å­è¿‘ä¼¼ä¼˜åŒ–ç®—æ³•*ä¸­ä½¿ç”¨çš„ç›¸åŒæŠ€æœ¯ï¼Œç”¨äºåœ¨é‡å­ç¡¬ä»¶ä¸Šè¿è¡ŒQAOAæ‰§è¡Œã€‚ä»¤äººå…´å¥‹ï¼Œä¸æ˜¯å—ï¼Ÿè¯·è€å¿ƒç­‰å¾…ä¸‹ä¸€ç« çš„ç»“å°¾ã€‚
- en: '*# Summary'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '*# æ‘˜è¦'
- en: This has been a long journey, hasnâ€™t it? In this chapter, we first introduced
    quantum neural networks as quantum analogs of classical neural networks. We have
    seen how the training of a quantum neural network is very similar to that of a
    classical one, and weâ€™ve also explored the differentiation methods that make this
    possible.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å·²ç»æ˜¯ä¸€æ®µæ¼«é•¿çš„æ—…ç¨‹äº†ï¼Œä¸æ˜¯å—ï¼Ÿåœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬é¦–å…ˆä»‹ç»äº†é‡å­ç¥ç»ç½‘ç»œä½œä¸ºç»å…¸ç¥ç»ç½‘ç»œçš„é‡å­æ¨¡æ‹Ÿã€‚æˆ‘ä»¬çœ‹åˆ°äº†é‡å­ç¥ç»ç½‘ç»œçš„è®­ç»ƒè¿‡ç¨‹ä¸ç»å…¸ç¥ç»ç½‘ç»œéå¸¸ç›¸ä¼¼ï¼Œæˆ‘ä»¬è¿˜æ¢è®¨äº†ä½¿è¿™æˆä¸ºå¯èƒ½çš„ä¸åŒåŒ–æ–¹æ³•ã€‚
- en: With the theory out of the way, we got our keyboards ready to do some work.
    We learned how to implement and train a quantum neural network using PennyLane,
    and we also discussed some technicalities about this framework, such as details
    about the differentiation methods that it provides.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ç†è®ºéƒ¨åˆ†ç»“æŸåï¼Œæˆ‘ä»¬å‡†å¤‡å¥½äº†é”®ç›˜å¼€å§‹å·¥ä½œã€‚æˆ‘ä»¬å­¦ä¹ äº†å¦‚ä½•ä½¿ç”¨PennyLaneå®ç°å’Œè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬è¿˜è®¨è®ºäº†å…³äºè¿™ä¸ªæ¡†æ¶çš„ä¸€äº›æŠ€æœ¯ç»†èŠ‚ï¼Œä¾‹å¦‚å®ƒæä¾›çš„ä¸åŒåŒ–æ–¹æ³•ã€‚
- en: PennyLane comes with some wonderful simulators, but â€” as we already mentioned
    in *Chapter* [*2*](ch009.xhtml#x1-400002)*, The Tools of the Trade in Quantum
    Computing* â€” itâ€™s also integrated with quantum hardware platforms such as Amazon
    Braket and IBM Quantum. Thus, your ability to train quantum neural networks on
    actual quantum computers is at your fingertips!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: PennyLaneé™„å¸¦ä¸€äº›å‡ºè‰²çš„æ¨¡æ‹Ÿå™¨ï¼Œä½†â€”â€”æ­£å¦‚æˆ‘ä»¬å·²ç»åœ¨*ç¬¬* *[*2*](ch009.xhtml#x1-400002)* *ç« *ï¼Œé‡å­è®¡ç®—çš„å·¥å…·ä¸­æ‰€æåˆ°çš„â€”â€”å®ƒè¿˜ä¸é‡å­ç¡¬ä»¶å¹³å°ï¼ˆå¦‚Amazon
    Braketå’ŒIBM Quantumï¼‰é›†æˆã€‚å› æ­¤ï¼Œä½ èƒ½å¤Ÿåœ¨å®é™…é‡å­è®¡ç®—æœºä¸Šè®­ç»ƒé‡å­ç¥ç»ç½‘ç»œçš„èƒ½åŠ›å°±åœ¨ä½ çš„æŒ‡å°–ï¼
- en: We concluded the chapter with a short overview of how to work with quantum neural
    networks in Qiskit.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æœ¬ç« çš„ç»“å°¾ç®€è¦æ¦‚è¿°äº†å¦‚ä½•åœ¨Qiskitä¸­å¤„ç†é‡å­ç¥ç»ç½‘ç»œã€‚
- en: By now, you have a solid understanding of quantum neural networks. Combined
    with your previous knowledge of quantum support vector machines, this gives you
    a fairly solid foundation in quantum machine learning. In the following chapter
    â€” which will be very practically-oriented â€” we will explore more complex model
    architectures based on quantum neural networks.*******
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œä½ å·²ç»å¯¹é‡å­ç¥ç»ç½‘ç»œæœ‰äº†æ‰å®çš„ç†è§£ã€‚ç»“åˆä½ ä¹‹å‰å¯¹é‡å­æ”¯æŒå‘é‡æœºçš„çŸ¥è¯†ï¼Œè¿™ä¸ºä½ æ‰“ä¸‹äº†é‡å­æœºå™¨å­¦ä¹ ç›¸å½“åšå®çš„åŸºç¡€ã€‚åœ¨æ¥ä¸‹æ¥çš„ç« èŠ‚â€”â€”å®ƒå°†éå¸¸æ³¨é‡å®è·µâ€”â€”æˆ‘ä»¬å°†æ¢è®¨åŸºäºé‡å­ç¥ç»ç½‘ç»œçš„æ›´å¤æ‚æ¨¡å‹æ¶æ„ã€‚******
