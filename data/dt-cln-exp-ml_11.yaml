- en: '*Chapter 8*: Support Vector Regression'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第8章*：支持向量回归'
- en: '**Support vector regression** (**SVR**) can be an excellent option when the
    assumptions of linear regression models do not hold, such as when the relationship
    between our features and our target is too complicated to be described by a linear
    combination of weights. Even better, SVR allows us to model that complexity without
    having to expand the feature space.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量回归**（**SVR**）在线性回归模型的假设不成立时可以是一个极佳的选择，例如当我们的特征与目标之间的关系过于复杂，无法用权重的线性组合来描述时。甚至更好，SVR允许我们无需扩展特征空间来建模这种复杂性。'
- en: Support vector machines identify the hyperplane that maximizes the margin between
    two classes. The support vectors are the data points closest to the margin that
    *support* it, if you will. This turns out to be as useful for regression modeling
    as it is for classification. SVR finds the hyperplane containing the greatest
    number of data points. We will discuss how that works in the first section of
    this chapter.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机识别出最大化两个类别之间边界的超平面。支持向量是距离边界最近的数据点，它们*支持*这个边界，如果可以这样表达的话。这证明它在回归建模中与在分类中一样有用。SVR找到包含最多数据点的超平面。我们将在本章的第一节中讨论它是如何工作的。
- en: Rather than minimizing the sum of the squared residuals, as ordinary least squares
    regression does, SVR minimizes the coefficients within an acceptable error range.
    Like ridge and lasso regression, this can reduce model variance and the risk of
    overfitting. SVR works best when we are working with a small- to medium-sized
    dataset.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 与普通最小二乘回归不同，SVR不是最小化平方残差的和，而是在一个可接受的误差范围内最小化系数。像岭回归和Lasso回归一样，这可以减少模型方差和过拟合的风险。SVR在处理中小型数据集时效果最佳。
- en: The algorithm is also quite flexible, allowing us to specify the acceptable
    error range, use kernels to model nonlinear relationships, and adjust hyperparameters
    to get the best bias-variance tradeoff possible. We will demonstrate that in this
    chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法也非常灵活，允许我们指定可接受的误差范围，使用核来建模非线性关系，并调整超参数以获得最佳的偏差-方差权衡。我们将在本章中展示这一点。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Key concepts of SVR
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVR的关键概念
- en: SVR with a linear model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于线性模型的支持向量回归
- en: Using kernels for nonlinear SVR
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用核进行非线性SVR
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: In this chapter, we will be working with the scikit-learn and `matplotlib` libraries.
    You can use `pip` to install these packages.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用scikit-learn和`matplotlib`库。您可以使用`pip`安装这些包。
- en: Key concepts of SVR
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SVR的关键概念
- en: We will start this section by discussing how support vector machines are used
    for classification. We will not go into much detail here, leaving a detailed discussion
    of support vector classification to [*Chapter 13*](B17978_13_ePub.xhtml#_idTextAnchor152),
    *Support Vector Machine Classification*. But starting with support vector machines
    for classification will lead nicely to an explanation of SVR.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个部分开始讨论支持向量机在分类中的应用。在这里我们不会深入细节，将支持向量分类的详细讨论留给[*第13章*](B17978_13_ePub.xhtml#_idTextAnchor152)，*支持向量机分类*。但首先从支持向量机在分类中的应用开始，将很好地过渡到SVR的解释。
- en: 'As I discussed at the beginning of this chapter, support vector machines find
    the hyperplane that maximizes the margin between classes. When there are only
    two features present, that hyperplane is just a line. Consider the following example
    plot:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本章开头讨论的那样，支持向量机找到最大化类别之间边界的超平面。当只有两个特征存在时，这个超平面仅仅是一条线。考虑以下示例图：
- en: '![Figure 8.1 – Support vector machine classification based on two features
    ](img/B17978_08_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1 – 基于两个特征的支持向量机分类](img/B17978_08_001.jpg)'
- en: Figure 8.1 – Support vector machine classification based on two features
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 – 基于两个特征的支持向量机分类
- en: The two classes in this diagram, represented by red circles and blue squares,
    are **linearly separable** using the two features, x1 and x2\. The bold line is
    the decision boundary. It is the line that is furthest away from border data points
    for each class, or the maximum margin. These points are known as the support vectors.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的两个类别，用红色圆圈和蓝色正方形表示，可以使用两个特征，x1和x2，进行**线性可分**。粗体线是决策边界。它是每个类别离边界数据点最远的线，或者说是最大边界。这些点被称为支持向量。
- en: Since the data in the preceding plot is linearly separable, we can use what
    is known as **hard margin classification** without problems; that is, we can be
    strict about all the observations for each class being on the correct side of
    the decision boundary. But what if our data points look like what’s shown in the
    following plot?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于前一个图中的数据是线性可分的，我们可以无问题地使用所谓的**硬间隔分类**；也就是说，我们可以对每个类别的所有观测值位于决策边界正确一侧的要求非常严格。但如果我们数据点的样子像下面这个图所示的呢？
- en: '![Figure 8.2 – Support vector machine classification with soft margins ](img/B17978_08_002.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2 – 具有软间隔的支持向量机分类](img/B17978_08_002.jpg)'
- en: Figure 8.2 – Support vector machine classification with soft margins
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 – 具有软间隔的支持向量机分类
- en: These data points are not linearly separable. In this case, we can choose **soft
    margin classification** and ignore the outlier red circles.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这些数据点不是线性可分的。在这种情况下，我们可以选择**软间隔分类**并忽略异常值红色圆圈。
- en: We will discuss support vector classification in much greater detail in [*Chapter
    13*](B17978_13_ePub.xhtml#_idTextAnchor152), *Support Vector Machine Classification*,
    but this illustrates some of the key support vector machine concepts. These concepts
    can be applied well to models involving a continuous target. This is called **support
    vector regression** or **SVR**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在[*第 13 章*](B17978_13_ePub.xhtml#_idTextAnchor152)“支持向量机分类”中更详细地讨论支持向量分类，但这也说明了支持向量机的一些关键概念。这些概念可以很好地应用于涉及连续目标值的模型。这被称为**支持向量回归**或**SVR**。
- en: 'When building an SVR model, we decide on the acceptable amount of prediction
    error, ɛ. Errors within ɛ of our prediction, ![](img/B17978_08_001.png), in a
    one-feature model are not penalized. This is sometimes referred to as the epsilon-insensitive
    tube. SVR minimizes the coefficients consistent with all data points falling within
    that range. This is illustrated in the following plot:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建 SVR 模型时，我们决定可接受的预测误差量，ɛ。在一个特征模型中，预测值 ![](img/B17978_08_001.png) 在 ɛ 范围内的误差不会被惩罚。这有时被称为
    epsilon-insensitive tube。SVR 最小化系数，使得所有数据点都落在该范围内。这在下图中得到说明：
- en: '![Figure 8.3 – SVR with an acceptable error range ](img/B17978_08_003.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3 – 具有可接受误差范围的 SVR](img/B17978_08_003.jpg)'
- en: Figure 8.3 – SVR with an acceptable error range
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 – 具有可接受误差范围的 SVR
- en: Stated more precisely, SVR minimizes the square of the coefficients, subject
    to the constraint that the error, ε, does not exceed a given amount.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 更精确地说，SVR 在满足误差 ε 不超过给定量的约束条件下，最小化系数的平方。
- en: It minimizes ![](img/B17978_08_002.png) with the constraint that ![](img/B17978_08_003.png),
    where ![](img/B17978_08_004.png) is a vector of weights (or coefficients), ![](img/B17978_08_005.png)
    is the actual target value minus the predicted value, and ![](img/B17978_08_006.png)
    is the acceptable amount of error.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 它在满足 ![](img/B17978_08_003.png) 的约束条件下最小化 ![](img/B17978_08_002.png)，其中 ![](img/B17978_08_004.png)
    是权重（或系数）向量，![](img/B17978_08_005.png) 是实际目标值与预测值的差，![](img/B17978_08_006.png)
    是可接受的误差量。
- en: Of course, it is not reasonable to expect all the data points to fall within
    the desired range. But we can still seek to minimize that deviation. Let’s denote
    the distance of the wayward points from the margin as ξ. This gives us a new objective
    function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，期望所有数据点都落在期望范围内是不合理的。但我们仍然可以寻求最小化这种偏差。让我们用 ξ 表示偏离边缘的距离。这给我们一个新的目标函数。
- en: 'We minimize ![](img/B17978_08_007.png) with the constraint that ![](img/B17978_08_008.png),
    where *C* is a hyperparameter indicating how tolerant the model should be of errors
    outside the margin. A value of 0 for *C* means that it is not at all tolerant
    of those large errors. This is equivalent to the original objective function:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在满足 ![](img/B17978_08_008.png) 的约束条件下最小化 ![](img/B17978_08_007.png)，其中 *C*
    是一个超参数，表示模型对边缘外误差的容忍度。*C* 的值为 0 表示它根本不容忍那些大误差。这等价于原始目标函数：
- en: '![Figure 8.4 – SVR with data points outside the acceptable range ](img/B17978_08_004.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.4 – 具有超出可接受范围的点的 SVR](img/B17978_08_004.jpg)'
- en: Figure 8.4 – SVR with data points outside the acceptable range
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 – 具有超出可接受范围的点的 SVR
- en: Here, we can see several advantages of SVR. It is sometimes more important that
    our errors will not exceed a certain amount, than picking a model with the lowest
    absolute error. It may matter more if we are often off by a little but rarely
    by a lot than if we are often spot on but occasionally way off. Since this approach
    also minimizes our weights, it has the same advantages as regularization, and
    we reduce the likelihood of overfitting.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们可以看到SVR的几个优点。有时，我们的误差不超过一定量比选择一个具有最低绝对误差的模型更重要。如果我们经常略微偏离但很少大幅偏离，可能比我们经常准确但偶尔严重偏离更重要。由于这种方法也最小化了我们的权重，它具有与正则化相同的优点，我们减少了过拟合的可能性。
- en: Nonlinear SVR and the kernel trick
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性SVR和核技巧
- en: 'We have not yet fully addressed the issue of linear separability with SVR.
    For simplicity, we will return to a classification problem involving two features.
    Let’s look at a plot of two features against a categorical target. The target
    has two possible values, represented by the dots and squares. x1 and x2 are numeric
    and have negative values:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们尚未完全解决SVR中线性可分性的问题。为了简单起见，我们将回到一个涉及两个特征的分类问题。让我们看看两个特征与分类目标的关系图。目标有两个可能的值，由点和正方形表示。x1和x2是数值，具有负值：
- en: '![Figure 8.5 – Class labels not linearly separable with two features ](img/B17978_08_005.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5 – 使用两个特征无法线性分离的类别标签](img/B17978_08_005.jpg)'
- en: Figure 8.5 – Class labels not linearly separable with two features
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 – 使用两个特征无法线性分离的类别标签
- en: 'What can we do in a case like this to identify a margin between the classes?
    It is often the case that a margin can be identified at a higher dimension. In
    this example, we can use a polynomial transformation, as illustrated in the following
    plot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们该如何识别类别之间的边界呢？通常情况下，在更高的维度上可以识别出边界。在这个例子中，我们可以使用多项式变换，如下面的图表所示：
- en: '![Figure 8.6 – Using polynomial transformation to establish the margin ](img/B17978_08_006.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6 – 使用多项式变换建立边界](img/B17978_08_006.jpg)'
- en: Figure 8.6 – Using polynomial transformation to establish the margin
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 – 使用多项式变换建立边界
- en: There is now a third dimension, which is the sum of the squares of x1 and x2\.
    The dots are all higher than the squares. This is similar to how we used polynomial
    transformation in the previous chapter to specify a nonlinear regression model.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有一个第三维度，它是x1和x2平方的和。所有的点都高于平方。这与我们在上一章中使用多项式变换来指定非线性回归模型的方式相似。
- en: One drawback of this approach is that we can quickly end up with too many features
    for our model to perform well. This is where the **kernel trick** comes in very
    handy. SVR can use a kernel function to expand the feature space implicitly without
    actually creating more features. This is done by creating a vector of values that
    can be used to fit a nonlinear margin.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点之一是我们可能会迅速拥有太多特征，导致模型无法良好地表现。这时，**核技巧**就非常实用了。SVR可以通过使用核函数隐式地扩展特征空间，而不实际创建更多特征。这是通过创建一个值向量来完成的，这些值可以用来拟合非线性边界。
- en: While this allows us to fit a polynomial transformation such as the hypothetical
    one illustrated in the preceding plot, the most frequently used kernel function
    with SVR is the **radial basis function** (**RBF**). RBF is popular because it
    is faster than the other common kernel functions and because its gamma parameter
    makes it very flexible. We will explore how to use it in the last section of this
    chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这允许我们拟合如前述图表中所示的一个假设的多项式变换，但SVR中最常用的核函数是**径向基函数**（**RBF**）。RBF之所以受欢迎，是因为它比其他常见的核函数更快，并且因为它的伽马参数使其非常灵活。我们将在本章的最后部分探讨如何使用它。
- en: But for now, let’s start with a relatively straightforward linear model to see
    SVR in action.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在，让我们从一个相对简单的线性模型开始，看看SVR的实际应用。
- en: SVR with a linear model
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性模型的SVR
- en: We often have enough domain knowledge to take an approach that is more nuanced
    than simply minimizing prediction errors in our training data. Using this knowledge
    may allow us to accept more bias in our model, when small amounts of bias do not
    matter much substantively, to reduce variance. With SVR, we can adjust hyperparameters
    such as epsilon (the acceptable error range) and *C* (which adjusts the tolerance
    for errors outside of that range) to improve our model’s performance.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常有足够的领域知识，可以采取比仅仅最小化训练数据中的预测误差更细致的方法。利用这些知识，我们可能允许模型接受更多的偏差，当少量的偏差在实质上并不重要时，以减少方差。在使用SVR时，我们可以调整超参数，如epsilon（可接受的误差范围）和*C*（调整该范围之外错误的容忍度），以改善模型的表现。
- en: 'If a linear model can perform well on your data, linear SVR might be a good
    choice. We can build a linear SVR model with scikit-learn’s `LinearSVR` class.
    Let’s try creating a linear SVR model with the gasoline tax data that we used
    in the previous chapter:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果线性模型可以在你的数据上表现良好，线性SVR可能是一个不错的选择。我们可以使用scikit-learn的`LinearSVR`类构建一个线性SVR模型。让我们尝试使用我们在上一章中使用的汽油税数据创建一个线性SVR模型：
- en: 'We need many of the same libraries that we used in the previous chapter to
    create the training and testing DataFrames and to preprocess the data. We also
    need to import the `LinearSVR` and `uniform` modules from scikit-learn and scipy,
    respectively:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用与上一章相同的许多库来创建训练和测试DataFrame，并预处理数据。我们还需要从scikit-learn和scipy中分别导入`LinearSVR`和`uniform`模块：
- en: '[PRE0]'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We also need to import the `OutlierTrans` class, which we first discussed in
    [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091), *Linear Regression Models*,
    to handle outliers:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要导入`OutlierTrans`类，这是我们首先在第7章中讨论的，*线性回归模型*，用于处理异常值：
- en: '[PRE1]'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Next, we load the gasoline tax data and create training and testing DataFrames.
    We create lists for numerical and binary features, as well as a separate list
    for `motorization_rate`. As we saw when we looked at the data in the previous
    chapter, we need to do a little more preprocessing with `motorization_rate`.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们加载汽油税数据并创建训练和测试DataFrame。我们创建了数值和二进制特征的列表，以及一个单独的`motorization_rate`列表。正如我们在上一章查看数据时所见，我们需要对`motorization_rate`进行一些额外的预处理。
- en: 'This dataset contains gasoline tax data for each country in 2014, as well as
    fuel income dependence and measures of the strength of the democratic institutions:
    `polity`, `democracy_polity`, and `autocracy_polity`. `democracy_polity` is a
    binarized `polity` variable, taking on a value of 1 for countries with high `polity`
    scores. `autocracy_polity` has a value of 1 for countries with low `polity` scores.
    The `polity` feature is a measure of how democratic a country is:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集包含了2014年每个国家的汽油税数据，以及燃料收入依赖性和衡量民主制度强度的指标：`polity`、`democracy_polity`和`autocracy_polity`。`democracy_polity`是一个二进制`polity`变量，对于`polity`得分高的国家取值为1。`autocracy_polity`对于`polity`得分低的国家取值为1。`polity`特征是衡量一个国家民主程度的一个指标：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let’s look at summary statistics for the training data. We will need to standardize
    the data since there are dramatically different ranges and SVR performs much better
    on standardized data. Also, notice that `motorization_rate` has a lot of missing
    values. We may want to do better than simple imputation with that feature. We
    have decent non-missing counts for the dummy columns:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看训练数据的摘要统计。我们需要标准化数据，因为数据范围差异很大，SVR在标准化数据上表现更好。注意，`motorization_rate`有很多缺失值。我们可能想在这个特征上做得比简单的插补更好。对于虚拟列，我们有相当多的非缺失计数：
- en: '[PRE3]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We need to build a column transformer to handle different data types. We can
    use `SimpleImputer` for the categorical features and numerical features, except
    for `motorization_rate`. We will use KNN imputation for the `motorization_rate`
    feature later:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要构建一个列转换器来处理不同的数据类型。我们可以使用`SimpleImputer`来处理分类特征和数值特征，除了`motorization_rate`。我们将在稍后使用KNN插补来处理`motorization_rate`特征：
- en: '[PRE4]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now, we are ready to fit our linear SVR model. We will choose a value of `0.2`
    for `epsilon`. This means that we are fine with any error within 0.2 standard
    deviations of the actual value (we use `TransformedTargetRegressor` to standardize
    the target). We will leave *C* – the hyperparameter determining our model’s tolerance
    for values outside of epsilon – at its default value of 1.0.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好拟合我们的线性SVR模型。我们将为`epsilon`选择`0.2`的值。这意味着我们对实际值0.2标准差范围内的任何误差都感到满意（我们使用`TransformedTargetRegressor`来标准化目标）。我们将把*C*——决定模型对epsilon之外值容忍度的超参数——保留在其默认值1.0。
- en: Before we fit our model, we still need to handle missing values for `motorization_rate`.
    We will add the KNN imputer to a pipeline after the column transformations. Since
    `motorization_rate` will be the only feature with missing values after the column
    transformations, the KNN imputer only changes values for that feature.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们拟合模型之前，我们仍然需要处理`motorization_rate`的缺失值。我们将在列转换之后将KNN填充器添加到管道中。由于`motorization_rate`将是列转换后唯一的具有缺失值的特征，KNN填充器只会改变该特征的价值。
- en: We need to use the target transformer because the column transformer will only
    change the features, not the target. We will pass the pipeline we just created
    to the target transformer’s `regressor` parameter to do the feature transformations,
    and indicate that we just want to do standard scaling for the target.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要使用目标转换器，因为列转换器只会改变特征，而不会改变目标。我们将把刚刚创建的管道传递给目标转换器的`regressor`参数以进行特征转换，并指出我们只想对目标进行标准缩放。
- en: 'Note that the default loss function for linear SVR is L1, but we could have
    chosen L2 instead:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，线性SVR的默认损失函数是L1，但我们可以选择L2：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can use `ttr.regressor_` to access all the elements of the pipeline, including
    the `linearsvr` object. This is how we get to the `coef_` attribute. The coefficients
    that are substantially different from 0 are `VAT_Rate` and the autocracy and national
    oil company dummies. Our model estimates a positive relationship between value-added
    tax rates and gasoline taxes, all else being equal. It estimates a negative relationship
    between having an autocracy or having a national oil company, and gasoline taxes:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`ttr.regressor_`来访问管道中的所有元素，包括`linearsvr`对象。这就是我们如何获得`coef_`属性。与0显著不同的系数是`VAT_Rate`以及专制和国家级石油公司虚拟变量。我们的模型估计，在其他条件相同的情况下，增值税率和汽油税之间存在正相关关系。它估计拥有专制或国家级石油公司与汽油税之间存在负相关关系：
- en: '[PRE6]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Notice that we have not done any feature selection here. Instead, we are relying
    on the L1 regularization to push feature coefficients to near 0\. If we had many
    more features, or we were more concerned about computation time, it would be important
    to think about our feature selection strategy more carefully.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里没有进行任何特征选择。相反，我们依赖于L1正则化将特征系数推向接近0。如果我们有更多的特征，或者我们更关心计算时间，那么仔细思考我们的特征选择策略就很重要了。
- en: 'Let’s do some cross-validation on this model. The mean absolute error and r-squared
    are not great, though that is certainly impacted by the small sample size:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们在该模型上进行一些交叉验证。平均绝对误差和r-squared值并不理想，这当然受到了小样本量的影响：
- en: '[PRE7]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We have not done any hyperparameter tuning yet. We do not know if our values
    for `epsilon` and *C* are the best ones for our model. Therefore, we need to do
    a grid search to experiment with different hyperparameter values. We will start
    with an exhaustive grid search, which often is not practical ( I recommend not
    running the next few steps on your machine unless you have a fairly high-performing
    one). After the exhaustive grid search, we will do a randomized grid search, which
    is usually substantially easier on system resources.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有进行任何超参数调整。我们不知道我们的`epsilon`和*C*的值是否是模型的最佳值。因此，我们需要进行网格搜索来尝试不同的超参数值。我们将从穷举网格搜索开始，这通常不切实际（除非你有一个性能相当高的机器，否则我建议不要运行接下来的几个步骤）。在穷举网格搜索之后，我们将进行随机网格搜索，这通常对系统资源的影响要小得多。
- en: We will start by creating a `LinearSVR` object without the `epsilon` hyperparameter
    specified, and we will recreate the pipeline. Then, we will create a dictionary,
    `svr_params`, with values to check for `epsilon` and *C*, called `regressor_linearsvr_epsilon`
    and `regressor_linearsvr_C`, respectively.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先创建一个没有指定`epsilon`超参数的`LinearSVR`对象，并重新创建管道。然后，我们将创建一个字典`svr_params`，其中包含用于检查`epsilon`和*C*的值，分别称为`regressor_linearsvr_epsilon`和`regressor_linearsvr_C`。
- en: Remember from our grid search from the previous chapter that the names of the
    keys must correspond with our pipeline steps. Our pipeline, which in this case
    can be accessed as the transformed target’s `regressor` object, has a `linearsvr`
    object with attributes for `epsilon` and *C*.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 记住我们从前一章的网格搜索中提到的，键的名称必须与我们的管道步骤相对应。在这个例子中，我们可以通过转换目标对象的`regressor`属性来访问我们的管道。管道中有一个`linearsvr`对象，具有`epsilon`和*C*的属性。
- en: We will pass the `svr_params` dictionary to a `GridSearchCV` object and indicate
    that we want the scoring to be based on r-squared (if we wanted to base scoring
    on the mean absolute error, we could have also done that).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`svr_params`字典传递给`GridSearchCV`对象，并指示我们希望评分基于r-squared（如果我们想基于平均绝对误差进行评分，我们也可以这样做）。
- en: 'Then, we will run the `fit` method of the grid search object. I should repeat
    the warning I mentioned previously that you may not want to run an exhaustive
    grid search unless you are using a high-performing machine, or you do not mind
    letting it run while you go get a cup of coffee. Note that it takes about 26 seconds
    to run each time on my machine:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将运行网格搜索对象的`fit`方法。我应该重复之前提到的警告，你可能不希望运行穷举网格搜索，除非你使用的是高性能机器，或者你不在乎在去拿一杯咖啡的时候让它运行。请注意，在我的机器上每次运行大约需要26秒：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now, we can use the `best_params_` attribute of the grid search to get the
    hyperparameters associated with the highest score. We can see the score with those
    parameters with the `best_scores_` attribute. This tells us that we get the highest
    r-squared, which is 0.6, with a *C* of 0.1 and an `epsilon` value of 0.2:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以使用网格搜索的`best_params_`属性来获取与最高分数相关的超参数。我们可以通过`best_scores_`属性查看这些参数的分数。这告诉我们，我们以0.1的*C*值和0.2的`epsilon`值获得了最高的r-squared值，即0.6：
- en: '[PRE9]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: It is good to know which values to choose for our hyperparameters. However,
    the exhaustive grid search was quite expensive computationally. Let’s try a randomized
    search instead.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 了解为我们的超参数选择哪些值是很好的。然而，穷举网格搜索在计算上相当昂贵。让我们尝试随机搜索。
- en: 'We will indicate that the random values for both `epsilon` and *C* should come
    from a uniform distribution with values between 0 and 1.5\. Then, we will pass
    that dictionary to a `RandomizedSearchCV` object. This runs substantially faster
    than the exhaustive grid search – a little over 1 second per iteration. This gives
    us higher `epsilon` and *C* values than the exhaustive grid search – that is,
    0.23 and 0.7, respectively. The r-squared value is a little lower, however:'
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将指示`epsilon`和*C*的随机值应来自介于0和1.5之间的均匀分布。然后，我们将该字典传递给`RandomizedSearchCV`对象。这比穷举网格搜索快得多——每次迭代略超过1秒。这给我们提供了比穷举网格搜索更高的`epsilon`和*C*值——即，分别为0.23和0.7。然而，r-squared值略低：
- en: '[PRE10]'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s look at the predictions based on the best model from the randomized grid
    search. The randomized grid search object’s `predict` method can generate those
    predictions for us:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们查看基于随机网格搜索最佳模型的预测。随机网格搜索对象的`predict`方法可以为我们生成这些预测：
- en: '[PRE11]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, let’s look at the distribution of our residuals:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看看我们残差的分布：
- en: '[PRE12]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This produces the following plot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 8.7 – Residual distribution for the gasoline tax linear SVR model
    ](img/B17978_08_007.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图8.7 – 加油税线性SVR模型的残差分布](img/B17978_08_007.jpg)'
- en: Figure 8.7 – Residual distribution for the gasoline tax linear SVR model
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 – 加油税线性SVR模型的残差分布
- en: Here, there is a little bit of bias (some overpredicting overall) and some positive
    skew.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，存在一点偏差（整体上有些过度预测）和一些正偏斜。
- en: 'Let’s also view a scatterplot of the predicted values against the residuals:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再查看一下预测值与残差之间的散点图：
- en: '[PRE13]'
  id: totrans-87
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This produces the following plot:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 8.8 – Scatterplot of predictions and residuals for the gasoline tax
    linear SVR model ](img/B17978_08_008.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图8.8 – 加油税线性SVR模型的预测值与残差的散点图](img/B17978_08_008.jpg)'
- en: Figure 8.8 – Scatterplot of predictions and residuals for the gasoline tax linear
    SVR model
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 – 加油税线性SVR模型的预测值与残差的散点图
- en: These residuals are problematic. We are always overpredicting (predicted values
    are higher than actual values) at the lower and upper range of the predicted values.
    This is not what we want and is perhaps warning us of an unaccounted-for nonlinear
    relationship.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这些残差是有问题的。我们在预测值的低和高范围内总是过度预测（预测值高于实际值）。这不是我们想要的，也许在提醒我们存在未考虑的非线性关系。
- en: When our data is linearly separable, linear SVR can be an efficient choice.
    It can be used in many of the same situations where we would have used linear
    regression or linear regression with regularization. Its relative efficiency means
    we are not as concerned about using it with datasets that contain more than 10,000
    observations as we are with nonlinear SVR. However, when linear separability is
    not possible, we should explore nonlinear models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的数据是线性可分时，线性支持向量回归（SVR）可以是一个高效的选择。它可以用在许多我们本会使用线性回归或带有正则化的线性回归的相同情况下。它的相对效率意味着我们对于使用包含超过10,000个观测值的数据集并不像使用非线性SVR那样担忧。然而，当线性可分性不可能时，我们应该探索非线性模型。
- en: Using kernels for nonlinear SVR
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用核函数进行非线性SVR
- en: Recall from our discussion at the beginning of this chapter that we can use
    a kernel function to fit a nonlinear epsilon-insensitive tube. In this section,
    we will run a nonlinear SVR with the land temperatures data that we worked with
    in the previous chapter. But first, we will construct a linear SVR with the same
    data for comparison.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下本章开头我们讨论的内容，我们可以使用核函数来拟合一个非线性ε敏感管。在本节中，我们将使用我们在上一章中使用过的土地温度数据运行非线性SVR。但首先，我们将使用相同的数据构建一个线性SVR以进行比较。
- en: 'We will model the average temperature for weather stations as a function of
    latitude and elevation. Follow these steps:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把气象站的平均温度建模为纬度和海拔的函数。按照以下步骤进行：
- en: 'We will begin by loading the familiar libraries. The only new class is `SVR`
    from scikit-learn:'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载熟悉的库。唯一的新类是来自scikit-learn的`SVR`：
- en: '[PRE14]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Next, we will load the land temperatures data and create training and testing
    DataFrames. We will also take a look at some descriptive statistics. There are
    several missing values for elevation and the ranges of the two features are very
    different. There are also some exceedingly low average temperatures:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将加载土地温度数据并创建训练和测试数据框。我们还将查看一些描述性统计。海拔高度有多个缺失值，两个特征的范围差异很大。还有一些异常低的平均温度：
- en: '[PRE15]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Let’s start with a linear SVR model of average temperatures. We can be fairly
    conservative with how we handle the outliers, only setting them to missing when
    the interquartile range is more than three times above or below the interquartile
    range. (We created the `OutlierTrans` class in [*Chapter 7*](B17978_07_ePub.xhtml#_idTextAnchor091),
    *Linear Regression Models*.) We will use KNN imputation for the missing elevation
    values and scale the data. Remember that we need to use the target transformer
    to scale the target variable.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从平均温度的线性SVR模型开始。我们可以对处理异常值保持相当保守，只有当四分位数范围超过三倍时，才将它们设置为缺失值。（我们在[*第7章*](B17978_07_ePub.xhtml#_idTextAnchor091)，*线性回归模型*）中创建了`OutlierTrans`类。）我们将使用KNN插补缺失的海拔值并缩放数据。记住，我们需要使用目标转换器来缩放目标变量。
- en: Just as we did in the previous section, we will use a dictionary, `svr_params`,
    to indicate that we want to sample values from a uniform distribution for our
    hyperparameters – that is, `epsilon` and *C*. We will pass this dictionary to
    the `RandomizedSearchCV` object.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中所做的那样，我们将使用一个字典，`svr_params`，来表示我们想要从均匀分布中采样超参数的值——即`epsilon`和*C*。我们将把这个字典传递给`RandomizedSearchCV`对象。
- en: 'After running `fit`, we can get the best parameters for `epsilon` and *C*,
    and the mean absolute error for the best model. The mean absolute error is fairly
    decent at about 2.8 degrees:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行`fit`之后，我们可以得到`epsilon`和*C*的最佳参数，以及最佳模型的平均绝对误差。平均绝对误差相当不错，大约为2.8度：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s look at the predictions:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们看看预测结果：
- en: '[PRE17]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This produces the following plot:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 8.9 – Scatterplot of predictions and residuals for the land temperatures
    linear SVR model ](img/B17978_08_009.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图8.9 – 土地温度线性SVR模型的预测值和残差散点图](img/B17978_08_009.jpg)'
- en: Figure 8.9 – Scatterplot of predictions and residuals for the land temperatures
    linear SVR model
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 – 土地温度线性SVR模型的预测值和残差散点图
- en: There is a good amount of overpredicting at the upper range of the predicted
    values. We typically underpredict values just below that, between predicted gas
    tax values from 15 to 25 degrees. Perhaps we can improve the fit with a nonlinear
    model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测值的上限范围内有相当多的过度预测。我们通常在预测的汽油税值15到25度之间低估值。也许我们可以通过非线性模型来提高拟合度。
- en: 'We do not have to change much to run a nonlinear SVR. We just need to create
    an `SVR` object and choose a kernel function. `rbf` is typically selected. (You
    may not want to fit this model on your machine unless you are using good hardware,
    or do not mind doing something else for a while and coming back for your results.)
    Take a look at the following code:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行非线性SVR不需要做太多改变。我们只需要创建一个`SVR`对象并选择一个核函数。通常选择`rbf`。（除非你使用的是良好的硬件，或者你不在乎暂时做其他事情然后回来获取结果，否则你可能不想在你的机器上拟合这个模型。）看看下面的代码：
- en: '[PRE18]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: There is a noticeable improvement in terms of the mean absolute error. Here,
    we can see that the `gamma` and C hyperparameters are doing a fair bit of work
    for us. If we are okay being about 2 degrees off on average, this model gets us
    there.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在平均绝对误差方面有明显的改进。在这里，我们可以看到`gamma`和C超参数为我们做了很多工作。如果我们对平均偏差2度左右没有异议，这个模型就能达到这个目标。
- en: We go into much more detail regarding the gamma and C hyperparameters in [*Chapter
    13*](B17978_13_ePub.xhtml#_idTextAnchor152)*, Support Vector Machine Classification*.
    We also explore other kernels besides the rbf kernel.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第13章*](B17978_13_ePub.xhtml#_idTextAnchor152)*，支持向量机分类*中，我们详细讨论了gamma和C超参数。我们还探讨了除了rbf核以外的其他核函数。
- en: 'Let’s look again at the residuals to see if there is something problematic
    in how our errors are distributed, as was the case with our linear model:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们再次查看残差，看看我们的误差分布是否有问题，就像我们的线性模型那样：
- en: '[PRE19]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This produces the following plot:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 8.10 – Scatterplot of predictions and residuals for the land temperatures
    nonlinear SVR model ](img/B17978_08_010.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图8.10 – 非线性SVR模型预测值和残差的散点图](img/B17978_08_010.jpg)'
- en: Figure 8.10 – Scatterplot of predictions and residuals for the land temperatures
    nonlinear SVR model
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 – 非线性SVR模型预测值和残差的散点图
- en: These residuals look substantially better than those for the linear model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这些残差看起来比线性模型的残差要好得多。
- en: This illustrates how using a kernel function can increase the complexity of
    our model without us having to increase the feature space. By using the `rbf`
    kernel and adjusting the C and `gamma` hyperparameters, we address some of the
    underfitting we saw with the linear model. This is one of the great advantages
    of nonlinear SVR. The disadvantage, as we also saw, was that it was quite taxing
    on system resources. A dataset that contains 12,000 observations is at the upper
    limit of what can be handled easily with nonlinear SVR, particularly with a grid
    search for the best hyperparameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这说明了使用核函数如何在不增加特征空间的情况下增加我们模型的复杂性。通过使用`rbf`核并调整C和`gamma`超参数，我们解决了线性模型中看到的欠拟合问题。这是非线性SVR的一个巨大优点。缺点，正如我们之前看到的，是对系统资源的需求很大。包含12,000个观测值的数据库是非线性SVR可以轻松处理的极限，尤其是在进行最佳超参数的网格搜索时。
- en: Summary
  id: totrans-121
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: The examples in this chapter illustrated some of the advantages of SVR. The
    algorithm allows us to adjust hyperparameters to address underfitting or overfitting.
    This can be done without increasing the number of features. SVR is also less sensitive
    to outliers than methods such as linear regression.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的示例展示了SVR的一些优点。该算法允许我们调整超参数来解决欠拟合或过拟合问题。这可以在不增加特征数量的情况下完成。与线性回归等方法相比，SVR对异常值也不那么敏感。
- en: When we can build a good model with linear SVR, it is a perfectly reasonable
    choice. It can be trained much faster than a nonlinear model. However, we can
    often improve performance with a nonlinear SVR, as we saw in the last section
    of this chapter.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们可以用线性SVR构建一个好的模型时，这是一个完全合理的选择。它比非线性模型训练得快得多。然而，我们通常可以通过非线性SVR来提高性能，就像我们在本章的最后部分看到的那样。
- en: 'This discussion leads us to what we will explore in the next chapter, where
    we will look at two popular non-parametric regression algorithms: k-nearest neighbors
    and decision tree regression. These two algorithms make almost no assumptions
    about the distribution of our features and targets. Similar to SVR, they can capture
    complicated relationships in the data without increasing the feature space.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这一讨论引出了我们在下一章将要探讨的内容，我们将探讨两种流行的非参数回归算法：k近邻和决策树回归。这两个算法对特征和目标分布几乎没有假设。与SVR类似，它们可以在不增加特征空间的情况下捕捉数据中的复杂关系。
