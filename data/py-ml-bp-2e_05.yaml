- en: Create a Custom Newsfeed
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建自定义新闻订阅源
- en: I read *a lot*. Some might even say compulsively. I've been known to consume
    more than a hundred articles on some days. But despite this, I frequently find
    myself searching for more to read. I suffer from this sneaking suspicion that
    I have missed something interesting, and will forever suffer a gap in my knowledge!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 我阅读的*很多*。有些人甚至会说是强迫症。我曾经一天看过超过一百篇文章。但即便如此，我经常感到自己还在寻找更多的文章。总有一种隐隐的感觉，觉得自己错过了什么有趣的东西，永远会在知识的空白中受困！
- en: If you suffer from similar symptoms, fear not, because in this chapter, I'm
    going to reveal one simple trick to finding all the articles you want to read
    without having to dig through the dozens that you don't.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你也有类似的困扰，不用担心，因为在本章中，我将揭示一个简单的技巧，帮助你找到所有你想阅读的文章，而不必翻找那些你不感兴趣的。
- en: By the end of this chapter, you'll have learned how to build a system that understands
    your taste in news, and will send you a personally tailored newsletter each day.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将学会如何构建一个能够理解你新闻兴趣的系统，并且每天向你发送个性化的新闻简报。
- en: 'Here''s what we''ll cover in this chapter:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章内容概述：
- en: Creating a supervised training set with the Pocket app
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Pocket 应用创建监督学习数据集
- en: Leveraging the Pocket API to retrieve stories
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Pocket API 获取文章
- en: Using the Embedly API to extract story bodies
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Embedly API 提取文章正文
- en: Natural language processing basics
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理基础
- en: Support vector machines
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机
- en: IFTTT integration with RSS feeds and Google Sheets
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IFTTT 与 RSS 订阅和 Google Sheets 的集成
- en: Setting up a daily personal newsletter
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置每日个人新闻简报
- en: Creating a supervised training set with Pocket
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Pocket 创建一个监督学习数据集
- en: Before we can create a model of our taste in news articles, we need training
    data. This training data will be fed into our model in order to teach it to discriminate
    between the articles we'd be interested in and those we would not. To build this
    corpus, we will need to annotate a large number of articles to correspond to these
    interests. We'll label each article either `y` or `n`, indicating whether it is
    the type of article we would want to have sent to us in our daily digest or not.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建一个能够理解我们新闻兴趣的模型之前，我们需要训练数据。这些训练数据将输入到我们的模型中，以教会它区分我们感兴趣的文章和不感兴趣的文章。为了构建这个语料库，我们需要标注大量文章，标注它们是否符合我们的兴趣。我们将为每篇文章标记`y`或`n`，表示它是否是我们希望每天收到的新闻简报中的文章。
- en: To simplify this process, we'll use the Pocket app. Pocket is an application
    that allows you to save stories to read later. You simply install the browser
    extension, and then click on the Pocket icon in your browser's toolbar when you
    wish to save a story. The article is saved to your personal repository. One of
    the great features of Pocket for our purposes is the ability to save the article
    with a tag of your choosing. We'll use this to mark interesting articles as `y`
    and non-interesting articles as `n`.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化这个过程，我们将使用Pocket应用。Pocket是一个可以让你保存稍后阅读的故事的应用程序。你只需安装浏览器扩展程序，然后在想保存文章时，点击浏览器工具栏中的Pocket图标。文章会保存到你的个人仓库。Pocket的一个强大功能是你可以为保存的文章打上自定义标签。我们将用`y`标记有趣的文章，用`n`标记不感兴趣的文章。
- en: Installing the Pocket Chrome Extension
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 安装 Pocket Chrome 扩展
- en: 'I''m using Google Chrome for this, but other browsers should work similarly. Follow
    the steps for installing the Pocket Chrome Extention:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在使用 Google Chrome，但其他浏览器应该也能类似操作。按照以下步骤安装 Pocket Chrome 扩展：
- en: 'For Chrome, go to the Google app store and look for the Extensions section:'
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于 Chrome，访问 Google 应用商店并查找扩展程序部分：
- en: '![](img/bec36363-4072-441d-9ee3-5e8d34d870e2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bec36363-4072-441d-9ee3-5e8d34d870e2.png)'
- en: Pocket Chrome Extention
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Pocket Chrome 扩展
- en: Click on Add to Chrome. If you already have an account, log in, and, if not,
    go ahead and sign up (it's free).
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“添加到Chrome”。如果你已经有账户，登录即可；如果没有，注册一个（是免费的）。
- en: Once that is complete, you should see the Pocket icon in the upper-right corner
    of your browser.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，你应该能在浏览器的右上角看到 Pocket 图标。
- en: 'It will be grayed out, but once there is an article you wish to save, you can
    click it. It will turn red once the article has been saved:'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它会变灰，但一旦有你想保存的文章，你可以点击它。文章保存后，它会变为红色：
- en: '![](img/0600e60d-722d-40bc-ae27-2ac2aac6598e.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0600e60d-722d-40bc-ae27-2ac2aac6598e.png)'
- en: 'The saved page can be seen as below:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 保存的页面如下所示：
- en: '![](img/f0eba099-38cb-46dc-b437-6676aa2cbf4d.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f0eba099-38cb-46dc-b437-6676aa2cbf4d.png)'
- en: The New York Times saved page
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 《纽约时报》保存页面
- en: Now the fun part! As you go through your day, start saving articles you'd like
    to read, as well as those you wouldn't. Tag the interesting ones with `y`, and
    the non-interesting ones with `n`. This is going to take some work. Your end results
    will only be as good as your training set, so you're going to need to do this
    for hundreds of articles. If you forget to tag an article when you save it, you
    can always go to the site, [http://www.get.pocket.com](https://getpocket.com/),
    to tag it there.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是有趣的部分！在你的一天中，开始保存你想阅读的文章，以及你不想阅读的文章。将感兴趣的文章标记为`y`，将不感兴趣的文章标记为`n`。这将需要一些工作。你的最终结果将与训练集的质量直接相关，因此你需要为成百上千的文章进行标记。如果你保存文章时忘记标记，可以随时访问[http://www.get.pocket.com](https://getpocket.com/)，在那里进行标记。
- en: Using the Pocket API to retrieve stories
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Pocket API来检索文章
- en: 'Now that you''ve diligently saved your articles to Pocket, the next step is
    to retrieve them. To accomplish this, we''ll use the Pocket API. You can sign
    up for an account at [https://getpocket.com/developer/apps/new](https://getpocket.com/developer/apps/new).
    Follow the steps to achieve that:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经认真地将文章保存到 Pocket，下一步是检索它们。为此，我们将使用 Pocket API。你可以在[https://getpocket.com/developer/apps/new](https://getpocket.com/developer/apps/new)注册一个账户。请按照步骤完成：
- en: Click on Create a New App in the upper-left corner and fill in the details to
    get your API key.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击左上角的“创建新应用”，并填写详细信息以获取你的API密钥。
- en: 'Make sure to click all of the permissions so that you can add, change, and
    retrieve articles:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保点击所有权限，以便你可以添加、更改和检索文章：
- en: '![](img/d70f782e-626c-411d-b2d7-111535a45741.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d70f782e-626c-411d-b2d7-111535a45741.png)'
- en: Once you have that filled in and submitted, you will receive your **consumer
    key**.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦你填写并提交了这些，你将收到你的**消费者密钥**。
- en: 'You can find that in the upper-left corner, under My Apps. It will look like
    the following screenshot, but obviously with a real key:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以在左上角的“我的应用”下找到它。它看起来会像下面的截图，但显然会有一个真实的密钥：
- en: '![](img/2f8553e5-5a88-4939-8c58-1e115884f87e.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2f8553e5-5a88-4939-8c58-1e115884f87e.png)'
- en: Once that is set, you are ready to move on to the next step, which is to set
    up authorizations. We'll do that now.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦设置完成，你就可以继续进行下一步，即设置授权。我们现在开始操作。
- en: 'It requires you to input your consumer key and a redirect URL. The redirect
    URL can be anything. Here, I have used my Twitter account:'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这要求你输入消费者密钥和重定向URL。重定向URL可以是任何内容。在这里，我使用了我的Twitter账户：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The preceding code results in the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将产生以下输出：
- en: '![](img/e651dd33-b0ae-439a-8d0e-512fcdacdde3.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e651dd33-b0ae-439a-8d0e-512fcdacdde3.png)'
- en: 'The output will have the code you''ll need for the next step. Place the following
    in your browser bar:'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出将包含你下一步所需的代码。将以下内容放入浏览器地址栏：
- en: '[https://getpocket.com/auth/authorize?request_token=some_long_access_code&amp;redirect_uri=https%3A//www.twitter.com/acombs](https://getpocket.com/auth/authorize?request_token=some_long_access_code&redirect_uri=https%3A//www.twitter.com/acombs)'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://getpocket.com/auth/authorize?request_token=some_long_access_code&amp;redirect_uri=https%3A//www.twitter.com/acombs](https://getpocket.com/auth/authorize?request_token=some_long_access_code&redirect_uri=https%3A//www.twitter.com/acombs)'
- en: If you change the redirect URL to one of your own, make sure to URL encode it
    (that's the `%3A` type stuff you see in the preceding URL).
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你将重定向URL更改为你自己的某个URL，请确保进行URL编码（就是你在前面的URL中看到的`%3A`类型的东西）。
- en: 'At this point, you should be presented with an authorization screen. Go ahead
    and approve it, and then we can move on to the next step:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此时，你应该会看到一个授权屏幕。点击同意，然后我们可以继续进行下一步：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The preceding code results in the following output:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将产生以下输出：
- en: '![](img/f4c276fc-3893-4910-b350-f332d3d02864.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f4c276fc-3893-4910-b350-f332d3d02864.png)'
- en: 'We''ll use the output code here, to move on to retrieving the stories. First,
    we retrieve the stories tagged `n`:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用这里的输出代码，继续检索这些文章。首先，我们检索标记为`n`的文章：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The preceding code results in the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将产生以下输出：
- en: '![](img/0ed35aed-75d2-4df9-8091-fc87965664d3.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0ed35aed-75d2-4df9-8091-fc87965664d3.png)'
- en: You'll notice that we have a long JSON string on all the articles that we tagged
    `n`. There are several keys in this, but we are really only interested in the
    URL at this point.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们在所有标记为`n`的文章中有一个很长的JSON字符串。这个字符串中有多个键，但此时我们只关心URL。
- en: 'We''ll go ahead and create a list of all the URLs from this:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将继续创建一个包含所有URL的列表：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The preceding code results in the following output:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 上面的代码将产生以下输出：
- en: '![](img/bac52869-eb17-41e6-99f6-e15668064bfd.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bac52869-eb17-41e6-99f6-e15668064bfd.png)'
- en: List of URLs
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: URL列表
- en: 'This list contains all the URLs of stories we aren''t interested in. Let''s
    now put that in a DataFrame and tag it as such:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个列表包含了我们不感兴趣的所有故事的 URL。现在我们将它放入一个 DataFrame，并标记为不感兴趣：
- en: '[PRE4]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The preceding code results in the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生了以下输出：
- en: '![](img/7b330ba0-fb54-44e7-b899-a03304214597.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7b330ba0-fb54-44e7-b899-a03304214597.png)'
- en: Tagging the URLs
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 标记 URL
- en: 'Now we''re all set with the unwanted stories. Let''s do the same thing with
    those stories we are interested in:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经处理好了不感兴趣的故事。让我们对我们感兴趣的故事做同样的事情：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The preceding code results in the following output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生了以下输出：
- en: '![](img/e0cf64cd-f8cc-4071-9d3b-ee31e97bdd51.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0cf64cd-f8cc-4071-9d3b-ee31e97bdd51.png)'
- en: Tagging the URLs of stories we are interested in
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 标记我们感兴趣的故事的 URL
- en: 'Now that we have both types of stories for our training data, let''s join them
    together into a single DataFrame:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了两种类型的故事作为训练数据，让我们将它们合并成一个单一的 DataFrame：
- en: '[PRE6]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The preceding code results in the following output:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生了以下输出：
- en: '![](img/7a12a25e-1a50-471b-b1af-6836de25339a.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7a12a25e-1a50-471b-b1af-6836de25339a.png)'
- en: Joining the URLs- both interested and not interested
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 合并 URL——感兴趣的和不感兴趣的
- en: Now that we're set with all our URLs and their corresponding tags in a single
    frame, we'll move on to downloading the HTML for each article. We'll use another
    free service for this, called Embedly.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有 URL 和对应的标签放入一个框架中，接下来我们将开始下载每篇文章的 HTML 内容。我们将使用另一个免费的服务来完成这个操作，名为
    Embedly。
- en: Using the Embedly API to download story bodies
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Embedly API 下载故事正文
- en: We have all the URLs for our stories, but, unfortunately, this isn't enough
    to train on; we'll need the full article body. This in itself could become a huge
    challenge if we want to roll our own scraper, especially if we are going to be
    pulling stories from dozens of sites. We would need to write code to target the
    article body while carefully avoiding all the other site gunk that surrounds it.
    Fortunately, as far as we are concerned, there are a number of free services that
    will do this for us. I'm going to be using Embedly to do this, but there are a
    number of other services that you could use instead.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经拥有了所有故事的 URL，但不幸的是，这还不足以进行训练；我们还需要完整的文章正文。如果我们想自己编写抓取程序，这可能会变成一个巨大的挑战，特别是当我们需要从数十个网站上提取故事时。我们需要编写代码来专门提取文章正文，同时小心避免抓取到周围的其他无关内容。幸运的是，就我们而言，有一些免费的服务可以帮我们做到这一点。我将使用
    Embedly 来完成这项任务，但你也可以选择其他服务。
- en: 'The first step is to sign up for Embedly API access. You can do that at [https://app.embed.ly/signup](https://app.embed.ly/signup).
    It is a straightforward process. Once you confirm your registration, you will
    receive an API key. That''s really all you''ll need. You''ll just use that key
    in your HTTP request. Let''s do that now:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是注册 Embedly API 访问权限。你可以在 [https://app.embed.ly/signup](https://app.embed.ly/signup)
    上进行注册。这个过程很简单。确认注册后，你会收到一个 API 密钥。这就是你需要的一切。你只需要在 HTTP 请求中使用这个密钥。现在我们来操作一下：
- en: '[PRE7]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code results in the following output:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生了以下输出：
- en: '![](img/6b4b9a5b-6e6e-4345-8a5c-766a71a49269.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6b4b9a5b-6e6e-4345-8a5c-766a71a49269.png)'
- en: HTTP requests
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP 请求
- en: And with that, we have the HTML of each story.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经获取了每篇故事的 HTML 内容。
- en: 'Since the content is embedded in HTML markup, and we want to feed plain text
    into our model, we''ll use a parser to strip out the markup tags:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 由于内容嵌入在 HTML 标记中，而我们希望将纯文本输入到模型中，我们将使用解析器来剥离掉标记标签：
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code results in the following output:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码产生了以下输出：
- en: '![](img/fb4aa1e9-f5f4-4a26-923c-b4cfa49d89e4.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fb4aa1e9-f5f4-4a26-923c-b4cfa49d89e4.png)'
- en: And with that, we have our training set ready. We can now move on to a discussion
    of how to transform our text into something that a model can work with.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们已经准备好了训练集。现在可以开始讨论如何将文本转化为模型能够处理的形式。
- en: Basics of Natural Language Processing
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自然语言处理基础
- en: If machine learning models only operate on numerical data, how can we transform
    our text into a numerical representation? That is exactly the focus of **Natural
    Language Processing** (**NLP**). Let's take a brief look at how this is done.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果机器学习模型只能处理数字数据，我们该如何将文本转化为数字表示呢？这正是**自然语言处理**（**NLP**）的核心问题。我们来简单了解一下它是如何做到的。
- en: 'We''ll begin with a small corpus of three sentences:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个包含三句话的小语料库开始：
- en: The new kitten played with the other kittens
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 那只新小猫和其他小猫一起玩
- en: She ate lunch
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 她吃了午饭
- en: She loved her kitten
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 她爱她的小猫
- en: 'We''ll first convert our corpus into a **bag-of-words** (**BOW**) representation.
    We''ll skip preprocessing for now. Converting our corpus into a BOW representation
    involves taking each word and its count to create what''s called a **term-document
    matrix**. In a term-document matrix, each unique word is assigned to a column,
    and each document is assigned to a row. At the intersection of the two is the
    count:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先将语料库转换为**词袋模型**（**BOW**）表示。暂时跳过预处理。将语料库转换为BOW表示包括获取每个词及其出现次数，以创建所谓的**词项-文档矩阵**。在词项-文档矩阵中，每个独特的单词会被分配到一列，每个文档会被分配到一行。它们的交点处会记录出现次数：
- en: '| **Sr. no.**  | **the** | **new** | **kitten** | **played** | **with** | **other**
    | **kittens** | **she** | **ate** | **lunch** | **loved** | **her** |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| **序号**  | **the** | **新** | **小猫** | **玩** | **与** | **其他** | **小猫们** | **她**
    | **吃** | **午餐** | **爱** | **她** |'
- en: '| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |'
- en: '| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 | 0 | 0 |'
- en: '| 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 1 | 1 |'
- en: Notice that, for these three short sentences, we already have 12 features. As
    you might imagine, if we were dealing with actual documents, such as news articles
    or even books, the number of features would explode into the hundreds of thousands.
    To mitigate this explosion, we can take a number of steps to remove features that
    add little to no informational value to our analysis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于这三句话，我们已经有了12个特征。正如你所想象的那样，如果我们处理的是实际的文档，比如新闻文章或甚至是书籍，特征的数量会激增到数十万。为了减少这种膨胀，我们可以采取一系列措施，删除那些对分析几乎没有信息价值的特征。
- en: 'The first step we can take is to remove **stop words**. These are words that
    are so common that they typically tell you nothing about the content of the document.
    Common examples of English stop words are *the*, *is*, *at*, *which*, and *on*.
    We''ll remove those, and recompute our term-document matrix:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以采取的第一步是移除**停用词**。这些是那些如此常见的词，通常不会提供关于文档内容的任何信息。常见的英语停用词有*the*、*is*、*at*、*which*和*on*。我们将删除这些词，并重新计算我们的词项-文档矩阵：
- en: '| **Sr. no.** | **new** | **kitten** | **played** | **kittens** | **ate** |
    **lunch** | **loved** |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| **序号** | **新** | **小猫** | **玩** | **小猫们** | **吃** | **午餐** | **爱** |'
- en: '| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 1 | 1 | 0 | 0 | 0 |'
- en: '| 2 | 0 | 0 | 0 | 0 | 1 | 1 | 0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0 | 0 | 1 | 1 | 0 |'
- en: '| 3 | 0 | 1 | 0 | 0 | 0 | 0 | 1 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 1 | 0 | 0 | 0 | 0 | 1 |'
- en: 'As you can see, the number of features was reduced from 12 to 7\. This is great,
    but we can take it even further. We can perform **stemming** or **lemmatization**
    to reduce the features further. Notice that in our matrix, we have both *kitten*
    and *kittens*. By using stemming or lemmatization, we can consolidate that into
    just *kitten*:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，特征数量从12个减少到了7个。这很好，但我们还可以进一步减少特征。我们可以进行**词干提取**或**词形还原**来进一步减少特征。注意，在我们的矩阵中，我们有*kitten*和*kittens*两个词。通过词干提取或词形还原，我们可以将其合并成*kitten*：
- en: '| **Sr. no.** | **new** | **kitten** | **play** | **eat** | **lunch** | **love**
    |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| **序号** | **新** | **小猫** | **玩** | **吃** | **午餐** | **爱** |'
- en: '| 1 | 1 | 2 | 1 | 0 | 0 | 0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 2 | 1 | 0 | 0 | 0 |'
- en: '| 2 | 0 | 0 | 0 | 1 | 1 | 0 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0 | 1 | 1 | 0 |'
- en: '| 3 | 0 | 1 | 0 | 0 | 0 | 1 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 1 | 0 | 0 | 0 | 1 |'
- en: Our new matrix consolidated *kittens* and *kitten*, but something else happened
    as well. We lost the suffixes to *played* and *loved*, and *ate* was transformed
    to *eat*. Why? This is what lemmatization does. If you remember your grade school
    grammar classes, we've gone from the inflectional form to the base form of the
    word. If that is lemmatization, what is stemming? Stemming has the same goal,
    but uses a less sophisticated approach. This approach can sometimes produce pseudo-words
    rather than the actual base form. For example, in lemmatization, if you were to
    reduce *ponies*, you would get *pony*, but with stemming, you'd get *poni*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新矩阵将*kittens*和*kitten*合并了，但还发生了其他变化。我们丢失了*played*和*loved*的后缀，*ate*被转换成了*eat*。为什么？这就是词形还原的作用。如果你还记得小学的语法课，我们已经从动词的屈折形式转到了词根形式。如果这就是词形还原，那词干提取是什么呢？词干提取有相同的目标，但采用的是一种不那么精细的方法。这种方法有时会生成伪单词，而不是实际的词根形式。例如，在词形还原中，如果将*ponies*还原，你会得到*pony*，但在词干提取中，你会得到*poni*。
- en: Let's now go further to apply another transformation to our matrix. So far,
    we have used a simple count of each word, but we can apply an algorithm that will
    act like a filter on our data to enhance the words that are unique to each document.
    This algorithm is called **term frequency-inverse document frequency** (**tf-idf**)**.**
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进一步应用另一个变换到我们的矩阵。到目前为止，我们使用了每个词的简单计数，但我们可以应用一种算法，这种算法会像一个过滤器一样作用于我们的数据，增强每个文档中独特的词汇。这个算法叫做**词频-逆文档频率** (**tf-idf**)**。**
- en: We calculate this tf-idf ratio for each term in our matrix. Let's calculate
    it for a couple of examples. For the word *new* in document one, the term frequency
    is just the count, which is `1`. The inverse document frequency is calculated
    as the log of the number of documents in the corpus over the number of documents
    the term appears in. For *new*, this is *log (3/1)*, or .4471\. So, for the complete
    tf-idf value, we have *tf * idf*, or, here, it is *1 x .4471*, or just .4471\.
    For the word *kitten* in document one, the tf-idf is *2 * log (3/2)*, or .3522.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为矩阵中的每个词项计算tf-idf比率。让我们通过几个例子来计算。对于文档一中的词汇*new*，词频就是它的出现次数，即`1`。逆文档频率是通过计算语料库中所有文档数与该词出现的文档数的比值的对数来得出的。对于*new*，这个值是*log
    (3/1)*，即.4471。所以，完整的tf-idf值就是*tf * idf*，或者在这里，它是*1 x .4471*，即.4471。对于文档一中的词汇*kitten*，tf-idf是*2
    * log (3/2)*，即.3522。
- en: 'Completing this for the remainder of the terms and documents, we have the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 完成其余词项和文档的计算后，我们得到了以下结果：
- en: '| **Sr. no.** | **new** | **kitten** | **play** | **eat** | **lunch** | **love**
    |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| **序号** | **new** | **kitten** | **play** | **eat** | **lunch** | **love**
    |'
- en: '| 1 | .4471 | .3522 | .4471 | 0 | 0 | 0 |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 1 | .4471 | .3522 | .4471 | 0 | 0 | 0 |'
- en: '| 2 | 0 | 0 | 0 | .4471 | .4471 | 0 |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0 | .4471 | .4471 | 0 |'
- en: '| 3 | 0 | .1761 | 0 | 0 | 0 | .4471 |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | .1761 | 0 | 0 | 0 | .4471 |'
- en: Why do all of this? Let's say, for example, we have a corpus of documents about
    many subjects (medicine, computing, food, animals, and so on) and we want to classify
    them into topics. Very few documents would contain the word *sphygmomanometer*,
    which is the device used to measure blood pressure; and all the documents that
    did would likely concern the topic of medicine. And obviously, the more times
    this word appears in a document, the more likely it is to be about medicine. So
    a term that occurs rarely across our entire corpus, but that is present many times
    in a document, makes it likely that this term is tied closely to the topic of
    that document. In this way, documents can be said to be represented by those terms
    with high tf-idf values.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么要做这些呢？假设我们有一个关于多个主题（医学、计算机、食物、动物等）的文档集，并且我们想将它们分类成不同的主题。很少有文档会包含词汇*sphygmomanometer*，它是用来测量血压的仪器；而所有包含这个词的文档，可能都涉及医学主题。显然，这个词在文档中出现的次数越多，它就越可能与医学相关。所以，一个在整个语料库中很少出现，但在某个文档中出现多次的词汇，很可能与该文档的主题紧密相关。通过这种方式，可以认为文档是通过那些具有高tf-idf值的词汇来表示的。
- en: 'With the help of this framework, we''ll now convert our training set into a
    tf-idf matrix:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个框架的帮助下，我们现在将训练集转换为tf-idf矩阵：
- en: '[PRE9]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With those three lines, we have converted all our documents into a tf-idf vector.
    We passed in a number of parameters: `ngram_range`, `stop_words`, and `min_df`.
    Let''s discuss each.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这三行，我们已将所有文档转换为tf-idf向量。我们传入了若干参数：`ngram_range`、`stop_words`和`min_df`。接下来我们逐一讨论。
- en: 'First, `ngram_range` is how the document is tokenized. In our previous examples,
    we used each word as a token, but here, we are using all one- to three-word sequences
    as tokens. Let''s take our second sentence, *She ate lunch*. We''ll ignore stop
    words for the moment. The n-grams for this sentence would be: *she*, *she ate*,
    *she ate lunch*, *ate*, *ate lunch*, and *lunch*.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`ngram_range`是文档的分词方式。在之前的示例中，我们使用每个单词作为一个词元，但在这里，我们将所有的1到3个词的序列作为词元。以我们第二句“*She
    ate lunch*”为例。我们暂时忽略停用词。这个句子的n-grams会是：*she*、*she ate*、*she ate lunch*、*ate*、*ate
    lunch*和*lunch*。
- en: Next, we have `stop_words`. We pass in `english` for this to remove all the
    English stop words. As discussed previously, this removes all terms that lack
    informational content.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`stop_words`。我们传入`english`，以移除所有的英文停用词。正如之前讨论的，这会移除所有缺乏信息内容的词汇。
- en: And finally, we have `min_df`. This removes all words from consideration that
    don't appear in at least three documents. Adding this removes very rare terms
    and reduces the size of our matrix.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有`min_df`。这个参数会移除所有在至少三个文档中未出现的词汇。添加这个选项可以去除非常稀有的词汇，并减少我们的矩阵大小。
- en: Now that our article corpus is in a workable numerical format, we'll move on
    to feeding it to our classifier.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的文章语料库已经转化为可操作的数值格式，我们将继续将其输入到分类器中。
- en: Support Vector Machines
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 支持向量机
- en: We're going to be utilizing a new classifier in this chapter, a linear **Support
    Vector Machine** (**SVM**). An SVM is an algorithm that attempts to linearly separate
    data points into classes using a **maximum-margin hyperplane**. That's a mouthful,
    so let's look at what it really means.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将使用一种新的分类器——线性 **支持向量机**（**SVM**）。SVM 是一种算法，试图通过使用 **最大边距超平面** 来将数据点线性地分到不同类别。这个词听起来很复杂，所以我们来看看它到底是什么意思。
- en: 'Suppose we have two classes of data, and we want to separate them with a line.
    (We''ll just deal with two features, or dimensions, here.) What is the most effective
    way to place that line? Lets have a look at an illustration:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有两个类别的数据，并且我们想用一条线将它们分开。（在这里我们只处理两个特征或维度。）放置这条线的最有效方式是什么呢？让我们看看下面的插图：
- en: '![](img/13ff1cec-f4de-4170-b1c7-e2feadd686af.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](img/13ff1cec-f4de-4170-b1c7-e2feadd686af.png)'
- en: 'In the preceding diagram, line **H[1]** does not effectively discriminate between
    the two classes, so we can eliminate that one. Line **H[2]** is able to discriminate
    between them cleanly, but **H[3]** is the maximum-margin line. This means that
    the line is centered between the two nearest points of each class, which are known
    as the **support vectors**. These can be seen as the dotted lines in the following
    diagram:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，线 **H[1]** 并不能有效地区分这两个类别，所以我们可以去掉它。线 **H[2]** 能够干净利落地区分它们，但 **H[3]**
    是最大边界线。这意味着这条线位于每个类别的两个最近点之间的中间，这些点被称为 **支持向量**。这些可以在下图中看到为虚线：
- en: '![](img/30e96749-12d3-4f34-be03-81b0701fff2c.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](img/30e96749-12d3-4f34-be03-81b0701fff2c.png)'
- en: 'What if the data isn''t able to be separated into classes so neatly? What if
    there is an overlap between the points? In that situation, there are still options.
    One is to use what''s called a **soft-margin SVM**. This formulation still maximizes
    the margin, but with the trade-off being a penalty for points that fall on the
    wrong side of the margin. The other option is to use what''s called the **kernel
    trick**. This method transforms the data into a higher dimensional space where
    the data can be linearly separated. An example is provided here:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据无法如此整齐地被分割成不同类别呢？如果数据点之间存在重叠呢？在这种情况下，仍然有一些选项。一种方法是使用所谓的 **软边距 SVM**。这种公式仍然最大化边距，但其代价是对落在边距错误一侧的点进行惩罚。另一种方法是使用所谓的
    **核技巧**。这种方法将数据转换到一个更高维度的空间，在那里数据可以被线性分割。这里提供了一个示例：
- en: '![](img/bb67ef6e-3a1e-4060-8da9-6e6fcfd5ea5f.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bb67ef6e-3a1e-4060-8da9-6e6fcfd5ea5f.png)'
- en: 'The two-dimensional representation is a follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 二维表示如下：
- en: '![](img/74a17d51-a26b-4487-9d36-644d544e09fd.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/74a17d51-a26b-4487-9d36-644d544e09fd.png)'
- en: We have taken a one-dimensional feature space and mapped it onto a two-dimensional
    feature space. The mapping simply takes each *x* value and maps it to *x*, *x²*.
    Doing so allows us to add a linear separating plane.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将一维特征空间映射到二维特征空间。这个映射只是将每个 *x* 值映射到 *x*，*x²*。这样做使我们能够添加一个线性分隔平面。
- en: 'With that covered, let''s now feed our tf-idf matrix into our SVM:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 既然已经涵盖了这些，我们现在将我们的 tf-idf 矩阵输入到 SVM 中：
- en: '[PRE10]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '`tv` is our matrix, and `df[''wanted'']` is our list of labels. Remember this
    is either `y` or `n`, denoting whether we are interested in the article. Once
    that runs, our model is trained.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: '`tv` 是我们的矩阵，而 `df[''wanted'']` 是我们的标签列表。记住，这个列表的内容要么是 `y`，要么是 `n`，表示我们是否对这篇文章感兴趣。一旦运行完毕，我们的模型就训练完成了。'
- en: One thing we aren't doing in this chapter is formally evaluating our model.
    You should almost always have a hold-out set to evaluate your model against, but
    because we are going to be continuously updating our model, and evaluating it
    daily, we'll skip that step for this chapter. Just remember that this is generally
    a terrible idea.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们没有正式评估我们的模型。你几乎总是应该有一个保留集来评估你的模型，但由于我们将不断更新模型并每天评估它，所以本章将跳过这一步。请记住，这通常是一个非常糟糕的主意。
- en: Let's now move on to setting up our daily feed of news items.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续设置我们的每日新闻订阅源。
- en: IFTTT integration with feeds, Google Sheets, and email
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: IFTTT 与订阅源、Google Sheets 和电子邮件的集成
- en: We used Pocket to build our training set, but now we need a streaming feed of
    articles to run our model against. To set this up, we'll use IFTTT once again,
    as well as Google Sheets, and a Python library that will allow us to work with
    Google Sheets.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Pocket来构建我们的训练集，但现在我们需要一个文章的流媒体订阅源来运行我们的模型。为了设置这个，我们将再次使用IFTTT、Google Sheets，以及一个允许我们与Google
    Sheets进行交互的Python库。
- en: Setting up news feeds and Google Sheets through IFTTT
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过IFTTT设置新闻订阅源和Google Sheets
- en: 'Hopefully, you have an IFTTT account set up at this point, but if not, go ahead
    and set that up now. Once that is done, you''ll need to set up integration with
    feeds and with Google Sheets:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 希望此时你已经设置好IFTTT帐户，如果没有，请现在设置好。完成后，你需要设置与订阅源和Google Sheets的集成：
- en: 'First, search for feeds in the search box on the home page, then click on Services,
    and click to set that up:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，在主页的搜索框中搜索“feeds”，然后点击“服务”，接着点击进行设置：
- en: '![](img/fea512df-fbd0-44dd-a31d-fe6600f32ce9.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fea512df-fbd0-44dd-a31d-fe6600f32ce9.png)'
- en: 'You''ll just need to click Connect:'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你只需要点击“连接”：
- en: '![](img/6389031a-ce09-45cd-bf3d-41080cd1cc73.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6389031a-ce09-45cd-bf3d-41080cd1cc73.png)'
- en: 'Next, search for `Google Drive` under Services:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，在“服务”下搜索`Google Drive`：
- en: '![](img/03e1346f-8188-45e6-9a4a-3e44ae1bab67.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](img/03e1346f-8188-45e6-9a4a-3e44ae1bab67.png)'
- en: 'Click on that. It should take you to a page where you select the Google account
    you want to connect to. Choose the account and then click Allow to enable IFTTT
    to access your Google Drive account. Once that''s done, you should see the following:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击该按钮，它将带你到一个页面，在这里你可以选择你想要连接的Google帐户。选择帐户后，点击“允许”以使IFTTT访问你的Google Drive帐户。完成后，你应该会看到以下内容：
- en: '![](img/f2445244-6fc6-4787-90a3-0ac7823c8d23.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f2445244-6fc6-4787-90a3-0ac7823c8d23.png)'
- en: 'Now, with our channels connected, we can set up our feed. Click on New Applet
    in the dropdown under your username in the right-hand corner. This will bring
    you here:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，连接了我们的频道，我们可以设置订阅源。点击右上角用户名下拉菜单中的“新建Applet”。这将带你到这里：
- en: '![](img/9bf9d660-7dba-4d1b-b951-8ebb2d523f60.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9bf9d660-7dba-4d1b-b951-8ebb2d523f60.png)'
- en: 'Click on +this. Search for `RSS Feed`, and then click on that. That should
    bring you here:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击+this，搜索`RSS Feed`，然后点击它。这将带你到如下页面：
- en: '![](img/edc23225-750c-4932-bf9f-5aeaf1001c77.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](img/edc23225-750c-4932-bf9f-5aeaf1001c77.png)'
- en: 'From here, click on New feed item:'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里，点击“新建订阅源项”：
- en: '![](img/fe70de1e-5a10-47a3-8aea-e12bedde546d.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/fe70de1e-5a10-47a3-8aea-e12bedde546d.png)'
- en: 'Then, add the URL to the box and click Create trigger. Once that is done, you''ll
    be brought back to add the +that action:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将URL添加到框中并点击“创建触发器”。完成后，你将返回继续添加+that动作：
- en: '![](img/bff82527-32fb-4ed2-8e67-fcebfc39e102.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bff82527-32fb-4ed2-8e67-fcebfc39e102.png)'
- en: 'Click on +that, search for `Sheets`, and then click on its icon. Once that
    is done, you''ll find yourself here:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击+that，搜索`Sheets`，然后点击它的图标。完成后，你会看到如下界面：
- en: '![](img/89fbc4e5-1aad-42d7-84c8-39773626cec5.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/89fbc4e5-1aad-42d7-84c8-39773626cec5.png)'
- en: 'We want our news items to flow into a Google Drive spreadsheet, so click on
    Add row to spreadsheet. You''ll then have an opportunity to customize the spreadsheet:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们希望我们的新闻项流入Google Drive电子表格，因此点击“添加行到电子表格”。然后你将有机会自定义电子表格：
- en: '![](img/e9c7a419-8c0c-486a-97ad-22ed50fafff1.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e9c7a419-8c0c-486a-97ad-22ed50fafff1.png)'
- en: I gave the spreadsheet the name `NewStories`, and placed it in a Google Drive
    folder called `IFTTT`. Click Create Action to finish the recipe, and soon you'll
    start seeing news items flow into your Google Drive spreadsheet. Note that it
    will only add new items as they come in, not items that existed at the time you
    created the sheet. I recommend adding a number of feeds. You will need to create
    individual recipes for each. It is best if you add feeds for the sites that are
    in your training set, in other words, the ones you saved with Pocket.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我将电子表格命名为`NewStories`，并将其放入名为`IFTTT`的Google Drive文件夹中。点击“创建动作”完成配方设置，很快你就会看到新闻项开始流入你的Google
    Drive电子表格。请注意，它只会添加新进项，而不是你创建表格时已经存在的项。我建议你添加多个订阅源。你需要为每个源创建单独的配方。最好添加你训练集中网站的订阅源，换句话说，就是你在Pocket中保存的那些网站。
- en: 'Give those stories a day or two to build up in the sheet, and then it should
    look something like this:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 给这些故事一两天的时间在表格中积累，然后它应该会看起来像这样：
- en: '![](img/4a6b042c-a32e-41a0-93a5-f9d4b27e1efd.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](img/4a6b042c-a32e-41a0-93a5-f9d4b27e1efd.png)'
- en: Fortunately, the full article HTML body is included. This means we won't have
    to use Embedly to download it for each article. We will still need to download
    the articles from Google Sheets, and then process the text to strip out the HTML
    tags, but this can all be done rather easily.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，完整的文章HTML主体已包括在内。这意味着我们不需要使用Embedly为每篇文章下载它。我们仍然需要从Google Sheets下载文章，然后处理文本以去除HTML标签，但这一切都可以相对轻松地完成。
- en: 'To pull down the articles, we''ll use a Python library called `gspread`. This
    can be pip installed. Once that is installed, you need to follow the direction
    for setting up **OAuth 2**. That can be found at [http://gspread.readthedocs.org/en/latest/oauth2.html](http://gspread.readthedocs.org/en/latest/oauth2.html).
    You will end up downloading a JSON credentials file. It is critical that, once
    you have that file, you find the email address in it with the `client_email` key.
    You then need to share the `NewStories` spreadsheet you are sending the stories
    to with that email. Just click on the blue Share button in the upper-right corner
    of the sheet, and paste the email in there. You will end up receiving a *failed
    to send* message in your Gmail account, but that is expected. Make sure to swap
    in your path to the file and the name of the file in the following code:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 为了下载文章，我们将使用一个名为`gspread`的Python库。可以通过pip安装。一旦安装完成，你需要按照**OAuth 2**的设置步骤进行操作。具体可以参考[http://gspread.readthedocs.org/en/latest/oauth2.html](http://gspread.readthedocs.org/en/latest/oauth2.html)。你最终会下载一个JSON凭证文件。至关重要的是，在获得该文件后，找到其中带有`client_email`键的电子邮件地址。然后，你需要将你要发送故事的`NewStories`电子表格与该电子邮件地址共享。只需点击表格右上角的蓝色“分享”按钮，并粘贴电子邮件地址。你会在Gmail账户中收到*发送失败*的消息，但这是预期的。确保在以下代码中替换为你文件的路径和文件名：
- en: '[PRE11]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Now, if everything went well, it should run without errors. Next, you can download
    the stories:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果一切顺利，它应该能正常运行。接下来，你可以下载这些故事：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The preceding code results in the following output:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![](img/cb48abf5-89de-4090-8fc2-47ddc3a33bf0.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](img/cb48abf5-89de-4090-8fc2-47ddc3a33bf0.png)'
- en: 'With that, we downloaded all of the articles from our feed and placed them
    into a DataFrame. We now need to strip out the HTML tags. We can use the function
    we used earlier to retrieve the text. We''ll then transform it using our tf-idf
    vectorizer:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一过程，我们从源中下载了所有文章，并将它们放入了DataFrame中。现在我们需要去除HTML标签。我们可以使用之前用来提取文本的函数。然后，我们将使用tf-idf向量化器进行转换：
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The preceding code results in the following output:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![](img/0d864c05-e9fc-4aad-b78f-902084ce2524.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d864c05-e9fc-4aad-b78f-902084ce2524.png)'
- en: 'Here, we see that our vectorization was successful. Let''s now pass it into
    our model to get back the results:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到我们的向量化已经成功。接下来，我们将其传入模型以获取结果：
- en: '[PRE14]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The preceding code results in the following output:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![](img/2292ef3e-aa9e-4b12-9703-f4e5ec5b44cc.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2292ef3e-aa9e-4b12-9703-f4e5ec5b44cc.png)'
- en: 'We see here that we have results for each of the stories. Let''s now join them
    with the stories themselves so that we can evaluate the results:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到每篇故事都有对应的结果。现在让我们将它们与故事本身合并，这样我们就可以评估结果：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The preceding code results in the following output:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![](img/c5dd103e-6d29-4d96-afaf-3271f3696e49.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c5dd103e-6d29-4d96-afaf-3271f3696e49.png)'
- en: 'At this point, we can improve the model by going through the results and correcting
    the errors. You''ll need to do this for yourself, but here is how I made changes
    to my own:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在此时，我们可以通过查看结果并纠正错误来改进模型。你需要自己完成这个步骤，但这是我如何修改我自己的模型：
- en: '[PRE16]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The preceding code results in the following output:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![](img/25c89735-a506-4343-834a-8ea97965d1c2.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](img/25c89735-a506-4343-834a-8ea97965d1c2.png)'
- en: 'This may look like a lot of changes, but of the over 900 articles evaluated,
    I had to change very few. By making these corrections, we can now feed this back
    into our model to improve it even more. Let''s add these results to our earlier
    training data and then rebuild the model:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些看起来可能是很多更改，但在评估的900多篇文章中，我只需要更改非常少的几篇。通过这些修正，我们现在可以将其反馈到我们的模型中，以进一步改进它。让我们将这些结果添加到我们之前的训练数据中，然后重新构建模型：
- en: '[PRE17]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code results in the following output:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码会产生以下输出：
- en: '![](img/b97a3db6-da79-4fbc-9a5d-6216fbde7bd7.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b97a3db6-da79-4fbc-9a5d-6216fbde7bd7.png)'
- en: 'Retrain the model with following code:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码重新训练模型：
- en: '[PRE18]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Now we have retrained our model with all the available data. You may want to
    do this a number of times as you get more results over the days and weeks. The
    more you add, the better your results will be.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用所有可用的数据重新训练了我们的模型。随着时间的推移，您可能需要多次执行此操作，以获取更多的结果。添加的数据越多，结果将会越好。
- en: We'll assume you have a well-trained model at this point, and are ready to begin
    using it. Let's now see how we can deploy this to set up a personalized news feed.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我们假设您已经有一个训练有素的模型，并准备开始使用它。现在让我们看看如何部署它来设置个性化新闻订阅。
- en: Setting up your daily personal newsletter
  id: totrans-199
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置您的每日个性化新闻简报
- en: 'In order to set up a personal email with news stories, we''re going to utilize
    IFTTT again. As before, in [Chapter 3](73ff80b5-e844-4791-bf95-b678215c00a8.xhtml),
    *Build an App to Find Cheap Airfares*, we''ll use the Webhooks channel to send
    a `POST` request. But this time, the payload will be our news stories. If you
    haven''t set up the Webhooks channel, do so now. Instructions can be found in
    [Chapter 3](73ff80b5-e844-4791-bf95-b678215c00a8.xhtml), *Build an App to Find
    Cheap Airfares*. You should also set up the Gmail channel. Once that is complete,
    we''ll add a recipe to combine the two. Follow the steps to set up IFTTT:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置一个包含新闻故事的个人电子邮件，我们将再次利用 IFTTT。与之前一样，在[第三章](73ff80b5-e844-4791-bf95-b678215c00a8.xhtml)中，*构建一个查找廉价机票的应用*，我们将使用
    Webhooks 渠道发送 `POST` 请求。但这次，负载将是我们的新闻故事。如果您还没有设置 Webhooks 渠道，请立即执行此操作。有关说明，请参阅[第三章](73ff80b5-e844-4791-bf95-b678215c00a8.xhtml)。您还应该设置
    Gmail 渠道。完成后，我们将添加一个配方来将两者结合起来。按照以下步骤设置 IFTTT：
- en: 'First, click New Applet from the IFTTT home page and then click +this. Then,
    search for the Webhooks channel:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，从 IFTTT 主页点击新 Applet，然后点击 +this。然后，搜索 Webhooks 渠道：
- en: '![](img/9dfbd355-91a9-4624-9ccf-155e0796cb1b.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/9dfbd355-91a9-4624-9ccf-155e0796cb1b.png)'
- en: 'Select that, and then select Receive a web request:'
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择它，然后选择接收 Web 请求：
- en: '![](img/35ed4dcf-c87b-42ae-b804-c13f352ac26a.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](img/35ed4dcf-c87b-42ae-b804-c13f352ac26a.png)'
- en: 'Then, give the request a name. I''m using `news_event`:'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，给请求一个名称。我使用 `news_event`：
- en: '![](img/aaa279e1-2abc-42e7-97cd-4d2ff759aa9f.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](img/aaa279e1-2abc-42e7-97cd-4d2ff759aa9f.png)'
- en: 'Finish by clicking Create trigger. Next, click on +that to set up the email
    piece. Search for Gmail and click on that:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，点击创建触发器。接下来，点击 +that 来设置电子邮件部分。搜索 Gmail 并点击它：
- en: '![](img/0f4c3ac5-e8a7-4d0e-9eef-9e4da3764925.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0f4c3ac5-e8a7-4d0e-9eef-9e4da3764925.png)'
- en: 'Once you have clicked Gmail, click Send yourself an email. From there, you
    can customize your email message:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦您点击了 Gmail，请点击发送自己邮件。从那里，您可以自定义您的电子邮件消息：
- en: '![](img/36ee5b08-8d0d-45a2-a692-936f5e2b9b74.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](img/36ee5b08-8d0d-45a2-a692-936f5e2b9b74.png)'
- en: Input a subject line, and include `{{Value1}}` in the email body. We will pass
    our story title and link into this with our `POST` request. Click on Create action
    and then Finish to finalize it.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 输入主题行，并在电子邮件正文中包含 `{{Value1}}`。我们将通过我们的 `POST` 请求将故事标题和链接传递到这里。点击创建操作，然后完成以完成设置。
- en: 'Now, we''re ready to generate the script that will run on a schedule, automatically
    sending us articles of interest. We''re going to create a separate script for
    this, but one last thing we need to do in our existing code is serialize our vectorizer
    and our model, as demonstrated in the following code block:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们准备生成一个计划运行的脚本，自动发送我们感兴趣的文章。我们将为此创建一个单独的脚本，但我们现有的代码中还需要做一件事情，即序列化我们的向量化器和模型，如下面的代码块所示：
- en: '[PRE19]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With that, we have saved everything we need from our model. In our new script,
    we will read those in to generate our new predictions. We''re going to use the
    same scheduling library to run the code as we used in [Chapter 3](73ff80b5-e844-4791-bf95-b678215c00a8.xhtml), *Build
    an App to Find Cheap Airfares*. Putting it all together, we have the following
    script:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经保存了我们模型所需的一切。在我们的新脚本中，我们将读取它们以生成我们的新预测。我们将使用与我们在[第三章](73ff80b5-e844-4791-bf95-b678215c00a8.xhtml)中使用的相同调度库来运行代码，*构建一个查找廉价机票的应用*。将所有内容组合起来，我们有以下脚本：
- en: '[PRE20]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: What this script will do is run every 4 hours, pull down the news stories from
    Google Sheets, run the stories through the model, generate an email by sending
    a `POST` request to IFTTT for those stories that are predicted to be of interest,
    and then, finally, it will clear out the stories in the spreadsheet so only new
    stories get sent in the next email.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这个脚本将每 4 小时运行一次，从 Google Sheets 下载新闻故事，通过模型运行故事，通过向 IFTTT 发送 `POST` 请求生成邮件，预测为感兴趣的故事，最后清空电子表格中的故事，以便在下一封电子邮件中只发送新故事。
- en: Congratulations! You now have your own personalized news feed!
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你现在拥有了属于自己的个性化新闻推送！
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we've learned how to work with text data when training machine
    learning models. We've also learned the basics of NLP and of SVMs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何在训练机器学习模型时处理文本数据。我们还学习了自然语言处理（NLP）和支持向量机（SVM）的基础知识。
- en: In the next chapter, we'll develop these skills further and attempt to predict
    what sort of content will go viral.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将进一步发展这些技能，并尝试预测哪些内容会变得流行。
