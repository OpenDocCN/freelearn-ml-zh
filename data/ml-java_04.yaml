- en: Customer Relationship Prediction with Ensembles
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成进行客户关系预测
- en: Any type of company that offers a service, product, or experience needs a solid
    understanding of their relationship with their customers; therefore, **customer
    relationship management** (**CRM**) is a key element of modern marketing strategies.
    One of the biggest challenges that businesses face is the need to understand exactly
    what causes a customer to buy new products.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 任何提供服务、产品或体验的公司都需要对其与客户的关系有一个稳固的理解；因此，**客户关系管理**（CRM）是现代营销策略的关键要素。企业面临的最大挑战之一是了解确切是什么原因导致客户购买新产品。
- en: 'In this chapter, we will work on a real-world marketing database provided by
    the French telecom company, Orange. The task will be to estimate the likelihood
    of the following customer actions:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用法国电信公司Orange提供的真实世界营销数据库。任务将是估计以下客户行为的可能性：
- en: Switch provider (churn)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换供应商（流失）
- en: Buy new products or services (appetency)
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 购买新产品或服务（需求）
- en: Buy upgrades or add-ons proposed to them to make the sale more profitable (upselling)
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向他们推荐升级或附加产品以使销售更有利可图（升级销售）
- en: We will tackle the **Knowledge Discovery and Data Mining** (**KDD**) Cup 2009
    challenge and show the steps to process the data using Weka. First, we will parse
    and load the data and implement the basic baseline models. Later, we will address
    advanced modeling techniques, including data preprocessing, attribute selection,
    model selection, and evaluation.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将解决2009年KDD杯挑战，并展示使用Weka处理数据的步骤。首先，我们将解析和加载数据并实现基本基线模型。随后，我们将处理高级建模技术，包括数据预处理、属性选择、模型选择和评估。
- en: The KDD Cup is the leading data mining competition in the world. It is organized
    annually by the ACM **Special Interest Group on Knowledge Discovery and Data Mining**.
    The winners are announced at the Conference on Knowledge Discovery and Data Mining,
    which is usually held in August. Yearly archives, including all of the corresponding
    datasets, are available at [http://www.kdd.org/kdd-cup](http://www.kdd.org/kdd-cup).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: KDD杯是全球领先的数据挖掘竞赛。它由ACM**知识发现与数据挖掘特别兴趣小组**每年组织。获胜者通常在8月份举行的**知识发现与数据挖掘会议**上宣布。包括所有对应数据集的年度存档可在[http://www.kdd.org/kdd-cup](http://www.kdd.org/kdd-cup)找到。
- en: The customer relationship database
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户关系数据库
- en: The most practical way to build knowledge on customer behavior is to produce
    scores that explain a target variable, such as churn, appetency, or upselling.
    The score is computed by a model using input variables that describe customers;
    for example, their current subscription, purchased devices, consumed minutes,
    and so on. The scores are then used by the information system for things like
    providing relevant personalized marketing actions.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 建立关于客户行为知识的最实用方式是产生解释目标变量（如流失、需求或升级销售）的分数。该分数是通过使用描述客户的输入变量（例如，他们的当前订阅、购买的设备、消耗的分钟数等）的模型计算得出的。然后，这些分数被信息系统用于提供相关个性化营销行动等活动。
- en: A customer is the main entity in most of the customer-based relationship databases;
    getting to know the customer's behavior is important. The customer's behavior
    produces a score in relation to the churn, appetency, or upselling. The basic
    idea is to produce a score using a computational model, which may use different
    parameters, such as the current subscription of the customer, devices purchased,
    minutes consumed, and so on. Once the score is formed, it is used by the information
    system to decide on the next strategy, which is especially designed for the customer,
    based on his or her behavior.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 客户是大多数基于客户的关系数据库中的主要实体；了解客户的行为很重要。客户的行为会在流失、需求或升级销售方面产生一个分数。基本思想是使用计算模型产生一个分数，该模型可能使用不同的参数，例如客户的当前订阅、购买的设备、消耗的分钟数等。一旦分数形成，它就会被信息系统用来决定针对客户行为的下一个策略，该策略特别设计用于客户。
- en: In 2009, the conference on KDD organized a machine learning challenge on customer
    relationship prediction.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 2009年，KDD会议组织了一次关于客户关系预测的机器学习挑战。
- en: Challenge
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 挑战
- en: 'Given a large set of customer attributes, the task in the challenge was to
    estimate the following target variables:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一组大量的客户属性，挑战中的任务是估计以下目标变量：
- en: '**Churn probability**: This is the likelihood that a customer will switch providers.
    The churn rate is also known as the attrition rate or the participant turnover
    rate, and is a measure used to find the number of individuals, objects, terms,
    or items moving into or out of a given collection, over a given time period. The
    term is heavily used in industries that are driven by customers and use subscriber-based
    models; for example, the cell phone industry and cable TV operators.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户流失概率**：这是客户更换提供商的可能性。客户流失率也被称为流失率或参与者周转率，是用于计算在给定时间段内进入或离开给定集合的个人、对象、术语或项目数量的度量。这个术语在以客户驱动和基于订阅者模型的行业中广泛使用；例如，手机行业和有线电视运营商。'
- en: '**Appetency probability**: This is the propensity to buy a service or product.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**购买意愿概率**：这是购买服务或产品的倾向性。'
- en: '**Upselling probability**: This is the likelihood that a customer will buy
    an add-on or upgrade. Upselling implies selling something in addition to what
    the customer is already using. Consider it like the value-added services that
    are provided by most cell phone operators. Using sales techniques, salesmen try
    to make customers opt for value-added services, which will bring more revenue.
    Many times, customers are not aware of other options, and the salesmen convince
    them to use or consider those options.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级销售概率**：这是客户购买附加产品或升级的可能性。升级销售意味着在客户已经使用的产品之外销售其他产品。可以将其视为大多数手机运营商提供的有增值服务。销售人员使用销售技巧试图让客户选择增值服务，这将带来更多收入。很多时候，客户并不了解其他选项，销售人员会说服他们使用或考虑这些选项。'
- en: The challenge was to beat the in-house system developed by Orange Labs. This
    was an opportunity for the participants to prove that they could handle a large
    database, including heterogeneous, noisy data, and unbalanced class distributions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战的目标是击败Orange Labs开发的内部系统。这是参与者证明他们能够处理大型数据库的机会，包括异构、噪声数据和不平衡的类别分布。
- en: Dataset
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: For the challenge, Orange released a large dataset of customer data, containing
    about one million customers, described in ten tables with hundreds of fields.
    In the first step, they resampled the data to select a less unbalanced subset,
    containing 100,000 customers. In the second step, they used an automatic feature
    construction tool that generated 20,000 features describing the customers, which
    was then narrowed down to 15,000 features. In the third step, the dataset was
    anonymized by randomizing the order of features, discarding the attribute names,
    replacing the nominal variables with randomly generated strings, and multiplying
    the continuous attributes by a random factor. Finally, all of the instances were
    split randomly into training and testing datasets.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个挑战，Orange发布了一个包含大约一百万客户的大型客户数据集，这些数据被描述在十个表格中，每个表格包含数百个字段。在第一步中，他们重新采样数据以选择一个更平衡的子集，包含10万名客户。在第二步中，他们使用了一个自动特征构建工具，生成了描述客户的20,000个特征，然后缩减到15,000个特征。在第三步中，通过随机化特征顺序、丢弃属性名称、用随机生成的字符串替换名义变量以及将连续属性乘以随机因子来匿名化数据集。最后，所有实例都被随机分成训练集和测试集。
- en: The KDD Cup provided two sets of data, a large set and a small set, corresponding
    to fast and slow challenges, respectively. Both the training and testing sets
    contained 50,000 examples, and the data was split similarly, but the samples were
    ordered differently for each set.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: KDD杯提供了两组数据，一组是大数据集，另一组是小数据集，分别对应快速和慢速挑战。训练集和测试集都包含50,000个示例，数据分割方式相似，但每个集合的样本顺序不同。
- en: In this chapter, we will work with the small dataset, consisting of 50,000 instances,
    each described with 230 variables. Each of the 50,000 rows of data corresponds
    to a client, and they are associated with three binary outcomes, one for each
    of the three challenges (upselling, churn, and appetency).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用包含50,000个实例的小型数据集，每个实例由230个变量描述。这50,000行数据对应于一个客户，并且它们与三个二进制结果相关联，每个结果对应于三个挑战（升级销售、客户流失和购买意愿）。
- en: 'To make this clearer, the following table illustrates the dataset:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这一点更清晰，以下表格展示了数据集：
- en: '![](img/fda8bb7a-606b-453b-9097-f8d72776f4eb.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fda8bb7a-606b-453b-9097-f8d72776f4eb.png)'
- en: The table depicts the first 25 instances, that is, customers, each described
    with 250 attributes. For this example, only a selected subset of 10 attributes
    is shown. The dataset contains many missing values, and even empty or constant
    attributes. The last three columns of the table correspond to the three distinct
    class labels involving the ground truth, that is, if the customer indeed switched
    providers (churn), bought a service (appetency), or bought an upgrade (upsell).
    However, note that the labels are provided separately from the data in three distinct
    files, hence it is essential to retain the order of the instances and the corresponding
    class labels to ensure proper correspondence.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 表格展示了前25个实例，即客户，每个客户都描述了250个属性。在这个例子中，只展示了10个属性的子集。数据集包含许多缺失值，甚至有空值或常量属性。表格的最后三列对应于三个不同的类别标签，涉及地面真相，即客户是否真的更换了服务提供商（客户流失），购买了服务（需求），或购买了升级（升级销售）。然而，请注意，标签是分别从三个不同的文件中提供的，因此保留实例和相应类别标签的顺序对于确保适当的对应关系至关重要。
- en: Evaluation
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估
- en: The submissions were evaluated according to the arithmetic mean of the area
    under the ROC curve for the three tasks (churn, appetency, and upselling). The
    ROC curve shows the performance of the model as a curve obtained by plotting the
    sensitivity against specificity for various threshold values used to determine
    the classification result (refer to [Chapter 1](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml),
    *Applied Machine Learning Quick Start*, in the section *ROC curves*). Now, the
    **area under the ROC** **curve** (**AUC**) is related to the area under this curve
    – the larger the area, the better the classifier). Most toolboxes, including Weka,
    provide an API to calculate the AUC score.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提交是根据三个任务（客户流失、需求和升级销售）的ROC曲线下面积（AUC）的算术平均值进行评估的。ROC曲线显示了模型性能，是通过绘制用于确定分类结果的阈值的各种敏感度对特异性进行绘图得到的曲线（参见[第1章](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml)，*应用机器学习快速入门*，*ROC曲线*部分）。现在，**ROC曲线下的面积**（AUC）与该曲线下的面积相关——面积越大，分类器越好。大多数工具箱，包括Weka，都提供了一个API来计算AUC得分。
- en: Basic Naive Bayes classifier baseline
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本朴素贝叶斯分类器基线
- en: As per the rules of the challenge, the participants had to outperform the basic
    Naive Bayes classifier in order to qualify for prizes, which makes an assumption
    that features are independent (refer to [Chapter 1](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml),
    *Applied Machine Learning Quick Start*).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 根据挑战赛的规则，参与者必须超越基本的朴素贝叶斯分类器才能有资格获奖，这假设特征是独立的（参见[第1章](11a9489b-c4dd-4544-ace8-f84533d8fd7c.xhtml)，*应用机器学习快速入门*）。
- en: 'The KDD Cup organizers ran the vanilla Naive Bayes classifier, without any
    feature selection or hyperparameter adjustments. For the large dataset, the overall
    scores of the Naive Bayes on the test set were as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: KDD Cup组织者运行了基本的朴素贝叶斯分类器，没有进行任何特征选择或超参数调整。对于大数据集，朴素贝叶斯在测试集上的总体分数如下：
- en: '**Churn problem**: AUC = 0.6468'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**客户流失问题**: AUC = 0.6468'
- en: '**Appetency problem**: AUC = 0.6453'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需求问题**: AUC = 0.6453'
- en: '**Upselling problem**: AUC=0.7211'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**升级销售问题**: AUC=0.7211'
- en: Note that the baseline results are only reported for the large dataset. Moreover,
    while both the training and testing datasets are provided at the KDD Cup site,
    the actual true labels for the test set are not provided. Therefore, when we process
    the data with our models, there is no way to know how well the models will perform
    on the test set. What we will do is only use the training data, and evaluate our
    models with cross-validation. The results will not be directly comparable, but
    nevertheless, we will have an idea about what a reasonable magnitude of the AUC
    score should be.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，基线结果仅报告了大数据集。此外，虽然训练集和测试集都提供在KDD Cup网站上，但测试集的实际真实标签并未提供。因此，当我们用我们的模型处理数据时，我们无法知道模型在测试集上的表现如何。我们将只使用训练数据，并通过交叉验证评估我们的模型。结果将不可直接比较，但无论如何，我们将对AUC得分的合理幅度有一个概念。
- en: Getting the data
  id: totrans-34
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'At the KDD Cup web page ([http://kdd.org/kdd-cup/view/kdd-cup-2009/Data](http://kdd.org/kdd-cup/view/kdd-cup-2009/Data)),
    you should see a page that looks similar to the following screenshot. First, under
    the Small version (230 var.) header, download `orange_small_train.data.zip`. Next,
    download the three sets of true labels associated with this training data. The
    following files are found under the Real binary targets (small) header:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在KDD Cup网页([http://kdd.org/kdd-cup/view/kdd-cup-2009/Data](http://kdd.org/kdd-cup/view/kdd-cup-2009/Data))上，你应该会看到一个类似于以下截图的页面。首先，在“Small版本（230变量）”标题下，下载`orange_small_train.data.zip`。接下来，下载与该训练数据相关的三组真实标签。以下文件位于“真实二进制目标（小型）”标题下：
- en: '`orange_small_train_appentency.labels`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orange_small_train_appentency.labels`'
- en: '`orange_small_train_churn.labels`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orange_small_train_churn.labels`'
- en: '`orange_small_train_upselling.labels`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`orange_small_train_upselling.labels`'
- en: 'Save and unzip all of the files marked in the red boxes, as shown in the screenshot:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 保存并解压截图中的红色框内标记的所有文件：
- en: '![](img/80d5b9c9-2bfa-43a1-9351-51a883eae104.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/80d5b9c9-2bfa-43a1-9351-51a883eae104.png)'
- en: In the following sections, first, we will load the data into Weka and apply
    basic modeling with the Naive Bayes classifier, in order to obtain our own baseline
    AUC scores. Later, we will look at more advanced modeling techniques and tricks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，首先，我们将加载数据到Weka中，并使用朴素贝叶斯分类器进行基本建模，以获得我们自己的基线AUC分数。稍后，我们将探讨更高级的建模技术和技巧。
- en: Loading the data
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加载数据
- en: 'We will load the data to Weka directly from the `.csv` format. For this purpose,
    we will write a function that accepts the path to the data file and the true labels
    file. The function will load and merge both datasets and remove empty attributes.
    We will begin with the following code block:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将直接从`.csv`格式将数据加载到Weka中。为此，我们将编写一个函数，该函数接受数据文件路径和真实标签文件路径。该函数将加载和合并两个数据集，并删除空属性。我们将从以下代码块开始：
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'First, we load the data using the `CSVLoader()` class. Additionally, we specify
    the `\t` tab as a field separator and force the last 40 attributes to be parsed
    as nominal:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`CSVLoader()`类加载数据。此外，我们指定`\t`制表符作为字段分隔符，并强制将最后40个属性解析为名义属性：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The `CSVLoader` class accepts many additional parameters, specifying the column
    separator, string enclosures, whether a header row is present, and so on. The
    complete documentation is available at [http://weka.sourceforge.net/doc.dev/weka/core/converters/CSVLoader.html](http://weka.sourceforge.net/doc.dev/weka/core/converters/CSVLoader.html).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '`CSVLoader`类接受许多其他参数，指定列分隔符、字符串定界符、是否存在标题行等。完整的文档可在[http://weka.sourceforge.net/doc.dev/weka/core/converters/CSVLoader.html](http://weka.sourceforge.net/doc.dev/weka/core/converters/CSVLoader.html)找到。'
- en: 'Some of the attributes do not contain a single value, and Weka automatically
    recognizes them as `String` attributes. We actually do not need them, so we can
    safely remove them by using the `RemoveType` filter. Additionally, we specify
    the `-T` parameters, which removes an attribute of a specific type and specifies
    the attribute type that we want to remove:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一些属性不包含单个值，Weka会自动将它们识别为`String`属性。实际上我们不需要它们，因此我们可以安全地使用`RemoveType`过滤器删除它们。此外，我们指定了`-T`参数，该参数删除特定类型的属性并指定我们想要删除的属性类型：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Alternatively, we could use the `void deleteStringAttributes()` method, implemented
    within the `Instances` class, which has the same effect; for example, `data.removeStringAttributes()`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以使用`Instances`类中实现的`void deleteStringAttributes()`方法，它具有相同的效果；例如，`data.removeStringAttributes()`。
- en: 'Now, we will load and assign class labels to the data. We will utilize `CVSLoader`
    again, where we specify that the file does not have any header line, that is,
    `setNoHeaderRowPresent(true)`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将加载数据并分配类标签。我们将再次使用`CVSLoader`，其中我们指定文件没有标题行，即`setNoHeaderRowPresent(true)`：
- en: '[PRE3]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Once we have loaded both files, we can merge them together by calling the `Instances.mergeInstances
    (Instances, Instances)` static method. The method returns a new dataset that has
    all of the attributes from the first dataset, plus the attributes from the second
    set. Note that the number of instances in both datasets must be the same:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 加载完两个文件后，我们可以通过调用`Instances.mergeInstances (Instances, Instances)`静态方法将它们合并在一起。该方法返回一个新的数据集，它包含第一个数据集的所有属性，以及第二个集合的属性。请注意，两个数据集中的实例数量必须相同：
- en: '[PRE4]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we set the last attribute, that is, the label attribute that we just
    added, as a target variable, and return the resulting dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将最后一个属性，即我们刚刚添加的标签属性，设置为目标变量，并返回结果数据集：
- en: '[PRE5]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The function provides a summary as output, as shown in the following code block,
    and returns the labeled dataset:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 函数提供输出摘要，如下面的代码块所示，并返回标记的数据集：
- en: '[PRE6]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Basic modeling
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本建模
- en: In this section, we will implement our own baseline model by following the approach
    that the KDD Cup organizers took. However, before we get to the model, let's first
    implement the evaluation engine that will return the AUC on all three problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将按照KDD Cup组织者采取的方法实现我们自己的基线模型。然而，在我们到达模型之前，让我们首先实现评估引擎，该引擎将返回所有三个问题的AUC。
- en: Evaluating models
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估模型
- en: 'Now, let''s take a closer look at the evaluation function. The evaluation function
    accepts an initialized model, cross-validates the model on all three problems,
    and reports the results as an area under the ROC curve (AUC), as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更仔细地看看评估函数。评估函数接受一个初始化的模型，在所有三个问题上对模型进行交叉验证，并报告结果为ROC曲线下的面积（AUC），如下所示：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'First, we call the `Instance loadData(String, String)` function that we implemented
    earlier to load the training data and merge it with the selected labels:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们调用我们之前实现的`Instance loadData(String, String)`函数来加载数据并与其选定的标签合并：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we initialize the `weka.classifiers.Evaluation` class and pass our dataset.
    (The dataset is only used to extract data properties; the actual data is not considered.)
    We call the `void crossValidateModel(Classifier, Instances, int, Random)` method
    to begin cross-validation, and we create five folds. As validation is done on
    random subsets of the data, we need to pass a random seed, as well:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们初始化`weka.classifiers.Evaluation`类并传递我们的数据集。（数据集仅用于提取数据属性；实际数据不考虑。）我们调用`void
    crossValidateModel(Classifier, Instances, int, Random)`方法开始交叉验证，并创建五个折。由于验证是在数据的随机子集中进行的，我们需要传递一个随机种子：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'After the evaluation completes, we read the results by calling the `double
    areUnderROC(int)` method. As the metric depends on the target value that we are
    interested in, the method expects a class value index, which can be extracted
    by searching the index of the `"1"` value in the class attribute, as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 评估完成后，我们通过调用`double areUnderROC(int)`方法读取结果。由于该指标依赖于我们感兴趣的靶值，该方法期望一个类别值索引，可以通过在类别属性中搜索`"1"`值的索引来提取，如下所示：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, the results are averaged and returned:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，结果被平均并返回：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Implementing the Naive Bayes baseline
  id: totrans-72
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现朴素贝叶斯基线
- en: Now, when we have all of the ingredients, we can replicate the Naive Bayes approach
    that we are expected to outperform. This approach will not include any additional
    data preprocessing, attribute selection, or model selection. As we do not have
    true labels for the test data, we will apply five-fold cross-validation to evaluate
    the model on a small dataset.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们有了所有这些原料时，我们可以复制我们期望超越的朴素贝叶斯方法。这种方法将不包括任何额外的数据预处理、属性选择或模型选择。由于我们没有测试数据的真实标签，我们将应用五折交叉验证来评估模型在小数据集上的性能。
- en: 'First, we initialize a Naive Bayes classifier, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们初始化一个朴素贝叶斯分类器，如下所示：
- en: '[PRE12]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Next, we pass the classifier to our evaluation function, which loads the data
    and applies cross-validation. The function returns an area under the ROC curve
    score for all three problems, and the overall results:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将分类器传递给我们的评估函数，该函数加载数据并应用交叉验证。该函数返回所有三个问题的ROC曲线下的面积分数和总体结果：
- en: '[PRE13]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In our case, the model returns the following results:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，模型返回以下结果：
- en: '[PRE14]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: These results will serve as a baseline when we tackle the challenge with more
    advanced modeling. If we process the data with significantly more sophisticated,
    time-consuming, and complex techniques, we expect the results to be much better.
    Otherwise, we are simply wasting resources. In general, when solving machine learning
    problems, it is always a good idea to create a simple baseline classifier that
    serves us as an orientation point.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果将作为我们处理更高级建模挑战时的基线。如果我们使用显著更复杂、耗时和复杂的技术处理数据，我们期望结果会更好。否则，我们只是在浪费资源。一般来说，在解决机器学习问题时，创建一个简单的基线分类器作为我们的定位点是件好事。
- en: Advanced modeling with ensembles
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用集成进行高级建模
- en: In the previous section, we implemented an orientation baseline; now, let's
    focus on heavy machinery. We will follow the approach taken by the KDD Cup 2009
    winning solution, developed by the IBM research team (Niculescu-Mizil and others).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们实现了一个定位基线；现在，让我们专注于重型机械。我们将遵循KDD Cup 2009获奖解决方案的方法，该方案由IBM研究团队（Niculescu-Mizil等人）开发。
- en: 'To address this challenge, they used the ensemble selection algorithm (Caruana
    and Niculescu-Mizil, 2004). This is an ensemble method, which means it constructs
    a series of models and combines their output in a specific way, in order to provide
    the final classification. It has several desirable properties that make it a good
    fit for this challenge, as follows:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这一挑战，他们使用了集成选择算法（Caruana和Niculescu-Mizil，2004）。这是一种集成方法，意味着它构建了一系列模型，并以特定的方式组合它们的输出，以提供最终的分类。它具有几个理想的特性，使其非常适合这一挑战，如下所示：
- en: It was proven to be robust, yielding excellent performance.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这已被证明是稳健的，性能卓越。
- en: It can be optimized for a specific performance metric, including AUC.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以针对特定的性能指标进行优化，包括AUC。
- en: It allows for different classifiers to be added to the library.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许向库中添加不同的分类器。
- en: It is an anytime method, meaning that if we run out of time, we have a solution
    available.
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是一种任何时间方法，这意味着如果我们用完时间，我们有一个可用的解决方案。
- en: In this section, we will loosely follow the steps as they are described in their
    report. Note that this is not an exact implementation of their approach, but rather
    a solution overview that will include the necessary steps to dive deeper.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将大致遵循他们在报告中描述的步骤。请注意，这并不是他们方法的精确实现，而是一个包括深入探索所需步骤的解决方案概述。
- en: 'A general overview of the steps is as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤的一般概述如下：
- en: First, we will preprocess the data by removing attributes that clearly do not
    bring any value – for example, all of the missing or constant values; fixing missing
    values, in order to help machine learning algorithms, which cannot deal with them;
    and converting categorical attributes to numerical attributes.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们将通过移除明显不会带来任何价值的属性来预处理数据——例如，所有缺失或常量值；修复缺失值，以便帮助机器学习算法，因为它们无法处理这些值；以及将分类属性转换为数值属性。
- en: Next, we will run the attribute selection algorithm to select only a subset
    of attributes that can help in the prediction of tasks.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将运行属性选择算法，仅选择有助于预测任务的属性子集。
- en: In the third step, we will instantiate the ensemble selection algorithms with
    a wide variety of models, and finally, we will evaluate the performance.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在第三步，我们将使用各种模型实例化集成选择算法，最后我们将评估性能。
- en: Before we start
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在开始之前
- en: 'For this task, we will need an additional Weka package, `ensembleLibrary`.
    Weka 3.7.2 and higher versions support external packages, mainly developed by
    the academic community. A list of WEKA Packages is available at [http://weka.sourceforge.net/packageMetaData](http://weka.sourceforge.net/packageMetaData),
    as shown in the following screenshot:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们需要一个额外的Weka包，`ensembleLibrary`。Weka 3.7.2及以上版本支持外部包，主要由学术社区开发。Weka包的列表可在[http://weka.sourceforge.net/packageMetaData](http://weka.sourceforge.net/packageMetaData)找到，如下截图所示：
- en: '![](img/fbd6a8f5-6251-4690-a3f0-014f6baab657.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/fbd6a8f5-6251-4690-a3f0-014f6baab657.png)'
- en: Find and download the latest available version of the `ensembleLibrary` package
    at [http://prdownloads.sourceforge.net/weka/ensembleLibrary1.0.5.zip?download](http://prdownloads.sourceforge.net/weka/ensembleLibrary1.0.5.zip?download).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在[http://prdownloads.sourceforge.net/weka/ensembleLibrary1.0.5.zip?download](http://prdownloads.sourceforge.net/weka/ensembleLibrary1.0.5.zip?download)找到并下载`ensembleLibrary`包的最新可用版本。
- en: 'After you unzip the package, locate `ensembleLibrary.jar` and import it into
    your code, as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在解压包后，找到`ensembleLibrary.jar`并将其导入到您的代码中，如下所示：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Data preprocessing
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'First, we will utilize Weka''s built-in `weka.filters.unsupervised.attribute.RemoveUseless`
    filter, which works exactly as its name suggests. It removes the attributes that
    do not vary much, for instance, all constant attributes are removed. The maximum
    variance, which is only applied to nominal attributes, is specified with the `-M`
    parameter. The default parameter is 99%, which means that if more than 99% of
    all instances have unique attribute values, the attribute is removed, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将利用Weka内置的`weka.filters.unsupervised.attribute.RemoveUseless`过滤器，它的工作方式正如其名称所暗示的那样。它移除变化不大的属性，例如，所有常量属性都被移除。最大方差（仅适用于名义属性）由`-M`参数指定。默认参数是99%，这意味着如果所有实例中有超过99%具有唯一的属性值，则该属性将被移除，如下所示：
- en: '[PRE16]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we will replace all of the missing values in the dataset with the modes
    (nominal attributes) and means (numeric attributes) from the training data, by
    using the `weka.filters.unsupervised.attribute.ReplaceMissingValues` filter. In
    general, missing value replacement should be proceeded with caution, while taking
    into consideration the meaning and context of the attributes:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`weka.filters.unsupervised.attribute.ReplaceMissingValues`过滤器，将数据集中的所有缺失值替换为训练数据中的众数（名义属性）和均值（数值属性）。一般来说，在替换缺失值时应谨慎行事，同时考虑属性的意义和上下文：
- en: '[PRE17]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Finally, we will discretize numeric attributes, that is, we will transform
    numeric attributes into intervals by using the `weka.filters.unsupervised.attribute.Discretize`
    filter. With the `-B` option, we set splitting numeric attributes into four intervals,
    and the `-R` option specifies the range of attributes (only numeric attributes
    will be discretized):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用`weka.filters.unsupervised.attribute.Discretize`过滤器对数值属性进行离散化，即通过该过滤器将数值属性转换为区间。使用`-B`选项，我们将数值属性分割成四个区间，而`-R`选项指定了属性的取值范围（只有数值属性将被离散化）：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Attribute selection
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 属性选择
- en: 'In the next step, we will select only informative attributes, that is, attributes
    that are more likely to help with prediction. A standard approach to this problem
    is to check the information gain carried by each attribute. We will use the `weka.attributeSelection.AttributeSelection`
    filter, which requires two additional methods: an evaluator (how attribute usefulness
    is calculated) and search algorithms (how to select a subset of attributes).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一步中，我们将仅选择具有信息量的属性，即更有可能帮助预测的属性。解决此问题的标准方法是检查每个属性携带的信息增益。我们将使用`weka.attributeSelection.AttributeSelection`过滤器，该过滤器需要两个额外的方法：一个评估器（如何计算属性的有用性）和搜索算法（如何选择属性子集）。
- en: 'In our case, first, we initialize `weka.attributeSelection.InfoGainAttributeEval`,
    which implements the calculation of information gain:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，首先，我们初始化`weka.attributeSelection.InfoGainAttributeEval`，该类实现了信息增益的计算：
- en: '[PRE19]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To only select the top attributes above a threshold, we initialize `weka.attributeSelection.Ranker`,
    in order to rank the attributes with information gain above a specific threshold.
    We specify this with the `-T` parameter, while keeping the value of the threshold
    low, in order to keep the attributes with at least some information:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 为了只选择高于阈值的顶级属性，我们初始化`weka.attributeSelection.Ranker`，以便根据信息增益对属性进行排序，信息增益高于特定阈值。我们使用`-T`参数指定此阈值，同时保持阈值较低，以保留至少包含一些信息的属性：
- en: '[PRE20]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The general rule for setting this threshold is to sort the attributes by information
    gain and pick the threshold where the information gain drops to a negligible value.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 设置此阈值的通用规则是按信息增益对属性进行排序，并选择信息增益降至可忽略值时的阈值。
- en: 'Next, we can initialize the `AttributeSelection` class, set the evaluator and
    ranker, and apply the attribute selection to our dataset, as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以初始化`AttributeSelection`类，设置评估器和排序器，并将属性选择应用于我们的数据集，如下所示：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Finally, we remove the attributes that were not selected in the last run by
    calling the `reduceDimensionality(Instances)` method:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，通过调用`reduceDimensionality(Instances)`方法，我们移除了上一次运行中未选择的属性：
- en: '[PRE22]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: In the end, we are left with 214 out of 230 attributes.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们保留了230个属性中的214个。
- en: Model selection
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型选择
- en: Over the years, practitioners in the field of machine learning have developed
    a wide variety of learning algorithms and improvements for existing ones. There
    are so many unique supervised learning methods that it is challenging to keep
    track of all of them. As the characteristics of the datasets vary, no one method
    is the best in all of the cases, but different algorithms are able to take advantage
    of the different characteristics and relationships of a given dataset.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，机器学习领域的从业者已经开发了许多学习算法和现有算法的改进。有如此多的独特监督学习方法，以至于难以跟踪所有这些方法。由于数据集的特征各异，没有一种方法在所有情况下都是最好的，但不同的算法能够利用给定数据集的不同特征和关系。
- en: 'First, we need to create the model library by initializing the `weka.classifiers.EnsembleLibrary`
    class, which will help us define the models:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过初始化`weka.classifiers.EnsembleLibrary`类来创建模型库，这将帮助我们定义模型：
- en: '[PRE23]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we add the models and their parameters to the library as string values;
    for example, we can add three decision tree learners with different parameters,
    as follows:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将模型及其参数作为字符串值添加到库中；例如，我们可以添加三个具有不同参数的决策树学习器，如下所示：
- en: '[PRE24]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'If you are familiar with the Weka graphical interface, you can also explore
    the algorithms and their configurations there and copy the configuration, as shown
    in the following screenshot. Right-click on the algorithm name and navigate to
    Edit configuration | Copy configuration string:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你熟悉Weka图形界面，你还可以在那里探索算法及其配置，并复制配置，如下面的截图所示。右键单击算法名称，导航到编辑配置 | 复制配置字符串：
- en: '![](img/0824df77-7948-4eea-bc5a-dcf2f64fe21e.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0824df77-7948-4eea-bc5a-dcf2f64fe21e.png)'
- en: 'To complete this example, we added the following algorithms and their parameters:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个示例，我们添加了以下算法及其参数：
- en: 'The Naive Bayes that was used as the default baseline:'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为默认基线使用的朴素贝叶斯：
- en: '[PRE25]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The k-nearest neighbors, based on lazy models:'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于懒惰模型的k近邻算法：
- en: '[PRE26]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Logistic regression as a simple logistic with default parameters:'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为简单逻辑回归默认参数的逻辑回归：
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Support vector machines with default parameters:'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认参数的支持向量机：
- en: '[PRE28]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '`AdaBoost`, which is, in itself, an ensemble method:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本身就是集成方法的`AdaBoost`：
- en: '[PRE29]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '`LogitBoost`, an ensemble method based on logistic regression:'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于逻辑回归的集成方法`LogitBoost`：
- en: '[PRE30]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`DecisionStump`, an ensemble method based on one-level decision trees:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于单层决策树的集成方法`DecisionStump`：
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'As the `EnsembleLibrary` implementation is primarily focused on GUI and console
    users, we have to save the models into a file by calling the `saveLibrary(File,
    EnsembleLibrary, JComponent)` method, as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`EnsembleLibrary`实现主要针对GUI和控制台用户，我们必须通过调用`saveLibrary(File, EnsembleLibrary,
    JComponent)`方法将模型保存到文件中，如下所示：
- en: '[PRE32]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we can initialize the ensemble selection algorithm by instantiating the
    `weka.classifiers.meta.EnsembleSelection` class. First, let''s review the following
    method options:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过实例化`weka.classifiers.meta.EnsembleSelection`类来初始化集成选择算法。首先，让我们回顾以下方法选项：
- en: '`-L </path/to/modelLibrary>`: This specifies the `modelLibrary` file, continuing
    the list of all models.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-L </path/to/modelLibrary>`: 这指定了`modelLibrary`文件，继续列出所有模型。'
- en: '`-W </path/to/working/directory>`: This specifies the working directory, where
    all models will be stored.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-W </path/to/working/directory>`: 这指定了工作目录，所有模型都将保存在这里。'
- en: '`-B <numModelBags>`: This sets the number of bags, that is, the number of iterations
    to run the ensemble selection algorithm.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-B <numModelBags>`: 这设置了袋的数量，即运行集成选择算法的迭代次数。'
- en: '`-E <modelRatio>`: This sets the ratio of library models that will be randomly
    chosen to populate each bag of models.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-E <modelRatio>`: 这设置了随机选择填充每个模型包的库模型的比例。'
- en: '`-V <validationRatio>`: This sets the ratio of the training dataset that will
    be reserved for validation.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-V <validationRatio>`: 这设置了保留用于验证的训练数据集的比例。'
- en: '`-H <hillClimbIterations>`: This sets the number of hill climbing iterations
    to be performed on each model bag.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-H <hillClimbIterations>`: 这设置了在每个模型包上要执行的爬山迭代次数。'
- en: '`-I <sortInitialization>`: This sets the ratio of the ensemble library that
    the sort initialization algorithm will be able to choose from, while initializing
    the ensemble for each model bag.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-I <sortInitialization>`: 这设置了排序初始化算法在初始化每个模型包时可以选择的集成库的比例。'
- en: '`-X <numFolds>`: This sets the number of cross-validation folds.'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-X <numFolds>`: 这设置了交叉验证的折数。'
- en: '`-P <hillclimbMetric>`: This specifies the metric that will be used for model
    selection during the hill climbing algorithm. Valid metrics include the accuracy,
    rmse, roc, precision, recall, fscore, and all.'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-P <hillclimbMetric>`: 这指定了在爬山算法中用于模型选择的度量标准。有效的度量包括准确率、rmse、roc、精确率、召回率、fscore以及所有这些。'
- en: '`-A <algorithm>`: This specifies the algorithm to be used for ensemble selection.
    Valid algorithms include forward (default) for forward selection, backward for
    backward elimination, both for both forward and backward elimination, best to
    simply print the top performer from the ensemble library, and library to only
    train the models in the ensemble library.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-A <algorithm>`: 这指定了用于集成选择的算法。有效的算法包括forward（默认）用于前向选择，backward用于后向消除，both用于前向和后向消除，best用于简单地从集成库中打印出表现最好的模型，以及library仅训练集成库中的模型。'
- en: '`-R`: This flags whether the models can be selected more than once for an ensemble.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-R`: 这标志是否可以将模型多次选入一个集成。'
- en: '`-G`: This states whether the sort initialization greedily stops adding models
    when the performance degrades.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-G`: 这表示排序初始化是否在性能下降时贪婪地停止添加模型。'
- en: '`-O`: This is a flag for verbose output. This prints the performance of all
    of the selected models.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-O`: 这是一个用于详细输出的标志。这会打印出所有选定模型的性能。'
- en: '`-S <num>`: This is a random number seed (the default is `1`).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-S <num>`: 这是一个随机数种子（默认为`1`）。'
- en: '`-D`: If set, the classifier is run in debug mode, and may provide additional
    information to the console as output.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-D`: 如果设置，分类器将以调试模式运行，并且可能向控制台提供额外的输出信息。'
- en: 'We initialize the algorithm with the following initial parameters, where we
    specify optimizing the ROC metric:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下初始参数初始化算法，其中我们指定优化ROC指标：
- en: '[PRE33]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Performance evaluation
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能评估
- en: The evaluation is heavy, both computationally and memory-wise, so make sure
    that you initialize the JVM with extra heap space (for instance, `java -Xmx16g`).
    The computation can take a couple of hours or days, depending on the number of
    algorithms that you include in the model library. This example took 4 hours and
    22 minutes on a 12-core Intel Xeon E5-2420 CPU with 32 GB of memory and utilizing
    10% CPU and 6 GB of memory on average.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 评估在计算和内存方面都很重，所以请确保你初始化JVM时带有额外的堆空间（例如，`java -Xmx16g`）。计算可能需要几个小时或几天，具体取决于你包含在模型库中的算法数量。这个例子在一个12核心的Intel
    Xeon E5-2420 CPU上，32 GB的内存，平均使用了10%的CPU和6 GB的内存，耗时4小时22分钟。
- en: 'We call our evaluation method and provide the results as output, as follows:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们称我们的评估方法并提供结果作为输出，如下所示：
- en: '[PRE34]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The specific set of classifiers in the model library achieved the following
    result:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 模型库中的特定分类器集达到了以下结果：
- en: '[PRE35]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Overall, the approach has brought us to a significant improvement of more than
    15 percentage points, compared to the initial baseline that we designed at the
    beginning of this chapter. While it is hard to give a definite answer, the improvement
    was mainly due to three factors: data preprocessing and attribute selection, the
    exploration of a large variety of learning methods, and the use of an ensemble-building
    technique that is able to take advantage of the variety of base classifiers without
    overfitting. However, the improvement requires a significant increase in processing
    time, as well as working memory.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，这种方法使我们相比本章开头设计的初始基线有了超过15个百分点的显著改进。虽然很难给出一个明确的答案，但改进主要归因于三个因素：数据预处理和属性选择、探索大量不同的学习方法，以及使用一种能够利用多种基分类器而不过度拟合的集成构建技术。然而，这种改进需要显著增加处理时间以及工作内存。
- en: Ensemble methods – MOA
  id: totrans-168
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成方法 – MOA
- en: To ensemble, as the word suggests, is to view together, or at the same time.
    It is used to combine multiple learner algorithms, in order to obtain better results
    and performance. There are various techniques that you can use for an ensemble.
    Some commonly used ensemble techniques or classifiers include bagging, boosting,
    stacking, a bucket of models, and so on.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 如同其词意，集成就是一起查看，或者同时进行。它用于结合多个学习算法，以获得更好的结果和性能。你可以使用各种技术进行集成。一些常用的集成技术或分类器包括袋装、提升、堆叠、模型桶等。
- en: '**Massive Online Analysis** (**MOA**) supports ensemble classifiers, such as
    accuracy weighted ensembles, accuracy updated ensembles, and many more. In this
    section, we will show you how to use the leveraging bagging algorithm:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '**大规模在线分析**（**MOA**）支持集成分类器，如准确度加权的集成、准确度更新的集成等。在本节中，我们将向您展示如何使用利用袋装算法：'
- en: 'Open the Terminal and execute the following command:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开终端并执行以下命令：
- en: '[PRE36]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Select the Classification tab and click on the Configure button:'
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择分类标签并点击配置按钮：
- en: '![](img/902e7a65-f7d5-486b-840f-dfd65d228b1d.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/902e7a65-f7d5-486b-840f-dfd65d228b1d.png)'
- en: This will open the Configure task option.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开配置任务选项。
- en: 'In the learner option, select bayes.NaiveBayes, and then, in the stream option,
    click on Edit, as shown in the following screenshot:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在学习器选项中，选择bayes.NaiveBayes，然后，在流选项中，点击编辑，如图所示：
- en: '![](img/1fd2f759-4284-4077-a77d-e5802c8f3fac.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1fd2f759-4284-4077-a77d-e5802c8f3fac.png)'
- en: 'Select ConceptDriftStream, and, in stream and driftstream, select the AgrawalGenerator;
    it will use the Agrawal dataset for the stream generator:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择ConceptDriftStream，并在流和漂移流中，选择AgrawalGenerator；它将使用Agrawal数据集作为流生成器：
- en: '![](img/d730b01b-0e88-4452-94a4-c46d1f63eff9.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d730b01b-0e88-4452-94a4-c46d1f63eff9.png)'
- en: 'Close all of the windows and click on the Run button:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭所有窗口并点击运行按钮：
- en: '![](img/7ffe2372-c8cb-4cb3-a1df-404c63e883da.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7ffe2372-c8cb-4cb3-a1df-404c63e883da.png)'
- en: 'This will run the task and generate the following output:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这将运行任务并生成以下输出：
- en: '![](img/9795215c-7212-4bb4-ba51-471c42c203bf.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9795215c-7212-4bb4-ba51-471c42c203bf.png)'
- en: 'Let''s use the LeveragingBag option. For this, open the Configure task window
    and select the Edit option in baseLearner, which will show the following; select
    LeveragingBag from the first drop-down box. You can find other options, such as
    boosting and average weight ensembles, in the first drop-down box:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们使用LeveragingBag选项。为此，打开配置任务窗口，并在baseLearner中选择编辑选项，这将显示以下内容；从第一个下拉框中选择LeveragingBag。你可以在第一个下拉框中找到其他选项，例如提升和平均权重集成：
- en: '![](img/04f39f1f-151f-496c-ba95-3c1db7d83177.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/04f39f1f-151f-496c-ba95-3c1db7d83177.png)'
- en: 'Leave the stream as AgrawalGenerator, as shown in the following screenshot:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 将流设置为AgrawalGenerator，如下截图所示：
- en: '![](img/eeba95e7-a148-45d0-aeb7-30083591f15a.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/eeba95e7-a148-45d0-aeb7-30083591f15a.png)'
- en: 'Close the Configure task window and click on the Run button; this will take
    some time to complete:'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 关闭配置任务窗口并点击运行按钮；这需要一些时间来完成：
- en: '![](img/7f2c9091-b62a-4db9-aed2-5b14052603a8.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7f2c9091-b62a-4db9-aed2-5b14052603a8.png)'
- en: The output shows the evaluation after every 10,000 instances, how much RAM time
    is taken with classification correctness, as well as Kappa statistics. As you
    can see, over time, the classification correctness increases, along with the increasing
    instances. The graph in the preceding screenshot shows the correctness and the
    number of instances.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示了每10,000个实例后的评估，包括分类正确性所需的RAM时间以及Kappa统计。正如你所见，随着时间的推移，分类正确性随着实例数量的增加而提高。前一个截图中的图表显示了正确性和实例数量。
- en: Summary
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we tackled the KDD Cup 2009 challenge on customer relationship
    predictions, implementing the data preprocessing steps and addressing the missing
    values and redundant attributes. We followed the winning KDD Cup solution and
    studied how to leverage ensemble methods by using a basket of learning algorithms,
    which can significantly boost classification performance.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们解决了2009年KDD杯关于客户关系预测的挑战，实现了数据预处理步骤，并处理了缺失值和冗余属性。我们遵循了获胜的KDD杯解决方案，并研究了如何通过使用一系列学习算法来利用集成方法，这可以显著提高分类性能。
- en: 'In the next chapter, we will tackle another problem concerning customer behavior:
    purchasing behavior. You will learn how to use algorithms that detect frequently
    occurring patterns.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将解决另一个关于客户行为的问题：购买行为。你将学习如何使用检测频繁发生模式的算法。
