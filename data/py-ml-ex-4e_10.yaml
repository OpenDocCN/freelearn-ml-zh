- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Machine Learning Best Practices
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习最佳实践
- en: After working on multiple projects covering important machine learning concepts,
    techniques, and widely used algorithms, you have a broad picture of the machine
    learning ecosystem, as well as solid experience in tackling practical problems
    using machine learning algorithms and Python. However, there will be issues once
    we start working on projects from scratch in the real world. This chapter aims
    to get us ready for it with 21 best practices to follow throughout the entire
    machine learning solution workflow.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理了多个涵盖重要机器学习概念、技术和广泛使用的算法的项目之后，你已经对机器学习生态系统有了全面的了解，并且在使用机器学习算法和Python解决实际问题方面积累了扎实的经验。然而，当我们开始从零开始在现实世界中开展项目时，仍会面临一些问题。本章旨在通过21条最佳实践，帮助我们为此做好准备，贯穿整个机器学习解决方案的工作流程。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Machine learning solution workflow
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习解决方案工作流程
- en: Best practices in the data preparation stage
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备阶段的最佳实践
- en: Best practices in the training set generation stage
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集生成阶段的最佳实践
- en: Best practices in the model training, evaluation, and selection stage
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练、评估和选择阶段的最佳实践
- en: Best practices in the deployment and monitoring stage
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署与监控阶段的最佳实践
- en: Machine learning solution workflow
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习解决方案工作流程
- en: 'In general, the main tasks involved in solving a machine learning problem can
    be summarized into four areas, as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解决机器学习问题的主要任务可以总结为四个方面，如下所示：
- en: Data preparation
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据准备
- en: Training set generation
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集生成
- en: Model training, evaluation, and selection
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练、评估与选择
- en: Deployment and monitoring
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署与监控
- en: 'Starting from data sources and ending with the final machine learning system,
    a machine learning solution basically follows the paradigm shown here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据源开始，到最终的机器学习系统，机器学习解决方案基本上遵循如下模式：
- en: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_10_01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![A picture containing text, screenshot, diagram, font  Description automatically
    generated](img/B21047_10_01.png)'
- en: 'Figure 10.1: The life cycle of a machine learning solution'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.1：机器学习解决方案的生命周期
- en: In the following sections, we will learn about the typical tasks, common challenges,
    and best practices for each of these four stages.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的部分中，我们将学习这些四个阶段的典型任务、常见挑战以及最佳实践。
- en: Best practices in the data preparation stage
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据准备阶段的最佳实践
- en: No machine learning system can be built without data. Therefore, **data collection**
    should be our first focus.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 没有数据就无法构建机器学习系统。因此，**数据收集**应当是我们首先关注的重点。
- en: Best practice 1 – Completely understanding the project goal
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 1 – 完全理解项目目标
- en: Before starting to collect data, we should make sure that the goal of the project
    and the business problem are completely understood, as this will guide us on what
    data sources to look into, and where sufficient domain knowledge and expertise
    is also required. For example, in a previous chapter, *Chapter 5*, *Predicting
    Stock Prices with Regression Algorithms*, our goal was to predict the future prices
    of the stock index, so we first collected data on its past performance, instead
    of the past performance of an irrelevant European stock. In *Chapter 3*, *Predicting
    Online Ad Click-Through with Tree-Based Algorithms*, for example, the business
    problem was to optimize advertising, targeting efficiency measured by click-through
    rate, so we collected the clickstream data of who clicked or did not click on
    what ad on what page, instead of merely using how many ads were displayed in a
    web domain.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始收集数据之前，我们应该确保完全理解项目的目标和业务问题，因为这将指导我们选择哪些数据来源，以及在哪些领域需要充足的领域知识和专业技能。例如，在前面的章节中，*第5章*，*使用回归算法预测股价*，我们的目标是预测股票指数的未来价格，因此我们首先收集了其历史表现的数据，而不是收集与之无关的欧洲股票的历史表现数据。在*第3章*，*使用基于树的算法预测在线广告点击率*中，业务问题是优化广告投放，目标是提高点击率，因此我们收集了点击流数据，记录谁在什么页面点击或未点击了哪个广告，而不是仅仅使用广告在网页域名中展示的数量。
- en: Best practice 2 – Collecting all fields that are relevant
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 2 – 收集所有相关字段
- en: 'With a set goal in mind, we can narrow down potential data sources to investigate.
    Now the question becomes: is it necessary to collect the data of all fields available
    in a data source, or is a subset of attributes enough? It would be perfect if
    we knew in advance which attributes were key indicators or key predictive factors.
    However, it is in fact very difficult to ensure that the attributes hand-picked
    by a domain expert will yield the best prediction results. Hence, for each data
    source, it is recommended to collect all of the fields that are related to the
    project, especially in cases where recollecting the data is time-consuming, or
    even impossible.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 确定了目标后，我们可以缩小潜在的数据源进行调查。现在的问题是：是否有必要收集数据源中所有领域的数据，还是仅仅一个属性的子集就足够了？如果我们能事先知道哪些属性是关键指标或预测因素，那将是最理想的。然而，实际上很难确保由领域专家挑选的属性能够产生最佳的预测结果。因此，对于每个数据源，建议收集与项目相关的所有字段，尤其是在重新收集数据既费时又几乎不可能的情况下。
- en: For example, in the stock price prediction example, we collected the data of
    all fields, including **Open**, **High**, **Low**, and **Volume**, even though
    we were initially not certain of how useful **high** and **low** predictions would
    be. Retrieving the stock data is quick and easy, however. In another example,
    if we ever want to collect data ourselves by scraping online articles for topic
    classification, we should store as much information as possible. Otherwise, if
    any piece of information is not collected but is later found to be valuable, such
    as hyperlinks in an article, the article might already have been removed from
    the web page; if it still exists, rescraping those pages can be costly.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在股价预测的案例中，我们收集了所有字段的数据，包括**Open**、**High**、**Low**和**Volume**，尽管最初我们不确定**high**和**low**预测的用处。然而，获取股市数据非常迅速且容易。在另一个例子中，如果我们想要通过抓取在线文章进行主题分类并自己收集数据，我们应该尽可能多地存储信息。否则，如果某些信息没有被收集，但后来发现它很有价值，例如文章中的超链接，文章可能已经从网页上删除；如果它仍然存在，重新抓取这些页面的成本可能会很高。
- en: After collecting the datasets that we think are useful, we need to ensure the
    data quality by inspecting its **consistency** and **completeness**. Consistency
    refers to how the distribution of data changes over time. Completeness means how
    much data is present across fields and samples. They are explained in detail in
    the following two practices.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在收集了我们认为有用的数据集之后，我们需要通过检查其**一致性**和**完整性**来确保数据质量。一致性是指数据分布随时间的变化情况。完整性是指在字段和样本中的数据量。这将在以下两个实践中详细解释。
- en: Best practice 3 – Maintaining the consistency and normalization of field values
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 3 – 维护字段值的一致性和标准化
- en: In a dataset that already exists, or in one that we collect from scratch, we
    often see different values representing the same meaning. For example, we see
    *American*, *US*, and *U.S.A* in the `Country` field, and *male* and *M* in the
    `Gender` field. It is necessary to unify or standardize the values in a field,
    otherwise, it will mess up the algorithms in later stages as different feature
    values will be treated differently even if they have the same meaning. For example,
    we keep only the three options *M*, *F*, and *gender-diverse* in the `Gender`
    field, and replace other alternative values. It is also a great practice to keep
    track of what values are mapped to the default value of a field.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个已经存在的数据集或我们从零开始收集的数据集中，我们经常看到不同的值表示相同的含义。例如，在`Country`字段中，我们看到*American*、*US*和*U.S.A*，在`Gender`字段中，我们看到*male*和*M*。有必要统一或标准化字段中的值，否则在后期阶段，算法会混淆处理不同的特征值，即使它们具有相同的含义。例如，我们只保留`Gender`字段中的三种选项：*M*、*F*和*gender-diverse*，并替换其他替代值。记录哪些值被映射到字段的默认值也是一个很好的做法。
- en: In addition, the format of values in the same field should also be consistent.
    For instance, in the *age* field, there could be true age values, such as *21*
    and *35*, and incorrect age values, such as *1990* and *1978*; in the *rating*
    field, both cardinal numbers and English numerals could be found, such as *1*,
    *2*, and *3*, and *one*, *two*, and *three*. Transformation and reformatting should
    be conducted in order to ensure data consistency.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，同一字段中值的格式也应保持一致。例如，在*age*字段中，可能会有真实年龄值，如*21*和*35*，也可能会有不正确的年龄值，如*1990*和*1978*；在*rating*字段中，可能会看到基数数字和英文数字，如*1*、*2*、*3*，以及*one*、*two*、*three*。应该进行转换和重新格式化，以确保数据的一致性。
- en: Best practice 4 – Dealing with missing data
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 4 – 处理缺失数据
- en: Due to various reasons, datasets in the real world are rarely completely clean
    and often contain missing or corrupted values. They are usually presented as blanks,
    *Null*, *-1, 999999*, *unknown*, or any other placeholder. Samples with missing
    data not only provide incomplete predictive information but also confuse the machine
    learning model as it cannot tell whether *-1* or *unknown* holds a meaning. It
    is important to pinpoint and deal with missing data in order to avoid jeopardizing
    the performance of models in the later stages.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 由于各种原因，现实世界中的数据集很少完全干净，通常包含缺失或损坏的值。它们通常被呈现为空白、*Null*、*-1, 999999*、*unknown*
    或其他任何占位符。带有缺失数据的样本不仅提供不完整的预测信息，还会使机器学习模型困惑，因为它无法确定 *-1* 或 *unknown* 是否具有特定含义。准确定位并处理缺失数据是非常重要的，以避免在后期模型的性能受到影响。
- en: 'Here are three basic strategies that we can use to tackle the missing data
    issue:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有三种基本策略，我们可以用来处理缺失数据问题：
- en: Discarding samples containing any missing values.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃包含任何缺失值的样本。
- en: Discarding fields containing missing values in any sample.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 丢弃任何样本中包含缺失值的字段。
- en: Inferring the missing values based on the known part of the attribute. This
    process is called **missing data imputation**. Typical imputation methods include
    replacing missing values with the mean or median value of the field across all
    samples, or the most frequent value for categorical data.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据属性的已知部分推断缺失值。这个过程称为**缺失数据填补**。典型的填补方法包括用所有样本中字段的平均值或中位数替换缺失值，或者对于分类数据，用最频繁出现的值替换。
- en: The first two strategies are simple to implement; however, they come at the
    expense of the data lost, especially when the original dataset is not large enough.
    The third strategy doesn’t abandon any data but does try to fill in the blanks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种策略实现简单；然而，它们牺牲了数据的丢失，特别是当原始数据集不够大时。第三种策略并不放弃任何数据，而是试图填补空缺。
- en: 'Let’s look at how each strategy is applied in an example where we have a dataset
    (age, income) consisting of six samples – (30, 100), (20, 50), (35, *unknown*),
    (25, 80), (30, 70), and (40, 60):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看每种策略在一个包含六个样本（年龄、收入）的数据集中如何应用 - (30, 100), (20, 50), (35, *unknown*), (25,
    80), (30, 70), 和 (40, 60)。
- en: If we process this dataset using the first strategy, it becomes (30, 100), (20,
    50), (25, 80), (30, 70), and (40, 60).
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用第一种策略处理这个数据集，它将变成 (30, 100), (20, 50), (25, 80), (30, 70), 和 (40, 60)。
- en: If we employ the second strategy, the dataset becomes (30), (20), (35), (25),
    (30), and (40), where only the first field remains.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们采用第二种策略，数据集变成了 (30), (20), (35), (25), (30), 和 (40)，只有第一个字段保留了下来。
- en: If we decide to complete the unknown value instead of skipping it, the sample
    (35, *unknown*) can be transformed into (35, 72) with the mean of the rest of
    the values in the second field, or (35, 70), with the median value in the second
    field.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们决定补全未知值而不是跳过它，例如样本 (35, *unknown*) 可以被转换为 (35, 72)，其中第二个字段的值是其余值的平均值，或者转换为
    (35, 70)，其中第二个字段的值是中位数。
- en: 'In scikit-learn, the `SimpleImputer` class ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))
    provides a nicely written imputation transformer. We can use it for the following
    small example:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，`SimpleImputer` 类 ([https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html](https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html))
    提供了一个写得很好的填补转换器。我们可以用它来做下面这个小例子：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Represent the unknown value with `np.nan` in `numpy`, as detailed in the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `numpy` 中，用 `np.nan` 表示未知值，如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Initialize the imputation transformer with the mean value and obtain the mean
    value from the original data:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 使用平均值初始化填补转换器，并从原始数据中获取平均值：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Complete the missing value as follows:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 补全缺失值如下：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Similarly, initialize the imputation transformer with the median value, as
    detailed in the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，使用中位数初始化填补转换器，如下所示：
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'When new samples come in, the missing values (in any attribute) can be imputed
    using the trained transformer, for example, with the mean value, as shown here:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当新样本出现时，可以使用训练好的转换器填补缺失值（在任何属性中），例如用平均值，如下所示：
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that `30` in the age field is the mean of those six age values in the original
    dataset.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，年龄字段中的 `30` 是原始数据集中这六个年龄值的平均值。
- en: 'Now that we have seen how imputation works, as well as its implementation,
    let’s explore how the strategy of imputing missing values and discarding missing
    data affects the prediction results through the following example:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了填补工作的方式及其实施，让我们通过以下示例来探讨填充缺失值和丢弃缺失数据策略如何影响预测结果：
- en: 'First, we load the diabetes dataset, as shown here:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们加载糖尿病数据集，如下所示：
- en: '[PRE6]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Simulate a corrupted dataset by adding 25% missing values:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加 25% 的缺失值来模拟一个损坏的数据集：
- en: '[PRE7]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Randomly select the `m_missing` samples, as follows:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择`m_missing`个样本，如下所示：
- en: '[PRE8]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'For each missing sample, randomly select 1 out of `n` features:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个缺失的样本，随机选择 `n` 个特征中的一个：
- en: '[PRE9]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Represent missing values with `nan`, as shown here:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用`nan`表示缺失值，如下所示：
- en: '[PRE10]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then, we deal with this corrupted dataset by discarding the samples containing
    a missing value:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们通过丢弃包含缺失值的样本来处理这个损坏的数据集：
- en: '[PRE11]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Measure the effects of using this strategy by estimating the averaged regression
    score *R*², with a regression forest model in a cross-validation manner. Estimate
    *R*² on the dataset with the missing samples removed, as follows:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过交叉验证方式，在移除了缺失样本的数据集上，估计使用这种策略的平均回归分数 *R*²，如下所示：
- en: '[PRE12]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we approach the corrupted dataset differently by imputing missing values
    with the mean, as shown here:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们通过使用均值来填补缺失值来处理这个损坏的数据集，如下所示：
- en: '[PRE13]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Similarly, measure the effects of using this strategy by estimating the averaged
    *R*², as follows:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 类似地，通过估计平均 *R*² 来衡量使用这种策略的效果，如下所示：
- en: '[PRE14]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'An imputation strategy works better than discarding in this case. So, how far
    is the imputed dataset from the original full one? We can check it again by estimating
    the averaged regression score on the original dataset, as follows:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，填补策略比丢弃策略更有效。那么，填补后的数据集与原始完整数据集相比有多大差距？我们可以通过在原始数据集上估计平均回归分数来再次检查，如下所示：
- en: '[PRE15]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It turns out that little information is compromised in the imputed dataset.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，在补充后的数据集中丢失了少量信息。
- en: However, there is no guarantee that an imputation strategy always works better,
    and sometimes, dropping samples with missing values can be more effective. Hence,
    it is a great practice to compare the performance of different strategies via
    cross-validation, as we have done previously.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，并不能保证填补策略总是更好，有时，丢弃带有缺失值的样本可能更有效。因此，通过交叉验证比较不同策略的性能是一个很好的实践，正如我们之前所做的。
- en: Best practice 5 – Storing large-scale data
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 5 – 存储大规模数据
- en: 'With the ever-growing size of data, oftentimes, we can’t simply fit the data
    on our single local machine and need to store it on the cloud or distributed filesystems.
    As this is mainly a book on machine learning with Python, we will just touch on
    some basic areas that you can look into. The two main strategies for storing big
    data are **scale up** and **scale out**:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据规模的不断增长，我们往往不能简单地将数据安装在我们的单个本地机器上，需要将其存储在云端或分布式文件系统中。由于这主要是一本关于Python机器学习的书籍，我们只会涉及一些你可以深入了解的基本领域。存储大数据的两种主要策略是
    **扩展** 和 **扩展**：
- en: A **scale-up** approach increases storage capacity if data exceeds the current
    system capacity, such as by adding more disks. This is useful in fast-access platforms.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**扩展** 方法通过增加更多磁盘等方式来增加存储容量，如果数据超出了当前系统容量。这在快速访问平台上非常有用。'
- en: In a **scale-out** approach, storage capacity grows incrementally with additional
    nodes in a storage cluster. Hadoop Distributed File System (HDFS) ([https://hadoop.apache.org/](https://hadoop.apache.org/))
    and Spark ([https://spark.apache.org/](https://spark.apache.org/)) are used to
    store and process big data in scale-out clusters, where data is spread across
    hundreds or even thousands of nodes. Also, there are cloud-based distributed file
    services, such as S3 in Amazon Web Services ([https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)),
    Google Cloud Storage in Google Cloud ([https://cloud.google.com/storage/](https://cloud.google.com/storage/)),
    and Storage in Microsoft Azure ([https://azure.microsoft.com/en-us/services/storage/](https://azure.microsoft.com/en-us/services/storage/)).
    They are massively scalable and are designed for secure and durable storage.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在**扩展性**方法中，存储容量随着存储集群中新增节点的加入而逐步增长。Hadoop分布式文件系统（HDFS）（[https://hadoop.apache.org/](https://hadoop.apache.org/)）和Spark（[https://spark.apache.org/](https://spark.apache.org/)）用于在扩展集群中存储和处理大数据，其中数据分布在成百上千个节点上。此外，还有基于云的分布式文件服务，如亚马逊Web
    Services中的S3（[https://aws.amazon.com/s3/](https://aws.amazon.com/s3/)）、Google
    Cloud中的Google Cloud Storage（[https://cloud.google.com/storage/](https://cloud.google.com/storage/)）和Microsoft
    Azure中的Storage（[https://azure.microsoft.com/en-us/services/storage/](https://azure.microsoft.com/en-us/services/storage/)）。它们具有大规模可扩展性，设计用于安全且持久的存储。
- en: 'Besides choosing the right storage system to increase capacity, you also need
    to pay attention to the following practices:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 除了选择合适的存储系统以增加容量外，你还需要关注以下做法：
- en: '**Data partitioning**: Divide your data into smaller partitions or shards.
    This distributes the load across multiple servers or nodes, enabling better parallel
    processing and retrieval.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分区**：将数据划分为更小的分区或碎片。这可以将负载分配到多个服务器或节点，从而实现更好的并行处理和检索。'
- en: '**Data compression and encoding**: Implement data compression techniques to
    reduce storage space and optimize data retrieval times.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据压缩和编码**：实施数据压缩技术以减少存储空间，并优化数据检索时间。'
- en: '**Replication and redundancy**: Replicate data across multiple storage nodes
    or geographical locations to ensure data availability and fault tolerance.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复制和冗余**：将数据复制到多个存储节点或地理位置，以确保数据的可用性和容错性。'
- en: '**Security and access control**: Implement robust access control mechanisms
    to ensure that only authorized personnel can access sensitive data.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**安全性和访问控制**：实施强大的访问控制机制，确保只有授权人员能够访问敏感数据。'
- en: With well-prepared data, it is safe to move on to the training set generation
    stage. Let’s see the next section.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备充分的情况下，可以安全地进入训练集生成阶段。让我们来看下一部分。
- en: Best practices in the training set generation stage
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练集生成阶段的最佳实践
- en: 'Typical tasks in this stage can be summarized into two major categories: **data
    preprocessing** and **feature engineering**.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这个阶段的典型任务可以总结为两个主要类别：**数据预处理**和**特征工程**。
- en: To begin, data preprocessing usually involves categorical feature encoding,
    feature scaling, feature selection, and dimensionality reduction.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，数据预处理通常包括类别型特征编码、特征缩放、特征选择和降维。
- en: Best practice 6 – Identifying categorical features with numerical values
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践6 – 识别具有数值值的类别型特征
- en: In general, categorical features are easy to spot, as they convey qualitative
    information, such as risk level, occupation, and interests. However, it gets tricky
    if the feature takes on a discreet and countable (limited) number of numerical
    values, for instance, 1 to 12 representing months of the year, and 1 and 0 indicating
    true and false.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，类别型特性容易辨认，因为它们传递的是定性信息，如风险等级、职业和兴趣。然而，如果特性具有离散且可计数（有限）数量的数值，例如表示月份的1到12，或者表示真假值的1和0时，判断就会变得复杂。
- en: The key to identifying whether such a feature is categorical or numerical is
    whether it provides a mathematical or ranking implication; if it does, it is a
    numerical feature, such as a product rating from 1 to 5; otherwise, it is categorical,
    such as the month, or day of the week.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 判断某个特性是类别型还是数值型的关键在于它是否提供数学或排名的含义；如果有，则是数值型特性，例如从1到5的产品评分；否则，是类别型特性，例如月份或星期几。
- en: Best practice 7 – Deciding whether to encode categorical features
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践7 – 决定是否对类别型特征进行编码
- en: If a feature is considered categorical, we need to decide whether we should
    encode it. This depends on what prediction algorithm(s) we will use in later stages.
    Naïve Bayes and tree-based algorithms can directly work with categorical features,
    while other algorithms in general cannot, in which case encoding is essential.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 如果某个特征被视为类别型特征，我们需要决定是否对其进行编码。这取决于我们在后续阶段将使用哪些预测算法。朴素贝叶斯和基于树的算法可以直接处理类别型特征，而其他算法通常不能，在这种情况下，编码是必需的。
- en: As the output of the feature generation stage is the input of the model training
    stage, *steps taken in the feature generation stage should be compatible with
    the prediction algorithm*. Therefore, we should look at the two stages of feature
    generation and predictive model training as a whole, instead of two isolated components.
    The next two practical tips also reinforce this point.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 由于特征生成阶段的输出是模型训练阶段的输入，*特征生成阶段的步骤应与预测算法兼容*。因此，我们应将特征生成和预测模型训练这两个阶段作为一个整体来看待，而不是将其视为两个孤立的部分。接下来的两个实用建议也强调了这一点。
- en: Best practice 8 – Deciding whether to select features and, if so, how to do
    so
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 8 – 决定是否选择特征，如果选择，应该如何操作
- en: 'You have seen, in *Chapter 4*, *Predicting Online Ad Click-Through with Logistic
    Regression*, how feature selection can be performed using L1-based regularized
    logistic regression and random forest. The benefits of feature selection include
    the following:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第4章*《使用逻辑回归预测在线广告点击率》中，你已经看到了如何使用基于L1正则化的逻辑回归和随机森林进行特征选择。特征选择的好处包括以下几点：
- en: Reducing the training time of prediction models as redundant or irrelevant features
    are eliminated
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低预测模型的训练时间，因为冗余或无关特征已被剔除
- en: Reducing overfitting for the same preceding reason
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于前述原因，减少过拟合
- en: Likely improving performance, as prediction models will learn from data with
    more significant features
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能提升性能，因为预测模型将从具有更重要特征的数据中学习
- en: 'Note that we used the word *likely* because there is no absolute certainty
    that feature selection will increase prediction accuracy. It is, therefore, good
    practice to compare the performances of conducting feature selection and not doing
    so via cross-validation. For example, by executing the following steps, we can
    measure the effects of feature selection by estimating the averaged classification
    accuracy with an `SVC` model in a cross-validation manner:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们使用了*可能*这个词，因为无法绝对确定特征选择一定会提高预测精度。因此，最好通过交叉验证比较进行特征选择与不进行特征选择的表现。例如，通过执行以下步骤，我们可以通过`SVC`模型以交叉验证方式估算特征选择的影响，进而衡量其对分类准确率的平均影响：
- en: 'First, we load the handwritten digits dataset from `scikit-learn`, as follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从`scikit-learn`加载手写数字数据集，如下所示：
- en: '[PRE16]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, estimate the accuracy of the original dataset, which is 64-dimensional,
    as detailed here:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，估算原始数据集（64维）的准确率，具体如下所示：
- en: '[PRE17]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Then, conduct feature selection based on random forest and sort the features
    based on their importance scores:'
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，基于随机森林进行特征选择，并根据特征的重要性分数对其进行排序：
- en: '[PRE18]'
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Now select a different number of top features to construct a new dataset, and
    estimate the accuracy on each dataset, as follows:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在选择不同数量的前几个特征来构建新数据集，并在每个数据集上估算准确率，具体如下：
- en: '[PRE19]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we use the top 25 features selected by random forest, the SVM classification
    performance can increase from `0.9` to `0.95`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用随机森林选择的前25个特征，SVM分类性能可以从`0.9`提升到`0.95`。
- en: Best practice 9 – Deciding whether to reduce dimensionality and, if so, how
    to do so
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 9 – 决定是否降维，如果降维，应该如何操作
- en: 'Feature selection and dimensionality are different in the sense that the former
    chooses features from the original data space, while the latter does so from a
    projected space from the original space. Dimensionality reduction has the following
    advantages that are similar to feature selection:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择和降维的区别在于，前者是从原始数据空间中选择特征，而后者则是从原始空间的投影空间中选择特征。降维具有与特征选择相似的以下优点：
- en: Reducing the training time of prediction models, as redundant or correlated
    features are merged into new ones
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低预测模型的训练时间，因为冗余或相关特征已被合并为新的特征
- en: Reducing overfitting for the same reason
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于同样的原因，减少过拟合
- en: Likely improving performance, as prediction models will learn from data with
    less redundant or correlated features
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能提高性能，因为预测模型将从具有较少冗余或相关特征的数据中学习
- en: 'Again, it is not guaranteed that dimensionality reduction will yield better
    prediction results. In order to examine its effects, integrating dimensionality
    reduction in the model training stage is recommended. Reusing the preceding handwritten
    digits example, we can measure the effects of **Principal Component Analysis**
    (**PCA**)-based dimensionality reduction, where we keep a different number of
    top components to construct a new dataset, and estimate the accuracy on each dataset:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，降维并不能保证产生更好的预测结果。为了检验其效果，建议在模型训练阶段集成降维方法。以之前的手写数字示例为例，我们可以衡量基于**主成分分析**（**PCA**）的降维效果，其中我们保留不同数量的主要成分来构建新数据集，并估算每个数据集的准确度：
- en: '[PRE20]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If we use the top 15 features generated by PCA, the SVM classification performance
    can increase from `0.9` to `0.95`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用PCA生成的前15个特征，SVM分类性能可以从`0.9`提高到`0.95`。
- en: Best practice 10 – Deciding whether to rescale features
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践10 – 决定是否重新缩放特征
- en: As seen in *Chapter 5*, *Predicting Stock Prices with Regression Algorithms*,
    and *Chapter 6*, *Predicting Stock Prices with Artificial Neural Networks*, SGD-based
    linear regression, SVR, and the neural network model require features to be standardized
    by removing the mean and scaling to unit variance. So, when is feature scaling
    needed, and when is it not?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如在*第5章*《使用回归算法预测股票价格》和*第6章*《使用人工神经网络预测股票价格》中所见，基于SGD的线性回归、SVR和神经网络模型要求特征通过去均值并缩放到单位方差进行标准化。那么，什么时候需要特征缩放，什么时候不需要呢？
- en: In general, Naïve Bayes and tree-based algorithms are not sensitive to features
    at different scales, as they look at each feature independently.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，朴素贝叶斯和基于树的算法对不同尺度的特征不敏感，因为它们独立地看待每个特征。
- en: In most cases, an algorithm that involves any form of distance (or separation
    in spaces) of samples in learning requires scaled/standardized inputs, such as
    SVC, SVR, k-means clustering, and **k-nearest neighbors** (**KNN**) algorithms.
    Feature scaling is also a must for any algorithm using SGD for optimization, such
    as linear or logistic regression with gradient descent, and neural networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，涉及任何形式的样本距离（或空间分离）的学习算法都需要缩放/标准化的输入，如SVC、SVR、k-means聚类和**k近邻**（**KNN**）算法。对于任何使用SGD进行优化的算法（例如带有梯度下降的线性回归或逻辑回归，以及神经网络），特征缩放也是必须的。
- en: We have so far covered tips regarding data preprocessing and will next discuss
    best practices of feature engineering as another major aspect of training set
    generation. We will do so from two perspectives.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了数据预处理的技巧，接下来将讨论特征工程的最佳实践，作为训练集生成的另一个主要方面。我们将从两个角度进行探讨。
- en: Best practice 11 – Performing feature engineering with domain expertise
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践11 – 利用领域专业知识进行特征工程
- en: If we are lucky enough to possess sufficient domain knowledge, we can apply
    it in creating domain-specific features; we utilize our business experience and
    insights to identify what is in the data and formulate new data that correlates
    to the prediction target. For example, in *Chapter 5*, *Predicting Stock Prices
    with Regression Algorithms*, we designed and constructed feature sets for the
    prediction of stock prices based on factors that investors usually look at when
    making investment decisions.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们幸运地拥有足够的领域知识，我们可以应用它来创建领域特定的特征；我们利用我们的商业经验和洞察力，识别数据中的信息，并制定与预测目标相关的新数据。例如，在*第5章*《使用回归算法预测股票价格》中，我们根据投资者在做出投资决策时通常关注的因素，设计并构建了用于股票价格预测的特征集。
- en: While particular domain knowledge is required, sometimes we can still apply
    some general tips in this category. For example, in fields related to customer
    analytics, such as marketing and advertising, the time of the day, day of the
    week, and month are usually important signals. Given a data point with the value
    *2020/09/01* in the `Date` column and *14:34:21* in the `Time` column, we can
    create new features including *afternoon*, *Tuesday*, and *September*. In retail,
    information covering a period of time is usually aggregated to provide better
    insights. The number of times a customer visited a store in the past three months,
    or the average number of products purchased weekly in the previous year, for instance,
    can be good predictive indicators for customer behavior prediction.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然特定的领域知识是必需的，但有时我们仍然可以应用一些通用的技巧。例如，在与客户分析相关的领域，如营销和广告，一天中的时间、一周中的日子和月份通常是重要的信号。在`Date`列中给定数据点的值为*2020/09/01*，`Time`列中的值为*14:34:21*，我们可以创建新特征，包括*下午*、*星期二*和*九月*。在零售业中，通常会聚合一段时间内的信息以提供更好的洞察力。例如，过去三个月内客户访问店铺的次数，或者上一年每周平均购买产品的次数，可以成为客户行为预测的良好预测指标。
- en: Best practice 12 – Performing feature engineering without domain expertise
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践12 – 在没有领域专业知识的情况下进行特征工程
- en: If, unfortunately, we have very little domain knowledge, how can we generate
    features? Don’t panic. There are several generic approaches that you can follow,
    such as binarization, discretization, interaction, and polynomial transformation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不幸我们缺乏领域知识，我们如何生成特征？不要惊慌。有几种通用方法可以遵循，例如二值化、离散化、交互和多项式转换。
- en: Binarization and discretization
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 二值化和离散化
- en: '**Binarization** is the process of converting a numerical feature to a binary
    one with a preset threshold. For example, in spam email detection, for the feature
    (or term) *prize*, we can generate a new feature, `whether_term_prize_occurs`:
    any term frequency value greater than 1 becomes 1; otherwise, it is 0\. The feature
    *number of visits per week* can be used to produce a new feature, `is_frequent_visitor`,
    by judging whether the value is greater than or equal to 3\. We implement such
    binarization using scikit-learn, as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '**二值化**是将数值特征转换为二进制特征的过程，具有预设阈值。例如，在垃圾邮件检测中，对于特征（或术语）*prize*，我们可以生成新特征`whether_term_prize_occurs`：任何频率大于1的术语值变为1；否则为0。特征*每周访问次数*可用于生成新特征`is_frequent_visitor`，判断其值是否大于或等于3。我们使用scikit-learn来实现这种二值化，如下所示：'
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**Discretization** is the process of converting a numerical feature to a categorical
    feature with limited possible values. Binarization can be viewed as a special
    case of discretization. For example, we can generate an *age group* feature: “*18-24*”
    for ages from 18 to 24, “*25-34*” for ages from 25 to 34, “*34-54*”, and “*55+*”.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**离散化**是将数值特征转换为具有有限可能值的分类特征的过程。二值化可以看作是离散化的一种特殊情况。例如，我们可以生成*年龄组*特征：“*18-24*”适用于18到24岁的年龄，“*25-34*”适用于25到34岁的年龄，“*34-54*”，和“*55+*”。'
- en: Interaction
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 交互
- en: This includes the sum, multiplication, or any operations of two numerical features,
    and the joint condition check of two categorical features. For example, *the number
    of visits per week* and *the number of products purchased per week* can be used
    to generate *the number of products purchased per visit* feature; *interest and
    occupation*, such as *sports* and *engineer*, can form *occupation AND interest*,
    such as *engineer interested in sports*.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括两个数值特征的求和、乘积或任何操作，以及两个分类特征的联合条件检查。例如，*每周访问次数*和*每周购买产品数量*可用于生成*每次访问购买产品数量*特征；*兴趣和职业*，如*体育*和*工程师*，可以形成*职业和兴趣*，例如*对体育感兴趣的工程师*。
- en: Polynomial transformation
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多项式转换
- en: 'This is the process of generating polynomial and interaction features. For
    two features, *a* and *b*, the two degrees of polynomial features generated are
    *a*², *ab*, and *b*². In scikit-learn, we can use the `PolynomialFeatures` class
    ([https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html))
    to perform polynomial transformation, as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这是生成多项式和交互特征的过程。对于两个特征*a*和*b*，生成的二次多项式特征包括*a*²、*ab*和*b*²。在scikit-learn中，我们可以使用`PolynomialFeatures`类（[https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html)）来执行多项式转换，如下所示：
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Note the resulting new features consist of *1* (bias, intercept), *a*, *b*,
    *a*², *ab*, and *b*².
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，生成的新特征包括*1*（偏置，截距）、*a*、*b*、*a*²、*ab*和*b*²。
- en: Best practice 13 – Documenting how each feature is generated
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践13 – 记录每个特征的生成过程
- en: 'We have covered the rules of feature engineering with domain knowledge, and
    in general, there is one more thing worth noting: documenting how each feature
    is generated. It sounds trivial, but oftentimes we just forget about how a feature
    is obtained or created. We usually need to go back to this stage after some failed
    trials in the model training stage and attempt to create more features with the
    hope of improving performance. We have to be clear on what and how features are
    generated, in order to remove those that do not quite work out, and to add new
    ones that have more potential.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论了如何结合领域知识进行特征工程的规则，总的来说，还有一件事值得注意：记录每个特征的生成过程。听起来这似乎微不足道，但实际上我们常常忘记特征是如何获得或创建的。在模型训练阶段经过一些失败的尝试后，我们通常需要回到这一阶段，尝试创建更多的特征，以期提高性能。我们必须清楚特征是如何生成的，以及哪些特征没有发挥作用，哪些特征有更多潜力。
- en: Best practice 14 – Extracting features from text data
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践14 – 从文本数据中提取特征
- en: 'We will start with a traditional approach to extract features from text, tf,
    and tf-idf. Then, we will continue with a modern approach: word embedding. Specifically,
    we will look at word embedding using `Word2Vec` models, and embedding layers in
    neural network models.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从传统的特征提取方法——tf和tf-idf开始。然后，我们将继续介绍一种现代方法：词嵌入。具体来说，我们将讨论使用`Word2Vec`模型的词嵌入，以及神经网络模型中的嵌入层。
- en: tf and tf-idf
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: tf 和 tf-idf
- en: We have worked intensively with text data in *Chapter 7*, *Mining the 20 Newsgroups
    Dataset with Text Analysis Techniques*, and *Chapter 8*, *Discovering Underlying
    Topics in the Newsgroups Dataset with Clustering and Topic Modeling*, where we
    extracted features from text based on **term frequency** (**tf**) and **term frequency-inverse
    document frequency** (**tf-idf**). Both methods consider each document of words
    (terms) a collection of words, or a **bag of words** (**BoW**), disregarding the
    order of the words but keeping multiplicity. A tf approach simply uses the counts
    of tokens, while tf-idf extends tf by assigning each tf a weighting factor that
    is inversely proportional to the document frequency. With the idf factor incorporated,
    tf-idf diminishes the weight of common terms (such as “get” and “make”) that occur
    frequently, and emphasizes terms that rarely occur but convey important meaning.
    Hence, oftentimes, features extracted from tf-idf are more representative than
    those from tf.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*，*《使用文本分析技术挖掘20个新闻组数据集》*和*第8章*，*《通过聚类和主题建模发现新闻组数据集中的潜在主题》*中，我们深入处理了文本数据，并根据**词频**（**tf**）和**词频-逆文档频率**（**tf-idf**）提取了文本特征。两种方法都将每个文档的单词（术语）视为一个单词集合或**词袋**（**BoW**），忽略单词顺序但保留单词的多重性。tf方法仅使用词汇的计数，而tf-idf则通过为每个tf分配一个与文档频率成反比的加权因子来扩展tf。通过引入idf因子，tf-idf减小了那些频繁出现的常见术语（如“get”和“make”）的权重，强调那些罕见但传达重要含义的术语。因此，通常从tf-idf提取的特征比从tf提取的特征更具代表性。
- en: As you may remember, a document is represented by a very sparse vector where
    only present terms have non-zero values. The vector’s dimensionality is usually
    high, which is determined by the size of the vocabulary and the number of unique
    terms. Also, such a one-hot encoding approach treats each term as an independent
    item and does not consider the relationship across words (referred to as “context”
    in linguistics).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能记得的，一个文档通常由一个非常稀疏的向量表示，只有当前出现的术语才有非零值。该向量的维度通常很高，这由词汇表的大小和独特术语的数量决定。此外，这种独热编码方法将每个术语视为独立项，并且不考虑单词之间的关系（在语言学中称为“上下文”）。
- en: Word embedding
  id: totrans-146
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 词嵌入
- en: On the contrary, another approach, called **word embedding**, is able to capture
    the meanings of words and their context. In this approach, a word is represented
    by a vector of float numbers. Its dimensionality is a lot lower than the size
    of the vocabulary and is usually several hundred only.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，另一种方法称为**词嵌入**，它能够捕捉词汇的意义及其上下文。在这种方法中，一个词通过一个浮动数值的向量来表示。它的维度远小于词汇表的大小，通常仅为几百。
- en: The embedding vectors are of real values, where each dimension encodes an aspect
    of meaning for the words in the vocabulary. This helps preserve the semantic information
    of the words, as opposed to discarding it as in the one-hot encoding approach
    using tf or tf-idf. An interesting phenomenon is that vectors from semantically
    similar words are proximate to each other in geometric space. For example, both
    the words *clustering and grouping* refer to unsupervised clustering in the context
    of machine learning, hence their embedding vectors are close together.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入向量是实值的，每个维度表示词汇表中单词的某个意义方面。这有助于保留单词的语义信息，而不是像在使用tf或tf-idf的one-hot编码方法中那样丢弃它。一个有趣的现象是，语义相似的单词的向量在几何空间中是彼此接近的。例如，*clustering*
    和 *grouping* 两个词都指代机器学习中的无监督聚类，因此它们的嵌入向量是相近的。
- en: 'Here are some popular ways to obtain word embeddings:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是获得词嵌入的一些常见方法：
- en: '**Word2Vec**: Train your own Word2Vec embeddings on your specific corpus using
    the Skip-gram or Continuous Bag of Words (CBOW) models. We covered this in *Chapter
    7*, *Mining the 20 Newsgroups Dataset with Text Analysis Techniques*. Libraries
    like Gensim in Python provide easy-to-use interfaces for training Word2Vec embeddings.
    We will present a simple example shortly.'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Word2Vec**：使用Skip-gram或连续词袋（CBOW）模型，在你的特定语料库上训练自己的Word2Vec嵌入。我们在*第7章*中讨论过这个内容，*使用文本分析技术挖掘20个新闻组数据集*。像Python中的Gensim这样的库提供了易于使用的接口来训练Word2Vec嵌入。我们稍后会展示一个简单的示例。'
- en: '**Pre-trained embeddings**: Use pre-trained word embeddings that are trained
    on large corpora. We also talked about this in *Chapter 7*. Popular examples include:'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预训练嵌入**：使用在大语料库上训练的预训练词嵌入。我们在*第7章*中也讨论过这个内容。常见的例子包括：'
- en: FastText
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FastText
- en: '**GloVe** (**Global Vectors for Word Representation**)'
  id: totrans-153
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GloVe**（**全局词向量表示**）'
- en: '**BERT** (**Bidirectional Encoder Representations from Transformers**)'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**BERT**（**双向编码器表示变换器**）'
- en: '**GPT** (**Generative Pre-trained Transformer**)'
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPT**（**生成式预训练变换器**）'
- en: '**USE** (**Universal Sentence Encoder**) embeddings'
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**USE**（**通用句子编码器**）嵌入'
- en: '**Training custom models with an embedding layer**: If you have a specific
    domain or dataset, you can train your own word embeddings using custom neural
    network models.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练带嵌入层的自定义模型**：如果你有特定的领域或数据集，可以使用自定义神经网络模型训练自己的词嵌入。'
- en: Word2Vec embedding
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Word2Vec嵌入
- en: 'Prior to delving into training a custom model for word embeddings, let’s begin
    with the following example of training a basic `Word2Vec` model using `gensim`:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入研究训练自定义词嵌入模型之前，我们先看一个基本的`Word2Vec`模型训练示例，使用`gensim`：
- en: 'We first import the `gensim` module:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入`gensim`模块：
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We define some sample sentences for training:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一些用于训练的示例句子：
- en: '[PRE24]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: In practice, you will need to format the sentences in plain text into a list
    of word lists just like the `sentences` object.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际操作中，你需要将句子格式化为类似`sentences`对象的单词列表。
- en: 'We then create a Word2Vec model with various parameters, such as `vector_size`
    (embedding dimension), `window` (context window size), `min_count` (minimum frequency
    of words), and `sg` (training algorithm – `0` for CBOW, `1` for Skip-gram):'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一个Word2Vec模型，设置多个参数，如`vector_size`（嵌入维度）、`window`（上下文窗口大小）、`min_count`（词频的最小值）和`sg`（训练算法—`0`表示CBOW，`1`表示Skip-gram）：
- en: '[PRE25]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After training, we access word vectors using the model’s `wv` property. Here,
    we display the embedding vector for the word *machine:*
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以使用模型的`wv`属性来访问词向量。这里，我们展示单词*machine*的嵌入向量：
- en: '[PRE26]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Keep in mind that this is a basic example. In practice, you might need to preprocess
    your data more thoroughly, adjust hyperparameters, and train on a larger corpus
    for better embeddings.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，这只是一个基本示例。在实际应用中，你可能需要更彻底地预处理数据，调整超参数，并在更大的语料库上训练，以获得更好的嵌入。
- en: Embedding layers in custom neural networks
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 自定义神经网络中的嵌入层
- en: In a complete deep neural network for NLP tasks, we would typically combine
    an embedding layer with other layers, like fully connected (dense) layers, or
    recurrent layers (we will talk about recurrent layers in *Chapter 12**, Making
    Predictions with Sequences Using Recurrent Neural Networks*) to build a more sophisticated
    model. The embedding layer allows the network to learn meaningful representations
    for words in the input data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在用于NLP任务的完整深度神经网络中，我们通常会将嵌入层与其他层（如全连接（dense）层或循环层）结合使用（我们将在*第12章*中讨论循环层，*使用循环神经网络进行序列预测*），以构建一个更复杂的模型。嵌入层使网络能够学习输入数据中单词的有意义表示。
- en: 'Let’s look at a simplified example of using an embedding layer for word embeddings.
    In PyTorch, we use the `nn.Embedding` module ([https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html))
    for embedding layers:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一个简化的例子，展示如何使用嵌入层进行词向量表示。在 PyTorch 中，我们使用 `nn.Embedding` 模块（[https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html](https://pytorch.org/docs/stable/generated/torch.nn.Embedding.html)）来实现嵌入层：
- en: '[PRE27]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this example, we first import the necessary modules from PyTorch. We define
    some sample input data containing word indices (for example, 1 represents *I*,
    2 represents *love*, 3 represents *machine*, and 4 represents *learning*). Then,
    we define the embedding layer using `nn.Embedding` with `vocab_size` as the total
    number of unique words in the vocabulary, and `embedding_dim` as the desired dimensionality
    of the embeddings. The embedding layer is usually the first layer of a neural
    network model after the input layer. Upon completion of network training, when
    we pass the input data through the embedding layer, it returns embedded vectors
    for each input word index. The shape of output `embedded_data` will be `(sample
    size, sequence length, embedding_dim)`, which is `(2, 4, 3)` in our case.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们首先从 PyTorch 中导入必要的模块。我们定义了一些包含单词索引的示例输入数据（例如，1 代表 *I*，2 代表 *love*，3
    代表 *machine*，4 代表 *learning*）。然后，我们使用 `nn.Embedding` 定义嵌入层，其中 `vocab_size` 为词汇表中独特单词的总数，`embedding_dim`
    为嵌入的目标维度。嵌入层通常是神经网络模型中的第一层，位于输入层之后。在完成网络训练后，当我们将输入数据传递通过嵌入层时，它会返回每个输入单词索引的嵌入向量。输出
    `embedded_data` 的形状为 `(sample size, sequence length, embedding_dim)`，在我们的例子中为
    `(2, 4, 3)`。
- en: Once again, this is a simplified example. In practice, the embedding layers
    are involved in more complex architectures with additional layers, in order to
    process and interpret the embeddings for specific tasks, such as classification,
    sentiment analysis, or sequence generation. Keep an eye out for the upcoming *Chapter
    12*.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 再次说明，这是一个简化的例子。实际上，嵌入层通常涉及到更复杂的架构，并且有更多的层次，以便处理和解读嵌入向量，执行特定任务，如分类、情感分析或序列生成。请留意即将到来的
    *第 12 章*。
- en: Curious about the choice between tf-idf and word embeddings? In conventional
    NLP applications, such as simple text classification and topic modeling, tf, or
    tf-idf, remains an exceptional method for feature extraction. In more complicated
    areas, such as text summarization, machine translation, named entity resolution,
    question answering, and information retrieval, word embeddings are extensively
    utilized and yield significantly enhanced features compared to conventional methods.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 想了解 tf-idf 与词嵌入的选择吗？在传统的自然语言处理（NLP）应用中，例如简单的文本分类和主题建模，tf 或 tf-idf 仍然是一个非常出色的特征提取方法。在更复杂的领域，如文本摘要、机器翻译、命名实体识别、问答和信息检索中，词嵌入被广泛使用，并且相比传统方法，提供了显著增强的特征。
- en: Now that you have reviewed the best practices for data and feature generation,
    let’s look at model training next.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经回顾了数据和特征生成的最佳实践，让我们接下来看看模型训练。
- en: Best practices in the model training, evaluation, and selection stage
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型训练、评估和选择阶段的最佳实践
- en: Given a supervised machine learning problem, the first question many people
    ask is usually *What is the best classification or regression algorithm to solve
    it?* However, there is no one-size-fits-all solution and no free lunch. No one
    could know which algorithm will work best before trying multiple ones and fine-tuning
    the optimal one. We will be looking into best practices around this in this section.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在给定的监督学习问题中，许多人首先问的问题通常是 *解决这个问题的最佳分类或回归算法是什么？* 然而，并没有一刀切的解决方案，也没有免费的午餐。没有人能在尝试多个算法并对最优算法进行微调之前知道哪个算法效果最好。在本节中，我们将深入探讨与此相关的最佳实践。
- en: Best practice 15 – Choosing the right algorithm(s) to start with
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 15 – 选择合适的算法开始
- en: Due to the fact that there are several parameters to tune for an algorithm,
    exhausting all algorithms and fine-tuning each one can be extremely time-consuming
    and computationally expensive. We should instead shortlist one to three algorithms
    to start with using the general guidelines that follow (note we herein focus on
    classification, but the theory transcends to regression, and there is usually
    a counterpart algorithm in regression).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法需要调节多个参数，耗尽所有算法并对每个算法进行微调可能是极其耗时且计算开销巨大的。我们应该根据以下通用指导原则，挑选一到三个算法开始（注意，我们这里关注的是分类问题，但理论也适用于回归问题，并且通常回归中也有对应的算法）。
- en: 'There are several things we need to be clear about before shortlisting potential
    algorithms, as described in the following:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在筛选潜在算法之前，我们需要明确几个方面，具体如下：
- en: The size of the training dataset
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据集的大小
- en: The dimensionality of the dataset
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的维度
- en: Whether the data is linearly separable
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是否线性可分
- en: Whether features are independent
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征是否独立
- en: Tolerance and trade-off of bias and variance
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差与方差的容忍度与权衡
- en: Whether online learning is required
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否需要在线学习
- en: Now, let’s look at how we choose the right algorithm to start with, taking into
    account the aforementioned perspectives.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们从上述角度出发，看看如何选择合适的算法来开始。
- en: Naïve Bayes
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: This is a very simple algorithm. For a relatively small training dataset, if
    features are independent, Naïve Bayes will usually perform well. For a large dataset,
    Naïve Bayes will still work well as feature independence can be assumed in this
    case, regardless of the truth. The training of Naïve Bayes is usually faster than
    any other algorithm due to its computational simplicity. However, this may lead
    to a high bias (but low variance).
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单的算法。对于相对较小的训练数据集，如果特征独立，朴素贝叶斯通常能表现良好。对于大型数据集，尽管实际情况未必如此，但在假设特征独立的前提下，朴素贝叶斯依然能表现出色。由于其计算简洁，朴素贝叶斯的训练通常比其他任何算法都要快。然而，这也可能导致较高的偏差（但低方差）。
- en: Logistic regression
  id: totrans-192
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: This is probably the most widely used classification algorithm, and the first
    algorithm that a machine learning practitioner usually tries when given a classification
    problem. It performs well when data is linearly separable or approximately **linearly
    separable**. Even if it is not linearly separable, it might be possible to convert
    the linearly non-separable features into separable ones and apply logistic regression
    afterward.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是最广泛使用的分类算法，也是机器学习从业者在面对分类问题时通常会尝试的第一个算法。当数据是线性可分或近似**线性可分**时，它表现良好。即使数据不可线性分离，也有可能将线性不可分的特征转化为可分特征，然后应用逻辑回归。
- en: 'In the following instance, data in the original space is not linearly separable,
    but it becomes separable in a transformed space created from the interaction of
    two features:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下实例中，原始空间中的数据是线性不可分的，但通过两个特征的交互作用，转化后的空间中的数据变得可分：
- en: '![A screenshot of a cell phone  Description automatically generated with low
    confidence](img/B21047_10_02.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![A screenshot of a cell phone  Description automatically generated with low
    confidence](img/B21047_10_02.png)'
- en: 'Figure 10.2: Transforming features from linearly non-separable to separable'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2：将特征从线性不可分转化为可分
- en: Also, logistic regression is extremely scalable to large datasets with SGD optimization,
    which makes it efficient in solving big data problems. Plus, it makes online learning
    feasible. Although logistic regression is a low-bias, high-variance algorithm,
    we overcome the potential overfitting by adding L1, L2, or a mix of the two regularizations.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，逻辑回归通过使用 SGD 优化，能够极大地扩展到大数据集，这使其在解决大数据问题时非常高效。同时，它也使得在线学习成为可能。尽管逻辑回归是一个低偏差、高方差的算法，但我们通过加入
    L1、L2 或两者的混合正则化来克服可能的过拟合问题。
- en: SVM
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）
- en: This is versatile enough to adapt to the linear separability of data. For a
    separable dataset, SVM with a linear kernel performs comparably to logistic regression.
    Beyond this, SVM also works well for a non-separable dataset if equipped with
    a non-linear kernel, such as RBF. Logistic regression may face challenges in high-dimensional
    datasets, while SVM still performs well. A good example of this can be in news
    classification, where the feature dimensionality is in the tens of thousands.
    In general, very high accuracy can be achieved by SVM with the right kernel and
    parameters. However, this might be at the expense of intense computation and high
    memory consumption.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法足够通用，可以适应数据的线性可分性。对于可分的数据集，使用线性核的 SVM 与逻辑回归的表现相当。除此之外，如果使用非线性核（如 RBF），SVM
    也能很好地处理非可分数据集。而逻辑回归在高维数据集上可能会遇到挑战，而 SVM 仍能表现良好。一个很好的例子是新闻分类，其中特征的维度通常达到数万维。总的来说，使用合适的核和参数，SVM
    能实现非常高的准确性，但这可能会以大量计算和高内存消耗为代价。
- en: Random forest (or decision tree)
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机森林（或决策树）
- en: The linear separability of the data does not matter to this algorithm, and it
    works directly with categorical features without encoding, which provides great
    ease of use. Also, the trained model is very easy to interpret and explain to
    non-machine learning practitioners, which cannot be achieved with most other algorithms.
    Additionally, random forest boosts the decision tree algorithm, which can reduce
    overfitting by ensembling a collection of separate trees. Its performance is comparable
    to SVM, while fine-tuning a random forest model is less difficult compared to
    SVM and neural networks.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的线性可分性对该算法并不重要，它可以直接处理类别特征而无需编码，这提供了极大的使用便利。此外，训练后的模型非常容易解释，并且可以向非机器学习从业者说明，这一点是大多数其他算法无法做到的。另外，随机森林增强了决策树算法，通过集成多个独立的树来减少过拟合。其性能可与SVM相媲美，同时相比于SVM和神经网络，微调随机森林模型要容易得多。
- en: Neural networks
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络
- en: 'These are extremely powerful, especially with the development of deep learning.
    However, finding the right topology (layers, nodes, activation functions, and
    so on) is not easy, not to mention the time-consuming model of training and tuning.
    Hence, they are not recommended as an algorithm to start with for general machine
    learning problems. However, for computer vision and many NLP tasks, the neural
    network is still the go-to model. In summary, here are some scenarios where using
    neural networks is particularly beneficial:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络非常强大，特别是随着深度学习的发展。然而，找到合适的拓扑结构（层、节点、激活函数等）并不容易，更不用说耗时的训练和调优过程了。因此，它们并不适合作为一般机器学习问题的起始算法。然而，对于计算机视觉和许多NLP任务，神经网络仍然是首选模型。总之，以下是使用神经网络特别有益的一些场景：
- en: '**Complex patterns**: When the task involves learning complex patterns or relationships
    within the data that may be difficult for traditional algorithms to capture.'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复杂模式**：当任务涉及学习数据中的复杂模式或关系，而这些模式对于传统算法来说可能很难捕捉时。'
- en: '**Large amounts of data**: Neural networks tend to perform well when you have
    a substantial amount of data available for training, as they are capable of learning
    from large datasets.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**大量数据**：当你有足够的数据可用于训练时，神经网络通常表现得很好，因为它们能够从大量数据集中学习。'
- en: '**Unstructured data**: Neural networks excel in handling unstructured data
    types like images, audio, and text, where traditional methods might struggle to
    extract meaningful features. For NLP tasks like sentiment analysis, machine translation,
    named entity recognition, and text generation, neural networks, especially recurrent
    and transformer models, have shown remarkable performance. In image classification,
    object detection, segmentation, and image generation tasks, deep neural networks
    have revolutionized computer vision.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**非结构化数据**：神经网络擅长处理非结构化数据类型，如图像、音频和文本，而传统方法可能在提取有意义特征时遇到困难。在情感分析、机器翻译、命名实体识别和文本生成等NLP任务中，神经网络，特别是循环神经网络和变换器模型，已经展现了卓越的性能。在图像分类、目标检测、图像分割和图像生成任务中，深度神经网络彻底改变了计算机视觉领域。'
- en: Best practice 16 – Reducing overfitting
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践16 – 减少过拟合
- en: 'We touched on ways to avoid overfitting when discussing the pros and cons of
    algorithms in the last practice. We herein formally summarize them, as follows:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论算法的优缺点时，我们提到了避免过拟合的方法。这里我们正式总结如下：
- en: '**More data, if possible**: Increase the size of your training dataset. More
    data can help the model learn relevant patterns and reduce its tendency to memorize
    noise.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更多数据，若可能**：增加训练数据集的规模。更多数据可以帮助模型学习到相关的模式，并减少其记忆噪声的倾向。'
- en: '**Simplification, if possible**: The more complex the model is, the higher
    the chance of overfitting. Complex models include a tree or forest with excessive
    depth, a linear regression with a high degree of polynomial transformation, and
    an SVM with a complicated kernel.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简化，若可能**：模型越复杂，过拟合的可能性越高。复杂模型包括具有过深深度的树或森林、具有高次多项式转换的线性回归、以及具有复杂核函数的SVM。'
- en: '**Cross-validation**: A good habit that we have built over all of the chapters
    in this book.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：这是我们在本书所有章节中培养出的一个良好习惯。'
- en: '**Regularization**: This adds penalty terms to reduce the error caused by fitting
    the model perfectly on the given training set.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正则化**：这通过添加惩罚项来减少因在给定训练集上完美拟合模型而导致的误差。'
- en: '**Early stopping**: Monitor the model’s performance on a validation set during
    training. Stop training when the performance on the validation set starts to degrade,
    indicating that the model is starting to overfit.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**早停法**：在训练过程中监控模型在验证集上的表现。当验证集上的表现开始下降时，停止训练，表明模型开始出现过拟合。'
- en: '**Dropout**: In neural networks, apply dropout layers during training. Dropout
    randomly drops out a fraction of neurons during each forward pass, preventing
    reliance on specific neurons.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**（丢弃法）：在神经网络中，在训练过程中应用丢弃层。丢弃法在每次前向传播时随机丢弃一部分神经元，从而防止对特定神经元的依赖。'
- en: '**Feature selection**: Select a subset of relevant features. Removing irrelevant
    or redundant features can prevent the model from fitting noise.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征选择**：选择一个相关特征的子集。去除无关或冗余的特征可以防止模型对噪声进行拟合。'
- en: '**Ensemble learning**: This involves combining a collection of weak models
    to form a stronger one.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**集成学习**：这涉及将一组弱模型结合成一个更强的模型。'
- en: So, how can we tell whether a model suffers from overfitting, or the other extreme,
    underfitting? Let’s see the next section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何判断一个模型是出现了过拟合，还是另一种极端，欠拟合呢？我们来看看下一节。
- en: Best practice 17 – Diagnosing overfitting and underfitting
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 17 – 诊断过拟合与欠拟合
- en: A **learning curve** is usually used to evaluate the bias and variance of a
    model. A learning curve is a graph that compares the cross-validated training
    and testing scores over a given number of training samples.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习曲线**通常用来评估模型的偏差和方差。学习曲线是一种图表，比较了在一定数量的训练样本上，交叉验证的训练和测试得分。'
- en: For a model that fits well on the training samples, the performance of the training
    samples should be beyond what’s desired. Ideally, as the number of training samples
    increases, the model performance on the testing samples will improve; eventually,
    the performance on the testing samples will become close to that of the training
    samples.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在训练样本上拟合良好的模型，训练样本的表现应该超过期望值。理想情况下，随着训练样本数量的增加，模型在测试样本上的表现会有所提升；最终，测试样本的表现会接近训练样本的表现。
- en: When the performance on the testing samples converges at a value much lower
    than that of the training performance, overfitting can be concluded. In this case,
    the model fails to generalize to instances that have not been seen.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当测试样本上的表现收敛到一个远低于训练表现的值时，可以得出过拟合的结论。在这种情况下，模型无法推广到未见过的实例。
- en: 'For a model that does not even fit well on the training samples, underfitting
    is easily spotted: both performances on the training and testing samples are below
    the desired performance in the learning curve.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个在训练样本上表现都不佳的模型，容易识别出欠拟合：训练和测试样本上的表现都低于学习曲线中期望的表现。
- en: 'Here is an example of the learning curve in an ideal case:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个理想情况下学习曲线的例子：
- en: '![A picture containing text, screenshot, line, diagram  Description automatically
    generated](img/B21047_10_03.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、截图、线条、图表的图片 自动生成的描述](img/B21047_10_03.png)'
- en: 'Figure 10.3: Ideal learning curve'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3：理想学习曲线
- en: 'An example of the learning curve for an overfitted model is shown in the following
    diagram:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合模型的学习曲线示例如下图所示：
- en: '![A diagram of training data  Description automatically generated with low
    confidence](img/B21047_10_04.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![训练数据图示 自动生成的描述，置信度较低](img/B21047_10_04.png)'
- en: 'Figure 10.4: Overfitting learning curve'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4：过拟合学习曲线
- en: 'The learning curve for an underfitted model may look like the following diagram:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合模型的学习曲线可能如下图所示：
- en: '![A picture containing text, screenshot, line, diagram  Description automatically
    generated](img/B21047_10_05.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![一张包含文本、截图、线条、图表的图片 自动生成的描述](img/B21047_10_05.png)'
- en: 'Figure 10.5: Underfitting learning curve'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5：欠拟合学习曲线
- en: To generate the learning curve, you can utilize the `learning_curve` module
    ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve))
    from scikit-learn, and the `plot_learning_curve` function defined at [https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成学习曲线，你可以使用 scikit-learn 中的 `learning_curve` 模块 ([https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.learning_curve.html#sklearn.model_selection.learning_curve))，以及在
    [https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_learning_curve.html)
    定义的 `plot_learning_curve` 函数。
- en: Best practice 18 – Modeling on large-scale datasets
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 18 - 在大规模数据集上建模
- en: We gained experience working with large datasets in *Chapter 4*, *Predicting
    Online Ad Click-Through with Logistic Regression*. There are a few tips that can
    help you model on large-scale data more efficiently.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在*第四章*《使用逻辑回归预测在线广告点击率》中积累了处理大规模数据集的经验。这里有一些技巧可以帮助你更高效地在大规模数据上建模。
- en: First, start with a small subset, for instance, a subset that can fit on your
    local machine. This can help speed up early experimentation. Obviously, you don’t
    want to train on the entire dataset just to find out whether SVM or random forest
    works better. Instead, you can randomly sample data points and quickly run a few
    models on the selected set.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，从一个较小的子集开始，例如一个可以在本地机器上运行的子集。这有助于加速早期实验。显然，你不想在整个数据集上进行训练，只是为了判断 SVM 或随机森林哪个效果更好。相反，你可以随机采样数据点，并在选定的子集上快速运行几个模型。
- en: The second tip is choosing scalable algorithms, such as logistic regression,
    linear SVM, and SGD-based optimization. This is quite intuitive.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个建议是选择可扩展的算法，如逻辑回归、线性 SVM 和基于 SGD 的优化算法。这一点非常直观。
- en: 'Here are other best practices for modeling on large-scale datasets:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 这里还有其他在大规模数据集上建模的最佳实践：
- en: '**Sampling and subset selection**: When starting model development, work with
    smaller subsets of your data to iterate and experiment quickly. Once your model
    architecture and parameters are tuned, scale up to the full dataset.'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**采样和子集选择**：在开始模型开发时，使用较小的子集来快速迭代和实验。一旦模型架构和参数调整完毕，再扩展到完整的数据集。'
- en: '**Distributed computing**: Utilize distributed computing frameworks like Apache
    Spark to handle large-scale data processing and model training across multiple
    nodes or clusters.'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式计算**：利用像 Apache Spark 这样的分布式计算框架，在多个节点或集群上处理大规模数据处理和模型训练。'
- en: '**Feature engineering**: Focus on relevant features and avoid unnecessary dimensions.
    Use dimensionality reduction techniques like PCA or t-SNE to reduce feature space
    if needed.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征工程**：专注于相关特征，避免不必要的维度。如果需要，可以使用像 PCA 或 t-SNE 这样的降维技术来减少特征空间。'
- en: '**Parallelization**: Explore techniques to parallelize training, like data
    parallelism or model parallelism, to leverage multiple GPUs or distributed systems.'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**并行化**：探索并行化训练的技术，如数据并行或模型并行，以利用多个 GPU 或分布式系统。'
- en: '**Memory management**: Optimize memory usage by using data generators, streaming
    data from storage, and releasing memory when no longer needed.'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存管理**：通过使用数据生成器、从存储流式传输数据以及在不再需要时释放内存来优化内存使用。'
- en: '**Optimized libraries**: Choose libraries and frameworks that are optimized
    for large-scale data, such as TensorFlow, PyTorch, scikit-learn, and XGBoost.'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化的库**：选择针对大规模数据优化的库和框架，如 TensorFlow、PyTorch、scikit-learn 和 XGBoost。'
- en: '**Incremental learning**: For streaming data or dynamic datasets, consider
    incremental learning techniques that update the model as new data arrives.'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增量学习**：对于流式数据或动态数据集，考虑使用增量学习技术，在新数据到达时更新模型。'
- en: Last but not least, don’t forget to save the trained model. Training on a large
    dataset takes a long time, which you would want to avoid redoing, if possible.
    We will explore saving and loading models in detail in *Best practice 19 – Saving,
    loading, and reusing models*, which is a part of the deployment and monitoring
    stage.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，别忘了保存训练好的模型。训练大规模数据集需要很长时间，如果可能的话，你希望避免重复这一过程。我们将在*最佳实践 19 - 保存、加载和重用模型*中详细探讨保存和加载模型的内容，这是部署和监控阶段的一部分。
- en: Best practices in the deployment and monitoring stage
  id: totrans-246
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 部署和监控阶段的最佳实践
- en: After performing all processes in the previous three stages, we now have a well-established
    data preprocessing pipeline and a correctly trained prediction model. The last
    stage of a machine learning system involves saving those resulting models from
    previous stages and deploying them on new data, as well as monitoring their performance
    and updating the prediction models regularly. We also need to implement monitoring
    and logging to track model performance, training progress, and potential issues
    during training.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 完成前面三个阶段的所有过程后，我们现在拥有了一个完善的数据预处理流水线和一个正确训练的预测模型。机器学习系统的最后阶段包括保存前面阶段产生的模型，并将它们部署到新数据上，同时监控它们的性能并定期更新预测模型。我们还需要实现监控和日志记录，以跟踪模型的性能、训练进度以及训练过程中可能出现的问题。
- en: Best practice 19 – Saving, loading, and reusing models
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践 19 – 保存、加载和重用模型
- en: When machine learning is deployed, new data should go through the same data
    preprocessing procedures (scaling, feature engineering, feature selection, dimensionality
    reduction, and so on) as in the previous stages. The preprocessed data is then
    fed into the trained model. We simply cannot rerun the entire process and retrain
    the model every time new data comes in. Instead, we should save the established
    preprocessing models and trained prediction models after the corresponding stages
    have been completed. In deployment mode, these models are loaded in advance and
    are used to produce prediction results from the new data. Let’s explore methods
    for saving and loading models using pickle, TensorFlow, and PyTorch below.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 当机器学习被部署时，新的数据应经过与前面阶段相同的数据预处理程序（缩放、特征工程、特征选择、降维等）。预处理后的数据将输入到训练好的模型中。我们不能在每次新数据到来时重新运行整个过程并重新训练模型。相反，我们应该在相应阶段完成后保存已经建立的预处理模型和训练好的预测模型。在部署模式下，这些模型会提前加载，并用来从新数据中生成预测结果。接下来，让我们探讨如何使用
    pickle、TensorFlow 和 PyTorch 保存和加载模型的方法。
- en: Saving and restoring models using pickle
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 pickle 保存和恢复模型
- en: 'We start with using `pickle`. This can be illustrated via the diabetes example,
    where we standardize the data and employ an `SVR` model, as follows:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从使用 `pickle` 开始。通过糖尿病示例来说明这一点，其中我们对数据进行标准化，并使用 `SVR` 模型，如下所示：
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Preprocess the training data with scaling, as shown in the following commands:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 对训练数据进行缩放预处理，如下命令所示：
- en: '[PRE29]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now save the established standardizer, the `scaler` object with `pickle`, as
    follows:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在使用 `pickle` 保存已建立的标准化器和 `scaler` 对象，如下所示：
- en: '[PRE30]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This generates a `scaler.p` file.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个 `scaler.p` 文件。
- en: 'Move on to training an `SVR` model on the scaled data, as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，继续在缩放数据上训练一个 `SVR` 模型，如下所示：
- en: '[PRE31]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Save the trained `regressor` object with `pickle`, as follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pickle` 保存训练好的 `regressor` 对象，如下所示：
- en: '[PRE32]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This generates a `regressor.p` file.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这会生成一个 `regressor.p` 文件。
- en: 'In the deployment stage, we first load the saved standardizer and the `regressor`
    object from the preceding two files, as follows:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署阶段，我们首先从前面两个文件加载保存的标准化器和 `regressor` 对象，如下所示：
- en: '[PRE33]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Then, we preprocess the new data using the standardizer and make a prediction
    with the `regressor` object just loaded, as follows:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用标准化器对新数据进行预处理，并使用刚加载的 `regressor` 对象进行预测，如下所示：
- en: '[PRE34]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Saving and restoring models in TensorFlow
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在 TensorFlow 中保存和恢复模型
- en: 'I will also demonstrate how to save and restore models in TensorFlow. As an
    example, we will train a simple logistic regression model on the cancer dataset,
    save the trained model, and reload it in the following steps:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我还将演示如何在 TensorFlow 中保存和恢复模型。作为示例，我们将在癌症数据集上训练一个简单的逻辑回归模型，保存训练好的模型，并在接下来的步骤中重新加载：
- en: 'Import the necessary TensorFlow modules and load the cancer dataset from `scikit-learn`
    and rescale the data:'
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的 TensorFlow 模块，并从 `scikit-learn` 加载癌症数据集并对数据进行重缩放：
- en: '[PRE35]'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Build a simple logistic regression model using the Keras Sequential API, along
    with several specified parameters:'
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Keras Sequential API 构建一个简单的逻辑回归模型，并指定几个参数：
- en: '[PRE36]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Train the TensorFlow model against the data:'
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据训练 TensorFlow 模型：
- en: '[PRE37]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Display the model’s architecture:'
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型的架构：
- en: '[PRE38]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We will see if we can retrieve the same model later.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将检查是否能够稍后恢复相同的模型。
- en: 'Hopefully, the previous steps look familiar to you. If not, feel free to review
    our TensorFlow implementation. Now we save the model to a path:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 希望前面的步骤你已经熟悉。如果不熟悉，可以随时查看我们的 TensorFlow 实现。现在我们将模型保存到一个路径中：
- en: '[PRE39]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: After this, you will see that a folder called `model_tf` is created. The folder
    contains the trained model’s architecture, weights, and training configuration.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你会看到一个名为`model_tf`的文件夹被创建。该文件夹包含训练模型的架构、权重和训练配置。
- en: 'Finally, we load the model from the previous path and display the loaded model’s
    path:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们从之前的路径加载模型，并显示加载后的模型路径：
- en: '[PRE40]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We just loaded back the exact same model.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚重新加载了完全相同的模型。
- en: Saving and restoring models in PyTorch
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在PyTorch中保存和恢复模型
- en: 'Finally, let’s see how to save and restore models in PyTorch. Similarly, we
    will train a simple logistic regression model on the same cancer dataset, save
    the trained model, and reload it in the following steps:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看一下如何在PyTorch中保存和恢复模型。同样，我们将在相同的癌症数据集上训练一个简单的逻辑回归模型，保存训练好的模型，并在接下来的步骤中重新加载它：
- en: 'Convert the data `torch` tensors used for modeling:'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换用于建模的`torch`张量：
- en: '[PRE41]'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Build a simple logistic regression model using the `nn.sequential` module,
    along with the loss function and optimizer:'
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`nn.sequential`模块以及损失函数和优化器构建一个简单的逻辑回归模型：
- en: '[PRE42]'
  id: totrans-289
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Reuse the `train_step` function we developed previously in *Chapter 6*, *Predicting
    Stock Prices with Artificial Neural Networks*, and train the `PyTorch` model against
    the data for 10 iterations:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重用我们在*第6章 - 使用人工神经网络预测股票价格*中开发的`train_step`函数，并将`PyTorch`模型在数据上训练10轮：
- en: '[PRE43]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Display the model’s architecture:'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型的架构：
- en: '[PRE44]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: We will see if we can retrieve the same model later.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将看看是否可以稍后检索到相同的模型。
- en: 'Hopefully, the previous steps look familiar to you. If not, feel free to review
    our PyTorch implementation. Now we save the model to a path:'
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 希望之前的步骤对你来说是熟悉的。如果不是，可以随时查看我们的PyTorch实现。现在，我们将模型保存到一个路径：
- en: '[PRE45]'
  id: totrans-296
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: After this, you will see that a folder called `model.pth` is created. The folder
    contains the entire trained model’s architecture, weights, and training configuration.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，你会看到一个名为`model.pth`的文件夹被创建。该文件夹包含整个训练模型的架构、权重和训练配置。
- en: 'Finally, we load the model from the previous path and display the loaded model’s
    path:'
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们从之前的路径加载模型，并显示加载后的模型路径：
- en: '[PRE46]'
  id: totrans-299
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: We just loaded back the exact same model.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚重新加载了完全相同的模型。
- en: Best practice 20 – Monitoring model performance
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践20 - 监控模型性能
- en: 'The machine learning system is now up and running. To make sure everything
    is on the right track, we need to conduct performance checks on a regular basis.
    To do so, besides making a prediction in real time, we should also record the
    ground truth at the same time. Here are some best practices for monitoring model
    performance:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习系统现在已启动并运行。为了确保一切正常，我们需要定期进行性能检查。为此，除了实时预测之外，我们还应该同时记录真实值。以下是一些监控模型性能的最佳实践：
- en: '**Define evaluation metrics**: Choose appropriate evaluation metrics that align
    with your problem’s goals. Accuracy, precision, recall, F1-score, AUC-ROC, *R*²,
    and mean squared error are some common metrics.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定义评估指标**：选择与问题目标相符的评估指标。准确率、精确度、召回率、F1分数、AUC-ROC、*R*²和均方误差是一些常见的指标。'
- en: '**Baseline performance**: Establish a baseline model or a simple rule-based
    approach to compare your model’s performance. This provides context for understanding
    whether your model is adding value.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基线性能**：建立一个基线模型或简单的基于规则的方法，以比较模型的性能。这为理解模型是否有价值提供了背景。'
- en: '**Learning curves**: Plot learning curves showing training and validation loss
    or evaluation metrics over epochs. This helps identify overfitting or underfitting
    issues, as mentioned in *Best practice 17 – Diagnosing overfitting and underfitting*.'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**学习曲线**：绘制学习曲线，展示训练和验证损失或评估指标随周期的变化。这有助于识别过拟合或欠拟合问题，如在*最佳实践17 - 诊断过拟合和欠拟合*中所提到的。'
- en: 'Continuing with the diabetes example from earlier in the chapter, we conduct
    a performance check as follows:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 继续使用本章前面提到的糖尿病示例，我们进行如下性能检查：
- en: '[PRE47]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: We should log the performance and set up an alert for any decayed performance.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该记录性能并设置警报以监控性能衰退。
- en: Best practice 21 – Updating models regularly
  id: totrans-309
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最佳实践21 - 定期更新模型
- en: 'If the performance is getting worse, chances are that the pattern of data has
    changed. We can work around this by updating the model. Depending on whether online
    learning is feasible or not with the model, the model can be modernized with the
    new set of data (online updating) or retrained completely with the most recent
    data. Here are some best practices for the last section of the chapter:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果表现变差，可能是数据的模式发生了变化。我们可以通过更新模型来解决这个问题。根据模型是否支持在线学习，模型可以通过新的数据集进行在线更新，或使用最新的数据完全重新训练。以下是本章最后部分的一些最佳实践：
- en: '**Monitor model performance**: Continuously monitor model performance metrics.
    If there’s a significant drop, it’s a sign that the model needs updating.'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控模型表现**：持续监控模型表现指标。如果出现显著下降，这通常意味着模型需要更新。'
- en: '**Scheduled updates**: Implement a schedule for model updates based on the
    frequency of data changes and business needs. This ensures that the model remains
    relevant, without unnecessary updates.'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期更新**：根据数据变化频率和业务需求实施模型更新计划。这样可以确保模型保持相关性，避免不必要的更新。'
- en: '**Online updating**: For models that support online learning, update the model
    incrementally with new data. This applies to models based on gradient descent
    algorithms, or Naïve Bayes. Online updating minimizes the need for retraining
    the entire model and adapts it to changing patterns over time.'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在线更新**：对于支持在线学习的模型，使用新数据逐步更新模型。这适用于基于梯度下降算法或朴素贝叶斯的模型。在线更新可以减少重新训练整个模型的需要，并使模型随着时间推移适应变化的模式。'
- en: '**Version control**: Maintain version control of models and datasets to track
    changes and facilitate rollback if necessary. This helps in comparing model performance
    over time and reverting to previous versions if updates lead to performance degradation.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**版本控制**：保持模型和数据集的版本控制，以便跟踪变更，并在必要时进行回滚。这有助于在一段时间内对比模型表现，并在更新导致表现下降时恢复到先前版本。'
- en: '**Regular auditing**: Periodically review model performance, reevaluate business
    goals, and update your evaluation metrics if needed.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**定期审计**：定期审查模型表现，重新评估业务目标，并在必要时更新评估指标。'
- en: Remember that monitoring should be an ongoing process, starting from model development
    through deployment and maintenance. It ensures that your machine learning models
    remain effective, trustworthy, and aligned with your business objectives.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，监控应该是一个持续的过程，从模型开发到部署和维护，确保你的机器学习模型始终有效、值得信赖，并与业务目标保持一致。
- en: Summary
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'The purpose of this chapter is to prepare you for real-world machine learning
    problems. We started with the general workflow that a machine learning solution
    follows: data preparation, training set generation, algorithm training, evaluation
    and selection, and finally, system deployment and monitoring. We then went, in
    depth, through the typical tasks, common challenges, and best practices for each
    of these four stages.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的目的是为你准备好应对真实世界中的机器学习问题。我们从机器学习解决方案的通用工作流程开始：数据准备、训练集生成、算法训练、评估与选择，最后是系统部署和监控。然后，我们深入探讨了每个阶段的典型任务、常见挑战和最佳实践。
- en: Practice makes perfect. The most important best practice is practice itself.
    Get started with a real-world project to deepen your understanding and apply what
    you have learned so far.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 实践出真知。最重要的最佳实践就是实践本身。通过一个真实世界的项目开始，深化你的理解，并应用你所学到的知识。
- en: In the next chapter, we will start our deep learning journey by categorizing
    clothing images using convolutional neural networks.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，我们将开始我们的深度学习之旅，使用卷积神经网络对服装图片进行分类。
- en: Exercises
  id: totrans-321
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: Can you use word embedding to extract text features and develop a multiclass
    classifier to classify the newsgroup data? (Note that you might not be able to
    get better results with word embedding than tf-idf, but it is good practice.)
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能使用词嵌入提取文本特征，并开发一个多类分类器来分类新闻组数据吗？（请注意，使用词嵌入可能无法比 tf-idf 获得更好的结果，但它是一个很好的实践。）
- en: Can you find several challenges in Kaggle ([www.kaggle.com](https://www.kaggle.com))
    and practice what you have learned throughout the entire book?
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你能在 Kaggle ([www.kaggle.com](https://www.kaggle.com)) 上找到一些挑战并实践你在整本书中学到的内容吗？
- en: Join our book’s Discord space
  id: totrans-324
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 讨论空间
- en: 'Join our community’s Discord space for discussions with the authors and other
    readers:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们社区的 Discord 讨论空间，与作者和其他读者交流：
- en: '[https://packt.link/yuxi](https://packt.link/yuxi)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/yuxi](https://packt.link/yuxi)'
- en: '![](img/QR_Code1878468721786989681.png)'
  id: totrans-327
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code1878468721786989681.png)'
