- en: Classification and Regression with Trees
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基于树的分类与回归
- en: 'Tree based algorithms are very popular for two reasons: they are interpretable,
    and they make sound predictions that have won many machine learning competitions
    on online platforms, such as Kaggle. Furthermore, they have many use cases outside
    of machine learning for solving problems, both simple and complex.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法因其可解释性和声音预测而非常流行，在在线平台如Kaggle上赢得了许多机器学习竞赛。此外，它们在解决问题的简单和复杂用例中也有许多应用。
- en: Building a tree is an approach to decision-making used in almost all industries.
    Trees can be used to solve both classification- and regression-based problems,
    and have several use cases that make them the go-to solution!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 构建树是几乎所有行业中用于决策制定的方法。树可用于解决基于分类和回归的问题，并有几个用例使其成为首选解决方案！
- en: 'This chapter is broadly divided into the following two sections:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节主要分为以下两个部分：
- en: Classification trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类树
- en: Regression trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归树
- en: Each section will cover the fundamental theory of different types of tree based
    algorithms, along with their implementation in scikit-learn. By the end of this
    chapter, you will have learned how to aggregate several algorithms into an **ensemble**
    and have them vote on what the best prediction is.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 每个部分将涵盖不同类型基于树的算法的基本理论，以及它们在scikit-learn中的实现。通过本章结束时，您将学会如何将多个算法聚合到一个**集成**中，并让它们投票决定最佳预测结果。
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will be required to have Python 3.6 or greater, Pandas ≥ 0.23.4, Scikit-learn
    ≥ 0.20.0, and Matplotlib ≥ 3.0.0 installed on your system.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在系统上安装 Python 3.6 或更高版本，Pandas ≥ 0.23.4，Scikit-learn ≥ 0.20.0 和 Matplotlib
    ≥ 3.0.0。
- en: 'The code files of this chapter can be found on GitHub:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码文件可在GitHub上找到：
- en: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb)[.](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb)'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb)[.](https://github.com/PacktPublishing/Machine-Learning-with-scikit-learn-Quick-Start-Guide/blob/master/Chapter_06.ipynb)'
- en: 'Check out the following video to see the code in action:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 观看以下视频，了解代码的实际操作：
- en: '[http://bit.ly/2SrPP7R](http://bit.ly/2SrPP7R)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://bit.ly/2SrPP7R](http://bit.ly/2SrPP7R)'
- en: Classification trees
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类树
- en: Classification trees are used to predict a category or class. This is similar
    to the classification algorithms that you have learned about previously in this
    book, such as the k-nearest neighbors algorithm or logistic regression.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 分类树用于预测类别或类别。这类似于您在本书中之前学习过的分类算法，如k最近邻算法或逻辑回归。
- en: 'Broadly speaking, there are three tree based algorithms that are used to solve
    classification problems:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 广义上讲，有三种基于树的算法用于解决分类问题：
- en: The decision tree classifier
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: The random forest classifier
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: The AdaBoost classifier
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaBoost分类器
- en: In this section, you will learn how each of these tree based algorithms works,
    in order to classify a row of data as a particular class or category.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习每种基于树的算法如何工作，以便将数据行分类为特定的类别或类别。
- en: The decision tree classifier
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树分类器
- en: 'The decision tree is the simplest tree based algorithm, and serves as the foundation
    for the other two algorithms. Let''s consider the following simple decision tree:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是最简单的基于树的算法，也是其他两种算法的基础。让我们考虑以下简单的决策树：
- en: '![](img/93f38146-379b-4b23-8502-5318cb631c9f.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/93f38146-379b-4b23-8502-5318cb631c9f.png)'
- en: A simple decision tree
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 简单的决策树
- en: 'A decision tree, in simple terms, is a set of rules that help us classify observations
    into distinct groups. In the previous diagram, the rule could be written as the
    following:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，决策树是一组帮助我们将观察结果分类为不同组的规则。在前面的图表中，规则可以写成如下形式：
- en: '[PRE0]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The preceding decision tree perfectly divides the observations into two distinct
    groups. This is a characteristic of the ideal decision tree. The first box on
    the top is called the **root** of the tree, and is the most important feature
    of the tree when it comes to deciding how to group the observations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 先前的决策树完美地将观察结果划分为两个不同的组。这是理想决策树的特征。顶部的第一个框被称为**根**，在决定如何分组观察结果时是树的最重要特征。
- en: The boxes under the root node are known as the **children**. In the preceding
    tree, the **children** are also the **leaf** nodes. The **leaf** is the last set
    of boxes, usually in the bottommost part of the tree. As you might have guessed,
    the decision tree represents a regular tree, but inverted, or upside down.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 根节点下的框被称为**子节点**。在前面的树中，**子节点**也是**叶节点**。**叶节点**是最后一组框，通常位于树的最底部。如你所料，决策树实际上是一个常规的树形结构，只不过是倒置的。
- en: Picking the best feature
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择最佳特征
- en: 'How does the decision tree decide which feature is the best? The best feature
    is one that offers the best possible split, and divides the tree into two or more
    distinct groups, depending on the number of classes or categories that we have
    in the data. Let''s have a look at the following diagram:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树如何决定哪个特征最好？最佳特征是提供最佳划分的特征，它将树划分为两个或更多不同的组，具体取决于数据中包含的类别或类的数量。让我们看以下图示：
- en: '![](img/019a959a-1aad-4ef8-841b-bfc4290a4c20.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![](img/019a959a-1aad-4ef8-841b-bfc4290a4c20.png)'
- en: A decision tree showing a good split
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 展示良好划分的决策树
- en: 'In the preceding diagram, the following happens:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，发生了以下情况：
- en: The tree splits the data from the root node into two distinct groups.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该树将根节点中的数据划分为两个不同的组。
- en: In the left-hand group, we see that there are two triangles and one circle.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧组中，我们看到有两个三角形和一个圆形。
- en: In the right-hand group, we see that there are two circles and one triangle.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在右侧组中，我们看到有两个圆形和一个三角形。
- en: Since the tree got the majority of each class into one group, we can say that
    the tree has done a good job when it comes to splitting the data into distinct
    groups.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于树将每个类的大多数元素分配到一个组中，我们可以说，在将数据划分成不同组方面，树做得很好。
- en: 'Let''s take a look at another example—this time, one in which the split is
    bad. Consider the following diagram:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子——这次是一个划分不好的例子。考虑下面的图示：
- en: '![](img/ff259ed3-b1df-482f-aab2-76d4b8185849.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ff259ed3-b1df-482f-aab2-76d4b8185849.png)'
- en: A decision tree with a bad split
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 展示坏划分的决策树
- en: 'In the preceding diagram, the following happens:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，发生了以下情况：
- en: The tree splits the data in the root node into four distinct groups. This is
    bad in itself, as it is clear that there are only two categories (circle and triangle).
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该树将根节点中的数据划分成四个不同的组。从本身来看，这是不好的，因为显然只有两种类别（三角形和圆形）。
- en: Furthermore, each group has one triangle and one circle.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此外，每个组都有一个三角形和一个圆形。
- en: There is no majority class or category in any one of the four groups. Each group
    has 50% of one category; therefore, the tree cannot come to a conclusive decision,
    unless it relies on more features, which then increases the complexity of the
    tree.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这四个组中没有任何一个组有明显的主要类别或类别。每个组中有50%的某个类别；因此，除非树依赖于更多特征，否则无法得出最终决策，这样会增加树的复杂性。
- en: The Gini coefficient
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基尼系数
- en: 'The metric that the decision tree uses to decide if the root node is called
    the *Gini coefficient*. The higher the value of this coefficient, the better the
    job that this particular feature does at splitting the data into distinct groups.
    In order to learn how to compute the Gini coefficient for a feature, let''s consider
    the following diagram:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树用来判断根节点的度量被称为*基尼系数*。基尼系数的值越高，说明这个特征在将数据划分为不同组方面的效果越好。为了学习如何计算特征的基尼系数，我们来看一下以下图示：
- en: '![](img/c9879aa0-1163-476e-bbcf-1e026de8d171.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c9879aa0-1163-476e-bbcf-1e026de8d171.png)'
- en: Computing the Gini coefficient
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 计算基尼系数
- en: 'In the preceding diagram, the following happens:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，发生了以下情况：
- en: The feature splits the data into two groups.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该特征将数据划分成两个组。
- en: In the left-hand group, we have two triangles and one circle.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧组中，我们有两个三角形和一个圆形。
- en: Therefore, the Gini for the left-hand group is (2 triangles/3 total data points)^2+
    (1 circle/3 total data points)^2.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，左侧组的基尼系数为（2个三角形/3个总数据点）^2 + （1个圆形/3个总数据点）^2。
- en: 'To calculate this, do the following: ![](img/c7c76b50-1651-43c4-92b6-066af865a3eb.png)0.55.'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算此值的方法如下： ![](img/c7c76b50-1651-43c4-92b6-066af865a3eb.png)0.55。
- en: A value of 0.55 for the Gini coefficient indicates that the root of this tree
    splits the data in such a way that each group has a majority category.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基尼系数为0.55表示该树的根节点以这样的方式划分数据：每个组都有一个主要类别。
- en: A perfect root feature would have a Gini coefficient of 1\. This means that
    each group has only one class/category.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个完美的根特征的基尼系数为1。这意味着每个组只有一个类/类别。
- en: A bad root feature would have a Gini coefficient of 0.5, which indicates that
    there is no distinct class/category in a group.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个不好的根特征的基尼系数为0.5，这意味着组内没有明显的类或类别。
- en: In reality, the decision tree is built in a recursive manner, with the tree
    picking a random attribute for the root and then computing the Gini coefficient
    for that attribute. It does this until it finds the attribute that best splits
    the data in a node into groups that have distinct classes and categories.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，决策树是以递归方式构建的，树会选择一个随机属性作为根节点，然后计算该属性的基尼系数。它会一直这样做，直到找到能够最好地将数据在节点中划分为具有不同类别和类别的属性。
- en: Implementing the decision tree classifier in scikit-learn
  id: totrans-57
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现决策树分类器
- en: 'In this section, you will learn how to implement the decision tree classifier
    in scikit-learn. We will work with the same fraud detection dataset. The first
    step is to load the dataset into the Jupyter Notebook. We can do this by using
    the following code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何在scikit-learn中实现决策树分类器。我们将使用相同的欺诈检测数据集。第一步是将数据集加载到Jupyter Notebook中。我们可以使用以下代码来实现：
- en: '[PRE1]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The next step is to split the data into training and test sets. We can do this
    using the following code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将数据拆分为训练集和测试集。我们可以使用以下代码实现：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can now build the initial decision tree classifier on the training data,
    and test its accuracy on the test data, by using the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以通过以下代码，在训练数据上构建初始决策树分类器，并在测试数据上测试其准确性：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In the preceding code, we do the following:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行以下操作：
- en: First, we import `DecisionTreeClassifier` from scikit-learn.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从scikit-learn导入`DecisionTreeClassifier`。
- en: We then initialize a `DecisionTreeClassifier` object with two arguments. The
    first, `criterion`, is the metric with which the tree picks the most important
    features in a recursive manner, which, in this case, is the Gini coefficient.
    The second is `random_state`, which is set to 50 so that the model produces the
    same result every time we run it.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们用两个参数初始化`DecisionTreeClassifier`对象。第一个是`criterion`，它是树递归选择最重要特征的度量方式，在本例中是基尼系数。第二个是`random_state`，设置为50，以便每次运行时，模型都会产生相同的结果。
- en: Finally, we fit the model on the training data and evaluate its accuracy on
    the test data.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们在训练数据上拟合模型，并在测试数据上评估其准确性。
- en: Hyperparameter tuning for the decision tree
  id: totrans-68
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树的超参数调优
- en: 'The decision tree has a plethora of hyperparameters that require fine-tuning
    in order to derive the best possible model that reduces the generalization error
    as much as possible. In this section, we will focus on two specific hyperparameters:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树有大量的超参数，需要微调才能得到最优模型，从而尽可能减少泛化误差。在本节中，我们将重点讨论两个特定的超参数：
- en: '**Max depth**: This is the maximum number of children nodes that can grow out
    from the decision tree until the tree is cut off. For example, if this is set
    to 3, then the tree will use three children nodes and cut the tree off before
    it can grow any more.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最大深度：** 这是决策树可以生长的最大子节点数，直到树被截断。例如，如果将其设置为3，则树将使用三个子节点，并在无法再生长时截断树。'
- en: '**Min samples leaf:** This is the minimum number of samples, or data points,
    that are required to be present in the leaf node. The leaf node is the last node
    of the tree. If this parameter is, for example, set to a value of 0.04, it tells
    the tree that it must grow until the last node contains 4% of the total samples
    in the data.'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最小叶子样本数：** 这是在叶节点中需要存在的最小样本数或数据点。叶节点是树的最后一个节点。如果这个参数设置为0.04，它告诉树必须生长到最后一个节点，其中包含数据总样本的4%。'
- en: 'In order to optimize the ideal hyperparameter and to extract the best possible
    decision tree, we use the `GridSearchCV` module from scikit-learn. We can set
    this up using the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了优化理想的超参数并提取最佳的决策树，我们使用scikit-learn中的`GridSearchCV`模块。我们可以使用以下代码进行设置：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the preceding code, we do the following:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行以下操作：
- en: We first import the `GridSearchCV` module from scikit-learn.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从scikit-learn导入`GridSearchCV`模块。
- en: Next, we create a dictionary of possible values for the hyperparameters and
    store it as `grid_params`.
  id: totrans-76
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含超参数可能值的字典，并将其存储为`grid_params`。
- en: Finally, we create a `GridSearchCV` object with the decision tree classifier
    as the estimator; that is, the dictionary of hyperparameter values.
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们创建一个`GridSearchCV`对象，使用决策树分类器作为估算器；即包含超参数值的字典。
- en: We set the `scoring` argument as `accuracy`, since we want to extract the accuracy
    of the best model found by `GridSearchCV`.
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`scoring`参数设置为`accuracy`，因为我们希望提取`GridSearchCV`找到的最佳模型的准确度。
- en: 'We then fit this grid object to the training data using the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用以下代码将此网格对象拟合到训练数据上：
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can then extract the best set of parameters using the following code:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码提取最佳参数集：
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The output of the preceding code indicates that a maximum depth of 1 and a
    minimum number of samples at the leaf node of 0.02 are the best parameters for
    this data. We can use these optimal parameters and construct a new decision tree
    using the following code:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的输出表明，最大深度为1，叶节点的最小样本数为0.02是此数据的最佳参数。我们可以使用这些最优参数，并使用以下代码构建一个新的决策树：
- en: '[PRE7]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Visualizing the decision tree
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化决策树
- en: 'One of the best aspects of building and implementing a decision tree in order
    to solve problems is that it can be interpreted quite easily, using a decision
    tree diagram that explains how the algorithm that you built works. In order to
    visualize a simple decision tree for the fraud detection dataset, we use the following
    code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 构建并实现决策树来解决问题的一个最佳方面是，它可以通过决策树图进行轻松解释，展示你构建的算法如何工作。为了可视化一个简单的决策树，应用于欺诈检测数据集，我们使用以下代码：
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We start by importing the required packages. The new packages here are the
    following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先导入所需的包。这里的新包如下：
- en: '`StringIO`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StringIO`'
- en: '`Image`'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Image`'
- en: '`export_graphviz`'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`export_graphviz`'
- en: '`pydotplus`'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pydotplus`'
- en: '`tree`'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tree`'
- en: The installations of the packages were covered in [Chapter 1](d81461f2-02a5-4154-a9b1-7a1f91882534.xhtml),
    *Introducing Machine Learning with scikit-learn*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 包的安装在[第1章](d81461f2-02a5-4154-a9b1-7a1f91882534.xhtml)中进行了说明，*《引入机器学习与scikit-learn》*。
- en: 'Then, we read in the dataset and initialize a decision tree classifier, as
    shown in the following code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 接着，我们读取数据集并初始化一个决策树分类器，如下代码所示：
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, we fit the tree on the features and target, and then extract the feature
    names separately:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将树拟合到特征和目标上，然后分别提取特征名称：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can then visualize the decision tree using the following code:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以使用以下代码可视化决策树：
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the preceding code, we do the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中，我们执行了以下操作：
- en: We use the `tree.export_graphviz()` function in order to construct the decision
    tree object, and store it in a variable called `data`.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`tree.export_graphviz()`函数构建决策树对象，并将其存储在一个名为`data`的变量中。
- en: 'This function uses a couple of arguments: `dt` is the decision tree that you
    built; `out_file` is set to `None`, as we do not want to send the tree visualization
    to any file outside our Jupyter Notebook; the `feature_names` are those we defined
    earlier; and `proportion` is, set to `True` (this will be explained in more detail
    later).'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该函数使用了几个参数：`dt`是你构建的决策树；`out_file`设置为`None`，因为我们不想将树的可视化结果保存到Jupyter Notebook以外的任何文件中；`feature_names`是我们之前定义的特征名称；`proportion`设置为`True`（稍后会详细解释）。
- en: We then construct a graph of the data contained within the tree so that we can
    visualize this decision tree graph by using the `pydotplus. graph_from_dot_data()`
    function on the `data` variable, which contains data about the decision tree.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们构建包含树中数据的图形，以便通过使用`pydotplus.graph_from_dot_data()`函数将决策树图可视化，这个函数作用于包含决策树数据的`data`变量。
- en: Finally, we visualize the decision tree using the `Image()` function, by passing
    the graph of the decision tree to it.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过`Image()`函数可视化决策树，将决策树图传递给它。
- en: 'This results in a decision tree like that illustrated in the following diagram:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这将得到一个类似于下图所示的决策树：
- en: '![](img/39330691-35d8-4b9e-a633-54f1aa59bf15.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/39330691-35d8-4b9e-a633-54f1aa59bf15.png)'
- en: The resultant decision tree
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 结果决策树
- en: 'The tree might seem pretty complex to interpret at first, but it''s not! In
    order to interpret this tree, let''s consider the root node and the first two
    children only. This is illustrated in the following diagram:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这棵树一开始可能看起来比较复杂，但其实并不是！为了理解这棵树，我们先考虑根节点和前两个子节点。这在下面的图示中进行了说明：
- en: '![](img/6cd82ac6-bd08-4705-9ffb-c72ef6898597.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](img/6cd82ac6-bd08-4705-9ffb-c72ef6898597.png)'
- en: A snippet of the decision tree
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的片段
- en: 'In the preceding diagram, note the following:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的图示中，请注意以下几点：
- en: In the root node, the tree has identified the 'step' feature as the feature
    with the highest Gini value.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在根节点中，树已识别出“step”特征为具有最高Gini值的特征。
- en: The root node makes the split in such a way that 0.71, or 71%, of the data falls
    into the non-fraudulent transactions, while 0.29, or 29%, of the transactions
    fall into the fraudulent transactions category.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根节点通过这样的方式进行分裂：71%的数据（即0.71）进入非欺诈交易类别，而29%的数据（即0.29）进入欺诈交易类别。
- en: If the step is greater than or equal to 7.5 (the right-hand side), then all
    of the transactions are classified as fraudulent.
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果步骤大于或等于7.5（右侧），则所有交易都被分类为欺诈交易。
- en: If the step is less than or equal to 7.5 (the left-hand side), then 0.996, or
    99.6%, of the transactions are classified as non-fraudulent, while 0.004, or 0.4%,
    of the transactions are classified as fraudulent.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果步骤小于或等于7.5（左侧），则99.6%的交易（即0.996）被分类为非欺诈交易，而0.4%的交易（即0.004）被分类为欺诈交易。
- en: If the amount is greater than or equal to 4,618,196.0, then all of the transactions
    are classified as fraudulent.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果金额大于或等于4,618,196.0，则所有交易都被分类为欺诈交易。
- en: If the amount is less than or equal to 4,618,196.0, then 0.996, or 99.6%, of
    the transactions are classified as non-fraudulent, while 0.004, or 0.4%, of the
    transactions are classified as fraudulent.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果金额小于或等于4,618,196.0，则99.6%的交易（即0.996）被分类为非欺诈交易，而0.4%的交易（即0.004）被分类为欺诈交易。
- en: Note how the decision tree is simply a set of If-then rules, constructed in
    a nested manner.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，决策树实际上仅仅是一组“如果-那么”规则，这些规则是以嵌套的方式构建的。
- en: The random forests classifier
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林分类器
- en: Now that you understand the core principles of the decision tree at a very foundational
    level, we will next explore what random forests are. Random forests are a form
    of *ensemble* learning. An ensemble learning method is one that makes use of multiple
    machine learning models to make a decision.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经从最基础的层面理解了决策树的核心原理，接下来我们将探索什么是随机森林。随机森林是一种*集成*学习方法。集成学习方法是通过多个机器学习模型共同做出决策的方式。
- en: 'Let''s consider the following diagram:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下图示：
- en: '![](img/85f521c4-4bb7-43af-afbb-d53dece02a4f.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](img/85f521c4-4bb7-43af-afbb-d53dece02a4f.png)'
- en: The concept of ensemble learning
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 集成学习的概念
- en: 'The random forest algorithm operates as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林算法的操作流程如下：
- en: Assume that you initially have a dataset with 100 features.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设你最初有一个包含100个特征的数据集。
- en: From this, we will build a decision tree with 10 features initially. The features
    are selected randomly.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从这里开始，我们将首先构建一个包含10个特征的决策树。这些特征是随机选择的。
- en: Now, using a random selection of the remaining 90 features, we construct the
    next decision tree, again with 10 features.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，使用剩余的90个特征中的随机选择，我们构建下一个决策树，同样使用10个特征。
- en: This process continues until there are no more features left to build a decision
    tree with.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会继续，直到没有更多特征可以用来构建决策树。
- en: At this point in time, we have 10 decision trees, each with 10 features.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经有了10棵决策树，每棵树有10个特征。
- en: Each decision tree is known as the **base estimator** of the random forest.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每棵决策树被称为随机森林的**基础估计器**。
- en: Thus, we have a forest of trees, each built using a random set of 10 features.
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 因此，我们有一片树林，每棵树都是使用一组随机选择的10个特征构建的。
- en: 'The next step for the algorithm is to make the prediction. In order to better
    understand how the random forest algorithm makes predictions, consider the following
    diagram:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的下一步是做出预测。为了更好地理解随机森林算法如何进行预测，考虑以下图示：
- en: '![](img/33763702-7826-4c87-a6b2-f1d7a923426d.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](img/33763702-7826-4c87-a6b2-f1d7a923426d.png)'
- en: The process of making predictions in random forests
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中进行预测的过程
- en: 'In the preceding diagram, the following occurs:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图示中，发生了以下情况：
- en: Let's assume that there are 10 decision trees in the random forest.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设随机森林中有10棵决策树。
- en: Each decision tree makes a single prediction for the data that comes in.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每棵决策树对进入的数据做出单一的预测。
- en: If six trees predict class A, and four trees predict class B, then the final
    prediction of the random forest algorithm is class A, as it had the majority vote.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果六棵树预测类别A，四棵树预测类别B，那么随机森林算法的最终预测结果是类别A，因为它获得了多数票。
- en: This process of voting on a prediction, based on the outputs of multiple models,
    is known as ensemble learning.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于多个模型的输出进行投票预测的过程被称为集成学习。
- en: Now that you have learned how the algorithm works internally, we can implement
    it using scikit-learn!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了算法的内部工作原理，我们可以使用scikit-learn来实现它！
- en: Implementing the random forest classifier in scikit-learn
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现随机森林分类器
- en: 'In this section, we will implement the random forest classifier in scikit-learn.
    The first step is to read in the data, and split it into training and test sets.
    This can be done by using the following code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将在scikit-learn中实现随机森林分类器。第一步是读取数据，并将其拆分为训练集和测试集。我们可以使用以下代码来完成：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The next step is to build the random forest classifier. We can do that using
    the following code:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建随机森林分类器。我们可以使用以下代码来实现：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In the preceding code block, we do the following:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，我们执行以下操作：
- en: We first import `RandomForestClassifier` from scikit-learn.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从scikit-learn导入`RandomForestClassifier`。
- en: Next, we initialize a random forest classifier model.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个随机森林分类器模型。
- en: We then fit this model to our training data, and evaluate its accuracy on the
    test data.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将该模型拟合到训练数据，并在测试数据上评估其准确性。
- en: Hyperparameter tuning for random forest algorithms
  id: totrans-151
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林算法的超参数调优
- en: 'In this section, we will learn how to optimize the hyperparameters of the random
    forest algorithm. Since random forests are fundamentally based on multiple decision
    trees, the hyperparameters are very similar. In order to optimize the hyperparameters,
    we use the following code:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，我们将学习如何优化随机森林算法的超参数。由于随机森林本质上是基于多个决策树，因此它的超参数与决策树非常相似。为了优化超参数，我们使用以下代码：
- en: '[PRE14]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the preceding code block, we do the following:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码块中，我们执行以下操作：
- en: We first import the `GridSearchCV` package.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入`GridSearchCV`包。
- en: We initialize a dictionary of hyperparameter values. The `max_depth` and `min_samples_leaf`
    values are similar to those of the decision tree.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化一个包含超参数值的字典。`max_depth`和`min_samples_leaf`的值类似于决策树的超参数。
- en: However, `n_estimators` is a new parameter, covering the total number of trees
    that you want your random forest algorithm to consider while making the final
    prediction.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，`n_estimators`是一个新参数，表示在做最终预测时，随机森林算法考虑的树的总数。
- en: We then build and fit the `gridsearch` object to the training data and extract
    the optimal parameters.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们构建并拟合`gridsearch`对象到训练数据，并提取出最佳参数。
- en: The best model is then extracted using these optimal hyperparameters.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，使用这些最佳超参数提取出最优模型。
- en: The AdaBoost classifier
  id: totrans-160
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost分类器
- en: In the section, you will learn how the AdaBoost classifier works internally,
    and how the concept of boosting might be used to give you better results. Boosting
    is a form of ensemble machine learning, in which a machine learning model learns
    from the mistakes of the models that were previously built, thereby increasing
    its final prediction accuracy.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分中，你将了解AdaBoost分类器的内部工作原理，以及提升（boosting）概念如何帮助你获得更好的结果。提升是一种集成机器学习方法，其中一个机器学习模型通过学习之前构建的模型的错误，从而提高最终预测的准确性。
- en: AdaBoost stands for Adaptive Boosting, and is a boosting algorithm in which
    a lot of importance is given to the rows of data that the initial predictive model
    got wrong. This ensures that the next predictive model will not make the same
    mistakes.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost代表自适应提升，它是一种提升算法，重点关注初始预测模型出错的数据行。这样，下一模型就不会犯同样的错误。
- en: 'The process by which the AdaBoost algorithm works is illustrated in the following
    diagram:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost算法工作的过程在下图中进行了说明：
- en: '![](img/e0a5e241-8f39-4f5c-a0df-b6a79aab1801.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e0a5e241-8f39-4f5c-a0df-b6a79aab1801.png)'
- en: An outline of the AdaBoost algorithm
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: AdaBoost算法概述
- en: 'In the preceding diagram of the AdaBoost algorithm, the following happens:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述AdaBoost算法的示意图中，发生了以下情况：
- en: The first decision tree is built and outputs a set of predictions.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个决策树被构建，并输出一组预测结果。
- en: The predictions that the first decision tree got wrong are given a weight of
    `w`. This means that, if the weight is set to 2, then two instances of that particular
    sample are introduced into the dataset.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个决策树预测错误的样本会被赋予一个权重`w`。这意味着，如果权重设置为2，那么该样本的两个实例会被加入数据集中。
- en: This enables decision tree 2 to learn at a faster rate, since we have more samples
    of the data in which an error was made beforehand.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这使得决策树2能够以更快的速度学习，因为我们有更多的样本数据，这些样本在之前的预测中出现了错误。
- en: This process is repeated until all the trees are built.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个过程会一直重复，直到所有树都构建完成。
- en: Finally, the predictions of all the trees are gathered, and a weighted vote
    is initiated in order to determine the final prediction.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，收集所有树的预测结果，并启动加权投票以确定最终预测。
- en: Implementing the AdaBoost classifier in scikit-learn
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在scikit-learn中实现AdaBoost分类器
- en: In this section, we will learn how we can implement the AdaBoost classifier
    in scikit-learn in order to predict if a transaction is fraudulent or not. As
    usual, the first step is to import the data and split it into training and testing
    sets.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 scikit-learn 中实现 AdaBoost 分类器，以预测一个交易是否为欺诈交易。像往常一样，第一步是导入数据并将其拆分为训练集和测试集。
- en: 'This can be done with the following code:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下代码完成：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The next step is to build the AdaBoost classifier. We can do this using the
    following code:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建 AdaBoost 分类器。我们可以使用以下代码实现：
- en: '[PRE16]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In the preceding code block, we do the following:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码块中，我们执行了以下操作：
- en: We first import the `AdaBoostClassifier` package from scikit-learn.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从 scikit-learn 导入 `AdaBoostClassifier` 包。
- en: Next, we initialize a decision tree that forms the base of our AdaBoost classifier.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们初始化一个决策树，作为我们 AdaBoost 分类器的基础。
- en: We then build the AdaBoost classifier, with the base estimator as the decision
    tree, and we specify that we want 100 decision trees in total.
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们构建 AdaBoost 分类器，基学习器为决策树，并指定总共需要 100 棵决策树。
- en: Finally, we fit the classifier to the training data, and extract the accuracy
    scores from the test data.
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们将分类器拟合到训练数据中，并从测试数据中提取准确率评分。
- en: Hyperparameter tuning for the AdaBoost classifier
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AdaBoost 分类器的超参数调优
- en: In this section, we will learn how to tune the hyperparameters of the AdaBoost
    classifier. The AdaBoost classifier has only one parameter of interest—the number
    of base estimators, or decision trees.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何调整 AdaBoost 分类器的超参数。AdaBoost 分类器只有一个需要关注的参数——基学习器的数量，或决策树的数量。
- en: 'We can optimize the hyperparameters of the AdaBoost classifier using the following
    code:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码优化 AdaBoost 分类器的超参数：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'In the preceding code, we do the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们执行了以下操作：
- en: We first import the `GridSearchCV` package.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入 `GridSearchCV` 包。
- en: We initialize a dictionary of hyperparameter values. In this case, `n_estimators`
    is the number of decision trees.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们初始化了一个超参数值字典。在这种情况下，`n_estimators` 是决策树的数量。
- en: We then build and fit the `gridsearch` object to the training data and extract
    the best parameters.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们构建并拟合 `gridsearch` 对象到训练数据，并提取最佳参数。
- en: The best model is then extracted using these optimal hyperparameters.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后使用这些最优超参数提取最佳模型。
- en: Regression trees
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树
- en: 'You have learned how trees are used in order to classify a prediction as belonging
    to a particular class or category. However, trees can also be used to solve problems
    related to predicting numeric outcomes. In this section, you will learn about
    the three types of tree based algorithms that you can implement in scikit-learn
    in order to predict numeric outcomes, instead of classes:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了如何使用决策树将预测分类为属于特定的类别或类别。然而，决策树也可以用于解决与预测数值结果相关的问题。在本节中，你将学习三种基于树的算法，这些算法可以在
    scikit-learn 中实现，以预测数值结果，而不是类别：
- en: The decision tree regressor
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树回归器
- en: The random forest regressor
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林回归器
- en: The gradient boosted tree
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: The decision tree regressor
  id: totrans-197
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 决策树回归器
- en: When we have data that is non-linear in nature, a linear regression model might
    not be the best model to choose. In such situations, it makes sense to choose
    a model that can fully capture the non-linearity of such data. A decision tree
    regressor can be used to predict numeric outcomes, just like that of the linear
    regression model.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有非线性数据时，线性回归模型可能不是最佳选择。在这种情况下，选择一个能够完全捕捉数据非线性的模型是有意义的。决策树回归器可以像线性回归模型一样用于预测数值结果。
- en: In the case of the decision tree regressor, we use the mean squared error, instead
    of the Gini metric, in order to determine how the tree is built. You will learn
    about the mean squared error in detail in [Chapter 8](99286f39-a802-4285-a217-547b2ff62d71.xhtml),
    *Performance Evaluation Methods*. In a nutshell, the mean squared error is used
    to tell us about the prediction error rate.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于决策树回归器，我们使用均方误差，而不是基尼指数，来决定树的构建方式。你将在[第 8 章](99286f39-a802-4285-a217-547b2ff62d71.xhtml)中详细了解均方误差，*性能评估方法*。简而言之，均方误差用于告诉我们预测误差率。
- en: 'Consider the tree shown in the following diagram:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 请考虑下图所示的树：
- en: '![](img/42ed3bf2-2be0-462f-8fdf-66e18617c5db.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](img/42ed3bf2-2be0-462f-8fdf-66e18617c5db.png)'
- en: An example decision tree for regression
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一个用于回归的决策树示例
- en: 'When considering the preceding diagram of the decision tree, note the following:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑前面的决策树图示时，请注意以下几点：
- en: We are trying to predict the amount of a mobile transaction using the tree.
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们试图预测通过树来进行的移动交易金额。
- en: When the tree tries to decide on a split, it chooses the node in such a way
    that the target value is closest to the mean values of the target in that node.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当树试图决定如何分割时，它会选择一个节点，使得该节点中的目标值最接近目标变量的均值。
- en: You will notice that, as you go down the tree to the left, along the `True`
    cases, the mean squared error of the nodes decreases.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你会注意到，当你沿着树的左侧，沿着`True`的分支向下走时，节点的均方误差会逐渐减小。
- en: Therefore, the nodes are built in a recursive fashion, such that it reduces
    the overall mean squared error, thereby obtaining the `True` value.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，节点是以递归的方式构建的，从而减少了整体的均方误差，进而获得了`True`值。
- en: In the preceding tree, if the old balance of origination is less than 600,281,
    then the amount (here, coded as `value`) is 80,442, and if it's greater than 600,281,
    then the amount is 1,988,971.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在前面的树中，如果原始余额小于 600,281，则金额（这里用 `value` 表示）为 80,442；如果大于 600,281，则金额为 1,988,971。
- en: Implementing the decision tree regressor in scikit-learn
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现决策树回归器
- en: 'In this section, you will learn how to implement the decision tree regressor
    in scikit-learn. The first step is to import the data, and create the features
    and target variables. We can do this using the following code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，你将学习如何在 scikit-learn 中实现决策树回归器。第一步是导入数据，并创建特征和目标变量。我们可以使用以下代码来完成：
- en: '[PRE18]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note how, in the case of regression, the target variable is the amount, and
    not the `isFraud` column.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在回归的情况下，目标变量是金额，而不是`isFraud`列。
- en: 'Next, we split the data into training and test sets, and build the decision
    tree regressor, as shown in the following code:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分成训练集和测试集，并构建决策树回归器，如以下代码所示：
- en: '[PRE19]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'In the preceding code, we do the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们做了以下操作：
- en: We first import the required packages and split the data into training and test
    sets.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先导入所需的包，并将数据分成训练集和测试集。
- en: Next, we build the decision tree regressor using the `DecisionTreeRegressor()`
    function.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `DecisionTreeRegressor()` 函数构建决策树回归器。
- en: 'We specify two hyperparameter arguments: `max_depth`, which tells the algorithm
    how many branches the tree must have, and `min_sample_leaf`, which tells the tree
    about the minimum number of samples that each node must have. The latter is set
    to 20%, or 0.2 of the total data, in this case.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们指定了两个超参数：`max_depth`，它告诉算法树必须有多少个分支，和 `min_sample_leaf`，它告诉树每个节点必须包含的最小样本数。在这种情况下，后者被设置为
    20%，即总数据的 0.2。
- en: '`random_state` is set to 50 to ensure that the same tree is built every time
    we run the code.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`random_state` 设置为 50，确保每次运行代码时构建相同的树。'
- en: We then fit the tree to the training data.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们将树拟合到训练数据上。
- en: Visualizing the decision tree regressor
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 可视化决策树回归器
- en: Just as we visualized the decision tree classifier, we can also visualize the
    decision tree regressor. Instead of showing you the classes or categories to which
    the node of a tree belongs, you will now be shown the value of the target variable.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们可视化决策树分类器一样，我们也可以可视化决策树回归器。不同的是，显示的将是目标变量的值，而不是树节点所属的类别或类别。
- en: 'We can visualize the decision tree regressor by using the following code:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码来可视化决策树回归器：
- en: '[PRE20]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The code follows the exact same methodology as that of the decision tree classifier,
    and will not be discussed in detail here. This produces a decision tree regressor
    like that in the following diagram:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 代码与决策树分类器的实现方法完全相同，这里不会详细讨论。这样就得到了如下图所示的决策树回归器：
- en: '![](img/e84a5850-7db2-49e0-b8fa-b16ba817ec8d.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e84a5850-7db2-49e0-b8fa-b16ba817ec8d.png)'
- en: A visualization of the decision tree regressor
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树回归器的可视化
- en: The random forest regressor
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 随机森林回归器
- en: 'The random forest regressor takes the decision tree regressor as the base estimator,
    and makes predictions in a method similar to that of the random forest classifier,
    as illustrated by the following diagram:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林回归器以决策树回归器为基本估计器，使用类似于随机森林分类器的方法进行预测，如下图所示：
- en: '![](img/c0365b85-8717-4817-9b84-c3324b1c7c59.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c0365b85-8717-4817-9b84-c3324b1c7c59.png)'
- en: Making the final prediction in the random forest regressor
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林回归器中做最终预测
- en: The only difference between the random forest classifier and the random forest
    regressor is the fact that, in the case of the latter, the base estimator is a
    decision tree regressor.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器和随机森林回归器之间唯一的区别在于，后者的基估计器是决策树回归器。
- en: Implementing the random forest regressor in scikit-learn
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现随机森林回归器
- en: 'In this section, you will learn how you can implement the random forest regressor
    in scikit-learn. The first step is to import the data and split it into training
    and testing sets. This can be done using the following code:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何在 scikit-learn 中实现随机森林回归器。第一步是导入数据并将其划分为训练集和测试集。可以使用以下代码来完成此操作：
- en: '[PRE21]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The next step is to build the random forest regressor. We can do this using
    the following code:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建随机森林回归器。我们可以使用以下代码来完成此操作：
- en: '[PRE22]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In the preceding code, we do the following:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们做了以下操作：
- en: We first import the `RandomForestRegressor` module from scikit-learn.
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从 scikit-learn 中导入 `RandomForestRegressor` 模块。
- en: We then initialize a random forest regressor object, called `rf_reg`, with a
    maximum depth of 10 for each decision tree, and the minimum number of data and
    samples in each tree as 20% of the total data.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化一个随机森林回归器对象，命名为 `rf_reg`，为每棵决策树设置最大深度为 10，并将每棵树的数据和样本数最小值设置为总数据量的 20%。
- en: We then fit the tree to the training set.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将树拟合到训练集上。
- en: The gradient boosted tree
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 梯度提升树
- en: In this section, you will learn how the gradient boosted tree is used for regression,
    and how you can implement this using scikit-learn.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何将梯度提升树用于回归，并了解如何在 scikit-learn 中实现这一过程。
- en: 'In the AdaBoost classifier that you learned about earlier in this chapter,
    weights are added to the examples that the classifier predicted in correctly.
    In the gradient boosted tree, however, instead of weights, the residual errors
    are used as labels in each tree in order to make future predictions. This concept
    is illustrated for you in the following diagram:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面学习的 AdaBoost 分类器中，错误分类的样本会被赋予权重。而在梯度提升树中，不使用权重，而是将残差误差作为每棵树的标签，以便进行未来的预测。以下图示展示了这一概念：
- en: '![](img/0d3486ae-2764-4bc3-8d70-ac0625150960.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![](img/0d3486ae-2764-4bc3-8d70-ac0625150960.png)'
- en: 'Here is what occurs in the preceding diagram:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是前面图示中发生的过程：
- en: The first decision tree is trained with the data that you have, and the target
    variable **Y**.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一棵决策树使用您提供的数据和目标变量 **Y** 进行训练。
- en: We then compute the residual error for this tree.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们计算该树的残差误差。
- en: The residual error is given by the difference between the predicted value and
    the actual value.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 残差误差是通过预测值和实际值之间的差异来计算的。
- en: The second tree is now trained, using the residuals as the target.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二棵树现在已开始训练，使用残差作为目标。
- en: This process of building multiple trees is iterative, and continues for the
    number of base estimators that we have.
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建多个树的过程是迭代的，并且会持续进行，直到达到我们指定的基估计器数量。
- en: The final prediction is made by adding the target value predicted by the first
    tree to the product of the shrinkage and the residuals for all the other trees.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最终预测是通过将第一个树预测的目标值加上所有其他树的残差与收缩因子的乘积来完成的。
- en: The shrinkage is a factor with which we control the rate at which we want this
    gradient boosting process to take place.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 收缩因子是我们控制梯度提升过程速率的一个因子。
- en: A small value of shrinkage (learning rate) implies that the algorithm will learn
    more quickly, and therefore, must be compensated with a larger number of base
    estimators (that is, decision trees) in order to prevent overfitting.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较小的收缩因子（学习率）意味着算法会学习得更快，因此需要通过更多的基估计器（即决策树）来防止过拟合。
- en: A larger value of shrinkage (learning rate) implies that the algorithm will
    learn more slowly, and thus requires fewer trees in order to reduce the computational
    time.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 较大的收缩因子（学习率）意味着算法会学习得更慢，因此需要较少的树来减少计算时间。
- en: Implementing the gradient boosted tree in scikit-learn
  id: totrans-256
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现梯度提升树
- en: 'In this section, we will learn how we can implement the gradient boosted regressor
    in scikit-learn. The first step, as usual, is to import the dataset, define the
    features and target arrays, and split the data into training and test sets. This
    can be done using the following code:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在 scikit-learn 中实现梯度提升回归器。首先，像往常一样，我们需要导入数据集，定义特征和目标数组，并将数据划分为训练集和测试集。可以使用以下代码完成此操作：
- en: '[PRE23]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The next step is to build the gradient boosted regressor. This can be done
    using the following code:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是构建梯度提升回归器。这可以通过以下代码实现：
- en: '[PRE24]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In the preceding code, we do the following:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们做了以下操作：
- en: We first import `GradientBoostingRegressor` from scikit-learn.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从 scikit-learn 导入 `GradientBoostingRegressor`。
- en: 'We the build a gradient boosted regressor object with three main arguments:
    the maximum depth of each tree, the total number of trees, and the learning rate.'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们构建一个梯度提升回归器对象，包含三个主要参数：每棵树的最大深度、树的总数和学习率。
- en: We then fit the regressor on the training data.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将回归器拟合到训练数据上。
- en: Ensemble classifier
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 集成分类器
- en: The concept of ensemble learning was explored in this chapter, when we learned
    about random forests, AdaBoost, and gradient boosted trees. However, this concept
    can be extended to classifiers outside of trees.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 本章探讨了集成学习的概念，介绍了随机森林、AdaBoost 和梯度提升树。然而，这一概念可以扩展到树以外的分类器。
- en: If we had built a logistic regression, random forest, and k-nearest neighbors
    classifiers, and we wanted to group them all together and extract the final prediction
    through majority voting, then we could do this by using the ensemble classifier.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们建立了逻辑回归、随机森林和 k-近邻分类器，并希望将它们组合在一起，通过多数投票提取最终预测结果，那么我们可以使用集成分类器来实现。
- en: 'This concept can be better understood with the aid of the following diagram:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下图表可以更好地理解这个概念：
- en: '![](img/15463931-a9d9-4b97-aec4-75fa2bfc4f6f.png)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![](img/15463931-a9d9-4b97-aec4-75fa2bfc4f6f.png)'
- en: Ensemble learning with a voting classifier to predict fraud transactions
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 使用投票分类器进行集成学习来预测欺诈交易
- en: 'When examining the preceding diagram, note the following:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看前面的图表时，请注意以下几点：
- en: The random forest classifier predicted that a particular transaction was fraudulent,
    while the other two classifiers predicted that the transaction was not fraudulent.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林分类器预测某一交易为欺诈交易，而其他两个分类器预测该交易不是欺诈交易。
- en: The voting classifier sees that two out of three (that is, a majority) of the
    predictions are **Not Fraud**, and hence, outputs the final prediction as **Not
    Fraud**.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票分类器看到三者中有两项预测为**非欺诈**，因此，输出最终预测为**非欺诈**。
- en: Implementing the voting classifier in scikit-learn
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中实现投票分类器
- en: 'In this section, you will learn how to implement the voting classifier in scikit-learn.
    The first step is to import the data, create the feature and target arrays, and
    create the training and testing splits. This can be done using the following code:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将学习如何在 scikit-learn 中实现投票分类器。第一步是导入数据，创建特征和目标数组，并创建训练集和测试集划分。可以使用以下代码来完成：
- en: '[PRE25]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, we will build two classifiers that include the voting classifier: the
    decision tree classifier and the random forest classifier. This can be done using
    the following code:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将构建两个包括投票分类器的分类器：决策树分类器和随机森林分类器。这可以通过以下代码实现：
- en: '[PRE26]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we will build the voting classifier by using the following code:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用以下代码构建投票分类器：
- en: '[PRE27]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'In the preceding code, we do the following:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们做了以下操作：
- en: We first import the `VotingClassifier` module from scikit-learn.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先从 scikit-learn 导入 `VotingClassifier` 模块。
- en: Next, we create a list of all the models that we want to use in our voting classifier.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含所有要在投票分类器中使用的模型的列表。
- en: In the list of classifiers, each model is stored in a tuple, along with the
    model's name in a string and the model itself.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在分类器列表中，每个模型都以元组的形式存储，其中包括模型的名称（字符串）和模型本身。
- en: We then initialize a voting classifier with the list of models built in step
    2.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们初始化一个投票分类器，使用第二步中构建的模型列表。
- en: Finally, the model is fitted to the training data and the accuracy is extracted
    from the test data.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将模型拟合到训练数据上，并从测试数据中提取准确度。
- en: Summary
  id: totrans-287
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'While this chapter was rather long, you have entered the world of tree based
    algorithms, and left with a wide arsenal of tools that you can implement in order
    to solve both small- and large-scale problems. To summarize, you have learned
    the following:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章较长，但您已经进入了基于树的算法世界，并带着一套可以解决小规模和大规模问题的工具离开。总结一下，您已经学习了以下内容：
- en: How to use decision trees for classification and regression
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用决策树进行分类和回归
- en: How to use random forests for classification and regression
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用随机森林进行分类和回归
- en: How to use AdaBoost for classification
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用 AdaBoost 进行分类
- en: How to use gradient boosted trees for regression
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何使用梯度提升树进行回归
- en: How the voting classifier can be used to build a single model out of different
    models
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投票分类器如何用于将不同的模型组合成一个单一模型
- en: In the upcoming chapter, you will learn how you can work with data that does
    not have a target variable or labels, and how to perform unsupervised machine
    learning in order to solve such problems!
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，你将学习如何处理没有目标变量或标签的数据，以及如何进行无监督机器学习来解决此类问题！
