- en: '*Chapter 1*: Examining the Distribution of Features and Targets'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第一章*：检查特征和目标的分布'
- en: Machine learning writing and instruction are often algorithm-focused. Sometimes,
    this gives the impression that all we have to do is choose the right model and
    that organization-changing insights will follow. But the best place to begin a
    machine learning project is with an understanding of how the features and targets
    we will use are distributed.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习写作和指导通常是算法导向的。有时，这给人一种印象，我们只需要选择正确的模型，组织变革的见解就会随之而来。但开始机器学习项目的最佳地方是对我们将使用的特征和目标分布的理解。
- en: It is important to make room for the same kind of learning from data that has
    been central to our work as analysts for decades – studying the distribution of
    variables, identifying anomalies, and examining bivariate relationships – even
    as we focus more and more on the accuracy of our predictions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们作为分析师几十年来一直重视的数据学习，留出空间是非常重要的——研究变量的分布、识别异常值以及检查双变量关系——即使我们越来越关注我们预测的准确性。
- en: We will explore tools for doing so in the first three chapters of this book,
    while also considering implications for model building.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书的前三章中探索用于此目的的工具，同时考虑对模型构建的影响。
- en: In this chapter, we will use common NumPy and pandas techniques to get a better
    sense of the attributes of our data. We want to know how key features are distributed
    before we do any predictive analyses. We also want to know the central tendency,
    shape, and spread of the distribution of each continuous feature and have a count
    for each value for categorical features. We will take advantage of very handy
    NumPy and pandas tools for generating summary statistics, such as the mean, min,
    and max, as well as standard deviation.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用常见的NumPy和pandas技术来更好地了解我们数据的属性。在我们进行任何预测分析之前，我们想知道关键特征的分布情况。我们还想知道每个连续特征的集中趋势、形状和分布范围，以及分类特征的每个值的计数。我们将利用非常方便的NumPy和pandas工具来生成汇总统计信息，例如均值、最小值和最大值，以及标准差。
- en: After that, we will create visualizations of key features, including histograms
    and boxplots, to give us a better sense of the distribution of each feature than
    we can get by just looking at summary statistics. We will hint at the implications
    of feature distribution for data transformation, encoding and scaling, and the
    modeling that we will be doing in subsequent chapters with the same data.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，我们将创建关键特征的可视化，包括直方图和箱线图，以帮助我们更好地了解每个特征的分布，而不仅仅是通过查看汇总统计信息。我们将暗示特征分布对数据转换、编码和缩放以及我们在后续章节中用相同数据进行建模的影响。
- en: 'Specifically, in this chapter, we are going to cover the following topics:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，在本章中，我们将涵盖以下主题：
- en: Subsetting data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据子集
- en: Generating frequencies for categorical features
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为分类特征生成频率
- en: Generating summary statistics for continuous features
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为连续特征生成汇总统计信息
- en: Identifying extreme values and outliers in univariate analysis
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单变量分析中识别极端值和异常值
- en: Using histograms, boxplots, and violin plots to examine the distribution of
    continuous features
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用直方图、箱线图和小提琴图来检查连续特征的分布
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter will rely heavily on the pandas, NumPy, and Matplotlib libraries,
    but you don't require any prior knowledge of these. If you have installed Python
    from a scientific distribution, such as Anaconda or WinPython, then these libraries
    are probably already installed. If you need to install one of them to run the
    code in this chapter, you can run `pip install [package name]` from a terminal.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将大量依赖pandas、NumPy和Matplotlib库，但你不需要对这些库有任何先前的知识。如果你从科学发行版，如Anaconda或WinPython安装了Python，那么这些库可能已经安装好了。如果你需要安装其中之一来运行本章中的代码，你可以在终端中运行`pip
    install [package name]`。
- en: Subsetting data
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据子集
- en: Almost every statistical modeling project I have worked on has required removing
    some data from the analysis. Often, this is because of missing values or outliers.
    Sometimes, there are theoretical reasons for limiting our analysis to a subset
    of the data. For example, we have weather data going back to 1600, but our analysis
    goals only involve changes in weather since 1900\. Fortunately, the subsetting
    tools in pandas are quite powerful and flexible. We will work with data from the
    United States **National Longitudinal Survey** (**NLS**) of Youth in this section.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎我参与的每一个统计建模项目都需要从分析中移除一些数据。这通常是因为缺失值或异常值。有时，我们限制分析数据集的理论原因。例如，我们拥有从1600年以来的天气数据，但我们的分析目标仅涉及1900年以来的天气变化。幸运的是，pandas中的子集工具非常强大且灵活。在本节中，我们将使用美国**国家青年纵向调查**（**NLS**）的数据。
- en: Note
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The NLS of Youth is conducted by the United States Bureau of Labor Statistics.
    This survey started with a cohort of individuals in 1997 who were born between
    1980 and 1985, with annual follow-ups each year through 2017\. For this recipe,
    I pulled 89 variables on grades, employment, income, and attitudes toward government
    from the hundreds of data items on the survey. Separate files for SPSS, Stata,
    and SAS can be downloaded from the repository. The NLS data is available for public
    use at [https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 青年NLS是由美国劳工统计局进行的。这项调查始于1997年，当时出生在1980年至1985年之间的一批个人，每年通过2017年进行年度跟踪。对于这个配方，我从调查的数百个数据项中提取了关于成绩、就业、收入和对政府态度的89个变量。可以从存储库下载SPSS、Stata和SAS的单独文件。NLS数据可在[https://www.nlsinfo.org/investigator/pages/search](https://www.nlsinfo.org/investigator/pages/search)公开使用。
- en: 'Let''s start subsetting the data using pandas:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用pandas开始子集数据：
- en: 'We will start by loading the NLS data. We also set an index:'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将首先加载NLS数据。我们还设置了一个索引：
- en: '[PRE0]'
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Let''s select a few columns from the NLS data. The following code creates a
    new DataFrame that contains some demographic and employment data. A useful feature
    of pandas is that the new DataFrame retains the index of the old DataFrame, as
    shown here:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们从NLS数据中选择几个列。以下代码创建了一个新的DataFrame，其中包含一些人口统计和就业数据。pandas的一个有用特性是，新的DataFrame保留了旧DataFrame的索引，如下所示：
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can use slicing to select rows by position. `nls97demo[1000:1004]` selects
    every row, starting from the row indicated by the integer to the left of the colon
    (`1000`, in this case) up to, but not including, the row indicated by the integer
    to the right of the colon (`1004`). The row at `1000` is the 1,001st row because
    of zero-based indexing. Each row appears as a column in the output since we have
    transposed the resulting DataFrame:'
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用切片通过位置选择行。`nls97demo[1000:1004]`选择从冒号左侧的整数（在这种情况下是`1000`）指示的行开始，直到但不包括冒号右侧的整数（在这种情况下是`1004`）指示的行。`1000`行是第1,001行，因为索引是从0开始的。由于我们将结果DataFrame进行了转置，所以每一行都作为输出中的一个列出现：
- en: '[PRE2]'
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We can also skip rows over the interval by setting a value for the step after
    the second colon. The default value for the step is 1\. The value for the following
    step is 2, which means that every other row between `1000` and `1004` will be
    selected:'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以通过在第二个冒号之后设置步长值来跳过区间内的行。步长的默认值是1。以下步长的值是2，这意味着在`1000`和`1004`之间的每隔一行将被选中：
- en: '[PRE3]'
  id: totrans-26
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If we do not include a value to the left of the colon, row selection will start
    with the first row. Notice that this returns the same DataFrame as the `head`
    method does:'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在冒号左侧不包含值，行选择将从第一行开始。请注意，这返回的DataFrame与`head`方法返回的相同：
- en: '[PRE4]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'If we use a negative number, -n, to the left of the colon, the last n rows
    of the DataFrame will be returned. This returns the same DataFrame as the `tail`
    method does:'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们在冒号左侧使用负数-n，将返回DataFrame的最后n行。这返回的DataFrame与`tail`方法返回的相同：
- en: '[PRE5]'
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can select rows by index value using the `loc` accessor. Recall that for
    the `nls97demo` DataFrame, the index is `personid`. We can pass a list of the
    index labels to the `loc` accessor, such as `loc[[195884,195891,195970]]`, to
    get the rows associated with those labels. We can also pass a lower and upper
    bound of index labels, such as `loc[195884:195970]`, to retrieve the indicated
    rows:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`loc`访问器通过索引值选择行。回想一下，对于`nls97demo` DataFrame，索引是`personid`。我们可以向`loc`访问器传递一个索引标签列表，例如`loc[[195884,195891,195970]]`，以获取与这些标签相关的行。我们还可以传递索引标签的下限和上限，例如`loc[195884:195970]`，以检索指定的行：
- en: '[PRE6]'
  id: totrans-32
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To select rows by position, rather than by index label, we can use the `iloc`
    accessor. We can pass a list of position numbers, such as `iloc[[0,1,2]]`, to
    the accessor to get the rows at those positions. We can pass a range, such as
    `iloc[0:3]`, to get rows between the lower and upper bound, not including the
    row at the upper bound. We can also use the `iloc` accessor to select the last
    n rows. `iloc[-3:]` selects the last three rows:'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要按位置选择行，而不是按索引标签，我们可以使用`iloc`访问器。我们可以传递一个位置数字列表，例如`iloc[[0,1,2]]`，到访问器以获取那些位置的行。我们可以传递一个范围，例如`iloc[0:3]`，以获取介于下限和上限之间的行，不包括上限所在的行。我们还可以使用`iloc`访问器来选择最后n行。`iloc[-3:]`选择最后三行：
- en: '[PRE7]'
  id: totrans-34
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Often, we need to select rows based on a column value or the values of several
    columns. We can do this in pandas by using Boolean indexing. Here, we pass a vector
    of Boolean values (which can be a Series) to the `loc` accessor or the bracket
    operator. The Boolean vector needs to have the same index as the DataFrame.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要根据列值或几个列的值来选择行。在pandas中，我们可以通过布尔索引来完成此操作。这里，我们向`loc`访问器或方括号运算符传递布尔值向量（可以是序列）。布尔向量需要与DataFrame的索引相同。
- en: 'Let''s try this using the `nightlyhrssleep` column on the NLS DataFrame. We
    want a Boolean Series that is `True` for people who sleep 6 or fewer hours a night
    (the 33rd percentile) and `False` if `nightlyhrssleep` is greater than 6 or is
    missing. `sleepcheckbool = nls97.nightlyhrssleep<=lowsleepthreshold` creates the
    boolean Series. If we display the first few values of `sleepcheckbool`, we will
    see that we are getting the expected values. We can also confirm that the `sleepcheckbool`
    index is equal to the `nls97` index:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们尝试使用NLS DataFrame上的`nightlyhrssleep`列。我们想要一个布尔序列，对于每晚睡眠6小时或更少（第33个百分位数）的人为`True`，如果`nightlyhrssleep`大于6或缺失则为`False`。`sleepcheckbool
    = nls97.nightlyhrssleep<=lowsleepthreshold`创建布尔序列。如果我们显示`sleepcheckbool`的前几个值，我们将看到我们得到了预期的值。我们还可以确认`sleepcheckbool`的索引等于`nls97`的索引：
- en: '[PRE8]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Since the `sleepcheckbool` Series has the same index as `nls97`, we can just
    pass it to the `loc` accessor to create a DataFrame containing people who sleep
    6 hours or less a night. This is a little pandas magic here. It handles the index
    alignment for us:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`sleepcheckbool`序列与`nls97`的索引相同，我们可以直接将其传递给`loc`访问器以创建一个包含每晚睡眠6小时或更少的人的DataFrame。这里有一些pandas的魔法。它为我们处理索引对齐：
- en: '[PRE9]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We could have created the `lowsleep` subset of our data in one step, which
    is what we would typically do unless we need the Boolean Series for some other
    purpose:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们本可以在一步中创建数据的`lowsleep`子集，这是我们通常会做的，除非我们需要布尔序列用于其他目的：
- en: '[PRE10]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can pass more complex conditions to the `loc` accessor and evaluate the
    values of multiple columns. For example, we can select rows where `nightlyhrssleep`
    is less than or equal to the threshold and `childathome` (number of children living
    at home) is greater than or equal to `3`:'
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以向`loc`访问器传递更复杂的条件并评估多个列的值。例如，我们可以选择`nightlyhrssleep`小于或等于阈值且`childathome`（在家居住的儿童数量）大于或等于`3`的行：
- en: '[PRE11]'
  id: totrans-43
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Each condition in `nls97.loc[(nls97.nightlyhrssleep<=lowsleepthreshold) & (nls97.childathome>3)]`
    is placed in parentheses. An error will be generated if the parentheses are excluded.
    The `&` operator is the equivalent of `and` in standard Python, meaning that *both*
    conditions have to be `True` for the row to be selected. We could have used `|`
    for `or` if we wanted to select the row if *either* condition was `True`.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`nls97.loc[(nls97.nightlyhrssleep<=lowsleepthreshold) & (nls97.childathome>3)]`中的每个条件都放在括号内。如果省略括号，将生成错误。`&`运算符相当于标准Python中的`and`，意味着必须`两个`条件都为`True`，行才能被选中。如果我们想选择如果任一条件为`True`的行，我们可以使用`|`表示`or`。'
- en: 'Finally, we can select rows and columns at the same time. The expression to
    the left of the comma selects rows, while the list to the right of the comma selects
    columns:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们可以同时选择行和列。逗号左边的表达式选择行，而逗号右边的列表选择列：
- en: '[PRE12]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We used three different tools to select columns and rows from a pandas DataFrame
    in the last two sections: the `[]` bracket operator and two pandas-specific accessors,
    `loc` and `iloc`. This will be a little confusing if you are new to pandas, but
    it becomes clear which tool to use in which situation after just a few months.
    If you came to pandas with a fair bit of Python and NumPy experience, you will
    likely find the `[]` operator most familiar. However, the pandas documentation
    recommends against using the `[]` operator for production code. The `loc` accessor
    is used for selecting rows by Boolean indexing or by index label, while the `iloc`
    accessor is used for selecting rows by row number.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一两节中，我们使用了三种不同的工具从pandas DataFrame中选择列和行：`[]`括号操作符和两个pandas特定访问器`loc`和`iloc`。如果你是pandas的新手，这可能会有些令人困惑，但经过几个月后，你会清楚地知道在哪种情况下使用哪种工具。如果你带着相当多的Python和NumPy经验来到pandas，你可能会发现`[]`操作符最为熟悉。然而，pandas文档建议不要在生产代码中使用`[]`操作符。`loc`访问器用于通过布尔索引或索引标签选择行，而`iloc`访问器用于通过行号选择行。
- en: This section was a brief primer on selecting columns and rows with pandas. Although
    we did not go into too much detail on this, most of what you need to know to subset
    data was covered, as well as everything you need to know to understand the pandas-specific
    material in the rest of this book. We will start putting some of that to work
    in the next two sections by creating frequencies and summary statistics for our
    features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分是关于使用pandas选择列和行的简要入门。尽管我们没有对此进行过多详细说明，但涵盖了你需要了解的大部分内容，包括了解本书其余部分中pandas特定材料所需的一切。我们将在下一两节中开始应用这些知识，通过为我们的特征创建频率和汇总统计。
- en: Generating frequencies for categorical features
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成类别特征的频率
- en: Categorical features can be either nominal or ordinal. **Nominal** features,
    such as gender, species name, or country, have a limited number of possible values,
    and are either strings or are numerical without having any intrinsic numerical
    meaning. For example, if country is represented by 1 for Afghanistan, 2 for Albania,
    and so on, the data is numerical but it does not make sense to perform arithmetic
    operations on those values.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 分类别特征可以是名义的或有序的。**名义**特征，例如性别、物种名称或国家，具有有限的可能值，可以是字符串或数值，但没有内在的数值意义。例如，如果国家用1代表阿富汗，2代表阿尔巴尼亚，以此类推，数据是数值的，但对这些值进行算术运算是没有意义的。
- en: '**Ordinal** features also have a limited number of possible values but are
    different from nominal features in that the order of the values matters. A **Likert
    scale** rating (ranging from 1 for very unlikely to 5 for very likely) is an example
    of an ordinal feature. Nonetheless, arithmetic operations would not typically
    make sense because there is no uniform and meaningful distance between values.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**有序**特征也有有限的可能值，但与名义特征不同，值的顺序很重要。**李克特量表**评分（从1表示非常不可能到5表示非常可能）是一个有序特征的例子。尽管如此，通常不会进行算术运算，因为值之间没有统一和有意义的距离。'
- en: 'Before we begin modeling, we want to have counts of all the possible values
    for the categorical features we may use. This is typically referred to as a one-way
    frequency distribution. Fortunately, pandas makes this very easy to do. We can
    quickly select columns from a pandas DataFrame and use the `value_counts` method
    to generate counts for each categorical value:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始建模之前，我们希望对可能使用的类别特征的所有可能值进行计数。这通常被称为单向频率分布。幸运的是，pandas使这变得非常容易。我们可以快速从pandas
    DataFrame中选择列，并使用`value_counts`方法为每个类别值生成计数：
- en: 'Let''s load the NLS data, create a DataFrame that contains just the first 20
    columns of the data, and look at the data types:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们加载NLS数据，创建一个只包含数据前20列的DataFrame，并查看数据类型：
- en: '[PRE13]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'All of the object type columns in the preceding code are categorical. We can
    use `value_counts` to see the counts for each value for `maritalstatus`. We can
    also use `dropna=False` to get `value_counts` to show the missing values (`NaN`):'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 上一段代码中的所有对象类型列都是类别特征。我们可以使用`value_counts`来查看`maritalstatus`每个值的计数。我们还可以使用`dropna=False`来让`value_counts`显示缺失值（`NaN`）：
- en: '[PRE14]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'If we just want the number of missing values, we can chain the `isnull` and
    `sum` methods. `isnull` returns a Boolean Series containing `True` values when
    `maritalstatus` is missing and `False` otherwise. `sum` then counts the number
    of `True` values, since it will interpret `True` values as 1 and `False` values
    as 0:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果我们只想得到缺失值的数量，我们可以链式调用`isnull`和`sum`方法。`isnull`在`maritalstatus`缺失时返回包含`True`值的布尔序列，否则返回`False`。然后`sum`计算`True`值的数量，因为它将`True`值解释为1，将`False`值解释为0：
- en: '[PRE15]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'You have probably noticed that the `maritalstatus` values were sorted by frequency
    by default. You can sort them alphabetically by values by sorting the index. We
    can do this by taking advantage of the fact that `value_counts` returns a Series
    with the values as the index:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可能已经注意到，`maritalstatus`的值默认按频率排序。你可以通过排序索引按值进行字母排序。我们可以利用`value_counts`返回一个以值为索引的序列这一事实来做到这一点：
- en: '[PRE16]'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To sort the index, we just need to call `sort_index`:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要对索引进行排序，我们只需调用`sort_index`：
- en: '[PRE17]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Of course, we could have gotten the same results in one step with `nls97.maritalstatus.value_counts(dropna=False).sort_index()`.
    We can also show ratios instead of counts by setting `normalize` to `True`. In
    the following code, we can see that 34% of the responses were `Married` (notice
    that we did not set `dropna` to `True`, so missing values have been excluded):'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，我们也可以通过`nls97.maritalstatus.value_counts(dropna=False).sort_index()`一步得到相同的结果。我们还可以通过将`normalize`设置为`True`来显示比率而不是计数。在下面的代码中，我们可以看到34%的回应是`Married`（注意我们没有将`dropna`设置为`True`，所以缺失值已被排除）：
- en: '[PRE18]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'pandas has a category data type that can store data much more efficiently than
    the object data type when a column has a limited number of values. Since we already
    know that all of our object columns contain categorical data, we should convert
    those columns into the category data type. In the following code, we''re creating
    a list that contains the column names for the object columns, `catcols`. Then,
    we''re looping through those columns and using `astype` to change the data type
    to `category`:'
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当一列有有限数量的值时，pandas的类别数据类型可以比对象数据类型更有效地存储数据。由于我们已经知道所有对象列都包含类别数据，我们应该将这些列转换为类别数据类型。在下面的代码中，我们创建了一个包含对象列名称的列表，`catcols`。然后，我们遍历这些列，并使用`astype`将数据类型更改为`category`：
- en: '[PRE19]'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let''s check our category features for missing values. There are no missing
    values for `gender` and very few for `highestdegree`. But the overwhelming majority
    of values for `govprovidejobs` (the government should provide jobs) and `govpricecontrols`
    (the government should control prices) are missing. This means that those features
    probably won''t be useful for most modeling:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们检查我们的类别特征中的缺失值。`gender`没有缺失值，`highestdegree`的缺失值非常少。但`govprovidejobs`（政府应该提供工作）和`govpricecontrols`（政府应该控制价格）的绝大多数值都是缺失的。这意味着这些特征可能对大多数建模没有用：
- en: '[PRE20]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'We can generate frequencies for multiple features at once by passing a `value_counts`
    call to `apply`. We can use `filter` to select the columns that we want – in this
    case, all the columns with *gov* in their name. Note that the missing values for
    each feature have been omitted since we did not set `dropna` to `False`:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过将`value_counts`调用传递给`apply`来一次生成多个特征的频率。我们可以使用`filter`来选择我们想要的列——在这种情况下，所有名称中包含`*gov*`的列。注意，由于我们没有将`dropna`设置为`False`，因此已经省略了每个特征的缺失值：
- en: '[PRE21]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can use the same frequencies on a subset of our data. If, for example, we
    want to see the responses of only married people to the government role questions,
    we can do that subsetting by placing `nls97abb[nls97abb.maritalstatus=="Married"]`
    before `filter`:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以在数据的一个子集上使用相同的频率。例如，如果我们只想查看已婚人士对政府角色问题的回答，我们可以通过在`filter`之前放置`nls97abb[nls97abb.maritalstatus=="Married"]`来执行这个子集操作：
- en: '[PRE22]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Since, in this case, there were only two *gov* columns, it may have been easier
    to do the following:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这种情况下，由于只有两个`*gov*`列，可能更容易执行以下操作：
- en: '[PRE23]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Nonetheless, it will often be easier to use `filter` since it is not unusual
    to have to do the same cleaning or exploration task on groups of features with
    similar names.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，使用`filter`通常会更简单，因为在具有相似名称的特征组上执行相同的清理或探索任务并不罕见。
- en: There are times when we may want to model a continuous or discrete feature as
    categorical. The NLS DataFrame contains `highestgradecompleted`. A year increase
    from 5 to 6 may not be as important as that from 11 to 12 in terms of its impact
    on a target. Let's create a dichotomous feature instead – that is, 1 when the
    person has completed 12 or more grades, 0 if they have completed less than that,
    and missing when `highestgradecompleted` is missing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，我们可能希望将连续或离散特征建模为分类特征。NLS DataFrame 包含 `highestgradecompleted`。从 5 年级到 6
    年级的增长可能不如从 11 年级到 12 年级的增长对目标的影响重要。让我们创建一个二进制特征——即当一个人完成了 12 年级或以上时为 1，如果他们完成的少于这个数则为
    0，如果 `highestgradecompleted` 缺失则为缺失。
- en: 'We need to do a little bit of cleaning up first, though. `highestgradecompleted`
    has two logical missing values – an actual NaN value that pandas recognizes as
    missing and a 95 value that the survey designers intend for us to also treat as
    missing for most use cases. Let''s use `replace` to fix that before moving on:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 不过，我们首先需要做一些清理工作。`highestgradecompleted` 有两个逻辑缺失值——一个 pandas 识别为缺失的实际 NaN 值和一个调查设计者意图让我们在大多数情况下也视为缺失的
    95 值。让我们在继续之前使用 `replace` 来修复它：
- en: '[PRE24]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can use NumPy''s `where` function to assign values to `highschoolgrad` based
    on the values of `highestgradecompleted`. If `highestgradecompleted` is null (`NaN`),
    we assign `NaN` to our new column, `highschoolgrad`. If the value for `highestgradecompleted`
    is not null, the next clause tests for a value less than 12, setting `highschoolgrad`
    to 0 if that is true, and to 1 otherwise. We can confirm that the new column,
    `highschoolgrad`, contains the values we want by using `groupby` to get the min
    and max values of `highestgradecompleted` at each level of `highschoolgrad`:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 NumPy 的 `where` 函数根据 `highestgradecompleted` 的值分配 `highschoolgrad` 的值。如果
    `highestgradecompleted` 为空（`NaN`），我们将 `NaN` 分配给我们的新列 `highschoolgrad`。如果 `highestgradecompleted`
    的值不为空，下一个子句检查值是否小于 12，如果是，则将 `highschoolgrad` 设置为 0，否则设置为 1。我们可以通过使用 `groupby`
    来获取 `highschoolgrad` 每个级别的 `highestgradecompleted` 的最小值和最大值来确认新列 `highschoolgrad`
    包含我们想要的值：
- en: '[PRE25]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: While 12 makes conceptual sense as the threshold for classifying our new feature,
    `highschoolgrad`, this would present some modeling challenges if we intended to
    use `highschoolgrad` as a target. There is a pretty substantial class imbalance,
    with `highschoolgrad` equal to 1 class being more than 4 times the size of the
    0 group. We should explore using more groups to represent `highestgradecompleted`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 12 作为将我们的新特征 `highschoolgrad` 分类为类的阈值在概念上是合理的，但如果我们打算将 `highschoolgrad` 作为目标使用，这可能会带来一些建模挑战。存在相当大的类别不平衡，`highschoolgrad`
    等于 1 的类别是 0 类的四倍多。我们应该探索使用更多组来表示 `highestgradecompleted`。
- en: 'One way to do this with pandas is with the `qcut` function. We can set the
    `q` parameter of `qcut` to `6` to create six groups that are as evenly distributed
    as possible. These groups are now closer to being balanced:'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 pandas 实现这一点的其中一种方法是使用 `qcut` 函数。我们可以将 `qcut` 的 `q` 参数设置为 `6` 以创建尽可能均匀分布的六个组。现在这些组更接近平衡：
- en: '[PRE26]'
  id: totrans-83
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Finally, I typically find it helpful to generate frequencies for all the categorical
    features and save that output so that I can refer to it later. I rerun that code
    whenever I make some change to the data that may change these frequencies. The
    following code iterates over all the columns that are of the category data type
    and runs `value_counts`:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我发现通常生成所有分类特征的频率并将其保存下来很有帮助，这样我就可以稍后参考。每当我对数据进行一些可能改变这些频率的更改时，我都会重新运行那段代码。以下代码遍历所有数据类型为分类数据的列，并运行
    `value_counts`：
- en: '[PRE27]'
  id: totrans-85
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: These are the key techniques for generating one-way frequencies for the categorical
    features in your data. The real star of the show has been the `value_counts` method.
    We can use `value_counts` to create frequencies a Series at a time, use it with
    `apply` for multiple columns, or iterate over several columns and call `value_counts`
    each time. We have looked at examples of each in this section. Next, let's explore
    some techniques for examining the distribution of continuous features.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在您的数据中生成分类特征的单一频率的关键技术。真正的明星是 `value_counts` 方法。我们可以一次创建一个 Series 的频率使用 `value_counts`，也可以使用
    `apply` 对多个列进行操作，或者遍历多个列并在每次迭代中调用 `value_counts`。我们已经在本节中查看了一些示例。接下来，让我们探讨一些检查连续特征分布的技术。
- en: Generating summary statistics for continuous and discrete features
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成连续和离散特征的摘要统计量
- en: Getting a feel for the distribution of continuous or discrete features is a
    little more complicated than it is for categorical features. A continuous feature
    can take an infinite number of values. An example of a continuous feature is weight,
    as someone can weigh 70 kilograms, or 70.1, or 70.01\. Discrete features have
    a finite number of values, such as the number of birds sighted, or the number
    of apples purchased. One way of thinking about the difference is that a discrete
    feature is typically something that has been counted, while a continuous feature
    is usually captured by measurement, weighing, or timekeeping.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 获取连续或离散特征的分布感比分类特征要复杂一些。连续特征可以取无限多个值。一个连续特征的例子是体重，因为某人可能重70公斤，或者70.1，或者70.01。离散特征具有有限个值，例如看到的鸟的数量，或者购买苹果的数量。思考它们之间差异的一种方式是，离散特征通常是已经被计数的东西，而连续特征通常是通过测量、称重或计时来捕捉的。
- en: Continuous features will generally be stored as floating-point numbers unless
    they have been constrained to be whole numbers. In that case, they may be stored
    as integers. Age for individual humans, for example, is continuous but is usually
    truncated to an integer.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 连续特征通常会被存储为浮点数，除非它们被限制为整数。在这种情况下，它们可能以整数的形式存储。例如，个人的年龄是连续的，但通常会被截断为整数。
- en: For most modeling purposes, continuous and discrete features are treated similarly.
    We would not model age as a categorical feature. We assume that the interval between
    ages has largely the same meaning between 25 and 26 as it has between 35 and 36,
    though this breaks down at the extremes. The interval between 1 and 2 years of
    age for humans is not at all like that between 71 and 72\. Data analysts and scientists
    are usually skeptical of assumed linear relationships between continuous features
    and targets, though modeling is much easier when that is true.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数建模目的，连续特征和离散特征被同等对待。我们不会将年龄建模为分类特征。我们假设年龄在25岁到26岁之间的间隔与35岁到36岁之间的间隔具有大致相同的意义，尽管在极端情况下这种假设会失效。人类1岁到2岁之间的间隔与71岁到72岁之间的间隔完全不同。数据分析师和科学家通常对连续特征和目标之间的假设线性关系持怀疑态度，尽管当这种关系成立时建模会更容易。
- en: To understand how a continuous feature (or discrete feature) is distributed,
    we must examine its central tendency, shape, and spread. Key summary statistics
    are mean and median for central tendency, skewness and kurtosis for shape, and
    range, interquartile range, variance, and standard deviation for spread. In this
    section, we will learn how to use pandas, supplemented by the **SciPy** library,
    to get these statistics. We will also discuss important implications for modeling.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解连续特征（或离散特征）的分布，我们必须检查其中心趋势、形状和分布范围。关键摘要统计量包括均值和中位数用于中心趋势，偏度和峰度用于形状，以及范围、四分位数范围、方差和标准差用于分布范围。在本节中，我们将学习如何使用pandas，辅以**SciPy**库，来获取这些统计量。我们还将讨论建模的重要影响。
- en: We will work with COVID-19 data in this section. The dataset contains one row
    per country, with total cases and deaths through June 2021, as well as demographic
    data for each country.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用COVID-19数据。数据集包含每个国家的总病例和死亡数，以及截至2021年6月的人口统计数据。
- en: Note
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: '*Our World in Data* provides COVID-19 public use data at [https://ourworldindata.org/coronavirus-source-data](https://ourworldindata.org/coronavirus-source-data).
    The data that will be used in this section was downloaded on July 9, 2021\. There
    are more columns in the data than I have included. I created the region column
    based on country.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*我们的世界数据*在[https://ourworldindata.org/coronavirus-source-data](https://ourworldindata.org/coronavirus-source-data)提供COVID-19公共使用数据。本节中使用的数据是在2021年7月9日下载的。数据中的列比我包含的要多。我根据国家创建了地区列。'
- en: 'Follow these steps to generate the summary statistics:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 按照以下步骤生成摘要统计量：
- en: 'Let''s load the COVID `.csv` file into pandas, set the index, and look at the
    data. There are 221 rows and 16 columns. The index we set, `iso_code`, contains
    a unique value for each row. We use `sample` to view two countries randomly, rather
    than the first two (we set a value for `random_state` to get the same results
    each time we run the code):'
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们将COVID `.csv`文件加载到pandas中，设置索引，并查看数据。有221行和16列。我们设置的索引`iso_code`为每一行包含一个唯一值。我们使用`sample`来随机查看两个国家，而不是前两个（我们为`random_state`设置了一个值，以便每次运行代码时都能得到相同的结果）：
- en: '[PRE28]'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Just by looking at these two rows, we can see significant differences in cases
    and deaths between Iceland and Czechia, even in terms of population size. (`total_cases_mill`
    and `total_deaths_mill` divide cases and deaths per million of population, respectively.)
    Data analysts are very used to wondering if there is anything else in the data
    that may explain substantially higher cases and deaths in Czechia than in Iceland.
    In a sense, we are always engaging in feature selection.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 只需看这两行，我们就可以看到冰岛和捷克在病例和死亡人数方面的显著差异，即使在人口规模方面也是如此。（`total_cases_mill` 和 `total_deaths_mill`
    分别表示每百万人口中的病例和死亡人数。）数据分析师非常习惯于思考数据中是否还有其他因素可以解释捷克比冰岛病例和死亡人数显著更高的原因。从某种意义上说，我们总是在进行特征选择。
- en: 'Let''s take a look at the data types and number of non-null values for each
    column. Almost all of the columns are continuous or discrete. We have data on
    cases and deaths, as well as likely targets, for 192 and 185 rows, respectively.
    An important data cleaning task we''ll have to do will be figuring out what, if
    anything, we can do about countries that have missing values for our targets.
    We''ll discuss how to handle missing values later:'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看每列的数据类型和空值数量。几乎所有列都是连续的或离散的。我们有关于病例和死亡的数据，分别有192行和185行。我们必须要做的一个重要数据清洗任务是确定我们能对那些对于我们的目标有缺失值的国家的数据做些什么。我们将在稍后讨论如何处理缺失值：
- en: '[PRE29]'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now, we are ready to examine the distribution of some of the features. We can
    get most of the summary statistics we want by using the `describe` method. The
    mean and median (50%) are good indicators of the center of the distribution, each
    with its strengths. It is also good to notice substantial differences between
    the mean and median, as an indication of skewness. For example, we can see that
    the mean cases per million is almost twice the median, with 36.7 thousand compared
    to 19.5 thousand. This is a clear indicator of positive skew. This is also true
    for deaths per million.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好检查一些特征的分部情况。我们可以通过使用 `describe` 方法来获取我们想要的绝大部分的摘要统计信息。平均值和中位数（50%）是分布中心的良好指标，各有其优势。注意到平均值和中位数之间的显著差异，作为偏斜的指示也是很好的。例如，我们可以看到每百万病例的平均值几乎是中位数的两倍，分别是36.7千和19.5千。这是一个明显的正偏斜指标。死亡人数每百万的情况也是如此。
- en: 'The interquartile range is also quite large for cases and deaths, with the
    75th percentile value being about 25 times larger than the 25th percentile value
    in both cases. We can compare that with the percentage of the population aged
    65 and older and diabetes prevalence, where the 75th percentile is just four times
    or two times that of the 25th percentile, respectively. We can tell right away
    that those two possible features (`aged_65_older` and `diabetes_prevalence`) would
    have to do a lot of work to explain the huge variance in our targets:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 病例和死亡人数的四分位距也相当大，两种情况下的第75百分位数值大约是第25百分位数值的25倍。我们可以将这一点与65岁及以上人口比例和糖尿病患病率进行比较，其中第75百分位数分别是第25百分位数的四倍或两倍。我们可以立即得出结论，这两个可能的特征（`aged_65_older`
    和 `diabetes_prevalence`）必须做大量工作来解释我们目标中的巨大方差：
- en: '[PRE30]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'I sometimes find it helpful to look at the decile values to get a better sense
    of the distribution. The `quantile` method can take a single value for quantile,
    such as `quantile(0.25)` for the 25th percentile, or a list or tuple, such as
    `quantile((0.25,0.5))` for the 25th and 50th percentiles. In the following code,
    we''re using `arange` from NumPy (`np.arange(0.0, 1.1, 0.1)`) to generate an array
    that goes from 0.0 to 1.0 with a 0.1 increment. We would get the same result if
    we were to use `covidkeys.quantile([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])`:'
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我有时发现查看十分位数值有助于更好地了解分布。`quantile` 方法可以接受一个百分位数值，例如 `quantile(0.25)` 表示第25百分位数，或者一个列表或元组，例如
    `quantile((0.25,0.5))` 表示第25和第50百分位数。在下面的代码中，我们使用 NumPy 的 `arange` 函数（`np.arange(0.0,
    1.1, 0.1)`) 生成一个从0.0到1.0，增量是0.1的数组。如果我们使用 `covidkeys.quantile([0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0])`，我们会得到相同的结果：
- en: '[PRE31]'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: For cases, deaths, and diabetes prevalence, much of the range (the distance
    between the min and max values) is in the last 10% of the distribution. This is
    particularly true for deaths. This hints at possible modeling problems and invites
    us to take a close look at outliers, something we will do in the next section.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 对于病例、死亡和糖尿病患病率，大部分的范围（最小值和最大值之间的距离）位于分布的最后10%。这一点对于死亡尤其如此。这暗示了可能存在建模问题，并邀请我们仔细查看异常值，我们将在下一节中这样做。
- en: 'Some machine learning algorithms assume that our features have normal (also
    referred to as Gaussian) distributions, that they are distributed symmetrically
    (have low skew), and that they have relatively normal tails (neither excessively
    high nor excessively low kurtosis). The statistics we have seen so far already
    suggest a high positive skew for our two likely targets – that is, total cases
    and deaths per million people in the population. Let''s put a finer point on this
    by calculating both skew and kurtosis for some of the features. For a Gaussian
    distribution, we expect a value near 0 for skew and 3 for kurtosis. `total_deaths_mill`
    has values for skew and kurtosis that are worth noting, and the `total_cases_mill`
    and `aged_65_older` features have excessively low kurtosis (skinny tails):'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一些机器学习算法假设我们的特征具有正态分布（也称为高斯分布），它们分布对称（具有低偏度），并且它们的尾部相对正常（既不过度高也不过度低的高斯峰）。我们迄今为止看到的统计数据已经表明，我们的两个可能的目标——即总人口中的总病例数和死亡人数——具有很高的正偏度。让我们通过计算一些特征的偏度和峰度来对此进行更细致的分析。对于高斯分布，我们期望偏度为0附近，峰度为3。`total_deaths_mill`的偏度和峰度值值得关注，而`total_cases_mill`和`aged_65_older`特征具有过低的峰度（瘦尾巴）：
- en: '[PRE32]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can also explicitly test each distribution''s normality by looping over
    the features in the `keyvars` list and running a `scistat.shapiro(covidkeys[var].dropna())`).
    Notice that we need to drop missing values with `dropna` for the test to run.
    p-values less than 0.05 indicate that we can reject the null hypothesis of normal,
    which is the case for each of the four features:'
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还可以通过遍历`keyvars`列表中的特征并运行`scistat.shapiro(covidkeys[var].dropna())`来显式测试每个分布的正态性。请注意，我们需要使用`dropna`删除缺失值以运行测试。p值小于0.05表明我们可以拒绝正态性的零假设，这对于四个特征中的每一个都是如此：
- en: '[PRE33]'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: These results should make us pause if we are considering parametric models such
    as linear regression. None of the distributions approximates a normal distribution.
    However, this is not determinative. It is not as simple as deciding that we should
    use certain models when we have normally distributed features and non-parametric
    models (say, k-nearest neighbors) when we do not.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果应该让我们在考虑参数模型，如线性回归时停下来思考。没有任何分布近似正态分布。然而，这并不是决定性的。这并不像在具有正态分布特征时使用某些模型（比如k-最近邻）而在不具有时使用非参数模型那么简单。
- en: We want to do additional data cleaning before we make any modeling decisions.
    For example, we may decide to remove outliers or determine that it is appropriate
    to transform the data. We will explore transformations, such as log and polynomial
    transformations, in several chapters in this book.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在做出任何建模决策之前，我们希望进行额外的数据清洗。例如，我们可能决定删除异常值或确定数据转换是合适的。我们将在本书的几个章节中探索转换，例如对数和多项式转换。
- en: This section showed you how to use pandas and SciPy to understand how continuous
    and discrete features are distributed, including their central tendency, shape,
    and spread. It makes sense to generate these statistics for any feature or target
    that might be included in our modeling. This also points us in the direction of
    more work we need to do to prepare our data for analysis. We need to identify
    missing values and outliers and figure out how we will handle them. We should
    also visualize the distribution of our continuous features. This rarely fails
    to yield additional insights. We will learn how to identify outliers in the next
    section and create visualizations in the following section.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 本节向您展示了如何使用pandas和SciPy来了解连续和离散特征的分布情况，包括它们的中心趋势、形状和分布范围。为任何可能包含在我们建模中的特征或目标生成这些统计数据是有意义的。这也指引我们进一步的工作方向，以准备我们的数据进行分析。我们需要识别缺失值和异常值，并找出我们将如何处理它们。我们还应该可视化连续特征的分布。这很少不会带来额外的见解。我们将在下一节学习如何识别异常值，并在下一节创建可视化。
- en: Identifying extreme values and outliers in univariate analysis
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在单变量分析中识别极端值和异常值
- en: An outlier can be thought of as an observation with feature values, or relationships
    between feature values, that are so unusual that they cannot help explain relationships
    in the rest of the data. This matters for modeling because we cannot assume that
    the outliers will have a neutral impact on our parameter estimates. Sometimes,
    our models work so hard to construct parameter estimates that can account for
    patterns in outlier observations that we compromise the model's explanatory or
    predictive power for all other observations. Raise your hand if you have ever
    spent days trying to interpret a model only to discover that your coefficients
    and predictions completely changed once you removed a few outliers.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值可以被视为具有特征值或特征值之间关系非常不寻常的观测值，以至于它们无法解释数据中其他关系。这对于建模很重要，因为我们不能假设异常值将对我们的参数估计产生中性影响。有时，我们的模型会努力构建参数估计，以解释异常观测值中的模式，这可能会损害模型对所有其他观测值的解释或预测能力。如果你曾经花了好几天时间尝试解释一个模型，最终发现一旦移除几个异常值，你的系数和预测就完全改变了，请举手。
- en: I should quickly add that there is no agreed-upon definition of an outlier.
    I offer the preceding definition for use in this book because it helps us distinguish
    between outliers, as I have described them, and extreme values. There is a fair
    bit of overlap between the two, but many extreme values are not outliers. This
    is because such values reflect a natural and explainable trend in a feature, or
    because they reflect the same relationship between features as is observed throughout
    the data. The reverse is also true. Some outliers are not extreme values. For
    example, a target value might be right in the middle of the distribution but have
    quite unexpected predictor values.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我应该快速补充一下，目前还没有关于异常值的统一定义。我提供前面的定义是为了在本书中使用，因为它有助于我们区分异常值，正如我描述的那样，以及极端值。这两个概念之间有相当大的重叠，但许多极端值并不是异常值。这是因为这样的值反映了特征中的自然和可解释的趋势，或者因为它们反映了与数据中观察到的特征之间的相同关系。反之亦然。有些异常值不是极端值。例如，一个目标值可能正好位于分布的中间，但具有相当意外的预测值。
- en: For our modeling, then, it is hard to say that a particular feature or target
    value is an outlier without referencing multivariate relationships. But it should
    at least raise a red flag when, in our univariate analysis, we see values well
    to the left or right of the center. This should prompt us to investigate the observation
    at that value further, including examining the values of other features. We will
    look at multivariate relationships in more detail in the next two chapters. Here,
    and in the next section on visualizations, we will focus on identifying extreme
    values and outliers when looking at one variable.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们的建模来说，很难在没有参考多元关系的情况下说一个特定的特征或目标值是异常值。但是，当我们在单变量分析中看到位于中心左侧或右侧的值时，这至少应该引起我们的注意。这应该促使我们进一步调查该值，包括检查其他特征值。我们将在下一章中更详细地探讨多元关系。在这里，以及在下节关于可视化的内容中，我们将专注于在查看单个变量时识别极端值和异常值。
- en: A good starting point for identifying an extreme value is to look at its distance
    from the middle of the distribution. One common method for doing that is to calculate
    each value's distance from the **interquartile range** (**IQR**), which is the
    distance between the first quartile value and the third quartile value. We often
    flag any value that is more than 1.5 times the interquartile range above the third
    quartile or below the first quartile. We can use this method to identify outliers
    in the COVID-19 data.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 识别极端值的一个良好起点是查看其与分布中间的距离。实现这一目标的一种常见方法是将每个值与**四分位数范围**（**IQR**）的距离进行计算，即第一个四分位数值与第三个四分位数值之间的距离。我们通常将任何高于第三四分位数1.5倍或低于第一四分位数的值标记出来。我们可以使用这种方法来识别
    COVID-19 数据中的异常值。
- en: 'Let''s get started:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'Let''s start by importing the libraries we will need. In addition to pandas
    and NumPy, we will use Matplotlib and statsmodels for the plots we will create.
    We will also load the COVID data and select the variables we need:'
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们导入我们将需要的库。除了 pandas 和 NumPy，我们还将使用 Matplotlib 和 statsmodels 来创建图表。我们还将加载
    COVID 数据并选择所需的变量：
- en: '[PRE34]'
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Let''s take a look at `total_cases_mill`. We get the first and third quartile
    values and calculate the interquartile range, `1.5*(thirdq-firstq)`. Then, we
    calculate their thresholds to determine the high and low extreme values, which
    are `interquartilerange+thirdq` and `firstq-interquartilerange`, respectively
    (if you are familiar with boxplots, you will notice that this is the same calculation
    that''s used for the whiskers of a boxplot; we will cover boxplots in the next
    section):'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们来看看`total_cases_mill`。我们得到第一和第三四分位数，并计算四分位数范围，`1.5*(thirdq-firstq)`。然后，我们计算它们的阈值以确定高和低极端值，分别是`interquartilerange+thirdq`和`firstq-interquartilerange`（如果你熟悉箱线图，你会注意到这是用于箱线图须的相同计算；我们将在下一节中介绍箱线图）：
- en: '[PRE35]'
  id: totrans-123
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This calculation indicates that any value for `total_cases_mill` that''s above
    158,337 can be considered extreme. We can ignore extreme values on the low end
    because they would be negative:'
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这个计算表明，任何`total_cases_mill`的值，如果高于158,337，都可以被认为是极端的。我们可以忽略低端的极端值，因为它们将是负数：
- en: '[PRE36]'
  id: totrans-125
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Andorra, Montenegro, and Seychelles all have `total_cases_mill` above the threshold
    amount. This invites us to explore other ways these countries might be exceptional,
    and whether our features can capture that. We will not dive deeply into multivariate
    analysis here since we will do that in the next chapter, but it is a good idea
    to start wrapping our brains around why these extreme values may or may not make
    sense. Having some means across the full dataset may help us here:'
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 安道尔、黑山和塞舌尔的所有`total_cases_mill`都超过了阈值。这促使我们探索这些国家可能异常的其他方式，以及我们的特征是否能够捕捉到这一点。我们不会在这里深入多元分析，因为我们将那部分内容放在下一章中，但开始思考为什么这些极端值可能或可能没有意义是个好主意。在整个数据集上有一个平均值可能有助于我们：
- en: '[PRE37]'
  id: totrans-127
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The main difference between these three countries and others is that they have
    very low populations. Surprisingly, each has a much lower population density than
    average. That is the opposite of what you would expect and merits further consideration
    in the analysis we will do throughout this book.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个国家与其他国家的主要区别在于它们的人口非常少。令人惊讶的是，每个国家的人口密度都比平均水平低得多。这与你预期的相反，值得我们在这本书的整个分析中进一步考虑。
- en: An Alternative to the IQR Calculation
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: IQR计算的替代方法
- en: An alternative to using the interquartile range to identify an extreme value
    would be to use several standard deviations away from the mean, say 3\. One drawback
    of this method is that it is a little more susceptible to extreme values than
    using the interquartile range.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用四分位数范围来识别极端值的另一种方法是使用与平均值几个标准差的位置，比如说3个。这种方法的一个缺点是，它比使用四分位数范围更容易受到极端值的影响。
- en: 'I find it helpful to produce this kind of analysis for all the key targets
    and features in my data, so let''s automate this method of identifying extreme
    values. We should also output the results to a file so that we can use them when
    we need them:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为为我的数据中的所有关键目标和特征生成这种分析是有帮助的，因此让我们自动化识别极端值的方法。我们还应该将结果输出到文件中，这样我们就可以在我们需要时使用它们：
- en: 'Let''s define a function, `getextremevalues`, that iterates over all of the
    columns of our DataFrame (except for the first one, which contains the location
    column), calculates the interquartile range for that column, selects all the observations
    with values above the high threshold or below the low threshold for that column,
    and then appends the results to a new DataFrame (`dfout`):'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，`getextremevalues`，它遍历我们的DataFrame（除了包含位置列的第一个列之外的所有列），计算该列的四分位数范围，选择所有高于该列高阈值或低于低阈值的观测值，然后将结果追加到一个新的DataFrame（`dfout`）中：
- en: '[PRE38]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Now, we can pass our `covidkeys` DataFrame to the `getextremevalues` function
    to get a DataFrame that contains the extreme values for each column. Then, we
    can display the number of extreme values for each column, which tells us that
    there were four extreme values for the total deaths per million people in the
    population (`total_deaths_mill`). Now, we can output the data to an Excel file:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们可以将我们的`covidkeys` DataFrame传递给`getextremevalues`函数，以获取包含每个列的极端值的DataFrame。然后，我们可以显示每个列的极端值数量，这告诉我们人口中每百万人的总死亡数（`total_deaths_mill`）有四个极端值。现在，我们可以将数据输出到Excel文件中：
- en: '[PRE39]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let''s take a closer look at the extreme deaths per million values. We can
    query the DataFrame we just created to get the `threshhigh` value for `total_deaths_mill`,
    which is `2654`. We can also get other key features for those countries with the
    extreme values since we have included that data in the new DataFrame:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看每百万极端死亡值。我们可以查询我们刚刚创建的DataFrame，以获取`total_deaths_mill`的`threshhigh`值，该值为`2654`。我们还可以获取具有极端值的那些国家的其他关键特征，因为我们已经将那些数据包含在新的DataFrame中：
- en: '[PRE40]'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Peru, Hungary, Bosnia and Herzegovina, and Czechia have `total_deaths_mill`
    above the extreme value threshold. One thing that stands out for three of these
    countries is how much above the average for percent of population 65 or older
    they are as well (the average for that feature, as we displayed in a preceding
    table, is 9). Although these are extreme values for deaths, the relationship between
    the elderly percentage of the population and COVID deaths may account for much
    of this and can do so without overfitting the model to these extreme cases. We
    will go through some strategies for teasing that out in the next chapter.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 秘鲁、匈牙利、波斯尼亚和黑塞哥维那以及捷克共和国的`total_deaths_mill`值超过了极端值阈值。这三个国家中的一个突出特点是，它们65岁或以上人口百分比远高于平均水平（该特征的均值为9，正如我们在先前的表中显示的那样）。尽管这些是死亡极端值，但老年人口百分比与COVID死亡之间的关系可能解释了其中很大一部分，并且可以在不过度拟合这些极端案例的情况下做到这一点。我们将在下一章中介绍一些提取这些策略的方法。
- en: So far, we have discussed outliers and extreme values without referencing distribution
    shape. What we have implied so far is that an extreme value is a rare value –
    significantly rarer than the values near the center of the distribution. But this
    makes the most sense when the feature's distribution approaches normal. If, on
    the other hand, a feature had a uniform distribution, a very high value would
    be no rarer than any other value.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了异常值和极端值，但没有提及分布形状。我们迄今为止所暗示的是，极端值是一个罕见值——比分布中心附近的值要罕见得多。但是，当特征分布接近正态分布时，这最有意义。另一方面，如果一个特征具有均匀分布，那么一个非常高的值并不比任何其他值罕见。
- en: 'In practice, then, we think about extreme values or outliers relative to the
    distribution of the feature. **Quantile-quantile** (**Q-Q**) plots can improve
    our sense of that distribution by allowing us to view it graphically relative
    to a theoretical distribution: normal, uniform, log, or others. Let''s take a
    look:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们会考虑相对于特征分布的极端值或异常值。**分位数-分位数**（**Q-Q**）图可以通过允许我们相对于理论分布（正态分布、均匀分布、对数分布或其他）图形化地查看它来提高我们对该分布的感觉：让我们看一下：
- en: 'Let''s create a Q-Q plot of total cases per million that''s relative to the
    normal distribution. We can use the `statsmodels` library for this:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个与正态分布相关的每百万总病例的Q-Q图。我们可以使用`statsmodels`库来完成这个任务：
- en: '[PRE41]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'This produces the following plot:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下图表：
- en: '![Figure 1.1 – Q-Q plot of total cases per million ](img/B17978_01_001.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 每百万总病例的Q-Q图](img/B17978_01_001.jpg)'
- en: Figure 1.1 – Q-Q plot of total cases per million
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 每百万总病例的Q-Q图
- en: This Q-Q plot makes it clear that the distribution of total cases across countries
    is not normal. We can see this by how much the data points deviate from the red
    line. It is a Q-Q plot that we would expect from a distribution with some positive
    skew. This is consistent with the summary statistics we have already calculated
    for the total cases feature. It further reinforces our developing sense, in that
    we will need to be cautious about parametric models and that we will probably
    have to account for more than just one or two outlier observations.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Q-Q图清楚地表明，各国总病例的分布不是正态分布。我们可以通过数据点与红线偏离的程度来看到这一点。这是一个我们预期来自具有某些正偏斜的分布的Q-Q图。这与我们已为总病例特征计算出的摘要统计一致。它进一步强化了我们正在形成的认识，即我们需要对参数模型保持谨慎，我们可能需要考虑的不仅仅是单个或两个异常观测值。
- en: Let's look at a Q-Q plot for a feature with a distribution that is a little
    closer to normal. There isn't a great candidate in the COVID data, so we will
    work with data from the United States National Oceanic and Atmospheric Administration
    on land temperatures in 2019.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个分布略接近正态分布的特征的Q-Q图。在COVID数据中没有很好的候选者，所以我们将使用美国国家海洋和大气管理局2019年陆地温度的数据。
- en: Data Note
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 数据备注
- en: The land temperature dataset contains the average temperature readings (in Celsius)
    in 2019 from over 12,000 stations across the world, though the majority of the
    stations are in the United States. The raw data was retrieved from the Global
    Historical Climatology Network integrated database. It has been made available
    for public use by the United States National Oceanic and Atmospheric Administration
    at [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4)2.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 陆地温度数据集包含了来自全球超过 12,000 个站点的 2019 年平均温度读数（摄氏度），尽管大多数站点位于美国。原始数据是从全球历史气候学网络综合数据库中检索的。美国国家海洋和大气管理局已将其公开供公众使用，网址为
    [https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-](https://www.ncdc.noaa.gov/data-access/land-based-station-data/land-based-datasets/global-historical-climatology-network-monthly-version-4)2。
- en: 'First, let''s load the data into a pandas DataFrame and run some descriptive
    statistics on the temperature feature, `avgtemp`. We must add a few percentile
    statistics to the normal `describe` output to get a better sense of the range
    of values. `avgtemp` is the average temperature for the year at each of the 12,095
    weather stations. The average temperature across all stations was 11.2 degrees
    Celsius. The median was 10.4\. However, there were some very negative values,
    including 14 weather stations with an average temperature of less than -25\. This
    contributes to a moderately negative skew, though both the skew and kurtosis are
    closer to what would be expected from a normal distribution:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，让我们将数据加载到 pandas DataFrame 中，并对温度特征 `avgtemp` 运行一些描述性统计。我们必须向正常的 `describe`
    输出中添加一些百分位数统计，以更好地了解值的范围。`avgtemp` 是每个 12,095 个气象站一年的平均温度。所有站点的平均温度为 11.2 摄氏度。中位数为
    10.4。然而，有一些非常低的值，包括 14 个平均温度低于 -25 的气象站。这导致了一中度负偏斜，尽管偏斜和峰度都更接近正态分布的预期：
- en: '[PRE42]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, let''s take a look at a Q-Q plot of the average temperature:'
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们看一下平均温度的 Q-Q 图：
- en: '[PRE43]'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'This produces the following plot:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 1.2 – Q-Q plot of average temperatures ](img/B17978_01_002.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.2 – 平均温度的 Q-Q 图](img/B17978_01_002.jpg)'
- en: Figure 1.2 – Q-Q plot of average temperatures
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 – 平均温度的 Q-Q 图
- en: Along most of the range, the distribution of average temperatures looks pretty
    close to normal. The exceptions are the extremely low temperatures, contributing
    to a small amount of negative skew. There is also some deviation from normal at
    the high end, though this is much less of an issue (you may have noticed that
    Q-Q plots for features with negative skew have an umbrella-like shape, while those
    with positive skews, such as total cases, have more of a bowl-like shape).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数范围内，平均温度的分布看起来非常接近正态分布。例外的是极低温度，这导致了一小部分负偏斜。在高端也有一些偏离正态分布的情况，尽管这并不是一个大问题（你可能已经注意到，具有负偏斜的特征的
    Q-Q 图具有雨伞状形状，而具有正偏斜的特征，如总病例数，则具有更多碗状形状）。
- en: We are off to a good start in our efforts to understand the distribution of
    possible features and targets and, to a related effort, to identify extreme values
    and outliers. This is important information to have at our fingertips when we
    construct, refine, and interpret our models. But there is more that we can do
    to improve our intuition about the data. A good next step is to construct visualizations
    of key features.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在理解可能特征和目标分布以及识别极端值和异常值的相关努力中取得了良好的开端。当我们构建、改进和解释模型时，这些重要信息对我们来说至关重要。但我们还可以做更多的事情来提高我们对数据的直觉。一个好的下一步是构建关键特征的可视化。
- en: Using histograms, boxplots, and violin plots to examine the distribution of
    features
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用直方图、箱线图和提琴图来检查特征分布
- en: We have already generated many of the numbers that would make up the data points
    of a histogram or boxplot. But we often improve our understanding of the data
    when we see it represented graphically. We see observations bunched around the
    mean, we notice the size of the tails, and we see what seem to be extreme values.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经生成了构成直方图或箱线图数据点的许多数字。但当我们看到数据以图形方式表示时，我们往往能更好地理解数据。我们看到观测值围绕着平均值聚集，我们注意到尾部的尺寸，我们看到似乎极端的值。
- en: Using histograms
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用直方图
- en: 'Follow these steps to create a histogram:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下步骤创建直方图：
- en: 'We will work with both the COVID data and the temperatures data in this section.
    In addition to the libraries we have worked with so far, we must import Seaborn
    to create some plots more easily than we could in Matplotlib:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本节中，我们将同时处理COVID数据和温度数据。除了我们迄今为止使用的库之外，我们还必须导入Seaborn，以便比在Matplotlib中更容易地创建一些图表：
- en: '[PRE44]'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Now, let''s create a simple histogram. We can use Matplotlib''s `hist` method
    to create a histogram of total cases per million. We will also draw lines for
    the mean and median:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们创建一个简单的直方图。我们可以使用Matplotlib的`hist`方法创建每百万人口中总病例的直方图。我们还将绘制均值和中位数的线条：
- en: '[PRE45]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'This produces the following plot:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 1.3 – Total COVID cases ](img/B17978_01_003.jpg)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图1.3 – 总COVID病例](img/B17978_01_003.jpg)'
- en: Figure 1.3 – Total COVID cases
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 – 总COVID病例
- en: One aspect of the total distribution that this histogram highlights is that
    most countries (more than 100 of the 192) are in the very first bin, between 0
    cases per million and 25,000 cases per million. Here, we can see the positive
    skew, with the mean pulled to the right by extreme high values. This is consistent
    with what we discovered when we used Q-Q plots in the previous section.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这个直方图突出显示的总体分布的一个方面是，大多数国家（192个中的100多个）都在第一个区间内，即每百万人口0例到25,000例之间。在这里，我们可以看到正偏斜，由于极端高值，均值被拉向右边。这与我们在上一节中使用Q-Q图发现的情况一致。
- en: 'Let''s create a histogram of average temperatures from the land temperatures
    dataset:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 让我们创建一个来自陆地温度数据集的平均温度直方图：
- en: '[PRE46]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This produces the following plot:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 1.4 – Average land temperatures ](img/B17978_01_004.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 平均陆地温度](img/B17978_01_004.jpg)'
- en: Figure 1.4 – Average land temperatures
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 平均陆地温度
- en: The histogram for the average land temperatures from the land temperatures dataset
    looks quite different. Except for a few highly negative values, this distribution
    looks closer to normal. Here, we can see that the mean and the median are quite
    close and that the distribution looks fairly symmetrical.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 来自陆地温度数据集的平均陆地温度直方图看起来相当不同。除了少数几个高度负值外，这种分布看起来更接近正态分布。在这里，我们可以看到均值和众数非常接近，分布看起来相当对称。
- en: 'We should take a look at the observations at the extreme left of the distribution.
    They are all in Antarctica or the extreme north of Canada. Here, we have to wonder
    if it makes sense to include observations with such extreme values in the models
    we construct. However, it would be premature to make that determination based
    on these results alone. We will come back to this in the next chapter when we
    examine multivariate techniques for identifying outliers:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们应该看一下分布最左端的观测值。它们都在南极洲或加拿大最北部。在这里，我们必须怀疑是否应该将具有如此极端值的观测值包含在我们构建的模型中。然而，仅基于这些结果就做出这样的决定还为时过早。我们将在下一章中回到这个问题，当我们检查用于识别异常值的多变量技术时：
- en: '[PRE47]'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: An excellent way to visualize central tendency, spread, and outliers at the
    same time is with a boxplot.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 同时可视化集中趋势、离散度和异常值的一个极好方法是使用箱线图。
- en: Using boxplots
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用箱线图
- en: '**Boxplots** show us the interquartile range, with whiskers representing 1.5
    times the interquartile range, and data points beyond that range that can be considered
    extreme values. If this calculation seems familiar, it''s because it''s the same
    one we used earlier in this chapter to identify extreme values! Let''s get started:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**箱线图**显示了四分位距，其中触须代表四分位距的1.5倍，以及超出该范围的数据点，这些数据点可以被认为是极端值。如果这个计算看起来很熟悉，那是因为我们之前在本章中用来识别极端值的就是这个计算！让我们开始吧：'
- en: 'We can use the Matplotlib `boxplot` method to create a boxplot of total cases
    per million people in the population. We can draw arrows to show the interquartile
    range (the first quartile, median, and third quartile) and the extreme value threshold.
    The three circles above the threshold can be considered extreme values. The line
    from the interquartile range to the extreme value threshold is typically referred
    to as the whisker. There are usually whiskers above and below the interquartile
    range, but the threshold value below the first quartile value would be negative
    in this case:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用Matplotlib的`boxplot`方法创建每百万人口中总病例的箱线图。我们可以画箭头来表示四分位距（第一四分位数、中位数和第三四分位数）以及极端值阈值。阈值上方的三个圆圈可以被认为是极端值。从四分位距到极端值阈值的线通常被称为触须。通常在四分位距上方和下方都有触须，但在这个情况下，低于第一四分位数阈值的值将是负数：
- en: '[PRE48]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'This produces the following plot:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 1.5 – Boxplot of total cases ](img/B17978_01_005.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.5 – 总病例数的箱线图](img/B17978_01_005.jpg)'
- en: Figure 1.5 – Boxplot of total cases
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.5 – 总病例数的箱线图
- en: It is helpful to take a closer look at the interquartile range, specifically
    where the median falls within the range. For this boxplot, the median is at the
    lower end of the range. This is what we see in distributions with positive skews.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察四分位数范围是有帮助的，特别是中位数在范围内的位置。对于这个箱线图，中位数位于范围的低端。这就是我们在正偏分布中看到的情况。
- en: 'Now, let''s create a boxplot for the average temperature. All of the extreme
    values are now at the low end of the distribution. Unsurprisingly, given what
    we have already seen with the average temperature feature, the median line is
    closer to the center of the interquartile range than with our previous boxplot
    (we will not annotate the plot this time – we only did this last time for explanatory
    purposes):'
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们为平均温度创建一个箱线图。所有的极端值现在都位于分布的低端。不出所料，鉴于我们已经看到的平均温度特征，中位数线比我们之前的箱线图更接近四分位数范围的中心（这次我们不会注释图表——我们上次这样做是为了解释目的）：
- en: '[PRE49]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This produces the following plot:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 1.6 – Boxplot of average temperature ](img/B17978_01_006.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 平均温度的箱线图](img/B17978_01_006.jpg)'
- en: Figure 1.6 – Boxplot of average temperature
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 平均温度的箱线图
- en: Histograms help us see the spread of a distribution, while boxplots make it
    easy to identify outliers. We can get a good sense of both the spread of the distribution
    and the outliers in one graphic with a violin plot.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图帮助我们了解分布的分布情况，而箱线图则使识别异常值变得容易。我们可以通过小提琴图在一个图表中很好地了解分布的分布情况和异常值。
- en: Using violin plots
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用小提琴图
- en: Violin plots combine histograms and boxplots into one plot. They show the IQR,
    median, and whiskers, as well as the frequency of the observations at all the
    value ranges.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 小提琴图结合了直方图和箱线图，在一个图表中显示。它们显示了四分位数范围、中位数和触须，以及所有值范围内的观测频率。
- en: 'Let''s get started:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧：
- en: 'We can use Seaborn to create violin plots of both the COVID cases per million
    and the average temperature features. I am using Seaborn here, rather than Matplotlib,
    because I prefer its default options for violin plots:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 Seaborn 为每百万个 COVID 病例和平均温度特征创建 violin 图。我在这里使用 Seaborn 而不是 Matplotlib，因为我更喜欢其默认的
    violin 图选项：
- en: '[PRE50]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'This produces the following plot:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下图表：
- en: '![Figure 1.7 – Violin plots of COVID cases and land temperatures ](img/B17978_01_007.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – COVID 病例和陆地温度的小提琴图](img/B17978_01_007.jpg)'
- en: Figure 1.7 – Violin plots of COVID cases and land temperatures
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – COVID 病例和陆地温度的小提琴图
- en: The black bar with the white dot in the middle is the interquartile range, while
    the white dot represents the median. The height at each point (when the violin
    plot is horizontal) gives us the relative frequency. The thin black lines to the
    right of the interquartile range for cases per million, and to the right and left
    for the average temperature are the whiskers. The extreme values are shown in
    the part of the distribution beyond the whiskers.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 中间带有白色圆点的黑色条形表示四分位数范围，而白色圆点代表中位数。当小提琴图水平时，每个点的高度（即当小提琴图水平时）给出了相对频率。四分位数范围右侧的细黑线表示每百万个案例，平均温度的左右两侧是触须。极端值显示在触须之外的分布部分。
- en: If I am going to create just one plot for a numeric feature, I will create a
    violin plot. Violin plots allow me to see central tendency, shape, and spread
    all in one graphic. Space does not permit it here, but I usually like to create
    violin plots of all of my continuous features and save those to a PDF file for
    later reference.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我要为数值特征创建一个图表，我会创建一个小提琴图。小提琴图让我能够在一个图表中看到集中趋势、形状和分布。空间不允许在这里展示，但我通常喜欢创建所有连续特征的
    violin 图，并将它们保存到 PDF 文件中以供以后参考。
- en: Summary
  id: totrans-204
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at some common techniques for exploring data. We
    learned how to retrieve subsets of data when that is required for our analysis.
    We also used pandas methods to generate key statistics on features such as mean,
    interquartile range, and skew. This gave us a better sense of the central tendency,
    spread, and shape of the distribution of each feature. It also put us in a better
    position to identify outliers. Finally, we used the Matplotlib and Seaborn libraries
    to create histograms, boxplots, and violin plots. This yielded additional insights
    about the distribution of features, such as the length of the tail and divergence
    from the normal distribution.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了探索数据的一些常用技术。我们学习了如何在需要时检索数据的子集以供我们的分析使用。我们还使用了pandas方法来生成关于均值、四分位数范围和偏度等特征的关键统计数据。这让我们对每个特征的分布的中心趋势、分布范围和形状有了更好的理解。这也使我们能够更好地识别异常值。最后，我们使用了Matplotlib和Seaborn库来创建直方图、箱线图和小提琴图。这为我们提供了关于特征分布的额外见解，例如尾部长度和与正态分布的偏离程度。
- en: Visualizations are a great supplement to the tools for univariate analysis that
    we have discussed in this chapter. Histograms, boxplots, and violin plots display
    the shape and spread of each feature's distribution. Graphically, they show what
    we may miss by examining a few summary statistics, such as where there is a bulge
    (or bulges) in the distribution and where the extreme values are. These visualizations
    will be every bit as helpful when we explore bivariate and multivariate relationships,
    which we will do in [*Chapter 2*](B17978_02_ePub.xhtml#_idTextAnchor025), *Examining
    Bivariate and Multivariate Relationships between Features and Targets*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化是本章讨论的单变量分析工具的绝佳补充。直方图、箱线图和小提琴图显示了每个特征分布的形状和分布范围。从图形上看，它们显示了通过检查一些汇总统计数据可能遗漏的内容，例如分布中的隆起（或隆起）位置以及极端值所在的位置。当我们探索特征与目标之间的双变量和多变量关系时，这些可视化将同样有用，我们将在[*第二章*](B17978_02_ePub.xhtml#_idTextAnchor025)“检查特征与目标之间的双变量和多变量关系”中进行探讨。
