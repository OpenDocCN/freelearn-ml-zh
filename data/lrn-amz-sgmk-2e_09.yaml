- en: 'Chapter 7: Extending Machine Learning Services Using Built-In Frameworks'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 7 章：使用内置框架扩展机器学习服务
- en: In the last three chapters, you learned how to use built-in algorithms to train
    and deploy models without having to write a line of machine learning code. However,
    these algorithms don't cover the full spectrum of machine learning problems. In
    a lot of cases, you'll need to write your own code. Thankfully, several open source
    frameworks make this reasonably easy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的三章中，你学习了如何使用内置算法来训练和部署模型，而无需编写一行机器学习代码。然而，这些算法并没有涵盖所有的机器学习问题。在许多情况下，你需要编写自己的代码。幸运的是，几个开源框架让这个过程相对容易。
- en: 'In this chapter, you will learn how to train and deploy models with the most
    popular open source frameworks for machine learning and deep learning. We will
    cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用最流行的开源机器学习和深度学习框架来训练和部署模型。我们将涵盖以下主题：
- en: Discovering the built-in frameworks in Amazon SageMaker
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现 Amazon SageMaker 中的内置框架
- en: Running your framework code on Amazon SageMaker
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上运行你的框架代码
- en: Using the built-in frameworks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用内置框架
- en: Let's get started!
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need an AWS account to run the examples included in this chapter. If
    you haven't got one already, please point your browser to [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    to create one. You should also familiarize yourself with the AWS Free Tier ([https://aws.amazon.com/free/](https://aws.amazon.com/free/)),
    which lets you use many AWS services for free within certain usage limits.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个 AWS 账户来运行本章中包含的示例。如果你还没有账户，请访问 [https://aws.amazon.com/getting-started/](https://aws.amazon.com/getting-started/)
    创建一个。你还应该了解 AWS 免费套餐（[https://aws.amazon.com/free/](https://aws.amazon.com/free/)），它允许你在一定的使用限制内免费使用许多
    AWS 服务。
- en: You will need to install and configure the AWS command-line interface for your
    account ([https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要为你的账户安装并配置 AWS 命令行界面（[https://aws.amazon.com/cli/](https://aws.amazon.com/cli/)）。
- en: You will need a working Python 3.x environment. Installing the Anaconda distribution
    ([https://www.anaconda.com/](https://www.anaconda.com/)) is not mandatory but
    strongly encouraged, as it includes many projects that we will need (Jupyter,
    `pandas`, `numpy`, and more).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个可用的 Python 3.x 环境。安装 Anaconda 发行版（[https://www.anaconda.com/](https://www.anaconda.com/)）不是必须的，但强烈建议安装，因为它包含了我们需要的许多项目（Jupyter、`pandas`、`numpy`
    等）。
- en: You will need a working Docker installation. You can find installation instructions
    and the necessary documentation at [https://docs.docker.com](https://docs.docker.com).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要一个可用的 Docker 安装。你可以在 [https://docs.docker.com](https://docs.docker.com) 找到安装说明和相关文档。
- en: The code examples included in the book are available on GitHub at [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition).
    You will need to install a Git client to access them ([https://git-scm.com/](https://git-scm.com/)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中包含的代码示例可以在 GitHub 上找到，网址是 [https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition](https://github.com/PacktPublishing/Learn-Amazon-SageMaker-second-edition)。你需要安装一个
    Git 客户端才能访问它们（[https://git-scm.com/](https://git-scm.com/)）。
- en: Discovering the built-in frameworks in Amazon SageMaker
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发现 Amazon SageMaker 中的内置框架
- en: 'SageMaker lets you train and deploy your models with the following machine
    learning and deep learning frameworks:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 让你使用以下机器学习和深度学习框架来训练和部署模型：
- en: '**Scikit-learn**, undoubtedly the most widely used open source library for
    machine learning. If you''re new to this topic, start here: [https://scikit-learn.org](https://scikit-learn.org).'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scikit-learn**，无疑是最广泛使用的开源机器学习库。如果你是这个领域的新手，可以从这里开始： [https://scikit-learn.org](https://scikit-learn.org)。'
- en: '**XGBoost**, an extremely popular and versatile open source algorithm for regression,
    classification, and ranking problems ([https://xgboost.ai](https://xgboost.ai)).
    It''s also available as a built-in algorithm, as presented in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*. Using it in framework mode will give us more
    flexibility.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XGBoost**，一种非常流行且多功能的开源算法，适用于回归、分类和排序问题（[https://xgboost.ai](https://xgboost.ai)）。它也作为内置算法提供，如在[*第
    4 章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)《训练机器学习模型》中所展示的那样。以框架模式使用它将为我们提供更多的灵活性。'
- en: '**TensorFlow**, an extremely popular open source library for deep learning
    ([https://www.tensorflow.org](https://www.tensorflow.org)). SageMaker also supports
    the lovable **Keras** API ([https://keras.io](https://keras.io)).'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow**，一个极受欢迎的深度学习开源库（[https://www.tensorflow.org](https://www.tensorflow.org)）。SageMaker
    还支持受人喜爱的 **Keras** API（[https://keras.io](https://keras.io)）。'
- en: '**PyTorch**, another highly popular open source library for deep learning ([https://pytorch.org](https://pytorch.org)).
    Researchers, in particular, enjoy its flexibility.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PyTorch**，另一个备受欢迎的深度学习开源库（[https://pytorch.org](https://pytorch.org)）。特别是研究人员喜欢它的灵活性。'
- en: '**Apache MXNet**, an interesting challenger for deep learning. Natively implemented
    in C++, it''s often faster and more scalable than its competitors. Its **Gluon**
    API provides rich toolkits for computer vision ([https://gluon-cv.mxnet.io](https://gluon-cv.mxnet.io)),
    **Natural Language Processing** (**NLP**) ([https://gluon-nlp.mxnet.io](https://gluon-nlp.mxnet.io)),
    and time series data ([https://gluon-ts.mxnet.io](https://gluon-ts.mxnet.io)).'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Apache MXNet**，一个有趣的深度学习挑战者。它是用 C++ 原生实现的，通常比其竞争对手更快且更具可扩展性。其 **Gluon** API
    提供了丰富的计算机视觉工具包（[https://gluon-cv.mxnet.io](https://gluon-cv.mxnet.io)）、**自然语言处理**（**NLP**）（[https://gluon-nlp.mxnet.io](https://gluon-nlp.mxnet.io)）和时间序列数据（[https://gluon-ts.mxnet.io](https://gluon-ts.mxnet.io)）。'
- en: '**Chainer**, another worthy challenger for deep learning ([https://chainer.org](https://chainer.org)).'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chainer**，另一个值得关注的深度学习挑战者（[https://chainer.org](https://chainer.org)）。'
- en: '**Hugging Face**, the most popular collection of state-of-the-art tools and
    models for NLP ([https://huggingface.co](https://huggingface.co)).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Hugging Face**，最流行的、用于自然语言处理的最前沿工具和模型集合（[https://huggingface.co](https://huggingface.co)）。'
- en: Frameworks for **reinforcement learning**, such as **Intel Coach**, **Ray RLlib**,
    and **Vowpal Wabbit**. I won't discuss this topic here as it could take up another
    book!
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**强化学习**框架，如 **Intel Coach**、**Ray RLlib** 和 **Vowpal Wabbit**。由于这可能需要一本书的篇幅，我在这里不会讨论这个话题！'
- en: '**Spark**, thanks to a dedicated SDK that lets you train and deploy models
    directly from your Spark application using either **PySpark** or **Scala** ([https://github.com/aws/sagemaker-spark](https://github.com/aws/sagemaker-spark)).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark**，借助一个专用的 SDK，允许你直接从 Spark 应用程序中使用 **PySpark** 或 **Scala** 训练和部署模型（[https://github.com/aws/sagemaker-spark](https://github.com/aws/sagemaker-spark)）。'
- en: You'll find plenty of examples of all of these at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-python-sdk)
    找到这些的许多示例。
- en: 'In this chapter, we''ll focus on the most popular ones: XGBoost, scikit-learn,
    TensorFlow, PyTorch, and Spark.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将重点关注最流行的框架：XGBoost、scikit-learn、TensorFlow、PyTorch 和 Spark。
- en: The best way to get started is to run a first simple example. As you will see,
    the workflow is the same as for built-in algorithms. We'll highlight a few differences
    along the way, which we'll dive into later in this chapter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的入门方式是先运行一个简单的示例。如你所见，工作流与内置算法是相同的。我们将在过程中强调一些差异，稍后将在本章深入讨论。
- en: Running a first example with XGBoost
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 XGBoost 运行第一个示例
- en: In this example, we'll build a binary classification model with the XGBoost
    built-in framework. At the time of writing, the latest version supported by SageMaker
    is 1.3-1.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用 XGBoost 内置框架构建一个二分类模型。写这篇文章时，SageMaker 支持的最新版本是 1.3-1。
- en: 'We''ll use our own training script based on the `xgboost.XGBClassifier` object
    and the Direct Marketing dataset, which we used in [*Chapter 3*](B17705_03_Final_JM_ePub.xhtml#_idTextAnchor049),
    *AutoML with Amazon SageMaker Autopilot*:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用基于 `xgboost.XGBClassifier` 对象和直接营销数据集的自定义训练脚本，这个数据集我们在 [*第 3 章*](B17705_03_Final_JM_ePub.xhtml#_idTextAnchor049)，*使用
    Amazon SageMaker Autopilot 进行自动化机器学习* 中用过：
- en: 'First, we download and extract the dataset:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们下载并解压数据集：
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We import the SageMaker SDK and define an S3 prefix for the job:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入 SageMaker SDK，并为任务定义一个 S3 前缀：
- en: '[PRE1]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We load the dataset and apply very basic processing (as it''s not our focus
    here). Simply one-hot encode the categorical features, move the labels to the
    first column (an XGBoost requirement), shuffle the dataset, split it for training
    and validation, and save the results in two separate CSV files:'
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载数据集并进行非常基础的处理（因为这不是我们的重点）。简单地对分类特征进行独热编码，将标签移动到第一列（这是 XGBoost 的要求），打乱数据集，分割为训练集和验证集，并将结果保存在两个单独的
    CSV 文件中：
- en: '[PRE2]'
  id: totrans-35
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We upload the two files to S3:'
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将这两个文件上传到 S3：
- en: '[PRE3]'
  id: totrans-37
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We define two inputs, with data in CSV format:'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义两个输入，数据格式为 CSV：
- en: '[PRE4]'
  id: totrans-39
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Define an estimator for the training job. Of course, we could use the generic
    `Estimator` object and pass the name of the XGBoost container hosted in `XGBoost`
    estimator, which automatically selects the right container:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为训练任务定义一个估算器。当然，我们也可以使用通用的 `Estimator` 对象，并传递 XGBoost 容器在 `XGBoost` 估算器中的名称，这样就会自动选择正确的容器：
- en: '[PRE5]'
  id: totrans-41
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Several parameters are familiar here: the role, the infrastructure requirements,
    and the output path. What about the other ones? `entry_point` is the path of our
    training script (available in the GitHub repository for this book). `hyperparameters`
    is passed to the training script. We also have to select a `framework_version`
    value; this is the version of XGBoost that we want to use.'
  id: totrans-42
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里有几个熟悉的参数：角色、基础设施要求和输出路径。其他参数呢？`entry_point` 是我们训练脚本的路径（可以在本书的GitHub仓库中找到）。`hyperparameters`
    会传递给训练脚本。我们还需要选择一个 `framework_version` 值；这是我们想要使用的XGBoost版本。
- en: 'We train as usual:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们照常进行训练：
- en: '[PRE6]'
  id: totrans-44
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We also deploy as usual, creating a unique endpoint name:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们也照常进行部署，创建一个唯一的端点名称：
- en: '[PRE7]'
  id: totrans-46
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, we load a few samples from the validation set and send them for prediction
    in CSV format. The response contains a score between 0 and 1 for each sample:'
  id: totrans-47
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 然后，我们从验证集加载一些样本，并将它们以CSV格式发送进行预测。响应包含每个样本的0到1之间的分数：
- en: '[PRE8]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This prints out the following probabilities:'
  id: totrans-49
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将输出以下概率：
- en: '[PRE9]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'When we''re done, we delete the endpoint:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，我们删除端点：
- en: '[PRE10]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: We used XGBoost here, but the workflow would be identical for another framework.
    This standard way of training and deploying makes it really easy to switch from
    built-in algorithms to frameworks, or from one framework to the next.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里使用了XGBoost，但这个工作流程对于其他框架也是完全相同的。这种标准的训练和部署方式使得从内置算法切换到框架，或从一个框架切换到下一个框架变得非常简单。
- en: 'The points that we need to focus on here are as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要重点关注的要点如下：
- en: '**Framework containers**: What are they? Can we see how they''re built? Can
    we customize them? Can we use them to train on our local machine?'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**框架容器**：它们是什么？我们能看到它们是如何构建的吗？我们可以自定义它们吗？我们能用它们在本地机器上进行训练吗？'
- en: '**Training**: How does a SageMaker training script differ from vanilla framework
    code? How does it receive hyperparameters? How should it read input data? Where
    should it save the model?'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：SageMaker 训练脚本与普通框架代码有何不同？它如何接收超参数？它应该如何读取输入数据？模型应该保存在哪里？'
- en: '**Deploying**: How is the model deployed? Should the script provide some code
    for this? What''s the input format for prediction?'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**部署**：模型是如何部署的？脚本需要提供一些相关代码吗？预测的输入格式是什么？'
- en: '`entry_point` script? Can we add libraries for training and deployment?'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`entry_point` 脚本？我们可以为训练和部署添加库吗？'
- en: All these questions will be answered now!
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些问题现在都将得到解答！
- en: Working with framework containers
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用框架容器
- en: SageMaker contains a training and inference container for each built-in framework,
    and they are updated regularly to the latest versions. Different containers are
    also available for CPU and GPU instances. All these containers are collectively
    known as **Deep Learning Containers** ([https://aws.amazon.com/machine-learning/containers](https://aws.amazon.com/machine-learning/containers)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker 为每个内置框架提供训练和推理容器，并定期更新到最新版本。不同的容器也可供CPU和GPU实例使用。所有这些容器统称为**深度学习容器**
    ([https://aws.amazon.com/machine-learning/containers](https://aws.amazon.com/machine-learning/containers))。
- en: As we saw in the previous example, they let you use your own code without having
    to maintain bespoke containers. In most cases, you won't need to look any further,
    and you can happily forget that these containers even exist. If this topic feels
    too advanced for now, feel free to skip it for now, and move on to the *Training
    and deploying locally* section.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的例子中看到的，它们允许你使用自己的代码，而无需维护定制的容器。在大多数情况下，你不需要再进一步了解容器的细节，你可以高兴地忘记它们的存在。如果这个话题目前对你来说太高级，可以暂时跳过，继续阅读*本地训练与部署*部分。
- en: 'If you''re curious or have custom requirements, you''ll be happy to learn that
    the code for these containers is open source:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感到好奇或有定制需求，你会很高兴得知这些容器的代码是开源的：
- en: '**Scikit-learn**: [https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container)'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Scikit-learn**: [https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container)'
- en: '**XGBoost**: [https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container)'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**XGBoost**: [https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container)'
- en: '**TensorFlow, PyTorch, Apache MXNet, and Hugging Face**: [https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TensorFlow，PyTorch，Apache MXNet和Hugging Face**：[https://github.com/aws/deep-learning-containers](https://github.com/aws/deep-learning-containers)'
- en: '**Chainer**: [https://github.com/aws/sagemaker-chainer-container](https://github.com/aws/sagemaker-chainer-container)'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Chainer**：[https://github.com/aws/sagemaker-chainer-container](https://github.com/aws/sagemaker-chainer-container)'
- en: 'For starters, this lets you understand how these containers are built and how
    SageMaker trains and predicts with them. You could also do the following:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，这可以帮助你理解这些容器是如何构建的，以及SageMaker是如何使用它们进行训练和预测的。你还可以执行以下操作：
- en: Build and run them on your local machine for local experimentation.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地机器上构建并运行它们进行本地实验。
- en: Build and run them on your favorite managed Docker service, such as **Amazon
    ECS**, **Amazon EKS**, or **Amazon Fargate** ([https://aws.amazon.com/containers](https://aws.amazon.com/containers)).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你最喜欢的托管Docker服务上构建并运行它们，例如**Amazon ECS**、**Amazon EKS**或**Amazon Fargate**（[https://aws.amazon.com/containers](https://aws.amazon.com/containers)）。
- en: Customize them, push them to Amazon ECR, and use them with the estimators present
    in the SageMaker SDK. We'll demonstrate this in [*Chapter 8*](B17705_08_Final_JM_ePub.xhtml#_idTextAnchor147),
    *Using Your Algorithms and Code*.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义它们，推送到Amazon ECR，并使用SageMaker SDK中的估算器。我们将在[*第8章*](B17705_08_Final_JM_ePub.xhtml#_idTextAnchor147)中演示这一点，*使用你的算法和代码*。
- en: These containers have another nice property. You can use them with the SageMaker
    SDK to train and deploy models on your local machine. Let's see how this works.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这些容器有一个很好的特性。你可以与SageMaker SDK一起使用它们，在本地机器上训练和部署模型。让我们看看这个是如何工作的。
- en: Training and deploying locally
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 本地训练和部署
- en: '**Local mode** is the ability to train and deploy models with the SageMaker
    SDK without firing up on-demand managed infrastructure in AWS. You use your local
    machine instead. In this context, "local" means the machine running the notebook:
    it could be your laptop, a local server, or a small **notebook instance**.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '**本地模式**是通过SageMaker SDK训练和部署模型，而无需启动AWS中的按需托管基础设施。你将使用本地机器代替。在此上下文中，“本地”指的是运行笔记本的机器：它可以是你的笔记本电脑、本地服务器，或者一个小型**笔记本实例**。'
- en: Note
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: At the time of writing, local mode is not available in SageMaker Studio.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在写本文时，本地模式在SageMaker Studio中不可用。
- en: This is an excellent way to quickly experiment and iterate on a small dataset.
    You won't have to wait for instances to come up, and you won't have to pay for
    them either!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个快速实验和迭代小型数据集的极好方式。你无需等待实例启动，也无需为此支付费用！
- en: 'Let''s revisit our previous XGBoost example, highlighting the changes required
    to use local mode:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视之前的XGBoost示例，重点讲解使用本地模式时所需的更改：
- en: 'Explicitly set the name of the IAM role. `get_execution_role()` does not work
    on your local machine (it does on a notebook instance):'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显式设置IAM角色的名称。`get_execution_role()`在本地机器上不起作用（在笔记本实例上有效）：
- en: '[PRE11]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Load the training and validation datasets from local files. Store the model
    locally in `/tmp`:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从本地文件加载训练和验证数据集。将模型存储在本地`/tmp`目录中：
- en: '[PRE12]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the `XGBoost` estimator, set `instance_type` to `local`. For local GPU training,
    we would use `local_gpu`.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`XGBoost`估算器中，将`instance_type`设置为`local`。对于本地GPU训练，我们将使用`local_gpu`。
- en: In `xgb_estimator.deploy()`, set `instance_type` to `local`.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在`xgb_estimator.deploy()`中，将`instance_type`设置为`local`。
- en: That's all it takes to train on your local machine using the same container
    you would use at scale on AWS. This container will be pulled once to your local
    machine and you'll be using it from then on. When you're ready to train at scale,
    just replace the `local` or `local_gpu` instance type with the appropriate AWS
    instance type and you're good to go.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是使用与AWS大规模环境中相同的容器在本地机器上进行训练所需的一切。此容器会被拉取到本地机器，之后你将一直使用它。当你准备好大规模训练时，只需将`local`或`local_gpu`实例类型替换为适当的AWS实例类型，就可以开始训练了。
- en: Troubleshooting
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 故障排除
- en: If you see strange deployment errors, try restarting Docker (`sudo service docker
    restart`). I found that it doesn't like being interrupted during deployment, which
    it tends to do a lot when working inside Jupyter Notebooks!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果遇到奇怪的部署错误，可以尝试重启Docker（`sudo service docker restart`）。我发现它在部署过程中经常被中断，尤其是在Jupyter
    Notebooks中工作时！
- en: Now, let's see what it takes to run our own code inside these containers. This
    feature is called **script mode**.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看在这些容器中运行自己代码所需的条件。这个功能叫做**脚本模式**。
- en: Training with script mode
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用脚本模式进行训练
- en: 'Since your training code runs inside a SageMaker container, it needs to be
    able to do the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您的训练代码运行在 SageMaker 容器内，它需要能够执行以下操作：
- en: Receive hyperparameters passed to the estimator.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接收传递给估算器的超参数。
- en: Read data available in input channels (training, validation, and more).
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 读取输入通道中可用的数据（训练数据、验证数据等）。
- en: Save the trained model in the right place.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将训练好的模型保存到正确的位置。
- en: 'Script mode is how SageMaker makes this possible. The name comes from the way
    your code is invoked in the container. Looking at the training log for our XGBoost
    job, we see this:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本模式是 SageMaker 使这一切成为可能的方式。该名称来自于您的代码在容器中被调用的方式。查看我们 XGBoost 作业的训练日志，我们可以看到：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Our code is invoked like a plain Python script (hence the name script mode).
    We can see that hyperparameters are passed as command-line arguments, which answers
    the question of what we should use inside the script to read them: `argparse`.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码像普通的 Python 脚本一样被调用（因此称为脚本模式）。我们可以看到，超参数作为命令行参数传递，这也回答了我们应该在脚本中使用什么来读取它们：`argparse`。
- en: 'Here''s the corresponding code snippet in our script:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们脚本中相应的代码片段：
- en: '[PRE14]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'What about the location of the input data and the saved model? If we look at
    the log a little more closely, we''ll see this:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 那么输入数据和保存模型的位置呢？如果我们稍微仔细查看日志，就会看到：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: These three environment variables define **local paths inside the container**,
    pointing to the respective locations for the training data, validation data, and
    the saved model. Does this mean we have to manually copy the datasets and the
    model from and to S3? No! SageMaker takes care of all this automatically for us.
    This is part of the support code present in the container.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个环境变量定义了**容器内的本地路径**，指向训练数据、验证数据和保存模型的相应位置。这是否意味着我们必须手动将数据集和模型从 S3 复制到容器中并返回？不！SageMaker
    会为我们自动处理这一切。这是容器中支持代码的一部分。
- en: Our script only needs to read these variables. I recommend using `argparse`
    again, as this will let us pass the paths to our script when we train outside
    of SageMaker (more on this soon).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的脚本只需要读取这些变量。我建议再次使用 `argparse`，这样我们可以在 SageMaker 之外训练时将路径传递给脚本（稍后会详细介绍）。
- en: 'Here''s the corresponding code snippet in our script:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们脚本中相应的代码片段：
- en: '[PRE16]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Channel names
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 通道名称
- en: The `SM_CHANNEL_xxx` variables are named according to the channels passed to
    `fit()`. For instance, if your algorithm required a channel named `foobar`, you'd
    name it `foobar` in `fit()` and `SM_CHANNEL_FOOBAR` in your script. In your container,
    the data for that channel would automatically be available in `/opt/ml/input/data/foobar`.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '`SM_CHANNEL_xxx` 变量是根据传递给 `fit()` 的通道命名的。例如，如果您的算法需要一个名为 `foobar` 的通道，您需要在
    `fit()` 中将其命名为 `foobar`，并在脚本中使用 `SM_CHANNEL_FOOBAR`。在您的容器中，该通道的数据会自动保存在 `/opt/ml/input/data/foobar`
    目录下。'
- en: 'To sum things up, in order to train framework code on SageMaker, we only need
    to do the following:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，为了在 SageMaker 上训练框架代码，我们只需要做以下几件事：
- en: Use `argparse` to read hyperparameters passed as command-line arguments. Chances
    are you're already doing this in your code anyway!
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `argparse` 读取作为命令行参数传递的超参数。您可能已经在代码中这样做了！
- en: Read the `SM_CHANNEL_xxx` environment variables and load data from there.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 `SM_CHANNEL_xxx` 环境变量并从中加载数据。
- en: Read the `SM_MODEL_DIR` environment variable and save the trained model there.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 `SM_MODEL_DIR` 环境变量并将训练好的模型保存到该位置。
- en: Now, let's talk about deploying models trained in script mode.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论在脚本模式下部署训练好的模型。
- en: Understanding model deployment
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解模型部署
- en: 'In general, your script needs to include the following:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您的脚本需要包括以下内容：
- en: A function to load the model
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个加载模型的函数。
- en: A function to process input data before it's passed to the model
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在将输入数据传递给模型之前处理数据的函数。
- en: A function to process predictions before they're returned to the caller
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个在返回预测结果给调用方之前处理预测结果的函数。
- en: The amount of actual work required depends on the framework and the input format
    you use. Let's see what this means for TensorFlow, PyTorch, MXNet, XGBoost, and
    scikit-learn.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的实际工作量取决于您使用的框架和输入格式。让我们看看这对 TensorFlow、PyTorch、MXNet、XGBoost 和 scikit-learn
    意味着什么。
- en: Deploying with TensorFlow
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 部署
- en: The TensorFlow inference container relies on the **TensorFlow Serving** model
    server for model deployment ([https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)).
    For this reason, your training code must save the model in this format. Model
    loading and prediction are available automatically.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 推理容器依赖于**TensorFlow Serving**模型服务器进行模型部署（[https://www.tensorflow.org/tfx/guide/serving](https://www.tensorflow.org/tfx/guide/serving)）。因此，你的训练代码必须以这种格式保存模型。模型加载和预测功能会自动提供。
- en: JSON is the default input format for prediction, and it also works for `numpy`
    arrays thanks to automatic serialization. JSON Lines and CSV are also supported.
    For other formats, you can implement your own preprocessing and postprocessing
    functions, `input_handler()` and `output_handler()`. You'll find more information
    at [https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator](https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: JSON是预测的默认输入格式，并且由于自动序列化，它也适用于`numpy`数组。JSON Lines和CSV也被支持。对于其他格式，你可以实现自己的预处理和后处理函数，`input_handler()`和`output_handler()`。你可以在[https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator](https://sagemaker.readthedocs.io/en/stable/using_tf.html#deploying-from-an-estimator)找到更多信息。
- en: You can also dive deeper into the TensorFlow inference container at [https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference](https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以深入了解TensorFlow推理容器，访问[https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference](https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference)。
- en: Deploying with PyTorch
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用PyTorch进行部署
- en: The PyTorch inference container relies on the `__call__()` method. If not, you
    should provide a `predict_fn()` function in the inference script.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch推理容器依赖于`__call__()`方法。如果没有，你应该在推理脚本中提供`predict_fn()`函数。
- en: For prediction, `numpy` is the default input format. JSON Lines and CSV are
    also supported. For other formats, you can implement your own preprocessing and
    postprocessing functions. You'll find more information at [https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预测，`numpy`是默认的输入格式。JSON Lines和CSV也被支持。对于其他格式，你可以实现自己的预处理和后处理函数。你可以在[https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model](https://sagemaker.readthedocs.io/en/stable/frameworks/pytorch/using_pytorch.html#serve-a-pytorch-model)找到更多信息。
- en: You can dive deeper into the PyTorch inference container at [https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference](https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以深入了解PyTorch推理容器，访问[https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference](https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference)。
- en: Deploying with Apache MXNet
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Apache MXNet进行部署
- en: The Apache MXNet inference container relies on **Multi-Model Server** (**MMS**)
    for model deployment ([https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)).
    It uses the default MXNet model format.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Apache MXNet 推理容器依赖于**多模型服务器**（**MMS**）进行模型部署（[https://github.com/awslabs/multi-model-server](https://github.com/awslabs/multi-model-server)）。它使用默认的MXNet模型格式。
- en: Models based on the `Module` API do not require a model loading function. For
    prediction, they support data in JSON, CSV, or `numpy` format.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于`Module` API的模型不需要模型加载函数。对于预测，它们支持JSON、CSV或`numpy`格式的数据。
- en: Gluon models do require a model loading function as parameters need to be explicitly
    initialized. Data can be sent in JSON or `numpy` format.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: Gluon模型确实需要一个模型加载函数，因为参数需要显式初始化。数据可以通过JSON或`numpy`格式发送。
- en: For other data formats, you can implement your own preprocessing, prediction,
    and postprocessing functions. You can find more information at [https://sagemaker.readthedocs.io/en/stable/using_mxnet.html](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他数据格式，你可以实现自己的预处理、预测和后处理函数。你可以在[https://sagemaker.readthedocs.io/en/stable/using_mxnet.html](https://sagemaker.readthedocs.io/en/stable/using_mxnet.html)找到更多信息。
- en: You can dive deeper into the MXNet inference container at [https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker](https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以深入了解MXNet推理容器，访问[https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker](https://github.com/aws/deep-learning-containers/tree/master/mxnet/inference/docker)。
- en: Deploying XGBoost and scikit-learn
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用XGBoost和scikit-learn进行部署
- en: Likewise, XGBoost and scikit-learn rely on [https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container)
    and [https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container),
    respectively.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，XGBoost和scikit-learn分别依赖于[https://github.com/aws/sagemaker-xgboost-container](https://github.com/aws/sagemaker-xgboost-container)和[https://github.com/aws/sagemaker-scikit-learn-container](https://github.com/aws/sagemaker-scikit-learn-container)。
- en: 'Your script needs to provide the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 你的脚本需要提供以下内容：
- en: A `model_fn()` function to load the model. Just like for training, the location
    of the model to load is passed in the `SM_MODEL_DIR` environment variable.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个`model_fn()`函数用于加载模型。与训练类似，加载模型的位置通过`SM_MODEL_DIR`环境变量传递。
- en: Two optional functions to deserialize and serialize prediction data, named `input_fn()`
    and `output_fn()`. These functions are only required if you need another input
    format other than JSON, CSV, or `numpy`.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个可选函数，用于反序列化和序列化预测数据，分别命名为`input_fn()`和`output_fn()`。只有当你需要其他格式的输入数据（例如非JSON、CSV或`numpy`）时，才需要这些函数。
- en: An optional `predict_fn()` function passes deserialized data to the model and
    returns a prediction. This is only required if you need to preprocess data before
    predicting it, or to postprocess predictions.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可选的`predict_fn()`函数将反序列化的数据传递给模型并返回预测结果。仅当你需要在预测之前对数据进行预处理，或对预测结果进行后处理时才需要这个函数。
- en: 'For XGBoost and scikit-learn, the `model_fn()` function is extremely simple
    and quite generic. Here are a couple of examples that should work in most cases:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于XGBoost和scikit-learn，`model_fn()`函数非常简单且通用。以下是一些大多数情况下都能正常工作的示例：
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: SageMaker also lets you import and export models. You can upload an existing
    model to S3 and deploy it directly on SageMaker. Likewise, you can copy a trained
    model from S3 and deploy it elsewhere. We'll look at this in detail in [*Chapter
    11*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237), *Deploying Machine Learning
    Models*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker还允许你导入和导出模型。你可以将现有模型上传到S3并直接在SageMaker上部署。同样，你也可以将训练好的模型从S3复制到其他地方进行部署。我们将在[*第11章*](B17705_11_Final_JM_ePub.xhtml#_idTextAnchor237)，《部署机器学习模型》中详细介绍这一点。
- en: Now, let's talk about training and deployment dependencies.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来讨论训练和部署所需的依赖关系。
- en: Managing dependencies
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 管理依赖关系
- en: In many cases, you'll need to add extra source files and libraries to the framework's
    containers. Let's see how we can easily do this.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，你需要向框架的容器中添加额外的源文件和库。让我们看看如何轻松做到这一点。
- en: Adding source files for training
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加训练所需的源文件
- en: By default, all estimators load the entry point script from the current directory.
    If you need additional source files for training, estimators let you pass a `source_dir`
    parameter, which points at the directory storing the extra files. Please note
    that the entry point script must be in the same directory.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有估算器都会从当前目录加载入口脚本。如果你需要额外的源文件来进行训练，估算器允许你传递一个`source_dir`参数，指向存储额外文件的目录。请注意，入口脚本必须位于同一目录中。
- en: 'In the following example, `myscript.py` and all additional source files must
    be placed in the `src` directory. SageMaker will automatically package the directory
    and copy it inside the training container:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`myscript.py`和所有额外的源文件必须放在`src`目录中。SageMaker将自动打包该目录并将其复制到训练容器中：
- en: '[PRE18]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Adding libraries for training
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 添加训练所需的库
- en: You can use different techniques to add libraries that are required for training.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用不同的技术来添加训练所需的库。
- en: For libraries that can be installed with `pip`, the simplest technique is to
    add a `requirements.txt` file in the same folder as the entry point script. SageMaker
    will automatically install these libraries inside the container.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可以通过`pip`安装的库，最简单的方法是将`requirements.txt`文件与入口脚本放在同一文件夹中。SageMaker会自动在容器内安装这些库。
- en: 'Alternatively, you can use `pip` to install libraries directly in the training
    script by issuing a `pip install` command. We used this in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Natural Language Processing Models*, with LDA and NTM. This is useful
    when you don''t want to or cannot modify the SageMaker code that launches the
    training job:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 另外，你可以通过在训练脚本中执行`pip install`命令，使用`pip`直接安装库。我们在[*第6章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)，《训练自然语言处理模型》中使用了这个方法，处理了LDA和NTM。这个方法在你不想或者不能修改启动训练作业的SageMaker代码时非常有用：
- en: '[PRE19]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: For libraries that can't be installed with `pip`, you should use the `dependencies`
    parameter. It's available in all estimators, and it lets you list libraries to
    add to the training job. These libraries need to be present locally, in a virtual
    environment or a bespoke directory. SageMaker will package them and copy them
    inside the training container.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不能通过 `pip` 安装的库，你应该使用 `dependencies` 参数。这个参数在所有估算器中都可用，它允许你列出要添加到训练作业中的库。这些库需要在本地、虚拟环境或特定目录中存在。SageMaker
    会将它们打包并复制到训练容器中。
- en: 'In the following example, `myscript.py` needs the `mylib` library. We install
    it in the `lib` local directory:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，`myscript.py` 需要 `mylib` 库。我们将在 `lib` 本地目录中安装它：
- en: '[PRE20]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Then, we pass its location to the estimator:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将其位置传递给估算器：
- en: '[PRE21]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The last technique is to install libraries in the Dockerfile for the container,
    rebuild the image, and push it to Amazon ECR. If you also need the libraries at
    prediction time (say, for preprocessing), this is the best option.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 最后的技术是将库安装到 Dockerfile 中的容器里，重建镜像并将其推送到 Amazon ECR。如果在预测时也需要这些库（例如，用于预处理），这是最好的选择。
- en: Adding libraries for deployment
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为部署添加库
- en: If you need specific libraries to be available at prediction time, you can use
    a `requirements.txt` file for libraries that can be installed with `pip`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要在预测时提供特定的库，可以使用一个 `requirements.txt` 文件，列出那些可以通过 `pip` 安装的库。
- en: 'For other libraries, the only option is to customize the framework container.
    You can pass its name to the estimator with the `image_uri` parameter:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 对于其他库，唯一的选择是自定义框架容器。你可以通过`image_uri`参数将其名称传递给估算器：
- en: '[PRE22]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We covered a lot of technical topics in this section. Now, let's look at the
    big picture.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本节中涵盖了许多技术主题。现在，让我们来看一下大局。
- en: Putting it all together
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: 'The typical workflow when working with frameworks looks like this:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 使用框架时的典型工作流程如下所示：
- en: Implement script mode in your code; that is, read the necessary hyperparameters,
    input data, and output location.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在你的代码中实现脚本模式；也就是说，读取必要的超参数、输入数据和输出位置。
- en: If required, add a `model_fn()` function to load the model.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有需要，添加一个 `model_fn()` 函数来加载模型。
- en: Test your training code locally, outside of any SageMaker container.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地测试你的训练代码，避免使用任何 SageMaker 容器。
- en: Configure the appropriate estimator (`XGBoost`, `TensorFlow`, and so on).
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置适当的估算器（如`XGBoost`、`TensorFlow`等）。
- en: Train in local mode using the estimator, with either the built-in container
    or a container you've customized.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用估算器在本地模式下训练，使用内置容器或你自定义的容器。
- en: Deploy in local mode and test your model.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在本地模式下部署并测试你的模型。
- en: Switch to a managed instance type (say, `ml.m5.large`) for training and deployment.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到托管实例类型（例如，`ml.m5.large`）进行训练和部署。
- en: This logical progression requires little work at each step. It minimizes friction,
    the risk of mistakes, and frustration. It also optimizes instance time and cost—no
    need to wait and pay for managed instances if your code crashes immediately because
    of a silly bug.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这个逻辑进展每一步所需的工作很少。它最大程度地减少了摩擦、错误的风险和挫败感。它还优化了实例时间和成本——如果你的代码因为一个小错误立即崩溃，就不必等待并支付托管实例的费用。
- en: Now, let's put this knowledge to work. In the next section, we're going to run
    a simple scikit-learn example. The purpose is to make sure we understand the workflow
    we just discussed.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们开始运用这些知识。在接下来的部分中，我们将运行一个简单的 scikit-learn 示例。目的是确保我们理解刚刚讨论的工作流程。
- en: Running your framework code on Amazon SageMaker
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Amazon SageMaker 上运行你的框架代码
- en: 'We will start from a vanilla scikit-learn program that trains and saves a linear
    regression model on the Boston Housing dataset, which we used in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从一个简单的 scikit-learn 程序开始，该程序在波士顿住房数据集上训练并保存一个线性回归模型，数据集在[*第4章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)中使用过，*训练机器学习模型*：
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Let's update it so that it runs on SageMaker.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更新它，使其可以在 SageMaker 上运行。
- en: Implementing script mode
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现脚本模式
- en: 'Now, we will use the framework to implement script mode, as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用框架实现脚本模式，如下所示：
- en: 'First, read the hyperparameters as command-line arguments:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，读取命令行参数中的超参数：
- en: '[PRE24]'
  id: totrans-182
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Read the input and output paths as command-line arguments. We could decide
    to remove the splitting code and pass two input channels instead. Let''s stick
    to one channel, that is, `training`:'
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入和输出路径作为命令行参数读取。我们可以决定去除拆分代码，改为传递两个输入通道。我们还是坚持使用一个通道，也就是`training`：
- en: '[PRE25]'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'As we''re using scikit-learn, we need to add `model_fn()` to load the model
    at deployment time:'
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们使用的是scikit-learn，我们需要添加`model_fn()`以便在部署时加载模型：
- en: '[PRE26]'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: With that, we're done. Time to test!
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 到此为止，我们完成了。是时候测试了！
- en: Testing locally
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 本地测试
- en: First, we test our script on our local machine in a Python 3 environment, outside
    of any SageMaker container. We just need to make sure that we have `pandas` and
    scikit-learn installed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在本地机器上的Python 3环境中测试我们的脚本，不依赖任何SageMaker容器。我们只需要确保安装了`pandas`和scikit-learn。
- en: 'We set the environment variables to empty values as we will pass the paths
    on the command line:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将环境变量设置为空值，因为我们将在命令行上传递路径：
- en: '[PRE27]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Nice. Our code runs fine with command-line arguments. We can use this for local
    development and debugging, until we're ready to move it to SageMaker local mode.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 很好。我们的代码在命令行参数下运行得很顺利。我们可以使用它进行本地开发和调试，直到我们准备好将其迁移到SageMaker本地模式。
- en: Using local mode
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用本地模式
- en: 'We''ll get started using the following steps:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将按照以下步骤开始：
- en: 'Still on our local machine, we configure an `SKLearn` estimator in local mode,
    setting the role according to the setup we''re using. Use local paths only:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仍然在我们的本地机器上，我们配置一个`SKLearn`估算器以本地模式运行，根据我们使用的设置来设定角色。只使用本地路径：
- en: '[PRE28]'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'As expected, we can see how our code is invoked in the training log. Of course,
    we get the same outcome:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如预期的那样，我们可以在训练日志中看到如何调用我们的代码。当然，我们得到的是相同的结果：
- en: '[PRE29]'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'We deploy locally and send some CSV samples for prediction:'
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在本地部署并发送一些CSV样本进行预测：
- en: '[PRE30]'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'By printing the response, we will see the predicted values:'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通过打印响应，我们将看到预测值：
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: With local mode, we can quickly iterate on our model. We're only limited by
    the compute and storage capabilities of the local machine. When that happens,
    we can easily move to managed infrastructure.
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 使用本地模式，我们可以快速迭代模型。我们仅受限于本地机器的计算和存储能力。当达到限制时，我们可以轻松迁移到托管基础设施。
- en: Using managed infrastructure
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用托管基础设施
- en: 'When it''s time to train at scale and deploy in production, all we have to
    do is make sure the input data is in S3 and replace the "local" instance type
    with an actual instance type:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 当需要进行大规模训练并在生产环境中部署时，我们只需确保输入数据存储在S3中，并将“本地”实例类型替换为实际的实例类型：
- en: '[PRE32]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Since we''re using the same container, we can be confident that training and
    deployment will work as expected. Again, I strongly recommend that you follow
    this logical progression: local work first, then SageMaker local mode, and finally,
    SageMaker managed infrastructure. It will help you focus on what needs to be done
    and when.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是相同的容器，我们可以放心训练和部署会按预期工作。再次强调，我强烈建议您遵循以下逻辑流程：首先进行本地工作，然后是SageMaker本地模式，最后是SageMaker托管基础设施。这将帮助你集中精力处理需要做的事以及何时做。
- en: For the remainder of this chapter, we're going to run additional examples.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的其余部分，我们将运行更多示例。
- en: Using the built-in frameworks
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用内建框架
- en: We've covered XGBoost and scikit-learn already. Now, it's time to see how we
    can use deep learning frameworks. Let's start with TensorFlow and Keras.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经覆盖了XGBoost和scikit-learn。现在，是时候看看如何使用深度学习框架了。让我们从TensorFlow和Keras开始。
- en: Working with TensorFlow and Keras
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用TensorFlow和Keras
- en: In this example, we're going to use TensorFlow 2.4.1 to train a simple convolutional
    neural network on the Fashion-MNIST dataset ([https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)).
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将使用TensorFlow 2.4.1来训练一个简单的卷积神经网络，数据集使用Fashion-MNIST（[https://github.com/zalandoresearch/fashion-mnist](https://github.com/zalandoresearch/fashion-mnist)）。
- en: 'Our code is split into two source files: one for the entry point script (`fmnist.py`)
    and one for the model (`model.py`, based on Keras layers). For the sake of brevity,
    I will only discuss the SageMaker steps. You can find the full code in the GitHub
    repository for this book:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码分成了两个源文件：一个是入口点脚本（`fmnist.py`），另一个是模型（`model.py`，基于Keras层）。为了简洁起见，我只讨论SageMaker的步骤。你可以在本书的GitHub仓库中找到完整代码：
- en: '`fmnist.py` starts by reading hyperparameters from the command line:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fmnist.py`首先从命令行读取超参数：'
- en: '[PRE33]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Next, we read the environment variables, that is, the input paths for the training
    set and the validation set, the output path for the model, and the number of GPUs
    available on the instance. It''s the first time we''re using the latter. It comes
    in handy to adjust the batch size for multi-GPU training as it''s common practice
    to multiply the initial batch''s size by the number of GPUs:'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们读取环境变量，即训练集和验证集的输入路径、模型的输出路径以及实例上可用的 GPU 数量。这是我们第一次使用后者。它对于调整多 GPU 训练的批量大小非常有用，因为通常做法是将初始批量大小乘以
    GPU 数量：
- en: '[PRE34]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Store the arguments in local variables. Then, load the dataset. Each channel
    provides us with a compressed `numpy` array for storing images and labels:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将参数存储在本地变量中。然后，加载数据集。每个通道为我们提供一个压缩的`numpy`数组，用于存储图像和标签：
- en: '[PRE35]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Then, prepare the data for training by reshaping the image tensors, normalizing
    the pixel values, one-hot encoding the image labels, and creating the `tf.data.Dataset`
    objects that will feed data to the model.
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，通过重新调整图像张量的形状、标准化像素值、进行独热编码图像标签，并创建将数据传输给模型的`tf.data.Dataset`对象，为训练准备数据。
- en: Create the model, compile it, and fit it.
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建模型、编译并拟合它。
- en: 'Once training is complete, save the model in TensorFlow Serving format at the
    appropriate output location. This step is important as this is the model server
    that SageMaker uses for TensorFlow models:'
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练完成后，将模型以 TensorFlow Serving 格式保存到适当的输出位置。此步骤非常重要，因为这是 SageMaker 用于 TensorFlow
    模型的模型服务器：
- en: '[PRE36]'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We train and deploy the model using the usual workflow:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用常规工作流程训练和部署模型：
- en: 'In a notebook powered by a TensorFlow 2 kernel, we download the dataset and
    upload it to S3:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个由 TensorFlow 2 内核支持的笔记本中，我们下载数据集并将其上传到 S3：
- en: '[PRE37]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We configure the `TensorFlow` estimator. We also set the `source_dir` parameter
    so that our model''s file is also deployed in the container:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们配置`TensorFlow`估算器。我们还设置`source_dir`参数，以便将模型文件也部署到容器中：
- en: '[PRE38]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Train and deploy as usual. We will go straight for managed infrastructure,
    but the same code will work fine on your local machine in local mode:'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 像往常一样训练和部署。我们将直接使用托管基础设施，但相同的代码也可以在本地模式下在你的本地机器上正常运行：
- en: '[PRE39]'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The validation accuracy should be 91-92%. By loading and displaying a few sample
    images from the validation dataset, we can predict their labels. The `numpy` payload
    is automatically serialized to JSON, which is the default format for prediction
    data:'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 验证准确率应为 91-92%。通过加载并显示一些验证数据集中的样本图像，我们可以预测它们的标签。`numpy`负载会自动序列化为 JSON，这是预测数据的默认格式：
- en: '[PRE40]'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'The output should look as follows:'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出应如下所示：
- en: '![Figure 7.1 – Viewing predicted classes'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 7.1 – 查看预测类别'
- en: '](img/B17705_07_1.jpg)'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_07_1.jpg)'
- en: Figure 7.1 – Viewing predicted classes
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 7.1 – 查看预测类别
- en: 'When we''re done, we delete the endpoint:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，我们删除端点：
- en: '[PRE41]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see, the combination of script mode and built-in containers makes
    it easy to run TensorFlow on SageMaker. Once you get into the routine, you'll
    be surprised at how fast you can move your models from your laptop to AWS.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，脚本模式与内置容器的结合使得在 SageMaker 上运行 TensorFlow 变得非常简单。一旦进入常规流程，你会惊讶于将模型从笔记本电脑迁移到
    AWS 的速度有多快。
- en: Now, let's take a look at PyTorch.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来看一下 PyTorch。
- en: Working with PyTorch
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 PyTorch
- en: PyTorch is extremely popular for computer vision, NLP, and more.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch 在计算机视觉、自然语言处理等领域非常流行。
- en: In this example, we're going to train a **Graph Neural Network** (**GNN**).
    This category of networks works particularly well on graph-structured data, such
    as social networks, life sciences, and more. In fact, our PyTorch code will use
    the **Deep Graph Library** (**DGL**), an open source library that makes it easier
    to build and train GNNs with TensorFlow, PyTorch, and Apache MXNet ([https://www.dgl.ai/](https://www.dgl.ai/)).
    DGL is already installed in these containers, so let's get to work directly.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我们将训练一个**图神经网络**（**GNN**）。这种类型的网络在图结构数据上表现特别好，如社交网络、生命科学等。事实上，我们的 PyTorch
    代码将使用**深度图书馆**（**DGL**），这是一个开源库，可以更轻松地使用 TensorFlow、PyTorch 和 Apache MXNet 构建和训练
    GNN（[https://www.dgl.ai/](https://www.dgl.ai/)）。DGL 已经安装在这些容器中，所以我们可以直接开始工作。
- en: 'We''re going to work with the Zachary Karate Club dataset ([http://konect.cc/networks/ucidata-zachary/](http://konect.cc/networks/ucidata-zachary/)).
    The following is the graph for this:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Zachary 空手道俱乐部数据集（[http://konect.cc/networks/ucidata-zachary/](http://konect.cc/networks/ucidata-zachary/))。以下是该图的内容：
- en: '![Figure 7.2 – The Zachary Karate Club dataset'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.2 – Zachary 空手道俱乐部数据集'
- en: '](img/B17705_07_2.jpg)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_07_2.jpg)'
- en: Figure 7.2 – The Zachary Karate Club dataset
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – Zachary空手道俱乐部数据集
- en: Nodes 0 and 33 are teachers, while the other nodes are students. Edges represent
    ties between these people. As the story goes, the two teachers had an argument
    and the club needs to be split in two.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 节点0和33是教师，而其他节点是学生。边表示这些人之间的关系。故事是这样的，两位老师发生了争执，俱乐部需要被分成两部分。
- en: The purpose of the training job is to find the "best" split. This can be defined
    as a semi-supervision classification task. The first teacher (node 0) is assigned
    class 0, while the second teacher (node 33) is assigned class 1\. All the other
    nodes are unlabeled, and their classes will be computed by a **graph convolutional
    network**. At the end of the last epoch, we'll retrieve the node classes and split
    the club accordingly.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 训练任务的目的是找到“最佳”分割。这可以定义为一个半监督分类任务。第一位老师（节点0）被分配为类别0，而第二位老师（节点33）被分配为类别1。所有其他节点是未标记的，它们的类别将由**图卷积网络**计算。在最后一个周期结束时，我们将提取节点类别，并根据这些类别来分割俱乐部。
- en: 'The dataset is stored as a pickled Python list containing edges. Here are the
    first few edges:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集被存储为一个包含边的pickle格式的Python列表。以下是前几条边：
- en: '[PRE42]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The SageMaker code is as simple as it gets. We upload the dataset to S3, create
    a `PyTorch` estimator, and train it:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker的代码简洁明了。我们将数据集上传到S3，创建一个`PyTorch`估算器，然后进行训练：
- en: '[PRE43]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: This hardly needs any explaining at all, does it?
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这一点几乎无需任何解释，对吧？
- en: 'Let''s take a look at the abbreviated training script, where we''re using script
    mode once again. The full version is available in the GitHub repository for this
    book:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下简化的训练脚本，在这里我们再次使用了脚本模式。完整版本可在本书的GitHub仓库中找到：
- en: '[PRE44]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The following classes are predicted. Nodes 0 and 1 are class 0, node 2 is class
    1, and so on:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 预测了以下类别。节点0和1是类别0，节点2是类别1，依此类推：
- en: '[PRE45]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'By plotting them, we can see that the club has been cleanly split:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 通过绘制它们，我们可以看到俱乐部已经被干净利落地分开了：
- en: '![Figure 7.3 – Viewing predicted classes'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 7.3 – 查看预测类别'
- en: '](img/B17705_07_3.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B17705_07_3.jpg)'
- en: Figure 7.3 – Viewing predicted classes
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 查看预测类别
- en: Once again, the SageMaker code doesn't stand in your way. The workflow and APIs
    are consistent from one framework to the next, and you can focus on the machine
    learning problem itself. Now, let's do another example with Hugging Face, where
    we'll also see how to deploy a PyTorch model with the built-in PyTorch container.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，SageMaker的代码并不会妨碍你。工作流和API在各个框架之间保持一致，你可以专注于机器学习问题本身。现在，让我们做一个新的示例，使用Hugging
    Face，同时还会看到如何通过内建的PyTorch容器部署一个PyTorch模型。
- en: Working with Hugging Face
  id: totrans-264
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 与Hugging Face合作
- en: '**Hugging Face** ([https://huggingface.co](https://huggingface.co)) has quickly
    become the most popular collection of open source models for NLP. At the time
    of writing, they host almost 10,000 state-of-the-art models ([https://huggingface.co/models](https://huggingface.co/models)),
    pretrained on datasets ([https://huggingface.co/datasets](https://huggingface.co/datasets))
    in over 250 languages ([https://huggingface.co/languages](https://huggingface.co/languages)).'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**Hugging Face** ([https://huggingface.co](https://huggingface.co)) 已迅速成为最受欢迎的自然语言处理开源模型集合。在撰写本文时，他们托管了近10,000个最先进的模型（[https://huggingface.co/models](https://huggingface.co/models)），并在超过250种语言的预训练数据集上进行了训练（[https://huggingface.co/datasets](https://huggingface.co/datasets)）。'
- en: 'To make it easy to quickly build high-quality NLP applications, Hugging Face
    actively developed three open source libraries:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速构建高质量的自然语言处理应用，Hugging Face积极开发了三个开源库：
- en: '**Transformers**: Train, fine-tune, and predict with Hugging Face models ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformers**：使用Hugging Face模型进行训练、微调和预测（[https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)）。'
- en: '**Datasets**: Download and process Hugging Face datasets ([https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)).'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Datasets**：下载并处理Hugging Face数据集（[https://github.com/huggingface/datasets](https://github.com/huggingface/datasets)）。'
- en: '**Tokenizers**: Tokenize text for training and prediction with Hugging Face
    models ([https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)).'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Tokenizers**：为Hugging Face模型的训练和预测进行文本标记化（[https://github.com/huggingface/tokenizers](https://github.com/huggingface/tokenizers)）。'
- en: Hugging Face tutorial
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: Hugging Face教程
- en: If you are completely new to Hugging Face, please run through their tutorial
    first at [https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html).
  id: totrans-271
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果你完全是Hugging Face的新手，请先通过他们的教程：[https://huggingface.co/transformers/quicktour.html](https://huggingface.co/transformers/quicktour.html)。
- en: SageMaker added support for Hugging Face in March 2021, on both TensorFlow and
    PyTorch. As you would expect, you can use a `HuggingFace` estimator and built-in
    containers. Let's run an example where we build a sentiment analysis model for
    English language customer reviews. For this purpose, we'll fine-tune a **DistilBERT**
    model ([https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)) implemented
    with PyTorch and pretrained on two large English language datasets (Wikipedia
    and the BookCorpus dataset).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker在2021年3月增加了对Hugging Face的支持，涵盖TensorFlow和PyTorch。正如你所料，你可以使用`HuggingFace`估算器和内置容器。让我们运行一个示例，构建一个英文客户评论的情感分析模型。为此，我们将微调一个**DistilBERT**模型（[https://arxiv.org/abs/1910.01108](https://arxiv.org/abs/1910.01108)），该模型使用PyTorch实现，并在两个大型英语语料库（Wikipedia和BookCorpus数据集）上进行了预训练。
- en: Preparing the dataset
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据集
- en: 'In this example, we''ll use a Hugging Face dataset named `generated_reviews_enth`
    ([https://huggingface.co/datasets/generated_reviews_enth](https://huggingface.co/datasets/generated_reviews_enth)).
    It includes an English review, its Thai translation, a flag indicating whether
    the translation is correct or not, and a star rating:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用一个名为`generated_reviews_enth`的Hugging Face数据集（[https://huggingface.co/datasets/generated_reviews_enth](https://huggingface.co/datasets/generated_reviews_enth)）。它包含英文评论、其泰语翻译、一个标记指示翻译是否正确，以及星级评分：
- en: '[PRE46]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'This is the format that the DistilBERT tokenizer expects: a `labels` variable
    (`0` for negative sentiment, `1` for positive) and a `text` variable with the
    English language review:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这是DistilBERT分词器所期望的格式：一个`labels`变量（`0`代表负面情绪，`1`代表正面情绪）和一个`text`变量，包含英文评论：
- en: '[PRE47]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Let''s get to work! I''ll show you the individual steps, and you''ll also find
    a **SageMaker Processing** version in the GitHub repository for this book:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！我将向你展示每个步骤，你也可以在这本书的GitHub仓库中找到一个**SageMaker处理**版本：
- en: 'We first install the `transformers` and `datasets` libraries:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先安装`transformers`和`datasets`库：
- en: '[PRE48]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'We download the dataset, which is already split for training (141,369 instances)
    and validation (15,708 instances). All data is in JSON format:'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载了数据集，该数据集已经分为训练集（141,369个实例）和验证集（15,708个实例）。所有数据均为JSON格式：
- en: '[PRE49]'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'In each review, we create a new variable named `labels`. We set it to `1` when
    `review_star` is equal to or higher than 4, and to `0` otherwise:'
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个评论中，我们创建一个名为`labels`的新变量。当`review_star`大于或等于4时，我们将其设置为`1`，否则设置为`0`：
- en: '[PRE50]'
  id: totrans-284
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The reviews are nested JSON documents, making it difficult to remove variables
    we don''t need. Let''s flatten both datasets:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些评论是嵌套的JSON文档，这使得移除我们不需要的变量变得困难。让我们将两个数据集扁平化：
- en: '[PRE51]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'We can now easily drop unwanted variables. We also rename the `translation.en`
    variable to `text`:'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以轻松丢弃不需要的变量。我们还将`translation.en`变量重命名为`text`：
- en: '[PRE52]'
  id: totrans-288
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The training and validation instances now have the format expected by the DistilBERT
    tokenizer. We already covered tokenization in [*Chapter 6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108),
    *Training Natural Language Processing Models*. A significant difference is that
    we use a tokenizer that was pretrained on the same English language corpus as
    the model:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，训练和验证实例已经具有DistilBERT分词器所期望的格式。我们已经在[*第6章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)，*训练自然语言处理模型*中讲解了分词。一个显著的区别是，我们使用的是一个在与模型相同的英语语料库上预训练的分词器：
- en: 'We download the tokenizer for our pretrained model:'
  id: totrans-290
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们下载我们预训练模型的分词器：
- en: '[PRE53]'
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'We tokenize both datasets. Words and punctuation are replaced with appropriate
    tokens. If needed, each sequence is padded or truncated to fit the input layer
    of the model (512 tokens):'
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们对两个数据集进行分词。单词和标点符号会被相应的标记替换。如果需要，每个序列会被填充或截断，以适应模型的输入层（512个标记）：
- en: '[PRE54]'
  id: totrans-293
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We drop the `text` variable, as it''s not needed anymore:'
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们丢弃`text`变量，因为它不再需要：
- en: '[PRE55]'
  id: totrans-295
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Printing out an instance, we see the attention mask (all ones, meaning no token
    is masked in the input sequence), the inputs IDs (the sequence of tokens), and
    the label:'
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印出一个实例，我们可以看到注意力掩码（全为1，意味着输入序列中的没有标记被掩盖）、输入ID（标记序列）和标签：
- en: '[PRE56]'
  id: totrans-297
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Data preparation is complete. Let's move on to training the model.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备工作完成。接下来我们进入模型训练阶段。
- en: Fine-tuning a Hugging Face model
  id: totrans-299
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 微调Hugging Face模型
- en: 'We''re not going to train from scratch: it would talk far too long, and we
    probably don''t have enough data anyway. Instead, we''re going to fine-tune the
    model. Starting from a model trained on a very large text corpus, we will just
    train it for one additional epoch on our own data, so that it picks up the particular
    patterns present in our data:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不打算从头开始训练：那样会花费太长时间，而且我们可能数据量也不足。相反，我们将对模型进行微调。从一个在大规模文本语料库上训练的模型开始，我们只会在我们自己的数据上再训练一个epoch，以便模型能够学习到数据中的特定模式：
- en: 'We start by uploading both datasets to S3\. The `datasets` library provides
    a convenient API to do this:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先将两个数据集上传到S3。`datasets`库提供了一个方便的API来实现这一点：
- en: '[PRE57]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We define hyperparameters and configure a `HuggingFace` estimator. Note that
    we''ll fine-tune the model for just one epoch:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们定义超参数并配置一个`HuggingFace`估算器。请注意，我们将仅对模型进行一个epoch的微调：
- en: '[PRE58]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'For the sake of brevity, I won''t discuss the training script (`train.py`),
    which is available in the GitHub repository for this book. There''s nothing particular
    about it: we use the `Trainer` Hugging Face API, as well as script mode to interface
    it with SageMaker. As we only train for a single epoch, checkpointing is disabled
    (`save_strategy=''no''`). This helps cuts down on training time (not saving checkpoints)
    and deployment time (the model artifact is smaller).'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 为了简洁起见，我不会讨论训练脚本（`train.py`），该脚本可以在本书的GitHub仓库中找到。它没有特别之处：我们使用`Trainer` Hugging
    Face API，并通过脚本模式与SageMaker进行接口。由于我们只训练一个epoch，因此禁用了检查点保存（`save_strategy='no'`）。这有助于缩短训练时间（不保存检查点）和部署时间（模型工件较小）。
- en: It's also worth noting that you can generate boilerplate code for your estimator
    on the Hugging Face website. As shown in the following screenshot, you can click
    on **Amazon SageMaker**, pick a task type, and copy and paste the generated code:![Figure
    7.4 – Viewing our model on the Hugging Face website
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 还值得注意的是，你可以在Hugging Face网站上为你的估算器生成模板代码。如以下截图所示，你可以点击**Amazon SageMaker**，选择任务类型，然后复制并粘贴生成的代码：![图7.4
    – 在Hugging Face网站上查看我们的模型](img/B17705_07_4.jpg)
- en: '](img/B17705_07_4.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_07_4.jpg)'
- en: Figure 7.4 – Viewing our model on the Hugging Face website
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.4 – 在Hugging Face网站上查看我们的模型
- en: 'We launch the training job as usual, and it lasts about 42 minutes:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们像往常一样启动训练作业，持续了大约42分钟：
- en: '[PRE59]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Just like with other frameworks, we could call the `deploy()` API in order to
    deploy our model to a SageMaker endpoint. You can find an example at [https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 就像其他框架一样，我们可以调用`deploy()` API来将模型部署到SageMaker端点。你可以在[https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/](https://aws.amazon.com/blogs/machine-learning/announcing-managed-inference-for-hugging-face-models-in-amazon-sagemaker/)找到一个示例。
- en: Instead, let's see how we can deploy our model with the built-in PyTorch container
    and **TorchServe**. In fact, this deployment example can generalize to any PyTorch
    model that you'd like to serve with TorchServe.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们看看如何使用内置的PyTorch容器和**TorchServe**部署我们的模型。实际上，这个部署示例可以推广到任何你希望通过TorchServe提供的PyTorch模型。
- en: 'I find this superb blog post by my colleague Todd Escalona extremely helpful
    in understanding how to serve PyTorch models with TorchServe: [https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/](https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/).'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现我同事Todd Escalona写的这篇精彩博客文章在理解如何通过TorchServe提供PyTorch模型方面非常有帮助：[https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/](https://aws.amazon.com/blogs/machine-learning/serving-pytorch-models-in-production-with-the-amazon-sagemaker-native-torchserve-integration/)。
- en: Deploying a Hugging Face model
  id: totrans-314
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 部署一个Hugging Face模型
- en: 'The only difference compared to previous examples is that we have to use the
    model artifact in S3 to create a `PyTorchModel` object, and to build a `Predictor`
    model that we can use `deploy()` and `predict()` on:'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的示例相比，唯一的区别是我们必须使用S3中的模型工件来创建一个`PyTorchModel`对象，并构建一个`Predictor`模型，我们可以在其上使用`deploy()`和`predict()`。
- en: 'Starting from the model artifact, we define a `Predictor` object, and we create
    a `PyTorchModel` with it:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从模型工件开始，我们定义一个`Predictor`对象，然后用它创建一个`PyTorchModel`：
- en: '[PRE60]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Zooming in on the inference script (`torchserve-predictor.py`), we write a
    model loading function to account for Hugging Face peculiarities that the PyTorch
    container can''t handle by default:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 聚焦于推理脚本（`torchserve-predictor.py`），我们编写了一个模型加载函数，解决 Hugging Face 特有的 PyTorch
    容器无法默认处理的情况：
- en: '[PRE61]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We also add a prediction function that returns a text label:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还添加了一个返回文本标签的预测函数：
- en: '[PRE62]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The inference script also includes basic `input_fn()` and `output_fn()` functions
    to check that data is in JSON format. You'll find the code in the GitHub repository
    for the book.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 推理脚本还包括基本的 `input_fn()` 和 `output_fn()` 函数，用于检查数据是否为 JSON 格式。你可以在本书的 GitHub
    仓库中找到相关代码。
- en: 'Coming back to our notebook, we deploy the model as usual:'
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 回到我们的笔记本，我们像往常一样部署模型：
- en: '[PRE63]'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Once the endpoint is up, we predict a text sample and print the result:'
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦端点启动，我们预测一个文本样本并打印结果：
- en: '[PRE64]'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Finally, we delete the endpoint:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们删除端点：
- en: '[PRE65]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: As you can see, it's really easy to work with Hugging Face models. It's also
    a cost-effective way to build high-quality NLP models, as we typically fine-tune
    them for a very small number of epochs.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，使用 Hugging Face 模型非常简单。这也是一种具有成本效益的构建高质量 NLP 模型的方式，因为我们通常只需对模型进行非常少的训练周期（epoch）微调。
- en: To close this chapter, let's look at how SageMaker and Apache Spark can work
    together.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束本章，让我们看看 SageMaker 和 Apache Spark 如何协同工作。
- en: Working with Apache Spark
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 Apache Spark
- en: In addition to the Python SageMaker SDK that we've been using so far, SageMaker
    also includes an SDK for Spark ([https://github.com/aws/sagemaker-spark](https://github.com/aws/sagemaker-spark)).
    This lets you run SageMaker jobs directly from a PySpark or Scala application
    running on a Spark cluster.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们一直在使用的 Python SageMaker SDK，SageMaker 还包括 Spark 的 SDK（[https://github.com/aws/sagemaker-spark](https://github.com/aws/sagemaker-spark)）。这使你能够直接从运行在
    Spark 集群上的 PySpark 或 Scala 应用程序中运行 SageMaker 作业。
- en: Combining Spark and SageMaker
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 结合 Spark 和 SageMaker
- en: First, you can decouple the **Extract-Transform-Load** (**ETL**) step and the
    machine learning step. Each usually has different infrastructure requirements
    (instance type, instance count, storage) that need to be the right size both technically
    and financially. Setting up your Spark cluster just right for ETL and using on-demand
    infrastructure in SageMaker for training and prediction is a powerful combination.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以将**提取-转换-加载**（**ETL**）步骤与机器学习步骤解耦。每个步骤通常有不同的基础设施需求（实例类型、实例数量、存储），这些需求需要在技术上和财务上都能满足。为
    ETL 设置合适的 Spark 集群，并在 SageMaker 上使用按需基础设施进行训练和预测是一个强大的组合。
- en: Second, although Spark's MLlib is an amazing library, you may need something
    else, such as custom algorithms in different languages or deep learning.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，尽管 Spark 的 MLlib 是一个令人惊叹的库，但你可能还需要其他工具，如不同语言的自定义算法或深度学习。
- en: Finally, deploying models for prediction on Spark clusters may not be the best
    option. SageMaker endpoints should be considered instead, especially since they
    support the **MLeap** format ([https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/)).
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将模型部署到 Spark 集群进行预测可能不是最佳选择。应考虑使用 SageMaker 端点，尤其是因为它们支持**MLeap**格式（[https://combust.github.io/mleap-docs/](https://combust.github.io/mleap-docs/)）。
- en: In the following example, we'll combine SageMaker and Spark to build a spam
    detection model. Data will be hosted in S3, with one text file for spam messages
    and one for non-spam ("ham") messages. We'll use Spark running on an Amazon EMR
    cluster to preprocess it. Then, we'll train and deploy a model with the XGBoost
    algorithm that's available in SageMaker. Finally, we'll predict data with it on
    our Spark cluster. For the sake of language diversity, we'll code with Scala this
    time.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们将结合 SageMaker 和 Spark 构建一个垃圾邮件检测模型。数据将托管在 S3 中，垃圾邮件和非垃圾邮件（“ham”）各有一个文本文件。我们将使用在
    Amazon EMR 集群上运行的 Spark 进行数据预处理。然后，我们将使用 SageMaker 中可用的 XGBoost 算法训练并部署模型。最后，我们将在
    Spark 集群上进行预测。为了语言的多样性，这次我们使用 Scala 编写代码。
- en: First of all, we need to build a Spark cluster.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要构建一个 Spark 集群。
- en: Creating a Spark cluster
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建 Spark 集群
- en: 'We will create the cluster as follows:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将如下方式创建集群：
- en: Starting from the `sagemaker-cluster`, click on **Next** again, and then click
    on **Create cluster**. You can find additional details at [https://docs.aws.amazon.com/emr/](https://docs.aws.amazon.com/emr/).
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `sagemaker-cluster` 开始，再次点击**下一步**，然后点击**创建集群**。你可以在[https://docs.aws.amazon.com/emr/](https://docs.aws.amazon.com/emr/)找到更多详细信息。
- en: While the cluster is being created, we define our Git repository in the **Notebooks**
    entry in the left-hand side vertical menu. Then, we click on **Add repository**:![Figure
    7.6 – Adding a Git repository
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在集群创建过程中，我们在左侧垂直菜单的**Notebooks**条目中定义我们的Git仓库。然后，我们点击**Add repository**：![图7.6
    – 添加Git仓库
- en: '](img/B17705_07_6.jpg)'
  id: totrans-343
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_07_6.jpg)'
- en: Figure 7.6 – Adding a Git repository
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.6 – 添加Git仓库
- en: Then, we create a Jupyter notebook connected to the cluster. Starting from the
    **Notebooks** entry in the left-hand side vertical menu, as shown in the following
    screenshot, we give it a name and select both the EMR cluster and the repository
    we just created. Then, we click on **Create notebook**:![Figure 7.7 – Creating
    a Jupyter notebook
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个连接到集群的Jupyter笔记本。从左侧垂直菜单中的**Notebooks**条目开始，如下图所示，我们为其命名，并选择我们刚刚创建的EMR集群和仓库。然后，我们点击**Create
    notebook**：![图7.7 – 创建Jupyter笔记本
- en: '](img/B17705_07_7.jpg)'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B17705_07_7.jpg)'
- en: Figure 7.7 – Creating a Jupyter notebook
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.7 – 创建Jupyter笔记本
- en: Once the cluster and the notebook are ready, we can click on **Open in Jupyter**,
    which takes us to the familiar Jupyter interface.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦集群和笔记本准备好，我们可以点击**Open in Jupyter**，这将带我们进入熟悉的Jupyter界面。
- en: Everything is now ready. Let's write a spam classifier!
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪。让我们编写一个垃圾邮件分类器！
- en: Building a spam classification model with Spark and SageMaker
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Spark和SageMaker构建垃圾邮件分类模型
- en: 'In this example, we''re going to use the combined benefits of Spark and SageMaker
    to train, deploy, and predict with a spam classification model, thanks to just
    a few lines of Scala code:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我们将利用Spark和SageMaker的结合优势，通过几行Scala代码来训练、部署和预测垃圾邮件分类模型：
- en: 'First, we need to make sure that our dataset is available in S3\. On our local
    machine, upload the two files to the default SageMaker bucket (feel free to use
    another bucket):'
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要确保数据集已在S3中可用。在本地机器上，将这两个文件上传到默认的SageMaker桶（也可以使用其他桶）：
- en: '[PRE66]'
  id: totrans-353
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Back in the Jupyter notebook, make sure it's running the Spark kernel. Then,
    import the necessary objects from Spark MLlib and the SageMaker SDK.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回到Jupyter笔记本，确保它正在运行Spark内核。然后，从Spark MLlib和SageMaker SDK中导入必要的对象。
- en: 'Load the data from S3\. Convert all the sentences into lowercase. Then, remove
    all punctuation and numbers and trim any whitespace:'
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从S3加载数据。将所有句子转换为小写字母。然后，移除所有标点符号和数字，并修剪掉任何空白字符：
- en: '[PRE67]'
  id: totrans-356
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'Then, split the messages into words and hash these words into 200 buckets.
    This technique is much less sophisticated than the word vectors we used in [*Chapter
    6*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108), *Training Natural Language
    Processing Models*, but it should do the trick:'
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，将消息拆分成单词，并将这些单词哈希到200个桶中。这个技术比我们在[*第6章*](B17705_06_Final_JM_ePub.xhtml#_idTextAnchor108)，“*训练自然语言处理模型*”中使用的单词向量简单得多，但应该能奏效：
- en: '[PRE68]'
  id: totrans-358
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'For example, the following message has one occurrence of a word from bucket
    15, one from bucket 83, two words from bucket 96, and two from bucket 188:'
  id: totrans-359
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 例如，以下消息中，桶15中的单词出现了一次，桶83中的单词出现了一次，桶96中的单词出现了两次，桶188中的单词也出现了两次：
- en: '[PRE69]'
  id: totrans-360
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We assign a `1` label for spam messages and a `0` label for ham messages:'
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们为垃圾邮件消息分配`1`标签，为正常邮件消息分配`0`标签：
- en: '[PRE70]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Merge the messages and encode them in **LIBSVM** format, one of the formats
    supported by **XGBoost**:'
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并消息并将其编码为**LIBSVM**格式，这是**XGBoost**支持的格式之一：
- en: '[PRE71]'
  id: totrans-364
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'The samples now look similar to this:'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 现在样本看起来类似于这样：
- en: '[PRE72]'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Split the data for training and validation:'
  id: totrans-367
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和验证集：
- en: '[PRE73]'
  id: totrans-368
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Configure the XGBoost estimator available in the SageMaker SDK. Here, we''re
    going to train and deploy in one single step:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置SageMaker SDK中可用的XGBoost估算器。在这里，我们将一次性训练并部署：
- en: '[PRE74]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Fire up a training job and a deployment job on the managed infrastructure,
    exactly like when we worked with built-in algorithms in [*Chapter 4*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069),
    *Training Machine Learning Models*. The SageMaker SDK automatically passes the
    Spark DataFrame to the training job, so no work is required from our end:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启动一个训练任务和一个部署任务，在托管基础设施上，就像我们在[*第4章*](B17705_04_Final_JM_ePub.xhtml#_idTextAnchor069)，“*训练机器学习模型*”中使用内置算法时那样。SageMaker
    SDK会自动将Spark DataFrame传递给训练任务，因此我们无需做任何工作：
- en: '[PRE75]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: As you would expect, these activities are visible in SageMaker Studio in the
    **Experiments** section.
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如你所期待的，这些活动将在SageMaker Studio的**Experiments**部分中可见。
- en: 'When the deployment is complete, transform the test set and score the model.
    This automatically invokes the SageMaker endpoint. Once again, we don''t need
    to worry about data movement:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 部署完成后，转换测试集并对模型进行评分。这会自动调用 SageMaker 端点。再次提醒，我们无需担心数据迁移：
- en: '[PRE76]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: The accuracy should be around 97%, which is not too bad!
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 准确率应该在 97% 左右，表现得还不错！
- en: 'Once done, delete all SageMaker resources created by the job. This will delete
    the model, the endpoint, and the endpoint configuration (an object we haven''t
    discussed yet):'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 完成后，删除作业创建的所有 SageMaker 资源。这将删除模型、端点以及端点配置（一个我们还没有讨论过的对象）：
- en: '[PRE77]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: Don't forget to terminate the notebook and the EMR cluster too. You can easily
    do this in the EMR console.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 别忘了终止笔记本和 EMR 集群。你可以在 EMR 控制台轻松完成这一步。
- en: This example demonstrates how easy it is to combine the respective strengths
    of Spark and SageMaker. Another way to do this is to build MLlib pipelines with
    a mix of Spark and SageMaker stages. You'll find examples of this at [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这个示例演示了如何轻松地结合 Spark 和 SageMaker 各自的优势。另一种方法是构建包含 Spark 和 SageMaker 阶段的 MLlib
    流水线。你可以在 [https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/sagemaker-spark)
    找到相关示例。
- en: Summary
  id: totrans-381
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Open source frameworks such as scikit-learn and TensorFlow have made it simple
    to write machine learning and deep learning code. They've become immensely popular
    in the developer community and for good reason. However, managing training and
    deployment infrastructure still requires a lot of effort and skills that data
    scientists and machine learning engineers typically do not possess. SageMaker
    simplifies the whole process. You can go quickly from experimentation to production,
    without ever worrying about infrastructure.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 开源框架，如 scikit-learn 和 TensorFlow，简化了机器学习和深度学习代码的编写。它们在开发者社区中非常受欢迎，原因显而易见。然而，管理训练和部署基础设施仍然需要大量努力和技能，而这些通常是数据科学家和机器学习工程师所不具备的。SageMaker
    简化了整个过程。你可以迅速从实验阶段过渡到生产环境，无需担心基础设施问题。
- en: In this chapter, you learned about the different frameworks available in SageMaker
    for machine learning and deep learning, as well as how to customize their containers.
    You also learned how to use script mode and local mode for fast iteration until
    you're ready to deploy in production. Finally, you ran several examples, including
    one that combines Apache Spark and SageMaker.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解了 SageMaker 中用于机器学习和深度学习的不同框架，以及如何自定义它们的容器。你还学习了如何使用脚本模式和本地模式进行快速迭代，直到你准备好在生产环境中部署。最后，你运行了几个示例，其中包括一个结合了
    Apache Spark 和 SageMaker 的示例。
- en: In the next chapter, you will learn how to use your own custom code on SageMaker,
    without having to rely on a built-in container.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，你将学习如何在 SageMaker 上使用你自己的自定义代码，而无需依赖内置容器。
