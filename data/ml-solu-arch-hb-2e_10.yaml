- en: '10'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Advanced ML Engineering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 高级机器学习工程
- en: Congratulations on making it so far! By now, you should have developed a good
    understanding of the core fundamental skills that an ML solutions architect needs
    in order to operate effectively across the ML lifecycle. In this chapter, we will
    delve into advanced ML concepts. Our focus will be on exploring a range of options
    for distributed model training for large models and datasets. Understanding the
    concept and techniques for distributed training is becoming increasingly important
    as all large-scale model training such as GPT will require distributed training
    architecture. Furthermore, we’ll delve into diverse technical approaches aimed
    at optimizing model inference latency. As model sizes grow larger, having a good
    grasp on how to optimize models for low-latency inference is becoming an essential
    skill in ML engineering. Lastly, we will close this chapter with a hands-on lab
    on distributed model training.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜你走到了这里！到目前为止，你应该已经对机器学习解决方案架构师在操作整个机器学习生命周期中所需的核心基本技能有了很好的理解。在本章中，我们将深入探讨高级机器学习概念。我们的重点是探索为大型模型和大数据集进行分布式模型训练的各种选项。随着所有大规模模型训练（如GPT）都需要分布式训练架构，理解分布式训练的概念和技术变得越来越重要。此外，我们还将深入研究旨在优化模型推理延迟的多种技术方法。随着模型尺寸的增大，掌握如何优化模型以实现低延迟推理成为机器学习工程中的一项基本技能。最后，我们将通过一个关于分布式模型训练的动手实验室来结束本章。
- en: 'Specifically, we will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将涵盖以下主题：
- en: Training large-scale models with distributed training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分布式训练训练大规模模型
- en: Achieving low-latency model inference
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现低延迟模型推理
- en: Hands-on lab – running distributed model training with PyTorch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动手实验室 – 使用PyTorch运行分布式模型训练
- en: Technical requirements
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You will need access to your AWS environment for the hands-on portion of this
    chapter. All the code samples are located at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要访问你的AWS环境来完成本章的动手部分。所有代码示例都位于[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10)。
- en: Training large-scale models with distributed training
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用分布式训练训练大规模模型
- en: 'As ML algorithms grow more complex and the volumes of available training data
    expand exponentially, model training times have become a major bottleneck. Single-device
    training on massive datasets or gigantic models like large language models is
    increasingly impractical given memory, time, and latency constraints. For example,
    state-of-the-art language models have rapidly scaled from millions of parameters
    a decade ago to hundreds of billions today. The following graph illustrates how
    language models have evolved in recent years:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习算法变得越来越复杂，可用的训练数据量呈指数级增长，模型训练时间已成为一个主要瓶颈。考虑到内存、时间和延迟的限制，在大型数据集或像大型语言模型这样的大型模型上进行单设备训练越来越不切实际。例如，最先进的语言模型参数量从十年前的数百万迅速增长到今天的数百亿。以下图表展示了近年来语言模型的发展情况：
- en: '![Figure 10.1 – The growth of language models  ](img/B20836_10_01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1 – 语言模型的发展](img/B20836_10_01.png)'
- en: 'Figure 10.1: The growth of language models'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1：语言模型的发展
- en: 'To overcome computational challenges, distributed training techniques have
    become critical to accelerate model development by parallelizing computation across
    clusters of GPUs or TPUs in the cloud. By sharding data and models across devices
    and nodes, distributed training enables the scaling out of computation to train
    modern massive models and data volumes in reasonable timeframes. There are two
    main types of distributed training: data parallelism and model parallelism. Before
    we get into the details of distributed training, let’s quickly review how a neural
    network trains again:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服计算挑战，分布式训练技术已成为通过在云中的GPU或TPU集群之间并行计算来加速模型开发的关键。通过在设备和节点之间分片数据和模型，分布式训练能够扩展计算以在合理的时间内训练现代大规模模型和数据量。分布式训练主要有两种类型：数据并行和模型并行。在我们深入了解分布式训练的细节之前，让我们快速回顾一下神经网络是如何训练的：
- en: '![Figure 10.2 – Deep neural network training ](img/B20836_10_02.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2 – 深度神经网络训练](img/B20836_10_02.png)'
- en: 'Figure 10.2: Deep neural network training'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2：深度神经网络训练
- en: The preceding diagram shows how an **artificial neural network** (**ANN**) trains.
    The training data is fed to the ANN in a forward pass. The loss (the difference
    between the predicted value and the true value) is calculated at the end of the
    forward pass, and the backward pass calculates the gradients for all the parameters.
    These parameters are updated with new values for the next step until the loss
    is minimized. In the following sections, we’ll look at distributed model training
    using data parallelism and model parallelism, two methods for scaling model training
    for large training datasets and large model sizes.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示展示了**人工神经网络**（**ANN**）的训练过程。训练数据通过正向传递被输入到ANN中。在正向传递结束时，计算损失（预测值与真实值之间的差异），然后反向传递计算所有参数的梯度。这些参数通过新的值进行更新，直到损失最小化。在接下来的章节中，我们将探讨使用数据并行和模型并行进行分布式模型训练，这两种方法用于扩展模型训练以适应大型训练数据集和大型模型尺寸。
- en: Distributed model training using data parallelism
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用数据并行进行分布式模型训练
- en: The data-parallel distributed training approach partitions a large training
    dataset into smaller subsets and trains each subset on different devices concurrently.
    This parallelization allows multiple training processes to run simultaneously
    on available compute resources, accelerating the overall training time. To leverage
    data-parallel training, the ML frameworks and algorithms used need to have support
    for distributed training. Frameworks like TensorFlow and PyTorch both provide
    modules and libraries for data parallelism training.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 数据并行分布式训练方法将大型训练数据集划分为更小的子集，并在不同的设备上同时训练每个子集。这种并行化允许多个训练过程在可用的计算资源上同时运行，从而加速整体训练时间。为了利用数据并行训练，所使用的机器学习框架和算法需要支持分布式训练。例如，TensorFlow和PyTorch都提供了用于数据并行训练的模块和库。
- en: 'As we discussed earlier, one key task in training **deep learning** (**DL**)
    models is to calculate the gradients concerning the loss function for every batch
    of the data, and then update the model parameters with gradient information to
    minimize the loss gradually. Instead of running the gradient calculations and
    parameter updates on a single device, the basic concept behind data-parallel distributed
    training is to run multiple training processes using the same algorithm in parallel,
    with each process using a different subset of the training dataset. The following
    diagram shows the main concept behind data parallelism in training:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，训练**深度学习**（**DL**）模型的一个关键任务是计算关于损失函数的梯度，针对数据中的每一批数据，然后使用梯度信息更新模型参数，以逐步最小化损失。而不是在单个设备上运行梯度计算和参数更新，数据并行分布式训练的基本概念是并行运行多个训练过程，使用相同的算法，每个过程使用训练数据集的不同子集。以下图示展示了训练中数据并行的基本概念：
- en: '![Figure 10.3 – Data parallelism concept ](img/B20836_10_03.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图10.3 – 数据并行概念](img/B20836_10_03.png)'
- en: 'Figure 10.3: Data parallelism concept'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3：数据并行概念
- en: 'As you can see, there are three nodes in a cluster participating in a distributed
    data-parallel training job, with each node having two devices. The partial gradients
    that are calculated by each device are represented by w0 ~ w5 for each of the
    devices on the nodes, while W is the value for a global parameter for the model.
    Specifically, data-parallel distributed training has the following main steps:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在分布式数据并行训练作业中，集群中有三个节点参与，每个节点有两个设备。每个设备计算出的局部梯度由节点上每个设备的w0 ~ w5表示，而W是模型的全局参数值。具体来说，数据并行分布式训练有以下几个主要步骤：
- en: Each device (CPU or GPU) on every node loads a copy of the same algorithm and
    a subset of the training data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个节点上的每个设备（CPU或GPU）都加载了相同的算法和训练数据的一个子集。
- en: Each device runs a training loop to calculate the gradients (w0~w5) to optimize
    its loss function and exchange the gradients with other devices in the cluster
    at each training step.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个设备运行一个训练循环来计算梯度（w0~w5），以优化其损失函数，并在每个训练步骤与其他设备在集群中交换梯度。
- en: The gradients from all the devices are aggregated and the common model parameters
    (W) are calculated using these aggregated gradients.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有设备的梯度汇总，并使用这些汇总梯度计算公共模型参数（W）。
- en: Each device pulls down the newly calculated common model parameters (W) and
    continues with the next step of model training.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个设备拉取新计算出的公共模型参数（W），并继续进行模型训练的下一步。
- en: '*Steps 2* to *4* are repeated until the model training is completed.'
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*步骤2*到*4*会重复进行，直到模型训练完成。'
- en: 'In a distributed training setting, efficiently exchanging gradients and parameters
    across processes is one of the most important aspects of ML system engineering
    design. Several distributed training topologies have been developed over the years
    to optimize communications across different training processes. In this chapter,
    we will discuss two of the most widely adopted topologies for data-parallel distributed
    training: **parameter server** and **AllReduce**.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式训练环境中，高效地在进程间交换梯度和参数是机器学习系统工程设计最重要的方面之一。多年来，已经开发出几种分布式训练拓扑来优化不同训练进程间的通信。在本章中，我们将讨论数据并行分布式训练中最广泛采用的两种拓扑：**参数服务器**和**AllReduce**。
- en: Parameter server overview
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数服务器概述
- en: 'The **parameter server** (**PS**) is a topology built on the concept of server
    nodes and worker nodes. The worker nodes are responsible for running the training
    loops and calculating the gradients, while the server nodes are responsible for
    aggregating the gradients and calculating the globally shared parameters. The
    following diagram shows the architecture of a PS:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '**参数服务器**（**PS**）是基于服务器节点和工作节点概念构建的拓扑。工作节点负责运行训练循环和计算梯度，而服务器节点负责聚合梯度和计算全局共享参数。以下图显示了PS的架构：'
- en: '![Figure 10.4 – Parameter server architecture ](img/B20836_10_04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图10.4 – 参数服务器架构](img/B20836_10_04.png)'
- en: 'Figure 10.4: Parameter server architecture'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4：参数服务器架构
- en: Here, the server node is called the PS, and is usually implemented as a key-value
    or vector store for storing gradients and parameters. As the number of model parameters
    to manage can become very large, there can also be multiple server nodes for managing
    the global parameters and gradient aggregations. In a multi-parameter server configuration,
    there is also a server manager that manages and coordinates all the server nodes
    to ensure consistency.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，服务器节点被称为PS，通常实现为一个键值或向量存储，用于存储梯度和参数。由于要管理的模型参数数量可能非常大，因此也可以有多个服务器节点来管理全局参数和梯度聚合。在多参数服务器配置中，还有一个服务器管理器，它管理和协调所有服务器节点以确保一致性。
- en: In this architecture, the worker nodes only communicate with the PS nodes to
    exchange gradients and parameters, and not with each other. In a multi-server
    node environment, each server node also communicates with every other server node
    to replicate the parameters for reliability and scalability. The gradients and
    parameters are exchanged so that updates can be implemented synchronously and
    asynchronously. The synchronous gradient update strategy blocks the devices from
    processing the next mini-batch of data until the gradients from all the devices
    have been synchronized. This means that each update has to wait for the slowest
    device to complete. This can slow down training and make the training process
    less robust in terms of device failure.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个架构中，工作节点仅与PS节点通信以交换梯度和参数，而不与彼此通信。在多服务器节点环境中，每个服务器节点也会与其他所有服务器节点通信，以复制参数以实现可靠性和可扩展性。梯度和参数的交换是为了同步和异步地实施更新。同步梯度更新策略阻止设备在所有设备的梯度同步完成之前处理下一个小批量数据。这意味着每个更新都必须等待最慢的设备完成。这可能会减慢训练速度，并使训练过程在设备故障方面变得不那么稳健。
- en: On the positive side, synchronous updates do not have to worry about stale gradients,
    which can lead to higher model accuracy. Asynchronous updates do not need to wait
    for all the devices to be synchronized before processing the next mini-batch of
    data, though this might lead to reduced accuracy.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 优点方面，同步更新不必担心过时的梯度，这可能导致更高的模型精度。虽然异步更新不需要在处理下一个小批量数据之前等待所有设备同步，但这可能会导致精度降低。
- en: The main limitation of this approach is that the PS can become a communication
    bottleneck, particularly for large models with billions or trillions of parameters.
    As the model size grows, the amount of data that needs to be transmitted between
    the workers and the PS increases significantly, leading to potential communication
    overhead and bandwidth constraints.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要局限性是PS可能成为通信瓶颈，尤其是对于具有数十亿或数万亿参数的大型模型。随着模型规模的增加，工作节点和PS之间需要传输的数据量显著增加，导致潜在的通信开销和带宽限制。
- en: Additionally, as the number of worker nodes increases, the PS needs to handle
    an increasing number of gradient updates and parameter distributions, which can
    become a scalability challenge. This centralized architecture can limit the overall
    throughput and efficiency of the distributed training process, especially when
    the number of workers becomes very large. Furthermore, slower or underperforming
    worker nodes can slow down the entire training process, as the PS must wait for
    all gradients before updating the parameters.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，随着工作节点数量的增加，PS需要处理越来越多的梯度更新和参数分布，这可能会成为可扩展性的挑战。这种集中式架构可能会限制分布式训练过程的整体吞吐量和效率，尤其是在工作节点数量非常大时。此外，较慢或表现不佳的工作节点可能会减慢整个训练过程，因为PS必须等待所有梯度更新完毕后才能更新参数。
- en: Implementing the PS in frameworks
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在框架中实现PS
- en: 'PS distributed training is natively supported by several DL frameworks, including
    TensorFlow. Specifically, TensorFlow supports PS-based distributed training natively
    with its `ParameterServerStrategy` API. The following code sample shows how to
    instantiate the `ParameterServerStrategy` API for TensorFlow:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PS分布式训练被多个深度学习框架原生支持，包括TensorFlow。具体来说，TensorFlow通过其`ParameterServerStrategy`
    API原生支持基于PS的分布式训练。以下代码示例展示了如何为TensorFlow实例化`ParameterServerStrategy` API：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this code sample, the `cluster_resolver` parameter helps discover and resolve
    the IP addresses of workers.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码示例中，`cluster_resolver`参数有助于发现和解析工作节点的IP地址。
- en: '`ParameterServerStrategy` can be used directly with the `model.fit()` function
    of Keras or a custom training loop by wrapping the model with the `strategy.scope()`
    syntax. See the following sample syntax on how to use `scope()` to wrap a model
    for distributed training:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '`ParameterServerStrategy`可以直接与Keras的`model.fit()`函数或通过使用`strategy.scope()`语法包装模型的自定义训练循环一起使用。以下是如何使用`scope()`包装模型以进行分布式训练的示例语法：'
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: In addition to a PS implementation, which is natively supported within DL libraries,
    there are also general-purpose PS training frameworks, such as BytePS from ByteDance
    and Herring from Amazon, which work with different DL frameworks. SageMaker uses
    Herring under the hood for data-parallel distributed training through its SageMaker
    distributed training library.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 除了在深度学习库中原生支持的PS实现之外，还有通用PS训练框架，例如来自字节跳动的BytePS和来自亚马逊的Herring，它们可以与不同的深度学习框架协同工作。SageMaker通过其SageMaker分布式训练库使用Herring进行数据并行分布式训练。
- en: One of the shortcomings of the PS strategy is the inefficient use of network
    bandwidth. The Herring library addresses this shortcoming by combining AWS **Elastic
    Fabric Adapter** (**EFA**) and the parameter sharding technique, which makes use
    of network bandwidth to achieve faster distributed training. EFA takes advantage
    of cloud resources and their characteristics, such as multi-path backbones, to
    improve network communication efficiency. You can find out more about Herring
    at [https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud](https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PS策略的一个缺点是网络带宽的效率低下。Herring库通过结合AWS **弹性织物适配器**（**EFA**）和参数分片技术来解决这一缺点，利用网络带宽实现更快的分布式训练。EFA利用云资源及其特性，如多路径骨干，以提高网络通信效率。您可以在[https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud](https://www.amazon.science/publications/herring-rethinking-the-parameter-server-at-scale-for-the-cloud)了解更多关于Herring的信息。
- en: AllReduce overview
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AllReduce概述
- en: While the PS architecture is easy to understand and set up, it does come with
    several challenges. For example, the PS architecture requires additional nodes
    for the PSs, and it is also hard to determine the right ratio between server nodes
    and worker nodes to ensure the server nodes do not become bottlenecks.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PS架构易于理解和设置，但它确实带来了一些挑战。例如，PS架构需要额外的节点来运行PS，而且很难确定服务器节点和工作节点之间的正确比例，以确保服务器节点不会成为瓶颈。
- en: 'The AllReduce topology tries to improve some of the limitations of PSs by eliminating
    the server nodes and distributing all the gradient aggregation and global parameter
    updates to all workers, hence it’s called **AllReduce**. The following diagram
    shows the topology of AllReduce:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: AllReduce拓扑试图通过消除服务器节点并将所有梯度聚合和全局参数更新分布到所有工作节点来改进PS的一些局限性，因此被称为**AllReduce**。以下图表显示了AllReduce的拓扑结构：
- en: '![Figure 10.5 – AllReduce architecture ](img/B20836_10_05.png)Figure 10.5:
    AllReduce architecture'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.5 – AllReduce架构](img/B20836_10_05.png)图10.5：AllReduce架构'
- en: In an AllReduce topology, each node sends gradients of parameters to all the
    other nodes at each training step. Then, each node aggregates the gradients and
    performs a reduce function (such as `average`, `sum`, or `max`) locally before
    calculating the new parameters using the next training step. Since every node
    needs to communicate with every other node, this results in a large number of
    networks of communication between the nodes, and duplicate compute and storage
    are required as every node has a copy of all the gradients.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在AllReduce拓扑中，每个节点在每个训练步骤将参数的梯度发送到所有其他节点。然后，每个节点本地聚合梯度并执行reduce函数（如`average`、`sum`或`max`），在下一个训练步骤中使用这些梯度计算新的参数。由于每个节点都需要与其他每个节点通信，这导致节点之间有大量的通信网络，并且每个节点都需要复制所有梯度，因此需要额外的计算和存储。
- en: 'A more efficient AllReduce architecture is Ring AllReduce. In this architecture,
    each node only sends some gradients to its next neighboring node, and each node
    is responsible for aggregating the gradients for the global parameters that it
    is assigned to calculate. This architecture greatly reduces the amount of network
    communication in a cluster and compute overhead, so it is more efficient for model
    training. The following diagram shows the Ring AllReduce architecture:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 环形AllReduce是一种更有效的AllReduce架构。在这个架构中，每个节点只向其下一个相邻节点发送一些梯度，并且每个节点负责聚合分配给其计算的全局参数的梯度。这种架构大大减少了集群中的网络通信量和计算开销，因此对于模型训练来说更加高效。以下图示展示了环形AllReduce架构：
- en: '![Figure 10.6 – Ring AllReduce ](img/B20836_10_06.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图10.6 – 环形AllReduce](img/B20836_10_06.png)'
- en: 'Figure 10.6: Ring AllReduce'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6：环形AllReduce
- en: Compared to the PS approach, the Ring AllReduce approach has better scalability
    as the number of workers increases. Since there is no centralized PS, the communication
    load is distributed among the workers, reducing the potential bottleneck. The
    Ring AllReduce approach also has a lower communication overhead, especially for
    large models with billions or trillions of parameters. Instead of sending individual
    gradients to a central server, the gradients are summed and passed along a ring
    topology, reducing the overall amount of data that needs to be transmitted.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 与PS方法相比，随着工作节点数量的增加，环形AllReduce方法具有更好的可扩展性。由于没有集中的PS，通信负载在各个工作节点之间分配，减少了潜在的瓶颈。环形AllReduce方法还具有更低的通信开销，特别是对于具有数十亿或数万亿参数的大型模型。不是将单个梯度发送到中央服务器，而是将梯度求和并通过环形拓扑传递，从而减少了需要传输的总数据量。
- en: Overall, the Ring AllReduce approach offers better scalability and efficiency
    for large-scale distributed training. It distributes the communication load among
    workers, reducing potential bottlenecks and synchronization overhead. However,
    the PS approach may still be suitable for smaller-scale distributed training scenarios
    or when fault tolerance is less of a concern.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，环形AllReduce方法为大规模分布式训练提供了更好的可扩展性和效率。它将通信负载分配给各个工作节点，减少了潜在的瓶颈和同步开销。然而，PS方法可能仍然适用于较小规模的分布式训练场景或当容错不是主要关注点时。
- en: Implementing AllReduce and Ring AllReduce in frameworks
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在框架中实现AllReduce和环形AllReduce
- en: The AllReduce and Ring AllReduce architectures are natively supported within
    multiple DL frameworks, including TensorFlow and PyTorch.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: AllReduce和环形AllReduce架构在多个深度学习框架中原生支持，包括TensorFlow和PyTorch。
- en: 'TensorFlow supports AllReduce distributed training across multiple GPUs on
    one machine with its `tf.distribute.MirroredStrategy` API. With this strategy,
    each GPU has a copy of the model, and all the model parameters are mirrored across
    different devices. An efficient AllReduce mechanism is used to keep these parameters
    in sync. The following code sample shows how to instantiate the `MirroredStrategy`
    API:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow通过其`tf.distribute.MirroredStrategy` API支持在单台机器上的多个GPU之间进行AllReduce分布式训练。使用此策略，每个GPU都有一个模型副本，并且所有模型参数都在不同设备之间进行镜像。使用高效的AllReduce机制来保持这些参数同步。以下代码示例展示了如何实例化`MirroredStrategy`
    API：
- en: '[PRE2]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'For multi-machine distributed training, TensorFlow uses the `tf.distribute.MultiWorkerMirroredStrategy`
    API. Similar to `MirroredStrategy`, `MultiWorkerMirroredStrategy` creates copies
    of all the parameters across all the devices on all the machines and synchronizes
    them with the AllReduce mechanism. The following code sample shows how to instantiate
    the `MultiWorkerMirroredStrategy` API:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 对于多机分布式训练，TensorFlow使用`tf.distribute.MultiWorkerMirroredStrategy` API。类似于`MirroredStrategy`，`MultiWorkerMirroredStrategy`在所有机器的所有设备上创建所有参数的副本，并通过AllReduce机制进行同步。以下代码示例展示了如何实例化`MultiWorkerMirroredStrategy`
    API：
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Similar to `ParameterServerStrategy`, `MirroredStrategy` and `MultiWorkerMirroredStrategy`
    can work with the `keras model.fit()` function or a custom training loop. To associate
    a model with a training strategy, you can use the same `strategy.scope()` syntax.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于`ParameterServerStrategy`、`MirroredStrategy`和`MultiWorkerMirroredStrategy`，它们可以与`keras
    model.fit()`函数或自定义训练循环一起工作。要将模型与训练策略关联，您可以使用相同的`strategy.scope()`语法。
- en: 'PyTorch also provides native support for AllReduce-based distributed training
    via its `torch.nn.DataParallel` and `torch.nn.parallel.DistributedDataParallel`
    APIs. The `torch.nn.DataParallel` API supports single-process multi-threading
    across GPUs on the same machine, while `torch.nn.parallel.DistributedDataParallel`
    supports multi-processing across GPUs and machines. The following code sample
    shows how to initiate a distributed training cluster and wrap a model for distributed
    training using the `DistributedDataParallel` API:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch还通过其`torch.nn.DataParallel`和`torch.nn.parallel.DistributedDataParallel`
    API提供了基于AllReduce的分布式训练的原生支持。`torch.nn.DataParallel` API支持同一台机器上GPU的单进程多线程，而`torch.nn.parallel.DistributedDataParallel`支持跨GPU和机器的多进程。以下代码示例展示了如何使用`DistributedDataParallel`
    API启动分布式训练集群并包装模型以进行分布式训练：
- en: '[PRE4]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Another popular implementation of the general-purpose Ring AllReduce architecture
    is **Horovod**, which was created by the engineers at Uber. Horovod works with
    multiple DL frameworks, including TensorFlow and PyTorch. You can find out more
    about Horovod at [https://github.com/horovod/horovod](https://github.com/horovod/horovod).
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种通用环AllReduce架构的流行实现是**Horovod**，由Uber的工程师创建。Horovod与多个深度学习框架一起工作，包括TensorFlow和PyTorch。您可以在[https://github.com/horovod/horovod](https://github.com/horovod/horovod)了解更多关于Horovod的信息。
- en: Distributed model training using model parallelism
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用模型并行进行分布式模型训练
- en: Compared to data parallelism, model parallelism is still relatively low in its
    adoption since most of the distributed training that happens today involves data
    parallelism that deals with large datasets. However, the applications of state-of-the-art
    big DL algorithms such as BERT, GPT, and T5 are driving the increasing adoption
    of model parallelism. The qualities of these models are known to increase with
    the model’s size, and these large NLP models require a large amount of memory
    to store the model’s states (which include the model’s parameters, optimizer states,
    and gradients) and memory for other overheads.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据并行相比，模型并行在其采用率上仍然相对较低，因为今天发生的分布式训练大多涉及处理大数据集的数据并行。然而，BERT、GPT和T5等最先进的大规模深度学习算法的应用正在推动模型并行采用率的增加。这些模型的品质已知会随着模型大小的增加而提高，这些大型自然语言处理模型需要大量的内存来存储模型的状态（包括模型的参数、优化器状态和梯度）以及其他开销的内存。
- en: 'As such, these models can no longer fit into the memory of a single GPU. While
    data parallelism helps solve the large dataset challenge, it cannot help with
    training large models due to its large memory size requirements. Model parallelism
    allows you to split a single large model across multiple devices so that the total
    memory across multiple devices is enough to hold a copy of the model. Model parallelism
    also allows for a larger batch size for model training as a result of the larger
    collective memory across multiple devices. There are two main approaches to splitting
    the model for parallel distributed training: splitting by layers and splitting
    by tensors. Next, let’s explore these two approaches in more detail.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这些模型已经无法适应单个GPU的内存。虽然数据并行有助于解决大数据集的挑战，但由于其庞大的内存需求，它无法帮助训练大型模型。模型并行允许您将单个大型模型分割到多个设备上，这样多个设备上的总内存就足以存储模型的一个副本。模型并行还由于多个设备上的更大集体内存而允许模型训练有更大的批量大小。分割模型进行并行分布式训练主要有两种方法：按层分割和按张量分割。接下来，让我们更详细地探讨这两种方法。
- en: Naïve model parallelism overview
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 原始模型并行概述
- en: 'As an ANN consists of many layers, one way to split the model is to distribute
    the layers across multiple devices. For example, if you have an 8-layer **multi-layer
    perceptron** (**MLP**)network and two GPUs (GPU0 and GPU1), you can simply place
    the first four layers in GPU0 and the last four layers in GPU1\. During training,
    the first four layers of the model are trained as you would normally train a model
    in a single device. When the first four layers are complete, the output from the
    fourth layer will be copied from GPU0 to GPU1, incurring a communication overhead.
    After getting the output from GPU0, GPU1 continues training layers five to eight.
    The following diagram illustrates splitting a model by layers across multiple
    devices:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于人工神经网络由许多层组成，一种将模型分割的方法是将层分布到多个设备上。例如，如果你有一个8层的**多层感知器**（**MLP**）网络和两个GPU（GPU0和GPU1），你可以简单地将前四层放在GPU0上，后四层放在GPU1上。在训练过程中，模型的前四层会像在单个设备上训练模型一样进行训练。当前四层完成时，第四层的输出将从GPU0复制到GPU1，产生通信开销。在获得GPU0的输出后，GPU1继续训练五到八层。以下图展示了通过层在多个设备上分割模型：
- en: '![Figure 10.7 – Naïve model parallelism ](img/B20836_10_07.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图10.7 – 简单模型并行](img/B20836_10_07.png)'
- en: 'Figure 10.7: Naïve model parallelism'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7：简单模型并行
- en: 'Implementing model parallelism by splitting requires knowledge of the training
    task. It is not a trivial task to design an efficient model parallelism strategy.
    Here are a few heuristics that could be helpful for the split-layer design:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 通过分割实现模型并行需要了解训练任务。设计一个有效的模型并行策略并非易事。以下是一些有助于分割层设计的启发式方法：
- en: Place neighboring layers on the same devices to minimize communication overhead.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将相邻层放置在同一设备上以最小化通信开销。
- en: Balance the workload between devices.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在设备之间平衡工作负载。
- en: Different layers have different compute and memory utilization properties.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的层具有不同的计算和内存利用率特性。
- en: Training an ANN model is inherently a sequential process, which means that the
    network layers are processed sequentially, while the backward process will only
    start when the forward process is completed. When you’re splitting layers across
    multiple devices, only the device currently processing the layers on it will be
    busy; the other devices will be idle, wasting compute resources, which results
    in a waste of hardware resources.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 训练人工神经网络模型本质上是一个顺序过程，这意味着网络层是顺序处理的，而反向过程只有在正向过程完成后才会开始。当你将层分割到多个设备上时，只有当前处理其上层的设备会忙碌；其他设备将处于空闲状态，浪费计算资源，这导致硬件资源的浪费。
- en: 'The following diagram illustrates the processing of sequences for the forward
    and backward passes for one batch of data:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了对于一批数据的前向和反向传递的序列处理：
- en: '![Figure 10.8 – Naïve model parallelism  ](img/B20836_10_08.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![图10.8 – 简单模型并行](img/B20836_10_08.png)'
- en: 'Figure 10.8: Naïve model parallelism'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8：简单模型并行
- en: In the preceding diagram, **F0**, **F1**, and **F2** are the forward passes
    on the different neural network layers on each device. **B2**, **B1**, and **B0**
    are the backward passes for the layers on each device. As you can see, when one
    of the devices is busy with either a forward pass or a backward pass, the other
    devices are idle.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，**F0**、**F1** 和 **F2** 是每个设备上不同神经网络层的正向传递。**B2**、**B1** 和 **B0** 是每个设备上层的反向传递。正如你所见，当一个设备忙于正向传递或反向传递时，其他设备处于空闲状态。
- en: Naïve model parallelism has the benefit of implementation simplicity, and it
    is suitable for models with a large number of layers. However, it has scalability
    challenges due to the sequential nature of layer execution. In addition, it may
    run into potential load imbalance issues if layers have varying computational
    requirements.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 简单模型并行具有实现简单的优点，并且适用于具有大量层的模型。然而，由于层执行的顺序性，它面临着可扩展性的挑战。此外，如果层有不同的计算需求，它可能会遇到潜在的负载不均衡问题。
- en: Next, let’s look at an approach (pipeline model parallelism) that can help increase
    resource utilization.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看一种可以增加资源利用率的解决方案（管道模型并行）。
- en: Pipeline model parallelism overview
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 管道模型并行概述
- en: 'To resolve the resource-idling issue, pipeline model parallelism can be implemented.
    This improves on naïve model parallelism so that different devices can work in
    parallel on the different stages of the training pipeline on a smaller chunk of
    data batch, commonly known as a micro-batch. The following diagram shows how pipeline
    model parallelism works:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决资源闲置问题，可以实现管道模型并行性。这改进了简单的模型并行性，使得不同的设备可以在更小的数据批次（通常称为微批次）的不同训练管道阶段上并行工作。以下图表展示了管道模型并行性的工作原理：
- en: '![Figure 10.9 – Pipeline model parallelism ](img/B20836_10_09.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图10.9 – 管道模型并行性](img/B20836_10_09.png)'
- en: 'Figure 10.9: Pipeline model parallelism'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9：管道模型并行性
- en: With pipeline model parallelism, instead of processing one batch of data through
    each full forward and backward pass, that one batch of data is broken down into
    smaller mini-batches. In the preceding diagram, after **Device 0** completes the
    forward pass for the first mini-batch, **Device 1** can start its forward pass
    on the output of the **Device 1** forward pass. Instead of waiting for **Device
    1** and **Device 2** to complete their forward passes and backward passes, **Device
    0** starts to process the next mini-batch of data. This scheduled pipeline allows
    for higher utilization of the hardware resources, resulting in faster model training.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用管道模型并行性，不是通过每个完整的正向和反向传递处理一个数据批次，而是将这个数据批次分解成更小的迷你批次。在前面的图表中，**设备0**完成第一个迷你批次的前向传递后，**设备1**可以开始对**设备1**前向传递的输出进行前向传递。而不是等待**设备1**和**设备2**完成它们的前向和反向传递，**设备0**开始处理下一个迷你批次的数据。这个预定的管道允许更高效地利用硬件资源，从而加快模型训练。
- en: 'There are other variations of pipeline parallelism. One example is interleaved
    parallelism, where a backward execution is prioritized whenever possible. This
    improves the utilization of devices for end-to-end model training. The following
    diagram shows how an interleaved pipeline works:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 管道并行性还有其他变体。一个例子是交错并行性，其中在可能的情况下优先执行反向执行。这提高了设备在端到端模型训练中的利用率。以下图表展示了交错管道的工作原理：
- en: '![Figure 10.10 – Interleaved pipeline ](img/B20836_10_10.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图10.10 – 交错管道](img/B20836_10_10.png)'
- en: 'Figure 10.10: Interleaved pipeline'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10：交错管道
- en: Pipeline model parallelism has been implemented in various frameworks and products
    such as the SageMaker distributed training library and DeepSpeed distributed training
    framework, which we will cover in greater detail in a later section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 管道模型并行性已在各种框架和产品中得到实现，例如SageMaker分布式训练库和DeepSpeed分布式训练框架，我们将在后面的章节中更详细地介绍。
- en: Next, let’s look at an overview of tensor parallelism, also known as tensor
    slicing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看一下张量并行性的概述，也称为张量切片。
- en: Tensor parallelism/tensor slicing overview
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 张量并行性/张量切片概述
- en: As we mentioned earlier, tensor parallelism is another approach to split a large
    model to make it fit into memory. Before we dive into this, let’s quickly review
    what a tensor is and how it is processed by an ANN.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，张量并行性是将大型模型分割以适应内存的另一种方法。在我们深入探讨之前，让我们快速回顾一下什么是张量以及它是如何被人工神经网络处理的。
- en: A **tensor** is a multi-dimensional matrix of a single data type such as a 32-bit
    floating-point or 8-bit integer. In the forward pass of neural network training,
    a dot product is used on the input tensor and weight matrix tensors (the connections
    between the input tensors and the neurons in the hidden layer). You can find out
    more about dot products at [https://en.wikipedia.org/wiki/Dot_product](https://en.wikipedia.org/wiki/Dot_product).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**张量**是一个单数据类型的多维矩阵，例如32位浮点数或8位整数。在神经网络训练的前向传递中，输入张量和权重矩阵张量（输入张量与隐藏层中的神经元之间的连接）使用点积。您可以在[https://en.wikipedia.org/wiki/Dot_product](https://en.wikipedia.org/wiki/Dot_product)了解更多关于点积的信息。'
- en: 'The following diagram illustrates a dot product between the input vector and
    the weight matrix:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表展示了输入向量和权重矩阵之间的点积：
- en: '![Figure 10.11 – Matrix calculation  ](img/B20836_10_11.png)'
  id: totrans-98
  prefs: []
  type: TYPE_IMG
  zh: '![图10.11 – 矩阵计算](img/B20836_10_11.png)'
- en: 'Figure 10.11: Matrix calculation'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11：矩阵计算
- en: In this matrix calculation, you get an output vector of **[5,11,17]**. If there
    is a single device for dot product calculation, three separate calculations will
    be performed sequentially to get the output vector.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个矩阵计算中，你得到一个输出向量**[5,11,17]**。如果只有一个设备用于点积计算，将依次进行三个单独的计算以获得输出向量。
- en: 'But what if we break up the single weights matrix into three vectors and use
    a dot product separately? This can be seen in the following diagram:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们把单个权重矩阵拆分成三个向量，并分别使用点积呢？这可以在以下图中看到：
- en: '![Figure 10.12 – Splitting the matrix calculation ](img/B20836_10_12.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.12 – 分割矩阵计算](img/B20836_10_12.png)'
- en: 'Figure 10.12: Splitting the matrix calculation'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.12：分割矩阵计算
- en: As you can see, you would get three separate values that are the same as the
    individual values in the output vector in the preceding diagram. If there are
    three separate devices for performing dot product calculations, we can perform
    these three dot product calculations in parallel and combine the values into a
    single vector at the end if needed. This is the basic concept of how tensor parallelism
    works. With tensor parallelism, each device works independently without the need
    for any communication until the end, which is when the results need to be synchronized.
    This strategy allows for faster tensor processing as multiple devices can work
    in parallel to reduce the training time and increase the utilization of computing
    devices.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您将得到三个单独的值，这些值与前面图中输出向量中的单个值相同。如果有三个单独的设备用于执行点积计算，我们可以在需要时并行执行这三个点积计算，并将值组合成一个向量。这是张量并行基本工作原理。使用张量并行，每个设备独立工作，直到需要同步结果时才需要通信。这种策略允许更快的张量处理，因为多个设备可以并行工作以减少训练时间并提高计算设备的利用率。
- en: Implementing model-parallel training
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 实现模型并行训练
- en: To implement model parallelism, you can manually design the parallelism strategy
    by deciding how to split the layers and tensors, as well as their placements,
    across different devices and nodes. However, it is not trivial to do this efficiently,
    especially for large clusters. To make the model parallelism implementation easier,
    several model parallelism library packages have been developed. In this section,
    we’ll take a closer look at some of these libraries. Note that the frameworks
    we will discuss can support both data parallelism and model parallelism and that
    both techniques are often used together to train large models with large training
    datasets.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现模型并行，你可以通过决定如何在不同设备和节点上分割层和张量以及它们的放置来手动设计并行策略。然而，这样做并不简单，尤其是在大型集群中。为了使模型并行实现更容易，已经开发了几个模型并行库包。在本节中，我们将更详细地探讨一些这些库。请注意，我们将讨论的框架可以支持数据并行和模型并行，并且这两种技术通常一起使用来训练具有大量训练数据集的大型模型。
- en: Megatron-LM
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Megatron-LM
- en: Megatron-LM is an open-source distributed training framework developed by Nvidia.
    It supports data parallelism, tensor parallelism, and pipeline model parallelism,
    as well as a combination of all three for extreme-scale model training.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Megatron-LM 是由 Nvidia 开发的开源分布式训练框架。它支持数据并行、张量并行和流水线模型并行，以及所有三种技术的组合，用于极端规模模型训练。
- en: 'Megatron-LM implements micro-batch-based pipeline model parallelism to improve
    device utilization. It also implements periodic pipeline flushes to ensure that
    the optimizer steps are synchronized across devices. Two different pipeline schedules
    are supported by Megatron-LM, as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Megatron-LM 实现了基于微批次的流水线模型并行来提高设备利用率。它还实现了周期性流水线刷新，以确保优化器步骤在设备之间同步。Megatron-LM
    支持两种不同的流水线调度，如下所示：
- en: The default schedule works by completing the forward pass for all micro-batches
    first, before starting the backward pass for all the batches.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认调度通过首先完成所有微批次的正向传递，然后再开始所有批次的反向传递来工作。
- en: The interleaved stage schedule works by running multiple different subsets of
    layers on a single device, instead of running just a single continuous set of
    layers. This can further improve the utilization of devices and reduce idle time.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交错阶段调度通过在单个设备上运行多个不同的层子集来工作，而不是运行单个连续的层集。这可以进一步提高设备的利用率并减少空闲时间。
- en: 'Megatron-LM implements a specific tensor parallelism strategy for transformer-based
    models. A transformer consists mainly of self-attention blocks, followed by a
    two-layer MLP. For the MLP portion, Megatron-LM splits the weight matrix by columns.
    The matrices for the self-attention heads are also partitioned by columns. The
    following diagram shows how the different parts of the transformers can be split:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Megatron-LM 为基于变换器的模型实现了一种特定的张量并行策略。变换器主要由自注意力块组成，后面跟着一个两层 MLP。对于 MLP 部分，Megatron-LM
    按列分割权重矩阵。自注意力头的矩阵也按列分割。以下图表显示了变换器的不同部分如何分割：
- en: '![Figure 10.13 – Tensor parallelism for transformers ](img/B20836_10_13.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图 10.13 – 变换器的张量并行](img/B20836_10_13.png)'
- en: 'Figure 10.13: Tensor parallelism for transformers'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.13：变换器的张量并行
- en: Using data parallelism, pipeline model parallelism, and tensor parallelism together,
    Megatron-LM can be used to train extremely large transformer-based models (with
    a trillion parameters) scaled across thousands of GPUs.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 使用数据并行、流水线模型并行和张量并行结合，Megatron-LM 可以用于训练极其大的基于变换器的模型（具有万亿参数），并扩展到数千个 GPU 上。
- en: 'Training using Megatron-LM involves the following key steps:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Megatron-LM 训练涉及以下关键步骤：
- en: Initializing the Megatron library using the `initialize_megatron()` function.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `initialize_megatron()` 函数初始化 Megatron 库。
- en: Setting up the Megatron model optimizer using the `setup_model_and_optimizer()`
    function by wrapping the original model.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `setup_model_and_optimizer()` 函数通过包装原始模型来设置 Megatron 模型优化器。
- en: Training the model using the `train()` function, which takes the Megatron model
    and optimizer as input.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train()` 函数训练模型，该函数以 Megatron 模型和优化器作为输入。
- en: Megatron-LM has been used for many large-model training projects, such as BERT,
    GPT, and the Biomedical domain language model. Its scalable architecture can be
    used to train models with trillions of parameters.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Megatron-LM 已被用于许多大型模型训练项目，如 BERT、GPT 和生物医学领域语言模型。其可扩展的架构可用于训练具有万亿参数的模型。
- en: DeepSpeed
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: DeepSpeed
- en: DeepSpeed is an open-source distributed training framework developed by Microsoft.
    Similar to Megatron-LM, DeepSpeed also supports tensor-slicing (another name for
    splitting tensors) parallelism, pipeline parallelism, and data parallelism.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed 是由微软开发的开源分布式训练框架。类似于 Megatron-LM，DeepSpeed 也支持张量切片（张量分割的另一种称呼）并行、流水线并行和数据并行。
- en: DeepSpeed implements micro-batch-based pipeline model parallelism, where a batch
    is broken into micro-batches to be processed by different devices in parallel.
    Specifically, DeepSpeed implements interleaved pipeline parallelism to optimize
    resource efficiency and utilization. Similar to Megatron-LM, DeepSpeed can use
    data parallelism, pipeline model parallelism, and tensor parallelism together
    to train extremely large deep neural networks. This is also known as DeepSpeed
    3D parallelism.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed 实现了基于微批次的流水线模型并行，其中将批次拆分为微批次，由不同的设备并行处理。具体来说，DeepSpeed 实现了交错流水线并行以优化资源效率和利用率。类似于
    Megatron-LM，DeepSpeed 可以结合使用数据并行、流水线模型并行和张量并行来训练极其大的深度神经网络。这被称为 DeepSpeed 3D 并行。
- en: One core capability of the DeepSpeed framework is its **Zero Redundancy Optimizer**
    (**ZeRO**). ZeRO is capable of managing memory efficiently by partitioning parameters,
    optimizer states, and gradients across devices instead of keeping a copy on all
    devices. The partitions are brought together at runtime when needed. This allows
    ZeRO to reduce the memory footprint by eight times compared to regular data parallelism
    techniques. ZeRO is also capable of using CPU and GPU memory together to train
    large models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed 框架的一个核心能力是其 **零冗余优化器**（**ZeRO**）。ZeRO 通过在设备间分割参数、优化器状态和梯度来有效地管理内存，而不是在所有设备上保留副本。当需要时，在运行时将分区合并在一起。这使得
    ZeRO 相比常规数据并行技术可以减少八倍的内存占用。ZeRO 还能够结合使用 CPU 和 GPU 内存来训练大型模型。
- en: The attention-based mechanism is widely adopted in DL models, such as the transformer
    model, to address text and image inputs. However, its ability to address long
    input sequences is limited due to its large memory and compute requirements. DeepSpeed
    helps alleviate this issue with its implementation of a sparse attention kernel
    – a technology that reduces the compute and memory requirements of attention computation
    via block-sparse computation.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 基于注意力的机制在深度学习模型中得到了广泛应用，例如 transformer 模型，用于处理文本和图像输入。然而，由于其大内存和计算需求，它处理长输入序列的能力有限。DeepSpeed
    通过其实施稀疏注意力内核来帮助缓解这个问题——这是一种通过块稀疏计算减少注意力计算的计算和内存需求的技术。
- en: One major bottleneck in large-scale distributed training is the communication
    overhead due to gradient sharing and updates. Communication compression, such
    as 1-bit compression, has been adopted as an effective mechanism to reduce the
    communication overhead. DeepSpeed has an implementation of a 1-bit Adam optimizer,
    which can reduce the communication overhead by up to five times to improve the
    training speed. 1-bit compression works by representing each number using 1 bit,
    combined with error compensation, which remembers the error during gradient compression
    and adds the error back to the next step to compensate for the error.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模分布式训练中，一个主要瓶颈是由于梯度共享和更新导致的通信开销。通信压缩，如 1-bit 压缩，已被采用作为一种有效的机制来减少通信开销。DeepSpeed
    实现了一个 1-bit Adam 优化器，可以将通信开销减少多达五倍，从而提高训练速度。1-bit 压缩通过使用 1 位表示每个数字，并结合误差补偿来实现，误差补偿在梯度压缩期间记住误差，并在下一步中将误差添加回去以补偿误差。
- en: 'To use DeepSpeed, you need to modify your training script. The following steps
    explain the main changes you need to make to a training script to run distributed
    training:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 DeepSpeed，您需要修改您的训练脚本。以下步骤解释了您需要对训练脚本进行的主要更改以运行分布式训练：
- en: Use the `deepspeed.initialize()` function to wrap the model and return a DeepSpeed
    model engine. This model engine will be used to run a forward pass and a backward
    pass.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `deepspeed.initialize()` 函数包装模型并返回一个 DeepSpeed 模型引擎。这个模型引擎将用于执行正向传递和反向传递。
- en: Use the returned DeepSpeed model engine to run the forward pass, backward pass,
    and step function to update the model parameters.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用返回的 DeepSpeed 模型引擎来运行正向传递、反向传递，并使用步进函数更新模型参数。
- en: DeepSpeed primarily supports the PyTorch framework and requires minor code changes
    to adopt model training using PyTorch. DeepSpeed has been used for training models
    with hundreds of billions of parameters and has delivered some of the fastest
    model training times. You can find out more about DeepSpeed at [https://www.deepspeed.ai](https://www.deepspeed.ai).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: DeepSpeed 主要支持 PyTorch 框架，并需要少量代码更改以采用 PyTorch 的模型训练。DeepSpeed 已被用于训练具有数百亿参数的模型，并实现了最快的模型训练时间之一。您可以在
    [https://www.deepspeed.ai](https://www.deepspeed.ai) 上了解更多关于 DeepSpeed 的信息。
- en: SageMaker distributed training library
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: SageMaker 分布式训练库
- en: Amazon’s **SageMaker distributed training** (**SMD**) library is part of the
    Amazon SageMaker service offering. SMD supports data parallelism (by using Herring
    under the hood) and interleaved pipeline model parallelism. Unlike DeepSpeed and
    Megatron-LM, where you need to manually decide on your model partitions, **SageMaker
    Model Parallel** (**SMP**) has a feature for automated model splitting support.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊的 **SageMaker 分布式训练**（**SMD**）库是 Amazon SageMaker 服务的一部分。SMD 支持数据并行（通过底层使用
    Herring）和交错管道模型并行。与 DeepSpeed 和 Megatron-LM 不同，您需要手动决定模型分区，**SageMaker 模型并行**（**SMP**）具有自动模型拆分支持的功能。
- en: This automated model-splitting feature of SMP balances memory and communication
    constraints between devices to optimize performance. Automated model splitting
    takes place during the first training step, where a version of a model is constructed
    in CPU memory. The graph is analyzed, a partition decision is made, and different
    model partitions are loaded into different GPUs. The partition software performs
    framework-specific analysis for TensorFlow and PyTorch to determine the partition
    decision. It considers graph structures such as variable/parameter sharing, parameter
    sizes, and constraints to balance the number of variables and the number of operations
    for each device to come up with split decisions.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: SMP 的这种自动模型拆分功能通过平衡设备间的内存和通信约束来优化性能。自动模型拆分发生在第一次训练步骤中，此时在 CPU 内存中构建模型的一个版本。分析图结构，做出分区决策，并将不同的模型分区加载到不同的
    GPU 上。分区软件为 TensorFlow 和 PyTorch 执行框架特定的分析，以确定分区决策。它考虑了诸如变量/参数共享、参数大小和约束等图结构，以平衡每个设备上的变量数量和操作数量，从而得出拆分决策。
- en: 'To use the SMD library, you need to make some changes to your existing training
    scripts and create SageMaker training jobs. There are different instructions for
    TensorFlow and PyTorch. The following are examples for the PyTorch framework:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 SMD 库，您需要对现有的训练脚本进行一些修改并创建 SageMaker 训练作业。TensorFlow 和 PyTorch 有不同的说明。以下是对
    PyTorch 框架的示例：
- en: 'Modify the PyTorch training script:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改 PyTorch 训练脚本：
- en: Call `smp.init()` to initialize the library.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 调用 `smp.init()` 来初始化库。
- en: Wrap the model with `smp.DistributedModel()`.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `smp.DistributedModel()` 包装模型。
- en: Wrap the optimizer with `smp.DistributedOptimizer()`.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `smp.DistributedOptimizer()` 包装优化器。
- en: Restrict each process to its own device through `torch.cuda.set_device(smp.local_rank())`.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过 `torch.cuda.set_device(smp.local_rank())` 限制每个进程使用其自己的设备。
- en: Use the wrapped model to perform a forward pass and a backward pass.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用包装后的模型执行正向传递和反向传递。
- en: Use the distributed optimizer to update the parameters.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分布式优化器更新参数。
- en: Create a SageMaker training job using SageMaker PyTorch Estimator and enable
    SMP distributed training.
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 SageMaker PyTorch Estimator 创建 SageMaker 训练作业并启用 SMP 分布式训练。
- en: FairScale
  id: totrans-143
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: FairScale
- en: FairScale is a distributed training framework developed by Facebook AI Research
    (FAIR). It is built on top of the popular PyTorch deep learning library and provides
    a set of utilities and APIs for efficient distributed training.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: FairScale 是由 Facebook AI Research (FAIR) 开发的一种分布式训练框架。它建立在流行的 PyTorch 深度学习库之上，并提供了一套用于高效分布式训练的实用工具和
    API。
- en: FairScale supports various distributed training paradigms, including data parallelism,
    model parallelism, and a combination of both. FairScale provides efficient implementations
    of these techniques, along with optimizations and techniques to reduce communication
    overhead and improve scalability.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: FairScale 支持各种分布式训练范式，包括数据并行、模型并行以及两者的结合。FairScale 提供了这些技术的有效实现，以及减少通信开销和提高可扩展性的优化和技巧。
- en: One of the key features of FairScale is its support for various model parallel
    strategies, such as tensor parallelism, pipeline parallelism, and hybrid parallelism.
    These strategies allow users to distribute large models across multiple accelerators
    in different ways, enabling efficient utilization of available hardware resources
    and better scaling for massive models.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: FairScale 的一个关键特性是支持各种模型并行策略，例如张量并行、流水线并行和混合并行。这些策略允许用户以不同的方式将大型模型分布到多个加速器上，从而实现硬件资源的有效利用和大规模模型的更好扩展。
- en: In addition to distributed training capabilities, FairScale also offers tools
    for optimizing memory usage, such as activation checkpointing, gradient checkpointing,
    and mixed precision training. These techniques help reduce the memory footprint
    of large models, allowing users to train models that would otherwise exceed the
    available memory on a single device.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 除了分布式训练功能外，FairScale 还提供优化内存使用的工具，如激活检查点、梯度检查点和混合精度训练。这些技术有助于减少大型模型的内存占用，使用户能够训练那些否则会超出单个设备可用内存的模型。
- en: FairScale is designed to be user-friendly and easy to integrate into existing
    PyTorch codebases. It provides a high-level API that abstracts away many of the
    complexities of distributed training, allowing users to focus on model development
    and experimentation rather than low-level implementation details. To use FairScale,
    you simply install the package using `pip install fairscale` and import the library
    into your training script using `import fairscale`. You can then use its various
    supported features for distributed data and model-parallel training.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: FairScale被设计成用户友好且易于集成到现有的PyTorch代码库中。它提供了一个高级API，抽象掉了分布式训练的许多复杂性，使用户能够专注于模型开发和实验，而不是低级实现细节。要使用FairScale，您只需使用`pip
    install fairscale`安装包，并在训练脚本中使用`import fairscale`导入库。然后，您可以使用其各种支持的功能进行分布式数据和模型并行训练。
- en: While distributed model training allows us to train extremely large models,
    running inferences on these large models can result in high latency due to the
    size of the models and other technological constraints. Next, let’s explore the
    various techniques we can use to achieve low-latency inference.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管分布式模型训练使我们能够训练极其大的模型，但由于模型的大小和其他技术限制，在这些大型模型上运行推理可能会导致高延迟。接下来，让我们探讨我们可以使用的各种技术，以实现低延迟推理。
- en: Achieving low-latency model inference
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实现低延迟模型推理
- en: As ML models continue to grow and get deployed to different hardware devices,
    latency can become an issue for certain inference use cases that require low-latency
    and high-throughput inferences, such as real-time fraud detection.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随着机器学习模型持续增长并部署到不同的硬件设备上，对于需要低延迟和高吞吐量推理的某些推理用例，如实时欺诈检测，延迟可能成为一个问题。
- en: To reduce the overall model inference latency for a real-time application, there
    are different optimization considerations and techniques we can use, including
    model optimization, graph optimization, hardware acceleration, and inference engine
    optimization.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 为了降低实时应用的总体模型推理延迟，我们可以使用不同的优化考虑和技巧，包括模型优化、图优化、硬件加速和推理引擎优化。
- en: In this section, we will focus on model optimization, graph optimization, and
    hardware optimization. Before we get into these various topics, let’s first understand
    how model inference works, specifically for DL models, since that’s what most
    of the inference optimization processes focus on.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注模型优化、图优化和硬件优化。在我们深入探讨这些各种主题之前，让我们首先了解模型推理的工作原理，特别是对于深度学习模型，因为大多数推理优化过程都集中在这一点上。
- en: How model inference works and opportunities for optimization
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型推理的工作原理和优化机会
- en: As we discussed earlier in this book, DL models are constructed as computational
    graphs with nodes and edges, where the nodes represent the different operations
    and the edges represent the data flow. Examples of such operations include addition,
    matrix multiplication, activation (for example, Sigmoid and ReLU), and pooling.
    These operations perform computations on tensors as inputs and produce tensors
    as outputs. For example, the *c=matmul(a,b)* operation takes *a* and *b* as input
    tensors and produces *c* as the output tensor. Deep learning frameworks, such
    as TensorFlow and PyTorch, have built-in operators to support different operations.
    The implementation of an operator is also called a kernel.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在本书中之前讨论的那样，深度学习模型是由节点和边构成的计算图，其中节点代表不同的操作，边代表数据流。这类操作的例子包括加法、矩阵乘法、激活（例如Sigmoid和ReLU）以及池化。这些操作在张量上执行计算，并产生张量作为输出。例如，`*c=matmul(a,b)*`操作将输入张量`*a*`和`*b*`作为输入，并产生输出张量`*c*`。深度学习框架，如TensorFlow和PyTorch，内置了支持不同操作的算子。算子的实现也称为内核。
- en: During inference time for a trained model, the DL framework’s runtime will walk
    through the computational graph and invoke the appropriate kernels (such as add
    or Sigmoid) for each of the nodes in the graph. The kernel will take various inputs,
    such as the inference data samples, learned model parameters, and intermediate
    outputs, from the preceding operators and perform specific computations according
    to the data flow defined by the computational graph to produce the final predictions.
    The size of a trained model is mainly determined by the number of nodes in a graph,
    as well as the number of model parameters and their numerical precisions (for
    example, floating-point 32, floating-point 16, or integer 8).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型的推理时间期间，深度学习框架的运行时会遍历计算图并调用图中每个节点的适当内核（如加法或Sigmoid）。内核将从前面的操作符获取各种输入，例如推理数据样本、学习到的模型参数和中间输出，并根据计算图定义的数据流执行特定计算，以产生最终的预测。训练模型的大小主要取决于图中节点的数量，以及模型参数的数量和它们的数值精度（例如，浮点32、浮点16或整数8）。
- en: Different hardware providers such as Nvidia and Intel also provide hardware-specific
    implementations of kernels for common computational graph operations. cuDNN is
    the library from Nvidia for optimized kernel implementations for their GPU devices,
    while MKL-DNN is the library from Intel for optimized kernel implementations for
    Intel chips. These hardware-specific implementations take advantage of the unique
    capabilities of the underlying hardware architecture. They can perform better
    than the kernels that are implemented by the DL framework implementation since
    the framework implementations are hardware agnostic.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的硬件供应商，如Nvidia和Intel，也提供了针对常见计算图操作的硬件特定内核实现。cuDNN是Nvidia为其GPU设备提供的优化内核实现库，而MKL-DNN是Intel为其Intel芯片提供的优化内核实现库。这些硬件特定实现利用了底层硬件架构的独特能力。它们的表现可以优于由深度学习框架实现实现的内核，因为框架实现是硬件无关的。
- en: Now we understand how inference works, let’s explore some common optimization
    techniques we can use to improve model latency.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了推理的工作原理，让我们来探讨一些常见的优化技术，这些技术可以帮助我们提高模型延迟。
- en: Hardware acceleration
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬件加速
- en: Different hardware produces varying inference latency performance for different
    ML models. The list of common hardware for model inference includes the CPU, GPU,
    **application-specific integrated circuit** (**ASIC**), **field-programmable gate
    array** (**FPGA**), and edge hardware (such as Nvidia Jetson Nano). In this section,
    we will review the core architecture characteristics for some of these pieces
    of hardware and how their designs help with model inference acceleration. It is
    worth noting that while this section focuses on inference, some of the hardware
    is also suitable for training acceleration.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的硬件为不同的机器学习模型产生不同的推理延迟性能。常见的用于模型推理的硬件列表包括CPU、GPU、**专用集成电路**（**ASIC**）、**现场可编程门阵列**（**FPGA**）和边缘硬件（如Nvidia
    Jetson Nano）。在本节中，我们将回顾一些这些硬件的核心架构特性以及它们的设计如何帮助加速模型推理。值得注意的是，尽管本节侧重于推理，但一些硬件也适用于训练加速。
- en: Central processing units (CPUs)
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 中央处理单元（CPUs）
- en: 'A CPU is a general-purpose chip for running computer programs. It consists
    of four main building blocks:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: CPU是用于运行计算机程序的一般用途芯片。它由四个主要构建块组成：
- en: The control unit is the brain of the CPU that directs the operations of the
    CPU; that is, it instructs other components such as memory.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制单元是CPU的大脑，它指导CPU的操作；也就是说，它指导其他组件，如内存。
- en: The **arithmetic logic unit** (**ALU**) is the basic unit that performs arithmetic
    and logical operations, such as addition and subtraction, on the input data.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算术逻辑单元**（**ALU**）是执行算术和逻辑运算的基本单元，例如加法和减法。'
- en: The address generation unit is used for calculating an address to access memory.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地址生成单元用于计算访问内存的地址。
- en: Memory management, which is used for all memory components such as the main
    memory and the local cache. A CPU can also be made up of multiple cores, with
    each core having a control unit and ALUs.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存管理，用于所有内存组件，如主存储器和本地缓存。CPU也可以由多个核心组成，每个核心都有一个控制单元和ALU。
- en: The degree of parallel executions in a CPU mainly depends on how many cores
    it has. Each core normally runs a single thread at a time, except for hyper-threading
    (a proprietary simultaneous multi-threading implementation from Intel). The more
    cores it has, the higher the degree of parallel executions. A CPU is designed
    to handle a large set of instructions and manage the operations of many other
    components; it usually has high performance and a complex core, but there aren’t
    many of them. For example, the Intel Xeon processor can have up to 56 cores.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: CPU中并行执行的程度主要取决于它有多少核心。每个核心通常一次运行一个线程，除了英特尔的超线程（一种专有的同时多线程实现）。核心越多，并行执行的级别就越高。CPU被设计用来处理大量指令并管理许多其他组件的操作；它通常具有高性能和复杂的内核，但数量并不多。例如，英特尔至强处理器可以达到56核心。
- en: CPUs are usually not suited for neural network-based model inference if low
    latency is the main requirement. Neural network inference mainly involves operations
    that can be parallelized at a large scale (for example, matrix multiplication).
    Since the total number of cores for a CPU is usually small, it cannot be parallelized
    at scale to meet the needs of neural network inference. On the positive side,
    CPUs are more cost-effective and usually have good memory capacities for hosting
    larger models.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果低延迟是主要要求，CPU通常不适合基于神经网络的模型推理。神经网络推理主要涉及可以大规模并行化的操作（例如，矩阵乘法）。由于CPU的总核心数通常较小，因此无法进行大规模并行化以满足神经网络推理的需求。从积极的一面来看，CPU更具成本效益，并且通常具有良好的内存容量，可以容纳更大的模型。
- en: Graphics processing units (GPUs)
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图形处理单元（GPU）
- en: The design of a GPU is the opposite of the design of a CPU. Instead of having
    a few powerful cores, it has thousands of less powerful cores that are designed
    to perform a small set of instructions highly efficiently. The basic design of
    a GPU core is like that of a CPU. It also contains a control unit, ALU, and a
    local memory cache. However, the GPU control unit handles a much simpler instruction
    set, and the local memory is much smaller.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: GPU的设计与CPU的设计相反。它不是拥有几个强大的核心，而是拥有成千上万个较不强大的核心，这些核心被设计成高效执行一小组指令。GPU核心的基本设计类似于CPU。它也包含一个控制单元、ALU和一个局部内存缓存。然而，GPU控制单元处理的指令集要简单得多，而局部内存也小得多。
- en: When the GPU processes instructions, it schedules blocks of threads, and within
    each block of threads, all the threads perform the same operations but on different
    pieces of data – a parallelization scheme called **Single Instruction Multiple
    Data** (**SIMD**). This architecture fits nicely with how a DL model works, where
    many neurons perform the same operation (mainly matrix multiplication) on different
    pieces of data.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当GPU处理指令时，它会调度线程块，并且在每个线程块内部，所有线程执行相同的操作，但针对不同的数据——这是一种称为**单指令多数据**（**SIMD**）的并行化方案。这种架构非常适合深度学习模型的工作方式，其中许多神经元对不同的数据进行相同的操作（主要是矩阵乘法）。
- en: 'The Nvidia GPU architecture contains two main components:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 英伟达GPU架构包含两个主要组件：
- en: The global memory component
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 全局内存组件
- en: The **streaming multiprocessor** (**SM**) component
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**流式多处理器**（**SM**）组件'
- en: An SM is analogous to a CPU and each SM has many **Computer Unified Device Architecture**
    (**CUDA**) cores, special functional units that perform different arithmetic operations.
    It also has a small, shared memory and cache, and many registers. A CUDA core
    is responsible for functions such as floating-point/integer operations, logic
    calculation, and branching. The thread block mentioned previously is executed
    by the SM. The global memory is located on the same GPU board. When you’re training
    an ML model, both the model and the data need to be loaded into the global memory.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 一个SM类似于CPU，每个SM拥有许多**统一计算设备架构**（**CUDA**）核心，这些是执行不同算术运算的特殊功能单元。它还包含一个小型的共享内存和缓存，以及许多寄存器。CUDA核心负责诸如浮点/整数运算、逻辑计算和分支等函数。之前提到的线程块是由SM执行的。全局内存位于同一GPU板上。当你训练一个机器学习模型时，模型和数据都需要加载到全局内存中。
- en: In a multi-GPU configuration, low-latency and high-throughput communication
    channels are available, such as the Nvidia NVLink. GPUs are well suited for low-latency
    and high-throughput neural network model inferences due to their massive number
    of CUDA cores for large-scale parallelism.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在多GPU配置中，有低延迟和高吞吐量的通信通道可用，例如Nvidia NVLink。由于GPU拥有大量的CUDA核心，非常适合进行低延迟和高吞吐量的神经网络模型推理。
- en: At the time of writing, the latest Nvidia GPU generation is the Blackwell B200
    GPU. It is built with 208 billion transistors, and its NVLink switch system allows
    multi-GPU communication across multiple servers at 1.8TB/s. For large-scale distributed
    training, a cluster of 576 B200 GPUs can work together. Another noteworthy feature
    of the B200 is its 2^(nd) generation Transformer Engine designed to accelerate
    the training of transformers.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，最新的Nvidia GPU代号为Blackwell B200 GPU。它由2080亿个晶体管组成，其NVLink交换系统允许在多个服务器之间以1.8TB/s的速度进行多GPU通信。对于大规模分布式训练，一组576个B200
    GPU可以协同工作。B200的另一个显著特点是它的第二代Transformer Engine，该引擎旨在加速Transformer的训练。
- en: Application-specific integrated circuit
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 专用集成电路
- en: 'An **application-specific integrated circuit** (**ASIC**) is a primary alternative
    to a GPU. ASIC chips are purpose-designed for particular DL architectures for
    computation and data flow, so are faster and require less power than GPUs. For
    example, Google’s **Tensor Processing Unit** (**TPU**) has dedicated **Matrix
    Units** (**MXUs**) designed for efficient matrix computations, and AWS offers
    the Inferentia chip, an ASIC designed for model inference. To speed up model inference,
    the Amazon Inferentia chip and Google’s TPU chip both use the systolic array mechanism
    to speed up arithmetic calculations for deep neural networks. While general-purpose
    chips such as CPUs and GPUs use local registers between different ALU computations
    to transfer data and results, a systolic array allows you to chain multiple ALUs
    to reduce register access to speed up processing. The following diagram shows
    how data flows within a systolic array architecture versus a regular architecture
    that’s used in CPUs and GPUs:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**专用集成电路**（**ASIC**）是GPU的主要替代品。ASIC芯片是为特定深度学习架构专门设计的，用于计算和数据流，因此比GPU更快，功耗更低。例如，Google的**张量处理单元**（**TPU**）拥有专为高效矩阵计算设计的专用**矩阵单元**（**MXUs**），AWS提供Inferentia芯片，这是一款专为模型推理设计的ASIC。为了加快模型推理，亚马逊Inferentia芯片和谷歌的TPU芯片都使用收缩阵列机制来加速深度神经网络的算术计算。虽然通用芯片如CPU和GPU使用不同ALU计算之间的本地寄存器来传输数据和结果，但收缩阵列允许你将多个ALU链接起来，以减少寄存器访问次数，从而加快处理速度。以下图表显示了收缩阵列架构与在CPU和GPU中使用的常规架构之间的数据流：'
- en: '![Figure 10.14 – Systolic array processing versus CPU/GPU processing ](img/B20836_10_14.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图10.14 – 收缩阵列处理与CPU/GPU处理对比](img/B20836_10_14.png)'
- en: 'Figure 10.14: Systolic array processing versus CPU/GPU processing'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.14：收缩阵列处理与CPU/GPU处理对比
- en: The Amazon Inferentia chip can be used directly with Amazon SageMaker for inference
    with improved latency. You can do this by selecting one of the supported Inferentia
    chips for model deployment.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊Inferentia芯片可以直接与Amazon SageMaker配合使用，以降低延迟进行推理。你可以通过选择支持的Inferentia芯片之一来部署模型。
- en: 'While not directly applicable to inference, it is worth mentioning that AWS
    the also provides AWS Trainium accelerator, a purpose-built chip for training
    large deep learning models. Each Trainium accelerator consists of 2 NeuronCore
    cores. Each core is an independent compute engine with 4 main engines: the TensorEngine,
    VectorEngine, ScalarEngine, and GPSIMD-Engine. It also has on-chip SRAM memory.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它不直接适用于推理，但值得一提的是，AWS还提供了AWS Trainium加速器，这是一款专为训练大型深度学习模型而设计的专用芯片。每个Trainium加速器包含2个NeuronCore核心。每个核心都是一个独立的计算引擎，具有4个主要引擎：TensorEngine、VectorEngine、ScalarEngine和GPSIMD-Engine。它还拥有片上SRAM内存。
- en: Each engine of the NeuronCore is optimized for unique computations. The ScalarEngine
    is optimized for scalar computation and can be highly parallelized with support
    for various data types such as FP32, FP16, BF16, INT8, INT16, and Int32\. It can
    perform 1,600 floating-point operations per cycle. The VectorEngine is optimized
    for vector computation. The VectorEngine is also highly parallelized and can perform
    2,500 floating-point operations per cycle. The TensorEngine is based on a power-optimized
    systolic array, which is highly optimized for tensor computations. Each TensorEngine
    can deliver over 100 TFLOPS of FP16/BF16 tensor computations. GPSIMD-Engine consists
    of 8 fully programmable 512-bit wide general-purpose processors, which can execute
    straight-line C-code, and have direct access to the other NeuronCore-v2 engines,
    as well as the SRAM memory.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: NeuronCore的每个引擎都针对独特的计算进行了优化。ScalarEngine针对标量计算进行了优化，并支持FP32、FP16、BF16、INT8、INT16和Int32等多种数据类型，可以实现高度并行化。它每周期可以执行1,600次浮点运算。VectorEngine针对向量计算进行了优化。VectorEngine也高度并行化，每周期可以执行2,500次浮点运算。TensorEngine基于功率优化的阵列，高度优化了张量计算。每个TensorEngine可以提供超过100
    TFLOPS的FP16/BF16张量计算。GPSIMD-Engine由8个完全可编程的512位宽通用处理器组成，可以执行直线C代码，并直接访问其他NeuronCore-v2引擎以及SRAM内存。
- en: Model optimization
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型优化
- en: 'When you’re processing computational graphs for DL model inference, the size
    of the neural network (such as its number of layers, neurons, and so on), the
    number of model parameters, and the numerical precision of the model parameters
    directly impact the performance of model inference. The model optimization approach
    focuses on reducing the size of the neural network, the number of model parameters,
    and the numerical precisions to reduce inference latency. In general, there are
    two main approaches to model optimization: quantization and pruning.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 当你处理用于深度学习模型推理的计算图时，神经网络的大小（如层数、神经元数量等）、模型参数的数量以及模型参数的数值精度会直接影响模型推理的性能。模型优化方法侧重于减少神经网络的大小、模型参数的数量和数值精度，以降低推理延迟。一般来说，模型优化主要有两种方法：量化和剪枝。
- en: Quantization
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 量化
- en: Traditionally, deep neural networks are trained with **floating-point 32 bit**
    (**FP32**). However, for many neural networks, FP32 is not needed for the required
    precision.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，深度神经网络使用**浮点32位**（**FP32**）进行训练。然而，对于许多神经网络来说，FP32并不需要达到所需的精度。
- en: Quantization for DL is a network compression approach that uses lower precision
    numbers, such as **floating-point 16 bit** (**FP16**) or **integer 8 bit** (**INT8**)
    instead of FP32, to represent static model parameters and perform numerical computation
    with dynamic data inputs/activation, all while having minimal or no impact on
    model performance. For example, an INT8 representation takes up four times less
    space than the FP32 representation, which significantly reduces the memory requirements
    and computational costs for neural networks, which means it can improve the overall
    latency for model inference.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的量化是一种网络压缩方法，使用低精度数字，如**浮点16位**（**FP16**）或**整数8位**（**INT8**），而不是FP32来表示静态模型参数，并使用动态数据输入/激活进行数值计算，同时对模型性能的影响最小或没有影响。例如，INT8表示法比FP32表示法占用空间少四倍，这显著降低了神经网络对内存和计算成本的需求，这意味着它可以提高模型推理的整体延迟。
- en: There are different types of quantization algorithms, including uniform and
    non-uniform quantization algorithms. Both approaches map real values in a continuous
    domain to discrete lower-precision values in the quantized domain. In the uniform
    case, the quantized values in the quantized domains are evenly spaced, whereas
    the non-uniform case has varying quantized values.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着不同类型的量化算法，包括均匀和非均匀量化算法。这两种方法都将连续域中的实数值映射到量化域中的离散低精度值。在均匀情况下，量化域中的量化值均匀分布，而在非均匀情况下，量化值是变化的。
- en: 'The following diagram shows the difference between a uniform and a non-uniform
    quantization:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表显示了均匀量化和非均匀量化的区别：
- en: '![Figure 10.15 – Uniform and non-uniform quantization ](img/B20836_10_15.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图10.15 – 均匀和非均匀量化](img/B20836_10_15.png)'
- en: 'Figure 10.15: Uniform and non-uniform quantization'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.15：均匀和非均匀量化
- en: Quantization can be performed both post-training and during training (quantization-aware
    training). Post-training quantization takes a trained model, quantizes the weights,
    and regenerates a quantized model. Quantization-aware training involves fine-tuning
    a full precision model. During training, the higher-precision real numbers are
    reduced to lower-precision numbers.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 量化可以在训练后和训练期间（量化感知训练）进行。训练后量化将一个训练好的模型量化权重，并重新生成一个量化模型。量化感知训练涉及微调全精度模型。在训练过程中，高精度实数被降低到低精度数字。
- en: Other techniques include mixed-precision training, a method that utilizes lower
    precision, such as 16-bit floating-point or 8-bit integers, for computations during
    the training of ML models, while retaining higher precision (32-bit) for weight
    updates. This approach aims to achieve notable speedups and memory savings, particularly
    on hardware optimized for low-precision operations. Another technique is quantization
    with knowledge distillation, which involves transferring knowledge from a larger,
    high-precision teacher model to a smaller, quantized student model. The student
    model is trained to replicate the outputs of the teacher model, enabling it to
    acquire a more accurate quantized representation. While this technique can achieve
    higher accuracy compared to direct quantization, it necessitates an additional
    training process.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 其他技术包括混合精度训练，这是一种在训练机器学习模型时使用较低精度（如16位浮点数或8位整数）进行计算的方法，同时保留较高的精度（32位）用于权重更新。这种方法旨在实现显著的加速和内存节省，尤其是在针对低精度操作优化的硬件上。另一种技术是带有知识蒸馏的量化，它涉及将知识从较大的、高精度教师模型转移到较小的、量化的学生模型。学生模型被训练以复制教师模型的输出，使其能够获得更准确的量化表示。虽然这种技术可以比直接量化实现更高的精度，但它需要额外的训练过程。
- en: Quantization support is natively available in DL frameworks such as PyTorch
    and TensorFlow. For example, PyTorch supports both forms of quantization via its
    `torch.quantization` package. TensorFlow supports quantization through the `tf.lite`
    package.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习框架中，如PyTorch和TensorFlow，量化支持是原生可用的。例如，PyTorch通过其`torch.quantization`包支持两种量化形式。TensorFlow通过`tf.lite`包支持量化。
- en: Pruning (also known as sparsity)
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 剪枝（也称为稀疏性）
- en: '**Pruning** is another network compression technique that eliminates some of
    the model weights and neurons that don’t impact model performance to reduce the
    size of the model to make inference faster. For example, weights that are close
    to zero or redundant can usually be removed.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**剪枝**是另一种网络压缩技术，通过消除对模型性能没有影响的某些模型权重和神经元来减小模型的大小，以使推理更快。例如，接近零或冗余的权重通常可以被移除。'
- en: Pruning techniques can be classified into static and dynamic pruning. Static
    pruning takes place offline before the model is deployed, while dynamic pruning
    is performed during runtime. Here, we will discuss some of the key concepts and
    approaches for static pruning.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 剪枝技术可以分为静态剪枝和动态剪枝。静态剪枝在模型部署之前离线进行，而动态剪枝在运行时进行。在这里，我们将讨论静态剪枝的一些关键概念和方法。
- en: 'Static pruning mainly consists of three steps:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 静态剪枝主要包含三个步骤：
- en: Parameter selection for pruning targeting.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 针对剪枝的参数选择。
- en: Pruning the neurons.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 剪枝神经元。
- en: Fine-tuning or retraining if needed. Retraining may improve the model performance
    of the pruned neural network.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有必要，进行微调或重新训练。重新训练可能会提高剪枝神经网络的模型性能。
- en: 'There are several approaches for selecting the parameters for static pruning,
    including the magnitude-based approach, the penalty-based approach, and dropout
    removal:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 静态剪枝参数的选择有几种方法，包括基于幅度的方法、基于惩罚的方法和dropout移除：
- en: '**Magnitude-based approach**: It is widely accepted that large model weights
    are more important than small model weights. So, one intuitive way to select weights
    for pruning is to look at zero-value weights or those weights within a defined
    absolute threshold. The magnitude of the neural network activation layer can also
    be used to determine whether the associated neurons can be removed.'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于幅度的方法**：普遍认为，大型模型权重比小型模型权重更重要。因此，选择剪枝权重的一种直观方法是查看零值权重或那些在定义的绝对阈值内的权重。神经网络激活层的幅度也可以用来确定相关的神经元是否可以被移除。'
- en: '**Penalty-based approach**: In the penalty-based approach, the goal is to modify
    the loss function or add additional constraints so that some weights are forced
    to become zeros or near-zeros. The weights that are zeros or close to zeros can
    then be pruned. An example of the penalty-based approach is using LASSO to shrink
    the weights of features.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于惩罚的方法**：在基于惩罚的方法中，目标是修改损失函数或添加额外的约束，以便某些权重被迫变为零或接近零。然后可以剪除零或接近零的权重。基于惩罚方法的例子是使用
    LASSO 来缩小特征权重。'
- en: '**Dropout removal**: Dropout layers are used in deep neural network training
    as regularizers to avoid overfitting data. While dropout layers are useful in
    training, they are not useful for inference and can be removed to reduce the number
    of parameters without impacting the model’s performance.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout 移除**：Dropout 层在深度神经网络训练中用作正则化器，以避免过拟合数据。虽然 Dropout 层在训练中很有用，但在推理中并不有用，可以移除以减少参数数量而不影响模型性能。'
- en: '**Regularization-based pruning**: This technique involves adding regularization
    terms to the loss function during training, which encourages the model to learn
    sparse representations. Examples include L1 regularization (LASSO), which promotes
    sparsity by driving some weights to exactly zero, and Group Lasso, which prunes
    entire filters or channels.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于正则化的剪枝**：这种技术涉及在训练期间向损失函数添加正则化项，这鼓励模型学习稀疏表示。例子包括 L1 正则化（LASSO），它通过将一些权重驱动到正好为零来促进稀疏性，以及
    Group Lasso，它可以剪除整个滤波器或通道。'
- en: '**Reinforcement-based pruning**: This method adopts RL to compress models automatically.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于强化学习的剪枝**：这种方法采用强化学习来自动压缩模型。'
- en: DL frameworks, such as TensorFlow and PyTorch, provide APIs for pruning models.
    For example, you can use the `tensorflow_model_optimization` package and its `prune_low_magnitude`
    API for magnitude-based pruning. PyTorch provides model-pruning support via its
    `torch.nn.utils.prune` API.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: DL 框架，例如 TensorFlow 和 PyTorch，提供了模型剪枝的 API。例如，你可以使用 `tensorflow_model_optimization`
    包及其 `prune_low_magnitude` API 进行基于幅度的剪枝。PyTorch 通过其 `torch.nn.utils.prune` API
    提供模型剪枝支持。
- en: The primary trade-off with both quantization and pruning is the potential loss
    of model accuracy or performance in exchange for efficiency gains. More aggressive
    quantization or pruning can lead to higher compression rates but may also result
    in a larger drop in accuracy. Finding the right balance between compression and
    accuracy is crucial. Quantized or pruned models may also have limited portability
    across different hardware platforms or deep learning frameworks, as the optimized
    formats and sparse structures may not be universally supported.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 量化和剪枝的主要权衡是在效率提升的同时可能损失模型精度或性能。更激进的量化和剪枝可能导致更高的压缩率，但也可能导致精度下降更大。在压缩和精度之间找到合适的平衡至关重要。量化和剪枝的模型也可能在不同硬件平台或深度学习框架之间具有有限的可移植性，因为优化的格式和稀疏结构可能不被普遍支持。
- en: Graph and operator optimization
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图和操作符优化
- en: In addition to hardware acceleration and model optimization, there are additional
    optimization techniques that focus on the execution optimization of the computational
    graph, as well as hardware-specific operator and tensor optimization.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了硬件加速和模型优化之外，还有其他优化技术，这些技术专注于计算图的执行优化，以及特定硬件的操作符和张量优化。
- en: Graph optimization
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 图优化
- en: Graph optimization focuses on reducing the number of operations that are performed
    in computational graphs to speed up inference. Multiple techniques are used for
    graph optimization, including operator fusion, dead code elimination, and constant
    folding.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图优化侧重于减少计算图中执行的操作数量以加快推理。图优化使用了多种技术，包括操作融合、死代码消除和常量折叠。
- en: Operator fusion combines multiple operations in a subgraph into a single operation
    to improve latency. In a typical execution of a subgraph with multiple operations,
    system memory is accessed for read/write to transfer data between operations,
    which is an expensive task. Operator fusion reduces the number of memory accesses,
    as well as optimizing the overall computations since the computations are now
    happening in a single kernel without the intermediate results being saved to memory.
    This approach also reduces the memory footprint due to a smaller number of operations
    being performed.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 算子融合将子图中的多个操作合并成一个操作，以降低延迟。在具有多个操作的子图典型执行过程中，系统内存被访问以进行读写，以在操作之间传输数据，这是一个昂贵的任务。算子融合减少了内存访问次数，同时也优化了整体计算，因为计算现在在一个内核中发生，中间结果不再保存到内存中。这种方法还由于操作数量减少而减少了内存占用。
- en: 'The following diagram shows the concept of operator fusion:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 下图展示了算子融合的概念：
- en: '![Figure 10.16 – Graph operator fusion ](img/B20836_10_16.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图10.16 – 图算子融合](img/B20836_10_16.png)'
- en: 'Figure 10.16: Graph operator fusion'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.16：图算子融合
- en: In the preceding diagram, the **matrix multiplication**, **add**, and **ReLU**
    operators are being fused into a single operator for execution in a single kernel
    to reduce memory access and the time needed to start multiple kernels.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，**矩阵乘法**、**加法**和**ReLU**算子正在融合成一个单独的算子，以便在一个内核中执行，以减少内存访问和启动多个内核所需的时间。
- en: 'Constant folding is the process of evaluating constants at compile time instead
    of runtime to speed up processing during runtime. For example, for the following
    expression, *A* can be assigned to a value of 300 at compile time instead of being
    dynamically calculated at runtime, which requires a more computational cycle:
    *A = 100 + 200*. Dead code elimination removes the code that does not affect the
    program’s results. This ensures that the program doesn’t waste computation on
    useless operations.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 常量折叠是在编译时评估常量，而不是在运行时，以加快运行时的处理速度。例如，对于以下表达式，*A*可以在编译时被分配一个值为300，而不是在运行时动态计算，这需要更多的计算周期：*A
    = 100 + 200*。死代码消除移除不影响程序结果的代码。这确保程序不会在无用的操作上浪费计算。
- en: Operator optimization
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 算子优化
- en: 'Operator optimization (also known as tensor optimization) focuses on hardware-specific
    optimization for a specific model. Different hardware devices have different memory
    layouts and computational units and, as such, hardware-specific optimization is
    often required to take full advantage of the hardware architecture. Multiple techniques
    have been developed for operator optimization on different hardware devices, including
    the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 算子优化（也称为张量优化）专注于特定模型的硬件特定优化。不同的硬件设备有不同的内存布局和计算单元，因此通常需要进行硬件特定优化以充分利用硬件架构。已经为不同硬件设备上的算子优化开发了多种技术，包括以下内容：
- en: Nested parallelism, which takes advantage of the GPU memory hierarchy and enables
    data reuse across threads through shared memory regions.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌套并行性，利用GPU内存层次结构，并通过共享内存区域实现线程间的数据重用。
- en: Memory latency hiding, which overlaps the memory operation with computation
    to maximize memory and compute resources.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存延迟隐藏，通过将内存操作与计算重叠来最大化内存和计算资源。
- en: While graph optimization, operator optimization, and model optimization address
    different areas of optimization, they are often combined to provide end-to-end
    optimization.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图优化、算子优化和模型优化针对不同的优化领域，但它们通常结合在一起以提供端到端优化。
- en: Model compilers
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型编译器
- en: Manually optimizing end-to-end model performance is non-trivial. Adding the
    dimensions of multiple ML frameworks and a wide range of target hardware devices
    for optimization makes this a very challenging problem. To simplify the optimization
    process for different ML frameworks and different devices, several open-source
    and commercial products have been developed. We will briefly talk about a few
    such packages in this section.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 手动优化端到端模型性能并非易事。添加多个机器学习框架的维度以及广泛的优化目标硬件设备使得这个问题变得非常具有挑战性。为了简化不同机器学习框架和不同设备的优化过程，已经开发了几种开源和商业产品。我们将在本节简要介绍一些这样的包。
- en: TensorFlow XLA
  id: totrans-229
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: TensorFlow XLA
- en: TensorFlow **Accelerated Linear Algebra** (**XLA**) is a DL compiler for TensorFlow.
    It compiles a TensorFlow graph into a sequence of execution kernels specifically
    optimized for the model. XLA transforms the original TensorFlow graph into an
    **intermediate representation** (**IR**) before performing several optimizations
    on the IR, such as operator fusion for faster computation. The output from the
    optimization step is then used for generating hardware-specific code that optimizes
    the performance of the different target hardware devices, such as CPUs and GPUs.
    XLA is used at Google in production for many accelerators.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow **加速线性代数**（**XLA**）是 TensorFlow 的深度学习编译器。它将 TensorFlow 图编译成针对模型特别优化的执行内核序列。XLA
    在对 IR 执行多个优化之前，将原始 TensorFlow 图转换为 **中间表示**（**IR**），例如为了更快计算进行的算子融合。优化步骤的输出随后用于生成针对不同目标硬件设备（如
    CPU 和 GPU）的性能优化特定硬件代码。XLA 在 Google 的生产环境中用于许多加速器。
- en: PyTorch Glow
  id: totrans-231
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PyTorch Glow
- en: PyTorch Glow is a DL compiler for multiple DL frameworks. Similar to XLA, it
    also uses an IR to represent the original computational graph to perform optimizations.
    Unlike XLA, PyTorch Glow uses two layers of IRs. The first layer is used for performing
    domain-specific optimizations such as quantization, while the second IR layer
    is used for memory-related optimization such as memory latency hiding. After the
    second layer of IR optimization, the target device-dependent code is generated
    for running the models on different devices.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch Glow 是针对多个深度学习框架的深度学习编译器。类似于 XLA，它也使用 IR 来表示原始计算图以执行优化。与 XLA 不同，PyTorch
    Glow 使用两层 IR。第一层用于执行特定领域的优化，如量化，而第二层 IR 用于与内存相关的优化，如内存延迟隐藏。在第二层 IR 优化之后，为在不同设备上运行模型生成依赖于目标设备的代码。
- en: Apache TVM
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Apache TVM
- en: Apache **Tensor Virtual Machine** (**TVM**) is an open-source compiler framework
    for model optimization. It optimizes and compiles models built with different
    frameworks, such as PyTorch and TensorFlow, for different target CPUs, GPUs, and
    specialized hardware devices for accelerated performance. TVM supports optimization
    at different levels, including graph optimization and operator optimization targeting
    specific hardware. It also comes with a runtime for efficiently executing the
    compiled models.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: Apache **张量虚拟机**（**TVM**）是一个开源的模型优化编译框架。它优化并编译使用不同框架（如 PyTorch 和 TensorFlow）构建的模型，针对不同的目标
    CPU、GPU 和用于加速性能的专用硬件设备。TVM 支持不同级别的优化，包括针对特定硬件的图优化和算子优化。它还附带一个运行时，用于高效执行编译后的模型。
- en: One key feature of TVM is AutoTVM, which uses ML to search for the optimal sequences
    of code execution for different hardware devices. This ML-based search algorithm
    can significantly outperform baseline benchmarks by using vendor-provided optimization
    libraries such as cuDNN. This ML-based approach also enables efficient compilation
    scaling for a large number of hardware devices.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: TVM 的一个关键特性是 AutoTVM，它使用机器学习来搜索针对不同硬件设备的代码执行最佳序列。这种基于机器学习的搜索算法可以通过使用供应商提供的优化库（如
    cuDNN）显著超越基线基准。这种基于机器学习的方法还可以实现针对大量硬件设备的有效编译扩展。
- en: Amazon SageMaker Neo
  id: totrans-236
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Amazon SageMaker Neo
- en: Amazon SageMaker Neo is the model-compiling feature in SageMaker. It mainly
    uses Apache TVM as its underlying compiler library. With SageMaker Neo, you take
    a model that’s been trained on different ML/DL frameworks such as TensorFlow and
    PyTorch, choose the target processors such as Intel, Apple, ARM, or Nvidia, and
    then SageMaker Neo compiles an optimized model for the target hardware. Neo also
    provides a runtime library for each target platform to load and execute the compiled
    model. SageMaker Neo is a managed offering, so you don’t need to manage the underlying
    infrastructure and processes for model compilation and deployment.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker Neo 是 SageMaker 中的模型编译功能。它主要使用 Apache TVM 作为其底层编译库。使用 SageMaker
    Neo，你可以将已在 TensorFlow 和 PyTorch 等不同的机器学习/深度学习框架上训练的模型，选择目标处理器，如英特尔、苹果、ARM 或英伟达，然后
    SageMaker Neo 为目标硬件编译一个优化的模型。Neo 还为每个目标平台提供运行时库，用于加载和执行编译后的模型。SageMaker Neo 是一个托管服务，因此你不需要管理模型编译和部署的基础基础设施和流程。
- en: Inference engine optimization
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 推理引擎优化
- en: One common model deployment pattern is to use open-source inference engines
    or commercial hosting platforms for model serving. So, inference engine optimization
    is another approach that helps reduce model latency and inference throughput.
    In this section, we will talk about a few considerations. Note that there are
    no universal rules for inference engine optimization as it is sometimes engine-
    and model-specific. It is important to test and validate different configurations
    for the final deployment.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的模型部署模式是使用开源推理引擎或商业托管平台进行模型服务。因此，推理引擎优化是另一种有助于减少模型延迟和推理吞吐量的方法。在本节中，我们将讨论一些考虑因素。请注意，推理引擎优化没有普遍适用的规则，因为它有时是针对特定引擎和模型的。测试和验证不同配置以供最终部署是很重要的。
- en: Inference batching
  id: totrans-240
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 推理批处理
- en: If you have a large number of inference requests and there is no strict latency
    requirement on a single prediction request, then inference batching is a technique
    that can help reduce the total inference time for the requests. With inference
    batching, instead of running predictions one at a time for each request, multiple
    requests are batched together and sent to the inference engine. This technique
    reduces the total number of requests round-trips, thus reducing the total inference
    time. Inference engines such as TensorFlow Serving and TorchServe provide built-in
    support for batch inference. You can find the configuration details for TorchServe
    and TensorFlow Serving batch inference at [https://pytorch.org/serve/batch_inference_with_ts.html](https://pytorch.org/serve/batch_inference_with_ts.html)
    and [https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration),
    respectively.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有大量推理请求，并且对单个预测请求没有严格的延迟要求，那么推理批处理是一种可以减少请求总推理时间的技巧。使用推理批处理，不是为每个请求逐个运行预测，而是将多个请求批量一起发送到推理引擎。这种技术减少了请求往返的总次数，从而减少了总推理时间。TensorFlow
    Serving和TorchServe等推理引擎提供了内置的批推理支持。您可以在[https://pytorch.org/serve/batch_inference_with_ts.html](https://pytorch.org/serve/batch_inference_with_ts.html)和[https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration](https://www.tensorflow.org/tfx/serving/serving_config#batching_configuration)找到TorchServe和TensorFlow
    Serving批推理的配置细节。
- en: Enabling parallel serving sessions
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 启用并行服务会话
- en: If your model hosting server has multiple compute cores, you can configure the
    number of parallel serving sessions to maximize the utilization of the available
    cores. For example, you can configure the `TENSORFLOW_INTRA_OP_PARALLELISM` setting
    in TensorFlow Serving based on the number of cores that can run multiple serving
    sessions in parallel to optimize throughput. TorchServe has settings for the number
    of workers per model and the number of threads for parallelization optimization.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的模型托管服务器有多个计算核心，您可以配置并行服务会话的数量以最大化可用核心的利用率。例如，您可以根据可以并行运行多个服务会话的核心数量配置TensorFlow
    Serving中的`TENSORFLOW_INTRA_OP_PARALLELISM`设置以优化吞吐量。TorchServe有针对每个模型的worker数量和用于并行化优化的线程数量的设置。
- en: Picking a communication protocol
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择通信协议
- en: Inference engines such as TensorFlow and TorchServe provide support for the
    gRPC protocol, which is a faster serialization format than the REST protocol.
    The gPRC protocol provides better overall performance but does have performance
    benchmarks as different models could behave differently. The REST protocol may
    be your preferred option depending on your specific requirements.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow和TorchServe等推理引擎支持gRPC协议，这是一种比REST协议更快的序列化格式。gRPC协议提供了更好的整体性能，但确实有性能基准，因为不同的模型可能会有不同的行为。根据您的具体需求，REST协议可能是您更喜欢的选项。
- en: With that, you have learned about the technical approaches to large-scale training
    and low-latency model inference.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这样，您已经了解了大规模训练和低延迟模型推理的技术方法。
- en: Now that we have discussed some of the considerations for inference optimization,
    we will next explore some of the more recent advancements in inference technology
    for large language models.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了一些推理优化的考虑因素，接下来我们将探讨一些大型语言模型推理技术方面的最新进展。
- en: Inference in large language models
  id: totrans-248
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 大型语言模型的推理
- en: As language models continue to grow in size, the task of deploying and running
    them becomes increasingly challenging. Even with model optimization efforts, these
    expansive models often exceed the memory capacity of a single GPU. To address
    this hurdle and enable the deployment of these large language models, numerous
    ML inference frameworks have emerged. Let’s delve into a few of these frameworks
    to gain insight into how they facilitate the inference process for these language
    models.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 随着语言模型规模的持续增长，部署和运行它们的任务变得越来越具有挑战性。即使有模型优化努力，这些庞大的模型通常也会超出单个GPU的内存容量。为了克服这一障碍并使这些大型语言模型得以部署，已经出现了许多机器学习推理框架。让我们深入了解这些框架，以了解它们如何促进这些语言模型的推理过程。
- en: Text Generation Inference (TGI)
  id: totrans-250
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 文本生成推理（TGI）
- en: 'TGI is an optimized serving solution by HuggingFace for deploying open-source
    large language models like Falcon and FLAN-T5\. TGI has the following key capabilities
    for large language model inference:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: TGI是HuggingFace为部署开源大型语言模型（如Falcon和FLAN-T5）提供的优化服务解决方案。TGI具有以下针对大型语言模型推理的关键能力：
- en: '**Tensor parallelism**: This feature allows a large language model to be deployed
    across multiple GPUs so it can fit into the combined GPU memory as well as faster
    inference across multiple GPUs.'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**张量并行**：此功能允许大型语言模型部署到多个GPU上，以便它可以适应组合GPU内存，并在多个GPU上实现更快的推理。'
- en: '**Quantization**: TGI can perform model quantization with the bitsandbytes
    and GPT-Q quantization library packages to reduce the model size.'
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化**：TGI可以使用bitsandbytes和GPT-Q量化库包执行模型量化，以减小模型大小。'
- en: '**Continuous batching**: This feature increases the throughput of the inference
    by running multiple input sequences by using the same loaded model parameters.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**连续批处理**：此功能通过使用相同的加载模型参数运行多个输入序列来增加推理的吞吐量。'
- en: DeepSpeed-Inference
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DeepSpeed-Inference
- en: 'In addition to being a framework for large-scale distributed training, DeepSpeed
    can also help optimize the inference of large language models. It has the following
    key features for inference:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 除了是一个用于大规模分布式训练的框架之外，DeepSpeed还可以帮助优化大型语言模型的推理。它在推理方面具有以下关键特性：
- en: '**Model parallelism**: This feature fits a large model that would otherwise
    not fit into GPU memory by splitting a model across multiple GPU devices.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型并行**：此功能通过将模型分割到多个GPU设备上，适合那些否则无法适应GPU内存的大型模型。'
- en: '**Inference-optimized kernel**: DeepSpeed can fuse element-wise operations,
    matrix multiplications, transpositions, and reductions all into a single kernel,
    significantly reducing the number of kernel invocations as well as main memory
    access to reduce main memory access latency. DeepSpeed-Inference kernels are also
    fine-tuned to maximize the memory bandwidth utilization for loading the parameters.'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理优化内核**：DeepSpeed可以将逐元素操作、矩阵乘法、转置和归约全部融合到一个内核中，显著减少内核调用次数以及主内存访问，从而降低主内存访问延迟。DeepSpeed-Inference内核也经过微调，以最大化内存带宽利用率，用于加载参数。'
- en: '**Quantization**: A novel approach to quantizing models, called Mixture of
    Quantization, involves both shrinking the model and reducing the inference cost
    during production.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**量化**：一种名为混合量化的新型模型量化方法，涉及在生产过程中缩小模型并降低推理成本。'
- en: FastTransformer
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FastTransformer
- en: 'FasterTransformer is a high-performance library developed by Nvidia, designed
    to accelerate the inference of transformer-based neural networks, particularly
    for large models that span multiple GPUs and nodes in a distributed setup. This
    open-source framework is focused on optimizing transformer blocks, encompassing
    both encoder and decoder components. It has the following key features supporting
    inference of transformer-based models:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: FasterTransformer是由Nvidia开发的一个高性能库，旨在加速基于transformer的神经网络的推理，特别是对于跨越多个GPU和分布式设置中的节点的庞大模型。这个开源框架专注于优化transformer块，包括编码器和解码器组件。它具有以下支持基于transformer模型推理的关键特性：
- en: '**Model parallelism**: With FastTransformer, a transformer model can be split
    across multiple GPUs for faster inference.'
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型并行**：使用FastTransformer，可以将transformer模型分割到多个GPU上以实现更快的推理。'
- en: '**Optimization support:** FasterTransformer optimizes transformer-based models
    with techniques like layer fusion, multi-head attention acceleration using data
    caching, **General Matrix Multiply** (**GEMM**) kernel autotuning for efficient
    matrix multiplication, and support for lower-precision data types like FP16, BF16,
    and INT8\. Operations on these data types can be accelerated by Tensor Cores on
    the recent NVIDIA GPUs.'
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化支持**：FasterTransformer使用层融合、使用数据缓存的多头注意力加速、**通用矩阵乘法**（**GEMM**）内核自动调优以及支持FP16、BF16和INT8等低精度数据类型等技术来优化基于transformer的模型。这些数据类型的操作可以通过最近NVIDIA
    GPU上的Tensor Cores进行加速。'
- en: Hands-on lab – running distributed model training with PyTorch
  id: totrans-264
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 实践实验室 - 使用PyTorch运行分布式模型训练
- en: As an ML solutions architect, you need to explore and design different model-training
    paradigms to meet different model-training requirements. In this hands-on lab,
    you will use the SageMaker Training service to run data-parallel distributed training.
    We will use PyTorch’s `torch.nn.parallel.DistributedDataParallel` API as the distributed
    training framework and run the training job on a small cluster. We will reuse
    the dataset and training scripts from the hands-on lab in *Chapter 8*, *Building
    a Data Science Environment Using AWS ML Services*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 作为ML解决方案架构师，您需要探索和设计不同的模型训练范式以满足不同的模型训练需求。在本实践实验室中，您将使用SageMaker训练服务运行数据并行分布式训练。我们将使用PyTorch的`torch.nn.parallel.DistributedDataParallel`
    API作为分布式训练框架，并在小型集群上运行训练作业。我们将重用*第8章*中*构建数据科学环境使用AWS ML服务*的实验室中的数据集和训练脚本。
- en: Problem statement
  id: totrans-266
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题陈述
- en: In *Chapter 8*, we trained a financial sentiment model using the data science
    environment you created using SageMaker. The model was trained using a single
    GPU in the Studio Notebook and SageMaker training service. Anticipating the future
    needs of model training with large datasets, we need to design an ML training
    process using multiple GPUs to scale out training horizontally.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第8章*中，我们使用您使用SageMaker创建的数据科学环境训练了一个金融情感模型。该模型是在Studio笔记本和SageMaker训练服务中使用单个GPU进行训练的。考虑到未来对大型数据集进行模型训练的需求，我们需要设计一个使用多个GPU进行水平扩展的ML训练流程。
- en: Dataset description
  id: totrans-268
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集描述
- en: 'We will use the financial phrase dataset for this lab: [https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news).'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用金融短语数据集进行此实验室：[https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news](https://www.kaggle.com/ankurzing/sentiment-analysis-for-financial-news)。
- en: Modifying the training script
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改训练脚本
- en: 'First, we need to add distributed training support to the training script.
    To start, create a `code` directory in your Studio notebook environment, create
    a copy of the `train.py` file and save it to the `code` directory, and then rename
    the file `train-dis.py`, and open the `train-dis.py` file. You will need to make
    changes to the following three main functions. The following steps are meant to
    highlight the key changes needed. To run the lab, you can download the modified
    `train-dis.py` file from [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10).
    The following are the key changes to be made in the `train-dis.py` file:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要将分布式训练支持添加到训练脚本中。为此，请在您的Studio笔记本环境中创建一个`code`目录，复制`train.py`文件并将其保存到`code`目录中，然后将文件重命名为`train-dis.py`，并打开`train-dis.py`文件。您需要对以下三个主要函数进行修改。以下步骤旨在突出需要进行的重点更改。要运行实验室，您可以从[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/tree/main/Chapter10)下载修改后的`train-dis.py`文件。以下是在`train-dis.py`文件中需要做出的关键更改：
- en: '**Modifying the** `train()` **function**: You need to make some changes to
    the `train()` function to enable distributed training. The following are the key
    changes that are required:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修改** `train()` **函数**：您需要对`train()`函数进行一些修改以启用分布式训练。以下是需要进行的重点更改：'
- en: '**Process group initialization**: To enable distributed training, we need to
    initialize and register each training process on each device to be included in
    the training group. This can be achieved by calling the `torch.distributed.init_process_group()`
    function. This function will block until all the processes have been registered.
    There are a few concepts that we need to be familiar with during this initialization
    step:'
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**进程组初始化**：为了启用分布式训练，我们需要初始化和注册每个设备上的每个训练过程，以便将其包含在训练组中。这可以通过调用`torch.distributed.init_process_group()`函数来实现。此函数将阻塞，直到所有进程都已注册。在初始化步骤中，我们需要熟悉一些概念：'
- en: '**World size**: This is the total number of processes in a distributed training
    group. Since we will run one process on each device (CPU or GPU), the world size
    is also the same as the total number of devices in a training cluster. For example,
    if you have two servers and each server has two GPUs, then the world size is four
    for this training group. The `torch.distributed.init_process_group()` function
    uses this information to understand how many processes to include in the distributed
    training job.'
  id: totrans-274
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全局大小**：这是分布式训练组中进程的总数。由于我们将在每个设备（CPU或GPU）上运行一个进程，因此全局大小也等于训练集群中设备总数的总和。例如，如果您有两个服务器，每个服务器有两个GPU，那么这个训练组的全局大小是四。`torch.distributed.init_process_group()`函数使用这些信息来了解在分布式训练作业中应包含多少个进程。'
- en: '**Rank**: This is the unique index that’s assigned to each process in the training
    group. For example, the ranks for all the processes in a training group with a
    world size of four would be [0,1,2,3]. This unique index helps uniquely identify
    each process within a training group for communication.'
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**排名**：这是分配给训练组中每个进程的唯一索引。例如，具有四个全局大小的训练组中所有进程的排名将是[0,1,2,3]。这个唯一的索引有助于在训练组内唯一地标识每个进程以进行通信。'
- en: '**Local rank**: This uniquely identifies a device in a server node. For example,
    if there are two devices in a server node, the local rank for two devices would
    be [0,1]. Local rank allows you to select a specific device to load the model
    and data for model training.'
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地排名**：这唯一地标识了服务器节点中的一个设备。例如，如果服务器节点中有两个设备，两个设备的本地排名将是[0,1]。本地排名允许您选择一个特定的设备来加载模型和数据以进行模型训练。'
- en: '**Backend**: This is the low-level communication library for exchanging and
    aggregating data among the different processes. PyTorch distributed training supports
    several communication backends, including NCCL, MPI, and Gloo. You choose a different
    backend based on the device and networking configuration. It uses these backends
    to send, receive, broadcast, or reduce data during distributed training. We are
    not going to get into the technical details of these backends in this book. If
    you are interested in how these backends work, you can easily find internet sources
    that cover these topics.'
  id: totrans-277
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后端**：这是在不同进程之间交换和聚合数据的高级通信库。PyTorch分布式训练支持包括NCCL、MPI和Gloo在内的多个通信后端。您可以根据设备和网络配置选择不同的后端。它使用这些后端在分布式训练期间发送、接收、广播或减少数据。我们不会在本书中详细介绍这些后端的技术细节。如果您对这些后端的工作原理感兴趣，可以轻松地在网上找到涵盖这些主题的资源。'
- en: '**Wrap the training algorithm with PyTorch distributed library**: To use the
    PyTorch distributed library support for training, you need to wrap the algorithm
    with the PyTorch distributed training library. You can achieve this with the `torch.nn.parallel.DistributedDataParallel()`
    API. This allows the algorithm to participate in distributed training to exchange
    gradients and update global parameters.'
  id: totrans-278
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用PyTorch分布式库包装训练算法**：为了使用PyTorch分布式库对训练的支持，您需要使用PyTorch分布式训练库来包装算法。您可以使用`torch.nn.parallel.DistributedDataParallel()`
    API实现这一点。这允许算法参与分布式训练，交换梯度并更新全局参数。'
- en: '**Saving model using a single device**: In a multi-device server node, you
    only want one device to save the final model to avoid I/O conflicts. You can achieve
    this by selecting a device with a specific local rank ID.'
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用单个设备保存模型**：在多设备服务器节点中，您只想让一个设备保存最终模型以避免I/O冲突。您可以通过选择具有特定本地排名ID的设备来实现这一点。'
- en: '**Modifying the** `get_data_loader()` **function**: To ensure a different subset
    of training data is loaded onto different devices on the server nodes, we need
    to configure the PyTorch DataLoader API to load data based on the rank of the
    training process. This can be done using the `torch.utils.data.distributed.DistributedSampler`
    API.'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**修改** `get_data_loader()` **函数**：为了确保不同的训练数据子集被加载到服务器节点上的不同设备上，我们需要配置PyTorch
    DataLoader API，以便根据训练进程的rank加载数据。这可以通过使用`torch.utils.data.distributed.DistributedSampler`
    API来完成。'
- en: '**Adding multi-processing launch support for multi-device server nodes**: For
    server nodes with multiple devices, we need to spawn several parallel processes
    based on the number of devices available. To enable this, we can use `torch.multiprocessing`
    to kick off multiple running processes on each node.'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**为多设备服务器节点添加多进程启动支持**：对于具有多个设备的服务器节点，我们需要根据可用的设备数量启动多个并行进程。为了启用此功能，我们可以使用`torch.multiprocessing`在每个节点上启动多个运行进程。'
- en: With the training script modified, we will update the laucher notebook next.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 修改训练脚本后，我们将更新启动笔记本。
- en: Modifying and running the launcher notebook
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修改并运行启动笔记本
- en: 'We are now ready to modify the launcher notebook to kick off the model training
    job:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好修改启动笔记本以启动模型训练作业：
- en: 'To start, copy the `bert-financial-sentiment-Launcher.ipynb` file from `chapter
    8` and save it as `bert-financial-sentiment-dis-Launcher.ipynb`. Open the new
    notebook and replace the second cell’s content with the following code:'
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，将`bert-financial-sentiment-Launcher.ipynb`文件从`第8章`复制出来，并保存为`bert-financial-sentiment-dis-Launcher.ipynb`。打开新笔记本，并将第二个单元格的内容替换为以下代码：
- en: '[PRE5]'
  id: totrans-286
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The main code changes are as follows:'
  id: totrans-287
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 主要代码更改如下：
- en: Point the entry point to the new training script (`entry_point="train-dis.py"`)
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将入口点指向新的训练脚本（`entry_point="train-dis.py"`）
- en: Increase the number of compute instance from 1 to 2 for multi-node training
    (`instance_count=2`)
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将计算实例数量从1增加到2以进行多节点训练（`instance_count=2`）
- en: Change the instance type for multi-GPU support (`instance_type= "ml.g4dn.12xlarge"`)
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改实例类型以支持多GPU（`instance_type= "ml.g4dn.12xlarge"`）
- en: 'Increase the batch size because there are now more GPUs to handle a larger
    batch size (`"batch_size" : 64`)'
  id: totrans-291
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '增加批大小，因为现在有更多的GPU可以处理更大的批大小（`"batch_size" : 64`）'
- en: Download [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt)
    and upload it to the `code` directory in your Studio Notebook. This `requirements.txt`
    contains the library packages to be installed in the trainer container. In this
    case, we want to install the transformer package.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/code/requirements.txt)并将其上传到您的Studio笔记本中的`code`目录。此`requirements.txt`包含需要在训练容器中安装的库包。在这种情况下，我们想要安装transformer包。
- en: If you don’t want to manually revise the launcher notebook, you can download
    the revised launcher notebook at [https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb).
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不想手动修改启动笔记本，您可以下载修改后的启动笔记本，链接为[https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb](https://github.com/PacktPublishing/The-Machine-Learning-Solutions-Architect-and-Risk-Management-Handbook-Second-Edition/blob/main/Chapter10/bert-financial-sentiment-dis-launcher.ipynb)。
- en: Now, just execute each cell in the new notebook to kick off the distributed
    training. You can track the training status directly inside the notebook, and
    the detail status in CloudWatch Logs. You should see a total of eight processes
    running in parallel. Take note of the total training time and accuracy and see
    how they compare with the results you got from *Chapter 8*, *Building a Data Science
    Environment Using AWS ML Services*.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，只需在新笔记本中执行每个单元格，即可启动分布式训练。您可以直接在笔记本中跟踪训练状态，并在CloudWatch日志中查看详细状态。您应该看到总共八个进程并行运行。请注意总训练时间和准确率，并查看它们与您在*第8章*，*使用AWS
    ML服务构建数据科学环境*中得到的成果相比如何。
- en: Congratulations! You have successfully trained a BERT model using the PyTorch
    distributed training library.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经成功使用PyTorch分布式训练库训练了一个BERT模型。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we delved into the advanced topics of ML engineering. We covered
    distributed training for handling extensive datasets and large-scale models, along
    with strategies for achieving low-latency inference. Hopefully, you now have a
    solid understanding of data parallelism and model parallelism, as well as the
    diverse technology choices available, such as the PyTorch distributed library
    and SageMaker distributed training library, for implementing distributed training
    using these approaches. Additionally, you should be well equipped to discuss various
    techniques for optimizing models to minimize inference latency, including the
    utilization of model compiler tools designed for automated model optimization.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了机器学习工程的先进主题。我们涵盖了处理大量数据集和大规模模型的分布式训练，以及实现低延迟推理的策略。希望你现在对数据并行和模型并行有了坚实的理解，以及实现分布式训练时可供选择的多样化技术，例如PyTorch分布式库和SageMaker分布式训练库。此外，你应该已经准备好讨论各种优化模型以最小化推理延迟的技术，包括用于自动化模型优化的模型编译工具的使用。
- en: So far, we have focused on training ML models from scratch and designing ML
    platforms for the training and deployment of ML models to support the development
    of intelligent applications. However, we don’t always need to build models from
    scratch. In the next chapter, we will explore ready-to-use AI services and see
    how AI services can be used to build intelligent applications quickly.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于从头开始训练机器学习模型，并设计机器学习平台以支持机器学习模型的训练和部署，从而支持智能应用的开发。然而，我们并不总是需要从头开始构建模型。在下一章中，我们将探讨现成的AI服务，并了解AI服务如何被用来快速构建智能应用。
- en: Leave a review!
  id: totrans-299
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 留下评论！
- en: Enjoying this book? Help readers like you by leaving an Amazon review. Scan
    the QR code below to get a free eBook of your choice.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 喜欢这本书吗？通过留下亚马逊评论来帮助像你这样的读者。扫描下面的二维码，获取你选择的免费电子书。
- en: '![](img/Review_Copy.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![评论副本](img/Review_Copy.png)'
- en: '**Limited Offer*'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: '**限时优惠*'
