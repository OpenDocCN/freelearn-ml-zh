- en: Exploring Structure from Motion Using OpenCV
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用OpenCV探索运动结构
- en: In this chapter, we will discuss the notion of **Structure from Motion** (**SfM**),or
    better put, extracting geometric structures from images taken with a camera under
    motion, using OpenCV's API to help us. First, let's constrain the otherwise very
    b road approach to SfM using a single camera, usually called a **monocular** approach,
    and a discrete and sparse set of frames rather than a continuous video stream.
    These two constrains will greatly simplify the system we will sketch out in the
    coming pages, and help us understand the fundamentals of any SfM method. To implement
    our method, we will follow in the footsteps of Hartley and Zisserman (hereafter
    referred to as H&Z, for brevity), as documented in Chapters 9 through 12 of their
    seminal book *Multiple View Geometry in Computer Vision*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论**运动结构**（**SfM**）的概念，或者更确切地说，使用OpenCV的API帮助我们从相机运动中提取几何结构。首先，让我们将使用单相机（通常称为**单目**方法）和离散且稀疏的帧集而不是连续视频流来约束SfM的非常宽泛的方法。这两个约束将极大地简化我们在接下来的页面中将要绘制的系统，并帮助我们理解任何SfM方法的基本原理。为了实现我们的方法，我们将遵循Hartley和Zisserman（以下简称H&Z）的步伐，正如他们在其开创性著作《计算机视觉中的多视图几何》的第9章至第12章中所记录的。
- en: 'In this chapter, we will cover the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下内容：
- en: Structure from Motion concepts
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运动结构概念
- en: Estimating the camera motion from a pair of images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一对图像中估计相机运动
- en: Reconstructing the scene
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重建场景
- en: Reconstructing from many views
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多个视角重建
- en: Refining the reconstruction
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化重建
- en: Throughout the chapter, we assume the use of a calibrated camera, one that was
    calibrated beforehand. *Calibration* is a ubiquitous operation in Computer Vision,
    fully supported in OpenCV using command-line tools, and was discussed in previous
    chapters. We, therefore, assume the existence of the camera's **intrinsic parameters**
    embodied in the K matrix and distortionn coefficients vector - the outputs from
    the calibration process.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们假设使用的是校准过的相机，即事先已经校准过的相机。*校准*是计算机视觉中的一个普遍操作，在OpenCV中通过命令行工具完全支持，并在前几章中进行了讨论。因此，我们假设存在相机内参参数，这些参数体现在K矩阵和畸变系数向量中——这是校准过程的输出。
- en: To make things clear in terms of language, from this point on, we will refer
    to a camera as a single view of the scene rather than to the optics and hardware
    taking the image. A camera has a 3D position in space (translation) and a 3D direction
    of view (orientation). In general, we describe this as the 6 **Degree of Freedom**
    (**DOF**) camera pose, sometimes referred to as **extrinsic parameters**. Between
    two cameras, therefore, there is a 3D translation element (movement through space)
    and a 3D rotation of the direction of view.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在语言上明确，从现在开始，我们将把相机称为场景的单个视图，而不是成像的光学和硬件。相机在空间中有一个3D位置（平移）和一个3D观察方向（方向）。通常，我们将其描述为具有6个**自由度**（**DOF**）的相机姿态，有时也称为**外参数**。因此，在两个相机之间，存在一个3D平移元素（空间中的移动）和一个观察方向的3D旋转。
- en: We will also unify the terms for the point in the scene, world, real, or 3D
    to be the same thing, a point that exists in our real world. The same goes for
    points in an image or 2D, which are points in the image coordinates of some real
    3D point that was projected on the camera sensor at that location and time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将统一场景中的点、世界、真实或3D的术语，它们都是指存在于我们真实世界中的点。同样，图像中的点或2D点也是指图像坐标中的点，这些点是在该位置和时间被投影到相机传感器上的某个真实3D点的图像坐标。
- en: In the chapter's code sections, you will notice references to *Multiple View
    Geometry in Computer Vision*, for example `// HZ 9.12`. This refers to equation
    number 12 of Chapter 9 of the book. Also, the text will include excerpts of code
    only; while the complete runnable code is included in the material accompanied
    with the book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的代码部分，您将注意到对*计算机视觉中的多视图几何*的引用，例如`// HZ 9.12`。这指的是该书的第9章的第12个方程。此外，文本将包括代码摘录；而完整的可运行代码包含在随书附带的材料中。
- en: 'The following flow diagram describes the process in the SfM pipeline we will
    implement. We begin by triangulating an initial reconstructed point cloud of the
    scene, using 2D features matched across the image set and a calculation of two
    camera poses. We then add more views to the reconstruction by matching more points
    into the forming point cloud, calculating camera poses and triangulating their
    matching points. In between, we will also perform bundle adjustment to minimize
    the error in the reconstruction. All the steps are detailed in the next sections
    of this chapter, with relevant code excerpts, pointers to useful OpenCV functions,
    and mathematical reasoning:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的流程图描述了我们将要实现的SfM流程中的过程。我们首先通过在图像集中匹配的二维特征和两个摄像头的位姿计算，对场景的初始重建点云进行三角测量。然后，我们通过将更多点匹配到正在形成的点云中，计算摄像头位姿并对它们的匹配点进行三角测量，来添加更多视图到重建中。在此之间，我们还将执行捆绑调整以最小化重建中的误差。所有步骤都在本章的下一节中详细说明，包括相关的代码片段、指向有用的OpenCV函数的指针和数学推理：
- en: '![](img/B05389_03_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05389_03_01.png)'
- en: Structure from Motion concepts
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运动结构概念
- en: The first discrimination we should make is the difference between stereo (or
    indeed any multiview) and 3D reconstruction using calibrated rigs and SfM. A rig
    of two or more cameras assumes that we already know the *motion* between the cameras,
    while in SfM, we don't know what this motion is and we wish to find it. Calibrated
    rigs, from a simplistic point of view, allow a much more accurate reconstruction
    of 3D geometry because there is no error in estimating the distance and rotation
    between the cameras, it is already known. The first step in implementing an SfM
    system is finding the motion between the cameras. OpenCV may help us in a number
    of ways to obtain this motion, specifically using the `findFundamentalMat` and
    `findEssentialMat` functions.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该区分的是立体（或实际上任何多视图）与使用校准设备和运动结构（SfM）进行的三维重建之间的区别。由两个或更多摄像头组成的设备假设我们已经知道了摄像头之间的*运动*，而在SfM中，我们并不知道这种运动是什么，我们希望找到它。从简单观点来看，校准设备允许更精确地重建三维几何形状，因为没有误差在估计摄像头之间的距离和旋转，这些信息已经已知。实现SfM系统的第一步是找到摄像头之间的运动。OpenCV可以通过多种方式帮助我们获得这种运动，特别是使用`findFundamentalMat`和`findEssentialMat`函数。
- en: 'Let''s think for one moment of the goal behind choosing an SfM algorithm. In
    most cases, we wish to obtain the geometry of the scene, for example, where objects
    are in relation to the camera and what their form is. Having found the motion
    between the cameras picturing the same scene, from a reasonably similar point
    of view, we would now like to reconstruct the geometry. In Computer Vision jargon,
    this is known as **triangulation**, and there are plenty of ways to go about it.
    It may be done by way of ray intersection, where we construct two rays-one from
    each camera''s center of projection and a point on each of the image planes. The
    intersection of these rays in space will, ideally, intersect at one 3D point in
    the real world that is imaged in each camera, as shown in the following diagram:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们思考一下选择SfM算法背后的目标。在大多数情况下，我们希望获得场景的几何形状，例如，物体相对于摄像头的位置以及它们的形状。在找到描绘相同场景、从合理相似的角度拍摄的两个摄像头之间的运动后，我们现在希望重建几何形状。在计算机视觉术语中，这被称为**三角测量**，并且有众多方法可以实现。这可能通过射线交点来完成，其中我们构建两条射线——一条来自每个摄像头的投影中心，以及每个图像平面上的一点。这些射线在空间中的交点理想情况下将交于一个3D点，该点在每个摄像头中成像，如图所示：
- en: '![](img/B05389_04_30-1.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05389_04_30-1.jpg)'
- en: In reality, ray intersection is highly unreliable; H&Z recommend against it.
    This is because the rays usually do not intersect, making us fall back to using
    the middle point on the shortest segment connecting the two rays. OpenCV contains
    a simple API for a more accurate form of triangulation--the `triangulatePoints`
    function--so we do not need to code this part on our own.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实中，射线交点非常不可靠；H&Z建议不要使用它。这是因为射线通常不会相交，这使我们不得不退回到使用连接两条射线的最短线段的中点。OpenCV包含一个用于更精确形式三角测量的简单API——`triangulatePoints`函数，因此我们不需要自己编写这部分代码。
- en: After you learn how to recover 3D geometry from two views, we will see how we
    can incorporate more views of the same scene to get an even richer reconstruction.
    At that point, most SfM methods try to optimize the bundle of estimated positions
    of our cameras and 3D points by means of **Bundle Adjustment**, in the *Refinement
    of the reconstruction* section. OpenCV contains the means for Bundle Adjustment
    in its new Image Stitching Toolbox. However, the beauty of working with OpenCV
    and C++ is the abundance of external tools that can be easily integrated into
    the pipeline. We will, therefore, see how to integrate an external bundle adjuster,
    the Ceres nonlinear optimization package.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在你学习了如何从两个视图中恢复 3D 几何形状之后，我们将看到如何将同一场景的更多视图纳入以获得更丰富的重建。在那个阶段，大多数 SfM 方法试图通过在
    *重建细化* 部分中通过 **捆绑调整** 来优化我们相机的估计位置和 3D 点的集合。OpenCV 在其新的图像拼接工具箱中包含了捆绑调整的手段。然而，与
    OpenCV 和 C++ 一起工作的美妙之处在于，有大量的外部工具可以轻松集成到流程中。因此，我们将看到如何集成外部捆绑调整器，即 Ceres 非线性优化包。
- en: Now that we have sketched an outline of our approach to SfM using OpenCV, we
    will see how each element can be implemented.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经概述了使用 OpenCV 进行 SfM 的方法，我们将看到每个元素是如何实现的。
- en: Estimating the camera motion from a pair of images
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从一对图像中估计相机运动
- en: Before we set out to actually find the motion between two cameras, let's examine
    the inputs and the tools we have at hand to perform this operation. First, we
    have two images of the same scene from (hopefully not extremely) different positions
    in space. This is a powerful asset, and we will make sure that we use it. As for
    tools, we should take a look at mathematical objects that impose constraints over
    our images, cameras, and the scene.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们着手实际找到两个相机之间的运动之前，让我们检查一下我们执行此操作时拥有的输入和工具。首先，我们有来自（希望不是极端的）不同空间位置的同一场景的两个图像。这是一个强大的资产，我们将确保我们使用它。至于工具，我们应该看看那些对图像、相机和场景施加约束的数学对象。
- en: 'Two very useful mathematical objects are the fundamental matrix (denoted by
    `F`) and the essential matrix (denoted by `E`), which impose a constraint over
    corresponding 2D points in two images of the scene. They are mostly similar, except
    that the essential matrix is assuming usage of calibrated cameras; this is the
    case for us, so we will choose it. OpenCV allows us to find the fundamental matrix
    via the `findFundamentalMat` function and the essential matrix via the `findEssentialMatrix`
    function. Finding the essential matrix can be done as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 两个非常有用的数学对象是基本矩阵（用 `F` 表示）和基本矩阵（用 `E` 表示），它们对场景的两个图像中的对应 2D 点施加约束。它们大部分是相似的，除了基本矩阵假设使用校准过的相机；对我们来说就是这样，所以我们将选择它。OpenCV
    允许我们通过 `findFundamentalMat` 函数找到基本矩阵，通过 `findEssentialMatrix` 函数找到基本矩阵。找到基本矩阵的方法如下：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This function makes use of matching points in the left-hand side image, `leftPoints`,
    and right-hand side image, `rightPoints`, which we will discuss shortly, as well
    as two additional pieces of information from the camera''s calibration: the focal
    length, `focal`, and principal point, `pp`.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数利用左侧图像中的匹配点 `leftPoints` 和右侧图像中的匹配点 `rightPoints`（我们将在稍后讨论），以及来自相机校准的额外两条信息：焦距
    `focal` 和主点 `pp`。
- en: 'The essential matrix E is a 3x3 sized matrix, which imposes the following constraint
    on a point *x *in one image and a point and a point *x'' *corresponding image:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 基本矩阵 \( E \) 是一个 3x3 的大矩阵，它对一个图像中的点 \( x \) 和对应图像中的点 \( x' \) 施加以下约束：
- en: '*x''K^TEKx = 0*'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: \( x'K^TEKx = 0 \)
- en: Here, *K* is the calibration matrix.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，\( K \) 是校准矩阵。
- en: This is extremely useful, as we are about to see. Another important fact we
    use is that the essential matrix is all we need in order to recover the two cameras'
    positions from our images, although only up to an arbitrary unit of scale. So,
    if we obtain the essential matrix, we know where each camera is positioned in
    space, and where it is looking. We can easily calculate the matrix if we have
    enough of those constraint equations, simply because each equation can be used
    to solve for a small part of the matrix. In fact, OpenCV internally calculates
    it using just five point-pairs, but through the **Random Sample Consensus algorithm
    (RANSAC)**, many more pairs can be used and they make for a more robust solution.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常实用，正如我们即将看到的。我们使用的另一个重要事实是，基本矩阵是我们从图像中恢复两个相机位置所需的所有内容，尽管仅限于任意单位尺度。因此，如果我们获得了基本矩阵，我们就知道每个相机在空间中的位置以及它所朝向的方向。如果我们有足够的约束方程，我们可以轻松地计算出这个矩阵，因为每个方程都可以用来求解矩阵的一部分。实际上，OpenCV内部仅使用五个点对来计算它，但通过**随机样本一致性算法（RANSAC**），可以使用更多的点对，这提供了更鲁棒的解决方案。
- en: Point matching using rich feature descriptors
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用丰富特征描述符的点匹配
- en: Now, we will make use of our constraint equations to calculate the essential
    matrix. To get our constraints, remember that for each point in image A, we must
    find a corresponding point in image B. We can achieve such a matching using OpenCV's
    extensive 2D feature-matching framework, which has greatly matured in the past
    few years.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将利用我们的约束方程来计算基本矩阵。为了获取我们的约束，请记住，对于图像A中的每个点，我们必须在图像B中找到一个相应的点。我们可以使用OpenCV广泛的2D特征匹配框架来实现这种匹配，该框架在过去几年中已经得到了很大的发展。
- en: Feature extraction and descriptor matching is an essential process in Computer
    Vision, and is used in many methods to perform all sorts of operations, for example,
    detecting the position and orientation of an object in an image or searching a
    big database of images for similar images through a given query. In essence, *feature
    extraction* means selecting points in the image that would make for good features
    and computing a descriptor for them. A *descriptor* is a vector of numbers that
    describes the surrounding environment around a feature point in an image. Different
    methods have different length and data types for their descriptor vectors. **Descriptor
    Matching** is the process of finding a corresponding feature of one set in another
    using its descriptor. OpenCV provides very easy and powerful methods to support
    feature extraction and matching.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取和描述符匹配是计算机视觉中的基本过程，并被用于许多方法来执行各种操作，例如，检测图像中物体的位置和方向，或者通过给定的查询在大型图像数据库中搜索相似图像。本质上，*特征提取*意味着在图像中选择将构成良好特征的点，并为它们计算描述符。*描述符*是一个描述图像中特征点周围环境的数字向量。不同的方法有不同的描述符向量的长度和数据类型。**描述符匹配**是使用其描述符在另一集中找到对应特征的过程。OpenCV提供了非常简单且强大的方法来支持特征提取和匹配。
- en: 'Let''s examine a very simple feature extraction and matching scheme:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考察一个非常简单的特征提取和匹配方案：
- en: '[PRE1]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You may have already seen similar OpenCV code, but let''s review it quickly.
    Our goal is to obtain three elements: feature points for two images, descriptors
    for them, and a matching between the two sets of features. OpenCV provides a range
    of feature detectors, descriptor extractors, and matchers. In this simple example,
    we use the ORB class to get both the 2D location of **Oriented BRIEF (ORB)**(where,
    **BRIEF** stands for **Binary Robust Independent Elementary Features**) feature
    points and their respective descriptors. ORB may be preferred over traditional
    2D features such as the **Speeded-Up Robust Features (SURF)** or **Scale Invariant
    Feature Transform (SIFT)** because it is unencumbered with intellectual property
    and shown to be faster to detect, compute, and match.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经看到了类似的OpenCV代码，但让我们快速回顾一下。我们的目标是获得三个元素：两张图像的特征点、它们的描述符以及这两组特征之间的匹配。OpenCV提供了一系列特征检测器、描述符提取器和匹配器。在这个简单的例子中，我们使用ORB类来获取**Oriented
    BRIEF (ORB**)（其中，**BRIEF**代表**二进制鲁棒独立基本特征**)特征点的2D位置以及它们各自的描述符。ORB可能比传统的2D特征（如**加速鲁棒特征（SURF**）或**尺度不变特征变换（SIFT**）更受欢迎，因为它不受知识产权的约束，并且已被证明在检测、计算和匹配方面更快。
- en: We use a *bruteforce* binary matcher to get the matching, which simply matches
    two feature sets by comparing each feature in the first set to each feature in
    the second set (hence the phrasing *bruteforce*).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个*暴力*二进制匹配器来获取匹配，它简单地通过比较第一组中的每个特征与第二组中的每个特征来匹配两个特征集（因此称为*暴力*）。
- en: 'In the following image, we will see a matching of feature points on two images
    from the Fountain P11 sequence can be found at [h t t p ://c v l a b . e p f l
    . c h /~s t r e c h a /m u l t i v i e w /d e n s e M V S . h t m l](http://cvlab.epfl.ch/~strecha/multiview/denseMVS.html):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图像中，我们将看到来自Fountain P11序列的两张图像上的特征点匹配可以在[http://cvlab.epfl.ch/~strecha/multiview/denseMVS.html](http://cvlab.epfl.ch/~strecha/multiview/denseMVS.html)找到：
- en: '![](img/B05389_04_01.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05389_04_01.jpg)'
- en: Practically, raw matching like we just performed is good only up to a certain
    level, and many matches are probably erroneous. For that reason, most SfM methods
    perform some form of filtering on the matches to ensure correctness and reduce
    errors. One form of filtering, which is built into OpenCV's brute-force matcher,
    is **cross-check filtering**. That is, a match is considered true if a feature
    of the first image matches a feature of the second image, and the reverse check
    also matches the feature of the second image with the feature of the first image.
    Another common filtering mechanism, used in the provided code, is to filter based
    on the fact that the two images are of the same scene and have a certain stereo-view
    relationship between them. In practice, the filter tries to robustly calculate
    the fundamental or essential matrix which we will learn about in the *Finding
    camera matrices* section and retain those feature pairs that correspond with this
    calculation with small errors.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们刚才进行的原始匹配只适用于一定水平，并且许多匹配可能是错误的。因此，大多数SfM方法都会对匹配进行某种形式的过滤，以确保正确性并减少错误。一种内置在OpenCV暴力匹配器中的过滤形式是**交叉检查过滤**。也就是说，如果一个图像的特征与另一个图像的特征匹配，并且反向检查也匹配第二个图像的特征与第一个图像的特征，那么这个匹配被认为是真实的。另一种常见的过滤机制，在提供的代码中使用，是基于两个图像是同一场景并且它们之间存在某种立体视觉关系的实际情况。在实践中，过滤器试图稳健地计算基础矩阵或本质矩阵，我们将在*寻找相机矩阵*部分学习到这些内容，并保留与这种计算有微小误差的特征对。
- en: An alternative to using rich features, such as ORB, is to use **optical flow**.
    The following information box provides a short overview of optical flow. It is
    possible to use optical flow instead of descriptor matching to find the required
    point matching between two images, while the rest of the SfM pipeline remains
    the same. OpenCV recently extended its API to get the flow field from two images
    and now it is faster and more powerful.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 使用如ORB等丰富特征的一种替代方法是使用**光流**。以下信息框提供了一个关于光流的简要概述。可以使用光流代替描述符匹配来在两张图像之间找到所需的关键点匹配，而SfM管道的其他部分保持不变。OpenCV最近扩展了其API，可以从两张图像中获取光流场，现在它更快、更强大。
- en: '**Optical flow** is the process of matching selected points from one image
    to another, assuming both images are part of a sequence and relatively close to
    one another. Most optical flow methods compare a small region, known as the **search
    window** or patch, around each point from *image A* to the same area in *image
    B*. Following a very common rule in Computer Vision, called the **brightness constancy
    constraint** (and other names), the small patches of the image will not change
    drastically from one image to the other, and therefore the magnitude of their
    subtraction should be close to zero. In addition to matching patches, newer methods
    of optical flow use a number of additional methods to get better results. One
    is using image pyramids, which are smaller and smaller resized versions of the
    image, which allow for working *from-coarse-to-fine*, a very well-used trick in
    Computer Vision. Another method is to define global constraints on the flow field,
    assuming that the points close to each other move together in the same direction.
    A more in-depth review of optical flow methods in OpenCV can be found in a chapter
    named *Developing Fluid Wall Using the Microsoft Kinect* which is available on
    the Packt website.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**光流**是将一个图像中选定的点匹配到另一个图像的过程，假设这两个图像是序列的一部分，并且彼此相对较近。大多数光流方法比较每个点周围的一个小区域，称为**搜索窗口**或补丁，从*图像A*到*图像B*中的相同区域。遵循计算机视觉中一个非常常见的规则，称为**亮度恒常约束**（以及其他名称），图像的小补丁不会从一个图像剧烈变化到另一个图像，因此它们减法的结果的幅度应该接近于零。除了匹配补丁之外，较新的光流方法使用一些额外的技术来获得更好的结果。其中一种是使用图像金字塔，这是图像的越来越小的缩放版本，允许从粗到细地工作，这是计算机视觉中一个非常常用的技巧。另一种方法是定义流场的全局约束，假设彼此靠近的点以相同方向移动。在Packt网站上可以找到关于OpenCV中光流方法的更深入回顾，章节名为*Developing
    Fluid Wall Using the Microsoft Kinect*。'
- en: Finding camera matrices
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找相机矩阵
- en: 'Now that we have obtained matches between keypoints, we can calculate the essential
    matrix. However, we must first align our matching points into two arrays, where
    an index in one array corresponds to the same index in the other. This is required
    by the `findEssentialMat` function as we''ve seen in the *Estimating Camera Motion *section.
    We would also need to convert the `KeyPoint` structure to a `Point2f` structure.
    We must pay special attention to the `queryIdx` and `trainIdx` member variables
    of `DMatch`, the OpenCV struct that holds a match between two keypoints, as they
    must align with the way we used the `DescriptorMatcher::match()` function. The
    following code section shows how to align a matching into two corresponding sets
    of 2D points, and how these can be used to find the essential matrix:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了关键点的匹配，我们可以计算基础矩阵。然而，我们首先必须将匹配点对齐到两个数组中，其中一个数组中的索引与另一个数组中的相同索引相对应。这是由我们在“估计相机运动”部分看到的`findEssentialMat`函数所要求的。我们还需要将`KeyPoint`结构转换为`Point2f`结构。我们必须特别注意`DMatch`成员变量`queryIdx`和`trainIdx`，因为它们必须与我们在使用`DescriptorMatcher::match()`函数时的方式相匹配。以下代码部分展示了如何将匹配对齐到两个对应的二维点集，以及如何使用这些点集来找到基础矩阵：
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We may, later, use the `status` binary vector to prune those points that align
    with the recovered essential matrix. Look at the following image for an illustration
    of point matching after pruning. The red arrows mark feature matches that were
    removed in the process of finding the matrix, and the green arrows are feature
    matches that were retained:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能稍后使用`status`二进制向量来修剪与恢复的基础矩阵对齐的点。查看以下图像以了解修剪后的点匹配的说明。红色箭头标记了在寻找矩阵过程中被移除的特征匹配，绿色箭头是保留的特征匹配：
- en: '![](img/B05389_04_21.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05389_04_21.jpg)'
- en: 'Now we are ready to find the camera matrices. This process is described at
    length in a chapter of H&Z''s book; however, the new OpenCV 3 API makes things
    very easy for us by introducing the `recoverPose` function. First, we will briefly
    examine the structure of the camera matrix we are going to use:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好找到相机矩阵。这个过程在H&Z的书中被详细描述；然而，新的OpenCV 3 API通过引入`recoverPose`函数，使我们能够非常容易地完成这个任务。首先，我们将简要检查我们将要使用的相机矩阵的结构：
- en: '![](img/B05389_04_18.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05389_04_18.png)'
- en: 'This is the model for our camera pose, which consists of two elements: rotation
    (denoted by **R**) and translation (denoted by **t**). The interesting thing is
    that it holds a very essential equation: *x = PX*, where *x* is a 2D point on
    the image and *X* is a 3D point in space. There is more to it, but this matrix
    gives us a very important relationship between the image points and the scene
    points. So, now that we have a motivation for finding the camera matrices, we
    will see how it can be done. The following code section shows how to decompose
    the essential matrix into the rotation and translation elements:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的相机姿态模型，它由两个元素组成：旋转（用**R**表示）和平移（用**t**表示）。有趣的是，它包含一个非常关键的方程：*x = PX*，其中*x*是图像上的一个2D点，*X*是空间中的一个3D点。还有更多内容，但这个矩阵给我们提供了图像点和场景点之间的重要关系。因此，既然我们已经有了寻找相机矩阵的动机，我们将看到它是如何实现的。以下代码部分展示了如何将基础矩阵分解为旋转和平移元素：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Very simple. Without going too deeply into the mathematical interpretation,
    this conversion of the essential matrix to rotation and translation is possible
    because the essential matrix was originally composed by these two elements. Strictly
    for satisfying our curiosity, we can look at the following equation for the essential
    matrix, which appears in the literature: *E=[t][x]R*. We see it is composed of
    (some form of) a translation element *t* and a rotational element *R*.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 非常简单。不深入数学解释，将基础矩阵转换为旋转和平移是可能的，因为基础矩阵最初是由这两个元素组成的。严格来说，为了满足我们的好奇心，我们可以查看文献中出现的以下关于基础矩阵的方程：*E=[t][x]R*。我们看到它由（某种形式的）平移元素*t*和旋转元素*R*组成。
- en: Note that a *cheirality check* is internally performed in the `recoverPose`
    function. The cheirality check makes sure that all triangulated 3D points are
    *in front* of the reconstructed camera. H&Z show that camera matrix recovery from
    the essential matrix has in fact four possible solutions, but the only correct
    solution is the one that will produce triangulated points in front of the camera,
    hence the need for a cheirality check. We will learn about triangulation and 3D
    reconstruction in the next section.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`recoverPose`函数中会内部执行一个**手性检查**。手性检查确保所有三角化的3D点都位于重建相机的**前方**。H&Z表明，从基础矩阵恢复相机矩阵实际上有四种可能的解，但唯一正确的解是能够产生位于相机前方的三角化点的解，因此需要进行手性检查。我们将在下一节学习三角化和3D重建。
- en: 'Note what we just did only gives us one camera matrix, and for triangulation,
    we require two camera matrices. This operation assumes that one camera matrix
    is fixed and canonical (no rotation and no translation, placed at the *world origin*):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们刚才所做的只给我们提供了一个相机矩阵，而对于三角化，我们需要两个相机矩阵。这个操作假设一个相机矩阵是固定的和规范的（没有旋转和没有平移，放置在**世界原点**）：
- en: '![](img/B05389_04_19.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05389_04_19.png)'
- en: The other camera that we recovered from the essential matrix has moved and rotated
    in relation to the fixed one. This also means that any of the 3D points that we
    recover from these two camera matrices will have the first camera at the world
    origin point (0, 0, 0). The assumption of a canonical camera is just how `cv::recoverPose`
    works; however in other situations, the *origin* camera pose matrix may be different
    than the canonical and still be valid for 3D points' triangulation, as we will
    see later when we will not use `cv::recoverPose` to get a new camera pose matrix.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基础矩阵中恢复的另一个相机相对于固定的相机已经移动并旋转。这也意味着，我们从这两个相机矩阵中恢复的任何3D点都将具有第一个相机位于世界原点（0,
    0, 0）。规范相机的假设正是`cv::recoverPose`的工作方式；然而，在其他情况下，**原点**相机的姿态矩阵可能不同于规范，但仍适用于3D点的三角化，正如我们稍后将看到的那样，当我们不使用`cv::recoverPose`来获取新的相机姿态矩阵时。
- en: 'One more thing we can think of adding to our method is error checking. Many
    times, the calculation of an essential matrix from point matching is erroneous,
    and this affects the resulting camera matrices. Continuing to triangulate with
    faulty camera matrices is pointless. We can install a check to see if the rotation
    element is a valid rotation matrix. Keeping in mind that rotation matrices must
    have a determinant of 1 (or -1), we can simply do the following:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以考虑添加到我们方法中的一个额外内容是错误检查。很多时候，从点匹配计算基础矩阵是错误的，这会影响结果的相机矩阵。继续使用有缺陷的相机矩阵进行三角化是没有意义的。我们可以安装一个检查来查看旋转元素是否是有效的旋转矩阵。考虑到旋转矩阵必须有一个行列式为1（或-1），我们可以简单地做以下操作：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Think of `EPS` (from Epsilon) as a very small number that helps us cope with
    numerical calculation limits of our CPU. In reality, we may define the following
    in code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 将`EPS`（来自Epsilon）想象成一个非常小的数，它帮助我们应对CPU的数值计算限制。实际上，我们可能在代码中定义如下：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can now see how all these elements combine into a function that recovers
    the `P` matrices. First, we will introduce some convenience data structures and
    type shorthand:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以看到所有这些元素是如何组合成一个恢复`P`矩阵的函数。首先，我们将介绍一些便利的数据结构和类型简写：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we can write the camera matrix finding function:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写相机矩阵查找函数：
- en: '[PRE7]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: At this point, we have the two cameras that we need in order to reconstruct
    the scene. The canonical first camera in the `Pleft` variable, and the second
    camera we calculated form the essential matrix in the `Pright` variable.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们已经拥有了重建场景所需的两个相机。一个是`Pleft`变量中的标准第一相机，另一个是通过`Pright`变量计算出的从基础矩阵得到的第二相机。
- en: Choosing the image pair to use first
  id: totrans-65
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择首先使用的图像对
- en: Given we have more than just two image views of the scene, we must choose which
    two views we will start the reconstruction from. In their paper, *Snavely et al.* suggest
    to picking the two views that have the least number of **homography** inliers.
    A homography is a relationship between two images or sets of points that lie on
    a plane; the **homography matrix** defines the transformation from one plane to
    another. In case of an image or a set of 2D points, the homography matrix is of
    size 3x3.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们场景的图像视图不止两个，我们必须选择从哪个两个视图开始重建。在他们的论文中，*Snavely等人*建议选择具有最少单应性内点的两个视图。单应性是两个图像或位于平面上的点集之间的关系；**单应性矩阵**定义了一个平面到另一个平面的转换。对于图像或一组2D点，单应性矩阵的大小为3x3。
- en: When *Snavely et al.* look for the lowest inlier ratio, they essentially suggest
    that you calculate the homography matrix between all pairs of images and pick
    the pair whose points mostly do not correspond with the homography matrix. This
    means that the geometry of the scene in these two views is not planar, or at least,
    not the same plane in both views, which helps when doing 3D reconstruction. For
    reconstruction, it is best to look at a complex scene with non-planar geometry,
    with things closer and farther away from the camera.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当*Snavely等人*寻找最低的内点比率时，他们实际上建议你计算所有图像对之间的单应性矩阵，并选择那些点大部分不与单应性矩阵对应的对。这意味着这两个视图中的场景几何不是平面的，或者至少，两个视图中的平面不相同，这有助于进行三维重建。对于重建，最好查看一个具有非平面几何的复杂场景，其中物体距离相机远近不一。
- en: 'The following code snippet shows how to use OpenCV''s `findHomography` function
    to count the number of inliers between two views whose features were already extracted
    and matched:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何使用OpenCV的`findHomography`函数来计算两个已经提取并匹配的特征点视图之间的内点数：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The next step is to perform this operation on all pairs of image views in our
    bundle and sort them based on the ratio of homography inliers to outliers:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是对我们捆绑中的所有图像对视图执行此操作，并根据单应性内点与外点比率进行排序：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note that `std::map<float, ImagePair>` will internally sort the pairs based
    on the map''s key: the inliers ratio. We then simply need to traverse this map
    from the beginning to find the image pair with least inlier ratio, and if that
    pair cannot be used, we can easily skip ahead to the next pair. The next section
    will reveal how we use these cameras pair to obtain a 3D structure of the scene.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`std::map<float, ImagePair>`将内部根据映射的键：内点比率进行排序。然后我们只需从映射的开头遍历，找到内点比率最低的图像对，如果这对不能使用，我们可以轻松地跳到下一个对。下一节将揭示我们如何使用这些相机对来获得场景的3D结构。
- en: Reconstructing the scene
  id: totrans-73
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建场景
- en: Next, we look into the matter of recovering the 3D structure of the scene from
    the information we have acquired so far. As we had done before, we should look
    at the tools and information we have at hand to achieve this. In the preceding
    section, we obtained two camera matrices from the essential matrix; we already
    discussed how these tools would be useful for obtaining the 3D position of a point
    in space. Then, we can go back to our matched point pairs to fill in our equations
    with numerical data. The point pairs will also be useful in calculating the error
    we get from all our approximate calculations.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们探讨从我们迄今为止获得的信息中恢复场景的 3D 结构的问题。正如我们之前所做的那样，我们应该看看我们手头上的工具和信息，以实现这一目标。在前一节中，我们从基础矩阵中获得了两个相机矩阵；我们已经讨论了这些工具如何有助于获得空间中一点的
    3D 位置。然后，我们可以回到我们的匹配点对，用数值数据填充我们的方程。点对也将有助于计算我们从所有近似计算中得到的误差。
- en: 'This is the time to see how we can perform triangulation using OpenCV. Luckily,
    OpenCV supplies us with a number of functions that make this process easy to implement:
    `triangulatePoints`, `undistortPoints`, and `convertPointsFromHomogeneous`.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候看看我们如何使用 OpenCV 来执行三角测量了。幸运的是，OpenCV 为我们提供了一系列函数，使得这个过程易于实现：`triangulatePoints`、`undistortPoints`
    和 `convertPointsFromHomogeneous`。
- en: 'Remember we had two key equations arising from the 2D point matching and *P*
    matrices: *x=PX* and *x''= P''X*, where *x* and *x''* are matching 2D points and
    X is a real-world 3D point imaged by the two cameras. If we examine these equations,
    we will see that the x vector that represents a 2D point should be of size (*3x1*)
    and X that represents a 3D point should be (*4x1*). Both points received an extra
    entry in the vector; this is called **Homogeneous Coordinates**. We use these
    coordinates to streamline the triangulation process.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们有两个关键方程是从 2D 点匹配和 *P* 矩阵中产生的：*x=PX* 和 *x'= P'X*，其中 *x* 和 *x'* 是匹配的 2D 点，X
    是由两个相机成像的真实世界 3D 点。如果我们检查这些方程，我们会看到代表 2D 点的 x 向量应该大小为 (*3x1*)，而代表 3D 点的 X 应该是
    (*4x1*)。这两个点在向量中都有一个额外的条目；这被称为**齐次坐标**。我们使用这些坐标来简化三角测量过程。
- en: 'The equation *x = PX* (where *x* is a 2D image point, *X* is a world 3D point,
    and *P* is a camera matrix) is missing a crucial element: the camera calibration
    parameters matrix, *K*. The matrix K is used to transform 2D image points from
    pixel coordinates to **normalized coordinates** (in the [-1, 1] range) removing
    the dependency on the size of the image in pixels, which is absolutely necessary.
    For example, a 2D point *x[1] = (160, 120)* in a 320x240 image, may transform
    to *x[1]'' = (0, 0)* under certain circumstances. To that end, we use the `undistortPoints`
    function:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 方程式 *x = PX*（其中 *x* 是一个 2D 图像点，*X* 是一个世界 3D 点，而 *P* 是一个相机矩阵）缺少一个关键元素：相机标定参数矩阵，*K*。矩阵
    K 用于将 2D 图像点从像素坐标转换为**归一化坐标**（在 [-1, 1] 范围内），从而消除对图像像素大小的依赖，这是绝对必要的。例如，在 320x240
    图像中的一个 2D 点 *x[1] = (160, 120)*，在特定情况下可能会转换为 *x[1]' = (0, 0)*。为此，我们使用 `undistortPoints`
    函数：
- en: '[PRE10]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We are now ready to triangulate the normalized 2D image points into 3D world
    points:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已准备好将归一化的 2D 图像点三角测量为 3D 世界点：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following image, we can see a triangulation result of two images out
    of the Fountain P-11 sequence at [http://cvlabwww.epfl.ch/data/multiview/denseMVS.html](http://cvlabwww.epfl.ch/data/multiview/denseMVS.html).
    The two images at the top are the original two views of the scene, and the bottom
    pair is the view of the reconstructed point cloud from the two views, including
    the estimated cameras looking at the fountain. We can see how the right-hand side
    section of the red brick wall was reconstructed, and also the fountain that protrudes
    from the wall:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图像中，我们可以看到 Fountain P-11 序列中两幅图像的三角测量结果，[http://cvlabwww.epfl.ch/data/multiview/denseMVS.html](http://cvlabwww.epfl.ch/data/multiview/denseMVS.html)。顶部两幅图像是场景的原始两个视图，底部一对是两个视图重建的点云视图，包括观察喷泉的估计相机。我们可以看到红砖墙右侧部分的重建情况，以及从墙上突出的喷泉：
- en: '![](img/B05389_04_26.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05389_04_26.png)'
- en: However, as we discussed earlier, we have an issue with the reconstruction being
    only up to scale. We should take a moment to understand what up to scale means.
    The motion we obtained between our two cameras is going to have an arbitrary unit
    of measurement that is, it is not in centimeters or inches, but simply a given
    unit of scale. Our reconstructed cameras we will be one unit of scale distance
    apart. This has big implications, should we decide to recover more cameras later,
    as each pair of cameras will have their own units of scale, rather than a common
    one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们之前讨论的，我们有一个问题，即重建仅限于比例。我们应该花点时间理解“限于比例”的含义。我们在两个相机之间获得的运动将有一个任意的测量单位，即它不是以厘米或英寸为单位，而只是一个给定的比例单位。我们的重建相机将相隔一个比例单位距离。如果我们决定稍后恢复更多相机，这将有很大的影响，因为每一对相机将有自己的比例单位，而不是一个共同的单位。
- en: 'We will now discuss how the error measure that we set up may help us in finding
    a more robust reconstruction. First, we should note that reprojection means we
    simply take the triangulated 3D point and reimage it on a camera to get a reprojected
    2D point, we then compare the distance between the original 2D point and the reprojected
    2D point. If this distance is large, this means we may have an error in triangulation,
    so we may not want to include this point in the final result. Our global measure
    is the average reprojection distance and may give us a hint to how our triangulation
    performed overall. High average reprojection rates may point to a problem with
    the *P* matrices, and therefore a possible problem with the calculation of the
    essential matrix or the matched feature points. To reproject points, OpenCV offers
    the `projectPoints` function:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将讨论我们设置的误差度量如何帮助我们找到更鲁棒的重建方法。首先，我们应该注意，重投影意味着我们只需将三角化的3D点重新投影到相机上以获得重投影的2D点，然后我们比较原始2D点和重投影2D点之间的距离。如果这个距离很大，这意味着我们可能在三角化中存在误差，所以我们可能不想将这个点包含在最终结果中。我们的全局度量是平均重投影距离，可能会给我们一些关于我们的三角化整体表现的建议。高平均重投影率可能表明*P*矩阵存在问题，因此可能存在计算基础矩阵或匹配特征点的问题。为了重投影点，OpenCV提供了`projectPoints`函数：
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Next, we will take a look at recovering more cameras looking at the same scene,
    and combining the 3D reconstruction results.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将探讨恢复更多观察同一场景的相机，并组合3D重建结果。
- en: Reconstruction from many views
  id: totrans-87
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从多个视图中进行重建
- en: Now that we know how to recover the motion and scene geometry from two cameras,
    it would seem simple to get the parameters of additional cameras and more scene
    points simply by applying the same process. This matter is in fact not so simple,
    as we can only get a reconstruction that is upto scale, and each pair of pictures
    has a different scale.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了如何从两个相机中恢复运动和场景几何形状，似乎通过应用相同的过程简单地获取额外相机的参数和更多场景点似乎是件简单的事。实际上，这个问题并不简单，因为我们只能得到一个到比例的重建，并且每一对图片都有不同的比例。
- en: There are a number of ways to correctly reconstruct the 3D scene data from multiple
    views. One way to achieve **camera pose estimation **or **camera resectioning**,
    is the **Perspective N-Point**(**PnP**) algorithm, where we try to solve for the
    position of a new camera using *N* 3D scene points, which we have already found
    and their respective 2D image points. Another way is to triangulate more points
    and see how they fit into our existing scene geometry; this will tell us the position
    of the new camera by means of **point cloud registration**. In this section, we
    will discuss using OpenCV's `solvePnP` functions that implements the first method.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 从多个视图中正确重建3D场景数据有几种方法。一种实现**相机姿态估计**或**相机重投影**的方法是**透视N点**(**PnP**)算法，其中我们尝试使用*N*个3D场景点（我们已找到并对应于2D图像点）来求解新相机的位置。另一种方法是三角化更多点，看看它们如何适应我们现有的场景几何形状；这将通过**点云配准**告诉我们新相机的位置。在本节中，我们将讨论使用OpenCV的`solvePnP`函数实现第一种方法。
- en: The first step we choose in this kind of reconstruction, incremental 3D reconstruction
    with camera resection, is to get a baseline scene structure. As we will look for
    the position of any new camera based on a known structure of the scene, we need
    to find an initial structure to work with. We can use the method we previously
    discussed-for example, between the first and second frames, to get a baseline
    by finding the camera matrices (using the `findCameraMatricesFromMatch` function)
    and triangulate the geometry (using `triangulatePoints`).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种重建中，我们选择的第一步是增量3D重建与相机重投影，目的是获取基线场景结构。因为我们将根据场景的已知结构寻找任何新相机的位置，所以我们需要找到一个初始结构来工作。我们可以使用之前讨论过的方法——例如，在第一帧和第二帧之间，通过找到相机矩阵（使用`findCameraMatricesFromMatch`函数）和三角化几何形状（使用`triangulatePoints`）来获取基线。
- en: Having found an initial structure, we may continue; however, our method requires
    quite a bit of bookkeeping. First we should note that the `solvePnP` function
    needs aligned vectors of 3D and 2D points. Aligned vectors mean that the ith position
    in one vector aligns with the i^(th) position in the other. To obtain these vectors
    we need to find those points among the 3D points that we recovered earlier, which
    align with the 2D points in our new frame. A simple way to do this is to attach,
    for each 3D point in the cloud, a vector denoting the 2D points it came from.
    We can then use feature matching to get a matching pair.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 找到初始结构后，我们可以继续；然而，我们的方法需要做大量的记录。首先我们应该注意，`solvePnP`函数需要3D和2D点的齐次向量。齐次向量意味着一个向量中的第i个位置与另一个向量中的第i^(th)个位置对齐。为了获得这些向量，我们需要找到我们之前恢复的3D点中的那些点，它们与我们的新帧中的2D点对齐。一种简单的方法是为云中的每个3D点附加一个表示它来自的2D点的向量。然后我们可以使用特征匹配来获取匹配对。
- en: 'Let''s introduce a new structure for a 3D point as follows:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下方式引入一个3D点的新结构：
- en: '[PRE13]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: It holds, on top of the 3D point, an index to the 2D point inside the vector
    of 2D points that each frame has, which had contributed to this 3D point. The
    information for `Point3DInMap::originatingViews` must be initialized when triangulating
    a new 3D point, recording which cameras were involved in the triangulation. We
    can then use it to trace back from our 3D point cloud to the 2D point in each
    frame.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在3D点之上，它还保留了一个指向向量中2D点的索引，该向量是每个帧贡献给这个3D点的。当三角化新的3D点时，必须初始化`Point3DInMap::originatingViews`的信息，记录哪些相机参与了三角化。然后我们可以用它来从我们的3D点云追踪到每个帧中的2D点。
- en: 'Let''s add some convenience definitions:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们添加一些便利的定义：
- en: '[PRE14]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, let''s see how to get aligned 2D-3D point vectors to use with `solvePnP`.
    The following code segment illustrates the process of finding 2D points in a new
    image from the existing 3D point cloud augmented with the originating 2D views.
    Simply put, the algorithm scans the existing 3D points in the cloud, looks at
    their originating 2D points, and tries to find a match (via the feature descriptors)
    to 2D points in the new image. If such a match is found, it may indicate that
    this 3D point also appears in the new image at a specific 2D point:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何获取用于`solvePnP`的齐次2D-3D点向量。以下代码段说明了从现有的3D点云（包含原始的2D视图）中找到新图像中2D点的过程。简单来说，算法扫描云中的现有3D点，查看它们的原始2D点，并尝试找到匹配（通过特征描述符）到新图像中的2D点。如果找到这样的匹配，这可能表明这个3D点也出现在新图像的特定2D点上：
- en: '[PRE15]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now we have aligned the pairing of 3D points in the scene to the 2D points
    in a new frame, and we can use them to recover the camera position as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将场景中的3D点对齐到新帧中的2D点，我们可以使用它们来恢复相机位置如下：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that we are using the `solvePnPRansac` function rather than the `solvePnP`
    function as it is more robust to outliers. Now that we have a new `P` matrix,
    we can simply use the `triangulatePoints` function as we did earlier and populate
    our point cloud with more 3D points.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用的是`solvePnPRansac`函数而不是`solvePnP`函数，因为它对异常值更鲁棒。现在我们有了新的`P`矩阵，我们可以简单地使用之前使用的`triangulatePoints`函数，并用更多的3D点填充我们的点云。
- en: 'In the following image, we see an incremental reconstruction of the Fountain-P11
    scene at [http://cvlabwww.epfl.ch/data/multiview/denseMVS.html](http://cvlabwww.epfl.ch/data/multiview/denseMVS.html),
    starting from the fourth image. The top-left image is the reconstruction after
    four images were used; the participating cameras are shown as red pyramids with
    a white line showing the direction. The other images show how more cameras add
    more points to the cloud:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图像中，我们看到的是 Fountain-P11 场景的增量重建，链接为 [http://cvlabwww.epfl.ch/data/multiview/denseMVS.html](http://cvlabwww.epfl.ch/data/multiview/denseMVS.html)，从第四张图像开始。左上角的图像是使用四张图像后的重建结果；参与重建的相机以红色金字塔的形式显示，白色线条表示方向。其他图像展示了增加更多相机如何向点云中添加更多点：
- en: '![](img/B05389_04_27.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B05389_04_27.png)'
- en: Refinement of the reconstruction
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 重建的细化
- en: One of the most important parts of an SfM method is refining and optimizing
    the reconstructed scene, also known as the process of **Bundle Adjustment** (**BA**).
    This is an optimization step where all the data we gathered is fitted to a monolithic
    model. Both the position of the recovered 3D points and the positions of the cameras
    are optimized, so re-projection errors are minimized. In other words, recovered
    3D points that are re-projected on the image are expected to lie close to the
    position of originating 2D feature points that generated them. The BA process
    we use will try to minimize this error for all 3D points together, making for
    a very big system of simultaneous linear equations with on the order of thousands
    of parameters.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: SfM 方法中最重要的部分之一是对重建场景进行精炼和优化，也称为**捆绑调整**（**BA**）。这是一个优化步骤，我们将收集到的所有数据拟合到一个单一模型中。我们优化了恢复的3D点的位置和相机的位置，以最小化重投影误差。换句话说，重新投影到图像上的恢复3D点应位于生成它们的原始2D特征点的位置附近。我们使用的BA过程将尝试最小化所有3D点的这个误差，从而形成一个包含数千个参数的非常大的同时线性方程组。
- en: We will implement a BA algorithm using the **Ceres** library, a well-known optimization
    package from Google. Ceres has built-in tools to help with BA, such as automatic
    differentiation and many flavors of linear and nonlinear optimization schemes,
    which result in less code and more flexibility.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用**Ceres**库实现BA算法，这是Google的一个知名优化包。Ceres内置了帮助进行BA的工具，例如自动微分和多种线性及非线性优化方案，这减少了代码量并增加了灵活性。
- en: To make things simple and easy to implement, we will make a few assumptions,
    whereas in a real SfM system, these things cannot be neglected. Firstly, we will
    assume a simple intrinsic model for our cameras, specifically that the focal length
    in *x* and *y* is the same and the center of projection is exactly the middle
    of the image. We further assume that all cameras share the same intrinsic parameters,
    meaning that the same camera takes all the images in the bundle with the exact
    configuration (for example, zoom). These assumptions greatly reduce the number
    of parameters to optimize, which in turn makes the optimization not only easier
    to code but also faster to converge.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使事情简单且易于实现，我们将做出一些假设，而在实际的SfM系统中，这些假设不能被忽视。首先，我们将假设我们的相机具有简单的内在模型，具体来说，*x*和*y*方向上的焦距相同，投影中心正好位于图像的中间。我们进一步假设所有相机共享相同的内在参数，这意味着相同的相机以精确的配置（例如，缩放）拍摄了捆绑中的所有图像。这些假设大大减少了需要优化的参数数量，从而使得优化不仅更容易编码，而且收敛速度更快。
- en: 'To start, we will model the *error function*, sometimes also called the **cost
    function**, which is, simply put, the way the optimization knows how good the
    new parameters are and also which way to go to get even better parameters. We
    can write the following functor that makes use of Ceres'' Auto Differentiation
    mechanism:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将建模*误差函数*，有时也称为**成本函数**，简单来说，这是优化知道新参数有多好以及如何获得更好的参数的方式。我们可以编写以下使用Ceres自动微分机制的functor：
- en: '[PRE17]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This functor calculates the deviation a 3D point has from its originating 2D
    point by re-projecting it using simplified extrinsic and intrinsic camera parameters.
    The error in *x* and *y* is saved as the residual, which guides the optimization.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这个functor通过使用简化的外在和内在相机参数重新投影3D点，计算该3D点与其原始2D点之间的偏差。*x*和*y*方向上的误差被保存为残差，它指导优化过程。
- en: There is quite a bit of additional code that goes into the BA implementation,
    but it primarily handles bookkeeping of cloud 3D points, originating 2D points,
    and their respective cameras. The readers may wish to review how this is done
    in the code attached to the book.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: BA 实现中包含了很多额外的代码，但它主要处理云 3D 点、起源 2D 点及其相应相机的簿记。读者可能希望查看书中附带的代码是如何实现这一点的。
- en: 'The following image shows the effects of BA. The two images on the left are
    the points of the point cloud before adjustment from two perspectives, and the
    images on the right show the optimized cloud. The change is quite dramatic, and
    many misalignments between points triangulated from different views are now mostly
    consolidated. We can also notice how the adjustment created a far better reconstruction
    of flat surfaces:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像显示了 BA 的影响。左侧的两个图像是调整前点云的视角点，右侧的图像显示了优化的云。变化非常显著，许多来自不同视角的三角化点之间的不匹配现在大部分已经合并。我们还可以注意到调整如何创建了一个远比以前更好的平面表面重建：
- en: '![](img/B05389_04_28.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B05389_04_28.png)'
- en: Using the example code
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用示例代码
- en: 'We can find the example code for SfM with the supporting material of this book.
    We will now see how we can build, run, and make use of it. The code makes use
    of **CMake**, a cross-platform build environment similar to Maven or SCons. We
    should also make sure that we have all the following prerequisites to build the
    application:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在本书的支持材料中找到 SfM 的示例代码。现在我们将看到如何构建、运行并使用它。该代码使用了 **CMake**，这是一个类似于 Maven
    或 SCons 的跨平台构建环境。我们还应该确保我们拥有以下所有先决条件来构建应用程序：
- en: OpenCV v3.0 or higher
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OpenCV v3.0 或更高版本
- en: Ceres v1.11 or higher
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ceres v1.11 或更高版本
- en: Boost v1.54 or higher
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boost v1.54 或更高版本
- en: First, we must set up the build environment. To that end, we may create a folder
    named `build` in which all build-related files will go; we will now assume that
    all command-line operations are within the `build/` folder, although the process
    is similar (up to the locations of the files) even if not using the `build` folder.
    We should also make sure that CMake can find boost and Ceres.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须设置构建环境。为此，我们可以在其中放置所有构建相关文件的文件夹中创建一个名为 `build` 的文件夹；我们现在假设所有命令行操作都在 `build/`
    文件夹内，尽管即使不使用 `build` 文件夹，过程也是相似的（直到文件的位置）。我们还应该确保 CMake 可以找到 boost 和 Ceres。
- en: 'If we are using Windows as the operating system, we can use Microsoft Visual
    Studio to build; therefore, we should run the following command:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 Windows 作为操作系统，我们可以使用 Microsoft Visual Studio 来构建；因此，我们应该运行以下命令：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'If we are using Linux, Mac OS, or another Unix-like operating system, we execute
    the following command:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用 Linux、Mac OS 或其他类 Unix 操作系统，我们将执行以下命令：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If we prefer to use XCode on Mac OS, execute the following command:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们更喜欢在 Mac OS 上使用 XCode，请执行以下命令：
- en: '[PRE20]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: CMake also has the ability to build macros for Eclipse, Codeblocks, and more.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: CMake 还具有为 Eclipse、Codeblocks 等构建宏的能力。
- en: After CMake is done creating the environment, we are ready to build. If we are
    using a Unix-like system, we can simply execute the `make` utility, else we should
    use our development environment's building process.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 CMake 完成创建环境后，我们就准备好构建了。如果我们使用类 Unix 系统，我们可以简单地执行 `make` 工具，否则我们应该使用我们的开发环境的构建过程。
- en: After the build has finished, we should be left with an executable named `ExploringSfM`,
    which runs the SfM process. Running it with no arguments
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完成后，我们应该得到一个名为 `ExploringSfM` 的可执行文件，该文件运行 SfM 过程。不带参数运行它
- en: 'will result in the following:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 将导致以下结果：
- en: '[PRE21]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: To execute the process over a set of images, we should supply a location on
    the drive to find image files. If a valid location is supplied, the process should
    start and we should see the progress and debug information on the screen. If no
    errors arise, the process will end with a message stating that the point cloud
    that arises from the images was saved to PLY files, which can be opened in most
    3D editing software.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 要在图像集上执行过程，我们应该提供驱动器上的位置以查找图像文件。如果提供了有效位置，则过程应开始，我们应在屏幕上看到进度和调试信息。如果没有错误发生，则过程将以一条消息结束，表明从图像中产生的点云已保存到
    PLY 文件中，这些文件可以在大多数 3D 编辑软件中打开。
- en: Summary
  id: totrans-132
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw how OpenCV v3 can help us approach Structure from Motion
    in a manner that is both simple to code and simple to understand. OpenCV v3's
    new API contains a number of useful functions and data structures that make our
    lives easier and also assist in a cleaner implementation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们看到了OpenCV v3如何帮助我们以简单编码和简单理解的方式接近运动结构（Structure from Motion）。OpenCV
    v3的新API包含了许多有用的函数和数据结构，使我们的工作更加轻松，并有助于更清晰的实现。
- en: However, the state-of-the-art SfM methods are far more complex. There are many
    issues we choose to disregard in favor of simplicity, and plenty more error examinations
    that are usually in place. Our chosen methods for the different elements of SfM
    can also be revisited. For one, H&Z propose a highly accurate triangulation method
    that minimizes the reprojection error in the image domain. Some methods even use
    the N-view triangulation once they understand the relationship between the features
    in multiple images.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，最先进的SfM方法要复杂得多。我们选择忽略许多问题以换取简单性，还有更多通常存在的错误检查。我们对SfM不同元素选择的方法也可以重新审视。例如，H&Z提出了一种高度精确的三角测量方法，该方法在图像域中最小化了重投影误差。一些方法甚至在理解了多张图像中特征之间的关系后，就使用N视图三角测量。 '
- en: If we would like to extend and deepen our familiarity with SfM, we will certainly
    benefit from looking at other open source SfM libraries. One particularly interesting
    project is libMV, which implements a vast array of SfM elements that may be interchanged
    to get the best results. There is a great body of work from University of Washington
    that provides tools for many flavors of SfM (Bundler and VisualSfM). This work
    inspired an online product from Microsoft called **PhotoSynth** and **123D Catch**
    from Adobe. There are many more implementations of SfM readily available online,
    and one must only search to find quite a lot of them.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要扩展和深化我们对结构光束测距（SfM）的了解，我们当然会从查看其他开源的SfM库中受益。一个特别有趣的项目是libMV，它实现了一系列可互换的SfM元素，以获得最佳结果。华盛顿大学有一大批研究成果，提供了许多SfM（Bundler和VisualSfM）的工具。这项工作启发了微软的一个在线产品**PhotoSynth**和Adobe的**123D
    Catch**。还有许多SfM的实现可以在网上找到，只需搜索就能找到很多。
- en: Another important relationship we have not discussed in depth is that of SfM
    and Visual Localization and Mapping, better known as **Simultaneous Localization
    and Mapping (SLAM)** methods. In this chapter, we dealt with a given dataset of
    images and a video sequence, and using SfM is practical in those cases; however,
    some applications have no prerecorded dataset and must bootstrap the reconstruction
    on the fly. This process is better known as **Mapping**, and it is done while
    we are creating a 3D map of the world, using feature matching and tracking in
    2D, and after triangulation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们没有深入讨论的重要关系是SfM与视觉定位和制图的关系，更广为人知的是**同时定位与制图（SLAM）**方法。在这一章中，我们处理了一个给定的图像数据集和视频序列，在这些情况下使用SfM是实用的；然而，有些应用没有预先记录的数据集，必须在运行时启动重建。这个过程更广为人知的是**制图**，它在我们创建世界3D地图的同时进行，使用二维特征匹配和跟踪，并在三角测量之后完成。
- en: In the next chapter, we will see how OpenCV can be used for extracting license
    plate numbers from images, using various techniques in machine learning.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将看到如何使用OpenCV从图像中提取车牌号码，利用机器学习中的各种技术。
- en: References
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: '*Hartley, Richard, and Andrew Zisserman, Multiple View Geometry in Computer
    Vision, Cambridge University Press, 2003*'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hartley, Richard, 和 Andrew Zisserman, 计算机视觉中的多视图几何，剑桥大学出版社，2003*'
- en: '*Hartley, Richard I., and Peter Sturm; Triangulation, Computer Vision and image
    understanding 68.2 (1997): 146-157*'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Hartley, Richard I., 和 Peter Sturm; 三角测量，计算机视觉与图像理解 68.2 (1997): 146-157*'
- en: '*Snavely, Noah, Steven M. Seitz, and Richard Szeliski; Photo Tourism: Exploring
    Photo Collections in 3D, ACM Transactions on Graphics (TOG). Vol. 25\. No. 3\.
    ACM, 2006*'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Snavely, Noah, Steven M. Seitz, 和 Richard Szeliski; 照片旅游：在3D中探索照片收藏，ACM Transactions
    on Graphics (TOG). 第25卷. 第3期. ACM，2006*'
- en: '*Strecha, Christoph, et al, On Benchmarking Camera Calibration and Multi-view
    Stereo for High Resolution Imagery, IEEE Conference on Computer Vision and Pattern
    Recognition (CVPR) 2008*'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Strecha, Christoph, 等人，关于高分辨率图像的相机标定和多视图立体视觉的基准测试，IEEE计算机视觉与模式识别会议（CVPR）2008*'
- en: '[h t t p ://c v l a b w w w . e p f l . c h /d a t a /m u l t i v i e w /d
    e n s e M V S . h t m l h t t p s ://d e v e l o p e r . b l e n d e r . o r g
    /t a g /l i b m v /](http://cvlabwww.epfl.ch/data/multiview/denseMVS.htmlhttps://developer.blender.org/tag/libmv/)'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[EPFL CVLab 多视图数据](http://cvlabwww.epfl.ch/data/multiview/denseMVS.htmlhttps://developer.blender.org/tag/libmv/)'
- en: '[h t t p ://c c w u . m e /v s f m /](http://ccwu.me/vsfm/)'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[ccwu.me VSFM](http://ccwu.me/vsfm/)'
- en: '[h t t p ://w w w . c s . c o r n e l l . e d u /~s n a v e l y /b u n d l
    e r /](http://www.cs.cornell.edu/~snavely/bundler/)'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Cornell University Snavely''s Bundler](http://www.cs.cornell.edu/~snavely/bundler/)'
- en: '[h t t p ://p h o t o s y n t h . n e t](http://photosynth.net)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[PhotoSynth](http://photosynth.net)'
- en: '[h t t p ://e n . w i k i p e d i a . o r g /w i k i /S i m u l t a n e o u
    s _ l o c a l i z a t i o n _ a n d _ m a p p i n g](http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[同时定位与地图构建](http://en.wikipedia.org/wiki/Simultaneous_localization_and_mapping)'
- en: '[h t t p ://w w w . c m a k e . o r g](http://www.cmake.org)'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[CMake 官网](http://www.cmake.org)'
- en: '[h t t p ://c e r e s - s o l v e r . o r g](http://ceres-solver.org)'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[Ceres Solver](http://ceres-solver.org)'
- en: '[h t t p ://w w w . 123d a p p . c o m /c a t c h](http://www.123dapp.com/catch)'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[123D Catch](http://www.123dapp.com/catch)'
