- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Working with Unlabeled Data – Clustering Analysis
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用未标记数据进行聚类分析
- en: In the previous chapters, we used supervised learning techniques to build machine
    learning models, using data where the answer was already known—the class labels
    were already available in our training data. In this chapter, we will switch gears
    and explore cluster analysis, a category of **unsupervised learning** techniques
    that allows us to discover hidden structures in data where we do not know the
    right answer upfront. The goal of **clustering** is to find a natural grouping
    in data so that items in the same cluster are more similar to each other than
    to those from different clusters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们使用监督学习技术构建机器学习模型，使用的是答案已经知道的数据——训练数据中已经有类标签。在本章中，我们将转变思路，探索聚类分析，它是**无监督学习**的一种技术，允许我们在数据中发现隐藏的结构，而我们并不知道正确答案。**聚类**的目标是找到数据中的自然分组，使得同一聚类中的项彼此之间比与其他聚类中的项更相似。
- en: 'Given its exploratory nature, clustering is an exciting topic, and in this
    chapter, you will learn about the following concepts, which can help us to organize
    data into meaningful structures:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 由于聚类具有探索性，因此是一个令人兴奋的主题，在本章中，你将学习以下概念，这些概念有助于我们将数据组织成有意义的结构：
- en: Finding centers of similarity using the popular **k-means** algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用流行的**k-means**算法查找相似性的中心
- en: Taking a bottom-up approach to building hierarchical clustering trees
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用自下而上的方法构建层次聚类树
- en: Identifying arbitrary shapes of objects using a density-based clustering approach
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于密度的聚类方法识别任意形状的物体
- en: Grouping objects by similarity using k-means
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means按相似性对物体进行分组
- en: In this section, we will learn about one of the most popular clustering algorithms,
    k-means, which is widely used in academia as well as in industry. Clustering (or
    cluster analysis) is a technique that allows us to find groups of similar objects
    that are more related to each other than to objects in other groups. Examples
    of business-oriented applications of clustering include the grouping of documents,
    music, and movies by different topics, or finding customers that share similar
    interests based on common purchase behaviors as a basis for recommendation engines.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习一种最流行的聚类算法——k-means，它在学术界和工业界都得到了广泛应用。聚类（或聚类分析）是一种技术，它可以帮助我们找到相似物体的组，这些物体之间比与其他组中的物体更为相关。聚类在商业中的应用实例包括根据不同主题对文档、音乐和电影进行分组，或者基于共同的购买行为找到具有相似兴趣的客户，从而为推荐引擎提供依据。
- en: K-means clustering using scikit-learn
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn进行K-means聚类
- en: As you will see in a moment, the k-means algorithm is extremely easy to implement,
    but it is also computationally very efficient compared to other clustering algorithms,
    which might explain its popularity. The k-means algorithm belongs to the category
    of **prototype-based clustering**. We will discuss two other categories of clustering,
    **hierarchical** and **density-based clustering**, later in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你将在稍后看到的，k-means算法极其容易实现，但与其他聚类算法相比，它在计算上也非常高效，这可能解释了它的流行。k-means算法属于**基于原型的聚类**范畴。我们将在本章稍后讨论另外两种聚类类别——**层次聚类**和**基于密度的聚类**。
- en: Prototype-based clustering means that each cluster is represented by a prototype,
    which is usually either the **centroid** (*average*) of similar points with continuous
    features, or the **medoid** (the most *representative* or the point that minimizes
    the distance to all other points that belong to a particular cluster) in the case
    of categorical features. While k-means is very good at identifying clusters with
    a spherical shape, one of the drawbacks of this clustering algorithm is that we
    have to specify the number of clusters, *k*, *a priori*. An inappropriate choice
    for *k* can result in poor clustering performance. Later in this chapter, we will
    discuss the **elbow** method and **silhouette plots**, which are useful techniques
    to evaluate the quality of a clustering to help us determine the optimal number
    of clusters, *k*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 基于原型的聚类意味着每个聚类由一个原型表示，该原型通常是具有连续特征的相似点的**质心**（*平均值*），或者在处理类别特征时，表示为**中位点**（最*具代表性*的点或最小化与其他属于特定聚类点之间的距离的点）。虽然k-means非常擅长识别具有球形的聚类，但这种聚类算法的一个缺点是我们必须*先验*指定聚类的数量*k*。不合适的*k*选择可能导致聚类效果不佳。稍后我们将在本章中讨论**肘部法则**和**轮廓图**，这些都是评估聚类质量的有用技术，帮助我们确定最佳的聚类数量*k*。
- en: 'Although k-means clustering can be applied to data in higher dimensions, we
    will walk through the following examples using a simple two-dimensional dataset
    for the purpose of visualization:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 k-means 聚类可以应用于高维数据，但我们将使用一个简单的二维数据集来进行讲解，以便于可视化：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The dataset that we just created consists of 150 randomly generated points
    that are roughly grouped into three regions with higher density, which is visualized
    via a two-dimensional scatterplot:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚创建的数据集包含 150 个随机生成的点，这些点大致被分为三个高密度区域，通过二维散点图进行可视化：
- en: '![](img/B13208_11_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_01.png)'
- en: 'In real-world applications of clustering, we do not have any ground truth category
    information (information provided as empirical evidence as opposed to inference)
    about those examples; if we were given class labels, this task would fall into
    the category of supervised learning. Thus, our goal is to group the examples based
    on their feature similarities, which can be achieved using the k-means algorithm,
    as summarized by the following four steps:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类的实际应用中，我们没有关于这些示例的任何真实类别信息（与推理相对的经验性证据提供的信息）；如果我们有类标签，这项任务将属于监督学习的范畴。因此，我们的目标是根据特征相似性对示例进行分组，这可以通过
    k-means 算法实现，概述如下四个步骤：
- en: Randomly pick *k* centroids from the examples as initial cluster centers.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机从示例中挑选 *k* 个质心作为初始聚类中心。
- en: Assign each example to the nearest centroid, ![](img/B13208_11_001.png).
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个示例分配给最近的质心，![](img/B13208_11_001.png)。
- en: Move the centroids to the center of the examples that were assigned to it.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将质心移动到分配给它的示例的中心。
- en: Repeat steps 2 and 3 until the cluster assignments do not change or a user-defined
    tolerance or maximum number of iterations is reached.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 和 3，直到聚类分配不再变化，或者达到用户定义的容忍度或最大迭代次数。
- en: 'Now, the next question is, *how do we measure similarity between objects*?
    We can define similarity as the opposite of distance, and a commonly used distance
    for clustering examples with continuous features is the **squared Euclidean distance**
    between two points, *x* and *y*, in *m*-dimensional space:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，下一个问题是，*我们如何衡量对象之间的相似性*？我们可以将相似性定义为距离的反面，而用于聚类具有连续特征的示例的常用距离是两个点 *x* 和 *y*
    在 *m* 维空间中的 **平方欧几里得距离**：
- en: '![](img/B13208_11_002.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_002.png)'
- en: Note that, in the preceding equation, the index *j* refers to the *j*th dimension
    (feature column) of the example inputs, *x* and *y*. In the rest of this section,
    we will use the superscripts *i* and *j* to refer to the index of the example
    (data record) and cluster index, respectively.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在前面的方程中，索引 *j* 指的是示例输入 *x* 和 *y* 的 *j* 维度（特征列）。在本节的其余部分，我们将使用上标 *i* 和 *j*
    分别表示示例（数据记录）和聚类索引的索引。
- en: 'Based on this Euclidean distance metric, we can describe the k-means algorithm
    as a simple optimization problem, an iterative approach for minimizing the within-cluster
    **sum of squared errors** (**SSE**), which is sometimes also called **cluster
    inertia**:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个欧几里得距离度量，我们可以将 k-means 算法描述为一个简单的优化问题，即一个迭代方法，用于最小化聚类内部的 **平方误差和**（**SSE**），有时也叫做
    **聚类惯性**：
- en: '![](img/B13208_11_003.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_003.png)'
- en: Here, ![](img/B13208_11_004.png) is the representative point (centroid) for
    cluster *j*. ![](img/B13208_11_005.png) if the example, ![](img/B13208_11_006.png),
    is in cluster *j*, or 0 otherwise.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/B13208_11_004.png) 是聚类 *j* 的代表点（质心）。如果示例 ![](img/B13208_11_006.png)
    在聚类 *j* 中，则为 ![](img/B13208_11_005.png)，否则为 0。
- en: '![](img/B13208_11_047.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_047.png)'
- en: 'Now that you have learned how the simple k-means algorithm works, let''s apply
    it to our example dataset using the `KMeans` class from scikit-learn''s `cluster`
    module:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了简单的 k-means 算法是如何工作的，让我们通过使用 scikit-learn 的 `cluster` 模块中的 `KMeans`
    类，将它应用到我们的示例数据集上：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using the preceding code, we set the number of desired clusters to `3`; specifying
    the number of clusters *a priori* is one of the limitations of k-means. We set
    `n_init=10` to run the k-means clustering algorithms 10 times independently, with
    different random centroids to choose the final model as the one with the lowest
    SSE. Via the `max_iter` parameter, we specify the maximum number of iterations
    for each single run (here, `300`). Note that the k-means implementation in scikit-learn
    stops early if it converges before the maximum number of iterations is reached.
    However, it is possible that k-means does not reach convergence for a particular
    run, which can be problematic (computationally expensive) if we choose relatively
    large values for `max_iter`. One way to deal with convergence problems is to choose
    larger values for `tol`, which is a parameter that controls the tolerance with
    regard to the changes in the within-cluster SSE to declare convergence. In the
    preceding code, we chose a tolerance of `1e-04` (=0.0001).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前面的代码，我们将期望的聚类数量设置为 `3`；*预先*指定聚类数量是 k-means 的一个限制。我们设置 `n_init=10`，使得 k-means
    聚类算法独立运行 10 次，并使用不同的随机质心来选择 SSE 最小的最终模型。通过 `max_iter` 参数，我们指定每次运行的最大迭代次数（这里为 `300`）。需要注意的是，如果在达到最大迭代次数之前就已经收敛，scikit-learn
    中的 k-means 实现会提前停止。然而，也有可能 k-means 在某次运行中未能收敛，如果我们选择较大的 `max_iter` 值，这可能会带来计算上的问题。解决收敛问题的一种方法是选择更大的
    `tol` 值，这是一个控制聚类内 SSE 变化的容差参数，以宣告收敛。在前面的代码中，我们选择了容差 `1e-04` (=0.0001)。
- en: A problem with k-means is that one or more clusters can be empty. Note that
    this problem does not exist for k-medoids or fuzzy C-means, an algorithm that
    we will discuss later in this section.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 的一个问题是一个或多个聚类可能为空。需要注意的是，这个问题在 k-medoids 或模糊 C-means 中并不存在，这是一种我们将在本节稍后讨论的算法。
- en: However, this problem is accounted for in the current k-means implementation
    in scikit-learn. If a cluster is empty, the algorithm will search for the example
    that is farthest away from the centroid of the empty cluster. Then it will reassign
    the centroid to be this farthest point.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这个问题在当前的 scikit-learn 中的 k-means 实现中得到了考虑。如果一个聚类为空，算法将搜索距离该空聚类质心最远的样本点，然后将质心重新分配到这个最远点。
- en: '**Feature scaling**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征缩放**'
- en: When we are applying k-means to real-world data using a Euclidean distance metric,
    we want to make sure that the features are measured on the same scale and apply
    z-score standardization or min-max scaling if necessary.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用欧几里得距离度量将 k-means 应用于真实世界的数据时，我们希望确保特征在相同的尺度上进行衡量，并在必要时应用 z-score 标准化或最小-最大缩放。
- en: 'Having predicted the cluster labels, `y_km`, and discussed some of the challenges
    of the k-means algorithm, let''s now visualize the clusters that k-means identified
    in the dataset together with the cluster centroids. These are stored under the
    `cluster_centers_` attribute of the fitted `KMeans` object:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测了聚类标签 `y_km` 并讨论了 k-means 算法的一些挑战之后，接下来我们将可视化 k-means 在数据集中识别出的聚类以及聚类质心。这些信息保存在拟合后的
    `KMeans` 对象的 `cluster_centers_` 属性中：
- en: '[PRE2]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the following scatterplot, you can see that k-means placed the three centroids
    at the center of each sphere, which looks like a reasonable grouping given this
    dataset:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的散点图中，你可以看到 k-means 将三个质心放置在每个球体的中心，考虑到这个数据集，这看起来是一个合理的分组：
- en: '![](img/B13208_11_02.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_02.png)'
- en: 'Although k-means worked well on this toy dataset, we will highlight another
    drawback of k-means: we have to specify the number of clusters, *k*, *a priori*.
    The number of clusters to choose may not always be so obvious in real-world applications,
    especially if we are working with a higher dimensional dataset that cannot be
    visualized. The other properties of k-means are that clusters do not overlap and
    are not hierarchical, and we also assume that there is at least one item in each
    cluster. Later in this chapter, we will encounter different types of clustering
    algorithms, hierarchical and density-based clustering. Neither type of algorithm
    requires us to specify the number of clusters upfront or assume spherical structures
    in our dataset.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 k-means 在这个玩具数据集上表现良好，但我们将突出 k-means 的另一个缺点：我们必须*预先*指定聚类的数量 *k*。在实际应用中，选择聚类的数量可能并不总是那么明显，尤其是当我们处理的是无法可视化的高维数据集时。k-means
    的其他特点是聚类不重叠且没有层次结构，我们还假设每个聚类中至少有一个样本。在本章后面，我们将遇到不同类型的聚类算法，包括层次聚类和基于密度的聚类。两种算法都不要求我们预先指定聚类数量，也不假设数据集具有球形结构。
- en: In the next subsection, we will cover a popular variant of the classic k-means
    algorithm called **k-means++**. While it doesn't address those assumptions and
    drawbacks of k-means that were discussed in the previous paragraph, it can greatly
    improve the clustering results through more clever seeding of the initial cluster
    centers.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一小节中，我们将介绍经典k-means算法的一个流行变体——**k-means++**。虽然它没有解决前面一段中讨论的k-means的假设和缺点，但通过更加智能地初始化聚类中心，它可以显著改善聚类结果。
- en: A smarter way of placing the initial cluster centroids using k-means++
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用k-means++来智能地放置初始聚类质心。
- en: So far, we have discussed the classic k-means algorithm, which uses a random
    seed to place the initial centroids, which can sometimes result in bad clusterings
    or slow convergence if the initial centroids are chosen poorly. One way to address
    this issue is to run the k-means algorithm multiple times on a dataset and choose
    the best performing model in terms of the SSE.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了经典的k-means算法，该算法使用随机种子来放置初始质心，如果初始质心选择不当，可能会导致不良的聚类结果或收敛缓慢。解决这一问题的一种方法是多次运行k-means算法，并选择SSE表现最好的模型。
- en: 'Another strategy is to place the initial centroids far away from each other
    via the k-means++ algorithm, which leads to better and more consistent results
    than the classic k-means (*k-means++: The Advantages of Careful Seeding*, *D.
    Arthur* and *S. Vassilvitskii* in *Proceedings of the eighteenth annual ACM-SIAM
    symposium on Discrete algorithms*, pages 1027-1035\. *Society for Industrial and
    Applied Mathematics*, *2007*).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '另一种策略是通过k-means++算法将初始质心放置得彼此远离，这比经典的k-means算法能得到更好且更一致的结果（*k-means++: The
    Advantages of Careful Seeding*，*D. Arthur*和*S. Vassilvitskii*在*Proceedings of
    the eighteenth annual ACM-SIAM symposium on Discrete algorithms*，第1027-1035页，*Society
    for Industrial and Applied Mathematics*，*2007*）。'
- en: 'The initialization in k-means++ can be summarized as follows:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: k-means++的初始化可以总结如下：
- en: Initialize an empty set, **M**, to store the *k* centroids being selected.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个空集**M**，用于存储正在选择的*k*个质心。
- en: Randomly choose the first centroid, ![](img/B13208_11_007.png), from the input
    examples and assign it to **M**.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择第一个质心，![](img/B13208_11_007.png)，从输入示例中选取并分配给**M**。
- en: For each example, ![](img/B13208_11_008.png), that is not in **M**, find the
    minimum squared distance, ![](img/B13208_11_009.png), to any of the centroids
    in **M**.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个不在**M**中的示例，![](img/B13208_11_008.png)，找到与**M**中任何质心的最小平方距离，![](img/B13208_11_009.png)。
- en: To randomly select the next centroid, ![](img/B13208_11_010.png), use a weighted
    probability distribution equal to ![](img/B13208_11_011.png).
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了随机选择下一个质心，![](img/B13208_11_010.png)，使用一个等于![](img/B13208_11_011.png)的加权概率分布。
- en: Repeat steps 2 and 3 until *k* centroids are chosen.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3，直到选择出*k*个质心。
- en: Proceed with the classic k-means algorithm.
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续使用经典的k-means算法。
- en: To use k-means++ with scikit-learn's `KMeans` object, we just need to set the
    `init` parameter to `'k-means++'`. In fact, `'k-means++'` is the default argument
    to the `init` parameter, which is strongly recommended in practice. The only reason
    we didn't use it in the previous example was to not introduce too many concepts
    all at once. The rest of this section on k-means will use k-means++, but you are
    encouraged to experiment more with the two different approaches (classic k-means
    via `init='random'` versus `k-means++` via `init='k-means++'`) for placing the
    initial cluster centroids.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要在scikit-learn的`KMeans`对象中使用k-means++，只需要将`init`参数设置为`'k-means++'`。事实上，`'k-means++'`是`init`参数的默认值，这在实践中是强烈推荐的。我们在之前的示例中没有使用它的唯一原因是为了避免一次性引入太多概念。接下来的这一部分关于k-means的内容将使用k-means++，但你可以尝试更多两种不同的初始化方法（经典k-means通过`init='random'`与k-means++通过`init='k-means++'`）来放置初始聚类质心。
- en: Hard versus soft clustering
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 硬聚类与软聚类。
- en: '**Hard clustering** describes a family of algorithms where each example in
    a dataset is assigned to exactly one cluster, as in the k-means and k-means++
    algorithms that we discussed earlier in this chapter. In contrast, algorithms
    for **soft clustering** (sometimes also called **fuzzy clustering**) assign an
    example to one or more clusters. A popular example of soft clustering is the **fuzzy
    C-means** (**FCM**) algorithm (also called **soft k-means** or **fuzzy k-means**).
    The original idea goes back to the 1970s, when Joseph C. Dunn first proposed an
    early version of fuzzy clustering to improve k-means (*A Fuzzy Relative of the
    ISODATA Process and Its Use in Detecting Compact Well-Separated Clusters*, *J.
    C. Dunn*, *1973*). Almost a decade later, James C. Bedzek published his work on
    the improvement of the fuzzy clustering algorithm, which is now known as the FCM
    algorithm (*Pattern Recognition with Fuzzy Objective Function Algorithms*, *J.
    C. Bezdek*, *Springer Science+Business Media*, *2013*).'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**硬聚类**描述了一类算法，其中数据集中的每个示例都被分配到一个簇中，就像我们在本章之前讨论的k-means和k-means++算法一样。相反，**软聚类**（有时也称为**模糊聚类**）的算法会将一个示例分配给一个或多个簇。**模糊C均值**（**FCM**）算法（也叫做**软k-means**或**模糊k-means**）是软聚类的一个流行例子。最初的想法可以追溯到1970年代，当时Joseph
    C. Dunn首次提出了一个早期版本的模糊聚类，以改进k-means（*A Fuzzy Relative of the ISODATA Process and
    Its Use in Detecting Compact Well-Separated Clusters*，*J. C. Dunn*，*1973*）。近十年后，James
    C. Bedzek发表了关于模糊聚类算法改进的工作，这一算法现在被称为FCM算法（*Pattern Recognition with Fuzzy Objective
    Function Algorithms*，*J. C. Bezdek*，*Springer Science+Business Media*，*2013*）。'
- en: 'The FCM procedure is very similar to k-means. However, we replace the hard
    cluster assignment with probabilities for each point belonging to each cluster.
    In k-means, we could express the cluster membership of an example, *x*, with a
    sparse vector of binary values:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: FCM过程与k-means非常相似。然而，我们将硬聚类分配替换为每个点属于每个簇的概率。在k-means中，我们可以用一个稀疏的二进制向量来表示示例*x*的簇成员关系：
- en: '![](img/B13208_11_012.png)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_012.png)'
- en: 'Here, the index position with value 1 indicates the cluster centroid, ![](img/B13208_11_013.png),
    that the example is assigned to (assuming ![](img/B13208_11_014.png)![](img/B13208_11_0141.png)).
    In contrast, a membership vector in FCM could be represented as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，值为1的索引位置表示示例被分配到的簇质心![](img/B13208_11_013.png)（假设![](img/B13208_11_014.png)![](img/B13208_11_0141.png)）。相反，FCM中的成员关系向量可以表示如下：
- en: '![](img/B13208_11_015.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_015.png)'
- en: 'Here, each value falls in the range [0, 1] and represents a probability of
    membership of the respective cluster centroid. The sum of the memberships for
    a given example is equal to 1\. As with the k-means algorithm, we can summarize
    the FCM algorithm in four key steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每个值的范围是[0, 1]，表示该点属于相应簇质心的概率。给定示例的所有成员关系之和等于1。与k-means算法一样，我们可以用四个关键步骤总结FCM算法：
- en: Specify the number of *k* centroids and randomly assign the cluster memberships
    for each point.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 指定*k*个质心的数量，并随机分配每个点的簇成员关系。
- en: Compute the cluster centroids, ![](img/B13208_11_016.png).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算簇质心，![](img/B13208_11_016.png)。
- en: Update the cluster memberships for each point.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每个点的簇成员关系。
- en: Repeat steps 2 and 3 until the membership coefficients do not change or a user-defined
    tolerance or maximum number of iterations is reached.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和步骤3，直到成员系数不再变化，或者达到用户定义的容差或最大迭代次数。
- en: 'The objective function of FCM—we abbreviate it as ![](img/B13208_11_017.png)—looks
    very similar to the within-cluster SSE that we minimize in k-means:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: FCM的目标函数——我们简写为![](img/B13208_11_017.png)——看起来非常类似于我们在k-means中最小化的簇内SSE：
- en: '![](img/B13208_11_018.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_018.png)'
- en: However, note that the membership indicator, ![](img/B13208_11_019.png), is
    not a binary value as in k-means (![](img/B13208_11_020.png)), but a real value
    that denotes the cluster membership probability (![](img/B13208_11_021.png)).
    You also may have noticed that we added an additional exponent to ![](img/B13208_11_022.png);
    the exponent *m*, any number greater than or equal to one (typically *m* = 2),
    is the so-called **fuzziness coefficient** (or simply **fuzzifier**), which controls
    the degree of *fuzziness*.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，请注意，成员指示符![](img/B13208_11_019.png)不像k-means中的二值值（![](img/B13208_11_020.png)），而是一个实值，表示簇成员的概率（![](img/B13208_11_021.png)）。你可能还注意到，我们在![](img/B13208_11_022.png)中添加了一个额外的指数；这个指数*m*是一个大于或等于1的任意数字（通常*m*=2），它是所谓的**模糊度系数**（或简称**模糊因子**），控制着*模糊性*的程度。
- en: 'The larger the value of *m*, the smaller the cluster membership, ![](img/B13208_11_023.png),
    becomes, which leads to fuzzier clusters. The cluster membership probability itself
    is calculated as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '*m*值越大，簇成员资格的值，![](img/B13208_11_023.png)，就越小，这会导致更模糊的簇。簇成员概率本身的计算公式如下：'
- en: '![](img/B13208_11_024.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_024.png)'
- en: 'For example, if we chose three cluster centers, as in the previous k-means
    example, we could calculate the membership of ![](img/B13208_11_025.png) belonging
    to the ![](img/B13208_11_026.png) cluster as follows:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们选择了三个簇中心，如前面的k-means例子中所示，我们可以按照如下方式计算![](img/B13208_11_025.png)属于![](img/B13208_11_026.png)簇的成员资格：
- en: '![](img/B13208_11_027.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_027.png)'
- en: 'The center, ![](img/B13208_11_028.png), of a cluster itself is calculated as
    the mean of all examples weighted by the degree to which each example belongs
    to that cluster (![](img/B13208_11_029.png) ):'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 一个簇的中心，![](img/B13208_11_028.png)，是通过计算所有样本的加权平均得到的，权重取决于每个样本属于该簇的程度（![](img/B13208_11_029.png)）：
- en: '![](img/B13208_11_030.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_030.png)'
- en: 'Just by looking at the equation to calculate the cluster memberships, we can
    say that each iteration in FCM is more expensive than an iteration in k-means.
    On the other hand, FCM typically requires fewer iterations overall to reach convergence.
    Unfortunately, the FCM algorithm is currently not implemented in scikit-learn.
    However, it has been found, in practice, that both k-means and FCM produce very
    similar clustering outputs, as described in a study (*Comparative Analysis of
    k-means and Fuzzy C-Means Algorithms*, *S. Ghosh*, and *S. K. Dubey*, *IJACSA*,
    4: 35–38, *2013*).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '仅仅从计算簇成员资格的方程式来看，我们可以说，FCM中的每次迭代比k-means中的迭代更加昂贵。另一方面，FCM通常需要更少的迭代次数才能达到收敛。不幸的是，FCM算法目前并未在scikit-learn中实现。然而，实际应用中发现，k-means和FCM产生的聚类结果非常相似，正如一项研究所描述的那样（*Comparative
    Analysis of k-means and Fuzzy C-Means Algorithms*，*S. Ghosh*和*S. K. Dubey*，*IJACSA*，4:
    35–38，*2013*）。'
- en: Using the elbow method to find the optimal number of clusters
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用肘部法则找到最佳簇数
- en: One of the main challenges in unsupervised learning is that we do not know the
    definitive answer. We don't have the ground truth class labels in our dataset
    that allow us to apply the techniques that we used in *Chapter 6*, *Learning Best
    Practices for Model Evaluation and Hyperparameter Tuning*, in order to evaluate
    the performance of a supervised model. Thus, to quantify the quality of clustering,
    we need to use intrinsic metrics—such as the within-cluster SSE (distortion)—to
    compare the performance of different k-means clusterings.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习中的主要挑战之一是我们并不知道确切的答案。我们的数据集中没有可以用来评估监督学习模型表现的真实标签，这使得我们无法应用在*第六章*中介绍的技术，*学习最佳实践：模型评估和超参数调优*，来评估监督模型的性能。因此，为了量化聚类质量，我们需要使用内在度量——比如簇内SSE（失真度）——来比较不同k-means聚类的表现。
- en: 'Conveniently, we don''t need to compute the within-cluster SSE explicitly when
    we are using scikit-learn, as it is already accessible via the `inertia_` attribute
    after fitting a `KMeans` model:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 方便的是，在使用scikit-learn时，我们不需要显式计算簇内SSE，因为在拟合`KMeans`模型之后，它已经可以通过`inertia_`属性直接访问：
- en: '[PRE3]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Based on the within-cluster SSE, we can use a graphical tool, the so-called
    **elbow method**, to estimate the optimal number of clusters, *k*, for a given
    task. We can say that if *k* increases, the distortion will decrease. This is
    because the examples will be closer to the centroids they are assigned to. The
    idea behind the elbow method is to identify the value of *k* where the distortion
    begins to increase most rapidly, which will become clearer if we plot the distortion
    for different values of *k*:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 基于簇内SSE，我们可以使用图形化工具，也就是所谓的**肘部法则**，来估算给定任务的最佳簇数*k*。我们可以说，如果*k*增加，失真度会减小。这是因为样本会更接近它们被分配的质心。肘部法则的核心思想是找出*k*的值，在这个值上，失真度开始最急剧增加，如果我们绘制不同*k*值的失真度曲线，效果会更加清晰：
- en: '[PRE4]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'As you can see in the following plot, the *elbow* is located at *k* = 3, so
    this is evidence that *k* = 3 is indeed a good choice for this dataset:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，*肘部*位于*k* = 3，因此这是证明*k* = 3确实是这个数据集的一个好选择的证据：
- en: '![](img/B13208_11_03.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_03.png)'
- en: Quantifying the quality of clustering via silhouette plots
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过轮廓图量化聚类质量
- en: 'Another intrinsic metric to evaluate the quality of a clustering is **silhouette
    analysis**, which can also be applied to clustering algorithms other than k-means,
    which we will discuss later in this chapter. Silhouette analysis can be used as
    a graphical tool to plot a measure of how tightly grouped the examples in the
    clusters are. To calculate the **silhouette coefficient** of a single example
    in our dataset, we can apply the following three steps:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种评估聚类质量的内在指标是**轮廓分析**，它也可以应用于k-means以外的聚类算法，我们将在本章稍后讨论。轮廓分析可以作为一种图形工具，绘制出聚类内样本紧密程度的度量。为了计算我们数据集中单个样本的**轮廓系数**，我们可以执行以下三个步骤：
- en: Calculate the **cluster cohesion**, ![](img/B13208_11_031.png), as the average
    distance between an example, ![](img/B13208_11_032.png), and all other points
    in the same cluster.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算**聚类内聚度**，![](img/B13208_11_031.png)，即样本与同一聚类内所有其他点之间的平均距离，![](img/B13208_11_032.png)。
- en: Calculate the **cluster separation**, ![](img/B13208_11_033.png), from the next
    closest cluster as the average distance between the example, ![](img/B13208_11_034.png),
    and all examples in the nearest cluster.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算**聚类分离度**，![](img/B13208_11_033.png)，即通过下一个最接近的聚类与样本之间的平均距离，![](img/B13208_11_034.png)，以及该聚类中的所有样本之间的距离。
- en: 'Calculate the silhouette, ![](img/B13208_11_035.png), as the difference between
    cluster cohesion and separation divided by the greater of the two, as shown here:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算轮廓系数，![](img/B13208_11_035.png)，即通过聚类内聚度和分离度之差除以这两者中的较大者，如下所示：
- en: '![](img/B13208_11_036.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_036.png)'
- en: The silhouette coefficient is bounded in the range –1 to 1\. Based on the preceding
    equation, we can see that the silhouette coefficient is 0 if the cluster separation
    and cohesion are equal (![](img/B13208_11_037.png)). Furthermore, we get close
    to an ideal silhouette coefficient of 1 if ![](img/B13208_11_038.png), since ![](img/B13208_11_039.png)
    quantifies how dissimilar an example is from other clusters, and ![](img/B13208_11_040.png)
    tells us how similar it is to the other examples in its own cluster.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数的范围为–1到1。根据前述公式，我们可以看到，当聚类分离度和内聚度相等时，轮廓系数为0（![](img/B13208_11_037.png)）。此外，当聚类分离度较大时，轮廓系数接近理想值1（![](img/B13208_11_038.png)），因为![](img/B13208_11_039.png)量化了样本与其他聚类的差异，![](img/B13208_11_040.png)则告诉我们该样本与自己聚类内其他样本的相似度。
- en: 'The silhouette coefficient is available as `silhouette_samples` from scikit-learn''s
    `metric` module, and optionally, the `silhouette_scores` function can be imported
    for convenience. The `silhouette_scores` function calculates the average silhouette
    coefficient across all examples, which is equivalent to `numpy.mean(silhouette_samples(...))`.
    By executing the following code, we will now create a plot of the silhouette coefficients
    for a k-means clustering with *k* = 3:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓系数可以通过scikit-learn的`metric`模块中的`silhouette_samples`获得，并且可以选择导入`silhouette_scores`函数以方便使用。`silhouette_scores`函数计算所有样本的平均轮廓系数，这等价于`numpy.mean(silhouette_samples(...))`。通过执行以下代码，我们将创建一个k-means聚类的轮廓系数图，*k*
    = 3：
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Through a visual inspection of the silhouette plot, we can quickly scrutinize
    the sizes of the different clusters and identify clusters that contain *outliers*:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对轮廓图的可视化检查，我们可以迅速审查不同聚类的大小，并识别包含*异常值*的聚类：
- en: '![](img/B13208_11_04.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_04.png)'
- en: However, as you can see in the preceding silhouette plot, the silhouette coefficients
    are not even close to **0**, which is, in this case, an indicator of a *good*
    clustering. Furthermore, to summarize the goodness of our clustering, we added
    the average silhouette coefficient to the plot (dotted line).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你在之前的轮廓图中看到的，轮廓系数甚至没有接近**0**，这在本例中是*良好*聚类的一个指示符。此外，为了总结我们的聚类效果，我们将平均轮廓系数添加到图中（虚线）。
- en: 'To see what a silhouette plot looks like for a relatively *bad* clustering,
    let''s seed the k-means algorithm with only two centroids:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看相对*差*的聚类的轮廓图是什么样子，我们将k-means算法的初始质心数设置为仅有两个：
- en: '[PRE6]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you can see in the resulting plot, one of the centroids falls between two
    of the three spherical groupings of the input data.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在结果图中看到的，某个质心位于输入数据的三个球形聚类之间。
- en: 'Although the clustering does not look completely terrible, it is suboptimal:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管聚类看起来并不完全糟糕，但它仍然是次优的：
- en: '![](img/B13208_11_05.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_05.png)'
- en: 'Please keep in mind that we typically do not have the luxury of visualizing
    datasets in two-dimensional scatterplots in real-world problems, since we typically
    work with data in higher dimensions. So, next, we will create the silhouette plot
    to evaluate the results:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在实际问题中，我们通常没有将数据集可视化为二维散点图的奢侈，因为我们通常处理的是高维数据。所以接下来，我们将创建轮廓图来评估结果：
- en: '[PRE7]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see in the resulting plot, the silhouettes now have visibly different
    lengths and widths, which is evidence of a relatively *bad* or at least *suboptimal*
    clustering:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果图所示，轮廓现在有明显不同的长度和宽度，这是相对*差*或者至少是*不理想*的聚类的证据：
- en: '![](img/B13208_11_06.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_06.png)'
- en: Organizing clusters as a hierarchical tree
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将簇组织为层次树
- en: 'In this section, we will take a look at an alternative approach to prototype-based
    clustering: **hierarchical clustering**. One advantage of the hierarchical clustering
    algorithm is that it allows us to plot **dendrograms** (visualizations of a binary
    hierarchical clustering), which can help with the interpretation of the results
    by creating meaningful taxonomies. Another advantage of this hierarchical approach
    is that we do not need to specify the number of clusters upfront.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将关注一种基于原型的聚类的替代方法：**层次聚类**。层次聚类算法的一个优点是，它允许我们绘制**树状图**（二叉层次聚类的可视化），通过创建有意义的分类法来帮助解释结果。另一个优点是这种层次方法不需要我们提前指定簇的数量。
- en: The two main approaches to hierarchical clustering are **agglomerative** and
    **divisive** hierarchical clustering. In divisive hierarchical clustering, we
    start with one cluster that encompasses the complete dataset, and we iteratively
    split the cluster into smaller clusters until each cluster only contains one example.
    In this section, we will focus on agglomerative clustering, which takes the opposite
    approach. We start with each example as an individual cluster and merge the closest
    pairs of clusters until only one cluster remains.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类的两种主要方法是**聚合式**层次聚类和**分裂式**层次聚类。在分裂式层次聚类中，我们从包含完整数据集的一个簇开始，然后迭代地将该簇拆分为更小的簇，直到每个簇只包含一个样本。在本节中，我们将重点讨论聚合式聚类，它采取相反的方法。我们从每个样本作为一个单独的簇开始，逐步合并最接近的簇，直到只剩下一个簇。
- en: Grouping clusters in bottom-up fashion
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自下而上的簇合并
- en: 'The two standard algorithms for agglomerative hierarchical clustering are **single
    linkage** and **complete linkage**. Using single linkage, we compute the distances
    between the most similar members for each pair of clusters and merge the two clusters
    for which the distance between the most similar members is the smallest. The complete
    linkage approach is similar to single linkage but, instead of comparing the most
    similar members in each pair of clusters, we compare the most dissimilar members
    to perform the merge. This is shown in the following diagram:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 两种标准的聚合式层次聚类算法是**单一连接法**和**完全连接法**。使用单一连接法时，我们计算每对簇中最相似成员之间的距离，并合并最相似成员之间距离最小的两个簇。完全连接法类似于单一连接法，但不同之处在于，我们比较每对簇中最不相似的成员进行合并。这在下面的图示中有所展示：
- en: '![](img/B13208_11_07.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_07.png)'
- en: '**Alternative types of linkages**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**替代类型的连接法**'
- en: Other commonly used algorithms for agglomerative hierarchical clustering include
    **average linkage** and **Ward's linkage**. In average linkage, we merge the cluster
    pairs based on the minimum average distances between all group members in the
    two clusters. In Ward's linkage, the two clusters that lead to the minimum increase
    of the total within-cluster SSE are merged.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其他常用的聚合层次聚类算法包括**平均连接法**和**Ward连接法**。在平均连接法中，我们根据两个簇中所有组成员之间的最小平均距离来合并簇对。在Ward连接法中，我们合并两个簇，这两个簇的合并会导致总的簇内平方误差（SSE）最小的增加。
- en: 'In this section, we will focus on agglomerative clustering using the complete
    linkage approach. Hierarchical complete linkage clustering is an iterative procedure
    that can be summarized by the following steps:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点讨论使用完全连接法的聚合式聚类。层次完全连接聚类是一个迭代过程，可以通过以下步骤总结：
- en: Compute the distance matrix of all examples.
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有样本的距离矩阵。
- en: Represent each data point as a singleton cluster.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点表示为一个单独的簇。
- en: Merge the two closest clusters based on the distance between the most dissimilar
    (distant) members.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于最不相似（最远）成员之间的距离合并两个最接近的簇。
- en: Update the similarity matrix.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新相似度矩阵。
- en: Repeat steps 2-4 until one single cluster remains.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2-4，直到只剩下一个簇。
- en: 'Next, we will discuss how to compute the distance matrix (step 1). But first,
    let''s generate a random data sample to work with. The rows represent different
    observations (IDs 0-4), and the columns are the different features (`X`, `Y`,
    `Z`) of those examples:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论如何计算距离矩阵（步骤 1）。但首先，让我们生成一个随机数据样本来进行操作。行表示不同的观察值（ID 0-4），列表示这些示例的不同特征（`X`，`Y`，`Z`）：
- en: '[PRE8]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'After executing the preceding code, we should now see the following data frame
    containing the randomly generated examples:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 执行前述代码后，我们现在应该看到以下包含随机生成示例的数据框：
- en: '![](img/B13208_11_08.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_08.png)'
- en: Performing hierarchical clustering on a distance matrix
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对距离矩阵进行层次聚类
- en: 'To calculate the distance matrix as input for the hierarchical clustering algorithm,
    we will use the `pdist` function from SciPy''s `spatial.distance` submodule:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算作为层次聚类算法输入的距离矩阵，我们将使用 SciPy 的 `spatial.distance` 子模块中的 `pdist` 函数：
- en: '[PRE9]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using the preceding code, we calculated the Euclidean distance between each
    pair of input examples in our dataset based on the features `X`, `Y`, and `Z`.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用前述代码，我们基于特征 `X`，`Y` 和 `Z` 计算了数据集中每一对输入示例之间的欧几里得距离。
- en: 'We provided the condensed distance matrix—returned by `pdist`—as input to the
    `squareform` function to create a symmetrical matrix of the pair-wise distances,
    as shown here:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将由 `pdist` 返回的压缩距离矩阵作为输入传递给 `squareform` 函数，以创建一个对称的成对距离矩阵，如下所示：
- en: '![](img/B13208_11_09.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_09.png)'
- en: Next, we will apply the complete linkage agglomeration to our clusters using
    the `linkage` function from SciPy's `cluster.hierarchy` submodule, which returns
    a so-called **linkage matrix**.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用 SciPy 的 `cluster.hierarchy` 子模块中的 `linkage` 函数对我们的簇应用完全连接聚合，该函数返回所谓的
    **连接矩阵**。
- en: 'However, before we call the `linkage` function, let''s take a careful look
    at the function documentation:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在调用 `linkage` 函数之前，让我们仔细查看一下该函数的文档：
- en: '[PRE10]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Based on the function description, we understand that we can use a condensed
    distance matrix (upper triangular) from the `pdist` function as an input attribute.
    Alternatively, we could also provide the initial data array and use the `''euclidean''`
    metric as a function argument in `linkage`. However, we should not use the `squareform`
    distance matrix that we defined earlier, since it would yield different distance
    values than expected. To sum it up, the three possible scenarios are listed here:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 根据函数说明，我们可以使用 `pdist` 函数返回的压缩距离矩阵（上三角）作为输入属性。或者，我们也可以提供初始数据数组，并在 `linkage` 中使用
    `'euclidean'` 度量作为函数参数。然而，我们不应使用之前定义的 `squareform` 距离矩阵，因为它会产生与预期不同的距离值。总而言之，以下是三种可能的情况：
- en: '**Incorrect approach**: Using the `squareform` distance matrix as shown in
    the following code snippet leads to incorrect results:'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**错误方法**：使用以下代码片段中显示的 `squareform` 距离矩阵会导致错误的结果：'
- en: '[PRE11]'
  id: totrans-131
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '**Correct approach**: Using the condensed distance matrix as shown in the following
    code example yields the correct linkage matrix:'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确方法**：使用以下代码示例中显示的压缩距离矩阵可以得到正确的连接矩阵：'
- en: '[PRE12]'
  id: totrans-133
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Correct approach**: Using the complete input example matrix (the so-called
    design matrix) as shown in the following code snippet also leads to a correct
    linkage matrix similar to the preceding approach:'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正确方法**：使用以下代码片段中显示的完整输入示例矩阵（即所谓的设计矩阵）也能得到与前述方法类似的正确连接矩阵：'
- en: '[PRE13]'
  id: totrans-135
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To take a closer look at the clustering results, we can turn those results
    into a pandas `DataFrame` (best viewed in a Jupyter Notebook) as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更仔细地查看聚类结果，我们可以将这些结果转换为 pandas `DataFrame`（在 Jupyter Notebook 中查看效果最佳），如下所示：
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As shown in the following screenshot, the linkage matrix consists of several
    rows where each row represents one merge. The first and second columns denote
    the most dissimilar members in each cluster, and the third column reports the
    distance between those members.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，连接矩阵由若干行组成，每行表示一次合并。第一列和第二列表示每个簇中最不相似的成员，第三列表示这两个成员之间的距离。
- en: 'The last column returns the count of the members in each cluster:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一列返回每个簇中成员的数量：
- en: '![](img/B13208_11_10.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_10.png)'
- en: 'Now that we have computed the linkage matrix, we can visualize the results
    in the form of a dendrogram:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了连接矩阵，可以以树状图的形式可视化结果：
- en: '[PRE15]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you are executing the preceding code or reading an e-book version of this
    book, you will notice that the branches in the resulting dendrogram are shown
    in different colors. The color scheme is derived from a list of Matplotlib colors
    that are cycled for the distance thresholds in the dendrogram. For example, to
    display the dendrograms in black, you can uncomment the respective sections that
    were inserted in the preceding code:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在执行前面的代码或阅读本书的电子书版本，你会注意到生成的树状图中的分支显示为不同的颜色。这种配色方案来自Matplotlib的颜色列表，用于在树状图中的距离阈值之间循环。例如，要以黑色显示树状图，你可以取消注释前面代码中插入的相关部分：
- en: '![](img/B13208_11_11.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_11.png)'
- en: Such a dendrogram summarizes the different clusters that were formed during
    the agglomerative hierarchical clustering; for example, you can see that the examples
    `ID_0` and `ID_4`, followed by `ID_1` and `ID_2`, are the most similar ones based
    on the Euclidean distance metric.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的树状图总结了在凝聚层次聚类过程中形成的不同聚类；例如，你可以看到，`ID_0`和`ID_4`，接着是`ID_1`和`ID_2`，是基于欧几里得距离度量最相似的样本。
- en: Attaching dendrograms to a heat map
  id: totrans-146
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将树状图附加到热力图
- en: In practical applications, hierarchical clustering dendrograms are often used
    in combination with a **heat map**, which allows us to represent the individual
    values in the data array or matrix containing our training examples with a color
    code. In this section, we will discuss how to attach a dendrogram to a heat map
    plot and order the rows in the heat map correspondingly.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际应用中，层次聚类树状图通常与**热力图**结合使用，允许我们通过颜色编码表示包含训练示例的数据数组或矩阵中的各个值。在这一部分，我们将讨论如何将树状图附加到热力图上，并相应地对热力图中的行进行排序。
- en: 'However, attaching a dendrogram to a heat map can be a little bit tricky, so
    let''s go through this procedure step by step:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，将树状图附加到热力图上可能有点棘手，因此让我们一步步来执行这个过程：
- en: 'We create a new `figure` object and define the *x* axis position, *y* axis
    position, width, and height of the dendrogram via the `add_axes` attribute. Furthermore,
    we rotate the dendrogram 90 degrees counter-clockwise. The code is as follows:'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个新的`figure`对象，并通过`add_axes`属性定义树状图的* x *轴位置、* y *轴位置、宽度和高度。此外，我们将树状图逆时针旋转90度。代码如下：
- en: '[PRE16]'
  id: totrans-150
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Next, we reorder the data in our initial `DataFrame` according to the clustering
    labels that can be accessed from the `dendrogram` object, which is essentially
    a Python dictionary, via the `leaves` key. The code is as follows:'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们根据可以从`dendrogram`对象中访问的聚类标签重新排序我们初始的`DataFrame`，该对象本质上是一个Python字典，可以通过`leaves`键访问。代码如下：
- en: '[PRE17]'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now, we construct the heat map from the reordered `DataFrame` and position
    it next to the dendrogram:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，我们从重新排序的`DataFrame`构建热力图，并将其放置在树状图旁边：
- en: '[PRE18]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Finally, we modify the aesthetics of the dendrogram by removing the axis ticks
    and hiding the axis spines. Also, we add a color bar and assign the feature and
    data record names to the *x* and *y* axis tick labels, respectively:'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们通过去除坐标轴刻度并隐藏坐标轴脊线来修改树状图的美学效果。我们还添加了一个颜色条，并将特征和数据记录名称分别分配给* x *和* y *轴刻度标签：
- en: '[PRE19]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After following the previous steps, the heat map should be displayed with the
    dendrogram attached:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前面的步骤操作后，热力图应该会显示附带树状图的效果：
- en: '![](img/B13208_11_12.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_12.png)'
- en: As you can see, the order of rows in the heat map reflects the clustering of
    the examples in the dendrogram. In addition to a simple dendrogram, the color-coded
    values of each example and feature in the heat map provide us with a nice summary
    of the dataset.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，热力图中行的顺序反映了树状图中示例的聚类。此外，热力图中每个示例和特征的颜色编码值为我们提供了数据集的一个很好的概述。
- en: Applying agglomerative clustering via scikit-learn
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过scikit-learn应用凝聚层次聚类
- en: 'In the previous subsection, you saw how to perform agglomerative hierarchical
    clustering using SciPy. However, there is also an `AgglomerativeClustering` implementation
    in scikit-learn, which allows us to choose the number of clusters that we want
    to return. This is useful if we want to prune the hierarchical cluster tree. By
    setting the `n_cluster` parameter to `3`, we will now cluster the input examples
    into three groups using the same complete linkage approach based on the Euclidean
    distance metric, as before:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一小节中，你已经了解了如何使用 SciPy 执行凝聚层次聚类。然而，scikit-learn 中也有一个`AgglomerativeClustering`实现，它允许我们选择希望返回的簇的数量。如果我们想修剪层次聚类树，这非常有用。通过将`n_cluster`参数设置为`3`，我们将像之前一样使用基于欧几里得距离度量的完整连接方法，将输入示例分为三个组：
- en: '[PRE20]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Looking at the predicted cluster labels, we can see that the first and the
    fifth examples (`ID_0` and `ID_4`) were assigned to one cluster (label `1`), and
    the examples `ID_1` and `ID_2` were assigned to a second cluster (label `0`).
    The example `ID_3` was put into its own cluster (label `2`). Overall, the results
    are consistent with the results that we observed in the dendrogram. We should
    note, though, that `ID_3` is more similar to `ID_4` and `ID_0` than to `ID_1`
    and `ID_2`, as shown in the preceding dendrogram figure; this is not clear from
    scikit-learn''s clustering results. Let''s now rerun the `AgglomerativeClustering`
    using `n_cluster=2` in the following code snippet:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看预测的聚类标签，我们可以看到，第一个和第五个示例（`ID_0`和`ID_4`）被分配到了一个簇（标签为`1`），而示例`ID_1`和`ID_2`被分配到了第二个簇（标签为`0`）。示例`ID_3`被放入了它自己的簇（标签为`2`）。总体来看，结果与我们在树状图中观察到的结果一致。然而我们应该注意到，`ID_3`与`ID_4`和`ID_0`的相似度要高于`ID_1`和`ID_2`，正如之前树状图中所示；这一点在
    scikit-learn 的聚类结果中并不明显。接下来，我们将使用`n_cluster=2`重新运行`AgglomerativeClustering`，如以下代码片段所示：
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: As you can see, in this *pruned* clustering hierarchy, label `ID_3` was assigned
    to the same cluster as `ID_0` and `ID_4`, as expected.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在这个*修剪后的*聚类层次结构中，标签`ID_3`被分配到了与`ID_0`和`ID_4`相同的簇中，正如预期的那样。
- en: Locating regions of high density via DBSCAN
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过 DBSCAN 定位高密度区域
- en: 'Although we can''t cover the vast amount of different clustering algorithms
    in this chapter, let''s at least include one more approach to clustering: **density-based
    spatial clustering of applications with noise** (**DBSCAN**), which does not make
    assumptions about spherical clusters like k-means, nor does it partition the dataset
    into hierarchies that require a manual cut-off point. As its name implies, density-based
    clustering assigns cluster labels based on dense regions of points. In DBSCAN,
    the notion of density is defined as the number of points within a specified radius,
    ![](img/B13208_11_041.png).'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在本章中我们无法覆盖大量不同的聚类算法，但我们至少再介绍一种聚类方法：**基于密度的空间聚类应用及噪声**（**DBSCAN**），它不像 k-means
    那样对球形簇做出假设，也不像层次聚类那样将数据集分割成需要手动切割的层次结构。正如其名字所示，基于密度的聚类是根据点的密集区域分配簇标签的。在 DBSCAN
    中，密度的概念是通过在指定半径内的点数来定义的，![](img/B13208_11_041.png)。
- en: 'According to the DBSCAN algorithm, a special label is assigned to each example
    (data point) using the following criteria:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 根据 DBSCAN 算法，每个示例（数据点）都会根据以下标准分配一个特殊的标签：
- en: A point is considered a **core point** if at least a specified number (MinPts)
    of neighboring points fall within the specified radius, ![](img/B13208_11_0411.png).
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个点至少有指定数量（MinPts）的邻近点位于指定的半径范围内，则该点被视为**核心点**，![](img/B13208_11_0411.png)。
- en: A **border point** is a point that has fewer neighbors than MinPts within ε,
    but lies within the ![](img/B13208_11_0412.png) radius of a core point.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**边界点**是指在ε半径内邻近点数目少于MinPts，但位于核心点的![](img/B13208_11_0412.png)半径范围内的点。'
- en: All other points that are neither core nor border points are considered **noise
    points**.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有既不是核心点也不是边界点的其他点都被视为**噪声点**。
- en: 'After labeling the points as core, border, or noise, the DBSCAN algorithm can
    be summarized in two simple steps:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在将点标记为核心点、边界点或噪声点后，DBSCAN 算法可以通过两个简单的步骤总结：
- en: Form a separate cluster for each core point or connected group of core points.
    (Core points are connected if they are no farther away than ![](img/B13208_11_041.png).)
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个核心点或核心点的连接组形成一个独立的簇。（如果核心点之间的距离不超过![](img/B13208_11_041.png)，则它们是连接的。）
- en: Assign each border point to the cluster of its corresponding core point.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个边界点分配给其对应核心点的簇。
- en: 'To get a better understanding of what the result of DBSCAN can look like, before
    jumping to the implementation, let''s summarize what we have just learned about
    core points, border points, and noise points in the following figure:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解DBSCAN的结果是什么样的，在实现之前，让我们总结一下刚才关于核心点、边界点和噪声点的知识，如下图所示：
- en: '![](img/B13208_11_13.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_13.png)'
- en: One of the main advantages of using DBSCAN is that it does not assume that the
    clusters have a spherical shape as in k-means. Furthermore, DBSCAN is different
    from k-means and hierarchical clustering in that it doesn't necessarily assign
    each point to a cluster but is capable of removing noise points.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 使用DBSCAN的主要优势之一是，它不假设聚类具有像k-means那样的球形结构。此外，DBSCAN与k-means和层次聚类的不同之处在于，它不一定会将每个点分配到一个聚类中，而是能够去除噪声点。
- en: 'For a more illustrative example, let''s create a new dataset of half-moon-shaped
    structures to compare k-means clustering, hierarchical clustering, and DBSCAN:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供一个更具示范性的例子，让我们创建一个新的半月形状结构的数据集，来比较k-means聚类、层次聚类和DBSCAN。
- en: '[PRE22]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'As you can see in the resulting plot, there are two visible, half-moon-shaped
    groups consisting of 100 examples (data points) each:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在结果图中看到的，有两个明显的半月形状的组，每个组包含100个示例（数据点）：
- en: '![](img/B13208_11_14.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_14.png)'
- en: 'We will start by using the k-means algorithm and complete linkage clustering
    to see if one of those previously discussed clustering algorithms can successfully
    identify the half-moon shapes as separate clusters. The code is as follows:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用k-means算法和完全链接聚类，看看这些之前讨论过的聚类算法是否能成功地将半月形状识别为独立的聚类。代码如下：
- en: '[PRE23]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Based on the visualized clustering results, we can see that the k-means algorithm
    was unable to separate the two clusters, and also, the hierarchical clustering
    algorithm was challenged by those complex shapes:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 根据可视化的聚类结果，我们可以看到k-means算法无法将两个聚类分开，而且层次聚类算法也在面对这些复杂形状时遇到了挑战：
- en: '![](img/B13208_11_15.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_15.png)'
- en: 'Finally, let''s try the DBSCAN algorithm on this dataset to see if it can find
    the two half-moon-shaped clusters using a density-based approach:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在这个数据集上尝试DBSCAN算法，看看它是否能使用基于密度的方法找到两个半月形状的聚类：
- en: '[PRE24]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The DBSCAN algorithm can successfully detect the half-moon shapes, which highlights
    one of the strengths of DBSCAN – clustering data of arbitrary shapes:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN算法可以成功地检测到半月形状，这突显了DBSCAN的一个优势——能够对任意形状的数据进行聚类：
- en: '![](img/B13208_11_16.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B13208_11_16.png)'
- en: 'However, we should also note some of the disadvantages of DBSCAN. With an increasing
    number of features in our dataset—assuming a fixed number of training examples—the
    negative effect of the **curse of dimensionality** increases. This is especially
    a problem if we are using the Euclidean distance metric. However, the problem
    of the curse of dimensionality is not unique to DBSCAN: it also affects other
    clustering algorithms that use the Euclidean distance metric, for example, k-means
    and hierarchical clustering algorithms. In addition, we have two hyperparameters
    in DBSCAN (MinPts and ![](img/B13208_11_0411.png)) that need to be optimized to
    yield good clustering results. Finding a good combination of MinPts and ![](img/B13208_11_0413.png)
    can be problematic if the density differences in the dataset are relatively large.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们也应注意到DBSCAN的一些缺点。随着数据集中特征数量的增加——假设训练样本数量固定——**维度灾难**的负面影响会增加。如果我们使用欧几里得距离度量，这个问题尤为严重。然而，维度灾难问题并非DBSCAN特有：它也影响其他使用欧几里得距离度量的聚类算法，比如k-means和层次聚类算法。此外，DBSCAN有两个需要优化的超参数（MinPts和![](img/B13208_11_0411.png)），只有优化它们，才能得到良好的聚类结果。如果数据集中的密度差异较大，找到MinPts和![](img/B13208_11_0413.png)的良好组合可能会变得有问题。
- en: '**Graph-based clustering**'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于图的聚类**'
- en: 'So far, we have seen three of the most fundamental categories of clustering
    algorithms: prototype-based clustering with k-means, agglomerative hierarchical
    clustering, and density-based clustering via DBSCAN. However, there is also a
    fourth class of more advanced clustering algorithms that we have not covered in
    this chapter: **graph-based clustering**. Probably the most prominent members
    of the graph-based clustering family are the **spectral clustering** algorithms.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经了解了三种最基本的聚类算法类别：基于原型的k均值聚类、凝聚层次聚类和基于密度的DBSCAN聚类。然而，还有第四类更先进的聚类算法我们在本章中没有涉及：**基于图的聚类**。在基于图的聚类家族中，最为突出的算法可能是**谱聚类**算法。
- en: 'Although there are many different implementations of spectral clustering, what
    they all have in common is that they use the eigenvectors of a similarity or distance
    matrix to derive the cluster relationships. Since spectral clustering is beyond
    the scope of this book, you can read the excellent tutorial by Ulrike von Luxburg
    to learn more about this topic (*A tutorial on spectral clustering*, *U. Von Luxburg*,
    *Statistics and Computing*, 17(4): 395–416, *2007*). It is freely available from
    arXiv at [http://arxiv.org/pdf/0711.0189v1.pdf](http://arxiv.org/pdf/0711.0189v1.pdf).'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '尽管谱聚类有许多不同的实现方式，但它们的共同点是使用相似度或距离矩阵的特征向量来推导聚类关系。由于谱聚类超出了本书的讨论范围，你可以阅读Ulrike
    von Luxburg的精彩教程来了解更多相关内容（*谱聚类教程*，*U. Von Luxburg*，*统计与计算*，17(4): 395–416，*2007*）。这篇教程可以从arXiv免费下载，网址为[http://arxiv.org/pdf/0711.0189v1.pdf](http://arxiv.org/pdf/0711.0189v1.pdf)。'
- en: Note that, in practice, it is not always obvious which clustering algorithm
    will perform best on a given dataset, especially if the data comes in multiple
    dimensions that make it hard or impossible to visualize. Furthermore, it is important
    to emphasize that a successful clustering does not only depend on the algorithm
    and its hyperparameters; rather, the choice of an appropriate distance metric
    and the use of domain knowledge that can help to guide the experimental setup
    can be even more important.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，实际上并不总是显而易见，哪种聚类算法在给定数据集上表现最好，尤其是当数据维度较高，难以甚至不可能可视化时。此外，需要特别强调的是，成功的聚类不仅仅依赖于算法及其超参数；选择合适的距离度量方法和利用领域知识来指导实验设置可能更为重要。
- en: In the context of the curse of dimensionality, it is thus common practice to
    apply dimensionality reduction techniques prior to performing clustering. Such
    dimensionality reduction techniques for unsupervised datasets include principal
    component analysis and radial basis function kernel principal component analysis,
    which we covered in *Chapter 5*, *Compressing Data via Dimensionality Reduction*.
    Also, it is particularly common to compress datasets down to two-dimensional subspaces,
    which allows us to visualize the clusters and assigned labels using two-dimensional
    scatterplots, which are particularly helpful for evaluating the results.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在高维诅咒的背景下，通常的做法是在执行聚类之前应用降维技术。用于无监督数据集的降维技术包括主成分分析和径向基函数核主成分分析，这些内容我们在*第五章*《通过降维压缩数据》中已有涉及。此外，特别常见的做法是将数据集压缩到二维子空间，这使我们能够通过二维散点图可视化聚类和分配的标签，这对于评估结果尤为有帮助。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, you learned about three different clustering algorithms that
    can help us with the discovery of hidden structures or information in data. We
    started this chapter with a prototype-based approach, k-means, which clusters
    examples into spherical shapes based on a specified number of cluster centroids.
    Since clustering is an unsupervised method, we do not enjoy the luxury of ground
    truth labels to evaluate the performance of a model. Thus, we used intrinsic performance
    metrics, such as the elbow method or silhouette analysis, as an attempt to quantify
    the quality of clustering.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了三种不同的聚类算法，这些算法可以帮助我们发现数据中的隐藏结构或信息。我们从基于原型的方法——k均值聚类开始，它根据指定数量的聚类中心将样本聚集成球形。由于聚类是无监督方法，我们没有真实标签来评估模型的表现。因此，我们使用了内在的性能度量方法，如肘部法则或轮廓分析，试图量化聚类的质量。
- en: 'We then looked at a different approach to clustering: agglomerative hierarchical
    clustering. Hierarchical clustering does not require specifying the number of
    clusters upfront, and the result can be visualized in a dendrogram representation,
    which can help with the interpretation of the results. The last clustering algorithm
    that we covered in this chapter was DBSCAN, an algorithm that groups points based
    on local densities and is capable of handling outliers and identifying non-globular
    shapes.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接着看了一种不同的聚类方法：凝聚层次聚类。层次聚类不需要预先指定聚类的数量，结果可以通过树状图（dendrogram）形式进行可视化，这有助于对结果的解释。本章我们讲解的最后一个聚类算法是DBSCAN，这是一种基于局部密度对点进行分组的算法，能够处理异常值并识别非球形的形状。
- en: 'After this excursion into the field of unsupervised learning, it is now about
    time to introduce some of the most exciting machine learning algorithms for supervised
    learning: multilayer artificial neural networks. After their recent resurgence,
    neural networks are once again the hottest topic in machine learning research.
    Thanks to recently developed deep learning algorithms, neural networks are considered
    state-of-the-art for many complex tasks such as image classification and speech
    recognition. In *Chapter 12*, *Implementing a Multilayer Artificial Neural Network
    from Scratch*, we will construct our own multilayer neural network. In *Chapter
    13*, *Parallelizing Neural Network Training with TensorFlow*, we will work with
    the TensorFlow library, which specializes in training neural network models with
    multiple layers very efficiently by utilizing graphics processing units.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次关于无监督学习的探讨之后，现在是时候介绍一些最激动人心的监督学习算法：多层人工神经网络。随着其近期的复兴，神经网络再次成为机器学习研究中最热门的话题。得益于最近开发的深度学习算法，神经网络被认为是许多复杂任务（如图像分类和语音识别）的最前沿技术。在*第12章*，*从零开始实现多层人工神经网络*中，我们将构建自己的多层神经网络。在*第13章*，*使用TensorFlow并行化神经网络训练*中，我们将使用TensorFlow库，该库专门利用图形处理单元（GPU）非常高效地训练具有多层的神经网络模型。
