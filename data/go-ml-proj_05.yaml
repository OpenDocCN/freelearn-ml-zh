- en: Clean Up Your Personal Twitter Timeline by Clustering Tweets
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过聚类推文清理您的个人Twitter时间线
- en: 'Here''s a little bit of gossip for you: The original project for this title
    had to do with detecting foreign influence on US elections in social media. At
    about the same time, I was also applying for a visa to the United States, to give
    a series of talks. It later transpired that I hadn''t needed the visa after all;
    ESTA covered all the things I had wanted to do in the United States. But as I
    was preparing for the visa, an attorney gave me a very stern talking-to about
    writing a book on the politics of the United States. The general advice is this—if
    I don''t want trouble with US Customs and Border Patrol, I should not write or
    say anything on social media about American politics, and especially not write
    a chapter of a book on it. So, I had to hastily rewrite this chapter. The majority
    of methods used in this chapter can be used for the original purpose, but the
    content is a lot milder.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一点点八卦：这个标题的原始项目是关于检测社交媒体上外国对美国选举的影响。大约在同一时间，我还在申请美国的签证，去发表一系列演讲。后来发现，我根本不需要签证；ESTA涵盖了我在美国想要做的所有事情。但在准备签证的过程中，一位律师严厉地告诫我不要写一本关于美国政治的书。一般的建议是——如果我不想与美国海关和边境巡逻队发生麻烦，我就不应该在社交媒体上写或说任何关于美国政治的事情，更不要说写一本书的章节。所以，我不得不匆忙重写这一章。这一章中使用的多数方法都可以用于原始目的，但内容要温和得多。
- en: I use Twitter a lot. I mainly tweet and read Twitter in my downtime. I follow
    many people who share similar interests, among other things, machine learning,
    artificial intelligence, Go, linguistics, and programming languages. These people
    not only share interests with me; they also share interests with one another.
    As such, sometimes, multiple people may be tweeting about the same topic.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我大量使用Twitter。我主要在空闲时间发推文和阅读Twitter。我关注了许多有相似兴趣的人，包括机器学习、人工智能、围棋、语言学和编程语言。这些人不仅与我分享兴趣，彼此之间也分享兴趣。因此，有时可能会有多个人就同一主题发推文。
- en: As may be obvious from the fact that I use Twitter a lot, I am a novelty junkie.
    I like new things. Multiple people tweeting about the same topic is nice if I
    am interested in the differing viewpoints, but I don't use Twitter like that.
    I use Twitter as a sort of summary of interesting topics. Events X, Y, and Z happened.
    It's good enough that I know they happened. For most topics, there is no benefit
    for me to go deep and learn what the finer points are, and 140 characters is not
    a lot of characters for nuance anyway. Therefore, a shallow overview is enough
    to keep my general knowledge abreast with the rest of the population.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 从我大量使用Twitter这一事实可能显而易见，我是一个新鲜事物爱好者。我喜欢新鲜事物。如果我对不同的观点感兴趣，那么多人就同一主题发推文是件好事，但我不那样使用Twitter。我使用Twitter作为一种有趣话题的总结。X、Y、Z事件发生了。知道它们发生了就足够了。对于大多数话题，深入学习和了解细节对我没有好处，而且140个字符对于细微差别来说也不算多。因此，一个浅显的概述就足以让我的一般知识跟上其他人。
- en: Thus, when multiple people tweet about the same topic, that's repetition in
    my newsfeed. That's annoying. What if, instead of that, my feed could just be
    one instance of each topic?
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当多个人就同一主题发推文时，在我的新闻源中就是重复。这很烦人。如果我的新闻源中每个主题只出现一次，那会怎么样呢？
- en: I think of my Twitter-reading habit as happening in sessions. Each session is
    typically five minutes. I really only read about 100 tweets each session. If out
    of 100 tweets I read, 30% of the people I follow overlap on topics, then I really
    only have read 30 tweets of real content. That's not efficient at all! Efficiency
    means being able to cover more topics per session.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为我的Twitter阅读习惯是分批进行的。每次会话通常是五分钟。我实际上每次会话只阅读大约100条推文。如果在我阅读的100条推文中，我关注的30%的人重叠在某个话题上，那么实际上我只阅读了30条真实内容的推文。这根本就不高效！效率意味着每次会话能够覆盖更多的话题。
- en: So, how do you increase efficiency in reading tweets? Well, remove the tweets
    that cover the same topic of course! There is the secondary matter of choosing
    the best tweet that summarizes the topic, but that's a subject for another day.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，如何提高阅读推文的效率呢？当然，移除覆盖相同主题的推文！还有选择最好的一条总结该主题的推文的问题，但这将是另一天的主题。
- en: The project
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 项目
- en: What we're going to do is to cluster tweets on Twitter. We will be using two
    different clustering techniques, K-means and DBSCAN. For this chapter, we're going
    to rely on some skills we built up in [Chapter 2](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml),
    *Linear Regression – House Price Prediction*. We will also be using the same libraries
    used in Chapter 2, *Linear Regression – House Price Prediction*. On top of that,
    we will also be using the clusters library by mpraski.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的就是在Twitter上对推文进行聚类。我们将使用两种不同的聚类技术，K-means和DBSCAN。对于本章，我们将依赖我们在[第二章](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml)中构建的一些技能，*线性回归
    – 房价预测*。我们还将使用第二章中使用的相同库，*线性回归 – 房价预测*。除此之外，我们还将使用mpraski的聚类库。
- en: By the end of the project, we will be able to clean up any collection of tweets
    from Twitter, and cluster them into groups. The main body of code that fulfills
    the objective is very simple, it's only about 150 lines of code in total. The
    rest of the code is for fetching and preprocessing data.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 到项目结束时，我们将能够清理Twitter上的任何推文集合，并将它们聚类成组。实现目标的主体代码非常简单，总共只有大约150行代码。其余的代码用于获取和预处理数据。
- en: K-means
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means
- en: '**K-means** is a method of clustering data. The problem is posed as this—given
    a dataset of N items, we wish to partition the data into K groups. How do you
    do so?'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '**K-means**是一种数据聚类的方法。问题可以这样提出——给定一个包含N个项目的数据集，我们希望将数据划分为K组。你该如何做呢？'
- en: Allow me to take a side bar and explore the wonderful world of coordinates.
    No, no, don't run! It's very visual.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 让我稍微偏离一下主题，探索一下坐标的奇妙世界。不，不，别跑！这非常直观。
- en: '![](img/2b3bd494-5b0d-41b0-a81e-0401099e2b92.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2b3bd494-5b0d-41b0-a81e-0401099e2b92.png)'
- en: Which line is longer? How do you know?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 哪条线更长？你怎么知道？
- en: '![](img/7b12adde-1086-432d-bbcc-5ad5ccd91311.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7b12adde-1086-432d-bbcc-5ad5ccd91311.png)'
- en: 'You know which line is longer because you can measure each line from points
    a, b, c, and d. Now, let''s try something different:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道哪条线更长，因为你可以从点a、b、c和d测量每条线。现在，让我们尝试一些不同的事情：
- en: '![](img/2c45dafa-dfaa-4e0b-b663-7766b5ef57ec.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2c45dafa-dfaa-4e0b-b663-7766b5ef57ec.png)'
- en: Which dot is closest to X? How do you know?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个点离X最近？你怎么知道？
- en: 'You know because again, you can measure the distance between the dots. And
    now, for our final exercise:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道，因为你可以测量点之间的距离。现在，让我们进行最后的练习：
- en: '![](img/6035e6fc-1038-4cbe-9bfa-15c56ba47388.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6035e6fc-1038-4cbe-9bfa-15c56ba47388.png)'
- en: 'Consider the distance between the following:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下距离：
- en: '**A** and **X**'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** 和 **X**'
- en: '**A** and **Y**'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** 和 **Y**'
- en: '**A** and **Z**'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**A** 和 **Z**'
- en: '**B** and **X**'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** 和 **X**'
- en: '**B** and **Y**'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** 和 **Y**'
- en: '**B** and **Z**'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**B** 和 **Z**'
- en: '**C** and **X**'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C** 和 **X**'
- en: '**C** and **Y**'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C** 和 **Y**'
- en: '**C** and **Z**'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**C** 和 **Z**'
- en: What is the average distance between **A** and **X**, **B** and **X**, and **C**
    and **X**? What is the average distance between **A** and **Y**, **B** and **Y**
    and **C** and **Y**? What is the average distance between **A** and **Z**, **B**
    and **Z**, and **C** and **Z**?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**A** 和 **X**、**B** 和 **X**、**C** 和 **X**之间的平均距离是多少？**A** 和 **Y**、**B** 和 **Y**、**C**
    和 **Y**之间的平均距离是多少？**A** 和 **Z**、**B** 和 **Z**、**C** 和 **Z**之间的平均距离是多少？'
- en: If you had to choose one point between **X**, **Y**, and **Z** to represent
    the **A**, **B**, and **C**, which would you choose?
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你必须在**X**、**Y**和**Z**之间选择一个点来代表**A**、**B**和**C**，你会选择哪一个？
- en: Congratulations! You just did a very simple and abbreviated version of K-means
    clustering. Specifically, you did a variant where *k = 1*. If you had to pick
    two points between **X**, **Y**, and **Z**, then that's *k = 2*. A cluster is
    therefore the set of points that make it such that the average distance of the
    group is minimal.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你刚刚完成了一个非常简单和简化的K-means聚类。具体来说，你做的是一个变体，其中*k = 1*。如果你必须在**X**、**Y**和**Z**之间选择两个点，那么那将是*k
    = 2*。因此，聚类是由使组内平均距离最小化的点集组成的。
- en: That's a mouthful, but think back to what you just did. Now, instead of just
    three points, **A**, **B**, and **C**, you have many points. And you aren't given
    **X**, **Y**, or **Z**; you'd have to generate your own **X**, **Y**, and **Z**
    points. Then, you have to find the groups that minimize the distance to each possible
    points of **X**, **Y**, and **Z**.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这听起来很复杂，但回想一下你刚才做了什么。现在，你不再只有三个点**A**、**B**和**C**，而是有很多点。你没有给出**X**、**Y**或**Z**；你必须生成自己的**X**、**Y**和**Z**点。然后，你必须找到使每个可能的**X**、**Y**和**Z**点距离最小的组。
- en: That is, in a nutshell, K-means. It's easy to understand, but hard to implement
    it well. It turns out K-means is NP-hard; it may not be solved in polynomial time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 概而言之，这就是K-means。它容易理解，但难以良好实现。结果证明K-means是NP难的；它可能无法在多项式时间内解决。
- en: DBSCAN
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN
- en: '**DBSCAN** inherits the idea that data can be represented as multidimensional
    points. Again, sticking with a two-dimensional example, this is in rough steps
    how DBSCAN works:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '**DBSCAN**继承了数据可以表示为多维点的想法。再次，以二维为例，以下是DBSCAN大致的工作步骤：'
- en: Pick a point that has not been visited before.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个尚未访问的点。
- en: Draw a circle with the point as the center. The radius of the circle is epsilon.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以点为中心画一个圆。圆的半径是epsilon。
- en: Count how many other points fall into the circle. If there are more than a specified
    threshold, we mark all the points as being part of the same cluster.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算有多少其他点落入圆内。如果有超过指定的阈值，我们将所有点标记为属于同一个簇。
- en: Recursively do the same for each point in this cluster. Doing so expands the
    cluster.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对这个簇中的每个点递归地执行相同的操作。这样做会扩大簇。
- en: Repeat these steps.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复这些步骤。
- en: I highly encourage you to do this on dotted paper and try to draw this out yourself.
    Start by plotting random points, and use pencils to draw circles on paper. This
    will give you an intuition of how DBSCAN works. The picture shows my working that
    enhanced my intuition about how DBSCAN works. I found this intuition to be very
    useful.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你们在点状纸上尝试自己画出这个图。首先，绘制随机点，然后用铅笔在纸上画圆。这将给你们一个关于DBSCAN如何工作的直观感受。图片显示了我增强对DBSCAN工作原理直觉的工作。我发现这种直觉非常有用。
- en: Data acquisition
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据获取
- en: In the earlier exercises, I asked you to look at the dots and figure out the
    distance. This gives a hint as to how we need to think of our data. We need to
    think of our data as coordinates in some imaginary coordinate space. Now, our
    data won't be just two-dimensional, because it's textual. Instead, it'll be multidimensional.
    This gives us hints as to how our data will look—slices of numbers representing
    a coordinate in some arbitrarily large N-dimensional space.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的练习中，我要求你们观察点并计算出距离。这为我们如何思考数据提供了一些线索。我们需要将数据视为某个想象中的坐标系中的坐标。现在，我们的数据不会仅仅是二维的，因为它是文本的。相反，它将是多维的。这为我们提供了关于数据外观的线索——代表某个任意大的N维空间中坐标的数字切片。
- en: But, first, we'll need to get the data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，首先，我们需要获取数据。
- en: To acquire the tweets from the feed, we'll be using Aditya Mukherjee's excellent
    Anaconda library. To install it, simply run `go get -u github.com/ChimeraCoder/Anaconda`.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获取推文，我们将使用Aditya Mukherjee的出色Anaconda库。要安装它，只需运行`go get -u github.com/ChimeraCoder/Anaconda`。
- en: 'Of course, one can''t just grab data from Twitter willy-nilly. We will need
    to acquire data via the Twitter API. The documentation of Twitter''s API is the
    best source to get started: [https://developer.twitter.com/en/docs/basics/getting-started](https://developer.twitter.com/en/docs/basics/getting-started).'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，不能随意从Twitter抓取数据。我们需要通过Twitter API获取数据。Twitter API的文档是开始的好资源：[https://developer.twitter.com/en/docs/basics/getting-started](https://developer.twitter.com/en/docs/basics/getting-started)。
- en: 'You will need to first apply for a Twitter developer account (if you don''t
    already have it): [https://developer.twitter.com/en/apply/user](https://developer.twitter.com/en/apply/user).
    The process is rather lengthy and requires human approval for a developer account.
    Despite this, you don''t need developer access to develop this project. I thought
    I had access to Twitter''s API when I started, but it turns out I didn''t. The
    good news is, the Twitter API documentation page does provide enough examples
    to get started with developing the necessary data structures.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 你们需要首先申请一个Twitter开发者账户（如果你们还没有的话）：[https://developer.twitter.com/en/apply/user](https://developer.twitter.com/en/apply/user)。这个过程相当漫长，需要人工批准开发者账户。尽管如此，你们不需要开发者访问权限来开发这个项目。我开始时以为我有访问Twitter
    API的权限，但结果证明我没有。好消息是，Twitter API文档页面提供了足够的示例，可以帮助你们开始开发必要的数据结构。
- en: 'The specific end point that we''re interested in is this: [https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html).'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们感兴趣的具体终点是：[https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline.html)。
- en: Exploratory data analysis
  id: totrans-51
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: 'Let''s look at the `JSON` acquired from the Twitter API endpoint. A single
    tweet looks something like this (from the Twitter API documentation example):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看从Twitter API端点获取的`JSON`。单个推文看起来可能像这样（来自Twitter API文档示例）：
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We will be representing each individual tweet in a data structure that looks
    like this:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用类似这样的数据结构来表示每个单独的推文：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Note that we embed `anaconda.Tweet`, which is given as such in the Anaconda
    package:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们嵌入`anaconda.Tweet`，这在Anaconda包中是这样给出的：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In the interest of building the program, we''ll use the example tweets supplied
    by Twitter. I saved the example responses into a file called `example.json` and
    then a `mock` function is created to mock calling the API:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建程序，我们将使用Twitter提供的示例推文。我将示例响应保存到一个名为`example.json`的文件中，然后创建了一个`mock`函数来模拟调用API：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The utility function `dieIfErr` is defined as usual:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 实用函数`dieIfErr`被定义为通常：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note that in `mock`, no API calls to Twitter were made. In the future, we will
    be creating a function with a similar API so we can just replace the mock version
    of this function with the real one, which acquires the timeline from the API.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在`mock`中，没有对Twitter进行API调用。将来，我们将创建一个具有类似API的函数，这样我们就可以用真实的版本替换这个函数的模拟版本，从API获取时间线。
- en: 'For now, we can test that this works by the following program:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我们可以通过以下程序测试它是否工作：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This is the output I got:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我的输出结果：
- en: '[PRE6]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Data massage
  id: totrans-67
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据整理
- en: When we tested that the data structure made sense, we printed the `FullText`
    field. We wish to cluster based on the content of the tweet. What matters to us
    is that content. This can be found in the `FullText` field of the struct. Later
    on in the chapter, we will see how we may use the metadata of the tweets, such
    as location, to help cluster the tweets better.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们测试数据结构是否合理时，我们打印了`FullText`字段。我们希望根据推文的内文进行聚类。对我们来说，重要的是内容。这可以在结构的`FullText`字段中找到。在章节的后面，我们将看到我们如何可能使用推文的元数据，例如位置，来帮助更好地聚类推文。
- en: 'As mentioned in the previous sections, each individual tweet needs to be represented
    as a coordinate in some higher-dimensional space. Thus, our goal is to take all
    the tweets in a timeline and preprocess them in such a way that we can get this
    output table:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几节所述，每个单独的推文都需要在某个高维空间中表示为一个坐标。因此，我们的目标是获取时间线中的所有推文，并预处理它们，以便我们得到以下输出表：
- en: '[PRE7]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Each row in the table represents a tweet, indexed by the tweet ID. The columns
    that follow are words that exist in the tweet, indexed by its header. So, in the
    first row, `test` appears in the tweet, while `twitter`, `right`, and `wrong`
    do not. The slice of numbers `[0 1 0 0]` in the first row is the input we require
    for the clustering algorithms.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 表中的每一行代表一个推文，通过推文ID进行索引。接下来的列是推文中存在的单词，通过其标题进行索引。因此，在第一行中，`test`出现在推文中，而`twitter`、`right`和`wrong`没有出现。第一行中的数字切片`[0
    1 0 0]`是我们对聚类算法所需的输入。
- en: Of course, binary numbers indicating the presence of a word in a tweet isn't
    the best. It'd be more interesting if the relative importance of the word is used
    instead. Again, we turn to the familiar TF-IDF, first introduced in [Chapter 2](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml),
    *Linear Regression – House Price Prediction*, for this. More advanced techniques
    such as using word embeddings exist. But you'd be surprised how well something
    as simple as TF-IDF can perform.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，表示推文中单词存在的二进制数字并不是最好的。如果使用单词的相对重要性会更有趣。再次，我们转向熟悉的TF-IDF，它首次在[第2章](12c81095-6fcf-4da9-b554-6367d45b34f8.xhtml)中介绍，*线性回归
    – 房价预测*。更高级的技术，如使用词嵌入，也存在。但你会惊讶于像TF-IDF这样简单的东西可以表现得有多好。
- en: By now, the process should be familiar—we want to represent the text as a slice
    of numbers, not as a slice of bytes. In order to do so, we would have to require
    some sort of dictionary to convert the words in the text into IDs. From there,
    we can built the table.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，这个过程应该很熟悉了——我们希望将文本表示为数字切片，而不是字节切片。为了做到这一点，我们需要某种类型的字典来将文本中的单词转换为ID。从那里，我们可以构建表格。
- en: Again, like in [Chapter 2](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46), *Linear
    Regression – House Price* Prediction, we shall approach this with a simple tokenization
    strategy. More advanced tokenizers are nice, but not necessary for our purpose.
    Instead, we'll rely on good old `strings.Field`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，就像在 [第2章](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46)
    中，*线性回归 – 房价预测*，我们将采用简单的标记化策略。更高级的标记化器很棒，但对我们来说不是必需的。相反，我们将依赖古老的 `strings.Field`。
- en: The processor
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理器
- en: 'Having laid out our requirements, we can combine them into a single data structure
    that contains the things we need. Here''s how the processor data structure looks:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了我们的需求后，我们可以将它们组合成一个包含所需内容的单一数据结构。以下是处理器数据结构的外观：
- en: '[PRE8]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: For now, ignore the `locations` field. We shall look into how metadata might
    be useful in clustering.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，忽略 `locations` 字段。我们将研究元数据在聚类中的用途。
- en: 'To create a new `processor`, the following function is defined:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个新的 `processor`，定义了以下函数：
- en: '[PRE9]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Here, we see some interesting decisions. The corpus is constructed with a number
    of special strings—`mention`, `hashtag`, `retweet`, and `url.` These are defined
    as follows:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到一些有趣的决策。语料库是用一些特殊字符串构建的——`mention`、`hashtag`、`retweet` 和 `url`。这些定义如下：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Some of the designs of this is for historical reasons. A long time ago, before
    Twitter supported retweets as an action, people manually retweeted tweets by prepending
    `RT` on to tweets. If we are to analyze data far into the past (which we won't
    for this chapter), then we'd have to be aware of the historical designs of Twitter
    as well. So, you must design for that.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分设计的历史原因。很久以前，在Twitter支持转发作为动作之前，人们通过在推文前加上 `RT` 来手动转发推文。如果我们必须分析很久以前的数据（我们不会在本章中这样做），那么我们必须了解Twitter的历史设计。因此，你必须为此进行设计。
- en: But having constructed a corpus with special keywords implies something. It
    implies that when converting the text of a tweet into a bunch of IDs and numbers,
    mentions, hashtags, retweets, and URLs are all treated as the same. It implies
    we don't really want to care what the URL is, or who is mentioned. However, when
    it comes to hashtags, that's the interesting case.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，构建包含特殊关键词的语料库意味着某些事情。它意味着在将推文的文本转换为一系列ID和数字、提及、hashtag、转发和URL时，它们都被视为相同的。它意味着我们并不真正关心URL是什么，或者谁被提及。然而，当涉及到hashtag时，这是一个有趣的情况。
- en: A hashtag is typically used to denote the topic of the tweet. Think `#MeToo` or
    `#TimesUp`. A hashtag contains information. Compressing all hashtags into one
    single ID may not be useful. This is a point to note when we experiment later
    on.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 常用hashtag来表示推文的主题。例如 `#MeToo` 或 `#TimesUp`。hashtag包含信息。将所有hashtag压缩成一个单一的ID可能没有用。这是我们稍后实验时需要注意的一个点。
- en: 'Having said all that, here''s how a list of `*processedTweet` is processed.
    We will be revisiting and revising the function as the chapter goes on:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，以下是处理 `*processedTweet` 列表的方法。随着章节的进行，我们将重新访问和修改这个函数：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Let's go through this function line by line.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行分析这个函数。
- en: We start by ranging over all the `*processedTweets`. `a` is `[]*processedTweet`
    for a good reason—we want to modify the structure as we go along. If `a` were
    `[]processedTweet`, then we would have to either allocate a lot more, or have
    complicated modification schemes.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先遍历所有的 `*processedTweets`。`a` 是 `[]*processedTweet` 的原因——我们希望在过程中修改结构。如果
    `a` 是 `[]processedTweet`，那么我们就必须分配更多的空间，或者有复杂的修改方案。
- en: 'Each tweet is comprised of its `FullText`. We want to extract each word from
    the text, and then give each word its own ID. To do that, this is the loop:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每条推文由其 `FullText` 组成。我们想要从文本中提取每个单词，然后为每个单词分配一个ID。为此，这是循环：
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Preprocessing a single word
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理单个单词
- en: 'The `p.single` processes a single word. It returns the ID of the word, and
    whether to add it to the list of words that make up the tweet. It is defined as
    follows:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`p.single` 处理单个单词。它返回单词的ID，以及是否将其添加到构成推文的单词列表中。它定义如下：'
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We start by making the word lowercase. This makes words such as `café` and `Café`
    equivalent.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将单词转换为小写。这使得像 `café` 和 `Café` 这样的单词等效。
- en: Speaking of `café`, what would happen if there are two tweets mentioning a `café`,
    but one user writes `café` and the other writes cafe? Assume, of course, they
    both refer to the same thing. We'd need some form of normalization to tell us
    that they're the same.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 说到`café`，如果有两条推文都提到了`café`，但一个用户写成`café`，另一个用户写成cafe？当然，假设他们都指的是同一件事。我们需要某种归一化形式来告诉我们它们是相同的。
- en: Normalizing a string
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 字符串归一化
- en: First, the word is to be normalized into `NFKC` form. In [Chapter 2](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46), *Linear
    Regression–House Price Prediction*, this was introduced, but I then mentioned
    that LingSpam basically provides normalized datasets. In real-world data, which
    Twitter is, data is often dirty. Hence, we need to be able to compare them on
    an apples-to-apples basis.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，单词需要被归一化为`NFKC`形式。在[第2章](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46)，*线性回归-房价预测*中，这被介绍过，但我随后提到LingSpam基本上提供了归一化数据集。在现实世界的数据中，比如Twitter，数据通常是杂乱的。因此，我们需要能够以苹果对苹果的方式比较它们。
- en: 'To show this, let''s write a side program:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这一点，让我们写一个辅助程序：
- en: '[PRE14]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The first thing to note is that there are at least three ways of writing the
    word `café`, which for the purposes of this demonstration means coffee shop. It's
    clear from the first two comparisons that the words are not the same. But since
    they mean the same thing, a comparison should return `true`.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 首先要注意的是，至少有三种方式可以写出单词`café`，在这个演示中意味着咖啡馆。从前两个比较中很明显，这两个单词是不相同的。但既然它们意味着相同的事情，比较应该返回`true`。
- en: 'To do that, we will need to transform all the text to one form, and then comapare
    it. To do so, we would need to define a transformer:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要将所有文本转换成一种形式，然后进行比较。为此，我们需要定义一个转换器：
- en: '[PRE15]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This transformer is a chain of text transformers, applied one after another.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 这个转换器是一系列文本转换器的链，一个接一个地应用。
- en: First, we convert all the text to its decomposing form, NFD. This would turn
    `café` into `cafe\u0301`.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将所有文本转换为它的分解形式，NFD。这将`café`转换为`cafe\u0301`。
- en: 'Then, we remove any non-spacing mark. This turns `cafe\u0301` into `cafe`.
    This removal function is done with the `isMn` function, defined as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们移除任何非间隔符号。这会将`cafe\u0301`转换为`cafe`。这个移除函数是通过`isMn`函数完成的，定义如下：
- en: '[PRE16]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Lastly, convert everything to NKFC form for maximum compatibility and space
    saving. All three strings are now equal.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将所有内容转换为NKFC形式以实现最大兼容性和节省空间。现在这三个字符串都是相等的。
- en: 'Note that this type of comparison is done with one single assumption that belies
    it all: there is one language that we''re doing our comparisons in—English. **Café**
    in French means **coffee** as well as **coffee shop**. This kind of normalization,
    where we remove diacritical marks, works so long as removing a diacritic mark
    does not change the meaning of the word. We''d have to be more careful around
    normalization when dealing with multiple languages. But for this project, this
    is a good enough assumption.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种比较是基于一个单一的假设：我们正在进行比较的语言是英语。法语中的**Café**意味着**咖啡**以及**咖啡馆**。这种去除重音符号的归一化，只要去除重音符号不会改变单词的意义，就可以工作。在处理多种语言时，我们需要在归一化方面更加小心。但在这个项目中，这是一个足够好的假设。
- en: 'With this new knowledge, we will need to update our `processor` type:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 带着这些新知识，我们需要更新我们的`processor`类型：
- en: '[PRE17]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The first line of our `p.single` function would have to change too, from this:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们`p.single`函数的第一行也需要改变，从以下内容变为：
- en: '[PRE18]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It will change to this:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 它将变成这样：
- en: '[PRE19]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If you're feeling extra hard-working, try making `strings.ToLower` a `transform.Transformer`.
    It is harder than you might expect, but not as hard as it appears.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感觉特别勤奋，尝试将`strings.ToLower`转换为`transform.Transformer`。这比你想象的要难，但也没有你想象的那么难。
- en: Preprocessing stopwords
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理停用词
- en: Enough about normalization. We now turn our focus to `stopwords`.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 关于归一化就说到这里。我们现在将注意力转向`stopwords`。
- en: Recall from [Chapter 2](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46), *Linear
    Regression–House Price Prediction*, that `stopwords` are words such as **the**,
    **there**, **from**, and so on. They're connective words, useful in understanding
    the specific context of sentences, but for a naive statistical analysis, they
    often add nothing more than noise. So, we have to remove them.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第二章](https://cdp.packtpub.com/go_machine_learning_projects/wp-admin/post.php?post=28&action=edit#post_46)，*线性回归-房价预测*，停用词是一些像**the**、**there**、**from**这样的词。它们是连接词，有助于理解句子的特定上下文，但对于简单的统计分析来说，它们通常只会增加噪音。因此，我们必须移除它们。
- en: 'A check for `stopwords` is simple. If a word matches a `stopwords`, we''ll
    return `false` for whether to add the word ID into the sentence:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对停用词的检查很简单。如果一个词匹配`stopwords`，我们将返回`false`以确定是否将词ID添加到句子中：
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Where does the list of `stopwords` come from? It''s simple enough that I just
    wrote this in `stopwords.go`:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词列表从哪里来？这很简单，我就在`stopwords.go`中写了这个：
- en: '[PRE21]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[PRE22]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: And that's it! A tweet with content that looks like this—*an apple a day keeps
    the doctor away* would have the IDs for *apple*, *day*, *doctor*, and *away*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！内容看起来像这样的推文——*一天一个苹果，医生远离我*——会有*apple*、*day*、*doctor*和*away*的ID。
- en: The list of stopwords is adapted from the list that is used in the `lingo` package.
    The list of stopwords in the `lingo` package is meant to be used on lemmatized
    words. Because we're not lemmatizing, some words were manually added. It's not
    perfect but works well enough for our purpose.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词列表是从`lingo`包中使用的列表改编而来的。`lingo`包中的停用词列表是用来在词干化的单词上使用的。因为我们没有进行词干化，所以一些词是手动添加的。它并不完美，但足够满足我们的目的。
- en: Preprocessing Twitter entities
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预处理推特实体
- en: 'After we''ve removed the stopwords, it''s time to process the special Twitter
    entities:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们移除了停用词之后，就到了处理特殊的推特实体的时候了：
- en: '[PRE23]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: These are straightforwards enough.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都很直接。
- en: If a word begins with `"#"`, then it's a hashtag. We might want to come back
    to this later, so it's good to keep this in mind.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个词以`"#"`开头，那么它是一个标签。我们可能稍后会回到这个话题，所以记住这一点是好的。
- en: Any word that begins with a `"@"` is a mention. This is a little tricky. Sometimes,
    people tweet things such as `I am @PlaceName`, indicating a location, as opposed
    to mentioning a user (indeed, one may find `@PlaceName` does not exist). Or, alternatively,
    people may tweet something such as `I am @ PlaceName`. In this case, the solo `"@"` would
    still be treated as a mention. I found that for the former (`@PlaceName`), it
    doesn't really matter if the word is treated as a mention. Twitter's API does
    indeed return a list of mentions that you may check against. But for my personal
    timeline, this was extra work that isn't necessary. So, think of this as an extra
    credit project—check against the list of mentions from the API.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 任何以`"@"`开头的词都是提及。这有点棘手。有时，人们会发推文说诸如`I am @PlaceName`这样的话，表示一个地点，而不是提及一个用户（实际上，可能会发现`@PlaceName`并不存在）。或者，人们可能会发推文说`I
    am @ PlaceName`。在这种情况下，单独的`"@"`仍然会被视为提及。我发现对于前者（`@PlaceName`），将这个词视为提及并没有太大的关系。Twitter的API确实会返回一个提及列表，你可以对其进行检查。但对我来说，这只是一个不必要的额外工作。所以，把这当作一个加分项目——检查API返回的提及列表。
- en: Of course, we shan't be as lazy as to leave everything to extra credit; simple
    checks can be made—if `@` is solo, then we shouldn't treat it as a mention. It
    should be treated as `at`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们不应该那么懒惰，把所有事情都留给加分项目；可以做一些简单的检查——如果`@`是单独的，那么我们不应该将其视为提及。它应该被视为`at`。
- en: Now, we check for URLs. The line `if strings.HasPrefix(word, "http://")`checks
    for a `http://` prefix. This isn't good. This doesn't account for URLs with a
    `https` scheme.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们检查URL。行`if strings.HasPrefix(word, "http://")`检查`http://`前缀。这并不好。这没有考虑到使用`https`方案的URL。
- en: 'Now we know how to modify this section of the code. It looks like this:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了如何修改这段代码。它看起来是这样的：
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Lastly, a final line of code is added to handle historical tweets before retweets
    were supported by Twitter:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，添加了一行代码来处理在Twitter支持转发之前的历史推文：
- en: '[PRE25]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Processing a single tweet
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理单个推文
- en: 'Consider the following snippet of code:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码片段：
- en: '[PRE26]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: What it says is after we've preprocessed every single word, we simply add that
    word to the TFIDF.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 它的意思是在我们预处理了每个单词之后，我们只需简单地将该单词添加到TFIDF中。
- en: Clustering
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: The purpose of this project is to clean up the amount of tweets that I have
    to read. If there is a reading budget of 100 tweets, I don't want to be reading
    50 tweets on the same topic; they may well represent different viewpoints, but
    in general for skimming purposes, are not relevant to my interests. Clustering
    provides a good solution to this problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这个项目的目的是清理我需要阅读的推文数量。如果有100条推文的阅读预算，我不想阅读50条同一主题的推文；它们可能代表不同的观点，但一般来说，对于浏览目的，它们与我感兴趣的不相关。聚类为这个问题提供了一个很好的解决方案。
- en: First, if the tweets are clustered, the 50 tweets on the same topic will be
    grouped in the same cluster. This allows me to dig in deeper if I wish. Otherwise,
    I can just skip those tweets and move on.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，如果推文被聚类，同一主题的50条推文将被分组在同一个聚类中。这样，如果我想深入研究，我可以这样做。否则，我可以跳过这些推文并继续。
- en: In this project, we wish to use K-means. To do so, we'll use Marcin Praski's
    `clusters` library. To install it, simply run `go get -u github.com/mpraski/clusters`.
    It's a good library, and it comes built in with multiple clustering algorithms.
    I introduced K-means before, but we're also going to be using DBSCAN.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们希望使用K-means。为此，我们将使用Marcin Praski的`clusters`库。要安装它，只需运行`go get -u github.com/mpraski/clusters`。这是一个好的库，它内置了多个聚类算法。我之前介绍了K-means，但我们还将使用DBSCAN。
- en: Last, we're going to be using the DMMClust algorithm to compare against. The
    DMMClust algorithm is in a different library. To install it, simply run `go get
    -u github.com/go-nlp/dmmclust`. The purpose of DMMClust is to cluster small texts
    using an innovative process.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用DMMClust算法进行比较。DMMClust算法位于不同的库中。要安装它，只需运行`go get -u github.com/go-nlp/dmmclust`。DMMClust的目的使用创新的过程对小型文本进行聚类。
- en: Clustering with K-means
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means聚类
- en: 'As a recap, here''s what we did so far—we processed each tweet in a list of
    tweets from the home timeline to be a slice of `float64`. These represent the
    coordinates in the higher-dimensional space. Now, all we need to do is the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，到目前为止我们做了什么——我们将来自主页时间线的推文列表中的每条推文处理成`float64`的切片。这些代表高维空间中的坐标。现在，我们只需要做以下事情：
- en: Create a clusterer.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个聚类器。
- en: Create a `[][]float64` representing all the tweets from the timeline.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个`[][]float64`来表示时间线上的所有推文。
- en: Train the clusterer.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练聚类器。
- en: Predict which tweet belongs in which cluster.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测每条推文属于哪个聚类。
- en: 'It can be done as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 可以这样做：
- en: '[PRE27]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Surprised? Let's break it down.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 惊讶吗？让我们来分析一下。
- en: 'The first few lines are for processing `tweets`:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 前几行是用于处理`tweets`的：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We then create a clusterer:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们创建一个聚类器：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Here, we say we want a K-means clusterer. We'll train on the data 10,000 times,
    and we want it to find 25 clusters, using the `EuclideanDistance` method to calculate
    distances. The Euclidean distance is your bog standard distance calculation, the
    same one you'd use to calculate the distance between two points in the exercises
    in the K-means section before. There are other methods of calculating distances,
    which are more suited for textual data. Later in this chapter, I'll show you how
    to create a distance function, the Jacard distance, which is much better than
    Euclidean distance when used on text.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们说我们想要一个K-means聚类器。我们将对数据进行10,000次训练，并希望找到25个聚类，使用`EuclideanDistance`方法计算距离。欧几里得距离是标准的距离计算方法，与你在K-means部分之前的练习中计算两点之间距离的方法相同。还有其他计算距离的方法，它们更适合文本数据。在本章的后面部分，我将向你展示如何创建一个距离函数，即Jaccard距离，当用于文本时，它比欧几里得距离要好得多。
- en: 'After we''ve created a clusterer, we need to convert our list of `tweets` into
    a matrix. We then train the clusterer:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建聚类器之后，我们需要将我们的`tweets`列表转换为矩阵。然后我们训练聚类器：
- en: '[PRE30]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And, finally, we display the `clusters`:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们显示`clusters`：
- en: '[PRE31]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Clustering with DBSCAN
  id: totrans-166
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DBSCAN聚类
- en: 'Clustering with DBSCAN using Marcin''s package is equally simple. In fact,
    you would just need to change one single line of code from this:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Marcin的包进行DBSCAN聚类同样简单。实际上，你只需要更改一行代码，如下所示：
- en: '[PRE32]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'You would change it to this:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要将其改为这样：
- en: '[PRE33]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Now, of course, the question is what values should `eps` and `minPts` be?
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现在的问题是`eps`和`minPts`应该取什么值？
- en: '`eps` represents the minimum distance required for two points to be considered
    a neighbor. `minPts` is the minimum number of points to form a dense cluster.
    Let''s address `eps` first.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '`eps`代表两个点被认为是邻居所需的最小距离。`minPts`是形成密集聚类的最小点数。让我们先讨论`eps`。'
- en: How do we know what the best distance is? A good way to figure this out is usually
    to visualize the data. In fact, this is what the original inventors of the DBSCAN
    algorithm suggests. But what exactly are we to visualize?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道最佳的距离是多少？通常，一个好的方法是通过可视化数据来找出答案。事实上，这正是DBSCAN算法的原始发明者所建议的。但我们究竟要可视化什么呢？
- en: 'We want to visualize the distance between the tweets. Given a dataset, we can
    compute a distance matrix that looks something like this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要可视化推文之间的距离。给定一个数据集，我们可以计算一个看起来像这样的距离矩阵：
- en: '[PRE34]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'To do so, we write the following function:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们编写了以下函数：
- en: '[PRE35]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This function takes a matrix of floats; each row represents a tweet, and finds
    the top k-nearest neighbors. Let's walk through the algorithm. As we walk though
    the algorithm, bear in mind that each row is a tweet; you can think of each row
    therefore as a very complicated coordinate.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数接受一个浮点数矩阵；每一行代表一条推文，并找到最接近的k个邻居。让我们来分析一下这个算法。在我们分析算法的过程中，请记住每一行代表一条推文；因此，你可以将每一行想象成一个非常复杂的坐标。
- en: 'The first thing we want to do is to find the distance between a tweet and another
    tweet, hence the following block:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要做的第一件事是找到一条推文与另一条推文之间的距离，因此有以下的代码块：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Of particular note are the two expressions `for _, row := range a` and `for
    _, row2 := range a`. In a normal KNN function, you'd have two matrices, `a` and
    `b`, and you'd find the distance between a tweet in `a` and a tweet in `b`. But
    for the purposes of drawing this chart, we are going to compare tweets within
    the same dataset.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 特别值得注意的是两个表达式`for _, row := range a`和`for _, row2 := range a`。在一个普通的KNN函数中，你会有两个矩阵，`a`和`b`，你会在`a`中的推文和`b`中的推文之间找到距离。但为了绘制这张图表的目的，我们将比较同一数据集中的推文。
- en: 'Once we acquired all the distances, we want to find the closest neighbors,
    so we sort the list and then put them in the distance matrix:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们获得了所有的距离，我们想要找到最近的邻居，所以我们排序列表，然后将它们放入距离矩阵中：
- en: '[PRE37]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This, in a very quick way, is how to do K-nearest neighbors. Of course, it's
    not the most efficient. The algorithm I've shown here is *O(n^2)*. There are better
    ways of doing things, but for the purpose of this project, this suffices.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 这，以非常快捷的方式，就是如何进行K近邻算法。当然，这并不是最有效的方法。我在这里展示的算法是*O(n^2)*。当然有更好的方法来做这件事，但出于这个项目的目的，这已经足够了。
- en: 'After that, we grab the last column of the matrix and sort the last column.
    This is what we wish to plot. The plotting code is not unlike that seen in previous
    chapters. I shall provide it here with no further elaboration on how to use it:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们抓取矩阵的最后一列并对其进行排序。这是我们想要绘制的。绘图代码与之前章节中看到的不太一样。我将在这里提供它，不再进一步解释如何使用它：
- en: '[PRE38]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'When I plot the real Twitter data to figure out the ideal `eps`, I get the
    following output:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 当我绘制真实的Twitter数据以确定理想的`eps`值时，我得到了以下输出：
- en: '![](img/ee722fa9-fd28-45b5-9f84-2f49e9dc4954.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ee722fa9-fd28-45b5-9f84-2f49e9dc4954.png)'
- en: What you want to find is an `elbow` or `knee` in the picture. Unfortunately,
    as you can tell, there are many of them. This is going to make clustering with
    the DBSCAN algorithm difficult. What this means is that the data is rather noisy.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要找到的是图片中的“肘部”或“膝盖”。不幸的是，正如你可以看到的，有很多这样的点。这将使得使用DBSCAN算法进行聚类变得困难。这意味着数据相当嘈杂。
- en: One of the things that is of particular importance is the distance function
    used. I will go into this a little further in following sections on tweaking the
    program.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个特别重要的事情是使用的距离函数。我将在后续的章节中进一步介绍如何调整程序。
- en: Clustering with DMMClust
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DMMClust进行聚类
- en: Having been somewhat discouraged by the distance plot of my Twitter home feed,
    I looked into another way of clustering tweets. To that end, I used the `dmmclust`
    library (of which I am the primary author). The purpose of the DMMClust algorithm
    is that it is able to handle small texts quite well. Indeed, it was written to
    handle the problem of having small text.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的Twitter主频道的距离图让我有些气馁之后，我寻找了另一种聚类推文的方法。为此，我使用了`dmmclust`库（我是其主要作者）。DMMClust算法的目的在于它能够很好地处理小文本。事实上，它是为了处理小文本的问题而编写的。
- en: What exactly is a small text? Most text clustering research out there is done
    on texts with large amounts of words. Twitter, up to very recently, only supported
    140 characters. As you may imagine, the amount of information that 140 characters
    to be transmitted as human language is not very much.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是小文本？大多数文本聚类研究都是在大量单词的文本上进行的。直到最近，Twitter只支持140个字符。正如你可以想象的，140个字符作为人类语言传递的信息量并不多。
- en: The DMMClust algorithm works very much like students joining high school social
    clubs. Imagine the tweets as a bunch of students. Each student randomly joins
    a social club. Within each social club, they may like their fellow members of
    the club, or they may not. If they do not like the people in the group, they are
    allowed to change social clubs. This happens until all the clubs have people who
    like each other the most, or until the amount of iterations runs out.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: DMMClust算法的工作方式非常类似于学生加入高中社交俱乐部。想象一下推文就像一群学生。每个学生随机加入一个社交俱乐部。在每一个社交俱乐部中，他们可能喜欢俱乐部的其他成员，或者他们可能不喜欢。如果他们不喜欢小组中的人，他们被允许更换社交俱乐部。这会一直发生，直到所有俱乐部都有最喜欢彼此的人，或者直到迭代次数用完。
- en: This, in a nutshell, is how the DMMClust algorithm works.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，这就是DMMClust算法的工作原理。
- en: Real data
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 真实数据
- en: Up to this point, we've been working on an example `JSON` that the Twitter documentation
    provides. I assume by now you have your Twitter API access. So, let's get real
    Twitter data!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在处理Twitter文档提供的示例`JSON`。我假设您现在已经有了Twitter API访问权限。那么，让我们获取真实的Twitter数据吧！
- en: 'To get your API keys from the developer portal, click on the Get Started link.
    You will come to a page such as this:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要从开发者门户获取您的API密钥，请点击“开始”链接。您将来到如下页面：
- en: '![](img/ba823fed-8ca0-4c68-94b9-4dceb42f62da.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ba823fed-8ca0-4c68-94b9-4dceb42f62da.png)'
- en: 'Select Create an app. You will be brought to a page that looks like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 选择“创建应用”。您将被带到如下页面：
- en: '![](img/5bfe0179-c124-46df-b8f7-421c22ec9fea.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/5bfe0179-c124-46df-b8f7-421c22ec9fea.png)'
- en: 'I had previously created a Twitter app a long time ago (it had very similar
    features to the one we''re creating in this project); hence, I have an app there
    already. Click on the blue Create an app button at the top right. You will be
    brought to the following form:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前创建了一个Twitter应用很久以前（它具有与我们在本项目创建的应用非常相似的功能）；因此，我已经在那里有一个应用了。点击右上角的蓝色“创建应用”按钮。您将被带到以下表单：
- en: '![](img/241b3ba0-075d-4278-91d6-9cc220c5d281.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/241b3ba0-075d-4278-91d6-9cc220c5d281.png)'
- en: 'Fill in the form then click submit. It might take a few days before you receive
    an email saying the app has been approved for development. Be sure to be truthful
    in the description. Lastly, you should then be able to click into your app, and
    get the following page, which shows your API key and secret:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 填写表格后点击提交。可能需要几天时间您才会收到一封邮件，告知您的应用已获批准开发。请确保在描述中保持真实。最后，您应该能够点击进入您的应用，并看到以下页面，其中显示了您的API密钥和密钥：
- en: '![](img/02da6d9d-162e-4652-a314-42b97726412e.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/02da6d9d-162e-4652-a314-42b97726412e.png)'
- en: Click Create to create your access token and access token secret. You'll be
    needing them.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“创建”以创建您的访问令牌和访问令牌密钥。您将需要它们。
- en: 'Now that we have our API access key, this is how you''d access Twitter using
    the Anaconda package:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了API访问密钥，这是使用Anaconda包访问Twitter的方法：
- en: '[PRE39]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'At first glance, this snippet of code is a little weird. Let''s go through
    the code line by line. The first six lines deal with the access tokens and keys.
    Obviously, they should not be hardcoded in. A good way to handle secrets like
    these is to put them in environment variables. I''ll leave that as an exercise
    to the reader. We''ll move on to the rest of the code:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 初看，这段代码有点奇怪。让我们逐行分析代码。前六行处理访问令牌和密钥。显然，它们不应该被硬编码。处理这类秘密的一个好方法是将它们放入环境变量中。我将把这留作读者的练习。我们将继续分析代码的其余部分：
- en: '[PRE40]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'These two lines uses the Anaconda library to get the tweets found in the Home
    timeline. The `nil` being passed in may be of interest. Why would one do this?
    The `GetHomeTimeline` method takes a map of `url.Values`. The package can be found
    in the standard library as `net/url`. Values is defined thus:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这两行代码使用Anaconda库获取主时间线中找到的推文。传入的`nil`可能值得关注。为什么会这样做呢？`GetHomeTimeline`方法接受一个`url.Values`映射。该包可以在标准库中找到，作为`net/url`。`Values`定义如下：
- en: '[PRE41]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'But what do the values represent? It turns out that you may pass some parameters
    to the Twitter API. The parameters and what they do are enumerated here: [https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline).
    I don''t wish to limit anything, so passing in `nil` is acceptable.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些值代表什么呢？实际上，您可以向Twitter API传递一些参数。参数及其作用在此列举：[https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline](https://developer.twitter.com/en/docs/tweets/timelines/api-reference/get-statuses-home_timeline)。我不想限制任何东西，所以传入`nil`是可以接受的。
- en: 'The result is `[]anaconda.Tweet`, all neatly packaged up for us to use. The
    following few lines are therefore quite odd:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 `[]anaconda.Tweet`，所有内容都整齐地打包供我们使用。因此，以下几行相当奇怪：
- en: '[PRE42]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Why would I want to save this as a `JSON` file? The answer is simple—when using
    machine learning algorithms, you may need to tune the algorithm. Saving the request
    as a `JSON` file serves two purposes:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我想将其保存为 `JSON` 文件？答案很简单——当使用机器学习算法时，你可能需要调整算法。将请求保存为 `JSON` 文件有两个目的：
- en: It allows for consistency. Under active development, you would expect to tweak
    the algorithm a lot. If the JSON file keeps changing, how do you know if it's
    the tweaks that are making the improvements, and not because the JSON has changed?
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它允许保持一致性。在积极开发中，你可能会大量调整算法。如果 JSON 文件不断变化，你怎么知道是调整带来了改进，而不是因为 JSON 的变化？
- en: Being a good citizen. Twitter's API is rate limited. This means you cannot request
    the same thing over and over again too many times. While testing and tuning machine
    learning algorithms, you are likely to have to repeatedly process your data over
    and over again. Instead of hammering the Twitter servers, you should be a good
    citizen and use a locally cached copy.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成为一名好公民。Twitter 的 API 有速率限制。这意味着你不能反复多次请求相同的内容。在测试和调整机器学习算法时，你可能会不得不反复处理你的数据。与其不断敲打
    Twitter 服务器，你更应该成为一名好公民，并使用本地缓存的副本。
- en: We defined `load` earlier. Again, we shall see its usefulness in the context
    of tweaking the algorithms.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前定义了 `load`。再次强调，我们将看到它在调整算法方面的用途。
- en: The program
  id: totrans-220
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 程序
- en: 'Once we''ve done that, we may move the previous `main()` into a different function,
    leaving ourselves with a blank canvas for `main()` again. We''re now ready for
    the meat of the program. This is a skeleton program. You''re encouraged to actually
    actively change the program while writing this:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们这样做，我们可以将之前的 `main()` 移入不同的函数中，再次为我们留下一个空白的 `main()` 画布。我们现在准备好程序的实质性内容。这是一个骨架程序。鼓励你在编写这个程序时实际积极地修改程序：
- en: '[PRE43]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'There are some utility functions that I have yet to show you. Now it''s time
    to define them:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我还有一些实用函数尚未向你展示。现在是时候定义它们了：
- en: '[PRE44]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: These are some of the utility functions that may be found in `utils.go`. They
    mainly help with tweaking the program. Now run the program by typing `go run *.go`.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是一些可能在 `utils.go` 中找到的实用函数。它们主要帮助调整程序。现在通过输入 `go run *.go` 来运行程序。
- en: Tweaking the program
  id: totrans-226
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整程序
- en: If you have been following up to this point, you may get very poor results from
    all the clustering algorithms. I'd like to remind you that the stated objective
    of this book in general is to impart an understanding of what it's like to do
    data science in Go. For the most part, I have advocated a method that can be described
    as think hard about the problem, then write the answers down. But the reality
    is that often trial and error are required.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直跟到现在，你可能从所有聚类算法中得到非常差的结果。我想提醒你，这本书的一般目标通常是传授在 Go 中进行数据科学的感觉。在大多数情况下，我提倡一种可以描述为深入思考问题，然后写下答案的方法。但现实是，通常需要试错。
- en: The solution that works for me on my Twitter home timeline may not work for
    you. For example, this code works well on a friend's Twitter feed. Why is this?
    He follows a lot of similar people who talk about similar things at the same time.
    It's a little harder to cluster tweets in my Twitter home feed. I follow a diverse
    array of people. The people I follow don't have set schedules of tweeting and
    do not generally interact with other Twitter users. Therefore, the tweets are
    generally quite diverse already.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的 Twitter 主时间线中对我有效的解决方案可能对你不起作用。例如，这段代码在朋友的 Twitter 动态上运行良好。为什么是这样？他关注了很多同时谈论相似话题的相似的人。在我自己的
    Twitter 主时间线中聚类推文要困难一些。我关注的人群多样化。我关注的人没有固定的发推时间表，并且通常不与其他 Twitter 用户互动。因此，推文本身就已经非常多样化了。
- en: It is with this in mind that I encourage you to experiment and tweak your program.
    In the subsections that follow, I shall outline what worked for me. It may not
    work for you.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 正是出于这个考虑，我鼓励你实验并调整你的程序。在接下来的小节中，我将概述对我有效的方法。这可能对你不起作用。
- en: Tweaking distances
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整距离
- en: 'Up to this point, we had been using Euclidean distance as provided by the `Marcin`
    library. The Euclidean distance is computed as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用由 `Marcin` 库提供的欧几里得距离。欧几里得距离的计算方法如下：
- en: '[PRE45]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The `EuclideanDistance` is a good metric to use when it comes to coordinates
    in a Cartesian space. Indeed, earlier I had drawn up an analogy of thinking of
    a tweet as a bunch of coordinates in space, to explain K-means and DBSCAN. The
    reality is that text documents aren't really in Cartesian space. You may think
    of them as being in Cartesian space, but they are not strictly so.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '`EuclideanDistance`是处理笛卡尔空间中的坐标时一个好的度量标准。确实，我之前曾将推文比作空间中的一组坐标，以解释K-means和DBSCAN。但现实是文本文档并不真正位于笛卡尔空间中。你可以将它们视为位于笛卡尔空间中，但它们并不严格如此。'
- en: So, allow me to introduce another type of distance, one that is more suited
    to dealing with textual elements in a bag-of-words-style setting that we're currently
    doing, the Jaccard distance.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我介绍另一种类型的距离，它更适合处理我们在当前设置中使用的词袋模型中的文本元素，即Jaccard距离。
- en: 'The Jaccard distance is defined as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Jaccard距离定义为以下：
- en: '[PRE46]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Here, `$A$` and `$B$` are sets of words in each tweet. The implementation of
    the Jaccard distance in Go is rudimentary, but it works:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`$A$`和`$B$`是每个推文中的单词集合。Go语言中Jaccard距离的实现是基础的，但它是有效的：
- en: '[PRE47]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Tweaking the preprocessing step
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 调整预处理步骤
- en: One thing you may note is that the preprocessing of tweets is very minimal,
    and some of the rules are odd. For example, all hashtags are treated as one, as
    are all links and mentions. When this project started, it seemed like a good reason.
    There is no other justification than it seemed like a good reason; one always
    needs a springboard from which to jump off in any project. A flimsy excuse at
    that point is as good as any other.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能注意到，推文的预处理非常简单，其中一些规则很奇怪。例如，所有哈希标签都被视为一个，所有链接和提及也是如此。当这个项目开始时，这似乎是一个合理的理由。没有其他理由比这更合理；在任何项目中，总是需要一个跳板来开始。在那个阶段，一个薄弱的借口和其他借口一样好。
- en: 'Nonetheless, I have tweaked my preprocessing steps. These are the functions
    that I finally settled on. Do observe the difference between this and the original, listed
    in previous sections:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我还是调整了我的预处理步骤。这些是我最终确定下来的函数。请注意，这与之前章节中列出的原始版本之间的差异：
- en: '[PRE48]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The most notable thing that I have changed is that I now consider a hashtag
    a word. Mentions are removed. As for URLs, in one of the attempts at clustering,
    I realized that the clustering algorithms were clustering all the tweets with
    a URL into the same cluster. That realization made me remove hashtags, mentions,
    and URLs. Hashtags have the `#` removed and are treated as if they were normal
    words.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我所做的最显著的变化是，现在我将哈希标签视为一个单词。提及被移除。至于URL，在一次尝试聚类时，我意识到聚类算法将所有包含URL的推文聚到了同一个簇中。这个认识让我移除了哈希标签、提及和URL。哈希标签的`#`被移除，并被视为普通单词。
- en: 'Furthermore, you may note that I added some quick and dirty ways to `clean`
    certain things:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可能注意到我添加了一些快速且简单的方法来`清理`某些事物：
- en: '[PRE49]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Here, I used regular expressions to replace multiple newlines with just one,
    and to replace all HTML-encoded text with nothing. Lastly, I removed all punctuation.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我使用了正则表达式将多个换行符替换为一个，并将所有HTML编码的文本替换为空。最后，我移除了所有标点符号。
- en: In a more formal setting, I would use a proper lexer to handle my text. The
    lexer I'd use would come from Lingo (`github.com/chewxy/lingo`). But given that
    Twitter is a low value environment, there wasn't much point in doing so. A proper
    lexer like the one in lingo flags text as multiple things, allowing for easy removal.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 在更正式的设置中，我会使用一个合适的词法分析器来处理我的文本。我会使用的词法分析器来自Lingo（`github.com/chewxy/lingo`）。但鉴于Twitter是一个低价值环境，这样做并没有太多意义。Lingo中的合适词法分析器会将文本标记为多个类别，从而便于移除。
- en: 'Another thing you might notice is that I changed the definition of what a tweet
    is mid-flight:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 另一件你可能注意到的事情是，我在中途改变了关于推文的定义：
- en: '[PRE50]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: This block of code says if a tweet is indeed a retweeted status, replace the
    tweet with the retweeted tweet. This works for me. But it may not work for you.
    I personally consider any retweet to be the same as repeating a tweet. So, I do
    not see why they should be separate. Additionally, Twitter allows for users to
    comment on a retweet. If you want to include that, you'd have to change the logic
    a little bit more. Either way, the way I got to this was by manually inspecting
    the `JSON` file I had saved.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码表示如果一条推文确实是一条转发的状态，则用转发的推文替换该推文。这对我是有效的。但可能对你来说不一定有效。我个人认为任何转发都等同于重复一条推文。因此，我认为它们不应该分开。此外，Twitter允许用户对转发进行评论。如果你想包含这些评论，你可能需要稍微改变一下逻辑。无论如何，我是通过手动检查我保存的`JSON`文件来达到这个结果的。
- en: It's asking these questions and then making a judgment call what is important
    in doing data science, either in Go or any other language. It's not about blindly
    applying algorithms. Rather, it's always driven by what the data tells you.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 它是询问这些问题，然后做出判断，确定在数据科学中什么重要，无论是使用Go语言还是任何其他语言。这并不是盲目地应用算法。相反，它始终由数据告诉你的信息所驱动。
- en: 'One last thing that you may note is this curious block of code:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点你可能注意到的是这个奇怪的代码块：
- en: '[PRE51]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Here, I only consider English tweets. I follow many people who tweet in a variety
    of languages. At any given time, my home timeline would have about 15% of tweets
    in French, Chinese, Japanese, or German. Clustering tweets in a different language
    is a whole different ballgame, so I chose to omit them.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我只考虑英文推文。我关注许多使用各种语言发推的人。在任何给定的时间，我的主页时间线大约有15%的推文是法语、中文、日语或德语。对不同语言的推文进行聚类是完全不同的游戏，所以我选择省略它们。
- en: Summary
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have learned how to cluster tweets using a variety of clustering
    methods. Though frequently touted as one of the most robust algorithms, we've
    shown that DBSCAN has problems with clustering tweets due to the nature of tweets
    being noisy. Instead, we see that older, more traditional methods, as well as
    a new method of clustering, would yield better results.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了如何使用各种聚类方法对推文进行聚类。尽管经常被吹捧为最稳健的算法之一，但我们已经表明，由于推文的本质是嘈杂的，DBSCAN在聚类推文时存在问题。相反，我们发现，较老的传统方法以及一种新的聚类方法会得到更好的结果。
- en: This points to a lesson—there is no one machine-learning algorithm to rule them
    all; there is no ultimate algorithm. Instead, we need to try more than one thing.
    In the chapters that follow, this theme will be more apparent, and we shall approach
    these with more rigor. In the next chapter, we will learn about basics of neural
    networks and apply them on handwriting to recognize digits.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这指出了一个教训——没有一种机器学习算法可以统治一切；没有终极算法。相反，我们需要尝试不止一种方法。在接下来的章节中，这个主题将更加明显，我们将以更严谨的态度来处理这些问题。在下一章中，我们将学习神经网络的基础知识，并将它们应用于手写数字的识别。
