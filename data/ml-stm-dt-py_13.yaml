- en: '*Chapter 10*: Feature Transformation and Scaling'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：特征转换和扩展'
- en: In the previous chapter, you have seen how to manage drift and drift detection
    in streaming and online machine learning models. Drift detection, although not
    the main concept in machine learning, is a very important accessory aspect of
    machine learning in production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，您已经看到了如何在流式和在线机器学习模型中管理漂移和漂移检测。漂移检测，虽然不是机器学习中的主要概念，但在生产中的机器学习是一个非常重要的辅助方面。
- en: Although many secondary topics are important in machine learning, some of the
    accessory topics are especially important with online models. Drift detection
    is particularly important, as the model's autonomy in relearning makes it slightly
    more black-box to the developer or data scientist. This has great advantages only
    as long as the retraining process is correctly managed by drift detection and
    comparable methods.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管许多次要主题在机器学习中都很重要，但一些辅助主题对于在线模型尤为重要。漂移检测尤其重要，因为模型在重新学习时的自主性使得它对开发者或数据科学家来说稍微更黑盒。只要重新训练过程由漂移检测和类似方法正确管理，这就有很大的优势。
- en: In this chapter, you will see another secondary machine learning topic that
    has important implications for online machine learning and streaming. Feature
    transformation and scaling are practices that are relatively well defined in traditional,
    batch machine learning. They do not generally pose any theoretical difficulty.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您将看到另一个对在线机器学习和流式学习有重要影响的次要机器学习主题。特征转换和扩展是在传统、批量机器学习中相对定义良好的实践。它们通常不会带来任何理论上的困难。
- en: In online machine learning, scaling and feature transformation is not as straightforward.
    It is necessary to adapt the practice to the possibility that new data is not
    exactly comparable to the original data. This causes questions as to whether or
    not to refit feature transformations and scalers on every new piece of data arriving,
    but also on whether such practices will introduce bias into your already trained
    and continuously re-training models.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在在线机器学习中，扩展和特征转换并不像传统的那样直接。必须将实践适应新数据可能并不完全与原始数据可比的情况。这引发了是否需要在每条新到达的数据上重新拟合特征转换和扩展器的问题，同时也引发了是否这些做法会引入偏差到您已经训练并持续再训练的模型中。
- en: 'The topics that are covered in this chapter are as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的主题如下：
- en: Challenges of data preparation with streaming data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式数据数据准备挑战
- en: Scaling data for streaming
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式数据的数据扩展
- en: Transforming features in a streaming context
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在流式环境中转换特征
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'You can find all the code for this book on GitHub at the following link: [https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python).
    If you are not yet familiar with Git and GitHub, the easiest way to download the
    notebooks and code samples is the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下链接在GitHub上找到本书的所有代码：[https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python](https://github.com/PacktPublishing/Machine-Learning-for-Streaming-Data-with-Python)。如果您还不熟悉Git和GitHub，以下是最简单的下载笔记本和代码示例的方法：
- en: Go to the link of the repository.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 前往存储库的链接。
- en: Go to the green **Code** button.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击绿色的**代码**按钮。
- en: Select **Download ZIP**.
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择**下载ZIP**。
- en: When you download the ZIP file, unzip it in your local environment, and you
    will be able to access the code through your preferred Python editor.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当您下载ZIP文件时，在您的本地环境中解压缩它，您将能够通过您首选的Python编辑器访问代码。
- en: Python environment
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python环境
- en: To follow along with this book, you can download the code in the repository
    and execute it using your preferred Python editor.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随本书学习，您可以下载存储库中的代码，并使用您首选的Python编辑器执行它。
- en: If you are not yet familiar with Python environments, I would advise you to
    check out Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual)),
    which comes with Jupyter Notebook and JupyterLabs, which are both great for executing
    notebooks. It also comes with Spyder and VSCode for editing scripts and programs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还不熟悉Python环境，我建议您查看Anaconda ([https://www.anaconda.com/products/individual](https://www.anaconda.com/products/individual))，它包含Jupyter
    Notebook和JupyterLabs，这两个都是执行笔记本的绝佳选择。它还包含Spyder和VSCode，用于编辑脚本和程序。
- en: If you have difficulty installing Python or the associated programs on your
    machine, you can check out Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    or Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code)),
    which both allow you to run Python code in online notebooks for free, without
    any setup required.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你难以在你的机器上安装 Python 或相关程序，你可以查看 Google Colab ([https://colab.research.google.com/](https://colab.research.google.com/))
    或 Kaggle Notebooks ([https://www.kaggle.com/code](https://www.kaggle.com/code))，这两个都允许你免费在线笔记本中运行
    Python 代码，无需任何设置。
- en: Note
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The code in the book will generally use Colab and Kaggle Notebooks with Python
    version *3.7.13*, and you can set up your own environment to mimic this.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的代码通常使用带有 Python 版本 *3.7.13* 的 Colab 和 Kaggle Notebooks，你可以设置自己的环境来模拟这种情况。
- en: Challenges of data preparation with streaming data
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 流式数据数据准备的挑战
- en: 'Before deep-diving into specific algorithms and solutions, let''s first have
    a general discussion of why data preparation may be different when working with
    data that arrives in a streaming fashion. Multiple reasons can be identified,
    such as the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨特定算法和解决方案之前，让我们首先讨论一下为什么在处理以流式方式到达的数据时，数据准备可能有所不同。可以识别出多个原因，例如以下内容：
- en: The first, obvious issue is data drift. As discussed in much detail in the previous
    chapter, trends and descriptive statistics of your data can slowly change over
    time due to data drift. If your feature engineering or data preparation processes
    are too dependent on your data following certain distributions, you may run into
    problems when data drift occurs. As many solutions for this have been proposed
    in the previous chapter, this topic will be left out of consideration in the current
    chapter.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个，明显的问题是数据漂移。正如在前一章中详细讨论的那样，由于数据漂移，你的数据趋势和描述性统计可能会随着时间的推移而缓慢变化。如果你的特征工程或数据准备过程过于依赖你的数据遵循某些分布，当数据漂移发生时，你可能会遇到问题。由于前一章已经提出了许多解决方案，因此这个主题将在当前章节中不予考虑。
- en: 'The second issue is that population parameters are unknown. When observing
    data in a streaming fashion, it is possible, and even likely, that your estimates
    of the population parameters are slowly going to improve over time. As seen in
    [*Chapter 3*](B18335_03_ePub.xhtml#_idTextAnchor051), precision in your estimates
    of descriptive statistics will improve with the amount of data you have. When
    the descriptive statistic estimates are improving, the fact that they are changing
    over time does not make it easy to fix your formulas for data preparation, feature
    engineering, scaling, and the like:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个问题是总体参数未知。在以流式方式观察数据时，你的总体参数估计可能会随着时间的推移而缓慢提高，这是可能的，甚至很可能是。正如在 [*第3章*](B18335_03_ePub.xhtml#_idTextAnchor051)
    中所见，你估计描述性统计的精度会随着你拥有的数据量而提高。当描述性统计估计在提高时，它们随时间变化的事实并不容易修复你的数据准备、特征工程、缩放等公式的公式：
- en: As the first example of this, consider the range. The range represents the minimum
    and maximum values of the data that you observe. This is used extensively in data
    scaling and also in other algorithms. Now, imagine that the range (minimum and
    maximum values) in a batch can be different from the global range (global minimum
    and global maximum) of the data. After all, when new data arrives, you may observe
    a value that is higher or lower than anything observed in your historical data,
    just by the process of random sampling. Observing an observation that is higher
    than your maximum may cause an issue in scaling if you do not treat it right.
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为这个问题的第一个例子，考虑范围。范围代表你观察到的数据的最大值和最小值。这在数据缩放和其他算法中得到了广泛的应用。现在，想象一下，一批数据中的范围（最小值和最大值）可能与数据的全局范围（全局最小值和全局最大值）不同。毕竟，当新数据到达时，你可能会观察到由于随机抽样的过程而高于或低于你历史数据中观察到的任何值的值。如果你没有正确处理，观察到高于你最大值的值可能会导致缩放时出现问题。
- en: Another example of this is when scaling with a normal distribution. The standard
    deviation and average in your batch may be different from the population standard
    deviation and population average. This may cause your scaler to behave differently
    after some time, which is a sort of data drift that is induced by your own scaling
    algorithm. Clearly, this must be avoided.
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个例子是在使用正态分布进行缩放时。你的批次中的标准差和平均值可能与总体标准差和总体平均值不同。这可能导致你的缩放器在一段时间后表现不同，这是一种由你自己的缩放算法引起的数据漂移。显然，这必须避免。
- en: Many other cases of such problems exist, including observing new categories
    in a categorical value, which will lead to problems with your one-hot encoder
    or your models that use categorical variables. You can also imagine that occurring
    new types of values in your data such as NAs and InFs need to be managed well,
    rather than having them cause bugs. This is true in general, but when working
    with streaming, this tends to cause even more trouble than with regular data.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在许多此类问题的其他情况，包括在分类值中观察到新的类别，这可能导致你的one-hot编码器或使用分类变量的模型出现问题。你还可以想象，在数据中出现新的值类型，如NAs和InFs，需要得到妥善管理，而不是让它们引起错误。这在一般情况下是正确的，但在处理流数据时，这往往比处理常规数据造成更多麻烦。
- en: In the next section, we will learn what scaling is and how to work with it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将学习缩放是什么以及如何与之合作。
- en: Scaling data for streaming
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 为流数据缩放
- en: In the first part of this section, let's start by looking at some solutions
    for streaming scaling data. Before going into the solutions, let's do a quick
    recap of what scaling is and how it works.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的第一个部分，让我们先看看一些用于流式缩放数据的解决方案。在进入解决方案之前，让我们快速回顾一下缩放是什么以及它是如何工作的。
- en: Introducing scaling
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍缩放
- en: Numerical variables can be of any scale, meaning they can have very high average
    values or low average values, for example. Some machine learning algorithms are
    not at all impacted by the scale of a variable, whereas other machine learning
    algorithms can be strongly impacted.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 数值变量可以是任何尺度，这意味着它们可以具有非常高的平均值或很低的平均值，例如。一些机器学习算法根本不受变量尺度的任何影响，而其他机器学习算法则可能受到强烈影响。
- en: Scaling is the practice of taking a numerical variable and reducing its range,
    and potentially its standard deviation, to a pre-specified range. This will allow
    all machine learning algorithms to learn from the data without problems.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放是将数值变量缩小范围，并可能缩小其标准差，到一个预指定的范围的做法。这将允许所有机器学习算法在没有问题的前提下从数据中学习。
- en: Scaling with MinMaxScaler
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MinMaxScaler进行缩放
- en: To achieve this goal, a commonly used method is the Min-Max scaler. The Min-Max
    scaler will take an input variable in any range and reduce all of the values to
    fall in between the range (`0` to `1`), meaning that the minimum value of the
    scaled variable will be `0` and the maximum of the scaled variable will be `1`.
    Sometimes, an alternative is used in which the minimum is not `0`, but `-1`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一目标，常用的方法是最小-最大缩放器。最小-最大缩放器将输入变量接受任何范围，并将所有值减少到介于该范围（`0`到`1`）之间，这意味着缩放变量的最小值将是`0`，缩放变量的最大值将是`1`。有时，会使用一个替代方案，其中最小值不是`0`，而是`-1`。
- en: 'The mathematical formula for Min-Max scaling is the following:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 最小-最大缩放的数学公式如下：
- en: '![](img/Formula_10_001.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_001.jpg)'
- en: Scaling with StandardScaler
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用StandardScaler进行缩放
- en: Another very common approach to scaling is standardizing. Standardizing is a
    method strongly based on statistics, which allows you to take any variable and
    take it back to a standard normal distribution. The standard normal distribution
    has an average of `0` and a standard deviation of `1`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常常见的缩放方法是标准化。标准化是一种基于统计学的强有力方法，它允许你将任何变量转换回标准正态分布。标准正态分布的平均值为`0`，标准差为`1`。
- en: 'The mathematical formula for the StandardScaler is the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 标准缩放器的数学公式如下：
- en: '![](img/Formula_10_002.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_002.jpg)'
- en: The values of the scaled variable will not be in any specific range; the new
    value of the scaled variable represents the number of standard deviations that
    the original value is away from the original mean. A very extreme value (imagine
    four or five standard deviations away from the mean) would have a value of four
    or five, which by the way can be both positive and negative.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放变量的值不会在任何特定范围内；缩放变量的新值表示原始值与原始平均值之间的标准差数。一个非常极端的值（想象一下平均值的四到五个标准差之外）将具有四到五个的值，顺便说一下，这个值可以是正的也可以是负的。
- en: Choosing your scaling method
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 选择你的缩放方法
- en: The choice of scaling algorithm depends on the use case, and it is generally
    a good idea to do tuning of your machine learning pipeline in which different
    scaling methods are used with different algorithms. After all, the choice of scaling
    method has an impact on the performance of the training of the method.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放算法的选择取决于用例，通常在机器学习管道中使用不同的缩放方法与不同的算法进行调优是一个好主意。毕竟，缩放方法的选择会影响方法的训练性能。
- en: The Min-Max scaler is known to have difficulty with outliers. After all, a very
    extreme outlier would be set to the maximum value, that is, to `1`. Then, this
    may cause the other values to be reduced to a much smaller range.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Min-Max缩放器众所周知在处理异常值方面有困难。毕竟，一个非常极端的异常值会被设置为最大值，即`1`。然后，这可能会导致其他值被减少到一个更小的范围。
- en: The StandardScaler deals with this in a better way, as the outliers would still
    be outliers and simply take on high values in the scaled variable. This can be
    a disadvantage at the same time, mainly when you are using machine learning algorithms
    that need the values to be between `0` and `1`.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: StandardScaler处理这个问题的方式更好，因为异常值仍然是异常值，只是在缩放变量中取高值。这同时可能是一个缺点，主要是在你使用需要值在`0`到`1`之间的机器学习算法时。
- en: Adapting scaling to a streaming context
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 适应流式上下文缩放
- en: Let's now have a look at how we can adapt each of those approaches to the case
    of streaming data. We'll start with the Min-Max scaler.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看我们如何将每种方法适应流式数据的情况。我们将从Min-Max缩放器开始。
- en: Adapting the MinMaxScaler to streaming
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将MinMaxScaler适应流式
- en: The MinMaxScaler works perfectly on a fixed dataset. It guarantees that the
    values of the scaled data will be between `0` and `1`, just as required by some
    machine learning algorithms. However, in the case of streaming data, this is much
    less easy to manage.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: MinMaxScaler在固定数据集上工作得很好。它保证了缩放数据的值将在`0`到`1`之间，正如某些机器学习算法所要求的。然而，在流式数据的情况下，这要难得多管理。
- en: 'When new data arrives one by one (in a stream), it is impossible to decide
    on the minimum or maximum value. After all, you cannot expect one value to be
    both minimum and maximum. The same problem occurs when batching: there is no guarantee
    that the batch maximum is higher than the global maximum, and the same for the
    minimum.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 当新数据一个接一个地到达（在流中），不可能决定最小值或最大值。毕竟，你不能期望一个值既是最小值又是最大值。当批处理时，也没有保证批最大值高于全局最大值，对于最小值也是如此。
- en: You could use the training data to decide on the minimum and the maximum, but
    then the problem is that your new data could be above the training maximum or
    below the training minimum. This would result in the scaled values being outside
    of the range (`0` to `1`).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用训练数据来决定最小值和最大值，但问题是你新的数据可能会高于训练最大值或低于训练最小值。这将导致缩放值超出范围（`0`到`1`）。
- en: A solution for this is to use a running minimum and a running maximum. This
    means that you continue updating the MinMaxScaler so that every time a lower minimum
    is observed, you update the minimum in the MinMaxScaler formula, and every time
    a higher maximum is observed, you update the maximum.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的方法是用运行中的最小值和最大值。这意味着你继续更新MinMaxScaler，以便每次观察到更低的最低值时，你就在MinMaxScaler公式中更新最低值，每次观察到更高的最高值时，你就在公式中更新最高值。
- en: The advantage of this method is that it guarantees that your scaled data will
    always be between `0` and `1`. A disadvantage is that the first values for training
    the MinMaxScaler will be scaled quite badly. This is easily solved by using some
    training data to initialize the MinMaxScaler. Outliers can also be a problem,
    as having one very extreme value will strongly affect the MinMaxScaler's formula,
    and scores will be very different after that. This could be solved by using an
    outlier detection method as described extensively in [*Chapter 5*](B18335_05_ePub.xhtml#_idTextAnchor097).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是它保证了你的缩放数据始终在`0`和`1`之间。一个缺点是训练MinMaxScaler的第一个值会被缩放得很糟糕。这可以通过使用一些训练数据来初始化MinMaxScaler来轻松解决。异常值也可能成为问题，因为一个极端的异常值会强烈影响MinMaxScaler的公式，并且在那之后分数会有很大的不同。这可以通过使用如[*第5章*](B18335_05_ePub.xhtml#_idTextAnchor097)中详细描述的异常值检测方法来解决。
- en: 'Let''s now move on to a Python implementation of an adaptive MinMaxScaler:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们继续到一个自适应MinMaxScaler的Python实现：
- en: 'For this, we will use the implementation of the MinMaxScaler in the Python
    library, `River`. We will use the following data for this example:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们将使用Python库`River`中MinMaxScaler的实现。我们将使用以下数据作为这个示例：
- en: Code Block 10-1
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-1
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The histogram of this data can be created using the following code:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可以使用以下代码创建这个数据的直方图：
- en: Code Block 10-2
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-2
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The resulting histogram looks like the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 结果直方图看起来如下：
- en: '![Figure 10.1 – Resulting histogram of Code Block 10-2'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – 代码块10-2的结果直方图'
- en: '](img/B18335_10_1.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_10_1.jpg)'
- en: Figure 10.1 – Resulting histogram of Code Block 10-2
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – 代码块10-2的结果直方图
- en: 'Now, to scale this data, let''s use the `MinMaxScaler` function from River.
    Looping through the data will simulate the data arriving in a streaming fashion,
    and the use of the `learn_one` method shows that the data is updated step by step:'
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了缩放这些数据，让我们使用来自 River 的 `MinMaxScaler` 函数。通过遍历数据可以模拟数据以流式方式到达，而使用 `learn_one`
    方法表明数据是逐步更新的：
- en: Code Block 10-3
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 10-3
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, it will be interesting to see the histogram of the scaled data. It can
    be created as follows:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将很有趣地看到缩放数据的直方图。它可以创建如下：
- en: Code Block 10-4
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 10-4
- en: '[PRE3]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The histogram is shown in the following:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图如下所示：
- en: '![Figure 10.2 – Resulting histogram of Code Block 10-4'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.2 – 代码块 10-4 的结果直方图'
- en: '](img/B18335_10_2.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_10_2.jpg)'
- en: Figure 10.2 – Resulting histogram of Code Block 10-4
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.2 – 代码块 10-4 的结果直方图
- en: This histogram clearly shows that we have been successful in scaling the data
    into the `0` to `1` range.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个直方图清楚地表明我们已经成功将数据缩放到 `0` 到 `1` 的范围。
- en: Now that you have seen the theory and implementation of the MinMaxScaler, let's
    now see the StandardScaler, a common alternative to this method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了 MinMaxScaler 的理论和实现，让我们现在看看 StandardScaler，这是该方法的一个常见替代方案。
- en: Adapting the Standard Scaler to streaming
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将标准缩放器适应流式处理
- en: The problem that may occur in standard scaling when observing more extreme data
    in the future, is not exactly the same problem as the one that is seen in Min-Max
    scaling. Where the Min-Max scaler uses the minimum and the maximum to compute
    the scaling method, the standard scaler uses the mean and standard deviation.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在未来观察更多极端数据时，标准缩放可能遇到的问题，与 Min-Max 缩放中看到的问题并不完全相同。Min-Max 缩放器使用最小值和最大值来计算缩放方法，而标准缩放器使用均值和标准差。
- en: The reason why this is very different is that the minimum and maximum are relatively
    likely to be surpassed at one point in time. This would result in the scaled values
    being higher than `1` or lower than `0`, which may pose real problems for your
    machine learning algorithms.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以非常不同，是因为最小值和最大值在某个时间点被超越的可能性相对较高。这将导致缩放值高于 `1` 或低于 `0`，这可能会对你的机器学习算法造成真正的问题。
- en: In the standard scaler, any extreme values occurring in the future will impact
    your estimate of the global mean and standard deviation, but they are much less
    likely to impact them very severely. After all, the mean and the standard deviation
    are much less sensitive to the observation of a small number of extreme values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准缩放器中，未来发生的任何极端值将影响你对全局均值和标准差的估计，但它们不太可能非常严重地影响它们。毕竟，均值和标准差对少数极端值的观察要敏感得多。
- en: Given this theoretical consideration, you may conclude that it isn't really
    necessary to update the standard scaler. However, it may be best to update it
    anyway, as this is a good way to keep your machine learning methods up to date.
    The added value of this will be less impacting than when using the Min-Max scaler,
    but it is a best practice to do it anyway.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个理论考虑的基础上，你可能会得出结论，实际上没有必要更新标准缩放器。然而，无论如何更新它可能更好，因为这是保持你的机器学习方法更新的好方法。这种增加的价值将比使用
    Min-Max 缩放器时的影响小，但无论如何，这是一个最佳实践。
- en: 'One solution that you can use is to use the AdaptiveStandardScaler in the `Riverml`
    package. It uses an exponentially-weighted running mean and variance to make sure
    that slight drifts of the normal distribution of your data are taken into account
    without having it weigh too strongly. Let''s see a Python example of how to use
    the AdaptiveStandardScaler:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用的一种解决方案是使用 `Riverml` 包中的 `AdaptiveStandardScaler`。它使用指数加权的移动平均和方差来确保在不过度强调的情况下考虑数据正态分布的轻微漂移。让我们看看如何使用
    AdaptiveStandardScaler 的 Python 示例：
- en: 'We will use the following data for this example:'
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用以下数据作为本例：
- en: Code Block 10-5
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 10-5
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This data follows a normal distribution, as you can see from the histogram.
    You can create a histogram as follows:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这组数据遵循正态分布，正如从直方图中所见。你可以按照以下步骤创建直方图：
- en: Code Block 10-6
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 10-6
- en: '[PRE5]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The resulting histogram is shown here:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果直方图如下所示：
- en: '![Figure 10.3 – Resulting histogram of Code Block 10-6'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.3 – 代码块 10-6 的结果直方图'
- en: '](img/B18335_10_3.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18335_10_3.jpg)'
- en: Figure 10.3 – Resulting histogram of Code Block 10-6
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 – 代码块 10-6 的结果直方图
- en: The data clearly follows a normal distribution, but it is not centered around
    `0` and it is not standardized to a standard deviation of `1`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 数据明显遵循正态分布，但它不是围绕 `0` 对齐的，也没有标准化到标准差为 `1`。
- en: 'Now, to scale this data, let''s use `StandardScaler` from River. Again, we
    will loop through the data to simulate streaming. Also, we again use the `learn_one`
    method to update the data step by step:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，为了扩展这些数据，让我们使用来自 River 的 `StandardScaler`。同样，我们将遍历数据以模拟流式传输。此外，我们再次使用 `learn_one`
    方法逐步更新数据：
- en: Code Block 10-7
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 10-7
- en: '[PRE6]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To verify that it has worked correctly, let''s redo the histogram using the
    following code:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了验证它是否正确工作，让我们使用以下代码重新绘制直方图：
- en: Code Block 10-8
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块 10-8
- en: '[PRE7]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The histogram is shown here:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了直方图：
- en: '![Figure 10.4 – Resulting histogram of Code Block 10-8'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.4 – 代码块 10-8 的结果直方图'
- en: '](img/B18335_10_4.jpg)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_10_4.jpg]'
- en: Figure 10.4 – Resulting histogram of Code Block 10-8
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 – 代码块 10-8 的结果直方图
- en: As you can see, the data is clearly centered around `0`, and the new, scaled
    value indicates the number of standard deviations that each data point is away
    from the mean.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，数据明显集中在 `0` 附近，新的缩放值表示每个数据点与平均值相差多少个标准差。
- en: In the next section, you will see how to adapt feature transformation in a streaming
    context.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将看到如何在流式环境中调整特征转换。
- en: Transforming features in a streaming context
  id: totrans-107
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在流式环境中转换特征
- en: Scaling data is a way of pre-processing data for machine learning, but many
    other statistical methods can be used for data preparation. In this second part
    of this chapter let's deep dive into the **principal component analysis** (**PCA**)
    method, a much-used method for preparing data at the beginning of any machine
    learning.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 数据缩放是机器学习数据预处理的一种方法，但许多其他统计方法也可以用于数据准备。在本章的第二部分，让我们深入了解**主成分分析**（**PCA**）方法，这是一种在机器学习开始时常用的数据准备方法。
- en: Introducing PCA
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍 PCA
- en: PCA is a machine learning method that can be used for multiple applications.
    When working with highly multivariate data, PCA can be used in an interpretative
    way, where you use it to make sense of and analyze multivariate datasets. This
    is a use of PCA in data analysis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 是一种可以用于多种应用的机器学习方法。当处理高度多变量数据时，PCA 可以以解释的方式使用，即您可以使用它来理解并分析多变量数据集。这是数据分析中
    PCA 的一个应用。
- en: Another way to use PCA is to prepare data for machine learning. From a high-level
    point of view, PCA could be seen as an alternative to scaling that reduces the
    number of variables of your data to make it easier for the model to fit. This
    is the use of PCA that is most relevant for the current chapter, and this is how
    it will be used in the example.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PCA 的另一种方式是为机器学习准备数据。从高层次的角度来看，PCA 可以被视为缩放的替代方案，它减少了数据变量的数量，使模型更容易拟合。这是与本章最相关的
    PCA 应用，这也是它在示例中将如何使用的方式。
- en: Mathematical definition of PCA
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PCA 的数学定义
- en: PCA works on multivariate data (or data with multiple columns). These columns
    generally have a business definition. The goal of PCA is to keep all information
    in the data but change the current variable definitions into variables with different
    interpretations.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 在多变量数据（或具有多个列的数据）上工作。这些列通常具有业务定义。PCA 的目标是保留数据中的所有信息，但将当前的变量定义转换为具有不同解释的变量。
- en: The new variables are called the **principal components**, and they are found
    in such a way that the first component contains the most possible variation, and
    the second component is the component that is orthogonal to the first one and
    explains the most variation possible while being orthogonal.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 新变量被称为**主成分**，它们是以这样的方式找到的：第一个成分包含尽可能多的变化，第二个成分是与第一个成分正交的成分，它解释了尽可能多的变化，同时保持正交。
- en: 'A schematical overview is shown here:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示了一个示意图：
- en: '![Figure 10.5 – Schematic overview of PCA'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 10.5 – PCA 的示意图概述'
- en: '](img/B18335_10_5.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18335_10_5.jpg]'
- en: Figure 10.5 – Schematic overview of PCA
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.5 – PCA 的示意图概述
- en: This example clearly shows how the original data on the left is transformed
    into principal components on the right. The first principal component has much
    more value in terms of information than any of the original variables. When working
    with hundreds of variables, you can imagine that you will need to retain only
    a limited number of components (based on different criteria and your use case),
    which may make it easier for your machine learning algorithm to learn from the
    data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子清楚地展示了原始数据左侧如何转换为右侧的主成分。第一个主成分在信息量方面比任何原始变量都更有价值。当处理数百个变量时，您可以想象您只需要保留有限数量的组件（基于不同的标准和您的用例），这可能使您的机器学习算法更容易从数据中学习。
- en: Regular PCA in Python
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Python 中的常规 PCA
- en: 'To have a good comparison between regular and incremental PCA, it is good to
    get everybody up to speed and do a quick example of a regular PCA first:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在常规和增量PCA之间进行良好的比较，最好让每个人都跟上进度，并先快速举一个常规PCA的例子：
- en: 'To do this, let''s create some simulated sample data to work on the example.
    We can make a small example dataset as follows:'
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要做到这一点，让我们创建一些模拟样本数据来处理这个例子。我们可以创建如下的小示例数据集：
- en: Code Block 10-9
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-9
- en: '[PRE8]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The data looks like this:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 数据看起来如下：
- en: '![Figure 10.6 – The resulting data'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.6 – 结果数据'
- en: '](img/B18335_10_6.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_10_6.jpg)'
- en: Figure 10.6 – The resulting data
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 – 结果数据
- en: 'You can make a plot of this data as follows:'
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以按照以下方式绘制此数据：
- en: Code Block 10-10
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-10
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The scatter plot shows a plot that is quite similar to the sketch in the earlier
    schematic drawing:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图显示了一个与早期示意图中草图相当相似的绘图：
- en: '![Figure 10.7 – The resulting image of Code Block 10-10'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.7 – 代码块10-10的结果图像'
- en: '](img/B18335_10_7.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_10_7.jpg)'
- en: Figure 10.7 – The resulting image of Code Block 10-10
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 – 代码块10-10的结果图像
- en: 'Let''s now use a regular PCA to identify the components and transform the data.
    The following block of code shows how to fit a PCA using `scikit-learn`:'
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们使用常规PCA来识别组件并转换数据。以下代码块展示了如何使用`scikit-learn`来拟合PCA：
- en: Code Block 10-11
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-11
- en: '[PRE10]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The transformed data looks as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 转换后的数据看起来如下：
- en: '![Figure 10.8 – The transformed data'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.8 – 转换后的数据'
- en: '](img/B18335_10_8.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_10_8.jpg)'
- en: Figure 10.8 – The transformed data
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 – 转换后的数据
- en: 'We can plot it just like we did with the previous data. This can be done using
    the following code:'
  id: totrans-143
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以像处理之前的数据一样绘制它。这可以通过以下代码完成：
- en: Code Block 10-12
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-12
- en: '[PRE11]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The plot looks as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 绘图看起来如下：
- en: '![Figure 10.9 – Plot of the transformed data'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.9 – 转换数据的绘图'
- en: '](img/B18335_10_9.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_10_9.jpg)'
- en: Figure 10.9 – Plot of the transformed data
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 – 转换数据的绘图
- en: You can clearly see that this looks a lot like the resulting plot in the earlier
    theoretical introduction. This PCA has successfully identified the first principal
    component to be the component that explains the largest part of the data. The
    second component explains the largest part of the remaining data (after the first
    component).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到，这非常类似于早期理论介绍中的结果绘图。这个PCA成功识别出第一个主成分是解释数据最大部分的组件。第二个组件解释了剩余数据（在第一个组件之后）的最大部分。
- en: Incremental PCA for streaming
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量PCA用于流处理
- en: PCA, in a streaming context, cannot be easily calculated on individual data
    points. After all, you can imagine that it is impossible to determine the standard
    deviation of a single data point, and therefore, there is no possible way to determine
    the best components.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在流处理上下文中，PCA不能轻易地计算在单个数据点上。毕竟，你可以想象，确定单个数据点的标准差是不可能的，因此，没有可能确定最佳组件。
- en: 'The proposed solution is to do this through batches and to compute your PCA
    in batches rather than all at once. The `scikit-learn` package has a functionality
    called `IncrementalPCA`, which allows you to fit PCA in batches. Let''s use the
    following code for fitting `IncrementalPCA` on the same data as before and compare
    the results. The code to fit and transform using `IncrementalPCA` is shown in
    the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的解决方案是通过批量处理来完成，并且批量计算PCA而不是一次性计算。`scikit-learn`包有一个名为`IncrementalPCA`的功能，允许你批量拟合PCA。让我们使用以下代码在之前相同的数据上拟合`IncrementalPCA`并比较结果。使用`IncrementalPCA`拟合和转换的代码如下所示：
- en: Code Block 10-13
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-13
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The transformed data using this second method looks as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第二种方法转换的数据看起来如下：
- en: '![Figure 10.10 – The transformed data using incremental PCA'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.10 – 使用增量PCA转换的数据'
- en: '](img/B18335_10_10.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_10_10.jpg)'
- en: Figure 10.10 – The transformed data using incremental PCA
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10 – 使用增量PCA转换的数据
- en: 'Now, let''s also make a plot of this data to see whether this batch-wise PCA
    was successful in fitting the real components, or whether it is far away from
    the original PCA:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们也绘制这个数据的图，看看这个批处理PCA是否成功拟合了真实组件，或者它是否远离原始PCA：
- en: Code Block 10-14
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 代码块10-14
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '[PRE18]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[PRE20]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The resulting scatter plot is shown in the following:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 结果散点图如下所示：
- en: '![Figure 10.11 – The scatter plot of the transformed data using incremental
    PCA'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.11 – 使用增量PCA转换数据的散点图'
- en: '](img/B18335_10_11.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18335_10_11.jpg)'
- en: Figure 10.11 – The scatter plot of the transformed data using incremental PCA
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11 – 使用增量PCA转换数据的散点图
- en: This scatter plot shows that the PCA has been correctly fitted. Do not be confused
    by the fact that the incremental PCA has inversed the first component (the image
    is mirrored left to right compared to the preceding one). This is not wrong but
    just mirrored. This incremental PCA has captured the two components very well.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这个散点图显示PCA已经正确拟合。不要被增量PCA反转了第一个组件（与前面的图像相比，图像左右镜像）的事实所迷惑。这并不是错误，只是镜像。这个增量PCA很好地捕捉了两个组件。
- en: Summary
  id: totrans-175
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you have seen some common methods for data preparation being
    adapted to streaming and online data. For streaming data, it is important to have
    easily refitting or re-estimating models.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你已经看到了一些常见的数据准备方法被应用于流式和在线数据。对于流式数据，重要的是要能够轻松地重新拟合或重新估计模型。
- en: In the first part of the chapter, you have seen two methods for scaling. The
    MinMaxScaler scales the data to the `0` to `1` range and, therefore, needs to
    make sure that none of the new data points get outside of this range. The StandardScaler
    uses a statistical normalization process using the mean and standard deviation.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的第一部分，你已经看到了两种缩放方法。MinMaxScaler将数据缩放到`0`到`1`的范围，因此需要确保新的数据点不会超出这个范围。StandardScaler使用基于均值和标准差的统计归一化过程。
- en: The second part of the chapter demonstrated a regular PCA and a new, incremental
    version called `IncrementalPCA`. This incremental method allows you to fit PCA
    in batches, which can help you when fitting PCA on streaming data.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第二部分演示了常规PCA和一个新的增量版本，称为`IncrementalPCA`。这种增量方法允许你分批拟合PCA，这在处理流式数据拟合PCA时可能很有帮助。
- en: 'With scaling and feature transformation in this chapter, and drift detection
    in the previous chapter, you have already seen a good part of the auxiliary tasks
    of machine learning on streaming. In the coming chapter, you will see the third
    and last secondary topic to machine learning and streaming, which is catastrophic
    forgetting: an impactful problem that can occur in online machine learning, causing
    the model to forget important learned trends. The chapter will explain how to
    detect and avoid it.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，通过缩放和特征转换，以及在前一章中的漂移检测，你已经看到了流式机器学习的许多辅助任务。在接下来的章节中，你将看到机器学习和流式处理的第三个也是最后一个次要主题：灾难性遗忘：在线机器学习中可能发生的一个有影响的问题，会导致模型忘记重要的学习趋势。本章将解释如何检测和避免它。
- en: Further reading
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: '*MinMaxScaler in River*: [https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/](https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/)'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*River中的MinMaxScaler*：[https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/](https://riverml.xyz/latest/api/preprocessing/MinMaxScaler/)'
- en: '*StandardScaler in River*: [https://riverml.xyz/latest/api/preprocessing/StandardScaler/](https://riverml.xyz/latest/api/preprocessing/StandardScaler/)'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*River中的StandardScaler*：[https://riverml.xyz/latest/api/preprocessing/StandardScaler/](https://riverml.xyz/latest/api/preprocessing/StandardScaler/)'
- en: '*PCA in scikit-learn*: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*scikit-learn中的PCA*：[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)'
- en: '*Incremental PCA in scikit-learn*: [https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*scikit-learn中的增量PCA*：[https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.IncrementalPCA.html)'
