- en: '*Chapter 5*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*'
- en: Autoencoders
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to do the following:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，您将能够完成以下内容：
- en: Explain where autoencoders can be applied and their use cases
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释自编码器的应用领域及其使用案例
- en: Understand how artificial neural networks are implemented and used
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解人工神经网络的实现与应用
- en: Implement an artificial neural network using the Keras framework
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 框架实现一个人工神经网络
- en: Explain how autoencoders are used in dimensionality reduction and denoising
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释自编码器在降维和去噪中的应用
- en: Implement an autoencoder using the Keras framework
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Keras 框架实现自编码器
- en: Explain and implement an autoencoder model using convolutional neural networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解释并实现一个基于卷积神经网络的自编码器模型
- en: In this chapter, we will take a look at autoencoders and their applications.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将介绍自编码器及其应用。
- en: Introduction
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 引言
- en: This chapter continues our discussion of dimensionality reduction techniques
    as we turn our attention to autoencoders. Autoencoders are a particularly interesting
    area of focus as they provide a means of using supervised learning based on artificial
    neural networks, but in an unsupervised context. Being based on artificial neural
    networks, autoencoders are an extremely effective means of dimensionality reduction,
    but also provide additional benefits. With recent increases in the availability
    of data, processing power, and network connectivity, autoencoders are experiencing
    a resurgence in usage and study from their origins in the late 1980s. This is
    also consistent with the study of artificial neural networks, which was first
    described and implemented as a concept in the 1960s. Presently, you would only
    need to conduct a cursory internet search to discover the popularity and power
    of neural nets.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章继续讨论降维技术，我们将焦点转向自编码器。自编码器是一个特别有趣的研究领域，因为它们提供了一种基于人工神经网络的有监督学习方法，但又是在无监督的环境下进行的。基于人工神经网络的自编码器是一种极为有效的降维手段，并且提供了额外的好处。随着数据、处理能力和网络连接的可用性不断提升，自编码器自1980年代末期以来，正在经历一场复兴，重新被广泛使用和研究。这也与人工神经网络的研究相一致，后者最早在1960年代被提出和实现。如今，只需进行简单的互联网搜索，就能发现神经网络的流行性和强大能力。
- en: Autoencoders can be used for de-noising images and generating artificial data
    samples in combination with other methods, such as recurrent or **Long Short-Term
    Memory** (**LSTM**) architectures, to predict sequences of data. The flexibility
    and power that arises from the use of artificial neural networks also enables
    autoencoders to form very efficient representations of the data, which can then
    be used either directly as an extremely efficient search method, or as a feature
    vector for later processing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以用于图像去噪和生成人工数据样本，结合其他方法，如递归神经网络或**长短期记忆**（**LSTM**）架构，用于预测数据序列。人工神经网络的灵活性和强大功能使得自编码器能够形成数据的高效表示，之后这些表示可以直接用于极其高效的搜索方法，或作为特征向量进行后续处理。
- en: Consider the use of an autoencoder in an image de-noising application, where
    we are presented with the image on the left in [*Figure 5.1*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor109).
    We can see that the image is affected by the addition of some random noise. We
    can use a specially trained autoencoder to remove this noise, as represented by
    the image on the right in *Figure 5.1*. In learning how to remove this noise,
    the autoencoder has also learned to encode the important information that composes
    the image and how to reconstruct (or decode) this information into a clearer version
    of the original image.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在图像去噪应用中使用自编码器，我们展示的是左侧的图像（见[*图 5.1*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor109)）。可以看到，图像受到一些随机噪声的影响。我们可以使用经过特殊训练的自编码器去除这些噪声，右侧的图像就是去噪后的结果（见*图
    5.1*）。在学习如何去除噪声的过程中，自编码器也学习到了如何编码构成图像的重要信息，并将这些信息解码（或重构）为原始图像的更清晰版本。
- en: '![Figure 5.1: Autoencoder de-noising](img/C12626_05_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1：自编码器去噪](img/C12626_05_01.jpg)'
- en: 'Figure 5.1: Autoencoder de-noising'
  id: totrans-16
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.1：自编码器去噪
- en: Note
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This image is modified from [http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/](http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/)
    under CC0.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 此图像修改自 [http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/](http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/)
    并遵循 CC0 许可协议。
- en: This example demonstrates one aspect of autoencoders that makes them useful
    for unsupervised learning (the encoding stage), and one that is useful in generating
    new images (decoding). Throughout this chapter, we will delve further into these
    two useful stages of autoencoders and apply the output of the autoencoder to clustering
    the CIFAR-10 dataset.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本例演示了自编码器在无监督学习中有用的一个方面（编码阶段），以及在生成新图像时有用的另一个方面（解码阶段）。在本章中，我们将进一步探讨自编码器的这两个有用阶段，并将自编码器的输出应用于CIFAR-10数据集的聚类。
- en: 'Here is a representation of an encoder and decoder:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是编码器和解码器的表示：
- en: '![Figure 5.2: Encoder/decoder representation](img/C12626_05_02.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.2：编码器/解码器表示](img/C12626_05_02.jpg)'
- en: 'Figure 5.2: Encoder/decoder representation'
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.2：编码器/解码器表示
- en: Fundamentals of Artificial Neural Networks
  id: totrans-23
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 人工神经网络基础
- en: Given that autoencoders are based on artificial neural networks, an understanding
    of how neural networks is also critical for understanding autoencoders. This section
    of the chapter will briefly review the fundamentals of artificial neural networks.
    It is important to note that there are many aspects of neural nets that are outside
    of the scope of this book. The topic of neural networks could easily, and has,
    filled many books on its own, and this section is not to be considered an exhaustive
    discussion of the topic.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于自编码器是基于人工神经网络的，因此理解神经网络的原理对于理解自编码器至关重要。本章的这一部分将简要回顾人工神经网络的基础知识。需要注意的是，神经网络的许多方面超出了本书的范围。神经网络的主题本身就能填满许多本书，这一部分并不是对该主题的详尽讨论。
- en: As described earlier, artificial neural networks are primarily used in supervised
    learning problems, where we have a set of input information, say a series of images,
    and we are training an algorithm to map the information to a desired output, such
    as a class or category. Consider the CIFAR-10 dataset ([*Figure 5.3*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor111))
    as an example, which contains images of 10 different categories (airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per category.
    When neural nets are used in a supervised learning context, the images are fed
    to the network with a representation of the corresponding category labels being
    the desired output of the network.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，人工神经网络主要用于监督学习问题，其中我们有一组输入信息，例如一系列图像，我们正在训练一个算法将这些信息映射到所需的输出，例如类别或标签。以CIFAR-10数据集（[*图
    5.3*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor111)）为例，它包含了10个不同类别的图像（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车），每个类别有6,000张图像。当神经网络用于监督学习时，图像被输入到网络中，而对应的类别标签则是网络的期望输出。
- en: The network is then trained to maximize its ability to infer or predict the
    correct label for a given image.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，网络经过训练，以最大化推断或预测给定图像正确标签的能力。
- en: '![Figure 5.3: CIFAR-10 dataset](img/C12626_05_03.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3：CIFAR-10 数据集](img/C12626_05_03.jpg)'
- en: 'Figure 5.3: CIFAR-10 dataset'
  id: totrans-28
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.3：CIFAR-10 数据集
- en: Note
  id: totrans-29
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: This image is taken from [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    from Learning Multiple Layers of Features from Tiny Images, Alex Krizhevsky, 2009.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 此图来自[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)，出自《从微小图像中学习多层特征》，Alex
    Krizhevsky，2009年。
- en: The Neuron
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元
- en: 'The artificial neural network derives its name from the biological neural networks
    commonly found in the brain. While the accuracy of the analogy can certainly be
    questioned, it is a useful metaphor to break down the concept of artificial neural
    networks and facilitate understanding. As with their biological counterparts,
    the neuron is the building block on which all neural networks are constructed,
    connecting a number of neurons in different configurations to form more powerful
    structures. Each neuron ([*Figure 5.4*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor113))
    is composed of four individual parts: an input value, a tunable weight (theta),
    an activation function that operates on the input value, and the resulting output
    value:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络得名于生物神经网络，这些神经网络通常存在于大脑中。虽然这一类比的准确性是值得质疑的，但它是帮助理解人工神经网络概念的有用隐喻。与生物神经元类似，神经元是构建所有神经网络的基本单元，通过不同的配置将多个神经元连接起来，从而形成更强大的结构。每个神经元（[*图
    5.4*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor113)）由四个部分组成：输入值、可调权重（θ）、作用于输入值的激活函数以及最终的输出值：
- en: '![Figure 5.4: Anatomy of a neuron](img/C12626_05_04.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4：神经元的解剖结构](img/C12626_05_04.jpg)'
- en: 'Figure 5.4: Anatomy of a neuron'
  id: totrans-34
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.4：神经元的解剖结构
- en: The activation function is specifically chosen depending upon the objective
    of the neural network being designed, and there are a number of common functions,
    including `tanh`, `sigmoid`, `linear`, `sigmoid`, and `ReLU` (rectified linear
    unit). Throughout this chapter, we will use both the `sigmoid` and `ReLU` activation
    functions, so let's look at them in a little more detail.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择是根据神经网络的目标来特定选择的，有许多常用的函数，包括`tanh`、`sigmoid`、`linear`、`sigmoid`和`ReLU`（修正线性单元）。在本章中，我们将同时使用`sigmoid`和`ReLU`激活函数，接下来我们将更详细地讨论它们。
- en: Sigmoid Function
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'The sigmoid activation function is very commonly used as an output in the classification
    of neural networks due to its ability to shift the input values to approximate
    a binary output. The sigmoid function produces the following output:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Sigmoid激活函数因其能将输入值转化为接近二进制的输出，因此在神经网络分类任务中非常常见。Sigmoid函数的输出如下：
- en: '![](img/C12626_05_05.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_05_05.jpg)'
- en: 'Figure 5.5: Output of the sigmoid function'
  id: totrans-39
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.5：Sigmoid函数的输出
- en: We can see in [*Figure 5.5*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor115)
    that the output of the sigmoid function asymptotes (approaches but never reaches)
    1 as *x* increases and asymptotes 0 as *x* moves further away from 0 in the negative
    direction. This function is used in classification tasks as it provides close
    to a binary output and is not a member of class (0) or is a member of the class
    (1).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在[*图 5.5*](C12626_05_ePub_Final_SZ.xhtml#_idTextAnchor115)中看到，sigmoid函数的输出随着*x*的增大渐近于1，而当*x*在负方向远离0时，输出渐近于0。这个函数常用于分类任务，因为它提供接近二进制的输出，表示是否属于类（0）或类（1）。
- en: Rectified Linear Unit (ReLU)
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: The rectified linear unit is a very useful activation function that's commonly
    used at intermediary stages of neural networks. Simply put, the value 0 is assigned
    to values less than 0, and the value is returned for greater than 0.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元是一个非常有用的激活函数，通常在神经网络的中间阶段使用。简单来说，输入小于0时输出为0，大于0时输出为输入值本身。
- en: '![Figure 5.6: Output of ReLU](img/C12626_05_06.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6：ReLU的输出](img/C12626_05_06.jpg)'
- en: 'Figure 5.6: Output of ReLU'
  id: totrans-44
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.6：ReLU的输出
- en: 'Exercise 18: Modeling the Neurons of an Artificial Neural Network'
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 18：模拟人工神经网络的神经元
- en: 'In this exercise, we will practically introduce a programmatic representation
    of the neuron in NumPy using the sigmoid function. We will keep the inputs fixed
    and adjust the tunable weights to investigate the effect on the neuron. Interestingly,
    this model is also very close to the supervised learning method of logistic regression.
    Perform the following steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过使用sigmoid函数，实际介绍神经元在NumPy中的编程表示。我们将固定输入并调整可调权重，以研究其对神经元的影响。有趣的是，这个模型也非常接近于逻辑回归的监督学习方法。请执行以下步骤：
- en: 'Import the `numpy` and matplotlib packages:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`和matplotlib包：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Configure matplotlib to enable the use of Latex to render mathematical symbols
    in the images:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置matplotlib以启用使用Latex渲染图像中的数学符号：
- en: '[PRE1]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the `sigmoid` function as a Python function:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`sigmoid`函数定义为Python函数：
- en: '[PRE2]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here, we''re using the sigmoid function. You could also use the ReLU function.
    The ReLU activation function, while being powerful in artificial neural networks,
    is easy to define. It simply needs to return the input value if greater than 0;
    otherwise, it returns 0:'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是sigmoid函数。你也可以使用ReLU函数。尽管ReLU激活函数在人工神经网络中非常强大，但它的定义非常简单。它只需要在输入大于0时返回输入值，否则返回0：
- en: '`def relu(x):`'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`def relu(x):`'
- en: '`return np.max(0, x)`'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`return np.max(0, x)`'
- en: 'Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta= 1`:'
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经元的输入（`x`）和可调权重（`theta`）。在这个示例中，输入（`x`）将是100个在`-5`和`5`之间线性分布的数字。设置`theta
    = 1`：
- en: '[PRE3]'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'A section of the output is as follows:'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出的一部分如下：
- en: '![Figure 5.7: Printing the inputs](img/C12626_05_07.jpg)'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.7：打印输入](img/C12626_05_07.jpg)'
- en: 'Figure 5.7: Printing the inputs'
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.7：打印输入
- en: 'Compute the outputs (`y`) of the neuron:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神经元的输出（`y`）：
- en: '[PRE4]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Plot the output of the neuron versus the input:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制神经元的输出与输入的关系图：
- en: '[PRE5]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The output is as follows:'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.8: Plot of neurons versus inputs](img/C12626_05_08.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.8：神经元与输入的关系图](img/C12626_05_08.jpg)'
- en: 'Figure 5.8: Plot of neurons versus inputs'
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.8：神经元与输入的关系图
- en: 'Set the tunable parameter, `theta`, to `5`, and recompute and store the output
    of the neuron:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可调参数 `theta` 设置为 5，重新计算并存储神经元的输出：
- en: '[PRE6]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Change the tunable parameter, `theta`, to `0.2`, and recompute and store the
    output of the neuron:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可调参数 `theta` 更改为 `0.2`，然后重新计算并存储神经元的输出：
- en: '[PRE7]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, `theta = 0.2`) on one graph:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一图表上绘制神经元的三条不同输出曲线（`theta = 1`，`theta = 5`，`theta = 0.2`）：
- en: '[PRE8]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下所示：
- en: '![Figure 5.9: Output curves of neurons](img/C12626_05_09.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.9: 神经元的输出曲线](img/C12626_05_09.jpg)'
- en: 'Figure 5.9: Output curves of neurons'
  id: totrans-77
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.9: 神经元的输出曲线'
- en: In this exercise, we modeled the basic building block of an artificial neural
    network with a sigmoid activation function. We can see that using the sigmoid
    function increases the steepness of the gradient and means that only small values
    of x will push the output to either close to 1 or 0\. Similarly, reducing `theta`
    reduces the sensitivity of the neuron to non-zero values and results in much extreme
    input values being required to push the result of the output to either 0 or 1,
    tuning the output of the neuron.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们使用了一个具有 Sigmoid 激活函数的人工神经网络基本构建模块。我们可以看到，使用 Sigmoid 函数会增加梯度的陡峭度，这意味着只有较小的
    x 值才能将输出推向接近 1 或 0。同样，减小 `theta` 会降低神经元对非零值的敏感度，导致需要更极端的输入值才能将输出推向 0 或 1，从而调节神经元的输出。
- en: 'Activity 8: Modeling Neurons with a ReLU Activation Function'
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 8：使用 ReLU 激活函数建模神经元
- en: 'In this activity, we will investigate the ReLU activation function and the
    effect tunable weights have in modifying the output of ReLU units:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将研究 ReLU 激活函数以及可调权重对修改 ReLU 单元输出的影响：
- en: Import `numpy` and matplotlib.
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy` 和 matplotlib。
- en: Define the ReLU activation function as a Python function.
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ReLU 激活函数定义为一个 Python 函数。
- en: Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta = 1`.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经元的输入（`x`）和可调权重（`theta`）。在这个示例中，输入（`x`）将是100个在线性间隔内从`-5`到`5`的数字。设置`theta
    = 1`。
- en: Compute the output (`y`).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出（`y`）。
- en: Plot the output of the neuron versus the input.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制神经元的输出与输入的关系图。
- en: Now, set `theta = 5`, and recompute and store the output of the neuron.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将 `theta` 设置为 5，重新计算并存储神经元的输出。
- en: Now, set `theta = 0.2`, and recompute and store the output of the neuron.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，将 `theta` 设置为 `0.2`，然后重新计算并存储神经元的输出。
- en: Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, and `theta = 0.2`) on one graph.
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一图表上绘制神经元的三条不同输出曲线（`theta = 1`，`theta = 5`，和 `theta = 0.2`）。
- en: 'By the end of this activity, you will have developed a range of response curves
    for the ReLU activated neuron. You will also be able to describe the effect of
    changing the value of theta on the output of the neuron. The output will look
    as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动结束时，您将为 ReLU 激活神经元开发一系列响应曲线。您还将能够描述改变 `theta` 值对神经元输出的影响。输出将如下所示：
- en: '![Figure 5.10: Expected output curves](img/C12626_05_10.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.10: 预期的输出曲线](img/C12626_05_10.jpg)'
- en: 'Figure 5.10: Expected output curves'
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.10: 预期的输出曲线'
- en: Note
  id: totrans-92
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 333.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案可以在第 333 页找到。
- en: 'Neural Networks: Architecture Definition'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络：架构定义
- en: Individual neurons aren't particularly useful in isolation; they provide an
    activation function and a means of tuning the output, but a single neuron would
    have an limited learning ability. Neurons become much more powerful when many
    of them are combined and connected together in a network structure. By using a
    number of different neurons and combining the outputs of individual neurons, more
    complex relationships can be established and more powerful learning algorithms
    can be built. In this section, we will briefly discuss the structure of a neural
    network and implement a simple neural network using the Keras machine learning
    framework ([https://keras.io/](https://keras.io/)).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元在孤立状态下并不是特别有用；它提供一个激活函数和调节输出的手段，但单个神经元的学习能力有限。当多个神经元被组合并连接成网络结构时，它们的功能会更强大。通过使用多个不同的神经元并结合各个神经元的输出，可以建立更复杂的关系并构建更强大的学习算法。在本节中，我们将简要讨论神经网络的结构，并使用
    Keras 机器学习框架([https://keras.io/](https://keras.io/)) 实现一个简单的神经网络。
- en: '![Figure 5.11: Simplified representation of a neural network](img/C12626_05_11.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.11: 神经网络的简化表示](img/C12626_05_11.jpg)'
- en: 'Figure 5.11: Simplified representation of a neural network'
  id: totrans-97
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.11：简化表示的神经网络
- en: '*Figure 5.11* illustrates the structure of a two-layered, fully-connected neural
    network. One of the first observations we can make is that there is a lot of information
    contained within this structure, with a high degree of connectivity as represented
    by the arrows that point to and from each of the nodes. Working from the left-hand
    side of the image, we can see the input values to the neural network, as represented
    by the (*x*) values. In this example, we have *m* input values per sample, and
    only the first sample is being fed into the network, hence, values from ![A close
    up of a stool'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.11*展示了一个两层全连接神经网络的结构。我们可以做出的第一个观察是，这个结构包含了大量的信息，并且具有高度的连接性，这通过指向每个节点的箭头表示。从图像的左侧开始，我们可以看到神经网络的输入值，表示为(*x*)值。在这个例子中，每个样本有*m*个输入值，而且只有第一个样本被输入到网络中，因此，值来自于！[凳子的特写'
- en: Description automatically generated](img/C12626_05_Formula_01.png) to ![A close
    up of a sign
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_05_Formula_01.png) 到 ![标志的特写
- en: Description automatically generated](img/C12626_05_Formula_02.png). These values
    are then multiplied by the corresponding weights of the first layer of the neural
    network (![](img/C12626_05_Formula_03.png)) before being passed into the activation
    function of the corresponding neuron. This is known as a **feedforward** neural
    network. The notation used in *Figure 5.11* to identify the weights is ![A close
    up of a logo
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_05_Formula_02.png)。这些值随后与神经网络第一层的相应权重相乘（![](img/C12626_05_Formula_03.png)），然后传递到相应神经元的激活函数中。这被称为**前馈**神经网络。*图5.11*中用于标识权重的符号是！[标志的特写
- en: Description automatically generated](img/C12626_05_Formula_04.png), where *i*
    is the layer the weight belongs to, *j* is the input node number (starting with
    1 at the top), and *k* is the node in the subsequent layer that the weight feeds
    into to.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 描述自动生成](img/C12626_05_Formula_04.png)，其中*i*是权重所属的层，*j*是输入节点的编号（从顶部开始为1），*k*是权重传递到的后续层节点。
- en: Looking at the inter-connectivity between the outputs of layer 1 (also known
    as the **hidden layer**) and the inputs to the output layer, we can see that there
    is a large number of trainable parameters (weights) that can be used to map the
    input to the desired output. The network of *Figure 5.11* represents an *n* class
    neural network classifier, where the output for each of the *n* nodes represents
    the probability of the input belonging to the corresponding class.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 观察第一层（也称为**隐藏层**）输出与输出层输入之间的互联性，我们可以看到，有大量的可训练参数（权重）可以用于将输入映射到期望的输出。*图5.11*中的网络表示一个*n*类神经网络分类器，其中每个*n*节点的输出表示输入属于相应类别的概率。
- en: Each layer is able to use a different activation function as described by ![](img/C12626_05_Formula_05.png)
    and ![](img/C12626_05_Formula_06.png), thus allowing different activation functions
    to be mixed, in which the first layer could use ReLU, the second could use tanh,
    and the third could use sigmoid, for example. The final output is calculated by
    taking the sum of the product of the output of the previous layer with the corresponding
    weights.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都可以使用不同的激活函数，如![](img/C12626_05_Formula_05.png)和![](img/C12626_05_Formula_06.png)所示，从而允许不同的激活函数混合使用，例如，第一层可以使用ReLU，第二层可以使用tanh，第三层可以使用sigmoid。最终输出通过将前一层输出与相应的权重相乘并求和来计算。
- en: 'If we consider the output of the first node of layer 1, it can be calculated
    by multiplying the inputs by the corresponding weights, adding the result, and
    passing it through the activation function:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑第一层节点的输出，可以通过将输入值与相应的权重相乘，求和结果，并通过激活函数来计算：
- en: '![](img/C12626_05_12.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_05_12.jpg)'
- en: 'Figure 5.12: Calculating the output of the last node'
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.12：计算最后一个节点的输出
- en: As we increase the number of layers between the input and output of the network,
    we increase the depth of the network. An increase in the depth is also an increase
    in the number of trainable parameters, as well as the complexity of the relationships
    within the data, as described by the network. It is, typically, harder to train
    networks with increased depth because the types of features selected for the input
    become more critical. Additionally, as we add more neurons to each layer, we increase
    the height of the neural network. By adding more neurons, the ability of the network
    to describe the dataset increases as we add more trainable parameters. If too
    many neurons are added, the network can memorize the dataset but fails to generalize
    new samples. The trick in constructing neural networks is to find the balance
    between sufficient complexity to be able to describe the relationships within
    the data and not be so complicated as to memorize the training samples.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 随着输入和输出之间的层数增加，我们增加了网络的深度。深度的增加也意味着可训练参数的数量增多，同时数据中描述关系的复杂性增加。通常，随着深度的增加，训练网络会变得更加困难，因为选择用于输入的特征变得更加关键。此外，随着我们向每一层添加更多的神经元，我们也增加了神经网络的高度。通过增加更多的神经元，网络描述数据集的能力增强，同时可训练的参数也增多。如果添加了过多的神经元，网络可能会记住数据集中的样本，但无法对新样本进行泛化。构建神经网络的关键在于找到一个平衡点，使得模型既有足够的复杂性来描述数据中的关系，又不会复杂到只会记忆训练样本。
- en: 'Exercise 19: Defining a Keras Model'
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习19：定义Keras模型
- en: In this exercise, we will define a neural network architecture (similar to *Figure
    5.11*) using the Keras machine learning framework to classify images for the CIFAR-10
    dataset. As each input image is 32 x 32 pixels in size, the input vector will
    comprise 32*32 = 1,024 values. With 10 individual classes in CIFAR-10, the output
    of the neural network will be composed of 10 individual values, with each value
    representing the probability of the input data belonging to the corresponding
    class.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将使用Keras机器学习框架定义一个神经网络架构（类似于*图5.11*），用于对CIFAR-10数据集中的图像进行分类。由于每个输入图像的大小为32
    x 32像素，输入向量将包含32*32 = 1,024个值。CIFAR-10有10个不同的类别，因此神经网络的输出将由10个独立的值组成，每个值表示输入数据属于相应类别的概率。
- en: 'For this exercise, we will require the Keras machine learning framework. Keras
    is a high-level neural network API that is used on top of an existing library,
    such as TensorFlow or Theano. Keras makes it easy to switch between lower-level
    frameworks because the high-level interface it provides remains the same irrespective
    of the underlying library. In this book, we will be using TensorFlow as the underlying
    library. If you have yet to install Keras and TensorFlow, do so using `conda`:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个练习，我们将需要Keras机器学习框架。Keras是一个高层神经网络API，通常用于现有库之上，如TensorFlow或Theano。Keras使得在底层框架之间切换变得更加容易，因为它提供的高层接口在不同的底层库中保持一致。在本书中，我们将使用TensorFlow作为底层库。如果你还没有安装Keras和TensorFlow，请使用`conda`安装：
- en: '[PRE9]'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Alternatively, you can install it using `pip`:'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你也可以使用`pip`安装它：
- en: '[PRE10]'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We will require the `Sequential` and `Dense` classes from `keras.models` and
    `keras.layers`, respectively. Import these classes:'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要从`keras.models`和`keras.layers`中导入`Sequential`和`Dense`类。导入这些类：
- en: '[PRE11]'
  id: totrans-115
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As described earlier, the input layer will receive 1,024 values. The second
    layer (Layer 1) will have 500 units and, because the network is to classify one
    of 10 different classes, the output layer will have 10 units. In Keras, a model
    is defined by passing an ordered list of layers to the `Sequential` model class.
    This example uses the `Dense` layer class, which is a fully-connected neural network
    layer. The first layer will use a ReLU activation function, while the output will
    use the `softmax` function to determine the probability of each class. Define
    the model:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如前所述，输入层将接收1,024个值。第二层（层1）将包含500个单元，并且因为网络需要分类10个不同的类别，所以输出层将包含10个单元。在Keras中，模型是通过将有序的层列表传递给`Sequential`模型类来定义的。此示例使用了`Dense`层类，它是一个全连接神经网络层。第一层将使用ReLU激活函数，而输出层将使用`softmax`函数来确定每个类别的概率。定义模型：
- en: '[PRE12]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'With the model defined, we can use the `summary` method to confirm the structure
    and the number of trainable parameters (or weights) within the model:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在定义了模型之后，我们可以使用`summary`方法来确认模型的结构以及其中可训练的参数（或权重）的数量：
- en: '[PRE13]'
  id: totrans-119
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output is as follows:'
  id: totrans-120
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.13: Structure and count of trainable parameters in the model](img/C12626_05_13.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.13：模型中可训练参数的结构与数量](img/C12626_05_13.jpg)'
- en: 'Figure 5.13: Structure and count of trainable parameters in the model'
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.13：模型中可训练参数的结构与数量
- en: 'This table summarizes the structure of the neural network. We can see that
    there are the two layers that we specified, with 500 units in the first layer
    and 10 output units in the second layer. The `Param #` column tells us how many
    trainable weights are available in that specific layer. The table also tells us
    that there are 517,510 trainable weights in total within the network.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '该表总结了神经网络的结构。我们可以看到，模型中有我们指定的两层，第一层包含 500 个单元，第二层包含 10 个输出单元。`Param #` 列显示了每一层中可训练权重的数量。表格还告诉我们，网络中总共有
    517,510 个可训练权重。'
- en: In this exercise, we created a neural network model in Keras that contains a
    network of over 500,000 weights that can be used to classify the images of CIFAR-10\.
    In the next section, we will train the model.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们在 Keras 中创建了一个神经网络模型，该模型包含超过 500,000 个权重，可以用来对 CIFAR-10 的图像进行分类。在接下来的章节中，我们将训练该模型。
- en: 'Neural Networks: Training'
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经网络：训练
- en: With the neural network model defined, we can begin the training process; at
    this stage, we will be training the model in a supervised fashion to develop some
    familiarity with the Keras framework before moving on to training autoencoders.
    Supervised learning models are trained by providing the model with both the input
    information as well as the known output; the goal of training is to construct
    a network that takes the input information and returns the known output using
    only the parameters of the model.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好神经网络模型后，我们可以开始训练过程；在这个阶段，我们将以监督学习的方式训练模型，以便在继续训练自编码器之前，对 Keras 框架有一定的熟悉度。监督学习模型通过提供输入信息和已知输出信息来训练模型；训练的目标是构建一个网络，使其能够仅通过模型的参数，将输入信息映射到已知输出。
- en: In a supervised classification example such as CIFAR-10, the input information
    is an image and the known output is the class that the image belongs to. During
    training, for each sample prediction, the errors in the feedforward network predictions
    are calculated using a specified error function. Each of the weights within the
    model is then tuned in an attempt to reduce the error. This tuning process is
    known as **back-propagation** because the error is propagated backward through
    the network from the output to the start of the network.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 CIFAR-10 这样的监督分类示例中，输入信息是一张图像，而已知输出是该图像所属的类别。在训练过程中，对于每一个样本的预测，使用指定的误差函数计算前馈网络的预测误差。然后，模型中的每个权重都会进行调整，试图减少误差。这一调整过程被称为**反向传播**，因为误差是从输出开始通过网络向后传播，直到网络的起始位置。
- en: 'During back-propagation, each trainable weight is adjusted in proportion to
    its contribution to the overall error multiplied by a value known as the **learning
    rate**, which controls the rate of change in the trainable weights. Looking at
    *Figure 5.14*, we can see that increasing the value of the learning rate can increase
    the speed at which the error is reduced, but risks not converging on a minimum
    error as we step over the values. A learning rate that''s too small may lead to
    us running out of patience or simply not having sufficient time to find the global
    minimum. Thus, finding the correct learning rate is a trial and error process,
    though starting with a larger learning rate and reducing it can often be a productive
    method. The following figure represents the selection of the learning rate:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，每个可训练的权重都会根据它对整体误差的贡献进行调整，调整幅度与一个被称为**学习率**的值成正比，该值控制可训练权重变化的速率。看一下*图
    5.14*，我们可以看到，增加学习率的值可以加快误差减少的速度，但也有可能因为步长过大而无法收敛到最小误差。学习率过小可能会导致我们失去耐心，或者根本没有足够的时间找到全局最小值。因此，找到正确的学习率是一个试错过程，尽管从较大的学习率开始并逐步减少通常是一个有效的方法。以下图示展示了学习率的选择：
- en: '![Figure 5.14: Selecting the correct learning rate (one epoch is one learning
    step)](img/C12626_05_14.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.14：选择正确的学习率（一个 epoch 是一次学习步骤）](img/C12626_05_14.jpg)'
- en: 'Figure 5.14: Selecting the correct learning rate (one epoch is one learning
    step)'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.14：选择正确的学习率（一个 epoch 是一次学习步骤）
- en: Training is repeated until the error in the predictions stop reducing or the
    developer runs out of patience waiting for a result. In order to complete the
    training process, we first need to make some design decisions, the first being
    the most appropriate error function. There are a range of error functions available
    for use, from a simple mean squared difference to more complex options. Categorical
    cross entropy (which is used in the following exercise) is a very useful error
    function for classifying more than one class.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 训练会重复进行，直到预测中的误差不再减少，或者开发者在等待结果时失去耐心。为了完成训练过程，我们首先需要做出一些设计决策，第一个是选择最合适的误差函数。可用的误差函数有很多种，从简单的均方差到更复杂的选项。类别交叉熵（在接下来的练习中使用）是一个非常有用的误差函数，适用于分类多个类别。
- en: With the error function defined, we need to choose the method of updating the
    trainable parameters using the error function. One of the most memory-efficient
    and effective update methods is stochastic gradient descent (SGD); there are a
    number of variants of SGD, all of which involve adjusting each of the weights
    in accordance with their individual contribution to the calculated error. The
    final training design decision to be made is the performance metric by which the
    model is evaluated and the best architecture selected; in a classification problem,
    this may be the classification accuracy of the model or perhaps the model that
    produces the lowest error score in a regression problem. These comparisons are
    generally made using a method of cross-validation.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义了误差函数之后，我们需要选择一种使用误差函数更新可训练参数的方法。最节省内存且有效的更新方法之一是随机梯度下降法（SGD）；SGD有许多变种，所有变种都涉及根据每个权重对计算误差的贡献来调整权重。最后的训练设计决策是模型评估的性能指标，以及选择最佳架构；在分类问题中，这可能是模型的分类准确度，或者在回归问题中，可能是产生最低误差分数的模型。这些比较通常是通过交叉验证方法进行的。
- en: 'Exercise 20: Training a Keras Neural Network Model'
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习20：训练Keras神经网络模型
- en: 'Thankfully, we don''t need to worry about manually programming the components
    of the neural network, such as backpropagation, because the Keras framework manages
    this for us. In this exercise, we will use Keras to train a neural network to
    classify a small subset of the CIFAR-10 dataset using the model architecture defined
    in the previous exercise. As with all machine learning problems, the first and
    the most important step is to understand as much as possible about the dataset,
    and this will be the initial focus of the exercise:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要担心手动编写神经网络的各个组件，如反向传播，因为Keras框架会为我们管理这些。在本次练习中，我们将使用Keras训练一个神经网络，使用前面练习中定义的模型架构对CIFAR-10数据集的一个小子集进行分类。与所有机器学习问题一样，第一步也是最重要的一步是尽可能多地了解数据集，这将是本次练习的初步重点：
- en: Note
  id: totrans-135
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` and `batches.meta` files from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise20)下载`data_batch_1`和`batches.meta`文件。
- en: 'Import `pickle`, `numpy`, `matplotlib` and the `Sequential` class from `keras.models`,
    and import `Dense` from `keras.layers`:'
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`、`matplotlib`以及从`keras.models`导入`Sequential`类，并从`keras.layers`导入`Dense`：
- en: '[PRE14]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Load the sample of the CIFAR-10 dataset that is provided with the accompanying
    source code in the `data_batch_1` file:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载与随附源代码一起提供的CIFAR-10数据集样本，该样本位于`data_batch_1`文件中：
- en: '[PRE15]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The data is loaded as a dictionary. Display the keys of the dictionary:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据作为字典加载。显示字典的键：
- en: '[PRE16]'
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The output is as follows:'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE17]'
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Note that the keys are stored as binary strings as denoted by `b''`. We are
    interested in the contents of data and labels. Let''s look at labels first:'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 注意，键是作为二进制字符串存储的，表示为`b'`。我们关心的是数据和标签的内容。我们先来看标签：
- en: '[PRE18]'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The output is as follows:'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12626_05_15.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_05_15.jpg)'
- en: 'Figure 5.15: Displaying the labels'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.15：显示标签
- en: 'We can see that the labels are a list of values 0 – 9, indicating which class
    each sample belongs to. Now, look at the contents of the `data` key:'
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到标签是一个0到9的值列表，表示每个样本所属的类别。现在，来看一下`data`键的内容：
- en: '[PRE19]'
  id: totrans-151
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The output is as follows:'
  id: totrans-152
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.16: Content of the data key](img/C12626_05_16.jpg)'
  id: totrans-153
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.16：data键的内容](img/C12626_05_16.jpg)'
- en: 'Figure 5.16: Content of the data key'
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.16：数据键的内容
- en: The data key provides a NumPy array with all the image data stored within the
    array. What is the shape of the image data?
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据键提供了一个包含所有图像数据的NumPy数组。图像数据的形状是什么？
- en: '[PRE20]'
  id: totrans-156
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE21]'
  id: totrans-158
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'We can see that we have 1000 samples, but each sample is a single dimension
    of 3,072 samples. Aren''t the images supposed to be 32 x 32 pixels? Yes, they
    are, but because the images are color or RGB images, they contain three channels
    (red, green, and blue), which means the images are 32 x 32 x 3\. They are also
    flattened, providing 3,072 length vectors. So, we can reshape the array and then
    visualize a sample of images. According to the CIFAR-10 documentation, the first
    1,024 samples are red, the second 1,024 are green, and the third 1,024 are blue:'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到我们有1000个样本，但每个样本是一个长度为3,072的一维数组。难道图像不应该是32 x 32像素吗？是的，它们是，但由于图像是彩色的或RGB图像，它们包含三个通道（红、绿、蓝），这意味着图像的尺寸是32
    x 32 x 3。它们还被展开，提供3,072长度的向量。所以，我们可以重新调整数组形状，然后可视化一个样本图像。根据CIFAR-10文档，前1,024个样本是红色，第二个1,024个是绿色，第三个1,024个是蓝色：
- en: '[PRE22]'
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Display the first 12 images, along with their labels:'
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示前12张图片及其标签：
- en: '[PRE23]'
  id: totrans-162
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.17: The first 12 images](img/C12626_05_17.jpg)'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.17：前12张图片](img/C12626_05_17.jpg)'
- en: 'Figure 5.17: The first 12 images'
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.17：前12张图片
- en: 'What is the actual meaning of the labels? To find out, load the `batches.meta`
    file:'
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 标签的实际意义是什么？要了解这一点，加载 `batches.meta` 文件：
- en: '[PRE24]'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'The output is as follows:'
  id: totrans-168
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12626_05_18.jpg)'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_05_18.jpg)'
- en: 'Figure 5.18: Meaning of the labels'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.18：标签的含义
- en: 'Decode the binary strings to get the actual labels:'
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码二进制字符串以获取实际标签：
- en: '[PRE25]'
  id: totrans-172
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output is as follows:'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![](img/C12626_05_19.jpg)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![](img/C12626_05_19.jpg)'
- en: 'Figure 5.19: Printing the actual labels'
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.19：打印实际标签
- en: 'Print the labels for the first 12 images:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印前12张图片的标签：
- en: '[PRE26]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.20: Labels of the first 12 images](img/C12626_05_20.jpg)'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.20：前12张图片的标签](img/C12626_05_20.jpg)'
- en: 'Figure 5.20: Labels of the first 12 images'
  id: totrans-180
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.20：前12张图片的标签
- en: 'Now we need to prepare the data for training the model. The first step is to
    prepare the output. Currently, the output is a list of numbers 0 – 9, but we need
    each sample to be represented as a vector of 10 units as per the previous model.
    The encoded output will be a NumPy array with a shape of 10000 x 10:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要准备数据来训练模型。第一步是准备输出。目前，输出是一个0到9的数字列表，但我们需要每个样本都用一个包含10个单位的向量表示，正如前面模型所要求的那样。编码后的输出将是一个形状为10000
    x 10的NumPy数组：
- en: Note
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE27]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Display the one hot encoding values for the first 12 samples:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示前12个样本的独热编码值：
- en: '[PRE28]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The output is as follows:'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.21: One hot encoding values for first 12 samples](img/C12626_05_21.jpg)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.21：前12个样本的独热编码值](img/C12626_05_21.jpg)'
- en: 'Figure 5.21: One hot encoding values for first 12 samples'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.21：前12个样本的独热编码值
- en: 'The model has 1,024 inputs because it expects a 32 x 32 grayscale image. Take
    the average of the three channels for each image to convert it to RGB:'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型有1,024个输入，因为它期望输入的是32 x 32的灰度图像。将每张图像的三个通道的平均值取出，将其转换为RGB：
- en: '[PRE29]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Display the first 12 images again:'
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次显示前12张图片：
- en: '[PRE30]'
  id: totrans-192
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output is as follows:'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.22: Displaying the first 12 images again.](img/C12626_05_22.jpg)'
  id: totrans-194
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.22：再次显示前12张图片。](img/C12626_05_22.jpg)'
- en: 'Figure 5.22: Displaying the first 12 images again.'
  id: totrans-195
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.22：再次显示前12张图片。
- en: 'Finally, scale the images to be between 0 and 1, which is required for all
    inputs to a neural network. As the maximum value in an image is 255, we will simply
    divide by 255:'
  id: totrans-196
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将图片缩放到0和1之间，这是神经网络所有输入所需要的。由于图像中的最大值为255，我们将其直接除以255：
- en: '[PRE31]'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We also need the images to be in the shape 10,000 x 1,024:'
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要将图像的形状调整为10,000 x 1,024：
- en: '[PRE32]'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Redefine the model with the same architecture as *Exercise 19*, *Defining a
    Keras Model*:'
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新定义模型，使用与*练习19*，*定义Keras模型*相同的架构：
- en: '[PRE33]'
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now we can train the model in Keras. We first need to compile the method to
    specify the training parameters. We will be using categorical cross-entropy, with
    stochastic gradient descent and a performance metric of classification accuracy:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以在Keras中训练模型。我们首先需要编译方法，以指定训练参数。我们将使用分类交叉熵，随机梯度下降，并使用分类准确率作为性能指标：
- en: '[PRE34]'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Train the model using back-propagation for 100 epochs and the `fit` method
    of the model:'
  id: totrans-204
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播方法训练模型，训练100个周期，并使用模型的 `fit` 方法：
- en: '[PRE35]'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'The output is as follows:'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.23: Training the model](img/C12626_05_23.jpg)'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.23：训练模型](img/C12626_05_23.jpg)'
- en: 'Figure 5.23: Training the model'
  id: totrans-208
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.23：训练模型
- en: 'We achieved approximately 90% classification accuracy for the 1,000 samples
    using this network. Examine the predictions made for the first 12 samples again:'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用这个网络对 1,000 个样本进行了分类，并取得了大约 90% 的分类准确率。请再次检查对前 12 个样本的预测结果：
- en: '[PRE36]'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.24: Printing the predictions](img/C12626_05_24.jpg)'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.24：打印预测结果](img/C12626_05_24.jpg)'
- en: 'Figure 5.24: Printing the predictions'
  id: totrans-213
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.24：打印预测结果
- en: 'We can use the `argmax` method to determine the most likely class for each
    sample:'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用 `argmax` 方法来确定每个样本最可能的类别：
- en: '[PRE37]'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The output is as follows:'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE38]'
  id: totrans-217
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Compare with the labels:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与标签进行比较：
- en: '[PRE39]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'The output is as follows:'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE40]'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The network made one error in these samples, that is, it classified the second
    last samples as a 2 (bird) instead of a 4 (deer). Congratulations! You have just
    successfully trained a neural network model in Keras. Complete the next activity
    to further reinforce your skills in training neural networks.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 网络在这些样本中犯了一个错误，即它将倒数第二个样本分类为2（鸟）而不是4（鹿）。恭喜你！你刚刚成功训练了一个 Keras 神经网络模型。完成下一个活动，进一步巩固你在训练神经网络方面的技能。
- en: 'Activity 9: MNIST Neural Network'
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 9：MNIST 神经网络
- en: In this activity, you will train a neural network to identify images in the
    MNIST dataset and reinforce your skills in training neural networks. This activity
    forms the basis of many neural network architectures in different classification
    problems, particularly in computer vision. From object detection and identification
    to classification, this general structure is used in a variety of applications.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，你将训练一个神经网络来识别 MNIST 数据集中的图像，并进一步巩固你在训练神经网络方面的技能。这个活动为许多不同分类问题中的神经网络架构奠定了基础，尤其是在计算机视觉领域。从目标检测和识别到分类，这种通用结构被应用于多种不同的场景。
- en: 'These steps will help you complete the activity:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助你完成活动：
- en: Import `pickle`, `numpy`, `matplotlib`, and the `Sequential` and `Dense` classes
    from Keras.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pickle`、`numpy`、`matplotlib`，以及来自 Keras 的 `Sequential` 和 `Dense` 类。
- en: Load the `mnist.pkl` file that contains the first 10,000 images and the corresponding
    labels from the MNIST dataset that are available in the accompanying source code.
    The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits,
    0 through 9\. Extract the images and labels.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载包含前 10,000 张图像及其相应标签的 `mnist.pkl` 文件，这些数据来自附带源代码中的 MNIST 数据集。MNIST 数据集是一系列
    28 x 28 像素的手写数字灰度图像，数字范围从 0 到 9。提取图像和标签。
- en: Note
  id: totrans-228
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the `mnist.pkl` file at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09).
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity09)
    找到 `mnist.pkl` 文件。
- en: Plot the first 10 samples along with the corresponding labels.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制前 10 个样本及其对应标签。
- en: Encode the labels using one hot encoding.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用独热编码对标签进行编码。
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备图像以便输入神经网络。提示：这个过程有 **两个** 独立的步骤。
- en: Construct a neural network model in Keras that accepts the prepared images and
    has a hidden layer of 600 units with a ReLU activation function and an output
    of the same number of units as classes. The output layer uses a `softmax` activation
    function.
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 Keras 中构建一个神经网络模型，接受已准备好的图像，并具有 600 单元的隐藏层，激活函数为 ReLU，输出层的单元数与类别数相同。输出层使用
    `softmax` 激活函数。
- en: Compile the model using multiclass cross-entropy, stochastic gradient descent,
    and an accuracy performance metric.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多类交叉熵、随机梯度下降和准确性性能指标编译模型。
- en: Train the model. How many epochs are required to achieve at least 95% classification
    accuracy on the training data?
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。要达到训练数据至少95%的分类准确率，需要多少个训练周期（epoch）？
- en: By completing this activity, you have trained a simple neural network to identify
    handwritten digits 0 through 9\. You have also developed a general framework for
    building neural networks for classification problems. With this framework, you
    can extend upon and modify the network for a range of other tasks.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完成此活动，你已经训练了一个简单的神经网络来识别手写数字 0 到 9。你还开发了一个用于构建分类问题神经网络的通用框架。通过这个框架，你可以扩展和修改网络以适应其他任务。
- en: Note
  id: totrans-237
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 335.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解答可以在第335页找到。
- en: Autoencoders
  id: totrans-239
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'Now that we are comfortable developing supervised neural network models in
    Keras, we can return our attention to unsupervised learning and the main subject
    of this chapter—autoencoders. Autoencoders are a specifically designed neural
    network architecture that aims to compress the input information into lower dimensional
    space in an efficient yet descriptive manner. Autoencoder networks can be decomposed
    into two individual sub-networks or stages: an **encoding** stage and a **decoding**
    stage. The first, or encoding, stage takes the input information and compresses
    it through a subsequent layer that has fewer units than the size of the input
    sample. The latter stage, that is, the decoding stage, then expands the compressed
    form of the image and aims to return the compressed data to its original form.
    As such, the inputs and desired outputs of the network are the same; the network
    takes, say, an image in the CIFAR-10 dataset and tries to return the same image.
    This network architecture is shown in *Figure 5.25*; in this image, we can see
    that the encoding stage of the autoencoder reduces the number of neurons to represent
    the information, while the decoding stage takes the compressed format and returns
    it to its original state. The use of the decoding stage helps to ensure that the
    encoder has correctly represented the information, because the compressed representation
    is all that is provided to restore the image in its original state. We will now
    work through a simplified autoencoder model using the CIFAR-10 dataset:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经习惯在Keras中开发监督学习神经网络模型，我们可以将注意力转回到无监督学习及本章的主要主题——自编码器。自编码器是一种专门设计的神经网络架构，旨在以高效且具有描述性的方式将输入信息压缩到较低维度空间。自编码器网络可以分解为两个独立的子网络或阶段：**编码**阶段和**解码**阶段。首先，编码阶段将输入信息通过一个后续的层进行压缩，该层的单元数少于输入样本的大小。随后，解码阶段扩展压缩后的图像，并试图将压缩数据恢复为其原始形式。因此，网络的输入和期望输出是相同的；网络输入的是CIFAR-10数据集中的一张图像，并试图返回相同的图像。该网络架构如*图5.25*所示；在此图中，我们可以看到自编码器的编码阶段将表示信息的神经元数量减少，而解码阶段则将压缩格式的图像恢复到其原始状态。解码阶段的使用有助于确保编码器正确地表示了信息，因为恢复图像到原始状态所需的所有信息都来自于压缩后的表示。接下来，我们将使用CIFAR-10数据集来实现一个简化的自编码器模型：
- en: '![Figure 5.25: Simple autoencoder network architecture](img/C12626_05_25.jpg)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![图5.25：简单自编码器网络架构](img/C12626_05_25.jpg)'
- en: 'Figure 5.25: Simple autoencoder network architecture'
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.25：简单自编码器网络架构
- en: 'Exercise 21: Simple Autoencoder'
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习21：简单自编码器
- en: In this exercise, we will construct a simple autoencoder for the sample of the
    CIFAR-10 dataset, compressing the information stored within the images for later
    use.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将为CIFAR-10数据集的样本构建一个简单的自编码器，将图像中存储的信息压缩以供后续使用。
- en: Note
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise21)下载`data_batch_1`文件。
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及从`keras.models`中导入`Model`类，从`keras.layers`中导入`Input`和`Dense`：
- en: '[PRE41]'
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Load the data:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据：
- en: '[PRE42]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是无监督学习方法，我们只关注图像数据。按照前一个练习加载图像数据：
- en: '[PRE43]'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度图像，缩放至0到1之间，并将每个图像展平为一个长度为1,024的向量：
- en: '[PRE44]'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Define the autoencoder model. As we need access to the output of the encoder
    stage, we will need to define the model using a slightly different method to that
    previously used. Define an input layer of `1024` units:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义自编码器模型。由于我们需要访问编码器阶段的输出，我们将使用一种稍微不同于之前的方法来定义模型。定义一个包含`1024`个单元的输入层：
- en: '[PRE45]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Define a subsequent `Dense` layer of `256` units (a compression ratio of 1024/256
    = 4) and a ReLU activation function as the encoding stage. Note that we have assigned
    the layer to a variable and passed the previous layer to a call method for the
    class:'
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义后续的`Dense`层，包含`256`个单元（压缩比为1024/256 = 4），并使用ReLU激活函数作为编码阶段。请注意，我们已将该层分配给一个变量，并将前一层传递给类的调用方法：
- en: '[PRE46]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Define a subsequent decoder layer using the sigmoid function as an activation
    function and the same shape as the input layer. The sigmoid function has been
    selected because the input values to the network are only between 0 and 1:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个后续解码器层，使用sigmoid函数作为激活函数，并与输入层具有相同的形状。选择sigmoid函数是因为网络的输入值仅介于0和1之间：
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给`Model`类来构建模型：
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Compile the autoencoder using a binary cross-entropy loss function and adadelta
    gradient descent:'
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和adadelta梯度下降法编译自编码器：
- en: '[PRE49]'
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Note
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '`adadelta` is a more sophisticated version of stochastic gradient descent where
    the learning rate is adjusted on the basis of a window of recent gradient updates.
    Compared to the other methods of modifying the learning rate, this prevents the
    gradient of very old epochs from influencing the learning rate.'
  id: totrans-266
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`adadelta`是随机梯度下降法的一个更复杂版本，其中学习率根据最近一段时间内的梯度更新窗口进行调整。与其他学习率调整方法相比，这避免了非常旧的周期的梯度影响学习率。'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们拟合模型；同样，我们将图像作为训练数据并作为期望的输出。训练100个周期：
- en: '[PRE50]'
  id: totrans-268
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The output is as follows:'
  id: totrans-269
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.26: Training the model](img/C12626_05_26.jpg)'
  id: totrans-270
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.26：训练模型](img/C12626_05_26.jpg)'
- en: 'Figure 5.26: Training the model'
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.26：训练模型
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储编码阶段前五个样本的输出：
- en: '[PRE51]'
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Reshape the encoder output to 16 x 16 (16 x 16 = 256) pixels and multiply by
    255:'
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出重塑为16 x 16（16 x 16 = 256）像素，并乘以255：
- en: '[PRE52]'
  id: totrans-275
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储解码阶段前五个样本的输出：
- en: '[PRE53]'
  id: totrans-277
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Reshape the output of the decoder to 32 x 32 and multiply by 255:'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器的输出重塑为32 x 32并乘以255：
- en: '[PRE54]'
  id: totrans-279
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Reshape the original images:'
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重塑原始图像：
- en: '[PRE55]'
  id: totrans-281
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The output is as follows:'
  id: totrans-282
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![](img/C12626_05_27.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_05_27.jpg)'
- en: 'Figure 5.27: Output of simple autoencoder'
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.27：简单自编码器的输出
- en: In *Figure 5.27*, we can see three rows of images. The first row is the original
    grayscale image, the second row is the corresponding autoencoder output for the
    original image, and finally, the third row is the reconstruction of the original
    image from the encoded input. We can see that the decoded images in the third
    row contain information about the basic shape of the image; we can see the main
    body of the frog and the deer, as well as the outline of the trucks and cars in
    the sample. Given that we only trained the model for 100 samples, this exercise
    would also benefit from an increase in the number of training epochs to further
    improve the performance of both the encoder and decoder. Now that we have the
    output of the autoencoder stage trained, we can use it as the feature vector for
    other unsupervised algorithms, such as K-means or K nearest neighbors.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 5.27*中，我们可以看到三行图像。第一行是原始的灰度图像，第二行是对应于原始图像的自编码器输出，最后，第三行是从编码输入中重建的原始图像。我们可以看到第三行中的解码图像包含了图像的基本形状信息；我们可以看到青蛙和鹿的主体，以及卡车和汽车的轮廓。鉴于我们只训练了100个样本，这个练习也可以通过增加训练周期的数量来进一步提高编码器和解码器的性能。现在我们已经获得了训练好的自编码器阶段的输出，我们可以将其作为其他无监督算法的特征向量，如K-means或K近邻。
- en: 'Activity 10: Simple MNIST Autoencoder'
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动10：简单MNIST自编码器
- en: 'In this activity, you will create an autoencoder network for the MNIST dataset
    contained within the accompanying source code. An autoencoder network such as
    the one built in this activity can be an extremely useful in the pre-processing
    stage of unsupervised learning. The encoded information produced by the network
    can be used in clustering or segmentation analysis, such as image-based web searches:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 在此活动中，您将为包含在附带源代码中的MNIST数据集创建一个自编码器网络。像本活动中构建的自编码器网络，可以在无监督学习的预处理阶段非常有用。网络产生的编码信息可以用于聚类或分割分析，例如基于图像的网络搜索：
- en: Import `pickle`, `numpy`, and `matplotlib`, and the `Model`, `Input`, and `Dense`
    classes from Keras.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及来自Keras的`Model`、`Input`和`Dense`类。
- en: Load the images from the supplied sample of the MNIST dataset that is provided
    with the accompanying source code (`mnist.pkl`).
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随附源代码提供的MNIST数据集样本加载图像（`mnist.pkl`）。
- en: Note
  id: totrans-290
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `mnist.pklP-code` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10).
  id: totrans-291
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity10)下载`mnist.pklP-code`文件。
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备输入神经网络的图像。作为提示，这个过程分为**两个**独立的步骤。
- en: Construct a simple autoencoder network that reduces the image size to 10 x 10
    after the encoding stage.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个简单的自编码器网络，在编码阶段后将图像大小缩小到10 x 10。
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  id: totrans-294
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二进制交叉熵损失函数和`adadelta`梯度下降法编译自编码器。
- en: Fit the encoder model.
  id: totrans-295
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 拟合编码器模型。
- en: Calculate and store the output of the encoding stage for the first five samples.
  id: totrans-296
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出。
- en: Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255.
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出重塑为10 x 10（10 x 10 = 100）像素并乘以255。
- en: Calculate and store the output of the decoding stage for the first five samples.
  id: totrans-298
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的解码阶段输出。
- en: Reshape the output of the decoder to 28 x 28 and multiply by 255.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器的输出重塑为28 x 28并乘以255。
- en: Plot the original image, the encoder output, and the decoder.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、编码器输出和解码器输出。
- en: 'In completing this activity, you will have successfully trained an autoencoder
    network that extracts the critical information from the dataset, preparing it
    for later processing. The output will be similar to the following:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，你将成功训练一个自编码器网络，该网络能够提取数据集中关键信息，并为后续处理做好准备。输出将类似于以下内容：
- en: '![](img/C12626_05_28.jpg)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_05_28.jpg)'
- en: 'Figure 5.28: Expected plot of original image, the encoder output, and the decoder'
  id: totrans-303
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.28：原始图像、编码器输出和解码器的预期图示
- en: Note
  id: totrans-304
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 338.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 此活动的解决方案可以在第338页找到。
- en: 'Exercise 22: Multi-Layer Autoencoder'
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习22：多层自编码器
- en: 'In this exercise, we will construct a multi-layer autoencoder for the sample
    of the CIFAR-10 dataset, compressing the information stored within the images
    for later use:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将为CIFAR-10数据集的样本构建一个多层自编码器，将图像中存储的信息压缩以备后续使用：
- en: Note
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise22)下载`data_batch_1`文件。
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及来自`keras.models`的`Model`类，并导入来自`keras.layers`的`Input`和`Dense`类：
- en: '[PRE56]'
  id: totrans-311
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Load the data:'
  id: totrans-312
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据：
- en: '[PRE57]'
  id: totrans-313
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  id: totrans-314
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是无监督学习方法，我们只关心图像数据。按照之前的练习加载图像数据：
- en: '[PRE58]'
  id: totrans-315
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度，缩放到0到1之间，并将每个图像展平为一个长度为1,024的向量：
- en: '[PRE59]'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Define the multi-layer autoencoder model. We will use the same shape input
    as the simple autoencoder model:'
  id: totrans-318
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义多层自编码器模型。我们将使用与简单自编码器模型相同形状的输入：
- en: '[PRE60]'
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We will add another layer before the 256 autoencoder stage, this time with
    512 neurons:'
  id: totrans-320
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在256自编码器阶段之前添加另一个层，这次使用512个神经元：
- en: '[PRE61]'
  id: totrans-321
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Using the same size autoencoder as the previous exercise, but the input to
    the layer is the `hidden_encoding` layer this time:'
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与之前练习相同大小的自编码器，但这次输入层是`hidden_encoding`层：
- en: '[PRE62]'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Add a decoding hidden layer:'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加解码隐藏层：
- en: '[PRE63]'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Use the same output stage as in the previous exercise, this time connected
    to the hidden decoding stage:'
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与之前练习相同的输出阶段，这次连接到隐藏的解码阶段：
- en: '[PRE64]'
  id: totrans-327
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-328
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给 `Model` 类来构建模型：
- en: '[PRE65]'
  id: totrans-329
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-330
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和 `adadelta` 梯度下降法编译自编码器：
- en: '[PRE66]'
  id: totrans-331
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  id: totrans-332
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们拟合模型；再次，我们将图像作为训练数据和期望的输出。训练 100 个 epoch：
- en: '[PRE67]'
  id: totrans-333
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output is as follows:'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.29: Training the model](img/C12626_05_29.jpg)'
  id: totrans-335
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.29: 训练模型](img/C12626_05_29.jpg)'
- en: 'Figure 5.29: Training the model'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.29: 训练模型'
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出：
- en: '[PRE68]'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255:'
  id: totrans-339
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出调整为 10 x 10（10 x 10 = 100）像素，并乘以 255：
- en: '[PRE69]'
  id: totrans-340
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的解码阶段输出：
- en: '[PRE70]'
  id: totrans-342
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Reshape the output of the decoder to 28 x 28 and multiply by 255:'
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器的输出调整为 28 x 28，并乘以 255：
- en: '[PRE71]'
  id: totrans-344
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Plot the original image, the encoder output, and the decoder:'
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、编码器输出和解码器：
- en: '[PRE72]'
  id: totrans-346
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'The output is as follows:'
  id: totrans-347
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.30: Output of multi-layer autoencoder](img/C12626_05_30.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.30: 多层自编码器的输出](img/C12626_05_30.jpg)'
- en: 'Figure 5.30: Output of multi-layer autoencoder'
  id: totrans-349
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 5.30: 多层自编码器的输出'
- en: By looking at the error score produced by both the simple and multilayer autoencoders
    and by comparing *Figure 5.27* and *Figure 5.30*, we can see that there is little
    difference between the output of the two encoder structures. The middle row of
    both figures show that the features learned by the two models are, in fact, different.
    There are a number of options we can use to improve both of these models, such
    as training for more epochs, using a different number of units or neurons in the
    layers, or using varying numbers of layers. This exercise was constructed to demonstrate
    how to build and use an autoencoder, but optimization is often a process of systematic
    trial and error. We encourage you to adjust some of the parameters of the model
    and investigate the different results for yourself.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看简单和多层自编码器产生的误差得分，并比较 *图 5.27* 和 *图 5.30*，我们可以看到这两种编码器结构的输出几乎没有区别。两幅图的中间行显示，两个模型学习到的特征实际上是不同的。我们可以使用许多选项来改进这两个模型，例如，训练更多的
    epochs，使用不同数量的单元或神经元，或使用不同数量的层。这个练习的构建旨在展示如何构建和使用自编码器，但优化通常是一个系统性的试错过程。我们鼓励您调整模型的一些参数，并自己研究不同的结果。
- en: Convolutional Neural Networks
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In constructing all of our previous neural network models, you would have noticed
    that we removed all the color information when converting the image to grayscale,
    and then flattened each image into a single vector of length 1,024\. In doing
    so, we essentially threw out a lot of information that may be of use to us. The
    colors in the images may be specific to the class or objects in the image; additionally,
    we lost a lot of our spatial information about the image, for example, the position
    of the trailer in the truck image relative to the cab or the legs of the deer
    relative to the head. Convolutional neural networks do not suffer from this information
    loss. This is because rather than using a flat structure of trainable parameters,
    they store the weights in a grid or matrix, which means that each group of parameters
    can have many layers in their structure. By organizing the weights in a grid,
    we prevent the loss of spatial information because the weights are applied in
    a sliding fashion across the image. Also, by having many layers, we can retain
    the color channels associated with the image.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建我们之前的所有神经网络模型时，您可能会注意到，我们在将图像转换为灰度图像时移除了所有颜色信息，并将每张图像展平为一个长度为 1,024 的单一向量。通过这种方式，我们本质上丢失了可能对我们有用的许多信息。图像中的颜色可能是特定于图像中的类别或物体的；此外，我们还丢失了许多关于图像的空间信息，例如，卡车图像中拖车相对于驾驶室的位置，或鹿的腿部相对于头部的位置。卷积神经网络不会遭遇这种信息丢失的问题。这是因为，卷积神经网络不是使用平坦的可训练参数结构，而是将权重存储在网格或矩阵中，这意味着每组参数可以在其结构中具有多个层。通过将权重组织成网格，我们防止了空间信息的丢失，因为这些权重以滑动的方式应用于图像。此外，通过拥有多个层，我们可以保留与图像相关的颜色通道。
- en: 'In developing convolutional neural-network-based autoencoders, the MaxPooling2D
    and Upsampling2D layers are very important. The MaxPooling 2D layer downsamples
    or reduces the size of an input matrix in two dimensions by selecting the maximum
    value within a window of the input. Say we had a 2 x 2 matrix, where three cells
    have a value of 1 and one single cell has a value of 2:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发基于卷积神经网络的自编码器时，MaxPooling2D和Upsampling2D层非常重要。MaxPooling2D层通过在输入的窗口中选择最大值，来对输入矩阵在两个维度上进行降采样或减小尺寸。假设我们有一个2
    x 2的矩阵，其中三个单元的值为1，一个单元的值为2：
- en: '![Figure 5.31: Demonstration of sample matrix](img/C12626_05_31.jpg)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![图5.31：示例矩阵演示](img/C12626_05_31.jpg)'
- en: 'Figure 5.31: Demonstration of sample matrix'
  id: totrans-355
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.31：示例矩阵演示
- en: If provided to the MaxPooling2D layer, this matrix would return a single value
    of 2, thus reducing the size of the input in both directions by one half.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供给MaxPooling2D层，这个矩阵会返回一个单一的2值，从而在两个方向上都将输入的尺寸减半。
- en: The UpSampling2D layer has the opposite effect as that of the MaxPooling2D layer,
    increasing the size of the input rather than reducing it. The upsampling process
    repeats the rows and columns of the data, thus doubling the size of the input
    matrix.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: UpSampling2D层的作用与MaxPooling2D层相反，它增加了输入的尺寸，而不是减少它。上采样过程会重复数据的行和列，从而将输入矩阵的大小加倍。
- en: 'Exercise 23: Convolutional Autoencoder'
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习23：卷积自编码器
- en: 'In this exercise, we will develop a convolutional neural-network-based autoencoder
    and compare the performance to the previous fully-connected neural network autoencoder:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将开发一个基于卷积神经网络的自编码器，并将其性能与之前的全连接神经网络自编码器进行比较：
- en: Note
  id: totrans-360
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23).
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Exercise23)下载`data_batch_1`文件。
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`:'
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`、`matplotlib`，以及来自`keras.models`的`Model`类，和来自`keras.layers`的`Input`、`Conv2D`、`MaxPooling2D`、`UpSampling2D`：
- en: '[PRE73]'
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Load the data:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据：
- en: '[PRE74]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the previous exercise:'
  id: totrans-366
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个无监督学习方法，我们只对图像数据感兴趣。根据之前的练习加载图像数据：
- en: '[PRE75]'
  id: totrans-367
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'As we are using a convolutional network, we can use the images with only rescaling:'
  id: totrans-368
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们使用的是卷积网络，因此可以仅通过重新缩放图像来使用它们：
- en: '[PRE76]'
  id: totrans-369
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Define the convolutional autoencoder model. We will use the same shape input
    as an image:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义卷积自编码器模型。我们将使用与图像相同形状的输入：
- en: '[PRE77]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Add a convolutional stage with 32 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using the same padding, which means the output
    has the same length as the input image:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个卷积层，包含32个层或滤波器，一个3 x 3的权重矩阵，ReLU激活函数，并使用相同的填充方式，这意味着输出的尺寸与输入图像相同：
- en: Note
  id: totrans-373
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: '[PRE78]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Add a max pooling layer to the encoder with a 2 x 2 kernel. `MaxPooling` looks
    at all the values in an image, scanning through with a 2 x 2 matrix. The maximum
    value in each 2 x 2 area is returned, thus reducing the size of the encoded layer
    by a half:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向编码器添加一个最大池化层，使用2 x 2的卷积核。`MaxPooling`查看图像中的所有值，使用2 x 2的矩阵进行扫描。在每个2 x 2区域中，返回最大值，从而将编码层的尺寸减小一半：
- en: '[PRE79]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Add a decoding convolutional layer (this layer should be identical to the previous
    convolutional layer):'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个解码卷积层（此层应与之前的卷积层相同）：
- en: '[PRE80]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Now we need to return the image to its original size, for which we will upsample
    by the same size as `MaxPooling2D`:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要将图像恢复到原始尺寸，因此我们将进行与`MaxPooling2D`相同大小的上采样：
- en: '[PRE81]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Add the final convolutional stage using three layers for the RGB channels of
    the images:'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加最后的卷积层，为图像的RGB通道使用三个层：
- en: '[PRE82]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给`Model`类来构建模型：
- en: '[PRE83]'
  id: totrans-384
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'Display the structure of the model:'
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型的结构：
- en: '[PRE84]'
  id: totrans-386
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: Note that we have far fewer trainable parameters as compared to the previous
    autoencoder examples. This has been a specific design decision to ensure that
    the example runs on a wide variety of hardware. Convolutional networks typically
    require a lot more processing power and often special hardware such as Graphical
    Processing Units (GPUs).
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，与之前的自动编码器示例相比，我们的可训练参数要少得多。这是一个特定的设计决策，目的是确保该示例可以在各种硬件上运行。卷积网络通常需要更多的处理能力，并且经常需要特殊硬件，如图形处理单元（GPU）。
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和`adadelta`梯度下降法编译自动编码器：
- en: '[PRE85]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs, because convolutional networks take
    a lot longer to compute:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们训练模型；再次地，我们将图像作为训练数据，并将其作为期望的输出。训练20个epoch，因为卷积神经网络的计算需要更多时间：
- en: '[PRE86]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'The output is as follows:'
  id: totrans-392
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.32: Training the model](img/C12626_05_32.jpg)'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图 5.32：训练模型](img/C12626_05_32.jpg)'
- en: 'Figure 5.32: Training the model'
  id: totrans-394
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.32：训练模型
- en: Note that the error was already less than in the previous autoencoder exercise
    after the second epoch, suggesting a better encoding/decoding model. This reduced
    error can be mostly attributed to the fact that the convolutional neural network
    did not discard a lot of data, and the encoded images are 16 x 16 x 32, which
    is significantly larger than the previous 16 x 16 size. Additionally, we have
    not compressed the images per se as they now contain fewer pixels (16 x 16 x 32
    = 8,192), but with more depth (32 x 32 x 3,072) than before. This information
    has been rearranged to allow more effective encoding/decoding processes.
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 请注意，在第二个epoch之后，误差已经比之前的自动编码器练习要小，这表明编码/解码模型有所改进。这种误差的减少主要归因于卷积神经网络没有丢弃太多数据，并且编码后的图像是16
    x 16 x 32，这比之前的16 x 16尺寸要大得多。此外，我们没有对图像进行压缩，因为它们现在包含的像素较少（16 x 16 x 32 = 8,192），但深度（32
    x 32 x 3,072）比之前更多。这些信息已被重新排列，以便更有效地进行编码/解码过程。
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出：
- en: '[PRE87]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Each encoded image has a shape of 16 x 16 x 32 due to the number of filters
    selected for the convolutional stage. As such, we cannot visualize them without
    modification. We will reshape them to be 256 x 32 in size for visualization:'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个编码图像的形状为16 x 16 x 32，这是由于为卷积阶段选择的滤波器数量。因此，我们不能在不进行修改的情况下直接可视化它们。我们将它们重塑为256
    x 32大小以进行可视化：
- en: '[PRE88]'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Get the output of the decoder for the first five images:'
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取前五个图像的解码器输出：
- en: '[PRE89]'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Plot the original image, the mean encoder output, and the decoder:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、均值编码器输出和解码器：
- en: '[PRE90]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'The output is as follows:'
  id: totrans-404
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.33: The original image, the encoder output, and the decoder](img/C12626_05_33.jpg)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.33：原始图像、编码器输出和解码器](img/C12626_05_33.jpg)'
- en: 'Figure 5.33: The original image, the encoder output, and the decoder'
  id: totrans-406
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 5.33：原始图像、编码器输出和解码器
- en: 'Activity 11: MNIST Convolutional Autoencoder'
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 11：MNIST卷积自动编码器
- en: 'In this activity, we will reinforce our knowledge of convolutional autoencoders
    using the MNIST dataset. Convolutional autoencoders typically achieve significantly
    improved performance when working with image-based datasets of a reasonable size.
    This is particularly useful when using autoencoders to generate artificial image
    samples:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动中，我们将通过使用MNIST数据集来加深对卷积自动编码器的理解。当使用合理大小的基于图像的数据集时，卷积自动编码器通常能显著提高性能。这在使用自动编码器生成人工图像样本时尤其有用：
- en: Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`.
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及从`keras.models`中导入`Model`类，从`keras.layers`中导入`Input`、`Conv2D`、`MaxPooling2D`和`UpSampling2D`。
- en: Load the `mnist.pkl` file, which contains the first 10,000 images and corresponding
    labels from the MNIST dataset, which are available in the accompanying source
    code.
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`mnist.pkl`文件，该文件包含来自MNIST数据集的前10,000张图像及其对应标签，这些文件可在随附的源代码中找到。
- en: Note
  id: totrans-411
  prefs:
  - PREF_IND
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `mnist.pkl` file from [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11).
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以从[https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson05/Activity11)下载`mnist.pkl`文件。
- en: Rescale the images to have values between 0 and 1.
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像重新缩放为值在0和1之间。
- en: We need to reshape the images to add a single depth channel for use with convolutional
    stages. Reshape the images to have a shape of 28 x 28 x 1.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重塑图像，添加一个单一的深度通道，以便与卷积阶段一起使用。将图像重塑为28 x 28 x 1的形状。
- en: Define an input layer. We will use the same shape input as an image.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义输入层。我们将使用与图像相同形状的输入。
- en: Add a convolutional stage, with 16 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using same padding, which means the output has
    the same length as the input image.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个卷积阶段，使用16层或过滤器，3 x 3的权重矩阵，ReLU激活函数，并使用相同填充，这意味着输出的大小与输入图像相同。
- en: Add a max pooling layer to the encoder with a 2 x 2 kernel.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在编码器中添加一个最大池化层，使用2 x 2的卷积核。
- en: Add a decoding convolutional layer.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个解码卷积层。
- en: Add an upsampling layer.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个上采样层。
- en: Add the final convolutional stage using 1 layer as per the initial image depth.
  id: totrans-420
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照初始图像深度，使用1层添加最终的卷积阶段。
- en: Construct the model by passing the first and last layers of the network to the
    `Model` class.
  id: totrans-421
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给`Model`类来构建模型。
- en: Display the structure of the model.
  id: totrans-422
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型的结构。
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和`adadelta`梯度下降法编译自编码器。
- en: Now, let's fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs as convolutional networks take a lot
    longer to compute.
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们来拟合模型；再次将图像作为训练数据和期望的输出。训练20个周期，因为卷积网络需要较长的计算时间。
- en: Calculate and store the output of the encoding stage for the first five samples.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储编码阶段对前五个样本的输出。
- en: Reshape the encoder output for visualization, where each image is X*Y in size.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了可视化，将编码器的输出重塑为每张图像大小为X*Y。
- en: Get the output of the decoder for the first five images.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取解码器对前五个图像的输出。
- en: Reshape the decoder output to be 28 x 28 in size.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器输出重塑为28 x 28的大小。
- en: Reshape the original images back to be 28 x 28 in size.
  id: totrans-429
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始图像重新调整回28 x 28的大小。
- en: Plot the original image, the mean encoder output, and the decoder.
  id: totrans-430
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、平均编码器输出和解码器。
- en: 'At the end of this activity, you will have developed an autoencoder comprising
    convolutional layers within the neural network. Note the improvements made in
    the decoder representations. The output will be similar to the following:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动结束时，你将开发一个包含卷积层的自编码器神经网络。注意解码器表示的改进。输出将类似于以下内容：
- en: '![](img/C12626_05_34.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![](img/C12626_05_34.jpg)'
- en: 'Figure 5.34: Expected original image, the encoder output, and the decoder'
  id: totrans-433
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图5.34：期望的原始图像、编码器输出和解码器
- en: Note
  id: totrans-434
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity can be found on page 340.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第340页找到。
- en: Summary
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started with an introduction to artificial neural networks,
    how they are structured, and the processes by which they learn to complete a particular
    task. Starting with a supervised learning example, we built an artificial neural
    network classifier to identify objects within the CIFAR-10 dataset. We then progressed
    to the autoencoder architecture of neural networks and learned how we can use
    these networks to prepare a dataset for use in an unsupervised learning problem.
    Finally, we completed this investigation with autoencoders, looking at convolutional
    neural networks and the benefits these additional layers can provide. This chapter
    prepared us well for the final instalment in dimensionality reduction, as we look
    at using and visualizing the encoded data with t-distributed nearest neighbors
    (t-SNE). T-distributed nearest neighbors provides an extremely effective method
    of visualizing high-dimensional data even after applying reduction techniques
    such as PCA. T-SNE is particularly useful method for unsupervised learning.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了人工神经网络，它们的结构以及它们如何学习完成特定任务。从一个有监督学习的例子开始，我们构建了一个人工神经网络分类器来识别CIFAR-10数据集中的物体。然后，我们深入了解了神经网络的自编码器架构，学习了如何利用这些网络为无监督学习问题准备数据集。最后，我们完成了这项调查，查看了卷积神经网络以及这些附加层能带来的好处。通过这章内容，我们为接下来进行降维的最终章节做好了准备，学习如何使用和可视化通过t-分布近邻（t-SNE）编码的数据。t-SNE提供了一种非常有效的方法来可视化高维数据，即使在应用PCA等降维技术后也是如此。t-SNE是无监督学习中特别有用的方法。
