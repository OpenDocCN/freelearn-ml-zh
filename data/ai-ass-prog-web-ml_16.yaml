- en: '16'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '16'
- en: 'Unsupervised Learning: Clustering and PCA'
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习：聚类和PCA
- en: Introduction
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Unsupervised learning models find patterns in unlabeled data. Clustering is
    a technique for finding groups of objects such that the objects in a group are
    like one another, yet objects in different groups are dissimilar. **Principal
    component analysis** (**PCA**) is a technique for reducing the dimensionality
    of data. We will discuss both techniques in the context of product clustering,
    which uses textual product descriptions to group similar products together.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习模型在未标记数据中寻找模式。聚类是一种寻找对象组的技巧，使得组内的对象彼此相似，而不同组内的对象则不相似。**主成分分析**（**PCA**）是一种降低数据维度的技巧。我们将在这两种技术中讨论产品聚类，它使用文本产品描述将相似产品分组在一起。
- en: 'In this chapter, we will:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将：
- en: 'Discuss two unsupervised learning techniques: clustering and principal component
    analysis.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论两种无监督学习技术：聚类和主成分分析。
- en: Use the K-means clustering algorithm.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K-means聚类算法。
- en: Breaking the problem down into features
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将问题分解为特征
- en: 'To break down the problems into features, we need to consider:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 要将问题分解为特征，我们需要考虑：
- en: '**Data preparation:** Load the dataset and inspect the data to understand its
    structure, missing values, and overall characteristics. Preprocess the data, which
    may involve handling missing values, data type conversions, and data cleaning.'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**数据准备**：加载数据集并检查数据以了解其结构、缺失值和整体特征。预处理数据，可能涉及处理缺失值、数据类型转换和数据清洗。'
- en: '**Feature engineering:** Select relevant features, extract features from text,
    and derive new features.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**特征工程**：选择相关特征，从文本中提取特征，并推导新特征。'
- en: '**Text data preprocessing:** Tokenize text, remove punctuation, and stop words.
    Convert text to numerical format using the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) technique.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**文本数据预处理**：分词、去除标点符号和停用词。使用**词频-逆文档频率**（**TF-IDF**）技术将文本转换为数值格式。'
- en: '**Apply clustering algorithm:** Create a K-means clustering model and determine
    the optimal number of clusters using appropriate techniques like the elbow method
    and silhouette score.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**应用聚类算法**：创建K-means聚类模型，并使用如肘部方法和轮廓分数等适当技术确定最佳聚类数量。'
- en: '**Evaluate and visualize clustering results:** Assess clustering performance
    and visualize the results using PCA in reduced dimensionality space.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**评估和可视化聚类结果**：评估聚类性能并使用PCA在降维空间中可视化结果。'
- en: We will use the TAG prompt pattern as described in *Chapter 2*, that is, specify
    the task, actions to take, and guidance needed.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用第2章中描述的TAG提示模式，即指定任务、采取的操作和所需的指导。
- en: Prompt strategy
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提示策略
- en: 'In this chapter, we’re using the TAG pattern (Task-Action-Guidance) as described
    in *Chapter 2*. We know the following of our problem to solve:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用第2章中描述的TAG模式（任务-操作-指导）。我们了解以下要解决的问题：
- en: '**Task**:Create a customer segmentation clustering model.'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：创建客户细分聚类模型。'
- en: '**Action**: We need to ask for steps to take and techniques to use.'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作**：我们需要询问采取的步骤和使用的技巧。'
- en: '**Guidance**: Asking to learn step-by-step.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指导**：逐步学习请求。'
- en: Customer segmentation
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 客户细分
- en: Clustering can help segment customers based on their purchasing behavior, preferences,
    or demographic information. By analyzing customer data such as browsing history,
    purchase history, location, and demographic details, you can apply clustering
    algorithms to identify distinct customer segments. This information can then be
    used to personalize marketing campaigns, recommend relevant products, or tailor
    the user experience to different customer groups.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类可以帮助根据客户的购买行为、偏好或人口统计信息进行客户细分。通过分析客户数据，如浏览历史、购买历史、位置和人口统计细节，您可以应用聚类算法来识别不同的客户细分。这些信息可以用于个性化营销活动、推荐相关产品或根据不同的客户群体定制用户体验。
- en: The dataset
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据集
- en: 'We will use the e-commerce dataset, which can be downloaded as a CSV file from
    the UCI Machine Learning Repository: [https://archive.ics.uci.edu/dataset/352/online+retail](https://archive.ics.uci.edu/dataset/352/online+retail).
    It contains data for all the transactions that occurred between 01/12/2010 and
    09/12/2011 for a UK-based and registered non-store online retailer.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用电子商务数据集，该数据集可以从UCI机器学习仓库下载为CSV文件：[https://archive.ics.uci.edu/dataset/352/online+retail](https://archive.ics.uci.edu/dataset/352/online+retail)。它包含2010年12月1日至2011年9月9日之间一个英国注册的非实体店在线零售商的所有交易数据。
- en: 'The dataset contains the following columns:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含以下列：
- en: '`InvoiceNo`: A 6-digit integral number uniquely assigned to each transaction'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InvoiceNo`：唯一分配给每笔交易的6位整数'
- en: '`StockCode`: A 5-digit integral number uniquely assigned to each distinct product'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`StockCode`：唯一分配给每个不同产品的5位整数'
- en: '`Description`: A textual description of each product name: text'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Description`：每个产品名称的文本描述：文本'
- en: '`Quantity`: The quantities of each product (item) per transaction'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Quantity`：每笔交易中每种产品的数量'
- en: '`InvoiceDate`: The day and time when each transaction was generated'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`InvoiceDate`：每笔交易生成的时间和日期'
- en: '`UnitPrice`: The product price per unit sterling no.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UnitPrice`：每单位英镑的产品价格'
- en: '`CustomerID`: A 5-digit integral number uniquely assigned to each customer'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CustomerID`：唯一分配给每位客户的5位整数'
- en: '`Country`: The name of the country where each customer resides'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Country`：每个客户居住的国家的名称'
- en: Adding AI assistance to the unsupervised learning model development process
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将人工智能辅助添加到无监督学习模型开发过程中
- en: Let’s leverage ChatGPT to add AI assistance to the unsupervised learning model
    development process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用ChatGPT将人工智能辅助添加到无监督学习模型开发过程中。
- en: 'Let’s start by outlining our task to ChatGPT. Let’s see how we can use the
    TAG pattern:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从向ChatGPT概述我们的任务开始。让我们看看我们如何使用TAG模式：
- en: '**Task**: Our task is to create a Segmentation Clustering model'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**任务**：我们的任务是创建一个细分聚类模型'
- en: '**Action**: We want to know from the AI assistant what actions to take'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作**：我们想知道人工智能助手应该采取哪些行动'
- en: '**Guidance**: We need the AI assistant to be efficient'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**指导**：我们需要人工智能助手高效工作'
- en: 'Based on that, let’s write our prompt:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些信息，让我们编写我们的提示：
- en: '**[Prompt]**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'Can you please provide guidance on how to load and understand a dataset for
    creating a Customer Segmentation Clustering model using the data from the `data.csv`
    file? The dataset contains the following columns: `InvoiceNo` (integer), `StockCode`
    (string), `Description` (string), `Quantity` (integer), `InvoiceDate` (date),
    `UnitPrice` (decimal), `CustomerID` (integer), `Country` (string).'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 请提供有关如何使用`data.csv`文件中的数据创建客户细分聚类模型的指导，以便加载数据集和理解。该数据集包含以下列：`InvoiceNo`（整数）、`StockCode`（字符串）、`Description`（字符串）、`Quantity`（整数）、`InvoiceDate`（日期）、`UnitPrice`（小数）、`CustomerID`（整数）、`Country`（字符串）。
- en: 'I would like to know the necessary steps and techniques to perform this task
    effectively using Python. I will be using Jupyter Notebook. The dataset is in
    the GitHub repository and is using latin1 encoding. Remember I want to learn it
    step by step where the steps are:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我想了解使用Python有效地执行此任务所需的必要步骤和技术。我将使用Jupyter Notebook。数据集位于GitHub仓库中，并使用latin1编码。请记住，我想一步一步地学习，步骤如下：
- en: '**Load the Dataset**: Read the dataset into a pandas DataFrame to work with
    the data effectively.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**加载数据集**：将数据集读入pandas DataFrame以有效地处理数据。'
- en: '**Inspect the Data**: Display the first few rows of the DataFrame to get a
    glimpse of the data. Check the column names, data types, and any missing values.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**检查数据**：显示DataFrame的前几行以了解数据。检查列名、数据类型和任何缺失值。'
- en: '**Summary Statistics**: Calculate summary statistics such as mean, median,
    minimum, maximum, and quartiles for numeric columns. This helps in understanding
    the distribution and range of values.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**摘要统计**：计算数值列的摘要统计，如平均值、中位数、最小值、最大值和四分位数。这有助于了解值的分布和范围。'
- en: '**[End of prompt]**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: Sure! Let’s go through the step-by-step process of loading the dataset, inspecting
    the data, and calculating summary statistics for the Customer Segmentation Clustering
    model using Python in a Jupyter Notebook. We will assume the dataset is in a file
    named “data.csv” located in a GitHub repository with the “latin1” encoding.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们一步一步地通过使用Python在Jupyter Notebook中加载数据集、检查数据和计算客户细分聚类模型的摘要统计来了解加载数据集的过程。我们将假设数据集存储在一个名为“data.csv”的文件中，该文件位于使用“latin1”编码的GitHub仓库中。
- en: 'First, make sure you have the necessary libraries installed:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保您已安装必要的库：
- en: '[PRE0]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now, let’s implement the three blocks of code as you requested:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们实现您请求的三个代码块：
- en: 'Load the Dataset:'
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: '[PRE1]'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Inspect the Data:'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据：
- en: '[PRE2]'
  id: totrans-55
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Summary Statistics:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摘要统计：
- en: '[PRE3]'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, let me explain the code:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我来解释一下代码：
- en: 'Load the Dataset:'
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据集：
- en: We import the necessary libraries, mainly `pandas` for data manipulation and
    analysis.
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们导入必要的库，主要是`pandas`用于数据处理和分析。
- en: We define the `data_url` variable to point to the raw CSV file on GitHub.
  id: totrans-61
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们定义`data_url`变量以指向GitHub上的原始CSV文件。
- en: We use `pd.read_csv` to read the data from the CSV file into a pandas DataFrame
    called `df`. We display the shape of the DataFrame using `df.shape` to get an
    idea of the number of rows and columns in the dataset.
  id: totrans-62
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`pd.read_csv`从CSV文件中读取数据到名为`df`的pandas DataFrame中。我们使用`df.shape`显示DataFrame的形状，以了解数据集中的行数和列数。
- en: We display the first few rows of the DataFrame using `df.head()` to take a quick
    look at the data.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`df.head()`显示DataFrame的前几行，以便快速查看数据。
- en: 'Inspect the Data:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据：
- en: We use `df.dtypes` to display the column names and their corresponding data
    types to understand the types of data we have.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`df.dtypes`来显示列名及其对应的数据类型，以了解我们拥有的数据类型。
- en: We use `df.isnull().sum()` to check for missing values in each column. This
    will give us the count of missing values for each attribute.
  id: totrans-66
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`df.isnull().sum()`来检查每列的缺失值。这将给出每个属性的缺失值计数。
- en: 'Summary Statistics:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 摘要统计信息：
- en: We calculate summary statistics using `df.describe(include='all')`. The `include='all'`
    argument ensures that statistics are calculated for both numeric and non-numeric
    columns.
  id: totrans-68
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`df.describe(include='all')`计算摘要统计信息。`include='all'`参数确保对数值和非数值列都计算统计信息。
- en: We transpose the summary statistics using `summary_stats.transpose()` to get
    a more readable format where columns represent attributes.
  id: totrans-69
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用`summary_stats.transpose()`将摘要统计信息转置，以获得一个更易读的格式，其中列代表属性。
- en: Finally, we display the summary statistics using `print(summary_stats)`.
  id: totrans-70
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用`print(summary_stats)`显示摘要统计信息。
- en: Remember to replace `"yourusername/yourrepository"` with the actual username
    and repository name in the `data_url` variable. Now you should be able to execute
    the code blocks sequentially in a Jupyter Notebook to load, inspect, and understand
    the dataset for your Customer Segmentation Clustering model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，将`data_url`变量中的`"yourusername/yourrepository"`替换为实际的用户名和存储库名。现在你应该能够按顺序在Jupyter
    Notebook中执行代码块，以加载、检查和理解客户细分聚类模型的数据集。
- en: '**[End of response]**'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束回复]**'
- en: Load the dataset
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 加载数据集
- en: 'After loading the dataset and running `df.head()`, you should see an output
    similar to this text:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载数据集并运行`df.head()`后，你应该看到类似于以下文本的输出：
- en: '[PRE4]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Inspect the data
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查数据
- en: 'Running the code to inspect the data using `df.info()` will give us some initial
    information about the columns, data types, and missing values:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`df.info()`运行检查数据的代码将给我们一些关于列、数据类型和缺失值的初步信息：
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_01.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21232_16_01.png)'
- en: 'Figure 16.1: Output of DataFrame after running df.info()'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1：运行df.info()后的DataFrame输出
- en: Summary statistics
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要统计信息
- en: 'We can get further information about the data by executing the `Summary Statistics`
    script, `df.describe()`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 通过执行`Summary Statistics`脚本，`df.describe()`，我们可以获取有关数据的更多信息：
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_02.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21232_16_02.png)'
- en: 'Figure 16.2: Summary of statistics running df.describe()'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2：运行df.describe()的统计摘要
- en: We will need to preprocess the data before we can use it for our Customer Segmentation
    Clustering model.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以使用数据为我们的客户细分聚类模型之前，我们需要对数据进行预处理。
- en: We will cover preprocessing in the next subsection.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一小节中介绍预处理。
- en: Preprocessing the data
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'After inspecting the data, we can see that there are some missing values in
    the `CustomerID` column. We will need to remove these missing values before we
    can use the data for our Customer Segmentation Clustering model. Quantitative
    columns, such as `Quantity` and `UnitPrice`, also have some negative values, which
    we will need to remove. We will drop some of the columns that we will not use
    for our Customer Segmentation Clustering model: StockCode, InvoiceDate, Description,
    and Country.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在检查数据后，我们可以看到`CustomerID`列中存在一些缺失值。在我们可以使用数据为我们的客户细分聚类模型之前，我们需要删除这些缺失值。定量列，如`Quantity`和`UnitPrice`，也存在一些负值，我们需要删除。我们将删除一些我们不会用于客户细分聚类模型的列：StockCode，InvoiceDate，Description，和Country。
- en: Let’s create ChatGPT prompts for each of these steps.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这些步骤中的每一个创建ChatGPT提示。
- en: '**[Prompt]**'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'I would like you to help me implement the following steps to preprocess the
    data for my Customer Segmentation Clustering model using Python in a Jupyter Notebook:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能帮助我在Jupyter Notebook中使用Python实现以下步骤，以预处理我的客户细分聚类模型的数据：
- en: '**Remove Missing Values**: Remove rows with missing values in the `CustomerID`
    column.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除缺失值**：删除`CustomerID`列中存在缺失值的行。'
- en: '**Remove Negative Values**: Remove rows with negative values in the `Quantity`
    and `UnitPrice` columns.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**移除负值**：删除`Quantity`和`UnitPrice`列中存在负值的行。'
- en: '**Remove Unused Columns**: Remove the `StockCode`, `InvoiceDate`, `Description`,
    `Country` columns.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**删除未使用列**：删除`StockCode`，`InvoiceDate`，`Description`，`Country`列。'
- en: '**[End of prompt]**'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Sure! I’ll guide you through the steps to preprocess the data for your Customer
    Segmentation Clustering model using Python in a Jupyter Notebook. Let’s start
    with the data preprocessing steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！我将指导您使用Python在Jupyter Notebook中预处理您的客户细分聚类模型的数据。让我们从数据预处理步骤开始：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'In the code above:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We use the `dropna()` function to remove rows with missing values in the `CustomerID`
    column. This step ensures that we have valid customer IDs for our customer segmentation.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`dropna()`函数删除`CustomerID`列中存在缺失值的行。这一步骤确保我们有有效的客户ID用于我们的客户细分。
- en: We use boolean indexing to filter out rows with negative values in the `Quantity`
    and `UnitPrice` columns. We only keep rows where both `Quantity` and `UnitPrice`
    are non-negative.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用布尔索引来过滤掉`Quantity`和`UnitPrice`列中存在负值的行。我们只保留`Quantity`和`UnitPrice`都为非负值的行。
- en: We use the `drop()` function to remove the columns ‘StockCode’, ‘InvoiceDate’,
    ‘Description’, and ‘Country’ as they are not used for the customer segmentation
    in this case.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`drop()`函数删除了‘StockCode’，‘InvoiceDate’，‘Description’，和‘Country’列，因为这些列在此情况下不用于客户细分。
- en: Make sure to run this block of code after running the previous blocks to continue
    from the preprocessed DataFrame `df_cleaned`. Also, keep in mind that data preprocessing
    is an essential step to ensure the quality of the data used for clustering. These
    steps will help remove invalid or irrelevant data before applying the clustering
    algorithm for customer segmentation.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在运行前面的代码块之后运行此代码块，以便从预处理后的DataFrame `df_cleaned`继续。同时，请注意，数据预处理是确保用于聚类的数据质量的一个关键步骤。这些步骤将有助于在应用聚类算法进行客户细分之前移除无效或不相关的数据。
- en: 'Let’s run the preprocessing code and check the output:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行预处理代码并检查输出：
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_03.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21232_16_03.png)'
- en: 'Figure 16.3: Output of cleaned DataFrame'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3：清洗后的DataFrame输出
- en: '**[End of response]**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Feature engineering
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: 'We will need to perform some feature engineering to create new features that
    will help us in our Customer Segmentation Clustering model. We will create a new
    column called `Amount` by multiplying the `Quantity` and `UnitPrice` columns.
    We will then create a `TotalAmount` column by summing up `Amount` for each `CustomerID`.
    We will also create a `TotalTransactions` column for each customer by counting
    the number of invoices for this customer. The new dataset that we will use for
    customer segmentation will have two columns: `TotalAmount` and `TotalTransactions`.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要执行一些特征工程来创建新的特征，这些特征将有助于我们的客户细分聚类模型。我们将通过将`Quantity`和`UnitPrice`列相乘来创建一个名为`Amount`的新列。然后，我们将通过为每个`CustomerID`相加`Amount`来创建一个`TotalAmount`列。我们还将为每个客户创建一个`TotalTransactions`列，通过计算此客户的发票数量。我们将使用的新数据集将包含两列：`TotalAmount`和`TotalTransactions`。
- en: 'Let’s create ChatGPT prompts for each of these steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为这些步骤创建ChatGPT提示：
- en: '**[Prompt]**'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: '**Create Amount Column**: Create a new column called `Amount` by multiplying
    the `Quantity` and `UnitPrice` columns.'
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建金额列**：通过将`Quantity`和`UnitPrice`列相乘来创建一个名为`Amount`的新列。'
- en: '**Create TotalAmount Column**: Create a new column called `TotalAmount` by
    summing up `Amount` for each `CustomerID`. Drop the `Amount` column.'
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建总金额列**：通过将每个`CustomerID`的`Amount`相加，创建一个名为`TotalAmount`的新列。删除`Amount`列。'
- en: '**Create TotalTransactions Column**: Create a new column called `TotalTransactions`
    by counting the number of invoices for each `CustomerID`.'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建总交易数列**：通过计算每个`CustomerID`的发票数量来创建一个名为`TotalTransactions`的新列。'
- en: '**Create Customer Segmentation Dataset**: Create a new DataFrame called `df_segmentation`
    with two columns: `TotalAmount` and `TotalTransactions`.'
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**创建客户细分数据集**：创建一个名为`df_segmentation`的新DataFrame，包含两列：`TotalAmount`和`TotalTransactions`。'
- en: '**[End of prompt]**'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Sure! I’ll guide you through the feature engineering steps to create new features
    for your Customer Segmentation Clustering model using Python in a Jupyter Notebook.
    Let’s proceed with the feature engineering:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！我将指导你通过特征工程步骤，在Jupyter Notebook中使用Python创建新的特征，以构建你的客户分段聚类模型。让我们继续进行特征工程：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the code above:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We create a new column `Amount` by multiplying the `Quantity` and `UnitPrice`
    columns, capturing the total amount spent on each transaction.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过将`Quantity`和`UnitPrice`列相乘创建一个新的`Amount`列，以捕捉每个交易的总支出金额。
- en: We group the data by `CustomerID` and calculate the `TotalAmount` for each customer
    by summing up the `Amount` for all their transactions. We use the `groupby()`
    and `sum()` functions for this step.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们按`CustomerID`对数据进行分组，并通过对所有交易的`Amount`求和来计算每个客户的`TotalAmount`。我们使用`groupby()`和`sum()`函数来完成这一步骤。
- en: We also group the data by `CustomerID` and count the number of unique invoices
    (transactions) for each customer to create the `TotalTransactions` column using
    the `groupby()` and `nunique()` functions.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还按`CustomerID`对数据进行分组，并计算每个客户的唯一发票（交易）数量，以使用`groupby()`和`nunique()`函数创建`TotalTransactions`列。
- en: We create the final DataFrame `df_segmentation` by merging `df_total_amount`
    and `df_total_transactions` on the `CustomerID` column.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过在`CustomerID`列上合并`df_total_amount`和`df_total_transactions`创建最终的DataFrame
    `df_segmentation`。
- en: 'The resulting `df_segmentation` DataFrame will have two columns: `TotalAmount`
    and `TotalTransactions`, which will be used as features for customer segmentation.
    These new features can provide valuable insights for clustering customers based
    on their spending behavior and transaction frequency.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的`df_segmentation` DataFrame将包含两列：`TotalAmount`和`TotalTransactions`，这些将用作客户分段的特征。这些新特征可以为基于客户的消费行为和交易频率进行聚类提供有价值的见解。
- en: '**[End of response]**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: 'After running the code, we should be able to see the output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们应该能够看到以下输出：
- en: '![A screenshot of a computer screen  Description automatically generated](img/B21232_16_04.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21232_16_04.png)'
- en: 'Figure 16.4: Result of df.segmentation, TotalAmount, and TotalTransactions'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4：df.segmentation的结果，TotalAmount和TotalTransactions
- en: Checking for outliers
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 检查异常值
- en: Next, we’re going to check for outliers, which are data points that significantly
    differ from other observations in our dataset.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查异常值，这些是与其他数据观察值显著不同的数据点。
- en: '**[Prompt]**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'We are going to perform segmentation based on the `TotalAmount` and `TotalTransactions`
    features. Clustering algorithms are sensitive to outliers. So, let’s check if
    we have any outliers in our data. Please help me to: drop `CustomerID` column,
    check if there are many outliers in the resultant dataset that will be used for
    Customer segmentation.'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将根据`TotalAmount`和`TotalTransactions`特征进行分段。聚类算法对异常值敏感。因此，让我们检查我们的数据中是否有任何异常值。请帮助我：删除`CustomerID`列，检查结果数据集中是否有许多异常值，这些数据将用于客户分段。
- en: '**[End of prompt]**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Sure! Let’s proceed with dropping the `CustomerID` column from the `df_segmentation`
    DataFrame and then check for outliers in the resultant dataset:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们继续从`df_segmentation` DataFrame中删除`CustomerID`列，然后检查结果数据集中的异常值：
- en: '[PRE7]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'In the code above:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We drop the `CustomerID` column from the `df_segmentation` DataFrame using the
    `drop()` function with `axis=1` to specify that we are dropping a column.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`drop()`函数和`axis=1`参数从`df_segmentation` DataFrame中删除`CustomerID`列，以指定我们正在删除一列。
- en: We use box plots to visualize the distribution of `TotalAmount` and `TotalTransactions`
    and identify any potential outliers. The box plots will show the median, quartiles,
    and any data points beyond the whiskers, which can be considered as potential
    outliers.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用箱线图来可视化`TotalAmount`和`TotalTransactions`的分布，并识别任何潜在的异常值。箱线图将显示中位数、四分位数以及任何超出触须的数据点，这些可以被认为是潜在的异常值。
- en: '**[End of response]**'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Inspect the box plot to see if there are any points significantly far from the
    central distribution. Outliers, if present, will appear as individual data points
    outside the whiskers of the box plot. Depending on the visualization, you can
    decide whether to handle the outliers’ using techniques like capping, removing
    them, or using robust clustering algorithms that are less sensitive to outliers.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 检查箱线图，看是否有任何点显著偏离中心分布。如果存在异常值，它们将作为箱线图外的独立数据点出现。根据可视化结果，你可以决定是否使用如截顶、移除或使用对异常值不太敏感的鲁棒聚类算法等技术来处理异常值。
- en: Keep in mind that the definition of an outlier depends on the context of your
    data and the requirements of your customer segmentation. Outliers may represent
    significant or rare customer behavior and removing them should be done with caution
    and based on domain knowledge.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，异常值的定义取决于你的数据上下文和客户细分的需求。异常值可能代表重要的或罕见客户行为，移除它们应谨慎进行，并基于领域知识。
- en: '![A diagram of a box plot  Description automatically generated](img/B21232_16_05.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![箱线图的示意图  自动生成的描述](img/B21232_16_05.png)'
- en: 'Figure 16.5: Box plot of TotalAmount and TotalTransactions columns'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5：TotalAmount和TotalTransactions列的箱线图
- en: Removing outliers
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 移除异常值
- en: Next, let’s remove the outliers in our dataset.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们从数据集中移除异常值。
- en: '**[Prompt]**'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: The ‘Box plot of TotalAmount and TotalTransactions’ showed that there are outliers
    in the `TotalAmount` column of the dataset. I would like to remove the outliers
    from the dataset.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: “TotalAmount和TotalTransactions的箱线图”显示了数据集中`TotalAmount`列存在异常值。我希望从数据集中移除这些异常值。
- en: '**[End of prompt]**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束提示]**'
- en: '**[Prompt response]**'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示回复]**'
- en: 'Sure! To remove outliers from the `df_segmentation` DataFrame based on the
    `TotalAmount` column, we can use various methods. One common approach is to use
    the Interquartile Range (IQR) method to detect and remove outliers. Here’s how
    you can do it:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！要从基于`TotalAmount`列的`df_segmentation` DataFrame中移除异常值，我们可以使用各种方法。一种常见的方法是使用四分位距（IQR）方法来检测和移除异常值。以下是你可以这样做的方法：
- en: '[PRE8]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'In the code above:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We calculate the Interquartile Range (IQR) for the `TotalAmount` column using
    the `quantile()` function.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`quantile()`函数计算`TotalAmount`列的四分位距（IQR）。
- en: We define the lower and upper bounds for outlier detection as 1.5 times the
    IQR below the first quartile (Q1) and above the third quartile (Q3), respectively.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将异常检测的下限和上限定义为第一四分位数（Q1）下方1.5倍的四分位距（IQR）和第三四分位数（Q3）上方的四分位距（IQR）。
- en: We use boolean indexing to keep only the rows where the `TotalAmount` values
    are within the defined bounds, effectively removing the outliers.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用布尔索引来保留`TotalAmount`值在定义的界限内的行，从而有效地移除异常值。
- en: Finally, we display the updated shape of the DataFrame after removing outliers.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们显示移除异常值后DataFrame的更新形状。
- en: Remember that removing outliers can affect the representativeness of your data
    and the clustering results. Before removing outliers, it’s essential to understand
    the impact they might have on your customer segmentation and consider the underlying
    reasons for their presence in the data. Depending on the context and your specific
    use case, you may choose to handle outliers differently, such as capping them,
    transforming the data, or using robust clustering algorithms.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，移除异常值可能会影响数据的代表性和聚类结果。在移除异常值之前，了解它们可能对客户细分产生的影响以及它们在数据中存在的原因是至关重要的。根据上下文和你的具体用例，你可能选择以不同的方式处理异常值，例如截顶、转换数据或使用鲁棒聚类算法。
- en: '**[End of response]**'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束回复]**'
- en: 'Let’s run the code for step 7 and check the output:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行第7步的代码并检查输出：
- en: '![A close-up of a white background  Description automatically generated](img/B21232_16_06.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![白色背景的特写  自动生成的描述](img/B21232_16_06.png)'
- en: 'Figure 16.6: Showing the difference in the DataFrame after having removed outliers'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6：显示移除异常值后DataFrame的差异
- en: Data scaling using standardization
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用标准化进行数据缩放
- en: Next, let’s carry out a preprocessing step called “data scaling using standardization.”
    It will involve transforming our data so that it has a mean of 0 and a standard
    deviation of 1\. It’s useful when our data have different units or scales and
    ensures each feature contributes equally to the analysis we’re carrying out.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们执行一个称为“使用标准化进行数据缩放”的预处理步骤。这涉及到将我们的数据转换，使其均值为0，标准差为1。当我们的数据具有不同的单位或尺度时，这很有用，并确保每个特征在进行的分析中贡献相等。
- en: '**[Prompt]**'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: Clustering methods are sensitive to the data scaling. I would like to scale
    the dataset.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类方法对数据缩放敏感。我想对数据集进行缩放。
- en: '**[End of prompt]**'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'You’re right! Clustering methods, particularly distance-based ones like K-means,
    are sensitive to the scale of the features. To ensure that all features have the
    same scale and contribute equally to the clustering process, we need to perform
    data scaling. A common approach is to use standardization (also known as z-score
    scaling) or normalization. Here, I’ll show you how to use standardization to scale
    the dataset:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 你是对的！聚类方法，尤其是像K-means这样的基于距离的方法，对特征的规模敏感。为了确保所有特征具有相同的规模并平等地贡献于聚类过程，我们需要进行数据缩放。一种常见的方法是使用标准化（也称为z分数缩放）或归一化。在这里，我将向您展示如何使用标准化来缩放数据集：
- en: '[PRE9]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In the code above:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We import `StandardScaler` from the `sklearn.preprocessing` module, which will
    help us perform the standardization.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们从`sklearn.preprocessing`模块导入`StandardScaler`，这将帮助我们执行标准化。
- en: We initialize the `StandardScaler` as `scaler`.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`StandardScaler`初始化为`scaler`。
- en: We use `scaler.fit_transform()` to perform standardization on the `df_segmentation`
    DataFrame, which contains the `TotalAmount` and `TotalTransactions` columns.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`scaler.fit_transform()`对包含`TotalAmount`和`TotalTransactions`列的`df_segmentation`
    DataFrame进行标准化。
- en: The result of the `fit_transform()` operation is an array, so we convert it
    back to a DataFrame using `pd.DataFrame()` with the original column names.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`fit_transform()`操作的结果是一个数组，因此我们使用`pd.DataFrame()`将原始列名转换回DataFrame。'
- en: After scaling, both `TotalAmount` and `TotalTransactions` will have a mean of
    0 and a standard deviation of 1, which helps in the clustering process, ensuring
    that no single feature dominates the clustering algorithm due to a larger scale.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放后，`TotalAmount`和`TotalTransactions`都将具有0均值和1标准差，这有助于聚类过程，确保没有单个特征由于较大的规模而主导聚类算法。
- en: Now, you have a scaled DataFrame `df_scaled`, which is ready to be used for
    customer segmentation using clustering methods.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您有一个缩放后的DataFrame `df_scaled`，它已准备好用于使用聚类方法进行客户细分。
- en: '**[End of response]**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: 'Running code for step 8 will result in this ouput:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 运行第8步的代码将产生以下输出：
- en: '[PRE10]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Deciding on the number of clusters
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 决定簇的数量
- en: K-means is a popular clustering algorithm used to partition a dataset into k
    clusters, where each data point belongs to a cluster of the nearest mean. The
    goal of using K-means is to identify patterns, by grouping similar data points
    together. You also want to reduce the complexity of your dataset. Other benefits
    include improved analysis.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是一种流行的聚类算法，用于将数据集划分为k个簇，其中每个数据点属于最近的均值簇。使用K-means的目标是通过将相似数据点分组来识别模式。您还希望降低数据集的复杂性。其他好处包括分析改进。
- en: 'To effectively apply a clustering method like K-means, I need to decide on
    the number of clusters. There are several methods we could use to find that number;
    let’s ask ChatGPT:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了有效地应用聚类方法如K-means，我需要决定簇的数量。我们可以使用几种方法来找到那个数字；让我们询问ChatGPT：
- en: '**[Prompt]**'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: How can I determine the optimum number of clusters for the dataset?
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我如何确定数据集的最佳簇数量？
- en: '**[End of prompt]**'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Determining the optimum number of clusters, often referred to as the “elbow”
    point, is a crucial step when applying clustering methods like K-means. There
    are several methods you can use to find the appropriate number of clusters for
    your dataset. One of the most common approaches is the “Elbow Method,” but other
    methods like the “Silhouette Score” and “Gap Statistics” can also be helpful.
    Here’s how you can use these methods:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 确定最佳簇数量，通常被称为“肘部点”，在应用聚类方法如K-means时是一个关键步骤。您可以使用几种方法来找到适合您数据集的适当簇数量。最常见的方法之一是“肘部方法”，但其他方法如“轮廓分数”和“间隙统计”也可能很有帮助。以下是您可以使用这些方法的方式：
- en: '**Elbow Method**: The Elbow Method involves plotting the **within-cluster sum
    of squares** (**WCSS**) against the number of clusters. WCSS represents the sum
    of squared distances between each point and the centroid of its assigned cluster.
    The idea is to find the point on the plot where the decrease in WCSS starts to
    slow down, forming an “elbow” shape.'
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**肘部方法**：肘部方法涉及将**簇内平方和**（WCSS）与簇的数量绘制成图。WCSS代表每个点与其分配簇的质心之间的平方距离之和。想法是在图中找到WCSS开始减慢下降的点，形成一个“肘部”形状。'
- en: '[PRE11]'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In the Elbow Method graph, look for the “elbow” point, which is the point where
    the curve starts to flatten. This point indicates the optimal number of clusters
    for your dataset. However, keep in mind that the Elbow Method is not always definitive,
    and the elbow point might not be very pronounced in some cases.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在肘部方法图中，寻找“肘部”点，这是曲线开始变平的点。这个点表示您数据集的最佳聚类数量。然而，请注意，肘部方法并不总是确定的，有时肘部点可能不太明显。
- en: '**Silhouette Score**: The Silhouette Score measures how well each data point
    fits its assigned cluster and ranges from -1 to 1\. A higher Silhouette Score
    indicates better-defined clusters. To use the Silhouette Score, loop through different
    values of K and calculate the average Silhouette Score for each value.'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**轮廓分数**：轮廓分数衡量每个数据点与其分配的聚类的拟合程度，范围从-1到1。较高的轮廓分数表示定义更清晰的聚类。要使用轮廓分数，对不同的K值进行循环，并计算每个值的平均轮廓分数。'
- en: '[PRE12]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The optimal number of clusters is often associated with the highest Silhouette
    Score.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳聚类数量通常与最高的轮廓分数相关联。
- en: '**Gap Statistics**: Gap Statistics compare the WCSS of the clustering algorithm
    with the WCSS of randomly generated data. It helps determine if the clustering
    structure in your data is significant compared to random data.'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**差距统计**：差距统计比较聚类算法的WCSS与随机生成数据的WCSS。它有助于确定您的数据中的聚类结构相对于随机数据是否显著。'
- en: 'Implementing Gap Statistics is more involved, but libraries like `scikit-learn-extra`
    provide functions to calculate it:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 实现差距统计更为复杂，但像`scikit-learn-extra`这样的库提供了计算它的函数：
- en: '[PRE13]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Again, look for the point where the Gap Value starts to flatten, indicating
    the optimum number of clusters.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 再次寻找差距值开始变平的点，这表明最佳聚类数量。
- en: Remember that the choice of the optimal number of clusters is often a subjective
    decision based on the insights you seek and the specific problem context. These
    methods can give you a starting point, but you may need to experiment and validate
    the clustering results based on your domain knowledge and business requirements.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，选择最佳聚类数量通常是一个基于您寻求的见解和特定问题背景的主观决定。这些方法可以为您提供起点，但您可能需要根据您的领域知识和业务需求进行实验和验证聚类结果。
- en: '**[End of response]**'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: 'Let’s execute the code for the elbow method and the silhouette score and inspect
    the output:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行肘部方法和轮廓分数的代码，并检查输出：
- en: '![A graph showing a number of clusters  Description automatically generated](img/B21232_16_07.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![显示聚类数量的图表，自动生成描述](img/B21232_16_07.png)'
- en: 'Figure 16.7: Number of clusters using the elbow method'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7：使用肘部方法确定的聚类数量
- en: '![A graph with a line  Description automatically generated](img/B21232_16_08.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![带有线条的图表，自动生成描述](img/B21232_16_08.png)'
- en: 'Figure 16.8: Number of clusters using the silhouette score method'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8：使用轮廓分数方法确定的聚类数量
- en: The elbow method and the silhouette score method that we used to determine the
    optimal number of clusters suggested six clusters. We will use six clusters for
    our K-means model. Let’s create a ChatGPT prompt to generate a Customer Segmentation
    K-means clustering model with six clusters.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们用来确定最佳聚类数量的肘部方法和轮廓分数方法建议使用六个聚类。我们将使用六个聚类来构建我们的K-means模型。让我们创建一个ChatGPT提示来生成一个包含六个聚类的客户细分K-means聚类模型。
- en: Creating a clustering model
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建聚类模型
- en: Now that we’ve determined the number of clusters to use, let’s create a cluster
    model next.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了要使用的聚类数量，接下来让我们创建一个聚类模型。
- en: '**[Prompt]**'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: Please help me to create a Customer Segmentation K-means clustering model with
    6 clusters.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 请帮助我创建一个包含6个聚类的客户细分K-means聚类模型。
- en: '**[End of prompt]**'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Sure! Let’s create a Customer Segmentation K-means clustering model with 6
    clusters using the scaled data `df_scaled`. We’ll use the `KMeans` class from
    scikit-learn to perform the clustering:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们使用缩放数据`df_scaled`创建一个包含6个聚类的客户细分K-means聚类模型。我们将使用scikit-learn的`KMeans`类来进行聚类：
- en: '[PRE14]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the code above:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We specify `num_clusters` as 6, which will create 6 clusters for customer segmentation.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将`num_clusters`指定为6，这将为客户细分创建6个聚类。
- en: We create the K-means clustering model using `KMeans` with the desired number
    of clusters and set `random_state` for reproducibility.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`KMeans`创建所需的聚类数量的K-means聚类模型，并设置`random_state`以实现可重复性。
- en: The model is fitted to the scaled data `df_scaled` using `fit()`.
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型使用`fit()`方法拟合到缩放数据`df_scaled`。
- en: We add the cluster labels to the original DataFrame `df_segmentation` using
    `kmeans_model.labels_`, and store it in a new column called ‘Cluster’.
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`kmeans_model.labels_`将簇标签添加到原始DataFrame `df_segmentation`中，并将其存储在新列‘Cluster’中。
- en: The resulting `df_segmentation` DataFrame will now include the ‘Cluster’ column
    indicating the cluster assignment for each customer based on their ‘TotalAmount’
    and ‘TotalTransactions’. Each customer will be associated with one of the six
    clusters.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`df_segmentation` DataFrame现在将包括表示每个客户基于其‘TotalAmount’和‘TotalTransactions’的簇分配的‘Cluster’列。每个客户将与六个簇中的一个相关联。
- en: Keep in mind that the interpretation of the clusters will depend on the context
    of your data and your domain knowledge. You may want to analyze the characteristics
    of each cluster to gain insights about different customer segments and tailor
    your marketing or business strategies accordingly. Visualization techniques, such
    as scatter plots or cluster profiles, can be helpful in understanding and interpreting
    the results of the customer segmentation.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，簇的解释将取决于您数据的环境和您的领域知识。您可能想要分析每个簇的特征，以获得关于不同客户细分市场的见解，并据此调整您的营销或商业策略。可视化技术，如散点图或簇轮廓，有助于理解和解释客户细分的结果。
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_09.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图 自动生成描述](img/B21232_16_09.png)'
- en: 'Figure 16.9: Output from running df.segmentation()'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9：运行df.segmentation()的输出
- en: '**[End of response]**'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Visualize clustering results
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化聚类结果
- en: 'Now, we can visualize the results of the clustering. Let’s create the next
    prompt:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以可视化聚类的结果。让我们创建下一个提示：
- en: '**[Prompt]**'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'I would like to visualize the results of the clustering. Please create a scatter
    plot with the following specifications: the horizontal axis should be “Total Amount”,
    the vertical axis should be “Total Transactions”, and the color of the dots should
    be determined by the cluster number. Please also add a legend to the plot.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望可视化聚类的结果。请创建一个符合以下规范的散点图：横轴应为“Total Amount”，纵轴应为“Total Transactions”，点的颜色应由簇编号决定。请也为图表添加图例。
- en: '**[End of prompt]**'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: Sure! Let’s create a scatter plot to visualize the results of the clustering.
    We’ll use the `matplotlib` library for plotting, and the `seaborn` library for
    color mapping and a more visually appealing plot. If you don’t have the `seaborn`
    library installed, you can install it using `pip install seaborn`.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们创建一个散点图来可视化聚类的结果。我们将使用`matplotlib`库进行绘图，使用`seaborn`库进行颜色映射和更吸引人的图形。如果您尚未安装`seaborn`库，可以使用`pip
    install seaborn`进行安装。
- en: '[PRE15]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the code above:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在上面的代码中：
- en: We use `seaborn.scatterplot()` to create the scatter plot. The ‘TotalAmount’
    is plotted on the horizontal axis (`x`), the ‘TotalTransactions’ on the vertical
    axis (`y`), and the ‘Cluster’ column is used for color mapping using the `hue`
    parameter.
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`seaborn.scatterplot()`创建散点图。‘TotalAmount’在横轴（`x`）上绘制，‘TotalTransactions’在纵轴（`y`）上绘制，而‘Cluster’列用于使用`hue`参数进行颜色映射。
- en: We set the color palette to ‘viridis’ using the `palette` parameter, which will
    provide distinct colors for each cluster. The `s=50` parameter sets the size of
    the dots in the scatter plot to 50 for better visibility.
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用`palette`参数将调色板设置为‘viridis’，这将为每个簇提供独特的颜色。`s=50`参数将散点图中点的尺寸设置为50，以获得更好的可见性。
- en: We add axis labels, a plot title, a legend, and grid lines to improve the plot’s
    readability.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们添加了轴标签、图表标题、图例和网格线，以提高图表的可读性。
- en: '**[End of response]**'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: The resulting scatter plot will display each customer as a dot in the 2-dimensional
    space defined by ‘TotalAmount’ and ‘TotalTransactions’. The dots will be colored
    according to their cluster assignment, allowing you to visually identify the different
    customer segments. The legend will help you understand which color corresponds
    to which cluster number.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的散点图将显示每个客户在由‘TotalAmount’和‘TotalTransactions’定义的二维空间中的点。点将根据其簇分配进行着色，使您能够直观地识别不同的客户细分市场。图例将帮助您理解哪种颜色对应哪种簇编号。
- en: '![A graph of a customer segmentation  Description automatically generated](img/B21232_16_10.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![客户细分图 自动生成描述](img/B21232_16_10.png)'
- en: 'Figure 16.10: Data points divided up into six clusters'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10：数据点被划分为六个簇
- en: Final thoughts on clustering and the prompting process
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对聚类和提示过程的最终思考
- en: We have completed Customer Segmentation Clustering with the assistance of ChatGPT.
    To achieve our goal, we have used several different types of prompts.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在ChatGPT的帮助下完成了客户细分聚类。为了实现我们的目标，我们使用了多种不同类型的提示。
- en: 'The prompt often begins by describing the dataset to be used, such as the source,
    columns, and their types, which sets a high-level context. ChatGPT then requests
    assistance in implementing a Customer Segmentation clustering model using a specific
    algorithm, in this case, K-means clustering. To summarize the prompts used, we
    can see that it blends a set of different techniques, which mirrors how you would
    carry out data science without an AI assistant. You can see below the different
    types of prompt types and how they helped with the clustering process:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 提示通常从描述要使用的数据集开始，例如来源、列及其类型，这为设置了一个高级上下文。ChatGPT随后请求协助使用特定的算法实现客户细分聚类模型，在这种情况下，是K-means聚类算法。为了总结所使用的提示，我们可以看到它融合了一系列不同的技术，这反映了在没有AI助手的情况下如何执行数据科学。以下展示了不同类型的提示类型以及它们如何帮助聚类过程：
- en: '**Step-by-step instructions**: These provided a step-by-step guide on how to
    approach the problem, which includes loading and understanding the dataset, data
    preprocessing (removing missing values, negative values, and unused columns),
    and feature engineering (creating new features like Amount, TotalAmount, and TotalTransactions).'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逐步指导**：这些提供了如何解决问题的逐步指南，包括加载数据集、理解数据集、数据预处理（移除缺失值、负值和未使用的列）以及特征工程（创建新的特征，如Amount、TotalAmount和TotalTransactions）。'
- en: '**Clustering algorithm choice**: This specified the choice of the clustering
    algorithm, which is K-means clustering in this case, along with the number of
    clusters to be used for segmentation.'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类算法选择**：这指定了聚类算法的选择，在本例中是K-means聚类，以及用于聚类的聚类数量。'
- en: '**Outlier handling**: addressed the sensitivity of clustering algorithms to
    outliers and requests to remove outliers from the data using the **interquartile
    range** (**IQR**) method.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理异常值**：解决了聚类算法对异常值的敏感性，并请求使用**四分位数范围**（**IQR**）方法从数据中移除异常值。'
- en: '**Data scaling**: This emphasized the importance of scaling the data to ensure
    clustering accuracy and instructs to use standardization to scale the features.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据缩放**：这强调了缩放数据以确保聚类准确性的重要性，并指导使用标准化来缩放特征。'
- en: '**Cluster visualization**: This asked for visualization techniques to display
    the results of the clustering model, typically through scatter plots, with TotalAmount
    on the x-axis, TotalTransactions on the y-axis, and color-coded points representing
    different clusters.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚类可视化**：这要求提供可视化技术来显示聚类模型的结果，通常通过散点图，TotalAmount在x轴上，TotalTransactions在y轴上，用颜色编码的点代表不同的聚类。'
- en: '**Optimal number of clusters**: The prompt guides in determining the optimal
    number of clusters using the elbow method and silhouette score, which helps in
    finding the right number of clusters that best represent the data.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最佳聚类数量**：提示指导使用肘部方法和轮廓分数来确定最佳聚类数量，这有助于找到最能代表数据的正确数量的聚类。'
- en: '**Combined script request**: The prompt requests to combine all the generated
    code into a single script, including detailed explanations as comments, to provide
    a comprehensive solution for implementing the Customer Segmentation clustering
    model.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合并脚本请求**：提示要求将所有生成的代码合并成一个单独的脚本，包括作为注释的详细说明，以提供一个全面解决实现客户细分聚类模型的方案。'
- en: Product clustering for an e-commerce project
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 电子商务项目的产品聚类
- en: In this task, we will perform product clustering using the same e-commerce dataset.
    The goal is to create an unsupervised learning model to cluster products based
    on their textual descriptions using Python and common libraries.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个任务中，我们将使用相同的电子商务数据集执行产品聚类。目标是创建一个无监督学习模型，根据产品的文本描述使用Python和常用库进行聚类。
- en: 'Your initial prompt: Set context'
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 您的初始提示：设置上下文
- en: As usual, when using ChatGPT, it’s important to set a high-level context. For
    data science, high-level context means describing things like the data used (e-commerce
    data), what we’re trying to achieve (get data insights), and how we’re looking
    to achieve it (clustering).
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 像往常一样，当使用ChatGPT时，设置一个高级上下文很重要。对于数据科学来说，高级上下文意味着描述数据使用（电子商务数据）、我们试图实现的目标（获取数据洞察）以及我们打算如何实现它（聚类）。
- en: 'Below is an example of a prompt providing all the suggested context we just
    described in the preceding paragraph:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个示例提示，提供了前面段落中我们刚刚描述的所有建议的上下文：
- en: '**[Prompt]**'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'I would like you to act as an experienced data scientist and help me implement
    Product Categorization model using clustering. The dataset is e-commerce data
    from the UCI Machine Learning Repository. It is a transnational dataset that contains
    all the transactions occurring between 01/12/2010 and 09/12/2011 for a UK-based
    and registered non-store online retailer. The dataset contains the following columns:
    `InvoiceNo` (integer), `StockCode` (string), `Description` (string), `Quantity`
    (integer), `InvoiceDate` (date), `UnitPrice` (decimal), `CustomerID` (integer),
    `Country` (string). Can you suggest the steps needed to perform Product Categorization
    using the data from a csv file.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能扮演一位经验丰富的数据科学家，并帮助我实现使用聚类进行产品分类的模型。数据集来自UCI机器学习仓库的电子商务数据。这是一个跨国数据集，包含了2010年12月1日至2011年9月9日之间一个英国注册的非实体店在线零售商的所有交易。数据集包含以下列：`InvoiceNo`（整数），`StockCode`（字符串），`Description`（字符串），`Quantity`（整数），`InvoiceDate`（日期），`UnitPrice`（小数），`CustomerID`（整数），`Country`（字符串）。你能建议使用csv文件中的数据进行产品分类所需的步骤吗？
- en: '**[End of prompt]**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: 'You should receive a response like the below response:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该收到如下响应：
- en: '**[Prompt response]**'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Certainly! Product categorization using clustering is a great approach to group
    similar products together based on their attributes. Here’s a step-by-step guide
    to perform product categorization using the given e-commerce dataset:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！使用聚类进行产品分类是将具有相似属性的产品分组在一起的好方法。以下是一个使用给定电子商务数据集进行产品分类的逐步指南：
- en: 'Step 1: Load and Preprocess the Data'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 第1步：加载数据并进行预处理
- en: Load the dataset from the CSV file into a suitable data structure, like a Pandas
    DataFrame in Python.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据集从CSV文件加载到合适的数据结构中，例如Python中的Pandas DataFrame。
- en: Inspect the data to understand its structure, missing values, and overall characteristics.
  id: totrans-264
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查数据以了解其结构、缺失值和整体特征。
- en: Preprocess the data, which may involve handling missing values, data type conversions,
    and data cleaning.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据，这可能涉及处理缺失值、数据类型转换和数据清理。
- en: 'Step 2: Feature Engineering'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 第2步：特征工程
- en: Select relevant features for clustering. In this case, you can consider features
    like ‘StockCode’, ‘Description’, ‘Quantity’, ‘UnitPrice’, etc. Customer-related
    features like ‘CustomerID’ might not be directly relevant for product categorization.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择与聚类相关的特征。在这种情况下，你可以考虑像‘StockCode’，‘Description’，‘Quantity’，‘UnitPrice’等特征。与客户相关的特征，如‘CustomerID’，可能对产品分类并不直接相关。
- en: Create a feature matrix, where each row corresponds to a product and each column
    corresponds to a feature.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个特征矩阵，其中每一行对应一个产品，每一列对应一个特征。
- en: 'Step 3: Text Data Preprocessing'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步：文本数据预处理
- en: For text-based features like ‘Description’, perform text preprocessing steps
    such as lowercasing, tokenization, removing punctuation, and stopwords.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于基于文本的特征，如‘Description’，执行文本预处理步骤，例如小写化、分词、去除标点符号和停用词。
- en: Convert the text data into numerical format using techniques like TF-IDF (Term
    Frequency-Inverse Document Frequency) or word embeddings.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用TF-IDF（词频-逆文档频率）或词嵌入等技术将文本数据转换为数值格式。
- en: 'Step 4: Choose Clustering Algorithm'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 第4步：选择聚类算法
- en: Select an appropriate clustering algorithm. Common choices include K-means,
    Hierarchical clustering, and DBSCAN.
  id: totrans-273
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择合适的聚类算法。常见的选项包括K-means，层次聚类和DBSCAN。
- en: Tune hyperparameters if needed.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如有必要，调整超参数。
- en: 'Step 5: Feature Scaling'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 第5步：特征缩放
- en: Scale the numerical features to have similar ranges. This is important for algorithms
    like K-means that are distance-based.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数值特征缩放到相似的范围内。这对于基于距离的算法，如K-means来说非常重要。
- en: 'Step 6: Apply Clustering Algorithm'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 第6步：应用聚类算法
- en: Apply the chosen clustering algorithm to the preprocessed data.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将选定的聚类算法应用于预处理后的数据。
- en: Determine the optimal number of clusters. You can use techniques like the Elbow
    Method, Silhouette Score, or Gap Statistic.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确定最佳聚类数量。你可以使用肘部方法、轮廓分数或差距统计等技术。
- en: 'Step 7: Interpret Clusters'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 第7步：解释聚类
- en: Once you have clusters, analyze the products within each cluster to understand
    the common characteristics.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一旦有了聚类，分析每个聚类中的产品以了解其共同特征。
- en: Assign meaningful labels to the clusters based on your analysis. These labels
    will represent the product categories.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你的分析为聚类分配有意义的标签。这些标签将代表产品类别。
- en: 'Step 8: Visualize Clusters (Optional)'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 第8步：可视化聚类（可选）
- en: Use visualization techniques such as scatter plots, t-SNE, or PCA to visualize
    the clusters in a reduced dimensional space.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用散点图、t-SNE或PCA等可视化技术，在降维空间中可视化聚类。
- en: 'Step 9: Assign Categories to Products'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 第9步：将产品分配到类别
- en: Assign each product to its respective category based on the cluster it belongs
    to.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据产品所属的聚类将其分配到相应的类别。
- en: 'Step 10: Evaluate and Refine'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 第10步：评估和细化
- en: Evaluate the quality of your clustering results. You can use domain knowledge
    or external validation metrics if available.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估聚类结果的质量。如果你有领域知识或外部验证指标，可以使用它们。
- en: Refine the process by adjusting preprocessing steps, algorithm parameters, or
    features as needed.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需要调整预处理步骤、算法参数或特征以细化过程。
- en: Remember that clustering is an iterative process, and the quality of your results
    may depend on various factors such as data quality, feature selection, and algorithm
    choice. It’s also important to consider the business context and domain knowledge
    when interpreting and labeling the clusters.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，聚类是一个迭代过程，你的结果质量可能取决于各种因素，如数据质量、特征选择和算法选择。在解释和标记聚类时，考虑业务背景和领域知识也很重要。
- en: '**[End of response]**'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Having reviewed the steps in the response, you can now proceed with the next
    prompt.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 审查了响应中的步骤后，你现在可以继续进行下一个提示。
- en: Load and preprocess the data
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载数据并进行预处理
- en: Now that we have a series of steps described to us how to perform clustering,
    we want to carry out the first step from the response namely loading and preprocessing
    the data.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经被告知了一系列步骤来执行聚类，我们想要从响应中执行第一步，即加载数据并进行预处理。
- en: '**[Prompt]**'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'I would like to do it step by step. Please guide me through Step 1: Load and
    Preprocess the Data. My data URL is “../Datasets/data.csv” and the CSV file is
    using latin1 encoding. I would use the column ‘Description’ for clustering to
    identify product categories.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望一步一步来做。请指导我进行第一步：加载数据并进行预处理。我的数据URL是“../Datasets/data.csv”，CSV文件使用latin1编码。我将使用‘描述’列进行聚类以识别产品类别。
- en: '**[End of prompt]**'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: 'You should receive a response like the following text:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该收到如下文本的响应：
- en: '**[Prompt response]**'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Of course! Let’s start with Step 1: Load and Preprocess the Data.'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们从第一步：加载数据并进行预处理开始。
- en: '[PRE16]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This code will load the dataset from the provided CSV file using the Latin1
    encoding. It will then display basic information about the dataset, such as the
    data types of each column and the number of non-null values.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将使用Latin1编码从提供的CSV文件加载数据集。然后，它将显示有关数据集的基本信息，例如每列的数据类型和非空值的数量。
- en: Additionally, it will display the first few rows of the dataset to give you
    a sense of the data’s structure.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，它还将显示数据集的前几行，以让你对数据的结构有一个概念。
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_11.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![计算机描述自动生成的截图](img/B21232_16_11.png)'
- en: 'Figure 16.11: First few rows of the dataset'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11：数据集的前几行
- en: Now, let’s preprocess the data and focus on the ‘Description’ column for clustering.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们先预处理数据，并专注于‘描述’列进行聚类。
- en: '[PRE17]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: In this code snippet, we first drop rows with missing ‘Description’ values since
    we’ll be using this column for clustering. Next, we display basic statistics of
    the numerical columns to get an overview of the data’s distribution. After that,
    we count the number of unique descriptions to understand the variety of products.
    Finally, we display the top 10 most common descriptions to see the most frequent
    products in the dataset.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个代码片段中，我们首先删除了具有缺失‘描述’值的行，因为我们将在聚类中使用这个列。接下来，我们显示数值列的基本统计信息，以了解数据的分布概览。然后，我们计算唯一描述的数量，以了解产品的多样性。最后，我们显示最常见的10个描述，以查看数据集中最频繁的产品。
- en: At this point, you have loaded the data, dropped missing values in the ‘Description’
    column, and gained some insights into the data’s characteristics. The next step
    will involve text data preprocessing for the ‘Description’ column. Let me know
    if you’re ready to move on to Step 3, or if you have any questions about the current
    steps!
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经加载数据，删除了‘描述’列中的缺失值，并对数据的特征有了一些了解。下一步将涉及对‘描述’列的文本数据预处理。如果你准备好了进入第三步，或者对当前步骤有任何疑问，请告诉我！
- en: '**[End of response]**'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: After copying the code to a Jupyter Notebook, we find that it works perfectly
    fine, and we learn that 541,909 dataset records contain 4,223 unique product descriptions.
    The next step is to perform text data preprocessing for the ‘Description’ column.
    Let’s proceed with the next prompt.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码复制到Jupyter Notebook后，我们发现它运行得非常好，我们了解到541,909个数据集记录包含4,223个独特的商品描述。下一步是对‘描述’列进行文本数据预处理。让我们继续下一个提示。
- en: Feature engineering and text data preprocessing
  id: totrans-312
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程和文本数据预处理
- en: 'At this point, the data has been read and we know the size of the dataset as
    well. Let’s carry on with the next two steps, feature engineering and text data
    preprocessing:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，数据已经被读取，我们也知道了数据集的大小。让我们继续进行下一步，特征工程和文本数据预处理：
- en: '**[Prompt]**'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示**]'
- en: The dataset contains 541909 entries and there are 4223 unique product descriptions.
    I would like to drop duplicate ‘Descriptions’ and perform other text data preprocessing
    and feature engineering. Please create a script with comments and include libraries
    and `nltk` resources to download.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含541909条记录，有4223个独特的商品描述。我想删除重复的‘描述’并执行其他文本数据预处理和特征工程。请创建一个带有注释的脚本，并包括库和`nltk`资源下载。
- en: '**[End of prompt]**'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束**]'
- en: '**[Prompt response]**'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应**]'
- en: 'Sure, I’ll provide you with a script that covers text data preprocessing and
    feature engineering steps. Let’s get started:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我会提供一个包括文本数据预处理和特征工程步骤的脚本。让我们开始吧：
- en: '[PRE18]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In this script:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在此脚本中：
- en: We import necessary libraries including `pandas`, `nltk`, and `TfidfVectorizer`
    from `sklearn.feature_extraction.text`.
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们导入必要的库，包括`pandas`、`nltk`和来自`sklearn.feature_extraction.text`的`TfidfVectorizer`。
- en: We download NLTK resources (`punkt` for tokenization and `stopwords` for removing
    stopwords) if they are not already downloaded.
  id: totrans-322
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果尚未下载，我们将下载NLTK资源（`punkt`用于分词和`stopwords`用于去除停用词）。
- en: We load the dataset and drop rows with missing ‘Description’ values and duplicate
    descriptions.
  id: totrans-323
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们加载数据集，并删除包含缺失的‘描述’值和重复描述的行。
- en: 'We perform text preprocessing steps on the ‘Description’ column: converting
    to lowercase, tokenizing, removing punctuation, and stopwords.'
  id: totrans-324
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们在‘描述’列上执行文本预处理步骤：转换为小写，分词，去除标点符号和停用词。
- en: We convert the tokenized descriptions back to strings.
  id: totrans-325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将分词后的描述转换回字符串。
- en: We use TF-IDF vectorization to convert the preprocessed text data into numerical
    features. You can adjust the `max_features` parameter to control the number of
    features.
  id: totrans-326
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用TF-IDF向量化将预处理后的文本数据转换为数值特征。你可以调整`max_features`参数来控制特征的数量。
- en: The TF-IDF matrix is converted to a DataFrame for further analysis.
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将TF-IDF矩阵转换为DataFrame以进行进一步分析。
- en: This script will preprocess the text data and perform feature engineering using
    TF-IDF vectorization. The resulting TF-IDF matrix can be used as input for clustering
    algorithms in the subsequent steps.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将预处理文本数据，并使用TF-IDF向量化进行特征工程。生成的TF-IDF矩阵可以用作后续步骤中聚类算法的输入。
- en: '**[End of response]**'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束**]'
- en: After pasting the script to Jupyter Notebook, we find that it works perfectly
    fine, and we learn that the TF-IDF matrix has 4,223 rows (one for each unique
    product description) and 1,000 columns (one for each feature).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 将脚本粘贴到Jupyter Notebook后，我们发现它运行得非常好，我们了解到TF-IDF矩阵有4,223行（每个独特的商品描述一行）和1,000列（每个特征一列）。
- en: '![A screenshot of a computer  Description automatically generated](img/B21232_16_12.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![计算机屏幕截图，描述自动生成](img/B21232_16_12.png)'
- en: 'Figure 16.12: TF-IDF matrix'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12：TF-IDF矩阵
- en: Feature engineering
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征工程
- en: We will continue with feature engineering. Let’s visualize the most frequent
    features in the TF-IDF matrix.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续进行特征工程。让我们可视化TF-IDF矩阵中最频繁的特征。
- en: '**[Prompt]**'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示**]'
- en: I would like to visualize the top 100 most frequent features from the TF-IDF
    matrix. Please create a bar chart with words on the vertical axis.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我想可视化TF-IDF矩阵中最频繁的100个特征。请创建一个以单词为纵轴的柱状图。
- en: '**[End of prompt]**'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束**]'
- en: '**[Prompt response]**'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应**]'
- en: 'Certainly! To visualize the top 100 most frequent features from the TF-IDF
    matrix, you can create a bar chart using libraries like `matplotlib`. Here’s how
    you can do it:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！为了可视化TF-IDF矩阵中最频繁的100个特征，你可以使用像`matplotlib`这样的库创建柱状图。以下是你可以这样做的步骤：
- en: '[PRE19]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This script calculates the sum of TF-IDF scores for each feature and selects
    the top 100 features by frequency. It then creates a horizontal bar chart using
    `matplotlib` to visualize the top features. Adjust the figure size, labels, and
    other parameters as needed to customize the visualization to your preferences.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本计算每个特征的TF-IDF分数总和，并按频率选择前100个特征。然后使用`matplotlib`创建水平条形图来可视化顶级特征。根据需要调整图形大小、标签和其他参数以自定义可视化以符合您的偏好。
- en: '**[End of response]**'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: '**[回复结束]**'
- en: 'Running this code in Jupyter Notebook will produce the following bar chart:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 在Jupyter Notebook中运行此代码将生成以下条形图：
- en: '![A graph of a number of people  Description automatically generated with medium
    confidence](img/B21232_16_13.png)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![人数图表，描述自动生成，中等置信度](img/B21232_16_13.png)'
- en: 'Figure 16.13: Most frequent features'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.13：最频繁的特征
- en: Let’s analyze this chart and proceed with the next prompt.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析这张图表并继续下一个提示。
- en: '**[Prompt]**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: We can see that the features contain numbers and words representing colors,
    like red, blue, and other colors. These words will not be useful for product categorization.
    Can you create a list of words that can represent color.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，特征中包含数字和表示颜色的词汇，如红色、蓝色和其他颜色。这些词汇对产品分类没有帮助。你能创建一个可以表示颜色的词汇列表吗？
- en: '**[End of prompt]**'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示回复]**'
- en: Certainly! Here’s a list of common words that represent colors. You can use
    this list to filter out color-related words from your features before performing
    product categorization.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！以下是一份表示颜色的常用词汇列表。您可以使用此列表在产品分类之前过滤掉与颜色相关的词汇。
- en: '[PRE20]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'You can use this list to filter out color-related words from your features
    during the text preprocessing step. For example:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用此列表在文本预处理步骤中过滤掉与颜色相关的词汇。例如：
- en: '[PRE21]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '**[End of response]**'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**[回复结束]**'
- en: By removing color-related words, you can help improve the quality of your product
    categorization by focusing on more meaningful features.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 通过移除与颜色相关的词汇，您可以通过关注更有意义的特征来帮助提高产品分类的质量。
- en: '**[Prompt]**'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: Modify the script to remove `color_words` and words that are less than three
    characters long. Refactor code to encapsulate data preprocessing and vectorisation
    into a function `preprocess_and_vectorize_data`.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 修改脚本以移除`color_words`和长度小于三个字符的词汇。将代码重构为将数据预处理和向量化封装到`preprocess_and_vectorize_data`函数中。
- en: '**[End of prompt]**'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示回复]**'
- en: 'Certainly! Here’s the modified script with the requested changes, including
    encapsulating the data preprocessing and vectorization steps into a function:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！以下是经过修改的脚本，包括将数据预处理和向量化步骤封装到函数中：
- en: '[PRE22]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This script encapsulates the data preprocessing and vectorization steps into
    the `preprocess_and_vectorize_data` function. The function takes the raw data
    as input, performs the required preprocessing, and returns the TF-IDF matrix.
    The rest of the script remains similar to before.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 此脚本将数据预处理和向量化步骤封装到`preprocess_and_vectorize_data`函数中。该函数接收原始数据作为输入，执行所需的预处理，并返回TF-IDF矩阵。脚本的其他部分与之前类似。
- en: '**[End of response]**'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**[回复结束]**'
- en: After running the script, we can visualize the updated set of the most frequent
    words from the TF-IDF matrix output.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，我们可以可视化TF-IDF矩阵输出的最频繁词汇的更新集合。
- en: '![A graph of a number of people  Description automatically generated with medium
    confidence](img/B21232_16_14.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![人数图表，描述自动生成，中等置信度](img/B21232_16_14.png)'
- en: 'Figure 16.14: Most frequent features that are not colors'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.14：不是颜色的最频繁特征
- en: Let’s proceed with our feature engineering. We will use ChatGPT to suggest options
    that can be explored with TF-IDF vectorization.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行特征工程。我们将使用ChatGPT来建议可以探索的TF-IDF向量化选项。
- en: '**[Prompt]**'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: The TF-IDF vectorization is initialized with `max_features` parameter. What
    are other initialization options?
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF向量化使用`max_features`参数初始化。还有其他初始化选项吗？
- en: '**[End of prompt]**'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示回复]**'
- en: When initializing the TF-IDF vectorizer in scikit-learn, you can set several
    parameters to customize its behavior.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中初始化TF-IDF向量器时，您可以设置多个参数来自定义其行为。
- en: 'In addition to the `max_features` parameter, here are some other important
    parameters you can consider:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`max_features`参数外，还有一些其他重要的参数您可以考虑：
- en: '`max_df` and `min_df`: These parameters control the threshold for the document
    frequency of terms. For example, you can set `max_df` to ignore terms that appear
    in a high percentage of documents, and `min_df` to ignore terms that appear in
    a low percentage of documents.'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`max_df`和`min_df`：这些参数控制术语的文档频率阈值。例如，您可以设置`max_df`以忽略在大量文档中出现的术语，并设置`min_df`以忽略在少量文档中出现的术语。'
- en: '`ngram_range`: This parameter specifies the range of n-grams to consider during
    tokenization. For example, setting `ngram_range=(1, 2)` will consider both individual
    words and pairs of consecutive words (bigrams).'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`ngram_range`：此参数指定在分词过程中要考虑的n-gram的范围。例如，将`ngram_range=(1, 2)`设置为考虑单个单词和连续单词对（二元组）。'
- en: '`stop_words`: You can provide a list of stopwords to be ignored during tokenization.
    This can be a custom list or one of the built-in lists like `''english''`.'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`stop_words`：您可以在分词过程中提供要忽略的停用词列表。这可以是一个自定义列表或内置列表之一，如`''english''`。'
- en: '`tokenizer`: You can specify a custom tokenizer function if you want to perform
    additional text preprocessing or tokenization.'
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`tokenizer`：如果您想要执行额外的文本预处理或分词，可以指定自定义的分词函数。'
- en: '`use_idf`: This parameter controls whether to use the inverse-document-frequency
    (IDF) re-weighting of the term frequencies.'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`use_idf`：此参数控制是否使用逆文档频率（IDF）重新加权词频。'
- en: '`smooth_idf`: This parameter controls whether to add a smoothing term to the
    IDF to avoid division by zero errors.'
  id: totrans-380
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`smooth_idf`：此参数控制是否向IDF添加平滑项以避免除以零错误。'
- en: '`sublinear_tf`: If set to `True`, it applies sublinear scaling to the term
    frequency, which can help to prevent very frequent terms from dominating the calculation.'
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sublinear_tf`：如果设置为`True`，则对词频应用次线性缩放，这有助于防止非常频繁的术语主导计算。'
- en: 'Here’s an example of how you can initialize the TF-IDF vectorizer with some
    of these parameters:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个例子，说明您如何使用一些这些参数初始化TF-IDF向量器：
- en: '[PRE23]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '**[End of response]**'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Feel free to experiment with these parameters based on your specific dataset
    and goals. The choice of parameters may depend on the nature of the text data
    and the desired outcomes of your clustering or categorization task.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您特定的数据集和目标，随意实验这些参数。参数的选择可能取决于文本数据的性质以及您聚类或分类任务所期望的结果。
- en: Choose clustering algorithm
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 选择聚类算法
- en: There are quite a few clustering algorithms we could choose. There is, for example,
    K-means, that we used for another use case; there’s also hierarchical clustering,
    **DBSCAN** (**Density-based clustering algorithm**), **Gaussian mixture models**
    (**GMMs**), and agglomerative clustering. We’re going to go with K-means, but
    why is that? The reason is that we’re working with customers and want to divide
    them up into groups based on purchasing behavior, demographic properties, and
    so on. Another reason for choosing K-means is its simplicity and being able to
    work with large datasets.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择相当多的聚类算法。例如，K-means，我们用于另一个用例；还有层次聚类、**DBSCAN**（基于密度的聚类算法）、**高斯混合模型**（GMMs）和凝聚聚类。我们将选择K-means，但为什么是K-means呢？原因是我们在处理客户，并希望根据购买行为、人口统计属性等将他们分成组。选择K-means的另一个原因是它的简单性以及能够处理大型数据集。
- en: 'You might wonder when to apply the other mentioned clustering algorithms in
    the context of e-commerce. Well, let’s look at some interesting areas in this
    domain:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会想知道在电子商务的背景下何时应用其他提到的聚类算法。好吧，让我们看看这个领域的一些有趣领域：
- en: '**Product recommendation**: Here, it would be more appropriate to use hierarchical
    clustering as products usually come in hierarchies and we want to understand the
    relationship between products.'
  id: totrans-389
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**产品推荐**：在这里，使用层次聚类会更合适，因为产品通常以层次结构出现，我们想要了解产品之间的关系。'
- en: '**Anomaly detection**: Another important area is anomaly detection, especially
    fraudulent transactions. Here, you want to understand unusual customer behavior
    to identify outliers; for that use case, DBSCAN is the best algorithm to go with.'
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常检测**：另一个重要领域是异常检测，特别是欺诈交易。在这里，您想要了解不寻常的客户行为以识别异常值；为此用例，DBSCAN（基于密度的聚类算法）是最佳算法。'
- en: '**Personalization**: You want to ensure customers feel like they have an experience
    uniquely fitted to their behaviors on the website. GMM is a great choice for this
    use case when a customer’s behavior is a bit more complex.'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**个性化**：您希望确保客户感觉他们所拥有的体验是独特地适应他们在网站上的行为的。当客户的行为稍微复杂一些时，GMM（高斯混合模型）是这种用例的一个很好的选择。'
- en: Feature scaling
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征缩放
- en: 'Feature scaling and TF-IDF are both preprocessing techniques and it only makes
    sense to choose one of them. Which one to choose depends on what we want to use
    it for. In the context of e-commerce, here’s how you could reason:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放和TF-IDF都是预处理技术，只选择其中之一是有意义的。选择哪一个取决于我们想用它做什么。在电子商务的背景下，您可以这样推理：
- en: Choose feature scaling if you’re dealing with numerical features like customer
    age, purchase amounts, and purchase frequency (e.g., grouping customers based
    on their purchasing behavior).
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您处理的是像客户年龄、购买金额和购买频率这样的数值特征（例如，根据客户的购买行为对客户进行分组），请选择特征缩放。
- en: Choose the TF-IDF matrix if you’re looking at other aspects of e-commerce like
    customer reviews, product descriptions, and search queries.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您正在查看电子商务的其他方面，如客户评论、产品描述和搜索查询，请选择TF-IDF矩阵。
- en: We can skip this step as we are using a TF-IDF matrix that is already scaled.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们使用的是已经缩放的TF-IDF矩阵，我们可以跳过这一步。
- en: Apply clustering algorithm
  id: totrans-397
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用聚类算法
- en: 'Next, let’s apply our chosen clustering algorithm:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们应用我们选择的聚类算法：
- en: '**[Prompt]**'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: 'I would like to use english `stop_words`, no `max_features`**,** and pass `min_df`
    to `preprocess_and_vectorize_data` function to set minimum number of documents
    the feature appears in. I would like to try a range of values for `min_df: 5,
    10, 20, 30` and compare the silhouette score from the K-means output. I would
    like to visualize the result.'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 我想使用英文`stop_words`，不使用`max_features`**，**并将`min_df`传递给`preprocess_and_vectorize_data`函数以设置特征在文档中出现的最小文档数。我想尝试`min_df`的一系列值：5,
    10, 20, 30，并比较K-means输出的轮廓得分。我想可视化结果。
- en: '**[End of prompt]**'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Sure! Let’s modify the script to accommodate your requirements for `min_df`
    and the comparison of silhouette scores for different values of `min_df` using
    `KMeans` clustering. Here’s the updated script:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们修改脚本以适应您对`min_df`的要求，并使用`KMeans`聚类比较不同`min_df`值的轮廓得分。以下是更新的脚本：
- en: '[PRE24]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In this script, we’re looping through different values of `min_df` (5, 10,
    20, and 30) and performing the following steps for each value:'
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在此脚本中，我们正在遍历不同的`min_df`值（5, 10, 20和30），并为每个值执行以下步骤：
- en: Preprocess and vectorize the data using the `preprocess_and_vectorize_data`
    function with the specified `min_df` value.
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用指定的`min_df`值，通过`preprocess_and_vectorize_data`函数预处理和向量化数据。
- en: Perform K-means clustering with 10 clusters (you can adjust the number of clusters
    as needed).
  id: totrans-407
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用10个簇执行K-means聚类（您可以根据需要调整簇的数量）。
- en: Calculate the silhouette score for the clustering results and store it in the
    `silhouette_scores` list.
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算聚类结果的轮廓得分并将其存储在`silhouette_scores`列表中。
- en: Finally, we visualize the silhouette scores for different `min_df` values using
    a line plot. This will help you compare how different values of `min_df` impact
    the quality of the clustering results.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用折线图可视化不同`min_df`值的轮廓得分。这将帮助您比较不同`min_df`值如何影响聚类结果的质量。
- en: '![A graph with a line  Description automatically generated](img/B21232_16_15.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![带有线条描述的图形自动生成](img/B21232_16_15.png)'
- en: 'Figure 16.15: Silhouette score versus min_df'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.15：轮廓得分与min_df的关系
- en: '**[End of response]**'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Next, we will investigate the impact of the number of clusters on the quality
    of the clustering results. Let’s proceed with the next prompt.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将研究簇数量对聚类结果质量的影响。让我们进行下一个提示。
- en: '**[Prompt]**'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: I would also like to try different number of clusters `num_clusters = [3,4,
    5,6,7]` and visually compare silhouette score outputs for `min_df`, `num_clusters`
    combinations. Please add the script that generates a plot of cluster and silhouette
    score values for each of the values from `num_clusters`.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 我还想尝试不同的簇数量`num_clusters = [3,4, 5,6,7]`，并可视化`min_df`和`num_clusters`组合的轮廓得分输出。请添加生成每个`num_clusters`值簇和轮廓得分值的图标的脚本。
- en: '**[End of prompt]**'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Certainly! Here’s the updated script that generates a plot of silhouette scores
    for different combinations of `min_df` and `num_clusters`:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！以下是生成不同组合的`min_df`和`num_clusters`轮廓得分的更新的脚本：
- en: '[PRE25]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In this script, we use the `itertools.product` function to create combinations
    of `min_df` and `num_clusters` values. We calculate silhouette scores for each
    combination and store them in the `silhouette_scores` list. Then, we convert the
    silhouette scores to a DataFrame for easier visualization. The resulting plot
    shows how the silhouette score varies for different values of `num_clusters` while
    considering different `min_df` values.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个脚本中，我们使用`itertools.product`函数创建`min_df`和`num_clusters`值的组合。我们计算每个组合的轮廓分数并将它们存储在`silhouette_scores`列表中。然后，我们将轮廓分数转换为DataFrame以便于可视化。生成的图表显示了在考虑不同的`min_df`值时，轮廓分数如何随`num_clusters`的不同值而变化。
- en: '**[End of response]**'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: 'Here is the output of the script:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是脚本的输出：
- en: '![A graph showing the number of clusters  Description automatically generated](img/B21232_16_16.png)'
  id: totrans-423
  prefs: []
  type: TYPE_IMG
  zh: '![显示簇数量的图表，描述自动生成](img/B21232_16_16.png)'
- en: 'Figure 16.16: Silhouette score versus different min_df values'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.16：轮廓分数与不同的`min_df`值
- en: Let’s refine the plot to show the number of features corresponding to the `min_fd`
    parameter value.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们细化这个图表，以显示与`min_fd`参数值对应的特征数量。
- en: '**[Prompt]**'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: The plot is good. I would like a small change by adding the number of features
    for each of the `min_df` values to the `label=f'min_df = {min_df}`
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表很好。我想通过添加每个`min_df`值的特征数量来做一个小的更改，标签为`label=f'min_df = {min_df}'`
- en: '**[End of prompt]**'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束提示]**'
- en: '**[Prompt response]**'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: Certainly! You can add the number of features corresponding to each `min_df`
    value to the legend labels.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以！您可以将对应于每个`min_df`值的特征数量添加到图例标签中。
- en: 'Here’s the modified code with that change:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是经过修改的代码，包含那个更改：
- en: '[PRE26]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In this modified script, we calculate the number of features for each `min_df`
    value using the `shape[1]` attribute of the TF-IDF DataFrame. Then, we include
    this information in the legend labels for each line on the plot. This change will
    help you visualize the impact of both `min_df` and the number of features on the
    silhouette scores.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个修改后的脚本中，我们使用TF-IDF DataFrame的`shape[1]`属性计算每个`min_df`值的特征数量。然后，我们将这些信息包含在图表上每条线的图例标签中。这个更改将帮助您可视化`min_df`和特征数量对轮廓分数的影响。
- en: '![A graph showing the number of clusters  Description automatically generated](img/B21232_16_17.png)'
  id: totrans-434
  prefs: []
  type: TYPE_IMG
  zh: '![显示簇数量的图表，描述自动生成](img/B21232_16_17.png)'
- en: 'Figure 16.17: Silhouette score versus different min_df values with number of
    features'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.17：轮廓分数与不同的`min_df`值及特征数量
- en: '**[End of response]**'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Let’s proceed with the next prompt.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行下一个提示。
- en: '**[Prompt]**'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: I will use the value of `min_df_value = 20`, `num_clusters= [3, 4, 5, 6]`. I
    would like to know the number of products in each cluster. Can you add a plot
    `Products per Cluster` to the previous code to show number of products per cluster
    for `num_clusters_values` in [3, 4, 5, 6]
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用`min_df_value = 20`，`num_clusters= [3, 4, 5, 6]`的值。我想知道每个簇中的产品数量。你能在之前的代码中添加一个`Products
    per Cluster`的图表来显示`num_clusters_values`在[3, 4, 5, 6]范围内的产品数量吗？
- en: '**[End of prompt]**'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束提示]**'
- en: '**[Prompt response]**'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Certainly! You can add a plot to visualize the number of products in each cluster
    for different `num_clusters` values. Here’s the modified script with the requested
    changes:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 当然可以！您可以通过添加一个图表来可视化不同`num_clusters`值下每个簇中的产品数量。以下是经过修改的脚本，包含所需更改：
- en: '[PRE27]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: In this modified script, we calculate the number of products in each cluster
    using `pd.Series(cluster_labels).value_counts().sort_index()` and store the results
    along with silhouette scores in the `results` list. We then create a DataFrame
    from the results for easier visualization.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个修改后的脚本中，我们使用`pd.Series(cluster_labels).value_counts().sort_index()`计算每个簇中的产品数量，并将结果与轮廓分数一起存储在`results`列表中。然后，我们创建一个DataFrame从结果中以便于可视化。
- en: 'The script includes two plots side by side: the first plot displays the silhouette
    scores for different `num_clusters` values, and the second plot displays the number
    of products in each cluster for the same `num_clusters` values. This will help
    you understand the relationship between the number of clusters, silhouette scores,
    and the distribution of products among clusters.'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本包括两个并排的图表：第一个图表显示不同`num_clusters`值的轮廓分数，第二个图表显示相同`num_clusters`值下每个簇中的产品数量。这将帮助您理解簇数量、轮廓分数以及产品在簇中分布之间的关系。
- en: 'After running the script, you should see the following output:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 运行脚本后，你应该看到以下输出：
- en: '![A comparison of graphs with lines  Description automatically generated with
    medium confidence](img/B21232_16_18.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![带有线条的图表比较  有中等置信度的自动生成的描述](img/B21232_16_18.png)'
- en: 'Figure 16.18: Added plot of products per cluster'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.18：添加了每个簇的产品绘图
- en: '**[End of response]**'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束响应]**'
- en: We will now select the number of clusters and `min_df` value and proceed with
    the next prompt.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将选择簇的数量和`min_df`值，并继续下一个提示。
- en: Interpret clusters and visualize results
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释簇并可视化结果
- en: We’ll be looking into interpreting clusters and visualizing the results. What
    are the things we should bear in mind?
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将探讨解释簇和可视化结果。我们应该注意哪些事项？
- en: Interpreting cluster
  id: totrans-453
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释簇
- en: 'For cluster interpretation, we should consider things like:'
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 对于簇的解释，我们应该考虑以下因素：
- en: 'Cohesion: Ensure data points are similar to each other. You can find this out
    by examining terms that are the most representative in each cluster.'
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚合度：确保数据点彼此相似。你可以通过检查每个簇中最具代表性的术语来找出这一点。
- en: 'Separation: Clusters should be well separated ensuring they are distinct and
    meaningful.'
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分隔：簇应该很好地分隔，确保它们是独特且有意义的。
- en: 'Labels: Clusters should also have descriptive labels.'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：簇也应该有描述性的标签。
- en: 'Outliers: It’s important to identify outliers that don’t fit well in any cluster.'
  id: totrans-458
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值：识别不适合任何簇的异常值很重要。
- en: Visualizing clusters
  id: totrans-459
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化簇
- en: 'For visualizing clusters, here are some things to consider:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 对于可视化簇，以下是一些需要考虑的事项：
- en: 'Dimensionality reduction: You want to reduce the high-dimensional TF-IDF matrix
    to 2D or 3D to make it easier to visualize. For this, a technique like PCA comes
    in handy.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低：你希望将高维的TF-IDF矩阵降低到2D或3D，以便更容易可视化。为此，PCA等技术很有帮助。
- en: 'Scatter plots: Scatter plots are great for visualizing data points; a point
    can be a document, for example, and colors or shapes could represent different
    clusters.'
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 散点图：散点图非常适合可视化数据点；一个点可以是一个文档，例如，颜色或形状可以代表不同的簇。
- en: 'Word clouds: You could also use word clouds if you look at text, to see what
    the most common words used are.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词云：如果你查看文本，也可以使用词云来查看最常用的单词。
- en: 'Heat maps: Heat maps are another great way to visualize frequency and will
    help highlight the most important terms per cluster.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热图：热图是另一种很好的可视化频率的方法，将有助于突出每个簇中最重要的术语。
- en: 'Interactivity: Ensure you allow for exploration to help you drill down into
    specific data points by using, for example, Plotly.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交互性：确保你允许探索，例如使用Plotly，以便通过钻入特定数据点来深入了解。
- en: Next, let’s craft a prompt for visualization.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为可视化制作一个提示。
- en: '**[Prompt]**'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: I will use the value of `min_df_value = 20` and 5 clusters. I would like to
    use PCA with 3 components to visualize product clusters on a 3D graph. The colors
    of clusters should be mapped from the seaborn `color_palette`.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用`min_df_value = 20`和5个簇。我希望使用PCA的3个成分在3D图中可视化产品簇。簇的颜色应映射自seaborn的`color_palette`。
- en: '**[End of prompt]**'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束提示]**'
- en: '**[Prompt response]**'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Certainly! In order to visualize the product clusters in a 3D graph using PCA
    with 3 components and mapping cluster colors from a seaborn color palette, you
    can follow these steps:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！为了使用PCA的3个成分和从seaborn颜色调色板映射簇颜色来在3D图中可视化产品簇，你可以遵循以下步骤：
- en: Import necessary libraries.
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入必要的库。
- en: Preprocess the data and perform clustering.
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预处理数据并执行聚类。
- en: Apply PCA to reduce the dimensionality to 3 components.
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将主成分分析（PCA）应用于将维度降低到3个成分。
- en: Plot the clusters in a 3D scatter plot with seaborn color palette.
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用seaborn颜色调色板在3D散点图中绘制簇。
- en: 'Here’s the code to achieve this:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 这是实现此目的的代码：
- en: '[PRE28]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: '**[End of response]**'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '**[结束响应]**'
- en: This code will preprocess the data, perform clustering, apply PCA to reduce
    dimensionality to 3 components, and then plot the product clusters in a 3D scatter
    plot with cluster colors mapped from a seaborn color palette. Adjust the figure
    size, labels, and other plot parameters as needed to customize the visualization.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将预处理数据，执行聚类，将维度降低到3个成分，然后在3D散点图中绘制产品簇，簇颜色映射自seaborn颜色调色板。根据需要调整图形大小、标签和其他绘图参数以自定义可视化。
- en: '![A graph of clustering data  Description automatically generated](img/B21232_16_19.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![聚类数据的图表  自动生成的描述](img/B21232_16_19.png)'
- en: 'Figure 16.19: Product clusters visualized in 3D space'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.19：在3D空间中可视化的产品簇
- en: Creating a word cloud
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 创建词云
- en: Now, we will examine the most frequent words in each of the clusters. Let’s
    proceed with the next prompt.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将检查每个聚类的最频繁出现的词语。让我们继续下一个提示。
- en: '**[Prompt]**'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: I would like to visualize the most frequent words occurring in product descriptions
    assigned to clusters. I would like you to use the word cloud library and create
    a function that will output word clouds for each of the clusters.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 我想可视化分配给聚类的产品描述中最频繁出现的词语。我希望您使用词云库并创建一个函数，该函数将为每个聚类输出词云。
- en: '**[End of prompt]**'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应提示]**'
- en: 'Sure! To visualize the most frequent words occurring in product descriptions
    assigned to clusters, you can use the `WordCloud` library. Here’s how you can
    create a function that generates word clouds for each of the clusters:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！为了可视化分配给聚类的产品描述中最频繁出现的词语，您可以使用`WordCloud`库。以下是如何创建一个为每个聚类生成词云的函数：
- en: '[PRE29]'
  id: totrans-489
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: This code defines a function `generate_wordclouds` that generates word clouds
    for each cluster based on the product descriptions assigned to that cluster. Adjust
    the parameters of the `WordCloud` object (e.g., `width`, `height`, `background_color`,
    `max_words`) as needed for your visualization preferences.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码定义了一个名为`generate_wordclouds`的函数，该函数根据分配给该聚类的产品描述生成每个聚类的词云。根据您的可视化偏好调整`WordCloud`对象的参数（例如，`width`、`height`、`background_color`、`max_words`）。
- en: The function will plot the word clouds for each cluster.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 函数将绘制每个聚类的词云。
- en: '![A close-up of words  Description automatically generated](img/B21232_16_20.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![词语的特写  自动生成的描述](img/B21232_16_20.png)'
- en: 'Figure 16.20: Word cloud for the first cluster'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.20：第一个聚类的词云
- en: '![A word cloud with different colored words  Description automatically generated](img/B21232_16_21.png)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色的词云  自动生成的描述](img/B21232_16_21.png)'
- en: 'Figure 16.21: Word cloud for the second cluster'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.21：第二个聚类的词云
- en: '![A word cloud with different colored text  Description automatically generated](img/B21232_16_22.png)'
  id: totrans-496
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色的文本词云  自动生成的描述](img/B21232_16_22.png)'
- en: 'Figure 16.22: Word cloud for the third cluster'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.22：第三个聚类的词云
- en: '![A word cloud with different colored words  Description automatically generated](img/B21232_16_23.png)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![不同颜色的词云  自动生成的描述](img/B21232_16_23.png)'
- en: 'Figure 16.23: Word cloud for the fourth cluster'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.23：第四个聚类的词云
- en: '**[End of response]**'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Assigning categories to products and evaluating and refining
  id: totrans-501
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将类别分配给产品以及评估和细化
- en: Neither assigning categories to products nor evaluating and refining steps were
    in scope for this chapter to solve but let’s describe what they are to give you
    an idea of how you could explore those should you want to.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 将类别分配给产品或评估和细化步骤都不在本章的解决范围内，但让我们描述一下它们是什么，以便您了解如果您想探索这些内容应该如何进行。
- en: 'Our dataset contains invoices, and for us to categorize products, we would
    first need to understand the use cases we could possibly have. Those could be,
    for example:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集包含发票，为了对我们来说对产品进行分类，我们首先需要了解我们可能拥有的用例。例如，这些用例可能是：
- en: Organizing inventory
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组织库存
- en: Improving search functionality
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进搜索功能
- en: Enhancing the user experience
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升用户体验
- en: As you can see, this is very centered on product data, which means we would
    probably need a completely different dataset containing product titles, descriptions,
    and more. Let’s describe at a high level what our approach could be using ChatGPT.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这非常侧重于产品数据，这意味着我们可能需要一个包含产品标题、描述以及更多内容的完全不同的数据集。让我们使用ChatGPT在较高层次上描述我们的方法。
- en: '**[Prompt]**'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: I’m looking at an e-commerce dataset. What can you tell me about the process
    of assigning categories to products?
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 我正在查看一个电子商务数据集。你能告诉我关于将类别分配给产品的过程吗？
- en: '**[End of prompt]**'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应提示]**'
- en: 'Assigning categories to products in an e-commerce dataset is a crucial task
    for organizing inventory, improving search functionality, and enhancing the user
    experience. Here’s a step-by-step guide to help you through the process:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子商务数据集中将产品分配到类别是一个至关重要的任务，对于组织库存、改进搜索功能和提升用户体验至关重要。以下是一个逐步指南，帮助您完成这个过程：
- en: 'Steps to Assign Categories to Products:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 将产品分配到类别的步骤：
- en: '**Understand the Data**:'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**理解数据**：'
- en: '**Review Product Descriptions**: Examine the product titles, descriptions,
    and any other relevant attributes.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审查产品描述**：检查产品标题、描述以及任何其他相关属性。'
- en: '**Identify Existing Categories**: If there are existing categories, review
    them to understand the current classification system.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**识别现有类别**：如果有现有类别，审查它们以了解当前的分类系统。'
- en: '**Define Categories**:'
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: '**定义类别**：'
- en: '**Create a Category Hierarchy**: Develop a hierarchical structure of categories
    and subcategories that makes sense for your products.'
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创建类别层次结构**：为你的产品开发一个有意义的类别和子类别层次结构。'
- en: '**Ensure Completeness**: Make sure your categories cover all types of products
    in your dataset.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**确保完整性**：确保你的类别涵盖了数据集中所有类型的产品。'
- en: '**Feature Extraction**:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: '**特征提取**：'
- en: '**Text Features**: Extract relevant text features from product titles and descriptions
    using techniques like TF-IDF or word embeddings.'
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**文本特征**：使用TF-IDF或词嵌入等技术从产品标题和描述中提取相关文本特征。'
- en: '**Other Features**: Consider other attributes like price, brand, and specifications
    that might help in categorization.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**其他特征**：考虑其他可能有助于分类的属性，如价格、品牌和规格。'
- en: '**Manual Labeling**:'
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: '**手动标记**：'
- en: '**Sample Labeling**: Manually label a sample of products to create a training
    dataset. This helps in understanding the nuances of each category.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**样本标记**：手动标记产品样本以创建训练数据集。这有助于理解每个类别的细微差别。'
- en: '**Model Selection**:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型选择**：'
- en: '**Choose a Classification Model**: Select a machine learning model suitable
    for text classification, such as Naive Bayes, SVM, or deep learning models like
    LSTM or BERT.'
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**选择分类模型**：选择适合文本分类的机器学习模型，例如朴素贝叶斯、SVM或LSTM或BERT等深度学习模型。'
- en: '**Training the Model**:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '**训练模型**：'
- en: '**Train the Model**: Use the labeled data to train your classification model.'
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练模型**：使用标记数据来训练你的分类模型。'
- en: '**Evaluate Performance**: Evaluate the model’s performance using metrics like
    accuracy, precision, recall, and F1-score.'
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估性能**：使用准确率、精确率、召回率和F1分数等指标评估模型的性能。'
- en: '**Assign Categories**:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '**分配类别**：'
- en: '**Predict Categories**: Use the trained model to predict categories for the
    unlabeled products.'
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测类别**：使用训练好的模型预测未标记产品的类别。'
- en: '**Review Predictions**: Manually review a subset of the predictions to ensure
    accuracy.'
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**审查预测**：手动审查预测的一部分以确保准确性。'
- en: '**Iterate and Improve**:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '**迭代和改进**：'
- en: '**Refine Categories**: Based on the results, refine your category definitions
    and model.'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**细化类别**：根据结果，细化你的类别定义和模型。'
- en: '**Continuous Learning**: Continuously update the model with new data to improve
    its accuracy over time.'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续学习**：持续用新数据更新模型，以提高其随时间推移的准确性。'
- en: 'Example in Python:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 示例（Python）：
- en: 'Here’s a simplified example using a Naive Bayes classifier for text classification:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个使用朴素贝叶斯分类器进行文本分类的简化示例：
- en: '[PRE30]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: This code provides a basic framework for categorizing products based on their
    names. For a real-world application, you would need a larger dataset and more
    sophisticated preprocessing and modeling techniques.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码提供了一个基于产品名称对产品进行分类的基本框架。对于实际应用，你需要一个更大的数据集和更复杂的预处理及建模技术。
- en: If you have any specific questions or need further details, feel free to ask!
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有任何具体问题或需要更多信息，请随时提问！
- en: '**[End of response]**'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: We’ll leave it up to you to pursue this further. From what you’ve hopefully
    seen in this chapter already, start with a high-level prompt explaining what dataset
    you have (you have to find a dataset for the above problem) and what you aim to
    do. On the latter, you already have the steps, so you’re suggested to craft a
    prompt explaining what to do for each step and what code to run.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将把这个任务留给你去进一步探索。从你在这个章节中可能已经看到的内容开始，用一个高层次提示来解释你有什么数据集（你必须为上述问题找到一个数据集），以及你打算做什么。在后者上，你已经有了步骤，所以建议你编写一个提示来解释每个步骤应该做什么以及要运行什么代码。
- en: Evaluate and refine
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 评估和细化
- en: This is also out of scope, but let’s get you started on a ChatGPT prompt that
    you can then keep working on.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 这也超出了范围，但让我们帮你开始一个ChatGPT提示，然后你可以继续工作。
- en: '**[Prompt]**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示]**'
- en: Describe evaluate and refine step in the context of e-commerce data on invoices.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 在发票电子商务数据背景下描述评估和细化步骤。
- en: '**[End of prompt]**'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示结束]**'
- en: '**[Prompt response]**'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '**[提示响应]**'
- en: 'Sure! Let’s break down the evaluate and refine steps in the context of e-commerce
    data on invoices:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 当然！让我们在发票电子商务数据背景下分解评估和细化步骤：
- en: 'Evaluate Step:'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 评估步骤：
- en: 'Model Performance Metrics:'
  id: totrans-551
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型性能指标：
- en: '**Accuracy**: Measure how often the model correctly predicts the category of
    an invoice.'
  id: totrans-552
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**准确率**：衡量模型正确预测发票类别频率。'
- en: '**Precision and Recall**: Evaluate the precision (correct positive predictions)
    and recall (ability to find all relevant instances) for each category.'
  id: totrans-553
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**精确度和召回率**：评估每个类别的精确度（正确的正预测）和召回率（找到所有相关实例的能力）。'
- en: '**F1-Score**: Combine precision and recall into a single metric to balance
    both aspects.'
  id: totrans-554
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F1 分数**：将精确度和召回率结合成一个单一指标，以平衡这两个方面。'
- en: 'Confusion Matrix:'
  id: totrans-555
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混淆矩阵：
- en: '**Visualize Predictions**: Use a confusion matrix to see where the model is
    making correct and incorrect predictions. This helps identify specific categories
    where the model may be struggling.'
  id: totrans-556
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**可视化预测**：使用混淆矩阵来查看模型在何处做出正确和错误的预测。这有助于识别模型可能遇到困难的特定类别。'
- en: 'Cross-Validation:'
  id: totrans-557
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 交叉验证：
- en: '**Robust Evaluation**: Perform cross-validation to ensure the model’s performance
    is consistent across different subsets of the data.'
  id: totrans-558
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**稳健评估**：执行交叉验证以确保模型在不同数据子集上的性能一致。'
- en: 'Error Analysis:'
  id: totrans-559
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 错误分析：
- en: '**Analyze Misclassifications**: Examine the invoices that were misclassified
    to understand why the model made errors. Look for patterns or common issues.'
  id: totrans-560
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分析误分类**：检查被误分类的发票，以了解模型为何出错。寻找模式或常见问题。'
- en: 'Refine Step:'
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 精细步骤：
- en: 'Feature Engineering:'
  id: totrans-562
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征工程：
- en: '**Enhance Features**: Create new features or improve existing ones to provide
    more relevant information to the model. For example, extracting keywords from
    invoice descriptions or adding metadata like invoice date and amount.'
  id: totrans-563
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增强特征**：创建新特征或改进现有特征，为模型提供更多相关信息。例如，从发票描述中提取关键词或添加元数据，如发票日期和金额。'
- en: 'Hyperparameter Tuning:'
  id: totrans-564
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 超参数调整：
- en: '**Optimize Model Parameters**: Use techniques like grid search or random search
    to find the best hyperparameters for your model, improving its performance.'
  id: totrans-565
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**优化模型参数**：使用网格搜索或随机搜索等技术来找到模型的最佳超参数，从而提高其性能。'
- en: 'Model Selection:'
  id: totrans-566
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型选择：
- en: '**Try Different Models**: Experiment with different algorithms (e.g., decision
    trees, random forests, neural networks) to see if another model performs better.'
  id: totrans-567
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**尝试不同的模型**：实验不同的算法（例如，决策树、随机森林、神经网络），看看是否有其他模型表现更好。'
- en: 'Data Augmentation:'
  id: totrans-568
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据增强：
- en: '**Increase Data Variety**: Augment your dataset by generating synthetic data
    or using techniques like oversampling for underrepresented categories.'
  id: totrans-569
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**增加数据多样性**：通过生成合成数据或使用过采样等技术来增加数据集，以增强代表性不足的类别。'
- en: 'Regular Updates:'
  id: totrans-570
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定期更新：
- en: '**Continuous Learning**: Regularly update the model with new invoice data to
    keep it current and improve its accuracy over time.'
  id: totrans-571
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**持续学习**：定期用新的发票数据更新模型，以保持其最新状态并随着时间的推移提高其准确性。'
- en: 'Example in Python:'
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: Python 示例：
- en: 'Here’s a simplified example of evaluating and refining a model using cross-validation
    and hyperparameter tuning:'
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个使用交叉验证和超参数调整评估和改进模型的简化示例：
- en: '[PRE31]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This code provides a framework for evaluating and refining a model using cross-validation
    and hyperparameter tuning.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码提供了一个框架，用于使用交叉验证和超参数调整来评估和改进模型。
- en: If you have any specific questions or need further details, feel free to ask!
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有任何具体问题或需要更多信息，请随时提问！
- en: '**[End of response]**'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '**[响应结束]**'
- en: Reflection on prompts for this use case
  id: totrans-578
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对此用例提示的反思
- en: 'Just like with our first use case in this chapter, we followed a specific method
    of first setting high-level context of describing our role, the shape of the dataset,
    and what we were looking to do. Then, we followed the below process to first breaking
    down the problem in steps, getting code and how we could continue to improve and
    refine and finally visualize the results:'
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在本章的第一个用例中一样，我们首先设定了一个高级上下文，描述我们的角色、数据集的形状以及我们想要做什么。然后，我们遵循以下步骤来首先分解问题，编写代码，以及如何继续改进和细化，最后可视化结果：
- en: '**Step-by-step guidance:** The requests are structured as a series of step-by-step
    tasks, breaking down the larger problem into manageable components. This makes
    it easier to follow and implement the solution incrementally.'
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**逐步指导**：请求被结构化为一系列逐步任务，将更大的问题分解为可管理的组件。这使得跟踪和逐步实施解决方案变得更容易。'
- en: '**Specify inputs and requirements:** Provide clear and specific information
    about the dataset, its columns, and the requirements for preprocessing, clustering,
    and visualization. This helps ensure that the assistance received is tailored
    to the particular needs.'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**指定输入和要求**：提供关于数据集、其列以及预处理、聚类和可视化的清晰和具体信息。这有助于确保收到的帮助符合特定需求。'
- en: '**Request for code with comments:** Code snippets request to include comments
    to explain each step and clarify the purpose of the code. This helps in understanding
    the code and code validation and facilitates learning.'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**代码注释请求：** 代码片段请求包含注释以解释每一步并阐明代码的目的。这有助于理解代码和代码验证，并促进学习。'
- en: '**Iteration and refinement:** Iteration of prompts, asking for additional modifications
    and visualizations after the initial guidance. This iterative approach allows
    for refining and improving the solution progressively.'
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**迭代与完善：** 在初始指导后，通过请求额外的修改和可视化来迭代提示。这种迭代方法允许逐步完善和改进解决方案。'
- en: '**Visualization and interpretation:** Visualizing and interpreting the results
    allows focusing on deriving meaningful insights from the data.'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**可视化与解释：** 可视化和解释结果有助于集中精力从数据中提取有意义的见解。'
- en: Assignment
  id: totrans-585
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 作业
- en: 'In the previous section, we used traditional embedding with TF-IDF to transform
    text data into numerical representations, which can then be used for various **natural
    language processing** (**NLP**) tasks such as clustering. Let’s now try and improve
    the clustering results by using a more advanced embedding technique. We will use
    the Hugging Face Transformers library to get pre-trained embeddings for our product
    descriptions:'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了传统的TF-IDF嵌入将文本数据转换为数值表示，这些表示可以用于各种**自然语言处理**（**NLP**）任务，如聚类。现在，让我们尝试使用更先进的嵌入技术来改进聚类结果。我们将使用Hugging
    Face Transformers库为我们产品的描述获取预训练嵌入：
- en: Ask ChatGPT to explain Hugging Face Transformers’ advantages over TF-IDF vectorization
    for clustering use cases.
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请问ChatGPT如何解释Hugging Face Transformers在聚类用例中相对于TF-IDF向量化技术的优势。
- en: Use ChatGPT to generate and create product clusters using Hugging Face Transformers
    embeddings.
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用ChatGPT生成并创建产品聚类，利用Hugging Face Transformers嵌入。
- en: Compare the results with the previous clustering results using TF-IDF vectorization.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将结果与使用TF-IDF向量化的先前聚类结果进行比较。
- en: Solution
  id: totrans-590
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 解答
- en: 'See the solution in the repository: https://github.com/PacktPublishing/AI-Assisted-Software-Development-with-GitHub-Copilot-and-ChatGPT'
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 在仓库中查看解决方案：https://github.com/PacktPublishing/AI-Assisted-Software-Development-with-GitHub-Copilot-and-ChatGPT
- en: Summary
  id: totrans-592
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter focused on clustering and how it could be used to group your data
    into separate areas. Creating these areas made it easier to understand our data
    points. Through visualization like heat maps, word clouds, and more, you were
    given the insight that data benefits from being shown in different ways. You also
    saw how the clustering process helped identify outliers, that is, data that vastly
    differs and can’t easily be assigned to any one cluster. For the ChatGPT and prompting
    part, you saw how setting a high-level context describing the dataset helped generate
    a suitable set of steps you could follow from top to bottom. The same high-level
    context also helped ChatGPT recommend a clustering algorithm.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 本章重点介绍了聚类及其如何用于将数据分组到不同的区域。创建这些区域使得理解我们的数据点变得更容易。通过热图、词云等多种可视化方式，您了解到数据通过以不同的方式展示而受益。您还看到了聚类过程如何帮助识别异常值，即与任何单个聚类有很大差异且难以归类的数据。对于ChatGPT和提示部分，您看到了如何设置一个描述数据集的高级上下文，这有助于生成您可以从上到下遵循的合适步骤集。相同的高级上下文也有助于ChatGPT推荐聚类算法。
- en: Join our community on Discord
  id: totrans-594
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord
- en: 'Join our community’s Discord space for discussions with the author and other
    readers:'
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的社区Discord空间，与作者和其他读者进行讨论：
- en: '[https://packt.link/aicode](https://packt.link/aicode)'
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/aicode](https://packt.link/aicode)'
- en: '![](img/QR_Code510410532445718281.png)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code510410532445718281.png)'
