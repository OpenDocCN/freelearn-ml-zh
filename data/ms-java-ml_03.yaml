- en: Chapter 3. Unsupervised Machine Learning Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章 无监督机器学习技术
- en: In the last chapter, we focused on supervised learning, that is, learning from
    a training dataset that was labeled. In the real world, obtaining data with labels
    is often difficult. In many domains, it is virtually impossible to label data
    either due to the cost of labeling or difficulty in labeling due to the sheer
    volume or velocity at which data is generated. In those situations, unsupervised
    learning, in its various forms, offers the right approaches to explore, visualize,
    and perform descriptive and predictive modeling. In many applications, unsupervised
    learning is often coupled with supervised learning as a first step to isolate
    interesting data elements for labeling.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们专注于监督学习，即从标记的训练数据集中学习。在现实世界中，获取带有标签的数据往往很困难。在许多领域，由于数据生成的规模或速度，即使由于标记的成本或难度，实际上也不可能对数据进行标记。在这些情况下，无监督学习，以各种形式，提供了探索、可视化和执行描述性和预测建模的正确方法。在许多应用中，无监督学习通常与监督学习结合作为第一步，以隔离有趣的数据元素进行标记。
- en: In this chapter, we will focus on various methodologies, techniques, and algorithms
    that are practical and well-suited for unsupervised learning. We begin by noting
    the issues that are common between supervised and unsupervised learning when it
    comes to handling data and transformations. We will then briefly introduce the
    particular challenges faced in unsupervised learning owing to the lack of "ground
    truth" and the nature of learning under those conditions.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将关注各种适用于无监督学习的方法、技术和算法。我们首先指出在处理数据和转换方面，监督学习和无监督学习共有的问题。然后，我们将简要介绍由于缺乏“真实情况”和在这些条件下的学习性质，无监督学习面临的特定挑战。
- en: We will then discuss the techniques of feature analysis and dimensionality reduction
    applied to unlabeled datasets. This is followed by an introduction to the broad
    spectrum of clustering methods and discussions on the various algorithms in practical
    use, just as we did with supervised learning in [Chapter 2](ch02.html "Chapter 2. Practical
    Approach to Real-World Supervised Learning"), *Practical Approach to Real-World
    Supervised Learning*, showing how each algorithm works, when to use it, and its
    advantages and limitations. We will conclude the section on clustering by presenting
    the different cluster evaluation techniques.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将讨论应用于未标记数据集的特征分析和降维技术。接下来是聚类方法的广泛介绍和关于实际应用中各种算法的讨论，就像我们在[第2章](ch02.html
    "第2章 实际应用中的监督学习")中做的那样，*实际应用中的监督学习*，展示每个算法的工作原理、何时使用它以及其优势和局限性。我们将通过介绍不同的聚类评估技术来结束对聚类的讨论。
- en: Following the treatment of clustering, we will approach the subject of outlier
    detection. We will contrast various techniques and algorithms that illustrate
    what makes some objects outliers—also called anomalies—within a given dataset.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理聚类之后，我们将探讨异常检测的主题。我们将对比各种技术和算法，说明为什么某些对象在给定数据集中是异常的——也称为异常值。
- en: The chapter will conclude with clustering and outlier detection experiments,
    conducted with a real-world dataset and an analysis of the results obtained. In
    this case study, we will be using ELKI and SMILE Java libraries for the machine
    learning tasks and will present code and results from the experiments. We hope
    that this will provide the reader with a sense of the power and ease of use of
    these tools.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将以聚类和异常检测实验结束，这些实验使用真实世界的数据集进行，并对获得的结果进行分析。在本案例研究中，我们将使用ELKI和SMILE Java库进行机器学习任务，并展示实验的代码和结果。我们希望这将为读者提供对这些工具强大功能和易用性的感觉。
- en: Issues in common with supervised learning
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与监督学习共有的问题
- en: 'Many of the issues that we discussed related to supervised learning are also
    common with unsupervised learning. Some of them are listed here:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的许多与监督学习相关的问题也与无监督学习相关。其中一些列在这里：
- en: '**Types of features handled by the algorithm**: Most clustering and outlier
    algorithms need numeric representation to work effectively. Transforming categorical
    or ordinal data has to be done carefully'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**算法处理的特征类型**：大多数聚类和异常算法需要数值表示才能有效工作。对分类或有序数据进行转换必须谨慎进行'
- en: '**Curse of dimensionality**: Having a large number of features results in sparse
    spaces and affects the performance of clustering algorithms. Some option must
    be chosen to suitably reduce dimensionality—either feature selection where only
    a subset of the most relevant features are retained, or feature extraction, which
    transforms the feature space into a new set of principal variables of a lower
    dimensional space'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**维度灾难**：具有大量特征会导致稀疏空间，影响聚类算法的性能。必须选择一些选项来适当地降低维度——要么是特征选择，只保留最相关特征的一个子集，要么是特征提取，将特征空间转换成低维空间的新一组主变量'
- en: '**Scalability in memory and training time**: Many unsupervised learning algorithms
    cannot scale up to more than a few thousands of instances either due to memory
    or training time constraints'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内存和训练时间上的可扩展性**：许多无监督学习算法由于内存或训练时间限制，无法扩展到几千个实例以上'
- en: '**Outliers and noise in data**: Many algorithms are affected by noise in the
    features, the presence of anomalous data, or missing values. They need to be transformed
    and handled appropriately'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据中的异常值和噪声**：许多算法受到特征中的噪声、异常数据的存在或缺失值的影响。它们需要被适当地转换和处理'
- en: Issues specific to unsupervised learning
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习的特定问题
- en: 'The following are some issues that pertain to unsupervised learning techniques:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些与无监督学习技术相关的问题：
- en: '**Parameter setting**: Deciding on number of features, usefulness of features,
    number of clusters, shapes of clusters, and so on, pose enormous challenges to
    certain unsupervised methods'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数设置**：确定特征数量、特征的有用性、聚类数量、聚类的形状等问题，对某些无监督方法构成了巨大的挑战'
- en: '**Evaluation methods**: Since unsupervised learning methods are ill-posed due
    to lack of ground-truth, evaluation of algorithms becomes very subjective.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评估方法**：由于缺乏真实标签，无监督学习方法是不良设定的，因此算法的评估变得非常主观。'
- en: '**Hard or soft labeling**: Many unsupervised learning problems require giving
    labels to the data in an exclusive or probabilistic manner. This poses a problem
    for many algorithms'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**硬标签或软标签**：许多无监督学习问题需要以排他性或概率方式对数据进行标记。这对许多算法来说是一个问题'
- en: '**Interpretability of results and models**: Unlike supervised learning, the
    lack of ground truth and the nature of some algorithms make interpreting the results
    from both model and labeling even more difficult'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**结果和模型的可解释性**：与监督学习不同，缺乏真实标签和一些算法的性质使得从模型和标签中解释结果变得更加困难'
- en: Feature analysis and dimensionality reduction
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征分析和降维
- en: Among the first tools to master are the different feature analysis and dimensionality
    reduction techniques. As in supervised learning, the need for reducing dimensionality
    arises from numerous reasons similar to those discussed earlier for feature selection
    and reduction.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 需要掌握的第一批工具是不同的特征分析和降维技术。与监督学习一样，降维的需求源于与之前讨论的特征选择和降维类似的多重原因。
- en: A smaller number of discriminating dimensions makes visualization of data and
    clusters much easier. In many applications, unsupervised dimensionality reduction
    techniques are used for compression, which can then be used for transmission or
    storage of data. This is particularly useful when the larger data has an overhead.
    Moreover, applying dimensionality reduction techniques can improve the scalability
    in terms of memory and computation speeds of many algorithms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 较少的判别维度使得数据的可视化和聚类变得容易得多。在许多应用中，无监督降维技术被用于压缩，这可以用于数据的传输或存储。当大数据有额外开销时，这尤其有用。此外，应用降维技术可以提高许多算法在内存和计算速度方面的可扩展性。
- en: Notation
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 符号表示
- en: 'We will use similar notation to what was used in the chapter on supervised
    learning. The examples are in *d* dimensions and are represented as vector:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用与监督学习章节中使用的类似符号。示例在 *d* 维度上，表示为向量：
- en: '**x** = *(x**[1]**,x**[2]**,…x**[d]* *)**^T*'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**x** = *(x**[1]**,x**[2]**,…x**[d]* *)**^T*'
- en: 'The entire dataset containing *n* examples can be represented as an observation
    matrix:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 包含 *n* 个示例的整个数据集可以表示为一个观察矩阵：
- en: '![Notation](img/B05137_03_005.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![符号表示](img/B05137_03_005.jpg)'
- en: The idea of dimensionality reduction is to find k ≤ *d* features either by transformation
    of the input features, projecting or combining them such that the lower dimension
    *k* captures or preserves interesting properties of the original dataset.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的思想是找到 k ≤ *d* 个特征，通过输入特征的变换、投影或组合，使得较低维度的 *k* 能够捕获或保留原始数据集的有趣特性。
- en: Linear methods
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性方法
- en: Linear dimensionality methods are some of the oldest statistical techniques
    to reduce features or transform the data into lower dimensions, preserving interesting
    discriminating properties.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 线性降维方法是一些最古老的统计技术，用于减少特征或将数据转换到较低维度，同时保留有趣的判别特性。
- en: 'Mathematically, with linear methods we are performing a transformation, such
    that a new data element is created using a linear transformation of the original
    data element:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们使用线性方法执行一个变换，即通过原始数据元素的线性变换来创建一个新的数据元素：
- en: '![Linear methods](img/B05137_03_008.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![线性方法](img/B05137_03_008.jpg)'
- en: '**s = Wx**'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '**s = Wx**'
- en: Here, **W**[k × d] is the linear transformation matrix. The variables **s**
    are also referred to as latent or hidden variables.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**W**[k × d] 是线性变换矩阵。变量 **s** 也被称为潜在或隐藏变量。
- en: In this topic, we will discuss the two most practical and often-used methodologies.
    We will list some variants of these techniques so that the reader can use the
    tools to experiment with them. The main assumption here—which often forms the
    limitation—is the linear relationships between the transformations.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个主题中，我们将讨论两种最实用且经常使用的方 法。我们将列出这些技术的变体，以便读者可以使用这些工具进行实验。这里的主要假设——通常形成限制——是变换之间的线性关系。
- en: Principal component analysis (PCA)
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）
- en: PCA is a widely-used technique for dimensionality reduction(*References* [1]).
    The original coordinate system is rotated to a new coordinate system that exploits
    the directions of maximum variance in the data, resulting in uncorrelated variables
    in a lower-dimensional subspace that were correlated in the original feature space.
    PCA is sensitive to the scaling of the features.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是降维中广泛使用的技术（*参考文献* [1]）。原始坐标系被旋转到一个新的坐标系，该坐标系利用了数据中的最大方差方向，从而在低维子空间中产生了在原始特征空间中相关的变量。PCA对特征的缩放敏感。
- en: Inputs and outputs
  id: totrans-36
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: PCA is generally effective on numeric datasets. Many tools provide the categorical-to-continuous
    transformations for the nominal features, but this affects the performance. The
    number of principal components, or *k*, is also an input provided by the user.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PCA通常对数值数据集有效。许多工具提供了对名义特征的类别到连续的转换，但这会影响性能。主成分的数量，或*k*，也是用户提供的输入。
- en: How does it work?
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: PCA, in its most basic form, tries to find projections of data onto new axes,
    which are known as **principal components**. Principal components are projections
    that capture maximum variance directions from the original space. In simple words,
    PCA finds the first principal component through rotation of the original axes
    of the data in the direction of maximum variance. The technique finds the next
    principal component by again determining the next best axis, orthogonal to the
    first axis, by seeking the second highest variance and so on until most variances
    are captured. Generally, most tools give either a choice of number of principal
    components or the option to keep finding components until some percentage, for
    example, 99%, of variance in the original dataset is captured.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PCA在其最基本的形式中，试图将数据投影到新的轴上，这些轴被称为**主成分**。主成分是从原始空间捕获最大方差方向的投影。简单来说，PCA通过将数据的原始轴旋转到最大方差的方向来找到第一个主成分。该技术通过再次确定下一个最佳轴（与第一个轴正交），寻求第二个最高方差，以此类推，直到捕获大多数方差。通常，大多数工具提供选择主成分数量的选项，或者选择继续寻找组件，直到捕获原始数据集中方差的一定百分比，例如99%。
- en: Mathematically, the objective of finding maximum variance can be written as
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，寻找最大方差的目标可以写成
- en: '![How does it work?](img/B05137_03_013.jpg)![How does it work?](img/B05137_03_014.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_013.jpg)![如何工作？](img/B05137_03_014.jpg)'
- en: '*λ* **v** = **Cv** is the eigendecomposition'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '*λ* **v** = **Cv** 是特征分解'
- en: 'This is equivalent to:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当于：
- en: '![How does it work?](img/B05137_03_017.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_017.jpg)'
- en: Here, **W** is the principal components and **S** is the new transformation
    of the input data. Generally, eigenvalue decomposition or singular value decomposition
    is used in the computation part.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，**W** 是主成分，**S** 是输入数据的新变换。通常，在计算部分使用特征值分解或奇异值分解。
- en: '![How does it work?](img/B05137_03_021.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_03_021.jpg)'
- en: 'Figure 1: Principal Component Analysis'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1：主成分分析
- en: Advantages and limitations
  id: totrans-48
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: One of the advantages of PCA is that it is optimal in that it minimizes the
    reconstruction error of the data.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 的一个优点是它在最小化数据重建误差方面是最佳的。
- en: PCA assumes normal distribution.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 假设正态分布。
- en: The computation of variance-covariance matrix can become intensive for large
    datasets with high-dimensions. Alternatively, **Singular Value Decomposition**
    (**SVD**) can be used as it works iteratively and there is no need for an explicit
    covariance matrix.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于高维的大型数据集，计算协方差矩阵可能会变得非常密集。作为替代，可以使用**奇异值分解**（**SVD**），因为它是迭代的，并且不需要显式的协方差矩阵。
- en: PCA has issues when there is noise in the data.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据中存在噪声时，PCA 会存在问题。
- en: PCA fails when the data lies in the complex manifold, a topic that we will discuss
    in the non-linear dimensionality reduction section.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当数据位于复杂的流形上时，PCA 会失败，这是一个我们将在非线性降维部分讨论的话题。
- en: PCA assumes a correlation between the features and in the absence of those correlations,
    it is unable to do any transformations; instead, it simply ranks them.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 假设特征之间存在相关性；在没有这些相关性的情况下，它无法进行任何转换；相反，它只是对它们进行排序。
- en: By transforming the original feature space into a new set of variables, PCA
    causes a loss in interpretability of the data.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过将原始特征空间转换成一组新的变量，PCA 导致数据可解释性降低。
- en: There are many other variants of PCA that are popular and overcome some of the
    biases and assumptions of PCA.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多其他流行的 PCA 变体，它们克服了 PCA 的一些偏差和假设。
- en: '**Independent Component Analysis** (**ICA**) assumes that there are mixtures
    of non-Gaussians from the source and, using the generative technique, tries to
    find the decompositions of original data in the smaller mixtures or components
    (*References* [2]). The key difference between PCA and ICA is that PCA creates
    components that are uncorrelated, while ICA creates components that are independent.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**独立成分分析**（**ICA**）假设存在来自源的混合的非高斯分布，并使用生成技术尝试在较小的混合或组件中找到原始数据的分解（*参考文献* [2]）。PCA
    和 ICA 之间的关键区别在于，PCA 创建的是不相关的组件，而 ICA 创建的是独立的组件。'
- en: 'Mathematically, it assumes ![Advantages and limitations](img/B05137_03_022.jpg)
    as a mixture of independent sources ∈ ![Advantages and limitations](img/B05137_03_024.jpg),
    such that each data element *y* = [*y* *¹* *,y* *²* *,….y* *^k* ]*^T* and independence
    is implied by ![Advantages and limitations](img/B05137_03_026.jpg):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，它假设 ![优点和局限性](img/B05137_03_022.jpg) 是独立源 ∈ ![优点和局限性](img/B05137_03_024.jpg)
    的混合，使得每个数据元素 *y* = [*y* *¹* *,y* *²* *,….y* *^k* ]*^T*，独立性由 ![优点和局限性](img/B05137_03_026.jpg)
    表示：
- en: '**Probabilistic Principal Component Analysis** (**PPCA**) is based on finding
    the components using mixture models and maximum likelihood formulations using
    **Expectation Maximization** (**EM**) (*References* [3]). It overcomes the issues
    of missing data and outlier impacts that PCA faces.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**概率主成分分析**（**PPCA**）基于使用混合模型和最大似然公式通过**期望最大化**（**EM**）来找到组件。它克服了 PCA 面临的缺失数据和异常值影响的问题。'
- en: Random projections (RP)
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机投影（RP）
- en: When data is separable by a large margin—even if it is high-dimensional data—one
    can randomly project the data down to a low-dimensional space without impacting
    separability and achieve good generalization with a relatively small amount of
    data. Random Projections use this technique and the details are described here
    (*References* [4]).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据可以通过大间隔分离——即使它是高维数据——一个人可以将数据随机投影到低维空间，而不会影响分离性，并且使用相对较少的数据实现良好的泛化。随机投影使用这种技术，具体细节在此描述（*参考文献*
    [4]）。
- en: Inputs and outputs
  id: totrans-62
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Random projections work with both numeric and categorical features, but categorical
    features are transformed into binary. Outputs are lower dimensional representations
    of the input data elements. The number of dimensions to project, *k*, is part
    of user-defined input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 随机投影可以与数值和分类特征一起工作，但分类特征会被转换为二进制。输出是输入数据元素的低维表示。要投影的维度数 *k* 是用户定义输入的一部分。
- en: How does it work?
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'This technique uses random projection matrices to project the input data into
    a lower dimensional space. The original data ![How does it work?](img/B05137_03_027.jpg)
    is transformed to the lower dimension space ![How does it work?](img/B05137_03_028.jpg)
    where *k << p* using:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种技术使用随机投影矩阵将输入数据投影到低维空间。原始数据 ![如何工作？](img/B05137_03_027.jpg) 被转换到低维空间 ![如何工作？](img/B05137_03_028.jpg)，其中
    *k << p*，使用以下方法：
- en: '![How does it work?](img/B05137_03_030.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_030.jpg)'
- en: Here columns in the *k* x *d* matrix **R** are i.i.d zero mean normal variables
    and are scaled to unit length. There are variants of how the random matrix **R**
    is constructed using probabilistic sampling. Computational complexity of RP is
    *O(knd)*, which scales much better than PCA. In many practical datasets, it has
    been shown that RP gives results comparable to PCA and can scale to large dimensions
    and datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*k* x *d* 矩阵 **R** 中的列是独立同分布的零均值正态变量，并缩放到单位长度。**R** 随机矩阵的构建有几种使用概率抽样的变体。RP的计算复杂度为
    *O(knd)*，这比PCA的缩放效果要好得多。在许多实际数据集中，已经证明RP的结果与PCA相当，并且可以扩展到大型维度和数据集。
- en: Advantages and limitations
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: It scales to very large values of dataset size and dimensionalities. In text
    and image learning problems, with large dimensions, this technique has been successfully
    used as the preprocessing technique.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以扩展到非常大的数据集大小和维度值。在文本和图像学习问题中，对于大型维度，该技术已被成功用作预处理技术。
- en: Sometimes a large information loss can occur while using RP.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用RP时，有时可能会发生较大的信息损失。
- en: Multidimensional Scaling (MDS)
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多维尺度分析（MDS）
- en: There are many forms of MDS—classical, metric, and non-metric. The main idea
    of MDS is to preserve the pairwise similarity/distance values. It generally involves
    transforming the high dimensional data into two or three dimensions (*References*
    [5]).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: MDS有许多形式——经典、度量和非度量。MDS的主要思想是保留成对相似度/距离值。它通常涉及将高维数据转换到两个或三个维度（*参考文献* [5]）。
- en: Inputs and outputs
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: MDS can work with both numeric and categorical data based on the user-selected
    distance function. The number of dimensions to transform to, *k*, is a user-defined
    input.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: MDS可以根据用户选择的距离函数与数值和分类数据一起工作。要转换到的维数 *k* 是用户定义的输入。
- en: How does it work?
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: Given *n* data elements, an *n* x *n* affinity or distance matrix is computed.
    There are choices of using distances such as Euclidean, Mahalanobis, or similarity
    concepts such as cosine similarity, Jaccard coefficients, and so on. MDS in its
    very basic form tries to find a mapping of the distance matrix in a lower dimensional
    space where the Euclidean distance between the transformed points is similar to
    the affinity matrix.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *n* 个数据元素，计算一个 *n* x *n* 的亲和度或距离矩阵。有使用距离的选择，例如欧几里得距离、马氏距离，或者使用相似度概念，如余弦相似度、Jaccard系数等。MDS在其最基本的形式中试图在低维空间中找到距离矩阵的映射，其中变换点之间的欧几里得距离与亲和度矩阵相似。
- en: 'Mathematically:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲：
- en: '![How does it work?](img/B05137_03_036.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_036.jpg)'
- en: Here ![How does it work?](img/B05137_03_037.jpg) input space and ![How does
    it work?](img/B05137_03_038.jpg) mapped space.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![如何工作？](img/B05137_03_037.jpg) 输入空间和 ![如何工作？](img/B05137_03_038.jpg) 映射空间。
- en: If the input affinity space is transformed using kernels then the MDS becomes
    a non-linear method for dimensionality reduction. Classical MDS is equivalent
    to PCA when the distances between the points in input space is Euclidean distance.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用核将输入亲和度空间进行转换，则MDS成为降维的非线性方法。当输入空间中点之间的距离是欧几里得距离时，经典MDS与PCA等价。
- en: Advantages and limitations
  id: totrans-81
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The key disadvantage is the subjective choice of the lower dimension needed
    to interpret the high dimensional data, normally restricted to two or three for
    humans. Some data may not map effectively in this lower dimensional space.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关键缺点是需要主观选择所需的低维来解释高维数据，通常限制在两个或三个维度内，这对于人类来说是有局限的。一些数据可能无法有效地映射到这个低维空间。
- en: The advantage is you can perform linear and non-linear mapping to the lowest
    dimensions using the framework.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优点是您可以使用该框架将线性和非线性映射到最低维度。
- en: Nonlinear methods
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 非线性方法
- en: In general, nonlinear dimensionality reduction involves either performing nonlinear
    transformations to the computations in linear methods such as KPCA or finding
    nonlinear relationships in the lower dimension as in manifold learning. In some
    domains and datasets, the structure of the data in lower dimensions is nonlinear—and
    that is where techniques such as KPCA are effective—while in some domains the
    data does not unfold in lower dimensions and you need manifold learning.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，非线性降维涉及对线性方法（如KPCA）中的计算执行非线性变换，或者在低维中找到非线性关系，如流形学习。在某些领域和数据集中，低维数据结构是非线性的——这就是KPCA等技术的有效之处——而在某些领域，数据在低维中不会展开，你需要流形学习。
- en: Kernel Principal Component Analysis (KPCA)
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 核主成分分析（KPCA）
- en: Kernel PCA uses the Kernel trick described in [Chapter 2](ch02.html "Chapter 2. Practical
    Approach to Real-World Supervised Learning"), *Practical Approach to Real-World
    Supervised Learning*, with the PCA algorithm for transforming the data in a high-dimensional
    space to find effective mapping (*References* [6]).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 核PCA使用[第2章](ch02.html "第2章. 实际应用中的监督学习")中描述的核技巧，*实际应用中的监督学习*，与PCA算法一起在多维空间中转换数据，以找到有效的映射（*参考文献*
    [6]）。
- en: Inputs and outputs
  id: totrans-88
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Similar to PCA with addition of choice of kernel and kernel parameters. For
    example, if **Radial Basis Function** (**RBF**) or Gaussian Kernel is chosen,
    then the kernel, along with the gamma parameter, becomes user-selected values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 与PCA类似，增加了核和核参数的选择。例如，如果选择**径向基函数**（**RBF**）或高斯核，则核以及伽马参数成为用户选择的值。
- en: How does it work?
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: In the same way as **Support Vector Machines** (**SVM**) was discussed in the
    previous chapter, KPCA transforms the input space to high dimensional feature
    space using the "kernel trick". The entire PCA machinery of finding maximum variance
    is then carried out in the transformed space.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一章中讨论的**支持向量机**（**SVM**）一样，KPCA使用“核技巧”将输入空间转换为高维特征空间。然后在转换空间中执行寻找最大方差的所有PCA机制。
- en: 'As in PCA:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如PCA：
- en: '![How does it work?](img/B05137_03_014.jpg)![How does it work?](img/B05137_03_040.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_014.jpg)![如何工作？](img/B05137_03_040.jpg)'
- en: Instead of linear covariance matrix, a nonlinear transformation is applied to
    the input space using kernel methods by constructing the *N* x *N* matrix, in
    place of doing the actual transformations using *ϕ* *(x)*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通过构建*N* x *N*矩阵，而不是使用*ϕ* *(x)*进行实际变换，使用核方法对输入空间应用非线性变换，而不是线性协方差矩阵。
- en: '*k(x,y) = ((* *ϕ* *(x),* *ϕ* *(y)) =* *ϕ* *(x)* *^T* *ϕ* *(y)*'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '*k(x,y) = ((* *ϕ* *(x),* *ϕ* *(y)) =* *ϕ* *(x)* *^T* *ϕ* *(y)*'
- en: 'Since the kernel transformation doesn''t actually transform the features into
    explicit feature space, the principal components found can be interpreted as projections
    of data onto the components. In the following figure, a binary nonlinear dataset,
    generated using the scikit-learn example on circles (*References* [27]), demonstrates
    the linear separation after KPCA using the RBF kernel and returning to almost
    similar input space by the inverse transform:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于核变换实际上并没有将特征转换到显式的特征空间，因此找到的主成分可以解释为数据在组件上的投影。在以下图中，一个使用scikit-learn示例生成的二进制非线性数据集（*参考文献*
    [27]）展示了使用RBF核进行KPCA后的线性分离，并通过逆变换返回几乎相同的输入空间：
- en: '![How does it work?](img/B05137_03_046.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_046.jpg)'
- en: 'Figure 2: KPCA on Circle Dataset and Inverse Transform.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图2：圆数据集上的KPCA和逆变换。
- en: Advantages and limitations
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: KPCA overcomes the nonlinear mapping presented by PCA.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KPCA克服了PCA所呈现的非线性映射。
- en: KPCA has similar issues with outlier, noisy, and missing values to standard
    PCA. There are robust methods and variations to overcome this.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KPCA与标准PCA在异常值、噪声和缺失值方面存在类似问题。有稳健的方法和变体可以克服这些问题。
- en: KPCA has scalability issues in space due to an increase in the kernel matrix,
    which can become a bottleneck in large datasets with high dimensions. SVD can
    be used in these situations, as an alternative.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于核矩阵的增加，KPCA在空间上存在可扩展性问题，这可能在具有高维的大型数据集中成为瓶颈。在这些情况下，可以使用SVD作为替代。
- en: Manifold learning
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 流形学习
- en: When high dimensional data is embedded in lower dimensions that are nonlinear,
    but have complex structure, manifold learning is very effective.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当高维数据嵌入到非线性但具有复杂结构的低维空间时，流形学习非常有效。
- en: Inputs and outputs
  id: totrans-105
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Manifold learning algorithms require two user-provided parameters: *k*, representing
    the number of neighbors for the initial search, and *n*, the number of manifold
    coordinates.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 流形学习算法需要两个由用户提供的参数：*k*，代表初始搜索的邻居数量，以及*n*，流形坐标的数量。
- en: How does it work?
  id: totrans-107
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: 'As seen in the following figure, the three-dimensional S-Curve, plotted using
    the scikit-learn utility (*References* [27]), is represented in 2D PCA and in
    2D manifold using LLE. It is interesting to observe how the blue, green, and red
    dots are mixed up in the PCA representation while the manifold learning representation
    using LLE cleanly separates the colors. It can also be observed that the rank
    ordering of Euclidean distances is not maintained in the manifold representation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图中所示，使用scikit-learn工具（*参考文献* [27]）绘制的三维S曲线，在2D PCA和2D流形中使用LLE表示。观察蓝色、绿色和红色点在PCA表示中混合在一起，而使用LLE进行流形学习表示时，颜色被干净地分离。还可以观察到，欧几里得距离的秩排序在流形表示中不保持：
- en: '![How does it work?](img/B05137_03_049.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_049.jpg)'
- en: 'Figure 3: Data representation after PCA and manifold learning'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图3：PCA和流形学习后的数据表示
- en: To preserve the structure, the geodesic distance is preserved instead of the
    Euclidean distance. The general approach is to build a graph structure such as
    an adjacency matrix, and then compute geodesic distance using different assumptions.
    In the Isomap Algorithm, the global pairwise distances are preserved (*References*
    [7]). In the **Local Linear Embedding** (**LLE**) Algorithm, the mapping is done
    to take care of local neighborhood, that is, nearby points map to nearby points
    in the transformation (*References* [9]). Laplacian Eigenmaps is similar to LLE,
    except it tries to maintain the "locality" instead of "local linearity" in LLE
    by using graph Laplacian (*References* [8]).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保留结构，保留的是测地距离而不是欧几里得距离。一般的方法是构建一个图结构，如邻接矩阵，然后根据不同的假设计算测地距离。在Isomap算法中，全局成对距离被保留（*参考文献*
    [7]）。在**局部线性嵌入**（**LLE**）算法中，映射是为了处理局部邻域，即附近的点在变换中映射到附近的点（*参考文献* [9]）。拉普拉斯特征映射与LLE类似，但它通过使用图拉普拉斯来尝试保持LLE中的“局部性”而不是“局部线性”，（*参考文献*
    [8]）。
- en: Advantages and limitations
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Isomap is non-parametric; it preserves the global structure, and has no local
    optimum, but is hampered by speed.
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isomap是非参数的；它保留了全局结构，没有局部最优解，但速度受限。
- en: LLE and Laplacian Eigenmaps are non-parametric, have no local optima, are fast,
    but don't preserve global structure.
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LLE和拉普拉斯特征映射是非参数的，没有局部最优解，速度快，但不保留全局结构。
- en: Clustering
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Clustering algorithms can be categorized in different ways based on the techniques,
    the outputs, the process, and other considerations. In this topic, we will present
    some of the most widely used clustering algorithms.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法可以根据技术、输出、过程和其他考虑因素以不同的方式进行分类。在本主题中，我们将介绍一些最广泛使用的聚类算法。
- en: Clustering algorithms
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类算法
- en: There is a rich set of clustering techniques in use today for a wide variety
    of applications. This section presents some of them, explaining how they work,
    what kind of data they can be used with, and what their advantages and drawbacks
    are. These include algorithms that are prototype-based, density-based, probabilistic
    partition-based, hierarchy-based, graph-theory-based, and those based on neural
    networks.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有丰富的聚类技术用于广泛的多种应用。本节介绍其中的一些，解释它们的工作原理，它们可以用于哪些类型的数据，以及它们的优缺点。这些包括基于原型、基于密度、基于概率分区、基于层次、基于图理论和基于神经网络的算法。
- en: k-Means
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-Means
- en: k-means is a centroid- or prototype-based iterative algorithm that employs partitioning
    and relocation methods (*References* [10]). k-means finds clusters of spherical
    shape depending on the distance metric used, as in the case of Euclidean distance.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: k-means是一种基于质心或原型迭代的算法，采用分区和重新定位方法（*参考文献* [10]）。k-means根据使用的距离度量找到球形形状的簇，例如在欧几里得距离的情况下。
- en: Inputs and outputs
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: k-means can handle mostly numeric features. Many tools provide categorical to
    numeric transformations, but having a large number of categoricals in the computation
    can lead to non-optimal clusters. User-defined *k*, the number of clusters to
    be found, and the distance metric to use for computing closeness are two basic
    inputs. k-means generates clusters, association of data to each cluster, and centroids
    of clusters as the output.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: k-means可以处理大多数数值特征。许多工具提供类别到数值的转换，但计算中存在大量类别可能会导致非最优的聚类。用户定义的*k*，即要找到的聚类数量，以及用于计算接近度的距离度量是两个基本输入。k-means生成聚类，将数据关联到每个聚类，以及聚类的质心作为输出。
- en: How does it work?
  id: totrans-123
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The most common variant known as Lloyd's algorithm initializes *k* centroids
    for the given dataset by picking data elements randomly from the set. It assigns
    each data element to the centroid it is closest to, using some distance metric
    such as Euclidean distance. It then computes the mean of the data points for each
    cluster to form the new centroid and the process is repeated until either the
    maximum number of iterations is reached or there is no change in the centroids.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的变体称为Lloyd算法，它通过从集合中随机选择数据元素来初始化给定数据集的*k*个质心。它使用某些距离度量（如欧几里得距离）将每个数据元素分配到最近的质心。然后，它计算每个聚类的数据点的平均值以形成新的质心，这个过程重复进行，直到达到最大迭代次数或质心没有变化。
- en: 'Mathematically, each step of the clustering can be seen as an optimization
    step where the equation to optimize is given by:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，聚类的每一步都可以看作是一个优化步骤，其中要优化的方程由以下给出：
- en: '![How does it work?](img/B05137_03_053.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_053.jpg)'
- en: Here, ci is all points belong to cluster *i*. The problem of minimizing is classified
    as NP-hard and hence k-Means has a tendency to get stuck in local optimum.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ci是属于聚类*i*的所有点。最小化问题被归类为NP-hard，因此k-Means有陷入局部最优的倾向。
- en: Advantages and limitations
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The choice of the number of clusters, *k*, is difficult to pick, but normally
    search techniques such as varying *k* for different values and measuring metrics
    such as sum of square errors can be used to find a good threshold. For smaller
    datasets, hierarchical k-Means can be tried.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择聚类数量*k*是困难的，但通常可以使用搜索技术，如为不同的值改变*k*并测量如平方误差和等指标来找到良好的阈值。对于较小的数据集，可以尝试层次k-means。
- en: k-means can converge faster than most algorithms for smaller values of *k* and
    can find effective global clusters.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较小的*k*值，k-means的收敛速度可以比大多数算法快，并且可以找到有效的全局聚类。
- en: k-means convergence can be affected by initialization of the centroids and hence
    there are many variants to perform random restarts with different seeds and so
    on.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means的收敛可能受到质心初始化的影响，因此有许多变体执行具有不同种子的随机重启等。
- en: k-means can perform badly when there are outliers and noisy data points. Using
    robust techniques such as medians instead of means, k-Medoids, overcomes this
    to a certain extent.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当存在异常值和噪声数据点时，k-means的表现可能会很糟糕。使用稳健技术（如中位数而不是平均值）、k-Medoids在一定程度上克服了这一点。
- en: k-means does not find effective clusters when they are of arbitrary shapes or
    have different densities.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当聚类形状任意或具有不同的密度时，k-means无法找到有效的聚类。
- en: DBSCAN
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: DBSCAN
- en: Density-based spatial clustering of applications with noise (DBSCAN) is a density-based
    partitioning algorithm. It separates dense region in the space from sparse regions
    (*References* [14]).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的空间聚类应用噪声（DBSCAN）是一种基于密度的划分算法。它将空间中的密集区域与稀疏区域分开（*参考文献* [14]）。
- en: Inputs and outputs
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Only numeric features are used in DBSCAN. The user-defined parameters are *MinPts*
    and the neighborhood factor given by *ϵ*.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN仅使用数值特征。用户定义的参数是*MinPts*和由*ϵ*给出的邻域因子。
- en: How does it work?
  id: totrans-138
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: The algorithm first finds the ϵ-neighborhood of every point *p*, given by ![How
    does it work?](img/B05137_03_058.jpg). A *high density* region is identified as
    a region where the number of points in a ϵ-neighborhood is greater than or equal
    to the given *MinPts*; the point such a ϵ-neighborhood is defined around is called
    a *core points*. Points within the ϵ-neighborhood of a *core point* are said to
    be *directly reachable*. All *core points* that can in effect be reached by hopping
    from one directly reachable core point to another point *directly reachable* from
    the second point, and so on, are considered to be in the same cluster. Further,
    any point that has fewer than *MinPts* in its *ϵ*-neighborhood, but is directly
    reachable from a core point, belongs to the same cluster as the core point. These
    points at the edge of a cluster are called *border points*. A *noise point* is
    any point that is neither a core point nor a border point.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 算法首先找到每个点*p*的ϵ邻域，由![如何工作？](img/B05137_03_058.jpg)给出。一个**高密度**区域被定义为在ϵ邻域中点的数量大于或等于给定*MinPts*的区域；围绕这样一个ϵ邻域定义的点被称为**核心点**。位于**核心点**的ϵ邻域内的点被认为是**直接可达**的。所有可以通过从一个直接可达的核心点到另一个直接可达的第二点的直接可达点跳跃，以此类推，实际上可以到达的核心点被认为是属于同一个簇。此外，任何在其*ϵ*-邻域中少于*MinPts*的点，但可以从核心点直接到达的点，属于与核心点相同的簇。这些位于簇边缘的点被称为**边界点**。任何既不是核心点也不是边界点的点被称为**噪声点**。
- en: Advantages and limitations
  id: totrans-140
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The DBSCAN algorithm does not require the number of clusters to be specified
    and can find it automatically from the data.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN算法不需要指定簇的数量，可以从数据中自动找到它。
- en: DBSCAN can find clusters of various shapes and sizes.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN可以找到各种形状和大小的簇。
- en: DBSCAN has in-built robustness to noise and can find outliers from the datasets.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN具有内置的鲁棒性以应对噪声，并可以从数据集中找到异常值。
- en: DBSCAN is not completely deterministic in its identification of the points and
    its categorization into border or core depends on the order of data processed.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DBSCAN在识别点和将其分类为边界或核心时并不完全确定，这取决于数据处理顺序。
- en: Distance metrics selected such as Euclidean distance can often affect performance
    due to the curse of dimensionality.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择如欧几里得距离的距离度量通常会影响性能，这是由于维度的诅咒。
- en: When there are clusters with large variations in the densities, the static choice
    of *{MinPts,* *ϵ**}* can pose a big limitation.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当存在密度变化大的簇时，*{MinPts,* *ϵ**}*的静态选择可能是一个很大的限制。
- en: Mean shift
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 均值漂移
- en: Mean shift is a very effective clustering algorithm in many image, video, and
    motion detection based datasets (*References* [11]).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移是许多基于图像、视频和运动检测数据集的有效聚类算法（*参考文献* [11]）。
- en: Inputs and outputs
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Only numeric features are accepted as data input in the mean shift algorithm.
    The choice of kernel and the bandwidth of the kernel are user-driven choices that
    affect the performance. Mean shift generates modes of data points and clusters
    data around the modes.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在均值漂移算法中，只接受数值特征作为数据输入。核的选择和核带宽是用户驱动的选择，这些选择会影响性能。均值漂移生成数据点的模式并在模式周围聚类数据。
- en: How does it work?
  id: totrans-151
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: Mean shift is based on the statistical concept of **kernel density estimation**
    (**KDE**), which is a probabilistic method to estimate the underlying data distribution
    from the sample.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 均值漂移基于统计概念**核密度估计**（**KDE**），这是一种概率方法，用于从样本中估计潜在的数据分布。
- en: 'A kernel density estimate for kernel *K* (**x**) of given bandwidth *h* is
    given by:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定带宽*h*的核*K*（**x**）的核密度估计由以下公式给出：
- en: '![How does it work?](img/B05137_03_063.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_063.jpg)'
- en: 'For *n* points with dimensionality *d*. The mean shift algorithm works by moving
    each data point in the direction of local increasing density. To estimate this
    direction, gradient is applied to the KDE and the gradient takes the form of:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有*d*维度的*n*个点。均值漂移算法通过将每个数据点移动到局部密度增加的方向来工作。为了估计这个方向，将梯度应用于核密度估计，梯度具有以下形式：
- en: '![How does it work?](img/B05137_03_065.jpg)![How does it work?](img/B05137_03_066.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_065.jpg)![如何工作？](img/B05137_03_066.jpg)'
- en: Here g(**x**)= –K'(**x**) is the derivative of the kernel. The vector, m(**x**),
    is called the mean shift vector and it is used to move the points in the direction
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这里g(**x**)= –K'(**x**)是核的导数。向量，m(**x**)，被称为均值漂移向量，它用于将点移动到局部密度增加的方向。
- en: '**x**^((t+1)) = **x**^t + m(**x**)'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**x**^((t+1)) = **x**^t + m(**x**)'
- en: Also, it is guaranteed to converge when the gradient of the density function
    is zero. Points that end up in a similar location are marked as clusters belonging
    to the same region.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当密度函数的梯度为零时，它保证收敛。最终位于相似位置的点被标记为属于同一区域的簇。
- en: Advantages and limitations
  id: totrans-160
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Mean shift is non-parametric and makes no underlying assumption on the data
    distribution.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值漂移是非参数的，不对数据分布做出任何假设。
- en: It can find non-complex clusters of varying shapes and sizes.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以找到形状和大小各异的非复杂簇。
- en: There is no need to explicitly give the number of clusters; the choice of the
    bandwidth parameter, which is used in estimation, implicitly controls the clusters.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有必要显式给出簇的数量；带宽参数的选择，该参数用于估计，隐式地控制簇。
- en: Mean shift has no local optima for a given bandwidth parameter and hence it
    is deterministic.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值漂移对于给定的带宽参数没有局部最优解，因此它是确定性的。
- en: Mean shift is robust to outliers and noisy points because of KDE.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于核密度估计（KDE），均值漂移对异常值和噪声点具有鲁棒性。
- en: The mean shift algorithm is computationally slow and does not scale well with
    large datasets.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 均值漂移算法计算缓慢，并且在大数据集上扩展性不好。
- en: Bandwidth selection should be done judiciously; otherwise it can result in merged
    modes, or the appearance of extra, shallow modes.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带宽选择应谨慎进行；否则可能会导致模式合并，或者出现额外的浅层模式。
- en: Expectation maximization (EM) or Gaussian mixture modeling (GMM)
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 期望最大化（EM）或高斯混合模型（GMM）
- en: GMM or EM is a probabilistic partition-based method that partitions data into
    *k* clusters using probability distribution-based techniques (*References* [13]).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: GMM或EM是一种基于概率划分的方法，它使用基于概率分布的技术将数据划分为 *k* 个聚类（*参考文献* [13]）。
- en: Input and output
  id: totrans-170
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Only numeric features are allowed in EM/GMM. The model parameter is the number
    of mixture components, given by *k*.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 只允许在EM/GMM中使用数值特征。模型参数是混合成分的数量，由 *k* 给定。
- en: How does it work?
  id: totrans-172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'GMM is a generative method that assumes that there are *k* Gaussian components,
    each Gaussian component has a mean *µ*[i] and covariance Ʃ[i]. The following expression
    represents the probability of the dataset given the *k* Gaussian components:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: GMM是一种生成方法，它假设存在 *k* 个高斯成分，每个高斯成分有一个均值 *µ*[i] 和协方差 Ʃ[i]。以下表达式表示给定 *k* 个高斯成分的数据集的概率：
- en: '![How does it work?](img/B05137_03_072.jpg)![How does it work?](img/B05137_03_073.jpg)![How
    does it work?](img/B05137_03_074.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_072.jpg)![如何工作？](img/B05137_03_073.jpg)![如何工作？](img/B05137_03_074.jpg)'
- en: The two-step task of finding the means {**µ**[1], **µ**[2], …**µ**[k]} for each
    of the *k* Gaussian components such that the data points assigned to each maximizes
    the probability of that component is done using the **Expectation Maximization**
    (**EM**) process.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个高斯成分，找到均值 {**µ**[1], **µ**[2], …**µ**[k]} 的两步任务，使得分配给每个数据点的概率最大化，这是通过**期望最大化**（**EM**）过程完成的。
- en: 'The iterative process can be defined into an E-step, that computes the *expected*
    cluster for all data points for the cluster, in an iteration *i*:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代过程可以定义为E步，在迭代 *i* 中计算每个数据点的**期望**簇：
- en: '![How does it work?](img/B05137_03_077.jpg)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_077.jpg)'
- en: 'The M-step maximizes to compute *µ*t+1 given the data points belonging to the
    cluster:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: M步最大化计算给定属于簇的数据点的 *µ*t+1：
- en: '![How does it work?](img/B05137_03_079.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_079.jpg)'
- en: The EM process can result in GMM convergence into local optimum.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: EM过程可能导致GMM收敛到局部最优。
- en: Advantages and limitations
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Works very well with any features; for categorical data, discrete probability
    is calculated, while for numeric a continuous probability function is estimated.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与任何特征都配合得很好；对于分类数据，计算离散概率，而对于数值数据，则估计连续概率函数。
- en: It has computational scalability problems. It can result in local optimum.
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有计算可扩展性问题。它可能导致局部最优。
- en: The value of *k* Gaussians has to be given *apriori*, similar to k-Means.
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*k* 个高斯成分的值必须**先验地**给出，类似于k-Means。'
- en: Hierarchical clustering
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Hierarchical clustering is a connectivity-based method of clustering that is
    widely used to analyze and explore the data more than it is used as a clustering
    technique (*References* [12]). The idea is to iteratively build binary trees either
    from top or bottom, such that similar points are grouped together. Each level
    of the tree provides interesting summarization of the data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种基于连接性的聚类方法，广泛用于分析和探索数据，而不仅仅用作聚类技术（*参考文献* [12]）。其思路是迭代地构建二叉树，从顶部或底部开始，使得相似点聚集在一起。树的每一层都提供了数据的有趣总结。
- en: Input and output
  id: totrans-187
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Hierarchical clustering generally works on similarity-based transformations
    and so both categorical and continuous data are accepted. Hierarchical clustering
    only needs the similarity or distance metric to compute similarity and does not
    need the number of clusters like in k-means or GMM.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类通常基于基于相似性的转换，因此接受分类数据和连续数据。层次聚类只需要相似度或距离度量来计算相似度，不需要像k-means或GMM那样需要聚类数量。
- en: How does it work?
  id: totrans-189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'There are many variants of hierarchical clustering, but we will discuss agglomerative
    clustering. Agglomerative clustering works by first putting all the data elements
    in their own groups. It then iteratively merges the groups based on the similarity
    metric used until there is a single group. Each level of the tree or groupings
    provides unique segmentation of the data and it is up to the analyst to choose
    the right level that fits the problem domain. Agglomerative clustering is normally
    visualized using a dendrogram plot, which shows merging of data points at similarity.
    The popular choices of similarity methods used are:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类有许多变体，但我们将讨论聚合聚类。聚合聚类首先将所有数据元素放入它们自己的组中。然后，它根据使用的相似度度量迭代合并组，直到只剩下一个组。树的每一层或分组提供了数据的独特分割，分析师需要选择适合问题域的正确层。聚合聚类通常使用树状图进行可视化，该图显示了在相似性处的数据点合并。常用的相似度方法选择包括：
- en: '**Single linkage**: Similarity is the minimum distance between the groups of
    points:![How does it work?](img/B05137_03_080.jpg)'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接**：相似性是点群之间的最小距离：![如何工作？](img/B05137_03_080.jpg)'
- en: '**Complete linkage**: Similarity is the maximum distance between the groups
    of points:![How does it work?](img/B05137_03_081.jpg)'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全链接**：相似性是点群之间的最大距离：![如何工作？](img/B05137_03_081.jpg)'
- en: '**Average linkage**: Average similarity between the groups of points:![How
    does it work?](img/B05137_03_082.jpg)'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链接**：点群之间的平均相似性：![如何工作？](img/B05137_03_082.jpg)'
- en: Advantages and limitations
  id: totrans-194
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Hierarchical clustering imposes a hierarchical structure on the data even when
    there may not be such a structure present.
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使数据中可能没有这样的结构，层次聚类也会在数据上施加层次结构。
- en: The choice of similarity metrics can result in a vastly different set of merges
    and dendrogram plots, so it has a large dependency on user input.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似度度量的选择可能导致合并和树状图的大幅不同，因此它对用户输入有很大的依赖性。
- en: Hierarchical clustering suffers from scalability with increased data points.
    Based on the distance metrics used, it can be sensitive to noise and outliers.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类在数据点增加时容易受到可扩展性的影响。根据使用的距离度量，它可能对噪声和异常值敏感。
- en: Self-organizing maps (SOM)
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自组织映射（SOM）
- en: SOM is a neural network based method that can be viewed as dimensionality reduction,
    manifold learning, or clustering technique (*References* [17]). Neurobiological
    studies show that our brains map different functions to different areas, known
    as topographic maps, which form the basis of this technique.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: SOM是一种基于神经网络的算法，可以被视为降维、流形学习或聚类技术（*参考文献* [17]）。神经生物学研究表明，我们的大脑将不同的功能映射到不同的区域，称为拓扑图，这是该技术的基础。
- en: Inputs and outputs
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Only numeric features are used in SOM. Model parameters consists of distance
    function, (generally Euclidean distance is used) and the lattice parameters in
    terms of width and height or number of cells in the lattice.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在SOM中仅使用数值特征。模型参数包括距离函数（通常使用欧几里得距离）以及以宽度和高度或晶格中的单元格数量表示的晶格参数。
- en: How does it work?
  id: totrans-202
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: SOM, also known as Kohonen networks, can be thought of as a two-layer neural
    network where each output layer is a two-dimensional lattice, arranged in rows
    and columns and each neuron is fully connected to the input layer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: SOM，也称为Kohonen网络，可以被视为一个双层神经网络，其中每个输出层是一个二维晶格，按行和列排列，每个神经元都与输入层完全连接。
- en: 'Like neural networks, the weights are initially generated using random values.
    The process has three distinct training phases:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 与神经网络类似，权重最初使用随机值生成。该过程有三个不同的训练阶段：
- en: '**Competitive phase**: Neurons in this phase compete based on the discriminant
    values, generally based on distance between neuron weight and input vector; such
    that the minimal distance between the two decides which neuron the input gets
    assigned to. Using Euclidean distance, the distance between an input *x*i and
    neuron in the lattice position *(j, i)* is given by *w*[ji]:![How does it work?](img/B05137_03_086.jpg)'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**竞争阶段**：在这个阶段，神经元根据判别值进行竞争，通常基于神经元权重与输入向量的距离；这样，两个神经元之间的最小距离决定了输入被分配给哪个神经元。使用欧几里得距离，输入
    *x*i 与晶格位置 *(j, i)* 的神经元之间的距离由 *w*[ji] 给出：![如何工作？](img/B05137_03_086.jpg)'
- en: '**Cooperation phase**: In this phase, the winning neurons find the best spatial
    location in the topological neighborhood. The topological neighborhood for the
    winning neuron *I*(**x**) for a given neuron *(j, i)*, at a distance *S*[ij],
    neighborhood of size σ, is defined by:![How does it work?](img/B05137_03_091.jpg)'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合作阶段**：在这个阶段，获胜神经元在其拓扑邻域中找到最佳空间位置。对于给定神经元 *(j, i)*，获胜神经元 *I*(**x**) 的拓扑邻域，距离
    *S*[ij]，邻域大小为 σ，定义为：![如何工作？](img/B05137_03_091.jpg)'
- en: 'The neighborhood size is defined in the way that it decreases with time using
    some well-known decay functions such as an exponential, function defined as follows:'
  id: totrans-207
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 邻域大小以某种已知衰减函数（如指数函数）随时间减少的方式定义：
- en: '![How does it work?](img/B05137_03_092.jpg)'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_092.jpg)'
- en: '**Adaptive phase**: In this phase, the weights of the winning neuron and its
    neighborhood neurons are updated. The update to weights is generally done using:![How
    does it work?](img/B05137_03_093.jpg)'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自适应阶段**：在这个阶段，获胜神经元及其邻域神经元的权重被更新。权重的更新通常使用以下方式完成：![如何工作？](img/B05137_03_093.jpg)'
- en: Here, the learning rate *n(t)* is again defined as exponential decay like the
    neighborhood size.
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，学习率 *n(t)* 再次定义为与邻域大小类似的指数衰减。
- en: SOM Visualization using Unified Distance Matrix (U-Matrix) creates a single
    metric of average distance between the weights of the neuron and its neighbors,
    which then can be visualized in different color intensities. This helps to identify
    *similar* neurons in the neighborhood.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 使用统一距离矩阵（U-Matrix）的 SOM 可视化创建了一个衡量神经元权重与其邻居之间平均距离的单个度量，然后可以通过不同的颜色强度进行可视化。这有助于识别邻域中的*相似*神经元。
- en: Advantages and limitations
  id: totrans-212
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The biggest advantage of SOM is that it is easy to understand and clustering
    of the data in two dimensions with U-matrix visualization enables understanding
    the patterns very effectively.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SOM 的最大优点是它易于理解，并且使用 U-matrix 可视化对数据进行二维聚类可以非常有效地理解模式。
- en: Choice of similarity/distance function makes vast difference in clusters and
    must be carefully chosen by the user.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似性/距离函数的选择对聚类有巨大影响，必须由用户仔细选择。
- en: SOM's computational complexity makes it impossible to use on datasets greater
    than few thousands in size.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SOM 的计算复杂度使得它无法在大于几千个数据集上使用。
- en: Spectral clustering
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谱聚类
- en: Spectral clustering is a partition-based clustering technique using graph theory
    as its basis (*References* [15]). It converts the dataset into a connected graph
    and does graph partitioning to find the clusters. This is a popular method in
    image processing, motion detection, and some unstructured data-based domains.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类是一种基于图理论的分区聚类技术（*参考文献* [15]）。它将数据集转换为连通图，并进行图分区以找到聚类。这是图像处理、运动检测和一些基于非结构化数据领域的流行方法。
- en: Inputs and outputs
  id: totrans-218
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Only numeric features are used in spectral clustering. Model parameters such
    as the choice of kernel, the kernel parameters, the number of eigenvalues to select,
    and partitioning algorithms such as k-Means must be correctly defined for optimum
    performance.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 谱聚类仅使用数值特征。模型参数，如核的选择、核参数、要选择的特征值数量以及分区算法（如 k-Means）必须正确定义以实现最佳性能。
- en: How does it work?
  id: totrans-220
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The following steps describe how the technique is used in practice:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤描述了该技术在实践中的应用：
- en: Given the data points, an affinity (or adjacency) matrix is computed using a
    smooth kernel function such as the Gaussian kernel:![How does it work?](img/B05137_03_095.jpg)For
    the points that are closer, ![How does it work?](img/B05137_03_096.jpg) and for
    points further away, ![How does it work?](img/B05137_03_097.jpg)
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定数据点，使用高斯核等平滑核函数计算亲和度（或邻接）矩阵：![如何工作？](img/B05137_03_095.jpg)对于较近的点，![如何工作？](img/B05137_03_096.jpg)而对于较远的点，![如何工作？](img/B05137_03_097.jpg)
- en: The next step is to compute the graph Laplacian matrix using various methods
    of normalizations. All Laplacian matrix methods use the diagonal degree matrix
    *D*, which measures degree at each node in the graph:![How does it work?](img/B05137_03_099.jpg)
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下一步是使用各种归一化方法计算图拉普拉斯矩阵。所有拉普拉斯矩阵方法都使用对角度矩阵 *D*，它测量图中每个节点的度：![如何工作？](img/B05137_03_099.jpg)
- en: A simple Laplacian matrix is *L = D (degree matrix) – A(affinity matrix)*.
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 简单的拉普拉斯矩阵是 *L = D (度矩阵) – A(亲和度矩阵)*。
- en: Compute the first *k* eigenvalues from the eigenvalue problem or the generalized
    eigenvalue problem.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特征值问题或广义特征值问题中计算前 *k* 个特征值。
- en: Use a partition algorithm such as k-Means to further separate clusters in the
    k-dimensional subspace.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分区算法，如 k-Means，在 k 维子空间中进一步分离聚类。
- en: Advantages and limitations
  id: totrans-227
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Spectral clustering works very well when the cluster shape or size is irregular
    and non-convex. Spectral clustering has too many parameter choices and tuning
    to get good results is quite an involved task.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当聚类形状或大小不规则且非凸时，谱聚类工作得非常好。谱聚类有太多的参数选择和调整，以获得良好的结果是一项相当复杂的任务。
- en: Spectral clustering has been shown, theoretically, to be more stable in the
    presence of noisy data. Spectral clustering has good performance when the clusters
    are not well separated.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理论上已经证明，在存在噪声数据的情况下，谱聚类具有更好的稳定性。当聚类没有很好地分离时，谱聚类表现良好。
- en: Affinity propagation
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 亲和传播
- en: Affinity propagation can be viewed as an extension of K-medoids method for its
    similarity with picking exemplars from the data (*References* [16]). Affinity
    propagation uses graphs with distance or the similarity matrix and picks all examples
    in the training data as exemplars. Iterative message passing as *affinities* between
    data points automatically detects clusters, the exemplars, and even the number
    of clusters.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 亲和传播可以被视为 K-medoids 方法的扩展，因为它与从数据中挑选示例相似（*参考文献* [16]）。亲和传播使用距离或相似度矩阵的图，并选择训练数据中的所有示例作为示例。作为数据点之间
    *亲和度* 的迭代消息传递自动检测聚类、示例，甚至聚类数量。
- en: Inputs and outputs
  id: totrans-232
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Typically, other than maximum number of iterations, which is common to most
    algorithms, no input parameters are required.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，除了最大迭代次数，这是大多数算法的共同点，不需要其他输入参数。
- en: How does it work?
  id: totrans-234
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'Two kinds of messages are exchanged between the data points that we will explain
    first:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先解释数据点之间交换的两种类型的信息：
- en: 'Responsibility *r(i,k)*: This is a message from the data point to the candidate
    exemplar. This gives a metric of how well the exemplar is suited for that data
    point compared to other exemplars. The rules for updating the responsibility are
    as follows: ![How does it work?](img/B05137_03_102.jpg) where *s(i, k)* = similarity
    between two data points *i* and *k*.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 责任 *r(i,k)*：这是从数据点到候选示例的消息。这提供了一个度量，说明与其它示例相比，该示例对该数据点的适应性如何。更新责任规则的规则如下：![如何工作？](img/B05137_03_102.jpg)其中
    *s(i, k)* = 数据点 *i* 和 *k* 之间的相似度。
- en: '*a(i, k)* = availability of exemplar *k* for *i*.'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*a(i, k)* = 示例 *k* 对 *i* 的可用性。'
- en: 'Availability *a(i,k)*: This is a message from the candidate exemplar to a data
    point. This gives a metric indicating how good of a support the exemplar can be
    to the data point, considering other data points in the calculations. This can
    be viewed as soft cluster assignment. The rule for updating the availability is
    as follows:![How does it work?](img/B05137_03_109.jpg)![How does it work?](img/B05137_03_110.jpg)![How
    does it work?](img/B05137_03_111.jpg)'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用性 *a(i,k)*：这是从候选示例到数据点的消息。这提供了一个度量，表示在考虑其他数据点的情况下，示例对数据点的支持有多好。这可以被视为软聚类分配。更新可用性的规则如下：![如何工作？](img/B05137_03_109.jpg)![如何工作？](img/B05137_03_110.jpg)![如何工作？](img/B05137_03_111.jpg)
- en: 'Figure 4: Message types used in Affinity Propagation'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 4：亲和传播中使用的消息类型
- en: 'The algorithm can be summed up as follows:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 算法可以总结如下：
- en: Initialize ![How does it work?](img/B05137_03_216.jpg)
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 ![如何工作？](img/B05137_03_216.jpg)
- en: For all increments *i* to *n*:![How does it work?](img/B05137_03_217.jpg)![How
    does it work?](img/B05137_03_218.jpg)![How does it work?](img/B05137_03_219.jpg)
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有增量*i*到*n*：![如何工作？](img/B05137_03_217.jpg)![如何工作？](img/B05137_03_218.jpg)![如何工作？](img/B05137_03_219.jpg)
- en: End.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束。
- en: For all **x**[i] such that *(r(i,i) + a(i,i) > 0)*
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于所有满足*(r(i,i) + a(i,i) > 0)*的**x**[i]
- en: '**x**[i] is exemplar.'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**x**[i]是示例。'
- en: All non-exemplars **x**[j] are assigned to the closest exemplar using the similarity
    measure *s(i, j)*.
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有非示例**x**[j]都使用相似度度量*s(i, j)*分配到最近的示例。
- en: End.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结束。
- en: Advantages and limitations
  id: totrans-248
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Affinity propagation is a deterministic algorithm. Both k-means or K-medoids
    are sensitive to the selection of initial points, which is overcome by considering
    every point as an exemplar.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相似性传播是一个确定性算法。k-means或K-medoids对初始点的选择很敏感，这通过将每个点视为示例来克服。
- en: The number of clusters doesn't have to be specified and is automatically determined
    through the process.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类的数量不必指定，并且通过过程自动确定。
- en: 'It works in non-metric spaces and doesn''t require distances/similarity to
    even have constraining properties such as triangle inequality or symmetry. This
    makes the algorithm usable on a wide variety of datasets with categorical and
    text data and so on:'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它适用于非度量空间，并且不需要距离/相似度来具有约束性质，例如三角不等式或对称性。这使得算法可以在具有分类和文本数据等多种数据集上使用：
- en: The algorithm can be parallelized easily due to its update methods and it has
    fast training time.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其更新方法，该算法可以轻松并行化，并且具有快速的训练时间。
- en: Clustering validation and evaluation
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类验证和评估
- en: 'Clustering validation and evaluation is one of the most important mechanisms
    to determine the usefulness of the algorithms (*References* [18]). These topics
    can be broadly classified into two categories:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类验证和评估是确定算法有用性的最重要的机制之一（*参考文献* [18]）。这些主题可以大致分为两类：
- en: '**Internal evaluation measures**: In this the measures uses some form of clustering
    quality from the data themselves, without any access to the ground truth.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内部评估指标**：在这种情况下，指标使用数据本身的一些聚类质量，而不需要访问真实值。'
- en: '**External evaluation measures**: In this the measures use some external information
    such as known ground truth or class labels.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**外部评估指标**：在这种情况下，指标使用一些外部信息，例如已知的真实值或类别标签。'
- en: Internal evaluation measures
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 内部评估指标
- en: 'Internal evaluation uses only the clusters and data information to gather metrics
    about how good the clustering results are. The applications may have some influence
    over the choice of the measures. Some algorithms are biased towards particular
    evaluation metrics. So care must be taken in choosing the right metrics, algorithms,
    and parameters based on these considerations. Internal evaluation measures are
    based on different qualities, as mentioned here:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 内部评估仅使用聚类和数据信息来收集有关聚类结果好坏的指标。应用程序可能对指标的选择有一定的影响。一些算法对特定的评估指标有偏见。因此，在选择合适的指标、算法和参数时必须谨慎，基于这些考虑：
- en: '**Compactness**: Variance in the clusters measured using different strategies
    is used to give compactness values; the lower the variance, the more compact the
    cluster.'
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**紧凑性**：使用不同策略测量的聚类方差用于给出紧凑性值；方差越低，聚类越紧凑。'
- en: '**Separation**: How well are the clusters separated from each other?'
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分离性**：聚类之间分离得有多好？'
- en: Notation
  id: totrans-261
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 符号
- en: 'Here''s a compact explanation of the notation used in what follows: dataset
    with all data elements =*D*, number of data elements =*n*, dimensions or features
    of each data element=*d*, center of entire data *D = c*, number of clusters =
    *NC*, *i*^(th) cluster = *C*[i], number of data in the *i*^(th) cluster =*n*[i],
    center of *i*^(th) cluster = *c*[i], variance in the *i*^(th) cluster = σ(*C*[i]),
    distance between two points *x* and *y* = *d (x,y)*.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是对以下内容中使用的符号的简要解释：所有数据元素的数据集=*D*，数据元素数量=*n*，每个数据元素的维度或特征=*d*，整个数据集的中心*D =
    c*，聚类数量=*NC*，第*i*个聚类=*C*[i]，第*i*个聚类中的数据数量=*n*[i]，第*i*个聚类的中心=*c*[i]，第*i*个聚类的方差=σ(*C*[i])，两点*x*和*y*之间的距离=*d
    (x,y)*。
- en: R-Squared
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R-Squared
- en: 'The goal is to measure the degree of difference between clusters using the
    ratio of the sum of squares between clusters to the total sum of squares on the
    whole data. The formula is given as follows:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是使用聚类之间的平方和总和与整个数据上的总平方和的比率来衡量聚类之间的差异程度。公式如下：
- en: '![R-Squared](img/B05137_03_129.jpg)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![R-Squared](img/B05137_03_129.jpg)'
- en: Dunn's Indices
  id: totrans-266
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Dunn的指数
- en: 'The goal is to identify dense and well-separated clusters. The measure is given
    by maximal values obtained from the following formula:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是识别密集且分离良好的聚类。该度量由以下公式的最大值给出：
- en: '![Dunn''s Indices](img/B05137_03_130.jpg)'
  id: totrans-268
  prefs: []
  type: TYPE_IMG
  zh: '![Dunn的指标](img/B05137_03_130.jpg)'
- en: Davies-Bouldin index
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Davies-Bouldin指数
- en: 'The goal is to identify clusters with low intra-cluster distances and high
    inter-cluster distances:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是识别具有低簇内距离和高簇间距离的聚类：
- en: '![Davies-Bouldin index](img/B05137_03_131.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![Davies-Bouldin指数](img/B05137_03_131.jpg)'
- en: Silhouette's index
  id: totrans-272
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 轮廓指数
- en: 'The goal is to measure the pairwise difference of between-cluster and within-cluster
    distances. It is also used to find optimal cluster number by maximizing the index.
    The formula is given by:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是测量簇间和簇内距离的成对差异。它还用于通过最大化指数来找到最佳聚类数量。公式如下：
- en: '![Silhouette''s index](img/B05137_03_132.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_IMG
  zh: '![轮廓指数](img/B05137_03_132.jpg)'
- en: Here ![Silhouette's index](img/B05137_03_133.jpg) and ![Silhouette's index](img/B05137_03_134.jpg).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 这里![轮廓指数](img/B05137_03_133.jpg)和![轮廓指数](img/B05137_03_134.jpg)。
- en: External evaluation measures
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 外部评估度量
- en: The external evaluation measures of clustering have similarity to classification
    metrics using elements from the confusion matrix or using information theoretic
    metrics from the data and labels. Some of the most commonly used measures are
    as follows.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的外部评估度量与使用混淆矩阵元素或使用数据和标签的信息论度量分类度量相似。以下是一些最常用的度量方法。
- en: Rand index
  id: totrans-278
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 兰德指数
- en: 'Rand index measures the correct decisions made by the clustering algorithm
    using the following formula:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 兰德指数使用以下公式衡量聚类算法做出的正确决策：
- en: '![Rand index](img/B05137_03_135.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![兰德指数](img/B05137_03_135.jpg)'
- en: F-Measure
  id: totrans-281
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: F-Measure
- en: 'F-Measure combines the precision and recall measures applied to clustering
    as given in the following formula:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: F-Measure结合了在聚类中应用的精确度和召回度度量，如下公式所示：
- en: '![F-Measure](img/B05137_03_136-New.jpg)![F-Measure](img/B05137_03_138.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![F-Measure](img/B05137_03_136-New.jpg)![F-Measure](img/B05137_03_138.jpg)'
- en: Here, *n*[ij] is the number of data elements of class *i* in the cluster *j*,
    *n*[j] is the number of data in the cluster *j* and *n*[i] is the number of data
    in the class *i*. The higher the F-Measure, the better the clustering quality.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n*[ij]是聚类*j*中类别*i*的数据元素数量，*n*[j]是聚类*j*中的数据数量，*n*[i]是类别*i*中的数据数量。F-Measure越高，聚类质量越好。
- en: Normalized mutual information index
  id: totrans-285
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 归一化互信息指数
- en: NMI is one of the many entropy-based measures applied to clustering. The entropy
    associated with a clustering *C* is a measure of the uncertainty about a cluster
    picking a data element randomly.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: NMI是应用于聚类的许多基于熵的度量之一。与聚类*C*相关的熵是关于簇随机选择数据元素的不确定性的度量。
- en: '![Normalized mutual information index](img/B05137_03_143.jpg) where ![Normalized
    mutual information index](img/B05137_03_144.jpg) is the probability of the element
    getting picked in cluster *C*i.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: '![归一化互信息指数](img/B05137_03_143.jpg)其中![归一化互信息指数](img/B05137_03_144.jpg)是元素在聚类*C*i中被选中的概率。'
- en: 'Mutual information between two clusters is given by:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 两个聚类之间的互信息由以下公式给出：
- en: '![Normalized mutual information index](img/B05137_03_146.jpg)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![归一化互信息指数](img/B05137_03_146.jpg)'
- en: Here ![Normalized mutual information index](img/B05137_03_147.jpg), which is
    the probability of the element being picked by both clusters *C* and *C^'*.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这里![归一化互信息指数](img/B05137_03_147.jpg)，这是元素被聚类*C*和*C^'*同时选中的概率。
- en: '**Normalized mutual information** (**NMI**) has many forms; one is given by:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '**归一化互信息**（**NMI**）有多种形式；其中一种如下所示：'
- en: '![Normalized mutual information index](img/B05137_03_150.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
  zh: '![归一化互信息指数](img/B05137_03_150.jpg)'
- en: Outlier or anomaly detection
  id: totrans-293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常值或异常检测
- en: Grubbs, in 1969, offers the definition, "An outlying observation, or outlier,
    is one that appears to deviate markedly from other members of the sample in which
    it occurs".
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 格鲁布斯在1969年给出了定义，“一个异常值，或称为异常值，是看起来明显偏离其所在样本中其他成员的观察值”。
- en: Hawkins, in 1980, defined outliers or anomaly as "an observation which deviates
    so much from other observations as to arouse suspicions that it was generated
    by a different mechanism".
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 霍金斯在1980年将异常值或异常定义为“一个与其他观察值差异如此之大，以至于引起怀疑它是由不同机制生成的观察”。
- en: Barnett and Lewis, 1994, defined it as "an observation (or subset of observations)
    which appears to be inconsistent with the remainder of that set of data".
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 巴内特和刘易斯在1994年将其定义为“一个（或一组）观察值，似乎与该数据集的其余部分不一致”。
- en: Outlier algorithms
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值算法
- en: 'Outlier detection techniques are classified based on different approaches to
    what it means to be an outlier. Each approach defines outliers in terms of some
    property that sets apart some objects from others in the dataset:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值检测技术根据不同的方法来分类，这些方法定义了异常值，即根据某些属性将数据集中的某些对象与其他对象区分开来：
- en: '**Statistical-based**: This is improbable according to a chosen distribution'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于统计的**：根据选择的分布来看，这是不可能的'
- en: '**Distance-based**: This is isolated from neighbors according to chosen distance
    measure and fraction of neighbors within threshold distance'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于距离的**：根据选择的距离度量，与邻居孤立，并且阈值距离内的邻居比例'
- en: '**Density-based**: This is more isolated from its neighbors than they are in
    turn from their neighbors'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的**：与邻居相比，它与其邻居相比更加孤立'
- en: '**Clustering-based**: This is in isolated clusters relative to other clusters
    or is not a member of any cluster'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于聚类的**：这是相对于其他簇孤立存在的簇，或者不是任何簇的成员'
- en: '**High-dimension-based**: This is an outlier by usual techniques after data
    is projected to lower dimensions, or by choosing an appropriate metric for high
    dimensions'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于高维度的**：这是在数据投影到低维度后，通过常规技术识别出的异常值，或者通过选择一个适合高维度的适当度量'
- en: Statistical-based
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于统计的方法
- en: Statistical-based techniques that use parametric methods for outlier detection
    assume some knowledge of the distribution of the data (*References* [19]). From
    the observations, the model parameters are estimated. Data points that have probabilities
    lower than a threshold value in the model are considered outliers. When the distribution
    is not known or none is suitable to assume, non-parametric methods are used.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 使用参数方法进行异常值检测的基于统计的技术假设对数据分布的一些了解（*参考文献* [19]）。从观察中，估计模型参数。在模型中概率低于阈值值的数据点被认为是异常值。当分布未知或没有合适的假设时，使用非参数方法。
- en: Inputs and outputs
  id: totrans-306
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Statistical methods for outlier detection work with real-valued datasets. The
    choice of distance metric may be a user-selected input in the case of parametric
    methods assuming multivariate distributions. In the case of non-parametric methods
    using frequency-based histograms, a user-defined threshold frequency is used.
    Selection of kernel method and bandwidth are also user-determined in Kernel Density
    Estimation techniques. The output from statistical-based methods is a score indicating
    outlierness.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 用于异常值检测的统计方法与实值数据集一起工作。在假设多元分布的参数方法中，距离度量的选择可能是一个用户选择的输入。在基于频率直方图的非参数方法中，使用用户定义的阈值频率。在核密度估计技术中，核方法和带宽的选择也是用户决定的。基于统计的方法的输出是一个表示异常程度的分数。
- en: How does it work?
  id: totrans-308
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'Most of the statistical-based outlier detections either assume a distribution
    or fit a distribution to the data to detect probabilistically the least likely
    data generated from the distribution. These methods have two distinct steps:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数基于统计的异常值检测要么假设一个分布，要么将分布拟合到数据中，以概率方式检测从分布中生成的最不可能的数据。这些方法有两个不同的步骤：
- en: '**Training step**: Here, an estimate of the model to fit the data is performed'
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**训练步骤**：在这里，执行拟合数据的模型估计'
- en: '**Testing step**: On each instance a goodness of fit is performed based on
    the model and the particular instance, yielding a score and the outlierness'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**测试步骤**：在每个实例上，基于模型和特定实例执行拟合优度检验，得到一个分数和异常值程度'
- en: Parametric-based methods assume a distribution model such as multivariate Gaussians
    and the training normally involves estimating the means and variance using techniques
    such as **Maximum Likelihood Estimates** (**MLE**). The testing typically includes
    techniques such as mean-variance or box-plot tests, accompanied by assumptions
    such as "if outside three standard deviations, then outlier".
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 基于参数的方法假设一个分布模型，如多元高斯分布，并且训练通常涉及使用诸如**最大似然估计（MLE）**等技术来估计均值和方差。测试通常包括均值-方差或箱线图测试等技术，并伴随假设“如果超出三个标准差，则视为异常值”。
- en: 'A normal multivariate distribution can be estimated as:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 一个正常的多元分布可以被估计为：
- en: '![How does it work?](img/B05137_03_151.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_151.jpg)'
- en: with the mean **µ** and covariance Ʃ.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 使用均值 **µ** 和协方差 Ʃ。
- en: The Mahalanobis distance can be the estimate of the data point from the distribution
    given by the equation ![How does it work?](img/B05137_03_154.jpg). Some variants
    such as **Minimum Covariant Determinant** (**MCD**) are also used when Mahalanobis
    distance is affected by outliers.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 马氏距离可以是数据点从由方程 ![如何工作？](img/B05137_03_154.jpg) 给出的分布的估计。当马氏距离受到异常值影响时，也会使用一些变体，例如**最小协方差行列式**（**MCD**）。
- en: 'A non-parametric method involves techniques such as constructing histograms
    for every feature using frequency or width-based methods. When the ratio of the
    data in a bin to that of the average over the histogram is below a user defined
    threshold, such a bin is termed sparse. A lower probability of feature results
    in a higher outlier score. The total outlier score can be computed as:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数方法涉及诸如使用频率或宽度方法为每个特征构建直方图等技术。当某个箱子中的数据与直方图平均值的比率低于用户定义的阈值时，这样的箱子被称为稀疏。特征的低概率会导致更高的异常值得分。总异常值得分可以计算如下：
- en: '![How does it work?](img/B05137_03_155.jpg)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_155.jpg)'
- en: Here, *w*[f] is the weight given to feature *f*, *p*[f] is the probability of
    the value of the feature in the test data point, and *F* is the sum of weights
    of the feature set. Kernel Density Estimations are also used in non-parametric
    methods using user-defined kernels and bandwidth.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*w*[f] 是赋予特征 *f* 的权重，*p*[f] 是测试数据点中特征值的概率，*F* 是特征集权重的总和。核密度估计也用于使用用户定义的核和带宽的非参数方法。
- en: Advantages and limitations
  id: totrans-320
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: When the model fits or distribution of the data is known, these methods are
    very efficient as you don't have to store entire data, just the key statistics
    for doing tests.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型拟合或数据分布已知时，这些方法非常高效，因为你不需要存储整个数据，只需存储进行测试所需的关键统计数据。
- en: Assumptions of distribution, however, can pose a big issue in parametric methods.
    Most non-parametric methods using kernel density estimates don't scale well with
    large datasets.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然而，分布的假设在参数方法中可能是一个大问题。大多数使用核密度估计的非参数方法在大数据集上扩展性不好。
- en: Distance-based methods
  id: totrans-323
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于距离的方法
- en: Distance-based algorithms work under the general assumption that normal data
    has other data points closer to it while anomalous data is well isolated from
    its neighbors (*References* [20]).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 基于距离的算法在一般假设下工作，即正常数据有其他数据点更接近它，而异常数据与其邻居很好地隔离（*参考文献* [20]）。
- en: Inputs and outputs
  id: totrans-325
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Distance-based techniques require natively numeric or categorical features to
    be transformed to numeric values. Inputs to distance-based methods are the distance
    metric used, the distance threshold ϵ, and π, the threshold fraction, which together
    determine if a point is an outlier. For KNN methods, the choice *k* is an input.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 基于距离的技术需要将原生数值或分类特征转换为数值。基于距离的方法的输入包括使用的距离度量、距离阈值 ϵ 和 π，以及阈值分数 π，它们共同决定一个点是否是异常值。对于KNN方法，选择
    *k* 是一个输入。
- en: How does it work?
  id: totrans-327
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'There are many variants of distance-based outliers and we will discuss how
    each of them works at a high level:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 基于距离的异常值有很多种变体，我们将从高层次上讨论它们各自的工作原理：
- en: 'DB (*ϵ**, π*) Algorithms: Given a radius of *ϵ* and threshold of π, a data
    point is considered as an outlier if π percentage of points have distance to the
    point less than *ϵ*. There are further variants using nested loop structures,
    grid-based structures, and index-based structures on how the computation is done.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DB (*ϵ*, π*) 算法：给定半径 *ϵ* 和阈值 π，如果一个数据点的 π 百分比的距离小于 *ϵ*，则该数据点被认为是异常值。还有进一步的变体，使用嵌套循环结构、基于网格的结构和基于索引的结构来执行计算。
- en: '*KNN*-based methods are also very common where the outlier score is computed
    either by taking the *KNN* distance to the point or the average distance to point
    from *{1NN,2NN,3NN…KNN}*.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于KNN的方法也非常常见，其中异常值得分是通过计算点到 *KNN* 的距离或从 *{1NN,2NN,3NN…KNN}* 到点的平均距离来计算的。
- en: Advantages and limitations
  id: totrans-331
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The main advantage of distance-based algorithms is that they are non-parametric
    and make no assumptions on distributions and how to fit models.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于距离的算法的主要优点是它们是非参数的，不对分布和模型拟合方式做出假设。
- en: The distance calculations are straightforward and computed in parallel, helping
    the algorithms to scale on large datasets.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 距离计算很简单，可以并行计算，有助于算法在大数据集上扩展。
- en: The major issues with distance-based methods is the curse of dimensionality
    discussed in the first chapter; for large dimensional data, sparsity can lead
    to noisy outlierness.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于距离的方法的主要问题是第一章中讨论的维度诅咒；对于高维数据，稀疏性可能导致异常值噪声。
- en: Density-based methods
  id: totrans-335
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于密度的方法
- en: Density-based methods extend the distance-based methods by not only measuring
    the local density of the given point, but also the local densities of its neighborhood
    points. Thus, the relative factor added gives it the edge in finding more complex
    outliers that are local or global in nature, but at the added cost of computation.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的方法通过不仅测量给定点的局部密度，还测量其邻域点的局部密度来扩展基于距离的方法。因此，添加的相对因子使其在寻找更复杂的局部或全局异常值方面具有优势，但这也增加了计算成本。
- en: Inputs and outputs
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Density-based algorithm must be supplied the minimum number of points *MinPts*
    in a neighborhood of input radius *ϵ* centered on an object that determines it
    is a core object in a cluster.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 基于密度的算法必须提供输入半径 *ϵ* 中心对象的最小点数 *MinPts*，以确定它是一个簇的核心对象。
- en: How does it work?
  id: totrans-339
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: We will first discuss the **Loca** **Outlier Factor** (**LOF**) method and then
    discuss some variants of LOF [21].
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论 **局部异常因子**（**LOF**）方法，然后讨论 LOF 的某些变体 [21]。
- en: 'Given the *MinPts* as the parameter, LOF of a data point is:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 给定 *MinPts* 作为参数，数据点的 LOF 为：
- en: '![How does it work?](img/B05137_03_168.jpg)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_168.jpg)'
- en: 'Here |*N* *[MinPts]* *(p)*| is the number of data points in the neighborhood
    of point *p*, and *lrd* *[MinPts]* is the local reachability density of the point
    and is defined as:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 |*N* *[MinPts]* *(p)*| 是点 *p* 邻域中的数据点数量，*lrd* *[MinPts]* 是点的局部可达密度，定义为：
- en: '![How does it work?](img/B05137_03_171.jpg)'
  id: totrans-344
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_171.jpg)'
- en: 'Here ![How does it work?](img/B05137_03_172.jpg) is the reachability of the
    point and is defined as:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 ![如何工作？](img/B05137_03_172.jpg) 是点的可达性，定义为：
- en: '![How does it work?](img/B05137_03_173.jpg)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_173.jpg)'
- en: 'One of the disadvantages of LOF is that it may miss outliers whose neighborhood
    density is close to that of its neighborhood. **Connectivity-based outliers**
    (**COF**) using set-based nearest path and set-based nearest trail originating
    from the data point are used to improve on LOF. COF treats the low-density region
    differently to the isolated region and overcomes the disadvantage of LOF:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: LOF 的一个缺点是它可能会错过那些其邻域密度接近其邻域密度的异常值。使用从数据点起源的基于集合的最近路径和基于集合的最近轨迹的**连通性异常值**（**COF**）被用来改进
    LOF。COF 对低密度区域和孤立区域的处理不同，克服了 LOF 的缺点：
- en: '![How does it work?](img/B05137_03_175.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_175.jpg)'
- en: 'Another disadvantage of LOF is that when clusters are in varying densities
    and not separated, LOF will generate counter-intuitive scores. One way to overcome
    this is to use the **influence space** (**IS**) of the points using KNNs and its
    reverse KNNs or RNNs. RNNs have the given point as one of their K nearest neighbors.
    Outlierness of the point is known as Influenced Outliers or INFLO and is given
    by:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: LOF 的另一个缺点是当簇在变化密度且未分离时，LOF 将生成反直觉的分数。克服这一点的办法之一是使用基于 KNN 和其反向 KNN 或 RNN 的点的**影响空间**（**IS**）。RNN
    将给定点作为其 K 个最近邻之一。点的异常性称为受影响异常值或 INFLO，其值为：
- en: '![How does it work?](img/B05137_03_176.jpg)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_176.jpg)'
- en: 'Here, *den*(*p*) is the local density of *p*:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*den*(*p*) 是 *p* 的局部密度：
- en: '![How does it work?](img/B05137_03_179.jpg)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_179.jpg)'
- en: 'Figure 5: Density-based outlier detection methods are particularly suited for
    finding local as well as global outliers'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5：基于密度的异常值检测方法特别适合寻找局部和全局异常值
- en: Advantages and limitations
  id: totrans-354
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: It has been shown that density-based methods are more effective than distance-based
    methods.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已证明基于密度的方法比基于距离的方法更有效。
- en: Density-based outlier detection has high computational cost and, often, poor
    interpretability.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于密度的异常值检测计算成本高，且通常可解释性较差。
- en: Clustering-based methods
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于聚类的算法
- en: Some believe that clustering techniques, where the goal is to find groups of
    data points located together, are in some sense antithetical to the problem of
    anomaly or outlier detection. However, as an advanced unsupervised learning technique,
    clustering analysis offers several methods to find interesting groups of clusters
    that are either located far off from other clusters or do not lie in any clusters
    at all.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 一些认为，旨在找到位于一起的数据点群体的聚类技术，在某种程度上与异常或异常值检测问题相对立。然而，作为一种高级无监督学习技术，聚类分析提供了几种方法来找到位于其他簇较远或根本不在任何簇中的有趣簇组。
- en: Inputs and outputs
  id: totrans-359
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'As seen before, clustering techniques work well with real-valued data, although
    some categorical values translated to numeric values are tolerated. In the case
    of k-Means and k-Medoids, input values include the number of clusters *k* and
    the distance metric. Variants may require a threshold score to identify outlier
    groups. For Gaussian Mixture Models using EM, the number of mixture components
    must be supplied by the user. When using CBLOF, two user-defined parameters are
    expected: the size of small clusters and the size of large clusters. Depending
    on the algorithm used, individual or groups of objects are output as outliers.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，聚类技术适用于实值数据，尽管可以容忍一些转换为数值的类别值。在k-Means和k-Medoids的情况下，输入值包括簇的数量*k*和距离度量。变体可能需要阈值得分来识别异常值组。对于使用EM的Gaussian
    Mixture Models，必须由用户提供混合成分的数量。当使用CBLOF时，期望两个用户定义的参数：小簇的大小和大簇的大小。根据使用的算法，单个对象或对象组作为异常值输出。
- en: How does it work?
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它是如何工作的？
- en: As we discussed in the section on clustering, there are various types of clustering
    methods and we will give a few examples of how clustering algorithms have been
    extended for outlier detection.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在聚类章节中讨论的，存在各种类型的聚类方法，我们将给出一些聚类算法如何扩展用于异常值检测的例子。
- en: k-Means or k-Medoids and their variants generally cluster data elements together
    and are affected by outliers or noise. Instead of preprocessing these data points
    by removal or transformation, such points that weaken the "tightness" of the clusters
    are considered outliers. Typically, outliers are revealed by a two-step process
    of first running clustering algorithms and then evaluating some form of outlier
    score that measures distance from point to centroid. Also, many variants treat
    clusters of size smaller than a threshold as an outlier group.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: k-Means或k-Medoids及其变体通常将数据元素聚在一起，并受异常值或噪声的影响。而不是通过删除或转换预处理这些数据点，这些削弱簇“紧密性”的点被视为异常值。通常，异常值通过先运行聚类算法然后评估某种形式的异常值得分来揭示，该得分衡量点与质心的距离。此外，许多变体将小于阈值大小的簇视为异常值组。
- en: '**Gaussian mixture modeling** (**GMM**) using **Expectation maximization**
    (**EM**) is another well-known clustering-based outlier detection technique, where
    a data point that has low probability of belonging to a cluster becomes an outlier
    and the outlier score becomes the inverse of the EM probabilistic output score.'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯混合模型**（**GMM**）使用**期望最大化**（**EM**）是另一种著名的基于聚类的异常值检测技术，其中属于某个簇的概率较低的数据点成为异常值，异常值得分成为EM概率输出得分的倒数。'
- en: '**Cluster-based Local Outlier Factor** (**CBLOF**) uses a two-stage process
    to find outliers. First, a clustering algorithm performs partitioning of data
    into clusters of various sizes. Using two user-defined parameters, size of large
    clusters, and size of small clusters, two sets of clusters are formed:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于簇的局部异常因子**（**CBLOF**）使用两阶段过程来寻找异常值。首先，聚类算法将数据划分为各种大小的簇。使用两个用户定义的参数，大簇的大小和小簇的大小，形成两套簇集：'
- en: '![How does it work?](img/B05137_03_215.jpg)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![它是如何工作的？](img/B05137_03_215.jpg)'
- en: Advantages and limitations
  id: totrans-367
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: Given that clustering-based techniques are well-understood, results are more
    interpretable and there are more tools available for these techniques.
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于基于聚类的技术已被充分理解，结果更具可解释性，并且有更多工具可用于这些技术。
- en: Many clustering algorithms only detect clusters, and are less effective in unsupervised
    techniques compared to outlier algorithms that give scores or ranks or otherwise
    identify outliers.
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多聚类算法仅检测簇，与给出得分、排名或其他识别异常值的异常值算法相比，在无监督技术中效果较差。
- en: High-dimensional-based methods
  id: totrans-370
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 基于高维的方法
- en: One of the key issues with distance-, density-, or even clustering-based methods,
    is the curse of dimensionality. As dimensions increase, the contrast between distances
    diminishes and the concept of neighborhood becomes less meaningful. The normal
    points in this case look like outliers and false positives increase by large volume.
    We will discuss some of the latest approaches taken in addressing this problem.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 距离、密度或甚至基于聚类的几种方法的关键问题之一是维度诅咒。随着维度的增加，距离之间的对比度减小，邻域的概念变得不那么有意义。在这种情况下，正常点看起来像是异常值，并且错误正例的数量大幅增加。我们将讨论一些解决这个问题的最新方法。
- en: Inputs and outputs
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: Algorithms that project data to lower-dimensional subspaces can handle missing
    data well. In these techniques, such as SOD, *ϕ*, the number of ranges in each
    dimension becomes an input (*References* [25]). When using an evolutionary algorithm,
    the number of cells with the lowest sparsity coefficients is another input parameter
    to the algorithm.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据投影到低维子空间中的算法可以很好地处理缺失数据。在这些技术中，如SOD，*ϕ*，每个维度的范围数量成为输入（*参考文献* [25]）。当使用进化算法时，具有最低稀疏系数的单元格数量是算法的另一个输入参数。
- en: How does it work?
  id: totrans-374
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: 'The broad idea to solve the high dimensional outlier issue is to:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 解决高维异常值问题的广泛思路是：
- en: Either have a robust distance metric coupled with all of the previous techniques,
    so that outliers can be identified in full dimensions
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者将鲁棒的距离度量与所有先前技术相结合，以便可以在完整维度中识别异常值
- en: Or project data on to smaller subspaces and find outliers in the smaller subspaces
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者将数据投影到较小的子空间中，并在较小的子空间中找到异常值
- en: The **Angle-based Outlier Degree** (**ABOD**) method uses the basic assumption
    that if a data point in high dimension is an outlier, all the vectors originating
    from it towards data points nearest to it will be in more or less the same direction.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: '**基于角度的异常值度**（**ABOD**）方法使用这样一个基本假设：如果一个高维数据点是异常值，那么从它出发指向最近数据点的所有向量在或多或少的同一方向上。'
- en: '![How does it work?](img/B05137_03_184.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_184.jpg)'
- en: 'Figure 6: The ABOD method of distinguishing outliers from inliers'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图6：区分异常值和内点的ABOD方法
- en: 'Given a point *p*, and any two points *x* and *y*, the angle between the two
    points and *p* is given by:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 给定点 *p* 和任意两点 *x* 和 *y*，这两点与 *p* 之间的角度由以下公式给出：
- en: '![How does it work?](img/B05137_03_187.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_187.jpg)'
- en: 'Measure of variance used as the ABOD score is given by:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 作为ABOD分数使用的方差度量由以下公式给出：
- en: '![How does it work?](img/B05137_03_188.jpg)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_188.jpg)'
- en: The smaller the ABOD value, the smaller the measure of variance in the angle
    spectrum, and the larger the chance of the point being the outlier.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ABOD值越小，角度谱中的方差度量越小，点成为异常值的可能性就越大。
- en: 'Another method that has been very useful in high dimensional data is using
    the **Subspace Outlier Detection** (**SOD**) approach (*References* [23]). The
    idea is to partition the high dimensional space such that there are an equal number
    of ranges, say *ϕ*, in each of the *d* dimensions. Then the Sparsity Coefficient
    for a cell *C* formed by picking a range in each of the *d* dimensions is measured
    as follows:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种在高维数据中非常有用的方法是使用**子空间异常值检测**（**SOD**）方法（*参考文献* [23]）。其想法是将高维空间划分为每个维度中范围数量相等的区域，例如在
    *d* 个维度中都有 *ϕ* 个范围。然后，通过在每个 *d* 维度中选择一个范围形成的单元格 *C* 的稀疏系数如下测量：
- en: '![How does it work?](img/B05137_03_192.jpg)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_192.jpg)'
- en: Here, *n* is the total number of data points and *N(C)* is the number of data
    points in cell *C*. Generally, the data points lying in cells with negative sparsity
    coefficient are considered outliers.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*n* 是数据点的总数，*N(C)* 是单元格 *C* 中数据点的数量。通常，位于具有负稀疏系数的单元格中的数据点被认为是异常值。
- en: Advantages and limitations
  id: totrans-389
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The ABOD method is *O(n*³*)* with the number of data points and becomes impractical
    with larger datasets.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ABOD方法的时间复杂度为 *O(n³*)*，与数据点的数量成正比，因此在大数据集上变得不切实际。
- en: The sparsity coefficient method in subspaces requires efficient search in lower
    dimension and the problem becomes NP-Hard and some form of evolutionary or heuristic
    based search is employed.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子空间中的稀疏系数方法需要高效地在低维中进行搜索，问题变得NP-Hard，因此采用了某种形式的进化或启发式搜索。
- en: The sparsity coefficient methods being NP-Hard can result in local optima.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稀疏系数方法由于是NP-Hard，可能会导致局部最优。
- en: One-class SVM
  id: totrans-393
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一类SVM
- en: 'In many domains there is a particular class or category of interest and the
    "rest" do not matter. Finding a boundary around this class of interest is the
    basic idea behind one-class SVM (*References* [26]). The basic assumption is that
    all the points of the positive class (class of interest) cluster together while
    the other class elements are spread around and we can find a tight hyper-sphere
    around the clustered instances. SVM, which has great theoretical foundations and
    applications in binary classifications is reformulated to solve one-class SVM.
    The following figure illustrates how a nonlinear boundary is simplified by using
    one-class SVM with slack so as to not overfit complex functions:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多领域，存在一个特定的感兴趣类或类别，而“其余”则无关紧要。找到围绕这个感兴趣类的边界是一类别SVM（*参考文献* [26]）背后的基本思想。基本假设是正类（感兴趣类）的所有点都聚集在一起，而其他类的元素则散布开来，我们可以在聚集实例周围找到一个紧密的超球体。SVM，具有强大的理论基础和在二类别分类中的应用，被重新公式化以解决一类别SVM。以下图示说明了如何使用一类别SVM和松弛项简化非线性边界，以避免过度拟合复杂函数：
- en: '![One-class SVM](img/B05137_03_197.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![一类别SVM](img/B05137_03_197.jpg)'
- en: 'Figure 7: One-Class SVM for nonlinear boundaries'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 图7：一类别SVM用于非线性边界
- en: Inputs and outputs
  id: totrans-397
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入和输出
- en: 'Data inputs are generally numeric features. Many SVMs can take nominal features
    and apply binary transformations to them. Also needed are: marking the class of
    interest, SVM hyper-parameters such as kernel choice, kernel parameters and cost
    parameter, among others. Output is a SVM model that can predict whether instances
    belong to the class of interest or not. This is different from scoring models,
    which we have seen previously.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 数据输入通常是数值特征。许多SVM可以接受名义特征并对它们应用二进制转换。还需要标记感兴趣类的类别，SVM超参数，如核选择、核参数和成本参数等。输出是一个可以预测实例是否属于感兴趣类的SVM模型。这与我们之前看到的评分模型不同。
- en: How does it work?
  id: totrans-399
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何工作？
- en: The input is training instances {**x**[1],**x**[2]…**x**[n]} with certain instances
    marked to be in class +1 and rest in -1.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 输入是训练实例{**x**[1],**x**[2]…**x**[n]}，其中某些实例被标记为属于+1类，其余属于-1类。
- en: 'The input to SVM also needs a kernel that does the transformation *ϕ* from
    input space to features space as ![How does it work?](img/B05137_03_199.jpg) using:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: SVM的输入也需要一个核，它将输入空间转换为特征空间的变换 *ϕ* 如 ![如何工作？](img/B05137_03_199.jpg) 使用：
- en: '![How does it work?](img/B05137_03_200.jpg)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_200.jpg)'
- en: 'Create a hyper-sphere that bounds the classes using SVM reformulated equation
    as:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SVM重新公式化的方程创建一个界定类的超球体：
- en: '![How does it work?](img/B05137_03_201.jpg)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作？](img/B05137_03_201.jpg)'
- en: Such that ![How does it work?](img/B05137_03_202.jpg)+![How does it work?](img/B05137_03_203.jpg),
    ![How does it work?](img/B05137_03_204.jpg)
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 如此![如何工作？](img/B05137_03_202.jpg)+![如何工作？](img/B05137_03_203.jpg), ![如何工作？](img/B05137_03_204.jpg)
- en: '*R* is the radius of the hyper-sphere with center **c** and *ν* ∈ (0,1] represents
    an upper bound on the fraction of the data that are outliers.'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '*R* 是以**c**为中心的超球体的半径，*ν* ∈ (0,1] 表示异常值数据所占分数的上限。'
- en: As in normal SVM, we perform optimization using quadratic programming is done
    to obtain the solution as the decision boundary.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 正常SVM一样，我们通过二次规划进行优化，以获得决策边界的解。
- en: Advantages and limitations
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 优点和局限性
- en: The key advantage to using one-class SVM—as is true of binary SVM—is the many
    theoretical guarantees in error and generalization bounds.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一类别SVM的关键优势——正如二类别SVM一样——是许多关于误差和泛化界限的理论保证。
- en: High-dimensional data can be easily mapped in one-class SVM.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维数据可以轻松地在一类别SVM中映射。
- en: Non-linear SVM with kernels can even find non-spherical shapes to bound the
    clusters of data.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 带核的非线性SVM甚至可以找到非球形的形状来界定数据的簇。
- en: The training cost in space and memory increases as the size of the data increases.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着数据量的增加，训练成本在空间和内存中增加。
- en: Parameter tuning, especially the kernel parameters and the cost parameter tuning
    with unlabeled data is a big challenge.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 参数调整，尤其是核参数和未标记数据的成本参数调整，是一个巨大的挑战。
- en: Outlier evaluation techniques
  id: totrans-414
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值评估技术
- en: Measuring outliers in terms of labels, ranks, and scores is an area of active
    research. When the labels or the ground truth is known, the idea of evaluation
    becomes much easier as the outlier class is known and standard metrics can be
    employed. But when the ground truth is not known, the evaluation and validation
    methods are very subjective and there is no well-defined, rigorous statistical
    process.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 以标签、排名和分数来衡量异常值是一个活跃的研究领域。当标签或真实标签已知时，由于已知异常值类别，评估的想法变得容易得多，可以采用标准指标。但是，当真实标签未知时，评估和验证方法非常主观，并且没有明确定义、严格的统计过程。
- en: Supervised evaluation
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督评估
- en: In cases where the ground truth is known, the evaluation of outlier algorithms
    is basically the task of finding the best thresholds for outlier scores (scoring-based
    outliers).
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在已知真实标签的情况下，对异常值算法的评估基本上是寻找异常值分数（基于评分的异常值）的最佳阈值。
- en: The balance between reducing the false positives and improving true positives
    is the key concept and Precision-Recall curves (described in [Chapter 2](ch02.html
    "Chapter 2. Practical Approach to Real-World Supervised Learning"), *Practical
    Approach to Real-World Supervised Learning*) are used to find the best optimum
    threshold. Confidence score, predictions, and actual labels are used in supervised
    learning to plot PRCurves, and instead of confidence scores, outlier scores are
    ranked and used here. ROC curves and area under curves are also used in many applications
    to evaluate thresholds. Comparing two or more algorithms and selection of the
    best can also be done using area under curve metrics when the ground truth is
    known.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 在减少假阳性并提高真阳性之间的平衡是关键概念，并且精度-召回曲线（在第2章[Chapter 2](ch02.html "Chapter 2. Practical
    Approach to Real-World Supervised Learning")中描述，*《面向现实世界监督学习的实用方法*）被用来找到最佳最优阈值。在监督学习中，置信度分数、预测和实际标签被用来绘制PR曲线，而在这里，不是使用置信度分数，而是对异常值分数进行排序并使用。ROC曲线和曲线下的面积也被广泛应用于评估阈值。当已知真实标签时，比较两个或多个算法并选择最佳算法也可以使用曲线下的面积指标来完成。
- en: Unsupervised evaluation
  id: totrans-419
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督评估
- en: In most real-world cases, knowing the ground truth is very difficult, at least
    during the modeling task. Hawkins describes the evaluation method in this case
    at a very high level as "a sample containing outliers would show up such characteristics
    as large gaps between 'outlying' and 'inlying' observations and the deviation
    between outliers and the group of inliers, as measured on some suitably standardized
    scale".
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数现实世界的案例中，知道真实标签是非常困难的，至少在建模任务期间是这样的。霍金斯以非常高的水平描述了这种情况下的评估方法，将其称为“包含异常值的样本将显示出'异常'和'内含'观察值之间的大差距以及异常值与内含组之间的偏差，这些偏差是在某些适当标准化的尺度上测量的”。
- en: 'The general technique used in evaluating outliers when the ground truth is
    not known is:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 当不知道真实标签时，在评估异常值时使用的通用技术是：
- en: '**Histogram of outlier scores**: A visualization-based method, where outlier
    scores are grouped into predefined bins and users can select thresholds based
    on outlier counts, scores, and thresholds.'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**异常值分数直方图**：一种基于可视化的方法，其中异常值分数被分组到预定义的箱中，用户可以根据异常值计数、分数和阈值来选择阈值。'
- en: '**Score normalization and distance functions**: In this technique, some form
    of normalization is done to make sure all outlier algorithms that produce scores
    have the same ranges. Some form of distance or similarity or correlation based
    method is used to find commonality of outliers across different algorithms. The
    general intuition here is: the more the algorithms that weigh the data point as
    outlier, the higher the probability of that point actually being an outlier.'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**评分归一化和距离函数**：在这种技术中，进行某种形式的归一化以确保所有产生分数的异常值算法具有相同的范围。使用某种形式的距离或相似性或相关性的方法来找到不同算法之间异常值的共性。这里的总体直觉是：越多的算法将数据点视为异常值，该点实际上为异常值的概率就越高。'
- en: Real-world case study
  id: totrans-424
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 现实世界案例研究
- en: Here we present a case study that illustrates how to apply clustering and outlier
    techniques described in this chapter in the real world, using open-source Java
    frameworks and a well-known image dataset.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们提供了一个案例研究，说明了如何使用本章中描述的聚类和异常值技术，通过开源Java框架和知名图像数据集在现实世界中应用。
- en: Tools and software
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工具和软件
- en: 'We will now introduce two new tools that were used in the experiments for this
    chapter: SMILE and Elki. SMILE features a Java API that was used to illustrate
    feature reduction using PCA, Random Projection, and IsoMap. Subsequently, the
    graphical interface of Elki was used to perform unsupervised learning—specifically,
    clustering and outlier detection. Elki comes with a rich set of algorithms for
    cluster analysis and outlier detection including a large number of model evaluators
    to choose from.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将介绍本章实验中使用的两种新工具：SMILE和Elki。SMILE提供了一个Java API，用于展示使用PCA、随机投影和IsoMap进行特征降维。随后，Elki的图形界面被用来执行无监督学习——具体来说，是聚类和异常值检测。Elki附带了一组丰富的聚类分析和异常值检测算法，包括大量可供选择的模型评估器。
- en: Note
  id: totrans-428
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 备注
- en: 'Find out more about SMILE at: [http://haifengl.github.io/smile/](http://haifengl.github.io/smile/)
    and to learn more about Elki, visit [http://elki.dbs.ifi.lmu.de/](http://elki.dbs.ifi.lmu.de/).'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 了解更多关于SMILE的信息：[http://haifengl.github.io/smile/](http://haifengl.github.io/smile/)，以及更多关于Elki的信息，请访问：[http://elki.dbs.ifi.lmu.de/](http://elki.dbs.ifi.lmu.de/)。
- en: Business problem
  id: totrans-430
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 商业问题
- en: Character-recognition is a problem that occurs in many business areas, for example,
    the translation of medical reports and hospital charts, postal code recognition
    in the postal service, check deposit service in retail banking, and others. Human
    handwriting can vary widely among individuals. Here, we are looking exclusively
    at handwritten digits, 0 to 9\. The problem is made interesting due to the verisimilitude
    within certain sets of digits, such as 1/2/7 and 6/9/0\. In our experiments in
    this chapter we use clustering and outlier analysis using several different algorithms
    to illustrate the relative strengths and weaknesses of the methods. Given the
    widespread use of these techniques in data mining applications, our main focus
    is to gain insights into the data and the algorithms and evaluation measures;
    we do not apply the models for prediction on test data.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 识别字符是许多商业领域遇到的问题，例如，医疗报告和医院图表的翻译，邮政服务中的邮政编码识别，零售银行的支票存款服务，以及其他一些领域。人类手写体在个体之间可能有很大的差异。在这里，我们专门关注手写数字，即0到9。由于某些数字集合（如1/2/7和6/9/0）之间的相似性，这个问题变得很有趣。在本章的实验中，我们使用了几种不同的算法进行聚类和异常值分析，以展示这些方法的相对优势和劣势。鉴于这些技术在数据挖掘应用中的广泛应用，我们的主要目标是深入了解数据和算法以及评估措施；我们不对测试数据进行预测。
- en: Machine learning mapping
  id: totrans-432
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习映射
- en: As suggested by the title of the chapter, our experiments aim to demonstrate
    Unsupervised Learning by ignoring the labels identifying the digits in the dataset.
    Having learned from the dataset, clustering and outlier analyses can yield invaluable
    information for describing patterns in the data, and are often used to explore
    these patterns and inter-relationships in the data, and not just to predict the
    class of unseen data. In the experiments described here, we are concerned with
    description and exploration rather than prediction. Labels are used when available
    by external evaluation measures, as they are in these experiments as well.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 如章节标题所示，我们的实验旨在通过忽略数据集中识别数字的标签来展示无监督学习。从数据集中学习后，聚类和异常值分析可以提供关于数据中模式的有价值信息，并且通常用于探索这些模式和相互关系，而不仅仅是预测未见数据的类别。在本章描述的实验中，我们关注的是描述和探索，而不是预测。当外部评估措施可用时，使用标签，正如这些实验中一样。
- en: Data collection
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据收集
- en: 'This is already done for us. For details on how the data was collected, see:
    The MNIST database: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经为我们准备好了。关于数据收集的详细信息，请参阅：MNIST数据库：[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)。
- en: Data quality analysis
  id: totrans-436
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据质量分析
- en: 'Each feature in a data point is the greyscale value of one of 784 pixels. Consequently,
    the type of all features is numeric; there are no categorical types except for
    the class attribute, which is a numeral in the range 0-9\. Moreover, there are
    no missing data elements in the dataset. Here is a table with some basic statistics
    for a few pixels. The images are pre-centred in the 28 x 28 box so in most examples,
    the data along the borders of the box are zeros:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 数据点中的每个特征都是784个像素中的一个的灰度值。因此，所有特征的类型都是数值型；除了类别属性是0到9范围内的数字之外，没有其他分类类型。此外，数据集中没有缺失数据元素。以下是几个像素的一些基本统计信息的表格。图像在28
    x 28的框中预先居中，因此在大多数例子中，框边沿的数据是零：
- en: '| Feature | Average | Std Dev | Min | Max |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 平均值 | 标准差 | 最小值 | 最大值 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-439
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| pixel300 | 94.25883 | 109.117 | 0 | 255 |'
  id: totrans-440
  prefs: []
  type: TYPE_TB
  zh: '| pixel300 | 94.25883 | 109.117 | 0 | 255 |'
- en: '| pixel301 | 72.778 | 103.0266 | 0 | 255 |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '| pixel301 | 72.778 | 103.0266 | 0 | 255 |'
- en: '| pixel302 | 49.06167 | 90.68359 | 0 | 255 |'
  id: totrans-442
  prefs: []
  type: TYPE_TB
  zh: '| pixel302 | 49.06167 | 90.68359 | 0 | 255 |'
- en: '| pixel303 | 28.0685 | 70.38963 | 0 | 255 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| pixel303 | 28.0685 | 70.38963 | 0 | 255 |'
- en: '| pixel304 | 12.84683 | 49.01016 | 0 | 255 |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| pixel304 | 12.84683 | 49.01016 | 0 | 255 |'
- en: '| pixel305 | 4.0885 | 27.21033 | 0 | 255 |'
  id: totrans-445
  prefs: []
  type: TYPE_TB
  zh: '| pixel305 | 4.0885 | 27.21033 | 0 | 255 |'
- en: '| pixel306 | 1.147 | 14.44462 | 0 | 254 |'
  id: totrans-446
  prefs: []
  type: TYPE_TB
  zh: '| pixel306 | 1.147 | 14.44462 | 0 | 254 |'
- en: '| pixel307 | 0.201667 | 6.225763 | 0 | 254 |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '| pixel307 | 0.201667 | 6.225763 | 0 | 254 |'
- en: '| pixel308 | 0 | 0 | 0 | 0 |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '| pixel308 | 0 | 0 | 0 | 0 |'
- en: '| pixel309 | 0.009167 | 0.710047 | 0 | 55 |'
  id: totrans-449
  prefs: []
  type: TYPE_TB
  zh: '| pixel309 | 0.009167 | 0.710047 | 0 | 55 |'
- en: '| pixel310 | 0.102667 | 4.060198 | 0 | 237 |'
  id: totrans-450
  prefs: []
  type: TYPE_TB
  zh: '| pixel310 | 0.102667 | 4.060198 | 0 | 237 |'
- en: '*Table 1: Summary of features from the original dataset before pre-processing*'
  id: totrans-451
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表1：预处理前原始数据集特征摘要*'
- en: The **Mixed National Institute of Standards and Technology** (**MNIST**) dataset
    is a widely used dataset for evaluating unsupervised learning methods. The MNIST
    dataset is mainly chosen because the clusters in high dimensional data are not
    well separated.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合国家标准与技术研究院**（**MNIST**）数据集是用于评估无监督学习方法的常用数据集。MNIST数据集之所以被选中，主要是因为高维数据中的簇没有很好地分离。'
- en: The original MNIST dataset had black and white images from NIST. They were normalized
    to fit in a 20 x 20 pixel box while maintaining the aspect ratio. The images were
    centered in a 28 x 28 image by computing the center of mass and translating it
    to position it at the center of the 28 x 28 dimension grid.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 原始MNIST数据集来自NIST的黑白图像。它们被归一化以适应20 x 20像素的框，同时保持宽高比。通过计算质心并将其平移到28 x 28维网格的中心，图像被定位在28
    x 28图像的中心。
- en: Each pixel is in a range from 0 to 255 based on the intensity. The 784 pixel
    values are flattened out and become a high dimensional feature set for each image.
    The following figure depicts a sample digit 3 from the data, with mapping to the
    grid where each pixel has an integer value from 0 to 255.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 每个像素的强度基于0到255的范围。784个像素值被展平，成为每个图像的高维特征集。以下图展示了数据中的一个样本数字3，以及映射到网格中，每个像素都有一个从0到255的整数值。
- en: The experiments described in this section are intended to show the application
    of unsupervised learning techniques to a well-known dataset. As was done in [Chapter
    2](ch02.html "Chapter 2. Practical Approach to Real-World Supervised Learning"),
    *Practical Approach to Real-World Supervised Learning* with supervised learning
    techniques, multiple experiments were carried out using several clustering and
    outlier methods. Results from experiments with and without feature reduction are
    presented for each of the selected methods followed by an analysis of the results.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中描述的实验旨在展示无监督学习技术在知名数据集上的应用。正如在[第2章](ch02.html "第2章。现实世界监督学习的实用方法")中所述，使用监督学习技术进行“现实世界监督学习的实用方法”，进行了多个实验，使用了多种聚类和异常值方法。每个选定方法的有无特征减少的实验结果都进行了展示，随后对结果进行了分析。
- en: Data sampling and transformation
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据采样和转换
- en: Since our focus is on exploring the dataset using various unsupervised techniques
    and not on the predictive aspect, we are not concerned with train, validation,
    and test samples here. Instead, we use the entire dataset to train the models
    to perform clustering analysis.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的重点是使用各种无监督技术探索数据集，而不是预测方面，所以我们在这里不关心训练、验证和测试样本。相反，我们使用整个数据集来训练模型以执行聚类分析。
- en: In the case of outlier detection, we create a reduced sample of only two classes
    of data, namely, 1 and 7\. The choice of a dataset with two similarly shaped digits
    was made in order to set up a problem space in which the discriminating power
    of the various anomaly detection techniques would stand out in greater relief.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在异常值检测的情况下，我们创建了一个仅包含两个数据类别的样本，即1和7。选择具有相似形状的数字数据集是为了设置一个问题空间，在这个空间中，各种异常检测技术的判别能力将更加突出。
- en: Feature analysis and dimensionality reduction
  id: totrans-459
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征分析和降维
- en: We demonstrate different feature analysis and dimensionality reduction methods—PCA,
    Random Projection, and IsoMap—using the Java API of the SMILE machine learning
    toolkit.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用SMILE机器学习工具包的Java API展示了不同的特征分析和降维方法——PCA、随机投影和IsoMap。
- en: '![Feature analysis and dimensionality reduction](img/B05137_03_209.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![特征分析和降维](img/B05137_03_209.jpg)'
- en: 'Figure 8: Showing digit 3 with pixel values distributed in a 28 by 28 matrix
    ranging from 0 to 254.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 图8：显示数字3，像素值分布在0到254的28 x 28矩阵中。
- en: 'The code for loading the dataset and reading the values is given here along
    with inline comments:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据集和读取值的代码在此处给出，并带有内联注释：
- en: '[PRE0]'
  id: totrans-464
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: PCA
  id: totrans-465
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PCA
- en: 'The following snippet illustrates dimensionality reduction achieved using the
    API for PCA support:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段说明了使用PCA支持API实现的降维：
- en: '[PRE1]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![PCA](img/B05137_03_210.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![PCA](img/B05137_03_210.jpg)'
- en: 'Figure 9: PCA on MNIST – On the left, we see that over 90 percent of variance
    in data is accounted for by fewer than half the original number of features; on
    the right, a representation of the data using the first two principal components.'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 图9：MNIST上的PCA – 在左侧，我们看到超过90%的数据方差由原始特征数量的一半以下解释；在右侧，使用前两个主成分表示数据。
- en: 'Table 2: Summary of set of 11 random features after PCA'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 表2：PCA后11个随机特征的汇总
- en: 'The PCA computation reduces the number of features to 274\. In the following
    table you can see basic statistics for a randomly selected set of features. Feature
    data has been normalized as part of the PCA:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: PCA计算将特征数量减少到274。在下面的表中，您可以看到随机选择的一组特征的基本统计信息。特征数据作为PCA的一部分进行了归一化：
- en: '| Features | Average | Std Dev | Min | Max |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | 平均值 | 标准差 | 最小值 | 最大值 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-473
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 0 | 2.982922 | -35.0821 | 19.73339 |'
  id: totrans-474
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 2.982922 | -35.0821 | 19.73339 |'
- en: '| 2 | 0 | 2.415088 | -32.6218 | 31.63361 |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 2.415088 | -32.6218 | 31.63361 |'
- en: '| 3 | 0 | 2.165878 | -21.4073 | 16.50271 |'
  id: totrans-476
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 2.165878 | -21.4073 | 16.50271 |'
- en: '| 4 | 0 | 1.78834 | -27.537 | 31.52653 |'
  id: totrans-477
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 1.78834 | -27.537 | 31.52653 |'
- en: '| 5 | 0 | 1.652688 | -21.4661 | 22.62837 |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0 | 1.652688 | -21.4661 | 22.62837 |'
- en: '| 6 | 0 | 1.231167 | -15.157 | 10.19708 |'
  id: totrans-479
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0 | 1.231167 | -15.157 | 10.19708 |'
- en: '| 7 | 0 | 0.861705 | -6.04737 | 7.220233 |'
  id: totrans-480
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0 | 0.861705 | -6.04737 | 7.220233 |'
- en: '| 8 | 0 | 0.631403 | -6.80167 | 3.633182 |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0 | 0.631403 | -6.80167 | 3.633182 |'
- en: '| 9 | 0 | 0.606252 | -5.46206 | 4.118598 |'
  id: totrans-482
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0 | 0.606252 | -5.46206 | 4.118598 |'
- en: '| 10 | 0 | 0.578355 | -4.21456 | 3.621186 |'
  id: totrans-483
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0 | 0.578355 | -4.21456 | 3.621186 |'
- en: '| 11 | 0 | 0.528816 | -3.48564 | 3.896156 |'
  id: totrans-484
  prefs: []
  type: TYPE_TB
  zh: '| 11 | 0 | 0.528816 | -3.48564 | 3.896156 |'
- en: '*Table 2: Summary of set of 11 random features after PCA*'
  id: totrans-485
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表2：PCA后11个随机特征的汇总*'
- en: Random projections
  id: totrans-486
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机投影
- en: 'Here, we illustrate the straightforward usage of the API for performing data
    transformation using random projection:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们展示了使用随机投影进行数据转换的API的简单用法：
- en: '[PRE2]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '![Random projections](img/B05137_03_211.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![随机投影](img/B05137_03_211.jpg)'
- en: 'Figure 10: PCA and Random projection - representations in two dimensions using
    Smile API'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 图10：PCA和随机投影 – 使用Smile API在二维中的表示
- en: ISOMAP
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ISOMAP
- en: 'This code snippet illustrates use of the API for Isomap transformation:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段说明了使用Isomap转换的API的用法：
- en: '[PRE3]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![ISOMAP](img/B05137_03_212.jpg)'
  id: totrans-494
  prefs: []
  type: TYPE_IMG
  zh: '![ISOMAP](img/B05137_03_212.jpg)'
- en: 'Figure 11: IsoMap – representation in two dimensions with k = 10 using Smile
    API'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 图11：IsoMap – 使用Smile API在k = 10的情况下在二维中的表示
- en: Observations on feature analysis and dimensionality reduction
  id: totrans-496
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 特征分析和降维观察
- en: 'We can make the following observations from the results shown in the plots:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从图中显示的结果中得出以下观察：
- en: The PCA variance and number of dimensions plot clearly shows that around 100
    linearly combined features have a similar representation or variance in the data
    (> 95%) as that of the 784 original features. This is the key first step in any
    unsupervised feature reduction analysis.
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）的方差和维度数量图清晰地显示，大约有100个线性组合的特征在数据中的表示或方差与原始的784个特征相似（> 95%）。这是任何无监督特征降维分析的关键第一步。
- en: Even PCA with two dimensions and not 100 as described previously shows some
    really good insights in the scatterplot visualization. Clearly, digits 2, 8, and
    4 are very well separated from each other and that makes sense as they are written
    quite distinctly from each other. Digits such as {1,7}, {3,0,5}, and {1,9} in
    the low dimensional space are either overlapping or tightly clustered. This shows
    that with just two features it is not possible to discriminate effectively. It
    also shows that there is overlap in the characteristics or features amongst these
    classes.
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使是两个维度而不是之前描述的100个维度，PCA也能在散点图可视化中显示出一些真正有价值的见解。显然，数字2、8和4彼此之间非常清晰地分开，这很合理，因为它们彼此之间写得很清晰。在低维空间中，如{1,7}、{3,0,5}和{1,9}这样的数字要么重叠，要么紧密聚集。这表明仅用两个特征是无法有效区分的。这也表明在这些类别之间存在着特征或属性的重叠。
- en: The next plot comparing PCA with Random Projections, both done in lower dimension
    of 2, shows that there is much in common between the outputs. Both have similar
    separation for distinct classes as described in PCA previously. It is interesting
    to note that PCA does much better in separating digits {8,9,4}, for example, than
    Random Projections.
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下一个图表比较了主成分分析（PCA）与随机投影在低维2维空间下的结果，显示两者输出之间有很多共同之处。它们在区分不同类别方面与之前在PCA中描述的相似。值得注意的是，PCA在区分数字{8,9,4}等方面比随机投影做得更好。
- en: Isomap, the next plot, shows good discrimination, similar to PCA. Subjectively,
    it seems to be separating the data better than Random Projections. Visually, for
    instance, {3,0,5} is better separated out in Isomap than PCA.
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Isomap的下一个图表显示了良好的区分度，类似于PCA。主观上，它似乎比随机投影更好地分离数据。例如，在Isomap中，{3,0,5}比在PCA中分离得更好。
- en: Clustering models, results, and evaluation
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类模型、结果和评估
- en: Two sets of experiments were conducted using the MNIST-6000 dataset. The dataset
    consists of 6,000 examples, each of which represents a hand-written digit as greyscale
    values of a 28 x 28 square of pixels.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 使用MNIST-6000数据集进行了两组实验。该数据集包含6,000个示例，每个示例代表一个手写数字，以28 x 28像素方格的灰度值表示。
- en: First, we run some clustering techniques to identify the 10 clusters of digits.
    For the experiments in this part of the case study, we use the software Elki.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们运行了一些聚类技术来识别数字的10个聚类。在本案例研究的这部分实验中，我们使用软件Elki。
- en: 'In the first set of experiments, there is no feature-reduction involved. All
    28x28 pixels are used. Clustering techniques including k-Means, EM (Diagonal Gaussian
    Model Factory), DBSCAN, Hierarchical (HDBSCAN Hierarchy Extraction), as well as
    Affinity Propagation were used. In each case, we use metrics from two internal
    evaluators: Davies Bouldin and Silhouette, and several external evaluators: Precision,
    Recall, F1 measure, and Rand Index.'
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一组实验中，没有涉及特征降维。所有28x28像素都被使用。使用了包括k-Means、EM（对角高斯模型工厂）、DBSCAN、层次结构（HDBSCAN层次提取）以及亲和传播在内的聚类技术。在每种情况下，我们使用两个内部评估器的指标：戴维斯-博尔丁和影子系数，以及几个外部评估器：精确度、召回率、F1度量以及兰德指数。
- en: '![Clustering models, results, and evaluation](img/B05137_03_213.jpg)'
  id: totrans-506
  prefs: []
  type: TYPE_IMG
  zh: '![聚类模型、结果和评估](img/B05137_03_213.jpg)'
- en: 'Figure 12: K-Means – using Sum of Squared Errors (SSE) to find optimal *k*,
    the number of clusters. An elbow in the curve, which is typically used to pick
    the optimal k value, is not particularly detectable in the plot.'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 图12：K-Means – 使用平方和误差（SSE）找到最优*k*，即聚类数量。曲线中的肘部，通常用于选择最优k值，在图表中并不特别明显。
- en: In the case of k-Means, we did several runs using a range of k values. The plot
    shows that the Sum of Squared Errors (SSE) metric decreases with k.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-Means的情况下，我们使用一系列k值进行了多次运行。图表显示，随着k值的增加，平方和误差（SSE）指标逐渐降低。
- en: 'The table shows results for *k=10* and ranks for each are in parentheses:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 表格显示了*k=10*的结果，每个结果对应的排名在括号中：
- en: '| Algorithm | Silhouette | Davies-Bouldin Index | Precision | Recall | F1 |
    Rand |'
  id: totrans-510
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 影子系数 | 戴维斯-博尔丁指数 | 精确度 | 召回率 | F1 | 兰德指数 |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| K-Means Lloyd | +-0.09 0.0737 (1) | 2.8489 (3) | 0.4463 (3) | 0.47843 (3)
    | 0.4618 (1) | 0.8881 (3) |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| K-Means Lloyd | +-0.09 0.0737 (1) | 2.8489 (3) | 0.4463 (3) | 0.47843 (3)
    | 0.4618 (1) | 0.8881 (3) |'
- en: '| EM (Diagonal Gaussian Model Factory) | NaN | 0 (1) | 0.1002 (6) | 1 (1) |
    0.1822 (4) | 0.1003 (5) |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| EM（对角高斯模型工厂） | NaN | 0 (1) | 0.1002 (6) | 1 (1) | 0.1822 (4) | 0.1003 (5)
    |'
- en: '| DBSCAN | 0 (4) | 0 (1) | 0.1003 (5) | 1 (1) | 0.1823 (3) | 0.1003 (5) |'
  id: totrans-514
  prefs: []
  type: TYPE_TB
  zh: '| DBSCAN | 0 (4) | 0 (1) | 0.1003 (5) | 1 (1) | 0.1823 (3) | 0.1003 (5) |'
- en: '| Hierarchical (HDBSCAN Hierarchy Extraction) | +-0.05 0.0435 (3) | 2.7294
    | 0.1632 (4) | 0.9151 (2) | 0.2770 (2) | 0.5211 (4) |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 层次结构（HDBSCAN层次提取） | +-0.05 0.0435 (3) | 2.7294 | 0.1632 (4) | 0.9151 (2)
    | 0.2770 (2) | 0.5211 (4) |'
- en: '| Hierarchical (Simplified Hierarchy Extraction) | NaN | 0 (1) | 1 (1) | 0.0017
    (5) | 0.0033 (6) | 0.8999 (2) |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| 层次结构（简化层次提取） | NaN | 0 (1) | 1 (1) | 0.0017 (5) | 0.0033 (6) | 0.8999 (2)
    |'
- en: '| Affinity Propagation | +-0.07 0.04690 (2) | 1.7872 (2) | 0.8279 (2) | 0.0281
    (4) | 0.0543 (5) | 0.9019 (1) |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 亲和传播 | +-0.07 0.04690 (2) | 1.7872 (2) | 0.8279 (2) | 0.0281 (4) | 0.0543
    (5) | 0.9019 (1) |'
- en: '*Table 3\. Evaluation of clustering algorithms for MNIST data*'
  id: totrans-518
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表3. MNIST数据的聚类算法评估*'
- en: 'In the second clustering experiment, the dataset was first pre-processed using
    PCA, and the resulting data with 273 features per example was used with the same
    algorithms as in the first experiment. The results are shown in the table:'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个聚类实验中，首先使用PCA对数据集进行了预处理，然后使用每个示例273个特征的所得数据，与第一个实验中相同的算法一起使用。结果如表所示：
- en: '| Algorithm | Silhouette | Davies-Bouldin Index | Precision | Recall | F1 |
    Rand |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | 影子系数 | 戴维斯-博尔丁指数 | 精确度 | 召回率 | F1 | Rand |'
- en: '| --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- |'
- en: '| K-Means Lloyd | +-0.14 0.0119 | 3.1830 | 0.3456 | 0.4418 | 0.3878 (1) | 0.8601
    |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| K-Means Lloyd | +-0.14 0.0119 | 3.1830 | 0.3456 | 0.4418 | 0.3878 (1) | 0.8601
    |'
- en: '| EM (Diagonal Gaussian Model Factory) | +-0.16 -0.0402 | 3.5429 | 0.1808 |
    0.3670 | 0.2422 | 0.7697 |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| EM（对角高斯模型工厂） | +-0.16 -0.0402 | 3.5429 | 0.1808 | 0.3670 | 0.2422 | 0.7697
    |'
- en: '| DBSCAN | +-0.13 -0.0351 | 1.3236 | 0.1078 | 0.9395 (1) | 0.1934 | 0.2143
    |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| DBSCAN | +-0.13 -0.0351 | 1.3236 | 0.1078 | 0.9395 (1) | 0.1934 | 0.2143
    |'
- en: '| Hierarchical (HDBSCAN Hierarchy Extraction) | +-0.05 0.7920 (1) | 0.0968
    | 0.1003 | 0.9996 | 0.1823 | 0.1005 |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 层次（HDBSCAN 层次提取） | +-0.05 0.7920 (1) | 0.0968 | 0.1003 | 0.9996 | 0.1823
    | 0.1005 |'
- en: '| Affinity Propagation | +-0.09 0.0575 | 1.6296 | 0.6130 (1) | 0.0311 | 0.0592
    | 0.9009 (1) |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 相似传播 | +-0.09 0.0575 | 1.6296 | 0.6130 (1) | 0.0311 | 0.0592 | 0.9009 (1)
    |'
- en: '| Subspace (DOC) | +-0.00 0.0 | 0 (1) | 0.1003 | 1 | 0.1823 | 0.1003 |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 子空间（DOC） | +-0.00 0.0 | 0 (1) | 0.1003 | 1 | 0.1823 | 0.1003 |'
- en: '*Table 4\. Evaluation of clustering algorithms for MNIST data after PCA*'
  id: totrans-528
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表4\. PCA后MNIST数据聚类算法的评估*'
- en: Observations and clustering analysis
  id: totrans-529
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察和聚类分析
- en: As shown in tables 2.1 and 2.2, different algorithms discussed in the sections
    on clustering are compared using different evaluation measures.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 如表2.1和表2.2所示，在聚类章节中讨论的不同算法使用不同的评估度量进行比较。
- en: 'Generally, comparing different internal and external measures based on technical,
    domain and business requirements is very important. When labels or outcomes are
    available in the dataset, using external measures becomes an easier choice. When
    labeled data is not available, the norm is to use internal measures with some
    ranking for each and looking at comparative ranking across all measures. The important
    and often interesting observations are made at this stage:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，根据技术、领域和业务需求比较不同的内部和外部度量标准非常重要。当数据集中有标签或结果时，使用外部度量标准成为一个更容易的选择。当没有标记数据时，通常使用内部度量标准，并对每个度量进行一些排名，然后查看所有度量之间的比较排名。在这个阶段，会做出一些重要且通常有趣的观察：
- en: Evaluating the performance of k-Means with varying *k*, (shown in the figure)
    using a measure such as Sum of Squared Errors, is the basic step to see "optimality"
    of number of clusters. The figure clearly shows that as *k* increases the score
    improves as cluster separation improves.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用如平方和误差等度量评估k-Means的k值（如图所示），是查看“最优性”的聚类数量的基本步骤。图清楚地表明，随着k的增加，分数提高，聚类分离度提高。
- en: 'When we analyze Table 2.1 where all 784 features were used and all evaluation
    measures for the different algorithms are shown, some key things stand out:'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们分析表2.1时，其中使用了所有784个特征，并显示了不同算法的所有评估度量，一些关键点脱颖而出：
- en: k-Means and Affinity Propagation both show a large overlap in the Silhouette
    index in terms of standard deviation and average, respectively (k-Means +-0.09
    0.0737; Affinity Propagation +-0.07 0.04690). Hence it is difficult to analyze
    them on this metric.
  id: totrans-534
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-Means和相似传播在标准差和平均值方面在影子系数指标上都有很大的重叠（k-Means +-0.09 0.0737；相似传播 +-0.07 0.04690）。因此，很难在这个指标上分析它们。
- en: In the measures such as DB Index (minimal is good), Rand Index (closer to 1
    is good), we see that Affinity Propagation and Hierarchical Clustering show very
    good results.
  id: totrans-535
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DB Index（最小值越好）、Rand Index（越接近1越好）等度量中，我们可以看到相似传播和层次聚类显示出非常好的结果。
- en: In the measures where the labels are taken into account, Hierarchical Clustering,
    DBSCAN, and EM has either high Precision or high Recall and consequently, the
    F1 measure is low. k-Means gives the highest F1 measure when precision and recall
    are taken into consideration.
  id: totrans-536
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在考虑标签的度量中，层次聚类、DBSCAN和EM要么具有高精确度，要么具有高召回率，因此F1度量较低。当考虑精确度和召回率时，k-Means给出了最高的F1度量。
- en: 'In Table 2.2 where the dataset with 273 features—reduced using PCA with 95%
    variance retained—is run through the same algorithms and evaluated by the same
    measures, we make the following interesting observations:'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表2.2中，使用PCA保留95%方差减少的273个特征的数据集通过相同的算法运行并通过相同的度量进行评估，我们得出以下有趣的观察：
- en: By reducing the features there is a negative impact on every measure for certain
    algorithms; for example, all the measures of k-Means degrade. An algorithm such
    as Affinity Propagation has a very low impact and in some cases even a positive
    impact when using reduced features. When compared to the results where all the
    features were used, AP shows similar Rand Index and F1, better Recall, DB Index
    and Silhouette measures, and small changes in Precision, demonstrating clear robustness.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 减少特征会对某些算法的每个度量指标产生负面影响；例如，k-Means的所有度量指标都会降低。例如，亲和传播算法在减少特征时影响非常小，在某些情况下甚至有积极的影响。与使用所有特征的结果相比，AP显示出相似的Rand指数和F1，更好的召回率、DB索引和轮廓度量，以及精度的小幅变化，显示出明显的鲁棒性。
- en: Hierarchical Clustering shows similar results as before in terms of better DB
    index and Rand Index, and scores close to AP in Rand Index.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类在更好的DB索引和Rand指数方面显示出与之前相似的结果，并且在Rand指数上的得分接近AP。
- en: Outlier models, results, and evaluation
  id: totrans-540
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值模型、结果和评估
- en: For the outlier detection techniques, we used a subset of the original dataset
    containing all examples of digit 1 and an under-sampled subset of digit 7 examples.
    The idea is that the similarity in shape of the two digits would cause the digit
    7 examples to be found to be outliers.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 对于异常值检测技术，我们使用了包含所有数字1示例和数字7示例的子集，以及数字7示例的欠采样子集。其想法是这两个数字的形状相似性会导致数字7示例被识别为异常值。
- en: The models used were selected from Angular, Distance-based, clustering, LOF,
    and One-Class SVM.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 使用的模型来自Angular、基于距离的聚类、LOF和单类SVM。
- en: The outlier metrics used in the evaluation were ROC AUC, Average Precision,
    R-Precision, and Maximum F1 measure.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 评估中使用的异常值度量指标包括ROC AUC、平均精度、R-精度和最大F1度量。
- en: 'The following table shows the results obtained, with ranks in parentheses:'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 下表显示了获得的结果，括号内为排名：
- en: '| Algorithm | ROC AUC | Avg. Precision | R-Precision | Maximum F1 |'
  id: totrans-545
  prefs: []
  type: TYPE_TB
  zh: '| 算法 | ROC AUC | 平均精度 | R-精度 | 最大F1 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-546
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Angular (ABOD) | 0.9515 (3) | 0.1908 (4) | 0.24 (4) | 0.3298 (4) |'
  id: totrans-547
  prefs: []
  type: TYPE_TB
  zh: '| 角度（ABOD） | 0.9515 (3) | 0.1908 (4) | 0.24 (4) | 0.3298 (4) |'
- en: '| Distance-based (KNN Outlier) | 0.9863 (1) | 0.4312 (3) | 0.4533 (3) | 0.4545
    (3) |'
  id: totrans-548
  prefs: []
  type: TYPE_TB
  zh: '| 基于距离（KNN异常值） | 0.9863 (1) | 0.4312 (3) | 0.4533 (3) | 0.4545 (3) |'
- en: '| Distance Based (Local Isolation Coefficient) | 0.9863 (1) | 0.4312 (3) |
    0.4533 (3) | 0.4545 (3) |'
  id: totrans-549
  prefs: []
  type: TYPE_TB
  zh: '| 基于距离（局部孤立系数） | 0.9863 (1) | 0.4312 (3) | 0.4533 (3) | 0.4545 (3) |'
- en: '| Clustering (EM Outlier) | 0.5 (5) | 0.97823827 (1) | 0.989 (1) | 0.9945 (1)
    |'
  id: totrans-550
  prefs: []
  type: TYPE_TB
  zh: '| 聚类（EM异常值） | 0.5 (5) | 0.97823827 (1) | 0.989 (1) | 0.9945 (1) |'
- en: '| LOF | 0.4577 (6) | 0.0499 (6) | 0.08 (6) | 0.0934 (6) |'
  id: totrans-551
  prefs: []
  type: TYPE_TB
  zh: '| LOF | 0.4577 (6) | 0.0499 (6) | 0.08 (6) | 0.0934 (6) |'
- en: '| LOF (ALOKI) | 0.5 (5) | 0.0110 (7) | 0.0110 (7) | 0.0218 (7) |'
  id: totrans-552
  prefs: []
  type: TYPE_TB
  zh: '| LOF（ALOKI） | 0.5 (5) | 0.0110 (7) | 0.0110 (7) | 0.0218 (7) |'
- en: '| LOF (COF) | 0.4577 (6) | 0.0499 (6) | 0.08 (6) | 0.0934 (6) |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '| LOF（COF） | 0.4577 (6) | 0.0499 (6) | 0.08 (6) | 0.0934 (6) |'
- en: '| One-Class SVM (RBF) | 0.9820 (2) | 0.5637 (2) | 0.5333 (2) | 0.5697 (2) |'
  id: totrans-554
  prefs: []
  type: TYPE_TB
  zh: '| 单类SVM（RBF） | 0.9820 (2) | 0.5637 (2) | 0.5333 (2) | 0.5697 (2) |'
- en: '| One-Class SVM (Linear) | 0.8298 (4) | 0.1137 (5) | 0.16 (5) | 0.1770 (5)
    |'
  id: totrans-555
  prefs: []
  type: TYPE_TB
  zh: '| 单类SVM（线性） | 0.8298 (4) | 0.1137 (5) | 0.16 (5) | 0.1770 (5) |'
- en: '*Table 5 Evaluation measures of Outlier analysis algorithms*'
  id: totrans-556
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*表5异常值分析算法评估度量*'
- en: Observations and analysis
  id: totrans-557
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 观察和分析
- en: 'In the same way as we evaluated different clustering methods, we used several
    observations to compare a number of outlier algorithms. Once again, the right
    methodology is to judge an algorithm based on ranking across all the metrics and
    then getting a sense of how it does across the board as compared to other algorithms.
    The outlier metrics used here are all standard external measures used to compare
    outlier algorithms:'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们评估不同的聚类方法一样，我们使用了一些观察结果来比较多个异常值算法。再次强调，正确的方法是根据所有指标对算法进行排名，然后了解它在所有指标上的表现与其他算法相比如何。这里使用的异常值度量指标都是用于比较异常值算法的标准外部度量：
- en: It is interesting to see with the right parameters, that is, *k=2*, EM can find
    the right distribution and find outliers more efficiently than most. It ranks
    very high and is first among the important metrics that include Maximum F1, R-Precision,
    and Avg. Precision.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有趣的是，当使用正确的参数，即*k=2*时，EM可以找到正确的分布，并且比大多数算法更有效地找到异常值。它在包括最大F1、R-精度和平均精度等重要指标中排名很高，位居第一。
- en: 1-Class SVM with non-linear RBF Kernel does consistently well across most measures,
    that is, ranks second best in ROC area, R-Precision and Avg. Precision, and Maximum
    F1\. The difference between Linear SVM, which ranks about fifth in most rankings
    and 1-Class SVM, which ranks second shows that the problem is indeed nonlinear
    in nature. Generally, when the dimensions are high (784), and outliers are nonlinear
    and rare, 1-Class SVM with kernels do really well.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1-Class SVM使用非线性RBF核在大多数指标上表现一致良好，即在ROC面积、R-Precision、平均精度和最大F1上排名第二。线性SVM（在大多数排名中大约排名第五）和1-Class
    SVM（排名第二）之间的差异表明，问题在本质上确实是非线性的。一般来说，当维度较高（784）且异常值非线性且稀有时，带有核的1-Class SVM表现确实很好。
- en: Local outlier-based techniques (LOF and its variants) are consistently ranked
    lower in almost all the measures. This gives the insight that the outlier problem
    may not be local, but rather global. Distance-based algorithms (KNN and Local
    Isolation) perform the best in ROC area under the curve and better than local
    outlier-based, even though using distance-based metrics gives the insight that
    the problem is indeed global and suited for distance-based measures.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于局部异常值（LOF）及其变体的技术几乎在所有指标中排名都较低。这表明异常值问题可能不是局部的，而是全局的。基于距离的算法（KNN和局部隔离）在ROC曲线下的面积和局部异常值算法相比表现更好，尽管使用基于距离的度量可以得出问题确实是全局性的，适合使用基于距离的度量。
- en: 'Table 1: Summary of features from the original dataset before pre-processing'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 表1：预处理前原始数据集特征摘要
- en: Summary
  id: totrans-563
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Both supervised and unsupervised learning methods share common concerns with
    respect to noisy data, high dimensionality, and demands on memory and time as
    the size of data grows. Other issues peculiar to unsupervised learning, due to
    the lack of ground truth, are questions relating to subjectivity in the evaluation
    of models and their interpretability, effect of cluster boundaries, and so on.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习和无监督学习方法在处理噪声数据、高维度以及随着数据规模增长对内存和时间的需求方面存在共同关注的问题。由于缺乏真实情况，无监督学习特有的其他问题是关于模型评估的主观性、模型的可解释性、聚类边界的影响等问题。
- en: Feature reduction is an important preprocessing step that mitigates the scalability
    problem, in addition to presenting other advantages. Linear methods such as PCA,
    Random Projection, and MDS, each have specific benefits and limitations, and we
    must be aware of the assumptions inherent in each. Nonlinear feature reduction
    methods include KPCA and Manifold learning.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 特征降维是一个重要的预处理步骤，它减轻了可扩展性问题，同时还具有其他优势。线性方法，如PCA、随机投影和MDS，各自具有特定的优点和局限性，我们必须意识到每个方法中固有的假设。非线性特征降维方法包括KPCA和流形学习。
- en: Among clustering algorithms, k-Means is a centroid-based technique initialized
    by selecting the number of clusters and it is sensitive to the initial choice
    of centroids. DBSCAN is one of the density-based algorithms that does not need
    initializing with number of clusters and is robust against noise and outliers.
    Among the probabilistic-based techniques are Mean Shift, which is deterministic
    and robust to noise, and EM/GMM, which performs well with all types of features.
    Both Mean Shift and EM/GMM tend to have scalability problems.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚类算法中，k-Means是一种基于质心的技术，通过选择聚类数量进行初始化，它对质心的初始选择敏感。DBSCAN是密度基于算法之一，不需要初始化聚类数量，对噪声和异常值具有鲁棒性。在基于概率的技术中包括均值漂移，它是确定性的且对噪声鲁棒，以及EM/GMM，它在所有类型的特征上表现良好。均值漂移和EM/GMM都倾向于存在可扩展性问题。
- en: Hierarchical clustering is a powerful method involving building binary trees
    that iteratively groups data points until a similarity threshold is reached. Tolerance
    to noise depends on the similarity metric used. SOM is a two-layer neural network,
    allowing visualization of clusters in a 2-D grid. Spectral clustering treats the
    dataset as a connected graph and identifies clusters by graph partitioning. Affinity
    propagation, another graph-based technique, uses message passing between data
    points as affinities to detect clusters.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类是一种强大的方法，涉及构建二叉树，迭代地将数据点分组，直到达到相似性阈值。对噪声的容忍度取决于使用的相似性度量。SOM是一个双层神经网络，允许在二维网格中可视化聚类。谱聚类将数据集视为一个连通图，通过图划分来识别聚类。亲和传播是另一种基于图的技巧，它使用数据点之间的消息传递作为亲和力来检测聚类。
- en: The validity and usefulness of clustering algorithms is demonstrated using various
    validation and evaluation measures. Internal measures have no access to ground
    truth; when labels are available, external measures can be used. Examples of internal
    measures are Silhouette index and Davies-Bouldin index. Rand index and F-measure
    are external evaluation measures.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 使用各种验证和评估度量来证明聚类算法的有效性和实用性。内部度量无法访问真实标签；当标签可用时，可以使用外部度量。内部度量的例子有轮廓指数和Davies-Bouldin指数。Rand指数和F度量是外部评估度量。
- en: Outlier and anomaly detection is an important area of unsupervised learning.
    Techniques are categorized as Statistical-based, Distance-based, Density-based,
    Clustering-based, High-dimensional-based, and One Class SVM. Outlier evaluation
    techniques include supervised evaluation, where ground truth is known, and unsupervised
    evaluation, when ground truth is not known.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测和异常检测是无监督学习的一个重要领域。技术被分为基于统计、基于距离、基于密度、基于聚类、高维和单类SVM。异常评估技术包括有监督评估，其中已知真实标签，和无监督评估，当真实标签未知时。
- en: Experiments using the SMILE Java API and Elki toolkit illustrate the use of
    the various clustering and outlier detection techniques on the MNIST6000 handwritten
    digits dataset. Results from different evaluation techniques are presented and
    compared.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SMILE Java API和Elki工具包进行的实验说明了在MNIST6000手写数字数据集上使用各种聚类和异常检测技术。不同评估技术的结果被展示和比较。
- en: References
  id: totrans-571
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: K. Pearson (1901). *On lines and planes of closest fit to systems of points
    in space*. Philosophical Magazine, 2:559–572.
  id: totrans-572
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: K. Pearson (1901). *关于空间中点系统的最接近拟合的线和面*. 哲学杂志，第2卷：559–572。
- en: A. D. Back (1997). "*A first application of independent component analysis to
    extracting structure from stock returns*," Neural Systems, vol. 8, no. 4, pp.
    473–484.
  id: totrans-573
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: A. D. Back (1997). "*独立成分分析在从股票回报中提取结构中的应用*"，神经系统，第8卷，第4期，第473–484页。
- en: Tipping ME, Bishop CM (1999). *Probabilistic principal component analysis*.
    Journal of the Royal Statistical Society, Series B, 61(3):611–622\. 10.1111/1467-9868.00196
  id: totrans-574
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tipping ME，Bishop CM (1999). *概率主成分分析*. 英国皇家统计学会会刊，系列B，第61卷，第3期：611–622\. 10.1111/1467-9868.00196
- en: Sanjoy Dasgupta (2000). *Experiments with random projection*. In Proceedings
    of the Sixteenth conference on Uncertainty in artificial intelligence (UAI'00),
    Craig Boutilier and Moisés Goldszmidt (Eds.). Morgan Kaufmann Publishers Inc.,
    San Francisco, CA, USA, 143-151.
  id: totrans-575
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sanjoy Dasgupta (2000). *随机投影实验*. 在第十六届不确定人工智能会议（UAI'00）论文集中，由Craig Boutilier和Moisés
    Goldszmidt（编者）. Morgan Kaufmann Publishers Inc.，旧金山，加利福尼亚州，美国，第143-151页。
- en: T. Cox and M. Cox (2001). *Multidimensional Scaling*. Chapman Hall, Boca Raton,
    2nd edition.
  id: totrans-576
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: T. Cox和M. Cox (2001). *多维尺度*. Chapman Hall，博卡拉顿，第2版。
- en: Bernhard Schoelkopf, Alexander J. Smola, and Klaus-Robert Mueller (1999). *Kernel
    principal component analysis*. In Advances in kernel methods, MIT Press, Cambridge,
    MA, USA 327-352.
  id: totrans-577
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Bernhard Schoelkopf，Alexander J. Smola和Klaus-Robert Mueller (1999). *核主成分分析*.
    在核方法进展，麻省理工学院出版社，剑桥，马萨诸塞州，美国 327-352。
- en: Tenenbaum, J.B.; De Silva, V.; & Langford, J.C (2000).*A global geometric framework
    for nonlinear dimensionality reduction*. Science. Vol. 290, Issue 5500, pp. 2319-2323
  id: totrans-578
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Tenenbaum, J.B.; De Silva, V.; & Langford, J.C (2000).*全局非线性降维的几何框架*. 科学. 第290卷，第5500期，第2319-2323页
- en: M. Belkin and P. Niyogi (2003). *Laplacian eigenmaps for dimensionality reduction
    and data representation*. Neural Computation, 15(6):1373–1396.
  id: totrans-579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Belkin and P. Niyogi (2003). *拉普拉斯特征映射用于降维和数据表示*. 神经计算，第15卷，第6期：1373–1396。
- en: S. Roweis and L. Saul (2000). *Nonlinear dimensionality reduction by locally
    linear embedding*. Science, 290:2323–2326.
  id: totrans-580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S. Roweis and L. Saul (2000). *局部线性嵌入进行非线性降维*. 科学，第290期：2323–2326。
- en: 'Hartigan, J. and Wong, M (1979). *Algorithm AS136: A k-means clustering algorithm*.
    Applied Statistics, 28, 100-108.'
  id: totrans-581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Hartigan, J. and Wong, M (1979). *算法AS136：k均值聚类算法*. 应用统计学，第28卷，第100-108页。
- en: 'Dorin Comaniciu and Peter Meer (2002). *Mean Shift: A robust approach toward
    feature space analysis*. IEEE Transactions on Pattern Analysis and Machine Intelligence
    pp. 603-619.'
  id: totrans-582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Dorin Comaniciu和Peter Meer (2002). *均值漂移：一种稳健的特征空间分析方法*. IEEE Transactions on
    Pattern Analysis and Machine Intelligence，第24卷，第603-619页。
- en: Hierarchical Clustering Jain, A. and Dubes, R (1988). *Algorithms for Clustering
    Data*. Prentice-Hall, Englewood Cliffs, NJ.
  id: totrans-583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 层次聚类 Jain, A. and Dubes, R (1988). *数据聚类算法*. Prentice-Hall，恩格尔伍德克利夫斯，新泽西州。
- en: 'Mclachlan, G. and Basford, K (1988). *Mixture Models: Inference and Applications
    to Clustering*. Marcel Dekker, New York, NY'
  id: totrans-584
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Mclachlan, G. and Basford, K (1988). *混合模型：推理及其在聚类中的应用*. Marcel Dekker，纽约，纽约州
- en: Ester, M., Kriegel, H-P., Sander, J. and Xu, X (1996). *A density-based algorithm
    for discovering clusters in large spatial databases with noise*. In Proceedings
    of the 2^(nd) ACM SIGKDD, 226-231, Portland, Oregon.
  id: totrans-585
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ester, M., Kriegel, H-P., Sander, J. 和 Xu, X (1996). *在大型空间数据库中带有噪声的发现聚类的一个基于密度的算法*.
    在第 2 届 ACM SIGKDD 会议的论文集中，第 226-231 页，俄勒冈州波特兰.
- en: 'Y. Ng, M. I. Jordan, and Y. Weiss (2001). *On spectral clustering: Analysis
    and an algorithm*, in Advances in Neural Information Processing Systems. MIT Press,
    pp. 849–856.'
  id: totrans-586
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Y. Ng, M. I. Jordan, 和 Y. Weiss (2001). *关于谱聚类：分析和算法*, 在神经信息处理系统进展中。麻省理工学院出版社，第
    849–856 页.
- en: Delbert Dueck and Brendan J. Frey (2007). *Non-metric affinity propagation for
    unsupervised image categorization*. In IEEE Int. Conf. Computer Vision (ICCV),
    pages 1–8.
  id: totrans-587
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Delbert Dueck 和 Brendan J. Frey (2007). *非度量亲和传播用于无监督图像分类*. 在 IEEE 国际计算机视觉会议（ICCV）中，第
    1–8 页.
- en: Teuvo Kohonen (2001). *Self-Organizing Map*. Springer, Berlin, Heidelberg. 1995.Third,
    Extended Edition.
  id: totrans-588
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Teuvo Kohonen (2001). *自组织映射*. Springer, Berlin, Heidelberg. 1995. 第三版，扩展版.
- en: M. Halkidi, Y. Batistakis, and M. Vazirgiannis (2001). *On clustering validation
    techniques*, J. Intell. Inf. Syst., vol. 17, pp. 107–145.
  id: totrans-589
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Halkidi, Y. Batistakis, 和 M. Vazirgiannis (2001). *关于聚类验证技术*, 智能信息系统杂志，第
    17 卷，第 107–145 页.
- en: 'M. Markou, S. Singh (2003). *Novelty detection: a review – part 1: statistical
    approaches*, Signal Process. 83 (12) 2481–2497'
  id: totrans-590
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: M. Markou, S. Singh (2003). *新颖性检测：综述 – 第一部分：统计方法*, 信号处理 83 (12) 2481–2497
- en: Byers, S. D. AND Raftery, A. E (1998). *Nearest neighbor clutter removal for
    estimating features in spatial point processes*. J. Amer. Statis. Assoc. 93, 577–584.
  id: totrans-591
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Byers, S. D. 和 Raftery, A. E (1998). *空间点过程特征估计中的最近邻杂波去除*. 美国统计学会杂志 93, 577–584.
- en: 'Breunig, M. M., Kriegel, H.-P., Ng, R. T., AND Sander, J (1999). *Optics-of:
    Identifying local outliers*. In Proceedings of the 3rd European Conference on
    Principles of Data Mining and Knowledge Discovery. Springer-Verlag, 262–270.'
  id: totrans-592
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'Breunig, M. M., Kriegel, H.-P., Ng, R. T., 和 Sander, J (1999). *Optics-of:
    识别局部异常值*. 在第三届欧洲数据挖掘和知识发现原则会议的论文集中，Springer-Verlag，第 262–270 页.'
- en: Brito, M. R., Chavez, E. L., Quiroz, A. J., AND yukich, J. E (1997). *Connectivity
    of the mutual k-nearest neighbor graph in clustering and outlier detection*. Statis.
    Prob. Lett. 35, 1, 33–42.
  id: totrans-593
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Brito, M. R., Chavez, E. L., Quiroz, A. J., 和 yukich, J. E (1997). *聚类和异常值检测中互为
    k 近邻图的连通性*. 统计概率信函 35, 1, 33–42.
- en: Aggarwal C and Yu P S (2000). *Outlier detection for high dimensional data*.
    In Proc ACM SIGMOD International Conference on Management of Data (SIGMOD), Dallas,
    TX.
  id: totrans-594
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Aggarwal C 和 Yu P S (2000). *高维数据中的异常值检测*. 在 ACM SIGMOD 国际数据管理会议（SIGMOD）的论文集中，德克萨斯州达拉斯.
- en: Ghoting, A., Parthasarathy, S., and Otey, M (2006). *Fast mining of distance-based
    outliers in high dimensional spaces* In Proceedings SIAM Int Conf on Data Mining
    (SDM) Bethesda ML dimensional spaces. In Proc. SIAM Int. Conf. on Data Mining
    (SDM), Bethesda, ML.
  id: totrans-595
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Ghoting, A., Parthasarathy, S., 和 Otey, M (2006). *基于距离的高维空间中快速挖掘异常值* 在 SIAM
    国际数据挖掘会议（SDM）的论文集中，Bethesda, ML. 在 SIAM 国际数据挖掘会议（SDM）的论文集中，Bethesda, ML.
- en: Kriegel, H.-P., Schubert, M., and Zimek, A (2008). *Angle-based outlier detection*,
    In Proceedings ACM SIGKDD Int. Conf on Knowledge Discovery and Data Mining (SIGKDD)
    Las Vegas NV Conf. on Knowledge Discovery and Data Mining (SIGKDD), Las Vegas,
    NV.
  id: totrans-596
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Kriegel, H.-P., Schubert, M., 和 Zimek, A (2008). *基于角度的异常值检测*, 在 ACM SIGKDD
    国际知识发现和数据挖掘会议（SIGKDD）的论文集中，内华达州拉斯维加斯. 在 ACM SIGKDD 国际知识发现和数据挖掘会议（SIGKDD）的论文集中，内华达州拉斯维加斯.
- en: Schoelkopf, B., Platt, J. C., Shawe-Taylor, J. C., Smola, A. J., AND Williamson,
    R. C (2001). *Estimating the support of a high-dimensional distribution*. Neural
    Comput. 13, 7, 1443–1471.
  id: totrans-597
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Schoelkopf, B., Platt, J. C., Shawe-Taylor, J. C., Smola, A. J., 和 Williamson,
    R. C (2001). *估计高维分布的支持*. 神经计算 13, 7, 1443–1471.
- en: 'F Pedregosa, et al. *Scikit-learn: Machine learning in Python*. Journal of
    Machine Learning Research, 2825-2830.'
  id: totrans-598
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 'F Pedregosa, 等人. *Scikit-learn: Python 中的机器学习*. 机器学习研究杂志，第 2825-2830 页.'
