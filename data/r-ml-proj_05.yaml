- en: Customer Segmentation Using Wholesale Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用批发数据进行的客户细分
- en: In today's competitive world, the success of an organization largely depends
    on how much it understands its customers' behavior. Understanding each customer
    individually to better tailor the organizational effort to individual needs is
    a very expensive task. Based on the size of the organization, this task can be
    very challenging as well. As an alternative, organizations rely on something called
    **segmentation**, which attempts to categorize customers into groups based on
    identified similarities. This critical aspect of customer segmentation allows
    organizations to extend their efforts to the individual needs of various customer
    subsets (if not catering to individual needs), therefore reaping greater benefits.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今竞争激烈的世界里，一个组织的成功在很大程度上取决于它对其客户行为的理解程度。为了更好地调整组织努力以适应个人需求，理解每个客户都是一项非常昂贵的任务。根据组织的大小，这项任务可能也非常具有挑战性。作为替代方案，组织依赖于一种称为**细分**的方法，该方法试图根据已识别的相似性将客户分类到不同的群体中。客户细分的关键方面允许组织将其努力扩展到各种客户子集的个人需求（如果不仅仅是满足个人需求），从而获得更大的收益。
- en: 'In this chapter, we will learn about the concept and importance of customer
    segmentation. We''ll then deep dive into learning the various **machine learning**
    (**ML**) methods to identify subgroups of customers based on customer characteristics.
    We''ll implement several projects using the wholesale dataset to understand the
    ML techniques for segmentation. In the next section, we''ll start by learning
    the foundations of customer segmentation and the need for ML techniques to achieve
    segmentation. We will cover the following topics as we progress:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习客户细分的概念和重要性。然后，我们将深入了解学习基于客户特征的客户子群体识别的各种**机器学习**（**ML**）方法。我们将使用批发数据集实施几个项目，以了解细分技术的机器学习技术。在下一节中，我们将从学习客户细分的基础和实现细分所需的机器学习技术的基础开始。随着我们的进展，我们将涵盖以下主题：
- en: Understanding customer segmentation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解客户细分
- en: Understanding the wholesale customer dataset and the segmentation problem
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解批发客户数据集和细分问题
- en: Identifying the customer segments in wholesale customer data using DIANA
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用DIANA在批发客户数据中识别客户细分
- en: Identifying the customer segments in wholesale customer data using AGNES
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用AGNES在批发客户数据中识别客户细分
- en: Understanding customer segmentation
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解客户细分
- en: 'Customer segmentation, or market segmentation, at a basic level, is the partitioning
    of a broad range of potential customers in a given market into specific subgroups
    of customers, where each of the subgroups contains customers that share certain
    similarities. The following diagram depicts the formal definition of customer
    segmentation where customers are identified into three groups:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本层面上，客户细分或市场细分是将特定市场中的广泛潜在客户划分为特定的客户子群体，其中每个子群体都包含具有某些相似性的客户。以下图表展示了客户细分的正式定义，其中客户被划分为三个群体：
- en: '![](img/674f3a2b-403c-4d6b-adf4-f7668ed5c29f.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/674f3a2b-403c-4d6b-adf4-f7668ed5c29f.png)'
- en: Illustration depicting customer segmentation definition
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 描述客户细分定义的插图
- en: 'Customer segmentation needs the organizations to gather data about customers
    and analyze it to identify patterns that can be used to determine subgroups. The
    segmentation of customers could be achieved through multiple data points related
    to customers. The following are some of the data points:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 客户细分需要组织收集有关客户的数据并分析它，以识别可用于确定子群体的模式。客户的细分可以通过与客户相关的多个数据点来实现。以下是一些数据点：
- en: '**Demographics**: This data point includes race, ethnicity, age, gender, religion,
    level of education, income, life stage, marital status, occupation'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**人口统计学**：这个数据点包括种族、民族、年龄、性别、宗教、教育水平、收入、生活阶段、婚姻状况、职业'
- en: '**Psychographics**: This data point includes lifestyle, values, socioeconomic
    standing, personality'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**心理统计学**：这个数据点包括生活方式、价值观、社会经济地位、个性'
- en: '**Behavioral**: This data point includes product usage, loyalties, awareness,
    occasions, knowledge, liking, and purchase patterns'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**行为学**：这个数据点包括产品使用、忠诚度、意识、场合、知识、喜好和购买模式'
- en: 'With billions of people in the world, efficiently making use of customer segmentation
    will help organizations narrow down the pool and reach only the people that mean
    something to their business, ultimately driving conversions and revenue. The following
    are some of the specific objectives that organizations attempt to achieve through
    identifying segments in their customers:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在世界上有数十亿人口的情况下，有效地利用客户细分可以帮助组织缩小范围，仅针对对其业务有意义的客户群体，最终推动转化和收入。以下是一些组织通过识别客户细分所试图实现的具体目标：
- en: Identifying higher-percentage opportunities that the sales team can pursue
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别销售团队能够追求的更高比例的机会
- en: Identifying customer groups that have a higher interest in the product, and
    customize the product according to the needs of high-interest customers
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别对产品有更高兴趣的客户群体，并根据高兴趣客户的需要定制产品
- en: Developing very focused marketing messages to specific customer groups so as
    to drive higher-quality inbound interest in the product
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发针对特定客户群体的非常专注的营销信息，以推动对产品的更高质量的外部兴趣
- en: Choosing the best communication channel for various segments, which might be
    email, social media, radio, or another approach, depending on the segment
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最适合各个细分市场的沟通渠道，这可能包括电子邮件、社交媒体、广播或其他方法，具体取决于细分市场
- en: Concentrating on the most profitable customers
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 专注于最有利可图的客户
- en: Upselling and cross-selling other products and services
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推销和交叉销售其他产品和服务
- en: Test pricing options
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试定价选项
- en: Identifying new product or service opportunities
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别新产品或服务的机会
- en: When an organization needs to perform segmentation, it can typically look for
    common characteristics, such as shared needs, common interests, similar lifestyles,
    or even similar demographic profiles and come up with segments in customer data.
    Unfortunately, creating segments is not that simple. With the availability of
    big data, organizations now have hundreds of characteristics of customers they
    can look at in order to come up with segments. It is not feasible for a person
    or few people in an organization to go through hundreds of types of data, find
    relationships between each of them, and then establish segments based on several
    different values possible for each data point. That's where unsupervised ML, called
    **clustering**, comes to rescue.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个组织需要执行细分时，它通常可以寻找共同的特征，例如共享需求、共同兴趣、相似的生活方式，甚至相似的人口统计特征，并在客户数据中制定细分。不幸的是，创建细分并不那么简单。随着大数据的出现，组织现在可以查看数百个客户特征，以便制定细分。一个人或组织中的少数人去处理数百种类型的数据，找出它们之间的关系，然后根据每个数据点的不同值建立细分是不切实际的。这就是无监督机器学习，称为**聚类**，发挥作用的地方。
- en: Clustering is the mechanism of using ML algorithms to identify relationships
    in different types of data, thereby yielding new segments based on those relationships.
    Simply put, clustering finds the relationship between data points so they can
    be segmented.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是使用机器学习算法识别不同类型数据之间关系的机制，从而基于这些关系产生新的细分。简单来说，聚类找到数据点之间的关系，以便它们可以被细分。
- en: The terms **cluster analysis** and **customer segmentation** are closely related
    and used interchangeably by ML practitioners. However, there is an important difference
    between these terms.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 术语**聚类分析**和**客户细分**在机器学习从业者中密切相关，并且可以互换使用。然而，这两个术语之间存在一个重要的区别。
- en: 'Clustering is a tool that helps organizations put together data based on similarities
    and statistical connections. Clustering is very helpful in guiding the development
    of suitable customer segments. It also provides useful statistical measures of
    the potential target customers. While the objective for an organization is to
    identify effective customer segments from data, simply applying a clustering technique
    on data and grouping the data in itself may or may not offer effective customer
    segments. This essentially means that the output obtained from clustering, that
    is, the **clusters**, need to be further analyzed to get insight into the meaning
    of each of the clusters, and then determine which clusters can be utilized for
    downstream activities, such as business promotions. The following is a flow diagram
    that helps us to understand the role of clustering in the customer-segmentation
    process:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类是一种帮助组织根据相似性和统计关系整理数据的工具。聚类在指导开发合适的客户细分方面非常有帮助。它还提供了潜在目标客户的 useful 统计指标。虽然组织的目的是从数据中识别有效的客户细分，但仅仅在数据上应用聚类技术并将数据进行分组本身可能或可能不会提供有效的客户细分。这本质上意味着聚类得到的输出，即**聚类**，需要进一步分析以了解每个聚类的含义，然后确定哪些聚类可以用于下游活动，例如商业促销。以下是一个流程图，有助于我们理解聚类在客户细分过程中的作用：
- en: '![](img/966e98d2-c974-4edb-97be-fe67c9089fd6.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/966e98d2-c974-4edb-97be-fe67c9089fd6.png)'
- en: Role of clustering in customer segmentation
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类在客户细分中的作用
- en: Now that we understand that clustering forms a stepping stone to performing
    customer segmentation, in the rest of the chapter, we will discuss various clustering
    techniques and implement projects around these techniques to create customer segments.
    For our projects, we make use of the wholesale customer dataset. Before delving
    into the projects, let's learn about the dataset and perform **exploratory data
    analysis** (**EDA**) to get a better understanding of the data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解到聚类是进行客户细分的基础，在本章的剩余部分，我们将讨论各种聚类技术，并围绕这些技术实施项目以创建客户细分。对于我们的项目，我们使用批发客户数据集。在深入项目之前，让我们了解数据集并执行**探索性数据分析**（**EDA**）以更好地理解数据。
- en: Understanding the wholesale customer dataset and the segmentation problem
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解批发客户数据集和细分问题
- en: 'The UCI Machine Learning Repository offers the wholesale customer dataset at
    [https://archive.ics.uci.edu/ml/datasets/wholesale+customers](https://archive.ics.uci.edu/ml/datasets/wholesale+customers).
    The dataset refers to clients of a wholesale distributor. It includes the annual
    spending in **monetary units** (**m.u.**) on diverse product categories. The goal
    of these projects is to apply clustering techniques to identify segments that
    are relevant for certain business activities, such as rolling out a marketing
    campaign. Before we actually use the clustering algorithms to get clusters, let''s
    first read the data and perform some EDA to understand the data using the following
    code block:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: UCI机器学习仓库在[https://archive.ics.uci.edu/ml/datasets/wholesale+customers](https://archive.ics.uci.edu/ml/datasets/wholesale+customers)提供了批发客户数据集。该数据集指的是批发分销商的客户。它包括各种产品类别的年度支出，以**货币单位**（**m.u.**）表示。这些项目的目标是应用聚类技术来识别与某些商业活动相关的细分市场，例如推出营销活动。在我们实际使用聚类算法获取聚类之前，让我们首先读取数据并执行一些EDA，以下代码块将帮助我们理解数据：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This will give the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/b7b0f664-aa72-4496-819a-c1aa3088fa7f.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b7b0f664-aa72-4496-819a-c1aa3088fa7f.png)'
- en: 'Now let''s check whether there are any entries with missing fields in our dataset:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查数据集中是否有任何缺失字段的条目：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'This will give the following output:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'From the EDA, we see that there are `440` observations available in this dataset
    and there are eight variables. The dataset does not have any missing values. While
    the last six variables are goods that were brought by distributors from the wholesaler,
    the first two variables are factors (categorical variables) representing the location
    and channel of purchase. In our projects, we intend to identify the segments based
    on the sales into different products, therefore, the location and channel variables
    in the data are not very useful. Let''s delete them from the dataset using the
    following code:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从 EDA 中，我们看到这个数据集中有 `440` 个观测值，并且有八个变量。数据集没有任何缺失值。尽管最后六个变量是批发商从批发商那里带来的商品，但前两个变量是因素（分类变量），代表购买的位置和渠道。在我们的项目中，我们打算根据不同产品的销售来识别细分市场，因此，数据中的位置和渠道变量并不很有用。让我们使用以下代码从数据集中删除它们：
- en: '[PRE4]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This gives us the following output:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了以下输出：
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We see that only six columns are retained, confirming that the deletion of non-required
    columns is successful. From the summary output in the EDA code, we can also observe
    that the scale across all the retained columns is the same so we do not have to
    explicitly normalize the data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到只保留了六列，这证实了非必需列的删除是成功的。从 EDA 代码的总结输出中，我们还可以观察到所有保留列的尺度是相同的，因此我们不需要显式地归一化数据。
- en: It may be noted that most clustering algorithms involve computation of distance
    of some form (such as Euclidean, Manhattan, Grower). It is important that data
    is scaled across the columns in the dataset so as to ensure a variable does not
    end up as a dominating one in distance computation just because of high scale.
    In case of different scales observed in columns of the data, we will rely on techniques
    such as Z-transform or min-max transform. Applying one of these techniques on
    the data ensures that the columns of the dataset are scaled appropriately therefore
    leaving no dominating variables in the dataset to be used with clustering algorithms.
    Fortunately, we do not have this issue so we can continue with the dataset as
    it is.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 可能需要注意的是，大多数聚类算法都涉及某种形式的距离计算（例如欧几里得、曼哈顿、Grower）。确保数据集列之间的尺度是一致的是非常重要的，以防止某个变量因为尺度较高而在距离计算中成为主导变量。在数据列中观察到不同尺度的情况下，我们将依赖诸如
    Z 变换或 min-max 变换等技术。对数据进行这些技术之一的应用确保了数据集列的适当缩放，因此在使用聚类算法时，数据集中没有主导变量。
- en: Clustering algorithms impose identification of subgroups in the input dataset
    even if there are no clusters present. To ensure that we get meaningful clusters
    as output from the clustering algorithms, it is important to check whether clusters
    exist in the data at all. **Clustering tendency**, or the feasibility of the clustering
    analysis, is the process of identifying whether the clusters exist in the dataset.
    Given an input dataset, the process determines whether it has a non-random or
    non-uniform data structure distribution that will lead to meaningful clusters.
    The Hopkins statistic measure is used to determine cluster tendency. It takes
    a value between 0 and 1, and if the value of the Hopkins statistic is close to
    0 (far below 0.5), it indicates the existence of valid clusters in the dataset.
    A Hopkins value closer to 1 indicates random structures in the dataset.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法强制在输入数据集中识别子组，即使没有集群存在。为了确保我们从聚类算法中获得有意义的集群输出，检查数据中是否存在集群是非常重要的。**聚类趋势**，或聚类分析的可行性，是识别数据集中是否存在集群的过程。给定一个输入数据集，这个过程确定它是否具有非随机或非均匀的数据结构分布，这将导致有意义的集群。Hopkins
    统计量用于确定聚类趋势。它取值在 0 到 1 之间，如果 Hopkins 统计量的值接近 0（远低于 0.5），则表明数据集中存在有效的集群。接近 1 的
    Hopkins 值表明数据集中存在随机结构。
- en: 'The `factoextra` library has a built-in `get_clust_tendency()` function that
    computes the Hopkins statistic on the input dataset. Let''s apply this function
    on our wholesale dataset to determine whether the dataset is valid for clustering
    at all. The following code accomplishes the computation of the Hopkins statistic:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`factoextra` 库有一个内置的 `get_clust_tendency()` 函数，该函数在输入数据集上计算 Hopkins 统计量。让我们将此函数应用于我们的批发数据集，以确定数据集是否适用于聚类。以下代码完成了
    Hopkins 统计量的计算：'
- en: '[PRE6]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will give the following output:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE7]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The Hopkins statistic output for our dataset is very close to 0, so we can conclude
    that we have a dataset that is a good candidate for our clustering exercise.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的Hopkins统计输出非常接近0，因此我们可以得出结论，我们有一个适合聚类练习的数据集。
- en: Categories of clustering algorithms
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类算法的分类
- en: 'There are numerous clustering algorithms available off the shelf in R. However,
    all these algorithms can be grouped into one of two categories:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: R中现成的聚类算法有很多。然而，所有这些算法都可以分为以下两类：
- en: '**Flat or partitioning algorithms**: These algorithms rely on an input parameter
    that defines the number of clusters to be identified in the dataset. The input
    parameter sometimes comes up as input from business directly or it can be established
    through certain statistical methods. For example, the **Elbow** method.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平面或划分算法**：这些算法依赖于一个输入参数，该参数定义了在数据集中要识别的聚类数量。输入参数有时直接来自业务，或者可以通过某些统计方法建立。例如，**肘部法**。'
- en: '**Hierarchical algorithms**: In these kinds of algorithms, the clusters are
    not identified in a single step. They involves multiple steps that run from a
    single cluster containing all the data points to *n* clusters containing single
    data point. Hierarchical algorithms can be further divided into the following
    two types:'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次算法**：在这些算法中，聚类不是在单一步骤中确定的。它们涉及多个步骤，从包含所有数据点的单个聚类开始，到包含单个数据点的*n*个聚类。层次算法可以进一步分为以下两种类型：'
- en: '**Divisive type**: A top-down clustering method where all points are initially
    assigned to a single cluster. In the next step, the cluster is split into two
    clusters which are least similar. The process of splitting the clusters is recursively
    done until each point has its own cluster, for example, the **DIvisive ANAlysis**
    (**DIANA**) clustering algorithm.'
  id: totrans-57
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**划分类型**：一种自上而下的聚类方法，其中所有点最初被分配到一个单个聚类中。在下一步中，聚类被分割成两个最不相似的聚类。分割聚类的过程递归进行，直到每个点都有自己的聚类，例如，**DIvisive
    ANAlysis**（DIANA）聚类算法。'
- en: '**Agglomerative type**: A bottom-up approach where, in the initial run, each
    point in the dataset is assigned *n* unique clusters, where *n* is equal to the
    number of observations in the dataset. In the next iteration, most similar clusters
    are merged (based on the distance between the clusters). The recursive process
    of merging the clusters continues until we are left with just one cluster, for
    example, **agglomerative nesting** (**AGNES**) algorithm.'
  id: totrans-58
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合类型**：一种自下而上的方法，在初始运行中，数据集中的每个点被分配*n*个独特的聚类，其中*n*等于数据集中的观测数。在下一个迭代中，最相似的聚类被合并（基于聚类之间的距离）。合并聚类的递归过程继续进行，直到我们只剩下一个聚类，例如，**聚合嵌套**（AGNES）算法。'
- en: As discussed earlier, there are numerous clustering algorithms available and
    we will focus on implementing projects using one algorithm for each type of clustering.
    We will implement project with k-means that is a flat or partitioning type clustering
    algorithm. We will then do customer segmentation with DIANA and AGNES, which are
    divisive and agglomerative, respectively.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，有众多聚类算法可供选择，我们将专注于使用每种聚类类型的一个算法来实现项目。我们将使用k-means算法来实现项目，它是一种平面或划分类型的聚类算法。然后我们将使用DIANA和AGNES进行客户细分，DIANA和AGNES分别是划分和聚合类型的算法。
- en: Identifying the customer segments in wholesale customer data using k-means clustering
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用k-means聚类在批发客户数据中识别客户细分
- en: The k-means algorithm is perhaps the most popular and commonly-used clustering
    method from partitioning clustering type. Though we usually call the clustering
    algorithm k-means, multiple implementations of this algorithm exist, namely the
    **MacQueen**, **Lloyd and Forgy**, and **Hartigan-Wong** algorithms. It has been
    studied and found that the Hartigan-Wong algorithm performs better than the other
    two algorithms in most situations. K-means in R makes use of the Hartigan-Wong
    implementation by default.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: k-means算法可能是从划分聚类类型中最受欢迎和最常用的聚类方法。尽管我们通常称之为k-means聚类算法，但这个算法有多个实现，包括**MacQueen**、**Lloyd和Forgy**以及**Hartigan-Wong**算法。研究表明，在大多数情况下，Hartigan-Wong算法的性能优于其他两种算法。R中的k-means默认使用Hartigan-Wong实现。
- en: 'The k-means algorithm requires the k-value to be passed as a parameter. The
    parameter indicates the number of clusters to be made with the input data. It
    is often a challenge for practitioners to determine the optimal k-value. Sometimes,
    we can go to a business and ask them how many clusters they would expect in the
    data. The answer from the business directly translates to be the *k* parameter
    value to be fed to the algorithm. In most cases though, the business is clueless
    as to the number of clusters. In such a case, the onus will be on the ML practitioner
    to determine the k-value. Fortunately, there are several methods available to
    determine this value. These methods can be classified into the following two categories:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: k均值算法需要将k值作为参数传递。该参数表示要使用输入数据创建的簇数。对于从业者来说，确定最佳k值通常是一个挑战。有时，我们可以去一家企业询问他们预计数据中有多少簇。企业的回答将直接转换为要输入算法的*k*参数值。然而，在大多数情况下，企业对簇数一无所知。在这种情况下，责任将落在机器学习从业者身上，他们需要确定k值。幸运的是，有几种方法可以确定这个值。这些方法可以分为以下两类：
- en: '**Direct methods**: These methods rely on optimizing a criterion, such as *within
    cluster sums of squares* or *the average silhouette*. Examples of this method
    include the **V Elbow method** and the **V Silhouette method**.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**直接方法**：这些方法依赖于优化一个标准，例如*簇内平方和*或*平均轮廓*。这种方法包括**V肘方法**和**V轮廓方法**。'
- en: '**Testing methods**: These methods consists of comparing evidence against a
    null hypothesis. Gap statistic is one popular example of this method.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试方法**：这些方法包括将证据与零假设进行比较。差距统计是这种方法的一个流行例子。'
- en: 'In addition to Elbow, Silhouette, and gap statistic methods, there are more
    than 30 other indices and methods that have been published for identifying the
    optimal number of clusters. We will not delve into the theoretical details of
    these methods, as covering 30 methods in a single chapter is not practical. However,
    R offers an excellent library function, called `NbClust` that makes it easy for
    us to implement all these methods in one go. The `NbClust` function is so powerful
    that it determines the optimal clusters by varying all combinations of number
    of clusters, distance measures, and clustering methods and all in one go! Once
    the library function computes all 30 indices, the *majority rule* is applied on
    the output to determine the optimal number of clusters, that is, the k-value to
    be used as input to the algorithm. Let''s implement `NbClust` for our wholesale
    dataset to determine the optimal k-value using the following code block:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 除了肘方法、轮廓方法和差距统计方法之外，还有30多种其他指数和方法被发表出来用于确定最佳簇数。我们不会深入探讨这些方法的理论细节，因为在一个章节中涵盖30种方法是不切实际的。然而，R提供了一个名为`NbClust`的出色库函数，使我们能够一次性实现所有这些方法。`NbClust`函数功能强大，它通过改变簇数、距离度量以及聚类方法的全部组合来确定最佳簇数！一旦库函数计算了所有30个指数，就会在输出上应用*多数规则*来确定最佳簇数，即作为算法输入的k值。让我们使用以下代码块为我们的批发数据集实现`NbClust`以确定最佳k值：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This will give the following output:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'As per the conclusion, we see the k-value that may be used for our problem
    is `3`. Additionally, plotting an elbow curve with the total within-groups sums
    of squares against the number of clusters in a k-means solution can be helpful
    in determining the optimal number of clusters. K-means is defined by the objective
    function, which tries to minimize the sum of all squared distances within a cluster
    (intra-cluster distance) for all clusters. In the elbow-curve plotting method,
    we compute the intra-cluster distance with different values of k, and the intra-cluster
    distance with different k''s is plotted as a graph. A bend in the elbow curve
    suggests the k-value that is optimal for the dataset. The elbow curve can be obtained
    within R using the following code block:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 根据结论，我们看到可能用于我们问题的k值是`3`。此外，通过将k均值解中的簇数与组内总平方和绘制肘曲线可以帮助确定最佳簇数。k均值由目标函数定义，该函数试图最小化所有簇内所有平方距离的总和（簇内距离）。在肘曲线绘制方法中，我们使用不同的k值计算簇内距离，并将不同k值的簇内距离绘制成图表。肘曲线的弯曲处表明了对于数据集而言的k值是最佳的。在R中，可以使用以下代码块获得肘曲线：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This will give the following output:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/5f2649b3-bb6f-4598-97e9-646ff9060718.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f2649b3-bb6f-4598-97e9-646ff9060718.png)'
- en: Even with the elbow curve method output, we see that the number of optimal clusters
    for our dataset is `3`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是肘部曲线法输出的结果，我们也可以看到，我们数据集的最佳聚类数量是`3`。
- en: We see from the `NbClust` function that we have used the Euclidean distance
    as the distance. There are a number of distance types (`euclidean`, `maximum`,
    `manhattan`, `canberra`, `binary`, `minkowski`) that we could have used as values
    for this distance parameter in the `NbClust` function. Let's understand what this
    distance actually means. We are already aware that each observation in our dataset
    is formed by values that represent features. This essentially means each observation
    of our dataset can be represented as points in multidimensional space. If we have
    to say that two observations are similar, we would expect the distance between
    the two points in the multidimensional space to be lower, that is, both these
    points in multidimensional space are close to each other. A high distance value
    between the two points indicates that they are very dissimilar.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从`NbClust`函数中我们可以看到，我们使用了欧几里得距离作为距离。在`NbClust`函数中，我们可以使用多种距离类型（`euclidean`、`maximum`、`manhattan`、`canberra`、`binary`、`minkowski`）作为距离参数的值。让我们理解这个距离实际上意味着什么。我们已经知道，我们数据集中的每个观察值是由表示特征的值组成的。这本质上意味着我们数据集中的每个观察值都可以表示为多维空间中的点。如果我们说两个观察值相似，我们期望这两个点在多维空间中的距离较低，即这两个点在多维空间中彼此靠近。两点之间的高距离值表明它们非常不相似。
- en: 'The Euclidean, Manhattan, and other types of distance measures are various
    ways in which distance can be measured given two points in a multidimensional
    space. Each of the distance measures involves a specific technique to compute
    the distance between the two points. The techniques involved in Manhattan and
    Euclidean, and the difference between their measures, are illustrated in the following
    screenshot:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得、曼哈顿以及其他类型的距离度量是多维空间中两点之间距离度量的各种方式。每种距离度量都涉及一种特定的技术来计算两点之间的距离。曼哈顿和欧几里得中涉及的技术以及它们度量的区别在下图中展示：
- en: '![](img/1c1e5f14-ef4e-4c6e-bba1-564f421de849.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/1c1e5f14-ef4e-4c6e-bba1-564f421de849.png)'
- en: Difference between Manhattan and Euclidean distance measures
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 曼哈顿距离与欧几里得距离度量的区别
- en: The Euclidean distance measures the shortest distance in the plane, whereas
    the Manhattan metric is the shortest path if one is allowed to move horizontally
    or vertically.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离度量平面上的最短距离，而曼哈顿度量是在允许水平或垂直移动的情况下最短路径。
- en: 'For example, if `a` and `b` are two points where `a= (0,0)`, `b = (3,4)`, then
    take a look at the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果`a`和`b`是两个点，其中`a= (0,0)`，`b = (3,4)`，那么看看以下内容：
- en: '`dist_euclid (a,b) = sqrt(3^2+4^2) = 5`'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dist_euclid (a,b) = sqrt(3^2+4^2) = 5`'
- en: '`dist_manhattan(a,b) = 3+4 = 7`'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dist_manhattan(a,b) = 3+4 = 7`'
- en: '`a=(a1,...,an), b=(b1,...,bn)` (in *n* dimensions and points)'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`a=(a1,...,an), b=(b1,...,bn)`（在*n*维度和点）'
- en: '`dist_euclid (a,b) = sqrt((a1-b1)^2 + ... + (an-bn)^2)`'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dist_euclid (a,b) = sqrt((a1-b1)^2 + ... + (an-bn)^2)`'
- en: '`dist_manhattan(a,b) = sum(abs(a1-b1) + ... + abs(an-bn))`'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dist_manhattan(a,b) = sum(abs(a1-b1) + ... + abs(an-bn))`'
- en: Both measure the shortest paths, but the Euclidean metric doesn't have any restrictions
    while the Manhattan metric only allows paths constant in all but one dimension.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种度量都测量最短路径，但欧几里得度量没有任何限制，而曼哈顿度量只允许除了一个维度外所有维度都保持恒定的路径。
- en: Likewise, the other distance measures also involve a certain unique to measure
    the similarity between given points. We will not be going through each one of
    the techniques in detail in this chapter, but the idea to get is that a distance
    measure basically defines the level of similarity between given observations.
    It may be noted that a distance measure is not just used in `NbClust` but in multiple
    ML algorithms, including k-means.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，其他距离度量也涉及某种独特的方法来度量给定点之间的相似性。在本章中，我们不会详细讲解每种技术，但需要理解的是，距离度量基本上定义了给定观察之间的相似程度。需要注意的是，距离度量不仅用于`NbClust`，还用于多个机器学习算法，包括k-means。
- en: 'Now that we''ve learned the various ways to identify our k-value and have implemented
    them to identify the optimal number of clusters for our wholesale dataset, let''s
    implement the k-means algorithm with the following code:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经学习了识别k值的各种方法，并将它们应用于识别批发数据集的最佳聚类数量，接下来让我们用以下代码实现k-means算法：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This will result in the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '[PRE14]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: From the output k-means, we can observe and infer several things about the output
    clusters. Obviously, three clusters are formed and this is in line with our k
    parameter that was passed to the algorithm. We see that the first cluster has
    330 observations in it, and the second and third clusters are small with just
    50 and 60 observations. The k-means output also provides us with the cluster centroids.
    The **centroid** is a representative of all points in a particular cluster. As
    it is not feasible to study each of the individual observations assigned to a
    cluster and determine the business characteristics of the cluster, the cluster
    centroid may be used as a pseudo for the points in the cluster. The cluster centroid
    helps us to quickly arrive at a conclusion in terms of the definition of contents
    of the cluster. The k-means output also produced the cluster assignment for each
    observation. Each of the observations in our wholesale dataset is assigned to
    one of the three clusters (1,2,3).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 从 k-means 的输出中，我们可以观察和推断关于输出聚类的几个方面。显然，形成了三个聚类，这与我们传递给算法的 k 参数一致。我们看到第一个聚类中有
    330 个观测值，而第二个和第三个聚类较小，只有 50 和 60 个观测值。k-means 输出还为我们提供了聚类中心。**中心点**是特定聚类中所有点的代表。由于不可能研究分配给每个聚类的每个单独的观测值并确定聚类的业务特征，因此聚类中心可以用来代表聚类中的点。聚类中心帮助我们快速得出关于聚类内容定义的结论。k-means
    输出还产生了每个观测值的聚类分配。我们批发数据集中的每个观测值都被分配到三个聚类中的一个（1，2，3）。
- en: 'It is possible to view the clustering results by using the `fviz_cluster()`
    function available in the `factoextra` library. The function provides a nice illustration
    of the clusters. If there are more than two dimensions (variables), `fviz_cluster`
    will perform **principal component analysis** (**PCA**) and plot the observations
    based on the first two principal components that explain the majority of the variance.
    The clusters visualization can be created though the following code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过使用 `factoextra` 库中的 `fviz_cluster()` 函数来查看聚类结果。该函数提供了聚类的一个良好说明。如果有超过两个维度（变量），`fviz_cluster`
    将执行 **主成分分析** (**PCA**) 并根据解释大部分方差的前两个主成分来绘制观测值。可以通过以下代码创建聚类可视化：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This will give the following graph as output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这将生成以下图形作为输出：
- en: '![](img/2fbbbb41-9ac6-4c9b-a97c-f552034ea036.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fbbbb41-9ac6-4c9b-a97c-f552034ea036.png)'
- en: Working mechanics of the k-means algorithm
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: k-means 算法的工作原理
- en: 'The execution of the k-means algorithm involves the following steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 算法的执行涉及以下步骤：
- en: Randomly select *k* observations from the dataset as the initial cluster centroids.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据集中随机选择 *k* 个观测值作为初始聚类中心。
- en: 'For each observation in the dataset, perform the following:'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于数据集中的每个观测值，执行以下操作：
- en: Compute the distance between the observation and each of the cluster centroids.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算观测值与每个聚类中心之间的距离。
- en: Identify the cluster centroid that has minimum distance with the observation.
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别与观测值距离最小的聚类中心。
- en: Assign the observation to such closest centroid.
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将观测值分配到最近的中心。
- en: With all points assigned to one of the cluster centroids, compute new cluster
    centroids. This can be done by taking the mean of all the points assigned to a
    cluster.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有点分配给其中一个聚类中心后，计算新的聚类中心。这可以通过计算分配给聚类的所有点的平均值来完成。
- en: Perform *step 2* and *step 3* repeatedly until the cluster centroids (mean)
    do not change or until a user-defined number of iterations is reached.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复执行 *步骤 2* 和 *步骤 3*，直到聚类中心（均值）不再改变或达到用户定义的迭代次数。
- en: One key thing to note in k-means is that the cluster centroids in the initial
    step are selected randomly and the initial cluster assignments are done based
    on the distance between the actual observations and the randomly-picked cluster
    centroids. This essentially means that if we were to pick observations as cluster
    centroids in the initial step other than the observations that were chosen, we
    would obtain different clusters than the one we have obtained. In technical terms,
    this is called a **non-globally-optimal solution** or a **locally-optimal solution**.
    The `cluster` library's k-means function has the `nstart` option, which works
    around this problem of the non-globally-optimal solution encountered with the
    k-means algorithm.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-means中需要注意的一个关键问题是，初始步骤中簇质心的选择是随机的，簇分配是基于实际观测值与随机选择的簇质心之间的距离进行的。这本质上意味着，如果我们最初选择除所选观测值之外的观测值作为簇质心，我们将获得与我们所获得的簇不同的簇。在技术术语中，这被称为**非全局最优解**或**局部最优解**。`cluster`库的k-means函数具有`nstart`选项，它可以解决k-means算法中遇到的非全局最优解问题。
- en: 'The `nstart` option enables the algorithm to try several random starts (instead
    of just one) by drawing a number of center observations from the datasets. It
    then checks the cluster sum of squares and proceeds with the best start, resulting
    in a more stable output. In our case, we set the `nstart` value as `50`, therefore
    the best start is chosen by k-means post checking it with 50 random initial sets
    of cluster centroids. The following diagram depicts the high-level steps involved
    in the k-means clustering algorithm:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`nstart`选项使算法能够通过从数据集中抽取多个中心观测值来尝试多个随机起始点（而不是只有一个），然后检查簇的平方和，并继续使用最佳起始点，从而得到更稳定的输出。在我们的案例中，我们将`nstart`值设置为`50`，因此通过50个随机初始簇质心集来检查后，选择最佳起始点。以下图表描述了k-means聚类算法中涉及的高级步骤：'
- en: '![](img/4fe651c3-aef1-42f3-902d-cc36a6222eb8.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4fe651c3-aef1-42f3-902d-cc36a6222eb8.png)'
- en: Steps in k-means clustering
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类步骤
- en: In supervised ML methods, such as classification, we have ground truth, therefore,
    we will be able to able to compare our predictions with the ground truth and measure
    to report the performance of our classification. Unlike the supervised ML method,
    in clustering, we do not have any ground truth. Therefore, computing the performance
    measurement with respect to clustering is a challenge.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督机器学习（ML）方法中，例如分类，我们有真实标签，因此我们可以将我们的预测与真实标签进行比较，并报告我们分类的性能。与监督机器学习方法不同，在聚类中，我们没有真实标签。因此，根据聚类计算性能度量是一个挑战。
- en: 'As an alternative to performance measurement, we use a pseudo-measure called
    **cluster quality**. The cluster quality is generally computed through measures
    known as intra-cluster distance and inter-cluster distance, which are illustrated
    in the following diagram:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 作为性能度量的替代，我们使用一个称为**簇质量**的伪度量。簇质量通常通过称为簇内距离和簇间距离的度量来计算，这些度量在以下图表中说明：
- en: '![](img/b460f049-6e7f-4fec-89d4-d2b37af5d12b.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b460f049-6e7f-4fec-89d4-d2b37af5d12b.png)'
- en: Intra-cluster distance and inter-cluster distance defined
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 已定义簇内距离和簇间距离
- en: The goal of the clustering task is to obtain good-quality clusters. Clusters
    are termed as **high-quality clusters** if the distance within the observations
    is minimum and the distance between the clusters themselves is maximum.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类任务的目的是获得高质量的簇。如果观测值之间的距离最小且簇之间的距离最大，则簇被称为**高质量簇**。
- en: 'There are multiple ways inter-cluster and intra-cluster distances can be measured:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多种测量簇间和簇内距离的方法：
- en: '**Intra-cluster**: This distance can be measured as (sum, minimum, maximum,
    or mean) of the (absolute/squared) distance between all pairs of points in the
    cluster (or) *diameter*–two farthest points (or) between the centroid and all
    points in the cluster.'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇内距离**：这个距离可以测量为簇中所有点对之间的（绝对/平方）距离的（总和、最小值、最大值或平均值），或者为（直径）——两个最远的点，或者为质心与簇中所有点之间的距离。'
- en: '**Inter-cluster**: This distance is measured as sum of the (squared) distance
    between all pairs of clusters, where distance between two clusters itself is computed
    as one of the following:'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**簇间距离**：这个距离是通过所有簇对之间的（平方）距离的总和来测量的，其中两个簇之间的距离是通过以下方式之一计算的：'
- en: Distance between their centroids
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 质心之间的距离
- en: Distance between farthest pair of points
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最远点对之间的距离
- en: Distance between the closest pair of points belonging to the clusters
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 属于簇的最接近一对点的距离
- en: Unfortunately, there is no way to pinpoint the preferred inter-cluster distance
    and intra-cluster distance values. The **Silhouette index** is one metric that
    is based on inter-cluster distance and intra-cluster distance that can be readily
    computed and easily interpreted.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我们无法精确地确定簇间距离和簇内距离的偏好值。**轮廓指数**是一个基于簇间距离和簇内距离的指标，可以轻松计算并易于解释。
- en: 'The Silhouette index is computed using the mean intra-cluster distance, *a*,
    and the mean nearest-cluster distance, *b*, for each of the observations participating
    in the clustering exercise. The Silhouette index for an observation is given by
    the following formula:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓指数是通过计算每个参与聚类练习的观测值的平均簇内距离 *a* 和平均最近簇距离 *b* 来计算的。一个观测值的轮廓指数由以下公式给出：
- en: '![](img/e22c4857-0e38-4471-8e3b-5902a1a357e5.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e22c4857-0e38-4471-8e3b-5902a1a357e5.png)'
- en: Here, *b* is the distance between an observation and the nearest cluster that
    the observation is not a part of.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*b* 是一个观测值与其不属于的最近簇之间的距离。
- en: Silhouette index value ranges between [-1, 1]. A value of +1 for an observation
    indicates that the observation is far away from its neighboring cluster and it
    is very close to the cluster it is assigned to. Similarly, a value of -1 tells
    us that the observation is close to its neighboring cluster than to the cluster
    it is assigned to. A value of 0 means it's at the boundary of the distance between
    the two clusters. A value of +1 is ideal and -1 is the least preferred. Hence,
    the higher the value, the better the quality of the cluster.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 轮廓指数的值介于 [-1, 1] 之间。对于一个观测值，+1 的值表示该观测值远离其邻近簇，并且非常接近其分配到的簇。同样，-1 的值告诉我们该观测值比其分配到的簇更接近其邻近簇。0
    的值表示它位于两个簇之间的距离边界上。+1 是理想的值，而 -1 是最不受欢迎的值。因此，值越高，簇的质量越好。
- en: 'The `cluster` library offers the Silhouette function, which can be readily
    used on our k-means clustering output to understand the quality of the clusters
    that were formed. The following code computes the Silhouette index for our three
    clusters:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`cluster` 库提供了轮廓函数，可以轻松地应用于我们的 k-means 聚类输出，以了解形成簇的质量。以下代码计算了我们的三个簇的轮廓指数：'
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will give us the following output:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE17]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As we have seen, the Silhouette index can range from -1 to +1, and the latter
    is preferred. From the output, we see that the clusters are all good quality clusters,
    as the average width is a positive number closer to 1 than -1.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，轮廓指数可以从 -1 到 +1 变化，后者更受欢迎。从输出结果来看，这些簇都是高质量的簇，因为平均宽度是一个接近 1 的正数，比 -1 更接近
    1。
- en: In fact, the Silhouette index can be used not just to measure the quality of
    clusters formed but also to compute the k-value. Similar to Elbow method, we can
    iterate through multiple values of k and then identify the k that yields the maximum
    Silhouette index values across the clusters. Clustering can then be performed
    using the k that was identified.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，轮廓指数不仅可以用来衡量形成簇的质量，还可以用来计算 k 值。类似于肘部方法，我们可以遍历多个 k 值，然后识别出在整个簇中产生最大轮廓指数值的
    k。然后可以使用识别出的 k 进行聚类。
- en: There are numerous cluster-quality measures described in the literature. The
    Silhouette index is just one measure we covered in this chapter because of its
    popularity in the ML community. The `clusterCrit` library offers a wide range
    of indices to measure the quality of clusters. We are not going to explore the
    other cluster-quality metrics here, but interested readers should refer to this
    library for further information on how to compute cluster quality.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 文献中描述了大量的簇质量度量。轮廓指数只是我们在本章中介绍的一个度量，因为它在机器学习社区中非常受欢迎。`clusterCrit` 库提供了广泛的指标来衡量簇的质量。我们不会在这里探索其他簇质量指标，但感兴趣的读者应参考此库以获取有关如何计算簇质量的更多信息。
- en: 'So far, we have covered the k-means clustering algorithm to identify the clusters,
    but the original segmentation task we started with does not end here. Segmentation
    further spans to the task of understanding what each of the clusters formed from
    the clustering exercise mean to businesses. For example, we take our cluster centroids
    obtained from k-means and an attempt is made to identify what these are:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了k-means聚类算法来识别聚类，但最初开始的原始细分任务并没有结束。细分进一步扩展到理解从聚类练习中形成的每个聚类对业务意味着什么。例如，我们使用从k-means获得的聚类质心，并尝试识别这些是什么：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here are some sample insights into each cluster:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个聚类的几个示例洞察：
- en: 'Cluster 1 is low spenders (average spending: 22,841.744), with the majority
    of spending allocated to the fresh category'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类1是低消费群体（平均消费：22,841.744），大部分消费分配到新鲜类别
- en: 'Cluster 2 is high spenders (average spending: 70,741.42), with the majority
    of spending in the grocery category'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类2是高消费群体（平均消费：70,741.42），大部分消费在杂货类别
- en: 'Cluster 3 is medium spenders (average spending : 59,077.568), with the majority
    of spending in the fresh category'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类3是中等消费群体（平均消费：59,077.568），大部分消费在新鲜类别
- en: 'Now, based on the business objective, one or more clusters may be selected
    to target. For example, if the objective is to have high spenders spend more,
    promotions may be rolled out to cluster 2 individuals with spending in the `Frozen`
    and `Delicatessen` products less than the centroid values (that is, `Frozen`:
    `1,996.680` and `Delicatessen`: `2,252.020`).'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，根据业务目标，可以选择一个或多个聚类进行目标定位。例如，如果目标是让高消费群体消费更多，可以推出针对在`冷冻`和`熟食店`产品上的消费低于质心值（即`冷冻`：`1,996.680`和`熟食店`：`2,252.020`）的聚类2个人的促销活动。
- en: Identifying the customer segments in the wholesale customer data using DIANA
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用DIANA在批发客户数据中识别客户细分
- en: Hierarchical clustering algorithms are a good choice when we don't necessarily
    have circular (or hyperspherical) clusters in the data, and we essentially don't
    know the number of clusters in advance. With hierarchical clustering algorithm,
    unlike the flat or partitioning algorithms, there is no requirement to decide
    and pass the number of clusters to be formed prior to applying the algorithm on
    the dataset.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据中不一定有圆形（或超球体）聚类时，层次聚类算法是一个很好的选择，并且我们事先也不知道聚类的数量。与平面或划分算法不同，使用层次聚类算法时，不需要在将算法应用于数据集之前决定并传递要形成的聚类数量。
- en: Hierarchical clustering results in a dendogram (tree diagram) that can be visually
    verified to easily determine the number of clusters. Visual verification enables
    us to perform cuts in the dendrogram at suitable places.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类会产生一个树状图（树形图），可以直观地验证以轻松确定聚类数量。直观验证使我们能够在树状图上合适的位置进行切割。
- en: The results produced by this type of clustering algorithm are reproducible as
    the algorithm is not sensitive to the choice of the distance metric. In other
    words, irrespective of the distance metric chosen, we will get the same results.
    This type of clustering is also suitable for datasets of a higher complexity (quadratic)
    and in particular for exploring the hierarchical relationships that exist between
    the clusters.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这种聚类算法产生的结果具有可重复性，因为算法对距离度量的选择不敏感。换句话说，无论选择哪种距离度量，我们都会得到相同的结果。这种聚类也适用于更复杂的（二次）数据集，特别是用于探索聚类之间存在的层次关系。
- en: 'Divisive hierarchical clustering, also known as **DIvisive ANAlysis** (**DIANA**),
    is a hierarchical clustering algorithm that follows a top-down approach to identify
    clusters in a given dataset. Here are the steps in DIANA to identify the clusters:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂层次聚类，也称为**DIvisive ANAlysis**（**DIANA**），是一种层次聚类算法，它采用自上而下的方法来识别给定数据集中的聚类。以下是DIANA识别聚类的步骤：
- en: All observations of the dataset are assigned to the root, so in the initial
    step only a single cluster is formed.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有数据集的观测值都被分配到根节点，因此在初始步骤中只形成一个单一聚类。
- en: In each iteration, the most heterogeneous cluster is partitioned into two.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每次迭代中，最异质的聚类被分割成两个。
- en: '**Step 2** is repeated until all the observations are in their own cluster:'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**步骤2**重复进行，直到所有观测值都在它们自己的聚类中：'
- en: '![](img/b267fbb4-0e11-429f-9534-8ac5cc8b72ce.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b267fbb4-0e11-429f-9534-8ac5cc8b72ce.png)'
- en: Working of divisive hierarchical clustering algorithm
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 分裂层次聚类算法的工作原理
- en: 'One obvious question that comes up is about the technique used by the algorithm
    to split the cluster into two. The answer is that it is performed according to
    some (dis)similarity measure. The Euclidean distance is used to measure the distance
    between two given points. This algorithm works by splitting the data on the basis
    of the farthest-distance measure of all the pairwise distances between the data
    points. Linkage defines the specific details of fartherness of the data points.
    The next figure illustrates the various linkages considered by DIANA for splitting
    the clusters. Here are some of the distances considered to split the groups:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 一个明显的问题就是关于算法用来将簇分割成两个的技术。答案是它根据某些（不）相似性度量来执行。欧几里得距离用于测量两个给定点之间的距离。该算法通过基于所有数据点成对距离的最远距离度量来分割数据。链接定义了数据点距离的具体细节。下一图展示了DIANA在分割簇时考虑的各种链接方式。以下是一些考虑用于分割组的一些距离：
- en: '**Single-link**: Nearest distance or single linkage'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单链接法**: 最近距离或单链接'
- en: '**Complete-link**: Farthest distance or complete linkage'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全链接法**: 最远距离或完全链接'
- en: '**Average-link**: Average distance or average linkage'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均链接法**: 平均距离或平均链接'
- en: '**Centroid-link**: Centroid distance'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重心链接法**: 重心距离'
- en: '**Ward''s method**: Sum of squared `euclidean` distance is minimized'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ward方法**: 最小化平方欧几里得距离和'
- en: 'Take a look at the following diagram to better understand the preceding distances:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下图表以更好地理解前面的距离：
- en: '![](img/4f0d00c3-798b-4422-9ab9-c5ab5cad4866.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/4f0d00c3-798b-4422-9ab9-c5ab5cad4866.png)'
- en: Illustration depicting various linkage types used by DIANA
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 展示DIANA使用的各种链接类型的示意图
- en: 'Generally, the linkage type to be used is passed as a parameter to the clustering
    algorithm. The `cluster` library offers the `diana` function to perform clustering.
    Let''s apply it on our wholesale dataset with the following code:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，要使用的链接类型作为参数传递给聚类算法。`cluster`库提供了`diana`函数来执行聚类。让我们用以下代码在我们的批发数据集上应用它：
- en: '[PRE19]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This will give us the following output:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE20]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Take a look at the following output:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 看看以下输出：
- en: '![](img/ccfc0e26-aee4-4f83-b19d-16d56310d220.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/ccfc0e26-aee4-4f83-b19d-16d56310d220.png)'
- en: 'The `plot.hclust()` and `plot.dendrogram()` functions may also be used on the
    DIANA clustering output. `plot.dendrogram()` yields the dendogram that follows
    the natural structure of the splits as done by the DIANA algorithm. Use the following
    code to generate the dendrogram:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`plot.hclust()`和`plot.dendrogram()`函数也可以用于DIANA聚类输出。`plot.dendrogram()`生成的树状图遵循DIANA算法执行的分割的自然结构。使用以下代码生成树状图：'
- en: '[PRE21]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This will give the following output:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/69e2660f-7ac4-44e3-a13c-640f2584e12b.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/69e2660f-7ac4-44e3-a13c-640f2584e12b.png)'
- en: In the dendrogram output, each leaf that appears on the right relates to each
    observation in the dataset. As we traverse from right to left, observations that
    are similar to each other are grouped into one branch, which are themselves fused
    at a higher level.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在树状图输出中，每个出现在右侧的叶子都与数据集中的每个观测相关。当我们从右向左遍历时，相似的观测被分组到一个分支中，这些分支本身在更高的级别上融合。
- en: The higher level of the fusion, provided on the horizontal axis, indicates the
    similarity between two observations. The higher the fusion, the more similar the
    observations are. It may be noted that conclusions about the proximity of two
    observations can be drawn only based on the level where branches containing those
    two observations are first fused. In order to identify clusters, we can cut the
    dendrogram at a certain level. The level at which the cut is made defines the
    number of clusters obtained.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 水平轴上提供的融合级别表示两个观测之间的相似性。融合级别越高，观测之间的相似性就越大。需要注意的是，关于两个观测之间接近性的结论只能基于包含这两个观测的分支首次融合的级别得出。为了识别簇，我们可以在树状图的一定级别处进行切割。切割所进行的级别定义了获得的簇的数量。
- en: 'We can make use of the `cutree()` function to obtain the cluster assignment
    for each of the observations in our dataset. Execute the following code to obtain
    the clusters and review the clustering output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`cutree()`函数来获取数据集中每个观测的簇分配。执行以下代码以获取簇并审查聚类输出：
- en: '[PRE22]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This will give the following output:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '[PRE23]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'We can also visualize the clustering output through the `fviz_cluster` function
    in the `factoextra` library. Use the following code to get the required visualization:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过`factoextra`库中的`fviz_cluster`函数可视化聚类输出。使用以下代码获取所需的可视化：
- en: '[PRE25]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'This will give you the following output:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/a5b6f638-67e4-46b8-ac3a-e1a4603ef222.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/a5b6f638-67e4-46b8-ac3a-e1a4603ef222.png)'
- en: 'It is also possible to color-code the clusters within the dendogram itself.
    This can be accomplished with the following code:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以在树状图中本身对簇进行着色编码。这可以通过以下代码实现：
- en: '[PRE26]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This will give the following output:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这将给出以下输出：
- en: '![](img/61cd4544-de37-4978-8a15-ff63d74d7ae6.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/61cd4544-de37-4978-8a15-ff63d74d7ae6.png)'
- en: Now that the clusters are identified, the steps we discussed to evaluate the
    cluster quality (through the Silhouette index) apply here as well. As we have
    already covered this topic under the k-means clustering algorithm, we are not
    going to repeat the steps here. The code and interpretation of the output remains
    the same as what was discussed under k-means.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在簇已经识别，我们讨论的评估簇质量（通过轮廓指数）的步骤也适用于此处。因为我们已经在k-means聚类算法下讨论了这一主题，所以我们不会在这里重复步骤。代码和输出解释与k-means下讨论的内容相同。
- en: As discussed earlier, the cluster's output is not the final point to customer
    segmentation exercise we have on hand. Similar to the discussion we had on under
    the k-means algorithm, we could analyze the DIANA cluster output to identify meaningful
    segments so as to roll out business objectives to those specifically-identified
    segments.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，簇的输出并不是我们手头上的客户细分练习的最终点。类似于我们在k-means算法下进行的讨论，我们可以分析DIANA簇的输出以识别有意义的细分，以便将这些业务目标推广到特定识别的细分中。
- en: Identifying the customer segments in the wholesale customers data using AGNES
  id: totrans-187
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AGNES在批发客户数据中识别客户细分
- en: 'AGNES is the reverse of DIANA in the sense that it follows a bottom-up approach
    to clustering the dataset. The following diagram illustrates the working principle
    of the AGNES algorithm for clustering:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: AGNES在聚类数据集时遵循自下而上的方法，与DIANA相反。以下图解说明了AGNES算法的聚类工作原理：
- en: '![](img/56080695-a543-418e-ae0c-0429532fa2b6.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/56080695-a543-418e-ae0c-0429532fa2b6.png)'
- en: Working of agglomerative hierarchical clustering algorithm
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类层次聚类算法的工作原理
- en: 'Except for the bottom-up approach followed by AGNES, the implementation details
    behind the algorithm are the same as for DIANA; therefore, we won''t repeat the
    discussion of the concepts here. The following code block clusters our wholesale
    dataset into three clusters with AGNES; it also creates a visualization of the
    clusters thus formed:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 除了AGNES遵循的从下而上的方法之外，算法背后的实现细节与DIANA相同；因此，我们在这里不会重复讨论概念。以下代码块使用AGNES将我们的批发数据集聚类成三个簇；它还创建了所形成的簇的可视化：
- en: '[PRE27]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This is the output that you will obtain:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你将获得的输出：
- en: '[PRE28]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Take a look at the following screenshot:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下屏幕截图：
- en: '![](img/809fb5ed-f69d-40e4-9f90-966bb3953734.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/809fb5ed-f69d-40e4-9f90-966bb3953734.png)'
- en: 'Take a look at the following code block:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下代码块：
- en: '[PRE29]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[PRE30]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Execute the following command:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 执行以下命令：
- en: '[PRE31]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The preceding command generates the following output:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令生成了以下输出：
- en: '![](img/f8337198-628a-46bc-a3fe-3a3ab237d047.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8337198-628a-46bc-a3fe-3a3ab237d047.png)'
- en: 'Take a look at the following commands:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下命令：
- en: '[PRE32]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The preceding commands generate the following output:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 上述命令生成了以下输出：
- en: '![](img/6abc2b7f-b61c-40c4-8c72-7606a16b01f8.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6abc2b7f-b61c-40c4-8c72-7606a16b01f8.png)'
- en: We can see from the AGNES clustering output that a large number of observations
    from the dataset are assigned to one cluster and very few observations were assigned
    to the other clusters. This is not a great output for our segmentation downstream
    exercise. To obtain better cluster assignments, you could try using other cluster-linkage
    methods aside from the default average linkage method currently used by the AGNES
    algorithm.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 从AGNES聚类输出中我们可以看到，数据集的大量观测值被分配到一个簇中，而分配到其他簇的观测值非常少。这对于我们的细分下游练习来说并不是一个好的输出。为了获得更好的簇分配，你可以尝试使用除了AGNES算法当前使用的默认平均链接方法之外的其他簇链接方法。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we learned about the concept of segmentation and its association
    with clustering, an ML unsupervised learning technique. We made use of the wholesale
    dataset available from the UCI repository and implemented clustering using the
    k-means, DIANA, and AGNES algorithms. During the course of this chapter, we also
    studied various aspects related to clustering, such as tendency to cluster, distance,
    linkage measures, and methods to identify the right number of clusters, and measuring
    the output of clustering. We also explored making use of the clustering output
    for customer-segmentation purposes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们学习了分割的概念及其与聚类的关系，聚类是一种机器学习无监督学习技术。我们使用了从UCI仓库可用的批发数据集，并实现了使用k-means、DIANA和AGNES算法进行聚类。在本章的过程中，我们还研究了与聚类相关的各个方面，例如聚类的趋势、距离、链接度量以及确定正确聚类数量的方法，以及测量聚类输出的方法。我们还探讨了如何利用聚类输出进行客户细分。
- en: Can computers see and identify objects and living creatures like humans do?
    Let's explore the answer to this question in the next chapter.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 计算机能否像人类一样看到并识别物体和生物？让我们在下一章中探索这个问题的答案。
