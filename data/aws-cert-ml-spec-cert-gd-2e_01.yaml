- en: '1'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '1'
- en: Machine Learning Fundamentals
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: For many decades, researchers have been trying to simulate human brain activity
    through the field known as **artificial intelligence**, or **AI** for short. In
    1956, a group of people met at the Dartmouth Summer Research Project on Artificial
    Intelligence, an event that is widely accepted as the first group discussion about
    AI as it’s known today. Researchers were trying to prove that many aspects of
    the learning process could be precisely described and, therefore, automated and
    replicated by a machine. Today, you know they were right!
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 几十年来，研究人员一直在通过被称为**人工智能**（或简称**AI**）的领域来模拟人类大脑活动。1956年，一群人在达特茅斯夏季人工智能研究项目上会面，这一事件被广泛接受为今天所知的第一次关于人工智能的集体讨论。研究人员试图证明学习过程的许多方面可以被精确描述，因此可以被机器自动化和复制。今天，你知道他们是正确的！
- en: Many other terms appeared in this field, such as **machine learning** (**ML**)
    and **deep learning** (**DL**). These sub-areas of AI have also been evolving
    for many decades (granted, nothing here is new to the science). However, with
    the natural advance of the information society and, more recently, the advent
    of **big data** platforms, AI applications have been reborn with much more applicability
    – power (because now there are more computational resources to simulate and implement
    them) and applicability (because now information is everywhere).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个领域还出现了许多其他术语，例如**机器学习**（**ML**）和**深度学习**（**DL**）。这些 AI 的子领域也已经发展了几十年（当然，这里没有什么是对科学界来说是新的）。然而，随着信息社会的自然进步，以及最近**大数据**平台的兴起，AI
    应用程序以更广泛的应用性重生——力量（因为现在有更多的计算资源来模拟和实现它们）和适用性（因为现在信息无处不在）。
- en: Even more recently, cloud service providers have put AI in the cloud. This helps
    all sizes of companies to reduce their operational costs and even lets them sample
    AI applications, considering that it could be too costly for a small company to
    maintain its own data center to scale an AI application.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 更近一些，云服务提供商已经将 AI 带入云端。这有助于各种规模的公司降低其运营成本，甚至让他们能够尝试 AI 应用程序，考虑到对于一家小公司来说，维护自己的数据中心以扩展
    AI 应用程序可能成本过高。
- en: An incredible journey of building cutting-edge AI applications has emerged with
    the popularization of big data and cloud services. In June 2020, one specific
    technology gained significant attention and put AI on the list of the most discussed
    topics across the technology industry – its name is ChatGPT.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 随着大数据和云服务的普及，出现了构建尖端 AI 应用程序的不凡之旅。2020年6月，一项具体的技术引起了广泛关注，并将 AI 列为技术行业中最受讨论的话题之一——它的名字叫
    ChatGPT。
- en: ChatGPT is a popular AI application that uses large language models (more specifically,
    **generative pre-trained transformers**) trained on massive amounts of text data
    to understand and generate human-like language. These models are designed to process
    and comprehend the complexities of human language, including grammar, context,
    and semantics.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ChatGPT 是一种流行的 AI 应用程序，它使用在大量文本数据上训练的大型语言模型（更具体地说，是**生成式预训练转换器**）来理解和生成类似人类的语言。这些模型旨在处理和理解人类语言的复杂性，包括语法、上下文和语义。
- en: Large language models utilize DL techniques (for example, deep neural networks
    based on transformer architecture) to learn patterns and relationships within
    textual data. They consist of millions of parameters, making them highly complex
    and capable of capturing very specific language structures.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 大型语言模型利用深度学习技术（例如，基于转换器架构的深度神经网络）来学习文本数据中的模式和关系。它们由数百万个参数组成，这使得它们高度复杂，能够捕捉非常具体的语言结构。
- en: 'Such mixing of terms and different classes of use cases might get one stuck
    on understanding the practical steps of implementing AI applications. That brings
    you to the goal of this chapter: being able to describe what the terms AI, ML,
    and DL mean, as well as understanding all the nuances of an ML pipeline. Avoiding
    confusion about these terms and knowing what exactly an ML pipeline is will allow
    you to properly select your services, develop your applications, and master the
    AWS Machine Learning Specialty exam.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这种术语和不同类别的用例的混合可能会让人在理解实施 AI 应用程序的实用步骤时感到困惑。这把你带到了本章的目标：能够描述 AI、ML 和 DL 这些术语的含义，以及理解
    ML 管道的所有细微差别。避免对这些术语的混淆，并确切了解 ML 管道是什么，将允许你正确选择服务，开发你的应用程序，并掌握 AWS 机器学习专业考试。
- en: Making the Most Out of this Book – Your Certification and Beyond
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 充分利用这本书——你的认证以及更远
- en: This book and its accompanying online resources are designed to be a complete
    preparation tool for your **MLS-C01 Exam**.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书及其附带在线资源旨在成为你**MLS-C01考试**的完整准备工具。
- en: The book is written in a way that you can apply everything you’ve learned here
    even after your certification. The online practice resources that come with this
    book (*Figure 1**.1*) are designed to improve your test-taking skills. They are
    loaded with timed mock exams, interactive flashcards, and exam tips to help you
    work on your exam readiness from now till your test day.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是以一种方式编写的，你可以在认证后应用在这里学到的所有内容。这本书附带在线实践资源（*图1**.1*）旨在提高你的应试技巧。它们包含了定时模拟考试、互动闪卡和考试技巧，帮助你从现在开始直到考试当天准备考试。
- en: Before You Proceed
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前
- en: To learn how to access these resources, head over to [*Chapter 11*](B21197_11.xhtml#_idTextAnchor1477),
    *Accessing the Online Practice Resources*, at the end of the book.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何访问这些资源，请前往书的末尾的[*第11章*](B21197_11.xhtml#_idTextAnchor1477)，*访问在线实践资源*。
- en: '![Figure 1.1 – Dashboard interface of the online practice resources](img/B21197_01_01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1 – 在线实践资源的仪表板界面](img/B21197_01_01.jpg)'
- en: Figure 1.1 – Dashboard interface of the online practice resources
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 – 在线实践资源的仪表板界面
- en: 'Here are some tips on how to make the most out of this book so that you can
    clear your certification and retain your knowledge beyond your exam:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些如何充分利用这本书以通过你的认证并在考试后保留知识的技巧：
- en: Read each section thoroughly.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仔细阅读每一部分。
- en: '`BACK TO THE BOOK` link from the Dashboard to access the book in **Packt Reader**.
    You can highlight specific sections of the book there.'
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从仪表板到**Packt Reader**中访问书籍的`BACK TO THE BOOK`链接。你可以在那里突出显示书籍的特定部分。
- en: '**Chapter Review Questions**: At the end of this chapter, you’ll find a link
    to review questions for this chapter. These are designed to test your knowledge
    of the chapter. Aim to score at least **75%** before moving on to the next chapter.
    You’ll find detailed instructions on how to make the most of these questions at
    the end of this chapter in the *Exam Readiness Drill - Chapter Review Questions*
    section. That way, you’re improving your exam-taking skills after each chapter,
    rather than at the end.'
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**章节复习题**：在本章末尾，你会找到一个链接来查看本章的复习题。这些题目旨在测试你对本章的知识。目标是在进入下一章之前至少得分**75%**。你可以在本章末尾的*考试准备练习
    - 章节复习题*部分找到如何充分利用这些题目的详细说明。这样，你就可以在每一章之后提高你的应试技巧，而不是在最后。'
- en: '**Flashcards**: After you’ve gone through the book and scored **75%** more
    in each of the chapter review questions, start reviewing the online flashcards.
    They will help you memorize key concepts.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**闪卡**：在你阅读完本书并在每个章节复习题中得分提高**75%**之后，开始复习在线闪卡。它们将帮助你记住关键概念。'
- en: '**Mock Exams**: Solve the mock exams that come with the book till your exam
    day. If you get some answers wrong, go back to the book and revisit the concepts
    you’re weak in.'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**模拟考试**：在考试当天之前，解决书中附带的所有模拟考试。如果你有一些答案错误，回到书中并重新审视你薄弱的概念。'
- en: '**Exam Tips**: Review these from time to time to improve your exam readiness
    even further.'
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**考试技巧**：不时回顾这些内容，以进一步提高你的考试准备。'
- en: 'The main topics of this chapter are as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的主要内容包括以下内容：
- en: Comparing AI, ML, and DL
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较人工智能、机器学习和深度学习
- en: Classifying supervised, unsupervised, and reinforcement learning
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对监督学习、无监督学习和强化学习进行分类
- en: The CRISP-DM modeling life cycle
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CRISP-DM建模生命周期
- en: Data splitting
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据拆分
- en: Modeling expectations
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建模预期
- en: Introducing ML frameworks
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍机器学习框架
- en: ML in the cloud
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云中的机器学习
- en: Comparing AI, ML, and DL
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比较人工智能、机器学习和深度学习
- en: 'AI is a broad field that studies different ways to create systems and machines
    that will solve problems by simulating human intelligence. There are different
    levels of sophistication to create these programs and machines, which go from
    simple rule-based engines to complex self-learning systems. AI covers, but is
    not limited to, the following sub-areas:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 人工智能是一个广泛的研究领域，研究通过模拟人类智能来创建系统和机器以解决问题的不同方法。创建这些程序和机器的复杂性不同，从简单的基于规则的引擎到复杂的自我学习系统。人工智能涵盖了但不限于以下子领域：
- en: Robotics
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器人技术
- en: '**Natural language** **processing** (**NLP**)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自然语言处理**（NLP）'
- en: Rule-based systems
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于规则的系统
- en: Machine learning (ML)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习（ML）
- en: Computer vision
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机视觉
- en: The area this certification exam focuses on is ML.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这项认证考试关注的领域是机器学习（ML）。
- en: Examining ML
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检验机器学习
- en: ML is a sub-area of AI that aims to create systems and machines that can learn
    from experience, without being explicitly programmed. As the name suggests, the
    system can observe its underlying environment, learn, and adapt itself without
    human intervention. Algorithms behind ML systems usually extract and improve knowledge
    from the data and conditions that are available to them.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是人工智能的一个子领域，旨在创建可以从经验中学习，而不需要明确编程的系统。正如其名称所暗示的，系统可以观察其底层环境，学习并适应自身，而不需要人为干预。机器学习系统背后的算法通常从它们可用的数据和条件下提取和改进知识。
- en: '![Figure 1.2 – Hierarchy of AI, ML, and DL](img/B21197_01_02.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图1.2 – 人工智能、机器学习和深度学习的层次结构](img/B21197_01_02.jpg)'
- en: Figure 1.2 – Hierarchy of AI, ML, and DL
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.2 – 人工智能、机器学习和深度学习的层次结构
- en: You should keep in mind that there are different classes of ML algorithms. For
    example, decision tree-based models, probabilistic-based models, and neural network
    models. Each of these classes might contain dozens of specific algorithms or architectures
    (some of them will be covered in later sections of this book).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该记住，存在不同类别的机器学习算法。例如，基于决策树的模型、基于概率的模型和神经网络模型。这些类别中的每一个都可能包含数十种特定的算法或架构（其中一些将在本书的后续章节中介绍）。
- en: 'As you might have noticed in *Figure 1**.2*, you can be even more specific
    and break the ML field down into another very important topic for the Machine
    Learning Specialty exam: deep learning, or DL for short.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能已经在*图1.2*中注意到的，你可以更加具体，并将机器学习领域分解为机器学习专业考试中另一个非常重要的主题：深度学习，或简称DL。
- en: Examining DL
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查深度学习
- en: DL is a subset of ML that aims to propose algorithms that connect multiple layers
    to solve a particular problem. The knowledge is then passed through, layer by
    layer, until the optimal solution is found. The most common type of DL algorithm
    is deep neural networks.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，旨在提出将多个层连接起来以解决特定问题的算法。然后，知识逐层传递，直到找到最佳解决方案。最常见类型的深度学习算法是深度神经网络。
- en: At the time of writing this book, DL is a very hot topic in the field of ML.
    Most of the current state-of-the-art algorithms for machine translation, image
    captioning, and computer vision were proposed in the past few years and are a
    part of the DL field (GPT-4, used by the ChatGPT application, is one of these
    algorithms).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本书时，深度学习是机器学习领域的一个非常热门的话题。目前大多数最先进的机器翻译、图像标题和计算机视觉算法都是在过去几年中提出的，并且是深度学习领域的一部分（ChatGPT应用中使用的GPT-4就是这些算法之一）。
- en: Now that you have an overview of types of AI, take a look at some of the ways
    you can classify ML.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了人工智能的类型概述，让我们看看你可以如何对机器学习进行分类。
- en: Classifying supervised, unsupervised, and reinforcement learning
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对监督学习、无监督学习和强化学习进行分类
- en: 'ML is a very extensive field of study; that’s why it is very important to have
    a clear definition of its sub-divisions. From a very broad perspective, you can
    split ML algorithms into two main classes: supervised learning and unsupervised
    learning.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是一个非常广泛的研究领域；这就是为什么清晰地定义其子领域非常重要。从非常广泛的角度来看，你可以将机器学习算法分为两大类：监督学习和无监督学习。
- en: Introducing supervised learning
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍监督学习
- en: Supervised algorithms use a class or label (from the input data) as support
    to find and validate the optimal solution. In *Table 1.1*, there is a dataset
    that aims to classify fraudulent transactions from a financial company.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 监督算法使用来自输入数据的类别或标签作为支持来寻找和验证最佳解决方案。在*表1.1*中，有一个数据集旨在从一家金融公司分类欺诈交易。
- en: '| **Day of** **the week** | **Hour** | **Transaction amount** | **Merchant
    type** | **Is fraud?** |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| **星期** | **小时** | **交易金额** | **商户类型** | **是否欺诈** |'
- en: '| Mon | 09:00 | $1000 | Retail | No |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 周一 | 09:00 | $1000 | 零售 | 否 |'
- en: '| Tue | 23:00 | $5500 | E-commerce | Yes |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 周二 | 23:00 | $5500 | 电子商务 | 是 |'
- en: '| Fri | 14:00 | $500 | Travel | No |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 周五 | 14:00 | $500 | 旅行 | 否 |'
- en: '| Mon | 10:00 | $100 | Retail | No |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 周一 | 10:00 | $100 | 零售 | 否 |'
- en: '| Tue | 22:00 | $100 | E-commerce | No |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 周二 | 22:00 | $100 | 电子商务 | 否 |'
- en: '| Tue | 22:00 | $6000 | E-commerce | Yes |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 周二 | 22:00 | $6000 | 电子商务 | 是 |'
- en: Table 1.1 – Sample dataset for supervised learning
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.1 – 监督学习样本数据集
- en: The first four columns are known as **features** or **independent variables**,
    and they can be used by a supervised algorithm to find fraudulent patterns. For
    example, by combining those four features (day of the week, EST hour, transaction
    amount, and merchant type) and six observations (each row is technically one observation),
    you can infer that e-commerce transactions with a value greater than $5,000 and
    processed at night are potentially fraudulent cases.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 前四列被称为**特征**或**自变量**，它们可以被监督算法用来寻找欺诈模式。例如，通过结合这四个特征（星期几、东部标准时间小时、交易金额和商家类型）以及六个观察值（每一行在技术上是一个观察值），你可以推断出价值超过$5,000且在夜间处理的电子商务交易可能是欺诈案例。
- en: Important note
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: In a real scenario, you will have more observations in order to have statistical
    support to make this type of inference.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际场景中，你将拥有更多的观察值，以便有统计支持来做出这种类型的推断。
- en: The key point is that you were able to infer a potential fraudulent pattern
    just because you knew, *a priori*, what is fraud and what is not fraud. This information
    is present in the last column of *Table 1.1* and is commonly referred to as a
    target variable, label, response variable, or dependent variable. If the input
    dataset has a target variable, you should be able to apply supervised learning.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 关键点在于，你能够仅仅因为事先知道什么是欺诈以及什么不是欺诈，就推断出一个潜在的欺诈模式。这种信息在*表1.1*的最后一列中，通常被称为目标变量、标签、响应变量或因变量。如果输入数据集有一个目标变量，你应该能够应用监督学习。
- en: 'In supervised learning, the target variable might store different types of
    data. For instance, it could be a binary column (yes or no), a multi-class column
    (class A, B, or C), or even a numerical column (any real number, such as a transaction
    amount). According to the data type of the target variable, you will find which
    type of supervised learning your problem refers to. *Table 1.2* shows how to classify
    supervised learning into two main groups: **classification** and **regression**
    algorithms:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在监督学习中，目标变量可能存储不同类型的数据。例如，它可能是一个二元列（是或否），一个多类列（A类、B类或C类），甚至是一个数值列（任何实数，如交易金额）。根据目标变量的数据类型，你可以找到你的问题属于哪种类型的监督学习。*表1.2*展示了如何将监督学习分为两大组：**分类**和**回归**算法：
- en: '| **Data type of the** **target variable** | **Sub data type of the** **target
    variable** | **Type of supervised** **learning applicable** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| **目标变量的数据类型** | **目标变量的子数据类型** | **适用的监督学习类型** |'
- en: '| Categorical | Binary | Binary classification |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 二元 | 二元分类 |'
- en: '| Categorical | Multi class | Multi classification |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 分类 | 多类 | 多分类 |'
- en: '| Numerical | N/A | Regression |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 数值 | N/A | 回归 |'
- en: Table 1.2 – Choosing the right type of supervised learning given the target
    variable
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 表1.2 – 根据目标变量选择合适的监督学习类型
- en: While classification algorithms predict a class (either binary or multiple classes),
    regression algorithms predict a real number (either continuous or discrete).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分类算法预测一个类别（要么是二分类，要么是多分类），回归算法预测一个实数（要么是连续的，要么是离散的）。
- en: 'Understanding data types is important to make the right decisions on ML projects.
    You can split data types into two main categories: numerical and categorical data.
    Numerical data can then be split into continuous or discrete subclasses, while
    categorical data might refer to ordinal or nominal data:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 理解数据类型对于在机器学习项目中做出正确的决策非常重要。你可以将数据类型分为两大类：数值数据和分类数据。数值数据可以进一步分为连续或离散子类，而分类数据可能指的是有序或无序数据：
- en: '**Numerical/discrete data** refers to individual and countable items (for example,
    the number of students in a classroom or the number of items in an online shopping
    cart).'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值/离散数据**指的是单个和可数的项目（例如，教室里的学生数量或在线购物车中的商品数量）。'
- en: '**Numerical/continuous data** refers to an infinite number of possible measurements
    and they often carry decimal points (for example, temperature).'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数值/连续数据**指的是无限可能的测量值，它们通常带有小数点（例如，温度）。'
- en: '**Categorical/nominal data** refers to labeled variables with no quantitative
    value (for example, name or gender).'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类/名义数据**指的是没有数量值的标签变量（例如，姓名或性别）。'
- en: '**Categorical/ordinal data** adds a sense of order to a labeled variable (for
    example, education level or employee title level).'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类/有序数据**为标签变量增加了一种顺序感（例如，教育水平或员工职称级别）。'
- en: 'In other words, when choosing an algorithm for your project, you should ask
    yourself: *do I have a target variable? Does it store categorical or numerical
    data?* Answering these questions will put you in a better position to choose a
    potential algorithm that will solve your problem.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，在选择项目中的算法时，你应该问自己：*我有一个目标变量吗？它存储的是分类数据还是数值数据？* 回答这些问题将使你处于更好的位置，以便选择一个可能解决问题的潜在算法。
- en: However, what if you don’t have a target variable? In that case, you are facing
    an unsupervised learning problem. Unsupervised problems do not provide labeled
    data; instead, they provide all the independent variables (or features) that will
    allow unsupervised algorithms to find patterns in the data. The most common type
    of unsupervised learning is **clustering**, which aims to group the observations
    of the dataset into different clusters, purely based on their features. Observations
    from the same cluster are expected to be similar to each other, but very different
    from observations from other clusters. Clustering will be covered in more detail
    in future chapters of this book.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果你没有目标变量怎么办？在这种情况下，你面临的是一个无监督学习问题。无监督问题不提供标记数据；相反，它们提供所有独立的变量（或特征），这将允许无监督算法在数据中找到模式。最常见的一种无监督学习是**聚类**，它旨在根据特征将数据集的观测值分组到不同的簇中，纯粹基于它们的特征。来自同一簇的观测值预计将彼此相似，但与其他簇的观测值非常不同。聚类将在本书未来的章节中详细介绍。
- en: '**Semi-supervised learning** is also present in the ML literature. This type
    of algorithm can learn from partially labeled data (some observations contain
    a label and others do not).'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**半监督学习**也存在于机器学习文献中。这类算法可以从部分标记的数据中学习（一些观测值包含标签，而另一些则没有）。'
- en: Finally, another learning approach that has been taken by another class of ML
    algorithms is **reinforcement learning**. This approach rewards the system based
    on the good decisions that it has made autonomously; in other words, the system
    learns by experience.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，另一类机器学习算法采用的学习方法是**强化学习**。这种方法根据系统自主做出的良好决策进行奖励；换句话说，系统通过经验学习。
- en: You have been learning about approaches and classes of algorithms at a very
    broad level. However, it is time to get specific and introduce the term **model**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你一直在非常广泛地学习算法的途径和类别。然而，现在是时候具体化并介绍术语**模型**了。
- en: The CRISP-DM modeling life cycle
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CRISP-DM 建模生命周期
- en: '**Modeling** is a very common term used in ML when you want to specify the
    steps taken to solve a particular problem. For example, you could create a binary
    classification model to predict whether the transactions from *Table 1.1* are
    fraudulent or not.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，当你想要指定解决特定问题的步骤时，**建模**是一个非常常见的术语。例如，你可以创建一个二元分类模型来预测*表 1.1*中的交易是否欺诈。
- en: 'A model, in this context, represents all the steps to create a solution as
    a whole, which includes (but is not limited to) the algorithm. The **Cross-Industry
    Standard Process for Data Mining**, more commonly referred to as **CRISP-DM**,
    is one of the methodologies that provides guidance on the common steps that you
    should follow to create models. This methodology is widely used by the market
    and is covered in the AWS Machine Learning Specialty exam:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个背景下，一个模型代表创建解决方案的所有步骤，作为一个整体，包括（但不限于）算法。《跨行业数据挖掘标准流程》，更常被称为**CRISP-DM**，是提供指导的方法之一，指导你应遵循的常见步骤来创建模型。这种方法在市场上被广泛使用，并在
    AWS 机器学习专业考试中有所涉及：
- en: '![Figure 1.3 – CRISP-DM methodology](img/B21197_01_03.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.3 – CRISP-DM 方法](img/B21197_01_03.jpg)'
- en: Figure 1.3 – CRISP-DM methodology
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.3 – CRISP-DM 方法
- en: Everything starts with business understanding, which will produce the business
    objectives (including success criteria), situation assessment, data mining goals,
    and project plan (with an initial assessment of tools and techniques). During
    the situation assessment, you should also look into an inventory of resources,
    requirements, assumptions and constraints, risks, terminology, costs, and benefits.
    Every single assumption and success criterion matters when you are modeling.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 一切始于业务理解，这将产生业务目标（包括成功标准）、情况评估、数据挖掘目标以及项目计划（包括对工具和技术的初步评估）。在情况评估期间，你还应该考虑资源清单、需求、假设和约束、风险、术语、成本和收益。当你建模时，每一个假设和成功标准都很重要。
- en: The next step is known as data understanding, where you will collect raw data,
    describe it, explore it, and check its quality. This is an initial assessment
    of the data that will be used to create the model. Again, data scientists must
    be skeptical. You must be sure you understand all the nuances of the data and
    its source.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个步骤被称为数据理解，在这个阶段，你将收集原始数据，描述它，探索它，并检查其质量。这是对将要用于创建模型的数据的初步评估。再次强调，数据科学家必须保持怀疑态度。你必须确保你理解数据的所有细微差别及其来源。
- en: The data preparation phase is actually the one that usually consumes most of
    the time during modeling. In this phase, you need to select and filter the data,
    clean it according to the task that needs to be performed, come up with new attributes,
    integrate the data with other data sources, and format it as expected by the algorithm
    that will be applied. These tasks are often called **feature engineering**.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备阶段实际上是建模过程中通常消耗最多时间的一个阶段。在这个阶段，你需要选择和过滤数据，根据需要执行的任务对其进行清理，提出新的属性，将数据与其他数据源集成，并按照将要应用的算法的预期格式化。这些任务通常被称为**特征工程**。
- en: 'Once the data is prepared, you can finally start the modeling phase. Here is
    where the algorithms come in. You should start by ensuring the selection of the
    right technique. Remember: according to the presence or absence of a target variable
    (and its data type), you will have different algorithms to choose from. Each modeling
    technique might carry some implicit assumptions of which you have to be aware.
    For example, if you choose a multiple linear regression algorithm to predict house
    prices, you should be aware that this type of model expects a linear relationship
    between the variables of your data.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备就绪，你就可以最终开始建模阶段。这是算法发挥作用的地方。你应该首先确保选择正确的技术。记住：根据目标变量（及其数据类型）的存在与否，你将拥有不同的算法可供选择。每种建模技术可能都包含一些你必须注意的隐含假设。例如，如果你选择多重线性回归算法来预测房价，你应该意识到这种类型的模型期望你的数据变量之间存在线性关系。
- en: There are hundreds of algorithms out there and each of them might have its own
    assumptions. After choosing the ones that you want to test in your project, you
    should spend some time checking their specifics. In later chapters of this book,
    you will learn about some of them.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在有数百种算法，每种算法可能都有自己的假设。在选择你想要在项目中测试的算法之后，你应该花一些时间检查它们的特定细节。在本书的后续章节中，你将了解其中的一些。
- en: Important note
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Some algorithms incorporate in their logic a sub-process known as **feature
    selection**. This is a step where the most important features will be selected
    to build your best model. Decision trees are examples of algorithms that perform
    feature selection automatically. You will learn about feature selection in more
    detail later on, since there are different ways to select the best variables for
    your model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法在其逻辑中集成了称为**特征选择**的子过程。这是一个选择最重要的特征以构建最佳模型的过程。决策树是自动执行特征选择的算法的例子。你将在稍后更详细地了解特征选择，因为选择模型最佳变量的方法有很多种。
- en: During the modeling phase, you should also design a testing approach for the
    model, defining which evaluation metrics will be used and how the data will be
    split. With that in place, you can finally build the model by setting the hyperparameters
    of the algorithm and feeding the model with data. This process of feeding the
    algorithm with data to find a good estimator is known as the **training process**.
    The data used to feed the model is known as **training data**. There are different
    ways to organize the training and **testing data**, which you will learn about
    in this chapter.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模阶段，你还应该为模型设计一个测试方法，定义将使用哪些评估指标以及数据将如何分割。有了这些，你就可以通过设置算法的超参数并使用数据来喂养模型，最终构建模型。这个过程被称为**训练过程**，用于喂养模型的数据被称为**训练数据**。有不同方式来组织训练和**测试数据**，你将在本章中了解这些内容。
- en: Important note
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: 'ML algorithms are built of parameters and hyperparameters. Parameters are learned
    from the data; for example: a decision-tree-based algorithm might learn from the
    training data that a particular feature should compose its root level based on
    information gain assessments. Hyperparameters, on the other hand, are used to
    control the learning process. Taking the same example about decision trees, you
    could specify the maximum allowed depth of the tree (regardless of the training
    data). Hyperparameter tuning is a very important topic in the exam and will be
    covered in fine-grained detail later on.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法由参数和超参数组成。参数是从数据中学习的；例如：一个基于决策树的算法可能会从训练数据中学习到，根据信息增益评估，某个特征应该组成其根级别。另一方面，超参数用于控制学习过程。以相同的决策树为例，你可以指定树的最大允许深度（无论训练数据如何）。超参数调整是考试中的一个非常重要的话题，将在稍后进行详细讨论。
- en: Once the model is trained, you can evaluate and review the results in order
    to propose the next steps. If the results are not acceptable (based on business
    success criteria), you should go back to earlier steps to check what else can
    be done to improve the model’s results. It could either be the subtle tuning of
    the hyperparameters of the algorithm, a new data preparation step, or even the
    redefinition of business drivers. On the other hand, if the model quality is acceptable,
    you can move on to the deployment phase.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，你可以评估和审查结果，以便提出下一步行动。如果结果不可接受（基于业务成功标准），你应该回到早期步骤，检查还可以做些什么来提高模型的结果。这可能包括算法超参数的微妙调整、新的数据准备步骤，甚至重新定义业务驱动因素。另一方面，如果模型质量可接受，你可以进入部署阶段。
- en: 'In this last phase of the CRISP-DM methodology, you have to think about the
    deployment plan, monitoring, and maintenance of the model. You can look at this
    step from two perspectives: training and inference. The **training pipeline**
    consists of those steps needed to train the model, which include data preparation,
    hyperparameter definition, data splitting, and model training itself. Somehow,
    you must store all the model artifacts somewhere, since they will be used by the
    next pipeline that needs to be developed: the **inference pipeline**.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在CRISP-DM方法论的最后一个阶段，你必须考虑模型的部署计划、监控和维护。你可以从两个角度看待这一步：训练和推理。**训练管道**包括训练模型所需的步骤，包括数据准备、超参数定义、数据拆分和模型训练本身。无论如何，你必须将所有模型工件存储在某个地方，因为它们将被下一个需要开发的管道使用：**推理管道**。
- en: The inference pipeline just uses model artifacts to execute the model against
    brand-new observations (data that has never been seen by the model during the
    training phase). For example, if the model was trained to identify fraudulent
    transactions, this is the time when new transactions will pass through the model
    to be classified.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 推理管道仅使用模型工件来对全新的观测值（在训练阶段从未被模型见过的数据）执行模型。例如，如果模型被训练来识别欺诈交易，那么这就是新交易将通过模型进行分类的时候。
- en: In general, models are trained once (through the training pipeline) and executed
    many times (through the inference pipeline). However, after some time, it is expected
    that there will be some model degradation, also known as **model drift**. This
    phenomenon happens because the model is usually trained in a static training set
    that aims to represent the business scenario at a given point in time; however,
    businesses evolve, and it might be necessary to retrain the model on more recent
    data to capture new business aspects. That’s why it is important to keep tracking
    model performance even after model deployment.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型只训练一次（通过训练管道）并执行多次（通过推理管道）。然而，经过一段时间后，预计会出现一些模型退化，也称为**模型漂移**。这种现象发生是因为模型通常在静态训练集上训练，旨在代表某一时间点的业务场景；然而，业务在不断发展，可能需要使用更近期的数据进行模型重新训练，以捕捉新的业务方面。这就是为什么在模型部署后，跟踪模型性能同样重要的原因。
- en: 'The CRISP-DM methodology is so important to the context of the AWS Machine
    Learning Specialty exam that, if you look at the four domains covered by AWS,
    you will realize that they were generalized from the CRISP-DM stages: data engineering,
    exploratory data analysis, modeling, and ML implementation and operations.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: CRISP-DM方法论对于AWS机器学习专业考试的内容至关重要，如果你查看AWS涵盖的四个领域，你会意识到它们是从CRISP-DM阶段概括出来的：数据工程、探索性数据分析、建模和机器学习实施与运营。
- en: You now understand all the key stages of a modeling pipeline and you know that
    the algorithm itself is just part of a larger process! Next, you will see how
    to split your data to create and validate ML models.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经理解了建模流程的所有关键阶段，并且知道算法本身只是更大流程的一部分！接下来，你将了解如何拆分数据以创建和验证机器学习模型。
- en: Data splitting
  id: totrans-103
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据拆分
- en: Training and evaluating ML models are key tasks of the modeling pipeline. ML
    algorithms need data to find relationships among features in order to make inferences,
    but those inferences need to be validated before they are moved to production
    environments.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练和评估机器学习模型是建模流程中的关键任务。机器学习算法需要数据来找到特征之间的关系以便进行推断，但这些推断在移动到生产环境之前需要得到验证。
- en: The dataset used to train ML models is commonly called the training set. This
    training data must be able to represent the real environment where the model will
    be used; it will be useless if that requirement is not met.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 用于训练机器学习模型的集合通常被称为训练集。这些训练数据必须能够代表模型将被使用的真实环境；如果不符合这一要求，它们将毫无用处。
- en: Coming back to the fraud example presented in *Table 1.1*, based on the training
    data, you found that e-commerce transactions with a value greater than $5,000
    and processed at night are potentially fraudulent cases. With that in mind, after
    applying the model in a production environment, the model is supposed to flag
    similar cases, as learned during the training process.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 回到*表1.1*中提到的欺诈示例，根据训练数据，你发现价值超过$5,000且在夜间处理的电子商务交易可能是欺诈案例。考虑到这一点，在将模型应用于生产环境后，模型应该标记出在训练过程中学习到的类似案例。
- en: 'Therefore, if those cases only exist in the training set, the model will flag
    **false positive** cases in production environments. The opposite scenario is
    also true: if there is a particular fraud case in production data, not reflected
    in the training data, the model will flag a lot of **false negative** cases. False
    positive and false negative ratios are just two of many quality metrics that you
    can use for model validation. These metrics will be covered in much more detail
    later on.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果这些案例仅存在于训练集中，模型在生产环境中将标记**假阳性**案例。相反的情况也是真实的：如果生产数据中存在某个欺诈案例，而在训练数据中没有反映出来，模型将标记大量**假阴性**案例。假阳性和假阴性比率只是许多可用于模型验证的质量指标中的两个。这些指标将在稍后的内容中详细讨论。
- en: 'By this point, you should have a clear understanding of the importance of having
    a good training set. Now, supposing you do have a valid training set, how could
    you have some level of confidence that this model will perform well in production
    environments? The answer is by using testing and validation sets:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你应该已经清楚地理解了拥有一个好的训练集的重要性。现在，假设你确实有一个有效的训练集，你如何能够有一定程度的信心认为这个模型将在生产环境中表现良好？答案是使用测试集和验证集：
- en: '![Figure 1.4 – Data splitting](img/B21197_01_04.jpg)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图1.4 – 数据拆分](img/B21197_01_04.jpg)'
- en: Figure 1.4 – Data splitting
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4 – 数据拆分
- en: '*Figure 1**.4* shows the different types of data splitting that you can have
    during training and inference pipelines. The training data is used to create the
    model; the testing data is used to extract the final model quality metrics. The
    testing data *cannot* be used during the training process for any reason other
    than to extract model metrics.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.4*展示了在训练和推理管道中可能遇到的不同数据拆分类型。训练数据用于创建模型；测试数据用于提取最终的模型质量指标。出于任何原因，测试数据*不能*在训练过程中使用，除了提取模型指标。'
- en: 'The reason to avoid using the testing data during training is simple: you *cannot*
    let the model learn on top of the data that will be used to validate it. This
    technique of holding one piece of the data for testing is often called **hold-out
    validation**.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 避免在训练过程中使用测试数据的原因很简单：你*不能*让模型在用于验证的数据上学习。这种保留一部分数据用于测试的技术通常被称为**保留验证**。
- en: The box on the right side of *Figure 1**.4* represents the production data.
    Production data usually comes in continuously and you have to execute the inference
    pipeline in order to extract model results from it. No training, nor any other
    type of recalculation, is performed on top of production data; you just have to
    pass it through the inference pipeline as it is.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '*图1.4*右侧的框代表生产数据。生产数据通常连续不断地到来，你必须执行推理管道以从中提取模型结果。在生产数据上不进行任何训练，也不进行任何其他类型的重新计算；你只需将其按原样通过推理管道即可。'
- en: From a technical perspective, most ML libraries implement training steps with
    the `.fit` method, while inference steps are implemented by the `.transform` *or*
    `.predict` method. Again, this is just a common pattern used by most ML libraries,
    but be aware that you might find different name conventions across ML libraries.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，大多数机器学习库使用`.fit`方法实现训练步骤，而推理步骤则通过`.transform`或`.predict`方法实现。再次强调，这只是一个大多数机器学习库使用的常见模式，但请注意，你可能会在不同的机器学习库中找到不同的命名约定。
- en: Still looking at *Figure 1**.4*, there is another box, close to the training
    data, named **Validation data**. This is a subset of the training set often used
    to support the creation of the best model, before moving on to the testing phase.
    You will learn about validation sets in much more detail, but first, you should
    understand why you need them.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然参考*图1.4*，还有一个靠近训练数据的数据框，名为**验证数据**。这是训练集的一个子集，通常用于在进入测试阶段之前支持最佳模型的选择。你将更详细地了解验证集，但首先，你应该了解为什么你需要它们。
- en: Overfitting and underfitting
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: 'ML models might suffer from two types of fitting issues: **overfitting** and
    **underfitting**. Overfitting means that your model performs very well on the
    training data but cannot be generalized to other datasets, such as testing and,
    even worse, production data. In other words, if you have an overfitted model,
    it only works on your training data.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型可能会遭受两种类型的拟合问题：**过拟合**和**欠拟合**。过拟合意味着你的模型在训练数据上表现非常好，但不能推广到其他数据集，如测试数据，甚至更糟糕的是，生产数据。换句话说，如果你有一个过拟合的模型，它只在你训练的数据上工作。
- en: When you are building ML models, you want to create solutions that are able
    to generalize what they have learned and infer decisions on other datasets that
    follow the same data distribution. A model that only works on the data that it
    was trained on is useless. Overfitting usually happens due to the large number
    of features or the lack of configuration of the hyperparameters of the algorithm.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在构建机器学习模型时，你希望创建能够推广他们所学的知识并推断出其他遵循相同数据分布的数据集的决策的解决方案。一个只在其训练数据上工作的模型是无用的。过拟合通常是由于特征数量过多或算法超参数配置不足造成的。
- en: On the other hand, underfitted models cannot fit the data during the training
    phase. As a result, they are so generic that they can’t perform well within the
    training, testing, or production data. Underfitting usually happens due to the
    lack of good features/observations or due to the lack of time to train the model
    (some algorithms need more iterations to properly fit the model).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，欠拟合模型在训练阶段无法拟合数据。因此，它们过于通用，无法在训练、测试或生产数据中表现良好。欠拟合通常是由于缺乏良好的特征/观察结果，或者由于缺乏训练模型的时间（某些算法需要更多的迭代来正确拟合模型）。
- en: Both overfitting and underfitting need to be avoided. There are many modeling
    techniques to work around them. For instance, you will learn about the commonly
    used **cross-validation** technique and its relationship with the validation data
    box shown in *Figure 1**.4*.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 需要避免过拟合和欠拟合。有许多建模技术可以解决这些问题。例如，你将了解常用的**交叉验证**技术及其与图*1.4*中显示的验证数据框的关系。
- en: Applying cross-validation and measuring overfitting
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用交叉验证和测量过拟合
- en: Cross-validation is a technique where you split the training set into training
    and validation sets. The model is then trained on the training set and tested
    on the validation set. The most common cross-validation strategy is known as **k-fold
    cross-validation**, where *k* is the number of splits of the training set.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是一种将训练集分成训练集和验证集的技术。然后，模型在训练集上训练并在验证集上测试。最常用的交叉验证策略被称为**k折交叉验证**，其中k是训练集分割的数量。
- en: 'Using k-fold cross-validation and assuming the value of *k* equals 10, you
    are splitting the training set into 10 folds. The model will be trained and tested
    10 times. On each iteration, it uses 9 splits for training and leaves one split
    for testing. After 10 executions, the evaluation metrics extracted from each iteration
    are averaged and will represent the final model performance during the training
    phase, as shown in *Figure 1**.5*:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用k折交叉验证并假设k的值为10，你将训练集分成10个部分。模型将被训练和测试10次。在每次迭代中，它使用9个部分进行训练，留下一个部分进行测试。经过10次执行后，从每个迭代中提取的评估指标将被平均，并将代表训练阶段的最终模型性能，如图*1.5*所示：
- en: '![Figure 1.5 – Cross-validation in action](img/B21197_01_05.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![图1.5 – 交叉验证的实际应用](img/B21197_01_05.jpg)'
- en: Figure 1.5 – Cross-validation in action
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.5 – 交叉验证的实际应用
- en: Another common cross-validation technique is known as **leave-one-out cross-validation**
    (**LOOCV**). In this approach, the model is executed many times and, within each
    iteration, one observation is separated for testing and all the others are used
    for training.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的交叉验证技术被称为**留一法交叉验证**（**LOOCV**）。在这种方法中，模型被多次执行，并且在每次迭代中，一个观测值被分离出来用于测试，其余的用于训练。
- en: 'There are many advantages of using cross-validation during training:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间使用交叉验证有许多优点：
- en: You mitigate overfitting in the training data since the model is always trained
    on a particular chunk of data and tested on another chunk that hasn’t been used
    for training.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你通过在特定数据块上训练模型并在另一个未用于训练的数据块上测试来减轻训练数据中的过拟合问题。
- en: You avoid overfitting in the test data since there is no need to keep using
    the testing data to optimize the model.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你避免了测试数据中的过拟合，因为不需要继续使用测试数据来优化模型。
- en: You expose the presence of overfitting or underfitting. If the model performance
    in the training/validation data is very different from the performance observed
    in the testing data, something is wrong.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你揭示了过拟合或欠拟合的存在。如果模型在训练/验证数据中的性能与在测试数据中观察到的性能非常不同，那么有问题。
- en: It might be worth diving into the third item on that list since it is widely
    covered in the AWS Machine Learning Specialty exam. For instance, assume you are
    creating a binary classification model, using cross-validation during training,
    and using a testing set to extract final metrics (hold-out validation). If you
    get 80% accuracy in the cross-validation results and 50% accuracy in the testing
    set, it means that the model was overfitted to the training set, and so cannot
    be generalized to the testing set.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，列表上的第三项在AWS机器学习专业考试中被广泛涉及。例如，假设你正在创建一个二元分类模型，在训练期间使用交叉验证，并使用测试集来提取最终指标（保留验证）。如果你在交叉验证结果中得到80%的准确率，在测试集中得到50%的准确率，这意味着模型过度拟合了训练集，因此不能推广到测试集。
- en: On the other hand, if you get 50% accuracy in the training set and 80% accuracy
    in the testing set, there is a systemic issue in the data. It is very likely that
    the training and testing sets do not follow the same distribution.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你在训练集中得到50%的准确率，在测试集中得到80%的准确率，那么数据中存在系统性问题。很可能训练集和测试集没有遵循相同的分布。
- en: Important note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 重要注意事项
- en: Accuracy is a model evaluation metric commonly used on classification models.
    It measures how often the model made a correct decision during its inference process.
    That metric was selected just for the sake of demonstration, but be aware that
    there are many other evaluation metrics applicable for each type of model (which
    will be covered at the appropriate time).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是分类模型中常用的模型评估指标。它衡量模型在推理过程中做出正确决策的频率。该指标仅用于演示目的，但请注意，有许多其他适用于每种类型模型的评估指标（将在适当的时候介绍）。
- en: Bootstrapping methods
  id: totrans-135
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 自举方法
- en: Cross-validation is a good strategy to validate ML models, and you should try
    it in your daily activities as a data scientist. However, you should also know
    about other resampling techniques available out there. **Bootstrapping** is one
    of them.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 交叉验证是验证机器学习模型的好策略，你应该作为数据科学家在日常活动中尝试它。然而，你也应该了解其他可用的重采样技术。**自举**就是其中之一。
- en: While cross-validation works *with no replacement*, a bootstrapping approach
    works *with replacement*. With replacement means that, while you are drawing multiple
    random samples from a population dataset, the same observation might be duplicated
    across samples.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然交叉验证是无替换的，但自举方法是有替换的。有替换意味着，当你从总体数据集中抽取多个随机样本时，相同的观测值可能会在样本中重复出现。
- en: 'Usually, bootstrapping is not used to validate models as you do in the traditional
    cross-validation approach. The reason is simple: since it works with replacement,
    the same observation used for training could potentially be used for testing,
    too. This would result in inflated model performance metrics since the estimator
    is likely to be correct when predicting an observation that was already seen in
    the training set.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，自举不会像在传统的交叉验证方法中那样用于验证模型。原因是简单的：因为它是有替换的，用于训练的相同观测值可能会被用于测试。这会导致模型性能指标膨胀，因为估计器在预测训练集中已经看到的观测值时很可能是正确的。
- en: Bootstrapping is often used by ML algorithms in an embedded way that requires
    resampling capabilities to process the data. In this context, bootstrapping is
    not used to *validate* the model but to *create* the model. **Random forest**,
    which will be covered in [*Chapter 6*](B21197_06.xhtml#_idTextAnchor708)*, Applying
    Machine Learning Algorithms*, is one of those algorithms that use bootstrapping
    internally for model building.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 自举法通常被机器学习算法以嵌入式方式使用，这需要重采样能力来处理数据。在这种情况下，自举法不是用来**验证**模型，而是用来**创建**模型。将在[*第6章*](B21197_06.xhtml#_idTextAnchor708)“应用机器学习算法”中介绍的**随机森林**是那些在模型构建内部使用自举法的算法之一。
- en: Designing a good data splitting/sampling strategy is crucial to the success
    of the model or the algorithm. You should come up with different approaches to
    split your data, check how the model is performing on each split, and make sure
    those splits represent the real scenario where the model will be used.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 设计一个好的数据分割/采样策略对于模型或算法的成功至关重要。你应该想出不同的方法来分割你的数据，检查模型在每个分割上的表现，并确保这些分割代表了模型将被使用的真实场景。
- en: The variance versus bias trade-off
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 方差与偏差权衡
- en: 'Any ML model is supposed to contain errors. There are three types of errors
    that you can find in models: **bias** errors, **variance** errors, and **unexplained**
    errors. The last one, as expected, cannot be explained. It is often related to
    the context of the problem and the relationships between the variables (you can’t
    control it).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 任何机器学习模型都应包含错误。你可以在模型中找到三种类型的错误：**偏差**错误、**方差**错误和**未解释**错误。最后一个，正如预期的那样，无法解释。它通常与问题的上下文和变量之间的关系有关（你无法控制它）。
- en: The other two types of errors can be controlled during modeling. You can say
    that there is a trade-off between bias and variance errors because one will influence
    the other. In this case, increasing bias will decrease variance and vice versa.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 其他两种类型的错误可以在建模过程中进行控制。你可以这样说，偏差和方差错误之间存在权衡，因为一种将影响另一种。在这种情况下，增加偏差将减少方差，反之亦然。
- en: Bias errors relate to assumptions taken by the model to learn the target function,
    the one that you want to solve. Some types of algorithms, such as linear algorithms,
    usually carry over that type of error because they make a lot of assumptions during
    model training. For example, linear models assume that the relationship present
    in the data is linear. Linear regression and logistic regression are types of
    algorithms that, in general, contain high bias. Decision trees, on the other hand,
    are types of algorithms that make fewer assumptions about the data and contain
    less bias.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差错误与模型为了学习目标函数（即你想要解决的问题）所采取的假设相关。某些类型的算法，如线性算法，通常携带这种类型的错误，因为它们在模型训练期间做出了很多假设。例如，线性模型假设数据中存在线性关系。线性回归和逻辑回归是通常包含高偏差的算法类型。另一方面，决策树是关于数据做出较少假设并包含较少偏差的算法类型。
- en: Variance relates to the difference in estimations that the model performs on
    different training data. Models with high variance usually overfit the training
    set. Decision trees are examples of algorithms with high variance (they usually
    rely a lot on specifics of the training set, failing to generalize), and linear
    and logistic regression are examples of algorithms with low variance. It does
    not mean that decision trees are bad estimators; it just means that you need to
    prune (optimize) them during training.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 方差与模型在不同训练数据上执行估计的差异相关。具有高方差的模型通常会过度拟合训练集。决策树是具有高方差（通常过度依赖训练集的特定信息，未能泛化）的算法的例子，而线性回归和逻辑回归是具有低方差的算法的例子。这并不意味着决策树是差的估计器；这只是意味着你需要在训练过程中对其进行修剪（优化）。
- en: That being said, the goal of any model is to minimize both bias and variance.
    However, as already mentioned, each one will impact the other in the opposite
    direction. For the sake of demonstration, consider a decision tree to understand
    how this trade-off works.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，任何模型的目标都是最小化偏差和方差。然而，如前所述，每一个都会以相反的方向影响另一个。为了演示这种权衡是如何工作的，考虑一个决策树。
- en: Decision trees are nonlinear algorithms and often contain low bias and high
    variance. In order to decrease variance, you can prune the tree and set the `max_depth`
    hyperparameter (the maximum allowed depth of the tree) to 10\. That will force
    a more generic model, reducing variance. However, that change will also force
    the model to make more assumptions (since it is now more generic) and increase
    bias.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是非线性算法，通常具有低偏差和高方差。为了降低方差，你可以剪枝树并设置`max_depth`超参数（树的最大允许深度）为10。这将迫使模型更加通用，从而降低方差。然而，这种变化也将迫使模型做出更多假设（因为它现在更加通用）并增加偏差。
- en: Shuffling your training set
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 洗牌你的训练集
- en: Now that you know what variance and data splitting are, you can go a little
    deeper into the training dataset requirements. You are very likely to find questions
    around data shuffling in the exam. This process consists of randomizing your training
    dataset before you start using it to fit an algorithm.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了方差和数据拆分，你可以对训练数据集的要求进行更深入的了解。你很可能在考试中会遇到关于数据洗牌的问题。这个过程包括在你开始使用训练数据拟合算法之前对其进行随机化。
- en: Data shuffling will help the algorithm to reduce variance by creating a more
    generalizable model. For example, let’s say your training represents a binary
    classification problem and it is sorted by the target variable (all cases belonging
    to class “0” appear first, then all the cases belonging to class “1”).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 数据洗牌将帮助算法通过创建一个更具泛化能力的模型来减少方差。例如，假设你的训练数据代表一个二元分类问题，并且它是按照目标变量排序的（所有属于类别“0”的案例首先出现，然后是所有属于类别“1”的案例）。
- en: When you fit an algorithm on this sorted data (especially some algorithms that
    rely on **batch processing**), it will make strong assumptions about the pattern
    of one of the classes, since it is very likely that it won’t be able to create
    random batches of data with a good representation of both classes. Once the algorithm
    builds strong assumptions about the training data, it might be difficult for it
    to change them.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在这些排序后的数据上拟合一个算法（尤其是那些依赖于**批量处理**的算法）时，它将对某一类别的模式做出强烈的假设，因为很可能它无法创建出能良好代表两类数据的随机批次。一旦算法对训练数据建立了强烈的假设，它可能很难改变这些假设。
- en: Important note
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Some algorithms are able to execute the training process by fitting the data
    in chunks, also known as batches. This approach lets the model learn more frequently
    since it will make partial assumptions after processing each batch of data (instead
    of making decisions only after processing the entire dataset).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法能够通过将数据分块拟合来执行训练过程，也称为批次。这种方法让模型能够更频繁地学习，因为它会在处理完每一批次数据后做出部分假设（而不是在处理整个数据集之后做出决策）。
- en: On the other hand, there is no need to shuffle the testing set, since it will
    be used only by the inference process to check model performance.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，没有必要洗牌测试集，因为它将仅用于推理过程以检查模型性能。
- en: Modeling expectations
  id: totrans-155
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模期望
- en: So far, you have learned about model building, validation, and management. You
    can now complete the foundations of ML by learning about a couple of other expectations
    while modeling.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学习了模型构建、验证和管理。现在，你可以通过学习建模时的其他一些期望来完善机器学习的基础。
- en: 'The first one is **parsimony**. Parsimony describes models that offer the simplest
    explanation and fit the best results when compared with other models. Here’s an
    example: while creating a linear regression model, you realize that adding 10
    more features will improve your model’s performance by 0.001%. In this scenario,
    you should consider whether this performance improvement is worth the cost of
    parsimony (since your model will become more complex). Sometimes it is worth it,
    but most of the time it is not. You need to be skeptical and think according to
    your business case.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个原则是**简约性**。简约性描述的是那些提供最简单解释并且与其他模型相比拟合最佳结果的模型。这里有一个例子：在创建线性回归模型时，你意识到添加10个更多特征只会使你的模型性能提高0.001%。在这种情况下，你应该考虑这种性能提升是否值得简约性的成本（因为你的模型将变得更加复杂）。有时这是值得的，但大多数时候并不值得。你需要保持怀疑态度，并根据你的业务案例进行思考。
- en: 'Parsimony directly supports **interpretability**. The simpler your model is,
    the easier it is to explain it. However, there is a battle between interpretability
    and **predictivity**: if you focus on predictive power, you are likely to lose
    some interpretability. Again, you must select what is the best situation for your
    use case.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 简约性直接支持**可解释性**。你的模型越简单，解释它就越容易。然而，在**可解释性**和**预测性**之间存在着一场斗争：如果你专注于预测能力，你很可能会失去一些可解释性。再次强调，你必须选择最适合你用例的最佳情况。
- en: Introducing ML frameworks
  id: totrans-159
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍机器学习框架
- en: Being aware of some ML frameworks will put you in a much better position to
    pass the AWS Machine Learning Specialty exam. There is no need to master these
    frameworks since this is not a framework-specific certification; however, knowing
    some common terms and solutions will help you to understand the context of the
    problems/questions.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 了解一些机器学习框架将使你在通过AWS机器学习专业考试时处于更有利的地位。没有必要掌握这些框架，因为这不是针对特定框架的认证；然而，了解一些常见术语和解决方案将有助于你理解问题/问题的背景。
- en: '**scikit-learn** is probably the most popular ML framework that you should
    be aware of. It is an open source Python package that provides implementations
    of ML algorithms such as decision trees, support vector machines, linear regression,
    and many others. It also implements classes for data preprocessing, for example,
    one-hot encoding, label encoders, principal component analysis, and so on. All
    these preprocessing methods (and many others) will be covered in later sections
    of this book.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**scikit-learn**可能是你应该了解的最流行的机器学习框架。它是一个开源的Python包，提供了诸如决策树、支持向量机、线性回归等机器学习算法的实现。它还实现了数据预处理类，例如，独热编码、标签编码器、主成分分析等。本书后面的章节将涵盖所有这些预处理方法（以及许多其他方法）。'
- en: 'The downside of scikit-learn is the fact that it needs customization to scale
    up through multiple machines. There is another ML library that is very popular
    because of the fact that it can handle multiprocessing straight away: **Spark’s**
    **ML library**.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn的缺点是需要定制才能通过多台机器进行扩展。还有一个非常流行的机器学习库，因为它可以直接处理多进程：**Spark的ML库**。
- en: As the name suggests, it is an ML library that runs on top of **Apache Spark**,
    which is a unified analytical multi-processing framework used to process data
    on multiple machines. AWS offers a specific service that allows developers to
    create Spark clusters with a few clicks, known as **EMR**. Additionally, SageMaker
    (a fully managed ML service provided by AWS, which you will cover in a separate
    chapter) is well integrated with Apache Spark.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，它是一个在**Apache Spark**上运行的机器学习库，Apache Spark是一个用于在多台机器上处理数据的统一分析多处理框架。AWS提供了一种特定服务，允许开发者通过几点击就能创建Spark集群，称为**EMR**。此外，SageMaker（AWS提供的一个完全托管的机器学习服务，你将在单独的章节中介绍）与Apache
    Spark很好地集成。
- en: The Spark ML library is in constant development. As of the time of writing,
    it offers support to many ML classes of algorithms, such as classification and
    regression, clustering, and collaborative filtering. It also offers support for
    basic statistics computation, such as correlations and some hypothesis tests,
    as well as many data transformations, such as one-hot encoding, principal component
    analysis, min-max scaling, and others.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Spark ML库正在不断发展。截至本书编写时，它支持许多机器学习算法类别，如分类和回归、聚类和协同过滤。它还提供了基本统计计算的支持，例如相关性和一些假设检验，以及许多数据转换，如独热编码、主成分分析、最小-最大缩放等。
- en: Another very popular ML framework is known as **TensorFlow**. This ML framework
    was created by the Google team and it is used for numerical computation and large-scale
    ML model development. TensorFlow implements not only traditional ML algorithms
    but also DL models.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个非常流行的机器学习框架被称为**TensorFlow**。这个机器学习框架是由谷歌团队创建的，用于数值计算和大规模机器学习模型开发。TensorFlow不仅实现了传统的机器学习算法，还实现了深度学习模型。
- en: TensorFlow is considered a low-level API for model development, which means
    that it can be very complex to develop more sophisticated models, such as **transformers**
    (for text mining). As an attempt to facilitate model development, other ML frameworks
    were built on top of TensorFlow to make it easier. One of these high-level frameworks
    is **Keras**. With Keras, developers can create complex DL models with just a
    few lines of code. More recently, Keras was incorporated into TensorFlow and it
    can be now called inside the TensorFlow library.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow被认为是一个用于模型开发的低级API，这意味着开发更复杂的模型，例如**transformers**（用于文本挖掘），可能会非常复杂。为了便于模型开发，其他机器学习框架被构建在TensorFlow之上，使其更容易使用。其中之一的高级框架是**Keras**。使用Keras，开发者只需几行代码就能创建复杂的深度学习模型。最近，Keras被整合到TensorFlow中，现在可以在TensorFlow库内部调用。
- en: '**MXNet** is another open source DL library. Using MXNet, you can scale up
    neural network-based models using multiple GPUs running on multiple machines.
    It also supports different programming languages, such as Python, R, Scala, and
    Java.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**MXNet**是另一个开源的深度学习库。使用MXNet，你可以通过在多台机器上运行多个GPU来扩展基于神经网络的模型。它还支持不同的编程语言，如Python、R、Scala和Java。'
- en: '**Graphical processing unit** (**GPU**) support is particularly important in
    DL libraries such as TensorFlow and MXNet. These libraries allow developers to
    create and deploy neural network-based models with multiple layers. The training
    process of neural networks relies a lot on matrix operations, which perform much
    better on GPUs rather than on CPUs. That’s why these DL libraries commonly offer
    GPU support. AWS also offers EC2 instances with GPU enabled.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '**图形处理单元**（**GPU**）支持在深度学习库如TensorFlow和MXNet中尤为重要。这些库允许开发者创建和部署具有多个层的神经网络模型。神经网络训练过程很大程度上依赖于矩阵运算，这些运算在GPU上比在CPU上表现更好。这就是为什么这些深度学习库通常提供GPU支持。AWS还提供启用GPU的EC2实例。'
- en: These ML frameworks need a special channel to communicate with GPU units. NVIDIA,
    the most common supplier of GPUs nowadays, has created an API called the **Compute
    Unified Device Architecture** (**CUDA**). CUDA is used to configure GPU units
    on NVIDIA devices; for example, setting up caching memory and the number of threads
    needed to train a neural network model. There is no need to master CUDA or GPU
    architecture for the AWS Machine Learning Specialty exam, but you definitely need
    to know what they are and how DL models take advantage of them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 这些机器学习框架需要一个特殊的通道来与GPU单元通信。NVIDIA，目前最常见的GPU供应商，创建了一个名为**Compute Unified Device
    Architecture**（**CUDA**）的API。CUDA用于配置NVIDIA设备上的GPU单元；例如，设置缓存内存和训练神经网络模型所需的线程数。对于AWS机器学习专业考试，不需要掌握CUDA或GPU架构，但你确实需要了解它们是什么以及深度学习模型如何利用它们。
- en: 'Last, but not least, you should also be aware of some development frameworks
    widely used by the data science community, but not necessarily to create ML models.
    These frameworks interoperate with ML libraries to facilitate data manipulation
    and calculations. For example: **pandas** is a Python library that provides data
    processing capabilities and **NumPy** is an open source Python library that provides
    numerical computing.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，但同样重要的是，你还应该了解一些数据科学社区广泛使用的开发框架，但并不一定用于创建机器学习模型。这些框架与机器学习库交互，以促进数据处理和计算。例如：**pandas**是一个提供数据处理能力的Python库，**NumPy**是一个提供数值计算的开放源代码Python库。
- en: These terms and libraries are so incorporated into data scientists’ daily routines
    that they might come up during the exam to explain some problem domain for you.
    Being aware of what they are will help you to quickly understand the context of
    the question.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这些术语和库已经如此融入数据科学家的日常工作中，以至于它们可能在考试中用来为你解释某个问题领域。了解它们将有助于你快速理解问题的上下文。
- en: ML in the cloud
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 云端机器学习
- en: ML has gone to the cloud and developers can now use it as a service. AWS has
    implemented ML services at different levels of abstraction. ML application services,
    for example, aim to offer out-of-the-box solutions for specific problem domains.
    **AWS Lex** is a very clear example of an ML application as a service, where people
    can implement chatbots with minimum development.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习已经走向云端，开发者现在可以将其作为一项服务使用。AWS在不同的抽象级别上实现了机器学习服务。例如，机器学习应用服务旨在为特定问题领域提供即插即用的解决方案。**AWS
    Lex**是一个机器学习作为服务的非常清晰的例子，人们可以用最少的开发来实现聊天机器人。
- en: '**AWS Rekognition** is another example, which aims to identify objects, people,
    text, scenes, and activities in images and videos. AWS provides many other ML
    application services, which will be covered in the next chapter of this book.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '**AWS Rekognition**是另一个例子，旨在识别图像和视频中的对象、人物、文本、场景和活动。AWS提供许多其他机器学习应用服务，将在本书的下一章中介绍。'
- en: Apart from application services, AWS also provides ML development platforms,
    such as **SageMaker**. Unlike out-of-the-box services such as AWS Lex and Rekognition,
    SageMaker is a development platform that will let you build, train, and deploy
    your own models with much more flexibility.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 除了应用服务之外，AWS还提供机器学习开发平台，例如**SageMaker**。与AWS Lex和Rekognition等现成服务不同，SageMaker是一个开发平台，它将允许你以更大的灵活性构建、训练和部署自己的模型。
- en: SageMaker speeds up the development and deployment process by automatically
    handling the necessary infrastructure for the training and inference pipelines
    of your models. Behind the scenes, SageMaker orchestrates other AWS services (such
    as EC2 instances, load balancers, auto-scaling, and so on) to create a scalable
    environment for ML projects. SageMaker is probably the most important service
    that you should master for the AWS Machine Learning Specialty exam, and it will
    be covered in detail in a separate section. For now, you should focus on understanding
    the different approaches that AWS uses to offer ML-related services.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker通过自动处理模型训练和推理管道所需的基础设施来加速开发和部署过程。在幕后，SageMaker协调其他AWS服务（如EC2实例、负载均衡器、自动扩展等）以创建一个可扩展的机器学习项目环境。SageMaker可能是你应该掌握的AWS机器学习专业考试中最重要的服务之一，它将在单独的部分中详细说明。目前，你应该专注于理解AWS提供与机器学习相关的服务所采用的不同方法。
- en: 'The third option that AWS offers for deploying ML models is the most generic
    and flexible one: you can deploy ML models by combining different AWS services
    and managing them individually. This essentially does what SageMaker does for
    you, building your applications from scratch. For example, you could use EC2 instances,
    load balancers, auto-scaling, and an API gateway to create an inference pipeline
    for a particular model. If you prefer, you can also use AWS serverless architecture
    to deploy your solution, for example, using **AWS** **Lambda functions**.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: AWS提供的用于部署机器学习模型的第三种选项是最通用和灵活的：你可以通过组合不同的AWS服务并单独管理它们来部署机器学习模型。这本质上就是SageMaker为你做的事情，从头开始构建你的应用程序。例如，你可以使用EC2实例、负载均衡器、自动扩展和API网关来为特定模型创建推理管道。如果你愿意，你也可以使用AWS无服务器架构来部署你的解决方案，例如，使用**AWS**
    **Lambda函数**。
- en: Summary
  id: totrans-178
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: You are now heading toward the end of this chapter, in which you have learned
    about several important topics regarding the foundations of ML. You started the
    chapter with a theoretical discussion about AI, ML, and DL, and how this entire
    field has grown over the past few years due to the advent of big data platforms,
    cloud providers, and AI applications.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在正走向本章的结尾，其中你学习了关于机器学习基础的一些重要主题。你以关于人工智能、机器学习和深度学习以及这一整个领域在过去几年中由于大数据平台、云提供商和人工智能应用的兴起而不断增长的理论讨论开始本章。
- en: You then moved on to the differences between supervised, unsupervised, and reinforcement
    learning, highlighting some use cases related to each of them. This is likely
    to be a topic in the AWS Machine Learning Specialty exam.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你转向监督学习、无监督学习和强化学习之间的区别，强调了与每个相关的一些用例。这可能是AWS机器学习专业考试中的一个主题。
- en: You learned that an ML model is built in many different stages and the algorithm
    itself is just one part of the modeling process. You also learned about the expected
    behaviors of a good model.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你了解到机器学习模型是在许多不同的阶段构建的，算法本身只是建模过程的一部分。你还了解了良好模型预期的行为。
- en: You did a deep dive into data splitting, where you learned about different approaches
    to train and validate models, and you became aware of the mythic battle between
    variance and bias. You completed the chapter by getting a sense of ML frameworks
    and services.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你深入研究了数据拆分，学习了训练和验证模型的不同方法，并意识到了方差和偏差之间的神话之战。你通过了解机器学习框架和服务完成了这一章节。
- en: Coming up next, you will learn about AWS application services for ML, such as
    Amazon Polly, Amazon Rekognition, Amazon Transcribe, and many other AI-related
    AWS services. But first, look at some sample questions to give you an idea of
    what you can expect in the exam.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，你将了解 AWS 机器学习应用服务，例如 Amazon Polly、Amazon Rekognition、Amazon Transcribe 以及许多其他与
    AI 相关的 AWS 服务。但在那之前，先看看一些样题，以了解你在考试中可以期待什么。
- en: Exam Readiness Drill – Chapter Review Questions
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 考试准备练习 – 章节复习题
- en: Apart from a solid understanding of key concepts, being able to think quickly
    under time pressure is a skill that will help you ace your certification exam.
    That is why working on these skills early on in your learning journey is key.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对关键概念有扎实的理解外，能够在时间压力下快速思考是一项帮助你通过认证考试的重要技能。这就是为什么在学习的早期阶段就练习这些技能是关键。
- en: Chapter review questions are designed to improve your test-taking skills progressively
    with each chapter you learn and review your understanding of key concepts in the
    chapter at the same time. You’ll find these at the end of each chapter.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 章节复习题旨在随着你学习并复习每个章节来逐步提高你的应试技巧，同时复习章节中的关键概念。你可以在每个章节的末尾找到这些题目。
- en: How To Access These Resources
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如何访问这些资源
- en: To learn how to access these resources, head over to the chapter titled [*Chapter
    11*](B21197_11.xhtml#_idTextAnchor1477), *Accessing the Online* *Practice Resources*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何访问这些资源，请参阅标题为 [*第 11 章*](B21197_11.xhtml#_idTextAnchor1477) 的章节，*访问在线练习资源*。
- en: 'To open the Chapter Review Questions for this chapter, perform the following
    steps:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 要打开本章的章节复习题，请执行以下步骤：
- en: Click the link – [https://packt.link/MLSC01E2_CH01](https://packt.link/MLSC01E2_CH01).
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击链接 – [https://packt.link/MLSC01E2_CH01](https://packt.link/MLSC01E2_CH01)。
- en: 'Alternatively, you can scan the following **QR code** (*Figure 1**.6*):'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，你可以扫描以下 **二维码** (*图 1.6*)：
- en: '![Figure 1.6 – QR code that opens Chapter Review Questions for logged-in users](img/B21197_01_06.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.6 – 为登录用户打开章节复习题的二维码](img/B21197_01_06.jpg)'
- en: Figure 1.6 – QR code that opens Chapter Review Questions for logged-in users
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.6 – 为登录用户打开章节复习题的二维码
- en: 'Once you log in, you’ll see a page similar to the one shown in *Figure 1**.7*:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 登录后，你将看到一个类似于 *图 1.7* 所示的页面：
- en: '![Figure 1.7 – Chapter Review Questions for Chapter 1](img/B21197_01_07.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 1.7 – 第一章章节复习题](img/B21197_01_07.jpg)'
- en: Figure 1.7 – Chapter Review Questions for Chapter 1
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.7 – 第一章章节复习题
- en: Once ready, start the following practice drills, re-attempting the quiz multiple
    times.
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备就绪后，开始以下练习，多次重新尝试测验。
- en: Exam Readiness Drill
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 考试准备练习
- en: For the first three attempts, don’t worry about the time limit.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 对于前三次尝试，不要担心时间限制。
- en: ATTEMPT 1
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 1
- en: The first time, aim for at least **40%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix your learning gaps.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次尝试，目标至少达到 **40%**。看看你答错的答案，并再次阅读章节中相关的部分，以修复你的学习差距。
- en: ATTEMPT 2
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 2
- en: The second time, aim for at least **60%**. Look at the answers you got wrong
    and read the relevant sections in the chapter again to fix any remaining learning
    gaps.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次尝试，目标至少达到 **60%**。看看你答错的答案，并再次阅读章节中相关的部分，以修复任何剩余的学习差距。
- en: ATTEMPT 3
  id: totrans-204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 尝试 3
- en: The third time, aim for at least **75%**. Once you score 75% or more, you start
    working on your timing.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 第三次尝试，目标至少达到 **75%**。一旦得分达到 75% 或以上，你就可以开始练习时间管理。
- en: Tip
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You may take more than **three** attempts to reach 75%. That’s okay. Just review
    the relevant sections in the chapter till you get there.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能需要超过 **三次** 尝试才能达到 75%。没关系。只需复习章节中的相关部分，直到达到目标。
- en: Working On Timing
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习时间管理
- en: 'Target: Your aim is to keep the score the same while trying to answer these
    questions as quickly as possible. Here’s an example of how your next attempts
    should look like:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 目标：你的目标是保持分数不变，同时尽可能快地回答这些问题。以下是一个你接下来尝试应该看起来像的例子：
- en: '| **Attempt** | **Score** | **Time Taken** |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| **尝试** | **得分** | **用时** |'
- en: '| Attempt 5 | 77% | 21 mins 30 seconds |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 5 | 77% | 21 分 30 秒 |'
- en: '| Attempt 6 | 78% | 18 mins 34 seconds |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 6 | 78% | 18 分 34 秒 |'
- en: '| Attempt 7 | 76% | 14 mins 44 seconds |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 尝试 7 | 76% | 14 分 44 秒 |'
- en: Table 1.3 – Sample timing practice drills on the online platform
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1.3 – 在线平台上的样本时间练习
- en: Note
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The time limits shown in the above table are just examples. Set your own time
    limits with each attempt based on the time limit of the quiz on the website.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 上表中显示的时间限制只是示例。根据网站上的测验时间限制，为每次尝试设定自己的时间限制。
- en: With each new attempt, your score should stay above **75%** while your “time
    taken” to complete should “decrease”. Repeat as many attempts as you want till
    you feel confident dealing with the time pressure.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 每次新的尝试，你的得分应保持在**75%**以上，同时完成所需的时间“应减少”。你可以重复尽可能多的尝试，直到你觉得自己能够自信地应对时间压力。
