- en: '*Chapter 6*: Exploring Multi-Fidelity Optimization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第6章*: 探索多保真优化'
- en: '**Multi-Fidelity Optimization** (**MFO**) is the fourth of four groups of hyperparameter
    tuning methods. The main characteristic of this group is that all methods belonging
    to this group utilize the cheap approximation of the whole hyperparameter tuning
    pipeline so we can have similar performance results with a much lower computational
    cost and faster experiment time. This group is suitable when you have a very large
    model or a very large number of samples, for example, when you are developing
    a neural-network-based model.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**多保真优化**（**MFO**）是四组超参数调整方法中的第四组。这一组的主要特点是，这一组中的所有方法都利用了整个超参数调整管道的廉价近似，因此我们可以以更低的计算成本和更快的实验时间获得相似的性能结果。当您有一个非常大的模型或一个非常大的样本数量时，例如，当您正在开发基于神经网络的模型时，这一组是合适的。'
- en: In this chapter, we will discuss several methods in the MFO group, including
    coarse-to-fine search, successive halving, hyper band, and **Bayesian Optimization
    and Hyperband** (**BOHB**). As in [*Chapter 5*](B18753_05_ePub.xhtml#_idTextAnchor047)*,
    Exploring Heuristic Search* we will discuss the definition of each method, the
    differences between them, how they work, and the pros and cons of each.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论MFO组中的几种方法，包括从粗到细的搜索、连续减半、超带宽和**贝叶斯优化与超带宽**（**BOHB**）。与[*第5章*](B18753_05_ePub.xhtml#_idTextAnchor047)*探索启发式搜索*一样，我们将讨论每种方法的定义、它们之间的差异、它们的工作原理以及每种方法的优缺点。
- en: 'By the end of this chapter, you will be confident in explaining MFO and its
    variations, and also how they work at a high level and in a technical way. You
    will also be able to tell the differences between them, along with the pros and
    cons of each. You will also experience the crucial benefit of understanding each
    of the methods in practice: being able to configure the method to match your own
    problem and knowing what to do when there are errors or unexpected outputs from
    the method.'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您将能够自信地解释MFO及其变体，以及它们在高级和技术上是如何工作的。您还将能够区分它们之间的差异，以及每种方法的优缺点。您还将体验到理解每种方法在实际操作中的关键好处：能够配置方法以匹配您自己的问题，并在方法出现错误或意外输出时知道该怎么做。
- en: 'In this chapter, we’ll be covering the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主要主题：
- en: Introducing MFO
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍MFO
- en: Understanding coarse-to-fine search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解从粗到细的搜索
- en: Understanding successive halving
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解连续减半
- en: Understanding hyper band
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解超带宽
- en: Understanding BOHB
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解BOHB
- en: Introducing MFO
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍MFO
- en: MFO is a group of hyperparameter tuning methods that work by creating a cheap
    approximation of the whole hyperparameter tuning pipeline so that we can get similar
    performance results with much *lower computational cost* and *faster experiment
    time*. There are many ways to create a cheap approximation. For example, we can
    work only on the subsets of the full data in the first several steps rather than
    directly working on the full data, or we can also try to use fewer epochs when
    training a neural-network-based model before training our model with full epochs.
    In other words, MFO methods work by *combining cheap low-fidelity and expensive
    high-fidelity* evaluations, where usually the proportion of cheaper evaluations
    is much larger than the more expensive evaluations so that we can achieve lower
    computational cost and thus faster experiment time. However, MFO methods can also
    be categorized as part of the **informed search** category since they utilize
    knowledge from previous iterations to have a (hopefully) better search space in
    future.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: MFO是一组超参数调整方法，通过创建整个超参数调整管道的廉价近似，我们可以以更低的计算成本和更快的实验时间获得相似的性能结果。有许多方法可以创建廉价近似。例如，我们可以在前几个步骤中仅对完整数据的子集进行操作，而不是直接对完整数据进行操作，或者我们也可以在用完整周期训练模型之前尝试使用较少的周期来训练基于神经网络的模型。换句话说，MFO方法通过*结合廉价低保真和昂贵高保真*评估来工作，其中通常廉价评估的比例远大于昂贵评估，这样我们就可以实现更低的计算成本，从而更快地完成实验。然而，MFO方法也可以归类为**信息搜索**类别的一部分，因为它们利用先前迭代的知识来获得（希望）更好的搜索空间。
- en: All of the methods that we have learned in the previous chapters can be categorized
    as **black-box optimization** methods. All black-box optimization methods try
    to perform hyperparameter tuning without utilizing any information from what is
    happening inside the ML model or the data that is used by the model. A black-box
    optimizer will only focus on searching the best set of hyperparameters from the
    defined hyperparameter space and *treat other factors as a black box* (see *Figure
    6.1*). This characteristic has its own good and bad implications. It enables us
    to utilize a black-box optimizer, which is more flexible for various types of
    models or data, but it also costs us more since we do not consider other factors
    that may speed up the process.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中学到的所有方法都可以归类为**黑盒优化**方法。所有黑盒优化方法都试图在不利用ML模型内部发生的情况或模型使用的数据的任何信息的情况下进行超参数调整。黑盒优化器将只关注从定义的超参数空间中搜索最佳的超参数集，并将*其他因素视为黑盒*（见*图6.1*）。这种特性有其自身的优点和缺点。它使我们能够利用黑盒优化器，这对于各种类型的模型或数据来说更加灵活，但它也让我们付出了代价，因为我们没有考虑可能加快进程的其他因素。
- en: '![Figure 6.1 – Illustration of black-box optimizer'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.1 – 黑盒优化器的示意图'
- en: '](img/B18753_06_001.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_06_001.jpg)'
- en: Figure 6.1 – Illustration of black-box optimizer
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 – 黑盒优化器的示意图
- en: The expense of black-box optimization methods means we can’t utilize them when
    we are working with a *very large model* or *big data* that requires a very long
    time for just one training iteration. That’s where the MFO group of hyperparameter
    tuning methods comes into the picture! By considering other factors that are treated
    as black-box by black-box optimizers, we can have a faster process while sacrificing
    a bit of the generality that black-box optimizers have.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒优化方法的成本意味着当我们处理需要非常长的训练迭代时间才能完成的一个非常大的模型或大数据时，我们无法利用它们。这就是超参数调整方法的MFO组出现的地方！通过考虑黑盒优化器视为黑盒的其他因素，我们可以在牺牲一点黑盒优化器所具有的通用性的情况下，拥有一个更快的进程。
- en: Generality
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 通用性
- en: '**Generality** means the model is able to perform on many unseen cases.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '**通用性**意味着模型能够在许多未见过的案例上执行。'
- en: Furthermore, most of the methods categorized in this group can *utilize parallel
    computational resources* very nicely, which can further boost the speed of the
    hyperparameter tuning process. However, the benefit of faster processes offered
    by MFO methods comes with a cost. We may have *worse performing tuning results*
    since there is a chance we have excluded a better subspace during the cheap low-fidelity
    evaluations step. However, the *speedup is arguably more significant* than the
    estimation error, especially when we are working with a very large model and/or
    big data.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，本组分类的大多数方法都可以很好地利用并行计算资源，这可以进一步加快超参数调整过程的速度。然而，MFO方法提供的更快过程的好处是有代价的。我们可能会有*更差的调整结果*，因为我们有可能在低保真度评估步骤中排除了更好的子空间。然而，*速度提升可以说是更重要的*，特别是当我们处理一个非常大的模型和/或大数据时。
- en: Important Note
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The MFO group of hyperparameter tuning methods is *not* a completely different
    group compared to black-box optimization methods, including exhaustive search,
    Bayesian optimization, and heuristic search. In fact, we can also apply a similar
    procedure done in a multi-fidelity optimization method to a black-box optimizer.
    In other words, *we can combine black-box-and multi-fidelity* models so we can
    get the best of both worlds.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数调整方法的MFO组与包括穷举搜索、贝叶斯优化和启发式搜索在内的黑盒优化方法相比，并不是一个完全不同的组。事实上，我们也可以将多保真优化方法中使用的类似程序应用到黑盒优化器中。换句话说，*我们可以结合黑盒和多保真模型，从而获得两者的最佳效果*。
- en: For example, we can perform hyperparameter tuning with one of the **Bayesian
    Optimization** (**BO**) methods (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036),
    *Exploring Bayesian Optimization*) and also apply the successive halving method
    (see the *Understanding successive halving* section) on top of it. This way, we
    will ensure that we only perform BO on important subspace, rather than letting
    BO explore the whole hyperparameter space by itself. By doing this, we can have
    a faster experiment time with lower computational cost.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以使用**贝叶斯优化**（**BO**）方法之一（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)，*探索贝叶斯优化*）并在其上应用连续减半法（见*理解连续减半*部分）。这样，我们将确保我们只在重要的子空间上执行BO，而不是让BO自行探索整个超参数空间。通过这样做，我们可以拥有更快的实验时间，同时降低计算成本。
- en: Now that you are aware of what MFO is, how it differs from black-box optimization
    methods, and how it works at a high level, we will dive deeper into several MFO
    methods in the following sections.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了 MFO 是什么，它与黑盒优化方法有何不同，以及它在高层次上是如何工作的，我们将在以下几节中深入探讨几个 MFO 方法。
- en: Understanding coarse-to-fine search
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解粗细搜索
- en: '**Coarse-to-Fine Search** (**CFS**) is a combination of grid and random search
    hyperparameter tuning methods (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*,
    Exploring Exhaustive Search*). Unlike grid and random search, which are categorized
    in the **uninformed search** group of methods, CFS utilizes knowledge from previous
    iterations to have a (hopefully) better search space in the future. In other words,
    CFS is a *combination of sequential and parallel* hyperparameter tuning methods.
    It is indeed a very simple method since it is basically a *combination of two
    other simple methods: grid and random search*.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '**粗细搜索**（**CFS**）是网格搜索和随机搜索超参数调整方法的组合（参见[*第 3 章*](B18753_03_ePub.xhtml#_idTextAnchor031)*，探索穷举搜索）。与被归类在**无信息搜索**方法组的网格搜索和随机搜索不同，CFS
    利用先前迭代的知识来拥有（希望）更好的搜索空间。换句话说，CFS 是一种 *结合了顺序和并行* 超参数调整方法。实际上，这是一个非常简单的方法，因为它基本上是
    *两种其他简单方法的组合：网格搜索和随机搜索*。'
- en: CFS can be effectively utilized as a hyperparameter tuning method when you are
    working with a medium-sized model, for example, a shallow neural network (other
    types of models can also work) and a moderate amount of training data.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: CFS 可以有效地作为超参数调整方法，当你使用中等规模模型时，例如浅层神经网络（其他类型的模型也可以工作）以及适量的训练数据。
- en: The main idea of CFS is just to start with a *coarse* random search from the
    whole hyperparameter space, then gradually *refine* the search in more detail,
    either using random or grid search. The following figure summarizes how CFS works
    as a hyperparameter tuning method.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: CFS 的主要思想只是从整个超参数空间开始进行 *粗略* 随机搜索，然后逐渐 *细化* 搜索的细节，无论是使用随机搜索还是网格搜索。以下图总结了 CFS
    作为超参数调整方法的工作方式。
- en: '![Figure 6.2 – Illustration of CFS'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.2 – CFS 的说明'
- en: '](img/B18753_06_002.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_06_002.jpg]'
- en: Figure 6.2 – Illustration of CFS
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 – CFS 的说明
- en: 'As illustrated in *Figure 6.2*, CFS starts by performing a random search in
    the whole pre-defined hyperparameter space. Then, it looks for a promising subspace
    based on the first coarse random search evaluation results. The definition of
    a promising subspace may vary and can be adjusted to your own preference. The
    following list shows several definitions of a promising subspace that you can
    adopt:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如 *图 6.2* 所示，CFS 首先在整个预定义的超参数空间中进行随机搜索。然后，它根据第一次粗略随机搜索评估结果寻找一个有希望的子空间。有希望的子空间的定义可能不同，可以根据您的偏好进行调整。以下列表显示了您可以采用的几个有希望的子空间定义：
- en: Get only the *top N percentiles* of the best set of hyperparameters based on
    the evaluation performed in the previous trial.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅根据前一次试验中进行的评估，获取最佳超参数集的 *最高 N 分位数*。
- en: Put a *hard threshold* to filter out the bad set of hyperparameters from the
    previous trial.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对前一次试验中的不良超参数集设置一个 *硬阈值* 以过滤掉。
- en: Conduct a *univariate analysis* to get the best range of values for each hyperparameter.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行 *单变量分析* 以获取每个超参数的最佳值范围。
- en: No matter what definition you are using to define the promising subspace, we
    will always get a list of values for each hyperparameter. Then, we can create
    a new hyperparameter space based on the minimum and maximum values in each list
    of hyperparameter values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您使用什么定义来定义有希望的子空间，我们总会为每个超参数得到一个值列表。然后，我们可以根据每个超参数值列表中的最小值和最大值创建一个新的超参数空间。
- en: After getting the promising subspace, we can continue the process by performing
    a grid search or another random search in the smaller area. Note that you can
    also put a condition on when to keep using random search and when to start using
    grid search. Again, it is up to you to choose the appropriate condition. However,
    it is better to perform a random search than a grid search, so that we can have
    *more evaluations based on the cheap low-fidelity approach* compared to the expensive
    high-fidelity approach. We keep repeating this procedure until we reach the stopping
    criterion.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在得到有希望的子空间后，我们可以在较小的区域内通过执行网格搜索或另一个随机搜索来继续该过程。请注意，您还可以设置条件，以确定何时继续使用随机搜索以及何时开始使用网格搜索。再次强调，选择适当的条件取决于您。然而，进行随机搜索比进行网格搜索更好，这样我们就可以与昂贵的保真度高的方法相比，有更多的基于低成本低保真度方法的评估。我们重复执行此过程，直到达到停止标准。
- en: 'The following procedure explains in more detail how CFS works as a hyperparameter
    tuning method:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 以下过程更详细地解释了CFS作为超参数调整方法的工作原理：
- en: Split the original full data into a training set and a test set. (See [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,* *Evaluating Machine Learning Models*.)
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据集分为训练集和测试集。（参见[*第 1 章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，*评估机器学习模型*。）
- en: Define the hyperparameter space, `H`, with the accompanied distributions, the
    objective function, `f`, based on the training set, and the stopping criterion.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集和停止标准，定义超参数空间`H`及其伴随的分布，目标函数`f`。
- en: Define the grid size for creating the grid search hyperparameter space, `grid_size`,
    and the random search number of iterations, `random_iters`.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义创建网格搜索超参数空间`grid_size`的网格大小和随机搜索迭代次数`random_iters`。
- en: Define the criterion of a promising subspace by utilizing the objective function,
    `f`.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过利用目标函数`f`定义有希望的子空间的标准。
- en: Define the criterion of when to start using grid search.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义何时开始使用网格搜索的标准。
- en: Set the initial best set of hyperparameters, `best_set`, with the value `None`.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将初始最佳超参数集`best_set`设置为`None`。
- en: Perform a random search on the current hyperparameter space, `H`, for `random_iters`
    times.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在当前的超参数空间`H`上，进行`random_iters`次随机搜索。
- en: 'Select a promising subspace based on the criterion defined in *step 4*:'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据在*步骤 4*中定义的标准选择有希望的子空间：
- en: If the current best-performing set of hyperparameters is worse than the previous
    `best_set`, add `best_set` to the promising subspace.
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前表现最佳的超参数集比之前的`best_set`差，将`best_set`添加到有希望的子空间中。
- en: If the current best-performing set of hyperparameters is better than the previous
    `best_set`, update `best_set`.
  id: totrans-47
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前表现最佳的超参数集比之前的`best_set`好，更新`best_set`。
- en: 'If the criterion in `step 5` is met, do the following:'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果满足`步骤 5`中的标准，请执行以下操作：
- en: Update the current hyperparameter space, `H`, with the promising subspace selected
    in *step 8*, using unique `grid_size` values for each of the hyperparameters.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用每个超参数的独特`grid_size`值，将当前超参数空间`H`更新为在*步骤 8*中选定的有希望的子空间。
- en: Perform a grid search on the updated hyperparameter space, `H`.
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新的超参数空间`H`上执行网格搜索。
- en: 'If the criterion in *step 5* is not met, do the following:'
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果未满足*步骤 5*中的标准，请执行以下操作：
- en: Update the current hyperparameter space, `H`, with the promising subspace selected
    in *step 8* using the minimum and maximum values for each hyperparameter.
  id: totrans-52
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用每个超参数的最小值和最大值，将当前超参数空间`H`更新为在*步骤 8*中选定的有希望的子空间。
- en: Perform a random search on the updated hyperparameter space, `H`, for `random_iters`
    times.
  id: totrans-53
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在更新的超参数空间`H`上，进行`random_iters`次随机搜索。
- en: Repeat *steps 8 – 10* until the stopping criterion is met.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 8 – 10*，直到满足停止标准。
- en: Train on the full training set using the best hyperparameter combination.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最佳超参数组合在完整训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练的模型。
- en: In CFS, the multi-fidelity characteristic is based neither on the amount of
    data nor the number of training epochs, but on the *granularity of the search*
    performed in the search space during each trial. In other words, we will *keep
    using all of the data* and *all of the training epochs* with a *refined hyperparameter
    space* in each trial.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在CFS中，多保真特性既不是基于数据量也不是基于训练轮数，而是基于在每个试验中搜索空间中执行的*搜索粒度*。换句话说，我们将*继续使用所有数据*和*所有训练轮数*，在每个试验中使用*精细的超参数空间*。
- en: 'Let’s see how CFS works as a hyperparameter tuning method on dummy data generated
    by the `make_classification` to create dummy classification data with several
    customizable configurations. In this example, we use the following configurations
    to generate the dummy data:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看CFS作为超参数调整方法在由`make_classification`生成的虚拟数据上的工作情况，以创建具有几个可定制配置的虚拟分类数据。在这个例子中，我们使用以下配置来生成虚拟数据：
- en: '*Number of classes*. We set the number of target classes in the data to 2 by
    setting `n_classes=2`.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*类别数量*。我们将数据中的目标类别数量设置为2，通过设置`n_classes=2`。'
- en: '*Number of samples*. We set the number of samples to 500 by setting `n_samples=500`.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*样本数量*。我们将样本数量设置为500，通过设置`n_samples=500`。'
- en: '*Number of features*. We set the number of features or the number of dependent
    variables in the data to 25 by setting `n_features=25`.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征数量*。我们将特征数量或数据中的依赖变量数量设置为25，通过设置`n_features=25`。'
- en: '*Number of informative features*. We set the number of features that have high
    importance to distinguish between all of the target classes to 18 by setting `n_informative=18`.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*信息特征数量*。我们将具有高重要性以区分所有目标类的特征数量设置为18，通过设置`n_informative=18`。'
- en: '*Number of redundant features*. We set the number of features that are basically
    just a weighted sum from other features to 5 by setting `n_redundant=5`.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*冗余特征数量*。我们将基本上只是其他特征加权求和的特征数量设置为5，通过设置`n_redundant=5`。'
- en: '*Random seed*. To ensure reproducibility, we set `random_state=0`.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*随机种子*。为了确保可重复性，我们设置`random_state=0`。'
- en: 'We utilize a `12`, which acts as the stopping criterion. We set the number
    of iterations for each random search trial to `20`. Finally, we utilize the *top
    N percentiles* scheme to define the promising subspace in each trial, with `N=50`.
    We define the hyperparameter space as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个`12`作为停止标准。我们将每个随机搜索试验的迭代次数设置为`20`。最后，我们利用*顶部N百分位数*方案来定义每个试验中的具有希望的子空间，其中`N=50`。我们定义超参数空间如下：
- en: 'Number of neurons in the hidden layer: `hidden_layer_sizes=range(1,51)`'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层中的神经元数量：`hidden_layer_sizes=range(1,51)`
- en: 'Initial learning rate: `learning_rate_init=np.linspace(0.001,0.1,50)`'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始学习率：`learning_rate_init=np.linspace(0.001,0.1,50)`
- en: The following figure shows how CFS works in each iteration or trial. The *purple
    dots* refer to hyperparameter values tested in the current trial, while the *red
    rectangles* refer to the promising subspace to be searched in the next trial.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了CFS在每个迭代或试验中的工作方式。*紫色点*表示当前试验中测试的超参数值，而*红色矩形*表示下一个试验中要搜索的具有希望的子空间。
- en: '![Figure 6.3 – Illustration of the CFS process'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.3 – CFS过程的说明'
- en: '](img/B18753_06_003.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_003.jpg)'
- en: Figure 6.3 – Illustration of the CFS process
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 – CFS过程的说明
- en: In *Figure 6.3*, we can see clearly how CFS starts by working at the full hyperparameter
    space and then gradually searches in the smaller subspaces. It is also worth noting
    that although we only use random search in this example, we can see that CFS still
    increases its fidelity over the number of trials until we get a final set of hyperparameters
    in the last trial. We can also see the performance of each trial in the following
    figure.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.3*中，我们可以清楚地看到CFS是如何从整个超参数空间开始工作，然后逐渐在更小的子空间中搜索的。也值得注意，尽管在这个例子中我们只使用了随机搜索，但我们仍然可以看到CFS在试验次数增加时仍然提高了其保真度，直到在最后一个试验中得到最终的超参数集。我们还可以在以下图中看到每个试验的性能。
- en: '![Figure 6.4 – Convergence plot'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.4 – 收敛图'
- en: '](img/B18753_06_004.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_004.jpg)'
- en: Figure 6.4 – Convergence plot
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 – 收敛图
- en: The blue line in *Figure 6.4* reflects the average cross-validation scores from
    all tested hyperparameters (see the purple dots in *Figure 6.3*) at each trial.
    The red line reflects the cross-validation score of the best-performing set of
    hyperparameters at each trial. We can see that the red line has a nice *non-decreasing
    monotonic* characteristic. This happens because we always add back the best set
    of hyperparameters from all previous trials to the promising subspace definition,
    as defined in *step 8* in the previous procedure. We will learn how to implement
    CFS with scikit-learn in *Chapter 7, Hyperparameter Tuning via Scikit*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '*图6.4*中的蓝色线反映了所有测试超参数在每个试验中的平均交叉验证分数（参见*图6.3*中的紫色点）。红色线反映了每个试验中表现最佳的超参数集的交叉验证分数。我们可以看到红色线具有很好的*非递减单调*特性。这是因为我们总是将所有先前试验中最佳的超参数集添加回具有希望的子空间定义中，正如在先前程序中的*步骤8*所定义的。我们将在*第7章，通过Scikit进行超参数调整*中学习如何实现CFS。'
- en: The following table summarizes the pros and cons of utilizing CFS as a hyperparameter
    tuning method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了利用CFS作为超参数调整方法的优缺点。
- en: '![Figure 6.5 – Pros and cons of CFS'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.5 – CFS的优缺点'
- en: '](img/B18753_06_005.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_005.jpg)'
- en: Figure 6.5 – Pros and cons of CFS
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 – CFS的优缺点
- en: In this section, we have discussed CFS, looking at what it is, how it works,
    and the pros and cons. We will discuss another interesting MFO method in the next
    section.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了CFS，探讨了它是什么，如何工作，以及优缺点。我们将在下一节讨论另一个有趣的MFO方法。
- en: Understanding successive halving
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解连续减半
- en: '**Successive Halving** (**SH**) is an MFO method that is not only able to focus
    on a more promising hyperparameter subspace but can also *allocate computational
    cost wisely* in each trial. Unlike CFS, which utilizes all of the data in each
    trial, SH can utilize less data for a not-too-promising subspace while utilizing
    more data for a more promising subspace. It can be said that SH is a variant of
    CFS with a much clearer algorithm definition and is wiser in spending the computational
    cost. The most effective way to utilize SH as a hyperparameter tuning method is
    when you are working with a large model (for example, a deep neural network) and/or
    working with a large amount of data.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**连续减半**（**SH**）是一种MFO方法，它不仅能够专注于更有希望的超参数子空间，而且还能在每个试验中**明智地分配计算成本**。与CFS不同，CFS在每个试验中利用所有数据，而SH可以为不太有希望的子空间利用较少的数据，而为更有希望的子空间利用更多的数据。可以说，SH是CFS的一个变种，具有更清晰的算法定义，并且在计算成本的分配上更为明智。将SH作为超参数调整方法的最有效方式是在处理大型模型（例如，深度神经网络）和/或处理大量数据时。'
- en: Similar to CFS, SH also *utilizes grid search or random search* to search for
    the best set of hyperparameters. At the first iteration, SH will perform a grid
    or random search on the whole hyperparameter space with a small amount of **budget**
    or resources, and then it will gradually increase the budget while also removing
    the worst half of the hyperparameters candidates at each iteration. In other words,
    SH performs hyperparameter tuning with a lower budget on a bigger search space
    and a higher budget on a more promising smaller subspace. SH can also be seen
    as a **tournament** between hyperparameter candidates, where only the best candidate
    will survive at the end of the trials.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 与CFS类似，SH也**使用网格搜索或随机搜索**来搜索最佳的超参数集。在第一次迭代中，SH将在整个超参数空间上执行带有少量**预算**或资源的网格或随机搜索，然后它将逐渐增加预算，同时在每次迭代中移除最差的半数超参数候选者。换句话说，SH在更大的搜索空间上以较低的预算进行超参数调整，在更有希望的较小子空间上以较高的预算进行超参数调整。SH也可以被视为超参数候选者之间的**锦标赛**，只有最佳候选者才能在试验结束时幸存。
- en: Budget Definition in SH
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: SH中的预算定义
- en: In a default hyperparameter tuning setup, the budget is defined as the number
    of samples in the data. However, it is also possible to define the budget in other
    ways. For example, we can also define the budget as the maximum training time,
    number of iterations during XGBoost training steps, number of estimators in a
    random forest, or number of epochs when training a neural network model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在默认的超参数调整设置中，预算定义为数据中的样本数量。然而，也可以以其他方式定义预算。例如，我们也可以将预算定义为最大训练时间、XGBoost训练步骤中的迭代次数、随机森林中的估计器数量，或者训练神经网络模型时的epoch数量。
- en: To have a better understanding of SH, let’s look at the following example before
    we discuss how it works in a formal procedure. We utilize the same model and the
    same hyperparameter space definition used in the example in the *Understanding
    CFS* section. We also utilize a similar procedure to generate a dummy classification
    dataset a hundred times bigger in size, meaning we have `50000` samples instead
    of only `500` samples as in the CFS example.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何在正式程序中工作之前，为了更好地理解SH，让我们先看看以下示例。我们使用与*理解CFS*部分中使用的相同模型和相同的超参数空间定义。我们还使用类似的程序生成一个大小大一百倍的虚拟分类数据集，这意味着我们有`50000`个样本，而不是像CFS示例中那样只有`500`个样本。
- en: In this example, we utilize random search instead of grid search to sample the
    hyperparameter candidates in each trial. The following figure shows the accuracy
    scores of hyperparameter candidates over trials. Each line refers to the trend
    of each hyperparameter candidate’s objective function score, which in this case
    is the *seven-fold cross-validation accuracy score*, over the number of trials.
    The final objective function score, based on the best set of hyperparameters selected
    from the SH tuning process, is `0.984`. We will learn how to implement SH in [*Chapter
    7*](B18753_07_ePub.xhtml#_idTextAnchor062)*, Hyperparameter Tuning via Scikit*
    and [*Chapter 9*](B18753_09_ePub.xhtml#_idTextAnchor082)*, Hyperparameter Tuning
    via Optuna*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们使用随机搜索而不是网格搜索来在每个试验中采样超参数候选者。以下图显示了超参数候选者在试验中的准确度分数。每条线代表每个超参数候选者的目标函数分数的趋势，在这种情况下是*七折交叉验证准确度分数*，相对于试验次数。基于从SH调整过程中选择的最佳超参数集，最终的目标函数分数是`0.984`。我们将在[*第7章*](B18753_07_ePub.xhtml#_idTextAnchor062)*，通过Scikit进行超参数调整*和[*第9章*](B18753_09_ePub.xhtml#_idTextAnchor082)*，通过Optuna进行超参数调整*中学习如何实现SH。
- en: '![Figure 6.6 – Illustration of the SH process'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6 – SH过程的示意图'
- en: '](img/B18753_06_006.jpg)'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_006.jpg)'
- en: Figure 6.6 – Illustration of the SH process
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 – SH过程的示意图
- en: In *Figure 6.6*, we can clearly see how SH takes only the top hyperparameter
    candidates (see the orange ovals) from each trial for further evaluation in the
    next trial. In the first iteration, a random search is performed `240` times with
    only `600` out of `50000` of the samples available in the data. This means we
    have `240` hyperparameter candidates, `n_candidates`, in the first iteration.
    Out of those hyperparameter candidates, SF takes only the top `80` candidates
    to be evaluated with a larger number of samples in the second iteration, which
    is `1800` samples. For the third iteration, SF again takes only the top `27` candidates
    and evaluates them on `5400` samples.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图6.6*中，我们可以清楚地看到SH如何只从每个试验中选取顶部的超参数候选者（见橙色椭圆）以在下一个试验中进行进一步评估。在第一次迭代中，进行了`240`次随机搜索，只有`50000`个样本中的`600`个可用。这意味着我们在第一次迭代中有`240`个超参数候选者，`n_candidates`。在这些超参数候选者中，SF只选取了前`80`个候选者，在第二次迭代中使用更多的样本进行评估，即`1800`个样本。对于第三次迭代，SF再次只选取了前`27`个候选者，并在`5400`个样本上评估它们。
- en: This process continues until we *can’t use a larger number of samples* since
    it will be greater than the maximum resources, `max_resources`, defined in the
    first place. In this example, the maximum resources are defined as the number
    of samples that we have in the data. However, it can also be defined as the total
    number of epochs or training steps based on the definition of the budget or resources.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程会一直持续到我们*不能使用更多的样本数量*，因为这将超过最初定义的最大资源，`max_resources`。在这个例子中，最大资源定义为数据中的样本数量。然而，它也可以根据预算或资源的定义来定义为总轮数或训练步骤。
- en: In this example, we stopped at the fourth iteration, where we need to evaluate
    `3` candidates based on `48600` samples. The final hyperparameter candidate chosen
    is the one that has the highest seven-fold cross-validation accuracy score evaluated
    on those `48600` samples.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们在第4次迭代时停止，需要基于`48600`个样本评估`3`个候选者。最终选择的超参数候选者是那些在`48600`个样本上评估出的七折交叉验证准确度分数最高的一个。
- en: As you will notice, the gradual increment of the number of samples in each trial
    and the gradual decrement of the number of candidates in each trial follows the
    same multiplier factor, `factor`, which is `3` in this example. That’s why we
    have to stop at the fourth iteration, since if we continue to the fifth iteration,
    we would need `48600*3=145800` samples, while we only have `50000` samples in
    the data. Note that we have to set the value of the multiplier factor ourselves
    before running the SH tuning process. In other words, this multiplier factor is
    the hyperparameter for SH.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所注意到的，每次试验中样本数量的逐渐增加和候选人数量的逐渐减少遵循相同的乘数因子，即`factor`，在这个例子中是`3`。这就是为什么我们必须在第4次迭代时停止，因为如果我们继续到第5次迭代，我们需要`48600*3=145800`个样本，而我们只有`50000`个样本在数据中。请注意，在运行SH调整过程之前，我们必须自己设置乘数因子的值。换句话说，这个乘数因子是SH的超参数。
- en: Multiplier Factor in SH
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: SH中的乘数因子
- en: The halving term in SH refers to setting the multiplier factor value to two.
    In other words, only the best half of the hyperparameter candidates in each trial
    are passed to the next trial. However, we can also change this with another value.
    For example, when we set the multiplier factor as three, it means we take only
    the top one-third of hyperparameter candidates in each trial. In practice, setting
    the multiplier factor as three usually works better than setting it as two.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: SH 中的 halving term 指的是将乘数因子值设为二。换句话说，每个试验中只有最好的半数超参数候选者会传递到下一个试验。然而，我们也可以用另一个值来改变这一点。例如，当我们把乘数因子设为三时，这意味着我们只取每个试验中排名前三分之一的超参数候选者。在实践中，将乘数因子设为三通常比设为二效果更好。
- en: Besides the multiplier factor and maximum resources, SH also has other hyperparameters,
    such as the minimum number of resources to be used at the first iteration, `min_resources`,
    and the initial number of candidates to be evaluated at the first iteration, `n_candidates`.
    If grid search is utilized in the SH tuning process, `n_candidates` will equal
    the number of all combinations of hyperparameters in the search space. If a random
    search is utilized, then we have to set the value of `n_candidates` ourselves.
    In our example, where random search is utilized, we set `min_resources=600` and
    `n_candidates=240`.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 除了乘数因子和最大资源外，SH 还具有其他超参数，例如第一次迭代中使用的最小资源量，`min_resources`，以及第一次迭代中要评估的候选者初始数量，`n_candidates`。如果在
    SH 调整过程中使用了网格搜索，`n_candidates` 将等于搜索空间中超参数所有组合的数量。如果使用了随机搜索，那么我们必须自己设置 `n_candidates`
    的值。在我们的例子中，由于使用了随机搜索，我们设置了 `min_resources=600` 和 `n_candidates=240`。
- en: 'While setting `factor` to be equal to three is the common practice, this is
    not the case for `min_resources` and `n_candidates`. There are many factors to
    be considered before choosing the right values for both the `min_resources` and
    `n_candidates` hyperparameters. In other words, there is a trade-off between them,
    as explained here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `factor` 设为三是一种常见做法，但 `min_resources` 和 `n_candidates` 并非如此。在选择 `min_resources`
    和 `n_candidates` 超参数的正确值之前，需要考虑许多因素。换句话说，它们之间存在权衡，如下所述：
- en: Choosing a *bigger* value for `n_candidates` is useful when the bad and good
    hyperparameters can be easily distinguished with a smaller number of samples (a
    smaller value for `min_resources`).
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当好坏超参数可以通过较少的样本（较小的 `min_resources` 值）轻松区分时，为 `n_candidates` 选择一个较大的值是有用的。
- en: Choosing a *smaller* value for `n_candidates` is useful when we need a larger
    number of samples (a larger value for `min_resources`) to distinguish between
    the bad and good hyperparameters.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们需要更多的样本（更大的 `min_resources` 值）来区分好坏超参数时，为 `n_candidates` 选择一个较小的值是有用的。
- en: Another hyperparameter that SH has is the minimum early stopping rate, `min_early_stopping`.
    This integer-type hyperparameter has a default value of zero. If it is set to
    more than zero, it will reduce the number of iterations while increasing the number
    of resources to be used at the first iteration. In our previous example, we set
    `min_early_stopping=0`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: SH 还有一个超参数，即最小早期停止率，`min_early_stopping`。这个整型超参数的默认值为零。如果将其设置为大于零的值，它将减少迭代次数，同时增加第一次迭代中使用的资源数量。在我们之前的例子中，我们设置了
    `min_early_stopping=0`。
- en: 'To summarize, SH as a hyperparameter tuning method works as follows:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，SH 作为一种超参数调整方法，其工作原理如下：
- en: Split the original dataset into train and test sets.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始数据集分为训练集和测试集。
- en: Define the hyperparameter space, `H`, with the accompanied distributions, and
    the objective function, `f`, based on the training set.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义基于训练集的超参数空间，`H`，以及伴随的分布和目标函数，`f`。
- en: Define the budget/resources. Usually, this is defined as the number of samples
    or training epochs.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义预算/资源。通常，这被定义为样本数量或训练轮数。
- en: Define the maximum amount of resources, `max_resources`. Usually, this is defined
    as the total number of samples in data or the total number of epochs.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义最大资源量，`max_resources`。通常，这被定义为数据中的总样本数或总轮数。
- en: Define the multiplier factor, `factor`, the minimum amount of resources to be
    used at the first iteration, `min_resources`, and the minimum early stopping rate,
    `min_early_stopping`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义乘数因子，`factor`，第一次迭代中使用的最小资源量，`min_resources`，以及最小早期停止率，`min_early_stopping`。
- en: Define the initial number of hyperparameter candidates to be evaluated at the
    first iteration, `n_candidates`. If grid search is utilized, this will be automatically
    parsed from the total number of hyperparameter combinations in the search space.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义第一次迭代要评估的超参数候选者初始数量，`n_candidates`。如果使用网格搜索，这将从搜索空间中总超参数组合数自动解析。
- en: 'Calculate the maximum number of iterations, *n*iter, using the following formula:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下公式计算最大迭代次数*n*iter：
- en: '![](img/Formula_B18753_06_001.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_06_001.png)'
- en: Assert if `n_candidates` ≥ ![](img/Formula_B18753_06_002.png) to ensure there
    is at least one candidate in the last iteration.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 断言`n_candidates` ≥ ![公式](img/Formula_B18753_06_002.png)以确保在最后迭代中至少有一个候选者。
- en: 'Warm up the first iteration:'
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预热第一次迭代：
- en: Sample `n_candidates` sets of hyperparameters from the hyperparameter space.
    If grid search is utilized, just return all of the hyperparameter combinations
    in the space. This set of candidates is referred to as *candidates1*.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从超参数空间中采样`n_candidates`个超参数集。如果使用网格搜索，只需返回空间中的所有超参数组合。这组候选者被称为*candidates1*。
- en: Evaluate all *candidates1* sets of hyperparameters, using `min_resources`, based
    on the objective function, *f*.
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据目标函数*f*，使用`min_resources`评估所有*candidates1*超参数集。
- en: 'Calculate the *topK* value that will be used to select top candidates for the
    next iteration:'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算用于选择下一迭代顶级候选者的*topK*值：
- en: '![](img/Formula_B18753_06_003.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_06_003.png)'
- en: 'For each iteration, *i*, starting from the second iteration until *n*iter iteration,
    proceed as follows:'
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个迭代*i*，从第二次迭代开始直到*n*iter迭代，按以下步骤进行：
- en: Update the current set of candidates, *candidatesi*, by selecting *topK* candidates
    from *candidatesi-1* in terms of the most optimal objective function score.
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从*candidatesi-1*中选择*topK*个最优化目标函数得分的候选者来更新当前候选者集*candidatesi*。
- en: 'Update the current allocated resources, *resourcesi*, based on the following
    formula:'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据以下公式更新当前分配的资源*resourcesi*：
- en: '![](img/Formula_B18753_06_004.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_06_004.png)'
- en: Evaluate all *candidatesi* sets of hyperparameters, using *resourcesi*, based
    on the objective function, *f*.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据目标函数*f*，使用*resourcesi*评估所有*candidatesi*超参数集。
- en: 'Update the *topK* value based on the following formula:'
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据以下公式更新*topK*值：
- en: '![](img/Formula_B18753_06_005.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_06_005.png)'
- en: 'Return the best hyperparameter candidate:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回最佳超参数候选者：
- en: Evaluate all candidates in the last iteration using the allocated number of
    resources and the objective function, `f`. Note that it’s possible that the allocated
    resource in the last iteration is less than `max_resources`.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分配的资源数量和目标函数`f`评估最后迭代中的所有候选者。请注意，最后迭代中分配的资源可能少于`max_resources`。
- en: Select the candidate with the optimal objective function score.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择具有最佳目标函数得分的候选者。
- en: Train on the full train set using the best set of hyperparameters from *step
    11*.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第11步中最佳的超参数集在完整训练集上训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练的模型。
- en: Based on the previous example and the stated procedure, we can see that SH performs
    cheap, low-fidelity evaluations on the first several iterations by using a low
    number of resources and starts to perform more expensive high-fidelity evaluations
    on the final several iterations by using a high number of resources.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前面的示例和所述程序，我们可以看到SH在前几次迭代中通过使用少量资源进行低成本的、低保真度的评估，并在最后几次迭代中通过使用大量资源开始进行更昂贵的、高保真度的评估。
- en: Integration with Other Black-Box Methods
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他黑盒方法的集成
- en: SH can also be utilized along with other black-box hyperparameter tuning methods
    apart from grid and random search. For example, in the **Optuna** (see [*Chapter
    9*](B18753_09_ePub.xhtml#_idTextAnchor082)*, Hyperparameter Tuning via Optuna*)
    package, we can combine TPE (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*) with SH, where SH acts as a **pruner**. Note
    that in Optuna, the budget/resources is defined as the number of training steps
    or epochs instead of the number of samples.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: SH除了网格搜索和随机搜索之外，还可以与其他黑盒超参数调优方法结合使用。例如，在**Optuna**（见[*第9章*](B18753_09_ePub.xhtml#_idTextAnchor082)*，通过Optuna进行超参数调优*）包中，我们可以将TPE（见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*）与SH结合，其中SH充当**修剪器**。请注意，在Optuna中，预算/资源定义为训练步数或epoch数，而不是样本数。
- en: 'The following is a list of the pros and cons of SH as a hyperparameter tuning
    method:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个列表，列出了SH作为超参数调整方法的优缺点：
- en: '![Figure 6.7 – Pros and cons of SH'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.7 – SH的优缺点'
- en: '](img/B18753_06_007.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_007.jpg)'
- en: Figure 6.7 – Pros and cons of SH
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 – SH的优缺点
- en: In practice, most of the time, we do not know how to balance the trade-off between
    the number of resources and the number of candidates since there is no clear definition
    of how to distinguish bad and good hyperparameters. One thing that can help us
    to find a sweet spot in this trade-off is leveraging previous similar experiment
    configurations or by performing **meta-learning** based on the available meta-data
    from previous similar experiments.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，大多数时候，我们不知道如何平衡资源量和候选者数量之间的权衡，因为没有明确的定义来区分坏的和好的超参数。可以帮助我们在这种权衡中找到甜点的一点是利用之前的类似实验配置，或者基于之前类似实验的可用的元数据执行**元学习**。
- en: Now you are aware of SH, how it works, when to use it, and its pros and cons,
    in the next section, we will learn about an extension of this method that attempts
    to overcome the cons of SH.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了SH，它的运作方式，何时使用它，以及它的优缺点，在下一节中，我们将学习这个方法的扩展，它试图克服SH的缺点。
- en: Understanding hyper band
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解超带宽
- en: '**Hyper Band** (**HB**) is an extension of SH that is specifically designed
    to overcome issues inherent in SH (see *Figure 6.7*). Although we can perform
    meta-learning to help us balance the trade-off, most of the time we do not have
    the metadata that’s needed in practice. Furthermore, the possibility of SH removing
    better sets of hyperparameters in the first several iterations is also worrying
    and can’t be solved by just finding a sweet spot from the trade-off. HB tries
    to solve these issues by calling SH several times iteratively.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '**超带宽**（**HB**）是SH的一个扩展，专门设计用来克服SH固有的问题（参见*图6.7*）。尽管我们可以执行元学习来帮助我们平衡权衡，但在实践中我们大多数时候并没有所需的元数据。此外，SH在前几次迭代中可能会移除更好的超参数集，这也是令人担忧的，仅仅通过找到权衡的甜点是无法解决的。HB通过迭代调用SH多次来尝试解决这些问题。'
- en: Since HB is just an extension of SH, it is suggested that you utilize HB as
    your hyperparameter tuning method when you are working with a large model (for
    example, a deep neural network) and/or working with a large amount of data, just
    like SH. Furthermore, it is even better to utilize HB than SH when you do not
    have the time or metadata needed to help you configure the trade-off between the
    amount of resources and the number of candidates, which is the case most of the
    time.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于HB只是SH的一个扩展，建议你在处理大型模型（例如，深度神经网络）和/或处理大量数据时，像SH一样使用HB作为你的超参数调整方法。此外，当你没有时间或元数据来帮助你配置资源量和候选者数量之间的权衡时，使用HB甚至比SH更好，这种情况在大多数时候都会发生。
- en: The main difference between HB and SH is in their hyperparameters. HB has the
    same hyperparameters as SH (see the *Understanding SH* section) except for `n_candidates`.
    In HB, we don’t have to choose the best value for `n_candidates` since it is calculated
    automatically within the HB algorithm.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: HB和SH之间的主要区别在于它们的超参数。HB具有与SH相同的超参数（参见*理解SH*部分），除了`n_candidates`。在HB中，我们不需要选择`n_candidates`的最佳值，因为它在HB算法中自动计算。
- en: Basically, HB works by running SH iteratively with variations of `n_candidates`
    and `min_resources` in each of the `n_candidates` and the lowest possible value
    for `min_resources`, and going to the lowest possible value for `n_candidates`
    and the highest possible value for *resources* (see *Figure 6.8*). It’s like a
    brute-force approach to try *almost* all of the possible combinations of `n_candidates`
    and `min_resources`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，HB通过迭代运行SH，每次迭代中`n_candidates`和`min_resources`都有变化，以及每个`n_candidates`和`min_resources`可能的最小值，并逐渐降低`n_candidates`的可能值，同时提高*资源*可能的最大值（参见*图6.8*）。这就像是一种暴力方法，尝试*几乎*所有可能的`n_candidates`和`min_resources`的组合。
- en: '![Figure 6.8 – Illustration of the HB process. Here, nj and rj refer to n_candidates
    and min_resources for bracket-j, respectively'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.8 – HB过程的示意图。这里，nj和rj分别指代bracket-j的n_candidates和min_resources'
- en: '](img/B18753_06_008.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_008.jpg)'
- en: Figure 6.8 – Illustration of the HB process. Here, nj and rj refer to n_candidates
    and min_resources for bracket-j, respectively
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 – HB过程的示意图。这里，nj和rj分别指代bracket-j的n_candidates和min_resources
- en: As illustrated in *Figure 6.8*, assume that we set `factor=3`, `min_resources=1`,
    `max_resources=27`, and `min_early_stopping=0`. As you can see, HB allocates the
    minimum amount of resources with the maximum number of candidates in the first
    bracket, while it allocates the maximum amount of resources with the minimum number
    of candidates in the last bracket. Again, each bracket refers to each SH run,
    meaning we are running SH four times in this illustration, where the last bracket
    is basically the same as performing random or grid search on a small hyperparameter
    space.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图6.8*所示，假设我们设置`factor=3`，`min_resources=1`，`max_resources=27`，和`min_early_stopping=0`。正如你所见，HB在第一个括号中分配了最小数量的资源，同时分配了最大数量的候选者，而在最后一个括号中分配了最大数量的资源，同时分配了最小数量的候选者。再次强调，每个括号都指的是每次SH运行，这意味着在这个示例中我们运行了四次SH，其中最后一个括号基本上等同于在小的超参数空间上进行随机搜索或网格搜索。
- en: By testing *almost* all of the possible combinations of `n_candidates` and `min_resources`,
    HB is able to remove the trade-off in SH while also reducing the possibility of
    excluding better hyperparameters in the first iterations. However, this groundbreaking
    characteristic of HB *doesn’t ensure that it will be always better than SH*. Why?
    Because HB hasn’t actually tried all the possible combinations. We might find
    a better combination of `n_candidates` and `min_resources` values just by performing
    a single SH than all the possible combinations HB tried. However, this takes time
    and luck since we have to manually select the `n_candidates` and `min_resources`
    values.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 通过测试几乎所有的`n_candidates`和`min_resources`的可能组合，HB能够在SH中消除权衡，同时减少在第一次迭代中排除更好的超参数的可能性。然而，HB的这项开创性特性*并不能保证它总是比SH更好*。为什么？因为HB实际上并没有尝试所有可能的组合。我们可能通过执行一次SH就能找到比HB尝试的所有可能组合更好的`n_candidates`和`min_resources`值的组合。然而，这需要时间和运气，因为我们必须手动选择`n_candidates`和`min_resources`值。
- en: Integration with Other Black-Box Methods
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 集成其他黑盒方法
- en: In the original paper on HB, the authors utilize random search for each SH run.
    However, as with SH, we can also integrate HB with other black-box methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在HB的原始论文中，作者为每次SH运行使用了随机搜索。然而，与SH一样，我们也可以将HB与其他黑盒方法集成。
- en: 'The following procedure further states how HB works formally as a hyperparameter
    tuning method:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的过程进一步说明了HB作为超参数调整方法的正式工作原理：
- en: Split the full original dataset into train and test sets.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将完整原始数据集分为训练集和测试集。
- en: Define the hyperparameter space, `H`, with the accompanied distributions, and
    the objective function, `f`, based on the training set.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集定义超参数空间，`H`及其伴随的分布，以及目标函数，`f`。
- en: Define the `budget` resource. This is usually defined as the number of samples
    or training epochs.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义`budget`资源。这通常定义为样本数或训练epoch数。
- en: Define the maximum resources, `max_resources`. This is usually defined as the
    total number of samples in the data or the total number of epochs.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义最大资源，`max_resources`。这通常定义为数据中的总样本数或总epoch数。
- en: Define the multiplier factor, `factor`, the minimum early stopping rate, `min_early_stopping`,
    and the minimum number of resources for all brackets, `min_resources`. Usually,
    `min_resources` is set to one, if the budget is defined as the number of samples.
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义乘数因子，`factor`，最小早期停止率，`min_early_stopping`，以及所有括号的最小资源数量，`min_resources`。通常，如果预算定义为样本数，则`min_resources`设置为1。
- en: Create a dictionary, `top_candidates`, that will be utilized to store the best-performing
    set of hyperparameters from each SH run.
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个字典，`top_candidates`，它将被用来存储每次SH运行中表现最好的超参数集。
- en: 'Calculate the number of brackets, *nbrackets*, using the following formula:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下公式计算括号的数量，*nbrackets*：
- en: '![](img/Formula_B18753_06_016.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_06_016.png)'
- en: 'For each bracket-*j*, starting from `j=1` until `j=nbrackets`, do the following:'
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个括号-*j*，从`j=1`开始，直到`j=nbrackets`，执行以下操作：
- en: 'Calculate the minimum number of resources to be used at the first iteration
    of SH for bracket-*j*, ![](img/Formula_B18753_06_017.png), using the following
    formula:'
  id: totrans-161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下公式计算SH第一次迭代中用于括号-*j*的最小资源数量，![](img/Formula_B18753_06_017.png)：
- en: '![](img/Formula_B18753_06_018.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_06_018.png)'
- en: 'Calculate the initial number of hyperparameter candidates to be evaluated at
    the first iteration of SH for bracket-*j*, ![](img/Formula_B18753_06_019.png),
    using the following formula:'
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下公式计算SH第一次迭代中用于括号-*j*的初始超参数候选数量，![](img/Formula_B18753_06_019.png)：
- en: '![](img/Formula_B18753_06_020.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_06_020.png)'
- en: Do *steps 7 – 11* from the SH procedure given in the *Understanding SH* section
    by utilizing ![](img/Formula_B18753_06_021.png) as the `min_resources` and ![](img/Formula_B18753_06_022.png)
    as the `n_candidates` for the current SH run, respectively. Other hyperparameters
    for SH, such as `max_resources`, `min_early_stopping`, and `factor`, are inherited
    from HB.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*理解SH*部分给出的SH过程，利用 ![](img/Formula_B18753_06_021.png) 作为当前SH运行的`min_resources`，![](img/Formula_B18753_06_022.png)
    作为`n_candidates`执行*步骤 7 – 11*。
- en: Store the best set of hyperparameters output from the current SH run, along
    with the objective function score, in the `top_candidates` dictionary.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前SH运行输出的最佳超参数集以及目标函数分数存储在`top_candidates`字典中。
- en: Select the best candidate that has the most optimal objective function score
    from the `top_candidates` dictionary.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`top_candidates`字典中选择具有最佳目标函数分数的最佳候选者。
- en: Train on the full training set using the best set of hyperparameters from *step
    9*.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*步骤 9*中的最佳超参数集在完整训练集上训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: 'The following table summarizes the pros and cons of utilizing HB as a hyperparameter
    tuning method:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了利用HB作为超参数调整方法的优缺点：
- en: '![Figure 6.9 – Pros and cons of HB'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.9 – HB 的优缺点](img/B18753_06_009.jpg)'
- en: '](img/B18753_06_009.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_009.jpg)'
- en: Figure 6.9 – Pros and cons of HB
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 – HB 的优缺点
- en: It is worth noting that although HB can help us to deal with the trade-off of
    SH, it has a higher computational cost, since we have to run several SH rounds
    iteratively. It is even more costly when we are faced with a case where the bad
    and good hyperparameters cannot be easily distinguished with a small budget value.
    Why? The first several brackets of HB that utilize small budgets will result in
    a noisy estimation, since the relative rankings inside the SH iterations on smaller
    budgets do not reflect the actual relative rankings on higher budgets. In the
    most extreme case, the best set of hyperparameters will result from the last bracket
    (random search). If this is the case, then HB will run *n**brackets* times slower
    compared to random search.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管HB可以帮助我们处理SH的权衡，但它有更高的计算成本，因为我们必须迭代地运行几个SH轮次。当我们面临一个坏的和好的超参数难以通过小预算值区分的情况时，成本甚至更高。为什么？HB的前几个使用小预算的括号会导致噪声估计，因为较小预算上的SH迭代中的相对排名并不反映较高预算上的实际相对排名。在最极端的情况下，最佳的超参数集将来自最后一个括号（随机搜索）。如果这种情况发生，那么HB将比随机搜索慢
    *n**括号* 倍。
- en: In this section, we have discussed HB, what it is, how it works, and its pros
    and cons. We will discuss another interesting MFO method in the next section.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了HB，它是什么，它是如何工作的，以及它的优缺点。我们将在下一节讨论另一个有趣的MFO方法。
- en: Understanding BOHB
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解BOHB
- en: '**Bayesian Optimization and Hyper Band** (**BOHB**) is an extension of HB that
    is superior to CFS, SH, and HB, in terms of understanding the relationship between
    the hyperparameter candidates and the objective function. If CFS, SH, and HB are
    all part of the informed search group based on random search, BOHB is an informed
    search group that is based on the BO method. This means BOHB is able to decide
    which subspace needs to be searched based on previous experiences rather than
    luck.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '**贝叶斯优化和超带**（**BOHB**）是HB的一个扩展，它在理解超参数候选者与目标函数之间的关系方面优于CFS、SH和HB。如果CFS、SH和HB都是基于随机搜索的有信息搜索组，那么BOHB是基于BO方法的有信息搜索组。这意味着BOHB能够根据以往的经验而不是运气来决定需要搜索哪个子空间。'
- en: As its name implies, BOHB is the combination of the BO and HB methods. While
    SH and HB can also be utilized with other black-box methods (see the *Understanding
    SH* and *Understanding HB* sections), BOHB is specifically designed to utilize
    a BO method in a way that can support HB. Furthermore, the BO method in BOHB also
    tracks all the previous evaluations on all budgets, so that it can serve as the
    base for future evaluations. Note that the BO method used in BOHB is the **multivariate
    TPE**, which is able to take into account the interdependencies among hyperparameters
    (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring Bayesian
    Optimization*).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，BOHB是BO和HB方法的组合。虽然SH和HB也可以与其他黑盒方法一起使用（参见*理解SH*和*理解HB*部分），但BOHB专门设计用来以支持HB的方式利用BO方法。此外，BOHB中的BO方法还跟踪所有预算上的所有先前评估，以便它可以作为未来评估的基础。请注意，BOHB中使用的BO方法是**多变量TPE**，它能够考虑到超参数之间的相互依赖性（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*）。
- en: The main selling point of BOHB is its ability to achieve both a strong initial
    performance and a strong final performance. This can be easily seen in *Figure
    6.10*, from the original BOHB paper (see the following note for details). BO (without
    performing metalearning) will outperform random search if we have more time to
    let it learn from previous experiences. If we don’t have time, BO will deliver
    a similar or even worse performance compared to random search. On the other hand,
    HB performs much better than random search when we have limited time, but will
    perform similarly to random search if we allow more time for random search to
    explore the hyperparameter space. By combining the best of both worlds, BOHB is
    able to not only outperform random search in a limited time but also when given
    enough time for random search to catch up.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: BOHB的主要卖点是其能够实现强大的初始性能和最终性能。这可以从原始BOHB论文的*图6.10*中轻松看出（以下注释中提供详细信息）。如果没有进行元学习，BO（BO）将优于随机搜索，如果我们有更多时间让它从以往的经验中学习。如果没有时间，BO将提供与随机搜索相似甚至更差的性能。另一方面，当时间有限时，HB的表现远优于随机搜索，但如果允许随机搜索有更多时间探索超参数空间，HB的表现将与随机搜索相似。通过结合两者的优点，BOHB不仅能够在有限的时间内优于随机搜索，而且当给随机搜索足够的时间时也能做到。
- en: '![Figure 6.10 – Comparison between random search, BO, HB, and BOHB'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.10 – 随机搜索、BO、HB和BOHB的比较'
- en: '](img/B18753_06_010.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_010.jpg)'
- en: Figure 6.10 – Comparison between random search, BO, HB, and BOHB
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 – 随机搜索、BO、HB和BOHB的比较
- en: The Original BOHB Paper
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 原始BOHB论文
- en: '*BOHB: Robust and Efficient Hyperparameter Optimization at Scale* by Stefan
    Falkner, Aaron Klein, and Frank Hutter, Proceedings of the 35th International
    Conference on Machine Learning, PMLR 80:1437-1446, 2018 ([http://proceedings.mlr.press/v80/falkner18a.html](http://proceedings.mlr.press/v80/falkner18a.html)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '*《大规模鲁棒且高效的超参数优化：BOHB》由Stefan Falkner、Aaron Klein和Frank Hutter著，第35届国际机器学习会议论文集，PMLR
    80:1437-1446，2018 ([http://proceedings.mlr.press/v80/falkner18a.html](http://proceedings.mlr.press/v80/falkner18a.html))。'
- en: The following procedure further states how BOHB works formally as a hyperparameter
    tuning method. Note that BOHB and HB are very similar except that random search
    in HB is replaced by the combination of multivariate TPE and random search. Since
    HB just performs SH several times iteratively, the actual replacement is actually
    performed in each of the SH runs (each bracket) in HB.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 以下过程进一步说明了BOHB作为超参数调整方法在形式上是如何工作的。请注意，BOHB和HB非常相似，除了HB中的随机搜索被多变量TPE和随机搜索的组合所取代。由于HB只是多次迭代地执行SH，实际的替换实际上是在HB的每个SH运行（每个括号）中执行的。
- en: Let’s pick up from the previous instructions again.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从之前的说明重新开始。
- en: '*6\. (The first six steps are the same as those in the Understanding HB section.)*'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*6.（前六个步骤与理解HB部分中的相同。)*'
- en: 7\. Define the probability of just performing a random search rather than fitting
    the multivariate TPE, *random_prob*.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 定义仅执行随机搜索而不是拟合多变量TPE的概率，*random_prob*。
- en: 8\. Define the percentage of the good set of hyperparameters for the multivariate
    TPE fitting procedure, *top_n_percent*. (See [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*.)
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 定义多变量TPE拟合过程中良好超参数集的百分比，*top_n_percent*。（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*。）
- en: 9\. Define a dictionary, *candidates_dict*, that stores the budget/resources
    used in a particular SH iteration and the pairs of hyperparameter candidates and
    the objective function score as the key and value, respectively.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 9. 定义一个字典*candidates_dict*，该字典存储特定SH迭代中使用的预算/资源，以及超参数候选对和目标函数得分的键值对。
- en: 10\. Define the minimum number of sets of hyperparameters that are randomly
    sampled before starting to fit the multivariate TPE, `n_min`. By default, we set
    `n_min` to match the number of hyperparameters in the space plus one.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 10. 定义在开始拟合多变量TPE之前随机采样的超参数集的最小数量`n_min`。默认情况下，我们将`n_min`设置为空间中超参数数量加一。
- en: '11\. For each bracket-*j*, starting from `j=1` until `j=nbrackets`, do the
    following:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 11. 对于每个bracket-*j*，从`j=1`开始，直到`j=nbrackets`，执行以下操作：
- en: 'Calculate the minimum number of resources to be used on the first iteration
    of SH for bracket-*j*, ![](img/Formula_B18753_06_023.png), using the following
    formula:'
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下公式计算SH算法第一次迭代中用于bracket-*j*的最低资源量！[公式](img/Formula_B18753_06_023.png)：
- en: '![](img/Formula_B18753_06_024.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![公式图](img/Formula_B18753_06_024.png)'
- en: 'Calculate the initial number of hyperparameters candidates to be evaluated
    on the first iteration of SH for bracket-j, ![](img/Formula_B18753_06_025.png),
    using the following formula:'
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下公式计算SH算法第一次迭代中用于bracket-j的初始超参数候选数量！[公式](img/Formula_B18753_06_025.png)：
- en: '![](img/Formula_B18753_06_026.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_06_026.png)'
- en: 'Perform steps 7 – 11 from the SH procedure stated in the Understanding SH section
    by utilizing ![](img/Formula_B18753_06_027.png) as min_resources and ![](img/Formula_B18753_06_028.png)
    as n_candidates for the current SH run, respectively, where step 9\. I. is replaced
    with the following procedure:'
  id: totrans-197
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过利用![公式](img/Formula_B18753_06_027.png)作为min_resources和![公式](img/Formula_B18753_06_028.png)作为n_candidates，按照理解SH部分中所述的SH过程执行步骤7
    – 11，其中步骤9\. I.被以下程序替换：
- en: Generate a random number between zero and one from a uniform distribution, rnd.
  id: totrans-198
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从均匀分布中生成一个介于零和一之间的随机数，rnd。
- en: If `rnd<random_prod` or *models_dict* is empty, perform a random search to sample
    the initial hyperparameter candidates.
  id: totrans-199
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`rnd<random_prod`或*models_dict*为空，则执行随机搜索以采样初始超参数候选。
- en: Count the number of sampled hyperparameters in `candidates_dict[`![](img/Formula_B18753_06_029.png)`]`,
    and store it as `num_curr_candidates`.
  id: totrans-200
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算在`candidates_dict[`![公式](img/Formula_B18753_06_029.png)`]`中采样的超参数数量，并将其存储为`num_curr_candidates`。
- en: If `num_curr_candidates < n_min`, then perform a random search to sample the
    initial hyperparameter candidates.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果`num_curr_candidates < n_min`，则执行随机搜索以采样初始超参数候选。
- en: 'Alternatively, utilize the multivariate TPE (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization*) to sample the initial hyperparameter candidates.
    Note that we always utilize multivariate TPE on the largest budget available in
    `candidates_dict`. The number of hyperparameter sets for both good and bad groups
    is defined based on the following formula:'
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 或者，利用多变量TPE（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化）来采样初始超参数候选。请注意，我们总是在`candidates_dict`中使用的最大预算上利用多变量TPE。对于好组和坏组，超参数集的数量基于以下公式定义：
- en: '![](img/Formula_B18753_06_030.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_06_030.png)'
- en: '![](img/Formula_B18753_06_031.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![公式图](img/Formula_B18753_06_031.png)'
- en: Store the sampled initial hyperparameter candidates along with the objective
    function score (either from step ii, iv, or v) in `candidates_dict[`![](img/Formula_B18753_06_032.png)`]`.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将采样的初始超参数候选以及目标函数得分（来自步骤ii、iv或v）存储在`candidates_dict[`![公式](img/Formula_B18753_06_032.png)``]`中。
- en: Store the best set of hyperparameters output from the current SH run, along
    with the objective function score, in the `top_candidates` dictionary.
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将当前SH运行输出的最佳超参数集以及目标函数得分存储在`top_candidates`字典中。
- en: Select the best candidate that has the most optimal objective function score
    from the `top_candidates` dictionary.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`top_candidates`字典中选择具有最佳目标函数得分的最佳候选者。
- en: Train on the full training set using the best set of hyperparameters from *step
    14*.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第14步中找到的最佳超参数集在完整训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: 'Note that to ensure that BOHB tracks all of the evaluations on all budgets,
    we also need to store the hyperparameter candidates in each of the SH iterations
    for each HB bracket to `candidates_dict[budget]` along with their objective function
    score. Here, hyperparameter candidates in each of the SH iterations refer to *candidatesi*,
    while budget refers to *resourcesi* in *step 10* in the *Understanding SH* section,
    which also can be seen in the following figure:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了确保BOHB跟踪所有预算上的所有评估，我们还需要在每个HB分组的每个SH迭代中，将超参数候选者及其目标函数得分存储在`candidates_dict[budget]`中。在这里，每个SH迭代中的超参数候选者指的是*candidatesi*，而预算指的是*step
    10*部分中的*resourcesi*，这也可以在以下图中看到：
- en: '![Figure 6.11 – BOHB tracks all the evaluations on all budgets'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.11 – BOHB跟踪所有预算上的评估'
- en: '](img/B18753_06_011.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_011.jpg)'
- en: Figure 6.11 – BOHB tracks all the evaluations on all budgets
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 – BOHB跟踪所有预算上的评估
- en: You may wonder whether it is possible for BOHB to take advantage of parallel
    resources since it utilizes a BO method that is notorious for not being able to
    exploit parallel computing resources. The answer is *yes, it is possible*! You
    can take advantage of parallel resources since in each of the BOHB iterations,
    specifically in the HB iterations, we can utilize more than one worker to evaluate
    multiple sets of hyperparameters, in parallel.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道BOHB是否可以利用并行资源，因为它使用的是一种因无法利用并行计算资源而臭名昭著的BO方法。答案是*是的，这是可能的*！你可以利用并行资源，因为在每个BOHB迭代中，特别是在HB迭代中，我们可以利用多个工作者并行评估多组超参数。
- en: 'What about the sequential nature of the multivariate TPE utilized in BOHB?
    Yes, there may be some sequential processes that need to be performed inside the
    TPE model. However, BOHB limits the number of sets of hyperparameters given to
    the multivariate TPE so it might not take too much time. Furthermore, the limitation
    on the number of hyperparameter sets is actually specifically designed by the
    authors of BOHB. The following is a direct quote from the original paper on BOHB:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，BOHB中使用的多变量TPE的顺序性质如何呢？是的，TPE模型内部可能需要执行一些顺序过程。然而，BOHB限制了提供给多变量TPE的超参数组数，所以可能不会花费太多时间。此外，对超参数组数的限制实际上是BOHB的作者们专门设计的。以下是从BOHB原始论文中的直接引用：
- en: The parallelism in TPE is achieved by limiting the number of samples to optimize
    EI, purposefully not optimizing it fully to obtain diversity. This ensures that
    consecutive suggestions by the model are diverse enough to yield near-linear speedups
    when evaluated in parallel.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: TPE中的并行性是通过限制优化EI的样本数量来实现的，故意不全面优化以获得多样性。这确保了模型连续的建议在并行评估时足够多样化，从而产生接近线性的速度提升。
- en: It is also worth noting that we always utilize the multivariate TPE on the largest
    budget available to ensure that it is fitted on enough budget (high-fidelity)
    to minimize the chance of a noisy estimation. So, combined with the limitation
    on the number of hyperparameter sets passed to the TPE, we are trying to ensure
    that the multivariate TPE is fitted on the right number of hyperparameter sets.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得注意的是，我们始终在最大的预算上使用多变量TPE，以确保它有足够的预算（高保真度）来拟合，以最小化噪声估计的机会。因此，结合对传递给TPE的超参数组数的限制，我们试图确保多变量TPE被拟合在正确的超参数组数上。
- en: 'The following table summarizes the pros and cons of utilizing BOHB as a hyperparameter
    tuning method:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 下表总结了利用BOHB作为超参数调整方法的优缺点：
- en: '![Figure 6.12 – Pros and cons of BOHB'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.12 – BOHB的优缺点'
- en: '](img/B18753_06_012.jpg)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_06_012.jpg)'
- en: Figure 6.12 – Pros and cons of BOHB
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 – BOHB的优缺点
- en: Just as HB may run *nbrackets* times slower compared to random search when we
    are faced with a situation where the bad and good hyperparameters cannot be easily
    distinguished with a small budget value, BOHB will also run *nbrackets* times
    slower compared to the vanilla BO, where we are faced with the same condition.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 就像HB在面临无法用小预算值轻松区分好坏超参数的情况下，与随机搜索相比可能慢* nbrackets*倍一样，BOHB在面临相同条件时，与传统的BO相比也会慢*
    nbrackets*倍。
- en: In this section, we have covered BOHB in detail, including what it is, how it
    works, and its pros and cons.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们详细介绍了BOHB，包括它是什么，如何工作，以及它的优缺点。
- en: Summary
  id: totrans-224
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have discussed the fourth of the four groups of hyperparameter
    tuning methods, called the MFO group. We have discussed MFO in general and what
    makes it different from black-box optimization methods, as well as discussing
    several variants, including CFS, SH, HB, and BOHB. We have seen the differences
    between them and the pros and cons of each. From now on, you should be able to
    explain MFO with confidence when someone asks you about it. You should also be
    able to debug and set up the most suitable configuration for the chosen method
    that suits your specific problem definition.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了四种超参数调整方法中的第四组，称为MFO组。我们一般性地讨论了MFO，以及它区别于黑盒优化方法的特点，还讨论了包括CFS、SH、HB和BOHB在内的几个变体。我们看到了它们之间的差异以及每种方法的优缺点。从现在开始，当有人向你询问MFO时，你应该能够自信地解释它。你也应该能够调试并设置最适合你特定问题定义的所选方法的最佳配置。
- en: In the next chapter, we will begin implementing the various hyperparameter tuning
    methods that we have learned about so far using the scikit-learn package. We will
    become familiar with the scikit-learn package and learn how to utilize it in various
    hyperparameter tuning methods.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始使用scikit-learn包实现我们迄今为止所学的各种超参数调整方法。我们将熟悉scikit-learn包，并学习如何在各种超参数调整方法中利用它。
