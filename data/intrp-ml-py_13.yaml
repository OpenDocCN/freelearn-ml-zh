- en: '13'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '13'
- en: Adversarial Robustness
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对抗性鲁棒性
- en: Machine learning interpretation has many concerns, ranging from knowledge discovery
    to high-stakes ones with tangible ethical implications, like the fairness issues
    examined in the last two chapters. In this chapter, we will direct our attention
    to concerns involving reliability, safety, and security.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习解释有许多关注点，从知识发现到具有实际伦理影响的高风险问题，如上一两章中探讨的公平性问题。在本章中，我们将关注涉及可靠性、安全性和安全性的问题。
- en: As we realized using the **contrastive explanation method** in *Chapter 7*,
    *Visualizing Convolutional Neural Networks*, we can easily trick an image classifier
    into making embarrassingly false predictions. This ability can have serious ramifications.
    For instance, a perpetrator can place a black sticker on a yield sign, and while
    most drivers would still recognize this as a yield sign, a self-driving car may
    no longer recognize it and, as a result, crash. A bank robber could wear a cooling
    suit designed to trick the bank vault’s thermal imaging system, and while any
    human would notice it, the imaging system would fail to do so.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*第7章*，*可视化卷积神经网络*中使用的**对比解释方法**所意识到的那样，我们可以轻易地欺骗图像分类器做出令人尴尬的错误预测。这种能力可能具有严重的后果。例如，一个肇事者可以在让路标志上贴上一个黑色贴纸，尽管大多数司机仍然会将其识别为让路标志，但自动驾驶汽车可能就不再能识别它，从而导致撞车。银行劫匪可能穿着一种冷却服装来欺骗银行保险库的热成像系统，尽管任何人都可能注意到它，但成像系统却无法做到。
- en: The risk is not limited to sophisticated image classifiers. Other models can
    be tricked! The **counterfactual examples** produced in *Chapter 6**, Anchors
    and Counterfactual Explanations,* are like adversarial examples except with the
    goal of deception. An attacker could leverage any misclassification example, straddling
    the decision boundary adversarially. For instance, a spammer could realize that
    adjusting some email attributes increases the likelihood of circumventing spam
    filters.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 风险不仅限于复杂的图像分类器。其他模型也可能被欺骗！在*第6章*，锚点和反事实解释中产生的**反事实示例**，就像对抗性示例一样，但目的是欺骗。攻击者可以利用任何误分类示例，在决策边界上进行对抗性操作。例如，垃圾邮件发送者可能会意识到调整一些电子邮件属性可以增加绕过垃圾邮件过滤器的可能性。
- en: 'Complex models are more vulnerable to adversarial attacks. So why would we
    trust them?! We can certainly make them more foolproof, and that’s what adversarial
    robustness entails. An adversary can purposely thwart a model in many ways, but
    we will focus on evasion attacks and briefly explain other forms of attacks. Then
    we will explain two defense methods: spatial smoothing preprocessing and adversarial
    training. Lastly, we will demonstrate one robustness evaluation method.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂模型更容易受到对抗性攻击。那么我们为什么还要信任它们呢？！我们当然可以使它们更加可靠，这就是对抗性鲁棒性的含义。对手可以通过多种方式故意破坏模型，但我们将重点关注逃避攻击，并简要解释其他形式的攻击。然后我们将解释两种防御方法：空间平滑预处理和对抗性训练。最后，我们将展示一种鲁棒性评估方法。
- en: 'These are the main topics we will cover:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们将要讨论的主要主题：
- en: Learning about evasion attacks
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解逃避攻击
- en: Defending against targeted attacks with preprocessing
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预处理防御针对攻击
- en: Shielding against any evasion attack through adversarial training of a robust
    classifier
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过对鲁棒分类器的对抗性训练来防御任何逃避攻击
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: This chapter’s example uses the `mldatasets`, `numpy`, `sklearn`, `tensorflow`,
    `keras`, `adversarial-robustness-toolbox`, `matplotlib`, and `seaborn` libraries.
    Instructions on how to install all of these libraries are in the *Preface*.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的示例使用了`mldatasets`、`numpy`、`sklearn`、`tensorflow`、`keras`、`adversarial-robustness-toolbox`、`matplotlib`和`seaborn`库。如何安装所有这些库的说明在*前言*中。
- en: 'The code for this chapter is located here: [https://packt.link/1MNrL](https://packt.link/1MNrL)'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码位于此处：[https://packt.link/1MNrL](https://packt.link/1MNrL)
- en: The mission
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务
- en: The privately contracted security services industry market worldwide is valued
    at over USD 250 billion and is growing at around 5% annually. However, it faces
    many challenges, such as shortages of adequately trained guards and specialized
    security experts in many jurisdictions, and a whole host of unexpected security
    threats. These threats include widespread coordinated cybersecurity attacks, massive
    riots, social upheaval, and, last but not least, health risks brought on by pandemics.
    Indeed, 2020 tested the industry with a wave of ransomware, misinformation attacks,
    protests, and COVID-19 to boot.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 全球私人签约保安服务行业市场规模超过2500亿美元，年增长率为约5%。然而，它面临着许多挑战，例如在许多司法管辖区缺乏足够培训的保安和专业的安全专家，以及一系列意外的安全威胁。这些威胁包括广泛的协调一致的网络安全攻击、大规模暴乱、社会动荡，以及最后但同样重要的是，由大流行带来的健康风险。确实，2020年通过勒索软件、虚假信息攻击、抗议活动和COVID-19等一系列事件考验了该行业。
- en: In the wake of this, one of the largest hospital networks in the United States
    asked their contracted security company to monitor the correct use of masks by
    both visitors and personnel throughout the hospitals. The security company has
    struggled with this request because it diverts security personnel from tackling
    other threats, such as intruders, combative patients, and belligerent visitors.
    It has video surveillance in every hallway, operating room, waiting room, and
    hospital entrance. It’s impossible to have eyes on every camera feed every time,
    so the security company thought they could assist guards with deep learning models.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在此之后，美国最大的医院网络之一要求他们的签约保安公司监控医院内访客和员工佩戴口罩的正确性。保安公司因为这项请求而感到困扰，因为它分散了保安人员应对其他威胁（如入侵者、斗殴患者和挑衅访客）的精力。该公司在每个走廊、手术室、候诊室和医院入口都有视频监控。每次都不可能监控到每个摄像头的画面，因此保安公司认为他们可以用深度学习模型来协助保安：
- en: 'These models already alert unusual activities, such as running in the hallways
    and brandishing weapons anywhere on the premises. They have proposed to the hospital
    network that they would like to add a new model that detects masks’ correct usage.
    Before COVID-19, there were policies in place for mandatory mask usage in certain
    areas of each hospital, and during COVID-19, it was required everywhere. The hospital
    administrators would like to turn on and off this monitoring feature, depending
    on pandemic risk levels moving forward. They realize that personnel get fatigued
    and forget to put masks back on, or they partially slip off at times. Many visitors
    are also hostile toward using masks and may wear one when entering the hospital
    but take it off when no guard is around. This isn’t always intentional, so they
    wouldn’t want to dispatch guards on every alert, unlike other threats. Instead,
    they’d rather use awareness and a little bit of shame to modify behavior and only
    intervene with repeat offenders:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型已经能够检测到异常活动，例如在走廊里奔跑和在物业任何地方挥舞武器。他们向医院网络提出建议，希望添加一个新模型来检测口罩的正确使用。在COVID-19之前，医院各区域已经实施了强制佩戴口罩的政策，而在COVID-19期间，则要求在所有地方佩戴口罩。医院管理员希望根据未来的大流行风险水平来开启和关闭这一监控功能。他们意识到，人员会感到疲惫并忘记戴上口罩，或者有时口罩会部分滑落。许多访客也对佩戴口罩持敌对态度，他们可能会在进入医院时戴上口罩，但如果没有保安在场，就会摘下。这并不总是故意的，因此他们不希望像对其他威胁一样，在每次警报时都派遣保安：
- en: '![A yellow sign on a pole  Description automatically generated with low confidence](img/B18406_13_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![杆上的黄色标志  描述由低置信度自动生成](img/B18406_13_01.png)'
- en: 'Figure 13.1: Radar speed signs like this one help curb speeding'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1：像这样的雷达速度标志有助于遏制超速
- en: Awareness is a very effective method with radar speed signs (see *Figure 13.1*),
    which make roads safer by only making drivers aware that they are driving too
    fast. Likewise, having a screen at the end of heavily trafficked hallways showing
    snapshots of those who have recently either mistakenly or purposely not complied
    with mandatory mask usage potentially creates some embarrassment for offenders.
    The system will log repeat offenders so that security guards can look for them
    and either make them comply or ask them to vacate the premises.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 意识是雷达速度标志（见*图13.1*）的一种非常有效的方法，它通过仅让驾驶员意识到他们开得太快，从而使道路更安全。同样，在繁忙走廊的尽头设置屏幕，显示最近错误或故意未遵守强制佩戴口罩规定的人的快照，可能会让违规者感到尴尬。系统将记录反复违规者，以便保安可以找到他们，要么让他们遵守规定，要么要求他们离开现场。
- en: There’s some concern with visitors trying to trick the model into evading compliance,
    so the security company has hired you to ensure that the model is robust in the
    face of this kind of adversarial attack. Security officers have noticed some low-tech
    trickery before, such as people momentarily covering their faces with their hands
    or a part of their sweater when they realize cameras monitor them. Also, in one
    disturbing incident, a visitor dimmed the lights and sprayed some gel on a camera,
    and in another, an individual painted their mouth. However, there are concerns
    about higher-tech attacks, such as jamming the camera’s wireless signal or shining
    high-powered lasers directly into cameras. Devices that perform these attacks
    are increasingly easier to obtain and could impact other surveillance functions
    on a larger scale, like preventing theft. The security company hopes this robustness
    exercise can inform their efforts to improve every surveillance system and model.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 对于试图欺骗模型规避合规性的访客，存在一些担忧，因此安全公司雇佣你来确保模型在面对这种对抗性攻击时具有鲁棒性。安全官员在之前注意到一些低技术含量的诡计，例如人们在意识到摄像头正在监控他们时，会暂时用手或毛衣的一部分遮住他们的脸。在一个令人不安的事件中，访客降低了灯光，并在摄像头上喷了一些凝胶，在另一个事件中，有人涂鸦了他们的嘴巴。然而，人们对更高技术攻击的担忧，例如干扰摄像头的无线信号或直接向摄像头照射高功率激光。执行这些攻击的设备越来越容易获得，可能会对更大规模的监控功能，如防止盗窃，产生影响。安全公司希望这种鲁棒性练习能够为改善每个监控系统和模型的努力提供信息。
- en: Eventually, the security company would like to produce its own dataset with
    face images from the hospitals they monitor. Meanwhile, synthetically masked faces
    from external sources are the best they can do to productionize a model in the
    short term. To this end, you have been provided a large dataset of synthetically
    correctly and incorrectly masked faces and their unmasked counterparts. The two
    datasets were combined into a single one, and the original dimensions of 1,024
    × 1,024 were reduced to the thumbnail size of 124 × 124\. Also, for efficiency’s
    sake, 21,000 images were sampled from roughly 210,000 in these datasets.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，安全公司希望使用他们监控的医院中的面部图像来生成自己的数据集。同时，从外部来源合成的面具面部图像是他们短期内将模型投入生产的最佳选择。为此，你被提供了一组合成的正确和错误面具面部图像及其未面具对应图像的大型数据集。这两个数据集被合并成一个，原始的
    1,024 × 1,024 尺寸被减少到缩略图的 124 × 124 尺寸。此外，为了提高效率，从这些数据集中采样了大约 21,000 张图像。
- en: The approach
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 方法
- en: 'You’ve decided to take a four-fold approach:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经决定采取四步方法：
- en: Exploring several possible evasion attacks to understand how vulnerable the
    model is to them and how credible they are as threats
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索几种可能的规避攻击，以了解模型对这些攻击的脆弱性以及它们作为威胁的可靠性
- en: Using a preprocessing method to protect a model against these attacks
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预处理方法来保护模型免受这些攻击
- en: Leveraging adversarial retraining to produce a robust classifier that is intrinsically
    less prone to many of these attacks
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用对抗性再训练来生成一个本质上对许多此类攻击更不易受影响的鲁棒分类器
- en: Evaluating robustness with state-of-the-art methods to assure hospital administrators
    that the model is adversarially robust
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用最先进的方法评估鲁棒性，以确保医院管理员相信该模型具有对抗性鲁棒性
- en: Let’s get started!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: The preparations
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备工作
- en: 'You will find the code for this example here: [https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下位置找到这个示例的代码：[https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb](https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python-2E/tree/main/13/Masks.ipynb)
- en: Loading the libraries
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载库
- en: 'To run this example, you need to install the following libraries:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行此示例，你需要安装以下库：
- en: '`mldatasets` to load the dataset'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mldatasets` 用于加载数据集'
- en: '`numpy` and `sklearn` (scikit-learn) to manipulate it'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`numpy` 和 `sklearn` (scikit-learn) 用于操作它'
- en: '`tensorflow` to fit the models'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tensorflow` 用于拟合模型'
- en: '`matplotlib` and `seaborn` to visualize the interpretations'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`matplotlib` 和 `seaborn` 用于可视化解释'
- en: 'You should load all of them first:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该首先加载所有这些：
- en: '[PRE0]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s check that TensorFlow has loaded the right version with `print(tf.__version__)`.
    The version should be 2.0 and above.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用 `print(tf.__version__)` 检查 TensorFlow 是否加载了正确的版本。版本应该是 2.0 及以上。
- en: 'We should also disable eager execution and verify that it worked with the following
    command:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还应该禁用即时执行，并验证它是否已通过以下命令完成：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output should say that it’s `False`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该显示为 `False`。
- en: In TensorFlow, turning eager execution mode on means that it doesn’t require
    a computational graph or a session. It’s the default for TensorFlow 2.x and later
    but not in prior versions, so you need to disable it to avoid incompatibilities
    with code that was optimized for prior versions of TensorFlow.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow中，开启急切执行模式意味着它不需要计算图或会话。这是TensorFlow 2.x及以后版本的默认设置，但在之前的版本中不是，所以你需要禁用它以避免与为TensorFlow早期版本优化的代码不兼容。
- en: Understanding and preparing the data
  id: totrans-44
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解和准备数据
- en: 'We will load the data into four NumPy arrays, corresponding to the training/test
    datasets. While we are at it, we will divide `X` face images by 255 because, that
    way, they will be of values between zero and one, which is better for deep learning
    models. We call this feature scaling. We will need to record the `min_` and `max_`
    for the training data because we will need these later:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据加载到四个NumPy数组中，对应于训练/测试数据集。在此过程中，我们将`X`面部图像除以255，因为这样它们的值将在零和一之间，这对深度学习模型更好。我们称这种特征缩放。我们需要记录训练数据的`min_`和`max_`，因为我们稍后会需要这些信息：
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It’s always important to verify our data when we load it to make sure it didn’t
    get corrupted:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们加载数据时，始终验证数据非常重要，以确保数据没有被损坏：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Therefore, a preprocessing step we will need to perform is to **One-Hot Encode**
    (**OHE**) the `y` labels because we will need the OHE form to evaluate the model’s
    predictive performance. Once we initialize the `OneHotEncoder`, we will need to
    `fit` it into the training data (`y_train`). We can also extract the categories
    from the encoder into a list (`labels_l`) to verify that it has all three:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要执行的一个预处理步骤是将`y`标签**独热编码**（**OHE**），因为我们需要OHE形式来评估模型的预测性能。一旦我们初始化`OneHotEncoder`，我们就需要将其`fit`到训练数据（`y_train`）中。我们还可以将编码器中的类别提取到一个列表（`labels_l`）中，以验证它包含所有三个类别：
- en: '[PRE5]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For reproducibility’s sake, always initialize your random seeds like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保可复现性，始终以这种方式初始化你的随机种子：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Making machine learning truly reproducible means also making it deterministic,
    which means that training with the same data will produce a model with the same
    parameters. Determinism is very difficult with deep learning and is often session-,
    platform-, and architecture-dependent. If you use an NVIDIA GPU, you can install
    a library called `framework-reproducibility`.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 使机器学习真正可复现意味着也要使其确定性，这意味着使用相同的数据进行训练将产生具有相同参数的模型。在深度学习中实现确定性非常困难，并且通常依赖于会话、平台和架构。如果你使用NVIDIA
    GPU，你可以安装一个名为`framework-reproducibility`的库。
- en: 'Many of the adversarial attack, defense, and evaluation methods we will study
    in this chapter are very resource-intensive, so if we used the entire test dataset
    with them, they could likely take many hours on a single method! For efficiency,
    it is strongly suggested to use samples of the test dataset. Therefore, we will
    create a medium 200-image sample (`X_test_mdsample`, `y_test_mdsample`) and a
    small 20-image sample (`X_test_smsample`, `y_test_smsample`) using `np.random.choice`:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将要学习的许多对抗攻击、防御和评估方法都非常资源密集，所以如果我们用整个测试数据集来使用它们，它们可能需要数小时才能完成单个方法！为了提高效率，强烈建议使用测试数据集的样本。因此，我们将使用`np.random.choice`创建一个中等大小的200张图像样本（`X_test_mdsample`,
    `y_test_mdsample`）和一个小型20张图像样本（`X_test_smsample`, `y_test_smsample`）：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We have two sample sizes because some methods could take too long with a larger
    sample size. Now, let’s take a peek at what images are in our datasets. In the
    preceding code, we have taken a medium and small sample of our test dataset. We
    will place each image of our small sample in a 4 × 5 grid with the class label
    above it, with the following code:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个样本大小，因为某些方法在较大的样本大小下可能需要太长时间。现在，让我们看看我们的数据集中有哪些图像。在先前的代码中，我们已经从我们的测试数据集中取了一个中等和一个小样本。我们将使用以下代码将我们小样本中的每张图像放置在一个4
    × 5的网格中，类别标签位于其上方：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The preceding code plots the grid of images in *Figure 13.2*:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在*图13.2*中绘制了图像网格：
- en: '![A collage of a person  Description automatically generated with medium confidence](img/B18406_13_02.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![一个人物的拼贴画  描述由中等置信度自动生成](img/B18406_13_02.png)'
- en: 'Figure 13.2: A small test dataset sample of masked and unmasked faces'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2：一个带有遮挡和未遮挡面部的小型测试数据集样本
- en: '*Figure 13.2* depicts a variety of correctly and incorrectly masked and unmasked
    faces of all ages, genders, and ethnicities. Despite the variety, one thing to
    note about this dataset is that it only has light blue surgical masks represented,
    and images are mostly at a front-facing angle. Ideally, we would generate an even
    larger dataset with all colors and types of masks and augment it further with
    random rotations, shears, and brightness adjustments, either before or during
    training. These augmentations would make for a much more robust model. Nevertheless,
    we must differentiate between this general type of robustness and adversarial
    robustness.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 13.2* 展示了各种年龄、性别和种族的正面和反面、带口罩和不带口罩的面部图像。尽管种类繁多，但关于这个数据集的一个需要注意的事项是，它只展示了浅蓝色的外科口罩，且图像大多是正面角度。理想情况下，我们会生成一个包含所有颜色和类型口罩的更大数据集，并在训练前或训练期间对其进行随机旋转、剪切和亮度调整，以进一步增强模型的鲁棒性。这些增强将使模型更加鲁棒。尽管如此，我们必须区分这种一般类型的鲁棒性和对抗鲁棒性。'
- en: Loading the CNN base model
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载 CNN 基础模型
- en: 'You don’t have to train the CNN base model, but the code to do so is provided
    nonetheless in the GitHub repository. The pretrained model has also been stored
    there. We can quickly load the model and output its summary like this:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必训练 CNN 基础模型，但相关的代码已提供在 GitHub 仓库中。预训练模型也已存储在那里。我们可以快速加载模型并输出其摘要，如下所示：
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The preceding snippet outputs the following summary:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码片段输出了以下摘要：
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The summary has pretty much everything we need to know about the model. It has
    four convolutional layers (`Conv2D`), each followed by a max pool layer (`MaxPooling2D`).
    It then has a `Flatten` layer and a fully connected layer (`Dense`). Then, there’s
    more `Dropout` before the second `Dense` layer. Naturally, three neurons are in
    this final layer, corresponding to each class.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 摘要几乎包含了我们需要了解的所有关于模型的信息。它有四个卷积层 (`Conv2D`)，每个卷积层后面都跟着一个最大池化层 (`MaxPooling2D`)。然后是一个
    `Flatten` 层和一个全连接层 (`Dense`)。接着，在第二个 `Dense` 层之前还有更多的 `Dropout`。自然地，这个最终层有三个神经元，对应于每个类别。
- en: Assessing the CNN base classifier
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 评估 CNN 基础分类器
- en: 'We can evaluate the model using the test dataset with the `evaluate_multiclass_mdl`
    function. The arguments include the model (`base_model`), our test data (`X_test`),
    and the corresponding labels (`y_test`), as well as the class names (`labels_l`)
    and the encoder (`ohe`). Lastly, we don’t need to plot the ROC curves since, given
    the high accuracy, they won’t be very informative (`plot_roc=False`). This function
    returns the predicted labels and probabilities, which we can store as variables
    for later use:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `evaluate_multiclass_mdl` 函数和测试数据集来评估模型。参数包括模型 (`base_model`)、我们的测试数据
    (`X_test`) 和相应的标签 (`y_test`)，以及类名 (`labels_l`) 和编码器 (`ohe`)。最后，由于准确率很高，我们不需要绘制
    ROC 曲线（`plot_roc=False`）。此函数返回预测标签和概率，我们可以将这些变量存储起来以供以后使用：
- en: '[PRE11]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The preceding code generates *Figure 13.3*, with a confusion matrix and performance
    metrics for each class:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了 *图 13.3*，其中包含每个类别的混淆矩阵和性能指标：
- en: '![](img/B18406_13_03.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B18406_13_03.png)'
- en: 'Figure 13.3: The confusion matrix and predictive performance metrics for the
    base classifier, evaluated on the test dataset'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3：在测试数据集上评估的基础分类器的混淆矩阵和预测性能指标
- en: Even though the confusion matrix in *Figure 13.3* seems to suggest a perfect
    classification, pay attention to the circled areas. We can tell the model had
    some issues with misclassifying incorrectly masked faces once we see the recall
    (99.5%) breakdown.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图 13.3 中的混淆矩阵似乎表明分类完美，但请注意圈出的区域。一旦我们看到召回率（99.5%）的分解，我们就可以知道模型在错误地分类带口罩的面部图像时存在一些问题。
- en: Now, we can start attacking this model to assess how robust it actually is!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以开始攻击这个模型，以评估它的实际鲁棒性！
- en: Learning about evasion attacks
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解逃避攻击
- en: 'There are six broad categories of adversarial attacks:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 有六种广泛的对抗攻击类别：
- en: '**Evasion**: designing an input that can cause a model to make an incorrect
    prediction, especially when it wouldn’t fool a human observer. It can either be
    targeted or untargeted, depending on the attacker’s intention to fool the model
    into misclassifying a specific class (targeted) or, rather, misclassifying any
    class (untargeted). The attack methods can be white-box if the attacker has full
    access to the model and its training dataset, or black-box with only inference
    access. Gray-box sits in the middle. Black-box is always model-agnostic, whereas
    white and gray-box methods might be.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**规避攻击**：设计一个输入，使其能够导致模型做出错误的预测，尤其是当它不会欺骗人类观察者时。它可以是定向的或非定向的，这取决于攻击者意图欺骗模型将特定类别（定向）或任何类别（非定向）误分类。攻击方法可以是白盒攻击，如果攻击者可以完全访问模型及其训练数据集，或者黑盒攻击，只有推理访问。灰盒攻击位于中间。黑盒攻击总是模型无关的，而白盒和灰盒方法可能不是。'
- en: '**Poisoning**: injecting faulty training data or parameters into a model can
    come in many forms, depending on the attacker’s capabilities and access. For instance,
    for systems with user-generated data, the attacker may be capable of adding faulty
    data or labels. If they have more access, perhaps they can modify large amounts
    of data. They can also adjust the learning algorithm, hyperparameters, or data
    augmentation schemes. Like evasion, poisoning can also be targeted and untargeted.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**投毒攻击**：将错误的训练数据或参数注入模型，其形式取决于攻击者的能力和访问权限。例如，对于用户生成数据的系统，攻击者可能能够添加错误的数据或标签。如果他们有更多的访问权限，也许他们可以修改大量数据。他们还可以调整学习算法、超参数或数据增强方案。像规避攻击一样，投毒攻击也可以是定向的或非定向的。'
- en: '**Inference**: extracting the training dataset through model inference. Inference
    attacks also come in many forms and can be used for espionage (privacy attacks)
    through membership inference, which confirms if one example (for instance, a specific
    person) was in the training dataset. Attribute inference ascertains if an example
    category (for instance, an ethnicity) was represented in the training data. Input
    inference (also known as model inversion) has attack methods to extract a training
    dataset from a model rather than guessing and confirming. These have broad privacy
    and regulatory implications, especially in medical and legal applications.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**推理攻击**：通过模型推理提取训练数据集。推理攻击也以多种形式出现，可以通过成员推理进行间谍活动（隐私攻击），以确认一个示例（例如，一个特定的人）是否在训练数据集中。属性推理确定一个示例类别（例如，种族）是否在训练数据中表示。输入推理（也称为模型反演）有攻击方法可以从模型中提取训练数据集，而不是猜测和确认。这些具有广泛的隐私和监管影响，尤其是在医疗和法律应用中。'
- en: '**Trojaning**: this implants malicious functionality activated with a trigger
    during inference but requires retraining the model.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特洛伊木马攻击**：这会在推理期间通过触发器激活恶意功能，但需要重新训练模型。'
- en: '**Backdooring**: similar to trojans but a backdoor remains, even when a model
    is retrained from scratch.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后门攻击**：类似于特洛伊木马，但即使模型从头开始重新训练，后门仍然存在。'
- en: '**Reprogramming**: remote sabotaging of a model during training by sneaking
    in examples that are specifically designed to produce specific outputs. For instance,
    if you provide enough examples labeled as tiger shark where four small black squares
    are always in the same place, the model will learn that that is a tiger shark,
    regardless of what it is, thus intentionally forcing the model to overfit.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重编程**：在训练过程中通过悄悄引入专门设计以产生特定输出的示例来远程破坏模型。例如，如果你提供了足够多的标记为虎鲨的示例，其中四个小黑方块总是出现在相同的位置，模型就会学习到那是一个虎鲨，无论它是什么，从而故意迫使模型过度拟合。'
- en: 'The first three are the most studied forms of adversarial attacks. Attacks
    can be further subcategorized once we split them by stage and goal (see *Figure
    13.4*). The stage is when the attack is perpetrated because it can impact the
    model training or its inference, and the goal is what the attacker hopes to gain
    from it. This chapter will only deal with evasion sabotage attacks because we
    expect hospital visitors, patients, and personnel to occasionally sabotage the
    production model:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 前三种是最受研究的对抗攻击形式。一旦我们根据阶段和目标将它们分开，攻击可以进一步细分（见*图13.4*）。阶段是指攻击实施时，因为它可以影响模型训练或其推理，而目标是攻击者希望从中获得什么。本章将仅处理规避破坏攻击，因为我们预计医院访客、患者和工作人员偶尔会破坏生产模型：
- en: '![](img/B18406_13_04.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B18406_13_04.png)'
- en: 'Figure 13.4: Table of adversarial attack category methods by stage and goal'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4：按阶段和目标分类的对抗攻击方法表
- en: Even though we use white-box methods to attack, defend, and evaluate a model’s
    robustness, we don’t expect attackers to have this level of access. We will only
    use white-box methods because we have full access to the model. In other circumstances,
    such as a bank surveillance system with a thermal imaging system and a corresponding
    model to detect perpetrators, we could expect professional attackers to use black-box
    methods to find vulnerabilities! So, as defenders of this system, we would be
    wise to try the very same attack methods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们使用白盒方法来攻击、防御和评估模型的鲁棒性，但我们并不期望攻击者拥有这种级别的访问权限。我们只会使用白盒方法，因为我们完全访问了模型。在其他情况下，例如带有热成像系统和相应模型以检测犯罪者的银行监控系统，我们可能会预期专业攻击者使用黑盒方法来寻找漏洞！因此，作为该系统的防御者，我们明智的做法是尝试相同的攻击方法。
- en: 'The library we will use for adversarial robustness is called the **Adversarial
    Robustness Toolbox** (**ART**), and it’s supported by the **LF AI & Data Foundation**
    – the same folks that support other open-source projects such as AIX360 and AIF360,
    explored in *Chapter 11**, Bias Mitigation and Causal Inference Methods*. ART
    requires that attacked models are abstracted in an estimator or classifier, even
    if it’s a black-box one. We will use `KerasClassifier` for most of this chapter
    except for the last section, in which we will use `TensorFlowV2Classifier`. Initializing
    an ART classifier is simple. You must specify the `model`, and sometimes there
    are other required attributes. For `KerasClassifier`, all remaining attributes
    are optional, but it is recommended you use `clip_values` to specify the range
    of the features. Many attacks are input permutations, so knowing what input values
    are allowed or feasible is essential:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于对抗鲁棒性的库称为**对抗鲁棒性工具箱**（**ART**），它由**LF AI & 数据基金会**支持——这些人还支持其他开源项目，如 AIX360
    和 AIF360，这些项目在**第11章**中进行了探讨，即偏差缓解和因果推断方法。ART 要求攻击模型被抽象为估计器或分类器，即使它是黑盒的。在本章的大部分内容中，我们将使用
    `KerasClassifier`，但在最后一节中，我们将使用 `TensorFlowV2Classifier`。初始化ART分类器很简单。你必须指定 `model`，有时还有其他必需的属性。对于
    `KerasClassifier`，所有剩余的属性都是可选的，但建议你使用 `clip_values` 来指定特征的取值范围。许多攻击是输入排列，因此了解允许或可行的输入值是什么至关重要：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the preceding code, we also prepare two arrays with probabilities for the
    predicted class of the medium and small samples. It is entirely optional, but
    these assist in placing the predicted probability next to the predicted label
    when plotting some examples.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码中，我们还准备了两个数组，用于预测中等和较小样本的类别概率。这完全是可选的，但这些有助于在绘制一些示例时将预测概率放置在预测标签旁边。
- en: Fast gradient sign method attack
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 快速梯度符号法攻击
- en: One of the most popular attack methods is the **Fast Gradient Sign Method**
    (**FSGM** or **FGM**). As the name implies, it leverages a deep learning model’s
    gradient to find adversarial examples. It performs small perturbations on the
    pixels of the input image, either additions or subtractions. Which one to use
    depends on the gradient’s sign, which indicates what direction would increase
    or decrease the loss according to the pixel’s intensity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的攻击方法之一是**快速梯度符号法**（**FSGM** 或 **FGM**）。正如其名所示，它利用深度学习模型的梯度来寻找对抗性示例。它对输入图像的像素进行小的扰动，无论是加法还是减法。使用哪种方法取决于梯度的符号，这表明根据像素的强度，哪个方向会增加或减少损失。
- en: 'As with all ART attack methods, you first initialize it by providing the ART
    estimator or classifier. `FastGradientMethod` also requires an attack step size
    `eps`, which will condition the attack strength. Incidentally, `eps` stands for
    epsilon (![](img/B18406_13_001.png)), which represents error margins or infinitesimal
    approximation errors. A small step size will cause pixel intensity changes to
    be less visible, but it will also misclassify fewer examples. A larger step size
    will cause more examples to be misclassified with more visible changes:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有ART攻击方法一样，你首先通过提供ART估计器或分类器来初始化它。`FastGradientMethod` 还需要一个攻击步长 `eps`，这将决定攻击强度。顺便提一下，`eps`
    代表 epsilon (![](img/B18406_13_001.png))，它代表误差范围或无穷小近似误差。小的步长会导致像素强度变化不太明显，但它也会错误分类较少的示例。较大的步长会导致更多示例被错误分类，并且变化更明显：
- en: '[PRE13]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'After initializing, the next step is to `generate` the adversarial examples.
    The only required attribute is original examples (`X_test_mdsample`). Please note
    that FSGM can be targeted, so there’s an optional `targeted` attribute in the
    initialization, but you would also need to provide corresponding labels in the
    generation. This attack is untargeted because the attacker’s intent is to sabotage
    the model:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化后，下一步是`generate`对抗示例。唯一必需的属性是原始示例（`X_test_mdsample`）。请注意，FSGM可以是针对特定目标的，因此在初始化中有一个可选的`targeted`属性，但你还需要在生成时提供相应的标签。这种攻击是非针对特定目标的，因为攻击者的意图是破坏模型：
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Generating the adversarial examples with FSGM is quick, unlike other methods,
    hence the “Fast”!
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法相比，使用FSGM生成对抗示例非常快，因此称之为“快速”！
- en: 'Now, we will do two things in one swoop. First, evaluate the adversarial examples
    (`X_test_fgsm`) against our base classifier’s model (`base_classifier.model`)
    with `evaluate_multiclass_mdl`. Then we can employ `compare_image_predictions`
    to plot a grid of images, contrasting the randomly selected adversarial examples
    (`X_test_fgsm`) against the original ones (`X_test_mdsample`) and their corresponding
    predicted labels (`y_test_fgsm_pred`, `y_test_mdsample`) and probabilities (`y_test_fgsm_prob`,
    `y_test_mdsample_prob`). We customize the titles and limit the grid to four examples
    (`num_samples`). By default, `compare_image_predictions` only compares misclassifications
    but an optional attribute, `use_misclass`, can be set to `False` to compare correct
    classifications:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将一举两得。首先，使用`evaluate_multiclass_mdl`评估对抗示例（`X_test_fgsm`）对我们基础分类器模型（`base_classifier.model`）的模型。然后我们可以使用`compare_image_predictions`来绘制图像网格，对比随机选择的对抗示例（`X_test_fgsm`）与原始示例（`X_test_mdsample`）及其相应的预测标签（`y_test_fgsm_pred`，`y_test_mdsample`）和概率（`y_test_fgsm_prob`，`y_test_mdsample_prob`）。我们自定义标题并限制网格为四个示例（`num_samples`）。默认情况下，`compare_image_predictions`仅比较误分类，但可以通过将可选属性`use_misclass`设置为`False`来比较正确分类：
- en: '[PRE15]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The preceding code outputs a table first, which shows that the model has only
    44% accuracy with FSGM-attacked examples! And even though it wasn’t a targeted
    attack, it was most effective toward correctly masked faces. So hypothetically,
    if perpetrators managed to cause this level of signal distortion or interference,
    they would severely undermine the security companies’ ability to monitor mask
    compliance.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码首先输出一个表格，显示模型在FSGM攻击示例上的准确率仅为44%！尽管这不是针对特定目标的攻击，但它对正确遮挡的面部效果最为显著。所以假设，如果肇事者能够造成这种程度的信号扭曲或干扰，他们将严重削弱公司监控口罩合规性的能力。
- en: 'The code also outputs *Figure 13.5*, which shows some misclassifications caused
    by the FSGM attack. The attack pretty much evenly distributed noise throughout
    the images. It also shows that the image was only modified by a mean absolute
    error of 0.092, and since pixel values range between 0 and 1, this means 9.2%.
    If you were to calibrate attacks so that they are less detectable but still impactful,
    you must note that an `eps` of 0.1 causes a 9.2% mean absolute perturbation, which
    reduces accuracy to 44%:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 代码还输出了*图13.5*，该图显示了由FSGM攻击引起的一些误分类。攻击在图像中几乎均匀地分布了噪声。它还显示图像仅通过均方误差0.092进行了修改，由于像素值介于0和1之间，这意味着9.2%。如果你要校准攻击以使其更难检测但仍然具有影响力，你必须注意，`eps`为0.1会导致9.2%的平均绝对扰动，这会将准确性降低到44%：
- en: '![A collage of a person  Description automatically generated with low confidence](img/B18406_13_05.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![一个人物的拼贴画  描述由低置信度自动生成](img/B18406_13_05.png)'
- en: 'Figure 13.5: Plot comparing FSGM-attacked versus the original images for the
    base classifier'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.5：比较基础分类器FSGM攻击前后图像的图表
- en: Speaking of less detectable attacks, we will now learn about Carlini and Wagner
    attacks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 说到更难检测的攻击，我们现在将了解Carlini和Wagner攻击。
- en: Carlini and Wagner infinity norm attack
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Carlini和Wagner无穷范攻击
- en: 'In 2017, **Carlini and Wagner** (**C&W**) employed three **norm-based distance
    metrics**: ![](img/B18406_13_002.png), ![](img/B18406_13_003.png), and ![](img/B18406_13_004.png),
    measuring the difference between the original and adversarial example. In other
    papers, these metrics had already been discussed, including the FSGM one. The
    innovation introduced by C&W was how these metrics were leveraged, using a gradient
    descent-based optimization algorithm designed to approximate a loss function minima.
    Specifically, to avoid getting stuck at a local minimum, they use multiple starting
    points in the gradient descent. And so that the process “yields a valid image,”
    it evaluates three methods to box-constrain the optimization problem. In this
    case, we want to find an adversarial example where the distances between that
    example and the original image are minimal, while also remaining realistic.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年，**Carlini和Wagner**（**C&W**）采用了三种基于范数的距离度量：![img/B18406_13_002.png](img/B18406_13_002.png)，![img/B18406_13_003.png](img/B18406_13_003.png)，和![img/B18406_13_004.png](img/B18406_13_004.png)，测量原始样本与对抗样本之间的差异。在其他论文中，这些度量已经被讨论过，包括FSGM。C&W引入的创新是如何利用这些度量，使用基于梯度下降的优化算法来近似损失函数的最小值。具体来说，为了避免陷入局部最小值，他们在梯度下降中使用多个起始点。为了使过程“生成一个有效的图像”，它评估了三种方法来约束优化问题。在这种情况下，我们想要找到一个对抗样本，该样本与原始图像之间的距离是最小的，同时仍然保持现实性。
- en: 'All three C&W attacks (![](img/B18406_13_002.png), ![](img/B18406_13_003.png),
    and ![](img/B18406_13_004.png)) use the Adam optimizer to quickly converge. Their
    main difference is the distance metric, of which ![](img/B18406_13_004.png) is
    arguably the best one. It’s defined as follows:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的三种C&W攻击（![img/B18406_13_002.png](img/B18406_13_002.png)，![img/B18406_13_003.png](img/B18406_13_003.png)，和![img/B18406_13_004.png](img/B18406_13_004.png)）都使用Adam优化器快速收敛。它们的主要区别是距离度量，其中![img/B18406_13_004.png](img/B18406_13_004.png)可以说是最好的一个。它定义如下：
- en: '![](img/B18406_13_009.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![img/B18406_13_009.png](img/B18406_13_009.png)'
- en: And because it’s the maximum distance to any coordinate, you make sure that
    the adversarial example is not just “on average” minimally different but also
    not too different anywhere in the feature space. That’s what would make an attack
    less detectable!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它是到任何坐标的最大距离，你确保对抗样本不仅在“平均”上最小化差异，而且在特征空间的任何地方都不会有太大差异。这就是使攻击更难以检测的原因！
- en: 'Initializing C&W infinity norm attacks and generating adversarial examples
    with them is similar to FSGM. To initialize `CarliniLInfMethod`, we define optionally
    a `batch_size` (the default is `128`). Then, to `generate` an untargeted adversarial
    attack, the same applies as with FSGM. Only `X` is needed when untargeted, and
    `y` when targeted:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用C&W无穷范数攻击初始化和生成对抗样本与FSGM类似。要初始化`CarliniLInfMethod`，我们可以可选地定义一个`batch_size`（默认为`128`）。然后，为了`generate`一个非目标对抗攻击，与FSGM相同。在非目标攻击中只需要`X`，而在目标攻击中需要`y`：
- en: '[PRE16]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We will now evaluate the C&W adversarial examples (`X_test_cw`) just as we
    did with FSGM. It’s exactly the same code, only with `fsgm` replaced with `cw`
    and different titles in `compare_image_predictions`. Just as with FSGM, the following
    code will yield a classification report and grid of images (*Figure 13.6*):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将评估C&W对抗样本（`X_test_cw`），就像我们评估FSGM一样。代码完全相同，只是将`fsgm`替换为`cw`，并在`compare_image_predictions`中更改不同的标题。就像FSGM一样，以下代码将生成一个分类报告和图像网格（*图13.6*）：
- en: '[PRE17]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'As outputted by the preceding code, the C&W adversarial examples have a 92%
    accuracy with our base model. The drop is sufficient to render the model useless
    for its intended purpose. If the attacker disturbed a camera’s signal just enough,
    they could achieve the same results. And, as you can tell by *Figure 13.6*, the
    perturbation of 0.3% is tiny compared to FSGM, but it was sufficient to misclassify
    8%, including the four in the grid that seem apparent to the naked eye:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如前述代码输出，C&W对抗样本在我们的基础模型中具有92%的准确率。这种下降足以使模型对其预期用途变得无用。如果攻击者仅对摄像机的信号进行足够的干扰，他们就能达到相同的结果。而且，正如你从*图13.6*中可以看出，与FSGM相比，0.3%的扰动非常小，但它足以将8%的分类错误，包括网格中看起来明显的四个分类错误。
- en: '![A collage of a child  Description automatically generated with low confidence](img/B18406_13_06.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![一个孩子的拼贴，描述由低置信度自动生成](img/B18406_13_06.png)'
- en: 'Figure 13.6: Plot comparing C&W infinity norm-attacked versus the original
    images for the base classifier'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.6：比较基础分类器中C&W无穷范数攻击与原始图像的绘图
- en: Sometimes it doesn’t matter if an attack goes undetected or not. The point of
    it is to make a statement, and that’s what adversarial patches can do.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，攻击是否被检测到并不重要。重点是做出声明，这正是对抗补丁所能做到的。
- en: Targeted adversarial patch attack
  id: totrans-119
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标对抗补丁攻击
- en: '**Adversarial Patches** (**APs**) are a robust, universal, and targeted method.
    You generate a patch you can either superimpose on an image, or print and physically
    place in a scene to trick a classifier into ignoring everything else in the scene.
    It is designed to work under a wide variety of conditions and transformations.
    Unlike other adversarial example generation approaches, there’s no intention of
    camouflaging the attack because, essentially, you replace a detectable portion
    of the scene with the patch. The method works by leveraging a variant of **Expectation
    Over Transformation** (**EOT**), which trains images over transformations of a
    given patch on different locations of an image. What it learns is the patch that
    fools the classifier the most, given the training examples.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '**对抗性补丁**（**APs**）是一种鲁棒、通用且具有针对性的方法。你可以生成一个补丁，既可以叠加到图像上，也可以打印出来并物理放置在场景中以欺骗分类器忽略场景中的其他所有内容。它旨在在各种条件和变换下工作。与其他对抗性示例生成方法不同，没有意图隐藏攻击，因为本质上，你用补丁替换了场景中可检测的部分。该方法通过利用**期望变换**（**EOT**）的变体来工作，该变体在图像的不同位置对给定补丁的变换上进行图像训练。它所学习的是在训练示例中欺骗分类器最多的补丁。'
- en: 'This method requires more parameters and steps than FSGM and C&W. For starters,
    we will use `AdversarialPatchNumpy`, which is the variant that works with any
    neural network image or video classifier. There’s also one for TensorFlow v2,
    but our base classifier is a `KerasClassifier`. The first argument is the classifier
    (`base_classifier`), and the other ones we will define are optional but highly
    recommended. The scaling ranges `scale_min` and `scale_max` are particularly important
    because they define how big can patches be in relation to the images – in this
    case, we want to test no smaller than 40% and no larger than 70%. Besides that,
    it makes sense to define a target class (`target`). In this case, we want the
    patch to target the “Correct” class. For the `learning_rate` and max iterations
    (`max_iter`), we use the defaults but note that these can be tuned to improve
    patch adversarial effectiveness:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法比FSGM和C&W需要更多的参数和步骤。首先，我们将使用`AdversarialPatchNumpy`，这是可以与任何神经网络图像或视频分类器一起工作的变体。还有一个适用于TensorFlow
    v2的版本，但我们的基础分类器是`KerasClassifier`。第一个参数是分类器（`base_classifier`），我们将定义的其他参数是可选的，但强烈推荐。缩放范围`scale_min`和`scale_max`尤其重要，因为它们定义了补丁相对于图像的大小可以有多大——在这种情况下，我们想测试的最小值不小于40%，最大值不大于70%。除此之外，定义一个目标类别（`target`）也是有意义的。在这种情况下，我们希望补丁针对“正确”类别。对于`learning_rate`和最大迭代次数（`max_iter`），我们使用默认值，但请注意，这些可以调整以提高补丁对抗的有效性：
- en: '[PRE18]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We don’t want the patch generation algorithm to waste time testing patches
    everywhere in images, so we can direct this effort by using a Boolean mask. This
    mask tells it where it can center the patch. To make the mask, we start by creating
    an array of zeros, 128 × 128\. Then we place ones in the rectangular area between
    pixels 80–93 and 45–84, which loosely corresponds to cover the center of the mouth
    area in most of the images. Lastly, we expand the array’s dimensions so that it’s
    `(1, W, H)` and convert it to a Boolean. Then we can proceed to `generate` patches
    using the small-size test dataset samples and the mask:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不希望补丁生成算法在图像的每个地方都浪费时间测试补丁，因此我们可以通过使用布尔掩码来指导这种努力。这个掩码告诉它可以在哪里定位补丁。为了制作这个掩码，我们首先创建一个128
    × 128的零数组。然后我们在像素80–93和45–84之间的矩形区域内放置1，这大致对应于覆盖大多数图像中嘴巴的中心区域。最后，我们扩展数组的维度，使其变为`(1,
    W, H)`，并将其转换为布尔值。然后我们可以使用小尺寸测试数据集样本和掩码来继续`generate`补丁：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can now plot the patch with the following snippet:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用以下代码片段绘制补丁：
- en: '[PRE20]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The preceding code produced the image in *Figure 13.7*. As expected, it has
    plenty of the shades of blue found in masks. It also has bright red and yellow
    hues, mostly missing from training examples, which confuse the classifier:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码生成了*图13.7*中的图像。正如预期的那样，它包含了掩码中发现的许多蓝色阴影。它还包含明亮的红色和黄色色调，这些色调在训练示例中大多缺失，这会混淆分类器：
- en: '![Chart  Description automatically generated with low confidence](img/B18406_13_07.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成，置信度低](img/B18406_13_07.png)'
- en: 'Figure 13.7: AP generated image to misclassify as correctly masked'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.7：AP生成的图像，误分类为正确掩码
- en: 'Unlike other methods, `generate` didn’t produce adversarial examples but a
    single patch, which is an image we can then place on top of images to create adversarial
    examples. This task is performed with `apply_patch`, which takes the original
    examples `X_test_smsample` and a scale; we will use 55%. It is also recommended
    to use a `mask` that will make sure the patch is applied where it makes more sense
    – in this case, in the area around the mouth:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他方法不同，`generate`没有生成对抗样本，而是一个单独的补丁，这是一个我们可以放置在图像上以创建对抗样本的图像。这个任务是通过`apply_patch`完成的，它接受原始示例`X_test_smsample`和一个比例；我们将使用55%。还建议使用一个`mask`，以确保补丁被应用到更有意义的地方——在这种情况下，是嘴巴周围的区域：
- en: '[PRE21]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now it’s time to evaluate our attack and examine some misclassifications. We
    will do exactly as before and reuse the code that produced *Figure 13.5* and *Figure
    13.7*, except we replace the variables so that they have `ap` and a corresponding
    title:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候评估我们的攻击并检查一些误分类了。我们将像以前一样做，并重用生成*图13.5*和*图13.7*的代码，只是我们将变量替换为`ap`和相应的标题：
- en: '[PRE22]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code yields the accuracy result of our attack at 65%, which is
    quite good considering how few examples it was trained on. AP needs more data
    than other methods. Targeted attacks, in general, need more examples to understand
    how to best target one class. The preceding code also produced the grid of images
    in *Figure 13.8*, which demonstrates how, hypothetically, if people walked around
    holding a cardboard patch in front of their face, they could easily fool the model:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的代码给出了我们攻击的准确率结果为65%，考虑到它训练的样本数量很少，这个结果相当不错。与其它方法相比，AP需要更多的数据。一般来说，定向攻击需要更多的样本来理解如何最好地针对某一类。前面的代码还生成了*图13.8*中的图像网格，展示了假设人们如果在前额前拿着一张硬纸板，他们可以轻易地欺骗模型：
- en: '![A picture containing person, posing, different, same  Description automatically
    generated](img/B18406_13_08.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![包含人物、姿势、不同、相同 描述自动生成](img/B18406_13_08.png)'
- en: 'Figure 13.8: Plot comparing AP-attacked versus the original images for base
    classifier'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.8：比较AP攻击与原始图像的基础分类器的图表
- en: So far, we have studied three attack methods but haven’t yet tackled how to
    defend against these attacks. We will explore a couple of solutions next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了三种攻击方法，但还没有解决如何防御这些攻击的问题。接下来，我们将探讨一些解决方案。
- en: Defending against targeted attacks with preprocessing
  id: totrans-138
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用预处理防御定向攻击
- en: 'There are five broad categories of adversarial defenses:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 有五种广泛的对抗防御类别：
- en: '**Preprocessing**: changing the model’s inputs so that they are harder to attack.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理**：改变模型的输入，使其更难以攻击。'
- en: '**Training**: training a new robust model that is designed to overcome attacks.'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：训练一个新的健壮模型，该模型旨在克服攻击。'
- en: '**Detection**: detecting attacks. For instance, you can train a model to detect
    adversarial examples.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**检测**：检测攻击。例如，你可以训练一个模型来检测对抗样本。'
- en: '**Transformer**: modifying model architecture and training so that it’s more
    robust – this may include techniques such as distillation, input filters, neuron
    pruning, and unlearning.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Transformer**：修改模型架构和训练，使其更健壮——这可能包括蒸馏、输入过滤器、神经元剪枝和重新学习等技术。'
- en: '**Postprocessing**: changing model outputs to overcome production inference
    or model extraction attacks.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**后处理**：改变模型输出以克服生产推理或模型提取攻击。'
- en: 'Only the first four defenses work with evasion attacks, and in this chapter,
    we will only cover the first two: **preprocessing** and **adversarial training**.
    FGSM and C&W can be defended easily with either of these, but an AP is tougher
    to defend against, so it might require a stronger **detection** or **transformer**
    method.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 只有前四种防御可以与规避攻击一起工作，在本章中，我们只涵盖前两种：**预处理**和**对抗训练**。FGSM和C&W可以用这两种方法中的任何一种来防御，但AP更难防御，可能需要更强的**检测**或**Transformer**方法。
- en: Before we defend, we must create a targeted attack. We will employ **Projected
    Gradient Descent** (**PGD**), which is a strong attack very similar in output
    to FSGM – that is, it produces noisy images. We won’t explain PGD in detail here
    but what is important to note is, like FSGM, it is regarded as a **first-order
    adversary** because it leverages first-order information about a network (due
    to gradient descent). Also, experiments prove that robustness against PGD ensures
    robustness against any first-order adversary. Specifically, PGD is a strong attack,
    so it makes for conclusive benchmarks.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们进行防御之前，我们必须先发起有针对性的攻击。我们将使用**投影梯度下降法**（**PGD**），这是一种非常强大的攻击方法，其输出与FSGM非常相似——也就是说，它会产生噪声图像。在这里我们不会详细解释PGD，但重要的是要注意，就像FSGM一样，它被视为**一阶对抗者**，因为它利用了关于网络的一阶信息（由于梯度下降）。此外，实验证明，对PGD的鲁棒性确保了对任何一阶对抗者的鲁棒性。具体来说，PGD是一种强大的攻击，因此它为结论性的基准提供了依据。
- en: 'To create a targeted attack against the correctly masked class, it’s best that
    we only select examples that aren’t correctly masked (`x_test_notmasked`) and
    their corresponding labels (`y_test_notmasked`) and predicted probabilities (`y_test_notmasked_prob`).
    Then, we want to create an array with the class (`Correct`) that we want to generate
    adversarial examples for (`y_test_masked`):'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要对正确掩码的类别发起有针对性的攻击，最好只选择那些没有被正确掩码的示例（`x_test_notmasked`）、它们对应的标签（`y_test_notmasked`）和预测概率（`y_test_notmasked_prob`）。然后，我们想要创建一个包含我们想要生成对抗性示例的类别（`Correct`）的数组（`y_test_masked`）：
- en: '[PRE23]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We initialize `ProjectedGradientDescent` as we did with FSGM, except we will
    set the maximum perturbation (`eps`), attack step size (`eps_step`), maximum iterations
    (`max_iter`), and `targeted=True`. Precisely because it is targeted, we will set
    both `X` and `y`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`ProjectedGradientDescent`初始化与FSGM相同，除了我们将设置最大扰动（`eps`）、攻击步长（`eps_step`）、最大迭代次数（`max_iter`）和`targeted=True`。正是因为它是针对性的，所以我们将同时设置`X`和`y`：
- en: '[PRE24]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now let’s evaluate the PGD attack as we did before, but this time, let’s plot
    the confusion matrix (`plot_conf_matrix=True`):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们像之前一样评估PGD攻击，但这次，让我们绘制混淆矩阵（`plot_conf_matrix=True`）：
- en: '[PRE25]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![Chart  Description automatically generated](img/B18406_13_09.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图表描述自动生成](img/B18406_13_09.png)'
- en: 'Figure 13.9: Confusion matrix for PGD attacked examples evaluated against the
    base classifier'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.9：针对基础分类器评估的PGD攻击示例的混淆矩阵
- en: 'Next, let’s run `compare_image_prediction` to see some random misclassifications:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们运行`compare_image_prediction`来查看一些随机误分类：
- en: '[PRE26]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The preceding code plots the grid of images in *Figure 13.10*. The mean absolute
    perturbation is the highest we’ve seen so far at 14.7%, and all unmasked faces
    in the grid are classified as correctly masked:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在*图13.10*中绘制了图像网格。平均绝对扰动是我们迄今为止看到的最高值，达到14.7%，并且网格中所有未掩码的面部都被分类为正确掩码：
- en: '![A collage of a person  Description automatically generated with medium confidence](img/B18406_13_10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![一个人拼贴，描述自动生成，中等置信度](img/B18406_13_10.png)'
- en: 'Figure 13.10: Plot comparing PGD-attacked versus original images for the base
    classifier'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10：比较基础分类器的PGD攻击图像与原始图像的绘图
- en: The accuracy cannot get worse, and the images are grainy beyond repair. So how
    can we combat noise? If you recall, we have dealt with this problem before. In
    *Chapter 7*, *Visualizing Convolutional Neural Networks*, **SmoothGrad** improved
    saliency maps by averaging the gradients. It’s a different application but the
    same principle – just as with a human, a noisy saliency map is more challenging
    to interpret than a smooth one, and a grainy image is much more challenging for
    a model to interpret than a smooth one.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率不能变得更差，图像的颗粒度已经无法修复。那么我们如何对抗噪声呢？如果你还记得，我们之前已经处理过这个问题了。在*第7章*，*可视化卷积神经网络*中，**SmoothGrad**通过平均梯度改进了显著性图。这是一个不同的应用，但原理相同——就像人类一样，噪声显著性图比平滑的显著性图更难以解释，而颗粒图像比平滑图像对模型来说更难以解释。
- en: '**Spatial smoothing** is just a fancy way of saying blur! However, what’s novel
    about it being introduced as an adversarial defense method is that the proposed
    implementation (`SpatialSmoothing`) calls for using the median and not the mean
    in a sliding window. The `window_size` is configurable, and it is recommended
    to adjust it where it is most useful as a defense. Once the defense has been initialized,
    you plug in the adversarial examples (`X_test_pgd`). It will output spatially
    smoothed adversarial examples (`X_test_pgd_ss`):'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '**空间平滑**只是说模糊的一种花哨说法！然而，它作为对抗防御方法引入的新颖之处在于，所提出的实现（`SpatialSmoothing`）要求在滑动窗口中使用中位数而不是平均值。`window_size`是可配置的，建议在最有用的防御位置进行调整。一旦防御初始化，你就可以插入对抗示例（`X_test_pgd`）。它将输出空间平滑的对抗示例（`X_test_pgd_ss`）：'
- en: '[PRE27]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can take the blurred adversarial examples produced and evaluate them
    as we did before – first, with `evaluate_multiclass_mdl` to get predicted labels
    (`y_test_pgd_ss_pred`) and probabilities (`y_test_pgd_ss_prob`) and the output
    of some predictive performance metrics. With `compare_image_predictions` to plot
    a grid of images, let’s use `use_misclass=False` to compare properly classified
    images – in other words, the adversarial examples that were defended successfully:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将产生的模糊对抗示例评估如前所述——首先，使用`evaluate_multiclass_mdl`获取预测标签（`y_test_pgd_ss_pred`）和概率（`y_test_pgd_ss_prob`），以及一些预测性能指标输出。使用`compare_image_predictions`绘制图像网格，让我们使用`use_misclass=False`来正确分类图像——换句话说，就是成功防御的对抗示例：
- en: '[PRE28]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The preceding code yields an accuracy of 54%, which is much better than 0% before
    the spatial smoothing defense. It also produces *Figure 13.11*, which demonstrates
    how blur effectively thwarted the PGD attack. It even halved the mean absolute
    perturbation!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码得到54%的准确率，这比空间平滑防御之前的0%要好得多。它还生成了*图13.11*，展示了模糊如何有效地阻止PGD攻击。它甚至将平均绝对扰动减半！
- en: '![A collage of people  Description automatically generated with low confidence](img/B18406_13_11.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![一群人的拼贴，描述自动生成，置信度低](img/B18406_13_11.png)'
- en: 'Figure 13.11: Plot comparing spatially smoothed PGD-attacked images versus
    the original images for the base classifier'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11：比较空间平滑PGD攻击图像与基础分类器原始图像的图表
- en: 'Next, we will try another defense method in our toolbox: adversarial training!'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在我们的工具箱中尝试另一种防御方法：对抗训练！
- en: Shielding against any evasion attack by adversarial training of a robust classifier
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过对鲁棒分类器进行对抗训练来抵御任何逃避攻击
- en: In *Chapter 7*, *Visualizing Convolutional Neural Networks*, we identified a
    garbage image classifier that would likely perform poorly in the intended environment
    of a municipal recycling plant. The abysmal performance on out-of-sample data
    was due to the classifier being trained on a large variety of publicly available
    images that don’t match the expected conditions, or the characteristics of materials
    that are processed by a recycling plant. The chapter’s conclusion called for training
    a network with images that represent their intended environment to make for a
    more robust model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在*第7章*，*可视化卷积神经网络*中，我们确定了一个垃圾图像分类器，它很可能在市政回收厂预期的环境中表现不佳。在样本外数据上的糟糕表现是由于分类器是在大量公开可用的图像上训练的，这些图像与预期的条件不匹配，或者与回收厂处理的材料的特征不匹配。章节的结论呼吁使用代表其预期环境的图像来训练网络，以创建一个更鲁棒的模型。
- en: For model robustness, training data variety is critical, but only if it represents
    the intended environment. In statistical terms, it’s a question of using samples
    for training that accurately depict the population so that a model learns to classify
    them correctly. For adversarial robustness, the same principles apply. If you
    augment data to include plausible examples of adversarial attacks, the model will
    learn to classify them. That’s what adversarial training is in a nutshell.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 为了模型的鲁棒性，训练数据的多样性至关重要，但前提是它能够代表预期的环境。在统计学的术语中，这是一个关于使用样本进行训练的问题，这些样本能够准确描述总体，从而使模型学会正确地分类它们。对于对抗鲁棒性，同样的原则适用。如果你增强数据以包括可能的对抗攻击示例，模型将学会对它们进行分类。这就是对抗训练的本质。
- en: Machine learning researchers in the adversarial robustness field suggest this
    form of defense is very effective against any kind of evasion attack, essentially
    shielding it. That being said, it’s not impervious. Its effectiveness is contingent
    on using the right kind of adversarial examples in training, the optimal hyperparameters,
    and so forth. There are some guidelines outlined by researchers, such as increasing
    the number of neurons in the hidden layers and using PGD or BIM to produce adversarial
    examples for the training. **BIM** stands for **Basic Iterative Method**. It’s
    like FSGM but not fast because it iterates to approximate the best adversarial
    example within a ![](img/B18406_13_010.png)-neighborhood for the original image.
    The `eps` attribute bounds this neighborhood.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗鲁棒性领域的机器学习研究人员建议这种防御形式对任何类型的规避攻击都非常有效，本质上可以保护它。但话虽如此，它并非坚不可摧。其有效性取决于在训练中使用正确类型的对抗样本、最优的超参数等等。研究人员概述了一些指导方针，例如增加隐藏层中的神经元数量，并使用PGD或BIM生成训练对抗样本。**BIM**代表**基本迭代方法**。它类似于FSGM，但速度不快，因为它通过迭代来逼近原始图像在![](img/B18406_13_010.png)-邻域内的最佳对抗样本。`eps`属性限制了这一邻域。
- en: 'Training a robust model can be very resource-intensive. It is not required
    because we can download one already trained for us, but it’s important to understand
    how to perform this with ART. We will explain these steps to give the option of
    completing the model training with ART. Otherwise, just skip the steps and download
    the trained model. The `robust_model` is very much like the `base_model` except
    we use equal-sized filters in the four convolutional (`Conv2D`) layers. We do
    this to decrease complexity to counter the complexity we add by quadrupling the
    neurons in the first hidden (`Dense`) layer, as suggested by the machine learning
    researchers:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个鲁棒模型可能非常耗费资源。虽然我们也可以下载一个已经训练好的，但这很重要，要理解如何使用ART来完成这一过程。我们将解释这些步骤，以便有选择地使用ART完成模型训练。否则，只需跳过这些步骤并下载训练好的模型。`robust_model`与`base_model`非常相似，除了我们在四个卷积(`Conv2D`)层中使用等大小的过滤器。我们这样做是为了降低复杂性，以抵消我们通过将第一隐藏(`Dense`)层中的神经元数量翻倍所增加的复杂性，正如机器学习研究人员所建议的：
- en: '[PRE29]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The `summary()` in the preceding code outputs the following. You can see that
    trainable parameters total around 3.6 million – similar to the base model:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 前面代码中的`summary()`输出了以下内容。你可以看到可训练参数总数约为360万，与基础模型相似：
- en: '[PRE30]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Next, we can adversarially train the model by first initializing a new `KerasClassifier`
    with the `robust_model`. Then, we initialize a `BasicIterativeMethod` attack on
    this classifier. Lastly, we initialize `AdversarialTrainer` with the `robust_classifier`
    and the BIM attack and `fit` it. Please note that we saved the BIM attack into
    a variable called `attacks` because this could be a list of ART attacks instead
    of a single one. Also, note that `AdversarialTrainer` has an attribute called
    `ratio`. This attribute determines what percentage of the training examples are
    adversarial examples. This percentage dramatically impacts the effectiveness of
    adversarial attacks. If it’s too low, it might not perform well with adversarial
    examples, and if it’s too high, it might perform less effectively with non-adversarial
    examples. If we run the `trainer`, it will likely take many hours to complete,
    so don’t get alarmed:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以通过首先使用`robust_model`初始化一个新的`KerasClassifier`来对抗性地训练模型。然后，我们初始化这个分类器上的`BasicIterativeMethod`攻击。最后，我们使用`robust_classifier`和BIM攻击初始化`AdversarialTrainer`并对其进行`fit`。请注意，我们将BIM攻击保存到了一个名为`attacks`的变量中，因为这个变量可能是一系列ART攻击而不是单一的一个。另外，请注意`AdversarialTrainer`有一个名为`ratio`的属性。这个属性决定了训练样本中有多少是对抗样本。这个百分比会极大地影响对抗攻击的有效性。如果它太低，可能无法很好地处理对抗样本，如果太高，可能在与非对抗样本的交互中效果较差。如果我们运行`trainer`，它可能需要很多小时才能完成，所以请不要感到惊讶：
- en: '[PRE31]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'If you didn’t train the `robust_classifier`, you can download a pretrained
    `robust_model` and initialize the `robust_classifier` with it like this:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有训练`robust_classifier`，你可以下载一个预训练的`robust_model`，并像这样用它来初始化`robust_classifier`：
- en: '[PRE32]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, let’s evaluate the `robust_classifier` against the original test dataset
    using `evaluate_multiclass_mdl`. We set `plot_conf_matrix=True` to see the confusion
    matrix:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用`evaluate_multiclass_mdl`来评估`robust_classifier`对原始测试数据集的性能。我们将`plot_conf_matrix`设置为`True`以查看混淆矩阵：
- en: '[PRE33]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The preceding code outputs the confusion matrix and performance metrics in
    *Figure 13.12*. It’s 1.8% less accurate than the base classifier. Most of the
    misclassifications are with correctly masked faces getting classified as incorrectly
    masked. There’s certainly a trade-off when choosing a 50% adversarial example
    ratio, or perhaps we can tune the hyperparameters or the model architecture to
    improve this:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码输出了*图13.12*中的混淆矩阵和性能指标。它的准确率比基础分类器低1.8%。大多数错误分类都是将正确遮挡的面部错误地分类为错误遮挡。在选择50%的对抗示例比例时，肯定存在权衡，或者我们可以调整超参数或模型架构来改进这一点：
- en: '![Chart, treemap chart  Description automatically generated](img/Image15514.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图表，树状图图表  自动生成的描述](img/Image15514.png)'
- en: 'Figure 13.12: Robust classifier confusion metrics and performance metrics'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.12：鲁棒分类器的混淆度指标和性能指标
- en: 'Let’s see how the robust model fares against adversarial attacks. Let’s use
    `FastGradientMethod` again, but this time, replace `base_classifier` with `robust_classifier`:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看鲁棒模型在对抗攻击中的表现。我们再次使用`FastGradientMethod`，但这次，将`base_classifier`替换为`robust_classifier`：
- en: '[PRE34]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Next, we can employ `evaluate_multiclass_mdl` and `compare_image_predictions`
    to measure and observe the effectiveness of our attack, but this time against
    the `robust_classifier`:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用`evaluate_multiclass_mdl`和`compare_image_predictions`来衡量和观察我们攻击的有效性，但这次针对的是`robust_classifier`：
- en: '[PRE35]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![A collage of a baby  Description automatically generated with low confidence](img/B18406_13_13.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![婴儿拼贴  使用低置信度自动生成的描述](img/B18406_13_13.png)'
- en: 'Figure 13.13: Plot comparing FSGM-attacked versus the original images for the
    robust classifier'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.13：比较鲁棒分类器被FSGM攻击与原始图像的图表
- en: So far, we have evaluated the robustness of models but only against one attack
    strength, without factoring in possible defenses, thus evaluating its robustness.
    In the next section, we will study a method that does this.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只评估了模型的鲁棒性，但只针对一种攻击强度，没有考虑可能的防御措施，因此评估了其鲁棒性。在下一节中，我们将研究一种实现这一目标的方法。
- en: Evaluating adversarial robustness
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估对抗鲁棒性
- en: It’s necessary to test your systems in any engineering endeavor to see how vulnerable
    they are to attacks or accidental failures. However, security is a domain where
    you must stress-test your solutions to ascertain what level of attacks are needed
    to make your system break down beyond an acceptable threshold. Furthermore, figuring
    out what level of defense is needed to curtail an attack is useful information
    too.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何工程实践中测试你的系统以了解它们对攻击或意外故障的脆弱性是必要的。然而，安全是一个你必须对你的解决方案进行压力测试的领域，以确定需要多少级别的攻击才能使你的系统崩溃超过可接受的阈值。此外，弄清楚需要多少级别的防御来遏制攻击也是非常有用的信息。
- en: Comparing model robustness with attack strength
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较模型鲁棒性与攻击强度
- en: We now have two classifiers we can compare against an equally strengthened attack,
    and we try different attack strengths to see how they fare across all of them.
    We will use FSGM because it’s fast, but you could use any method!
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有两个分类器可以与同等强度的攻击进行比较，我们尝试不同的攻击强度，看看它们在所有这些攻击中的表现如何。我们将使用FSGM，因为它速度快，但你可以使用任何方法！
- en: 'The first attack strength we can assess is no attack strength. In other words,
    what is the classification accuracy against the test dataset with no attack? We
    already had stored the predicted labels for both the base (`y_test_pred`) and
    robust (`y_test_robust_pred`) models, so this is easy to obtain with scikit-learn’s
    `accuracy_score` metric:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以评估的第一个攻击强度是没有攻击强度。换句话说，没有攻击的情况下，对测试数据集的分类准确率是多少？我们已经有存储了基础模型（`y_test_pred`）和鲁棒模型（`y_test_robust_pred`）的预测标签，所以这可以通过scikit-learn的`accuracy_score`指标轻松获得：
- en: '[PRE36]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We can now iterate across a range of attack strengths (`eps_range`) between
    0.01 and 0.9\. Using `linspace`, we can generate 9 values between 0.01 and 0.09
    and 9 values between 0.1 and 0.9, and `concatenate` them into a single array.
    We will test attacks for these 18 `eps` values by `for`-looping through all of
    them, attacking each model, and retrieving the post-attack accuracies with `evaluate`.
    The respective accuracies are appended to two lists (`accuracy_base`, `accuracy_robust`).
    And after the `for` loop, we prepend 0 to the `eps_range` to account for the accuracies
    prior to any attacks:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以在0.01和0.9之间迭代一系列攻击强度（`eps_range`）。使用`linspace`，我们可以生成0.01和0.09之间的9个值和0.1和0.9之间的9个值，并将它们`concatenate`成一个单一的数组。我们将通过`for`循环测试这18个`eps`值的所有攻击，攻击每个模型，并使用`evaluate`检索攻击后的准确度。相应的准确度被附加到两个列表（`accuracy_base`，`accuracy_robust`）中。在`for`循环之后，我们将0添加到`eps_range`中，以考虑任何攻击之前的准确度：
- en: '[PRE37]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now, we can plot the accuracies for both classifiers across all attack strengths
    with the following code:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用以下代码绘制两个分类器在所有攻击强度下的准确度图：
- en: '[PRE38]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The preceding code generates *Figure 13.14*, which demonstrates that the robust
    model performs better between attack strengths of 0.02 and 0.3 but then does consistently
    about 10% worse:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码生成了**图13.14**，该图展示了鲁棒模型在攻击强度为0.02和0.3之间表现更好，但之后则始终比基准模型差大约10%：
- en: '![Chart, line chart  Description automatically generated](img/Image15613.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![图表，折线图  自动生成描述](img/Image15613.png)'
- en: 'Figure 13.14: Accuracy measured for the robust and base classifiers at different
    FSGM attack strengths'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.14：在多种FSGM攻击强度下对鲁棒和基准分类器的准确度进行测量
- en: One thing that *Figure 13.14* fails to account for is defenses. For example,
    if hospital cameras were constantly jammed or tampered with, the security company
    would be remiss not to defend their models. The easiest way to do so for this
    kind of attack is with some sort of smoothing.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '**图13.14**未能考虑的是防御措施。例如，如果医院摄像头持续受到干扰或篡改，安全公司不保护他们的模型将是失职的。对于这种攻击，最简单的方法是使用某种平滑技术。'
- en: Adversarial training also produces an empirically robust classifier that you
    cannot guarantee will work under certain pre-defined circumstances, which is why
    there’s a need for certifiable defenses.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性训练也产生了一个经验上鲁棒的分类器，你不能保证它在某些预定义的情况下会工作，这就是为什么需要可验证的防御措施。
- en: Mission accomplished
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 任务完成
- en: The mission was to perform some adversarial robustness tests on their face mask
    model to determine if hospital visitors and staff can evade mandatory mask compliance.
    The base model performed very poorly on many evasion attacks, from the most aggressive
    to the most subtle.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 任务是执行他们面部口罩模型的一些对抗性鲁棒性测试，以确定医院访客和员工是否可以规避强制佩戴口罩的规定。基准模型在许多规避攻击中表现非常糟糕，从最激进的到最微妙的。
- en: You also looked at possible defenses to these attacks, such as spatial smoothing
    and adversarial retraining. And then, you explored ways to evaluate the robustness
    of your proposed defenses. You can now provide an end-to-end framework to defend
    against this kind of attack. That being said, what you did was only a proof of
    concept.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 你还研究了这些攻击的可能防御措施，例如空间平滑和对抗性重新训练。然后，你探索了评估你提出的防御措施鲁棒性的方法。你现在可以提供一个端到端框架来防御这种攻击。话虽如此，你所做的一切只是一个概念验证。
- en: Now, you can propose training a certifiably robust model against the attacks
    the hospitals expect to encounter the most. But first, you need the ingredients
    for a generally robust model. To this end, you will need to take all 210,000 images
    in the original dataset, make many variations on mask colors and types with them,
    and augment them even further with reasonable brightness, shear, and rotation
    transformations. Lastly, the robust model needs to be trained with several kinds
    of attacks, including several kinds of APs. These are important because they mimic
    the most common compliance evasion behavior of concealing faces with body parts
    or clothing items.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以提出训练一个可验证的鲁棒模型来对抗医院预期遇到的最常见的攻击。但首先，你需要一个一般鲁棒模型的成分。为此，你需要使用原始数据集中的所有210,000张图片，对它们进行许多关于遮罩颜色和类型的变体，并使用合理的亮度、剪切和旋转变换进一步增强。最后，鲁棒模型需要用几种攻击进行训练，包括几种AP攻击。这些攻击很重要，因为它们模仿了最常见的合规规避行为，即用身体部位或衣物物品隐藏面部。
- en: Summary
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: After reading this chapter, you should understand how attacks can be perpetrated
    on machine learning models and evasion attacks in particular. You should know
    how to perform FSGM, BIM, PGD, C&W, and AP attacks, as well as how to defend against
    them with spatial smoothing and adversarial training. Last but not least, you
    know how to evaluate adversarial robustness.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读本章后，您应该了解如何对机器学习模型进行攻击，特别是逃避攻击。您应该知道如何执行 FSGM、BIM、PGD、C&W 和 AP 攻击，以及如何通过空间平滑和对抗性训练来防御它们。最后但同样重要的是，您应该了解如何评估对抗性鲁棒性。
- en: The next chapter is the last one, and it outlines some ideas on what’s next
    for machine learning interpretation.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章是最后一章，它概述了关于机器学习解释未来发展的想法。
- en: Dataset sources
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集来源
- en: 'Adnane Cabani, Karim Hammoudi, Halim Benhabiles, and Mahmoud Melkemi, 2020,
    *MaskedFace-Net - A dataset of correctly/incorrectly masked face images in the
    context of COVID-19*, Smart Health, ISSN 2352–6483, Elsevier: [https://doi.org/10.1016/j.smhl.2020.100144](https://doi.org/10.1016/j.smhl.2020.100144)
    (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adnane Cabani，Karim Hammoudi，Halim Benhabiles 和 Mahmoud Melkemi，2020，*MaskedFace-Net
    - 在 COVID-19 背景下正确/错误佩戴口罩的人脸图像数据集*，Smart Health，ISSN 2352–6483，Elsevier：[https://doi.org/10.1016/j.smhl.2020.100144](https://doi.org/10.1016/j.smhl.2020.100144)（由
    NVIDIA 公司提供的 Creative Commons BY-NC-SA 4.0 许可证）
- en: 'Karras, T., Laine, S., and Aila, T., 2019, *A Style-Based Generator Architecture
    for Generative Adversarial Networks*. 2019 IEEE/CVF Conference on Computer Vision
    and Pattern Recognition (CVPR), 4396–4405: [https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)
    (Creative Commons BY-NC-SA 4.0 license by NVIDIA Corporation)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Karras, T.，Laine, S.，和 Aila, T.，2019，*用于生成对抗网络的基于风格的生成器架构*。2019 IEEE/CVF 计算机视觉与模式识别会议（CVPR），4396–4405：[https://arxiv.org/abs/1812.04948](https://arxiv.org/abs/1812.04948)（由
    NVIDIA 公司提供的 Creative Commons BY-NC-SA 4.0 许可证）
- en: Further reading
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'Polyakov, A., 2019, Aug 6, *How to attack Machine Learning (Evasion, Poisoning,
    Inference, Trojans, Backdoors)* [blog post]: [https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c](https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c)'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Polyakov, A.，2019 年 8 月 6 日，*如何攻击机器学习（逃避、投毒、推理、木马、后门）* [博客文章]：[https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c](https://towardsdatascience.com/how-to-attack-machine-learning-evasion-poisoning-inference-trojans-backdoors-a7cb5832595c)
- en: 'Carlini, N., & Wagner, D., 2017, *Towards Evaluating the Robustness of Neural
    Networks*. 2017 IEEE Symposium on Security and Privacy (SP), 39–57: [https://arxiv.org/abs/1608.04644](https://arxiv.org/abs/1608.04644)'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Carlini, N.，& Wagner, D.，2017，*迈向评估神经网络的鲁棒性*。2017 IEEE 安全与隐私研讨会（SP），39–57:
    [https://arxiv.org/abs/1608.04644](https://arxiv.org/abs/1608.04644)'
- en: 'Brown, T., Mané, D., Roy, A., Abadi, M., and Gilmer, J., 2017, *Adversarial
    Patch*. ArXiv: [https://arxiv.org/abs/1712.09665](https://arxiv.org/abs/1712.09665)'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Brown, T.，Mané, D.，Roy, A.，Abadi, M.，和 Gilmer, J.，2017，*对抗性补丁*。ArXiv：[https://arxiv.org/abs/1712.09665](https://arxiv.org/abs/1712.09665)
- en: Learn more on Discord
  id: totrans-222
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在 Discord 上了解更多
- en: 'To join the Discord community for this book – where you can share feedback,
    ask the author questions, and learn about new releases – follow the QR code below:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要加入这本书的 Discord 社区——在那里您可以分享反馈、向作者提问，并了解新书发布——请扫描下面的二维码：
- en: '[https://packt.link/inml](Chapter_13.xhtml)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/inml](Chapter_13.xhtml)'
- en: '![](img/QR_Code107161072033138125.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![二维码](img/QR_Code107161072033138125.png)'
