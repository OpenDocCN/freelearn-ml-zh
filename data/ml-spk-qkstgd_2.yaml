- en: Setting Up a Local Development Environment
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置本地开发环境
- en: 'In this chapter, we will install, configure, and deploy a local analytical
    development environment by provisioning a self-contained single-node cluster that
    will allow us to do the following:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过配置一个自包含的单节点集群来安装、配置和部署一个本地分析开发环境，这将使我们能够执行以下操作：
- en: Prototype and develop machine learning models and pipelines in Python
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中原型设计和开发机器学习模型和管道
- en: Demonstrate the functionality and usage of Apache Spark's machine learning library,
    `MLlib`, via the Spark Python API (PySpark)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Spark Python API (PySpark) 展示 Apache Spark 的机器学习库 `MLlib` 的功能和用法
- en: Develop and test machine learning models on a single-node cluster using small
    sample datasets, and thereafter scale up to multi-node clusters processing much
    larger datasets with little or no code changes required
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用小型样本数据集在单节点集群上开发和测试机器学习模型，然后扩展到多节点集群，处理更大的数据集，而无需或只需进行少量代码更改
- en: 'Our single-node cluster will host the following technologies:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的单节点集群将托管以下技术：
- en: '**Operating system**: CentOS Linux 7'
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**操作系统**：CentOS Linux 7'
- en: '[https://www.centos.org/download/](https://www.centos.org/download/)'
  id: totrans-7
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://www.centos.org/download/](https://www.centos.org/download/)'
- en: '**General Purpose Programming Languages**:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用编程语言**：'
- en: Java SE Development Kit (JDK) 8 (8u181)
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Java SE 开发工具包 (JDK) 8 (8u181)
- en: '[https://www.oracle.com/technetwork/java/javase/downloads/index.html](https://www.oracle.com/technetwork/java/javase/downloads/index.html)'
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://www.oracle.com/technetwork/java/javase/downloads/index.html](https://www.oracle.com/technetwork/java/javase/downloads/index.html)'
- en: Scala 2.11.x (2.11.12) [https://www.scala-lang.org/download/all.html](https://www.scala-lang.org/download/all.html)
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scala 2.11.x (2.11.12) [https://www.scala-lang.org/download/all.html](https://www.scala-lang.org/download/all.html)
- en: Python 3.x (3.6) via Anaconda 5.x (5.2) Python Distribution
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Anaconda 5.x (5.2) Python 发行版使用 Python 3.x (3.6)
- en: '[https://www.anaconda.com/download/](https://www.anaconda.com/download/)'
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://www.anaconda.com/download/](https://www.anaconda.com/download/)'
- en: '**General purpose distributed processing engine**: Apache Spark 2.3.x (2.3.2)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**通用分布式处理引擎**：Apache Spark 2.3.x (2.3.2)'
- en: '[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)'
  id: totrans-15
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)'
- en: '**Distributed streaming platform**: Apache Kafka 2 (2.0.0)'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分布式流平台**：Apache Kafka 2 (2.0.0)'
- en: '[https://kafka.apache.org/downloads](https://kafka.apache.org/downloads)'
  id: totrans-17
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[https://kafka.apache.org/downloads](https://kafka.apache.org/downloads)'
- en: CentOS Linux 7 virtual machine
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: CentOS Linux 7 虚拟机
- en: 'First of all, we will assume that you have access to a physical or virtual
    machine provisioned with the CentOS 7 operating system. CentOS 7 is a free Linux
    distribution derived from **Red Hat Enterprise Linux** (**RHEL**). It is commonly
    used, along with its licensed upstream parent, RHEL, as the operating system of
    choice for Linux-based servers, since it is stable and backed by a large active
    community with detailed documentation. All the commands that we will use to install
    the various technologies listed previously will be Linux shell commands to be
    executed on a single CentOS 7 (or RHEL) machine, whether physical or virtual.
    If you do not have access to a CentOS 7 machine, then there are quite a few options
    available to provision a CentOS 7 virtual machine:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将假设您有权访问一个配置了 CentOS 7 操作系统的物理或虚拟机。CentOS 7 是一个由 **Red Hat Enterprise Linux**
    （**RHEL**）派生的免费 Linux 发行版。它通常与它的授权上游父级 RHEL 一起使用，作为基于 Linux 服务器的首选操作系统，因为它稳定，并得到一个庞大活跃社区的支持，拥有详细的文档。我们将使用所有这些命令来安装之前列出的各种技术，这些命令将是需要在单个
    CentOS 7（或 RHEL）机器上执行的 Linux shell 命令，无论是物理机还是虚拟机。如果您没有访问 CentOS 7 机器，那么有许多选项可以配置
    CentOS 7 虚拟机：
- en: 'Cloud computing platforms such as **Amazon Web Services** (**AWS**), **Microsoft
    Azure**, and the **Google Cloud Platform** (**GCP**) all allow you to stand up
    virtual machines using a **Pay-As-You-Go** (**PAYG**) pricing model. Often, the
    major cloud computing platforms also provide a free tier for new users with a
    small amount of free capacity in order to trial their services. To learn more
    about these major cloud computing platforms, please visit the following websites:'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 云计算平台，如 **Amazon Web Services** （**AWS**）、**Microsoft Azure** 和 **Google Cloud
    Platform** （**GCP**），都允许您使用 **按量付费** （**PAYG**）定价模式来启动虚拟机。通常，主要的云计算平台还为新手用户提供免费层，包含一定量的免费容量，以便试用他们的服务。要了解更多关于这些主要云计算平台的信息，请访问以下网站：
- en: 'AWS: [https://aws.amazon.com/](https://aws.amazon.com/)'
  id: totrans-21
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'AWS: [https://aws.amazon.com/](https://aws.amazon.com/)'
- en: 'Microsoft Azure: [https://azure.microsoft.com](https://azure.microsoft.com)'
  id: totrans-22
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Microsoft Azure: [https://azure.microsoft.com](https://azure.microsoft.com)'
- en: 'GCP: [https://cloud.google.com/](https://cloud.google.com/)'
  id: totrans-23
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'GCP: [https://cloud.google.com/](https://cloud.google.com/)'
- en: '**Virtual Private Server** (**VPS**) hosting companies, such as **Linode**
    and **Digital Ocean**, also provide the ability to provision low-cost CentOS virtual
    machines. These VPS providers often employ a much simpler pricing model consisting
    only of virtual machines of various specifications. To learn more about these
    major VPS providers, please visit the following websites:'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚拟专用服务器**（**VPS**）托管公司，如**Linode**和**Digital Ocean**，也提供配置低成本CentOS虚拟机的能力。这些VPS提供商通常采用一个更简单的定价模型，仅包括各种规格的虚拟机。要了解更多关于这些主要VPS提供商的信息，请访问以下网站：'
- en: 'Linode: [https://www.linode.com/](https://www.linode.com/)'
  id: totrans-25
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Linode: [https://www.linode.com/](https://www.linode.com/)'
- en: 'Digital Ocean: [https://www.digitalocean.com/](https://www.digitalocean.com/)'
  id: totrans-26
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Digital Ocean: [https://www.digitalocean.com/](https://www.digitalocean.com/)'
- en: 'A common and free option, particularly for local development environments used
    for prototyping and testing, is to provision your own virtual machine hosted by
    your personal physical desktop or laptop. Virtualization software such as **Oracle
    VirtualBox** (open source) and **VMWare Workstation Player** (free for personal
    use) allow you to set up and run virtual machines on your own personal physical
    devices. To learn more about these virtualization software services, please visit
    the following websites:'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个常见且免费的选项，尤其是在用于原型设计和测试的本地开发环境中，是配置由您个人物理桌面或笔记本电脑托管的虚拟机。如**Oracle VirtualBox**（开源）和**VMWare
    Workstation Player**（个人使用免费）这样的虚拟化软件允许您在自己的个人物理设备上设置和运行虚拟机。要了解更多关于这些虚拟化软件服务的信息，请访问以下网站：
- en: 'Oracle VirtualBox: [https://www.virtualbox.org/](https://www.virtualbox.org/)'
  id: totrans-28
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Oracle VirtualBox: [https://www.virtualbox.org/](https://www.virtualbox.org/)'
- en: 'VMWare Workstation Player: [https://www.vmware.com/](https://www.vmware.com/uk/products/workstation-player.html)'
  id: totrans-29
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'VMWare Workstation Player: [https://www.vmware.com/](https://www.vmware.com/uk/products/workstation-player.html)'
- en: 'For the remainder of this chapter, we will assume that you have provisioned
    a 64-bit CentOS 7 machine and that you have either direct desktop access to it,
    or network access to it via both HTTP and SSH protocols. Though the specifications
    of your virtual machine may differ, we would recommend the following minimum virtual
    hardware requirements in order to efficiently run the examples in the remainder
    of this book:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章剩余部分，我们将假设您已经配置了一台64位的CentOS 7机器，并且您可以直接通过桌面访问它，或者通过HTTP和SSH协议通过网络访问它。尽管您的虚拟机规格可能不同，但我们仍建议以下最低虚拟硬件要求，以便有效地运行本书剩余部分中的示例：
- en: 'Operating System: CentOS 7 (minimum installation)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作系统：CentOS 7（最小安装）
- en: 'Virtual CPUs: 4'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟CPU：4
- en: 'Memory: 8 GB'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存：8 GB
- en: 'Storage: 20 GB'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储：20 GB
- en: 'In our case, our virtual machine has the following network properties and will
    be referenced hereafter as such. These will be different for your virtual machine:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们的虚拟机具有以下网络属性，并将在此之后被如此引用。这些属性对于您的虚拟机可能会有所不同：
- en: 'Static IP address: `192.168.56.10`'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 静态IP地址：`192.168.56.10`
- en: 'Netmask: `255.255.255.0`'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子网掩码：`255.255.255.0`
- en: '**Fully qualified domain name** (**FQDN**): `packt.dev.keisan.io`'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全限定域名**（**FQDN**）：`packt.dev.keisan.io`'
- en: Note that the security of your virtual machine, and the subsequent software
    services that it hosts, including big data technologies, is beyond the scope of
    this book. Should you wish to learn more about how to harden your base operating
    system and common software services in order to protect against external attacks,
    we recommend visiting [https://www.cisecurity.org/cis-benchmarks/](https://www.cisecurity.org/cis-benchmarks/)
    as well as the individual software service websites themselves, such as [https://spark.apache.org/docs/latest/security.html](https://spark.apache.org/docs/latest/security.html)
    for Apache Spark security.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，您的虚拟机的安全性以及它所承载的后续软件服务（包括大数据技术）超出了本书的范围。如果您想了解更多关于如何加固基础操作系统和常见软件服务以抵御外部攻击的信息，我们建议访问[https://www.cisecurity.org/cis-benchmarks/](https://www.cisecurity.org/cis-benchmarks/)以及各个软件服务网站本身，例如[https://spark.apache.org/docs/latest/security.html](https://spark.apache.org/docs/latest/security.html)以了解Apache
    Spark的安全性。
- en: Java SE Development Kit 8
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Java SE 开发工具包 8
- en: 'Java is a general purpose programming language often used for **object-oriented
    programming** (**OOP**). Many of the distributed technologies that we discussed
    in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The Big Data Ecosystem*,
    are originally written in Java. As such, th**e Java Development Kit** (**JDK**)
    is required to be installed in our local development environment in order to run
    those software services within **Java Virtual Machines** (**JVM**). Apache Spark
    2.3.x requires Java 8+ in order to run. To install Oracle Java SE Development
    Kit 8, please execute the following shell commands as the Linux *root* user or
    another user with elevated privileges:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Java 是一种通用编程语言，常用于 **面向对象编程**（**OOP**）。我们在 [第 1 章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，“大数据生态系统”中讨论的许多分布式技术最初是用
    Java 编写的。因此，我们需要在我们的本地开发环境中安装 **Java 开发工具包**（**JDK**），以便在 **Java 虚拟机**（**JVM**）中运行这些软件服务。Apache
    Spark 2.3.x 需要 Java 8+ 才能运行。要安装 Oracle Java SE 开发工具包 8，请以 Linux *root* 用户或具有提升权限的其他用户身份执行以下
    shell 命令：
- en: '[PRE0]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'These commands will install JDK 8 and, thereafter, add the location of the
    Java binaries to the global `PATH` variable, allowing any local Linux user to
    run Java-based programs. To check that Java 8 has been installed successfully,
    the following command should return the version of Java installed, demonstrated
    as follows:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将安装 JDK 8，并在之后将 Java 二进制文件的路径添加到全局 `PATH` 变量中，使得任何本地 Linux 用户都可以运行基于 Java
    的程序。为了检查 Java 8 是否已成功安装，以下命令应返回已安装的 Java 版本，如下所示：
- en: '[PRE1]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Scala 2.11
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala 2.11
- en: Scala is a general purpose programming language used for both object-oriented
    programming and functional programming. Apache Spark is, in fact, written in the
    Scala programming language. However, as described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml),
    *The Big Data Ecosystem*, Spark applications can be written in a variety of languages,
    including Java, Scala, Python, and R. Though the pros and cons of Scala versus
    Python is beyond the scope of this book, Scala is generally faster than Python
    within the context of data analysis and naturally more tightly integrated with
    Spark. Python, however, currently offers a more comprehensive library of advanced
    third-party data science tools and frameworks and is arguably easier to learn
    and use. The code examples provided for this book have been written in Python
    3\. However, this sub-section describes the steps required in order to install
    Scala should you wish to develop Scala-based applications.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 是一种通用编程语言，用于面向对象编程和函数式编程。Apache Spark 实际上是用 Scala 编程语言编写的。然而，如 [第 1 章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)
    所述，“大数据生态系统”，Spark 应用程序可以用多种语言编写，包括 Java、Scala、Python 和 R。尽管 Scala 与 Python 的优缺点超出了本书的范围，但
    Scala 在数据分析的上下文中通常比 Python 快，并且与 Spark 的集成更为紧密。然而，Python 目前提供了更全面的先进第三方数据科学工具和框架库，并且可以说是更容易学习和使用。本书提供的代码示例是用
    Python 3 编写的。然而，本节描述了如果您想开发基于 Scala 的应用程序，所需的安装步骤。
- en: 'Referring to Scala specifically, Apache Spark 2.3.2 requires Scala 2.11.x in
    order to run Scala-based Spark applications. In order to install Scala 2.11.12,
    please execute the following shell commands as the Linux root user or another
    user with elevated privileges:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 具体到 Scala，Apache Spark 2.3.2 需要 Scala 2.11.x 才能运行基于 Scala 的 Spark 应用程序。为了安装
    Scala 2.11.12，请以 Linux 根用户或具有提升权限的其他用户身份执行以下 shell 命令：
- en: '[PRE2]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'These commands will install Scala 2.11.12 and place its binaries in a globally
    accessible location, allowing any local Linux user to run Scala applications,
    whether Spark-based or not. To check that Scala 2.11.12 has been installed successfully,
    the following command should return the version of Scala installed:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将安装 Scala 2.11.12 并将其二进制文件放置在全局可访问的位置，使得任何本地 Linux 用户都可以运行 Scala 应用程序，无论是否基于
    Spark。为了检查 Scala 2.11.12 是否已成功安装，以下命令应返回已安装的 Scala 版本：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can also access the Scala shell and execute interactive Scala commands
    as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过以下方式访问 Scala shell 并执行交互式 Scala 命令：
- en: '[PRE4]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Anaconda 5 with Python 3
  id: totrans-53
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Anaconda 5 与 Python 3
- en: Anaconda is a distribution of the Python general purpose programming language.
    Not only does it contain the Python interpreter, but it also comes bundled with
    a wide range of commonly used Python data science packages out of the box and
    a Python package management system called **conda**, making it quick and easy
    to provision a Python-based data science platform. In fact, we will be taking
    advantage of some of the pre-bundled Python packages in later chapters.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda是Python通用编程语言的发行版。它不仅包含Python解释器，而且还捆绑了大量的常用Python数据科学包，以及一个名为**conda**的Python包管理系统，这使得快速轻松地提供基于Python的数据科学平台变得可能。实际上，我们将在后面的章节中利用一些预捆绑的Python包。
- en: Anaconda 5.2 comes bundled with Python 3.6\. Apache Spark 2.3.x supports both
    branches of Python, namely Python 2 and Python 3\. Specifically, it supports Python
    2.7+ and Python 3.4+. As described earlier, the code examples provided for this
    book have been written in Python 3.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Anaconda 5.2捆绑了Python 3.6。Apache Spark 2.3.x支持Python的两个分支，即Python 2和Python 3。具体来说，它支持Python
    2.7+和Python 3.4+。如前所述，本书提供的代码示例是用Python 3编写的。
- en: 'In order to install Anaconda 5.2, please execute the following shell commands.
    You may or may not choose to execute these shell commands as the Linux root user.
    If you do not, Anaconda will be installed for the local Linux user running these
    commands and does not require administrator privileges:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 为了安装Anaconda 5.2，请执行以下shell命令。您可以选择是否以Linux root用户执行这些shell命令。如果您不这样做，Anaconda将为运行这些命令的本地Linux用户安装，并且不需要管理员权限：
- en: '[PRE5]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To check that Anaconda 5.2 and, hence, Python 3.6 have been installed successfully,
    the following command should return the version of conda installed:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查Anaconda 5.2及其Python 3.6是否已成功安装，以下命令应返回已安装的conda版本：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can also access the Python shell and execute interactive Python commands
    as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以访问Python shell并执行交互式Python命令，如下所示：
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Basic conda commands
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本conda命令
- en: In this sub-section, we will provide some basic conda commands for your reference.
    These commands assume that your virtual machine can access either the internet
    or a local Python repository.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将提供一些基本的conda命令供您参考。这些命令假设您的虚拟机可以访问互联网或本地Python仓库。
- en: 'In order to upgrade the version of conda and/or Anaconda as a whole, you can
    execute the following commands:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 为了升级conda版本或整个Anaconda，您可以执行以下命令：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To install or update individual Python packages, you can execute the following
    commands:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要安装或更新单个Python包，您可以执行以下命令：
- en: '[PRE9]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, in order to list the current Python packages and versions installed
    in your Anaconda distribution, you can execute the following command:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了列出您Anaconda发行版中当前安装的Python包及其版本，您可以执行以下命令：
- en: '[PRE10]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: To learn more about the conda package management system, please visit [https://conda.io/docs/index.html](https://conda.io/docs/index.html).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于conda包管理系统的信息，请访问[https://conda.io/docs/index.html](https://conda.io/docs/index.html)。
- en: Additional Python packages
  id: totrans-71
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 额外的Python包
- en: 'The following Python packages, which are not already contained within the default
    Anaconda distribution, are required for our local development environment. Please
    execute the following shell commands to install these prerequisite Python packages:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下Python包，这些包尚未包含在默认的Anaconda发行版中，是我们本地开发环境所必需的。请执行以下shell命令来安装这些先决条件Python包：
- en: '[PRE11]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Jupyter Notebook
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter Notebook
- en: Jupyter Notebook is an open source, web-based application designed for *interactive*
    analytics that comes bundled with the Anaconda distribution. Since it is designed
    for interactive analytics, it is best suited for ad hoc queries, live simulations,
    prototyping, and a means to visualize your data and to look for any trends and
    patterns prior to developing production-ready data science models. **Apache Zeppelin**
    is another example of an open source, web-based notebook used for similar purposes.
    Notebooks such as Jupyter Notebook and Apache Zeppelin tend to support multiple
    kernels, meaning that you can use various general purpose programming languages
    including Python and Scala.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: Jupyter Notebook是一个开源的、基于Web的应用程序，专为*交互式*分析设计，它包含在Anaconda发行版中。由于它专为交互式分析设计，因此非常适合即席查询、实时模拟、原型设计和在开发生产就绪的数据科学模型之前可视化数据和寻找任何趋势和模式。**Apache
    Zeppelin**是另一个用于类似目的的开源、基于Web的笔记本示例。Jupyter Notebook和Apache Zeppelin这样的笔记本通常支持多个内核，这意味着您可以使用包括Python和Scala在内的各种通用编程语言。
- en: One of the core advantages of notebooks is that they persist both your input
    code and any output data structures and visualizations that your code generates,
    including graphs, charts, and tables. However, they are *not* fully-fledged **integrated
    development environments** (**IDE**). This means that, in general, they should
    not be used for the development of code intended for production-grade data engineering
    or analytical pipelines. This is because they are difficult (but not impossible)
    to manage in version control systems such as Git, as they persist both input code
    and intermediate output structures. As such, they are also difficult to build
    code artifacts from and to deploy automatically using typical DevOps pipelines.
    Therefore, notebooks remain ideally suited for interactive analytics, ad hoc queries,
    and for prototyping.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本的一个核心优势是它们可以持久化您的输入代码以及任何由您的代码生成的输出数据结构和可视化，包括图表、图表和表格。然而，它们**并不是**完整的**集成开发环境**（IDE）。这意味着，通常情况下，它们不应用于开发用于生产级数据工程或分析管道的代码。这是因为它们在版本控制系统（如
    Git）中难以管理（但并非不可能），因为它们持久化了输入代码和中间输出结构。因此，它们也难以构建代码工件并使用典型的 DevOps 管道自动部署。因此，笔记本非常适合交互式分析、即兴查询和原型设计。
- en: 'The majority of the code files provided for this book are, in fact, Jupyter
    Notebook files (`.ipynb`), using the Python 3 kernel, so that readers may see
    the output of our models immediately. Should you, in the future, wish to write
    data science code that will eventually be deployed to production-grade systems,
    we strongly recommend writing your code in a proper IDE such as the following:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为本书提供的代码文件实际上大多数是 Jupyter Notebook 文件（`.ipynb`），使用 Python 3 内核，以便读者可以立即看到我们模型的输出。如果您将来希望编写最终将部署到生产级系统的数据科学代码，我们强烈建议您在以下适当的
    IDE 中编写代码：
- en: 'Eclipse: [https://www.eclipse.org/ide/](https://www.eclipse.org/ide/)'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Eclipse: [https://www.eclipse.org/ide/](https://www.eclipse.org/ide/)'
- en: 'IntelliJ IDEA: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'IntelliJ IDEA: [https://www.jetbrains.com/idea/](https://www.jetbrains.com/idea/)'
- en: 'PyCharm: [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/)'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'PyCharm: [https://www.jetbrains.com/pycharm/](https://www.jetbrains.com/pycharm/)'
- en: 'Microsoft **Visual Studio Code** (**VS Code**): [https://code.visualstudio.com/](https://code.visualstudio.com/)'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '微软 **Visual Studio Code**（**VS Code**）: [https://code.visualstudio.com/](https://code.visualstudio.com/)'
- en: 'As mentioned earlier, Jupyter Notebook is already bundled with the Anaconda
    distribution. However, a few configuration steps are recommended in order to access
    it. Please execute the following shell commands as your local Linux user in order
    to generate a per-user Jupyter Notebook configuration file that you can then edit
    based on per-user preferences:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Jupyter Notebook 已经包含在 Anaconda 发行版中。然而，为了访问它，建议执行以下配置步骤。请以您的本地 Linux 用户身份执行以下
    shell 命令，以生成一个基于用户的 Jupyter Notebook 配置文件，然后根据您的个人偏好进行编辑：
- en: '[PRE12]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: These commands will configure a per-user Jupyter Notebook instance to listen
    on a dedicated IP address (in our case, `192.168.56.10`) using a designated port
    (in our case `8888`), and to work from a pre-defined base directory in which to
    persist Jupyter Notebook code files (in our case `/data/workspaces/packt/jupyter/notebooks`).
    Note that you should amend these properties based on your specific environment.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这些命令将配置一个基于用户的 Jupyter Notebook 实例，使其在指定的 IP 地址（在我们的例子中为 `192.168.56.10`）上监听，并使用指定的端口（在我们的例子中为
    `8888`），以及从一个预定义的基本目录中工作，以持久化 Jupyter Notebook 代码文件（在我们的例子中为 `/data/workspaces/packt/jupyter/notebooks`）。请注意，您应根据您的具体环境修改这些属性。
- en: Starting Jupyter Notebook
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动 Jupyter Notebook
- en: 'If you have desktop-based access to your CentOS virtual machine, the easiest
    way to instantiate a new per-user Jupyter Notebook instance is to execute the
    following shell command as your local Linux user:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您可以通过桌面访问您的 CentOS 虚拟机，以最简单的方式实例化一个新的基于用户的 Jupyter Notebook 实例，请以您的本地 Linux
    用户身份执行以下 shell 命令：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'However, should you only have SSH or command-line access with no GUI, then
    you should use the following command instead:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果您只有 SSH 或命令行访问而没有 GUI，那么您应该使用以下命令代替：
- en: '[PRE14]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The latter command will stop Jupyter from automatically opening a local browser
    session. In either case, the resultant logs will state the full URL (including
    the security token by default) that can be used to access your instance of Jupyter
    Notebook. The URL should look similar to the following:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 后一个命令将阻止 Jupyter 自动打开本地浏览器会话。在任何情况下，生成的日志都将显示可以用来访问您的 Jupyter Notebook 实例的完整
    URL（默认情况下包括安全令牌）。URL 应该类似于以下内容：
- en: '`http://192.168.56.10:8888/?token=6ebb5f6a321b478162802a97b8e463a1a053df12fcf9d99c`'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://192.168.56.10:8888/?token=6ebb5f6a321b478162802a97b8e463a1a053df12fcf9d99c`'
- en: 'Please copy and paste this URL into an internet browser supported by Jupyter
    Notebook (Google Chrome, Mozilla Firefox, or Apple Safari). If successful, a screen
    similar to the screenshot illustrated in *Figure 2.1* should be returned:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请将此 URL 复制并粘贴到支持 Jupyter Notebook 的互联网浏览器中（Google Chrome、Mozilla Firefox 或 Apple
    Safari）。如果成功，应返回类似于 *图 2.1* 中所示截图的屏幕：
- en: '![](img/883be21b-b3bb-4c6b-88d3-16e3768932ae.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/883be21b-b3bb-4c6b-88d3-16e3768932ae.png)'
- en: 'Figure 2.1: Jupyter Notebook web session'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.1：Jupyter Notebook 网络会话
- en: Troubleshooting Jupyter Notebook
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Jupyter Notebook 故障排除
- en: 'Since Jupyter Notebook is a web-based application, it is accessible via the
    HTTP protocol at the designated port number. If you are accessing the generated
    URL via a remote internet browser and it cannot connect, then please check your
    firewall settings (and SELinux in the case of CentOS and RHEL) on your virtual
    machine to ensure that access to the designated port number is provisioned from
    your location. For example, the following shell commands executed by the Linux
    *root* user, or another user with elevated privileges, will open port 8888 in
    the CentOS 7 firewall via its public zone:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Jupyter Notebook 是一个基于 Web 的应用程序，它可以通过指定的端口号通过 HTTP 协议访问。如果您通过远程互联网浏览器访问生成的
    URL 并且无法连接，那么请检查您虚拟机上的防火墙设置（在 CentOS 和 RHEL 的情况下，还包括 SELinux），以确保从您的位置可以访问指定的端口号。例如，以下由
    Linux *root* 用户或具有提升权限的其他用户执行的 shell 命令，将通过其公共区域在 CentOS 7 防火墙中打开端口 8888：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Please contact your system administrator or refer to your cloud platform documentation
    for further network-related information and troubleshooting.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 请联系您的系统管理员或参考您的云平台文档以获取更多有关网络信息和故障排除的信息。
- en: To learn more about Jupyter Notebook, its configuration, and common troubleshooting,
    please visit [https://jupyter-notebook.readthedocs.io/en/stable/index.html](https://jupyter-notebook.readthedocs.io/en/stable/index.html).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 Jupyter Notebook、其配置和常见故障排除的信息，请访问 [https://jupyter-notebook.readthedocs.io/en/stable/index.html](https://jupyter-notebook.readthedocs.io/en/stable/index.html)。
- en: Apache Spark 2.3
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Spark 2.3
- en: As described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The
    Big Data Ecosystem*, Apache Spark is a general purpose distributed processing
    engine that is capable of performing data transformations, advanced analytics,
    machine learning, and graph analytics at scale over petabytes of data. Apache
    Spark can be deployed either in standalone mode (meaning that we utilize its in-built
    cluster manager) or integrated with other third-party cluster managers including
    Apache YARN and Apache Mesos.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 1 章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml) 所述，*大数据生态系统*，Apache Spark
    是一个通用目的的分布式处理引擎，能够在数PB的数据规模上执行数据转换、高级分析、机器学习和图分析。Apache Spark 可以以独立模式（意味着我们利用其内置的集群管理器）或与其他第三方集群管理器（包括
    Apache YARN 和 Apache Mesos）集成的方式部署。
- en: In the case of our single-node development cluster, we will deploy Apache Spark
    in standalone mode where our single-node will host both the Apache Spark Standalone
    Master server and a single worker node instance. Since Spark software services
    are designed to run in a JVM, it is perfectly acceptable to co-locate both standalone
    master and worker processes on a single node, though in practice, in real-world
    implementations of Apache Spark, clusters can be much larger with multiple worker
    nodes provisioned. Our single-node Apache Spark cluster will still allow us to
    prototype and develop Spark applications and machine learning models that can
    still take advantage of the parallelism offered by a multi-core single machine,
    and thereafter are capable of being deployed to larger clusters and datasets with
    ease.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们单节点开发集群的情况下，我们将以独立模式部署 Apache Spark，其中我们的单节点将托管 Apache Spark Standalone Master
    服务器和一个单独的工作节点实例。由于 Spark 软件服务旨在在 JVM 中运行，将独立主进程和工作进程同时放置在单个节点上是完全可接受的，尽管在实际的 Apache
    Spark 实施中，集群可以更大，配置了多个工作节点。我们的单节点 Apache Spark 集群仍然允许我们原型设计和开发可以利用多核单机提供的并行性的
    Spark 应用程序和机器学习模型，并且之后可以轻松地部署到更大的集群和数据集。
- en: Also note that we will be installing Apache Spark 2.3.2 direct from its pre-built
    binaries available on the official Apache Spark website at [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    Nowadays, it is common for distributed technologies such as Spark, Kafka, and
    Hadoop components to be installed together at the same time via consolidated big
    data platforms such as those offered by **Hortonworks Data Platform** (**HDP**),
    **Cloudera**, and **MapR**. The benefits of using consolidated platforms such
    as these include the deployment of individual component versions that have been
    fully tested together and guaranteed to fully integrate with one another, as well
    as web-based installation, monitoring, administration, and support.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 还请注意，我们将直接从 Apache Spark 官方网站提供的预构建二进制文件安装 Apache Spark 2.3.2，网址为 [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)。如今，像
    Spark、Kafka 和 Hadoop 组件这样的分布式技术通常通过像 **Hortonworks Data Platform** （**HDP**）、**Cloudera**
    和 **MapR** 提供的统一大数据平台一起安装。使用这些统一平台的好处包括部署经过完全测试并保证完全相互集成的单个组件版本，以及基于网络的安装、监控、管理和支持。
- en: Spark binaries
  id: totrans-104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 二进制文件
- en: 'Please execute the following shell commands as a local Linux user to extract
    the Apache Spark binaries. In our case, we will be installing the Spark binaries
    into `/opt`:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 请以本地 Linux 用户身份执行以下 Shell 命令以提取 Apache Spark 二进制文件。在我们的案例中，我们将把 Spark 二进制文件安装到
    `/opt`：
- en: '[PRE16]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The resultant Spark parent directory will have the following structure:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的 Spark 父目录将具有以下结构：
- en: '`bin`: Shell scripts for local Spark services, such as `spark-submit`'
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`bin`：本地 Spark 服务的 Shell 脚本，例如 `spark-submit`'
- en: '`sbin`: Shell scripts, including starting and stopping Spark services'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sbin`：Shell 脚本，包括启动和停止 Spark 服务'
- en: '`conf`: Spark configuration files'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`conf`：Spark 配置文件'
- en: '`jars`: Spark library dependencies'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`jars`：Spark 库依赖项'
- en: '`python`: Spark''s Python API, called PySpark'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`python`：Spark 的 Python API，称为 PySpark'
- en: '`R`: Spark''s R API, called SparkR'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`R`：Spark 的 R API，称为 SparkR'
- en: Local working directories
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地工作目录
- en: 'Each node in a Spark cluster (in our case, just the single node) will generate
    log files as well as local working files, such as when shuffling and serializing
    RDD data. The following commands will create defined local directories in which
    to store these local working outputs, the paths of which you can edit as per your
    preferences and which will be used in later configuration files:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 集群中的每个节点（在我们的案例中，仅为单个节点）都会生成日志文件以及本地工作文件，例如在洗牌和序列化 RDD 数据时。以下命令将在定义的本地目录中创建用于存储这些本地工作输出的目录，您可以根据您的偏好编辑路径，这些路径将在后续配置文件中使用：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Spark configuration
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 配置
- en: 'Configuration can be applied to Spark in the following ways:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 配置可以通过以下方式应用于 Spark：
- en: '**Spark properties** control application-level settings, including execution
    behavior, memory management, dynamic allocation, scheduling, and security, which
    can be defined in the following order of precedence:'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Spark 属性** 控制应用程序级别的设置，包括执行行为、内存管理、动态分配、调度和安全，这些可以在以下优先级顺序中定义：'
- en: Via a Spark configuration programmatic object called `SparkConf` defined in
    your driver program
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在驱动程序中定义的名为 `SparkConf` 的 Spark 配置程序性对象
- en: Via command-line arguments passed to `spark-submit` or `spark-shell`
  id: totrans-121
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过传递给 `spark-submit` 或 `spark-shell` 的命令行参数
- en: Via default options set in `conf/spark-defaults.conf`
  id: totrans-122
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在 `conf/spark-defaults.conf` 中设置的默认选项
- en: '**Environmental variables** control per-machine settings, such as the local
    IP address of the local worker node, and which can be defined in `conf/spark-env.sh`.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**环境变量** 控制每台机器的设置，例如本地工作节点的本地 IP 地址，这些可以在 `conf/spark-env.sh` 中定义。'
- en: Spark properties
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 属性
- en: 'In our case, we will set some basic default Spark properties via `conf/spark-defaults.conf`,
    freeing us to concentrate on the data science content in future chapters. This
    can be achieved by executing the following shell commands (edit the values as
    per your environment):'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将通过 `conf/spark-defaults.conf` 设置一些基本的默认 Spark 属性，这样我们就可以专注于未来章节中的数据科学内容。这可以通过执行以下
    Shell 命令实现（根据您的环境编辑值）：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Environmental variables
  id: totrans-127
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 环境变量
- en: 'We will also set basic environmental variables via `conf/spark-env.sh` as follows
    (edit the values as per your environment):'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将通过 `conf/spark-env.sh` 设置基本的环境变量，如下所示（根据您的环境编辑值）：
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To learn more about the various Spark configuration options available, including
    an exhaustive list of Spark properties and environmental variables, please visit
    [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html).
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关各种 Spark 配置选项的更多信息，包括 Spark 属性和环境变量的详尽列表，请访问 [https://spark.apache.org/docs/latest/configuration.html](https://spark.apache.org/docs/latest/configuration.html)。
- en: Standalone master server
  id: totrans-131
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 独立主服务器
- en: 'We are now ready to start the Spark standalone master server, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动 Spark 独立主服务器，如下所示：
- en: '[PRE20]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: To check whether this was successful, you can examine the Spark logs as written
    to `SPARK_LOG_DIR`. Spark applications can be submitted to the standalone master
    server at `spark://<Master IP Address>:7077` (port `7077` by default) or `spark://<Master
    IP Address>:6066` using its REST URL in cluster mode (port `6066` by default).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查是否成功，您可以检查写入 `SPARK_LOG_DIR` 的 Spark 日志。Spark 应用程序可以通过其 REST URL 提交到独立主服务器
    `spark://<Master IP 地址>:7077`（默认端口 `7077`）或 `spark://<Master IP 地址>:6066`（默认端口
    `6066`）。
- en: 'The Spark Master server also provides an out-of-the-box master web **User Interface**
    (**UI**) in which running Spark applications and workers can be monitored and
    performance diagnosed. By default, this master web UI is accessible via HTTP on
    port `8080`, in other words, `http://<Master IP Address>:8080`, the interface
    of which is illustrated in *Figure 2.2*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Master 服务器还提供了一个开箱即用的主网页 **用户界面**（**UI**），在其中可以监控正在运行的 Spark 应用程序和工作节点，并诊断性能。默认情况下，此主网页
    UI 通过 HTTP 在端口 `8080` 上可访问，换句话说，`http://<Master IP 地址>:8080`，其界面如图 2.2 所示：
- en: '![](img/295c2c65-f9a7-4fa9-bf7a-7e6ba03d5cc5.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/295c2c65-f9a7-4fa9-bf7a-7e6ba03d5cc5.png)'
- en: 'Figure 2.2: Spark standalone master server web UI'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2：Spark 独立主服务器网页界面
- en: Again, in the event that you cannot access this URL via a remote internet browser,
    you may need to open up port `8080` (by default) in your firewall and/or SELinux
    settings.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，如果您无法通过远程互联网浏览器访问此 URL，您可能需要在防火墙和/或 SELinux 设置中打开端口 `8080`（默认）。
- en: Spark worker node
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark 工作节点
- en: 'We are now ready to start our Spark Worker node, as follows:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以启动我们的 Spark Worker 节点，如下所示：
- en: '[PRE21]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Again, to check whether this was successful, you can examine the Spark logs
    as written to `SPARK_LOG_DIR`. You can also access the Spark Master web UI to
    confirm that the worker has been registered successfully, as illustrated in *Figure
    2.3*:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了检查是否成功，您可以检查写入 `SPARK_LOG_DIR` 的 Spark 日志。您还可以访问 Spark Master 网页界面以确认工作节点已成功注册，如图
    2.3 所示：
- en: '![](img/d3a310da-b0c6-493f-9897-8de1c32ee451.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d3a310da-b0c6-493f-9897-8de1c32ee451.png)'
- en: 'Figure 2.3: Spark worker successfully registered'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3：Spark 工作节点成功注册
- en: Note that Spark workers also expose a **Worker UI** via HTTP on port 8081 by
    default, in other words, `http://<Worker IP Address>:8081`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，Spark 工作节点默认通过 HTTP 在端口 8081 上暴露一个 **Worker UI**，换句话说，`http://<Worker IP
    地址>:8081`。
- en: PySpark and Jupyter Notebook
  id: totrans-146
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: PySpark 和 Jupyter Notebook
- en: Let's now integrate Jupyter Notebook with PySpark so that we can write our first
    Spark applications in Python! In the case of our local development environment,
    the easiest way to integrate Jupyter Notebook with PySpark is to set a global
    `SPARK_HOME` environmental variable that points to the directory containing the
    Spark binaries. Thereafter, we can employ the `findspark` Python package, as installed
    earlier, that will append the location of `SPARK_HOME`, and hence the PySpark
    API, to `sys.path` at runtime. Note that `findspark` should not be used for production-grade
    code development—instead, Spark applications should be deployed as code artifacts
    submitted via `spark-submit`.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将 Jupyter Notebook 与 PySpark 集成，这样我们就可以用 Python 编写我们的第一个 Spark 应用程序了！在我们的本地开发环境中，将
    Jupyter Notebook 与 PySpark 集成的最简单方法是为包含 Spark 二进制文件的目录设置一个全局 `SPARK_HOME` 环境变量。之后，我们可以使用之前安装的
    `findspark` Python 包，该包将在运行时将 `SPARK_HOME` 的位置和 PySpark API 添加到 `sys.path`。请注意，`findspark`
    不应用于生产级代码开发——相反，Spark 应用程序应作为通过 `spark-submit` 提交的代码工件进行部署。
- en: 'Please execute the following shell commands as the Linux root user, or another
    user with elevated privileges, in order to define a global environmental variable
    called `SPARK_HOME` (or, alternatively, add it to your local Linux user''s `.bashrc`
    file, which requires no administrative privileges):'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 请以 Linux 根用户或具有提升权限的其他用户身份执行以下 shell 命令，以便定义一个名为 `SPARK_HOME` 的全局环境变量（或者，也可以将其添加到您的本地
    Linux 用户的 `.bashrc` 文件中，这不需要管理员权限）：
- en: '[PRE22]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: You will need to restart any running Jupyter Notebook instances, and the underlying
    Terminal sessions from which they were spawned, in order for the SPARK_HOME environmental
    variable to be successfully recognized and registered by findspark.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使SPARK_HOME环境变量被findspark成功识别和注册，你需要重新启动任何正在运行的Jupyter Notebook实例及其底层的终端会话。
- en: 'We are now ready to write our first Spark application in Python! Instantiate
    a Jupyter Notebook instance, access it via your internet browser, and create a
    new Python 3 notebook containing the following code (it may be easier to split
    the following code over separate notebook cells for future ease of reference):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好用Python编写我们的第一个Spark应用程序了！实例化一个Jupyter Notebook实例，通过你的网络浏览器访问它，并创建一个新的Python
    3笔记本，包含以下代码（为了将来方便引用，你可能需要将以下代码拆分到单独的笔记本单元中）：
- en: '[PRE23]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'This PySpark application, at a high level, works as follows:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个PySpark应用程序在高层上工作如下：
- en: Import the required Python dependencies, including `findspark` and `pyspark`
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入所需的Python依赖项，包括`findspark`和`pyspark`
- en: Create a Spark context, which tells the Spark application how to connect to
    the Spark cluster, by instantiating it with a `SparkConf` object that provides
    application-level settings at a higher level of precedence
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过实例化一个`SparkConf`对象来创建一个Spark上下文，该对象提供高级别的应用程序级设置，从而告诉Spark应用程序如何连接到Spark集群
- en: Calculate the mathematical value of Pi π
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算数学值π
- en: Print the value of Pi and display it in Jupyter Notebook as a cell output
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印Pi的值，并在Jupyter Notebook中以单元格输出的形式显示它
- en: Stop the Spark context that terminates the Spark application
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 停止Spark上下文，这将终止Spark应用程序
- en: If you access the Spark Master web UI before executing `sc.stop()`, the Spark
    application will be listed under Running Applications, at which time you may view
    its underlying worker and executor log files. If you access the Spark Master web
    UI following execution of `sc.stop()`, the Spark application will be listed under
    Completed Applications.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在执行`sc.stop()`之前访问Spark Master的Web UI，Spark应用程序将列在“运行中的应用程序”下，此时你可以查看其底层的worker和executor日志文件。如果你在执行`sc.stop()`之后访问Spark
    Master的Web UI，Spark应用程序将列在“已完成的应用程序”下。
- en: Note that this notebook can be downloaded from the GitHub repository accompanying
    this book and is called `chp02-test-jupyter-notebook-with-pyspark.ipynb`.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个笔记本可以从本书配套的GitHub仓库中下载，并命名为`chp02-test-jupyter-notebook-with-pyspark.ipynb`。
- en: Apache Kafka 2.0
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Apache Kafka 2.0
- en: To finish off our local development environment, we will install Apache Kafka.
    As described in [Chapter 1](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml), *The
    Big Data Ecosystem*, Apache Kafka is a distributed streaming platform. We will
    use Apache Kafka in [Chapter 8](cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml), *Real-Time
    Machine Learning Using Apache Spark*, to develop a real-time analytical model
    by combining it with Spark Streaming and `MLlib`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的本地开发环境，我们将安装Apache Kafka。如[第1章](57c527fc-2555-49e9-bf1c-0567d05da388.xhtml)，“大数据生态系统”中所述，Apache
    Kafka是一个分布式流平台。我们将在[第8章](cad17bf3-6d9d-4486-a405-3d5103b072c5.xhtml)，“使用Apache
    Spark进行实时机器学习”中使用Apache Kafka，通过结合Spark Streaming和`MLlib`来开发一个实时分析模型。
- en: Again, for the purposes of our single-node development cluster, Apache Kafka
    will be deployed on the same single node as the Apache Spark software services.
    We will also be installing the version of Apache Kafka 2.0.0 that has been built
    for Scala 2.11.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，为了我们的单节点开发集群，Apache Kafka将部署在与Apache Spark软件服务相同的单节点上。我们还将安装为Scala 2.11构建的Apache
    Kafka 2.0.0版本。
- en: Kafka binaries
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka二进制文件
- en: 'After downloading the Kafka release, the first thing we need to do is to extract
    and install the pre-compiled binaries on our single-node cluster. In our case,
    we will be installing the Kafka binaries into `/opt`. Please execute the following
    shell commands as a local Linux user to extract the Apache Kafka binaries:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下载Kafka发布版后，我们首先需要做的是在我们的单节点集群上提取和安装预编译的二进制文件。在我们的案例中，我们将把Kafka二进制文件安装到`/opt`。请以本地Linux用户身份执行以下shell命令来提取Apache
    Kafka二进制文件：
- en: '[PRE24]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Local working directories
  id: totrans-167
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 本地工作目录
- en: 'As with Apache Spark processes, Apache Kafka processes also require their own
    local working directories to persist local data and log files. The following commands
    will create defined local directories in which to store these local working outputs,
    the paths of which you can edit as per your preferences:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 与Apache Spark进程一样，Apache Kafka进程也需要它们自己的本地工作目录来持久化本地数据和日志文件。以下命令将创建定义好的本地目录，用于存储这些本地工作输出，您可以根据自己的偏好编辑这些路径：
- en: '[PRE25]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Kafka configuration
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Kafka配置
- en: 'We will also set some basic configuration as follows (edit the values as per
    your environment):'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还将设置一些基本配置，如下（根据您的环境编辑值）：
- en: '[PRE26]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Start the Kafka server
  id: totrans-173
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 启动Kafka服务器
- en: 'We are now ready to start Apache Kafka as follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已准备好按照以下方式启动Apache Kafka：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Testing Kafka
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 测试Kafka
- en: 'Finally, we can test our Kafka installation by creating a test topic as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以通过创建一个测试主题来测试我们的Kafka安装，如下所示：
- en: '[PRE28]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Once we have created our test topic, let''s start a command-line producer application
    and send some test messages to this topic as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了测试主题，让我们启动一个命令行生产者应用程序，并按照以下方式向该主题发送一些测试消息：
- en: '[PRE29]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Finally, let''s start a command-line consumer application (in another Terminal
    session) to consume these test messages and print them to the console, as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们在另一个终端会话中启动一个命令行消费者应用程序，以消费这些测试消息并将它们打印到控制台，如下所示：
- en: '[PRE30]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In fact, if you keep typing new messages in the Terminal running the producer
    application, you will see them immediately being consumed by the consumer application
    and printed to the console in its Terminal!
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，如果您在运行生产者应用程序的终端中继续输入新消息，您将立即看到它们被消费者应用程序消费，并在其终端中打印出来！
- en: Summary
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have installed, configured, and deployed a local analytical
    development environment consisting of a single-node Apache Spark 2.3.2 and Apache
    Kafka 2.0.0 cluster that will also allow us to interactively develop Spark applications
    using Python 3.6 via Jupyter Notebook.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经安装、配置和部署了一个由单个节点Apache Spark 2.3.2和Apache Kafka 2.0.0集群组成的本地分析开发环境，这将使我们能够通过Jupyter
    Notebook使用Python 3.6交互式开发Spark应用程序。
- en: In the next chapter, we will discuss some of the high-level concepts behind
    common artificial intelligence and machine learning algorithms, as well as introducing
    Apache Spark's machine learning library, `MLlib`!
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将讨论一些常见人工智能和机器学习算法背后的高级概念，以及介绍Apache Spark的机器学习库`MLlib`！
