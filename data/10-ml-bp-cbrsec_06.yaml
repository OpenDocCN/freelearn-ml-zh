- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Detecting Deepfakes
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测深度伪造
- en: In recent times, the problem of deepfakes has become prevalent on the internet.
    Easily accessible technology allows attackers to create images of people who have
    never existed, through the magic of deep neural networks! These images can be
    used to enhance fraudulent or bot accounts to provide an illusion of being a real
    person. As if deepfake images were not enough, deepfake videos are just as easy
    to create. These videos allow attackers to either morph someone’s face onto a
    different person in an existing video, or craft a video clip in which a person
    says something. Deepfakes are a hot research topic and have far-reaching impacts.
    Abuse of deepfake technology can result in misinformation, identity theft, sexual
    harassment, and even political crises.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，深度伪造问题在互联网上变得普遍。易于获取的技术使得攻击者能够通过深度神经网络的魔法创建出从未存在过的人的图像！这些图像可以被用来增强欺诈或机器人账户，以提供真实人物的错觉。似乎深度伪造图像还不够，深度伪造视频同样容易创建。这些视频允许攻击者将某人的面孔
    morph 到现有视频中另一个人的脸上，或者制作一个视频中某人说某话的剪辑。深度伪造是一个热门的研究课题，具有深远的影响。深度伪造技术的滥用可能导致虚假信息、身份盗窃、性骚扰，甚至政治危机。
- en: This chapter will focus on machine learning methods to detect deepfakes. First,
    we will understand the theory behind deepfakes, how they are created, and what
    their impact can be. We will then cover two approaches to detecting deepfake images
    – vanilla approaches using standard machine learning models, followed by some
    advanced methods. Finally, as deepfake videos are major drivers of misinformation
    and have the most scope for exploitation, the last part of the chapter will focus
    on detecting deepfake videos.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将重点介绍用于检测深度伪造的机器学习方法。首先，我们将了解深度伪造背后的理论，它们是如何被创建的，以及它们可能产生的影响。然后，我们将介绍两种检测深度伪造图像的方法——使用标准机器学习模型的常规方法，接着是一些高级方法。最后，由于深度伪造视频是虚假信息的重大驱动因素，并且有最大的利用空间，本章的最后部分将专注于检测深度伪造视频。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: All about deepfakes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度伪造全解析
- en: Detecting fake images
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测虚假图像
- en: Detecting fake videos
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测虚假视频
- en: By the end of this chapter, you will have an in-depth understanding of deepfakes,
    the technology behind them, and how they can be detected.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将深入理解深度伪造、其背后的技术以及如何检测它们。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在 GitHub 上找到本章的代码文件，链接为 [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%205)。
- en: All about deepfakes
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深度伪造全解析
- en: The word *deepfake* is a combination of two words – *deep learning* and *fake*.
    Put simply, deepfakes are fake media created using deep learning technology. In
    the past decade, there have been significant advances in machine learning and
    generative models – models that create content instead of merely classifying it.
    These models (such as **Generative Adversarial Networks** (**GANs**)) can synthesize
    images that look real – even of human faces!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 词语 *deepfake* 是由两个词组合而成——*deep learning* 和 *fake*。简单来说，深度伪造是使用深度学习技术创建的虚假媒体。在过去十年中，机器学习和生成模型取得了重大进展——这些模型不是仅仅进行分类，而是创建内容。这些模型（如
    **生成对抗网络**（**GANs**））可以合成看起来真实的图像——甚至包括人类面孔！
- en: Deepfake technology is readily accessible to attackers and malicious actors
    today. It requires no sophistication or technical skills. As an experiment, head
    over to the website [thispersondoesnotexist.com](http://thispersondoesnotexist.com).
    This website allows you to generate images of people – people who have never existed!
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造技术如今对攻击者和恶意行为者来说触手可及。它不需要任何复杂的技术或技能。作为一个实验，请访问网站 [thispersondoesnotexist.com](http://thispersondoesnotexist.com)。这个网站允许你生成人们的图像——这些人从未存在过！
- en: For example, the people in the following figure are not real. They are deepfakes
    that have been generated by [thispersondoesnotexist.com](http://thispersondoesnotexist.com),
    and it only took a few seconds!
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，以下图中的人不是真实的。他们是 [thispersondoesnotexist.com](http://thispersondoesnotexist.com)
    生成的深度伪造，而且只需几秒钟！
- en: '![Figure 5.1 – Deepfakes generated from a website](img/B19327_05_01.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.1 – 来自网站的深度伪造](img/B19327_05_01.jpg)'
- en: Figure 5.1 – Deepfakes generated from a website
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 从网站生成的深度伪造图像
- en: Amazing, isn’t it? Let us now understand what makes generating these images
    possible, and the role machine learning has to play in it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 非常神奇，对吧？现在让我们了解是什么使得生成这些图像成为可能，以及机器学习在其中扮演的角色。
- en: A foray into GANs
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索GAN
- en: Let us now look at GANs, the technology that makes deepfakes possible.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看生成深度伪造的技术——GAN（生成对抗网络）。
- en: GANs are deep learning models that use neural networks to synthesize data rather
    than merely classify it. The name *adversarial* comes from the architectural design
    of these models; a GAN architecture consists of two neural networks, and we can
    force them into a cat-and-mouse game to generate synthetic media that can be passed
    off as real.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: GAN是使用神经网络合成数据而不是仅仅进行分类的深度学习模型。名称“对抗”来源于这些模型的架构设计；GAN架构由两个神经网络组成，我们可以迫使它们进行猫捉老鼠的游戏，以生成可以冒充真实的合成媒体。
- en: GAN architecture
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GAN架构
- en: 'A GAN consists of two main parts – the generator and the discriminator. Both
    models are deep neural networks. Synthetic images are generated by plotting these
    networks against each other:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一个GAN由两个主要部分组成——生成器和判别器。这两个模型都是深度神经网络。通过将这些网络相互绘制，可以生成合成图像：
- en: '**Generator**: The generator is the model that learns to generate real-looking
    data. It takes a fixed-length random vector as input and generates an output in
    the target domain, such as an image. The random vector is sampled randomly from
    a Gaussian distribution (that is, a standard normal distribution, where most observations
    cluster around the mean, and the further away an observation is from the mean,
    the lower the probability of it occurring is).'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成器**：生成器是学习生成看起来真实的数据的模型。它接收一个固定长度的随机向量作为输入，并在目标域（如图像）中生成输出。这个随机向量是从高斯分布（即标准正态分布，其中大多数观测值聚集在均值周围，观测值离均值越远，其发生的概率就越低）中随机采样的。'
- en: '**Discriminator**: The discriminator is a traditional machine learning model.
    It takes in data samples and classifies them as real or fake. Positive examples
    (those labeled as real) come from a training dataset, and negative examples (those
    labeled as fake) come from the generator.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**判别器**：判别器是一个传统的机器学习模型。它接收数据样本并将它们分类为真实或虚假。正例（标记为真实的样本）来自训练数据集，而负例（标记为虚假的样本）来自生成器。'
- en: GAN working
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: GAN工作原理
- en: Generating images is an unsupervised machine learning task and classifying the
    images is a supervised one. By training both the generator and discriminator jointly,
    we refine both and are able to obtain a generator that can generate samples good
    enough to fool the discriminator.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图像是一个无监督的机器学习任务，而图像分类则是一个监督学习任务。通过联合训练生成器和判别器，我们能够对两者进行优化，并得到一个能够生成足够好的样本以欺骗判别器的生成器。
- en: In joint training, the generator first generates a batch of sample data. These
    are treated as negative samples and augmented with images from a training dataset
    as positive samples. Together, these are used to fine-tune the discriminator.
    The discriminator model is updated to change parameters based on this data. Additionally,
    the discriminator loss (i.e., how well the generated images fooled the discriminator)
    is fed back to the generator. This loss is used to update the generator to generate
    better data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在联合训练中，生成器首先生成一批样本数据。这些数据被视为负样本，并使用来自训练数据集的图像作为正样本进行增强。这些数据一起被用来微调判别器。判别器模型根据这些数据更新参数。此外，判别器损失（即生成的图像欺骗判别器的程度）被反馈给生成器。这个损失用于更新生成器以生成更好的数据。
- en: '![Figure 5.2 – How a GAN model works](img/B19327_05_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – GAN模型的工作原理](img/B19327_05_02.jpg)'
- en: Figure 5.2 – How a GAN model works
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – GAN模型的工作原理
- en: At first, the generator produces random data that is clearly noisy and of poor
    quality. The discriminator learns to easily discern between real and fake examples.
    As the training progresses, the generator starts to get better as it leverages
    the signal from the discriminator, changing its generation parameters accordingly.
    In the end, in an ideal situation, the generator will have been so well trained
    that the generator loss decreases and the discriminator loss begins increasing,
    indicating that the discriminator can no longer effectively distinguish between
    actual data and generated data.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，生成器产生的是随机数据，显然是嘈杂且质量差的。判别器学会轻松地区分真实和伪造的例子。随着训练的进行，生成器开始变得更好，因为它利用了判别器的信号，相应地改变其生成参数。最终，在理想情况下，生成器将经过充分训练，生成器损失降低，判别器损失开始增加，这表明判别器不能再有效地区分实际数据和生成数据。
- en: How are deepfakes created?
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度伪造是如何被创建的？
- en: Along with being a portmanteau of the words *deep learning* and *fake*, the
    word *deepfake* has another origin. The very first deepfake video was created
    by a Reddit user by the name of `r/deepfakes`. This user used the open source
    implementation provided by Google to swap the face of several actresses into pornographic
    videos. Much of the amateur deepfakes in the wild today build upon this code to
    generate deepfakes.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 除了是“深度学习”和“伪造”这两个词的组合之外，“深度伪造”一词还有一个来源。第一个深度伪造视频是由一个名叫`r/deepfakes`的Reddit用户创建的。这位用户使用了谷歌提供的开源实现，将几位女演员的脸交换到色情视频中。今天野外的大部分业余深度伪造都是基于这个代码生成深度伪造的。
- en: 'In general, any deepfake creation entails the following four steps:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，任何深度伪造的创建都包括以下四个步骤：
- en: Analyzing the source image, identifying the area where the face is located,
    and cropping the image to that area.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分析源图像，识别人脸所在区域，并将图像裁剪到该区域。
- en: Computing features that are typically representations of the cropped image in
    a latent low-dimensional space, thus encoding the image into a feature vector.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算特征，这些特征通常是裁剪图像在潜在低维空间中的表示，从而将图像编码成特征向量。
- en: Modifying the generated feature vector based on certain signals, such as the
    destination image.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据某些信号（如目标图像）修改生成的特征向量。
- en: Decoding the modified vector and blending the image into the destination frame.
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码修改后的向量，并将图像融合到目标帧中。
- en: Most deepfake generation methods rely on neural networks and, in particular,
    the encoder-decoder architecture. The encoder transforms an image into a lower
    dimensional subspace and maps it to a latent vector (similar to the context vectors
    we described when discussing transformers). This latent vector captures features
    about the person in the picture, such as color, expression, facial structure,
    and body posture. The decoder does the reverse mapping and converts the latent
    representation into the target image. Adding a GAN into the mix leads to a much
    more robust and powerful deepfake generator.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数深度伪造生成方法依赖于神经网络，特别是编码器-解码器架构。编码器将图像转换到低维子空间，并将其映射到潜在向量（类似于我们在讨论转换器时描述的上下文向量）。这个潜在向量捕捉了图片中人物的特征，如颜色、表情、面部结构和身体姿势。解码器执行反向映射，将潜在表示转换为目标图像。加入GAN（生成对抗网络）可以使深度伪造生成器更加健壮和强大。
- en: The first commercial application of deepfakes started with the development of
    FakeApp in January 2018\. This is a desktop application that allows users to create
    videos with faces swapped for other faces. It is based on autoencoder architecture
    and consists of two encoder-decoder pairs. One encoder-decoder pair is trained
    on the images of the source image, and the other is trained on the images of the
    target. However, both encoders have shared common weights; in simpler terms, the
    same encoder is used in both autoencoders. This means that the latent vector representation
    for both images is in the same context, hence representing similar features. At
    the end of the training period, the common encoder will have learned to identify
    common features in both faces.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造技术的首次商业应用始于2018年1月FakeApp的开发。这是一个桌面应用程序，允许用户创建人脸交换的视频。它基于自动编码器架构，由两个编码器-解码器对组成。一对编码器-解码器在源图像上训练，另一对在目标图像上训练。然而，两个编码器共享相同的权重；换句话说，在两个自动编码器中都使用了相同的编码器。这意味着两个图像的潜在向量表示处于相同的环境中，因此表示相似的特征。在训练期结束时，共同的编码器将学会识别两个面孔中的共同特征。
- en: 'Let’s say A is the source image (the original image of our victim) and B is
    the target image (the one where we want to insert A). The high-level process to
    generate deepfakes using this methodology is as follows:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 A 是源图像（我们受害者的原始图像），B 是目标图像（我们想要插入 A 的图像）。使用此方法生成深度伪造的高级过程如下：
- en: Obtain multiple images of A and B using data augmentation techniques and image
    transformations so that the same picture from multiple viewpoints is considered.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数据增强技术和图像变换获取 A 和 B 的多张图像，以便考虑来自多个视角的同一张图片。
- en: Train the first autoencoder model on images of A.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在 A 图像上训练第一个自动编码器模型。
- en: Extract the encoder from the first autoencoder. Use this to train a second autoencoder
    model on images of B.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从第一个自动编码器中提取编码器。使用这个编码器在 B 图像上训练第二个自动编码器模型。
- en: At the end of training, we have two autoencoders with a shared encoder that
    can identify common features and characteristics in the images of A and B.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练结束时，我们有两个具有共享编码器的自动编码器，可以识别 A 和 B 图像中的共同特征和特性。
- en: To produce the deepfake, pass image A through the common encoder and obtain
    the latent representation.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要生成深度伪造，将图像 A 通过公共编码器，以获得潜在表示。
- en: 'Use the decoder from the second autoencoder (i.e., the decoder for images of
    B) to decode this back into the deepfake image. The following shows a diagram
    of this process:'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第二个自动编码器（即 B 图像的解码器）来将此解码回深度伪造图像。以下展示了这个过程：
- en: '![Figure 5.3 – Deepfake creation methodology](img/B19327_05_03.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 深度伪造创建方法](img/B19327_05_03.jpg)'
- en: Figure 5.3 – Deepfake creation methodology
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 深度伪造创建方法
- en: FakeApp has been widely used and inspired many other open source implementations,
    such as DeepFaceLab, FaceSwap, and Dfaker.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: FakeApp 已被广泛使用，并启发了许多其他开源实现，如 DeepFaceLab、FaceSwap 和 Dfaker。
- en: The social impact of deepfakes
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 深度伪造的社会影响
- en: While deepfake technology is certainly revolutionary in the field of machine
    learning and deep learning in particular, it has a far-reaching societal impact.
    Since their inception, deepfakes have been used for both benign and malicious
    purposes. We will now discuss both briefly. Understanding the full impact of deepfakes
    is essential to their study, especially for machine learning practitioners in
    the cybersecurity industry.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然深度伪造技术在机器学习领域，尤其是在深度学习领域，确实具有革命性，但它对社会有着深远的影响。自从它们出现以来，深度伪造就被用于善意的和恶意的目的。我们现在将简要讨论这两者。了解深度伪造的全面影响对于它们的研究至关重要，尤其是对于网络安全行业的机器学习从业者来说。
- en: Benign
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无害
- en: 'Not all deepfakes are bad. There are some very good reasons why the use of
    deepfakes may be warranted and, at times, beneficial. Deepfakes have shown to
    be of great utility in some fields:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有深度伪造都是有害的。有一些非常好的理由说明为什么使用深度伪造可能是合理的，有时甚至是有益的。深度伪造在以下领域已被证明具有极大的实用性：
- en: '**Entertainment**: Deepfake technology is now accessible to everyone. Powerful
    pre-trained models allow the exposure of endpoints through apps on smartphones,
    where they have been widely used to create entertaining videos. Such deepfakes
    include comedy videos where cartoons say certain dialogue, images where the faces
    of two people are swapped, or filters that generate human faces morphed into animals.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**娱乐**：深度伪造技术现在对每个人都是可访问的。强大的预训练模型允许通过智能手机上的应用程序暴露端点，它们已被广泛用于创建娱乐视频。这些深度伪造包括喜剧视频，其中卡通人物说出某些对话，交换两个人面孔的图像，或生成将人脸变形为动物的面部滤镜。'
- en: '**Resurrection**: An innovative use case of deepfakes has been reviving the
    deceased. Deepfakes can portray deceased people saying certain things, in their
    own voice! This is like magic, especially for someone who has no videographic
    memories of themselves left in this world. Deepfakes have also been used to create
    images that are able to portray how someone would have looked in a few years''
    or decades'' time.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**复活**：深度伪造的一个创新用途是使死者复活。深度伪造可以描绘死者以他们自己的声音说出某些话！这就像魔法一样，尤其是对于那些在这个世界上没有留下任何视频记忆的人来说。深度伪造也被用来创建能够描绘某人几年或几十年后外观的图像。'
- en: Malicious
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 恶意
- en: 'Unfortunately, however, every coin has two sides. The same deepfake technology
    that can power entertainment and enable resurrected digital personas can be used
    maliciously by attackers. Here are a few attack vectors that practitioners in
    this space should be aware of:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，不幸的是，每枚硬币都有两面。同样的深度伪造技术可以推动娱乐，并使复活的数字人格成为可能，但也可能被攻击者恶意使用。以下是这个领域从业者应该注意的几个攻击向量：
- en: '**Misinformation**: This has probably been the biggest use of deepfakes and
    the most challenging one to solve. Because deepfakes allow us to create videos
    of someone saying things they did not, this has been used to create videos that
    spread fake news. For example, a video featuring a surgeon general saying that
    vaccines are harmful and cause autism would certainly provoke widespread fear
    and lead people to believe that it is true. Malicious entities can create and
    disseminate such deepfakes to further their own causes. This can also lead to
    political crises and loss of life. In 2022, a deepfake video featuring the Ukrainian
    president Volodymyr Zelenskyy was created by Russian groups, where the former
    was shown to accept defeat and ask soldiers to stand down – this was not the case,
    but the video spread like wildfire on social media before it was removed.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**虚假信息**: 这可能是深度伪造最大的用途，也是最具挑战性的一个。因为深度伪造允许我们创建某人说出他们从未说过的话的视频，这已经被用来传播虚假新闻。例如，一个展示卫生部长说疫苗有害并导致自闭症的视频肯定会引起广泛的恐慌，并使人们相信这是真的。恶意实体可以创建和传播这样的深度伪造来推进自己的事业。这也可能导致政治危机和人员伤亡。2022年，俄罗斯团体制作了一个乌克兰总统弗拉基米尔·泽连斯基的深度伪造视频，视频中他似乎接受了失败并要求士兵放下武器——这不是事实，但视频在社交媒体上迅速传播，直到被删除。'
- en: '**Fraud**: Traditionally, many applications depended on visual identification
    verification. After the COVID-19 pandemic, most of these transitioned to verifying
    identities through documents and video online. Deepfakes have tremendous potential
    to be used for identity theft here; by crafting a deepfake video, you can pretend
    to be someone you are not and bypass automatic identity verification systems.
    In June 2022, the **Federal Bureau of Investigation** (**FBI**) published a public
    service announcement warning companies of deepfakes being used in online interviews
    and during remote work. Deepfakes can also be used to create sock puppet accounts
    on social media websites, GANs can be used to generate images, and deepfake videos
    can be used to enrich profiles with media (videos showing the person talking)
    so that they appear real.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欺诈**: 传统上，许多应用程序依赖于视觉身份验证。在COVID-19大流行之后，大多数这些应用程序转向通过在线文档和视频进行身份验证。深度伪造在这里被用于身份盗窃具有巨大的潜力；通过制作深度伪造视频，你可以假装成另一个人，绕过自动身份验证系统。2022年6月，**联邦调查局**（**FBI**）发布了一则公共服务公告，警告公司注意深度伪造在在线面试和远程工作中被使用。深度伪造还可以用于在社交媒体网站上创建木偶账户，生成图像可以使用GAN，深度伪造视频可以用来丰富个人资料中的媒体（显示人物讲话的视频），使其看起来真实。'
- en: '**Pornography**: The very first deepfake that was created was a pornographic
    movie. Revenge porn is an alarmingly growing application of deepfakes in today’s
    world. By using images of a person and any pornographic clip as a base, it is
    possible to depict the said person in the pornographic clip. The naked eye may
    not be able to ascertain that the video is a spoof.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**色情**: 第一个被创建的深度伪造是一个色情电影。报复色情是当今世界中深度伪造令人担忧的应用增长。通过使用一个人的图像和任何色情片段作为基础，可以描绘出该人在色情片段中的样子。肉眼可能无法确定视频是伪造的。'
- en: Detecting fake images
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测伪造图像
- en: In the previous section, we looked at how deepfake images and videos can be
    generated. As the technology to do so is accessible to everyone, we also discussed
    the impact that this can have at multiple levels. Now, we will look at how fake
    images can be detected. This is an important problem to solve and has far-reaching
    impacts on social media and the internet in general.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了如何生成深度伪造图像和视频。由于这项技术对每个人都是可访问的，我们也讨论了它可以在多个层面上产生的影响。现在，我们将探讨如何检测伪造图像。这是一个需要解决的问题，对社交媒体和互联网整体具有深远的影响。
- en: A naive model to detect fake images
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个用于检测伪造图像的简单模型
- en: We know that machine learning has driven significant progress in the domain
    of image processing. Convolutional neural networks (CNNs) have surpassed prior
    image detectors and achieved accuracy even greater than that of humans. As a first
    step toward detecting deepfake images, we will treat the task as a simple binary
    classification and use standard deep learning image classification approaches.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道机器学习在图像处理领域取得了显著的进步。卷积神经网络（CNN）已经超越了先前的图像检测器，并达到了甚至超过人类准确性的水平。为了检测深度伪造图像，我们将这项任务视为一个简单的二分类问题，并使用标准的深度学习图像分类方法作为第一步。
- en: The dataset
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: There are several publicly available datasets for deepfake detection. We will
    use the 140k Real and Fake Faces Dataset. This dataset is freely available to
    download from Kaggle ([https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces](https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 深伪影检测有多个公开可用的数据集。我们将使用 140k 真实与伪造人脸数据集。此数据集可在 Kaggle 上免费下载（[https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces](https://www.kaggle.com/datasets/xhlulu/140k-real-and-fake-faces))）。
- en: As the name suggests, the dataset consists of 140,000 images. Half of these
    are real faces from Flickr, and the remaining half are fake ones generated from
    a GAN. The real faces have been collected by researchers from NVIDIA, and there
    is significant coverage of multiple ethnicities, age groups, and accessories in
    the images.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，该数据集包含 140,000 张图像。其中一半来自 Flickr 的真实人脸，另一半是由 GAN 生成的伪造人脸。真实人脸由 NVIDIA
    的研究人员收集，图像中涵盖了多个民族、年龄组和配饰的显著覆盖。
- en: The dataset is fairly large (~4 GB). You will need to download the compressed
    dataset from Kaggle, unzip it, and store it locally. The root directory consists
    of three folders – one each for training, testing, and validation data.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集相当大（约 4 GB）。您需要从 Kaggle 下载压缩的数据集，解压并存储在本地。根目录包含三个文件夹 – 分别用于训练数据、测试数据和验证数据。
- en: The model
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: 'We are going to use a CNN model for classification. These neural networks specialize
    in understanding and classifying data in which the spatial representation of data
    matters. This makes it ideal for processing images, as images are fundamentally
    matrices of pixels arranged in a grid-like topology. A CNN typically has three
    main layers – convolution, pooling, and classification:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 CNN 模型进行分类。这些神经网络擅长理解和分类那些数据的空间表示很重要的数据。这使得它非常适合处理图像，因为图像本质上是由像素组成的矩阵，这些像素以网格状拓扑排列。CNN
    通常有三个主要层 – 卷积、池化和分类：
- en: '**Convolution layer**: This is the fundamental building block of a CNN. This
    layer traverses through an image and generates a matrix known as an **activation
    map**. Each convolution multiplies some part of the image with a kernel matrix
    (the weights in the matrix are parameters that can be learned using gradient descent
    and backpropagation). The convolution matrix slides across the image row by row
    and performs the dot product. The convolution layer aggregates multiple neighborhoods
    of the image and produces a condensed output, as shown in *Figure 5**.4*:'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**：这是 CNN 的基本构建块。该层遍历图像并生成一个称为 **激活图** 的矩阵。每次卷积将图像的一部分与核矩阵（矩阵中的权重是可以通过梯度下降和反向传播学习的参数）相乘。卷积矩阵逐行滑动图像并执行点积。卷积层聚合图像的多个邻域并生成一个压缩输出，如图
    *图 5**.4* 所示：'
- en: '![Figure 5.4 – The convolution layer](img/B19327_05_04.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 卷积层](img/B19327_05_04.jpg)'
- en: Figure 5.4 – The convolution layer
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 卷积层
- en: '**Pooling layer**: The pooling layer performs aggregations over the convolution
    output. It calculates a summary statistic over a neighborhood and replaces the
    neighborhood cells with the summary. The statistic can be the mean, max, median,
    or any other standard metric. Pooling reduces redundancy in the convolutional
    representation and reduces dimensions by summarizing over multiple elements, as
    shown in *Figure 5**.5*:'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化层**：池化层对卷积输出进行聚合。它在一个邻域上计算一个汇总统计量，并用汇总替换邻域细胞。该统计量可以是平均值、最大值、中位数或其他标准度量。池化通过在多个元素上汇总来减少卷积表示中的冗余并降低维度，如图
    *图 5**.5* 所示：'
- en: '![Figure 5.5 – The pooling layer](img/B19327_05_05.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 池化层](img/B19327_05_05.jpg)'
- en: Figure 5.5 – The pooling layer
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 池化层
- en: '**Fully connected layer**: The output from the pooling layer is flattened and
    converted into a one-dimensional vector. This is done simply by appending the
    rows to each other. This vector is now passed to a fully connected neural network,
    at the end of which is a **softmax layer**. This layer operates as a standard
    neural network, with weights being updated with gradient descent and backpropagation
    through multiple epochs. The softmax layer will output a probability distribution
    of the predicted class of the image, as shown in *Figure 5**.6*:'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**：池化层的输出被展平并转换为一条一维向量。这很简单，只需将行依次连接即可。现在，这个向量被传递到一个全连接神经网络中，其末尾是一个 **softmax
    层**。该层作为一个标准神经网络运行，权重通过多个周期使用梯度下降和反向传播进行更新。softmax 层将输出图像预测类别的概率分布，如图 *图 5**.6*
    所示：'
- en: '![Figure 5.6 – Flattening and a fully connected layer](img/B19327_05_06.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 展平层和全连接层](img/B19327_05_06.jpg)'
- en: Figure 5.6 – Flattening and a fully connected layer
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 展平层和全连接层
- en: While training, data flows into the network and undergoes convolution and pooling.
    We can stack multiple convolution-pooling layers one after the other; the hope
    is that each layer will learn something new from the image and obtain more and
    more specific representations. The pooling output after the last convolution layer
    flows into the fully connected neural network (which can also consist of multiple
    layers). After the softmax output, a loss is calculated, which then flows back
    through the whole network. All layers, including the convolution layers, update
    their weights, using gradient descent to minimize this loss.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，数据流入网络并经历卷积和池化。我们可以堆叠多个卷积-池化层，希望每一层都能从图像中学习到新的东西，并获得越来越具体的表示。最后一个卷积层之后的池化输出流入完全连接的神经网络（也可以由多个层组成）。在
    softmax 输出后，计算损失，然后整个网络反向传播。所有层，包括卷积层，都会更新它们的权重，使用梯度下降来最小化这个损失。
- en: Putting it all together
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有内容整合在一起
- en: We will now use a CNN model to detect deepfakes and run this experiment on the
    140k dataset.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用 CNN 模型来检测 deepfakes，并在 140k 数据集上运行这个实验。
- en: 'First, we import the required libraries:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入所需的库：
- en: '[PRE0]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Recall that the first step in the data science pipeline is data preprocessing.
    We need to parse the images we downloaded and convert them into a form suitable
    for consumption by the CNN model. Fortunately, the `ImageDataGenerator` class
    in the `keras` library helps us do just that. We will use this library and define
    our training and test datasets:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，数据科学流程中的第一步是数据预处理。我们需要解析我们下载的图像，并将它们转换为适合 CNN 模型消费的形式。幸运的是，`keras` 库中的
    `ImageDataGenerator` 类帮助我们做到了这一点。我们将使用这个库并定义我们的训练和测试数据集：
- en: '[PRE1]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After you run this, you should see an output somewhat like this:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此操作后，你应该会看到类似以下输出：
- en: '[PRE2]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that the data is preprocessed, we can define the actual model and specify
    the architecture of the CNN. Our model will consist of convolution, pooling, and
    the fully connected layer, as described earlier:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已经预处理完毕，我们可以定义实际的模型并指定 CNN 的架构。我们的模型将包括卷积、池化和完全连接层，如前所述：
- en: '[PRE3]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Let us take a closer look at what we did here. First, we set some basic model
    parameters and defined an empty model using the `Sequential` class. This model
    will be our base, where we pile on different layers to build the whole architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看我们在这里做了什么。首先，我们设置了一些基本模型参数，并使用 `Sequential` 类定义了一个空模型。这个模型将成为我们的基础，我们在其上堆叠不同的层来构建整个架构。
- en: Then, we defined our **convolution layers**. Each convolution layer has a **max
    pooling** **layer**, followed by a **normalization layer**. The normalization
    layer simply normalizes and rescales the pooled output. This results in stable
    gradients and avoids exploding loss.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们定义了我们的**卷积层**。每个卷积层都有一个**最大池化层**，后面跟着一个**归一化层**。归一化层只是对池化输出进行归一化和缩放。这导致梯度稳定，避免了损失爆炸。
- en: Then, we added an aggregation layer using the `GlobalAveragePooling2D` class.
    This will concatenate the previous output into a one-dimensional vector. Finally,
    we have a fully connected layer at the end with a softmax activation; this layer
    is responsible for the actual classification.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们使用 `GlobalAveragePooling2D` 类添加了一个聚合层。这将把之前的输出连接成一个一维向量。最后，我们在末尾有一个带有 softmax
    激活的完全连接层；这个层负责实际的分类。
- en: 'You can take a look at your model architecture by printing the summary:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过打印摘要来查看你的模型架构：
- en: '[PRE4]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, we can train the model we defined on the data we preprocessed:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以在预处理的数据上训练我们定义的模型：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This will take about 30 minutes to an hour to complete. Of course, the exact
    time will depend on your processor, scheduling, and system usage.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要大约 30 分钟到 1 小时才能完成。当然，确切的时间将取决于你的处理器、调度和系统使用情况。
- en: 'Once the model is trained, we can use it to make predictions for our test data
    and compare the predictions with the ground truth:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，我们可以用它来对测试数据进行预测，并将预测与真实值进行比较：
- en: '[PRE6]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now, you can plot the confusion matrix, just like we did for the other models
    in the previous chapters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以像我们在前几章中的其他模型一样绘制混淆矩阵。
- en: Playing around with the model
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在模型上尝试不同的操作
- en: 'In the previous section, we looked at how an end-to-end CNN model can be defined
    and how to train it. While doing so, we made several design choices that can potentially
    affect the performance of our model. As an experiment, you should explore these
    design choices and rerun the experiment with different parameters. We will not
    go through the analysis and hyperparameter tuning in detail, but we will discuss
    some of the parameters that can be tweaked. We will leave it up to you to test
    it out, as it is a good learning exercise. Here are some things that you could
    try out to play around with the model:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了如何定义端到端的CNN模型以及如何训练它。在这个过程中，我们做出了几个可能影响模型性能的设计选择。作为一个实验，你应该探索这些设计选择，并使用不同的参数重新运行实验。我们不会详细分析超参数调整，但我们会讨论一些可以调整的参数。我们将把它留给你去测试，因为这是一个很好的学习练习。以下是一些你可以尝试的事情来玩转模型：
- en: '**Convolutional layers**: In our model, we have three layers of convolution
    and max pooling. However, you can extend this to as many (or as few) layers as
    you want. What happens if you have 10 layers? What happens if you have only one?
    How does the resulting confusion matrix change?'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积层**：在我们的模型中，我们有三层卷积和最大池化层。然而，你可以扩展到任意多（或少）层。如果你有10层会发生什么？如果你只有一层会发生什么？结果混淆矩阵会如何变化？'
- en: '**Pooling layers**: We used max pooling in our model. However, as discussed
    in the CNN architecture, pooling can leverage any statistical aggregation. What
    happens if we use mean pooling or min pooling instead of max pooling? How does
    the resulting confusion matrix change?'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**池化层**：我们在模型中使用了最大池化。然而，正如CNN架构中讨论的那样，池化可以利用任何统计聚合。如果我们使用均值池化或最小池化而不是最大池化会发生什么？结果混淆矩阵会如何变化？'
- en: '**Fully connected layers**: Our model has just one fully connected layer at
    the end. However, this does not have to be the case; you can have a full-fledged
    neural network with multiple layers. You should examine what happens if the number
    of layers is increased.'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**全连接层**：我们的模型在最后只有一个全连接层。然而，这并不一定非得如此；你可以有一个具有多个层的完整神经网络。你应该检查如果层数增加会发生什么。'
- en: '**Dropout**: Note that each layer is followed by a dropout layer. Dropout is
    a technique used in neural networks to avoid overfitting. A certain fraction of
    the weights is randomly set to 0; these are considered to be “dropped out.” Here,
    we have a dropout fraction of 0.1\. What happens if it is increased? What happens
    if it is set to 0 (i.e., with no dropout at all)?'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Dropout**：请注意，每个层后面都跟着一个dropout层。Dropout是一种在神经网络中用于避免过拟合的技术。一定比例的权重被随机设置为0；这些被认为是“丢弃”的。在这里，我们有一个0.1的dropout比例。如果它增加会发生什么？如果它设置为0（即完全没有dropout）会发生什么？'
- en: This completes our discussion of deepfake image detection. Next, we will see
    how deepfakes transitioned from images to videos, and we will look at methods
    for detecting them.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对深度伪造图像检测的讨论。接下来，我们将看到深度伪造是如何从图像过渡到视频的，我们将探讨检测它们的方法。
- en: Detecting deepfake videos
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测深度伪造视频
- en: As if deepfake images were not enough, deepfake videos are now revolutionizing
    the internet. From benign uses such as comedy and entertainment to malicious uses
    such as pornography and political unrest, deepfake videos are taking social media
    by storm. Because deepfakes appear so realistic, simply looking at a video with
    the naked eye does not provide any clues as to whether it is real or fake. As
    a machine learning practitioner working in the security field, it is essential
    to know how to develop models and techniques to identify deepfake videos.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 就像深度伪造图像还不够一样，深度伪造视频现在正在改变互联网。从良性的用途，如喜剧和娱乐，到恶性的用途，如色情和政治动荡，深度伪造视频正在席卷社交媒体。因为深度伪造看起来如此逼真，仅凭肉眼观看视频并不能提供任何关于它是真实还是伪造的线索。作为一名在安全领域工作的机器学习从业者，了解如何开发模型和技术来识别深度伪造视频是至关重要的。
- en: A video can be thought of as an extension of an image. A video is multiple images
    arranged one after the other and viewed in quick succession. Each such image is
    known as a frame. By viewing the frames at a high speed (multiple frames per second),
    we see images moving.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一个视频可以被视为图像的扩展。视频是由多个图像依次排列并快速连续观看的。每个这样的图像被称为一帧。通过以高速（每秒多帧）查看这些帧，我们看到图像在移动。
- en: Neural networks cannot directly process videos – there does not exist an appropriate
    method to encode images and convert them into a form suitable for consumption
    by machine learning models. Therefore, deepfake video detection involves deepfake
    image detection on each frame of the video. We will look at the succession of
    frames, examine how they evolve, and determine whether the transformations and
    movements are normal or similar to those expected in real videos.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络不能直接处理视频——不存在一种适当的方法来编码图像并将它们转换为适合机器学习模型消费的形式。因此，深度伪造视频检测涉及对视频每一帧进行深度伪造图像检测。我们将查看帧的连续性，检查它们的演变，并确定变换和运动是否正常或类似于真实视频中的预期。
- en: 'In general, detecting video deepfakes follows the following pattern:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，检测视频深度伪造遵循以下模式：
- en: Parsing the video file and decomposing it into multiple frames.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析视频文件并将其分解成多个帧。
- en: Reading frames up to a maximum number of frames. If the number of frames is
    less than the maximum set, pad them with empty frames.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取最多帧数的帧。如果帧数少于设定的最大值，则用空帧填充。
- en: Detecting the faces in each frame.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检测每一帧中的面部。
- en: Cropping the faces in each frame.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 裁剪每一帧中的面部。
- en: Training a model with a sequence of cropped faces (one face obtained per frame)
    as input and real/fake labels as output.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用裁剪面部序列（每帧获得一个面部）作为输入，以及真实/伪造标签作为输出的模型进行训练。
- en: We will leverage these steps to detect fake videos in the next section.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节利用这些步骤来检测伪造视频。
- en: Building deepfake detectors
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 构建深度伪造检测器
- en: In this section, we will look at how to build models to detect deepfake videos.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨如何构建模型以检测深度伪造视频。
- en: The dataset
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据集
- en: We will use the dataset from the Kaggle Deepfake Detection Challenge ([https://www.kaggle.com/competitions/deepfake-detection-challenge/overview](https://www.kaggle.com/competitions/deepfake-detection-challenge/overview)).
    As part of the challenge, three datasets were provided – training, testing, and
    validation. With prizes worth $1 million, participants were required to submit
    code and output files, which were evaluated on a private test set.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Kaggle深度伪造检测挑战赛（[https://www.kaggle.com/competitions/deepfake-detection-challenge/overview](https://www.kaggle.com/competitions/deepfake-detection-challenge/overview)）的数据集。作为挑战的一部分，提供了三个数据集——训练、测试和验证。由于奖金高达100万美元，参与者需要提交代码和输出文件，这些文件在私有测试集上进行了评估。
- en: The actual dataset is too large (around 0.5 TB) for us to feasibly download
    and process, given that most of you will have access to only limited compute power.
    We will use a subset of the data available in the `train_sample_videos.zip` and
    `test_videos.zip` files, which are around 4 GB altogether. You will need to download
    this data and save it locally so that you can run the experiments.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 实际数据集太大（约0.5 TB），我们无法实际下载和处理，考虑到你们大多数人只有有限的计算能力。我们将使用`train_sample_videos.zip`和`test_videos.zip`文件中可用的数据子集，总共约4
    GB。您需要下载这些数据并将其保存在本地，以便您能够运行实验。
- en: The model
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型
- en: In the previous section, we developed our own CNN model and trained it end to
    end for classification. In this section, we will explore a different technique
    based on the concept of *transfer learning*. In machine learning, transfer learning
    is a paradigm where parameters learned from one task can be applied to another
    task and improve the performance of the latter.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们开发了我们的CNN模型，并从头到尾进行分类训练。在本节中，我们将探讨基于迁移学习概念的另一种技术。在机器学习中，迁移学习是一种范式，其中一个任务中学习的参数可以应用于另一个任务，并提高后者的性能。
- en: 'In transfer learning, we train a classification model on a base task. Once
    that model is trained, we can use it for another task in one of two ways:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在迁移学习中，我们在基础任务上训练一个分类模型。一旦该模型训练完成，我们可以通过以下两种方式将其用于另一个任务：
- en: '**Training the model again on new data from the second task**: By doing so,
    we will initialize the model weights as the ones produced after training on the
    first task. The hope is that the model (which will effectively be trained on both
    sets of data) will show a good performance on both tasks.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**在第二个任务的新数据上重新训练模型**：通过这样做，我们将初始化模型权重为在第一个任务上训练后产生的权重。希望模型（实际上将在两组数据上训练）在两个任务上都能表现出良好的性能。'
- en: '**Using the model as a feature extractor**: The final layer of the model is
    typically a softmax layer. We can extract the pre-final layer and use the model
    without softmax to generate features for our new data. These features can be used
    to train a downstream classification model.'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用模型作为特征提取器**：模型的最后一层通常是softmax层。我们可以提取预最终层，并使用没有softmax的模型为我们的新数据生成特征。这些特征可以用于训练下游分类模型。'
- en: Note that the second task is generally a refined and specific version of the
    first task. For example, the first task can be sentiment classification, and the
    second task can be movie review classification. The hope is that training on the
    first task will help the model learn high-level signals for sentiment classifications
    (keywords indicating certain sentiments, such as `excellent`, `amazing`, `good`,
    and `terrible`). Fine-tuning in the second task will help it learn task-specific
    knowledge in addition to the high-level knowledge (movie-specific keywords such
    as `blockbuster` and `flop-show`). Another example is that the base task is image
    classification on animals, and the fine-tuning task is a classifier for cat and
    dog images.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第二个任务通常是第一个任务的精炼和具体版本。例如，第一个任务可以是情感分类，第二个任务可以是电影评论分类。希望训练第一个任务将帮助模型学习情感分类的高级信号（表示特定情感的关键词，如`优秀`、`惊人`、`好`和`糟糕`）。在第二个任务中的微调将帮助它学习除了高级知识之外的任务特定知识（如电影特定的关键词`大片`和`烂片`）。另一个例子是，基础任务是动物图像分类，微调任务是猫和狗图像的分类器。
- en: We will use the second approach listed here. Instead of developing custom features,
    we will let a pre-trained model do all the work. The model we will use is the
    **InceptionV3 model**, which will extract features for every frame in the video.
    We will use a **Recurrent Neural Network** (**RNN**) to classify the video, based
    on the sequence of frames. This technique has been adapted from a submission to
    the challenge ([https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook](https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook)).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这里列出的第二种方法。我们不会开发自定义特征，而是让预训练的模型完成所有工作。我们将使用的模型是**InceptionV3模型**，它将为视频中的每一帧提取特征。我们将使用**循环神经网络**（**RNN**）根据帧序列对视频进行分类。这项技术是从挑战赛的一个提交中改编的（[https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook](https://www.kaggle.com/code/krooz0/deep-fake-detection-on-images-and-videos/notebook)）。
- en: Putting it all together
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 将所有这些放在一起
- en: 'First, we will import the necessary libraries:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入必要的库：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We will then read the video file and label for each video into a DataFrame.
    The dataset contains a metadata file for each set that provides us with this information.
    Note that you may have to adjust the paths depending on where and how you stored
    the data locally:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将读取每个视频的文件和标签，并将它们读入一个DataFrame。数据集包含每个集合的元数据文件，为我们提供这些信息。请注意，您可能需要根据您在本地存储数据的位置和方式调整路径：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Let’s take a look at one of the DataFrames:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看其中一个DataFrame：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'It should show you the list of video files along with their associated label
    (real or fake). We will now define a few helper functions. The first one is a
    cropping function that will take in a frame and crop it into a square:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 它应该显示视频文件及其相关标签（真实或伪造）的列表。现在，我们将定义几个辅助函数。第一个是一个裁剪函数，它将接受一个帧并将其裁剪成方形：
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The second helper function will parse the video file and extract frames from
    it up to a specified maximum number of frames. We will resize each frame as (`224,224`)
    for standardization and then crop it into a square. The output will be a sequence
    of uniformly sized and cropped frames:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个辅助函数将解析视频文件并从中提取指定最大帧数的帧。我们将每个帧调整大小为（`224,224`）以进行标准化，然后将其裁剪成方形。输出将是一系列大小均匀且裁剪过的帧：
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We will now define our feature extractor, which uses transfer learning and
    leverages the `InceptionV3` model to extract features from a frame. Fortunately,
    the `keras` library provides a convenient interface for us to load pre-trained
    models:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将定义我们的特征提取器，它使用迁移学习和利用`InceptionV3`模型从帧中提取特征。幸运的是，`keras`库为我们提供了一个方便的接口来加载预训练的模型：
- en: '[PRE12]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we will write another function that parses the entire dataset, generates
    a sequence of frames, extracts features from each frame, and generates a tuple
    of the feature sequence and the label (real or fake) for the video:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将编写另一个函数，该函数解析整个数据集，生成一系列帧，从每个帧中提取特征，并为视频生成特征序列和标签（真实或伪造）的元组：
- en: '[PRE13]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'To train the model, we need to split the available data into training and test
    sets (note that the test set provided does not come with labels, so we will not
    be able to evaluate our model on it). We will use our preprocessing function to
    prepare the data and convert it into the required form:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练模型，我们需要将可用的数据分成训练集和测试集（注意，提供的测试集没有标签，因此我们无法在它上面评估我们的模型）。我们将使用我们的预处理函数来准备数据并将其转换为所需的形式：
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we can define a model and use this data to train it. Here, we will use
    the pre-trained model only as a feature extractor. The extracted features will
    be passed to a downstream model:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以定义一个模型并使用这些数据来训练它。在这里，我们只将预训练模型用作特征提取器。提取的特征将被传递到下游模型：
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Here, we have used `8` neurons. The last layer contains a single neuron with
    a sigmoid activation. You can examine the architecture by printing it out:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了`8`个神经元。最后一层包含一个具有sigmoid激活的单个神经元。你可以通过打印出来检查架构：
- en: '[PRE16]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that, here, we don’t use softmax but sigmoid instead. Why do we do this?
    What we have here is a binary classification problem; the sigmoid will give us
    the probability that the sequence of frames is a deepfake. If we had a multi-class
    classification, we would have used softmax instead. Remember that softmax is just
    sigmoid generalized to multiple classes.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这里，我们不使用softmax，而是使用sigmoid。我们为什么要这样做？我们这里有一个二分类问题；sigmoid会给我们序列帧是deepfake的概率。如果我们有一个多分类问题，我们会使用softmax。记住，softmax只是sigmoid推广到多个类别。
- en: 'Finally, we train the model. As we use Keras, training is as simple as calling
    the `fit()` function on the model and passing it the required data:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们训练模型。由于我们使用Keras，训练就像在模型上调用`fit()`函数并将所需数据传递给它一样简单：
- en: '[PRE17]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You should be able to observe the training of the model through each epoch.
    At every epoch, you can see the loss of the model both over the training data
    and validation data.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该能够通过每个epoch观察到模型的训练。在每一个epoch中，你都可以看到模型在训练数据和验证数据上的损失。
- en: 'After we have the trained model, we can run the test examples through it and
    obtain the model prediction. We will obtain the output probability, and if it
    is greater than our threshold (generally, `0.5`), we classify it as fake (label
    = `1`):'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们获得训练好的模型后，我们可以将测试示例运行通过它，并获取模型预测。我们将获得输出概率，如果它大于我们的阈值（通常，`0.5`），我们将其分类为伪造（标签
    = `1`）：
- en: '[PRE18]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: To examine the performance of our model, we can compare the actual and predicted
    labels by generating a confusion matrix.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检验我们模型的性能，我们可以通过生成混淆矩阵来比较实际标签和预测标签。
- en: Playing with the model
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 玩转模型
- en: Recall that in the last section on detecting deepfake images, we included pointers
    to playing around with the model and tracking performance. We will do the same
    here for deepfake video classification as well. While such a task may seem redundant,
    given that we already have a model, in the real world and industry, model tuning
    is an essential task that data scientists have to handle.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，在上一个关于检测deepfake图像的章节中，我们包括了玩转模型和跟踪性能的提示。我们也将在这里为deepfake视频分类做同样的事情。虽然这样的任务可能看起来是多余的，因为我们已经有了模型，但在现实世界和行业中，模型调优是数据科学家必须处理的一项基本任务。
- en: 'As a learning exercise, you should experiment with the model and determine
    what the best set of parameters you can have is. Here are some of the things you
    can experiment with:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 作为学习练习，你应该尝试模型并确定你可以拥有的最佳参数集。以下是一些你可以尝试的事情：
- en: '**Image processing**: Note that we parsed videos into frames and resized each
    image to a 224 x 224 shape. What happens if this shape is changed? How do extremely
    large sizes (1,048 x 1,048) or extremely small sizes (10 x 10) affect the model?
    Is the performance affected?'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像处理**：注意，我们将视频解析为帧，并将每个图像调整为224 x 224的形状。如果这个形状改变会发生什么？极端大的尺寸（1,048 x 1,048）或极端小的尺寸（10
    x 10）会如何影响模型？性能会受到什么影响？'
- en: '**Data parameters**: In the preceding walkthrough, while preprocessing the
    data, we set two important parameters. The first is the *maximum sequence length*
    that determines how many frames we consider. The second is the *number of features*
    that controls the length of the feature vector extracted from the pre-trained
    model. How is the model affected if we change these parameters – both individually
    and in conjunction with one another?'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据参数**：在前面的操作过程中，在预处理数据时，我们设置了两个重要的参数。第一个是*最大序列长度*，它决定了我们考虑多少个帧。第二个是*特征数量*，它控制从预训练模型中提取的特征向量的长度。如果我们改变这些参数——无论是单独还是相互结合——会对模型产生什么影响？'
- en: '**Feature extraction**: We used the paradigms of transfer learning and a pre-trained
    model to process our input and give us our features. InceptionV3 is one model;
    however, there are several others that can be used. Do any other models result
    in better features (as evidenced by better model performance)?'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**特征提取**：我们使用了迁移学习和预训练模型来处理我们的输入并给出我们的特征。InceptionV3 是一个模型；然而，还有其他几个模型可以使用。是否有其他模型能产生更好的特征（如更好的模型性能所示）？'
- en: '**Model architecture**: The number of GRU layers, fully connected layers, and
    the neurons in each layer were chosen arbitrarily. What happens if, instead of
    32-16-8 GRU layers, we choose 16-8-8? How about 8-8-8? How about 5 layers, each
    with 16 neurons (i.e., 16-16-16-16-16)? Similar experiments can be done with the
    fully connected layers as well. The possibilities here are endless.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型架构**：GRU 层数、全连接层以及每层的神经元数量都是任意选择的。如果我们不选择 32-16-8 的 GRU 层，而是选择 16-8-8 会怎样？8-8-8
    又如何？5 层，每层 16 个神经元（即 16-16-16-16-16）又会怎样？同样，也可以对全连接层进行类似的实验。这里的可能性是无限的。'
- en: '**Training**: We trained our model over 10 epochs. What happens if we train
    for only 1 epoch? How about 50? 100? Intuitively, you would expect that more epochs
    leads to better performance, but is that really the case? Additionally, our prediction
    threshold is set to 0.5\. How are the precision and recall affected if we vary
    this threshold?'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练**：我们在 10 个时期上训练了我们的模型。如果我们只训练 1 个时期会怎样？50 个？100 个？直观上，你会期望更多的时期会导致更好的性能，但这是否真的是这样？此外，我们的预测阈值设置为
    0.5。如果我们改变这个阈值，精确度和召回率会受到怎样的影响？'
- en: This brings us to the end of our discussion on deepfake videos.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们的关于深度伪造视频的讨论达到了终点。
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we studied deepfakes, which are synthetic media (images and
    videos) that are created using deep neural networks. These media often show people
    in positions that they have not been in and can be used for several nefarious
    purposes, including misinformation, fraud, and pornography. The impact can be
    catastrophic; deepfakes can cause political crises and wars, cause widespread
    panic among the public, facilitate identity theft, and cause defamation and loss
    of life. After understanding how deepfakes are created, we focused on detecting
    them. First, we used CNNs to detect deepfake images. Then, we developed a model
    that parsed deepfake videos into frames and used transfer learning to convert
    them into vectors, the sequence of which was used for fake or real classification.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们研究了深度伪造，这是一种使用深度神经网络创建的合成媒体（图像和视频）。这些媒体通常显示人们不在的位置，可以用于多种恶意目的，包括虚假信息、欺诈和色情。影响可能是灾难性的；深度伪造可以引发政治危机和战争，在公众中引起广泛恐慌，促进身份盗窃，并导致诽谤和生命损失。在了解深度伪造是如何创建的之后，我们专注于检测它们。首先，我们使用卷积神经网络（CNNs）来检测深度伪造图像。然后，我们开发了一个模型，将深度伪造视频解析成帧，并使用迁移学习将它们转换为向量，这些向量的序列被用于伪造或真实分类。
- en: Deepfakes are a growing challenge and have tremendous potential for cybercrime.
    There is a strong demand in the industry for professionals who understand deepfakes,
    their generation, the social impact they can have, and most importantly, methods
    to counter them. This chapter provided you with a deep understanding of deepfakes
    and equips you with the tools and technology needed to detect them.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 深度伪造是一个日益增长的挑战，并为网络犯罪提供了巨大的潜力。行业对理解深度伪造、它们的生成、它们可能产生的社会影响以及最重要的是，对抗它们的方法的专家有强烈的需求。本章为您提供了对深度伪造的深入理解，并为您提供了检测它们的工具和技术。
- en: In the next chapter, we will look at the text counterpart of deepfake images
    – machine-generated text.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨深度伪造图像的文本对应物——机器生成的文本。
