- en: Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚类
- en: Congratulations! You have finished this book's introductory section, in which
    you have explored a great number of topics, and if you were able to follow it,
    you are prepared to start the journey of understanding the inner workings of many
    machine learning models.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你已经完成了这本书的引言部分，其中你探索了许多主题，如果你能够跟上，你就准备好开始了解许多机器学习模型的内部运作之旅了。
- en: In this chapter, we will explore some effective and simple approaches for automatically
    finding interesting data conglomerates, and so begin to research the reasons for
    natural groupings in data.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨一些有效且简单的方法来自动发现有趣的数据聚集体，并开始研究数据自然分组的原因。
- en: 'This chapter will covers the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: A line-by-line implementation of an example of the K-means algorithm, with explanations
    of the data structures and routines
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K-means算法的一个逐行实现的示例，包括对数据结构和例程的解释
- en: A thorough explanation of the **k-nearest neighbors (K-NN)** algorithm, using
    a code example to explain the whole process
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对**k-最近邻（K-NN）**算法的详细解释，使用代码示例来解释整个过程
- en: Additional methods of determining the optimal number of groups representing
    a set of samples
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定代表一组样本的最佳分组数量的额外方法
- en: Grouping as a human activity
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分组作为一种人类活动
- en: Humans typically tend to agglomerate everyday elements into groups of similar
    features. This feature of the human mind can also be replicated by an algorithm.
    Conversely, one of the simplest operations that can be initially applied to any
    unlabeled dataset is to group elements around common features.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人类通常倾向于将日常元素聚集到具有相似特征的组中。人类思维的这个特性也可以通过算法来复制。相反，可以最初应用于任何未标记数据集的最简单操作之一是将元素分组到共同特征周围。
- en: As we have described, in this stage of the development of the discipline, clustering
    is taught as an introductory theme that's applied to the simplest categories of
    element sets.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所描述的，在这个学科发展的阶段，聚类被教授为一个入门主题，应用于元素集合的最简单类别。
- en: But as an author, I recommend researching this domain, because the community
    is hinting that the current model's performance will all reach a plateau, before
    aiming for the full generalization of tasks in AI. And what kinds of method are
    the main candidates for the next stages of crossing the frontier towards AI? Unsupervised
    methods, in the form of very sophisticated variations of the methods explained
    here.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但作为一个作者，我建议研究这个领域，因为社区暗示当前模型的表现将都会达到一个平台期，在追求人工智能任务的全局泛化之前。那么，哪些方法将是跨越人工智能前沿下一阶段的主要候选者？形式上非常复杂的方法，正如这里所解释的方法。
- en: But let's not digress for now, and let's begin with the simplest of grouping
    criteria, the distance to a common center, which is called **K-means**.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但现在我们不要偏离主题，让我们从最简单的分组标准开始，即到共同中心的距离，这被称为**K-means**。
- en: Automating the clustering process
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自动化聚类过程
- en: 'The grouping of information for clustering follows a common pattern for all
    techniques. Basically, we have an initialization stage, followed by the iterative
    insertion of new elements, after which the new group relationships are updated.
    This process continues until the stop criteria is met, where the group characterization
    is finished. The following flow diagram illustrates this process:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 用于聚类的信息分组遵循所有技术的一个共同模式。基本上，我们有一个初始化阶段，随后是迭代插入新元素，之后更新新的分组关系。这个过程一直持续到满足停止标准，此时分组特征化完成。以下流程图说明了这个过程：
- en: '![](img/e8b3b8ac-4c99-4c46-85f3-2cc3a19196df.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e8b3b8ac-4c99-4c46-85f3-2cc3a19196df.png)'
- en: General scheme for a clustering algorithm
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法的一般方案
- en: After we get a clear sense of the overall process, let's start working with
    several cases where this scheme is applied, starting with **K-means**.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对整体过程有了清晰的认识之后，让我们开始处理一些应用了这种方案的案例，从**K-means**开始。
- en: Finding a common center - K-means
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找共同中心 - K-means
- en: Here we go! After some necessary preparation review, we will finally start to
    learn from data; in this case, we are looking to label data we observe in real
    life.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始了！经过一些必要的准备复习后，我们最终将开始从数据中学习；在这种情况下，我们希望对现实生活中观察到的数据进行标记。
- en: 'In this case, we have the following elements:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们有以下元素：
- en: A set of N-dimensional elements of numeric type
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一组N维数值类型的元素
- en: A predetermined number of groups (this is tricky because we have to make an
    educated guess)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预先确定的小组数量（这有点棘手，因为我们不得不做出一个有根据的猜测）
- en: A set of common representative points for each group (called **centroids**)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个组的一组常见代表点（称为**质心**）
- en: The main objective of this method is to split the dataset into an arbitrary
    number of clusters, each of which can be represented by the mentioned centroids.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的主要目标是把数据集分成任意数量的簇，每个簇都可以用提到的质心表示。
- en: 'The word centroid comes from the mathematics world, and has been translated
    to calculus and physics. Here we find a classical representation of the analytical
    calculation of a triangle''s centroid:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: “质心”这个词来自数学领域，已经被翻译到微积分和物理学中。在这里，我们找到了三角形质心分析计算的经典表示：
- en: '![](img/5c4e6e22-e42e-40f1-ac57-e3e33a1d3fb3.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5c4e6e22-e42e-40f1-ac57-e3e33a1d3fb3.png)'
- en: Graphical depiction of the centroid finding scheme for a triangle
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 三角形质心寻找方案的图形描述
- en: 'The centroid of a finite set of *k* points, *x[1 ,]x[2], ..., x[k]* in *R^n*,
    is as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在R^n空间中，有限个点集的质心，*x[1 ,]x[2], ..., x[k]*，如下所示：
- en: '![](img/e3fa38b1-903b-45f5-8b83-9110e30a4696.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](img/e3fa38b1-903b-45f5-8b83-9110e30a4696.png)'
- en: Analytic definition of centroid
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 质心的解析定义
- en: So, now that we have defined this central metric, let's ask the question, "*What
    does it have to do with the grouping of data elements?*"
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们已经定义了这个中心度量，让我们提出问题，“*这与数据元素的分组有什么关系？*”
- en: To answer this, we must first understand the concept of **distance to a centroid.**
    Distance has many definitions, which could be linear, quadratic, and other forms.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们首先必须理解**到质心的距离**的概念。距离有许多定义，可以是线性的、二次的，以及其他形式。
- en: 'So, let''s review some of the main distance types, and then we will mention
    which one is normally used:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们回顾一下主要的距离类型，然后我们将提到通常使用哪一种：
- en: In this review, we will work with 2D variables when defining the measure types,
    as a mean of simplification.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这篇综述中，当我们定义度量类型时，我们将使用二维变量，作为一种简化的手段。
- en: 'Let''s take a look at the following distance types:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看以下几种距离类型：
- en: '**Euclidean distance:** This distance metric calculates the distance in the
    form of a straight line between two points, or has the following formula:'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欧几里得距离**：这种距离度量计算两点之间的直线距离，或者具有以下公式：'
- en: '![](img/1878fb81-cfab-4c9a-8d97-a795402cd224.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1878fb81-cfab-4c9a-8d97-a795402cd224.png)'
- en: '**Chebyshev distance**: This distance is equivalent to the maximum distance,
    along any of the axes. It''s also called the **chess** distance, because it gives
    the minimum quantity of moves a king needs to get from the initial point to the
    final point. Its defined by the following formula:'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**切比雪夫距离**：这种距离等于沿任何轴的最大距离。它也被称为**棋盘**距离，因为它给出了国王从初始点到最终点所需的最小移动次数。其定义为以下公式：'
- en: '![](img/370b76dd-41f1-4534-b64b-02e57756ac1d.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![](img/370b76dd-41f1-4534-b64b-02e57756ac1d.png)'
- en: '**Manhattan distance**: This distance is the equivalent to going from one point
    to another in a city, with unit squares. This L1-type distance sums the number
    of horizontal units advanced, and the number of vertical ones. Its formula is
    as follows:'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿距离**：这种距离相当于在具有单位方格的城市中从一个点到另一个点的距离。这种L1型距离总结了水平前进的单位数和垂直的单位数。其公式如下：'
- en: '![](img/1f39a02e-79b1-4e3d-b620-4899a5a8e9ae.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](img/1f39a02e-79b1-4e3d-b620-4899a5a8e9ae.png)'
- en: 'The following diagram further explains the formulas for the different types
    of distance:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图表进一步解释了不同类型距离的公式：
- en: '![](img/864586ba-845d-4477-b165-21f2c4472c38.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](img/864586ba-845d-4477-b165-21f2c4472c38.png)'
- en: Graphical representation of some of the most well known distance types
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一些最著名的距离类型的图形表示
- en: The distance metric chosen for K-means is the Euclidean distance, which is easy
    to compute and scales well to many dimensions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: K-means选择的距离度量是欧几里得距离，它易于计算，并且很好地扩展到多个维度。
- en: 'Now that we have all the elements, it''s time to define the criteria we will
    use to define which label we will assign to any given sample. Let''s summarize
    the learning rule with the following statement:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了所有元素，是时候定义我们将用来定义我们将分配给任何给定样本的标签的标准了。让我们用以下陈述总结学习规则：
- en: '"A sample will be assigned to the group represented by the closest centroid."'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: “样本将被分配到由最近的质心表示的组。”
- en: The goal of this method is to minimize the sum of squared distances from the
    cluster’s members to the actual centroid of all clusters that contain samples.
    This is also known as **minimization of inertia.**
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的目的是最小化从簇成员到包含样本的所有簇的实际质心的平方距离之和。这也被称为**惯性最小化**。
- en: 'In the following diagram, we can see the results of a typical K-means algorithm
    applied to a sample population of blob-like groups, with a preset number of clusters
    of 3:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们可以看到将典型的K-means算法应用于类似blob的样本群体的结果，预设的簇数为3：
- en: '![](img/0bedcf60-5ed3-453b-91d1-aa195356380f.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0bedcf60-5ed3-453b-91d1-aa195356380f.png)'
- en: Typical result of a clustering process using K-means, with a seed of 3 centroids
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 使用3个质心种子进行K-means聚类的典型聚类过程结果
- en: K-means is a simple and effective algorithm that can be used to obtain a quick
    idea of how a dataset is organized. Its main differentiation is that objects belonging
    to the same class will share a common distance center, which will be incrementally
    upgraded with the addition of each new sample.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: K-means是一种简单而有效的算法，可以用来快速了解数据集是如何组织的。其主要区别在于属于同一类的对象将共享一个共同的距离中心，这个中心将随着每个新样本的添加而逐步升级。
- en: Pros and cons of K-means
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means的优缺点
- en: 'The advantages of this method are as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点如下：
- en: It scales very well (most of the calculations can be run in parallel)
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有良好的可扩展性（大多数计算可以并行运行）
- en: It has been used in a very large range of applications
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它已经被应用于非常广泛的领域
- en: 'But simplicity also has some costs (the no silver bullet rule applies):'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 但简单也有其代价（没有银弹规则适用）：
- en: It requires a priori knowledge (the number of possible clusters should be known
    beforehand)
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它需要先验知识（簇的可能数量应该事先知道）
- en: The outlier values can skew the values of the centroids, as they have the same
    weight as any other sample
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异常值可能会扭曲质心的值，因为它们与任何其他样本具有相同的权重
- en: As we assume that the figure is convex and isotropic, it doesn’t work very well
    with non blob alike clusters
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们假设该图是凸的和各向同性的，因此它与非blob相似的簇不太适用。
- en: K-means algorithm breakdown
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means算法分解
- en: 'The mechanism of the K-means algorithm can be summarized with the following
    flowchart:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: K-means算法的机制可以用以下流程图来概括：
- en: '![](img/f85cd455-dc0a-4cff-8f3b-db941c744c8b.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f85cd455-dc0a-4cff-8f3b-db941c744c8b.png)'
- en: Flowchart of the K-means process
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: K-means过程流程图
- en: 'Let''s describe the process in more detail:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地描述这个过程：
- en: We start with the unclassified samples and take K elements as the starting centroids.
    There are also possible simplifications of this algorithm that take the first
    element in the element list for the sake of brevity.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从未分类的样本开始，并取K个元素作为起始质心。为了简洁起见，也有可能简化这个算法，取元素列表中的第一个元素。
- en: We then calculate the distances between the samples and the first chosen samples,
    and so we get the first calculated centroids (or other representative values).
    You can see in the moving centroids in the illustration a shift toward a more
    intuitive (and mathematically correct) center location.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们然后计算样本与第一个选择的样本之间的距离，从而得到第一次计算出的质心（或其他代表性值）。您可以在插图中的移动质心中看到向更直观（并且数学上正确）的中心位置的偏移。
- en: After the centroids change, their displacement will cause the individual distances
    to change, and so the cluster membership may change.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在质心改变后，它们的位移将导致单个距离的变化，因此簇成员资格可能会改变。
- en: This is the time when we recalculate the centroids and repeat the first steps,
    in case the stop condition isn’t met.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们重新计算质心并重复第一步的时候，以防停止条件没有满足。
- en: 'The stopping conditions can be of various types:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 停止条件可以是各种类型：
- en: After n-iterations. It could be that either we chose a very large number, and
    we’ll have unnecessary rounds of computing, or it could converge slowly, and we
    will have very unconvincing results if the centroid doesn't have a very stable
    mean.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在n次迭代之后。可能的情况是，我们选择了一个非常大的数字，这将导致不必要的计算轮次，或者它可能收敛得很慢，如果质心没有非常稳定的均值，我们将得到非常不可信的结果。
- en: A possible better criterion for the convergence of the iterations is to take
    a look at the changes of the centroids, whether in total displacement or total
    cluster element switches. The last one is employed normally, so we will stop the
    process once there are no more elements changing from their current cluster to
    another one.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于迭代的收敛性，一个可能的更好的标准是查看质心的变化，无论是总的位移还是总的簇元素交换。通常使用后者，因此一旦没有更多的元素从当前簇转移到另一个簇，我们将停止这个过程。
- en: The N iterations condition could also be used as a last resort, because it could
    lead to very long processes where no observable change is observed on a large
    number of iterations.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: N次迭代条件也可以作为最后的手段，因为它可能导致非常长的过程，在大量迭代中观察不到任何可观察的变化。
- en: 'Let''s try to summarize the process of K-NN clustering visually, going through
    a few steps and looking at how the clusters evolve through time:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试通过几个步骤直观地总结K-NN聚类的过程，观察聚类随时间的变化：
- en: '![](img/ce222ab9-8ca8-44e8-a91f-45ef5c908a75.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/ce222ab9-8ca8-44e8-a91f-45ef5c908a75.png)'
- en: Graphical example of the cluster reconfiguration loop
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类重构循环的图形示例
- en: In subfigure 1, we start seeding the clusters with possible centroids at random
    places, to which we assign the closest data elements; and then in subfigure 2,
    we reconfigure the centroids to the center of the new clusters, which in turn
    reconfigures the clusters again (subfigure 3), until we reach a stationary status.
    The element aggregation could also be made element by element, which will trigger
    softer reconfigurations. This will be the implementation strategy for the practical
    part of this chapter.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在子图1中，我们开始在随机位置种下可能的质心，并将最接近的数据元素分配给它们；然后在子图2中，我们将质心重新配置为新聚类的中心，这反过来又重新配置了聚类（子图3），直到我们达到稳定状态。元素聚合也可以逐个进行，这将触发更柔和的重配置。这将是本章实际部分的实现策略。
- en: K-means implementations
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-means实现
- en: In this section, we will review the concept of K-means with a practical sample,
    from the very basic concepts.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将通过实际样本从基本概念开始回顾K-means的概念。
- en: 'First, we will import the libraries we need. In order to improve the understanding
    of the algorithms, we will use the `numpy` library. Then we will use the well-known
    `matplotlib` library for the graphical representation of the algorithms:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将导入所需的库。为了提高对算法的理解，我们将使用`numpy`库。然后我们将使用著名的`matplotlib`库来图形化表示算法：
- en: '[PRE0]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: These will be a number of 2D elements, and will then generate the candidate
    centers, which will be of four 2D elements.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将包含多个二维元素，然后生成候选中心，这些中心将包含四个二维元素。
- en: 'In order to generate a dataset, normally a random number generator is used,
    but in this case we want to set the samples to predetermined numbers, for convenience,
    and also to allow you to repeat these procedures manually:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了生成数据集，通常使用随机数生成器，但在此情况下，我们希望将样本设置为预定的数字，以便于操作，并允许您手动重复这些过程：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Let's represent the sample's center. First, we will initialize a new `matplotlib`
    figure, with the corresponding axes. The `fig` object will allow us to change
    the parameters of all the figures.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们表示样本的中心。首先，我们将初始化一个新的`matplotlib`图形，以及相应的坐标轴。`fig`对象将允许我们更改所有图形的参数。
- en: The `plt` and `ax` variable names are a standarized way to refer to the plot
    in general and one of the plots' axes.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '`plt`和`ax`变量名称是引用图形的一般方式和引用图形的一个轴的标准方式。'
- en: So let’s try to have an idea of what the samples look like. This will be done
    through the `scatter` drawing type of the `matplotlib` library. It takes as parameters
    the *x* coordinates, the *y* coordinates, size (in points squared), the marker
    type, and color.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们尝试了解样本的外观。这将通过`matplotlib`库的`scatter`绘图类型来完成。它接受参数为*x*坐标、*y*坐标、大小（以点平方为单位）、标记类型和颜色。
- en: There are a variety of markers to choose from, such as point (`.`), circle (`o`),
    square (`s`). To see the full list, visit [https://matplotlib.org/api/markers_api.html.](https://matplotlib.org/api/markers_api.html)
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多标记可供选择，例如点（`.`）、圆圈（`o`）、方块（`s`）。要查看完整列表，请访问[https://matplotlib.org/api/markers_api.html.](https://matplotlib.org/api/markers_api.html)
- en: 'Let''s take a look at the following code snippet:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下代码片段：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Let''s now take a look at the following graph:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看一下以下图表：
- en: '![](img/c394d54d-8441-4e7d-a7d7-df3090a154c0.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](img/c394d54d-8441-4e7d-a7d7-df3090a154c0.png)'
- en: Initial clusters status, Centers as black squares
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 初始聚类状态，中心以黑色方块表示
- en: 'Let’s define a function that, given a new sample, will return a list with the
    distances to all the current centroids in order to assign this new sample to one
    of them, and afterward, recalculate the centroids:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数，给定一个新样本，将返回一个包含到所有当前质心的距离的列表，以便将此新样本分配给其中一个，然后重新计算质心：
- en: '[PRE3]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Then we need a function that will build, one by one, the step-by-step graphic
    of our application.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们需要一个函数，该函数将逐步构建我们应用程序的图形。
- en: It expects a maximum of 12 subplots, and the `plotnumber` parameter will determine
    the position on the 6 x 2 matrix (`620` will be the upper-left subplot, 621 the
    following to the right, and so on writing order).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 它期望最多有12个子图，`plotnumber`参数将确定在6 x 2矩阵中的位置（`620`将是左上角的子图，621是右侧的下一个，依此类推）。
- en: 'After that, for each picture we will do a scatterplot of the clustered samples,
    and then of the current centroid position:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，对于每个图像，我们将对聚类样本进行散点图绘制，然后绘制当前质心的位置：
- en: '[PRE4]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The following function, called `kmeans`, will use the previous distance function
    to store the centroid that the samples are assigned to (it will be a number from
    `1` to `K`).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下名为`kmeans`的函数将使用之前的距离函数来存储样本分配到的质心（它将是一个从`1`到`K`的数字）。
- en: The main loop will go from sample `0` to `N`, and for each one, it will look
    for the closest centroid, assign the centroid number to index `n` of the clusters
    array, and sum the samples' coordinates to its currently assigned centroid.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 主循环将从样本`0`到`N`，对于每一个样本，它将寻找最近的质心，将质心编号分配给聚类数组的索引`n`，并将样本坐标加到当前分配的质心上。
- en: 'Then, to get the sample, we use the `bincount` method to count the number of
    samples for each centroid, and by building a `divisor` array, we divide the sum
    of a class elements by the previous `divisor` array, and there we have the new
    centroids:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为了获取样本，我们使用`bincount`方法计算每个质心的样本数量，并通过构建`divisor`数组，我们将一个类别的元素总和除以之前的`divisor`数组，从而得到新的质心：
- en: '[PRE5]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now it’s time to kickstart the K-means process, using the initial samples and
    centers we set up at first. The current algorithm will show how the clusters are
    evolving, starting from a few elements, into their final state:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候启动K-means过程了，使用我们最初设置的初始样本和中心。当前算法将展示聚类如何从少数元素开始，发展到最终状态：
- en: '[PRE6]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Let''s take a look at the following screenshot:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下以下屏幕截图：
- en: '![](img/bccc3f75-1b2f-4dbd-891f-8879acc742d9.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bccc3f75-1b2f-4dbd-891f-8879acc742d9.png)'
- en: Depiction of the clustering process, with the centroids represented as black
    squares
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类过程的描述，其中质心以黑色方块表示
- en: Nearest neighbors
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 最近邻
- en: K-NN is another classical method of clustering. It builds groups of samples,
    supposing that each new sample will have the same class as its neighbors, without
    looking for a global representative central sample. Instead, it looks at the environment,
    looking for the most frequent class on each new sample's environment.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN是另一种经典的聚类方法。它构建样本组，假设每个新样本将与它的邻居具有相同的类别，而不寻找全局代表性的中心样本。相反，它观察环境，寻找每个新样本环境中最频繁的类别。
- en: Mechanics of K-NN
  id: totrans-110
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-NN的原理
- en: K-NN can be implemented in many configurations, but in this chapter we will
    use the semi-supervised approach, starting from a certain number of already assigned
    samples, and later guessing the cluster membership using the main criteria.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN可以在许多配置中实现，但在这章中，我们将使用半监督方法，从一定数量的已分配样本开始，然后使用主要标准猜测聚类成员资格。
- en: 'In the following diagram, we have a breakdown of the algorithm. It can be summarized
    with the following steps:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，我们对算法进行了分解。它可以总结为以下步骤：
- en: '![](img/d9e62a74-25fe-4835-af90-154861815e43.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d9e62a74-25fe-4835-af90-154861815e43.png)'
- en: Flowchart for the K-NN clustering process
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN聚类过程的流程图
- en: 'Let''s go over all the following involved steps, in a simplified form:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以简化的形式回顾以下所有涉及步骤：
- en: We place the previously known samples on the data structures.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将先前已知的样本放置在数据结构中。
- en: We then read the next sample to be classified, and calculate the Euclidean distance
    from the new sample to every sample in the training set.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们读取下一个待分类的样本，并计算新样本与训练集中每个样本的欧几里得距离。
- en: We decide the class of the new element by selecting the class of the nearest
    sample, by Euclidean distance. The K-NN method requires the vote of the K closest
    samples.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过选择最近样本的类别来确定新元素的类别，通过欧几里得距离。K-NN方法需要K个最近样本的投票。
- en: We repeat the procedure until there are no more remaining samples.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们重复此过程，直到没有更多剩余样本。
- en: 'This picture will give us a idea of how the new samples are being added. In
    this case, we use a `K` of `1`, for simplicity:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片将给我们一个关于新样本如何被添加的思路。在这种情况下，我们使用`K`为`1`，以简化：
- en: '![](img/7a5f405c-465a-4427-9cda-db942851fb95.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7a5f405c-465a-4427-9cda-db942851fb95.png)'
- en: Sample application of a K-NN loop
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN循环的示例应用
- en: K-NN can be implemented in more than one of the configurations that we have
    learned, but in this chapter, we will use the semi-supervised approach; we will
    start from a certain number of already assigned samples, and we will later guess
    the cluster membership based on the characteristics of the training set.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: K-NN 可以使用我们所学到的多种配置之一来实现，但在这章中，我们将使用半监督方法；我们将从一个已经分配了一定数量的样本开始，然后根据训练集的特征来猜测簇的成员资格。
- en: Pros and cons of K-NN
  id: totrans-124
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-NN 的优缺点
- en: 'The advantages of this method are as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点如下：
- en: '**Simplicity**: There''s no need to tune parameters'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简单性**：无需调整参数'
- en: '**No formal training needed**: We just need more training examples to improve
    the model'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无需正式训练**：我们只需要更多的训练示例来改进模型'
- en: 'The disadvantage is as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 不利之处如下：
- en: It is computationally expensive - in a naive approach, all distances between
    points and every new sample have to be calculated, except when caching is implemented.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 计算成本高 - 在原始方法中，除了实现缓存时，必须计算所有点与新样本之间的距离。
- en: K-NN sample implementation
  id: totrans-130
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: K-NN 示例实现
- en: For this simple implementation of the K-NN method, we will use the NumPy and
    Matplotlib libraries. Also, as we will be generating a synthetic dataset for better
    comprehension, we will use the `make_blobs` method from scikit-learn, which will
    generate well-defined and separated groups of information so we have a sure reference
    for our implementation.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 K-NN 方法的简单实现，我们将使用 NumPy 和 Matplotlib 库。此外，为了更好地理解，我们将使用 scikit-learn 的 `make_blobs`
    方法生成合成数据集，这将生成定义良好且分离的信息组，以便我们有可靠的实现参考。
- en: 'Importing the required libraries:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 导入所需的库：
- en: '[PRE7]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'So, it''s time to generate the data samples for this example. The parameters
    of `make_blobs` are the number of samples, the number of features or dimensions,
    the quantity of centers or groups, whether the samples have to be shuffled, and
    the standard deviation of the cluster, to control how dispersed the group samples
    are:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在是生成这个示例数据样本的时候了。`make_blobs` 函数的参数包括样本数量、特征或维度的数量、中心或组数、样本是否需要打乱，以及簇的标准差，以控制组样本的分散程度：
- en: '[PRE8]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is a representation of the generated sample blobs:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是生成的样本聚类的表示：
- en: '![](img/994e5376-ce65-4ba5-a01c-5c77a6b94ac0.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/994e5376-ce65-4ba5-a01c-5c77a6b94ac0.png)'
- en: 'Firstly, let''s define our `distance` function, which will be necessary to
    find the neighbors of all the new elements. We basically provide one sample, and
    return the distance between the provided new element and all their counterparts:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义我们的 `distance` 函数，它将用于找到所有新元素的邻居。我们基本上提供一个样本，并返回提供的新元素与所有对应元素之间的距离：
- en: '[PRE9]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The `add_sample` function will receive a new 2D sample, the current dataset,
    and an array marking the group of the corresponding sample (from `0` to `3` in
    this case). In this case, we use `argpartition` to get the indices of the three
    nearest neighbors of the new sample, and then we use them to extract a subset
    of the `features` array. Then, `bincount` will return the count of any of the
    different classes on that three-element subset, and then with `argmax`, we will
    choose the index (in this case, the class number) of the group with the most elements
    in that two-element set:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`add_sample` 函数将接收一个新的二维样本、当前数据集和一个标记相应样本组（在这种情况下为 `0` 到 `3`）的数组。在这种情况下，我们使用
    `argpartition` 来获取新样本最近三个邻居的索引，然后使用它们从 `features` 数组中提取一个子集。然后，`bincount` 将返回该三个元素子集上任何不同类别的计数，然后通过
    `argmax`，我们将选择该两个元素集中元素最多的组的索引（在这种情况下，类号）：'
- en: '[PRE10]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Then we define our main `knn` function, which takes the new data to be added
    and uses the original classified data, represented by the `data` and `features`
    parameters, to decide the classes of the new elements:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义我们的主要 `knn` 函数，它接受要添加的新数据，并使用由 `data` 和 `features` 参数表示的原始分类数据来决定新元素的类别：
- en: '[PRE11]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, it''s time to kickstart the process. For this reason, we define a
    set of new samples in the range of -10, 10 on both the `x` and `y` dimensions,
    and we will call our `knn` routine with it:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，是时候启动这个过程了。因此，我们在 `-10` 到 `10` 的范围内定义了一组新的样本，在 `x` 和 `y` 维度上，然后我们将使用它调用我们的
    `knn` 例程：
- en: '[PRE12]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now it''s time to represent the final results. First, we will represent the
    initial samples, which are much more well-formed than our random values, and then
    our final values, represented by an empty square (`c=''none''`), so that they
    will serve as a marker of those samples:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候表示最终结果了。首先，我们将表示初始样本，它们比我们的随机值形成得更好，然后是我们的最终值，用空方框表示（`c='none'`），这样它们将作为这些样本的标记：
- en: '[PRE13]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let''s take a look at the following graph:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看下面的图表：
- en: '![](img/2fadc21f-1aac-4680-b539-0c6efa4933d6.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/2fadc21f-1aac-4680-b539-0c6efa4933d6.png)'
- en: Final clustering status (new classified items are marked with a square)
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 最终聚类状态（新分类的项目用方块标记）
- en: In the preceding graph, we can see how our simple model of three neighbors works
    well to qualify and reform the grouping as the process advances. As the graph
    shows, the new groupings aren't necessarily of a circular form; they change according
    to the way the incoming data progresses.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图表中，我们可以看到我们的简单模型如何随着过程的推进很好地对分组进行资格认证和改革。如图所示，新的分组不一定呈圆形；它们根据 incoming
    data 的进展方式而变化。
- en: Going beyond the basics
  id: totrans-152
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 超越基础
- en: Now that we are done reviewing illustrative cases of the two main clustering
    techniques, let's explore some more advanced metrics and techniques so we can
    have them in our toolbox.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了对两种主要聚类技术示例的审查，让我们探索一些更高级的指标和技术，这样我们就可以将它们放入我们的工具箱中。
- en: The Elbow method
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 拐点法
- en: One of the questions that may have arisen when implementing K-means could have
    been "how do I know that the target number of clusters is the best or most representative
    for the dataset?"
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现K-means时可能出现的疑问之一可能是“我怎么知道目标簇数量是最佳或对数据集最具代表性的？”
- en: For this task, we have the **Elbow** method. It consists of a unique statistical
    measure of the total group dispersion in a grouping. It works by repeating the
    K-means procedure, using an increasing number of initial clusters, and calculating
    the total intra-cluster internal distance for all the groups.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们有**拐点法**。它由一个独特的统计量组成，用于衡量分组中的总组分散。它通过重复K-means过程，使用不断增加的初始簇数量，并计算所有组的总簇内内部距离来实现。
- en: 'Normally, the method will start with a very high value (except if we start
    with the right number of centroids), and then we will observe that the total intra-cluster
    distance drops quite quickly, until we reach a point where it doesn''t change
    significantly. Congratulations, we have found the Elbow point, so-called for being
    an inflection on the following graph:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，该方法将以一个非常高的值开始（除非我们以正确的质心数量开始），然后我们会观察到总簇内距离会迅速下降，直到达到一个点，它不会显著变化。恭喜，我们找到了所谓的“拐点”，在下面的图表中表现为一个拐点：
- en: '![](img/16f79328-bc5a-400c-86cb-a4a631832f49.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/16f79328-bc5a-400c-86cb-a4a631832f49.png)'
- en: Graphical depiction of the error evolution, as the number of clusters increases,
    and the inflection point.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 随着簇数量的增加，错误演变的图形表示，以及拐点。
- en: Regarding the accuracy of this indicator, the Elbow method is, as you can see,
    a heuristic and not mathematically determined, but can be of use if you want to
    make a quick estimate of the right number of clusters, especially when the curve
    changes very abruptly at some point.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个指标的准确性，正如你所看到的，拐点法是一个启发式方法，而不是数学上确定的，但如果你想要快速估计正确的簇数量，它可能是有用的，尤其是在曲线在某些点突然变化时。
- en: Summary
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we have covered the simplest but still very practical machine
    learning models in an eminently practical way to get us started on the complexity
    scale.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们以极其实用的方式介绍了最简单但仍非常实用的机器学习模型，以便我们在复杂性尺度上开始。
- en: In the following chapter, where we will cover several regression techniques,
    it will be time to go and solve a new type of problem that we have not worked
    on, even if it's possible to solve the problem with clustering methods (regression),
    using new mathematical tools for approximating unknown values. In it, we will
    model past data using mathematical functions, and try to model new output based
    on those modeling functions.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍几种回归技术，那时我们将去解决一种我们尚未处理过的新类型的问题，即使使用聚类方法（回归）通过新的数学工具来近似未知值也是可能的。在它里面，我们将使用数学函数来建模过去的数据，并尝试基于这些建模函数来建模新的输出。
- en: References
  id: totrans-164
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Thorndike, Robert L, *Who belongs in the family?,* Psychometrika18.4 (1953):
    267-276.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Thorndike, Robert L, *Who belongs in the family?,* Psychometrika 18.4 (1953):
    267-276.'
- en: 'Steinhaus, H, *Sur la division des corp materiels en parties.* Bull. Acad.
    Polon. Sci 1 (1956): 801–804.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Steinhaus, H，*关于物质分割成部分的问题*。波兰科学院学报 1 (1956): 801–804。'
- en: MacQueen, James, *Some methods for classification and analysis of multivariate
    observations.* Proceedings of the fifth Berkeley symposium on mathematical statistics
    and probability. Vol. 1\. No. 14\. 1967.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MacQueen, James，*一些用于多元观测分类和分析的方法*。第五次伯克利数学统计与概率研讨会论文集。第 1 卷。第 14 号。1967 年。
- en: '*Cover, Thomas, and Peter Hart*, *Nearest neighbor pattern classification.*
    IEEE transactions on information theory 13.1 (1967): 21-27.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Cover, Thomas, 和 Peter Hart*，*最近邻模式分类*。IEEE 信息系统传输 13.1 (1967): 21-27。'
