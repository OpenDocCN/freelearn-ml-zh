- en: Chapter 13. Tracking Visual Motion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章。跟踪视觉运动
- en: 'In this chapter, we will cover the following recipes:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将涵盖以下菜谱：
- en: Tracing feature points in a video
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在视频中追踪特征点
- en: Estimating the optical flow
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 估计光流
- en: Tracking an object in a video
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在视频中跟踪对象
- en: Introduction
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 简介
- en: Video sequences are interesting because they show scenes and objects in motion.
    The preceding chapter introduced the tools for reading, processing, and saving
    videos. In this chapter, we will look at different algorithms that track the visible
    motion in a sequence of images. This visible or **apparent motion** can be caused
    by objects that move in different directions and at various speeds or by the motion
    of the camera (or a combination of both).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 视频序列很有趣，因为它们展示了运动中的场景和对象。上一章介绍了读取、处理和保存视频的工具。在这一章中，我们将探讨不同的算法，这些算法可以跟踪一系列图像中的可见运动。这种可见的或**明显的运动**可能是由在不同方向和不同速度上移动的对象引起的，或者是由摄像机的运动（或两者的组合）引起的。
- en: Tracking apparent motion is of utmost importance in many applications. It allows
    you to follow specific objects while they are moving in order to estimate their
    speed and determine where they are going. It also permits you to stabilize videos
    taken from handheld cameras by removing or reducing the amplitude of camera jitters.
    Motion estimation is also used in video coding to compress a video sequence in
    order to facilitate its transmission or storage. This chapter will present a few
    algorithms that track the motion in an image sequence, and as we will see, this
    tracking can be achieved either sparsely (that is, at few image locations, this
    is **sparse motion**) or densely (at every pixel of an image, this is **dense
    motion**).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多应用中，跟踪明显的运动至关重要。它允许你在对象移动时跟踪特定的对象，以估计其速度并确定它们将去哪里。它还允许你通过消除或减少手持摄像机拍摄的视频的摄像机抖动来稳定视频。运动估计也用于视频编码，以便压缩视频序列，以便于其传输或存储。本章将介绍一些跟踪图像序列中运动的算法，并且正如我们将看到的，这种跟踪可以是稀疏的（即在少数图像位置，这是**稀疏运动**）或密集的（在图像的每个像素上，这是**密集运动**）。
- en: Tracing feature points in a video
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在视频中追踪特征点
- en: We learned in previous chapters that analyzing an image through some of its
    most distinctive points can lead to effective and efficient computer vision algorithms.
    This is also true for image sequences in which the motion of some interest points
    can be used to understand how the different elements of a captured scene move.
    In this recipe, you will learn how to perform a temporal analysis of a sequence
    by tracking feature points as they move from frame to frame.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节中了解到，通过分析图像的一些最显著点可以导致有效且高效的计算机视觉算法。这在图像序列中也是正确的，其中某些兴趣点的运动可以用来理解捕获场景的不同元素是如何移动的。在这个菜谱中，你将学习如何通过跟踪特征点从一帧移动到另一帧来执行序列的时间分析。
- en: How to do it...
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: To start the tracking process, the first thing to do is to detect the feature
    points in an initial frame. You then try to track these points in the next frame.
    Obviously, since we are dealing with a video sequence, there is a good chance
    that the object, on which the feature points are found, has moved (this motion
    can also be due to camera movement). Therefore, you must search around a point's
    previous location in order to find its new location in the next frame. This is
    what accomplishes the `cv::calcOpticalFlowPyrLK` function. You input two consecutive
    frames and a vector of feature points in the first image; the function then returns
    a vector of new point locations. To track the points over a complete sequence,
    you repeat this process from frame to frame. Note that as you follow the points
    across the sequence, you will unavoidably lose track of some of them such that
    the number of tracked feature points will gradually reduce. Therefore, it could
    be a good idea to detect new features from time to time.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始跟踪过程，首先要做的是在初始帧中检测特征点。然后你尝试在下一帧中跟踪这些点。显然，由于我们处理的是视频序列，所以找到特征点的对象很可能已经移动了（这种运动也可能是由摄像机运动引起的）。因此，你必须在一个点的先前位置周围搜索，以找到它在下一帧中的新位置。这正是`cv::calcOpticalFlowPyrLK`函数所完成的。你输入两个连续帧和第一个图像中的特征点向量；该函数随后返回一个新点位置向量。为了在整个序列中跟踪点，你需要从一帧重复这个过程。请注意，当你跟随点穿越序列时，你不可避免地会失去一些点的跟踪，因此跟踪的特征点数量会逐渐减少。因此，不时地检测新特征可能是一个好主意。
- en: 'We will now take advantage of the video-processing framework we defined in
    [Chapter 12](ch12.html "Chapter 12. Processing Video Sequences") , *Processing
    Video Sequences*, and we will define a class that implements the `FrameProcessor`
    interface introduced in the *Processing the video frames* recipe of this chapter.
    The data attributes of this class include the variables that are required to perform
    both the detection of feature points and their tracking:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将利用我们在[第12章](ch12.html "第12章. 处理视频序列")中定义的视频处理框架，并定义一个实现本章“处理视频帧”配方中引入的`FrameProcessor`接口的类。这个类的数据属性包括执行特征点检测和跟踪所需的变量：
- en: '[PRE0]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define the `process` method that will be called for each frame of
    the sequence. Basically, we need to proceed as follows. First, the feature points
    are detected if necessary. Next, these points are tracked. You reject the points
    that you cannot track or you no longer want to track. You are now ready to handle
    the successfully tracked points. Finally, the current frame and its points become
    the previous frame and points for the next iteration. Here is how to do this:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`process`方法，该方法将为序列中的每一帧调用。基本上，我们需要按以下步骤进行。首先，如果需要，检测特征点。然后，跟踪这些点。你拒绝那些无法跟踪或不再想要跟踪的点。你现在可以处理成功跟踪的点。最后，当前帧及其点成为下一次迭代的上一帧和点。以下是这样做的方法：
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This method makes use of four utility methods. It should be easy for you to
    change any of these methods in order to define a new behavior for your own tracker.
    The first of these methods detects the feature points. Note that we have already
    discussed the `cv::goodFeatureToTrack` function in the first recipe of [Chapter
    8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting Interest Points*:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法使用四个实用方法。你应该很容易更改这些方法中的任何一个，以定义你自己的跟踪器的新行为。这些方法中的第一个用于检测特征点。请注意，我们已经在[第8章](ch08.html
    "第8章. 检测兴趣点")的“检测兴趣点”的第一个配方中讨论了`cv::goodFeatureToTrack`函数：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The second method determines whether new feature points should be detected.
    This will happen when a negligible number of tracked points remain:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法确定是否应该检测新的特征点。这将在跟踪点数量可忽略不计时发生：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The third method rejects some of the tracked points based on a criteria defined
    by the application. Here, we decided to reject the points that do not move (in
    addition to those that cannot be tracked by the `cv::calcOpticalFlowPyrLK` function).
    We consider that non-moving points belong to the background scene and are therefore
    uninteresting:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法根据应用程序定义的标准拒绝了一些跟踪点。在这里，我们决定拒绝那些不移动的点（除了那些无法通过`cv::calcOpticalFlowPyrLK`函数跟踪的点）。我们认为不移动的点属于背景场景，因此没有兴趣：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Finally, the fourth method handles the tracked feature points by drawing all
    the tracked points with a line that joins them to their initial position (that
    is, the position where they were detected the first time) on the current frame:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第四种方法通过在当前帧上用线连接它们到初始位置（即它们第一次被检测到的位置）来处理跟踪的特征点：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A simple main function to track the feature points in a video sequence would
    then be written as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 要在视频序列中跟踪特征点的一个简单主函数可以写成如下：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The resulting program will show you the evolution of the moving tracked features
    over time. Here are, for example, two such frames at two different instants. In
    this video, the camera is fixed. The young cyclist is therefore the only moving
    object. Here is the result that is obtained after a few frames have been processed:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的程序将显示随时间推移移动跟踪特征点的演变。例如，这里有两个不同时刻的两个这样的帧。在这段视频中，摄像机是固定的。因此，年轻的自行车手是唯一的移动对象。以下是经过几帧处理后的结果：
- en: '![How to do it...](img/image_13_001.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_13_001.jpg)'
- en: 'A few seconds later, we obtain the following frame:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，我们得到以下帧：
- en: '![How to do it...](img/image_13_002.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_13_002.jpg)'
- en: How it works...
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 工作原理...
- en: 'To track the feature points from frame to frame, we must locate the new position
    of a feature point in the subsequent frame. If we assume that the intensity of
    the feature point does not change from one frame to the next one, we are looking
    for a displacement `(u,v)` as follows:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要从一帧跟踪到另一帧的特征点，我们必须定位后续帧中特征点的新位置。如果我们假设特征点的强度从一帧到下一帧不会改变，我们正在寻找一个位移`(u,v)`如下：
- en: '![How it works...](img/B05388_13_12.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/B05388_13_12.jpg)'
- en: 'Here, `I[t]` and `I[t+1]` are the current frame and the one at the next instant,
    respectively. This constant intensity assumption generally holds for small displacement
    in images that are taken at two nearby instants. We can then use the Taylor expansion
    in order to approximate this equation by an equation that involves the image derivatives:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`I[t]`和`I[t+1]`分别是当前帧和下一个时刻的帧。这个常数强度假设通常适用于在两个相邻时刻拍摄的图像中的小位移。然后我们可以使用泰勒展开来近似这个方程，通过涉及图像导数的方程来近似这个方程：
- en: '![How it works...](img/B05388_13_13.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/B05388_13_13.jpg)'
- en: 'This latter equation leads us to another equation (as a consequence of the
    constant intensity assumption that cancels the two intensity terms):'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这个后一个方程引出了另一个方程（作为常数强度假设的后果，该假设抵消了两个强度项）：
- en: '![How it works...](img/B05388_13_14.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/B05388_13_14.jpg)'
- en: This constraint is the fundamental **optical flow** constraint equation and
    is known as the **brightness constancy equation**.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这个约束是基本的**光流**约束方程，也称为**亮度恒常方程**。
- en: This constraint is exploited by the so-called **Lukas-Kanade feature tracking**
    algorithm. In addition to using this constraint, the Lukas-Kanade algorithm also
    makes an assumption that the displacement of all the points in the neighborhood
    of the feature point is the same. We can therefore impose the optical flow constraint
    on all these points with a unique `(u,v)` unknown displacement. This gives us
    more equations than the number of unknowns (two), and therefore, we can solve
    this system of equations in a mean-square sense. In practice, it is solved iteratively,
    and the OpenCV implementation also offers us the possibility to perform this estimation
    at a different resolution in order to make the search more efficient and more
    tolerant to a larger displacement. By default, the number of image levels is `3`
    and the window size is `15`. These parameters can obviously be changed. You can
    also specify the termination criteria, which define the conditions that stop the
    iterative search. The sixth parameter of `cv::calcOpticalFlowPyrLK` contains the
    residual mean-square error that can be used to assess the quality of the tracking.
    The fifth parameter contains binary flags that tell us whether tracking the corresponding
    point was considered successful or not.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个约束条件被所谓的**卢卡斯-卡纳德特征跟踪**算法所利用。除了使用这个约束条件外，卢卡斯-卡纳德算法还假设特征点周围所有点的位移是相同的。因此，我们可以对所有这些点施加一个独特的`(u,v)`未知位移的光流约束。这给我们带来了比未知数（两个）更多的方程，因此，我们可以以均方误差的方式来解这个方程组。在实践中，它是通过迭代来解决的，OpenCV的实现也为我们提供了在不同分辨率下执行此估计的可能性，以使搜索更有效，更能容忍较大的位移。默认情况下，图像级别数为`3`，窗口大小为`15`。这些参数显然是可以改变的。您还可以指定终止条件，这些条件定义了停止迭代搜索的条件。`cv::calcOpticalFlowPyrLK`的第六个参数包含用于评估跟踪质量的残差均方误差。第五个参数包含二进制标志，告诉我们跟踪相应的点是否被认为是成功的。
- en: The preceding description represents the basic principles behind the Lukas-Kanade
    tracker. The current implementation contains other optimizations and improvements
    that make the algorithm more efficient in the computation of the displacement
    of a large number of feature points.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 上述描述代表了卢卡斯-卡纳德跟踪器背后的基本原理。当前的实现包含其他优化和改进，使算法在计算大量特征点位移时更加高效。
- en: See also
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: '[Chapter 8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting
    Interest Points*, where there is a discussion on feature point detection'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[第8章](ch08.html "第8章. 检测兴趣点")，*检测兴趣点*，其中讨论了特征点检测'
- en: The *Tracking an object in a video* recipe of this chapter uses feature point
    tracking in order to track objects
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的**视频中的物体跟踪**配方使用特征点跟踪来跟踪物体
- en: The classic article by *B. Lucas* and *T. Kanade*, *An Iterative Image Registration
    Technique with an Application to Stereo Vision*, at the *Int. Joint Conference
    in **Artificial Intelligence*, pp. 674-679, 1981, describes the original feature
    point tracking algorithm
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由*B. 卢卡斯*和*T. 卡纳德*撰写的经典文章，*一种迭代图像配准技术及其在立体视觉中的应用*，在*国际人工智能联合会议*上，第674-679页，1981年，描述了原始的特征点跟踪算法
- en: The article by J. Shi and C. Tomasi, *Good Features to Track*, at the *IEEE
    Conference on Computer Vision and Pattern Recognition*, pp. 593-600, 1994, describes
    an improved version of the original feature point tracking algorithm
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: J. Shi和C. Tomasi在1994年的*IEEE计算机视觉和模式识别会议*上发表的文章《Good Features to Track》描述了原始特征点跟踪算法的一个改进版本
- en: Estimating the optical flow
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计光流
- en: When a scene is observed by a camera, the observed brightness pattern is projected
    on the image sensor and thus forms an image. In a video sequence, we are often
    interested in capturing the motion pattern, that is the projection of the 3D motion
    of the different scene elements on an image plane. This image of projected 3D
    motion vectors is called the **motion field**. However, it is not possible to
    directly measure the 3D motion of scene points from a camera sensor. All we observe
    is a brightness pattern that is in motion from frame to frame. This apparent motion
    of the brightness pattern is called the **optical flow**. One might think that
    the motion field and optical flow should be equal, but this is not always true.
    An obvious case would be the observation of a uniform object; for example, if
    a camera moves in front of a white wall, then no optical flow is generated.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个场景被相机观察时，观察到的亮度模式被投影到图像传感器上，从而形成一个图像。在视频序列中，我们通常对捕捉运动模式感兴趣，即不同场景元素在图像平面上的3D运动的投影。这个投影3D运动矢量的图像被称为**运动场**。然而，从相机传感器直接测量场景点的3D运动是不可能的。我们所观察到的只是从一帧到另一帧的亮度模式在运动。这种亮度模式的明显运动被称为**光流**。有人可能会认为运动场和光流应该是相等的，但事实并非总是如此。一个明显的例子是观察一个均匀的物体；例如，如果相机在白色墙壁前移动，则不会产生光流。
- en: 'Another classical example is the illusion produced by a rotating barber pole:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个经典例子是由旋转的理发店旗杆产生的错觉：
- en: '![Estimating the optical flow](img/image_13_006.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![估计光流](img/image_13_006.jpg)'
- en: In this case, the motion field should show motion vectors in the horizontal
    direction as the vertical cylinder rotates around its main axis. However, observers
    perceive this motion as red and blue strips moving up and this is what the optical
    flow will show. In spite of these differences, the optical flow is considered
    to be a valid approximation of the motion field. This recipe will explain how
    the optical flow of an image sequence can be estimated.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，运动场应该显示在水平方向上的运动矢量，因为垂直圆柱围绕其主轴旋转。然而，观察者将这种运动感知为向上移动的红蓝条纹，这就是光流将显示的内容。尽管存在这些差异，但光流被认为是对运动场的有效近似。这个菜谱将解释如何估计图像序列的光流。
- en: Getting ready
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: Estimating the optical flow means quantifying the apparent motion of the brightness
    pattern in an image sequence. So let's consider one frame of the video at one
    given instant. If we look at one particular pixel `(x,y)` on the current frame,
    we would like to know where this point is moving in the subsequent frames. That
    is to say that the coordinates of this point are moving over time-a fact that
    can be expressed as `(x(t),y(t))`-and our goal is to estimate the velocity of
    this point `(dx/dt,dy/dt)`. The brightness of this particular point at time `t`
    can be obtained by looking at the corresponding frame of the sequence, that is,
    `I(x(t),y(t),t)`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 估计光流意味着量化图像序列中亮度模式的明显运动。所以让我们考虑视频在某一给定时刻的一帧。如果我们观察当前帧上的一个特定像素`(x,y)`，我们想知道这个点在后续帧中移动的位置。也就是说，这个点的坐标随时间变化——这可以用`(x(t),y(t))`来表示——我们的目标是估计这个点的速度`(dx/dt,dy/dt)`。在时间`t`时，这个特定点的亮度可以通过查看序列中的对应帧来获得，即`I(x(t),y(t),t)`。
- en: 'From our **image brightness constancy** assumption, we can write that the brightness
    of this point does not vary with respect to time:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的**图像亮度恒常**假设出发，我们可以写出这个点的亮度不会随时间变化：
- en: '![Getting ready](img/B05388_13_15.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B05388_13_15.jpg)'
- en: 'The chain rule allows us to write the following:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 链式法则允许我们写出以下内容：
- en: '![Getting ready](img/B05388_13_16.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![准备工作](img/B05388_13_16.jpg)'
- en: This equation is known as the **brightness constancy equation** and it relates
    the optical flow components (the derivatives of `x` and `y` with respect to time)
    with the image derivatives. This is exactly the equation we derived in the previous
    recipe; we simply demonstrated it differently.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程被称为**亮度恒常方程**，它将光流分量（`x`和`y`随时间的导数）与图像导数相关联。这正是我们在前一个菜谱中推导出的方程；我们只是以不同的方式演示了它。
- en: 'This single equation (composed of two unknowns) is however insufficient to
    compute the optical flow at a pixel location. We therefore need to add an additional
    constraint. A common choice is to assume the smoothness of the optical flow, which
    means that the neighboring optical flow vectors should be similar. Any departure
    from this assumption should therefore be penalized. One particular formulation
    for this constraint is based on the Laplacian of the optical flow:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这个单一方程（由两个未知数组成）然而不足以计算像素位置处的光流。因此，我们需要添加一个额外的约束。一个常见的选择是假设光流的平滑性，这意味着相邻的光流向量应该是相似的。因此，任何偏离这个假设的情况都应该受到惩罚。这个约束的一个特定公式是基于光流的拉普拉斯算子：
- en: '![Getting ready](img/equation.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![准备就绪](img/equation.jpg)'
- en: The objective is therefore to find the optical flow field that minimizes both
    the deviations from the brightness constancy equation and the Laplacian of the
    flow vectors.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，目标是找到最小化亮度恒常方程和光流向量拉普拉斯算子偏差的光流场。
- en: How to do it...
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何做...
- en: 'Several approaches have been proposed to solve the dense optical flow estimation
    problem, and OpenCV implements a few of them. Let''s use the `cv::DualTVL1OpticalFlow`
    class that is built as a subclass of the generic `cv::Algorithm` base class. Following
    the implemented pattern, the first thing to do is to create an instance of this
    class and obtain a pointer to it:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 已经提出了几种方法来解决密集光流估计问题，OpenCV实现了其中的一些。让我们使用`cv::DualTVL1OpticalFlow`类，它被构建为通用`cv::Algorithm`基类的子类。按照实现的模式，首先要做的是创建这个类的实例并获取它的指针：
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Since the object we just created is in a ready-to-use state, we simply call
    the method that calculates an optical flow field between the two frames:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们刚刚创建的对象处于可使用状态，我们只需调用计算两个帧之间光流场的方法：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The result is an image of 2D vectors (`cv::Point`) that represents the displacement
    of each pixel between the two frames. In order to display the result, we must
    therefore show these vectors. This is why we created a function that generates
    an image map for an optical flow field. To control the visibility of the vectors,
    we used two parameters. The first one is a stride value that is defined such that
    only one vector over a certain number of pixels will be displayed. This stride
    makes space for the display of the vectors. The second parameter is a scale factor
    that extends the vector length to make it more apparent. Each drawn optical flow
    vector is then a simple line that ends with a plain circle to symbolize the tip
    of an arrow. Our mapping function is therefore as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个表示两个帧之间每个像素位移的二维向量（`cv::Point`）的图像。为了显示结果，我们必须因此显示这些向量。这就是为什么我们创建了一个用于生成光流场图像映射的函数。为了控制向量的可见性，我们使用了两个参数。第一个参数是一个步长值，它被定义为在一定数量的像素中只显示一个向量。这个步长为向量的显示留出了空间。第二个参数是一个缩放因子，它扩展了向量的长度，使其更明显。然后，每个绘制的光流向量都是一个简单的线，以一个普通的圆圈结束，以象征箭头的尖端。因此，我们的映射函数如下：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Consider the following two frames:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下两个帧：
- en: '![How to do it...](img/image_13_010.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_13_010.jpg)'
- en: 'If these frames are used, then the estimated optical flow field can be visualized
    by calling our drawing function:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用这些帧，则可以通过调用我们的绘图函数来可视化估计的光流场：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The result is as follows:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 结果如下：
- en: '![How to do it...](img/image_13_011.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![如何做...](img/image_13_011.jpg)'
- en: How it works...
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: We explained in the first section of this recipe that an optical flow field
    can be estimated by minimizing a function that combines the brightness constancy
    constraint and a smoothness function. The equations we presented then constitute
    the classical formulation of the problem, and this one has been improved in many
    ways.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本食谱的第一节中，我们解释了可以通过最小化一个结合亮度恒常约束和光滑性函数的函数来估计光流场。我们提出的方程构成了问题的经典公式，并且这个公式已经以多种方式得到了改进。
- en: The method we used in the previous section is known as the **Dual TV L1** method.
    It has two main ingredients. The first one is the use of a smoothing constraint
    that aims at minimizing the absolute value of the optical flow gradient (instead
    of the square of it). This choice reduces the impact of the smoothing term, especially
    at regions of discontinuity where, for example, the optical flow vectors of a
    moving object are quite different from the ones of its background. The second
    ingredient is the use of a **first-order Taylor approximation**; this linearizes
    the formulation of the brightness constancy constraint. We will not enter into
    the details of this formulation here; it is suffice to say that this linearization
    facilitates the iterative estimation of the optical flow field. However, since
    the linear approximation is only valid for small displacements, the method requires
    a coarse-to-fine estimation scheme.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中我们使用的方法被称为**Dual TV L1**方法。它有两个主要成分。第一个成分是使用一个平滑约束，旨在最小化光流梯度的绝对值（而不是它的平方）。这种选择减少了平滑项的影响，尤其是在不连续区域，例如，移动物体的光流向量与其背景的光流向量有很大的不同。第二个成分是使用**一阶泰勒近似**；这线性化了亮度恒常约束的公式。我们在这里不会深入这个公式的细节；只需说这种线性化有助于光流场的迭代估计即可。然而，由于线性近似仅在小的位移下有效，因此该方法需要一个从粗到细的估计方案。
- en: 'In this recipe, we used this method with its default parameters. A number of
    setters and getters methods allow you to modify the ones which can have an impact
    on the quality of the solution and on the speed of the computation. For example,
    one can modify the number of scales used in the pyramidal estimation or specify
    a more or less strict stopping criterion to be adopted during each iterative estimation
    step. Another important parameter is the weight associated with the brightness
    constancy constraint versus the smoothness constraint. For example, if we reduce
    the importance given to brightness constancy by two, we then obtain a smoother
    optical flow field:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们使用默认参数使用这种方法。一些setter和getter方法允许你修改那些可能影响解决方案质量和计算速度的参数。例如，可以修改在金字塔估计中使用的缩放级别数量，或在每次迭代估计步骤中指定一个更严格或更宽松的停止标准。另一个重要参数是与亮度恒常约束相比的平滑约束的权重。例如，如果我们减少对亮度恒常重要性的考虑，那么我们就会得到一个更平滑的光流场：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![How it works...](img/image_13_012.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/image_13_012.jpg)'
- en: See also
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: The article by *B.K.P. Horn* and *B.G. Schunck*, *Determining optical flow,
    in Artificial Intelligence*, 1981, is the classical reference on optical flow
    estimation
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由*B.K.P. Horn*和*B.G. Schunck*撰写的文章《在人工智能中确定光流》，发表于1981年，是光流估计的经典参考文献。
- en: The article by *C. Zach*, *T. Pock*, and *H. Bischof*, *A duality based approach
    for real time tv-l 1 optical flow*, at *IEEE conference on Computer Vision and
    Pattern Recognition* 2007, describes the details of the `Dual TV-L1` method
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由*C. Zach*、*T. Pock*和*H. Bischof*撰写的文章《基于对偶的实时tv-l 1光流方法》，发表在2007年的*IEEE计算机视觉与模式识别会议*上，详细描述了`Dual
    TV-L1`方法的细节。
- en: Tracking an object in a video
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 视频中跟踪物体
- en: In the previous two recipes, we learned how to track the motion of points and
    pixels in an image sequence. In many applications, however, the requirement is
    rather to track a specific moving object in a video. An object of interest is
    first identified and then it must be followed over a long sequence. This is challenging
    because as it evolves in the scene, the image of this object will undergo many
    changes in appearance due to viewpoint and illumination variations, non-rigid
    motion, occlusion, and so on.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在前两个配方中，我们学习了如何跟踪图像序列中点和像素的运动。然而，在许多应用中，要求是跟踪视频中的特定移动物体。首先识别感兴趣的物体，然后必须在一个长序列中跟踪它。这是具有挑战性的，因为随着它在场景中的演变，这个物体的图像将因视角和光照变化、非刚性运动、遮挡等因素而经历许多外观变化。
- en: This recipe presents some of the object-tracking algorithms implemented in the
    OpenCV library. These implementations are based on a common framework, which facilitates
    the substitution of one method by another. Contributors have also made available
    a number of new methods. Note that, we have already presented a solution to the
    object-tracking problem in the *Counting pixels with integral images* recipe in
    [Chapter 4](ch04.html "Chapter 4. Counting the Pixels with Histograms") , *Counting
    the Pixels with Histograms*; this one was based on the use of histograms computed
    through integral images.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱展示了在OpenCV库中实现的一些目标跟踪算法。这些实现基于一个通用框架，便于用一种方法替换另一种方法。贡献者还提供了一些新的方法。请注意，我们已经在[第4章](ch04.html
    "第4章. 使用积分图像计数像素")的“使用积分图像计数像素”食谱中提出了解决目标跟踪问题的方案，*计数像素使用直方图*；这个方案是基于通过积分图像计算出的直方图的使用。
- en: How to do it...
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: The visual object-tracking problem generally assumes that no prior knowledge
    about the objects to be tracked is available. Tracking is therefore initiated
    by identifying the object in a frame, and tracking must start at this point. The
    initial identification of the object is achieved by specifying a bounding box
    inside which the target is inscribed. The objective of the tracker module is then
    to reidentify this object in a subsequent frame.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉目标跟踪问题通常假设没有关于要跟踪的对象的先验知识。因此，跟踪是通过在帧中识别对象来启动的，跟踪必须从这个点开始。通过指定一个包含目标的边界框来实现对象的初始识别。跟踪模块的目标是在后续帧中重新识别这个对象。
- en: The `cv::Tracker` class of OpenCV that defines the object-tracking framework
    has therefore, two main methods. The first one is the `init` method used to define
    the initial target bounding box. The second one is the `update` method that outputs
    a new bounding box, given a new frame. Both the methods accept a frame (a `cv::Mat`
    instance) and a bounding box (a `cv::Rect2D` instance) as arguments; in one case,
    the bounding box is an input, while for the second method, the bounding box is
    an output parameter.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，OpenCV中定义对象跟踪框架的`cv::Tracker`类有两个主要方法。第一个是`init`方法，用于定义初始目标边界框。第二个是`update`方法，它根据新帧输出一个新的边界框。这两个方法都接受一个帧（一个`cv::Mat`实例）和一个边界框（一个`cv::Rect2D`实例）作为参数；在一种情况下，边界框是输入，而在第二种情况下，边界框是输出参数。
- en: 'In order to test one of the proposed object tracker algorithms, we use the
    video-processing framework that has been presented in the previous chapter. In
    particular, we define a frame-processing subclass that will be called by our `VideoProcessor`
    class when each frame of the image sequence is received. This subclass has the
    following attributes:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试所提出的目标跟踪算法之一，我们使用上一章中提出的视频处理框架。特别是，我们定义了一个帧处理子类，当接收到图像序列的每一帧时，将由我们的`VideoProcessor`类调用。这个子类具有以下属性：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The `reset` attribute is set to `true` whenever the tracker has been reinitiated
    through the specification of a new target''s bounding box. It is the `setBoundingBox`
    method that is used to store a new object position:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当通过指定新目标边界框重新初始化跟踪器时，`reset`属性被设置为`true`。用于存储新对象位置的`setBoundingBox`方法就是用来做的：
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The callback method used to process each frame then simply calls the appropriate
    method of the tracker and draws the new computed bounding box on the frame to
    be displayed:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 用于处理每一帧的回调方法只是简单地调用跟踪器的适当方法，并在要显示的帧上绘制新的计算出的边界框：
- en: '[PRE14]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To demonstrate how an object can be tracked using the `VideoProcessor` and
    `FrameProcessor` instances, we use the **Median Flow tracker** defined in OpenCV:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示如何使用`VideoProcessor`和`FrameProcessor`实例跟踪一个对象，我们使用OpenCV中定义的**中值流跟踪器**：
- en: '[PRE15]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The first bounding box identifies one goose in our test image sequence. This
    one is then automatically tracked in the subsequent frames:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个边界框识别了我们测试图像序列中的一个鹅。然后，它将在后续帧中自动跟踪：
- en: '![How to do it...](img/image_13_013.jpg)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/image_13_013.jpg)'
- en: 'Unfortunately, as the sequence progresses, the tracker will unavoidably make
    errors. The accumulation of these small errors will cause the tracker to slowly
    drift from the real target position. Here is, for example, the estimated position
    of our target after `130` frames have been processed:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，随着序列的进行，跟踪器不可避免地会犯错误。这些小错误的累积将导致跟踪器逐渐偏离真实目标位置。例如，这是在处理了`130`帧之后我们目标的估计位置：
- en: '![How to do it...](img/image_13_014.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![如何操作...](img/image_13_014.jpg)'
- en: Eventually, the tracker will lose track of the object. The ability of a tracker
    to follow an object over a long period of time is the most important criteria
    that characterizes the performance of an object tracker.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，跟踪器将失去对物体的跟踪。跟踪器在长时间内跟踪物体的能力是表征物体跟踪器性能的最重要标准。
- en: How it works...
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we showed how the generic `cv::Tracker` class can be used to
    track an object in an image sequence. We selected the Median Flow tracker algorithm
    to illustrate the tracking result. This is a simple but effective method to track
    a textured object as long as its motion is not too rapid and it is not too severely
    occluded.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个菜谱中，我们展示了如何使用通用的`cv::Tracker`类在图像序列中跟踪一个物体。我们选择了Median Flow跟踪算法来展示跟踪结果。这是一个简单但有效的方法来跟踪纹理物体，只要其运动不是太快，并且没有被严重遮挡。
- en: 'The Median Flow tracker is based on feature point tracking. It first starts
    by defining a grid of points over the object to be tracked. One could have instead
    detected interest points on the object using, for instance, the `FAST` operator
    presented in [Chapter 8](ch08.html "Chapter 8. Detecting Interest Points") , *Detecting
    Interest Points*. However, using points at predefined locations presents a number
    of advantages. It saves time by avoiding the computation of interest points. It
    guarantees that a sufficient number of points will be available for tracking.
    It also makes sure that these points will be well distributed over the whole object.
    The Median Flow implementation uses, by default, a grid of `10x10` points:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Median Flow跟踪器基于特征点跟踪。它首先从定义一个要跟踪的物体的点阵开始。人们可以用例如在[第8章](ch08.html "第8章. 检测兴趣点")中介绍的`FAST`算子来检测物体上的兴趣点。然而，使用预定义位置上的点具有许多优点。它通过避免计算兴趣点来节省时间。它保证了将有足够数量的点可用于跟踪。它还确保这些点在整个物体上分布良好。Median
    Flow实现默认使用一个`10x10`点的网格：
- en: '![How it works...](img/image_13_015.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_13_015.jpg)'
- en: 'The next step is to use the Lukas-Kanade feature-tracking algorithm presented
    in the first recipe of this chapter, *Tracing feature points in a video*. Each
    point of the grid is then tracked over the next frame:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是使用本章第一道菜谱中介绍的Lukas-Kanade特征跟踪算法，*在视频中追踪特征点*。然后，网格中的每个点在下一帧中被跟踪：
- en: '![How it works...](img/image_13_016.jpg)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![如何工作...](img/image_13_016.jpg)'
- en: The Median Flow algorithm then estimates the errors made when tracking these
    points. These errors can be estimated, for example, by computing the sum of absolute
    pixel difference in a window around the point at its initial and tracked position.
    This is the type of error that is conveniently computed and returned by the `cv::calcOpticalFlowPyrLK`
    function. Another error measure proposed by the Median Flow algorithm is to use
    the so-called forward-backward error. After the points have been tracked between
    a frame and the next one, these points at their new position are backward-tracked
    to check whether they will return to their original position in the initial image.
    The difference between the thus obtained forward-backward position and the initial
    one is the error in tracking.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，Median Flow算法估计在跟踪这些点时产生的误差。这些误差可以通过计算点在初始位置和跟踪位置周围窗口中的绝对像素差之和来估计。这是`cv::calcOpticalFlowPyrLK`函数方便计算并返回的错误类型。Median
    Flow算法提出的另一种误差度量是所谓的正向-反向误差。在点在帧与下一帧之间被跟踪之后，这些点在新位置被反向跟踪以检查它们是否会返回到初始图像中的原始位置。由此获得的正向-反向位置与初始位置之间的差异是跟踪误差。
- en: Once the tracking error of each point has been computed, only 50 percent of
    the points having the smallest error are considered. This group is used to compute
    the new position of the bounding box in the next image. Each of these points votes
    for a displacement value, and the median of these possible displacements is retained.
    For the change in scale, the points are considered in pairs. The ratio of the
    distance between the two points in the initial frame and the next one is estimated.
    Again, it is the median of these scales that is finally applied.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦计算了每个点的跟踪误差，只有具有最小误差的50%的点被考虑。这个组被用来计算下一个图像中边界框的新位置。这些点中的每一个都对位移值进行投票，并保留这些可能位移的中位数。对于尺度变化，点被成对考虑。估计初始帧和下一帧中两点之间距离的比率。同样，最终应用的是这些尺度中的中位数。
- en: 'The Median Tracker is one of many other visual object trackers based on feature
    point tracking. Another family of solutions is the one that is based on template
    matching, a concept we discussed in the *Matching local templates* recipe in [Chapter
    9](ch09.html "Chapter 9. Describing and Matching Interest Points") , *Describing
    and Matching Interest Points*. A good representative of these kinds of approaches
    is the **Kernelized Correlation Filter** (**KCF**) algorithm, implemented as the
    `cv::TrackerKCF` class in OpenCV:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 中值跟踪器是许多基于特征点跟踪的视觉对象跟踪器之一。另一类解决方案是基于模板匹配，我们在 [第 9 章](ch09.html "第 9 章. 描述和匹配兴趣点")
    的“匹配局部模板”食谱中讨论了这个概念。这类方法的一个很好的代表是 **核相关滤波器** (**KCF**) 算法，它在 OpenCV 中实现为 `cv::TrackerKCF`
    类：
- en: '[PRE16]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Basically, this one uses the target''s bounding box as a template to search
    for the new object position in the next view. This is normally computed through
    a simple correlation, but KCF uses a special trick based on the Fourier transform
    that we briefly mentioned in the introduction of [Chapter 6](ch06.html "Chapter 6. Filtering
    the Images") , *Filtering the Images*. Without entering into any details, the
    signal-processing theory tells us that correlating a template over an image corresponds
    to simple image multiplication in the frequency domain. This considerably speeds
    up the identification of the matching window in the next frame and makes KCF one
    of the fastest and robust trackers. As an example, here is the position of the
    bounding box after a tracking of `130` frames using KCF:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，这种方法使用目标的边界框作为模板来搜索下一视图中的新对象位置。这通常通过简单的相关性计算得出，但 KCF 使用一种基于傅里叶变换的特殊技巧，我们在
    [第 6 章](ch06.html "第 6 章. 过滤图像") 的“过滤图像”引言中简要提到了它。不深入细节，信号处理理论告诉我们，在图像上对模板进行相关性计算相当于在频域中的简单图像乘法。这大大加快了下一帧中匹配窗口的识别速度，使
    KCF 成为最快且最稳健的跟踪器之一。例如，以下是使用 KCF 进行 `130` 帧跟踪后的边界框位置：
- en: '![How it works...](img/image_13_017.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![工作原理...](img/image_13_017.jpg)'
- en: See also
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参见
- en: 'The article by *Z. Kalal*, *K. Mikolajczyk*, and *J. Matas*, *Forward-backward
    error: Automatic detection of tracking failures*, in *Int. Conf. on Pattern Recognition*,
    2010, describes the Median Flow algorithm'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '由 *Z. Kalal*、*K. Mikolajczyk* 和 *J. Matas* 撰写的文章，题为 *Forward-backward error:
    Automatic detection of tracking failures*，发表在 *Int. Conf. on Pattern Recognition*，2010
    年，描述了中值流算法。'
- en: The article by *Z. Kalal*, *K. Mikolajczyk*, and *J. Matas*, *Tracking-learning-detection*,
    in *IEEE Transactions on Pattern Analysis and Machine Intelligence*, vol 34, no
    7, 2012, is an advanced tracking method that uses the Median Flow algorithm
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 *Z. Kalal*、*K. Mikolajczyk* 和 *J. Matas* 撰写的文章，题为 *Tracking-learning-detection*，发表在
    *IEEE Transactions on Pattern Analysis and Machine Intelligence* 期刊，第 34 卷，第 7
    期，2012 年，介绍了一种使用中值流算法的高级跟踪方法。
- en: The article by *J.F. Henriques*, *R. Caseiro*, *P. Martins*, *J. Batist*a, *High-Speed
    Tracking with Kernelized Correlation Filters*, in *IEEE Transactions on Pattern
    Analysis and Machine Intelligence*, vol 37, no 3, 2014, describes the KCF tracker
    algorithm
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由 *J.F. Henriques*、*R. Caseiro*、*P. Martins*、*J. Batista* 撰写的文章，题为 *High-Speed
    Tracking with Kernelized Correlation Filters*，发表在 *IEEE Transactions on Pattern
    Analysis and Machine Intelligence* 期刊，第 37 卷，第 3 期，2014 年，描述了 KCF 跟踪器算法。
