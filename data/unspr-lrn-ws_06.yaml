- en: 5\. Autoencoders
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5. 自编码器
- en: Overview
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 概述
- en: In this chapter, we will look at autoencoders and their applications. We will
    see how autoencoders are used in dimensionality reduction and denoising. We will
    implement an artificial neural network and an autoencoder using the Keras framework.
    By the end of this chapter, you will be able to implement an autoencoder model
    using convolutional neural networks.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论自编码器及其应用。我们将了解自编码器如何用于降维和去噪。我们将使用Keras框架实现一个人工神经网络和自编码器。到本章结束时，你将能够使用卷积神经网络实现一个自编码器模型。
- en: Introduction
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引言
- en: We'll continue our discussion of dimensionality reduction techniques as we turn
    our attention to autoencoders. Autoencoders are a particularly interesting area
    of focus as they provide a means of using supervised learning based on artificial
    neural networks but in an unsupervised context. Being based on artificial neural
    networks, autoencoders are an extremely effective means of performing dimensionality
    reduction, but also provide additional benefits. With recent increases in the
    availability of data, processing power, and network connectivity, autoencoders
    are experiencing a resurgence in usage and the study of them, the likes of which
    have not been seen since their origins in the late 1980s. This is also consistent
    with the study of artificial neural networks, which were first described and implemented
    as a concept in the 1960s. Presently, you would only need to conduct a cursory
    internet search to discover the popularity and power of neural networks.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将注意力转向自编码器时，我们将继续讨论降维技术。自编码器是一个特别有趣的研究领域，因为它提供了一种基于人工神经网络的监督学习方法，但在无监督的环境下使用。自编码器基于人工神经网络，是执行降维的极为有效的手段，同时也提供了额外的好处。随着数据、处理能力和网络连接的不断增加，自编码器在使用和研究上迎来了复兴，这种现象自1980年代末自编码器起源以来未曾见过。这与人工神经网络的研究是一致的，后者最早在1960年代被描述和实现为一种概念。目前，你只需进行简单的互联网搜索，就能发现神经网络的普及和强大功能。
- en: Autoencoders can be used for de-noising images and generating artificial data
    samples in combination with other methods, such as recurrent or **Long Short-Term
    Memory** (**LSTM**) architectures, to predict sequences of data. The flexibility
    and power that arises from the use of artificial neural networks also enable autoencoders
    to form very efficient representations of the data, which can then be used either
    directly as an extremely efficient search method, or as a feature vector for later
    processing.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器可以与其他方法结合使用，如递归神经网络或**长短期记忆网络**（**LSTM**）架构，用于去噪图像和生成人工数据样本，以预测数据序列。使用人工神经网络所带来的灵活性和强大功能，使得自编码器能够形成非常高效的数据表示，这些表示可以直接作为极其高效的搜索方法，或作为后续处理的特征向量使用。
- en: 'Consider the use of an autoencoder in an image de-noising application, where
    we are presented with the image on the left in *Figure 5.1*. We can see that the
    image is affected by the addition of some random noise. We can use a specially
    trained autoencoder to remove this noise, as represented in the image on the right
    in the following figure. In learning how to remove this noise, the autoencoder
    has also learned to encode the important information that composes the image and
    how to reconstruct (or decode) this information into a clearer version of the
    original image:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑在图像去噪应用中使用自编码器，我们看到左边的图像（图5.1），它受到了某些随机噪声的影响。我们可以使用专门训练的自编码器去除这些噪声，如下图右侧所示。通过学习如何去除噪声，自编码器还学会了如何编码组成图像的关键信息，并如何将这些信息解码（或重构）为更清晰的原始图像版本：
- en: '![Figure 5.1: Autoencoder de-noising'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.1：自编码器去噪'
- en: '](img/B15923_05_01.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_01.jpg)'
- en: 'Figure 5.1: Autoencoder de-noising'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1：自编码器去噪
- en: Note
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This image has been modified from [http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/](http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/)
    under CC0.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图片已从[http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/](http://www.freenzphotos.com/free-photos-of-bay-of-plenty/stormy-fishermen/)在CC0授权下修改。
- en: This example demonstrates one aspect of autoencoders that makes them useful
    for unsupervised learning (the encoding stage) and one that is useful in generating
    new images (decoding). We will delve further into these two useful stages of autoencoders
    and apply the output of the autoencoder to clustering the CIFAR-10 dataset ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子展示了自编码器的一个方面，使其在无监督学习（编码阶段）中非常有用，并且另一个方面使其在生成新图像时（解码阶段）也很有用。我们将进一步探讨自编码器的这两个有用阶段，并将自编码器的输出应用于CIFAR-10数据集的聚类（[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)）。
- en: 'Here is a representation of an encoder and a decoder:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是编码器和解码器的表示：
- en: '![Figure 5.2: Encoder/decoder representation'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.2：编码器/解码器表示'
- en: '](img/B15923_05_02.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_02.jpg)'
- en: 'Figure 5.2: Encoder/decoder representation'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2：编码器/解码器表示
- en: Fundamentals of Artificial Neural Networks
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 人工神经网络基础
- en: Given that autoencoders are based on artificial neural networks, an understanding
    of neural networks is also critical for understanding autoencoders. This section
    of the chapter will briefly review the fundamentals of artificial neural networks.
    It is important to note that there are many aspects of neural networks that are
    outside the scope of this book. The topic of neural networks could easily fill,
    and has filled, many books on its own, and this section is not to be considered
    an exhaustive discussion of the topic.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 由于自编码器基于人工神经网络，因此理解神经网络对理解自编码器也至关重要。本章的这一部分将简要回顾人工神经网络的基础知识。需要注意的是，神经网络有许多方面超出了本书的范围。神经网络的主题很容易填满，并且已经填满了许多书籍，这一部分并不打算成为该主题的详尽讨论。
- en: As described earlier, artificial neural networks are primarily used in supervised
    learning problems, where we have a set of input information, say a series of images,
    and we are training an algorithm to map the information to a desired output, such
    as a class or category. Consider the CIFAR-10 dataset shown in *Figure 5.3* as
    an example, which contains images of 10 different categories (airplane, automobile,
    bird, cat, deer, dog, frog, horse, ship, and truck), with 6,000 images per category.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，人工神经网络主要用于监督学习问题，在这些问题中，我们有一组输入信息，比如一系列图像，我们正在训练一个算法，将这些信息映射到期望的输出，比如类别或分类。以*图
    5.3*中的CIFAR-10数据集为例，它包含10个不同类别（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车）的图像，每个类别有6000张图像。
- en: '![Figure 5.3: CIFAR-10 dataset'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.3：CIFAR-10数据集'
- en: '](img/B15923_05_03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_03.jpg)'
- en: 'Figure 5.3: CIFAR-10 dataset'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3：CIFAR-10数据集
- en: When neural networks are used in a supervised learning context, the images are
    fed to the network with a representation of the corresponding category labels
    being the desired output of the network.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当神经网络用于监督学习时，图像被输入到网络中，网络的期望输出是对应类别标签的表示。
- en: The network is then trained to maximize its ability to infer or predict the
    correct label for a given image.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，网络将被训练以最大化其推断或预测给定图像的正确标签的能力。
- en: Note
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: This image is taken from [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)
    from *Learning Multiple Layers of Features from Tiny Images* ([https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf)),
    Alex Krizhevsky, 2009.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这张图来自[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)，出自*《从微小图像中学习多个特征层》*([https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf))，Alex
    Krizhevsky，2009年。
- en: The Neuron
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经元
- en: 'The artificial neural network derives its name from the biological neural networks
    commonly found in the brain. While the accuracy of the analogy can certainly be
    questioned, it is a useful metaphor to break down the concept of artificial neural
    networks and facilitate understanding. As with their biological counterparts,
    the neuron is the building block on which all neural networks are constructed,
    connecting a number of neurons in different configurations to form more powerful
    structures. Each neuron in *Figure 5.4* is composed of four individual parts:
    an input value, a tunable weight (theta), an activation function that operates
    on the product of the weight and input value, and the resulting output value:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络得名于大脑中常见的生物神经网络。虽然这种类比的准确性确实值得商榷，但它是一个有用的隐喻，可以帮助我们理解人工神经网络的概念。与生物神经网络一样，神经元是所有神经网络的构建块，通过不同的配置连接多个神经元，形成更强大的结构。在*图5.4*中，每个神经元由四个部分组成：一个输入值、一个可调权重（theta）、一个作用于权重与输入值乘积的激活函数，以及由此产生的输出值：
- en: '![Figure 5.4: Anatomy of a neuron'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4：神经元的解剖结构'
- en: '](img/B15923_05_04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_04.jpg)'
- en: 'Figure 5.4: Anatomy of a neuron'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4：神经元的解剖结构
- en: The activation function is specifically chosen depending upon the objective
    of the neural network being designed, and there are a number of common functions,
    including **tanh**, **sigmoid**, **linear**, and **ReLU** (rectified linear unit).
    Throughout this chapter, we will use both the **sigmoid** and **ReLU** activation
    functions, so let's look at them in a little more detail.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数的选择取决于神经网络设计的目标，常见的函数包括**tanh**、**sigmoid**、**linear**和**ReLU**（修正线性单元）。在本章中，我们将使用**sigmoid**和**ReLU**激活函数，因此我们可以稍微深入了解它们。
- en: The Sigmoid Function
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Sigmoid函数
- en: 'The sigmoid activation function is very commonly used as an output in the classification
    of neural networks due to its ability to shift the input values to approximate
    a binary output. The sigmoid function produces the following output:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于sigmoid激活函数能够将输入值转换为接近二进制的输出，因此它在神经网络分类中的输出中被广泛使用。Sigmoid函数产生以下输出：
- en: '![Figure 5.5: Output of the sigmoid function'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5：Sigmoid函数的输出'
- en: '](img/B15923_05_05.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_05.jpg)'
- en: 'Figure 5.5: Output of the sigmoid function'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5：Sigmoid函数的输出
- en: We can see in the preceding figure that the output of the sigmoid function asymptotes
    (approaches but never reaches) 1 as *x* increases, and asymptotes 0 as *x* moves
    further away from 0 in the negative direction. This function is used in classification
    tasks as it provides close to a binary output.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在前面的图中看到，随着*x*的增加，sigmoid函数的输出渐近于1（趋近但永远无法达到），而当*x*向负方向远离0时，输出渐近于0。该函数常用于分类任务，因为它提供接近二进制的输出。
- en: We can see that sigmoid has an asymptotic nature. Due to this, as the value
    of the input reaches the extremities, the training process slows down (known as
    **vanishing gradient**). This is a bottleneck in training. Therefore, to speed
    up the training process, intermediary stages of neural networks use **Rectified
    Linear Unit** (**ReLU**). However, this does have certain limitations as ReLU
    has the problems of dead cells and bias.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，sigmoid具有渐近性质。由于这一特性，当输入值接近极限时，训练过程会变得缓慢（称为**梯度消失**）。这是训练中的瓶颈。因此，为了加速训练过程，神经网络的中间阶段使用**修正线性单元**（**ReLU**）。然而，ReLU也有一定的局限性，因为它存在死神经元和偏置问题。
- en: Rectified Linear Unit (ReLU)
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）
- en: The rectified linear unit is a very useful activation function that's commonly
    used at intermediary stages of neural networks. Simply put, the value 0 is assigned
    to input values less than 0, and the actual value is returned for values greater
    than 0.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 修正线性单元（ReLU）是一种非常有用的激活函数，通常在神经网络的中间阶段使用。简而言之，对于小于0的输入值，ReLU将其输出为0，而对于大于0的输入值，则返回实际值。
- en: '![Figure 5.6: Output of ReLU'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.6：ReLU的输出'
- en: '](img/B15923_05_06.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_06.jpg)'
- en: 'Figure 5.6: Output of ReLU'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6：ReLU的输出
- en: 'Exercise 5.01: Modeling the Neurons of an Artificial Neural Network'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习5.01：建模人工神经网络中的神经元
- en: 'In this exercise, we will practically introduce a programmatic representation
    of the neuron in `NumPy` using the `sigmoid` function. We will keep the inputs
    fixed and adjust the tunable weights to investigate the effect on the neuron.
    To relate this framework back to a common model in supervised learning, our approach
    in this exercise is the same as logistic regression. Perform the following steps:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将实际介绍如何在`NumPy`中以编程方式表示神经元，并使用`sigmoid`函数。我们将固定输入，调整可调权重，以研究其对神经元的影响。为了将这一框架与监督学习中的常见模型关联起来，我们在本练习中的方法与逻辑回归相同。执行以下步骤：
- en: 'Import the `numpy` and `matplotlib` packages:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`numpy`和`matplotlib`包：
- en: '[PRE0]'
  id: totrans-48
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the `sigmoid` function as a Python function:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`sigmoid`函数定义为Python函数：
- en: '[PRE1]'
  id: totrans-50
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: 'Here, we''re using the `sigmoid` function. You could also use the `ReLU` function.
    The `ReLU` activation function, while being powerful in artificial neural networks,
    is easy to define. It simply needs to return the input value if greater than 0;
    otherwise, it returns 0:'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是`sigmoid`函数。你也可以使用`ReLU`函数。`ReLU`激活函数在人工神经网络中虽然非常强大，但其定义非常简单。它只需要在输入大于0时返回输入值；否则，返回0：
- en: '`def relu(x):`'
  id: totrans-53
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`def relu(x):`'
- en: '`return np.max(0, x)`'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`return np.max(0, x)`'
- en: 'Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be `100` numbers linearly spaced between `-5` and
    `5`. Set `theta = 1`:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经元的输入（`x`）和可调权重（`theta`）。在本例中，输入（`x`）将是`-5`到`5`之间线性间隔的`100`个数字。设置`theta =
    1`：
- en: '[PRE2]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The output is as follows:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.7: Printing the inputs'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.7：打印输入'
- en: '](img/B15923_05_07.jpg)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_07.jpg)'
- en: 'Figure 5.7: Printing the inputs'
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.7：打印输入
- en: 'Compute the outputs (`y`) of the neuron:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算神经元的输出（`y`）：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Plot the output of the neuron versus the input:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制神经元输出与输入的关系图：
- en: '[PRE4]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'In the following output, you can see the plotted `sigmoid` function – note
    that it passes through the origin at `0.5`:'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在以下输出中，您可以看到绘制的`sigmoid`函数——请注意，它通过原点并在`0.5`处交叉。
- en: '![Figure 5.8: Plot of neurons versus inputs'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.8：神经元与输入的关系图'
- en: '](img/B15923_05_08.jpg)'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_08.jpg)'
- en: 'Figure 5.8: Plot of neurons versus inputs'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.8：神经元与输入的关系图
- en: 'Set the tunable parameter, `theta`, to `5`, and recompute and store the output
    of the neuron:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可调参数`theta`设置为`5`，然后重新计算并存储神经元的输出：
- en: '[PRE5]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Change the tunable parameter, `theta`, to `0.2`, and recompute and store the
    output of the neuron:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将可调参数`theta`改为`0.2`，然后重新计算并存储神经元的输出：
- en: '[PRE6]'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, and `theta = 0.2`) on one graph:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在一个图表中绘制三条不同的神经元输出曲线（`theta = 1`、`theta = 5`和`theta = 0.2`）：
- en: '[PRE7]'
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.9: Output curves of neurons'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.9：神经元的输出曲线'
- en: '](img/B15923_05_09.jpg)'
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_09.jpg)'
- en: 'Figure 5.9: Output curves of neurons'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9：神经元的输出曲线
- en: In this exercise, we modeled the basic building block of an artificial neural
    network with a `sigmoid` activation function. We can see that using the `sigmoid`
    function increases the steepness of the gradient and means that only small values
    of *x* will push the output to either close to 1 or 0\. Similarly, reducing `theta`
    reduces the sensitivity of the neuron to non-zero values and results in extreme
    input values being required to push the result of the output to either 0 or 1,
    tuning the output of the neuron.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们用`sigmoid`激活函数模拟了人工神经网络的基本构建块。我们可以看到，使用`sigmoid`函数增加了梯度的陡峭度，这意味着只有小的*x*值才会将输出推向接近1或0。同样，减小`theta`会降低神经元对非零值的敏感度，导致需要极端的输入值才能将输出结果推向0或1，从而调节神经元的输出。
- en: Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2AE9Kwc](https://packt.live/2AE9Kwc).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2AE9Kwc](https://packt.live/2AE9Kwc)。
- en: You can also run this example online at [https://packt.live/3e59UdK](https://packt.live/3e59UdK).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3e59UdK](https://packt.live/3e59UdK)在线运行此示例。
- en: 'Exercise 5.02: Modeling Neurons with the ReLU Activation Function'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.02：使用ReLU激活函数建模神经元
- en: 'Similar to *Exercise 5.01*, *Modeling the Neurons of an Artificial Neural Network*,
    we will model a network again, this time with the ReLU activation function. In
    this exercise, we will develop a range of response curves for the ReLU activated
    neuron and describe the effect of changing the value of theta on the output of
    the neuron:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于 *练习 5.01*，*人工神经网络神经元建模*，我们将再次建模一个网络，这次使用 ReLU 激活函数。在这个练习中，我们将为 ReLU 激活的神经元开发一系列响应曲线，并描述改变
    theta 值对神经元输出的影响：
- en: 'Import `numpy` and `matplotlib`:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `numpy` 和 `matplotlib`：
- en: '[PRE8]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Define the ReLU activation function as a Python function:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 ReLU 激活函数定义为 Python 函数：
- en: '[PRE9]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Define the inputs (`x`) and tunable weights (`theta`) for the neuron. In this
    example, the inputs (`x`) will be 100 numbers linearly spaced between `-5` and
    `5`. Set `theta = 1`:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义神经元的输入（`x`）和可调权重（`theta`）。在这个示例中，输入（`x`）将是线性间隔在`-5`和`5`之间的100个数字。设置 `theta
    = 1`：
- en: '[PRE10]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.10: Printing the inputs'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.10：打印输入'
- en: '](img/B15923_05_10.jpg)'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_10.jpg)'
- en: 'Figure 5.10: Printing the inputs'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.10：打印输入
- en: 'Compute the output (`y`):'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算输出（`y`）：
- en: '[PRE11]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Plot the output of the neuron versus the input:'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制神经元输出与输入的关系图：
- en: '[PRE12]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The output is as follows:'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.11: Plot of the neuron versus the input'
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.11：神经元与输入的关系图'
- en: '](img/B15923_05_11.jpg)'
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_11.jpg)'
- en: 'Figure 5.11: Plot of the neuron versus the input'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.11：神经元与输入的关系图
- en: 'Now, set `theta = 5` and recompute and store the output of the neuron:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，设置 `theta = 5`，重新计算并保存神经元的输出：
- en: '[PRE13]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Now, set `theta = 0.2` and recompute and store the output of the neuron:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，设置 `theta = 0.2`，重新计算并保存神经元的输出：
- en: '[PRE14]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Plot the three different output curves of the neuron (`theta = 1`, `theta =
    5`, and `theta = 0.2`) on one graph:'
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在同一张图表上绘制神经元的三条不同输出曲线（`theta = 1`，`theta = 5`，和 `theta = 0.2`）：
- en: '[PRE15]'
  id: totrans-108
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The output is as follows:'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.12: Three output curves of the neuron'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.12：神经元的三条输出曲线'
- en: '](img/B15923_05_12.jpg)'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_12.jpg)'
- en: 'Figure 5.12: Three output curves of the neuron'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12：神经元的三条输出曲线
- en: In this exercise, we created a model of a ReLU-based artificial neural network
    neuron. We can see that the output of this neuron is very different to the sigmoid
    activation function. There is no saturation region for values greater than 0 because
    it simply returns the input value of the function. In the negative direction,
    there is a saturation region where only 0 will be returned if the input is less
    than 0\. The ReLU function is an extremely powerful and commonly used activation
    function that has shown itself to be more powerful than the sigmoid function in
    some circumstances. ReLU is often a good first-choice activation function.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们创建了一个基于 ReLU 的人工神经网络神经元模型。我们可以看到，这个神经元的输出与 sigmoid 激活函数的输出有很大不同。对于大于
    0 的值，没有饱和区域，因为它仅仅返回函数的输入值。在负方向上，当输入小于 0 时，存在饱和区域，只有 0 会被返回。ReLU 函数是一种非常强大且常用的激活函数，在某些情况下，它比
    sigmoid 函数更强大。ReLU 经常是首选的激活函数。
- en: Note
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2O5rnIn](https://packt.live/2O5rnIn).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定章节的源代码，请参考 [https://packt.live/2O5rnIn](https://packt.live/2O5rnIn)。
- en: You can also run this example online at [https://packt.live/3iJ2Kzu](https://packt.live/3iJ2Kzu).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在 [https://packt.live/3iJ2Kzu](https://packt.live/3iJ2Kzu) 上在线运行此示例。
- en: 'Neural Networks: Architecture Definition'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络：架构定义
- en: Individual neurons aren't particularly useful in isolation; they provide an
    activation function and a means of tuning the output, but a single neuron would
    have a limited learning ability. Neurons become much more powerful when many of
    them are combined and connected together in a network structure. By using a number
    of different neurons and combining the outputs of individual neurons, more complex
    relationships can be established and more powerful learning algorithms can be
    built. In this section, we will briefly discuss the structure of a neural network
    and implement a simple neural network using the Keras machine learning framework
    ([https://keras.io/](https://keras.io/)). Keras is a high-level neural network
    API that is used on top of an existing library, such as TensorFlow or Theano.
    Keras makes it easy to switch between lower-level frameworks because the high-level
    interface it provides remains the same irrespective of the underlying library.
    In this book, we will be using TensorFlow as the underlying library.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 单个神经元在孤立状态下并不是特别有用；它提供了激活函数和调节输出的手段，但单个神经元的学习能力是有限的。当多个神经元结合并在网络结构中连接在一起时，神经元的功能就变得更加强大。通过使用多个不同的神经元并结合各个神经元的输出，可以建立更复杂的关系，并构建更强大的学习算法。在本节中，我们将简要讨论神经网络的结构，并使用Keras机器学习框架实现一个简单的神经网络（[https://keras.io/](https://keras.io/)）。Keras是一个高层次的神经网络API，基于现有的库（如TensorFlow或Theano）之上。Keras使得在低层框架之间切换变得容易，因为它提供的高层接口在不同的底层库之间保持不变。在本书中，我们将使用TensorFlow作为底层库。
- en: 'The following is a simplified representation of a neural network with one hidden
    layer:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个具有隐藏层的神经网络的简化表示：
- en: '![Figure 5.13: Simplified representation of a neural network'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.13：神经网络的简化表示'
- en: '](img/B15923_05_13.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_13.jpg)'
- en: 'Figure 5.13: Simplified representation of a neural network'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13：神经网络的简化表示
- en: The preceding figure illustrates the structure of a two-layered, fully connected
    neural network. One of the first observations we can make is that there is a lot
    of information contained within this structure, with a high degree of connectivity,
    as represented by the arrows that point to and from each of the nodes. Working
    from the left-hand side of the image, we can see the input values to the neural
    network, as represented by the (*x*) values. In this example, we have *m* input
    values per sample, hence, values from *x*11 to *x*1m. These values per sample
    are called attributes or the features of the data and only one sample is fed at
    a time into the network. These values are then multiplied by the corresponding
    weights of the first layer of the neural network (![C:\Users\user\Desktop\Lesson05\C12626_05_Formula_03.png](img/B15923_05_Formula_01.png))
    before being passed into the activation function of the corresponding neuron.
    This is known as a **feed-forward** neural network. The notation used in the preceding
    figure to identify the weights is ![C:\Users\user\Desktop\Lesson05\C12626_05_Formula_04.png](img/B15923_05_Formula_02.png),
    where *i* is the layer the weight belongs to, *j* is the input node number (starting
    with 1 at the top), and *k* is the node in the subsequent layer that the weight
    feeds into.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上图展示了一个两层完全连接的神经网络结构。我们可以做出的第一个观察是，这个结构包含了大量的信息，并且具有高度的连接性，箭头表示了每个节点之间的连接。我们从图像的左侧开始，可以看到神经网络的输入值，由（*x*）值表示。在这个示例中，每个样本有*m*个输入值，因此，从*x*11到*x*1m的值代表这些输入值。每个样本的这些值被称为数据的属性或特征，并且每次仅输入一个样本到网络中。然后，这些值会与神经网络第一层对应的权重相乘
    (![C:\Users\user\Desktop\Lesson05\C12626_05_Formula_03.png](img/B15923_05_Formula_01.png))，然后传入对应神经元的激活函数。这被称为**前馈神经网络**。在上图中，用来标识权重的符号是![C:\Users\user\Desktop\Lesson05\C12626_05_Formula_04.png](img/B15923_05_Formula_02.png)，其中*i*是权重所属的层，*j*是输入节点的编号（从顶部开始为1），而*k*是后续层中该权重连接的节点。
- en: Looking at the inter-connectivity between the outputs of layer 1 (also known
    as the **hidden layer**) and the inputs to the output layer, we can see that there
    are a large number of tunable parameters (weights) that can be used to map the
    input to the desired output. The network of the preceding figure represents an
    *n* class neural network classifier, where the output for each of the *n* nodes
    represents the probability of the input belonging to the corresponding class.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 观察第一层（也叫做**隐藏层**）输出与输出层输入之间的互联关系，我们可以看到有大量可调节的参数（权重），这些参数可以用来将输入映射到期望的输出。前图的网络代表了一个
    *n* 类神经网络分类器，其中每个 *n* 个节点的输出表示输入属于相应类别的概率。
- en: Each layer is able to use a different activation function, as described by *h*1
    and *h*2, thus allowing different activation functions to be mixed, in which the
    first layer could use ReLU, the second could use tanh, and the third could use
    sigmoid, for example. The final output is calculated by taking the sum of the
    product of the output of the previous layer with the corresponding weights with
    the activation function applied.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层都可以使用不同的激活函数，如 *h*1 和 *h*2 所示，因此允许不同的激活函数混合使用，例如第一层可以使用 ReLU，第二层可以使用 tanh，第三层可以使用
    sigmoid。最终输出是通过将前一层输出与相应的权重相乘，并通过激活函数计算结果来获得的。
- en: 'If we consider the output of the first node of layer 1, it can be calculated
    by multiplying the inputs by the corresponding weights, adding the result, and
    passing it through the activation function:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们考虑第一层第一节点的输出，它可以通过将输入与相应的权重相乘，求和结果并通过激活函数来计算：
- en: '![Figure 5.14: Calculating the output of the last node'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14：计算最后一个节点的输出'
- en: '](img/B15923_05_14.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_14.jpg)'
- en: 'Figure 5.14: Calculating the output of the last node'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14：计算最后一个节点的输出
- en: As we increase the number of layers between the input and output of the network,
    we increase the depth of the network. An increase in the depth also constitutes
    an increase in the number of trainable parameters, as well as the complexity of
    the relationships within the data, as described by the network. Additionally,
    as we add more neurons to each layer, we increase the height of the neural network.
    By adding more neurons, the ability of the network to describe the dataset increases
    as we add more trainable parameters. If too many neurons are added, the network
    can memorize the dataset but fails to generalize new samples. The trick in constructing
    neural networks is to find the balance between sufficient complexity to be able
    to describe the relationships within the data and not being so complicated as
    to memorize the training samples.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随着输入和输出之间的层数增加，我们增加了网络的深度。深度的增加意味着可训练参数的增加，以及网络描述数据内在关系的复杂度增加。此外，当我们在每一层添加更多神经元时，我们增加了神经网络的高度。通过增加神经元，网络对数据集的描述能力增强，同时可训练参数也增多。如果增加了过多的神经元，网络可能会记住数据集的内容，但无法对新样本进行泛化。构建神经网络的诀窍在于找到一个平衡点，既能充分描述数据内在关系，又不会过于复杂以至于记住训练样本。
- en: 'Exercise 5.03: Defining a Keras Model'
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.03：定义一个 Keras 模型
- en: In this exercise, we will define a neural network architecture (similar to *Figure
    5.13*) using the Keras machine learning framework to classify images for the CIFAR-10
    dataset. As each input image is 32 x 32 pixels in size, the input vector will
    comprise 32*32 = 1,024 values. With 10 individual classes in CIFAR-10, the output
    of the neural network will be composed of 10 individual values, with each value
    representing the probability of the input data belonging to the corresponding
    class.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将使用 Keras 机器学习框架定义一个神经网络架构（类似于*图 5.13*），用于分类 CIFAR-10 数据集的图像。由于每个输入图像的大小为
    32 x 32 像素，输入向量将由 32*32 = 1,024 个值组成。CIFAR-10 有 10 个类别，神经网络的输出将由 10 个值组成，每个值表示输入数据属于相应类别的概率。
- en: Note
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The CIFAR-10 dataset ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    is made up of 60,000 images across 10 classes. These 10 classes are airplane,
    automobile, bird, cat, deer, dog, frog, horse, ship, and truck, with 6,000 images
    per class. Learn more about this dataset via the preceding link.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR-10 数据集 ([https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html))
    由 60,000 张图像组成，涵盖 10 个类别。这 10 个类别包括飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车，每个类别有 6,000 张图像。通过前面的链接了解更多关于这个数据集的信息。
- en: 'For this exercise, we will require the Keras machine learning framework. If
    you have yet to install Keras and TensorFlow, do so using `conda` from within
    your Jupyter notebook:'
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于本练习，我们将需要Keras机器学习框架。如果您还没有安装Keras和TensorFlow，请在Jupyter笔记本中使用`conda`进行安装：
- en: '[PRE16]'
  id: totrans-136
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Alternatively, you can install it using `pip`:'
  id: totrans-137
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 或者，您也可以通过`pip`安装：
- en: '[PRE17]'
  id: totrans-138
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'We will require the `Sequential` and `Dense` classes from `keras.models` and
    `keras.layers`, respectively. Import these classes:'
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将需要分别从`keras.models`和`keras.layers`导入`Sequential`和`Dense`类。导入这些类：
- en: '[PRE18]'
  id: totrans-140
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE18]'
- en: As described earlier, the input layer will receive 1,024 values. The second
    layer (Layer 1) will have 500 units and, because the network is to classify one
    of 10 different classes, the output layer will have 10 units. In Keras, a model
    is defined by passing an ordered list of layers to the `Sequential` model class.
  id: totrans-141
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如前所述，输入层将接收1,024个值。第二层（层1）将包含500个单元，并且由于该网络需要分类10个不同的类别，输出层将有10个单元。在Keras中，通过将有序的层列表传递给`Sequential`模型类来定义模型。
- en: 'This example uses the `Dense` layer class, which is a fully connected neural
    network layer. The first layer will use a ReLU activation function, while the
    output will use the `softmax` function to determine the probability of each class.
    Define the model:'
  id: totrans-142
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 本示例使用了`Dense`层类，这是一个全连接神经网络层。第一层将使用ReLU激活函数，而输出层将使用`softmax`函数来确定每个类别的概率。定义模型：
- en: '[PRE19]'
  id: totrans-143
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'With the model defined, we can use the `summary` method to confirm the structure
    and the number of trainable parameters (or weights) within the model:'
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义好模型后，我们可以使用`summary`方法确认模型的结构以及模型中的可训练参数（或权重）数量：
- en: '[PRE20]'
  id: totrans-145
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The output is as follows:'
  id: totrans-146
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.15: Structure and count of trainable parameters in the model'
  id: totrans-147
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.15：模型中可训练参数的结构和数量'
- en: '](img/B15923_05_15.jpg)'
  id: totrans-148
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_15.jpg)'
- en: 'Figure 5.15: Structure and count of trainable parameters in the model'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15：模型中可训练参数的结构和数量
- en: 'This table summarizes the structure of the neural network. We can see that
    there are the two layers that we specified, with 500 units in the first layer
    and 10 output units in the second layer. The `Param #` column tells us how many
    trainable weights are available in that specific layer. The table also tells us
    that there are 517,510 trainable weights in total within the network.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '该表总结了神经网络的结构。我们可以看到，我们指定的两个层，其中第一个层有500个单元，第二个层有10个输出单元。`Param #`列告诉我们该特定层中有多少个可训练的权重。该表还告诉我们，网络中总共有517,510个可训练的权重。'
- en: Note
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/31WaTdR](https://packt.live/31WaTdR).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问本节的源代码，请参考[https://packt.live/31WaTdR](https://packt.live/31WaTdR)。
- en: You can also run this example online at [https://packt.live/3gGEtbA](https://packt.live/3gGEtbA).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以在[https://packt.live/3gGEtbA](https://packt.live/3gGEtbA)上在线运行此示例。
- en: In this exercise, we created a neural network model in Keras that contains a
    network of over 500,000 weights that can be used to classify the images of CIFAR-10\.
    In the next section, we will train the model.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们创建了一个Keras神经网络模型，包含超过500,000个权重，可用于分类CIFAR-10图像。在接下来的章节中，我们将训练这个模型。
- en: 'Neural Networks: Training'
  id: totrans-155
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 神经网络：训练
- en: With the neural network model defined, we can begin the training process; at
    this stage, we will be training the model in a supervised fashion to develop some
    familiarity with the Keras framework before moving on to training autoencoders.
    Supervised learning models are trained by providing the model with both the input
    information as well as the known output; the goal of training is to construct
    a network that takes the input information and returns the known output using
    only the parameters of the model.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 定义好神经网络模型后，我们可以开始训练过程；在此阶段，我们将以监督方式训练模型，以便在开始训练自编码器之前对Keras框架有所了解。监督学习模型通过提供输入信息和已知输出进行训练；训练的目标是构建一个网络，使其仅使用模型的参数，接受输入信息并返回已知的输出。
- en: In a supervised classification example such as CIFAR-10, the input information
    is an image and the known output is the class that the image belongs to. During
    training, for each sample prediction, the errors in the feedforward network predictions
    are calculated using a specified error function. Each of the weights within the
    model is then tuned in an attempt to reduce the error. This tuning process is
    known as **backpropagation** because the error is propagated backward through
    the network from the output to the start of the network.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在像 CIFAR-10 这样的有监督分类示例中，输入信息是图像，而已知的输出是该图像所属的类别。在训练过程中，对于每个样本的预测，使用指定的误差函数计算前馈网络预测中的误差。然后，模型中的每个权重都会被调整，试图减少误差。这个调整过程被称为**反向传播**，因为误差从输出反向传播通过网络，直到网络的起始部分。
- en: During backpropagation, each trainable weight is adjusted in proportion with
    its contribution to the overall error multiplied by a value known as the **learning
    rate**, which controls the rate of change in the trainable weights. Looking at
    the following figure, we can see that increasing the value of the learning rate
    can increase the speed at which the error is reduced, but risks not converging
    on a minimum error as we step over the values. A learning rate that's too small
    may lead to us running out of patience or simply not having sufficient time to
    find the global minimum. During neural network training, our goal is to find the
    global minimum of errors – basically, the point in the training at which the weights
    are tuned in such a way that it is impossible to minimize the number of errors
    any further. Thus, finding the correct learning rate is a trial-and-error process,
    though starting with a larger learning rate and reducing it can often be a productive
    method. The following figure represents the effect of the selection of the learning
    rate on the optimization of the cost function.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中，每个可训练的权重都会根据其对总误差的贡献进行调整，调整的幅度与一个被称为**学习率**的值成比例，学习率控制着可训练权重变化的速度。观察下图，我们可以看到，增大学习率的值可以加快误差减少的速度，但也存在不能收敛到最小误差的风险，因为我们可能会越过最小值。学习率过小可能导致我们失去耐心，或者根本没有足够的时间找到全局最小值。在神经网络训练中，我们的目标是找到误差的全局最小值——基本上就是训练过程中，权重调节到一个无法再进一步减少错误的点。因此，找到合适的学习率是一个试错的过程，虽然从较大的学习率开始并逐渐减小它通常是一个有效的方法。下图表示选择学习率对成本函数优化的影响。
- en: '![Figure 5.16: Selecting the correct learning rate'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.16：选择正确的学习率'
- en: '](img/B15923_05_16.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_16.jpg)'
- en: 'Figure 5.16: Selecting the correct learning rate'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16：选择正确的学习率
- en: In the preceding figure, you see the learning errors over an epoch, which in
    this case is over time. One epoch corresponds to one complete cycle through the
    training dataset. Training is repeated until the error in the predictions stop
    reducing or the developer runs out of patience waiting for a result. In order
    to complete the training process, we first need to make some design decisions,
    the first being the most appropriate error function. There are a range of error
    functions available for use, from a simple mean squared difference to more complex
    options. Categorical cross-entropy (which is used in the following exercise) is
    a very useful error function for classifying more than one class.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，您可以看到一个周期内的学习误差，在这种情况下是随着时间的推移变化的。一个周期对应着训练数据集中的完整循环。训练会反复进行，直到预测误差不再减少，或者开发者等待结果时耐心耗尽。为了完成训练过程，我们首先需要做出一些设计决策，其中最重要的是选择最合适的误差函数。可供使用的误差函数种类繁多，从简单的均方差到更复杂的选项都有。分类交叉熵（在接下来的练习中使用）是一个非常有用的误差函数，尤其适用于多类分类问题。
- en: With the error function defined, we need to choose the method of updating the
    trainable parameters using the error function. One of the most memory-efficient
    and effective update methods is **Stochastic Gradient Descent** (**SGD**). There
    are a number of variants of SGD, all of which involve adjusting each of the weights
    in accordance with their individual contribution to the calculated error. The
    final training design decision to be made is the performance metric by which the
    model is evaluated and the best architecture selected; in a classification problem,
    this may be the classification accuracy of the model or perhaps the model that
    produces the lowest error score in a regression problem. These comparisons are
    generally made using a cross-validation method.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了误差函数后，我们需要选择更新可训练参数的方法。最节省内存且有效的更新方法之一是**随机梯度下降**（**SGD**）。SGD有多种变体，所有变体都涉及根据每个权重对计算误差的贡献来调整权重。最终的训练设计决策是选择评估模型的性能指标，并选择最佳架构；在分类问题中，这可能是模型的分类准确率，或者在回归问题中，可能是产生最低误差得分的模型。这些比较通常使用交叉验证方法进行。
- en: 'Exercise 5.04: Training a Keras Neural Network Model'
  id: totrans-164
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.04：训练一个Keras神经网络模型
- en: 'Thankfully, we don''t need to worry about manually programming the components
    of the neural network, such as backpropagation, because the Keras framework manages
    this for us. In this exercise, we will use Keras to train a neural network to
    classify a small subset of the CIFAR-10 dataset using the model architecture defined
    in the preceding exercise. As with all machine learning problems, the first and
    the most important step is to understand as much as possible about the dataset,
    and this will be the initial focus of the exercise:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 感谢我们不需要手动编程神经网络的组件，如反向传播，因为Keras框架会为我们管理这些。在本次练习中，我们将使用Keras训练一个神经网络，使用前一练习中定义的模型架构对CIFAR-10数据集的一个小子集进行分类。与所有机器学习问题一样，第一步也是最重要的一步是尽可能多地了解数据集，这将是本次练习的初步重点：
- en: Note
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` and `batches.meta` files from [https://packt.live/3eexo1s](https://packt.live/3eexo1s).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[https://packt.live/3eexo1s](https://packt.live/3eexo1s)下载`data_batch_1`和`batches.meta`文件。
- en: 'Import `pickle`, `numpy`, `matplotlib`, and the `Sequential` class from `keras.models`,
    and import `Dense` from `keras.layers`. We''ll use `pickle` for this exercise
    to serialize objects in Python for transfer or storage:'
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`、`matplotlib`以及`keras.models`中的`Sequential`类，和`keras.layers`中的`Dense`。我们将在本练习中使用`pickle`来序列化Python对象，以便传输或存储：
- en: '[PRE21]'
  id: totrans-169
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Load the sample of the CIFAR-10 dataset that is provided with the accompanying
    source code in the `data_batch_1` file:'
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载随附源代码提供的CIFAR-10数据集样本，该样本位于`data_batch_1`文件中：
- en: '[PRE22]'
  id: totrans-171
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The data is loaded as a dictionary. Display the keys of the dictionary:'
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据以字典形式加载。显示字典的键：
- en: '[PRE23]'
  id: totrans-173
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The output is as follows:'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE24]'
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Note that the keys are stored as binary strings as denoted by `b''`. We are
    interested in the contents of data and labels. Let''s look at labels first:'
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 请注意，键是以二进制字符串形式存储的，表示为`b'`。我们关注的是数据和标签的内容。首先查看标签：
- en: '[PRE25]'
  id: totrans-177
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'A section of output is as follows, with each class number corresponding to
    one of the text labels (airplane, car, and so on):'
  id: totrans-178
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一部分输出如下，每个类别号对应一个文本标签（飞机、汽车等）：
- en: '![Figure 5.17: Displaying the labels'
  id: totrans-179
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.17：显示标签'
- en: '](img/B15923_05_17.jpg)'
  id: totrans-180
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_17.jpg)'
- en: 'Figure 5.17: Displaying the labels'
  id: totrans-181
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.17：显示标签
- en: 'We can see that the labels are a list of values 0-9, indicating which class
    each sample belongs to. Now, look at the contents of the `data` key:'
  id: totrans-182
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到，标签是一个值为0-9的列表，表示每个样本所属的类别。现在，查看`data`键的内容：
- en: '[PRE26]'
  id: totrans-183
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The output is as follows:'
  id: totrans-184
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.18: Content of the data key'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.18：数据键的内容'
- en: '](img/B15923_05_18.jpg)'
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_18.jpg)'
- en: 'Figure 5.18: Content of the data key'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.18：数据键的内容
- en: The data key provides a NumPy array with all the image data stored within the
    array. What is the shape of the image data?
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 数据键提供了一个NumPy数组，其中存储了所有图像数据。图像数据的形状是什么？
- en: '[PRE27]'
  id: totrans-189
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The output is as follows:'
  id: totrans-190
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE28]'
  id: totrans-191
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We can see that we have 1,000 samples, but each sample is a single dimension
    of 3,072 samples. Aren''t the images supposed to be 32 x 32 pixels? Yes, they
    are, but because the images are color or RGB images, they contain three channels
    (red, green, and blue), which means the images are 32 x 32 x 3\. They are also
    flattened, providing 3,072 length vectors. So, we can reshape the array and then
    visualize a sample of images. According to the CIFAR-10 documentation, the first
    1,024 samples are red, the second 1,024 are green, and the third 1,024 are blue:'
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以看到我们有 1,000 个样本，但每个样本是一个维度为 3,072 的向量。难道这些图片不是应该是 32 x 32 像素吗？是的，它们是，但因为这些图像是彩色的或
    RGB 图像，它们包含三个通道（红色、绿色和蓝色），这意味着图像是 32 x 32 x 3 的大小。它们也被展开，提供 3,072 长度的向量。所以，我们可以重新调整数组形状，然后可视化一部分样本图像。根据
    CIFAR-10 的文档，前 1,024 个样本是红色，第二个 1,024 个是绿色，第三个 1,024 个是蓝色：
- en: '[PRE29]'
  id: totrans-193
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Display the first 12 images, along with their labels:'
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示前 12 张图片及其标签：
- en: '[PRE30]'
  id: totrans-195
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The following output shows a sample of low-resolution images from our dataset
    – which is a result of the 32 x 32 resolution we originally received:'
  id: totrans-196
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下输出显示了我们数据集中低分辨率图像的一个样本——这是由于我们最初收到的 32 x 32 分辨率图像所导致的：
- en: '![Figure 5.19: The first 12 images'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.19：前 12 张图片'
- en: '](img/B15923_05_19.jpg)'
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_19.jpg)'
- en: 'Figure 5.19: The first 12 images'
  id: totrans-199
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.19：前 12 张图片
- en: What is the actual meaning of the labels? We'll find that out in the next step.
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 标签的实际含义是什么？我们将在下一步中找到答案。
- en: 'Load the `batches.meta` file using the following code:'
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下代码加载 `batches.meta` 文件：
- en: '[PRE31]'
  id: totrans-202
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'The output is as follows:'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.20: Meaning of the labels'
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.20：标签的含义'
- en: '](img/B15923_05_20.jpg)'
  id: totrans-205
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_20.jpg)'
- en: 'Figure 5.20: Meaning of the labels'
  id: totrans-206
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.20：标签的含义
- en: 'Decode the binary strings to get the actual labels:'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解码二进制字符串以获得实际标签：
- en: '[PRE32]'
  id: totrans-208
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The output is as follows:'
  id: totrans-209
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.21: Printing the actual labels'
  id: totrans-210
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.21：打印实际标签'
- en: '](img/B15923_05_21.jpg)'
  id: totrans-211
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_21.jpg)'
- en: 'Figure 5.21: Printing the actual labels'
  id: totrans-212
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.21：打印实际标签
- en: 'Print the labels for the first 12 images:'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打印前 12 张图片的标签：
- en: '[PRE33]'
  id: totrans-214
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'The output is as follows:'
  id: totrans-215
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '[PRE34]'
  id: totrans-216
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now we need to prepare the data for training the model. The first step is to
    prepare the output. Currently, the output is a list of numbers 0-9, but we need
    each sample to be represented as a vector of 10 units as per the previous model.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要准备数据来训练模型。第一步是准备输出。目前，输出是一个包含数字 0-9 的列表，但我们需要每个样本表示为一个包含 10 个单元的向量，按照之前的模型来处理。
- en: '[PRE35]'
  id: totrans-218
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Display the one-hot encoding values for the first 12 samples:'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示前 12 个样本的 one-hot 编码值：
- en: '[PRE36]'
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The output is as follows:'
  id: totrans-221
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.22: One-hot encoding values for the first 12 samples'
  id: totrans-222
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.22：前 12 个样本的 one-hot 编码值'
- en: '](img/B15923_05_22.jpg)'
  id: totrans-223
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_22.jpg)'
- en: 'Figure 5.22: One-hot encoding values for the first 12 samples'
  id: totrans-224
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.22：前 12 个样本的 one-hot 编码值
- en: 'The model has 1,024 inputs because it expects a 32 x 32 grayscale image. Take
    the average of the three channels for each image to convert it to RGB:'
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型有 1,024 个输入，因为它期望一个 32 x 32 的灰度图像。对于每张图像，取三个通道的平均值将其转换为 RGB：
- en: '[PRE37]'
  id: totrans-226
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Display the first 12 images again:'
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 再次显示前 12 张图片：
- en: '[PRE38]'
  id: totrans-228
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output is as follows:'
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.23: Displaying the first 12 images again.'
  id: totrans-230
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.23：再次显示前 12 张图片。'
- en: '](img/B15923_05_23.jpg)'
  id: totrans-231
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_23.jpg)'
- en: 'Figure 5.23: Displaying the first 12 images again.'
  id: totrans-232
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.23：再次显示前 12 张图片。
- en: 'Finally, scale the images to be between 0 and 1, which is required for all
    inputs to a neural network. As the maximum value in an image is 255, we will simply
    divide by 255:'
  id: totrans-233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，将图像缩放到 0 到 1 之间，这是神经网络输入所要求的。由于图像中的最大值是 255，我们将其直接除以 255：
- en: '[PRE39]'
  id: totrans-234
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We also need the images to be in the shape 10,000 x 1,024\. We will select
    the first 7,000 samples for training and the last 3,000 samples to evaluate the
    model:'
  id: totrans-235
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还需要将图像调整为 10,000 x 1,024 的形状。我们将选择前 7,000 个样本进行训练，最后 3,000 个样本用于评估模型：
- en: '[PRE40]'
  id: totrans-236
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Redefine the model with the same architecture as *Exercise 5.03*, *Defining
    a Keras Model*:'
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与 *练习 5.03*、*定义一个 Keras 模型* 相同的架构重新定义模型：
- en: '[PRE41]'
  id: totrans-238
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Now we can train the model in Keras. We first need to compile the method to
    specify the training parameters. We will be using categorical cross-entropy, with
    Adam and a performance metric of classification accuracy:'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们可以在 Keras 中训练模型。我们首先需要编译方法来指定训练参数。我们将使用类别交叉熵、Adam 优化器和分类准确度作为性能度量：
- en: '[PRE42]'
  id: totrans-240
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Train the model using backpropagation for 100 epochs and the `fit` method of
    the model:'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用反向传播训练模型 100 个周期，并使用模型的 `fit` 方法：
- en: '[PRE43]'
  id: totrans-242
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The output is as follows. Please note, given the random nature of neural network
    training, that your results may differ slightly:'
  id: totrans-243
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下。请注意，由于神经网络训练的随机性，你的结果可能会略有不同：
- en: '![Figure 5.24: Training the model'
  id: totrans-244
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.24：训练模型'
- en: '](img/B15923_05_24.jpg)'
  id: totrans-245
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_24.jpg)'
- en: 'Figure 5.24: Training the model'
  id: totrans-246
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.24：训练模型
- en: Note
  id: totrans-247
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: Here, we are using Keras to train our neural network model. The initialization
    of weights in a Keras layer is done randomly and cannot be controlled by any random
    seed. Hence, the results may vary slightly each time the code is executed.
  id: totrans-248
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里，我们使用Keras来训练我们的神经网络模型。Keras层中的权重初始化是随机进行的，无法通过任何随机种子来控制。因此，每次运行代码时，结果可能会有所不同。
- en: 'We achieved approximately 75.67% classification accuracy over the training
    data and 32.47% classification accuracy over the validation data (seen in *Figure
    5.24* as `acc: 0.7567` and `val_acc: 0.3247`) for the 10,000 samples using this
    network. Examine the predictions made for the first 12 samples again:'
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '使用这个网络，我们在训练数据上达到了大约75.67%的分类准确率，在验证数据上达到了32.47%的分类准确率（在*图 5.24*中显示为`acc: 0.7567`和`val_acc:
    0.3247`），该网络处理了10,000个样本。再次检查前12个样本的预测结果：'
- en: '[PRE44]'
  id: totrans-250
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The output is as follows:'
  id: totrans-251
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.25: Printing the predictions'
  id: totrans-252
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.25：打印预测结果'
- en: '](img/B15923_05_25.jpg)'
  id: totrans-253
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_25.jpg)'
- en: 'Figure 5.25: Printing the predictions'
  id: totrans-254
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.25：打印预测结果
- en: 'We can use the `argmax` method to determine the most likely class for each
    sample:'
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以使用`argmax`方法来确定每个样本的最可能类别：
- en: '[PRE45]'
  id: totrans-256
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'The output is as follows:'
  id: totrans-257
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE46]'
  id: totrans-258
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Compare with the labels:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与标签进行比较：
- en: '[PRE47]'
  id: totrans-260
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'The output is as follows:'
  id: totrans-261
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '[PRE48]'
  id: totrans-262
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Note
  id: totrans-263
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2CgH25b](https://packt.live/2CgH25b).
  id: totrans-264
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2CgH25b](https://packt.live/2CgH25b)。
- en: You can also run this example online at [https://packt.live/38CKwuD](https://packt.live/38CKwuD).
  id: totrans-265
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/38CKwuD](https://packt.live/38CKwuD)在线运行这个示例。
- en: We have now trained a neural network model in Keras. Complete the next activity
    to further reinforce your skills in training neural networks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在Keras中训练了一个神经网络模型。完成下一个活动以进一步强化你在训练神经网络方面的技能。
- en: 'Activity 5.01: The MNIST Neural Network'
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动 5.01：MNIST神经网络
- en: In this activity, you will train a neural network to identify images in the
    MNIST dataset and reinforce your skills in training neural networks. This activity
    forms the basis of many neural network architectures in different classification
    problems, particularly in computer vision. From object detection and identification
    to classification, this general structure is used in a variety of applications.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，你将训练一个神经网络来识别MNIST数据集中的图像，并强化你在训练神经网络方面的技能。这个活动是许多神经网络架构的基础，尤其是在计算机视觉中的分类问题。从物体检测与识别到分类，这种通用结构在各种应用中得到了使用。
- en: 'These steps will help you to complete the activity:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这些步骤将帮助你完成该活动：
- en: Import `pickle`, `numpy`, `matplotlib`, and the `Sequential` and `Dense` classes
    from Keras.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`、`matplotlib`以及Keras中的`Sequential`和`Dense`类。
- en: Load the `mnist.pkl` file, which contains the first 10,000 images and the corresponding
    labels from the MNIST dataset that are available in the accompanying source code.
    The MNIST dataset is a series of 28 x 28 grayscale images of handwritten digits,
    0 through 9\. Extract the images and labels.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载`mnist.pkl`文件，它包含来自MNIST数据集的前10,000张图像及其对应的标签，源代码中提供了这些数据。MNIST数据集是一个包含0到9手写数字的28x28灰度图像系列。提取图像和标签。
- en: Note
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can find the `mnist.pkl` file at [https://packt.live/2JOLAQB](https://packt.live/2JOLAQB).
  id: totrans-273
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以在[https://packt.live/2JOLAQB](https://packt.live/2JOLAQB)找到`mnist.pkl`文件。
- en: Plot the first 10 samples along with the corresponding labels.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制前10个样本及其对应的标签。
- en: Encode the labels using one-hot encoding.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用独热编码对标签进行编码。
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备将图像输入到神经网络中。作为提示，这个过程包含**两个**独立的步骤。
- en: Construct a neural network model in Keras that accepts the prepared images and
    has a hidden layer of 600 units with a ReLU activation function and an output
    of the same number of units as classes. The output layer uses a `softmax` activation
    function.
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在Keras中构建一个神经网络模型，该模型接受准备好的图像，并具有600个单元的隐藏层，使用ReLU激活函数，输出层的单元数与类别数相同。输出层使用`softmax`激活函数。
- en: Compile the model using multiclass cross-entropy, stochastic gradient descent,
    and an accuracy performance metric.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用多类交叉熵、随机梯度下降和准确性度量来编译模型。
- en: Train the model. How many epochs are required to achieve at least 95% classification
    accuracy on the training data?
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练模型。需要多少个周期才能在训练数据上达到至少95%的分类准确率？
- en: 'By completing this activity, you will have trained a simple neural network
    to identify handwritten digits 0 through 9\. You will have also developed a general
    framework for building neural networks for classification problems. With this
    framework, you can extend and modify the network for a range of other tasks. A
    preview of the digits you will be classifying can be seen here:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个任务后，你将训练一个简单的神经网络来识别手写数字0到9。你还将开发一个通用框架，用于构建分类问题的神经网络。借助这个框架，你可以扩展和修改网络来处理其他各种任务。你将要分类的数字的预览图像如下所示：
- en: '![Figure 26: Preview of digits to be classified'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 26：待分类数字的预览'
- en: '](img/B15923_05_26.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_26.jpg)'
- en: 'Figure 5.26: Preview of digits to be classified'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.26：待分类数字的预览
- en: Note
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 449.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第449页找到。
- en: Autoencoders
  id: totrans-286
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 自编码器
- en: 'Autoencoders are a specifically designed neural network architecture that aims
    to compress the input information into lower dimensional space in an efficient
    yet descriptive manner. Autoencoder networks can be decomposed into two individual
    sub-networks or stages: an **encoding** stage and a **decoding** stage.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 自编码器是一种专门设计的神经网络架构，旨在以高效且描述性强的方式将输入信息压缩到较低的维度空间中。自编码器网络可以分解为两个独立的子网络或阶段：**编码**阶段和**解码**阶段。
- en: 'The following is a simplified autoencoder model using the CIFAR-10 dataset:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个简化的自编码器模型，使用CIFAR-10数据集：
- en: '![Figure 5.27: Simple autoencoder network architecture'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.27：简单的自编码器网络架构'
- en: '](img/B15923_05_27.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_27.jpg)'
- en: 'Figure 5.27: Simple autoencoder network architecture'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.27：简单的自编码器网络架构
- en: The first, or encoding, stage takes the input information and compresses it
    through a subsequent layer that has fewer units than the size of the input sample.
    The latter stage, that is, the decoding stage, then expands the compressed form
    of the image and aims to return the compressed data to its original form. As such,
    the inputs and desired outputs of the network are the same; the network takes,
    say, an image in the CIFAR-10 dataset and tries to return the same image. This
    network architecture is shown in the preceding figure; in this image, we can see
    that the encoding stage of the autoencoder reduces the number of neurons to represent
    the information, while the decoding stage takes the compressed format and returns
    it to its original state. The use of the decoding stage helps to ensure that the
    encoder has correctly represented the information because the compressed representation
    is all that is provided to restore the image in its original state.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个阶段，即编码阶段，将输入信息压缩到一个比输入样本大小更小的后续层中。后续的解码阶段则会扩展压缩后的图像数据，并尝试将其恢复为原始形式。因此，网络的输入和期望输出是相同的；网络输入，例如CIFAR-10数据集中的一张图像，并试图恢复成相同的图像。这个网络架构如上图所示；在这张图中，我们可以看到自编码器的编码阶段减少了表示信息的神经元数量，而解码阶段则将压缩格式恢复为原始状态。使用解码阶段有助于确保编码器正确表示了信息，因为恢复图像所需的仅仅是压缩后的表示。
- en: 'Exercise 5.05: Simple Autoencoder'
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.05：简单自编码器
- en: In this exercise, we will construct a simple autoencoder for the sample of the
    CIFAR-10 dataset, compressing the information stored within the images for later
    use.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将为CIFAR-10数据集样本构建一个简单的自编码器，压缩图像中的信息以供后续使用。
- en: Note
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: In this exercise, we will be using the `data_batch_1` file, which is a sample
    of the CIFAR-10 dataset. This file can be downloaded from [https://packt.live/3bYi5I8](https://packt.live/3bYi5I8).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们将使用`data_batch_1`文件，它是CIFAR-10数据集的一个样本。该文件可以从[https://packt.live/3bYi5I8](https://packt.live/3bYi5I8)下载。
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class, from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  id: totrans-297
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及从`keras.models`导入`Model`类，从`keras.layers`导入`Input`和`Dense`：
- en: '[PRE49]'
  id: totrans-298
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Load the data:'
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据：
- en: '[PRE50]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data:'
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个无监督学习方法，我们只关注图像数据。加载图像数据：
- en: '[PRE51]'
  id: totrans-302
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度图，缩放到0到1之间，并将每张图像展平为一个长度为1,024的向量：
- en: '[PRE52]'
  id: totrans-304
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The output is as follows:'
  id: totrans-305
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 5.28: Scaled image'
  id: totrans-306
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.28：缩放后的图像'
- en: '](img/B15923_05_28.jpg)'
  id: totrans-307
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_28.jpg)'
- en: 'Figure 5.28: Scaled image'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.28：缩放后的图像
- en: 'Define the autoencoder model. As we need access to the output of the encoder
    stage, we will need to define the model using a slightly different method to that
    used previously. Define an input layer of `1024` units:'
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义自编码器模型。由于我们需要访问编码器阶段的输出，因此我们将采用一种与之前略有不同的方法来定义模型。定义一个包含`1024`个单元的输入层：
- en: '[PRE53]'
  id: totrans-310
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Define a subsequent `Dense` layer of `256` units (a compression ratio of 1024/256
    = 4) and a ReLU activation function as the encoding stage. Note that we have assigned
    the layer to a variable and passed the previous layer to a `call` method for the
    class:'
  id: totrans-311
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个后续的`Dense`层，包含`256`个单元（压缩比为1024/256 = 4），并使用ReLU激活函数作为编码阶段。注意，我们已将该层分配给一个变量，并通过`call`方法将前一层传递给该类：
- en: '[PRE54]'
  id: totrans-312
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Define a subsequent decoder layer using the sigmoid function as an activation
    function and the same shape as the input layer. The sigmoid function has been
    selected because the input values to the network are only between 0 and 1:'
  id: totrans-313
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用sigmoid函数作为激活函数，并与输入层相同的形状定义一个后续的解码器层。选择sigmoid函数是因为输入值仅介于0和1之间：
- en: '[PRE55]'
  id: totrans-314
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给`Model`类来构建模型：
- en: '[PRE56]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-317
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和`adadelta`梯度下降编译自编码器：
- en: '[PRE57]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: Note
  id: totrans-319
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: '`adadelta` is a more sophisticated version of stochastic gradient descent where
    the learning rate is adjusted on the basis of a window of recent gradient updates.
    Compared to the other methods of modifying the learning rate, this prevents the
    gradient of very old epochs from influencing the learning rate.'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '`adadelta`是一种更为复杂的随机梯度下降版本，其中学习率基于最近的梯度更新窗口进行调整。与其他调整学习率的方法相比，它可以防止非常旧的周期梯度影响学习率。'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始训练模型；同样，我们将图像作为训练数据并作为期望的输出。训练100个周期：
- en: '[PRE58]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'The output is as follows:'
  id: totrans-323
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.29: Training the model'
  id: totrans-324
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.29：训练模型'
- en: '](img/B15923_05_29.jpg)'
  id: totrans-325
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_29.jpg)'
- en: 'Figure 5.29: Training the model'
  id: totrans-326
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.29：训练模型
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出：
- en: '[PRE59]'
  id: totrans-328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Reshape the encoder output to 16 x 16 (16 x 16 = 256) pixels and multiply by
    255:'
  id: totrans-329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出重新调整为16 x 16（16 x 16 = 256）像素，并乘以255：
- en: '[PRE60]'
  id: totrans-330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  id: totrans-331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的解码阶段输出：
- en: '[PRE61]'
  id: totrans-332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Reshape the output of the decoder to 32 x 32 and multiply by 255:'
  id: totrans-333
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器的输出重新调整为32 x 32并乘以255：
- en: '[PRE62]'
  id: totrans-334
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Reshape the original images:'
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新调整原始图像：
- en: '[PRE63]'
  id: totrans-336
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The output is as follows:'
  id: totrans-337
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.30: Output of the simple autoencoder'
  id: totrans-338
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.30：简单自编码器的输出'
- en: '](img/B15923_05_30.jpg)'
  id: totrans-339
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_30.jpg)'
- en: 'Figure 5.30: Output of the simple autoencoder'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.30：简单自编码器的输出
- en: In the preceding figure, we can see three rows of images. The first row is the
    original grayscale image, the second row is the corresponding autoencoder output
    for the original image, and finally, the third row is the reconstruction of the
    original image from the encoded input. We can see that the decoded images in the
    third row contain information about the basic shape of the image; we can see the
    main body of the frog and the deer, as well as the outline of the trucks and cars
    in the sample. Given that we only trained the model for 100 samples, this exercise
    would also benefit from an increase in the number of training epochs to further
    improve the performance of both the encoder and decoder. Now that we have the
    output of the autoencoder stage trained, we can use it as the feature vector for
    other unsupervised algorithms, such as K-means or K nearest neighbors.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的图中，我们可以看到三行图像。第一行是原始的灰度图像，第二行是对应的自编码器输出，第三行是从编码输入中重构的原始图像。我们可以看到第三行解码后的图像包含了图像基本形状的信息；我们可以看到青蛙和鹿的主体，以及样本中卡车和汽车的轮廓。由于我们只训练了100个样本，因此增加训练周期的数量将有助于进一步提升编码器和解码器的性能。现在，我们已经得到了训练好的自编码器阶段的输出，可以将其作为其他无监督算法（如K均值或K近邻）的特征向量。
- en: Note
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2BQH03R](https://packt.live/2BQH03R).
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此特定部分的源代码，请参考[https://packt.live/2BQH03R](https://packt.live/2BQH03R)。
- en: You can also run this example online at [https://packt.live/2Z9CMgI](https://packt.live/2Z9CMgI).
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2Z9CMgI](https://packt.live/2Z9CMgI)在线运行此示例。
- en: 'Activity 5.02: Simple MNIST Autoencoder'
  id: totrans-345
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.02：简单的MNIST自动编码器
- en: 'In this activity, you will create an autoencoder network for the MNIST dataset
    contained within the accompanying source code. An autoencoder network such as
    the one built in this activity can be extremely useful in the preprocessing stage
    of unsupervised learning. The encoded information produced by the network can
    be used in clustering or segmentation analysis, such as image-based web searches:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 在本活动中，您将为随附源代码中的MNIST数据集创建一个自动编码器网络。像本活动中构建的自动编码器网络在无监督学习的预处理阶段非常有用。网络生成的编码信息可以用于聚类或分割分析，例如基于图像的网页搜索：
- en: Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model`, `Input`,
    and `Dense` classes, from Keras.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及Keras中的`Model`、`Input`和`Dense`类。
- en: Load the images from the supplied sample of the MNIST dataset that is provided
    with the accompanying source code (`mnist.pkl`).
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从随附源代码中提供的MNIST数据集样本中加载图像（`mnist.pkl`）。
- en: Note
  id: totrans-349
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `mnist.pkl` file from [https://packt.live/2wmpyl5](https://packt.live/2wmpyl5).
  id: totrans-350
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以从[https://packt.live/2wmpyl5](https://packt.live/2wmpyl5)下载`mnist.pkl`文件。
- en: Prepare the images for input into a neural network. As a hint, there are **two**
    separate steps in this process.
  id: totrans-351
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为神经网络准备图像。作为提示，整个过程有**两个**独立的步骤。
- en: Construct a simple autoencoder network that reduces the image size to 10 x 10
    after the encoding stage.
  id: totrans-352
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个简单的自动编码器网络，将图像大小减少到编码阶段后的10 x 10。
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  id: totrans-353
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和`adadelta`梯度下降法编译自动编码器。
- en: Fit the encoder model.
  id: totrans-354
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 适配编码器模型。
- en: Calculate and store the output of the encoding stage for the first five samples.
  id: totrans-355
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出。
- en: Reshape the encoder output to 10 x 10 (10 x 10 = 100) pixels and multiply by
    255.
  id: totrans-356
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器输出重塑为10 x 10（10 x 10 = 100）像素，并乘以255。
- en: Calculate and store the output of the decoding stage for the first five samples.
  id: totrans-357
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储解码阶段前五个样本的输出。
- en: Reshape the output of the decoder to 28 x 28 and multiply by 255.
  id: totrans-358
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器的输出重塑为28 x 28并乘以255。
- en: Plot the original image, the encoder output, and the decoder.
  id: totrans-359
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、编码器输出和解码器的图像。
- en: 'By completing this activity, you will have successfully trained an autoencoder
    network that extracts the critical information from the dataset, preparing it
    for later processing. The output will be similar to the following:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，您将成功训练一个自动编码器网络，从数据集中提取关键信息，为后续处理做好准备。输出将类似于以下内容：
- en: '![Figure 5.31: Expected plot of the original image, the encoder output, and
    the decoder'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.31：原始图像、编码器输出和解码器的预期图]'
- en: '](img/B15923_05_31.jpg)'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_31.jpg)'
- en: 'Figure 5.31: Expected plot of the original image, the encoder output, and the
    decoder'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.31：原始图像、编码器输出和解码器的预期图
- en: Note
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The solution to this activity can be found on page 452.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 该活动的解决方案可以在第452页找到。
- en: 'Exercise 5.06: Multi-Layer Autoencoder'
  id: totrans-366
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习5.06：多层自动编码器
- en: 'In this exercise, we will construct a multi-layer autoencoder for the sample
    of the CIFAR-10 dataset, compressing the information stored within the images
    for later use:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在本练习中，我们将为CIFAR-10数据集样本构建一个多层自动编码器，将图像中存储的信息压缩，以便后续使用：
- en: Note
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` file from [https://packt.live/2VcY0a9](https://packt.live/2VcY0a9).
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[https://packt.live/2VcY0a9](https://packt.live/2VcY0a9)下载`data_batch_1`文件。
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class, from
    `keras.models`, and import `Input` and `Dense` from `keras.layers`:'
  id: totrans-370
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及`keras.models`中的`Model`类，导入`keras.layers`中的`Input`和`Dense`：
- en: '[PRE64]'
  id: totrans-371
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Load the data:'
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据：
- en: '[PRE65]'
  id: totrans-373
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the preceding exercise:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一个无监督学习方法，我们只关心图像数据。请按照前面的练习加载图像数据：
- en: '[PRE66]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Convert the image to grayscale, scale between 0 and 1, and flatten each to
    a single 1,024 length vector:'
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像转换为灰度图，缩放到0和1之间，并将每个图像展平为一个长度为1,024的向量：
- en: '[PRE67]'
  id: totrans-377
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'The output is as follows:'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.32: Scaled image'
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图5.32：缩放后的图像]'
- en: '](img/B15923_05_32.jpg)'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_32.jpg)'
- en: 'Figure 5.32: Scaled image'
  id: totrans-381
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.32：缩放图像
- en: 'Define the multi-layer autoencoder model. We will use the same shape input
    as the simple autoencoder model:'
  id: totrans-382
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义多层自动编码器模型。我们将使用与简单自动编码器模型相同的输入形状：
- en: '[PRE68]'
  id: totrans-383
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'We will add another layer before the 256 autoencoder stage – this time with
    512 neurons:'
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将在 256 自动编码器阶段之前添加另一个层——这次使用 512 个神经元：
- en: '[PRE69]'
  id: totrans-385
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'We''re using the same size autoencoder as in *Exercise 5.05*, *Simple Autoencoder*,
    but the input to the layer is the `hidden_encoding` layer this time:'
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们使用与*练习 5.05*、*简单自动编码器*相同大小的自动编码器，但这次层的输入是`hidden_encoding`层：
- en: '[PRE70]'
  id: totrans-387
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Add a decoding hidden layer:'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加解码隐藏层：
- en: '[PRE71]'
  id: totrans-389
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Use the same output stage as in the previous exercise, this time connected
    to the hidden decoding stage:'
  id: totrans-390
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用与上一练习相同的输出阶段，这次连接到隐藏解码阶段：
- en: '[PRE72]'
  id: totrans-391
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-392
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一个和最后一个层传递给`Model`类来构建模型：
- en: '[PRE73]'
  id: totrans-393
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二进制交叉熵损失函数和`adadelta`梯度下降编译自动编码器：
- en: '[PRE74]'
  id: totrans-395
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Train for 100 epochs:'
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们拟合模型；再次将图像作为训练数据和期望的输出。训练 100 epochs：
- en: '[PRE75]'
  id: totrans-397
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'The output is as follows:'
  id: totrans-398
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.33: Training the model'
  id: totrans-399
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.33：训练模型'
- en: '](img/B15923_05_33.jpg)'
  id: totrans-400
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_33.jpg)'
- en: 'Figure 5.33: Training the model'
  id: totrans-401
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.33：训练模型
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-402
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储编码阶段前五个样本的输出：
- en: '[PRE76]'
  id: totrans-403
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Reshape the encoder output to 16 x 16 (16 x 16 = 256) pixels and multiply by
    255:'
  id: totrans-404
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将编码器的输出调整为 16 x 16（16 x 16 = 256）像素并乘以 255：
- en: '[PRE77]'
  id: totrans-405
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Calculate and store the output of the decoding stage for the first five samples:'
  id: totrans-406
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储解码阶段前五个样本的输出：
- en: '[PRE78]'
  id: totrans-407
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'Reshape the output of the decoder to 32 x 32 and multiply by 255:'
  id: totrans-408
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器的输出调整为 32 x 32 并乘以 255：
- en: '[PRE79]'
  id: totrans-409
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'Plot the original image, the encoder output, and the decoder:'
  id: totrans-410
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、编码器输出和解码器：
- en: '[PRE80]'
  id: totrans-411
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'The output is as follows:'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.34: Output of the multi-layer autoencoder'
  id: totrans-413
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.34：多层自动编码器的输出'
- en: '](img/B15923_05_34.jpg)'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_34.jpg)'
- en: 'Figure 5.34: Output of the multi-layer autoencoder'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.34：多层自动编码器的输出
- en: By looking at the error score produced by both the simple and multilayer autoencoders
    and by comparing *Figure 5.30* and *Figure 5.34*, we can see that there is little
    difference between the output of the two encoder structures. The middle row of
    both figures show that the features learned by the two models are, in fact, different.
    There are a number of options we can use to improve both of these models, such
    as training for more epochs, using a different number of units or neurons in the
    layers, or using varying numbers of layers. This exercise was constructed to demonstrate
    how to build and use an autoencoder, but optimization is often a process of systematic
    trial and error. We encourage you to adjust some of the parameters of the model
    and investigate the different results for yourself.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看简单自动编码器和多层自动编码器产生的误差得分，并比较*图 5.30*和*图 5.34*，我们可以看到两种编码器结构的输出几乎没有差别。两张图的中间行显示出这两种模型学到的特征实际上是不同的。我们可以使用许多方法来改善这两种模型，例如训练更多的epochs、使用不同数量的单元或神经元，或使用不同数量的层。本练习的目的是展示如何构建和使用自动编码器，但优化通常是一个系统性的试错过程。我们鼓励你调整一些模型参数，并自己探索不同的结果。
- en: Note
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZbaT81](https://packt.live/2ZbaT81).
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 若要访问此特定部分的源代码，请参阅[https://packt.live/2ZbaT81](https://packt.live/2ZbaT81)。
- en: You can also run this example online at [https://packt.live/2ZHvOyo](https://packt.live/2ZHvOyo).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在[https://packt.live/2ZHvOyo](https://packt.live/2ZHvOyo)在线运行此示例。
- en: Convolutional Neural Networks
  id: totrans-420
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: In constructing all of our previous neural network models, you would have noticed
    that we removed all the color information when converting the image to grayscale,
    and then flattened each image into a single vector of length 1,024\. In doing
    so, we essentially threw out a lot of information that may be of use to us. The
    colors in the images may be specific to the class or objects in the image; additionally,
    we lost a lot of our spatial information pertaining to the image; for example,
    the position of the trailer in the truck image relative to the cab or the legs
    of the deer relative to the head. Convolutional neural networks do not suffer
    from this information loss. This is because, rather than using a flat structure
    of trainable parameters, they store the weights in a grid or matrix, which means
    that each group of parameters can have many layers in their structure. By organizing
    the weights in a grid, we prevent the loss of spatial information because the
    weights are applied in a sliding fashion across the image. Also, by having many
    layers, we can retain the color channels associated with the image.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建所有以前的神经网络模型时，您可能已经注意到，在将图像转换为灰度图像并将每个图像展平为长度为 1,024 的单一向量时，我们移除了所有颜色信息。这样做实质上丢失了很多可能对我们有用的信息。图像中的颜色可能与图像中的类或对象特定相关；此外，我们还丢失了关于图像的空间信息，例如卡车图像中拖车相对驾驶室的位置或鹿的腿相对头部的位置。卷积神经网络不会遭受这种信息丢失。这是因为它们不是使用可训练参数的平面结构，而是将权重存储在网格或矩阵中，这意味着每组参数可以在其结构中有多层。通过将权重组织在网格中，我们可以防止空间信息的丢失，因为权重是以滑动方式应用于图像的。此外，通过具有多个层，我们可以保留与图像相关的颜色通道。
- en: 'In developing convolutional neural network-based autoencoders, the MaxPooling2D
    and Upsampling2D layers are very important. The MaxPooling 2D layer downsamples
    or reduces the size of an input matrix in two dimensions by selecting the maximum
    value within a window of the input. Say we had a 2 x 2 matrix, where three cells
    have a value of 1 and one single cell has a value of 2:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发基于卷积神经网络的自编码器时，MaxPooling2D 和 Upsampling2D 层非常重要。MaxPooling 2D 层通过在输入的窗口内选择最大值来在两个维度上减少或缩小输入矩阵的大小。假设我们有一个
    2 x 2 的矩阵，其中三个单元格的值为 1，一个单元格的值为 2：
- en: '![Figure 5.35: Demonstration of a sample matrix'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.35：示例矩阵演示'
- en: '](img/B15923_05_35.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_35.jpg)'
- en: 'Figure 5.35: Demonstration of a sample matrix'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.35：示例矩阵演示
- en: If provided to the MaxPooling2D layer, this matrix would return a single value
    of 2, thus reducing the size of the input in both directions by one half.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 如果提供给 MaxPooling2D 层，则该矩阵将返回一个值为 2 的单个值，从而在两个方向上将输入的大小减少一半。
- en: The UpSampling2D layer has the opposite effect as that of the MaxPooling2D layer,
    increasing the size of the input rather than reducing it. The upsampling process
    repeats the rows and columns of the data, thus doubling the size of the input
    matrix. For the preceding example, you would have the 2 x 2 matrix converted into
    a 4 x 4 matrix, with the bottom right 4 pixels at value 2, and the rest at value
    1.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: UpSampling2D 层的作用与 MaxPooling2D 层相反，它增加输入的大小而不是减小它。上采样过程重复数据的行和列，从而使输入矩阵的大小加倍。对于前面的例子，您将把一个
    2 x 2 的矩阵转换成一个 4 x 4 的矩阵，其中右下角的 4 个像素值为 2，其余为 1。
- en: 'Exercise 5.07: Convolutional Autoencoder'
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习 5.07：卷积自编码器
- en: 'In this exercise, we will develop a convolutional neural network-based autoencoder
    and compare performance with the previous fully connected neural network autoencoder:'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将开发基于卷积神经网络的自编码器，并与之前的全连接神经网络自编码器性能进行比较：
- en: Note
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `data_batch_1` file from [https://packt.live/2x31ww3](https://packt.live/2x31ww3).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从 [https://packt.live/2x31ww3](https://packt.live/2x31ww3) 下载 `data_batch_1`
    文件。
- en: 'Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class, from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`:'
  id: totrans-432
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入 `pickle`、`numpy` 和 `matplotlib`，以及从 `keras.models` 导入 `Model` 类，从 `keras.layers`
    导入 `Input`、`Conv2D`、`MaxPooling2D` 和 `UpSampling2D`：
- en: '[PRE81]'
  id: totrans-433
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Load the data:'
  id: totrans-434
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据：
- en: '[PRE82]'
  id: totrans-435
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'As this is an unsupervised learning method, we are only interested in the image
    data. Load the image data as per the preceding exercise:'
  id: totrans-436
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于这是一种无监督学习方法，我们只对图像数据感兴趣。按照前面的练习加载图像数据：
- en: '[PRE83]'
  id: totrans-437
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'As we are using a convolutional network, we can use the images with only rescaling:'
  id: totrans-438
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们使用卷积网络，我们可以仅对图像进行重新缩放使用：
- en: '[PRE84]'
  id: totrans-439
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Define the convolutional autoencoder model. We will use the same shape input
    as an image:'
  id: totrans-440
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义卷积自编码器模型。我们将使用与图像相同的形状输入：
- en: '[PRE85]'
  id: totrans-441
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Add a convolutional stage with 32 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using the same padding, which means the output
    has the same length as the input image.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个包含32层或滤波器的卷积阶段，使用3 x 3的权重矩阵，ReLU激活函数，并使用相同的填充，这意味着输出的长度与输入图像相同。
- en: '[PRE86]'
  id: totrans-443
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Add a max pooling layer to the encoder with a 2 x 2 kernel. `MaxPooling` looks
    at all the values in an image, scanning through with a 2 x 2 matrix. The maximum
    value in each 2 x 2 area is returned, thus reducing the size of the encoded layer
    by a half:'
  id: totrans-444
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向编码器中添加一个2 x 2核的最大池化层。`MaxPooling`会查看图像中的所有值，使用2 x 2矩阵进行扫描。在每个2 x 2区域中返回最大值，从而将编码层的大小减少一半：
- en: '[PRE87]'
  id: totrans-445
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Add a decoding convolutional layer (this layer should be identical to the previous
    convolutional layer):'
  id: totrans-446
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个解码卷积层（该层应该与之前的卷积层相同）：
- en: '[PRE88]'
  id: totrans-447
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'Now we need to return the image to its original size, for which we will upsample
    by the same size as `MaxPooling2D`:'
  id: totrans-448
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们需要将图像恢复到原始大小，方法是将上采样设置为与`MaxPooling2D`相同的大小：
- en: '[PRE89]'
  id: totrans-449
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Add the final convolutional stage using three layers for the RGB channels of
    the images:'
  id: totrans-450
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加最后的卷积阶段，使用三层来处理图像的RGB通道：
- en: '[PRE90]'
  id: totrans-451
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Construct the model by passing the first and last layers of the network to
    the `Model` class:'
  id: totrans-452
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给`Model`类来构建模型：
- en: '[PRE91]'
  id: totrans-453
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Display the structure of the model:'
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型的结构：
- en: '[PRE92]'
  id: totrans-455
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE92]'
- en: 'The output is as follows:'
  id: totrans-456
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.36: Structure of the model'
  id: totrans-457
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.36：模型结构'
- en: '](img/B15923_05_36.jpg)'
  id: totrans-458
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_36.jpg)'
- en: 'Figure 5.36: Structure of the model'
  id: totrans-459
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.36：模型结构
- en: Note
  id: totrans-460
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: We have far fewer trainable parameters compared to the previous autoencoder
    examples. This has been a specific design decision to ensure that the example
    runs on a wide variety of hardware. Convolutional networks typically require a
    lot more processing power and often special hardware such as Graphical Processing
    Units (GPUs).
  id: totrans-461
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 与之前的自编码器示例相比，我们的可训练参数要少得多。这是一个特定的设计决策，旨在确保该示例能在各种硬件上运行。卷积网络通常需要更多的处理能力，并且常常需要像图形处理单元（GPU）这样的特殊硬件。
- en: 'Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent:'
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二元交叉熵损失函数和`adadelta`梯度下降编译自编码器：
- en: '[PRE93]'
  id: totrans-463
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'Now, let''s fit the model; again, we pass the images as the training data and
    as the desired output. Instead of training for 100 epochs like before, we will
    use 20 epochs, since convolutional networks take a lot longer to compute:'
  id: totrans-464
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们拟合模型；再次地，我们将图像作为训练数据和期望输出传递。与之前训练100个周期不同，这次我们将使用20个周期，因为卷积网络的计算时间要长得多：
- en: '[PRE94]'
  id: totrans-465
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'The output is as follows:'
  id: totrans-466
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.37: Training the model'
  id: totrans-467
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.37：训练模型'
- en: '](img/B15923_05_37.jpg)'
  id: totrans-468
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_37.jpg)'
- en: 'Figure 5.37: Training the model'
  id: totrans-469
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.37：训练模型
- en: Note that the error was already less than in the previous autoencoder exercise
    after the second epoch, suggesting a better encoding/decoding model. This reduced
    error can be mostly attributed to the fact that the convolutional neural network
    did not discard a lot of data, and the encoded images are 16 x 16 x 32, which
    is significantly larger than the previous 16 x 16 size. Additionally, we have
    not compressed the images *per se* as they now contain fewer pixels (16 x 16 x
    32 = 8,192), but with more depth (32 x 32 x 3 = 3,072) than before. This information
    has been rearranged to allow more effective encoding/decoding processes.
  id: totrans-470
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意，在第二个周期后，误差已经比之前的自编码器练习更小，这表明编码/解码模型更好。这个误差减少主要归功于卷积神经网络没有丢弃大量数据，且编码后的图像为16
    x 16 x 32，比之前的16 x 16尺寸要大得多。此外，我们没有压缩图像*本身*，因为它们现在包含的像素较少（16 x 16 x 32 = 8,192），但比之前有更多的深度（32
    x 32 x 3 = 3,072）。这些信息已经重新排列，以便进行更有效的编码/解码处理。
- en: 'Calculate and store the output of the encoding stage for the first five samples:'
  id: totrans-471
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出：
- en: '[PRE95]'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Each encoded image has a shape of 16 x 16 x 32 due to the number of filters
    selected for the convolutional stage. As such, we cannot visualize them without
    modification. We will reshape them to be 256 x 32 in size for visualization:'
  id: totrans-473
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个编码后的图像的形状为16 x 16 x 32，这是由于为卷积阶段选择的滤波器数量。因此，在没有修改的情况下，我们无法对其进行可视化。我们将其重塑为256
    x 32的大小，以便进行可视化：
- en: '[PRE96]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Get the output of the decoder for the first five images:'
  id: totrans-475
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取前五张图像的解码器输出：
- en: '[PRE97]'
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'Plot the original image, the mean encoder output, and the decoder:'
  id: totrans-477
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、平均编码器输出和解码器：
- en: '[PRE98]'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 'The output is as follows:'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 5.38: The original image, the encoder output, and the decoder output'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 5.38：原始图像、编码器输出和解码器输出'
- en: '](img/B15923_05_38.jpg)'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/B15923_05_38.jpg)'
- en: 'Figure 5.38: The original image, the encoder output, and the decoder output'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.38：原始图像、编码器输出和解码器输出
- en: Note
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: To access the source code for this specific section, please refer to [https://packt.live/2VYprpq](https://packt.live/2VYprpq).
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取此特定部分的源代码，请参考[https://packt.live/2VYprpq](https://packt.live/2VYprpq)。
- en: You can also run this example online at [https://packt.live/38EDgic](https://packt.live/38EDgic).
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在线运行此示例，网址为[https://packt.live/38EDgic](https://packt.live/38EDgic)。
- en: 'Activity 5.03: MNIST Convolutional Autoencoder'
  id: totrans-486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 活动5.03：MNIST卷积自编码器
- en: 'In this activity, we will reinforce our knowledge of convolutional autoencoders
    using the MNIST dataset. Convolutional autoencoders typically achieve significantly
    improved performance when working with image-based datasets of a reasonable size.
    This is particularly useful when using autoencoders to generate artificial image
    samples:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个活动中，我们将通过MNIST数据集加强卷积自编码器的知识。卷积自编码器通常在处理大小适中的基于图像的数据集时能够显著提高性能。这在使用自编码器生成人工图像样本时特别有用：
- en: Import `pickle`, `numpy`, and `matplotlib`, as well as the `Model` class, from
    `keras.models`, and import `Input`, `Conv2D`, `MaxPooling2D`, and `UpSampling2D`
    from `keras.layers`.
  id: totrans-488
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入`pickle`、`numpy`和`matplotlib`，以及从`keras.models`导入`Model`类，并从`keras.layers`导入`Input`、`Conv2D`、`MaxPooling2D`和`UpSampling2D`。
- en: Load the `mnist.pkl` file, which contains the first 10,000 images and corresponding
    labels from the MNIST dataset, which are available in the accompanying source
    code.
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载包含前10,000个图像及其对应标签的`mnist.pkl`文件，这些数据可以在附带的源代码中找到。
- en: Note
  id: totrans-490
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 注意
- en: You can download the `mnist.pkl` file from [https://packt.live/3e4HOR1](https://packt.live/3e4HOR1).
  id: totrans-491
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 你可以从[https://packt.live/3e4HOR1](https://packt.live/3e4HOR1)下载`mnist.pkl`文件。
- en: Rescale the images to have values between 0 and 1.
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像重新缩放，使其值介于0和1之间。
- en: We need to reshape the images to add a single depth channel for use with convolutional
    stages. Reshape the images to have a shape of 28 x 28 x 1.
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要重塑图像，增加一个单一的深度通道以供卷积阶段使用。将图像重塑为28 x 28 x 1的形状。
- en: Define an input layer. We will use the same shape input as an image.
  id: totrans-494
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个输入层。我们将使用与图像相同的输入形状。
- en: Add a convolutional stage, with 16 layers or filters, a 3 x 3 weight matrix,
    a ReLU activation function, and using the same padding, which means the output
    has the same length as the input image.
  id: totrans-495
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个卷积阶段，包含16层或滤波器，一个3 x 3的权重矩阵，一个ReLU激活函数，并使用相同的填充方式，这意味着输出图像的尺寸与输入图像相同。
- en: Add a max pooling layer to the encoder with a 2 x 2 kernel.
  id: totrans-496
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向编码器添加一个最大池化层，使用2 x 2的核。
- en: Add a decoding convolutional layer.
  id: totrans-497
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个解码卷积层。
- en: Add an upsampling layer.
  id: totrans-498
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加一个上采样层。
- en: Add the final convolutional stage using one layer as per the initial image depth.
  id: totrans-499
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据初始图像深度，添加最终的卷积阶段，使用一层。
- en: Construct the model by passing the first and last layers of the network to the
    `Model` class.
  id: totrans-500
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将网络的第一层和最后一层传递给`Model`类来构建模型。
- en: Display the structure of the model.
  id: totrans-501
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 显示模型的结构。
- en: Compile the autoencoder using a binary cross-entropy loss function and `adadelta`
    gradient descent.
  id: totrans-502
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用二进制交叉熵损失函数和`adadelta`梯度下降来编译自编码器。
- en: Now, let's fit the model; again, we pass the images as the training data and
    as the desired output. Train for 20 epochs as convolutional networks take a lot
    longer to compute.
  id: totrans-503
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，让我们开始拟合模型；我们再次将图像作为训练数据并作为期望的输出。训练20个周期，因为卷积神经网络需要更长的计算时间。
- en: Calculate and store the output of the encoding stage for the first five samples.
  id: totrans-504
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算并存储前五个样本的编码阶段输出。
- en: Reshape the encoder output for visualization, where each image is `X*Y` in size.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了可视化，重新调整编码器输出的形状，使每个图像为`X*Y`大小。
- en: Get the output of the decoder for the first five images.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取前五个图像的解码器输出。
- en: Reshape the decoder output to be `28 x 28` in size.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将解码器输出重塑为`28 x 28`的大小。
- en: Reshape the original images back to be `28 x 28` in size.
  id: totrans-508
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始图像重塑为`28 x 28`的大小。
- en: Plot the original image, the mean encoder output, and the decoder.
  id: totrans-509
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制原始图像、平均编码器输出和解码器输出。
- en: 'At the end of this activity, you will have developed an autoencoder comprising
    convolutional layers within the neural network. Note the improvements made in
    the decoder representations. The output will be similar to the following:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次活动结束时，你将开发出一个包含卷积层的自编码器神经网络。请注意，解码器表示中所做的改进。输出将类似于以下内容：
- en: '![Figure 5.39: Expected original image, the encoder output, and the decoder'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.39：预期的原始图像、编码器输出和解码器输出'
- en: '](img/B15923_05_39.jpg)'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B15923_05_39.jpg)'
- en: 'Figure 5.39: Expected original image, the encoder output, and the decoder'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.39：预期的原始图像、编码器输出和解码器
- en: Note
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 注释
- en: The solution to this activity can be found on page 455.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案可以在第455页找到。
- en: Summary
  id: totrans-516
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we started with an introduction to artificial neural networks,
    how they are structured, and the processes by which they learn to complete a particular
    task. Starting with a supervised learning example, we built an artificial neural
    network classifier to identify objects within the CIFAR-10 dataset. We then progressed
    to the autoencoder architecture of neural networks and learned how we can use
    these networks to prepare a dataset for use in an unsupervised learning problem.
    Finally, we completed this investigation with autoencoders, looking at convolutional
    neural networks and the benefits that these additional layers can provide. This
    chapter prepared us well for the final installment of dimensionality reduction,
    when we will look at using and visualizing the encoded data with t-distributed
    nearest neighbors (t-SNE). T-distributed nearest neighbors provides an extremely
    effective method for visualizing high-dimensional data even after applying reduction
    techniques such as PCA. T-SNE is a particularly useful method for unsupervised
    learning. In the next chapter, we will talk more about embeddings, which are critical
    tools that help us deal with high-dimensional data. As you saw with the CIFAR-10
    dataset in this chapter, color image files can rapidly increase in size and slow
    down the performance of any neural network algorithm. By using dimensionality
    reduction, we can minimize the impact of high-dimensional data.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们首先介绍了人工神经网络的基本概念，讲解了它们的结构以及它们是如何学习完成特定任务的。以一个有监督学习的例子为起点，我们构建了一个人工神经网络分类器来识别CIFAR-10数据集中的物体。接着，我们探讨了神经网络的自编码器架构，并学习了如何使用这些网络来准备数据集，以便在无监督学习问题中使用。最后，我们通过自编码器的研究，进一步了解了卷积神经网络，并探讨了这些额外层能够带来的好处。本章为我们最终探讨降维问题做好了准备，我们将在降维过程中学习如何使用和可视化编码后的数据，使用t分布最近邻（t-SNE）算法。t-SNE提供了一种极其有效的可视化高维数据的方法，即便在应用了诸如PCA等降维技术之后。t-SNE在无监督学习中尤其有用。在下一章中，我们将进一步探讨嵌入技术，它们是处理高维数据的重要工具。正如你在本章中的CIFAR-10数据集中看到的那样，彩色图像文件的大小可能会迅速增大，从而减慢任何神经网络算法的性能。通过使用降维技术，我们可以最小化高维数据的影响。
