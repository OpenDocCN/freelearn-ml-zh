- en: Chapter 10. From the Perceptron to Artificial Neural Networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章. 从感知机到人工神经网络
- en: In [Chapter 8](ch08.html "Chapter 8. The Perceptron"), *The Perceptron*, we
    introduced the perceptron, which is a linear model for binary classification.
    You learned that the perceptron is not a universal function approximator; its
    decision boundary must be a hyperplane. In the previous chapter we introduced
    the support vector machine, which redresses some of the perceptron's limitations
    by using kernels to efficiently map the feature representations to a higher dimensional
    space in which the instances are linearly separable. In this chapter, we will
    discuss **artificial neural networks**, which are powerful nonlinear models for
    classification and regression that use a different strategy to overcome the perceptron's
    limitations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](ch08.html "第8章. 感知机")，*感知机*中，我们介绍了感知机，它是一个用于二分类的线性模型。你了解到，感知机并不是一个通用的函数逼近器；它的决策边界必须是一个超平面。在前一章中，我们介绍了支持向量机，通过使用核函数将特征表示有效地映射到一个更高维的空间，从而使实例能够线性可分，解决了感知机的一些局限性。在本章中，我们将讨论**人工神经网络**，它是用于分类和回归的强大非线性模型，采用了一种不同的策略来克服感知机的局限性。
- en: If the perceptron is analogous to a neuron, an artificial neural network, or
    **neural net**, is analogous to a brain. As billions of neurons with trillions
    of synapses comprise a human brain, an artificial neural network is a directed
    graph of perceptrons or other artificial neurons. The graph's edges are weighted;
    these weights are the parameters of the model that must be learned.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果感知机类似于神经元，那么人工神经网络，或称**神经网络**，则类似于大脑。正如数十亿个神经元与数万亿个突触组成了人类大脑，人工神经网络是由感知机或其他人工神经元组成的有向图。该图的边有权重；这些权重是模型的参数，必须通过学习来获得。
- en: Entire books describe individual aspects of artificial neural networks; this
    chapter will provide an overview of their structure and training. At the time
    of writing, some artificial neural networks have been developed for scikit-learn,
    but they are not available in Version 0.15.2\. Readers can follow the examples
    in this chapter by checking out a fork of scikit-learn 0.15.1 that includes the
    neural network module. The implementations in this fork are likely to be merged
    into future versions of scikit-learn without any changes to the API described
    in this chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 整本书讲述了人工神经网络的各个方面；本章将概述其结构和训练方法。在写作时，已经为scikit-learn开发了一些人工神经网络，但它们在版本0.15.2中不可用。读者可以通过查看包含神经网络模块的scikit-learn
    0.15.1的分支来跟随本章的示例。该分支中的实现可能会在未来的scikit-learn版本中合并，且不会对本章中描述的API做出任何更改。
- en: Nonlinear decision boundaries
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非线性决策边界
- en: 'Recall from [Chapter 8](ch08.html "Chapter 8. The Perceptron"), *The Perceptron*,
    that while some Boolean functions such as AND, OR, and NAND can be approximated
    by the perceptron, the linearly inseparable function XOR cannot, as shown in the
    following plots:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第8章](ch08.html "第8章. 感知机")，*感知机*，我们提到过，虽然一些布尔函数如AND、OR和NAND可以通过感知机来近似实现，但如以下图所示，线性不可分的函数XOR是无法通过感知机来实现的：
- en: '![Nonlinear decision boundaries](img/8365OS_10_01.jpg)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![非线性决策边界](img/8365OS_10_01.jpg)'
- en: Let's review XOR in more detail to develop an intuition for the power of artificial
    neural networks. In contrast to AND, which outputs 1 when both of its inputs are
    equal to 1, and OR, which outputs 1 when at least one of the inputs are equal
    to 1, the output of XOR is 1 when exactly one of its inputs are equal to 1\. We
    could view XOR as outputting 1 when two conditions are true. The first condition
    is that at least one of the inputs must be equal to 1; this is the same condition
    that OR tests. The second condition is that not both of the inputs are equal to
    1; NAND tests this condition. We can produce the same output as XOR by processing
    the input with both OR and NAND and then verifying that the outputs of both functions
    are equal to 1 using AND. That is, the functions OR, NAND, and AND can be composed
    to produce the same output as XOR.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地回顾XOR（异或），以便直观理解人工神经网络的强大功能。与当两个输入都为1时，AND输出1；当至少有一个输入为1时，OR输出1不同，XOR的输出是1，仅当其中一个输入为1时。我们可以将XOR视为当两个条件成立时输出1。第一个条件是至少有一个输入必须为1；这与OR测试的条件相同。第二个条件是不能两个输入都为1；NAND测试这个条件。我们可以通过将输入先用OR和NAND处理，再用AND验证两个函数的输出都为1来产生与XOR相同的输出。也就是说，OR、NAND和AND这三个函数可以组合成与XOR相同的输出。
- en: 'The following tables provide the truth tables for XOR, OR, AND, and NAND for
    the inputs *A* and *B*. From these tables we can verify that inputting the output
    of OR and NAND to AND produces the same output as inputting *A* and *B* to XOR:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了输入 *A* 和 *B* 的异或、或、与、非与的真值表。通过这些表格，我们可以验证将或（OR）和非与（NAND）的输出输入到与（AND）中，得到的结果与将
    *A* 和 *B* 输入到异或（XOR）中得到的结果相同：
- en: '| A | B | A AND B | A NAND B | A OR B | A XOR B |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| A | B | A 和 B | A 非与 B | A 或 B | A 异或 B |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| 0 | 0 | 0 | 1 | 0 | 0 |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 | 0 | 0 |'
- en: '| 0 | 1 | 0 | 1 | 1 | 1 |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 | 1 | 1 | 1 |'
- en: '| 1 | 0 | 0 | 1 | 1 | 1 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 1 | 1 | 1 |'
- en: '| 1 | 1 | 1 | 0 | 1 | 0 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 0 | 1 | 0 |'
- en: '| A | B | A OR B | A NAND B | (A OR B) AND (A NAND B) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| A | B | A 或 B | A 非与 B | (A 或 B) 和 (A 非与 B) |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | 0 | 0 | 1 | 0 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 1 | 0 |'
- en: '| 0 | 1 | 1 | 1 | 1 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 1 | 1 | 1 |'
- en: '| 1 | 0 | 1 | 1 | 1 |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 1 | 1 | 1 |'
- en: '| 1 | 1 | 1 | 0 | 0 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 | 0 | 0 |'
- en: Instead of trying to represent XOR with a single perceptron, we will build an
    artificial neural network from multiple artificial neurons that each approximate
    a linear function. Each instance's feature representation will be input to two
    neurons; one neuron will represent NAND and the other will represent OR. The output
    of these neurons will be received by a third neuron that represents AND to test
    whether both of XOR's conditions are true.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会尝试用单个感知器表示异或（XOR），而是从多个人工神经元构建人工神经网络，每个神经元逼近一个线性函数。每个实例的特征表示将作为输入传递给两个神经元；一个神经元表示非与（NAND），另一个神经元表示或（OR）。这些神经元的输出将被第三个神经元接收，该神经元表示与（AND），用于测试异或的两个条件是否同时为真。
- en: Feedforward and feedback artificial neural networks
  id: totrans-22
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 前馈和反馈人工神经网络
- en: Artificial neural networks are described by three components. The first is the
    model's **architecture**, or topology, which describes the layers of neurons and
    structure of the connections between them. The second component is the activation
    function used by the artificial neurons. The third component is the learning algorithm
    that finds the optimal values of the weights.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络由三个部分组成。第一部分是模型的**架构**，或称拓扑结构，描述了神经元的层次以及它们之间连接的结构。第二部分是人工神经元使用的激活函数。第三部分是学习算法，用于寻找最优的权重值。
- en: There are two main types of artificial neural networks. **Feedforward neural
    networks** are the most common type of neural net, and are defined by their directed
    acyclic graphs. Signals only travel in one direction—towards the output layer—in
    feedforward neural networks. Conversely, **feedback neural networks**, or recurrent
    neural networks, do contain cycles. The feedback cycles can represent an internal
    state for the network that can cause the network's behavior to change over time
    based on its input. Feedforward neural networks are commonly used to learn a function
    to map an input to an output. The temporal behavior of feedback neural networks
    makes them suitable for processing sequences of inputs. Because feedback neural
    networks are not implemented in scikit-learn, we will limit our discussion to
    only feedforward neural networks.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络主要有两种类型。**前馈神经网络**是最常见的一种神经网络，其特点是定向无环图。前馈神经网络中的信号只会朝一个方向传播——向输出层传播。相反，**反馈神经网络**，或称递归神经网络，则包含循环。反馈循环可以表示网络的内部状态，根据输入的不同，反馈循环可以导致网络行为随时间发生变化。前馈神经网络通常用于学习一个函数，将输入映射到输出。由于反馈神经网络的时间行为特性，它们更适合处理输入序列。由于反馈神经网络在
    scikit-learn 中没有实现，我们将在讨论中仅限于前馈神经网络。
- en: Multilayer perceptrons
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 多层感知器
- en: The **multilayer perceptron** (**MLP**) is the one of the most commonly used
    artificial neural networks. The name is a slight misnomer; a multilayer perceptron
    is not a single perceptron with multiple layers, but rather multiple layers of
    artificial neurons that can be perceptrons. The layers of the MLP form a directed,
    acyclic graph. Generally, each layer is fully connected to the subsequent layer;
    the output of each artificial neuron in a layer is an input to every artificial
    neuron in the next layer towards the output. MLPs have three or more layers of
    artificial neurons.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '**多层感知器**（**MLP**）是最常用的人工神经网络之一。这个名字有些误导；多层感知器并不是一个具有多层的单一感知器，而是由多个层次的人工神经元组成，这些神经元可以是感知器。MLP
    的各层形成一个定向无环图。通常，每一层都与后续的层完全连接；每一层中的每个人工神经元的输出都是下一层中每个人工神经元的输入，直到输出层。MLP 通常有三层或更多层的人工神经元。'
- en: The **input layer** consists of simple input neurons. The input neurons are
    connected to at least one **hidden layer** of artificial neurons. The hidden layer
    represents latent variables; the input and output of this layer cannot be observed
    in the training data. Finally, the last hidden layer is connected to an **output**
    **layer**. The following diagram depicts the architecture of a multilayer perceptron
    with three layers. The neurons labeled **+1** are bias neurons and are not depicted
    in most architecture diagrams.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入层**由简单的输入神经元组成。 输入神经元连接到至少一个**隐藏层**的人工神经元。 隐藏层表示潜在变量； 该层的输入和输出在训练数据中无法观察到。
    最后，最后一个隐藏层连接到**输出层**。 以下图表描述了具有三层的多层感知器的架构。 标有**+1**的神经元是偏置神经元，大多数架构图中没有描绘。'
- en: '![Multilayer perceptrons](img/8365OS_10_02.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/8365OS_10_02.jpg)'
- en: 'The artificial neurons, or **units**, in the hidden layer commonly use nonlinear
    activation functions such as the hyperbolic tangent function and the logistic
    function, which are given by the following equations:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的人工神经元或**单元**通常使用非线性激活函数，如双曲正切函数和逻辑函数，其方程如下：
- en: '![Multilayer perceptrons](img/8365OS_10_03.jpg)![Multilayer perceptrons](img/8365OS_10_04.jpg)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/8365OS_10_03.jpg)![多层感知器](img/8365OS_10_04.jpg)'
- en: 'As with other supervised models, our goal is to find the values of the weights
    that minimize the value of a cost function. The mean squared error cost function
    is commonly used with multilayer perceptrons. It is given by the following equation,
    where *m* is the number of training instances:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他监督模型一样，我们的目标是找到最小化成本函数值的权重值。 平方误差成本函数通常与多层感知器一起使用。 其由以下方程给出，其中*m*是训练实例的数量：
- en: '![Multilayer perceptrons](img/8365OS_10_05.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![多层感知器](img/8365OS_10_05.jpg)'
- en: Minimizing the cost function
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 最小化成本函数
- en: The **backpropagation** algorithm is commonly used in conjunction with an optimization
    algorithm such as gradient descent to minimize the value of the cost function.
    The algorithm takes its name from a portmanteau of *backward propagation*, and
    refers to the direction in which errors flow through the layers of the network.
    Backpropagation can theoretically be used to train a feedforward network with
    any number of hidden units arranged in any number of layers, though computational
    power constrains this capability.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '**反向传播**算法通常与梯度下降等优化算法结合使用，以最小化成本函数的值。 该算法以*反向传播*的混成词命名，并指向网络层中错误流动的方向。 反向传播理论上可用于训练任意数量的隐藏单元排列在任意数量的层的前馈网络，尽管计算能力限制了这种能力。'
- en: Backpropagation is similar to gradient descent in that it uses the gradient
    of the cost function to update the values of the model parameters. Unlike the
    linear models we have previously seen, neural nets contain hidden units that represent
    latent variables; we can't tell what the hidden units should do from the training
    data. If we do not know what the hidden units should do, we cannot calculate their
    errors and we cannot calculate the gradient of cost function with respect to their
    weights. A naive solution to overcome this is to randomly perturb the weights
    for the hidden units. If a random change to one of the weights decreases the value
    of the cost function, we save the change and randomly change the value of another
    weight. An obvious problem with this solution is its prohibitive computational
    cost. Backpropagation provides a more efficient solution.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播类似于梯度下降，它使用成本函数的梯度来更新模型参数的值。 与我们之前看到的线性模型不同，神经网络包含表示潜在变量的隐藏单元； 我们无法从训练数据中知道隐藏单元应该做什么。
    如果我们不知道隐藏单元应该做什么，我们就无法计算它们的错误，也无法计算成本函数相对于它们的权重的梯度。 克服这一问题的一个天真解决方案是随机扰动隐藏单元的权重。
    如果对一个权重的随机改变减少了成本函数的值，我们保存该改变并随机改变另一个权重的值。 这种解决方案的明显问题是其昂贵的计算成本。 反向传播提供了一个更有效的解决方案。
- en: We will step through training a feedforward neural network using backpropagation.
    This network has two input units, two hidden layers that both have three hidden
    units, and two output units. The input units are both fully connected to the first
    hidden layer's units, called `Hidden1`, `Hidden2`, and `Hidden3`. The edges connecting
    the units are initialized to small random weights.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过反向传播训练一个前馈神经网络。该网络有两个输入单元，两个隐藏层，每个隐藏层有三个隐藏单元，和两个输出单元。输入单元完全连接到第一个隐藏层的单元，分别称为`Hidden1`、`Hidden2`和`Hidden3`。连接单元的边缘被初始化为小的随机权重。
- en: Forward propagation
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 前向传播
- en: 'During the forward propagation stage, the features are input to the network
    and fed through the subsequent layers to produce the output activations. First,
    we compute the activation for the unit `Hidden1`. We find the weighted sum of
    input to `Hidden1`, and then process the sum with the activation function. Note
    that `Hidden1` receives a constant input from a bias unit that is not depicted
    in the diagram in addition to the inputs from the input units. In the following
    diagram, ![Forward propagation](img/8365OS_10_42.jpg) is the activation function:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在前向传播阶段，特征被输入到网络，并通过后续层传递以产生输出激活值。首先，我们计算`Hidden1`单元的激活值。我们找到输入到`Hidden1`的加权和，然后通过激活函数处理这个和。注意，`Hidden1`除了接收来自输入单元的输入外，还接收来自偏置单元的恒定输入，偏置单元在图中未显示。在下图中，![前向传播](img/8365OS_10_42.jpg)是激活函数：
- en: '![Forward propagation](img/8365OS_10_21.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播](img/8365OS_10_21.jpg)'
- en: 'Next, we compute the activation for the second hidden unit. Like the first
    hidden unit, it receives weighted inputs from both of the input units and a constant
    input from a bias unit. We then process the weighted sum of the inputs, or **preactivation**,
    with the activation function as shown in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算第二个隐藏单元的激活值。与第一个隐藏单元一样，它接收来自两个输入单元的加权输入，以及来自偏置单元的恒定输入。然后，我们将加权输入的和或**预激活**通过激活函数处理，如下图所示：
- en: '![Forward propagation](img/8365OS_10_22.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播](img/8365OS_10_22.jpg)'
- en: 'We then compute the activation for `Hidden3` in the same manner:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们以相同的方式计算`Hidden3`的激活值：
- en: '![Forward propagation](img/8365OS_10_23.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播](img/8365OS_10_23.jpg)'
- en: 'Having computed the activations of all of the hidden units in the first layer,
    we proceed to the second hidden layer. In this network, the first hidden layer
    is fully connected to the second hidden layer. Similar to the units in the first
    hidden layer, the units in the second hidden layer receive a constant input from
    bias units that are not depicted in the diagram. We proceed to compute the activation
    of `Hidden4`:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 计算完第一层所有隐藏单元的激活值后，我们继续处理第二层隐藏单元。在这个网络中，第一层隐藏单元与第二层隐藏单元完全连接。与第一层隐藏单元类似，第二层隐藏单元接收来自偏置单元的恒定输入，这些偏置单元在图中未显示。接下来，我们计算`Hidden4`的激活值：
- en: '![Forward propagation](img/8365OS_10_24.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播](img/8365OS_10_24.jpg)'
- en: 'We next compute the activations of `Hidden5` and `Hidden6`. Having computed
    the activations of all of the hidden units in the second hidden layer, we proceed
    to the output layer in the following figure. The activation of `Output1` is the
    weighted sum of the second hidden layer''s activations processed through an activation
    function. Similar to the hidden units, the output units both receive a constant
    input from a bias unit:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算`Hidden5`和`Hidden6`的激活值。计算完第二层所有隐藏单元的激活值后，我们进入输出层，如下图所示。`Output1`的激活值是第二层隐藏单元的激活值的加权和，通过激活函数处理后得到。与隐藏单元类似，输出单元也接收来自偏置单元的恒定输入：
- en: '![Forward propagation](img/8365OS_10_25.jpg)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播](img/8365OS_10_25.jpg)'
- en: 'We calculate the activation of `Output2` in the same manner:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以相同的方式计算`Output2`的激活值：
- en: '![Forward propagation](img/8365OS_10_26.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![前向传播](img/8365OS_10_26.jpg)'
- en: We computed the activations of all of the units in the network, and we have
    now completed forward propagation. The network is not likely to approximate the
    true function well using the initial random values of the weights. We must now
    update the values of the weights so that the network can better approximate our
    function.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经计算出网络中所有单元的激活值，现在前向传播已完成。由于网络使用初始随机权重值，可能无法很好地逼近真实函数。我们现在必须更新权重值，使网络能够更好地逼近我们的函数。
- en: Backpropagation
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反向传播
- en: 'We can calculate the error of the network only at the output units. The hidden
    units represent latent variables; we cannot observe their true values in the training
    data and thus, we have nothing to compute their error against. In order to update
    their weights, we must propagate the network''s errors backwards through its layers.
    We will begin with `Output1`. Its error is equal to the difference between the
    true and predicted outputs, multiplied by the partial derivative of the unit''s
    activation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只能在输出单元处计算网络的误差。隐藏单元代表潜在变量；我们无法在训练数据中观察到它们的真实值，因此无法计算它们的误差。为了更新它们的权重，我们必须将网络的误差通过其各层反向传播。我们将从`Output1`开始。它的误差等于真实输出与预测输出之间的差，乘以该单元激活函数的偏导数：
- en: '![Backpropagation](img/8365OS_10_27.jpg)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_27.jpg)'
- en: 'We then calculate the error of the second output unit:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算第二个输出单元的误差：
- en: '![Backpropagation](img/8365OS_10_28.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_28.jpg)'
- en: 'We computed the errors of the output layer. We can now propagate these errors
    backwards to the second hidden layer. First, we will compute the error of hidden
    unit `Hidden4`. We multiply the error of `Output1` by the value of the weight
    connecting `Hidden4` and `Output1`. We similarly weigh the error of `Output2`.
    We then add these errors and calculate the product of their sum and the partial
    derivative of `Hidden4`:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了输出层的误差。现在我们可以将这些误差反向传播到第二个隐藏层。首先，我们将计算隐藏单元`Hidden4`的误差。我们将`Output1`的误差与连接`Hidden4`和`Output1`的权重值相乘。我们同样计算`Output2`的误差。然后我们将这些误差相加，并计算它们的和与`Hidden4`的偏导数的乘积：
- en: '![Backpropagation](img/8365OS_10_38.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_38.jpg)'
- en: 'We similarly compute the errors of `Hidden5`:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样计算了`Hidden5`的误差：
- en: '![Backpropagation](img/8365OS_10_29.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_29.jpg)'
- en: 'We then compute the `Hidden6` error in the following figure:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们计算下图中的`Hidden6`误差：
- en: '![Backpropagation](img/8365OS_10_30.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_30.jpg)'
- en: 'We calculated the error of the second hidden layer with respect to the output
    layer. Next, we will continue to propagate the errors backwards towards the input
    layer. The error of the hidden unit `Hidden1` is the product of its partial derivative
    and the weighted sums of the errors in the second hidden layer:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了第二隐藏层相对于输出层的误差。接下来，我们将继续将误差反向传播到输入层。隐藏单元`Hidden1`的误差是它的偏导数与第二隐藏层中误差的加权和的乘积：
- en: '![Backpropagation](img/8365OS_10_31.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_31.jpg)'
- en: 'We similarly compute the error for hidden unit `Hidden2`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样计算隐藏单元`Hidden2`的误差：
- en: '![Backpropagation](img/8365OS_10_32.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_32.jpg)'
- en: 'We similarly compute the error for `Hidden3`:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同样计算了`Hidden3`的误差：
- en: '![Backpropagation](img/8365OS_10_33.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_33.jpg)'
- en: We computed the errors of the first hidden layer. We can now use these errors
    to update the values of the weights. We will first update the weights for the
    edges connecting the input units to `Hidden1` as well as the weight for the edge
    connecting the bias unit to `Hidden1`. We will increment the value of the weight
    connecting `Input1` and `Hidden1` by the product of the learning rate, error of
    `Hidden1`, and the value of `Input1`.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计算了第一隐藏层的误差。现在我们可以使用这些误差来更新权重值。我们将首先更新连接输入单元到`Hidden1`的边的权重，以及连接偏置单元到`Hidden1`的边的权重。我们将通过学习率、`Hidden1`的误差和`Input1`的值的乘积来递增连接`Input1`和`Hidden1`的权重值。
- en: We will similarly increment the value of `Weight2` by the product of the learning
    rate, error of `Hidden1`, and the value of `Input2`. Finally, we will increment
    the value of the weight connecting the bias unit to `Hidden1` by the product of
    the learning rate, error of `Hidden1`, and one.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将同样通过学习率、`Hidden1`的误差和`Input2`的值的乘积来递增`Weight2`的值。最后，我们将通过学习率、`Hidden1`的误差和1的乘积来递增连接偏置单元到`Hidden1`的权重值。
- en: '![Backpropagation](img/8365OS_10_34.jpg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_34.jpg)'
- en: 'We will then update the values of the weights connecting hidden unit `Hidden2`
    to the input units and the bias unit using the same method:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们将使用相同的方法更新连接隐藏单元`Hidden2`到输入单元和偏置单元的权重值：
- en: '![Backpropagation](img/8365OS_10_35.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_35.jpg)'
- en: 'Next, we will update the values of the weights connecting the input layer to
    `Hidden3`:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将更新连接输入层到`Hidden3`的权重值：
- en: '![Backpropagation](img/8365OS_10_36.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_36.jpg)'
- en: 'Since the values of the weights connecting the input layer to the first hidden
    layer is updated, we can continue to the weights connecting the first hidden layer
    to the second hidden layer. We will increment the value of `Weight7` by the product
    of the learning rate, error of `Hidden4`, and the output of `Hidden1`. We continue
    to similarly update the values of weights `Weight8` to `Weight15`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 自输入层到第一个隐藏层的权重值更新后，我们可以继续处理连接第一个隐藏层到第二个隐藏层的权重。我们将`Weight7`的值增加学习率、`Hidden4`的误差和`Hidden1`的输出的乘积。接着，类似地更新`Weight8`到`Weight15`的权重值：
- en: '![Backpropagation](img/8365OS_10_37.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_37.jpg)'
- en: 'The weights for `Hidden5` and `Hidden6` are updated in the same way. We updated
    the values of the weights connecting the two hidden layers. We can now update
    the values of the weights connecting the second hidden layer and the output layer.
    We increment the values of weights `W16` through `W21` using the same method that
    we used for the weights in the previous layers:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '`Hidden5`和`Hidden6`的权重更新方式相同。我们更新了连接两个隐藏层的权重值。现在，我们可以更新连接第二个隐藏层和输出层的权重值。使用与前几层权重相同的方法，我们递增了`W16`到`W21`的权重值：'
- en: '![Backpropagation](img/8365OS_10_40.jpg)![Backpropagation](img/8365OS_10_41.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![反向传播](img/8365OS_10_40.jpg)![反向传播](img/8365OS_10_41.jpg)'
- en: After incrementing the value of `Weight21` by the product of the learning rate,
    error of `Output2`, and the activation of `Hidden6`, we have finished updating
    the values of the weights for the network. We can now perform another forward
    pass using the new values of the weights; the value of the cost function produced
    using the updated weights should be smaller. We will repeat this process until
    the model converges or another stopping criterion is satisfied. Unlike the linear
    models we have discussed, backpropagation does not optimize a convex function.
    It is possible that backpropagation will converge on parameter values that specify
    a local, rather than global, minimum. In practice, local optima are frequently
    adequate for many applications.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将`Weight21`的值增加学习率、`Output2`的误差和`Hidden6`的激活的乘积后，我们完成了对网络权重的值的更新。现在，我们可以使用新的权重值执行另一个前向传播；使用更新后的权重计算得到的成本函数值应该更小。我们将重复此过程，直到模型收敛或满足其他停止标准。与我们讨论过的线性模型不同，反向传播不会优化凸函数。反向传播可能会收敛于指定局部而非全局最小值的参数值。在实践中，对于许多应用而言，局部最优通常是足够的。
- en: Approximating XOR with Multilayer perceptrons
  id: totrans-80
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 用多层感知器逼近异或
- en: Let's train a multilayer perceptron to approximate the XOR function. At the
    time of writing, multilayer perceptrons have been implemented as part of a 2014
    Google Summer of Code project, but have not been merged or released. Subsequent
    versions of scikit-learn are likely to include this implementation of multilayer
    perceptrons without any changes to the API described in this section. In the interim,
    a fork of scikit-learn 0.15.1 that includes the multilayer perceptron implementation
    can be cloned from [https://github.com/IssamLaradji/scikit-learn.git](https://github.com/IssamLaradji/scikit-learn.git).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练一个多层感知器来逼近异或函数。在撰写本文时，多层感知器已作为2014年Google Summer of Code项目的一部分实现，但尚未合并或发布。未来版本的scikit-learn很可能会包含这个多层感知器的实现，而API描述的部分将不会有任何改变。在此期间，可以从[https://github.com/IssamLaradji/scikit-learn.git](https://github.com/IssamLaradji/scikit-learn.git)克隆包含多层感知器实现的scikit-learn
    0.15.1分支。
- en: 'First, we will create a toy binary classification dataset that represents XOR
    and split it into training and testing sets:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将创建一个玩具二元分类数据集，代表异或，并将其分为训练集和测试集：
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Next we instantiate `MultilayerPerceptronClassifier`. We specify the architecture
    of the network through the `n_hidden` keyword argument, which takes a list of
    the number of hidden units in each hidden layer. We create a hidden layer with
    two units that use the logistic activation function. The `MultilayerPerceptronClassifier`
    class automatically creates two input units and one output unit. In multi-class
    problems the classifier will create one output unit for each of the possible classes.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实例化`MultilayerPerceptronClassifier`。我们通过`n_hidden`关键字参数指定网络的架构，该参数接受一个隐藏层中隐藏单元数的列表。我们创建了一个使用逻辑激活函数的具有两个单元的隐藏层。`MultilayerPerceptronClassifier`类会自动创建两个输入单元和一个输出单元。在多类问题中，分类器将为每个可能的类别创建一个输出单元。
- en: Selecting an architecture is challenging. There are some rules of thumb to choose
    the numbers of hidden units and layers, but these tend to be supported only by
    anecdotal evidence. The optimal number of hidden units depends on the number of
    training instances, the noise in the training data, the complexity of the function
    that is being approximated, the hidden units' activation function, the learning
    algorithm, and the regularization employed. In practice, architectures can only
    be evaluated by comparing their performances through cross validation.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 选择一个架构是具有挑战性的。虽然有一些经验法则可以选择隐藏单元和层数的数量，但这些规则通常仅仅是通过轶事证据来支持的。最佳的隐藏单元数量取决于训练实例的数量、训练数据中的噪声、要逼近的函数的复杂性、隐藏单元的激活函数、学习算法以及采用的正则化方法。在实践中，架构只能通过交叉验证比较它们的性能来进行评估。
- en: 'We train the network by calling the fit() method:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用fit()方法训练网络：
- en: '[PRE1]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Finally, we print some predictions for manual inspection and evaluate the model''s
    accuracy on the test set. The network perfectly approximates the XOR function
    on the test set:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们打印一些预测结果以供手动检查，并评估模型在测试集上的准确性。该网络在测试集上完美地逼近了XOR函数：
- en: '[PRE2]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Classifying handwritten digits
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 手写数字分类
- en: 'In the previous chapter we used a support vector machine to classify the handwritten
    digits in the MNIST dataset. In this section we will classify the images using
    an artificial neural network:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用支持向量机对MNIST数据集中的手写数字进行了分类。在本节中，我们将使用人工神经网络对图像进行分类：
- en: '[PRE3]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'First we use the `load_digits` convenience function to load the MNIST dataset.
    We will fork additional processes during cross validation, which requires execution
    from a `main-`protected block:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`load_digits`便捷函数加载MNIST数据集。我们将在交叉验证期间分叉额外的进程，这需要从`main-`保护块中执行：
- en: '[PRE4]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Scaling the features is particularly important for artificial neural networks
    and will help some learning algorithms to converge more quickly. Next, we create
    a `Pipeline` class that scales the data before fitting a `MultilayerPerceptronClassifier`.
    This network contains an input layer, a hidden layer with 150 units, a hidden
    layer with 100 units, and an output layer. We also increased the value of the
    regularization hyperparameter `alpha` argument. Finally, we print the accuracies
    of the three cross validation folds. The code is as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 特征缩放对于人工神经网络尤为重要，它将帮助某些学习算法更快地收敛。接下来，我们创建一个`Pipeline`类，在拟合`MultilayerPerceptronClassifier`之前先对数据进行缩放。该网络包含一个输入层、一个具有150个单元的隐藏层、一个具有100个单元的隐藏层和一个输出层。我们还增加了正则化超参数`alpha`的值。最后，我们打印三个交叉验证折叠的准确率。代码如下：
- en: '[PRE5]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The mean accuracy is comparable to the accuracy of the support vector classifier.
    Adding more hidden units or hidden layers and grid searching to tune the hyperparameters
    could further improve the accuracy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 平均准确率与支持向量分类器的准确率相当。增加更多的隐藏单元或隐藏层，并进行网格搜索来调整超参数，可能会进一步提高准确率。
- en: Summary
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we introduced artificial neural networks, powerful models for
    classification and regression that can represent complex functions by composing
    several artificial neurons. In particular, we discussed directed acyclic graphs
    of artificial neurons called feedforward neural networks. Multilayer perceptrons
    are a type of feedforward network in which each layer is fully connected to the
    subsequent layer. An MLP with one hidden layer and a finite number of hidden units
    is a universal function approximator. It can represent any continuous function,
    though it will not necessarily be able to learn appropriate weights automatically.
    We described how the hidden layers of a network represent latent variables and
    how their weights can be learned using the backpropagation algorithm. Finally,
    we used scikit-learn's multilayer perceptron implementation to approximate the
    function XOR and to classify handwritten digits.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们介绍了人工神经网络，它们是用于分类和回归的强大模型，可以通过组合多个人工神经元来表示复杂的函数。特别地，我们讨论了人工神经元的有向无环图，称为前馈神经网络。多层感知机是前馈网络的一种类型，其中每一层都与后续层完全连接。一个具有一个隐藏层和有限数量隐藏单元的MLP是一个通用的函数逼近器。它可以表示任何连续函数，尽管它不一定能够自动学习适当的权重。我们描述了网络的隐藏层如何表示潜在变量，以及如何使用反向传播算法学习它们的权重。最后，我们使用scikit-learn的多层感知机实现来逼近XOR函数并分类手写数字。
- en: This chapter concludes the book. We discussed a variety of models, learning
    algorithms, and performance measures, as well as their implementations in scikit-learn.
    In the first chapter, we described machine learning programs as those that learn
    from experience to improve their performance at a task. Then, we worked through
    examples that demonstrated some of the most common experiences, tasks, and performance
    measures in machine learning. We regressed the prices of pizzas onto their diameters
    and classified spam and ham text messages. We clustered colors to compress images
    and clustered the SURF descriptors to recognize photographs of cats and dogs.
    We used principal component analysis for facial recognition, built a random forest
    to block banner advertisements, and used support vector machines and artificial
    neural networks for optical character recognition. Thank you for reading; I hope
    that you will be able to use scikit-learn and this book's examples to apply machine
    learning to your own experiences.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 本章总结了本书的内容。我们讨论了各种模型、学习算法和性能评估标准，以及它们在 scikit-learn 中的实现。在第一章中，我们将机器学习程序定义为那些通过经验学习来改善其任务表现的程序。随后，我们通过实例演示了机器学习中一些最常见的经验、任务和性能评估标准。我们对比萨的价格与其直径进行了回归分析，并对垃圾邮件和普通文本消息进行了分类。我们将颜色聚类用于图像压缩，并对SURF描述符进行了聚类以识别猫狗的照片。我们使用主成分分析进行面部识别，构建了随机森林以屏蔽横幅广告，并使用支持向量机和人工神经网络进行光学字符识别。感谢您的阅读；希望您能够利用
    scikit-learn 以及本书中的示例，将机器学习应用到您自己的实践中。
