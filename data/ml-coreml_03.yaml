- en: Recognizing Objects in the World
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别世界中的物体
- en: In this chapter, we will immerse ourselves in the world of **machine learning**
    (**ML**) and Core ML by working through what could be considered the 101 Core
    ML application. We will be using an image classification model to allow the user
    to point their iPhone at anything and have the app classify the most dominant
    object in the view.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过研究可能被认为是 101 Core ML 应用程序的内容，深入到机器学习（**ML**）和 Core ML 的世界中。我们将使用一个图像分类模型，使用户能够将
    iPhone 对准任何物体，并让应用识别视图中最占主导地位的物体。
- en: We will start off by first discussing the concept of **convolutional neural
    networks** (**ConvNets** or **CNNs**), a category of neural networks well suited
    to image classification, before jumping into implementation. Starting from a skeleton
    project, you will soon discover just how easy it is to integrate ML into your
    apps with the help of Core ML.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论**卷积神经网络**（**ConvNets** 或 **CNNs**），这是一种非常适合图像分类的神经网络类别，然后再进入实现阶段。从骨架项目开始，你很快就会发现，在
    Core ML 的帮助下，将机器学习集成到你的应用中是多么容易。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Gaining some intuition on how machines understand images
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得一些关于机器如何理解图像的直观认识
- en: Building out the example application for this chapter
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建本章的示例应用程序
- en: Capturing photo frames and preprocessing them before passing them to the Core
    ML model
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获照片帧并在传递给 Core ML 模型之前对其进行预处理
- en: Using the Core ML model to perform inference and interpreting the result
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Core ML 模型进行推理并解释结果
- en: Convolutional neural networks are commonly referred to as either CNNs or ConvNets,
    and these terms are used interchangeably throughout this book.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积神经网络通常被称为 CNNs 或 ConvNets，这两个术语在这本书中是互换使用的。
- en: Understanding images
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解图像
- en: As mentioned previously, it's not my intention to give you a theoretical or
    deep understanding of any particular ML algorithm, but rather gently introduce
    you to some of the main concepts. This will help you to gain an intuitive understanding
    of how they work so that you know where and how to apply them, as well as give
    you a platform to dive deeper into the particular subject, which I strongly encourage
    you to do.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我的意图不是给你一个特定 ML 算法的理论或深入理解，而是温和地介绍一些主要概念。这将帮助你获得对它们如何工作的直观理解，以便你知道在哪里以及如何应用它们，同时为你提供一个深入了解特定主题的平台，我强烈建议你这样做。
- en: 'For a good introductory text on deep learning, I strongly recommend Andrew
    Trask''s book *Grokking Deep Learning*. For a general introduction to ML, I would
    recommend Toby Segaran''s book *Programming Collective Intelligence: Building
    Smart Web 2.0 Applications*.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '对于一本关于深度学习的优秀入门书籍，我强烈推荐 Andrew Trask 的书《Grokking Deep Learning》。对于 ML 的一般介绍，我建议
    Toby Segaran 的书《Programming Collective Intelligence: Building Smart Web 2.0 Applications》。'
- en: In this section, we will be introducing CNNs, specifically introducing what
    they are and why they are well suited for spatial data, that is, images. But before
    discussing CNNs, we will start by inspecting the data; then we'll see why CNNs
    perform better than their counterpart, fully connected neural networks (or just neural
    networks).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 CNNs，特别是介绍它们是什么以及为什么它们非常适合空间数据，即图像。但在讨论 CNNs 之前，我们将首先检查数据；然后我们将看到为什么
    CNNs 比它们的对应物，即全连接神经网络（或简称神经网络）表现得更好。
- en: 'For the purpose of illustrating these concepts, consider the task of classifying
    the following digits, where each digit is represented as a 5 x 5 matrix of pixels.
    The dark gray pixels have a value of 1 and light gray pixels have a value of 0:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这些概念，考虑以下任务：对以下数字进行分类，其中每个数字都表示为 5 x 5 像素矩阵。深灰色像素的值为 1，浅灰色像素的值为 0：
- en: '![](img/9b143785-9e1f-4f17-b091-8bffa35774ab.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9b143785-9e1f-4f17-b091-8bffa35774ab.png)'
- en: 'Using a fully connected neural network (single hidden layer), our model would
    learn the joint probability of each pixel with respect to their associated label;
    that is, the model will assign positive weights to pixels that correlate with
    the label and using the output with the highest likelihood to be the most probable
    label. During training, we take each image and flatten it before feeding into
    our network, as shown in the following diagram:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 使用全连接神经网络（单个隐藏层），我们的模型将学习每个像素与其相关标签的联合概率；也就是说，模型将为与标签相关的像素分配正权重，并使用最可能性的输出作为最可能的标签。在训练过程中，我们将每个图像展平，然后将其输入到我们的网络中，如下面的图所示：
- en: '![](img/7c2c3b67-caff-406a-94c2-042feea11f35.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/7c2c3b67-caff-406a-94c2-042feea11f35.png)'
- en: This works remarkably well, and if you have experience with ML, particularly
    deep learning, you would have likely come across the MNIST dataset. It's a dataset
    consisting of labeled handwritten digits, where each digit is centrally rendered
    to a 28 x 28 gray scale (single channel with the pixel value ranging from 0-255)
    image. Using a single-layer fully connected network will likely result in a validation
    accuracy close to 90%. But what happens if we introduce some complexities such
    as moving the image around a larger space, as illustrated in the following diagram?
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这效果非常好，如果你有机器学习，尤其是深度学习的经验，你很可能已经遇到过MNIST数据集。这是一个包含标记的手写数字数据集，每个数字都中心渲染成28 x
    28的灰度图（单通道，像素值范围从0-255）。使用单层全连接网络很可能会得到接近90%的验证准确率。但是，如果我们引入一些复杂性，比如将图像移动到一个更大的空间中，如图所示，会发生什么呢？
- en: '![](img/36c99f3d-9e48-4a1c-8eba-eba8cda49949.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/36c99f3d-9e48-4a1c-8eba-eba8cda49949.png)'
- en: 'The fully connected network has no concept of space or local relationships;
    in this case, the model would need to learn all variants of each digit at each
    possible location. To further emphasize the importance of being able to capture
    the relationship of spatial data, consider the need to learn more complex images,
    such as classifying dogs and cats using a network that discards 2D information.
    Individual pixels alone are unable to portray complex shapes such as eyes, a nose,
    or ears; it''s only when you consider neighboring pixels that you can describe
    these more complex shapes:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 全连接网络没有空间或局部关系的概念；在这种情况下，模型需要学习每个数字在每个可能位置的所有变体。为了进一步强调能够捕捉空间数据关系的重要性，考虑需要学习更复杂的图像，例如使用丢弃2D信息的网络来对猫和狗进行分类。单独的像素无法描绘出眼睛、鼻子或耳朵等复杂形状；只有当你考虑相邻像素时，你才能描述这些更复杂的形状：
- en: '![](img/bb9e8cad-f0c3-4b80-988d-ffaa5431a611.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/bb9e8cad-f0c3-4b80-988d-ffaa5431a611.png)'
- en: Images taken from the Kaggle competition cats vs dogs (https://www.kaggle.com/c/dogs-vs-cats)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图片来自Kaggle竞赛猫狗对比（https://www.kaggle.com/c/dogs-vs-cats）
- en: 'We need something that can abstract away from the raw pixels, something that
    can describe images using high-level features. Let''s return to our digits dataset
    and investigate how we might go about extracting higher-level features for the
    task of classification. As alluded to in an earlier example, we need a set of
    features that abstracts away from the raw pixels, is unaffected by position, and
    preserves 2D spatial information. If you''re familiar with image processing, or
    even image processing tools, you would have most probably come across the idea
    and results of **edge detection** or **edge filters**; in simplest terms, these
    work by passing a set of kernels across the whole image, where the output is the
    image with its edges emphasized. Let''s see how this looks diagrammatically. First,
    we have our set of kernels; each one extracts a specific feature of the image,
    such as the presence of horizontal edges, vertical edges, or edges at a 45 degree
    angle:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种能够从原始像素中抽象出来的东西，能够使用高级特征来描述图像。让我们回到我们的数字数据集，并研究我们如何可能提取用于分类任务的高级特征。正如在先前的例子中提到的，我们需要一组从原始像素中抽象出来的特征，不受位置影响，并保留2D空间信息。如果你熟悉图像处理，或者甚至图像处理工具，你很可能已经遇到过**边缘检测**或**边缘滤波器**的概念和结果；最简单的说法，这些是通过在整个图像上传递一组核来实现，输出是强调边缘的图像。让我们看看这图示上是如何的。首先，我们有我们的核集；每个核提取图像的特定特征，例如水平边缘、垂直边缘或45度角的边缘：
- en: '![](img/8861c305-175e-4535-ba4e-b42c9f240a96.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/8861c305-175e-4535-ba4e-b42c9f240a96.png)'
- en: 'For each of these filters, we pass them over our image, extracting each of
    the features; to help illustrate this, let''s take one digit and pass the vertical
    kernel over it:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些滤波器中的每一个，我们将它们应用到我们的图像上，提取每个特征；为了帮助说明这一点，让我们取一个数字并将垂直核应用到它上面：
- en: '![](img/c152549c-5150-4a50-a20d-f08a457765c8.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/c152549c-5150-4a50-a20d-f08a457765c8.png)'
- en: 'As illustrated in the previous diagram, we slide the horizontal kernel across
    the image, producing a new image using the values of the image and kernel. We
    continue until we have reached the bounds of the image, as shown in the following
    diagram:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如前图所示，我们将水平核在图像上滑动，使用图像和核的值生成一个新的图像。我们继续这样做，直到达到图像的边界，如下图所示：
- en: '![](img/e2d32e91-c97a-42ac-901c-9d35a04a902a.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/e2d32e91-c97a-42ac-901c-9d35a04a902a.png)'
- en: The output of this is a map showing the presence of vertical lines detected
    within the image. Using this and the other kernels, we can now describe each class
    by its dominant gradients rather than using pixel positions. This higher level
    abstraction allows us to recognize classes independent of their location as well
    as describe more complex objects.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这的输出是一个显示图像内检测到的垂直线的映射。使用这个和其他内核，我们现在可以用每个类的主导梯度来描述每个类，而不是使用像素位置。这个更高层次的抽象使我们能够独立于位置识别类别，以及描述更复杂的对象。
- en: Two useful things to be aware of when dealing with kernels are the **stride**
    **value** and **padding**. Strides determines how large your step size is when
    sliding your kernel across the image. In the preceding example, our stride is
    set to 1; that is, we're sliding only by a single value. Padding refers to how
    you deal with the boundaries; here, we are using **valid**, where we only process
    pixels within valid ranges. **same **would mean adding a border around the image
    to ensure that the output remains the same size as the input.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理内核时，有两个需要注意的有用事项是**步长**值和**填充**。步长决定了你在滑动内核跨过图像时步长的大小。在上一个例子中，我们的步长设置为1；也就是说，我们只滑动一个值。填充指的是你如何处理边界；在这里，我们使用**valid**，这意味着我们只处理有效范围内的像素。**same**则意味着在图像周围添加一个边界，以确保输出大小与输入相同。
- en: 'What we have performed here is known as **feature engineering** and something
    neural networks perform automatically; in particular, this is what CNNs do. They
    create a series of kernels (or convolution matrices) that are used to convolve
    the image to extract local features from neighboring pixels. Unlike our previous
    engineered example, these kernels are learned during training. Because they are
    learned automatically, we can afford to create many filters that can extract granular nuances
    of the image as well, allowing us to effectively stack convolution layers on top
    of each other. This allows for increasingly higher levels of abstraction to learn. For
    example, your first layer may learn to detect simple edges, and your second layer
    (operating on the previous extracted features) may learn to extract simple shapes.
    The deeper we go, the higher the level achieved by our features, as illustrated
    in the diagram:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里所做的是所谓的**特征工程**，这是神经网络自动执行的事情；特别是，这正是CNN所做的事情。它们创建一系列内核（或卷积矩阵），用于通过卷积图像从相邻像素中提取局部特征。与我们的先前工程示例不同，这些内核是在训练期间学习的。因为它们是自动学习的，我们可以创建许多过滤器，这些过滤器可以提取图像的细微差别，从而允许我们有效地堆叠卷积层。这允许我们学习更高层次的抽象。例如，第一层可能学会检测简单的边缘，而第二层（在先前提取的特征上操作）可能学会提取简单的形状。我们走得越深，我们的特征达到的层次就越高，如图所示：
- en: '![](img/89d3eafc-e01f-4b41-91fe-cc6d32977f97.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/89d3eafc-e01f-4b41-91fe-cc6d32977f97.png)'
- en: And there we have it! An architecture capable of understanding the world by
    learning features and layers of abstraction to efficiently describe it. Let's
    now put this into practice using a pretrained model and Core ML to get our phone
    to recognize the objects it sees.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！一个能够通过学习特征和抽象层来高效描述世界的架构。现在，让我们通过使用预训练模型和Core ML来实践，让我们的手机能够识别它所看到的物体。
- en: Recognizing objects in the world
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 识别世界中的物体
- en: To recap, our goal in this chapter is to create an application that will recognize what
    it sees. We will start by first capturing video frames, prepare these frames for
    our model, and finally feed them into a Core ML model to perform inference. Let's
    get started.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾，我们本章的目标是创建一个能够识别它所看到的应用程序。我们将首先捕获视频帧，为我们的模型准备这些帧，最后将它们输入到Core ML模型中进行推理。让我们开始吧。
- en: Capturing data
  id: totrans-35
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据采集
- en: 'If you haven''t done it already, download the latest code from the accompanying
    repository: [https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml).
    Once downloaded, navigate to the directory `Chapter3/Start/ObjectRecognition/`
    and open the project `ObjectRecognition.xcodeproj`. Once loaded, you will see
    the skeleton project for this chapter, as shown in the following screenshot:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有这样做，请从配套仓库下载最新代码：[https://github.com/packtpublishing/machine-learning-with-core-ml](https://github.com/packtpublishing/machine-learning-with-core-ml)。下载后，导航到目录`Chapter3/Start/ObjectRecognition/`并打开项目`ObjectRecognition.xcodeproj`。加载后，你将看到本章的骨架项目，如下面的截图所示：
- en: '![](img/f875374a-88c0-44b2-a401-c61345016010.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f875374a-88c0-44b2-a401-c61345016010.png)'
- en: 'To help you navigate around the project, here is a list of core files/classes
    and their main functions:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您在项目中导航，以下是一个核心文件/类及其主要功能的列表：
- en: '`VideoCapture` will be responsible for the management and handling of the camera,
    including capturing video frames'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`VideoCapture` 将负责管理和处理摄像头，包括捕获视频帧'
- en: '`CaptureVideoPreviewView.swift` contains the class `CapturePreviewView`, which
    will be used to present the captured frames'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CaptureVideoPreviewView.swift` 包含 `CapturePreviewView` 类，它将被用来展示捕获的帧'
- en: '`CIImage` provides convenient extensions to the class `CIImage`, used for preparing
    the frame for the Core ML model'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CIImage` 为 `CIImage` 类提供了便利的扩展，用于准备帧以供 Core ML 模型使用'
- en: '`VideoController`, as you would expect, is the controller for the application
    and is responsible for interfacing with the imported Core ML model'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如您所期望的，`VideoController` 是应用程序的控制器，负责与导入的 Core ML 模型进行接口交互
- en: We will be making changes to each of these in the following sections in order
    to realize the desired functionality. Our first task will be to get access to
    the camera and start capturing frames; to do this, we will be making use of Apple's
    iOS frameworks **AVFoundation** and **CoreVideo**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中对这些进行修改，以实现所需的功能。我们的第一个任务将是获取对摄像头的访问权限并开始捕获帧；为此，我们将利用 Apple 的 iOS
    框架 **AVFoundation** 和 **CoreVideo**。
- en: The AVFoundation framework encompasses classes for handing capturing, processing,
    synthesizing, controlling, importing, and exporting of audiovisual media on iOS
    and other platforms. In this chapter, we are most interested in a subset of this
    framework for dealing with cameras and media capture, but you can learn more about
    the AVFoundation framework on Apple's official documentation site at [https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: AVFoundation 框架包含用于在 iOS 和其他平台上处理捕获、处理、合成、控制、导入和导出音视频媒体类的集合。在本章中，我们最感兴趣的是该框架的一个子集，用于处理摄像头和媒体捕获，但您可以在
    Apple 的官方文档网站上了解更多关于 AVFoundation 框架的信息：[https://developer.apple.com/documentation/avfoundation](https://developer.apple.com/documentation/avfoundation)。
- en: CoreVideo provides a pipeline-based API for manipulating digital videos, capable
    of accelerating the process using support from both Metal and OpenGL.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CoreVideo 提供了一个基于管道的 API，用于操作数字视频，能够通过 Metal 和 OpenGL 的支持来加速处理过程。
- en: We will designate the responsibility of setting up and capturing frames from
    the camera to the class `VideoCapture`; let's jump into the code now. Select `VideoCapture.swift`
    from the left-hand side panel to open in the editing window. Before making amendments,
    let's inspect what is already there and what's left to do.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将指定将设置和从摄像头捕获帧的责任分配给 `VideoCapture` 类；现在让我们直接进入代码。从左侧面板中选择 `VideoCapture.swift`
    以在编辑窗口中打开。在做出修改之前，让我们检查已经存在的内容以及还需要完成的工作。
- en: 'At the top of the class, we have the protocol `VideoCaptureDelegate` defined:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在类的顶部，我们定义了 `VideoCaptureDelegate` 协议：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`VideoCapture` will pass through the captured frames to a registered delegate,
    thus allowing the `VideoCapture` class to focus solely on the task of capturing
    the frames. What we pass to the delegate is a reference to itself, the image data
    (captured frame) of type `CVPixelBuffer` and the timestamp as type `CMTime`. `CVPixelBuffer`
    is a CoreVideo data structure specifically for holding pixel data, and the data
    structure our Core ML model is expecting (which we''ll see in a short while).
    `CMTTime` is just a struct for encapsulating a timestamp, which we''ll obtain
    directly from the video frame.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`VideoCapture` 将将捕获的帧传递给已注册的代理，从而使 `VideoCapture` 类能够专注于捕获帧的任务。我们传递给代理的是对自身的引用、类型为
    `CVPixelBuffer` 的图像数据（捕获帧）以及类型为 `CMTime` 的时间戳。`CVPixelBuffer` 是 CoreVideo 专门用于存储像素数据的数据结构，也是我们的
    Core ML 模型所期望的数据结构（我们将在稍后看到）。`CMTime` 是一个用于封装时间戳的结构体，我们将直接从视频帧中获取它。'
- en: 'Under the protocol, we have the skeleton of our `VideoCapture` class. We will
    be walking through it in this section, along with an extension to implement the
    `AVCaptureVideoDataOutputSampleBufferDelegate` protocol, which we will use to
    capture frames:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在协议下，我们有 `VideoCapture` 类的骨架。在本节中，我们将逐步介绍它，以及一个扩展来实现 `AVCaptureVideoDataOutputSampleBufferDelegate`
    协议，我们将使用它来捕获帧：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Most of this should be self-explanatory, so I will only highlight the not-so-obvious
    parts, starting with the variables `fps` and `lastTimestamp`. We use these together
    to throttle how quickly we pass frames back to the delegate; we do this as it's
    our assumption that we capture frames far quicker than we can process them. And
    to avoid having our camera lag or jump, we explicitly limit how quickly we pass
    frames to the delegate. **Frames per second** (**fps**) sets this frequency while
    `lastTimestamp` is used in conjunction to calculate the elapsed time since the
    last processing of a frame.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分内容应该是显而易见的，所以我只会强调不那么明显的地方，从变量`fps`和`lastTimestamp`开始。我们使用这些变量一起控制我们向代理传递帧的速率；我们这样做是因为我们假设我们捕获帧的速度远快于我们处理它们的速度。为了避免我们的摄像头出现延迟或跳跃，我们明确限制向代理传递帧的速率。**每秒帧数**（**fps**）设置这个频率，而`lastTimestamp`则与计算自上次处理帧以来经过的时间一起使用。
- en: The only other part of the code I will highlight here is the `asyncStartCapturing`
    and `asyncStopCapturing` methods; these methods, as the names imply, are responsible
    for starting and stopping the capture session respectively. Because they both
    will be using blocking methods, which can take some time, we will dispatch the
    task off the main thread to avoid blocking it and affecting the user's experience.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里要强调的代码的另一部分是`asyncStartCapturing`和`asyncStopCapturing`方法；正如其名称所暗示的，这些方法分别负责启动和停止捕获会话。因为它们都将使用阻塞方法，这可能会花费一些时间，所以我们将任务从主线程派发出去，以避免阻塞它并影响用户体验。
- en: 'Finally, we have the extension; it implements the `AVCaptureVideoDataOutputSampleBufferDelegate`
    protocol:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们有扩展；它实现了`AVCaptureVideoDataOutputSampleBufferDelegate`协议：
- en: '[PRE2]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We will discuss the details shortly, but essentially it is the delegate that
    we assign to the camera for handling incoming frames of the camera. We will then
    proxy it through to the `VideoCaptureDelegate` delegate assigned to this class.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在稍后讨论细节，但基本上是我们分配给摄像头以处理摄像头传入帧的代理。然后我们将它代理到分配给这个类的`VideoCaptureDelegate`代理。
- en: Let's now walk through implementing the methods of this class, starting with
    `initCamera`. In this method, we want to set up the pipeline that will grab the
    frames from the physical camera of the device and pass them onto our delegate
    method. We do this by first getting a reference to the physical camera and then
    wrapping it in an instance of the `AVCaptureDeviceInput` class, which takes care
    of managing the connection and communication with the physical camera. Finally,
    we add a destination for the frames, which is where we use an instance of `AVCaptureVideoDataOutput`,
    assigning ourselves as the delegate for receiving these frames. This pipeline
    is wrapped in something called `AVCaptureSession`, which is responsible for coordinating
    and managing this pipeline.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来逐步实现这个类的各个方法，从`initCamera`方法开始。在这个方法中，我们想要设置一个管道，该管道将从设备的物理摄像头捕获帧并将它们传递到我们的代理方法。我们通过首先获取物理摄像头的引用，然后将其包装在`AVCaptureDeviceInput`类的一个实例中来实现这一点，该类负责管理与物理摄像头的连接和通信。最后，我们为帧添加一个目的地，这是我们在其中使用`AVCaptureVideoDataOutput`类的一个实例的地方，我们将自己指定为接收这些帧的代理。这个管道被包裹在称为`AVCaptureSession`的东西中，它负责协调和管理这个管道。
- en: 'Let''s now define some instance variables we''ll need; inside the class `VideoCapture`,
    add the following variables:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们定义一些我们将需要的实例变量；在类`VideoCapture`内部添加以下变量：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: We mentioned the purpose of `captureSession` previously, but also introduced
    a `DispatchQueue`. When adding a delegate to `AVCaptureVideoDataOutput` (for handling
    the arrival of new frames), you also pass in a `DispatchQueue`; this allows you
    to control which queue the frames are managed on. For our example, we will be
    handling the processing of the images off the main thread so as to avoid impacting
    the performance of the user interface.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到了`captureSession`的目的，但也介绍了一个`DispatchQueue`。当将代理添加到`AVCaptureVideoDataOutput`（用于处理新帧的到来）时，你也会传递一个`DispatchQueue`；这允许你控制帧在哪个队列上被管理。在我们的例子中，我们将处理图像的操作从主线程上移除，以避免影响用户界面的性能。
- en: 'With our instance variables now declared, we will turn our attention to the
    `initCamera` method, breaking it down into small snippets of code. Add the following
    within the body of the method:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经声明了实例变量，我们将把注意力转向`initCamera`方法，将其分解成小的代码片段。在方法体内添加以下内容：
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We signal to the `captureSession` that we want to batch multiple configurations
    by calling the method `beginConfiguration`; these changes won''t be made until
    we commit them by calling the session''s `commitConfiguration` method. Then, in
    the next line of code, we set the desired quality level:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过调用方法 `beginConfiguration` 向 `captureSession` 信号我们想要批量配置多个配置；这些更改不会在我们通过调用会话的
    `commitConfiguration` 方法提交之前生效。然后，在下一行代码中，我们设置所需的品质级别：
- en: '[PRE5]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In the next snippet, we obtain the physical device; here, we are obtaining the
    default device capable of recording video, but you can just as easily search for
    one with specific capabilities, such as the front camera. After successfully obtaining
    the device, we wrap it in an instance of `AVCaptureDeviceInput` that will be responsible
    for capturing data from the physical camera and finally adding it to the session.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码片段中，我们获取物理设备；在这里，我们获取默认的能够录制视频的设备，但您也可以轻松地搜索具有特定功能（如前置摄像头）的设备。在成功获取设备后，我们将它包装在一个
    `AVCaptureDeviceInput` 的实例中，该实例将负责从物理摄像头捕获数据并将其最终添加到会话中。
- en: 'We now have to add the destination for these frames; again, add the following
    snippet to the `initCamera` method where you left off:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须添加这些帧的目的地；再次，将以下片段添加到 `initCamera` 方法中，您之前停止的地方：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: In the previous code snippet, we create, set up, and added our output. We start
    by instantiating an instance of `AVCaptureVideoDataOutput`, before defining what
    data we want. Here, we are requesting full color (`kCVPixelFormatType_32BGRA`),
    but depending on your model, it may be more efficient to request images in grayscale
    (`kCVPixelFormatType_8IndexedGray_WhiteIsZero`).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的代码片段中，我们创建、设置并添加了我们的输出。我们首先实例化 `AVCaptureVideoDataOutput` 的一个实例，然后定义我们想要的数据。在这里，我们请求全彩（`kCVPixelFormatType_32BGRA`），但根据您的模型，请求灰度图像（`kCVPixelFormatType_8IndexedGray_WhiteIsZero`）可能更有效率。
- en: 'Setting `alwaysDiscardsLateVideoFrames` to true means any frames that arrive
    while the dispatch queue is busy will be discarded—a desirable feature for our
    example. We then assign ourselves along with our dedicated dispatch queue as the
    delegate for handing incoming frames using the method `videoOutput.setSampleBufferDelegate(self,
    queue: sessionQueue)`. Once we have configured our output, we are ready to add
    it to our session as part of our configuration request. To prevent our images
    from being rotated by 90 degrees, we then request that our images are in portrait
    orientation.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '将 `alwaysDiscardsLateVideoFrames` 设置为 true 意味着在调度队列忙碌时到达的任何帧都将被丢弃——这对于我们的示例来说是一个理想的功能。然后，我们使用方法
    `videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)` 将我们自己以及我们的专用调度队列指定为处理传入帧的代理。一旦我们配置了输出，我们就可以将其作为配置请求的一部分添加到会话中。为了防止我们的图像被旋转
    90 度，我们随后请求图像以竖直方向显示。'
- en: 'Add the final statement to commit these configurations; it''s only after we
    do this that these changes will take effect:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 添加最终语句以提交这些配置；只有在我们这样做之后，这些更改才会生效：
- en: '[PRE7]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This now completes our `initCamera` method; let''s swiftly (excuse the pun)
    move onto the methods responsible for starting and stopping this session. Add
    the following code to the body of the `asyncStartCapturing` method:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这现在完成了我们的 `initCamera` 方法；让我们迅速（请原谅这个双关语）转到负责启动和停止此会话的方法。将以下代码添加到 `asyncStartCapturing`
    方法的主体中：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As mentioned previously, the `startRunning` and `stopRunning` methods both block
    the main thread and can take some time to complete; for this reason, we execute
    them off the main thread, again to avoid affecting the responsiveness of the user
    interface. Invoking `startRunning` will start the flow of data from the subscribed
    inputs (camera) to the subscribed outputs (delegate).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`startRunning` 和 `stopRunning` 方法都会阻塞主线程，并且可能需要一些时间来完成；因此，我们将它们在主线程之外执行，再次避免影响用户界面的响应性。调用
    `startRunning` 将启动从订阅的输入（摄像头）到订阅的输出（代理）的数据流。
- en: Errors, if any, are reported through the notification `AVCaptureSessionRuntimeError`.
    You can subscribe to listen to it using the default `NotificationCenter`. Similarly,
    you can subscribe to listen when the session starts and stops with the notifications
    `AVCaptureSessionDidStartRunning` and `AVCaptureSessionDidStopRunning`, respectively.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有任何错误，将通过通知 `AVCaptureSessionRuntimeError` 报告。您可以使用默认的 `NotificationCenter`
    订阅以监听它。同样，您可以通过通知 `AVCaptureSessionDidStartRunning` 和 `AVCaptureSessionDidStopRunning`
    分别订阅以监听会话开始和停止。
- en: 'Similarly, add the following code to the method `asyncStopCapturing`, which
    will be responsible for stopping the current session:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，将以下代码添加到 `asyncStopCapturing` 方法中，该方法将负责停止当前会话：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Within the `initCamera` method, we subscribed ourselves as the delegate to
    handle arriving frames using the statement `videoOutput.setSampleBufferDelegate(self,
    queue: sessionQueue)`; let''s now turn our attention to handling this. As you
    may recall, we included an extension of the `VideoCapture` class to implement
    the `AVCaptureVideoDataOutputSampleBufferDelegate` protocol within the `captureOutput`
    method. Add the following code:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '在`initCamera`方法中，我们使用语句`videoOutput.setSampleBufferDelegate(self, queue: sessionQueue)`将自己注册为代理以处理到达的帧；现在让我们关注如何处理它。如您所回忆的那样，我们在`captureOutput`方法中为`VideoCapture`类添加了一个扩展来实现`AVCaptureVideoDataOutputSampleBufferDelegate`协议。添加以下代码：'
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Before walking through this code snippet, it's worth mentioning what parameters
    this method is passed and how we use them. The first parameter, `output`, is of the
    type `AVCaptureVideoDataOutput` and references the associated output that this
    frame originated from. The next parameter, `sampleBuffer`, is of the type `CMSampleBuffer` and
    this is what we will use to access data of the current frame. Along with the frames,
    the duration, format, and timestamp associated with each frame can also be obtained.
    The final parameter, `connection`, is of the type `AVCaptureConnection` and provides
    a reference to the connection associated with the received frame.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在浏览这段代码片段之前，值得提一下这个方法接收了哪些参数以及我们如何使用它们。第一个参数，`output`，其类型为`AVCaptureVideoDataOutput`，它引用了与此帧相关的输出。下一个参数，`sampleBuffer`，其类型为`CMSampleBuffer`，这是我们用来访问当前帧数据的。与帧一起，每个帧关联的持续时间、格式和时间戳也可以获得。最后一个参数，`connection`，其类型为`AVCaptureConnection`，它提供了与接收到的帧相关联的连接的引用。
- en: Now, walking through the code, we start by guarding against any occurrences
    where no delegate is assigned, and returning early if so. Then we determine whether
    enough time has elapsed since the last time we processed a frame, remembering
    that we are throttling how frequently we process a frame to ensure a seamless
    experience. Here, instead of using the systems clock, we obtain the time associated
    with the latest frame via the statement `let timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)`;
    this ensures that we are measuring against the relative time with respect to the
    frame rather than absolute time of the system. Given that enough time has passed,
    we proceed to get a reference to the sample's image buffer via the statement `CMSampleBufferGetImageBuffer(sampleBuffer)`,
    finally passing it over to the assigned delegate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们逐行分析代码，我们首先检查是否有未分配的代理，并在必要时提前返回。然后我们确定自上次处理帧以来是否已经过去了足够的时间，记住我们正在限制处理帧的频率以确保流畅的体验。在这里，我们不是使用系统时钟，而是通过语句`let
    timestamp = CMSampleBufferGetPresentationTimeStamp(sampleBuffer)`获取与最新帧相关的时间；这确保了我们是在相对于帧的相对时间而不是系统的绝对时间进行测量。鉴于已经过去了足够的时间，我们继续通过语句`CMSampleBufferGetImageBuffer(sampleBuffer)`获取样本的图像缓冲区引用，最后将其传递给指定的代理。
- en: 'This now completes our `VideoCapture` class; let''s move on to hooking it up
    to our view using the `ViewController`. But before jumping into the code, let''s
    inspect the interface via the storyboard to better understand where we''ll be
    presenting the video stream. Within Xcode, select `Main.storyboard` from the Project
    Navigator panel on the left to open up interface builder; when opened, you will
    be presented with a layout similar to the following screenshot:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在已经完成了`VideoCapture`类的编写；让我们继续使用`ViewController`将其连接到我们的视图。但在跳入代码之前，让我们通过故事板检查接口，以便更好地理解我们将在哪里展示视频流。在Xcode中，从左侧的项目导航器面板中选择`Main.storyboard`以打开界面构建器；打开后，您将看到一个类似于以下截图的布局：
- en: '![](img/f8c13e02-eba6-4a0c-8ea0-92e3cf37c079.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f8c13e02-eba6-4a0c-8ea0-92e3cf37c079.png)'
- en: 'Nothing complicated; we have a label to present our results and a view to render
    our video frames onto. If you select the VideoPreview view and inspect the class
    assigned to it, you will see we have a custom class to handle the rendering called,
    appropriately, CapturePreviewView. Let''s jump into the code for this class and
    make the necessary changes:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么复杂的；我们有一个标签来显示我们的结果，还有一个视图来渲染我们的视频帧。如果您选择VideoPreview视图并检查分配给它的类，您将看到我们有一个名为CapturePreviewView的自定义类来处理渲染。让我们跳入这个类的代码并进行必要的修改：
- en: '[PRE11]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Fortunately, `AVFoundation` makes available a subclass of `CALayer` specifically
    for rendering frames from the camera; all that remains for us to do is to override
    the view''s `layerClass` property and return the appropriate class. Add the following
    code to the `CapturePreviewView` class:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，`AVFoundation` 提供了一个专门用于从摄像头渲染帧的 `CALayer` 子类；我们剩下的工作就是重写视图的 `layerClass`
    属性并返回适当的类。将以下代码添加到 `CapturePreviewView` 类中：
- en: '[PRE12]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'This method is called early during the creation of the view and is used to
    determine what `CALayer` to instantiate and associate with this view. As previously
    mentioned, the `AVCaptureVideoPreviewLayer` is—as the name suggests—specifically
    for handling video frames. In order to get the frames rendered, we simply assign
    `AVCaptureSession` with the `AVCaptureVideoPreviewLayer.session` property. Let''s
    do that now; first open up the `ViewController` class in Xcode and add the following
    variable (in bold):'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在视图创建的早期阶段就被称作，用于确定要实例化哪个 `CALayer` 并将其与该视图关联。正如之前提到的，`AVCaptureVideoPreviewLayer`
    ——正如其名称所暗示的那样——专门用于处理视频帧。为了获取渲染的帧，我们只需将 `AVCaptureSession` 赋值为 `AVCaptureVideoPreviewLayer.session`
    属性。现在让我们来做这件事；首先在 Xcode 中打开 `ViewController` 类，并添加以下变量（粗体）：
- en: '[PRE13]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `previewView` and `classifiedLabel` are existing variables associated with
    the interface via the Interface Builder. Here, we are creating an instance of
    `VideoCapture`, which we had implemented earlier. Next, we will set up and start
    the camera using the `VideoCapture` instance, before assigning the session to
    our `previewView` layer. Add the following code within the `ViewDidLoad` method
    under the statement `super.viewDidLoad()`:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`previewView` 和 `classifiedLabel` 是通过 Interface Builder 与界面关联的现有变量。在这里，我们创建了一个
    `VideoCapture` 的实例，这是我们之前实现的。接下来，我们将使用 `VideoCapture` 实例设置并启动摄像头，然后将会话分配给我们的 `previewView`
    层。在 `ViewDidLoad` 方法下的 `super.viewDidLoad()` 语句中添加以下代码：'
- en: '[PRE14]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Most of the code should look familiar to you as a lot of it is using the methods
    we have just implemented. First we initialize the camera, calling the `initCamera`
    method of the `VideoCamera` class. Then, if successful, we assign the created
    `AVCaptureSession` to the layer's session. We also hint to the layer how we want
    it to handle the content, in this case filling the screen whilst respecting its
    aspect ratio. Finally, we start the camera by calling `videoCapture.asyncStartCapturing()`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 大部分的代码应该对你来说都很熟悉，因为其中很多都是使用我们刚刚实现的方法。首先我们初始化摄像头，调用 `VideoCamera` 类的 `initCamera`
    方法。然后，如果成功，我们将创建的 `AVCaptureSession` 分配给层的会话。我们还向层暗示了我们希望它如何处理内容，在这种情况下是填充屏幕同时尊重其宽高比。最后，我们通过调用
    `videoCapture.asyncStartCapturing()` 启动摄像头。
- en: With that now completed, it's a good time to test that everything is working
    correctly. If you build and deploy on an iOS 11+ device, you should see the video
    frames being rendered on your phone's screen.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切完成，是时候测试一切是否正常工作了。如果你在 iOS 11+ 设备上构建和部署，你应该能在手机屏幕上看到渲染的视频帧。
- en: In the next section, we will walk through how to capture and process them for
    our model before performing inference (recognition).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将介绍如何在执行推理（识别）之前，如何捕获和处理这些帧以供我们的模型使用。
- en: Preprocessing the data
  id: totrans-95
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据预处理
- en: 'At this stage, we have the app rendering the frames from the camera, but we
    are not yet receiving any frames. To do this, we will assign ourselves to receive
    these frames, as implemented in the previous section. The existing `ViewController`
    class already has an extension implementing the `VideoCaptureDelegate` protocol.
    What''s left to do is to assign ourselves as the delegate of the `VideoCapture`
    instance and implement the details of the callback method; the following is the
    code for `extension`:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们的应用正在渲染来自摄像头的帧，但我们还没有接收到任何帧。为了做到这一点，我们将自己设置为接收这些帧，正如前一个章节中实现的那样。现有的
    `ViewController` 类已经有一个扩展实现了 `VideoCaptureDelegate` 协议。剩下要做的就是将我们自己设置为 `VideoCapture`
    实例的代理并实现回调方法的细节；以下是为 `extension` 编写的代码：
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Depending on your coding style, you can just as easily implement the protocols
    inside the main class. I tend to make use of extensions to implement the protocols—a
    personal preference.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的编码风格，你同样可以在主类内部实现协议。我倾向于使用扩展来实现协议——这是一个个人偏好。
- en: 'First, let''s assign ourselves as the delegate to start receiving the frames;
    within the `ViewDidLoad` method of the `ViewController` class, we add the following
    statement just before we initialize the camera:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将自己设置为代理以开始接收帧；在 `ViewController` 类的 `ViewDidLoad` 方法中，在我们初始化摄像头之前添加以下语句：
- en: '[PRE16]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now that we have assigned ourselves as the delegate, we will receive frames
    (at the defined frame rate) via the callback:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将自己指定为代理，我们将通过回调接收帧（在定义的帧率下）：
- en: '[PRE17]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It's within this method that we will prepare and feed the data to the model
    to classify the dominant object within the frame. What the model is expecting
    is dependent on the model, so to get a better idea of what we need to pass it,
    let's download the trained model we will be using for this example and import
    it into our project.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方法中，我们将准备并喂给模型数据以对帧内的主要对象进行分类。模型期望的内容取决于模型本身，因此为了更好地了解我们需要传递什么，让我们下载我们将用于此示例的训练好的模型并将其导入到我们的项目中。
- en: 'Trained models can be obtained from a variety of sources; in some instances,
    you will need to convert them, and in other cases, you will need to train the
    model yourself. But in this instance, we can make use of the models Apple has
    made available; open up your web browser and navigate to [https://developer.apple.com/machine-learning/](https://developer.apple.com/machine-learning/):'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的模型可以从各种来源获得；在某些情况下，您可能需要将它们进行转换，而在其他情况下，您可能需要自己训练模型。但在这个例子中，我们可以利用苹果公司提供的模型；打开您的网络浏览器，导航到 [https://developer.apple.com/machine-learning/](https://developer.apple.com/machine-learning/)：
- en: '![](img/b03ed353-ba5c-4d37-aa90-c54468fc8831.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](img/b03ed353-ba5c-4d37-aa90-c54468fc8831.png)'
- en: You will be taken to a web page where Apple has made available a range of pretrained
    and converted models. Conveniently, most of the available models are specifically
    for object classification; given our use case, we're particularly interested in
    the models trained on a large array of objects. Our options include MobileNet, SqueezeNet, ResNet50, Inception
    v3, and VGG16\. Most of these have been trained on the ImageNet dataset, a dataset
    with reference to over 10 million URLs' images that have been manually assigned
    to one of 1,000 classes. References to the original research papers and performance
    can be obtained via the View original model details link. For this example, we'll
    use Inception v3, a good balance between size and accuracy.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您将被带到苹果公司提供了一系列预训练和转换模型的网页。方便的是，大多数可用的模型都是专门用于对象分类的；鉴于我们的用例，我们特别感兴趣的是在大量对象上训练的模型。我们的选项包括
    MobileNet、SqueezeNet、ResNet50、Inception v3 和 VGG16。其中大多数都是在 ImageNet 数据集上训练的，这是一个包含超过
    1000 万个图像的参考数据集，这些图像已被手动分配到 1000 个类别之一。可以通过“查看原始模型详细信息”链接获取对原始研究论文和性能的引用。对于这个例子，我们将使用
    Inception v3，它在大小和准确性之间取得了良好的平衡。
- en: Here, we are using the Inception v3 model, but the effort to swap the model
    is minimal; it requires updating the references as the generated classes are prefixed
    with the model's name, as you will soon see, and ensuring that you are conforming
    to the expected inputs of the model (which can be alleviated by using the Vision
    framework, as you will see in future chapters).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用的是 Inception v3 模型，但更换模型的工作量很小；它需要更新引用，因为生成的类名都带有模型名称的前缀，正如您很快就会看到的，并确保您符合模型预期的输入（这可以通过使用
    Vision 框架来缓解，您将在未来的章节中看到）。
- en: 'Click on the Download Core ML Model link to proceed to download and, once downloaded,
    drag the `Inceptionv3.mlmodel` file onto the Project Navigator panel on the left
    of Xcode, checking Copy items if needed if desired or else leaving everything
    as default. Select the `Inceptionv3.mlmodel` file from the Project Navigator panel
    on the left to bring up the details within the Editor area, as shown in the following
    screenshot:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“下载 Core ML 模型”链接以继续下载，下载完成后，将 `Inceptionv3.mlmodel` 文件拖放到 Xcode 左侧的“项目导航器”面板上，如果需要则检查“复制项目”，否则保持默认设置。从左侧的项目导航器面板中选择
    `Inceptionv3.mlmodel` 文件，以便在编辑器区域中显示详细信息，如下面的截图所示：
- en: '![](img/d7672f9b-67a1-4755-befe-e69d230b3937.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d7672f9b-67a1-4755-befe-e69d230b3937.png)'
- en: It is important to ensure that the model is correctly assigned to the appropriate
    target; in this example, this means verifying that the ObjectRecognition target
    is checked, as seen here on the Utilities panel to the right. Also worth noting
    are the expected inputs and outputs of the model. Here, the model is expecting
    a color image of size 299 x 299 for its input, and it returns a single class label
    as a string and a dictionary of string-double pairs of probabilities of all the
    classes.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 确保模型被正确分配到适当的目标是重要的；在这个例子中，这意味着验证 ObjectRecognition 目标是否被选中，如右边的工具面板上所见。还值得注意的是模型的预期输入和输出。在这里，模型期望一个
    299 x 299 大小的彩色图像作为输入，并返回一个字符串形式的单个类标签以及所有类别的字符串-双精度浮点数概率字典。
- en: 'When a `.mlmodel` file is imported, Xcode will generate a wrapper for the model
    itself and the input and output parameters to interface with the model; this is
    illustrated here:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 当导入 `.mlmodel` 文件时，Xcode 将为模型本身以及输入和输出参数生成包装器，以便与模型接口；这在此处得到了说明：
- en: '![](img/db3c0a61-38ff-4a49-9f2c-66eb98f65aa9.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/db3c0a61-38ff-4a49-9f2c-66eb98f65aa9.png)'
- en: 'You can easily access this by tapping on the arrow button next to the `Inceptionv3`
    label within the Model Class section; when tapped, you will see the following
    code (separated into three distinct blocks to make it more legible):'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过在模型类部分旁边的箭头按钮上轻触来轻松访问它；轻触后，你会看到以下代码（分为三个不同的块以提高可读性）：
- en: '[PRE18]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The first block of the preceding code is the input for our model. This class
    implements the `MLFeatureProvider` protocol, a protocol representing a collection
    of feature values for the model, in this case, the image feature. Here, you can
    see the expected data structure, `CVPixelBuffer`, along with the specifics declared
    (handily) in the comments. Let''s continue on with our inspection of the generated
    classes by looking at the binding for the output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的第一个块是我们模型的输入。这个类实现了 `MLFeatureProvider` 协议，该协议代表模型的一组特征值，在这种情况下，是图像特征。在这里，你可以看到预期的数据结构，`CVPixelBuffer`，以及注释中声明的具体细节。让我们继续检查生成的类，看看输出的绑定：
- en: '[PRE19]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'As previously mentioned, the output exposes a directory of probabilities and
    a string for the dominated class, each exposed as properties or accessible using
    the getter method `featureValue(for featureName: String)` by passing in the feature''s
    name. Our final extract for the generated code is the model itself; let''s inspect
    that now:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，输出公开了一个概率目录和一个表示主导类的字符串，每个都作为属性公开或可以通过传递特征名称的 getter 方法 `featureValue(for
    featureName: String)` 访问。我们最后提取生成的代码是模型本身；现在让我们检查一下：'
- en: '[PRE20]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This class wraps the model class and provides strongly typed methods for performing
    inference via the `prediction(input: Inceptionv3Input)` and `prediction(image:
    CVPixelBuffer)` methods, each returning the output class we saw previously—`Inceptionv3Output`.
    Now, knowing what our model is expecting, let''s continue to implement the preprocessing
    functionality required for the captured frames in order to feed them into the
    model.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '这个类封装了模型类，并为通过 `prediction(input: Inceptionv3Input)` 和 `prediction(image: CVPixelBuffer)`
    方法执行推理提供了强类型方法，每个方法都返回我们之前看到的输出类——`Inceptionv3Output`。现在，知道了我们的模型期望什么，让我们继续实现预处理功能，以便将捕获的帧输入到模型中。'
- en: 'Core ML 2 introduced a the ability to work with batches; if your model was
    compiled with Xcode 10+ then you will also see the additional method `<CODE>func
    predictions(from: MLBatchProvider, options: MLPredictionOptions)</CODE>` allowing
    you to perform inference on a batch of inputs.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 'Core ML 2 引入了一种处理批量的能力；如果你的模型是用 Xcode 10+ 编译的，你还将看到额外的 `<CODE>func predictions(from:
    MLBatchProvider, options: MLPredictionOptions)</CODE>` 方法，允许你对输入批次进行推理。'
- en: At this stage, we know that we are receiving the correct data type (`CVPixelBuffer`)
    and image format (explicitly defined in the settings when configuring the capture
    video output instance `kCVPixelFormatType_32BGRA`) from the camera. But we are
    receiving an image significantly larger than the expected size of 299 x 299\.
    Our next task will be to create some utility methods to perform resizing and cropping.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，我们知道我们正在接收正确的数据类型 (`CVPixelBuffer`) 和图像格式（在配置捕获视频输出实例时在设置中明确定义的 `kCVPixelFormatType_32BGRA`），来自相机。但我们接收到的图像比预期的
    299 x 299 大得多。我们的下一个任务将是创建一些实用方法来进行调整大小和裁剪。
- en: For this, we will be extending `CIImage` to wrap and process the pixel data
    we receive along with making use of `CIContext` to obtain the raw pixels again.
    If you're unfamiliar with the CoreImage framework, then it suffices to say that
    it is a framework dedicated to efficiently processing and analyzing images. `CIImage` can
    be considered the base data object of this framework that is often used in conjunction
    with other CoreImage classes such as `CIFilter`, `CIContext`, `CIVector`, and
    `CIColor`. Here, we are interested in `CIImage` as it provides convenient methods
    for manipulating images along with `CIContext` to extract the raw pixel data from
    `CIImage` (`CVPixelBuffer`).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们将扩展`CIImage`以包装和处理我们接收到的像素数据，并利用`CIContext`再次获取原始像素。如果您不熟悉CoreImage框架，那么可以说它是一个致力于高效处理和分析图像的框架。`CIImage`可以被认为是该框架的基数据对象，通常与`CIFilter`、`CIContext`、`CIVector`和`CIColor`等其他CoreImage类一起使用。在这里，我们关注`CIImage`，因为它提供了方便的方法来操作图像，以及与`CIContext`一起提取`CIImage`中的原始像素数据（`CVPixelBuffer`）。
- en: 'Back in Xcode, select the `CIImage.swift` file from the Project navigator to
    open it up in the Editor area. In this file, we have extended the `CIImage` class
    with a method responsible for rescaling and another for returning the raw pixels (`CVPixelBuffer`),
    a format required for our Core ML model:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在Xcode中，从项目导航器中选择`CIImage.swift`文件以在编辑器区域中打开它。在这个文件中，我们扩展了`CIImage`类，添加了一个负责缩放的方法以及一个返回原始像素（`CVPixelBuffer`）的方法，这是我们的Core
    ML模型所需的格式：
- en: '[PRE21]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let''s start by implementing the `resize` method; this method is passed in
    the desired size, which we''ll use to calculate the relative scale; then we''ll
    use this to scale the image uniformly. Add the following code snippet to the `resize`
    method, replacing the `fatalError("Not implemented")` statement:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实现`resize`方法开始；这个方法接收期望的大小，我们将使用它来计算相对比例；然后我们将使用这个比例来均匀缩放图像。将以下代码片段添加到`resize`方法中，替换`fatalError("Not
    implemented")`语句：
- en: '[PRE22]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Unless the image is a square, we are likely to have an overflow either vertically
    or horizontally. To handle this, we will simply center the image and crop it to
    the desired size; do this by appending the following code to the `resize` method
    (beneath the code written in the preceding snippet):'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 除非图像是正方形，否则我们可能会在垂直或水平方向上出现溢出。为了处理这个问题，我们将简单地居中图像并将其裁剪到期望的大小；通过在`resize`方法中添加以下代码来实现（在前面代码片段下方）：
- en: '[PRE23]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'We now have the functionality to rescale the image; our next piece of functionality
    is to obtain a `CVPixelBuffer` from the `CIImage`. Let''s do that by implementing
    the body of the `toPixelBuffer` method. Let''s first review the method''s signature
    and then briefly talk about the functionality required:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有了调整图像大小的功能；我们下一个功能是获取`CIImage`的`CVPixelBuffer`。让我们通过实现`toPixelBuffer`方法的主体来实现这一点。首先，让我们回顾一下方法签名，然后简要谈谈所需的功能：
- en: '[PRE24]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: This method is expecting a `CIContext` and flag indicating whether the image
    should be grayscale (single channel) or full color; `CIContext` will be used to
    render the image to a pixel buffer (our `CVPixelBuffer`). Let's now flesh out
    the implementation for `toPixelBuffer` piece by piece.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法期望一个`CIContext`和一个标志，指示图像应该是灰度（单通道）还是全色；`CIContext`将用于将图像渲染到像素缓冲区（我们的`CVPixelBuffer`）。现在让我们逐步完善`toPixelBuffer`的实现。
- en: The preprocessing required on the image (resizing, grayscaling, and normalization)
    is dependent on the Core ML model and the data it was trained on. You can get
    a sense of these parameters by inspecting the Core ML model in Xcode. If you recall,
    the expected input to our model is (image color 299 x 299); this tells us that
    the Core ML model is expecting the image to be color (three channels) and 299
    x 299 in size.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像上所需的预处理（调整大小、灰度化和归一化）取决于Core ML模型及其训练数据。您可以通过在Xcode中检查Core ML模型来了解这些参数。如果您还记得，我们模型期望的输入是（图像颜色299
    x 299）；这告诉我们Core ML模型期望图像是彩色（三个通道）且大小为299 x 299。
- en: 'We start by creating the pixel buffer we will be rendering our image to; add
    the following code snippet to the body of the `toPixelBuffer` method, replacing
    the `fatalError("Not implemented")` statement:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建我们将渲染图像的像素缓冲区；将以下代码片段添加到`toPixelBuffer`方法的主体中，替换`fatalError("Not implemented")`语句：
- en: '[PRE25]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We first create an array to hold the attributes defining the compatibility requirements
    for our pixel buffer; here, we specify that we want our pixel buffer to be compatible
    with `CGImage` types (`kCVPixelBufferCGImageCompatibilityKey`) and compatible
    with CoreGraphics bitmap contexts (`kCVPixelBufferCGBitmapContextCompatibilityKey`).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建一个数组来保存定义像素缓冲区兼容性要求的属性；在这里，我们指定我们希望我们的像素缓冲区与`CGImage`类型兼容（`kCVPixelBufferCGImageCompatibilityKey`）以及与CoreGraphics位图上下文兼容（`kCVPixelBufferCGBitmapContextCompatibilityKey`）。
- en: 'We then proceed to create a pixel buffer, passing in our compatibility attributes,
    the format (either grayscale or full color depending on the value of `gray`),
    width, height, and pointer to the variable. Next, we unwrap the nullable pixel
    buffer as well as ensure that the call was successful; if either of these is `false`,
    we return `NULL`. Otherwise, we''re ready to render our `CIImage` into the newly
    created pixel buffer. Append the following code to the `toPixelBuffer` method:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来创建一个像素缓冲区，传入我们的兼容性属性、格式（取决于`gray`的值，可以是灰度或全色）、宽度、高度以及变量的指针。然后解包可空的像素缓冲区，并确保调用成功；如果这些中的任何一个为`false`，则返回`NULL`。否则，我们就可以将`CIImage`渲染到新创建的像素缓冲区中。将以下代码追加到`toPixelBuffer`方法中：
- en: '[PRE26]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Before drawing, we lock the address of the pixel buffer via `CVPixelBufferLockBaseAddress`
    and then unlock once we've finished using the `CVPixelBufferUnlockBaseAddress`
    method. We are required to do this when accessing pixel data from the CPU, which
    we are doing here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在绘制之前，我们通过`CVPixelBufferLockBaseAddress`锁定像素缓冲区的地址，然后使用`CVPixelBufferUnlockBaseAddress`方法解锁。当我们从CPU访问像素数据时，我们必须这样做，我们在这里就是这样做的。
- en: Once locked, we simply use the `CIContext` to render the scaled image to the
    buffer, passing in the destination rectangle (in this case, the full size of the
    pixel buffer) and destination color space, which is full color or grayscale depending
    on the value of `gray` as mentioned previously. After unlocking the pixel buffer,
    as described earlier, we return our newly created pixel buffer.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦锁定，我们只需使用`CIContext`将缩放后的图像渲染到缓冲区，传入目标矩形（在这种情况下，像素缓冲区的完整大小）和目标颜色空间，这取决于之前提到的`gray`的值。在解锁像素缓冲区后，如前所述，我们返回我们新创建的像素缓冲区。
- en: 'We have now extended the `CIImage` with two convenient methods, one responsible
    for rescaling and the other for creating a pixel buffer representation of itself.
    We will now return to the `ViewController` class to handle the preprocessing steps
    required before passing our data into the model. Select the `ViewController.swift`
    file from the Projector navigator panel within Xcode to bring up the source code,
    and within the body of the `ViewController` class, add the following variable:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在扩展了`CIImage`，添加了两个方便的方法，一个负责缩放，另一个负责创建其自身的像素缓冲区表示。我们现在将返回到`ViewController`类，处理在将数据传递给模型之前所需的预处理步骤。在Xcode的项目导航面板中选择`ViewController.swift`文件以显示源代码，并在`ViewController`类的主体中添加以下变量：
- en: '[PRE27]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'As previously discussed, we will be passing this to our `CIImage.toPixelBuffer`
    method for rendering the image to the pixel buffer. Now return to the `onFrameCaptured`
    method and add the following code, to make use of the methods we''ve just created
    for preprocessing:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们将将其传递给`CIImage.toPixelBuffer`方法以将图像渲染到像素缓冲区。现在返回到`onFrameCaptured`方法，并添加以下代码，以使用我们刚刚创建的预处理方法：
- en: '[PRE28]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: We first unwrap the `pixelBuffer`, returning if it is `NULL`; then we create
    an instance of `CIImage`, passing in the current frame and then chaining our extension
    methods to perform rescaling (299 x 299) and rendering out to a pixel buffer (setting
    the gray parameter to false as the model is expecting full color images). If successful,
    we are returned a image ready to be passed to our model for inference, the focus
    of the next section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先解包`pixelBuffer`，如果它是`NULL`则返回；然后我们创建一个`CIImage`实例，传入当前帧，然后使用我们的扩展方法执行缩放（299
    x 299）并将渲染输出到像素缓冲区（将灰度参数设置为false，因为模型期望全色图像）。如果成功，我们将返回一个准备好的图像，可以传递给我们的模型进行推理，这是下一节的重点。
- en: Performing inference
  id: totrans-145
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行推理
- en: This may come as a bit of an anticlimax for someone expecting some hardcore
    coding, but its simplicity definitely pays tribute to the effort of Apple's engineers
    in making this framework one of the most accessible ways to work with a ML model.
    Without further ado, let's put the final pieces together; we start by instantiating
    an instance of our model we had imported in the previous section.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于期待一些硬核编码的人来说，这可能有点令人失望，但它的简单性无疑是对苹果工程师在使这个框架成为与机器学习模型工作最便捷方式之一所付出的努力的致敬。无需多言，让我们把最后几块拼图放在一起；我们从之前章节导入的模型实例化开始。
- en: 'Near the top, but within the body of the `ViewController` class, add the following
    line:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ViewController`类的主体部分靠近顶部的地方，添加以下行：
- en: '[PRE29]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Our model is now ready; we return to the `onFrameCaptured` method, starting
    from where we previously left off, and add the following code snippet:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模式现在准备好了；我们回到`onFrameCaptured`方法，从我们之前离开的地方开始，并添加以下代码片段：
- en: '[PRE30]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: In case you have missed it, I have made the statement performing inference in
    bold. That's it!
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您错过了，我在进行推理的地方使用了粗体。就是这样！
- en: After performing inference, we simply assign the `classLabel` property (the
    class with the highest probability) to our `UILabel`, `classifiedLabel`.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 执行推理后，我们只需将`classLabel`属性（概率最高的类别）分配给我们的`UILabel`，`classifiedLabel`。
- en: With the final piece put in place, we build and deploy. And see how well our
    app performs, recognizing some objects we have lying nearby. Once you're done
    surveying your space, return here, where we will wrap up this chapter and move
    on to greater and more impressive examples.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在放置好最后一块拼图后，我们构建并部署。看看我们的应用程序表现如何，识别我们附近的一些物体。在你完成对空间的调查后，回到这里，我们将结束本章，并继续更伟大、更令人印象深刻的例子。
- en: Summary
  id: totrans-154
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we introduced object recognition, the 101 project for ML with
    Core ML. We spent some time introducing CNNs or ConvNets, a category of neural
    networks well suited for extracting patterns from images. We discussed how they
    build increasing levels of abstraction with each convolutional layer. We then
    proceeded to make use of our newfound knowledge by implementing the functionality
    that allowed our application to recognize the physical world through its camera.
    We saw firsthand that the majority of the work wasn't performing inference but
    rather implementing the functionality to facilitate and make use of it. This is
    the take-away; intelligence by itself is not useful. What we are interested in
    exploring in this book is the application of trained models to deliver intuitive and
    intelligent experiences. For instance, this example can easily be turned into
    a language tutor assistant, allowing the user to learn a new language by observing
    the world around them.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了对象识别，这是使用Core ML的机器学习101项目。我们花了一些时间介绍CNN或卷积神经网络，这是一类非常适合从图像中提取模式的神经网络。我们讨论了它们如何通过每个卷积层构建越来越高的抽象层次。然后我们利用我们新获得的知识，通过实现允许我们的应用程序通过其摄像头识别物理世界的功能来使用它。我们亲眼看到，大部分工作不是执行推理，而是实现促进和利用它的功能。这就是我们的收获；智能本身并没有用。我们在这本书中感兴趣的是将训练好的模型应用于提供直观和智能的体验。例如，这个例子可以轻松地变成一个语言辅导助手，使用户通过观察周围的世界来学习一门新语言。
- en: In the next chapter, we will continue our journey into the world of computer
    vision with Core ML by looking at how we can infer the emotional state of someone
    by recognizing their facial expressions. Let's get to it.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续我们的Core ML计算机视觉之旅，看看我们如何通过识别某人的面部表情来推断他们的情绪状态。让我们开始吧。
