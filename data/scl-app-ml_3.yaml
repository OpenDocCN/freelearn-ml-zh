- en: Part III. Module 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分. 模块 3
- en: '**Mastering Scala Machine Learning**'
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**掌握 Scala 机器学习**'
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Advance your skills in efficient data analysis and data processing using the
    powerful tools of Scala, Spark, and Hadoop*'
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*利用 Scala、Spark 和 Hadoop 的强大工具提高您在高效数据分析与数据处理方面的技能*'
- en: Chapter 1. Exploratory Data Analysis
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第一章. 探索性数据分析
- en: 'Before I dive into more complex methods to analyze your data later in the book,
    I would like to stop at basic data exploratory tasks on which almost all data
    scientists spend at least 80-90% of their productive time. The data preparation,
    cleansing, transforming, and joining the data alone is estimated to be a $44 billion/year
    industry alone (*Data Preparation in the Big Data Era* by *Federico Castanedo*
    and *Best Practices for Data Integration*, *O''Reilly Media*, *2015*). Given this
    fact, it is surprising that people only recently started spending more time on
    the science of developing best practices and establishing good habits, documentation,
    and teaching materials for the whole process of data preparation (*Beautiful Data:
    The Stories Behind Elegant Data Solutions*, edited by *Toby Segaran* and *Jeff
    Hammerbacher*, *O''Reilly Media*, *2009* and *Advanced Analytics with Spark: Patterns
    for Learning from Data at Scale* by *Sandy Ryza et al.*, *O''Reilly Media*, *2015*).'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: '在我深入探讨本书后面更复杂的数据分析方法之前，我想停下来谈谈基本的数据探索性任务，几乎所有数据科学家至少花费 80-90% 的生产力时间在这些任务上。仅数据准备、清洗、转换和合并数据本身就是一个价值
    440 亿美元/年的产业（*《大数据时代的数据准备》*，作者：*Federico Castanedo* 和 *Best Practices for Data
    Integration*，*O''Reilly Media*，*2015*）。鉴于这一事实，人们最近才开始在开发最佳实践的科学、建立良好的习惯、文档和整个数据准备过程的教学材料上投入更多时间，这确实令人惊讶（*《美丽的数据：优雅数据解决方案背后的故事》*，由
    *Toby Segaran* 和 *Jeff Hammerbacher* 编著，*O''Reilly Media*，*2009* 以及 *Sandy Ryza
    等人所著的 *Advanced Analytics with Spark: Patterns for Learning from Data at Scale*，*O''Reilly
    Media*，*2015*）。'
- en: Few data scientists would agree on specific tools and techniques—and there are
    multiple ways to perform the exploratory data analysis, ranging from Unix command
    line to using very popular open source and commercial ETL and visualization tools.
    The focus of this chapter is how to use Scala and a laptop-based environment to
    benefit from techniques that are commonly referred as a functional paradigm of
    programming. As I will discuss, these techniques can be transferred to exploratory
    analysis over distributed system of machines using Hadoop/Spark.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 很少有数据科学家会就特定的工具和技术达成一致意见——进行探索性数据分析有多种方法，从 Unix 命令行到使用非常流行的开源和商业 ETL 和可视化工具。本章的重点是如何使用
    Scala 和基于笔记本电脑的环境来利用通常被称为编程的函数式范式的技术。正如我将讨论的，这些技术可以转移到使用 Hadoop/Spark 的分布式机器系统上的探索性分析中。
- en: What has functional programming to do with it? Spark was developed in Scala
    for a good reason. Many basic principles that lie at the foundation of functional
    programming, such as lazy evaluation, immutability, absence of side effects, list
    comprehensions, and monads go really well with processing data in distributed
    environments, specifically, when performing the data preparation and transformation
    tasks on big data. Thanks to abstractions, these techniques work well on a local
    workstation or a laptop. As mentioned earlier, this does not preclude us from
    processing very large datasets up to dozens of TBs on modern laptops connected
    to distributed clusters of storage/processing nodes. We can do it one topic or
    focus area at the time, but often we even do not have to sample or filter the
    dataset with proper partitioning. We will use Scala as our primary tool, but will
    resort to other tools if required.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式编程与这有什么关系？Spark 是用 Scala 开发的，这并非没有原因。许多位于函数式编程基础之上的基本原则，如惰性评估、不可变性、无副作用、列表推导和单子，非常适合在分布式环境中处理数据，特别是在对大数据进行数据准备和转换任务时。得益于抽象，这些技术在本地工作站或笔记本电脑上也能很好地工作。如前所述，这并不妨碍我们在连接到分布式存储/处理节点集群的现代笔记本电脑上处理数十
    TB 的非常大的数据集。我们可以一次处理一个主题或关注领域，但通常我们甚至不需要对数据集进行适当的分区采样或过滤。我们将使用 Scala 作为我们的主要工具，但在需要时也会求助于其他工具。
- en: While Scala is complete in the sense that everything that can be implemented
    in other languages can be implemented in Scala, Scala is fundamentally a high-level,
    or even a scripting, language. One does not have to deal with low-level details
    of data structures and algorithm implementations that in their majority have already
    been tested by a plethora of applications and time, in, say, Java or C++—even
    though Scala has its own collections and even some basic algorithm implementations
    today. Specifically, in this chapter, I'll be focusing on using Scala/Spark only
    for high-level tasks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Scala在某种意义上是完整的，即其他语言可以实现的任何内容都可以在Scala中实现，但Scala本质上是一种高级语言，甚至是一种脚本语言。你不必处理数据结构和算法实现中的低级细节，这些细节在Java或C++等语言中已经由大量的应用程序和时间测试过——尽管Scala今天有自己的集合和一些基本的算法实现。具体来说，在本章中，我将专注于使用Scala/Spark仅进行高级任务。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing Scala
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装Scala
- en: Learning simple techniques for initial data exploration
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习简单的数据探索技术
- en: Learning how to downsample the original dataset for faster turnover
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何对原始数据集进行下采样以加快周转速度
- en: Discussing the implementation of basic data transformation and aggregations
    in Scala
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论在Scala中实现基本数据转换和聚合的实现
- en: Getting familiar with big data processing tools such as Spark and Spark Notebook
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉大数据处理工具，如Spark和Spark Notebook
- en: Getting code for some basic visualization of datasets
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取一些基本数据集可视化的代码
- en: Getting started with Scala
  id: totrans-16
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala入门
- en: If you have already installed Scala, you can skip this paragraph. One can get
    the latest Scala download from [http://www.scala-lang.org/download/](http://www.scala-lang.org/download/).
    I used Scala version 2.11.7 on Mac OS X El Capitan 10.11.5\. You can use any other
    version you like, but you might face some compatibility problems with other packages
    such as Spark, a common problem in open source software as the technology adoption
    usually lags by a few released versions.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经安装了Scala，你可以跳过这一段。你可以从[http://www.scala-lang.org/download/](http://www.scala-lang.org/download/)获取最新的Scala下载。我在Mac
    OS X El Capitan 10.11.5上使用了Scala版本2.11.7。你可以使用你喜欢的任何其他版本，但可能与其他包（如Spark）存在一些兼容性问题，这是开源软件中常见的问题，因为技术的采用通常落后于几个发布版本。
- en: Tip
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: In most cases, you should try to maintain precise match between the recommended
    versions as difference in versions can lead to obscure errors and a lengthy debugging
    process.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你应该尝试保持推荐版本之间的精确匹配，因为版本之间的差异可能导致模糊的错误和漫长的调试过程。
- en: 'If you installed Scala correctly, after typing `scala`, you should see something
    similar to the following:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正确安装了Scala，在输入`scala`后，你应该看到以下类似的内容：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This is a Scala **read-evaluate-print-loop** (**REPL**) prompt. Although Scala
    programs can be compiled, the content of this chapter will be in REPL, as we are
    focusing on interactivity with, maybe, a few exceptions. The `:help` command provides
    a some utility commands available in REPL (note the colon at the start):'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个Scala **读取-评估-打印循环**（**REPL**）提示。虽然Scala程序可以编译，但本章的内容将在REPL中进行，因为我们专注于与交互，可能会有一些例外。`:help`命令提供了REPL中可用的某些实用命令（注意开头的冒号）：
- en: '![Getting started with Scala](img/image01626.jpeg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![Scala入门](img/image01626.jpeg)'
- en: Distinct values of a categorical field
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类的字段的不同值
- en: Now, you have a dataset and a computer. For convenience, I have provided you
    a small anonymized and obfuscated sample of clickstream data with the book repository
    that you can get at [https://github.com/alexvk/ml-in-scala.git](https://github.com/alexvk/ml-in-scala.git).
    The file in the `chapter01/data/clickstream` directory contains lines with timestamp,
    session ID, and some additional event information such as URL, category information,
    and so on at the time of the call. The first thing one would do is apply transformations
    to find out the distribution of values for different columns in the dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你有一个数据集和一台计算机。为了方便，我为你提供了一个小型的匿名和混淆的点击流数据样本，你可以从[https://github.com/alexvk/ml-in-scala.git](https://github.com/alexvk/ml-in-scala.git)获取这个样本。`chapter01/data/clickstream`目录中的文件包含时间戳、会话ID以及一些额外的调用时的事件信息，如URL、分类信息等。首先要做的事情是对数据进行转换，以找出数据集中不同列的值分布。
- en: '*Figure 01-1 shows* screenshot shows the output of the dataset in the terminal
    window of the `gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz | less
    –U` command. The columns are tab (`^I`) separated. One can notice that, as in
    many real-world big data datasets, many values are missing. The first column of
    the dataset is recognizable as the timestamp. The file contains complex data such
    as arrays, structs, and maps, another feature of big data datasets.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 01-1 展示的截图显示了* `gzcat chapter01/data/clickstream/clickstream_sample.tsv.gz
    | less –U` 命令在终端窗口中的数据集输出。列由制表符（`^I`）分隔。可以注意到，正如许多现实世界的大数据数据集一样，许多值是缺失的。数据集的第一列可识别为时间戳。该文件包含复杂的数据，如数组、结构体和映射，这是大数据数据集的另一个特征。'
- en: 'Unix provides a few tools to dissect the datasets. Probably, **less**, **cut**,
    **sort**, and **uniq** are the most frequently used tools for text file manipulations.
    **Awk**, **sed**, **perl**, and **tr** can do more complex transformations and
    substitutions. Fortunately, Scala allows you to transparently use command-line
    tools from within Scala REPL, as shown in the following screenshot:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Unix 提供了一些工具来剖析数据集。可能，**less**、**cut**、**sort** 和 **uniq** 是最常用于文本文件操作的工具。**Awk**、**sed**、**perl**
    和 **tr** 可以执行更复杂的转换和替换。幸运的是，Scala 允许你在 Scala REPL 中透明地使用命令行工具，如下面的截图所示：
- en: '![Distinct values of a categorical field](img/image01627.jpeg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![分类字段的唯一值](img/image01627.jpeg)'
- en: Figure 01-1\. The clickstream file as an output of the less -U Unix command
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-1\. `less -U` Unix 命令的输出作为点击流文件
- en: 'Fortunately, Scala allows you to transparently use command-line tools from
    within Scala REPL:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Scala 允许你在 Scala REPL 中透明地使用命令行工具：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I used the `scala.sys.process` package to call familiar Unix commands from Scala
    REPL. From the output, we can immediately see the customers of our Webshop are
    mostly interested in men's shoes and running, and that most visitors are using
    the referral code, **KW_0611081618**.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用了 `scala.sys.process` 包从 Scala REPL 调用熟悉的 Unix 命令。从输出中，我们可以立即看到我们网店的主要客户对男鞋和跑步感兴趣，并且大多数访客正在使用推荐代码，**KW_0611081618**。
- en: Tip
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: One may wonder when we start using complex Scala types and algorithms. Just
    wait, a lot of highly optimized tools were created before Scala and are much more
    efficient for explorative data analysis. In the initial stage, the biggest bottleneck
    is usually just the disk I/O and slow interactivity. Later, we will discuss more
    iterative algorithms, which are usually more memory intensive. Also note that
    the UNIX pipeline operations can be implicitly parallelized on modern multi-core
    computer architectures, as they are in Spark (we will show it in the later chapters).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用复杂的 Scala 类型和方法时，可能会有人想知道。请稍等，在 Scala 之前已经创建了大量的高度优化的工具，它们对于探索性数据分析来说效率更高。在初始阶段，最大的瓶颈通常是磁盘
    I/O 和缓慢的交互性。稍后，我们将讨论更多迭代算法，这些算法通常更占用内存。还要注意，UNIX 管道操作可以在现代多核计算机架构上隐式并行化，就像在 Spark
    中一样（我们将在后面的章节中展示）。
- en: It has been shown that using compression, implicit or explicit, on input data
    files can actually save you the I/O time. This is particularly true for (most)
    modern semi-structured datasets with repetitive values and sparse content. Decompression
    can also be implicitly parallelized on modern fast multi-core computer architectures,
    removing the computational bottleneck, except, maybe in cases where compression
    is implemented implicitly in hardware (SSD, where we don't need to compress the
    files explicitly). We also recommend using directories rather than files as a
    paradigm for the dataset, where the insert operation is reduced to dropping the
    data file into a directory. This is how the datasets are presented in big data
    Hadoop tools such as Hive and Impala.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，在输入数据文件上使用压缩，无论是隐式还是显式，实际上可以节省 I/O 时间。这对于（大多数）现代半结构化数据集尤其如此，这些数据集具有重复的值和稀疏的内容。在现代快速的多核计算机架构上，解压缩也可以隐式并行化，从而消除计算瓶颈，除非，可能在硬件中隐式实现压缩的情况下（SSD，在这种情况下我们不需要显式压缩文件）。我们还建议使用目录而不是文件作为数据集的模式，其中插入操作简化为将数据文件放入目录中。这就是大数据
    Hadoop 工具（如 Hive 和 Impala）展示数据集的方式。
- en: Summarization of a numeric field
  id: totrans-36
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数值字段的摘要
- en: 'Let''s look at the numeric data, even though most of the columns in the dataset
    are either categorical or complex. The traditional way to summarize the numeric
    data is a five-number-summary, which is a representation of the median or mean,
    interquartile range, and minimum and maximum. I''ll leave the computations of
    the median and interquartile ranges till the Spark DataFrame is introduced, as
    it makes these computations extremely easy; but we can compute mean, min, and
    max in Scala by just applying the corresponding operators:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看数值数据，尽管数据集中的大多数列都是分类的或复杂的。总结数值数据的传统方法是五数摘要，它表示中位数或平均值、四分位数范围以及最小值和最大值。我将把中位数和四分位数范围的计算留到Spark
    DataFrame介绍时，因为它使这些计算变得极其简单；但我们可以通过应用相应的运算符在Scala中计算平均值、最小值和最大值：
- en: '[PRE2]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Grepping across multiple fields
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在多个字段中进行grep搜索
- en: Sometimes one needs to get an idea of how a certain value looks across multiple
    fields—most common are IP/MAC addresses, dates, and formatted messages. For examples,
    if I want to see all IP addresses mentioned throughout a file or a document, I
    need to replace the `cut` command in the previous example by `grep -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}`,
    where the `–o` option instructs `grep` to print only the matching parts—a more
    precise regex for the IP address should be `grep –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)`,
    but is about 50% slower on my laptop and the original one works in most practical
    cases. I'll leave it as an excursive to run this command on the sample file provided
    with the book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有时需要了解某个值在多个字段中的外观——最常见的是IP/MAC地址、日期和格式化消息。例如，如果我想查看文件或文档中提到的所有IP地址，我需要将上一个示例中的`cut`命令替换为`grep
    -o -E [1-9][0-9]{0,2}(?:\\.[1-9][0-9]{0,2}){3}`，其中`-o`选项指示`grep`只打印匹配的部分——一个更精确的IP地址正则表达式应该是`grep
    –o –E (?:(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)\.){3}(?:25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)`，但在我和原始的笔记本电脑上大约慢50%，但原始的正则表达式在大多数实际情况下都适用。我将把它作为一个练习，在书中提供的样本文件上运行这个命令。
- en: Basic, stratified, and consistent sampling
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本抽样、分层抽样和一致抽样
- en: I've met quite a few data practitioners who scorn sampling. Ideally, if one
    can process the whole dataset, the model can only improve. In practice, the tradeoff
    is much more complex. First, one can build more complex models on a sampled set,
    particularly if the time complexity of the model building is non-linear—and in
    most situations, if it is at least *N* log(N)*. A faster model building cycle
    allows you to iterate over models and converge on the best approach faster. In
    many situations, *time to action* is beating the potential improvements in the
    prediction accuracy due to a model built on complete dataset.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到过很多数据从业者，他们轻视抽样。理想情况下，如果一个人能够处理整个数据集，模型只会得到改进。在实践中，这种权衡要复杂得多。首先，一个人可以在抽样集上构建更复杂的模型，尤其是如果模型构建的时间复杂度是非线性的——在大多数情况下，至少是*N*
    log(N)*。更快的模型构建周期允许你更快地迭代模型并收敛到最佳方法。在许多情况下，“行动时间”会打败基于完整数据集构建的模型在预测精度上的潜在改进。
- en: Sampling may be combined with appropriate filtering—in many practical situation,
    focusing on a subproblem at a time leads to better understanding of the whole
    problem domain. In many cases, this partitioning is at the foundation of the algorithm,
    like in decision trees, which are considered later. Often the nature of the problem
    requires you to focus on the subset of original data. For example, a cyber security
    analysis is often focused around a specific set of IPs rather than the whole network,
    as it allows to iterate over hypothesis faster. Including the set of all IPs in
    the network may complicate things initially if not throw the modeling off the
    right track.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 抽样可以与适当的过滤相结合——在许多实际情况下，一次关注一个子问题可以更好地理解整个问题域。在许多情况下，这种划分是算法的基础，例如在稍后讨论的决策树中。通常，问题的性质要求你关注原始数据的一个子集。例如，网络安全分析通常关注一组特定的IP地址，而不是整个网络，因为它允许更快地迭代假设。如果不在正确的轨道上，将网络中所有IP地址的集合包括在内可能会在最初使事情复杂化。
- en: When dealing with rare events, such as clickthroughs in ADTECH, sampling the
    positive and negative cases with different probabilities, which is also sometimes
    called oversampling, often leads to better predictions in short amount of time.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理罕见事件，例如ADTECH中的点击次数时，以不同的概率对正负案例进行抽样，这有时也被称为过采样，通常能在短时间内带来更好的预测结果。
- en: 'Fundamentally, sampling is equivalent to just throwing a coin—or calling a
    random number generator—for each data row. Thus it is very much like a stream
    filter operation, where the filtering is on an augmented column of random numbers.
    Let''s consider the following example:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，采样等同于对每一行数据抛硬币——或者调用随机数生成器。因此，它非常类似于流过滤操作，这里的过滤是在随机数增强列上进行的。让我们考虑以下示例：
- en: '[PRE3]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This is all good, but it has the following disadvantages:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切都很好，但它有以下缺点：
- en: The number of lines in the resulting file is not known beforehand—even though
    on average it should be 5% of the original file
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果文件的行数事先是未知的——尽管平均来说应该是原始文件的5%
- en: The results of the sampling is non-deterministic—it is hard to rerun this process
    for either testing or verification
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采样的结果是非确定的——很难重新运行此过程进行测试或验证
- en: 'To fix the first point, we''ll need to pass a more complex object to the function,
    as we need to maintain the state during the original list traversal, which makes
    the original algorithm less functional and parallelizable (this will be discussed
    later):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个问题，我们需要传递一个更复杂的对象给函数，因为我们需要在原始列表遍历期间保持状态，这使得原始算法功能性和并行性降低（这将在稍后讨论）：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This will output `numLines` lines. Similarly to reservoir sampling, stratified
    sampling is guaranteed to provide the same ratios of input/output rows for all
    strata defined by levels of another attribute. We can achieve this by splitting
    the original dataset into *N* subsets corresponding to the levels, performing
    the reservoir sampling, and merging the results afterwards. However, MLlib library,
    which will be covered in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*, already has stratified
    sampling implementation:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这将输出`numLines`行。类似于蓄水池采样，分层采样保证为所有由另一个属性的级别定义的层提供相同的输入/输出行比例。我们可以通过将原始数据集分割成与级别相对应的*N*个子集，执行蓄水池采样，然后合并结果来实现这一点。然而，将在[第3章](part0249.xhtml#aid-7DES21
    "第3章。使用Spark和MLlib")中介绍的MLlib库，*使用Spark和MLlib*，已经实现了分层采样的实现：
- en: '[PRE5]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The other bullet point is more subtle; sometimes we want a consistent subset
    of values across multiple datasets, either for reproducibility or to join with
    another sampled dataset. In general, if we sample two datasets, the results will
    contain random subsets of IDs which might have very little or no intersection.
    The cryptographic hashing functions come to the help here. The result of applying
    a hash function such as MD5 or SHA1 is a sequence of bits that is statistically
    uncorrelated, at least in theory. We will use the `MurmurHash` function, which
    is part of the `scala.util.hashing` package:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个要点更为微妙；有时我们希望在多个数据集中保持值的一致子集，无论是为了可重复性还是为了与其他采样数据集连接。一般来说，如果我们采样两个数据集，结果将包含随机子集的ID，这些ID可能几乎没有交集或完全没有交集。密码学哈希函数在这里提供了帮助。应用MD5或SHA1等哈希函数的结果是一系列在理论上至少是统计上不相关的位序列。我们将使用`MurmurHash`函数，它是`scala.util.hashing`包的一部分：
- en: '[PRE6]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This function is guaranteed to return exactly the same subset of records based
    on the value of the first field—it is either all records where the first field
    equals a certain value or none—and will come up with approximately one-sixteenth
    of the original sample; the range of `hash` is `0` to `65,535`.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数保证基于第一个字段的值返回完全相同的记录子集——要么是第一个字段等于某个特定值的所有记录，要么是没有任何记录——并且将产生大约原始样本的六分之一；`hash`的范围是`0`到`65,535`。
- en: Note
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: MurmurHash? It is not a cryptographic hash!
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: MurmurHash？它不是一个密码学哈希！
- en: Unlike cryptographic hash functions, such as MD5 and SHA1, MurmurHash is not
    specifically designed to be hard to find an inverse of a hash. It is, however,
    really fast and efficient. This is what really matters in our use case.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与MD5和SHA1等密码学哈希函数不同，MurmurHash并不是专门设计成难以找到哈希的逆函数。然而，它确实非常快且高效。在我们的用例中，这真正是重要的。
- en: Working with Scala and Spark Notebooks
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Scala和Spark笔记本工作
- en: Often the most frequent values or five-number summary are not sufficient to
    get the first understanding of the data. The term **descriptive statistics** is
    very generic and may refer to very complex ways to describe the data. Quantiles,
    a **Paretto** chart or, when more than one attribute is analyzed, correlations
    are also examples of descriptive statistics. When sharing all these ways to look
    at the data aggregates, in many cases, it is also important to share the specific
    computations to get to them.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，最频繁的值或五数摘要不足以获得对数据的初步理解。术语 **描述性统计** 非常通用，可能指代描述数据非常复杂的方法。分位数、**帕累托**图，或者当分析多个属性时，相关性也是描述性统计的例子。当分享所有这些查看数据聚合的方法时，在许多情况下，分享得到这些计算的具体计算也很重要。
- en: Scala or Spark Notebook [https://github.com/Bridgewater/scala-notebook](https://github.com/Bridgewater/scala-notebook),
    [https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)
    record the whole transformation path and the results can be shared as a JSON-based
    `*.snb` file. The Spark Notebook project can be downloaded from [http://spark-notebook.io](http://spark-notebook.io),
    and I will provide a sample `Chapter01.snb` file with the book. I will use Spark,
    which I will cover in more detail in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Scala 或 Spark Notebook [https://github.com/Bridgewater/scala-notebook](https://github.com/Bridgewater/scala-notebook),
    [https://github.com/andypetrella/spark-notebook](https://github.com/andypetrella/spark-notebook)
    记录整个转换路径，结果可以作为基于 JSON 的 `*.snb` 文件共享。Spark Notebook 项目可以从 [http://spark-notebook.io](http://spark-notebook.io)
    下载，我将提供与本书一起的示例 `Chapter01.snb` 文件。我将使用 Spark，我将在第 3 章（part0249.xhtml#aid-7DES21
    "第 3 章. 使用 Spark 和 MLlib"）中更详细地介绍，*使用 Spark 和 MLlib*。
- en: For this particular example, Spark will run in the local mode. Even in the local
    mode Spark can utilize parallelism on your workstation, but it is limited to the
    number of cores and hyperthreads that can run on your laptop or workstation. With
    a simple configuration change, however, Spark can be pointed to a distributed
    set of machines and use resources across a distributed set of nodes.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的例子，Spark 将在本地模式下运行。即使在本地模式下，Spark 也可以在您的工作站上利用并行性，但受限于可以在您的笔记本电脑或工作站上运行的内核和超线程数量。然而，通过简单的配置更改，Spark
    可以指向一组分布式机器，并使用分布式节点集的资源。
- en: 'Here is the set of commands to download the Spark Notebook and copy the necessary
    files from the code repository:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是下载 Spark Notebook 并从代码仓库复制必要文件的命令集：
- en: '[PRE7]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now you can open the notebook at `http://localhost:9000` in your browser, as
    shown in the following screenshot:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以在浏览器中打开 `http://localhost:9000` 上的笔记本，如下面的截图所示：
- en: '![Working with Scala and Spark Notebooks](img/image01628.jpeg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/image01628.jpeg)'
- en: Figure 01-2\. The first page of the Spark Notebook with the list of notebooks
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-2\. Spark 笔记本的第一页，列出了笔记本列表
- en: 'Open the `Chapter01` notebook by clicking on it. The statements are organized
    into cells and can be executed by clicking on the small right arrow at the top,
    as shown in the following screenshot, or run all cells at once by navigating to
    **Cell** | **Run All**:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 通过点击它打开 `Chapter01` 笔记本。语句被组织成单元格，可以通过点击顶部的较小右箭头来执行，如下面的截图所示，或者通过导航到 **单元格**
    | **运行所有** 来一次性运行所有单元格：
- en: '![Working with Scala and Spark Notebooks](img/image01629.jpeg)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/image01629.jpeg)'
- en: Figure 01-3\. Executing the first few cells in the notebook
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-3\. 执行笔记本中的前几个单元格
- en: 'First, we will look at the values of all or some of discrete variables. For
    example, to get the distribution of the labels, issue the following code:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将查看所有或某些离散变量的值。例如，要获取标签的分布，请执行以下代码：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The first time I read the dataset, it took about a minute on MacBook Pro, but
    Spark caches the data in memory and the subsequent aggregation runs take only
    about a second. Spark Notebook provides you the distribution of the values, as
    shown in the following screenshot:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次读取数据集时，在 MacBook Pro 上大约花费了一分钟，但 Spark 将数据缓存到内存中，后续的聚合运行只需大约一秒钟。Spark Notebook
    提供了值的分布，如下面的截图所示：
- en: '![Working with Scala and Spark Notebooks](img/image01630.jpeg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/image01630.jpeg)'
- en: Figure 01-4\. Computing the distribution of values for a categorical field
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-4\. 计算分类字段的值分布
- en: 'I can also look at crosstab counts for pairs of discrete variables, which gives
    me an idea of interdependencies between the variables using [http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions)—the
    object does not support computing correlation measures such as chi-square yet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我还可以查看离散变量的交叉表计数，这让我对变量之间的相互依赖性有了了解，请参阅[http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions](http://spark.apache.org/docs/latest/api/scala/index.html#org.apache.spark.sql.DataFrameStatFunctions)——该对象尚不支持计算如卡方检验之类的相关度量：
- en: '![Working with Scala and Spark Notebooks](img/image01631.jpeg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/image01631.jpeg)'
- en: Figure 01-5\. Contingency table or crosstab
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-5\. 列联表或交叉表
- en: However, we can see that the most popular service is private and it correlates
    well with the `SF` flag. Another way to analyze dependencies is to look at `0`
    entries. For example, the `S2` and `S3` flags are clearly related to the SMTP
    and FTP traffic since all other entries are `0`.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们可以看到最受欢迎的服务是私有的，并且它与 `SF` 标志的相关性很好。另一种分析依赖关系的方法是查看 `0` 条目。例如，`S2` 和 `S3`
    标志显然与 SMTP 和 FTP 流量相关，因为所有其他条目都是 `0`。
- en: Of course, the most interesting correlations are with the target variable, but
    these are better discovered by supervised learning algorithms that I will cover
    in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark and MLlib"),
    *Working with Spark and MLlib,* and [Chapter 5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression
    and Classification"), *Regression and Classification*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，最有趣的关联是与目标变量相关，但这些最好通过我在第 3 章（part0249.xhtml#aid-7DES21 "第 3 章。使用 Spark 和
    MLlib"）和第 5 章（part0260.xhtml#aid-7NUI81 "第 5 章。回归和分类"）中将要介绍的监督学习算法来发现，*使用 Spark
    和 MLlib* 和 *回归和分类*。
- en: '![Working with Scala and Spark Notebooks](img/image01632.jpeg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![使用 Scala 和 Spark 笔记本](img/image01632.jpeg)'
- en: Figure 01-6\. Computing simple aggregations using org.apache.spark.sql.DataFrameStatFunctions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 01-6\. 使用 org.apache.spark.sql.DataFrameStatFunctions 计算简单聚合。
- en: 'Analogously, we can compute correlations for numerical variables with the `dataFrame.stat.corr()`
    and `dataFrame.stat.cov()` functions (refer to *Figure 01-6)*. In this case, the
    class supports the **Pearson correlation coefficient**. Alternatively, we can
    use the standard SQL syntax on the parquet file directly:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，我们可以使用 `dataFrame.stat.corr()` 和 `dataFrame.stat.cov()` 函数计算数值变量的相关性（参见图
    01-6）。在这种情况下，该类支持**皮尔逊相关系数**。或者，我们可以直接在 parquet 文件上使用标准的 SQL 语法：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Finally, I promised you to compute percentiles. Computing percentiles usually
    involves sorting the whole dataset, which is expensive; however, if the tile is
    one of the first or the last ones, usually it is possible to optimize the computation:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我答应您计算百分位数。计算百分位数通常涉及对整个数据集进行排序，这是昂贵的；然而，如果分块是前几个或最后几个之一，通常可以优化计算：
- en: '[PRE10]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Computing the exact percentiles for a more generic case is more computationally
    expensive and is provided as a part of the Spark Notebook example code.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更通用的案例，计算确切的百分位数计算成本更高，它是 Spark 笔记本示例代码的一部分。
- en: Basic correlations
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 基本相关性
- en: You probably noticed that detecting correlations from contingency tables is
    hard. Detecting patterns takes practice, but many people are much better at recognizing
    the patterns visually. Detecting actionable patterns is one of the primary goals
    of machine learning. While advanced supervised machine learning techniques that
    will be covered in [Chapter 4](part0256.xhtml#aid-7K4G02 "Chapter 4. Supervised
    and Unsupervised Learning"), *Supervised and Unsupervised Learning* and [Chapter
    5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression and Classification"), *Regression
    and Classification* exist, initial analysis of interdependencies between variables
    can help with the right transformation of variables or selection of the best inference
    technique.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到，从列联表中检测相关性是困难的。检测模式需要练习，但许多人更擅长直观地识别模式。检测可操作的模式是机器学习的主要目标之一。虽然将在第 4
    章（part0256.xhtml#aid-7K4G02 "第 4 章。监督学习和无监督学习"）和第 5 章（part0260.xhtml#aid-7NUI81
    "第 5 章。回归和分类"）中介绍的高级监督机器学习技术，*监督学习和无监督学习*以及*回归和分类*存在，但变量之间相互依赖性的初步分析有助于正确转换变量或选择最佳推理技术。
- en: Multiple well-established visualization tools exist and there are multiple sites,
    such as [http://www.kdnuggets.com](http://www.kdnuggets.com), which specialize
    on ranking and providing recommendations on data analysis, data explorations,
    and visualization software. I am not going to question the validity and accuracy
    of such rankings in this book, and very few sites actually mention Scala as a
    specific way to visualize the data, even if this is possible with, say, a `D3.js`
    package. A good visualization is a great way to deliver your findings to a larger
    audience. One look is worth a thousand words.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 存在多个成熟的可视化工具，并且有多个网站，例如[http://www.kdnuggets.com](http://www.kdnuggets.com)，它们专注于对数据分析、数据探索和可视化软件进行排名和提供推荐。在这本书中，我不会质疑这些排名的有效性和准确性，实际上很少有网站提到Scala作为可视化数据的具体方式，即使使用`D3.js`包也是可能的。一个好的可视化是向更广泛的受众传达发现的好方法。一看胜千言。
- en: 'For the purposes of this chapter, I will use **Grapher** that is present on
    every Mac OS notebook. To open **Grapher**, go to Utilities (*shift* + *command*
    + *U* in Finder) and click on the **Grapher** icon (or search by name by pressing
    *command* + *space*). Grapher presents many options, including the following **Log-Log**
    and **Polar** coordinates:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，我将使用每个Mac OS笔记本上都有的**Grapher**。要打开**Grapher**，请转到实用工具（在Finder中按*shift*
    + *command* + *U*）并单击**Grapher**图标（或按*command* + *space*按名称搜索）。Grapher提供了许多选项，包括以下**对数-对数**和**极坐标**：
- en: '![Basic correlations](img/image01633.jpeg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![基本相关性](img/image01633.jpeg)'
- en: Figure 01-7\. The Grapher window
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图01-7\. Grapher窗口
- en: Fundamentally, the amount of information that can be delivered through visualization
    is limited by the number of pixels on the screen, which, for most modern computers,
    is in millions and color variations, which arguably can also be in millions (*Judd*,
    *Deane B.*; *Wyszecki*, *Günter* (*1975*). *Color in Business, Science and Industry*.
    *Wiley Series in Pure and Applied Optics (3rd ed.)*. New York). If I am working
    on a multidimensional TB dataset, the dataset first needs to be summarized, processed,
    and reduced to a size that can be viewed on a computer screen.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 从根本上讲，通过可视化可以传达的信息量受屏幕上像素数量的限制，对于大多数现代计算机来说，这是数百万，以及颜色变化，这也可以说是数百万（*Judd*，*Deane
    B.*；*Wyszecki*，*Günter*（1975）。*商业、科学和工业中的颜色*。*纯与应用光学系列（第3版）*。纽约）。如果我正在处理一个多维TB数据集，该数据集首先需要被总结、处理，并减少到可以在计算机屏幕上查看的大小。
- en: 'For the purpose of illustration, I will use the Iris UCI dataset that can be
    found at [https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris).
    To bring the dataset into the tool, type the following code (on Mac OS):'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，我将使用可以在[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)找到的Iris
    UCI数据集。要将数据集引入工具，请输入以下代码（在Mac OS上）：
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Open the new **Point Set** in the **Grapher** (*command* + *alt* + *P*), press
    **Edit Points…** and paste the data by pressing *command* + *V*. The tools has
    line-fitting capabilities with basic linear, polynomial, and exponential families
    and provides the popular chi-squared metric to estimate the goodness of the fit
    with respect to the number of free parameters:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在**Grapher**中打开新的**点集**（*command* + *alt* + *P*），按**编辑点…**并按*command* + *V*粘贴数据。该工具具有基本的线性、多项式和指数族等线拟合能力，并提供流行的卡方指标来估计拟合的优良程度，相对于自由参数的数量：
- en: '![Basic correlations](img/image01634.jpeg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![基本相关性](img/image01634.jpeg)'
- en: Figure 01-8\. Fitting the Iris dataset using Grapher on Mac OS X
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图01-8\. 在Mac OS X上使用Grapher拟合Iris数据集
- en: We will cover how to estimate the goodness of model fit in the following chapters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在接下来的章节中介绍如何估计模型拟合的优良程度。
- en: Summary
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: I've tried to establish a common ground to perform a more complex data science
    later in the book. Don't expect these to be a complete set of exploratory techniques,
    as the exploratory techniques can extend to running very complex modes. However,
    we covered simple aggregations, sampling, file operations such as read and write,
    working with tools such as notebooks and Spark DataFrames, which brings familiar
    SQL constructs into the arsenal of an analyst working with Spark/Scala.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我试图在书中建立一个共同的基础，以便在后面的章节中进行更复杂的数据科学。不要期望这些是完整的探索性技术集，因为探索性技术可以扩展到运行非常复杂模式。然而，我们已经涵盖了简单的聚合、抽样、读写等文件操作，以及使用笔记本和Spark
    DataFrames等工具，这些工具将熟悉的SQL结构引入了与Spark/Scala一起工作的分析师的武器库中。
- en: 'The next chapter will take a completely different turn by looking at the data
    pipelines as a part of a data-driven enterprise and cover the data discovery process
    from the business perspective: what are the ultimate goals we are trying to accomplish
    by doing the data analysis. I will cover a few traditional topics of ML, such
    as supervised and unsupervised learning, after this before delving into more complex
    representations of the data, where Scala really shows it''s advantage over SQL.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将完全转变方向，将数据管道视为数据驱动企业的组成部分，并从业务角度涵盖数据发现过程：我们通过数据分析试图实现哪些最终目标。在这之后，我将介绍一些传统的机器学习主题，如监督学习和无监督学习，然后再深入研究更复杂的数据表示，Scala在这里真正显示出其相对于SQL的优势。
- en: Chapter 2. Data Pipelines and Modeling
  id: totrans-105
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章：数据管道和建模
- en: We have looked at basic hands-on tools for exploring the data in the previous
    chapter, thus we now can delve into more complex topics of statistical model building
    and optimal control or science-driven tools and problems. I will go ahead and
    say that we will only touch on some topics in optimal control since this book
    really is just about ML in Scala and not the theory of data-driven business management,
    which might be an exciting topic for a book on its own.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了探索数据的基本动手工具，因此我们现在可以深入研究更复杂的主题，如统计模型构建、最优控制或科学驱动工具和问题。我将继续说，我们只会触及一些最优控制的话题，因为这本书实际上只是关于Scala中的机器学习，而不是数据驱动业务管理的理论，这可能是一个单独成书的激动人心的主题。
- en: 'In this chapter, I will stay away from specific implementations in Scala and
    discuss the problem of building a data-driven enterprise at a high level. Later
    chapters will address how to solve these smaller pieces of the puzzle. A special
    emphasis will be given to handing uncertainty. Uncertainty usually comes in several
    flavors: first, there can be noise in the information we are provided with. Secondly,
    the information can be incomplete. The system may have some degree of freedom
    in filling the missing pieces, which results in uncertainty. Finally, there may
    be variations in the interpretation of the models and the resulting metrics. The
    final point is subtle, as most classic textbooks assume that we can measure things
    directly. Not only the measurements may be noisy, but the definition of the measure
    may change in time—try measuring satisfaction or happiness. Certainly, we can
    avoid the ambiguity by saying that we can optimize only measurable metrics, as
    people usually do, but it will significantly limit the application domain in practice.
    Nothing prevents the scientific machinery from handling the uncertainty in the
    interpretation into account as well.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将避免具体实现Scala，并从高层次讨论构建数据驱动企业的相关问题。后面的章节将解决这些难题的解决方案。特别强调处理不确定性。不确定性通常有几种形式：首先，我们提供的信息中可能存在噪声。其次，信息可能不完整。系统在填补缺失部分时可能有一定的自由度，这导致不确定性。最后，模型解释和结果指标可能存在差异。最后一个观点很微妙，因为大多数经典教科书都假设我们可以直接测量事物。不仅测量可能存在噪声，而且测量的定义可能随时间变化——尝试测量满意度或幸福感。当然，我们可以通过说我们只能优化可测量的指标来避免这种歧义，就像人们通常做的那样，但这将显著限制实际应用的范围。没有任何东西阻止科学机制在处理解释不确定性时将其考虑在内。
- en: The predictive models are often built just for data understanding. From the
    linguistic derivation, model is a simplified representation of the actual complex
    buildings or processes for exactly the purpose of making a point and convincing
    people, one or another way. The ultimate goal for predictive modeling, the modeling
    I am concerned about in this book and this chapter specifically, is to optimize
    the business processes by taking the most important factors into account in order
    to make the world a better place. This was certainly a sentence with a lot of
    uncertainty entrenched, but at least it looks like a much better goal than optimizing
    a click-through rate.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 预测模型通常只是为了数据理解而构建。从语言学的推导来看，模型是对实际复杂建筑或过程的简化表示，其目的正是为了阐明观点和说服人们，无论通过何种方式。预测建模的最终目标，也就是我在本书和本章中关注的目标，是通过考虑最重要的因素来优化业务流程，以便让世界变得更加美好。这当然是一个充满不确定性的句子，但至少它看起来比优化点击率要好得多。
- en: 'Let''s look at a traditional business decision-making process: a traditional
    business might involve a set of C-level executives making decisions based on information
    that is usually obtained from a set of dashboards with graphical representation
    of the data in one or several DBs. The promise of an automated data-driven business
    is to be able to automatically make most of the decisions provided the uncertainties
    eliminating human bias. This is not to say that we no longer need C-level executives,
    but the C-level executives will be busy helping the machines to make the decisions
    instead of the other way around.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看传统的商业决策过程：一家传统的企业可能涉及一组C级高管根据通常从一组包含一个或多个数据库中数据图形表示的仪表板中获得的信息做出决策。自动化数据驱动型企业的承诺是，在消除人类偏见的情况下，能够自动做出大多数决策。这并不是说我们不再需要C级高管，但C级高管将忙于帮助机器做出决策，而不是相反。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Going through the basics of influence diagrams as a tool for decision making
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索影响图作为决策工具的基本原理
- en: Looking at variations of the pure decision making optimization in the context
    of adaptive **Markov Decision** making process and **Kelly Criterion**
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在自适应**马尔可夫决策过程**和**凯利准则**的背景下查看纯决策优化变体
- en: Getting familiar with at least three different practical strategies for exploration-exploitation
    trade-off
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉至少三种不同的探索-利用权衡策略
- en: Describing the architecture of a data-driven enterprise
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述数据驱动型企业的架构
- en: Discussing major architectural components of a decision-making pipeline
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论决策管道的主要架构组件
- en: Getting familiar with standard tools for building data pipelines
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉构建数据管道的标准工具
- en: Influence diagrams
  id: totrans-117
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 影响图
- en: 'While the decision making process can have multiple facets, a book about decision
    making under uncertainty would be incomplete without mentioning influence diagrams
    (*Influence Diagrams for Team Decision Analysis*, Decision Analysis 2 (4): 207–228),
    which help the analysis and understanding of the decision-making process. The
    decision may be as mundane as selection of the next news article to show to a
    user in a personalized environment or a complex one as detecting malware on an
    enterprise network or selecting the next research project.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然决策过程可能具有多个方面，但一本关于不确定条件下决策的书如果没有提到影响图（*团队决策分析中的影响图*，《决策分析》第2卷（4）：207–228），就会显得不完整。影响图有助于分析和理解决策过程。决策可能像选择在个性化环境中向用户展示的下一条新闻文章这样平凡，也可能像在企业网络中检测恶意软件或选择下一个研究项目这样复杂。
- en: 'Depending on the weather she can try and go on a boat trip. We can represent
    the decision-making process as a diagram. Let''s decide whether to take a river
    boat tour during her stay in Portland, Oregon:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据天气情况，她可以尝试进行一次乘船之旅。我们可以将决策过程表示为图表。让我们决定在她在俄勒冈州波特兰逗留期间是否乘坐游船：
- en: '![Influence diagrams](img/image01635.jpeg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![影响图](img/image01635.jpeg)'
- en: Figure 02-1\. A simple vacation influence diagram to represent a simple decision-making
    process. The diagram contains decision nodes such as Vacation Activity, observable
    and unobservable information nodes such as Weather Forecast and Weather, and finally
    the value node such as Satisfaction
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图02-1。一个简单的假期影响图，用于表示简单的决策过程。该图包含决策节点，如度假活动，可观察和不可观察的信息节点，如天气预报和天气，以及最终的价值节点，如满意度
- en: The preceding diagram represents this situation. The decision whether to participate
    in the activity is clearly driven by the potential to get certain satisfaction,
    which is a function of the decision itself and the weather at the time of the
    activity. While the actual weather conditions are unknown at the time of the trip
    planning, we believe there is a certain correlation between the weather forecast
    and the actual weather experienced during the trip, which is represented by the
    edge between the **Weather** and **Weather Forecast** nodes. The **Vacation Activity**
    node is the decision node, it has only one parent as the decision is made solely
    based on **Weather Forecast**. The final node in the DAG is **Satisfaction**,
    which is a function of the actual whether and the decision we made during the
    trip planning—obviously, *yes + good weather* and *no + bad weather* are likely
    to have the highest scores. The *yes + bad weather* and *no + good weather* would
    be a bad outcome—the latter case is probably just a missed opportunity, but not
    necessarily a bad decision, provided an inaccurate weather forecast.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表代表了这种情况。是否参加活动的决策明显是由获得一定满意度的可能性驱动的，这是决策本身和活动时的天气的函数。虽然旅行计划时的实际天气条件是未知的，但我们相信天气预报和旅行期间实际经历的天气之间存在某种相关性，这由**天气**和**天气预报**节点之间的边表示。**度假活动**节点是决策节点，它只有一个父节点，因为决策完全基于**天气预报**。DAG中的最后一个节点是**满意度**，它是实际天气和我们在旅行计划期间所做的决策的函数——显然，“是
    + 好天气”和“否 + 坏天气”可能得分最高。而“是 + 坏天气”和“否 + 好天气”将是一个不良的结果——后者可能是错过机会，但并不一定是一个糟糕的决策，前提是天气预报不准确。
- en: The absence of an edge carries an independence assumption. For example, we believe
    that **Satisfaction** should not depend on **Weather Forecast**, as the latter
    becomes irrelevant once we are on the boat. Once the vacation plan is finalized,
    the actual weather during the boating activity can no longer affect the decision,
    which was made solely based on the weather forecast; at least in our simplified
    model, where we exclude the option of buying a trip insurance.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘的缺失包含了一个独立性假设。例如，我们相信**满意度**不应该依赖于**天气预报**，因为后者在我们上船后就变得无关紧要了。一旦度假计划确定，实际天气在划船活动期间就不再影响决策，该决策完全基于天气预报；至少在我们的简化模型中，我们排除了购买旅行保险的选项。
- en: 'The graph shows different stages of decision making and the flow of information
    (we will provide an actual graph implementation in Scala in [Chapter 7](part0283.xhtml#aid-8DSF61
    "Chapter 7. Working with Graph Algorithms"), *Working with Graph Algorithms*).
    There is only one piece of information required to make the decision in our simplified
    diagram: the weather forecast. Once the decision is made, we can no longer change
    it, even if we have information about the actual weather at the time of the trip.
    The weather and the decision data can be used to model her satisfaction with the
    decision she has made.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图表展示了决策的不同阶段和信息流（我们将在第7章[Chapter 7](part0283.xhtml#aid-8DSF61 "第7章。使用图算法")，*使用图算法*）中提供实际的Scala实现）。在我们的简化图中，做出决策只需要一条信息：天气预报。一旦做出决策，我们就无法更改它，即使我们有关于旅行时实际天气的信息。天气和决策数据可以用来模拟她对所做决策的满意度。
- en: 'Let''s map this approach to an advertising problem as an illustration: the
    ultimate goal is to get user satisfaction with the targeted ads, which results
    in additional revenue for an advertiser. The satisfaction is the function of user-specific
    environmental state, which is unknown at the time of decision making. Using machine
    learning algorithms, however, we can forecast this state based on the user''s
    recent Web visit history and other information that we can gather, such as geolocation,
    browser-agent string, time of day, category of the ad, and so on (refer to *Figure
    02-2*).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这种方法映射到一个广告问题作为说明：最终目标是获得用户对目标广告的满意度，这将为广告商带来额外的收入。满意度是用户特定环境状态的函数，在决策时是未知的。然而，使用机器学习算法，我们可以根据用户的最近网页访问历史和其他我们可以收集的信息（如地理位置、浏览器代理字符串、一天中的时间、广告类别等）来预测这种状态（参见图2-2）。
- en: 'While we are unlikely to measure the level of dopamine in the user''s brain,
    which will certainly fall under the realm of measurable metrics and probably reduce
    the uncertainty, we can measure the user satisfaction indirectly by the user''s
    actions, either the fact that they responded to the ad or even the measure of
    time the user spent between the clicks to browse relevant information, which can
    be used to estimate the effectiveness of our modeling and algorithms. Here is
    an influence diagram, similar to the one for "vacation", adjusted for the advertising
    decision-making process:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们不太可能测量用户大脑中的多巴胺水平，这肯定会落入可测量指标的范畴，并可能减少不确定性，但我们可以通过用户的行为间接测量用户满意度，无论是他们是否对广告做出了回应，还是用户在点击浏览相关信息之间花费的时间，这可以用来估计我们建模和算法的有效性。以下是一个影响图，类似于“假期”的影响图，调整用于广告决策过程：
- en: '![Influence diagrams](img/image01636.jpeg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![影响图](img/image01636.jpeg)'
- en: Figure 02-2\. The vacation influence diagram adjusted to the online advertising
    decision-making case. The decisions for online advertising can be made thousand
    times per second
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 02-2\. 调整后的在线广告决策案例的假期影响图。在线广告的决策可以每秒进行数千次
- en: The actual process might be more complex, representing a chain of decisions,
    each one depending on a few previous time slices. For example, the so-called **Markov
    Chain Decision Process**. In this case, the diagram might be repeated over multiple
    time slices.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 实际过程可能更复杂，代表一系列决策，每个决策都依赖于几个先前的时间切片。例如，所谓的**马尔可夫链决策过程**。在这种情况下，图表可能需要在多个时间切片上重复。
- en: Yet another example might be Enterprise Network Internet malware analytics system.
    In this case, we try to detect network connections indicative of either **command
    and control** (**C2**), lateral movement, or data exfiltration based on the analysis
    of network packets flowing through the enterprise switches. The goal is to minimize
    the potential impact of an outbreak with minimum impact on the functioning systems.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子可能是企业网络互联网恶意软件分析系统。在这种情况下，我们试图根据对企业交换机流经的网络数据包的分析来检测指示命令和控制（**C2**）、横向移动或数据泄露的网络连接。目标是最大限度地减少爆发对系统运行的最小影响。
- en: One of the decisions we might take is to reimage a subset of nodes or to at
    least isolate them. The data we collect may contain uncertainty—many benign software
    packages may send traffic in suspicious ways, and the models need to differentiate
    between them based on the risk and potential impact. One of the decisions in this
    specific case may be to collect additional information.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能做出的一个决定是重新映像一部分节点，或者至少将它们隔离。我们收集的数据可能包含不确定性——许多良性软件包可能会以可疑的方式发送流量，而模型需要根据风险和潜在影响来区分它们。在这个特定案例中，可能的一个决定是收集更多信息。
- en: I will leave it to the reader to map this and other potential business cases
    to the corresponding diagram as an exercise. Let's consider a more complex optimization
    problem now.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 我将把这个以及其他潜在的商业案例映射到相应的图表上作为练习留给读者。现在让我们考虑一个更复杂的优化问题。
- en: Sequential trials and dealing with risk
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 顺序试验和风险处理
- en: 'What if my preferences for making an extra few dollars outweigh the risk of
    losing the same amount? I will stop on why one''s preferences might be asymmetric
    in a little while in this section, and there is scientific evidence that this
    asymmetry is ingrained in our minds for evolutionary reasons, but you are right,
    I have to optimize the expected value of the asymmetric function of the parameterized
    utility now, as follows:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我为了多赚几美元而牺牲同样风险的偏好是什么？我将在本节稍后停止讨论为什么一个人的偏好可能是不对称的，并且有科学证据表明这种不对称性是受进化原因根植于我们心中的，但你是对的，我现在必须优化参数化效用函数的不对称函数的期望值，如下所示：
- en: '![Sequential trials and dealing with risk](img/image01637.jpeg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险处理](img/image01637.jpeg)'
- en: 'Why would an asymmetric function surface in the analysis? One example is repeated
    bets or re-investments, also known as the Kelly Criterion problem. Although originally,
    the Kelly Criterion was developed for a specific case of binary outcome as in
    a gambling machine and the optimization of the fraction of money to bet in each
    round (*A New Interpretation of Information Rate*, Bell System Technical Journal
    35 (4): 917–926, 1956), a more generic formulation as an re-investment problem
    involves a probabilistic distribution of possible returns.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '为什么分析中会出现非对称函数？一个例子是重复投注或再投资，也称为凯利公式问题。虽然最初，凯利公式是为赌博机等二元结果的具体情况开发的，用于优化每轮投注的资金比例（*《信息率的新解释》*，贝尔系统技术期刊
    35 (4): 917–926，1956），但作为一个更通用的再投资问题，它涉及到可能回报的概率分布。'
- en: 'The return over multiple bets is a product of individual return rates on each
    of the bets—the return rate is the ratio between the bankroll after the bet to
    the original bankroll before each individual bet, as follows:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 多次投注的回报是每次投注的个别回报率的乘积——回报率是投注后的资金与每次个别投注前的原始资金的比率，如下所示：
- en: '![Sequential trials and dealing with risk](img/image01638.jpeg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险管理](img/image01638.jpeg)'
- en: 'This does not help us much to optimize the total return as we don''t know how
    to optimize the product of *i.i.d*. random variables. However, we can convert
    the product to a sum using log transformation and apply the **central limit theorem**
    (**CLT**) to approximate the sum of *i.i.d*. variables (provided that the distribution
    of *r* *[i]* is subect to CLT conditions, for example, has a finite mean and variance),
    as follows:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们优化总回报帮助不大，因为我们不知道如何优化独立同分布（*i.i.d*）随机变量的乘积。然而，我们可以通过对数变换将乘积转换为和，并应用**中心极限定理**（**CLT**）来近似独立同分布变量的和（假设*r*的分布满足CLT条件，例如，具有有限的均值和方差），如下所示：
- en: '![Sequential trials and dealing with risk](img/image01639.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险管理](img/image01639.jpeg)'
- en: Thus, the cumulative result of making *N* bets would look like the result of
    making *N* bets with expected return of ![Sequential trials and dealing with risk](img/image01640.jpeg),
    and not ![Sequential trials and dealing with risk](img/image01641.jpeg)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，进行*N*次投注的累积结果将类似于进行*N*次投注，期望回报为![顺序试验和风险管理](img/image01640.jpeg)，而不是![顺序试验和风险管理](img/image01641.jpeg)
- en: 'As I mentioned before, the problem is most often applied for the case of binary
    bidding, although it can be easily generalized, in which case there is an additional
    parameter: *x*, the amount of money to bid in each round. Let''s say I make a
    profit of *W* with probability *p* or completely lose my bet otherwise with the
    probability *(1-p)*. Optimizing the expected return with respect to the following
    additional parameter:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，这个问题最常应用于二元竞标的情况，尽管它可以很容易地推广，在这种情况下，还有一个额外的参数：*x*，即每轮投注的金额。假设我以概率*p*获得*W*的利润，或者以概率*(1-p)*完全输掉投注。优化与以下额外参数相关的期望回报：
- en: '![Sequential trials and dealing with risk](img/image01642.jpeg)![Sequential
    trials and dealing with risk](img/image01643.jpeg)![Sequential trials and dealing
    with risk](img/image01644.jpeg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险管理](img/image01642.jpeg)![顺序试验和风险管理](img/image01643.jpeg)![顺序试验和风险管理](img/image01644.jpeg)'
- en: The last equation is the Kelly Criterion ratio and gives you the optimal amount
    to bet.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个方程是凯利公式比率，它给出了最佳投注金额。
- en: 'The reason that one might bet less than the total amount is that even if the
    average return is positive, there is still a possibility to lose the whole bankroll,
    particularly, in highly skewed situations. For example, even if the probability
    of making *10 x* on your bet is *0.105* (*W = 10*, the expected return is *5%)*,
    the combinatorial analysis show that even after *60* bets, there is roughly a
    *50%* chance that the overall return will be negative, and there is an *11%* chance,
    in particular, of losing *(57 - 10 x 3) = 27* times your bet or more:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个人可能会投注少于总金额的原因是，即使平均回报是正的，仍然有可能输掉全部资金，尤其是在高度偏斜的情况下。例如，即使你投注获得*10 x*的概率是*0.105*（*W
    = 10*，期望回报是*5%*），组合分析表明，即使经过*60*次投注，整体回报为负的概率大约为*50*%，而且有*11*%的几率，特别是会输掉*(57 -
    10 x 3) = 27*倍或更多的投注：
- en: '[PRE12]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Note that to recover the *27 x* amount, one would need to play only ![Sequential
    trials and dealing with risk](img/image01645.jpeg) additional rounds on average
    with these favourable odds, but one must have something to bet to start with.
    The Kelly Criterion provides that the optimal is to bet only *1.55%* of our bankroll.
    Note that if I bet the whole bankroll, I would lose all my money with 89.5% certainty
    in the first round (the probability of a win is only *0.105*). If I bet only a
    fraction of the bankroll, the chances of staying in the game are infinitely better,
    but the overall returns are smaller. The plot of expected log of return is shown
    in *Figure 02-3* as a function of the portions of the bankroll to bet, *x*, and
    possible distribution of outcomes in 60 bets that I just computed. In 24% of the
    games we''ll do worse than the lower curve, in 39% worse than the next curve,
    in about half—44%—a gambler we''ll do the same or better than the black curve
    in the middle, and in 30% of cases better than the top one. The optimal Kelly
    Criterion value for *x* is *0.0155*, which will eventually optimize the overall
    return over infinitely many rounds:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，为了恢复 *27 x* 的金额，平均只需要再玩 ![顺序试验和风险处理](img/image01645.jpeg) 轮额外的游戏，但必须先有赌注才能开始。凯利公式提供的是，最佳策略是只投注我们赌注的
    *1.55%*。注意，如果我投注全部赌注，我将在第一轮（获胜的概率仅为 *0.105*）以 89.5% 的确定性输掉所有钱。如果我只投注赌注的一小部分，留在游戏中的机会将无限好，但整体回报较小。期望对数收益的图示如
    *图 02-3* 所示，作为投注赌注的份额 *x* 和我刚刚计算的 60 轮可能结果分布的函数。在 24% 的游戏中，我们的表现将不如下方的曲线，在 39%
    的游戏中不如下一条曲线，大约一半——44%——的赌徒的表现将与中间的黑曲线相同或更好，而在 30% 的情况下表现将优于最上方的一条。对于 *x* 的最佳凯利公式值为
    *0.0155*，这将最终在无限多轮中优化整体回报：
- en: '![Sequential trials and dealing with risk](img/image01646.jpeg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险处理](img/image01646.jpeg)'
- en: Figure 02-3\. The expected log of return as a function of the bet amount and
    possible outcomes in 60 rounds (see equation (2.2))
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图 02-3. 作为投注金额和 60 轮可能结果函数的期望对数收益（见方程（2.2））
- en: The Kelly Criterion has been criticized for being both too aggressive (gamblers
    tend to overestimate their winning potential/ratio and underestimate the probability
    of a ruin), as well as for being too conservative (the value at risk should be
    the total available capital, not just the bankroll), but it demonstrates one of
    the examples where we need to compensate our intuitive understanding of the "benefit"
    with some additional transformations.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 凯利公式因其过于激进（赌徒倾向于高估他们的获胜潜力和比例，同时低估破产的概率）以及过于保守（风险价值应该是总可用资本，而不仅仅是赌注）而受到批评，但它展示了我们需要用一些额外的转换来补偿我们对“收益”的直观理解的一个例子。
- en: 'From the financial point of view, the Kelly Criterion is a much better description
    of risk than the standard definition as volatility or variance of the returns.
    For a generic parametrized payoff distribution, *y(z)*, with a probability distribution
    function, *f(z)*, the equation (2.3) can be reformulated as follows. after the
    substitution *r(x) = 1 + x y(z)*, where *x* is still the amount to bet:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从金融角度来看，凯利公式比标准定义（如收益的波动性或方差）更好地描述了风险。对于一个通用的参数化收益分布 *y(z)*，其概率分布函数为 *f(z)*，方程（2.3）可以重新表述如下。在替换
    *r(x) = 1 + x y(z)* 后，其中 *x* 仍然是投注的金额：
- en: '![Sequential trials and dealing with risk](img/image01647.jpeg)![Sequential
    trials and dealing with risk](img/image01648.jpeg)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险处理](img/image01647.jpeg)![顺序试验和风险处理](img/image01648.jpeg)'
- en: 'It can also be written in the following manner in the discrete case:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在离散情况下，它也可以写成以下形式：
- en: '![Sequential trials and dealing with risk](img/image01649.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![顺序试验和风险处理](img/image01649.jpeg)'
- en: Here, the denominator emphasizes the contributions from the regions with negative
    payoffs. Specifically, the possibility of losing all your bankroll is exactly
    where the denominator ![Sequential trials and dealing with risk](img/image01650.jpeg)
    is zero.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，分母强调了负收益区域的贡献。具体来说，失去所有赌注的可能性正好是分母 ![顺序试验和风险处理](img/image01650.jpeg) 为零的地方。
- en: As I mentioned before, interestingly, risk aversion is engrained in our intuitions
    and there seems to be a natural risk-aversion system of preferences encoded in
    both humans and primates (*A Monkey Economy as Irrational as Ours* by Laurie Santos,
    TED talk, 2010). Now enough about monkeys and risk, let's get into another rather
    controversial subject—the exploration-exploitation trade-off, where one might
    not even know the payoff trade-offs initially.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，有趣的是，风险规避植根于我们的直觉，似乎在人类和灵长类动物中编码了一个自然的风险规避偏好系统（参见Laurie Santos的《像我们一样不理性的猴子经济》，TED演讲，2010年）。现在关于猴子和风险的话题就足够了，让我们进入另一个相当有争议的主题——探索与利用的权衡，在这个权衡中，一个人甚至可能一开始都不知道收益权衡。
- en: Exploration and exploitation
  id: totrans-157
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索与利用
- en: 'The exploration-exploitation trade-off is another problem that has its apparent
    origin within gambling, even though the real applications range from allocation
    of funding to research projects to self-driving cars. The traditional formulation
    is a multi-armed bandit problem, which refers to an imaginary slot machine with
    one or more arms. Sequential plays of each arm generate *i.i.d* `.` returns with
    unknown probabilities for each arm; the successive plays are independent in the
    simplified models. The rewards are assumed to be independent across the arms.
    The goal is to maximize the reward—for example, the amount of money won, and to
    minimize the learning loss, or the amount spend on the arms with less than optimal
    winning rate, provided an agreed upon arm selection policy. The obvious trade-off
    is between the **exploration** in search of an arm that produces the best return
    and **exploitation** of the best-known arm with optimal return:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 探索与利用的权衡是另一个问题，尽管其真实应用范围从研究项目的资金分配到自动驾驶汽车，但其明显的起源在赌博中。传统的表述是多臂老虎机问题，它指的是一个或多个臂的虚拟老虎机。每个臂的连续操作产生*i.i.d*的回报，每个臂的回报概率未知；在简化模型中，连续操作是独立的。假设奖励在臂之间是独立的。目标是最大化奖励——例如，赢得的金额，并最小化学习损失，即花费在获胜率低于最优的臂上的金额，前提是有一个商定的臂选择策略。明显的权衡是在寻找产生最佳回报的臂的**探索**和利用已知最佳回报的**利用**之间：
- en: '![Exploration and exploitation](img/image01651.jpeg)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![探索与利用](img/image01651.jpeg)'
- en: 'The **pseudo-regret** is then the difference:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，**伪后悔**是以下差值：
- en: '![Exploration and exploitation](img/image01652.jpeg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![探索与利用](img/image01652.jpeg)'
- en: Here, ![Exploration and exploitation](img/image01653.jpeg) is the *i^(th)* arm
    selection out of *N* trials. The multi-armed bandit problem was extensively studied
    in the 1930s and again during the early 2000s, with the application in finance
    and ADTECH. While in general, due to stochastic nature of the problem, it is not
    possible to provide a bound on the expected regret better than the square root
    of *N*, the pseudo-regret can be controlled so that we are able to bound it by
    a log of *N* (*Regret Analysis of Stochastic and Nonstochastic Multi-armed Bandit
    Problems* by Sebastien Bubeck and Nicolo Cesa-Bianchi, [http://arxiv.org/pdf/1204.5721.pdf](http://arxiv.org/pdf/1204.5721.pdf)).
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![探索与利用](img/image01653.jpeg)是从*N*次试验中选出的*i*臂。多臂老虎机问题在20世纪30年代和21世纪初都得到了广泛的研究，其应用领域包括金融和ADTECH。尽管由于问题的随机性，通常无法提供一个比*N*的平方根更好的期望后悔的上界，但可以通过控制伪后悔来使其受到*N*的对数的约束（参见Sebastien
    Bubeck和Nicolo Cesa-Bianchi的论文《随机和非随机多臂老虎机问题的后悔分析》，[http://arxiv.org/pdf/1204.5721.pdf](http://arxiv.org/pdf/1204.5721.pdf)）。
- en: One of the most common strategies used in practice is epsilon strategies, where
    the optimal arm is chosen with the probability of ![Exploration and exploitation](img/image01654.jpeg)
    and one of the other arms with the remaining probability. The drawback of this
    approach is that we might spend a lot of exploration resources on the arms that
    are never going to provide any rewards. The UCB strategy improves the epsilon
    strategy by choosing an arm with the largest estimate of the return, plus some
    multiple or fraction of the standard deviation of the return estimates. The approach
    needs the recomputation of the best arm to pull at each round and suffers from
    approximations made to estimate the mean and standard deviation. Besides, UCB
    requires the recomputation of the estimates for each successive pull, which might
    be a scalability problem.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 实践中最常用的策略之一是ε策略，其中最优臂以![探索与利用](img/image01654.jpeg)的概率被选中，而其他臂则以剩余的概率被选中。这种方法的缺点是我们可能会在永远不会提供任何奖励的臂上花费大量的探索资源。UCB策略通过选择具有最大回报估计值的臂，以及回报估计值的标准差的某个倍数或分数来改进ε策略。这种方法需要在每一轮重新计算最佳臂，并且由于对均值和标准差的估计所做的近似而受到影响。此外，UCB需要为每次连续抽取重新计算估计值，这可能会成为可扩展性问题。
- en: 'Finally, the Thompson sampling strategy uses a fixed random sample from Beta-Bernoulli
    posterior estimates and assigns the next arm to the one that gives the minimal
    expected regret, for which real data can be used to avoid parameter recomputation.
    Although the specific numbers may depend on the assumptions, one available comparison
    for these model performances is provided in the following diagram:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Thompson抽样策略使用Beta-Bernoulli后验估计的固定随机样本，并将下一个臂分配给给出最小预期后悔的臂，为此可以使用实际数据来避免参数重新计算。尽管具体数字可能取决于假设，但以下图表提供了这些模型性能的一个可用比较：
- en: '![Exploration and exploitation](img/image01655.jpeg)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![探索与利用](img/image01655.jpeg)'
- en: Figure 02-3\. The simulation results for different exploration exploitation
    strategies for K = 5, one-armed bandits, and different strategies.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图02-3。对于K = 5，单臂老虎机和不同策略的不同探索/利用策略的模拟结果。
- en: '*Figure 02-3* shows simulation results for different strategies (taken from
    the Rich Relevance website at [http://engineering.richrelevance.com/recommendations-thompson-sampling](http://engineering.richrelevance.com/recommendations-thompson-sampling)).
    The **Random** strategy just allocates the arms at random and corresponds to pure
    exploration. The **Naive** strategy is random up to a certain threshold and than
    switches to pure Exxploitation mode. **Upper Confidence Bound** (**UCB**) with
    95% confidence level. UCB1 is a modification of UCB to take into account the log-normality
    of the distributions. Finally the Thompson sampling strategy makes a random sample
    from actual posterior distribution to optimize the regret.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '*图02-3*显示了不同策略的模拟结果（摘自Rich Relevance网站[http://engineering.richrelevance.com/recommendations-thompson-sampling](http://engineering.richrelevance.com/recommendations-thompson-sampling))。**随机**策略随机分配臂，对应于纯探索。**天真**策略在某个阈值内是随机的，然后切换到纯利用模式。**上置信界**（**UCB**）以95%的置信水平。UCB1是UCB的一种修改，以考虑分布的对数正态性。最后，Thompson抽样策略从实际后验分布中随机抽取样本以优化后悔。'
- en: Exploration/exploitation models are known to be very sensitive to the initial
    conditions and outliers, particularly on the low-response side. One can spend
    enormous amount of trials on the arms that are essentially dead.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 探索/利用模型众所周知对初始条件和异常值非常敏感，尤其是在低响应方面。人们可能会在实际上已经无望的臂上花费大量的试验。
- en: Other improvements on the strategies are possible by estimating better priors
    based on additional information, such as location, or limiting the set of arms
    to explore—*K*—due to such additional information, but these aspects are more
    domain-specific (such as personalization or online advertising).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 通过基于额外信息（如位置）估计更好的先验，或者由于这种额外信息而限制要探索的臂的集合—*K*—，可以对这些策略进行其他改进，但这些方面更具有领域特定性（如个性化或在线广告）。
- en: Unknown unknowns
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 未知之未知
- en: 'Unknown unknowns have been largely made famous due to a phrase from a response
    the United States Secretary of Defense, Donald Rumsfeld, gave to a question at
    a United States **Department of Defense** (**DoD**) news briefing on February
    12, 2002 about the lack of evidence linking the government of Iraq with the supply
    of weapons of mass destruction to terrorist groups, and books by Nassim Taleb
    (*The Black Swan: The Impact of the Highly Improbable* by Nassim Taleb, Random
    House, 2007).'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 未知未知的事物在很大程度上是因为美国国防部长唐纳德·拉姆斯菲尔德在2002年2月12日的一次美国国防部（**DoD**）新闻发布会上对关于伊拉克政府与向恐怖组织供应大规模杀伤性武器缺乏证据的提问的回答而闻名，以及纳西姆·尼古拉斯·塔勒布的书籍（《黑天鹅：几乎不可能发生的事件的影响》由纳西姆·尼古拉斯·塔勒布著，Random
    House出版社，2007年）。
- en: Note
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Turkey paradox**'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '**火鸡悖论**'
- en: Arguably, the unknown unknown is better explained by the turkey paradox. Suppose
    you have a family of turkeys playing in the backyard and enjoying protection and
    free food. Across the fence, there is another family of turkeys. This all works
    day after day, and month after month, until Thanksgiving comes—Thanksgiving Day
    is a national holiday celebrated in Canada and the United States, where it's customary
    to roast the turkeys in an oven. The turkeys are very likely to be harvested and
    consumed at this point, although from the turkey's point of view, there is no
    discernable signal that anything will happen on the second Monday of October in
    Canada and the fourth Thursday of November in the United States. No amount of
    modeling on the within-the-year data can fix this prediction problem from the
    turkey's point of view besides the additional year-over-year information.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 争议性地，未知未知的事物可以通过火鸡悖论更好地解释。假设你有一群火鸡在后院玩耍，享受保护和免费食物。栅栏的另一边，还有另一群火鸡。这一切日复一日，月复一月，直到感恩节到来——感恩节是加拿大和美国庆祝的全国性假日，在这一天，人们习惯于在烤箱里烤火鸡。火鸡很可能在这个时候被收割并消费，尽管从火鸡的角度来看，没有任何可辨别的信号表明在加拿大的10月第二个星期一或美国的11月第四个星期四会发生任何事情。除了额外的年度信息之外，没有任何模型可以在年内数据的基础上解决这个问题。
- en: The unknown unknown is something that is not in the model and cannot be anticipated
    to be in the model. In reality, the only unknown unknowns that are of interest
    are the ones that affect the model so significantly that the results that were
    previously virtually impossible, or possible with infinitesimal probability, now
    become the reality. Given that most of the practical distributions are from exponential
    family with really thin tails, the deviation from normal does not have to be more
    than a few sigmas to have devastating results on the standard model assumptions.
    While one has still to come up with an actionable strategy of how to include the
    unknown factors in the model—a few ways have been proposed, including fractals,
    but few if any are actionable—the practitioners have to be aware of the risks,
    and here the definition of the risk is exactly the possibility of delivering the
    models useless. Of course, the difference between the known unknown and unknown
    unknown is exactly that we understand the risks and what needs to be explored.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 未知未知的事物是模型中没有的，并且无法预测它们会在模型中。实际上，唯一真正感兴趣的未知未知的事物是那些对模型影响如此之大，以至于之前几乎不可能或几乎不可能发生的结果现在变成了现实。鉴于大多数实际分布都属于指数家族，尾部非常薄，因此，与正态分布的偏差不必超过几个标准差，就会对标准模型假设产生破坏性的影响。尽管人们仍然需要想出一个可行的策略来如何在模型中包含未知因素——已经提出了几种方法，包括分形，但几乎没有可行的——从业者必须意识到风险，而这里的定义风险正是模型无用的可能性。当然，已知未知和未知未知之间的区别正是我们理解风险和需要探索的内容。
- en: As we looked at the basic scope of problems that the decision-making systems
    are facing, let's look at the data pipelines, the software systems that provide
    information for making the decisions, and more practical aspects of designing
    the data pipeline for a data-driven system.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们审视决策系统面临的基本问题范围时，让我们来看看数据管道，这些软件系统为做出决策提供信息，以及设计数据驱动系统数据管道的更实际方面。
- en: Basic components of a data-driven system
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据驱动系统的基本组件
- en: 'In short, a data-driven architecture contains the following components—at least
    all the systems I''ve seen have them—or can be reduced to these components:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，数据驱动架构包含以下组件——至少我所见到的所有系统都有这些组件——或者可以简化为这些组件：
- en: '**Data ingest**: We need to collect the data from systems and devices. Most
    of the systems have logs, or at least an option to write files into a local filesystem.
    Some can have capabilities to report information to network-based interfaces such
    as syslog, but the absence of persistence layer usually means potential data loss,
    if not absence of audit information.'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据采集**：我们需要从系统和设备中收集数据。大多数系统都有日志，或者至少有一个将文件写入本地文件系统的选项。一些系统可能具有将信息报告给基于网络的接口（如syslog）的能力，但通常没有持久化层意味着可能存在数据丢失的风险，如果不是审计信息缺失的话。'
- en: '**Data transformation layer**: It was also historically called **extract, transform,
    and load** (**ETL**). Today the data transformation layer can also be used to
    have real-time processing, where the aggregates are computed on the most recent
    data. The data transformation layer is also traditionally used to reformat and
    index the data to be efficiently accessed by a UI component of algorithms down
    the pipeline.'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据转换层**：它也被称为**提取、转换和加载**（**ETL**）。今天，数据转换层也可以用于实时处理，其中聚合是在最新数据上计算的。数据转换层也传统上用于重新格式化和索引数据，以便高效地由管道下游的算法的UI组件访问。'
- en: '**Data analytics and machine learning engine**: The reason this is not part
    of the standard data transformation layer is usually that this layer requires
    quite different skills. The mindset of people who build reasonable statistical
    models is usually different from people who make terabytes of data move fast,
    even though occasionally I can find people with both skills. Usually, these unicorns
    are called data scientists, but the skills in any specific field are usually inferior
    to ones who specialize in a particular field. We need more of either, though.
    Another reason is that machine learning, and to a certain extent, data analysis,
    requires multiple aggregations and passes over the same data, which as opposed
    to a more stream-like ETL transformations, requires a different engine.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据分析与机器学习引擎**：这个层次不是标准数据转换层的一部分的原因通常是因为这个层次需要相当不同的技能。构建合理统计模型的人的心态通常与那些使数以TB计的数据快速移动的人不同，尽管偶尔我也能找到具备这两种技能的人。通常，这些“独角兽”被称为数据科学家，但任何特定领域的技能通常都不如专门从事该领域的人。尽管如此，我们仍然需要更多这样的人。另一个原因是，机器学习，以及在某种程度上数据分析，需要多次对相同数据进行聚合和遍历，这与更流式的ETL转换不同，需要不同的引擎。'
- en: '**UI component**: Yes, UI stands for user interface, which most often is a
    set of components that allow you to communicate with the system via a browser
    (it used to be a native GUI, but these days the web-based JavaScript or Scala-based
    frameworks are much more powerful and portable). From the data pipeline and modeling
    perspective, this component offers an API to access internal representation of
    data and models.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**UI组件**：是的，UI代表用户界面，它通常是一组组件，允许您通过浏览器与系统通信（它曾经是一个本地的GUI，但如今基于Web的JavaScript或Scala框架要强大得多，并且更易于移植）。从数据管道和建模的角度来看，该组件提供了一个API来访问数据和模型的内部表示。'
- en: '**Actions engine**: This is usually a configurable rules engine to optimize
    the provided metrics based on insights. The actions may be either real-time, like
    in online advertising, in which case the engine should be able to supply real-time
    scoring information, or a recommendation for a user action, which can take the
    form of an e-mail alert.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动作引擎**：这通常是一个可配置的规则引擎，根据洞察力优化提供的指标。动作可以是实时的，例如在线广告中的情况，在这种情况下，引擎应该能够提供实时评分信息，或者为用户动作提供推荐，这可能采取电子邮件警报的形式。'
- en: '**Correlation engine**: This is an emerging component that may analyze the
    output of data analysis and machine learning engine to infer additional insights
    into data or model behavior. The actions might also be triggered by an output
    from this layer.'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关联引擎**：这是一个新兴的组件，它可能分析数据分析与机器学习引擎的输出，以推断数据或模型行为方面的额外见解。这些动作也可能由该层的输出触发。'
- en: '**Monitoring**: This is a complex system will be incomplete without logging,
    monitoring, and some way to change system parameters. The purpose of monitoring
    is to have a nested decision-making system regarding the optimal health of the
    system and either to mitigate the problem(*s*) automatically or to alert the system
    administrators about the problem(*s*).'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监控**：这是一个复杂的系统，如果没有日志、监控以及某种方式来更改系统参数，它将是不完整的。监控的目的是拥有一个嵌套的决策系统，关于系统的最佳健康状况，要么自动减轻问题（*s*），要么向系统管理员发出关于问题（*s*）的警报。'
- en: Let's discuss each of the components in detail in the following sections.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在以下各节中详细讨论每个组件。
- en: Data ingest
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据摄取
- en: With the proliferation of smart devices, information gathering has become less
    of a problem and more of a necessity for any business that does more than a type-written
    text. For the purpose of this chapter, I will assume that the device or devices
    are connected to the Internet or have some way of passing this information via
    home dialing or direct network connection.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 随着智能设备的普及，信息收集不再是问题，而是任何从事除打字文本之外业务的企业的一种必要需求。为了本章的目的，我将假设设备或设备已连接到互联网或以某种方式通过家庭拨号或直接网络连接传递此信息。
- en: 'The major purpose of this component is to collect all relevant information
    that can be relevant for further data-driven decision making. The following table
    provides details on the most common implementations of the data ingest:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 此组件的主要目的是收集所有可能对后续数据驱动决策相关的相关信息。以下表格提供了关于数据摄取最常见实现的详细信息：
- en: '| Framework | When used | Comments |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 当使用 | 评论 |'
- en: '| --- | --- | --- |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Syslog** | Syslog is one of the most common standards to pass messages
    between the machines on Unix. Syslog usually listens on port 514 and the transport
    protocol can be configured either with UDP (unreliable) or with TCP. The latest
    enhanced implementation on CentOS and Red Hat Linux is rsyslog, which includes
    many advanced options such as regex-based filtering that is useful for system-performance
    tuning and debugging. Apart from slightly inefficient raw message representation—plain
    text, which might be inefficient for long messages with repeated strings—the syslog
    system can support tens of thousands of messages per second. | Syslog is one of
    the oldest protocols developed in the 1980s by Eric Allman as part of Sendmail.
    While it does not guarantee delivery or durability, particularly for distributed
    systems, it is one of the most widespread protocols for message passing. Some
    of the later frameworks, such as Flume and Kafka, have syslog interfaces as well.
    |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| **Syslog** | Syslog 是 Unix 机器之间传递消息的最常见标准之一。Syslog 通常监听端口 514，传输协议可以配置为 UDP（不可靠）或
    TCP。在 CentOS 和 Red Hat Linux 上的最新增强实现是 rsyslog，它包括许多高级选项，如基于正则表达式的过滤，这对于系统性能调整和调试非常有用。除了略微低效的原始消息表示——纯文本，对于重复字符串的长消息可能效率不高——syslog
    系统可以每秒支持数万条消息。Syslog 是由 Eric Allman 在 1980 年代作为 Sendmail 的一部分开发的最早协议之一。虽然它不保证交付或持久性，尤其是对于分布式系统，但它是最广泛的消息传递协议之一。一些后来的框架，如
    Flume 和 Kafka，也有 syslog 接口。|'
- en: '| **Rsync** | Rsync is a younger framework developed in the 1990s. If the data
    is put in the flat files on a local filesystem, rsync might be an option. While
    rsync is more traditionally used to synchronize two directories, it also can be
    run periodically to transfer log data in batches. Rsync uses a recursive algorithm
    invented by an Australian computer programmer, Andrew Tridgell, for efficiently
    detecting the differences and transmitting a structure (such as a file) across
    a communication link when the receiving computer already has a similar, but not
    identical, version of the same structure. While it incurs extra communication,
    it is better from the point of durability, as the original copy can always be
    retrieved. It is particularly appropriate if the log data is known to arrive in
    batches in the first place (such as uploads or downloads). | Rsync has been known
    to be hampered by network bottlenecks, as it ultimately passes more information
    over the network when comparing the directory structures. However, the transferred
    files may be compressed when passed over the network. The network bandwidth can
    be limited per command-line flags. |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| **Rsync** | Rsync 是一个在 1990 年代开发的较新的框架。如果数据被放置在本地文件系统上的平面文件中，rsync 可能是一个选择。虽然
    rsync 传统上用于同步两个目录，但它也可以定期运行以批量传输日志数据。Rsync 使用由澳大利亚计算机程序员 Andrew Tridgell 发明的递归算法，在接收计算机已经有一个类似但不完全相同的相同结构版本的情况下，高效地检测差异并跨通信链路传输结构（如文件）。虽然它会产生额外的通信，但从持久性的角度来看，它更好，因为原始副本始终可以检索。如果已知日志数据最初是以批量形式到达的（例如上传或下载），则特别适用。Rsync
    已知会受到网络瓶颈的限制，因为它在比较目录结构时最终会在网络上传递更多信息。然而，传输的文件在网络传输时可能会被压缩。网络带宽可以通过命令行标志进行限制。|'
- en: '| **Flume** | Flume is one of the youngest frameworks developed by Cloudera
    in 2009-2011 and open sourced. Flume—we refer to the more popular flume-ng implementation
    as Flume as opposed to an older regular Flume—consists of sources, pipes, and
    sinks that may be configured on multiple nodes for high availability and redundancy
    purposes. Flume was designed to err on the reliability side at the expense of
    possible duplication of data. Flume passes the messages in the **Avro** format,
    which is also open sourced and the transfer protocol, as well as messages can
    be encoded and compressed. | While Flume originally was developed just to ship
    records from a file or a set of files, it can also be configured to listen to
    a port, or even grab the records from a database. Flume has multiple adapters
    including the preceding syslog. |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **Flume** | Flume是Cloudera在2009-2011年间开发的一个较年轻的框架，并已开源。Flume——我们指的是更流行的flume-ng实现，称为Flume，而不是较老的常规Flume——由源、管道和可能配置在多个节点上的汇组成，以实现高可用性和冗余。Flume被设计为在可靠性的代价下尽可能避免数据重复。Flume以**Avro**格式传递消息，该格式也是开源的，传输协议以及消息都可以进行编码和压缩。|
    虽然Flume最初是为了从文件或一组文件中传输记录而开发的，但它也可以配置为监听端口，甚至从数据库中抓取记录。Flume有多个适配器，包括前面的syslog。|'
- en: '| **Kafka** | Kafka is the latest addition to the log-processing framework
    developed by LinkedIn and is open sourced. Kafka, compared to the previous frameworks,
    is more like a distributed reliable message queue. Kafka keeps a partitioned,
    potentially between multiple distributed machines; buffer and one can subscribe
    to or unsubscribe from getting messages for a particular topic. Kafka was built
    with strong reliability guarantees in mind, which is achieved through replication
    and consensus protocol. | Kafka might not be appropriate for small systems (<
    five nodes) as the benefits of the fully distributed system might be evident only
    at larger scales. Kafka is commercially supported by Confluent. |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| **Kafka** | Kafka是LinkedIn开发的日志处理框架的最新补充，并已开源。与之前的框架相比，Kafka更像是一个分布式可靠的消息队列。Kafka保持分区，可能分布在多个分布式机器上；缓冲区，并且可以订阅或取消订阅特定主题的消息。Kafka在设计时考虑了强大的可靠性保证，这是通过复制和共识协议实现的。|
    Kafka可能不适合小型系统（小于五个节点），因为完全分布式系统的优势可能只有在更大规模时才明显。Kafka由Confluent提供商业支持。|'
- en: The transfer of information usually occurs in batches, or micro batches if the
    requirements are close to real time. Usually the information first ends up in
    a file, traditionally called log, in a device's local filesystem, and then is
    transferred to a central location. Recently developed Kafka and Flume are often
    used to manage these transfers, together with a more traditional syslog, rsync,
    or netcat. Finally, the data can be placed into a local or distributed storage
    such as HDFS, Cassandra, or Amazon S3.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 信息传输通常以批量或微批量的形式进行，如果需求接近实时，则可能为微批量。通常，信息首先存储在设备本地文件系统中的一个文件中，传统上称为日志文件，然后传输到中央位置。最近开发的Kafka和Flume常用于管理这些传输，同时还有更传统的syslog、rsync或netcat。最后，数据可以存储在本地或分布式存储中，如HDFS、Cassandra或Amazon
    S3。
- en: Data transformation layer
  id: totrans-197
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据转换层
- en: 'After the data ends up in HDFS or other storage, the data needs to be made
    available for processing. Traditionally, the data is processed on a schedule and
    ends up partitioned by time-based buckets. The processing can happen daily or
    hourly, or even on a sub-minute basis with the new Scala streaming framework,
    depending on the latency requirements. The processing may involve some preliminary
    feature construction or vectorization, even though it is traditionally considered
    a machine-learning task. The following table summarizes some available frameworks:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 数据最终存储在HDFS或其他存储中后，需要使数据可用于处理。传统上，数据按计划处理，并最终按基于时间的桶进行分区。处理可以按日、按小时，甚至在新型的Scala流框架的基础上按亚分钟级进行，具体取决于延迟要求。处理可能涉及一些初步的特征构造或矢量化，尽管它传统上被认为是机器学习任务。以下表格总结了一些可用的框架：
- en: '| Framework | When used | Comments |'
  id: totrans-199
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 使用情况 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Oozie** | This is one of the oldest open source frameworks developed by
    Yahoo. This has good integration with big data Hadoop tools. It has limited UI
    that lists the job history. | The whole workflow is put into one big XML file,
    which might be considered a disadvantage from the modularity point of view. |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| **Oozie** | 这是雅虎（Yahoo）开发的最古老的开放源代码框架之一。它与大数据Hadoop工具具有良好的集成。它具有有限的用户界面，列出了作业历史。|
    整个工作流被放入一个大的XML文件中，这可能从模块化的角度来看被认为是一个缺点。|'
- en: '| **Azkaban** | This is an alternative open source workflow-scheduling framework
    developed by LinkedIn. Compared to Oozie, this arguably has a better UI. The disadvantage
    is that all high-level tasks are executed locally, which might present a scalability
    problem. | The idea behind Azkaban is to create a fully modularized drop-in architecture
    where the new jobs/tasks can be added with as few modifications as possible. |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| **Azkaban** | 这是由领英（LinkedIn）开发的一个替代开源工作流调度框架。与Oozie相比，它可能具有更好的用户界面。缺点是所有高级任务都在本地执行，这可能会带来可扩展性问题。Azkaban背后的理念是创建一个完全模块化的即插即用架构，其中新作业/任务可以尽可能少地修改后添加。|'
- en: '| **StreamSets** | StreamSets is the latest addition build by the former Informix
    and Cloudera developers. It has a very developed UI and supports a much richer
    set of input sources and output destinations. | This is a fully UI-driven tool
    with an emphasis on data curation, for example, constantly monitoring the data
    stream for problems and abnormalities. |'
  id: totrans-203
  prefs: []
  type: TYPE_TB
  zh: '| **StreamSets** | StreamSets是由前Informix和Cloudera的开发者最新构建的。它具有非常发达的用户界面，并支持更丰富的输入源和输出目标。|
    这是一个完全由用户界面驱动的工具，强调数据管理，例如，持续监控数据流中的问题和异常。|'
- en: Separate attention should be given to stream-processing frameworks, where the
    latency requirements are reduced to one or a few records at a time. First, stream
    processing usually requires much more resources dedicated to processing, as it
    is more expensive to process individual records at a time as opposed to batches
    of records, even if it is tens or hundreds of records. So, the architect needs
    to justify the additional costs based on the value of more recent result, which
    is not always warranted. Second, stream processing requires a few adjustments
    to the architecture as handling the more recent data becomes a priority; for example,
    a delta architecture where the more recent data is handled by a separate substream
    or a set of nodes became very popular recently with systems such as **Druid**
    ([http://druid.io](http://druid.io)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 应当特别关注流处理框架，其中延迟需求降低到每次一个或几个记录。首先，流处理通常需要更多资源用于处理，因为与处理记录批次相比，每次处理单个记录的成本更高，即使只有几十或几百条记录也是如此。因此，架构师需要根据更近期结果的价值来证明额外成本是合理的，而这种价值并不总是有保证的。其次，流处理需要对架构进行一些调整，因为处理更近期数据成为优先事项；例如，最近在像**Druid**（[http://druid.io](http://druid.io)）这样的系统中，一个处理更近期数据的独立子流或节点集的delta架构变得非常流行。|
- en: Data analytics and machine learning
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据分析和机器学习
- en: For the purpose of this chapter, **Machine Learning** (**ML**) is any algorithm
    that can compute aggregates or summaries that are actionable. We will cover more
    complex algorithms from [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib* to [Chapter 6](part0273.xhtml#aid-84B9I2
    "Chapter 6. Working with Unstructured Data"), *Working with Unstructured Data*,
    but in some cases, a simple sliding-window average and deviation from the average
    may be sufficient signal for taking an action. In the past few years, it just
    works in A/B testing somehow became a convincing argument for model building and
    deployment. I am not speculating that solid scientific principles might or might
    not apply, but many fundamental assumptions such as *i.i.d.*, balanced designs,
    and the thinness of the tail just fail to hold for many big data situation. Simpler
    models tend to be faster and to have better performance and stability.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，**机器学习**（**ML**）是指任何可以计算可操作聚合或摘要的算法。我们将从[第3章](part0249.xhtml#aid-7DES21
    "第3章。使用Spark和MLlib") *使用Spark和MLlib* 到[第6章](part0273.xhtml#aid-84B9I2 "第6章。使用非结构化数据")
    *使用非结构化数据*，涵盖更复杂的算法，但在某些情况下，一个简单的滑动窗口平均值和平均值偏差可能就足够作为采取行动的信号。在过去几年中，A/B测试中的“它有效”某种程度上成为模型构建和部署的有力论据。我并不是在猜测是否可能有或可能没有坚实的科学原理适用，但许多基本假设，如*i.i.d*、平衡设计和尾部稀薄性，在许多大数据情况下都未能成立。更简单的模型往往速度更快，性能和稳定性更好。|
- en: For example, in online advertising, one might just track average performance
    of a set of ads over a certain similar properties over times to make a decision
    whether to have this ad displayed. The information about anomalies, or deviation
    from the previous behavior, may signal a new unknown unknown, which signals that
    the old data no longer applies, in which case, the system has no choice but to
    start the new exploration cycle.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在在线广告中，人们可能会跟踪一组广告在一段时间内某些相似属性的平均性能，以决定是否显示该广告。关于异常或行为偏离的信息可能表明一个新未知的新情况，这表明旧数据不再适用，在这种情况下，系统别无选择，只能开始新的探索周期。
- en: I will talk about more complex non-structured, graph, and pattern mining later
    in [Chapter 6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured
    Data"), *Working with Unstructured Data*, [Chapter 8](part0288.xhtml#aid-8IL202
    "Chapter 8. Integrating Scala with R and Python"), *Integrating Scala with R and
    Python* and [Chapter 9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP in Scala"),
    *NLP in Scala*.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在[第6章](part0273.xhtml#aid-84B9I2 "第6章。处理非结构化数据")、*处理非结构化数据*、[第8章](part0288.xhtml#aid-8IL202
    "第8章。Scala与R和Python的集成")、*Scala与R和Python的集成*和[第9章](part0291.xhtml#aid-8LGJM2 "第9章。Scala中的NLP")、*Scala中的NLP*中更晚些时候讨论更复杂的非结构化、图和模式挖掘。
- en: UI component
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: UI组件
- en: Well, UI is for wimps! Just joking...maybe it's too harsh, but in reality, UI
    usually presents a syntactic sugar that is necessary to convince the population
    beyond the data scientists. A good analyst should probably be able to figure out
    t-test probabilities by just looking at a table with numbers.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，UI是给弱者的！只是开玩笑...也许有点严厉，但现实中，UI通常提供一种必要的语法糖，以说服数据科学家之外的人群。一个好的分析师可能只需通过查看数字表格就能找出t检验的概率。
- en: However, one should probably apply the same methodologies we used at the beginning
    of the chapter, assessing the usefulness of different components and the amount
    of cycles put into them. The presence of a good UI is often justified, but depends
    on the target audience.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，可能应该应用我们在本章开头使用的相同方法，评估不同组件的有用性和投入其中的周期数量。良好的用户界面通常是有理由的，但它取决于目标受众。
- en: 'First, there are a number of existing UIs and reporting frameworks. Unfortunately,
    most of them are not aligned with the functional programming methodologies. Also,
    the presence of complex/semi-structured data, which I will describe in [Chapter
    6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured Data"), *Working
    with Unstructured Data* in more detail, presents a new twist that many frameworks
    are not ready to deal with without implementing some kind of DSL. Here are a few
    frameworks for building the UI in a Scala project that I find particularly worthwhile:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，存在许多现有的UI和报告框架。不幸的是，其中大多数与函数式编程方法不一致。此外，复杂/半结构化数据的存在，我将在[第6章](part0273.xhtml#aid-84B9I2
    "第6章。处理非结构化数据")、*处理非结构化数据*中更详细地描述，为许多框架带来了新的挑战，它们在没有实现某种类型的领域特定语言（DSL）的情况下无法应对。以下是我认为特别有价值的几个用于在Scala项目中构建UI的框架：
- en: '| Framework | When used | Comments |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| 框架 | 当使用时 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Scala Swing** | If you used Swing components in Java and are proficient
    with them, Scala Swing is a good choice for you. Swing component is arguably the
    least portable component of Java, so your mileage can vary on different platforms.
    | The `Scala.swing` package uses the standard Java Swing library under the hood,
    but it has some nice additions. Most notably, as it''s made for Scala, it can
    be used in a much more concise way than the standard Swing. |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| **Scala Swing** | 如果你使用了Java中的Swing组件并且熟练掌握它们，Scala Swing是一个不错的选择。Swing组件可以说是Java中最不便携的组件，所以你在不同平台上的表现可能会有所不同。
    | `Scala.swing`包在底层使用标准的Java Swing库，但它有一些很好的补充。最值得注意的是，由于它是为Scala设计的，它可以比标准的Swing以更简洁的方式使用。
    |'
- en: '| **Lift** | Lift is a secure, developer-centric, scalable, and interactive
    framework written in Scala. Lift is open sourced under Apache 2.0 license. | The
    open source Lift framework was launched in 2007 by David Polak, who was dissatisfied
    with certain aspects of the Ruby on Rails framework. Any existing Java library
    and web container can be used in running Lift applications. Lift web applications
    are thus packaged as WAR files and deployed on any servlet 2.4 engine (for example,
    Tomcat 5.5.xx, Jetty 6.0, and so on). Lift programmers may use the standard Scala/Java
    development toolchain, including IDEs such as Eclipse, NetBeans, and IDEA. Dynamic
    web content is authored via templates using standard HTML5 or XHTML editors. Lift
    applications also benefit from native support for advanced web development techniques,
    such as Comet and Ajax. |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| **Lift** | Lift是一个安全、以开发者为中心、可扩展和交互式的框架，用Scala编写。Lift在Apache 2.0许可下开源。 |
    开源Lift框架于2007年由David Polak启动，他对Ruby on Rails框架的某些方面感到不满意。任何现有的Java库和Web容器都可以用于运行Lift应用程序。因此，Lift
    Web应用程序被打包为WAR文件，并部署在任何servlet 2.4引擎上（例如，Tomcat 5.5.xx、Jetty 6.0等）。Lift程序员可以使用标准的Scala/Java开发工具链，包括Eclipse、NetBeans和IDEA等IDE。动态Web内容通过模板使用标准的HTML5或XHTML编辑器编写。Lift应用程序还受益于对高级Web开发技术（如Comet和Ajax）的原生支持。
    |'
- en: '| **Play** | Play is arguably better aligned with Scala as a functional language
    than any other platform—it is officially supported by Typesafe, the commercial
    company behind Scala. The Play framework 2.0 builds on Scala, Akka, and sbt to
    deliver superior asynchronous request handling, fast and reliable. Typesafe templates,
    and a powerful build system with flexible deployment options. Play is open sourced
    under Apache 2.0 license. | The open source Play framework was created in 2007
    by Guillaume Bort, who sought to bring a fresh web development experience inspired
    by modern web frameworks like Ruby on Rails to the long-suffering Java web development
    community. Play follows a familiar stateless model-view-controller architectural
    pattern, with a philosophy of convention-over-configuration and an emphasis on
    developer productivity. Unlike traditional Java web frameworks with their tedious
    compile-package-deploy-restart cycles, updates to Play applications are instantly
    visible with a simple browser refresh. |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| **Play** | Play框架可以说是比任何其他平台都更符合Scala作为函数式语言的特点——它由Scala背后的商业公司Typesafe官方支持。Play框架2.0建立在Scala、Akka和sbt之上，提供卓越的异步请求处理、快速和可靠的性能。Typesafe模板以及一个功能强大的构建系统，具有灵活的部署选项。Play在Apache
    2.0许可下开源。 | 开源Play框架于2007年由Guillaume Bort创建，他希望为长期受苦的Java Web开发社区带来一个受现代Web框架如Ruby
    on Rails启发的全新Web开发体验。Play遵循熟悉的无状态模型-视图-控制器（MVC）架构模式，强调约定优于配置和开发者生产力。与传统的Java Web框架相比，它们有繁琐的编译-打包-部署-重启周期，Play应用程序的更新只需简单的浏览器刷新即可立即可见。
    |'
- en: '| **Dropwizard** | The dropwizard ([www.dropwizard.io](http://www.dropwizard.io))
    project is an attempt to build a generic RESTful framework in both Java and Scala,
    even though one might end up using more Java than Scala. What is nice about this
    framework is that it is flexible enough to be used with arbitrary complex data
    (including semi-structured).This is licensed under Apache License 2.0. | RESTful
    API assumes state, while functional languages shy away from using state. Unless
    you are flexible enough to deviate from a pure functional approach, this framework
    is probably not good enough for you. |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| **Dropwizard** | Dropwizard项目试图在Java和Scala中构建一个通用的RESTful框架，尽管最终可能使用Java多于Scala。这个框架的优点在于它足够灵活，可以用于任意复杂的数据（包括半结构化数据）。此框架的许可协议为Apache
    License 2.0。 | RESTful API假设状态，而函数式语言则避免使用状态。除非你足够灵活，能够偏离纯函数式方法，否则这个框架可能不适合你。
    |'
- en: '| **Slick** | While Slick is not a UI component, it is Typesafe''s modern database
    query and access library for Scala, which can serve as a UI backend. It allows
    you to work with the stored data almost as if you were using Scala collections,
    while at the same time, giving you full control over when a database access occurs
    and what data is transferred. You can also use SQL directly. Use it if all of
    your data is purely relational. This is open sourced under BSD-Style license.
    | Slick was started in 2012 by Stefan Zeiger and maintained mainly by Typesafe.
    It is useful for mostly relational data. |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| **Slick** | 虽然 Slick 不是一个 UI 组件，但它是由 Typesafe 开发的 Scala 的现代数据库查询和访问库，可以作为
    UI 后端使用。它允许您几乎像使用 Scala 集合一样处理存储的数据，同时同时，让您完全控制数据库访问发生的时间和传输的数据。您还可以直接使用 SQL。如果您的所有数据都是纯关系型数据，请使用它。该项目采用
    BSD-Style 许可证开源。Slick 由 Stefan Zeiger 于 2012 年启动，主要由 Typesafe 维护。它主要用于关系型数据。|'
- en: '| **NodeJS** | Node.js is a JavaScript runtime, built on Chrome''s V8 JavaScript
    engine. Node.js uses an event-driven, non-blocking I/O model that makes it lightweight
    and efficient. Node.js'' package ecosystem, npm, is the largest ecosystem of open
    source libraries in the world. It is open sourced under MIT License. | Node.js
    was first introduced in 2009 by Ryan Dahl and other developers working at Joyent.
    Originally Node.js supported only Linux, but now it runs on OS X and Windows.
    |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| **NodeJS** | Node.js 是一个基于 Chrome 的 V8 JavaScript 引擎构建的 JavaScript 运行时。Node.js
    使用事件驱动、非阻塞 I/O 模型，使其轻量级且高效。Node.js 的包生态系统 npm 是世界上最大的开源库生态系统。该项目采用 MIT 许可证开源。Node.js
    首次由 Ryan Dahl 和在 Joyent 工作的其他开发者于 2009 年推出。最初 Node.js 只支持 Linux，但现在它可以在 OS X 和
    Windows 上运行。|'
- en: '| **AngularJS** | AngularJS ([https://angularjs.org](https://angularjs.org))
    is a frontend development framework, built to simplify development of one-page
    web applications. This is open sourced under MIT License. | AngularJS was originally
    developed in 2009 by Misko Hevery at Brat Tech LLC. AngularJS is mainly maintained
    by Google and by a community of individual developers and corporations, and thus
    is specifically for Android platform (support for IE8 is dropped in versions 1.3
    and later). |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| **AngularJS** | AngularJS ([https://angularjs.org](https://angularjs.org))
    是一个前端开发框架，旨在简化单页网络应用程序的开发。该项目采用 MIT 许可证开源。AngularJS 最初于 2009 年由 Brat Tech LLC
    的 Misko Hevery 开发。AngularJS 主要由 Google 和一群个人开发者及企业维护，因此特别适用于 Android 平台（从 1.3
    版本开始不再支持 IE8）。|'
- en: Actions engine
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 动作引擎
- en: While this is the heart of the data-oriented system pipeline, it is also arguably
    the easiest one. Once the system of metrics and values is known, the system decides,
    based on the known equations, whether to take a certain set of actions or not,
    based on the information provided. While the triggers based on a threshold is
    the most common implementation, the significance of probabilistic approaches that
    present the user with a set of possibilities and associated probabilities is emerging—or
    just presenting the user with the top *N* relevant choices like a search engine
    does.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是面向数据系统管道的核心，但也有人认为它是最容易的一个。一旦知道了指标和值的系统，系统就会根据已知的方程式，根据提供的信息，决定是否采取某些行动。虽然基于阈值的触发器是最常见的实现方式，但向用户提供一系列可能性和相关概率的概率方法的重要性正在显现——或者就像搜索引擎那样，向用户提供最相关的
    *N* 个选择。
- en: The management of the rules might become pretty involved. It used to be that
    managing the rules with a rule engine, such as **Drools** ([http://www.drools.org](http://www.drools.org)),
    was sufficient. However, managing complex rules becomes an issue that often requires
    development of a DSL (*Domain-Specific Languages* by Martin Fowler, Addison-Wesley,
    2010). Scala is particularly fitting language for the development of such an actions
    engine.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 规则的管理可能会变得相当复杂。过去，使用规则引擎（如 **Drools** ([http://www.drools.org](http://www.drools.org)））管理规则是足够的。然而，管理复杂的规则成为一个问题，通常需要开发一个
    DSL（马丁·福勒的《领域特定语言》，Addison-Wesley，2010 年）。Scala 特别适合开发这样的动作引擎。
- en: Correlation engine
  id: totrans-225
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关引擎
- en: The more complex the decision-making system is, the more it requires a secondary
    decision-making system to optimize its management. DevOps is turning into DataOps
    (*Getting Data Right* by Michael Stonebraker et al., Tamr, 2015). Data collected
    about the performance of a data-driven system are used to detect anomalies and
    semi-automated maintenance.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 决策系统越复杂，就越需要一个二级决策系统来优化其管理。DevOps正在转变为DataOps（Michael Stonebraker等人所著的《Getting
    Data Right》，Tamr，2015）。关于数据驱动型系统性能收集的数据用于检测异常和半自动化维护。
- en: Models are often subject to time drift, where the performance might deteriorate
    either due to the changes in the data collection layers or the behavioral changes
    in the population (I will cover model drift in [Chapter 10](part0297.xhtml#aid-8R7N21
    "Chapter 10. Advanced Model Monitoring"), *Advanced Model Monitoring*). Another
    aspect of model management is to track model performance, and in some cases, use
    "collective" intelligence of the models by various consensus schemes.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 模型通常会受到时间漂移的影响，性能可能会因为数据收集层的变化或人群行为的变化而下降（我将在[第10章](part0297.xhtml#aid-8R7N21
    "第10章。高级模型监控")*高级模型监控*中讨论模型漂移）。模型管理的另一个方面是跟踪模型性能，在某些情况下，通过各种共识方案使用模型的“集体智慧”。
- en: Monitoring
  id: totrans-228
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监控
- en: Monitoring a system involves collecting information about system performance
    either for audit, diagnostic, or performance-tuning purposes. While it is related
    to the issues raised in the previous sections, monitoring solution often incorporates
    diagnostic and historical storage solutions and persistence of critical data,
    such as a black box on an airplane. In the Java and, thus, Scala world, a popular
    tool of choice is Java performance beans, which can be monitored in the Java Console.
    While Java natively supports MBean for exposing JVM information over JMX, **Kamon**
    ([http://kamon.io](http://kamon.io)) is an open source library that uses this
    mechanism to specifically expose Scala and Akka metrics.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 监控一个系统涉及收集有关系统性能的信息，无论是为了审计、诊断还是性能调整。虽然它与前面章节中提出的问题有关，但监控解决方案通常包含诊断和历史存储解决方案，以及关键数据的持久化，就像飞机上的黑匣子。在Java和Scala的世界里，一个流行的选择工具是Java性能bean，可以在Java控制台中监控。虽然Java原生支持MBean通过JMX暴露JVM信息，但**Kamon**([http://kamon.io](http://kamon.io))是一个开源库，它使用这种机制专门暴露Scala和Akka指标。
- en: Some other popular monitoring open source solutions are **Ganglia** ([http://ganglia.sourceforge.net/](http://ganglia.sourceforge.net/))
    and **Graphite** ([http://graphite.wikidot.com](http://graphite.wikidot.com)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他流行的开源监控解决方案包括**Ganglia**([http://ganglia.sourceforge.net/](http://ganglia.sourceforge.net/))和**Graphite**([http://graphite.wikidot.com](http://graphite.wikidot.com))。
- en: I will stop here, as I will address system and model monitoring in more detail
    in [Chapter 10](part0297.xhtml#aid-8R7N21 "Chapter 10. Advanced Model Monitoring"),
    *Advanced Model Monitoring*.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里就不再继续了，因为我在[第10章](part0297.xhtml#aid-8R7N21 "第10章。高级模型监控")*高级模型监控*中会更详细地讨论系统和模型监控。
- en: Optimization and interactivity
  id: totrans-232
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化和交互性
- en: 'While the data collected can be just used for understanding the business, the
    final goal of any data-driven business is to optimize the business behavior by
    automatically making data-based and model-based decisions. We want to reduce human
    intervention to minimum. The following simplified diagram can be depicted as a
    cycle:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然收集到的数据可以仅用于理解业务，但任何数据驱动型企业的最终目标是通过自动做出基于数据和模型的决定来优化业务行为。我们希望将人为干预降到最低。以下简化的图可以描述为一个循环：
- en: '![Optimization and interactivity](img/image01656.jpeg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![优化和交互性](img/image01656.jpeg)'
- en: Figure 02-4\. The predictive model life cycle
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图02-4. 预测模型生命周期
- en: The cycle is repeated over and over for new information coming into the system.
    The parameters of the system may be tuned to improve the overall system performance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当系统中有新信息进入时，这个循环会一次又一次地重复。系统参数可能需要调整以提高整体系统性能。
- en: Feedback loops
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 反馈循环
- en: While humans are still likely to be kept in the loop for most of the systems,
    last few years saw an emergence of systems that can manage the complete feedback
    loop on their own—ranging from advertisement systems to self-driving cars.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人类可能仍然会在大多数系统中保持参与，但最近几年出现了能够独立管理完整反馈循环的系统——从广告系统到自动驾驶汽车。
- en: The classical formulation of this problem is the optimal control theory, which
    is also an optimization problem to minimize cost functional, given a set of differential
    equations describing the system. An optimal control is a set of control policies
    to minimize the cost functional given constraints. For example, the problem might
    be to find a way to drive the car to minimize its fuel consumption, given that
    it must complete a given course in a time not exceeding some amount. Another control
    problem is to maximize profit for showing ads on a website, provided the inventory
    and time constraints. Most software packages for optimal control are written in
    other languages such as C or MATLAB (PROPT, SNOPT, RIOTS, DIDO, DIRECT, and GPOPS),
    but can be interfaced with Scala.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题的经典表述是最优控制理论，它也是一个优化问题，旨在最小化成本泛函，给定一组描述系统的微分方程。最优控制是一组控制策略，用于在给定约束条件下最小化成本泛函。例如，问题可能是在不超过某个时间限制的情况下，找到一种驾驶汽车以最小化其燃油消耗的方法。另一个控制问题是在满足库存和时间约束的条件下，最大化在网站上展示广告的利润。大多数最优控制软件包是用其他语言编写的，如C或MATLAB（PROPT、SNOPT、RIOTS、DIDO、DIRECT和GPOPS），但可以与Scala接口。
- en: However, in many cases, the parameters for the optimization or the state transition,
    or differential equations, are not known with certainty. **Markov Decision Processes**
    (**MDPs**) provide a mathematical framework to model decision making in situations
    where outcomes are partly random and partly under the control of the decision
    maker. In MDPs, we deal with a discrete set of possible states and a set of actions.
    The "rewards" and state transitions depend both on the state and actions. MDPs
    are useful for studying a wide range of optimization problems solved via dynamic
    programming and reinforcement learning.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在许多情况下，优化的参数或状态转移，或微分方程，并不确定。**马尔可夫决策过程（MDPs**）提供了一个数学框架来模拟在结果部分随机且部分受决策者控制的情况下的决策。在MDPs中，我们处理一组可能的状态和一组动作。奖励和状态转移既取决于状态也取决于动作。MDPs对于通过动态规划和强化学习解决广泛的优化问题非常有用。
- en: Summary
  id: totrans-241
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I described a high-level architecture and approach to design
    a data-driven enterprise. I also introduced you to influence diagrams, a tool
    for understanding how the decisions are made in traditional and data-driven enterprises.
    I stopped on a few key models, such as Kelly Criterion and multi-armed bandit,
    essential to demonstrate the issues from the mathematical point of view. I built
    on top of this to introduce some Markov decision process approaches where we deal
    with decision policies based on the results of the previous decisions and observations.
    I delved into more practical aspects of building a data pipeline for decision-making,
    describing major components and frameworks that can be used to built them. I also
    discussed the issues of communicating the data and modeling results between different
    stages and nodes, presenting the results to the user, feedback loop, and monitoring.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我描述了一个设计数据驱动企业的整体架构和途径。我还向您介绍了影响图，这是一种理解在传统和数据驱动企业中如何做出决策的工具。我简要介绍了几个关键模型，如凯利准则和多臂老虎机，这些模型对于从数学角度展示问题至关重要。在此基础上，我介绍了基于先前决策和观察结果的处理决策策略的马尔可夫决策过程方法。我深入探讨了构建决策数据管道的更多实际方面，描述了可以用来构建它们的主要组件和框架。我还讨论了在不同阶段和节点之间传达数据和建模结果的问题，包括向用户展示结果、反馈循环和监控。
- en: In the next chapter, I will describe MLlib, a library for machine learning over
    distributed set of nodes written in Scala.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我将描述MLlib，这是一个用于在Scala编写的分布式节点集上进行机器学习的库。
- en: Chapter 3. Working with Spark and MLlib
  id: totrans-244
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3章：使用Spark和MLlib
- en: Now that we are powered with the knowledge of where and how statistics and machine
    learning fits in the global data-driven enterprise architecture, let's stop at
    the specific implementations in Spark and MLlib, a machine learning library on
    top of Spark. Spark is a relatively new member of the big data ecosystem that
    is optimized for memory usage as opposed to disk. The data can be still spilled
    to disk as necessary, but Spark does the spill only if instructed to do so explicitly,
    or if the active dataset does not fit into the memory. Spark stores lineage information
    to recompute the active dataset if a node goes down or the information is erased
    from memory for some other reason. This is in contrast to the traditional MapReduce
    approach, where the data is persisted to the disk after each map or reduce task.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经掌握了统计和机器学习在全球数据驱动企业架构中如何定位以及如何应用的知识，让我们专注于Spark和MLlib的具体实现。MLlib是Spark之上的一个机器学习库。Spark是大数据生态系统中的相对较新成员，它优化了内存使用而不是磁盘。数据在必要时仍然可以溢出到磁盘，但Spark只有在被明确指令或活动数据集不适合内存时才会进行溢出。Spark存储
    lineage 信息，以便在节点故障或由于其他原因信息从内存中删除时重新计算活动数据集。这与传统的MapReduce方法形成对比，在每次map或reduce任务之后，数据都会持久化到磁盘。
- en: Spark is particularly suited for iterative or statistical machine learning algorithms
    over a distributed set of nodes and can scale out of core. The only limitation
    is the total memory and disk space available across all Spark nodes and the network
    speed. I will cover the basics of Spark architecture and implementation in this
    chapter.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: Spark特别适合在分布式节点上执行迭代或统计机器学习算法，并且可以扩展到核心之外。唯一的限制是所有Spark节点上可用的总内存和磁盘空间以及网络速度。我将在本章中介绍Spark架构和实现的基础知识。
- en: One can direct Spark to execute data pipelines either on a single node or across
    a set of nodes with a simple change in the configuration parameters. Of course,
    this flexibility comes at a cost of slightly heavier framework and longer setup
    times, but the framework is very parallelizable and as most of modern laptops
    are already multithreaded and sufficiently powerful, this usually does not present
    a big issue.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过简单地更改配置参数，将Spark直接指向在单个节点或一组节点上执行数据管道。当然，这种灵活性是以稍微更重的框架和更长的设置时间为代价的，但框架非常易于并行化，并且由于大多数现代笔记本电脑已经多线程且足够强大，这通常不会成为一个大问题。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing and configuring Spark if you haven't done so yet
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，安装和配置Spark
- en: Learning the basics of Spark architecture and why it is inherently tied to the
    Scala language
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Spark架构的基础以及为什么它与Scala语言紧密相连
- en: Learning why Spark is the next technology after sequential implementations and
    Hadoop MapReduce
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习为什么Spark是继顺序实现和Hadoop MapReduce之后的下一代技术
- en: Learning about Spark components
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Spark组件
- en: Looking at the simple implementation of word count in Scala and Spark
  id: totrans-253
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看Scala和Spark中单词计数的简单实现
- en: Looking at the streaming word count implementation
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看流式单词计数实现
- en: Seeing how to create Spark DataFrames from either a distributed file or a distributed
    database
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解如何从分布式文件或分布式数据库创建Spark DataFrame
- en: Learning about Spark performance tuning
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Spark性能调优
- en: Setting up Spark
  id: totrans-257
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置Spark
- en: 'If you haven''t done so yet, you can download the pre-build Spark package from
    [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html).
    The latest release at the time of writing is **1.6.1**:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，您可以从 [http://spark.apache.org/downloads.html](http://spark.apache.org/downloads.html)
    下载预构建的Spark包。写作时的最新版本是 **1.6.1**：
- en: '![Setting up Spark](img/image01657.jpeg)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![设置Spark](img/image01657.jpeg)'
- en: Figure 03-1\. The download site at http://spark.apache.org with recommended
    selections for this chapter
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-1\. 该章节推荐的下载网站 http://spark.apache.org，以及本章的推荐选择
- en: 'Alternatively, you can build the Spark by downloading the full source distribution
    from [https://github.com/apache/spark](https://github.com/apache/spark):'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以通过从 [https://github.com/apache/spark](https://github.com/apache/spark)
    下载完整的源代码分布来构建Spark：
- en: '[PRE13]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The command will download the necessary dependencies and create the `spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`
    file in the Spark directory; the version is 2.0.0, as it is the next release version
    at the time of writing. In general, you do not want to build from trunk unless
    you are interested in the latest features. If you want a released version, you
    can checkout the corresponding tag. Full list of available versions is available
    via the `git branch –r` command. The `spark*.tgz` file is all you need to run
    Spark on any machine that has Java JRE.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将下载必要的依赖并创建位于 Spark 目录下的 `spark-2.0.0-SNAPSHOT-bin-alex-spark-build-2.6-yarn.tgz`
    文件；版本号为 2.0.0，因为它是写作时的下一个发布版本。通常情况下，除非你对最新特性感兴趣，否则你不想从主干分支构建。如果你想获取发布版本，可以检出相应的标签。可以通过
    `git branch –r` 命令查看可用的完整版本列表。`spark*.tgz` 文件是你在任何安装了 Java JRE 的机器上运行 Spark 所需要的全部。
- en: The distribution comes with the `docs/building-spark.md` document that describes
    other options for building Spark and their descriptions, including incremental
    Scala compiler, zinc. Full Scala 2.11 support is in the works for the next Spark
    2.0.0 release.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 该发行版附带 `docs/building-spark.md` 文档，该文档描述了构建 Spark 的其他选项及其描述，包括增量 Scala 编译器 zinc。下一个
    Spark 2.0.0 版本的完整 Scala 2.11 支持正在开发中。
- en: Understanding Spark architecture
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解 Spark 架构
- en: A parallel execution involves splitting the workload into subtasks that are
    executed in different threads or on different nodes. Let's see how Spark does
    this and how it manages execution and communication between the subtasks.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 并行执行涉及将工作负载拆分为在不同线程或不同节点上执行的子任务。让我们看看 Spark 如何做到这一点，以及它是如何管理子任务之间的执行和通信的。
- en: Task scheduling
  id: totrans-267
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 任务调度
- en: 'Spark workload splitting is determined by the number of partitions for **Resilient
    Distributed Dataset** (**RDD**), the basic abstraction in Spark, and the pipeline
    structure. An RDD represents an immutable, partitioned collection of elements
    that can be operated on in parallel. While the specifics might depend on the mode
    in which Spark runs, the following diagram captures the Spark task/resource scheduling:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 工作负载拆分由 **Resilient Distributed Dataset**（**RDD**）的分区数量决定，RDD 是 Spark
    中的基本抽象，以及管道结构。RDD 代表了一个不可变、分区元素集合，这些元素可以并行操作。虽然具体细节可能取决于 Spark 运行的模式，但以下图表捕捉了
    Spark 任务/资源调度的过程：
- en: '![Task scheduling](img/image01658.jpeg)'
  id: totrans-269
  prefs: []
  type: TYPE_IMG
  zh: '![任务调度](img/image01658.jpeg)'
- en: Figure 03-2\. A generic Spark task scheduling diagram. While not shown explicitly
    in the figure, Spark Context opens an HTTP UI, usually on port 4040 (the concurrent
    contexts will open 4041, 4042, and so on), which is present during a task execution.
    Spark Master UI is usually 8080 (although it is changed to 18080 in CDH) and Worker
    UI is usually 7078\. Each node can run multiple executors, and each executor can
    run multiple tasks.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-2\. 一个通用的 Spark 任务调度图。虽然图中没有明确显示，但 Spark Context 会打开一个 HTTP UI，通常在端口 4040（并发上下文将打开
    4041、4042 等端口），在任务执行期间存在。Spark Master UI 通常为 8080（虽然在 CDH 中被改为 18080），Worker UI
    通常为 7078。每个节点可以运行多个执行器，每个执行器可以运行多个任务。
- en: Tip
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: You will find that Spark, as well as Hadoop, has a lot of parameters. Some of
    them are specified as environment variables (refer to the `$SPARK_HOME/conf/spark-env.sh`
    file), and yet some can be given as a command-line parameter. Moreover, some files
    with pre-defined names can contain parameters that will change the Spark behavior,
    such as `core-site.xml`. This might be confusing, and I will cover as much as
    possible in this and the following chapters. If you are working with **Hadoop
    Distributed File System** (**HDFS**), then the `core-site.xml` and `hdfs-site.xml`
    files will contain the pointer and specifications for the HDFS master. The requirement
    for picking this file is that it has to be on `CLASSPATH` Java process, which,
    again, may be set by either specifying `HADOOP_CONF_DIR` or `SPARK_CLASSPATH`
    environment variables. As is usual with open source, you need to grep the code
    sometimes to understand how various parameters work, so having a copy of the source
    tree on your laptop is a good idea.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现 Spark，以及 Hadoop，有很多参数。其中一些被指定为环境变量（参见图 `$SPARK_HOME/conf/spark-env.sh`
    文件），还有一些可以作为命令行参数提供。此外，一些具有预定义名称的文件可以包含将改变 Spark 行为的参数，例如 `core-site.xml`。这可能会让人困惑，我将在本章和下一章尽可能多地涵盖这些内容。如果你正在使用
    **Hadoop 分布式文件系统**（**HDFS**），那么 `core-site.xml` 和 `hdfs-site.xml` 文件将包含 HDFS 主节点的指针和规范。选择此文件的要求是它必须位于
    `CLASSPATH` Java 进程中，这可以通过指定 `HADOOP_CONF_DIR` 或 `SPARK_CLASSPATH` 环境变量来设置。与开源项目一样，有时你需要
    grep 代码来理解各种参数的工作方式，因此在你笔记本电脑上保留源代码树副本是个好主意。
- en: Each node in the cluster can run one or more executors, and each executor can
    schedule a sequence of tasks to perform the Spark operations. Spark driver is
    responsible for scheduling the execution and works with the cluster scheduler,
    such as Mesos or YARN to schedule the available resources. Spark driver usually
    runs on the client machine, but in the latest release, it can also run in the
    cluster under the cluster manager. YARN and Mesos have the ability to dynamically
    manage the number of executors that run concurrently on each node, provided the
    resource constraints.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中的每个节点可以运行一个或多个执行器，每个执行器可以调度一系列任务以执行 Spark 操作。Spark 驱动程序负责调度执行，并与集群调度器（如 Mesos
    或 YARN）一起调度可用资源。Spark 驱动程序通常在客户端机器上运行，但在最新版本中，它也可以在集群管理器下运行。YARN 和 Mesos 有能力动态管理每个节点上并发运行的执行器数量，前提是满足资源限制。
- en: 'In the Standalone mode, **Spark Master** does the work of the cluster scheduler—it
    might be less efficient in allocating resources, but it''s better than nothing
    in the absence of preconfigured Mesos or YARN. Spark standard distribution contains
    shell scripts to start Spark in Standalone mode in the `sbin` directory. Spark
    Master and driver communicate directly with one or several Spark workers that
    run on individual nodes. Once the master is running, you can start Spark shell
    with the following command:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Standalone 模式下，**Spark Master** 执行集群调度器的工作——在资源分配方面可能不太高效，但在没有预配置 Mesos 或
    YARN 的情况下，总比没有好。Spark 标准发行版包含在 `sbin` 目录下启动 Spark 的 shell 脚本。Spark Master 和驱动程序直接与运行在各个节点上的一个或多个
    Spark 工作节点通信。一旦主节点运行，你可以使用以下命令启动 Spark Shell：
- en: '[PRE14]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Tip
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: Note that you can always run Spark in local mode, which means that all tasks
    will be executed in a single JVM, by specifying `--master local[2]`, where `2`
    is the number of threads that have to be at least `2`. In fact, we will be using
    the local mode very often in this book for running small examples.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你始终可以在本地模式下运行 Spark，这意味着所有任务都将在一个 JVM 中执行，通过指定 `--master local[2]`，其中 `2`
    是至少需要 `2` 个线程的数量。实际上，我们将在本书中非常频繁地使用本地模式来运行小型示例。
- en: 'Spark shell is an application from the Spark point of view. Once you start
    a Spark application, you will see it under **Running Applications** in the Spark
    Master UI (or in the corresponding cluster manager), which can redirect you to
    the Spark application HTTP UI at port 4040, where one can see the subtask execution
    timeline and other important properties such as environment setting, classpath,
    parameters passed to the JVM, and information on resource usage (refer to *Figure
    3-3*):'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: Spark Shell 是从 Spark 视角的一个应用程序。一旦你启动一个 Spark 应用程序，你将在 Spark Master UI（或相应的集群管理器）中的
    **运行中的应用程序** 下看到它，这可以让你重定向到 Spark 应用程序 HTTP UI，端口为 4040，在那里可以看到子任务执行时间线以及其他重要属性，例如环境设置、类路径、传递给
    JVM 的参数以及资源使用信息（参见图 3-3）：
- en: '![Task scheduling](img/image01659.jpeg)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![任务调度](img/image01659.jpeg)'
- en: Figure 03-3\. Spark Driver UI in Standalone mode with time decomposition
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-3\. 独立模式下的 Spark 驱动器 UI 与时间分解
- en: 'As we saw, with Spark, one can easily switch between local and cluster mode
    by providing the `--master` command-line option, setting a `MASTER` environment
    variable, or modifying `spark-defaults.conf`, which should be on the classpath
    during the execution, or even set explicitly using the `setters` method on the
    `SparkConf` object directly in Scala, which will be covered later:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用 Spark，可以通过提供 `--master` 命令行选项、设置 `MASTER` 环境变量或修改 `spark-defaults.conf`（应在执行期间位于类路径上）来轻松地在本地模式和集群模式之间切换，或者甚至可以直接在
    Scala 中使用 `SparkConf` 对象的 `setters` 方法显式设置，这将在后面介绍：
- en: '| Cluster Manager | MASTER env variable | Comments |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 集群管理器 | `MASTER` 环境变量 | 注释 |'
- en: '| --- | --- | --- |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Local (single node, multiple threads) | `local[n]` | *n* is the number of
    threads to use, should be greater than or equal to *2*. If you want Spark to communicate
    with other Hadoop tools such as Hive, you still need to point it to the cluster
    by either setting the `HADOOP_CONF_DIR` environment variable or copying the Hadoop
    `*-site.xml` configuration files into the `conf` subdirectory. |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 本地模式（单节点，多线程） | `local[n]` | *n* 是要使用的线程数，应大于或等于 *2*. 如果您想让 Spark 与其他 Hadoop
    工具（如 Hive）通信，您仍然需要通过设置 `HADOOP_CONF_DIR` 环境变量或将 Hadoop `*-site.xml` 配置文件复制到 `conf`
    子目录来指向集群。 |'
- en: '| Standalone (Daemons running on the nodes) | `spark:// master-address>:7077`
    | This has a set of start/stop scripts in the `$SPARK_HOME/sbin` directory. This
    also supports the HA mode. More details can be found at [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html).
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 独立模式（在节点上运行的守护进程） | `spark:// master-address>:7077` | 此模式在 `$SPARK_HOME/sbin`
    目录下有一组启动/停止脚本。此模式也支持高可用性模式。更多详情请参阅 [https://spark.apache.org/docs/latest/spark-standalone.html](https://spark.apache.org/docs/latest/spark-standalone.html)。
    |'
- en: '| Mesos | `mesos://host:5050` or `mesos://zk://host:2181`(multimaster) | Here,
    you need to set `MESOS_NATIVE_JAVA_LIBRARY=<path to libmesos.so>` and `SPARK_EXECUTOR_URI=<URL
    of spark-1.5.0.tar.gz>`. The default is fine-grained mode, where each Spark task
    runs as a separate Mesos task. Alternatively, the user can specify the coarse-grained
    mode, where the Mesos tasks persists for the duration of the application. The
    advantage is lower total start-up costs. This can use dynamic allocation (refer
    to the following URL) in coarse-grained mode. More details can be found at [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html).
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| Mesos | `mesos://host:5050` 或 `mesos://zk://host:2181`(多主) | 在这里，您需要设置 `MESOS_NATIVE_JAVA_LIBRARY=<libmesos.so
    路径>` 和 `SPARK_EXECUTOR_URI=<spark-1.5.0.tar.gz 的 URL>`. 默认为细粒度模式，其中每个 Spark 任务作为一个独立的
    Mesos 任务运行。用户还可以指定粗粒度模式，其中 Mesos 任务持续整个应用程序的运行时间。这种模式的优点是总启动成本较低。在粗粒度模式下，可以使用动态分配（参考以下
    URL）。更多详情请参阅 [https://spark.apache.org/docs/latest/running-on-mesos.html](https://spark.apache.org/docs/latest/running-on-mesos.html)。
    |'
- en: '| YARN | `yarn` | Spark driver can run either in the cluster or on the client
    node, which is managed by the `--deploy-mode` parameter (cluster or client, shell
    can only run in the client mode). Set `HADOOP_CONF_DIR` or `YARN_CONF_DIR` to
    point to the YARN config files. Use the `--num-executors` flag or `spark.executor.instances`
    property to set a fixed number of executors (default).Set `spark.dynamicAllocation.enabled`
    to `true` to dynamically create/kill executors depending on the application demand.
    More details are available at [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html).
    |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| YARN | `yarn` | Spark 驱动器可以在集群或客户端节点上运行，由 `--deploy-mode` 参数（集群或客户端，shell
    只能在客户端模式下运行）管理。设置 `HADOOP_CONF_DIR` 或 `YARN_CONF_DIR` 以指向 YARN 配置文件。使用 `--num-executors`
    标志或 `spark.executor.instances` 属性设置固定数量的执行器（默认）。将 `spark.dynamicAllocation.enabled`
    设置为 `true` 以根据应用程序需求动态创建/销毁执行器。更多详情请参阅 [https://spark.apache.org/docs/latest/running-on-yarn.html](https://spark.apache.org/docs/latest/running-on-yarn.html)。
    |'
- en: 'The most common ports are 8080, the master UI, and 4040, the application UI.
    Other Spark ports are summarized in the following table:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的端口是 8080（主 UI）和 4040（应用程序 UI）。其他 Spark 端口总结在下表中：
- en: '| Standalone ports |   |   |   |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 独立模式端口 |   |   |   |'
- en: '| --- | --- | --- | --- |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| From | To | Default Port | Purpose | Configuration Setting |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 从 | 到 | 默认端口 | 用途 | 配置设置 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Browser | Standalone Master | 8080 | Web UI | `spark.master.ui.port /SPARK_MASTER_WEBUI_PORT`
    |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 独立模式主节点 | 8080 | Web UI | `spark.master.ui.port /SPARK_MASTER_WEBUI_PORT`
    |'
- en: '| Browser | Standalone worker | 8081 | Web UI | `spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT`
    |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 独立工作者 | 8081 | Web UI | `spark.worker.ui.port /SPARK_WORKER_WEBUI_PORT`
    |'
- en: '| Driver / Standalone worker | Standalone Master | 7077 | Submit job to cluster
    / Join cluster | `SPARK_MASTER_PORT` |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '| 驾驶员/独立工作者 | 独立主节点 | 7077 | 将作业提交到集群/加入集群 | `SPARK_MASTER_PORT` |'
- en: '| Standalone master | Standalone worker | (random) | Schedule executors | `SPARK_WORKER_PORT`
    |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 独立主节点 | 独立工作者 | (随机) | 调度执行器 | `SPARK_WORKER_PORT` |'
- en: '| Executor / Standalone master | Driver | (random) | Connect to application
    / Notify executor state changes | `spark.driver.port` |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 执行器/独立主节点 | 驱动器 | (随机) | 连接到应用程序/通知执行器状态变化 | `spark.driver.port` |'
- en: '| **Other ports** |   |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| **其他端口** |   |'
- en: '| **From** | **To** | **Default Port** | **Purpose** | **Configuration Setting**
    |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| **从** | **到** | **默认端口** | **用途** | **配置设置** |'
- en: '| Browser | Application | 4040 | Web UI | `spark.ui.port` |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 应用程序 | 4040 | Web UI | `spark.ui.port` |'
- en: '| Browser | History server | 18080 | Web UI | `spark.history.ui.port` |'
  id: totrans-301
  prefs: []
  type: TYPE_TB
  zh: '| 浏览器 | 历史服务器 | 18080 | Web UI | `spark.history.ui.port` |'
- en: '| Driver | Executor | (random) | Schedule tasks | `spark.executor.port` |'
  id: totrans-302
  prefs: []
  type: TYPE_TB
  zh: '| 驱动器 | 执行器 | (随机) | 调度任务 | `spark.executor.port` |'
- en: '| Executor | Driver | (random) | File server for files and jars | `spark.fileserver.port`
    |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '| 执行器 | 驱动器 | (随机) | 文件服务器（用于文件和jar文件） | `spark.fileserver.port` |'
- en: '| Executor | Driver | (random) | HTTP broadcast | `spark.broadcast.port` |'
  id: totrans-304
  prefs: []
  type: TYPE_TB
  zh: '| 执行器 | 驱动器 | (随机) | HTTP广播 | `spark.broadcast.port` |'
- en: Also, some of the documentation is available with the source distribution in
    the `docs` subdirectory, but may be out of date.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，一些文档还可在源分布的`docs`子目录中找到，但可能已过时。
- en: Spark components
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark组件
- en: 'Since the emergence of Spark, multiple applications that benefit from Spark''s
    ability to cache RDDs have been written: Shark, Spork (Pig on Spark), graph libraries
    (GraphX, GraphFrames), streaming, MLlib, and so on; some of these will be covered
    here and in later chapters.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 自从Spark出现以来，已经编写了多个利用Spark缓存RDD能力的好处应用程序：Shark、Spork（Spark上的Pig）、图库（GraphX、GraphFrames）、流处理、MLlib等；其中一些将在本章和后续章节中介绍。
- en: 'In this section, I will cover major architecture components to collect, store,
    and analyze the data in Spark. While I will cover a more complete data life cycle
    architecture in [Chapter 2](part0242.xhtml#aid-76P842 "Chapter 2. Data Pipelines
    and Modeling"), *Data Pipelines and Modeling*, here are Spark-specific components:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍Spark中收集、存储和分析数据的主要架构组件。虽然我将在[第2章](part0242.xhtml#aid-76P842 "第2章。数据管道和建模")中介绍更完整的数据生命周期架构，*数据管道和建模*，但以下是Spark特定的组件：
- en: '![Spark components](img/image01660.jpeg)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![Spark组件](img/image01660.jpeg)'
- en: Figure 03-4\. Spark architecture and components.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图03-4。Spark架构和组件。
- en: MQTT, ZeroMQ, Flume, and Kafka
  id: totrans-311
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MQTT、ZeroMQ、Flume和Kafka
- en: 'All of these are different ways to reliably move data from one place to another
    without loss and duplication. They usually implement a publish-subscribe model,
    where multiple writers and readers can write and read from the same queues with
    different guarantees. Flume stands out as a first distributed log and event management
    implementation, but it is slowly replaced by Kafka, a fully functional publish-subscribe
    distributed message queue optionally persistent across a distributed set of nodes
    developed at LinkedIn. We covered Flume and Kafka briefly in the previous chapter.
    Flume configuration is file-based and is traditionally used to deliver messages
    from a Flume source to one or several Flume sinks. One of the popular sources
    is `netcat`—listening on raw data over a port. For example, the following configuration
    describes an agent receiving data and then writing them to HDFS every 30 seconds
    (default):'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都是在不丢失和重复的情况下，可靠地将数据从一个地方移动到另一个地方的不同方式。它们通常实现一个发布-订阅模型，其中多个编写者和读者可以从同一个队列中写入和读取，并具有不同的保证。Flume作为一个第一个分布式日志和事件管理实现脱颖而出，但它正逐渐被LinkedIn开发的具有完全功能的发布-订阅分布式消息队列Kafka所取代，Kafka可以选择性地在分布式节点集上持久化。我们在上一章中简要介绍了Flume和Kafka。Flume配置是基于文件的，传统上用于将消息从Flume源传递到Flume的一个或多个接收器。其中一种流行的源是`netcat`——监听端口的原始数据。例如，以下配置描述了一个每30秒（默认）接收数据并将其写入HDFS的代理：
- en: '[PRE15]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This file is included as part of the code provided with this book in the `chapter03/conf`
    directory. Let''s download and start Flume agent (check the MD5 sum with one provided
    at [http://flume.apache.org/download.html](http://flume.apache.org/download.html)):'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 此文件作为本书提供的代码的一部分包含在`chapter03/conf`目录中。让我们下载并启动Flume代理（使用[http://flume.apache.org/download.html](http://flume.apache.org/download.html)提供的MD5校验和进行检查）：
- en: '[PRE16]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now, in a separate window, you can type a `netcat` command to send text to
    the Flume agent:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在另一个窗口中，你可以输入一个 `netcat` 命令将文本发送到 Flume 代理：
- en: '[PRE17]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The Flume agent will first create a `*.tmp` file and then rename it to a file
    without extension (the file extension can be used to filter out files being written
    to):'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: Flume 代理首先创建一个 `*.tmp` 文件，然后将其重命名为没有扩展名的文件（文件扩展名可以用来过滤正在写入的文件）：
- en: '[PRE18]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Here, each row is a Unix time in milliseconds and data received. In this case,
    we put the data into HDFS, from where they can be analyzed by a Spark/Scala program,
    we can exclude the files being written to by the `*.tmp` filename pattern. However,
    if you are really interested in up-to-the-last-minute values, Spark, as well as
    some other platforms, supports streaming, which I will cover in a few sections.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，每一行都是一个以毫秒为单位的 Unix 时间和接收到的数据。在这种情况下，我们将数据放入 HDFS，然后 Spark/Scala 程序可以从那里进行分析，我们可以排除以
    `*.tmp` 文件名模式正在写入的文件。然而，如果你真的对最后一分钟的数据感兴趣，Spark 以及一些其他平台支持流式处理，我将在接下来的几节中介绍。
- en: HDFS, Cassandra, S3, and Tachyon
  id: totrans-321
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: HDFS、Cassandra、S3 和 Tachyon
- en: 'HDFS, Cassandra, S3, and Tachyon are the different ways to get the data into
    persistent storage and compute nodes as necessary with different guarantees. HDFS
    is a distributed storage implemented as a part of Hadoop, which serves as the
    backend for many products in the Hadoop ecosystem. HDFS divides each file into
    blocks, which are 128 MB in size by default, and stores each block on at least
    three nodes. Although HDFS is reliable and supports HA, a general complaint about
    HDFS storage is that it is slow, particularly for machine learning purposes. Cassandra
    is a general-purpose key/value storage that also stores multiple copies of a row
    and can be configured to support different levels of consistency to optimize read
    or write speeds. The advantage that Cassandra over HDFS model is that it does
    not have a central master node; the reads and writes are completed based on the
    consensus algorithm. This, however, may sometimes reflect on the Cassandra stability.
    S3 is the Amazon storage: The data is stored off-cluster, which affects the I/O
    speed. Finally, the recently developed Tachyon claims to utilize node''s memory
    to optimize access to working sets across the nodes.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: HDFS、Cassandra、S3 和 Tachyon 是将数据以不同保证的方式放入持久存储和计算节点的方法。HDFS 是作为 Hadoop 的一部分实现的分布式存储，为
    Hadoop 生态系统中的许多产品提供后端服务。HDFS 将每个文件划分为块，默认块大小为 128 MB，并将每个块存储在至少三个节点上。尽管 HDFS 是可靠的并支持高可用性，但关于
    HDFS 存储的一般抱怨是它速度较慢，尤其是在机器学习方面。Cassandra 是一种通用键/值存储，也存储行的多个副本，并可以配置为支持不同级别的数据一致性以优化读写速度。Cassandra
    相比于 HDFS 模型的优势在于它没有中央主节点；读取和写入是基于共识算法完成的。然而，这有时可能会反映在 Cassandra 的稳定性上。S3 是亚马逊存储：数据存储在集群之外，这影响了
    I/O 速度。最后，最近开发的 Tachyon 声称利用节点的内存来优化节点间工作集的访问。
- en: Additionally, new backends are being constantly developed, for example, Kudu
    from Cloudera ([http://getkudu.io/kudu.pdf](http://getkudu.io/kudu.pdf)) and **Ignite
    File System** (**IGFS**) from GridGain ([http://apacheignite.gridgain.org/v1.0/docs/igfs)](http://apacheignite.gridgain.org/v1.0/docs/igfs).
    Both are open source and Apache-licensed.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，新的后端正在不断开发中，例如，来自 Cloudera 的 Kudu ([http://getkudu.io/kudu.pdf](http://getkudu.io/kudu.pdf))
    和来自 GridGain 的 **Ignite 文件系统** (**IGFS**) ([http://apacheignite.gridgain.org/v1.0/docs/igfs)](http://apacheignite.gridgain.org/v1.0/docs/igfs))。两者都是开源的，并拥有
    Apache 许可证。
- en: Mesos, YARN, and Standalone
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Mesos、YARN 和 Standalone
- en: 'As we mentioned before, Spark can run under different cluster resource schedulers.
    These are various implementations to schedule Spark''s containers and tasks on
    the cluster. The schedulers can be viewed as cluster kernels, performing functions
    similar to the operating system kernel: resource allocation, scheduling, I/O optimization,
    application services, and UI.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，Spark 可以在不同的集群资源调度器下运行。这些是用于在集群上调度 Spark 容器和任务的多种实现。调度器可以被视为集群内核，执行类似于操作系统内核的功能：资源分配、调度、I/O
    优化、应用程序服务和用户界面。
- en: Mesos is one of the original cluster managers and is built using the same principles
    as the Linux kernel, only at a different level of abstraction. A Mesos slave runs
    on every machine and provides API's for resource management and scheduling across
    entire datacenter and cloud environments. Mesos is written in C++.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: Mesos 是最早的集群管理器之一，它使用与 Linux 内核相同的原理构建，只是在不同的抽象级别上。Mesos 从节点在每个机器上运行并提供跨整个数据中心和云环境的资源管理和调度
    API。Mesos 使用 C++ 编写。
- en: YARN is a more recent cluster manager developed by Yahoo. Each node in YARN
    runs a **Node Manager**, which communicates with the **Resource Manager** which
    may run on a separate node. The resource manager schedules the task to satisfy
    memory and CPU constraints. The Spark driver itself can run either in the cluster,
    which is called the cluster mode for YARN. Otherwise, in the client mode, only
    Spark executors run in the cluster and the driver that schedules Spark pipelines
    runs on the same machine that runs Spark shell or submit program. The Spark executors
    will talk to the local host over a random open port in this case. YARN is written
    in Java with the consequences of unpredictable GC pauses, which might make latency's
    long tail fatter.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: YARN是由Yahoo开发的一个较新的集群管理器。YARN中的每个节点运行一个**节点管理器**，它与可能运行在单独节点上的**资源管理器**通信。资源管理器调度任务以满足内存和CPU限制。Spark驱动程序本身可以在集群中运行，这被称为YARN的集群模式。否则，在客户端模式下，只有Spark执行器在集群中运行，而调度Spark管道的驱动程序运行在运行Spark
    shell或提交程序的同一台机器上。在这种情况下，Spark执行器将通过一个随机开放的端口与本地主机通信。YARN是用Java编写的，其后果是GC暂停不可预测，这可能会使延迟的长尾更宽。
- en: Finally, if none of these resource schedulers are available, the standalone
    deployment mode starts a `org.apache.spark.deploy.worker.Worker` process on each
    node that communicates with the Spark Master process run as `org.apache.spark.deploy.master.Master`.
    The worker process is completely managed by the master and can run multiple executors
    and tasks (refer to *Figure 3-2*).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果这些资源调度器都不可用，独立部署模式将在每个节点上启动一个`org.apache.spark.deploy.worker.Worker`进程，该进程与作为`org.apache.spark.deploy.master.Master`运行的Spark
    Master进程通信。工作进程完全由主进程管理，可以运行多个执行器和任务（参见图3-2）。
- en: In practical implementations, it is advised to track the program parallelism
    and required resources through driver's UI and adjust the parallelism and available
    memory, increasing the parallelism if necessary. In the following section, we
    will start looking at how Scala and Scala in Spark address different problems.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实施中，建议通过驱动程序的UI跟踪程序的并行性和所需资源，并根据需要调整并行性和可用内存，如果需要的话增加并行性。在下一节中，我们将开始探讨Scala和Spark中的Scala如何解决不同的问题。
- en: Applications
  id: totrans-330
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用程序
- en: Let's consider a few practical examples and libraries in Spark/Scala starting
    with a very traditional problem of word counting.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一些Spark/Scala中的实际示例和库，从一个非常传统的单词计数问题开始。
- en: Word count
  id: totrans-332
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 单词计数
- en: Most modern machine learning algorithms require multiple passes over data. If
    the data fits in the memory of a single machine, the data is readily available
    and this does not present a performance bottleneck. However, if the data becomes
    too large to fit into RAM, one has a choice of either dumping pieces of the data
    on disk (or database), which is about 100 times slower, but has a much larger
    capacity, or splitting the dataset between multiple machines across the network
    and transferring the results. While there are still ongoing debates, for most
    practical systems, analysis shows that storing the data over a set of network
    connected nodes has a slight advantage over repeatedly storing and reading it
    from hard disks on a single node, particularly if we can split the workload effectively
    between multiple CPUs.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代机器学习算法需要对数据进行多次遍历。如果数据适合单个机器的内存，数据就可以随时可用，这不会成为性能瓶颈。然而，如果数据变得太大而无法放入RAM中，可以选择将数据的一部分（或数据库）写入磁盘，这大约慢100倍，但容量要大得多，或者在网络中的多台机器之间分割数据集并传输结果。尽管仍有持续的争论，但对于大多数实际系统，分析表明，在一系列网络连接的节点上存储数据，与在单个节点上反复从硬盘存储和读取数据相比，略有优势，尤其是如果我们能够有效地在多个CPU之间分配工作负载。
- en: Tip
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: An average disk has bandwidth of about 100 MB/sec and transfers with a few mms
    latency, depending on the rotation speed and caching. This is about 100 times
    slower than reading the data from memory, depending on the data size and caching
    implementation again. Modern data bus can transfer data at over 10 GB/sec. While
    the network speed still lags behind the direct memory access, particularly with
    standard TCP/IP kernel networking layer overhead, specialized hardware can reach
    tens of GB/sec and if run in parallel, it can be potentially as fast as reading
    from the memory. In practice, the network-transfer speeds are somewhere between
    1 to 10 GB/sec, but still faster than the disk in most practical systems. Thus,
    we can potentially fit the data into combined memory of all the cluster nodes
    and perform iterative machine learning algorithms across a system of them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 一块普通磁盘的带宽大约为100 MB/sec，传输延迟仅为几毫秒，这取决于旋转速度和缓存。这比从内存中读取数据慢大约100倍，具体取决于数据大小和缓存实现。现代数据总线可以以超过10
    GB/sec的速度传输数据。尽管网络速度仍然落后于直接内存访问，尤其是在标准TCP/IP内核网络层开销的情况下，但专用硬件可以达到数十GB/sec，如果并行运行，其速度可能接近从内存中读取。实际上，网络传输速度在1到10
    GB/sec之间，但在大多数实际系统中仍然比磁盘快。因此，我们有可能将数据放入所有集群节点的组合内存中，并在它们组成的系统中执行迭代机器学习算法。
- en: 'One problem with memory, however, is that it is does not persist across node
    failures and reboots. A popular big data framework, Hadoop, made possible with
    the help of the original Dean/Ghemawat paper (Jeff Dean and Sanjay Ghemawat, *MapReduce:
    Simplified Data Processing on Large Clusters*, OSDI, 2004.), is using exactly
    the disk layer persistence to guarantee fault tolerance and store intermediate
    results. A Hadoop MapReduce program would first run a `map` function on each row
    of a dataset, emitting one or more key/value pairs. These key/value pairs then
    would be sorted, grouped, and aggregated by key so that the records with the same
    key would end up being processed together on the same reducer, which might be
    running on same or another node. The reducer applies a `reduce` function that
    traverses all the values that were emitted for the same key and aggregates them
    accordingly. The persistence of intermediate results would guarantee that if a
    reducer fails for one or another reason, the partial computations can be discarded
    and the reduce computation can be restarted from the checkpoint-saved results.
    Many simple ETL-like applications traverse the dataset only once with very little
    information preserved as state from one record to another.'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: '然而，内存的一个问题是它无法在节点故障和重启后持久化。一个流行的大数据框架Hadoop，在原始的Dean/Ghemawat论文（Jeff Dean和Sanjay
    Ghemawat，*MapReduce: Simplified Data Processing on Large Clusters*，OSDI，2004年）的帮助下成为可能，正是使用磁盘层持久化来保证容错性和存储中间结果。一个Hadoop
    MapReduce程序首先在数据集的每一行上运行一个`map`函数，输出一个或多个键值对。然后，这些键值对将被排序、分组和按键聚合，以便具有相同键的记录最终会在同一个reducer上一起处理，这个reducer可能运行在同一个或另一个节点上。reducer应用一个`reduce`函数，遍历为相同键发出的所有值，并相应地聚合它们。中间结果的持久化将保证如果reducer由于一个或另一个原因失败，可以丢弃部分计算，并从检查点保存的结果重新启动reduce计算。许多简单的ETL-like应用程序仅遍历数据集一次，并且从一条记录到另一条记录只保留很少的信息作为状态。'
- en: 'For example, one of the traditional applications of MapReduce is word count.
    The program needs to count the number of occurrences of each word in a document
    consisting of lines of text. In Scala, the word count is readily expressed as
    an application of the `foldLeft` method on a sorted list of words:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，MapReduce的一个传统应用是词频统计。程序需要统计文档中每行文本中每个单词的出现次数。在Scala中，词频统计可以很容易地表示为对排序单词列表应用`foldLeft`方法：
- en: '[PRE19]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'If I run this program, the output will be a list of (word, count) tuples. The
    program splits the lines into words, sorts the words, and then matches each word
    with the latest entry in the list of (word, count) tuples. The same computation
    in MapReduce would be expressed as follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我运行这个程序，输出将是一个包含(word, count)元组的列表。程序将行分割成单词，对单词进行排序，然后将每个单词与(word, count)元组列表中的最新条目进行匹配。在MapReduce中，同样的计算可以表示如下：
- en: '[PRE20]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: First, we need to process each line of the text by splitting the line into words
    and generation `(word, 1)` pairs. This task is easily parallelized. Then, to parallelize
    the global count, we need to split the counting part by assigning a task to do
    the count for a subset of words. In Hadoop, we compute the hash of the word and
    divide the work based on the value of the hash.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要通过将行拆分为单词和生成`(word, 1)`对来处理文本的每一行。这个任务很容易并行化。然后，为了并行化全局计数，我们需要通过为单词的子集分配一个任务来拆分计数部分。在Hadoop中，我们计算单词的哈希值并根据哈希值来划分工作。
- en: Once the map task finds all the entries for a given hash, it can send the key/value
    pairs to the reducer, the sending part is usually called shuffle in MapReduce
    vernacular. A reducer waits until it receives all the key/value pairs from all
    the mappers, combines the values—a partial combine can also happen on the mapper,
    if possible—and computes the overall aggregate, which in this case is just sum.
    A single reducer will see all the values for a given word.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦map任务找到了给定哈希的所有条目，它就可以将键值对发送给reducer，这部分发送通常在MapReduce术语中称为shuffle。reducer会等待从所有mapper那里接收到所有的键值对，合并值——如果可能的话，在mapper上也可以进行部分合并——并计算整体汇总，在这种情况下就是求和。单个reducer将看到给定单词的所有值。
- en: 'Let''s look at the log output of the word count operation in Spark (Spark is
    very verbose by default, you can manage the verbosity level by modifying the `conf/log4j.properties`
    file by replacing `INFO` with `ERROR` or `FATAL`):'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看Spark中单词计数操作的日志输出（Spark默认非常详细，你可以通过修改`conf/log4j.properties`文件来管理详细程度，将`INFO`替换为`ERROR`或`FATAL`）：
- en: '[PRE21]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'At this stage, the only thing that happened is metadata manipulations, Spark
    has not touched the data itself. Spark estimates that the size of the dataset
    and the number of partitions. By default, this is the number of HDFS blocks, but
    we can specify the minimum number of partitions explicitly with the `minPartitions`
    parameter:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，唯一发生的事情是元数据操作，Spark还没有触及数据本身。Spark估计数据集的大小和分区数。默认情况下，这是HDFS块的数量，但我们可以使用`minPartitions`参数显式指定最小分区数：
- en: '[PRE22]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We just defined another RDD derived from the original `linesRdd`:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚定义了另一个由原始`linesRdd`派生出的RDD：
- en: '[PRE23]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Word count over 2 GB of text data—40,291 lines and 353,087 words—took under
    a second to read, split, and group by words.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 对超过2 GB的文本数据进行单词计数——40,291行和353,087个单词——读取、拆分和按单词分组耗时不到一秒。
- en: 'With extended logging, you could see the following:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 使用扩展日志，你可以看到以下内容：
- en: Spark opens a few ports to communicate with the executors and users
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark打开了一些端口以与executors和用户通信
- en: Spark UI runs on port 4040 on `http://localhost:4040`
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark UI在`http://localhost:4040`的4040端口上运行
- en: You can read the file either from local or distributed storage (HDFS, Cassandra,
    and S3)
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以从本地或分布式存储（HDFS、Cassandra和S3）读取文件
- en: Spark will connect to Hive if Spark is built with Hive support
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果Spark带有Hive支持构建，Spark将连接到Hive
- en: Spark uses lazy evaluation and executes the pipeline only when necessary or
    when output is required
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark使用懒加载评估，仅在必要时或需要输出时才执行管道
- en: Spark uses internal scheduler to split the job into tasks, optimize the execution,
    and execute the tasks
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark使用内部调度器将作业拆分为任务，优化执行并执行任务
- en: The results are stored into RDDs, which can either be saved or brought into
    RAM of the node executing the shell with collect method
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果存储到RDD中，可以使用`save`方法保存或使用`collect`方法将其带入执行shell的节点的RAM中
- en: The art of parallel performance tuning is to split the workload between different
    nodes or threads so that the overhead is relatively small and the workload is
    balanced.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 并行性能调优的艺术在于在不同节点或线程之间分配工作负载，以便开销相对较小且工作负载平衡。
- en: Streaming word count
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 流式单词计数
- en: Spark supports listening on incoming streams, partitioning it, and computing
    aggregates close to real-time. Currently supported sources are Kafka, Flume, HDFS/S3,
    Kinesis, Twitter, as well as the traditional MQs such as ZeroMQ and MQTT. In Spark,
    streaming is implemented as micro-batches. Internally, Spark divides input data
    into micro-batches, usually from subseconds to minutes in size and performs RDD
    aggregation operations on these micro-batches.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持监听传入的流，对其进行分区，并接近实时地计算汇总。目前支持的资源包括Kafka、Flume、HDFS/S3、Kinesis、Twitter，以及传统的MQs，如ZeroMQ和MQTT。在Spark中，流式处理作为微批处理实现的。内部，Spark将输入数据划分为微批处理，通常从亚秒到分钟大小，并对这些微批处理执行RDD聚合操作。
- en: 'For example, let''s extend the Flume example that we covered earlier. We''ll
    need to modify the Flume configuration file to create a Spark polling sink. Instead
    of HDFS, replace the sink section:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们扩展一下我们之前提到的 Flume 示例。我们需要修改 Flume 配置文件以创建一个 Spark 轮询接收器。用 HDFS 代替接收器部分：
- en: '[PRE24]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, instead of writing to HDFS, Flume will wait for Spark to poll for data:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，Flume 将等待 Spark 轮询数据，而不是写入 HDFS：
- en: '[PRE25]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To run the program, start the Flume agent in one window:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行程序，在一个窗口中启动 Flume 代理：
- en: '[PRE26]'
  id: totrans-366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Then run the `FlumeWordCount` object in another:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在另一个窗口中运行 `FlumeWordCount` 对象：
- en: '[PRE27]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, any text typed to the `netcat` connection will be split into words and
    counted every two seconds for a six second sliding window:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，任何输入到 `netcat` 连接的文本将被分割成单词，并在每两秒为一个六秒滑动窗口内进行计数：
- en: '[PRE28]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Spark/Scala allows to seamlessly switch between the streaming sources. For
    example, the same program for Kafka publish/subscribe topic model looks similar
    to the following:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Spark/Scala 允许无缝地在流源之间切换。例如，用于 Kafka 发布/订阅主题模型的相同程序看起来类似于以下内容：
- en: '[PRE29]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To start the Kafka broker, first download the latest binary distribution and
    start ZooKeeper. ZooKeeper is a distributed-services coordinator and is required
    by Kafka even in a single-node deployment:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动 Kafka 代理，首先下载最新的二进制发行版并启动 ZooKeeper。ZooKeeper 是分布式服务协调器，即使在单节点部署中 Kafka
    也需要它：
- en: '[PRE30]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In another window, start the Kafka server:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个窗口中，启动 Kafka 服务器：
- en: '[PRE31]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Run the `KafkaWordCount` object:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 `KafkaWordCount` 对象：
- en: '[PRE32]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Now, publishing the stream of words into the Kafka topic will produce the window
    counts:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将单词流发布到 Kafka 主题将生成窗口计数：
- en: '[PRE33]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As you see, the programs output every two seconds. Spark streaming is sometimes
    called **micro-batch processing**. Streaming has many other applications (and
    frameworks), but this is too big of a topic to be entirely considered here and
    needs to be covered separately. I'll cover some ML on streams of data in [Chapter
    5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression and Classification"), *Regression
    and Classification*. Now, let's get back to more traditional SQL-like interfaces.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，程序每两秒输出一次。Spark 流处理有时被称为 **微批处理**。流处理有其他许多应用（和框架），但这个话题太大，不能完全在这里讨论，需要单独介绍。我将在
    [第 5 章](part0260.xhtml#aid-7NUI81 "第 5 章。回归和分类")，*回归和分类* 中介绍一些关于数据流中的机器学习。现在，让我们回到更传统的类似
    SQL 的接口。
- en: Spark SQL and DataFrame
  id: totrans-382
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Spark SQL 和 DataFrame
- en: DataFrame was a relatively recent addition to Spark, introduced in version 1.3,
    allowing one to use the standard SQL language for data analysis. We already used
    some SQL commands in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis* for the exploratory data analysis.
    SQL is really great for simple exploratory analysis and data aggregations.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrame 是 Spark 中相对较新的功能，自 1.3 版本引入，允许使用标准 SQL 语言进行数据分析。我们已经在 [第 1 章](part0235.xhtml#aid-703K61
    "第 1 章。探索性数据分析")，*探索性数据分析* 中使用了一些 SQL 命令，用于探索性数据分析。SQL 对于简单的探索性分析和数据聚合来说非常好用。
- en: According to the latest poll results, about 70% of Spark users use DataFrame.
    Although DataFrame recently became the most popular framework for working with
    tabular data, it is a relatively heavyweight object. The pipelines that use DataFrames
    may execute much slower than the ones that are based on Scala's vector or LabeledPoint,
    which will be discussed in the next chapter. The evidence from different developers
    is that the response times can be driven to tens or hundreds of milliseconds depending
    on the query, from submillisecond on simpler objects.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 根据最新的调查结果，大约 70% 的 Spark 用户使用 DataFrame。尽管 DataFrame 最近已成为处理表格数据最流行的框架，但它是一个相对较重的对象。使用
    DataFrame 的管道可能比基于 Scala 的向量或 LabeledPoint 的管道执行得慢得多，这些将在下一章中讨论。来自不同开发者的证据表明，响应时间可以根据查询从几十毫秒到几百毫秒不等，对于更简单的对象，甚至可以低于毫秒。
- en: 'Spark implements its own shell for SQL, which can be invoked in addition to
    the standard Scala REPL shell: `./bin/spark-sql` can be used to access the existing
    Hive/Impala or relational DB tables:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 实现了自己的 SQL shell，除了标准的 Scala REPL shell 之外还可以调用：`./bin/spark-sql` 可以用来访问现有的
    Hive/Impala 或关系型数据库表：
- en: '[PRE34]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'In standard Spark''s REPL, the same query can be performed by running the following
    command:'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在标准的 Spark REPL 中，可以通过运行以下命令执行相同的查询：
- en: '[PRE35]'
  id: totrans-388
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ML libraries
  id: totrans-389
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习库
- en: Spark, particularly with memory-based storage systems, claims to substantially
    improve the speed of data access within and between nodes. ML seems to be a natural
    fit, as many algorithms require multiple passes over the data, or repartitioning.
    MLlib is the open source library of choice, although private companies are catching,
    up with their own proprietary implementations.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Spark，尤其是与基于内存的存储系统结合使用，声称可以显著提高节点内和节点间的数据访问速度。机器学习似乎是一个自然的选择，因为许多算法需要对数据进行多次遍历或重新分区。MLlib是首选的开源库，尽管私有公司也在追赶，推出了自己的专有实现。
- en: 'As I will chow in [Chapter 5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression
    and Classification"), *Regression and Classification*, most of the standard machine
    learning algorithms can be expressed as an optimization problem. For example,
    classical linear regression minimizes the sum of squares of *y* distance between
    the regression line and the actual value of *y*:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我将在[第5章](part0260.xhtml#aid-7NUI81 "第5章。回归与分类")中阐述的，*回归与分类*，大多数标准机器学习算法都可以表示为一个优化问题。例如，经典线性回归最小化回归线与实际值
    *y* 之间 *y* 距离的平方和：
- en: '![ML libraries](img/image01661.jpeg)'
  id: totrans-392
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习库](img/image01661.jpeg)'
- en: 'Here, ![ML libraries](img/image01662.jpeg) are the predicted values according
    to the linear expression:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![机器学习库](img/image01662.jpeg)是根据线性表达式预测的值：
- en: '![ML libraries](img/image01663.jpeg)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习库](img/image01663.jpeg)'
- en: '*A* is commonly called the slope, and *B* the intercept. In a more generalized
    formulation, a linear optimization problem is to minimize an additive function:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '*A* 通常被称为斜率，而 *B* 被称为截距。在更广义的表述中，线性优化问题是最小化一个加性函数：'
- en: '![ML libraries](img/image01664.jpeg)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![机器学习库](img/image01664.jpeg)'
- en: 'Here, ![ML libraries](img/image01665.jpeg) is a loss function and ![ML libraries](img/image01666.jpeg)
    is a regularization function. The regularization function is an increasing function
    of model complexity, for example, the number of parameters (or a natural logarithm
    thereof). Most common loss functions are given in the following table:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![机器学习库](img/image01665.jpeg)是一个损失函数，![机器学习库](img/image01666.jpeg)是一个正则化函数。正则化函数是模型复杂度的递增函数，例如参数的数量（或其自然对数）。以下表格给出了最常见的损失函数：
- en: '|   | Loss function L | Gradient |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|   | 损失函数 L | 梯度 |'
- en: '| --- | --- | --- |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Linear | ![ML libraries](img/image01667.jpeg) | ![ML libraries](img/image01668.jpeg)
    |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 线性 | ![机器学习库](img/image01667.jpeg) | ![机器学习库](img/image01668.jpeg) |'
- en: '| Logistic | ![ML libraries](img/image01669.jpeg) | ![ML libraries](img/image01670.jpeg)
    |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | ![机器学习库](img/image01669.jpeg) | ![机器学习库](img/image01670.jpeg) |'
- en: '| Hinge | ![ML libraries](img/image01671.jpeg) | ![ML libraries](img/image01672.jpeg)
    |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '| 拉链损失 | ![机器学习库](img/image01671.jpeg) | ![机器学习库](img/image01672.jpeg) |'
- en: 'The purpose of the regularizer is to penalize more complex models to avoid
    overfitting and improve generalization error: more MLlib currently supports the
    following regularizers:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化器的目的是惩罚更复杂的模型，以避免过拟合并提高泛化误差：目前MLlib支持以下正则化器：
- en: '|   | Regularizer R | Gradient |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|   | 正则化器 R | 梯度 |'
- en: '| --- | --- | --- |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| L2 | ![ML libraries](img/image01673.jpeg) | ![ML libraries](img/image01674.jpeg)
    |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| L2 | ![机器学习库](img/image01673.jpeg) | ![机器学习库](img/image01674.jpeg) |'
- en: '| L1 | ![ML libraries](img/image01675.jpeg) | ![ML libraries](img/image01676.jpeg)
    |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| L1 | ![机器学习库](img/image01675.jpeg) | ![机器学习库](img/image01676.jpeg) |'
- en: '| Elastic net | ![ML libraries](img/image01677.jpeg) | ![ML libraries](img/image01678.jpeg)
    |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 弹性网络 | ![机器学习库](img/image01677.jpeg) | ![机器学习库](img/image01678.jpeg) |'
- en: Here, *sign(w)* is the vector of the signs of all entries of *w*.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*sign(w)* 是 *w* 所有元素的符号向量。
- en: 'Currently, MLlib includes implementation of the following algorithms:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，MLlib包括以下算法的实现：
- en: 'Basic statistics:'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基本统计学：
- en: Summary statistics
  id: totrans-412
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概率统计
- en: Correlations
  id: totrans-413
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关系数
- en: Stratified sampling
  id: totrans-414
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分层抽样
- en: Hypothesis testing
  id: totrans-415
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设检验
- en: Streaming significance testing
  id: totrans-416
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式显著性检验
- en: Random data generation
  id: totrans-417
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机数据生成
- en: 'Classification and regression:'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类与回归：
- en: Linear models (SVMs, logistic regression, and linear regression)
  id: totrans-419
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型（SVM、逻辑回归和线性回归）
- en: Naive Bayes
  id: totrans-420
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯
- en: Decision trees
  id: totrans-421
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树
- en: Ensembles of trees (Random Forests and Gradient-Boosted Trees)
  id: totrans-422
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的集成（随机森林和梯度提升树）
- en: Isotonic regression
  id: totrans-423
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 等距回归
- en: 'Collaborative filtering:'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 协同过滤：
- en: '**Alternating least squares** (**ALS**)'
  id: totrans-425
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交替最小二乘法** (**ALS**)'
- en: 'Clustering:'
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类：
- en: k-means
  id: totrans-427
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: k-means
- en: Gaussian mixture
  id: totrans-428
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类** (**PIC**)'
- en: '**Latent Dirichlet allocation** (**LDA**)'
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配** (**LDA**)'
- en: Bisecting k-means
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分k-means
- en: Streaming k-means
  id: totrans-432
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式k-means
- en: 'Dimensionality reduction:'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低：
- en: '**Singular Value Decomposition** (**SVD**)'
  id: totrans-434
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**奇异值分解**（**SVD**）'
- en: '**Principal Component Analysis** (**PCA**)'
  id: totrans-435
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**主成分分析**（**PCA**）'
- en: Feature extraction and transformation
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取和转换
- en: 'Frequent pattern mining:'
  id: totrans-437
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频繁模式挖掘：
- en: FP-growth?Association rules
  id: totrans-438
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: FP-growth？关联规则
- en: PrefixSpan
  id: totrans-439
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: PrefixSpan
- en: 'Optimization:'
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化：
- en: '**Stochastic Gradient Descent** (**SGD**)'
  id: totrans-441
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降**（**SGD**）'
- en: '**Limited-Memory BFGS** (**L-BFGS**)'
  id: totrans-442
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有限内存BFGS**（**L-BFGS**）'
- en: I will go over some of the algorithms in [Chapter 5](part0260.xhtml#aid-7NUI81
    "Chapter 5. Regression and Classification"), *Regression and Classification*.
    More complex non-structured machine learning methods will be considered in [Chapter
    6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured Data"), *Working
    with Unstructured Data*.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在[第5章](part0260.xhtml#aid-7NUI81 "第5章。回归和分类")中介绍一些算法，*回归和分类*。更复杂的非结构化机器学习方法将在[第6章](part0273.xhtml#aid-84B9I2
    "第6章。处理非结构化数据")中考虑，*处理非结构化数据*。
- en: SparkR
  id: totrans-444
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SparkR
- en: R is an implementation of popular S programming language created by John Chambers
    while working at Bell Labs. R is currently supported by the **R Foundation for
    Statistical Computing**. R's popularity has increased in recent years according
    to polls. SparkR provides a lightweight frontend to use Apache Spark from R. Starting
    with Spark 1.6.0, SparkR provides a distributed DataFrame implementation that
    supports operations such as selection, filtering, aggregation, and so on, which
    is similar to R DataFrames, dplyr, but on very large datasets. SparkR also supports
    distributed machine learning using MLlib.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: R是由约翰·查默斯在贝尔实验室工作时创建的流行S编程语言的实现。R目前由**R统计计算基金会**支持。根据调查，R的普及率近年来有所增加。SparkR提供了从R使用Apache
    Spark的轻量级前端。从Spark 1.6.0开始，SparkR提供了支持选择、过滤、聚合等操作的分布式DataFrame实现，这与R DataFrames、dplyr类似，但适用于非常大的数据集。SparkR还支持使用MLlib的分布式机器学习。
- en: SparkR required R version 3 or higher, and can be invoked via the `./bin/sparkR`
    shell. I will cover SparkR in [Chapter 8](part0288.xhtml#aid-8IL202 "Chapter 8. Integrating
    Scala with R and Python"), *Integrating Scala with R and Python*.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR需要R版本3或更高版本，可以通过`./bin/sparkR` shell调用。我将在[第8章](part0288.xhtml#aid-8IL202
    "第8章。集成Scala与R和Python")中介绍SparkR，*集成Scala与R和Python*。
- en: Graph algorithms – GraphX and GraphFrames
  id: totrans-447
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图算法 – GraphX和GraphFrames
- en: Graph algorithms are one of the hardest to correctly distribute between nodes,
    unless the graph itself is naturally partitioned, that is, it can be represented
    by a set of disconnected subgraphs. Since the social networking analysis on a
    multi-million node scale became popular due to companies such as Facebook, Google,
    and LinkedIn, researches have been coming up with new approaches to formalize
    the graph representations, algorithms, and types of questions asked.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图算法是节点间正确分配中最困难的一类，除非图本身是自然划分的，也就是说，它可以表示为一组不连接的子图。由于Facebook、Google和LinkedIn等公司使得在数百万节点规模上的社交网络分析变得流行，研究人员一直在提出新的方法来形式化图表示、算法和提出的问题类型。
- en: 'GraphX is a modern framework for graph computations described in a 2013 paper
    (*GraphX: A Resilient Distributed Graph System on Spark* by Reynold Xin, Joseph
    Gonzalez, Michael Franklin, and Ion Stoica, GRADES (SIGMOD workshop), 2013). It
    has graph-parallel frameworks such as Pregel, and PowerGraph as predecessors.
    The graph is represented by two RDDs: one for vertices and another one for edges.
    Once the RDDs are joined, GraphX supports either Pregel-like API or MapReduce-like
    API, where the map function is applied to the node''s neighbors and reduce is
    the aggregation step on top of the map results.'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 'GraphX是一个现代的图计算框架，由2013年的一篇论文描述（Reynold Xin、Joseph Gonzalez、Michael Franklin和Ion
    Stoica的《GraphX: A Resilient Distributed Graph System on Spark》，GRADES（SIGMOD workshop），2013）。它有Pregel和PowerGraph这样的图并行框架作为前身。图由两个RDD表示：一个用于顶点，另一个用于边。一旦RDDs被连接，GraphX支持Pregel-like
    API或MapReduce-like API，其中map函数应用于节点的邻居，reduce是在map结果之上的聚合步骤。'
- en: 'At the time of writing, GraphX includes the implementation for the following
    graph algorithms:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，GraphX包括以下图算法的实现：
- en: PageRank
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PageRank
- en: Connected components
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连通分量
- en: Triangle counting
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三角计数
- en: Label propagation
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签传播
- en: SVD++ (collaborative filtering)
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVD++（协同过滤）
- en: Strongly connected components
  id: totrans-456
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强连通分量
- en: 'As GraphX is an open source library, changes to the list are expected. GraphFrames
    is a new implementation from Databricks that fully supports the following three
    languages: Scala, Java, and Python, and is build on top of DataFrames. I''ll discuss
    specific implementations in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms*.'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GraphX是一个开源库，预计列表会有所变化。GraphFrames是Databricks的新实现，完全支持以下三种语言：Scala、Java和Python，并且建立在DataFrames之上。我将在[第7章](part0283.xhtml#aid-8DSF61
    "第7章。使用图算法")*使用图算法*中讨论具体的实现。
- en: Spark performance tuning
  id: totrans-458
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark性能调优
- en: 'While efficient execution of the data pipeline is prerogative of the task scheduler,
    which is part of the Spark driver, sometimes Spark needs hints. Spark scheduling
    is primarily driven by the two parameters: CPU and memory. Other resources, such
    as disk and network I/O, of course, play an important part in Spark performance
    as well, but neither Spark, Mesos or YARN can currently do anything to actively
    manage them.'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据管道的高效执行是任务调度器的特权，它是Spark驱动程序的一部分，但有时Spark需要提示。Spark调度主要受两个参数驱动：CPU和内存。当然，其他资源，如磁盘和网络I/O，在Spark性能中也扮演着重要角色，但Spark、Mesos或YARN目前无法主动管理它们。
- en: The first parameter to watch is the number of RDD partitions, which can be specified
    explicitly when reading the RDD from a file. Spark usually errs on the side of
    too many partitions as it provides more parallelism, and it does work in many
    cases as the task setup/teardown times are relatively small. However, one might
    experiment with decreasing the number of partitions, especially if one does aggregations.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 需要关注的第一个参数是RDD分区数，在从文件读取RDD时可以显式指定。Spark通常倾向于过多的分区，因为它提供了更多的并行性，而且在许多情况下确实有效，因为任务设置/拆除时间相对较小。然而，人们可以尝试减少分区数，尤其是在进行聚合时。
- en: The default number of partitions per RDD and the level of parallelism is determined
    by the `spark.default.parallelism` parameter, defined in the `$SPARK_HOME/conf/spark-defaults.conf`
    configuration file. The number of partitions for a specific RDD can also be explicitly
    changed by the `coalesce()` or `repartition()` methods.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，每个RDD的分区数和并行级别由`spark.default.parallelism`参数决定，该参数定义在`$SPARK_HOME/conf/spark-defaults.conf`配置文件中。特定RDD的分区数也可以通过`coalesce()`或`repartition()`方法显式更改。
- en: The total number of cores and available memory is often the reason for deadlocks
    as the tasks cannot proceed further. One can specify the number of cores for each
    executor with the `--executor-cores` flag when invoking spark-submit, spark-shell,
    or PySpark from the command line. Alternatively, one can set the corresponding
    parameters in the `spark-defaults.conf` file discussed earlier. If the number
    of cores is set too high, the scheduler will not be able to allocate resources
    on the nodes and will deadlock.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 总核心数和可用内存通常是死锁的原因，因为任务无法进一步执行。在通过命令行调用spark-submit、spark-shell或PySpark时，可以使用`--executor-cores`标志指定每个executor的核心数。或者，也可以在前面讨论的`spark-defaults.conf`文件中设置相应的参数。如果核心数设置得太高，调度器将无法在节点上分配资源，从而导致死锁。
- en: In a similar way, `--executor-memory` (or the `spark.executor.memory` property)
    specifies the requested heap size for all the tasks (the default is 1g). If the
    executor memory is specified too high, again, the scheduler may be deadlocked
    or will be able to schedule only a limited number of executors on a node.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，`--executor-memory`（或`spark.executor.memory`属性）指定了所有任务请求的堆大小（默认为1g）。如果executor内存设置得太高，同样，调度器可能会死锁，或者只能在节点上调度有限数量的executor。
- en: 'The implicit assumption in Standalone mode when counting the number of cores
    and memory is that Spark is the only running application—which may or may not
    be true. When running under Mesos or YARN, it is important to configure the cluster
    scheduler that it has the resources available to schedule the executors requested
    by the Spark Driver. The relevant YARN properties are: `yarn.nodemanager.resource.cpu-vcores`
    and `yarn.nodemanager.resource.memory-mb`. YARN may round the requested memory
    up a little. YARN''s `yarn.scheduler.minimum-allocation-mb` and `yarn.scheduler.increment-allocation-mb`
    properties control the minimum and increment request values respectively.'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立模式下，在计算核心数和内存时隐含的假设是Spark是唯一运行的应用程序——这可能是也可能不是真的。当在Mesos或YARN下运行时，配置集群调度器以使其具有Spark驱动器请求的执行器资源非常重要。相关的YARN属性是：`yarn.nodemanager.resource.cpu-vcores`和`yarn.nodemanager.resource.memory-mb`。YARN可能会将请求的内存向上取整一点。YARN的`yarn.scheduler.minimum-allocation-mb`和`yarn.scheduler.increment-allocation-mb`属性分别控制最小和增量请求值。
- en: JVMs can also use some memory off heap, for example, for interned strings and
    direct byte buffers. The value of the `spark.yarn.executor.memoryOverhead` property
    is added to the executor memory to determine the full memory request to YARN for
    each executor. It defaults to max (*384, .07 * spark.executor.memory*).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: JVM也可以使用一些堆外内存，例如，用于内部字符串和直接字节数据缓冲区。`spark.yarn.executor.memoryOverhead`属性的值被添加到执行器内存中，以确定每个执行器对YARN的完整内存请求。默认值为最大值（384
    + 0.07 * spark.executor.memory）。
- en: Since Spark can internally transfer the data between executors and client node,
    efficient serialization is very important. I will consider different serialization
    frameworks in [Chapter 6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working with Unstructured
    Data"), *Working with Unstructured Data*, but Spark uses Kryo serialization by
    default, which requires the classes to be registered explicitly in a static method.
    If you see a serialization error in your code, it is likely because the corresponding
    class has not been registered or Kryo does not support it, as it happens with
    too nested and complex data types. In general, it is recommended to avoid complex
    objects to be passed between the executors unless the object serialization can
    be done very efficiently.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark可以在执行器和客户端节点之间内部传输数据，因此高效的序列化非常重要。我将在[第6章](part0273.xhtml#aid-84B9I2
    "第6章。处理非结构化数据")中考虑不同的序列化框架，*处理非结构化数据*，但Spark默认使用Kryo序列化，这要求类必须显式地在静态方法中注册。如果你在代码中看到序列化错误，很可能是因为相应的类尚未注册或Kryo不支持它，就像在过于嵌套和复杂的数据类型中发生的那样。一般来说，建议避免在执行器之间传递复杂对象，除非对象序列化可以非常高效地进行。
- en: 'Driver has similar parameters: `spark.driver.cores`, `spark.driver.memory`,
    and `spark.driver.maxResultSize`. The latter one sets the limit for the results
    collected from all the executors with the `collect` method. It is important to
    protect the driver process from out-of-memory exceptions. The other way to avoid
    out-of-memory exceptions and consequent problems are to either modify the pipeline
    to return aggregated or filtered results or use the `take` method instead.'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 驱动器有类似的参数：`spark.driver.cores`、`spark.driver.memory`和`spark.driver.maxResultSize`。后者设置了使用`collect`方法收集的所有执行器的结果限制。保护驱动器进程免受内存不足异常非常重要。避免内存不足异常及其后续问题的另一种方法是修改管道以返回聚合或过滤后的结果，或者使用`take`方法代替。
- en: Running Hadoop HDFS
  id: totrans-468
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 运行Hadoop HDFS
- en: A distributed processing framework wouldn't be complete without distributed
    storage. One of them is HDFS. Even if Spark is run on local mode, it can still
    use a distributed file system at the backend. Like Spark breaks computations into
    subtasks, HDFS breaks a file into blocks and stores them across a set of machines.
    For HA, HDFS stores multiple copies of each block, the number of copies is called
    replication level, three by default (refer to *Figure 3-5*).
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 一个分布式处理框架如果没有分布式存储就不会完整。其中之一是HDFS。即使Spark在本地模式下运行，它仍然可以使用后端分布式文件系统。就像Spark将计算分解为子任务一样，HDFS将文件分解为块并在多台机器上存储它们。对于高可用性，HDFS为每个块存储多个副本，副本的数量称为复制级别，默认为三个（参见图3-5）。
- en: '**NameNode** is managing the HDFS storage by remembering the block locations
    and other metadata such as owner, file permissions, and block size, which are
    file-specific. **Secondary Namenode** is a slight misnomer: its function is to
    merge the metadata modifications, edits, into fsimage, or a file that serves as
    a metadata database. The merge is required, as it is more practical to write modifications
    of fsimage to a separate file instead of applying each modification to the disk
    image of the fsimage directly (in addition to applying the corresponding changes
    in memory). Secondary **Namenode** cannot serve as a second copy of the **Namenode**.
    A **Balancer** is run to move the blocks to maintain approximately equal disk
    usage across the servers—the initial block assignment to the nodes is supposed
    to be random, if enough space is available and the client is not run within the
    cluster. Finally, the **Client** communicates with the **Namenode** to get the
    metadata and block locations, but after that, either reads or writes the data
    directly to the node, where a copy of the block resides. The client is the only
    component that can be run outside the HDFS cluster, but it needs network connectivity
    with all the nodes in the cluster.'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: '**NameNode** 通过记住块位置和其他元数据（如所有者、文件权限和块大小，这些是文件特定的）来管理 HDFS 存储。**Secondary Namenode**
    是一个轻微的错误名称：其功能是将元数据修改、编辑合并到 fsimage 中，或者是一个充当元数据库的文件。合并是必要的，因为将 fsimage 的修改写入单独的文件比直接将每个修改应用于
    fsimage 的磁盘镜像更实际（除了在内存中应用相应的更改）。二级 **Namenode** 不能作为 **Namenode** 的第二个副本。运行 **Balancer**
    以将块移动到服务器之间保持大约相等的磁盘使用率——节点上的初始块分配应该是随机的，如果空间足够且客户端不在集群内运行。最后，**Client** 与 **Namenode**
    通信以获取元数据和块位置，但之后，要么直接读取或写入数据到节点，其中包含块的副本。客户端是唯一可以在 HDFS 集群外运行的组件，但它需要与集群中所有节点建立网络连接。'
- en: 'If any of the node dies or disconnects from the network, the **Namenode** notices
    the change, as it constantly maintains the contact with the nodes via heartbeats.
    If the node does not reconnect to the **Namenode** within 10 minutes (by default),
    the **Namenode** will start replicating the blocks in order to achieve the required
    replication level for the blocks that were lost on the node. A separate block
    scanner thread in the **Namenode** will scan the blocks for possible bit rot—each
    block maintains a checksum—and will delete corrupted and orphaned blocks:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 如果任何节点死亡或从网络断开连接，**Namenode** 会注意到这种变化，因为它通过心跳不断与节点保持联系。如果节点在 10 分钟内（默认值）未能重新连接到
    **Namenode**，则 **Namenode** 将开始复制块，以实现节点上丢失的块所需的复制级别。**Namenode** 中的单独块扫描线程将扫描块以查找可能的位错——每个块都维护一个校验和——并将损坏的和孤立的块删除：
- en: '![Running Hadoop HDFS](img/image01679.jpeg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![运行 Hadoop HDFS](img/image01679.jpeg)'
- en: Figure 03-5\. This is the HDFS architecture. Each block is stored in three separate
    locations (the replication level).
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 图 03-5\. 这是 HDFS 架构。每个块存储在三个不同的位置（复制级别）。
- en: 'To start HDFS on your machine (with replication level 1), download a Hadoop
    distribution, for example, from [http://hadoop.apache.org](http://hadoop.apache.org):'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在您的机器上启动 HDFS（复制级别为 1）时，请下载一个 Hadoop 发行版，例如，从 [http://hadoop.apache.org](http://hadoop.apache.org)：
- en: '[PRE36]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'To get the minimal HDFS configuration, modify the `core-site.xml` and `hdfs-site.xml`
    files, as follows:'
  id: totrans-476
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要获取最小的 HDFS 配置，修改 `core-site.xml` 和 `hdfs-site.xml` 文件，如下所示：
- en: '[PRE37]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: This will put the Hadoop HDFS metadata and data directories under the `/tmp/hadoop-$USER`
    directories. To make this more permanent, we can add the `dfs.namenode.name.dir`,
    `dfs.namenode.edits.dir`, and `dfs.datanode.data.dir` parameters, but we will
    leave these out for now. For a more customized distribution, one can download
    a Cloudera version from [http://archive.cloudera.com/cdh](http://archive.cloudera.com/cdh).
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这将把 Hadoop HDFS 元数据和数据目录放在 `/tmp/hadoop-$USER` 目录下。为了使其更加持久，我们可以添加 `dfs.namenode.name.dir`、`dfs.namenode.edits.dir`
    和 `dfs.datanode.data.dir` 参数，但现在我们将省略这些。对于更定制的发行版，可以从 [http://archive.cloudera.com/cdh](http://archive.cloudera.com/cdh)
    下载 Cloudera 版本。
- en: 'First, we need to write an empty metadata:'
  id: totrans-479
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们需要编写一个空元数据：
- en: '[PRE38]'
  id: totrans-480
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Then start the `namenode`, `secondarynamenode`, and `datanode` Java processes
    (I usually open three different command-line windows to see the logs, but in a
    production environment, these are usually daemonized):'
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后启动 `namenode`、`secondarynamenode` 和 `datanode` Java 进程（我通常打开三个不同的命令行窗口来查看日志，但在生产环境中，这些通常被作为守护进程运行）：
- en: '[PRE39]'
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We are now ready to create the first HDFS file:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在我们已经准备好创建第一个 HDFS 文件：
- en: '[PRE40]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Of course, in this particular case, the actual file is stored only on one node,
    which is the same node we run `datanode` on (localhost). In my case, it is the
    following:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当然，在这种情况下，实际文件只存储在一个节点上，这个节点就是我们运行 `datanode` 的节点（localhost）。在我的情况下，如下所示：
- en: '[PRE41]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The Namenode UI can be found at `http://localhost:50070` and displays a host
    of information, including the HDFS usage and the list of DataNodes, the slaves
    of the HDFS Master node as follows:![Running Hadoop HDFS](img/image01680.jpeg)
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Namenode UI 可在 `http://localhost:50070` 找到，并显示大量信息，包括 HDFS 使用情况和 DataNodes 列表，即
    HDFS 主节点的奴隶，如下所示：![运行 Hadoop HDFS](img/image01680.jpeg)
- en: Figure 03-6\. A snapshot of HDFS NameNode UI.
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 03-6\. HDFS NameNode UI 的快照。
- en: The preceding figure shows HDFS Namenode HTTP UI in a single node deployment
    (usually, `http://<namenode-address>:50070`). The **Utilities** | **Browse the
    file system** tab allows you to browse and download the files from HDFS. Nodes
    can be added by starting DataNodes on a different node and pointing to the Namenode
    with the `fs.defaultFS=<namenode-address>:8020` parameter. The Secondary Namenode
    HTTP UI is usually at `http:<secondarynamenode-address>:50090`.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图显示了单节点部署中的 HDFS Namenode HTTP UI（通常为 `http://<namenode-address>:50070`）。**实用工具**
    | **浏览文件系统**选项卡允许您浏览和下载 HDFS 中的文件。可以通过在另一个节点上启动 DataNodes 并使用 `fs.defaultFS=<namenode-address>:8020`
    参数指向 Namenode 来添加节点。辅助 Namenode HTTP UI 通常位于 `http:<secondarynamenode-address>:50090`。
- en: Scala/Spark by default will use the local file system. However, if the `core-site/xml`
    file is on the classpath or placed in the `$SPARK_HOME/conf` directory, Spark
    will use HDFS as the default.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: Scala/Spark 默认将使用本地文件系统。然而，如果 `core-site/xml` 文件位于类路径中或放置在 `$SPARK_HOME/conf`
    目录中，Spark 将使用 HDFS 作为默认。
- en: Summary
  id: totrans-491
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I covered the Spark/Hadoop and their relationship with Scala
    and functional programming at a very high level. I considered a classic word count
    example and it's implementation in Scala and Spark. I also provided high-level
    components of Spark ecosystem with specific examples of word count and streaming.
    I now have all the components to start looking at the specific implementation
    of classic machine learning algorithms in Scala/Spark. In the next chapter, I
    will start by covering supervised and unsupervised learning—a traditional division
    of learning algorithms for structured data.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我以非常高的水平介绍了 Spark/Hadoop 以及它们与 Scala 和函数式编程的关系。我考虑了一个经典的词频统计示例及其在 Scala
    和 Spark 中的实现。我还提供了 Spark 生态系统的高级组件，包括词频统计和流处理的特定示例。我现在有了所有组件来开始查看 Scala/Spark
    中经典机器学习算法的具体实现。在下一章中，我将首先介绍监督学习和无监督学习——这是结构化数据学习算法的传统划分。
- en: Chapter 4. Supervised and Unsupervised Learning
  id: totrans-493
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章 监督学习和无监督学习
- en: I covered the basics of the MLlib library in the previous chapter, but MLlib,
    at least at the time of writing this book, is more like a fast-moving target that
    is gaining the lead rather than a well-structured implementation that everyone
    uses in production or even has a consistent and tested documentation. In this
    situation, as people say, rather than giving you the fish, I will try to focus
    on well-established concepts behind the libraries and teach the process of fishing
    in this book in order to avoid the need to drastically modify the chapters with
    each new MLlib release. For better or worse, this increasingly seems to be a skill
    that a data scientist needs to possess.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 我在上一章中介绍了 MLlib 库的基础知识，但至少在撰写本书时，MLlib 更像是一个快速移动的目标，它正在取得领先地位，而不是一个结构良好的实现，每个人都用于生产中，甚至没有一个一致且经过测试的文档。在这种情况下，正如人们所说，与其给你鱼，我更愿意专注于库背后的成熟概念，并在本书中教授钓鱼的过程，以避免每次新的
    MLlib 发布都需要大幅修改章节。不管好坏，这越来越像是一个数据科学家需要掌握的技能。
- en: Statistics and machine learning inherently deal with uncertainty, due to one
    or another reason we covered in [Chapter 2](part0242.xhtml#aid-76P842 "Chapter 2. Data
    Pipelines and Modeling"), *Data Pipelines and Modeling*. While some datasets might
    be completely random, the goal here is to find trends, structure, and patterns
    beyond what a random number generator will provide you. The fundamental value
    of ML is that we can generalize these patterns and improve on at least some metrics.
    Let's see what basic tools are available within Scala/Spark.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 统计学和机器学习本质上处理不确定性，由于我们在[第2章](part0242.xhtml#aid-76P842 "第2章。数据管道和建模")中讨论的一个或另一个原因，*数据管道和建模*。虽然一些数据集可能是完全随机的，但这里的目的是找到趋势、结构和模式，这些是随机数生成器无法提供的。机器学习的基本价值在于我们可以泛化这些模式并在至少一些指标上取得改进。让我们看看Scala/Spark中可用的基本工具。
- en: In this chapter, I am covering supervised and unsupervised leaning, the two
    historically different approaches. Supervised learning is traditionally used when
    we have a specific goal to predict a label, or a specific attribute of a dataset.
    Unsupervised learning can be used to understand internal structure and dependencies
    between any attributes of a dataset, and is often used to group the records or
    attributes in meaningful clusters. In practice, both methods can be used to complement
    and aid each other.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 本章，我将介绍监督学习和无监督学习，这两种历史上不同的方法。监督学习传统上用于当我们有一个特定的目标来预测一个标签，或数据集的特定属性时。无监督学习可以用来理解数据集中任何属性之间的内部结构和依赖关系，并且通常用于将记录或属性分组到有意义的聚类中。在实践中，这两种方法都可以用来补充和辅助对方。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: Learning standard models for supervised learning – decision trees and logistic
    regression
  id: totrans-498
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习监督学习的标准模型——决策树和逻辑回归
- en: Discussing the staple of unsupervised learning – k-means clustering and its
    derivatives
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论无监督学习的基础——k-均值聚类及其衍生
- en: Understanding metrics and methods to evaluate the effectiveness of the above
    algorithms
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解评估上述算法有效性的指标和方法
- en: Having a glimpse of extending the above methods on special cases of streaming
    data, sparse data, and non-structured data
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 略窥上述方法在流数据、稀疏数据和非结构化数据特殊情况的扩展
- en: Records and supervised learning
  id: totrans-502
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录和监督学习
- en: 'For the purpose of this chapter, a record is an observation or measurement
    of one or several attributes. We assume that the observations might contain noise
    ![Records and supervised learning](img/image01681.jpeg) (or be inaccurate for
    one or other reason):'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 为了本章的目的，记录是对一个或多个属性的一个观察或测量。我们假设这些观察可能包含噪声 ![记录和监督学习](img/image01681.jpeg)（或者由于某种原因不准确）：
- en: '![Records and supervised learning](img/image01682.jpeg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![记录和监督学习](img/image01682.jpeg)'
- en: While we believe that there is some pattern or correlation between the attributes,
    the one that we are after and want to uncover, the noise is uncorrelated across
    either the attributes or the records. In statistical terms, we say that the values
    for each record are drawn from the same distribution and are independent (or *i.i.d*.
    in statistical terms). The order of records does not matter. One of the attributes,
    usually the first, might be designated to be the label.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们相信属性之间存在某种模式或相关性，但我们追求并希望揭示的那个，噪声在属性或记录之间是不相关的。在统计术语中，我们说每个记录的值来自相同的分布，并且是独立的（或统计术语中的
    *i.i.d*）。记录的顺序并不重要。其中一个属性，通常是第一个，可能被指定为标签。
- en: 'Supervised learning is when the goal is to predict the label *yi*:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习的目标是预测标签 *yi*：
- en: '![Records and supervised learning](img/image01683.jpeg)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![记录和监督学习](img/image01683.jpeg)'
- en: Here, *N* is the number of remaining attributes. In other words, the goal is
    to generalize the patterns so that we can predict the label by just knowing the
    other attributes, whether because we cannot physically get the measurement or
    just want to explore the structure of the dataset without having the immediate
    goal to predict the label.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*N* 是剩余属性的数量。换句话说，目标是泛化模式，以便我们只需知道其他属性就可以预测标签，无论是由于我们无法物理获取测量值，还是只想探索数据集的结构而不具有立即预测标签的目标。
- en: The unsupervised learning is when we don't use the label—we just try to explore
    the structure and correlations to understand the dataset to, potentially, predict
    the label better. The number of problems in this latter category has increased
    recently with the emergence of learning for unstructured data and streams, each
    of which, I'll be covering later in the book in separate chapters.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习是在我们不使用标签的情况下进行的——我们只是尝试探索结构和相关性，以理解数据集，从而可能更好地预测标签。随着无结构数据学习和流的学习的出现，这一类问题最近数量有所增加，我将在本书的单独章节中分别介绍。
- en: Iris dataset
  id: totrans-510
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Iris数据集
- en: 'I will demonstrate the concept of records and labels based on one of the most
    famous datasets in machine learning, the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)).
    The Iris dataset contains 50 records for each of the three types of Iris flower,
    150 lines of total five fields. Each line is a measurement of the following:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 我将通过机器学习中最著名的数据库之一，Iris数据集，来演示记录和标签的概念（[https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)）。Iris数据集包含三种Iris花类型各50条记录，总共150行，五个字段。每一行是对以下内容的测量：
- en: Sepal length in cm
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片长度（厘米）
- en: Sepal width in cm
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 萼片宽度（厘米）
- en: Petal length in cm
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣长度（厘米）
- en: Petal width in cm
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 花瓣宽度（厘米）
- en: 'With the final field being the type of the flower (*setosa*, *versicolor*,
    or *virginica*). The classic problem is to predict the label, which, in this case,
    is a categorical attribute with three possible values as a function of the first
    four attributes:'
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个字段是花的类型（*setosa*、*versicolor*或*virginica*）。经典问题是要预测标签，在这种情况下，这是一个具有三个可能值的分类属性，这些值是前四个属性的功能：
- en: '![Iris dataset](img/image01684.jpeg)'
  id: totrans-517
  prefs: []
  type: TYPE_IMG
  zh: '![Iris数据集](img/image01684.jpeg)'
- en: 'One option would be to draw a plane in the four-dimensional space that separates
    all four labels. Unfortunately, as one can find out, while one of the classes
    is clearly separable, the remaining two are not, as shown in the following multidimensional
    scatterplot (we have used Data Desk software to create it):'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 一种选择是在四维空间中绘制一个平面，该平面可以分隔所有四个标签。不幸的是，正如人们可以发现的，虽然其中一个类别可以清楚地分离，但剩下的两个类别则不行，如下面的多维散点图所示（我们使用了Data
    Desk软件创建它）：
- en: '![Iris dataset](img/image01685.jpeg)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![Iris数据集](img/image01685.jpeg)'
- en: Figure 04-1\. The Iris dataset as a three-dimensional plot. The Iris setosa
    records, shown by crosses, can be separated from the other two types based on
    petal length and width.
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 图04-1. Iris数据集的三维图。Iris setosa记录，用交叉表示，可以根据花瓣长度和宽度与其他两种类型分开。
- en: 'The colors and shapes are assigned according to the following table:'
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色和形状是根据以下表格分配的：
- en: '| Label | Color | Shape |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 标签 | 颜色 | 形状 |'
- en: '| --- | --- | --- |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Iris setosa* | Blue | x |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| *Iris setosa* | 蓝色 | x |'
- en: '| *Iris versicolor* | Green | Vertical bar |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| *Iris versicolor* | 绿色 | 竖条 |'
- en: '| *Iris virginica* | Purple | Horizontal bar |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| *Iris virginica* | 紫色 | 水平条形 |'
- en: The *Iris setosa* is separable because it happens to have a very short petal
    length and width compared to the two other types.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '*Iris setosa*是可分离的，因为它与其他两种类型相比，花瓣长度和宽度非常短。'
- en: Let's see how we can use MLlib to find that separating multidimensional plane.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何使用MLlib找到那个分隔多维平面的方法。
- en: Labeled point
  id: totrans-529
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记点
- en: 'The labeled datasets used to have a very special place in ML—we will discuss
    unsupervised learning later in the chapter, where we do not need a label, so MLlib
    has a special data type to represent a record with a `org.apache.spark.mllib.regression.LabeledPoint`
    label (refer to [https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)).
    To read the Iris dataset from a text file, we need to transform the original UCI
    repository file into the so-called LIBSVM text format. While there are plenty
    of converters from CSV to LIBSVM format, I''d like to use a simple AWK script
    to do the job:'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 之前使用的标记数据集在机器学习（ML）中占有非常重要的位置——我们将在本章后面讨论无监督学习，在那里我们不需要标签，因此MLlib有一个特殊的数据类型来表示带有`org.apache.spark.mllib.regression.LabeledPoint`标签的记录（请参阅[https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point](https://spark.apache.org/docs/latest/mllib-data-types.html#labeled-point)）。要从文本文件中读取Iris数据集，我们需要将原始UCI仓库文件转换为所谓的LIBSVM文本格式。虽然有很多从CSV到LIBSVM格式的转换器，但我希望使用一个简单的AWK脚本来完成这项工作：
- en: '[PRE42]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Note
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Why do we need LIBSVM format?**'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么我们需要LIBSVM格式？**'
- en: LIBSVM is the format that many libraries use. First, LIBSVM takes only continuous
    attributes. While a lot of datasets in the real world contain discrete or categorical
    attributes, internally they are always converted to a numerical representation
    for efficiency reasons, even if the L1 or L2 metrics on the resulting numerical
    attribute does not make much sense in the unordered discrete values. Second, the
    LIBSVM format allows for efficient sparse data representation. While the Iris
    dataset is not sparse, almost all of the modern big data sources are sparse, and
    the format allows for efficient storage by only storing the provided values. Many
    modern big data key-value and traditional RDBMS databases actually do the same
    for efficiency reasons.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: LIBSVM是许多库使用的格式。首先，LIBSVM只接受连续属性。虽然现实世界中的许多数据集包含离散或分类属性，但出于效率原因，它们在内部始终转换为数值表示，即使结果数值属性的L1或L2度量在无序的离散值上没有太多意义。其次，LIBSVM格式允许高效地表示稀疏数据。虽然Iris数据集不是稀疏的，但几乎所有现代大数据源都是稀疏的，该格式通过仅存储提供的值来实现高效存储。许多现代大数据键值和传统关系型数据库管理系统实际上出于效率原因也这样做。
- en: The code might be more complex for missing values, but we know that the Iris
    dataset is not sparse—otherwise we'd complement our code with a bunch of if statements.
    We mapped the last two labels to 1 for our purpose now.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 对于缺失值，代码可能更复杂，但我们知道Iris数据集不是稀疏的——否则我们会用一堆if语句来补充我们的代码。现在，我们将最后两个标签映射到1。
- en: SVMWithSGD
  id: totrans-536
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVMWithSGD
- en: 'Now, let''s run the **Linear Support Vector Machine** (**SVM**) SVMWithSGD
    code from MLlib:'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行MLlib中的**线性支持向量机**（**SVM**）SVMWithSGD代码：
- en: '[PRE43]'
  id: totrans-538
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'So, you just run one of the most complex algorithms in the machine learning
    toolbox: SVM. The result is a separating plane that distinguishes *Iris setosa*
    flowers from the other two types. The model in this case is exactly the intercept
    and the coefficients of the plane that best separates the labels:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你只需运行机器学习工具箱中最复杂的算法之一：SVM。结果是区分**Iris setosa**花与其他两种类型的分离平面。在这种情况下，模型正是最佳分离标签的截距和平面系数：
- en: '[PRE44]'
  id: totrans-540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'If one looks under the hood, the model is stored in a `parquet` file, which
    can be dumped using `parquet-tool`:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 如果深入了解，模型存储在一个`parquet`文件中，可以使用`parquet-tool`进行转储：
- en: '[PRE45]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The **Receiver Operating Characteristic** (**ROC**) is a common measure of the
    classifier to be able to correctly rank the records according to their numeric
    label. We will consider precision metrics in more detail in [Chapter 9](part0291.xhtml#aid-8LGJM2
    "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: '**受试者工作特征**（**ROC**）是评估分类器能否根据其数值标签正确排序记录的常用指标。我们将在[第9章](part0291.xhtml#aid-8LGJM2
    "第9章。Scala中的NLP")，*Scala中的NLP*中更详细地考虑精确度指标。'
- en: Tip
  id: totrans-544
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**What is ROC?**'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: '**ROC是什么？**'
- en: 'ROC has emerged in signal processing with the first application to measure
    the accuracy of analog radars. The common measure of accuracy is area under ROC,
    which, shortly, is the probability of two randomly chosen points to be ranked
    correctly according to their labels (the *0* label should always have a lower
    rank than the *1* label). AUROC has a number of attractive characteristics:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: ROC最初出现在信号处理领域，首次应用是测量模拟雷达的准确性。准确性的常用指标是ROC曲线下的面积，简而言之，是随机选择两个点按其标签正确排序的概率（**0**标签应始终具有比**1**标签低的排名）。AUROC具有许多吸引人的特性：
- en: The value, at least theoretically, does not depend on the oversampling rate,
    that is, the rate at which we see *0* labels as opposed to *1* labels.
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该值，至少从理论上讲，不依赖于过采样率，即我们看到**0**标签而不是**1**标签的比率。
- en: The value does not depend on the sample size, excluding the expected variance
    due to the limited sample size.
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 该值不依赖于样本大小，排除了由于样本量有限而产生的预期方差。
- en: Adding a constant to the final score does not change the ROC, thus the intercept
    can always be set to *0*. Computing the ROC requires a sort with respect to the
    generated score.
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在最终得分中添加一个常数不会改变ROC，因此截距可以始终设置为**0**。计算ROC需要对生成的得分进行排序。
- en: 'Of course, separating the remaining two labels is a harder problem since the
    plane that separated *Iris versicolor* from *Iris virginica* does not exist: the
    AUROC score will be less than *1.0*. However, the SVM method will find the plane
    that best differentiates between the latter two classes.'
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，分离剩余的两个标签是一个更难的问题，因为将**Iris versicolor**与**Iris virginica**分开的平面不存在：AUROC分数将小于**1.0**。然而，SVM方法将找到最佳区分后两个类别的平面。
- en: Logistic regression
  id: totrans-551
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Logistic regression is one of the oldest classification methods. The outcome
    of the logistic regression is also a set of weights, which define the hyperplane,
    but the loss function is logistic loss instead of *L2*:'
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是最古老的分类方法之一。逻辑回归的结果也是一组权重，这些权重定义了超平面，但损失函数是逻辑损失而不是 *L2*：
- en: '![Logistic regression](img/image01686.jpeg)'
  id: totrans-553
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image01686.jpeg)'
- en: 'Logit function is a frequent choice when the label is binary (as *y = +/- 1*
    in the above equation):'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 当标签是二元时（如上式中的 *y = +/- 1*），对数函数是一个常见的选择：
- en: '[PRE46]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: The labels in this case can be any integer in the range *[0, k)*, where *k*
    is the total number of classes (the correct class will be determined by building
    multiple binary logistic regression models against the pivot class, which in this
    case, is the class with the *0* label) (*The Elements of Statistical Learning*
    by *Trevor Hastie*, *Robert Tibshirani*, *Jerome Friedman*, *Springer Series in
    Statistics*).
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，标签可以是范围 *[0, k)* 中的任何整数，其中 *k* 是类的总数（正确的类别将通过针对基准类（在这种情况下，是带有 *0* 标签的类别）构建多个二元逻辑回归模型来确定）(*《统计学习的要素》*，作者：*Trevor
    Hastie*，*Robert Tibshirani*，*Jerome Friedman*，*Springer Series in Statistics*)。
- en: The accuracy metric is precision, or the percentage of records predicted correctly
    (which is 95% in our case).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 准确性指标是精确度，即正确预测的记录百分比（在我们的案例中为95%）。
- en: Decision tree
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策树
- en: 'The preceding two methods describe linear models. Unfortunately, the linear
    approach does not always work for complex interactions between attributes. Assume
    that the label looks like an exclusive *OR: 0* if *X ? Y* and *1* if *X = Y*:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种方法描述了线性模型。不幸的是，线性方法并不总是适用于属性之间的复杂交互。假设标签看起来像这样的独热编码：如果 *X ? Y* 则为 *0*，如果
    *X = Y* 则为 *1*：
- en: '| X | Y | Label |'
  id: totrans-560
  prefs: []
  type: TYPE_TB
  zh: '| X | Y | 标签 |'
- en: '| --- | --- | --- |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | 0 | 0 |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 |'
- en: '| 0 | 1 | 0 |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 |'
- en: '| 1 | 1 | 1 |'
  id: totrans-564
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 1 |'
- en: '| 0 | 0 | 1 |'
  id: totrans-565
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 1 |'
- en: 'There is no hyperplane that can differentiate between the two labels in the
    *XY* space. Recursive split solution, where the split on each level is made on
    only one variable or a linear combination thereof might work a bit better in these
    case. Decision trees are also known to work well with sparse and interaction-rich
    datasets:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在 *XY* 空间中，没有超平面可以区分这两个标签。在这种情况下，递归分割解决方案，其中每个级别的分割仅基于一个变量或其线性组合，可能会稍微好一些。决策树也已知与稀疏和交互丰富的数据集配合得很好：
- en: '[PRE47]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As you can see, the error (misprediction) rate on hold-out 30% sample is only
    2.6%. The 30% sample of 150 is only 45 records, which means we missed only 1 record
    from the whole test set. Certainly, the result might and will change with a different
    seed, and we need a more rigorous cross-validation technique to prove the accuracy
    of the model, but this is enough for a rough estimate of model performance.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，在保留30%样本的情况下，错误（误预测）率仅为2.6%。150个样本中的30%只有45条记录，这意味着我们只从整个测试集中遗漏了1条记录。当然，结果可能会随着不同的种子而改变，并且我们需要更严格的交叉验证技术来证明模型的准确性，但这已经足够对模型性能进行粗略估计。
- en: Decision tree generalizes on regression case, that is, when the label is continuous
    in nature. In this case, the splitting criterion is minimization of weighted variance,
    as opposed to entropy gain or gini in the case of classification. I will talk
    more about the differences in [Chapter 5](part0260.xhtml#aid-7NUI81 "Chapter 5. Regression
    and Classification"), *Regression and Classification*.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树在回归案例中进行了泛化，即当标签在本质上连续时。在这种情况下，分割标准是最小化加权方差，而不是分类情况中的熵增益或基尼指数。我将在[第5章](part0260.xhtml#aid-7NUI81
    "第5章。回归和分类")中更多地讨论回归和分类之间的差异。
- en: 'There are a number of parameters, which can be tuned to improve the performance:'
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多参数可以调整以提高性能：
- en: '| Parameter | Description | Recommended value |'
  id: totrans-571
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 描述 | 推荐值 |'
- en: '| --- | --- | --- |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `maxDepth` | This is the maximum depth of the tree. Deep trees are costly
    and usually are more likely to overfit. Shallow trees are more efficient and better
    for bagging/boosting algorithms such as AdaBoost. | This depends on the size of
    the original dataset. It is worth experimenting and plotting the accuracy of the
    resulting tree versus the parameter to find out the optimum. |'
  id: totrans-573
  prefs: []
  type: TYPE_TB
  zh: '| `maxDepth` | 这是树的最大深度。深度树成本较高，通常更容易过拟合。浅层树更高效，更适合像AdaBoost这样的bagging/boosting算法。
    | 这取决于原始数据集的大小。值得实验并绘制结果的树准确性与参数之间的关系图，以找出最佳值。 |'
- en: '| `minInstancesPerNode` | This also limits the size of the tree: once the number
    of instances falls under this threshold, no further splitting occurs. | The value
    is usually 10-100, depending on the complexity of the original dataset and the
    number of potential labels. |'
  id: totrans-574
  prefs: []
  type: TYPE_TB
  zh: '| `minInstancesPerNode` | 这也限制了树的大小：一旦实例数量低于此阈值，就不会再进行进一步分割。 | 该值通常为10-100，具体取决于原始数据集的复杂性和潜在标签的数量。
    |'
- en: '| `maxBins` | This is used only for continuous attributes: the number of bins
    to split the original range. | Large number of bins increase computation and communication
    cost. One can also consider the option of pre-discretizing the attribute based
    on domain knowledge. |'
  id: totrans-575
  prefs: []
  type: TYPE_TB
  zh: '| `maxBins` | 这仅用于连续属性：分割原始范围的箱数。 | 大量的箱会增加计算和通信成本。也可以考虑根据领域知识对属性进行预离散化的选项。
    |'
- en: '| `minInfoGain` | This is the amount of information gain (entropy), impurity
    (gini), or variance (regression) gain to split a node. | The default is *0*, but
    you can increase the default to limit the tree size and reduce the risk of overfitting.
    |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '| `minInfoGain` | 这是分割节点所需的信息增益（熵）、不纯度（基尼）或方差（回归）增益。 | 默认值为*0*，但你可以增加默认值以限制树的大小并降低过拟合的风险。
    |'
- en: '| `maxMemoryInMB` | This is the amount of memory to be used for collecting
    sufficient statistics. | The default value is conservatively chosen to be 256
    MB to allow the decision algorithm to work in most scenarios. Increasing `maxMemoryInMB`
    can lead to faster training (if the memory is available) by allowing fewer passes
    over the data. However, there may be decreasing returns as `maxMemoryInMB` grows,
    as the amount of communication on each iteration can be proportional to `maxMemoryInMB`.
    |'
  id: totrans-577
  prefs: []
  type: TYPE_TB
  zh: '| `maxMemoryInMB` | 这是用于收集足够统计信息的内存量。 | 默认值保守地选择为256 MB，以便决策算法在大多数场景下都能工作。增加`maxMemoryInMB`可以通过允许对数据进行更少的遍历来加快训练速度（如果内存可用）。然而，随着`maxMemoryInMB`的增加，每次迭代的通信量可能成比例增加，这可能导致收益递减。
    |'
- en: '| `subsamplingRate` | This is the fraction of the training data used for learning
    the decision tree. | This parameter is most relevant for training ensembles of
    trees (using `RandomForest` and `GradientBoostedTrees`), where it can be useful
    to subsample the original data. For training a single decision tree, this parameter
    is less useful since the number of training instances is generally not the main
    constraint. |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '| `subsamplingRate` | 这是用于学习决策树的训练数据的一部分。 | 此参数对于训练树集合（使用`RandomForest`和`GradientBoostedTrees`）最为相关，其中对原始数据进行子采样可能很有用。对于训练单个决策树，此参数不太有用，因为训练实例的数量通常不是主要限制因素。
    |'
- en: '| `useNodeIdCache` | If this is set to true, the algorithm will avoid passing
    the current model (tree or trees) to executors on each iteration. | This can be
    useful with deep trees (speeding up computation on workers) and for large random
    forests (reducing communication on each iteration). |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '| `useNodeIdCache` | 如果设置为true，算法将避免在每次迭代中将当前模型（树或树集合）传递给执行器。 | 这对于深度树（加快工作节点的计算速度）和大型随机森林（减少每次迭代的通信量）很有用。
    |'
- en: '| `checkpointDir:` | This is the directory for checkpointing the node ID cache
    RDDs. | This is an optimization to save intermediate results to avoid recomputation
    in case of node failure. Set it in large clusters or with unreliable nodes. |'
  id: totrans-580
  prefs: []
  type: TYPE_TB
  zh: '| `checkpointDir:` | 这是用于检查点化节点ID缓存RDD的目录。 | 这是一个优化，用于将中间结果保存下来，以避免节点故障时的重新计算。在大集群或不稳定的节点上设置。
    |'
- en: '| `checkpointInterval` | This is the frequency for checkpointing the node ID
    cache RDDs. | Setting this too low will cause extra overhead from writing to HDFS
    and setting this too high can cause problems if executors fail and the RDD needs
    to be recomputed. |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '| `checkpointInterval` | 这是检查点化节点ID缓存RDD的频率。 | 设置得太低会导致写入HDFS的额外开销，设置得太高则可能在执行器失败且需要重新计算RDD时引起问题。
    |'
- en: Bagging and boosting – ensemble learning methods
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Bagging和boosting – 集成学习方法
- en: As a portfolio of stocks has better characteristics compared to individual equities,
    models can be combined to produce better classifiers. Usually, these methods work
    really well with decision trees as the training technique can be modified to produce
    models with large variations. One way is to train the model on random subsets
    of the original data or random subsets of attributes, which is called random forest.
    Another way is to generate a sequence of models, where misclassified instances
    are reweighted to get a larger weight in each subsequent iteration. It has been
    shown that this method has a relation to gradient descent methods in the model
    parameter space. While these are valid and interesting techniques, they usually
    require much more space in terms of model storage and are less interpretable compared
    to bare decision tree models. For Spark, the ensemble models are currently under
    development—the umbrella issue is SPARK-3703 ([https://issues.apache.org/jira/browse/SPARK-3703](https://issues.apache.org/jira/browse/SPARK-3703)).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 由于股票组合的特性优于单个股票，可以将模型结合起来产生更好的分类器。通常，这些方法与决策树作为训练技术结合得很好，因为训练技术可以被修改以产生具有较大变异的模型。一种方法是在原始数据的随机子集或属性的随机子集上训练模型，这被称为随机森林。另一种方法是通过生成一系列模型，将误分类实例重新加权，以便在后续迭代中获得更大的权重。已经证明这种方法与模型参数空间中的梯度下降方法有关。虽然这些是有效且有趣的技术，但它们通常需要更多的模型存储空间，并且与裸决策树模型相比，可解释性较差。对于
    Spark，集成模型目前正处于开发中——主要问题为 SPARK-3703 ([https://issues.apache.org/jira/browse/SPARK-3703](https://issues.apache.org/jira/browse/SPARK-3703))。
- en: Unsupervised learning
  id: totrans-584
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 无监督学习
- en: If we get rid of the label in the Iris dataset, it would be nice if some algorithm
    could recover the original grouping, maybe without the exact label names—*setosa*,
    *versicolor*, and *virginica*. Unsupervised learning has multiple applications
    in compression and encoding, CRM, recommendation engines, and security to uncover
    internal structure without actually having the exact labels. The labels sometimes
    can be given base on the singularity in attribute value distributions. For example,
    *Iris setosa* can be described as a *Flower with Small Leaves*.
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在 Iris 数据集中去掉标签，如果某些算法能够恢复原始分组，即使没有确切的标签名称——*setosa*、*versicolor* 和 *virginica*——那将很理想。无监督学习在压缩和编码、客户关系管理（CRM）、推荐引擎和安全领域有多个应用，可以在不实际拥有确切标签的情况下揭示内部结构。标签有时可以根据属性值分布的奇异性给出。例如，*Iris
    setosa* 可以描述为 *小叶花*。
- en: While a supervised learning problem can always be cast as unsupervised by disregarding
    the label, the reverse is also true. A clustering algorithm can be cast as a density-estimation
    problem by assigning label *1* to all vectors and generating random vectors with
    label *0* (*The Elements of Statistical Learning* by *Trevor Hastie*, *Robert
    Tibshirani*, *Jerome Friedman*, *Springer Series in Statistics*). The difference
    between the two is formal and it's even fuzzier with non-structured and nested
    data. Often, running unsupervised algorithms in labeled datasets leads to a better
    understanding of the dependencies and thus a better selection and performance
    of the supervised algorithm.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以通过忽略标签将监督学习问题视为无监督问题，但反之亦然。可以将聚类算法视为密度估计问题，通过将所有向量分配标签 *1* 并生成带有标签 *0* 的随机向量（参见
    *Trevor Hastie*、*Robert Tibshirani*、*Jerome Friedman* 的 *《统计学习基础》*，*Springer 统计系列*）。这两种方法之间的区别是正式的，对于非结构化和嵌套数据来说甚至更加模糊。通常，在标记数据集上运行无监督算法可以更好地理解依赖关系，从而更好地选择和表现监督算法。
- en: 'One of the most popular algorithms for clustering and unsupervised learning
    in k-means (and its variants, k-median and k-center, will be described later):'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类和无监督学习中最受欢迎的算法之一是 k-means（以及其变体，k-median 和 k-center，将在后面描述）：
- en: '[PRE48]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: One can see that the first center, the one with index `0`, has petal length
    and width of `1.464` and `0.244`, which is much shorter than the other two—`5.715`
    and `2.054`, `4.389` and `1.434`). The prediction completely matches the first
    cluster, corresponding to *Iris setosa*, but has a few mispredictions for the
    other two.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 可以看到第一个中心，即索引为 `0` 的中心，花瓣长度和宽度为 `1.464` 和 `0.244`，这比其他两个——`5.715` 和 `2.054`、`4.389`
    和 `1.434`——要短得多。预测完全符合第一个聚类，对应于 *Iris setosa*，但对于其他两个有一些误判。
- en: 'The measure of cluster quality might depend on the (desired) labels if we want
    to achieve a desired classification result, but since the algorithm has no information
    about the labeling, a more common measure is the sum of distances from centroids
    to the points in each of the clusters. Here is a graph of `WSSSE`, depending on
    the number of clusters:'
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要实现一个期望的分类结果，簇质量的度量可能取决于（期望的）标签，但鉴于算法没有关于标签的信息，一个更常见的度量是每个簇中从质心到点的距离之和。以下是一个关于`WSSSE`的图表，它取决于簇的数量：
- en: '[PRE49]'
  id: totrans-591
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'As expected, the average distance is decreasing as more clusters are configured.
    A common method to determine the optimal number of clusters—in our example, we
    know that there are three types of flowers—is to add a penalty function. A common
    penalty is the log of the number of clusters as we expect a convex function. What
    would be the coefficient in front of log? If each vector is associated with its
    own cluster, the sum of all distances will be zero, so if we would like a metric
    that achieves approximately the same value at both ends of the set of possible
    values, `1` to `150`, the coefficient should be `680.8244/log(150)`:'
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，随着配置的簇越来越多，平均距离在减小。确定最佳簇数量的一个常见方法是添加一个惩罚函数。一个常见的惩罚是簇数量的对数，因为我们期望一个凸函数。对数前面的系数是多少？如果每个向量都与自己的簇相关联，所有距离的总和将为零，因此如果我们想要一个在可能值集合的两端都达到大约相同值的度量，`1`到`150`，系数应该是`680.8244/log(150)`：
- en: '[PRE50]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Here is how the sum of the squared distances with penalty looks as a graph:'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 这是带有惩罚的平方距离之和的图形表示：
- en: '![Unsupervised learning](img/image01687.jpeg)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/image01687.jpeg)'
- en: Figure 04-2\. The measure of the clustering quality as a function of the number
    of clusters
  id: totrans-596
  prefs: []
  type: TYPE_NORMAL
  zh: 图 04-2\. 随簇数量变化的聚类质量度量
- en: 'Besides k-means clustering, MLlib also has implementations of the following:'
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 除了k-means聚类外，MLlib还实现了以下功能：
- en: Gaussian mixture
  id: totrans-598
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯混合
- en: '**Power Iteration Clustering** (**PIC**)'
  id: totrans-599
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**幂迭代聚类**（**PIC**）'
- en: '**Latent Dirichlet Allocation** (**LDA**)'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**潜在狄利克雷分配**（**LDA**）'
- en: Streaming k-means
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式k-means
- en: 'The Gaussian mixture is another classical mechanism, particularly known for
    spectral analysis. Gaussian mixture decomposition is appropriate, where the attributes
    are continuous and we know that they are likely to come from a set of Gaussian
    distributions. For example, while the potential groups of points corresponding
    to clusters may have the average for all attributes, say **Var1** and **Var2**,
    the points might be centered around two intersecting hyperplanes, as shown in
    the following diagram:'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: 高斯混合是另一种经典机制，尤其以频谱分析而闻名。当属性是连续的，并且我们知道它们可能来自一组高斯分布时，高斯混合分解是合适的。例如，当对应于簇的点的潜在组可能具有所有属性的均值，比如**Var1**和**Var2**时，点可能围绕两个相交的超平面中心，如下面的图所示：
- en: '![Unsupervised learning](img/image01688.jpeg)'
  id: totrans-603
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/image01688.jpeg)'
- en: Figure 04-3\. A mixture of two Gaussians that cannot be properly described by
    k-means clustering
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 图 04-3\. 无法用k-means聚类正确描述的两个高斯混合
- en: This renders the k-means algorithm ineffective as it will not be able to distinguish
    between the two (of course a simple non-linear transformation such as a distance
    to one of the hyperplanes will solve the problem, but this is where domain knowledge
    and expertise as a data scientist are handy).
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得k-means算法无效，因为它无法区分这两者（当然，一个简单的非线性变换，如到其中一个超平面的距离，可以解决这个问题，但这就是领域知识和数据科学家专业知识派上用场的地方）。
- en: PIC is using clustering vertices of a graph provided pairwise similarity measures
    given as edge properties. It computes a pseudo-eigenvector of the normalized affinity
    matrix of the graph via power iteration and uses it to cluster vertices. MLlib
    includes an implementation of PIC using GraphX as its backend. It takes an RDD
    of (`srcId`, `dstId`, similarity) tuples and outputs a model with the clustering
    assignments. The similarities must be non-negative. PIC assumes that the similarity
    measure is symmetric. A pair (`srcId`, `dstId`) regardless of the ordering should
    appear at most once in the input data. If a pair is missing from the input, their
    similarity is treated as zero.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: PIC 使用图中的聚类顶点，并提供了作为边属性的成对相似度度量。它通过幂迭代计算图的归一化亲和矩阵的伪特征向量，并使用它来聚类顶点。MLlib 包含了一个使用
    GraphX 作为其后端的 PIC 实现。它接受一个包含 (`srcId`, `dstId`, similarity) 元组的 RDD，并输出一个具有聚类分配的模型。相似度必须是非负的。PIC
    假设相似度度量是对称的。无论顺序如何，一对 (`srcId`, `dstId`) 在输入数据中最多只能出现一次。如果一对在输入中缺失，它们的相似度被视为零。
- en: LDA can be used for clustering documents based on keyword frequencies. Rather
    than estimating a clustering using a traditional distance, LDA uses a function
    based on a statistical model of how text documents are generated.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 可以用于基于关键词频率对文档进行聚类。LDA 不是使用传统的距离来估计聚类，而是使用基于文本文档生成统计模型的函数。
- en: 'Finally, streaming k-means is a modification of the k-means algorithm, where
    the clusters can be adjusted with new batches of data. For each batch of data,
    we assign all points to their nearest cluster, compute new cluster centers based
    on the assignment, and then update each cluster parameters using the equations:'
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，流式 k-means 是 k-means 算法的一种改进，其中聚类可以根据新的数据批次进行调整。对于每个数据批次，我们将所有点分配到最近的聚类，根据分配计算新的聚类中心，然后使用以下方程更新每个聚类的参数：
- en: '![Unsupervised learning](img/image01689.jpeg)![Unsupervised learning](img/image01690.jpeg)'
  id: totrans-609
  prefs: []
  type: TYPE_IMG
  zh: '![无监督学习](img/image01689.jpeg)![无监督学习](img/image01690.jpeg)'
- en: Here, *c* *t* and *c'* *t* are the centers of from the old model and the ones
    computed for the new batch and *n* *t* and *n'* *t* are the number of vectors
    from the old model and for the new batch. By changing the *a* parameter, we can
    control how much information from the old runs can influence the clustering—*0*
    means the new cluster centers are totally based on the points in the new batch,
    while *1* means that we accommodate for all points that we have seen so far.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*c* *t* 和 *c'* *t* 是旧模型和为新批次计算的中心的坐标，而 *n* *t* 和 *n'* *t* 是旧模型和新批次中的向量数量。通过改变
    *a* 参数，我们可以控制旧运行的信息对聚类的影响程度——*0* 表示新的聚类中心完全基于新批次中的点，而 *1* 表示我们考虑到目前为止看到的所有点。
- en: k-means clustering has many modifications. For example, k-medians computes the
    cluster centers as medians of the attribute values, not mean, which works much
    better for some distributions and with *L1* target distance metric (absolute value
    of the difference) as opposed to *L2* (the sum of squares). K-medians centers
    are not necessarily present as a specific point in the dataset. K-medoids is another
    algorithm from the same family, where the resulting cluster center has to be an
    actual instance in the input set and we actually do not need to have the global
    sort, only the pairwise distances between the points. Many variations of the techniques
    exist on how to choose the original seed cluster centers and converge on the optimal
    number of clusters (besides the simple log trick I have shown).
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类算法有许多改进版本。例如，k-medians 计算聚类中心是属性值的中位数，而不是平均值，这对于某些分布和与 *L1* 目标距离度量（差值的绝对值）相比
    *L2*（平方和）来说效果更好。K-medians 的中心不一定是数据集中具体的一个点。K-medoids 是同一家族中的另一个算法，其中结果聚类中心必须是输入集中的一个实际实例，我们实际上不需要全局排序，只需要点之间的成对距离。关于如何选择原始种子聚类中心和收敛到最佳聚类数量（除了我展示的简单对数技巧之外）的技术有许多变体。
- en: Another big class of clustering algorithms is hierarchical clustering. Hierarchical
    clustering is either done from the top—akin to the decision tree algorithms—or
    from the bottom; we first find the closest neighbors, pair them, and continue
    the pairing process up the hierarchy until all records are merged. The advantage
    of hierarchical clustering is that it can be made deterministic and relatively
    fast, even though the cost of one iteration in k-means is probably going to be
    better. However, as mentioned, the unsupervised problem can actually be converted
    to a density-estimation supervised problem, with all the supervised learning techniques
    available. So have fun understanding the data!
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 另一大类聚类算法是层次聚类。层次聚类可以是自顶向下进行的——类似于决策树算法——或者自底向上；我们首先找到最近的邻居，将它们配对，然后继续配对过程，直到所有记录都被合并。层次聚类的优点是它可以被设计成确定性的，并且相对较快，尽管k-means的一次迭代的成本可能更好。然而，如前所述，无监督问题实际上可以被转换为一个密度估计的监督问题，可以使用所有可用的监督学习技术。所以，享受理解数据吧！
- en: Problem dimensionality
  id: totrans-613
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 问题维度
- en: The larger the attribute space or the number of dimensions, the harder it is
    to usually predict the label for a given combination of attribute values. This
    is mostly due to the fact that the total number of possible distinct combinations
    of attributes increases exponentially with the dimensionality of the attribute
    space—at least in the case of discrete variables (in case of continuous variables,
    the situation is more complex and depends on the metrics used), and it is becoming
    harder to generalize.
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 属性空间越大或维度数越多，通常预测给定属性值组合的标签就越困难。这主要是因为属性空间中可能的不同属性组合的总数随着属性空间维度的增加而指数增长——至少在离散变量的情况下（对于连续变量，情况更复杂，取决于使用的度量），并且泛化的难度也在增加。
- en: The effective dimensionality of the problem might be different from the dimensionality
    of the input space. For example, if the label depends only on the linear combination
    of the (continuous) input attributes, the problem is called linearly separable
    and its internal dimensionality is one—we still have to find the coefficients
    for this linear combination like in logistic regression though.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 问题的有效维度可能与输入空间的维度不同。例如，如果标签仅依赖于（连续的）输入属性的线性组合，则该问题称为线性可分，其内部维度为一——尽管我们仍然需要像逻辑回归那样找到这个线性组合的系数。
- en: This idea is also sometimes referred to as a **Vapnik–Chervonenkis** (**VC**)
    dimension of a problem, model, or algorithm—the expressive power of the model
    depending on how complex the dependencies that it can solve, or shatter, might
    be. More complex problems require algorithms with higher VC dimensions and larger
    training sets. However, using an algorithm with higher VC dimension on a simple
    problem can lead to overfitting and worse generalization to new data.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 这种想法有时也被称为问题的、模型的或算法的**Vapnik–Chervonenkis**（**VC**）维度——模型的表达能力取决于它能够解决或分解的依赖关系的复杂性。更复杂的问题需要具有更高VC维度的算法和更大的训练集。然而，在简单问题上使用具有更高VC维度的算法可能会导致过拟合，并且对新数据的泛化更差。
- en: 'If the units of input attributes are comparable, say all of them are meters
    or units of time, PCA, or more generally, kernel methods, can be used to reduce
    the dimensionality of the input space:'
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入属性的单元是可比的，比如说它们都是米或时间的单位，则可以使用PCA，或者更一般地说，使用核方法，来降低输入空间的维度：
- en: '[PRE51]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Here, we reduced the original four-dimensional problem to two-dimensional. Like
    averaging, computing linear combinations of input attributes and selecting only
    those that describe most of the variance helps to reduce noise.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将原始的四维问题降低到二维。像平均一样，计算输入属性的线性组合并仅选择描述大部分方差的那些属性有助于减少噪声。
- en: Summary
  id: totrans-620
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we looked at supervised and unsupervised learning and a few
    examples of how to run them in Spark/Scala. We considered SVM, logistic regression,
    decision tree, and k-means in the example of UCI Iris dataset. This is in no way
    a complete guide, and many other libraries either exist or are being made as we
    speak, but I would bet that you can solve 99% of the immediate data analysis problems
    just with these tools.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了监督学习和无监督学习，以及如何在Spark/Scala中运行它们的几个示例。在UCI Iris数据集的示例中，我们考虑了SVM、逻辑回归、决策树和k-means。这绝对不是一份完整的指南，而且现在存在或正在制作许多其他库，但我敢打赌，你只需使用这些工具就能解决99%的即时数据分析问题。
- en: This will give you a very fast shortcut on how to start being productive with
    a new dataset. There are many other ways to look at the datasets, but before we
    get into more advanced topics, let's discuss regression and classification in
    the next chapter, that is, how to predict continuous and discrete labels.
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为你提供一个快速捷径，了解如何开始使用新的数据集进行高效工作。查看数据集的方法有很多，但在我们深入更高级的主题之前，让我们在下一章讨论回归和分类，即如何预测连续和离散标签。
- en: Chapter 5. Regression and Classification
  id: totrans-623
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。回归和分类
- en: In the previous chapter, we got familiar with supervised and unsupervised learning.
    Another standard taxonomy of the machine learning methods is based on the label
    is from continuous or discrete space. Even if the discrete labels are ordered,
    there is a significant difference, particularly how the goodness of fit metrics
    is evaluated.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们熟悉了监督学习和无监督学习。机器学习方法的另一种标准分类是基于标签来自连续空间还是离散空间。即使离散标签是有序的，也存在显著差异，尤其是在评估拟合优度指标方面。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-625
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about the origin of the word regression
  id: totrans-626
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习回归这个词的起源
- en: Learning metrics for evaluating the goodness of fit in continuous and discrete
    space
  id: totrans-627
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习在连续和离散空间中评估拟合优度指标的方法
- en: Discussing how to write simple code in Scala for linear and logistic regression
  id: totrans-628
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论如何用Scala编写简单的线性回归和逻辑回归代码
- en: Learning about advanced concepts such as regularization, multiclass predictions,
    and heteroscedasticity
  id: totrans-629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关于正则化、多类预测和异方差性等高级概念
- en: Discussing an example of MLlib application for regression tree analysis
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论MLlib应用回归树分析的示例
- en: Learning about the different ways of evaluating classification models
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习评估分类模型的不同方法
- en: What regression stands for?
  id: totrans-632
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归代表什么？
- en: 'While the word classification is intuitively clear, the word regression does
    not seem to imply a predictor of a continuous label. According to the Webster
    dictionary, regression is:'
  id: totrans-633
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然分类这个词直观上很清楚，但回归这个词似乎并不暗示连续标签的预测器。根据韦伯斯特词典，回归是：
- en: '*"a return to a former or less developed state."*'
  id: totrans-634
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “回到以前或较少发展的状态。”
- en: 'It does also mention a special definition for statistics as *a measure of the
    relation between the mean value of one variable (for example, output) and corresponding
    values of other variables (for example, time and cost)*, which is actually correct
    these days. However, historically, the regression coefficient was meant to signify
    the hereditability of certain characteristics, such as weight and size, from one
    generation to another, with the hint of planned gene selection, including humans
    ([http://www.amstat.org/publications/jse/v9n3/stanton.html](http://www.amstat.org/publications/jse/v9n3/stanton.html)).
    More specifically, in 1875, Galton, a cousin of Charles Darwin and an accomplished
    19th-century scientist in his own right, which was also widely criticized for
    the promotion of eugenics, had distributed packets of sweet pea seeds to seven
    friends. Each friend received seeds of uniform weight, but with substantial variation
    across the seven packets. Galton''s friends were supposed to harvest the next
    generation seeds and ship them back to him. Galton then proceeded to analyze the
    statistical properties of the seeds within each group, and one of the analysis
    was to plot the regression line, which always appeared to have the slope less
    than one—the specific number cited was 0.33 (Galton, F. (1894), Natural Inheritance
    (5th ed.), New York: Macmillan and Company), as opposed to either *0*, in the
    case of no correlation and no inheritance; or *1*, in the case the total replication
    of the parent''s characteristics in the descendants. We will discuss why the coefficient
    of the regression line should always be less than *1* in the presence of noise
    in the data, even if the correlation is perfect. However, beyond the discussion
    and details, the origin of the term regression is partly due to planned breeding
    of plants and humans. Of course, Galton did not have access to PCA, Scala, or
    any other computing machinery at the time, which might shed more light on the
    differences between correlation and the slope of the regression line.'
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 它也提到了统计学的一个特殊定义，即*一个变量（例如，输出）的平均值与相应变量的值（例如，时间和成本）之间的关系度量*，这在当今实际上是正确的。然而，从历史上看，回归系数原本是用来表示某些特征（如体重和大小）从一代传到另一代的遗传性，暗示着有计划的基因选择，包括人类([http://www.amstat.org/publications/jse/v9n3/stanton.html](http://www.amstat.org/publications/jse/v9n3/stanton.html))。更具体地说，在1875年，查尔斯·达尔文的表亲、一位杰出的19世纪科学家高尔顿，也因为推广优生学而受到广泛批评，他向七个朋友分发了甜豌豆种子。每个朋友都收到了重量均匀的种子，但七个包裹之间有显著的差异。高尔顿的朋友们应该收获下一代种子并将它们寄回给他。高尔顿随后分析了每个群体中种子的统计特性，其中一项分析就是绘制回归线，这条线似乎总是具有小于1的斜率——具体引用的数字是0.33（高尔顿，F.
    (1894)，《自然遗传》（第5版），纽约：麦克米伦公司），与没有相关性且没有遗传的情况下的*0*相反；或者与父母特征在后代中完全复制的*1*相反。我们将在有噪声数据的情况下讨论为什么回归线的系数应该始终小于*1*，即使相关性是完美的。然而，除了讨论和细节之外，回归这个术语的起源部分是由于植物和人类的计划育种。当然，高尔顿当时没有访问PCA、Scala或其他任何计算设备，这些设备可能会更多地阐明相关性和回归线斜率之间的差异。
- en: Continuous space and metrics
  id: totrans-636
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连续空间和度量
- en: 'As most of this chapter''s content will be dealing with trying to predict or
    optimize continuous variables, let''s first understand how to measure the difference
    in a continuous space. Unless a drastically new discovery is made pretty soon,
    the space we live in is a three-dimensional Euclidian space. Whether we like it
    or not, this is the world we are mostly comfortable with today. We can completely
    specify our location with three continuous numbers. The difference in locations
    is usually measured by distance, or a metric, which is a function of a two arguments
    that returns a single positive real number. Naturally, the distance, ![Continuous
    space and metrics](img/image01691.jpeg), between *X* and *Y* should always be
    equal or smaller than the sum of distances between *X* and *Z* and *Y* and *Z*:'
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本章的大部分内容将涉及尝试预测或优化连续变量，让我们首先了解如何在连续空间中测量差异。除非很快有重大发现，我们所处的空间是一个三维欧几里得空间。无论我们是否喜欢，这是我们今天大多数人都比较适应的世界。我们可以用三个连续的数字完全指定我们的位置。位置之间的差异通常通过距离或度量来衡量，度量是一个关于两个参数的函数，它返回一个正实数。自然地，*X*
    和 *Y* 之间的距离，![连续空间和度量](img/image01691.jpeg)，应该始终等于或小于 *X* 和 *Z* 以及 *Y* 和 *Z* 之间距离之和：
- en: '![Continuous space and metrics](img/image01692.jpeg)'
  id: totrans-638
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01692.jpeg)'
- en: 'For any *X*, *Y*, and *Z*, which is also called triangle inequality. The two
    other properties of a metric is symmetry:'
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何 *X*，*Y* 和 *Z*，这也就是三角不等式。度量的另外两个性质是对称性：
- en: '![Continuous space and metrics](img/image01693.jpeg)'
  id: totrans-640
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01693.jpeg)'
- en: 'Non-negativity of distance:'
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 距离的非负性：
- en: '![Continuous space and metrics](img/image01694.jpeg)![Continuous space and
    metrics](img/image01695.jpeg)'
  id: totrans-642
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01694.jpeg)![连续空间和度量](img/image01695.jpeg)'
- en: 'Here, the metric is `0` if, and only if, *X=Y*. The ![Continuous space and
    metrics](img/image01696.jpeg) distance is the distance as we understand it in
    everyday life, the square root of the sum of the squared differences along each
    of the dimensions. A generalization of our physical distance is p-norm (*p = 2*
    for the ![Continuous space and metrics](img/image01696.jpeg) distance):'
  id: totrans-643
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，如果且仅当 *X=Y* 时，度量是 `0`。![连续空间和度量](img/image01696.jpeg) 距离是我们对日常生活中的距离的理解，即每个维度上平方差的平方根。我们物理距离的推广是
    p-范数（对于 ![连续空间和度量](img/image01696.jpeg) 距离，*p = 2*）：
- en: '![Continuous space and metrics](img/image01697.jpeg)'
  id: totrans-644
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01697.jpeg)'
- en: 'Here, the sum is the overall components of the *X* and *Y* vectors. If *p=1*,
    the 1-norm is the sum of absolute differences, or Manhattan distance, as if the
    only path from point *X* to point *Y* would be to move only along one of the components.
    This distance is also often referred to as ![Continuous space and metrics](img/image01698.jpeg)
    distance:'
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，总和是 *X* 和 *Y* 向量的整体分量。如果 *p=1*，1-范数是绝对差分的总和，或曼哈顿距离，就像从点 *X* 到点 *Y* 的唯一路径是只沿一个分量移动一样。这种距离也常被称为
    ![连续空间和度量](img/image01698.jpeg) 距离：
- en: '![Continuous space and metrics](img/image01699.jpeg)'
  id: totrans-646
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01699.jpeg)'
- en: Figure 05-1\. The ![Continuous space and metrics](img/image01698.jpeg) circle
    in two-dimensional space (the set of points exactly one unit from the origin (0,
    0))
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-1\. 二维空间中的 ![连续空间和度量](img/image01698.jpeg) 圆（距离原点 (0, 0) 精确为一单位的点的集合）
- en: 'Here is a representation of a circle in a two-dimensional space:'
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是二维空间中圆的一个表示：
- en: '![Continuous space and metrics](img/image01700.jpeg)'
  id: totrans-649
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01700.jpeg)'
- en: Figure 05-2\. ![Continuous space and metrics](img/image01696.jpeg) circle in
    two-dimensional space (the set of points equidistant from the origin (0, 0)),
    which actually looks like a circle in our everyday understanding of distance.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-2\. ![连续空间和度量](img/image01696.jpeg) 圆在二维空间中（距离原点 (0, 0) 等距离的点的集合），在我们对距离的日常理解中，它实际上看起来像一个圆。
- en: 'Another frequently used special case is ![Continuous space and metrics](img/image01701.jpeg),
    the limit when ![Continuous space and metrics](img/image01702.jpeg), which is
    the maximum deviation along any of the components, as follows:'
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常用的特殊情况是 ![连续空间和度量](img/image01701.jpeg)，当 ![连续空间和度量](img/image01702.jpeg)
    时，即沿任何组件的最大偏差，如下所示：
- en: '![Continuous space and metrics](img/image01703.jpeg)'
  id: totrans-652
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01703.jpeg)'
- en: 'The equidistant circle for the ![Continuous space and metrics](img/image01701.jpeg)
    distance is shown in *Figure 05-3*:'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ![连续空间和度量](img/image01701.jpeg) 距离的等距圆，请参见 *图 05-3*：
- en: '![Continuous space and metrics](img/image01704.jpeg)'
  id: totrans-654
  prefs: []
  type: TYPE_IMG
  zh: '![连续空间和度量](img/image01704.jpeg)'
- en: Figure 05-3\. ![Continuous space and metrics](img/image01701.jpeg) circle in
    two-dimensional space (the set of points equidistant from the origin (0, 0)).
    This is a square as the ![Continuous space and metrics](img/image01701.jpeg) metric
    is the maximum distance along any of the components.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-3\. ![连续空间和度量](img/image01701.jpeg) 圆在二维空间中（距离原点 (0, 0) 等距离的点的集合）。这是一个正方形，因为
    ![连续空间和度量](img/image01701.jpeg) 度量是沿任何组件的最大距离。
- en: I'll consider the **Kullback-Leibler** (**KL**) distance later when I talk about
    classification, which measures the difference between two probability distributions,
    but it is an example of distance that is not symmetric and thus it is not a metric.
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 当我谈到分类时，我会稍后考虑 **Kullback-Leibler** （**KL**） 距离，它衡量两个概率分布之间的差异，但它是一个不对称的距离的例子，因此它不是一个度量。
- en: The metric properties make it easier to decompose the problem. Due to the triangle
    inequality, one can potentially reduce a difficult problem of optimizing a goal
    by substituting it by a set of problems by optimizing along a number of dimensional
    components of the problem separately.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 度量性质使得问题分解更容易。由于三角不等式，可以通过分别优化问题的多个维度分量来替换一个困难的目标优化问题。
- en: Linear regression
  id: totrans-658
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性回归
- en: As explained in [Chapter 2](part0242.xhtml#aid-76P842 "Chapter 2. Data Pipelines
    and Modeling"), *Data Pipelines and Modeling*, most complex machine learning problems
    can be reduced to optimization as our final goal is to optimize the whole process
    where the machine is involved as an intermediary or the complete solution. The
    metric can be explicit, such as error rate, or more indirect, such as **Monthly
    Active Users** (**MAU**), but the effectiveness of an algorithm is finally judged
    by how it improves some metrics and processes in our lives. Sometimes, the goals
    may consist of multiple subgoals, or other metrics such as maintainability and
    stability might eventually be considered, but essentially, we need to either maximize
    or minimize a continuous metric in one or other way.
  id: totrans-659
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第2章](part0242.xhtml#aid-76P842 "第2章。数据管道和建模") 所述，*数据管道和建模*，大多数复杂的机器学习问题都可以归结为优化，因为我们的最终目标是优化整个流程，其中机器作为中介或完整解决方案。指标可以是明确的，如错误率，或者更间接的，如
    **月活跃用户** (**MAU**)，但算法的有效性最终是通过它如何改善我们生活中的某些指标和流程来评判的。有时，目标可能包括多个子目标，或者维护性和稳定性等指标最终也可能被考虑，但本质上，我们需要以某种方式最大化或最小化一个连续指标。
- en: 'For the rigor of the flow, let''s show how the linear regression can be formulated
    as an optimization problem. The classical linear regression needs to optimize
    the cumulative ![Linear regression](img/image01696.jpeg) error rate:'
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 为了流程的严谨性，让我们展示如何将线性回归表述为一个优化问题。经典的线性回归需要优化累积 ![线性回归](img/image01696.jpeg) 错误率：
- en: '![Linear regression](img/image01705.jpeg)'
  id: totrans-661
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01705.jpeg)'
- en: 'Here, ![Linear regression](img/image01706.jpeg) is the estimate given by a
    model, which, in the case of linear regression, is as follows:'
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![线性回归](img/image01706.jpeg)是模型给出的估计值，在线性回归的情况下，如下所示：
- en: '![Linear regression](img/image01707.jpeg)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01707.jpeg)'
- en: '(Other potential **loss functions** have been enumerated in [Chapter 3](part0249.xhtml#aid-7DES21
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*). As
    the ![Linear regression](img/image01696.jpeg) metric is a differentiable convex
    function of *a*, *b*, the extreme value can be found by equating the derivative
    of the cumulative error rate to `0`:'
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: （[第3章](part0249.xhtml#aid-7DES21 "第3章。使用 Spark 和 MLlib") 中已列举了其他潜在的 **损失函数**）。由于
    ![线性回归](img/image01696.jpeg) 指标是 *a*、*b* 的可微凸函数，可以通过将累积错误率的导数等于 `0` 来找到极值：
- en: '![Linear regression](img/image01708.jpeg)'
  id: totrans-665
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01708.jpeg)'
- en: 'Computing the derivatives is straightforward in this case and leads to the
    following equation:'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，计算导数是直接的，并导致以下方程：
- en: '![Linear regression](img/image01709.jpeg)![Linear regression](img/image01710.jpeg)'
  id: totrans-667
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01709.jpeg)![线性回归](img/image01710.jpeg)'
- en: 'This can be solved to give:'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下方式解决：
- en: '![Linear regression](img/image01711.jpeg)![Linear regression](img/image01712.jpeg)'
  id: totrans-669
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01711.jpeg)![线性回归](img/image01712.jpeg)'
- en: 'Here, *avg()* denotes the average overall input records. Note that if *avg(x)=0*
    the preceding equation is reduced to the following:'
  id: totrans-670
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，*avg()* 表示整体输入记录的平均值。注意，如果 *avg(x)=0*，则前面的方程简化为以下形式：
- en: '![Linear regression](img/image01713.jpeg)![Linear regression](img/image01714.jpeg)'
  id: totrans-671
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01713.jpeg)![线性回归](img/image01714.jpeg)'
- en: 'So, we can quickly compute the linear regression coefficients using basic Scala
    operators (we can always make *avg(x)* to be zero by performing a ![Linear regression](img/image01715.jpeg)):'
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以快速使用基本的 Scala 操作符计算线性回归系数（我们可以通过执行 ![线性回归](img/image01715.jpeg) 使 *avg(x)*
    为零）：
- en: '[PRE52]'
  id: totrans-673
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Didn't I inform you previously that Scala is a very concise language? We just
    did linear regression with five lines of code, three of which were just data-generation
    statements.
  id: totrans-674
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前没有告诉你 Scala 是一种非常简洁的语言吗？我们只用五行代码就完成了线性回归，其中三行只是数据生成语句。
- en: Although there are libraries written in Scala for performing (multivariate)
    linear regression, such as Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze)),
    which provides a more extensive functionality, it is nice to be able to use pure
    Scala functionality to get some simple statistical results.
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然有使用 Scala 编写的用于执行（多元）线性回归的库，例如 Breeze ([https://github.com/scalanlp/breeze](https://github.com/scalanlp/breeze))，它提供了更广泛的功能，但能够使用纯
    Scala 功能来获取一些简单的统计结果是非常好的。
- en: 'Let''s look at the problem of Mr. Galton, where he found that the regression
    line always has the slope of less than one, which implies that we should always
    regress to some predefined mean. I will generate the same points as earlier, but
    they will be distributed along the horizontal line with some predefined noise.
    Then, I will rotate the line by *45* degrees by doing a linear rotation transformation
    in the *xy*-space. Intuitively, it should be clear that if anything, *y* is strongly
    correlated with x and absent, the *y* noise should be nothing else but *x*:'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看加尔顿先生的问题，他发现回归线总是小于一的斜率，这意味着我们应该始终回归到某个预定义的均值。我将生成与之前相同的点，但它们将分布在水平线上，并带有一些预定义的噪声。然后，我将通过在
    *xy*-空间中进行线性旋转变换将线旋转 *45* 度。直观上，应该很明显，如果 *y* 与 *x* 强烈相关且不存在，那么 *y* 的噪声就只能是 *x*：
- en: '[PRE53]'
  id: totrans-677
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The slope is only `0.81`! Note that if one runs PCA on the `x1` and `y1` data,
    the first principal component is correctly along the diagonal.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率仅为 `0.81`！请注意，如果对 `x1` 和 `y1` 数据运行 PCA，第一个主成分将正确地沿着对角线。
- en: 'For completeness, I am giving a plot of (*x1, y1*) zipped here:'
  id: totrans-679
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我给出了 (*x1, y1*) 的一个绘图，如下所示：
- en: '![Linear regression](img/image01716.jpeg)'
  id: totrans-680
  prefs: []
  type: TYPE_IMG
  zh: '![线性回归](img/image01716.jpeg)'
- en: Figure 05-4\. The regression curve slope of a seemingly perfectly correlated
    dataset is less than one. This has to do with the metric the regression problem
    optimizes (y-distance).
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 图 05-4\. 一个看似完美相关的数据集的回归曲线斜率小于一。这与回归问题优化的度量（y 距离）有关。
- en: I will leave it to the reader to find the reason why the slope is less than
    one, but it has to do with the specific question the regression problem is supposed
    to answer and the metric it optimizes.
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: 我将留给读者去找到斜率小于一的原因，但这与回归问题应该回答的具体问题和它优化的度量有关。
- en: Logistic regression
  id: totrans-683
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 逻辑回归
- en: 'Logistic regression optimizes the logit loss function with respect to *w*:'
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归优化了关于 *w* 的对数损失函数：
- en: '![Logistic regression](img/image01717.jpeg)'
  id: totrans-685
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image01717.jpeg)'
- en: Here, *y* is binary (in this case plus or minus one). While there is no closed-form
    solution for the error minimization problem like there was in the previous case
    of linear regression, logistic function is differentiable and allows iterative
    algorithms that converge very fast.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*y* 是二进制的（在这种情况下是正负一）。虽然与之前线性回归的情况不同，误差最小化问题没有封闭形式的解，但逻辑函数是可微的，并允许快速收敛的迭代算法。
- en: 'The gradient is as follows:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度如下：
- en: '![Logistic regression](img/image01718.jpeg)'
  id: totrans-688
  prefs: []
  type: TYPE_IMG
  zh: '![逻辑回归](img/image01718.jpeg)'
- en: 'Again, we can quickly concoct a Scala program that uses the gradient to converge
    to the value, where ![Logistic regression](img/image01719.jpeg) (we use the MLlib
    `LabeledPoint` data structure only for convenience of reading the data):'
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们可以快速编写一个 Scala 程序，该程序使用梯度收敛到值，如 ![逻辑回归](img/image01719.jpeg)（我们仅使用 MLlib
    `LabeledPoint` 数据结构是为了读取数据的方便）：
- en: '[PRE54]'
  id: totrans-690
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The logistic regression was reduced to only one line of Scala code! The last
    line was to normalize the weights—only the relative values are important to define
    the separating plane—to compare them to the one obtained with the MLlib in previous
    chapter.
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归被简化为只有一行 Scala 代码！最后一行是为了归一化权重——只有相对值对于定义分离平面很重要——以便与之前章节中 MLlib 获得的值进行比较。
- en: 'The **Stochastic Gradient Descent** (**SGD**) algorithm used in the actual
    implementation is essentially the same gradient descent, but optimized in the
    following ways:'
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现中使用的 **随机梯度下降**（**SGD**）算法本质上与梯度下降相同，但以下方面进行了优化：
- en: The actual gradient is computed on a subsample of records, which may lead to
    faster conversion due to less rounding noise and avoid local minima.
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实际梯度是在记录的子样本上计算的，这可能会由于减少了舍入噪声而加快转换速度，并避免局部最小值。
- en: The step—a fixed *0.1* in our case—is a monotonically decreasing function of
    the iteration as ![Logistic regression](img/image01720.jpeg), which might also
    lead to better conversion.
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长——在我们的例子中是固定的 *0.1*——是迭代的单调递减函数，如 ![逻辑回归](img/image01720.jpeg)，这也可能导致更好的转换。
- en: It incorporates regularization; instead of minimizing just the loss function,
    you minimize the sum of the loss function, plus some penalty metric, which is
    a function of model complexity. I will discuss this in the following section.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它包含了正则化；不是仅仅最小化损失函数，而是最小化损失函数的总和，加上一些惩罚度量，这是一个关于模型复杂度的函数。我将在下一节讨论这个问题。
- en: Regularization
  id: totrans-696
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正则化
- en: The regularization was originally developed to cope with ill-poised problems,
    where the problem was underconstrained—allowed multiple solutions given the data—or
    the data and the solution that contained too much noise (*A.N. Tikhonov*, *A.S.
    Leonov*, *A.G. Yagola. Nonlinear Ill-Posed Problems*, *Chapman and Hall*, *London*,
    *Weinhe*). Adding additional penalty function that skews a solution if it does
    not have a desired property, such as the smoothness in curve fitting or spectral
    analysis, usually solves the problem.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化最初是为了应对病态问题而开发的，其中问题是不受约束的——给定数据允许有多个解，或者数据和包含过多噪声的解（*A.N. Tikhonov*，*A.S.
    Leonov*，*A.G. Yagola. 非线性病态问题*，*Chapman and Hall*，*London*，*Weinhe*）。添加额外的惩罚函数，如果解没有期望的特性，如曲线拟合或频谱分析中的平滑性，通常可以解决问题。
- en: The choice of the penalty function is somewhat arbitrary, but it should reflect
    a desired skew in the solution. If the penalty function is differentiable, it
    can be incorporated into the gradient descent process; ridge regression is an
    example where the penalty is the ![Regularization](img/image01696.jpeg)metric
    for the weights or the sum of squares of the coefficients.
  id: totrans-698
  prefs: []
  type: TYPE_NORMAL
  zh: 惩罚函数的选择在一定程度上是任意的，但它应该反映对解的期望偏斜。如果惩罚函数是可微分的，它可以被纳入梯度下降过程；岭回归就是一个例子，其中惩罚是权重或系数平方和的![正则化](img/image01696.jpeg)度量。
- en: MLlib currently implements ![Regularization](img/image01696.jpeg), ![Regularization](img/image01698.jpeg),
    and a mixture thereof called **Elastic Net**, as was shown in [Chapter 3](part0249.xhtml#aid-7DES21
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*. The
    ![Regularization](img/image01698.jpeg) regularization effectively penalizes for
    the number of non-zero entries in the regression weights, but has been known to
    have slower convergence. **Least Absolute Shrinkage and Selection Operator** (**LASSO**)
    uses the ![Regularization](img/image01698.jpeg) regularization.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib目前实现了![正则化](img/image01696.jpeg)，![正则化](img/image01698.jpeg)，以及称为**弹性网络**的混合形式，如[第3章](part0249.xhtml#aid-7DES21
    "第3章。使用Spark和MLlib")所示，*使用Spark和MLlib*。![正则化](img/image01698.jpeg)正则化有效地惩罚了回归权重中非零项的数量，但已知其收敛速度较慢。**最小绝对收缩和选择算子**（**LASSO**）使用了![正则化](img/image01698.jpeg)正则化。
- en: Another way to reduce the uncertainty in underconstrained problems is to take
    the prior information that may be coming from domain experts into account. This
    can be done using Bayesian analysis and introducing additional factors into the
    posterior probability—the probabilistic rules are generally expressed as multiplication
    rather than sum. However, since the goal is often minimizing the log likelihood,
    the Bayesian correction can often be expressed as standard regularizer as well.
  id: totrans-700
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种减少受约束问题不确定性的方法是考虑可能来自领域专家的先验信息。这可以通过贝叶斯分析实现，并在后验概率中引入额外的因素——概率规则通常用乘法而不是加法表示。然而，由于目标通常是最小化对数似然，贝叶斯校正通常也可以表示为标准正则化器。
- en: Multivariate regression
  id: totrans-701
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多元回归
- en: It is possible to minimize multiple metrics at the same time. While Spark only
    has a few multivariate analysis tools, other more traditional well-established
    packages come with **Multivariate Analysis of Variance** (**MANOVA**), a generalization
    of **Analysis of Variance** (**ANOVA**) method. I will cover ANOVA and MANOVA
    in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working with Graph Algorithms"),
    *Working with Graph Algorithms*.
  id: totrans-702
  prefs: []
  type: TYPE_NORMAL
  zh: 同时最小化多个度量是可能的。虽然Spark只有少数多元分析工具，但其他更传统且已建立的包包含了**多元方差分析**（**MANOVA**），它是**方差分析**（**ANOVA**）方法的一种推广。我将在[第7章](part0283.xhtml#aid-8DSF61
    "第7章。处理图算法")，*处理图算法*中介绍ANOVA和MANOVA。
- en: For a practical analysis, we first need to understand if the target variables
    are correlated, for which we can use the PCA Spark implementation covered in [Chapter
    3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark and MLlib"), *Working
    with Spark and MLlib*. If the dependent variables are strongly correlated, maximizing
    one leads to maximizing the other, and we can just maximize the first principal
    component (and potentially build a regression model on the second component to
    understand what drives the difference).
  id: totrans-703
  prefs: []
  type: TYPE_NORMAL
  zh: 对于实际分析，我们首先需要了解目标变量是否相关，我们可以使用[第3章](part0249.xhtml#aid-7DES21 "第3章。使用Spark和MLlib")中介绍的PCA
    Spark实现来做到这一点，*使用Spark和MLlib*。如果因变量高度相关，最大化一个会导致最大化另一个，我们只需最大化第一个主成分（并且可能基于第二个成分构建回归模型来理解驱动差异的因素）。
- en: If the targets are uncorrelated, building a separate model for each of them
    can pinpoint the important variables that drive either and whether these two sets
    are disjoint. In the latter case, we could build two separate models to predict
    each of the targets independently.
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标不相关，可以为每个目标构建一个单独的模型，以确定驱动它们的变量以及这两个集合是否互斥。在后一种情况下，我们可以构建两个单独的模型来独立预测每个目标。
- en: Heteroscedasticity
  id: totrans-705
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异方差性
- en: One of the fundamental assumptions in regression approach is that the target
    variance is not correlated with either independent (attributes) or dependent (target)
    variables. An example where this assumption might break is counting data, which
    is generally described by **Poisson distribution**. For Poisson distribution,
    the variance is proportional to the expected value, and the higher values can
    contribute more to the final variance of the weights.
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 回归方法中的一个基本假设是目标方差与独立（属性）或依赖（目标）变量不相关。一个可能违反此假设的例子是计数数据，它通常由**泊松分布**描述。对于泊松分布，方差与期望值成正比，高值可以更多地贡献到权重的最终方差。
- en: 'While heteroscedasticity may or may not significantly skew the resulting weights,
    one practical way to compensate for heteroscedasticity is to perform a log transformation,
    which will compensate for it in the case of Poisson distribution:'
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然异方差性可能会或可能不会显著地扭曲结果权重，但一种补偿异方差性的实际方法是进行对数变换，这在泊松分布的情况下可以补偿：
- en: '![Heteroscedasticity](img/image01721.jpeg)![Heteroscedasticity](img/image01722.jpeg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![异方差性](img/image01721.jpeg)![异方差性](img/image01722.jpeg)'
- en: 'Some other (parametrized) transformations are the **Box-Cox transformation**:'
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 一些其他（参数化）的变换是**Box-Cox变换**：
- en: '![Heteroscedasticity](img/image01723.jpeg)'
  id: totrans-710
  prefs: []
  type: TYPE_IMG
  zh: '![异方差性](img/image01723.jpeg)'
- en: 'Here, ![Heteroscedasticity](img/image01724.jpeg) is a parameter (the log transformation
    is a partial case, where ![Heteroscedasticity](img/image01725.jpeg)) and Tuckey''s
    lambda transformation (for attributes between *0* and *1*):'
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![异方差性](img/image01724.jpeg)是一个参数（对数变换是部分情况，其中![异方差性](img/image01725.jpeg)）和Tuckey的lambda变换（对于介于*0*和*1*之间的属性）：
- en: '![Heteroscedasticity](img/image01726.jpeg)'
  id: totrans-712
  prefs: []
  type: TYPE_IMG
  zh: '![异方差性](img/image01726.jpeg)'
- en: These compensate for Poisson binomial distributed attributes or the estimates
    of the probability of success in a sequence of trails with potentially a mix of
    *n* Bernoulli distributions.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变换用于补偿泊松二项分布的属性或一系列试验中成功概率的估计，这些试验可能包含混合的*n*个伯努利分布。
- en: Heteroscedasticity is one of the main reasons that logistic function minimization
    works better than linear regression with ![Heteroscedasticity](img/image01696.jpeg)
    minimization in a binary prediction problem. Let's consider discrete labels in
    more details.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 异方差性是逻辑函数最小化在二元预测问题中比带有![回归树](img/image01696.jpeg)最小化的线性回归表现更好的主要原因之一。让我们更详细地考虑离散标签。
- en: Regression trees
  id: totrans-715
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 回归树
- en: 'We have seen classification trees in the previous chapter. One can build a
    recursive split-and-concur structure for a regression problem, where a split is
    chosen to minimize the remaining variance. Regression trees are less popular than
    decision trees or classical ANOVA analysis; however, let''s provide an example
    of a regression tree here as a part of MLlib:'
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一章中看到了分类树。可以为回归问题构建一个递归的分割和合并结构，其中分割是为了最小化剩余的方差。回归树不如决策树或经典ANOVA分析流行；然而，让我们在这里提供一个回归树的例子，作为MLlib的一部分：
- en: '[PRE55]'
  id: totrans-717
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The splits at each level are made to minimize the variance, as follows:'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: 每一层的分割都是为了最小化方差，如下所示：
- en: '![Regression trees](img/image01727.jpeg)'
  id: totrans-719
  prefs: []
  type: TYPE_IMG
  zh: '![回归树](img/image01727.jpeg)'
- en: which is equivalent to minimizing the ![Regression trees](img/image01696.jpeg)
    distances between the label values and their mean within each leaf summed over
    all the leaves of the node.
  id: totrans-720
  prefs: []
  type: TYPE_NORMAL
  zh: 这等价于最小化标签值与其在每个叶节点内的平均值之间的![回归树](img/image01696.jpeg)距离，并求和所有叶节点的总和。
- en: Classification metrics
  id: totrans-721
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分类度量
- en: If the label is discrete, the prediction problem is called classification. In
    general, the target can take only one of the values for each record (even though
    multivalued targets are possible, particularly for text classification problems
    to be considered in [Chapter 6](part0273.xhtml#aid-84B9I2 "Chapter 6. Working
    with Unstructured Data"), *Working with Unstructured Data*).
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标签是离散的，预测问题称为分类。通常，每个记录的目标只能取一个值（尽管可能存在多值目标，尤其是在[第6章](part0273.xhtml#aid-84B9I2
    "第6章。处理非结构化数据")中考虑的文本分类问题，*处理非结构化数据*）。
- en: If the discrete values are ordered and the ordering makes sense, such as *Bad*,
    *Worse*, *Good*, the discrete labels can be cast into integer or double, and the
    problem is reduced to regression (we believe if you are between *Bad* and *Worse*,
    you are definitely farther away from being *Good* than *Worse*).
  id: totrans-723
  prefs: []
  type: TYPE_NORMAL
  zh: 如果离散值是有序的，并且排序有意义，例如 *Bad*、*Worse*、*Good*，则可以将离散标签转换为整数或双精度浮点数，问题就简化为回归（我们相信如果你在
    *Bad* 和 *Worse* 之间，你肯定比 *Worse* 更远离 *Good*）。
- en: 'A generic metric to optimize is the misclassification rate is as follows:'
  id: totrans-724
  prefs: []
  type: TYPE_NORMAL
  zh: 要优化的通用度量是误分类率，如下所示：
- en: '![Classification metrics](img/image01728.jpeg)'
  id: totrans-725
  prefs: []
  type: TYPE_IMG
  zh: '![分类度量](img/image01728.jpeg)'
- en: However, if the algorithm can predict the distribution of possible values for
    the target, a more general metric such as the KL divergence or Manhattan can be
    used.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果算法可以预测目标可能值的分布，可以使用更通用的度量，如 KL 散度或曼哈顿距离。
- en: 'KL divergence is a measure of information loss when probability distribution
    ![Classification metrics](img/image01729.jpeg) is used to approximate probability
    distribution ![Classification metrics](img/image01730.jpeg):'
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: KL 散度是当使用概率分布 ![分类度量](img/image01729.jpeg) 来近似概率分布 ![分类度量](img/image01730.jpeg)
    时信息损失的一个度量：
- en: '![Classification metrics](img/image01731.jpeg)'
  id: totrans-728
  prefs: []
  type: TYPE_IMG
  zh: '![分类度量](img/image01731.jpeg)'
- en: It is closely related to entropy gain split criteria used in the decision tree
    induction, as the latter is the sum of KL divergences of the node probability
    distribution to the leaf probability distribution over all leaf nodes.
  id: totrans-729
  prefs: []
  type: TYPE_NORMAL
  zh: 它与决策树归纳中使用的熵增益分割标准密切相关，因为后者是所有叶节点上节点概率分布到叶概率分布的 KL 散度的总和。
- en: Multiclass problems
  id: totrans-730
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多类问题
- en: If the number of possible outcomes for target is larger than two, in general,
    we have to predict either the expected probability distribution of the target
    values or at least the list of ordered values—hopefully augmented by a rank variable,
    which can be used for additional analysis.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 如果目标可能的结果数量超过两个，通常，我们必须预测目标值的期望概率分布，或者至少是有序值的列表——最好是通过一个排名变量来增强，该变量可以用于额外的分析。
- en: While some algorithms, such as decision trees, can natively predict multivalued
    attributes. A common technique is to reduce the prediction of one of the *K* target
    values to *(K-1)* binary classification problems by choosing one of the values
    as the base and building *(K-1)* binary classifiers. It is usually a good idea
    to select the most populated level as the base.
  id: totrans-732
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些算法，如决策树，可以原生地预测多值属性。一种常见的技术是通过选择一个值作为基准，将一个 *K* 个目标值的预测减少到 *(K-1)* 个二元分类问题，构建
    *(K-1)* 个二元分类器。通常选择最密集的级别作为基准是一个好主意。
- en: Perceptron
  id: totrans-733
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 感知器
- en: 'In the early days of machine learning, researchers were trying to imitate the
    functionality of the human brain. At the beginning of the 20th century, people
    thought that the human brain consisted entirely of cells that are called neurons—cells
    with long appendages called axons that were able to transmit signals by means
    of electric impulses. The AI researchers were trying to replicate the functionality
    of neurons by a perceptron, which is a function that is firing, based on a linearly-weighted
    sum of its input values:'
  id: totrans-734
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习的早期，研究人员试图模仿人脑的功能。20世纪初，人们认为人脑完全由称为神经元的细胞组成——具有长突起的细胞称为轴突，能够通过电脉冲传递信号。AI研究人员试图通过感知器来复制神经元的功能，感知器是一个基于其输入值的线性加权和的激活函数：
- en: '![Perceptron](img/image01732.jpeg)'
  id: totrans-735
  prefs: []
  type: TYPE_IMG
  zh: '![感知器](img/image01732.jpeg)'
- en: This is a very simplistic representation of the processes in the human brain—biologists
    have since then discovered other ways in which information is transferred besides
    electric impulses such as chemical ones. Moreover, they have found over 300 different
    types of cells that may be classified as neurons ([http://neurolex.org/wiki/Category:Neuron](http://neurolex.org/wiki/Category:Neuron)).
    Also, the process of neuron firing is more complex than just linear transmission
    of voltages as it involves complex time patterns as well. Nevertheless, the concept
    turned out to be very productive, and multiple algorithms and techniques were
    developed for neural nets, or the sets of perceptions connected to each other
    in layers. Specifically, it can be shown that the neural network, with certain
    modification, where the step function is replaced by a logistic function in the
    firing equation, can approximate an arbitrary differentiable function with any
    desired precision.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对人脑中过程的一种非常简单的表示——自那时起，生物学家已经发现了除了电脉冲之外的其他信息传递方式，例如化学脉冲。此外，他们已经发现了300多种可能被归类为神经元的细胞类型（[http://neurolex.org/wiki/Category:Neuron](http://neurolex.org/wiki/Category:Neuron)）。此外，神经元放电的过程比仅仅电压的线性传输要复杂得多，因为它还涉及到复杂的时间模式。尽管如此，这个概念证明是非常有成效的，为神经网络或层间相互连接的感知集开发了许多算法和技术。具体来说，可以证明，通过某些修改，在放电方程中将步函数替换为逻辑函数，神经网络可以以任何所需的精度逼近任意可微函数。
- en: 'MLlib implements **Multilayer Perceptron Classifier** (**MLCP**) as an `org.apache.spark.ml.classification.MultilayerPerceptronClassifier`
    class:'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib实现了**多层感知器分类器**（**MLCP**）作为一个`org.apache.spark.ml.classification.MultilayerPerceptronClassifier`类：
- en: '[PRE56]'
  id: totrans-738
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Generalization error and overfitting
  id: totrans-739
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 泛化误差和过拟合
- en: So, how do we know that the model we have discussed is good? One obvious and
    ultimate criterion is its performance in practice.
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，我们如何知道我们讨论的模型是好的呢？一个明显且最终的标准是其实践中的表现。
- en: One common problem that plagues the more complex models, such as decision trees
    and neural nets, is overfitting. The model can minimize the desired metric on
    the provided data, but does a very poor job on a slightly different dataset in
    practical deployments, Even a standard technique, when we split the dataset into
    training and test, the training for deriving the model and test for validating
    that the model works well on a hold-out data, may not capture all the changes
    that are in the deployments. For example, linear models such as ANOVA, logistic,
    and linear regression are usually relatively stable and less of a subject to overfitting.
    However, you might find that any particular technique either works or doesn't
    work for your specific domain.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的难题困扰着更复杂的模型，例如决策树和神经网络，那就是过拟合问题。模型可以在提供的数据上最小化期望的指标，但在实际部署中，对稍微不同的数据集却表现得很差。即使是一个标准的技巧，当我们把数据集分成训练集和测试集，用于推导模型的训练和验证模型在保留数据集上表现良好的测试，也可能无法捕捉到部署中所有的变化。例如，线性模型如方差分析、逻辑回归和线性回归通常相对稳定，不太容易过拟合。然而，你可能会发现，任何特定的技术对于你的特定领域要么有效要么无效。
- en: Another case when generalization may fail is time-drift. The data may change
    over time significantly so that the model trained on the old data no longer generalizes
    on the new data in a deployment. In practice, it is always a good idea to have
    several models in production and constantly monitor their relative performance.
  id: totrans-742
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个可能导致泛化失败的情况是时间漂移。数据可能会随着时间的推移发生显著变化，以至于在旧数据上训练的模型在部署中的新数据上不再泛化。在实践中，始终拥有几个生产中的模型并持续监控它们的相对性能总是一个好主意。
- en: I will consider standard ways to avoid overfitting such as hold out datasets
    and cross-validation in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms* and model monitoring
    in [Chapter 9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP in Scala"), *NLP in Scala*.
  id: totrans-743
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在第7章[《使用图算法》](part0283.xhtml#aid-8DSF61 "第7章。使用图算法")中考虑避免过拟合的标准方法，如保留数据集和交叉验证，以及在第9章[《Scala中的NLP》](part0291.xhtml#aid-8LGJM2
    "第9章。Scala中的NLP")中的模型监控。
- en: Summary
  id: totrans-744
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: We now have all the necessary tools to look at more complex problems that are
    more commonly called the big data problems. Armed with standard statistical algorithms—I
    understand that I have not covered many details and I am completely ready to accept
    the criticism—there is an entirely new ground to explore where we do not have
    clearly defined records, the variables in the datasets may be sparse and nested,
    and we have to cover a lot of ground and do a lot of preparatory work just to
    get to the stage where we can apply the standard statistical models. This is where
    Scala shines best.
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在拥有了所有必要的工具来查看更复杂的问题，这些更常见的问题通常被称为大数据问题。装备了标准的统计算法——我明白我没有涵盖很多细节，我完全准备好接受批评——有一个全新的领域可以探索，在那里我们没有明确定义的记录，数据集中的变量可能是稀疏和嵌套的，我们必须覆盖大量领域并做大量准备工作才能达到可以应用标准统计模型的地步。这正是
    Scala 发挥最佳作用的地方。
- en: In the next chapter, we will look more at working with unstructured data.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将更深入地探讨如何处理非结构化数据。
- en: Chapter 6. Working with Unstructured Data
  id: totrans-747
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 6 章：处理非结构化数据
- en: 'I am very excited to introduce you to this chapter. Unstructured data is what,
    in reality, makes big data different from the old data, it also makes Scala to
    be the new paradigm for processing the data. To start with, unstructured data
    at first sight seems a lot like a derogatory term. Notwithstanding, every sentence
    in this book is unstructured data: it does not have the traditional record / row
    / column semantics. For most people, however, this is the easiest thing to read
    rather than the book being presented as a table or spreadsheet.'
  id: totrans-748
  prefs: []
  type: TYPE_NORMAL
  zh: 我非常激动地向大家介绍这一章。非结构化数据是现实中使大数据与旧数据不同的东西，它也使 Scala 成为处理数据的新范式。首先，非结构化数据乍一看似乎是一个贬义词。尽管如此，这本书中的每一句话都是非结构化数据：它没有传统的记录/行/列语义。然而，对大多数人来说，这比将书籍呈现为表格或电子表格要容易阅读得多。
- en: In practice, the unstructured data means nested and complex data. An XML document
    or a photograph are good examples of unstructured data, which have very rich structure
    to them. My guess is that the originators of the term meant that the new data,
    the data that engineers at social interaction companies such as Google, Facebook,
    and Twitter saw, had a different structure to it as opposed to a traditional flat
    table that everyone used to see. These indeed did not fit the traditional RDBMS
    paradigm. Some of them can be flattened, but the underlying storage would be too
    inefficient as the RDBMSs were not optimized to handle them and also be hard to
    parse not only for humans, but for the machines as well.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，非结构化数据意味着嵌套和复杂的数据。一个 XML 文档或一张照片都是非结构化数据的良好例子，它们具有非常丰富的结构。我的猜测是，这个术语的创造者意味着新的数据，工程师在像
    Google、Facebook 和 Twitter 这样的社交互动公司看到的数据，与传统大家习惯看到的传统平面表结构不同。这些确实不符合传统的 RDBMS
    范式。其中一些可以被展平，但底层存储将过于低效，因为 RDBMS 没有优化来处理它们，而且不仅对人类，对机器来说也难以解析。
- en: A lot of techniques introduced in this chapter were created as an emergency
    Band-Aid to deal with the need to just process the data.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中介绍的大多数技术都是作为应急的 Band-Aid 来应对仅仅处理数据的需要。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Learning about the serialization, popular serialization frameworks, and language
    in which the machines talk to each other
  id: totrans-752
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关于序列化、流行的序列化框架以及机器之间交流的语言
- en: Learning about Avro-Parquet encoding for nested data
  id: totrans-753
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习关于嵌套数据的 Avro-Parquet 编码
- en: Learning how RDBMs try to incorporate nested structures in modern SQL-like languages
    to work with them
  id: totrans-754
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习 RDBMs 如何尝试在现代类似 SQL 的语言中融入嵌套结构以与之交互
- en: Learning how you can start working with nested structures in Scala
  id: totrans-755
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何在 Scala 中开始使用嵌套结构
- en: Seeing a practical example of sessionization—one of the most frequent use cases
    for unstructured data
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看一个会话化的实际例子——这是非结构化数据最常用的用例之一
- en: Seeing how Scala traits and match/case statements can simplify path analysis
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 看看 Scala 特性和 match/case 语句如何简化路径分析
- en: Learning where the nested structures can benefit your analysis
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习嵌套结构如何使你的分析受益
- en: Nested data
  id: totrans-759
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 嵌套数据
- en: 'You already saw unstructured data in the previous chapters, the data was an
    array of **LabeledPoint**, which is a tuple **(label: Double, features: Vector)**.
    The label is just a number of type **Double**. **Vector** is a sealed trait with
    two subclasses: **SparseVector** and **DenseVector**. The class diagram is as
    follows:'
  id: totrans-760
  prefs: []
  type: TYPE_NORMAL
  zh: '在前面的章节中，你已经看到了非结构化数据，数据是一个**LabeledPoint**数组的集合，其中**LabeledPoint**是一个元组**（label:
    Double, features: Vector**）。标签只是一个**Double**类型的数字。**Vector**是一个密封的特质，有两个子类：**SparseVector**和**DenseVector**。类图如下：'
- en: '![Nested data](img/image01733.jpeg)'
  id: totrans-761
  prefs: []
  type: TYPE_IMG
  zh: '![嵌套数据](img/image01733.jpeg)'
- en: 'Figure 1: The LabeledPoint class structure is a tuple of label and features,
    where features is a trait with two inherited subclasses {Dense,Sparse}Vector.
    DenseVector is an array of double, while SparseVector stores only size and non-default
    elements by index and value.'
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 图1：LabeledPoint类结构是一个标签和特征的元组，其中特征是一个具有两个继承子类{Dense,Sparse}Vector的特质。DenseVector是一个double数组，而SparseVector通过索引和值存储大小和非默认元素。
- en: 'Each observation is a tuple of label and features, and features can be sparse.
    Definitely, if there are no missing values, the whole row can be represented as
    vector. A dense vector representation requires (*8 x size + 8*) bytes. If most
    of the elements are missing—or equal to some default value—we can store only the
    non-default elements. In this case, we would require (*12 x non_missing_size +
    20*) bytes, with small variations depending on the JVM implementation. So, the
    threshold for switching between one or another, from the storage point of view,
    is when the size is greater than *1.5 x* ( *non_missing_size + 1* ), or if roughly
    at least 30% of elements are non-default. While the computer languages are good
    at representing the complex structures via pointers, we need some convenient form
    to exchange these data between JVMs or machines. First, let''s see first how Spark/Scala
    does it, specifically persisting the data in the Parquet format:'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: 每个观测值是一个标签和特征的元组，特征可以是稀疏的。当然，如果没有缺失值，整个行可以表示为一个向量。密集向量表示需要(*8 x size + 8*)字节。如果大多数元素是缺失的——或者等于某个默认值——我们只能存储非默认元素。在这种情况下，我们需要(*12
    x non_missing_size + 20*)字节，具体取决于JVM实现的小幅变化。因此，从存储的角度来看，在大小大于*1.5 x* (*non_missing_size
    + 1*)或大约至少30%的元素是非默认值时，我们需要在一种或另一种表示之间切换。虽然计算机语言擅长通过指针表示复杂结构，但我们还需要一种方便的形式来在JVM或机器之间交换这些数据。首先，让我们看看Spark/Scala是如何做的，特别是如何将数据持久化在Parquet格式中：
- en: '[PRE57]'
  id: totrans-764
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: What we did was create a new RDD dataset from command line, or we could use
    `org.apache.spark.mllib.util.MLUtils` to load a text file, converted it to a DataFrames
    and create a serialized representation of it in the Parquet file under the `points`
    directory.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 我们所做的是从命令行创建一个新的RDD数据集，或者我们可以使用`org.apache.spark.mllib.util.MLUtils`来加载一个文本文件，将其转换为DataFrames，并在`points`目录下创建其序列化表示的Parquet文件。
- en: Note
  id: totrans-766
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**What Parquet stands for?**'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: '**Parquet是什么意思？**'
- en: Apache Parquet is a columnar storage format, jointly developed by Cloudera and
    Twitter for big data. Columnar storage allows for better compression of values
    in the datasets and is more efficient if only a subset of columns need to be retrieved
    from the disk. Parquet was built from the ground up with complex nested data structures
    in mind and uses the record shredding and assembly algorithm described in the
    Dremel paper ([https://blog.twitter.com/2013/dremel-made-simple-with-parquet](https://blog.twitter.com/2013/dremel-made-simple-with-parquet)).
    Dremel/Parquet encoding uses definition/repetition fields to denote the level
    in the hierarchy the data is coming from, which covers most of the immediate encoding
    needs, as it is sufficient to store optional fields, nested arrays, and maps.
    Parquet stores the data by chunks, thus probably the name Parquet, which means
    flooring composed of wooden blocks arranged in a geometric pattern. Parquet can
    be optimized for reading only a subset of blocks from disk, depending on the subset
    of columns to be read and the index used (although it very much depends on whether
    the specific implementation is aware of these features). The values in the columns
    can use dictionary and **Run-Length Encoding** (**RLE**), which provides exceptionally
    good compression for columns with many duplicate entries, a frequent use case
    in big data.
  id: totrans-768
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Parquet是一种列式存储格式，由Cloudera和Twitter共同开发，用于大数据。列式存储允许对数据集中的值进行更好的压缩，并且在只需要从磁盘检索部分列时更为高效。Parquet是从头开始构建的，考虑到复杂嵌套数据结构，并使用了Dremel论文中描述的记录切割和组装算法（[https://blog.twitter.com/2013/dremel-made-simple-with-parquet](https://blog.twitter.com/2013/dremel-made-simple-with-parquet)）。Dremel/Parquet编码使用定义/重复字段来表示数据来自层次结构中的哪个级别，这覆盖了大多数直接的编码需求，因为它足以存储可选字段、嵌套数组和映射。Parquet通过块存储数据，因此可能得名Parquet，其意为由按几何图案排列的木块组成的地面。Parquet可以优化为只从磁盘读取部分块，这取决于要读取的列子集和使用的索引（尽管这很大程度上取决于特定实现是否了解这些功能）。列中的值可以使用字典和**运行长度编码**（**RLE**），这对于具有许多重复条目的列提供了非常好的压缩效果，这在大数据中是一个常见的用例。
- en: 'Parquet file is a binary format, but you might look at the information in it
    using `parquet-tools`, which are downloadable from [http://archive.cloudera.com/cdh5/cdh/5](http://archive.cloudera.com/cdh5/cdh/5):'
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet文件是一种二进制格式，但您可能可以使用`parquet-tools`来查看其中的信息，这些工具可以从[http://archive.cloudera.com/cdh5/cdh/5](http://archive.cloudera.com/cdh5/cdh/5)下载：
- en: '[PRE58]'
  id: totrans-770
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Let''s look at the schema, which is very close to the structure depicted in
    *Figure 1*: first member is the label of type double and the second and last one
    is features of composite type. The keyword optional is another way of saying that
    the value can be null (absent) in the record for one or another reason. The lists
    or arrays are encoded as a repeated field. As the whole array may be absent (it
    is possible for all features to be absent), it is wrapped into optional groups
    (indices and values). Finally, the type encodes whether it is a sparse or dense
    representation:'
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看模式，它与*图1*中描述的结构非常接近：第一个成员是类型为double的标签，第二个和最后一个成员是复合类型的特征。关键字`optional`是另一种表示值可以在记录中为空（缺失）的方式。列表或数组被编码为重复字段。由于整个数组可能不存在（所有特征都可能不存在），它被包裹在可选组（索引和值）中。最后，类型编码表示它是一个稀疏或密集表示：
- en: '[PRE59]'
  id: totrans-772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'You are probably a bit confused about the `R`: and `D`: in the output. These
    are the repetition and definition levels as described in the Dremel paper and
    they are necessary to efficiently encode the values in the nested structures.
    Only repeated fields increment the repetition level and only non-required fields
    increment the definition level. Drop in `R` signifies the end of the list(array).
    For every non-required level in the hierarchy tree, one needs a new definition
    level. Repetition and definition level values are small by design and can be efficiently
    stored in a serialized form.'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能对输出中的`R`和`D`感到有些困惑。这些是Dremel论文中描述的重复和定义级别，并且对于有效地编码嵌套结构中的值是必要的。只有重复字段会增加重复级别，只有非必需字段会增加定义级别。`R`的下降表示列表（数组）的结束。对于层次结构树中的每个非必需级别，都需要一个新的定义级别。重复和定义级别值设计得较小，可以有效地以序列化形式存储。
- en: What is best, if there are many duplicate entries, they will all be placed together.
    The case for which the compression algorithm (by default, it is gzip) are optimized.
    Parquet also implements other algorithms exploiting repeated values such as dictionary
    encoding or RLE compression.
  id: totrans-774
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有大量重复条目，它们都将放在一起。这是压缩算法（默认为gzip）优化的情况。Parquet还实现了其他利用重复值的算法，例如字典编码或RLE压缩。
- en: This is a simple and efficient serialization out of the box. We have been able
    to write a set of complex objects to a file, each column stored in a separate
    block, representing all values in the records and nested structures.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个简单且高效的默认序列化。我们已经能够将一组复杂对象写入文件，每个列存储在一个单独的块中，代表记录和嵌套结构中的所有值。
- en: 'Let''s now read the file and recover RDD. The Parquet format does not know
    anything about the `LabeledPoint` class, so we''ll have to do some typecasting
    and trickery here. When we read the file, we''ll see a collection of `org.apache.spark.sql.Row`:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来读取文件并恢复RDD。Parquet格式对`LabeledPoint`类一无所知，因此我们在这里需要进行一些类型转换和技巧。当我们读取文件时，我们会看到一个`org.apache.spark.sql.Row`的集合：
- en: '[PRE60]'
  id: totrans-777
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'Personally, I think that this is pretty cool: without any compilation, we can
    encode and decide complex objects. One can easily create their own objects in
    REPL. Let''s consider that we want to track user''s behavior on the web:'
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 个人认为，这相当酷：无需任何编译，我们就可以编码和决定复杂对象。在REPL中，人们可以轻松创建自己的对象。让我们考虑我们想要跟踪用户在网上的行为：
- en: '[PRE61]'
  id: totrans-779
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'As a matter of good practice, we need to register the newly created classes
    with the `Kryo` `serializer`—Spark will use another serialization mechanism to
    pass the objects between tasks and executors. If the class is not registered,
    Spark will use default Java serialization, which might be up to *10 x* slower:'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 作为良好的实践，我们需要将新创建的类注册到`Kryo` `序列化器`中——Spark将使用另一种序列化机制在任务和执行器之间传递对象。如果类未注册，Spark将使用默认的Java序列化，这可能会慢上*10倍*：
- en: '[PRE62]'
  id: totrans-781
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: If you are deploying the code on a cluster, the recommendation is to put this
    code in a jar on the classpath.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在将代码部署到集群上，建议将此代码放在类路径上的jar文件中。
- en: I've certainly seen examples of up to 10 level deep nesting in production. Although
    this might be an overkill for performance reasons, nesting is required in more
    and more production business use cases. Before we go into the specifics of constructing
    a nested object in the example of sessionization, let's get an overview of serialization
    in general.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实在生产中看到了多达10层嵌套的例子。尽管这可能在性能上可能有些过度，但在越来越多的生产业务用例中，嵌套是必需的。在我们深入到构建嵌套对象的特定示例（例如会话化）之前，让我们先对序列化的一般情况有一个概述。
- en: Other serialization formats
  id: totrans-784
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他序列化格式
- en: I do recommend the Parquet format for storing the data. However, for completeness,
    I need to at least mention other serialization formats, some of them like Kryo
    will be used implicitly for you during Spark computations without your knowledge
    and there is obviously a default Java serialization.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实推荐使用Parquet格式来存储数据。然而，为了完整性，我至少需要提及其他序列化格式，其中一些，如Kryo，将在Spark计算过程中不为人知地隐式使用，并且显然存在默认的Java序列化。
- en: Tip
  id: totrans-786
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Object-oriented approach versus functional approach**'
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: '**面向对象方法与函数式方法**'
- en: Objects in object-oriented approach are characterized by state and behavior.
    Objects are the cornerstone of object-oriented programming. A class is a template
    for objects with fields that represent the state, and methods that may represent
    the behavior. Abstract method implementation may depend on the instance of the
    class. In functional approach, the state is usually frowned upon; in pure programming
    languages, there should be no state, no side effects, and every invocation should
    return the same result. The behaviors may be expressed though additional function
    parameters and higher order functions (functions over functions, such as currying),
    but should be explicit unlike the abstract methods. Since Scala is a mix of object-oriented
    and functional language, some of the preceding constraints are violated, but this
    does not mean that you have to use them unless absolutely necessary. It is best
    practice to store the code in jar packages while storing the data, particularly
    for the big data, separate from code in data files (in a serialized form); but
    again, people often store data/configurations in jar files, and it is less common,
    but possible to store code in the data files.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 在面向对象的方法中，对象以其状态和行为为特征。对象是面向对象编程的基石。一个类是具有表示状态的字段和可能表示行为的方法的对象的模板。抽象方法实现可能依赖于类的实例。在函数式方法中，状态通常是不受欢迎的；在纯编程语言中，不应该有状态，没有副作用，并且每次调用都应该返回相同的结果。行为可以通过额外的函数参数和高级函数（如柯里化函数）来表示，但应该像抽象方法一样明确。由于Scala是面向对象和函数式语言的混合，一些先前的约束被违反了，但这并不意味着你必须在绝对必要时才使用它们。最佳实践是在存储数据的同时将代码存储在jar包中，尤其是对于大数据，应将数据文件（以序列化形式）与代码分开；但再次强调，人们经常将数据/配置存储在jar文件中，而将代码存储在数据文件中则较少见，但也是可能的。
- en: The serialization has been an issue since the need to persist data on disk or
    transfer object from one JVM or machine to another over network appeared. Really,
    the purpose of serialization is to make complex nested objects be represented
    as a series of bytes, understandable by machines, and as you can imagine, this
    might be language-dependent. Luckily, serialization frameworks converge on a set
    of common data structures they can handle.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 序列化问题自从需要在磁盘上持久化数据或通过网络将对象从一个JVM或机器传输到另一个机器以来一直存在。实际上，序列化的目的是将复杂的嵌套对象表示为一系列机器可理解的字节，正如你可以想象的那样，这可能是语言相关的。幸运的是，序列化框架在它们可以处理的一组常见数据结构上达成了一致。
- en: 'One of the most popular serialization mechanisms, but not the most efficient,
    is to dump an object in an ASCII file: CSV, XML, JSON, YAML, and so on. They do
    work for more complex nested data like structures, arrays, and maps, but are inefficient
    from the storage space perspective. For example, a Double represents a continuous
    number with 15-17 significant digits that will, without rounding or trivial ratios,
    take 15-17 bytes to represent in US ASCII, while the binary representation takes
    only 8 bytes. Integers may be stored even more efficiently, particularly if they
    are small, as we can compress/remove zeroes.'
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是最受欢迎的序列化机制之一，但不是最有效的，即在一个ASCII文件中转储对象：CSV、XML、JSON、YAML等。它们对于更复杂的嵌套数据结构，如结构、数组和映射，是有效的，但从存储空间的角度来看效率低下。例如，一个Double表示一个具有15-17位有效数字的连续数字，在没有舍入或简单比率的情况下，将需要15-17个字节来表示，而二进制表示只需要8个字节。整数可能存储得更有效率，尤其是如果它们很小，因为我们可以压缩/删除零。
- en: One advantage of text encoding is that they are much easier to visualize with
    simple command-line tools, but any advanced serialization framework now comes
    with a set of tools to work with raw records such as `avro` *-* or `parquet-tools`.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 文本编码的一个优点是它们使用简单的命令行工具更容易可视化，但现在任何高级序列化框架都附带了一套用于处理原始记录（如`avro` *-* 或 `parquet-tools`）的工具。
- en: 'The following table provides an overview for most common serialization frameworks:'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格提供了大多数常见序列化框架的概述：
- en: '| Serialization Format | When developed | Comments |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '| 序列化格式 | 开发时间 | 评论 |'
- en: '| --- | --- | --- |'
  id: totrans-794
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| XML, JSON, YAML | This was a direct response to the necessity to encode nested
    structures and exchange the data between machines. | While grossly inefficient,
    these are still used in many places, particularly in web services. The only advantage
    is that they are relatively easy to parse without machines. |'
  id: totrans-795
  prefs: []
  type: TYPE_TB
  zh: '| XML, JSON, YAML | 这是对编码嵌套结构和在机器之间交换数据的必要性的直接回应。 | 虽然效率低下，但它们仍然被许多地方使用，尤其是在网络服务中。唯一的优点是它们相对容易解析，无需机器。'
- en: '| Protobuf | Developed by Google in the early 2000s. This implements the Dremel
    encoding scheme and supports multiple languages (Scala is not officially supported
    yet, even though some code exists). | The main advantage is that Protobuf can
    generate native classes in many languages. C++, Java, and Python are officially
    supported. There are ongoing projects in C, C#, Haskell, Perl, Ruby, Scala, and
    more. Run-time can call native code to inspect/serialize/deserialize the objects
    and binary representations. |'
  id: totrans-796
  prefs: []
  type: TYPE_TB
  zh: '| Protobuf | 由谷歌在2000年代初开发。该协议实现了Dremel编码方案，并支持多种语言（Scala尚未官方支持，尽管存在一些代码）。主要优势是Protobuf可以在许多语言中生成本地类。C++、Java和Python是官方支持的语言。C、C#、Haskell、Perl、Ruby、Scala和其他语言正在进行中的项目。运行时可以调用本地代码来检查/序列化/反序列化对象和二进制表示。|'
- en: '| Avro | Avro was developed by Doug Cutting while he was working at Cloudera.
    The main objective was to separate the encoding from a specific implementation
    and language, allowing better schema evolution. | While the arguments whether
    Protobuf or Avro are more efficient are still ongoing, Avro supports a larger
    number of complex structures, say unions and maps out of the box, compared to
    Protobuf. Scala support is still to be strengthened to the production level. Avro
    files have schema encoded with every file, which has its pros and cons. |'
  id: totrans-797
  prefs: []
  type: TYPE_TB
  zh: '| Avro | Avro是由Doug Cutting在Cloudera工作时开发的。主要目标是使编码与特定实现和语言分离，从而实现更好的模式演变。|
    虽然关于Protobuf或Avro哪个更高效的争论仍在继续，但与Protobuf相比，Avro支持更多的复杂结构，例如开箱即用的联合和映射。Scala的支持仍需加强以达到生产水平。Avro文件包含每个文件的编码模式，这既有优点也有缺点。|'
- en: '| Thrift | The Apache Thrift was developed at Facebook for the same purpose
    Protobuf was developed. It probably has the widest selection of supported languages:
    C++, Java, Python, PHP, Ruby, Erlang, Perl, Haskell, C#, Cocoa, JavaScript, Node.js,
    Smalltalk, OCaml, Delphi, and other languages. Again, Twitter is hard at work
    for making the Thrift code generation in Scala ([https://twitter.github.io/scrooge/](https://twitter.github.io/scrooge/)).
    | Apache Thrift is often described as a framework for cross-language services
    development and is most frequently used as **Remote Procedure Call** (**RPC**).
    Even though it can be used directly for serialization/deserialization, other frameworks
    just happen to be more popular. |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '| Thrift | Apache Thrift是在Facebook开发的，目的是与Protobuf相同。它可能支持的语言种类最广泛：C++、Java、Python、PHP、Ruby、Erlang、Perl、Haskell、C#、Cocoa、JavaScript、Node.js、Smalltalk、OCaml、Delphi和其他语言。再次，Twitter正在努力为Scala的Thrift代码生成提供支持（[https://twitter.github.io/scrooge/](https://twitter.github.io/scrooge/))。|
    Apache Thrift通常被描述为跨语言服务开发的框架，并且最常用于**远程过程调用**（**RPC**）。尽管它可以直接用于序列化/反序列化，但其他框架却更受欢迎。|'
- en: '| Parquet | Parquet was developed in a joint effort between Twitter and Cloudera.
    Compared to the Avro format, which is row-oriented, Parquet is columnar storage
    that results in better compression and performance if only a few columns are to
    be selected. The interval encoding is Dremel or Protobuf-based, even though the
    records are presented as Avro records; thus, it is often called **AvroParquet**.
    | Advances features such as indices, dictionary encoding, and RLE compression
    potentially make it very efficient for pure disk storage. Writing the files may
    be slower as Parquet requires some preprocessing and index building before it
    can be committed to the disk. |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '| Parquet | Parquet是由Twitter和Cloudera共同开发的。与以行为导向的Avro格式相比，Parquet是列式存储，如果只选择少量列，则可以提供更好的压缩和性能。区间编码基于Dremel或Protobuf，尽管记录以Avro记录的形式呈现；因此，它通常被称为**AvroParquet**。|
    索引、字典编码和RLE压缩等高级功能可能使其对于纯磁盘存储非常高效。由于Parquet需要在提交到磁盘之前进行一些预处理和索引构建，因此写入文件可能会更慢。|'
- en: '| Kryo | This is a framework for encoding arbitrary classes in Java. However,
    not all built-in Java collection classes can be serialized. | If one avoids non-serializable
    exceptions, such as priority queues, Kryo can be very efficient. Direct support
    in Scala is also under way. |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '| Kryo | 这是一个用于在Java中编码任意类的框架。然而，并非所有内置的Java集合类都可以序列化。如果避免非序列化异常，例如优先队列，Kryo可以非常高效。Scala的直接支持也在进行中。|'
- en: Certainly, Java has a built-in serialization framework, but as it has to support
    all Java cases, and therefore is overly general, the Java serialization is far
    less efficient than any of the preceding methods. I have certainly seen other
    companies implement their own proprietary serialization earlier, which would beat
    any of the preceding serialization for the specific cases. Nowadays, it is no
    longer necessary, as the maintenance costs definitely overshadow the converging
    inefficiency of the existing frameworks.
  id: totrans-801
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，Java有一个内置的序列化框架，但由于它必须支持所有Java情况，因此过于通用，Java序列化比任何先前的序列化方法都要低效得多。我确实看到其他公司更早地实现了它们自己的专有序列化，这会优于先前的任何序列化方法。如今，这已不再必要，因为维护成本肯定超过了现有框架的收敛低效。
- en: Hive and Impala
  id: totrans-802
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Hive和Impala
- en: 'One of the design considerations for a new framework is always the compatibility
    with the old frameworks. For better or worse, most data analysts still work with
    SQL. The roots of the SQL go to an influential relational modeling paper (*Codd,
    Edgar F* (June 1970). *A Relational Model of Data for Large Shared Data Banks*.
    *Communications of the ACM (Association for Computing Machinery) 13 (6): 377–87*).
    All modern databases implement one or another version of SQL.'
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 新框架的设计考虑之一总是与旧框架的兼容性。不论好坏，大多数数据分析师仍在使用SQL。SQL的根源可以追溯到一篇有影响力的关系建模论文（Codd, Edgar
    F. （1970年6月）. 《大型共享数据银行的数据关系模型》. 《ACM通讯》（计算机机械协会）13（6）：377–87）。所有现代数据库都实现了SQL的一个或多个版本。
- en: 'While the relational model was influential and important for bringing the database
    performance, particularly for **Online Transaction Processing** (**OLTP**) to
    the competitive levels, the significance of normalization for analytic workloads,
    where one needs to perform aggregations, and for situations where relations themselves
    change and are subject to analysis, is less critical. This section will cover
    the extensions of standard SQL language for analysis engines traditionally used
    for big data analytics: Hive and Impala. Both of them are currently Apache licensed
    projects. The following table summarizes the complex types:'
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然关系模型对提高数据库性能有影响，尤其是对于**在线事务处理**（**OLTP**）的竞争力水平，但对于需要执行聚合操作的分析工作负载，以及对于关系本身发生变化并受到分析的情况，规范化的重要性较低。本节将涵盖用于大数据分析的传统分析引擎的标准SQL语言的扩展：Hive和Impala。它们目前都是Apache许可项目。以下表格总结了复杂类型：
- en: '| Type | Hive support since version | Impala support since version | Comments
    |'
  id: totrans-805
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | Hive支持版本 | Impala支持版本 | 备注 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-806
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| `ARRAY` | This is supported since 0.1.0, but the use of non-constant index
    expressions is allowed only as of 0.14. | This is supported since 2.3.0 (only
    for Parquet tables). | This can be an array of any type, including complex. The
    index is `int` in Hive (`bigint` in Impala) and access is via array notation,
    for example, `element[1]` only in Hive (`array.pos` and `item pseudocolumns` in
    Impala). |'
  id: totrans-807
  prefs: []
  type: TYPE_TB
  zh: '| `ARRAY` | 自0.1.0版本起支持，但非常量索引表达式的使用仅限于0.14版本之后。 | 自2.3.0版本起支持（仅限Parquet表）。
    | 可以是任何类型的数组，包括复杂类型。在Hive中索引为`int`（在Impala中为`bigint`），访问通过数组表示法，例如，在Hive中为`element[1]`（在Impala中为`array.pos`和`item`伪列）。
    |'
- en: '| `MAP` | This is supported since 0.1.0, but the use of non-constant index
    expressions is allowed only as of 0.14. | This is supported since 2.3.0 (only
    for Parquet tables). | The key should be of primitive type. Some libraries support
    keys of the string type only. Fields are accessed using array notation, for example,
    `map["key"]` only in Hive (map key and value pseudocolumns in Impala). |'
  id: totrans-808
  prefs: []
  type: TYPE_TB
  zh: '| `MAP` | 自0.1.0版本起支持，但非常量索引表达式的使用仅限于0.14版本之后。 | 自2.3.0版本起支持（仅限Parquet表）。 |
    键应为原始类型。一些库仅支持字符串类型的键。字段使用数组表示法访问，例如，在Hive中为`map["key"]`（在Impala中为map键和值的伪列）。
    |'
- en: '| `STRUCT` | This is supported since 0.5.0. | This is supported since 2.3.0
    (only for Parquet tables). | Access is using dot notation, for example, `struct.element`.
    |'
  id: totrans-809
  prefs: []
  type: TYPE_TB
  zh: '| `STRUCT` | 自0.5.0版本起支持。 | 自2.3.0版本起支持（仅限Parquet表）。 | 使用点表示法访问，例如，`struct.element`。
    |'
- en: '| `UNIONTYPE` | This is supported since 0.7.0. | This is not supported in Impala.
    | Support is incomplete: queries that reference `UNIONTYPE` fields in `JOIN` (HIVE-2508),
    `WHERE`, and `GROUP BY` clauses will fail, and Hive does not define the syntax
    to extract the tag or value fields of `UNIONTYPE`. This means that `UNIONTYPEs`
    are effectively look-at-only. |'
  id: totrans-810
  prefs: []
  type: TYPE_TB
  zh: '| `UNIONTYPE` | 自0.7.0以来支持。 | 在Impala中不支持。 | 支持不完整：引用`UNIONTYPE`字段的`JOIN`（HIVE-2508）、`WHERE`和`GROUP
    BY`子句的查询将失败，并且Hive没有定义提取`UNIONTYPE`的标签或值字段的语法。这意味着`UNIONTYPEs`实际上只能查看。|'
- en: 'While Hive/Impala tables can be created on top of many underlying file formats
    (Text, Sequence, ORC, Avro, Parquet, and even custom format) and multiple serializations,
    in most practical instances, Hive is used to read lines of text in ASCII files.
    The underlying serialization/deserialization format is `LazySimpleSerDe` (**Serialization**/**Deserialization**
    (**SerDe**)). The format defines several levels of separators, as follows:'
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Hive/Impala 表可以建立在许多底层文件格式（文本、序列、ORC、Avro、Parquet以及甚至自定义格式）和多种序列化之上，但在大多数实际情况下，Hive
    用于读取 ASCII 文件中的文本行。底层的序列化/反序列化格式是 `LazySimpleSerDe`（**序列化**/**反序列化**（**SerDe**））。该格式定义了多个分隔符级别，如下所示：
- en: '[PRE63]'
  id: totrans-812
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'The default for separators are `''\001''` or `^A`, `''\002''` or `^B`, and
    `''\003''` or `^B`. In other words, it''s using the new separator at each level
    of the hierarchy as opposed to the definition/repetition indicator in the Dremel
    encoding. For example, to encode the `LabeledPoint` table that we used before,
    we need to create a file, as follows:'
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 分隔符的默认值是`'\001'`或`^A`，`'\002'`或`^B`，以及`'\003'`或`^B`。换句话说，它是在层次结构的每一级使用新的分隔符，而不是在Dremel编码中使用定义/重复指示符。例如，为了对之前使用的`LabeledPoint`表进行编码，我们需要创建一个文件，如下所示：
- en: '[PRE64]'
  id: totrans-814
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'Download Hive from [http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz)
    and perform the follow:'
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 从[http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz](http://archive.cloudera.com/cdh5/cdh/5/hive-1.1.0-cdh5.5.0.tar.gz)下载Hive并执行以下操作：
- en: '[PRE65]'
  id: totrans-816
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: In Spark, select from a relational table is supported via the `sqlContext.sql`
    method, but unfortunately the Hive union types are not directly supported as of
    Spark 1.6.1; it does support maps and arrays though. The supportability of complex
    objects in other BI and data analysis tools still remains the biggest obstacle
    to their adoption. Supporting everything as a rich data structure in Scala is
    one of the options to converge on nested data representation.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，可以通过`sqlContext.sql`方法从关系型表中选择，但遗憾的是，截至Spark 1.6.1，Hive联合类型并不直接支持；尽管如此，它支持映射和数组。在其他BI和数据分析工具中支持复杂对象仍然是它们被采用的最大障碍之一。将所有内容作为丰富的数据结构在Scala中支持是收敛到嵌套数据表示的一种选项。
- en: Sessionization
  id: totrans-818
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 会话化
- en: I will demonstrate the use of the complex or nested structures in the example
    of sessionization. In sessionization, we want to find the behavior of an entity,
    identified by some ID over a period of time. While the original records may come
    in any order, we want to summarize the behavior over time to derive trends.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 我将以会话化为例，演示复杂或嵌套结构的用法。在会话化中，我们想要找到某个ID在一段时间内实体的行为。虽然原始记录可能以任何顺序到来，但我们想要总结随时间推移的行为以推导出趋势。
- en: We already analyzed web server logs in [Chapter 1](part0235.xhtml#aid-703K61
    "Chapter 1. Exploratory Data Analysis"), *Exploratory Data Analysis*. We found
    out how often different web pages are accessed over a period of time. We could
    dice and slice this information, but without analyzing the sequence of pages visited,
    it would be hard to understand each individual user interaction with the website.
    In this chapter, I would like to give this analysis more individual flavor by
    tracking the user navigation throughout the website. Sessionization is a common
    tool for website personalization and advertising, IoT tracking, telemetry, and
    enterprise security, in fact anything to do with entity behavior.
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第1章](part0235.xhtml#aid-703K61 "第1章. 探索性数据分析")中分析了网络服务器日志，*探索性数据分析*。我们发现了在一段时间内不同网页被访问的频率。我们可以对这一信息进行切块和切片，但如果没有分析页面访问的顺序，就很难理解每个用户与网站的交互。在这一章中，我想通过跟踪用户在整个网站中的导航来给这种分析增加更多的个性化特点。会话化是网站个性化、广告、物联网跟踪、遥测和企业安全等与实体行为相关的常见工具。
- en: 'Let''s assume the data comes as tuples of three elements (fields `1`, `5`,
    `11` in the original dataset in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*):'
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 假设数据以三个元素的元组形式出现（原始数据集中的字段`1`、`5`、`11`，见[第1章](part0235.xhtml#aid-703K61 "第1章.
    探索性数据分析")，*探索性数据分析*）：
- en: '[PRE66]'
  id: totrans-822
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Here, `id` is a unique entity ID, timestamp is an event `timestamp` (in any
    sortable format: Unix timestamp or an ISO8601 date format), and `path` is some
    indication of the location on the web server page hierarchy.'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`id`是一个唯一的实体ID，时间戳是一个事件的`timestamp`（任何可排序的格式：Unix时间戳或ISO8601日期格式），而`path`是关于Web服务器页面层次结构的某种指示。
- en: 'For people familiar with SQL, sessionization, or at least a subset of it, is
    better known as a windowing analytics function:'
  id: totrans-824
  prefs: []
  type: TYPE_NORMAL
  zh: 对于熟悉SQL或至少熟悉其子集的人来说，会话化（sessionization）更知名为窗口分析函数：
- en: '[PRE67]'
  id: totrans-825
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Here `ANALYTIC_FUNCTION` is some transformation on the sequence of paths for
    a given `id`. While this approach works for a relatively simple function, such
    as first, last, lag, average, expressing a complex function over a sequence of
    paths is usually very convoluted (for example, nPath from Aster Data ([https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf](https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf))).
    Besides, without additional preprocessing and partitioning, these approaches usually
    result in big data transfers across multiple nodes in a distributed setting.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`ANALYTIC_FUNCTION`是对给定`id`的路径序列进行的一些转换。虽然这种方法对于相对简单的函数，如第一个、最后一个、滞后、平均来说有效，但在路径序列上表达复杂函数通常非常复杂（例如，来自Aster
    Data的nPath ([https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf](https://www.nersc.gov/assets/Uploads/AnalyticsFoundation5.0previewfor4.6.x-Guide.pdf)))）。此外，在没有额外的预处理和分区的情况下，这些方法通常会导致在分布式环境中多个节点之间的大数据传输。
- en: While in a pure functional approach, one would just have to design a function—or
    a sequence of function applications—to produce the desired answers from the original
    set of tuples, I will create two helper objects that will help us to simplify
    working with the concept of a user session. As an additional benefit, the new
    nested structures can be persisted on a disk to speed up getting answers on additional
    questions.
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: 在纯函数式方法中，人们只需要设计一个函数或一系列函数应用来从原始的元组集中生成所需的答案，我将创建两个辅助对象，这将帮助我们简化与用户会话概念的工作。作为额外的优势，新的嵌套结构可以持久化到磁盘上，以加快对额外问题的答案获取速度。
- en: 'Let''s see how it''s done in Spark/Scala using case classes:'
  id: totrans-828
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在Spark/Scala中使用案例类是如何实现的：
- en: '[PRE68]'
  id: totrans-829
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: The first class will represent a single page view with a timestamp, which, in
    this case, is an ISO8601 `String`, while the second a sequence of page views.
    Could we do it by encoding both members as a `String` with a object separator?
    Absolutely, but representing the fields as members of a class gives us nice access
    semantics, together with offloading some of the work that we need to perform on
    the compiler, which is always nice.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个类将代表一个带有时间戳的单页浏览，在这种情况下，是一个ISO8601 `String`，而第二个是一个页浏览序列。我们能通过将这两个成员编码为一个带有对象分隔符的`String`来实现吗？当然可以，但将字段表示为类的成员提供了良好的访问语义，同时将一些需要在编译器上执行的工作卸载掉，这总是件好事。
- en: 'Let''s read the previously described log files and construct the objects:'
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取之前描述的日志文件并构建对象：
- en: '[PRE69]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Bingo! We have an RDD of Sessions, one per each unique IP address. The IP `189.248.74.238`
    has a session that lasted from `23:09:16` to `23:15:10`, and seemingly ended after
    browsing for men''s shoes. The session for IP `82.166.130.148` contains only one
    hit. The last session concentrated on sports watch and lasted for over three minutes
    from `2015-08-23 22:36:10` to `2015-08-23 22:39:26`. Now, we can easily ask questions
    involving specific navigation path patterns. For example, we want analyze all
    the sessions that resulted in checkout (the path contains `checkout`) and see
    the number of hits and the distribution of times after the last hit on homepage:'
  id: totrans-833
  prefs: []
  type: TYPE_NORMAL
  zh: 哈哈！我们得到了一个包含会话的RDD，每个唯一的IP地址对应一个会话。IP `189.248.74.238`有一个从`23:09:16`到`23:15:10`的会话，看起来是在浏览男士鞋子后结束的。IP
    `82.166.130.148`的会话只有一个点击。最后一个会话专注于运动手表，从`2015-08-23 22:36:10`到`2015-08-23 22:39:26`持续了超过三分钟。现在，我们可以轻松地询问涉及特定导航路径模式的问题。例如，我们想要分析所有导致结账（路径中包含`checkout`）的会话，并查看主页上最后点击后的点击次数和分布：
- en: '[PRE70]'
  id: totrans-834
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'The sessions last from 1 to 121 hits with a mode at 8 hits and from 15 to 2653
    seconds (or about 45 minutes). Why would you be interested in this information?
    Long sessions might indicate that there was a problem somewhere in the middle
    of the session: a long delay or non-responsive call. It does not have to be: the
    person might just have taken a long lunch break or a call to discuss his potential
    purchase, but there might be something of interest here. At least one should agree
    that this is an outlier and needs to be carefully analyzed.'
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 这些会话持续时间为1到121次点击，平均点击次数为8次，以及从15到2653秒（或大约45分钟）。你为什么会对这个信息感兴趣？长时间的会话可能表明会话中间出现了问题：长时间的延迟或无响应的电话。这并不一定意味着：这个人可能只是吃了个漫长的午餐休息或打电话讨论他的潜在购买，但这里可能有一些有趣的东西。至少应该同意这一点是一个异常值，需要仔细分析。
- en: Let's talk about persisting this data to the disk. As you've seen, our transformation
    is written as a long pipeline, so there is nothing in the result that one could
    not compute from the raw data. This is a functional approach, the data is immutable.
    Moreover, if there is an error in our processing, let's say I want to change the
    homepage to some other anchor page, I can always modify the function as opposed
    to data. You may be content or not with this fact, but there is absolutely no
    additional piece of information in the result—transformations only increase the
    disorder and entropy. They might make it more palatable for humans, but this is
    only because humans are a very inefficient data-processing apparatus.
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们谈谈将数据持久化到磁盘。正如你所看到的，我们的转换被编写为一个长管道，所以结果中没有任何东西是不能从原始数据计算出来的。这是一个函数式方法，数据是不可变的。此外，如果我们的处理过程中出现错误，比如说我想将主页更改为其他锚定页面，我总是可以修改函数而不是数据。你可能对这个事实感到满意或不满意，但结果中绝对没有额外的信息——转换只会增加无序和熵。它们可能使人类更容易接受，但这仅仅是因为人类是一个非常低效的数据处理装置。
- en: Tip
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: '**Why rearranging the data makes the analysis faster?**'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么重新排列数据会使分析更快？**'
- en: Sessionization seems just a simple rearranging of data—we just put the pages
    that were accessed in sequence together. Yet, in many cases, it makes practical
    data analysis run 10 to 100 times faster. The reason is data locality. The analysis,
    like filtering or path matching, most often tends to happen on the pages in one
    session at a time. Deriving user features requires all page views or interactions
    of the user to be in one place on disk and memory. This often beats other inefficiencies
    such as the overhead of encoding/decoding the nested structures as this can happen
    in local L1/L2 cache as opposed to data transfers from RAM or disk, which are
    much more expensive in modern multithreaded CPUs. This very much depends on the
    complexity of the analysis, of course.
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 会话化似乎只是简单的数据重新排列——我们只是将按顺序访问的页面放在一起。然而，在许多情况下，它使实际数据分析的速度提高了10到100倍。原因是数据局部性。分析，如过滤或路径匹配，通常倾向于一次在单个会话的页面上进行。推导用户特征需要用户的全部页面浏览或交互都在磁盘和内存的一个地方。这通常比其他低效性更好，比如编码/解码嵌套结构的开销，因为这可以在本地L1/L2缓存中发生，而不是从RAM或磁盘进行数据传输，这在现代多线程CPU中要昂贵得多。当然，这非常取决于分析复杂性。
- en: There is a reason to persist the new data to the disk, and we can do it with
    either CSV, Avro, or Parquet format. The reason is that we do not want to reprocess
    the data if we want to look at them again. The new representation might be more
    compact and more efficient to retrieve and show to my manager. Really, humans
    like side effects and, fortunately, Scala/Spark allows you to do this as was described
    in the previous section.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 有理由将新数据持久化到磁盘上，我们可以使用CSV、Avro或Parquet格式来完成。原因是我们不希望在再次查看数据时重新处理数据。新的表示可能更紧凑，更高效地检索和展示给我的经理。实际上，人类喜欢副作用，幸运的是，Scala/Spark允许你像前一个部分描述的那样做。
- en: 'Well, well, well...will say the people familiar with sessionization. This is
    only a part of the story. We want to split the path sequence into multiple sessions,
    run path analysis, compute conditional probabilities for page transitions, and
    so on. This is exactly where the functional paradigm shines. Write the following
    function:'
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，嗯，嗯……熟悉会话化的人会这么说。这只是故事的一部分。我们想要将路径序列分割成多个会话，运行路径分析，计算页面转换的条件概率等等。这正是函数式范式大放异彩的地方。编写以下函数：
- en: '[PRE71]'
  id: totrans-842
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Then run the following code:'
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下代码：
- en: '[PRE72]'
  id: totrans-844
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: Bingo! The result is the session's split. I intentionally left the implementation
    out; it's the implementation that is user-dependent, not the data, and every analyst
    might have it's own way to split the sequence of page visits into sessions.
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 哈哈！结果是会话的拆分。我故意省略了实现；实现是用户依赖的，而不是数据，每个分析师可能有自己将页面访问序列拆分成会话的方式。
- en: 'Another use case to apply the function is feature generation for applying machine
    learning…well, this is already hinting at the side effect: we want to modify the
    state of the world to make it more personalized and user-friendly. I guess one
    cannot avoid it after all.'
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 将函数应用于特征生成以应用机器学习……这已经暗示了副作用：我们希望修改世界的状态，使其更加个性化和用户友好。我想最终是无法避免的。
- en: Working with traits
  id: totrans-847
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与特性一起工作
- en: As we saw, case classes significantly simplify handling of new nested data structures
    that we want to construct. The case class definition is probably the most convincing
    reason to move from Java (and SQL) to Scala. Now, what about the methods? How
    do we quickly add methods to a class without expensive recompilation? Scala allows
    you to do this transparently with traits!
  id: totrans-848
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，case类显著简化了我们构造的新嵌套数据结构的处理。case类定义可能是从Java（和SQL）迁移到Scala的最有说服力的理由。现在，关于方法呢？我们如何快速向类中添加方法而不需要昂贵的重新编译？Scala允许你通过特性透明地做到这一点！
- en: A fundamental feature of functional programming is that functions are a first
    class citizen on par with objects. In the previous section, we defined the two
    `EpochSeconds` functions that transform the ISO8601 format to epoch time in seconds.
    We also suggested the `splitSession` function that provides a multi-session view
    for a given IP. How do we associate this or other behavior with a given class?
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 函数式编程的一个基本特征是函数与对象一样是第一类公民。在前一节中，我们定义了两个将ISO8601格式转换为秒的纪元时间的`EpochSeconds`函数。我们还建议了`splitSession`函数，它为给定的IP提供多会话视图。我们如何将这种行为或其他行为与给定的类关联起来？
- en: 'First, let''s define a desired behavior:'
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义一个期望的行为：
- en: '[PRE73]'
  id: totrans-851
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'This basically creates a `PageView`-specific function that converts a string
    representation for datetime to epoch time in seconds. Now, if we just make the
    following transformation:'
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 这基本上创建了一个针对`PageView`的特定函数，该函数将日期时间的字符串表示转换为秒的纪元时间。现在，如果我们只是进行以下转换：
- en: '[PRE74]'
  id: totrans-853
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'We now have a new RDD of page views with additional behavior. For example,
    if we want to find out what is the time spent on each individual page in a session
    is, we will run a pipeline, as follows:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个包含额外行为的页面浏览RDD。例如，如果我们想找出会话中每个单独页面的时间花费是多少，我们将运行以下管道：
- en: '[PRE75]'
  id: totrans-855
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Multiple traits can be added at the same time without affecting either the original
    class definitions or original data. No recompilation is required.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 可以同时添加多个特性，而不会影响原始类定义或原始数据。不需要重新编译。
- en: Working with pattern matching
  id: totrans-857
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与模式匹配一起工作
- en: 'No Scala book would be complete without mentioning the match/case statements.
    Scala has a very rich pattern-matching mechanism. For instance, let''s say we
    want to find all instances of a sequence of page views that start with a homepage
    followed by a products page—we really want to filter out the determined buyers.
    This may be accomplished with a new function, as follows:'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 没有一本Scala书籍会不提及match/case语句。Scala有一个非常丰富的模式匹配机制。例如，假设我们想找出所有以主页开始，然后是产品页面的页面浏览序列的实例——我们真正想过滤掉确定的买家。这可以通过以下新函数实现：
- en: '[PRE76]'
  id: totrans-859
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'Note that we explicitly put `PageView` constructors in the case statement!
    Scala will traverse the `visits` sequence and generate new sessions that match
    the specified two `PageViews`, as follows:'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在case语句中明确放置了`PageView`构造函数！Scala将遍历`visits`序列，并生成与指定的两个`PageViews`匹配的新会话，如下所示：
- en: '[PRE77]'
  id: totrans-861
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: I leave it to the reader to write a function that also filters only those sessions
    where the user spent less than 10 seconds before going to the products page. The
    epoch trait or the previously defined to the `EpochSeconds` function may be useful.
  id: totrans-862
  prefs: []
  type: TYPE_NORMAL
  zh: 我留给读者编写一个函数，该函数仅过滤那些用户在转到产品页面之前花费不到10秒的会话。纪元特性或先前定义的`EpochSeconds`函数可能很有用。
- en: The match/case function can be also used for feature generation and return a
    vector of features over a session.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: match/case 函数也可以用于特征生成，并返回一个会话上的特征向量。
- en: Other uses of unstructured data
  id: totrans-864
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非结构化数据的其他用途
- en: The personalization and device diagnostic obviously are not the only uses of
    unstructured data. The preceding case is a good example as we started from structured
    record and quickly converged on the need to construct an unstructured data structure
    to simplify the analysis.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 个性化设备和诊断显然不是非结构化数据的唯一用途。前面的例子是一个很好的例子，因为我们从结构化记录开始，很快转向了构建非结构化数据结构以简化分析的需求。
- en: In fact, there are many more unstructured data than there are structured; it
    is just the convenience of having the flat structure for the traditional statistical
    analysis that makes us to present the data as a set of records. Text, images,
    and music are the examples of semi-structured data.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，非结构化数据比结构化数据要多得多；只是传统统计分析中扁平结构的便利性让我们将数据呈现为记录集。文本、图像和音乐是半结构化数据的例子。
- en: One example of non-structured data is denormalized data. Traditionally the record
    data are normalized mostly for performance reasons as the RDBMSs have been optimized
    to work with structured data. This leads to foreign key and lookup tables, but
    these are very hard to maintain if the dimensions change. Denormalized data does
    not have this problem as the lookup table can be stored with each record—it is
    just an additional table object associated with a row, but may be less storage-efficient.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 非结构化数据的一个例子是非规范化数据。传统上，记录数据主要是为了性能原因而规范化的，因为关系型数据库管理系统已经针对结构化数据进行了优化。这导致了外键和查找表，但如果维度发生变化，这些表就非常难以维护。非规范化数据没有这个问题，因为查找表可以与每条记录一起存储——它只是一个与行关联的附加表对象，但可能不太节省存储空间。
- en: Probabilistic structures
  id: totrans-868
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 概率结构
- en: Another use case is the probabilistic structures. Usually people assume that
    answering a question is deterministic. As I showed in [Chapter 2](part0242.xhtml#aid-76P842
    "Chapter 2. Data Pipelines and Modeling"), *Data Pipelines and Modeling*, in many
    cases, the true answer has some uncertainty associated with it. One of the most
    popular ways to encode uncertainty is probability, which is a frequentist approach,
    meaning that the simple count of when the answer does happen to be the true answer,
    divided by the total number of attempts—the probability also can encode our beliefs.
    I will touch on probabilistic analysis and models in the following chapters, but
    probabilistic analysis requires storing each possible outcome with some measure
    of probability, which happens to be a nested structure.
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个用例是概率结构。通常人们认为回答问题是有确定性的。正如我在[第2章](part0242.xhtml#aid-76P842 "第2章。数据管道和建模")，*数据管道和建模*中所示，在许多情况下，真正的答案与一些不确定性相关。编码不确定性的最流行的方法之一是概率，这是一种频率论方法，意味着当答案确实发生时，简单的计数除以总尝试次数——概率也可以编码我们的信念。我将在以下章节中涉及到概率分析和模型，但概率分析需要存储每个可能的输出及其概率度量，这恰好是一个嵌套结构。
- en: Projections
  id: totrans-870
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 投影
- en: 'One way to deal with high dimensionality is projections on a lower dimensional
    space. The fundamental basis for why projections might work is Johnson-Lindenstrauss
    lemma. The lemma states that a small set of points in a high-dimensional space
    can be embedded into a space of much lower dimension in such a way that distances
    between the points are nearly preserved. We will touch on random and other projections
    when we talk about NLP in [Chapter 9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP
    in Scala"), *NLP in Scala*, but the random projections work well for nested structures
    and functional programming language, as in many cases, generating a random projection
    is the question of applying a function to a compactly encoded data rather than
    flattening the data explicitly. In other words, the Scala definition for a random
    projection may look like functional paradigm shines. Write the following function:'
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 处理高维性的一个方法是在低维空间上进行投影。投影可能有效的基本依据是Johnson-Lindenstrauss引理。该引理指出，高维空间中的一小部分点可以嵌入到一个维度低得多的空间中，使得点之间的距离几乎得到保留。当我们谈到第[第9章](part0291.xhtml#aid-8LGJM2
    "第9章。Scala中的NLP")，*Scala中的NLP*中的NLP时，我们将涉及到随机和其他投影，但随机投影对于嵌套结构和函数式编程语言来说效果很好，因为在许多情况下，生成随机投影是应用一个函数到紧凑编码的数据上，而不是显式地展平数据。换句话说，Scala中随机投影的定义可能看起来像是函数式范式闪耀。编写以下函数：
- en: '[PRE78]'
  id: totrans-872
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: Here, `Vector` is in low dimensional space.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，`向量`处于低维空间中。
- en: The map used for embedding is at least Lipschitz, and can even be taken to be
    an orthogonal projection.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 用于嵌入的映射至少是Lipschitz连续的，甚至可以被视为一个正交投影。
- en: Summary
  id: totrans-875
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we saw examples of how to represent and work with complex and
    nested data in Scala. Obviously, it would be hard to cover all the cases as the
    world of unstructured data is much larger than the nice niche of structured row-by-row
    simplification of the real world and is still under construction. Pictures, music,
    and spoken and written language have a lot of nuances that are hard to capture
    in a flat representation.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了如何在Scala中表示和操作复杂和嵌套数据的示例。显然，要涵盖所有情况是困难的，因为非结构化数据的世界比现实世界中结构化数据按行逐行简化的美好领域要大得多，而且仍在建设中。图片、音乐以及口语和书面语言有很多细微差别，很难在平面表示中捕捉到。
- en: While for ultimate data analysis, we eventually convert the datasets to the
    record-oriented flat representation, at least at the time of collection, one needs
    to be careful to store that data as it is and not throw away useful information
    that might be contained in data or metadata. Extending the databases and storage
    with a way to record this useful information is the first step. The next one is
    to use languages that can effectively analyze this information; which is definitely
    Scala.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行最终数据分析时，我们最终会将数据集转换为面向记录的平面表示，但至少在收集数据时，需要小心地存储数据原样，不要丢弃可能包含在数据或元数据中的有用信息。通过扩展数据库和存储，以记录这些有用信息的方式是第一步。接下来的一步是使用能够有效分析这些信息的语言；这无疑是Scala。
- en: In the next chapter we'll look at somewhat related topic of working with graphs,
    a specific example of non-structured data.
  id: totrans-878
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨与图相关的话题，这是非结构化数据的一个特定示例。
- en: Chapter 7. Working with Graph Algorithms
  id: totrans-879
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。使用图算法
- en: In this chapter, I'll delve into graph libraries and algorithm implementations
    in Scala. In particular, I will introduce Graph for Scala ([http://www.scala-graph.org](http://www.scala-graph.org)),
    an open source project that was started in 2011 in the EPFL Scala incubator. Graph
    for Scala does not support distributed computing yet—the distributed computing
    aspects of popular graph algorithms is available in GraphX, which is a part of
    MLlib library that is part of Spark project ([http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)).
    Both, Spark and MLlib were started as class projects at UC Berkeley around or
    after 2009\. I considered Spark in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib* and introduced an RDD.
    In GraphX, a graph is a pair of RDDs, each of which is partitioned among executors
    and tasks, represents vertices and edges in a graph.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将深入探讨Scala中的图库和算法实现。特别是，我将介绍Graph for Scala（[http://www.scala-graph.org](http://www.scala-graph.org)），这是一个始于2011年EPFL
    Scala孵化器的开源项目。Graph for Scala目前不支持分布式计算——流行图算法的分布式计算方面可在GraphX中找到，它是Spark项目（[http://spark.apache.org/docs/latest/mllib-guide.html](http://spark.apache.org/docs/latest/mllib-guide.html)）的一部分MLlib库。Spark和MLlib都始于2009年左右或之后在加州大学伯克利分校的课堂项目。我在[第3章](part0249.xhtml#aid-7DES21
    "第3章。使用Spark和MLlib")中考虑了Spark，并在*使用Spark和MLlib*中介绍了RDD。在GraphX中，图是一对RDD，每个RDD都在执行器和任务之间分区，代表图中的顶点和边。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Configuring **Simple Build Tool** (**SBT**) to use the material in this chapter
    interactively
  id: totrans-882
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置**简单构建工具**（**SBT**）以交互式地使用本章中的材料
- en: Learning basic operations on graphs supported by Graph for Scala
  id: totrans-883
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Graph for Scala支持的图的基本操作
- en: Learning how to enforce graph constraints
  id: totrans-884
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何强制执行图约束
- en: Learning how to import/export graphs in JSON
  id: totrans-885
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何导入/导出JSON中的图
- en: Performing connected components, triangle count, and strongly connected components
    running on Enron e-mail data
  id: totrans-886
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Enron电子邮件数据上执行连通分量、三角形计数和强连通分量的计算
- en: Performing PageRank computations on Enron e-mail data
  id: totrans-887
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Enron电子邮件数据上执行PageRank计算
- en: Learning how to use SVD++
  id: totrans-888
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用SVD++
- en: A quick introduction to graphs
  id: totrans-889
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 图的快速介绍
- en: What is a graph? A graph is a set of **vertices** where some pairs of these
    vertices are linked with **edges**. If every vertex is linked with every other
    vertex, we say the graph is a complete graph. On the contrary, if it has no edges,
    the graph is said to be empty. These are, of course, extremes that are rarely
    encountered in practice, as graphs have varying degrees of density; the more edges
    it has proportional to the number of vertices, the more dense we say it is.
  id: totrans-890
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是图？图是一组**顶点**，其中一些顶点对通过**边**相互连接。如果每个顶点都与每个其他顶点相连，我们说这个图是一个完全图。相反，如果没有边，我们说这个图是空的。当然，这些是实践中很少遇到的情况，因为图具有不同的密度；边的数量与顶点数量的比例越高，我们说它越密集。
- en: Depending on what algorithms we intend to run on a graph and how dense is it
    expected to be, we can choose how to appropriately represent the graph in memory.
    If the graph is really dense, it pays off to store it as a square *N x N* matrix,
    where *0* in the *n*th row and *m*th column means that the *n* vertex is not connected
    to the *m* vertex. A diagonal entry expresses a node connection to itself. This
    representation is called the adjacency matrix.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们打算在图上运行哪些算法以及预期的密度如何，我们可以选择如何适当地在内存中表示图。如果图非常密集，将其存储为方形的*N x N*矩阵是有益的，其中第*n*行和*m*列的*0*表示第*n*个顶点没有与第*m*个顶点相连。对角线条目表示节点与其自身的连接。这种表示称为邻接矩阵。
- en: If there are not many edges and we need to traverse the whole edge set without
    distinction, often it pays off to store it as a simple container of pairs. This
    structure is called an **edge list**.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 如果边不多，并且我们需要无区别地遍历整个边集，通常将其存储为对偶对的简单容器是有益的。这种结构称为**边表**。
- en: In practice, we can model many real-life situations and events as graphs. We
    could imagine cities as vertices and plane routes as edges. If there is no flight
    between two cities, there is no edge between them. Moreover, if we add the numerical
    costs of plane tickets to the edges, we say that the graph is **weighted**. If
    there are some edges where only travels in one direction exist, we can represent
    that by making a graph directed as opposed to an undirected graph. So, for an
    undirected graph, it is true that the graph is symmetric, that is, if *A* is connected
    to *B*, then *B* is also connected to *A*—that is not necessarily true for a directed
    graph.
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我们可以将许多现实生活中的情况和事件建模为图。我们可以想象城市作为顶点，平面航线作为边。如果两个城市之间没有航班，它们之间就没有边。此外，如果我们将机票的数值成本添加到边中，我们说这个图是**加权**的。如果有些边只存在单向旅行，我们可以通过使图有向而不是无向图来表示这一点。因此，对于一个无向图，它是对称的，即如果*A*与*B*相连，那么*B*也与*A*相连——这不一定适用于有向图。
- en: Graphs without cycles are called acyclic. Multigraph can contain multiple edges,
    potentially of different type, between the nodes. Hyperedges can connect arbitrary
    number of nodes.
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 没有环的图称为无环图。多重图可以在节点之间包含多条边，这些边可能是不同类型的。超边可以连接任意数量的节点。
- en: The most popular algorithm on the undirected graphs is probably **connected
    components**, or partitioning of a graph into subgraph, in which any two vertices
    are connected to each other by paths. Partitioning is important to parallelize
    the operations on the graphs.
  id: totrans-895
  prefs: []
  type: TYPE_NORMAL
  zh: 在无向图中，最流行的算法可能是**连通分量**，或者将图划分为子图，其中任何两个顶点都通过路径相互连接。划分对于并行化图上的操作很重要。
- en: Google and other search engines made PageRank popular. According to Google,
    PageRank estimates of how important the website is by counting the number and
    quality of links to a page. The underlying assumption is that more important websites
    are likely to receive more links from other websites, especially more highly ranked
    ones. PageRank can be applied to many problems outside of websites ranking and
    is equivalent to finding eigenvectors and the most significant eigenvalue of the
    connectivity matrix.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: Google和其他搜索引擎使PageRank变得流行。根据Google的说法，PageRank通过计算指向一个页面的链接数量和质量来估计网站的重要性。其基本假设是，更重要的网站更有可能从其他网站（尤其是排名更高的网站）那里获得更多链接。PageRank可以应用于网站排名以外的许多问题，并且相当于寻找连通矩阵的特征向量和最重要的特征值。
- en: The most basic, nontrivial subgraph, consists of three nodes. Triangle counting
    finds all the possible fully connected (or complete) triples of nodes and is another
    well-known algorithm used in community detection and CAD.
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的、非平凡的子图由三个节点组成。三角形计数找出所有可能的完全连接（或完整）的节点三元组，这是在社区检测和CAD中使用的另一个众所周知的算法。
- en: 'A **clique** is a fully connected subgraph. A strongly connected component
    is an analogous notion for a directed graph: every vertex in a subgraph is reachable
    from every other vertex. GraphX provides an implementation for both.'
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: '**团**是一个完全连接的子图。强连通分量是针对有向图的类似概念：子图中的每个顶点都可以从其他每个顶点访问。GraphX 提供了这两种实现的实现。'
- en: 'Finally, a recommender graph is a graph connecting two types of nodes: users
    and items. The edges can additionally contain the strength of a recommendation
    or a measure of satisfaction. The goal of a recommender is to predict the satisfaction
    for potentially missing edges. Multiple algorithms have been developed for a recommendation
    engine, such as SVD and SVD++, which are considered at the end of this chapter.'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，推荐图是连接两种类型节点的图：用户和项目。边可以包含推荐的强度或满意度的度量。推荐系统的目标是预测可能缺失的边的满意度。已经为推荐引擎开发了多种算法，例如
    SVD 和 SVD++，这些算法在本章的末尾进行讨论。
- en: SBT
  id: totrans-900
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SBT
- en: Everyone likes Scala REPL. REPL is the command line for Scala. It allows you
    to type Scala expressions that are evaluated immediately and try and explore things.
    As you saw in the previous chapters, one can simply type `scala` at the command
    prompt and start developing complex data pipelines. What is even more convenient
    is that one can press *tab* to have auto-completion, a required feature of any
    fully developed modern IDE (such as Eclipse or IntelliJ, *Ctrl +*. or *Ctrl +
    Space*) by keeping track of the namespace and using reflection mechanisms. Why
    would we need one extra tool or framework for builds, particularly that other
    builds management frameworks such as Ant, Maven, and Gradle exist in addition
    to IDEs? As the SBT authors argue, even though one might compile Scala using the
    preceding tools, all of them have inefficiencies, as it comes to interactivity
    and reproducibility of Scala builds (*SBT in Action* by *Joshua Suereth* and *Matthew
    Farwell*, Nov 2015).
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人都喜欢 Scala REPL。REPL 是 Scala 的命令行。它允许你输入立即评估的 Scala 表达式，尝试和探索事物。正如你在前面的章节中看到的，你可以在命令提示符中简单地输入
    `scala` 并开始开发复杂的数据管道。更方便的是，你可以按 *tab* 键进行自动完成，这是任何成熟现代 IDE（如 Eclipse 或 IntelliJ，*Ctrl
    +*. 或 *Ctrl + Space*）的必需功能，通过跟踪命名空间和使用反射机制。我们为什么需要一个额外的构建工具或框架呢，尤其是 Ant、Maven
    和 Gradle 等其他构建管理框架已经存在于 IDE 之外？正如 SBT 的作者所争论的，尽管一个人可能使用前面的工具编译 Scala，但所有这些工具在交互性和
    Scala 构建的再现性方面都有效率低下的问题（*Joshua Suereth* 和 *Matthew Farwell* 的 *SBT in Action*，2015
    年 11 月）。
- en: One of the main SBT features for me is interactivity and the ability to seamlessly
    work with multiple versions of Scala and dependent libraries. In the end, what
    is critical for software development is the speed with which one can prototype
    and test new ideas. I used to work on mainframes using punch cards, where the
    programmers were waiting to execute their programs and ideas, sometimes for hours
    and days. The efficiency of the computers mattered more, as this was the bottleneck.
    These days are gone, and a personal laptop is probably having more computing power
    than rooms full of servers a few decades back. To take advantage of this efficiency,
    we need to utilize human time more efficiently by speeding up the program development
    cycle, which also means interactivity and more versions in the repositories.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我来说，SBT 的一个主要特性是交互性和能够无缝地与多个版本的 Scala 和依赖库一起工作。最终，对于软件开发来说，关键在于能够多快地原型设计和测试新想法。我曾经在大型机使用穿孔卡片工作，程序员们等待执行他们的程序和想法，有时需要几个小时甚至几天。计算机的效率更为重要，因为这是瓶颈。那些日子已经过去了，现在个人笔记本电脑的计算能力可能比几十年前满屋的服务器还要强大。为了利用这种效率，我们需要通过加快程序开发周期来更有效地利用人的时间，这也意味着交互性和仓库中的更多版本。
- en: 'Apart from the ability to handle multiple versions and REPL, SBT''s main features
    are as follows:'
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理多个版本和 REPL 的能力之外，SBT 的主要特性如下：
- en: Native support for compiling Scala code and integrating with many test frameworks,
    including JUnit, ScalaTest, and Selenium
  id: totrans-904
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原生支持编译 Scala 代码以及与多个测试框架集成，包括 JUnit、ScalaTest 和 Selenium
- en: Build descriptions written in Scala using a DSL
  id: totrans-905
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DSL 编写的 Scala 构建描述
- en: Dependency management using Ivy (which also supports Maven-format repositories)
  id: totrans-906
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ivy 进行依赖管理（同时支持 Maven 格式仓库）
- en: Continuous execution, compilation, testing, and deployment
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续执行、编译、测试和部署
- en: Integration with the Scala interpreter for rapid iteration and debugging
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与 Scala 解释器集成以实现快速迭代和调试
- en: Support for mixed Java/Scala projects
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持混合 Java/Scala 项目
- en: Support for testing and deployment frameworks
  id: totrans-910
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持测试和部署框架
- en: Ability to complement the tool with custom plugins
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够通过自定义插件来补充工具
- en: Parallel execution of tasks
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务并行执行
- en: SBT is written in Scala and uses SBT to build itself (bootstrapping or dogfooding).
    SBT became the de facto build tool for the Scala community, and is used by the
    **Lift** and **Play** frameworks.
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: SBT是用Scala编写的，并使用SBT来构建自身（自举或自用）。SBT已成为Scala社区的默认构建工具，并被**Lift**和**Play**框架所使用。
- en: 'While you can download SBT directly from [http://www.scala-sbt.org/download](http://www.scala-sbt.org/download),
    the easiest way to install SBT on Mac is to run MacPorts:'
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然您可以从[http://www.scala-sbt.org/download](http://www.scala-sbt.org/download)直接下载SBT，但要在Mac上安装SBT最简单的方法是运行MacPorts：
- en: '[PRE79]'
  id: totrans-915
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: 'You can also run Homebrew:'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以运行Homebrew：
- en: '[PRE80]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'While other tools exist to create SBT projects, the most straightforward way
    is to run the `bin/create_project.sh` script in the GitHub book project repository
    provided for each chapter:'
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然存在其他工具来创建SBT项目，但最直接的方法是在为每个章节提供的GitHub项目存储库中运行`bin/create_project.sh`脚本：
- en: '[PRE81]'
  id: totrans-919
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'This will create main and test source subdirectories (but not the code). The
    project directory contains project-wide settings (refer to `project/build.properties`).
    The target will contain compiled classes and build packages (the directory will
    contain different subdirectories for different versions of Scala, for example,
    2.10 and 2.11). Finally, any jars or libraries put into the `lib` directory will
    be available across the project (I personally recommend using the `libraryDependencies`
    mechanism in the `build.sbt` file, but not all libraries are available via centralized
    repositories). This is the minimal setup, and the directory structure may potentially
    contain multiple subprojects. The Scalastyle plugin will even check the syntax
    for you ([http://www.scalastyle.org/sbt.html](http://www.scalastyle.org/sbt.html)).
    Just add `project/plugin.sbt`:'
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 这将创建主和测试源子目录（但不包括代码）。项目目录包含项目范围内的设置（请参阅`project/build.properties`）。目标将包含编译后的类和构建包（目录将包含不同版本的Scala的不同子目录，例如2.10和2.11）。最后，任何放入`lib`目录的jar或库都将在整个项目中可用（我建议在`build.sbt`文件中使用`libraryDependencies`机制，但并非所有库都可通过集中式存储库获得）。这是最小设置，目录结构可能包含多个子项目。Scalastyle插件甚至会为您检查语法（[http://www.scalastyle.org/sbt.html](http://www.scalastyle.org/sbt.html)）。只需添加`project/plugin.sbt`：
- en: '[PRE82]'
  id: totrans-921
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: Finally, the SBT creates Scaladoc documentation with the `sdbt doc` command.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，SBT使用`sdbt doc`命令创建Scaladoc文档。
- en: Note
  id: totrans-923
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Blank lines and other settings in build.sbt**'
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: '**build.sbt中的空白行和其他设置**'
- en: 'Probably most of the `build.sbt` files out there are double spaced: this is
    a remnant of old versions. You no longer need them. As of version 0.13.7, the
    definitions do not require extra lines.'
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 可能大多数的`build.sbt`文件都是双倍间距：这是旧版本的遗留。您不再需要它们。从版本0.13.7开始，定义不需要额外的行。
- en: There are many other settings that you can use on `build.sbt` or `build.properties`,
    the up-to-date documentation is available at [http://www.scala-sbt.org/documentation.html](http://www.scala-sbt.org/documentation.html).
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `build.sbt` 或 `build.properties` 中，你可以使用许多其他设置，最新的文档可在 [http://www.scala-sbt.org/documentation.html](http://www.scala-sbt.org/documentation.html)
    查找。
- en: When run from the command line, the tool will automatically download and use
    the dependencies, in this case, `graph-{core,constrained,json}` and `lift-json`.
    In order to run the project, simply type `sbt run`.
  id: totrans-927
  prefs: []
  type: TYPE_NORMAL
  zh: 当从命令行运行时，工具将自动下载并使用依赖项，在这种情况下，是 `graph-{core,constrained,json}` 和 `lift-json`。为了运行项目，只需输入
    `sbt run`。
- en: In continuous mode, SBT will automatically detect changes to the source file
    and rerun the command(s). In order to continuously compile and run the code, type
    `~~ run` after starting REPL with `sbt`.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 在连续模式下，SBT将自动检测源文件的变化并重新运行命令。为了在启动REPL后连续编译和运行代码，请在REPL启动后输入`~~ run`。
- en: 'To get help on the commands, run the following command:'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取命令的帮助，请运行以下命令：
- en: '[PRE83]'
  id: totrans-930
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: While SBT will be sufficient for our use even with a simple editor such as **vi**
    or **Emacs**, the `sbteclipse` project at [https://github.com/typesafehub/sbteclipse](https://github.com/typesafehub/sbteclipse)
    will create the necessary project files to work with your Eclipse IDE.
  id: totrans-931
  prefs: []
  type: TYPE_NORMAL
  zh: 即使使用简单的编辑器如**vi**或**Emacs**，SBT也足以满足我们的需求，但`sbteclipse`项目在[https://github.com/typesafehub/sbteclipse](https://github.com/typesafehub/sbteclipse)中会创建必要的项目文件，以便与您的Eclipse
    IDE一起使用。
- en: Graph for Scala
  id: totrans-932
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scala的Graph
- en: 'For this project, I will create a `src/main/scala/InfluenceDiagram.scala` file.
    For demo purpose, I will just recreate the graph from [Chapter 2](part0242.xhtml#aid-76P842
    "Chapter 2. Data Pipelines and Modeling"), *Data Pipelines and Modeling*:'
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我将创建一个`src/main/scala/InfluenceDiagram.scala`文件。为了演示目的，我将仅重新创建来自[第2章](part0242.xhtml#aid-76P842
    "第2章。数据管道和建模") *数据管道和建模*的图：
- en: '[PRE84]'
  id: totrans-934
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'The `~+>` operator is used to create a directed labeled edge between two nodes
    defined in `scalax/collection/edge/Implicits.scala`, which, in our case, are of
    the `String` type. The list of other edge types and operators is provided in the
    following table:'
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: '`~+>`运算符用于在`scalax/collection/edge/Implicits.scala`中定义的两个节点之间创建一个有向标签边，在我们的案例中，这些节点是`String`类型。其他边类型和运算符的列表如下表所示：'
- en: '*The following table shows* graph edges from `scalax.collection.edge.Implicits`
    (from [http://www.scala-graph.org/guides/core-initializing.html](http://www.scala-graph.org/guides/core-initializing.html))'
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: '*以下表格显示了来自`scalax.collection.edge.Implicits`（来自[http://www.scala-graph.org/guides/core-initializing.html](http://www.scala-graph.org/guides/core-initializing.html)）的图边*'
- en: '| Edge Class | Shortcut/Operator | Description |'
  id: totrans-937
  prefs: []
  type: TYPE_TB
  zh: '| 边类 | 速记/运算符 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-938
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| **Hyperedges** |'
  id: totrans-939
  prefs: []
  type: TYPE_TB
  zh: '| **超边** |'
- en: '| `HyperEdge` | `~` | hyperedge |'
  id: totrans-940
  prefs: []
  type: TYPE_TB
  zh: '| `HyperEdge` | `~` | 超边 |'
- en: '| `WHyperEdge` | `~%` | weighted hyperedge |'
  id: totrans-941
  prefs: []
  type: TYPE_TB
  zh: '| `WHyperEdge` | `~%` | 加权超边 |'
- en: '| `WkHyperEdge` | `~%#` | key-weighted hyperedge |'
  id: totrans-942
  prefs: []
  type: TYPE_TB
  zh: '| `WkHyperEdge` | `~%#` | 关键加权超边 |'
- en: '| `LHyperEdge` | `~+` | labeled hyperedge |'
  id: totrans-943
  prefs: []
  type: TYPE_TB
  zh: '| `LHyperEdge` | `~+` | 标签超边 |'
- en: '| `LkHyperEdge` | `~+#` | key-labeled hyperedge |'
  id: totrans-944
  prefs: []
  type: TYPE_TB
  zh: '| `LkHyperEdge` | `~+#` | 关键标签超边 |'
- en: '| `WLHyperEdge` | `~%+` | weighted labeled hyperedge |'
  id: totrans-945
  prefs: []
  type: TYPE_TB
  zh: '| `WLHyperEdge` | `~%+` | 加权标签超边 |'
- en: '| `WkLHyperEdge` | `~%#+` | key-weighted labeled hyperedge |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '| `WkLHyperEdge` | `~%#+` | 关键加权标签超边 |'
- en: '| `WLkHyperEdge` | `~%+#` | weighted key-labeled hyperedge |'
  id: totrans-947
  prefs: []
  type: TYPE_TB
  zh: '| `WLkHyperEdge` | `~%+#` | 加权关键标签超边 |'
- en: '| `WkLkHyperEdge` | `~%#+#` | key-weighted key-labeled hyperedge |'
  id: totrans-948
  prefs: []
  type: TYPE_TB
  zh: '| `WkLkHyperEdge` | `~%#+#` | 关键加权关键标签超边 |'
- en: '| **Directed hyperedges** |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '| **有向超边** |'
- en: '| `DiHyperEdge` | `~>` | directed hyperedge |'
  id: totrans-950
  prefs: []
  type: TYPE_TB
  zh: '| `DiHyperEdge` | `~>` | 有向超边 |'
- en: '| `WDiHyperEdge` | `~%>` | weighted directed hyperedge |'
  id: totrans-951
  prefs: []
  type: TYPE_TB
  zh: '| `WDiHyperEdge` | `~%>` | 加权有向超边 |'
- en: '| `WkDiHyperEdge` | `~%#>` | key-weighted directed hyperedge |'
  id: totrans-952
  prefs: []
  type: TYPE_TB
  zh: '| `WkDiHyperEdge` | `~%#>` | 关键加权有向超边 |'
- en: '| `LDiHyperEdge` | `~+>` | labeled directed hyperedge |'
  id: totrans-953
  prefs: []
  type: TYPE_TB
  zh: '| `LDiHyperEdge` | `~+>` | 标签有向超边 |'
- en: '| `LkDiHyperEdge` | `~+#>` | key-labeled directed hyperedge |'
  id: totrans-954
  prefs: []
  type: TYPE_TB
  zh: '| `LkDiHyperEdge` | `~+#>` | 关键标签有向超边 |'
- en: '| `WLDiHyperEdge` | `~%+>` | weighted labeled directed hyperedge |'
  id: totrans-955
  prefs: []
  type: TYPE_TB
  zh: '| `WLDiHyperEdge` | `~%+>` | 加权标签有向超边 |'
- en: '| `WkLDiHyperEdge` | `~%#+>` | key-weighted labeled directed hyperedge |'
  id: totrans-956
  prefs: []
  type: TYPE_TB
  zh: '| `WkLDiHyperEdge` | `~%#+>` | 关键加权标签有向超边 |'
- en: '| `WLkDiHyperEdge` | `~%+#>` | weighted key-labeled directed hyperedge |'
  id: totrans-957
  prefs: []
  type: TYPE_TB
  zh: '| `WLkDiHyperEdge` | `~%+#>` | 加权关键标签有向超边 |'
- en: '| `WkLkDiHyperEdge` | `~%#+#>` | key-weighted key-labeled directed hyperedge
    |'
  id: totrans-958
  prefs: []
  type: TYPE_TB
  zh: '| `WkLkDiHyperEdge` | `~%#+#>` | 关键加权关键标签有向超边 |'
- en: '| **Undirected edges** |'
  id: totrans-959
  prefs: []
  type: TYPE_TB
  zh: '| **无向边** |'
- en: '| `UnDiEdge` | `~` | undirected edge |'
  id: totrans-960
  prefs: []
  type: TYPE_TB
  zh: '| `UnDiEdge` | `~` | 无向边 |'
- en: '| `WUnDiEdge` | `~%` | weighted undirected edge |'
  id: totrans-961
  prefs: []
  type: TYPE_TB
  zh: '| `WUnDiEdge` | `~%` | 加权无向边 |'
- en: '| `WkUnDiEdge` | `~%#` | key-weighted undirected edge |'
  id: totrans-962
  prefs: []
  type: TYPE_TB
  zh: '| `WkUnDiEdge` | `~%#` | 关键加权无向边 |'
- en: '| `LUnDiEdge` | `~+` | labeled undirected edge |'
  id: totrans-963
  prefs: []
  type: TYPE_TB
  zh: '| `LUnDiEdge` | `~+` | 标签无向边 |'
- en: '| `LkUnDiEdge` | `~+#` | key-labeled undirected edge |'
  id: totrans-964
  prefs: []
  type: TYPE_TB
  zh: '| `LkUnDiEdge` | `~+#` | 关键标签无向边 |'
- en: '| `WLUnDiEdge` | `~%+` | weighted labeled undirected edge |'
  id: totrans-965
  prefs: []
  type: TYPE_TB
  zh: '| `WLUnDiEdge` | `~%+` | 加权标签无向边 |'
- en: '| `WkLUnDiEdge` | `~%#+` | key-weighted labeled undirected edge |'
  id: totrans-966
  prefs: []
  type: TYPE_TB
  zh: '| `WkLUnDiEdge` | `~%#+` | 关键加权标签无向边 |'
- en: '| `WLkUnDiEdge` | `~%+#` | weighted key-labeled undirected edge |'
  id: totrans-967
  prefs: []
  type: TYPE_TB
  zh: '| `WLkUnDiEdge` | `~%+#` | 加权关键标签无向边 |'
- en: '| `WkLkUnDiEdge` | `~%#+#` | key-weighted key-labeled undirected edge |'
  id: totrans-968
  prefs: []
  type: TYPE_TB
  zh: '| `WkLkUnDiEdge` | `~%#+#` | 关键加权关键标签无向边 |'
- en: '| **Directed edges** |'
  id: totrans-969
  prefs: []
  type: TYPE_TB
  zh: '| **有向边** |'
- en: '| `DiEdge` | `~>` | directed edge |'
  id: totrans-970
  prefs: []
  type: TYPE_TB
  zh: '| `DiEdge` | `~>` | 有向边 |'
- en: '| `WDiEdge` | `~%>` | weighted directed edge |'
  id: totrans-971
  prefs: []
  type: TYPE_TB
  zh: '| `WDiEdge` | `~%>` | 加权有向边 |'
- en: '| `WkDiEdge` | `~%#>` | key-weighted directed edge |'
  id: totrans-972
  prefs: []
  type: TYPE_TB
  zh: '| `WkDiEdge` | `~%#>` | 关键加权有向边 |'
- en: '| `LDiEdge` | `~+>` | labeled directed edge |'
  id: totrans-973
  prefs: []
  type: TYPE_TB
  zh: '| `LDiEdge` | `~+>` | 标签有向边 |'
- en: '| `LkDiEdge` | `~+#>` | key-labeled directed edge |'
  id: totrans-974
  prefs: []
  type: TYPE_TB
  zh: '| `LkDiEdge` | `~+#>` | 关键标签有向边 |'
- en: '| `WLDiEdge` | `~%+>` | weighted labeled directed edge |'
  id: totrans-975
  prefs: []
  type: TYPE_TB
  zh: '| `WLDiEdge` | `~%+>` | 加权标签有向边 |'
- en: '| `WkLDiEdge` | `~%#+>` | key-weighted labeled directed edge |'
  id: totrans-976
  prefs: []
  type: TYPE_TB
  zh: '| `WkLDiEdge` | `~%#+>` | 关键加权标签有向边 |'
- en: '| `WLkDiEdge` | `~%+#>` | weighted key-labeled directed edge |'
  id: totrans-977
  prefs: []
  type: TYPE_TB
  zh: '| `WLkDiEdge` | `~%+#>` | 加权关键标签有向边 |'
- en: '| `WkLkDiEdge` | `~%#+#>` | key-weighted key-labeled directed edge |'
  id: totrans-978
  prefs: []
  type: TYPE_TB
  zh: '| `WkLkDiEdge` | `~%#+#>` | 关键加权关键标签有向边 |'
- en: 'You saw the power of graph for Scala: the edges can be weighted and we may
    potentially construct a multigraph (key-labeled edges allow multiple edges for
    a pair of source and destination nodes).'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到了Scala中图的力量：边可以有权重，我们可能构建一个多图（键标签边允许一对源节点和目标节点有多个边）。
- en: 'If you run SBT on the preceding project with the Scala file in the `src/main/scala`
    directory, the output will be as follows:'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用SBT在`src/main/scala`目录中的Scala文件运行前面的项目，输出将如下所示：
- en: '[PRE85]'
  id: totrans-981
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: If continuous compilation is enabled, the main method will be run as soon as
    SBT detects that the file has changed (in the case of multiple classes having
    the main method, SBT will ask you which one to run, which is not great for interactivity;
    so you might want to limit the number of executable classes).
  id: totrans-982
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了连续编译，主方法将在SBT检测到文件已更改时立即运行（如果有多个类具有主方法，SBT将询问你想要运行哪一个，这对交互性来说不是很好；因此，你可能想要限制可执行类的数量）。
- en: I will cover different output formats in a short while, but let's first see
    how to perform simple operations on the graph.
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 我将在短时间内介绍不同的输出格式，但首先让我们看看如何在图上执行简单操作。
- en: Adding nodes and edges
  id: totrans-984
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 添加节点和边
- en: 'First, we already know that the graph is directed and acyclic, which is a required
    property for all decision diagrams so that we know we did not make a mistake.
    Let''s say that I want to make the graph more complex and add a node that will
    indicate the likelihood of me recommending a vacation in Portland, Oregon to another
    person. The only thing I need to add is the following line:'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们已经知道图是有向和无环的，这是所有决策图所需的一个属性，这样我们知道我们没有犯错误。假设我想使图更复杂，并添加一个节点来表示我向另一个人推荐俄勒冈州波特兰度假的可能性。我需要添加的只是以下这一行：
- en: '[PRE86]'
  id: totrans-986
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'If you have continuous compilation/run enabled, you will immediately see the
    changes after pressing the **Save File** button:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你启用了连续编译/运行，按下**保存文件**按钮后你将立即看到变化：
- en: '[PRE87]'
  id: totrans-988
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Now, if we want to know the parents of the newly introduced node, we can simply
    run the following code:'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想知道新引入的节点的父节点，我们可以简单地运行以下代码：
- en: '[PRE88]'
  id: totrans-990
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: 'This will give us a set of parents for a specific node—and thus drive the decision
    making process. If we add a cycle, the acyclic method will automatically detect
    it:'
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 这将为我们提供一个特定节点的父节点集——从而驱动决策过程。如果我们添加一个循环，无环方法将自动检测到：
- en: '[PRE89]'
  id: totrans-992
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'Note that you can create the graphs completely programmatically:'
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你可以完全通过编程创建图：
- en: '[PRE90]'
  id: totrans-994
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: Here, the element computation provided as the second parameter to the fill method
    is repeated `45` times (the first parameter). The graph connects every node to
    all of its predecessors, which is also known as a clique in the graph theory.
  id: totrans-995
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，作为填充方法第二个参数提供的元素计算被重复`45`次（第一个参数）。图将每个节点连接到其所有前驱节点，这在图论中也称为团。
- en: Graph constraints
  id: totrans-996
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 图约束
- en: 'Graph for Scala enables us to set constraints that cannot be violated by any
    future graph update. This comes in handy when we want to preserve some detail
    in the graph structure. For example, a **Directed Acyclic Graph** (**DAG**) should
    not contain cycles. Two constraints are currently implemented as a part of the
    `scalax.collection.constrained.constraints` package—connected and acyclic, as
    follows:'
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: Graph for Scala使我们能够设置任何未来的图更新都不能违反的约束。当我们想要保留图结构中的某些细节时，这很有用。例如，**有向无环图**（DAG）不应包含循环。目前有两个约束作为`scalax.collection.constrained.constraints`包的一部分实现——连通和无环，如下所示：
- en: '[PRE91]'
  id: totrans-998
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: 'Here is the command to run the program that tries to add or remove nodes that
    violate the constraints:'
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 这是运行尝试添加或删除违反约束的节点的程序的命令：
- en: '[PRE92]'
  id: totrans-1000
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Adding or subtracting nodes that violate one of the constraints is rejected.
    The programmer can also specify a side effect if an attempt to add or subtract
    a node that violates the condition is made.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 添加或减去违反约束之一的节点将被拒绝。如果尝试添加或减去违反条件的节点，程序员还可以指定副作用。
- en: JSON
  id: totrans-1002
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: JSON
- en: 'Graph for Scala supports importing/exporting graphs to JSON, as follows:'
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: Graph for Scala支持将图导入/导出到JSON，如下所示：
- en: '[PRE93]'
  id: totrans-1004
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'To produce a JSON representation for a sample graph, run:'
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 要为示例图生成JSON表示，请运行：
- en: '[PRE94]'
  id: totrans-1006
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: For more complex structures, one might need to write custom descriptors, serializers,
    and deserializers (refer to [http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package](http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package)).
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 对于更复杂的结构，可能需要编写自定义描述符、序列化和反序列化程序（参考[http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package](http://www.scala-graph.org/api/json/api/#scalax.collection.io.json.package)）。
- en: GraphX
  id: totrans-1008
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: GraphX
- en: 'While graph for Scala may be considered a DSL for graph operations and querying,
    one should go to GraphX for scalability. GraphX is build on top of a powerful
    Spark framework. As an example of Spark/GraphX operations, I''ll use the CMU Enron
    e-mail dataset (about 2 GB). The actual semantic analysis of the e-mail content
    is not going to be important to us until the next chapters. The dataset can be
    downloaded from the CMU site. It has e-mail from mailboxes of 150 users, primarily
    Enron managers, and about 517,401 e-mails between them. The e-mails may be considered
    as an indication of a relation (edge) between two people: Each email is an edge
    between a source (`From:`) and a destination (`To:`) vertices.'
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Graph for Scala可能被认为是图操作和查询的DSL，但应该转向GraphX以实现可扩展性。GraphX建立在强大的Spark框架之上。作为一个Spark/GraphX操作的例子，我将使用CMU
    Enron电子邮件数据集（约2 GB）。实际上，对电子邮件内容的语义分析对我们来说直到下一章才重要。数据集可以从CMU网站下载。它包含150个用户（主要是Enron经理）的电子邮件，以及他们之间大约517,401封电子邮件。这些电子邮件可以被视为两个人之间关系（边）的指示：每封电子邮件都是一个源（`From:`）和目标（`To:`）顶点的边。
- en: 'Since GraphX requires the data in RDD format, I''ll have to do some preprocessing.
    Luckily, it is extremely easy with Scala—this is why Scala is the perfect language
    for semi-structured data. Here is the code:'
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 由于GraphX需要RDD格式的数据，我必须做一些预处理。幸运的是，使用Scala来做这一点非常简单——这就是为什么Scala是半结构化数据的完美语言。以下是代码：
- en: '[PRE95]'
  id: totrans-1011
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: First, we use the `MurmurHash3` class to generate node IDs, which are of type
    `Long`, as they are required for each node in GraphX. The `emailRe` and `messageRe`
    are used to match the file content to find the required content. Scala allows
    you to parallelize the programs without much work.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用`MurmurHash3`类生成节点ID，它们是`Long`类型，因为它们在GraphX中的每个节点都是必需的。`emailRe`和`messageRe`用于将文件内容与所需内容匹配。Scala允许您在不费太多功夫的情况下并行化程序。
- en: Note the `par` call on line 50, `getFileTree(new File(args(0))).par.map`. This
    will make the loop parallel. If processing the whole Enron dataset can take up
    to an hour even on 3 GHz processor, adding parallelization reduces it by about
    8 minutes on a 32-core Intel Xeon E5-2630 2.4 GHz CPU Linux machine (it took 15
    minutes on an Apple MacBook Pro with 2.3 GHz Intel Core i7).
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 注意第50行的`par`调用，`getFileTree(new File(args(0))).par.map`。这将使循环并行化。即使是在3 GHz处理器上处理整个Enron数据集也可能需要长达一个小时，但添加并行化可以在一个32核心的Intel
    Xeon E5-2630 2.4 GHz CPU Linux机器上减少大约8分钟（在2.3 GHz Intel Core i7的Apple MacBook
    Pro上需要15分钟）。
- en: 'Running the code will produce a set of JSON records that can be loaded into
    Spark (to run it, you''ll need to put **joda-time** and **lift-json** library
    jars on the classpath), as follows:'
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码将生成一组JSON记录，这些记录可以加载到Spark中（要运行它，您需要在类路径上放置**joda-time**和**lift-json**库jar文件），如下所示：
- en: '[PRE96]'
  id: totrans-1015
  prefs: []
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Nice! Spark was able to figure out the fields and types on it''s own. If Spark
    was not able to parse all the records, one would have a `_corrupt_record` field
    containing the unparsed records (one of them is the `[success]` line at the end
    of the dataset, which can be filtered out with a `grep -Fv [success]`). You can
    see them with the following command:'
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 很好！Spark能够自己确定字段和类型。如果Spark无法解析所有记录，则会有一个`_corrupt_record`字段包含未解析的记录（其中一个是数据集末尾的`[success]`行，可以使用`grep
    -Fv [success]`过滤掉）。您可以使用以下命令查看它们：
- en: '[PRE97]'
  id: totrans-1017
  prefs: []
  type: TYPE_PRE
  zh: '[PRE97]'
- en: 'The nodes (people) and edges (relations) datasets can be extracted with the
    following commands:'
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 节点（人）和边（关系）数据集可以通过以下命令提取：
- en: '[PRE98]'
  id: totrans-1019
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: Note
  id: totrans-1020
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Node IDs in GraphX**'
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: '**GraphX中的节点ID**'
- en: As we saw in Graph for Scala, specifying the edges is sufficient for defining
    the nodes and the graph. In Spark/GraphX, nodes need to be extracted explicitly,
    and each node needs to be associated with *n* id of the `Long` type. While this
    potentially limits the flexibility and the number of unique nodes, it enhances
    the efficiency. In this particular example, generating node ID as a hash of the
    e-mail string was sufficient as no collisions were detected, but the generation
    of unique IDs is usually a hard problem to parallelize.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在Graph for Scala中看到的，指定边就足以定义节点和图。在Spark/GraphX中，节点需要显式提取，并且每个节点都需要与一个`Long`类型的*n*
    ID相关联。虽然这可能会限制灵活性和唯一节点的数量，但它提高了效率。在这个特定的例子中，将电子邮件字符串的哈希值作为节点ID是足够的，因为没有检测到冲突，但生成唯一ID通常是一个难以并行化的难题。
- en: 'The first GraphX graph is ready!! It took a bit more work than Scala for Graph,
    but now it''s totally ready for distributed processing. A few things to note:
    first, we needed to explicitly convert the fields to `Long` and `String` as the
    `Edge` constructor needed help in figuring out the types. Second, Spark might
    need to optimize the number of partitions (likely, it created too many):'
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个GraphX图已经准备好了！它比Scala的Graph多花了一些时间，但现在它完全准备好进行分布式处理了。需要注意几点：首先，我们需要明确地将字段转换为`Long`和`String`，因为`Edge`构造函数需要帮助确定类型。其次，Spark可能需要优化分区数量（很可能创建了太多的分区）：
- en: '[PRE99]'
  id: totrans-1024
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: 'To repartition, there are two calls: repartition and coalesce. The latter tries
    to avoid shuffle, as follows:'
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 要重新分区，有两个调用：repartition和coalesce。后者试图避免shuffle，如下所示：
- en: '[PRE100]'
  id: totrans-1026
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: 'However, this might limit parallelism if one performs computations over a large
    cluster. Finally, it''s a good idea to use `cache` method that pins the data structure
    in memory:'
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果在大集群上执行计算，这可能会限制并行性。最后，使用`cache`方法将数据结构固定在内存中是个好主意：
- en: '[PRE101]'
  id: totrans-1028
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'It took a few more commands to construct a graph in Spark, but four is not
    too bad. Let''s compute some statistics (and show the power of Spark/GraphX, in
    the following table:'
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中构建图需要更多的命令，但四个并不是太多。让我们计算一些统计数据（并展示Spark/GraphX的强大功能，如下表所示：
- en: Computing basic statistics on Enron e-mail graph.
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 在Enron电子邮件图上计算基本统计数据。
- en: '| Statistics | Spark command | Value for Enron |'
  id: totrans-1031
  prefs: []
  type: TYPE_TB
  zh: '| 统计信息 | Spark命令 | Enron的值 |'
- en: '| --- | --- | --- |'
  id: totrans-1032
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Total # of relations (pairwise communications) | `graph.numEdges` | 3,035,021
    |'
  id: totrans-1033
  prefs: []
  type: TYPE_TB
  zh: '| 总关系数（成对沟通） | `graph.numEdges` | 3,035,021 |'
- en: '| Number of e-mails (message IDs) | `graph.edges.map( e => e.attr._1 ).distinct.count`
    | 371,135 |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '| 电子邮件（消息ID）数量 | `graph.edges.map( e => e.attr._1 ).distinct.count` | 371,135
    |'
- en: '| Number of connected pairs | `graph.edges.flatMap( e => List((e.srcId, e.dstId),
    (e.dstId, e.srcId))).distinct.count / 2` | 217,867 |'
  id: totrans-1035
  prefs: []
  type: TYPE_TB
  zh: '| 连接对数 | `graph.edges.flatMap( e => List((e.srcId, e.dstId), (e.dstId, e.srcId))).distinct.count
    / 2` | 217,867 |'
- en: '| Number of one-way communications | `graph.edges.flatMap( e => List((e.srcId,
    e.dstId), (e.dstId, e.srcId))).distinct.count - graph.edges.map( e => (e.srcId,
    e.dstId)).distinct.count` | 193,183 |'
  id: totrans-1036
  prefs: []
  type: TYPE_TB
  zh: '| 单向沟通数 | `graph.edges.flatMap( e => List((e.srcId, e.dstId), (e.dstId, e.srcId))).distinct.count
    - graph.edges.map( e => (e.srcId, e.dstId)).distinct.count` | 193,183 |'
- en: '| Number of distinct subject lines | `graph.edges.map( e => e.attr._2 ).distinct.count`
    | 110,273 |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '| 不同的主题行数 | `graph.edges.map( e => e.attr._2 ).distinct.count` | 110,273 |'
- en: '| Total # of nodes | `graph.numVertices` | 23,607 |'
  id: totrans-1038
  prefs: []
  type: TYPE_TB
  zh: '| 节点总数 | `graph.numVertices` | 23,607 |'
- en: '| Number of destination-only nodes | `graph. numVertices - graph.edges.map(
    e => e.srcId).distinct.count` | 17,264 |'
  id: totrans-1039
  prefs: []
  type: TYPE_TB
  zh: '| 仅目标节点数 | `graph. numVertices - graph.edges.map( e => e.srcId).distinct.count`
    | 17,264 |'
- en: '| Number of source-only nodes | `graph. numVertices - graph.edges.map( e =>
    e.dstId).distinct.count` | 611 |'
  id: totrans-1040
  prefs: []
  type: TYPE_TB
  zh: '| 仅源节点数 | `graph. numVertices - graph.edges.map( e => e.dstId).distinct.count`
    | 611 |'
- en: Who is getting e-mails?
  id: totrans-1041
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 谁在接收电子邮件？
- en: 'One of the most straightforward ways to estimate people''s importance in an
    organization is to look at the number of connections or the number of incoming
    and outgoing communicates. The GraphX graph has built-in `inDegrees` and `outDegrees`
    methods. To rank the emails with respect to the number of incoming emails, run:'
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 评估一个人在组织中的重要性最直接的方法之一是查看连接数或进出沟通的数量。GraphX图内置了`inDegrees`和`outDegrees`方法。要按收到的邮件数量对电子邮件进行排名，请运行：
- en: '[PRE102]'
  id: totrans-1043
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: 'To rank the emails with respect to the number of egressing emails, run:'
  id: totrans-1044
  prefs: []
  type: TYPE_NORMAL
  zh: 要按发出的电子邮件数量对电子邮件进行排名，请运行：
- en: '[PRE103]'
  id: totrans-1045
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Let's apply some more complex algorithms to the Enron dataset.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在Enron数据集上应用一些更复杂的算法。
- en: Connected components
  id: totrans-1047
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 连接组件
- en: 'Connected components determine whether the graph is naturally partitioned into
    several parts. In the Enron relationship graph, this would mean that two or several
    groups communicate mostly between each other:'
  id: totrans-1048
  prefs: []
  type: TYPE_NORMAL
  zh: 连接组件确定图是否自然地分为几个部分。在Enron关系图中，这意味着两个或多个群体主要相互沟通：
- en: '[PRE104]'
  id: totrans-1049
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: 'We see 18 groups. Each one of the groups can be counted and extracted by filtering
    the ID. For instance, the group associated with `<[etc.survey@enron.com](mailto:etc.survey@enron.com)>`
    can be found by running a SQL query on DataFrame:'
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了18个群体。每个群体都可以通过过滤ID进行计数和提取。例如，与`<[etc.survey@enron.com](mailto:etc.survey@enron.com)>`关联的群体可以通过在DataFrame上运行SQL查询来找到：
- en: '[PRE105]'
  id: totrans-1051
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: This group is based on a single e-mail sent on September 19, 2000, from `<[survey.test@enron.com](mailto:survey.test@enron.com)>`
    to `<[etc.survey@enron](mailto:etc.survey@enron)>`. The e-mail is listed twice,
    only because it ended up in two different folders (and has two distinct message
    IDs). Only the first group, the largest subgraph, contains more than two e-mail
    addresses in the organization.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 这个群体基于 2000 年 9 月 19 日发送的一封电子邮件，从 `<[survey.test@enron.com](mailto:survey.test@enron.com)>`
    发送到 `<[etc.survey@enron](mailto:etc.survey@enron)>`。这封电子邮件被列出了两次，仅仅是因为它最终落入了两个不同的文件夹（并且有两个不同的消息
    ID）。只有第一个群体，最大的子图，包含组织中的超过两个电子邮件地址。
- en: Triangle counting
  id: totrans-1053
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 三角形计数
- en: 'The triangle counting algorithm is relatively straightforward and can be computed
    in the following three steps:'
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 三角形计数算法相对简单，可以按以下三个步骤计算：
- en: Compute the set of neighbors for each vertex.
  id: totrans-1055
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个顶点的邻居集合。
- en: For each edge, compute the intersection of the sets and send the count to both
    vertices.
  id: totrans-1056
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每条边，计算集合的交集并将计数发送到两个顶点。
- en: Compute the sum at each vertex and divide by two, as each triangle is counted
    twice.
  id: totrans-1057
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个顶点计算总和，然后除以二，因为每个三角形被计算了两次。
- en: 'We need to convert the multigraph to an undirected graph with `srcId < dstId`,
    which is a precondition for the algorithm:'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将多重图转换为具有 `srcId < dstId` 的无向图，这是算法的一个先决条件：
- en: '[PRE106]'
  id: totrans-1059
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: While there is no direct relationship between the triangle count and the importance
    of people in the organization, the people with higher triangle count arguably
    are more social—even though a clique or a strongly connected component count might
    be a better measure.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管三角形计数与组织中人们的重要性之间没有直接关系，但具有更高三角形计数的那些人可能更具社交性——尽管 clique 或强连通分量计数可能是一个更好的衡量标准。
- en: Strongly connected components
  id: totrans-1061
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强连通分量
- en: In the mathematical theory of directed graphs, a subgraph is said to be strongly
    connected if every vertex is reachable from every other vertex. It could happen
    that the whole graph is just one strongly connected component, but on the other
    end of the spectrum, each vertex could be its own connected component.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 在有向图的数学理论中，如果一个子图中的每个顶点都可以从另一个顶点到达，那么这个子图被称为强连通。可能整个图只是一个强连通分量，但另一方面，每个顶点可能就是它自己的连通分量。
- en: If you contract each connected component to a single vertex, you get a new directed
    graph that has a property to be without cycles—acyclic.
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将每个连通分量收缩为一个单一点，你会得到一个新的有向图，它具有无环的性质——无环。
- en: 'The algorithm for SCC detection is already built into GraphX:'
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: SCC 检测算法已经内置到 GraphX 中：
- en: '[PRE107]'
  id: totrans-1065
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: There are 18,200 strongly connected components with only an average 23,787/18,200
    = 1.3 users per group.
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 有 18,200 个强连通分量，每个组平均只有 23,787/18,200 = 1.3 个用户。
- en: PageRank
  id: totrans-1067
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PageRank
- en: 'The PageRank algorithm gives us an estimate of how important a person by analysing
    the links, which are the emails in this case. For example, let''s run PageRank
    on Enron email graph:'
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: PageRank 算法通过分析链接（在这种情况下是电子邮件）来估计一个人的重要性。例如，让我们在 Enron 电子邮件图中运行 PageRank：
- en: '[PRE108]'
  id: totrans-1069
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: Ostensibly, these are the go-to people. PageRank tends to emphasize the incoming
    edges, and Tana Jones returns to the top of the list compared to the 9th place
    in the triangle counting.
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 表面上，这些人就是目标人物。PageRank 倾向于强调入边，Tana Jones 相比于三角形计数的第 9 位，回到了列表的顶端。
- en: SVD++
  id: totrans-1071
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: SVD++
- en: 'SVD++ is a recommendation engine algorithm, developed specifically for Netflix
    competition by Yahuda Koren and team in 2008—the original paper is still out there
    in the public domain and can be Googled as `kdd08koren.pdf`. The specific implementation
    comes from the .NET *MyMediaLite* library by ZenoGarther ([https://github.com/zenogantner/MyMediaLite](https://github.com/zenogantner/MyMediaLite)),
    who granted Apache 2 license to the Apache Foundation. Let''s assume I have a
    set of users (on the left) and items (on the right):'
  id: totrans-1072
  prefs: []
  type: TYPE_NORMAL
  zh: SVD++ 是一种推荐引擎算法，由 Yahuda Koren 和团队在 2008 年专门为 Netflix 竞赛开发——原始论文仍在公共领域，可以通过
    Google 搜索 `kdd08koren.pdf` 获得。具体的实现来自 ZenoGarther 的 .NET *MyMediaLite* 库（[https://github.com/zenogantner/MyMediaLite](https://github.com/zenogantner/MyMediaLite)），他已将
    Apache 2 许可证授予 Apache 基金会。假设我有一组用户（在左侧）和物品（在右侧）：
- en: '![SVD++](img/image01734.jpeg)'
  id: totrans-1073
  prefs: []
  type: TYPE_IMG
  zh: '![SVD++](img/image01734.jpeg)'
- en: Figure 07-1\. A graphical representation of a recommendation problem as a bipartite
    graph.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 图 07-1\. 将推荐问题作为二分图的一个图形表示。
- en: The preceding diagram is a graphical representation of the recommendation problem.
    The nodes on the left represent users. The nodes on the right represent items.
    User **1** recommends items **A** and **C**, while users **2** and **3** recommend
    only a single item **A**. The rest of the edges are missing. The common question
    is to find recommendation ranking of the rest of the items, the edges may also
    have a weight or recommendation strength attached to them. The graph is usually
    sparse. Such graph is also often called bipartite, as the edges only go from one
    set of nodes to another set of nodes (the user does not recommend another user).
  id: totrans-1075
  prefs: []
  type: TYPE_NORMAL
  zh: 上述图表是推荐问题的图形表示。左侧的节点代表用户。右侧的节点代表物品。用户**1**推荐物品**A**和**C**，而用户**2**和**3**只推荐单个物品**A**。其余的边缺失。常见的问题是找到其余物品的推荐排名，边也可能附有权重或推荐强度。该图通常是稀疏的。这种图也常被称为二分图，因为边只从一个节点集到另一个节点集（用户不会推荐其他用户）。
- en: 'For the recommendation engine, we typically need two types of nodes—users and
    items. The recommendations are based on the rating matrix of (user, item, and
    rating) tuples. One of the implementation of the recommendation algorithm is based
    on **Singular Value Decomposition** (**SVD**) of the preceding matrix. The final
    scoring has four components: the baseline, which is the sum of average for the
    whole matrix, average for the users, and average for the items, as follows:'
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 对于推荐引擎，我们通常需要两种类型的节点——用户和物品。推荐基于（用户、物品和评分）元组的评分矩阵。推荐算法的一种实现是基于前面矩阵的**奇异值分解**（**SVD**）。最终的评分有四个组成部分：基线，即整个矩阵的平均值、用户平均和物品平均，如下所示：
- en: '![SVD++](img/image01735.jpeg)'
  id: totrans-1077
  prefs: []
  type: TYPE_IMG
  zh: '![SVD++](img/image01735.jpeg)'
- en: 'Here, the ![SVD++](img/image01736.jpeg), ![SVD++](img/image01737.jpeg), and
    ![SVD++](img/image01738.jpeg) can be understood as the averages for the whole
    population, user (among all user recommendations), and item (among all the users).
    The final part is the Cartesian product of two rows:'
  id: totrans-1078
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![SVD++](img/image01736.jpeg)、![SVD++](img/image01737.jpeg)和![SVD++](img/image01738.jpeg)可以理解为整个群体的平均值、用户（在所有用户推荐中）和物品（在所有用户中）。最后一部分是两行的笛卡尔积：
- en: '![SVD++](img/image01739.jpeg)'
  id: totrans-1079
  prefs: []
  type: TYPE_IMG
  zh: '![SVD++](img/image01739.jpeg)'
- en: 'The problem is posed as a minimization problem (refer to [Chapter 4](part0256.xhtml#aid-7K4G02
    "Chapter 4. Supervised and Unsupervised Learning"), *Supervised and Unsupervised
    Learning*):'
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 该问题被表述为一个最小化问题（参见[第4章](part0256.xhtml#aid-7K4G02 "第4章。监督学习和无监督学习"))：
- en: '![SVD++](img/image01740.jpeg)'
  id: totrans-1081
  prefs: []
  type: TYPE_IMG
  zh: '![SVD++](img/image01740.jpeg)'
- en: 'Here, ![SVD++](img/image01741.jpeg) is a regularization coefficient also discussed
    in [Chapter 4](part0256.xhtml#aid-7K4G02 "Chapter 4. Supervised and Unsupervised
    Learning"), *Supervised and Unsupervised Learning*. So, each user is associated
    with a set of numbers (![SVD++](img/image01742.jpeg), and each item with ![SVD++](img/image01743.jpeg),
    ![SVD++](img/image01744.jpeg). In this particlar implementation, the optimal coefficients
    are found by gradient descent. This is the basic of SVD optimization. In linear
    algebra, SVD takes an arbitrary ![SVD++](img/image01745.jpeg) matrix *A* and represents
    it as a product of an orthogonal ![SVD++](img/image01745.jpeg) matrix *U*, a diagonal
    ![SVD++](img/image01745.jpeg) matrix ![SVD++](img/image01746.jpeg), and a ![SVD++](img/image01745.jpeg)
    unitary matrix *V*, for example, the columns are mutually orthogonal. Arguably,
    if one takes the largest ![SVD++](img/image01747.jpeg) entries of the ![SVD++](img/image01746.jpeg)
    matrix, the product is reduced to the product of a very tall ![SVD++](img/image01748.jpeg)
    matrix and a wide ![SVD++](img/image01749.jpeg) matric, where ![SVD++](img/image01747.jpeg)
    is called the rank of decomposition. If the remaining values are small, the new
    ![SVD++](img/image01750.jpeg) numbers approximate the original ![SVD++](img/image01745.jpeg)
    numbers for the relation, *A*. If *m* and *n* are large to start with, and in
    practical online shopping situations, *m* is the items and can be in hundreds
    of thousands, and *n* is the users and can be hundreds of millions, the saving
    can be substantial. For example, for *r=10*, *m=100,000*, and *n=100,000,000*,
    the savings are as follows:'
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![SVD++](img/image01741.jpeg) 是在 [第 4 章](part0256.xhtml#aid-7K4G02 "第 4
    章。监督学习和无监督学习") 中讨论的正则化系数，*监督学习和无监督学习*。因此，每个用户都与一组数字 ([SVD++](img/image01742.jpeg)，以及每个项目与
    ![SVD++](img/image01743.jpeg)，![SVD++](img/image01744.jpeg) 相关联。在这个特定的实现中，最优系数是通过梯度下降法找到的。这是
    SVD 优化的基础。在线性代数中，SVD 将一个任意的 ![SVD++](img/image01745.jpeg) 矩阵 *A* 表示为一个正交 ![SVD++](img/image01745.jpeg)
    矩阵 *U*、一个对角 ![SVD++](img/image01745.jpeg) 矩阵 ![SVD++](img/image01746.jpeg) 和一个
    ![SVD++](img/image01745.jpeg) 单位矩阵 *V* 的乘积，例如，列是相互正交的。可以说，如果取 ![SVD++](img/image01746.jpeg)
    矩阵中最大的 ![SVD++](img/image01747.jpeg) 个条目，乘积就简化为一个非常高 ![SVD++](img/image01748.jpeg)
    的矩阵和一个很宽 ![SVD++](img/image01749.jpeg) 的矩阵的乘积，其中 ![SVD++](img/image01747.jpeg)
    被称为分解的秩。如果剩余的值很小，新的 ![SVD++](img/image01750.jpeg) 数字就近似于原始 ![SVD++](img/image01745.jpeg)
    数字，对于关系 *A*。如果 *m* 和 *n* 起初就很大，在实践中的在线购物场景中，*m* 是商品，可能有数十万，而 *n* 是用户，可能有数亿，这种节省可能是巨大的。例如，对于
    *r=10*，*m=100,000*，和 *n=100,000,000*，节省如下：
- en: '![SVD++](img/image01751.jpeg)'
  id: totrans-1083
  prefs: []
  type: TYPE_IMG
  zh: '![SVD++](img/image01751.jpeg)'
- en: 'SVD can also be viewed as PCA for matrices with ![SVD++](img/image01752.jpeg).
    In the Enron case, we can treat senders as users and recipients as items (we''ll
    need to reassign the node IDs), as follows:'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 也可以看作是针对 ![SVD++](img/image01752.jpeg) 矩阵的 PCA。在 Enron 案例中，我们可以将发件人视为用户，收件人视为商品（我们需要重新分配节点
    ID），如下所示：
- en: '[PRE109]'
  id: totrans-1085
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: 'The `svdRanks` is the user-part of the ![SVD++](img/image01753.jpeg) prediction.
    The distribution lists take a priority as this is usually used for mass e-mailing.
    To get the user-specific part, we need to provide the user ID:'
  id: totrans-1086
  prefs: []
  type: TYPE_NORMAL
  zh: '`svdRanks` 是 ![SVD++](img/image01753.jpeg) 预测的用户部分。分布列表具有优先级，因为这通常用于群发电子邮件。要获取用户特定的部分，我们需要提供用户
    ID：'
- en: '[PRE110]'
  id: totrans-1087
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Here, we computed the top five recommended e-mail-to list for top in-degree
    and out-degree users.
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们计算了度数最高的前五个推荐电子邮件列表。
- en: SVD has only 159 lines of code in Scala and can be the basis for some further
    improvements. SVD++ includes a part based on implicit user feedback and item similarity
    information. Finally, the Netflix winning solution had also taken into consideration
    the fact that user preferences are time-dependent, but this part has not been
    implemented in GraphX yet.
  id: totrans-1089
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 在 Scala 中只有 159 行代码，可以成为一些进一步改进的基础。SVD++ 包括基于隐式用户反馈和商品相似性信息的一部分。最后，Netflix
    获胜方案也考虑到了用户偏好随时间变化的事实，但这一部分尚未在 GraphX 中实现。
- en: Summary
  id: totrans-1090
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: While one can easily create their own data structures for graph problems, Scala's
    support for graphs comes from both semantic layer—Graph for Scala is effectively
    a convenient, interactive, and expressive language for working with graphs—and
    scalability via Spark and distributed computing. I hope that some of the material
    exposed in this chapter will be useful for implementing algorithms on top of Scala,
    Spark, and GraphX. It is worth mentioning that bot libraries are still under active
    development.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然人们可以轻松地为图问题创建自己的数据结构，但Scala对图的支持既来自语义层——对于Scala来说，Graph实际上是一种方便、交互式且表达性强的语言，用于处理图——也来自通过Spark和分布式计算的可扩展性。我希望本章中暴露的一些材料将对在Scala、Spark和GraphX之上实现算法有所帮助。值得一提的是，这两个库仍在积极开发中。
- en: In the next chapter, we'll step down from from our flight in the the skies and
    look at Scala integration with traditional data analysis frameworks such as statistical
    language R and Python, which are often used for data munching. Later, in [Chapter
    9](part0291.xhtml#aid-8LGJM2 "Chapter 9. NLP in Scala"), *NLP in Scala*. I'll
    look at NLP Scala tools, which leverage complex data structures extensively.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将从天空中我们的飞行中降下来，看看Scala与传统的数据分析框架的集成，如统计语言R和Python，这些语言通常用于数据处理。稍后，在[第9章](part0291.xhtml#aid-8LGJM2
    "第9章. Scala中的NLP")中，我将探讨*Scala中的NLP*。我会查看NLP Scala工具，这些工具广泛利用复杂的数据结构。
- en: Chapter 8. Integrating Scala with R and Python
  id: totrans-1093
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第8章. 将Scala与R和Python集成
- en: While Spark provides MLlib as a library for machine learning, in many practical
    situations, R or Python present a more familiar and time-tested interface for
    statistical computations. In particular, R's extensive statistical library includes
    very popular ANOVA/MANOVA methods of analyzing variance and variable dependencies/independencies,
    sets of statistical tests, and random number generators that are not currently
    present in MLlib. The interface from R to Spark is available under SparkR project.
    Finally, data analysts know Python's NumPy and SciPy linear algebra implementations
    for their efficiency as well as other time-series, optimization, and signal processing
    packages. With R/Python integration, all these familiar functionalities can be
    exposed to Scala/Spark users until the Spark/MLlib interfaces stabilize and the
    libraries make their way into the new framework while benefiting the users with
    Spark's ability to execute workflows in a distributed way across multiple machines.
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Spark提供了MLlib作为机器学习库，但在许多实际情况下，R或Python提供了更熟悉且经过时间考验的统计计算接口。特别是，R的广泛统计库包括非常流行的方差和变量依赖/独立分析方法（ANOVA/MANOVA）、一系列统计测试和随机数生成器，这些目前尚未出现在MLlib中。R到Spark的接口可在SparkR项目中找到。最后，数据分析师知道Python的NumPy和SciPy线性代数实现因其效率以及其他时间序列、优化和信号处理包而闻名。通过R/Python集成，所有这些熟悉的功能都可以暴露给Scala/Spark用户，直到Spark/MLlib接口稳定，并且库进入新框架，同时利用Spark在多台机器上以分布式方式执行工作流的能力为用户带来好处。
- en: When people program in R or Python, or with any statistical or linear algebra
    packages for this matter, they are usually not specifically focusing on the functional
    programming aspects. As I mentioned in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*, Scala should be treated as a high-level
    language and this is where it shines. Integration with highly efficient C and
    Fortran implementations, for example, of the freely available **Basic Linear Algebra
    Subprograms** (**BLAS**), **Linear Algebra** **Package** (**LAPACK**), and **Arnoldi
    Package** (**ARPACK**), is known to find its way into Java and thus Scala ([http://www.netlib.org](http://www.netlib.org),
    [https://github.com/fommil/netlib-java](https://github.com/fommil/netlib-java)).
    I would like to leave Scala at what it's doing best. In this chapter, however,
    I will focus on how to use these languages with Scala/Spark.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们在R或Python中编程，或者使用任何统计或线性代数包时，他们通常不会特别关注函数式编程方面。正如我在[第1章](part0235.xhtml#aid-703K61
    "第1章. 探索性数据分析")中提到的，*探索性数据分析*，Scala应该被视为一种高级语言，这正是它的亮点。与高效且免费可用的**基本线性代数子程序**（**BLAS**）、**线性代数包**（**LAPACK**）和**Arnoldi包**（**ARPACK**）的C和Fortran实现集成，已知其可以进入Java和Scala（[http://www.netlib.org](http://www.netlib.org)，[https://github.com/fommil/netlib-java](https://github.com/fommil/netlib-java)）。我希望将Scala留在它最擅长的地方。然而，在本章中，我将专注于如何使用这些语言与Scala/Spark一起使用。
- en: I will use the publicly available United States Department of Transportation
    flights dataset for this chapter ([http://www.transtats.bts.gov](http://www.transtats.bts.gov)).
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用公开可用的美国交通部航班数据集来介绍本章内容（[http://www.transtats.bts.gov](http://www.transtats.bts.gov)）。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Installing R and configuring SparkR if you haven't done so yet
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请安装R和配置SparkR
- en: Learning about R (and Spark) DataFrames
  id: totrans-1099
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解R（和Spark）DataFrame
- en: Performing linear regression and ANOVA analysis with R
  id: totrans-1100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R进行线性回归和方差分析
- en: Performing **Generalized Linear Model** (**GLM**) modeling with SparkR
  id: totrans-1101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用SparkR进行**广义线性模型**（**GLM**）建模
- en: Installing Python if you haven't done so yet
  id: totrans-1102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您还没有这样做，请安装Python
- en: Learning how to use PySpark and call Python from Scala
  id: totrans-1103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习如何使用PySpark并从Scala调用Python
- en: Integrating with R
  id: totrans-1104
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与R集成
- en: As with many advanced and carefully designed technologies, people usually either
    love or hate R as a language. One of the reason being that R was one of the first
    language implementations that tries to manipulate complex objects, even though
    most of them turn out to be just a list as opposed to struct or map as in more
    mature modern implementations. R was originally created at the University of Auckland
    by Ross Ihaka and Robert Gentleman around 1993, and had its roots in the S language
    developed at Bell Labs around 1976, when most of the commercial programming was
    still done in Fortran. While R incorporates some functional features such as passing
    functions as a parameter and map/apply, it conspicuously misses some others such
    as lazy evaluation and list comprehensions. With all this said, R has a very good
    help system, and if someone says that they never had to go back to the `help(…)`
    command to figure out how to run a certain data transformation or model better,
    they are either lying or just starting in R.
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 就像许多高级且精心设计的科技一样，人们对R语言通常要么爱得要命，要么恨之入骨。其中一个原因在于，R语言是首批尝试操作复杂对象的编程语言之一，尽管大多数情况下它们最终只是列表，而不是像更成熟的现代实现那样是结构体或映射。R语言最初由罗素·伊哈卡和罗伯特·甘特曼于1993年左右在奥克兰大学创建，其根源可以追溯到1976年左右在贝尔实验室开发的S语言，当时大多数商业编程仍在Fortran语言中进行。虽然R语言包含一些功能特性，如将函数作为参数传递和map/apply，但它明显缺少一些其他特性，如惰性评估和列表推导。尽管如此，R语言有一个非常好的帮助系统，如果有人说他们从未需要回到`help(…)`命令来了解如何更好地运行某个数据转换或模型，那么他们要么在撒谎，要么是刚开始使用R。
- en: Setting up R and SparkR
  id: totrans-1106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置R和SparkR
- en: To run SparkR, you'll need R version 3.0 or later. Follow the given instructions
    for the installation, depending on you operating system.
  id: totrans-1107
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行SparkR，您需要R版本3.0或更高版本。根据您的操作系统，遵循给定的安装说明。
- en: Linux
  id: totrans-1108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Linux
- en: 'On a Linux system, detailed installation documentation is available at [https://cran.r-project.org/bin/linux](https://cran.r-project.org/bin/linux).
    However, for example, on a Debian system, one installs it by running the following
    command:'
  id: totrans-1109
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux系统上，详细的安装文档可在[https://cran.r-project.org/bin/linux](https://cran.r-project.org/bin/linux)找到。然而，例如，在Debian系统上，您可以通过运行以下命令来安装它：
- en: '[PRE111]'
  id: totrans-1110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: 'To list installed/available packages on the Linux repository site, perform
    the following command:'
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 要列出Linux存储库站点上安装的/可用的包，请执行以下命令：
- en: '[PRE112]'
  id: totrans-1112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: 'R packages, which are a part of `r-base` and `r-recommended`, are installed
    into the `/usr/lib/R/library` directory. These can be updated using the usual
    package maintenance tools such as `apt-get` or aptitude. The other R packages
    available as precompiled Debian packages, `r-cran-*` and `r-bioc-*`, are installed
    into `/usr/lib/R/site-library`. The following command shows all packages that
    depend on `r-base-core`:'
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: R包，它们是`r-base`和`r-recommended`的一部分，被安装到`/usr/lib/R/library`目录中。这些可以使用常规的包维护工具进行更新，例如`apt-get`或aptitude。其他作为预编译的Debian包提供的R包，`r-cran-*`和`r-bioc-*`，被安装到`/usr/lib/R/site-library`。以下命令显示了所有依赖于`r-base-core`的包：
- en: '[PRE113]'
  id: totrans-1114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: 'This comprises of a large number of contributed packages from CRAN and other
    repositories. If you want to install R packages that are not provided as package,
    or if you want to use newer versions, you need to build them from source that
    requires the `r-base-dev` development package that can be installed by the following
    command:'
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 这包括来自CRAN和其他存储库的大量贡献包。如果您想安装作为包未提供的R包，或者如果您想使用更新的版本，您需要从源代码构建它们，这需要安装`r-base-dev`开发包，可以通过以下命令安装：
- en: '[PRE114]'
  id: totrans-1116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: 'This pulls in the basic requirements to compile R packages, such as the development
    tools group install. R packages may then be installed by the local user/admin
    from the CRAN source packages, typically from inside R using the `R> install.packages()`
    function or `R CMD INSTALL`. For example, to install the R `ggplot2` package,
    run the following command:'
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 这将引入编译 R 包的基本要求，例如开发工具组的安装。然后，本地用户/管理员可以从 CRAN 源代码包中安装 R 包，通常在 R 中使用 `R> install.packages()`
    函数或 `R CMD INSTALL` 来安装。例如，要安装 R 的 `ggplot2` 包，请运行以下命令：
- en: '[PRE115]'
  id: totrans-1118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: 'This will download and optionally compile the package and its dependencies
    from one of the available sites. Sometime R is confused about the repositories;
    in this case, I recommend creating a `~/.Rprofile` file in the home directory
    pointing to the closest CRAN repository:'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 这将从可用的站点之一下载并可选地编译该包及其依赖项。有时 R 会弄混存储库；在这种情况下，我建议在主目录中创建一个 `~/.Rprofile` 文件，指向最近的
    CRAN 存储库：
- en: '[PRE116]'
  id: totrans-1120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: '`~/.Rprofile` contains commands to customize your sessions. One of the commands
    I recommend to put in there is `options (prompt="R> ")` to be able to distinguish
    the shell you are working in by the prompt, following the tradition of most tools
    in this book. The list of known mirrors is available at [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html).'
  id: totrans-1121
  prefs: []
  type: TYPE_NORMAL
  zh: '`~/.Rprofile` 包含用于自定义会话的命令。我建议在其中放入的命令之一是 `options (prompt="R> ")`，以便能够通过提示符区分您正在使用的
    shell，遵循本书中大多数工具的传统。已知镜像列表可在 [https://cran.r-project.org/mirrors.html](https://cran.r-project.org/mirrors.html)
    查找。'
- en: 'Also, it is good practice to specify the directory to install `system/site/user`
    packages via the following command, unless your OS setup does it already by putting
    these commands into `~/.bashrc` or system `/etc/profile`:'
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，指定通过以下命令安装 `system/site/user` 包的目录是一种良好的做法，除非您的操作系统设置已经通过将这些命令放入 `~/.bashrc`
    或系统 `/etc/profile` 来完成：
- en: '[PRE117]'
  id: totrans-1123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: Mac OS
  id: totrans-1124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Mac OS
- en: 'R for Mac OS can be downloaded, for example, from [http://cran.r-project.org/bin/macosx](http://cran.r-project.org/bin/macosx).
    The latest version at the time of the writing is 3.2.3\. Always check the consistency
    of the downloaded package. To do so, run the following command:'
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: R for Mac OS 可以从 [http://cran.r-project.org/bin/macosx](http://cran.r-project.org/bin/macosx)
    下载。写作时的最新版本是 3.2.3。始终检查下载的包的一致性。为此，请运行以下命令：
- en: '[PRE118]'
  id: totrans-1126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: The environment settings in the preceding subsection also apply to the Mac OS
    setup.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 前一小节中的环境设置也适用于 Mac OS 设置。
- en: Windows
  id: totrans-1128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Windows
- en: R for Windows can be downloaded from [https://cran.r-project.org/bin/windows/](https://cran.r-project.org/bin/windows/)
    as an exe installer. Run this executable as an administrator to install R.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: R for Windows 可以从 [https://cran.r-project.org/bin/windows/](https://cran.r-project.org/bin/windows/)
    下载为一个 exe 安装程序。以管理员身份运行此可执行文件来安装 R。
- en: One can usually edit the environment setting for **System/User** by following
    the **Control Panel** | **System and Security** | **System** | **Advanced system
    settings** | **Environment Variables** path from the Windows menu.
  id: totrans-1130
  prefs: []
  type: TYPE_NORMAL
  zh: 通常可以通过按照 Windows 菜单中的 **控制面板** | **系统和安全** | **系统** | **高级系统设置** | **环境变量**
    路径来编辑 **系统/用户** 的环境设置。
- en: Running SparkR via scripts
  id: totrans-1131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过脚本运行 SparkR
- en: 'To run SparkR, one needs to run install the `R/install-dev.sh` script that
    comes with the Spark git tree. In fact, one only needs the shell script and the
    content of the `R/pkg` directory, which is not always included with the compiled
    Spark distributions:'
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行 SparkR，需要运行 Spark git 树中包含的 `R/install-dev.sh` 脚本。实际上，只需要 shell 脚本和 `R/pkg`
    目录的内容，这些内容并不总是包含在编译好的 Spark 发行版中：
- en: '[PRE119]'
  id: totrans-1133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Running Spark via R's command line
  id: totrans-1134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通过 R 的命令行运行 Spark
- en: 'Alternatively, we can also initialize Spark from the R command line directly
    (or from RStudio at [http://rstudio.org/](http://rstudio.org/)) using the following
    commands:'
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们也可以直接从 R 命令行（或从 RStudio [http://rstudio.org/](http://rstudio.org/)）使用以下命令初始化
    Spark：
- en: '[PRE120]'
  id: totrans-1136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: As described previously in [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working
    with Spark and MLlib"), *Working with Spark and MLlib*, the `SPARK_HOME` environment
    variable needs to point to your local Spark installation directory and `SPARK_MASTER`
    and `YARN_CONF_DIR` to the desired cluster manager (local, standalone, mesos,
    and YARN) and YARN configuration directory if one is using Spark with the YARN
    cluster manager.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 如前文第 3 章 [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark
    and MLlib") 中所述，在 *Working with Spark and MLlib* 中，`SPARK_HOME` 环境变量需要指向您的本地 Spark
    安装目录，`SPARK_MASTER` 和 `YARN_CONF_DIR` 指向所需的集群管理器（本地、独立、mesos 和 YARN）以及 YARN 配置目录，如果使用
    Spark 与 YARN 集群管理器一起使用的话。
- en: Although most all of the distributions come with a UI, in the tradition of this
    book and for the purpose of this chapter I'll use the command line.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数分布都带有UI，但根据本书的传统和本章的目的，我将使用命令行。
- en: DataFrames
  id: totrans-1139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrames
- en: The DataFrames originally came from R and Python, so it is natural to see them
    in SparkR.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: DataFrames最初来自R和Python，所以在SparkR中看到它们是很自然的。
- en: Note
  id: totrans-1141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Please note that the implementation of DataFrames in SparkR is on top of RDDs,
    so they work differently than the R DataFrames.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，SparkR中DataFrame的实现是在RDD之上，因此它们的工作方式与R DataFrame不同。
- en: The question of when and where to store and apply the schema and other metadata
    like types has been a topic of active debate recently. On one hand, providing
    the schema early with the data enables thorough data validation and potentially
    optimization. On the other hand, it may be too restrictive for the original data
    ingest, whose goal is just to capture as much data as possible and perform data
    formatting/cleansing later on, the approach often referred as schema on read.
    The latter approach recently won more ground with the tools to work with evolving
    schemas such as Avro and automatic schema discovery tools, but for the purpose
    of this chapter, I'll assume that we have done the schema discovery part and can
    start working with a DataFrames.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 最近，关于何时何地存储和应用模式以及类型等元数据的问题一直是活跃讨论的课题。一方面，在数据早期提供模式可以实现对数据的彻底验证和潜在的优化。另一方面，对于原始数据摄取来说可能过于限制，其目标只是尽可能多地捕获数据，并在之后进行数据格式化和清洗，这种方法通常被称为“读取时模式”。后者最近由于有了处理演变模式的工具（如Avro）和自动模式发现工具而获得了更多支持，但为了本章的目的，我将假设我们已经完成了模式发现的部分，并可以开始使用DataFrames。
- en: 'Let''s first download and extract a flight delay dataset from the United States
    Department of Transportation, as follows:'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先从美国交通部下载并提取一个航班延误数据集，如下所示：
- en: '[PRE121]'
  id: totrans-1145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: 'If you have Spark running on the cluster, you want to copy the file in HDFS:'
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在集群上运行了Spark，你想要将文件复制到HDFS：
- en: '[PRE122]'
  id: totrans-1147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: 'The `flights/readme.html` files gives you detailed metadata information, as
    shown in the following image:'
  id: totrans-1148
  prefs: []
  type: TYPE_NORMAL
  zh: '`flights/readme.html`文件提供了详细的元数据信息，如下所示：'
- en: '![DataFrames](img/image01754.jpeg)'
  id: totrans-1149
  prefs: []
  type: TYPE_IMG
  zh: '![DataFrames](img/image01754.jpeg)'
- en: 'Figure 08-1: Metadata provided with the On-Time Performance dataset released
    by the US Department of Transportation (for demo purposes only)'
  id: totrans-1150
  prefs: []
  type: TYPE_NORMAL
  zh: 图08-1：美国交通部发布的准时性能数据集提供的元数据（仅用于演示目的）
- en: 'Now, I want you to analyze the delays of `SFO` returning flights and possibly
    find the factors contributing to the delay. Let''s start with the R `data.frame`:'
  id: totrans-1151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我希望你分析`SFO`返程航班的延误情况，并可能找出导致延误的因素。让我们从R的`data.frame`开始：
- en: '[PRE123]'
  id: totrans-1152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: If you were flying from Salt Lake City on Sunday with Alaska Airlines in July
    2015, consider yourself unlucky (we have only done simple analysis so far, so
    one shouldn't attach too much significance to this result). There may be multiple
    other random factors contributing to the delay.
  id: totrans-1153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是在2015年7月周日的阿拉斯加航空从盐湖城起飞，那么你觉得自己很不幸（我们到目前为止只进行了简单的分析，所以人们不应该过分重视这个结果）。可能有多个其他随机因素导致延误。
- en: 'Even though we ran the example in SparkR, we still used the R `data.frame`.
    If we want to analyze data across multiple months, we will need to distribute
    the load across multiple nodes. This is where the SparkR distributed DataFrame
    comes into play, as it can be distributed across multiple threads even on a single
    node. There is a direct way to convert the R DataFrame to SparkR DataFrame (and
    thus to RDD):'
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在SparkR中运行了示例，但我们仍然使用了R的`data.frame`。如果我们想分析跨多个月份的数据，我们需要在多个节点之间分配负载。这就是SparkR分布式DataFrame发挥作用的地方，因为它可以在单个节点上的多个线程之间进行分布。有一个直接的方法可以将R
    DataFrame转换为SparkR DataFrame（以及因此转换为RDD）：
- en: '[PRE124]'
  id: totrans-1155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE124]'
- en: 'If I run it on a laptop, I will run out of memory. The overhead is large due
    to the fact that I need to transfer the data between multiple threads/nodes, we
    want to filter it as soon as possible:'
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我在笔记本电脑上运行它，我会耗尽内存。由于我需要在多个线程/节点之间传输数据，开销很大，我们希望尽可能早地进行过滤：
- en: '[PRE125]'
  id: totrans-1157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE125]'
- en: 'This will run even on my laptop. There is, of course, a reverse conversion
    from Spark''s DataFrame to R''s `data.frame`:'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 这甚至可以在我的笔记本电脑上运行。当然，从Spark的DataFrame到R的`data.frame`也有反向转换：
- en: '[PRE126]'
  id: totrans-1159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE126]'
- en: 'Alternatively, I can use the `spark-csv` package to read it from the `.csv`
    file, which, if the original `.csv` file is in a distributed filesystem such as
    HDFS, will avoid shuffling the data over network in a cluster setting. The only
    drawback, at least currently, is that Spark cannot read from the `.zip` files
    directly:'
  id: totrans-1160
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我可以使用 `spark-csv` 包从 `.csv` 文件中读取它，如果原始的 `.csv` 文件位于HDFS等分布式文件系统中，这将避免在集群设置中通过网络在集群中移动数据。目前唯一的缺点是Spark不能直接从
    `.zip` 文件中读取：
- en: '[PRE127]'
  id: totrans-1161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE127]'
- en: 'Note that we loaded the additional `com.databricks:spark-csv_2.10:1.3.0` package
    by supplying the `--package` flag on the command line; we can easily go distributed
    by using a Spark instance over a cluster of nodes or even analyze a larger dataset:'
  id: totrans-1162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们通过在命令行上提供 `--package` 标志加载了额外的 `com.databricks:spark-csv_2.10:1.3.0` 包；我们可以通过在节点集群上使用Spark实例轻松地进行分布式处理，甚至分析更大的数据集：
- en: '[PRE128]'
  id: totrans-1163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE128]'
- en: 'This will download and put the on-time performance data in the flight''s directory
    (remember, as we discussed in [Chapter 1](part0235.xhtml#aid-703K61 "Chapter 1. Exploratory
    Data Analysis"), *Exploratory Data Analysis*, we would like to treat directories
    as big data datasets). We can now run the same analysis over the whole period
    of 2015 (for the available data):'
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 这将下载并将准点率数据放在航班的目录中（记住，正如我们在[第1章](part0235.xhtml#aid-703K61 "第1章. 探索性数据分析")中讨论的，*探索性数据分析*，我们希望将目录视为大数据数据集）。现在我们可以对2015年（可用数据）的整个时期进行相同的分析：
- en: '[PRE129]'
  id: totrans-1165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE129]'
- en: 'Note that we used a `cache()` call to pin the dataset to the memory as we will
    use it again later. This time it''s Minneapolis/United on Saturday! However, you
    probably already know why: there is only one record for this combination of `DayOfWeek`,
    `Origin`, and `UniqueCarrier`; it''s most likely an outlier. The average over
    about `30` flights for the previous outlier was reduced to `30` minutes:'
  id: totrans-1166
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们使用了一个 `cache()` 调用来将数据集固定在内存中，因为我们稍后会再次使用它。这次是周六的明尼阿波利斯/联合！然而，你可能已经知道原因：对于这种
    `DayOfWeek`、`Origin` 和 `UniqueCarrier` 组合，只有一个记录；这很可能是异常值。之前异常值的平均大约 `30` 个航班现在减少到
    `30` 分钟：
- en: '[PRE130]'
  id: totrans-1167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE130]'
- en: Sunday still remains a problem in terms of delays. The limit to the amount of
    data we can analyze now is only the number of cores on the laptop and nodes in
    the cluster. Let's look at more complex machine learning models now.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 周日仍然是一个延迟问题。我们现在能分析的数据量限制仅是笔记本电脑上的核心数和集群中的节点数。现在让我们看看更复杂的机器学习模型。
- en: Linear models
  id: totrans-1169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 线性模型
- en: 'Linear methods play an important role in statistical modeling. As the name
    suggests, linear model assumes that the dependent variable is a weighted combination
    of independent variables. In R, the `lm` function performs a linear regression
    and reports the coefficients, as follows:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 线性方法在统计建模中扮演着重要角色。正如其名所示，线性模型假设因变量是自变量的加权组合。在R中，`lm` 函数执行线性回归并报告系数，如下所示：
- en: '[PRE131]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE131]'
- en: 'The `summary` function provides even more information:'
  id: totrans-1172
  prefs: []
  type: TYPE_NORMAL
  zh: '`summary` 函数提供了更多信息：'
- en: '[PRE132]'
  id: totrans-1173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE132]'
- en: While we considered generalized linear models in [Chapter 3](part0249.xhtml#aid-7DES21
    "Chapter 3. Working with Spark and MLlib"), *Working with Spark and MLlib*, and
    we will also consider the `glm` implementation in R and SparkR shortly, linear
    models provide more information in general and are an excellent tool for working
    with noisy data and selecting the relevant attribute for further analysis.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在[第3章](part0249.xhtml#aid-7DES21 "第3章. 使用Spark和MLlib")中考虑了广义线性模型，*使用Spark和MLlib*，我们也将很快考虑R和SparkR中的
    `glm` 实现，但线性模型通常提供更多信息，并且是处理噪声数据和选择相关属性进行进一步分析的出色工具。
- en: Note
  id: totrans-1175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Data analysis life cycle**'
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: '**数据分析生命周期**'
- en: While most of the statistical books focus on the analysis and best use of available
    data, the results of statistical analysis in general should also affect the search
    for the new sources of information. In the complete data life cycle, discussed
    at the end of [Chapter 3](part0249.xhtml#aid-7DES21 "Chapter 3. Working with Spark
    and MLlib"), *Working with Spark and MLlib*, a data scientist should always transform
    the latest variable importance results into the theories of how to collect data.
    For example, if the ink usage analysis for home printers points to an increase
    in ink usage for photos, one could potentially collect more information about
    the format of the pictures, sources of digital images, and paper the user prefers
    to use. This approach turned out to be very productive in a real business situation
    even though not fully automated.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然大多数统计书籍都关注分析和最佳使用现有数据，但统计分析的结果通常也应该影响对新信息来源的搜索。在完整的数据生命周期中，如[第3章](part0249.xhtml#aid-7DES21
    "第3章。使用Spark和MLlib")所述，*使用Spark和MLlib*，数据科学家应始终将最新的变量重要性结果转化为如何收集数据的理论。例如，如果家用打印机的墨水使用分析表明照片墨水使用量增加，那么可以收集更多关于图片格式、数字图像来源以及用户偏好的纸张类型的信息。这种方法在实际的商业环境中证明非常有效，尽管它并没有完全自动化。
- en: 'Specifically, here is a short description of the output that linear models
    provide:'
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，以下是线性模型提供的输出结果的简要描述：
- en: '**Residuals**: These are statistics for the difference between the actual and
    predicted values. A lot of techniques exist to detect the problems with the models
    on patterns of the residual distribution, but this is out of scope of this book.
    A detailed residual table can be obtained with the `resid(model)` function.'
  id: totrans-1179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差**：这些是实际值与预测值之间差异的统计数据。存在许多技术可以检测残差分布模式中的模型问题，但这超出了本书的范围。可以通过`resid(model)`函数获得详细的残差表。'
- en: '**Coefficients**: These are the actual linear combination coefficients; the
    t-value represents the ratio of the value of the coefficient to the estimate of
    the standard error: higher values mean a higher likelihood that this coefficient
    has a non-trivial effect on the dependent variable. The coefficients can also
    be obtained with `coef(model)` functions.'
  id: totrans-1180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**系数**：这些是实际的线性组合系数；t值表示系数值与标准误差估计值的比率：数值越高意味着这个系数对因变量的非平凡影响的可能性越高。这些系数也可以通过`coef(model)`函数获得。'
- en: '**Residual standard error**: This reports the standard mean square error, the
    metric that is the target of optimization in a straightforward linear regression.'
  id: totrans-1181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**残差标准误差**：这报告了标准均方误差，这是简单线性回归中优化的目标。'
- en: '**Multiple R-squared**: This is the fraction of the dependent variable variance
    that is explained by the model. The adjusted value accounts for the number of
    parameters in your model and is considered to be a better metric to avoid overfitting
    if the number of observations does not justify the complexity of the models, which
    happens even for big data problems.'
  id: totrans-1182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多重R平方**：这是由模型解释的因变量方差的比例。调整后的值考虑了模型中的参数数量，被认为是避免过度拟合的更好指标，如果观察数量不足以证明模型的复杂性，这种情况甚至在大数据问题中也会发生。'
- en: '**F-statistic**: The measure of model quality. In plain terms, it measures
    how all the parameters in the model explain the dependent variable. The p-value
    provides the probability that the model explains the dependent variable just due
    to random chance. The values under 0.05 (or 5%) are, in general, considered satisfactory.
    While in general, a high value probably means that the model is probably not statistically
    valid and "nothing else matters, the low F-statistic does not always mean that
    the model will work well in practice, so it cannot be directly applied as a model
    acceptance criterion.'
  id: totrans-1183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**F统计量**：模型质量的衡量标准。简单来说，它衡量模型中所有参数解释因变量的程度。p值提供了模型仅因随机机会解释因变量的概率。一般来说，小于0.05（或5%）的值被认为是令人满意的。虽然一般来说，高值可能意味着模型可能没有统计上的有效性，“其他因素都不重要”，但低F统计量并不总是意味着模型在实际中会表现良好，因此不能直接将其作为模型接受标准。'
- en: Once the linear models are applied, usually more complex `glm` or recursive
    models, such as decision trees and the `rpart` function, are applied to find interesting
    variable interactions. Linear models are good for establishing baseline on the
    other models that can improve.
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用了线性模型，通常还会应用更复杂的`glm`或递归模型，如决策树和`rpart`函数，以寻找有趣的变量交互。线性模型对于在可以改进的其他模型上建立基线是很好的。
- en: 'Finally, ANOVA is a standard technique to study the variance if the independent
    variables are discrete:'
  id: totrans-1185
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，方差分析（ANOVA）是研究独立变量离散时的标准技术：
- en: '[PRE133]'
  id: totrans-1186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE133]'
- en: The measure of the model quality is F-statistics. While one can always run R
    algorithms with RDD using the pipe mechanism with `Rscript`, I will partially
    cover this functionality with respect to **Java Specification Request** (**JSR**)
    223 Python integration later. In this section, I would like to explore specifically
    a generalized linear regression `glm` function that is implemented both in R and
    SparkR natively.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 模型质量的衡量标准是F统计量。虽然可以使用`Rscript`通过管道机制运行RDD上的R算法，但我将在稍后部分部分介绍与**Java规范请求**（**JSR**）223
    Python集成相关的此功能。在本节中，我想特别探讨一个在R和SparkR中本地实现的广义线性回归`glm`函数。
- en: Generalized linear model
  id: totrans-1188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 广义线性模型
- en: 'Once again, you can run either R `glm` or SparkR `glm`. The list of possible
    link and optimization functions for R implementation is provided in the following
    table:'
  id: totrans-1189
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，你可以运行R `glm`或SparkR `glm`。以下表格提供了R实现可能的链接和优化函数列表：
- en: 'The following list shows possible options for R `glm` implementation:'
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了R `glm`实现的可能选项：
- en: '| Family | Variance | Link |'
  id: totrans-1191
  prefs: []
  type: TYPE_TB
  zh: '| Family | Variance | Link |'
- en: '| --- | --- | --- |'
  id: totrans-1192
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| gaussian | gaussian | identity |'
  id: totrans-1193
  prefs: []
  type: TYPE_TB
  zh: '| gaussian | gaussian | identity |'
- en: '| binomial | binomial | logit, probit or cloglog |'
  id: totrans-1194
  prefs: []
  type: TYPE_TB
  zh: '| binomial | binomial | logit, probit 或 cloglog |'
- en: '| poisson | poisson | log, identity or sqrt |'
  id: totrans-1195
  prefs: []
  type: TYPE_TB
  zh: '| poisson | poisson | log, identity 或 sqrt |'
- en: '| Gamma | Gamma | inverse, identity or log |'
  id: totrans-1196
  prefs: []
  type: TYPE_TB
  zh: '| Gamma | Gamma | inverse, identity 或 log |'
- en: '| inverse.gaussian | inverse.gaussian | 1/mu^2 |'
  id: totrans-1197
  prefs: []
  type: TYPE_TB
  zh: '| inverse.gaussian | inverse.gaussian | 1/mu^2 |'
- en: '| quasi | user-defined | user-defined |'
  id: totrans-1198
  prefs: []
  type: TYPE_TB
  zh: '| quasi | 用户定义 | 用户定义 |'
- en: 'I will use a binary target, `ArrDel15`, which indicates whether the plane was
    more than 15 minutes late for the arrival. The independent variables will be `DepDel15`,
    `DayOfWeek`, `Month`, `UniqueCarrier`, `Origin`, and `Dest`:'
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用二元目标`ArrDel15`，它表示飞机是否晚于15分钟到达。自变量将是`DepDel15`、`DayOfWeek`、`Month`、`UniqueCarrier`、`Origin`和`Dest`：
- en: '[PRE134]'
  id: totrans-1200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE134]'
- en: 'While you wait for the results, open another shell and run `glm` in the `SparkR`
    mode on the full seven months of data:'
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 当你等待结果时，打开另一个shell并在SparkR模式下对完整七个月的数据运行`glm`：
- en: '[PRE135]'
  id: totrans-1202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE135]'
- en: Here we try to build a model explaining delays as an effect of carrier, day
    of week, and origin on destination airports, which is captured by the formular
    construct `ArrDel15 ~ UniqueCarrier + DayOfWeek + Origin + Dest`.
  id: totrans-1203
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们试图构建一个模型，解释延误作为承运人、星期几和出发机场对目的地机场的影响，这由公式构造`ArrDel15 ~ UniqueCarrier +
    DayOfWeek + Origin + Dest`来捕捉。
- en: Note
  id: totrans-1204
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Nulls, big data, and Scala**'
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: '**nulls，大数据和Scala**'
- en: Note that in the SparkR case of `glm`, I had to explicitly filter out the non-cancelled
    flights and removed the NA—or nulls in the C/Java lingo. While R does this for
    you by default, NAs in big data are very common as the datasets are typically
    sparse and shouldn't be treated lightly. The fact that we have to deal with nulls
    explicitly in MLlib warns us about some additional information in the dataset
    and is definitely a welcome feature. The presence of an NA can carry information
    about the way the data was collected. Ideally, each NA should be accompanied by
    a small `get_na_info` method as to why this particular value was not available
    or collected, which leads us to the `Either` type in Scala.
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在SparkR的`glm`情况下，我必须明确过滤掉未取消的航班并移除NA——或者用C/Java术语来说，是nulls。虽然R默认会为你做这件事，但大数据中的NA非常常见，因为数据集通常是稀疏的，不应轻视。我们必须在MLlib中明确处理nulls的事实提醒我们数据集中有额外的信息，这绝对是一个受欢迎的功能。NA的存在可以携带有关数据收集方式的信息。理想情况下，每个NA都应该有一个小的`get_na_info`方法来解释为什么这个特定的值不可用或未收集，这使我们想到了Scala中的`Either`类型。
- en: Even though nulls are inherited from Java and a part of Scala, the `Option`
    and `Either` types are new and more robust mechanism to deal with special cases
    where nulls were traditionally used. Specifically, `Either` can provide a value
    or exception message as to why it was not computed; while `Option` can either
    provide a value or be `None`, which can be readily captured by the Scala pattern-matching
    framework.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管空值是从 Java 继承的，也是 Scala 的一部分，但 `Option` 和 `Either` 类型是新的且更健壮的机制，用于处理传统上使用空值的情况。具体来说，`Either`
    可以提供一个值或异常信息，说明为什么没有计算；而 `Option` 可以提供一个值或为 `None`，这可以很容易地被 Scala 的模式匹配框架捕获。
- en: 'One thing you will notice is that SparkR will run multiple threads, and even
    on a single node, it will consume CPU time from multiple cores and returns much
    faster even with a larger size of data. In my experiment on a 32-core machine,
    it was able to finish in under a minute (as opposed to 35 minutes for R `glm`).
    To get the results, as in the R model case, we need to run the `summary()` method:'
  id: totrans-1208
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到 SparkR 会运行多个线程，甚至在单个节点上，它也会消耗多个核心的 CPU 时间，并且即使数据量较大，返回速度也很快。在我的 32 核机器上的实验中，它能在不到一分钟内完成（相比之下，R
    `glm` 需要 35 分钟）。要获取结果，就像 R 模型的情况一样，我们需要运行 `summary()` 方法：
- en: '[PRE136]'
  id: totrans-1209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE136]'
- en: 'The worst performer is `NK` (Spirit Airlines). Internally, SparkR uses limited-memory
    BFGS, which is a limited-memory quasi-Newton optimization method that is similar
    to the results obtained with R `glm` on the July data:'
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 表现最差的航空公司是 `NK`（精神航空公司）。SparkR 内部使用的是有限内存的 BFGS，这是一种类似于 R `glm` 在 7 月数据上获得结果的有限内存拟牛顿优化方法：
- en: '[PRE137]'
  id: totrans-1211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE137]'
- en: 'Other parameters of SparkR `glm` implementation are provided in the following
    table:'
  id: totrans-1212
  prefs: []
  type: TYPE_NORMAL
  zh: SparkR `glm` 实现的其他参数如下表所示：
- en: 'The following table shows a list of parameters for SparkR `glm` implementation:'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格显示了 SparkR `glm` 实现的参数列表：
- en: '| Parameter | Possible Values | Comments |'
  id: totrans-1214
  prefs: []
  type: TYPE_TB
  zh: '| 参数 | 可能的值 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `formula` | A symbolic description like in R | Currently only a subset of
    formula operators are supported: ''`~`'', ''`.`'', ''`:`'', ''`+`'', and ''`-`''
    |'
  id: totrans-1216
  prefs: []
  type: TYPE_TB
  zh: '| `formula` | R 中的符号描述 | 目前仅支持公式运算符的子集："`~`", "`.`", "`:`", "`+`", 和 "`-`"
    |'
- en: '| `family` | gaussian or binomial | Needs to be in quotes: gaussian -> linear
    regression, binomial -> logistic regression |'
  id: totrans-1217
  prefs: []
  type: TYPE_TB
  zh: '| `family` | gaussian or binomial | 需要加引号：gaussian -> 线性回归，binomial -> 逻辑回归
    |'
- en: '| `data` | DataFrame | Needs to be SparkR DataFrame, not `data.frame` |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
  zh: '| `data` | DataFrame | 需要是 SparkR DataFrame，而不是 `data.frame` |'
- en: '| `lambda` | positive | Regularization coefficient |'
  id: totrans-1219
  prefs: []
  type: TYPE_TB
  zh: '| `lambda` | positive | 正则化系数 |'
- en: '| `alpha` | positive | Elastic-net mixing parameter (refer to glmnet''s documentation
    for details) |'
  id: totrans-1220
  prefs: []
  type: TYPE_TB
  zh: '| `alpha` | positive | 弹性网络混合参数（详细信息请参阅 glmnet 的文档） |'
- en: '| `standardize` | TRUE or FALSE | User-defined |'
  id: totrans-1221
  prefs: []
  type: TYPE_TB
  zh: '| `standardize` | TRUE or FALSE | 用户定义 |'
- en: '| `solver` | l-bfgs, normal or auto | auto will choose the algorithm automatically,
    l-bfgs means limited-memory BFGS, normal means using normal equation as an analytical
    solution to the linear regression problem |'
  id: totrans-1222
  prefs: []
  type: TYPE_TB
  zh: '| `solver` | l-bfgs, normal or auto | auto 会自动选择算法，l-bfgs 表示有限内存 BFGS，normal
    表示使用正规方程作为线性回归问题的解析解 |'
- en: Reading JSON files in SparkR
  id: totrans-1223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SparkR 中读取 JSON 文件
- en: 'Schema on Read is one of the convenient features of big data. The DataFrame
    class has the ability to figure out the schema of a text file containing a JSON
    record per line:'
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 读取文本文件中的 JSON 记录的 Schema on Read 是大数据的一个便利特性。DataFrame 类能够确定每行包含一个 JSON 记录的文本文件的架构：
- en: '[PRE138]'
  id: totrans-1225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE138]'
- en: Writing Parquet files in SparkR
  id: totrans-1226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在 SparkR 中写入 Parquet 文件
- en: 'As we mentioned in the previous chapter, the Parquet format is an efficient
    storage format, particularly for low cardinality columns. Parquet files can be
    read/written directly from R:'
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一章中提到的，Parquet 格式是一种高效的存储格式，尤其是对于低基数列。Parquet 文件可以直接从 R 中读取/写入：
- en: '[PRE139]'
  id: totrans-1228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE139]'
- en: 'You can see that the new Parquet file is 66 times smaller that the original
    zip file downloaded from the DoT:'
  id: totrans-1229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，新的 Parquet 文件比从 DoT 下载的原始 zip 文件小 66 倍：
- en: '[PRE140]'
  id: totrans-1230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE140]'
- en: Invoking Scala from R
  id: totrans-1231
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 R 调用 Scala
- en: Let's assume that one has an exceptional implementation of a numeric method
    in Scala that we want to call from R. One way of doing this would be to use the
    R `system()` function that invokes `/bin/sh` on Unix-like systems. However, the
    `rscala` package is a more efficient way that starts a Scala interpreter and maintains
    communication over TCP/IP network connection.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个在 Scala 中实现的数值方法的异常实现，我们想从 R 中调用它。一种方法是通过 R 的 `system()` 函数调用 Unix-like
    系统上的 `/bin/sh`。然而，`rscala` 包是一个更有效的方法，它启动 Scala 解释器并维护 TCP/IP 网络连接的通信。
- en: 'Here, the Scala interpreter maintains the state (memoization) between the calls.
    Similarly, one can define functions, as follows:'
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，Scala解释器在调用之间维护状态（记忆化）。同样，可以定义函数，如下所示：
- en: '[PRE141]'
  id: totrans-1234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE141]'
- en: 'R from Scala can be invoked using the `!` or `!!` Scala operators and `Rscript`
    command:'
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: Scala可以使用`!`或`!!`操作符和`Rscript`命令调用R：
- en: '[PRE142]'
  id: totrans-1236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE142]'
- en: Using Rserve
  id: totrans-1237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Rserve
- en: 'A more efficient way is to use the similar TCP/IP binary transport protocol
    to communicate with R with `Rsclient/Rserve` ([http://www.rforge.net/Rserve](http://www.rforge.net/Rserve)).
    To start `Rserve` on a node that has R installed, perform the following action:'
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更有效的方法是使用类似的TCP/IP二进制传输协议，通过`Rsclient/Rserve`与R通信（[http://www.rforge.net/Rserve](http://www.rforge.net/Rserve)）。要在已安装R的节点上启动`Rserve`，执行以下操作：
- en: '[PRE143]'
  id: totrans-1239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE143]'
- en: By default, `Rserv` opens a connection on `localhost:6311`. The advantage of
    the binary network protocol is that it is platform-independent and multiple clients
    can communicate with the server. The clients can connect to `Rserve`.
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`Rserv`在`localhost:6311`上打开一个连接。二进制网络协议的优点是它具有平台无关性，多个客户端可以与服务器通信。客户端可以连接到`Rserve`。
- en: Note that, while passing the results as a binary object has its advantages,
    you have to be careful with the type mappings between R and Scala. `Rserve` supports
    other clients, including Python, but I will also cover JSR 223-compliant scripting
    at the end of this chapter.
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，虽然将结果作为二进制对象传递有其优点，但你必须小心R和Scala之间的类型映射。`Rserve`支持其他客户端，包括Python，但我还会在本章末尾介绍JSR
    223兼容的脚本。
- en: Integrating with Python
  id: totrans-1242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 与Python集成
- en: Python has slowly established ground as a de-facto tool for data science. It
    has a command-line interface and decent visualization via matplotlib and ggplot,
    which is based on R's ggplot2\. Recently, Wes McKinney, the creator of Pandas,
    the time series data-analysis package, has joined Cloudera to pave way for Python
    in big data.
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: Python逐渐成为数据科学事实上的工具。它有一个命令行界面，以及通过matplotlib和ggplot（基于R的ggplot2）实现的不错的可视化。最近，Pandas时间序列数据分析包的创造者Wes
    McKinney加入了Cloudera，为Python在大数据领域铺平道路。
- en: Setting up Python
  id: totrans-1244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置Python
- en: Python is usually part of the default installation. Spark requires version 2.7.0+.
  id: totrans-1245
  prefs: []
  type: TYPE_NORMAL
  zh: Python通常是默认安装的一部分。Spark需要版本2.7.0+。
- en: 'If you don''t have Python on Mac OS, I recommend installing the Homebrew package
    manager from [http://brew.sh](http://brew.sh):'
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有在Mac OS上安装Python，我建议安装Homebrew包管理器，请访问[http://brew.sh](http://brew.sh)：
- en: '[PRE144]'
  id: totrans-1247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE144]'
- en: 'Otherwise, on a Unix-like system, Python can be compiled from the source distribution:'
  id: totrans-1248
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，在类Unix系统上，可以从源分发版编译Python：
- en: '[PRE145]'
  id: totrans-1249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE145]'
- en: 'It is good practice to place it in a directory different from the default Python
    installation. It is normal to have multiple versions of Python on a single system,
    which usually does not lead to problems as Python separates the installation directories.
    For the purpose of this chapter, as for many machine learning takes, I''ll also
    need a few packages. The packages and specific versions may differ across installations:'
  id: totrans-1250
  prefs: []
  type: TYPE_NORMAL
  zh: 将其放置在不同于默认Python安装的目录中是一种良好的实践。在单个系统上拥有多个Python版本是正常的，通常不会导致问题，因为Python会分离安装目录。为了本章的目的，就像许多机器学习应用一样，我还需要一些包。这些包和具体版本可能因安装而异：
- en: '[PRE146]'
  id: totrans-1251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE146]'
- en: If everything compiles—SciPy uses a Fortran compiler and libraries for linear
    algebra—we are ready to use Python 2.7.11!
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一切编译成功——SciPy使用Fortran编译器和线性代数库——我们就准备好使用Python 2.7.11了！
- en: Note
  id: totrans-1253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: Note that if one wants to use Python with the `pipe` command in a distributed
    environment, Python needs to be installed on every node in the network.
  id: totrans-1254
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果想在分布式环境中使用`pipe`命令与Python一起使用，Python需要安装在网络中的每个节点上。
- en: PySpark
  id: totrans-1255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PySpark
- en: 'As `bin/sparkR` launches R with preloaded Spark context, `bin/pyspark` launches
    Python shell with preloaded Spark context and Spark driver running. The `PYSPARK_PYTHON`
    environment variable can be used to point to a specific Python version:'
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`bin/sparkR`使用预加载的Spark上下文启动R，`bin/pyspark`使用预加载的Spark上下文和Spark驱动程序启动Python
    shell。可以使用`PYSPARK_PYTHON`环境变量指向特定的Python版本：
- en: '[PRE147]'
  id: totrans-1257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE147]'
- en: 'PySpark directly supports most of MLlib functionality on Spark RDDs ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)),
    but it is known to lag a few releases behind the Scala API ([http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)).
    As of the 1.6.0+ release, it also supports DataFrames ([http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)):'
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 直接支持 Spark RDDs 上的大多数 MLlib 功能（[http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)），但它已知落后
    Scala API 几个版本（[http://spark.apache.org/docs/latest/api/python](http://spark.apache.org/docs/latest/api/python)）。截至
    1.6.0+ 版本，它还支持 DataFrames（[http://spark.apache.org/docs/latest/sql-programming-guide.html](http://spark.apache.org/docs/latest/sql-programming-guide.html)）：
- en: '[PRE148]'
  id: totrans-1259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE148]'
- en: Calling Python from Java/Scala
  id: totrans-1260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 Java/Scala 调用 Python
- en: As this is really a book about Scala, we should also mention that one can call
    Python code and its interpreter directly from Scala (or Java). There are a few
    options available that will be discussed in this chapter.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这本书实际上是关于 Scala 的，我们也应该提到，可以直接从 Scala（或 Java）调用 Python 代码及其解释器。本章将讨论一些可用的选项。
- en: Using sys.process._
  id: totrans-1262
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用 sys.process._
- en: 'Scala, as well as Java, can call OS processes via spawning a separate thread,
    which we already used for interactive analysis in [Chapter 1](part0235.xhtml#aid-703K61
    "Chapter 1. Exploratory Data Analysis"), *Exploratory Data Analysis*: the `.!`
    method will start the process and return the exit code, while `.!!` will return
    the string that contains the output:'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: Scala，以及 Java，可以通过启动一个单独的线程来调用操作系统进程，这我们在第 1 章[探索性数据分析](part0235.xhtml#aid-703K61
    "第 1 章. 探索性数据分析")中已经使用过，`.!` 方法将启动进程并返回退出代码，而 `.!!` 将返回包含输出的字符串：
- en: '[PRE149]'
  id: totrans-1264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE149]'
- en: 'Let''s try a more complex SVD computation (similar to the one we used in SVD++
    recommendation engine, but this time, it invokes BLAS C-libraries at the backend).
    I created a Python executable that takes a string representing a matrix and the
    required rank as an input and outputs an SVD approximation with the provided rank:'
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个更复杂的 SVD 计算（类似于我们在 SVD++ 推荐引擎中使用的，但这次，它在后端调用 BLAS C 库）。我创建了一个 Python
    可执行文件，它接受一个表示矩阵的字符串和所需的秩作为输入，并输出具有提供秩的 SVD 近似：
- en: '[PRE150]'
  id: totrans-1266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE150]'
- en: 'Let''s call it `svd.py` and put in in the current directory. Given a matrix
    and rank as an input, it produces an approximation of a given rank:'
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其命名为 `svd.py` 并将其放在当前目录中。给定一个矩阵和秩作为输入，它会产生给定秩的近似：
- en: '[PRE151]'
  id: totrans-1268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE151]'
- en: 'To call it from Scala, let''s define the following `#<<<` method in our DSL:'
  id: totrans-1269
  prefs: []
  type: TYPE_NORMAL
  zh: 要从 Scala 中调用它，让我们在我们的 DSL 中定义以下 `#<<<` 方法：
- en: '[PRE152]'
  id: totrans-1270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE152]'
- en: 'Now, we can use the `#<<<` operator to call Python''s SVD method:'
  id: totrans-1271
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `#<<<` 操作符来调用 Python 的 SVD 方法：
- en: '[PRE153]'
  id: totrans-1272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE153]'
- en: 'Note that as we requested the resulting matrix rank to be one, all rows and
    columns are linearly dependent. We can even pass several lines of input at a time,
    as follows:'
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于我们要求结果的矩阵秩为 1，所有行和列都是线性相关的。我们甚至可以一次传递多行输入，如下所示：
- en: '[PRE154]'
  id: totrans-1274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE154]'
- en: Spark pipe
  id: totrans-1275
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Spark 管道
- en: 'SVD decomposition is usually a pretty heavy operation, so the relative overhead
    of calling Python in this case is small. We can avoid this overhead if we keep
    the process running and supply several lines at a time, like we did in the last
    example. Both Hadoop MR and Spark implement this approach. For example, in Spark,
    the whole computation will take only one line, as shown in the following:'
  id: totrans-1276
  prefs: []
  type: TYPE_NORMAL
  zh: SVD 分解通常是一个相当耗时的操作，因此在这种情况下调用 Python 的相对开销很小。如果我们保持进程运行并一次提供多行，就像我们在上一个例子中所做的那样，我们可以避免这种开销。Hadoop
    MR 和 Spark 都实现了这种方法。例如，在 Spark 中，整个计算只需一行，如下所示：
- en: '[PRE155]'
  id: totrans-1277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE155]'
- en: The whole pipeline is ready to be distributed across a cluster of multicore
    workstations! I think you will be in love with Scala/Spark already.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: 整个管道已准备好在多核工作站集群中分发！我想你已经爱上 Scala/Spark 了。
- en: Note that debugging the pipelined executions might be tricky as the data is
    passed from one process to another using OS pipes.
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，调试管道化执行可能很棘手，因为数据是通过操作系统管道从一个进程传递到另一个进程的。
- en: Jython and JSR 223
  id: totrans-1280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Jython 和 JSR 223
- en: For completeness, we need to mention Jython, a Java implementation of Python
    (as opposed to a more familiar C implementation, also called CPython). Jython
    avoids the problem of passing input/output via OS pipelines by allowing the users
    to compile Python source code to Java byte codes, and running the resulting bytecodes
    on any Java virtual machine. As Scala also runs in Java virtual machine, it can
    use the Jython classes directly, although the reverse is not true in general;
    Scala classes sometimes are not compatible to be used by Java/Jython.
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完整性，我们需要提及 Jython，这是 Python 的 Java 实现（与更熟悉的 C 实现不同，也称为 CPython）。Jython 通过允许用户将
    Python 源代码编译成 Java 字节码，并在任何 Java 虚拟机上运行这些字节码，避免了通过操作系统管道传递输入/输出的问题。由于 Scala 也在
    Java 虚拟机上运行，它可以直接使用 Jython 类，尽管通常情况并非如此；Scala 类有时与 Java/Jython 不兼容。
- en: Note
  id: totrans-1282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**JSR 223**'
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: '**JSR 223**'
- en: In this particular case, the request is for "Scripting for the JavaTM Platform"
    and was originally filed on Nov 15th 2004 ([https://www.jcp.org/en/jsr/detail?id=223](https://www.jcp.org/en/jsr/detail?id=223)).
    At the beginning, it was targeted towards the ability of the Java servlet to work
    with multiple scripting languages. The specification requires the scripting language
    maintainers to provide a Java JAR with corresponding implementations. Portability
    issues hindered practical implementations, particularly when platforms require
    complex interaction with OS, such as dynamic linking in C or Fortran. Currently,
    only a handful languages are supported, with R and Python being supported, but
    in incomplete form.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定案例中，请求的是“JavaTM 平台的脚本编程”功能，最初于 2004 年 11 月 15 日提出 ([https://www.jcp.org/en/jsr/detail?id=223](https://www.jcp.org/en/jsr/detail?id=223))。最初，它针对的是
    Java servlet 与多种脚本语言协同工作的能力。规范要求脚本语言维护者提供包含相应实现的 Java JAR。可移植性问题阻碍了实际的应用，尤其是在需要与操作系统进行复杂交互的平台，如
    C 或 Fortran 中的动态链接。目前，只有少数语言受到支持，R 和 Python 被支持，但形式并不完整。
- en: 'Since Java 6, JSR 223: Scripting for Java added the `javax.script` package
    that allows multiple scripting languages to be called through the same API as
    long as the language provides a script engine. To add the Jython scripting language,
    download the latest Jython JAR from the Jython site at [http://www.jython.org/downloads.html](http://www.jython.org/downloads.html):'
  id: totrans-1285
  prefs: []
  type: TYPE_NORMAL
  zh: 自 Java 6 以来，JSR 223：Java 的脚本编程添加了 `javax.script` 包，它允许通过相同的 API 调用多种脚本语言，只要该语言提供脚本引擎。要添加
    Jython 脚本语言，请从 Jython 网站下载最新的 Jython JAR 文件，网址为 [http://www.jython.org/downloads.html](http://www.jython.org/downloads.html)：
- en: '[PRE156]'
  id: totrans-1286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE156]'
- en: 'Now, I can use the Jython/Python scripting engine:'
  id: totrans-1287
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我可以使用 Jython/Python 脚本引擎：
- en: '[PRE157]'
  id: totrans-1288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE157]'
- en: It is worth giving a disclaimer here that not all Python modules are available
    in Jython. Modules that require a C/Fortran dynamic linkage for the library that
    doesn't exist in Java are not likely to work in Jython. Specifically, NumPy and
    SciPy are not supported in Jython as they rely on C/Fortran. If you discover some
    other missing modules, you can try copying the `.py` file from a Python distribution
    to a `sys.path` Jython directory—if this works, consider yourself lucky.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里值得声明的是，并非所有 Python 模块都在 Jython 中可用。需要 C/Fortran 动态链接库的模块，而这些库在 Java 中不存在，在
    Jython 中可能无法工作。具体来说，NumPy 和 SciPy 在 Jython 中不受支持，因为它们依赖于 C/Fortran。如果你发现其他缺失的模块，可以尝试将
    Python 发行版中的 `.py` 文件复制到 Jython 的 `sys.path` 目录中——如果这样做有效，那么你很幸运。
- en: 'Jython has the advantage of accessing Python-rich modules without the necessity
    of starting the Python runtime on each call, which might result in a significant
    performance saving:'
  id: totrans-1290
  prefs: []
  type: TYPE_NORMAL
  zh: Jython 的优点是无需在每个调用时启动 Python 运行时即可访问丰富的 Python 模块，这可能会带来显著的性能提升：
- en: '[PRE158]'
  id: totrans-1291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE158]'
- en: Jython JSR 223 call is 10 times faster!
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: Jython JSR 223 调用速度快 10 倍！
- en: Summary
  id: totrans-1293
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: R and Python are like bread and butter for a data scientist. Modern frameworks
    tend to be interoperable and borrow from each other's strength. In this chapter,
    I went over the plumbing of interoperability with R and Python. Both of them have
    packages (R) and modules (Python) that became very popular and extend the current
    Scala/Spark functionality. Many consider R and Python existing libraries to be
    crucial for their implementations.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: R 和 Python 对于数据科学家来说就像面包和黄油一样。现代框架往往具有互操作性，并互相借鉴彼此的优势。在本章中，我介绍了 R 和 Python 之间互操作性的底层结构。它们都有流行的包（R）和模块（Python），这些扩展了当前的
    Scala/Spark 功能。许多人认为 R 和 Python 的现有库对于它们的实现至关重要。
- en: This chapter demonstrated a few ways to integrate these packages and provide
    the tradeoffs of using these integrations so that we can proceed on to the next
    chapter, looking at the NLP, where functional programming has been traditionally
    used from the start.
  id: totrans-1295
  prefs: []
  type: TYPE_NORMAL
  zh: 本章演示了几种集成这些包的方法，并提供了使用这些集成所带来的权衡，以便我们能够进入下一章，探讨NLP，在NLP中，函数式编程从一开始就被传统地使用。
- en: Chapter 9. NLP in Scala
  id: totrans-1296
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第9章：Scala中的NLP
- en: This chapter describes a few common techniques of **Natural Language Processing**
    (**NLP**), specifically, the ones that can benefit from Scala. There are some
    NLP packages in the open source out there. The most famous of them is probably
    NLTK ([http://www.nltk.org](http://www.nltk.org)), which is written in Python,
    and ostensibly even a larger number of proprietary software solutions emphasizing
    different aspects of NLP. It is worth mentioning Wolf ([https://github.com/wolfe-pack](https://github.com/wolfe-pack)),
    FACTORIE ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)), and ScalaNLP
    ([http://www.scalanlp.org](http://www.scalanlp.org)), and skymind ([http://www.skymind.io](http://www.skymind.io)),
    which is partly proprietary. However, few open source projects in this area remain
    active for a long period of time for one or another reason. Most projects are
    being eclipsed by Spark and MLlib capabilities, particularly, in the scalability
    aspect.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了几种常见的**自然语言处理**（**NLP**）技术，特别是那些可以从Scala中受益的技术。开源领域中有一些NLP包。其中最著名的是NLTK
    ([http://www.nltk.org](http://www.nltk.org))，它是用Python编写的，并且可能还有更多强调NLP不同方面的专有软件解决方案。值得提及的是Wolf
    ([https://github.com/wolfe-pack](https://github.com/wolfe-pack))、FACTORIE ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu))、ScalaNLP
    ([http://www.scalanlp.org](http://www.scalanlp.org))和skymind ([http://www.skymind.io](http://www.skymind.io))，其中skymind部分是专有的。然而，由于一个或多个原因，这个领域的许多开源项目在一段时间内都保持活跃。大多数项目正被Spark和MLlib的能力所取代，尤其是在可扩展性方面。
- en: Instead of giving a detailed description of each of the NLP projects, which
    also might include speech-to-text, text-to-speech, and language translators, I
    will provide a few basic techniques focused on leveraging Spark MLlib in this
    chapter. The chapter comes very naturally as the last analytics chapter in this
    book. Scala is a very natural-language looking computer language and this chapter
    will leverage the techniques I developed earlier.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会详细描述每个NLP项目，这些项目可能包括语音转文本、文本转语音和语言翻译，而是将在本章提供一些基本技术，专注于利用Spark MLlib。这一章作为本书的最后一个分析章节，显得非常自然。Scala是一种看起来非常自然语言的计算机语言，本章将利用我之前开发的技术。
- en: NLP arguably is the core of AI. Originally, the AI was created to mimic the
    humans, and natural language parsing and understanding is an indispensable part
    of it. Big data techniques has started to penetrate NLP, even though traditionally,
    NLP is very computationally intensive and is regarded as a small data problem.
    NLP often requires extensive deep learning techniques, and the volume of data
    of all written texts appears to be not so large compared to the logs volumes generated
    by all the machines today and analyzed by the big data machinery.
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: NLP可以说是AI的核心。最初，AI被创造出来是为了模仿人类，而自然语言解析和理解是其不可或缺的一部分。大数据技术已经开始渗透NLP，尽管传统上NLP非常计算密集，被视为小数据问题。NLP通常需要广泛的深度学习技术，而所有书面文本的数据量似乎与今天所有机器生成的日志量以及大数据机器分析的数据量相比并不大。
- en: Even though the Library of Congress counts millions of documents, most of them
    can be digitized in PBs of actual digital data, a volume that any social websites
    is able to collect, store, and analyze within a few seconds. Complete works of
    most prolific authors can be stored within a few MBs of files (refer to *Table
    09-1*). Nonetheless, the social network and ADTECH companies parse text from millions
    of users and in hundreds of contexts every day.
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管国会图书馆拥有数百万份文件，但其中大部分可以以PB（实际数字数据量）为单位进行数字化，这是一个任何社交网站都能在几秒钟内收集、存储和分析的量级。大多数多产作者的完整作品可以存储在几MB的文件中（参考
    *表09-1*）。然而，社交网络和ADTECH公司每天都会从数百万用户和数百个上下文中解析文本。
- en: '| The complete works of | When lived | Size |'
  id: totrans-1301
  prefs: []
  type: TYPE_TB
  zh: '| 完整作品 | 生活时期 | 大小 |'
- en: '| --- | --- | --- |'
  id: totrans-1302
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| *Plato* | 428/427 (or 424/423) - 348/347 BC | 2.1 MB |'
  id: totrans-1303
  prefs: []
  type: TYPE_TB
  zh: '| *柏拉图* | 428/427 (或 424/423) - 348/347 BC | 2.1 MB |'
- en: '| *William Shakespeare* | 26 April 1564 (baptized) - 23 April 1616 | 3.8 MB
    |'
  id: totrans-1304
  prefs: []
  type: TYPE_TB
  zh: '| *威廉·莎士比亚* | 1564年4月26日（洗礼）- 1616年4月23日 | 3.8 MB |'
- en: '| *Fyodor Dostoevsky* | 11 November 1821 - 9 February 1881 | 5.9 MB |'
  id: totrans-1305
  prefs: []
  type: TYPE_TB
  zh: '| *费奥多尔·陀思妥耶夫斯基* | 1821年11月11日 - 1881年2月9日 | 5.9 MB |'
- en: '| *Leo Tolstoy* | 9 September 1828 - 20 November 1910 | 6.9 MB |'
  id: totrans-1306
  prefs: []
  type: TYPE_TB
  zh: '| *列夫·托尔斯泰* | 1828年9月9日 - 1910年11月20日 | 6.9 MB |'
- en: '| *Mark Twain* | November 30, 1835 - April 21, 1910 | 13 MB |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '| *马克·吐温* | 1835年11月30日 - 1910年4月21日 | 13 MB |'
- en: Table 09-1\. Complete Works collections of some famous writers (most can be
    acquired on Amazon.com today for a few dollars, later authors, although readily
    digitized, are more expensive)
  id: totrans-1308
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表09-1\. 一些著名作家的全集（大多数现在可以在Amazon.com上以几美元的价格购买，后来的作者，尽管已经数字化，但价格更高）
- en: The natural language is a dynamic concept that changes over time, technology,
    and generations. We saw the appearance of emoticons, three-letter abbreviations,
    and so on. Foreign languages tend to borrow from each other; describing this dynamic
    ecosystem is a challenge on itself.
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言是一个动态的概念，随着时间的推移、技术和几代人的变化而变化。我们看到了表情符号、三字母缩写等的出现。外语往往相互借鉴；描述这个动态生态系统本身就是一项挑战。
- en: As in the previous chapters, I will focus on how to use Scala as a tool to orchestrate
    the language analysis rather than rewriting the tools in Scala. As the topic is
    so large, I will not claim to cover all aspects of NLP here.
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，我将专注于如何使用Scala作为工具来编排语言分析，而不是在Scala中重写工具。由于这个主题非常广泛，我无法声称在这里涵盖NLP的所有方面。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Discussing NLP with the example of text processing pipeline and stages
  id: totrans-1312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以文本处理流程和阶段为例，讨论自然语言处理（NLP）
- en: Learning techniques for simple text analysis in terms of bags
  id: totrans-1313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从词袋的角度学习简单的文本分析方法
- en: Learning about **Term Frequency Inverse Document Frequency** (**TF-IDF**) technique
    that goes beyond simple bag analysis and de facto the standard in **Information
    Retrieval** (**IR**)
  id: totrans-1314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解**词频逆文档频率**（**TF-IDF**）技术，它超越了简单的词袋分析，并且在**信息检索**（**IR**）中实际上是标准技术
- en: Learning about document clustering with the example of the **Latent Dirichlet
    Allocation** (**LDA**) approach
  id: totrans-1315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以**潜在狄利克雷分配**（**LDA**）方法为例，了解文档聚类
- en: Performing semantic analysis using word2vec n-gram-based algorithms
  id: totrans-1316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用基于word2vec n-gram算法进行语义分析
- en: Text analysis pipeline
  id: totrans-1317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 文本分析流程
- en: Before we proceed to detailed algorithms, let's look at a generic text-processing
    pipeline depicted in *Figure 9-1*. In text analysis, the input is usually presented
    as a stream of characters (depending on the specific language).
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续详细算法之前，让我们看看*图9-1*中描述的通用文本处理流程。在文本分析中，输入通常以字符流的形式呈现（具体取决于特定的语言）。
- en: 'Lexical analysis has to do with breaking this stream into a sequence of words
    (or lexemes in linguistic analysis). Often it is also called tokenization (and
    the words called the tokens). **ANother Tool for Language Recognition** (**ANTLR**)
    ([http://www.antlr.org/](http://www.antlr.org/)) and Flex ([http://flex.sourceforge.net](http://flex.sourceforge.net))
    are probably the most famous in the open source community. One of the classical
    examples of ambiguity is lexical ambiguity. For example, in the phrase *I saw
    a bat.* *bat* can mean either an animal or a baseball bat. We usually need context
    to figure this out, which we will discuss next:'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 词汇分析涉及将这个流分解成一系列单词（或语言学分析中的词素）。通常它也被称为分词（而单词被称为标记）。**ANother Tool for Language
    Recognition**（**ANTLR**）([http://www.antlr.org/](http://www.antlr.org/))和Flex
    ([http://flex.sourceforge.net](http://flex.sourceforge.net))可能是开源社区中最著名的。词汇歧义的一个经典例子是词汇歧义。例如，在短语*I
    saw a bat.*中，*bat*可以指动物或棒球棒。我们通常需要上下文来弄清楚这一点，我们将在下一节讨论：
- en: '![Text analysis pipeline](img/image01755.jpeg)'
  id: totrans-1320
  prefs: []
  type: TYPE_IMG
  zh: '![文本分析流程](img/image01755.jpeg)'
- en: Figure 9-1\. Typical stages of an NLP process.
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-1\. NLP过程的典型阶段。
- en: Syntactic analysis, or parsing, traditionally deals with matching the structure
    of the text with grammar rules. This is relatively more important for computer
    languages that do not allow any ambiguity. In natural languages, this process
    is usually called chunking and tagging. In many cases, the meaning of the word
    in human language can be subject to context, intonation, or even body language
    or facial expression. The value of such analysis, as opposed to the big data approach,
    where the volume of data trumps complexity is still a contentious topic—one example
    of the latter is the word2vec approach, which will be described later.
  id: totrans-1322
  prefs: []
  type: TYPE_NORMAL
  zh: 句法分析，或称为解析，传统上处理的是将文本结构与语法规则相匹配。这对于不允许任何歧义的计算机语言来说相对更重要。在自然语言中，这个过程通常被称为分块和标记。在许多情况下，人类语言中单词的意义可能受语境、语调，甚至肢体语言或面部表情的影响。与大数据方法相比，大数据方法中数据的量胜过复杂性，这种分析的价值仍然是一个有争议的话题——后者之一是
    word2vec 方法，稍后将进行描述。
- en: 'Semantic analysis is the process of extracting language-independent meaning
    from the syntactic structures. As much as possible, it also involves removing
    features specific to particular cultural and linguistic contexts, to the extent
    that such a project is possible. The sources of ambiguity at this stage are: phrase
    attachment, conjunction, noun group structure, semantic ambiguity, anaphoric non-literal
    speech, and so on. Again, word2vec partially deals with these issues.'
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 语义分析是从句法结构中提取语言无关意义的过程。在尽可能的范围内，它还涉及去除特定于特定文化和语言背景的特征，到这种项目可能实现的程度。这一阶段的歧义来源包括：短语附着、连词、名词组结构、语义歧义、指代非字面言语等。再次强调，word2vec
    部分解决了这些问题。
- en: 'Disclosure integration partially deals with the issue of the context: the meaning
    of a sentence or an idiom can depend on the sentences or paragraphs before that.
    Syntactic analysis and cultural background play an important role here.'
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示整合部分解决了上下文的问题：一个句子或成语的意义可能取决于之前的句子或段落。句法分析和文化背景在这里起着重要作用。
- en: Finally, pragmatic analysis is yet another layer of complexity trying to reinterpret
    what is said in terms of what the intention was. How does this change the state
    of the world? Is it actionable?
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，实用分析是试图根据意图重新解释所说内容的另一层复杂性。这如何改变世界的状态？它是否可行？
- en: Simple text analysis
  id: totrans-1326
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 简单文本分析
- en: 'The straightforward representation of the document is a bag of words. Scala,
    and Spark, provides an excellent paradigm to perform analysis on the word distributions.
    First, we read the whole collection of texts, and then count the unique words:'
  id: totrans-1327
  prefs: []
  type: TYPE_NORMAL
  zh: 文档的直接表示是一个词袋。Scala 和 Spark 提供了一个出色的范例来对词分布进行分析。首先，我们读取整个文本集合，然后计算独特的单词数量：
- en: '[PRE159]'
  id: totrans-1328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE159]'
- en: 'This gives us just an estimate of the number of distinct words in the repertoire
    of quite different authors. The simplest way to find intersection between the
    two corpuses is to find the common vocabulary (which will be quite different as
    *Leo Tolstoy* wrote in Russian and French, while *Shakespeare* was an English-writing
    author):'
  id: totrans-1329
  prefs: []
  type: TYPE_NORMAL
  zh: 这仅仅为我们提供了一个关于不同作者词汇库中不同单词数量的估计。找到两个语料库之间交集的最简单方法就是找到共同的词汇（由于列夫·托尔斯泰用俄语和法语写作，而莎士比亚是一位英语作家，所以这将非常不同）：
- en: '[PRE160]'
  id: totrans-1330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE160]'
- en: 'A few thousands word indices are manageable with the current implementations.
    For any new story, we can determine whether it is more likely to be written by
    Leo Tolstoy or *William Shakespeare*. Let''s take a look at *The King James Version
    of the Bible*, which also can be downloaded from Project Gutenberg ([https://www.gutenberg.org/files/10/10-h/10-h.htm](https://www.gutenberg.org/files/10/10-h/10-h.htm)):'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: 几千个单词索引在当前实现中是可管理的。对于任何新的故事，我们可以确定它更有可能是由列夫·托尔斯泰还是威廉·莎士比亚所写。让我们看看《圣经的詹姆斯国王版》，它也可以从古腾堡计划下载（[https://www.gutenberg.org/files/10/10-h/10-h.htm](https://www.gutenberg.org/files/10/10-h/10-h.htm)）：
- en: '[PRE161]'
  id: totrans-1332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE161]'
- en: 'This seems reasonable as the religious language was popular during the Shakespearean
    time. On the other hand, plays by *Anton Chekhov* have a larger intersection with
    the *Leo Tolstoy* vocabulary:'
  id: totrans-1333
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是有道理的，因为在莎士比亚时代，宗教语言很流行。另一方面，安东·契诃夫的戏剧与列夫·托尔斯泰的词汇有更大的交集：
- en: '[PRE162]'
  id: totrans-1334
  prefs: []
  type: TYPE_PRE
  zh: '[PRE162]'
- en: This is a very simple approach that works, but there are a number of commonly
    known improvements we can make. First, a common technique is to stem the words.
    In many languages, words have a common part, often called root, and a changeable
    prefix or suffix, which may depend on the context, gender, time, and so on. Stemming
    is the process of improving the distinct count and intersection by approximating
    this flexible word form to the root, base, or a stem form in general. The stem
    form does not need to be identical to the morphological root of the word, it is
    usually sufficient that related words map to the same stem, even if this stem
    is not in itself a valid grammatical root. Secondly, we probably should account
    for the frequency of the words—while we will describe more elaborate methods in
    the next section, for the purpose of this exercise, we'll exclude the words with
    very high count, that usually are present in any document such as articles and
    possessive pronouns, which are usually called stop words, and the words with very
    low count. Specifically, I'll use the optimized **Porter Stemmer** implementation
    that I described in more detail at the end of the chapter.
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常简单但有效的方法，但我们还可以进行一些常见的改进。首先，一个常见的技巧是对单词进行词干提取。在许多语言中，单词有一个共同的组成部分，通常称为词根，以及一个可变的前缀或后缀，这可能会根据上下文、性别、时间等因素而变化。词干提取是通过将这种灵活的词形近似到词根、词干或一般形式来提高独特计数和交集的过程。词干形式不需要与单词的形态学词根完全相同，通常只要相关单词映射到相同的词干就足够了，即使这个词干本身不是一个有效的语法词根。其次，我们可能应该考虑单词的频率——虽然我们将在下一节中描述更复杂的方法，但为了这个练习的目的，我们将排除计数非常高的单词，这些单词通常在任何文档中都存在，如文章和所有格代词，这些通常被称为停用词，以及计数非常低的单词。具体来说，我将使用我详细描述在章节末尾的优化版**Porter词干提取器**实现。
- en: Note
  id: totrans-1336
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: The [http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)
    site contains some of the Porter Stemmer implementations in Scala and other languages,
    including a highly optimized ANSI C, which may be more efficient, but here I will
    provide another optimized Scala version that can be used immediately with Spark.
  id: totrans-1337
  prefs: []
  type: TYPE_NORMAL
  zh: '[http://tartarus.org/martin/PorterStemmer/](http://tartarus.org/martin/PorterStemmer/)网站包含了一些Porter词干提取器的Scala和其他语言的实现，包括高度优化的ANSI
    C，这可能更有效率，但在这里我将提供另一个优化的Scala版本，它可以立即与Spark一起使用。'
- en: 'The Stemmer example will stem the words and count the relative intersections
    between them, removing the stop words:'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: 词干提取示例将对单词进行词干提取，并计算它们之间的相对交集，同时移除停用词：
- en: '[PRE163]'
  id: totrans-1339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE163]'
- en: 'When one runs the main class example from the command line, it outputs the
    stemmed bag sizes and intersection for datasets specified as parameters (these
    are directories in the home filesystem with documents):'
  id: totrans-1340
  prefs: []
  type: TYPE_NORMAL
  zh: 当从命令行运行主类示例时，它将输出指定为参数的数据集的词干包大小和交集（这些是主文件系统中的目录，包含文档）：
- en: '[PRE164]'
  id: totrans-1341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE164]'
- en: This, in this case, just confirms the hypothesis that Bible's vocabulary is
    closer to *William Shakespeare* than to Leo Tolstoy and other sources. Interestingly,
    modern vocabularies of *NY Times* articles and Enron's e-mails from the previous
    chapters are much closer to *Leo Tolstoy's*, which is probably more an indication
    of the translation quality.
  id: totrans-1342
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，这仅仅证实了圣经的词汇比**列奥·托尔斯泰**和其他来源更接近**威廉·莎士比亚**的假设。有趣的是，现代的**《纽约时报》**文章和上一章中提到的安然电子邮件的词汇与**列奥·托尔斯泰**的词汇非常接近，这可能是翻译质量的更好指示。
- en: Another thing to notice is that the pretty complex analysis took about *40*
    lines of Scala code (not counting the libraries, specifically the Porter Stemmer,
    which is about ~ *100* lines) and about 12 seconds. The power of Scala is that
    it can leverage other libraries very efficiently to write concise code.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 另一点要注意的是，这个相当复杂的分析大约需要40行Scala代码（不包括库，特别是Porter词干提取器，大约有100行）和大约12秒。Scala的强大之处在于它可以非常有效地利用其他库来编写简洁的代码。
- en: Note
  id: totrans-1344
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**Serialization**'
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: '**序列化**'
- en: We already talked about serialization in [Chapter 6](part0273.xhtml#aid-84B9I2
    "Chapter 6. Working with Unstructured Data"), *Working with Unstructured Data*.
    As Spark's tasks are executed in different threads and potentially JVMs, Spark
    does a lot of serialization/deserialization when passing the objects. Potentially,
    I could use `map { val stemmer = new Stemmer; stemmer.stem(_) }` instead of `map
    { stemmer.stem(_) }`, but the latter reuses the object for multiple iterations
    and seems to be linguistically more appealing. One suggested performance optimization
    is to use *Kryo serializer*, which is less flexible than the Java serializer,
    but more performant. However, for integrative purpose, it is much easier to just
    make every object in the pipeline serializable and use default Java serialization.
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在[第6章](part0273.xhtml#aid-84B9I2 "第6章。处理非结构化数据")*处理非结构化数据*中讨论了序列化。由于Spark的任务在不同的线程和潜在的JVM中执行，Spark在传递对象时进行了大量的序列化和反序列化。潜在地，我可以用`map
    { val stemmer = new Stemmer; stemmer.stem(_) }`代替`map { stemmer.stem(_) }`，但后者在多次迭代中重用对象，在语言上似乎更吸引人。一种建议的性能优化是使用*Kryo序列化器*，它比Java序列化器灵活性更低，但性能更好。然而，为了集成目的，只需使管道中的每个对象可序列化并使用默认的Java序列化就更容易了。
- en: 'As another example, let''s compute the distribution of word frequencies, as
    follows:'
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 作为另一个例子，让我们计算单词频率的分布，如下所示：
- en: '[PRE165]'
  id: totrans-1348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE165]'
- en: 'The distribution of relative frequencies on the log-log scale is presented
    in the following diagram. With the exception of the first few tokens, the dependency
    of frequency on rank is almost linear:'
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图表中展示了对数-对数尺度上的相对频率分布。除了前几个标记之外，频率对排名的依赖性几乎是线性的：
- en: '![Simple text analysis](img/image01756.jpeg)'
  id: totrans-1350
  prefs: []
  type: TYPE_IMG
  zh: '![简单的文本分析](img/image01756.jpeg)'
- en: Figure 9-2\. A typical distribution of word relative frequencies on log-log
    scale (Zipf's Law)
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 图9-2\. 对数-对数尺度上单词相对频率的典型分布（Zipf定律）
- en: MLlib algorithms in Spark
  id: totrans-1352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Spark中的MLlib算法
- en: Let's halt at MLlib that complements other NLP libraries written in Scala. MLlib
    is primarily important because of scalability, and thus supports a few of the
    data preparation and text processing algorithms, particularly in the area of feature
    construction ([http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html)).
  id: totrans-1353
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在MLlib上停下来，MLlib是Scala编写的其他NLP库的补充。MLlib之所以重要，主要是因为其可扩展性，因此支持一些数据准备和文本处理算法，尤其是在特征构造领域([http://spark.apache.org/docs/latest/ml-features.html](http://spark.apache.org/docs/latest/ml-features.html))。
- en: TF-IDF
  id: totrans-1354
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: TF-IDF
- en: Although the preceding analysis can already give a powerful insight, the piece
    of information that is missing from the analysis is term frequency information.
    The term frequencies are relatively more important in information retrieval, where
    the collection of documents need to be searched and ranked in relation to a few
    terms. The top documents are usually returned to the user.
  id: totrans-1355
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管前面的分析已经可以提供强大的洞察力，但分析中缺失的信息是术语频率信息。在信息检索中，术语频率相对更重要，因为需要根据一些术语对文档集合进行搜索和排序。通常将顶级文档返回给用户。
- en: TF-IDF is a standard technique where term frequencies are offset by the frequencies
    of the terms in the corpus. Spark has an implementation of the TF-IDF. Spark uses
    a hash function to identify the terms. This approach avoids the need to compute
    a global term-to-index map, but can be subject to potential hash collisions, the
    probability of which is determined by the number of buckets of the hash table.
    The default feature dimension is *2^20=1,048,576*.
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是一种标准技术，其中术语频率被语料库中术语的频率所抵消。Spark实现了TF-IDF。Spark使用哈希函数来识别术语。这种方法避免了计算全局术语到索引映射的需要，但可能会受到潜在的哈希冲突的影响，其概率由哈希表的桶数决定。默认特征维度是*2^20=1,048,576*。
- en: 'In the Spark implementation, each document is a line in the dataset. We can
    convert it into to an RDD of iterables and compute the hashing by the following
    code:'
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark实现中，数据集中的每一行都是一个文档。我们可以将其转换为可迭代的RDD，并使用以下代码进行哈希计算：
- en: '[PRE166]'
  id: totrans-1358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE166]'
- en: 'When computing `hashingTF`, we only need a single pass over the data, applying
    IDF needs two passes: first to compute the IDF vector and second to scale the
    term frequencies by IDF:'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算`hashingTF`时，我们只需要对数据进行单次遍历，应用IDF需要两次遍历：首先计算IDF向量，然后使用IDF缩放术语频率：
- en: '[PRE167]'
  id: totrans-1360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE167]'
- en: Here we see each document represented by a set of terms and their scores.
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们看到每个文档由一组术语及其分数表示。
- en: LDA
  id: totrans-1362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: LDA
- en: LDA in Spark MLlib is a clustering mechanism, where the feature vectors represent
    the counts of words in a document. The model maximizes the probability of observing
    the word counts, given the assumption that each document is a mixture of topics
    and the words in the documents are generated based on **Dirichlet distribution**
    (a generalization of beta distribution on multinomial case) for each of the topic
    independently. The goal is to derive the (latent) distribution of the topics and
    the parameters of the words generation statistical model.
  id: totrans-1363
  prefs: []
  type: TYPE_NORMAL
  zh: Spark MLlib中的LDA是一种聚类机制，其中特征向量表示文档中单词的计数。该模型最大化观察到的单词计数的概率，假设每个文档是主题的混合，文档中的单词是基于**Dirichlet分布**（多项式情况下的beta分布的推广）独立地为每个主题生成的。目标是推导出（潜在）主题分布和单词生成统计模型的参数。
- en: The MLlib implementation is based on 2009 LDA paper ([http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf))
    and uses GraphX to implement a distributed **Expectation Maximization** (**EM**)
    algorithm for assigning topics to the documents.
  id: totrans-1364
  prefs: []
  type: TYPE_NORMAL
  zh: MLlib的实现基于2009年的LDA论文([http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf](http://www.jmlr.org/papers/volume10/newman09a/newman09a.pdf))，并使用GraphX实现一个分布式**期望最大化**(**EM**)算法，用于将主题分配给文档。
- en: 'Let''s take the Enron e-mail corpus discussed in [Chapter 7](part0283.xhtml#aid-8DSF61
    "Chapter 7. Working with Graph Algorithms"), *Working with Graph Algorithms*,
    where we tried to figure out communications graph. For e-mail clustering, we need
    to extract the body of the e-mail and place is as a single line in the training
    file:'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以第七章中讨论的安然电子邮件语料库为例，*使用图算法*，在那里我们试图分析通信图。对于电子邮件聚类，我们需要提取电子邮件正文并将其作为单行放置在训练文件中：
- en: '[PRE168]'
  id: totrans-1366
  prefs: []
  type: TYPE_PRE
  zh: '[PRE168]'
- en: 'Now, let''s use Scala/Spark to construct a corpus dataset containing the document
    ID, followed by a dense array of word counts in the bag:'
  id: totrans-1367
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Scala/Spark构建一个包含文档ID的语料库数据集，后面跟着一个密集的单词计数数组：
- en: '[PRE169]'
  id: totrans-1368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE169]'
- en: 'We can also list the words and their relative importance for the topic in the
    descending order:'
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以按主题的相对重要性降序列出单词及其相关重要性：
- en: '[PRE170]'
  id: totrans-1370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE170]'
- en: 'To find out the top documents per topic or top topics per document, we need
    to convert this model to `DistributedLDA` or `LocalLDAModel`, which extend `LDAModel`:'
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 要找出每个主题的前置文档或每个文档的前置主题，我们需要将此模型转换为`DistributedLDA`或`LocalLDAModel`，它们扩展了`LDAModel`：
- en: '[PRE171]'
  id: totrans-1372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE171]'
- en: Segmentation, annotation, and chunking
  id: totrans-1373
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 分割、标注和分块
- en: When the text is presented in digital form, it is relatively easy to find words
    as we can split the stream on non-word characters. This becomes more complex in
    spoken language analysis. In this case, segmenters try to optimize a metric, for
    example, to minimize the number of distinct words in the lexicon and the length
    or complexity of the phrase (*Natural Language Processing with Python* by *Steven
    Bird et al*, *O'Reilly Media Inc*, 2009).
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: 当文本以数字形式呈现时，找到单词相对容易，因为我们可以在非单词字符上分割流。在口语语言分析中，这变得更加复杂。在这种情况下，分词器试图优化一个指标，例如，最小化词典中不同单词的数量以及短语的长短或复杂性（*《Python自然语言处理》*，作者*Steven
    Bird等人*，*O'Reilly Media Inc*，2009年）。
- en: Annotation usually refers to parts-of-speech tagging. In English, these are
    nouns, pronouns, verbs, adjectives, adverbs, articles, prepositions, conjunctions,
    and interjections. For example, in the phrase *we saw the yellow dog*, *we* is
    a pronoun, *saw* is a verb, *the* is an article, *yellow* is an adjective, and
    *dog* is a noun.
  id: totrans-1375
  prefs: []
  type: TYPE_NORMAL
  zh: 标注通常指的是词性标注。在英语中，这些是名词、代词、动词、形容词、副词、冠词、介词、连词和感叹词。例如，在短语*我们看到了黄色的狗*中，*我们*是代词，*看到了*是动词，*the*是冠词，*yellow*是形容词，*dog*是名词。
- en: In some languages, the chunking and annotation depends on context. For example,
    in Chinese, *????* literally translates to *love country person* and can mean
    either *country-loving person* or *love country-person*. In Russian, *???????
    ?????? ??????????*, literally translating to *execute not pardon*, can mean *execute,
    don't pardon*, or *don't execute, pardon*. While in written language, this can
    be disambiguated using commas, in a spoken language this is usually it is very
    hard to recognize the difference, even though sometimes the intonation can help
    to segment the phrase properly.
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些语言中，分块和标注取决于上下文。例如，在中文中，*爱国者*字面意思是*爱国家的人*，可以指*爱国家的人*或*爱国家的人*。在俄语中，*???????
    ?????? ??????????*字面意思是*执行不赦免*，可以指*执行，不赦免*或*不执行，赦免*。虽然在书面语言中，这可以通过逗号来消除歧义，但在口语中，这通常很难识别差异，尽管有时语调可以帮助正确地划分短语。
- en: For techniques based on word frequencies in the bags, some extremely common
    words, which are of little value in helping select documents, are explicitly excluded
    from the vocabulary. These words are called stop words. There is no good general
    strategy for determining a stop list, but in many cases, this is to exclude very
    frequent words that appear in almost every document and do not help to differentiate
    between them for classification or information retrieval purposes.
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于词袋中词频的技术，一些极其常见的单词，它们在帮助选择文档时价值不大，被明确地从词汇表中排除。这些单词被称为停用词。没有好的通用策略来确定停用词表，但在许多情况下，这是排除几乎出现在每份文档中的非常频繁的单词，这些单词对于分类或信息检索目的没有帮助来区分它们。
- en: POS tagging
  id: totrans-1378
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词性标注
- en: 'POS tagging probabilistically annotates each word with it''s grammatical function—noun,
    verb, adjective, and so on. Usually, POS tagging serves as an input to syntactic
    and semantic analysis. Let''s demonstrate POS tagging on the FACTORIE toolkit
    example, a software library written in Scala ([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu)).
    To start, you need to download the binary image or source files from [https://github.com/factorie/factorie.git](https://github.com/factorie/factorie.git)
    and build it:'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: 词性标注以概率方式将每个词标注为它的语法功能——名词、动词、形容词等等。通常，词性标注作为句法和语义分析输入。让我们在FACTORIE工具包示例中演示词性标注，这是一个用Scala编写的软件库([http://factorie.cs.umass.edu](http://factorie.cs.umass.edu))。首先，你需要从[https://github.com/factorie/factorie.git](https://github.com/factorie/factorie.git)下载二进制镜像或源文件并构建它：
- en: '[PRE172]'
  id: totrans-1380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE172]'
- en: 'After the build, which also includes model training, the following command
    will start a network server on `port 3228`:'
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建过程中，这还包括模型训练，以下命令将在`port 3228`上启动一个网络服务器：
- en: '[PRE173]'
  id: totrans-1382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE173]'
- en: 'Now, all traffic to `port 3228` will be interpreted (as text), and the output
    will be tokenized and annotated:'
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，所有流向`port 3228`的流量都将被解释（作为文本），输出将被分词和标注：
- en: '[PRE174]'
  id: totrans-1384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE174]'
- en: This POS is a single-path left-right tagger that can process the text as a stream.
    Internally, the algorithm uses probabilistic techniques to find the most probable
    assignment. Let's also look at other techniques that do not use grammatical analysis
    and yet proved to be very useful for language understanding and interpretation.
  id: totrans-1385
  prefs: []
  type: TYPE_NORMAL
  zh: 这种词性标注是一个单一路径的左右标注器，可以像流一样处理文本。内部，算法使用概率技术来找到最可能的分配。让我们也看看其他不使用语法分析但已被证明对语言理解和解释非常有用的技术。
- en: Using word2vec to find word relationships
  id: totrans-1386
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用word2vec查找词关系
- en: 'Word2vec has been developed by Tomas Mikolov at Google, around 2012\. The original
    idea behind word2vec was to demonstrate that one might improve efficiency by trading
    the model''s complexity for efficiency. Instead of representing a document as
    bags of words, word2vec takes each word context into account by trying to analyze
    n-grams or skip-grams (a set of surrounding tokens with potential the token in
    question skipped). The words and word contexts themselves are represented by an
    array of floats/doubles ![Using word2vec to find word relationships](img/image01757.jpeg).
    The objective function is to maximize log likelihood:'
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: Word2vec 是由谷歌的托马斯·米科尔洛夫（Tomas Mikolov）在2012年左右开发的。word2vec背后的原始想法是通过以效率换取模型的复杂性来提高效率。word2vec不是将文档表示为词袋，而是通过尝试分析n-gram或skip-gram（一个包含潜在问题标记的周围标记集）来考虑每个词的上下文。单词及其上下文本身由浮点数/双精度浮点数数组表示![使用word2vec查找词关系](img/image01757.jpeg)。目标函数是最大化对数似然：
- en: '![Using word2vec to find word relationships](img/image01758.jpeg)'
  id: totrans-1388
  prefs: []
  type: TYPE_IMG
  zh: '![使用word2vec查找词关系](img/image01758.jpeg)'
- en: 'Where:'
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '![Using word2vec to find word relationships](img/image01759.jpeg)'
  id: totrans-1390
  prefs: []
  type: TYPE_IMG
  zh: '![使用word2vec查找词关系](img/image01759.jpeg)'
- en: By choosing the optimal ![Using word2vec to find word relationships](img/image01757.jpeg)
    and to get a comprehensive word representation (also called **map optimization**).
    Similar words are found based on cosine similarity metric (dot product) of ![Using
    word2vec to find word relationships](img/image01757.jpeg). Spark implementation
    uses hierarchical softmax, which reduces the complexity of computing the conditional
    probability to ![Using word2vec to find word relationships](img/image01760.jpeg),
    or log of the vocabulary size *V*, as opposed to ![Using word2vec to find word
    relationships](img/image01761.jpeg), or proportional to *V*. The training is still
    linear in the dataset size, but is amenable to big data parallelization techniques.
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择最优的 ![使用 word2vec 找到单词关系](img/image01757.jpeg) 并获得全面的单词表示（也称为 **映射优化**）。基于余弦相似度度量（点积）找到相似单词
    ![使用 word2vec 找到单词关系](img/image01757.jpeg)。Spark 实现使用层次 softmax，将计算条件概率的复杂性降低到
    ![使用 word2vec 找到单词关系](img/image01760.jpeg)，或词汇大小 *V* 的对数，而不是 ![使用 word2vec 找到单词关系](img/image01761.jpeg)，或与
    *V* 成正比。训练仍然是数据集大小的线性，但适用于大数据并行化技术。
- en: '`Word2vec` is traditionally used to predict the most likely word given context
    or find similar words with a similar meaning (synonyms). The following code trains
    in `word2vec` model on *Leo Tolstoy''s Wars and Peace*, and finds synonyms for
    the word *circle*. I had to convert the Gutenberg''s representation of *War and
    Peace* to a single-line format by running the `cat 2600.txt | tr "\n\r" " " >
    warandpeace.txt` command:'
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: '`Word2vec` 通常是用来根据上下文预测最可能的单词，或者找到具有相似意义的相似单词（同义词）。以下代码在 *列夫·托尔斯泰的《战争与和平》*
    上训练 `word2vec` 模型，并找到单词 *circle* 的同义词。我不得不通过运行 `cat 2600.txt | tr "\n\r" " " >
    warandpeace.txt` 命令将古腾堡的 *《战争与和平》* 表现转换为单行格式：'
- en: '[PRE175]'
  id: totrans-1393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE175]'
- en: While in general, it is hard to some with an objective function, and `freedom`
    is not listed as a synonym to `life` in the English Thesaurus, the results do
    make sense.
  id: totrans-1394
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在一般情况下，找到一个客观函数是困难的，并且 `freedom` 在英语同义词词典中没有被列为 `life` 的同义词，但结果确实是有意义的。
- en: 'Each word in the word2vec model is represented as an array of doubles. Another
    interesting application is to find associations *a to b is the same as c to ?*
    by performing subtraction *vector(a) - vector(b) + vector(c)*:'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 在 word2vec 模型中，每个单词都表示为一个双精度浮点数数组。另一个有趣的应用是找到关联 *a 到 b 与 c 到 ? 相同* 通过执行减法 *vector(a)
    - vector(b) + vector(c)*：
- en: '[PRE176]'
  id: totrans-1396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE176]'
- en: This can be used to find relationships in the language.
  id: totrans-1397
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以用来在语言中找到关系。
- en: A Porter Stemmer implementation of the code
  id: totrans-1398
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Porter Stemmer 的代码实现
- en: 'Porter Stemmer was first developed around the 1980s and there are many implementations.
    The detailed steps and original reference are provided at [http://tartarus.org/martin/PorterStemmer/def.txt](http://tartarus.org/martin/PorterStemmer/def.txt).
    It consists of roughly 6-9 steps of suffix/endings replacements, some of which
    are conditional on prefix or stem. I will provide a Scala-optimized version with
    the book code repository. For example, step 1 covers the majority of stemming
    cases and consists of 12 substitutions: the last 8 of which are conditional on
    the number of syllables and the presence of vowels in the stem:'
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: Porter Stemmer 首次在 20 世纪 80 年代开发，有许多实现方式。详细的步骤和原始参考可以在 [http://tartarus.org/martin/PorterStemmer/def.txt](http://tartarus.org/martin/PorterStemmer/def.txt)
    找到。它大致包括 6-9 步的词尾/结尾替换，其中一些取决于前缀或词根。我将提供一个与书籍代码仓库优化的 Scala 版本。例如，步骤 1 覆盖了大多数词干化情况，并包括
    12 个替换：最后 8 个取决于音节数和词根中的元音存在：
- en: '[PRE177]'
  id: totrans-1400
  prefs: []
  type: TYPE_PRE
  zh: '[PRE177]'
- en: The complete code is available at [https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala](https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala).
  id: totrans-1401
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的代码可以在 [https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala](https://github.com/alexvk/ml-in-scala/blob/master/chapter09/src/main/scala/Stemmer.scala)
    找到。
- en: Summary
  id: totrans-1402
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, I described basic NLP concepts and demonstrated a few basic
    techniques. I hoped to demonstrate that pretty complex NLP concepts could be expressed
    and tested in a few lines of Scala code. This is definitely just the tip of the
    iceberg as a lot of NLP techniques are being developed now, including the ones
    based on in-CPU parallelization as part of GPUs. (refer to, for example, **Puck**
    at [https://github.com/dlwh/puck](https://github.com/dlwh/puck)). I also gave
    a flavor of major Spark MLlib NLP implementations.
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我描述了基本的NLP概念，并演示了一些基本技术。我希望展示，相当复杂的NLP概念可以用几行Scala代码表达和测试。这无疑是冰山一角，因为现在正在开发许多NLP技术，包括基于GPU的CPU内并行化的技术。（例如，参考[https://github.com/dlwh/puck](https://github.com/dlwh/puck)中的**Puck**)。我还介绍了主要的Spark
    MLlib NLP实现。
- en: In the next chapter, which will be the final chapter of this book, I'll cover
    systems and model monitoring.
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，也就是本书的最后一章，我将介绍系统和模型监控。
- en: Chapter 10. Advanced Model Monitoring
  id: totrans-1405
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章 高级模型监控
- en: Even though this is the last chapter of the book, it can hardly be an afterthought
    even though monitoring in general often is in practical situations, quite unfortunately.
    Monitoring is a vital deployment component for any long execution cycle component
    and thus is part of the finished product. Monitoring can significantly enhance
    product experience and define future success as it improves problem diagnostic
    and is essential to determine the improvement path.
  id: totrans-1406
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是本书的最后一章，但在实际情况下，监控通常被视为一个事后考虑的问题，这实在是不幸。监控是任何长时间执行周期组件的重要部署组件，因此它是最终产品的一部分。监控可以显著提升产品体验，并定义未来的成功，因为它改善了问题诊断，并对于确定改进路径至关重要。
- en: One of the primary rules of successful software engineering is to create systems
    as if they were targeted for personal use when possible, which fully applies to
    monitoring, diagnostic, and debugging—quite hapless name for fixing existing issues
    in software products. Diagnostic and debugging of complex systems, particularly
    distributed systems, is hard, as the events often can be arbitrary interleaved
    and program executions subject to race conditions. While there is a lot of research
    going in the area of distributed system devops and maintainability, this chapter
    will scratch the service and provide guiding principle to design a maintainable
    complex distributed system.
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: 成功软件工程的一个基本原则是，在可能的情况下，将系统设计成针对个人使用，这一点完全适用于监控、诊断和调试——对于修复软件产品中现有问题的名称来说，这实在是一个相当不幸的名称。诊断和调试复杂系统，尤其是分布式系统，是困难的，因为事件往往可以任意交织，程序执行可能受到竞态条件的影响。尽管在分布式系统devops和可维护性领域有很多研究正在进行，但本章将探讨这一领域，并提供设计可维护的复杂分布式系统的指导原则。
- en: To start with, a pure functional approach, which Scala claims to follow, spends
    a lot of time avoiding side effects. While this idea is useful in a number of
    aspects, it is hard to imagine a useful program that has no effect on the outside
    world, the whole idea of a data-driven application is to have a positive effect
    on the way the business is conducted, a well-defined side effect.
  id: totrans-1408
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，一个纯函数式方法，Scala声称遵循这种方法，花费了大量时间避免副作用。虽然这个想法在许多方面都是有用的，但很难想象一个对现实世界没有影响的程序是有用的，数据驱动应用程序的整个理念就是要对业务运营产生积极影响，这是一个定义良好的副作用。
- en: Monitoring clearly falls in the side effect category. Execution needs to leave
    a trace that the user can later parse in order to understand where the design
    or implementation went awry. The trace of the execution can be left by either
    writing something on a console or into a file, usually called a log, or returning
    an object that contains the trace of the program execution, and the intermediate
    results. The latter approach, which is actually more in line with functional programming
    and monadic philosophy, is actually more appropriate for the distributed programming
    but often overlooked. This would have been an interesting topic for research,
    but unfortunately the space is limited and I have to discuss the practical aspects
    of monitoring in contemporary systems that is almost always done by logging. Having
    the monadic approach of carrying an object with the execution trace on each call
    can certainly increase the overhead of the interprocess or inter-machine communication,
    but saves a lot of time in stitching different pieces of information together.
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: 监控明显属于副作用类别。执行过程需要留下痕迹，以便用户日后可以解析，从而了解设计或实现出现偏差的地方。执行痕迹可以通过在控制台或文件（通常称为日志文件）中写入内容，或者返回一个包含程序执行痕迹和中间结果的对象来实现。后者实际上更符合函数式编程和单子哲学，对于分布式编程来说更为合适，但往往被忽视。这本来可以是一个有趣的研究课题，但遗憾的是空间有限，我不得不讨论当代系统中监控的实用方面，这些监控几乎总是通过日志来完成的。在每个调用中携带带有执行痕迹的对象的单子方法，无疑会增加进程间或机器间通信的开销，但可以节省大量时间来拼接不同的信息。
- en: 'Let''s list the naive approaches to debugging that everyone who needed to find
    a bug in the code tried:'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们列出每个人在需要找到代码中的错误时尝试的简单调试方法：
- en: Analyzing program output, particularly logs produced by simple print statements
    or built-in logback, java.util.logging, log4j, or the slf4j façade
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析程序输出，尤其是由简单的打印语句或内置的 logback、java.util.logging、log4j 或 slf4j 门面产生的日志
- en: Attaching a (remote) debugger
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接（远程）调试器
- en: Monitoring CPU, disk I/O, memory (to resolve higher level resource-utilization
    issues)
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控 CPU、磁盘 I/O、内存（以解决高级资源利用率问题）
- en: More or less, all these approaches fail if we have a multithreaded or distributed
    system—and Scala is inherently multithreaded as Spark is inherently distributed.
    Collecting logs over a set of nodes is not scalable (even though a few successful
    commercial systems exist that do this). Attaching a remote debugger is not always
    possible due to security and network restrictions. Remote debugging can also induce
    substantial overhead and interfere with the program execution, particularly for
    ones that use synchronization. Setting the debug level to the `DEBUG` or `TRACE`
    level helps sometimes, but leaves you at the mercy of the developer who may or
    may not have thought of a particular corner case you are dealing with right at
    the moment. The approach we take in this book is to open a servlet with enough
    information to glean into program execution and application methods real-time,
    as much as it is possible with the current state of Scala and Scalatra.
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 大体上，所有这些方法在存在多线程或分布式系统的情况下都会失败——Scala 本身就是多线程的，而 Spark 本身就是分布式的。在多个节点上收集日志不可扩展（尽管存在一些成功的商业系统这样做）。由于安全和网络限制，远程调试并不总是可能的。远程调试也可能引起大量的开销，并干扰程序执行，尤其是对于使用同步的程序。将调试级别设置为
    `DEBUG` 或 `TRACE` 级别有时有帮助，但将你置于开发者手中，开发者可能或可能没有考虑到你当前正在处理的特定角落情况。本书采取的方法是打开一个包含足够信息的
    servlet，以便实时了解程序执行和应用方法，尽可能多地在 Scala 和 Scalatra 的当前状态下做到这一点。
- en: 'Enough about the overall issues of debugging the program execution. Monitoring
    is somewhat different, as it is concerned with only high-level issue identification.
    Intersection with issue investigation or resolution happens, but usually is outside
    of monitoring. In this chapter, we will cover the following topics:'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 关于程序执行调试的总体问题已经说得够多了。监控有所不同，因为它只关注高级问题识别。与问题调查或解决相交，但通常发生在监控之外。在本章中，我们将涵盖以下主题：
- en: Understanding major areas for monitoring and monitoring goals
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解监控的主要领域和监控目标
- en: Learning OS tools for Scala/Java monitoring to support issue identification
    and debugging
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习用于 Scala/Java 监控的操作系统工具，以支持问题识别和调试
- en: Learning about MBeans and MXBeans
  id: totrans-1418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解 MBeans 和 MXBeans
- en: Understanding model performance drift
  id: totrans-1419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模型性能漂移
- en: Understanding A/B testing
  id: totrans-1420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解 A/B 测试
- en: System monitoring
  id: totrans-1421
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 系统监控
- en: 'While there are other types of monitoring dealing specifically with ML-targeted
    tasks, such as monitoring the performance of the models, let me start with basic
    system monitoring. Traditionally, system monitoring is a subject of operating
    system maintenance, but it is becoming a vital component of any complex application,
    specifically running over a set of distributed workstations. The primary components
    of the OS are CPU, disk, memory, network, and energy on battery-powered machines.
    The traditional OS-like tools for monitoring system performance are provided in
    the following table. We limit them to Linux tools as this is the platform for
    most Scala applications, even though other OS vendors provide OS monitoring tools
    such as **Activity Monitor**. As Scala runs in Java JVM, I also added Java-specific
    monitoring tools that are specific to JVMs:'
  id: totrans-1422
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在其他类型的监控，专门针对机器学习任务，例如监控模型的性能，但让我从基本的系统监控开始。传统上，系统监控是操作系统维护的一个主题，但它正成为任何复杂应用的一个关键组成部分，尤其是在多个分布式工作站上运行的应用。操作系统的核心组件包括CPU、磁盘、内存、网络以及电池供电机器上的能源。以下表格中提供了传统的类似操作系统的监控工具。我们将它们限制为Linux工具，因为这是大多数Scala应用的平台，尽管其他操作系统供应商提供了诸如**活动监视器**之类的操作系统监控工具。由于Scala运行在Java
    JVM上，我还添加了针对JVM的特定Java监控工具：
- en: '| Area | Programs | Comments |'
  id: totrans-1423
  prefs: []
  type: TYPE_TB
  zh: '| 区域 | 项目 | 备注 |'
- en: '| --- | --- | --- |'
  id: totrans-1424
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| CPU | `htop,` `top`, `sar-u` | `top` has been the most often used performance
    diagnostic tool, as CPU and memory have been the most constraint resources. With
    the advent of distributed programming, network and disk tend to be the most constraint.
    |'
  id: totrans-1425
  prefs: []
  type: TYPE_TB
  zh: '| CPU | `htop,` `top`, `sar-u` | `top`一直是使用最频繁的性能诊断工具，因为CPU和内存一直是受限制的资源。随着分布式编程的出现，网络和磁盘往往成为受限制的资源。|'
- en: '| Disk | `iostat`, `sar -d`, `lsof` | The number of open files, provided by
    `lsof`, is often a constraining resource as many big data applications and daemons
    tend to keep multiple files open. |'
  id: totrans-1426
  prefs: []
  type: TYPE_TB
  zh: '| 磁盘 | `iostat`, `sar -d`, `lsof` | `lsof`提供的打开文件数量通常是一个限制性资源，因为许多大数据应用和守护进程倾向于保持多个文件打开。|'
- en: '| Memory | `top`, `free`, `vmstat`, `sar -r` | Memory is used by OS in multiple
    ways, for example to maintain disk I/O buffers so that having extra buffered and
    cached memory helps performance. |'
  id: totrans-1427
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | `top`, `free`, `vmstat`, `sar -r` | 内存以多种方式被操作系统使用，例如维护磁盘I/O缓冲区，因此额外的缓冲和缓存内存有助于性能。|'
- en: '| Network | `ifconfig`, `netstat`, `tcpdump`, `nettop`, `iftop`, `nmap` | Network
    is how the distributed systems talk and is an important OS component. From the
    application point of view, watch for errors, collisions, and dropped packets as
    an indicator of problems. |'
  id: totrans-1428
  prefs: []
  type: TYPE_TB
  zh: '| 网络 | `ifconfig`, `netstat`, `tcpdump`, `nettop`, `iftop`, `nmap` | 网络是分布式系统之间通信的方式，是重要的操作系统组件。从应用的角度来看，关注错误、冲突和丢失的数据包，作为问题的指示器。|'
- en: '| Energy | `powerstat` | While power consumption is traditionally not a part
    of OS monitoring, it is nevertheless a shared resource, which recently became
    one of the major costs for maintaining a working system. |'
  id: totrans-1429
  prefs: []
  type: TYPE_TB
  zh: '| 能源 | `powerstat` | 虽然传统上功耗不是操作系统监控的一部分，但它毕竟是一个共享资源，最近已成为维护工作系统的主要成本之一。|'
- en: '| Java | `jconsole`, `jinfo`, `jcmd`, `jmc` | All these tools allow you to
    examine configuration and run-time properties of an application. **Java Mission
    Control** (**JMC**) is shipped with JDK starting with version 7u40. |'
  id: totrans-1430
  prefs: []
  type: TYPE_TB
  zh: '| Java | `jconsole`, `jinfo`, `jcmd`, `jmc` | 所有这些工具都允许您检查应用程序的配置和运行时属性。**Java
    Mission Control**（**JMC**）从JDK 7u40版本开始随JDK一起提供。|'
- en: Table 10.1\. Common Linux OS monitoring tools
  id: totrans-1431
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 表10.1. 常见的Linux操作系统监控工具
- en: In many cases, the tools are redundant. For example, the CPU and memory information
    can be obtained with `top`, `sar`, and `jmc` commands.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，工具是多余的。例如，可以使用`top`、`sar`和`jmc`命令获取CPU和内存信息。
- en: There are a few tools for collecting this information over a set of distributed
    nodes. Ganglia is a BSD-licensed scalable distributed monitoring system ([http://ganglia.info](http://ganglia.info)).
    It is based on a hierarchical design and is very careful about data structure
    and algorithm designs. It is known to scale to 10,000s of nodes. It consists of
    a gmetad daemon that is collects information from multiple hosts and presents
    it in a web interface, and gmond daemons running on each individual host. The
    communication happens on the 8649 port by default, which spells Unix. By default,
    gmond sends information about CPU, memory, and network, but multiple plugins exist
    for other metrics (or can be created). Gmetad can aggregate the information and
    pass it up the hierarchy chain to another gmetad daemon. Finally, the data is
    presented in a Ganglia web interface.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 有一些工具可以跨分布式节点收集这些信息。Ganglia是一个BSD许可的可伸缩分布式监控系统([http://ganglia.info](http://ganglia.info))。它基于分层设计，非常注重数据结构和算法设计。它已知可以扩展到10,000多个节点。它由一个gmetad守护进程组成，该守护进程从多个主机收集信息并在Web界面中展示，以及在每个单独的主机上运行的gmond守护进程。默认情况下，通信发生在8649端口，这代表着Unix。默认情况下，gmond发送有关CPU、内存和网络的信息，但存在多个插件用于其他指标（或可以创建）。Gmetad可以聚合信息并将其传递到层次链中的另一个gmetad守护进程。最后，数据在Ganglia
    Web界面中展示。
- en: Graphite is another monitoring tool that stores numeric time-series data and
    renders graphs of this data on demand. The web app provides a /render endpoint
    to generate graphs and retrieve raw data via a RESTful API. Graphite has a pluggable
    backend (although it has it's own default implementation). Most of the modern
    metrics implementations, including scala-metrics used in this chapter, support
    sending data to Graphite.
  id: totrans-1434
  prefs: []
  type: TYPE_NORMAL
  zh: Graphite是另一个监控工具，它存储数值时间序列数据，并在需要时渲染这些数据的图表。该Web应用程序提供了一个/render端点来生成图表并通过RESTful
    API检索原始数据。Graphite有一个可插拔的后端（尽管它有自己的默认实现）。大多数现代指标实现，包括本章中使用的scala-metrics，都支持将数据发送到Graphite。
- en: Process monitoring
  id: totrans-1435
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进程监控
- en: The tools described in the previous section are not application-specific. For
    a long-running process, it often necessary to provide information about the internal
    state to either a monitoring a graphing solution such as Ganglia or Graphite,
    or just display it in a servlet. Most of these solutions are read-only, but in
    some cases, the commands give the control to the users to modify the state, such
    as log levels, or to trigger garbage collection.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中描述的工具不是特定于应用的。对于长时间运行的过程，通常需要向监控或图形解决方案提供有关内部状态的信息，例如Ganglia或Graphite，或者只是在一个servlet中显示它。大多数这些解决方案是只读的，但在某些情况下，命令会赋予用户修改状态的控制权，例如日志级别，或者触发垃圾回收。
- en: 'Monitoring, in general is supposed to do the following:'
  id: totrans-1437
  prefs: []
  type: TYPE_NORMAL
  zh: 监控通常应该执行以下操作：
- en: Provide high-level information about program execution and application-specific
    metrics
  id: totrans-1438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供关于程序执行和特定于应用的指标的高级信息
- en: Potentially, perform health-checks for critical components
  id: totrans-1439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能执行对关键组件的健康检查
- en: Might incorporate alerting and thresholding on some critical metrics
  id: totrans-1440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会包含对一些关键指标的警报和阈值设置
- en: I have also seen monitoring to include update operations to either update the
    logging parameters or test components, such as trigger model scoring with predefined
    parameters. The latter can be considered as a part of parameterized health check.
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: 我还看到监控包括更新操作，以更新日志参数或测试组件，例如使用预定义参数触发模型评分。后者可以被视为参数化健康检查的一部分。
- en: 'Let''s see how it works on the example of a simple `Hello World` web application
    that accepts REST-like requests and assigns a unique ID for different users written
    in the Scalatra framework ([http://scalatra.org](http://scalatra.org)), a lightweight
    web-application development framework in Scala. The application is supposed to
    respond to CRUD HTTP requests to create a unique numeric ID for a user. To implement
    the service in Scalatra, we need just to provide a `Scalate` template. The full
    documentation can be found at [http://scalatra.org/2.4/guides/views/scalate.html](http://scalatra.org/2.4/guides/views/scalate.html),
    the source code is provided with the book and can be found in `chapter10` subdirectory:'
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个简单的`Hello World` Web应用程序为例来了解它是如何工作的，该应用程序接受类似REST的请求并为不同的用户分配唯一的ID，该应用程序是用Scala框架Scalatra编写的([http://scalatra.org](http://scalatra.org))，这是一个Scala中的轻量级Web应用程序开发框架。该应用程序应该响应CRUD
    HTTP请求为用户创建一个唯一的数字ID。要在Scalatra中实现此服务，我们只需要提供一个`Scalate`模板。完整的文档可以在[http://scalatra.org/2.4/guides/views/scalate.html](http://scalatra.org/2.4/guides/views/scalate.html)找到，源代码与本书一起提供，可以在`chapter10`子目录中找到：
- en: '[PRE178]'
  id: totrans-1443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE178]'
- en: 'First, the code gets the `name` parameter from the request (REST-like parameter
    parsing is also supported). Then, it checks the internal HashMap for existing
    entries, and if the entry does not exist, it creates a new index using a synchronized
    call to increment `hwCounter` (in a real-world application, this information should
    be persistent in a database such as HBase, but I''ll skip this layer in this section
    for the purpose of simplicity). To run the application, one needs to download
    the code, start `sbt`, and type `~;jetty:stop;jetty:start` to enable continuous
    run/compilation as in [Chapter 7](part0283.xhtml#aid-8DSF61 "Chapter 7. Working
    with Graph Algorithms"), *Working with Graph Algorithms*. The modifications to
    the file will be immediately picked up by the build tool and the jetty server
    will restart:'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，代码从请求中获取`name`参数（也支持类似REST的参数解析）。然后，它检查内部HashMap中是否存在条目，如果不存在条目，它将使用对`hwCounter`的同步调用创建一个新的索引（在实际应用中，此类信息应持久存储在数据库中，如HBase，但在此部分中为了简化，我将跳过这一层）。要运行应用程序，需要下载代码，启动`sbt`，并输入`~;jetty:stop;jetty:start`以启用连续运行/编译，如[第7章](part0283.xhtml#aid-8DSF61
    "第7章。使用图算法")中所述，*使用图算法*。对文件的修改将立即被构建工具捕获，jetty服务器将重新启动：
- en: '[PRE179]'
  id: totrans-1445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE179]'
- en: 'When the servlet is started on port 8080, issue a browser request:'
  id: totrans-1446
  prefs: []
  type: TYPE_NORMAL
  zh: 当servlet在8080端口启动时，发出浏览器请求：
- en: Tip
  id: totrans-1447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: I pre-created the project for this book, but if you want to create a Scalatra
    project from scratch, there is a `gitter` command in `chapter10/bin/create_project.sh`.
    Gitter will create a `project/build.scala` file with a Scala object, extending
    build that will set project parameters and enable the Jetty plugin for the SBT.
  id: totrans-1448
  prefs: []
  type: TYPE_NORMAL
  zh: 我为这本书预先创建了项目，但如果你想要从头开始创建Scalatra项目，`chapter10/bin/create_project.sh`中有一个`gitter`命令。Gitter将创建一个`project/build.scala`文件，其中包含一个Scala对象，扩展了构建，这将设置项目参数并启用SBT的Jetty插件。
- en: '`http://localhost:8080/hw/Joe`.'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '`http://localhost:8080/hw/Joe`.'
- en: 'The output should look similar to the following screenshot:'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应类似于以下截图：
- en: '![Process monitoring](img/image01762.jpeg)'
  id: totrans-1451
  prefs: []
  type: TYPE_IMG
  zh: '![进程监控](img/image01762.jpeg)'
- en: 'Figure 10-1: The servlet web page.'
  id: totrans-1452
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-1：servlet网页。
- en: If you call the servlet with a different name, it will assign a distinct ID,
    which will be persistent across the lifetime of the application.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用不同的名称调用servlet，它将分配一个唯一的ID，该ID将在应用程序的生命周期内保持持久。
- en: 'As we also enabled console logging, you will also see something similar to
    the following command on the console:'
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们也启用了控制台日志记录，你将在控制台上看到类似以下命令的内容：
- en: '[PRE180]'
  id: totrans-1455
  prefs: []
  type: TYPE_PRE
  zh: '[PRE180]'
- en: While retrieving and analyzing logs, which can be redirected to a file, is an
    option and there are multiple systems to collect, search, and analyze logs from
    a set of distributed servers, it is often also important to have a simple way
    to introspect the running code. One way to accomplish this is to create a separate
    template with metrics, however, Scalatra provides metrics and health support to
    enable basic implementations for counts, histograms, rates, and so on.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: 在检索和分析日志时，可以将日志重定向到文件，并且有多个系统可以收集、搜索和分析来自一组分布式服务器的日志，但通常还需要一种简单的方法来检查运行中的代码。实现这一目标的一种方法是为指标创建一个单独的模板，然而，Scalatra提供了指标和健康支持，以实现计数、直方图、速率等基本实现。
- en: I will use the Scalatra metrics support. The `ScalatraBootstrap` class has to
    implement the `MetricsBootstrap` trait. The `org.scalatra.metrics.MetricsSupport`
    and `org.scalatra.metrics.HealthChecksSupport` traits provide templating similar
    to the Scalate templates, as shown in the following code.
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用Scalatra指标支持。`ScalatraBootstrap`类必须实现`MetricsBootstrap`特质。`org.scalatra.metrics.MetricsSupport`和`org.scalatra.metrics.HealthChecksSupport`特质提供了类似于Scalate模板的模板，如下面的代码所示。
- en: 'The following is the content of the `ScalatraTemplate.scala` file:'
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为`ScalatraTemplate.scala`文件的內容：
- en: '[PRE181]'
  id: totrans-1459
  prefs: []
  type: TYPE_PRE
  zh: '[PRE181]'
- en: 'The following is the content of the `ServletWithMetrics.scala` file:'
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 以下为`ServletWithMetrics.scala`文件的內容：
- en: '[PRE182]'
  id: totrans-1461
  prefs: []
  type: TYPE_PRE
  zh: '[PRE182]'
- en: 'If you run the server again, the `http://localhost:8080/admin` page will show
    a set of links for operational information, as shown in the following screenshot:'
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你再次运行服务器，`http://localhost:8080/admin`页面将显示一组操作信息的链接，如下面的截图所示：
- en: '![Process monitoring](img/image01763.jpeg)'
  id: totrans-1463
  prefs: []
  type: TYPE_IMG
  zh: '![进程监控](img/image01763.jpeg)'
- en: 'Figure 10-2: The admin servlet web page'
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-2：管理servlet网页
- en: 'The **Metrics** link will lead to the metrics servlet depicted in *Figure 10-3*.
    The `org.akozlov.exampes.ServletWithMetrics.counter` will have a global count
    of requests, and `org.akozlov.exampes.ServletWithMetrics.histogram` will show
    the distribution of accumulated values, in this case, the name lengths. More importantly,
    it will compute `50`, `75`, `95`, `98`, `99`, and `99.9` percentiles. The meter
    counter will show rates for the last `1`, `5`, and `15` minutes:'
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: '**指标**链接将引导到*图10-3*中所示的指标servlet。`org.akozlov.exampes.ServletWithMetrics.counter`将具有请求的全局计数，而`org.akozlov.exampes.ServletWithMetrics.histogram`将显示累积值的分布，在这种情况下，是名称长度。更重要的是，它将计算`50`、`75`、`95`、`98`、`99`和`99.9`分位数。计量计数器将显示过去`1`、`5`和`15`分钟内的速率：'
- en: '![Process monitoring](img/image01764.jpeg)'
  id: totrans-1466
  prefs: []
  type: TYPE_IMG
  zh: '![流程监控](img/image01764.jpeg)'
- en: 'Figure 10-3: The metrics servlet web page'
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-3：指标servlet网页
- en: 'Finally, one can write health checks. In this case, I will just check whether
    the result of the response function contains the string that it has been passed
    as a parameter. Refer to the following *Figure 10.4*:'
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，可以编写健康检查。在这种情况下，我将只检查响应函数的结果是否包含作为参数传递的字符串。请参考以下*图10.4*：
- en: '![Process monitoring](img/image01765.jpeg)'
  id: totrans-1469
  prefs: []
  type: TYPE_IMG
  zh: '![流程监控](img/image01765.jpeg)'
- en: 'Figure 10-4: The health check servlet web page.'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: 图10-4：健康检查servlet网页。
- en: The metrics can be configured to report to Ganglia or Graphite data collection
    servers or periodically dump information into a log file.
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 可以配置指标以向Ganglia或Graphite数据收集服务器报告，或者定期将信息记录到日志文件中。
- en: 'Endpoints do not have to be read-only. One of the pre-configured components
    is the timer, which measures the time to complete a task—which can be used for
    measuring scoring performance. Let''s put the code in the `ServletWithMetrics`
    class:'
  id: totrans-1472
  prefs: []
  type: TYPE_NORMAL
  zh: 端点不必是只读的。预配置的组件之一是计时器，它测量完成任务所需的时间——可以用来测量评分性能。让我们将代码放入`ServletWithMetrics`类中：
- en: '[PRE183]'
  id: totrans-1473
  prefs: []
  type: TYPE_PRE
  zh: '[PRE183]'
- en: Accessing `http://localhost:8080/time` will trigger code execution, which will
    be timed with a timer in metrics.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 访问`http://localhost:8080/time`将触发代码执行，这将通过指标中的计时器进行计时。
- en: Analogously, the put operation, which can be created with the `put()` template,
    can be used to either adjust the run-time parameters or execute the code in-situ—which,
    depending on the code, might need to be secured in production environments.
  id: totrans-1475
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，可以使用`put()`模板创建的put操作，可以用来调整运行时参数或就地执行代码——这取决于代码，可能需要在生产环境中进行安全加固。
- en: Note
  id: totrans-1476
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: '**JSR 110**'
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: '**JSR 110**'
- en: JSR 110 is another **Java Specification Request** (**JSR**), commonly known
    as **Java Management Extensions** (**JMX**). JSR 110 specifies a number of APIs
    and protocols in order to be able to monitor the JVM executions remotely. A common
    way to access JMX Services is via the `jconsole` command that will connect to
    one of the local processes by default. To connect to a remote host, you need to
    provide the `-Dcom.sun.management.jmxremote.port=portNum` property on the Java
    command line. It is also advisable to enable security (SSL or password-based authentication).
    In practice, other monitoring tools use JMX for monitoring, as well as managing
    the JVM, as JMX allows callbacks to manage the system state.
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: JSR 110是另一个**Java规范请求**（**JSR**），通常称为**Java管理扩展**（**JMX**）。JSR 110指定了一系列API和协议，以便能够远程监控JVM执行。访问JMX服务的一种常见方式是通过默认连接到本地进程之一的`jconsole`命令。要连接到远程主机，您需要在Java命令行上提供`-Dcom.sun.management.jmxremote.port=portNum`属性。还建议启用安全性（SSL或基于密码的认证）。在实践中，其他监控工具使用JMX进行监控，以及管理JVM，因为JMX允许回调来管理系统状态。
- en: You can provide your own metrics that are exposed via JMX. While Scala runs
    in JVM, the implementation of JMX (via MBeans) is very Java-specific, and it is
    not clear how well the mechanism will play with Scala. JMX Beans can certainly
    be exposed as a servlet in Scala though.
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过JMX公开自己的指标。虽然Scala在JVM中运行，但JMX（通过MBeans）的实现非常特定于Java，不清楚该机制与Scala的兼容性如何。尽管如此，JMX
    Beans可以在Scala中作为servlet公开。
- en: The JMX MBeans can usually be examined in JConsole, but we can also expose it
    as `/jmx servlet`, the code provided in the book repository ([https://github.com/alexvk/ml-in-scala](https://github.com/alexvk/ml-in-scala)).
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: JMX MBeans通常可以在JConsole中检查，但我们也可以将其公开为`/jmx servlet`，书中代码库中提供的代码（[https://github.com/alexvk/ml-in-scala](https://github.com/alexvk/ml-in-scala)）。
- en: Model monitoring
  id: totrans-1481
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 模型监控
- en: 'We have covered basic system and application metrics. Lately, a new direction
    evolved for using monitoring components to monitor statistical model performance.
    The statistical model performance covers the following:'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经涵盖了基本的系统和应用指标。最近，一个新的方向是利用监控组件来监控统计模型性能。统计模型性能包括以下内容：
- en: How the model performance evolved over time
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型性能随时间的变化
- en: When is the time to retire the model
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时退役模型
- en: Model health check
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型健康检查
- en: Performance over time
  id: totrans-1486
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 随时间推移的性能
- en: 'ML models deteriorate with time, or ''age'': While this process is not still
    well understood, the model performance tends to change with time, if even due
    to concept drift, where the definition of the attributes change, or the changes
    in the underlying dependencies. Unfortunately, model performance rarely improves,
    at least in my practice. Thus, it is imperative to keep track of models. One way
    to do this is by monitoring the metrics that the model is intended to optimize,
    as in many cases, we do not have a ready-labeled set of data.'
  id: totrans-1487
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型会随着时间的推移而退化，或者说“老化”：尽管这个过程还没有被充分理解，但模型性能往往会随时间变化，即使是因为概念漂移，即属性的定义发生变化，或底层依赖关系的变化。不幸的是，模型性能很少提高，至少在我的实践中是这样。因此，跟踪模型至关重要。一种方法是监控模型旨在优化的指标，因为在许多情况下，我们没有现成的标签数据集。
- en: In many cases, the model performance deterioration is not related directly to
    the quality of the statistical modeling, even though simpler models such as linear
    and logistic regression tend to be more stable than more complex models such as
    decision trees. Schema evolution or unnoticed renaming of attributes may cause
    the model to not perform well.
  id: totrans-1488
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，模型性能的下降并不是直接与统计建模的质量相关，尽管像线性回归和逻辑回归这样的简单模型通常比决策树等更复杂的模型更稳定。模式演变或未注意到的属性重命名可能导致模型表现不佳。
- en: Part of model monitoring should be running the health check, where a model periodically
    scores either a few records or a known scored set of data.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 模型监控的一部分应该是运行健康检查，其中模型定期对一些记录或已知评分的数据集进行评分。
- en: Criteria for model retiring
  id: totrans-1490
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型退役标准
- en: A very common case in practical deployments is that data scientists come with
    better sets of models every few weeks. However, if this does not happen, one needs
    come up with a set of criteria to retire a model. As real-world traffic rarely
    comes with the scored data, for example, the data that is already scored, the
    usual way to measure model performance is via a proxy, which is the metric that
    the model is supposed to improve.
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际部署中一个非常常见的案例是，数据科学家每隔几周就会带来更好的模型集。然而，如果这种情况没有发生，就需要制定一套标准来退役模型。由于现实世界的流量很少带有评分数据，例如，已经评分的数据，衡量模型性能的通常方式是通过代理，即模型应该改进的指标。
- en: A/B testing
  id: totrans-1492
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: A/B 测试
- en: 'A/B testing is a specific case of controlled experiment in e-commerce setting.
    A/B testing is usually applied to versions of a web page where we direct completely
    independent subset of users to each of the versions. The dependent variable to
    test is usually the response rate. Unless any specific information is available
    about users, and in many cases, it is not unless a cookie is placed in the computer,
    the split is random. Often the split is based on unique userID, but this is known
    not to work too well across multiple devices. A/B testing is subject to the same
    assumptions the controlled experiments are subject to: the tests should be completely
    independent and the distribution of the dependent variable should be `i.i.d.`.
    Even though it is hard to imagine that all people are truly `i.i.d.`, the A/B
    test has been shown to work for practical problems.'
  id: totrans-1493
  prefs: []
  type: TYPE_NORMAL
  zh: A/B 测试是电子商务环境中控制实验的一个特定案例。A/B 测试通常应用于网页的不同版本，我们将完全独立的用户子集引导到每个版本。要测试的因变量通常是响应率。除非有关于用户的特定信息，并且在许多情况下，除非在计算机上放置了cookie，否则这种分割通常是随机的。通常分割是基于唯一的userID，但已知这并不适用于多个设备。A/B
    测试受到与控制实验相同的假设：测试应该是完全独立的，因变量的分布应该是`i.i.d.`。尽管很难想象所有人都是真正的`i.i.d.`，但A/B 测试已被证明适用于实际问题。
- en: In modeling, we split the traffic to be scored into two or multiple channels
    to be scored by two or multiple models. Further, we need to measure the cumulative
    performance metric for each of the channels together with estimated variance.
    Usually, one of the models is treated as a baseline and is associated with the
    null hypothesis, and for the rest of the models, we run a t-test, comparing the
    ratio of the difference to the standard deviation.
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 在建模中，我们将要评分的流量分成两个或多个通道，由两个或多个模型进行评分。进一步地，我们需要测量每个通道的累积性能指标以及估计的方差。通常，其中一个模型被视为基线，并与零假设相关联，而对于其他模型，我们运行t检验，比较差异与标准差的比例。
- en: Summary
  id: totrans-1495
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This chapter described system, application, and model monitoring goals together
    with the existing monitoring solutions for Scala, and specifically Scalatra. Many
    metrics overlap with standard OS or Java monitoring, but we also discussed how
    to create application-specific metrics and health checks. We talked about a new
    emerging field of model monitoring in an ML application, where statistical models
    are subject to deterioration, health, and performance monitoring. I also touched
    on monitoring distributed systems, a topic that really deserves much more space,
    which unfortunately, I did not have.
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了系统、应用和模型监控目标，以及Scala和Scalatra现有的监控解决方案。许多指标与标准操作系统或Java监控重叠，但我们还讨论了如何创建特定于应用的指标和健康检查。我们讨论了机器学习应用中新兴的模型监控领域，其中统计模型受到退化、健康和性能监控的影响。我还简要提到了监控分布式系统，这是一个真正值得更多篇幅讨论的话题，但遗憾的是，我没有足够的空间来展开。
- en: This is the end of the book, but in no way is it the end of the journey. I am
    sure, new frameworks and applications are being written as we speak. Scala has
    been a pretty awesome and succinct development tool in my practice, with which
    I've been able to achieve results in hours instead of days, which is the case
    with more traditional tools, but it is yet to win the popular support, which I
    am pretty sure it. We just need to emphasize its advantages in the modern world
    of interactive analysis, complex data, and distributed processing.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的结尾，但绝对不是旅程的终点。我相信，当我们说话的时候，新的框架和应用正在被编写。在我的实践中，Scala已经是一个非常出色且简洁的开发工具，我能够用几个小时而不是几天就实现结果，这是更传统工具的情况，但它尚未赢得广泛的认可，我对此非常确信。我们只需要强调它在现代交互式分析、复杂数据和分布式处理世界中的优势。
