- en: Chapter 5. May JS Be with You! Control Your Browser with Motion
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章。愿JS与你同在！用运动控制你的浏览器
- en: Imagine how exciting it would be to be able to control your browser using neither
    the keyboard nor mouse. There are many fields in computer science that tend to
    create a good human interface. One of those fields is Computer Vision. It provides
    outstanding methods that can help you to create something useful rapidly and you
    do not even need devices such as Kinect! The human interface in Computer Vision
    highly depends on object tracking. In the previous chapter, we already saw some
    object tracking examples, such as Camshift. Here, we will introduce more algorithms
    to play with. First, we will start with the basic tracking algorithms, which do
    not have any assumptions about an object from a previous frame. Next, we will
    move on to **Head-coupled perspective**; this is a technique that uses the head
    (or eye) position to simulate a 3D environment on a screen. This will be covered
    by the headtrackr library, which we have seen in the previous chapter ([https://github.com/auduno/headtrackr](https://github.com/auduno/headtrackr)).
    Finally, we will move on to the optical flow algorithms, with the help of which
    you can track many different objects in your application and even create programs
    which can be controlled by gestures. We will create an interesting example that
    uses that type of control. Here, we will introduce a new library ([https://github.com/anvaka/oflow](https://github.com/anvaka/oflow)),
    which provides an excellent way to generate the optical flow of an image. To track
    multiple points at once, we will use the JSFeat ([http://inspirit.github.io/jsfeat/](http://inspirit.github.io/jsfeat/))
    library. Let's get started!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果能够不用键盘和鼠标就能控制你的浏览器，那将有多么激动人心。计算机科学中有许多领域倾向于创造良好的人机界面。其中之一是计算机视觉。它提供了出色的方法，可以帮助你快速创建有用的东西，而且你甚至不需要像Kinect这样的设备！计算机视觉中的人机界面高度依赖于对象跟踪。在前一章中，我们已经看到了一些对象跟踪的示例，例如Camshift。在这里，我们将介绍更多的算法来探索。首先，我们将从基本的跟踪算法开始，这些算法对前一帧中的对象没有任何假设。接下来，我们将转向**头部耦合透视**；这是一种使用头部（或眼睛）位置在屏幕上模拟3D环境的技术。这将由我们在前一章中提到的headtrackr库来完成（[https://github.com/auduno/headtrackr](https://github.com/auduno/headtrackr)）。最后，我们将转向光流算法，借助这些算法，你可以在你的应用程序中跟踪许多不同的对象，甚至创建可以通过手势控制的程序。我们将创建一个有趣的示例，展示这种类型的控制。在这里，我们将介绍一个新的库（[https://github.com/anvaka/oflow](https://github.com/anvaka/oflow)），它提供了一种生成图像光流的好方法。为了同时跟踪多个点，我们将使用JSFeat（[http://inspirit.github.io/jsfeat/](http://inspirit.github.io/jsfeat/)）库。让我们开始吧！
- en: 'We will cover the following topics in this chapter:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中将涵盖以下主题：
- en: Basic tracking with tracking.js
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用tracking.js进行基本跟踪
- en: Controlling objects with head motion
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过头部运动控制对象
- en: Optical flow for motion estimation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于运动估计的光流
- en: Basic tracking with tracking.js
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用tracking.js进行基本跟踪
- en: In this section, we will refresh our knowledge about object detection and create
    a sample project, which will show how object detection can be presented as object
    tracking. This is a relatively simple topic but, in some cases, it can outperform
    other methods. Remember that object detection deals with detecting instances of
    objects, while tracking deals with locating moving objects over time. If you have
    only one unique object and you assume it will still be unique on the next frames,
    then you can calculate its location over time. In that case, we do not need to
    worry about tracking techniques because the tracking can be done using object
    detection. Here, we will focus mostly on the tracking.js library ([http://trackingjs.com](http://trackingjs.com))
    since it provides the easiest way to do that.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将回顾关于对象检测的知识，并创建一个示例项目，展示如何将对象检测呈现为对象跟踪。这是一个相对简单的话题，但在某些情况下，它可能优于其他方法。记住，对象检测涉及检测对象的实例，而跟踪涉及随时间定位移动的对象。如果你只有一个独特的对象，并且你假设它在下一帧中仍然独特，那么你可以计算其随时间的位置。在这种情况下，我们不需要担心跟踪技术，因为跟踪可以使用对象检测来完成。在这里，我们将主要关注tracking.js库（[http://trackingjs.com](http://trackingjs.com)），因为它提供了最简单的方式来做到这一点。
- en: An example of an object tracking application
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 一个对象跟踪应用的示例
- en: We already mentioned the idea of using object detection as tracking. The idea
    is using a unique object on a scene. This can be a colored ball, your head, hand,
    or anything that has something special and can help to distinguish it from other
    objects in a scene. When you have that object, you just detect it on a frame and
    calculate its centroid to get its position.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经提到了使用物体检测作为跟踪的想法。这个想法是使用场景中的一个独特物体。这可能是一个彩色球，你的头部，手，或者任何有特殊之处并能帮助区分场景中其他物体的东西。当你拥有那个物体时，你只需在帧上检测它并计算其质心来获取其位置。
- en: To explore this concept, we will use the tracking.js library. We will draw a
    small ball with its center at the centroid of the detected object.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了探索这个概念，我们将使用tracking.js库。我们将绘制一个以检测到的物体质心为中心的小球。
- en: 'First, we will place the necessary tags for a video and the ball scene:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将放置视频和球场景所需的必要标签：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We need to get the ball''s context and its parameters to be able to draw on
    it:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要获取球体的上下文及其参数，以便能够在其上绘制：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The object we want to track is just a simple colored object, so we will use
    `ColorTracker`. To remove noise, we set the minimum dimensions of a detected object
    to `20`:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要跟踪的对象只是一个简单的彩色物体，因此我们将使用`ColorTracker`。为了去除噪声，我们将检测到的物体的最小尺寸设置为`20`：
- en: '[PRE2]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'When we detect an object, we need to clear the context that contains the ball.
    In addition, we take the first detected object and use it to move the ball to
    a new position:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们检测到一个物体时，我们需要清除包含球的上下文。此外，我们使用第一个检测到的物体将球移动到新的位置：
- en: '[PRE3]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The `move` function is defined in the following code:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '`move`函数定义在以下代码中：'
- en: '[PRE4]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, we take the center of a detected rectangle and draw a ball using
    it.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们取检测到的矩形的中心，并使用它来画一个球。
- en: 'To start the tracker, we just initiate it using the `track` function on the
    `<video>` tag:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动跟踪器，我们只需在`<video>`标签上使用`track`函数来初始化它：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Here, we did not place the code for displaying the tracked object on a video
    because we already covered it in [Chapter 3](cv-web_ch03.html#aid-MSDG1 "Chapter 3. Easy
    Object Detection for Everyone"), *Easy Object Detection for Everyone*.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们没有在视频上放置显示跟踪物体的代码，因为我们已经在[第3章](cv-web_ch03.html#aid-MSDG1 "第3章。面向所有人的简单物体检测")中介绍了它，*面向所有人的简单物体检测*。
- en: 'Here is what we get:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们的结果：
- en: '![An example of an object tracking application](img/image00129.jpeg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![一个物体跟踪应用的示例](img/image00129.jpeg)'
- en: The preceding image shows the detection of a bird in different frames and the
    estimated positions of the corresponding ball below the detections. As we can
    see, our main assumption about the object's (bird) uniqueness is observed. The
    only question is about the tracking—the bird's head is not perfectly detected.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示了在不同帧中检测到的鸟以及对应检测下方估计的球的位置。正如我们所见，我们对物体（鸟）独特性的主要假设得到了观察。唯一的问题是关于跟踪——鸟的头没有被完美地检测到。
- en: We saw the basic object detection, which is one step closer to tracking but
    it is not 100 percent. What we need to do is to remember the old coordinates of
    a previous object position and compute the motion vector. Let's move on!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了基本的物体检测，这使我们更接近跟踪，但还不是100%。我们需要做的是记住先前物体位置的旧坐标并计算运动向量。让我们继续前进！
- en: Controlling objects with the head motion
  id: totrans-29
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过头部运动控制物体
- en: Creating a human interface in Computer Vision is not an easy task. One of the
    exciting fields is Head-coupled perspective. This technique is used for rendering
    the scene on the screen, which responds naturally to changes in the head position
    of a viewer relative to the display. Simply put, the technology creates a 3D display
    without using any additional devices except the camera.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中创建一个用户界面不是一个容易的任务。其中一个令人兴奋的领域是头部耦合透视。这项技术用于在屏幕上渲染场景，它对观众相对于显示器的头部位置变化做出自然反应。简单来说，这项技术创建了一个3D显示，除了相机之外不需要任何额外的设备。
- en: In the previous chapter, we saw how to track a head with the headtrackr library.
    It was done using the Camshift algorithm. In this section, we will explain the
    background of the function for Head-coupled perspective and how to use it in your
    projects to create an amazing human interface. To present a scene, the headtrackr
    library uses one of the most popular JavaScript libraries for 3D modeling—three.js
    ([http://threejs.org](http://threejs.org)). We will begin with an explanation
    of the core function and then see an example of its usage.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们看到了如何使用headtrackr库跟踪头部。这是通过Camshift算法完成的。在本节中，我们将解释头部耦合视角函数的背景以及如何在项目中使用它来创建令人惊叹的人机界面。为了呈现场景，headtrackr库使用了最流行的JavaScript
    3D建模库之一——three.js（[http://threejs.org](http://threejs.org)）。我们将从解释核心功能开始，然后展示其使用示例。
- en: The Head-coupled perspective
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 头部耦合视角
- en: As we mentioned earlier, the headtrackr library works with three.js. The three.js
    library provides a clear API and exceptional functionality for 3D modeling. If
    you want, you can switch to another library, but in that case, you will need to
    rewrite some code from the headtrackr library.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前提到的，headtrackr库与three.js库一起工作。three.js库为3D建模提供了清晰的API和卓越的功能。如果您愿意，可以切换到另一个库，但那样的话，您将需要重写headtrackr库中的一些代码。
- en: The headtrackr library provides a good explanation of the whole algorithm; you
    can refer to it at [http://auduno.com/post/25125149521/head-tracking-with-webrtc](http://auduno.com/post/25125149521/head-tracking-with-webrtc).
    To help you better understand the whole process, and in case you want to modify
    the functionality of the head tracking or use other libraries for the 3D modeling,
    here we will focus on the code of the core function.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: headtrackr库提供了整个算法的良好解释；您可以在[http://auduno.com/post/25125149521/head-tracking-with-webrtc](http://auduno.com/post/25125149521/head-tracking-with-webrtc)中参考。为了帮助您更好地理解整个过程，以及如果您想修改头部跟踪的功能或使用其他库进行3D建模，这里我们将关注核心函数的代码。
- en: 'The function itself is called:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 函数本身被调用：
- en: '[PRE6]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To change the perspective of a scene appropriately, we need to know the movement
    of a head in three directions: *X*, *Y*, and *Z*. To do this, we need to assume
    some scene attributes. The core assumption this method makes is that, at the first
    initialization of the algorithm, the distance between the user who sits in front
    of the screen and the camera is 60cm. In addition to this, the method defines
    the screen height, which is 20cm by default. Using these parameters, we can find
    the **Field of View** (**fov**):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了适当地改变场景的视角，我们需要知道头部在三个方向上的运动：*X*、*Y*和*Z*。为此，我们需要假设一些场景属性。该方法的核心假设是，在算法的第一次初始化时，坐在屏幕前的用户与相机之间的距离是60厘米。除此之外，该方法还定义了屏幕高度，默认为20厘米。使用这些参数，我们可以找到**视场**（**fov**）：
- en: '![The Head-coupled perspective](img/image00130.jpeg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![头部耦合视角](img/image00130.jpeg)'
- en: In the scene, we will create a camera that will represent a user (your head);
    we will call it **camera on a scene**. Do not confuse it with the camera that
    is used to capture your face, for example, the laptop's camera.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在场景中，我们将创建一个代表用户（您的头部）的相机；我们将称之为**场景中的相机**。不要将其与用于捕获您脸部的相机混淆，例如，笔记本电脑的摄像头。
- en: The bigger the fov angle, the more objects fit on a screen and the further they
    appear to be. The Fov is computed in the first frame where a head is detected,
    taking into account previously mentioned assumptions.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 视场角越大，屏幕上能容纳的物体就越多，它们看起来也越远。视场是在检测到头部时计算的第一帧，考虑到了之前提到的假设。
- en: 'There are several parameters that the headtrackr function uses:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: headtrackr函数使用了一些参数：
- en: '**camera**: This is the `PerspectiveCamera` object from the three.js library.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**camera**: 这是来自three.js库的`PerspectiveCamera`对象。'
- en: '**scaling**: This is the vertical size of a screen in a 3D model. Basically,
    it scales the whole scene by the constant you define.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**scaling**: 这是3D模型中屏幕的垂直尺寸。基本上，它通过您定义的常数来缩放整个场景。'
- en: '**fixedPosition**: This is the initial position of a scene camera.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fixedPosition**: 这是场景相机的初始位置。'
- en: '**lookAt**: This is the position of the object you look at, which should be
    THREE.Vector3 which contains the 3 coordinates of an object.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**lookAt**: 这是您所看物体的位置，它应该是包含一个物体三个坐标的THREE.Vector3。'
- en: '**params**: This includes a `screenHeight` field in centimeters. This is an
    optional parameter and it defines the height of your monitor.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**params**: 这包括一个以厘米为单位的`screenHeight`字段。这是一个可选参数，它定义了您监视器的高度。'
- en: 'During the algorithm initialization, first, we set the scene camera position
    and the position of the object that this camera should point at:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在算法初始化期间，首先，我们设置场景摄像头的位置和这个摄像头应该指向的物体的位置：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, we define the screen width and height using the camera aspect ratio and
    scaling parameters:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用摄像头的纵横比和缩放参数定义屏幕宽度和高度：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'To get a head position in each frame, we need to add a listener to `headtrackingEvent`:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要在每一帧中获取头部位置，我们需要向`headtrackingEvent`添加一个监听器：
- en: '[PRE9]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The headtrackr library returns an estimated position of a head in each frame.
    The event contains the `x`, `y`, and `z` fields.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: headtrackr库在每个帧中返回一个头部位置的估计值。事件包含`x`、`y`和`z`字段。
- en: 'Since our camera represents our head position, to update its parameters, we
    need to change the position of the camera with respect to event data; do not forget
    about scaling:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的摄像头代表我们的头部位置，为了更新其参数，我们需要根据事件数据更改摄像头的位置；不要忘记缩放：
- en: '[PRE10]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You need to keep an object in the center of the screen. To do so, the method
    sets the view offset using the `setViewOffset` method. The first two parameters
    define the size of the whole view, the last four parameters are the parameters
    of a view rectangle:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要将一个物体保持在屏幕中央。为此，该方法使用`setViewOffset`方法设置视图偏移。前两个参数定义了整个视图的大小，最后四个参数是视图矩形的参数：
- en: '[PRE11]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The last attribute we want to update is the field of view, for which we use
    the `atan2` function. It returns the result from `-PI` to `PI` in radians; we
    need to convert it to degrees and multiply it by `2`, since we use only half of
    a screen in our computation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要更新的最后一个属性是视野，我们使用`atan2`函数来更新它。它以弧度从`-PI`到`PI`返回结果；我们需要将其转换为度数并乘以`2`，因为我们只使用了屏幕的一半来计算：
- en: '[PRE12]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After this, we update the camera parameters:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们更新摄像头参数：
- en: '[PRE13]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we saw, all we need to do is to work with the scene camera. If you want modify
    the code or use another library, it should not be a problem for you now.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，我们所需做的只是与场景摄像头一起工作。如果您想修改代码或使用另一个库，现在对您来说应该不是问题。
- en: Controlling a simple box
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 控制一个简单的盒子
- en: The example which is provided by the headtrackr library uses an old version
    of three.js but the good point is that it can be applied to new versions too!
    We will follow the cube example, which is available at [http://threejs.org/examples/#canvas_geometry_cube](http://threejs.org/examples/#canvas_geometry_cube).
    You can copy and paste the whole code from there; we will do only basic modifications.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: headtrackr库提供的示例使用的是three.js的老版本，但优点是它也可以应用于新版本！我们将遵循立方体示例，该示例可在[http://threejs.org/examples/#canvas_geometry_cube](http://threejs.org/examples/#canvas_geometry_cube)找到。您可以从那里复制并粘贴整个代码；我们只会进行基本的修改。
- en: 'First, you need to update the scripts section and add the headtrackr library:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要更新脚本部分并添加headtrackr库：
- en: '[PRE14]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'To track your face, you will need to define the video and canvas tags, which
    will be used by the headtrackr library:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟踪您的面部，您需要定义视频和画布标签，这些标签将被headtrackr库使用：
- en: '[PRE15]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Far plane of a scene camera in the three.js example is too close for us, we
    better set it a bit further away:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: three.js示例中的场景摄像头的远平面对我们来说太近了，我们最好将其设置得远一些：
- en: '[PRE16]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Initialization of the tracking process is done by the function that we reviewed
    in the previous section; take a look at the third parameter of a function below
    - camera position. The cube location in the example is `[0, 150, 0]` and its dimensions
    are 200 pixels. We set the camera initialization position at the cube plane:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪过程的初始化是通过我们在上一节中审查的函数完成的；查看下面函数的第三个参数——摄像头位置。示例中的立方体位置是`[0, 150, 0]`，其尺寸为200像素。我们将摄像头的初始化位置设置为立方体平面：
- en: '[PRE17]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we create a tracker:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个跟踪器：
- en: '[PRE18]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'In the previous section, we reviewed the parameters that can be used while
    you track a face. Now, let''s see what you can use for the head position estimation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们回顾了在跟踪面部时可以使用的参数。现在，让我们看看您可以用什么来估计头部位置：
- en: '**cameraOffset**: This is the distance from your laptop camera to the center
    of a screen, which is 11.5cm (half of the height of a regular laptop screen) by
    default.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**cameraOffset**：这是您的笔记本电脑摄像头到屏幕中央的距离，默认为11.5厘米（普通笔记本电脑屏幕高度的一半）。'
- en: '**fov**: This is the horizontal field of view used by the camera in degrees.
    By default, the algorithm automatically estimates this.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**fov**：这是摄像头在度数上使用的水平视野。默认情况下，算法会自动估计这个值。'
- en: 'Now, we get the video and canvas on the JavaScript side. Then, we get the initialization
    and start the tracker:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们在JavaScript侧获取视频和画布。然后，我们获取初始化并启动跟踪器：
- en: '[PRE19]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is a rough sketch of what you will see while using the application:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是您在使用应用程序时将看到的粗略草图：
- en: '![Controlling a simple box](img/image00131.jpeg)'
  id: totrans-81
  prefs: []
  type: TYPE_IMG
  zh: '![Controlling a simple box](img/image00131.jpeg)'
- en: Keep in mind that when you move your head to the left, the camera will display
    the movement to the right. You can move your head in any direction, the only issue
    with that method is that it does not calculate the position of your eyes and because
    of this, the image will not be as perfect as a 3D model. This can be solved using
    more advanced techniques that involve tracking eyes, but in that case, the performance
    will not be in real-time for JavaScript applications.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当您向左移动头部时，相机将显示向右的运动。您可以向任何方向移动头部，但这种方法唯一的问题是它不计算您眼睛的位置，因此图像不会像3D模型那样完美。这可以通过涉及跟踪眼睛的更高级技术来解决，但在这种情况下，JavaScript应用程序的性能将不会是实时的。
- en: Optical flow for motion estimation
  id: totrans-83
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 光流用于运动估计
- en: We saw how to track different objects in a scene and how to make a human interface
    using them, but we did not see a more generalized approach. When an object changes
    its position, it moves through a scene and it is interesting to estimate the overall
    movement of the scene. Here, we will introduce the concept of optical flow, and
    will see how to use it for object tracking. In the first part, we will focus on
    the theory and then present two wonderful examples of the optical flow usage.
    Finally, we will create a simple gesture application.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到了如何在场景中跟踪不同的物体，以及如何使用它们来制作人机界面，但我们没有看到一个更通用的方法。当一个物体改变其位置时，它会穿过场景，估计场景的整体运动是有趣的。在这里，我们将介绍光流的概念，并展示如何将其用于物体跟踪。在第一部分，我们将关注理论，然后展示两个光流应用的精彩示例。最后，我们将创建一个简单的手势应用程序。
- en: The Lucas-Kanade optical flow
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Lucas-Kanade光流
- en: 'There are many definitions of optical flow, the main one is: it is the change
    in structured intensities of an image due to relative motion between the eyeball
    (camera) and the scene ([http://www.scholarpedia.org/article/Optic_flow](http://www.scholarpedia.org/article/Optic_flow)).
    According to another definition, it is the distribution of the apparent velocities
    of objects in an image ([http://www.mathworks.com/discovery/optical-flow.html](http://www.mathworks.com/discovery/optical-flow.html)).
    To get the idea, look at the following image:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 光流有许多定义，主要的一个是：它是由于眼球（相机）与场景之间的相对运动而导致图像结构强度变化（[http://www.scholarpedia.org/article/Optic_flow](http://www.scholarpedia.org/article/Optic_flow)）。根据另一个定义，它是图像中物体表观速度的分布（[http://www.mathworks.com/discovery/optical-flow.html](http://www.mathworks.com/discovery/optical-flow.html)）。为了理解这个概念，请看以下图像：
- en: '![The Lucas-Kanade optical flow](img/image00132.jpeg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![The Lucas-Kanade optical flow](img/image00132.jpeg)'
- en: Here, we are just moving a box and the arrows in the third picture show this
    movement. Simply put, the optical flow shows the displacement of objects. It can
    be used not only for object tracking, but also for video compression and stabilization.
    Furthermore, you can get a structure of a scene using the optical flow. For example,
    if you record a still environment with a moving camera, the objects that are closer
    to the camera change their destination faster than objects that are far from the
    camera.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只是在移动一个方框，第三张图中的箭头显示了这种移动。简单来说，光流显示了物体的位移。它不仅可用于物体跟踪，还可用于视频压缩和稳定。此外，您还可以使用光流获取场景的结构。例如，如果您用移动的相机记录一个静态环境，靠近相机的物体比远离相机的物体改变目标位置的速度更快。
- en: The optical flow can be computed in many ways. The basic assumption of optical
    flow algorithms is that the object intensities of neighboring frames do not change
    rapidly. The most popular method is the Lucas-Kanade method. In addition to the
    previous assumption, it states that the displacement of objects in nearby frames
    is not large. Moreover, the method takes an *NxN* patch, typically 3 x 3, around
    each pixel and it assumes that the motion of all these pixels is the same. Using
    these assumptions and the knowledge of changes in pixel intensities (gradient)
    around each pixel of the patch, the algorithm calculates its displacement. The
    changes in intensities are computed in *x* and *y* dimensions and, in time. Here,
    by time, we mean the difference between the previous and a current frame.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 光流可以通过多种方式计算。光流算法的基本假设是相邻帧中的对象强度不会迅速变化。最流行的方法是Lucas-Kanade方法。除了之前的假设外，它还指出，在附近的帧中对象的位移不会很大。此外，该方法在每个像素周围取一个*NxN*的补丁，通常是3x3，并假设所有这些像素的运动是相同的。利用这些假设以及补丁周围每个像素的像素强度变化（梯度）的知识，算法计算其位移。强度变化在*x*和*y*维度以及时间上计算。在这里，我们所说的“时间”是指前一帧和当前帧之间的差异。
- en: It's only a 3x 3 patch? What should we do with the fast moving objects? This
    problem can be solved using image pyramids. In this case, we will downsample an
    image and look for the same movement as that of a 3 x 3 patch, but at a lower
    resolution.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个3x3的补丁吗？我们应该如何处理快速移动的物体？这个问题可以通过图像金字塔来解决。在这种情况下，我们将对图像进行下采样，并寻找与3x3补丁相同的运动，但分辨率更低。
- en: The other improvement is the iterative Lucas-Kanade method. After getting a
    flow vector for each pixel, we move pixels by those vectors and try to match the
    previous and current images. In an ideal situation, those images would be matched,
    but with real videos there might be errors due to changes in pixels brightness.
    To avoid an error, we reiterate the process before we get a small error or exceed
    the maximum number of iterations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个改进是迭代Lucas-Kanade方法。在为每个像素获取光流向量之后，我们通过这些向量移动像素，并尝试匹配前一帧和当前帧。在理想情况下，这些图像将被匹配，但鉴于真实视频，由于像素亮度的变化，可能会有错误。为了避免错误，我们在得到小的错误或超过最大迭代次数之前重复这个过程。
- en: We discussed the theoretical part, now let's move to the two amazing libraries
    that can provide the implementation of the optical flow. They can be used for
    different purposes. The core of both libraries is the Lucas-Kanade method.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论了理论部分，现在让我们转向两个可以提供光流实现功能的惊人库。它们可以用作不同的目的。这两个库的核心是Lucas-Kanade方法。
- en: Optical flow map with oflow
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 光流地图 with oflow
- en: We start with a small library—oflow ([https://github.com/anvaka/oflow](https://github.com/anvaka/oflow)).
    This is a simple library that just calculates displacement vectors of each patch
    and returns the overall movement of a scene. We will use this movement to control
    the ball that we already used in this chapter. Unfortunately, the library does
    not use an image pyramid to calculate optical flow and because of that, it is
    better suited for getting the whole scene movement than object tracking.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个小的库——oflow ([https://github.com/anvaka/oflow](https://github.com/anvaka/oflow))——开始。这是一个简单的库，它只计算每个补丁的位移向量，并返回场景的整体运动。我们将使用这个运动来控制我们已经在本章中使用的球体。不幸的是，这个库不使用图像金字塔来计算光流，因此它更适合获取整个场景的运动而不是对象跟踪。
- en: 'We start by defining the library in our project:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先在我们的项目中定义库：
- en: '[PRE20]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As we did previously, we create a video input and a ball canvas. In addition
    to this, we add a canvas for displaying the optical flow map with `id=''flow''`:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所做的那样，我们创建了一个视频输入和一个球体画布。除此之外，我们添加了一个用于显示光流地图的画布，其`id='flow'`：
- en: '[PRE21]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we define an object that will be used for optical flow calculation. You
    can create it not only for a video as shown here, but also for a web camera (`WebFlow`)
    and a canvas (`CanvasFlow`). The `zoneSize` variable defines the half dimension
    of a patch, which is set to `8` by default:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个用于光流计算的物体。您不仅可以为如这里所示的视频创建它，还可以为网络摄像头（`WebFlow`）和画布（`CanvasFlow`）创建它。`zoneSize`变量定义了补丁的半维度，默认设置为`8`：
- en: '[PRE22]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Here is a short example of what we receive in the end—the video on the left
    and the directions of optical flow on the right:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个我们最终收到的简短示例——左边的视频和右边的光流方向：
- en: '![Optical flow map with oflow](img/image00133.jpeg)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![光流地图 with oflow](img/image00133.jpeg)'
- en: Each arrow shows the movement direction of a patch, of course, there is some
    noise, but most of the directions show the correct result. How do we receive the
    result?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 每个箭头显示一个补丁的运动方向，当然，有一些噪声，但大多数方向显示的是正确的结果。我们如何接收结果？
- en: 'After the computation is done for each frame, we need to handle the result.
    In the following, we receive directions for each patch. Then, we draw arrows that
    point to the direction of a patch displacement; we multiply that displacement
    by four so we can see the result in a better manner. You can choose any other
    multiplier, since it is used only for displaying the optical flow and not for
    the actual calculation:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个帧的计算完成后，我们需要处理结果。在以下内容中，我们为每个补丁接收方向。然后，我们画箭头指向补丁位移的方向；我们将该位移乘以四，以便更好地查看结果。你可以选择任何其他乘数，因为它仅用于显示光流，而不是实际计算：
- en: '[PRE23]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'To calculate the overall displacement, the library just adds together all the
    vectors. Using the common vector, we draw a ball on its context. If the ball exceeds
    the screen dimensions, we draw it on the opposite side. The overall direction
    is returned using the `u` and `v` fields of the result:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算整体位移，库只是将所有向量相加。使用公共向量，我们在其上下文中画一个球。如果球超出屏幕尺寸，我们就在对面画它。整体方向是通过结果的`u`和`v`字段返回的：
- en: '[PRE24]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'To start the computational process, just call the following function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始计算过程，只需调用以下函数：
- en: '[PRE25]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'After several frames of the video, we get the following result:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频的几帧之后，我们得到以下结果：
- en: '![Optical flow map with oflow](img/image00134.jpeg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![Optical flow map with oflow](img/image00134.jpeg)'
- en: 'This is where the ball placement is in the first and last frames, respectively:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在第一帧和最后一帧中球的位置，分别：
- en: '![Optical flow map with oflow](img/image00135.jpeg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![Optical flow map with oflow](img/image00135.jpeg)'
- en: Using this library, it is easy to create a simple game that is controlled by
    gestures, for example. The library usage is not limited to that and you can create
    something different very fast.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个库，很容易创建一个由手势控制的简单游戏，例如。库的使用不仅限于这一点，你可以非常快地创建出不同的东西。
- en: Track points with JSFeat
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用JSFeat跟踪点
- en: The JSFeat library extends the functionality of optical flow and it can even
    track image points. You can use those points to track objects and control your
    browser. The implementation of optical flow in JSFeat library uses the iterative
    Lucas-Kanade method with pyramids and the result it provides is very smooth.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: JSFeat库扩展了光流的功能，甚至可以跟踪图像点。你可以使用这些点来跟踪对象和控制浏览器。JSFeat库中光流的实现使用带有金字塔的迭代Lucas-Kanade方法，它提供的结果非常平滑。
- en: 'In JSFeat, to work with a video, we need to include an additional JavaScript
    file, which is provided by this library:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在JSFeat中，要处理视频，我们需要包含一个额外的JavaScript文件，该文件由这个库提供：
- en: '[PRE26]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, we define a video to be processed and a canvas for displaying the content:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义要处理的视频和用于显示内容的画布：
- en: '[PRE27]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'There are a lot of variables that need to be defined:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 需要定义很多变量：
- en: '[PRE28]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'The last row, from left to right shows: current image pyramid, image pyramid
    from the previous level, number of points that are tracked, and status of points.
    The status indicates whether a point has its representation on a new frame; if
    there is no such point, then the method assumes that the tracking of this point
    was lost and it is removed from the tracking process. The last two variables contain
    the point coordinates of the previous and current frames.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最右边的一行，从左到右显示：当前图像金字塔、上一级的图像金字塔、跟踪点的数量和点的状态。状态表示一个点是否在新的帧上有其表示；如果没有这样的点，则该方法假定该点的跟踪已丢失，并将其从跟踪过程中删除。最后两个变量包含上一帧和当前帧的点的坐标。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'We do not provide functions to select original points here, but you can see
    them in the JSFeat example:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里不提供选择原始点的函数，但你可以在JSFeat示例中看到它们：
- en: '[https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html](https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html).'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html](https://github.com/inspirit/jsfeat/blob/gh-pages/sample_oflow_lk.html).'
- en: 'The following function initializes all necessary variables:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数初始化所有必要的变量：
- en: '[PRE29]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To start getting video frames, we need to call the following function:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始获取视频帧，我们需要调用以下函数：
- en: '[PRE30]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'It uses the `process` function to work with each video frame:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 它使用`process`函数处理每个视频帧：
- en: '[PRE31]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We copy the points and pyramid variables from the previous frame with the `curr_`
    prefix to variables with the `prev_` prefix:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`curr_`前缀将点和平面变量从前一帧复制到带有`prev_`前缀的变量中：
- en: '[PRE32]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we compute an image pyramid for the current frame:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们为当前帧计算一个图像金字塔：
- en: '[PRE33]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'To call a function for optical flow, we introduce four more variables that
    this function needs. The first variable is `win_size`, which is the size of a
    patch for searching the flow vector; the `max_iter` variable is the maximum number
    of iterations; the `eps` variable is the algorithm, which stops updating a point
    when the movement is less than `eps`; and the `min_eigen_threshold` variable,
    which is the threshold for removing bad points. The `process` function computes
    new point coordinates on a new frame. After this, we call the `prune_oflow_points`
    function. If you continue a video, you can probably loose some points on a future
    frame. In that case, they will not be tracked anymore and will be removed from
    the `curr_xy` variable by this function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 要调用光流函数，我们引入了四个该函数需要的额外变量。第一个变量是`win_size`，它是搜索流向量的补丁大小；`max_iter`变量是最大迭代次数；`eps`变量是当移动小于`eps`时停止更新点的算法；`min_eigen_threshold`变量是移除坏点的阈值。`process`函数在新的帧上计算新的点坐标。之后，我们调用`prune_oflow_points`函数。如果您继续视频，您可能会在未来帧上丢失一些点。在这种情况下，它们将不再被跟踪，并且将被此函数从`curr_xy`变量中移除：
- en: '[PRE34]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Here is the result we received:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们得到的结果：
- en: '![Track points with JSFeat](img/image00136.jpeg)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![使用JSFeat跟踪点](img/image00136.jpeg)'
- en: As we can see, all points were successfully tracked; this is an ideal example,
    where the object was moving smoothly. In many cases, points can be tracked correctly,
    especially if a video is not edited.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有点都成功跟踪了；这是一个理想的例子，其中对象移动得非常平滑。在许多情况下，点可以被正确跟踪，尤其是如果视频没有被编辑的话。
- en: The functionality of the library provides an excellent opportunity to track
    an object by defining its unique points, for example, you can predefine those
    points using FAST corner detection from [Chapter 3](cv-web_ch03.html#aid-MSDG1
    "Chapter 3. Easy Object Detection for Everyone"), *Easy Object Detection for Everyone*.
    In addition to that, you can stabilize a video in real time and do other amazing
    things with it.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 库的功能提供了通过定义其独特点来跟踪对象的绝佳机会，例如，您可以使用[第3章](cv-web_ch03.html#aid-MSDG1 "第3章。人人都能轻松进行目标检测")中的FAST角点检测预先定义这些点，*人人都能轻松进行目标检测*。除此之外，您还可以实时稳定视频，并使用它进行其他令人惊叹的事情。
- en: Zooming with gestures
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 手势缩放
- en: What if we want to extend the functionality a bit? Suppose we want to add a
    simple zooming feature to our website. We can use the optical flow for that.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想稍微扩展一下功能呢？假设我们想在网站上添加一个简单的缩放功能。我们可以使用光流来实现这一点。
- en: 'To start, we create our content tag we want to zoom with the following style:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用以下样式创建我们想要缩放的标签内容：
- en: '[PRE35]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'To use a webcam with JSFeat, run the following function from compatibility.js,
    which simply initializes your camera:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用JSFeat与Webcam，从兼容性.js中运行以下函数，该函数简单地初始化您的摄像头：
- en: '[PRE36]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'For zooming, we need only two points. So, after receiving the result from the
    optical flow algorithm, we check whether there are two points and if so, we call
    the zoom method:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于缩放，我们只需要两个点。因此，在接收到光流算法的结果后，我们检查是否有两个点，如果有，我们就调用缩放方法：
- en: '[PRE37]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The method itself is very simple. We save the original size and change it based
    on the information we receive from the points of optical flow. We check the distance
    between two points and if it changed, we change the `<div>` content with respect
    to it:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该方法本身非常简单。我们保存原始大小，并根据从光流点接收到的信息进行更改。我们检查两点之间的距离，如果它发生了变化，我们就根据它更改`<div>`内容：
- en: '[PRE38]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Here is an example of zooming, where we used the Canny edge detector in addition
    to the whole process:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个缩放的例子，我们在整个过程中使用了Canny边缘检测器：
- en: '![Zooming with gestures](img/image00137.jpeg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![手势缩放](img/image00137.jpeg)'
- en: There is no function to find your fingers on a video, so you need to select
    them before using the zoom function. If you want to find them by yourself, it
    is all in your hands! Probably, you could create a new era in web browser user
    experience.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 没有函数可以在视频中找到您的手指，因此您在使用缩放功能之前需要选择它们。如果您想自己找到它们，那就全凭您了！也许您能创造一个全新的网络浏览器用户体验时代。
- en: Summary
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This is the last chapter of this book. Here, we saw examples that aggregate
    many techniques from the previous chapters. We covered object detection and tracking
    by color, which you can use to create your first tracking application very quickly.
    We explored the power of Head-coupled perspective, which is a new way to present
    the content on your websites in a fresh manner or create funny browser games with
    a human interface. In addition to that, the optical flow provides many extensions
    to this field too. It provides you with an excellent way to track points and objects.
    Moreover, now you can create a simple application that uses gestures to zoom the
    web content. The usage of optical flow is not limited to that and is very flexible,
    and it can be combined with many techniques.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这是这本书的最后一章。在这里，我们看到了汇集了前几章许多技术的例子。我们介绍了通过颜色进行目标检测和跟踪的方法，这可以帮助你非常快速地创建你的第一个跟踪应用程序。我们探索了头部耦合视角的力量，这是一种以新颖的方式展示你网站内容或创建带有人类界面的有趣浏览器游戏的新方法。除此之外，光流也为这个领域提供了许多扩展。它为你提供了跟踪点和对象的绝佳方式。此外，现在你可以创建一个简单的应用程序，使用手势来缩放网页内容。光流的使用不仅限于这些，而且非常灵活，它可以与许多技术相结合。
