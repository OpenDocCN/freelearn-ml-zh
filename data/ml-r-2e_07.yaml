- en: Chapter 7. Black Box Methods – Neural Networks and Support Vector Machines
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章 黑箱方法——神经网络与支持向量机
- en: The late science fiction author Arthur C. Clarke wrote, "any sufficiently advanced
    technology is indistinguishable from magic."This chapter covers a pair of machine
    learning methods that may appear at first glance to be magic. Though they are
    extremely powerful, their inner workings can be difficult to understand.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 已故科幻作家阿瑟·C·克拉克曾写道：“任何足够先进的技术都无法与魔法区分开。”本章涵盖了一对初看起来像魔法的机器学习方法。尽管它们极其强大，但其内部原理可能难以理解。
- en: In engineering, these are referred to as **black box** processes because the
    mechanism that transforms the input into the output is obfuscated by an imaginary
    box. For instance, the black box of closed-source software intentionally conceals
    proprietary algorithms, the black box of political lawmaking is rooted in the
    bureaucratic processes, and the black box of sausage-making involves a bit of
    purposeful (but tasty) ignorance. In the case of machine learning, the black box
    is due to the complex mathematics allowing them to function.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在工程学中，这些被称为**黑箱**过程，因为将输入转化为输出的机制被一个虚拟的盒子所遮掩。例如，闭源软件的黑箱故意隐藏专有算法，政治立法的黑箱根植于官僚流程，而香肠制造的黑箱则涉及一些故意的（但美味的）无知。在机器学习的情况下，黑箱则源于其运作所依赖的复杂数学。
- en: 'Although they may not be easy to understand, it is dangerous to apply black
    box models blindly. Thus, in this chapter, we''ll peek inside the box and investigate
    the statistical sausage-making involved in fitting such models. You''ll discover:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然它们可能不易理解，但盲目应用黑箱模型是危险的。因此，在本章中，我们将一窥黑箱内部，并调查拟合此类模型所涉及的统计香肠制作过程。你将发现：
- en: Neural networks mimic the structure of animal brains to model arbitrary functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络模仿动物大脑的结构来模拟任意函数
- en: Support vector machines use multidimensional surfaces to define the relationship
    between features and outcomes
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持向量机使用多维表面来定义特征与结果之间的关系
- en: Despite their complexity, these can be applied easily to real-world problems
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尽管它们的复杂性，依然可以轻松应用于现实世界问题
- en: With any luck, you'll realize that you don't need a black belt in statistics
    to tackle black box machine's learning methods—there's no need to be intimidated!
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如果幸运的话，你会意识到，解决黑箱机器学习方法并不需要统计学的黑带——完全没有必要感到畏惧！
- en: Understanding neural networks
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 了解神经网络
- en: An **Artificial Neural Network** (**ANN**) models the relationship between a
    set of input signals and an output signal using a model derived from our understanding
    of how a biological brain responds to stimuli from sensory inputs. Just as a brain
    uses a network of interconnected cells called **neurons** to create a massive
    parallel processor, ANN uses a network of artificial neurons or **nodes** to solve
    learning problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**人工神经网络**（**ANN**）使用一种基于我们对生物大脑如何响应来自感官输入的刺激的理解所衍生的模型，来模拟输入信号与输出信号之间的关系。就像大脑通过一组叫做**神经元**的相互连接的细胞构建一个巨大的并行处理器，ANN则利用一组人工神经元或**节点**来解决学习问题。'
- en: The human brain is made up of about 85 billion neurons, resulting in a network
    capable of representing a tremendous amount of knowledge. As you might expect,
    this dwarfs the brains of other living creatures. For instance, a cat has roughly
    a billion neurons, a mouse has about 75 million neurons, and a cockroach has only
    about a million neurons. In contrast, many ANNs contain far fewer neurons, typically
    only several hundred, so we're in no danger of creating an artificial brain anytime
    in the near future—even a fruit fly brain with 100,000 neurons far exceeds the
    current state-of-the-art ANN.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 人脑由约850亿个神经元组成，形成了一个能够表示大量知识的网络。正如你所预料的，这一数量远远超过其他生物的脑量。例如，一只猫大约有10亿个神经元，一只老鼠大约有7500万个神经元，而一只蟑螂只有大约100万个神经元。相比之下，许多ANN包含的神经元要少得多，通常只有几百个，因此我们目前离制造人工大脑还远——即便是一个拥有10万个神经元的果蝇大脑，也远远超出了现有的最先进ANN的能力。
- en: Though it may be unfeasible to completely model a cockroach's brain, a neural
    network may still provide an adequate heuristic model of its behavior. Suppose
    that we develop an algorithm that can mimic how a roach flees when discovered.
    If the behavior of the robot roach is convincing, does it matter whether its brain
    is as sophisticated as the living creature's? This question is the basis of the
    controversial **Turing test**, proposed in 1950 by the pioneering computer scientist
    Alan Turing, proposed in 1950 by the pioneering computer scientist Alan Turing,
    which grades a machine as intelligent if a human being cannot distinguish its
    behavior from a living creature's.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然完全模拟一只蟑螂的大脑可能不可行，但神经网络仍然可以提供一个足够的启发式模型来模拟其行为。假设我们开发了一个算法，能够模仿蟑螂在被发现时逃跑的反应。如果机器人蟑螂的行为让人信服，那么它的大脑是否与活体生物一样复杂重要吗？这个问题正是有争议的**图灵测试**的基础。图灵测试由开创性计算机科学家艾伦·图灵于1950年提出，旨在通过判断一个人类是否无法将机器的行为与活体生物区分开，来评定机器是否具备智能。
- en: 'Rudimentary ANNs have been used for over 50 years to simulate the brain''s
    approach to problem-solving. At first, this involved learning simple functions
    like the logical AND function or the logical OR function. These early exercises
    were used primarily to help scientists understand how biological brains might
    operate. However, as computers have become increasingly powerful in the recent
    years, the complexity of ANNs has likewise increased so much that they are now
    frequently applied to more practical problems including:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 基本的人工神经网络（ANNs）已经被使用超过50年，用来模拟大脑解决问题的方法。最初，这包括学习简单的函数，比如逻辑与（AND）函数或逻辑或（OR）函数。这些早期的练习主要是为了帮助科学家理解生物大脑如何运作。然而，随着近年来计算机性能的不断增强，人工神经网络的复杂性也大幅增加，以至于现在它们常常被应用于更实际的问题，包括：
- en: Speech and handwriting recognition programs like those used by voicemail transcription
    services and postal mail sorting machines
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 语音和手写识别程序，例如语音邮件转录服务和邮政邮件分拣机所使用的技术
- en: The automation of smart devices like an office building's environmental controls
    or self-driving cars and self-piloting drones
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能设备的自动化，如办公楼的环境控制、自动驾驶汽车和自驾无人机
- en: Sophisticated models of weather and climate patterns, tensile strength, fluid
    dynamics, and many other scientific, social, or economic phenomena
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精密的天气和气候模式、抗拉强度、流体动力学以及许多其他科学、社会或经济现象的模型
- en: 'Broadly speaking, ANNs are versatile learners that can be applied to nearly
    any learning task: classification, numeric prediction, and even unsupervised pattern
    recognition.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 广义来说，人工神经网络是多才多艺的学习者，几乎可以应用于任何学习任务：分类、数值预测，甚至是无监督的模式识别。
- en: Tip
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Whether deserving or not, ANN learners are often reported in the media with
    great fanfare. For instance, an "*artificial brain*" developed by Google was recently
    touted for its ability to identify cat videos on YouTube. Such hype may have less
    to do with anything unique to ANNs and more to do with the fact that ANNs are
    captivating because of their similarities to living minds.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是否值得，人工神经网络学习器常常在媒体中被大肆宣传。例如，谷歌最近开发的“*人工大脑*”因其能够识别YouTube上的猫视频而受到推崇。这种炒作可能与人工神经网络的独特性关系不大，而更多地与人工神经网络因其与活体思维的相似性而引人入胜有关。
- en: ANNs are best applied to problems where the input data and output data are well-defined
    or at least fairly simple, yet the process that relates the input to output is
    extremely complex. As a black box method, they work well for these types of black
    box problems.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络最适用于那些输入数据和输出数据定义明确或至少相对简单，但输入与输出之间的过程极其复杂的问题。作为一种黑箱方法，它们在这类黑箱问题中表现优异。
- en: From biological to artificial neurons
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从生物神经元到人工神经元
- en: Because ANNs were intentionally designed as conceptual models of human brain
    activity, it is helpful to first understand how biological neurons function. As
    illustrated in the following figure, incoming signals are received by the cell's
    **dendrites** through a biochemical process. The process allows the impulse to
    be weighted according to its relative importance or frequency. As the **cell body**
    begins accumulating the incoming signals, a threshold is reached at which the
    cell fires and the output signal is transmitted via an electrochemical process
    down the **axon**. At the axon's terminals, the electric signal is again processed
    as a chemical signal to be passed to the neighboring neurons across a tiny gap
    known as a **synapse**.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 因为人工神经网络（ANNs）是故意设计为人类大脑活动的概念模型，因此首先理解生物神经元的功能是很有帮助的。如以下图所示，细胞的**树突**通过生化过程接收传入的信号。这个过程使得冲动信号可以根据其相对重要性或频率进行加权。当**细胞体**开始积累传入的信号时，会达到一个阈值，在此阈值下细胞会激发，并通过电化学过程将输出信号沿着**轴突**传递。在轴突的末端，电信号再次被处理为化学信号，并通过一个被称为**突触**的小间隙传递给相邻的神经元。
- en: '![From biological to artificial neurons](img/3905_07_01.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![从生物神经元到人工神经元](img/3905_07_01.jpg)'
- en: 'The model of a single artificial neuron can be understood in terms very similar
    to the biological model. As depicted in the following figure, a directed network
    diagram defines a relationship between the input signals received by the dendrites
    (*x* variables), and the output signal (*y* variable). Just as with the biological
    neuron, each dendrite''s signal is weighted (*w* values) according to its importance—ignore,
    for now, how these weights are determined. The input signals are summed by the
    cell body and the signal is passed on according to an **activation function**
    denoted by *f*:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 单个人工神经元的模型可以用非常类似于生物模型的方式理解。如以下图所示，一个有向网络图定义了输入信号（由树突接收的 *x* 变量）和输出信号（*y* 变量）之间的关系。就像生物神经元一样，每个树突的信号根据其重要性被加权（*w*
    值）——暂时忽略这些权重是如何确定的。输入信号被细胞体汇总，并根据**激活函数** *f* 传递信号：
- en: '![From biological to artificial neurons](img/3905_07_02.jpg)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![从生物神经元到人工神经元](img/3905_07_02.jpg)'
- en: 'A typical artificial neuron with *n* input dendrites can be represented by
    the formula that follows. The *w* weights allow each of the *n* inputs (denoted
    by *x[i]*) to contribute a greater or lesser amount to the sum of input signals.
    The net total is used by the activation function *f(x)*, and the resulting signal,
    *y(x)*, is the output axon:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的人工神经元有*n*个输入树突，可以通过以下公式表示。*w* 权重使得每个*n*个输入（用*x[i]*表示）能够对输入信号的总和贡献更多或更少。总和会被激活函数
    *f(x)* 使用，结果信号 *y(x)* 就是输出轴突的信号：
- en: '![From biological to artificial neurons](img/3905_07_03.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![从生物神经元到人工神经元](img/3905_07_03.jpg)'
- en: 'Neural networks use neurons defined this way as building blocks to construct
    complex models of data. Although there are numerous variants of neural networks,
    each can be defined in terms of the following characteristics:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络使用这样定义的神经元作为构建块来构建复杂的数据模型。尽管神经网络有许多变种，但每个变种都可以通过以下特征来定义：
- en: An **activation function**, which transforms a neuron's combined input signals
    into a single output signal to be broadcasted further in the network
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**激活函数**，它将神经元的综合输入信号转换为一个单一的输出信号，然后将其进一步广播到网络中'
- en: A **network topology** (or architecture), which describes the number of neurons
    in the model as well as the number of layers and manner in which they are connected
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网络拓扑**（或架构），它描述了模型中神经元的数量、层数以及它们如何连接的方式'
- en: The **training algorithm** that specifies how connection weights are set in
    order to inhibit or excite neurons in proportion to the input signal
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练算法**，它指定了如何设置连接权重，以便根据输入信号来抑制或激发神经元'
- en: Let's take a look at some of the variations within each of these categories
    to see how they can be used to construct typical neural network models.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下每个类别内的一些变化，看看它们如何被用来构建典型的神经网络模型。
- en: Activation functions
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 激活函数
- en: The activation function is the mechanism by which the artificial neuron processes
    incoming information and passes it throughout the network. Just as the artificial
    neuron is modeled after the biological version, so is the activation function
    modeled after nature's design.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是人工神经元处理传入信息并将其传递到整个网络的机制。就像人工神经元是基于生物神经元模型的，激活函数也是基于自然界的设计模型的。
- en: In the biological case, the activation function could be imagined as a process
    that involves summing the total input signal and determining whether it meets
    the firing threshold. If so, the neuron passes on the signal; otherwise, it does
    nothing. In ANN terms, this is known as a **threshold activation function**, as
    it results in an output signal only once a specified input threshold has been
    attained.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在生物学的情况下，激活函数可以被想象为一个过程，涉及将所有输入信号加总并确定是否达到触发阈值。如果是，神经元将传递信号；否则，它什么也不做。在人工神经网络（ANN）术语中，这被称为**阈值激活函数**，因为它只有在达到指定的输入阈值后才会产生输出信号。
- en: The following figure depicts a typical threshold function; in this case, the
    neuron fires when the sum of input signals is at least zero. Because its shape
    resembles a stair, it is sometimes called a **unit step activation function**.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图显示了一个典型的阈值函数；在这种情况下，当输入信号的总和至少为零时，神经元触发。因为它的形状类似楼梯，有时被称为**单位阶跃激活函数**。
- en: '![Activation functions](img/3905_07_04.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/3905_07_04.jpg)'
- en: Although the threshold activation function is interesting due to its parallels
    with biology, it is rarely used in artificial neural networks. Freed from the
    limitations of biochemistry, the ANN activation functions can be chosen based
    on their ability to demonstrate desirable mathematical characteristics and accurately
    model relationships among data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管阈值激活函数因其与生物学的相似性而具有趣味性，但在人工神经网络中很少使用。摆脱了生物化学的限制，ANN的激活函数可以根据它们展示期望的数学特性和准确建模数据之间关系的能力来选择。
- en: Perhaps the most commonly used alternative is the **sigmoid activation function**
    (more specifically, the *logistic* sigmoid) shown in the following figure. Note
    that in the formula shown, *e* is the base of the natural logarithm (approximately
    2.72). Although it shares a similar step or "S" shape with the threshold activation
    function, the output signal is no longer binary; output values can fall anywhere
    in the range from 0 to 1\. Additionally, the sigmoid is **differentiable**, which
    means that it is possible to calculate the derivative across the entire range
    of inputs. As you will learn later, this feature is crucial to create efficient
    ANN optimization algorithms.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 或许最常用的替代函数是**sigmoid激活函数**（更具体来说，是*logistic* sigmoid），如以下图所示。请注意，在公式中，*e*是自然对数的底数（约为2.72）。尽管它与阈值激活函数共享类似的阶梯或“S”形状，但输出信号不再是二进制的；输出值可以落在0到1的范围内。此外，sigmoid是**可微分**的，这意味着可以计算整个输入范围内的导数。如你稍后所学，这一特性对于创建高效的ANN优化算法至关重要。
- en: '![Activation functions](img/3905_07_05.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/3905_07_05.jpg)'
- en: 'Although sigmoid is perhaps the most commonly used activation function and
    is often used by default, some neural network algorithms allow a choice of alternatives.
    A selection of such activation functions is shown in the following figure:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管sigmoid可能是最常用的激活函数，且通常作为默认选项使用，但一些神经网络算法允许选择其他替代函数。以下图展示了这种激活函数的选择：
- en: '![Activation functions](img/3905_07_06.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![激活函数](img/3905_07_06.jpg)'
- en: The primary detail that differentiates these activation functions is the output
    signal range. Typically, this is one of (0, 1), (-1, +1), or (-inf, +inf). The
    choice of activation function biases the neural network such that it may fit certain
    types of data more appropriately, allowing the construction of specialized neural
    networks. For instance, a linear activation function results in a neural network
    very similar to a linear regression model, while a Gaussian activation function
    results in a model called a **Radial Basis Function** (**RBF**) network. Each
    of these has strengths better suited for certain learning tasks and not others.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 区分这些激活函数的主要细节是输出信号的范围。通常，这个范围是（0，1）、（-1，+1）或（-∞，+∞）。激活函数的选择会影响神经网络的偏置，使其能够更适合某些类型的数据，从而构建专门的神经网络。例如，线性激活函数会使神经网络非常类似于线性回归模型，而高斯激活函数则会导致一个称为**径向基函数**（**RBF**）网络的模型。每个模型都有其更适合某些学习任务的优点。
- en: It's important to recognize that for many of the activation functions, the range
    of input values that affect the output signal is relatively narrow. For example,
    in the case of sigmoid, the output signal is always nearly 0 or 1 for an input
    signal below *-5* or above *+5*, respectively. The compression of signal in this
    way results in a saturated signal at the high and low ends of very dynamic inputs,
    just as turning a guitar amplifier up too high results in a distorted sound due
    to clipping of the peaks of sound waves. Because this essentially squeezes the
    input values into a smaller range of outputs, activation functions like the sigmoid
    are sometimes called **squashing functions**.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要认识到，对于许多激活函数来说，影响输出信号的输入值范围相对较窄。例如，在Sigmoid函数的情况下，当输入信号低于*-5*或高于*+5*时，输出信号始终接近0或1。这种信号压缩会导致在高低端出现饱和信号，就像将吉他放大器音量调得过高导致声音失真一样，因为音波的峰值被削波。由于这会将输入值压缩到更小的输出范围，像Sigmoid这样的激活函数有时被称为**压缩函数**。
- en: The solution to the squashing problem is to transform all neural network inputs
    such that the features' values fall within a small range around 0\. Typically,
    this involves standardizing or normalizing the features. By restricting the range
    of input values, the activation function will have action across the entire range,
    preventing large-valued features such as household income from dominating small-valued
    features such as the number of children in the household. A side benefit is that
    the model may also be faster to train, since the algorithm can iterate more quickly
    through the actionable range of input values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 解决压缩问题的方法是对所有神经网络输入进行变换，使得特征的值落在接近0的一个小范围内。通常，这涉及对特征进行标准化或归一化。通过限制输入值的范围，激活函数将在整个范围内起作用，从而防止像家庭收入这样的高值特征主导像家庭中孩子数量这样的低值特征。一个额外的好处是，模型的训练速度可能更快，因为算法可以更快速地在有效的输入值范围内进行迭代。
- en: Tip
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Although theoretically a neural network can adapt to a very dynamic feature
    by adjusting its weight over many iterations. In extreme cases, many algorithms
    will stop iterating long before this occurs. If your model is making predictions
    that do not make sense, double-check whether you've correctly standardized the
    input data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管理论上神经网络可以通过多次迭代调整其权重来适应非常动态的特征，但在极端情况下，许多算法会在此之前停止迭代。如果你的模型预测结果不合理，请仔细检查是否正确标准化了输入数据。
- en: Network topology
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 网络拓扑
- en: 'The ability of a neural network to learn is rooted in its **topology**, or
    the patterns and structures of interconnected neurons. Although there are countless
    forms of network architecture, they can be differentiated by three key characteristics:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的学习能力根植于其**拓扑结构**，即互联神经元的模式和结构。尽管网络架构形式多种多样，但它们可以通过三个关键特征来区分：
- en: The number of layers
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层数
- en: Whether information in the network is allowed to travel backward
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中的信息是否允许反向传播
- en: The number of nodes within each layer of the network
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络中每一层的节点数量
- en: The topology determines the complexity of tasks that can be learned by the network.
    Generally, larger and more complex networks are capable of identifying more subtle
    patterns and complex decision boundaries. However, the power of a network is not
    only a function of the network size, but also the way units are arranged.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑结构决定了网络能够学习的任务复杂性。通常，更大且更复杂的网络能够识别更微妙的模式和复杂的决策边界。然而，网络的能力不仅取决于网络的大小，还与单元的排列方式有关。
- en: The number of layers
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 层数
- en: To define topology, we need a terminology that distinguishes artificial neurons
    based on their position in the network. The figure that follows illustrates the
    topology of a very simple network. A set of neurons called **input nodes** receives
    unprocessed signals directly from the input data. Each input node is responsible
    for processing a single feature in the dataset; the feature's value will be transformed
    by the corresponding node's activation function. The signals sent by the input
    nodes are received by the output node, which uses its own activation function
    to generate a final prediction (denoted here as *p*).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 为了定义拓扑结构，我们需要一种术语来区分基于其在网络中位置的人工神经元。接下来的图示例了一个非常简单网络的拓扑结构。一组被称为**输入节点**的神经元直接接收来自输入数据的未处理信号。每个输入节点负责处理数据集中一个单独的特征；该特征的值将通过对应节点的激活函数进行转换。输入节点发送的信号被输出节点接收，输出节点利用自己的激活函数生成最终预测（此处表示为*p*）。
- en: The input and output nodes are arranged in groups known as **layers**. Because
    the input nodes process the incoming data exactly as it is received, the network
    has only one set of connection weights (labeled here as *w[1]*, *w[2]*, and *w[3]*).
    It is therefore termed a **single-layer network**. Single-layer networks can be
    used for basic pattern classification, particularly for patterns that are linearly
    separable, but more sophisticated networks are required for most learning tasks.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 输入节点和输出节点按**层**的形式分组排列。由于输入节点以接收到的数据的原始形式进行处理，因此网络只有一组连接权重（此处标记为*w[1]*，*w[2]*，和*w[3]*）。因此，它被称为**单层网络**。单层网络可以用于基本的模式分类，尤其是对于线性可分的模式，但大多数学习任务需要更复杂的网络。
- en: '![The number of layers](img/3905_07_07.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![层数](img/3905_07_07.jpg)'
- en: As you might expect, an obvious way to create more complex networks is by adding
    additional layers. As depicted here, a **multilayer network** adds one or more
    **hidden layers** that process the signals from the input nodes prior to it reaching
    the output node. Most multilayer networks are **fully connected**, which means
    that every node in one layer is connected to every node in the next layer, but
    this is not required.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你可能预期的那样，创建更复杂网络的一种明显方法是通过增加额外的层。如图所示，**多层网络**增加了一个或多个**隐藏层**，它们在信号到达输出节点之前处理来自输入节点的信号。大多数多层网络是**全连接**的，这意味着一个层中的每个节点都与下一层中的每个节点相连，但这并不是必须的。
- en: '![The number of layers](img/3905_07_08.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![层数](img/3905_07_08.jpg)'
- en: The direction of information travel
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 信息流动的方向
- en: You may have noticed that in the prior examples, arrowheads were used to indicate
    signals traveling in only one direction. Networks in which the input signal is
    fed continuously in one direction from connection to connection until it reaches
    the output layer are called **feedforward** networks.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在之前的例子中，箭头用来表示信号仅朝一个方向流动。那些输入信号从一个连接到另一个连接，持续朝一个方向传输直到到达输出层的网络被称为**前馈网络**。
- en: In spite of the restriction on information flow, feedforward networks offer
    a surprising amount of flexibility. For instance, the number of levels and nodes
    at each level can be varied, multiple outcomes can be modeled simultaneously,
    or multiple hidden layers can be applied. A neural network with multiple hidden
    layers is called a **Deep Neural Network** (**DNN**) and the practice of training
    such network is sometimes referred to as **deep learning**.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管存在信息流动的限制，前馈网络仍然提供了令人惊讶的灵活性。例如，可以改变每一层的节点数和层数，可以同时建模多个结果，或可以应用多个隐藏层。具有多个隐藏层的神经网络被称为**深度神经网络**（**DNN**），而训练这种网络的实践有时被称为**深度学习**。
- en: '![The direction of information travel](img/B03905_07_09.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![信息流动的方向](img/B03905_07_09.jpg)'
- en: 'In contrast, a **recurrent network** (or **feedback network**) allows signals
    to travel in both directions using loops. This property, which more closely mirrors
    how a biological neural network works, allows extremely complex patterns to be
    learned. The addition of a short-term memory, or **delay**, increases the power
    of recurrent networks immensely. Notably, this includes the capability to understand
    the sequences of events over a period of time. This could be used for stock market
    prediction, speech comprehension, or weather forecasting. A simple recurrent network
    is depicted as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，**递归网络**（或**反馈网络**）允许信号通过循环在两个方向上传播。这一特性，更加接近生物神经网络的工作方式，使得学习极为复杂的模式成为可能。加入短期记忆或**延迟**，极大增强了递归网络的能力。特别是，这使得网络能够理解一段时间内的事件序列。这可以用于股市预测、语音理解或天气预报等应用。一个简单的递归网络如下所示：
- en: '![The direction of information travel](img/3905_07_10.jpg)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![信息传递方向](img/3905_07_10.jpg)'
- en: In spite of their potential, recurrent networks are still largely theoretical
    and are rarely used in practice. On the other hand, feedforward networks have
    been extensively applied to real-world problems. In fact, the multilayer feedforward
    network, sometimes called the **Multilayer Perceptron** (**MLP**), is the de facto
    standard ANN topology. If someone mentions that they are fitting a neural network,
    they are most likely referring to a MLP.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管具有潜力，递归网络仍然主要是理论性的，实际中很少使用。另一方面，前馈网络已广泛应用于现实问题中。实际上，多层前馈网络，有时被称为**多层感知器**（**MLP**），是事实上的标准人工神经网络拓扑结构。如果有人提到他们正在拟合神经网络，那么他们很可能是在指MLP。
- en: The number of nodes in each layer
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 每层的节点数量
- en: In addition to the variations in the number of layers and the direction of information
    travel, neural networks can also vary in complexity by the number of nodes in
    each layer. The number of input nodes is predetermined by the number of features
    in the input data. Similarly, the number of output nodes is predetermined by the
    number of outcomes to be modeled or the number of class levels in the outcome.
    However, the number of hidden nodes is left to the user to decide prior to training
    the model.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 除了层数和信息传递方向的变化外，神经网络的复杂性还可以通过每层的节点数量来变化。输入节点的数量由输入数据中的特征数量预先确定。类似地，输出节点的数量由要建模的结果数量或结果中的类别层次数预先确定。然而，隐藏层节点的数量则由用户在训练模型之前决定。
- en: Unfortunately, there is no reliable rule to determine the number of neurons
    in the hidden layer. The appropriate number depends on the number of input nodes,
    the amount of training data, the amount of noisy data, and the complexity of the
    learning task, among many other factors.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，没有可靠的规则来确定隐藏层中的神经元数量。合适的数量取决于输入节点的数量、训练数据的数量、噪声数据的数量以及学习任务的复杂性等许多因素。
- en: In general, more complex network topologies with a greater number of network
    connections allow the learning of more complex problems. A greater number of neurons
    will result in a model that more closely mirrors the training data, but this runs
    a risk of overfitting; it may generalize poorly to future data. Large neural networks
    can also be computationally expensive and slow to train.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，具有更多网络连接的复杂网络拓扑结构可以学习更复杂的问题。更多的神经元将导致一个更贴近训练数据的模型，但这也带来了过拟合的风险；它可能在未来数据上表现不佳。大型神经网络还可能在计算上非常昂贵，并且训练速度较慢。
- en: The best practice is to use the fewest nodes that result in adequate performance
    in a validation dataset. In most cases, even with only a small number of hidden
    nodes—often as few as a handful—the neural network can offer a tremendous amount
    of learning ability.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 最佳做法是使用最少的节点，以在验证数据集上获得足够的性能。在大多数情况下，即使只有少量的隐藏节点——通常只有几个——神经网络也能提供极大的学习能力。
- en: Tip
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: It has been proven that a neural network with at least one hidden layer of sufficient
    neurons is a **universal function approximator**. This means that neural networks
    can be used to approximate any continuous function to an arbitrary precision over
    a finite interval.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 已经证明，至少具有一个隐藏层且神经元足够的神经网络是**通用函数逼近器**。这意味着神经网络可以用于在有限区间内，以任意精度逼近任何连续函数。
- en: Training neural networks with backpropagation
  id: totrans-73
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用反向传播训练神经网络
- en: The network topology is a blank slate that by itself has not learned anything.
    Like a newborn child, it must be trained with experience. As the neural network
    processes the input data, connections between the neurons are strengthened or
    weakened, similar to how a baby's brain develops as he or she experiences the
    environment. The network's connection weights are adjusted to reflect the patterns
    observed over time.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑结构本身是空白的，尚未学习到任何内容。就像一个新生儿，它必须通过经验来训练。当神经网络处理输入数据时，神经元之间的连接会被加强或削弱，类似于婴儿大脑在经历环境时的发育过程。网络的连接权重会根据观察到的模式随时间调整。
- en: Training a neural network by adjusting connection weights is very computationally
    intensive. Consequently, though they had been studied for decades prior, ANNs
    were rarely applied to real-world learning tasks until the mid-to-late 1980s,
    when an efficient method of training an ANN was discovered. The algorithm, which
    used a strategy of back-propagating errors, is now known simply as **backpropagation**.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整连接权重训练神经网络非常耗费计算资源。因此，尽管人工神经网络（ANNs）在此之前已被研究了几十年，但直到1980年代中后期，当一种高效的ANN训练方法被发现时，ANN才被应用于现实世界的学习任务。这种算法通过反向传播误差的策略训练网络，现在通常被称为**反向传播（backpropagation）**。
- en: Note
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'Coincidentally, several research teams independently discovered and published
    the backpropagation algorithm around the same time. Among them, perhaps the most
    often cited work is: Rumelhart DE, Hinton GE, Williams RJ. Learning representations
    by back-propagating errors. *Nature*. 1986; 323:533-566.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 巧合的是，多个研究团队在大致相同的时间独立地发现并发布了反向传播算法。其中，最常被引用的工作之一是：Rumelhart DE, Hinton GE, Williams
    RJ. 通过反向传播误差学习表示。*自然*杂志，1986年；323：533-566。
- en: 'Although still notoriously slow relative to many other machine learning algorithms,
    the backpropagation method led to a resurgence of interest in ANNs. As a result,
    multilayer feedforward networks that use the backpropagation algorithm are now
    common in the field of data mining. Such models offer the following strengths
    and weaknesses:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管相对于许多其他机器学习算法仍然非常缓慢，反向传播方法却促使了对人工神经网络（ANNs）的重新关注。因此，使用反向传播算法的多层前馈网络现在在数据挖掘领域中非常常见。这些模型具有以下优点和缺点：
- en: '| Strengths | Weaknesses |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 劣势 |'
- en: '| --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can be adapted to classification or numeric prediction problems
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以适用于分类或数值预测问题
- en: Capable of modeling more complex patterns than nearly any algorithm
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够建模比几乎任何其他算法更复杂的模式
- en: Makes few assumptions about the data's underlying relationships
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数据的潜在关系假设较少
- en: '|'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Extremely computationally intensive and slow to train, particularly if the network
    topology is complex
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算量极大且训练速度慢，特别是当网络拓扑结构复杂时
- en: Very prone to overfitting training data
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非常容易导致过拟合训练数据
- en: Results in a complex black box model that is difficult, if not impossible, to
    interpret
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果是一个复杂的黑箱模型，难以理解，甚至不可能理解。
- en: '|'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 'In its most general form, the backpropagation algorithm iterates through many
    cycles of two processes. Each cycle is known as an **epoch**. Because the network
    contains no *a priori* (existing) knowledge, the starting weights are typically
    set at random. Then, the algorithm iterates through the processes, until a stopping
    criterion is reached. Each epoch in the backpropagation algorithm includes:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在其最一般的形式中，反向传播算法通过两个过程的多个循环进行迭代。每个循环被称为**一个周期（epoch）**。因为网络不包含任何*先验*（已有的）知识，所以起始权重通常是随机设置的。然后，算法在两个过程中迭代，直到达到停止标准。反向传播算法中的每个周期包括：
- en: A **forward phase** in which the neurons are activated in sequence from the
    input layer to the output layer, applying each neuron's weights and activation
    function along the way. Upon reaching the final layer, an output signal is produced.
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**前向阶段**，在该阶段，神经元从输入层到输出层依次激活，并在过程中应用每个神经元的权重和激活函数。当达到最后一层时，会生成一个输出信号。
- en: A **backward phase** in which the network's output signal resulting from the
    forward phase is compared to the true target value in the training data. The difference
    between the network's output signal and the true value results in an error that
    is propagated backwards in the network to modify the connection weights between
    neurons and reduce future errors.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**后向阶段**，在该阶段，网络在前向阶段产生的输出信号与训练数据中的真实目标值进行比较。网络输出信号与真实值之间的差异导致一个误差，该误差向后传播至网络，以修改神经元之间的连接权重，减少未来的误差。
- en: 'Over time, the network uses the information sent backward to reduce the total
    error of the network. Yet one question remains: because the relationship between
    each neuron''s inputs and outputs is complex, how does the algorithm determine
    how much a weight should be changed? The answer to this question involves a technique
    called **gradient descent**. Conceptually, it works similarly to how an explorer
    trapped in the jungle might find a path to water. By examining the terrain and
    continually walking in the direction with the greatest downward slope, the explorer
    will eventually reach the lowest valley, which is likely to be a riverbed.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 随着时间的推移，网络会利用向后传播的信息来减少网络的总误差。然而，仍然有一个问题：由于每个神经元的输入和输出之间的关系是复杂的，算法如何确定权重应变化多少？这个问题的答案涉及到一个名为**梯度下降**的技术。从概念上讲，它的工作原理类似于被困在丛林中的探险者如何找到通往水源的路径。通过检查地形并不断朝着最大下坡的方向前进，探险者最终会到达最低的山谷，这里很可能就是河床。
- en: In a similar process, the backpropagation algorithm uses the derivative of each
    neuron's activation function to identify the gradient in the direction of each
    of the incoming weights—hence the importance of having a differentiable activation
    function. The gradient suggests how steeply the error will be reduced or increased
    for a change in the weight. The algorithm will attempt to change the weights that
    result in the greatest reduction in error by an amount known as the **learning
    rate**. The greater the learning rate, the faster the algorithm will attempt to
    descend down the gradients, which could reduce the training time at the risk of
    overshooting the valley.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在类似的过程中，反向传播算法使用每个神经元激活函数的导数来识别各输入权重方向上的梯度——因此，具有可微分激活函数非常重要。梯度指示了权重变化时，误差会如何陡峭地增加或减少。该算法将尝试通过一个称为**学习率**的量来改变权重，以实现误差的最大减少。学习率越大，算法越快地沿着梯度下降，这可以减少训练时间，但也可能有超越最低点的风险。
- en: '![Training neural networks with backpropagation](img/B03905_07_11.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![使用反向传播训练神经网络](img/B03905_07_11.jpg)'
- en: Although this process seems complex, it is easy to apply in practice. Let's
    apply our understanding of multilayer feedforward networks to a real-world problem.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这个过程看起来复杂，但在实际应用中却很容易操作。我们将运用对多层前馈网络的理解来解决一个现实问题。
- en: Example – Modeling the strength of concrete with ANNs
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用ANNs建模混凝土强度
- en: In the field of engineering, it is crucial to have accurate estimates of the
    performance of building materials. These estimates are required in order to develop
    safety guidelines governing the materials used in the construction of buildings,
    bridges, and roadways.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在工程领域，准确估算建筑材料的性能至关重要。这些估算对于制定建筑物、桥梁和道路施工材料的安全指南是必要的。
- en: Estimating the strength of concrete is a challenge of particular interest. Although
    it is used in nearly every construction project, concrete performance varies greatly
    due to a wide variety of ingredients that interact in complex ways. As a result,
    it is difficult to accurately predict the strength of the final product. A model
    that could reliably predict concrete strength given a listing of the composition
    of the input materials could result in safer construction practices.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 估算混凝土的强度是一个特别具有挑战性的任务。尽管混凝土几乎在每个建筑项目中都有使用，但其性能因多种不同的成分以及复杂的相互作用而存在很大差异。因此，很难准确预测最终产品的强度。一个能够根据输入材料的组成来可靠预测混凝土强度的模型，可能会导致更安全的建筑实践。
- en: Step 1 – collecting data
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第一步 – 收集数据
- en: For this analysis, we will utilize data on the compressive strength of concrete
    donated to the UCI Machine Learning Data Repository ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml))
    by I-Cheng Yeh. As he found success using neural networks to model these data,
    we will attempt to replicate his work using a simple neural network model in R.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本分析，我们将使用由叶怡诚提供给UCI机器学习数据集库的混凝土抗压强度数据（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）。由于他成功地利用神经网络建模这些数据，我们也将尝试使用简单的神经网络模型在R中复制他的工作。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'For more information on Yeh''s approach to this learning task, refer to: Yeh
    IC. Modeling of strength of high performance concrete using artificial neural
    networks. *Cement and Concrete Research*. 1998; 28:1797-1808.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 欲了解叶氏方法在此学习任务中的应用，请参阅：叶怡诚. 使用人工神经网络建模高性能混凝土的强度. *水泥与混凝土研究*. 1998; 28:1797-1808.
- en: According to the website, the concrete dataset contains 1,030 examples of concrete
    with eight features describing the components used in the mixture. These features
    are thought to be related to the final compressive strength and they include the
    amount (in kilograms per cubic meter) of cement, slag, ash, water, superplasticizer,
    coarse aggregate, and fine aggregate used in the product in addition to the aging
    time (measured in days).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 根据网站信息，混凝土数据集包含1,030个混凝土样本，这些样本有八个特征，描述了混合物中所用成分的情况。这些特征被认为与最终的抗压强度有关，包含了水泥、矿渣、粉煤灰、水、减水剂、粗骨料和细骨料的数量（单位为千克每立方米），以及养护时间（以天为单位）。
- en: Tip
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To follow along with this example, download the `concrete.csv` file from the
    Packt Publishing website and save it to your R working directory.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随这个示例，请从Packt Publishing网站下载`concrete.csv`文件，并将其保存到您的R工作目录中。
- en: Step 2 – exploring and preparing the data
  id: totrans-107
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步 – 探索和准备数据
- en: 'As usual, we''ll begin our analysis by loading the data into an R object using
    the `read.csv()` function, and confirming that it matches the expected structure:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们将通过使用`read.csv()`函数将数据加载到R对象中，并确认它符合预期的结构：
- en: '[PRE0]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The nine variables in the data frame correspond to the eight features and one
    outcome we expected, although a problem has become apparent. Neural networks work
    best when the input data are scaled to a narrow range around zero, and here, we
    see values ranging anywhere from zero up to over a thousand.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框中的九个变量对应着我们预期的八个特征和一个结果，尽管一个问题已经显现出来。神经网络在输入数据缩放到接近零的狭窄范围时表现最佳，而在这里，我们看到的值从零到一千多不等。
- en: Typically, the solution to this problem is to rescale the data with a normalizing
    or standardization function. If the data follow a bell-shaped curve (a normal
    distribution as described in [Chapter 2](ch02.html "Chapter 2. Managing and Understanding
    Data"), *Managing and Understanding Data*), then it may make sense to use standardization
    via R's built-in scale() function. On the other hand, if the data follow a uniform
    distribution or are severely nonnormal, then normalization to a 0-1 range may
    be more appropriate. In this case, we'll use the latter.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，解决这个问题的方法是使用归一化或标准化函数对数据进行重新缩放。如果数据遵循钟形曲线（如[第2章](ch02.html "第2章 管理和理解数据")，*管理和理解数据*中所描述的正态分布），那么使用R的内置`scale()`函数进行标准化可能是有意义的。另一方面，如果数据呈均匀分布或严重偏离正态分布，则可能更适合将数据归一化到0-1范围。在这种情况下，我们将使用后者。
- en: 'In [Chapter 3](ch03.html "Chapter 3. Lazy Learning – Classification Using Nearest
    Neighbors"), *Lazy Learning – Classification Using Nearest Neighbors*, we defined
    our own `normalize()` function as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第3章](ch03.html "第3章 懒学习——使用最近邻进行分类")，*懒学习——使用最近邻进行分类*中，我们定义了自己的`normalize()`函数，如下所示：
- en: '[PRE1]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After executing this code, our `normalize()` function can be applied to every
    column in the concrete data frame using the `lapply()` function as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码后，我们的`normalize()`函数可以使用`lapply()`函数应用于混凝土数据框的每一列，如下所示：
- en: '[PRE2]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'To confirm that the normalization worked, we can see that the minimum and maximum
    strength are now 0 and 1, respectively:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确认归一化操作已生效，我们可以看到，最小强度和最大强度现在分别为0和1：
- en: '[PRE3]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'In comparison, the original minimum and maximum values were 2.33 and 82.60:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，原始的最小值和最大值分别为2.33和82.60：
- en: '[PRE4]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Tip
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Any transformation applied to the data prior to training the model will have
    to be applied in reverse later on, in order to convert back to the original units
    of measurement. To facilitate the rescaling, it is wise to save the original data
    or at least the summary statistics of the original data.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型之前对数据应用的任何转换，之后都必须逆向应用，以便转换回原始的计量单位。为了方便重新缩放，最好保存原始数据，或者至少保存原始数据的汇总统计信息。
- en: 'Following Yeh''s precedent in the original publication, we will partition the
    data into a training set with 75 percent of the examples and a testing set with
    25 percent. The CSV file we used was already sorted in random order, so we simply
    need to divide it into two portions:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Yeh在原始出版物中的做法，我们将数据分为训练集和测试集，训练集占75%的样本，测试集占25%。我们使用的CSV文件已经是随机排序的，因此我们只需要将其分成两部分：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We'll use the training dataset to build the neural network and the testing dataset
    to evaluate how well the model generalizes to future results. As it is easy to
    overfit a neural network, this step is very important.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用训练数据集来构建神经网络，并使用测试数据集评估模型如何将结果推广到未来。由于神经网络容易发生过拟合，因此这个步骤非常重要。
- en: Step 3 – training a model on the data
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步 – 在数据上训练模型
- en: To model the relationship between the ingredients used in concrete and the strength
    of the finished product, we will use a multilayer feedforward neural network.
    The `neuralnet` package by Stefan Fritsch and Frauke Guenther provides a standard
    and easy-to-use implementation of such networks. It also offers a function to
    plot the network topology. For these reasons, the `neuralnet` implementation is
    a strong choice for learning more about neural networks, though this is not to
    say that it cannot be used to accomplish real work as well—it's quite a powerful
    tool, as you will soon see.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了建模混凝土中所用原料与成品强度之间的关系，我们将使用一个多层前馈神经网络。Stefan Fritsch和Frauke Guenther的`neuralnet`包提供了这样网络的标准且易于使用的实现。它还提供了一个函数用于绘制网络拓扑。因此，`neuralnet`的实现是学习神经网络的一个强有力选择，尽管这并不意味着它不能用于完成实际工作——它是一个非常强大的工具，正如你很快会看到的那样。
- en: Tip
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: There are several other commonly used packages to train ANN models in R, each
    with unique strengths and weaknesses. Because it ships as a part of the standard
    R installation, the `nnet` package is perhaps the most frequently cited ANN implementation.
    It uses a slightly more sophisticated algorithm than standard backpropagation.
    Another strong option is the `RSNNS` package, which offers a complete suite of
    neural network functionality with the downside being that it is more difficult
    to learn.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 还有几个其他常用的包可以用来训练R中的ANN模型，每个包都有独特的优缺点。由于它作为R标准安装的一部分，`nnet`包可能是引用最多的ANN实现。它使用的算法比标准的反向传播算法稍微复杂一些。另一个强大的选择是`RSNNS`包，它提供了完整的神经网络功能，但缺点是它更难学习。
- en: 'As `neuralnet` is not included in base R, you will need to install it by typing
    `install.packages("neuralnet")` and load it with the `library(neuralnet)` command.
    The included `neuralnet()` function can be used for training neural networks for
    numeric prediction using the following syntax:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`neuralnet`不包含在基础R中，你需要通过输入`install.packages("neuralnet")`来安装它，并使用`library(neuralnet)`命令加载它。所包含的`neuralnet()`函数可以用于训练用于数值预测的神经网络，语法如下：
- en: '![Step 3 – training a model on the data](img/3905_07_12.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![第三步 – 在数据上训练模型](img/3905_07_12.jpg)'
- en: 'We''ll begin by training the simplest multilayer feedforward network with only
    a single hidden node:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从训练最简单的多层前馈网络开始，只有一个隐藏节点：
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can then visualize the network topology using the `plot()` function on the
    resulting model object:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以使用结果模型对象上的`plot()`函数来可视化网络拓扑：
- en: '[PRE7]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Step 3 – training a model on the data](img/3905_07_13.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![第三步 – 在数据上训练模型](img/3905_07_13.jpg)'
- en: In this simple model, there is one input node for each of the eight features,
    followed by a single hidden node and a single output node that predicts the concrete
    strength. The weights for each of the connections are also depicted, as are the
    **bias terms** (indicated by the nodes labeled with the number **1**). The bias
    terms are numeric constants that allow the value at the indicated nodes to be
    shifted upward or downward, much like the intercept in a linear equation.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简单模型中，每个八个特征都有一个输入节点，接着是一个隐藏节点和一个输出节点，输出节点预测混凝土强度。每个连接的权重也被描绘出来，**偏置项**（由标记为**1**的节点表示）也被显示。偏置项是数值常数，它允许指示节点的值上下移动，类似于线性方程中的截距。
- en: Tip
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: A neural network with a single hidden node can be thought of as a distant cousin
    of the linear regression models we studied in [Chapter 6](ch06.html "Chapter 6. Forecasting
    Numeric Data – Regression Methods"), *Forecasting Numeric Data – Regression Methods*.
    The weight between each input node and the hidden node is similar to the regression
    coefficients, and the weight for the bias term is similar to the intercept.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有一个隐藏节点的神经网络可以被看作是我们在[第6章](ch06.html "第6章. 预测数值数据 – 回归方法")中学习的线性回归模型的远亲，*预测数值数据
    – 回归方法*。每个输入节点和隐藏节点之间的权重类似于回归系数，而偏置项的权重类似于截距。
- en: At the bottom of the figure, R reports the number of training steps and an error
    measure called the **Sum of Squared Errors** (**SSE**), which as you might expect,
    is the sum of the squared predicted minus actual values. A lower SSE implies better
    predictive performance. This is helpful for estimating the model's performance
    on the training data, but tells us little about how it will perform on unseen
    data.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的底部，R报告了训练步骤的数量和一个名为**平方误差和**（**SSE**）的误差度量，正如你所预期的那样，它是预测值减去实际值的平方和。较低的SSE意味着更好的预测性能。这有助于估计模型在训练数据上的表现，但不能告诉我们它在未见数据上的表现。
- en: Step 4 – evaluating model performance
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: 'The network topology diagram gives us a peek into the black box of the ANN,
    but it doesn''t provide much information about how well the model fits future
    data. To generate predictions on the test dataset, we can use the `compute()`
    as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 网络拓扑图为我们提供了一窥人工神经网络黑匣子的视角，但并未提供关于模型如何拟合未来数据的详细信息。要在测试数据集上生成预测，我们可以使用如下的`compute()`函数：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The `compute()` function works a bit differently from the `predict()` functions
    we''ve used so far. It returns a list with two components: `$neurons`, which stores
    the neurons for each layer in the network, and `$net.result`, which stores the
    predicted values. We''ll want the latter:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`compute()`函数的工作方式与我们迄今使用的`predict()`函数有所不同。它返回一个包含两个组件的列表：`$neurons`，用于存储网络中每层的神经元，以及`$net.result`，用于存储预测值。我们将需要后者：'
- en: '[PRE9]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Because this is a numeric prediction problem rather than a classification problem,
    we cannot use a confusion matrix to examine model accuracy. Instead, we must measure
    the correlation between our predicted concrete strength and the true value. This
    provides insight into the strength of the linear association between the two variables.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一个数值预测问题，而不是分类问题，我们不能使用混淆矩阵来检查模型的准确性。相反，我们必须衡量我们预测的混凝土强度与真实值之间的相关性。这提供了两个变量之间线性关联强度的洞察。
- en: 'Recall that the `cor()` function is used to obtain a correlation between two
    numeric vectors:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，`cor()`函数用于获取两个数值向量之间的相关性：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Tip
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Don't be alarmed if your result differs. Because the neural network begins with
    random weights, the predictions can vary from model to model. If you'd like to
    match these results exactly, try using `set.seed(12345)` before building the neural
    network.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的结果不同，不要惊慌。由于神经网络以随机权重开始，预测结果可能因模型而异。如果你想完全匹配这些结果，请尝试在构建神经网络之前使用`set.seed(12345)`。
- en: Correlations close to 1 indicate strong linear relationships between two variables.
    Therefore, the correlation here of about 0.806 indicates a fairly strong relationship.
    This implies that our model is doing a fairly good job, even with only a single
    hidden node.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接近1的相关性表明两个变量之间有很强的线性关系。因此，这里约为0.806的相关性表明了相当强的关系。这意味着我们的模型做得相当不错，即使只有一个隐藏节点。
- en: Given that we only used one hidden node, it is likely that we can improve the
    performance of our model. Let's try to do a bit better.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于我们只使用了一个隐藏节点，我们很可能可以提高模型的性能。让我们试着再做得更好一些。
- en: Step 5 – improving model performance
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5步 – 改进模型性能
- en: 'As networks with more complex topologies are capable of learning more difficult
    concepts, let''s see what happens when we increase the number of hidden nodes
    to five. We use the `neuralnet()` function as before, but add the `hidden = 5`
    parameter:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于更复杂的拓扑结构的网络能够学习更复杂的概念，让我们看看当我们将隐藏节点数增加到五时会发生什么。我们像之前一样使用`neuralnet()`函数，但添加了`hidden
    = 5`参数：
- en: '[PRE11]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Plotting the network again, we see a drastic increase in the number of connections.
    We can see how this impacted the performance as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 再次绘制网络，我们看到连接数量大幅增加。我们可以看到这如何影响了性能，如下所示：
- en: '[PRE12]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![Step 5 – improving model performance](img/3905_07_14.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![第5步 – 改进模型性能](img/3905_07_14.jpg)'
- en: Notice that the reported error (measured again by SSE) has been reduced from
    5.08 in the previous model to 1.63 here. Additionally, the number of training
    steps rose from 4,882 to 86,849, which should come as no surprise given how much
    more complex the model has become. More complex networks take many more iterations
    to find the optimal weights.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，报告的误差（再次由SSE测量）已从前一个模型的5.08降至1.63。此外，训练步骤的数量从4,882增加到86,849，这并不奇怪，因为模型变得更加复杂。更复杂的网络需要更多的迭代来找到最优权重。
- en: 'Applying the same steps to compare the predicted values to the true values,
    we now obtain a correlation around 0.92, which is a considerable improvement over
    the previous result of 0.80 with a single hidden node:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 应用相同的步骤将预测值与真实值进行比较，我们现在得到的相关性约为0.92，相比之前单个隐藏节点得到的0.80，这是一个相当可观的改进：
- en: '[PRE13]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Interestingly, in the original publication, Yeh reported a mean correlation
    of 0.885 using a very similar neural network. This means that with relatively
    little effort, we were able to match the performance of a subject-matter expert.
    If you'd like more practice with neural networks, you might try applying the principles
    learned earlier in this chapter to see how it impacts model performance. Perhaps
    try using different numbers of hidden nodes, applying different activation functions,
    and so on. The `?neuralnet` help page provides more information on the various
    parameters that can be adjusted.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，在最初的出版物中，Yeh报告使用一个非常相似的神经网络获得了0.885的平均相关性。这意味着，凭借相对较少的努力，我们就能够匹配专家的表现。如果你想更深入地练习神经网络，可能可以尝试应用本章早些时候学到的原理，看看它如何影响模型性能。也许可以尝试使用不同数量的隐藏节点，应用不同的激活函数等等。`?neuralnet`帮助页面提供了更多关于可调参数的信息。
- en: Understanding Support Vector Machines
  id: totrans-162
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解支持向量机
- en: A **Support Vector Machine** (**SVM**) can be imagined as a surface that creates
    a boundary between points of data plotted in multidimensional that represent examples
    and their feature values. The goal of a SVM is to create a flat boundary called
    a **hyperplane**, which divides the space to create fairly homogeneous partitions
    on either side. In this way, the SVM learning combines aspects of both the instance-based
    nearest neighbor learning presented in [Chapter 3](ch03.html "Chapter 3. Lazy
    Learning – Classification Using Nearest Neighbors"), *Lazy Learning – Classification
    Using Nearest Neighbors*, and the linear regression modeling described in [Chapter
    6](ch06.html "Chapter 6. Forecasting Numeric Data – Regression Methods"), *Forecasting
    Numeric Data – Regression Methods*. The combination is extremely powerful, allowing
    SVMs to model highly complex relationships.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量机**（**SVM**）可以被想象为一个表面，它在多维空间中为数据点创建边界，这些数据点代表示例及其特征值。SVM的目标是创建一个平坦的边界，称为**超平面**，该超平面将空间划分为相对均匀的两侧。通过这种方式，SVM学习结合了[第3章](ch03.html
    "第3章 懒惰学习——使用最近邻进行分类")中介绍的基于实例的最近邻学习，*懒惰学习——使用最近邻进行分类*，以及[第6章](ch06.html "第6章
    预测数值数据——回归方法")中描述的线性回归建模，*预测数值数据——回归方法*。这种结合极为强大，使SVM能够建模高度复杂的关系。'
- en: Although the basic mathematics that drive SVMs have been around for decades,
    they have recently exploded in popularity. This is, of course, rooted in their
    state-of-the-art performance, but perhaps also due to the fact that award winning
    SVM algorithms have been implemented in several popular and well-supported libraries
    across many programming languages, including R. SVMs have thus been adopted by
    a much wider audience, might have otherwise been unable to apply the somewhat
    complex math needed to implement a SVM. The good news is that although the math
    may be difficult, the basic concepts are understandable.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管驱动SVM的基础数学已经存在几十年，但它们最近爆发式地流行起来。当然，这源于它们的先进性能，但也可能与获奖的SVM算法在多个流行且得到良好支持的库中实现有关，涵盖了包括R在内的多种编程语言。因此，SVM被更广泛的用户群体采用，原本这些用户可能无法应用实现SVM所需的复杂数学。好消息是，尽管数学可能很困难，但基本概念是可以理解的。
- en: 'SVMs can be adapted for use with nearly any type of learning task, including
    both classification and numeric prediction. Many of the algorithm''s key successes
    have come in pattern recognition. Notable applications include:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: SVM可以适应几乎任何类型的学习任务，包括分类和数值预测。该算法的许多关键成功案例出现在模式识别中。显著的应用包括：
- en: Classification of microarray gene expression data in the field of bioinformatics
    to identify cancer or other genetic diseases
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生物信息学领域，对微阵列基因表达数据进行分类，以识别癌症或其他遗传疾病
- en: Text categorization such as identification of the language used in a document
    or the classification of documents by subject matter
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本分类，例如识别文档使用的语言或根据主题对文档进行分类
- en: The detection of rare yet important events like combustion engine failure, security
    breaches, or earthquakes
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检测稀有但重要的事件，如燃烧发动机故障、安全漏洞或地震
- en: SVMs are most easily understood when used for binary classification, which is
    how the method has been traditionally applied. Therefore, in the remaining sections,
    we will focus only on SVM classifiers. Don't worry, however, as the same principles
    you learn here will apply while adapting SVMs to other learning tasks such as
    numeric prediction.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: SVM最容易理解当它用于二元分类时，这也是该方法传统应用的方式。因此，在剩余的部分中，我们将仅专注于SVM分类器。但不要担心，因为你在这里学到的原则同样适用于将SVM适应到其他学习任务，比如数值预测。
- en: Classification with hyperplanes
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 超平面分类
- en: As noted previously, SVMs use a boundary called a hyperplane to partition data
    into groups of similar class values. For example, the following figure depicts
    hyperplanes that separate groups of circles and squares in two and three dimensions.
    Because the circles and squares can be separated perfectly by the straight line
    or flat surface, they are said to be **linearly separable**. At first, we'll consider
    only the simple case where this is true, but SVMs can also be extended to problems
    where the points are not linearly separable.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面所述，SVM使用一种称为超平面的边界来将数据分成类似类值的群体。例如，下图展示了在二维和三维中分隔圆圈和方块的超平面。因为圆圈和方块可以通过直线或平面完美分开，所以它们被称为**线性可分**。首先，我们将仅考虑这种情况，在其中这是真实的，但SVM也可以扩展到数据点不是线性可分的问题。
- en: '![Classification with hyperplanes](img/3905_07_15.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![超平面分类](img/3905_07_15.jpg)'
- en: Tip
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 小贴士
- en: For convenience, the hyperplane is traditionally depicted as a line in 2D space,
    but this is simply because it is difficult to illustrate space in greater than
    two dimensions. In reality, the hyperplane is a flat surface in a high-dimensional
    space—a concept that can be difficult to get your mind around.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了方便起见，在2D空间中，超平面通常被描绘为一条线，但这只是因为在超过两个维度的空间中描绘空间是困难的。实际上，超平面是高维空间中的一个平面——这是一个可能很难理解的概念。
- en: In two dimensions, the task of the SVM algorithm is to identify a line that
    separates the two classes. As shown in the following figure, there is more than
    one choice of dividing line between the groups of circles and squares. Three such
    possibilities are labeled **a**, **b**, and **c**. How does the algorithm choose?
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在二维空间中，SVM算法的任务是识别一个分隔两个类别的直线。如下图所示，在圆圈和方块的群体之间有多种分隔线的选择。这些可能性标记为**a**、**b**和**c**。算法是如何选择的？
- en: '![Classification with hyperplanes](img/3905_07_16.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![超平面分类](img/3905_07_16.jpg)'
- en: The answer to that question involves a search for the **Maximum Margin Hyperplane**
    (**MMH**) that creates the greatest separation between the two classes. Although
    any of the three lines separating the circles and squares would correctly classify
    all the data points, it is likely that the line that leads to the greatest separation
    will generalize the best to the future data. The maximum margin will improve the
    chance that, in spite of random noise, the points will remain on the correct side
    of the boundary.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 那个问题的答案涉及寻找**最大间隔超平面**（MMH），它在两个类别之间创建了最大的分离。虽然分隔圆圈和方块的三条线都可以正确分类所有数据点，但最大间隔的线可能对未来数据的泛化效果最好。最大间隔将提高即使在随机噪声的情况下，点仍然保持在边界的正确一侧的机会。
- en: The **support vectors** (indicated by arrows in the figure that follows) are
    the points from each class that are the closest to the MMH; each class must have
    at least one support vector, but it is possible to have more than one. Using the
    support vectors alone, it is possible to define the MMH. This is a key feature
    of SVMs; the support vectors provide a very compact way to store a classification
    model, even if the number of features is extremely large.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**支持向量**（在接下来的图中用箭头标出）是每个类别中距离MMH最近的点；每个类别必须至少有一个支持向量，但可以有多个。仅使用支持向量，就可以定义MMH。这是SVM的一个关键特征；支持向量提供了一种非常紧凑的方式来存储分类模型，即使特征数量极大。'
- en: '![Classification with hyperplanes](img/3905_07_17.jpg)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![超平面分类](img/3905_07_17.jpg)'
- en: The algorithm to identify the support vectors relies on vector geometry and
    involves some fairly tricky math that is outside the scope of this book. However,
    the basic principles of the process are fairly straightforward.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 识别支持向量的算法依赖于向量几何，并涉及一些超出本书范围的相当棘手的数学。然而，该过程的基本原理是相当简单的。
- en: Note
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: 'More information on the mathematics of SVMs can be found in the classic paper:
    Cortes C, Vapnik V. Support-vector network. *Machine Learning*. 1995; 20:273-297\.
    A beginner level discussion can be found in: Bennett KP, Campbell C. Support vector
    machines: hype or hallelujah. SIGKDD Explorations. 2003; 2:1-13\. A more in-depth
    look can be found in: Steinwart I, Christmann A. *Support Vector Machines*. New
    York: Springer; 2008.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '有关支持向量机（SVM）数学原理的更多信息，可以参考经典论文：Cortes C, Vapnik V. Support-vector network.
    *Machine Learning*. 1995; 20:273-297。初学者级别的讨论可以参考：Bennett KP, Campbell C. Support
    vector machines: hype or hallelujah. SIGKDD Explorations. 2003; 2:1-13。更深入的探讨可以参阅：Steinwart
    I, Christmann A. *Support Vector Machines*. New York: Springer; 2008。'
- en: The case of linearly separable data
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 线性可分数据的案例
- en: It is easiest to understand how to find the maximum margin under the assumption
    that the classes are linearly separable. In this case, the MMH is as far away
    as possible from the outer boundaries of the two groups of data points. These
    outer boundaries are known as the **convex hull**. The MMH is then the perpendicular
    bisector of the shortest line between the two convex hulls. Sophisticated computer
    algorithms that use a technique known as **quadratic optimization** are capable
    of finding the maximum margin in this way.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在假设类别是线性可分的情况下，最容易理解如何找到最大间隔。在这种情况下，最大间隔超平面（MMH）距离两组数据点的外边界尽可能远。这些外边界被称为 **凸包**。最大间隔超平面是两条凸包之间最短线段的垂直平分线。使用一种叫做
    **二次优化** 的技术的复杂计算机算法能够通过这种方式找到最大间隔。
- en: '![The case of linearly separable data](img/3905_07_18.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![线性可分数据的案例](img/3905_07_18.jpg)'
- en: An alternative (but equivalent) approach involves a search through the space
    of every possible hyperplane in order to find a set of two parallel planes that
    divide the points into homogeneous groups yet themselves are as far apart as possible.
    To use a metaphor, one can imagine this process as similar to trying to find the
    thickest mattress that can fit up a stairwell to your bedroom.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种（但等效的）方法涉及通过搜索每个可能的超平面空间，以找到一对将数据点分成同质组的平行平面，并且这两个平面之间的距离尽可能远。用一个比喻来说，可以把这个过程想象成在试图找到一个能顺利通过楼梯到卧室的最厚的床垫。
- en: 'To understand this search process, we''ll need to define exactly what we mean
    by a hyperplane. In *n*-dimensional space, the following equation is used:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解这个搜索过程，我们需要准确地定义什么是超平面。在 *n* 维空间中，使用以下方程：
- en: '![The case of linearly separable data](img/3905_07_19.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![线性可分数据的案例](img/3905_07_19.jpg)'
- en: If you aren't familiar with this notation, the arrows above the letters indicate
    that they are vectors rather than single numbers. In particular, *w* is a vector
    of *n* weights, that is, *{w[1], w[2], ..., w[n]}*, and *b* is a single number
    known as the **bias**. The bias is conceptually equivalent to the intercept term
    in the slope-intercept form discussed in [Chapter 6](ch06.html "Chapter 6. Forecasting
    Numeric Data – Regression Methods"), *Forecasting Numeric Data – Regression Methods*.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不熟悉这种符号，上面字母上的箭头表示它们是向量，而不是单个数字。特别地，*w* 是一个 *n* 维的权重向量，即 *{w[1], w[2], ...,
    w[n]}*，而 *b* 是一个单一数字，称为 **偏置**。偏置在概念上等同于 [第6章](ch06.html "第6章. 数值数据预测 - 回归方法")
    中讨论的斜截式方程中的截距项，*数值数据预测 - 回归方法*。
- en: Tip
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: If you're having trouble imagining the plane, don't worry about the details.
    Simply think of the equation as a way to specify a surface, much like when the
    slope-intercept form (*y = mx + b*) is used to specify lines in 2D space.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你很难想象这个平面，不要担心细节。只需将这个方程视为指定一个表面的方式，就像在二维空间中使用斜截式方程 (*y = mx + b*) 来指定直线一样。
- en: 'Using this formula, the goal of the process is to find a set of weights that
    specify two hyperplanes, as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个公式，过程的目标是找到一组权重，指定两个超平面，如下所示：
- en: '![The case of linearly separable data](img/3905_07_20.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![线性可分数据的案例](img/3905_07_20.jpg)'
- en: We will also require that these hyperplanes are specified such that all the
    points of one class fall above the first hyperplane and all the points of the
    other class fall beneath the second hyperplane. This is possible so long as the
    data are linearly separable.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要要求这些超平面是按以下方式指定的：使得一个类别的所有点都位于第一个超平面之上，而另一个类别的所有点都位于第二个超平面之下。只要数据是线性可分的，这是可能的。
- en: 'Vector geometry defines the distance between these two planes as:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 向量几何定义了这两个平面之间的距离如下：
- en: '![The case of linearly separable data](img/3905_07_21.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![线性可分数据的案例](img/3905_07_21.jpg)'
- en: 'Here, *||w||* indicates the **Euclidean norm** (the distance from the origin
    to vector *w*). Because *||w||* is in the denominator, to maximize distance, we
    need to minimize *||w||*. The task is typically reexpressed as a set of constraints,
    as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*||w||* 表示**欧几里得范数**（即原点到向量 *w* 的距离）。由于 *||w||* 位于分母，为了最大化距离，我们需要最小化 *||w||*。这个任务通常会被重新表达为一组约束条件，如下所示：
- en: '![The case of linearly separable data](img/3905_07_22.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![线性可分数据的情况](img/3905_07_22.jpg)'
- en: Although this looks messy, it's really not too complicated to understand conceptually.
    Basically, the first line implies that we need to minimize the Euclidean norm
    (squared and divided by two to make the calculation easier). The second line notes
    that this is subject to (*s.t.*), the condition that each of the *y[i]* data points
    is correctly classified. Note that *y* indicates the class value (transformed
    to either +1 or -1) and the upside down "A" is shorthand for "for all."
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这看起来有些混乱，但实际上从概念上理解并不太复杂。基本上，第一行表示我们需要最小化欧几里得范数（平方后除以 2 以简化计算）。第二行表示这是受约束（*s.t.*）的，即每个
    *y[i]* 数据点必须被正确分类。请注意，*y* 表示类别值（转化为 +1 或 -1），而倒立的 "A" 是 "对于所有" 的简写。
- en: As with the other method for finding the maximum margin, finding a solution
    to this problem is a task best left for quadratic optimization software. Although
    it can be processor-intensive, specialized algorithms are capable of solving these
    problems quickly even on fairly large datasets.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与寻找最大间隔的另一种方法一样，解决这个问题的任务最好交给二次优化软件。尽管这可能会占用大量处理器资源，但专门的算法能够迅速解决这些问题，即使是在相当大的数据集上。
- en: The case of nonlinearly separable data
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非线性可分数据的情况
- en: 'As we''ve worked through the theory behind SVMs, you may be wondering about
    the elephant in the room: what happens if the data are not linearly separable?
    The solution to this problem is the use of a **slack variable**, which creates
    a soft margin that allows some points to fall on the incorrect side of the margin.
    The figure that follows illustrates two points falling on the wrong side of the
    line with the corresponding slack terms (denoted with the Greek letter Xi):'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们分析了 SVM 背后的理论后，你可能会想知道一个问题：如果数据不是线性可分的，会发生什么？解决这个问题的方法是使用**松弛变量**，它创建了一个软间隔，允许一些数据点落在间隔的错误一侧。接下来的图示说明了两个数据点落在直线的错误一侧，并显示了相应的松弛项（用希腊字母
    Xi 表示）：
- en: '![The case of nonlinearly separable data](img/3905_07_23.jpg)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![非线性可分数据的情况](img/3905_07_23.jpg)'
- en: 'A cost value (denoted as *C*) is applied to all points that violate the constraints,
    and rather than finding the maximum margin, the algorithm attempts to minimize
    the total cost. We can therefore revise the optimization problem to:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对所有违反约束的点应用一个成本值（记为 *C*），与其寻找最大间隔，算法会尝试最小化总成本。因此，我们可以将优化问题修改为：
- en: '![The case of nonlinearly separable data](img/3905_07_24.jpg)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![非线性可分数据的情况](img/3905_07_24.jpg)'
- en: If you're still confused, don't worry, you're not alone. Luckily, SVM packages
    will happily optimize this for you without you having to understand the technical
    details. The important piece to understand is the addition of the cost parameter
    *C*. Modifying this value will adjust the penalty, for example, the fall on the
    wrong side of the hyperplane. The greater the cost parameter, the harder the optimization
    will try to achieve 100 percent separation. On the other hand, a lower cost parameter
    will place the emphasis on a wider overall margin. It is important to strike a
    balance between these two in order to create a model that generalizes well to
    future data.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仍然感到困惑，不用担心，你不是唯一一个。幸运的是，SVM 软件包会很高兴地为你优化这一过程，而无需你理解技术细节。需要理解的重要部分是**成本参数
    C**的引入。修改该值将调整惩罚，例如，数据点落在超平面错误的一侧。成本参数越大，优化过程越会努力实现 100% 的分类准确率。另一方面，较低的成本参数则会将重点放在更宽的整体间隔上。为了创建一个能很好地泛化到未来数据的模型，平衡这两者是非常重要的。
- en: Using kernels for non-linear spaces
  id: totrans-207
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用核函数处理非线性空间
- en: In many real-world applications, the relationships between variables are nonlinear.
    As we just discovered, a SVM can still be trained on such data through the addition
    of a slack variable, which allows some examples to be misclassified. However,
    this is not the only way to approach the problem of nonlinearity. A key feature
    of SVMs is their ability to map the problem into a higher dimension space using
    a process known as the **kernel trick**. In doing so, a nonlinear relationship
    may suddenly appear to be quite linear.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多实际应用中，变量之间的关系是非线性的。正如我们刚刚发现的，SVM 仍然可以通过添加松弛变量来训练这类数据，从而允许某些示例被误分类。然而，这并不是处理非线性问题的唯一方式。SVM
    的一个关键特点是它们能够使用一种称为 **核技巧** 的过程将问题映射到一个更高维度的空间。在这样做的过程中，非线性关系可能突然变得非常线性。
- en: 'Though this seems like nonsense, it is actually quite easy to illustrate by
    example. In the following figure, the scatterplot on the left depicts a nonlinear
    relationship between a weather class (sunny or snowy) and two features: latitude
    and longitude. The points at the center of the plot are members of the snowy class,
    while the points at the margins are all sunny. Such data could have been generated
    from a set of weather reports, some of which were obtained from stations near
    the top of a mountain, while others were obtained from stations around the base
    of the mountain.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这看起来像是废话，但实际上通过示例很容易说明。在下图中，左侧的散点图描绘了天气类别（晴天或雪天）与两个特征：纬度和经度之间的非线性关系。图中心的点属于雪天类别，而图边缘的点都是晴天。这些数据可能来自一组天气报告，其中一些来自山顶附近的气象站，另一些则来自山脚下的气象站。
- en: '![Using kernels for non-linear spaces](img/3905_07_25.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![使用核方法处理非线性空间](img/3905_07_25.jpg)'
- en: 'On the right side of the figure, after the kernel trick has been applied, we
    look at the data through the lens of a new dimension: altitude. With the addition
    of this feature, the classes are now perfectly linearly separable. This is possible
    because we have obtained a new perspective on the data. In the left figure, we
    are viewing the mountain from a bird''s eye view, while in the right one, we are
    viewing the mountain from a distance at the ground level. Here, the trend is obvious:
    snowy weather is found at higher altitudes.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在图的右侧，应用了核技巧后，我们通过一个新维度——海拔高度来观察数据。随着这个特征的加入，类别现在可以完美地线性分开。这之所以可能，是因为我们从一个新的视角来看待数据。在左图中，我们是从鸟瞰视角观察这座山，而在右图中，我们是从地面远处观察这座山。在这里，趋势非常明显：雪天出现在更高的海拔。
- en: SVMs with nonlinear kernels add additional dimensions to the data in order to
    create separation in this way. Essentially, the kernel trick involves a process
    of constructing new features that express mathematical relationships between measured
    characteristics. For instance, the altitude feature can be expressed mathematically
    as an interaction between latitude and longitude—the closer the point is to the
    center of each of these scales, the greater the altitude. This allows SVM to learn
    concepts that were not explicitly measured in the original data.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非线性核的 SVM 通过增加数据的额外维度来创造分离。实际上，核技巧包括构建新特征的过程，这些特征表达了度量特征之间的数学关系。例如，海拔特征可以数学地表达为纬度和经度之间的交互作用——该点离这两个尺度的中心越近，海拔就越高。这使得
    SVM 能够学习到原始数据中没有明确度量的概念。
- en: 'SVMs with nonlinear kernels are extremely powerful classifiers, although they
    do have some downsides, as shown in the following table:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用非线性核的 SVM 是非常强大的分类器，尽管它们确实有一些缺点，如下表所示：
- en: '| Strengths | Weaknesses |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '| 优势 | 弱点 |'
- en: '| --- | --- |'
  id: totrans-215
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can be used for classification or numeric prediction problems
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用于分类或数值预测问题
- en: Not overly influenced by noisy data and not very prone to overfitting
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不容易受到噪声数据的影响，也不容易发生过拟合
- en: May be easier to use than neural networks, particularly due to the existence
    of several well-supported SVM algorithms
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能比神经网络更容易使用，尤其是因为存在多个得到良好支持的 SVM 算法
- en: Gaining popularity due to its high accuracy and high-profile wins in data mining
    competitions
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于其高准确率和在数据挖掘竞赛中的高调获胜，SVM 正在获得越来越多的关注
- en: '|'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Finding the best model requires testing of various combinations of kernels and
    model parameters
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 寻找最佳模型需要测试不同核函数和模型参数的组合
- en: Can be slow to train, particularly if the input dataset has a large number of
    features or examples
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练可能较慢，特别是当输入数据集具有大量特征或示例时
- en: Results in a complex black box model that is difficult, if not impossible, to
    interpret
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 结果是一个复杂的黑箱模型，难以解释，甚至可能无法解释
- en: '|'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Kernel functions, in general, are of the following form. The function denoted
    by the Greek letter phi, that is, ϕ(x), is a mapping of the data into another
    space. Therefore, the general kernel function applies some transformation to the
    feature vectors *x[i]* and *x[j]* and combines them using the **dot product**,
    which takes two vectors and returns a single number.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数通常具有以下形式。由希腊字母 phi 表示的函数，即 ϕ(x)，是将数据映射到另一个空间。因此，一般的核函数对特征向量 *x[i]* 和 *x[j]*
    进行某种变换，并使用 **点积**将它们结合，点积操作将两个向量转换为一个单一的数字。
- en: '![Using kernels for non-linear spaces](img/3905_07_26.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![使用核函数处理非线性空间](img/3905_07_26.jpg)'
- en: Using this form, kernel functions have been developed for many different domains
    of data. A few of the most commonly used kernel functions are listed as follows.
    Nearly all SVM software packages will include these kernels, among many others.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种形式，已经为许多不同领域的数据开发了核函数。以下列出了几种最常用的核函数。几乎所有的 SVM 软件包都会包含这些核函数及其他许多核函数。
- en: 'The **linear kernel** does not transform the data at all. Therefore, it can
    be expressed simply as the dot product of the features:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性核**完全不对数据进行转换。因此，它可以简单地表示为特征的点积：'
- en: '![Using kernels for non-linear spaces](img/3905_07_27.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![使用核函数处理非线性空间](img/3905_07_27.jpg)'
- en: 'The **polynomial kernel** of degree *d* adds a simple nonlinear transformation
    of the data:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**多项式核**的度数为 *d*，对数据进行简单的非线性变换：'
- en: '![Using kernels for non-linear spaces](img/3905_07_28.jpg)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![使用核函数处理非线性空间](img/3905_07_28.jpg)'
- en: 'The **sigmoid kernel** results in a SVM model somewhat analogous to a neural
    network using a sigmoid activation function. The Greek letters kappa and delta
    are used as kernel parameters:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '**Sigmoid 核**导致的 SVM 模型与使用 sigmoid 激活函数的神经网络有些类似。希腊字母 kappa 和 delta 被用作核参数：'
- en: '![Using kernels for non-linear spaces](img/3905_07_29.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![使用核函数处理非线性空间](img/3905_07_29.jpg)'
- en: 'The **Gaussian RBF kernel** is similar to a RBF neural network. The RBF kernel
    performs well on many types of data and is thought to be a reasonable starting
    point for many learning tasks:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**高斯 RBF 核**类似于 RBF 神经网络。RBF 核在许多类型的数据上表现良好，且被认为是许多学习任务的合理起点：'
- en: '![Using kernels for non-linear spaces](img/3905_07_30.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![使用核函数处理非线性空间](img/3905_07_30.jpg)'
- en: There is no reliable rule to match a kernel to a particular learning task. The
    fit depends heavily on the concept to be learned as well as the amount of training
    data and the relationships among the features. Often, a bit of trial and error
    is required by training and evaluating several SVMs on a validation dataset. This
    said, in many cases, the choice of kernel is arbitrary, as the performance may
    vary slightly. To see how this works in practice, let's apply our understanding
    of SVM classification to a real-world problem.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 没有可靠的规则可以将特定的核函数与某个学习任务匹配。匹配程度很大程度上依赖于要学习的概念、训练数据的数量以及特征之间的关系。通常，需要通过在验证数据集上训练和评估多个
    SVM 来进行一些试错法。话虽如此，在许多情况下，核函数的选择是任意的，因为性能可能会有所波动。为了实践这种方法，让我们将 SVM 分类应用到一个现实问题中。
- en: Example – performing OCR with SVMs
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例 – 使用 SVM 进行 OCR（光学字符识别）
- en: Image processing is a difficult task for many types of machine learning algorithms.
    The relationships linking patterns of pixels to higher concepts are extremely
    complex and hard to define. For instance, it's easy for a human being to recognize
    a face, a cat, or the letter "A", but defining these patterns in strict rules
    is difficult. Furthermore, image data is often noisy. There can be many slight
    variations in how the image was captured, depending on the lighting, orientation,
    and positioning of the subject.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图像处理是许多类型的机器学习算法面临的困难任务。像素模式与更高层次概念之间的关系极为复杂且难以定义。例如，人类很容易识别出一张脸、一只猫或字母 "A"，但将这些模式定义为严格的规则却十分困难。此外，图像数据往往存在噪声。图像的捕获方式可能因光线、方向和主体位置的不同而有所变化。
- en: SVMs are well-suited to tackle the challenges of image data. Capable of learning
    complex patterns without being overly sensitive to noise, they are able to recognize
    visual patterns with a high degree of accuracy. Moreover, the key weakness of
    SVMs—the black box model representation—is less critical for image processing.
    If an SVM can differentiate a cat from a dog, it does not matter much how it is
    doing so.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量机（SVM）非常适合应对图像数据的挑战。它们能够学习复杂的模式而不对噪声过于敏感，从而能够高精度地识别视觉模式。此外，SVM的一个关键弱点——黑箱模型表示——对于图像处理的影响较小。如果一个SVM能区分猫和狗，那么它是如何做到的就不那么重要了。
- en: In this section, we will develop a model similar to those used at the core of
    the **Optical Character Recognition** (**OCR**) software often bundled with desktop
    document scanners. The purpose of such software is to process paper-based documents
    by converting printed or handwritten text into an electronic form to be saved
    in a database. Of course, this is a difficult problem due to the many variants
    in handwritten style and printed fonts. Even so, software users expect perfection,
    as errors or typos can result in embarrassing or costly mistakes in a business
    environment. Let's see whether our SVM is up to the task.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发一个类似于**光学字符识别**（**OCR**）软件中核心使用的模型，这类软件通常与桌面文档扫描仪捆绑在一起。此类软件的目的是通过将打印或手写文本转换为电子形式来处理纸质文档，以便保存到数据库中。当然，由于手写风格和打印字体的多样性，这是一个困难的问题。尽管如此，软件用户仍然期望完美，因为错误或错别字可能在商业环境中导致尴尬或代价高昂的错误。让我们看看我们的SVM是否能够完成这项任务。
- en: Step 1 – collecting data
  id: totrans-242
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤1 – 收集数据
- en: When OCR software first processes a document, it divides the paper into a matrix
    such that each cell in the grid contains a single **glyph**, which is just a term
    referring to a letter, symbol, or number. Next, for each cell, the software will
    attempt to match the glyph to a set of all characters it recognizes. Finally,
    the individual characters would be combined back together into words, which optionally
    could be spell-checked against a dictionary in the document's language.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 当OCR软件首次处理文档时，它会将纸张分割成一个矩阵，使得网格中的每个单元格包含一个单独的**字形**，字形只是指一个字母、符号或数字的术语。接下来，软件将尝试将每个单元格的字形与它识别的所有字符集合进行匹配。最后，单个字符将被重新组合成单词，并且可以选择通过字典对文档语言中的单词进行拼写检查。
- en: In this exercise, we'll assume that we have already developed the algorithm
    to partition the document into rectangular regions each consisting of a single
    character. We will also assume the document contains only alphabetic characters
    in English. Therefore, we'll simulate a process that involves matching glyphs
    to one of the 26 letters, A through Z.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在本次练习中，我们假设我们已经开发了算法，将文档划分为每个包含单个字符的矩形区域。我们还假设文档仅包含英文字母字符。因此，我们将模拟一个过程，涉及将字形与26个字母（A到Z）中的一个进行匹配。
- en: To this end, we'll use a dataset donated to the UCI Machine Learning Data Repository
    ([http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)) by W. Frey and
    D. J. Slate. The dataset contains 20,000 examples of 26 English alphabet capital
    letters as printed using 20 different randomly reshaped and distorted black and
    white fonts.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用W. Frey和D. J. Slate捐赠给UCI机器学习数据集库（[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)）的数据集。该数据集包含20,000个示例，展示了使用20种不同随机形变和失真的黑白字体打印的26个英文字母的大写字母。
- en: Note
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on this dataset, refer to Slate DJ, Frey W. Letter recognition
    using Holland-style adaptive classifiers*. Machine Learning.* 1991; 6:161-182.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 有关此数据集的更多信息，请参考Slate DJ, Frey W.《使用霍兰德风格适应分类器的字母识别*》。机器学习.* 1991; 6:161-182。
- en: 'The following figure, published by Frey and Slate, provides an example of some
    of the printed glyphs. Distorted in this way, the letters are challenging for
    a computer to identify, yet are easily recognized by a human being:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图像由Frey和Slate发布，提供了一些打印字形的示例。这些字形被扭曲后，计算机很难识别，但人类却能轻松识别：
- en: '![Step 1 – collecting data](img/3905_07_31.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![步骤1 – 收集数据](img/3905_07_31.jpg)'
- en: Step 2 – exploring and preparing the data
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2 – 探索和准备数据
- en: According to the documentation provided by Frey and Slate, when the glyphs are
    scanned into the computer, they are converted into pixels and 16 statistical attributes
    are recorded.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Frey和Slate提供的文档，当字形被扫描到计算机中时，它们会被转换为像素，并记录16个统计属性。
- en: The attributes measure such characteristics as the horizontal and vertical dimensions
    of the glyph, the proportion of black (versus white) pixels, and the average horizontal
    and vertical position of the pixels. Presumably, differences in the concentration
    of black pixels across various areas of the box should provide a way to differentiate
    among the 26 letters of the alphabet.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这些属性测量了字形的水平和垂直维度、黑色（与白色）像素的比例，以及像素的平均水平和垂直位置。可以推测，字形中黑色像素在不同区域的浓度差异应能提供一种区分26个字母的方法。
- en: Tip
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: To follow along with this example, download the `letterdata.csv` file from the
    Packt Publishing website, and save it to your R working directory.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 为了跟随这个例子，请从Packt Publishing网站下载`letterdata.csv`文件，并将其保存到你的R工作目录中。
- en: 'Reading the data into R, we confirm that we have received the data with the
    16 features that define each example of the letter class. As expected, letter
    has 26 levels:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据读入R后，我们确认已经接收到包含定义每个字母类别的16个特征的数据。正如预期的那样，字母类有26个水平：
- en: '[PRE14]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Recall that SVM learners require all features to be numeric, and moreover, that
    each feature is scaled to a fairly small interval. In this case, every feature
    is an integer, so we do not need to convert any factors into numbers. On the other
    hand, some of the ranges for these integer variables appear fairly wide. This
    indicates that we need to normalize or standardize the data. However, we can skip
    this step for now, because the R package that we will use for fitting the SVM
    model will perform the rescaling automatically.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，SVM学习器要求所有特征都是数值型的，而且每个特征都要缩放到一个相对较小的区间。在这种情况下，每个特征都是整数，因此我们不需要将任何因子转换为数字。另一方面，这些整数变量的某些范围似乎相当宽泛，这表明我们需要对数据进行归一化或标准化。然而，我们现在可以跳过这一步，因为我们将使用的R包在拟合SVM模型时会自动执行重新缩放。
- en: 'Given that the data preparation has been largely done for us, we can move directly
    to the training and testing phases of the machine learning process. In the previous
    analyses, we randomly divided the data between the training and testing sets.
    Although we could do so here, Frey and Slate have already randomized the data,
    and therefore suggest using the first 16,000 records (80 percent) to build the
    model and the next 4,000 records (20 percent) to test. Following their advice,
    we can create training and testing data frames as follows:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 由于数据准备工作已经基本完成，我们可以直接进入机器学习过程的训练和测试阶段。在之前的分析中，我们随机地将数据分为训练集和测试集。尽管我们可以在这里这样做，但Frey和Slate已经对数据进行了随机化，因此建议使用前16,000条记录（80%）来构建模型，使用接下来的4,000条记录（20%）进行测试。根据他们的建议，我们可以按以下方式创建训练和测试数据框：
- en: '[PRE15]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: With our data ready to go, let's start building our classifier.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 数据准备好后，让我们开始构建分类器。
- en: Step 3 – training a model on the data
  id: totrans-261
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3 – 在数据上训练模型
- en: When it comes to fitting an SVM model in R, there are several outstanding packages
    to choose from. The `e1071` package from the Department of Statistics at the Vienna
    University of Technology (TU Wien) provides an R interface to the award winning
    LIBSVM library, a widely used open source SVM program written in C++. If you are
    already familiar with LIBSVM, you may want to start here.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中拟合SVM模型时，有几个出色的包可供选择。来自维也纳大学（TU Wien）统计学系的`e1071`包提供了一个R接口，连接到获奖的LIBSVM库，这是一个广泛使用的用C++编写的开源SVM程序。如果你已经熟悉LIBSVM，你可能希望从这里开始。
- en: Note
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For more information on LIBSVM, refer to the authors' website at [http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 有关LIBSVM的更多信息，请访问作者的官方网站：[http://www.csie.ntu.edu.tw/~cjlin/libsvm/](http://www.csie.ntu.edu.tw/~cjlin/libsvm/)。
- en: Similarly, if you're already invested in the SVMlight algorithm, the `klaR`
    package from the Department of Statistics at the Dortmund University of Technology
    (TU Dortmund) provides functions to work with this SVM implementation directly
    within R.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，如果你已经使用SVMlight算法，来自多特蒙德大学（TU Dortmund）统计学系的`klaR`包提供了直接在R中使用该SVM实现的功能。
- en: Note
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For information on SVMlight, have a look at [http://svmlight.joachims.org/](http://svmlight.joachims.org/).
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 关于SVMlight的信息，可以查看[http://svmlight.joachims.org/](http://svmlight.joachims.org/)。
- en: Finally, if you are starting from scratch, it is perhaps best to begin with
    the SVM functions in the `kernlab` package. An interesting advantage of this package
    is that it was developed natively in R rather than C or C++, which allows it to
    be easily customized; none of the internals are hidden behind the scenes. Perhaps
    even more importantly, unlike the other options, `kernlab` can be used with the
    `caret` package, which allows SVM models to be trained and evaluated using a variety
    of automated methods (covered in [Chapter 11](ch11.html "Chapter 11. Improving
    Model Performance"), *Improving Model Performance*).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果你是从头开始，最好从`kernlab`包中的SVM函数开始。这个包的一个有趣的优点是它是原生用R开发的，而不是用C或C++，这使得它非常容易自定义；包的内部实现完全透明，不会隐藏任何内容。或许更重要的是，与其他选项不同，`kernlab`可以与`caret`包一起使用，这使得SVM模型能够使用各种自动化方法进行训练和评估（在[第11章](ch11.html
    "第11章 提升模型性能")中有介绍，*提升模型性能*）。
- en: Note
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 注意
- en: For a more thorough introduction to `kernlab`, please refer to the authors'
    paper at [http://www.jstatsoft.org/v11/i09/](http://www.jstatsoft.org/v11/i09/).
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想更详细地了解`kernlab`，请参阅作者的论文，地址为[http://www.jstatsoft.org/v11/i09/](http://www.jstatsoft.org/v11/i09/)。
- en: The syntax for training SVM classifiers with `kernlab` is as follows. If you
    do happen to be using one of the other packages, the commands are largely similar.
    By default, the `ksvm()` function uses the Gaussian RBF kernel, but a number of
    other options are provided.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kernlab`训练SVM分类器的语法如下。如果你确实使用的是其他包，命令大体是类似的。默认情况下，`ksvm()`函数使用高斯RBF核，但也提供了其他多种选项。
- en: '![Step 3 – training a model on the data](img/3905_07_32.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![第3步 – 在数据上训练模型](img/3905_07_32.jpg)'
- en: 'To provide a baseline measure of SVM performance, let''s begin by training
    a simple linear SVM classifier. If you haven''t already, install the `kernlab`
    package to your library, using the `install.packages("kernlab")` command. Then,
    we can call the `ksvm()` function on the training data and specify the linear
    (that is, vanilla) kernel using the `vanilladot` option, as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供SVM性能的基准衡量标准，我们从训练一个简单的线性SVM分类器开始。如果你还没有安装，可以通过`install.packages("kernlab")`命令将`kernlab`包安装到你的库中。然后，我们可以在训练数据上调用`ksvm()`函数，并使用`vanilladot`选项指定线性（即基础）核，如下所示：
- en: '[PRE16]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Depending on the performance of your computer, this operation may take some
    time to complete. When it finishes, type the name of the stored model to see some
    basic information about the training parameters and the fit of the model.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你计算机的性能，这个操作可能需要一些时间才能完成。当完成时，输入已存储模型的名称，以查看一些关于训练参数和模型拟合的基本信息。
- en: '[PRE17]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: This information tells us very little about how well the model will perform
    in the real world. We'll need to examine its performance on the testing dataset
    to know whether it generalizes well to unseen data.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这些信息对我们了解模型在实际应用中的表现几乎没有帮助。我们需要检查它在测试数据集上的表现，才能知道它是否能够很好地推广到未见过的数据。
- en: Step 4 – evaluating model performance
  id: totrans-278
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4步 – 评估模型性能
- en: 'The `predict()` function allows us to use the letter classification model to
    make predictions on the testing dataset:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`predict()`函数允许我们使用字母分类模型对测试数据集进行预测：'
- en: '[PRE18]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Because we didn''t specify the type parameter, the `type = "response"` default
    was used. This returns a vector containing a predicted letter for each row of
    values in the test data. Using the `head()` function, we can see that the first
    six predicted letters were `U`, `N`, `V`, `X`, `N`, and `H`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们没有指定类型参数，所以使用了默认值`type = "response"`。这将返回一个向量，其中包含测试数据中每一行值对应的预测字母。通过使用`head()`函数，我们可以看到前六个预测字母分别是`U`、`N`、`V`、`X`、`N`和`H`：
- en: '[PRE19]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'To examine how well our classifier performed, we need to compare the predicted
    letter to the true letter in the testing dataset. We''ll use the `table()` function
    for this purpose (only a portion of the full table is shown here):'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查我们的分类器表现如何，我们需要将预测的字母与测试数据集中的真实字母进行比较。我们将使用`table()`函数来完成这一任务（这里只显示了完整表格的一部分）：
- en: '[PRE20]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The diagonal values of `144`, `121`, `120`, `156`, and `127` indicate the total
    number of records where the predicted letter matches the true value. Similarly,
    the number of mistakes is also listed. For example, the value of `5` in row `B`
    and column `D` indicates that there were five cases where the letter `D` was misidentified
    as a `B`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: '`144`、`121`、`120`、`156`和`127`这些对角线上的值表示预测字母与真实值匹配的记录总数。类似地，错误的数量也列出了。例如，`B`行和`D`列中值为`5`表示有5个案例将字母`D`错误地识别为`B`。'
- en: Looking at each type of mistake individually may reveal some interesting patterns
    about the specific types of letters the model has trouble with, but this is time
    consuming. We can simplify our evaluation instead by calculating the overall accuracy.
    This considers only whether the prediction was correct or incorrect, and ignores
    the type of error.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 分别查看每种类型的错误可能会揭示一些关于模型在处理特定字母时遇到的困难的有趣模式，但这需要大量时间。我们可以通过计算整体准确率来简化评估过程。这只考虑预测是否正确，而忽略错误的类型。
- en: 'The following command returns a vector of `TRUE` or `FALSE` values, indicating
    whether the model''s predicted letter agrees with (that is, matches) the actual
    letter in the test dataset:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令返回一个`TRUE`或`FALSE`值的向量，表示模型预测的字母是否与测试数据集中实际的字母一致（即，是否匹配）：
- en: '[PRE21]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Using the `table()` function, we see that the classifier correctly identified
    the letter in 3,357 out of the 4,000 test records:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`table()`函数，我们看到分类器在4000个测试记录中的3357个记录上正确识别了字母：
- en: '[PRE22]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'In percentage terms, the accuracy is about 84 percent:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 按百分比计算，准确率大约为84%：
- en: '[PRE23]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Note that when Frey and Slate published the dataset in 1991, they reported a
    recognition accuracy of about 80 percent. Using just a few lines of R code, we
    were able to surpass their result, although we also have the benefit of over two
    decades of additional machine learning research. With this in mind, it is likely
    that we are able to do even better.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，当Frey和Slate在1991年发布数据集时，他们报告的识别准确率大约为80%。仅通过几行R代码，我们就超越了他们的结果，尽管我们也得到了二十多年额外机器学习研究的好处。考虑到这一点，我们可能能够做得更好。
- en: Step 5 – improving model performance
  id: totrans-294
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五步 – 提高模型性能
- en: Our previous SVM model used the simple linear kernel function. By using a more
    complex kernel function, we can map the data into a higher dimensional space,
    and potentially obtain a better model fit.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前的SVM模型使用了简单的线性核函数。通过使用更复杂的核函数，我们可以将数据映射到更高维的空间，并可能获得更好的模型拟合。
- en: 'It can be challenging, however, to choose from the many different kernel functions.
    A popular convention is to begin with the Gaussian RBF kernel, which has been
    shown to perform well for many types of data. We can train an RBF-based SVM, using
    the `ksvm()` function as shown here:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，从众多不同的核函数中选择合适的一个可能是具有挑战性的。一种常见的做法是从高斯RBF核开始，因为它已被证明在许多类型的数据上表现良好。我们可以像这样使用`ksvm()`函数训练基于RBF的SVM：
- en: '[PRE24]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Next, we make predictions as done earlier:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们像之前一样进行预测：
- en: '[PRE25]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Finally, we''ll compare the accuracy to our linear SVM:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将与线性SVM的准确率进行比较：
- en: '[PRE26]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提示
- en: Your results may differ from those shown here due to randomness in the `ksvm`
    RBF kernel. If you'd like them to match exactly, use `set.seed(12345)` prior to
    running the ksvm() function.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`ksvm` RBF核函数的随机性，你的结果可能与这里展示的有所不同。如果你希望它们完全匹配，可以在运行ksvm()函数之前使用`set.seed(12345)`。
- en: By simply changing the kernel function, we were able to increase the accuracy
    of our character recognition model from 84 percent to 93 percent. If this level
    of performance is still unsatisfactory for the OCR program, other kernels could
    be tested, or the cost of constraints parameter C could be varied to modify the
    width of the decision boundary. As an exercise, you should experiment with these
    parameters to see how they impact the success of the final model.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 通过简单地更改核函数，我们成功将字符识别模型的准确率从84%提高到了93%。如果OCR程序的表现仍然不令人满意，可以尝试其他核函数，或者调整约束参数C的成本，以修改决策边界的宽度。作为练习，你应该尝试这些参数，看看它们如何影响最终模型的成功。
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we examined two machine learning methods that offer a great
    deal of potential, but are often overlooked due to their complexity. Hopefully,
    you now see that this reputation is at least somewhat undeserved. The basic concepts
    that drive ANNs and SVMs are fairly easy to understand.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们考察了两种机器学习方法，它们具有巨大的潜力，但由于复杂性经常被忽视。希望你现在能看到，这种声誉至少在某种程度上是不公平的。驱动ANNs和SVM的基本概念相当容易理解。
- en: On the other hand, because ANNs and SVMs have been around for many decades,
    each of them has numerous variations. This chapter just scratches the surface
    of what is possible with these methods. By utilizing the terminology you learned
    here, you should be capable of picking up the nuances that distinguish the many
    advancements that are being developed every day.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，由于人工神经网络（ANNs）和支持向量机（SVMs）已经存在了几十年，每种方法都有许多变种。本章只是简单介绍了这些方法可能实现的部分内容。通过运用你在这里学到的术语，你应该能够掌握那些每天都在发展的各种进展之间的细微差别。
- en: Now that we have spent some time learning about many different types of predictive
    models from simple to sophisticated; in the next chapter, we will begin to consider
    methods for other types of learning tasks. These unsupervised learning techniques
    will bring to light fascinating patterns within the data.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经花了一些时间学习了从简单到复杂的多种预测模型；在下一章，我们将开始考虑其他类型学习任务的方法。这些无监督学习技术将揭示数据中的迷人模式。
