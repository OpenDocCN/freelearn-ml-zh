- en: '5'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '5'
- en: Understanding Neural Networks and Deep Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解神经网络和深度学习
- en: 'Since its debut in 2012, **Deep Learning** (**DL**) has made a huge breakthrough
    and has been applied in many research and industrial areas including computer
    vision, **Natural Language Processing** (**NLP**), and so on. In this chapter,
    we will introduce basic concepts, including the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 自2012年首次亮相以来，**深度学习**（**DL**）取得了巨大的突破，并在包括计算机视觉、**自然语言处理**（**NLP**）在内的许多研究和工业领域得到应用。在本章中，我们将介绍以下基本概念：
- en: Neural networks and DL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络和深度学习
- en: The cost function
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成本函数
- en: The optimizer algorithm
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化算法
- en: The activation functions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'After we master the concepts, we will discuss several neural network models
    and their business use cases, including the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在掌握概念之后，我们将讨论几个神经网络模型及其商业用例，包括以下内容：
- en: '**Convolutional Neural Networks** (**CNNs**)'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**卷积神经网络**（**CNNs**）'
- en: '**Recurrent Neural Networks** (**RNNs**)'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**循环神经网络**（**RNNs**）'
- en: '**L****ong Short-Term Memory** (**LSTM**) networks'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆**（**LSTM**）网络'
- en: '**Generative Adversarial Networks** (**GANs**)'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）'
- en: Understanding neural networks and DL concepts, common models, and business use
    cases is extremely important in our cloud ML journey. Let’s get started.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的云机器学习之旅中，理解神经网络和深度学习概念、常见模型和商业用例至关重要。让我们开始吧。
- en: Neural networks and DL
  id: totrans-13
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络和深度学习
- en: In the history of us human beings, there are many interesting milestones, from
    vision development and language development to making and using tools. How did
    humans evolve and how can we train a computer to *see*, *speak*, and *use* tools?
    Looking for answers to these questions has led us to the modern AI arena.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们人类的历史中，有许多有趣的里程碑，从视觉发展和语言发展到制造和使用工具。人类是如何进化的，我们如何训练计算机来*看*、*说*和*使用*工具？寻找这些问题的答案引领我们进入了现代人工智能领域。
- en: 'How do our brains work? Modern science reveals that in the brain, there is
    a layered neural network consisting of a set of neurons. A typical neuron collects
    electrical signals from others through a fine structure called **dendrites** and
    sends out spikes of signals through a conducting structure called an **axon**,
    which splits into many branches. At the end of each branch, a synapse converts
    the signals from the axon into electrical effects to excite activity on the target
    neuron. *Figure 5.1* shows the working mechanism of a biological neuron:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大脑是如何工作的？现代科学揭示，在大脑中，存在一个由神经元组成的分层神经网络。一个典型的神经元通过称为**树突**的精细结构从其他神经元收集电信号，并通过称为**轴突**的传导结构发送信号的尖峰，该轴突分裂成许多分支。每个分支的末端，一个突触将轴突的信号转换为电效应，以激发目标神经元的活性。*图5.1*展示了生物神经元的运作机制：
- en: '![Figure 5.1 – How a biological neuron works ](img/Figure_5.1.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图5.1 – 生物神经元的工作原理](img/Figure_5.1.jpg)'
- en: Figure 5.1 – How a biological neuron works
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 – 生物神经元的工作原理
- en: 'Inspired by the biological neural network model, an **Artificial Neural Network**
    (**ANN**) model consists of artificial neurons called **perceptrons**. A perceptron
    receives weighted inputs from the other perceptrons, applies the transfer function,
    which is the sum of the weighted inputs, and the activation function, which adds
    nonlinear activation to the sum, and outputs to excite the next perceptron. *Figure
    5.2* shows the working mechanism of an artificial neuron (perceptron):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 受生物神经网络模型的启发，**人工神经网络**（**ANN**）模型由称为**感知器**的人工神经元组成。感知器接收来自其他感知器的加权输入，应用传递函数，即加权输入的总和，以及激活函数，它向总和添加非线性激活，并将输出以激发下一个感知器。*图5.2*展示了人工神经元（感知器）的运作机制：
- en: '![Figure 5.2 – How an artificial neuron (perceptron) works ](img/Figure_5.2.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图5.2 – 人工神经元（感知器）的工作原理](img/Figure_5.2.jpg)'
- en: Figure 5.2 – How an artificial neuron (perceptron) works
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 人工神经元（感知器）的工作原理
- en: 'ANNs consist of perceptrons working together via layers. *Figure 5.3* shows
    the structure of a multilayer ANN where each circular node represents a perceptron,
    and a line represents the connection from the output of one perceptron to the
    input of another. There are three types of layers in a neural network: an input
    layer, one or more hidden layers, and an output layer. The neural network in *Figure
    5.3* has one input layer, two hidden layers, and an output layer:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ANNs由通过层协同工作的感知器组成。*图5.3*展示了多层ANN的结构，其中每个圆形节点代表一个感知器，一条线代表一个感知器的输出到另一个感知器的输入的连接。神经网络中有三种类型的层：输入层、一个或多个隐藏层和输出层。*图5.3*中的神经网络有一个输入层、两个隐藏层和一个输出层：
- en: '![Figure 5.3 – A multilayer ANN ](img/Figure_5.3.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.3 – 多层人工神经网络](img/Figure_5.3.jpg)'
- en: Figure 5.3 – A multilayer ANN
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.3 – 多层人工神经网络
- en: 'Using neural networks to perform ML model training, the data flows in the network
    as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用神经网络进行机器学习模型训练时，数据在网络中的流动如下：
- en: A dataset (*x*1*, x*2*, x*3*, ..., x*n) is prepared and sent to the input layer,
    which has the same amount of perceptrons as the number of features of the dataset.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 准备一个数据集 (*x*1*, x*2*, x*3*, ..., x*n*) 并将其发送到输入层，该层的感知器数量与数据集的特征数量相同。
- en: The data then moves through to the hidden layers. At each hidden layer, the
    perceptron processes the weighted inputs (sum and activate, as described earlier),
    and sends the output to the neurons at the next hidden layer.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，数据移动到隐藏层。在每个隐藏层中，感知器处理加权输入（求和并激活，如前所述），并将输出发送到下一隐藏层的神经元。
- en: After the hidden layers, the data finally moves to the output layer, which provides
    the outputs.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在隐藏层之后，数据最终移动到输出层，该层提供输出。
- en: 'The objective of the neural network is to determine the weights that minimize
    the cost function (average prediction error for the dataset). Similar to the regression
    model training process we discussed in the previous chapters, DL model training
    is implemented by iterations of a two-part process, forward propagation and backpropagation,
    as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的目标是确定最小化成本函数（数据集的平均预测误差）的权重。类似于我们在前几章中讨论的回归模型训练过程，深度学习模型训练通过迭代两个部分的过程实现，即正向传播和反向传播，如下所示：
- en: '**Forward propagation** is the path that information flows from the input layer
    to the output layer, through the hidden layers. At the beginning of the training
    process, data arrives at the input layer where they are multiplied with the weights
    randomly initialized, then passed to the first hidden layer. Since the input layer
    has multiple nodes, each one is connected to each node in the first hidden layer;
    a node in the hidden layer sums up the weighted values to it and applies an activation
    function (adds nonlinearity). It then sends the output to the nodes of the next
    layer, where the nodes do the same, till the output of the last hidden layer is
    multiplied by the weights and becomes the input to the final output layer, where
    further functions are applied to generate the output.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**正向传播**是信息从输入层流向输出层，通过隐藏层的路径。在训练过程的开始，数据到达输入层，在那里它们与随机初始化的权重相乘，然后传递到第一隐藏层。由于输入层有多个节点，每个节点都与第一隐藏层中的每个节点相连；隐藏层中的节点将加权值求和并应用激活函数（添加非线性）。然后它将输出发送到下一层的节点，那里的节点执行相同的操作，直到最后一个隐藏层的输出乘以权重并成为最终输出层的输入，在该层进一步应用函数以生成输出。'
- en: '**Backpropagation** is the path information flows from the output layer all
    the way back to the input layer. During this process, the neural network compares
    the predicted output to the actual output as the first step of backpropagation
    and calculates the cost function or prediction error. If the cost function is
    not good enough, it moves back to adjust the weights based on algorithms such
    as **Gradient Descent** (**GD**) and then starts the forward propagation again
    with the new weights.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**反向传播**是信息从输出层流回输入层的路径。在这个过程中，神经网络将预测输出与实际输出进行比较，作为反向传播的第一步，并计算成本函数或预测误差。如果成本函数不够好，它就会根据如**梯度下降**（**GD**）等算法回退以调整权重，然后使用新的权重再次开始正向传播。'
- en: Forward propagation and backpropagation are repeated multiple times—each time
    the network adjusts the weights, trying to get a better cost function value—until
    the network gets a good cost function (an acceptable accuracy) at the output layer.
    At this time, the model training is completed and we have got the optimized weights,
    which are the results of the training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 正向传播和反向传播会重复多次——每次网络调整权重，试图获得更好的成本函数值——直到网络在输出层获得良好的成本函数（可接受的准确度）。此时，模型训练完成，我们得到了优化的权重，这是训练的结果。
- en: DL is training ML models with neural networks. If you compare the preceding
    DL model training process using neural networks with the ML model training process
    we discussed in the *Training the model* section in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094),
    *Developing and Deploying ML Models*, you will find that the concepts of ML and
    DL are very similar. Via iterative forward propagation and backward propagation,
    both are trying to minimize the cost function of the model—ML is more about computers
    learning from data with traditional algorithms, while DL is more about computers
    learning from data mimicking the human brain and neural networks. Relatively speaking,
    ML requires less computing power and DL needs less human intervention. In the
    following sections, we will take a close look at the cost function, the optimizer
    algorithm, and the activation function for DL with neural networks.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL）是使用神经网络训练机器学习模型。如果你将使用神经网络的前一DL模型训练过程与我们讨论的[*第4章*](B18333_04.xhtml#_idTextAnchor094)中的[*训练模型*](B18333_04.xhtml#_idTextAnchor094)部分，即[*开发和部署机器学习模型*]的过程进行比较，你会发现机器学习和深度学习的概念非常相似。通过迭代的前向传播和反向传播，两者都试图最小化模型的代价函数——机器学习更多地涉及计算机使用传统算法从数据中学习，而深度学习更多地涉及计算机模仿人脑和神经网络从数据中学习。相对而言，机器学习需要的计算能力较少，深度学习需要较少的人工干预。在接下来的章节中，我们将仔细研究深度学习中的代价函数、优化器算法和激活函数。
- en: The cost function
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代价函数
- en: We introduced the concept of the cost function in the *Linear regression* section
    in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094). The cost function gives us
    a mathematical way of determining how much error the current model has—it assigns
    a cost for making an incorrect prediction and provides a way to measure the model
    performance. The cost function is a key metric in ML model training—choosing the
    right cost function can improve model performance dramatically.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[*第4章*](B18333_04.xhtml#_idTextAnchor094)的[*线性回归*](B18333_04.xhtml#_idTextAnchor094)部分介绍了代价函数的概念。代价函数为我们提供了一种数学方法来确定当前模型有多少误差——它为错误的预测分配代价，并提供了一种衡量模型性能的方法。代价函数是机器学习模型训练中的一个关键指标——选择正确的代价函数可以显著提高模型性能。
- en: The common cost functions for regression models are MAE and MSE. As we have
    discussed in previous chapters, MAE defines a summation of the absolute differences
    between the prediction values and the label values. MSE defines the summation
    of squares of the differences between the prediction values and the label values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的回归模型代价函数是MAE和MSE。正如我们在前几章所讨论的，MAE定义了预测值和标签值之间绝对差异的总和。MSE定义了预测值和标签值之间差异平方的总和。
- en: 'The cost functions for classification models are quite different. Conceptually,
    the cost function for a classification model is the difference between the probability
    distributions for different classes. For binary classification models where the
    model outputs are binary, 1 for yes or 0 for no, we use **binary cross entropy**.
    For multi-class classification models, depending on the dataset labels, we use
    **categorical cross entropy** and **sparse categorical cross entropy** as follows:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的代价函数相当不同。从概念上讲，分类模型的代价函数是不同类别的概率分布之间的差异。对于模型输出为二进制（1代表是，0代表否）的二分类模型，我们使用**二元交叉熵**。对于多分类模型，根据数据集标签的不同，我们使用**分类交叉熵**和**稀疏分类交叉熵**，如下所示：
- en: If the labels are integers, for example, to classify an image of a dog, a cat,
    or a cow, then we use sparse categorical cross entropy since the output is one
    exclusive class.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标签是整数，例如，为了对狗、猫或牛的图片进行分类，那么我们使用稀疏分类交叉熵，因为输出是唯一的一个类别。
- en: Otherwise, if the labels are encoded as a series of zeros and ones for each
    class (same for the one-hot-encoding format that we have discussed in the previous
    chapters), we’ll use categorical cross entropy. For example, given an image, you
    need to detect whether there exists a driver’s license, a passport, or a social
    security card, we will use categorical cross entropy as cost functions since the
    output has a combination of classes.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 否则，如果标签被编码为每个类的一系列零和一（与我们在前几章讨论的one-hot编码格式相同），我们将使用分类交叉熵。例如，给定一张图片，你需要检测是否存在驾驶证、护照或社会保障卡，我们将使用分类交叉熵作为代价函数，因为输出是类别的组合。
- en: The cost function is a way of measuring our models so we can adjust the model
    parameters to minimize them—the model optimization process. In the following section,
    we’ll talk about the optimizer algorithms that minimize the cost function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 成本函数是我们衡量模型的方式，以便我们可以调整模型参数以最小化它们——模型优化过程。在下一节中，我们将讨论最小化成本函数的优化器算法。
- en: The optimizer algorithm
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化器算法
- en: 'In the *Linear regression* section in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094),
    we discussed the **GD** algorithm, which optimizes the linear regression cost
    function. In neural networks, the optimizer is an algorithm used to minimize the
    cost function in model training. The commonly used optimizers are **Stochastic
    Gradient Descent** (**SGD**), **RMSprop**, and **Adam** as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在[*第 4 章*](B18333_04.xhtml#_idTextAnchor094)的[*线性回归*](B18333_04.xhtml#_idTextAnchor094)部分，我们讨论了**梯度下降法（GD**）算法，该算法优化线性回归成本函数。在神经网络中，优化器是用于在模型训练中最小化成本函数的算法。常用的优化器包括**随机梯度下降法（SGD**）、**RMSprop**和**Adam**，如下所示：
- en: '**SGD** is useful for very large datasets. Instead of GD, which runs through
    all of the samples in your training dataset to update parameters, SGD uses one
    or a subset of training samples.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机梯度下降法（SGD**）适用于非常大的数据集。与遍历训练数据集中所有样本以更新参数的梯度下降法（GD）不同，SGD使用一个或多个训练样本的子集。'
- en: '**RMSprop** improves SGD by introducing variable learning rates. The learning
    rate, as we discussed in [*Chapter 4*](B18333_04.xhtml#_idTextAnchor094), impacts
    model performances—larger learning rates can reduce training time but may lead
    to model oscillation and may miss the optimal model parameter values. Lower learning
    rates can make the training process longer. In SGD, the learning rate is fixed.
    RMSprop adapts the learning rate as training progresses, and thus it allows you
    to start with big learning rates when the model has a high cost function, but
    it gradually reduces the learning rate when the cost function decreases.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**RMSprop**通过引入可变学习率来改进SGD。正如我们在[*第 4 章*](B18333_04.xhtml#_idTextAnchor094)中讨论的那样，学习率会影响模型性能——较大的学习率可以减少训练时间，但可能导致模型振荡并错过最佳模型参数值。较低的学习率可以使训练过程更长。在SGD中，学习率是固定的。RMSprop随着训练的进行调整学习率，因此它允许你在模型具有高成本函数时以较大的学习率开始，但当成本函数降低时，它会逐渐降低学习率。'
- en: '**Adam** stands for **Adaptive Moment Estimation** and is one of the most widely
    used optimizers. Adam adds momentum to the adaptive learning rate from RMSprop,
    and thus it allows changes to the model to accelerate while moving in the same
    direction during training, making the model training process quicker and better.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Adam**代表**自适应矩估计**，是最广泛使用的优化器之一。Adam在RMSprop的自适应学习率中添加了动量，因此它允许在训练过程中向同一方向移动时加速模型的变化，使模型训练过程更快、更好。'
- en: Choosing the right cost function and optimizer algorithms is very important
    for model performance and training speed. Google’s TensorFlow framework provides
    many optimizer algorithms. For further details, please refer to [https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 选择合适的成本函数和优化算法对于模型性能和训练速度非常重要。Google的TensorFlow框架提供了许多优化算法。有关更多详细信息，请参阅[https://www.tensorflow.org/api_docs/python/tf/keras/optimizers](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)。
- en: Other important features for neural networks are non-linearity and output normalization,
    which are provided by the activation functions. We will examine them in the following
    section.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的其他重要特性是非线性和输出归一化，这些特性由激活函数提供。我们将在下一节中检查它们。
- en: The activation functions
  id: totrans-47
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 激活函数
- en: 'As you can see from the preceding section, the activation function is part
    of the training process. The purpose of the activation function is to transform
    the weighted-sum input to the nodes: non-linearize and change the output range.
    There are many activation functions in neural networks. We will discuss some of
    the most used ones: the sigmoid function, the tanh activation function, the ReLu
    function, and the LeakyReLU function. *Figure 5.4* shows the curves of these functions:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，激活函数是训练过程的一部分。激活函数的目的是将加权求和输入转换为节点：非线性化和改变输出范围。神经网络中有许多激活函数。我们将讨论一些最常用的：Sigmoid函数、tanh激活函数、ReLU函数和LeakyReLU函数。*图
    5.4*显示了这些函数的曲线：
- en: '![Figure 5.4 – Activation functions ](img/Figure_5.4.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.4 – 激活函数](img/Figure_5.4.jpg)'
- en: Figure 5.4 – Activation functions
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 – 激活函数
- en: 'Let’s inspect each of the preceding activation functions as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下方式检查前面的每个激活函数：
- en: The sigmoid activation function was discussed earlier ithe T*he cost function*
    section. We use the sigmoid function to change continuous values to a range between
    0 and 1, which fits the models to predict the probability as an output.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: sigmoid 激活函数在之前的“成本函数”部分已经讨论过。我们使用 sigmoid 函数将连续值转换为 0 到 1 之间的范围，这样模型就可以预测概率作为输出。
- en: The tanh activation function is very similar to sigmoid, but the output is from
    -1 to +1 and thus it is preferred to sigmoid due to the output being zero-centered.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tanh 激活函数与 sigmoid 非常相似，但输出范围是从 -1 到 +1，因此由于输出是零中心化的，它比 sigmoid 更受欢迎。
- en: The ReLU activation function stands for Rectified Linear Unit. It is widely
    used since it converts the negative values to 0 and keeps the positive values
    as such. Its range is between 0 and infinity. Because the gradient value is 0
    in the negative area, the weights and biases for some neurons may not be updated
    during the training process, causing dead neurons that never get activated.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ReLU 激活函数代表线性整流单元。它被广泛使用，因为它将负值转换为 0 并保持正值不变。其范围在 0 到无穷大之间。由于负区域的梯度值为 0，因此在训练过程中，某些神经元的权重和偏差可能不会更新，导致这些神经元永远不会被激活。
- en: The LeakyReLU is an improved version of the ReLU function to solve the dying
    ReLU problem as it has a small positive slope in the negative area. The advantages
    of LeakyReLU are the same as that of the ReLU, in addition to the fact that it
    enables training even for negative input values.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LeakyReLU 是 ReLU 函数的改进版本，它通过在负区域具有一个小正斜率来解决“死亡 ReLU”问题。LeakyReLU 的优点与 ReLU 相同，除此之外，它还允许对负输入值进行训练。
- en: Another activation function is the *softmax* function, which is often used in
    the output layer for multi-class classifications. The softmax activation function
    converts the output layer values into probabilities summing up to 1 and thus outputs
    probabilities for each class in multi-class classification problems.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个激活函数是 *softmax* 函数，它通常用于多类分类的输出层。softmax 激活函数将输出层值转换为概率之和为 1 的概率，从而在多类分类问题中为每个类别输出概率。
- en: Among all of these activation functions, which shall we choose? The answer depends
    on factors such as the type of predictions, the architecture of the network, the
    number of layers, the current layer in the network, and so on. For example, sigmoid
    is more used for binary classification use cases, whereas softmax is often applied
    for multi-classifications, and regression problems may or may not use activation
    functions. While there will be trial and error involved at the beginning, experience
    will build up good practices.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些激活函数中，我们应该选择哪一个？答案取决于诸如预测类型、网络架构、层数、网络中的当前层等因素。例如，sigmoid 函数更常用于二分类用例，而
    softmax 函数通常用于多分类，回归问题可能或可能不使用激活函数。虽然一开始可能会有试错的过程，但经验会积累出良好的实践。
- en: Now that we have introduced the concepts of neural networks and activation functions,
    let’s examine some neural networks that are commonly used in computer vision,
    **Natural Language Processing** (**NLP**), and other areas.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经介绍了神经网络和激活函数的概念，让我们来看看在计算机视觉、**自然语言处理**（NLP）和其他领域常用的神经网络。
- en: Convolutional Neural Networks
  id: totrans-59
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 卷积神经网络
- en: Now that we have learned about neural networks and DL, let’s look at some business
    use cases.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了神经网络和深度学习，让我们看看一些商业用例。
- en: The first case is image recognition. How can we teach a computer to recognize
    an image? It is an easy task for a human being but a very difficult one for a
    computer. The first thing we need to do, since computers are only good at working
    with 1s and 0s, is to transform the image into a numerical matrix using pixels.
    As an example, *Figure 5.5* shows a black and white image for a single digit number,
    *8*, represented by a 28x28 pixel matrix. While human beings can easily recognize
    the image as a number *8* by some *magic sensors* in our eyes, a computer needs
    to input all of the 28x28=784 pixels, each having a **pixel value—a** single number
    representing the brightness of the pixel. The pixel value has possible values
    from 0 to 255, with 0 as black and 255 as white. Values in between make up the
    different shades of gray. If we have a color image, the pixel will have three
    numerical RGB values (red, green, and blue) to represent its color instead of
    one black value.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个案例是图像识别。我们如何教会计算机识别图像呢？这对人类来说很容易，但对计算机来说却非常困难。由于计算机只擅长处理 1 和 0，我们首先需要做的是将图像转换成一个数值矩阵，使用像素来实现。例如，*图
    5.5* 展示了一个单位数 8 的黑白图像，它由一个 28x28 像素矩阵表示。人类可以通过眼睛中的某些*神奇传感器*轻松地将图像识别为数字 *8*，而计算机则需要输入所有
    28x28=784 个像素，每个像素都有一个**像素值**——一个代表像素亮度的单个数字。像素值可能的范围从 0 到 255，其中 0 为黑色，255 为白色。介于两者之间的值构成了不同的灰度。如果我们有一个彩色图像，像素将具有三个数值
    RGB 值（红色、绿色和蓝色）来表示其颜色，而不是一个黑色值。
- en: '![Figure 5.5 – Representing the number 8 with pixel values ](img/Figure_5.5.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5 – 使用像素值表示数字 8](img/Figure_5.5.jpg)'
- en: Figure 5.5 – Representing the number 8 with pixel values
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 – 使用像素值表示数字 8
- en: After we have a pixel matrix representation of the image, we can start developing
    a **Multi-Layer Perceptron** (**MLP**) network for training. We will construct
    the input layer with 784 nodes and input 784 pixel values, one for each. Each
    node from the input layer will then output to each node in the next layer (a hidden
    layer), and so on. When the number of layers increases, the total number of calculations
    will be huge for the entire network. To decrease the total calculations, the idea
    of feature filtering comes into play and leads to the concept of a **CNN**.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到图像的像素矩阵表示后，我们可以开始开发用于训练的**多层感知器**（**MLP**）网络。我们将构建一个包含 784 个节点的输入层，并输入
    784 个像素值，每个像素一个。输入层中的每个节点然后将输出到下一层（隐藏层）中的每个节点，依此类推。当层数增加时，整个网络的计算量将变得巨大。为了减少总计算量，特征过滤的概念应运而生，并导致了**卷积神经网络**（**CNN**）的概念。
- en: 'CNNs are widely used in computer vision, especially in image recognition and
    processing. A CNN consists of three layers: the convolutional layer, the pooling
    layer, and the fully connected layer. The convolutional layer convolutes the inputs
    and filters the image features, the pooling layer compresses the filtered features,
    and the fully connected layer, which is basically an MLP, does the model training.
    Let’s examine each of these layers in detail.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs 在计算机视觉中得到了广泛的应用，尤其是在图像识别和处理方面。一个 CNN 由三个层组成：卷积层、池化层和全连接层。卷积层对输入数据进行卷积，过滤图像特征，池化层压缩过滤后的特征，而全连接层，基本上是一个
    MLP，负责模型训练。让我们详细考察这些层的每一个。
- en: The convolutional layer
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 卷积层
- en: 'A **convolutional layer** performs convolution, which is applied to the input
    data to filter the information and produce a feature map. The filter is used as
    a sliding window to scan the entire image and autonomously recognize features
    in the images. As shown in *Figure 5.6*, a 3x3 filter, which is also called the
    **Kernel** (**K**), scans the whole **Image** (**I**) and generates a feature
    map, denoted as *I*K* since its element comes from the product of *I* and *K*
    (in the example of *Figure 5.6*: *1x1+0x0+1x0+0x1+1x1+0x0+1x1+0x1+1x1=4*).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: '**卷积层**执行卷积操作，该操作应用于输入数据以过滤信息并生成特征图。过滤器用作滑动窗口来扫描整个图像并自主识别图像中的特征。如图 5.6 所示，一个
    3x3 的过滤器，也称为**核**（**K**），扫描整个**图像**（**I**）并生成一个特征图，用 *I*K* 表示，因为其元素来自 *I* 和 *K*
    的乘积（例如图 5.6 中的例子：*1x1+0x0+1x0+0x1+1x1+0x0+1x1+0x1+1x1=4*）。'
- en: '![ Figure 5.6 – The convolution operation ](img/Figure_5.6.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.6 – 卷积操作](img/Figure_5.6.jpg)'
- en: Figure 5.6 – The convolution operation
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 – 卷积操作
- en: Going through the convolution process extracts the image features and generates
    a feature map that still has a large amount of data and makes it hard to train
    the neural network. To compress the data, we go through the pooling layer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通过卷积过程提取图像特征并生成一个仍然包含大量数据的特征图，这使得训练神经网络变得困难。为了压缩数据，我们通过池化层进行处理。
- en: The pooling layer
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 池化层
- en: A **pooling layer** receives the results from a convolutional layer, the feature
    map, and compresses it using a filter. Depending on the function used for calculation,
    it can either be maximum pooling or average pooling. As shown in *Figure 5.7*,
    a 2x2 filter patch scans the feature map and compresses it. With max pooling,
    it takes the maximum value from the scanning windows, *max(15,8,20,9) = 20*, and
    so on. With average pooling, it takes the average value, *average(15,8,20,9) =
    13*. As you can see, the filter of a pooling layer is always smaller than a feature
    map.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**池化层**接收卷积层的输出，即特征图，并使用过滤器对其进行压缩。根据所使用的计算函数，它可以是最大池化或平均池化。如图*图5.7*所示，一个2x2的过滤器块扫描特征图并对其进行压缩。使用最大池化时，它从扫描窗口中取最大值，*max(15,8,20,9)
    = 20*，依此类推。使用平均池化时，它取平均值，*average(15,8,20,9) = 13*。正如你所见，池化层的过滤器始终小于特征图。'
- en: '![Figure 5.7 – The pooling layer ](img/Figure_5.7.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图5.7 – 池化层](img/Figure_5.7.jpg)'
- en: Figure 5.7 – The pooling layer
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 – 池化层
- en: From the input image, the process of convolution and pooling iterates, and the
    final result is input to a fully connected layer (MLP) to process.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 从输入图像开始，卷积和池化过程迭代进行，最终结果输入到全连接层（MLP）进行处理。
- en: The fully connected layer
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全连接层
- en: After the convolution and pooling layers, we need to flatten the result and
    pass it to an MLP, a fully connected neural network, for classification. The final
    result will then be activated with the softmax activation function to yield the
    final output – an understanding of the image.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积和池化层之后，我们需要将结果展平并传递给MLP，一个全连接神经网络，以进行分类。最终结果将通过softmax激活函数激活，得到最终输出——对图像的理解。
- en: Recurrent Neural Networks
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 循环神经网络
- en: The second type of neural network is an RNN. RNNs are widely used in time series
    analysis, such as NLP. The concept of an RNN came about in the 1980s, but it’s
    not until recently that it gained its momentum in DL.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种神经网络类型是循环神经网络（RNN）。RNN在时间序列分析，如NLP中得到了广泛应用。RNN的概念在20世纪80年代出现，但直到最近，它在深度学习（DL）中才获得了动力。
- en: As we can see, in traditional feedforward neural networks such as CNNs, a node
    in the neural network only counts the current input and does not memorize the
    precious inputs. Therefore, it cannot handle time series data, which needs the
    previous inputs. For example, to predict the next word of a sentence, the previous
    words will be required to do the inference. By introducing a hidden state, which
    remembers some information about the sequence, RNNs solved this issue.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在传统的前馈神经网络，如CNN中，神经网络中的一个节点只计算当前输入，并不记忆先前输入。因此，它无法处理需要先前输入的时间序列数据。例如，为了预测句子中的下一个单词，需要先前的单词来进行推理。通过引入一个隐藏状态，该状态记忆序列的一些信息，RNN解决了这个问题。
- en: 'Different from feedforward networks, RNNs are a type of neural network where
    the output from the previous step is fed as the input to the current step; using
    a loop structure to keep the information allows the neural network to take the
    sequence of input. As shown in *Figure 5.8*, a loop for node *A* is unfolded to
    explain its process; first, node *A* takes *X*0 from the sequence of input, and
    then it outputs *h*0, which, together with *X*1, is the input for the next step.
    Similarly, *h*1 and *X*2 are inputs for the next step, and so on and so forth.
    Using the loop, the network keeps remembering the context while training:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 与前馈网络不同，RNN是一种神经网络，其中前一步的输出作为当前步骤的输入；使用循环结构保持信息，使神经网络能够接受输入序列。如图*图5.8*所示，节点*A*的循环展开以解释其过程；首先，节点*A*从输入序列中取*X*0，然后输出*h*0，它与*X*1一起是下一步的输入。同样，*h*1和*X*2是下一步的输入，依此类推。使用循环，网络在训练过程中持续记忆上下文：
- en: '![Figure 5.8 – The RNN unrolled loop (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    ](img/Figure_5.8.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图5.8 – RNN展开循环（来源：https://colah.github.io/posts/2015-08-Understanding-LSTMs/）](img/Figure_5.8.jpg)'
- en: 'Figure 5.8 – The RNN unrolled loop (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 – RNN展开循环（来源：https://colah.github.io/posts/2015-08-Understanding-LSTMs/）
- en: The drawback for a simple RNN model is the vanishing gradient problem, which
    is caused by the fact that the same weights are used to calculate a node’s output
    at each time step during training and also done during backpropagation. When we
    move backward further, the error signal becomes bigger or smaller, thus causing
    difficulty in memorizing the contexts that are further away in the sequence. To
    overcome this drawback, the **LSTM** neural network was developed.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 简单RNN模型的缺点是梯度消失问题，这是由于在训练过程中以及反向传播时使用相同的权重来计算每个时间步的节点输出。当我们向后移动时，误差信号变得更大或更小，这导致难以记住序列中更远的上下文。为了克服这一缺点，开发了**LSTM**神经网络。
- en: Long Short-Term Memory Networks
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: An LSTM network was designed to overcome the vanishing gradient problem. LSTMs
    have feedback connections, and the key to LSTMs is the cell state—a horizontal
    line running through the entire chain with only minor linear interactions, which
    persists the context information. LSTM adds or removes information to the cell
    state by gates, which are composed of activation functions, such as sigmoid or
    tanh, and a pointwise multiplication operation.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM网络被设计用来克服梯度消失问题。LSTMs具有反馈连接，LSTMs的关键是细胞状态——一条贯穿整个链的横向线，只有微小的线性交互，它持续保持上下文信息。LSTM通过门控机制向细胞状态添加或移除信息，门控机制由激活函数（如sigmoid或tanh）和点积乘法操作组成。
- en: '![Figure 5.9 – An LSTM model (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)
    ](img/Figure_5.9.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图5.9 – LSTM模型（来源：https://colah.github.io/posts/2015-08-Understanding-LSTMs/）](img/Figure_5.9.jpg)'
- en: 'Figure 5.9 – An LSTM model (source: https://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 – LSTM模型（来源：https://colah.github.io/posts/2015-08-Understanding-LSTMs/）
- en: '*Figure 5.9* shows an LSTM that has the gates to protect and control the cell
    state. Using the cell state, LSTM solves the issue of vanishing gradients and
    thus is particularly good at processing time series sequences of data, such as
    text and speech inference.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '*图5.9* 展示了一个具有保护和控制细胞状态的LSTM。使用细胞状态，LSTM解决了梯度消失的问题，因此特别擅长处理时间序列数据，如文本和语音推理。'
- en: Generative Adversarial networks
  id: totrans-90
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: '**GANs** are algorithmic architectures that are used to generate new synthetic
    instances of data that can pass for real data. As shown in *Figure 5.10*, GAN
    is a generative model that trains the following two models simultaneously:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '**GANs** 是一种算法架构，用于生成可以以真实数据为假的新合成数据实例。如图5.10所示，GAN是一种生成模型，它同时训练以下两个模型：'
- en: A **Generative** (**G**) model that captures the data distribution to generate
    plausible data. The latent space input and random noise can be sampled and fed
    into the generator network to generate samples that become the negative training
    examples for the discriminator.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**生成性**（**G**）模型，它捕捉数据分布以生成可信数据。潜在空间输入和随机噪声可以被采样并输入到生成器网络中，以生成样本，这些样本成为判别器的负训练示例。
- en: A **Discriminative** (**D**) model that compares the generated image with a
    real image and tries to identify whether the given image is fake or real. It estimates
    the probability that a sample came from the training data rather than the real
    data to distinguish the generator’s fake data from real data. The discriminator
    penalizes the generator for producing implausible results.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**判别性**（**D**）模型，它将生成的图像与真实图像进行比较，并试图识别给定的图像是伪造的还是真实的。它估计样本来自训练数据而不是真实数据的概率，以区分生成器的伪造数据和真实数据。判别器对生成器产生不可信结果进行惩罚。
- en: '![Figure 5.10 – The GAN (source: https://developers.google.com/machine-learning/recommendation)
    ](img/Figure_5.10.jpg)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图5.10 – GAN（来源：https://developers.google.com/machine-learning/recommendation）](img/Figure_5.10.jpg)'
- en: 'Figure 5.10 – The GAN (source: https://developers.google.com/machine-learning/recommendation)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 – GAN（来源：https://developers.google.com/machine-learning/recommendation）
- en: The model training starts with the generator generating fake data and the discriminator
    learns to tell that it’s false by comparing it with real samples. The GAN then
    sends the results to the generator and the discriminator to update the model.
    This fine tuning training process iterates and finally produces some extremely
    real-looking data. GANs can be used to generate text, images, and video, and color
    or denoise images.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练从生成器生成伪造数据开始，判别器通过比较与真实样本来学习识别它是虚假的。然后GAN将结果发送给生成器和判别器以更新模型。这种微调训练过程迭代进行，最终产生一些极其逼真的数据。GAN可以用于生成文本、图像和视频，以及彩色或去噪图像。
- en: Summary
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: Neural networks and DL have added the modern color to the traditional ML spectrum.
    In this chapter, we started by learning the concepts of neural networks and DL
    by examining the cost functions, optimizer algorithms, and activation functions.
    Then, we introduced advanced neural networks, including CNN, RNN, LSTM, and GAN.
    As we can see, by introducing neural networks, DL extended ML concepts and made
    a breakthrough in many applications such as computer vision, NLP, and others.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习为传统的机器学习光谱增添了现代色彩。在本章中，我们首先通过检查成本函数、优化器算法和激活函数来学习神经网络和深度学习的概念。然后，我们介绍了高级神经网络，包括CNN、RNN、LSTM和GAN。正如我们所见，通过引入神经网络，深度学习扩展了机器学习概念，并在计算机视觉、NLP和其他许多应用中取得了突破。
- en: 'This chapter concludes part two of the book: *Machine Learning and Deep Learning*.
    In part three, we will focus on *Machine Learning the Google Way*, where we will
    talk about how Google does ML and DL in Google Cloud. We will start part three
    with learning about BQML, Google TensorFlow, and Keras in the following chapter.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束了本书的第二部分：*机器学习和深度学习*。在第三部分，我们将专注于*Google方式学习机器学习*，我们将讨论Google如何在Google Cloud上实现机器学习和深度学习。我们将从下一章学习BQML、Google
    TensorFlow和Keras开始。
- en: Further reading
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步阅读
- en: 'For further insights on the topics learned in this chapter, you can refer to
    the following links:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 若想深入了解本章学习的内容，您可以参考以下链接：
- en: '[https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy)'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy](https://developers.google.com/machine-learning/crash-course/introduction-to-neural-networks/anatomy)'
- en: '[https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks)'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks](https://www.ibm.com/cloud/blog/ai-vs-machine-learning-vs-deep-learning-vs-neural-networks)'
- en: '[https://aws.amazon.com/what-is/neural-network/](https://aws.amazon.com/what-is/neural-network/)'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://aws.amazon.com/what-is/neural-network/](https://aws.amazon.com/what-is/neural-network/)'
- en: '[https://developers.google.com/machine-learning/gan](https://developers.google.com/machine-learning/gan)'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://developers.google.com/machine-learning/gan](https://developers.google.com/machine-learning/gan)'
- en: 'Part 3: Mastering ML in GCP'
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分：在GCP中精通机器学习
- en: 'In this part, we learn how Google does ML in the Google Cloud Platform. First,
    we discover Google’s BigQuery ML for structured data, and then we look at Google’s
    ML frameworks, TensorFlow and Keras. We examine Google’s end-to-end ML suite,
    Vertex AI, and the ML services it provides. We then look at the Google pre-trained
    model APIs for ML development: GCP ML APIs. We end this part with a summary of
    the ML implementation best practices in Google Cloud.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在本部分，我们学习如何在Google Cloud Platform上实现机器学习。首先，我们了解Google的BigQuery ML用于结构化数据，然后我们查看Google的机器学习框架，TensorFlow和Keras。我们检查Google的端到端机器学习套件，Vertex
    AI，以及它提供的机器学习服务。然后，我们查看用于机器学习开发的Google预训练模型API：GCP ML API。本部分最后总结了Google Cloud中机器学习实现的最佳实践。
- en: 'This part comprises the following chapters:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 本部分包括以下章节：
- en: '[*Chapter 6*](B18333_06.xhtml#_idTextAnchor133), Learning BQML, TensorFlow,
    and Keras'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第6章*](B18333_06.xhtml#_idTextAnchor133), 学习BQML、TensorFlow和Keras'
- en: '[*Chapter 7*](B18333_07.xhtml#_idTextAnchor143), Exploring Google Cloud Vertex
    AI'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第7章*](B18333_07.xhtml#_idTextAnchor143), 探索Google Cloud Vertex AI'
- en: '[*Chapter 8*](B18333_08.xhtml#_idTextAnchor159), Discovering Google Cloud ML
    API'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第8章*](B18333_08.xhtml#_idTextAnchor159), 发现Google Cloud ML API'
- en: '[*Chapter 9*](B18333_09.xhtml#_idTextAnchor168), Using Google Cloud ML Best
    Practices'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*第9章*](B18333_09.xhtml#_idTextAnchor168), 使用Google Cloud ML最佳实践'
