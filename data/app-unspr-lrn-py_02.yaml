- en: '*Chapter 2*'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第2章*'
- en: Hierarchical Clustering
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: Learning Objectives
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 学习目标
- en: 'By the end of this chapter, you will be able to:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将能够：
- en: Implement the hierarchical clustering algorithm from scratch by using packages
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始实现层次聚类算法，使用软件包
- en: Perform agglomerative clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行聚合聚类
- en: Compare k-means with hierarchical clustering
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较k-means和层次聚类
- en: In this chapter, we will use hierarchical clustering to build stronger groupings
    which make more logical sense.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将使用层次聚类来构建更强的分组，这些分组在逻辑上更合理。
- en: Introduction
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 介绍
- en: In this chapter, we will expand on the basic ideas that we built in *Chapter
    1*, *Introduction to Clustering*, by surrounding clustering with the concept of
    similarity. Once again, we will be implementing forms of the Euclidean distance
    to capture the notion of similarity. It is important to bear in mind that the
    Euclidean distance just happens to be one of the most popular distance metrics
    and not the only one! Through these distance metrics, we will expand on the simple
    neighbor calculations that we explored in the previous chapter by introducing
    the concept of hierarchy. By using hierarchy to convey clustering information,
    we can build stronger groupings that make more logical sense.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将在*第1章*《聚类介绍》中构建的基本思想基础上扩展，通过将聚类与相似性的概念结合起来。我们将再次实现欧几里得距离的各种形式来捕捉相似性这一概念。需要记住的是，欧几里得距离只是最流行的距离度量之一，而不是唯一的度量！通过这些距离度量，我们将通过引入层次结构的概念，扩展我们在上一章中探索的简单邻居计算。通过使用层次结构传递聚类信息，我们可以构建出更强的、在逻辑上更合理的分组。
- en: Clustering Refresher
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚类复习
- en: '*Chapter 1*, *Introduction to Clustering*, covered both the high-level intuition
    and in-depth details of one of the most basic clustering algorithms: k-means.
    While it is indeed a simple approach, do not discredit it; it will be a valuable
    addition to your toolkit as you continue your exploration of the unsupervised
    learning world. In many real-world use cases, companies experience groundbreaking
    discoveries through the simplest methods, such as k-means or linear regression
    (for supervised learning). As a refresher, let''s quickly walk through what clusters
    are and how k-means works to find them:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*第1章*《聚类介绍》涵盖了最基础聚类算法之一：k-means的高层次直觉和深入细节。虽然这确实是一种简单的方法，但不要小看它；它将是你继续探索无监督学习世界时，工具箱中的一个宝贵补充。在许多实际应用场景中，公司通过最简单的方法，比如k-means或线性回归（监督学习）取得了突破性的发现。作为复习，我们快速回顾一下什么是聚类以及k-means如何找到它们：'
- en: '![Figure 2.1: The attributes that separate supervised and unsupervised problems'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.1：区分监督学习和无监督学习问题的属性](img/C12626_02_01.jpg)'
- en: '](img/C12626_02_01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_01.jpg)'
- en: 'Figure 2.1: The attributes that separate supervised and unsupervised problems'
  id: totrans-14
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.1：区分监督学习和无监督学习问题的属性
- en: If you were given a random collection of data without any guidance, you would
    likely start your exploration using basic statistics – for example, what the mean,
    median, and mode values are of each of the features. Remember that, from a high-level
    data model that simply exists, knowing whether it is supervised or unsupervised
    learning is ascribed by the data goals that you have set for yourself or that
    were set by your manager. If you were to determine that one of the features was
    actually a label and you wanted to see how the remaining features in the dataset
    influence it, this would become a supervised learning problem. However, if after
    initial exploration you realize that the data you have is simply a collection
    of features without a target in mind (such as a collection of health metrics,
    purchase invoices from a web store, and so on), then you could analyze it through
    unsupervised methods.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你被给了一组没有任何指导的随机数据，你可能会开始使用基本统计方法进行探索——例如，求出每个特征的均值、中位数和众数。记住，从一个简单存在的高层次数据模型来看，是否是监督学习或无监督学习是由你为自己设定的目标或由经理设定的目标决定的。如果你确定其中一个特征实际上是标签，并且你想查看数据集中其余特征对它的影响，那么这将成为一个监督学习问题。然而，如果在初步探索后你意识到你所拥有的数据实际上只是没有目标的特征集合（例如一组健康指标、网店的购买发票等），那么你可以通过无监督方法来分析它。
- en: A classic example of unsupervised learning is finding clusters of similar customers
    in a collection of invoices from a web store. Your hypothesis is that by understanding
    which people are most similar, you can create more granular marketing campaigns
    that appeal to each cluster's interests. One way to achieve these clusters of
    similar users is through k-means.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的一个经典例子是在来自网店发票的集合中找到相似客户的簇。您的假设是通过了解哪些人最相似，您可以创建更细粒度的营销活动，以迎合每个簇的兴趣。实现这些相似用户簇的一种方式是通过k-means。
- en: k-means Refresher
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: k-means 刷新
- en: k-means clustering works by finding "k" number clusters in your data through
    pairwise Euclidean distance calculations. "K" points (also called centroids) are
    randomly initialized in your data and the distance is calculated from each data
    point to each of the centroids. The minimum of these distances designates which
    cluster a data point belongs to. Once every point has been assigned to a cluster,
    the mean intra-cluster data point is calculated as the new centroid. This process
    is repeated until the newly-calculated cluster centroid no longer changes position.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: k-means 聚类是通过在您的数据中找到“k”个簇来工作，通过成对的欧氏距离计算。“K”点（也称为质心）在您的数据中随机初始化，并且计算每个数据点到每个质心的距离。这些距离的最小值指定了数据点属于哪个簇。一旦每个点都被分配到一个簇中，就会计算出簇内数据点的平均值作为新的质心。这个过程重复进行，直到新计算出的簇质心不再改变位置。
- en: The Organization of Hierarchy
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 层次结构的组织
- en: Both the natural and human-made world contain many examples of organizing systems
    into hierarchies and why, for the most part, it makes a lot of sense. A common
    representation that is developed from these hierarchies can be seen in tree-based
    data structures. Imagine that you had a parent node with any number of child nodes
    that could subsequently be parent nodes themselves. By organizing concepts into
    a tree structure, you can build an information-dense diagram that clearly shows
    how things are related to their peers and their larger abstract concepts.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 自然界和人造世界都包含许多将系统组织成层次结构的例子，大多数情况下这是很有道理的。从这些层次结构中开发出的常见表示可以在基于树的数据结构中看到。想象一下，你有一个父节点和任意数量的子节点，这些子节点随后可以成为自己的父节点。通过将概念组织成树形结构，您可以构建一个信息密集的图表，清晰地显示事物与其同行及其更大抽象概念的关系。
- en: 'An example from the natural world to help illustrate this concept can be seen
    in how we view the hierarchy of animals, which goes from parent classes to individual
    species:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 一个帮助说明这一概念的自然界的例子可以在如何看待动物的层次结构中看到，这些层次结构从父类到个体物种：
- en: '![Figure 2.2: Navigating the relationships of animal species in a hierarchical
    tree structure'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.2: 在层次树结构中导航动物物种的关系'
- en: '](img/C12626_02_02.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_02.jpg)'
- en: 'Figure 2.2: Navigating the relationships of animal species in a hierarchical
    tree structure'
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图2.2: 在层次树结构中导航动物物种的关系'
- en: In Figure 2.2, you can see an example of how relational information between
    varieties of animals can be easily mapped out in a way that both saves space and
    still transmits a large amount of information. This example can be seen as both
    a tree of its own (showing how cats and dogs are different but both domesticated
    animals), and as a potential piece of a larger tree that shows a breakdown of
    domesticated versus non-domesticated animals.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.2中，您可以看到动物品种之间关系信息的一个示例，以一种既节省空间又传递大量信息的方式进行映射。这个例子可以看作是其自身的一棵树（显示猫和狗如何不同但都是驯养动物），也可以看作是一个更大树的一部分，显示驯养与非驯养动物的分解。
- en: 'In the event that most of you are not biologists, let''s move back toward the
    concept of a web store selling products. If you sold a large variety of products,
    then you would likely want to create a hierarchical system of navigation for your
    customers. By withholding all of the information in your product catalog, customers
    will only be exposed to the path down the tree that matches their interests. An
    example of the hierarchical benefits of navigation can be seen in Figure 2.3:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 假设大多数人不是生物学家，让我们回到一个销售产品的网店的概念。如果你销售多种产品，那么你可能希望为客户创建一个层次化的导航系统。通过仅展示产品目录中的所有信息，客户将只暴露于与其兴趣相匹配的树路径。层次化导航的好处的一个示例可以在图2.3中看到：
- en: '![Figure 2.3: Navigating product categories in a hierarchical tree structure'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.3: 在层次树结构中导航产品类别'
- en: '](img/C12626_02_03.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_03.jpg)'
- en: 'Figure 2.3: Navigating product categories in a hierarchical tree structure'
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.3：在层次树结构中导航产品类别
- en: Clearly, the benefits of a hierarchical system of navigation cannot be overstated
    in terms of improving your customer experience. By organizing information into
    a hierarchical structure, you can build an intuitive structure out of your data
    that demonstrates explicit nested relationships. If this sounds like another approach
    to finding clusters in your data, then you're definitely on the right track! Through
    the use of similar distance metrics such as the Euclidean distance from k-means,
    we can develop a tree that shows the many cuts of data that allow a user to subjectively
    create clusters at their discretion.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，层级导航系统的优势在改善客户体验方面是无法过分强调的。通过将信息组织成层级结构，你可以从数据中构建一个直观的结构，展示明确的嵌套关系。如果这听起来像是另一种在数据中寻找聚类的方法，那么你绝对走在正确的轨道上！通过使用类似欧氏距离这样的距离度量方法（如k-means中的欧氏距离），我们可以开发出一棵树，展示数据的许多切割点，允许用户根据需要主观地创建聚类。
- en: Introduction to Hierarchical Clustering
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分层聚类简介
- en: 'Until this point, we have shown that hierarchies can be excellent structures
    in which to organize information that clearly show nested relationships among
    data points. While this is helpful in gaining an understanding of the parent/child
    relationships between items, it can also be very handy when forming clusters.
    Expanding on the animal example of the prior section, imagine that you were simply
    presented with two features of animals: their height (measured from the tip of
    the nose to the end of the tail) and their weight. Using this information, you
    then have to recreate the same structure in order to identify which records in
    your dataset correspond to dogs or cats, as well as their relative subspecies.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 直到这一点为止，我们已经展示了层级结构可以作为一个很好的组织信息的方式，清晰地展示了数据点之间的嵌套关系。虽然这在理解项目之间的父子关系时非常有帮助，但在形成聚类时也非常有用。以前一节的动物示例为扩展，假设你仅仅获得了两种动物的特征：它们的身高（从鼻尖到尾端的长度）和体重。利用这些信息，你需要重新构建相同的结构，以便识别数据集中哪些记录对应于狗或猫，并区分它们的亚种。
- en: 'Since you are only given animal heights and weights, you won''t be able to
    extrapolate the specific names of each species. However, by analyzing the features
    that you have been provided, you can develop a structure within the data that
    serves as an approximation of what animal species exist in your data. This perfectly
    sets the stage for an unsupervised learning problem that is well solved with hierarchical
    clustering. In the following plot, you will see the two features that we created
    on the left: with animal height in the left-hand column and animal weight in the
    right-hand column. This is then charted on a two-axis plot with the height on
    the x axis and the weight on the y axis:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你仅仅获得了动物的身高和体重信息，你无法推断出每个物种的具体名称。然而，通过分析你所得到的特征，你可以在数据中构建一个结构，用以近似表示数据中存在的动物物种。这为无监督学习问题提供了一个理想的背景，而分层聚类则是一个非常适合解决该问题的方法。在下面的图表中，你将看到我们在左侧创建的两个特征：左栏是动物的身高，右栏是动物的体重。接着，这些数据被绘制在一个二维坐标图上，身高为x轴，体重为y轴：
- en: '![Figure 2.4: An example of a two-feature dataset comprising animal height
    and animal weight'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.4：一个包含动物身高和体重的二维特征数据集示例'
- en: '](img/C12626_02_04.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_04.jpg)'
- en: 'Figure 2.4: An example of a two-feature dataset comprising animal height and
    animal weight'
  id: totrans-36
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2.4：一个包含动物身高和体重的二维特征数据集示例
- en: One way to approach hierarchical clustering is by starting with each data point
    serving as its own cluster and recursively joining the similar points together
    to form clusters – this is known as **agglomerative** hierarchical clustering.
    We will go into more detail about the different ways of approaching hierarchical
    clustering in a later section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一种进行分层聚类的方法是从每个数据点作为其自身的聚类开始，并递归地将相似的点结合在一起形成聚类——这被称为**凝聚**型分层聚类。我们将在后面的章节中详细介绍不同的分层聚类方法。
- en: In the agglomerative hierarchical clustering approach, the concept of data point
    similarity can be thought of in the paradigm that we saw during k-means. In k-means,
    we used the Euclidean distance to calculate the distance from the individual points
    to the centroids of the expected "k" clusters. For this approach to hierarchical
    clustering, we will reuse the same distance metric to determine the similarity
    between the records in our dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在凝聚型层次聚类方法中，数据点相似性的概念可以在我们在k-means中看到的范式下进行思考。在k-means中，我们使用欧几里得距离来计算个体点到预期“k”簇的质心之间的距离。对于这种层次聚类方法，我们将重新使用相同的距离度量来确定数据集中记录之间的相似性。
- en: Eventually, by grouping individual records from the data with their most similar
    records recursively, you end up building a hierarchy from the bottom up. The individual
    single-member clusters join together into one single cluster at the top of our
    hierarchy.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，通过递归地将数据中每个记录与其最相似的记录进行组合，您会从底部构建一个层次结构。所有单成员簇将汇聚成一个顶层簇，形成层次结构的最上层。
- en: Steps to Perform Hierarchical Clustering
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行层次聚类的步骤
- en: 'To understand how agglomerative hierarchical clustering works, we can trace
    the path of a simple toy program as it merges together to form a hierarchy:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解凝聚型层次聚类的工作原理，我们可以通过一个简单的示例程序追踪它如何合并并形成一个层次结构：
- en: Given n sample data points, view each point as an individual "cluster" with
    just that one point as a member.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定n个样本数据点，将每个点视为一个单独的“簇”，其成员仅为该点。
- en: Calculate the pairwise Euclidean distance between the centroids of all the clusters
    in your data.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有簇的质心之间的成对欧几里得距离。
- en: Group the closest point pairs together.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最接近的点对聚集在一起。
- en: Repeat *Step 2* and *Step 3* until you reach a single cluster containing all
    the data in your set.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 2* 和 *步骤 3*，直到您得到一个包含所有数据的单一簇。
- en: Plot a dendrogram to show how your data has come together in a hierarchical
    structure. A dendrogram is simply a diagram that is used to represent a tree structure,
    showing an arrangement of clusters from top to bottom.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制一个树状图，显示您的数据如何以层次结构的形式汇聚在一起。树状图只是用来表示树形结构的图表，展示簇从上到下的排列。
- en: Decide what level you want to create the clusters at.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定要在哪个层级创建簇。
- en: An Example Walk-Through of Hierarchical Clustering
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分层聚类的示例演示
- en: 'While slightly more complex than k-means, hierarchical clustering does not
    change too much from a logistical perspective. Here is a simple example that walks
    through the preceding steps in slightly more detail:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管比k-means稍微复杂一些，层次聚类在逻辑上并没有太大变化。以下是一个简单的示例，通过稍微详细的步骤演示前述过程：
- en: 'Given a list of four sample data points, view each point as a centroid that
    is also its own cluster with the point indices from 0 to 3:'
  id: totrans-50
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定四个样本数据点的列表，将每个点视为一个质心，该质心也是它自身的簇，点的索引为0到3：
- en: 'Clusters (4): [ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]'
  id: totrans-51
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 簇（4）：[ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]
- en: 'Centroids (4): [ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]'
  id: totrans-52
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 质心（4）：[ (1,7) ], [ (-5,9) ], [ (-9,4) ] , [ (4, -2) ]
- en: 'Calculate the pairwise Euclidean distance between the centroids of all the
    clusters. In the matrix displayed in the following diagram, the point indices
    are between 0 and 3 both horizontally and vertically, showing the distance between
    the respective points. Along the diagonal are extremely high values to ensure
    that we do not select a point as its own neighbor (since it technically is the
    "closest" point). Notice that the values are mirrored across the diagonal:![Figure
    2.5: An array of distances'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有簇的质心之间的成对欧几里得距离。在下图显示的矩阵中，点的索引在水平方向和垂直方向上都介于0和3之间，表示各自点之间的距离。沿对角线的值极高，以确保我们不会将一个点选为其自身的邻居（因为它在技术上是“最接近”的点）。请注意，这些值在对角线两侧是对称的：![图
    2.5：距离数组
- en: '](img/C12626_02_05.jpg)'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_05.jpg)'
- en: 'Figure 2.5: An array of distances'
  id: totrans-55
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.5：距离数组
- en: Group the closest point pairs together.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将最接近的点对聚集在一起。
- en: 'In this case, points [1,7] and [-5,9] join into a cluster since they are closest,
    with the remaining two points left as single-member clusters:'
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在这种情况下，点[1,7]和[-5,9]由于最接近而合并成一个簇，其余两个点则保持为单成员簇：
- en: '![Figure 2.6: An array of distances'
  id: totrans-58
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.6：距离数组'
- en: '](img/C12626_02_06.jpg)'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_06.jpg)'
- en: 'Figure 2.6: An array of distances'
  id: totrans-60
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.6：距离数组
- en: 'Here are the resulting three clusters:'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这里是最终得到的三个簇：
- en: '[PRE0]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Calculate the centroid of the two-member cluster, as follows:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两个成员簇的质心，如下所示：
- en: '[PRE1]'
  id: totrans-64
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Add the centroid to the two single-member centroids and recalculate the distances.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将聚类中心添加到两个单一成员的聚类中心，并重新计算距离。
- en: 'Clusters (3):'
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '聚类 (3):'
- en: '[PRE2]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Centroids (3):'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '聚类中心 (3):'
- en: '[PRE3]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The output will be similar to the following diagram, with the shortest distance
    called using a red arrow:'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出将类似于以下图示，最短距离通过红色箭头标示：
- en: '![Figure 2.7: An array of distances'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.7：距离数组'
- en: '](img/C12626_02_07.jpg)'
  id: totrans-72
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_07.jpg)'
- en: 'Figure 2.7: An array of distances'
  id: totrans-73
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.7：距离数组
- en: 'Since it has the shortest distance, point [-9,4] is added to cluster 1:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于它具有最短的距离，点[-9,4]被添加到聚类 1：
- en: 'Clusters (2):'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '聚类 (2):'
- en: '[PRE4]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'With only point (4,-2) left as the furthest distance away from its neighbors,
    you can just add it to cluster 1 to unify all the clusters:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于只有点(4,-2)与其邻近点的距离最远，你可以将它添加到聚类 1，从而统一所有聚类：
- en: 'Clusters (1):'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '聚类 (1):'
- en: '[PRE5]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Plot a dendrogram to show the relationship between the points and the clusters:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制树状图以展示点与聚类之间的关系：
- en: '![Figure 2.8: A dendrogram showing the relationship between the points and
    the clusters'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.8：展示点与聚类之间关系的树状图'
- en: '](img/C12626_02_08.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_08.jpg)'
- en: 'Figure 2.8: A dendrogram showing the relationship between the points and the
    clusters'
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.8：展示点与聚类之间关系的树状图
- en: At the end of this process you can visualize the hierarchical structure that
    you created through a dendrogram. This plot shows how data points are similar
    and will look familiar to the hierarchical tree structures that we discussed earlier.
    Once you have this dendrogram structure, you can interpret how the data points
    relate to each other and subjectively decide at which "level" the clusters should
    exist.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个过程的最后，你可以通过树状图可视化你创建的层次结构。这个图展示了数据点的相似性，看起来与我们之前讨论的层次树状结构相似。一旦你拥有了这个树状图结构，你可以解释数据点之间的关系，并主观决定聚类应存在于哪个“层级”。
- en: 'Revisiting the previous animal taxonomy example from that involved dog and
    cat species, imagine that you were presented with the following dendrogram:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾之前涉及狗和猫物种的动物分类示例，假设你面临以下树状图：
- en: '![Figure 2.9: An animal taxonomy dendrogram'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.9：一个动物分类的树状图'
- en: '](img/C12626_02_09.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_09.jpg)'
- en: 'Figure 2.9: An animal taxonomy dendrogram'
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.9：一个动物分类的树状图
- en: The great thing about hierarchical clustering and dendrograms is that you can
    see the entire breakdown of potential clusters to choose from. If you were just
    interested in grouping your species dataset into dogs and cats, you could stop
    clustering at the first level of the grouping. However, if you wanted to group
    all species into domesticated or non-domesticated animals, you could stop clustering
    at level two.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类和树状图的优点在于，你可以看到所有可能的聚类拆分。如果你只对将物种数据集分为狗和猫感兴趣，你可以在分组的第一级停止聚类。然而，如果你想将所有物种分为家养动物和非家养动物，你可以在第二级停止聚类。
- en: 'Exercise 7: Building a Hierarchy'
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 7：构建层次结构
- en: 'Let''s try implementing the preceding hierarchical clustering approach in Python.
    With the framework for the intuition laid out, we can now explore the process
    of building a hierarchical cluster with some helper functions provided in `SciPy`.
    This exercise uses `SciPy`, an open source library that packages functions that
    are helpful in scientific and technical computing; examples of this include easy
    implementations of linear algebra and calculus-related methods. In addition to
    `SciPy`, we will be using Matplotlib to complete this exercise:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试在 Python 中实现前述的层次聚类方法。通过为直觉打下框架，现在我们可以探索如何使用`SciPy`提供的辅助函数构建层次聚类。这个练习使用了`SciPy`，这是一个开源库，提供了许多有助于科学和技术计算的函数；例如，它简化了线性代数和微积分相关方法的实现。除了`SciPy`，我们还将使用
    Matplotlib 来完成这个练习：
- en: 'Generate some dummy data as follows:'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成如下虚拟数据：
- en: '[PRE6]'
  id: totrans-93
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Visualize the data as follows:'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化数据如下：
- en: '[PRE7]'
  id: totrans-95
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The output is as follows:'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.10: A plot of the dummy data'
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.10：虚拟数据的绘图'
- en: '](img/C12626_02_10.jpg)'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_10.jpg)'
- en: 'Figure 2.10: A plot of the dummy data'
  id: totrans-99
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.10：虚拟数据的绘图
- en: After plotting this simple toy example, it should be pretty clear that our dummy
    data is comprised of eight clusters.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在绘制这个简单的示例数据之后，应该很清楚我们的虚拟数据包含了八个聚类。
- en: 'We can easily generate the distance matrix using the built-in `SciPy` package,
    ''`linkage`'':'
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们可以通过内置的`SciPy`包轻松生成距离矩阵，使用'`linkage`'：
- en: '[PRE8]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output is as follows:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出结果如下：
- en: '![Figure 2.11: A matrix of the distances'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.11：距离矩阵'
- en: '](img/C12626_02_11.jpg)'
  id: totrans-105
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_11.jpg)'
- en: 'Figure 2.11: A matrix of the distances'
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.11：距离的矩阵
- en: In the first situation, you can see that customizing the hyperparameters really
    drives the performance when finding the ideal linkage matrix. If you recall our
    previous steps, linkage works by simply calculating the distances between each
    of the data points. In the `linkage` function, we have the option to select both
    the metric and the method (we will cover more on this later).
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在第一种情况中，你可以看到，定制超参数确实会影响找到理想链接矩阵的性能。如果你回顾我们之前的步骤，链接的工作原理是通过计算每个数据点之间的距离来完成的。在
    `linkage` 函数中，我们可以选择度量标准和方法（稍后我们会详细讲解）。
- en: 'After we determine the linkage matrix, we can easily pass it through the dendrogram
    function provided by `SciPy`:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们确定了链接矩阵后，可以轻松地将其传递给 `SciPy` 提供的树状图函数：
- en: '[PRE9]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'The output is as follows:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.12: A dendrogram of the distances'
  id: totrans-111
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.12：距离的树状图'
- en: '](img/C12626_02_12.jpg)'
  id: totrans-112
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_12.jpg)'
- en: 'Figure 2.12: A dendrogram of the distances'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.12：距离的树状图
- en: This plot will give us some perspective on the potential breakouts of our data.
  id: totrans-114
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这个图将帮助我们更好地理解数据的潜在分组情况。
- en: Using this information, we can wrap up our exercise on hierarchical clustering
    by using the `fcluster` function from `SciPy`. The number `3` in the
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用这些信息，我们可以通过使用 `SciPy` 的 `fcluster` 函数来完成我们的层次聚类练习。公式中的数字 `3` 在这里表示
- en: 'following example represents the maximum inter-cluster distance threshold hyperparameter
    that you will set. This hyperparameter is tunable based on the dataset that you
    are looking at; however, it is supplied for you as `3` for this exercise:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以下示例表示你将设置的最大簇间距离阈值超参数。这个超参数可以根据你所使用的数据集进行调整；然而，对于本次练习，它的默认值为 `3`：
- en: '[PRE10]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output is as follows:'
  id: totrans-118
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.13: A scatter plot of the distances'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.13：距离的散点图'
- en: '](img/C12626_02_13.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_13.jpg)'
- en: 'Figure 2.13: A scatter plot of the distances'
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.13：距离的散点图
- en: By simply calling a few helper functions provided by SciPy, you can easily implement
    agglomerative clustering in just a few lines of code. While SciPy does help with
    many of the intermediate steps, this is still an example that is a bit more verbose
    than what you will probably see in your regular work. We will cover more streamlined
    implementations later.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 只需调用 `SciPy` 提供的几个辅助函数，你就可以轻松地在几行代码中实现聚合聚类。尽管 `SciPy` 在许多中间步骤中提供了帮助，但这个例子仍然稍显冗长，可能不是你在日常工作中会遇到的情况。我们将在稍后的部分介绍更加简洁的实现方式。
- en: Linkage
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 链接
- en: In *Exercise 7*, *Building a Hierarchy*, you implemented hierarchical clustering
    using what is known as **Centroid Linkage**. Linkage is the concept of determining
    how you can calculate the distances between clusters and is dependent on the type
    of problem you are facing. Centroid linkage was chosen for the first activity
    as it essentially mirrors the new centroid search that we used in k-means. However,
    this is not the only option when it comes to clustering data points together.
    Two other popular choices for determining distances between clusters are single
    linkage and complete linkage.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在*练习 7*，*构建层次结构*中，你使用了被称为**质心链接**的层次聚类方法。链接是指确定如何计算簇间距离的概念，这取决于你所面临的问题类型。选择质心链接是因为它本质上与我们在
    k-means 中使用的新的质心搜索方法相似。然而，在将数据点聚类时，这并不是唯一的选择。另两种常见的用于计算簇间距离的方法是单链接和完全链接。
- en: '**Single Linkage** works by finding the minimum distance between a pair of
    points between two clusters as its criteria for linkage. Put simply, it essentially
    works by combining clusters based on the closest points between the two clusters.
    This is expressed mathematically as follows:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '**单链接**通过找到两个簇之间一对点的最小距离来确定链接标准。简单来说，它通过基于两簇之间最接近的点来合并簇。数学表达式如下：'
- en: dist(a,b) = min( dist( a[i]), b[j] ) )
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: dist(a,b) = min( dist( a[i]), b[j] ) )
- en: '**Complete Linkage** is the opposite of single linkage and it works by finding
    the maximum distance between a pair of points between two clusters as its criteria
    for linkage. Put simply, it works by combining clusters based on the furthest
    points between the two clusters. This is mathematically expressed as follows:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '**完全链接**与单链接相反，它通过找到两个簇之间一对点的最大距离来确定链接标准。简单来说，它通过基于两簇之间最远的点来合并簇。数学表达式如下：'
- en: dist(a,b) = max( dist( a[i]), b[j] ) )
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: dist(a,b) = max( dist( a[i]), b[j] ) )
- en: Determining what linkage criteria is best for your problem is as much about
    art as it is about science and it is heavily dependent on your particular dataset.
    One reason to choose single linkage is that your data is similar in a nearest-neighbor
    sense, therefore, when there are differences, then the data is extremely dissimilar.
    Since single linkage works by finding the closest points, it will not be affected
    by these distant outliers. Conversely, complete linkage may be a better option
    if your data is distant in terms of inter-cluster, however, it is quite dense
    intra-cluster. Centroid linkage has similar benefits but falls apart if the data
    is very noisy and there are less clearly defined "centers" of clusters. Typically,
    the best approach is to try a few different linkage criteria options and to see
    which fits your data in a way that's most relevant to your goals.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 确定哪种连接标准最适合你的问题既是艺术也是科学，并且极大依赖于你特定的数据集。选择单一连接的一种原因是你的数据在邻近点上非常相似，因此，当存在差异时，这些数据就会表现出极大的不同。由于单一连接通过找到最接近的点来工作，因此它不会受到这些远离点的影响。相反，如果你的数据在类间较远，但类内相对密集，那么完全连接可能是一个更好的选择。质心连接有类似的优点，但如果数据非常嘈杂且聚类的“中心”不明确，它可能会失效。通常，最好的方法是尝试几种不同的连接标准选项，看看哪种最符合你的数据，并与目标最相关。
- en: 'Activity 2: Applying Linkage Criteria'
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 2：应用连接标准
- en: Recall the dummy data of the eight clusters that we generated in the previous
    exercise. In the real world, you may be given real data that resembles discrete
    Gaussian blobs in the same way. Imagine that the dummy data represents different
    groups of shoppers in a particular store. The store manager has asked you to analyze
    the shopper data in order to classify the customers into different groups, so
    that they can tailor marketing materials to each group.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在上一练习中生成的八个聚类的虚拟数据。在现实世界中，你可能会获得类似的实际数据，这些数据表现得像离散的高斯“团块”。假设这些虚拟数据代表了某个特定商店中不同的顾客群体。商店经理要求你分析顾客数据，将顾客分类成不同的群体，以便根据每个群体的特点定制营销材料。
- en: Using the data already generated in the previous exercise, or by generating
    new data, you are going to analyze which linkage types do the best job of grouping
    the customers into distinct clusters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在上一练习中已经生成的数据，或生成新数据，你将分析哪些连接方式最适合将顾客分为不同的群体。
- en: 'Once you have generated the data, view the documents supplied using SciPy to
    understand what linkage types are available in the `linkage` function. Then, evaluate
    the linkage types by applying them to your data. The linkage types you should
    test are shown in the following list:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你生成了数据，使用SciPy提供的文档来查看`linkage`函数中可用的连接类型。然后，通过将它们应用到你的数据中来评估这些连接类型。你应该测试的连接类型在以下列表中：
- en: '[PRE11]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: By completing this activity, you will gain an understanding of the linkage criteria
    – which is important to understand how effective your hierarchical clustering
    is. The aim is to gain an understanding of how linkage criteria play a role in
    different datasets and how it can make a useless clustering into a valid one.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 完成这个活动后，你将理解连接标准——了解它对于你的层次聚类效果至关重要。目标是理解连接标准在不同数据集中的作用，以及它如何将一个无效的聚类转变为一个有效的聚类。
- en: You may realize that we have not covered all of the previously mentioned linkage
    types – a key part of this activity is to learn how to parse the docstrings provided
    using packages to explore all of their capabilities.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会发现我们没有涵盖所有之前提到的连接类型——这项活动的关键部分是学习如何解析包提供的文档字符串，以探索它们的所有功能。
- en: 'Here are the steps required to complete this activity:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成此活动所需的步骤：
- en: Visualize the dataset that we created in *Exercise 7*, *Building a Hierarchy*.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化我们在*练习 7*中创建的数据集，*构建层次结构*。
- en: Create a list with all the possible linkage method hyperparameters.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含所有可能连接方法超参数的列表。
- en: Loop through each of the methods in the list that you just created and display
    the effect they have on the same dataset.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 遍历你刚刚创建的列表中的每个方法，并展示它们对相同数据集的影响。
- en: You should generate a plot for each linkage type and use the plots to comment
    on which linkage types are most suitable for this data.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该为每种连接方式生成一个图表，并利用这些图表评论哪种连接方式最适合该数据。
- en: 'The plots that you will generate should look similar to the ones in the following
    diagram:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你将生成的图表应该类似于下图所示：
- en: '![Figure 2.14: The expected scatter plots for all methods'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.14: 所有方法的预期散点图'
- en: '](img/C12626_02_14.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_14.jpg)'
- en: 'Figure 2.14: The expected scatter plots for all methods'
  id: totrans-145
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.14：所有方法的预期散点图
- en: Note
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity is on page 310.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这个活动的解决方案在第310页。
- en: Agglomerative versus Divisive Clustering
  id: totrans-148
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 聚合式与分裂式聚类
- en: 'Our instances of hierarchical clustering so far have all been agglomerative
    – that is, they have been built from the bottom up. While this is typically the
    most common approach for this type of clustering, it is important to know that
    it is not the only way a hierarchy can be created. The opposite hierarchical approach,
    that is, built from the top up, can also be used to create your taxonomy. This
    approach is called **Divisive** Hierarchical Clustering and works by having all
    the data points in your dataset in one massive cluster. Many of the internal mechanics
    of the divisive approach will prove to be quite similar to the agglomerative approach:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 迄今为止，我们的层次聚类实例都是聚合式的——也就是说，它们是从下往上构建的。虽然这种方法通常是这种类型聚类中最常见的做法，但重要的是要知道，它并不是创建层次结构的唯一方式。相反的层次方法，即从上往下构建，也可以用于创建分类法。这个方法叫做**分裂式**层次聚类，它的工作原理是将数据集中所有的数据点放在一个大的聚类中。分裂式方法的许多内部机制与聚合式方法非常相似：
- en: '![Figure 2.15: Agglomerative versus divisive hierarchical clustering'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.15：聚合式与分裂式层次聚类](img/C12626_02_15.jpg)'
- en: '](img/C12626_02_15.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_15.jpg)'
- en: 'Figure 2.15: Agglomerative versus divisive hierarchical clustering'
  id: totrans-152
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.15：聚合式与分裂式层次聚类
- en: As with most problems in unsupervised learning, deciding the best approach is
    often highly dependent on the problem you are faced with solving.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数无监督学习中的问题一样，选择最佳方法通常高度依赖于你所面临的具体问题。
- en: Imagine that you are an entrepreneur who has just bought a new grocery store
    and needs to stock it with goods. You receive a large shipment of food and drink
    in a container, but you've lost track of all the shipment information! In order
    to most effectively sell your products, you must group similar products together
    (your store will be a huge mess if you just put everything on the shelves in a
    random order). Setting out on this organizational goal, you can take either a
    bottom-up or top-down approach. On the bottom-up side, you will go through the
    shipping container and think of everything as disorganized – you will then pick
    up a random object and find its most similar product. For example, you may pick
    up apple juice and realize that it makes sense to group it together with orange
    juice. With the top-down approach, you will view everything as organized in one
    large group. Then, you will move through your inventory and split the groups based
    on the largest differences in similarity. For example, you may originally think
    that apple juice and tofu go together, but on second thoughts, they are really
    different. Therefore, you will break them into smaller, dissimilar groups.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一位刚刚购买了一家新杂货店的企业家，需要为商店进货。你收到了一个装满食物和饮料的大货柜，但你已经丢失了所有的货物信息！为了最有效地销售商品，你必须将类似的商品分组（如果你把所有东西随机摆放到货架上，商店将会是一团糟）。为了解决这个组织问题，你可以采取自下而上或自上而下的方法。从自下而上的角度，你会将整个运输容器视为杂乱无章——然后，你会拿起一个随机的物品，并找到它最相似的商品。例如，你可能会拿起苹果汁，意识到将它与橙汁放在一起是合理的。采用自上而下的方法，你会将所有物品视为一个大组。然后，你会遍历库存，并根据最大差异将这些组拆分开来。例如，你最初可能会认为苹果汁和豆腐是搭配的，但仔细想想，它们实际上非常不同。因此，你会把它们分成更小、不相似的组。
- en: In general, it helps to think of agglomerative as the bottom-up approach and
    divisive as the top-down approach – but how do they trade off in performance?
    Due to the greedy nature of Agglomerative, it has the potential to be fooled by
    local neighbors and not see the larger implications of clusters it forms at any
    given time. On the flip side, the divisive approach has the benefit of seeing
    the entire data distribution as one from the beginning and choosing the best way
    to break down clusters. This insight into what the entire dataset looks like is
    helpful for potentially creating more accurate clusters and should not be overlooked.
    Unfortunately, a top-down approach, typically, trades off greater accuracy with
    deeper complexity. In practice, an agglomerative approach works most of the time
    and should be the preferred starting point when it comes to hierarchical clustering.
    If, after reviewing the hierarchies, you are unhappy with the results, it may
    help to take a divisive approach.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，帮助理解聚合方法是自下而上的方式，而分裂方法是自上而下的方式——但它们在性能上如何权衡呢？由于聚合方法的贪婪性质，它可能会被局部邻居所迷惑，无法看到任何时候形成的聚类的更大意义。另一方面，分裂方法的好处在于，它从一开始就能看到整个数据分布，并选择最佳的方式来拆分聚类。了解整个数据集的分布有助于潜在地创建更准确的聚类，这一点不容忽视。不幸的是，通常情况下，顶层的分裂方法会以更深的复杂度来交换更高的准确度。在实际应用中，聚合方法通常能正常工作，并且在层次聚类中应该是优先的起始点。如果在检查层次结构后，你对结果不满意，可以考虑使用分裂方法。
- en: 'Exercise 8: Implementing Agglomerative Clustering with scikit-learn'
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 8：使用 scikit-learn 实现聚合层次聚类
- en: 'In most real-world use cases, you will likely find yourself implementing hierarchical
    clustering with a package that abstracts everything away, such as scikit-learn.
    Scikit-learn is a free package that is indispensable when it comes to machine
    learning in Python. It conveniently provides highly optimized forms of the most
    popular algorithms, such as regression, classification, and, of book, clustering.
    By using an optimized package such as scikit-learn, your work becomes much easier.
    However, you should only use it when you fully understand how hierarchical clustering
    works from the prior sections. The following exercise will compare two potential
    routes that you can take when forming clusters – using SciPy and scikit-learn.
    By completing the exercise, you will learn what the pros and cons are of each,
    and which suits you best from a user perspective:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数实际应用中，你可能会发现自己在使用一个将所有内容抽象化的包来实现层次聚类，比如 scikit-learn。Scikit-learn 是一个免费且不可或缺的
    Python 机器学习包。它便捷地提供了许多流行算法的高度优化版本，如回归、分类和聚类等。通过使用像 scikit-learn 这样的优化包，你的工作变得更加轻松。然而，只有在你完全理解了前面章节中层次聚类的工作原理后，才应使用它。以下练习将比较两种形成聚类的潜在方法——使用
    SciPy 和 scikit-learn。通过完成这个练习，你将了解它们各自的优缺点，以及从用户角度看，哪种方法最适合你：
- en: 'Scikit-learn makes implementation as easy as just a few lines of code:'
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Scikit-learn 使得实现变得非常简单，只需几行代码：
- en: '[PRE12]'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: First, we assign the model to the `ac` variable, by passing in parameters that
    we are familiar with, such as `affinity` (the distance function) and `linkage`
    (explore your options as we did in *Activity 2*, *Implementing Linkage Criteria*).
  id: totrans-160
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 首先，我们通过传入我们熟悉的参数，如 `affinity`（距离函数）和 `linkage`（如同我们在*活动 2*中所做的那样，探索你的选项，*实现连接准则*），将模型分配给
    `ac` 变量。
- en: After instantiating our model into a variable, we can simply pass through the
    dataset we are interested in in order to determine where the cluster memberships
    lie using `.fit_predict()` and assigning it to an additional variable.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在实例化模型到变量后，我们可以简单地传入我们感兴趣的数据集，并使用 `.fit_predict()` 来确定聚类的归属，并将结果分配给另一个变量。
- en: 'We can then compare how each of the approaches work by comparing the final
    cluster results through plotting. Let''s take a look at the clusters from the
    scikit-learn approach:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们可以通过绘图比较每种方法的最终聚类结果，看看 scikit-learn 方法的聚类：
- en: '[PRE13]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the output for the clusters from the scikit-learn approach:'
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 以下是 scikit-learn 方法聚类的输出：
- en: '![Figure 2.16: A plot of the Scikit-Learn approach'
  id: totrans-165
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '![图 2.16：Scikit-Learn 方法的图示'
- en: '](img/C12626_02_16.jpg)'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '](img/C12626_02_16.jpg)'
- en: 'Figure 2.16: A plot of the Scikit-Learn approach'
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.16：Scikit-Learn 方法的图示
- en: 'Take a look at the clusters from the SciPy Learn approach:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 看看 SciPy 学习方法的聚类：
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The output is as follows:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 输出如下：
- en: '![Figure 2.17: A plot of the SciPy approach'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.17：SciPy 方法的图示'
- en: '](img/C12626_02_17.jpg)'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_17.jpg)'
- en: 'Figure 2.17: A plot of the SciPy approach'
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.17：SciPy 方法的图示
- en: As you can see in our example problem, the two converge to basically the same
    clusters. While this is great from a toy-problem perspective, you will soon learn,
    in the next activity, that small changes to the input parameters can lead to wildly
    different results!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在我们的示例问题中看到的，两个方法最终收敛到了基本相同的聚类。虽然从玩具问题的角度来看这很好，但你很快会在下一个活动中学到，输入参数的微小变化可能会导致完全不同的结果！
- en: 'Activity 3: Comparing k-means with Hierarchical Clustering'
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 活动 3：比较 k-means 和层次聚类
- en: You are managing a store's inventory and receive a large shipment of wine, but
    the brand labels have fallen off the bottles during transit. Fortunately, your
    supplier has provided you with the chemical readings for each bottle, along with
    their respective serial numbers. Unfortunately, you aren't able to open each bottle
    of wine and taste test the difference – you must find a way to group the unlabeled
    bottles back together according to their chemical readings! You know from the
    order list that you ordered three different types of wine and are given only two
    wine attributes to group the wine types back together. In this activity, we will
    be using the wine dataset.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在管理一家商店的库存，并收到一大批葡萄酒，但在运输过程中瓶上的品牌标签掉了。幸运的是，供应商提供了每瓶酒的化学成分数据及其对应的序列号。不幸的是，你不能打开每一瓶酒进行品尝测试，你必须找到一种方法，根据化学成分将未标记的瓶子重新分组！你知道从订单列表中，自己订购了三种不同类型的葡萄酒，并且只给出了两种葡萄酒属性来将它们重新分组。在这个活动中，我们将使用葡萄酒数据集。
- en: Note
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The wine dataset can be downloaded from [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/).
    It can be accessed at [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activ](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activity03)ity03.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 葡萄酒数据集可以从 [https://archive.ics.uci.edu/ml/machine-learning-databases/wine/](https://archive.ics.uci.edu/ml/machine-learning-databases/wine/)
    下载。也可以通过 [https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activity03](https://github.com/TrainingByPackt/Applied-Unsupervised-Learning-with-Python/tree/master/Lesson02/Activity03)
    访问。
- en: 'UCI Machine Learning Repository [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)].
    Irvine, CA: University of California, School of Information and Computer Science'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: UCI 机器学习库 [[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)]。加利福尼亚州欧文市：加利福尼亚大学信息与计算机科学学院
- en: The aim of this activity is to implement k-means and hierarchical clustering
    on the wine dataset and to explore which approach ends up being more accurate
    or easier for you to use. You can try different combinations of scikit-learn implementations
    and use helper functions in SciPy and NumPy. You can use the silhouette score
    to compare the different clustering methods and visualize the clusters on a graph.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的目的是在葡萄酒数据集上实现 k-means 和层次聚类，并探索哪种方法最终更准确或更易于使用。你可以尝试不同的 scikit-learn 实现组合，并使用
    SciPy 和 NumPy 中的辅助函数。你可以使用轮廓系数（silhouette score）比较不同的聚类方法，并在图表上可视化聚类结果。
- en: 'Expected Outcome:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 预期结果：
- en: After completing this activity, you will have gained an understanding of how
    k-means and hierarchical clustering work on similar datasets. You will likely
    notice that one method performs better than the other depending on how the data
    is shaped. Another key outcome from this activity is gaining an understanding
    of how important hyperparameters are in any given use case.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 完成此活动后，你将理解 k-means 和层次聚类在相似数据集上的工作原理。你可能会注意到，根据数据的形状，一个方法比另一个方法表现得更好。此活动的另一个关键收获是理解在任何给定用例中超参数的重要性。
- en: 'Here are the steps to complete this activity:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是完成此活动的步骤：
- en: Import the necessary packages from scikit-learn (`KMeans`, `AgglomerativeClustering`,
    and `silhouette_score`).
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 scikit-learn 导入必要的包（`KMeans`、`AgglomerativeClustering` 和 `silhouette_score`）。
- en: Read the wine dataset into the pandas DataFrame and print a small sample.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将葡萄酒数据集读入 pandas DataFrame 并打印一个小样本。
- en: Visualize the wine dataset to understand its data structure.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化葡萄酒数据集，以理解其数据结构。
- en: Use the sklearn implementation of k-means on the wine dataset, knowing that
    there are three wine types.
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 sklearn 实现的 k-means 算法处理葡萄酒数据集，已知有三种葡萄酒类型。
- en: Use the sklearn implementation of hierarchical clustering on the wine dataset.
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在葡萄酒数据集上使用 sklearn 实现的层次聚类。
- en: Plot the predicted clusters from k-means.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制 k-means 聚类的预测结果。
- en: Plot the predicted clusters from hierarchical clustering.
  id: totrans-190
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制来自层次聚类的预测聚类。
- en: Compare the silhouette score of each clustering method.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较每种聚类方法的轮廓系数。
- en: 'Plot the predicted clusters from the k-means clustering method as follows:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，绘制来自 k-means 聚类方法的预测聚类：
- en: '![Figure 2.18: The expected clusters from the k-means method'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.18：k-means 方法的预期聚类'
- en: '](img/C12626_02_18.jpg)'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_18.jpg)'
- en: 'Figure 2.18: The expected clusters from the k-means method'
  id: totrans-195
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.18：k-means 方法的预期聚类
- en: 'Plot the predicted clusters from the agglomerative clustering method, as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如下所示，绘制来自凝聚层次聚类方法的预测聚类：
- en: '![Figure 2.19: The expected clusters from the agglomerative method'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 2.19：来自凝聚方法的预期聚类'
- en: '](img/C12626_02_19.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/C12626_02_19.jpg)'
- en: 'Figure 2.19: The expected clusters from the agglomerative method'
  id: totrans-199
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2.19：来自凝聚方法的预期聚类
- en: Note
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The solution for this activity is on page 312.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 本活动的解决方案见第 312 页。
- en: k-means versus Hierarchical Clustering
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: k-means 与层次聚类
- en: Now that we have expanded our understanding of how k-means clustering works,
    it is important to explore where hierarchical clustering fits into the picture.
    As mentioned in the linkage criteria section, there is some potential direct overlap
    when it comes to grouping data points together using centroids. Universal to all
    of the approaches mentioned so far, is also the use of a distance function to
    determine similarity. Due to our in-depth exploration in the previous chapter,
    we have kept using the Euclidean distance, but we understand that any distance
    function can be used to determine similarity.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经扩展了对 k-means 聚类如何工作的理解，接下来要探讨的是层次聚类在整个过程中所扮演的角色。如在连接准则部分所提到的，当使用质心进行数据点分组时，确实存在一些直接重叠的可能性。到目前为止，所有提到的方法都通用地使用了距离函数来确定相似度。由于我们在上一章的深入探讨，我们一直使用欧几里得距离，但我们理解，任何距离函数都可以用来确定相似度。
- en: 'In practice, here are some quick highlights for choosing one clustering method
    over another:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 实际操作中，选择某种聚类方法而非另一种方法时，有一些快速的要点：
- en: Hierarchical clustering benefits from not needing to pass in an explicit "k"
    number of clusters apriori. This means that you can find all the potential clusters
    and decide which clusters make the most sense after the algorithm has completed.
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类的优势在于不需要预先传入明确的“k”聚类数。这意味着你可以在算法完成后找到所有潜在的聚类，并决定哪些聚类最为合理。
- en: k-means clustering benefits from a simplicity perspective – oftentimes, in business
    use cases, there is a challenge to find methods that can be explained to non-technical
    audiences but still be accurate enough to generate quality results. k-means can
    easily fill this niche.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从简化的角度来看，k-means 聚类有其优势——在商业应用中，往往需要找到既能向非技术观众解释，又能生成高质量结果的方法。k-means 可以轻松地填补这一空白。
- en: Hierarchical clustering has more parameters to tweak than k-means clustering
    when it comes to dealing with abnormally shaped data. While k-means is great at
    finding discrete clusters, it can falter when it comes to mixed clusters. By tweaking
    the parameters in hierarchical clustering, you may find better results.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层次聚类在处理形状异常的数据时，比 k-means 聚类有更多的参数可供调整。虽然 k-means 非常擅长发现离散聚类，但当涉及到混合聚类时，它可能会出现问题。通过调整层次聚类中的参数，你可能会得到更好的结果。
- en: Vanilla k-means clustering works by instantiating random centroids and finding
    the closest points to those centroids. If they are randomly instantiated in areas
    of the feature space that are far away from your data, then it can end up taking
    quite some time to converge, or it may never even get to that point. Hierarchical
    clustering is less prone to falling prey to this weakness.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vanilla k-means 聚类通过初始化随机质心并找到与这些质心最接近的点来工作。如果它们在特征空间中被随机初始化在离数据很远的地方，那么可能需要相当长的时间才能收敛，或者甚至可能永远无法到达那个点。层次聚类则不太容易受到这种弱点的影响。
- en: Summary
  id: totrans-209
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'In this chapter, we discussed how hierarchical clustering works and where it
    may be best employed. In particular, we discussed various aspects of how clusters
    can be subjectively chosen through the evaluation of a dendrogram plot. This is
    a huge advantage compared to k-means clustering if you have absolutely no idea
    of what you''re looking for in the data. Two key parameters that drive the success
    of hierarchical clustering were also discussed: the agglomerative versus divisive
    approach and linkage criteria. Agglomerative clustering takes a bottom-up approach
    by recursively grouping nearby data together until it results in one large cluster.
    Divisive clustering takes a top-down approach by starting with the one large cluster
    and recursively breaking it down until each data point falls into its own cluster.
    Divisive clustering has the potential to be more accurate since it has a complete
    view of the data from the start; however, it adds a layer of complexity that can
    decrease the stability and increase the runtime.'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中，我们讨论了层次聚类的工作原理以及它最适合应用的场景。特别是，我们讨论了如何通过评估树状图（dendrogram）图来主观选择聚类的各个方面。如果你完全不知道数据中要寻找什么，这相较于
    k-means 聚类来说是一个巨大的优势。我们还讨论了驱动层次聚类成功的两个关键参数：聚合法和分裂法，以及连接准则。聚合法聚类采用自下而上的方法，通过递归地将相邻的数据组合在一起，直到形成一个大簇。而分裂法聚类则采用自上而下的方法，从一个大簇开始，递归地将其拆分，直到每个数据点都属于自己的簇。分裂法聚类由于从一开始就能完整地看到数据，具有更高的准确性；然而，它增加了一层复杂性，可能会降低稳定性并增加运行时间。
- en: 'Linkage criteria grapples with the concept of how distance is calculated between
    candidate clusters. We have explored how centroids can make an appearance again
    beyond k-means clustering, as well as single and complete linkage criteria. Single
    linkage finds cluster distances by comparing the closest points in each cluster,
    while complete linkage finds cluster distances by comparing more distant points
    in each cluster. From the understanding that you have gained in this chapter,
    you are now able to evaluate how both k-means and hierarchical clustering can
    best fit the challenge that you are working on. In the next chapter, we will cover
    a clustering approach that will serve us best in the highly complex data: **DBSCAN**
    (Density-Based Spatial Clustering of Applications with Noise).'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 连接准则处理的是如何计算候选簇之间距离的概念。我们已经探讨了质心如何在超越 k-means 聚类后再次出现，以及单链和完全链连接准则。单链连接通过比较每个簇中最接近的点来寻找簇间的距离，而完全链连接则通过比较每个簇中更远的点来寻找簇间的距离。从你在本章中获得的理解来看，你现在能够评估
    k-means 聚类和层次聚类如何最好地解决你所面临的挑战。在下一章中，我们将介绍一种在高度复杂数据中最适合我们的聚类方法：**DBSCAN**（基于密度的空间聚类噪声应用）。
