- en: '*Chapter 5*: Exploring Heuristic Search'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第五章*: 探索启发式搜索'
- en: '**Heuristic search** is the third out of four groups of hyperparameter tuning
    methods. The key difference between this group and the other groups is that all
    the methods that belong to this group work by performing *trial and error* to
    achieve the optimal solution. Similar to the acquisition function in Bayesian
    optimization (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*, Exploring
    Bayesian Optimization*), all methods in this group also employ the concept of
    *exploration versus exploitation*. **Exploration** means performing a search in
    the unexplored space to lower the probability of being stuck in the local optima,
    while **exploitation** means performing a search in the local space that is known
    to have a good chance of containing the optimal solution.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**启发式搜索**是四组超参数调整方法中的第三组。这一组与其他组的关键区别在于，属于这一组的所有方法都是通过进行*试错*来达到最优解。类似于贝叶斯优化中的获取函数（参见[*第四章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*），这一组的所有方法也采用了*探索与利用*的概念。**探索**意味着在未探索的空间中进行搜索，以降低陷入局部最优的概率，而**利用**则意味着在已知有很大可能包含最优解的局部空间中进行搜索。'
- en: In this chapter, we will discuss several methods that belong to the heuristic
    search group, including **simulated annealing** (**SA**), **genetic algorithms**
    (**GAs**), **particle swarm optimization** (**PSO**), and **Population-Based Training**
    (**PBT**). Similar to [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036), we
    will discuss the definition of each method, what the differences are between them,
    how they work, and the pros and cons of each method.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论属于启发式搜索组的几种方法，包括**模拟退火**（**SA**）、**遗传算法**（**GAs**）、**粒子群优化**（**PSO**）和**基于种群的训练**（**PBT**）。类似于[*第四章*](B18753_04_ePub.xhtml#_idTextAnchor036)，我们将讨论每种方法的定义、它们之间的区别、它们的工作原理以及每种方法的优缺点。
- en: By the end of this chapter, you will understand the concept of the aforementioned
    hyperparameter tuning methods that belong to the heuristic search group. You will
    be able to explain these methods with confidence when someone asks you, at both
    a high-level and detailed fashion, along with the pros and cons. Once you are
    confident enough to explain them to other people, this means you have understood
    the ins and outs of each method. Thus, in practice, you can understand what’s
    happening if there are errors or you don’t get the expected results; you will
    also know how to configure the method so that it matches your specific problem.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将理解上述属于启发式搜索组的超参数调整方法的概念。当有人以高层次和详细的方式询问你时，你将能够自信地解释这些方法，并包括它们的优缺点。一旦你足够自信地向其他人解释它们，这意味着你已经理解了每种方法的来龙去脉。因此，在实践中，如果你遇到错误或没有得到预期的结果，你可以理解发生了什么；你还将知道如何配置该方法，使其与你的特定问题相匹配。
- en: 'In this chapter, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将涵盖以下主题：
- en: Understanding simulated annealing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解模拟退火
- en: Understanding genetic algorithms
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解遗传算法
- en: Understanding particle swarm optimization
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解粒子群优化
- en: Understanding population based training
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解基于种群的训练
- en: Understanding simulated annealing
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解模拟退火
- en: '**SA** is the heuristic search method that is inspired by the process of **metal
    annealing** in metallurgy. This method is similar to the random search hyperparameter
    tuning method (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*, Exploring
    Exhaustive Search*), except for the existence of a criterion that guides how the
    hyperparameter tuning process works. In other words, SA is like a *smoothed version
    of random search*. Just like random search, it is suggested to use SA when each
    trial doesn’t take too much time and you have enough computational resources.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '**SA**是受冶金中**金属退火**过程启发的启发式搜索方法。这种方法与随机搜索超参数调整方法（参见[*第三章*](B18753_03_ePub.xhtml#_idTextAnchor031)*，探索穷举搜索*）类似，但存在一个指导超参数调整过程如何工作的标准。换句话说，SA就像是一个*平滑版的随机搜索*。就像随机搜索一样，建议在每次试验所需时间不多且你有足够的计算资源时使用SA。'
- en: In the metal annealing process, the metal is heated to a very high temperature
    for a certain time and slowly cooled to increase its strength, reducing its hardness
    and making it easier to work with. The goal of giving a very high heat is to excite
    the metal’s atoms so that they can move around freely and randomly. During this
    random movement, atoms usually tend to form a better configuration. Then, the
    slow cooling process is performed so that we can have a crystalline form of the
    material.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在金属退火过程中，金属被加热到非常高的温度一段时间，然后缓慢冷却以增加其强度，降低其硬度，使其更容易加工。给予非常高的热量的目的是激发金属的原子，使它们可以自由且随机地移动。在随机的移动过程中，原子通常会倾向于形成更好的配置。然后，执行缓慢的冷却过程，以便我们可以得到材料的晶体形式。
- en: Just like in the metal annealing process, SA works by randomly choosing the
    set of hyperparameters to be tested. At each trial, the method will consider some
    of the “neighbors” of the current set, randomly. If the acceptance criterion is
    met, then the method will change its focus to that “neighbor” set. The acceptance
    criterion is not a deterministic function, it is a stochastic function, which
    means probability comes into play during the process. This probabilistic way of
    deciding is similar to the cooling phase in the metal annealing process, where
    we accept a smaller number of bad hyperparameter sets as more parts of the search
    space are explored.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在金属退火过程中一样，SA通过随机选择要测试的超参数集来工作。在每次试验中，该方法将考虑当前集的一些“邻居”，随机地。如果满足接受准则，那么方法将改变其焦点到那个“邻居”集。接受准则不是一个确定性函数，它是一个随机函数，这意味着在过程中会涉及到概率。这种决定方式与金属退火过程中的冷却阶段相似，在那里我们接受更少的坏超参数集，因为更多的搜索空间被探索。
- en: SA is a modified version of one of the most popular heuristic optimization methods,
    known as **stochastic hill climbing** (**SHC**). SHC is very simple to understand
    and implement, which means that SA is as well. *In general*, SHC works by initializing
    the random point within a pre-defined bound (the *hyperparameter space, in our
    case*) and treating it as the current best solution. Then, it randomly searches
    for the next candidate within the surrounding of the selected point. Then, we
    need to compare the selected candidate with the current best solution. If the
    candidate is better than or equal to the current best solution, SHC will treat
    the candidate as the new best solution. This process is repeated until the stopping
    criterion is met.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: SA是最受欢迎的启发式优化方法之一——**随机爬山法**（**SHC**）的改进版本。SHC非常易于理解和实现，这意味着SA也是如此。*一般来说*，SHC通过在预定义的边界内初始化随机点（在我们的案例中是超参数空间）并将其视为当前最佳解来工作。然后，它会在所选点的周围随机搜索下一个候选点。然后，我们需要比较所选候选点与当前最佳解。如果候选点比当前最佳解更好或相等，SHC将候选点视为新的最佳解。这个过程会重复进行，直到满足停止准则。
- en: 'The following steps show how SHC optimization works in general:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤展示了SHC优化在一般情况下是如何工作的：
- en: Define the bound of the space, *B*, and the step size, *S*.
  id: totrans-15
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义空间边界*B*和步长*S*。
- en: Define the stopping criterion. Usually, it is defined as the number of iterations,
    but other stopping criteria definitions also work.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义停止准则。通常，它被定义为迭代次数，但其他停止准则的定义也适用。
- en: Initialize the random point within the bound, *B*.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在边界*B*内初始化随机点。
- en: Set the selected point from *Step 3* as the current point, *current_point*,
    as well as the best point, *best_point*.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将从*步骤3*中选定的点设置为当前点*current_point*，以及最佳点*best_point*。
- en: Randomly sample the next candidate within the *S* distance from *best_point*
    and within the bound, *B*, then store it as *candidate_point*.
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在距离*best_point*的*S*距离内，从边界*B*内随机采样下一个候选点，并将其存储为*candidate_point*。
- en: If *candidate_point* is better than or equal to *best_point*, then replace *best_point*
    with *candidate_point*.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果*candidate_point*比*best_point*更好或相等，则用*candidate_point*替换*best_point*。
- en: Replace *current_point* with *candidate_point*.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*current_point*替换为*candidate_point*。
- en: Repeat *Steps 5* to *7* until the stopping criterion is met.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤5*到*步骤7*，直到满足停止准则。
- en: 'The main difference between SA and SHC is located in *Steps 5* and *6*. In
    SHC, we always sample the next candidate from the surrounding of the *best_point*,
    while in SA, we sample from the surrounding of *current_point*. In SHC, we only
    accept a candidate that is better than or equal to the current best solution,
    while in SA, we *may also accept a worse candidate* with a certain probability
    that is guided by the acceptance criterion, *AC*, which is defined as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: SA（模拟退火）和SHC（基于热力学的冷却）之间的主要区别在于 *步骤 5* 和 *步骤 6*。在SHC中，我们总是从 *最佳点* 的周围采样下一个候选点，而在SA中，我们从
    *当前点* 的周围采样。在SHC中，我们只接受比当前最佳解更好或相等的候选点，而在SA中，我们 *可能也会以一定的概率接受更差的候选点*，这个概率由接受标准
    *AC* 指导，该标准定义如下：
- en: '![](img/Formula_B18753_05_001.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B18753_05_001.png](img/Formula_B18753_05_001.png)'
- en: Here, ![](img/Formula_B18753_05_002.png) , ![](img/Formula_B18753_05_003.png)is
    the objective function and ![](img/Formula_B18753_05_004.png) is *temperature*
    with a positive value. See [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)
    if you are not familiar with the objective function term.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![公式 B18753_05_002.png](img/Formula_B18753_05_002.png) 和 ![公式 B18753_05_003.png](img/Formula_B18753_05_003.png)
    是目标函数，![公式 B18753_05_004.png](img/Formula_B18753_05_004.png) 是具有正值的 *温度*。如果您不熟悉目标函数术语，请参阅
    [*第 4 章*](B18753_04_ePub.xhtml#_idTextAnchor036)。
- en: The ![](img/Formula_B18753_05_005.png)formula results in a value between 0 and
    1, where it always results in a value of 1 when the *candidate_point* is better
    than or equal to the *current_point*. In other words, we always accept the *candidate_point*
    when it is better than or equal to the *current_point*. It is worth noting that
    *better* does not necessarily mean has a greater value. If you are working with
    a maximization problem, then better means greater. However, if you are working
    with a minimization problem, then it is the other way around. For example, if
    the cross-validation score you are measuring is the **mean squared error** (**MSE**),
    where a lower score corresponds to better performance, then the *candidate_point*
    is considered better than the *current_point* if the ![](img/Formula_B18753_05_006.png)
    value is less than ![](img/Formula_B18753_05_007.png).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '![公式 B18753_05_005.png](img/Formula_B18753_05_005.png) 公式的结果在 0 和 1 之间，当 *候选点*
    比当前点更好或相等时，它总是产生 1 的值。换句话说，当 *候选点* 比当前点更好或相等时，我们总是接受它。值得注意的是，*更好* 并不一定意味着具有更大的值。如果您正在处理一个最大化问题，那么更好意味着更大。然而，如果您正在处理一个最小化问题，那么情况则相反。例如，如果您正在测量的交叉验证分数是
    **均方误差**（MSE），其中较低的分数对应于更好的性能，那么如果 ![公式 B18753_05_006.png](img/Formula_B18753_05_006.png)
    的值小于 ![公式 B18753_05_007.png](img/Formula_B18753_05_007.png)，则认为 *候选点* 比当前点更好。'
- en: 'Although ![](img/Formula_B18753_05_008.png) is impacted by both ![](img/Formula_B18753_05_009.png)
    and ![](img/Formula_B18753_05_010.png), we can only control the value of ![](img/Formula_B18753_05_011.png).
    In practice, the *initial value* of ![](img/Formula_B18753_05_012.png) *is treated
    as a hyperparameter* and is usually set to a high value. Over the number of trials,
    the value of ![](img/Formula_B18753_05_013.png) is decreased following the so-called
    **annealing schedule** or cooling schedule scheme. There are several annealing
    schedule schemes that we can follow. The three most popular schemes are as follows:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管以下公式 ![公式 B18753_05_008.png](img/Formula_B18753_05_008.png) 受到 ![公式 B18753_05_009.png](img/Formula_B18753_05_009.png)
    和 ![公式 B18753_05_010.png](img/Formula_B18753_05_010.png) 的影响，但我们只能控制 ![公式 B18753_05_011.png](img/Formula_B18753_05_011.png)
    的值。在实践中，![公式 B18753_05_012.png](img/Formula_B18753_05_012.png) 的 *初始值* 被视为一个超参数，通常设置为一个较高的值。在多次试验中，![公式
    B18753_05_013.png](img/Formula_B18753_05_013.png) 的值会根据所谓的 **退火计划** 或冷却计划方案而降低。我们可以遵循几种退火计划方案。以下是最受欢迎的三种方案：
- en: '**Geometric cooling**: This annealing schedule works by decreasing the temperature
    via a cooling factor of ![](img/Formula_B18753_05_014.png). In geometric cooling,
    the initial temperature, ![](img/Formula_B18753_05_015.png), is multiplied by
    the cooling factor ![](img/Formula_B18753_05_016.png) number of times, where ![](img/Formula_B18753_05_017.png)
    is the current number of iterations:'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**几何冷却**：这种退火计划通过冷却因子 ![公式 B18753_05_014.png](img/Formula_B18753_05_014.png)
    降低温度。在几何冷却中，初始温度 ![公式 B18753_05_015.png](img/Formula_B18753_05_015.png) 被冷却因子
    ![公式 B18753_05_016.png](img/Formula_B18753_05_016.png) 乘以 ![公式 B18753_05_017.png](img/Formula_B18753_05_017.png)
    次数，其中 ![公式 B18753_05_017.png](img/Formula_B18753_05_017.png) 是当前的迭代次数：'
- en: '![](img/Formula_B18753_05_018.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![公式 B18753_05_018.png](img/Formula_B18753_05_018.png)'
- en: 'This can be seen in the following graph:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在以下图表中看到：
- en: '![Figure 5.1 – Effect of the initial temperature in geometric cooling on the
    acceptable criterion'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 几何冷却中初始温度对可接受标准的影响](img/Formula_B18753_05_001.png)'
- en: '](img/B18753_05_001.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18753_05_001.jpg](img/B18753_05_001.jpg)'
- en: Figure 5.1 – Effect of the initial temperature in geometric cooling on the acceptable
    criterion
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 几何冷却中初始温度对可接受标准的影响
- en: '**Linear cooling**: This annealing schedule works by decreasing the temperature
    linearly via a cooling factor, ![](img/Formula_B18753_05_019.png). The value of
    ![](img/Formula_B18753_05_020.png)is chosen in such a way that ![](img/Formula_B18753_05_021.png)will
    still have a positive value after ![](img/Formula_B18753_05_022.png)iterations.
    For example, ![](img/Formula_B18753_05_023.png), where ![](img/Formula_B18753_05_024.png)
    is the expected final temperature after ![](img/Formula_B18753_05_025.png) iterations:'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性冷却**：这种退火计划通过冷却因子线性降低温度，![](img/Formula_B18753_05_019.png)。选择![](img/Formula_B18753_05_020.png)的值，使得![](img/Formula_B18753_05_021.png)在![](img/Formula_B18753_05_022.png)次迭代后仍然保持正值。例如，![](img/Formula_B18753_05_023.png)，其中![](img/Formula_B18753_05_024.png)是经过![](img/Formula_B18753_05_025.png)次迭代后的预期最终温度：'
- en: '![](img/Formula_B18753_05_026.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_026.png)'
- en: 'The following graph shows this annealing schedule:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图表显示了这种退火计划：
- en: '![Figure 5.2 – Effect of the initial temperature in linear cooling on the acceptable
    criterion'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.2 – 线性冷却中初始温度对可接受标准的影响'
- en: '](img/B18753_05_002.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_05_002.jpg)'
- en: Figure 5.2 – Effect of the initial temperature in linear cooling on the acceptable
    criterion
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 – 线性冷却中初始温度对可接受标准的影响
- en: '**Fast SA**: This annealing schedule works by decreasing the temperature proportional
    to the current number of iterations, ![](img/Formula_B18753_05_027.png):'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**快速SA**：这种退火计划通过按当前迭代次数成比例降低温度来实现，![](img/Formula_B18753_05_027.png)：'
- en: '![](img/Formula_B18753_05_028.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_028.png)'
- en: 'This annealing schedule can be seen in the following graph:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这种退火计划可以在以下图表中看到：
- en: '![Figure 5.3 – Effect of the initial temperature in fast SA on the acceptable
    criterion'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.3 – 快速SA中初始温度对可接受标准的影响'
- en: '](img/B18753_05_003.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_05_003.jpg)'
- en: Figure 5.3 – Effect of the initial temperature in fast SA on the acceptable
    criterion
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 – 快速SA中初始温度对可接受标准的影响
- en: Based on *Figures 5.1* to *5.3*, we can see that no matter what annealing schedule
    scheme we use and what the initial temperature is, we will always have a lower
    ![](img/Formula_B18753_05_029.png) value as the number of iterations increases,
    which means we will *accept fewer bad candidates as the number of iterations increases*.
    However, why do we need to accept bad candidates in the first place? The main
    purpose of SA not directly rejecting worse candidates, as in the SHC method, is
    to *balance the exploration and exploitation trade-off*. The high initial value
    of temperature allows SA to explore most of the parts of the hyperparameter space,
    and slowly focus on specific parts of the space as the number of iterations increases,
    just like how the metal annealing process works.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图5.1至图5.3，我们可以看到，无论我们使用什么退火计划方案以及初始温度是多少，随着迭代次数的增加，我们总是会得到一个更低的![](img/Formula_B18753_05_029.png)值，这意味着随着迭代次数的增加，我们将*接受更少的坏候选者*。然而，我们最初为什么要接受坏候选者呢？模拟退火（SA）方法的主要目的不是直接拒绝更差的候选者，正如在SHC方法中那样，而是*平衡探索和利用的权衡*。较高的初始温度值允许SA探索超参数空间的大部分区域，随着迭代次数的增加，逐渐聚焦于空间的具体部分，就像金属退火过程一样。
- en: 'Remember that ![](img/Formula_B18753_05_030.png)only takes ![](img/Formula_B18753_05_031.png)into
    account when the *candidate_point* is worse than the *current_point.* This means
    that, based on *Figure 5.4*, we can say that the worse the suggested candidate
    is (the higher ![](img/Formula_B18753_05_032.png) is), the lower the value of
    ![](img/Formula_B18753_05_033.png)will be, and thus, the lower the probability
    of accepting the suggested bad candidate. This is the other way around for ![](img/Formula_B18753_05_034.png)
    in that the higher the value of ![](img/Formula_B18753_05_035.png)is, the higher
    the value of ![](img/Formula_B18753_05_036.png)will be, and thus, the higher the
    probability of accepting the suggested bad candidate (see *Figures 5.1* to *5.3*):'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，![](img/Formula_B18753_05_030.png)仅在*候选点*比*当前点*更差时才考虑![](img/Formula_B18753_05_031.png)。这意味着，根据图5.4，我们可以这样说，建议的候选者越差（![](img/Formula_B18753_05_032.png)越高），![](img/Formula_B18753_05_033.png)的值就越低，因此，接受建议的坏候选者的概率就越低。对于![](img/Formula_B18753_05_034.png)来说，情况正好相反，![](img/Formula_B18753_05_035.png)的值越高，![](img/Formula_B18753_05_036.png)的值就越高，因此，接受建议的坏候选者的概率就越高（参见图5.1至图5.3）：
- en: '![Figure 5.4 – Effect of Δf on the acceptable criterion'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.4 – Δf对可接受标准的影响'
- en: '](img/B18753_05_004.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_05_004.jpg)'
- en: Figure 5.4 – Effect of Δf on the acceptable criterion
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 – Δf对可接受标准的影响
- en: 'To summarize, the following steps show how *SA works as a hyperparameter tuning
    method*:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，以下步骤展示了 *SA* 作为超参数调整方法的工作原理：
- en: Split the original full data into train and test sets (see [*Chapter 1*](B18753_01_ePub.xhtml#_idTextAnchor014)*,
    Evaluating Machine Learning Models*).
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据分割成训练集和测试集（见 [*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*）。
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用伴随的分布定义超参数空间，*H*。
- en: Define the initial temperature, *T0*.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义初始温度，*T0*。
- en: Define the objective function, *f*, based on the train set (see [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)).
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集定义目标函数，*f*（见 [*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）。
- en: Define the stopping criterion. Usually, the number of trials is used. However,
    it is also possible to use the time taken or convergence as the stopping criterion.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义停止标准。通常使用试验次数。然而，也可以使用时间或收敛性作为停止标准。
- en: Set the current temperature, *T*, using the value from *T0*.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *T0* 的值设置当前温度，*T*。
- en: Initialize a random set of hyperparameters that have been sampled from the hyperparameter
    space, *H.*
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一组从超参数空间，*H*，中采样的随机超参数。
- en: Set the selected set from *Step 7* as the current set, *current_set*, as well
    as the best set, *best_set*.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将从 *步骤 7* 中选择的集合设置为当前集合，*current_set*，以及最佳集合，*best_set*。
- en: Randomly sample the next candidate set, *candidate_set*, from the “neighbor”
    of the *current_set* within the hyperparameter space, *H*. The definition of the
    “neighbor” may differ across different types of hyperparameter distributions.
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从超参数空间，*H*，的“邻居”中随机采样下一个候选集合，*candidate_set*。不同类型的超参数分布中“邻居”的定义可能不同。
- en: Generate a random number between 0 and 1 from the uniform distribution and store
    it as *rnd*.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从均匀分布中生成一个介于0和1之间的随机数，并将其存储为 *rnd*。
- en: 'Decide whether to accept the *candidate_set* or not:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定是否接受 *candidate_set*：
- en: Calculate the value of ![](img/Formula_B18753_05_038.png) using the value of
    *T*, *f(candidate_set)*, and *f(current_set)*.
  id: totrans-63
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *T*、*f(candidate_set)* 和 *f(current_set)* 的值计算 ![](img/Formula_B18753_05_038.png)
    的值。
- en: If the value of *rnd* is smaller than ![](img/Formula_B18753_05_039.png), then
    replace *current_set* with *candidate_set*.
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *rnd* 的值小于 ![](img/Formula_B18753_05_039.png)，则用 *candidate_set* 替换 *current_set*。
- en: If *candidate_set* is better than or equal to *current_set*, then replace *best_set*
    with *candidate_set*.
  id: totrans-65
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *candidate_set* 比 *current_set* 更好或相等，则用 *candidate_set* 替换 *best_set*。
- en: Apply the annealing schedule to the temperature, *T*.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将退火计划应用于温度，*T*。
- en: Repeat *Steps 9* to *12* until the stopping criterion is met.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *步骤 9* 到 *12*，直到满足停止标准。
- en: Train on the full training set using the *best_set* hyperparameters.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 *best_set* 超参数在完整训练集上训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练的模型。
- en: 'The following table lists the list of pros and cons of SA as a hyperparameter
    tuning method:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了SA作为超参数调整方法的优缺点列表：
- en: '![Figure 5.5 – Pros and cons of SA'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.5 – SA的优缺点'
- en: '](img/B18753_05_005.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_05_005.jpg)'
- en: Figure 5.5 – Pros and cons of SA
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 – SA的优缺点
- en: In this section, we learned about SA, starting from what it is, how it works,
    what makes it different from SHC and random search, and its pros and cons. We
    will discuss another interesting heuristic search method that is inspired by the
    natural selection theory in the next section.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们从了解SA是什么、它是如何工作的、它区别于SHC和随机搜索的特点，以及它的优缺点开始，学习了SA。在下一节中，我们将讨论另一个有趣的启发式搜索方法，该方法受到自然选择理论的影响。
- en: Understanding genetic algorithms
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解遗传算法
- en: '**GAs** are popular heuristic search methods that are inspired by Charles Darwin’s
    *theory of natural selection*. Unlike SA, which is classified as a **single-point-based**
    heuristic search method, GAs are categorized as **population-based** methods since
    they maintain a group of possible candidate solutions instead of just a single
    candidate solution at each trial. As a hyperparameter tuning method, you are recommended
    to utilize a GA when each trial doesn’t take too much time and you have enough
    computational resources, such as parallel computing resources.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**遗传算法（GAs**）是受查尔斯·达尔文*自然选择理论*启发的流行启发式搜索方法。与被归类为**单点基于**启发式搜索方法的SA不同，GAs被归类为**基于群体**的方法，因为它们在每个试验中维护一组可能的候选解，而不是只维护一个候选解。作为一个超参数调整方法，当每个试验不需要太多时间并且你有足够的计算资源，如并行计算资源时，建议使用GA。'
- en: To have a better understanding of GAs, let’s start with a simple example. Let’s
    say we have a task to generate a pre-defined target word based on *only* a collection
    of words that are built from 26 alphabet letters in lowercase. For instance, the
    target word is “big,” and we have a collection that consists of the words “sea,”
    “pig,” “dog,” “bus,” and “tie.”
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解遗传算法（GAs），让我们从一个简单的例子开始。假设我们有一个任务，基于**仅**由26个小写字母组成的单词集合生成一个预定义的目标单词。例如，目标单词是“big”，我们有一个由单词“sea”、“pig”、“dog”、“bus”和“tie”组成的集合。
- en: Based on the given collection of words, what should we do to generate the word
    “big?” It is no doubt a very easy and straightforward task. We just have to pick
    the letter “b” from the word “bus,” “i” from the word “pig” or “tie,” and “g”
    from the word “dog.” Voila! We get the word “big.” You may be wondering how this
    example is related to the GA method or even the natural selection theory. This
    example is a very simple task and there is no need to utilize a GA to solve the
    problem. However, we need this kind of example so that you have a better understanding
    of how GAs work since you already know the correct answer in the first place.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于给定的单词集合，我们应该如何生成“big”这个单词？这无疑是一个非常简单且直接的任务。我们只需从“bus”这个单词中挑选字母“b”，从“pig”或“tie”这个单词中挑选字母“i”，从“dog”这个单词中挑选字母“g”。就这样！我们得到了“big”这个单词。你可能想知道这个例子如何与GA方法或自然选择理论相关。这个例子是一个非常简单的任务，没有必要利用GA来解决问题。然而，我们需要这样的例子，以便你更好地理解GAs是如何工作的，因为你一开始就知道正确的答案。
- en: To solve this task using GA, you must know the three key items in GA related
    to the evolution theory. The first key item is **variation**. Imagine if the given
    collection of words consists of only the word “sea.” There’s no way we can generate
    the word “big” based on only the word “sea.” This is why variation is needed in
    the **initial population** (*the collection of words, in our example*). Without
    enough variation, we may not be able to achieve the optimal solution (*to generate
    the word “big,” in our example*) since there is no **individual** (*each word
    in the collection of words, in our example*) within the population that can evolve
    to the target word.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用遗传算法（GA）解决这个问题，你必须了解与进化理论相关的GA中的三个关键项目。第一个关键项目是**变异**。想象一下，如果给定的单词集合只包含“sea”这个单词。我们无法仅基于“sea”这个单词生成“big”这个单词。这就是为什么在**初始群体**（在我们的例子中是单词集合）中需要变异。如果没有足够的变异，我们可能无法达到最优解（在我们的例子中是生成“big”这个单词），因为群体中没有**个体**（在我们的例子中是单词集合中的每个单词）能够进化到目标单词。
- en: Important Note
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Population is *not the hyperparameter space*. In GAs or other population-based
    heuristic search methods, population refers to the candidates of the optimal hyperparameter
    set.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 人口**不是超参数空间**。在遗传算法（GAs）或其他基于群体的启发式搜索方法中，群体指的是最优超参数集的候选者。
- en: The second key item is **selection**. You can think of this item as being similar
    to the idea of natural selection that happens in the real world. It’s about selecting
    individuals that are more suitable for the surrounding environment (*words that
    are similar to the word “big,” in our example*) and thus can survive in the world.
    In GAs, we need quantitative guidance for us to perform the selection, which is
    usually called the **fitness function**. This function helps us judge how good
    an individual is concerning the objective we want to achieve. In our example,
    we can create a fitness function that measures the proportion of indexes of the
    word that has the same letters as the target word in the corresponding indexes.
    For example, the word “tie” has a fitness score of ![](img/Formula_B18753_05_041.png)
    since only one out of three indexes contains the same letters as the target word,
    which is the index one that has the letter “i.”
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个关键项目是**选择**。你可以将这个项目视为类似于现实世界中发生的自然选择的概念。这是关于选择更适合周围环境的个体（在我们的例子中是类似于“big”这个单词的单词）并且因此能够在世界上生存下来。在GAs中，我们需要定量指导来执行选择，这通常被称为**适应度函数**。这个函数帮助我们判断一个个体相对于我们想要实现的目标有多好。在我们的例子中，我们可以创建一个适应度函数，该函数衡量单词中与目标单词在相应索引中具有相同字母的索引的比例。例如，“tie”这个单词的适应度分数为
    ![](img/Formula_B18753_05_041.png)，因为只有一个索引包含与目标单词相同的字母，即索引一，它包含字母“i”。
- en: Using this fitness function, we can evaluate the fitness score for each individual
    in the population, and then select which individuals should be added to the **mating
    pool** as **parents**. The mating pool is a collection of individuals that are
    considered high-quality individuals and thus called parents.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个适应度函数，我们可以评估群体中每个个体的适应度分数，然后选择哪些个体应该被添加到**配对池**中作为**父母**。配对池是一组被认为是高质量个体的集合，因此被称为父母。
- en: 'The third key item is **heredity**. This item refers to the concept of **reproduction**
    or passing parents’ **genes** (*each letter in the word, in our example*) to their
    children or **offspring**. How is reproduction done in GAs? Taking the same spirit
    of natural selection, in Gas, we only perform the reproduction step from parents
    in the mating pool, meaning we only want to mate high-quality individuals with
    the hope to get only high-quality offspring in the next **generation** (a *new
    population is created in the next iteration*). There are two steps in the reproduction
    phase, namely the **crossover** and **mutation** steps. The crossover step is
    when we randomly mix or permute parents’ genes to generate offspring’s genes,
    while the mutation step is when we randomly change the value of offspring’s genes
    to add variation to the genes (see *Figure 5.6*). An individual that is mutated
    is called a **mutant**. The random value that is used in the mutation step should
    be drawn from the same gene’s distribution, meaning we can only use lower-case
    letters as the random values in our example, not floating points or integers:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个关键要素是**遗传**。这个要素指的是**繁殖**或传递父母的**基因**（在我们的例子中是单词中的每个字母）给他们的孩子或**后代**。在GA中如何进行繁殖？遵循自然选择的相同精神，在GA中，我们只从配对池中的父母进行繁殖步骤，这意味着我们只想让高质量个体交配，希望在下一次**世代**（在下一个迭代中创建一个新的群体）中得到只有高质量的后代。繁殖阶段有两个步骤，即**交叉**和**变异**步骤。交叉步骤是我们随机混合或排列父母的基因以生成后代的基因，而变异步骤是我们随机改变后代的基因值以增加基因的变异（参见*图5.6*）。被变异的个体被称为**突变体**。变异步骤中使用的随机值应该来自相同的基因分布，这意味着在我们的例子中，我们只能使用小写字母作为随机值，不能使用浮点数或整数：
- en: '![Figure 5.6 – The crossover and mutation steps in a GA'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.6 – 遗传算法中的交叉和变异步骤]'
- en: '](img/B18753_05_006.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_05_006.jpg]'
- en: Figure 5.6 – The crossover and mutation steps in a GA
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 – 遗传算法中的交叉和变异步骤
- en: Now that you are aware of the three key items in a GA, we can start solving
    the task from the previous example using a GA. Let’s assume we haven’t been given
    the collection of words so that we can learn the complete procedures of the GA.
    The target word is still “big.”
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了遗传算法（GA）中的三个关键要素，我们可以从上一个示例开始使用GA来解决问题。假设我们没有给出单词集合，这样我们可以学习GA的完整过程。目标单词仍然是“big”。
- en: First, we must initialize a population with the *NPOP* number of individuals.
    The initialization process is usually done randomly to ensure we have enough variation
    in the population. By random, this means that the genes of each individual in
    the population are generated randomly. Let’s say we want to generate the initial
    population, which consists of seven individuals, where the generated results are
    “bee,” “tea,” “pie,” “bit,” “dog,” “cat,” and “dig.”
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须使用*NPOP*数量初始化一个个体群体。初始化过程通常是随机进行的，以确保我们在群体中拥有足够的变异。这里的随机意味着群体中每个个体的基因都是随机生成的。假设我们想要生成一个初始群体，该群体由七个个体组成，生成的结果为“bee”、“tea”、“pie”、“bit”、“dog”、“cat”和“dig”。
- en: Now, we can evaluate the fitness score of each individual in the population.
    Let’s say we use the fitness function that was defined previously. So, we got
    the following scores for each individual; “bee:” ![](img/Formula_B18753_05_042.png),
    “tea:” ![](img/Formula_B18753_05_043.png), “pie:” ![](img/Formula_B18753_05_044.png),
    “bit:” ![](img/Formula_B18753_05_045.png), ”dog:” ![](img/Formula_B18753_05_046.png),
    “cat:” ![](img/Formula_B18753_05_047.png), and “dig:” ![](img/Formula_B18753_05_048.png).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以评估群体中每个个体的适应度分数。假设我们使用之前定义的适应度函数。因此，我们得到了每个个体的以下分数；“bee:” ![](img/Formula_B18753_05_042.png)，“tea:”
    ![](img/Formula_B18753_05_043.png)，“pie:” ![](img/Formula_B18753_05_044.png)，“bit:”
    ![](img/Formula_B18753_05_045.png)，”dog:” ![](img/Formula_B18753_05_046.png)，“cat:”
    ![](img/Formula_B18753_05_047.png)，和“dig:” ![](img/Formula_B18753_05_048.png)。
- en: Based on the fitness score of each individual, we can select which individual
    should be added to the mating pool as a parent. There are many strategies that
    we can adopt to select the best individuals from the population, but in this case,
    let’s just get the top three individuals based on the fitness score and randomly
    select individuals that have the same fitness score. Let’s say that, after running
    the selection strategy, we get a mating pool that consists of “bit,” “dig,” and
    “bee” as parents.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 根据每个个体的适应度分数，我们可以选择哪些个体应该作为父母添加到交配池中。我们可以采用许多策略来从种群中选择最佳个体，但在这个例子中，我们只是根据适应度分数选择前三个个体，并随机选择具有相同适应度分数的个体。比如说，在执行选择策略后，我们得到了一个由“bit”、“dig”和“bee”作为父母的交配池。
- en: The next step is to perform the crossover and mutation steps. Before that, however,
    we need to specify the crossover probability, *CXPB*, and the mutation probability,
    *MUTPB*, which defines the probability of crossing two parents in the mating pool
    and mutating an offspring, respectively. This means we are neither performing
    crossover on all parent pairs nor mutating all offspring – we will only perform
    those steps based on the predefined probability. Let’s say that only “dig” and
    “bee” have chosen to be crossed, and the resulting offspring of the crossover
    is “deg” and “bie.” So, the mating pool currently consists of “bit,” “deg,” and
    “bie.” Now, we need to perform mutation on “deg” and “bie.” Let’s say that after
    mutating them, we got “den” and “tie.” This means that the mating pool is currently
    consisting of “bit,” “den,” and “tie.”
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是执行交叉和变异步骤。然而，在此之前，我们需要指定交叉概率*CXPB*和变异概率*MUTPB*，这定义了在交配池中交叉两个父母和变异一个后代的概率。这意味着我们既不对所有父母对执行交叉，也不对所有后代执行变异——我们只会根据预定义的概率执行这些步骤。比如说，只有“dig”和“bee”选择了交叉，交叉的结果是“deg”和“bie”。所以，当前的交配池由“bit”、“deg”和“bie”组成。现在，我们需要对“deg”和“bie”执行变异。比如说，变异后，我们得到了“den”和“tie”。这意味着当前的交配池由“bit”、“den”和“tie”组成。
- en: After performing the crossover and mutation steps, we need to generate a new
    population for the next generation. The new population will consist of all crossed
    parents, mutated offspring, as well as other individuals from the current population.
    So, the next population consists of “bit,” “den,” “tie,” “tea,” “pie,” “dog,”
    and “cat.”
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行交叉和变异步骤之后，我们需要为下一代生成一个新的种群。这个新种群将包括所有交叉的父母、变异的后代以及来自当前种群的其他个体。因此，下一个种群将包括“bit”、“den”、“tie”、“tea”、“pie”、“dog”和“cat”。
- en: Based on the new population, we have to repeat the selection, crossover, and
    mutation process. This procedure needs to be done *NGEN* times, where NGEN refers
    to the number of generations, and it is predefined by the developer.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 根据新种群，我们必须重复选择、交叉和变异过程。这个程序需要执行*NGEN*次，其中NGEN代表代数，由开发者预先定义。
- en: 'The following steps define *how GA works in general*, as an optimization method:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤定义了通用遗传算法（GA）作为优化方法的工作方式：
- en: Define the population size, *NPOP*, the crossover probability, *CXPB*, the mutation
    probability, *MUTPB*, and the number of generations or number of trials, *NGEN*.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义种群大小*NPOP*、交叉概率*CXPB*、变异概率*MUTPB*以及代数或试验次数*NGEN*。
- en: Define the fitness function, *f*.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义适应度函数*f*。
- en: Initialize a population with *NPOP* individuals, where each individual’s genes
    are initialized randomly.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*NPOP*个个体初始化一个种群，其中每个个体的基因都是随机初始化的。
- en: Evaluate all individuals in the population based on the fitness function, *f*.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据适应度函数*f*评估种群中的所有个体。
- en: Select the best individuals based on *Step 4* and store them in a mating pool.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据*步骤 4*选择最佳个体并将它们存储在交配池中。
- en: Perform the crossover process on the parents in the mating pool with a probability
    of *CXPB*.
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以*CXPB*的概率对交配池中的父母执行交叉过程。
- en: Perform the mutation process on the offspring results from *Step 8* with a probability
    of *MUTPB*.
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以*MUTPB*的概率对*步骤 8*产生的后代执行变异过程。
- en: Generate a new population consisting of all the individuals from *Step 6*, *Step
    7*, and the rest of the individuals from the current population.
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个新的种群，该种群由来自*步骤 6*、*步骤 7*以及当前种群中剩余的所有个体组成。
- en: Replace the current population with the new population.
  id: totrans-104
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用新种群替换当前种群。
- en: Repeat *Steps 6* to *9* *NGEN* times.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤 6*到*步骤 9*共*NGEN*次。
- en: 'Now, let’s look at a more concrete example of how a GA works in general. We
    will use the same objective function that we used in [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Evaluating Machine Learning Models* and treat this as a minimization problem.
    The objective function is defined as follows:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一个更具体的例子，说明 GA 通常是如何工作的。我们将使用与 [*第 4 章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，评估机器学习模型*
    中相同的目标函数，并将其视为一个最小化问题。目标函数定义如下：
- en: '![](img/Formula_B18753_05_049.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_049.png)'
- en: Here, ![](img/Formula_B18753_05_050.png) is the noise that follows the standard
    normal distribution. We are only going to perform a search within the ![](img/Formula_B18753_05_051.png)
    range. It is worth noting that in this example, we assume that we know what the
    true objective function is. However, in practice, this function is unknown. In
    this case, each individual will only have one gene, which is the value of ![](img/Formula_B18753_05_052.png)
    itself.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B18753_05_050.png) 是遵循标准正态分布的噪声。我们只将在 ![](img/Formula_B18753_05_051.png)
    范围内进行搜索。值得注意的是，在这个例子中，我们假设我们知道真正的目标函数是什么。然而，在实践中，这个函数是未知的。在这种情况下，每个个体将只有一个基因，即
    ![](img/Formula_B18753_05_052.png) 的值本身。
- en: Let’s say we define the hyperparameters for the GA method as *NPOP = 25*, *CXPB
    = 0.5*, *MUTPB = 0.15*, and *NGEN = 6*. As for the strategy of each **genetic
    operator**, we are using the **Tournament**, **Blend**, and **PolynomialBounded**
    strategies for selection, crossover, and mutation operators, respectively. The
    *Tournament* selection strategy works by selecting the best individuals among
    *tournsize* and the randomly chosen individual’s *NPOP* times, where *tournsize*
    is the number of individuals participating in the tournament. The *Blend* crossover
    strategy works by performing a linear combination between two continuous individual
    genes, where the weight of the linear combination is governed by the *alpha* hyperparameter.
    The *PolynomialBounded* mutation strategy works by passing continuous individual
    genes to a predefined polynomial mapping.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们定义 GA 方法的超参数为 *NPOP = 25*, *CXPB = 0.5*, *MUTPB = 0.15*, 和 *NGEN = 6*。至于每个
    **遗传算子** 的策略，我们分别使用 **锦标赛**、**混合** 和 **多项式边界** 策略来进行选择、交叉和变异操作。*锦标赛* 选择策略通过在 *tournsize*
    个个体中选出最佳个体，以及随机选择的个体的 *NPOP* 倍，其中 *tournsize* 是参加锦标赛的个体数量。*混合* 交叉策略通过执行两个连续个体基因的线性组合来实现，其中线性组合的权重由
    *alpha* 超参数控制。*多项式边界* 变异策略通过将连续个体基因传递到一个预定义的多项式映射中来实现。
- en: 'There are many strategies available that you can follow based on your hyperparameter
    space specification. We will talk more about different strategies and how to implement
    the GA method using the **DEAP** package in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*. For now, let’s see
    the results of applying a GA on the dummy objective function, *f*. Note that the
    points in each plot correspond to each individual in the population:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您的超参数空间定义，有许多可用的策略可供选择。我们将在 [*第 10 章*](B18753_10_ePub.xhtml#_idTextAnchor092)*，使用
    DEAP 和 Microsoft NNI 进行高级超参数调整* 中更多地讨论不同的策略以及如何使用 **DEAP** 包实现 GA 方法。现在，让我们看看对虚拟目标函数
    *f* 应用 GA 的结果。请注意，每个图中的点对应于种群中的每个个体：
- en: '![Figure 5.7 – GA process'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.7 – GA 流程'
- en: '](img/B18753_05_007.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_05_007.jpg)'
- en: Figure 5.7 – GA process
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 – GA 流程
- en: Based on the preceding figure, we can see that in the first generation, individuals
    are scattered all around the place since it is initialized randomly. In the second
    generation, several individuals that are initialized around point –1.0 moved to
    other places that have lower fitness scores. However, in the third generation,
    there are new individuals around point –1.0 again. This may be due to the random
    mutation operator that’s been applied to them. There are also several individuals
    stuck in the local optima, which is around point –0.5\. In the fourth generation,
    most of the individuals have moved to places with lower fitness scores, although
    some of them are still stuck in the local optima. In the fifth generation, individuals
    are starting to converge in several places.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 根据前一个图表，我们可以看到在第一代，由于是随机初始化，个体分布在整个地方。在第二代，围绕点 -1.0 初始化的几个个体移动到了具有较低适应度分数的其他地方。然而，在第三代，围绕点
    -1.0 又出现了新的个体。这可能是由于应用了随机变异算子。还有一些个体陷入了局部最优，大约在点 -0.5 附近。在第四代，大多数个体已经移动到了具有较低适应度分数的地方，尽管其中一些个体仍然陷入了局部最优。在第五代，个体开始在几个地方开始收敛。
- en: 'Finally, in the sixth generation, all of them converged to the near-global
    optima, which is around point 1.5\. Note that we still have *NPOP=25* individuals
    in the sixth generation, but all of them are located in the same place, which
    is why you can only see one dot in the plot. This also applies to other generations
    if you see that there are fewer than 25 individuals in the plot. The convergence
    trend across generations can be seen in the following graph:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在第六代，所有个体都收敛到了接近全局最优解，大约在点 1.5 附近。请注意，我们仍然在第六代有 *NPOP=25* 个个体，但它们都位于同一个地方，这就是为什么在图表中只能看到一个点。这也适用于其他世代，如果你在图表中看到少于
    25 个个体。各个世代的收敛趋势可以在以下图表中看到：
- en: '![Figure 5.8 – Convergence plot'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.8 – 收敛图'
- en: '](img/B18753_05_008.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_05_008.jpg]'
- en: Figure 5.8 – Convergence plot
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 – 收敛图
- en: The trend that’s shown in the preceding graph matches our previous analysis.
    However, we can get additional information from this plot. At first, many of the
    individuals are located in places with high fitness scores, but some individuals
    already get the best fitness score. Across generations, most of the individuals
    started to converge, and finally, in the last generation, all individuals had
    the best fitness score. It is worth noting that, in practice, it is not guaranteed
    that a GA will achieve the global optimal solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 前一个图表中显示的趋势与我们的先前分析相符。然而，我们可以从这个图中获得更多信息。起初，许多个体位于具有高适应度分数的地方，但一些个体已经获得了最佳的适应度分数。在各个世代中，大多数个体开始收敛，最终，在最后一代，所有个体都获得了最佳的适应度分数。值得注意的是，在实践中，并不能保证
    GA 会达到全局最优解。
- en: At this point, you may be wondering, how can a GA be adopted as a hyperparameter
    tuning method? What is the corresponding definition of all terms in the GA within
    the context of hyperparameter tuning? What does an individual mean when performing
    hyperparameter tuning with a GA?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能想知道，如何将 GA 作为超参数调整方法采用？在超参数调整的上下文中，GA 中所有术语的对应定义是什么？当使用 GA 进行超参数调整时，“个体”意味着什么？
- en: '*As a hyperparameter tuning method*, the GA method treats a set of hyperparameters
    as an individual where the hyperparameter values are the genes. To have better
    clarity on what each important term in the GA method means, in the context of
    hyperparameter tuning, please refer to the following table:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*作为超参数调整方法*，GA 方法将一组超参数视为一个个体，其中超参数的值是基因。为了更好地理解 GA 方法中每个重要术语的含义，在超参数调整的上下文中，请参考以下表格：'
- en: '![Figure 5.9 – Definition of GA method terms in the hyperparameter tuning context'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.9 – 超参数调整上下文中 GA 方法术语的定义'
- en: '](img/B18753_05_009.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_05_009.jpg]'
- en: Figure 5.9 – Definition of GA method terms in the hyperparameter tuning context
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 – 超参数调整上下文中 GA 方法术语的定义
- en: 'Now that you are aware of the corresponding definition of each important term
    in the GA method, we can define the formal procedure to utilize *the GA method
    as a hyperparameter tuning method*:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 既然你已经了解了 GA 方法中每个重要术语的相应定义，我们可以定义正式的程序来利用 *GA 方法作为超参数调整方法*：
- en: Split the original full data into train and test sets.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据集分为训练集和测试集。
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  id: totrans-127
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间，*H*，以及伴随的分布。
- en: Define the population size, *NPOP*.
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义种群大小，*NPOP*。
- en: Define the crossover probability, *CXPB*, and mutation probability, *MUTPB*.
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义交叉概率，*CXPB*，和变异概率，*MUTPB*。
- en: Define the number of trials, *NGEN*, as the stopping criterion.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将试验次数，*NGEN*，定义为停止标准。
- en: Define the objective function, *f*, based on the train set.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集定义目标函数，*f*。
- en: '*Initialize* a population with *NPOP* sets of hyperparameters, where each set
    is drawn randomly from the hyperparameter space, *H*.'
  id: totrans-132
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*初始化*一个包含 *NPOP* 组超参数的种群，每组超参数都是从超参数空间，*H*，中随机抽取的。'
- en: Evaluate all hyperparameter sets in the population based on the objective function,
    *f*.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据目标函数，*f*，评估种群中的所有超参数集。
- en: '*Select* several best candidate sets based on *Step 8*.'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据 *Step 8* 选择几个最佳候选集。
- en: Perform *crossover* on candidate sets from *Step 9* with a probability of *CXPB*.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以 *CXPB* 的概率对 *Step 9* 中的候选集进行交叉。
- en: Perform *mutation* on the crossed candidate sets from *Step 10* with a probability
    of *MUTPB*.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以 *MUTPB* 的概率对 *Step 10* 中的交叉候选集进行变异。
- en: Generate a new population consisting of all sets of hyperparameters from *Step
    10*, *Step 11*, and the rest of the sets from the current population. The new
    population will also consist of *NPOP* sets of hyperparameters.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个新的种群，该种群由 *Step 10*、*Step 11* 和当前种群中的其余超参数集的所有组合组成。新种群也将包含 *NPOP* 组超参数。
- en: Repeat *Steps 8* to *12* *NGEN* times.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复 *Steps 8* 到 *12* *NGEN* 次。
- en: Train on the full training set using the final hyperparameter values.
  id: totrans-139
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用最终的超参数值在完整的训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-140
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: It is worth noting that when utilizing a GA as a hyperparameter tuning method,
    the GA itself has four hyperparameters, namely *NPOP*, *CXPB*, *MUTPB*, and *NGEN*,
    that control the performance of the hyperparameter tuning results, as well as
    the *exploration versus exploitation trade-off*. To be more precise, *CXPB* and
    *MUTPB*, or the crossover and mutation probability, respectively, are responsible
    for controlling the *exploration* rate, while the *selection* step, along with
    its strategy, controls the *exploitation* rate.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，当利用 GA 作为超参数调整方法时，GA 本身有四个超参数，即 *NPOP*、*CXPB*、*MUTPB* 和 *NGEN*，它们控制超参数调整结果的表现，以及探索与利用之间的权衡。更精确地说，*CXPB*
    和 *MUTPB*，即交叉和变异概率，分别负责控制 *探索* 速率，而选择步骤及其策略控制 *利用* 速率。
- en: 'The following table lists the pros and cons of using a GA as a hyperparameter
    tuning method:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 下表列出了使用 GA 作为超参数调整方法的优缺点：
- en: '![Figure 5.10 – Pros and Cons of the GA method'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '![Figure 5.10 – Pros and Cons of the GA method'
- en: '](img/B18753_05_010.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '![img/B18753_05_010.jpg]'
- en: Figure 5.10 – Pros and Cons of the GA method
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 – GA 方法的优缺点
- en: The need to evaluate all individuals in each generation means we multiplied
    the original time complexity that our objective has by *NPOP * NGEN*. It’s very
    costly! That’s why the GA method is not suitable for you if you have an expensive
    objective function and/or low computational resources. However, if you do have
    time to wait for the experiment to be done, and you have massively parallel computing
    resources, then the GA method is suitable for you. From a theoretical perspective,
    the GA method can also work with various types of hyperparameters – we just need
    to choose the appropriate crossover and mutation strategies for the corresponding
    hyperparameters. The GA method is better than SA in terms of having a population
    to guide which part of the subspace needs to be exploited more. However, it is
    worth noting that the GA method can still be stuck in local optima.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 评估每一代中所有个体的需求意味着我们将我们目标函数的原始时间复杂度乘以 *NPOP * NGEN*。这非常昂贵！这就是为什么如果你有一个昂贵的目标函数和/或有限的计算资源，GA
    方法可能不适合你。然而，如果你有等待实验完成的时间，并且你有大量的并行计算资源，那么 GA 方法适合你。从理论角度来看，GA 方法也可以与各种类型的超参数一起工作——我们只需要为相应的超参数选择合适的交叉和变异策略。与
    SA 相比，GA 方法在拥有一个种群来指导需要更多探索的子空间部分方面更有优势。然而，值得注意的是，GA 方法仍然可能陷入局部最优。
- en: In this section, we discussed the GA method, starting with what it is, how it
    works both in terms of its general setup and the hyperparameter tuning context,
    and its pros and cons. We will discuss another interesting population-based heuristic
    search method in the next section.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了 GA 方法，从它是什么，以及它在一般设置和超参数调整环境中的工作方式，以及它的优缺点。在下一节中，我们将讨论另一个有趣的基于种群的启发式搜索方法。
- en: Understanding particle swarm optimization
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解粒子群优化
- en: '**PSO** is also a population-based heuristic search method, similar to the
    GA method. PSO is inspired by the schools of fish and flocks of birds’ social
    interaction in nature. As a hyperparameter tuning method, PSO is suggested to
    be utilized if your search space contains many non-categorical hyperparameters,
    each trial doesn’t take much time, and you have enough computational resources
    – especially parallel computing resources.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**PSO**也是一种基于种群的启发式搜索方法，类似于遗传算法（GA）方法。PSO受到自然界中鱼群和鸟群社会互动的启发。作为一种超参数调整方法，如果您的搜索空间包含许多非分类超参数，每次试验所需时间不多，并且您有足够的计算资源——特别是并行计算资源，建议使用PSO。'
- en: PSO is one of the most popular methods within the bigger **swarm intelligence**
    (**SI**) group of methods. There are various methods in SI that are inspired by
    the social interaction of animals in nature, such as herds of land animals, colonies
    of ants, flocks of birds, schools of fish, and many more. The common characteristics
    of SI methods are *population-based*, individuals within the population are relatively
    *similar to each other*, and the ability of the population to move in a specific
    direction systemically *without a single coordinator* inside or outside the population.
    In other words, the population can organize themselves based on the *local interactions*
    of individuals interacting with each other and/or the surrounding environment.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: PSO是更大群体智能（SI）方法组中最受欢迎的方法之一。群体智能中有各种方法，这些方法受到自然界中动物社会互动的启发，例如陆地动物的群、蚂蚁的群体、鸟群、鱼群等等。群体智能方法的共同特征是**基于种群**的，种群内的个体相对**相似**，种群能够系统性地在种群内部或外部没有单个协调者的情况下向特定方向移动。换句话说，种群可以根据个体之间以及与周围环境的**局部交互**来组织自己。
- en: When a flock of birds is looking for food, it is believed that each bird can
    contribute to the group by sharing information about their sights, so that the
    group can move in the right direction. PSO is a method that simulates the movement
    of a flock of birds to optimize the objective function. In PSO, the flock of birds
    is called a **swarm** and each bird is called a **particle**.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 当一群鸟在寻找食物时，人们认为每只鸟可以通过分享它们所看到的关于信息来为群体做出贡献，从而使群体能够朝正确的方向移动。粒子群优化（PSO）是一种模拟鸟群运动以优化目标函数的方法。在PSO中，鸟群被称为**群**，每只鸟被称为**粒子**。
- en: Each particle is defined by its **position** vector and **velocity** vector.
    The movement of each particle consists of both stochastic and deterministic components.
    In other words, the movement of each particle is not only based on a predefined
    rule but is also influenced by random components. Each particle also remembers
    its own **best position**, which gives the best objective function value along
    the trajectory it has passed. Then, along with the **global best position**, it
    is used to update the velocity and position of each particle at a particular time.
    The global best position is just the position of the best particle from the previous
    step.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每个粒子由其**位置**向量和**速度**向量定义。每个粒子的运动由随机和确定性成分组成。换句话说，每个粒子的运动不仅基于预定义的规则，还受到随机成分的影响。每个粒子还记住自己的**最佳位置**，这给出了它在轨迹上经过的最佳目标函数值。然后，结合**全局最佳位置**，它被用来在特定时间更新每个粒子的速度和位置。全局最佳位置只是上一步中最佳粒子的位置。
- en: 'Let’s say that ![](img/Formula_B18753_05_053.png) is the position vector in
    a *d*-dimensional space of the ![](img/Formula_B18753_05_054.png) particle out
    of *m* particles in the swarm, and that ![](img/Formula_B18753_05_055.png) is
    the velocity vector of the same size for the ![](img/Formula_B18753_05_056.png)
    particle, as shown here:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 假设![](img/Formula_B18753_05_053.png)是群中m个粒子中第![](img/Formula_B18753_05_054.png)个粒子在d维空间中的位置向量，而![](img/Formula_B18753_05_055.png)是相同大小的![](img/Formula_B18753_05_056.png)粒子的速度向量，如图所示：
- en: '![](img/Formula_B18753_05_057.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_057.png)'
- en: '![](img/Formula_B18753_05_058.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_058.png)'
- en: 'Let’s also define the best position for each particle and the global best position
    vectors, respectively:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也分别定义每个粒子的最佳位置和全局最佳位置向量：
- en: '![](img/Formula_B18753_05_059.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_059.png)'
- en: '![](img/Formula_B18753_05_060.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_060.png)'
- en: 'The following formulas define how each particle’s position and velocity vectors
    are updated in each iteration:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 以下公式定义了每个粒子在每次迭代中位置和速度向量的更新方式：
- en: '![](img/Formula_B18753_05_061.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_061.png)'
- en: '![](img/Formula_B18753_05_062.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_062.png)'
- en: Here, ![](img/Formula_B18753_05_063.png), ![](img/Formula_B18753_05_064.png),
    and ![](img/Formula_B18753_05_065.png) are the hyperparameters that control the
    *exploration versus exploitation trade-off*. ![](img/Formula_B18753_05_066.png)
    has a value between zero and one is usually called the **inertia weight coefficient**,
    while ![](img/Formula_B18753_05_067.png) and ![](img/Formula_B18753_05_068.png)
    are called the **cognitive** and **social coefficients**, respectively. ![](img/Formula_B18753_05_069.png)
    and ![](img/Formula_B18753_05_070.png) are the random values between zero and
    one and act as the stochastic components of the particle movement. Note that the
    *d*-dimensions of the position and velocity vectors refer to the number of hyperparameters
    we have in the search space, while the *m* particles refer to the number of candidate
    hyperparameters that are sampled from the hyperparameter space.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_05_063.png)、![](img/Formula_B18753_05_064.png) 和
    ![](img/Formula_B18753_05_065.png) 是控制*探索与利用权衡*的超参数。![](img/Formula_B18753_05_066.png)
    的值通常在零到一之间，被称为**惯性权重系数**，而 ![](img/Formula_B18753_05_067.png) 和 ![](img/Formula_B18753_05_068.png)
    分别被称为**认知**和**社会**系数。![](img/Formula_B18753_05_069.png) 和 ![](img/Formula_B18753_05_070.png)
    是介于零和一之间的随机值，充当粒子运动的随机成分。请注意，位置和速度向量的 *d* 维数指的是我们在搜索空间中的超参数数量，而 *m* 个粒子指的是从超参数空间中采样的候选超参数的数量。
- en: 'Updating the velocity vector may seem intimidating the first time, but actually,
    you can understand it more easily by treating the formula as three separate parts.
    The first part, or the left-most side of the formula, aims to update the next
    velocity proportional to the current velocity. The second part, or the middle
    part of the formula, aims to update the velocity toward the direction of the best
    position that the ![](img/Formula_B18753_05_071.png) particle has, while also
    adding a stochastic component to it. The third part, or the right-most side of
    the formula, aims to bring the ![](img/Formula_B18753_05_072.png) particle closer
    to the global best position, with additional random behavior applied to it. The
    following diagram helps illustrate this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次更新速度向量可能看起来有些令人畏惧，但实际上，通过将公式视为三个独立的部分，你可以更容易地理解它。第一部分，或者公式的最左侧，旨在按比例更新下一个速度，与当前速度成正比。第二部分，或者公式的中间部分，旨在将速度更新到
    ![](img/Formula_B18753_05_071.png) 粒子所具有的最佳位置的方向，同时向其中添加一个随机成分。第三部分，或者公式的最右侧，旨在将
    ![](img/Formula_B18753_05_072.png) 粒子带到全局最佳位置附近，并对其应用额外的随机行为。以下图示有助于说明这一点：
- en: '![Figure 5.11 – Updating the particle’s position and velocity'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.11 – 更新粒子的位置和速度'
- en: '](img/B18753_05_011.jpg)'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18753_05_011.jpg)'
- en: Figure 5.11 – Updating the particle’s position and velocity
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 – 更新粒子的位置和速度
- en: The preceding diagram isn’t the same as the stated formula since the random
    components and the hyperparameters are missing from the picture. However, this
    diagram can help us understand the high-level concept of how each particle’s position
    and velocity vectors are updated in each iteration. We can see that the final
    updated velocity (*see the orange line*) is calculated based on three vectors,
    namely the current velocity (*see the brown line*), the particle best position
    (*see the green line*), and the global best position (*see the purple line*).
    Based on the final updated velocity, we can get the updated position of the ![](img/Formula_B18753_05_073.png)
    particle – that is, ![](img/Formula_B18753_05_074.png).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图示与所给出的公式并不相同，因为图中缺少了随机成分和超参数。然而，这张图可以帮助我们理解每个粒子在每个迭代中位置和速度向量的更新高级概念。我们可以看到，最终更新的速度（*见橙色线*）是基于三个向量计算的，即当前速度（*见棕色线*）、粒子最佳位置（*见绿色线*）和全局最佳位置（*见紫色线*）。基于最终更新的速度，我们可以得到粒子的更新位置
    – 那就是图中的 ![](img/Formula_B18753_05_073.png)。
- en: Now, let’s discuss how the hyperparameters affect the formula. The inertia weight
    coefficient, ![](img/Formula_B18753_05_075.png),controls how much we want to put
    our focus on the current velocity when updating the velocity vector. On the other
    hand, the cognitive coefficient, ![](img/Formula_B18753_05_076.png), and the social
    coefficient, ![](img/Formula_B18753_05_077.png), control how much we should focus
    on the particle’s past trajectory history and swarm’s search result, respectively.
    When we set ![](img/Formula_B18753_05_078.png), we don’t take into account the
    influence of the best position of the ![](img/Formula_B18753_05_079.png) particle,
    which may lead us to be *trapped in the local optima*. When we set ![](img/Formula_B18753_05_080.png),
    we ignore the influence of the global best position, which may lead us to a *slower
    convergence* speed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们讨论超参数如何影响公式。惯性权重系数，![公式](img/Formula_B18753_05_075.png)，控制我们在更新速度向量时想要将多少注意力放在当前速度上。另一方面，认知系数，![公式](img/Formula_B18753_05_076.png)，和社会系数，![公式](img/Formula_B18753_05_077.png)，分别控制我们应该在多大程度上关注粒子的过去轨迹历史和群搜索结果。当我们设置![公式](img/Formula_B18753_05_078.png)时，我们不考虑最佳位置![公式](img/Formula_B18753_05_079.png)粒子的影响，这可能导致我们陷入*局部最优解*。当我们设置![公式](img/Formula_B18753_05_080.png)时，我们忽略了全局最佳位置的影响，这可能导致*收敛速度较慢*。
- en: 'Now that you are aware of the position and velocity components of each particle
    in the swarm, take a look at the following steps, which define *how PSO works
    in general* as an optimization method:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了群中每个粒子的位置和速度分量，请查看以下步骤，这些步骤定义了粒子群优化（PSO）作为优化方法的一般工作方式：
- en: Define the swarm size, *N*, the inertia weight coefficient, *w*, the cognitive
    coefficient, *c1*, the social coefficient, *c2*, and the maximum number of trials.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义群大小*N*，惯性权重系数*w*，认知系数*c1*，社会系数*c2*和最大尝试次数。
- en: Define the fitness function, *f*.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义适应度函数*f*。
- en: Initialize a swarm with *N* particles, where each particle’s position and velocity
    vectors are initialized randomly.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个包含*N*个粒子的群，其中每个粒子的位置和速度向量都是随机初始化的。
- en: Set each particle’s current position vector as their best position vector, *pbi*.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个粒子的当前位置向量设置为它们的最佳位置向量，*pbi*。
- en: Set the current global best position, *gb*, by selecting a position vector from
    all *N* particles that have the most optimal fitness score.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从所有*N*个粒子中选择具有最佳适应度分数的位置向量来设置当前全局最佳位置，*gb*。
- en: Update each particle’s position and velocity vector based on the updating formula.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据更新公式更新每个粒子的位置和速度向量。
- en: Evaluate all the particles in the swarm based on the fitness function, *f*.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据适应度函数*f*评估群中的所有粒子。
- en: 'Update each particle’s best position vectors, *pbi*:'
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每个粒子的最佳位置向量，*pbi*：
- en: Compare each particle’s current fitness score from *Step 7* with its *pbi* fitness
    score.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个粒子的当前适应度分数与*步骤7*中的*pbi*适应度分数进行比较。
- en: If the current fitness score is better than the *pbi* fitness score, update
    *pbi* with the current position vector.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前适应度分数优于*pbi*适应度分数，则使用当前位置向量更新*pbi*。
- en: 'Update the global best position vector, *gb*:'
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新全局最佳位置向量，*gb*：
- en: Compare each particle’s current fitness score from *Step 7* with the previous
    *gb* fitness score.
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个粒子的当前适应度分数与*步骤7*中的先前*gb*适应度分数进行比较。
- en: If the current fitness score is better than the *gb* fitness score, update *gb*
    with the current position vector.
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前适应度分数优于*gb*适应度分数，则使用当前位置向量更新*gb*。
- en: Update each particle’s position and velocity vector based on the updating formula.
  id: totrans-183
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据更新公式更新每个粒子的位置和速度向量。
- en: Repeat *Steps 7* to *10* until the maximum number of trials is reached.
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤7*到*10*，直到达到最大尝试次数。
- en: Return the final global best position, *gb*.
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回最终的全球最佳位置，*gb*。
- en: It is worth noting that the definition of the optimal fitness score (or a better
    fitness score in the previously stated procedure) will depend on what type of
    optimization problem you are trying to solve. If it is a minimization problem,
    then a smaller fitness score is better. If it is a maximization problem, then
    it is the other way around.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，最优适应度分数（或先前所述程序中的更好适应度分数）的定义将取决于你试图解决的优化问题类型。如果是最小化问题，则较小的适应度分数较好。如果是最大化问题，则情况相反。
- en: 'To have even a better understanding of how PSO works, let’s go through an example.
    Let’s define the fitness function as follows:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解PSO的工作原理，让我们通过一个例子来分析。让我们定义适应度函数如下：
- en: '![](img/Formula_B18753_05_081.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![公式](img/Formula_B18753_05_081.png)'
- en: 'Here, ![](img/Formula_B18753_05_082.png) and ![](img/Formula_B18753_05_083.png)
    are only defined within the ![](img/Formula_B18753_05_084.png) range. The following
    *contour plot* shows what our objective function looks like. We will learn more
    about how to implement PSO using the **DEAP** package in [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*,
    Advanced Hyperparameter Tuning with DEAP and Microsoft NNI*:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_B18753_05_082.png) 和 ![](img/Formula_B18753_05_083.png)
    仅在 ![](img/Formula_B18753_05_084.png) 范围内定义。下面的 *等高线图* 展示了我们的目标函数看起来是什么样子。我们将在
    [*第 10 章*](B18753_10_ePub.xhtml#_idTextAnchor092)*，使用 DEAP 和 Microsoft NNI 进行高级超参数调整中学习如何使用
    **DEAP** 包实现 PSO：
- en: '![Figure 5.12 – A contour plot showing the objective function and its global
    minimum'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.12 – 显示目标函数及其全局最小值的等高线图'
- en: '](img/B18753_05_012.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18753_05_012.jpg]'
- en: Figure 5.12 – A contour plot showing the objective function and its global minimum
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 – 显示目标函数及其全局最小值的等高线图
- en: Here, you can see that the global minimum (*see the red cross marker*) is located
    at (0.497, 0.295) with an objective function value of –0.649\. Let’s try to utilize
    PSO to see how well it estimates the minimum value of the objective function compared
    to the true global minimum. Let’s say we define the hyperparameter for PSO as
    *N=20*, *w=0.5*, *c1=0.3*, and *c2=0.5*, and set the maximum number of trials
    to 16\.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以看到全局最小值 (*见红色十字标记*) 位于 (0.497, 0.295)，目标函数值为 –0.649。让我们尝试使用 PSO 来看看它如何估计目标函数的最小值与真实全局最小值相比。假设我们定义
    PSO 的超参数为 *N=20*，*w=0.5*，*c1=0.3*，和 *c2=0.5*，并将最大尝试次数设置为 16。
- en: 'You can see the initial swarm illustration in the following contour plot. The
    blue dots refer to each of the particles, the blue arrow on each particle refers
    to the particle’s velocity vector, the black dots refer to each particle’s best
    position vectors, and the red star marker refers to the current global best position
    vector at a particular iteration:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下等高线图中看到初始群体示意图。蓝色点代表每个粒子，每个粒子上的蓝色箭头代表粒子的速度向量，黑色点代表每个粒子的最佳位置向量，红色星形标记代表特定迭代时的当前全局最佳位置向量：
- en: '![Figure 5.13 – A PSO initial swarm'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.13 – 一个 PSO 初始群体'
- en: '](img/B18753_05_013.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18753_05_013.jpg]'
- en: Figure 5.13 – A PSO initial swarm
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 – 一个 PSO 初始群体
- en: 'Since the initial particles at the swarm are initialized randomly, the direction
    of the velocity vectors is all over the place (see *Figure 5.13*). You can see
    how each particle’s position and velocity vectors are updated in each iteration,
    along with the global best position vector, as shown here:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于群体中的初始粒子是随机初始化的，因此速度向量的方向遍布各处（参见 *图 5.13*）。您可以看到每个粒子的位置和速度向量在每个迭代中是如何更新的，以及全局最佳位置向量，如下所示：
- en: '![Figure 5.14 – PSO process'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.14 – PSO 过程'
- en: '](img/B18753_05_014.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片 B18753_05_014.jpg]'
- en: Figure 5.14 – PSO process
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 – PSO 过程
- en: Even at the first iteration, each particle’s velocity vector is pointing toward
    the global minimum, which is located in the bottom left of the plot. In each iteration,
    the position and velocity vectors are updated and move closer to the global minimum.
    At the end of the iteration loop, most of the particles are located around the
    global minimum position, where the final global best position vector is located
    at (0.496, 0.290) with a fitness score of around –0.648\. This estimation is very
    close to the true global minimum of the objective function!
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在第一次迭代中，每个粒子的速度向量都指向全局最小值，该最小值位于图的左下角。在每个迭代中，位置和速度向量都会更新并接近全局最小值。在迭代循环结束时，大多数粒子都位于全局最小值位置附近，最终全局最佳位置向量位于
    (0.496, 0.290)，适应度分数约为 –0.648。这个估计非常接近目标函数的真实全局最小值！
- en: 'It is worth noting that the velocity vector of each particle contains two components:
    magnitude and direction. The magnitude will impact the length of the velocity
    vector in *Figure 5.14*. While you may not see the difference in length between
    each particle’s velocity vector, they are different from each other!'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，每个粒子的速度向量包含两个分量：大小和方向。大小将影响 *图 5.14* 中速度向量的长度。虽然您可能看不到每个粒子速度向量长度之间的差异，但它们彼此是不同的！
- en: Important Note
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '*As a hyperparameter tuning method*, in the PSO method, *particle* and *swarm*
    refer to the candidate set of hyperparameters that are sampled from the hyperparameter
    space and the collection of hyperparameter set candidates, respectively. The position
    vector of each particle refers to the values of each hyperparameter in a particle.
    Finally, the velocity vector refers to the *delta of hyperparameter values* that
    will be utilized to update the values of each hyperparameter in a particle.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 作为超参数调整方法，在PSO方法中，*粒子*和*群体*分别指从超参数空间和超参数集候选集合中采样的候选超参数集。每个粒子的位置向量指代粒子中每个超参数的值。最后，速度向量指用于更新粒子中每个超参数值的*超参数值变化量*。
- en: 'The following steps define *how PSO works as a hyperparameter tuning method*:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤定义了*PSO作为超参数调整方法的工作原理*：
- en: Split the original full data into train and test sets.
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据集分为训练集和测试集。
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  id: totrans-208
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用伴随分布定义超参数空间，*H*。
- en: Define the collection size, *N*, the inertia weight coefficient, *w*, the cognitive
    coefficient, *c1*, the social coefficient, *c2*, and the maximum number of trials.
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义集合大小*N*、惯性权重系数*w*、认知系数*c1*、社会系数*c2*和最大尝试次数。
- en: Define the objective function, *f*, based on the train set.
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据训练集定义目标函数*f*。
- en: Initialize a collection of *N* sets of hyperparameters, where each set is drawn
    randomly from the hyperparameter space, *H*.
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个包含*N*个超参数集的集合，其中每个集是从超参数空间*H*中随机抽取的。
- en: Randomly initialize the velocity vector for each set of hyperparameters in the
    collection.
  id: totrans-212
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机初始化集合中每个超参数集的速度向量。
- en: Set each set’s current hyperparameter values as their best values, *pbi*.
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个集的当前超参数值设置为它们的最佳值*pbi*。
- en: Set the current global best set of hyperparameters, *gb*, by selecting a set
    from all *N* sets of hyperparameters that have the most optimal objective function
    score.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置当前全局最佳超参数集，*gb*，通过从所有 *N* 个超参数集中选择一个具有最佳目标函数分数的集。
- en: Update each set’s hyperparameter values and velocity vector based on the updating
    formula.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据更新公式更新每个集的超参数值和速度向量。
- en: Evaluate all sets of hyperparameters in the collection based on the objective
    function, *f*.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据目标函数*f*评估集合中所有超参数集。
- en: 'Update each set’s best hyperparameter values, *pbi*:'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新每个集的最佳超参数值*pbi*：
- en: Compare each set’s current score from *Step 10* with its *pbi* score.
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第10步中每个集的当前分数与它的*pbi*分数进行比较。
- en: If the current score is better than the *pbi* score, update *pbi* with the current
    hyperparameter values.
  id: totrans-219
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前分数优于*pbi*分数，则使用当前超参数值更新*pbi*。
- en: 'Update the global best set of hyperparameters, *gb*:'
  id: totrans-220
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新全局最佳超参数集*gb*：
- en: Compare each set’s current score from *Step 10* with the previous *gb* score.
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将第10步中每个集的当前分数与之前的*gb*分数进行比较。
- en: If the current score is better than the *gb* score, update *gb* with the current
    set of hyperparameters.
  id: totrans-222
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果当前分数优于*gb*分数，则使用当前超参数集更新*gb*。
- en: Update each set’s hyperparameter values and velocity vector based on the updating
    formula.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据更新公式更新每个集的超参数值和速度向量。
- en: Repeat *Steps 10* to *13* until the maximum number of trials is reached.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复*步骤10*到*13*，直到达到最大尝试次数。
- en: Train on the full training set using the global best set of hyperparameters.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用全局最佳超参数集在完整训练集上进行训练。
- en: Evaluate the final trained model on the test set.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终训练好的模型。
- en: 'One issue with the updating formula in the PSO method is that it only works
    on numerical variables, especially continuous variables, meaning we can’t directly
    utilize the original PSO as a hyperparameter tuning method if our hyperparameter
    space contains discrete hyperparameters. Motivated by this issue, there are several
    variants of PSO that are designed to be able to work in discrete spaces as well.
    The first variant is designed to work specifically for binary variables and is
    called **binary PSO**. In this variant, the updating formula for the velocity
    vector is the same, meaning we still treat the velocity vector in a continuous
    space, but the updating formula for the position vector is modified, like so:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在PSO方法中的更新公式有一个问题，那就是它只适用于数值变量，尤其是连续变量，这意味着如果我们的超参数空间包含离散超参数，我们就不能直接利用原始PSO作为超参数调整方法。受此问题的启发，有几个PSO的变体被设计出来，以便能够在离散空间中也能工作。第一个变体是为了专门针对二进制变量设计的，被称为**二进制PSO**。在这个变体中，速度向量的更新公式是相同的，这意味着我们仍然将速度向量视为连续空间中的，但位置向量的更新公式被修改了，如下所示：
- en: '![](img/Formula_B18753_05_085.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_B18753_05_085.png)'
- en: Here, ![](img/Formula_B18753_05_086.png) is a random number drawn from a uniform
    distribution within the ![](img/Formula_B18753_05_087.png), ![](img/Formula_B18753_05_088.png)
    interval, and the *j* subscript refers to each component in the *i*thparticle.
    As you can see, in the binary PSO variant, we can work within the discrete space,
    but we are restricted to only having binary variables.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，![](img/Formula_B18753_05_086.png)是从![](img/Formula_B18753_05_087.png)，![](img/Formula_B18753_05_088.png)区间内均匀分布中抽取的一个随机数，*j*下标指的是第*i*个粒子的每个分量。正如你所见，在二进制PSO变体中，我们可以在离散空间中工作，但我们被限制只能使用二进制变量。
- en: What about when we have a combination of discrete and continuous numerical hyperparameters?
    For example, our hyperparameter space for a neural network model contains the
    learning rate, dropout rate, and the number of layers. We can’t utilize the original
    PSO method directly since the number of layers hyperparameter expects an integer
    input, not a continuous or floating-point input. We also can’t utilize the binary
    PSO variant since the learning rate and dropout rate are continuous, and the number
    of layers hyperparameter is also not binary.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个离散和连续数值超参数的组合时怎么办？例如，我们的神经网络模型超参数空间包含学习率、dropout率和层数。由于层数超参数期望的是整数输入，而不是连续或浮点输入，所以我们不能直接利用原始PSO方法。我们也不能利用二进制PSO变体，因为学习率和dropout率是连续的，而层数超参数也不是二进制的。
- en: One simple thing we can do is *round the updated velocity* vector component
    values, but only for components that correspond to the discrete position component,
    before passing it to the position vector updating formula. This way, we can ensure
    that our discrete hyperparameters will still always be within the discrete space.
    However, this workaround still has an issue. The rounding operation may make the
    updating procedure of the velocity vector suboptimal. Why? Because of the possibility
    that no matter the updated values of the velocity vector, so long as they are
    still within a similar range of one integer point, then the position vector will
    not be updated anymore. This will contribute to a lot of redundant computational
    costs.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做的简单事情之一是*四舍五入更新后的速度向量分量值*，但只针对与离散位置分量相对应的分量，在将其传递给位置向量更新公式之前。这样，我们可以确保我们的离散超参数仍然始终在离散空间内。然而，这个解决方案仍然存在问题。四舍五入操作可能会使速度向量的更新过程次优。为什么？因为速度向量的更新值无论是什么，只要它们仍然在一个整数点的相似范围内，位置向量就不再更新。这将导致大量的冗余计算成本。
- en: There is another workaround to make PSO operate well both in continuous and
    discrete spaces. On top of rounding the updated velocity vector component values,
    we can also *update the inertia weight coefficient dynamically*. The motivation
    is to help a particle focus on its past velocity values so that it is not stuck
    in the local or global optimum, which is influenced by ![](img/Formula_B18753_05_089.png)
    or ![](img/Formula_B18753_05_090.png). The dynamic inertia weight updating procedure
    can be done based on several factors, such as the relative distance between its
    current position vector and its best position vector, the difference between the
    current number of trials and the maximum number of trials, and many more.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 有另一种方法可以使PSO在连续和离散空间中都能良好地运行。除了四舍五入更新的速度向量分量值之外，我们还可以动态地更新惯性权重系数。动机是帮助粒子关注其过去的速度值，这样它就不会陷入局部或全局最优，这受到![](img/Formula_B18753_05_089.png)或![](img/Formula_B18753_05_090.png)的影响。动态惯性权重更新过程可以根据多个因素进行，例如其当前位置向量和最佳位置向量之间的相对距离，当前试验次数与最大试验次数之间的差异，等等。
- en: There are many variants of how we can dynamically update the inertia weight
    coefficient during trials; we will leave it to you to choose what works well for
    your specific case.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在试验过程中，我们可以有多种方式动态更新惯性权重系数的变体；我们将把它留给你来选择对你特定情况效果最好的方式。
- en: Although we can modify the updating formula in PSO to make it work not only
    for continuous but also discrete variables, we are still faced with several issues,
    as stated previously. Thus, to utilize the maximum power of PSO within the continuous
    space, there’s another variant of PSO that tries to synergize PSO with the Bayesian
    optimization method, called **PSO-BO**. The goal of PSO-BO is to utilize PSO as
    a replacement for Bayesian optimization’s acquisition function optimizer (see
    [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)). So, rather than using a
    second-order optimization method to optimize the acquisition function, we can
    utilize PSO as the optimizer to help decide which set of hyperparameters to be
    tested in the next trial of the Bayesian optimization hyperparameter tuning procedure.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以修改PSO中的更新公式，使其不仅适用于连续变量，也适用于离散变量，但我们仍然面临几个问题，如前所述。因此，为了在连续空间中充分利用PSO的最大功效，还有一种PSO的变体，试图将PSO与贝叶斯优化方法相结合，称为**PSO-BO**。PSO-BO的目标是将PSO作为贝叶斯优化获取函数优化器的替代品（参见[*第4章*](B18753_04_ePub.xhtml#_idTextAnchor036)）。因此，我们不需要使用二阶优化方法来优化获取函数，而可以使用PSO作为优化器来帮助决定在贝叶斯优化超参数调整过程的下一个试验中要测试的超参数集。
- en: 'The following table summarizes the pros and cons of utilizing PSO as a hyperparameter
    tuning method:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格总结了利用PSO作为超参数调整方法的优缺点：
- en: '![Figure 5.15 – Pros and cons of PSO'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.15 – PSO的优缺点'
- en: '](img/B18753_05_015.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.15 – PSO的优缺点](img/B18753_05_015.jpg)'
- en: Figure 5.15 – Pros and cons of PSO
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 – PSO的优缺点
- en: Now that you are aware of what PSO is, how it works, its several variants, and
    its pros and cons, let’s discuss another interesting population-based heuristic
    search method.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了PSO是什么，它是如何工作的，它的几个变体以及它的优缺点，让我们来讨论另一种有趣的基于群体的启发式搜索方法。
- en: Understanding Population-Based Training
  id: totrans-240
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解基于群体的训练
- en: '**PBT** is a population-based heuristic search method, just like the GA method
    and PSO. However, PBT is not a nature-inspired algorithm like GA or PSO. Instead,
    inspired by the GA method itself. PBT is suggested for when you are working with
    a neural-network-based type of model and *just need the final trained model* without
    knowing the specifically chosen hyperparameter configurations.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '**PBT**是一种基于群体的启发式搜索方法，就像GA方法和PSO一样。然而，PBT不是一个像GA或PSO那样的自然启发算法。相反，它受到GA方法本身的启发。当你在使用基于神经网络类型的模型，并且只需要最终的训练模型而不需要知道具体选择的超参数配置时，建议使用PBT。'
- en: PBT is specifically designed to *work only with a neural network-based* type
    of models, such as a multilayer perceptron, deep reinforcement learning, transformers,
    GAN, and any other neural network-based models. It can be said that PBT does both
    hyperparameter tuning and *model training* since the weights of the neural network
    model are inherited during the process. So, PBT is not only for choosing the most
    optimal hyperparameter configurations but also for transferring the weights or
    parameters of the model to other individuals within the population. That’s why
    the output of PBT is not a hyperparameter configuration but a model.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: PBT 是专门设计来**仅与基于神经网络的模型**一起工作的，例如多层感知器、深度强化学习、转换器、生成对抗网络以及任何其他基于神经网络的模型。可以说，PBT
    既能进行超参数调整，也能进行**模型训练**，因为在过程中神经网络模型的权重会被继承。因此，PBT 不仅是为了选择最优化超参数配置，也是为了将模型的权重或参数转移到种群中的其他个体。这就是为什么
    PBT 的输出不是一个超参数配置，而是一个模型。
- en: PBT is a *hybrid* method of the *random search* and *sequential search* methods,
    such as manual search and Bayesian search (see [*Chapter 3*](B18753_03_ePub.xhtml#_idTextAnchor031)*,
    Exploring Exhaustive Search* and [*Chapter 4*](B18753_04_ePub.xhtml#_idTextAnchor036)*,
    Exploring Bayesian Optimization* for more details). Random search is a very good
    method for finding a good subspace for sensitive hyperparameters. Sequential search
    methods tend to give better performance than random search if we have enough computational
    resources and time to execute the optimization process. However, the fact that
    those methods need to be executed sequentially makes the experiment take a very
    long time to run. PBT comes with a solution to combine the best of both worlds
    into a *single training optimization process*, meaning the model training and
    hyperparameter tuning process are merged into a single process.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: PBT 是**随机搜索**和**顺序搜索**方法的**混合**方法，例如手动搜索和贝叶斯搜索（详见[*第 3 章*](B18753_03_ePub.xhtml#_idTextAnchor031)*，探索穷举搜索*和[*第
    4 章*](B18753_04_ePub.xhtml#_idTextAnchor036)*，探索贝叶斯优化*以获取更多详细信息）。随机搜索是寻找对敏感超参数良好子空间的一个非常好的方法。如果我们在执行优化过程时有足够的计算资源和时间，顺序搜索方法往往比随机搜索给出更好的性能。然而，这些方法需要顺序执行的事实使得实验运行时间非常长。PBT
    提供了一种解决方案，将两种方法的优点结合成一个**单一的训练优化过程**，这意味着模型训练和超参数调整过程被合并成一个单一的过程。
- en: The term *Population-Based* in PBT comes from the fact that it is inspired by
    the GA method in terms of utilizing knowledge of the whole population to produce
    a better-performing individual. Note that the **individual** part of PBT refers
    to each of the *N* models with different parameters and hyperparameters in the
    **population** or a collection of all those *N* models.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: PBT 中的术语**基于种群**来源于它受到 GA 方法启发，即利用整个种群的知识来产生性能更好的个体。请注意，PBT 中的**个体**部分指的是种群中具有不同参数和超参数的每一个**N
    个模型**，或者所有这些 N 个模型的集合。
- en: The search process in PBT starts by *initializing a population*, *P*, that contains
    *N* models, ![](img/Formula_B18753_05_091.png), with their own randomly sampled
    parameters, ![](img/Formula_B18753_05_092.png), and randomly sampled hyperparameters,
    ![](img/Formula_B18753_05_093.png). Within each iteration of the search process,
    the *training step* is triggered for each of the *N* models. The training step
    consists of both forward and backward propagation procedures that utilize gradient-based
    optimization methods, just like the usual training procedure for a neural network-based
    model. Once the training step is done, the next step is to perform an *evaluation
    step*. The purpose of the evaluation step is to evaluate the current model’s *Mi*
    performance on the unseen validation data.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: PBT 的搜索过程首先**初始化一个包含 N 个模型**的种群 P，这些模型具有各自随机采样的参数，![](img/Formula_B18753_05_091.png)，以及随机采样的超参数，![](img/Formula_B18753_05_092.png)，![](img/Formula_B18753_05_093.png)。在搜索过程的每一轮迭代中，都会为
    N 个模型中的每一个触发**训练步骤**。训练步骤包括前向和反向传播过程，这些过程使用基于梯度的优化方法，就像基于神经网络的模型通常的训练过程一样。一旦完成训练步骤，下一步就是执行**评估步骤**。评估步骤的目的是评估当前模型在未见过的验证数据上的**Mi**性能。
- en: Once the model, *Mi*, is considered *ready*, PBT will trigger the *exploit*
    and *explore* steps. The definition of a model being ready may vary, but we can
    define “ready” as passing a predefined number of steps or passing a predefined
    performance threshold. Both the exploit and explore steps have the same goal,
    which is to update the model’s parameters and hyperparameters. The difference
    is determined by how they do the update process.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型，*Mi*，被认为*准备就绪*，PBT将触发*利用*和*探索*步骤。模型准备就绪的定义可能有所不同，但我们可以将“准备就绪”定义为通过预定义的步骤数或通过预定义的性能阈值。利用和探索步骤的目标相同，即更新模型的参数和超参数。它们之间的区别在于它们如何进行更新过程。
- en: The **exploit** step will decide, based on the evaluation results from the whole
    population, whether to keep utilizing the current set of parameters and hyperparameters
    or to focus on a more promising set. For example, the exploit step can be done
    by replacing a model that is considered as part of the bottom X% models in the
    whole population with a randomly sampled model from the top X% models in the population.
    Note that a model consists of all the parameters and hyperparameters. On the other
    hand, the **explore** step updates the model’s set of hyperparameters, *not parameters*,
    by proposing a new set. You can propose a new set by randomly perturbing the current
    set of hyperparameters with a predefined probability or by resampling the set
    of hyperparameters from the top X% models in the population. Note that this exploration
    step is only done on the chosen model from the exploitation step.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 根据整个种群的评估结果，*利用*步骤将决定是否继续使用当前的一组参数和超参数，或者关注更有希望的一组。例如，利用步骤可以通过用从种群顶部X%中随机采样的模型替换整个种群中被认为是底部X%模型的模型来完成。请注意，一个模型由所有参数和超参数组成。另一方面，*探索*步骤通过提出一组新参数来更新模型的超参数集，而不是参数。您可以通过以预定义的概率随机扰动当前的超参数集或从种群顶部X%中重新采样超参数集来提出一组新参数。请注意，此探索步骤仅在利用步骤中选择的模型上执行。
- en: Important Note
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: The exploration step in PBT is inspired by random search. This step can identify
    which subspace of hyperparameters needs to be explored more using *partially trained
    models* chosen from the exploitation step. The evaluation step that is done within
    the search process also enables us to remove the drawback of the sequential optimization
    process.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: PBT中的探索步骤受到随机搜索的启发。此步骤可以通过从利用步骤中选择的部分训练模型来识别需要更多探索的超参数子空间。在搜索过程中进行的评估步骤也使我们能够消除顺序优化过程的缺点。
- en: The exploitation and exploration procedure in the PBT method allows us to update
    a model’s set of hyperparameters in an *online fashion*, while also putting more
    focus on the promising hyperparameter and weight space. The iterative process
    of train-eval-exploit-explore is performed *asynchronously in parallel* for each
    of the *N* individuals in the population until the stopping criterion is met.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: PBT方法中的利用和探索过程使我们能够以*在线方式*更新模型的一组超参数，同时更加关注有希望的超参数和权重空间。对于种群中的每个*N*个个体，执行train-eval-exploit-explore的迭代过程是*异步并行*的，直到满足停止标准。
- en: 'The following steps summarize *how PBT works as a single training optimization
    process*:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 以下步骤总结了*PBT作为单个训练优化过程的工作原理*：
- en: Split the original full data into train, validation, and test sets (see [*Chapter
    1*](B18753_01_ePub.xhtml#_idTextAnchor014)*, Evaluating Machine Learning Models*).
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始完整数据分割成训练、验证和测试集（见[*第1章*](B18753_01_ePub.xhtml#_idTextAnchor014)*，评估机器学习模型*）。
- en: Define the hyperparameter space, *H*, with the accompanied distributions.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义超参数空间，*H*，以及伴随的分布。
- en: Define the population size, *N*, the exploration perturbation factor, *perturb_fact*,
    the exploration resampling probability, *resample_prob*, and the exploitation
    fraction, *frac*.
  id: totrans-254
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义种群大小，*N*，探索扰动因子，*perturb_fact*，探索重采样概率，*resample_prob*，以及利用分数，*frac*。
- en: Define the model’s *readiness criterion*. Usually, the number of SGD optimization
    steps is used. However, it is also possible to use the model’s performance threshold
    as the criterion.
  id: totrans-255
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义模型的*准备就绪标准*。通常，使用SGD优化步骤的数量。然而，也可以使用模型性能阈值作为标准。
- en: Define the *checkpoint directory* that is used to store the model’s weights
    and hyperparameters.
  id: totrans-256
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义用于存储模型权重和超参数的*检查点目录*。
- en: Define the evaluation function, *f*.
  id: totrans-257
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义评估函数，*f*。
- en: Initialize a population, *P*, that contains *N* models, ![](img/Formula_B18753_05_094.png),
    with their own randomly sampled parameters, ![](img/Formula_B18753_05_095.png),
    and randomly sampled hyperparameters, ![](img/Formula_B18753_05_096.png), from
    the hyperparameter space, *H*.
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化一个包含**N**个模型的人口，**P**，每个模型都有自己随机采样的参数，![](img/Formula_B18753_05_094.png)，以及从超参数空间**H**中随机采样的超参数，![](img/Formula_B18753_05_095.png)，和![](img/Formula_B18753_05_096.png)。
- en: 'For each model in the population, *P*, run the following steps *in parallel*:'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于人口**P**中的每个模型，**并行**运行以下步骤：
- en: Run one step of the training process for the model, *M*i, with the ![](img/Formula_B18753_05_097.png)
    parameter and a set of hyperparameters, ![](img/Formula_B18753_05_098.png).
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用![](img/Formula_B18753_05_097.png)参数和一组超参数![](img/Formula_B18753_05_098.png)对模型**M*i**执行训练过程的单步。
- en: 'If the *readiness criterion* has been met, do the following. If not, go back
    to *Step I*:'
  id: totrans-261
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果满足**准备标准**，请执行以下操作。如果不满足，请返回**步骤I**：
- en: Perform the *evaluation* step based on *f* on the validation set.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据验证集上的**f**值执行**评估**步骤。
- en: Perform the *exploitation* step on the model, *M*i, based on the predefined
    exploitation fraction, *frac*. This step will result in a new set of parameters
    and hyperparameters.
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据预定义的利用分数**frac**，对模型**M*i**执行**利用**步骤。这一步骤将产生一组新的参数和超参数。
- en: Perform the *exploration* step on the set of hyperparameters from the exploitation
    step based on the predefined *perturb_fact* and *resample_prob*.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据预定义的**扰动因子**和**重采样概率**，在利用步骤的超参数集上执行**探索**步骤。
- en: Perform the *evaluation* step on the new set of parameters and hyperparameters
    based on *f* on the validation set.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据验证集上的**f**值，对新的参数集和超参数集执行**评估**步骤。
- en: '*Update* the model, *M*i, with the new set of parameters and hyperparameters.'
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更新**模型**M*i**的新参数集和超参数集。'
- en: Repeat Steps I and II until the end of the training loop. Usually, it is defined
    by the number of epochs.
  id: totrans-267
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤I和II，直到训练循环结束。通常，它由epoch的数量定义。
- en: Return the model with the best evaluation score in the population, *P*.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回人口**P**中评估分数最高的模型。
- en: Evaluate the final model on the test set.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在测试集上评估最终模型。
- en: It is worth noting that, in practice, such as in the implementation of the **NNI**
    package (see [*Chapter 10*](B18753_10_ePub.xhtml#_idTextAnchor092)*, Advanced
    Hyperparameter Tuning with DEAP and Microsoft NNI*), the readiness criterion defined
    in *Step 4* is an epoch. In other words, the second step within *Step 8* will
    only be run after each training epoch, not in the middle of an epoch. It is also
    worth noting that the checkpoint directory defined in *Step 5* is needed because,
    in PBT, we need to copy weights from another model in the population, while that’s
    not the case for the other hyperparameter tuning methods we’ve learned about so
    far.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，在实践中，例如在**NNI**包的实现中（见[*第10章*](B18753_10_ePub.xhtml#_idTextAnchor092)*，使用DEAP和Microsoft
    NNI进行高级超参数调整），第4步中定义的准备标准是一个epoch。换句话说，第8步中的第二步将在每个训练epoch结束后运行，而不是在epoch的中间。还值得注意的是，第5步中定义的检查点目录是必需的，因为在PBT中，我们需要从人口中的另一个模型复制权重，而其他我们之前学过的超参数调整方法并不需要这样做。
- en: While the original PBT algorithm states that we can run *Step 8* asynchronously
    in parallel, this is not the case in the implementation of the **NNI** package,
    which will be used in this book to implement PBT. In the NNI package implementation,
    the process is run *synchronously*, meaning that we can continue to the next epoch
    once all of the individuals or models in the population have finished the previous
    epoch.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管原始的PBT算法声称我们可以异步并行地运行**步骤8**，但在本书中将使用的**NNI**包的实现中并非如此。在NNI包的实现中，过程是**同步**运行的，这意味着一旦人口中的所有个体或模型完成了上一个epoch，我们就可以继续到下一个epoch。
- en: 'The following table lists the pros and cons of the PBT method:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下表格列出了PBT方法的优缺点：
- en: '![Figure 5.16 – Pros and cons of PBT'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '![图5.16 – PBT的优缺点'
- en: '](img/B18753_05_016.jpg)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](img/B18753_05_016.jpg)'
- en: Figure 5.16 – Pros and cons of PBT
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.16 – PBT的优缺点
- en: In this section, you learned all you need to know about PBT, including what
    it is, how it works, what makes it different from other heuristic search methods,
    and its pros and cons.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你学习了关于PBT所需了解的所有内容，包括它是什么，它是如何工作的，它与其他启发式搜索方法的不同之处，以及它的优缺点。
- en: Summary
  id: totrans-277
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we discussed the third out of four groups of hyperparameter
    tuning methods, called the heuristic search group. We discussed what the heuristic
    search method is in general and several variants of heuristic search methods,
    including SA, the GA method, PSO, and PBT. We saw what makes each of the variants
    differ from each other, along with the pros and cons of each. At this point, you
    should be able to explain heuristic search in confidence when someone asks you.
    You should also be able to debug and set up the most suitable configuration of
    the chosen method that suits your specific problem definition.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了四种超参数调整方法中的第三组，称为启发式搜索组。我们一般讨论了启发式搜索方法是什么，以及包括模拟退火（SA）、遗传算法（GA）方法、粒子群优化（PSO）和参数贝叶斯树（PBT）在内的几种启发式搜索方法的变体。我们看到了这些变体之间的不同之处，以及每种方法的优缺点。此时，当有人询问你时，你应该能够自信地解释启发式搜索。你也应该能够调试并设置所选方法最适合你特定问题定义的最优配置。
- en: 'In the next chapter, we will start discussing multi-fidelity optimization,
    the last group of hyperparameter tuning methods. The goal of the next chapter
    is similar to this one’s: to provide a better understanding of the methods that
    belong to the multi-fidelity optimization group so that you can explain those
    methods in confidence when someone asks you. By doing this, you will be able to
    configure each of the methods for your specific problem!'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将开始讨论多保真优化，这是超参数调整方法的最后一组。下一章的目标与这一章类似：为了更好地理解属于多保真优化组的那些方法，以便当有人询问你时，你能自信地解释这些方法。通过这样做，你将能够为你的特定问题配置每种方法！
