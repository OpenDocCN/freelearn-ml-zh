- en: Hierarchical Clustering
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次聚类
- en: In this chapter, we're going to discuss a particular clustering technique called
    hierarchical clustering. Instead of working with the relationships existing in
    the whole dataset, this approach starts with a single entity containing all elements
    (divisive) or N separate elements (agglomerative), and proceeds by splitting or
    merging the clusters according to some specific criteria, which we're going to
    analyze and compare.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论一种称为层次聚类的特定聚类技术。这种方法不是与整个数据集中的关系一起工作，而是从一个包含所有元素的单个实体（分裂）或N个分离元素（聚合）开始，然后根据某些特定的标准分裂或合并簇，我们将分析和比较这些标准。
- en: Hierarchical strategies
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层次化策略
- en: 'Hierarchical clustering is based on the general concept of finding a hierarchy
    of partial clusters, built using either a bottom-up or a top-down approach. More
    formally, they are called:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类基于寻找部分簇的层次结构的一般概念，这些簇是通过自下而上或自上而下的方法构建的。更正式地说，它们被称为：
- en: '**Agglomerative clustering**:  The process starts from the bottom (each initial
    cluster is made up of a single element) and proceeds by merging the clusters until
    a stop criterion is reached. In general, the target has a sufficiently small number
    of clusters at the end of the process.'
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**聚合聚类**：过程从底部开始（每个初始簇由一个元素组成）并通过合并簇进行，直到达到停止标准。一般来说，目标在过程结束时具有足够小的簇数量。'
- en: '**Divisive clustering**:In this case, the initial state is a single cluster
    with all samples and the process proceeds by splitting the intermediate cluster
    until all elements are separated. At this point, the process continues with an
    aggregation criterion based on the dissimilarity between elements. A famous approach
    (which is beyond the scope of this book) called **DIANA** is described in Kaufman
    L., Roussew P.J., *Finding Groups In Data: An Introduction To Cluster Analysis*,
    Wiley.'
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分裂聚类**：在这种情况下，初始状态是一个包含所有样本的单簇，过程通过分裂中间簇直到所有元素分离。在这个点上，过程继续使用基于元素之间差异的聚合标准。一个著名的（超出了本书范围）方法称为**DIANA**，由Kaufman
    L.，Roussew P.J.，*在数据中寻找群体：聚类分析导论*，Wiley描述。'
- en: scikit-learn implements only the agglomerative clustering. However, this is
    not a real limitation because the complexity of divisive clustering is higher
    and the performances of agglomerative clustering are quite similar to the ones
    achieved by the divisive approach.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn仅实现聚合聚类。然而，这并不是一个真正的限制，因为分裂聚类的复杂度更高，而聚合聚类的性能与分裂方法达到的性能相当。
- en: Agglomerative clustering
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 聚合聚类
- en: 'Let''s consider the following dataset:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑以下数据集：
- en: '![](img/403e8d5c-2021-48a5-b640-1512d5f61bf3.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/403e8d5c-2021-48a5-b640-1512d5f61bf3.png)'
- en: 'We define **affinity**, a metric function of two arguments with the same dimensionality
    *m*. The most common metrics (also supported by scikit-learn) are:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义**亲和力**，这是一个具有相同维度 *m* 的两个参数的度量函数。最常见的度量（也由scikit-learn支持）是：
- en: '**Euclidean** or *L2*:'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**欧几里得**或 *L2*：'
- en: '![](img/6068dda2-c6f1-4f82-93c4-280ca75d55b5.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/6068dda2-c6f1-4f82-93c4-280ca75d55b5.png)'
- en: '**Manhattan** (also known as City Block) or *L1*:'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**曼哈顿**（也称为城市街区）或 *L1*：'
- en: '![](img/3c2e2e77-d772-4f7b-8661-4403eaa4e496.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/3c2e2e77-d772-4f7b-8661-4403eaa4e496.png)'
- en: '**Cosine distance**:'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**余弦距离**：'
- en: '![](img/d325da43-5b82-4426-8617-269e9039515c.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/d325da43-5b82-4426-8617-269e9039515c.png)'
- en: 'The Euclidean distance is normally a good choice, but sometimes it''s useful
    to a have a metric whose difference with the Euclidean one gets larger and larger.
    The Manhattan metric has this property; to show it, in the following figure there''s
    a plot representing the distances from the origin of points belonging to the line
    *y = x*:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离通常是好的选择，但有时拥有一个与欧几里得距离差异逐渐增大的度量是有用的。曼哈顿度量具有这种特性；为了展示这一点，在下面的图中有一个表示属于直线
    *y = x* 的点从原点到距离的图：
- en: '![](img/99347e35-cd84-405f-9c60-9d3b5e92e451.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/99347e35-cd84-405f-9c60-9d3b5e92e451.png)'
- en: The cosine distance, instead, is useful when we need a distance proportional
    to the angle between two vectors. If the direction is the same, the distance is
    null, while it is maximum when the angle is equal to 180° (meaning opposite directions).
    This distance can be employed when the clustering must not consider the *L2* norm
    of each point. For example, a dataset could contain bidimensional points with
    different scales and we need to group them into clusters corresponding to circular
    sectors. Alternatively, we could be interested in their position according to
    the four quadrants because we have assigned a specific meaning (invariant to the
    distance between a point and the origin) to each of them.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦距离，相反，在我们需要两个向量之间角度成比例的距离时很有用。如果方向相同，距离为零，而当角度等于180°（意味着相反方向）时，距离最大。这种距离可以在聚类必须不考虑每个点的*L2*范数时使用。例如，一个数据集可能包含具有不同尺度的二维点，我们需要将它们分组到对应于圆形扇区的聚类中。或者，我们可能对它们根据四个象限的位置感兴趣，因为我们已经为每个点分配了特定的含义（对点与原点之间的距离不变）。
- en: 'Once a metric has been chosen (let''s simply call it *d(x,y)*), the next step
    is defining a strategy (called **linkage**) to aggregate different clusters. There
    are many possible methods, but scikit-learn supports the three most common ones:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦选择了度量（让我们简单地称之为*d(x,y)*），下一步是定义一个策略（称为**连接**）来聚合不同的聚类。有许多可能的方法，但scikit-learn支持三种最常见的方法：
- en: '**Complete linkage**: For each pair of clusters, the algorithm computes and
    merges them to minimize the maximum distance between the clusters (in other words,
    the distance of the farthest elements):'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完全连接**：对于每一对聚类，算法计算并合并它们，以最小化聚类之间的最大距离（换句话说，最远元素的距离）：'
- en: '![](img/f6e971dc-fe8a-4bc9-b8ca-42be837dcb97.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/f6e971dc-fe8a-4bc9-b8ca-42be837dcb97.png)'
- en: '**Average linkage**: It''s similar to complete linkage, but in this case the
    algorithm uses the average distance between the pairs of clusters:'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**平均连接**：它与完全连接类似，但在这个情况下，算法使用聚类对之间的平均距离：'
- en: '![](img/0dfa57ed-60c7-4d4c-996c-3d8be1184fd2.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/0dfa57ed-60c7-4d4c-996c-3d8be1184fd2.png)'
- en: '**Ward''s linkage**: In this method, all clusters are considered and the algorithm
    computes the sum of squared distances within the clusters and merges them to minimize
    it. From a statistical viewpoint, the process of agglomeration leads to a reduction
    in the variance of each resulting cluster. The measure is:'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ward的连接**：在这个方法中，考虑所有聚类，算法计算聚类内的平方距离之和，并合并它们以最小化它。从统计学的角度来看，聚合过程导致每个结果聚类的方差减少。该度量是：'
- en: '![](img/b5095ee1-5ea0-40a3-b453-617cc1cda4b1.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/b5095ee1-5ea0-40a3-b453-617cc1cda4b1.png)'
- en: The Ward's linkage supports only the Euclidean distance.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ward的连接只支持欧几里得距离。
- en: Dendrograms
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 树状图
- en: To better understand the agglomeration process, it's useful to introduce a graphical
    method called a **dendrogram**, which shows in a static way how the aggregations are
    performed, starting from the bottom (where all samples are separated) till the
    top (where the linkage is complete). Unfortunately, scikit-learn doesn't support
    them. However, SciPy (which is a mandatory requirement for it) provides some useful
    built-in functions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解聚合过程，引入一种称为**树状图**的图形方法很有用，它以静态方式显示聚合是如何进行的，从底部（所有样本都分离）到顶部（连接完全）。不幸的是，scikit-learn不支持它们。然而，SciPy（它是其强制性要求）提供了一些有用的内置函数。
- en: 'Let''s start by creating a dummy dataset:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建一个虚拟数据集开始：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To avoid excessive complexity in the resulting plot, the number of samples
    has been kept very low. In the following figure, there''s a representation of
    the dataset:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免结果图过于复杂，样本数量已经保持得很低。在以下图中，有数据集的表示：
- en: '![](img/9fc2356b-469f-4608-9b89-8ddb84aaa110.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/9fc2356b-469f-4608-9b89-8ddb84aaa110.png)'
- en: 'Now we can compute the dendrogram. The first step is computing a distance matrix:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以计算树状图。第一步是计算距离矩阵：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We have chosen a Euclidean metric, which is the most suitable in this case.
    At this point, it''s necessary to decide which linkage we want. Let''s take Ward;
    however, all known methods are supported:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们选择了一个欧几里得度量，这在当前情况下是最合适的。此时，必须决定我们想要哪种连接。让我们选择Ward；然而，所有已知的方法都是支持的：
- en: '[PRE2]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now, it''s possible to create and visualize a dendrogram:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以创建并可视化树状图：
- en: '[PRE3]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The resulting plot is shown in the following screenshot:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示如下截图：
- en: '![](img/aeb9496e-6b92-4693-9997-536fa9fa4e81.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/aeb9496e-6b92-4693-9997-536fa9fa4e81.png)'
- en: In the *x* axis, there are the samples (numbered progressively), while the *y*
    axis represents the distance. Every arch connects two clusters that are merged
    together by the algorithm. For example, 23 and 24 are single elements merged together.
    The element 13 is then aggregated to the resulting cluster, and so the process
    continues.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在*x*轴上，有样本（按顺序编号），而*y*轴表示距离。每个弧连接两个由算法合并的簇。例如，23和24是合并在一起的单个元素。然后元素13被聚合到结果簇中，这个过程继续进行。
- en: 'As you can see, if we decide to cut the graph at the distance of 10, we get
    two separate clusters: the first one from 15 to 24 and the other one from 0 to
    20\. Looking at the previous dataset plot, all the points with *Y* < 10 are considered
    to be part of the first cluster, while the others belong to the second cluster.
    If we increase the distance, the linkage becomes very aggressive (particularly
    in this example with only a few samples) and with values greater than 27, only
    one cluster is generated (even if the internal variance is quite high!).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，如果我们决定在距离10处切割图，我们将得到两个独立的簇：第一个簇从15到24，另一个簇从0到20。查看之前的数据集图，所有*Y* < 10的点都被认为是第一个簇的一部分，而其他点属于第二个簇。如果我们增加距离，链接变得非常激进（特别是在这个只有少数样本的例子中），并且当值大于27时，只生成一个簇（即使内部方差相当高！）。
- en: Agglomerative clustering in scikit-learn
  id: totrans-44
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: scikit-learn中的层次聚类
- en: 'Let''s consider a more complex dummy dataset with 8 centers:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个具有8个中心的更复杂的虚拟数据集：
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'A graphical representation is shown in the following figure:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了图形表示：
- en: '![](img/f8717c2a-bc36-4558-91e9-a159466d9e6f.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/f8717c2a-bc36-4558-91e9-a159466d9e6f.png)'
- en: 'We can now perform an agglomerative clustering with different linkages (always
    keeping the Euclidean distance) and compare the results. Let''s start with a complete
    linkage (`AgglomerativeClustering` uses the method `fit_predict()` to train the
    model and transform the original dataset):'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用不同的链接方法（始终保持欧几里得距离）进行层次聚类，并比较结果。让我们从完全链接开始（`AgglomerativeClustering`使用`fit_predict()`方法来训练模型并转换原始数据集）：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A plot of the result (using both different markers and colors) is shown in
    the following figure:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了结果的图示（使用不同的标记和颜色）：
- en: '![](img/abd002fc-e335-49f4-90b6-b3164b37f179.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/abd002fc-e335-49f4-90b6-b3164b37f179.png)'
- en: 'The result is totally bad. This approach penalizes the inter-variance and merges
    cluster, which in most cases should be different. In the previous plot, the three
    clusters in the middle are quite fuzzy, and the probability of wrong placement
    is very high considering the variance of the cluster represented by dots. Let''s
    now consider the average linkage:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的结果完全糟糕。这种方法惩罚了组间方差并合并簇，这在大多数情况下应该是不同的。在之前的图中，中间的三个簇相当模糊，考虑到由点表示的簇的方差，错误放置的概率非常高。现在让我们考虑平均链接：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The result is shown in the following screenshot:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示在下述截图：
- en: '![](img/3e522106-8595-4b55-abd8-65a7588910ba.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/3e522106-8595-4b55-abd8-65a7588910ba.png)'
- en: 'In this case, the clusters are better defined, even if some of them could have
    become really small. It can also be useful to try other metrics (in particular
    *L1*) and compare the results. The last method, which is often the best (it''s
    the default one), is Ward''s linkage, that can be used only with a Euclidean metric
    (also the default one):'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，簇的定义更加清晰，尽管其中一些簇可能变得非常小。尝试其他度量标准（特别是*L1*）并比较结果也可能很有用。最后一种方法，通常是最佳方法（它是默认方法），是Ward的链接方法，只能与欧几里得度量一起使用（也是默认的）：
- en: '[PRE7]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The resulting plot is shown in the following figure:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了生成的结果图：
- en: '![](img/e4badc80-5485-42c6-8e06-fa09fc61ce90.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![截图](img/e4badc80-5485-42c6-8e06-fa09fc61ce90.png)'
- en: In this case, it's impossible to modify the metric so, as also suggested in
    the official scikit-learn documentation, a valid alternative could be the average
    linkage, which can be used with any affinity.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，无法修改度量标准，因此，正如官方scikit-learn文档中建议的那样，一个有效的替代方案可能是平均链接，它可以与任何亲和力一起使用：
- en: Connectivity constraints
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 连通性约束
- en: 'scikit-learn also allows specifying a connectivity matrix, which can be used
    as a constraint when finding the clusters to merge. In this way, clusters which
    are far from each other (non-adjacent in the connectivity matrix) are skipped.
    A very common method for creating such a matrix involves using the k-nearest neighbors
    graph function (implemented as `kneighbors_graph()`), that is based on the number
    of neighbors a sample has (according to a specific metric). In the following example,
    we consider a circular dummy dataset (often used in the official documentation
    also):'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn还允许指定连接矩阵，该矩阵在寻找要合并的聚类时可以用作约束。通过这种方式，彼此距离较远的聚类（在连接矩阵中不相邻）将被跳过。创建此类矩阵的一个非常常见的方法是使用基于样本邻居数量的k近邻图函数（作为`kneighbors_graph()`实现），该函数根据特定的度量来确定样本的邻居数量。在以下示例中，我们考虑了一个圆形虚拟数据集（常在官方文档中使用）：
- en: '[PRE8]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'A graphical representation is shown in the following figure:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 下图显示了图形表示：
- en: '![](img/d90add68-2585-4421-b9ab-1554e941611d.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/d90add68-2585-4421-b9ab-1554e941611d.png)'
- en: 'We start with unstructured agglomerative clustering based on average linkage
    and impose 20 clusters:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从基于平均连接的未结构化聚合聚类开始，并设定了20个聚类：
- en: '[PRE9]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'In this case, we have used the method `fit()` because the class `AgglomerativeClustering`,
    after being trained, exposes the labels (cluster number) through the instance
    variable `labels_`and it''s easier to use this variable when the number of clusters
    is very high. A graphical plot of the result is shown in the following figure:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们使用了`fit()`方法，因为`AgglomerativeClustering`类在训练后通过实例变量`labels_`公开标签（聚类编号），当聚类数量非常高时，使用此变量更方便。以下图显示了结果的图形表示：
- en: '![](img/a133383d-5e1a-4660-9264-31b8bbc09e14.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/a133383d-5e1a-4660-9264-31b8bbc09e14.png)'
- en: 'Now we can try to impose a constraint with different values for *k*:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以尝试为*k*设定不同的约束值：
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting plots are shown in the following screenshot:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 以下截图显示了生成的图表：
- en: '![](img/5f59b111-a461-4a2d-8118-6f62d97d9864.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](img/5f59b111-a461-4a2d-8118-6f62d97d9864.png)'
- en: As you can see, imposing a constraint (in this case, based on k-nearest neighbors)
    allows controlling how the agglomeration creates new clusters and can be a powerful
    tool for tuning the models, or for avoiding elements whose distance is large in
    the original space could be taken into account during the merging phase (this
    is particularly useful when clustering images).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，施加约束（在这种情况下，基于k近邻）可以控制聚合如何创建新的聚类，并且可以成为调整模型或避免在原始空间中距离较大的元素（这在聚类图像时特别有用）的有力工具。
- en: References
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 参考文献
- en: 'Kaufman L., Roussew P.J., *Finding Groups In Data: An Introduction To Cluster
    Analysis*, Wiley'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Kaufman L.，Roussew P.J.，*在数据中寻找群组：聚类分析导论*，Wiley
- en: Summary
  id: totrans-78
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'In this chapter, we have presented hierarchical clustering, focusing our attention
    on the agglomerative version, which is the only one supported by scikit-learn.
    We discussed the philosophy, which is rather different to the one adopted by many
    other methods. In agglomerative clustering, the process begins by considering
    each sample as a single cluster and proceeds by merging the blocks until the number
    of desired clusters is reached. In order to perform this task, two elements are
    needed: a metric function (also called affinity) and a linkage criterion. The
    former is used to determine the distance between the elements, while the latter
    is a target function that is used to determine which clusters must be merged.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了层次聚类，重点关注聚合版本，这是scikit-learn唯一支持的版本。我们讨论了哲学，这与许多其他方法采用的哲学相当不同。在聚合聚类中，过程从将每个样本视为单个聚类开始，并通过合并块直到达到所需的聚类数量。为了执行此任务，需要两个元素：一个度量函数（也称为亲和力）和一个连接标准。前者用于确定元素之间的距离，而后者是一个目标函数，用于确定哪些聚类必须合并。
- en: We also saw how to visualize this process through dendrograms using SciPy. This
    technique is quite useful when it's necessary to maintain a complete control of
    the process and the final number of clusters is initially unknown (it's easier
    to decide where to cut the graph). We showed how to use scikit-learn to perform
    agglomerative clustering with different metrics and linkages and, at the end of
    the chapter, we also introduced the connectivity constraints that are useful when
    it's necessary to force the process to avoid merging clusters which are too far
    apart.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还展示了如何使用SciPy通过树状图可视化这个过程。当需要保持对整个过程和最终聚类数量的完全控制，且初始时聚类数量未知（决定在哪里截断图更容易）时，这项技术非常有用。我们展示了如何使用scikit-learn执行基于不同指标和连接方式的层次聚类，并在本章末尾，我们还介绍了在需要强制过程避免合并距离过远的聚类时有用的连通性约束。
- en: In the next chapter, we're going to introduce the recommendation systems, that
    are employed daily by many different systems to automatically suggest items to
    a user, according to his/her similarity to other users and their preferences.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍推荐系统，这些系统被许多不同的系统日常使用，以根据用户与其他用户及其偏好的相似性自动向用户推荐项目。
