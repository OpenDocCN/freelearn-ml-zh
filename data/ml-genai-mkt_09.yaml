- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Creating Compelling Content with Zero-Shot Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用零样本学习创建引人入胜的内容
- en: Having introduced the promise of large language models in *Chapter 5*, we will
    go deeper into related topics in this chapter, extending our analysis from their
    role in data augmentation and sentiment analysis to their broader impact across
    different domains. This chapter introduces **zero-shot learning** (**ZSL**), a
    method in machine learning where a model can correctly make predictions for new,
    unseen classes without having received any specific training examples for those
    classes. It discusses the potential of ZSL and its application within the area
    of generative AI to create marketing copy. The discussion highlights how ZSL,
    an efficient tool to complement traditional marketing content creation processes,
    can revolutionize the generation of marketing copy.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在**第五章**中介绍了大型语言模型的潜力之后，我们将在本章深入探讨相关主题，将我们的分析从其在数据增强和情感分析中的作用扩展到其在不同领域的更广泛影响。本章介绍了**零样本学习**（**ZSL**），这是一种机器学习方法，模型可以在没有为这些类别接收任何特定训练示例的情况下，正确地对新、未见过的类别进行预测。它讨论了ZSL的潜力及其在生成式人工智能领域创建营销文案中的应用。讨论突出了ZSL作为一种高效工具，可以补充传统的营销内容创作流程，如何革命性地改变营销文案的生成。
- en: We will start with an in-depth discussion of the core principles of generative
    AI and navigate through the capabilities and limitations of these technologies,
    which will set the stage for our subsequent exploration of the importance of pre-trained
    models. We will finish the chapter with a practical walkthrough of ZSL, using
    hands-on examples to illustrate the flexibility of this approach and how we can
    use it to generate marketing content. This will equip you with the skills to understand
    and leverage this technique to elevate your marketing strategies to new heights.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将深入讨论生成式人工智能的核心原则，并探讨这些技术的能力和局限性，这将为我们随后对预训练模型重要性的探索奠定基础。我们将以ZSL的实际操作演练结束本章，通过动手示例展示这种方法的优势以及我们如何利用它来生成营销内容。这将使你具备理解和利用这项技术，将你的营销策略提升到新高度的能力。
- en: 'By the end of the chapter, you will be well versed in:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将熟练掌握：
- en: The fundamentals of generative AI and its versatile applications in marketing
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式人工智能的基础及其在市场营销中的多方面应用
- en: The principles of ZSL and its value in improving the efficiency of traditional
    content creation processes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ZSL的原则及其在提高传统内容创作流程效率中的价值
- en: Practical strategies and considerations when applying ZSL to create marketing
    copy
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在应用ZSL创建营销文案时的实际策略和考虑因素
- en: Fundamentals of generative AI
  id: totrans-8
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 生成式人工智能的基础
- en: '**Generative AI** (**GenAI**) refers to a subset of AI capable of generating
    new content, be it text, images, videos, or even synthetic data, that mirrors
    real-world examples. Unlike traditional AI models, which are designed to interpret,
    classify, or predict data based on inputs, GenAI takes it a step further by creating
    new, previously unseen outputs. It does this by understanding and learning from
    existing data patterns to produce novel outputs that maintain a logical continuity
    with the input data.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '**生成式人工智能**（**GenAI**）是指能够生成新内容的人工智能子集，无论是文本、图像、视频，甚至是反映现实世界示例的合成数据。与旨在根据输入解释、分类或预测数据的传统AI模型不同，GenAI更进一步，通过理解和学习现有数据模式来产生新的、以前未见过的输出。它通过理解并从现有数据模式中学习，以产生与输入数据保持逻辑连续性的新颖输出。'
- en: We were introduced to GenAI in *Chapter 1*, and we further touched upon it and
    its applications for sentiment analysis in *Chapter 5*. Before beginning our discussion
    of pre-trained models and ZSL, we will explore the fundamental technical considerations
    of GenAI, what it is (and is not), and why it’s so impactful for generating marketing
    content. While the focus of the hands-on examples in this chapter will involve
    text generation, important concepts that power GenAI’s capabilities in other applications
    such as images and video will also be discussed.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在**第一章**中介绍了GenAI，并在**第五章**中进一步探讨了它及其在情感分析中的应用。在开始我们关于预训练模型和ZSL的讨论之前，我们将探讨GenAI的基本技术考虑因素，它是什么（以及不是什么），以及为什么它对生成营销内容有如此大的影响。虽然本章的动手示例将涉及文本生成，但也将讨论支撑GenAI在其他应用（如图像和视频）中能力的重要概念。
- en: A probabilistic approach
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概率方法
- en: 'At the heart of GenAI is a probabilistic approach to modeling data distributions.
    This involves learning the underlying probability distribution of a dataset to
    generate new samples from that same distribution. A cornerstone of this approach
    is Bayesian inference, a principle that updates the probability of a hypothesis
    as more evidence or information becomes available. For instance, consider a simple
    equation that forms the mathematical foundation on which Bayesian inference is
    built, Bayes’ Theorem:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: GenAI 的核心是对数据分布的概率建模方法。这涉及到学习数据集的潜在概率分布，以从同一分布中生成新的样本。这种方法的一个基石是贝叶斯推理，这是一个随着更多证据或信息的出现而更新假设概率的原则。例如，考虑一个简单的方程，它是贝叶斯推理数学基础的基石，即贝叶斯定理：
- en: '![](img/B30999_09_001.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_001.png)'
- en: 'where:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 其中：
- en: '*P*(*A*∣*B*) is the posterior probability of hypothesis *A*, given the evidence
    *B*'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*∣*B*) 是在证据 *B* 下假设 *A* 的后验概率'
- en: '*P*(*B*∣*A*) is the likelihood of observing evidence *B*, given that hypothesis
    *A* is true'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*∣*A*) 是在假设 *A* 为真的情况下观察证据 *B* 的可能性'
- en: '*P*(*A*) is the prior probability of hypothesis *A*, or how likely we believe
    *A* to be true before seeing the evidence'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*A*) 是假设 *A* 的先验概率，或者在我们看到证据之前我们认为 *A* 为真的可能性'
- en: '*P*(*B*) is the probability of observing the evidence under all possible hypotheses'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*P*(*B*) 是在所有可能假设下观察证据 *B* 的概率'
- en: '**Bayes’ Theorem – a pillar of probabilistic reasoning**'
  id: totrans-19
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '**贝叶斯定理 – 概率推理的支柱**'
- en: Bayes’ Theorem is not just a cornerstone of GenAI but also a fundamental principle
    across a wide range of disciplines, from statistics and computer science to philosophy
    and medicine. At its core, Bayes’ Theorem allows us to refine our hypotheses in
    light of new evidence, offering a rigorous mathematical approach to the concept
    of learning from experience.
  id: totrans-20
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 贝叶斯定理不仅是 GenAI 的基石，也是统计学、计算机科学、哲学和医学等多个学科的基本原则。在其核心，贝叶斯定理允许我们在新证据的背景下完善我们的假设，为从经验中学习这一概念提供了一种严谨的数学方法。
- en: When extending the principles of Bayesian inference and incorporating deep learning
    models such as **recurrent neural networks** (**RNNs**), **long short-term memory
    networks** (**LSTMs**), or transformers to the generation of sequences, we enter
    the realm of conditional probabilities. This sequence generation process can be
    viewed through the lens of predicting each element based on its predecessors,
    a concept foundational not just to video and audio but also to time series modeling
    and other forms of sequential data generation applications, including text.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当将贝叶斯推理的原则扩展并融入如**循环神经网络**（**RNNs**）、**长短期记忆网络**（**LSTMs**）或转换器等深度学习模型到序列生成中时，我们进入了条件概率的领域。这个序列生成过程可以通过预测每个元素基于其前驱元素的概念来观察，这一概念不仅对视频和音频至关重要，也对时间序列建模和其他形式的序列数据生成应用，包括文本，至关重要。
- en: Training GenAI models involves vast amounts of data, and in the case of text,
    these must be broken down into smaller units known as tokens. These tokens often
    consist of subword units or phrases, making the models more efficient in understanding
    and generating natural language. The process of tokenizing text is crucial because
    it allows a model to learn the probability distribution of different sequences
    of words or subwords.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 训练 GenAI 模型需要大量的数据，在文本的情况下，这些数据必须被分解成更小的单元，称为标记。这些标记通常由子词单元或短语组成，这使得模型在理解和生成自然语言方面更加高效。文本标记化过程至关重要，因为它允许模型学习不同序列的单词或子词的概率分布。
- en: When we tokenize text, we break it down into manageable pieces that a model
    can process. Each token is then used as an input to the model during training.
    The model learns to predict the probability of the next token in a sequence, given
    the previous tokens. This probabilistic approach is where Bayesian principles
    come into play. By continuously updating the probability distribution of tokens
    as new data is introduced, the model becomes better at generating coherent and
    contextually relevant outputs.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们对文本进行标记化时，我们将其分解成模型可以处理的可管理部分。然后，每个标记在训练期间被用作模型的输入。模型学习预测序列中下一个标记的概率，给定之前的标记。这种概率方法正是贝叶斯原理发挥作用的地方。通过随着新数据的引入不断更新标记的概率分布，模型在生成连贯且上下文相关的输出方面变得更加出色。
- en: 'For example, in text generation, a model might predict the next word in a sentence
    based on the preceding words. This prediction process involves calculating the
    conditional probability:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在文本生成中，一个模型可能会根据前面的单词预测句子中的下一个单词。这个预测过程涉及计算条件概率：
- en: '![](img/B30999_09_002.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_002.png)'
- en: where *x*[t] represents the token at time *t* and *P*(*x*[t]∣*x*[1],*x*[2],…,*x*[t-1])
    denotes the probability of generating *x*[t], given the sequence of all preceding
    tokens.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x*[t] 代表时间 *t* 的标记，*P*(*x*[t]∣*x*[1]，*x*[2]，…，*x*[t-1]) 表示在给定所有先前标记的序列的情况下生成
    *x*[t] 的概率。
- en: 'In the case of video and audio generation, leveraging deep learning models
    informed by Bayesian principles helps in understanding and predicting temporal
    progression. Each frame or audio sample at time *t*(*x*[t])is predicated on the
    sequence of all previous frames or samples (*x*[1],*x*[2],…,*x*[t-1]). Mathematically,
    this relationship is captured by the previous equation:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在视频和音频生成的案例中，利用基于贝叶斯原理的深度学习模型有助于理解和预测时间进程。在时间 *t* 的每一帧或音频样本 *x*[t] 是基于所有先前帧或样本的序列（*x*[1]，*x*[2]，…，*x*[t-1]）预测的。从数学上讲，这种关系由前面的方程式捕捉：
- en: where *x*[t] represents the frame or audio sample at time *t* and ![](img/B30999_09_004.png)
    denotes the probability of generating *x*[t], given the sequence of all preceding
    samples.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *x*[t] 代表时间 *t* 的帧或音频样本，![](img/B30999_09_004.png) 表示在给定所有先前样本的序列的情况下生成 *x*[t]
    的概率。
- en: '**Source code and data**:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: '**源代码和数据**：'
- en: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.9](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.9)'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.9](https://github.com/PacktPublishing/Machine-Learning-and-Generative-AI-for-Marketing/tree/main/ch.9)'
- en: Foundational models
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基础模型
- en: 'There are several important foundational models in GenAI, each contributing
    uniquely to applications in image, text, and sequence generation. This section
    will cover some of the most important and widely used cases, such as:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成式AI中，有几个重要的基础模型，每个模型都对图像、文本和序列生成中的应用做出了独特的贡献。本节将涵盖一些最重要和最广泛使用的案例，例如：
- en: '**Generative adversarial networks** (**GANs**)'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成对抗网络**（**GANs**）'
- en: '**Variational autoencoders** (**VAEs**)'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**变分自编码器**（**VAEs**）'
- en: '**Long short-term memory networks** (**LSTMs**)'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**长短期记忆网络**（**LSTMs**）'
- en: Transformer-based models like the **Generative Pre-Trained Transformer** (**GPT**)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于Transformer的模型，如**生成预训练Transformer**（**GPT**）
- en: While comprehensive implementation examples and theory for each of these models
    are outside the scope of this chapter, we will discuss the core concepts for each
    model type, as well as provide simplified, illustrative examples of their architectures,
    in order to understand the importance of these models for marketing applications.
    In *Chapter 12*, we will extend our discussion to mention further model advances
    that have garnered more recent attention for their promise in advancing the field
    of GenAI.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管每个这些模型的全面实现示例和理论超出了本章的范围，但我们将讨论每种模型类型的核心概念，并提供其架构的简化、说明性示例，以便理解这些模型在营销应用中的重要性。在*第12章*中，我们将扩展我们的讨论，提及最近获得更多关注并有望推进生成式AI领域的模型进展。
- en: '**Exploring ML models with Google Colab notebooks**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用Google Colab笔记本探索机器学习模型**'
- en: 'Training your own state-of-the-art ML model can be computationally expensive.
    However, using Google Colab notebooks, you can train and tweak models without
    any setup on your own machine. The following are links to get started:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 训练自己的最先进机器学习模型可能成本高昂。然而，使用Google Colab笔记本，您可以在自己的机器上无需任何设置的情况下训练和调整模型。以下是一些入门链接：
- en: '**GANs** forhigh-quality image generation: [https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu](https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu
    )'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于高质量图像生成的**GANs**：[https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu](https://colab.research.google.com/drive/1uwPlY-4P_6fJ59SFRtgZLebVGgwGrUQu)
- en: '**VAEs** for image reconstruction: [https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb](https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb
    )'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**VAEs** 用于图像重建：[https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb](https://colab.research.google.com/github/smartgeometry-ucl/dl4g/blob/master/variational_autoencoder.ipynb)'
- en: '**GPTs** for language processing: [https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing
    )'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**GPTs**用于语言处理：[https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing](https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing)'
- en: '**LSTMs** for time series forecasting: [https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/lstm.ipynb](https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/lstm.ipynb)'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LSTMs**用于时间序列预测：[https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/lstm.ipynb](https://colab.research.google.com/github/d2l-ai/d2l-pytorch-colab/blob/master/chapter_recurrent-modern/lstm.ipynb)'
- en: Generative adversarial networks
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 生成对抗网络
- en: 'GANs have found applications across a wide range of domains, from image generation
    and style transfer to data augmentation and beyond. They are particularly impactful
    in applications where realistic image generation is crucial, and they have been
    used by NVIDIA and Adobe in their photo editing software to generate and modify
    images. Their applications include the following:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: GAN已在广泛的应用领域找到应用，从图像生成和风格迁移到数据增强等。它们在需要真实图像生成至关重要的应用中特别有影响力，NVIDIA和Adobe已在其照片编辑软件中使用GAN生成和修改图像。它们的应用包括以下内容：
- en: '**Content creation**: GANs can generate high-quality, realistic images, artwork,
    and videos, enabling new forms of creative content production'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容创作**：GAN可以生成高质量、逼真的图像、艺术作品和视频，从而实现新的创意内容生产形式'
- en: '**Image-to-image translation**: Applications like photo enhancement, photo-realistic
    rendering from sketches, and domain adaptation, such as day-to-night and summer-to-winter
    transformations, leverage GANs to transform images from one domain to another
    while preserving contextual details'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**图像到图像的翻译**：像照片增强、从草图生成逼真渲染和领域适应（如日夜转换和夏季到冬季转换）等应用利用GAN将图像从一个领域转换到另一个领域，同时保留上下文细节。'
- en: 'At their core, GANs consist of two neural networks that are trained simultaneously
    through a competitive process:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，GAN由两个通过竞争过程同时训练的神经网络组成：
- en: The generator (*G*) aims to generate data that is indistinguishable from real
    data
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器（*G*）旨在生成与真实数据无法区分的数据
- en: The discriminator (*D*) aims to accurately classify data as real or generated
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别器（*D*）旨在准确地将数据分类为真实或生成
- en: 'This can be illustrated by the following figure:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以通过以下图示来说明：
- en: '![A diagram of a generator  Description automatically generated](img/B30999_09_01.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![生成器图示 自动生成描述](img/B30999_09_01.png)'
- en: 'Figure 9.1: GAN workflow'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1：GAN工作流程
- en: The objective function for a GAN encapsulates the training dynamics between
    the generator and discriminator, creating a dynamic where both models improve
    in response to each other’s performance. This is similar to a two-player game
    where each player’s success is based on outsmarting their opponent. The game,
    in GAN’s case, reaches equilibrium when the generator produces perfect replicas
    of the real data, making it impossible for the discriminator to distinguish real
    from fake, which ideally results in a `0.5` probability of guessing correctly
    by the discriminator.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: GAN的目标函数封装了生成器和判别器之间的训练动态，创造了一个动态环境，其中两个模型都会根据对方的性能而提高。这类似于一个双玩家游戏，每个玩家的成功都基于智胜对手。在GAN的情况下，当生成器产生真实数据的完美复制品时，游戏达到平衡，这使得判别器无法区分真实和伪造，理想情况下，判别器正确猜测的概率为`0.5`。
- en: For GANs to be highly effective at tasks such as high-resolution image generation,
    both the generator and discriminator architectures must be carefully designed.
    This can involve incorporating advanced architectures that are effective at handling
    spatial hierarchy data, such as **convolutional neural networks** (**CNNs**).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使GAN在诸如高分辨率图像生成等任务上高度有效，生成器和判别器的架构都必须精心设计。这可能涉及采用有效的架构来处理空间层次数据，例如**卷积神经网络（CNNs）**。
- en: '**What are CNNs?**'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是卷积神经网络（CNNs）**？'
- en: CNNs are a cornerstone of machine learning for processing spatial data, such
    as images. They identify patterns using convolutional filters, excelling in tasks
    that require an understanding of spatial hierarchies. This makes CNNs indispensable
    in many GAN applications for image generation and recognition.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs是处理空间数据（如图像）的机器学习基石。它们使用卷积滤波器识别模式，在需要理解空间层次的任务中表现出色。这使得CNNs在许多GAN应用（如图像生成和识别）中变得不可或缺。
- en: In image-based GAN applications, the generator uses techniques to expand latent
    representations into detailed images, while the discriminator applies methods
    to reduce the dimensionality of the input image to assess its authenticity efficiently.
    The latent dimension, serving as the seed to generate new data instances, is a
    compact, high-dimensional space, encapsulating potential data variations in a
    compressed format.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于图像的GAN应用中，生成器使用技术将潜在表示扩展到详细图像，而判别器则应用方法降低输入图像的维度以高效地评估其真实性。潜在维度，作为生成新数据实例的种子，是一个紧凑的高维空间，以压缩格式封装潜在的数据变化。
- en: 'The following code shows the process for building the core structure of a simplified
    GAN for images, using Python, with the key steps described here:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了使用Python构建简化GAN图像核心结构的流程，其中关键步骤在此描述：
- en: 'Import the libraries needed to build the generator and discriminator:'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入构建生成器和判别器所需的库：
- en: '[PRE0]'
  id: totrans-61
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Define the generator model, which takes a latent space vector and produces
    a 28x28 image through a series of dense layers:'
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义生成器模型，它接受一个潜在空间向量并通过一系列密集层生成一个28x28的图像：
- en: '[PRE1]'
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Define the discriminator model, which takes an image and classifies it as real
    or generated through a series of dense layers:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义判别器模型，它接受一个图像并通过一系列密集层将其分类为真实或生成：
- en: '[PRE2]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Further technical considerations are necessary to address common challenges,
    such as limited output diversity and ensuring that the generated data is varied
    while still closely mirroring the real data distribution. These considerations
    include the choice of the activation function (`relu` for non-linear transformations),
    techniques to ensure consistent input distribution across layers, and strategies
    such as randomly omitting units during training to prevent overfitting.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决常见的挑战，如输出多样性有限和确保生成数据在保持与真实数据分布紧密相似的同时具有多样性，需要进一步的技术考虑。这些考虑包括激活函数（用于非线性变换的`relu`）的选择、确保层间输入分布一致的技术，以及如训练期间随机省略单元以防止过拟合的策略。
- en: 'For more details on this, you can refer to the recent paper on GANs: [https://www.researchgate.net/publication/380573076_Understanding_GANs_fundamentals_variants_training_challenges_applications_and_open_problems](https://www.researchgate.net/publication/380573076_Understanding_GANs_fundamentals_variants_training_challenges_applications_and_open_problems).'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于这方面的细节，您可以参考关于GANs的最新论文：[https://www.researchgate.net/publication/380573076_Understanding_GANs_fundamentals_variants_training_challenges_applications_and_open_problems](https://www.researchgate.net/publication/380573076_Understanding_GANs_fundamentals_variants_training_challenges_applications_and_open_problems)。
- en: Variational autoencoders
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 变分自编码器
- en: 'VAEs present a different approach to generative modeling as compared to GANs.
    VAEs offer a probabilistic way of learning latent representations of data and
    consist of two main components:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与GANs相比，VAEs（变分自编码器）在生成建模方面提出了不同的方法。VAEs提供了一种概率学习数据潜在表示的方法，并包含两个主要组件：
- en: The encoder compresses input data into a latent space representation
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码器将输入数据压缩到潜在空间表示
- en: The decoder reconstructs the data from this latent space
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器从该潜在空间重建数据
- en: Unlike traditional autoencoders, VAEs introduce a probabilistic twist where,
    rather than encoding input as a single point, they encode it as a distribution
    over the latent space.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 与传统的自动编码器不同，VAEs引入了一种概率转折，即它们不是将输入编码为单个点，而是将其编码为潜在空间上的分布。
- en: '**The versatile applications of VAEs**'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '**VAEs的多用途应用**'
- en: VAEs are instrumental in understanding and modeling complex distributions of
    data. One key area where VAEs excel is in data imputation, where they can predict
    missing information or forecast future trends in time-series data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: VAEs在理解和建模复杂数据分布方面发挥着重要作用。VAEs表现优异的关键领域之一是数据插补，其中它们可以预测缺失信息或预测时间序列数据的未来趋势。
- en: The loss function for VAEs combines reconstruction loss with the **Kullback-Leibler**
    (**KL**) divergence between the learned latent variable distribution and the prior
    distribution. The reconstruction loss measures how well the generated outputs
    match the original inputs, ensuring that a model creates accurate reproductions
    of the data, and the KL divergence acts as a form of regularization during training
    that ensures the model learns efficient and meaningful data representations. This
    regularization prevents the model from overfitting by encouraging it to generate
    outputs that are not just accurate but also generalize well to new, unseen data.
    By combining these two components, the VAE learns to produce high-quality, diverse
    outputs and encourages the model to learn efficient encodings of the data.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: VAE（变分自编码器）的损失函数结合了重建损失与学习到的潜在变量分布与先验分布之间的**Kullback-Leibler**（KL）散度。重建损失衡量生成的输出与原始输入匹配的程度，确保模型能够创建准确的数据复制品，而KL散度在训练期间充当正则化形式，确保模型学习到高效且有意义的数据表示。这种正则化通过鼓励模型生成不仅准确而且对新、未见数据具有良好泛化能力的输出，防止模型过拟合。通过结合这两个组件，VAE学习生成高质量、多样化的输出，并鼓励模型学习数据的有效编码。
- en: 'The following is a simplified example of constructing a VAE using Keras:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用Keras构建VAE的简化示例：
- en: 'We will use a flattened 28x28 image input as another example, and we will first
    sample from the latent distribution in the `sampling()` function:'
  id: totrans-77
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将使用一个28x28的扁平图像输入作为另一个示例，我们首先在`sampling()`函数中从潜在分布中采样：
- en: '[PRE3]'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'We then use the encoder to map the inputs into latent space:'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随后使用编码器将输入映射到潜在空间：
- en: '[PRE4]'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We then use the decoder to reconstruct the image from the latent space:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们随后使用解码器从潜在空间重建图像：
- en: '[PRE5]'
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: For the effective application of VAEs, selecting the right architecture for
    the encoder and decoder is crucial, often involving densely connected layers for
    basic tasks or more sophisticated structures, like CNNs, for image data. The dimensionality
    of the latent space is also vital – it must be large enough to capture relevant
    data variations but not so large that it leads to overfitting or meaningless reconstructions.
    When designed correctly, VAEs offer a principled approach to generative modeling,
    balancing the need for accurate data reconstruction with the flexibility to generate
    new, diverse samples from learned data distributions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于VAE的有效应用，选择合适的编码器和解码器架构至关重要，通常涉及密集连接层进行基本任务，或更复杂的结构，如CNN（卷积神经网络）进行图像数据。潜在空间的空间维度也非常关键——它必须足够大以捕捉相关的数据变化，但不能太大以至于导致过拟合或无意义的重建。当设计正确时，VAE提供了一种原则性的生成建模方法，在准确数据重建的需求与从学习到的数据分布生成新、多样化样本的灵活性之间取得平衡。
- en: Long short-term memory networks
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 长短期记忆网络
- en: LSTMs are a specialized type of RNN designed to learn long-term dependencies
    in sequential data. RNNs are a class of neural networks that include loops, allowing
    information to persist by passing it from one step of the network to the next.
    This looping mechanism makes RNNs suitable for processing sequences of data such
    as time series or text. However, standard RNNs often struggle with learning long-range
    dependencies, due to issues like vanishing and exploding gradients. This occurs
    because, during backpropagation, gradients can become exponentially small (vanish)
    or large (explode), making it difficult to update the network weights effectively.
    LSTMs address these challenges with a more complex internal structure that allows
    them to remember information for longer periods effectively.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM是一种专门设计的RNN（循环神经网络），用于学习序列数据中的长期依赖关系。RNN是一类包含循环的神经网络，允许信息通过将信息从一个网络步骤传递到下一个步骤来持续存在。这种循环机制使RNN适合处理时间序列或文本等数据序列。然而，标准RNN由于梯度消失和梯度爆炸等问题，在学习长期依赖关系时往往遇到困难。这是因为，在反向传播过程中，梯度可能变得非常小（消失）或非常大（爆炸），这使得有效地更新网络权重变得困难。LSTM通过一个更复杂的内部结构来解决这些挑战，使其能够有效地记住信息更长时间。
- en: 'The defining feature of LSTMs that enables them to remember information more
    effectively is their cell state, along with three types of gates: input, output,
    and forget gates. These components work together to regulate the flow of information,
    allowing a network to remember important information over long periods and to
    forget irrelevant data.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM（长短期记忆网络）的显著特征是它们能够更有效地记住信息，这得益于它们的细胞状态以及三种类型的门：输入门、输出门和遗忘门。这些组件共同工作，调节信息的流动，使网络能够在长时间内记住重要信息，并忘记无关数据。
- en: '**Key components of an LSTM**'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**LSTM的关键组件**'
- en: 'Core to the LSTM are its cell state and gates with the following functions:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM的核心是其细胞状态和具有以下功能的门：
- en: '**Input gate**: How much new information to store in the cell state'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入门**：决定在细胞状态中存储多少新信息'
- en: '**Forget gate**: What information is discarded from the cell state'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗忘门**：从细胞状态中丢弃哪些信息'
- en: '**Output gate**: The output of the cell state to the next layer'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出门**：细胞状态输出到下一层'
- en: 'The following code sets up a simple LSTM network for time-series prediction
    that could be used to predict the next day’s sales, based on different features
    from the previous day. In this architecture, we allow for 10 days to be captured
    by `sequence_length` and then 5 features by `num_feature`, which could include
    data points such as web traffic or previous sales. The LSTM layer with 50 units
    learns to recognize patterns in the sequence data, while the `Dense` layer outputs
    the prediction. Finally, the model is compiled with the Adam optimizer and mean
    squared error (`mse`) loss function, common choices for regression tasks:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码设置了一个简单的LSTM网络，用于时间序列预测，可用于根据前一天的不同特征预测第二天销售额。在这个架构中，我们允许`sequence_length`捕获10天的数据，然后通过`num_feature`捕获5个特征，这些特征可能包括如网站流量或之前的销售额等数据点。具有50个单位的LSTM层学习识别序列数据中的模式，而`Dense`层输出预测。最后，该模型使用Adam优化器和均方误差(`mse`)损失函数编译，这些是回归任务的常见选择：
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'To demonstrate the training process, we can generate synthetic time-series
    data that mimics sales prediction data. The synthetic data includes a base sine
    wave with added noise, simulating daily patterns with random fluctuations:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示训练过程，我们可以生成模拟销售预测数据的合成时间序列数据。合成数据包括一个基本正弦波和添加的噪声，模拟具有随机波动的每日模式：
- en: '[PRE7]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'After training, we can evaluate the model on test data by comparing the predicted
    sales values with the actual values:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，我们可以通过比较预测的销售额值与实际值来在测试数据上评估模型：
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code produces the following plot, showing how a simple LSTM model with
    training can quickly provide useful predictions for metrics, such as sales, if
    given appropriate input features:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 代码生成了以下图表，展示了如何通过训练一个简单的LSTM模型，如果给定适当的输入特征，可以快速提供有用的预测，例如销售额：
- en: '![](img/B30999_09_02.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_02.png)'
- en: 'Figure 9.2: Output of an LSTM sales prediction model'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2：LSTM销售预测模型的输出
- en: Transformers
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**Transformer**'
- en: Transformer-based models, such as the GPT series, have revolutionized natural
    language processing by introducing a model architecture that excels in capturing
    context and relationships within data. The core innovation of transformer models
    is the attention mechanism, which enables a model to weigh the importance of different
    parts of the input data differently. This mechanism allows GPT and similar models
    to understand the context and generate text that is coherent and contextually
    relevant.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Transformer的模型，如GPT系列，通过引入一个在捕捉数据中的上下文和关系方面表现卓越的模型架构，彻底改变了自然语言处理。Transformer模型的核心创新是注意力机制，它使模型能够根据输入数据的不同部分以不同的方式权衡其重要性。这种机制允许GPT和类似模型理解上下文并生成连贯且上下文相关的文本。
- en: 'GPT models leverage the transformer architecture for generative tasks, trained
    on vast amounts of text data to understand language patterns, grammar, and context.
    This pre-training enables GPT models to generate text that is highly coherent
    and contextually relevant to the input prompts. While building a GPT from scratch
    is a formidable task, at the conceptual level, there are some key components within
    a GPT architecture:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: GPT模型利用Transformer架构进行生成任务，在大量文本数据上进行训练以理解语言模式、语法和上下文。这种预训练使GPT模型能够根据输入提示生成高度连贯且上下文相关的文本。虽然从头开始构建GPT是一个艰巨的任务，但在概念层面上，GPT架构中存在一些关键组件：
- en: '**Embedding layer**: Converts token indices to dense vectors of fixed size'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**嵌入层**：将标记索引转换为固定大小的密集向量'
- en: '**Multi-head attention**: Allows a model to focus on different parts of the
    input sequence simultaneously, capturing various contextual relationships'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**多头注意力**：允许模型同时关注输入序列的不同部分，捕捉各种上下文关系'
- en: '**Layer normalization and residual connections**: Help stabilize and optimize
    the training process, ensuring that gradients flow smoothly throughout a network'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层归一化和残差连接**：有助于稳定和优化训练过程，确保梯度在整个网络中顺畅流动'
- en: 'The following code shows how a simplified GPT-like architecture can be created
    using Keras with the aforementioned components:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了如何使用Keras和上述组件创建一个简化的类似GPT的架构：
- en: '[PRE9]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '**The power of self-attention in GPT**'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**GPT中自注意力的力量**'
- en: Self-attention, the key innovation behind GPT models, allows a network to weigh
    the importance of different words in a sentence, enhancing its understanding of
    context and relationships between words.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 自注意力是GPT模型背后的关键创新，它允许网络权衡句子中不同单词的重要性，增强其对上下文和单词之间关系的理解。
- en: To learn more, check out the paper *Attention Is All You Need* by Illia Polosukhin
    et al. ([https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多信息，请参阅Illia Polosukhin等人撰写的论文《Attention Is All You Need》（[https://arxiv.org/pdf/1706.03762](https://arxiv.org/pdf/1706.03762)）。
- en: When GenAI is the right fit
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 当GenAI是合适的选择时
- en: 'While GenAI brings new realms of possibility to content creation and digital
    marketing, understanding its limitations is also crucial. GenAI shines in environments
    that demand innovation, creativity, and the ability to scale personalized content
    dynamically. More generally, GenAI can be a great fitfor your marketing campaign
    in the following cases:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然GenAI为内容创作和数字营销带来了新的可能性，但了解其局限性同样重要。GenAI在需要创新、创造力和动态扩展个性化内容的环境下表现出色。更普遍地说，以下情况下，GenAI可以成为您营销活动的绝佳选择：
- en: '**Brainstorming for creative campaigns**: Generating unique and compelling
    content, be it text, image, or video, to facilitate brainstorming for creative
    marketing campaigns that stand out in a crowded digital landscape. For example,
    we will use GenAI to generate text for a new product launch in this chapter and
    *Chapter 10*.'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创意活动的头脑风暴**：生成独特且引人入胜的内容，无论是文本、图像还是视频，以促进创意营销活动的头脑风暴，使其在拥挤的数字领域中脱颖而出。例如，我们将在本章和第10章中使用GenAI生成新产品发布文案。'
- en: '**Dynamic content personalization**: Enabling marketers to tailor content at
    scale while still addressing individual user preferences and behaviors, in order
    to increase engagement and conversion rates. For instance, we will show how GenAI
    combined with **retrieval-augmented generation** (**RAG**) can be used to create
    personalized recommendations and email content, based on individual browsing history
    and purchase behavior, in *Chapter 11*.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**动态内容个性化**：使营销人员能够大规模定制内容，同时仍然关注个别用户的偏好和行为，以提高参与度和转化率。例如，我们将在第11章中展示如何将GenAI与**检索增强生成**（**RAG**）相结合，根据个人的浏览历史和购买行为创建个性化的推荐和电子邮件内容。'
- en: '**Efficiency in content production**: Automating the content generation process,
    significantly reducing the time and resources needed to produce marketing materials.'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**内容生产效率**：自动化内容生成过程，显著减少生产营销材料所需的时间和资源。'
- en: However, GenAI isn’t a one-size-fits-all solution, particularly in marketing
    scenarios where accuracy and deep contextual understanding are needed. For example,
    its application in highly regulated industries or in sensitive campaigns around
    social issues, where a misstep can significantly impact a brand’s reputation,
    must be done with caution. While GenAI can help with brainstorming in these cases,
    the unpredictability of GenAI content could pose significant risks if deployed
    without careful monitoring.Further discussion of this topic will be presented
    in *Chapter 13*, along with strategies to improve its contextual understanding
    in *Chapters 10* and *11*.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，GenAI并非万能的解决方案，尤其是在需要准确性和深度上下文理解的营销场景中。例如，在高度监管的行业或围绕社会问题的敏感活动中，任何失误都可能对品牌声誉造成重大影响，因此必须谨慎行事。虽然GenAI可以帮助这些情况下的头脑风暴，但如果未经仔细监控就部署GenAI内容，其不可预测性可能会带来重大风险。关于这一主题的进一步讨论将在第13章中呈现，以及在第10章和第11章中介绍提高其上下文理解策略。
- en: '**GenAI in highly regulated industries**'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '**GenAI在高度监管的行业中的应用**'
- en: When applying GenAI in sectors like healthcare, financial services, insurance,
    and legal services, adherence to stringent regulatory standards is crucial. Marketing
    content for these applications requires extra scrutiny, as they must not only
    be accurate and transparent but also align with industry-specific compliance measures.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 当在医疗保健、金融服务、保险和法律服务等行业应用GenAI时，遵守严格的监管标准至关重要。这些应用的营销内容需要额外的审查，因为它们不仅需要准确和透明，还需要符合行业特定的合规措施。
- en: Introduction to pre-trained models and ZSL
  id: totrans-120
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 预训练模型和ZSL的介绍
- en: Building on the foundations of GenAI discussed in the chapter so far, we will
    now introduce some core concepts related to pre-trained models and **zero-shot
    learning** (**ZSL**). These concepts underly how models can take vast amounts
    of existing data to create realistic, new outputs for scenarios that have not
    yet been encountered, with little to no additional training. With a focus on text
    data, we will discuss how contextual embeddings and semantic proximity are two
    key concepts that facilitate this capability. With this knowledge, you will be
    equipped to understand and apply these concepts in this chapter and the ones to
    come.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 建立在前面章节所讨论的 GenAI 基础之上，我们现在将介绍一些与预训练模型和**零样本学习**（**ZSL**）相关的核心概念。这些概念解释了模型如何利用大量现有数据，在不进行额外训练的情况下，为尚未遇到的情况生成真实、新的输出。重点关注文本数据，我们将讨论上下文嵌入和语义邻近性是如何作为两个关键概念来促进这一能力的。有了这些知识，你将能够理解并应用这些概念在本章以及后续章节中。
- en: Contextual embeddings
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 上下文嵌入
- en: Contextual embeddings, enabled by advancements such as the LSTM and GPT models
    discussed earlier, are fundamental to how **large language models** (**LLM**s)
    interpret and generate language. As discussed in *Chapter 5*, embeddings are dense
    vector representations of data that capture key features in a high-dimensional
    space. Early models like Word2Vec and GloVe generate static embeddings where the
    same word always has the same vector. In contrast, advanced models like BERT and
    GPT create contextual embeddings, where word representations change based on their
    usage in context. Effective NLP embeddings preserve the semantic relationships
    of the original data, meaning similar vectors are closer together in vector space.
    This adaptability is foundational for applications such as ZSL, which rely on
    a model’s ability to apply learned knowledge to new tasks without specific training
    data.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文嵌入，得益于之前讨论过的 LSTM 和 GPT 模型的进步，对于**大型语言模型**（**LLM**s）如何理解和生成语言至关重要。正如在*第五章*中讨论的那样，嵌入是数据的密集向量表示，在多维空间中捕捉关键特征。早期的模型如
    Word2Vec 和 GloVe 生成静态嵌入，其中相同的单词总是有相同的向量。相比之下，先进的模型如 BERT 和 GPT 创建上下文嵌入，其中单词表示根据其在上下文中的使用而变化。有效的
    NLP 嵌入保留了原始数据的语义关系，这意味着在向量空间中相似的向量彼此更接近。这种适应性是 ZSL 等应用的基础，这些应用依赖于模型将学习到的知识应用于新任务的能力，而无需特定的训练数据。
- en: Earlier in the chapter in the exploration of GenAI’s probabilistic nature, we
    noted how text generation is analogous to sequence prediction in video or audio,
    in that the relevance of each piece of data depends on its predecessors. As an
    analogy, consider how Google’s auto-suggest feature adapts suggestions based on
    the context of the words already entered. This same concept underpins the transformative
    potential of models like BERT, which analyzes text from both preceding and subsequent
    contexts to enhance language comprehension and prediction accuracy, via their
    contextual embeddings.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章早期对 GenAI 的概率性质的探索中，我们注意到文本生成与视频或音频中的序列预测类似，即每份数据的相关性取决于其前驱。作为一个类比，考虑一下谷歌的自动补全功能如何根据已输入单词的上下文来调整建议。这个相同的概念是模型如
    BERT 的变革性潜力之所在，它通过上下文嵌入分析前后的文本上下文，以增强语言理解和预测准确性。
- en: GPT models take it a step further and adopt an autoregressive framework. The
    term “autoregressive” means that a model makes predictions based on its own previous
    outputs, meaning that it anticipates the subsequent word based on all preceding
    outputs of the model as context. For example, when developing a content calendar
    for a marketing blog, a GPT model can analyze past articles and trending topics
    to suggest new posts that align with the brand’s voice and audience interests.
    This is unlike transformer-based models, which can look at both preceding and
    following words simultaneously, as discussed earlier in the chapter. However,
    such autoregressive models can offer more nuanced text generation, enabling them
    to create narratives with a level of coherence that bidirectional models may not
    achieve as seamlessly.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: GPT 模型更进一步，采用自回归框架。术语“自回归”意味着模型基于其自身的先前输出进行预测，这意味着它根据模型的所有先前输出作为上下文来预测后续的单词。例如，当为营销博客开发内容日历时，GPT
    模型可以分析过去的文章和热门话题，以建议与品牌声音和受众兴趣相符的新帖子。这与本章前面讨论的基于转换器的模型不同，后者可以同时查看前后的单词。然而，这种自回归模型可以提供更细致的文本生成，使它们能够创建具有更高一致性的叙事，这可能比双向模型更无缝地实现。
- en: '**The importance of contextual embeddings**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '**上下文嵌入的重要性**'
- en: Contextual embeddings from LSTM or GPT models allow for nuanced understanding
    by evaluating more of the text in its entirety.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: LSTM 或 GPT 模型的上下文嵌入可以通过评估文本的整体更多内容来实现细微的理解。
- en: For instance, in filling in the blank in “The stormy seas calmed as the ___
    sailed into the harbor,” a model leveraging both prior and subsequent context
    could infer “ship” with greater accuracy, whereas a more naive model with only
    prior context might inaccurately predict the word “day.”
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在填写“暴风雨的海面平静下来，因为___驶进了港口”的空白处时，一个利用先前和后续上下文的模型可以更准确地推断出“船”，而一个只有先前上下文的更简单模型可能会错误地预测“天”这个词。
- en: Semantic proximity
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 语义邻近度
- en: Transitioning from our discussion on contextual embeddings and their critical
    role in language models, we will now explore **semantic proximity**. Contextual
    embeddings not only enhance the understanding of text by considering its dynamic
    contexts; they also serve as a fundamental tool in evaluating the semantic relationships
    between words or phrases within that text. This nuanced understanding is pivotal
    when we examine the concept of semantic proximity, which involves quantifying
    how closely related or distant two linguistic items are in meaning.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们对上下文嵌入及其在语言模型中关键作用的讨论中过渡，我们现在将探讨**语义邻近度**。上下文嵌入不仅通过考虑文本的动态上下文来增强对文本的理解；它们还作为评估文本中单词或短语之间语义关系的基本工具。当我们考察语义邻近度的概念时，这种细微的理解至关重要，它涉及量化两个语言项目在意义上的密切程度或距离。
- en: For example, consider the phrases “limited-time offer” and “exclusive deal.”
    These phrases have close semantic proximity because they both relate to targeted
    promotions for potential customers. Conversely, the phrases “limited-time offer”
    and “customer feedback” would have a much larger semantic distance.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑短语“限时优惠”和“独家交易”。这两个短语具有紧密的语义邻近度，因为它们都与针对潜在客户的定向促销相关。相反，“限时优惠”和“客户反馈”这两个短语会有更大的语义距离。
- en: 'Semantic proximity is effectively assessed through methods such as cosine similarity,
    which measures the angle between vectors representing these items in a high-dimensional
    space. This metric, rooted in the geometry of vector spaces, provides a clear,
    mathematical way to capture and compare the meanings encoded by embeddings. Mathematically,
    cosine similarity is given by:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 通过余弦相似度等方法，可以有效地评估语义邻近度，这些方法测量的是在多维空间中代表这些项目的向量的角度。这个基于向量空间几何的度量，提供了一种清晰、数学的方式来捕捉和比较嵌入编码的意义。数学上，余弦相似度如下所示：
- en: '![](img/B30999_09_005.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_09_005.png)'
- en: where *A* and *B* are vectors (semantic embeddings for a word, phrase, document,
    etc), *A*⋅*B* denotes their dot product, and ∥*A*∥ and ∥*B*∥ represent their magnitudes.
    The value of cosine similarity ranges from -1 to 1\. A value of 1 implies that
    the vectors are identical in direction, indicating maximum similarity. A value
    of 0 implies that the vectors are orthogonal, indicating no similarity.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *A* 和 *B* 是向量（一个词、短语、文档等的语义嵌入），*A*⋅*B* 表示它们的点积，而 ∥*A*∥ 和 ∥*B*∥ 表示它们的模长。余弦相似度的值范围从
    -1 到 1。值为 1 表示向量在方向上完全相同，表示最大相似度。值为 0 表示向量正交，表示没有相似度。
- en: 'While, in practice, many sophisticated text embeddings are high-dimensional
    and can range from hundreds to thousands of dimensions, we can more easily illustrate
    the concept of cosine similarity in 2D space via two vectors. In the following
    code, we illustrate the calculation of cosine similarity using vectors *A* and
    *B*, with the angle associated with their cosine similarity:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然，在实践中，许多复杂的文本嵌入是高维的，可以从几百维到几千维不等，但我们可以通过两个向量在二维空间中更直观地说明余弦相似度的概念。在下面的代码中，我们通过向量
    *A* 和 *B* 说明了余弦相似度的计算，以及与它们余弦相似度相关的角度：
- en: '[PRE10]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](img/B30999_09_03.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](img/B30999_09_03.png)'
- en: 'Figure 9.3: Visualization of cosine similarity between two vectors, A and B,
    showing their angular relationship in 2D space'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.3：两个向量 A 和 B 之间余弦相似度的可视化，显示了它们在二维空间中的角度关系
- en: 'Continuing from the mathematical foundation of cosine similarity, we can apply
    this same concept to explore the polysemous nature of words in different contexts.
    Consider the word “light” used in two different sentences:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 在余弦相似度的数学基础上继续，我们可以将这个相同的概念应用于探索不同上下文中单词的多义性。考虑在两个不同句子中使用的单词“light”：
- en: '*He turned on the* **light** *to read*.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '*他打开* **灯** *来阅读*。'
- en: '*The* **light** *fabric was perfect for summer*.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '*这种* **轻质** *织物非常适合夏天*。'
- en: These sentences demonstrate different semantic instances of the word “light,”
    and by employing contextual embedding models like BERT, we can quantify the semantic
    differences of “light” in these cases using cosine similarity.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些句子展示了“光”这个词的不同语义实例，通过使用BERT等上下文嵌入模型，我们可以使用余弦相似度来量化这些情况下“光”的语义差异。
- en: 'As an example, in the following code, we:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在以下代码中，我们：
- en: Import libraries and load the pre-trained BERT model and tokenizer.
  id: totrans-144
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 导入库并加载预训练的BERT模型和分词器。
- en: Tokenize the sentences, converting them into the format expected by the BERT
    model.
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将句子分词，将它们转换为BERT模型期望的格式。
- en: Pass the tokenized sentences through the BERT model to obtain the embeddings.
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将分词句子通过BERT模型传递以获得嵌入。
- en: Extract embeddings for the word “light” by finding the index of the word in
    each tokenized sentence and extracting its corresponding embedding from the model
    output.
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在每个分词句子中找到“光”这个词的索引并从模型输出中提取其对应的嵌入来提取“光”的嵌入。
- en: Compute cosine similarity and print the result.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算余弦相似度并打印结果。
- en: 'To do this, use the following code:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，请使用以下代码：
- en: '[PRE11]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: A cosine similarity value ranges from -1 to 1\. As mentioned earlier, a value
    of 1 indicates identical vectors, while a value of 0 implies orthogonal vectors
    with no similarity. In this case, a cosine similarity of 0.48 suggests that the
    embeddings for “light” in the two sentences are somewhat similar but not identical.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 余弦相似度值范围从-1到1。如前所述，1表示向量完全相同，而0表示正交向量，没有相似性。在这种情况下，余弦相似度为0.48表明两个句子中“光”的嵌入相似但并不相同。
- en: Pre-trained models
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 预训练模型
- en: Pre-trained models are machine learning algorithms that have been previously
    trained on large datasets to perform general tasks, such as understanding natural
    language or recognizing objects in images. The utility of pre-trained models is
    fundamentally rooted in their use of the embeddings they were trained on. For
    text, these embeddings not only enable models to grasp context dynamically but
    also serve as the foundation to adapt these models for specific tasks, such as
    sentiment analysis, as discussed in *Chapter 5*, with minimal to no additional
    training. This adaptability is critical for applications such as ZSL.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型是之前在大型数据集上训练过的机器学习算法，用于执行通用任务，如理解自然语言或识别图像中的对象。预训练模型的作用根本在于它们使用训练时使用的嵌入。对于文本，这些嵌入不仅使模型能够动态地掌握上下文，而且也是将这些模型适应特定任务（如第5章中讨论的情感分析）的基础，而无需或仅需少量额外的训练。这种适应性对于ZSL等应用至关重要。
- en: The advent of pre-trained models democratized access to state-of-the-art AI
    by offering a base model that can be fine-tuned or used for inference directly.
    These models not only reduce the cost and time to deployment for AI-driven solutions
    but also the need for computational resources and energy consumption, making AI
    both more accessible and environmentally friendly. In the marketing domain, pre-trained
    models offer significant advantages. They enable marketers to quickly deploy advanced
    AI solutions for tasks such as personalized content creation, customer sentiment
    analysis, and targeted advertising.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型的出现通过提供可以微调或直接用于推理的基础模型，使最先进的AI访问民主化。这些模型不仅降低了AI驱动解决方案的部署成本和时间，还降低了计算资源和能源消耗，使AI更加易于访问且环保。在营销领域，预训练模型提供了显著的优势。它们使营销人员能够快速部署先进的AI解决方案，用于个性化内容创建、客户情绪分析和定向广告等任务。
- en: Sophisticated AI models that were previously attainable only for large or highly
    specialized technology companies are now a possibility for smaller-sized businesses,
    and even individual consumers, at a fraction of the cost. For perspective, training
    a model such as GPT-3 from scratch was estimated to be in the millions of dollars
    at the time of its release in 2020, a figure that encapsulates computational costs
    and human expertise. Today, a user can perform inference to generate text using
    this same (or a more advanced) model through the company’s API, at the cost of
    a few cents for hundreds of words of content.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 以前只有大型或高度专业化的科技公司才能获得的复杂人工智能模型，现在以极低成本成为小型企业和甚至个人消费者的可能。以GPT-3为例，2020年发布时从头开始训练这样一个模型估计需要数百万美元，这个数字包括了计算成本和人力专业知识。如今，用户可以通过公司的API使用这个（或更先进的）模型进行推理，生成文本，费用仅为数百字内容的几分钱。
- en: 'The key components of a pre-trained model are:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型的关键组件包括：
- en: '**Weights**: They represent the learned parameters from training datasets and
    encode a wide range of knowledge and patterns needed for transfer learning'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**权重**：它们代表从训练数据集中学习到的参数，并编码了迁移学习所需的广泛知识和模式'
- en: '**Architecture**: The model’s structure, detailing how inputs are processed
    through various layers to generate outputs'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**架构**：模型的详细结构，说明输入如何通过各个层处理以生成输出'
- en: '**Pre-processing steps**: Procedures like tokenization and normalization to
    ensure data compatibility with the model’s training'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预处理步骤**：如分词和归一化等程序，以确保数据与模型训练的兼容性'
- en: Let’s look at these components in detail.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们详细看看这些组件。
- en: Model weights
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型权重
- en: Model weights are at the core of neural networks, including pre-trained models.
    Weights are the refined parameters developed through extensive training to minimize
    loss, acting as a repository of a model’s learned knowledge. For instance, in
    language models like GPT, these weights capture the intricacies of language, such
    as grammar and context, enabling the generation of text that’s not only coherent
    but also contextually rich. The effectiveness of pre-trained models in tasks like
    ZSL stems from these weights, which enable you to generalize from the model’s
    training data to new, unseen data with remarkable accuracy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 模型权重是神经网络的核心，包括预训练模型。权重是通过大量训练得到的优化参数，以最小化损失，作为模型学习知识的存储库。例如，在GPT等语言模型中，这些权重捕捉了语言的复杂性，如语法和上下文，使得生成的文本不仅连贯，而且上下文丰富。预训练模型在ZSL等任务中的有效性源于这些权重，它们使您能够从模型训练数据中泛化到新的、未见过的数据，并具有显著的准确性。
- en: 'To better understand where the model weights come from, consider the case of
    an **artificial neural network** (**ANN**), as shown in the following figure:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解模型权重从何而来，考虑以下**人工神经网络**（**ANN**）的例子，如图所示：
- en: '![](img/B30999_09_04.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_04.png)'
- en: 'Figure 9.4: Example ANN architecture from Chapter 6'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4：第6章中的示例ANN架构
- en: 'This consists of three main layers: the input layer, the hidden layer, and
    the output layer. During training, neural networks undergo forward and backward
    propagation. Forward propagation involves feeding data through a network to generate
    an output, while backward propagation adjusts weights based on the error of the
    predictions. Through these iterations, the network learns the optimal weights
    for each neuron, minimizing prediction error and enhancing a model’s performance.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这由三个主要层组成：输入层、隐藏层和输出层。在训练过程中，神经网络会经历正向传播和反向传播。正向传播涉及将数据通过网络生成输出，而反向传播则根据预测误差调整权重。通过这些迭代，网络学习每个神经元的最佳权重，最小化预测误差并提高模型性能。
- en: Model architecture
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型架构
- en: The architecture of a pre-trained model goes hand in hand with its weights.
    It delineates the structure of how data is processed and transformed across the
    model’s layers and also guides the adaptability of the model to perform novel
    tasks. For example, deeper language model architectures might be better suited
    for complex reasoning tasks, while models with specially configured attention
    mechanisms can offer finer control over the focus of the model during inference.
    For image recognition, the intermediate representations produced by these models,
    such as features extracted at various layers by a pre-trained CNN, can also serve
    as a valuable starting point for classification tasks.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练模型的架构与其权重密切相关。它界定了数据在模型层之间处理和转换的结构，并指导模型适应执行新任务。例如，更深层的语言模型架构可能更适合复杂推理任务，而具有特别配置的注意力机制的模型可以在推理过程中提供更精细的控制。对于图像识别，这些模型产生的中间表示，如预训练CNN在各个层提取的特征，也可以作为分类任务的宝贵起点。
- en: 'When understanding a machine learning model’s architecture, it can be helpful
    to plot it and its parameters to visualize its key aspects. Here, we illustrate
    this using Keras’s `plot_model()`, as a demonstration of a simple LSTM model composed
    of two LSTM layers:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在理解机器学习模型的架构时，绘制其架构及其参数有助于可视化其关键方面。在此，我们使用Keras的`plot_model()`来展示这一点，作为由两个LSTM层组成的简单LSTM模型的演示：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![](img/B30999_09_05.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_05.png)'
- en: 'Figure 9.5: A simple LSTM-based neural network architecture'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5：基于简单LSTM的神经网络架构
- en: 'This visualization clearly delineates the model’s structure, showing the progression
    from input through two LSTM layers, each with 64 units (or neurons), to a final
    dense output layer configured with 10 softmax units for class prediction. Based
    on the labels given in the figure, the structure can be further broken down as
    follows:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 这张可视化图清晰地描绘了模型的结构，显示了从输入经过两个具有64个单元（或神经元）的LSTM层，到配置有10个softmax单元的最终密集输出层的演变。根据图中的标签，结构可以进一步分解如下：
- en: '**Input layer (lstm_input)**: Takes in data with a shape of `(10, 128)`, processing
    sequences of 10 timesteps, each with 128 features'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输入层（lstm_input）**：接受形状为 `(10, 128)` 的数据，处理10个时间步长的序列，每个时间步长有128个特征。'
- en: '**First LSTM layer (lstm)**: Contains 64 units and returns sequences, processing
    the input and passing on sequences of the same length to the next LSTM layer'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第一层LSTM（lstm）**：包含64个单元，返回序列，处理输入并将相同长度的序列传递给下一层LSTM。'
- en: '**Second LSTM layer (lstm_1)**: Also has 64 units but does not return sequences,
    compressing the output from the first LSTM layer into a single vector'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第二层LSTM（lstm_1）**：也有64个单元，但不返回序列，将第一层LSTM的输出压缩成一个单一向量。'
- en: '**Dense output layer (dense)**: The final layer is a dense layer with 10 units
    and a softmax activation function, outputting a probability distribution over
    10 classes (categories or labels)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**密集输出层（密集）**：最后一层是一个包含10个单元的密集层，并使用softmax激活函数，输出10个类别（分类或标签）的概率分布。'
- en: Preprocessing steps
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预处理步骤
- en: 'Preprocessing steps are crucial for ensuring that data is compatible with a
    pre-trained model’s training procedure. Here are a couple of examples:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 预处理步骤对于确保数据与预训练模型的训练过程兼容至关重要。以下是一些例子：
- en: NLP **tokenization** breaks down text into words or subwords that are consistent
    with what an LLM model expects
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLP **分词**将文本分解成LLM模型期望的单词或子词。
- en: For images, **normalization** can adjust image pixel values to a common scale
    to facilitate a model’s ability to learn from and generate predictions for data
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于图像，**归一化**可以调整图像像素值到一个共同尺度，以促进模型从数据中学习和生成预测的能力。
- en: These preprocessing steps are essential for leveraging pre-trained models effectively,
    ensuring that input data mirrors the form of the data that the model was trained
    on.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这些预处理步骤对于有效地利用预训练模型至关重要，确保输入数据与模型训练时所用的数据形式相匹配。
- en: Zero-shot learning
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 零样本学习
- en: Following our exploration of pre-trained models and their fundamental components,
    we will now shift our focus to their application in ZSL. As we will discuss later,
    the fundamentals behind ZSL allow marketers to dynamically generate relevant content
    and even target their marketing campaigns to individual consumers in near real
    time. Before we get to those examples, this section will provide some background
    on ZSL and how it enables models to apply learned knowledge and infer information
    about tasks or classes that were not explicitly covered during their training.
    This capability extends the utility of pre-trained models, allowing them to generalize
    across unseen data and scenarios.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们探索了预训练模型及其基本组件之后，现在我们将关注它们在零样本学习（ZSL）中的应用。正如我们稍后将要讨论的，ZSL背后的基本原理允许营销人员动态生成相关内容，甚至几乎实时地针对个别消费者进行营销活动。在我们进入这些例子之前，本节将提供一些关于ZSL的背景信息以及它是如何使模型能够应用所学知识并推断在训练期间未明确涵盖的任务或类别的信息的。这种能力扩展了预训练模型的应用范围，使它们能够泛化到未见过的数据和场景。
- en: Mechanics of learning and prediction
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习和预测的机制
- en: 'At its core, ZSL operates by using transformations and mappings of the input
    and output in a high-dimensional embedding space, using two primary functions:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ZSL的核心操作是通过在高维嵌入空间中使用输入和输出的变换和映射，使用两个主要功能：
- en: 'An **embedding function** *f*: ![](img/B30999_09_006.png) that transforms input
    – such as images or text – into feature vectors within the embedding space.'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**嵌入函数** *f*：![](img/B30999_09_006.png)，它将输入（如图像或文本）转换为嵌入空间内的特征向量。
- en: 'A **semantic attribute function** *g*: ![](img/B30999_09_007.png) that associates
    class labels with semantic attributes in the same embedding space. These attributes
    describe classes in terms of universal, distinguishable features, existing within
    an attribute space *A*.'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个**语义属性函数** *g*：![](img/B30999_09_007.png)，它将类别标签与嵌入空间中的语义属性关联起来。这些属性用普遍的、可区分的特征来描述类别，存在于属性空间
    *A* 中。
- en: 'As a simplified example, take a case where ZSL is applied to distinguish between
    animals. The embedding function *f*: ![](img/B30999_09_008.png) would transform
    visual images of the animal into high-dimensional feature vectors within the embedding
    space – for example, taking images of a sparrow and an eagle that are processed
    to show distinct feature vectors, representing their visual characteristics. Simultaneously,
    the semantic attribute function *g*: ![](img/B30999_09_009.png) maps class labels,
    like *sparrow* and *eagle*, to vectors in the same embedding space based on semantic
    attributes. Attributes for *sparrow* might include `[′small_size′, ′brown_color′,
    ′has_wings′]`, whereas *eagle* could be characterized by `[′large_size′, ′sharp_beak′,
    ′has_wings′]`. These attributes are then quantified, where `small_size` might
    be encoded as `0.2` on a size scale, and `large_size` as `0.8`.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 作为简化的例子，考虑将ZSL应用于区分动物的情况。嵌入函数*f*：![](img/B30999_09_008.png)将动物的视觉图像转换为嵌入空间中的高维特征向量——例如，处理麻雀和鹰的图像以显示不同的特征向量，代表它们的视觉特征。同时，语义属性函数*g*：![](img/B30999_09_009.png)根据语义属性将类标签，如*麻雀*和*鹰*，映射到相同的嵌入空间中的向量。麻雀的属性可能包括`[′small_size′,
    ′brown_color′, ′has_wings′]`，而鹰可能被描述为`[′large_size′, ′sharp_beak′, ′has_wings′]`。然后对这些属性进行量化，其中`small_size`可能编码为尺寸尺度上的`0.2`，而`large_size`为`0.8`。
- en: '![](img/B30999_09_06.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_06.png)'
- en: 'Figure 9.6: Example mappings from an input image or class label to an embedding
    space'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6：从输入图像或类标签到嵌入空间的示例映射
- en: By placing both images and class attributes within the same embedding space,
    as shown in *Figure 9.6*, a model can match an image’s feature vector to the closest
    class attribute vector. This matching is facilitated even if the model has never
    seen an image of a sparrow or eagle during training, by recognizing the overlap
    in their attributes with those of known classes. To find the best class match,
    the algorithm is tasked with the optimization of a compatibility function, which
    quantifies the match between an input’s feature vector and a class’s attribute
    vector. The ZSL prediction then involves selecting the class that maximizes compatibility
    for an unseen instance.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图9.6*所示，通过将图像和类属性放置在相同的嵌入空间中，模型可以将图像的特征向量与最近的类属性向量相匹配。即使模型在训练过程中从未见过麻雀或鹰的图像，这种匹配也可以通过识别它们属性与已知类之间的重叠来实现。为了找到最佳的类匹配，算法负责优化兼容性函数，该函数量化了输入特征向量与类属性向量之间的匹配程度。ZSL预测随后涉及选择对未见实例兼容性最大的类。
- en: Output parameters
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输出参数
- en: 'There are a number of key parameters that can be specified to influence the
    output of ZSL models. These parameters tailor the output by adjusting the behavior
    of the model during the sampling process. Three of the most common ones used in
    the context of text generation with models like GPT include:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多关键参数可以指定以影响ZSL模型的输出。这些参数通过调整模型在采样过程中的行为来定制输出。在GPT等模型用于文本生成的情况下，最常用的三个参数包括：
- en: Temperature
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 温度
- en: Top P (nucleus sampling)
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Top P（核采样）
- en: Frequency penalty
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 频率惩罚
- en: By influencing how the model’s outputs are sampled from the probability distribution
    generated by the compatibility function, each of these parameters allows adjustments
    in the creativity, coherence, and diversity of the model’s outputs. The following
    sections provide further detail on the theory underlying each of these parameters.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 通过影响模型从兼容性函数生成的概率分布中采样的方式，每个参数都允许调整模型输出的创造力、连贯性和多样性。以下各节提供了关于每个这些参数背后理论的进一步细节。
- en: Temperature
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 温度
- en: 'The temperature parameter plays a crucial role in determining the level of
    randomness or confidence in the prediction distribution. Mathematically, adjusting
    the temperature modifies the `softmax` function used to calculate the probabilities
    of the next word in the following manner:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 温度参数在确定预测分布的随机性或置信水平方面起着至关重要的作用。从数学上讲，调整温度会以以下方式修改用于计算下一个单词概率的`softmax`函数：
- en: '![](img/B30999_09_010.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_010.png)'
- en: where *T* is the temperature and logit represents the raw outputs from the model.
    A lower temperature sharpens the distribution, making the model’s predictions
    more deterministic and less varied, whereas a higher temperature flattens the
    distribution, encouraging diversity in the predictions at the cost of potentially
    introducing less coherence, as the model becomes more likely to sample less probable
    words.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *T* 是温度，logit代表模型的原生输出。较低的温度使分布更尖锐，使模型的预测更确定性和更少变化，而较高的温度则使分布更平坦，鼓励预测的多样性，但可能会引入较少的连贯性，因为模型更有可能采样不太可能的单词。
- en: 'To illustrate the concept, let’s consider a predictive text generation scenario
    of the next word in the sentence “`The cat sat on the ___.`" For simplicity, assume
    our model considers five possible completions: “`mat`,” “`tree`,” “`ball`,” “`bed`,”
    and “`tabl`e,” with the initial logits reflecting their probabilities (assigned
    manually in this case for simplicity). We can use the following code to produce
    a visualization, demonstrating how changing the temperature parameter *T* affects
    the `softmax` function, altering the probability distribution of these potential
    next words:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这个概念，让我们考虑一个预测文本生成场景，即句子“`The cat sat on the ___.`”中的下一个词。为了简化，假设我们的模型考虑了五种可能的完成方式：“`mat`”，“`tree`”，“`ball`”，“`bed`”，和“`tabl`e”，其初始logit反映了它们的概率（在这种情况下为了简化，手动分配）。我们可以使用以下代码来生成一个可视化，展示如何改变温度参数
    *T* 影响softmax函数，改变这些潜在下一个词的概率分布：
- en: '[PRE14]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This gives us the following output:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们以下输出：
- en: '![](img/B30999_09_07.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_07.png)'
- en: 'Figure 9.7: Visualization of the impact of the temperature parameter on softmax
    probability distributions for potential next words in the sentence “The cat sat
    on the ___”'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7：温度参数对句子“The cat sat on the ___”中潜在下一个词的softmax概率分布的影响可视化
- en: As illustrated in the graph, at lower temperatures the distribution is sharper,
    concentrating the probability mass on fewer, more likely outcomes, like “`bed`.”
    As the temperature increases, the distribution becomes flatter, giving a higher
    probability to a broader set of potentially less likely word outcomes, such as
    `table` or `mat`.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，在较低的温度下，分布更尖锐，将概率质量集中在更少、更可能的输出上，如“`bed`”。随着温度的升高，分布变得更平坦，给更广泛的、可能不太可能的单词输出更高的概率，如`table`或`mat`。
- en: Top P
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Top P
- en: Top P, or **nucleus sampling**, offers a dynamic way to focus the generation
    process on the most plausible set of outcomes. Instead of considering the entire
    vocabulary, the model limits its choices to the smallest set of words whose cumulative
    probability exceeds the threshold P.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: Top P，或称为**核采样**，提供了一种动态的方式来聚焦生成过程，使其集中在最可能的输出集上。模型不是考虑整个词汇表，而是将其选择限制在累积概率超过阈值P的最小单词集。
- en: 'This approach can be thought of as dynamically adjusting the breadth of consideration
    based on the model’s confidence, according to the formula:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法可以被视为根据模型置信度动态调整考虑范围，其公式如下：
- en: '![](img/B30999_09_011.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_011.png)'
- en: where *N* is the number of words considered and *P*(*w*[i]) is the probability
    of the *i*^(th) word. This technique helps maintain a balance between variety
    and relevance, ensuring that the generated text remains plausible without being
    overly constrained by the most likely predictions.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N* 是考虑的单词数量，*P*(*w*[i]) 是第 *i* 个单词的概率。这项技术有助于在多样性和相关性之间保持平衡，确保生成的文本既合理又不受最可能预测的过度约束。
- en: Frequency penalty
  id: totrans-215
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 频率惩罚
- en: 'The **frequency penalty** addresses the tendency of models to repeat the same
    words or phrases, enhancing the diversity of the output. It modifies the probability
    of each word based on its previous occurrences in the generated text:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: '**频率惩罚**解决了模型倾向于重复相同单词或短语的问题，增强了输出的多样性。它根据生成文本中每个单词的先前出现次数修改每个单词的概率：'
- en: '![](img/B30999_09_012.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](img/B30999_09_012.png)'
- en: where *P*(*w*[i]) is the original probability of the word *w*[i] and *Occurrence*
    (*w*[i]) is the number of times *w*[i] has appeared in the text so far. This adjustment
    encourages the exploration of new vocabulary and ideas by penalizing words that
    the model has already used, promoting a richer and more varied output.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *P*(*w*[i]) 是单词 *w*[i] 的原始概率，*Occurrence* (*w*[i]) 是 *w*[i] 在文本中出现的次数。这种调整通过惩罚模型已经使用的单词来鼓励探索新的词汇和想法，从而促进更丰富和更多样化的输出。
- en: ZSL for marketing copy
  id: totrans-219
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 市场文案的零样本学习（ZSL）
- en: We will now discuss the practical application of ZSL through the example of
    an e-commerce brand that is launching new lines of eco-friendly kitchenware and
    fashion products. Traditionally, creating compelling product descriptions and
    promotional content could require significant research and creative effort from
    writers familiar with a brand’s tone and the intricacies of sustainable design.
    However, with ZSL, the brand can input a concise description of the product line,
    emphasizing keywords like “sustainable” and “eco-friendly activewear,” into a
    pre-trained model, immediately producing an output of brand-appropriate content
    that is ready for consideration across digital platforms. By automating these
    initial stages of content generation, the brand can now focus more on higher value
    efforts like strategy, engagement, and analyzing the effectiveness of its marketing
    efforts.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将通过一个电子商务品牌推出新的环保厨房用品和时尚产品线的例子来讨论ZSL的实际应用。传统上，创作吸引人的产品描述和促销内容可能需要熟悉品牌语气和可持续设计复杂性的作家进行大量研究和创意工作。然而，有了ZSL，品牌可以将产品线的简洁描述输入到一个预训练模型中，强调“可持续”和“环保运动装”等关键词，立即生成适合品牌且适用于数字平台的适当内容输出。通过自动化内容生成的初始阶段，品牌现在可以更多地关注战略、参与度和分析其营销活动的有效性等高价值工作。
- en: 'In general, to integrate ZSL effectively into your marketing strategy, consider
    the following iterative process as a template:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，为了有效地将ZSL整合到您的营销策略中，请考虑以下迭代过程作为模板：
- en: Define your content goals and the key messages you want to communicate.
  id: totrans-222
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义你的内容目标以及你想要传达的关键信息。
- en: Create concise prompts that encapsulate these goals and messages, incorporating
    relevant keywords and themes.
  id: totrans-223
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建简洁的提示，包含这些目标和信息，并融入相关关键词和主题。
- en: Experiment with different parameters (`temperature`, `Top P`, and `frequency
    penalty`) to adjust the style and diversity of the generated content.
  id: totrans-224
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试不同的参数（`温度`、`Top P`和`频率惩罚`）来调整生成内容的风格和多样性。
- en: Generate multiple content variations to explore different angles and ideas.
  id: totrans-225
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成多个内容变体，以探索不同的角度和想法。
- en: Review and refine the output, selecting the best options that align with your
    brand’s tone and objectives.
  id: totrans-226
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 审查和精炼输出，选择与您品牌语气和目标一致的最佳选项。
- en: Use the generated content as a starting point for further customization and
    optimization, based on audience feedback and performance metrics.
  id: totrans-227
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据受众反馈和性能指标，将生成的内容作为进一步定制和优化的起点。
- en: In the following subsections, we will focus on steps 1–4 of the preceding workflow,
    with steps 5 and 6 covered by examples in the chapters that will follow.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下子节中，我们将重点关注先前工作流程的第1-4步，第5和6步将在后续章节的示例中介绍。
- en: Preparing for ZSL in Python
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在Python中准备ZSL
- en: To demonstrate the Python setup process for ZSL, this section will walk through
    the basic steps of using both freely available, open-source models, as well as
    more advanced models requiring paid API-based implementations. We will demonstrate
    the former with models available in the Hugging Face Transformers library, and
    we will demonstrate the setup for the latter using OpenAI’s API service.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示ZSL的Python设置过程，本节将介绍使用免费开源模型以及需要付费API实现的更高级模型的基本步骤。我们将使用Hugging Face Transformers库中的模型演示前者，并使用OpenAI的API服务演示后者的设置。
- en: '**Staying ahead with Hugging Face updates**'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '**紧跟Hugging Face更新**'
- en: Hugging Face’s Transformers library frequently updates, making advanced models
    that were previously behind paid API services freely available. For the latest
    Hugging Face models available, consult their documentation at `https://huggingface.co/models`.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Hugging Face的Transformers库经常更新，使得之前仅通过付费API服务才能获得的先进模型现在可以免费使用。有关最新Hugging Face模型的详细信息，请参阅他们的文档：`https://huggingface.co/models`。
- en: 'The basic setup for ZSL tasks using the gpt2 model available on Hugging Face
    is:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Hugging Face上可用的gpt2模型进行ZSL任务的基本设置如下：
- en: '[PRE15]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Running this prompt results in an output such as the following:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此提示将产生如下输出：
- en: '[PRE16]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'We can contrast this with the output that is returned from a model with the
    same parameters but using the more advanced text generation capabilities of OpenAI’s
    GPT-4\. Executing this model currently requires the creation of an OpenAI account
    and then generating an API key that can be substituted in the following code:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这与其他具有相同参数但使用OpenAI的GPT-4更先进文本生成能力的模型的输出进行对比。执行此模型目前需要创建一个OpenAI账户，然后生成一个可以替换到以下代码中的API密钥：
- en: '[PRE17]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This results in a response such as:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 这会导致以下类型的响应：
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: It’s evident from these two responses that advanced models like GPT-4 significantly
    enhance the relevancy and quality of the generated content. This is because, compared
    to GPT-2, GPT-4 incorporates major advancements in deep learning architectures,
    larger training datasets, and more sophisticated fine-tuning techniques. For this
    reason, we will use results obtained from GPT-4 for the remainder of this chapter.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 从这两个响应中可以看出，像GPT-4这样的高级模型显著提高了生成内容的关联性和质量。这是因为，与GPT-2相比，GPT-4在深度学习架构、更大的训练数据集和更复杂的微调技术方面取得了重大进步。因此，我们将使用GPT-4获得的结果在本章的剩余部分进行使用。
- en: However, the key to leveraging ZSL effectively lies not just in a model’s capabilities
    but also in effectively choosing the input information that shapes the output.
    The most critical aspects here include creating an effective prompt, as well as
    setting other parameters such as temperature and Top P, as discussed earlier in
    the chapter.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有效利用ZSL的关键不仅在于模型的能力，还在于有效地选择塑造输出的输入信息。这里最关键的部分包括创建一个有效的提示，以及设置其他参数，如温度和Top
    P，如本章前面所讨论的。
- en: '**Use the outputs of LLMs with caution**'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '**谨慎使用LLM的输出**'
- en: Relying directly on the output of any LLM for critical campaign content without
    human review can be risky! Always consider treating the initial outputs of GenAI
    models as a creative starting point, and then iterate and refine them as needed
    to ensure alignment with your brand’s voice and objectives.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在没有人工审查的情况下直接依赖任何LLM的输出作为关键活动内容可能存在风险！始终考虑将GenAI模型的初始输出视为创意起点，然后根据需要迭代和改进它们，以确保与您的品牌声音和目标保持一致。
- en: Creating an effective prompt
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建一个有效的提示
- en: Creating an effective prompt is the most crucial step in leveraging ZSL for
    marketing copy. In ZSL, the prompt effectively becomes the instruction manual
    for a model, telling it what kind of content to generate, as well as its style,
    tone, and substance.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个有效的提示是利用ZSL进行营销文案的关键步骤。在ZSL中，提示实际上成为了一个模型的操作手册，告诉它需要生成什么类型的内容，以及其风格、语气和实质。
- en: 'The following are some guidelines around how to formulate prompts that will
    elicit the best possible marketing copy content from the model:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一些关于如何制定提示以从模型中获得最佳营销文案内容的指南：
- en: '**Clarity**: Ensure that your prompt is specific about what you want, whether
    it’s a product description, headline, or call to action.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**清晰**：确保您的提示具体说明您想要的内容，无论是产品描述、标题还是行动号召。'
- en: '**Contextual**: Provide sufficient background to guide a model. For eco-friendly
    products, mention key selling points like sustainability or biodegradability.'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情境**：提供足够的背景信息以引导模型。对于环保产品，提及关键卖点，如可持续性或生物降解性。'
- en: '**Creative**: While clarity is crucial, leaving room for creativity can yield
    surprising and innovative results. Phrases like “Imagine...” or “Create a story
    where...” can be particularly powerful.'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**创意**：虽然清晰度至关重要，但留出创意空间可以产生令人惊讶和创新的成果。像“想象...”或“创造一个故事，其中...”这样的短语可能特别有效。'
- en: '**Concise**: Lengthy prompts can dilute the focus. Aim for brevity while including
    essential details, ensuring that a model stays on topic.'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**简洁**：冗长的提示会分散焦点。在包含必要细节的同时追求简洁，确保模型保持主题。'
- en: 'In the following sections, we will illustrate the impact of prompt quality
    through examples, with different types of marketing copy. While good prompts elicit
    detailed, relevant, and engaging content, poor prompts can lead to vague and uninspiring
    outputs. To generate these responses, we will define the following function:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下章节中，我们将通过示例展示提示质量的影响，包括不同类型的营销文案。虽然好的提示可以引发详细、相关和吸引人的内容，但差的提示可能导致模糊和缺乏灵感的结果。为了生成这些响应，我们将定义以下函数：
- en: '[PRE19]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: This function will be used with different prompt types in the examples that
    follow.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中，这个功能将与不同的提示类型一起使用。
- en: 'Example 1: Product descriptions'
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例1：产品描述
- en: In this example, we will generate product descriptions for our e-commerce brand,
    which is launching new lines of eco-friendly kitchenware.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将为我们的电子商务品牌生成产品描述，该品牌即将推出环保厨房用品的新系列。
- en: 'The following is an example of a poor prompt:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个差的提示的例子：
- en: '[PRE20]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This produces:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生：
- en: '[PRE21]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now, let’s look at the following example of a good prompt:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看以下良好提示的例子：
- en: '[PRE22]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'This prompt produces the following output:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 此提示将产生以下输出：
- en: '[PRE23]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: From a marketing perspective, this example demonstrates the significance of
    creating detailed and audience-specific prompts with clear requirements when using
    ZSL for product descriptions, as well as how this leads to more specificity in
    the generated response. However, it is worth noting that older consumers may value
    more straightforward, factual information and, therefore, may favor the more generic
    prompt’s response from an engagement standpoint. Tailoring GenAI outputs at the
    level of the individual consumer can be crucial as well and is a topic discussed
    in *Chapter 11*.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 从营销角度来看，这个例子说明了在为产品描述使用ZSL时创建详细且针对特定受众的提示，以及这如何导致生成的响应更加具体。然而，值得注意的是，年长的消费者可能更重视直接、事实性的信息，因此，从参与度的角度来看，他们可能更喜欢更通用的提示的响应。在个人消费者层面调整GenAI的输出同样至关重要，这也是第11章讨论的主题。
- en: 'Example 2: Blog post titles'
  id: totrans-266
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例2：博客标题
- en: In our next example, we will focus on another type of marketing copy by generating
    blog post titles for our e-commerce brand.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的下一个例子中，我们将专注于另一种类型的营销文案，通过为我们的电子商务品牌生成博客标题。
- en: 'We’ll start by generating a poor prompt:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将生成一个差的提示：
- en: '[PRE24]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This produces the following:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE25]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Here’s an example of a good prompt:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个良好提示的例子：
- en: '[PRE26]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This gives us a more engaging result:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了更吸引人的结果：
- en: '[PRE27]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Comparing these prompts for blog post titles illustrates the impact of specificity
    and audience targeting on content effectiveness, where a specific prompt highlighting
    biodegradable kitchenware creates content aligned more with sustainability. In
    contrast, a vague prompt results in a generic title that would fail to differentiate
    itself amid a sea of similar content. To tailor the language produced by LLMs
    even further, we can also use **few-shot learning** (**FSL**), the topic of the
    next chapter. Used effectively, FSL can achieve the same specificity in language
    but in a way that’s aligned with a brand’s unique voice, in order to distinguish
    LLM outputs from what other LLMs might produce, even when given the same prompt.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 比较这些用于博客标题的提示，可以说明具体性和受众定位对内容有效性的影响，其中强调可生物降解厨房用品的具体提示会产生与可持续性更一致的内容。相比之下，模糊的提示会导致一个通用的标题，在众多类似内容中无法脱颖而出。为了进一步调整LLM生成的语言，我们还可以使用**少量样本学习**（**FSL**），这是下一章的主题。如果使用得当，FSL可以在语言上实现相同的具体性，但以一种与品牌独特声音一致的方式，以便区分LLM的输出与其他LLM可能产生的输出，即使给出相同的提示。
- en: '**Navigating topical content with AI**'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '**使用AI导航主题内容**'
- en: When generating blog posts, understanding your model’s training data recency
    is crucial. Without current data or web search capabilities, you risk creating
    content based on outdated trends that lack the necessary context for relevant
    outputs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在生成博客文章时，了解您模型的训练数据的新鲜度至关重要。如果没有当前数据或网络搜索功能，您可能会基于过时的趋势创建内容，这些趋势缺乏相关输出的必要背景。
- en: 'Example 3: Social media captions'
  id: totrans-279
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 示例3：社交媒体配文
- en: In this example, we will generate an Instagram caption for a post about our
    e-commerce brand.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们将为关于我们电子商务品牌的帖子生成Instagram配文。
- en: 'Let’s look at a poor prompt first:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看一个差的提示：
- en: '[PRE28]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This yields the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下结果：
- en: '[PRE29]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we will look at an example of a good prompt:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将查看一个良好提示的例子：
- en: '[PRE30]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This produces:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生：
- en: '[PRE31]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The difference between these two prompts for Instagram captions illustrates
    how a specific prompt generates a caption that not only engages with its witty
    language but also directly appeals to eco-conscious consumers, likely increasing
    likes, shares, and comments – all crucial metrics on social platforms. In contrast,
    the vague prompt results in a generic and broad caption that, while informative,
    lacks a focused appeal and may fail to capture the attention of certain potential
    customers looking for eco-friendly products.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个Instagram配文提示之间的差异说明了具体提示如何生成一个配文，不仅以其机智的语言吸引人，而且直接吸引环保意识强的消费者，这可能会增加点赞、分享和评论——所有都是社交平台上的关键指标。相比之下，模糊的提示导致了一个通用的、广泛的配文，虽然信息丰富，但缺乏专注的吸引力，可能无法吸引寻找环保产品的某些潜在客户。
- en: Impact of parameter tuning
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数调整的影响
- en: While creating an effective prompt lays the groundwork, fine-tuning the model’s
    parameters is equally essential to align the generated content with the desired
    marketing style. In this section, we will explore how adjusting parameters like
    temperature and Top P affect the output of a language model. Transitioning from
    kitchenware, we will demonstrate this by generating marketing slogans for an eco-friendly
    and sustainable fashion line launch.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然创建一个有效的提示是基础，但调整模型的参数同样重要，以确保生成的内容与期望的营销风格相一致。在本节中，我们将探讨如何通过调整温度和Top P等参数来影响语言模型的输出。从厨房用品过渡到，我们将通过为环保和可持续时尚线的发布生成营销口号来展示这一点。
- en: 'We will do this by defining the following Python function, outputting sets
    of three variants of the slogan for each alteration to one of these parameters:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过定义以下Python函数来完成这项工作，输出每个参数调整后口号的三种变体：
- en: '[PRE32]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Let’s use the base case, with both `temperature` and `Top P` set to values
    of `1.0`:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用基础情况，将`temperature`和`Top P`都设置为`1.0`的值：
- en: '[PRE33]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'This produces:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生：
- en: '[PRE34]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Now, let’s tweak each parameter and see the output that we get:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们调整每个参数并查看我们得到的输出：
- en: '**Temperature**: Increasing temperature makes responses more diverse and creative
    but potentially less coherent:'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Temperature**：提高温度会使响应更加多样化和富有创意，但可能不那么连贯：'
- en: '[PRE35]'
  id: totrans-300
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'This produces:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生：
- en: '[PRE36]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Conversely, lowering the temperature results in more predictable and consistent
    outputs:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，降低温度会导致更可预测和一致性的输出：
- en: '[PRE37]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This time, the prompt produces the following output:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，提示产生了以下输出：
- en: '[PRE38]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '**Top P**: Reducing `top_p` from its maximum value of 1.0 narrows the possible
    variety of generated slogans by making the model more conservative, as it tends
    to select only the most probable outputs:'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Top P**：将`top_p`从其最大值1.0降低，会使生成的口号的多样性减少，因为模型变得更加保守，它倾向于只选择最可能的输出：'
- en: '[PRE39]'
  id: totrans-308
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'This produces:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生：
- en: '[PRE40]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Through these examples, we can observe the significant impact that parameter
    adjustments can have on the nature of generated content. This demonstrates the
    importance of parameter tuning in creating marketing slogans that are not only
    relevant and engaging but also tailored to the style and message of a brand.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些示例，我们可以观察到参数调整对生成内容性质的重大影响。这证明了参数调整在创建既相关又吸引人、又符合品牌风格和信息的营销口号中的重要性。
- en: Summary
  id: totrans-312
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we went on a journey through GenAI, and ZSL, and their transformative
    potential in marketing content creation. We introduced foundational GenAI concepts
    and discussed the mechanisms that allow these models to generate text, images,
    and more, with a particular focus on text generation. Analyzing contextual embeddings
    and semantic proximity highlighted the nuances that pre-trained models like GPT
    and BERT bring to understanding and generating language with remarkable adaptability
    and accuracy.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们探讨了GenAI和ZSL在营销内容创作中的变革潜力。我们介绍了基础GenAI概念，并讨论了这些模型生成文本、图像等机制，特别关注文本生成。分析上下文嵌入和语义邻近性突出了预训练模型如GPT和BERT在理解和生成语言时带来的细微差别，这些模型具有显著的可适应性和准确性。
- en: Central to our discussion was the application of ZSL in creating marketing copy,
    which allows brands to generate compelling content without the need for extensive
    training data. We outlined a strategic process to integrate ZSL into marketing
    strategies with the help of examples that emphasize the importance of creating
    clear, contextual, and creative prompts. This step-by-step approach – defining
    content goals, experimenting with parameters, and refining outputs – enables marketers
    to harness the power of LLMs effectively. We also learned how adjusting parameters
    such as temperature and Top P can help fine-tune the creativity, coherence, and
    diversity of the generated content. These practical insights will help you optimize
    marketing copy to align with brand messaging and campaign objectives.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们讨论的核心是将ZSL应用于创建营销文案，这允许品牌在没有大量训练数据的情况下生成引人入胜的内容。我们概述了一个战略过程，通过强调创建清晰、情境化和创造性的提示的重要性，将ZSL整合到营销策略中。这种逐步的方法——定义内容目标、实验参数和优化输出——使营销人员能够有效地利用LLMs的力量。我们还学习了如何调整温度和Top
    P等参数可以帮助微调生成内容的创造力、连贯性和多样性。这些实用见解将帮助您优化营销文案，使其与品牌信息和活动目标保持一致。
- en: Looking ahead, the next chapter progresses into the more advanced territories
    of few-shot and transfer learning. Building on the ZSL foundation, we will explore
    how these techniques can further refine GenAI models for on-target messaging.
    This involves adapting models to new contexts with minimal examples (FSL) and
    updating them with brand or customer-specific information (transfer learning),
    ensuring consistency and relevance in the generated content.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 展望未来，下一章将深入探讨更高级的领域，即少样本学习和迁移学习。在ZSL的基础上，我们将探讨这些技术如何进一步优化GenAI模型以实现目标信息传播。这包括通过最小示例（FSL）调整模型以适应新环境，以及通过品牌或客户特定信息（迁移学习）更新模型，确保生成内容的一致性和相关性。
- en: Join our book’s Discord space
  id: totrans-316
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 加入我们书籍的 Discord 空间
- en: 'Join our Discord community to meet like-minded people and learn alongside more
    than 5000 members at:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 加入我们的 Discord 社区，与志同道合的人相聚，并与其他5000多名成员一起学习，详情如下：
- en: '[https://packt.link/genai](https://packt.link/genai)'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://packt.link/genai](https://packt.link/genai)'
- en: '![](img/QR_Code12856128601808671.png)'
  id: totrans-319
  prefs: []
  type: TYPE_IMG
  zh: '![](img/QR_Code12856128601808671.png)'
