- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Attacking Models with Adversarial Machine Learning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用对抗性机器学习攻击模型
- en: Recent advances in **machine learning** (**ML**) and **artificial intelligence**
    (**AI**) have increased our reliance on intelligent algorithms and systems. ML
    systems are used to make decisions on the fly in several critical applications.
    For example, whether a credit card transaction should be authorized or not or
    whether a particular Twitter account is a bot or not is decided by a model within
    seconds, and this decision affects steps taken in the real world (such as the
    transaction or account being flagged as fraudulent). Attackers use the reduced
    human involvement to their advantage and aim to attack models deployed in the
    real world. **Adversarial ML** (**AML**) is a field of ML that focuses on detecting
    and exploiting flaws in ML models.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来在 **机器学习**（**ML**）和 **人工智能**（**AI**）方面的最新进展增加了我们对智能算法和系统的依赖。机器学习系统用于在多个关键应用中实时做出决策。例如，是否授权信用卡交易或是否某个特定的
    Twitter 账户是机器人，模型在几秒钟内就能做出决定，而这个决定会影响现实世界中的步骤（例如，交易或账户被标记为欺诈）。攻击者利用减少的人为干预来达到自己的目的，并试图攻击现实世界部署的模型。**对抗性机器学习**（**AML**）是机器学习的一个领域，专注于检测和利用机器学习模型中的缺陷。
- en: Adversarial attacks can come in several forms. Attackers may try to manipulate
    the features of a data point so that it is misclassified by the model. Another
    threat vector is data poisoning, where attackers introduce perturbations into
    the training data itself so that the model learns from incorrect data and thus
    performs poorly. An attacker may also attempt to run membership inference attacks
    to determine whether an individual was included in the training data or not. Protecting
    ML models from adversarial attacks and, therefore, understanding the nature and
    workings of such attacks is essential for data scientists in the cybersecurity
    space.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性攻击可以有多种形式。攻击者可能会尝试操纵数据点的特征，使其被模型错误分类。另一种威胁向量是数据中毒，攻击者会在训练数据本身中引入扰动，使模型从错误的数据中学习，从而表现不佳。攻击者还可能尝试运行成员推理攻击，以确定个人是否包含在训练数据中。保护机器学习模型免受对抗性攻击，因此理解此类攻击的性质和工作原理对于网络安全领域的数据科学家至关重要。
- en: 'In this chapter, we will cover the following main topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主要内容：
- en: Introduction to AML
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AML 简介
- en: Attacking image models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击图像模型
- en: Attacking text models
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 攻击文本模型
- en: Developing robustness against adversarial attacks
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发对抗性攻击的鲁棒性
- en: This chapter will help you understand how adversarial attacks can manifest themselves,
    which will then help you uncover gaps and vulnerabilities in your ML infrastructure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将帮助您了解对抗性攻击如何表现出来，这将有助于您发现您机器学习基础设施中的差距和漏洞。
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 GitHub 上找到本章的代码文件，链接为 [https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209](https://github.com/PacktPublishing/10-Machine-Learning-Blueprints-You-Should-Know-for-Cybersecurity/tree/main/Chapter%209)。
- en: Introduction to AML
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: AML 简介
- en: In this section, we will learn about what AML exactly is. We will begin by understanding
    the importance ML plays in today’s world, followed by the various kinds of adversarial
    attacks on models.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将了解 AML 究竟是什么。我们将首先了解机器学习在当今世界中的重要性，然后了解对模型的各种对抗性攻击。
- en: The importance of ML
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 机器学习的重要性
- en: 'In recent times, our reliance on ML has increased. Automated systems and models
    are in every sphere of our life. These systems often allow for fast decision-making
    without the need for manual human intervention. ML is a boon to security tasks;
    a model can learn from historical behavior, identify and recognize patterns, extract
    features, and render a decision much faster and more efficiently than a human
    can. Examples of some ML systems handling security-critical decisions are given
    here:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，我们对机器学习的依赖性增加。自动系统和模型存在于我们生活的各个领域。这些系统通常允许快速做出决策，而无需人工干预。机器学习是安全任务的福音；模型可以从历史行为中学习，识别和识别模式，提取特征，并比人类更快、更有效地做出决策。以下是一些处理安全关键决策的机器学习系统示例：
- en: Real-time fraud detection in credit card usage often uses ML. Whenever a transaction
    is made, the model looks at your location, the amount, the billing code, your
    past transactions, historical patterns, and other behavioral features. These are
    fed into an ML model, which will render a decision of *FRAUD* or *NOT FRAUD*.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在信用卡使用中的实时欺诈检测通常使用机器学习。每当进行一笔交易时，模型都会查看你的位置、金额、账单代码、过去的交易、历史模式以及其他行为特征。这些被输入到机器学习模型中，该模型将做出*欺诈*或*非欺诈*的决定。
- en: Malware detection systems use ML models to detect malicious applications. The
    model uses API calls made, permissions requested, domains connected, and so on
    to classify the application as *MALWARE* or *BENIGN*.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件检测系统使用机器学习模型来检测恶意应用程序。模型使用API调用、请求的权限、连接的域名等来将应用程序分类为*恶意软件*或*良性*。
- en: Social media platforms use ML to identify hate speech or toxic content. Models
    can extract text and image content, topics, keywords, and URLs to determine whether
    a post is *TOXIC* or *NON-TOXIC*.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社交媒体平台使用机器学习（ML）来识别仇恨言论或有害内容。模型可以提取文本和图像内容、主题、关键词和URL，以确定帖子是*有害*还是*无害*。
- en: What is the goal behind listing these applications? In each case, you can see
    that ML plays a prominent role in detecting or identifying an adversary or attacker.
    The attacker, therefore, has an incentive to degrade the performance of the model.
    This leads us to the branch of AML and adversarial attacks.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列出这些应用背后的目标是什么？在每种情况下，你都可以看到机器学习在检测或识别对手或攻击者方面发挥着突出的作用。因此，攻击者有动机降低模型的性能。这使我们转向反洗钱（AML）和对抗性攻击的分支。
- en: Adversarial attacks
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗性攻击
- en: AML is a subfield of AI and ML concerned with the design and analysis of algorithms
    that can robustly and securely operate in adversarial environments. In these scenarios,
    an adversary with malicious intent can manipulate input data to disrupt the behavior
    of ML models, either by causing incorrect predictions or by compromising the confidentiality
    and privacy of the data.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 反洗钱（AML）是人工智能（AI）和机器学习（ML）的一个子领域，涉及设计和分析能够在对抗性环境中稳健和安全运行的算法。在这些情况下，具有恶意意图的对手可以操纵输入数据来干扰机器学习模型的行为，无论是通过造成错误预测还是通过损害数据的机密性和隐私。
- en: AML attacks are intentionally crafted inputs to ML models that cause them to
    behave in unintended and potentially harmful ways. They can be used for malicious
    purposes, such as compromising the security and privacy of ML models, disrupting
    their normal operation, or undermining their accuracy and reliability. In these
    scenarios, attackers may use adversarial examples to trick ML models into making
    incorrect predictions, compromising the confidentiality and privacy of sensitive
    data, or causing harm to the system or the people who use it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: AML攻击是有意设计的输入，旨在使机器学习模型以非预期和可能有害的方式行为。它们可以用于恶意目的，例如损害机器学习模型的安全性和隐私，干扰其正常操作，或损害其准确性和可靠性。在这些情况下，攻击者可能使用对抗性示例来欺骗机器学习模型做出错误预测，损害敏感数据的机密性和隐私，或对系统或使用它的人造成伤害。
- en: 'For example, we listed the applications of ML models in some critical tasks
    earlier. Here is how an attacker could manipulate them to their benefit:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们之前列出了一些关键任务中机器学习模型的应用。以下是攻击者如何操纵它们以获得利益的方法：
- en: In a fraud detection system, a smart attacker may try to abuse the credit card
    with multiple small purchases instead of a large one. The model may be fooled
    by the purchase amounts and will not flag them as abnormal. Or, the attacker may
    use a **virtual private network** (**VPN**) connection to appear closer to the
    victim and purchase gift cards online, thus evading the model’s location-based
    features.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在欺诈检测系统中，一个聪明的攻击者可能会尝试通过多次小额购买而不是一次大额购买来滥用信用卡。模型可能会被购买金额所欺骗，不会将其标记为异常。或者，攻击者可能会使用**虚拟私人网络**（VPN）连接，以看起来更接近受害者，并在网上购买礼品卡，从而规避模型基于位置的功能。
- en: A malware developer may know which features indicate malware presence. Therefore,
    they may try to mask that behavior by requesting some normal permissions or making
    redundant API calls so as to throw the classifier off in the predictions.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 恶意软件开发者可能知道哪些特征表明恶意软件的存在。因此，他们可能会通过请求一些正常权限或进行冗余API调用来掩盖这种行为，从而在预测中误导分类器。
- en: A user who wants to post toxic content or hate speech knows which words indicate
    abusive content. They will try to misspell those words so that they are not flagged
    by the model.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 想要发布有害内容或仇恨言论的用户知道哪些词表示侮辱性内容。他们会尝试拼写错误这些词，以便模型不会将其标记出来。
- en: Using adversarial attacks, an attacker can potentially fool a system and escape
    undetected. It is, therefore, important for researchers and practitioners in the
    field of ML to understand adversarial attacks and to develop methods for detecting
    and defending against them. This requires a deep understanding of the underlying
    mechanisms of these attacks and the development of new algorithms and techniques
    to prevent them.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对抗性攻击，攻击者可能能够欺骗系统并逃脱检测。因此，对于机器学习领域的学者和实践者来说，了解对抗性攻击并开发检测和防御它们的方法非常重要。这需要对这些攻击的潜在机制有深刻的理解，并开发新的算法和技术来防止它们。
- en: Adversarial tactics
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗策略
- en: 'The end goal of an adversarial attack is to degrade the performance of an ML
    model. Adversarial attacks generally employ one of three strategies: input perturbation,
    data poisoning, or model inversion attacks. We will cover these in detail next.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击的最终目标是降低机器学习模型的性能。对抗攻击通常采用以下三种策略之一：输入扰动、数据中毒或模型反转攻击。我们将在下一节中详细讨论这些内容。
- en: Input perturbation attacks
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 输入扰动攻击
- en: In input perturbation attacks, the attacker maliciously crafts input examples
    so that they will be misclassified by the model. The attacker makes slight changes
    to the input that are neither discernible to the naked eye nor large enough to
    be detected as anomalous or noisy. Typically, this can include changing a few
    pixels in an image or altering some characters in a word. **Deep learning** (**DL**)
    systems, favored because of their power, are very susceptible to input perturbation
    attacks. Because of non-linear functions and transforms, a small change in the
    input can cause a significant unexpected change in the output.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入扰动攻击中，攻击者恶意地构建输入示例，以便它们会被模型错误分类。攻击者对输入进行轻微的修改，这些修改既不会被肉眼察觉，也不足以被检测为异常或噪声。通常，这可以包括在图像中更改几个像素或在单词中更改一些字符。由于它们的强大功能，**深度学习**（**DL**）系统很容易受到输入扰动攻击。由于非线性函数和变换，输入的微小变化可能导致输出发生显著且意外的变化。
- en: 'For example, consider the following screenshot from a 2017 study showing two
    images of a **STOP** sign:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑以下2017年研究中的屏幕截图，展示了两个STOP标志的图像：
- en: '![Figure 9.1 – An actual image of a STOP sign (left) and the adversarially
    manipulated image (right)](img/B19327_09_01.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图9.1 – 一个实际的STOP标志图像（左）和被对抗性操纵的图像（右）](img/B19327_09_01.jpg)'
- en: Figure 9.1 – An actual image of a STOP sign (left) and the adversarially manipulated
    image (right)
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 – 一个实际的STOP标志图像（左）和被对抗性操纵的图像（右）
- en: The one on the left is the actual one from the street, and the one on the right
    is an identical one with some pieces of tape. These pieces of tape represent an
    input perturbation on the original. The researchers found that the image on the
    left was correctly detected as a STOP sign, but the model was fooled by the right
    one and detected it as a 45 MPH speed limit sign.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的是街道上的实际标志，右边的是带有一些胶带的相同标志。这些胶带代表对原始图像的输入扰动。研究人员发现，左边的图像被正确地检测为STOP标志，但模型被右边的图像欺骗，将其检测为45英里/小时的限速标志。
- en: Data poisoning attacks
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据中毒攻击
- en: Data poisoning attacks are malicious attacks in which an adversary manipulates
    or corrupts training data in order to degrade the performance of an ML model or
    cause it to behave in unexpected ways. The goal of these attacks is to cause the
    model to make incorrect predictions or decisions, leading to security vulnerabilities
    or privacy breaches. If the quality of data (in terms of the correctness of labels
    presented to the model) is bad, naturally the resulting model will also be bad.
    Due to incorrect labels, the model will learn correlations and features incorrectly.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据中毒攻击是一种恶意攻击，攻击者通过操纵或破坏训练数据来降低机器学习模型的性能或使其以意外的方式行为。这些攻击的目标是使模型做出错误的预测或决策，从而导致安全漏洞或隐私泄露。如果数据的质量（就模型呈现给模型的标签的正确性而言）很差，那么自然产生的模型也会很差。由于标签错误，模型将错误地学习相关性和特征。
- en: For example, in a **supervised ML** (**SML**) scenario, an adversary may manipulate
    the labeled data used for training in order to cause a classifier to misclassify
    certain instances, leading to security vulnerabilities. In another scenario, an
    adversary may add malicious instances to the training data in order to cause the
    model to overfit, leading to a decrease in performance on unseen data. For example,
    if an attacker adds several requests from a malicious domain and marks them as
    safe or benign in the training data, the model may learn that this domain indicates
    safe behavior and, therefore, will not mark other requests from that domain as
    malicious.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在**监督机器学习**（**SML**）场景中，攻击者可能会操纵用于训练的标记数据，以导致分类器错误分类某些实例，从而导致安全漏洞。在另一种场景中，攻击者可能会向训练数据中添加恶意实例，以导致模型过拟合，从而降低在未见数据上的性能。例如，如果攻击者向训练数据中添加来自恶意域的几个请求，并将它们标记为安全或良性的，那么模型可能会学习到这个域表示安全行为，因此不会将来自该域的其他请求标记为恶意。
- en: These attacks can be particularly dangerous because ML models are becoming increasingly
    widely used in a variety of domains, including security and privacy-sensitive
    applications.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些攻击尤其危险，因为机器学习模型正在越来越多地被用于各种领域，包括安全和隐私敏感的应用。
- en: Model inversion attacks
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 模型反演攻击
- en: Model inversion attacks are a type of privacy attack in which an adversary tries
    to reverse-engineer an ML model to obtain sensitive information about the training
    data or the individuals represented by the data. The goal of these attacks is
    to reveal information about the training data that would otherwise be protected.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型反演攻击是一种隐私攻击，攻击者试图逆向工程机器学习模型以获取关于训练数据或数据所代表个体的敏感信息。这些攻击的目标是揭示关于训练数据的信息，这些信息在其他情况下本应受到保护。
- en: For example, in a fraud detection scenario, a financial institution might use
    an ML model to identify instances of fraud in financial transactions. An adversary
    might attempt to reverse-engineer the model in order to obtain information about
    the characteristics of fraudulent transactions, such as the types of goods or
    services that are commonly purchased in fraud cases. The attacker may discover
    the important features that are used to discern fraud and, therefore, know what
    to manipulate. This information could then be used to commit more sophisticated
    fraud in the future.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在欺诈检测场景中，金融机构可能会使用机器学习模型来识别金融交易中的欺诈实例。攻击者可能会尝试逆向工程该模型，以获取有关欺诈交易特征的信息，例如在欺诈案例中通常购买的货物或服务类型。攻击者可能会发现用于识别欺诈的重要特征，因此知道要操纵什么。然后，可以使用这些信息在未来进行更复杂的欺诈活动。
- en: To carry out a model inversion attack, an adversary might start by submitting
    queries to the ML model with various inputs and observing the model’s predictions.
    Over time, the adversary could use this information to build an approximation
    of the model’s internal representation of the data. In some cases, the adversary
    might be able to obtain information about the training data itself, such as the
    values of sensitive features—for example, the age or address of individuals represented
    by the data.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行模型反演攻击，攻击者可能首先通过向机器学习模型提交各种输入并观察模型的预测来开始。随着时间的推移，攻击者可以使用这些信息构建模型内部数据表示的近似。在某些情况下，攻击者可能能够获取有关训练数据本身的信息，例如敏感特征的值——例如，数据所代表个人的年龄或地址。
- en: This concludes our discussion of various kinds of adversarial attacks. In the
    next section, we will turn to implementing a few adversarial attacks on image-based
    models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对各种对抗性攻击的讨论。在下一节中，我们将转向实现针对基于图像的模型的几种对抗性攻击。
- en: Attacking image models
  id: totrans-45
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 攻击图像模型
- en: 'In this section, we will look at two popular attacks on image classification
    systems: **Fast Gradient Sign Method** (**FGSM**) and the **Projected Gradient
    Descent** (**PGD**) method. We will first look at the theoretical concepts underlying
    each attack, followed by actual implementation in Python.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨两种流行的图像分类系统攻击：**快速梯度符号法**（**FGSM**）和**投影梯度下降法**（**PGD**）。我们将首先查看每个攻击背后的理论概念，然后是Python中的实际实现。
- en: FGSM
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FGSM
- en: FGSM is one of the earliest methods used for crafting adversarial examples for
    image classification models. Proposed by Goodfellow in 2014, it is a simple and
    powerful attack against **neural network** (**NN**)-based image classifiers.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: FGSM是用于为图像分类模型构建对抗示例的最早方法之一。由Goodfellow于2014年提出，它是对基于**神经网络**（**NN**）的图像分类器的一种简单而强大的攻击。
- en: FGSM working
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FGSM工作
- en: 'Recall that NNs are layers of neurons placed one after the other, and there
    are connections from neurons in one layer to the next. Each connection has an
    associated weight, and the weights represent the model parameters. The final layer
    produces an output that can be compared with the available ground truth to calculate
    the loss, which is a measure of how far off the prediction is from the actual
    ground truth. The loss is *backpropagated*, and the model *learns* by adjusting
    the parameters based on the gradient of the loss. This process is known as *gradient
    descent*. If θ is the parameter and L is the loss, the adjusted parameter θ ′ 
    is calculated as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，NN是由一个接一个放置的神经元层组成的，并且有一个层中的神经元到下一层的连接。每个连接都有一个相关的权重，权重代表模型参数。最后一层产生一个输出，可以与可用的真实值进行比较，以计算损失，损失是预测与实际真实值偏离程度的度量。损失是
    *反向传播* 的，模型通过根据损失的梯度调整参数来 *学习*。这个过程被称为 *梯度下降*。如果θ是参数，L是损失，则调整后的参数θ ′ 计算如下：
- en: θ′= θ − η  δL _ δθ
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: θ′= θ − η  δL _ δθ
- en: Here, the derivative term δL _ δθ  is known as the *gradient*.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，导数项 δL _ δθ 被称为 *梯度*。
- en: The FGSM adversarial attack leverages the gradients to craft adversarial examples.
    While the learning algorithm *minimizes* the loss by adjusting the weights, the
    FGSM attack works to adjust the input data so as to *maximize* the loss. During
    backpropagation, a small perturbation is added to the image based on the sign
    of the gradient.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: FGSM对抗攻击利用梯度来构建对抗示例。当学习算法通过调整权重来 *最小化* 损失时，FGSM攻击旨在调整输入数据以 *最大化* 损失。在反向传播过程中，根据梯度的符号向图像添加了小的扰动。
- en: 'Formally speaking, given an image X, a new (adversarial) image X ′  can be
    calculated as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，给定一个图像X，可以计算一个新的（对抗）图像X ′ 如下：
- en: X ′  = X+ ϵ ⋅ sign ( ∇ x ℒ(θ, x, y))
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: X ′  = X+ ϵ ⋅ sign ( ∇ x ℒ(θ, x, y))
- en: Here, ℒ represents the loss, θ represents the model parameters, and y refers
    to the ground-truth label. The term ℒ(θ, x, y) calculates the loss based on the
    model prediction and ground truth, and ∇ x calculates the gradient. The term ϵ
    is the perturbation added, which is either positive or negative depending on the
    sign of the gradient.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，ℒ代表损失，θ代表模型参数，y指的是真实标签。术语ℒ(θ, x, y)根据模型预测和真实值计算损失，∇ x计算梯度。术语ϵ是添加的扰动，它可以是正的或负的，取决于梯度的符号。
- en: 'A popular example that demonstrates the effectiveness of the FGSM attack is
    shown here:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 下面展示了展示FGSM攻击有效性的一个流行示例：
- en: '![Figure 9.2 – Adding noise to an image with FGSM](img/B19327_09_02.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图9.2 – 使用FGSM向图像添加噪声](img/B19327_09_02.jpg)'
- en: Figure 9.2 – Adding noise to an image with FGSM
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 – 使用FGSM向图像添加噪声
- en: The original image is predicted to be a panda with a confidence of 57.7%, which
    indicates that the model made the correct prediction. Adversarial noise is added
    to the image depending on the sign of the gradient per pixel (so, either 0.007,
    0, or -0.007). The resulting image is identical to the original one—no difference
    can be seen to the naked eye. This is expected because the human eye is not sensitive
    to such small differences at the pixel level. However, the model now predicts
    the image to be a gibbon with a 99.3% confidence.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原始图像被预测为一只熊猫，置信度为57.7%，这表明模型做出了正确的预测。根据梯度每像素的符号（因此，要么是0.007，要么是0，要么是-0.007），向图像中添加了对抗噪声。结果图像与原始图像完全相同——肉眼无法看出任何区别。这是预期的，因为人眼对像素级别的这种微小差异不敏感。然而，模型现在预测图像为长臂猿，置信度为99.3%。
- en: FGSM implementation
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: FGSM实现
- en: Let us now implement the FGSM attack method using the PyTorch library.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用PyTorch库实现FGSM攻击方法。
- en: 'We begin, as usual, by importing the required libraries:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们像往常一样，首先导入所需的库：
- en: '[PRE0]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will define a function that performs the FGSM attack and generates
    an adversarial example. This function calculates the gradient, followed by the
    perturbation, and generates an adversarial image. The `epsilon` parameter is passed
    to it, which indicates the degree of perturbation to be added. In short, the function
    works as follows:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将定义一个函数来执行FGSM攻击并生成对抗性示例。此函数计算梯度，然后进行扰动，并生成对抗性图片。`epsilon`参数传递给它，表示要添加的扰动程度。简而言之，该函数的工作方式如下：
- en: Takes in as input an image or an array of images.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入为一张图片或一组图片数组。
- en: Calculates the predicted label by running it through the model.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过运行模型来计算预测标签。
- en: Calculates the loss by comparing the predicted label with the actual ground
    truth.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过比较预测标签与实际真实标签来计算损失。
- en: Backpropagates the loss and calculates the gradient.
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播损失并计算梯度。
- en: Calculates the perturbation by multiplying `epsilon` with the sign of the gradient.
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`epsilon`与梯度的符号相乘来计算扰动。
- en: Adds this to the image to obtain the adversarial image.
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此添加到图片中以获得对抗性图片。
- en: 'The following code snippet shows how to perform the FGSM attack:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何执行FGSM攻击：
- en: '[PRE1]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we need a basic image classifier to use as the model to attack. As the
    data at hand is images, we will use a **convolutional neural network** (**CNN**).
    For this, we define a class that has two functions, as follows:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要一个基本的图像分类器作为攻击模型。由于手头的数据是图片，我们将使用**卷积神经网络**（**CNN**）。为此，我们定义了一个具有两个函数的类，如下所示：
- en: '**The constructor**: This function defines the basic structure of the NN to
    be used for the classification. We define the convolutional and fully connected
    layers that we need. The number of neurons and the number of layers here are all
    design choices.'
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**构造函数**：此函数定义了用于分类的神经网络的基本结构。我们定义了所需的卷积层和全连接层。这里的神经元数量和层数都是设计选择。'
- en: '**The forward function**: This function defines what happens during the forward
    pass of the NN. We take in the input data and pass it through the first convolutional
    layer. The output of this layer is processed through a ReLU activation function
    and then passed to the next layer. This continues for all convolutional layers
    we have. Finally, we flatten the output of the last convolutional layer and pass
    it through the fully connected layer.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**前向函数**：此函数定义了神经网络前向传播过程中发生的事情。我们接收输入数据并将其通过第一个卷积层。该层的输出通过ReLU激活函数处理，然后传递到下一层。这个过程会持续到所有的卷积层。最后，我们将最后一个卷积层的输出展平，并通过全连接层传递。'
- en: 'The process is illustrated in the following code snippet:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了这个过程：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'We will now write a function that loads the required datasets. For our experiment,
    we will be using the *CIFAR-10* dataset. Developed by the **Canadian Institute
    for Advanced Research** (**CIFAR**), the dataset consists of 60,000 images from
    10 different classes (airplane, automobile, bird, cat, deer, dog, frog, horse,
    ship, and truck). Each image is color and of size 32 x 32 pixels. The dataset
    has been divided into 50,000 training and 10,000 test images. As a standardized
    dataset in the world of ML, it is well integrated with Python and PyTorch. The
    following function provides the code to load the train and test sets. If the data
    is not locally available, it will first download it and then load it:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将编写一个函数来加载所需的数据库集。对于我们的实验，我们将使用*CIFAR-10*数据集。由**加拿大高级研究研究所**（**CIFAR**）开发，该数据集包含来自10个不同类别（飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车）的60,000张图片。每张图片都是彩色的，尺寸为32
    x 32像素。该数据集已被分为50,000张训练图片和10,000张测试图片。作为机器学习领域的标准化数据集，它与Python和PyTorch很好地集成。以下函数提供了加载训练集和测试集的代码。如果数据不在本地，它将首先下载，然后加载：
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: PyTorch provides a standard functionality known as data loaders that facilitates
    easy data manipulation for ML. Data loaders can generate the data needed by applying
    specific transformations, and the generated data can be consumed by ML models.
    Note that when defining the data loader, we have specified a `batch_size` parameter.
    This defines the number of data instances that will be read at a time. In our
    case, it is set to `128`, which means that the forward pass, loss calculation,
    backpropagation, and gradient descent will happen one by one for batches where
    each batch is of 128 images.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: PyTorch提供了一个称为数据加载器的标准功能，它简化了ML中的数据操作。数据加载器可以通过应用特定的转换来生成所需的数据，并且生成数据可以被ML模型消费。请注意，在定义数据加载器时，我们指定了一个`batch_size`参数。这定义了每次读取的数据实例数量。在我们的情况下，它设置为`128`，这意味着前向传递、损失计算、反向传播和梯度下降将逐个为每个包含128个图像的批次发生。
- en: 'Next, we will train a vanilla NN model for image classification on the CIFAR
    dataset. We first perform some boilerplate setup that includes the following:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将在CIFAR数据集上训练一个用于图像分类的普通NN模型。我们首先执行一些标准设置，包括以下内容：
- en: Setting the number of epochs to be used for training.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置用于训练的epoch数量。
- en: Loading the train and test data loaders.
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载训练和测试数据加载器。
- en: Initializing the model with the basic CNN model we defined earlier.
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们之前定义的基本CNN模型初始化模型。
- en: Defining the loss function to be cross-entropy and the optimizer to be Adam.
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义损失函数为交叉熵，优化器为Adam。
- en: Moving the model to CUDA if it is available.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果可用，将模型移动到CUDA。
- en: 'Then, we begin the training loop. In each iteration of the training loop, we
    do the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们开始训练循环。在训练循环的每一次迭代中，我们执行以下操作：
- en: Load a batch of training data.
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载一批训练数据。
- en: Move it to the GPU (CUDA) if needed and available.
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果需要且可用，将其移动到GPU（CUDA）。
- en: Zero out the optimizer gradients.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将优化器的梯度置零。
- en: Calculate the predicted output by running inference on the model.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在模型上运行推理来计算预测输出。
- en: Calculate the loss based on prediction and ground truth.
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于预测和真实值计算损失。
- en: Backpropagate the loss.
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 反向传播损失。
- en: 'Here is a code snippet that executes these steps one by one:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个执行这些步骤的代码片段：
- en: '[PRE4]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Finally, we evaluate our model. While evaluating, we evaluate the model on two
    sets of data—the original test data and the adversarial test data that we created
    using FGSM.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们评估我们的模型。在评估过程中，我们在两组数据上评估模型——原始测试数据和用FGSM创建的对抗测试数据。
- en: 'We first set the model to evaluation mode, which means that gradients are not
    computed and stored. This makes the operations more efficient, as we do not need
    the overhead of gradients during inference on the model. For every batch of the
    training data, we calculate the adversarial images using FGSM. Here, we have set
    the value of `epsilon` to 0.005\. Then, we run inference on the model using both
    the clean images (the original test set) and adversarial images (generated through
    FGSM). For every batch, we will calculate the number of examples for which the
    predicted and ground-truth labels match, which will give us the accuracy of the
    model. Comparing the accuracy of the clean and adversarial set shows us how effective
    our adversarial attack is:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将模型设置为评估模式，这意味着不会计算和存储梯度。这使得操作更高效，因为在模型推理过程中我们不需要梯度的开销。对于训练数据的每一个批次，我们使用FGSM计算对抗图像。在这里，我们将`epsilon`的值设置为0.005。然后，我们使用干净图像（原始测试集）和对抗图像（通过FGSM生成）对模型进行推理。对于每一个批次，我们将计算预测标签和真实标签匹配的示例数量，这将给出模型的准确率。比较干净和对抗集的准确率可以显示我们的对抗攻击有多有效：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This concludes our discussion of the FGSM attack. You can compare the accuracy
    before and after the adversarial perturbation (`clean_accuracy` and `fgsm_accuracy`,
    respectively). The drop in accuracy indicates the effectiveness of the adversarial
    attack.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这结束了我们对FGSM攻击的讨论。您可以比较对抗扰动前后的准确率（分别为`clean_accuracy`和`fgsm_accuracy`）。准确率的下降表明了对抗攻击的有效性。
- en: PGD
  id: totrans-101
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: PGD
- en: In the previous section, we discussed the FGSM attack method and how it can
    be used to generate adversarial images by adding small perturbations to the input
    image based on the sign of the gradient. The **PGD** method extends FGSM by applying
    it iteratively.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们讨论了FGSM攻击方法以及它是如何通过在输入图像上添加基于梯度的符号的小扰动来生成对抗图像的。**PGD**方法通过迭代应用FGSM来扩展它。
- en: PGD working
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PGD工作
- en: Specifically, the PGD attack will, for an input image, calculate a perturbation
    based on the FGSM attack. Adding this to the image will give us the perturbed
    image. Whereas the FGSM attack stops here, the PGD attack goes a step further.
    Once an adversarial image has been generated, we clip the image. Clipping refers
    to adjusting the image so that it remains in the neighborhood of the original
    image. Clipping is done on a per-pixel basis. After the image has been clipped,
    we repeat the process multiple times iteratively to obtain the final adversarial
    image.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，PGD攻击将针对输入图像，基于FGSM攻击计算一个扰动。将此添加到图像中将给我们一个扰动的图像。而FGSM攻击在这里停止，PGD攻击则更进一步。一旦生成了对抗图像，我们就对图像进行裁剪。裁剪是指调整图像，使其保持在原始图像的邻域内。裁剪是按像素进行的。图像裁剪后，我们重复这个过程多次迭代，以获得最终的对抗图像。
- en: 'Formally speaking, given an image X, a series of adversarial images can be
    calculated as follows:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 从形式上讲，给定一个图像X，一系列对抗图像可以按以下方式计算：
- en: X N+1 ′  = Clip X,ϵ( X N ′ + α ⋅ sign (∇ x ℒ(θ, x, y)))
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: X^(N+1)′ = Clip(X^(N)′ + α ⋅ sign(∇x ℒ(θ, x, y)))
- en: The notation here is slightly different from that for the FGSM attack. Here,
    α serves the same role as ϵ did in FGSM; it controls the amount of perturbation.
    Typically, it is set to 1, which means that each pixel is modified by at most
    one unit in each step. The process is repeated iteratively for a predetermined
    number of steps.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的符号与FGSM攻击的符号略有不同。在这里，α扮演着与FGSM中ϵ相同的角色；它控制扰动的量。通常，它被设置为1，这意味着每个像素在每一步最多修改一个单位。这个过程以预定的步数重复迭代。
- en: The function that implements this is quite straightforward. It simply uses FGSM
    iteratively and clips the generated images. The FGSM function must be modified
    to take in the predicted ground-truth label by the model, as it will change in
    every step and should not be recalculated by FGSM. So, we pass it the ground truth
    as a parameter and use that instead of recalculating it as a model prediction.
    In the FGSM function, we simply use the value that is passed in instead of running
    inference on the model.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实现此功能的函数相当直接。它简单地迭代地使用FGSM并对生成的图像进行裁剪。FGSM函数必须修改为接收模型预测的地面真实标签，因为每一步它都会改变，不应由FGSM重新计算。因此，我们将其作为参数传递，并使用它而不是重新计算它作为模型预测。在FGSM函数中，我们简单地使用传入的值，而不是在模型上运行推理。
- en: 'The modified FGSM function is shown as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 修改后的FGSM函数如下所示：
- en: '[PRE6]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'For every image, the PGD method attack function completes the following steps:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个图像，PGD方法攻击函数完成以下步骤：
- en: Calculates the predicted label by running inference on the model.
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过在模型上运行推理来计算预测标签。
- en: Sets the original image as the initial adversarial image *X*0.
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将原始图像设置为初始对抗图像X*0。
- en: Calculates the adversarial image using the FGSM attack method described in the
    previous section. In doing so, it passes the predicted value as a parameter so
    that FGSM does not recompute it in every cycle.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一节中描述的FGSM攻击方法计算对抗图像。在这个过程中，它将预测值作为参数传递，这样FGSM就不会在每一步重新计算它。
- en: Computes the difference between the image and the adversarially generated image.
    This is the perturbation to be added.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算图像与对抗生成图像之间的差异。这就是要添加的扰动。
- en: Clips this perturbation so that the adversarial image is within the neighborhood
    of the original image.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将此扰动裁剪，使对抗图像位于原始图像的邻域内。
- en: Adds the clipped perturbation to the image to obtain the adversarial image.
  id: totrans-117
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将裁剪后的扰动添加到图像中，以获得对抗图像。
- en: Repeats *steps 2*-*6* for the desired number of iterations to obtain the final
    adversarial image.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2-6所需的迭代次数，以获得最终的对抗图像。
- en: As you can see, the overarching idea remains the same as with FGSM, but only
    the process of generating the adversarial images changes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，总体思想与FGSM相同，但生成对抗图像的过程发生了变化。
- en: PGD implementation
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: PGD实现
- en: 'A code snippet for the PGD method is shown as follows:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个PGD方法的代码片段：
- en: '[PRE7]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This function can be used to generate adversarial images given an image using
    the PGD method. We will not repeat the experiment of model setup, training, and
    evaluation. Simply using the `Generate_PGDM_Image()` function instead of the `Generate_FGSM_Image()`
    function should allow you to run our analysis using this attack. How does the
    performance of this attack compare to the FGSM attack?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数可以使用PGD方法生成给定图像的对抗性图像。我们不会重复模型设置、训练和评估的实验。只需使用`Generate_PGDM_Image()`函数代替`Generate_FGSM_Image()`函数，就可以使用这种攻击来运行我们的分析。这种攻击的性能与FGSM攻击相比如何？
- en: This concludes our discussion of attacking image models. In the next section,
    we will discuss attacking text models.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对攻击图像模型的讨论。在下文中，我们将讨论攻击文本模型。
- en: Attacking text models
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 攻击文本模型
- en: '*Please note that this section contains examples of hate speech and racist*
    *content online.*'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '*请注意，本节包含在线仇恨言论和种族主义内容的例子*。'
- en: Just as with images, text models are also susceptible to adversarial attacks.
    Attackers can modify the text so as to trigger a misclassification by ML models.
    Doing so can allow an adversary to escape detection.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 就像图像一样，文本模型也容易受到对抗性攻击。攻击者可以修改文本，以触发机器模型的错误分类。这样做可以使对手逃避检测。
- en: A good example of this can be seen on social media platforms. Most platforms
    have rules against abusive language and hate speech. Automated systems such as
    keyword-based filters and ML models are used to detect such content, flag it,
    and remove it. If something outrageous is posted, the platform will block it at
    the source (that is, not allow it to be posted at all) or remove it in the span
    of a few minutes.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 在社交媒体平台上，我们可以看到这种攻击的一个很好的例子。大多数平台都有反对侮辱性语言和仇恨言论的规则。自动系统，如基于关键字的过滤器以及机器学习模型，被用来检测此类内容，将其标记并删除。如果发布了令人震惊的内容，平台将阻止其源头（也就是说，根本不允许发布）或在几分钟内将其删除。
- en: 'A malicious adversary can purposely manipulate the content in order to fool
    a model into thinking that the words are out of vocabulary or are not certain
    abusive words. For example, according to a study (*Poster | Proceedings of the
    2019 ACM SIGSAC Conference on Computer and Communications Security* ([https://dl.acm.org/doi/abs/10.1145/3319535.3363271](https://dl.acm.org/doi/abs/10.1145/3319535.3363271))),
    the attacker can manipulate their post as shown:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 一个恶意对手可以故意操纵内容，以欺骗模型认为这些词超出了词汇表，或者不是确定的侮辱性词汇。例如，根据一项研究（*海报 | 2019年ACM SIGSAC计算机和通信安全会议论文集*
    ([https://dl.acm.org/doi/abs/10.1145/3319535.3363271](https://dl.acm.org/doi/abs/10.1145/3319535.3363271)))，攻击者可以像下面这样操纵他们的帖子：
- en: '![Figure 9.3 – A hateful tweet and the adversarially manipulated version](img/B19327_09_03.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3 – 一个仇恨推文及其对抗性操纵版本](img/B19327_09_03.jpg)'
- en: Figure 9.3 – A hateful tweet and the adversarially manipulated version
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 – 一个仇恨推文及其对抗性操纵版本
- en: The original tweet that says “*go back to where you came from -- these fucking
    immigrants are destroying America!!!*” is clearly hate speech and racism against
    immigrants. Originally, this was classified to be 95% toxic, that is, a toxicity
    classifier assigned it the label *toxic* with 95% probability. Obviously, this
    classification is correct.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 原始推文声称“*滚回你们来的地方——这些该死的移民正在摧毁美国！！！*”，这显然是对移民的仇恨言论和种族主义。最初，这被归类为95%的有毒内容，也就是说，一个毒性分类器以95%的概率将其标记为*有毒*。显然，这种分类是正确的。
- en: In the obfuscated tweet, the attacker modified three words by eliminating one
    letter from three words. Note that we still very much recognize those words for
    what they are. The intent is clear, and this is still very much hate speech. An
    automated system, however, will not know this. Models and rules work by looking
    at words. To them, these new words are out of their vocabulary. They have not
    seen the words *imigrants*, *fuckng*, or *destroyin* in prior examples of hate
    speech during training. Therefore, the model misclassifies it and assigns it a
    label of not being toxic content with a probability of 63%. The attacker thus
    succeeded in fooling a classifier to pass off their toxic content as benign.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在被混淆的推文中，攻击者通过从三个词中删除一个字母来修改了三个词。请注意，我们仍然非常清楚地知道这些词是什么。意图是明显的，这仍然是非常明显的仇恨言论。然而，一个自动系统却不会知道这一点。模型和规则通过查看单词来工作。对他们来说，这些新词超出了它们的词汇表。在训练过程中，它们没有在仇恨言论的先例中看到过单词*imigrants*、*fuckng*或*destroyin*。因此，模型错误地将它归类，并赋予它63%的概率标签，表示这不是有毒内容。因此，攻击者成功地欺骗了分类器，将他们的有毒内容伪装成良性内容。
- en: 'The principle of adversarial attacks in text is the same as that of those in
    images: manipulating the input so as to confuse the model and not allowing it
    to recognize certain important features. However, two key differences set adversarial
    manipulation in text apart from adversarial manipulation in images.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中对抗攻击的原则与图像中的原则相同：操纵输入以混淆模型，并阻止它识别某些重要特征。然而，两个关键差异将文本中的对抗操纵与图像中的对抗操纵区分开来。
- en: First, the adversarially generated input should be reasonably similar to the
    original input. For example, we saw in the preceding section that the two images
    of the panda were nearly identical. This will not be the case with text—the changes
    made through manipulation will be visible and discernible to the naked eye. Looking
    at the screenshot of tweets that we just discussed, we can clearly see that the
    words are different. Manipulation in text is obvious whereas that in images is
    not. As a result, there is a limit to how much we can manipulate. We cannot change
    every word—too many changes will clearly indicate the manipulation and lead to
    discovery, thus defeating the goal.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，对抗生成的输入应与原始输入合理相似。例如，我们在上一节中看到，两只熊猫的图像几乎相同。这种情况在文本中不会发生——通过操纵所做的更改将用肉眼可见并可识别。查看我们刚才讨论的推文截图，我们可以清楚地看到单词是不同的。文本中的操纵是明显的，而图像中的操纵则不明显。因此，我们能够操纵的程度是有限的。我们不能更改每个单词——太多的更改将明显表明操纵并导致被发现，从而违背目标。
- en: Second, images are robust to change as compared to text. Changing multiple pixels
    in an image will still leave the larger image mainly unchanged (that is, a panda
    will still be recognizable, maybe with some distortion). On the other hand, text
    depends on words for the meaning it provides. Changing a few words will change
    the meaning entirely, or render the text senseless. This would be unacceptable—the
    goal of an adversarial attack is to still have the original meaning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，与文本相比，图像对变化具有鲁棒性。在图像中更改多个像素仍然会使整个图像主要保持不变（也就是说，熊猫仍然可以辨认，可能有些扭曲）。另一方面，文本依赖于单词来提供意义。更改几个单词将完全改变意义，或者使文本变得毫无意义。这是不可接受的——对抗攻击的目标是仍然保持原始意义。
- en: Manipulating text
  id: totrans-137
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 操纵文本
- en: In this section, we will explore techniques for manipulating text so that it
    may fool an ML classifier. We will show a few sample techniques and provide general
    guidelines at the end for further exploration.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨操纵文本以欺骗机器学习分类器的技术。我们将展示一些示例技术，并在最后提供一些一般性指南以供进一步探索。
- en: 'Recall the example of hate speech online that we discussed earlier: attackers
    can manipulate text so as to escape detection and post toxic content online. In
    this section, we will attempt to build such techniques and examine whether we
    can beat ML models.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们之前讨论的在线仇恨言论的例子：攻击者可以操纵文本以逃避检测并在网上发布有毒内容。在本节中，我们将尝试构建这样的技术并检查我们是否能够击败机器学习模型。
- en: Data
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 数据
- en: 'For this experiment, we will use the *Toxic Tweets* dataset (*Toxic Tweets
    Dataset | Kaggle*) ([https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset](https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset)).
    This data is made available freely as part of a Kaggle challenge online. You will
    have to download the data, and then unzip it to extract the CSV file. The data
    can then be read as follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个实验，我们将使用*Toxic Tweets*数据集（*Toxic Tweets Dataset | Kaggle*）([https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset](https://www.kaggle.com/datasets/ashwiniyer176/toxic-tweets-dataset))。这些数据作为Kaggle在线挑战的一部分免费提供。您需要下载这些数据，然后解压缩以提取CSV文件。数据可以按以下方式读取：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This should show you what the data looks like, as follows:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会向您展示数据的外观，如下所示：
- en: '![Figure 9.4 – Hate speech dataset](img/B19327_09_04.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4 – 仇恨言论数据集](img/B19327_09_04.jpg)'
- en: Figure 9.4 – Hate speech dataset
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 – 仇恨言论数据集
- en: 'You can also look at the distribution of the labels as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以按以下方式查看标签的分布：
- en: '[PRE9]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'This will show you the following output:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这将向您展示以下输出：
- en: '![Figure 9.5 – Distribution of tweets by toxicity label](img/B19327_09_05.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图9.5 – 按毒性标签分布的推文](img/B19327_09_05.jpg)'
- en: Figure 9.5 – Distribution of tweets by toxicity label
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 – 按毒性标签分布的推文
- en: The dataset contains approximately 24,000 tweets that are toxic and 32,500 that
    are not. In the next section, we will extract features from this data.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集包含大约24,000条有毒推文和32,500条非有毒推文。在下一节中，我们将从这些数据中提取特征。
- en: Extracting features
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 提取特征
- en: In our earlier chapters, we discussed that there needs to be a method to extract
    features from text, and such features must be numeric in value. One such method,
    which we have already used, is the **Term Frequency-Inverse Document Frequency**
    (**TF-IDF**) approach. Let us do a brief recap of TF-IDF.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们前面的章节中，我们讨论了需要有一种方法从文本中提取特征，并且这些特征必须是数值型的。我们已经使用过的一种方法就是**词频-逆文档频率**（**TF-IDF**）方法。让我们简要回顾一下TF-IDF。
- en: 'TF-IDF is a commonly used technique in **natural language processing** (**NLP**)
    to convert text into numeric features. Every word in the text is assigned a score
    that indicates how important the word is in that text. This is done by multiplying
    two metrics, as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF是**自然语言处理**（**NLP**）中常用的一种技术，用于将文本转换为数值特征。文本中的每个单词都被分配一个分数，表示该单词在该文本中的重要性。这是通过乘以两个度量来完成的：
- en: '**TF**: How frequently does the word appear in the text sample? This can be
    normalized by the length of the text in words, as texts that differ in length
    by a large number can cause skews. The TF metric measures how common a word is
    in this particular text.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**TF**：单词在文本样本中出现的频率是多少？这可以通过文本的单词长度进行归一化，因为长度差异很大的文本可能会造成偏差。TF度量衡量一个单词在这个特定文本中的普遍程度。'
- en: '**IDF**: How frequently does the word appear in the rest of the corpus? First,
    the number of text samples containing this word is obtained. The total number
    of samples is divided by this number. Simply put, IDF is the inverse of the fraction
    of text samples containing the word. The IDF metric measures how common the word
    is in the rest of the corpus.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**IDF**：单词在其余语料库中出现的频率是多少？首先，获得包含该单词的文本样本数量。然后将总样本数除以这个数。简单来说，IDF是包含该单词的文本样本分数的倒数。IDF度量衡量单词在其余语料库中的普遍程度。'
- en: 'More details on TF-IDF can be found in [*Chapter 7*](B19327_07.xhtml#_idTextAnchor019),
    *Attributing Authorship and How to Evade It*. For now, here is a code snippet
    to extract the TF-IDF features for a list of sentences:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于TF-IDF的细节可以在[*第7章*](B19327_07.xhtml#_idTextAnchor019)，*归因作者身份和如何规避它*中找到。现在，这里有一个代码片段，用于提取一系列句子的TF-IDF特征：
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Adversarial attack strategies
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对抗攻击策略
- en: 'We will attempt to evade our ML models using two adversarial strategies: appending
    a letter at the end of some words, and repeating certain vowels from some words.
    In each case, our end goal is to fool the classifier into thinking that there
    are words that it has not seen and, therefore, it does not recognize those words.
    Let us discuss these strategies one by one.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尝试使用两种对抗策略来规避我们的机器学习模型：在单词末尾添加一个字母，以及重复某些单词中的某些元音。在每种情况下，我们的最终目标都是欺骗分类器，让它认为有一些它没有见过的单词，因此它不识别这些单词。让我们逐一讨论这些策略。
- en: Doubling the last letter
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 将最后一个字母加倍
- en: In this strategy, we simply misspell the word by appending an additional letter
    at the end. We double the last letter so that the word appears to be unchanged,
    and is still recognizable. For example, *America* will become *Americaa*, and
    *immigrant* will become *immigrantt*. To a machine, these words are totally different
    from one another.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个策略中，我们只是通过在单词末尾添加一个额外的字母来拼写错误。我们将最后一个字母加倍，这样单词看起来没有变化，仍然可以识别。例如，*America*
    将变成 *Americaa*，*immigrant* 将变成 *immigrantt*。对于机器来说，这些单词彼此完全不同。
- en: 'Here is the code to implement this:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是实现这个功能的代码：
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Doubling vowels
  id: totrans-165
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 元音加倍
- en: 'In this attack, we will look for words with vowels, and on finding the first
    vowel, we will repeat it. For example, *Facebook* will become *Faacebook*, and
    *Coronavirus* will become *Cooronavirus*. It is fairly intuitive that repeated
    vowels often go unnoticed when reading text; this means that the text will appear
    to be unchanged to a quick reader. The following code snippet implements this
    attack:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种攻击中，我们将寻找含有元音的单词，并在找到第一个元音时重复它。例如，*Facebook* 将变成 *Faacebook*，*Coronavirus*
    将变成 *Cooronavirus*。从直觉上看，重复的元音在阅读文本时往往会被忽略；这意味着对于快速阅读者来说，文本看起来没有变化。以下代码片段实现了这种攻击：
- en: '[PRE12]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Executing the attacks
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 执行攻击
- en: 'Now that we have defined the two attacks we will implement, it is time to actually
    execute them. To achieve this, we will do the following:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了将要实现的两种攻击，现在是时候实际执行它们了。为了实现这一点，我们将做以下事情：
- en: Split the data into training and test sets.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集。
- en: Build a TF-IDF model on the training data and use it to extract features from
    the training set.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练数据上构建TF-IDF模型，并使用它从训练集中提取特征。
- en: Train a model based on the features extracted in *step 2*.
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基于步骤2中提取的特征训练一个模型。
- en: Use the same TF-IDF model to extract features on the test set and run inference
    on the trained model.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的 TF-IDF 模型在测试集上提取特征，并在训练好的模型上运行推理。
- en: Calculate metrics for classification—accuracy, precision, recall, and F-1 score.
    These are the baseline scores.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分类度量——准确率、精确率、召回率和 F-1 分数。这些是基线分数。
- en: Now, apply the attack functions and derive the adversarial test set.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，应用攻击函数并推导出对抗测试集。
- en: Use the same TF-IDF model to extract features from the adversarial test set
    and run inference on the trained model.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用相同的 TF-IDF 模型从对抗测试集中提取特征，并在训练好的模型上运行推理。
- en: Calculate metrics for classification—accuracy, precision, recall, and F-1 score.
    These are the scores upon adversarial attack.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算分类度量——准确率、精确率、召回率和 F-1 分数。这些是在对抗攻击后的分数。
- en: Comparing the scores obtained in *steps 5* and *8* will tell us what the effectiveness
    of our attack was.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 比较在第 *5* 步和 *8* 步获得的分数将告诉我们我们的攻击效果如何。
- en: 'First, we split the data and extract features for our baseline model:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将数据分割并提取用于我们的基线模型的特征：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let us also set up an evaluation function that takes in the actual and predicted
    values and prints out our metrics, such as accuracy, precision, recall, and F-1
    score:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再设置一个评估函数，该函数接受实际值和预测值，并打印出我们的度量，如准确率、精确率、召回率和 F-1 分数：
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, we build and evaluate our baseline model. This model has no adversarial
    perturbation:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们构建并评估我们的基线模型。这个模型没有对抗扰动：
- en: '[PRE15]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'This should result in an output as shown:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会产生如下所示的输出：
- en: '![Figure 9.6 – Metrics for classification of normal data](img/B19327_09_06.jpg)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.6 – 正常数据分类的度量](img/B19327_09_06.jpg)'
- en: Figure 9.6 – Metrics for classification of normal data
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 – 正常数据分类的度量
- en: 'Next, we actually execute the attack. We obtain adversarial samples but train
    the model on the clean data (as during training we do not have access to the attacker’s
    adversarial set). Here, we use the `double_last_letter()` adversarial function
    to compute our adversarial set. We then evaluate the model on the adversarial
    samples:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们实际执行攻击。我们获得了对抗样本，但在干净的数据上训练了模型（因为在训练过程中我们没有访问攻击者的对抗集）。在这里，我们使用 `double_last_letter()`
    对抗函数来计算我们的对抗集。然后我们在对抗样本上评估模型：
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This should show you another set of metrics:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会显示另一组度量：
- en: '![Figure 9.7 – Metrics for classification of adversarial data](img/B19327_09_07.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.7 – 对抗数据分类的度量](img/B19327_09_07.jpg)'
- en: Figure 9.7 – Metrics for classification of adversarial data
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.7 – 对抗数据分类的度量
- en: 'Carefully note the differences between the scores obtained with the clean and
    adversarial data; we will compare them side by side for clarity, as follows:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细注意使用干净数据和对抗数据获得的分数之间的差异；我们将为了清晰起见，将它们并排比较，如下所示：
- en: '| **Metric** | **Clean** | **Adversarial** |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| **度量** | **干净** | **对抗** |'
- en: '| Accuracy | 0.93 | 0.88 |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| 准确率 | 0.93 | 0.88 |'
- en: '| Precision | 0.93 | 0.91 |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| 精确率 | 0.93 | 0.91 |'
- en: '| Recall | 0.90 | 0.79 |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| 召回率 | 0.90 | 0.79 |'
- en: '| F-1 score | 0.92 | 0.85 |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| F-1 分数 | 0.92 | 0.85 |'
- en: Table 9.1 – Comparing accuracy of normal versus adversarial data
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1 – 比较正常数据与对抗数据的准确率
- en: You can clearly see that our adversarial attack has been successful—while accuracy
    dropped only by 5%, the recall dropped by 11%, causing a 7% drop in the F-1 score.
    If you consider this at the scale of a social media network such as Twitter, it
    translates to significant performance degradation.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以清楚地看到我们的对抗攻击已经成功——虽然准确率只下降了 5%，但召回率下降了 11%，导致 F-1 分数下降了 7%。如果你考虑像 Twitter
    这样的社交媒体网络的规模，这会导致显著的性能下降。
- en: You can similarly evaluate the effect caused by the double-vowel attack. Simply
    generate adversarial examples using the double-vowel function instead of the double-last
    letter function. We will leave this as an exercise to the reader.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以类似地评估双元音攻击的效果。只需使用双元音函数而不是双末字母函数来生成对抗示例。我们将把这个作为读者的练习题。
- en: Further attacks
  id: totrans-202
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 进一步的攻击
- en: 'The previous section covered only two basic attacks for attacking text-based
    models. As a data scientist, you must think of all possible attack surfaces and
    come up with potential new attacks. You are encouraged to develop and implement
    new attacks and examine how they affect model performance. Some potential attacks
    are presented here:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节仅介绍了两种基本的攻击方法来攻击基于文本的模型。作为一名数据科学家，你必须考虑所有可能的攻击面，并提出潜在的新攻击。我们鼓励你开发和实施新的攻击，并检查它们对模型性能的影响。这里提出了一些潜在的攻击方法：
- en: Combining two words from the sentence (for example, *Live and Let Live* will
    become *Liveand* *Let Live*)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将句子中的两个单词合并（例如，*Live and Let Live* 将变为 *Liveand* *Let Live*）
- en: Splitting a long word into two words (for example, *Immigrants will take away
    our jobs* will become *Immi grants will take away* *our jobs*)
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将长单词拆分为两个单词（例如，*Immigrants will take away our jobs* 将变为 *Immi grants will take
    away* *our jobs*）
- en: Adding hyphens or commas to long words (for example, *Immigrants will take away
    our jobs* will become *Im-migrants will take away* *our jobs*)
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在长单词中添加连字符或逗号（例如，*Immigrants will take away our jobs* 将变为 *Im-migrants will
    take away* *our jobs*）
- en: 'Additionally, readers should also experiment with different feature extraction
    methods to examine whether any of them is more robust to adversarial attacks than
    the others, or our TF-IDF method. A few examples of such methods are set out here:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，读者还应尝试不同的特征提取方法，以检验其中是否有任何方法比其他方法或我们的TF-IDF方法对对抗攻击更具鲁棒性。以下是一些此类方法的例子：
- en: Word embeddings
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词嵌入
- en: Contextual embeddings (**Bidirectional Encoder Representations from Transformers**,
    or **BERT**)
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 上下文嵌入（**双向编码器表示从转换器**，或 **BERT**）
- en: Character-level features
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符级特征
- en: This completes our discussion on how text models can be fooled. In the next
    section, we will briefly discuss how models can be made robust against adversarial
    attacks.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这完成了我们对文本模型如何被欺骗的讨论。在下一节中，我们将简要讨论如何使模型对对抗攻击具有鲁棒性。
- en: Developing robustness against adversarial attacks
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 发展对抗攻击的鲁棒性
- en: Adversarial attacks can be a serious threat to the security and reliability
    of ML systems. Several techniques can be used to improve the robustness of ML
    models against adversarial attacks. Some of these are described next.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗攻击可能对机器学习系统的安全和可靠性构成严重威胁。可以使用几种技术来提高机器学习模型对对抗攻击的鲁棒性。以下是一些描述：
- en: Adversarial training
  id: totrans-214
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 对抗性训练
- en: Adversarial training is a technique where the model is trained on adversarial
    examples in addition to the original training data. Adversarial examples are generated
    by perturbing the original input data in such a way that the perturbed input is
    misclassified by the model. By training the model on both the original and adversarial
    examples, the model learns to be more robust to adversarial attacks. The idea
    behind adversarial training is to simulate the types of attacks that the model
    is likely to face in the real world and make the model more resistant to them.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对抗性训练是一种技术，其中模型除了原始训练数据外，还在对抗性示例上进行训练。对抗性示例是通过扰动原始输入数据生成的，使得扰动后的输入被模型错误分类。通过在原始和对抗性示例上训练模型，模型学会对对抗攻击更加鲁棒。对抗性训练背后的思想是模拟模型在现实世界中可能遇到的攻击类型，并使模型对这些攻击更具抵抗力。
- en: Defensive distillation
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 防御蒸馏
- en: Defensive distillation is a technique that involves training a model on soft
    targets rather than hard targets. Soft targets are probability distributions over
    the classes, while hard targets are one-hot vectors indicating the correct class.
    By training on soft targets, the decision boundaries of the model become smoother
    and more difficult to attack. This is because the soft targets contain more information
    about the distribution of the classes, which makes it more difficult to create
    an adversarial example that will fool the model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 防御蒸馏是一种涉及在软目标上而不是在硬目标上训练模型的技术。软目标是关于类别的概率分布，而硬目标是指示正确类别的one-hot向量。通过在软目标上训练，模型的决策边界变得更加平滑且难以攻击。这是因为软目标包含了关于类别分布的更多信息，这使得创建能够欺骗模型的对抗性示例变得更加困难。
- en: Gradient regularization
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 梯度正则化
- en: Gradient regularization is a technique that involves adding a penalty term to
    the loss function of the model that encourages the gradients of the model to be
    small. This helps to prevent an attacker from creating an adversarial example
    by perturbing the input in the direction of the gradient. The penalty term can
    be added to the loss function in various ways, such as through L1 or L2 regularization
    or by using adversarial training. Gradient regularization can be combined with
    other techniques to improve the robustness of the model.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度正则化是一种涉及向模型损失函数中添加惩罚项的技术，该惩罚项鼓励模型梯度的值较小。这有助于防止攻击者通过扰动输入方向来创建对抗性示例。惩罚项可以通过多种方式添加到损失函数中，例如通过L1或L2正则化或使用对抗性训练。梯度正则化可以与其他技术结合使用，以提高模型的鲁棒性。
- en: Input preprocessing
  id: totrans-220
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 输入预处理
- en: Input preprocessing involves modifying the input data before it is fed into
    the model. This can include techniques such as data normalization, which helps
    to reduce the sensitivity of the model to small changes in the input. Other techniques
    include randomization of the input, which can help to disrupt the pattern of the
    adversarial attack, or filtering out anomalous input that may be indicative of
    an adversarial attack. Input preprocessing can be tailored to the specific model
    and the type of input data that it receives.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 输入预处理涉及在将数据输入模型之前修改输入数据。这可能包括数据归一化等技术，有助于减少模型对输入微小变化的敏感性。其他技术包括输入的随机化，这有助于破坏对抗性攻击的模式，或者过滤掉可能表明对抗性攻击的异常输入。输入预处理可以根据特定的模型和接收到的输入数据类型进行定制。
- en: Ensemble methods
  id: totrans-222
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 集成方法
- en: Ensemble methods involve combining multiple models to make a prediction. This
    can improve the robustness of the model by making it more difficult for an attacker
    to craft an adversarial example that will fool all the models in the ensemble.
    Ensemble methods can be used in conjunction with other techniques to further improve
    the robustness of the model.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法涉及结合多个模型进行预测。这可以通过使攻击者更难构建一个能够欺骗集成中所有模型的对抗性示例来提高模型的鲁棒性。集成方法可以与其他技术结合使用，以进一步提高模型的鲁棒性。
- en: Certified defenses
  id: totrans-224
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 认证防御
- en: Certified defenses involve creating a provable guarantee that a model will be
    robust to a certain level of adversarial attack. This can be done using techniques
    such as interval-bound propagation or randomized smoothing. Interval-bound propagation
    involves computing a range of values that the model’s output can take given a
    certain range of inputs. This range can be used to create a provable bound on
    the robustness of the model. Randomized smoothing involves adding random noise
    to the input data to make the model more robust to adversarial attacks. Certified
    defenses are a relatively new area of research, but hold promise for creating
    more robust ML models.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 认证防御涉及创建一个可证明的保证，即模型将对一定程度的对抗性攻击具有鲁棒性。这可以通过使用诸如区间传播或随机平滑等技术来实现。区间传播涉及计算模型输出可以取的值的范围，给定一定范围的输入。这个范围可以用来创建模型鲁棒性的可证明界限。随机平滑涉及向输入数据添加随机噪声，使模型对对抗性攻击更具鲁棒性。认证防御是一个相对较新的研究领域，但有望创建更鲁棒的机器学习模型。
- en: It’s worth noting that while these techniques can improve the robustness of
    ML models against adversarial attacks, they are not foolproof, and there is still
    ongoing research in this area. It’s important to use multiple techniques in combination
    to improve the robustness of the model.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，尽管这些技术可以提高机器学习模型对抗对抗性攻击的鲁棒性，但它们并非万无一失，并且在这个领域仍有持续的研究。使用多种技术相结合来提高模型的鲁棒性是很重要的。
- en: With that, we have come to the end of this chapter.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 由此，我们已到达本章的结尾。
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In recent times, human reliance on ML has grown exponentially. ML models are
    involved in several security-critical applications such as fraud, abuse, and other
    kinds of cybercrime. However, many models are susceptible to adversarial attacks,
    where attackers manipulate the input so as to fool the model. This chapter covered
    the basics of AML and the goals and strategies that attackers employ. We then
    discussed two popular adversarial attack methods, FGSM and PGD, along with their
    implementation in Python. Next, we learned about methods for manipulating text
    and their implementation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，人类对机器学习的依赖呈指数级增长。机器学习模型涉及多个安全关键应用，如欺诈、滥用和其他类型的网络犯罪。然而，许多模型容易受到对抗性攻击的影响，攻击者通过操纵输入来欺骗模型。本章涵盖了反洗钱（AML）的基础知识、攻击者的目标和策略。然后，我们讨论了两种流行的对抗性攻击方法，即FGSM和PGD，以及它们在Python中的实现。接下来，我们学习了操纵文本的方法及其实现。
- en: Because of the importance and prevalence of ML in our lives, it is necessary
    for security data scientists to understand adversarial attacks and learn to defend
    against them. This chapter provides a solid foundation for AML and the kinds of
    attacks involved.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器学习在我们的生活中的重要性和普遍性，安全数据科学家了解对抗性攻击并学会防御它们是必要的。本章为AML和涉及到的攻击类型提供了坚实的基础。
- en: So far, we have discussed multiple aspects of ML for security problems. In the
    next chapter, we will pivot to a closely related topic—privacy.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了机器学习在安全问题中的多个方面。在下一章中，我们将转向一个密切相关的话题——隐私。
